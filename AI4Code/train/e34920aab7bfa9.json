{"cell_type":{"2d65747a":"code","3e1388f0":"code","66a2d0d9":"code","e244fbfe":"code","cbfec4bb":"code","6c3aab0d":"code","a6677729":"code","09ea84e3":"code","039dbd1a":"code","c636e033":"code","c83093b6":"code","c0b263ad":"code","1247df72":"code","02209439":"code","3e061c9e":"code","2e09a34d":"code","08999a42":"code","bdac6d69":"code","9ff7e40a":"code","064377dc":"code","27774cb4":"code","e156345c":"code","657e7f1f":"code","d5028522":"code","7c38f5bd":"code","5698768e":"code","d39c32ab":"code","7bd1ee4c":"code","6ae7aa01":"code","435f03fe":"code","dbefb05e":"code","80e67f33":"code","bc882f00":"code","186dfee7":"code","85a4e059":"code","d6cd4d34":"code","e88b1575":"code","3052ad33":"code","bb9e462a":"code","cab9e58c":"code","b74bbcfa":"code","e4032ec6":"code","d76457fa":"code","b6ac4a3f":"code","4b447a49":"code","06580014":"code","94a203b3":"code","096b3dff":"code","f30947ca":"code","26e4b373":"code","aa09a942":"code","c5776e0a":"code","3dcd1667":"code","b2f2a92e":"code","31da4f17":"code","0e6dc66a":"code","65aedac5":"code","a34efd37":"code","0de3e414":"code","ef03c39c":"code","3351d1b9":"code","28c408a3":"code","82a8b563":"code","4f7ebafa":"code","e396ba96":"code","e9d79c6e":"code","9e20348f":"code","75d9304f":"code","04c0bbad":"code","9658294a":"code","0744b1c8":"code","0e7fa9b8":"code","9b3ec0e4":"code","f7ccd6ba":"code","215dbd77":"code","c9105cab":"code","e5890599":"code","7f03ee94":"code","e8d55325":"code","fc65070c":"code","5dd59b17":"code","2c7aa678":"code","887076d2":"code","7d0f9c88":"code","dcfe6cf0":"code","dfe79417":"code","bffa526e":"code","03d9b3b2":"code","fd8ea7f1":"code","1536627d":"markdown","a6272e27":"markdown","da7b8be3":"markdown","a01ad77f":"markdown","b3f94781":"markdown","c9d73bf0":"markdown","16bec727":"markdown","74aa9296":"markdown","1ac85732":"markdown","98b79e46":"markdown","2c350047":"markdown","37b3b7ca":"markdown","ccb85384":"markdown","a519ecee":"markdown","154cdccc":"markdown","398b9935":"markdown","c4bfdef3":"markdown","17cfc0e4":"markdown","1f658a8e":"markdown","6d1c818b":"markdown","85c9c77c":"markdown","f2146029":"markdown","0a846224":"markdown","dfde343d":"markdown","05e0f066":"markdown","92625267":"markdown","e303d1ec":"markdown","a81bbf20":"markdown","4f976b3a":"markdown","02df39bb":"markdown","378ad2a7":"markdown","8e70c154":"markdown","90163430":"markdown","d1ed8a76":"markdown","fa965171":"markdown","3b88658f":"markdown","d858160f":"markdown","9e2afdcb":"markdown","a195e21a":"markdown","a088aabf":"markdown","d9a848f6":"markdown","247cabb9":"markdown","aacdcfe6":"markdown","42773d67":"markdown","f778d442":"markdown"},"source":{"2d65747a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport statsmodels.api as sm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVR\nfrom sklearn.svm import SVR\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost\n\nsns.set(color_codes=False)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3e1388f0":"def display_scores(scores_train):\n    print(\"###### MODEL EVALUATION WITH CROSS VALIDATION######\") \n    print(\"Scores :\", scores_train)\n    print(\"Mean :\", scores_train.mean())   \n    print(\"Standard Deviation :\", scores_train.std())\n","66a2d0d9":"import pandas as pd\nSubmission = pd.read_csv(\"..\/input\/big-mart-sales-prediction\/Submission.csv\")\ntest = pd.read_csv(\"..\/input\/big-mart-sales-prediction\/Test.csv\")\ntrain = pd.read_csv(\"..\/input\/big-mart-sales-prediction\/Train.csv\")","e244fbfe":"train.head()","cbfec4bb":"train.describe().transpose()","6c3aab0d":"train.info()","a6677729":"test.info()","09ea84e3":"train.loc[:,['Outlet_Size','Outlet_Type','Outlet_Location_Type']].drop_duplicates()","039dbd1a":"train.loc[:,['Outlet_Size','Outlet_Type']].drop_duplicates()","c636e033":"train.loc[:,['Outlet_Size','Outlet_Location_Type']].drop_duplicates()","c83093b6":"test.loc[:,['Outlet_Size','Outlet_Type','Outlet_Location_Type']].drop_duplicates()","c0b263ad":"def data_treatment(train):\n    # Standardize the different variations of Low fat and regular.\n    train['Item_Fat_Content'] = train['Item_Fat_Content'].str.replace('LF', 'Low Fat')\n    train['Item_Fat_Content'] = train['Item_Fat_Content'].str.replace('reg', 'Regular')\n    train['Item_Fat_Content'] = train['Item_Fat_Content'].str.title()\n\n    # NA Treatment: Outlet size and item weight have null values.\n    # For Outlet size, we can refer to our data exploration below to see the best suitable replacement based on location type and outlet type\n    #if train['Outlet_Size'] == NA ]\n\n    for i in range(train.shape[0]):\n        if((train.loc[i, 'Outlet_Location_Type'] == 'Tier 2')):\n            train.loc[i, 'Outlet_Size'] = 'Small'\n\n        elif((train.loc[i, 'Outlet_Type'] == 'Grocery Store')): \n            train.loc[i, 'Outlet_Size'] = 'Small'\n\n    # For Item type, lets use the median value by item type and item fat content\n\n    WeightLookup = train.groupby(['Item_Fat_Content','Item_Type'])['Item_Weight'].agg(np.mean).reset_index()\n    train = pd.merge(train, WeightLookup, how = 'inner', on = ['Item_Fat_Content','Item_Type'])\n    train['Item_Wt'] = np.where(train['Item_Weight_x'].isnull(), train['Item_Weight_y'], train['Item_Weight_x'])\n    train.drop(['Item_Weight_x','Item_Weight_y'], axis=1, inplace=True)\n    return train","1247df72":"train = data_treatment(train)\ntest = data_treatment(test)","02209439":"sns.distplot(train['Item_Weight'],hist=True, kde=True, rug=False)","3e061c9e":"sns.boxplot(y=train['Item_Weight'])","2e09a34d":"sns.relplot(x='Item_Weight', y='Item_Outlet_Sales', data=train)","08999a42":"sns.lmplot( x=\"Item_Weight\", y=\"Item_Outlet_Sales\", data=train, fit_reg=False, hue='Outlet_Location_Type', legend=False, scatter_kws={\"alpha\":0.3,\"s\":20})","bdac6d69":"sns.distplot(train['Item_Visibility'], hist=True, kde=False, rug=False)","9ff7e40a":"sns.boxplot(y=train['Item_Visibility'])","064377dc":"sns.relplot(x='Item_Visibility', y='Item_Outlet_Sales', data=train)","27774cb4":"# Store sales by visibility and store type\nsns.relplot(x='Item_Visibility', y='Item_Outlet_Sales', col = 'Outlet_Type', data=train)","e156345c":"# Store sales by visibility and store loc type\nsns.relplot(x='Item_Visibility', y='Item_Outlet_Sales', col = 'Outlet_Location_Type', data=train)","657e7f1f":"# Store sales by visibility and store size\nsns.relplot(x='Item_Visibility', y='Item_Outlet_Sales', col = 'Outlet_Size' ,data=train)","d5028522":"train.columns","7c38f5bd":"sns.distplot(train['Item_MRP'])","5698768e":"# Sales by Item_MRP\nsns.relplot(x=\"Item_MRP\", y=\"Item_Outlet_Sales\", data=train)","d39c32ab":"train[\"Item_Fat_Content\"].value_counts(normalize=True)","7bd1ee4c":"#scatter plot\nsns.catplot(x=\"Item_Fat_Content\", y=\"Item_Outlet_Sales\", data=train )","6ae7aa01":"# Swarm Plot Useful for only small datasets\nsns.catplot(x=\"Item_Fat_Content\", y=\"Item_Outlet_Sales\", kind=\"swarm\", data=train )","435f03fe":"# Box plot\nsns.catplot(x=\"Item_Fat_Content\", y=\"Item_Outlet_Sales\", kind=\"box\", data=train )","dbefb05e":"sns.catplot(x=\"Item_Fat_Content\", y=\"Item_Outlet_Sales\", hue = \"Outlet_Location_Type\", kind=\"box\", data=train )","80e67f33":"# violinplot a boxplot with the kernel density estimation procedure\nsns.catplot(x=\"Item_Fat_Content\", y=\"Item_Outlet_Sales\", hue = \"Outlet_Location_Type\", kind=\"violin\", data=train);","bc882f00":"sns.barplot(x=\"Item_Fat_Content\", y=\"Item_Outlet_Sales\", data=train)","186dfee7":"# Variation is sales with Item Fat Content by store location\nsns.catplot(x=\"Item_Fat_Content\", y=\"Item_Outlet_Sales\", hue=\"Outlet_Location_Type\", kind=\"bar\", data=train)","85a4e059":"sns.catplot(x=\"Item_Fat_Content\", y=\"Item_Outlet_Sales\", hue=\"Outlet_Location_Type\", kind=\"point\", data=train)","d6cd4d34":"train[\"Item_Type\"].value_counts(normalize=True)","e88b1575":"sns.catplot(x=\"Item_Type\", y=\"Item_Outlet_Sales\", kind='bar', data=train, height=5, aspect=3)","3052ad33":"train[\"Outlet_Location_Type\"].value_counts(normalize=True)","bb9e462a":"sns.barplot(x=\"Outlet_Location_Type\", y=\"Item_Outlet_Sales\", data=train)","cab9e58c":"sns.catplot(x=\"Outlet_Location_Type\", y=\"Item_Outlet_Sales\", kind=\"box\", data=train)","b74bbcfa":"sns.catplot(x=\"Outlet_Location_Type\", y=\"Item_Outlet_Sales\", kind=\"box\", col='Outlet_Size',data=train)","e4032ec6":"sns.catplot(y=\"Outlet_Type\", x=\"Item_Outlet_Sales\", kind=\"bar\", data=train, height=5, aspect=2)","d76457fa":"sns.catplot(y=\"Outlet_Type\", x=\"Item_Outlet_Sales\", kind=\"box\", data=train, height=5, aspect=2)","b6ac4a3f":"sns.catplot(x=\"Outlet_Type\", y=\"Item_Outlet_Sales\", kind=\"violin\", data=train, height=5, aspect=2)","4b447a49":"sns.catplot(y=\"Outlet_Type\", x=\"Item_Outlet_Sales\", row=\"Outlet_Location_Type\", kind=\"box\", data=train, height=5, aspect=2)","06580014":"sns.catplot(y=\"Outlet_Type\", x=\"Item_Outlet_Sales\", row=\"Outlet_Size\", kind=\"box\", data=train, height=5, aspect=2)","94a203b3":"train.columns","096b3dff":"sns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind=\"bar\", data=train)","f30947ca":"sns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind=\"box\", data=train)","26e4b373":"sns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind=\"violin\", data=train)","aa09a942":"sns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", hue =\"Outlet_Location_Type\" ,kind=\"violin\", data=train)","c5776e0a":"sns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", col =\"Outlet_Type\" ,kind=\"violin\", data=train)","3dcd1667":"train[\"Outlet_Establishment_Year\"].value_counts(normalize=True)","b2f2a92e":"sns.catplot(x=\"Outlet_Establishment_Year\", y=\"Item_Outlet_Sales\", kind=\"bar\", data=train)","31da4f17":"sns.catplot(y=\"Item_Outlet_Sales\", x=\"Outlet_Establishment_Year\", kind=\"box\", data=train, height=5, aspect=2)","0e6dc66a":"# OneHot encoding\ntrain_ohe = pd.get_dummies(train, columns=['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type'], drop_first=True)\nX = train_ohe[ ['Item_Visibility', 'Item_MRP',\n       'Item_Wt',\n       'Item_Fat_Content_Regular', 'Item_Type_Breads', 'Item_Type_Breakfast',\n       'Item_Type_Canned', 'Item_Type_Dairy', 'Item_Type_Frozen Foods',\n       'Item_Type_Fruits and Vegetables', 'Item_Type_Hard Drinks',\n       'Item_Type_Health and Hygiene', 'Item_Type_Household', 'Item_Type_Meat',\n       'Item_Type_Others', 'Item_Type_Seafood', 'Item_Type_Snack Foods',\n       'Item_Type_Soft Drinks', 'Item_Type_Starchy Foods',\n       'Outlet_Establishment_Year_1987', 'Outlet_Establishment_Year_1997',\n       'Outlet_Establishment_Year_1998', 'Outlet_Establishment_Year_1999',\n       'Outlet_Establishment_Year_2002', 'Outlet_Establishment_Year_2004',\n       'Outlet_Establishment_Year_2007', 'Outlet_Establishment_Year_2009',\n       'Outlet_Size_Medium', 'Outlet_Size_Small',\n       'Outlet_Location_Type_Tier 2', 'Outlet_Location_Type_Tier 3',\n        'Outlet_Type_Supermarket Type1', 'Outlet_Type_Supermarket Type2',\n       'Outlet_Type_Supermarket Type3']]\n\nY = train_ohe['Item_Outlet_Sales']","65aedac5":"#OneHot Encoding Test\ntest_ohe = pd.get_dummies(test, columns=['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type'], drop_first=True)\nX_final_test = test_ohe[ ['Item_Visibility', 'Item_MRP',\n       'Item_Wt',\n       'Item_Fat_Content_Regular', 'Item_Type_Breads', 'Item_Type_Breakfast',\n       'Item_Type_Canned', 'Item_Type_Dairy', 'Item_Type_Frozen Foods',\n       'Item_Type_Fruits and Vegetables', 'Item_Type_Hard Drinks',\n       'Item_Type_Health and Hygiene', 'Item_Type_Household', 'Item_Type_Meat',\n       'Item_Type_Others', 'Item_Type_Seafood', 'Item_Type_Snack Foods',\n       'Item_Type_Soft Drinks', 'Item_Type_Starchy Foods',\n       'Outlet_Establishment_Year_1987', 'Outlet_Establishment_Year_1997',\n       'Outlet_Establishment_Year_1998', 'Outlet_Establishment_Year_1999',\n       'Outlet_Establishment_Year_2002', 'Outlet_Establishment_Year_2004',\n       'Outlet_Establishment_Year_2007', 'Outlet_Establishment_Year_2009',\n       'Outlet_Size_Medium', 'Outlet_Size_Small',\n       'Outlet_Location_Type_Tier 2', 'Outlet_Location_Type_Tier 3',\n        'Outlet_Type_Supermarket Type1', 'Outlet_Type_Supermarket Type2',\n       'Outlet_Type_Supermarket Type3']]\n","a34efd37":"features = ['Item_Visibility', 'Item_MRP',\n       'Item_Wt',\n       'Item_Fat_Content_Regular', 'Item_Type_Breads', 'Item_Type_Breakfast',\n       'Item_Type_Canned', 'Item_Type_Dairy', 'Item_Type_Frozen Foods',\n       'Item_Type_Fruits and Vegetables', 'Item_Type_Hard Drinks',\n       'Item_Type_Health and Hygiene', 'Item_Type_Household', 'Item_Type_Meat',\n       'Item_Type_Others', 'Item_Type_Seafood', 'Item_Type_Snack Foods',\n       'Item_Type_Soft Drinks', 'Item_Type_Starchy Foods',\n       'Outlet_Establishment_Year_1987', 'Outlet_Establishment_Year_1997',\n       'Outlet_Establishment_Year_1998', 'Outlet_Establishment_Year_1999',\n       'Outlet_Establishment_Year_2002', 'Outlet_Establishment_Year_2004',\n       'Outlet_Establishment_Year_2007', 'Outlet_Establishment_Year_2009',\n       'Outlet_Size_Medium', 'Outlet_Size_Small',\n       'Outlet_Location_Type_Tier 2', 'Outlet_Location_Type_Tier 3',\n        'Outlet_Type_Supermarket Type1', 'Outlet_Type_Supermarket Type2',\n       'Outlet_Type_Supermarket Type3']","0de3e414":"# Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42)","ef03c39c":"mod = sm.OLS(y_train, X_train)\nres = mod.fit()\nprint(res.summary())","3351d1b9":"# Linear Regression\nlm = LinearRegression().fit(X_train, y_train)\ny_predict = lm.predict(X_train)\ny_test_predict = lm.predict(X_test)\n\nprint('Linear model, coefficients: ', lm.coef_)\nprint('Root Mean squared error(Train): {:.2f}'.format(np.sqrt(mean_squared_error(y_train , y_predict))))\nprint('Root Mean squared error(Test): {:.2f}'.format(np.sqrt(mean_squared_error(y_test , y_test_predict))))\nprint('r2_score (linear model): {:.2f}'.format(r2_score(y_train, y_predict)))","28c408a3":"#checking the magnitude of coefficients\n\npredictors = X_train.columns\ncoef = pd.Series(lm.coef_,predictors).sort_values()\ncoef.plot(kind='bar', title='Modal Coefficients')","82a8b563":"# Model Evaluation using Cross Validation\nscores_train = cross_val_score(linreg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5)\nlinreg_rmse_scores = np.sqrt(-scores)\ndisplay_scores(linreg_rmse_scores)","4f7ebafa":"# Export Predicitons\ny_test_predict = linridge.predict(X_test_final_scaled)\ny_test_predict = np.where(y_test_predict < 0 , 0, y_test_predict )\nsum(y_test_predict < 0)\ntest['Item_Outlet_Sales'] = y_test_predict\nsubmission = test[['Item_Identifier', 'Outlet_Identifier','Item_Outlet_Sales']]\nsubmission.to_csv('submission6.csv',mode = 'w', index=False)","e396ba96":"features = ['Item_Visibility', 'Item_MRP', 'Item_Fat_Content_Regular',\n            'Outlet_Location_Type_Tier 2', 'Outlet_Location_Type_Tier 3',\n            'Outlet_Type_Supermarket Type1', 'Outlet_Type_Supermarket Type2',\n            'Outlet_Type_Supermarket Type3']\n\ntrain_sizes = [1, 100, 500, 2000, 4000, 5113]","e9d79c6e":"# Learning Curve\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, validation_scores = learning_curve(\nestimator = LinearRegression(),\nX = X_train[features],\ny = y_train, \ntrain_sizes = train_sizes, \ncv = 5,\nscoring = 'neg_mean_squared_error')","9e20348f":"print('Training scores:\\n\\n', train_scores)\nprint('\\n', '-' * 70) # separator to make the output easy to read\nprint('\\nValidation scores:\\n\\n', validation_scores)","75d9304f":"train_scores_mean = -train_scores.mean(axis = 1)\nvalidation_scores_mean = -validation_scores.mean(axis = 1)\nprint('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\nprint('\\n', '-' * 20) # separator\nprint('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))","04c0bbad":"import matplotlib.pyplot as plt\n\nplt.style.use('seaborn')\nplt.plot(train_sizes, train_scores_mean, label = 'Training error')\nplt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\nplt.ylabel('MSE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\nplt.legend()\nplt.ylim(0,2000000)","9658294a":"#for alpha in [0.01,0.05,0.1,0.5, 1, 2, 3, 5, 10, 20, 50]:\nlinridge = Ridge(alpha=50).fit(X_train, y_train)\ny_predict_train = linridge.predict(X_train)\ny_predict_test = linridge.predict(X_test)\nprint('Model results for alpha = {}'.format(alpha) )\nprint('ridge regression linear model intercept: {}'.format(linridge.intercept_))\n#print('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\nprint('R-squared score (training): {:.3f}'.format(linridge.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linridge.score(X_test, y_test)))\nprint('RMSE TRAIN: {:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_predict_train))))\nprint('RMSE TEST: {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_predict_test))))\nprint('Number of non-zero features: {}'.format(np.sum(linridge.coef_ != 0)))\nprint('#########################################################################')","0744b1c8":"#checking the magnitude of coefficients\n\npredictors = X_train.columns\ncoef = pd.Series(linridge.coef_,predictors).sort_values()\ncoef.plot(kind='bar', title='Modal Coefficients')","0e7fa9b8":"# Ridge regression with feature normalization\n\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_test_final_scaled = scaler.transform(X_final_test)\n\nlinridge = Ridge(alpha=0.01).fit(X_train_scaled, y_train)\ny_predict_train = linridge.predict(X_train_scaled) \ny_predict_test = linridge.predict(X_test_scaled)\n\n#print('Model results for alpha = {}'.format(alpha) )\nprint('ridge regression linear model intercept: {}'.format(linridge.intercept_))\nprint('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\nprint('R-squared score (training): {:.3f}'.format(linridge.score(X_train_scaled, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linridge.score(X_test_scaled, y_test)))\nprint('Number of non-zero features: {}'\n.format(np.sum(linridge.coef_ != 0)))\nprint('RMSE TRAIN: {:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_predict_train))))\nprint('RMSE TEST: {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_predict_test))))\nprint('#########################################################################')","9b3ec0e4":"predictors = X_train.columns\ncoef = pd.Series(linridge.coef_,predictors).sort_values()\ncoef.plot(kind='bar', title='Modal Coefficients')","f7ccd6ba":"from sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n#for alpha in [0.01, 0.1, 0.5, 1, 2, 3, 5, 10, 20, 50]:\nlinlasso = Lasso(alpha=0.05, max_iter = 10000).fit(X_train_scaled, y_train)\ny_predict = linlasso.predict(X_test_scaled)\nprint('ridge regression linear model intercept: {}'.format(linlasso.intercept_))\nprint('ridge regression linear model coeff:\\n{}'.format(linlasso.coef_))\nprint('R-squared score (training): {:.3f}'.format(linlasso.score(X_train_scaled, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linlasso.score(X_test_scaled, y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(linlasso.coef_ != 0)))\nprint('RMSE TRAIN: {:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_predict_train))))\nprint('RMSE TEST: {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_predict_test))))\nprint('#########################################################################')","215dbd77":"predictors = X_train.columns\ncoef = pd.Series(linlasso.coef_,predictors).sort_values()\ncoef.plot(kind='bar', title='Modal Coefficients')","c9105cab":"for epsilon in [1,3,5,7,9]:\n    svm_reg = LinearSVR(epsilon=epsilon)\n    svm_reg.fit(X_train,y_train)\n    y_predict_train = svm_reg.predict(X_train)\n    y_predict_test = svm_reg.predict(X_test)\n    print('Model results for epsilon = {}'.format(epsilon) )\n    print('ridge regression linear model intercept: {}'.format(svm_reg.intercept_))\n    #print('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\n    print('R-squared score (training): {:.3f}'.format(svm_reg.score(X_train, y_train)))\n    print('R-squared score (test): {:.3f}'.format(svm_reg.score(X_test, y_test)))\n    print('RMSE TRAIN: {:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_predict_train))))\n    print('RMSE TEST: {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_predict_test))))\n    print('#################################################################################')","e5890599":"svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\nsvm_poly_reg.fit(X_train,y_train)\ny_predict_train = svm_poly_reg.predict(X_train)\ny_predict_test = svm_poly_reg.predict(X_test)\nprint('Model results for epsilon = {}'.format(epsilon) )\nprint('ridge regression linear model intercept: {}'.format(svm_poly_reg.intercept_))\n#print('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\nprint('R-squared score (training): {:.3f}'.format(svm_poly_reg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(svm_poly_reg.score(X_test, y_test)))\nprint('RMSE TRAIN: {:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_predict_train))))\nprint('RMSE TEST: {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_predict_test))))\nprint('#################################################################################')","7f03ee94":"from sklearn.tree import DecisionTreeRegressor\n\nfor depth in [2,3,4,5,6,7,8,9]:\n    tree_reg = DecisionTreeRegressor(max_depth=depth)\n    tree_reg.fit(X_train, y_train)\n\n    y_predict_train = tree_reg.predict(X_train)\n    y_predict_test = tree_reg.predict(X_test)\n    \n    print('Depth equals: {:.1f}'.format(depth))\n    #print('ridge regression linear model intercept: {}'.format(tree_reg.intercept_))\n    #print('ridge regression linear model coeff:\\n{}'.format(tree_reg.coef_))\n    print('R-squared score (training): {:.3f}'.format(tree_reg.score(X_train, y_train)))   \n    print('RMSE Train: {:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_predict_train))))\n    print('RMSE Test: {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_predict_test))))\n    #print('Number of non-zero features: {}'.format(np.sum(tree_reg.coef_ != 0)))\n    print('#########################################################################')","e8d55325":"# Model Evaluation with Cross Validation\nscores = cross_val_score(tree_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5)\ntree_reg_rmse_scores = np.sqrt(-scores)\ndisplay_scores(tree_reg_rmse_scores)","fc65070c":"# Export Model Predictions\ny_test_predict = tree_reg.predict(X_final_test)\ny_test_predict = np.where(y_test_predict < 0 , 0, y_test_predict )\ntest['Item_Outlet_Sales'] = y_test_predict\nsubmission = test[['Item_Identifier', 'Outlet_Identifier','Item_Outlet_Sales']]\nsubmission.to_csv('submission1.csv',mode = 'w', index=False)","5dd59b17":"# Random Forest Model\n#for n in [3,6,9,12,15,18,21,24]:\nforest_reg = RandomForestRegressor(n_estimators= 500 , random_state=0)\nforest_reg.fit(X_train, y_train)\ny_predict_train = forest_reg.predict(X_train)\ny_predict_test = forest_reg.predict(X_test)\n#print('n_estimators: {:.1f}'.format(n))\n#print('ridge regression linear model intercept: {}'.format(tree_reg.intercept_))\n#print('ridge regression linear model coeff:\\n{}'.format(tree_reg.coef_))\nprint('R-squared score (training): {:.3f}'.format(forest_reg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(forest_reg.score(X_test, y_test)))\nprint('RMSE Train: {:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_predict_train))))\nprint('RMSE Test: {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_predict_test))))\n#print('Number of non-zero features: {}'.format(np.sum(tree_reg.coef_ != 0)))\nprint('#########################################################################')","2c7aa678":"# Feature importances\nfor name, score in zip(train_ohe.columns, forest_reg.feature_importances_):\n    print(name, score)","887076d2":"feature_imp = pd.DataFrame({'features':features, 'scores':forest_reg.feature_importances_})\nfeature_imp","7d0f9c88":"# Model Evaluation with Cross Validation\nscores = cross_val_score(forest_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5)\nforest_reg_rmse_scores = np.sqrt(-scores)\ndisplay_scores(forest_reg_rmse_scores)","dcfe6cf0":"# Hyperparameter Tuning\n\nparam_grid = [\n    {'n_estimators':[3,10,30,50], 'max_features':[4,8,12,16,20,24,28,32]},\n    {'bootstrap':[False],'n_estimators':[3,10],'max_features': [4,8,12,16,20]}\n];\n\nforest_regr = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_regr, param_grid, cv=5, scoring = 'neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(X_train, y_train)     \n     ","dfe79417":"# Best params\nprint(grid_search.best_params_)\n\n#Best Estimator\nprint(grid_search.best_estimator_)","bffa526e":"xgb_reg = xgboost.XGBRegressor()\nxgb_reg.fit(X_train, y_train)\ny_predict = xgb_reg.predict(X_test)\n#print('n_estimators: {:.1f}'.format(n))\n#print('ridge regression linear model intercept: {}'.format(tree_reg.intercept_))\n#print('ridge regression linear model coeff:\\n{}'.format(tree_reg.coef_))\nprint('R-squared score (training): {:.3f}'.format(xgb_reg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(xgb_reg.score(X_test, y_test)))\nprint('Root Mean squared error (linear model): {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_predict))))\n#print('Number of non-zero features: {}'.format(np.sum(tree_reg.coef_ != 0)))\nprint('#########################################################################')","03d9b3b2":"y_test_predict = xgb_reg.predict(X_final_test)\ny_test_predict = np.where(y_test_predict < 0 , 0, y_test_predict )","fd8ea7f1":"test['Item_Outlet_Sales'] = y_test_predict\nsubmission = test[['Item_Identifier', 'Outlet_Identifier','Item_Outlet_Sales']]\nsubmission.to_csv('submission8.csv',mode = 'w', index=False)","1536627d":"### *How does Sales of a product vary with Item Visibility ?*\nNo relationship between visibility and sales.","a6272e27":"* Tier1 location has SM1 and Grocery store.\n* Tier2 location has SM1.\n* Tier3 location has SM1, SM2, SM3, Grocery store.","da7b8be3":"## One Hot Encoding","a01ad77f":"# Exploration of Categorical features\n* Item_Fat_Content\n* Item_Type\n* Outlet_Identifier\n* Outlet_Establishment_Year\n* Outlet_Size\n* Outlet_Location_Type\n* Outlet_Type\n","b3f94781":"sku_store_sales = f(weight, fat_content, visibility, type, mrp) <br>\nsku_store_sales = g(est_yr, str_size, str_loc_type, str_type)","c9d73bf0":"## Item Visibility\nThe % of total display area of all products in a store allocated to the particular product","16bec727":"**Problem Statement**\n\nThe data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.\nUsing this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.\nPlease note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.\n\nObjective: To predict sales at a item store level.","74aa9296":"### *What is the average sales by outlet type?*\nMean sales Supermarket Type3 >> SuperMarketType1 > SuperMarketType2 >> Grocery Store ","1ac85732":"### XGBoost","98b79e46":"Mean sales Medium > High > Small","2c350047":"### *How does Item weight vary with item sales?*\nThere seems to be no relationship between item weight and item sales and I didn't expect any either","37b3b7ca":"# Outlet Size\n* Small - SM1, Grocery store\n* Medium - SM1, SM2, SM3\n* High - SM1","ccb85384":"### *How does the sales vary by outlet size?*","a519ecee":"# Data Treatment","154cdccc":"### *How does the distribuion of Item Visibility look like?*","398b9935":"## Linear Regression using Scikit Learn","c4bfdef3":"### Ridge Regression with feature normalization","17cfc0e4":"# Store Location Type (Tier)\n* Tier1 location has SM1 and Grocery store & Small, Medium stores\n* Tier2 location has SM1 & Medium size stores\n* Tier3 location has SM1, SM2, SM3, Grocery store & Meduim, High size stores","1f658a8e":"### Random Forest Regression","6d1c818b":"### *How does the sales vary by outlet size for different store tiers,type?*","85c9c77c":"# Outlet Establishment Year","f2146029":"## 1. Item Weight","0a846224":"### *How does sales vary by outlet type?*","dfde343d":"### *How does the sales of products vary across stores based on Item Fat Content?*\nMean sales for Low fat and regular items seems to be similar","05e0f066":"### Linear Regression using Statsmodels API","92625267":"### *How does the Sales of a product vary by establishment year of a store?* ","e303d1ec":"**Data related questions**\n* How many Item_Type do we have? How does sales relate to each Item_Type? <br> \n* How many Outlet_Location_Type do we have? How does sales relate to each Outlet_Location_Type?<br>\n* How many Outlet_Type do we have? How does sales relate to each Outlet_Type?<br>\n* How is sales related to all the features that we have? <br>\n* Are the independent variables correlated? <br>","a81bbf20":"# Item MRP\nNo clear relationship between MRP and sales","4f976b3a":"# Exploration of Continuous Variables\n* Item Weight\n* Item Visibility\n* Item_MRP\n* Item_Outlet_Sales","02df39bb":"### Utility Functions","378ad2a7":"Item_Weight, Outlet_Size have NULL values","8e70c154":"# Linear Models for regression","90163430":"### *How does the distribution of Item weight look like?*\nMultimodal distribution with no evident pattern","d1ed8a76":"### Distribution of Item_Fat_Content\nDistribution of low fat and regular item categories is similar","fa965171":"### Ridge Regression","3b88658f":"### At this stage I guess we are done with the data exploration, now lets try some ML algorithms on this data to predict the sales. Did not find a lot of patterns in this data apart from a few for categorical features","d858160f":"## Item Fat Content\n* Two types of items: Low Fat and regular\n* There are variations of low fat - Low Fat, LF, low fat\n* Low Fat items tend to have higher sales","9e2afdcb":"### Decision Tree Regression","a195e21a":"# Outlet Type","a088aabf":"### Lasso Regression","d9a848f6":"## Support Vector Regression","247cabb9":"### *How does the Sales of a product vary by establishment year of a store for different store tier, size, type?*","aacdcfe6":"### *What is the avg. sales by store tier?*\nMean sales shows Tier3 > Tier2 > Tier1 ","42773d67":"### *What is the avg. sales per item type?*","f778d442":"# Item Type"}}