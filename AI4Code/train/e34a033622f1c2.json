{"cell_type":{"1ebc1f3c":"code","67c39d6a":"code","4d140813":"code","4239e760":"code","273ae030":"code","4de6d673":"code","71047f5c":"code","2dbceed2":"code","84c135aa":"code","888e5658":"code","b4c76f78":"code","a4c94502":"markdown"},"source":{"1ebc1f3c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model","67c39d6a":"df = pd.read_csv('\/kaggle\/input\/eval-lab-1-f464-v2\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/eval-lab-1-f464-v2\/test.csv')\ndf.head()\n","4d140813":"df.isnull().sum()","4239e760":"df.fillna(value=df.mean(),inplace=True)\ndf.isnull().sum()","273ae030":"# df['type']\ntemp_code = {'old':0,'new':1}\ndf['type']=df['type'].map(temp_code)\ntest['type']=test['type'].map(temp_code)","4de6d673":"test.fillna(value=test.mean(),inplace=True)","71047f5c":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\n\ny = df['rating']\nX = df.drop(['rating'],axis=1)\nids = test['id']\n\n\n \nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nclf = GradientBoostingClassifier(n_estimators=571,max_depth=9)\ndlf = RandomForestRegressor(n_estimators=283,max_depth=6,random_state=0)\n\nmm_scaler = preprocessing.StandardScaler()\nX = mm_scaler.fit_transform(X)\ntest=mm_scaler.transform(test)\n\ndlf.fit(X,y)\nu1 = dlf.predict(test)\nclf.fit(X,y)\nu=clf.predict(test)","2dbceed2":"vv = ids.values\nans = []\nans2= []\nfor i in range(len(vv)):\n  ans.append([vv[i],round(u[i])])\n  ans2.append([vv[i],round(u1[i])])            \nfinans = pd.DataFrame(data=ans,columns=['id','rating'])\nfinans2 = pd.DataFrame(data=ans2,columns=['id','rating'])\n# print(finans)\nfinans2.to_csv('submission2.csv',index=False)\nfinans.to_csv('submission.csv',index=False)","84c135aa":"\ndef rms(a,b):\n  sum=0;\n  for i in range(len(a)):\n    sum+=(ff(a[i])-b[i])**2\n  return sum\/len(a)","888e5658":"import math\ndef ff(a):\n  t = math.floor(a)\n  b=a-t\n  if(b>0.6):\n    return t+1\n  else:\n    return t\n  ","b4c76f78":"# from sklearn import preprocessing\n \n# X['feature7']=(X['feature7']-X['feature7'].min())\/(X['feature7'].max()-X['feature7'].min())\n \n# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42) \n\n# mm_scaler = preprocessing.RobustScaler()\n# X_train_minmax = mm_scaler.fit_transform(X_train.values)\n# X_test=mm_scaler.transform(X_test)\n# # # print(X_train)\n# # X_train=mm_scaler.transform(X_train)\n# # # # print(X_train)\n# # import sklearn\n# # from sklearn.ensemble import GradientBoostingClassifier\n# # # from sklearn.model_selection import RandomizedSearchCV\n# # from sklearn.model_selection import cross_val_score\n# # from sklearn import metrics\n# # # clf = clf.fit(X_train,y_train.values)\n# # # u=clf.predict(X_test)\n\n# # from sklearn.ensemble import GradientBoostingRegressor\n# # # clf= RandomForestRegressor(max_depth=100, random_state=0,n_estimators=1000)\n\n# param_grid = {'n_estimators':np.arange(1,1000),'max_depth':np.arange(1,10)}\n\n# clf = RandomForestClassifier()\n\n# clf_cv = GridSearchCV(clf,param_grid,cv=5)\n\n# clf_cv.fit(X,y)\n\n# # scores = cross_val_score(clf,X,y,cv=5,scoring='mean_squared_error')\n# # sorted(sklearn.metrics.SCORERS.keys())\n# # print(scores.sum())\n\n# print(clf_cv.best_params_)\n\n\n# clf = clf.fit(X_train,y_train.values)\n# u=clf.predict(X_test)\n\n# # reg2 = linear_model.LinearRegression()\n# # reg.fit(X_train,y_train.values)\n\n# # u = reg.predict(X_test)\n# # print(type(u))\n# # print(type\/(X_test))\n# # print(u)\n# # print(y_test.v\/alues)\n# print(rms(u,y_test.values))\n# X.describe()","a4c94502":"test.head()"}}