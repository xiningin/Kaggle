{"cell_type":{"b2746ec7":"code","e81da83f":"code","c0115d66":"code","0e57d083":"code","7e425319":"code","36ae354d":"code","af87e83a":"code","cd1f954b":"code","a7c02a62":"code","5cc005ae":"code","2c491ffd":"code","b9be3dd3":"code","fb0f9b59":"code","eb3e09a2":"code","62ec60a2":"code","48e7b01e":"code","90e880da":"markdown","c32ac2d0":"markdown","6ffa9959":"markdown","ed05cbf1":"markdown","3158b955":"markdown","32d5c65c":"markdown","9a731984":"markdown","2d1b592d":"markdown","7b75ff99":"markdown","f9ac2d7f":"markdown","5a4c5748":"markdown","8baeebf5":"markdown","c81f935f":"markdown","b617f0cf":"markdown","88a4f58a":"markdown","ad1346d4":"markdown"},"source":{"b2746ec7":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms","e81da83f":"batch_size = 64\n\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\n\n# You need to have internet access on to get this data\ntrain_data = datasets.MNIST('~\/pytorch\/MNIST_data', download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=True,\n                                           batch_size=batch_size)","c0115d66":"imgs, label = next(iter(train_loader))\nimgs = imgs.numpy().transpose(0, 2, 3, 1)\nplt.imshow(imgs[1].squeeze())","0e57d083":"class Generator(nn.Module):\n    def __init__(self, nz, nhidden):\n        super(Generator, self).__init__()\n\n        # input is Z, noise vector, going into a dense layer\n        self.fc1 = nn.Linear(nz, nhidden)\n        self.fc2 = nn.Linear(nhidden, nhidden)\n        # Output layer. I'll reshape in forward()\n        self.fc3 = nn.Linear(nhidden, 784)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = torch.tanh(self.fc3(x))\n        \n        # Reshape to (batch-size, color channels, width, height)\n        return x.view(-1, 1, 28, 28)","7e425319":"class Discriminator(nn.Module):\n    def __init__(self, nhidden):\n        super(Discriminator, self).__init__()\n\n        self.fc1 = nn.Linear(784, nhidden)\n        self.fc2 = nn.Linear(nhidden, nhidden)\n        self.fc3 = nn.Linear(nhidden, 1)\n        \n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = F.leaky_relu(self.fc1(x))\n        x = F.leaky_relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        \n        return x.view(-1, 1)","36ae354d":"def imshow(tensor):\n    \"\"\" Display a tensor as an image. \"\"\"\n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().transpose(1, 2, 0)\n    plt.imshow(image.squeeze()*2 + 0.5)\n    plt.tight_layout()\n    plt.axis('off')\n    \ndef image_grid(tensor, figsize = (6,6)):\n    \"\"\" Display batched images as a grid. Batch size must be a perfect square. \"\"\"\n    bs, c, w, h = tensor.shape\n    image_grid = torch.zeros(c, int(w*np.sqrt(bs)), int(h*np.sqrt(bs)))\n    for ii, img in enumerate(tensor):\n        x = (ii % int(np.sqrt(bs))) * w\n        y = (ii \/\/ int(np.sqrt(bs))) * h\n        image_grid[:, x: x + w, y: y + h] = img\n    plt.figure(figsize=figsize)\n    plt.tight_layout()\n    imshow(image_grid)","af87e83a":"# Use a GPU if one is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlr = 0.0003\nbatch_size = train_loader.batch_size\nbeta1 = 0.5   # parameter for Adam optimizer\n\n# Fixed latent noise vectors for observing training progress\nnz = 100\nfixed_noise = torch.randn(25, nz, device=device)\n\nnetG = Generator(nz, 300).to(device)\nnetD = Discriminator(50).to(device)\n\ncriterion = nn.BCELoss()\n\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n# Labels we'll use for training, smoothing for the real labels\nreal_label = 0.9\nfake_label = 0","cd1f954b":"# You can get pretty good results with 80 epochs. More is better.\nepochs = 80\n\nstep = 0\nfor epoch in range(epochs):\n    for ii, (real_images, _) in enumerate(train_loader):\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        # Sometimes you'll get not full batches, so need to be flexible\n        batch_size = real_images.size(0)\n        \n        # Train discriminator on real images\n        real_images = real_images.to(device)\n        labels = torch.full((batch_size, 1), real_label, device=device)\n        netD.zero_grad()\n        output = netD(real_images)\n        errD_real = criterion(output, labels)\n        errD_real.backward()\n        \n        # For monitoring training progress\n        D_x = output.mean().item()\n\n        # Train with fake\n        noise = torch.randn(batch_size, nz, device=device)\n        fake = netG(noise)\n        # Changing labels in-place because gotta go fast\n        labels.fill_(fake_label)\n        # Detach here so we don't backprop through to the generator\n        output = netD(fake.detach())\n        errD_fake = criterion(output, labels)\n        errD_fake.backward()\n        optimizerD.step()\n        \n        # For monitoring training progress\n        D_G_z1 = output.mean().item()\n        errD = errD_real + errD_fake\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        labels.fill_(real_label)  # Real labels for fake images\n        output = netD(fake)\n        errG = criterion(output, labels)\n        errG.backward()\n        optimizerG.step()\n        \n        # For monitoring training progress\n        D_G_z2 = output.mean().item()\n        \n        if step % 200 == 0:\n            print('[%d\/%d][%d\/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f \/ %.4f'\n                  % (epoch + 1, epochs, ii, len(train_loader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n            \n        step += 1\n    else:\n        valid_image = netG(fixed_noise)\n        image_grid(valid_image)\n        plt.show()","a7c02a62":"image_grid(valid_image)","5cc005ae":"real_images, _ = next(iter(train_loader))\nimage_grid(real_images[:25])","2c491ffd":"torch.save(netG.state_dict(), 'generator.pth')\ntorch.save(netD.state_dict(), 'discriminator.pth')","b9be3dd3":"!mkdir images","fb0f9b59":"from torchvision.utils import save_image","eb3e09a2":"noise = torch.randn(10000, nz, device=device)\nimage_tensors = netG(noise)\n# Move values back to be between 0 and 1\nimage_tensors = (image_tensors * 0.5 + 0.5)\nfor ix, image in enumerate(image_tensors):\n    save_image(image, f'images\/image_{ix:05d}.png')","62ec60a2":"import shutil\nshutil.make_archive('images', 'zip', 'images')","48e7b01e":"!rm -r images","90e880da":"## Networks in competition\n\nThe main idea behind GANs is there are two networks competing with each other, a generator and a discriminator. The generator creates fake data while the discriminator does its best to tell if the data is fake or real. Theoretically, the generator could be used to create any type of data, but so far it's worked best with images. The networks are trained by showing the discriminator real images from some dataset and fake images from the generator. As the discriminator gets better at detecting fake images, the generator gets better at creating realistic images.\n\n<center><img src=\"https:\/\/i.imgur.com\/oXfBdjf.png\"><\/center>","c32ac2d0":"# Training the networks\n\nWriting the code for training the networks is surprisingly straightforward. We'll update the networks in three stages:\n\n1. Pass a batch of real images to the discriminator, set the labels to \"REAL\", calculate the loss, and get the gradients. This improves the discriminator on real images.\n2. Pass a batch of fake images to the discriminator, set the labels to \"FAKE\", calculate the loss, get the gradients, and update the discriminator. This improves the discriminator on fake images.\n3. Pass a batch of fake images to the discriminator, set the labels to \"REAL\", calculate the loss, and update the generator. We want the discriminator to think the fake images are real, so using \"REAL\" labels will train the generator to make images that the discriminator observes as real.\n\nHowever, finding the correct hyperparameters can be difficult. Since the generator and discriminator are competing, they need to be balanced so one doesn't dominate the other. The most effective way I've found to balance the models is adjusting the size of the generator and discriminator networks. You can add more hidden units to make wider networks, or add more layers.\n\n### Label smoothing\nStability is improved if the labels for real and fake images are slightly different than 1 and 0, respectively. From what I've seen, most implementations have the label for real images as 0.9 and fake images as 0.\n\n### Setting up things for training\n\nHere I'm going to define some parameters for training, create the models, define the loss, and create the optimizers.","6ffa9959":"In the next tutorial, I'll show you how build a more powerful GAN with convolutional networks, called DCGANs.","ed05cbf1":"After training, I'm displaying generate images from the final model and real images, for comparison.","3158b955":"# Save generated images\n\nFinally we want to generate a bunch of images and save them. For the kernel to work properly, we'll need to make sure the images are archived in a zip file and the individual images are deleted. Also, torchvision provides a utility function that speeds up saving the images.","32d5c65c":"Here we can see one of the images.","9a731984":"### Discriminator\n\nThe discriminator will be a common binary classification network. Again, using leaky ReLUs, but otherwise a normal classifier. The output will be the probability that the input is real.","2d1b592d":"# Models\n\nNext I'll define the generator and discriminator models for the GAN architecture. These will be simple dense networks with one hidden layer.\n\n<center><img src=\"https:\/\/imgur.com\/M8Ev03g.png\" width=550px><\/center>","7b75ff99":"# Save networks\n\nHere we can save the networks if we want to load them again.","f9ac2d7f":"First I'll define the generator then the discriminator.\n\n### The Generator\n\nThe goal of the generator is to take a noise vector and convert it into a 28x28 image. Important things here:\n\n- Leaky ReLU activations on the dense hidden layers. The output of normal ReLUs have a lot of zeros which leads to a lot of zeros in the gradients as well. This tends to make training GANs difficult, so we'll use leaky ReLUs to avoid these sparse gradients\n- Tanh on the output layer. The images we generate should have values between -1 and 1.\n- Reshape the final output to be the same shape as the real images\n","5a4c5748":"The generator takes a latent noise vector that it uses to generate an image. Conceptually, the generator maps the distribution of the latent vectors to the distribution of the real data. This means as you adjust the latent vector you are producing different images from the distribution of real images.\n\nFor this GAN, I'll be using the MNIST dataset of hand-written digits. The goal then is to train the generator to create images of hand-written digits. I'll be implementing the GAN in PyTorch, my preferred deep learning framework. First up, importing the modules we'll be using. Then I'll define the generator and discriminator networks, and finally show you how the networks are trained.","8baeebf5":"Now I'll zip up the image directory into an archive and delete the image folder itself.","c81f935f":"# Creating functions for displaying images\n\nIt's useful to watch the generated images as the networks train so I'm creating a couple functions here to  generate images from the tensors.","b617f0cf":"# Generative Adversarial Networks\n\nWelcome to this tutorial on generative adversarial networks, typically called GANs. The GAN architecture has proven to produce state-of-the-art performance on image generation tasks. For example, the GAN in this tutorial produced these images of hand-written digits.\n\n<center><img src=\"https:\/\/i.imgur.com\/q9z0Vcc.png\"><\/center>\n<br>\n\nIn this kernel, I'll show you the basic concepts behind GANs and how to implement a simple GAN with PyTorch.","88a4f58a":"Here I'm using `torchvision` to load the MNIST images. I'm also normalizing the images so they have values between -1 and 1. This is important as we'll make the generator output tensors with values between -1 and 1. I'm setting the batch size here too, this is how many images we'll pass to the networks in each update step.","ad1346d4":"# Training loop\n\nFinally we're at the training loop. You'll want to monitor the training progress to see if the networks are stable and learning together. To do this, I have it printing out a few metrics.\n\n- The discriminator loss, `Loss_D`. If this drops to 0, something is wrong.\n- The average prediction probability on real images `D(x)`. Around 0.5 - 0.7 is good.\n- The average prediction probability on fake images `D(G(z))`, before and after training the discriminator on fake images. Around 0.3 - 0.5 is good.\n\nI've found it most informative to watch `D(x)` and `D(G(z))`. If the discriminator is really good at detecting real images, `D(x)` will be close to 1 since it's predicting nearly all the real images as real. This means though that it's also really good at detecting fakes and `D(G(z))` will be near 0. In this case the discriminator is too strong. Try making the generator stronger or the discriminator weaker.\n\nIf the generator is successfully fooling the discriminator, `D(x)` and `D(G(z))` should be around 0.5 since the discriminator can't tell between the real and fake images. You'll usually find that `D(x)` is a bit larger that 0.5, but as long as the two networks are roughly balanced, the generator will continue to improve.\n\nI've also set it up to generate images after each epoch. This way you can watch the generator get better over time. If the images continue to look like noise, something is wrong. However, sometimes it can take a while before the images look like anything real, so I prefer to watch `D(x)` and `D(G(z))`."}}