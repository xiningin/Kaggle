{"cell_type":{"4c8baa0d":"code","3f9dd2a2":"code","77b7036a":"code","59eadd3d":"code","5370592a":"code","557a7185":"code","ccea06c4":"code","c56a0c9f":"code","591acd24":"code","1b6ab9d5":"code","eb534425":"code","cd7d5dc5":"code","1c58778a":"code","678e1974":"code","c9f5065e":"code","f9e71359":"code","938afdbd":"code","bf861a7a":"code","7b405cf2":"code","d2550a95":"code","7ff45107":"code","bc3ace8b":"code","f29ac10c":"code","d752a866":"code","644d6cbd":"code","d79a24a9":"code","5b2a4a51":"code","5e185923":"code","2fa2d227":"code","636fa6ed":"code","5f434ee5":"code","60523fd1":"code","f96bd8b2":"code","35354f64":"code","222092c9":"code","3114723d":"code","1f8ddcd5":"code","6b783cfc":"code","8097d05f":"code","421d06c6":"code","dd425418":"code","ba05a9b8":"code","a9364159":"code","b51b6ac9":"markdown","82249baf":"markdown","26b9671b":"markdown","2e2ec62a":"markdown","c598a5aa":"markdown","860d8459":"markdown","e3eb664a":"markdown","5434f597":"markdown","18233ac6":"markdown","79b9d667":"markdown","55cd245a":"markdown","cf6656d7":"markdown","5653dddf":"markdown","95e6f8a5":"markdown","f8191750":"markdown","c520170e":"markdown","f01f1c04":"markdown","6379b975":"markdown","f3a20feb":"markdown","9b926cdd":"markdown","854659c1":"markdown","13af03fe":"markdown","3484458f":"markdown","85a96724":"markdown","90ee4322":"markdown","a2ab6d6b":"markdown","33fba237":"markdown","c5d1873e":"markdown","cf7f1d5b":"markdown","a2e01577":"markdown","0e72004f":"markdown","6ec9bec3":"markdown","fb1faf7f":"markdown","b1332204":"markdown","781d8527":"markdown","41816026":"markdown","45d9332b":"markdown"},"source":{"4c8baa0d":"import pandas as pd\nimport numpy as np\nimport os\nfrom pandas_profiling import ProfileReport\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport folium\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.preprocessing import TransactionEncoder","3f9dd2a2":"for i in os.listdir('..\/input\/sales-product-data'):\n    print(i)","77b7036a":"# This part of the could is retrieved from KNIGHTBEARR's notebook (https:\/\/www.kaggle.com\/knightbearr\/sales-data-deep-analysis-knightbearr)\n\n# let's make a list compreension for all the data in the folder\nfiles = [file for file in os.listdir('..\/input\/sales-product-data')]\n\n# let's make a pandas DataFrame\nall_months_data = pd.DataFrame()\n\n# makes a loop for concat the data\nfor file in files:\n    data = pd.read_csv(\"..\/input\/sales-product-data\/\" + file)\n    all_months_data = pd.concat([all_months_data, data])","59eadd3d":"all_months_data.shape","5370592a":"all_months_data.tail()","557a7185":"all_months_data.head()","ccea06c4":"all_months_data.info()","c56a0c9f":"all_months_data.columns","591acd24":"all_months_data[['Order ID', 'Product', 'Order Date', 'Purchase Address']].describe()","1b6ab9d5":"all_months_data['Quantity Ordered'] = pd.to_numeric(all_months_data['Quantity Ordered'], errors = 'coerce')\nall_months_data['Price Each'] = pd.to_numeric(all_months_data['Price Each'], errors = 'coerce')","eb534425":"all_months_data.describe()","cd7d5dc5":"profile = ProfileReport(all_months_data)\nprofile","1c58778a":"all_months_data.isna().sum()","678e1974":"all_months_data[all_months_data[\"Order ID\"].isna()]","c9f5065e":"all_months_data = all_months_data.dropna()\nall_months_data.shape","f9e71359":"all_months_data.isna().sum()","938afdbd":"# Sale Related\nall_months_data['Sale'] = all_months_data['Quantity Ordered']*all_months_data['Price Each']","bf861a7a":"# Address Related\nall_months_data['City'] = all_months_data['Purchase Address'].apply(lambda x: x.split(',')[1])\nall_months_data['State'] = all_months_data['Purchase Address'].apply(lambda x: x.split()[-2])\nall_months_data['Postal Code'] = all_months_data['Purchase Address'].apply(lambda x: x.split()[-1])","7b405cf2":"# Time Related\nall_months_data['Order Date'] = pd.to_datetime(all_months_data['Order Date'])\n\nall_months_data['Year'] = all_months_data['Order Date'].dt.year\nall_months_data['Month'] = all_months_data['Order Date'].dt.month\nall_months_data['Hour'] = all_months_data['Order Date'].dt.hour","d2550a95":"all_months_data.head(10)","7ff45107":"print(all_months_data['Year'].value_counts())","bc3ace8b":"all_months_data = all_months_data.drop(all_months_data[all_months_data['Year']==2020].index)","f29ac10c":"all_months_data.shape","d752a866":"# data used in this section\ntemp_data = all_months_data.groupby(['Month']).sum().reset_index()\n\n#\nfig, axes = plt.subplots(2, 1, figsize = (40,25))\nfig.subplots_adjust(hspace=.3)\n\nsns.barplot(x='Month', y='Sale', data=temp_data, ax=axes[0])\naxes[0].set_xlabel(axes[0].get_xlabel(), size=30)\naxes[0].set_ylabel(axes[0].get_ylabel(), size=30)\naxes[0].set_xticklabels(axes[0].get_xticklabels(), size=30)\naxes[0].bar_label(axes[0].containers[0], fmt='%.2f', size=25)\naxes[0].set_title('Total Sales per Month', size= 40)\n\n# -------\n\n# \nsns.barplot(x='Month', y='Quantity Ordered', data=temp_data, ax=axes[1])\naxes[1].set_xlabel(axes[1].get_xlabel(), size=30)\naxes[1].set_ylabel(axes[1].get_ylabel(), size=30)\naxes[1].set_xticklabels(axes[1].get_xticklabels(), size=30)\naxes[1].bar_label(axes[1].containers[0], fmt='%.2f', size=25)\naxes[1].set_title('Total Quantity Ordered per Month', size= 40)","644d6cbd":"'''''\n# data used in this section\ntemp_data = all_months_data.groupby(['Month', 'Product']).sum().reset_index()\n\n#\nfig, axes = plt.subplots(2, 1, figsize = (40,25))\nsns.barplot(x='Month', y='Sale', hue='Product', data=temp_data, ax=axes[0])\n\n\n#\naxes[0].set_xlabel(axes[0].get_xlabel(), size=30)\naxes[0].set_ylabel(axes[0].get_ylabel(), size=30)\n\n#\naxes[0].set_xticklabels(axes[0].get_xticklabels(), size=30)\n\n#\n# -------\n#\nsns.barplot(x='Month', y='Quantity Ordered', hue='Product', data=temp_data, ax=axes[1])\n\n#\naxes[1].set_xlabel(axes[1].get_xlabel(), size=30)\naxes[1].set_ylabel(axes[1].get_ylabel(), size=30)\n\n#\naxes[1].set_xticklabels(axes[1].get_xticklabels(), size=30)\n'''''","d79a24a9":"# data used in this section\ntemp_data = all_months_data.groupby(['Month', 'Product']).sum().reset_index()\n\n#\ng = sns.FacetGrid(temp_data, col=\"Month\", hue='Month', col_wrap=4, size=8)\ng.map(sns.barplot, \"Quantity Ordered\", \"Product\")\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle('Title', fontsize=36)","5b2a4a51":"# data used in this section\ntemp_data = all_months_data.groupby(['Month', 'Product']).sum().reset_index()\n\n#\nax = sns.FacetGrid(temp_data, col=\"Month\", hue='Month', col_wrap=4, size=8)\nax.map(sns.barplot, \"Sale\", \"Product\")\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle('Title', fontsize=36)","5e185923":"# data used in this section\ntemp_data = all_months_data.groupby(['City']).sum().reset_index()\n\n#\nfig, axes = plt.subplots(2, 1, figsize = (50, 30))\nsns.barplot(x='City', y='Sale', data=temp_data, ax=axes[0])\naxes[0].set_xlabel(axes[0].get_xlabel(), size=30)\naxes[0].set_ylabel(axes[0].get_ylabel(), size=30)\naxes[0].set_xticklabels(axes[0].get_xticklabels(), size=30)\naxes[0].bar_label(axes[0].containers[0], fmt='%.2f', size=25)\naxes[0].set_title('h', size= 40)\n\n# -------\n\n# \nsns.barplot(x='City', y='Quantity Ordered', data=temp_data, ax=axes[1])\naxes[1].set_xlabel(axes[1].get_xlabel(), size=30)\naxes[1].set_ylabel(axes[1].get_ylabel(), size=30)\naxes[1].set_xticklabels(axes[1].get_xticklabels(), size=30)\naxes[1].bar_label(axes[1].containers[0], fmt='%.2f', size=25)\naxes[1].set_title('h', size= 40)","2fa2d227":"temp_data = all_months_data.groupby(['State']).sum().reset_index()[['State', 'Sale']]\n\nus_map = folium.Map(location=[40, -95], zoom_start=4)\n\nurl = (\n    \"https:\/\/raw.githubusercontent.com\/python-visualization\/folium\/master\/examples\/data\"\n)\nstate_geo = f\"{url}\/us-states.json\"\n\nfolium.Choropleth(\n    geo_data=state_geo,\n    name=\"choropleth\",\n    data=temp_data,\n    columns=[\"State\", \"Sale\"],\n    key_on=\"feature.id\",\n    fill_color=\"YlGn\",\n    fill_opacity=0.7,\n    line_opacity=.1,\n    legend_name=\"Sale ($)\",\n).add_to(us_map)\n\nfolium.LayerControl().add_to(us_map)\n\nus_map","636fa6ed":"# data used in this section\ntemp_data = all_months_data.groupby(['City', 'Product']).sum().reset_index()\n\nax = sns.FacetGrid(temp_data, col=\"City\", hue='City', col_wrap=3, size=8)\nax.map(sns.barplot, \"Sale\", \"Product\")\nax.fig.subplots_adjust(top=0.9)\nax.fig.suptitle('City\/Product\/Total Sale', fontsize=36)","5f434ee5":"# data used in this section\ntemp_data = all_months_data.groupby(['City', 'Product']).sum().reset_index()\n\nax = sns.FacetGrid(temp_data, col=\"City\", hue='City', col_wrap=3, size=8)\nax.map(sns.barplot, \"Quantity Ordered\", \"Product\")\nax.fig.subplots_adjust(top=0.9)\nax.fig.suptitle('City\/Product\/Quantity Ordered', fontsize=36)","60523fd1":"all_months_data[['Postal Code', 'State', 'City']].value_counts()","f96bd8b2":"# data used in this section\ntemp_data = all_months_data.groupby(['Postal Code', 'State', 'City']).sum().reset_index()\ntemp_data.head(11).sort_values(by=['Sale'], ascending=False)[['Postal Code', 'State', 'City', 'Sale', 'Quantity Ordered']]","35354f64":"# data used in this section\ntemp_data = pd.concat([all_months_data.groupby(['Hour']).count()['Product'], \n                       all_months_data.groupby(['Hour']).sum()[['Sale', 'Quantity Ordered']]], axis=1).reset_index()\ntemp_data.columns = ['Hour', 'Number of Orders', 'Total Sale', 'Quantity Ordered']\n\ntemp_data","222092c9":"fig, axes = plt.subplots(2, 1, figsize = (50, 50))\n\naxes[0].plot(temp_data['Hour'], temp_data['Quantity Ordered'], '-p', color='blue', markerfacecolor='blue', \n             markersize=20, linewidth=4, label = \"Quantity Ordered\")\naxes[0].plot(temp_data['Hour'], temp_data['Number of Orders'], '-p', color='red', markerfacecolor='red', \n             markersize=20, linewidth=4, label = \"Number of Orders\")\naxes[0].legend(fontsize=25)\n\naxes[1].plot(temp_data['Hour'], temp_data['Total Sale'], '-p', color='black', markerfacecolor='black', \n             markersize=20, linewidth=4, label = \"Total Sale\")\naxes[1].legend(fontsize=25)","3114723d":"# This line of code is just added here to enable us to import this library into a notebook on Kaggle.  \n# If you want to use this package in your notebook on Kaggle, you have to first download the whole package from the GitHub address that was provided above.\n# Then, you should add it as input data in your notebook and run the below code.\n\n!cp -r ..\/input\/aphorism\/apyori-master\/* .\/ \nfrom apyori import apriori","1f8ddcd5":"grouped_by_address_and_product = all_months_data.groupby(by = 'Purchase Address')['Product'].apply(lambda x:x.tolist())\ngrouped_by_address_and_product = grouped_by_address_and_product.reset_index()\ngrouped_by_address_and_product.columns = ['Addr', 'Products']\n\ngrouped_by_address_and_product['Len'] = grouped_by_address_and_product['Products'].apply(lambda x: len(x))","6b783cfc":"transactions_no_length = grouped_by_address_and_product['Products']\ntransactions_with_length = grouped_by_address_and_product[grouped_by_address_and_product['Len']>1]['Products']\n\nprint(transactions_no_length.head())\nprint('\\n')\nprint(transactions_with_length.head())","8097d05f":"transactions = transactions_no_length.tolist()\nprint(type(transactions))\n\nresults_no_length = list(apriori(transactions=transactions, min_support=0.0001))\nresults_no_length[10:15]","421d06c6":"transactions = transactions_with_length.tolist()\nprint(type(transactions))\n\nresults_with_length = list(apriori(transactions=transactions, min_support=0.0001))\nresults_with_length[10:15]","dd425418":"def inspect(results):\n    lhs = []\n    rhs = []\n    supports = []\n    confidences = []\n    lifts = []\n    \n    for result in results:\n        if len(tuple(result[2][0][1]))>1:\n            lhs.append(tuple(result[2][1][0])[0])\n            rhs.append(tuple(result[2][1][1])[0])\n            supports.append(result[1])\n            confidences.append(result[2][1][2])\n            lifts.append(result[2][1][3])\n            \n    return pd.DataFrame({'lhs': lhs,\n                         'rhs': rhs,\n                         'supports': supports,\n                         'confidences': confidences,\n                         'lifts': lifts})","ba05a9b8":"results_inspected=inspect(results_no_length)\nresults_inspected[results_inspected['supports']>0.008]","a9364159":"results_inspected=inspect(results_with_length)\nresults_inspected[results_inspected['supports']>0.03]","b51b6ac9":"<a id=\"36\"><\/a> <br> \n## How much of each product did we sell in each city?","82249baf":"<a id=\"37\"><\/a> <br> \n## How many of each product did we sell in each city?","26b9671b":"<a id=\"23\"><\/a> <br>\n## Handling Missing Values","2e2ec62a":"<a id=\"38\"><\/a> <br> \n## Do we have different customers in the same state? If yes, how much and how many has each of them ordered?","c598a5aa":"<a id=\"21\"><\/a> <br>\n## Basic Descriptions of the Data and Features","860d8459":"<a id=\"40\"><\/a> <br> \n## Association rules-apriori algorithm","e3eb664a":"<a id=\"25\"><\/a> <br>\n## Data Selection","5434f597":"<a id=\"31\"><\/a> <br> \n## How much and how many products did we sell in each month?","18233ac6":"In each following subsection here, I will propose a question and answer it along with some plots and analysis. So, to make this section clearer, I assumed that \"how much\" product refers to *Total Sale* and \"how many\" refers to *Quantity Ordered*. The idea of using *Total Sale* along with *Quantity Ordered* is that *Total Sale* may not be a suitable and sufficient measure to only take a look at. Maybee price of a product would be so high that it could influence the total sale of that month but, it is ordered rarely. Moreover, taking a look at the *Quantity Ordered* would provide us with more useful information that we can use in inventory management and logistics.","79b9d667":"As it is clear, we have two different customers in California. Let's look at how much and how many they have ordered in 2019.","55cd245a":"<a id=\"3\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">Sales Data Analysis<\/span><\/center>","cf6656d7":"In order to use such an algorithm, we need to have a list of transactions like this:\n> [[USB-C Charging Cable], [iPhone, Apple Airpods Headphones], [Wired Headphones, iPhone]]\n\nTo build such a list, we can use the groupby method in pandas. ","5653dddf":"We can also benefit from using Pandas Profiling as well. It is an amazing tool that can generate thorough profile reports and interesting plots for EDA and descriptive analysis purposes.","95e6f8a5":"An Interactive Geographical Map is an interesting tool to plot geospatial data. So, here we have a US state map, and a statistical variable, in this case, sale, is shown on the map. The intensity of the color,  which is basically the brightness and dullness of the color, shows us how much we have sold in each state. The color intensity scale is shown at the top of the map, and as it is clear, we have sold more in California.","f8191750":"<a id=\"1\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">If you liked my work then please upvote, Thank you.<\/span><\/center>","c520170e":"You can find a thorough rush hour analysis in the below plot based on the Quantity Ordered, Number of Orders, and Total Sales in each hour.","f01f1c04":"By using FacetGrid here, we can have different subplots with the same scale in the axes. Each subplot here refers to a specific month that is listed as a title above the subplot. *Y*-axis shows the products, and *X*-axis displays \"how much\" or \"how many\" of that product we have sold in that specific month. Since all axes have the same scale, now, it is easier to compare the *Total Sale* and *Quantity ordered* of different products over months.","6379b975":"<a id=\"1\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">If you liked my work then please upvote, Thank you.<\/span><\/center>","f3a20feb":"<a id=\"32\"><\/a> <br> \n## How much did each product sell each month?","9b926cdd":"<a id=\"35\"><\/a> <br> \n## How much and how many did we sell in each city? (Interactive Geographical Map )","854659c1":"As I looked through the transactions, I realized that many transactions have only one product in their baskets. This could affect the results of our analysis (take a look at the support formula). Therefore, I considered two types of setups for the apriori algorithm here. \n\n* First, I derived the association rules for all transactions without minding how many items are in a basket. \n* Second, I only chose those transactions that had more than 1 item in their baskets. \n\nBy doing so, we could also take a look at the results of the two setups, and see their differences too.","13af03fe":"The results of the apriori algorithm using the *Apyori* library may be somehow unclear. The following code will make a clear table of the results. Note that I was only interested in the rules with more than one item in them. That's why I added that if condition in the function. If you are interested in all of the rules, you have to change that part of the code.","3484458f":"To implement the apriori algorithm in Python, I've seen people using the *mlxtend* library in Kaggle but, I found the *Apyori* library a better option since it provides the confidence and the lift values as well. **Moreover, in association rules mining, the left-hand side (lhs) and right-hand side (rhs) of the rules are not interchangeable. Meaning that: $lhs -> rhs$ is not equal to $rhs -> lhs$.** Therefore, we have to specify those in the rules too.\n\nYou can download and install this package [here](https:\/\/github.com\/ymoch\/apyori).","85a96724":"For a deeper analysis, we can also take a look at the \"Sale\" and the \"Quantity Ordered\" of each product in each city as well.","90ee4322":"<a id=\"33\"><\/a> <br> \n## How many did each product sell each month?","a2ab6d6b":"<a id=\"39\"><\/a> <br> \n## When are the rush hours?","33fba237":"An interesting point here is that most of the rules that we found in of the setups could be found in the other but, it has different support, lift, and confidence values which we expected since we are using more transactions in one of the setups. Now, consider the first setup and the first found rule. The information that this rule gives us is something like these statements:\n\n> In 0.8% of transactions, AA Batteries (4-pack) and AAA Batteries (4-pack) are bought together.\n\n> If there are AA Batteries (4-pack) in a transaction, there is a 5% probability that this customer will buy AAA Batteries (4-pack) too.\n\n> the lift value of 0.42 will tell us that these two items are not much dependent on each other and are unlikely to be bought together.\n\nWe can have the same interpretation for the second setup as well. However, the assumption in that setup is that **we are certain that there is more than one item in the basket**.","c5d1873e":"<a id=\"34\"><\/a> <br> \n## How much and how many did we sell in each city? (Bar Chart)","cf7f1d5b":"<a id=\"22\"><\/a> <br>\n## Pandas Profiling","a2e01577":"# Contents\n\n* [Import Libraries and the Data](#1)\n* [Descriptive Analysis](#2)\n    - [Basic Descriptions of the Data and Features](#21)\n    - [Pandas Profiling](#22)\n    - [Handling Missing Values](#23)\n    - [Feature Engineering](#24)\n    - [Data Selection](#25)\n* [Sales Data Analysis](#3)\n    - [How much did we sell in each month?](#31)\n    - [How much and how many of each product did we sell each month?](#32)\n    - [How many of each product did we sell each month?](#33)\n    - [How much did we sell in each city? (Bar Chart)](#34)\n    - [How much did we sell in each state? (Interactive Geographical Map)](#35)\n    - [How much of each product did we sell in each city?](#36)\n    - [How many of each product did we sell in each city?](#37)\n    - [Do we have different customers in the same state? If yes, how much and how many has each of them ordered?](#38)\n    - [When are the rush hours?](#39)\n    - [Association rules-apriori algorithm](#40)","0e72004f":"There're lots of great articles and videos about what the apriori algorithm is. So, I won't go through the theory behind it but, in a nutshell, it is an analysis that at last, tells us sentences like these:\n\n- \"If *this* then *that*\" \n- \"People who did *that* also did *that*\" \n- \"People who watched *that* also watched *that*\" \n- \"People who bought *that* also bought *that*\" \n\nOne of the greatest stories in data science is the legend beer-and-diapers story. The legend says that (Full story [here](https:\/\/canworksmart.com\/diapers-beer-retail-predictive-analytics\/#:~:text=The%20legend%20says%20that%20a,have%20beer%20in%20their%20carts).):\n\n> A study was done by a retail grocery store. The findings were that men between 30-40 years in age, shopping between 5 pm and 7 pm on Fridays, who purchased **diapers** were most likely to also have **beer** in their carts.\n\nBesides the fact that why and how buying diapers and beers should be correlated, this sentence is exactly what kind of information we can extract from the apriori algorithm or in a wider and broader terminology, association rules.\n\nAs I mentioned, I won't go through the theory behind this algorithm but, there're three terminologies needed to be discussed:\n\n1. Support \n2. Confidence\n3. Lift\n\nConsider a Market Basket Optimization problem. Support in such a problem gives us an idea of how frequent a product or an item is in all the transactions. It has the following formula for product $1$ ($P1$):\n\n> $$Support(P1) = \\frac{\\text{# of transactions containing P1}}{\\text{Total # of transactions}}$$\n\nIn the same kind of problem, Confidence defines as how likely a product or an item is in the transaction given that the transaction already has a second product (or products). It has the following formula for product $1$ ($P1$) and product $2$ ($P2$):\n\n> $$Confidence(P1->P2) = \\frac{\\text{# of transactions containing P1 and P2}}{\\text{# of transactions containing P1}}$$\n\nSo in a simpler form, it says how often $P2$ appears in transactions that contain $P1$ only.\n\n\nFinally, lift tells us how likely $P2$ is purchased when $P1$ is purchased. In another term, how much our confidence is lifted that $P2$ will be purchased given that $P1$ is already purchased. It has the following formula:\n\n> $$Lift(P1) = \\frac{Confidence(P1->P2)}{Support(P1)}$$\n\nA lift value above $1$ means that $P2$ is likely to be bought if $P1$ is bought. Usually, a good value for lift is above $2$. Yet, I didn't use the same procedure for this problem in this notebook, and I only took a look at the supports.","6ec9bec3":"It might be beneficial to take a look at the different customers in the same state or even in the same city, which is not the case here. It can give us a good picture of our customers in a state and maybe leads us further to personalized marketing.","fb1faf7f":"<a id=\"24\"><\/a> <br>\n## Feature Engineering","b1332204":"<a id=\"1\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">Import Libraries and the Data<\/span><\/center>","781d8527":"Since a limited number of data is recorded in 2020, we only choose 2019 data for our analysis.","41816026":"<a id=\"2\"><\/a> <br>\n# <center><span style=\"font-family:cursive;\">Descriptive Analysis<\/span><\/center>","45d9332b":"As the table represents, the customer in San Fransisco has ordered more than the Los Angeles one. Now, to make an interesting note to keep in mind, we could also take a look at what products they have ordered more for a deeper analysis. In that way, we can see which products are more popular in which states\/cities. For this analysis plz take a look at this section, [How many of each product did we sell in each city?](#37)."}}