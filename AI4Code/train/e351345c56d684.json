{"cell_type":{"d15a8cf8":"code","fe909b95":"code","a0855617":"code","6773230e":"code","50c26c35":"code","5610b062":"code","8d16c915":"code","a3c0c5ba":"code","6b2dc701":"code","741d32aa":"code","49285770":"code","80e4f096":"code","1694262f":"code","0735ddd2":"code","8c9ff9e7":"code","1e119251":"code","077403f3":"code","12c06fff":"code","d63abb2d":"code","027b3d44":"code","053711d2":"code","0c17ebb4":"code","7b81ac7a":"code","2c6d5d55":"code","7f097583":"code","0bea2b8a":"code","19bb00e6":"code","e8e56fa9":"code","f3c02d91":"code","6c85ae8c":"code","2e77348b":"code","730caa50":"code","d39542e5":"code","46d9c810":"code","f5c0083a":"code","734475c3":"code","7efa406d":"code","ae0ae6b5":"code","f4bb8cb7":"code","90ed4232":"code","603b94a0":"code","36cb578c":"code","49cf8aec":"code","eba961ac":"code","fe9e12c1":"code","920b2dbf":"code","624ee84e":"code","3338b44c":"code","ab9e5a04":"code","4b2d22ad":"markdown","770fc505":"markdown","451b6d20":"markdown","3276926e":"markdown","a1c20488":"markdown","a305e7c5":"markdown","7704dabc":"markdown","dfd2efa1":"markdown","1881c42d":"markdown","80a353e7":"markdown","0dd887e0":"markdown","587e7126":"markdown","319a7eda":"markdown","ddc13b35":"markdown","3a0ab74a":"markdown","e7a0b408":"markdown","9f0a4646":"markdown","b7d815c1":"markdown","705c025b":"markdown","cf47e4e8":"markdown","94406814":"markdown","ce4dada8":"markdown","32b02e80":"markdown","3f59dd18":"markdown","84793967":"markdown"},"source":{"d15a8cf8":"!pip install seaborn==0.11.1","fe909b95":"#Basic Libraries\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n#Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Ensemble Model Library\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n\n#Evaluation Library\nfrom sklearn import metrics\n\n#Imbalanced Libraries\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.over_sampling import SMOTE","a0855617":"dataset = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndataset.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1, inplace=True)","6773230e":"dataset.head()","50c26c35":"dataset.describe()","5610b062":"sns.kdeplot(data=dataset, x=\"Customer_Age\", hue=\"Attrition_Flag\")","8d16c915":"sns.countplot(data=dataset, x=\"Gender\", hue=\"Attrition_Flag\")","a3c0c5ba":"sns.countplot(data=dataset, x=\"Dependent_count\", hue=\"Attrition_Flag\")","6b2dc701":"plt.xticks(rotation = 315)\nsns.countplot(data=dataset, x=\"Education_Level\", hue=\"Attrition_Flag\")","741d32aa":"sns.countplot(data=dataset, x=\"Marital_Status\", hue=\"Attrition_Flag\")","49285770":"plt.xticks(rotation=315)\nsns.countplot(data=dataset, x=\"Income_Category\", hue=\"Attrition_Flag\")","80e4f096":"sns.countplot(data=dataset, x=\"Card_Category\", hue=\"Attrition_Flag\")","1694262f":"sns.kdeplot(data=dataset, x=\"Months_on_book\", hue=\"Attrition_Flag\")","0735ddd2":"sns.countplot(data=dataset, x=\"Total_Relationship_Count\", hue=\"Attrition_Flag\")","8c9ff9e7":"sns.countplot(data=dataset, x=\"Months_Inactive_12_mon\", hue=\"Attrition_Flag\")","1e119251":"sns.countplot(data=dataset, x=\"Contacts_Count_12_mon\", hue=\"Attrition_Flag\")","077403f3":"sns.kdeplot(data=dataset, x=\"Credit_Limit\", hue=\"Attrition_Flag\")","12c06fff":"sns.kdeplot(data=dataset, x=\"Total_Revolving_Bal\", hue=\"Attrition_Flag\")","d63abb2d":"sns.kdeplot(data=dataset, x=\"Avg_Open_To_Buy\", hue=\"Attrition_Flag\")","027b3d44":"sns.kdeplot(data=dataset, x=\"Total_Amt_Chng_Q4_Q1\", hue=\"Attrition_Flag\")","053711d2":"sns.kdeplot(data=dataset, x=\"Total_Trans_Amt\", hue=\"Attrition_Flag\")","0c17ebb4":"sns.kdeplot(data=dataset, x=\"Total_Trans_Ct\", hue=\"Attrition_Flag\")","7b81ac7a":"sns.kdeplot(data=dataset, x=\"Total_Ct_Chng_Q4_Q1\", hue=\"Attrition_Flag\")","2c6d5d55":"sns.kdeplot(data=dataset, x=\"Avg_Utilization_Ratio\", hue=\"Attrition_Flag\")","7f097583":"corr = dataset.drop('CLIENTNUM', axis=1).corr()","0bea2b8a":"# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr, mask=mask, cmap='BrBG', vmin=-1, vmax=1, annot=True)","19bb00e6":"dataset = pd.concat([dataset.drop(['Dependent_count', 'Marital_Status', 'Income_Category', 'Card_Category', \n                                   'Education_Level'], axis=1), pd.get_dummies(dataset.Dependent_count), \n                     pd.get_dummies(dataset.Marital_Status), pd.get_dummies(dataset.Income_Category), \n                     pd.get_dummies(dataset.Card_Category), pd.get_dummies(dataset.Education_Level)], axis=1)","e8e56fa9":"dataset['Gender'] = dataset.Gender.map({'M':1, 'F':0})\ndataset['Attrition_Flag'] = dataset.Attrition_Flag.map({'Existing Customer':0, 'Attrited Customer':1})","f3c02d91":"y = dataset.Attrition_Flag\nX = dataset.drop(['Attrition_Flag', 'CLIENTNUM'], axis=1)","6c85ae8c":"X.columns = ['Customer_Age','Gender', 'Months_on_book', 'Total_Relationship_Count','Months_Inactive_12_mon',\n             'Contacts_Count_12_mon','Credit_Limit','Total_Revolving_Bal','Avg_Open_To_Buy','Total_Amt_Chng_Q4_Q1',\n             'Total_Trans_Amt', 'Total_Trans_Ct','Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio', 0, 1, 2, 3, 4, 5,\n             'Divorced','Married', 'Single', 'Marriage_Unknown','$120K +', '$40K - $60K', '$60K - $80K','$80K - $120K',\n             'Less than $40K','Income_Category_Unknown', 'Blue', 'Gold','Platinum', 'Silver', 'College','Doctorate',\n             'Graduate','High School','Post-Graduate', 'Uneducated','Education_Unknown']","2e77348b":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=68, test_size=0.3)","730caa50":"clf = RandomForestClassifier(max_depth=10)\nclf.fit(X_train, y_train)\ny_pred_proba = clf.predict_proba(X_test)[:,1]","d39542e5":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf, X_test, y_test) ","46d9c810":"clf_adaboost = AdaBoostClassifier(base_estimator=clf, n_estimators=100, random_state=0)\nclf_adaboost.fit(X_train, y_train)\ny_pred_proba = clf_adaboost.predict_proba(X_test)","f5c0083a":"print(metrics.roc_auc_score(y_test, y_pred_proba[:,1]))\nmetrics.plot_confusion_matrix(clf_adaboost, X_test, y_test)","734475c3":"y_train.value_counts()\/len(y_train)","7efa406d":"nm = NearMiss()\nX_under, y_under = nm.fit_resample(X_train, y_train)","ae0ae6b5":"y_under.value_counts()\/len(y_under)","f4bb8cb7":"clf = RandomForestClassifier(max_depth=10, random_state=0)\nclf.fit(X_under, y_under)\ny_pred_proba = clf.predict_proba(X_test)[:,1]","90ed4232":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf, X_test, y_test)","603b94a0":"clf_adaboost = AdaBoostClassifier(base_estimator=clf, n_estimators=200, random_state=0)\nclf_adaboost.fit(X_under, y_under)\ny_pred_proba = clf_adaboost.predict_proba(X_test)[:,1]","36cb578c":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf_adaboost, X_test, y_test)","49cf8aec":"y_train.value_counts()\/len(y_train)","eba961ac":"sm = SMOTE(sampling_strategy='auto', random_state=1234)\nX_over, y_over = sm.fit_resample(X_train, y_train)","fe9e12c1":"y_over.value_counts()\/len(y_over)","920b2dbf":"clf = RandomForestClassifier(max_depth=10, random_state=0)\nclf.fit(X_over, y_over)\ny_pred_proba = clf.predict_proba(X_test)[:,1]","624ee84e":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf, X_test, y_test)\n","3338b44c":"clf_adaboost = AdaBoostClassifier(base_estimator=clf, n_estimators=200, random_state=0)\nclf_adaboost.fit(X_over, y_over)\ny_pred_proba = clf_adaboost.predict_proba(X_test)[:,1]","ab9e5a04":"print(metrics.roc_auc_score(y_test, y_pred_proba))\nmetrics.plot_confusion_matrix(clf_adaboost, X_test, y_test)","4b2d22ad":"Compared to the undersampling dataset, we achieved a better ROC_AUC_score but worse TPR value which is 0.8307.\n\nNow lets train the AdaBoost Model.","770fc505":"As we can see, some features have (almost) normal distribution like *Total_Ct_Chng_Q4_Q1* and *Total_Trans_Amt*, some have Poisson distribution, some have other type of distribution.\n\nNow lets plot the correlation matrix to get another insight.","451b6d20":"We got a worse ROC_AUC_score, but on the other hand, we got a better TPR score as what have been asked in the task. We achieve 0.87992 TPR Score. \n\nNow, lets try Adaboost Method","3276926e":"Because we are working with imbalanced data I prefer to use ROC_AUC_score instead of accuracy. Then we also interested to see True Positive Rate (TPR) and we also want to avoid False Negative. From the matrix confusion, we got:\nTPR = TP\/(TP + FN)\n    = 357\/(357+151)\n    = 0.716867\n\nThis result isn't good enough. Now lets try to use Adaboost. Adaboost works as an ensemble learning. Adaboost combines plenty of weak classifier to produce a strong classifier. In this sample, we will use our random forest model as our weak classifier, then adaboost will try to combine this model to give a better result. ","a1c20488":"This time I will try to use NearMiss algorithm to undersampling the *Existing_Customer*. First let take a look at the distribution of our target values. It is shown that Attrited Customer is only 15.7% of our train set.","a305e7c5":"Seperate our features from our target, and drop unuse feature like *CLIENTNUM* which is just an id in database.","7704dabc":"We achieved TPR Score of 0.84843. Overall, if we compared Upsampling SMOTE and undersampling Near Miss dataset, we achieved a better result of AUC score. But if our target is to get higher TPR then, I will choose Near Miss dataset and use AdaBoost classifier which give me the best TPR score.","dfd2efa1":"SMOTE gives us the balanced train distribution, just like what Near Miss did. Now train the Random Forest Classifier using SMOTE dataset.","1881c42d":"This time, I will try to use SMOTE algorithm to oversample the minority class and train the Random Forest and Adaboost classifier again and see whether it will give a better result than the Near Miss undersampling.","80a353e7":"After appling Near Miss algorithm, I have a balanced data between Attrited Customer and Existing Customer. Lets try to train Random Forest and Adaboost again on this undersample dataset.","0dd887e0":"# Load and Find Insight From Dataset","587e7126":"From the correlation matrix we could see some features are correlated between each other, like *Avg_Open_To_Buy* is highly dependent\/correlated to *Credit Limit*, well if we take a look at the definition it is not a shocking fact. It is surprise me that *Months_on_book* is quite correlate with the *Customer_Age*. Well, for instance we could drop one of those correlated feature and just pick one of them because correlated features are just redundant information. But for this notebook, I will keep all those features. I will deal with correlated features for further investigation.","319a7eda":"We achieved a better ROC_AUC_score and also the highest TPR value which is 0.9035. So far, undersampling the dataset works well for this dataset. Next section I will try to overpsampling the minority class.","ddc13b35":"# Oversampling the Minority Class","3a0ab74a":"Adaboost Model has been successful to produce a greater learner. ROC_AUC_score has shown a better result. And also our TPR has been improved compared to the random forest classifier. We achieve TPR score of 0.80315. The next session, I will try to do something with the imbalanced data.","e7a0b408":"# Import Libraries needed","9f0a4646":"Now train a simple Random Forest Classifier and evaluate its performace, I leave all parameters to default except for the max_depth.","b7d815c1":"Now, lets one-hot encode the categorical features using *get_dummies* by *pandas*  ","705c025b":"# # Undersampling the Majority Class","cf47e4e8":"Split our dataset which will result in 70% of training set and 30% of test set.","94406814":"Lets rename our feature's columns. This is neccesary because we got similar name for several features caused by one-hot encoded process","ce4dada8":"# Preprocess Stage","32b02e80":"Lets see the descriptive statistics of our dataset","3f59dd18":"Lets load the dataset, drop all Naive_Bayes Column because those aren't used, and see the small sample of our dataset.","84793967":"From our descriptive statistics, we can see that the scale of our data is not the same for all features. But we won't be bothered, because the model that we are trying to trained is logical based which is not sensitive to input's scale. Another thing that could be noticed is if we are looking at features where currency is involved, the std deviation value tends to be bigger (sometimes even surpassed the mean value).\n\nNow lets visualize each feature distribution in respect to our target which is Attrited Customer or Existing Customer. For continuous feature I prefer to plot the kde (kernel density estimation) while for discrete feature I use histogram plot."}}