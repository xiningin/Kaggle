{"cell_type":{"9c54c662":"code","ee8931b3":"code","a922913f":"code","d6541ddb":"code","b055378f":"code","235027dd":"code","faf230fb":"code","7a84f2d6":"code","02161c75":"code","91ff0936":"code","99d69f55":"code","60c9a847":"code","93970644":"code","ff6058d1":"code","e6ada48e":"code","cd004af1":"code","a46104f2":"code","d0cc3634":"code","f76c8415":"code","44a641c5":"code","315120f7":"code","e39e0aae":"code","b32115b4":"code","a26e31ec":"markdown","b71fbf2f":"markdown","16a57fda":"markdown","a5e6786f":"markdown","bb13581b":"markdown","5dec81dd":"markdown"},"source":{"9c54c662":"import os\nimport sys \nimport json\nfrom glob import glob\nimport random\nimport collections\nimport time\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom sklearn import model_selection as sk_model_selection\nfrom torch.nn import functional as torch_functional\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","ee8931b3":"DATA_PATH = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\nMRI_TYPES = ['FLAIR','T1w','T1wCE','T2w']\nSIZE = 256\nNUM_IMAGES = 64\nSEED = 42","a922913f":"train_df = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv')\ntrain_df.head()","d6541ddb":"train_patients = glob(os.path.join(DATA_PATH, 'train\/*'))\ntest_patients = glob(os.path.join(DATA_PATH, 'test\/*'))","b055378f":"train_patients[0]","235027dd":"def natural_sort(l): \n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n    return sorted(l, key=alphanum_key)\n\n# key: patient id , values: image paths for 4 types (dictionary, key: MRI type, value: image paths)\ndef get_dicom_data(split='train'):\n    '''\n    dicoms = {\n        '00688' (patient_id) :\n            {\n                'FLAIR' : [image paths ...]\n                'T1w' : [image paths ...]\n                'T1wCE' : [image paths ...]\n                'T2w' : [image paths ...]\n            }\n        ...\n    }\n    '''\n    \n    assert split == 'train' or split == 'test'\n    \n    dicoms = {}\n\n    for patient in glob(os.path.join(DATA_PATH, f'{split}\/*')):\n        patient_id = patient.split('\/')[-1]\n\n        d = {}\n        for t in MRI_TYPES:\n            t_images = glob(os.path.join(patient, f'{t}\/*'))\n            d[f'{t}'] = natural_sort(t_images)\n\n        dicoms[f'{patient_id}'] = d\n    \n    return dicoms\n\ntrain_dicoms = get_dicom_data('train')\ntest_dicoms = get_dicom_data('test')","faf230fb":"sample_patient = list(train_dicoms.keys())[0]\nprint(f'patient id - {sample_patient}')\nfor i, v in train_dicoms[sample_patient].items():\n    print(f'{i} : {len(v)}')","7a84f2d6":"# Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\ndef read_mri(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","02161c75":"def visualize_sample(patient_id, slice_ratio=0.5):\n    \n    dicoms = train_dicoms[patient_id]\n    \n    plt.figure(figsize=(16, 5))\n    \n    for i, t in enumerate(MRI_TYPES, 1):\n        slice_idx = int(len(dicoms[t]) * slice_ratio) - 1\n        sample_dicom = dicoms[t][slice_idx]\n        data = read_mri(sample_dicom)\n        plt.subplot(1, 4, i)\n        plt.imshow(data, cmap=\"gray\")\n        plt.title(f\"{t}\", fontsize=16)\n        plt.axis(\"off\")\n\n    mgmt_value = train_df[train_df.BraTS21ID == int(patient_id)].MGMT_value.item()\n    plt.suptitle(f\"MGMT_value: {mgmt_value}\", fontsize=16)\n    plt.show()\n    \nvisualize_sample(sample_patient)","91ff0936":"from matplotlib import animation, rc\nrc('animation', html='jshtml')\n\n\ndef create_animation(ims):\n    fig = plt.figure(figsize=(6, 6))\n    plt.axis('off')\n    im = plt.imshow(ims[0], cmap=\"gray\")\n\n    def animate_func(i):\n        im.set_array(ims[i])\n        return [im]\n\n    return animation.FuncAnimation(fig, animate_func, frames = len(ims), interval = 1000\/\/24)\n\ndef load_dicom_line(t_paths):\n    images = []\n    for filename in t_paths:\n        data = read_mri(filename)\n        if data.max() == 0:\n            continue\n        images.append(data)\n        \n    return images\n\nimages = load_dicom_line(train_dicoms[sample_patient]['FLAIR'])\ncreate_animation(images)","99d69f55":"def load_dicom_image(path, img_size=SIZE, voi_lut=True, rotate=0):\n    dicom = pydicom.read_file(path)\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom) if voi_lut else dicom.pixel_array\n    else:\n        data = dicom.pixel_array\n        \n    if np.min(data)==np.max(data):\n        data = np.zeros((img_size,img_size))\n        return data\n        \n    if rotate > 0:\n        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        data = cv2.rotate(data, rot_choices[rotate])\n        \n    data = cv2.resize(data, (img_size, img_size))\n    return data\n\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"FLAIR\", split=\"train\"):\n    files = train_dicoms[scan_id][mri_type]\n    \n    images = []\n    for filename in files:\n        data = load_dicom_image(filename)\n        if data.max() == 0:\n            continue\n        images.append(data)\n    \n    if num_imgs > 0:\n        assert len(images) >= num_imgs, f'len(images)({len(images)}) is less than num_imgs({num_imgs})'\n\n        every_nth = len(images) \/ num_imgs\n        indexes = [min(int(round(i*every_nth)), len(images)-1) for i in range(0,num_imgs)]\n        selected_images = [images[i] for i in indexes]\n    else:\n        selected_images = images\n    \n    img3d = np.stack(selected_images).T\n    \n    img3d = img3d - np.min(img3d)\n    if np.max(img3d) != 0:\n        img3d = img3d \/ np.max(img3d)\n    \n    return np.expand_dims(img3d,0)\n\nb = load_dicom_images_3d(sample_patient)\nprint(b.shape)\nprint(np.min(b), np.max(b), np.mean(b), np.median(b))","60c9a847":"mri_anim = [(b[0,:,:,i] * 255).astype(np.uint8) for i in range(NUM_IMAGES)]\ncreate_animation(mri_anim)","93970644":"from IPython.display import clear_output\nimport time\n\nfor mri_image in mri_anim:\n    plt.imshow(mri_image, cmap='gray')\n    plt.show()\n    time.sleep(0.01)\n    clear_output(wait=True)","ff6058d1":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(SEED)","e6ada48e":"def convert_3d_to_list(arr):\n    return [(arr[0,:,:,i] * 255).astype(np.uint8) for i in range(arr.shape[3])]","cd004af1":"import imageio\nimport wandb\nwandb.login()","a46104f2":"CONFIG = {\n    'IMG_SIZE': 224, \n    'NUM_FRAMES': 14,\n    'competition': 'rsna-miccai-brain', \n}","d0cc3634":"run = wandb.init(\n    entity = 'monet-kaggle',\n    project='brain-tumor-viz',\n    config=CONFIG,\n    job_type='vis-dataset-tables')","f76c8415":"patient_ids = []\nfor patient_id in train_dicoms.keys():\n    if patient_id in ['00109', '00123', '00709']:\n        continue\n    patient_ids.append(patient_id)\nlen(patient_ids)","44a641c5":"data_at = wandb.Table(columns=['patent_id', 'target', 'FLAIR', 'T1w', 't1wCE', 'T2w'])\n\nfor i, patient_id in enumerate(patient_ids):\n    os.makedirs('tables-gif\/', exist_ok=True)\n    mgmt_value = train_df[train_df.BraTS21ID == int(patient_id)].MGMT_value.item()\n    \n    for j, mri_type in enumerate(MRI_TYPES):\n        arr_3d = load_dicom_images_3d(patient_id,\n                                      num_imgs=0,\n                                      img_size=CONFIG['IMG_SIZE'],\n                                      mri_type=mri_type,\n                                      split=\"train\")\n        frames = convert_3d_to_list(arr_3d)\n        imageio.mimsave(f'tables-gif\/out-{patient_id}-{j}.gif', frames)\n    \n    data_at.add_data(\n        patient_id,\n        mgmt_value,\n        wandb.Image(f'tables-gif\/out-{patient_id}-0.gif'),\n        wandb.Image(f'tables-gif\/out-{patient_id}-1.gif'),\n        wandb.Image(f'tables-gif\/out-{patient_id}-2.gif'),\n        wandb.Image(f'tables-gif\/out-{patient_id}-3.gif'),\n    )\n\nwandb.log({'MRI Sequencing Dataset' : data_at})\nwandb.finish()","315120f7":"# because of some missing MRIs, 3 samples could be excluded\nsamples_to_exclude = [109, 123, 709]\n\ntrain_df = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv')\nprint(\"original shape\", train_df.shape)\ntrain_df = train_df[~train_df.BraTS21ID.isin(samples_to_exclude)]\nprint(\"new shape\", train_df.shape)\ndisplay(train_df)\n\ndf_train, df_valid = sk_model_selection.train_test_split(\n    train_df, \n    test_size=0.2, \n    random_state=SEED, \n    stratify=train_df[\"MGMT_value\"],\n)","e39e0aae":"df_train.tail()","b32115b4":"class BrainDataset(torch_data.Dataset):\n    def __init__(self, paths, targets=None, mri_type=None, label_smoothing=0.01, split=\"train\", augment=False):\n        self.paths = paths\n        self.targets = targets\n        self.mri_type = mri_type\n        self.label_smoothing = label_smoothing\n        self.split = split\n        self.augment = augment\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        scan_id = self.paths[index]\n        if self.targets is None:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=self.split)\n        else:\n            if self.augment:\n                rotation = np.random.randint(0,4)\n            else:\n                rotation = 0\n\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=\"train\", rotate=rotation)\n\n        if self.targets is None:\n            return {\"X\": torch.tensor(data).float(), \"id\": scan_id}\n        else:\n            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n            return {\"X\": torch.tensor(data).float(), \"y\": y}\n","a26e31ec":"# Load 3D images","b71fbf2f":"## Model and training classes","16a57fda":"## Create GIF using Imageio and log WanDB","a5e6786f":"## train \/ test splits","bb13581b":"## Load 2D images","5dec81dd":"reference\n\n- https:\/\/www.kaggle.com\/ihelon\/brain-tumor-eda-with-animations-and-modeling\n- https:\/\/www.kaggle.com\/ayuraj\/brain-tumor-eda-and-interactive-viz-with-w-b\n- https:\/\/www.kaggle.com\/mikecho\/rsna-miccai-monai-ensemble?scriptVersionId=74508923"}}