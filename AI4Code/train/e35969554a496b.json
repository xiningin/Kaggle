{"cell_type":{"cf3df77b":"code","bc8f064e":"code","b287a2e0":"code","7ab4e8fa":"code","e3de7354":"code","cf533eba":"code","cefca8b4":"code","b0ba6809":"code","36549c34":"code","43552e53":"code","8586d949":"code","1f7b89aa":"code","0f0169f1":"code","97a5bbb2":"code","e9480a72":"code","599e447a":"code","50534dee":"code","3af65b93":"code","39e165c1":"code","5fb87c98":"code","b27a1742":"code","61c560c8":"code","d253a5bb":"code","f6642727":"code","7457955d":"code","3dc6e029":"code","4ff120cc":"code","31a5d3b4":"code","6ff31519":"code","c984124b":"code","ff13bdb9":"code","f84fd953":"code","b5924aff":"code","90bf9f24":"code","bed817ee":"code","7b7bac88":"code","ab328c06":"code","ffaa580d":"code","55af30d8":"code","59835902":"code","159d70d3":"code","7ad7f342":"code","bf991023":"code","92c9afc0":"code","4a5a373a":"code","8f704ca7":"code","b743b8fa":"code","ce218a11":"code","5654aebd":"code","26b7cb6c":"code","38e54f73":"code","fc626400":"code","7d6e2d8c":"code","de7be576":"markdown","eb85003e":"markdown","9eeced6c":"markdown","c8d85f94":"markdown","28c1c518":"markdown","a4bd4688":"markdown","218fd1ca":"markdown","a41ce92e":"markdown","b087b553":"markdown","fe775934":"markdown","fb94dd23":"markdown","41b71415":"markdown","97dc1828":"markdown","e5749a0a":"markdown","1566ff8c":"markdown","476e5fe5":"markdown","3ab1fd99":"markdown","187d8c5a":"markdown","e20baf16":"markdown","5d574981":"markdown","8ac2212e":"markdown","2cfb11a7":"markdown","a7e01015":"markdown","aaf30334":"markdown","21a7b10c":"markdown","6e28c061":"markdown","19d8a661":"markdown","86f6a05f":"markdown","5807e3a8":"markdown","96a3a3a5":"markdown","cf4d3728":"markdown","c9fb882c":"markdown","ae63b5b9":"markdown","ae7ff9eb":"markdown","cfd30566":"markdown","fdc43ac4":"markdown","d5514ec8":"markdown","f0b0fb4c":"markdown","39be227f":"markdown","bd360cbf":"markdown","497f7e61":"markdown","52b4d99d":"markdown","d9211907":"markdown","f5a313dc":"markdown","51ed6207":"markdown","7807b3ac":"markdown","2cac02e5":"markdown","25a2dbf9":"markdown","3ad33afb":"markdown","238b6196":"markdown","401f099e":"markdown","21ad0d4c":"markdown","4c52a2ac":"markdown","c650e5c3":"markdown","136a9c4d":"markdown","6070ac2b":"markdown","91ac2f35":"markdown","98e827c8":"markdown","72d8f595":"markdown","faba9626":"markdown","a07c9787":"markdown","67174870":"markdown","85d4da61":"markdown","e8ab3bda":"markdown","2279fb43":"markdown","09e14f9d":"markdown","cb861ef7":"markdown","ca1dca76":"markdown","b3177cee":"markdown","efd43f19":"markdown","9550875a":"markdown","84043370":"markdown","2279668a":"markdown","7da8440b":"markdown","0974b591":"markdown","8945ca71":"markdown","e3d4aee4":"markdown","a321ce3b":"markdown","36119b5c":"markdown","584456d0":"markdown","c1a4d90a":"markdown","db19edb3":"markdown","49e5625c":"markdown"},"source":{"cf3df77b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.api.types import CategoricalDtype\nimport optuna\nimport shap\nimport pickle\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\nfrom pathlib import Path","bc8f064e":"def load_data():\n    # Read data\n    data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing steps\n    df = clean(df)\n    df = encode(df)\n    df = impute_plus(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","b287a2e0":"data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\ndf = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n\ndf.Exterior2nd.unique()","7ab4e8fa":"def clean(df):\n    # Correct typo on Exterior2nd\n    df['Exterior2nd'] = df['Exterior2nd'].replace({'Brk Cmn': 'BrkComm'})\n    # Some values of GarageYrBlt are corrupt, so we'll replace them with the year house was built\n    df['GarageYrBlt'] = df['GarageYrBlt'].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    # Name beginning with numbers are awkward to work with\n    df.rename(columns={\n        '1stFlrSF': 'FirstFlrSF',\n        '2ndFlrSF': 'SecondFlrSF',\n        '3SsnPorch': 'Threeseasonporch'\n        }, inplace=True)\n    return df","e3de7354":"# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \n                \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \n                \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \n                \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \n                \"SaleType\", \"SaleCondition\"]\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df","cf533eba":"def impute_plus(df):\n    # Get names of columns with missing values\n    cols_with_missing = [col for col in df.columns if col != 'SalePrice' and df[col].isnull().any()]\n    # Make new columns indicating imputed features (`SalePrice` column exluded)\n    for col in cols_with_missing:\n        df[col + '_was_missing'] = df[col].isnull()\n        df[col + '_was_missing'] = (df[col + '_was_missing']) * 1\n    # Impute 0 for missing numeric values\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    # Impute \"None\" for missing categorical values\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","cefca8b4":"df_train, df_test = load_data()","b0ba6809":"# # Peek at the values\n# display(df_train)\ndisplay(df_test)\n\n# # Display information about dtypes and missing values\n# display(df_train.info())\n# display(df_test.info())","36549c34":"# My default XGB parameters\n\nxgb_params = dict(\n    max_depth=3,                           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.1,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=100,                     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,                    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=1,   # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=1,          # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0,        # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1,         # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,                   # set > 1 for boosted random forests\n)","43552e53":"def score_dataset(X, y, model=XGBRegressor(**xgb_params)):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Use RMSLE (Root Mean Squared Log Error) instead of MSE (Mean Squared Error)as evaluation metric\n    # (So, we need to log-transform y to train and exp-transform the predictions)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring='neg_mean_squared_error'\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","8586d949":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSE\")","1f7b89aa":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores): \n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n","0f0169f1":"mi_scores = make_mi_scores(X, y)\n\n# Show Mutual Information (MI) score plot\n# plt.figure(dpi=120, figsize=(8, 20))\n# plot_mi_scores(mi_scores)\n","97a5bbb2":"   \nX[\"AllPub\"] = X[\"Utilities\"] == 3\nX[\"HasPool\"] = X[\"PoolQC\"] != 0\n\nmi_scores = make_mi_scores(X, y)\n\n# Show Mutual Information (MI) score plot\nplt.figure(dpi=120, figsize=(8, 20))\nplot_mi_scores(mi_scores)","e9480a72":"def drop_uninformative(df, mi_scores, threshold=0.0):\n    return df.loc[:, mi_scores > threshold]","599e447a":"drop_uninformative(X, mi_scores)","50534dee":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\nmi_scores = make_mi_scores(X, y)\nX[\"AllPub\"] = X[\"Utilities\"] == \"AllPub\"\n# X[\"HasPool\"] = X[\"PoolQC\"] != \"None\"\nmi_scores = make_mi_scores(X, y)\nX = drop_uninformative(X, mi_scores)\nX.head()\n\n# Check out if this results in any improvement from the baseline score\nscore_dataset(X, y)","3af65b93":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes(['category']):\n        X[colname] = X[colname].cat.codes\n    return X","39e165c1":"def mathematical_transforms(df):\n    X = pd.DataFrame() # Just a dataframe to hold new features\n    X['LivLotRatio'] = df.GrLivArea \/ df.LotArea\n    X['Spaciousness'] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n    X['AgeAtTOS'] = df.YrSold - df.YearBuilt\n    return X","5fb87c98":"# Check out interaction between `BldgType` and `GrLivArea`\nfeature = \"GrLivArea\"\n\nsns.lmplot(\n    x=feature, y=\"SalePrice\", hue=\"BldgType\", col=\"BldgType\",\n    data=df_train, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);","b27a1742":"# Check out interaction between `BsmtCond` and `TotalBsmtSF`\nfeature = \"TotalBsmtSF\"\n\nsns.lmplot(\n    x=feature, y=\"SalePrice\", hue=\"BsmtCond\", col=\"BsmtCond\",\n    data=df, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);","61c560c8":"# Check out interaction between `GarageQual` and `GarageArea`\nfeature = \"GarageArea\"\n\nsns.lmplot(\n    x=feature, y=\"SalePrice\", hue=\"GarageQual\", col=\"GarageQual\",\n    data=df, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);","d253a5bb":"def interactions(df):\n    # BldgType interaction\n    X_inter_1 = pd.get_dummies(df.BldgType, prefix='Bldg')\n    X_inter_1 = X_inter_1.mul(df.GrLivArea, axis=0)\n    # Bsmt interaction\n    X_inter_2 = pd.get_dummies(df.BsmtCond, prefix='BsmtCond')\n    X_inter_2 = X_inter_2.mul(df.TotalBsmtSF, axis=0)\n    # Garage interaction\n    X_inter_3 = pd.get_dummies(df.GarageQual, prefix='GarageQual')\n    X_inter_3 = X_inter_3.mul(df.GarageArea, axis=0)\n    # Combine into one DataFrame\n    X = X_inter_1.join(X_inter_2)\n    #X = X.join(X_inter_3)\n    return X","f6642727":"def counts(df):\n    X = pd.DataFrame()\n    X['PorchTypes'] = df[['WoodDeckSF',\n                        'OpenPorchSF',\n                        'EnclosedPorch',\n                        'Threeseasonporch',\n                        'ScreenPorch'\n                        ]].gt(0.0).sum(axis=1)\n    X['TotalHalfBath'] = df.BsmtFullBath + df.BsmtHalfBath\n    X['TotalRoom'] = df.TotRmsAbvGrd + df.FullBath + df.HalfBath\n    return X","7457955d":"def group_transforms(df):\n    X = pd.DataFrame()\n    X['MedNhbdArea'] = df.groupby('Neighborhood')['GrLivArea'].transform('median')\n    X['MeanAgeAtTOS'] = df.groupby('Neighborhood')['AgeAtTOS'].transform('mean')\n    return X","3dc6e029":"cluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"FirstFlrSF\",\n    \"SecondFlrSF\",\n    \"GrLivArea\",\n]","4ff120cc":"def cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new","31a5d3b4":"# # Optimize number of clusters\n# # by comparing cross validation scores\n# # (Will take some time--like HOURS)\n\n# for n in list(range(1,21)):\n#     X_orig = df_train.copy().drop(\"SalePrice\", axis=1)\n#     X = cluster_labels(df_train, cluster_features, n_clusters=n)\n#     X = X_orig.join(X)\n#     score = score_dataset(X, y, xgb)\n#     print(\"Cross-validation score:\", score, \n#         \"\\n Value used for n_clusters (number of clusters) for labeling:n=\", n)","6ff31519":"def cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","c984124b":"# # Optimize number of clusters\n# # by comparing cross validation scores\n# # (Will take some time--like HOURS)\n\n# for n in list(range(1,21)):\n#     X_orig = df_train.copy().drop(\"SalePrice\", axis=1)\n#     X = cluster_distance(df_train, cluster_features, n_clusters=n)\n#     X = X_orig.join(X)\n#     score = score_dataset(X, y, xgb)\n#     print(\"Cross-validation score:\", score, \n#         \"\\n Value used for n_clusters (number of clusters) for labeling:n=\", n)","ff13bdb9":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\ncorrplot(df_train.iloc[:,:80], annot=None)","f84fd953":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","b5924aff":"pca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n]","90bf9f24":"X_temp = X.loc[:, pca_features]\n\n# `apply_pca`, defined above, reproduces the code from the tutorial\npca, X_pca, loadings = apply_pca(X_temp)\nprint(loadings)","bed817ee":"# Plot explained variance based on components from PCA\nplot_variance(pca)","7b7bac88":"def pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"GrLivAreaPlusBsmtSF\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"RecentRemodLargeBsmt\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca","ab328c06":"sns.catplot(\n    y=\"value\",\n    col=\"variable\",\n    data=X_pca.melt(),\n    kind='boxen',\n    sharey=False,\n    col_wrap=2,\n);","ffaa580d":"# Can change PC1 to PC2, PC3, or PC4\ncomponent = \"PC1\"\n\nidx = X_pca[component].sort_values(ascending=False).index\n\ndf_train[[\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + pca_features].iloc[idx]","55af30d8":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new","59835902":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","159d70d3":"# ADDITION: Looks like we can see how\n# 'finished' the basement is from the data\ndef calcFinishedPercent(row):\n    if row.TotalBsmtSF > 0:\n        return (row.BsmtFinSF1 + row.BsmtFinSF2)\/row.TotalBsmtSF\n    else:\n        return 0\n    \n# Step 10: Calculate how finished basement is\n# X_try = X.apply(calcFinishedPercent, axis=1)\n# X_try","7ad7f342":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop('SalePrice')\n    mi_scores = make_mi_scores(X, y)\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        y_test = X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Step 1: Drop features with low Mutual Information scores\n    # X = drop_uninformative(X, mi_scores)\n\n    # Step 2: Add features from mathematical transforms \n    ######## (`LivLotRatio`, `Spaciousness`)\n    X = X.join(mathematical_transforms(X))\n\n    # Step 3: Add features from known interaction effects \n    ######## (categorical-`BldgType`and continuous-`GrLivArea`)\n    #X = X.join(interactions(X))\n\n    # Step 4: Add new feature from counts \n    ######## (`PorchTypes`, `TotalHalfBath`, `TotalRoom`)\n    X = X.join(counts(X))\n\n    # Step 5: Add new feature from group transform\n    ######## (median `GrLivArea` by `neighborhood`), (mean home age by neighborhood)\n    X = X.join(group_transforms(X))\n\n    # Step 6: Add features from k-means clustering \n    ######## (cluster labels, cluster distance)\n    #X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    #X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Step 7: Add features from PCA\n    ######## (loadings-inspired features , PCA components, & outlier indicators)\n    X = X.join(pca_inspired(X))\n    #X = X.join(pca_components(X, pca_features))\n    #X = X.join(indicate_outliers(X))\n  \n    # Label encoding for the categorical features\n    X = label_encode(X)\n    \n    # Step 9: Calculate how finished basement is\n    # X['Bsmt_Pct_Fin'] = X.apply(calcFinishedPercent, axis=1)\n    \n    # Step 10: Drop a few more things\n    # This made the training score better, but submission score worse\n    if df_test is not None:\n        mi_scores = make_mi_scores(X, pd.concat([y, y_test]))\n    else:\n        mi_scores = make_mi_scores(X, y)\n    X = drop_uninformative(X, mi_scores, 0.02)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Step (last): Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X","bf991023":"df_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, 'SalePrice']\n\nscore_dataset(X_train, y_train)","92c9afc0":"X_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb_params = dict(\n    max_depth=4,                           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.0058603076512435655,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=5045,                     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=2,                    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.22556099175248345,   # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.5632348136091383,          # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.09888625622197889,        # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=0.00890758697724437,         # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,                   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb)","4a5a373a":"# def objective(trial):\n#     xgb_params = dict(\n#         max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n#         learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n#         n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n#         min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n#         colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#         subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n#         reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#         reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#     )\n#     xgb = XGBRegressor(**xgb_params)\n#     return score_dataset(X_train, y_train, xgb)\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=40)\n# xgb_params = study.best_params","8f704ca7":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but we want to minimize RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})","b743b8fa":"# Save Submission\noutput.to_csv('submission.csv', index=False)\nprint(\"Your predictions are successfully saved!\")","ce218a11":"# Save the XGB model\nfilename = 'ames_house_xgb_model.pkl'\npickle.dump(xgb, open(filename, 'wb'))\n\n# Save processed test data\nX_test.to_csv('df_test_processed.csv', index=False)","5654aebd":"# Pick an arbitrary row (first row starts at 0)\nrow_to_show = 42\ndata_for_prediction = X_test.iloc[[row_to_show]]\n\n# Generate prediction\ny_sample = np.exp(xgb.predict(data_for_prediction))\n\n# Create object that can calculate Shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# Calculate Shap values from prediction\nshap_values = explainer.shap_values(data_for_prediction)","26b7cb6c":"plt.title('Feature importance based on SHAP values?')\nshap.summary_plot(shap_values, data_for_prediction, plot_type=\"bar\")","38e54f73":"plt.title('Feature impact on model output (feature impact in details below)')\nshap.summary_plot(shap_values, data_for_prediction)\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","fc626400":"# Use test set to get predictions\ndata_for_prediction = X_test\n\n# Generate predictions\ny_sample = np.exp(xgb.predict(data_for_prediction))\n\n# Create object that can calculate Shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# Calculate Shap values from predictions\nshap_values = explainer.shap_values(data_for_prediction)","7d6e2d8c":"plt.title('Feature impact on overall model output (feature impact in details below)')\nshap.summary_plot(shap_values, data_for_prediction)\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","de7be576":"### Create Features with Pandas (Data Wrangling)","eb85003e":"**For a single prediction, what features in the data did the model think are most important?**","9eeced6c":"###########aelledge###########\n\nThis is the point where I'm starting to make modifications and see what I can add\n\n###########aelledge###########","c8d85f94":"The trend lines being significantly different from one category to the next indicates an interaction effect between `GrLivArea` and `BldgType` that relates to a home's `SalePrice`. \n\nBelow are several other detected interaction effects between categorical and numerical variables:","28c1c518":"## Load Data","a4bd4688":"Let's check if removing these features actually lead to a performance gain.","218fd1ca":"### Encode the Statistical Data Type","a41ce92e":"This notebook outlines the step-by-step process of creating a house price prediction model\u2014it includes data pre-processing, feature engineering, model training, hyperparameter tuning, and model explainability. The prediction model generated currently ranks in the top 8% of Kaggle's [House Price Prediction Competition](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) leaderboard and top 1% of [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) leaderboard (as of 10\/29\/2021).\n\nIf you're interested in learning more about model deployment (as an interactive [web app](https:\/\/share.streamlit.io\/ruthgn\/ames-housing-price-prediction\/main\/ames-house-ml-app.py)), check out [this notebook](https:\/\/www.kaggle.com\/ruthgn\/top-1-model-interpretation-deployment).","b087b553":"Let's go ahead and define one transformation now--a label encoding for the categorical features. \n\n*Note that we're specifically using label encoding for our unordered categories because we are using a tree-ensemble, XGBoost, particularly. If instead we decide to try using a linear regression model, we're going to have to use one-hot encoding for features with unordered categories.*","fe775934":"Before we delve into feature engineering, we're going to establish a baseline score to judge our upcoming feature sets against. We will make our predictions with an XGBoost model and create a function to compute the cross-validated *Root Mean Squared Error* (RMSE) score for each feature set our model trains on. \n\nXGBoost minimizes *Mean Squared Error* (MSE), but we want to minimize Root Mean Squared Error (RMSE) specifically, requiring us to \"reshape\" our target feature (`Sale Price`) using log transformation for training and later applying exponential transform to the predictions. Mathematically, this makes sense because we typically use the log scale for variables that change multiplicatively with other factors. How do we know when variables should be modeled as changing multiplicatively? \n- Every day language (surprise, surprise!). Examples include prices (\"foreclosed homes sell at a 20% to 30% discount\"), and sales (\"your yoy sales are up 20% accross models\").\n- More generally, variables that are strictly non-negative (e.g, volatility, counts of errors or events, rainfall) are often treated as changing linearly in a log scale.","fb94dd23":"Next, we're going to create another new feature `TotalHalfBath` that contains the sum of half-bathrooms within the property.","41b71415":"# Part 4 - Train Model and Create Predictions","97dc1828":"The PCA algorithm gives us *loadings* which describe each *component* of variation, and also the components which were the transformed datapoints. The loadings can suggest features to create. Additionally, we can use the components as features directly.","e5749a0a":"#### Interactions","1566ff8c":"Note:\n\nTo use, follow code sample below:\n\n`encoder = CrossFoldEncoder``(MEstimateEncoder, m=1)`\n\n`X_encoded = encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))`\n\n","476e5fe5":"# Part 2 - Feature Engineering","3ab1fd99":"This is a copy of the original, which was by ruthgn. I'm seeing if I can build on it to at least get a better understanding of ML and if at all possible, to get a slightly better score","187d8c5a":"###########aelledge###########\n\nI suspect the age of the home at the time of sale might be a useful metric\n\n###########aelledge###########","e20baf16":"###########aelledge###########\n\nIn the plot above, you can see the \"AllPub\" and \"HasPool\" categories now at least have a non-zero MI score\n\n###########aelledge###########","5d574981":"This baseline score should help us in knowing whether some set of features we've assembled in our experimentation actually led to any improvement or not.","8ac2212e":"Time to combine everything together.","2cfb11a7":"## Create Features","a7e01015":"###########aelledge###########\n\nI suspect the age of the home at the time of sale might separate out into some distinct groups, so I'm going to add a group transform for it\n\n###########aelledge###########","aaf30334":"###########aelledge###########\n\nI moved drop_uninformative to the end so I could work with a few of the features with 0 MI score before they were dropped\n\n###########aelledge###########","21a7b10c":"# Part 5 - Model Interpretation","6e28c061":"Additionally, we will also sum up the total number of rooms (including full and half bathrooms) in each property and store them in a new feature called `TotalRoom`.","19d8a661":"The value of a home often depends on how it compares to typical homes in its neighborhood. Therefore, let's create a new feature called `MedNhbdArea` that describes the *median* of `GrLivArea` grouped on `Neighborhood`.","86f6a05f":"Now that we are done creating out final set of features, it's time to do some hyperparameter tuning with XGBoost to optimize our model performance further.","5807e3a8":"It's time for us to take a closer look at the features we have in our dataset. We will analyze how much potential a feature has by computing its *utility score*.","96a3a3a5":"Applying PCA can also help us determine houses that are outliers or houses having values not well represented in the rest of the data.","cf4d3728":"#### Grouped Transform","c9fb882c":"Rather than just tuning them by hand, we're going to use a tuning library, Optuna, with XGBoost:","ae63b5b9":"### Target Encoding","ae7ff9eb":"During an exploratory analysis of our data, something interesting came up:","cfd30566":"We have a number of features that are highly informative and several that don't seem to be informative at all (at least by themselves). Therefore, we will focus our efforts on the top scoring features. Training on uninformative features can lead to overfitting as well, so features with 0.0 MI scores are going to be dropped entirely.","fdc43ac4":"# Part 3 - Hyperparameter Tuning","d5514ec8":"Some models can benefit from having these outliers indicated, which is what this next transform will do.","f0b0fb4c":"#### Mathematical Transforms (Ratios)","39be227f":"Next, we will encode each feature with its correct data type to ensure each feature is treated appropriately by whatever functions we use moving forward.\n\nThe numeric features in our particular dataset are already encoded correctly (`float` for continuous and `int` for discrete features). What we need to pay closer attention to is the categorical features. For instance, note in particular, that the 'MSSubClass' feature is read as an `int` type, but is actually a nominative categorical.","bd360cbf":"Next, we will use the *distance* of the observations to each cluster as another new feature.","497f7e61":"## Data Preprocessing","52b4d99d":"We'll impute 0 for missing numeric values and \"None\" for missing categorical values. Additionally, we will create \"missing value\" indicator columns--these columns will contain boolean values indicating whether a particular feature value was imputed for a sample.","d9211907":"###########aelledge###########\n\nOne of the features about to be dropped is \"Utilities\", which I find surprising. Glancing through the data, it looks like this feature may be of low importance because very few properties had less than all public utilities available. I created a new feature called \"AllPub\", which just indicated whether or not a property had all available public utilities\n\nI also think it's weird that Pool sq ft and Pool quality both have 0 MI score. I've simplified it down to \"HasPool\"\n\n###########aelledge###########","f5a313dc":"Now we can call the data loader and get the processed data splits--let's take a quick look:","51ed6207":"**How does each feature affect the model's predictions in a big-picture sense? In other words, what is its typical effect when considered over a large number of possible predictions?**","7807b3ac":"### Handle Missing Values","2cac02e5":"Let's pick a subset of features for PCA:","25a2dbf9":"_____\n\n# Acknowledgement\n\nSteps taken throughout the model-building process in this notebook are inspired by [this Kaggle notebook](https:\/\/www.kaggle.com\/ryanholbrook\/feature-engineering-for-house-prices) by Ryan Holbrook and Alexis Cook (modified for better performance). Check out their notebook for more ideas to improve the prediction model.\n\nSome text in the beginning of the Model Interpretation section is copied from Kaggle's fantastic [Machine Learning Explainability](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) course.\n\nOther quoted sources include [Business Data Science](https:\/\/www.amazon.com\/Business-Data-Science-Combining-Accelerate\/dp\/1260452778) by Matt Taddy.","3ad33afb":"# Part 1 - Preliminaries","238b6196":"###########aelledge###########\n\nAdding \"AllPub\" didn't affect the score at all so might be useful later, but adding \"HasPool\" increased the score so leaving it out\n\n###########aelledge###########","401f099e":"First, we'll use SHAP Values to explain individual predictions. Afterwards, we will look at model-level insights.","21ad0d4c":"As you can see, for each of the components there are several points lying at the extreme ends of the distributions -- they are outliers. Let's see those houses that sit at the extremes of a component:","4c52a2ac":"## Establish Baseline","c650e5c3":"## Introduction ","136a9c4d":"Nice! Removing our uninformative features does lead to a slight performance gain. We will add our new `drop_uninformative` function to our feature-creation pipeline.","6070ac2b":"#### PCA Application - Indicate Outliers","91ac2f35":"Our feature scores are listed below:","98e827c8":"### Principal Component Analysis","72d8f595":"### Create Final Feature Set","faba9626":"We will create a function to make corrections on several detected issues within the dataset:","a07c9787":"Let's use the help of an unsupervised algorithm (k-mean clustering) to create new features. We've selected the following features to determine what the clusters are based on.","67174870":"We're going to use target encoding using the following steps:\n1. Split the data into folds, each fold having two splits of the dataset.\n2. Train the encoder on one split but transform the values of the other.\n3. Repeat for all the splits.\n\nThe next cell contains a wrapper we can use with any target encoder:","85d4da61":"Ratios seem to be difficult for most models to learn, so creating new features expressing ratio combinations can often lead to some easy performance gains. We're going to create two new features expressing important ratios using mathematical transformation:\n\n- `LivLotRatio`: the ratio of `GrLivArea` to `LotArea`\n- `Spaciousness`: the sum of `FirstFlrSF` and `SecondFlrSF` divided by `TotRmsAbvGrd`\n","e8ab3bda":"This time we'll use PCA, another unsupervised learning method, to create more new features.\n\n*Note: We are not including missing value indicator columns when assessing correlations because the combinations of rows having many 0s in these indicator columns will yield a NaN result when the given numerator and denominator are equal to 0 during the calculation.*","2279fb43":"First, we will use the cluster labels generated by the algorithm as a new feature.","09e14f9d":"Optuna's recommended parameter values:\n\n\n```\nParameters: \n{'max_depth': 4, 'learning_rate': 0.0058603076512435655, 'n_estimators': 5045, 'min_child_weight': 2, 'colsample_bytree': 0.22556099175248345, 'subsample': 0.5632348136091383, 'reg_alpha': 0.09888625622197889, 'reg_lambda': 0.00890758697724437}. \nScore: 0.11442743288078303.\n```\n\n","cb861ef7":"### k-Means Clustering","ca1dca76":"Let's run our new function on the processed data and get a baseline score:","b3177cee":"To create our final predictions, we will take the following steps:\n* create your feature set from the original data\n* train XGBoost on the training data\n* use the trained model to make predictions from the test set\n* save the predictions to a CSV file\n\n","efd43f19":"Now that we've seen the inner workings of our model in making an individual prediction, let's aggregate all the information into powerful model-level insights.","9550875a":"### Clean Data","84043370":"Since our goal right now is to use the results of PCA to discover one or more new features that could improve the performance of our model, let's create features inspired by the loadings:","2279668a":"Let's create a new feature called `PorchTypes` that describes how many kinds of outdoor areas a dwelling has. We will count how many of the following are greater than 0.0:\n\n-`WoodDeckSF`\n\n-`OpenPorchSF`\n\n-`EnclosedPorch`\n\n-`Threeseasonporch`\n\n-`ScreenPorch`","7da8440b":"Notice that there are several dwellings listed as `Partial` sales in the `Edwards` neighborhood that stand out. A partial sale is what occurs when there are multiple owners of a property and one or more of them sell their \"partial\" ownership of the property. These kinds of sales often happen during the settlement of a family estate or the dissolution of a business and aren't advertised publicly, making these cases true outliers, especially within our supposed research context--houses on the open market.","0974b591":"Our final model landed in the top 8% of Kaggle House Prices Prediction leaderboard (as of 10\/24\/2021). Groovy! However, it's important for us to take it a step further.\n\n> Many people say machine learning models are \"black boxes\", in the sense that they can make good predictions but you can't understand the logic behind those predictions. This statement is true in the sense that most data scientists don't know how to extract insights from models yet.\n\nThere is an increasing need for data scientists who are able to extract insights from sophisticated machine learning models to help inform human decision-making. Some decisions are made automatically by models like the ones we have just built, but many important decisions are made by humans. For these decisions, insights can be more valuable than predictions. Beyond informing human decision-making, insights extracted from machine learning models have many other uses, including:\n- Debugging\n- Informing feature engineering\n- Directing future data collection\n- Building Trust\n\nRight now, we want to answer the following questions about our model:\n* What features in the data did the model think are most important?\n* For any single prediction from a model, how did each feature in the data affect that particular prediction?\n* How does each feature affect the model's predictions in a big-picture sense (what is its typical effect when considered over a large number of possible predictions)?","8945ca71":"## Feature Utility Scores","e3d4aee4":"A closer look at the dataset make it clear that there are categorical features with typos in the categories:","a321ce3b":"Putting the transformations into separate functions makes it easier to experiment with various combination.\n\nWe can modify any of these transformations or come up with some more ideas to add to the pipeline. At this stage, we have left the ones that gave the best results uncommented.","36119b5c":"## Imports and Configuration","584456d0":"###########aelledge###########\n\nAdded a optional threshold argument here to drop features of low but non-zero importance\n\n###########aelledge###########\n","c1a4d90a":"To make our feature engineering workflow more modular, we'll define a function that will take a prepared dataframe and pass it through a pipeline of transformations to get the final feature set.","db19edb3":"#### Counts","49e5625c":"**How did each feature in the data affect that particular prediction?**"}}