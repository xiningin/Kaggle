{"cell_type":{"acb2c02b":"code","fd2e9ad3":"code","39df6fce":"code","61c177e9":"code","a4801ae9":"code","229e6547":"code","be079f2f":"code","0203024f":"code","cea54640":"code","02bd0b72":"code","fe16c7ba":"code","b9fa4fe4":"code","a1a41565":"code","863e00ea":"code","bf8d3d31":"code","99a552d5":"code","14af4a57":"code","dc6bbe84":"code","eea94bb4":"code","9a1a0717":"code","7a802c94":"code","5cc32564":"code","041e46fd":"code","a706175d":"code","1c58b604":"code","5135f91d":"code","36317419":"code","541f3f37":"code","3124e52d":"code","c94aeaa2":"code","812028d1":"code","6343763e":"code","d0be3ba0":"markdown","11658d31":"markdown","987eada6":"markdown","282b1af2":"markdown","42c0e6b1":"markdown","e0613eca":"markdown","c3254505":"markdown","7c45bf4a":"markdown","ac5f323f":"markdown","0f351e12":"markdown","d598009b":"markdown","fbcf1c86":"markdown","9ceae15b":"markdown","2d3a91c4":"markdown","d3cef9b8":"markdown","5e511688":"markdown","0036fd46":"markdown","0945a6c1":"markdown","2e44e6dd":"markdown","aef69337":"markdown"},"source":{"acb2c02b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport zipfile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.api.types import is_numeric_dtype\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', None)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd2e9ad3":"X_list_num = ['full_sq', 'num_room', 'area_m', \n              'kremlin_km', 'big_road2_km', 'big_road1_km',\n              'workplaces_km',\n              'stadium_km', 'swim_pool_km', 'fitness_km', \n              'detention_facility_km', 'cemetery_km',\n              'radiation_km', 'oil_chemistry_km',\n              'theater_km', 'exhibition_km', 'museum_km', \n              'park_km', 'public_healthcare_km',  \n              'metro_min_walk','metro_km_avto', \n              'bus_terminal_avto_km', 'public_transport_station_min_walk',\n              'railroad_station_walk_min', 'railroad_station_avto_km',\n              'kindergarten_km', 'school_km', 'preschool_km',\n              'university_km', 'additional_education_km',\n              'shopping_centers_km', 'big_market_km',\n              'ekder_all', 'work_all', 'young_all', 'ID_metro', \n              'office_raion', 'sport_objects_raion',\n              'raion_popul', 'healthcare_centers_raion',\n              'school_education_centers_raion', \n              'preschool_education_centers_raion']\n\nX_list_cat = ['sub_area', 'ecology','big_market_raion', 'railroad_terminal_raion', 'timestamp','product_type']","39df6fce":"train = zipfile.ZipFile('..\/input\/sberbank-russian-housing-market\/train.csv.zip', 'r')\ntest = zipfile.ZipFile('..\/input\/sberbank-russian-housing-market\/test.csv.zip', 'r')\nmacro_train = zipfile.ZipFile('..\/input\/sberbank-russian-housing-market\/macro.csv.zip', 'r')\n\ntrain.extract('train.csv')\ntest.extract('test.csv')\nmacro_train.extract('macro.csv')\n\ntrain = pd.read_csv('.\/train.csv',sep='\\s*,\\s*')\ntest = pd.read_csv('.\/test.csv',sep='\\s*,\\s*')\nmacro_train = pd.read_csv('.\/macro.csv')","61c177e9":"train","a4801ae9":"macro_train","229e6547":"cat_features_macro=[]\n\nfor col in macro_train:\n    if not is_numeric_dtype(macro_train[col]):\n        cat_features_macro.append(col)\ncat_features_macro","be079f2f":"# Replace all empty strings by NaN\nmacro_train = macro_train.replace('', np.nan)\n\n# Holds the count of NaN values for all the features\nempty_col_list = []\n\n# Drop columns with more than 30% blank\nfor col in macro_train:\n    if macro_train[col].isnull().values.any():\n        empty_col_list.append((col, macro_train[col].isnull().sum()))\n    if macro_train[col].isnull().sum() > 658:\n        macro_train.drop(col, axis=1, inplace=True)        \n\nmacro_list = list(macro_train.columns)\n\n# Drop empty rows\nmacro_train.dropna(axis=0, how='all', thresh=None, inplace=True)\n# Drop rows with at least 50% empty\nmacro_train.dropna(axis=0, thresh=44, inplace=True)\n\n# Add price_doc to dataset\ntemp = train\nmacro_train = temp.merge(macro_train, how='left', on='timestamp')\nmacro_train = macro_train[macro_list+[\"price_doc\"]]\nmacro_train\n","0203024f":"# Display the feature correlation with the target\npearson_macro = macro_train.corr(method='pearson')\nmacro_corr_with_prices = pearson_macro[\"price_doc\"][:-1]\nmacro_corr_with_prices[abs(macro_corr_with_prices).argsort()[::-1]]","cea54640":"# Display the most correlated features, in the 0.1 range\ntop32_macro_features = macro_corr_with_prices[abs(macro_corr_with_prices).argsort()[::-1]][:32].index.values.tolist()\nprint('The most correlated with prices:\\n', top32_macro_features)","02bd0b72":"macro_corr_with_prices[abs(macro_corr_with_prices).argsort()[::-1]][:32]","fe16c7ba":"macro_train_na_data = pd.DataFrame(macro_train[top32_macro_features].isnull().sum().to_numpy().reshape(1, 32), columns = [top32_macro_features])\n\nmacro_train_na_data.loc[:, (macro_train_na_data != 0).all()]","b9fa4fe4":"# Drop variables with too many NaN values and categorical ones cos they dun look useful\n\ndrop_features = ['incidence_population', 'unprofitable_enterpr_share','profitable_enterpr_share',\n                'fin_res_per_cap', 'construction_value', 'grp', 'provision_doctors']\n\nmacro_train = macro_train[top32_macro_features+[\"price_doc\",\"timestamp\"]].drop(drop_features, axis=1)\nmacro_train","a1a41565":"# Display the correlation matrix features used in macro\nplt.figure(figsize=(18, 12))\nsns.heatmap(macro_train.corr(), cmap=\"mako\",\n            xticklabels=macro_train.corr().columns.values,\n            yticklabels=macro_train.corr().columns.values)\nplt.title(\"Correlation Matrix (Features used in Macro) \", fontsize=20);","863e00ea":"# Data exploration for numerical values of features in features_used\nrow = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\nnumeric_explore_macro = pd.DataFrame(index=row)\n\nfor feature in list(macro_train.columns):\n    if is_numeric_dtype(macro_train[feature]):\n        numeric_explore_macro[feature] = pd.Series(macro_train[feature].describe(), index=numeric_explore_macro.index)\n        \nnumeric_explore_macro","bf8d3d31":"train['price_doc'].describe()","99a552d5":"f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 6))\nsns.distplot(train['price_doc'].values, hist=True, kde=True, bins=100, ax=ax1, color = 'darkblue')\nplt.title('Distribution plot of price_doc', fontsize=18)\nax2.set_xlabel(\"Prices\")\nax1.set_ylabel(\"Distribution\")\n\nsns.distplot(np.log(train['price_doc'].values), bins=100, color='#228B22', ax=ax2)\nax2.set_xlabel(\"Logarithm of the variable 'Prices'\")\nax2.set_ylabel(\"Distribution\")\n\nplt.suptitle('Sberbank Russian Housing Data');","14af4a57":"# Create the table of descriptive statistics\nprint (\"Sberbank Russian Housing Dataset Statistics: \\n\")\nprint (\"Number of houses = \", len(train['price_doc']))\nprint (\"Minimum house price = \", np.min(train['price_doc']))\nprint (\"Maximum house price = \", np.max(train['price_doc']))\nprint (\"Mean house price = \", \"%.2f\" % np.mean(train['price_doc']))\nprint (\"Median house price = \", \"%.2f\" % np.median(train['price_doc']))\nprint (\"Standard deviation of house prices =\", \"%.2f\" % np.std(train['price_doc']))","dc6bbe84":"# Display the correlation matrix of features\nplt.figure(figsize=(20, 20))\nsns.heatmap(train.corr(), cmap='viridis',\n            xticklabels=train.corr().columns.values,\n            yticklabels=train.corr().columns.values)\nplt.title(\"Correlation Matrix (All Features)\", fontsize=20);","eea94bb4":"# Display the feature correlation with the target\npearson = train.corr(method='pearson')\ncorr_with_prices = pearson[\"price_doc\"][:-1]\ncorr_with_prices[abs(corr_with_prices).argsort()[::-1]]","9a1a0717":"# Display the most correlated features\ntop50_features = corr_with_prices[abs(corr_with_prices).argsort()[::-1]][:50].index.values.tolist()\nprint('The most correlated with prices:\\n', top50_features)\n","7a802c94":"# Display the correlation matrix of top 50 features\nplt.figure(figsize=(18, 12))\nsns.heatmap(train[top50_features+[\"price_doc\"]].corr(), cmap=\"mako\",\n            xticklabels=train[top50_features+[\"price_doc\"]].corr().columns.values,\n            yticklabels=train[top50_features+[\"price_doc\"]].corr().columns.values)\nplt.title(\"Correlation Matrix (Top 50) \", fontsize=20);","5cc32564":"# Display correlation value of selected features\nselected_features = X_list_num+X_list_cat\n\nfor x in selected_features:\n    if x not in top50_features:\n        try:\n            print(f\"{x}: {corr_with_prices[x]}\")\n            if abs(corr_with_prices[x]) < 0.1:\n                selected_features.remove(x)\n        except:\n            continue         ","041e46fd":"features_used = selected_features+top50_features\nfeatures_used = list(set(features_used))\nprint(len(features_used))\nfeatures_used","a706175d":"# Display the correlation matrix of features_used\nplt.figure(figsize=(18, 12))\nsns.heatmap(train[features_used+[\"price_doc\"]].corr(), cmap=\"magma\",\n            xticklabels=train[features_used+[\"price_doc\"]].corr().columns.values,\n            yticklabels=train[features_used+[\"price_doc\"]].corr().columns.values)\nplt.title(\"Correlation Matrix (Features Used)\", fontsize=20);","1c58b604":"train_na_data = pd.DataFrame(train[features_used].isnull().sum().to_numpy().reshape(1, 81), columns = [features_used])\n\ntrain_na_data.loc[:, (train_na_data != 0).all()]","5135f91d":"test_na_data = pd.DataFrame(test[features_used].isnull().sum().to_numpy().reshape(1, 81), columns = [features_used])\n\ntest_na_data.loc[:, (test_na_data != 0).all()]","36317419":"# Data exploration for numerical values of features in features_used\nrow = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\nnumeric_explore = pd.DataFrame(index=row)\n\nfor feature in features_used:\n    if is_numeric_dtype(train[feature]):\n        numeric_explore[feature] = pd.Series(train[feature].describe(), index=numeric_explore.index)\n        \nnumeric_explore","541f3f37":"train['year_month'] = train['timestamp'].apply(lambda x: x[:4] + x[5:7]).astype(int)\ntrain['month'] = train['timestamp'].apply(lambda x: x[5:7]).astype(int)\ntrain.drop('timestamp', axis=1, inplace=True)","3124e52d":"plt.figure(figsize=(10, 10))\nsns.barplot(x='year_month', y='price_doc', data=train)\nplt.title('Prices vs year_month', fontsize=18)\nplt.xticks(rotation='vertical')\nplt.title(\"Effect of year_month on prices\", fontsize=20);","c94aeaa2":"plt.figure(figsize=(10, 10))\nsns.barplot(x='month', y='price_doc', data=train)\nplt.xticks(rotation='vertical')\nplt.title(\"Effect of month on prices\", fontsize=20);","812028d1":"plt.figure(figsize=(10, 30))\nsns.barplot(y='sub_area', x='price_doc', data=train, orient='h', palette='light:#5A9', estimator=np.median)\nplt.title('Prices depending on sub-area', fontsize=18)","6343763e":"categorical_feats = ['ecology','big_market_raion', 'railroad_terminal_raion', 'product_type']\nnr_rows = 2\nnr_cols = 2\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        i_col = categorical_feats[i]\n        sns.countplot(x=i_col, data=train,ax = axs[r][c])\n\nfig.suptitle('Countplots for categorical features', y=1.02, fontsize=18)  \nplt.tight_layout()    ","d0be3ba0":"## Correlation map","11658d31":"The prices of the housing varies greatly depending on the subarea.","987eada6":"## Determine amount of NA values in selected features","282b1af2":"# Analysis of Macro.csv ","42c0e6b1":"### Features: big_market_raion, railroad_terminal_raion, ecology, product_type","e0613eca":"# Datasets and Inputs\n\nThe basis for the investigation is a large number of economic indicators for pricing and prices themselves (train.csv and test.csv). Macroeconomic variables are collected in a separate file for transaction dates (macro.csv). In addition, the detailed description of variables is provided (data_dictionary.txt).\n\nDue to the large number of features, We have chosen to analysize the following independent variables:\n\n1. the dollar rate, which traditionally affects the Russian real estate market;\n2. the distance in km from the Kremlin (the closer to the center of the city, the more expensive);\n3. indicators characterizing the availability of urban infrastructure nearby (schools, medical and sports centers, supermarkets, etc.) ;\n4. indicators of a particular living space (number of rooms, floor, etc.);\n5. proximity to transport nodes (for example, to the metro);\n6. indicators of population density and employment in the region of housing accommodation.\n\nAll these economic indicators have a strong influence on price formation and can be used as a basic set for regression analysis. \nExamples of numerical variables: the distance to the metro, the distance to the school, the dollar rate at the transaction moment, the area of the living space. \nExamples of categorical variables: neighborhoods, the nearest metro station, the number of rooms.\n","c3254505":"## Data Exploration of numerical features used","7c45bf4a":"## Data Exploration of categorical feature used","ac5f323f":"## Numeric exploration of the features","0f351e12":"# Analysis of Train.csv","d598009b":"As seen above, although the correlation value is not within the top 50, it is still relatively high. Thus we shall consider all features that has correlation more than 0.1 in our model. ","fbcf1c86":"In the second half of the year (and especially in October and November) prices are lower.","9ceae15b":"## Check NaN values in Macro.csv","2d3a91c4":"As seen, there is an increasing trend in prices of the russian housing as the years increases.","d3cef9b8":"# Domain Background\n\nAlthough the housing market is relatively stable in Russia, the country\u2019s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as a number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal.","5e511688":"### Feature: Timestamp","0036fd46":"### Feature: sub_area","0945a6c1":"## Correlation Map of the features used in macro.csv","2e44e6dd":"## Analysis of target variable (price) in Test data","aef69337":"Most selected categorical values are binary in nature. For features big_market_raion and railroad_terminal_raion, the data is very baised."}}