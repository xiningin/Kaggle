{"cell_type":{"11c93d86":"code","0a100b9e":"code","dadacb2b":"code","cd9df741":"code","693026a2":"code","0708610b":"code","a06c8a3c":"code","ca25cb7a":"code","70e8c3e1":"code","d3234dca":"code","33641a63":"code","83b75a2f":"code","abf08731":"code","ed6614c9":"code","41cd2bd2":"code","f8126350":"code","94b40a4c":"code","9f85e8e2":"code","7d85286f":"code","ec3f8ff7":"code","a67d296f":"code","a93f9c09":"code","7089a4dc":"code","001b6ad9":"code","a69b72b2":"code","1932c547":"code","ce301eb8":"code","02d0e859":"code","0db6c146":"code","2dac240b":"code","77ffc5ac":"code","87c4c9d1":"code","8dd38297":"code","5240caed":"code","088a488f":"code","371874e8":"code","b5d463dd":"code","023483cf":"code","049bae10":"code","43dda42a":"code","902f1ea1":"code","1b8e57a7":"code","9571ed66":"code","19704435":"code","e36133a4":"code","02b360e5":"code","992c7189":"code","48cd6665":"code","c1ba5943":"code","bee34835":"code","cfda71f4":"code","0a40b634":"code","bbb5654e":"code","49fbc70d":"code","be1c7998":"code","52acd36d":"code","f96a092f":"code","8c46ea0c":"code","e8e42f02":"code","fce2d7f7":"code","aa5c2177":"code","43b77c78":"code","7461163b":"code","c2585b02":"code","40735030":"code","7798d7a9":"code","657c4a53":"code","1dd53966":"code","fa320c9a":"code","23a74396":"markdown","b90a04e9":"markdown","5bd9d802":"markdown","ffdca92a":"markdown","14e1ce22":"markdown","832e24a6":"markdown","eb3efb74":"markdown","fcaa4bdb":"markdown","133c7bf7":"markdown","ded340e7":"markdown","f039be12":"markdown","a0b10fda":"markdown","f2b63867":"markdown","b75ccb96":"markdown","159be4d5":"markdown","db80028d":"markdown","074e7315":"markdown","f0cd9785":"markdown","e53f812a":"markdown","728af95b":"markdown","069043b6":"markdown","134ff68c":"markdown","b4c07a0d":"markdown","69359f9f":"markdown"},"source":{"11c93d86":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import RFECV, RFE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import export_graphviz\nfrom IPython import display\nfrom sklearn import tree\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler, MultiLabelBinarizer\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.impute import MissingIndicator, KNNImputer\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.feature_selection import f_classif, f_regression, mutual_info_regression, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedKFold\n\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport xgboost as xgb\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom scipy.stats import sem\nfrom sklearn.metrics import mean_squared_error\nfrom IPython.display import display, HTML\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import plot_tree\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\nfrom xgboost import plot_tree\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n\n\n%matplotlib inline","0a100b9e":"def scale_data(train, test=pd.DataFrame(), numeric_features=[],  categorical_features=[],  scaler=preprocessing.StandardScaler()):\n    \n    \"\"\"Scales separately both train and test dataframes\n    \n\n    A C   train_: + col first dataframe\n        test: second dataframe, if empty, only first one will be used\n        columns_to_encode, columns needing encoding \n        columns_to_scale, scalng needed\n        columns_to_binary, binary colmuns\n        scaler= Scaler function like StandardScaler()):\n    \n    Returns:\n         Scaled test and train dataframes (only first one is needed to be returned)\n         Column lists as encoding might have created new columns\n    \"\"\"\n\n\n    def scale(df, numeric_features, categorical_features, scaler):\n    \n        #scale and encode dataframe columns inplace\n        column_list_encode = []\n        if categorical_features != []:\n            df_encode=pd.DataFrame()\n            for col in categorical_features:\n                df[col]=df[col].astype('category')\n                df_col=pd.concat([df[col],pd.get_dummies(df[col], prefix='Category__' + col,dummy_na=False)],axis=1).drop(col,axis=1)\n                df_encode = pd.concat([df_encode, df_col],axis=1) \n            column_list_encode=df_encode.columns[df_encode.columns.str.startswith(\"Category__\")].tolist()\n        if numeric_features != []:\n            #note, we must store the original index \n            df_scale=pd.DataFrame(scaler.fit_transform(df[numeric_features]), columns=numeric_features, index=df.index)\n        if (categorical_features != []) & (numeric_features != []):\n            df=pd.concat([df_scale,df_encode],axis=1)\n        elif categorical_features != []:\n            df=df_encode\n        else:\n            df=df_scale\n        column_list = numeric_features + column_list_encode\n        return(df, column_list)\n\n\n    train_scaled, column_list_train = scale(train, numeric_features, categorical_features, scaler)\n    if not test.empty:\n        test_scaled, column_list_test = scale(test, numeric_features, categorical_features, scaler)\n\n        #The creation of dummy values can create different set of columns between train\/test\n        #therefore we need to take only those columns that are available on both\n        column_list_diff_train_test = list(set(column_list_train) - set(column_list_test))\n        column_list_diff_test_train = list(set(column_list_test) - set(column_list_train))\n        column_list = list(set(column_list_train) - set(column_list_diff_train_test) - set(column_list_diff_test_train))\n        return(train_scaled, test_scaled, column_list)\n    else:\n        return(train_scaled, column_list)\n        \n","dadacb2b":"def force_show_all(df):\n    #if we need to really show all\n    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n            display(HTML(df.to_html()))","cd9df741":"def select_kbest(X, y, kbest_score_func, k='all'):\n    selector = SelectKBest(kbest_score_func, k=k)\n    selector.fit(X, y)\n    features_names = X.columns\n    features_scores = selector.scores_\n    features_selected = selector.get_support()\n    \n    dict = {'Column': features_names, 'Score': features_scores, 'Selected': features_selected}\n    features_df = pd.DataFrame(dict)\n    features_df.sort_values('Score', inplace=True, ascending=False)\n    features_df.reset_index(drop=True, inplace=True)\n \n    return(features_df)\n    \ndef draw_features(y, x, title):\n    \n    #draw list of features \n    fig, ax = plt.subplots() \n    width = 0.4 # the width of the bars \n    ind = np.arange(len(y)) # the x locations for the groups\n    ax.barh(ind, y, width, color='green')\n    ax.set_yticks(ind+width\/10)\n    ax.set_yticklabels(x, minor=False)\n    plt.title(title)\n    plt.xlabel('Relative importance')\n    plt.ylabel('Feature') \n    fig.set_size_inches(11, 9, forward=True)\n    plt.plot()\n\n    \ndef draw_results(X, y, model):\n\n    #plots ROC and confusion matrix from y, X based on model model\n    #works only if the results are binary \n    \n    y_pred = model.predict(X)    \n    fpr, tpr, threshold = metrics.roc_curve(y, y_pred)\n    roc_auc = metrics.auc(fpr, tpr)\n\n    plt.figure(figsize=(15,7))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\ndef draw_confusion(X, y, model):\n\n    y_pred = model.predict(X)\n    y_pred_bin = np.where(model.predict(dtest)<0.5, 0,1)\n    tn, fp, fn, tp = confusion_matrix(y, y_pred_bin).ravel()\n    \n\n\n    n= tn+fp+fn+tp\n    n_success = tn + tp\n    n_fail = fp + fn\n    # Our priors Beta distribution parameters\n    prior_alpha = 1\n    prior_beta = 1\n     \n    posterior_distr = stats.beta(a, b)\n    p_low, p_high = posterior_distr.interval(0.95)\n    \n    lower, upper = proportion_confint(tn + tp, tn + fp + fn + tp, 0.05)\n    \n    print(\"Accuracy: {}\".format(accuracy_score(y, y_pred_bin)))\n    print(\"F1 Score: \", f1_score(y, y_pred_bin, average=\"macro\"))\n    print(\"Precision Score: \", precision_score(y, y_pred_bin, average=\"macro\"))\n    print(\"Recall Score: \", recall_score(y, y_pred_bin, average=\"macro\"))\n    print('Frequentist onfidence level lower=%.3f, upper=%.3f' % (lower, upper))\n    print('Bayesian credible level lower=%.3f, upper=%.3f' % (p_low, p_high))\n\n    cf_matrix = pd.DataFrame(confusion_matrix(y, y_pred_bin), index = ['survived','lost'], columns=['survived','lost'])\n    plt.figure(figsize=(10,7))\n    sns.heatmap(cf_matrix, annot=True, fmt='000', annot_kws={\"size\": 18})\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n\ndef create_submission(test, model, dtvalues):\n    \n    #creates a submission dataframe needed in Kaggle competition \n    y_pred = model.predict(dtvalues)\n    y_pred_bin = np.where(model.predict(dtvalues)<0.5, 0,1)\n    test.loc[:, target] = y_pred_bin\n    submission = test[['PassengerId',target]].copy()\n    submission[target] = submission[target].astype(int)\n    submission.sort_values('PassengerId').reset_index(drop=True, inplace=True)\n    return(submission)\n\ndef draw_true_vs_predicted(X, y, model, title, binarize=False):\n    \n    #this point with a histogram both predicted and true\n    y_pred =  model.predict(X)\n    if binarize:\n        y_pred = np.where(model.predict(dtest)<0.5, 0,1)\n    legend = ['True ' + title, 'Predicted ' + title]\n    plt.hist([y, y_pred], color=['orange', 'green'])\n    plt.ylabel(\"Frequency\")\n    plt.legend(legend)\n    plt.title('True vs- predicted ' + title)\n    plt.show()\n\ndef predict_and_store(test, train, model, dtvalues):\n    \n    #predicts values and stores them as target values to test dataframe \n    y_pred =  model.predict(dtvalues)\n    test.loc[:, target] = y_pred\n    df = pd.concat([train, test])\n    df.sort_values('PassengerId', inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    return(df)","693026a2":"def one_value(gridsearch_params,\n              small_better,\n              param_a,\n              params,\n              dvalue,\n              metrics,\n              early_stopping_rounds,\n              Skfold=True,\n              verbose=False):\n    \n    \"\"\"This searches the best hyperparameter for selected hyperparameter\n    \n\n    Args:\n        gridsearch_params: hyperparameter-values to be evaluated \n        small_better:  if small value is searched = true\n        param_a: name of the hyperparameter\n        params: parameters dictionary\n        dtrain: traindata\n        metrics: the metric that defines the score\n        num_boost_round: max evaluation round \n        Skfold: id used True\n        verbose: how much data is showed    \n    \n    Returns:\n        parameter dictionary\n\n    \"\"\"\n\n    \n    \n    if small_better == True:\n        result_best = float(999999)\n    else:\n        result_best = float(-999999)\n    best_params = None\n\n    for i in gridsearch_params:\n        # Update our parameters\n        if verbose:\n            print(\"xgb.cv with {}={}\".format(param_a, i))\n        params[param_a] = i\n  \n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dvalue,\n            nfold =3,\n            stratified=Skfold,\n            early_stopping_rounds=early_stopping_rounds)\n        \n        # Update best result\n        result_col = \"test-\" + metrics + \"-mean\"\n\n\n        if small_better == True:\n            result = cv_results[result_col].min()\n            boost_rounds = cv_results[result_col].argmin()\n            if result < result_best:\n                result_best = result\n                best_params = i\n        else:\n            result = cv_results[result_col].max()\n            boost_rounds = cv_results[result_col].argmin()\n            if result > result_best:\n                result_best = result\n                best_params = i\n        if verbose:\n            print(\"xgb.cv {} {} for {} rounds\".format(metrics, result,  boost_rounds))\n        \n    print(\"Best xgb.cv params: {} {}, {}: {}\".format(param_a, best_params, metrics, result_best))\n    params[param_a] = best_params\n    return(params)\n","0708610b":"def two_values(gridsearch_params,\n               small_better,\n               param_a,\n               param_b,\n               params,\n               dvalue,\n               metrics,\n               early_stopping_rounds,\n               Skfold=True,\n               verbose=False):\n\n    \"\"\"This searches the best hyperparameter for the two selected hyperparameters\n    \n\n    Args:\n        gridsearch_params: hyperparameter-values to be evaluated \n        small_better:  if small value is searched = true\n        param_a: name of the first hyperparameter\n        param_b: name of the second hyperparameter\n        params: parameters dictionary\n        dtrain: traindata\n        metrics: the metric that defines the score\n        num_boost_round: max evaluation round \n        Skfold: id used True\n        verbose: how much data is showed    \n    \n    Returns:\n        parameter dictionary\n\n    \"\"\"\n    \n    if small_better == True:\n        result_best = float(999999)\n    else:\n        result_best = float(-999999)\n    best_params = None\n\n    for i, j in gridsearch_params:\n        # Update our parameters\n \n        if verbose:\n            print(\"xgb.cv with {}={}, {}={}\".format(param_a, i, param_b, j))\n        params[param_a] = i\n        params[param_b] = j\n  \n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dvalue,\n            nfold =3,\n            stratified=Skfold,\n            early_stopping_rounds=early_stopping_rounds)\n       \n        # Update best result\n        result_col = \"test-\" + metrics + \"-mean\"\n    \n\n        if small_better == True:\n            result = cv_results[result_col].min()\n            boost_rounds = cv_results[result_col].argmin()\n            if result < result_best:\n                result_best = result\n                best_params = (i,j)\n        else:\n            result = cv_results[result_col].max()\n            boost_rounds = cv_results[result_col].argmax()\n            if result > result_best:\n                result_best = result\n                best_params = (i,j)\n        if verbose:\n            print(\"xgb.cv {} {} for {} rounds\".format(metrics, result, boost_rounds))\n        \n    print(\"Best xgb.cv params: {} {}, {} {}, {}: {}\".format(param_a, best_params[0], param_b, best_params[1], metrics, result_best))\n    \n    params[param_a] = best_params[0]\n    params[param_b] = best_params[1]\n    return(params)","a06c8a3c":"def hyperparameter_grid(params,\n                        dtrain,\n                        metrics,\n                        watclist,\n                        testing=True,\n                        Skfold=False,\n                        Verbose=False):\n    \n    \"\"\"This function finds the optimum hyperparameters with a loop\n    \n\n    Args:\n        params: parameter dictionary\n        dtrain: data\n        dtest: data\n        metrics: the optimization metrics {'auc'}\n        num_boost_round: the num_boost running value\n        testing: sets variables lighter when true\n        num_boost_round: max evaluation round \n        Skfold: id used True\n        verbose: how much data is showed    \n    \n    Returns:\n        trainde and optimized model\n    \"\"\"\n    num_boost_round = 2000\n    early_stopping_rounds = 10\n    \n    if Verbose == False:\n        verbose_eval = num_boost_round\n    else:\n        verbose_eval = 10\n    \n    model = xgb.train(\n        params,\n        dtrain,\n        verbose_eval=verbose_eval,\n        num_boost_round = num_boost_round,\n        early_stopping_rounds = early_stopping_rounds,\n        evals=watchlist,\n    )\n    #for testing purposes a light set to save some time\n    if testing:\n        rounds=1\n        print('testing')\n        gridsearch_params_tree = [\n            (i, j)\n            for i in range(1,8)\n            for j in range(1,5)\n            ]\n        gridsearch_params_0_1 = [i\/5. for i in range(0,6)]\n        gridsearch_params_0_1_deep = [i\/5. for i in range(0,6)]\n        gridsearch_params_gamma = [i\/5. for i in range(0,26)]\n        \n        gridsearch_params_pair_0_1 = [\n            (i0, i1)\n            for i0 in gridsearch_params_0_1\n            for i1 in gridsearch_params_0_1\n            ]\n    else: #for real\n        rounds=3\n        print('for real')\n        gridsearch_params_tree = [\n            (i, j)\n            for i in range(1,20)\n            for j in range(1,20)\n            ]\n        gridsearch_params_0_1 = [i\/20. for i in range(0,21)]\n        gridsearch_params_0_1_deep = [i\/50. for i in range(0,51)]\n        gridsearch_params_gamma = [i\/50. for i in range(0,251)]\n        gridsearch_params_pair_0_1 = [\n            (i0, i1)\n            for i0 in gridsearch_params_0_1_deep\n            for i1 in gridsearch_params_0_1_deep\n            ]\n    \n    dvalue = dtrain\n    result_col = \"test-\" + metrics + \"-mean\"\n    cv_results = xgb.cv(\n            params,\n            dvalue,\n            stratified=Skfold,\n            metrics= metrics\n    )\n   \n    print(\"Start with xgb.cv params: {}: {}\".format(metrics, cv_results[result_col].min()))\n\n\n    #Tries to do semi-automatic genetic model for hyperparameter selection\n    for round in range(rounds):\n    \n        #Maximum depth\/height of a tree\n        #Minimum sum of instance weight (hessian) needed in a child\n        param_a = 'max_depth'\n        param_b = 'min_child_weight'\n        params=two_values(gridsearch_params_tree, True, param_a, param_b, params, dvalue, metrics, early_stopping_rounds,  Skfold, Verbose)\n\n\n        #Gamma finds minimum loss reduction\/min_split_loss required to make a further partition \n        param_a = 'gamma'\n        paramns=one_value(gridsearch_params_gamma, True, param_a, params, dvalue, metrics, early_stopping_rounds,  Skfold, Verbose)\n    \n\n        #L1 regularization term on weights - alpha  - Lasso Regression \n        #adds \u201cabsolute value of magnitude\u201d of coefficient as penalty term to the loss function.\n        #L2 regularization term on weights - lambda  - Ridge Regression \n        #adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function.\n        #the sample is so small, so most propably no effect\n        param_a = 'lambda'\n        param_b = 'alpha'\n        params=two_values(gridsearch_params_pair_0_1 , True, param_a, param_b, params, dvalue, metrics, early_stopping_rounds, Skfold, Verbose)\n\n\n        #Subsamble denotes the fraction of observations to be randomly samples for each tree.\n        #Colsample_bytree enotes the fraction of columns to be randomly samples for each tree.\n        param_a = 'colsample_bytree'\n        param_b = 'subsample'\n        params=two_values(gridsearch_params_pair_0_1, True, param_a, param_b, params, dvalue, metrics,early_stopping_rounds, Skfold, Verbose)\n    \n        #Same as learning_rate - this needs to be in sync with num_boost_round (alias n_tree parameter)\n        param_a = 'eta'\n        paramns=one_value(gridsearch_params_0_1_deep, True, param_a, params, dvalue, metrics,early_stopping_rounds,  Skfold, Verbose)\n        \n        #Balance of positive and negative weights.  This is regression and binary classification only parameter.\n        if params['objective'].startswith('reg'):\n            param_a = 'scale_pos_weight'\n            paramns=one_value(gridsearch_params_0_1_deep, True, param_a, params, dvalue, metrics, early_stopping_rounds, Skfold, Verbose)\n\n\n    print('Found hyperparameters with {} rounds '.format(round+1))\n    print(params)\n    print()\n    \n    model = xgb.train(\n        params,\n        dtrain,\n        verbose_eval=verbose_eval,\n        evals=watchlist,\n        num_boost_round = num_boost_round,\n        early_stopping_rounds = early_stopping_rounds,\n        )\n        \n    num_boost_round = model.best_iteration + 1\n\n    best_model = xgb.train(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        verbose_eval=verbose_eval,\n        evals=watchlist)\n        \n    return(best_model)","ca25cb7a":"# The real work begins with reading the data\n\ntarget = 'Survived'\n#train = pd.read_csv('train.csv')\n#test = pd.read_csv('test.csv')\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Combine the train and test data into one file for some preprocessing\ndf = pd.concat([train, test])","70e8c3e1":"#what kind of nulls are there\n#cabin and age are worst\ntrain.isnull().sum().sort_values(ascending=False)","d3234dca":"test.isnull().sum().sort_values(ascending=False)","33641a63":"#Cabin has lots of null values\n#Embarked and fare only a few","83b75a2f":"#Some adversialvalidation after feature enigeering between train and test\n\n#Define which columns should be encoded vs scaled and what columns are binary\ntrain_no_na = train.dropna().copy()\ntest_no_na = test.dropna().copy()\nnumeric_features  = ['Age', 'Pclass', 'Fare', 'Parch', 'SibSp']\ntrain_scaled, test_scaled, features = scale_data(train_no_na, test_no_na, numeric_features  = numeric_features)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(17, 5))\nax1.set_ylim([0, 1.3])\nax1.set_title('Scaled train')\nsns.kdeplot(train_scaled['Age'], ax=ax1)\nsns.kdeplot(train['Pclass'], ax=ax1)\nsns.kdeplot(train_scaled['Fare'], ax=ax1)\nsns.kdeplot(train_scaled['Parch'], ax=ax1)\nsns.kdeplot(test_scaled['SibSp'], ax=ax1)\n\nax2.set_title('Scaled test')\nax2.set_ylim([0, 1.3])\nsns.kdeplot(test_scaled['Age'], ax=ax2)\nsns.kdeplot(test['Pclass'], ax=ax2)\nsns.kdeplot(test_scaled['Fare'], ax=ax2)\nsns.kdeplot(test_scaled['Parch'], ax=ax2)\nsns.kdeplot(test_scaled['SibSp'], ax=ax2)\n\nplt.show()","abf08731":"df[df['Embarked'].isnull()]","ed6614c9":"#Embarked null values all to Southampton i.e. S value, was the largest embarkement harbor anyway \ncolumn = \"Embarked\"\ndf[column].fillna(df[column].value_counts().index[0], inplace=True)","41cd2bd2":"df['Ticket'].head(5)","f8126350":"#from ticket the ticket number and type can be separated\n\ntitle = 'Ticket numbers'\ncolumn = 'Ticket'\n\ndef cleanTicket(ticket):\n        ticket = ticket.upper()\n        ticket = ticket.replace('.','')\n        ticket = ticket.replace('\/','')\n        ticket = ticket.split()\n        if len(ticket) >= 2:\n            return pd.Series([ticket[0][:3], int(ticket[-1])])\n        else:\n            if ticket[0].isdigit():\n                return pd.Series([\"000\", int(ticket[0])])\n            else:\n                return pd.Series([\"000\", 0])\n        \ncategory = column + \"_type_categories\"\ndf[category]=df[column].apply(cleanTicket)[0]\n\ntlist = df[category].value_counts().head(2).index.tolist()\ndf.loc[df[category].isin(tlist) == False,category] = '000'\n\ncategory = column + \"_number\"\ndf[category]=df[column].apply(cleanTicket)[1]","94b40a4c":"#Name to titles and to families\ntitles = {\n        \"Mr\" :         \"Mr\",\n        \"Mme\":         \"Mrs\",\n        \"Ms\":          \"Mrs\",\n        \"Mrs\" :        \"Mrs\",\n        \"Master\" :     \"Master\",\n        \"Mlle\":        \"Miss\",\n        \"Miss\" :       \"Miss\",\n        \"Capt\":        \"Officer\",\n        \"Col\":         \"Officer\",\n        \"Major\":       \"Officer\",\n        \"Dr\":          \"Officer\",\n        \"Rev\":         \"Officer\",\n        \"Jonkheer\":    \"Royalty\",\n        \"Don\":         \"Royalty\",\n        \"Sir\" :        \"Royalty\",\n        \"Countess\":    \"Royalty\",\n        \"Dona\":        \"Royalty\",\n        \"Lady\" :       \"Royalty\"\n        }\n\ncolumn = \"Name\"\ncategory = column + \"_title_categories\"\nextracted_titles = df[column].str.extract(' ([A-Za-z]+)\\.',expand=False)    \ndf[category] = extracted_titles.map(titles)\n\n#There may be passengers in train and test dataset from the same lastname\n#So the formulation of last names needs to be done before train\/test splits\ncategory = 'Lastname' \ndf[category] = df[column].str.split(',').str[0]","9f85e8e2":"#Separate sets\ntarget = 'Survived'\ntrain=df[(df[target].notnull())].copy()\ntest=df[(df[target].isnull())].copy()\n#now on we will show training dataframe","7d85286f":"pd.crosstab(train['Embarked'],train['Survived'], normalize='index').plot.bar(stacked=True, title='Embarkment')","ec3f8ff7":"pd.crosstab(train['Sex'],train['Survived'], normalize='index').plot.bar(stacked=True, title='Sex')","a67d296f":"fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(17, 12))\nFemale = train[train['Sex']=='female']\nMale = train[train['Sex']=='male']\nax = sns.distplot(Female[Female['Survived']==1].Age, label = \"OK\", ax = axes[0], kde =False)\nax = sns.distplot(Female[Female['Survived']==0].Age, label = \"Dead\", ax = axes[0], kde =False)\nax.set_title('F')\nax = sns.distplot(Male[Male['Survived']==1].Age, label = \"OK\", ax = axes[1], kde = False)\nax = sns.distplot(Male[Male['Survived']==0].Age, label = \"Dead\", ax = axes[1], kde = False)\nax.set_title('M')\nplt.show()","a93f9c09":"#figure out and plot some info based on Pclass, Age and survival\nfig, axes = plt.subplots(nrows=3, ncols=2, sharey=True,sharex=True, figsize=(14, 14))\n\nax = sns.distplot(Female[Female['Pclass']==1]['Age'], label = \"1\", ax = axes[0,0], kde =False)\nax = sns.distplot(Female[Female['Pclass']==2]['Age'], label = \"2\", ax = axes[0,0], kde =False)\nax = sns.distplot(Female[Female['Pclass']==3]['Age'], label = \"3\", ax = axes[0,0], kde =False)\nax.set_title('Female Pclass')\nax = sns.distplot(Male[Male['Pclass']==1]['Age'], label = \"1\", ax = axes[0,1], kde = False)\nax = sns.distplot(Male[Male['Pclass']==2]['Age'], label = \"2\", ax = axes[0,1], kde = False)\nax = sns.distplot(Male[Male['Pclass']==3]['Age'], label = \"3\", ax = axes[0,1], kde = False)\nax.set_title('Male Pclass')\n\nax = sns.distplot(Female[(Female['Pclass']==1) & (Female['Survived']==1)]['Age'], label = \"1\", ax = axes[1,0], kde =False)\nax = sns.distplot(Female[(Female['Pclass']==2) & (Female['Survived']==1)]['Age'], label = \"2\", ax = axes[1,0], kde =False)\nax = sns.distplot(Female[(Female['Pclass']==3) & (Female['Survived']==1)]['Age'], label = \"3\", ax = axes[1,0], kde =False)\nax.set_title('Female Pclass Survived')\nax = sns.distplot(Male[(Male['Pclass']==1) & (Male['Survived']==1)]['Age'], label = \"1\", ax = axes[1,1], kde = False)\nax = sns.distplot(Male[(Male['Pclass']==2) & (Male['Survived']==1)]['Age'], label = \"2\", ax = axes[1,1], kde = False)\nax = sns.distplot(Male[(Male['Pclass']==3) & (Male['Survived']==1)]['Age'], label = \"3\", ax = axes[1,1], kde = False)\nax.set_title('Male Pclass Survived')\n\n\nax = sns.distplot(Female[(Female['Pclass']==1) & (Female['Survived']==0)]['Age'], label = \"1\", ax = axes[2,0], kde =False)\nax = sns.distplot(Female[(Female['Pclass']==2) & (Female['Survived']==0)]['Age'], label = \"2\", ax = axes[2,0], kde =False)\nax = sns.distplot(Female[(Female['Pclass']==3) & (Female['Survived']==0)]['Age'], label = \"3\", ax = axes[2,0], kde =False)\nax.set_title('Female Pclass Not survived')\nax = sns.distplot(Male[(Male['Pclass']==1) & (Male['Survived']==0)]['Age'], label = \"1\", ax = axes[2,1], kde = False)\nax = sns.distplot(Male[(Male['Pclass']==2) & (Male['Survived']==0)]['Age'], label = \"2\", ax = axes[2,1], kde = False)\nax = sns.distplot(Male[(Male['Pclass']==3) & (Male['Survived']==0)]['Age'], label = \"3\", ax = axes[2,1], kde = False)\nax.set_title('Male Pclass Not survived')\n\n\nplt.show()","7089a4dc":"pd.crosstab(df['Ticket_type_categories'],df['Survived'], normalize='index').plot.bar(stacked=True, title='Ticket type')","001b6ad9":"pd.crosstab(df['Ticket_type_categories'],df['Cabin'].str[0])","a69b72b2":"#The question is if this all should be done after train test split\n#There may be passengers in train and test dataset from the same ticket or lastname\n#So, the calculation of family size needs to be done before any train\/test splitting\n#Therefore, train and test will be joined again to one dataframe\ndf = pd.concat([train, test])","1932c547":"#count family size and create it as a categoric column from SibSp and Parch columns \n\ncolumn1 = \"SibSp\"\ncolumn2 = \"Parch\"\ncategory = \"SibSP_Parch_family_categories\"\nmiddle_category_max = 4\n\ndf['SibSP_Parch_familysize'] = df[column1] + df[column2] + 1\ndf[category] = np.where(df['SibSP_Parch_familysize']==1,1, \n               np.where(df['SibSP_Parch_familysize'].between(1, middle_category_max, inclusive=True), 2, 3))    \n\ndf['Parch_family_or_not'] = np.where(df[column2]==0,0,1)\n\ndef calculate_familygroup(df,column):\n    \n    familysize = column + '_familysize'\n    df[familysize] = df.groupby(column)[column].transform('count')\n    category = column + \"_family_categories\"\n    df[category] = np.where(df[familysize]==1,1, \n                np.where(df[familysize].between(1, middle_category_max, inclusive=True), 2, 3))  \n\n    return(df)\n\ndf=calculate_familygroup(df,'Lastname').copy()\ndf=calculate_familygroup(df,'Ticket_number').copy()\n","ce301eb8":"#There may be passengers in train and test dataset from the same ticket\n#Due that the Fare_per_person calculation needs to be done for the whole dataset\n\n#Fare per person for same ticket holders. It is obvious that the price paid for all tickets\n#is set to all ticket holders,so it needs to be set toright median\/average vaues\n\ndf['Fare_per_person'] = df['Fare']\/(df['Ticket_number_familysize']+1)\n    \n#Only one missing fare Storey, Mr. Thomas, 3rd class passanger\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\nmed_fare = df.groupby(['Pclass', 'Ticket_number_familysize'])['Fare_per_person'].median()[13]\ndf.loc[df['Fare'].isna(),'Fare_per_person'] = med_fare\n\n#there are 0 fare passangers. They can be free riders or thr fare can be an error. In this case the best might be to set them look like other in the Pclass\nmed_fare = df.groupby(['Pclass', 'Ticket_number_familysize'])['Fare_per_person'].median()[0]\ndf.loc[(df['Fare']==0) & (df['Pclass']==1),'Fare_per_person'] = med_fare\n\nmed_fare = df.groupby(['Pclass', 'Ticket_number_familysize'])['Fare_per_person'].median()[7]\ndf.loc[(df['Fare']==0) & (df['Pclass']==2),'Fare_per_person'] = med_fare\n\nmed_fare = df.groupby(['Pclass', 'Ticket_number_familysize'])['Fare_per_person'].median()[13]\ndf.loc[(df['Fare']==0) & (df['Pclass']==3),'Fare_per_person'] = med_fare","02d0e859":"pd.crosstab(train['Name_title_categories'],train['Survived'], normalize='index').plot.bar(stacked=False, title='Titles')","0db6c146":"pd.crosstab(train['Name_title_categories'],train['Survived'])","2dac240b":"#The question is if this all should be done after train test split\n#There may be passengers in train and test dataset from the same ticket or lastname\n\ndef find_families(df):\n    #this function finds lastnames and if they have the same fare_per_person price\n    #they are considered to be one family. Pclass is not enough as a separator\n    \n    values = []\n    for (name, group) in df.groupby('Lastname'):\n        #common_pclass finds the Pclass that has most values in this Lastname group\n        same_fare = group['Fare_per_person'].value_counts().keys().tolist()[0]\n\n        #if the person is not in this group add it to values list\n        values.extend(group[group['Fare_per_person'] != same_fare]['PassengerId'].to_list())\n    return(values)\n\nvalues = find_families(df)\nwhile len(values) > 0:\n    #if same lastname add_x to the end\n    df.loc[df['PassengerId'].isin(values), 'Lastname'] = df[df['PassengerId'].isin(values)]['Lastname'].apply(lambda x: x + \"_X\")\n    values = find_families(df)","77ffc5ac":"#Separate all again to train and test\ntarget = 'Survived'\ntrain=df[(df[target].notnull())].copy()\ntest=df[(df[target].isnull())].copy()","87c4c9d1":"pd.crosstab(train[train['Ticket_number_familysize']<=3]['Ticket_number_familysize'],train[train['Ticket_number_familysize']<=3]['SibSP_Parch_familysize'], normalize='index').plot.bar(stacked=False, title='Family size')","8dd38297":"pd.crosstab(train[train['Lastname_familysize']<=3]['Lastname_familysize'],train[train['Lastname_familysize']<=3]['SibSP_Parch_familysize'], normalize='index').plot.bar(stacked=False, title='Family size')","5240caed":"pd.crosstab(train[train['Ticket_number_familysize']>3]['Ticket_number_familysize'],train[train['Ticket_number_familysize']>3]['SibSP_Parch_familysize'], normalize='index').plot.bar(stacked=False, title='Family size')","088a488f":"pd.crosstab(train[train['Lastname_familysize']>3]['Lastname_familysize'],train[train['Lastname_familysize']>3]['SibSP_Parch_familysize'], normalize='index').plot.bar(stacked=False, title='Family size')","371874e8":"#https:\/\/en.wikipedia.org\/wiki\/Master_(form_of_address)\n#it is possible to set some average value to masters without age\n\nfig, ax = plt.subplots(nrows=1, ncols=1,figsize=(17, 7))\nmaster = train[train['Name_title_categories']=='Master']\nax = sns.distplot(master[master['Survived']==1].Age, label = \"OK\", kde =False)\nax = sns.distplot(master[master['Survived']==0].Age, label = \"Dead\", kde =False)\nax.set_title('Master \/ Survival')\nplt.show()","b5d463dd":"print(train[(train['Name_title_categories']=='Master') & (train['Age']<=18)][['Parch', 'Pclass','Survived']].describe()[0:3])\nprint(train[(train['Name_title_categories']=='Mr') & (train['Age']<=18)][['Parch', 'Pclass','Survived']].describe()[0:3])","023483cf":"print(train[(train['Name_title_categories']=='Miss') & (train['Age']<=18)][['Parch', 'Pclass','Survived']].describe()[0:3])\nprint(train[(train['Name_title_categories']=='Mrs') & (train['Age']<=18)][['Parch', 'Pclass','Survived']].describe()[0:3])","049bae10":"fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(17, 12))\nMaster = train[(train['Name_title_categories']=='Master') & (train['Age']<=18)]\nMr = train[(train['Name_title_categories']=='Mr') & (train['Age']<=18)]\nax = sns.distplot(Master[Master['Survived']==1].Age, label = \"OK\", ax = axes[0], kde =False)\nax = sns.distplot(Master[Master['Survived']==0].Age, label = \"Dead\", ax = axes[0], kde =False)\nax.set_title('F')\nax = sns.distplot(Mr[Mr['Survived']==1].Age, label = \"OK\", ax = axes[1], kde = False)\nax = sns.distplot(Mr[Mr['Survived']==0].Age, label = \"Dead\", ax = axes[1], kde = False)\nax.set_title('M')\nplt.show()","43dda42a":"grouped_train = df.groupby(['Parch_family_or_not', 'Name_title_categories']).agg(['min', 'max', 'median', 'std', 'count'])\n\ngrouped_median_train = grouped_train\ngrouped_median_train = grouped_median_train.reset_index()[['Parch_family_or_not', 'Name_title_categories', 'Age']]\nprint(grouped_median_train)","902f1ea1":"#Predict age\ntarget = 'Age'\nnumeric_features = []\ncategorical_features = ['Pclass', 'Name_title_categories','Parch_family_or_not']\n\nfeatures = numeric_features + categorical_features\n\n#get data from original a bit preprocessed dataframe\ny = df[df[target].notnull()][target].copy()\ntrain = df[df[target].notnull()][features].copy()\ntest  = df[df[target].isnull()][features].copy()\n\n\n#set values to feature engineering attributes\n\n#Check\n#k_selected need to have correct values, if they are too large, evrything needs to be started again\n#'all' means all\nk_selected = 'all'\ntest_size = 0.2\n\n#Scalers attributes\nscaler = MinMaxScaler()\n#set values to model configuration attributes\nkbest_score_func = mutual_info_regression \n\n\nmetric = 'rmse'\nSkfold=False\nVerbose = False\ntesting=False\n\nparams = {\n    #Initial xgboost parameters to be automatically tuned\n    'objective':'reg:squarederror',\n    'booster' : 'gbtree',\n    'eval_metric' : metric\n    } \n","1b8e57a7":"#prepare data scale, onehot encoding etc.\ntr_train, tr_test, features = scale_data(train, test, numeric_features, categorical_features, scaler)\n\nfeatures_df = select_kbest(tr_train[features], y, kbest_score_func,k_selected)\nselected_columns =  features_df[features_df['Selected']]['Column'].tolist()\ntr_train = tr_train[selected_columns].copy()\ntr_test = tr_test[selected_columns].copy()\n\nX_train, X_test, y_train, y_test = train_test_split(tr_train, y, test_size=test_size) \ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\ndtest = xgb.DMatrix(X_test, label=y_test)\ndtvalues = xgb.DMatrix(tr_test)\n\nwatchlist = [(dtrain, 'train'), (dtest, 'test')]","9571ed66":"#draw feature importance based on kBest\ntitle = 'Features kbest ' + target\ndraw_features(features_df[features_df['Selected']==True]['Score'], features_df[features_df['Selected']==True]['Column'], title)","19704435":"#configure and finetune model\nmodel = hyperparameter_grid(params, dtrain, metric, watchlist, testing, Skfold, Verbose)    ","e36133a4":"#draw feature importance based on xgboost\nxgb.plot_importance(model)","02b360e5":"draw_true_vs_predicted(dtest, y_test, model, target)","992c7189":"#Impute the new found values of age\ntrain = df[df[target].notnull()].copy()\ntest  = df[df[target].isnull()].copy()\ndf=predict_and_store(test, train, model, dtvalues)","48cd6665":"child_age = 16\ndip1=22\ndip2=35\ndip3=42\n\n\ndef get_person(person):\n    age = person[0]\n    sex = person[1]\n    name_title = person[2]\n    if (age <= child_age) & (name_title != 'Mr'): \n        val = 'Child'\n    if (age <= child_age) & (name_title == 'Mr'): \n        val = 'Male_adult_young'\n    if (age > child_age) & (age <= dip1) & (sex == 'male'): \n        val = 'Male_adult_young'\n    if (age > dip1) & (age <= dip2) & (sex == 'male'): \n        val = 'Male_adult_middle'\n    if (age > dip2) & (age <= dip3) & (sex == 'male'): \n        val = 'Male_adult_middle'\n    if (age > dip3) & (sex == 'male'): \n        val = 'Male_adult_old'\n    if (age > child_age) & (age <= dip1) & (sex == 'female'): \n        val = 'Female_adult_young'\n    if (age > dip1) & (age <= dip2) & (sex == 'female'): \n        val = 'Female_adult_middle'\n    if (age > dip2) & (age <= dip3) & (sex == 'female'): \n        val = 'Female_adult_middle'\n    if (age > dip3) & (sex == 'female'): \n        val = 'Female_adult_old'\n    return val\ndf['Family_title'] = df[['Age', 'Sex', 'Name_title_categories']].apply(get_person, axis=1)\n\n#Separate all again to train and test\ntarget = 'Survived'\ntrain=df[(df[target].notnull())].copy()\ntest=df[(df[target].isnull())].copy()\n\n\npd.crosstab(train[train['Pclass']==1]['Family_title'],train[train['Pclass']==1]['Survived']).plot.bar(stacked=False, title='Titles\/Pclass 1')\npd.crosstab(train[train['Pclass']==2]['Family_title'],train[train['Pclass']==2]['Survived']).plot.bar(stacked=False, title='Titles\/Pclass 2')\npd.crosstab(train[train['Pclass']==3]['Family_title'],train[train['Pclass']==3]['Survived']).plot.bar(stacked=False, title='Titles\/Pclass 3')\n\n","c1ba5943":"def create_groupvalue(df, column, mask, col_name):\n    #create new columm col_name based on calculations of a column like lastname\n    #based on conditions described in mask\n\n    \n    temp_group_df = pd.DataFrame(df[column].value_counts())    \n    temp_group_df.reset_index(inplace=True)\n    temp_group_df.columns=[column, 'number']\n    temp_group_df.drop('number', axis=1, inplace=True)\n    temp_group_df.sort_values(column).reset_index(drop=True, inplace=True)\n\n    col_name = column + '_' + col_name\n\n    temp_df = pd.DataFrame(df[column][mask].value_counts()).reset_index()\n    temp_df.columns=[column, col_name]\n    temp_df.sort_values(column).reset_index(drop=True, inplace=True)\n    \n    temp_group_df = pd.merge(temp_group_df, temp_df, on=column,how='left')\n    temp_group_df[col_name].fillna(0, inplace=True)\n    temp_group_df.loc[temp_group_df[col_name] > 0, col_name] = 1\n     \n    df=pd.merge(df, temp_group_df, on=column,how='left')\n    df.reset_index(drop=True, inplace=True)\n    return(df)\n\ndef create_familygroup(df,column):\n    #create calculated colums\n    familysize = column + '_familysize'\n    familysize = 'SibSP_Parch_familysize'\n    \n\n    mask = (df['Family_title'].str.startswith('Female_adult')) & (df['Survived'] == 1) & (df[familysize] > 1)\n    df=create_groupvalue(df, column, mask, 'Survived_Female').copy()\n\n    mask = (df['Family_title'].str.startswith('Male_adult')) & (df['Survived'] == 1) & (df[familysize] > 1)\n    df=create_groupvalue(df, column, mask, 'Survived_Male').copy()\n\n    mask = (df['Family_title'] == 'Child') & (df['Survived'] == 1) & (df[familysize] > 1)\n    df=create_groupvalue(df, column, mask, 'Survived_Child').copy()\n\n    return(df)\n\ndf =create_familygroup(df,'Lastname').copy()\ndf =create_familygroup(df,'Ticket_number').copy()\n\n#Separate all again to train and test\ntarget = 'Survived'\ntrain=df[(df[target].notnull())].copy()\ntest=df[(df[target].isnull())].copy()\n\npd.crosstab(train['Ticket_number_Survived_Female'],train['Ticket_number_Survived_Male']).plot.bar(stacked=False, title='Same ticket')\npd.crosstab(train['Ticket_number_Survived_Female'],train['Ticket_number_Survived_Child']).plot.bar(stacked=False, title='Same ticket')\npd.crosstab(train['Ticket_number_Survived_Male'],train['Ticket_number_Survived_Female']).plot.bar(stacked=False, title='Same ticket')\npd.crosstab(train['Ticket_number_Survived_Child'],train['Ticket_number_Survived_Female']).plot.bar(stacked=False, title='Same ticket')","bee34835":"pd.crosstab(train['Cabin'].str[0],train['Survived']).plot.bar(stacked=False, title='Cabin')\n","cfda71f4":"#find values for deck for customers having already cabin number and customers without it\ncolumn = \"Cabin\" \ntarget = column  + \"_deck\" #target\n\n\n#deck =  {\"A\": 1, \"B\": 2, \"C\": 2, \"D\": 3, \"E\": 3, \"F\": 4, \"G\": 4, \"Y\": 0, \"T\": 1}\n\ndeck =  {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"Y\": 0, \"T\": 1}\n\n#So few in class G, combine it with F\n\n#Figure out Cabin categories, fill Na to \"Y\", it will always be the min value\n#if category not available find it from other members of the family, if possible\ndf[column] = df[column].fillna(\"Y\")\ndf[target] = df[column].str[0].map(deck)\n\nnumeric_features = ['Fare_per_person']\ncategorical_features = ['Pclass',  'Ticket_type_categories']\nfeatures = numeric_features + categorical_features \n\n#get data from original a bit preprocessed dataframe\n#note now the empty value is 0\ny = df[df[target]!=0][target].copy()\ntrain = df[df[target]!=0][features].copy()\ntest  = df[df[target]==0][features].copy()\n\n\n#set values to feature engineering attributes\n\n#Check\n#k_selected need to have correct values, if they are too large, evrything needs to be started again\nk_selected = 'all'\n\ntest_size = 0.3\n\n#Scalers attributes\nscaler = MinMaxScaler()\nkbest_score_func = mutual_info_classif\n#set values to model configuration attributes\n\nnum_class = len(df['Cabin_deck'].unique()) + 1 #how many classes we are working with\n\nmetric = 'mlogloss'\nSkfold=False\nVerbose = False\ntesting=False\n\nparams = {\n    # Parameters that we are going to tune.\n    'objective':'multi:softmax',\n    'num_class' : num_class,\n    'booster' : 'gbtree',\n    'eval_metric' : metric\n} \n","0a40b634":"#prepare data scale, onehot encoding etc.\ntr_train, tr_test, features = scale_data(train, test, numeric_features, categorical_features, scaler)\n\nfeatures_df = select_kbest(tr_train[features], y, kbest_score_func,k_selected)\nselected_columns =  features_df[features_df['Selected']]['Column'].tolist()\ntr_train = tr_train[selected_columns].copy()\ntr_test = tr_test[selected_columns].copy()\n\n#split the initial train dataframe to test\/train dataframes\nX_train, X_test, y_train, y_test = train_test_split(tr_train, y, test_size=test_size) \n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\ndtest = xgb.DMatrix(X_test, label=y_test)\ndtvalues = xgb.DMatrix(tr_test)\n\nwatchlist = [(dtrain, 'train'), (dtest, 'test')]","bbb5654e":"#draw feature importance based on kBest\ntitle = 'Features kbest ' + target\ndraw_features(features_df[features_df['Selected']==True]['Score'], features_df[features_df['Selected']==True]['Column'], title)","49fbc70d":"#configure and finetune model\nmodel = hyperparameter_grid(params, dtrain,metric, watchlist, testing, Skfold, Verbose)    ","be1c7998":"#draw feature importance based on xgboost\nxgb.plot_importance(model)","52acd36d":"draw_true_vs_predicted(dtest, y_test, model, target)","f96a092f":"y_pred = model.predict(dtest)\nlower, upper = proportion_confint(accuracy_score(y_test, y_pred) * len(y_test), len(y_test), 0.05)\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\nprint(\"F1 Score: \", f1_score(y_test, y_pred, average=\"macro\"))\nprint(\"Precision Score: \", precision_score(y_test, y_pred, average=\"macro\"))\nprint(\"Recall Score: \", recall_score(y_test, y_pred, average=\"macro\"))\nprint('Confidence level lower=%.3f, upper=%.3f' % (lower, upper))","8c46ea0c":"#Impute the new found values of cabin\ntrain = df[df[target]!=0].copy()\ntest  = df[df[target]==0].copy()\ndf=predict_and_store(test, train, model, dtvalues)\n\n\ndeck =  {1 : \"A\", 2 : \"B\", 3 : \"C\", 4 : \"D\", 5 : \"E\", 6 : \"F\", 7 : \"G\"}\ndf[target] = df[target].map(deck)","e8e42f02":"#Some adversialvalidation after feature enigeering phase between train and test\n\n#Define which columns should be encoded vs scaled and what columns are binary\n\n#Separate all again to train and test\ntarget = 'Survived'\ntrain=df[(df[target].notnull())].copy()\ntest=df[(df[target].isnull())].copy()\n\n\nnumeric_features = ['Age', 'Fare_per_person', 'Lastname_familysize']\n\ntrain_scaled, test_scaled, features = scale_data(train, test, numeric_features=numeric_features)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(17, 5))\nax1.set_ylim([0, 2])\nax1.set_title('Scaled train')\nsns.kdeplot(train_scaled['Fare_per_person'], ax=ax1)\nsns.kdeplot(train['Pclass'], ax=ax1)\nsns.kdeplot(train_scaled['Age'], ax=ax1)\nsns.kdeplot(train_scaled['Lastname_familysize'], ax=ax1)\n\nax2.set_title('Scaled test')\nax2.set_ylim([0, 2])\nsns.kdeplot(test_scaled['Fare_per_person'], ax=ax2)\nsns.kdeplot(test['Pclass'], ax=ax2)\nsns.kdeplot(test_scaled['Age'], ax=ax2)\nsns.kdeplot(test_scaled['Lastname_familysize'], ax=ax2)\n\nplt.show()","fce2d7f7":"#Predict survival through xgboost machine learning algorithms\ntarget = 'Survived'\nnumeric_features = []\ncategorical_features = ['Family_title','Pclass','Ticket_number_family_categories','Embarked','Parch_family_or_not', 'Cabin_deck']\nfeatures = numeric_features + categorical_features\n\n#get data from orginal  a bit preprocessed dataframe\ny = df[df[target].notnull()][target].copy()\ntrain = df[df[target].notnull()][features].copy()\ntest  = df[df[target].isnull()][features].copy()\n\n#set values to feature engineering attributes \n\n\n#Check\n#k_selected need to have correct values, if they are too large, evrything needs to be started again\nk_selected = 'all'\ntest_size = 0.20\nscaler = MinMaxScaler()\nkbest_score_func = mutual_info_classif\n\nnum_boost_round = 2000\nmetric = 'error'\nSkfold=True\nVerbose = False\ntesting=False\n\n#Set values for model configuration\nparams = {\n    # Initial xgboost parameters to be tuned\n    'objective':'binary:logistic',\n    'booster' : 'gbtree',\n    'eval_metric' : metric\n} ","aa5c2177":"#prepare data scale, onehot encoding etc.\ntr_train, tr_test, features = scale_data(train, test, numeric_features, categorical_features, scaler)\n\nfeatures_df = select_kbest(tr_train[features], y, kbest_score_func,k_selected)\nselected_columns =  features_df[features_df['Selected']]['Column'].tolist()\ntr_train = tr_train[selected_columns].copy()\ntr_test = tr_test[selected_columns].copy()\n\nX_train, X_test, y_train, y_test = train_test_split(tr_train, y, test_size=test_size) \n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\ndtest = xgb.DMatrix(X_test, label=y_test)\ndtvalues = xgb.DMatrix(tr_test)\n\nwatchlist = [(dtrain, 'train'), (dtest, 'test')]","43b77c78":"#draw feature importance based on kBest\ntitle = 'Features kbest ' + target\ndraw_features(features_df[features_df['Selected']==True]['Score'], features_df[features_df['Selected']==True]['Column'], title)","7461163b":"#configure and finetune model\nmodel = hyperparameter_grid(params, dtrain, metric, watchlist, testing, Skfold, Verbose)    ","c2585b02":"#draw feature importance based on xgboost\nxgb.plot_importance(model)","40735030":"draw_true_vs_predicted(dtest, y_test, model, target, binarize=True)","7798d7a9":"draw_results(dtest, y_test, model)","657c4a53":"draw_confusion(dtest, y_test, model)","1dd53966":"train = df[df[target].notnull()].copy()\ntest  = df[df[target].isnull()].copy()\nsubmission=create_submission(test,model, dtvalues)","fa320c9a":"submission.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","23a74396":"On April 15, 1912, the Titanic sank during its maiden voyage after colliding with an iceberg. It killed 1,502 out of 2,224 passengers and crew members. There were not enough lifeboats for passengers and crew. Some groups of people were more likely to survive than others, such as Female, children, and the upper class.\n\nhttps:\/\/fi.wikipedia.org\/wiki\/Female_and_Children_First\n\nThe sinking of the Titanic is mostly remembered for that statement.","b90a04e9":"Assumptions and null hypotheses of this study:\n\nIt seems clear that \u201cFemale and children first\u201d is the key to survival. My null hypothesis is that it is the only significant factor.\nH0 = Female and children survive, and men die\nAn alternative hypothesis is that there are other factors. Other factors should make the forecast more accurate.\nH1 = Families survive, people with better socioeconomic status or better cabins survive.\n\nFirst-class passengers have a higher chance of survival than second-class passengers; second-class passengers had more opportunities than third-class passengers. I assume that passengers with a first-class ticket were as close to the deck (and thus lifeboats) as passengers living under the deck.\n\n1) was this due to the better location of the cabins? that is, closer to lifeboats? 2) was this due to the socio-economic situation, i.e. people\u2019s better access to lifeboats? 3) Some combination of those","5bd9d802":"So both male and female child survive","ffdca92a":"Now we have unique family names i.e groups","14e1ce22":"# Titanic\n","832e24a6":"Ticket categories seem to bring some information related to cabin and survival rate","eb3efb74":"Show results","fcaa4bdb":"# Create model to find age and impute null valued ages through xgboost machine learning algorithms","133c7bf7":"Embarket have had some effect to survival","ded340e7":"It seems that Lastname and Ticket number finds pairs better, compared to Parch and SibSP values ","f039be12":" Numeric Features:\n - age: float.\n - fare: float.\n - SibSp\t# of siblings \/ spouses aboard the Titanic\t- Interval\n - Parch\t# of parents \/ children aboard the Titanic\t- Intervall\n\nCategorical Features:\n - embarked: categories encoded as strings {'C', 'S', 'Q'}. \n - sex: categories encoded as strings {'female', 'male'}.\n - pclass: ordinal integers {1, 2, 3}.\n - Survived 0 = No, 1 = Yes - Binary\n\n\nVariable Notes\npclass: A proxy of ticket class\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\nTicket\tTicket number\t\nFare\tPassenger fare\t- Intervall\nCabin\tCabin number\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\nEmbarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n","a0b10fda":"# Start processing of data","f2b63867":"# Feature enigeering finalized","b75ccb96":"Show results and predict ages to null valued","159be4d5":"Not worth of beeing an adult male traveling in third class","db80028d":"It is different to be Mr vs Master as a boy. Here on we will describe young Mr's as male","074e7315":"In this workbook, my goal is to prove that I can get a top-score in the Kaggle Titanic race, Titanic: Machine Learning from Disaster.\n\nhttps:\/\/www.kaggle.com\/c\/titanic\n\nThe aim is to predict, using various statistical methods and data processing, whether the passenger survived or died during the sinking of the Titanic.\n\nI use Python to pipe in each level. The main libraries concerned are\n1) Pandas for data processing\n2) Sklearn for machine learning and predictive modeling\n3) Matplotlib and seaborn for data visualization","f0cd9785":"# Create model to find cabin_deck and impute null valued cabins through xgboost machine learning algorithms","e53f812a":"In mrs miss no difference","728af95b":"There are a couple of general functions that help in presenting, managing and classifying data.","069043b6":"# General functions","134ff68c":"# Create survival model, show results, predict and submit","b4c07a0d":"Very clear difference between female and male, as earlier discussed","69359f9f":"Se those two to the most common i.e. Southhampton"}}