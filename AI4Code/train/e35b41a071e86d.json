{"cell_type":{"69a16f27":"code","4b01bf03":"code","0fc939ff":"code","557aaac5":"code","87ac4678":"code","21ad2d8b":"code","d7c90186":"code","0e4632e0":"code","73b7cba0":"markdown","62dc7d4d":"markdown","71452fef":"markdown","37a94e20":"markdown","9fa598c4":"markdown"},"source":{"69a16f27":"import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport copy","4b01bf03":"df_train = pd.read_csv('..\/input\/student-shopee-code-league-sentiment-analysis\/train.csv')\ndf_test = pd.read_csv('..\/input\/student-shopee-code-league-sentiment-analysis\/test.csv')","0fc939ff":"def compare(df,rating1,rating2):\n    df = copy.deepcopy(df)\n    df = df[(df.rating==rating1)|(df.rating==rating2)]\n    df = df.drop_duplicates(subset=['review'])\n    # replacing values so we can easily get unbiased weights\n    df.loc[df['rating']==rating1,'rating'] = 0\n    df.loc[df['rating']==rating2,'rating'] = 1\n    df = df.sample(frac=1)\n    text_clf = Pipeline([\n                      ('vect',CountVectorizer(ngram_range=(1,3),min_df=5,binary=True)),\n                      ('clf',LogisticRegression(max_iter=1000))]\n\n    )\n    text_clf.fit(df.review.values,df.rating.values)\n    coef = text_clf.named_steps['clf'].coef_ #this is w\n    vect = text_clf.named_steps['vect'].get_feature_names() # this is x\n    x = np.argsort(coef[0])[-10:][::-1]#argsort descending order (taking top 10 results with largest weights)\n    for i in x:\n        print(vect[i])","557aaac5":"compare(df_train,4,5)","87ac4678":"compare(df_train,1,5)","21ad2d8b":"compare(df_train,3,4)","d7c90186":"compare(df_train,1,3)","0e4632e0":"compare(df_train,2,3)","73b7cba0":"# Words that represent 3 over 1","62dc7d4d":"# Words that represent 4 over 3","71452fef":"# Words that represent 5 over 1","37a94e20":"# Words that represent 5 over 4","9fa598c4":"# See ngrams with the most weight when comparing 2 classes\n\nRecall formula for logistic regression where \n\n\\\\(\\sigma(\\sum_{i=0}^{n}w_i*x_i) >0.5 = 1\\\\)\n\n\\\\(\\sigma(\\sum_{i=0}^{n}w_i*x_i) <=0.5 = 0\\\\)\n\nHere n is the number of ngrams, \\\\(x_i\\\\) is the word and \\\\(w_i\\\\) is the weight for \\\\(x_i\\\\)\n\nIn the first step I map the target class `rating2` to 1 and `rating1` to 0.\n\nSince I initialize \\\\(x_i\\\\) with binary values(i.e. whether ngram is in sentence or not) from count vectorizer, \\\\(w_i\\\\) represents the absolute weight of the term.\n\nWith the top 10 \\\\(w_i\\\\) I get the corresponding \\\\(x_i\\\\) which is an ngram.\n\n\nThis represents the terms that best represent the `rating2` as compared to `rating1`."}}