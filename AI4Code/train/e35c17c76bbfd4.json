{"cell_type":{"02c52f6b":"code","12410741":"code","e303ec81":"code","335a38b0":"code","ee950711":"code","44b2cdf8":"code","b73425a4":"code","1104f198":"code","47503104":"code","9f29bc30":"code","90990401":"code","5e94cd99":"code","de3a9b43":"code","aa281883":"code","66780469":"code","d6464eab":"code","94ff8f74":"code","f4fabfb5":"code","810ca189":"code","69ec574d":"code","515de11f":"code","6f37a809":"code","f3de601a":"code","972b980e":"code","af583442":"code","b9c46ee1":"code","a25bd9a6":"code","0d1eeb48":"code","76d2b708":"markdown","9e6d2ea3":"markdown","9770fb2f":"markdown","506f75bc":"markdown","1123f996":"markdown","0e9a5a76":"markdown","60cdd573":"markdown","d3daddb3":"markdown","220b4a2e":"markdown"},"source":{"02c52f6b":"!pip install pygeohash\nimport pygeohash as pgh\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd \nimport datetime\nimport calendar\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# from tsfresh.examples.har_dataset import download_har_dataset, load_har_dataset, load_har_classes\n# from tsfresh import extract_features, extract_relevant_features, select_features\n# from tsfresh.utilities.dataframe_functions import impute\n# from tsfresh.feature_extraction import settings\n# import geopandas\n# import geopy\n# from geopy.extra.rate_limiter import RateLimiter\n# from geopy.geocoders import Nominatim\n\nimport folium\nimport os\nfrom tqdm import tqdm\nfrom IPython.display import display","12410741":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e303ec81":"# files = []\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         if 'addresses' in filename:\n#             continue\n#         files.append(os.path.join(dirname, filename))\n# cols = pd.read_csv(files[0]).columns\n# df = pd.concat([pd.read_csv(file, header=None, skiprows=1) for file in files])\n# df.columns = cols","335a38b0":"df = pd.read_csv('\/kaggle\/input\/df-with-gh-for-sales-analysis\/df_add_gh.csv', index_col='Unnamed: 0')","ee950711":"df.head()","44b2cdf8":"# df['Year'] = pd.to_datetime(df['Order Date'], errors='coerce').dt.year\n# df['Month'] = pd.to_datetime(df['Order Date'], errors='coerce').dt.month\n# df['Day'] = pd.to_datetime(df['Order Date'], errors='coerce').dt.dayofyear\n# df['Date'] = pd.to_datetime(df['Order Date'], errors='coerce').dt.date\n# df['Week'] = pd.to_datetime(df['Order Date'], errors='coerce').dt.week\n# df['Time'] = pd.to_datetime(df['Order Date'], errors='coerce').dt.time\n# df.drop('Order Date', inplace=True, axis=1)\n# df.drop(df['Product'] == 'Product', inplace=True)\n\n# df.dropna(inplace=True)\n# df[['Price Each', 'Quantity Ordered', 'Day', 'Week']] = df[['Price Each', 'Quantity Ordered', 'Day', 'Week']].astype('float32')\n# df['total'] = df['Quantity Ordered'] * df['Price Each']\n\n# df.to_csv('full_df.csv')\ndf.head()","b73425a4":"plt.barh(np.arange(len(df['Product'].value_counts())), df['Product'].value_counts(ascending=True))\nplt.yticks(np.arange(len(df['Product'].value_counts())), df['Product'].value_counts(ascending=True).index);","1104f198":"top_7 = list(df['Product'].value_counts(ascending=True).index[12:])","47503104":"month_sales = df.pivot_table(values='Quantity Ordered', index='Month', columns='Product', aggfunc='sum')\nmonth_sales","9f29bc30":"displays = ['20in Monitor', '27in 4K Gaming Monitor', '27in FHD Monitor', '34in Ultrawide Monitor']\napple = ['Apple Airpods Headphones', 'Lightning Charging Cable', 'Macbook Pro Laptop', 'iPhone']\nbatteries = ['AA Batteries (4-pack)', 'AAA Batteries (4-pack)']\nothers = ['Bose SoundSport Headphones', 'Flatscreen TV', 'Google Phone', 'LG Dryer', \n          'LG Washing Machine', 'ThinkPad Laptop', 'USB-C Charging Cable','Vareebadd Phone', \n          'Wired Headphones']","90990401":"fig, axes = plt.subplots(2, 2, figsize=(20, 10))\naxes = axes.flatten()\nfor ax, product in zip(axes, [displays, apple, batteries, others]):\n    ax.plot(month_sales[product]); \n\n    ax.legend(month_sales[product], ncol=3, fontsize=11)\n    ax.set_ylim(0, 4500)\n    ax.set_xticks(np.arange(13))\n    ax.set_xticklabels(calendar.month_name, rotation=40)\nfig.suptitle('Monthly sales with rollin mean and std', y=.91, x=.51);","5e94cd99":"def moving_average(product_name, n):\n    series = df[df['Product'] == product_name].groupby('Week').count()['Quantity Ordered']\n    \n    rolling_mean = series.rolling(window=n).mean()\n\n    rolling_std =  series.rolling(window=n).std()\n    upper_bound = rolling_mean+1.96*rolling_std\n    lower_bound = rolling_mean-1.96*rolling_std\n    return [series[n:], rolling_mean, upper_bound, lower_bound]","de3a9b43":"fig, axes = plt.subplots(6, 3, figsize=(20, 20))\naxes = axes.flatten()\nfor product, ax in zip(df.Product.unique(), axes):\n#     pass\n    sales, rol, ub, lb = moving_average(product, 3)\n    ax.plot(sales, 'r');\n    ax.plot(rol, 'b--')\n    ax.plot(ub, c='gray', ls='-.')\n    ax.plot(lb, c='gray', ls='-.')\n    ax.set_title(product, fontsize=10)\nfig.suptitle('Weekly sales (red) with rollin mean (blue) and std (gray)', y=.91, x=.51);\n\n","aa281883":"def exp_smoothing(product, alpha):\n    series = df[df['Product'] == product].groupby('Week').count()['Quantity Ordered'].values\n    result = [series[0]] \n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n\ndef double_exp_smoothing(product, alpha, beta):\n    series = df[df['Product'] == product].groupby('Week').count()['Quantity Ordered'].values\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series):\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result","66780469":"fig, axes = plt.subplots(6, 3, figsize=(20, 20))\naxes = axes.flatten()\nfor product, ax in zip(df.Product.unique(), axes):\n#     pass\n    exp_sm = exp_smoothing(product, .5)\n    ax.plot(exp_sm, 'b--');\n    ax.plot(df[df['Product'] == product].groupby('Week').count()['Quantity Ordered'], 'r')\n    ax.set_title(product, fontsize=10)\nfig.suptitle('Weekly sales (red) with rollin mean (blue): exp_smoothing', y=.91, x=.51);\n\n","d6464eab":"fig, axes = plt.subplots(6, 3, figsize=(20, 20))\naxes = axes.flatten()\nfor product, ax in zip(df.Product.unique(), axes):\n#     pass\n    double_exp_sm = double_exp_smoothing(product, .5, .5)\n    ax.plot(double_exp_sm, 'b--');\n    ax.plot(df[df['Product'] == product].groupby('Week').count()['Quantity Ordered'], 'r')\n    ax.set_title(product, fontsize=10)\nfig.suptitle('Weekly sales (red) with rollin mean (blue): double exp smoothing', y=.91, x=.51);","94ff8f74":"fig, ax1 = plt.subplots(figsize=(25, 5))\n\nax1.set_xlabel('Days')\nax1.set_ylabel('Total cost per day', color='tab:red')\nax1.plot(df.groupby('Day')['total'].sum(), alpha=.5)\nax1.tick_params(axis='y')\nax1.grid(False)\nax2 = ax1.twinx()  \nax2.set_ylabel('Mean price per day', color='tab:blue')  # we already handled the x-label with ax1\nax2.plot(df.groupby('Day')['Price Each'].mean(), color='tab:blue', alpha=.5)\nax2.grid(False)\nax2.tick_params(axis='y', labelcolor='tab:blue')\nplt.title('Mean price and total cost per day');","f4fabfb5":"fig, ax1 = plt.subplots(figsize=(25, 5))\n\nax1.set_xlabel('Days')\nax1.set_ylabel('Total cost per day', color='tab:red')\nax1.plot(df.groupby('Day')['total'].sum(), alpha=.5)\nax1.tick_params(axis='y')\nax1.grid(False)\nax2 = ax1.twinx()  \nax2.set_ylabel('Number of items', color='tab:blue')\nax2.plot(df.groupby('Day')['Quantity Ordered'].count(), color='tab:blue', alpha=.5)\nax2.grid(False)\nax2.tick_params(axis='y', labelcolor='tab:blue')\nplt.title('Number of items and total cost per day');","810ca189":"fig, ax1 = plt.subplots(figsize=(25, 5))\n\nax1.set_xlabel('Week')\nax1.set_ylabel('Total cost per week', color='tab:red')\nax1.plot(df.groupby('Week')['total'].sum(), alpha=.5)\nax1.tick_params(axis='y')\nax1.grid(False)\nax2 = ax1.twinx()  \nax2.set_ylabel('Mean price per week', color='tab:blue')  # we already handled the x-label with ax1\nax2.plot(df.groupby('Week')['Price Each'].mean(), color='tab:blue', alpha=.5)\nax2.grid(False)\nax2.tick_params(axis='y', labelcolor='tab:blue')\nplt.title('Mean price and total cost per week');","69ec574d":"fig, ax1 = plt.subplots(figsize=(25, 5))\n\nax1.set_xlabel('Week')\nax1.set_ylabel('Total cost per week', color='tab:red')\nax1.plot(df.groupby('Week')['total'].sum(), alpha=.5)\nax1.tick_params(axis='y')\nax1.grid(False)\nax2 = ax1.twinx()  \nax2.set_ylabel('Number of items', color='tab:blue')\nax2.plot(df.groupby('Week')['Quantity Ordered'].count(), color='tab:blue', alpha=.5)\nax2.grid(False)\nax2.tick_params(axis='y', labelcolor='tab:blue')\nplt.title('Number of items and total cost per week');","515de11f":"# addresses = pd.read_csv('\/kaggle\/input\/addresses-for-sales-analysis-2019\/addresses.csv', index_col='Unnamed: 0')\n# addresses.head()","6f37a809":"# df_add = pd.merge(df, addresses, left_on='Purchase Address', right_on='address')","f3de601a":"#worst way. May be in future\n\n# lat = []\n# lon = []\n# for i in tqdm(np.arange(df_add.shape[0])):\n#     long = float(df_add.iloc[i]['location'][1:-1][:df_add.iloc[i]['location'][1:-1].find(', ')])\n#     lati = float(df_add.iloc[i]['location'][1:-1][df_add.iloc[i]['location'][1:-1].find(', ')+1:])\n#     lat.append(lati)\n#     lon.append(long)\n# df_add['lat'] = lat\n# df_add['lon'] = lon\n# df_add.drop('location', axis=1)\n# df_add.head()","972b980e":"m = folium.Map(location=[29.627060, -96.052370], tiles=\"OpenStreetMap\", zoom_start=4)\nsample = df.sample(1000)\nfor i in range(sample.shape[0]):\n    folium.Marker([sample.iloc[i]['lon'], sample.iloc[i]['lat']], \n                  popup=sample.iloc[i]['Purchase Address']).add_to(m)\n    \nm","af583442":"# df_add['gh'] = df_add.apply(lambda x: pgh.encode(x['lat'], x['lon'], precision=6), axis=1)","b9c46ee1":"plt.figure(figsize=(15, 5))\nsns.heatmap(pd.crosstab(df[df['Product'].isin(top_7)]['Product'], \n                        df[df['Product'].isin(top_7)]['gh']), \n            cmap=\"gist_earth\")","a25bd9a6":"plt.figure(figsize=(14, 4))\nplt.bar(df['Quantity Ordered'].groupby(df['gh']).sum().sort_values(ascending=False).index[:10], \n        df['Quantity Ordered'].groupby(df['gh']).sum().sort_values(ascending=False)[:10], align='center')\nplt.xticks(rotation=40, ha='right');","0d1eeb48":"# df_add.to_csv('df_add_gh.csv')","76d2b708":"Create month sales table for each good using the `groupby` method. ","9e6d2ea3":"Lets group goods (that doesnt make sense, only for visualization).","9770fb2f":"As can be seen, high value of mean price not meaning the higher total cost. May be users order a cheaper product in several copies.","506f75bc":"Create bar plot with top of sold items. ","1123f996":"Lets plot month sales by groups","0e9a5a76":"Many items  show the increasing of sales in September. We will see this further.","60cdd573":"Lets plot these values for weeks. ","d3daddb3":"Collect files in one dataframe and create new columns with `Year, Month, Day, Week` and `Time` instead of one column `Order Date`, then drop `Order Date`, drop rows where `Product == Product` and drop `NA`. And also convert `Price Each` and `Quantity Order` to `float32`.  ","220b4a2e":"Plot moving average for each good by week. Then apply [exponential smoothing](https:\/\/en.wikipedia.org\/wiki\/Exponential_smoothing) and [double exponential smoothing](https:\/\/en.wikipedia.org\/wiki\/Exponential_smoothing#Double_exponential_smoothing)"}}