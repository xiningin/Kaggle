{"cell_type":{"41055829":"code","4238b5b5":"code","6b02056e":"code","02ab4542":"code","6324e960":"code","90f5bcbf":"code","8c019b2c":"code","18c3c273":"code","cb38f74e":"code","b6306927":"code","816d8979":"code","c5b72204":"code","a8138986":"code","122320cd":"code","cb04f890":"markdown","099f9d57":"markdown","82137a05":"markdown","5e6496af":"markdown","2530cfba":"markdown","09677e7a":"markdown","4f475bd8":"markdown","bf4a09f0":"markdown","900d81fd":"markdown","d66ce42f":"markdown","0772a394":"markdown","f9b5bdc4":"markdown","dfcce3c4":"markdown","ff783c0c":"markdown","8b97fa91":"markdown","db1956d4":"markdown","791e8354":"markdown","fff22012":"markdown","ce6e1364":"markdown","6016489d":"markdown","056d640f":"markdown","2bc5d5c5":"markdown","3980d461":"markdown","7cbf4ee0":"markdown","3013e88d":"markdown","be7fdc80":"markdown","b199e234":"markdown","a2da07f1":"markdown","f062a035":"markdown","8f3bcae4":"markdown","47378a37":"markdown","b8e94f91":"markdown","836471fe":"markdown","10d09aec":"markdown","210fd9f8":"markdown","9dc2f123":"markdown","2bbfc2ce":"markdown","d57a6c60":"markdown","3e5353c9":"markdown","b9d1c071":"markdown","7aa9f194":"markdown","22abf1d3":"markdown","c2a7240d":"markdown","784db7c9":"markdown","12fc963f":"markdown","c0fbeb6c":"markdown"},"source":{"41055829":"import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4238b5b5":"train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col=0)\n\nX = pd.concat([train.drop(\"SalePrice\", axis=1),test], axis=0)\ny = train[['SalePrice']]","6b02056e":"def df_preprocessing(df):\n    \n    ## Dropping Features that are not useful for model prediction\n    df.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars'], axis=1, inplace=True)    #drop highly correlated feature\n    df.drop(['PoolQC','MiscFeature','Alley'], axis=1, inplace=True)         #drop top 3 columns with most number of missing values\n    df.drop(['MoSold','YrSold'], axis=1, inplace=True)          #remove columns with no relationship with SalePrice\n    \n    df_col = df.columns     #remove columns with >96% same values\n    overfit_col = []\n    for i in df_col:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(X) * 100 > 96:\n            overfit_col.append(i)\n\n    overfit_col = list(overfit_col)\n    df = df.drop(overfit_col, axis=1)\n\n    \n    ## Removing outliers\n    global train\n    train = train.drop(train[train['LotFrontage'] > 200].index)\n    train = train.drop(train[train['LotArea'] > 100000].index)\n    train = train.drop(train[train['BsmtFinSF1'] > 4000].index)\n    train = train.drop(train[train['TotalBsmtSF'] > 5000].index)\n    train = train.drop(train[train['GrLivArea'] > 4000].index)\n    \n    \n    ## Impute missing values\n    ordd = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu','Fence',\"KitchenQual\",\n       \"HeatingQC\",'ExterQual','ExterCond']\n    df[ordd] = df[ordd].fillna(\"NA\")         #Ordinal columns replace missing values with NA\n    \n    cat = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"]\n    df[cat] = df.groupby(\"Neighborhood\")[cat].transform(lambda x: x.fillna(x.mode()[0]))      #Nominal columns replace missing value with most frequent occurrence aka mode\n    \n    df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))      #Replace with mean after grouping by Neighborhood\n    df['GarageArea'] = df.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean())) \n    df['MSZoning'] = df.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n    cont = [\"BsmtHalfBath\", \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\"]\n    df[cont] = X[cont] = X[cont].fillna(X[cont].mean())       #Replace missing values with respective mean values for continuous features\n    \n    \n    ## Mapping Ordinal Features\n    ordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\n    fintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\n    expose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\n    fence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}\n    \n    ord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\n    for col in ord_col:\n        df[col] = df[col].map(ordinal_map)\n\n    fin_col = ['BsmtFinType1','BsmtFinType2']\n    for col in fin_col:\n        df[col] = df[col].map(fintype_map)\n\n    df['BsmtExposure'] = df['BsmtExposure'].map(expose_map)\n    df['Fence'] = df['Fence'].map(fence_map)\n    \n    ## Change data type\n    df['MSSubClass'] = df['MSSubClass'].apply(str)\n    \n    return df\n\n\n## Feature Engineering\ndef feat_engineer(df):\n    \n    ## Add new features based on merging relevant existing features\n    X['TotalLot'] = X['LotFrontage'] + X['LotArea']\n    X['TotalBsmtFin'] = X['BsmtFinSF1'] + X['BsmtFinSF2']\n    X['TotalSF'] = X['TotalBsmtSF'] + X['2ndFlrSF']\n    X['TotalBath'] = X['FullBath'] + X['HalfBath']\n    X['TotalPorch'] = X['OpenPorchSF'] + X['EnclosedPorch'] + X['ScreenPorch']\n    \n    \n    ## Generate Binary columns indicating 0\/1 the presence of such features\n    colum = ['MasVnrArea','TotalBsmtFin','TotalBsmtSF','2ndFlrSF','WoodDeckSF','TotalPorch']\n    for col in colum:\n        col_name = col+'_bin'\n        df[col_name] = df[col].apply(lambda x: 1 if x > 0 else 0)\n    \n    \n    ## Convert categorical to numerical through One-hot encoding\n    df = pd.get_dummies(df)\n    return df\n\n\nX = df_preprocessing(X)\nX = feat_engineer(X)","02ab4542":"## Scaling\ncols = X.select_dtypes(np.number).columns\nX[cols] = RobustScaler().fit_transform(X[cols])\n\n## Log transformation of SalePrice\ny[\"SalePrice\"] = np.log(y['SalePrice'])\n\n## Return train and test index\nx = X.loc[train.index]\ny = y.loc[train.index]\ntest = X.loc[test.index]\n\n#Split train into train\/validation set for training\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=2020)\n\nprint(X_train.shape)","6324e960":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=500, criterion='friedman_mse',\n                                min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n                                min_impurity_split=None, random_state=2020, max_features='sqrt', \n                                alpha=0.9, tol=0.0001)   #default from documentation","90f5bcbf":"#Training\ngbr.fit(X_train, y_train.values.ravel())\n\n#Inference\ny_pred = gbr.predict(X_val)\n\n#Evaluate\nnp.sqrt(mean_squared_error(y_val,y_pred))","8c019b2c":"import xgboost as xgboost\n\nxgb = xgboost.XGBRegressor(n_estimators=500,\n                           max_depth=3,\n                           learning_rate=0.1,\n                           min_child_weight=2,\n                           grow_policy='lossguide',\n                           random_state=2020)","18c3c273":"#Training\nxgb.fit(X_train, y_train.values.ravel())\n\n#Inference\ny_pred = xgb.predict(X_val)\n\n#Evaluate\nnp.sqrt(mean_squared_error(y_val,y_pred))","cb38f74e":"import lightgbm as lightgbm\n\nlgb = lightgbm.LGBMRegressor(boosting_type='gbdt',\n                             max_depth=3,\n                             learning_rate=0.1,\n                             n_estimators=500,\n                             min_child_samples=2,\n                             reg_alpha=0.0,\n                             reg_lambda=0.1,\n                             random_state=2020\n                             )","b6306927":"#Training\nlgb.fit(X_train, y_train.values.ravel())\n\n#Inference\ny_pred = lgb.predict(X_val)\n\n#Evaluate\nnp.sqrt(mean_squared_error(y_val,y_pred))","816d8979":"from catboost import CatBoostRegressor\n\ncb = CatBoostRegressor(max_depth=3,\n                       learning_rate=0.1,\n                       n_estimators=500,\n                       loss_function='RMSE',\n                       boosting_type='Ordered',\n                       min_child_samples=2,\n                       l2_leaf_reg=0.1,\n                       random_state=2020,\n                       logging_level='Silent')","c5b72204":"#Training\ncb.fit(X_train, y_train.values.ravel())\n\n#Inference\ny_pred = cb.predict(X_val)\n\n#Evaluate\nnp.sqrt(mean_squared_error(y_val,y_pred))","a8138986":"from sklearn.model_selection import GridSearchCV\n\nlearning_rate = [0.0001, 0.001, 0.01, 0.1]\nn_estimators = [50, 100, 250, 500]\nmax_depth = [3,5,10]\n\nparam_grid = dict(learning_rate = learning_rate, #Dictionary with parameters names (str) as keys and lists of parameter settings to try as values\n             n_estimators = n_estimators,\n             max_depth = max_depth)\n\ngrid = GridSearchCV(estimator=gbr,\n                    param_grid=param_grid,\n                    scoring=\"neg_root_mean_squared_error\",\n                    verbose=1,\n                    n_jobs=-1)\n\ngrid_gbr = grid.fit(X_train,y_train)\nprint('Best Score: ', grid_gbr.best_score_)\nprint('Best Params: ', grid_gbr.best_params_)","122320cd":"from sklearn.model_selection import RandomizedSearchCV\n\nparam_distributions = {'learning_rate' : [0.0001, 0.001, 0.01, 0.1],\n                       'n_estimators' : [50, 100, 250, 500, 700, 900],\n                       'max_depth' : [i for i in range(10)],\n                       'min_samples_split': [2, 5, 10, 20, 40],\n                       'max_features' : [\"auto\", \"sqrt\", \"log2\"]}\n\nrdm_grid = RandomizedSearchCV(estimator=gbr,\n                             param_distributions = param_distributions,\n                             n_iter = 100,\n                             n_jobs = -1,\n                             scoring=\"neg_root_mean_squared_error\")\n\nrdm_gbr = rdm_grid.fit(X_train, y_train)\nprint('Best Score: ', rdm_gbr.best_score_)\nprint('Best Params: ', rdm_gbr.best_params_)","cb04f890":"![](https:\/\/imgur.com\/E7faDVa)","099f9d57":"Thats all folks! In this notebook, we have covered:\n1. Boosting Algorithms\n2. Hyperparameter tuning  \n\n\n\nI hope you have learnt something out of this notebook which might help in your future data science compeitions\/projects. \nDo give a **Upvote** if it has helped you :) Thank you!","82137a05":"Find out more about Catboost here:\n* https:\/\/medium.com\/@hanishsidhu\/whats-so-special-about-catboost-335d64d754ae#:~:text=CatBoost%20is%20based%20on%20gradient,needed%20for%20most%20business%20problems.\n* https:\/\/towardsdatascience.com\/introduction-to-gradient-boosting-on-decision-trees-with-catboost-d511a9ccbd14","5e6496af":"<a id='sec3.2'><\/a>\n### [3.2 Grid Search](#sec3.2)\nGrid Search is an approach where we prepare a set of candidate values for the hyperparameters, and train the model with every combination of the hyparameters values exhaustively. Lets see how we implement Grid Search on python","2530cfba":"#### Step 2 (c)\nNow that we have fitted the new tree-based learner using the residuals calculated from the previous learner, we can now proceed on to generate the output of the new tree model\n\n$$ y_{jm} = \\arg\\min_{\\gamma} \\sum_{x_{i}\\in R_{jm}}L(y_i, F_{m-1}(x_{i}) + \\gamma) $$ <br>\n\nFollowing this we calculate the output $ y_{jm} $ that will minimize the loss function $ \\sum_{x_{i}\\in R_{jm}}L(y_i, F_{m-1}(x_{i}) + \\gamma) $\n","09677e7a":"Find out more at:\n* https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\n* https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d","4f475bd8":"#### Categorical Feature Combinations\nCatBoost also combines mutiple cateogircal features atuomatically wherever it makes sense, by building a base tree with root node consisting of onlu a single feature. It then radomly select the other best feature as child nodes and represent it along with the root node feature. Deeper down the tree, the number of categorical feature combinations will increase.\n<img src='https:\/\/miro.medium.com\/max\/1400\/0*0SmuD3DHT013X2vd' width=400, height=300>","bf4a09f0":"<a id='sec2.4'><\/a>\n### [2.4 CatBoost](#sec2.4)\nCatBoost is a tree-based boosting algorithm developed by Yandex. Compared to the previous few tree based boosting algorithms, CatBoost implements **Symmetric Trees**, which helps in decreasing prediction time.\n<img src='https:\/\/miro.medium.com\/max\/1400\/1*AjrRnwvBuu-zK8CvEfM29w.png' width=600 height=500>\n","900d81fd":"<a id='sec2'><\/a>\n## [2. Boosting Algorithms](.sec2)\nIn this section i will like to explain what is **Boosting**, and the various popular boosting algorithms that people uses. We will dive deep into the following algorithms:\n1. Gradient Boost Regressor\n2. Extreme Gradient Boost\n3. LightGBM\n4. CatBoost","d66ce42f":"#### Step 2 (b)\n\nAfter we have gotten the residual, $ r_{im} $ from the base learner, we will generate a tree based learner; decision tree, which will fit to the residual values calculated earlier. Hence, we can see that the new learners are generated based on the previous generation.","0772a394":"Find out more at:\n* https:\/\/towardsdatascience.com\/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4\n* https:\/\/medium.com\/mlreview\/gradient-boosting-from-scratch-1e317ae4587d\n* https:\/\/explained.ai\/gradient-boosting\/L2-loss.html\n","f9b5bdc4":"<a id='sec3'><\/a>\n## [3. Hyperparameter Tuning](#sec3)\nBefore we start on the technicalities of Hyperparameter Tuning, lets first define what do we mean by *Hyperparameter*. Hyperparameter is defined as the parameters whose values are set before the learning process. Hyperparameter tuning refers to choosing a set of optimal hyperparameters for a learning algorithm. Machine Learning model don't learn hyperparameters, they learn parameters, which are the weights in the model","dfcce3c4":"#### Ensemble Methods\nBefore defining what is Boosting, we would need to understand idea of ensembling. Ensembling is a technique that creates multiple models and then combining them to make a prediction. This reduce the reliance on a single model for our prediction (which may not be accurate) and leverage on many models to give our final result.\nThe idea of Ensemble method is that with the use of multiple models and combining them to give our prediction, we can effectively:\n1. Improve accuracy\n2. Reduce Variance  ","ff783c0c":"<a id='sec3.1'><\/a>\n### [3.1 Hyperparameters for Gradient Boost](#sec3.1)","8b97fa91":"#### System Optimization\n1. **Parallelization**: XGBoost uses parallelized implementation for the sequential tree building. \n2. **Tree Pruning**: Uses 'max_depth' parameter for tree splitting, and start pruning trees backward. This approach improves the performance significantly\n3. **Hardware Optimization**: XGBoost makes efficient use of hardware resources. This is done by allocating internal buffers to each thread to store gradients statistics; cache awareness. 'Out-of-core' computing also optimize available disk space while handling big dataframe that do not into memory. ","db1956d4":"1. **min_samples_split**\n *   Defines the minimum number of sample which are required in a node before it is allowed to be considered for splitting\n *   High values prevent model from learning too specific relationship for a particular sample; reducing overfitting \n2. **min_sample_leaf**\n *   Minimum number of samples required for a leaf node\n *   Similar to min_sample_split; prevent overfitting\n3. **max_depth**\n *   Maximum depth of the tree\n *   Higher depth; over-fitting\n4. **max_leaf_node**\n *   Maximum number of leaf nodes in a tree\n *   Similar to max_depth\n *   Max_leaf_nodes=k gives comparable results to max_depth=k-1 but is significantly faster to train at the expense of a slightly higher training error.\n5. **max_features**\n *   Number of features to be considered in search of a best split\n *   Rule of thumb: square root of the total number of features","791e8354":"<a id='sec2.2'><\/a>\n### [2.2 Extreme Gradient Boost (XGBoost)](#sec2.2)\n\nXGBoost is an improved version of the Gradient Boost architecture with some additional improvements.\n\n<img src='https:\/\/miro.medium.com\/max\/1400\/1*FLshv-wVDfu-i54OqvZdHg.png' width=600, height=500>","fff22012":"#### Pros\n- Decreased processing time since we can control the number of parameter search\n- May be more effective than Grid Search","ce6e1364":"#### Handling Categorical Features\nCatBoost algorithm handles categorical data using the concept of ordered boosting, applying it to **response coding**. In a typical response coding, each categorical feature is represented using the mean to the target values of all the data points with the same categorical feature. This leads to the problem of **Target Leakage**. On the other hand, CatBoost only consider the previous data points to that time when calculating the mean to the target valus of the data points with same categorical features.","6016489d":"#### Cons\n- Time consuming and computationally expensive as it search every combination of values\n- It may neglect on important parameters and spend time on optimizing redundant parameters instead","056d640f":"<a id='sec2.1'><\/a>\n### [2.1 Gradient Boost Regressor](#sec2.1)\nGradient Boost is a kind of tree-based boosting model that uses gradient descent as its optimization algorithm. Gradient Boost Regressor is a variant that deals with regression problems; where the target variable is a continuous variable, which is *SalePrice*, in our case.","2bc5d5c5":"<a id='sec2.3'><\/a>\n### [2.3 LightGBM](#sec2.3)\nLightBGM is another gradient boosting framework that uses tree-based learning algorithm, created by Microsoft.\nFeatures of LightGBM includes:\n1. High speed\n2. Able to handle large dataset\n3. Takes less memory to run","3980d461":"#### Boosting\nBoosting is one of the ensembling techniques that we will be focusing on in this notebook. The idea of Boosting is to begin with a class of weak learners, and improving these weak learners into strong learners. It always starts off with a base learner; usually a tree-based model like Decision Tree. From there, the algorithm will train the Decision tree and identify mistakes that it has made. It then generates another decision tree, which place more emphasis in trying to correct these mistakes by giving more weights to the misclassified instance, while having less emphasis on the correct predictions. This process will go on until the algorithm no longer improves. Thus, we can see that Boosting builds models in a sequential manner where the current generation improves on the mistakes that the previous generation of models have made, learning and correcting these mistakes, ultimately improving from weak learner to a strong learner which can give accurate prediction.\n\n![](https:\/\/miro.medium.com\/max\/1400\/0*yDz8euzLbQvucBwx.png)","7cbf4ee0":"LightGBM grows tree vertically as opposed to other tree algorithm, which grows tree horizontally. This means that LightGBM grows tree leaf-wise, choosing the leaf with max delta loss to grow. Growing on the same leaf, leaf-wise algorithm will reduce more loss than level-wise algorithm. Leaf-wise algorithm will also tend to excel in larger dataset.\n<img src='https:\/\/miro.medium.com\/max\/700\/1*AZsSoXb8lc5N6mnhqX5JCg.png'>\n<img src='https:\/\/miro.medium.com\/max\/700\/1*whSa8rY4sgFQj1rEcWr8Ag.png'>\n\nLevel-wise training can be equate as a form of regularized training, since leaf wise-training can constrcut any tree the level-wise can, but not vice versa. Hence, leaf-wise training is more prone to overfitting albeit more flexible. Thus, having a larger dataset will be beneficial.","3013e88d":"Lets take a quick look at the algorithm for Gradient Boost <br>\n<img src=\"https:\/\/i.imgur.com\/OBmgEba.png\" width='500' height='40'> <br>\nWe will go through the algorithm step by step to understand the entire algorithm in an intuitive manner.","be7fdc80":"#### Cons\n- Depending on the number of searches and how large the parameter space is, some parameters might not be explored enough","b199e234":"#### Tree Specific Parameters\n![tree_param](https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2016\/02\/tree-infographic.png)","a2da07f1":"#### Histogram-Based Splitting Methods\nFinding the best split in the decision tree model is a key challenge. A naive traversal of every feature of every data point will be computationally expensive, with complexity of $O(n_{data}n_{features})$. Building on the observation that small changes in split does not make much difference in the decision tree performance, XGBoost and LightGBM both employed a Histogram-based method that takes advantage of this fact and group features into discrete bins and performs splitting on these bins. Hence, reducing complexity of splitting to $O(n_{data}n_{bins})$\n<img src='https:\/\/i2.wp.com\/mlexplained.com\/wp-content\/uploads\/2018\/01\/binned_split_gbdt.png?resize=300%2C260&ssl=1'>\n\nSuch methods of splitting also presents the user with some decisions to be made. \n1. Number of bins creates a trade-off between accuracy and speed, more bins will result in higher accuracy, but slower.\n2. How to divide the features into discrete bins is a non-trivial problem. Gradient statistics is the most widely used approach in dividing the bins. E.g 'approx, hist' under parameter *tree_method*","f062a035":"#### Boosting Parameters\n\n1. **learning_rate**\n *   Determines the impact of each tree on the final prediction\n2. **n_estimators**\n *   Number of trees to be modeled\n *   May overfit with higher number of trees\n3. **subsample**\n *   Fraction of samples to be selected for each tree\n *   Values slightly less than 1; more robust, reducing variance","8f3bcae4":"#### Step 2 (a)\n\n$$ r_{im} = -\\big[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\big] $$\n\nThis is the section we calculate the loss function with respect to the predicted values from the base learner. This differential formula will end up as pseudo-residuals. Residuals is the value between the actual truth and predicted values; $ r_{im} = (observed - predicted) $ This is differential formula effectively calculates the gradient, hence giving its name; **Gradient Boost Regressor**.\n\n\n","47378a37":"#### Step 2 (d)\nWe will then update the new model $ F_{m}(x) $ using the current generated tree model with the previous baseline classifier\n$$\n F_{m}(x) = F_{m-1}(x) + v\\sum_{j=1}^{J_{m}} \\gamma_{jm}I(x \\in R_{jm})\n $$\nwhere $ v $ is the learning rate","b8e94f91":"# Table of Content\n1. [Data Preprocesssing](#sec1)\n2. [Boosting Algorithms](#sec2)  \n    * [2.1 Gradient Boost Regressor](#sec2.1)\n    * [2.2 Extreme Gradient Boost](#sec2.2)\n    * [2.3 LightGBM](#sec2.3)\n    * [2.4 CatBoost](#sec2.4)\n3. [HyperParameter Tuning](#sec3)  \n    * [Boosting Hyperparameters](#sec3.1)\n    * [Grid Search](#sec3.2)\n    * [Randomized Search](#sec3.3)","836471fe":"#### Gradient-based One-Side Sampling (GOSS)\nNot all datapoints contributes equally to training; data points with small gradients tend to be more well-trained (close to local minima). Hence, it is more efficient to concentrate on data points with larger gradients. A naive approach will be to ignore the data points with small gradients when computing the best split; which may lead to biased sampling resulting in alteration of data distribution. LightGBM solve this problem by random sampling data with small gradient and increase the weights of these samples when computing their contribution to change in loss (a form of importance smapling)\n","10d09aec":"#### Random Permutation\nCatBoost further avoid overfitting the models by using random permutations on the dataset before applying ordered boosting on those random permutations. This feature can be tuned through the parameter *bagging_temperature*. ","210fd9f8":"#### Ordered Boosting\nThe training procedure of CatBoost differs from other boosting algorithms. Instead of calculating residuals on each data points using model that has been trained on the same dataset, Catboost calculates residuals on data points that the model has never seen before; trainind different models that is used to calculate residuals for different data points. Since training many different models is pretty computationally expensive, CatBoost trains only $log_{i}$ models, where $i$ refers to the number of data points","9dc2f123":"Find out more on LightGBM:\n* https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n* https:\/\/docs.microsoft.com\/en-gb\/archive\/blogs\/machinelearning\/lessons-learned-benchmarking-fast-machine-learning-algorithms\n* https:\/\/mlexplained.com\/2018\/01\/05\/lightgbm-and-xgboost-explained\/\n* https:\/\/towardsdatascience.com\/what-makes-lightgbm-lightning-fast-a27cf0d9785e","2bbfc2ce":"<a id='sec3.3'><\/a>\n### [3.3 Randomized Search](#sec3.3)\nRandomized search differs from grid search in that it search the sets of candidates hyperparameters values randomly instead of carrying out an exhaustive search like grid search. In sklearn, we will set the *n_iter* parameters that indicates the number of combinations we randomly try. ","d57a6c60":"#### Algorithmic Enhancement\n1. **Regularization**: It uses both L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.\n2. **Sparsity Awareness**: XGBoost admits sparse features for inputs by automatically 'learning' best missing value depending on training loss\n3. **Weighted Quantile Sketch**: Employs the distributed WQS algorithm to effectively find the optimal split points.\n4. **Cross-Validation**: built-in cross-validation method at each iteration","3e5353c9":"Suprisingly, based on [Bergstra et al., 2012](http:\/\/www.jmlr.org\/papers\/volume13\/bergstra12a\/bergstra12a.pdf) research, randomized search actually performs better than grid search. From the figure, we can see that as a result of a structured search space, grid search only generates 3 distinct places. On the other hand, because of randomness, randomized search was able to generate much more distinct places.\n\n<img src='https:\/\/miro.medium.com\/max\/1400\/0*gaxqCRZa22tunZBJ.png' width=500 height=400>\n\n","b9d1c071":"#### Step 1\n$$ F_{0}(x) = \\arg\\min_{\\gamma} \\sum^{n}_{i=1}L(y_i, \\gamma) $$ <br>\nWe will first build our base learner, $ F_{0}(x) $ using this formula. What this formula essentially says is that we need to find the value of $ \\gamma $ such that $ \\gamma $ minimize the loss function $ \\sum^{n}_{i=1}L(y_i, \\gamma) $ across all input space $ n $. And we assign this value $ \\gamma\\ $ to our model $ F_{0}(x) $, where our model will predict every\nthing as $ \\gamma $. In regression problem, our loss function $ L(y_i, \\gamma) $ is usually the mean square error, $ MSE = (actual - predicted)^2 $. In scenarios where MSE is our loss function, the value of $ \\gamma $ always turns out to the average value of all the target instances.  \nSo we will assign our base model $ F_{0}(x) $ with the average value of our target column. That sums up the step 1 of the gradient boosting algorithm.","7aa9f194":"# Introduction\nIn this notebook, i will be explaining in details the nuts and bolts of some of the popular boosting algorithms used today, as well as the features that differentiates them from one another. Towards the end, i will also bring you through the hyperparameter tuning process in detail. I hope that after reading the notebook, you can have a much better understanding of how each boosting algorithm works and gain insights to why they perform much better than the rest :)","22abf1d3":"Parameters of ensemble methods can be broken down into 3 categories:\n1. Tree-Specific Parameters: These affect each individual tree in the model.\n2. Boosting Parameters: These affect the boosting operation in the model.\n3. Miscellaneous Parameters: Other parameters for overall functioning.","c2a7240d":"#### Pros\n- Able to cover all possible prospective sets of parameters  ","784db7c9":"#### Tuning Strategies\nWhen we talk about hyperparameter tuning, they can be broken down into the following main strategies.\n1. Grid Search\n2. Randomized Search","12fc963f":"#### Exclusive Feature Bundling (EFB)\nIn order to speed up tree learning, LightGBM uses EFB which bundles data features together. In high dimensional data, many features are mutually exclusive (they never take zero values simultaneously). LightGBM identifies these features and bundles them into a single feature, which reduces training complexity. There is 2 parts to this algorithm:\n1. Identify features that could be bundled together\n2. Merging features within the same bundle","c0fbeb6c":"<a id='sec1'><\/a>\n## [1. Data Preprocessing](#sec1)\nIn this section, i have summarized all the steps for the data preprocessing and feature engineering process. Since the main focus of this notebook is on understanding Boosting Algorithms, we will skip this part. The in-depth visualization and step-by-step approach can be found in another of my notebook [here](https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning)"}}