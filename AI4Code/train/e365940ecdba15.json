{"cell_type":{"99f0bdd7":"code","676a21ae":"code","82a5fbf8":"code","fc3d5fa9":"code","e4a8051f":"code","f3d27358":"code","776ad461":"code","6be6d131":"code","c5368ab7":"code","c7c9afb1":"code","b7986199":"code","5d4420d9":"code","0448600f":"code","3d0d6a5b":"code","041b946f":"code","a283aaaf":"code","9895bbea":"code","14afcc46":"code","a9126a33":"code","3c35ffc9":"code","64e3117d":"code","485e7dd3":"code","f35fa06c":"code","44f3e8df":"code","59757824":"markdown","f8f15a0f":"markdown","0f06cbf0":"markdown","dc546059":"markdown","fef805bc":"markdown","7a09394c":"markdown","577a1e89":"markdown","ca42a214":"markdown"},"source":{"99f0bdd7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","676a21ae":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Tensorflow imports\n\n# for building model\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Flatten, GlobalMaxPooling1D, Dense, Dropout\n\n# for Padding\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# for Tokenization \nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# NLTK imports\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# For visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For regular expressions\nimport re\n\n# For data preprocessing\nfrom string import punctuation, digits\n","82a5fbf8":"# Use 'Latin 1' encoding to recognize latin characters\n# importing dataset\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", encoding=\"Latin-1\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\", encoding=\"Latin-1\")\n\nprint(f\"train dataset shape >> {train_df.shape}\")\nprint(f\"test dataset shape >> {test_df.shape}\")\n","fc3d5fa9":"# Creat new dataset (which will consist from 2 columns: label, data)\n\ndef data_label_split(dataset):\n    data = dataset['OriginalTweet']\n    label = dataset['Sentiment']\n    return data,label\n\ntrain_data,train_label = data_label_split(train_df)\ntest_data,test_label = data_label_split(test_df)\n\ntrain = pd.DataFrame({\n    'label':train_label,\n    'data':train_data\n})\n\ntest = pd.DataFrame({\n    'label':test_label,\n    'data':test_data\n})\n\n# Define function which will make new labels\n\ndef reassign_label(x):\n    if x == \"Extremely Positive\" or x == \"Positive\":\n        return 1\n    elif x ==\"Extremely Negative\" or x ==\"Negative\":\n        return -1\n    elif x ==\"Neutral\":\n        return 0\n\ntrain.label = train.label.apply(lambda x:reassign_label(x))\ntest.label = test.label.apply(lambda x:reassign_label(x))\n","e4a8051f":"def remove_punctuation(s):\n    list_punctuation = list(punctuation)\n    for i in list_punctuation:\n        s = s.replace(i,'')\n    return s.lower()\n\ndef clean_sentence(sentence):\n    sentence = sentence.lower()\n    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n    sentence = re.sub(r'(?P<url>https?:\/\/[^\\s]+)', '', sentence) # remove URL adresses\n    sentence = re.sub(r\"\\@(\\w+)\", '', sentence) # remove usernames\n    sentence = re.sub(r\"\\#(\\w+)\", '', sentence) # remove hashtags\n    sentence = re.sub(r\"\\$(\\w+)\", '', sentence) # remove cashtags\n    sentence = sentence.replace(\"-\",' ')\n    tokens = sentence.split()\n    tokens = [remove_punctuation(w) for w in tokens] # remove punctuations\n    stop_words = set(stopwords.words('english')) # remove stopwords\n    tokens = [w for w in tokens if not w in stop_words]\n    remove_digits = str.maketrans('', '', digits)\n    tokens = [w.translate(remove_digits) for w in tokens]\n    tokens = [w.strip() for w in tokens]\n    tokens = [w for w in tokens if w!=\"\"]\n    tokens = ' '.join(tokens)\n    return tokens","f3d27358":"# Clean sentences in train and test data\ntrain.data = train.data.apply(lambda sentence:clean_sentence(sentence))\ntest.data = test.data.apply(lambda sentence:clean_sentence(sentence))","776ad461":"# Remove letter ' \u00e2 ' in words\ndef rem_latin_a(sentences):\n    sentences = sentences.replace(\"\u00e2\u0092\", \"\")\n    return sentences\n\ntrain.data = train.data.apply(lambda sentences:rem_latin_a(sentences))\ntest.data = test.data.apply(lambda sentences:rem_latin_a(sentences))","6be6d131":"# Replace all empty lines in the 'data' column with np.nan objects \ntrain['data'].replace('', np.nan, inplace=True)\ntest['data'].replace('', np.nan, inplace=True)","c5368ab7":"# Discard null values\n\n# Originally there were 41157 rows, now it has become 41106. The training dataset was reduced by 51 lines.\ntrain.dropna(subset=['data'], inplace=True)\n\n# Originally there were 3798 lines, now it has become 3795. The test dataset was reduced by 3 lines.\ntest.dropna(subset=['data'], inplace=True)","c7c9afb1":"# Splitting data to train and test\n\n# Train data\ntrain_data = train.data\ntrain_label = train.label\n\n# Test data\ntest_data = test.data\ntest_label = test.label","b7986199":"# Plot a graph of the number of elements in each class in the train data\n\nsns.countplot(x=train_label)\nplt.title('The number of elements in each class in the train data', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})\nplt.tight_layout()\nplt.show()\n","5d4420d9":"# Plot a graph of the number of elements in each class in the test data\n\nsns.countplot(x=test_label)\nplt.title('The number of elements in each class in the test data', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})\nplt.tight_layout()\nplt.show()","0448600f":"# Convert categorical variable (in our case: -1, 0, 1) into dummy\/indicator variables. Such as -1 to 1 0 0 \ntrain_label = pd.get_dummies(train_label)\ntest_label = pd.get_dummies(test_label)","3d0d6a5b":"train_label","041b946f":"lemmatizer = WordNetLemmatizer()\n\n# Function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None\n\n# Function below is necessery to find lemma of each word\ndef lemmatize_sentence(de_punct_sent):\n    # Tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(de_punct_sent))\n    # Tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            # If there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n        else:\n            # else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(lemmatized_sentence)","a283aaaf":"# Lemmatization of train and test data\ntrain_lem = [lemmatize_sentence(i) for i in train_data]\ntest_lem = [lemmatize_sentence(i) for i in test_data]","9895bbea":"samples_1 = train_lem\nsamples_2 = test_lem\n\ntokenizer = Tokenizer(num_words=25000) # 25 000 most frequently used words\ntokenizer.fit_on_texts(samples_1) # Creat an index of all words from training data\n\n# Convert strings to integer index lists\n\n# After that, we pass the ready-made (from the line above) indexes for all words from train\ntrain_data = tokenizer.texts_to_sequences(samples_1)\n\n# And for test. So, we should get the same word indexes in the two texts\ntest_data = tokenizer.texts_to_sequences(samples_2)\n\nword_index = tokenizer.word_index # Find out the calculated index of words\nprint('Found %s unique tokens.' % len(word_index))","14afcc46":"### The preprocessing stage is done. Now we have the training, test data and their respective labels:\n    \n# -train_data\n# -test_data\n\n# -train_label\n# -test_label","a9126a33":"# Now we want to find out what is the maximum number of words in a sentence. \n# After that, we will fill in the sentences with the missing number of words.\n\nlens =  [len(s) for s in train_data]\n\nplt.title('The maximum number of words in a sentence', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 17})\nplt.xlabel('The max number of words in a sentence', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})\nplt.ylabel('The number of sentences', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})\nplt.hist(lens,bins=200)\nplt.show()","3c35ffc9":"maxlen = 40 # maximal length of sentences\n\n# Convert lists of integers to a two-dimensional tensor \n# with integers and with a shape (samples, max. length)\n\ntrain_data = preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen, padding='post', truncating='post') \ntest_data = preprocessing.sequence.pad_sequences(test_data, maxlen=maxlen, padding='post', truncating='post')\n","64e3117d":"# Define some parameters for the model\nembedding_dim = 32 # Dimension of the dense embedding.\n\nvocab_inp_size = len(word_index) + 1 # Size of the vocabulary\n\nhidden_size = 256","485e7dd3":"model = Sequential() \n\n# The Embedding layer is necessery in order to convert words (in out case from integers) to vectors\nmodel.add(Embedding(input_dim = vocab_inp_size, output_dim = embedding_dim, input_length = maxlen))\n\n# Apply Dropout to use recurrent decimation to combat overfitting \nmodel.add(LSTM(hidden_size, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)) \n\n# Use Flatten() or GlobalMaxPooling1D() to convert 3D output to 2D.\n# It allows you to add one or more Dense layers to your model \nmodel.add(Flatten()) # or model.add(GlobalMaxPooling1D())\n\n#model.add(Dense(64, activation='relu'))#as experiment we tried to add one additional layer,but it is not give as higher accuracy\n#model.add(Dropout(0.4))\n\nmodel.add(Dense(3, activation='softmax'))#activation function for classification\n\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['categorical_accuracy']) \n\nhistory = model.fit(train_data, train_label, epochs=10, batch_size=128, validation_split=0.2)\n","f35fa06c":"acc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy'] \nloss = history.history['loss'] \nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc') \nplt.title('Training and validation accuracy') \nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","44f3e8df":"model.evaluate(test_data,test_label)","59757824":"# 2.3 Lemmatization","f8f15a0f":"# 4 Model Evaluation","0f06cbf0":"# Sentiment analysis of Coronavirus tweets with LSTM","dc546059":"# 3 Building Classification Model","fef805bc":"# 2.1 Label preparation","7a09394c":"# 2 Data preprocessing","577a1e89":"# 2.4 Tokenization","ca42a214":"# 3.1 Visualization of model training"}}