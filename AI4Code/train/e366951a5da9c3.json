{"cell_type":{"fa007074":"code","9ebc7299":"code","1c125ecb":"code","9e01332d":"code","9687dfb8":"code","0022de97":"code","1714a7d9":"code","7aa99ffb":"code","59f2b1fd":"code","f0248ca0":"code","6bb99c33":"code","3f74b1ca":"code","e9b3cd99":"code","b05733ab":"code","a00d772a":"code","f5cbb7df":"code","8c54bbb3":"code","73a90e73":"code","35c4f011":"code","f9cf57f9":"code","95a2cd0a":"code","6a842e93":"code","57741a53":"code","43e54ab7":"code","da7eaf7f":"code","5b43af6b":"code","025e030e":"code","57d74c5b":"code","6adcbf1c":"code","a30bfe8c":"code","e308069d":"code","b0d61bf6":"code","886f5066":"code","a245d49b":"code","29659d87":"code","3ae49712":"code","dc07239f":"code","1072dd3a":"code","c2597fbd":"code","918cc7be":"code","f40543a1":"code","891194ce":"code","c3cda51a":"markdown","133bcc8e":"markdown","306fe12d":"markdown","136b6572":"markdown","0b8a8134":"markdown","fadfc682":"markdown","f211dff3":"markdown","58809bef":"markdown","a412647f":"markdown","7cf3b3b9":"markdown","ac9c3092":"markdown","4896ba15":"markdown","cbcfd59d":"markdown","5e2e52e4":"markdown","67f72e50":"markdown","3b888441":"markdown","fbacf959":"markdown","549d1e24":"markdown","b38c0b20":"markdown","96772e69":"markdown","95cbdcac":"markdown","a503c997":"markdown","a2e099f1":"markdown","28ac0146":"markdown"},"source":{"fa007074":"import numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom imblearn.over_sampling import SMOTENC\n\nfrom IPython.display import Image\n\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);","9ebc7299":"Image(filename=\"\/kaggle\/input\/predictive-model-process\/predictive_modeling.png\")","1c125ecb":"# Import dataset\ndf = pd.read_csv('\/kaggle\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')","9e01332d":"# Check data types\ndf.dtypes","9687dfb8":"# Create data summary\ndf.describe()","0022de97":"# Check Churn rate\ndf[\"Exited\"].value_counts()","1714a7d9":"# Check the difference between stay v.s churn\ndf.groupby('Exited').agg('mean')","7aa99ffb":"# Check the impact of churned clients \ndf.groupby('Exited').agg('sum')","59f2b1fd":"# Drop RowNumber, CustomerId and Surname, which are not useful for modeling\ndf_ml = df.drop(['RowNumber','CustomerId','Surname'], axis = 1)","f0248ca0":"# Explore categorical\/binary data \nfig, axarr = plt.subplots(2, 2, figsize = (20, 12))\nsb.countplot(x = 'Geography', hue = 'Exited', data = df_ml, ax = axarr[0][0],palette='OrRd')\nsb.countplot(x = 'Gender', hue = 'Exited', data = df_ml, ax = axarr[0][1],palette='OrRd')\nsb.countplot(x = 'HasCrCard', hue = 'Exited', data = df_ml, ax = axarr[1][0],palette='OrRd')\nsb.countplot(x = 'IsActiveMember', hue = 'Exited', data = df_ml, ax = axarr[1][1],palette='OrRd');","6bb99c33":"# Explore continuous variables\ndef kdeplot(var):\n    facet = sb.FacetGrid(df_ml, hue = 'Exited', aspect = 3,palette='OrRd')\n    facet.map(sb.kdeplot, var, shade = True)\n    facet.set(xlim = (0, df_ml[var].max()))\n    facet.add_legend();\n    \nkdeplot('CreditScore')\nkdeplot('EstimatedSalary')\nkdeplot('Tenure')\nkdeplot('Age')\nkdeplot('Balance')\nkdeplot('NumOfProducts')","3f74b1ca":"# Check outliers\nfig, axarr = plt.subplots(3, 2, figsize = (20, 12))\nsb.boxplot(y = 'CreditScore',x = 'Exited', hue = 'Exited', data = df_ml, ax = axarr[0][0],palette='OrRd')\nsb.boxplot(y = 'Age',x = 'Exited', hue = 'Exited', data = df_ml , ax = axarr[0][1],palette='OrRd')\nsb.boxplot(y = 'EstimatedSalary',x = 'Exited', hue = 'Exited', data = df_ml, ax = axarr[1][0],palette='OrRd')\nsb.boxplot(y = 'Balance',x = 'Exited', hue = 'Exited', data = df_ml, ax = axarr[1][1],palette='OrRd')\nsb.boxplot(y = 'Tenure',x = 'Exited', hue = 'Exited', data = df_ml, ax = axarr[2][0],palette='OrRd')\nsb.boxplot(y = 'NumOfProducts',x = 'Exited', hue = 'Exited', data = df_ml, ax = axarr[2][1],palette='OrRd');","e9b3cd99":"# Check Multicollinearity \n\n# No highly linear correlation\nf, ax = plt.subplots(figsize= [15,10])\nsb.heatmap(df_ml.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"Blues\" );","b05733ab":"# Create new features\ndf_ml['CreditByAge'] = df_ml['CreditScore'] \/ df_ml['Age'] \ndf_ml['SalaryByAge'] = df_ml['EstimatedSalary'] \/ df_ml['Age'] \ndf_ml['TenureByAge'] = df_ml['Tenure'] \/ df_ml['Age'] \ndf_ml['BalanceByAge'] = df_ml['Balance'] \/ df_ml['Age'] ","a00d772a":"kdeplot('CreditByAge')\nkdeplot('SalaryByAge')\nkdeplot('TenureByAge')\nkdeplot('BalanceByAge')","f5cbb7df":"# One-Hot encoding our categorical attributes\ncat_vars = ['Geography']\ndf_ml = pd.get_dummies(df_ml, columns = cat_vars, prefix = cat_vars)","8c54bbb3":"# Convert Gender to female:1, male:0\ndf_ml['Gender_Female'] = np.where(df_ml['Gender'] == 'Female', 1, 0)","73a90e73":"num_vars = ['CreditByAge','SalaryByAge','TenureByAge','BalanceByAge','NumOfProducts']\nbin_vars = ['HasCrCard','IsActiveMember','Geography_France','Geography_Germany','Geography_Spain','Gender_Female']","35c4f011":"# Split data into train and test sets\nseed = 7\ntest_size = 0.3\n\nX = df_ml[num_vars + bin_vars]\nY = df_ml['Exited']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)","f9cf57f9":"# Scale continuous variables \nscaler = MinMaxScaler()\nX_train_transform = scaler.fit_transform(X_train)\nX_test_tranform = scaler.transform(X_test)","95a2cd0a":"def model_train_eval(model, X_train, y_train, X_test, y_test):\n    model_fit = model.fit(X_train, y_train)\n    \n    pred_prob = model_fit.predict_proba(X_test)[:,1]\n    pred = model_fit.predict(X_test)\n    \n    f1 = f1_score(y_test.values, pred, average = 'macro')\n    auc = roc_auc_score(y_test.values,pred_prob) \n    \n    fpr, tpr, _ = roc_curve(y_test, pred_prob)\n    \n    return model_fit, f1, auc, fpr, tpr","6a842e93":"def base_models(X_train, y_train, X_test, y_test, cv, scoring):\n    model_list = []\n    model_name_list = []\n    f1_list = []\n    auc_list = []\n    cv_mean_list = []\n    cv_std_list = []\n    fpr_list = []\n    tpr_list = []\n    \n    model_list.append(('KNN', KNeighborsClassifier(n_neighbors = 7)))\n    model_list.append(('AdaBoost', AdaBoostClassifier(base_estimator = None, n_estimators = 200,\n                                                      learning_rate = 0.2)))\n    model_list.append(('GradientBoost', GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 200)))\n    model_list.append(('XGBoost', XGBClassifier(booster='gbtree', eta = 0.1, gamma = 0.01, \n                                                objective = 'binary:logistic', eval_metric = 'auc')))\n    model_list.append(('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy',\n                                                               class_weight = 'balanced')))\n    \n    for name, model in model_list:\n        model_fit, f1, auc, fpr, tpr = model_train_eval(model, X_train, y_train, X_test, y_test)\n        cv_score = cross_val_score(model_fit, X_train, y_train, cv = cv, scoring = scoring)\n\n        model_name_list.append(name)\n        f1_list.append(f1)\n        auc_list.append(auc)\n        cv_mean_list.append(cv_score.mean())\n        cv_std_list.append(cv_score.std())\n        fpr_list.append(fpr)\n        tpr_list.append(tpr)\n\n    performance = {'CV_Mean': cv_mean_list, 'CV_Std': cv_std_list, 'Test F1': f1_list, 'Test AUC': auc_list}\n    pf_metrics = pd.DataFrame(performance, index = model_name_list)\n    \n    #------ Plot ROC ------#      \n    plt.figure(figsize = (12,6), linewidth= 1)\n    for i in range(len(model_name_list)):\n        plt.plot(fpr_list[i], tpr_list[i], label = model_name_list[i]+': '+ str(round(auc_list[i], 3)))\n    plt.plot([0,1], [0,1], 'k--', label = 'Random guessing: 0.5')\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC Curve ')\n    plt.legend(loc='best')\n    plt.show()\n    \n    return pf_metrics","57741a53":"base_models(X_train_transform, y_train, X_test_tranform, y_test, 5, 'f1_macro')","43e54ab7":"# Create a balanced training dataset \nsm = SMOTENC(list(range(len(num_vars),10)),random_state = 101)\nX_train_over, y_train_over = sm.fit_resample(X_train_transform, y_train)","da7eaf7f":"base_models(X_train_over, y_train_over, X_test_tranform, y_test, 5, 'f1_macro')","5b43af6b":"#### Tuning based on oversampled data ####\ngb_Params = {'learning_rate' : [0.01, 0.05, 0.1, 0.15, 0.2],\n             'n_estimators': list(range(50, 300, 50)),\n             'min_samples_split': [60, 80, 100, 120],\n             'min_samples_leaf':[30, 40, 50, 60],\n             'max_depth': [5, 6, 7, 8]}\n\n# Initialization\ngbModelOver = GradientBoostingClassifier(random_state = 1234)\n\nrandSearchGbOver = RandomizedSearchCV(estimator = gbModelOver, param_distributions = gb_Params, n_iter = 100, \n                                      verbose=1, scoring = 'f1_macro', cv = 3, random_state = 1234)\n# Fit model with oversampled data\nrandSearchGbOver.fit(X_train_over, y_train_over)\n\n# Print best parameters and best score\nrandSearchGbOver.best_params_, randSearchGbOver.best_score_","025e030e":"#### Tuning based on orginal data ####\ngb_Params = {'learning_rate' : [0.01, 0.05, 0.1, 0.15, 0.2],\n             'n_estimators': list(range(50, 300, 50)),\n             'min_samples_split': [60, 80, 100, 120],\n             'min_samples_leaf':[30, 40, 50, 60],\n             'max_depth': [5, 6, 7, 8]}\n\n# Initialization\ngbModel = GradientBoostingClassifier(random_state = 1234)\n\nrandSearchGB = RandomizedSearchCV(estimator = gbModel, param_distributions = gb_Params, n_iter = 100, verbose=1,\n                                   scoring = 'f1_macro', cv = 3, random_state = 1234)\n# Fit model\nrandSearchGB.fit(X_train_transform, y_train)\n\n# Print best parameters and best score\nrandSearchGB.best_params_, randSearchGB.best_score_","57d74c5b":"def f1_auc(model, model_name, x_train, y_train, x_test, y_test):\n    bestModel = model.best_estimator_.fit(x_train, y_train)\n    \n    bestPredProb = bestModel.predict_proba(x_test)[:,1]\n    bestPred = bestModel.predict(x_test)\n    \n    bestAUC = roc_auc_score(y_test.values,bestPredProb)\n    bestF1 = f1_score(y_test.values, bestPred, average = 'macro')\n    \n    print('{} - F1: {:.3}, AUC: {:3}'.format(model_name, bestF1, bestAUC))\n    \n    return bestPred, bestModel","6adcbf1c":"# Evaluate models on test dataset\nbestBGPredOver, bestBGModelOver = f1_auc(randSearchGbOver,'GBboost + SMOTENC', X_train_over, y_train_over, X_test_tranform, y_test )\n\n\nbestBGPred, bestBGModel = f1_auc(randSearchGB,'GBboost', X_train_transform, y_train, X_test_tranform, y_test )","a30bfe8c":"# Create classification report\nprint('Classfication report for oversampled dataset')\nprint(classification_report(y_test.values, bestBGPredOver))\n\nprint('\\nClassfication report for orginal dataset')\nprint(classification_report(y_test.values, bestBGPred))","e308069d":"def feature_imp(model):\n    feature = pd.Series(model.feature_importances_, index = X.columns).sort_values(ascending = False)\n\n    sb.barplot(x = feature, y = feature.index)\n    plt.xlabel('Features')\n    plt.ylabel('Importance')\n    plt.show()","b0d61bf6":"feature_imp(bestBGModelOver)","886f5066":"feature_imp(bestBGModel)","a245d49b":"#### Tuning based on oversampled data ####\nxg_Params = {'eta' : [0.01, 0.05, 0.1, 0.15, 0.2],\n             'gamma': [0, 0.01,0.02],\n             'reg_alpha': [0, 0.5, 1],\n             'reg_lambda': [1, 1.5, 2],\n             'subsample': [0.6, 0.8, 1],\n             'n_estimators': list(range(50, 500, 50)),\n             'max_depth': [5, 6, 7, 8],\n             'min_child_weight': [0.5, 1.0, 3.0, 5.0]}\n\n# Initialization\nxgModelOver = XGBClassifier(booster='gbtree', objective = 'binary:logistic', eval_metric = 'auc', seed = 123)\n\nrandSearchXgOver = RandomizedSearchCV(estimator = xgModelOver, param_distributions = xg_Params, n_iter = 100, \n                                      verbose=1,scoring = 'f1_macro', cv = 3, random_state = 1234)\n\n# Fit model with oversampled data\nrandSearchXgOver.fit(X_train_over, y_train_over)\n\n# Print best parameters and best score\nrandSearchXgOver.best_params_, randSearchXgOver.best_score_","29659d87":"#### Tuning based on orginal data ####\nxg_Params = {'eta' : [0.01, 0.05, 0.1, 0.15, 0.2],\n             'gamma': [0, 0.01,0.02],\n             'reg_alpha': [0, 0.5, 1],\n             'reg_lambda': [1, 1.5, 2],\n             'subsample': [0.6, 0.8, 1],\n             'n_estimators': list(range(50, 500, 50)),\n             'max_depth': [5, 6, 7, 8],\n             'min_child_weight': [0.5, 1.0, 3.0, 5.0]}\n\n# Initialization\nxgModel = XGBClassifier(booster='gbtree', objective = 'binary:logistic', eval_metric = 'auc', seed = 123)\n\nrandSearchXG = RandomizedSearchCV(estimator = xgModel, param_distributions = xg_Params, n_iter = 100, verbose=1,\n                                   scoring = 'f1_macro', cv = 3, random_state = 1234)\n\n# Fit model\nrandSearchXG.fit(X_train_transform, y_train)\n\n\n# Print best parameters and best score\nrandSearchXG.best_params_, randSearchXG.best_score_","3ae49712":"# Evaluate models on test dataset\nbestXGPredOver, bestXGModelOver = f1_auc(randSearchXgOver,'XGboost + SMOTENC', X_train_over, y_train_over, X_test_tranform, y_test )\nbestXGPred, bestXGModel = f1_auc(randSearchXG,'XGboost', X_train_transform, y_train, X_test_tranform, y_test )","dc07239f":"# Create classification report\nprint('Classfication report for oversampled dataset')\nprint(classification_report(y_test.values, bestXGPredOver))\n\nprint('\\nClassfication report for orginal dataset')\nprint(classification_report(y_test.values, bestXGPred))","1072dd3a":"feature_imp(bestXGModelOver)","c2597fbd":"feature_imp(bestXGModel)","918cc7be":"# Concate X_train_transform and X_test_tranform\nX_transform = np.concatenate((X_train_transform,X_test_tranform), axis=0) \nY = np.concatenate((y_train,y_test), axis=0) ","f40543a1":"# Use the best parameter to fit\/predict entire dataset\nfinalModel = randSearchGbOver.best_estimator_.fit(X_transform, Y)\n\nfinalPredProb = finalModel.predict_proba(X_transform)[:,1]\nfinalPred = finalModel.predict(X_transform)\n\nfinalAUC = roc_auc_score(Y,finalPredProb)\nfinalF1 = f1_score(Y, finalPred, average = 'macro')\n\nprint('Final model results - F1: {:.3}, AUC: {:3}'.format(finalF1, finalAUC))","891194ce":"# Check final prediction output \nconfusion_matrix(Y, finalPred)","c3cda51a":"Clients who left have lower credit score, higher avg age, lower tenure, larger balance, lower # of prods, lower credit card%, lower active% and higher salary","133bcc8e":"Based on the bar charts:\n* 'Germany', 'Female', 'No Credit Card' and 'Not Active' groups have higher churn rates. \n* To understand why 'Germany' and 'Female' groups have higher churn rates, I would recommend the bank to perform deeper analysis to see whether clients in those groups have different behaviors\/perferences compared to other groups, then design some specific loyalty programs to better engage with clients in those two groups\n* Because clients who have credit card and are digitally active have lower churn rates, I would recommend the bank to run campaigns to promote its credit card and digital services ","306fe12d":"According to the SMOTENC results, the oversampling method improves the CV F1 score, but I also noticed that the results based on CV is better than the results based on Test data, so there is overfitting issue using SMOTENC\n\nBecause GradientBoost and XGBoost models outperform other models in both situations, so I choose those two models for further parameter tuning","136b6572":"**Data enrichment**\n* Historical investment balance and product data: when clients are planning to leave, they will start withdrawing money from accounts and terminating their products\/services, this trend is a good indicator for client attribution\n* Client survey scores and comments: survey results can help CWM understand whether its clients are satisfied with the services\n* Client engagement from different footprints: tracking clients footprints from different sources, such as website, email, mail, social media, etc., can help CWM understand how different client segments interact with different channels and which channels have better client retention rates\n\n**Other feature engineering methods**\n* Try different data transformation methods, such as ratio, log transform, polynomial transformation, etc., to capture non-linear relationships between dependent and independent variables\n\n**Other Machine Learning models**\n* Try other classifications models, such as SVM, Neural Network, etc., to find potential opportunities to improve prediction results","0b8a8134":"# **Model Training**","fadfc682":"My assumption is that credit score, salary, tenure and investment balance are correlated with age, so I want to check the true impacts of those 4 variables on the churn rate after controling age ","f211dff3":"# **Feature Engineering**","58809bef":"# **Recommendations**","a412647f":"## **GradientBoost**","7cf3b3b9":"# **Final Model Evaluation on Entire Dataset**","ac9c3092":"Client attrition results in $186M investment balance loss and 3,005 products loss","4896ba15":"Based on the summary, all variables have 10,000 values, there are no missing values ","cbcfd59d":"## **XGBoost**","5e2e52e4":"# **Prepare Train\/Test Data for Modeling**","67f72e50":"# **Oversampling: SMOTENC**","3b888441":"Churn rate is ~20%, based on the %, it's an relatively unbalanced dataset","fbacf959":"Based on the exploratory analysis above, the churned clients result in $186M balance loss and 3,005 product loss, so the capability to correctly classify churn clients will be the most important metric I am looking for. That's the reason why I choose Recall(class=1) to measure the model performance.\n\nAfter comparing the classification reports of the four models, the XGboost + SMOTENC has the highest Recall score, so I choose this combination as my final model.","549d1e24":"# **Model Tuning**","b38c0b20":"# **Exploratory Analysis and Data Preprocessing**","96772e69":"# **Quick Data Review**","95cbdcac":"Based on the distributions, I can conclude that:\n* If two clients are at the same age, the client who has lower credit score, lower salary, shorter tenure and lower balance have higher churn rate\n* The clients who meet the above criterias are highly likely that they are falling behind their financial planning. I would recommend the bank to develop certain programs to help those clients catch up with their financial planning, by doing so, the bank can deepen relationship with those clients","a503c997":"# **Content**\n1. Data Review\n2. Exploratory Analysis\n3. Feature Engineering\n4. Prepare Train\/Test Data for Modeling\n5. Modeling\n6. SMOTENC\n7. Model Tuning\n8. Final Evaluation on the Entire Dataset\n9. Recommendations","a2e099f1":"Althogh there are points sitting outside of the upper and lower boundaries, but they are within the normal range based on common sense. For example: age is within [18,100], credit score is within [350, 850]. so I won't make any modification to those 'outliers'","28ac0146":"Based on the kernel distributions:\n* There are no significant differences in CreditScore, EstimatedSalary and Tenure distributions between 'exited' and 'not exited' clients. \n* Clients with higher Age have higher churn rates.\n* Clients with higher Balance and higher NumofProducts have higher churn rates. This result may indicates that the clients have purchased more products than they need. Then when clients realized that, the trust between the clients and the bank would break and the clients would leave the bank. So I would recommend the bank to evaluate its sales strategy to balance the short-term profits v.s long-time client lifetime value."}}