{"cell_type":{"61f059a7":"code","806e3eb8":"code","a79a546d":"code","4d3f891d":"code","4c4fbd73":"code","1361a64d":"code","6c0744d7":"code","76aa63cb":"code","9aacdc28":"code","4d9eb35c":"code","d774ae6e":"code","eb12bc8b":"code","e9bd3788":"code","62a032d1":"code","4aea8d3f":"code","196ae410":"code","4b4f4b6f":"code","e23daed5":"code","ed37dc00":"code","caadd797":"code","2cc64602":"code","0ef14a81":"code","30f211eb":"code","7ad043c5":"code","ddab0789":"markdown","e8c0c862":"markdown","e2c6c407":"markdown","a20c6b0a":"markdown","3861e0bc":"markdown","b07796b0":"markdown","32c29b2a":"markdown","4cfe132a":"markdown","38dbf44f":"markdown","e43c9b54":"markdown","a9820999":"markdown","10088352":"markdown","ce84ed42":"markdown","25e8eba8":"markdown","ff6f0c8a":"markdown","e3796bcd":"markdown","c0fc76a0":"markdown","7ca87736":"markdown","5bf1b912":"markdown","fa940d7c":"markdown","e4f19823":"markdown","7724e433":"markdown","614e492b":"markdown","c302c74c":"markdown","cc4ac87c":"markdown","f45aff17":"markdown","5bf46d04":"markdown","9b23e5f9":"markdown","b6dce807":"markdown"},"source":{"61f059a7":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport os","806e3eb8":"train_tfrecords_dir = \"..\/input\/five-flowers\/tfrecords-jpeg-512x512\"\ntfrec_filenames = [os.path.join(train_tfrecords_dir, filename) for filename in os.listdir(train_tfrecords_dir)]","a79a546d":"train_raw_dataset = tf.data.TFRecordDataset(tfrec_filenames)","4d3f891d":"feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n    'class': tf.io.FixedLenFeature([], tf.int64)\n}\n\ndef _parse_image_function(example_proto):\n    features = tf.io.parse_single_example(example_proto, feature_description)\n    \n    image = tf.io.decode_image(features['image'])\n    image = tf.reshape(image, (512, 512, 3))\n    image = tf.cast(image, tf.float32)\n    image \/= 255.0\n    \n    label = tf.cast(features['class'], tf.int32)\n    \n    return image, label","4c4fbd73":"for image, label in train_raw_dataset.map(_parse_image_function).take(1):\n    print(image.shape)\n    print(label)","1361a64d":"train_pipeline = train_raw_dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32).prefetch(1)\ntrain_pipeline.element_spec","6c0744d7":"filter_kernel1 = [[-1, -1, -1, -1],\n                [-1, -1, -1, -1],\n                [1, 1, 1, 1],\n                [1, 1, 1, 1]]\n\nfilter_kernel2 = [[1, 1, 1, 1],\n                [1, 1, 1, 1],\n                [-1, -1, -1, -1],\n                [-1, -1, -1, -1]]\n\nfilter_kernel3 = [[-1, -1, 1, 1],\n                [-1, -1, 1, 1],\n                [-1, -1, 1, 1],\n                [-1, -1, 1, 1]]\n\nfilter_kernel4 = [[1, 1, -1, -1],\n                [1, 1, -1, -1],\n                [1, 1, -1, -1],\n                [1, 1, -1, -1]]\n\nfilter_kernel5 = [[0, -1, 0],\n                [-1, 4, -1],\n                [0, -1, 0]]\n\nfilter_kernel6 = [[-1, -1, -1],\n                [0, 0, 0],\n                [1, 1, 1]]\n\nfilter_kernel7 = [[-1, 0, 1],\n                [-1, 0, 1],\n                [-1, 0, 1]]\n\nplt.subplot(3,3,1)\nplt.imshow(filter_kernel1, cmap=\"gray\")\nplt.subplot(3,3,2)\nplt.imshow(filter_kernel2, cmap=\"gray\")\nplt.subplot(3,3,3)\nplt.imshow(filter_kernel3, cmap=\"gray\")\nplt.subplot(3,3,4)\nplt.imshow(filter_kernel4, cmap=\"gray\")\nplt.subplot(3,3,5)\nplt.imshow(filter_kernel5, cmap=\"gray\")\nplt.subplot(3,3,6)\nplt.imshow(filter_kernel6, cmap=\"gray\")\nplt.subplot(3,3,7)\nplt.imshow(filter_kernel7, cmap=\"gray\")","76aa63cb":"gray_image = np.dot(image, [0.2989, 0.5870, 0.1140])\nplt.imshow(gray_image, cmap='gray')\ngray_image.shape","9aacdc28":"plt.figure(figsize=(20,10))\n\nfor i,filter_kernel in enumerate([filter_kernel1, filter_kernel2, filter_kernel3, filter_kernel4, filter_kernel5, filter_kernel6, filter_kernel7]):\n    x = tf.constant(np.reshape(gray_image,(1,512,512,1)), dtype=tf.float32)\n    filter_kernel = np.array(filter_kernel)\n    kernel_in = np.reshape(filter_kernel, (*filter_kernel.shape,1,1))\n\n    kernel = tf.constant(kernel_in, dtype=tf.float32)\n    out_image = tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding='SAME')\n    \n    plt.subplot(3,6, 2*i+1)\n    plt.imshow(filter_kernel, cmap='gray')\n    plt.title(\"Filter Kernel \"+str(i+1))\n    plt.xticks([])\n    plt.yticks([])\n    \n    plt.subplot(3,6, 2*i+2)\n    plt.imshow(np.reshape(out_image, (512,512)), cmap='gray')\n    plt.title(\"Kernel Output \"+str(i+1))\n    plt.xticks([])\n    plt.yticks([])","4d9eb35c":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, 5, activation='relu', padding='same', input_shape=(512, 512, 3)),\n    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n    tf.keras.layers.Conv2D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n    tf.keras.layers.Conv2D(128, 5, activation='relu'),\n    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n    tf.keras.layers.Conv2D(256, 5, activation='relu'),\n    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(5, activation='softmax')\n])","d774ae6e":"model.summary()","eb12bc8b":"model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam',metrics=['accuracy'])","e9bd3788":"history = model.fit(train_pipeline, epochs=10)","62a032d1":"def print_image_summary(conv2d_image, cols=8):\n    channels = conv2d_image.shape[-1]\n    images = conv2d_image[0]\n    rows = channels \/\/ cols\n    plt.figure(figsize=(cols*2,rows*2))\n    for i in range(channels):\n        plt.subplot(rows,cols,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(images[:,:,i], cmap='gray')\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.show()","4aea8d3f":"for image, label in train_raw_dataset.map(_parse_image_function).take(1):\n    pass\nimage = tf.reshape(image, (-1, *image.shape))","196ae410":"plt.imshow(image[0])\nprint_image_summary(image, cols=3)","4b4f4b6f":"conv2d_layer1 = model.layers[0]\nmaxpool_layer1 = model.layers[1]\nconv2d_layer2 = model.layers[2]\nconv2d_image = conv2d_layer1(image)\nprint_image_summary(conv2d_image)","e23daed5":"conv2d_image = conv2d_layer1(image)\nconv2d_image = maxpool_layer1(conv2d_image)\nconv2d_image = conv2d_layer2(conv2d_image)\nprint_image_summary(conv2d_image)","ed37dc00":"conv2d_image = image\nfor layer in model.layers[:8]:\n    conv2d_image = layer(conv2d_image)\nprint_image_summary(conv2d_image)","caadd797":"for image, label in train_raw_dataset.map(_parse_image_function).skip(1).take(1):\n    pass\nimage = tf.reshape(image, (-1, *image.shape))","2cc64602":"plt.imshow(image[0])","0ef14a81":"conv2d_image = conv2d_layer1(image)\nprint_image_summary(conv2d_image)","30f211eb":"conv2d_image = conv2d_layer1(image)\nconv2d_image = maxpool_layer1(conv2d_image)\nconv2d_image = conv2d_layer2(conv2d_image)\nprint_image_summary(conv2d_image)","7ad043c5":"conv2d_image = image\nfor layer in model.layers[:8]:\n    conv2d_image = layer(conv2d_image)\nprint_image_summary(conv2d_image)","ddab0789":"# Visualizing Conv2d Layers","e8c0c862":"# Overview","e2c6c407":"I will be using a dataset of five-flowers for demonstration in this notebok","a20c6b0a":"### Conv2d_2","3861e0bc":"Kernel should have same depth as Image. For RGB image, our kernel should be of shape HxWx3, where H and W are height and width of kernel and 3 is the depth our Image.\n\nFor demonstration purpose we will convert our Image to grayscale for easy understanding","b07796b0":"### Conv2d_2","32c29b2a":"We can run the conv2d layers from our model by directly calling them using our input image. We need to make sure that  we follow the same order which we defined in our model above.","4cfe132a":"A Kernel(Filter) in Conv2d layers is convolved with the layer input to produce a tensor of output. Every Filter creates one output. Outputs from all the filters in a Conv2d layer form the depth of the output tensor.","38dbf44f":"# Training Pipelien","e43c9b54":"# Libraries","a9820999":"If we compare our outputs with previous image, we can see that few different kernels are responsible for detecting feature in this image. There may be kernels which may work for multiple images","10088352":"### Conv2d_1","ce84ed42":"As you can see our first conv2d layers tries to find some features from the image and only few kernels are responsible for finding those features. The remaining kernels show all blank image as they are responsible for detecting features which are not available in our selected image. They will become active when we use different flower image.","25e8eba8":"# Model","ff6f0c8a":"This layer tries to find patterns in our previous layer output.","e3796bcd":"# Training Data","c0fc76a0":"### Conv2d_All","7ca87736":"### Conv2d_All","5bf1b912":"### Conv2d_1","fa940d7c":"This below function visualizes each kernel output (depth of our image after conv2d) in a grid","e4f19823":"## Image 1","7724e433":"Lets try to use a CNN model and let our model find features in our dataset. We will then use the CNN layers in this model to visualize the kernel output and see what our model thinks is important for classification of our data","614e492b":"Now lets plot the kernel and its output when applied on the above image. If zoom the images, you will see that our kernels actually finds the edges in image. Edges are used as higher level features when using CNN as classifier.","c302c74c":"Lets us see what happens when our image is passed through all the feature detection layers(conv2d layers) in our model","cc4ac87c":"## Image 2","f45aff17":"This notebook shows what happens to a Image when it passes through convolution layers.","5bf46d04":"# What single Conv2d Filter does to Image","9b23e5f9":"Let us try to use a different Image so we can see the output from kernels which did not show anything in our current image","b6dce807":"Below we will use some filters to see what it does.\n\nAll the filters provided below are called edge filters. Edge filters tries to detect the edges in a image. All the values in a edge filter adds up to Zero"}}