{"cell_type":{"a69cf835":"code","505675ed":"code","9bfff64b":"code","7d63ba49":"code","d141a87a":"code","9dfc2408":"code","e2dd68b2":"code","07c5d54b":"code","d1b180ae":"code","3f878598":"code","d2bd58a4":"markdown","7f8723a9":"markdown","b4cb3d27":"markdown","cdfb4c2c":"markdown","1d72e859":"markdown","33963390":"markdown"},"source":{"a69cf835":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.model_selection import train_test_split,validation_curve","505675ed":"#load data\ndf = pd.read_csv('https:\/\/archive.ics.uci.edu\/ml\/'\n                 'machine-learning-databases'\n                 '\/breast-cancer-wisconsin\/wdbc.data', header=None)\ndf.head()","9bfff64b":"df.shape","7d63ba49":"df.loc[:, 1].unique()","d141a87a":"# split input data and input label\nX = df.loc[:, 2:].values\ny = df.loc[:, 1].values","9dfc2408":"# split train set and test set \nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, \n                     test_size=0.20,\n                     stratify=y,\n                     random_state=1)","e2dd68b2":"# make model by pipeline \npipe_lr = make_pipeline(StandardScaler(),\n                        LogisticRegression(penalty='l2', random_state=1,\n                                           max_iter=10000))","07c5d54b":"param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(\n                estimator=pipe_lr, # model \n                X=X_train,   # input data\n                y=y_train,   # input label\n                param_name='logisticregression__C',  # logistic regression C param \n                param_range=param_range, # parameter validation range\n                cv=10) # cross validation ","d1b180ae":"# train mean score , train std , test mean score , test std \ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)","3f878598":"# train mean \nplt.plot(param_range, train_mean, \n         color='blue', marker='o', \n         markersize=5, label='Training accuracy')\n\n# train mean score - std ~ train mean score + std\nplt.fill_between(param_range, train_mean + train_std,\n                 train_mean - train_std, alpha=0.15,\n                 color='blue')\n\n#test mean\nplt.plot(param_range, test_mean, \n         color='green', linestyle='--', \n         marker='s', markersize=5, \n         label='Validation accuracy')\n\n# test mean score - std ~ test mean score + std\nplt.fill_between(param_range, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color='green')\n\nplt.grid()\nplt.xscale('log')\nplt.legend(loc='lower right')\nplt.xlabel('Parameter C')\nplt.ylabel('Accuracy')\nplt.ylim([0.8, 1.0])\nplt.tight_layout()\n# plt.savefig('images\/06_06.png', dpi=300)\nplt.show()","d2bd58a4":"# draw validation curve","7f8723a9":"# get validation curve data","b4cb3d27":"# make model","cdfb4c2c":"# split data","1d72e859":"# get mean score and std ","33963390":"## == > optimal C : 10 "}}