{"cell_type":{"bd15110b":"code","f71c421b":"code","18573af4":"code","714ed61f":"code","43b79d52":"code","532a52b0":"code","c8daccb9":"code","de6e0c2f":"code","a09b7922":"code","cd296c72":"code","604e103a":"code","aa3a391d":"code","36851751":"code","3b1a074a":"code","80ac9602":"code","0eccd0c8":"markdown","a94895a9":"markdown","b7f718fc":"markdown"},"source":{"bd15110b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f71c421b":"train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = train.query('weight>0').reset_index(drop=True)\ntrain.fillna(train.median(),inplace=True)\ntrain['feature_stock_id_sum'] = train['feature_41'] + train['feature_42'] + train['feature_43']\ntrain['feature_1_2_cross'] = train['feature_1']\/(train['feature_2']+1e-5)\nNUM_TRAIN_EXAMPLES = len(train)","18573af4":"features = [c for c in train.columns if 'feature' in c]\nf_mean = np.nanmean(train[features[1:]].values,axis=0)","714ed61f":"#PATH = \"..\/input\/\"\nPATH ='..\/input\/neutralizing2'\n#PATH = '..\/input\/densenetneu'","43b79d52":"resp_cols = [c for c in train.columns if 'resp' in c]","532a52b0":"import tensorflow_addons as tfa\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\ndef mish(x):\n    return tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n\ntf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n\ndef create_model(input_shape):\n    \n    inp = tf.keras.layers.Input(input_shape)\n    tmp = tf.keras.layers.BatchNormalization()(inp)\n    xs = [tmp]\n    for _ in range(10):\n        if len(xs) > 1:\n            tmp = tf.keras.layers.Concatenate(axis=-1)(xs)\n        else:\n            tmp = xs[0]\n        tmp = tf.keras.layers.Dense(64,activation='mish')(tmp)\n        tmp = tf.keras.layers.BatchNormalization()(tmp)\n        tmp = tf.keras.layers.Dropout(0.2)(tmp)\n        xs.append(tmp)\n    \n    output = tf.keras.layers.Dense(len(resp_cols),activation='sigmoid')(tf.keras.layers.Concatenate()(xs))\n    model = tf.keras.models.Model(inp,output)\n    optimizer = tfa.optimizers.RectifiedAdam(1e-3)\n    model.compile(optimizer, loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001),\n                    metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n    return model","c8daccb9":"import random\nimport os\ndef set_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)","de6e0c2f":"#Designed to do all features at the same time, but Kaggle kernels are memory limited.\nclass NeutralizeTransform:\n    def __init__(self,proportion=1.0):\n        self.proportion = proportion\n    \n    def fit(self,X,y):\n        self.lms = []\n        self.mean_exposure = np.mean(y,axis=0)\n        self.y_shape = y.shape[-1]\n        for x in X.T:\n            scores = x.reshape((-1,1))\n            exposures = y\n            exposures = np.hstack((exposures, np.array([np.mean(scores)] * len(exposures)).reshape(-1, 1)))\n            \n            transform = np.linalg.lstsq(exposures, scores, rcond=None)[0]\n            self.lms.append(transform)\n            \n    def transform(self,X,y=None):\n        out = []\n        for i,transform in enumerate(self.lms):\n            x = X[:,i]\n            scores = x.reshape((-1,1))\n            exposures = np.repeat(self.mean_exposure,len(x),axis=0).reshape((-1,self.y_shape))\n            exposures = np.concatenate([exposures,np.array([np.mean(scores)] * len(exposures)).reshape((-1,1))],axis=1)\n            correction = self.proportion * exposures.dot(transform)\n            out.append(x - correction.ravel())\n            \n        return np.asarray(out).T\n    \n    def fit_transform(self,X,y):\n        self.fit(X,y)\n        return self.transform(X,y)\n        \n","a09b7922":"TRAINING = False","cd296c72":"%%time\nif TRAINING:\n    mask = train[features].isna()\n    train.fillna(0,inplace=True)\n    for feature in features:\n        nt = NeutralizeTransform(proportion=0.25)\n        train[feature] = nt.fit_transform(train[feature].values.reshape((-1,1)),\n                                          train['resp'].values.reshape((-1,1)))\n        pd.to_pickle(nt,f'NeutralizeTransform_{feature}.pkl')\n    train[mask] = np.nan\n    \nelse:\n    nts = []\n    for feature in features:\n        nt = pd.read_pickle(f'{PATH}\/NeutralizeTransform_{feature}.pkl')\n        nts.append(nt)","604e103a":"import gc\ngc.collect()","aa3a391d":"TRAINING = False","36851751":"# X_tr = train.query('date<440')[features].values\n# y_tr = (train.query('date<440')[resp_cols].values > 0).astype(int)\n    \n# X_val = train.query('date>460')[features].values\n# y_val = (train.query('date>460')[resp_cols].values > 0).astype(int)\n\nX_tr = train.query('date<350')[features].values\ny_tr = (train.query('date<350')[resp_cols].values > 0).astype(int)\n    \nX_val = train.query('date>400')[features].values\ny_val = (train.query('date>400')[resp_cols].values > 0).astype(int)\n\ndel train\ngc.collect()\n\n\nif TRAINING:\n    metric = {}\n    \n    #for seed in [6, 28, 496, 8128]:\n    for seed in [2020,1982]:\n        set_all_seeds(seed)\n\n        model = create_model(X_tr.shape[-1])\n        hist = model.fit(X_tr,y_tr,\n                         validation_data=(X_val,y_val),\n                         epochs=200,\n                         batch_size=8192,\n                         callbacks=[tf.keras.callbacks.EarlyStopping('val_binary_accuracy',mode='max',patience=20,restore_best_weights=True),\n                                   tf.keras.callbacks.ReduceLROnPlateau('val_binary_accuracy',mode='max',patience=10,cooldown=5)])\n        \n        model.save_weights(f'model_{seed}.tf')\n        metric[seed] = max(hist.history['val_binary_accuracy'])\n        \n        tf.keras.backend.clear_session()\n        \n    print(metric)        \nelse:\n    models = []\n    #for seed in [6, 28, 496, 8128]:\n    #for seed in [6,8128]:\n    for seed in [2020,1982]:\n        model = create_model(X_tr.shape[-1])\n        model.load_weights(f'{PATH}\/model_{seed}.tf')\n        model.call = tf.function(model.call, experimental_relax_shapes=True)\n        models.append(model)\n","3b1a074a":"# test_df_columns = ['weight'] + [f'feature_{i}' for i in range(130)] + ['date']\n# features = [c for c in train.columns if 'feature' in c]\n# #features = feat_cols\n# index_features = [n for n,col in enumerate(test_df_columns) if col in features]","80ac9602":"from tqdm import tqdm\nif not TRAINING:\n    f = np.median\n    import janestreet\n    #janestreet.competition.make_env.__called__ = False\n    env = janestreet.make_env()\n    #th = 0.50\n    th = 0.495\n    for (test_df, pred_df) in tqdm(env.iter_test()):\n        #if test_df['weight'].item() > 0:\n        if test_df['weight'].values[0] > 0:\n            \n            test_df['feature_stock_id_sum'] = test_df['feature_41'] + test_df['feature_42'] + test_df['feature_43']\n            test_df['feature_1_2_cross'] = test_df['feature_1']\/(test_df['feature_2']+1e-5)\n            \n            x_tt = test_df.loc[:, features].values\n            #x_tt = test_df.values[0][index_features].reshape(1, -1)\n            if np.isnan(x_tt[:, 1:].sum()):\n                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n \n            for i in range(len(nts)):\n                x_tt[:,i] = nts[i].transform(np.expand_dims(x_tt[:,i],0))\n            \n            p = f(np.mean([model(x_tt,training=False).numpy() for model in models],axis=0))\n    \n            pred_df.action = np.where(p > th, 1, 0).astype(int)\n        else:\n            pred_df[\"action\"].values[0] = 0\n        env.predict(pred_df)","0eccd0c8":"# 1st, I would like to say thank you to  \n \n@snippsy\n@code1110\n\nand eveyone share an ideas helped to make this code","a94895a9":"One of the adavantages of DenseNet is the ability to have a deep narrow network without loss in performance.","b7f718fc":"See https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/215305"}}