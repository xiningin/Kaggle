{"cell_type":{"e031a118":"code","fe82a0c4":"code","d9ca5704":"code","55c9808e":"code","30beac46":"code","06eef173":"code","99bb0548":"code","86034b3c":"code","2f694da2":"code","46400b36":"code","4aa2b760":"code","f70180db":"code","026081bb":"code","dd8d0dd2":"code","ac6ce8c3":"code","6257f94b":"code","419ad36f":"code","08878e9b":"code","59b08c0b":"code","8671242d":"code","a7ea29a4":"code","9f63894e":"markdown","fb2ed192":"markdown","738e5871":"markdown","45728ad7":"markdown","00e097f2":"markdown","d66cb9a6":"markdown","a17489b0":"markdown","61361152":"markdown","aabafdf8":"markdown","cff9823e":"markdown","c74188c6":"markdown"},"source":{"e031a118":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nseed = 42\nnp.random.seed(seed)","fe82a0c4":"train_data = pd.read_csv('..\/input\/data-science-london-scikit-learn\/train.csv', header=None)\ntrain_labels = pd.read_csv('..\/input\/data-science-london-scikit-learn\/trainLabels.csv', header=None, names = ['Label'])\ntest_data = pd.read_csv('..\/input\/data-science-london-scikit-learn\/test.csv', header=None)","d9ca5704":"train_data.head()","55c9808e":"train_labels.head()","30beac46":"train_data.shape, train_labels.shape","06eef173":"train_data.describe()","99bb0548":"train_labels.hist()","86034b3c":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV","2f694da2":"X, y = train_data, np.ravel(train_labels)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=seed)","46400b36":"def evaluate_model(model):\n    \n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n    print('Cross validation score - ', scores.mean()*100)\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_valid)\n\n    accuracy = accuracy_score(y_valid, y_pred) \n    print('Validation accuracy - ',accuracy*100)\n    \n    #Return trained model\n    return model","4aa2b760":"knn = KNeighborsClassifier()\n\nprint('\\nKNN Classifier ')\nknn = evaluate_model(knn)\n\ndtc = DecisionTreeClassifier(random_state=seed)\nprint('\\nDecision Tree Classifier')\ndtc = evaluate_model(dtc)\n\nrfc = RandomForestClassifier(n_estimators=10, random_state=seed)\nprint('\\nRandom Forest Classifier')\nrfc = evaluate_model(rfc)\n\nsvc = SVC(gamma='auto', random_state=seed)\nprint('\\nSVM Classifier')\nsvc = evaluate_model(svc)\n\n\ngbc = GradientBoostingClassifier(n_estimators=20, random_state=seed)\nprint('\\nGradient Boosting Classifier')\ngbc = evaluate_model(gbc)\n\nadc = AdaBoostClassifier(base_estimator=rfc, n_estimators=30, random_state=seed)\nprint('\\nAdaBoost classifier with Random Forest Classifier')\nadc = evaluate_model(adc)\n","f70180db":"from sklearn.decomposition import PCA\n\npca = PCA(random_state=seed)\n\npca.fit(X)\n\nfeatures = range(pca.n_components_)\nplt.figure(figsize=(13,6))\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.title('PCA Explained Variance')\nplt.show()","026081bb":"pca = PCA(n_components=13)\n\nX_reduced = pca.fit_transform(X)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_reduced, y, test_size=0.3, random_state = seed)","dd8d0dd2":"knn = KNeighborsClassifier()\n\nprint('\\nKNN Classifier ')\nknn = evaluate_model(knn)\n\ndtc = DecisionTreeClassifier(random_state=seed)\nprint('\\nDecision Tree Classifier')\ndtc = evaluate_model(dtc)\n\nrfc = RandomForestClassifier(n_estimators=10, random_state=seed)\nprint('\\nRandom Forest Classifier')\nrfc = evaluate_model(rfc)\n\nsvc = SVC(gamma='auto', random_state=seed)\nprint('\\nSVM Classifier')\nsvc = evaluate_model(svc)\n\n\ngbc = GradientBoostingClassifier(n_estimators=20, random_state=seed)\nprint('\\nGradient Boosting Classifier')\ngbc = evaluate_model(gbc)\n\nadc = AdaBoostClassifier(base_estimator=rfc, n_estimators=30, random_state=seed)\nprint('\\nAdaBoost classifier with Random Forest Classifier')\nadc = evaluate_model(adc)\n","ac6ce8c3":"def perform_grid_search(model, param_grid, cv = 10, scoring='accuracy'):\n    \n    grid_search_model = GridSearchCV(estimator=model, param_grid=param_grid, cv = cv,scoring=scoring,n_jobs=-1, iid=False)\n    grid_search_model.fit(X_train, y_train)\n\n\n    best_model = grid_search_model.best_estimator_\n    print('Best Accuracy :',grid_search_model.best_score_ * 100)\n    print('Best Parmas',grid_search_model.best_params_)\n    \n    y_pred = best_model.predict(X_valid)\n    print('Validation Accuracy',accuracy_score(y_valid, y_pred)*100)\n    \n    return best_model","6257f94b":"knn = KNeighborsClassifier()\n\nn_neighbors = [3,4,5,6,7,8,9,10]\nparam_grid_knn = dict(n_neighbors=n_neighbors)\n\nprint('\\nKNN Classifier')\nknn_best = perform_grid_search(knn, param_grid_knn)\n\nrfc = RandomForestClassifier(random_state=seed)\n\nn_estimators = [10, 50, 100, 200]\nmax_depth = [3, 10, 15, 30]\nparam_grid_rfc = dict(n_estimators=n_estimators,max_depth=max_depth)\n\nprint('\\nRandom Forest Classifier')\nrfc_best = perform_grid_search(rfc, param_grid_rfc)\n\n\nsvc = SVC(random_state=seed)\n\nparam_grid_svc = [{'kernel':['linear'],'C':[1,10, 50,100]},\n              {'kernel':['rbf'],'C':[1,10, 50,100],'gamma':[0.05,0.0001,0.01,0.001]}]\n\nprint('\\nSVM Classifier')\nsvc_best = perform_grid_search(svc, param_grid_svc)\n","419ad36f":"from sklearn.mixture import GaussianMixture\n\nX = np.r_[train_data,test_data]\n\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 12)\n\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.aic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(X)\ngmm_train = best_gmm.predict_proba(train_data)\ngmm_test = best_gmm.predict_proba(test_data)","08878e9b":"X_train, X_valid, y_train, y_valid = train_test_split(gmm_train, y, test_size=0.3, random_state = seed)","59b08c0b":"knn = KNeighborsClassifier()\n\nn_neighbors = [3,4,5,6,7,8,9,10]\nparam_grid_knn = dict(n_neighbors=n_neighbors)\n\nprint('\\nKNN Classifier')\nknn_best = perform_grid_search(knn, param_grid_knn)\n\nrfc_best = RandomForestClassifier(random_state=seed)\n\nn_estimators = [10, 50, 100, 200]\nmax_depth =  [3, 10, 15, 30]\nparam_grid_rfc = dict(n_estimators=n_estimators,max_depth=max_depth)\n\nprint('\\nRandom Forest Classifier')\nrfc = perform_grid_search(rfc, param_grid_rfc)\n\n\nsvc = SVC(random_state=seed)\n\nparam_grid_svc = [{'kernel':['linear'],'C':[1,10, 50,100]},\n              {'kernel':['rbf'],'C':[1,10, 50,100],'gamma':[0.05,0.0001,0.01,0.001]}]\n\nprint('\\nSVM Classifier')\nsvc_best = perform_grid_search(svc, param_grid_svc)\n","8671242d":"pred  = svc_best.predict(gmm_test)\nbest_pred = pd.DataFrame(pred)\n\nbest_pred.index += 1\n\nbest_pred.columns = ['Solution']\nbest_pred['Id'] = np.arange(1, best_pred.shape[0]+1)\nbest_pred = best_pred[['Id', 'Solution']]\n\nbest_pred.head()","a7ea29a4":"best_pred.to_csv('submission.csv',index=False)","9f63894e":"Selecting features with high explained variance and transforming the data.","fb2ed192":"Splitting Training data into train and validation sets","738e5871":"Using grid search again to get the best model.","45728ad7":"**KNN and SVM** classifiers have the highes accuracy**","00e097f2":"Using grid search to further improve model performance","d66cb9a6":"Dimensionality reduction","a17489b0":"**KNN, SVC and AdaBoost(with RFC)** gave the best accuracy after dimensionality reduction using PCA.","61361152":"Making predictions with the best model","aabafdf8":"Using Gaussian Mixture to furthur improve the performance.","cff9823e":"Splitting the transformed data into training and validation sets.","c74188c6":"Making predictions with transformed data "}}