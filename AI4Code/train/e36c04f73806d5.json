{"cell_type":{"7d2b2f88":"code","019dd042":"code","511271a8":"code","75e07436":"code","f76cec9c":"code","63d03b7d":"code","295668dd":"code","92c6ce38":"code","66fb1e34":"code","15a591ea":"code","dd604000":"code","e9a6f1be":"code","de8465ac":"code","84383838":"code","14e0898e":"code","1271d18d":"code","9b152bee":"code","8e392e28":"code","a1bd5abb":"code","d1974be8":"code","91d92111":"code","bb12312a":"code","bcbfe6cc":"code","7aa37177":"code","8d5aa6e6":"code","c7c7279b":"code","83655dfe":"code","c269b87b":"code","fe5c5c6a":"code","611678bd":"code","7ac8e0b9":"code","4f1dad26":"code","758f4991":"code","14e9cb4e":"markdown","3f7c2285":"markdown","3ccf4475":"markdown","7314bb1b":"markdown","48e9b762":"markdown","561a6fcb":"markdown","c518d54d":"markdown","75abe1da":"markdown","522b5fad":"markdown","403fe595":"markdown","97b56da5":"markdown","1c0fd2e8":"markdown","f1f7544d":"markdown","e752a084":"markdown","6bc82449":"markdown","0c21508e":"markdown","85c827f7":"markdown","d0b8d766":"markdown","dfe7ab12":"markdown","76ab1950":"markdown","a3dcd029":"markdown","6299989a":"markdown","7688a844":"markdown","4d1c29fd":"markdown","b4732d5b":"markdown","4be864d3":"markdown","06f9d037":"markdown","d71af680":"markdown","38d981d6":"markdown","bc1692aa":"markdown"},"source":{"7d2b2f88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","019dd042":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","511271a8":"dataset = pandas.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndataset.sample(10)","75e07436":"dataset.drop(\"Id\", axis=1, inplace=True)","f76cec9c":"dataset.info()","63d03b7d":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","295668dd":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","92c6ce38":"sns.set_style('darkgrid')\nfor c in categorical_list:\n    bar_plot(c)","66fb1e34":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","15a591ea":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with Histogram\".format(variable))\n    plt.show()","dd604000":"for n in numerical_float64_list:\n    plot_hist(n)","e9a6f1be":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,2,1)\nsns.histplot(dataset['SepalLengthCm'], color = 'red', kde = True).set_title('SepalLengthCm Interval and Counts')\n\nplt.subplot(2,2,2)\nsns.histplot(dataset['SepalWidthCm'], color = 'green', kde = True).set_title('SepalWidthCm Interval and Counts')\n\nplt.subplot(2,2,3)\nsns.histplot(dataset['PetalLengthCm'], kde = True, color = 'blue').set_title('PetalLengthCm Interval and Counts')\n\nplt.subplot(2,2,4)\nsns.histplot(dataset['PetalWidthCm'], kde = True, color = 'black').set_title('PetalWidthCm Interval and Counts')","de8465ac":"features = dataset.columns\nsns.set_style('darkgrid')\nsns.pairplot(dataset[features])","84383838":"sns.pairplot(dataset, hue = 'Species')","14e0898e":"dataset.corr()","1271d18d":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","9b152bee":"sns.set_style('darkgrid')\naxes = pandas.plotting.scatter_matrix(dataset, alpha = 0.3, figsize = (10,7), diagonal = 'kde' ,s=80)\ncorr = dataset.corr().values\n\nplt.xticks(fontsize =10,rotation =0)\nplt.yticks(fontsize =10)\nfor ax in axes.ravel():\n    ax.set_xlabel(ax.get_xlabel(),fontsize = 15, rotation = 60)\n    ax.set_ylabel(ax.get_ylabel(),fontsize = 15, rotation = 60)\n# put the correlation between each pair of variables on each graph\nfor i, j in zip(*np.triu_indices_from(axes, k=1)):\n    axes[i, j].annotate(\"%.3f\" %corr[i, j], (0.8, 0.8), xycoords=\"axes fraction\", ha=\"center\", va=\"center\")","8e392e28":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.barplot(x = 'Species', y = 'SepalLengthCm', data = dataset, palette=\"cubehelix\")\nplt.subplot(2,2,2)\nsns.barplot(x = 'Species', y = 'SepalWidthCm', data = dataset, palette=\"Oranges\")\nplt.subplot(2,2,3)\nsns.barplot(x = 'Species', y = 'PetalLengthCm', data = dataset, palette=\"Oranges\")\nplt.subplot(2,2,4)\nsns.barplot(x = 'Species', y = 'PetalWidthCm', data = dataset, palette=\"cubehelix\")","a1bd5abb":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.violinplot(x = 'Species', y = 'SepalLengthCm', data = dataset, palette=\"rocket_r\")\nplt.subplot(2,2,2)\nsns.violinplot(x = 'Species', y = 'SepalWidthCm', data = dataset, palette=\"rocket_r\")\nplt.subplot(2,2,3)\nsns.violinplot(x = 'Species', y = 'PetalLengthCm', data = dataset, palette=\"rocket_r\")\nplt.subplot(2,2,4)\nsns.violinplot(x = 'Species', y = 'PetalWidthCm', data = dataset, palette=\"rocket_r\")","d1974be8":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.boxplot(x = 'Species', y = 'SepalLengthCm', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(2,2,2)\nsns.boxplot(x = 'Species', y = 'SepalWidthCm', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(2,2,3)\nsns.boxplot(x = 'Species', y = 'PetalLengthCm', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(2,2,4)\nsns.boxplot(x = 'Species', y = 'PetalWidthCm', data = dataset, palette=\"gist_ncar_r\")","91d92111":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.distplot(dataset['SepalLengthCm'], color=\"red\").set_title('SepalLength Interval')\nplt.subplot(2,2,2)\nsns.distplot(dataset['SepalWidthCm'], color=\"green\").set_title('SepalWidth Interval')\nplt.subplot(2,2,3)\nsns.distplot(dataset['PetalLengthCm'], color=\"blue\").set_title('PetalLength Interval')\nplt.subplot(2,2,4)\nsns.distplot(dataset['PetalWidthCm'], color=\"black\").set_title('PetalWidth Interval')","bb12312a":"plt.figure(1, figsize=(5,5))\nplt.title(\"Distribution of Species\")\ndataset['Species'].value_counts().plot.pie(autopct=\"%1.1f%%\")","bcbfe6cc":"import pandas_profiling as pp\npp.ProfileReport(dataset)","7aa37177":"X = dataset.iloc[:,0:4].values \ny = dataset.iloc[:,4:].values ","8d5aa6e6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","c7c7279b":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","83655dfe":"models = {\n    'GaussianNB': GaussianNB(),\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorMachine': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n    'Neural Nets': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  score = model.score(X_valid, y_valid)\n  #print(f'{m} validation score => {score*100}')\n    \n  print(f'{m}') \n  train_score = model.score(X_train, y_train)\n  print(f'Train score of trained model: {train_score*100}')\n  trainScores.append(train_score*100)\n\n  validation_score = model.score(X_valid, y_valid)\n  print(f'Validation score of trained model: {validation_score*100}')\n  validationScores.append(validation_score*100)\n\n  test_score = model.score(X_test, y_test)\n  print(f'Test score of trained model: {test_score*100}')\n  testScores.append(test_score*100)\n  print(\" \")\n    \n  y_predictions = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_predictions, y_test)\n\n  print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\n  predictions = model.predict(X_test)\n  cm = confusion_matrix(predictions, y_test)\n\n  tn = conf_matrix[0,0]\n  fp = conf_matrix[0,1]\n  tp = conf_matrix[1,1]\n  fn = conf_matrix[1,0]\n  accuracy  = (tp + tn) \/ (tp + fp + tn + fn)\n  precision = tp \/ (tp + fp)\n  recall    = tp \/ (tp + fn)\n  f1score  = 2 * precision * recall \/ (precision + recall)\n  specificity = tn \/ (tn + fp)\n  print(f'Accuracy : {accuracy}')\n  print(f'Precision: {precision}')\n  print(f'Recall   : {recall}')\n  print(f'F1 score : {f1score}')\n  print(f'Specificity : {specificity}')\n  print(\"\") \n  print(f'Classification Report: \\n{classification_report(predictions, y_test)}\\n')\n  print(\"\")\n   \n  for m in range (1):\n    current = modelNames[m]\n    modelNames.remove(modelNames[m])\n\n  preds = model.predict(X_test)\n  confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\n  print(\"############################################################################\")\n  print(\"\")\n  print(\"\")\n  print(\"\")","c269b87b":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","fe5c5c6a":"for i in range(10):\n    print(f'Accuracy of {modelNames[i]} -----> {testScores[i]}')","611678bd":"models = {\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")","7ac8e0b9":"from sklearn.decomposition import PCA\nselectedX = X[:,]\n\npcaX = PCA(n_components=2)\npcaX = pcaX.fit(selectedX)\npcaX = pcaX.transform(X)\nprint(pcaX.shape)\n\nplt.scatter(pcaX[:,0], pcaX[:,1])\nplt.show()","4f1dad26":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nldaX = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\nprint(ldaX.shape)\n\nplt.scatter(ldaX[:,0], ldaX[:,1])\nplt.show()","758f4991":"ldaX = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\npcaX = PCA(n_components=2).fit_transform(X, y)\n\n\nplt.figure(figsize=(25,8))\n\nplt.subplot(1,2,1)\nplt.title('PCA')\nplt.scatter(pcaX[:,0], pcaX[:, 1])\n\nplt.subplot(1,2,2)\nplt.title('LDA')\nplt.scatter(ldaX[:,0], ldaX[:, 1])\n\n\nplt.show()","14e9cb4e":"Linear Discriminant Analysis is used as a dimension reduction technique in preprocessing stage for machine learning applications. The goal is to prevent overfitting and at the same time reduce computational costs. Although LDA is generally similar to PCA, the working logic of LDA includes maximizing the distance between classes. There is no class concept in PCA. PCA only tries to maximize the distance between data points.\n\nIn summary, the aim of LDA is to reduce the size of the data set by maximizing the difference between classes.","3f7c2285":"In data science, dimension reduction is the transformation of data from a high dimensional space to a low dimensional space without losing its meaning. Processing a high-dimensional data requires more processing load. Therefore, dimension reduction is frequently used in areas such as signal processing, speech recognition, neuroinformatics, and bioinformatics, where a large number of observations and variables are examined.","3ccf4475":"I dropped 'id' column because it can cause unwanted correlation.","7314bb1b":"<a id=\"9\"><\/a> \n# Pandas Profiling","48e9b762":"<a id=\"2\"><\/a> \n# Read Datas & Explanation of Features & Information About Datasets","561a6fcb":"<a id=\"7\"><\/a> \n# Correlation","c518d54d":"<a id=\"1\"><\/a> \n# Importing the Necessary Libraries","75abe1da":"# Exploratory Data Analysis and Machine Learning Classification on Iris Species","522b5fad":"<a id=\"10\"><\/a> \n# Train - Test Split","403fe595":"<a id=\"6\"><\/a> \n### Numerical Variables","97b56da5":"![image.png](attachment:image.png)\n\nSource: http:\/\/www.lac.inpe.br\/~rafael.santos\/Docs\/CAP394\/WholeStory-Iris.html","1c0fd2e8":"In this notebook, I examined Iris Species Dataset. Firstly, I made Exploratory Data Analysis, Visualization, then I applied Machine Learning algorithms to this dataset. \n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* If you liked this notebook, please let me know :)\n\n\n* ***Thank you for your time.***","f1f7544d":"<a id=\"5\"><\/a> \n### Categorical Variables","e752a084":"Standardization is a method in which the mean value is 0 and the standard deviation is 1, and the distribution approaches the normal. The formula is as follows, we subtract the average value from the value we have, then divide it by the variance value.","6bc82449":"Hello. In this notebook, I did a Visualization and Machine Learning study on the Iris Types Data Set. I performed Data Analysis on the dataset using visualization tools. At the end of the notebook, I performed Dimensionality Reduction using PCA and LDA. I hope it will be useful.","0c21508e":"<a id=\"8\"><\/a> \n# Data Visualization","85c827f7":"Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n   1. [Univariate Variable Analysis](#4)\n      1. [Categorical Variables](#5)\n      1. [Numerical Variables](#6)\n1. [Correlation](#7)\n1. [Data Visualization](#8)\n1. [Pandas Profiling](#9)\n1. [Train-Test Split](#10)\n1. [Scores of Models](#11)\n1. [Best Features Selection](#12)\n1. [Dimensionality Reduction](#13)\n   1. [Principle Component Analysis (PCA)](#14)\n   1. [Linear Discriminant Analysis (LDA)](#15)\n1. [Conclusion](#16)      ","d0b8d766":"<a id=\"16\"><\/a> \n# Conclusion","dfe7ab12":"<a id=\"4\"><\/a> \n## Univariate Variable Analysis","76ab1950":"<a id=\"13\"><\/a>\n# Dimensionality Reduction","a3dcd029":"### If you have questions please ask them on the comment section.\n\n### I will be glad if you can give feedback.","6299989a":"<a id=\"14\"><\/a>\n## Principle Component Analysis (PCA)","7688a844":"*** Categorical Variables:** ['Species']\n\n*** Numerical Variables:** ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']","4d1c29fd":"<a id=\"12\"><\/a>\n# Best Features Selection","b4732d5b":"PCA is a useful statistical technique used in the fields of recognition, classification, and image compression. It is a technique whose main purpose is to keep the data set with the highest variance in high dimensional data, but to provide dimension reduction while doing this. By finding general features in multi-dimensional data, it enables the reduction of the number of dimensions and the compression of the data. Certain features will be lost with size reduction; but the intention is that these lost traits contain little information about the population. This method combines highly correlated variables together to create a smaller set of artificial variables called principal components that make up the most variation in the data.\n\nPCA is a very effective method for revealing the necessary information in the data. The basic logic behind PCA is to show a multidimensional data with fewer variables by capturing the basic features in the data.","4be864d3":"<a id=\"3\"><\/a> \n## Variable Descriptions","06f9d037":"These are the ML algorithms that will apply to dataset. Results will contain train-validation-test scores, confusion matrix, statistical information and classification reports for each algorithm.","d71af680":"Pandas profiling is a useful library that generates interactive reports about the data. With using this library, we can see types of data, distribution of data and various statistical information. This tool has many features for data preparing. Pandas Profiling includes graphics about specific feature and correlation maps too. You can see more details about this tool in the following url: https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/","38d981d6":"<a id=\"11\"><\/a> \n# Scores of Models","bc1692aa":"<a id=\"15\"><\/a>\n## Linear Discriminant Analysis (LDA)"}}