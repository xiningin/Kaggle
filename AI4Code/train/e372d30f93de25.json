{"cell_type":{"67a3282d":"code","ef64d93b":"code","4a6aad6e":"code","5bb2a739":"code","8aa5f8ef":"code","a12289cf":"code","83ffae95":"code","1b50485e":"code","2766d9dc":"code","73c66110":"code","78951739":"code","c739f3e9":"code","4db0cbe2":"code","e5a57c50":"code","18005c80":"code","7eab1194":"code","806fbe9f":"code","1c3a3336":"code","bb77a8d8":"code","ca6d8042":"code","1768172c":"code","700435dd":"code","20d5e83b":"code","ca9596d8":"code","653311fc":"code","80156830":"code","dc818877":"code","21cde7db":"code","5987b3e6":"code","74d6b3a2":"code","d8377a0a":"markdown","4b3d3ea9":"markdown","878f4920":"markdown","739a9714":"markdown","8c7d17cb":"markdown","0d910262":"markdown","b9830765":"markdown","1cef4832":"markdown","b48c3740":"markdown","6840b1a7":"markdown","f0689297":"markdown","2abed7d3":"markdown","65fc26dd":"markdown","2fd95989":"markdown","01acebfa":"markdown","0c5f123d":"markdown","8496c163":"markdown","7ef960ff":"markdown","0da43f07":"markdown","6d1e93d7":"markdown","765090df":"markdown","9bdaaae1":"markdown","845a4cbb":"markdown","ce82a842":"markdown","df1d49a5":"markdown","b5adfcc4":"markdown","f709b2ba":"markdown","1ac540f2":"markdown","0ffce1ab":"markdown","4c8cba20":"markdown"},"source":{"67a3282d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ef64d93b":"data = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","4a6aad6e":"data.head()","5bb2a739":"data.describe().T","8aa5f8ef":"data.info()","a12289cf":"data.corr()","83ffae95":"data.Species.unique()","1b50485e":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nsns.violinplot(data=data, x='Species',y='PetalLengthCm')\nplt.subplot(2,2,2)\nsns.violinplot(data=data, x='Species',y='PetalWidthCm')\nplt.subplot(2,2,3)\nsns.violinplot(data=data, x='Species',y='SepalLengthCm')\nplt.subplot(2,2,4)\nsns.violinplot(data=data, x='Species',y='SepalWidthCm')\n","2766d9dc":"dataSetosa = data[data.Species == 'Iris-setosa'] \ndataVersicolor = data[data.Species == 'Iris-versicolor']\ndataVirginica = data[data.Species == 'Iris-virginica']\n\ntrace1 = go.Box(\n    y=dataSetosa.SepalLengthCm,\n    name = 'Setosa',\n    marker = dict(\n        color = 'rgb(12, 12, 140)',\n    )\n)\ntrace2 = go.Box(\n    y=dataVersicolor.SepalLengthCm,\n    name = 'Versicolor',\n    marker = dict(\n        color = 'rgb(12, 128, 128)',\n    )\n)\n\ntrace3 = go.Box(\n    y=dataVirginica.SepalLengthCm,\n    name = 'Virginica',\n    marker = dict(\n        color = 'rgb(12, 12, 140)',\n    )\n)\n\ndata1 = [trace1,trace2, trace3]\niplot(data1)","73c66110":"plt.figure(figsize = (25,10))\n\nplt.subplot(2,2,1)\nsns.swarmplot(x=\"SepalWidthCm\", y=\"SepalLengthCm\", hue=\"Species\",data = data, palette=\"Paired\")\nplt.subplot(2,2,2)\nsns.swarmplot(x=\"PetalWidthCm\", y=\"PetalLengthCm\", hue=\"Species\",data = data, palette=\"Paired\")\n","78951739":"data.head()","c739f3e9":"dataSetosa = data[data.Species == 'Iris-setosa'] \ndataVersicolor = data[data.Species == 'Iris-versicolor']\ndataVirginica = data[data.Species == 'Iris-virginica']\n\n\nnew_data = pd.concat([dataVersicolor,dataVirginica])\nnew_data = new_data.reset_index() \nnew_data","4db0cbe2":"# Kullanmad\u0131\u011f\u0131m\u0131z columnslar\u0131 drop edelim\n# axis = 1 : t\u00fcm bir columns\n# inplace : Drop et ve datan\u0131n i\u00e7ine kaydet\nnew_data.drop([\"index\",\"Id\"], axis=1,inplace = True)\nnew_data.head()","e5a57c50":"# String bar\u0131nd\u0131ran columns de\u011ferimizi de\u011fi\u015ftirdik\nnew_data.Species = [1 if each == \"Iris-versicolor\" else 0 for each in new_data.Species]\nnew_data.head()","18005c80":"y = new_data.Species.values\nx_data = new_data.drop([\"Species\"], axis=1) # Species de\u011ferlerimizin olmad\u0131\u011f\u0131 bir veri seti\n\nx_data.head()","7eab1194":"# (x - x(min)\/(max(x)- min(x))\n\n# All columns must be in the range 0 to 1. Features should not differ greatly between them.\n\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","806fbe9f":"from sklearn.model_selection import train_test_split\n\nx_train,x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state= 16)\n\n# Transpoz process\nx_train = x_train.T\nx_test = x_test.T#\ny_train = y_train.T\ny_test = y_test.T\n\n\n# shape values\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)\n","1c3a3336":"def initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01) # Create values with 0.01 values\n    \n    b = 0.0 \n    \n    # These numbers are usually selected when using w and b.\n    \n    return w,b\n","bb77a8d8":"# sigmoid func = 1\/ (1+ e^(-x))\n\ndef sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    \n    return y_head\n    ","ca6d8042":"print(x_train.shape[1])","1768172c":"def forward_backward_propagation(w,b,x_train,y_train):\n    \n    # forward propagation  \n    \n    # ((w values (1,30) * (4, 80) process) ve add bias values) = z\n    z = np.dot(w.T,x_train) + b       # z values \/ np.dot : sat\u0131r ve s\u00fct\u00fcn olarak \u00e7arp\n    y_head = sigmoid(z)               # The y_head value of our z value in s function\n    \n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) # loss value\n    cost = (np.sum(loss))\/x_train.shape[1]                      # \/x_train.shape[1]=80   to normalized\n    \n    \n    # backward propagation\n    \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]              # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                                # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias} # we used dictionary to store parameters\n    \n    return cost,gradients\n    ","700435dd":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 50 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","20d5e83b":"tahmini_deger = []\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n            tahmini_deger.append(0)\n        else:  \n            Y_prediction[0,i] = 1\n            tahmini_deger.append(1)\n\n    return Y_prediction\n\n","ca9596d8":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 1001)    \n\n","653311fc":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","80156830":"tahmin = np.array(tahmini_deger)\ntahmin","dc818877":"y_test","21cde7db":"Tahmin = np.concatenate((y_test,tahmin), axis=0)\n\nTahmin = Tahmin.reshape(2,20)\n\nTahmin","5987b3e6":"x_test.T # Data in index 33 failed under LR.","74d6b3a2":"deger = x_test.T\ndeger = deger.reset_index() \ndeger[\"Real_Values\"] = [\"Iris-versicolor\" if each == 0 else \"Iris-virginica\" for each in y_test]\ndeger[\"Test_Values\"] = [\"Iris-versicolor\" if each == 0 else \"Iris-virginica\" for each in tahmin]\ndeger[\"Success\"] = -(y_test - tahmin)+1\n\ndeger","d8377a0a":"## Libraries","4b3d3ea9":"## Sigmoid function","878f4920":"## Visualization and Understanding","739a9714":"# Logistic Regression","8c7d17cb":"## Logistic Regression","0d910262":"# Logistic Regression","b9830765":"## Referance\n\nhttps:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners","1cef4832":"### Difference analysis between seabon and species","b48c3740":"## Data Editing","6840b1a7":"## Sklearn with LR","f0689297":"## Update Parameter","2abed7d3":"# Logistic Regression with Iris","65fc26dd":"## Train Test Split","2fd95989":"## Reading Data","01acebfa":"## Test","0c5f123d":"![](https:\/\/www.aliozcan.org\/wp-content\/uploads\/2019\/11\/sigmoid-Fonksiyonu.png)","8496c163":"### Difference analysis between Plotly and species","7ef960ff":"# Iris Species","0da43f07":"## Cost, gradient","6d1e93d7":"![](https:\/\/www.researchgate.net\/publication\/303744090\/figure\/fig3\/AS:368958596239360@1464977992159\/Feedforward-Backpropagation-Neural-Network-architecture.png)","765090df":"* G\u00f6rselle\u015ftirmeler sonucunda Versicolor ve Virginica de\u011ferlerine logistic regression modeli olu\u015fturmaya karar verdik. \u00c7\u00fcnk\u00fc Setosa de\u011ferlerini g\u00f6z ile de tespit etmek kolay. Gelin verimizi d\u00fczenleyelim ve algoritmam\u0131z\u0131 olu\u015ftural\u0131m.","9bdaaae1":"### Difference analysis between seabon and species","845a4cbb":"## Content\n* Content\n* Logistic Regression\n* Data\n    * Iris Species\n    * Libraries\n    * Reading Data\n    * Visualization and Understanding\n        * Difference analysis between Seabon and species\n        * Difference analysis between Plotly and species\n    * Data Editing\n    \n    \n* Logistic Regression\n    * Normalization\n    * Train Test Split\n    * Parameter \u0130nitialize\n    * Sigmoid function\n    * Cost, gradient\n    * Update Parameter\n    * Predict\n    * Logistic Regression with Math\n    * Sklearn with LR\n* Reference","ce82a842":"## Predict","df1d49a5":"# Data","b5adfcc4":"![](https:\/\/gblobscdn.gitbook.com\/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvI8vNq_N7u3RWVAPLk%2F-LvJSdcFXzoI-WW0L3w5%2Fimage.png?alt=media&token=84526dc6-4634-4de5-aacf-00a179afac76)","f709b2ba":"The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\n* Id\n* SepalLengthCm\n* SepalWidthCm\n* PetalLengthCm\n* PetalWidthCm\n* Species\n\nhttps:\/\/www.kaggle.com\/uciml\/iris\n\n![](https:\/\/s3.amazonaws.com\/assets.datacamp.com\/blog_assets\/Machine+Learning+R\/iris-machinelearning.png)","1ac540f2":"## Normalization","0ffce1ab":"In statistics, the logistic model is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Wikipedia\n\n![](https:\/\/storage.ning.com\/topology\/rest\/1.0\/file\/get\/2808358994?profile=original)","4c8cba20":"## Parameter \u0130nitialize"}}