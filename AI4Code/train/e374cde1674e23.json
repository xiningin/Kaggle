{"cell_type":{"974bcd90":"code","444edad1":"code","6008e105":"code","bf3e3668":"code","bec3c3f8":"code","21a339b2":"code","8a4935c6":"code","1722dd38":"code","53189c7b":"code","ca4316c8":"code","add775cc":"code","b90a67d5":"code","f293084f":"code","ed683db3":"code","56e079ae":"code","02bfe6fd":"code","2cf04999":"code","cc214573":"code","a71cefd9":"code","9efecee5":"code","fe15e345":"code","92d0cf9c":"code","e898c014":"code","af6dbe87":"code","43e682e9":"code","153791d3":"code","08e0e82b":"code","75324d31":"code","c7d14479":"code","17e72871":"code","133c6e9f":"markdown","52664ec9":"markdown","8fa40d29":"markdown","76b3983a":"markdown","98d272d3":"markdown","4a61caea":"markdown","66357ffd":"markdown","036c5690":"markdown","5ca28f4e":"markdown","86db0f34":"markdown","50ea8115":"markdown","3f0ad61b":"markdown","3692d0cf":"markdown","d2b76744":"markdown","36474f60":"markdown","caea4f1b":"markdown","7ae16c19":"markdown","5b0f9f53":"markdown","f7628379":"markdown","646702fb":"markdown","7db24073":"markdown","5afa5bcc":"markdown"},"source":{"974bcd90":"# Data Analysis\nimport pandas as pd # data processing\nimport numpy as np # linear algebra\nfrom tqdm import tqdm # Instantly make your loops show a smart progress meter \nfrom scipy import stats\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport cufflinks as cf  #Interactive plots\ncf.go_offline()\n%matplotlib inline\n\n# Predictive model \n# Importing required ML packages\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import NuSVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_absolute_error","444edad1":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Results are saved as output.","6008e105":"train = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})","bf3e3668":"train.shape","bec3c3f8":"pd.options.display.precision = 15\ntrain.head(10)","21a339b2":"Seg_lenght = 150000\nSeg_dataframe = train.iloc[0:Seg_lenght]\n##\n# Edit the layout\nlayout = dict(title = 'First segment of earthquake acoustic data',\n              xaxis = dict(title = 'Time to Failure'),\n              yaxis = dict(title = 'Acoustic Data'),\n              )\nSeg_dataframe.iplot(kind='scatter',x='time_to_failure', y ='acoustic_data',mode = 'lines+markers',size=4,color = 'rgb(22, 96, 167)',layout = layout)","8a4935c6":"sns.set_style('whitegrid')\nsns.distplot(Seg_dataframe['acoustic_data'],kde=False,color='darkblue',bins=50)","1722dd38":"Seg_dataframe['acoustic_data'].describe()","53189c7b":"train.isnull().sum()","ca4316c8":"# Trend function from: https:\/\/www.kaggle.com\/jsaguiar\/baseline-with-abs-and-trend-features\n#\"Simple trend feature: fit a linear regression and return the coefficient\"\ndef add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]","add775cc":"# based on https:\/\/www.kaggle.com\/inversion\/basic-feature-benchmark\n#****************************\n    # Additional features\n    # [1] Quantiles based on https:\/\/www.kaggle.com\/andrekos\/basic-feature-benchmark-with-quantiles\n    # [2] Absolute values and Trend features from: https:\/\/www.kaggle.com\/jsaguiar\/baseline-with-abs-and-trend-features\n    # [3] Rolling Quantiles from: https:\/\/www.kaggle.com\/wimwim\/rolling-quantiles\n    # [4] Additional features from: https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples\n    # [5] Skewness and kurtusis from: https:\/\/www.kaggle.com\/gpreda\/lanl-earthquake-eda-and-prediction\n    #****************************\n    \nrows = 150_000 # Length of test segments\nsegments = int(np.floor(train.shape[0] \/ rows)) #Number of segments in the train dataset\n\n# New train features dataframe \ntrain_columns = ['mean', 'std', 'max', 'min', 'kurtosis', 'skew', 'mean10', 'X_seg_sum', \n                       'q001','q01', 'q05', 'q95', 'q99', 'q999', 'iqr','trend', 'abs_max', \n                       'abs_mean', 'abs_std', 'abs_X_seg_sum', 'abs_trend', 'abs_median',\n                       'abs_q95', 'abs_q99', 'F_test', 'p_test', 'mean_change_abs', \n                       'mean_change_rate', 'mean_roll_std_10', 'std_roll_std_10',\n                       'max_roll_std_10', 'min_roll_std_10','q01_roll_std_10', \n                       'q05_roll_std_10', 'q95_roll_std_10', 'q99_roll_std_10',\n                       'mean_change_abs_roll_std_10', 'abs_max_roll_std_10', \n                       'mean_roll_mean_10','std_roll_mean_10', 'max_roll_mean_10', \n                       'min_roll_mean_10', 'q01_roll_mean_10', 'q05_roll_mean_10', \n                       'q95_roll_mean_10', 'q99_roll_mean_10', 'mean_change_abs_roll_mean_10',\n                       'mean_change_rate_roll_mean_10', 'abs_max_roll_mean_10', \n                       'mean_roll_std_100', 'std_roll_std_100','max_roll_std_100',\n                       'min_roll_std_100','q01_roll_std_100', 'q05_roll_std_100', \n                       'q95_roll_std_100', 'q99_roll_std_100','mean_change_abs_roll_std_100', 'abs_max_roll_std_100', \n                       'mean_roll_mean_100','std_roll_mean_100', 'max_roll_mean_100', \n                       'min_roll_mean_100', 'q01_roll_mean_100', 'q05_roll_mean_100', \n                       'q95_roll_mean_100', 'q99_roll_mean_100', \n                       'mean_change_abs_roll_mean_100','mean_change_rate_roll_mean_100', \n                       'abs_max_roll_mean_100',\n                       'mean_roll_std_1000', 'std_roll_std_1000','max_roll_std_1000',\n                       'min_roll_std_1000','q01_roll_std_1000', 'q05_roll_std_1000', \n                       'q95_roll_std_1000', 'q99_roll_std_1000','mean_change_abs_roll_std_1000', 'abs_max_roll_std_100', \n                       'mean_roll_mean_1000','std_roll_mean_1000', 'max_roll_mean_1000', \n                       'min_roll_mean_1000', 'q01_roll_mean_1000', 'q05_roll_mean_1000', \n                       'q95_roll_mean_1000', 'q99_roll_mean_1000', \n                       'mean_change_abs_roll_mean_1000','mean_change_rate_roll_mean_1000', \n                       'abs_max_roll_mean_1000']\n\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=train_columns)\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])","b90a67d5":"# Function to create Features\ndef create_features(segment, DF_seg, DF_X_Output):\n    x = DF_seg ['acoustic_data']    \n    DF_X_Output.loc[segment, 'mean'] = x.mean()\n    DF_X_Output.loc[segment, 'std'] = x.std()\n    DF_X_Output.loc[segment, 'max'] = x.max()\n    DF_X_Output.loc[segment, 'min'] = x.min()\n    DF_X_Output.loc[segment, 'kurtosis'] = x.kurtosis() #[5]\n    DF_X_Output.loc[segment, 'skew'] = x.skew()#[5]\n    # Trimmed mean, which excludes the outliers, of an array, in this case excludes 10% at both ends\n    DF_X_Output.loc[segment, 'mean10'] = stats.trim_mean(x, 0.1)#[2]\n    DF_X_Output.loc[segment, 'X_seg_sum'] = x.sum() \n    \n    # Quantile\n    DF_X_Output.loc[segment, 'q001'] = np.quantile(x,0.001)#[2]\n    DF_X_Output.loc[segment, 'q01'] = np.quantile(x,0.01) #[1]\n    DF_X_Output.loc[segment, 'q05'] = np.quantile(x,0.05) #[1]\n    DF_X_Output.loc[segment, 'q95'] = np.quantile(x,0.95) #[1]\n    DF_X_Output.loc[segment, 'q99'] = np.quantile(x,0.99) #[1]\n    DF_X_Output.loc[segment, 'q999'] = np.quantile(x,0.999)#[2]\n    # Interquartile range IQR: The IQR describes the middle 50% of values when ordered from lowest to highest. \n    # IQR = q75 - q25\n    DF_X_Output.loc[segment, 'iqr'] = np.subtract(*np.percentile(x, [75, 25]))#[2]\n    \n    # Trends\n    DF_X_Output.loc[segment, 'trend'] = add_trend_feature(x)#[2]\n    \n    # Absolut Values\n    DF_X_Output.loc[segment, 'abs_max'] = np.abs(x).max()#[2]\n    DF_X_Output.loc[segment, 'abs_mean'] = np.abs(x).mean()#[2]\n    DF_X_Output.loc[segment, 'abs_std'] = np.abs(x).std()#[2]\n    DF_X_Output.loc[segment, 'abs_X_seg_sum'] = np.abs(x).sum() \n    DF_X_Output.loc[segment, 'abs_trend'] = add_trend_feature(x, abs_values=True)#[2]\n    DF_X_Output.loc[segment, 'abs_median'] = np.median(np.abs(x))#[3]\n    DF_X_Output.loc[segment, 'abs_q95'] = np.quantile(np.abs(x),0.95)#[3]\n    DF_X_Output.loc[segment, 'abs_q99'] = np.quantile(np.abs(x),0.99)#[3]\n    \n    # Change\/diff in acoustic data within a segment [3]\n    # Divide the segment in groups of 30000 sample as and do a oneway anova test\n    DF_X_Output.loc[segment, 'F_test'], DF_X_Output.loc[segment, 'p_test'] = stats.f_oneway(x[:30000],x[30000:60000],x[60000:90000],x[90000:120000],x[120000:])\n    DF_X_Output.loc[segment, 'mean_change_abs'] = np.mean(np.diff(x))\n    DF_X_Output.loc[segment, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    \n    # Rolling features [3], and [4] added 1000 windows\n    for windows in [10,100,1000]:\n        x_roll_std = x.rolling(windows).std().dropna().values\n        x_roll_mean = x.rolling(windows).mean().dropna().values\n        \n        DF_X_Output.loc[segment, 'mean_roll_std_' + str(windows)] = x_roll_std.mean()\n        DF_X_Output.loc[segment, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n        DF_X_Output.loc[segment, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n        DF_X_Output.loc[segment, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n        DF_X_Output.loc[segment, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std,0.01)\n        DF_X_Output.loc[segment, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std,0.05)\n        DF_X_Output.loc[segment, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std,0.95)\n        DF_X_Output.loc[segment, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std,0.99)\n        DF_X_Output.loc[segment, 'mean_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n        DF_X_Output.loc[segment, 'mean_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) \/ x_roll_std[:-1]))[0])\n        DF_X_Output.loc[segment, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n        \n        DF_X_Output.loc[segment, 'mean_roll_mean_' + str(windows)] = x_roll_mean.mean()\n        DF_X_Output.loc[segment, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n        DF_X_Output.loc[segment, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n        DF_X_Output.loc[segment, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n        DF_X_Output.loc[segment, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean,0.01)\n        DF_X_Output.loc[segment, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean,0.05)\n        DF_X_Output.loc[segment, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean,0.95)\n        DF_X_Output.loc[segment, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean,0.99)\n        DF_X_Output.loc[segment, 'mean_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        DF_X_Output.loc[segment, 'mean_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) \/ x_roll_mean[:-1]))[0])\n        DF_X_Output.loc[segment, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n","f293084f":"# Create new features for the training dataset\nfor segment_id in tqdm(range(segments)):\n    seg = train.iloc[segment_id*rows:segment_id*rows+rows]\n    create_features(segment_id, seg, X_train)\n    y_train.loc[segment_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]","ed683db3":"X_train.head()","56e079ae":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)","02bfe6fd":"y_train.head()","2cf04999":"#Create an instance of a LinearRegression() model named lm.\nlm = LinearRegression()\n\n# Train lm using the training data.\nlm.fit(X_train_scaled,y_train)","cc214573":"# Print the coefficients\nprint('Coefficients: \\n', lm.coef_)","a71cefd9":"svm = NuSVR()\nsvm.fit(X_train_scaled,y_train.values.flatten())","9efecee5":"Lm_y_pred = lm.predict(X_train_scaled)","fe15e345":"# Scatterplot of the real test values versus the \"predicted\" trained values.\ny_pred = Lm_y_pred\nplt.scatter(y_train,y_pred)\nplt.xlabel('Time to Failure From the Training Data')\nplt.ylabel('Predicted Time to Failure')\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)]) # Perfect correlation","92d0cf9c":"score = mean_absolute_error(y_train.values.flatten(), y_pred)\nprint(f'Score: {score:0.3f}')","e898c014":"NuSVR_y_pred = svm.predict(X_train_scaled)\n\n# Scatterplot of the real test values versus the \"predicted\" trained values.\ny_pred = NuSVR_y_pred\nplt.scatter(y_train,y_pred)\nplt.xlabel('Time to Failure From the Training Data')\nplt.ylabel('Predicted Time to Failure')\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)]) # Perfect correlation\nscore = mean_absolute_error(y_train.values.flatten(), y_pred)\nprint(f'Score: {score:0.3f}')","af6dbe87":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')","43e682e9":"X_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)\n# Create new features for the testing dataset\nfor segment_id in X_test.index:\n    seg = pd.read_csv('..\/input\/test\/' + segment_id + '.csv')\n    create_features(segment_id, seg, X_test)\n    \nX_test_scaled = scaler.transform(X_test)\n","153791d3":"X_test.head()","08e0e82b":"y_lm_submission= lm.predict(X_test_scaled)","75324d31":"y_svm_submission= svm.predict(X_test_scaled)","c7d14479":"columns = ['Training Error','Kaggle Score']\nindex = ['Linear Regresion']\nsummary = pd.DataFrame([[2.061,1.673],[2.014,1.563]],columns=columns,index=index)\nsummary","17e72871":"submission['time_to_failure'] = y_svm_submission\nsubmission.to_csv('submission.csv')","133c6e9f":"# 5. Feature engeneering.\n\nThe input dataset has just one feature that can be used to predict the time of failure, therefore we are going to decompose several features from the acoustic_data feature to be included in the prediction model.\n\ntime_to_failure = f(feature1, feature2, feature3)\n\nWhere all the features are derived from acoustic_data feature\n\n**New training dataframe**\n*  The input training dataframe contains a single, continuous training segment of experimental data\n* The input training dataframe will be used to create a new training dataframe. Each row of the new dataframe corresponds to an experiment segment that would correspond to the segment's lenght in the test dataset.  Example:\n\n**Segment         &nbsp; | &nbsp;   Segment Average &nbsp; | &nbsp;   std &nbsp; | &nbsp; \tmax &nbsp; | &nbsp; \tmin**\n <br>segment 1\n <br>segment 2\n <br>segment 3","52664ec9":" # LANL Earthquake Prediction\n# 1. Overview\nThe goal of this competition is to predict the time remaining before an earthquake takes place using laboratory data. A forecasting model of the time at which an earthquake will occur is imperative because of their devastating consequences","8fa40d29":"We use the entired dataset to train the model, before we do the submision, we would estimate the training error\n## 8.1 Training Error for the Linear Regresion Model ","76b3983a":"## 9.3 Submision file","98d272d3":"**Observations:**\n\n* The time of failure feature requires high precision to distinguish differences between observations\n* Acoustic data ocsilates from negative to positive values.","4a61caea":" ## 4.2. Data Visualization","66357ffd":"Create Testing DataFrame Based on Features we Created for the Training DataFrame\n\n--> Used the Beanchmark as an starting point from: https:\/\/www.kaggle.com\/inversion\/basic-feature-benchmark","036c5690":"## 9.1.  Prediction Time to Failure Linear Regresion\n### 9.1.1.  Linear Regresion Model","5ca28f4e":"Distribution of acustic data for the first segment","86db0f34":"# 7. Building a Predictive Model\nPredict whether a passenger will survive or not using Classification Algorithms.\n\n## 7.1 Linear Regresion model","50ea8115":"# 6. Scaling data","3f0ad61b":"# 2. Import Libraries","3692d0cf":"# 9.  Create a Submision File\nLoad files","d2b76744":"# 7.2 Nu Support Vector Regression","36474f60":"# 3. Acquire data","caea4f1b":"# 8. Evaluate training error","7ae16c19":"## 4.3. Missing Data\n\nThere are not missing data.","5b0f9f53":"### 9.1.2 . Nu Support Vector Regression","f7628379":"## 4.1. Data Preview\n\nThe training dataset has more thatn 600 million rows of data. \n\n### Features:\n\n** Training data\n\nThe input dataset has two continuous numerical features:\n1.  acoustic_data - the seismic signal [int16]\n2.  time_to_failure - the time (in seconds) until the next laboratory earthquake [float64]\n\n** Test data\n\n* The test data is a folder containing many small segments of test data.\n* Every segment of test data has one feature \"acoustic_data\" with **150,000** rows\n","646702fb":"## 9.2 Kaggle Score","7db24073":"## 8.2 Training Error for the Nu Support Vector Regression Model ","5afa5bcc":" # 4. Exploratory Data Analysis\nWe start by doing an exploratory data analysis."}}