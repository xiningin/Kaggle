{"cell_type":{"34a486c9":"code","5b60663d":"code","c58f2bc6":"code","84e89ab6":"code","e96d4a21":"code","a5b59090":"code","d6a25be0":"code","a8349333":"code","3aaeaae2":"code","f749fa10":"code","8238e74a":"code","ec01e53c":"code","3ef3f9b2":"code","d483ead5":"code","3e826477":"code","4bbf7781":"code","784563e2":"code","e396eca6":"code","8d6d74c2":"code","dc1c3b33":"code","a86a6115":"code","6a831492":"code","0ed642d9":"code","33c3d8ed":"code","b5787052":"code","d594852b":"code","d9ccab6d":"code","b0e54b5c":"code","12318e72":"code","62f938ab":"code","4c8e4d83":"code","642404ca":"code","d5e88380":"code","2aa774be":"code","b6b33c8f":"code","d08d3988":"code","e7084bc7":"code","5061a039":"code","ce6e64e9":"code","c5735384":"code","fff5584d":"code","f0030cff":"code","8aec756c":"code","99266c3a":"code","a26d4930":"code","ca2d1aff":"markdown","2e4c511b":"markdown","a8c9d0ef":"markdown","02920323":"markdown","889b0bdf":"markdown","ddd89119":"markdown","e382e722":"markdown","60392e34":"markdown","bdf5cb36":"markdown","8873f4ab":"markdown","b3e1435d":"markdown","13003b25":"markdown","5c69fd95":"markdown","fb873c48":"markdown","16e8a4c5":"markdown","3759389f":"markdown","92874b80":"markdown","1765f58f":"markdown","b7c9ac8c":"markdown","ec0f3e5f":"markdown","aacbac1e":"markdown","db452c40":"markdown","74c24a6c":"markdown","de71e8db":"markdown","9e0991a2":"markdown","12183299":"markdown","82d39d33":"markdown","ae640a87":"markdown","980c1aa1":"markdown","dd873311":"markdown"},"source":{"34a486c9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.transforms as transforms\nimport seaborn as sns\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5b60663d":"df = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","c58f2bc6":"df.head()","84e89ab6":"df.shape","e96d4a21":"df.info()","a5b59090":"df.isnull().sum()","d6a25be0":"df.describe()","a8349333":"plt.figure(figsize=(16,8))\nsns.countplot(df[\"quality\"], palette=\"Oranges\")\nplt.title(\"Distribution of Wine Qualities\", size=28, fontweight=\"bold\")\nplt.xlabel(\"Quality\", size=18, fontweight=\"bold\")\nplt.ylabel(\"Count\", size=18, fontweight=\"bold\")","3aaeaae2":"plt.figure(figsize=(16,8))\nsns.distplot(df['quality'], color=\"red\")\nplt.xlabel(\"Quality\", size=18, fontweight=\"bold\")","f749fa10":"mean_alcohol_list = []\nmean_ph_list = []\nmean_density_list = []\nmean_residual_sugar_list = []\n\nquality_list = df.quality.unique()\nquality_list.sort()\nfor i in quality_list:\n    df_quality = df[df[\"quality\"]==int(i)]\n    mean_alcohol = df_quality[\"alcohol\"].mean()\n    mean_ph = df_quality[\"pH\"].mean()\n    mean_density = df_quality[\"density\"].mean()\n    mean_residual_sugar = df_quality[\"residual sugar\"].mean()\n    \n    mean_alcohol_list.append(mean_alcohol)\n    mean_ph_list.append(mean_ph)\n    mean_density_list.append(mean_density)\n    mean_residual_sugar_list.append(mean_residual_sugar)\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(26,12))\nax1.bar(quality_list, mean_alcohol_list, color = \"#ECC679\")\nax1.set_title(\"Average Alcohol Values of Qualities \", size=20)\nax1.set_xlabel(\"Quality\", size=13)\nax1.set_ylabel(\"Alcohol\", size=13)\nax2.bar(quality_list, mean_ph_list, color = \"skyblue\")\nax2.set_title(\"Average pH Values of Qualities \", size=20)\nax2.set_xlabel(\"Quality\", size=13)\nax2.set_ylabel(\"pH\", size=13)\nax2.set_ylim([3,3.5])\nax3.bar(quality_list, mean_density_list, color = \"green\")\nax3.set_title(\"Average Density Values of Qualities \", size=20)\nax3.set_xlabel(\"Quality\", size=13)\nax3.set_ylabel(\"Density\", size=13)\nax3.set_ylim([0.99,1.0])\nax4.bar(quality_list, mean_residual_sugar_list, color = \"gray\")\nax4.set_title(\"Average Residual Sugar Values of Qualities \", size=20)\nax4.set_xlabel(\"Quality\", size=13)\nax4.set_ylabel(\"Residual Sugar\", size=13)\nax4.set_ylim([1.5,3])\nplt.tight_layout(pad=3)","8238e74a":"plt.figure(figsize=(16,8))\ndf_corr = df.corr()\nsns.heatmap(df_corr, annot=True, cmap=\"GnBu\")","ec01e53c":"for quality in df.quality:\n    if quality < 6.5:\n        df['quality'] = df['quality'].replace([int(quality)],'Bad')\n    else:\n        df['quality'] = df['quality'].replace([int(quality)],'Good')\nplt.figure(figsize = (16,8))\nlabels = df[\"quality\"].unique().tolist()\nsizes = df[\"quality\"].value_counts().tolist()\ncolors = [\"#59CBC0\", \"#BFD7D4\"]\nexplode = (0, 0)\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90, textprops={'fontsize': 14, \"fontweight\" : \"bold\"}, colors=colors)\nplt.title(\"Distribution of Wines\", size=28, fontweight=\"bold\")","3ef3f9b2":"recall_score_dict = {}\nacc_score_dict = {}\nprecision_score_dict = {}\n\nle = LabelEncoder()\ndf[\"quality\"] = le.fit_transform(df[\"quality\"])","d483ead5":"x = df.drop(\"quality\", axis=1)\ny = df[\"quality\"]","3e826477":"ss = StandardScaler()\nx = ss.fit_transform(x)","4bbf7781":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)","784563e2":"lr = LogisticRegression()\nlr.fit(X_train, y_train)","e396eca6":"y_predict_lr = lr.predict(X_test)\nprint(\"Accuracy Score :\",lr.score(X_test, y_test))","8d6d74c2":"cm_lr = confusion_matrix(y_test, y_predict_lr)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_lr, annot=True, cmap=\"Blues\", fmt=\".1f\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","dc1c3b33":"print(classification_report(y_test, y_predict_lr))","a86a6115":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train)\nprint(\"Quality Distribution Before SMOTE Operation : \\n\", y_train.value_counts(), \"\\n\")\nprint(\"Quality Distribution After SMOTE Operation : \\n\" ,y_train_res.value_counts())","6a831492":"plt.figure(figsize=(16,8))\ncolors = [\"#59CBC0\", \"#BFD7D4\"]\nlabels = [\"Bad\", \"Good\"]\ny_train_res.value_counts().plot(kind=\"pie\",shadow=True, autopct='%1.1f%%', \n                                textprops={'fontsize': 14, \"fontweight\" : \"bold\"},\n                                colors = colors, labels=labels)\nplt.title(\"Distribution of Wine Qualities After SMOTE\", size=15)","0ed642d9":"lr2 = LogisticRegression()\nlr2.fit(X_train_res, y_train_res)","33c3d8ed":"y_predict_lr2 = lr2.predict(X_test)\nacc_score_dict[\"LR\"] = lr2.score(X_test, y_test)\nrecall_score_dict[\"LR\"] =recall_score(y_test, y_predict_lr2)\nprecision_score_dict[\"LR\"] = precision_score(y_test, y_predict_lr2)\nprint(\"Accuracy Score :\",lr2.score(X_test, y_test))","b5787052":"cm_lr2 = confusion_matrix(y_test, y_predict_lr2)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_lr2, annot=True, cmap=\"Blues\", fmt=\".1f\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","d594852b":"print(classification_report(y_test, y_predict_lr2))","d9ccab6d":"print(\"CV Score : \", cross_val_score(estimator=lr2, X = X_train_res, y = y_train_res, cv=5).mean())","b0e54b5c":"dtc = DecisionTreeClassifier(criterion = 'gini', min_samples_split = 10, random_state=42)\ndtc.fit(X_train_res, y_train_res)","12318e72":"y_predict_dtc = dtc.predict(X_test)\nacc_score_dict[\"DTC\"] = dtc.score(X_test, y_test)\nrecall_score_dict[\"DTC\"] = recall_score(y_test, y_predict_dtc)\nprecision_score_dict[\"DTC\"] = precision_score(y_test, y_predict_dtc)\nprint(\"Accuracy Score :\",dtc.score(X_test, y_test))","62f938ab":"cm_dtc = confusion_matrix(y_test, y_predict_dtc)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_dtc, annot=True, cmap=\"Blues\", fmt=\".1f\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","4c8e4d83":"print(classification_report(y_test, y_predict_dtc))","642404ca":"print(\"CV Score : \", cross_val_score(estimator=dtc, X = X_train_res, y = y_train_res, cv=5).mean())","d5e88380":"rfc = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=10)\nrfc.fit(X_train_res, y_train_res)","2aa774be":"y_predict_rfc = rfc.predict(X_test)\nacc_score_dict[\"RFC\"] = rfc.score(X_test, y_test)\nrecall_score_dict[\"RFC\"] =recall_score(y_test, y_predict_rfc)\nprecision_score_dict[\"RFC\"] = precision_score(y_test, y_predict_rfc)\nprint(\"Accuracy Score :\",rfc.score(X_test, y_test))","b6b33c8f":"cm_rfc = confusion_matrix(y_test, y_predict_rfc)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_rfc, annot=True, cmap=\"Blues\",fmt=\".1f\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","d08d3988":"print(classification_report(y_test, y_predict_rfc))","e7084bc7":"print(\"CV Score : \", cross_val_score(estimator=rfc, X = X_train_res, y = y_train_res, cv=5).mean())","5061a039":"svm = SVC(probability=True)\nsvm.fit(X_train_res, y_train_res)","ce6e64e9":"y_predict_svm = svm.predict(X_test)\nacc_score_dict[\"SVM\"] = svm.score(X_test, y_test)\nrecall_score_dict[\"SVM\"] =recall_score(y_test, y_predict_svm)\nprecision_score_dict[\"SVM\"] = precision_score(y_test, y_predict_svm)\nprint(\"Accuracy Score :\",svm.score(X_test, y_test))","c5735384":"cm_svm = confusion_matrix(y_test, y_predict_svm)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_svm, annot=True, cmap=\"Blues\",fmt=\".1f\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","fff5584d":"print(classification_report(y_test, y_predict_svm))","f0030cff":"print(\"CV Score : \", cross_val_score(estimator=svm, X = X_train_res, y = y_train_res, cv=5).mean())","8aec756c":"print(\"Accuracy Scores for Each Model After SMOTE: \",acc_score_dict)\nprint(\"Recall Scores for Each Model After SMOTE: \",recall_score_dict)\nprint(\"Precision Scores for Each Model After SMOTE: \",precision_score_dict)","99266c3a":"labels = acc_score_dict.keys()\nacc_scores = acc_score_dict.values()\nrecall_scores = recall_score_dict.values()\nprecision_scores = precision_score_dict.values()\n\nx = np.arange(len(labels))\nwidth = 0.30\n\nfig, ax = plt.subplots(figsize=(16,8))\nrects1 = ax.bar(x - width, acc_scores, width, label='Accuracy', color=\"#056937\")\nrects2 = ax.bar(x, recall_scores, width, label='Recall', color=\"#062D5F\")\nrects3 = ax.bar(x + width, precision_scores, width, label='Precision', color=\"#AE4D4D\")\n\nax.set_xlabel('Model', fontsize=15)\nax.set_ylabel('Score', fontsize=15)\nax.set_title('Comparison of Accuracy, Recall and Precision Scores of Models', fontsize=22, fontweight=\"bold\", fontstyle=\"italic\")\nax.set_xticks(x)\nax.set_xticklabels(labels)\nplt.ylim([0,1])\nplt.xticks(fontsize=16)\nlegend = ax.legend(bbox_to_anchor=(1, 1), loc='upper left',prop={\"size\":18})\nlegend.set_title('Score',prop={'size':20})\n\nfor i, v in enumerate(acc_score_dict.values()):\n    plt.text(i-0.43, v+0.025, \"{:.4f}\".format(v), color='#056937', va='center', fontweight='bold', size=14)\nfor i, v in enumerate(recall_score_dict.values()):\n    plt.text(i-0.12, v+0.025, \"{:.4f}\".format(v), color='#062D5F', va='center', fontweight='bold',size=14)\nfor i, v in enumerate(precision_score_dict.values()):\n    plt.text(i+0.17, v+0.025, \"{:.4f}\".format(v), color='#AE4D4D', va='center', fontweight='bold',size=14)\n","a26d4930":"prob_lr = lr2.predict_proba(X_test)[:,1]\nprob_dtc = dtc.predict_proba(X_test)[:,1]\nprob_rfc = rfc.predict_proba(X_test)[:,1]\nprob_svm = svm.predict_proba(X_test)[:,1]\n\nprob_dict = {\"ROC LR\": prob_lr, \"ROC DTC\": prob_dtc, \"ROC RFC\": prob_rfc, \"ROC SVM\": prob_svm}\n\nfor model, prob in prob_dict.items():\n    fpr, tpr, threshold = metrics.roc_curve(y_test, prob)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.figure(figsize=(10,6))\n    plt.plot(fpr, tpr, color = \"b\", label = \"AUC = %0.2f\" %roc_auc)\n    plt.legend(loc=\"lower right\", prop={\"size\":15})\n    plt.xlabel(\"False Positive Rate\", size=12)\n    plt.ylabel(\"True Positive Rate\", size=12)\n    plt.plot([0,1], [0,1], \"r--\")\n    plt.title(str(model), size=20)","ca2d1aff":"<p style=\"color:Tomato;\">Score is 86.45%. Not bad! But, only the accuracy score makes a model nice?<\/p>\n<p style=\"color:Tomato;\">Let's take a look at the confusion matrix!<\/p>","2e4c511b":"# *Correlation Matrix*","a8c9d0ef":"<p style=\"color:Tomato;\">We know that we are dealing with an imbalanced dataset. But, before applying a method to handle with imbalanced dataset, let's see a bad model at first.<\/p>","02920323":"<p style=\"color:Tomato;\">Apply SMOTE method to handle with imbalanced target values.<\/p>","889b0bdf":"<p style=\"color:Tomato;\">Let's check if there are null values or not.<\/p>","ddd89119":"<p style=\"color:Tomato;\">Splitting dataset<\/p>","e382e722":"<h2 style=\"color:DodgerBlue;\">ROC Curves and AUCs<\/h2>","60392e34":"<p style=\"color:Tomato;\">Let's visualize the accuracy and recall scores to distinguish better. <\/p>","bdf5cb36":"<h2 style=\"color:DodgerBlue;\">Random Forest Classifier<\/h2>","8873f4ab":"<h2 style=\"color:DodgerBlue;\">SVM<\/h2>","b3e1435d":"<p style=\"color:Tomato;\">We have increased the number of good wine samples in the data. <\/p>\n<p style=\"color:Tomato;\">Let's apply the Logistic Regression again. <\/p>","13003b25":"<p style=\"color:Tomato;\">Necessary Libraries<\/p>","5c69fd95":"<p style=\"color:Tomato;\">Now, let's try other machine learning algorithms with SMOTE to compare results. <\/p>","fb873c48":"# *Data Analysis and Visualizations*","16e8a4c5":"# *Data Preparation*","3759389f":"# *Comparison*","92874b80":"# *Classification*","1765f58f":"<p style=\"color:DodgerBlue;\"><u><b>Some Positive Correlations<\/b><\/u> <p>\n<p style=\"color:Tomato;\">1-Fixed Acidity and Citric Acid<\/p>\n<p style=\"color:Tomato;\">2-Fixed Acidity and Density<\/p>\n<p style=\"color:Tomato;\">3-Total Sulfur Dioxide and Free Sulfur Dioxide<\/p>\n\n<p style=\"color:DodgerBlue;\"><u><b>Some Negative Correlations<\/b><\/u> <p>\n<p style=\"color:Tomato;\">1-pH and Fixed Acidity<\/p>\n<p style=\"color:Tomato;\">2-pH and Citric Acid<\/p>\n<p style=\"color:Tomato;\">3-Citric Acid and Volatile Acidity<\/p>\n<p style=\"color:Tomato;\">4-Alcohol and Density<\/p>","b7c9ac8c":"<h2 style=\"color:DodgerBlue;\">Decision Tree Classifier<\/h2>","ec0f3e5f":"<p style=\"color:Tomato;\">Let's check CV score as well.<\/p>","aacbac1e":"<h2 style=\"color:DodgerBlue;\">Logistic Regression with SMOTE<\/h2>","db452c40":"<p style=\"color:Tomato;\">Let's take a glance at the correlation matrix to understand the relations between feature columns and target column.<\/p>","74c24a6c":"<p style=\"color:Tomato;\">As it is seen on the pie chart. We have 86.4% bad quality wines and 13.6% good quality wines which is highly imbalanced. It may affect performances of our models.<\/p>\n<p style=\"color:Tomato;\">Let's arrange the data!<\/p>","de71e8db":"<p style=\"color:Tomato;\">There is no any null value.<\/p>","9e0991a2":"<p style=\"color:Tomato;\">Accuracy score has been decreased from 86.45% to 80.41% but, the model is better at identifying good quality wines. <\/p>","12183299":"<p style=\"color:Tomato;\">Let's check the ROC curves and AUCs for each model. Higher the AUC, better the model is at predicting good wines as good wines and bad wines as bad wines. <\/p>","82d39d33":"<h2 style=\"color:DodgerBlue;\">Logistic Regression<\/h2>","ae640a87":"<p style=\"color:Tomato;\">Even though, the accuracy score is high, the model is very bad at identifying of \"1\" values which are good quality wines.<\/p>\n<p style=\"color:Tomato;\">Let's check recall.<\/p>","980c1aa1":"<p style=\"color:Tomato;\">As we expected, recall score is very low at identifying good quality wines.<\/p>\n<p style=\"color:Tomato;\">That's not a good model due to imbalanced target values. Let's try to improve it!<\/p>","dd873311":"<p style=\"color:Tomato;\">Encoding and Normalization<\/p>"}}