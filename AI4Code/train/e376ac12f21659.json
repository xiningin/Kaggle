{"cell_type":{"727ac293":"code","9ca4e24a":"code","0619b9f3":"code","8e8ddef7":"code","54ca04df":"code","f3f5bd42":"code","ddbb7d11":"code","50d294c3":"code","0041045b":"code","a0120b35":"code","2a714ed9":"code","001d79f9":"code","1200f767":"code","d274190e":"code","1e0d20ce":"code","34d14a98":"code","086a3674":"code","0df7455c":"code","ef782923":"code","ae33edaa":"code","43a43db1":"code","387e569e":"code","2a24f7c6":"code","0284bd3a":"code","9748204b":"code","1d9aa0f3":"code","5233a50c":"code","137ef0df":"code","52367228":"code","5bd719d8":"code","f845f242":"code","d6fef13b":"code","f29d50b8":"code","570ac3ff":"code","750db570":"code","9814e1e4":"code","18b3665d":"code","9f63ec21":"code","503dd97d":"code","9277d65a":"code","52f69af4":"code","a4adccff":"code","8b826528":"code","3456a04c":"code","f04440fb":"code","875052e7":"code","a1fd9f0a":"code","a2dbdf3a":"code","755fb976":"code","11e8bbfc":"markdown","c6aca20a":"markdown","18db5cab":"markdown","dca68e04":"markdown","5a46d91d":"markdown","13f028f6":"markdown","8f1fcac5":"markdown","777dbd9f":"markdown","27da58aa":"markdown","fec02977":"markdown","7f364f24":"markdown","a6a15a90":"markdown","741ebe0f":"markdown","fc54c074":"markdown","1957d43c":"markdown","f21cbc11":"markdown","3082c0fe":"markdown","86c7cded":"markdown","0a9d11fe":"markdown","e86d11dc":"markdown","25bb4ade":"markdown","deaa0dd3":"markdown","575aa752":"markdown","a9b8f8d5":"markdown","a71283b1":"markdown","52136f26":"markdown","041255fb":"markdown","0ee2f9d7":"markdown"},"source":{"727ac293":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn as s\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport math\n\nfrom sklearn.preprocessing import StandardScaler","9ca4e24a":"dfw = pd.read_csv(\"..\/input\/ccdata\/CC GENERAL.csv\" ,sep=',',engine='python')\ndfw.head()","0619b9f3":"dfw.describe()","8e8ddef7":"dfw.info()","54ca04df":"dfw.isna().sum()\n","f3f5bd42":"dfh = dfw.copy()\ndfh.drop(columns=['CUST_ID'] , axis=1 , inplace=True)\n\nfor col in dfh:\n    dfh[[col]].hist()","ddbb7d11":"fig = plt.figure(figsize=(12,10))\nsns.heatmap(dfw.corr() , annot=True)","50d294c3":"dfh.isnull().sum()","0041045b":"dfh.loc[(dfh['MINIMUM_PAYMENTS'].isnull()==True),'MINIMUM_PAYMENTS']=dfh['MINIMUM_PAYMENTS'].mean()\ndfh.loc[(dfh['CREDIT_LIMIT'].isnull()==True),'CREDIT_LIMIT']=dfh['CREDIT_LIMIT'].mean()","a0120b35":"dfh.isnull().sum()","2a714ed9":"fig = plt.figure(figsize=(20,20))\nfor col in range(len(dfh.columns)) :\n    fig.add_subplot(6,3,col+1)\n    sns.boxplot(x=dfh.iloc[ : , col])\nplt.show()","001d79f9":"for col in dfh:\n    dfh[[col]].hist()","1200f767":"from sklearn.preprocessing import MinMaxScaler","d274190e":"scaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(dfh)","1e0d20ce":"dfn = pd.DataFrame(X_scaled) \n\n","34d14a98":"from sklearn import preprocessing","086a3674":"dfn","0df7455c":"dfn.columns","ef782923":"dfn.columns = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n            'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', \n            'CASH_ADVANCE', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n            'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', \n            'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS', \n            'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']","ae33edaa":"dfn.describe()","43a43db1":"fig = plt.figure(figsize=(20,20))\nfor col in range(len(dfn.columns)) :\n    fig.add_subplot(6,3,col+1)\n    sns.boxplot(x=dfn.iloc[ : , col])\nplt.show()","387e569e":"for col in dfn:\n    dfn[[col]].hist()","2a24f7c6":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(dfn)\ndfc = pca.transform(dfn)","0284bd3a":"dfc.shape","9748204b":"from sklearn.cluster import KMeans","1d9aa0f3":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(dfn)\n    sse[k] = kmeans.inertia_\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Numero de cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","5233a50c":"wcss= []\nrange_values = range(1, 20)\nfor i in range_values:\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(dfc)\n    wcss.append(kmeans.inertia_)","137ef0df":"\nplt.figure(figsize=(15,8)) \nplt.plot(wcss, 'bo-', color='c')\nplt.xlabel('Number of clusters Clusters', fontsize=14)\nplt.ylabel('WCSS', fontsize=14);","52367228":"kmeans =  KMeans(n_clusters=3, max_iter=600, algorithm = 'auto')\nkmeans.fit(dfc)","5bd719d8":"plt.scatter(dfc[:, 0], dfc[:, 1],\n            c= kmeans.labels_.astype(float), edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('Spectral', 10))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.colorbar();","f845f242":"dfn[\"cluster\"] = kmeans.labels_.astype(float)","d6fef13b":"dfn","f29d50b8":"for c in dfn:\n    grid= sns.FacetGrid(dfn, col='cluster')\n    grid.map(plt.hist, c)    ","570ac3ff":"dfc","750db570":"pca = PCA(n_components=2)\npca.fit(dfn)\ndfd = pca.transform(dfn)","9814e1e4":"from sklearn.cluster import DBSCAN","18b3665d":"db = DBSCAN(eps=0.5, min_samples=10).fit(dfd)\nlabels = db.labels_","9f63ec21":"plt.scatter(dfd[:, 0], dfd[:, 1],\n            c= labels.astype(float), edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('Spectral', 10))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.colorbar();","503dd97d":"dfn[\"clusterDBSCAN\"] = labels.astype(float)","9277d65a":"dfn","52f69af4":"for c in dfn:\n    grid= sns.FacetGrid(dfn, col='clusterDBSCAN')\n    grid.map(plt.hist, c)    ","a4adccff":"dff = dfn.groupby(\"cluster\")","8b826528":"pd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)","3456a04c":"dff.describe(include=\"all\")","f04440fb":"dfh[\"cluster\"] = kmeans.labels_.astype(float)","875052e7":"dfhh = dfh.groupby(\"cluster\")","a1fd9f0a":"dfh.describe()\nfor c in dfh:\n    grid= sns.FacetGrid(dfh, col='cluster')\n    grid.map(plt.hist, c)    ","a2dbdf3a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\ntree_builder =DecisionTreeClassifier(max_depth = 3)\nmodel = tree_builder.fit(dfh[['BALANCE',\n'ONEOFF_PURCHASES_FREQUENCY',\n'PURCHASES_INSTALLMENTS_FREQUENCY',\n'PURCHASES_FREQUENCY',\n'MINIMUM_PAYMENTS',\n'CREDIT_LIMIT']], dfh[\"cluster\"])","755fb976":"fig = plt.figure(figsize=(100,100))\ntree.plot_tree(model, feature_names=['BALANCE',\n'ONEOFF_PURCHASES_FREQUENCY',\n'PURCHASES_INSTALLMENTS_FREQUENCY',\n'PURCHASES_FREQUENCY',\n'MINIMUM_PAYMENTS',\n\"CREDIT_LIMIT\"],\nclass_names = ['NORMAL', 'NORMAL', 'PREMIUM'],\n)","11e8bbfc":"### Normalizacion","c6aca20a":"Volvimos a utilizar PCA para poder reducir la dimensionalidad de los datos.","18db5cab":"We did a Boxplot of the data to see if there were outliers and we found a large number of outliers in the data","dca68e04":"The clusters were divided mainly into two groups. The Premium group who are people with high purchasing power and tend to spend more money and then the Normal group who have a more regular check according to their credit history","5a46d91d":"# Parte 5","13f028f6":"Hicimos un Boxplot de los datos para ver si habia outliers y encontramos una gran cantidad de outliers dentro de losdatos ","8f1fcac5":"Many outliers were found so they could not be eliminated since the amount of data would be significantly reduced, so it was decided to normalize the data and avoid the problem that generates outliers.","777dbd9f":"Relizamos el elbow method para poder ver que cantidad de clusters podia ser el mas adecuado con estos datos\nSe busca definir la cantidad de cl\u00fasteres de manera que se minimice la variaci\u00f3n total dentro del cl\u00faster (conocida como WCSS \"quared distance between each point and the centroid in a cluster\").\nPara determinar el n\u00famero \u00f3ptimo de clusters , tenemos que seleccionar el valor de \"k\" en el \"codo\", es decir, el punto despu\u00e9s del cual la distorsi\u00f3n, inercia comienza a disminuir de forma lineal. Por lo tanto, para los datos dados, concluimos que el n\u00famero \u00f3ptimo de clusters para los datos es 3.\n\n","27da58aa":"The main concept of the DBSCAN algorithm is to locate high-density regions that are separated from each other by low-density regions. is a simple density algorithm that implements the notion of density by means of a center-based procedure.\nDBSCAN groups points that are close to each other based on a distance measurement (usually euclidean distance) and a minimum number of points. It also marks points that are in low-density regions as outliers.\nIt is efficient for large volumes of data and efficiently handles the presence of outliers and noise.","fec02977":"Here we can see that there was missing data in \"CREDIT_LIMIT\" and in \"MINIMUM_PAYMENTS\"","7f364f24":"Los algoritmos para clustering basado en densidad identifican regiones de alta densidad que estan rodeadas de areas poco densas en donde cada una de las regiones densas identificadas se corresponde con un cluster.\nEl concepto principal del algoritmo DBSCAN es localizar regiones de alta densidad que est\u00e1n separadas entre s\u00ed por regiones de baja densidad. es un algoritmo de densidad simple que implementa la nocion de densidad por medio de un procedimiento basado en centro.\nDBSCAN agrupa los puntos cercanos entre s\u00ed en funci\u00f3n de una medici\u00f3n de distancia (generalmente distancia euclidiana) y un n\u00famero m\u00ednimo de puntos. Tambi\u00e9n marca como valores at\u00edpicos los puntos que se encuentran en regiones de baja densidad.\nEs eficiente para grandes vol\u00famenes de datos y Maneja eficientemente la presencia de outliers y ruido. Se ve poco afectado por \u00e9stos.","a6a15a90":"Utilizamos PCA para poder reducir la dimensionalidad de los datos y poder graficar los clusters correctamente ","741ebe0f":"A decisiontree was made to be able to indicate through a survey to which group the new clients who do not have a credit history belong.","fc54c074":"Colocamos en el dataset a que cluster iba a pertenecer cada persona asi se pueden analizar los datos dividiendolos en clusters y entonces poder ver las caracterisiticas de cada grupo","1957d43c":"## K-means","f21cbc11":"## Elbow method","3082c0fe":"# Parte 4","86c7cded":"## DBSCAN","0a9d11fe":"We use PCA to reduce the dimensionality of the data so we could be able to graph the clusters correctly","e86d11dc":"# Parte 2","25bb4ade":"Eliminamos la columna CLUSTID porque no iba a aportar al estudio del caso","deaa0dd3":"# Parte 1","575aa752":"Se encontraron muchos outliers por lo que no se podian eliminar ya que se  reduciria notablemente la cantidad de datos, por lo que se decidio  normalizar los datosy asi se evita el problema que genera los outliers y que los datos estan en diferentes escalas ","a9b8f8d5":"We used the elbow method to see how many clusters could be the most appropriate with these data\nThe aim is to define the number of clusters in such a way as to minimize the total variation within the cluster (known as WCSS \"quared distance between each point and the centroid in a cluster\").\nTo determine the optimal number of clusters, we have to select the value of \"k\" at the \"elbow\", that is, the point after which the distortion, inertia begins to decrease linearly. Therefore, for the given data, we conclude that the optimal number of clusters for the data is 3.","a71283b1":"# Parte 3","52136f26":"Aca podemos ver que habia datos faltantes en los datos de \"CREDIT_LIMIT \" y en \" MINIMUM_PAYMENTS\"","041255fb":"### PCA","0ee2f9d7":"We put in the dataset which cluster each person was going to belong to, so that the data can be analyzed dividing them into clusters and then we can see the characteristics of each group"}}