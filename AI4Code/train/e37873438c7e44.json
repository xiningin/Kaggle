{"cell_type":{"9cc9392c":"code","975f7df5":"code","8ce257eb":"code","6b15ee0b":"code","a230ee4e":"code","1644c4d0":"code","7c3a17c3":"code","42fee799":"code","b77487a8":"code","601a32bb":"code","f8ed01ce":"code","39ed8ed1":"code","ae9c0705":"code","4e3a9d96":"code","9c588df3":"code","588d762c":"code","4853b375":"code","1d42ac8f":"code","e4f2a055":"markdown","659294c0":"markdown","3ba6b53e":"markdown","9e85691c":"markdown","edebdd57":"markdown","6e95acd0":"markdown","7e945f31":"markdown","880d28ea":"markdown","21784d4a":"markdown","c0b30228":"markdown","29e672e6":"markdown","8e160dff":"markdown","83925de6":"markdown"},"source":{"9cc9392c":"import time\nstart_time = time.time()","975f7df5":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score","8ce257eb":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsub_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","6b15ee0b":"train_df.head()","a230ee4e":"train_df.isna().sum()","1644c4d0":"X = train_df[\"text\"]\ny = train_df[\"target\"]\nX_test = test_df[\"text\"]\nX.shape, y.shape, X_test.shape","7c3a17c3":"X_for_tf_idf = pd.concat([X, X_test])","42fee799":"tfidf = TfidfVectorizer(\n                        stop_words = 'english',\n#                       token_pattern=r'(?u)\\b\\w\\w+\\b',\n                        token_pattern=r'(?u)(\\b\\w\\w+\\b|\\#|\\@)'                        \n)\n\n\ntfidf.fit(X_for_tf_idf)\n\nX = tfidf.transform(X)\nX_test = tfidf.transform(X_test)\ndel X_for_tf_idf","b77487a8":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)","601a32bb":"parameters = { \n            'gamma': [0.7 , 1. , 'auto', 'scale']\n}\nmodel = GridSearchCV(\n#                       SVC(kernel='rbf'), \n                        SVC(kernel='sigmoid'), \n                        parameters, cv=4, n_jobs=-1\n).fit(X_train, y_train)","f8ed01ce":"y_val_pred = model.predict(X_val)\naccuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred)","39ed8ed1":"confusion_matrix(y_val, y_val_pred)","ae9c0705":"y_test_pred = model.predict(X_test)\nsub_df[\"target\"] = y_test_pred","4e3a9d96":"train_df_copy = train_df\ntrain_df_copy = train_df_copy.fillna('None')\nag = train_df_copy.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\n\nag.sort_values('Disaster Probability', ascending=False).head(20)","9c588df3":"count = 2\nprob_disaster = 0.9\nkeyword_list_disaster = list(ag[(ag['Count']>count) & (ag['Disaster Probability']>=prob_disaster)].index)\n#we print the list of keywords which will be used for prediction correction \nkeyword_list_disaster","588d762c":"ids_disaster = test_df['id'][test_df.keyword.isin(keyword_list_disaster)].values\nsub_df['target'][sub_df['id'].isin(ids_disaster)] = 1\n","4853b375":"sub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head(20)","1d42ac8f":"print(\"--- %s seconds ---\" % (time.time() - start_time))","e4f2a055":"We split train data to train and validation subsets","659294c0":"### Using keywords for better prediction. ","3ba6b53e":"### Data loading","9e85691c":"Here we follow https:\/\/www.kaggle.com\/bandits\/using-keywords-for-prediction-improvement <br>\nThe idea is that some keywords with very high probability (sometimes = 1) signal we have  disaster (or usual) tweets. \nWe may add the extra 'keyword' feature to our model, but very simple approach also works. \nWe  make correction for the disaster tweets prediction of our model basing on the \"disaster\" keywords.\n","edebdd57":"### Make SVM prediction","6e95acd0":"### Training and Evaluating","7e945f31":"Results from https:\/\/www.kaggle.com\/ihelon\/starter-nlp-svm-tf-idf show the SVM approach may be quite promising for the Real or Not ? (disaster) Tweets classification. We use the notebook as our starting point. ","880d28ea":"We combine train and test datasets for fitting  in tf-idf.","21784d4a":"### TF-IDF preprocessing","c0b30228":"We change the default token pattern in TfidVectorizer from<br>\ntoken_pattern=r'(?u)\\b\\w\\w+\\b'<br>\nto<br>\ntoken_pattern=r'(?u)(\\b\\w\\w+\\b|\\#|\\@)'<br>\nSo, we extract and keep # and @ symbols as tokens in our vocabulary. \n","29e672e6":"We change the kernel from default( 'rbf')  to 'sigmoid' and use <br>\nGridSearchCV to find the best 'gamma' for SVM","8e160dff":"We use accuracy, confusion matrix and f1 to evaluate the model","83925de6":"### **Submission**"}}