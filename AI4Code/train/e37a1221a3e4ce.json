{"cell_type":{"7abe5d12":"code","9db4bea7":"code","128f1550":"code","50228dc1":"code","43949ff2":"code","8dfaa3ef":"code","53b48fe2":"code","d41c9027":"code","12cd6cb3":"code","3a730311":"code","840343eb":"code","aadf3c37":"code","6da09591":"code","0968cf3b":"code","8822e7e6":"code","b057439c":"code","b0e5dd09":"code","58d088e5":"code","c51d08e0":"code","9df1c5f2":"code","3f4764ef":"code","b5f01b96":"code","ba7b0c1a":"code","ffb0a1e4":"code","f05146f9":"code","c2ad17d0":"markdown","5da61c58":"markdown","d651b760":"markdown","dbaaa98e":"markdown","6349387d":"markdown"},"source":{"7abe5d12":"!conda remove -y greenlet\n!pip install pytorch-pretrained-bert\n!pip install allennlp\n!pip install https:\/\/github.com\/ceshine\/pytorch_helper_bot\/archive\/0.0.5.zip","9db4bea7":"!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-development.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-test.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-validation.tsv -q","128f1550":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"323\"\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\nfrom allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\n\nfrom helperbot import (\n    TriangularLR, BaseBot, WeightDecayOptimizerWrapper\n)","50228dc1":"BERT_MODEL = 'bert-large-uncased'\nCASED = False","43949ff2":"def extract_target(df):\n    df[\"Neither\"] = 0\n    df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n    df[\"target\"] = 0\n    df.loc[df['B-coref'] == 1, \"target\"] = 1\n    df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n    print(df.target.value_counts())\n    return df\n\ndf_train = pd.concat([\n    pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\"),\n    pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\n], axis=0)\ndf_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\ndf_train = extract_target(df_train)\ndf_test = extract_target(df_test)\nsample_sub = pd.read_csv(\"..\/input\/sample_submission_stage_1.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","8dfaa3ef":"df_train[\"text_length\"] = df_train.Text.str.len()\ndf_test[\"text_length\"] = df_test.Text.str.len()\ndf_train.sort_values(\"text_length\", inplace=True)\ndf_test.sort_values(\"text_length\", inplace=True)","53b48fe2":"df_train[\"A-offset\"].max(), df_train[\"B-offset\"].max(), df_train[\"Pronoun-offset\"].max()","d41c9027":"def tokenize(row, tokenizer):\n    break_points = sorted(\n        [\n            (\"A\", row[\"A-offset\"], row[\"A\"]),\n            (\"B\", row[\"B-offset\"], row[\"B\"]),\n            (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n        ], key=lambda x: x[0]\n    )\n    tokens, spans, current_pos = [], {}, 0\n    for name, offset, text in break_points:\n        tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n        # Make sure we do not get it wrong\n        assert row[\"Text\"][offset:offset+len(text)] == text\n        # Tokenize the target\n        tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n        spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n        tokens.extend(tmp_tokens)\n        current_pos = offset + len(text)\n    tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n    assert spans[\"P\"][0] == spans[\"P\"][1]\n    return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            self.y = df.target.values.astype(\"uint8\")\n        \n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            tokens, offsets = tokenize(row, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n        \n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx]\n\n    \ndef collate_examples(batch, truncate_len=450):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"    \n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets\n    labels = torch.LongTensor(transposed[2])\n    return token_tensor, offsets, labels","12cd6cb3":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n)","3a730311":"class FeatureExtractionModel(nn.Module):\n    \"\"\"To extract features from BERT.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device, use_layer: int = -2):\n        super().__init__()\n        self.device = device\n        self.use_layer = use_layer\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.span_extractor = EndpointSpanExtractor(\n            self.bert_hidden_size, \"x+y\"\n        ).to(device)            \n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        offsets = offsets.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=True)\n        bert_outputs = bert_outputs[self.use_layer]\n        spans_contexts = self.span_extractor(\n            bert_outputs, \n            offsets[:, :4].reshape(-1, 2, 2)\n        )\n        emb_P = torch.gather(\n            bert_outputs, 1,\n            offsets[:, [4]].unsqueeze(2).expand(-1, -1, self.bert_hidden_size)\n        ).squeeze(1)\n        return spans_contexts, emb_P","840343eb":"model = FeatureExtractionModel(BERT_MODEL, torch.device(\"cuda:0\"), use_layer=-2)\n# Make it deterministic\n_ = model.eval()","aadf3c37":"def extract_features(loader):\n    spc, embp = [], []\n    with torch.no_grad():\n        for token, offsets in tqdm_notebook(loader):\n            spans_contexts, emb_P = model(token, offsets)\n            spc.append(spans_contexts.cpu())\n            embp.append(emb_P.cpu())\n    return torch.cat(spc, dim=0), torch.cat(embp, dim=0)","6da09591":"train_ds = GAPDataset(df_train, tokenizer, labeled=False)\ntrain_loader = DataLoader(\n    train_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\nspc_train, embp_train = extract_features(train_loader)\nys_train = torch.from_numpy(df_train.target.values)\nspc_train.size(), embp_train.size(), ys_train.size()","0968cf3b":"test_ds = GAPDataset(df_test, tokenizer, labeled=False)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\nspc_test, embp_test = extract_features(test_loader)\nys_test = torch.from_numpy(df_test.target.values)\nspc_test.size(), embp_test.size(), ys_test.size()","8822e7e6":"torch.save([spc_train, embp_train, ys_train], \"train.pkl\")\ntorch.save([spc_test, embp_test, ys_test], \"test.pkl\")","b057439c":"spc_train, embp_train, ys_train = torch.load(\"train.pkl\")\nspc_test, embp_test, ys_test = torch.load(\"test.pkl\")","b0e5dd09":"assert np.array_equal(df_test.target, ys_test.numpy())\nassert np.array_equal(df_train.target, ys_train.numpy())","58d088e5":"class GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\".\/cache\/logs\/\", log_level=logging.INFO,\n        checkpoint_dir=\".\/cache\/model_cache\/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.6f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    def snapshot(self):\n        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir \/ \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n        self.logger.info(\"Saving checkpoint %s...\", target_path)\n        assert Path(target_path).exists()\n        return loss    ","c51d08e0":"class GAPMlpModel(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, span_dim: int, bert_hidden_size: int, device: torch.device):\n        super().__init__()\n        self.device = device\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(span_dim * 2 + bert_hidden_size),\n            nn.Dropout(0.5),\n            nn.Linear(span_dim * 2 + bert_hidden_size, 128),           \n            nn.ReLU(),\n            nn.BatchNorm1d(128),             \n            nn.Dropout(0.5),\n            nn.Linear(128, 128),           \n            nn.ReLU(),\n            nn.BatchNorm1d(128),             \n            nn.Dropout(0.5),                           \n#             nn.Linear(128, 128),           \n#             nn.ReLU(),\n#             nn.BatchNorm1d(128),             \n#             nn.Dropout(0.5),                        \n            nn.Linear(128, 3)\n        ).to(device)\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight)\n                print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n                \n    def forward(self, spans_contexts, emb_P):\n        return self.fc(torch.cat([\n            spans_contexts.reshape(spans_contexts.size(0), -1), emb_P\n        ], dim=1))","9df1c5f2":"test_ds = TensorDataset(spc_test, embp_test, ys_test)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","3f4764ef":"skf = StratifiedKFold(n_splits=5, random_state=234)\n\nval_preds, test_preds, val_ys, val_losses = [], [], [], []\nfor train_index, valid_index in skf.split(df_train, ys_train.numpy()):\n    print(\"=\" * 20)\n    print(f\"Fold {len(val_preds) + 1}\")\n    print(\"=\" * 20)\n    train_ds = TensorDataset(spc_train[train_index], embp_train[train_index], ys_train[train_index])\n    val_ds = TensorDataset(spc_train[valid_index], embp_train[valid_index], ys_train[valid_index])\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=32,\n        num_workers=0,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=128,\n        num_workers=0,\n        pin_memory=True,\n        shuffle=False\n    )\n    model = GAPMlpModel(spc_train.size(2), embp_train.size(1), torch.device(\"cuda:0\"))\n    optimizer = WeightDecayOptimizerWrapper(\n        torch.optim.Adam(model.parameters(), lr=1e-3),\n        0.05\n    )\n#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.005)\n    bot = GAPBot(\n        model, train_loader, val_loader,\n        optimizer=optimizer, echo=False,\n        avg_window=50\n    )\n    steps_per_epoch = len(train_loader) \n    n_steps = steps_per_epoch * 30\n    bot.train(\n        n_steps,\n        log_interval=steps_per_epoch \/\/ 1,\n        snapshot_interval=steps_per_epoch \/\/ 1,\n        scheduler=TriangularLR(\n            optimizer, 100, ratio=2, steps_per_cycle=n_steps)\n    )\n    # Load the best checkpoint\n    bot.load_model(bot.best_performers[0][1])\n    bot.remove_checkpoints(keep=0)    \n    val_preds.append(torch.softmax(bot.predict(val_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n    val_ys.append(df_train.iloc[valid_index].target.astype(\"uint8\").values)\n    val_losses.append(log_loss(val_ys[-1], val_preds[-1]))\n    bot.logger.info(\"Confirm val loss: %.4f\", val_losses[-1])\n    test_preds.append(torch.softmax(bot.predict(test_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())","b5f01b96":"val_losses","ba7b0c1a":"final_test_preds = np.mean(test_preds, axis=0)\n# final_test_preds = torch.stack(test_preds, dim=0).mean(dim=0).numpy()\nfinal_test_preds.shape","ffb0a1e4":"log_loss(df_test.target, final_test_preds)","f05146f9":"# Create submission file\ndf_sub = pd.DataFrame(final_test_preds, columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID.values\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","c2ad17d0":"Sort the data to make feature extraction slightly faster:","5da61c58":"## Train a Model based on the Extracted Features","d651b760":"\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead.","dbaaa98e":"Models in [my previous kernels](https:\/\/www.kaggle.com\/ceshine\/pytorch-bert-endpointspanextractor-kfold) incorporate the full BERT model stochastically, that is, in train mode (so things like dropouts are enabled). However, one big drawback of this approach is that BERT feed-forwards are very slow. This kernel adopts the approach used in other popular kernels (e.g. [Matei Ionita's work](https:\/\/www.kaggle.com\/mateiionita\/taming-the-bert-a-baseline)) that extract the features from a BERT model deterministically once and for all.\n\nOne problem of this new approach is that the model is not more prone to overfitting. So a new set of hyper-parameters must be used (I haven't found a proper one yet).","6349387d":"## Extract Features from BERT"}}