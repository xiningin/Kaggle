{"cell_type":{"29678b26":"code","c277d9b6":"code","3eee8b25":"code","47964b38":"code","8d0b2fa9":"code","9e504b3f":"code","87aefed6":"code","924e29ae":"code","06587e89":"code","3c5f7aa9":"code","085ff6c4":"code","a707c188":"code","16f3ed26":"code","e72d1e58":"code","671ac4a4":"code","c8bac97a":"code","b0d809b6":"code","5f418a72":"code","6eca1a8e":"code","34816fea":"code","971f15c1":"code","d8451138":"code","975a2232":"code","6e9cb013":"markdown","2108213e":"markdown","7e8040b9":"markdown","d9e9d0d8":"markdown","ca349251":"markdown","d026320f":"markdown","91e67aba":"markdown","bfec603b":"markdown","4a2e3d44":"markdown","d0cb8ad6":"markdown","5197936b":"markdown","5d51b39c":"markdown","30033366":"markdown"},"source":{"29678b26":"!pip install pycomp\n\nimport numpy as np \nimport pandas as pd\nimport cudf\nimport cupy\nimport time\nimport seaborn as sns\n\nfrom pycomp.viz.insights import *\nfrom cuml.preprocessing import LabelEncoder\nfrom cuml.preprocessing.model_selection import train_test_split\nfrom cuml.metrics import accuracy_score\nfrom cuml import PCA\nfrom cuml.manifold import UMAP, TSNE\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.ensemble import RandomForestClassifier as cuRFC\nfrom cuml.metrics import log_loss as logloss\nfrom tpot import TPOTClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_curve, log_loss\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c277d9b6":"def custom_palette(custom_colors):\n    customPalette = sns.set_palette(sns.color_palette(custom_colors))\n    sns.palplot(sns.color_palette(custom_colors),size=0.8)\n    plt.tick_params(axis='both', labelsize=0, length = 0)\n\nb = [\"#2a9d8f\",\"#e9c46a\",\"#f4a261\",\"#e76f51\"]\ncustom_palette(b)","3eee8b25":"train = cudf.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = cudf.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\")","47964b38":"train","8d0b2fa9":"test","9e504b3f":"train_p = train.to_pandas()\nlic = []\nfor col in train_p.columns[1:-1]:\n    lic.append(col)","87aefed6":"plot_donut_chart(df=train.to_pandas(), col='target',\n                 title='Target Value Distribution',colors=[b[1],b[2],b[3],b[0]])","924e29ae":"def plot(col):\n    plt.figure(figsize = (18, 8),dpi=80)\n    plt.rcParams[\"axes.linewidth\"] = 3\n    g = sns.countplot(x = col, hue = 'target', data = train_p)\n    plt.legend(loc='upper right')\n    plt.title(\"Distribution of \"+ col,fontsize=15)\n    plt.legend(title='Target',loc='upper right')\n    plt.show();\n\nfor col in lic:\n    plot(col)","06587e89":"le = LabelEncoder()\nencoded = le.fit_transform(train.target)\ntrain = train.assign(target=encoded)\ntrain.head()","3c5f7aa9":"plt.figure(figsize=(16,16),dpi=80)\ncorr=train.to_pandas().corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='PuRd', robust=True, center=0,\n            square=True, linewidths=.5)\nplt.title('Correlation', fontsize=15)\nplt.show()","085ff6c4":"def styling(cell):\n    if cell < 0 :\n        return 'background: #fde2e4; color:black'\n    else:\n        return 'background: #deaaff; color: white'\n\ntarget_df = pd.DataFrame(corr.target).iloc[:-1,:].T\ntarget_df.style.applymap(styling)","a707c188":"train_pp = train.to_pandas()\ndef plot_dr(technique,title):\n    start = time.time()\n    technique = technique(n_components=2)\n    result = technique.fit_transform(train_p[lic].values)\n    plt.figure(figsize = (16, 8))\n    plt.scatter(result[:,0], result[:,1], c = train_pp['target'].values, s = 0.7, cmap='cool')\n    plt.title(title,fontsize=18, fontweight='bold')\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()\n    print('Duration: {} seconds'.format(time.time() - start))","16f3ed26":"plot_dr(UMAP,\"UMAP\")","e72d1e58":"plot_dr(TSNE,\"TSNE\")","671ac4a4":"%%time\n\nX = train.drop([\"target\"],axis=1)\ny = train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, \n                                                    shuffle=False, stratify=y)","c8bac97a":"def training(model, X_train, y_train, X_val, y_val, model_name):\n    t1 = time.time()\n    \n    model.fit(X_train, y_train)\n    predicts = model.predict_proba(X_val)\n    logl = logloss(y_val, predicts)\n    \n    t2 = time.time()\n    training_time = t2-t1 \n    \n    print(\"\\t\\t\\t--- Model:\", model_name,\"---\")\n    print(\"Log loss: \", logl,\"\\t\\t\\t\",\"Training time:\",training_time,\"\\n\")","b0d809b6":"for col in X_train.columns:\n    X_train[col] = X_train[col].astype('float32')\n    \nfor col in X_val.columns:\n    X_val[col] = X_val[col].astype('float32')\n    \ny_train = y_train.astype('int32')\ny_val = y_val.astype('int32')","5f418a72":"lr = LogisticRegression(fit_intercept=True,penalty='l1')\n\nrf = cuRFC(n_estimators=500)\n\nm = [lr,rf]\nmn = [\"Logistic Regression\",\"Random Forest\"]\n\nfor i in range(0,len(m)):\n    training(model=m[i], X_train=X_train, y_train=y_train, X_val=X_val,y_val=y_val, model_name=mn[i])","6eca1a8e":"tpot = TPOTClassifier(\n   generations=5,\n   population_size=100,\n   scoring = 'roc_auc_ovr',\n   config_dict=\"TPOT cuML\",\n   cv=5,\n   verbosity=2\n)\n\n# for cuML with TPOT, we need to use CPU data\ntpot.fit(X_train.to_pandas(), y_train.to_pandas())\ntpot.export('tps-pipeline.py')","34816fea":"print('Accuracy :', tpot.score(X_val.to_pandas(), y_val.to_pandas()))\nfin_preds = tpot.predict_proba(test.to_pandas())","971f15c1":"%%time\n\nNUM_SPLITS = 5\nmodel = LGBMClassifier(**{'learning_rate': 0.05,\n                    'max_depth': 10,\n                    'num_leaves' : 63,\n                    'objective': 'multiclass',\n                    'metric': 'multi_logloss',\n                    'bagging_seed': 42,\n                    'boosting_type': 'gbdt',\n                    'is_unbalance': True})\n\nX_c = pd.concat([X_train.to_pandas(), X_val.to_pandas()])\ny_c = pd.concat([y_train.to_pandas(), y_val.to_pandas()])\n\ntest_p = test.to_pandas()\noof_pred = np.zeros((len(X_c), 4))\ntest_pred = np.zeros((len(test_p), 4))\n\nfolds = StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=2021)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_c,y_c)):\n    print('-- Fold:', fold_,'--' )\n    model = model.fit(X_c.iloc[trn_idx], y_c.iloc[trn_idx], eval_set=[(X_c.iloc[trn_idx],y_c.iloc[trn_idx]),(X_c.iloc[val_idx], y_c.iloc[val_idx])],\n                          eval_metric = 'multi_logloss',\n                          early_stopping_rounds = 100,verbose=250)\n         \n    temp_oof = model.predict_proba(X_c.iloc[val_idx])\n    oof_pred[val_idx] =  temp_oof\n    \n    print(f\"Log Loss: {log_loss(y_c.iloc[val_idx], temp_oof)}\")\n    \n    temp_test = model.predict_proba(test_p)\n    test_pred += test_pred\/NUM_SPLITS\n\nprint(f\"Overall Log Loss: {log_loss(y_c, oof_pred)}\")","d8451138":"%%time\n\nNUM_SPLITS = 10\nmodel2 = CatBoostClassifier()\n\noof_pred = np.zeros((len(X_c), 4))\ntest_pred = np.zeros((len(test_p), 4))\n\nfolds = StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=2021)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_c,y_c)):\n    print('-- Fold:', fold_,'--' )\n    model2 = model2.fit(X_c.iloc[trn_idx], y_c.iloc[trn_idx],eval_set = [(X_c.iloc[val_idx], y_c.iloc[val_idx])],\n                          early_stopping_rounds = 100,verbose=250)\n         \n    temp_oof = model.predict_proba(X_c.iloc[val_idx])\n    oof_pred[val_idx] =  temp_oof\n    \n    print(f\"Log Loss: {log_loss(y_c.iloc[val_idx], temp_oof)}\")\n    \n    temp_test = model.predict_proba(test_p)\n    test_pred += test_pred\/NUM_SPLITS\n    \nprint(f\"Overall Log Loss: {log_loss(y_c, oof_pred)}\")","975a2232":"predictions = cudf.DataFrame(fin_preds)\npredictions.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\npredictions['id'] = test['id']\npredictions = predictions[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4']]\n\npredictions.to_csv(\"\/kaggle\/working\/Predictions_teapot.csv\", index=False)\npredictions","6e9cb013":"Although the data used for this competition is synthetic, it is based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. \n> \ud83c\udfaf Goal: To predict the probability the id belongs to each class\n\n> \ud83d\udcd6 Data:\n> - ```train.csv``` - *training data*, one product (id) per row, with the associated features (feature_*) and class label (target)\n> - ```test.csv``` - *test data*","2108213e":"<center><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRAWAwYLg9InZxOdcxr7mEKMyVsNM8MHNXB0GDDKC5tqR0I71h9LbvXQLMCoSQk82vh3Zw&usqp=CAU\"><\/center>","7e8040b9":"# EDA \ud83d\udcca","d9e9d0d8":"# Label Encoding \ud83c\udff7\ufe0f","ca349251":"# Dimensionality Reduction \ud83d\udcad","d026320f":"<img src=\"https:\/\/i.imgur.com\/bGpKLYh.png\">","91e67aba":"# Submission file \ud83d\udcdd","bfec603b":"# Model training \u2699\ufe0f","4a2e3d44":"<center><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRrRyySk4pDN6tju38z-r8oVA6oha9WSJBl0gVxTNALk3gz8TXZaNjQQfPSjSisodD-upo&usqp=CAU\" ><\/center>","d0cb8ad6":"<center><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSjifalk1omESSaUXBBKVI16qaoPQYPxya-Sd5Gm__po7WPeP8R3aDBZD-hnYZbWYeSdg&usqp=CAU\"><\/center>","5197936b":"# Import libraries \ud83d\udcda","5d51b39c":"Work in progress \ud83d\udea7","30033366":"<center><img src=\"https:\/\/raw.githubusercontent.com\/EpistasisLab\/tpot\/master\/images\/tpot-logo.jpg\" width=\"250\"><\/center>"}}