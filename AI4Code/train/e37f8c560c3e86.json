{"cell_type":{"bbad4cb9":"code","a4e4ff68":"code","91d98547":"code","b03495bf":"code","8ab25495":"code","8c484bda":"code","1b0de4c4":"code","2744b9ef":"code","eea5d8d4":"markdown","110fd9cb":"markdown","b01d662c":"markdown","199f568a":"markdown","eee2d1b0":"markdown"},"source":{"bbad4cb9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nRaces = pd.read_csv('\/kaggle\/input\/skyrim-races-base-stats\/skyrimstats.csv')\nRaces.head(5)","a4e4ff68":"def JitterPoint(vector):\n    pointlist = [x + np.random.uniform(-0.5,0.5) for x in vector]\n    return(pointlist)\nprint(\"\"\"###################################\nThis is an example plot of two skills\n###################################\n\"\"\")\nfig,ax = plt.subplots()\nax.scatter(JitterPoint(Races['Smithing']),\n           JitterPoint(Races['Heavy Armor']),\n          alpha = 0.75,edgecolor = 'black')\nplt.xlabel('Smithing')\nplt.ylabel('Heavy Armor')\nplt.title('Sample stat distribution')\nplt.locator_params(axis=\"both\", integer=True)\nplt.ylim(10,30)\nplt.xlim(10,30)\nplt.show()","91d98547":"#K-means modeling, search for right k\n\nSearchGrid = []\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(Races.iloc[:,2:])\nfig,ax = plt.subplots()\nax.scatter(X[0],X[1],color = 'blue')\nplt.title('Example of Standardized Variables')\nplt.xlabel('Smithing')\nplt.ylabel('Heavy Armor')\nplt.show()","b03495bf":"for k in range(1,10):\n    km_search = KMeans(n_clusters=k,random_state=42)\n    km_search.fit(X)\n    SearchGrid.append([k,km_search.inertia_])\nSearchGrid = pd.DataFrame(SearchGrid)\nSearchGrid.columns = ['K','WSS_SUM']\nplt.figure()\nplt.plot(SearchGrid['K'],SearchGrid['WSS_SUM'],'r*')\nplt.xlabel('K Value')\nplt.ylabel('Within Sum of Squares')\nplt.title('K-Means model iterate through K values and plot WSS')\nplt.show()","8ab25495":"km = KMeans(n_clusters = 3, random_state = 42)\nkm.fit(X)\nGrouping_Labels = km.labels_\nprint(f'Labels alone \\n{Grouping_Labels}')\nRaces['Grouping'] = Grouping_Labels\nprint(Races.groupby('Grouping')['Race'].value_counts().reset_index(name = 'Flag').drop(columns = ['Flag']))","8c484bda":"from scipy.cluster.hierarchy import linkage, dendrogram\n\nmergings = linkage(X,method = 'complete') \nmergings","1b0de4c4":"dendrogram(mergings,\n          labels = list(Races['Race']),\n          leaf_rotation = 90,\n          leaf_font_size = 7)\nplt.title('Dendrogram of Skyrim races by Starting Attributes')\nplt.show()","2744b9ef":"from scipy.cluster.hierarchy import fcluster\nlabels_hc = fcluster(mergings,4,criterion = 'distance')\nprint(labels_hc)\npairs = pd.DataFrame({'labels': labels_hc,'Race': Races['Race']})\nprint(pairs.sort_values('labels'))\nprint('')\nlables_hc2 = fcluster(mergings,5,criterion = 'distance')\nprint(lables_hc2)\npairs = pd.DataFrame({'labels': lables_hc2,'Race': Races['Race']})\nprint(pairs.sort_values('labels'))\nprint('')\nlables_hc3 = fcluster(mergings,6,criterion = 'distance')\nprint(lables_hc3)\npairs = pd.DataFrame({'labels': lables_hc3,'Race': Races['Race']})\nprint(pairs.sort_values('labels'))","eea5d8d4":"We can also look at the grouping using a method call dendrograms, which employs hierarchical clustering.\nThis method allows us to decide how many groups there are by looking at the distance between the clusters.  \nEssentially, this shows how simialar each of the groups is to one another based on the data.\nNote that there are differnet ways to determine the distance between groups, but the 'complete' method is chosen here for simplisity.","110fd9cb":"Once we have a value of K to use, we can then run the K-means function on the data to get a vector of labels.\nWe do not know what these labels actually mean at this time but we will use the data, along with domain knowledge, to determine thier significance. ","b01d662c":"First, we need to standardize the data.  \nThis is so a large variance in one variable does not overpower a smaller variance in another variable.\nAs an example, imagine variable A has a range of 0 to 5 while Variable B has a range of -100 to 100\nBy standardizing, we convert both variables so they are centered around the mean of the transformed variable... 0 with a standard deviation of 1 unit.  \nYou could also normalize the data which would convert the variables to all be with a 0 to 1 scale.  ","199f568a":"We can specify the cut off to say what groupings to use.  \nThe output below show the differnt groups based on the differnt cutoffs.\nThe \"label\" variable shows the group that each member belongs to.","eee2d1b0":"The plot below shows how the Within Sum of Squares changes based on the number of centroids chosen (K values).  \nThe main goal of this plot, when not knowing the K-Value apriori, is to find the \"correct\" value of K that best describes the data.\nEssentially, you're looking for the maximum inflection point of the curve formed by the data.  \nIn this case, the value falls at k = 3."}}