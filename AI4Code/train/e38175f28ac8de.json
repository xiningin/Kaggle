{"cell_type":{"a1b5f6e6":"code","18243cf3":"code","f43d2d3f":"code","6fc80bd7":"code","8e507ba5":"code","ca6d9929":"code","1f8fe88d":"code","0eeb37be":"code","210f45fc":"code","913acabd":"code","b90cc080":"code","ae2d580b":"code","8fc1d2b0":"code","30cc3776":"code","8d9a39e2":"code","a4d27222":"code","a98c54b9":"code","32545002":"code","d23971f4":"code","16b934f1":"code","5a363174":"code","b336d3a6":"code","c76d997f":"code","5b8ad6c3":"code","b866bd90":"code","3e75739b":"code","029afb0d":"code","e02a4753":"code","d0513925":"code","52130b16":"code","8d0fbafa":"code","a0aabad6":"code","f4c26b73":"code","602c2b3f":"code","d605cffc":"code","463b0e8d":"code","90236ef2":"code","00064c98":"code","56b1b05c":"code","9fd2c673":"code","d741ae7c":"code","cf3ed732":"code","f750ec57":"code","9345689f":"code","524de737":"code","406eea48":"code","d2c87630":"code","3cc0b62b":"code","b708e363":"code","5f2a201e":"code","401a83e9":"code","42eab8c5":"code","9e6d4310":"code","1e0883ed":"code","c5e5e896":"code","0bda598c":"code","4a676dc4":"code","f796168e":"code","3bc29bfe":"code","7c5ab059":"code","17da03f8":"code","095aad54":"code","a3aa8866":"code","63d98125":"code","2695ae7f":"code","b79d74b3":"code","e81b6149":"code","37983a84":"code","16a48fb5":"code","97ec2651":"code","ff800224":"code","07616e47":"code","deb0631c":"code","fa1c1f7c":"code","66ec994f":"code","d2a682b6":"code","1c8ad655":"code","4ca71b26":"code","19f3f461":"code","22276cc9":"code","7bcd18a1":"code","7c8348ad":"code","e60f840c":"code","4f256935":"code","f4004bfd":"code","661f63ec":"code","b5d949f9":"markdown","91864e8a":"markdown","92b3637c":"markdown","8e7cd3c3":"markdown","9f8e52e2":"markdown","36e1165b":"markdown","4b3af097":"markdown","948761a4":"markdown","851f16ed":"markdown","bd09a8e1":"markdown","dd198405":"markdown","4c6860d0":"markdown","0882b870":"markdown","c4456149":"markdown","ea3316e3":"markdown","c002642c":"markdown","0e8af116":"markdown","2a9f908b":"markdown","c0e92bad":"markdown","c4fe98b0":"markdown","79cf8f09":"markdown","5aebb6b5":"markdown","bdc6643a":"markdown","73ce3958":"markdown","7742ec16":"markdown","d9e820c0":"markdown","bebe9ae5":"markdown","b9e1569b":"markdown","719b44be":"markdown","7d27f8e2":"markdown","fe03a4d6":"markdown","6b1ebca6":"markdown","f78841b3":"markdown","be368f74":"markdown","23d1e07b":"markdown","18345974":"markdown","997c35d2":"markdown","c7e3f320":"markdown","84ee6988":"markdown"},"source":{"a1b5f6e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18243cf3":"# Load libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom scipy.stats import norm\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')","f43d2d3f":"# lets load data\ntrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","6fc80bd7":"# create copy of above dataframe. \ntrain = train_df.copy()\ntest = test_df.copy()","8e507ba5":"# lets see shape of datas\nprint('train data shape: ', train.shape)\nprint('test data shape: ', test.shape)","ca6d9929":"# lets view first five records in train data\ntrain.head()","1f8fe88d":"# lets view first 5 observations in test data\ntest.head()","0eeb37be":"# you can see null values even in firstt 5 observations as seen above\n# lets find the null values in data\n\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","210f45fc":"# Id attributes have no special meaning in in regression so lets drop them\ntrain.drop('Id', axis=1, inplace = True)\ntest.drop('Id', axis=1, inplace=True)","913acabd":"## plotting distribution of target feature\nsns.distplot(train['SalePrice'])\nplt.show()","b90cc080":"# Numerical features\nNumerical_feat = [feature for feature in train.columns if train[feature].dtypes != 'O']\nprint('Total numerical features: ', len(Numerical_feat))\nprint('\\nNumerical Features: ', Numerical_feat)","ae2d580b":"# making a glance of first 5 observations\ntrain[Numerical_feat].head()","8fc1d2b0":"# Zoomed heatmap, correlation matrix\nsns.set(rc={'figure.figsize':(12,8)})\ncorrelation_matrix = train.corr()\n\nk = 10             #number of variables for heatmap\ncols = correlation_matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","30cc3776":"## these are selected features from heatmap\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n","8d9a39e2":"# Discrete features\n\ndiscrete_feat = [feature for feature in Numerical_feat if len(train[feature].unique())<25]\nprint('Total discrete features: ', len(discrete_feat))\nprint('\\n', discrete_feat)","a4d27222":"# glancing first five records of discrete features\ntrain[discrete_feat].head()","a98c54b9":"train[discrete_feat].info()","32545002":"# Lets find unique values in each discrete features\nfor feature in discrete_feat:\n    print('Uique values of ', feature, ':')\n    print(train[feature].unique())\n    print('\\n')\n    ","d23971f4":"## Lets Find the realtionship between discrete features and SalePrice\n\n#plt.figure(figsize=(8,6))\n\nfor feature in discrete_feat:\n    data=train.copy()\n    plt.figure(figsize=(8,6))\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","16b934f1":"continuous_features = [feature for feature in Numerical_feat if feature not in discrete_feat]\nprint('The numbers of continuous features: ', len(continuous_features))\nprint('\\n', continuous_features)","5a363174":"## Lets analyse the continuous values by creating histograms to understand the distribution\n\ntrain[continuous_features].hist(bins=25)\nplt.show()","b336d3a6":"## let us now examine the relationship between continuous features and SalePrice\n## Before that lets find continous features that donot contain zero values\n\ncontinuous_nozero = [feature for feature in continuous_features if 0 not in data[feature].unique() and feature not in ['YearBuilt', 'YearRemodAdd']]\n\nfor feature in continuous_nozero:\n    plt.figure(figsize=(8,6))\n    data = train.copy()\n    data[feature] = np.log(data[feature])\n    data['SalePrice'] = np.log(data['SalePrice'])\n    plt.scatter(data[feature], data['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","c76d997f":"## Normality and distribution checking for continous features\nfor feature in continuous_nozero:\n    plt.figure(figsize=(6,6))\n    data = train.copy()\n    sns.distplot(data[feature])\n    plt.show()","5b8ad6c3":"# categorical features\ncategorical_feat = [feature for feature in train.columns if train[feature].dtypes=='O']\nprint('Total categorical features: ', len(categorical_feat))\nprint('\\n',categorical_feat)","b866bd90":"# lets view few samples \ntrain[categorical_feat].head()","3e75739b":"# lets find unique values in each categorical features\nfor feature in categorical_feat:\n    print('{} has {} categories. They are:'.format(feature,len(train[feature].unique())))\n    print(train[feature].unique())\n    print('\\n')\n","029afb0d":"# let us find relationship of categorical with target variable\n\nfor feature in categorical_feat:\n    data=train_df.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","e02a4753":"Train = train_df.shape[0]\nTest = test_df.shape[0]\ntarget_feature = train_df.SalePrice.values\ncombined_data = pd.concat((train_df, test_df)).reset_index(drop=True)\ncombined_data.drop(['SalePrice','Id'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(combined_data.shape))","d0513925":"# let's find the missing data in combined dataset\n\ntotal = combined_data.isna().sum().sort_values(ascending=False)\npercent = (combined_data.isnull().sum()\/combined_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","52130b16":"# Lets first handle numerical features will nan value\nnumerical_nan = [feature for feature in combined_data.columns if combined_data[feature].isna().sum()>1 and combined_data[feature].dtypes!='O']\nnumerical_nan","8d0fbafa":"combined_data[numerical_nan].isna().sum()","a0aabad6":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=combined_data[feature].median()\n    \n    combined_data[feature].fillna(median_value,inplace=True)\n    \ncombined_data[numerical_nan].isnull().sum()","f4c26b73":"# categorical features with missing values\ncategorical_nan = [feature for feature in combined_data.columns if combined_data[feature].isna().sum()>1 and combined_data[feature].dtypes=='O']\nprint(categorical_nan)","602c2b3f":"combined_data[categorical_nan].isna().sum()","d605cffc":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    combined_data[feature] = combined_data[feature].fillna('None')","463b0e8d":"combined_data[categorical_nan].isna().sum()","90236ef2":"# these are selected features from EDA section\nfeatures = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']","00064c98":"# plot bivariate distribution (above given features with saleprice(target feature))\nfor feature in features:\n    if feature!='SalePrice':\n        plt.scatter(train_df[feature], train_df['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","56b1b05c":"#Deleting outliers for GrLivArea\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\n\nplt.scatter(train_df['GrLivArea'], train_df['SalePrice'])\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.show()","9fd2c673":"#Deleting outliers for TotalBsmtSF\ntrain_df = train_df.drop(train_df[(train_df['TotalBsmtSF']>5000) & (train_df['SalePrice']<300000)].index)\n\nplt.scatter(train_df['TotalBsmtSF'],train_df['SalePrice'])\nplt.xlabel('TotalBsmtSF')\nplt.ylabel('SalePrice')\nplt.show()","d741ae7c":"# these are selected features from EDA section\nfeatures = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n\n# selecting continuous features from above\ncontinuous_features = ['SalePrice', 'GrLivArea', 'TotalBsmtSF']","cf3ed732":"# checking distribution of continuous features(histogram plot)\nfor feature in continuous_features:\n    if feature!='SalePrice':\n        sns.distplot(combined_data[feature], fit=norm)\n        plt.show()\n    else:\n        sns.distplot(train_df['SalePrice'], fit=norm)\n        plt.show()","f750ec57":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\n# This idea is from Pedro Marcelino, PhD notebook.\n\ncombined_data['HasBsmt'] = 0  # at first o for all observations in 'HasBsmt'\ncombined_data.loc[combined_data['TotalBsmtSF']>0,'HasBsmt'] = 1  # assign 1 for those with no basement ","9345689f":"#transform data\ncombined_data.loc[combined_data['HasBsmt']==1,'TotalBsmtSF'] = np.log(combined_data['TotalBsmtSF'])\ncombined_data['GrLivArea'] = np.log(combined_data['GrLivArea'])\ntrain_df['SalePrice'] = np.log(train_df['SalePrice'])","524de737":"# we have log transormed above skewed data. Now lets see their distribution\nfor feature in continuous_features:\n    if feature!='SalePrice':\n        sns.distplot(combined_data[feature], fit=norm)\n        plt.show()\n    else:\n        sns.distplot(train_df['SalePrice'],fit=norm)\n        plt.show()","406eea48":"## these are features that seems to give information in order form\n## taken from Serigne's notebook\n\nordinal_features = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n                 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n                 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n                 'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n                 'YrSold', 'MoSold']\nprint(len(ordinal_features))","d2c87630":"## Credit for Serigne \n\n#MSSubClass=The building class\ncombined_data['MSSubClass'] = combined_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ncombined_data['OverallCond'] = combined_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\n#combined_data['YrSold'] = combined_data['YrSold'].astype(str)\n#combined_data['MoSold'] = combined_data['MoSold'].astype(str)\n","3cc0b62b":"combined_data[ordinal_features].head(10)","b708e363":"# so let's label encode above ordinal features\nfrom sklearn.preprocessing import LabelEncoder\nfor feature in ordinal_features:\n    encoder = LabelEncoder()\n    combined_data[feature] = encoder.fit_transform(list(combined_data[feature].values))","5f2a201e":"# Now lets see label encoded data\ncombined_data[ordinal_features].head()","401a83e9":"## One hot encoding or getting dummies \n\ndummy_ordinals = pd.get_dummies(ordinal_features) \ndummy_ordinals.head()","42eab8c5":"# creating dummy variables\n\ncombined_data = pd.get_dummies(combined_data)\nprint(combined_data.shape)","9e6d4310":"combined_data.head(10)","1e0883ed":"# let's first see descriptive stat info \ncombined_data.describe()","c5e5e896":"## we willtake all features from combined_dummy_data \nfeatures_to_scale = [feature for feature in combined_data]\nprint(len(features_to_scale))","0bda598c":"## Now here is where we will scale our data using sklearn module.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ncols = combined_data.columns  # columns of combined_dummy_data\n\nscaler = MinMaxScaler()\ncombined_data = scaler.fit_transform(combined_data[features_to_scale])","4a676dc4":"# after scaling combined_data it is now in ndarray datypes\n# so we will create DataFrame from it\ncombined_scaled_data = pd.DataFrame(combined_data, columns=[cols])","f796168e":"combined_scaled_data.head() # this is the same combined_dummy_data in scaled form.","3bc29bfe":"# lets see descriptive stat info \ncombined_scaled_data.describe()","7c5ab059":"train_df.shape, test_df.shape, combined_scaled_data.shape, combined_data.shape","17da03f8":"# separate train data and test data \ntrain_data = combined_scaled_data.iloc[:1460,:]\ntest_data = combined_scaled_data.iloc[1460:,:]\n\ntrain_data.shape, test_data.shape","095aad54":"## lets add target feature to train_data\ntrain_data['SalePrice']= train_df['SalePrice']  # This saleprice is normalized. Its very impportant","a3aa8866":"train_data = train_data\ntrain_data.head(10)","63d98125":"test_data = test_data.reset_index()\ntest_data.tail(10)","2695ae7f":"## ugh.. it seems outliers that we droped earlier haven't droped from combined data.\n## that makes sense since we had droped only from train data before not from combined data.\n## S0 we will drop them here\n\n#Deleting outliers for TotalBsmtSF\n#train_data = train_data.drop(train_data[(train_data['TotalBsmtSF']>5000) & (train_data['SalePrice']<300000)].index)\n\n#Deleting outliers for GrLivArea\n#train_data = train_data.drop(train_data[(train_data['GrLivArea']>4000) & (train_data['SalePrice']<300000)].index)\n","b79d74b3":"dataset = train_data.copy()  # copy train_data to dataset variable","e81b6149":"dataset.head()","37983a84":"dataset = dataset.dropna()","16a48fb5":"## lets create dependent and target feature vectors\n\nX = dataset.drop(['SalePrice'],axis=1)\nY = dataset[['SalePrice']]\n\nX.shape, Y.shape","97ec2651":"Y.head()","ff800224":"# lets do feature selection here\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\n# define feature selection\nfs = SelectKBest(score_func=f_regression, k=27)\n# apply feature selection\nX_selected = fs.fit_transform(X, Y)\nprint(X_selected.shape)\n","07616e47":"cols = list(range(1,28))\n\n## create dataframe of selected features\n\nselected_feat = pd.DataFrame(data=X_selected,columns=[cols])\nselected_feat.head()","deb0631c":"# perform train_test_split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(selected_feat,Y,test_size=0.3,random_state=0)","fa1c1f7c":"x_train.shape, x_test.shape","66ec994f":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nlr = LinearRegression()\nlr.fit(x_train,y_train)","d2a682b6":"y_pred = lr.predict(x_test) # predicting test data\ny_pred[:10]","1c8ad655":"# Evaluating the model\nprint('R squared score',metrics.r2_score(y_test,y_pred))\n\nprint('\\nMean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","4ca71b26":"# check for underfitting and overfitting\nprint('Train Score: ', lr.score(x_train,y_train))\nprint('Test Score: ', lr.score(x_test,y_test))","19f3f461":"## scatter plot of original and predicted target test data\nplt.figure(figsize=(8,6))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_tes')\nplt.ylabel('y_pred')\nplt.show()","22276cc9":"## Lets do error plot\n## to get error in prediction just substract predicted values from original values\n\nerror = list(y_test.values-y_pred)\nplt.figure(figsize=(8,6))\nsns.distplot(error)","7bcd18a1":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_estimators=100)\nrf_reg.fit(x_train,y_train)","7c8348ad":"y_pred = rf_reg.predict(x_test)\nprint(y_pred[:10])","e60f840c":"## evaluating the model\n\nprint('R squared error',metrics.r2_score(y_test,y_pred))\n\nprint('\\nMean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","4f256935":"# check score\nprint('Train Score: ', rf_reg.score(x_train,y_train))\nprint('Test Score: ', rf_reg.score(x_test,y_test))","f4004bfd":"## scatter plot of original and predicted target test data\nplt.figure(figsize=(8,6))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_tes')\nplt.ylabel('y_pred')\nplt.show()","661f63ec":"## Lets do error plot\n## to get error in prediction just substract predicted values from original values\n\nerror = list(y_test.values-y_pred)\nplt.figure(figsize=(8,6))\nsns.distplot(error)","b5d949f9":"If we look at the above table we can see some of the features are numeric\/discrete but in realty they can be consdered categorical, its jsut that they are avalable in encoded form in this dataset.\nso when we label encode above ordinal_features, those features that are numeric\/discrete remans unchanged as they are already in encoded form. The ","91864e8a":"Such a nice error plot. We can see the errors are normally distributed.","92b3637c":"Target feature is not normally distributed and shows positive skewness. We need to do some transformation like log normal which se wll do in feature engineering section","8e7cd3c3":"## 2. Feature Engineering","9f8e52e2":"### 2.1.1 Numerical features(handling missing data)\nWe shall deal with missing datas in numerical features here.","36e1165b":"Above train score and test score  comparable which is good. Even though it shows a slight case of underfitting but thats fine here.","4b3af097":"### 3.2.2 RandomForestRegressor","948761a4":"**Let us first check the distribution of target feature**","851f16ed":"### 1.2 Categorical Features","bd09a8e1":"### 2.4 Label encoding, One-Hot-Encoding\/dummies\nWe will label encode some features and perform one hot encoding on categorical features that are not ordinal(doesnot show any information in order form).\nBut yes, there are some features given as numerical\/discrete numerical but actually looks like categorical which gives information in order form for example 'Overallcond' feature rates the overall condition of the house in range 1 to 10.","dd198405":"Initially, train data had **1460** observations but we had droped 2 oo 3 in outlier handling section so now we have **4581** observations.","4c6860d0":"Alright, we will now combine train set and test set. since we are doing feature engineering here, combining them together and performing feature engineering will save time from having to repeat the same process for test data.","0882b870":"### 2.3 Normalizing some numerical data\nWe know some numerical data shows skewness, we will normalize\/transform them to normal distribution by using log normal transformation","c4456149":"I haven't included which features are selected. just know that 30 features are selected","ea3316e3":"### 2.1 Missing data and handling them.","c002642c":"### 2.5 Feature Scaling\nMany machine learning algorithms especially in Regression, models seems to have pooer performance on unscaled data. So what we will do here is scale the data in same range between 0 and 1. Even if we scale the distance between points still remains unchanged.","0e8af116":"We can see that 30 best\/important features have been selected. ","2a9f908b":"### 2.2 Outliers\nOutliers are the data points that just deviates from other normal data points. Outliers can have a great effect in performance of ML models. So we have to be very careful handling them. Removing outliers always may not be good choice, we should see the nature of data and other aspects when handling them.","c0e92bad":"They shows quite a skewness.","c4fe98b0":" We have train set accuracy of **0.9792356047371404** and test set accuracy of **0.8848913083086402**. Here we can see overfiiting issue but for now we will leave it alone. They are still preety good score.","79cf8f09":"# Detailed EDA + Feature Engineering + Feature Selection + Model Building\n\n**If you find this notebook useful or even liked than you can greatly upvate to motivate me further....**","5aebb6b5":"**Valueable suggestions and Feedbacks are always welcomed, if you find this notebook useful or liked than you can upvote this work. Thank you everyone for visiting this notebook.**","bdc6643a":"From above heatmap we can select those dependent features which have high correlations with target feature but low correlation among dependent features. these selected features given below in next cell.","73ce3958":"\n\nI have always believed that as long as you do good feature engineering on your data you dont need fancy algorithms to improve your model performance. This is exactly what i have done in this notebook. I have used simple algirithms yet have got quite high accuracy score.\n\n\n\n## Any data science projecct involves following steps\n1. Data Analysis\n1. Feature Engineering\n1. Feature Selection\n1. Model Building\n1. Model Deployment\n\nWe have followed exactly these steps excluding last step;.\n\n### 1. Data Analysis\nwe have performed given analyis in in this section. \n1. Missing Values\n1. All The Numerical Variables\n1. Distribution of the Numerical Variables\n1. Categorical Variables\n1. Cardinality of Categorical Variables\n1. Outliers\n1. Relationship between independent and dependent feature(SalePrice)\n\n### 2. Feature Engineering\nWe have done following in this section:\n\n1. Missing values handling\n2. Outlier handling\n3. Normalization\n4. Label Encoding\/One Hot Encoding\n4. Feature Scaling etc..\n\n### 3. Feature Selection\nFeature selction means selecting those features that have improved performance on our model. I have used some Machine Learning and Statistical methods to select most promising features for improved model performance.\n\n### 4. Model Building\nThis is the last section in our notebook .i have used simple algorithms   Linear  Regression, RandomForestRegressor etc to build model. You can't believe with good feature engineering i was able to build pretty good model.\n","7742ec16":"## 1. Detailed EDA","d9e820c0":"### 3.2.1 Linear Regression","bebe9ae5":"### 1.1 Numerical Features","b9e1569b":"### 2.1.2 Categorical features(handling missing data)\nWE shall handle missing datas in this section for categorical features.","719b44be":"**1.1.1 Discrete Features**","7d27f8e2":"We can see a clear otliers in **GrLivAreea** and **TotalBsmtSF**. I mean it just doesn't make sense for larger values **GrLivAreea** and **TotalBsmtSF** to have low value of SalePrice. There might be some reason for this but we wll consider them outlier here and drop them.","fe03a4d6":"### 3 Feature Selection and Model Building\nHere we will first select those feature that are most omportant for our models and we shall build models using them","6b1ebca6":"We can see from above two dataframe tables that datas are now scaled.","f78841b3":"### 3.1 Feature selection","be368f74":"**1.1.2 Continuous Features**","23d1e07b":"### 3.2 Model Building\nWe will build machine learning models using above selected features","18345974":"Above plots shows skewness, peakness and no normality. We need to dosome transformations like log noraml which we will do in Feature Engineering section.","997c35d2":" R square score is preety high almost 90% score which is preety good.\n MAE, MSE and RMSE values also shows pretty good result.","c7e3f320":"we can see above data range differs so much. so we need to scale them to same range.","84ee6988":"We can see we have attained a bit of normality here. There are also other few continuous features that might have skewness but above features have more effect on target features so we have only considered them here."}}