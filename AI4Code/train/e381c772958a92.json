{"cell_type":{"84034a25":"code","f2de26e3":"code","4b924abe":"code","b6100a0a":"code","f0d9e35b":"code","d2c1a23f":"code","7c220970":"code","f9b256ef":"code","41414d9a":"code","9d0b5272":"code","e6aed32e":"code","8a45dd35":"code","05e3ce1f":"code","9cbd3343":"code","834ca6b9":"code","36974d58":"code","15b5d778":"code","284aa38b":"code","a916056f":"code","0474b485":"code","a903763b":"code","b15c279f":"code","f5826616":"code","f785a60c":"code","cbec3361":"code","eaf72029":"code","7b5cb48e":"code","3eaccdae":"code","3b7dd294":"code","09935c7a":"code","8613386d":"code","37defe51":"code","a67ac94e":"code","8af922fa":"code","61a30ce4":"markdown","2eee1fd9":"markdown","5f17bd45":"markdown","b8da038a":"markdown","41aef3d7":"markdown","88039b8e":"markdown","be9f16a1":"markdown","1f2af100":"markdown","1bcc6f62":"markdown","07de5788":"markdown","d4fff021":"markdown","7b51db61":"markdown","172b9028":"markdown"},"source":{"84034a25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f2de26e3":"from pathlib import Path\nDATA =  Path(\"\/kaggle\/input\/zhwikisource-title-draft\/cntext_rule_sep.csv\")","4b924abe":"df = pd.read_csv(DATA)","b6100a0a":"df.sample(10)","f0d9e35b":"df = df.sample(frac = 1.).reset_index().drop(\"index\",axis=1)","d2c1a23f":"df[~df.isy].sample(10)","7c220970":"from transformers import BertTokenizer,BertModel","f9b256ef":"tok = BertTokenizer.from_pretrained(\"bert-base-chinese\")","41414d9a":"tok.encode_plus(\"\u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\u5173\u4e8e\u5904\u7406\u81ea\u9996\u548c\u7acb\u529f\u5177\u4f53\u5e94\u7528\u6cd5\u5f8b\u82e5\u5e72\u95ee\u9898\u7684\u89e3\u91ca\"),\ntok.encode_plus(\"\u6b3d\u5b9a\u516b\u65d7\u901a\u5fd7 (\u56db\u5eab\u5168\u66f8\u672c)\/\u5377021\")","9d0b5272":"bert = BertModel.from_pretrained(\"bert-base-chinese\")","e6aed32e":"import torch","8a45dd35":"test_toks = tok.encode_plus(\"\u6b3d\u5b9a\u516b\u65d7\u901a\u5fd7 (\u56db\u5eab\u5168\u66f8\u672c)\/\u5377021\")[\"input_ids\"]\ntest_x = torch.LongTensor(test_toks)[None,:]","05e3ce1f":"with torch.no_grad():\n    y_1,y_2 = bert(test_x)","9cbd3343":"y_1.shape","834ca6b9":"y_2.shape","36974d58":"CUDA = torch.cuda.is_available()","15b5d778":"if CUDA:\n    bert.cuda()","284aa38b":"from torch import nn\nclass newTop(nn.Module):\n    def __init__(self,in_ = 768,hs = 768):\n        super().__init__()\n        self.ff = nn.Sequential(*[\n            nn.Linear(in_,hs,bias = False),\n            nn.BatchNorm1d(hs),\n            nn.ReLU(),\n            nn.Linear(hs,1)\n        ])\n        \n    def forward(self,x):\n        return self.ff(x)","a916056f":"from torch.utils.data.dataset import Dataset\n\nclass TextDS(Dataset):\n    def __init__(self,df):\n        self.df = df\n        self.pre = list(df.preview)\n        self.y = list(df.isy)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        txt = self.pre[idx]\n        ids = tok.encode(txt,max_length=100,pad_to_max_length=True)\n        return np.array(ids),np.array([1 if self.y[idx] else 0,])","0474b485":"!pip install forgebox","a903763b":"from forgebox.ftorch.train import Trainer\nfrom forgebox.ftorch.callbacks import stat","b15c279f":"t = Trainer(TextDS(df),val_dataset=TextDS(df),shuffle=False,\n        batch_size = 32, print_on = 5, \n        callbacks=[stat],val_callbacks=[stat],using_gpu = CUDA)","f5826616":"model = newTop()\nif CUDA:\n    model.cuda()","f785a60c":"t.opt[\"adm_top\"] = torch.optim.Adam(model.parameters())","cbec3361":"# The loos function that will reduce down the loss to a single scalar\/ mini-batch\nlossf = nn.BCEWithLogitsLoss()\n# The loss function that will keep the dimension on mini-batch\nloss_e = nn.BCEWithLogitsLoss(reduce = False)","eaf72029":"from forgebox.ftorch.metrics import metric4_bi","7b5cb48e":"class Looper:\n    def __init__(self):\n        self.record = []\n        \n    def __call__(self,*args):\n        for arg in args:\n            self.record.append(arg)","3eaccdae":"@t.step_train\ndef action(batch):\n    if batch.i == 0: batch.l = Looper()\n    batch.opt.zero_all()\n    x,y = batch.data\n    x = x.long()\n    y = y.float()\n\n    with torch.no_grad():\n        _,vec = bert(x)\n    y_ = model(vec)\n    loss = lossf(y_,y)\n    \n    loss.backward()\n    batch.opt.step_all()\n    acc,rec,prec,f1 = metric4_bi(y_,y)\n    return {\"loss\":loss.item(),\n            \"f1\":f1.item(),\n            \"acc\":acc.item(),\n            \"rec\":rec.item(),\n            \"prec\":prec.item()}\n\n@t.step_val\ndef val_action(batch):\n    if batch.i == 0: \n        batch.l = Looper()\n        bert.eval()\n        model.eval()\n    with torch.no_grad():\n        x,y = batch.data\n        x = x.long()\n        y = y.float()\n\n        _,vec = bert(x)\n        y_ = model(vec.detach())\n        loss = loss_e(y_,y)\n    \n    batch.l(*loss.cpu().numpy())\n    return {\"loss\":loss.mean().item(),}","3b7dd294":"t.train(1)","09935c7a":"torch.save(model.state_dict(),\"top_layer.pth\")","8613386d":"t.epoch = 0\nt.val_track[0] = list()\nt.val_gen = iter(t.val_data)\n\nval_t = t.progress(t.val_len)\nfor i in val_t:\n    t.val_iteration(i, val_t)\n\nfor v_cb_func in t.val_callbacks:\n    v_cb_func(record=t.val_track[0] )","37defe51":"df[\"loss\"] = t.l.record\ndf[\"loss\"] = df.loss.apply(lambda x:x[0])","a67ac94e":"df.sort_values(by = \"loss\",ascending = False)","8af922fa":"df.sort_values(by = \"loss\",ascending = False).reset_index().drop(\"index\",axis=1).to_csv(\"cntext_loss.csv\",index = False)","61a30ce4":"### Train and eval\n* Train the model according to the preliminary label\n* Save the loss to the list ","2eee1fd9":"### Forgebox train loop toolkit\nThis is my personal toolkit, feel welcomed to use other solutions","5f17bd45":"Recording every loss element on every line","b8da038a":"### Samples on negtive lables","41aef3d7":"### Review the toploss rows","88039b8e":"# Using NLP AI model to refine classification\n> Base on the keyword rule based label\n\n* I intend to build a neat dataset about ancient Chinese literatures, all the poem, article, history, etc.\n* [Wikisource dump on zhongwen](https:\/\/dumps.wikimedia.org\/zhwikisource\/20200301\/) is a wonderfule starting point\n* The problem is, it contains too much morden text, include government decree, notice, law etc.\n\n### So the task here is to classify the text","be9f16a1":"### Samples on positive label","1f2af100":"### Pytorch dataset\n\n> a pytorch dataset, generating tokenized and encoded textual info and y label","1bcc6f62":"We only train the top layer module","07de5788":"### Save top layer model","d4fff021":"Attach loss to dataframe","7b51db61":"The data was created from [this notebook](https:\/\/github.com\/raynardj\/python4ml\/blob\/master\/experiments\/ancient_cn\/07_filter_out_morden_cn_part2.ipynb)","172b9028":"### Save the loss to csv"}}