{"cell_type":{"82fcae4b":"code","887a1ce1":"code","3f3ee668":"code","040d1e41":"code","f431d015":"code","8df07bde":"code","7d5bc94c":"code","739c8f8f":"code","c1454ebc":"code","3dd303a5":"code","46d6249e":"code","892cb291":"code","1ac4ef7b":"code","dd1fecda":"code","9c0e9587":"code","9d2c3041":"code","9d17c126":"code","e79b3bc8":"code","d7635d24":"code","abc94729":"code","e1026b51":"code","e271b043":"code","3987cd11":"code","aa99e31e":"code","c8a56e86":"code","1ad5c51a":"code","3f8a8500":"code","f37beca2":"code","aec62fcc":"code","1c4ebbef":"code","16fd0f7d":"code","0515f721":"code","7eae8dca":"code","3a16abb0":"code","acf25804":"code","fa22c13f":"code","809b074a":"code","146c3e24":"code","11318d39":"code","c3906e6b":"code","37d35e8f":"code","c3b90df0":"code","77723aa2":"code","5f9dd59f":"code","fde1d76b":"code","5d66b201":"code","4d9993ca":"code","97b47eb4":"code","9895664f":"code","e4391550":"code","ba32482e":"code","2874e113":"code","2f6e2abb":"code","1ee5337f":"code","c79afb29":"code","46ca5a44":"code","6d35cb25":"code","86077b17":"code","e4cafd38":"code","5ba03722":"code","0cfb83d8":"code","b15f4c56":"code","694b0e50":"code","7409827b":"markdown"},"source":{"82fcae4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np\nimport numpy  # linear algebra\nfrom IPython.display import display, HTML\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nfrom keras.models import *\nfrom keras.layers import Input, Dense, merge\nfrom numpy import newaxis as na\nfrom pickle import load\nfrom numpy import argmax\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import RNN\n\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model\nfrom keras.utils import plot_model\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport string\nimport re\nfrom pickle import dump\nfrom unicodedata import normalize\nfrom numpy import array\nfrom pickle import load\nfrom numpy.random import shuffle\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","887a1ce1":"# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, mode='rt', encoding='utf-8')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text","3f3ee668":"# split a loaded document into sentences\ndef to_pairs(doc):\n    lines = doc.strip().split('\\n')\n    pairs = [line.split('\\t') for line in lines]\n    return pairs","040d1e41":"# clean a list of lines\ndef clean_pairs(lines):\n    cleaned = list()\n    # prepare regex for char filtering\n    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n    re_print = re.compile('[^%s]' % re.escape(string.printable))\n    for pair in lines:\n        clean_pair = list()\n        for line in pair:\n            # normalize unicode characters\n            line = normalize('NFD', line).encode('ascii', 'ignore')\n            line = line.decode('UTF-8')\n            # tokenize on white space\n            line = line.split()\n            # convert to lowercase\n            line = [word.lower() for word in line]\n            # remove punctuation from each token\n            line = [re_punc.sub('', w) for w in line]\n            # remove non-printable chars form each token\n            line = [re_print.sub('', w) for w in line]\n            # remove tokens with numbers in them\n            line = [word for word in line if word.isalpha()]\n            # store as string\n            clean_pair.append(' '.join(line))\n        cleaned.append(clean_pair)\n    return array(cleaned)","f431d015":"# save a list of clean sentences to file\ndef save_clean_data(sentences, filename):\n    dump(sentences, open(filename, 'wb'))\n    print('Saved: %s' % filename)","8df07bde":"# load dataset\nfilename = '..\/input\/engliesh-german\/deu.txt'\ndoc = load_doc(filename)\n# split into english-german pairs\npairs = to_pairs(doc)\n# clean sentences\nclean_pairs = clean_pairs(pairs)\n# save clean pairs to file\nsave_clean_data(clean_pairs, 'english-german.pkl')\n# spot check\nfor i in range(2):\n    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))","7d5bc94c":"\n# load a clean dataset\ndef load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n\n","739c8f8f":"# save a list of clean sentences to file\ndef save_clean_data(sentences, filename):\n    dump(sentences, open(filename, 'wb'))\n    print('Saved: %s' % filename)","c1454ebc":"# load dataset\nraw_dataset = load_clean_sentences('english-german.pkl')\n# reduce dataset size\nn_sentences = 10000\ndataset = raw_dataset[:n_sentences, :]\n# random shuffle\nshuffle(dataset)\n# split into train\/test\ntrain, test = dataset[:9000], dataset[9000:]\n# save\nsave_clean_data(dataset, 'english-german-both.pkl')\nsave_clean_data(train, 'english-german-train.pkl')\nsave_clean_data(test, 'english-german-test.pkl')","3dd303a5":"# load a clean dataset\ndef load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n","46d6249e":"# load datasets\ndataset = load_clean_sentences('english-german-both.pkl')\ntrain = load_clean_sentences('english-german-train.pkl')\nfor i in train[1:5]:\n    print(i)\ntest = load_clean_sentences('english-german-test.pkl')\nfor i in test[1:5]:\n    print(i)","892cb291":"# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","1ac4ef7b":"# max sentence length\ndef max_length(lines):\n    return max(len(line.split()) for line in lines)","dd1fecda":"# prepare english tokenizer\neng_tokenizer = create_tokenizer(dataset[:, 0])\n\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_length = max_length(dataset[:, 0])\nprint('English Vocabulary Size: %d' % eng_vocab_size)\nprint('English Max Length: %d' % (eng_length))\n# prepare german tokenizer\nger_tokenizer = create_tokenizer(dataset[:, 1])\nger_vocab_size = len(ger_tokenizer.word_index) + 1\nger_length = max_length(dataset[:, 1])\nprint('German Vocabulary Size: %d' % ger_vocab_size)\nprint('German Max Length: %d' % (ger_length))","9c0e9587":"# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    # integer encode sequences\n    X = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    X = pad_sequences(X, maxlen=length, padding='post')\n    return X","9d2c3041":"# one hot encode target sequence\ndef encode_output(sequences, vocab_size):\n    ylist = list()\n    for sequence in sequences:\n        encoded = to_categorical(sequence, num_classes=vocab_size)\n        ylist.append(encoded)\n    y = array(ylist)\n    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n    return y","9d17c126":"#define NMT model\n#define NMT model\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n    model = Sequential()\n    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n    model.add(LSTM(n_units))\n    model.add(RepeatVector(tar_timesteps))\n    model.add(LSTM(n_units, return_sequences=True))\n    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n    model.compile(optimizer='adam', loss='categorical_crossentropy')\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model\n# compile model\n    \n   # model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n    #inputs1 =  model.add(LSTM(n_units,return_sequences=True))\n   # attention_probs = model.add((Dense(tar_vocab, activation='softmax')))\n    #outputs1 = (merge([inputs1, attention_probs], name='attention_mul', mode='mul'))\n    #model.add(RepeatVector(tar_timesteps))\n    #model.add(LSTM(n_units, return_sequences=True))(outputs1)\n   # model.add((Dense(tar_vocab, activation='softmax')))\n    # compile model\n    model.compile(optimizer='adam', loss='categorical_crossentropy')\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    #return model\n","e79b3bc8":"def build_model():\n    inputs = Input(shape=(input_dim,))\n\n    # ATTENTION PART STARTS HERE\n    attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)\n    attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul')\n    # ATTENTION PART FINISHES HERE\n\n    attention_mul = Dense(64)(attention_mul)\n    output = Dense(1, activation='sigmoid')(attention_mul)\n    model = Model(input=[inputs], output=output)\n    return model","d7635d24":"# prepare training data\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\ntrainY = encode_output(trainY, eng_vocab_size)\n\n","abc94729":"# prepare validation data\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\ntestY = encode_output(testY, eng_vocab_size)","e1026b51":"# define model\nmodel = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n","e271b043":"\n#fit model\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1,\nsave_best_only=True, mode='min')\nmodel.fit(trainX, trainY, epochs=300, batch_size=1000, validation_data=(testX, testY),\ncallbacks=[checkpoint], verbose=2)","3987cd11":"model.save_weights(\"weights_NMT.h5\")","aa99e31e":" = model.layers[0].get_weights()[0]","c8a56e86":"embedings5 = model.layers[0].get_weights()[0]","1ad5c51a":"model.layers","3f8a8500":"embedings3[0]","f37beca2":"# generate target given source sequence\ndef predict_sequence(model, tokenizer, source):\n    prediction = model.predict(source, verbose=0)[0]\n    integers = [argmax(vector) for vector in prediction]\n    target = list()\n    for i in integers:\n        #print(i)\n        \n        word = word_for_id(i, tokenizer)\n        #print(word)\n        if word is None:\n            break\n        target.append(word)\n    return ' '.join(target)","aec62fcc":"def word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        \n        if index == integer:\n            \n            return word\n    return None\n        ","1c4ebbef":"trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])","16fd0f7d":"# evaluate the skill of the model\ndef evaluate_model(model,sources, raw_dataset):\n    actual, predicted = list(), list()\n    for i, source in enumerate(sources):\n        # translate encoded source text\n        #print(source.shape[0])\n        source = source.reshape((1, source.shape[0]))\n        #print(source.shape)\n        translation = predict_sequence(model, eng_tokenizer, source)\n        raw_target, raw_src = raw_dataset[i]\n        if i < 20:\n            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n        actual.append(raw_target.split())\n        predicted.append(translation.split())\n    # calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))","0515f721":"evaluate_model(model, trainX[10:20], train[10:20])","7eae8dca":"#test[0:100]","3a16abb0":"plot_model(model, to_file='model.png')","acf25804":"model.layers[3].weights","fa22c13f":"first_layer_weights = model.layers[0].get_weights()[0]\n","809b074a":"second_layer_weights_xh = model.layers[1].get_weights()[0]\nsecond_layer_weights_hh = model.layers[1].get_weights()[1]\nsecond_layer_bias_xh = model.layers[1].get_weights()[2]\n","146c3e24":"third_layer_weights_xh = model.layers[2].get_weights()[0]\nthird_layer_weights_hh = model.layers[2].get_weights()[1]\nthird_layer_bias_xh = model.layers[2].get_weights()[2]\n","11318d39":"fourth_layer_weights = model.layers[3].get_weights()[0]\nfourth_layer_bias = model.layers[3].get_weights()[1]\n","c3906e6b":"model.weights","37d35e8f":"T = len(testX[0])\nd   = int(second_layer_weights_xh.shape[1]\/4) \n# initialize\ngates_xh  = np.zeros((T, 4*d))  \ngates_hh  = np.zeros((T, 4*d)) \ngates_pre = np.zeros((T, 4*d))  # gates i, g, f, o pre-activation\ngates     = np.zeros((T, 4*d))  # gates i, g, f, o activation\nh         = np.zeros((T+1, d))\nc         = np.zeros((T+1, d))","c3b90df0":"gates_xh_2  = np.zeros((T, 4*d))  \ngates_hh_2  = np.zeros((T, 4*d)) \ngates_pre_2 = np.zeros((T, 4*d))  # gates i, g, f, o pre-activation\ngates_2     = np.zeros((T, 4*d))  # gates i, g, f, o activation\nh_2         = np.zeros((T+1, d))\nc_2         = np.zeros((T+1, d))","77723aa2":"gates_xh_3  = np.zeros((T, 4*d))  \ngates_hh_3  = np.zeros((T, 4*d)) \ngates_pre_3 = np.zeros((T, 4*d))  # gates i, g, f, o pre-activation\ngates_3     = np.zeros((T, 4*d))  # gates i, g, f, o activation\nh_3         = np.zeros((T+1, d))\nc_3         = np.zeros((T+1, d))","5f9dd59f":"\ny         = np.zeros((T+1,2309))","fde1d76b":"y_2         = np.zeros((T+1,2309))","5d66b201":"e      = first_layer_weights.shape[1]                # word embedding dimension\nx      = np.zeros((T, e))\n#input1 = [\"tom\"]\n#testX = encode_sequences(ger_tokenizer, ger_length, input1)\n#testX\narray1 = np.append(testX[50][0:4],testX[51][0:6])\narray1\nx[:,:] = first_layer_weights[trainX[19],:]","4d9993ca":"idx    = np.hstack((np.arange(0,d), np.arange(d,2*d),np.arange(3*d,4*d))).astype(int) # indices of the gates i,f,o\nfor t in range(T):\n    gates_xh[t]    = np.dot(np.transpose(second_layer_weights_xh),(x[t])) +  second_layer_bias_xh\n    gates_hh[t]    =   np.dot(np.transpose(second_layer_weights_hh),(h[t-1]))  \n    gates_pre[t]   = gates_xh[t] + gates_hh[t]\n    gates[t,idx]   = 1.0\/(1.0 + np.exp(- gates_pre[t,idx]))\n    gates[t,2*d:3*d] = np.tanh(gates_pre[t,2*d:3*d]) \n    c[t]           = gates[t,d:2*d]*c[t-1] + gates[t,0:d]*gates[t,2*d:3*d]\n    h[t]           = gates[t,3*d:4*d]*np.tanh(c[t])\n   \n\nfor t in range(T):\n    gates_xh_2[t]   = np.dot(np.transpose(third_layer_weights_xh),(h[t])) +  third_layer_bias_xh\n    gates_hh_2[t]   =   np.dot(np.transpose(third_layer_weights_hh),(h_2[t-1]))  \n    gates_pre_2[t]   = gates_xh_2[t] + gates_hh_2[t]\n    gates_2[t,idx]   = 1.0\/(1.0 + np.exp(- gates_pre_2[t,idx]))\n    gates_2[t,2*d:3*d] = np.tanh(gates_pre_2[t,2*d:3*d]) \n    c_2[t]           = gates_2[t,d:2*d]*c_2[t-1] + gates_2[t,0:d]*gates_2[t,2*d:3*d]\n    h_2[t]           = gates_2[t,3*d:4*d]*np.tanh(c_2[t])\n    y[t]             = np.dot(np.transpose(fourth_layer_weights),h_2[t]) + fourth_layer_bias\n    \n\"\"\"for t in range(T):\n    gates_xh_3[t]   = np.dot(np.transpose(third_layer_weights_xh),(h_2[t])) +  third_layer_bias_xh\n    gates_hh_3[t]   =   np.dot(np.transpose(third_layer_weights_hh),(h_3[t-1]))  \n    gates_pre_3[t]   = gates_xh_3[t] + gates_hh_3[t]\n    gates_3[t,idx]   = 1.0\/(1.0 + np.exp(- gates_pre_3[t,idx]))\n    gates_3[t,2*d:3*d] = np.tanh(gates_pre_3[t,2*d:3*d]) \n    c_3[t]           = gates_3[t,d:2*d]*c_3[t-1] + gates_3[t,0:d]*gates_3[t,2*d:3*d]\n    h_3[t]           = gates_3[t,3*d:4*d]*np.tanh(c_3[t])\n    y_2[t]             = np.dot(np.transpose(fourth_layer_weights),h_3[t]) + fourth_layer_bias\"\"\"\n    \n    \n            ","97b47eb4":"print(model.png)","9895664f":"def soft_max(z):\n    t = np.exp(z)\n    a = np.exp(z) \/ np.sum(t)\n    return a","e4391550":"target = list()\nfor vector in y:\n    integer = argmax(soft_max(vector))\n    print(integer)\n    word = word_for_id(integer, eng_tokenizer)\n    print(word)\n    if word is None:\n        break\n    target.append(word)\n    ","ba32482e":"for i in target:\n    print(i)","2874e113":"def lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor, debug=False):\n    \"\"\"\n    LRP for a linear layer with input dim D and output dim M.\n    Args:\n    - hin:            forward pass input, of shape (D,)\n    - w:              connection weights, of shape (D, M)\n    - b:              biases, of shape (M,)\n    - hout:           forward pass output, of shape (M,) (unequal to np.dot(w.T,hin)+b if more than one incoming layer!)\n    - Rout:           relevance at layer output, of shape (M,)\n    - bias_nb_units:  number of lower-layer units onto which the bias\/stabilizer contribution is redistributed\n    - eps:            stabilizer (small positive number)\n    - bias_factor:    for global relevance conservation set to 1.0, otherwise 0.0 to ignore bias redistribution\n    Returns:\n    - Rin:            relevance at layer input, of shape (D,)\n    \"\"\"\n    #print(\"hout\")\n    #print(hout)\n    #for i in hout[:5]:\n      #  print(i)\n    sign_out = np.where(hout[na,:]>=0, 1., -1.) # shape (1, M)\n    #sign_out = hout[na,:]\n    \n    #print(na)\n    #print(sign_out)\n   \n\n    \n    numer    = (w* hin[:,na]) + ( (bias_factor*b[na,:]*1. + eps*sign_out*1.) * 1.\/bias_nb_units ) # shape (D, M)\n    #print(\"Numerator\")\n    \n   # print(\"################Numerator###################\")\n    #print(numer)\n    denom    = hout[na,:] + (eps*sign_out*1.)   # shape (1, M)\n    #print(\"denominator\")\n    #print(denom)\n    message  = (numer\/denom) * Rout[na,:]       # shape (D, M)\n    \n    Rin      = message.sum(axis=1)              # shape (D,)\n    \n    # Note: local  layer   relevance conservation if bias_factor==1.0 and bias_nb_units==D\n    #       global network relevance conservation if bias_factor==1.0 (can be used for sanity check)\n    if debug:\n        print(\"local diff: \", Rout.sum() - Rin.sum())\n    \n    return Rin","2f6e2abb":"\nT = len(x)\nd   = int(second_layer_weights_xh.shape[1]\/4) \ne      = first_layer_weights.shape[1]                # word embedding dimension\nC      = 2309  # number of classes\nidx    = np.hstack((np.arange(0,d), np.arange(d,2*d),np.arange(3*d,4*d))).astype(int)\nLRP_class = argmax(soft_max(y[1]))\nRx = np.zeros(x.shape)\nRh  = np.zeros((T+1, d))\nRc  = np.zeros((T+1, d))\nRg  = np.zeros((T,   d)) # gate g only\n\nRx_2 = np.zeros(x.shape)\nRh_2  = np.zeros((T+1, d))\nRc_2  = np.zeros((T+1, d))\nRg_2  = np.zeros((T,   d)) # gate g only\nRout_mask  = np.zeros((C))\nRout_mask[LRP_class] = 1.0 \neps=0.1\n#print(h[9])\nbias_factor=1.0\nRh_2[T-1]  = lrp_linear(h_2[T-1],  fourth_layer_weights , fourth_layer_bias, y[1], y[1]*Rout_mask, 2*d, eps, bias_factor, debug=False)\n\nfor t in reversed(range(T)):\n    Rc_2[t]   += Rh_2[t]\n    Rc_2[t-1]  = lrp_linear(gates_2[t,d:2*d]*c_2[t-1],     np.identity(d), np.zeros((d)), c_2[t], Rc_2[t], 2*d, eps, bias_factor, debug=False)\n    Rg_2[t]    = lrp_linear(gates_2[t,0:d]*gates_2[t,2*d:3*d], np.identity(d), np.zeros((d)),c_2[t], Rc_2[t], 2*d, eps, bias_factor, debug=False)\n    Rh[t]   = lrp_linear(h[t],third_layer_weights_xh[t,2*d:3*d],third_layer_bias_xh[2*d:3*d],gates_pre_2[t,2*d:3*d], Rg_2[t], d+e, eps, bias_factor, debug=False)\n    \n    Rh_2[t-1]  = lrp_linear(h_2[t-1], third_layer_weights_hh[t,2*d:3*d], third_layer_bias_xh[2*d:3*d], gates_pre_2[t,2*d:3*d], Rg_2[t], d+e, eps, bias_factor, debug=False)\n          \n \n    Rc[t]   += Rh[t]\n    Rc[t-1]  = lrp_linear(gates[t,d:2*d]*c[t-1],     np.identity(d), np.zeros((d)), c[t], Rc[t], 2*d, eps, bias_factor, debug=False)\n    Rg[t]    = lrp_linear(gates[t,0:d]*gates[t,2*d:3*d], np.identity(d), np.zeros((d)),c[t], Rc[t], 2*d, eps, bias_factor, debug=False)\n    Rx[t]   = lrp_linear(x[t],second_layer_weights_xh[t,2*d:3*d],second_layer_bias_xh[2*d:3*d],gates_pre[t,2*d:3*d], Rg[t], d+e, eps, bias_factor, debug=False)\n    \n    Rh[t-1]  = lrp_linear(h[t-1], second_layer_weights_hh[t,2*d:3*d], second_layer_bias_xh[2*d:3*d], gates_pre[t,2*d:3*d], Rg[t], d+e, eps, bias_factor, debug=False)\n                    \n ","1ee5337f":"R_words             = np.sum(Rx, axis=1)","c79afb29":"print(R_words)","46ca5a44":"print(gates_pre[1,2*d:3*d])","6d35cb25":"def rotate(l, n):\n    return np.append(l[n:] , l[:n])","86077b17":"#R_words = rotate(R_words,1)","e4cafd38":"R_words_previous             = np.sum(Rh, axis=1)\n","5ba03722":"import matplotlib.pyplot as plt\n\ndef rescale_score_by_abs (score, max_score, min_score):\n    \"\"\"\n    rescale positive score to the range [0.5, 1.0], negative score to the range [0.0, 0.5],\n    using the extremal scores max_score and min_score for normalization\n    \"\"\"\n    \n    # CASE 1: positive AND negative scores occur --------------------\n    if max_score>0 and min_score<0:\n    \n        if max_score >= abs(min_score):   # deepest color is positive\n            if score>=0:\n                return 0.5 + 0.5*(score\/max_score)\n            else:\n                return 0.5 - 0.5*(abs(score)\/max_score)\n\n        else:                             # deepest color is negative\n            if score>=0:\n                return 0.5 + 0.5*(score\/abs(min_score))\n            else:\n                return 0.5 - 0.5*(score\/min_score)   \n    \n    # CASE 2: ONLY positive scores occur -----------------------------       \n    elif max_score>0 and min_score>=0: \n        if max_score == min_score:\n            return 1.0\n        else:\n            return 0.5 + 0.5*(score\/max_score)\n    \n    # CASE 3: ONLY negative scores occur -----------------------------\n    elif max_score<=0 and min_score<0: \n        if max_score == min_score:\n            return 0.0\n        else:\n            return 0.5 - 0.5*(score\/min_score)    \n  \n      \ndef getRGB (c_tuple):\n    return \"#%02x%02x%02x\"%(int(c_tuple[0]*255), int(c_tuple[1]*255), int(c_tuple[2]*255))\n\n     \ndef span_word (word, score, colormap):\n    return \"<span style=\\\"background-color:\"+getRGB(colormap(score))+\"\\\">\"+word+\"<\/span>\"\n\n\ndef html_heatmap (words, scores, cmap_name=\"bwr\"):\n    \n    colormap  = plt.get_cmap(cmap_name)\n    print(len(words)) \n    assert len(words)==len(scores)\n    max_s     = max(scores)\n    min_s     = min(scores)\n    \n    output_text = \"\"\n    \n    for idx, w in enumerate(words):\n        score       = rescale_score_by_abs(scores[idx], max_s, min_s)\n        output_text = output_text + str(score) + span_word(w, score, colormap) + \" \"\n    \n    return output_text + \"\\n\"","0cfb83d8":"#print (\"prediction scores: \",   scores)\nprint (\"\\nLRP target class:  \", LRP_class)\nword = word_for_id(LRP_class, eng_tokenizer)\nprint(word)\nprint (\"\\nLRP relevances:\")\ntarget = list()\nfor integer in trainX[19]:\n    print(integer)\n    word = word_for_id(integer, ger_tokenizer)\n    if word is None:\n        break\n    target.append(word)\nprint (\"\\nLRP heatmap:\")    \ndisplay(HTML(html_heatmap(target, R_words[0: len(target)])))\n","b15f4c56":"#print (\"prediction scores: \",   scores)\nprint (\"\\nLRP target class:  \", LRP_class)\nword = word_for_id(LRP_class, eng_tokenizer)\nprint(word)\nprint (\"\\nLRP relevances:\")\ntarget = list()\nfor vector in y:\n    integer = argmax(soft_max(vector))\n    print(integer)\n    word = word_for_id(integer, eng_tokenizer)\n    if word is None:\n        break\n    target.append(word)\nprint (\"\\nLRP heatmap:\")    \ndisplay(HTML(html_heatmap(target, R_words_previous[0: len(target)])))\n","694b0e50":"    def lrp( w, LRP_class, eps=0.001, bias_factor=1.0):\n        \"\"\"\n        Update the hidden layer relevances by performing LRP for the target class LRP_class\n        \"\"\"\n        # forward pass\n         \n        \n        T      = len(w)\n        d      = int(Wxh_Left.shape[0]\/4)\n        e      = self.E.shape[1] \n        C      = self.Why_Left.shape[0]  # number of classes\n        idx    = np.hstack((np.arange(0,d), np.arange(2*d,4*d))).astype(int) \n        \n        # initialize\n        Rx       = np.zeros(self.x.shape)\n        Rx_rev   = np.zeros(self.x.shape)\n        \n        Rh_Left  = np.zeros((T+1, d))\n        Rc_Left  = np.zeros((T+1, d))\n        Rg_Left  = np.zeros((T,   d)) # gate g only\n        Rh_Right = np.zeros((T+1, d))\n        Rc_Right = np.zeros((T+1, d))\n        Rg_Right = np.zeros((T,   d)) # gate g only\n        \n        Rout_mask            = np.zeros((C))\n        Rout_mask[LRP_class] = 1.0  \n        \n        # format reminder: lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor)\n        Rh_Left[T-1]  = lrp_linear(self.h_Left[T-1],  self.Why_Left.T , np.zeros((C)), self.s, self.s*Rout_mask, 2*d, eps, bias_factor, debug=False)\n        Rh_Right[T-1] = lrp_linear(self.h_Right[T-1], self.Why_Right.T, np.zeros((C)), self.s, self.s*Rout_mask, 2*d, eps, bias_factor, debug=False)\n        \n        for t in reversed(range(T)):\n            #print(Rc_Left[t])\n            Rc_Left[t]   += Rh_Left[t]\n            Rc_Left[t-1]  = lrp_linear(self.gates_Left[t,2*d:3*d]*self.c_Left[t-1],     np.identity(d), np.zeros((d)), self.c_Left[t], Rc_Left[t], 2*d, eps, bias_factor, debug=False)\n            Rg_Left[t]    = lrp_linear(self.gates_Left[t,0:d]*self.gates_Left[t,d:2*d], np.identity(d), np.zeros((d)), self.c_Left[t], Rc_Left[t], 2*d, eps, bias_factor, debug=False)\n            Rx[t]         = lrp_linear(self.x[t],        self.Wxh_Left[d:2*d].T, self.bxh_Left[d:2*d]+self.bhh_Left[d:2*d], self.gates_pre_Left[t,d:2*d], Rg_Left[t], d+e, eps, bias_factor, debug=False)\n            Rh_Left[t-1]  = lrp_linear(self.h_Left[t-1], self.Whh_Left[d:2*d].T, self.bxh_Left[d:2*d]+self.bhh_Left[d:2*d], self.gates_pre_Left[t,d:2*d], Rg_Left[t], d+e, eps, bias_factor, debug=False)\n            \n            Rc_Right[t]  += Rh_Right[t]\n            Rc_Right[t-1] = lrp_linear(self.gates_Right[t,2*d:3*d]*self.c_Right[t-1],     np.identity(d), np.zeros((d)), self.c_Right[t], Rc_Right[t], 2*d, eps, bias_factor, debug=False)\n            Rg_Right[t]   = lrp_linear(self.gates_Right[t,0:d]*self.gates_Right[t,d:2*d], np.identity(d), np.zeros((d)), self.c_Right[t], Rc_Right[t], 2*d, eps, bias_factor, debug=False)\n            Rx_rev[t]     = lrp_linear(self.x_rev[t],     self.Wxh_Right[d:2*d].T, self.bxh_Right[d:2*d]+self.bhh_Right[d:2*d], self.gates_pre_Right[t,d:2*d], Rg_Right[t], d+e, eps, bias_factor, debug=False)\n            Rh_Right[t-1] = lrp_linear(self.h_Right[t-1], self.Whh_Right[d:2*d].T, self.bxh_Right[d:2*d]+self.bhh_Right[d:2*d], self.gates_pre_Right[t,d:2*d], Rg_Right[t], d+e, eps, bias_factor, debug=False)\n                   \n        return Rx, Rx_rev[::-1,:], Rh_Left[-1].sum()+Rc_Left[-1].sum()+Rh_Right[-1].sum()+Rc_Right[-1].sum()\n\n  ","7409827b":"**the below is the mathematical model for LSTM and dense layer, we are directly sending the word embedings of input into the for loop, where in the above cell we have taken the word embedings for a input sequence.**\n****"}}