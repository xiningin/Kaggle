{"cell_type":{"a7f07962":"code","fb2f793c":"code","58446e4a":"code","3ffedfab":"code","c68273b0":"code","b5a9dbe9":"code","cb0283b3":"code","984298cf":"code","7c663709":"code","2efd2615":"code","89b2b554":"code","212df5a6":"code","f994f63f":"code","9d9b8758":"code","05d46886":"code","4a43fadf":"code","86eff673":"code","bf69968f":"code","1bc76ce4":"code","407dcafa":"code","571b08e5":"code","2f47b786":"code","de59ae78":"code","850602fe":"code","ec9520bb":"code","4d222830":"code","eb8f38e6":"code","d6d2c698":"code","9a8c2441":"code","a01e8ef4":"code","a1f3d4ab":"code","2244a266":"code","c29aa5bd":"code","ce676596":"code","345860ea":"code","64707531":"markdown","2ac11d80":"markdown","240c16da":"markdown","368e3929":"markdown","b8cdce65":"markdown","32442311":"markdown","02e7fdc2":"markdown","78c67f0d":"markdown","b158e92b":"markdown","309707a9":"markdown","aca4ba33":"markdown","89e663af":"markdown","523cee69":"markdown","c07eeb69":"markdown","6d2bc6f2":"markdown","7f55e67f":"markdown","fdd356cb":"markdown","37a3e1fb":"markdown","0e43df66":"markdown","8696c149":"markdown","93e70fa1":"markdown","06ac3d6e":"markdown","6dbd959e":"markdown","41fa6fe8":"markdown","8b447d15":"markdown","4cc1967b":"markdown","9c8bc91e":"markdown","9cca054b":"markdown","dc3dbeea":"markdown","9227ae00":"markdown","e96b86d4":"markdown"},"source":{"a7f07962":"import time\nstart=time.time()\n'''execution function'''\nend=time.time()\nprint(end-start)","fb2f793c":"import numpy as np\nimport pandas as pd\n\nallData =pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')\n","58446e4a":"#for train\nallData.Name.head() \nallData['nickName']=allData.Name.str.extract('([A-Za-z]+)\\.')\nallData.nickName.value_counts()\nallData.nickName.replace(to_replace=['Dr', 'Rev', 'Col', 'Major', 'Capt'],value='captain',inplace=True)\nallData.nickName.replace(to_replace=['Dona', 'Jonkheer', 'Countess', 'Sir', 'Lady', 'Don'],value='noble') \nallData.nickName.replace({'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs'}, inplace = True)\nallData.Name.head() \n","3ffedfab":"#for test\ntest.Name.head()                           \ntest['nickName']=test.Name.str.extract('([A-Za-z]+)\\.')\ntest.nickName.value_counts()\ntest.nickName.replace(to_replace=['Dr', 'Rev', 'Col', 'Major', 'Capt'],value='captain',inplace=True)\ntest.nickName.replace(to_replace=['Dona', 'Jonkheer', 'Countess', 'Sir', 'Lady', 'Don'],value='noble') \ntest.nickName.replace({'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs'}, inplace = True)\ntest.Name.head()                           \n","c68273b0":"#for train\nallData['Family_size'] = allData.SibSp + allData.Parch + 1  \nallData.Family_size.value_counts()\nallData.Family_size.replace(to_replace = [1], value = 'single', inplace = True)\nallData.Family_size.replace(to_replace = [2,3], value = 'small', inplace = True)\nallData.Family_size.replace(to_replace = [4,5], value = 'medium', inplace = True)\nallData.Family_size.replace(to_replace = [6, 7, 8, 11], value = 'large', inplace = True)\n","b5a9dbe9":"#for test\ntest['Family_size'] = test.SibSp + test.Parch + 1  \ntest.Family_size.value_counts()\ntest.Family_size.replace(to_replace = [1], value = 'single', inplace = True)\ntest.Family_size.replace(to_replace = [2,3], value = 'small', inplace = True)\ntest.Family_size.replace(to_replace = [4,5], value = 'medium', inplace = True)\ntest.Family_size.replace(to_replace = [6, 7, 8, 11], value = 'large', inplace = True)\n","cb0283b3":"#train\nallData.Embarked.fillna(value='S',inplace=True)\nallData.Fare.fillna(value=allData.Fare.mean(),inplace=True)\nallData.Age = allData.groupby(['nickName', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\nallData.isnull().sum()\n","984298cf":"#test\ntest.Embarked.fillna(value='S',inplace=True)\ntest.Fare.fillna(value=test.Fare.mean(),inplace=True)\ntest.Age = test.groupby(['nickName', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\ntest.isnull().sum()","7c663709":"  \nage_categories=['infants','child','teen','young_adult','adult','geriatric']\nage_numbers=[0,5,12,18,35,60,81]\nallData['new_age']=pd.cut(allData.Age,age_numbers,labels=age_categories)\ndisplay(allData[['new_age','Age']].head())\n\ntest['new_age']=pd.cut(test.Age,age_numbers,labels=age_categories)\n","2efd2615":"\nfare_categories=['low','moderate','high','extreme']\nfare_numbers=[-1,150,250,350,550]\nallData['new_fare']=pd.cut(allData.Fare,fare_numbers,labels=fare_categories)\n\ntest['new_fare']=pd.cut(test.Fare,fare_numbers,labels=fare_categories)","89b2b554":"allData.drop(columns=['Age',\n                       'Fare',\n                       'PassengerId',\n                       'Cabin',\n                       'Name',\n                       'SibSp', \n                       'Parch',\n                       'Ticket'],inplace=True,axis=1)\n\nallData.columns\n","212df5a6":"ids=test['PassengerId']\ntest.drop(columns=['Age',\n                       'Fare',\n                       'Cabin',\n                       'PassengerId',\n                       'Name',\n                       'SibSp', \n                       'Parch',\n                       'Ticket'],inplace=True,axis=1)\n\ntest.columns","f994f63f":"allData.dtypes\nallData.loc[:,['Sex','Embarked','Family_size','nickName']]=allData.loc[:,['Sex','Embarked','Family_size','nickName']].astype('category')\nallData.dtypes\n","9d9b8758":"\ntest.dtypes\ntest.loc[:,['Sex','Embarked','Family_size','nickName']]=test.loc[:,['Sex','Embarked','Family_size','nickName']].astype('category')\n\ntest.dtypes","05d46886":"concat_data=pd.concat([allData,test],sort=False)#to prevent features loss\nconcat_data=pd.get_dummies(concat_data)\ntraining=concat_data.iloc[:891,:]\nx = training.drop(\"Survived\", axis=1)\ny = training[\"Survived\"]\n\ntesting=concat_data.iloc[891:,:]\nx_testing=testing.drop('Survived',axis=1)","4a43fadf":"from sklearn.model_selection  import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.25,random_state=1)\n","86eff673":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n","bf69968f":"logReg=LogisticRegression(solver='liblinear')\nlogReg.fit(x_train, y_train) \ny_pred=logReg.predict(x_test)                  \nlogReg_acc=metrics.accuracy_score(y_test,y_pred)\nlogReg_accuracy = np.round(logReg_acc*100, 2)\nprint(logReg_accuracy)","1bc76ce4":"from sklearn import svm\nSvm=svm.SVC(kernel='rbf',C=1,gamma=.1)\nSvm.fit(x_train,y_train)\ny_pred=Svm.predict(x_test)\nsvm_acc=metrics.accuracy_score(y_test,y_pred)\nsvm_accuracy=np.round(svm_acc*100,2)\nprint(svm_accuracy)","407dcafa":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100)\nrfc.fit(x_train,y_train)\ny_pred=rfc.predict(x_test)\nrfc_acc=metrics.accuracy_score(y_test,y_pred)\nrfc_accuracy=np.round(rfc_acc*100,2)\nprint(rfc_accuracy)","571b08e5":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(x_train,y_train)\ny_pred=knn.predict(x_test)\nknn_acc=metrics.accuracy_score(y_test,y_pred)\nknn_accuracy=np.round(knn_acc*100,2)\nprint(knn_accuracy)","2f47b786":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)\ny_pred=gnb.predict(x_test)\ngnb_acc=metrics.accuracy_score(y_test,y_pred)\ngnb_accuracy=np.round(gnb_acc*100,2)\nprint(gnb_accuracy)","de59ae78":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train)\ny_pred=dtc.predict(x_test)\ndtc_acc=metrics.accuracy_score(y_test,y_pred)\ndtc_accuracy=np.round(dtc_acc*100,2)\nprint(dtc_accuracy)","850602fe":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\ny_pred=gbc.predict(x_test)\ngbc_acc=metrics.accuracy_score(y_test,y_pred)\ngbc_accuracy=np.round(gbc_acc*100,2)\nprint(gbc_accuracy)","ec9520bb":"from sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier()\nabc.fit(x_train,y_train)\ny_pred=abc.predict(x_test)\nabc_acc=metrics.accuracy_score(y_test,y_pred)\nabc_accuracy=np.round(abc_acc*100,2)\nprint(abc_accuracy)","4d222830":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier()\netc.fit(x_train,y_train)\ny_pred=etc.predict(x_test)\netc_acc=metrics.accuracy_score(y_test,y_pred)\netc_accuracy=np.round(etc_acc*100,2)\nprint(etc_accuracy)\n","eb8f38e6":"from sklearn.model_selection import cross_val_score\ndef cross_val_accuracy(m):\n    crossvalscore = cross_val_score(m,x_train,y_train,cv=10,scoring='accuracy').mean()\n    model_accuracy=np.round(crossvalscore*100,2)\n    return model_accuracy\n\n\n#try it\n'''cross_val_models=pd.DataFrame({'Cross_val_accuracy':[\n        cross_val_accuracy(logReg),\n        cross_val_accuracy(Svm),\n        cross_val_accuracy(rfc),\n        cross_val_accuracy(knn),\n        cross_val_accuracy(gnb),\n        cross_val_accuracy(dtc),\n        cross_val_accuracy(gbc),\n        cross_val_accuracy(abc),\n        cross_val_accuracy(etc) \n        ]})\n    \n    \n\n\ncross_val_models.index=[\n           'LogisticRegression',\n                      'Support vector machine',\n           'RandomForestClassifier',\n           'KNeighborsClassifier',\n           'GaussianNB',\n           'DecisionTreeClassifier',\n           'GradientBoostingClassifier',\n           'AdaBoostClassifier',\n           'ExtraTreesClassifier',\n        ]\n\ncross_val_accuracy = cross_val_models.sort_values(by = 'Cross_val_accuracy', ascending = False)\n'''\n","d6d2c698":"#hyper parameters\nlogReg_params={'penalty':['l1','l2'],\n               'C':np.logspace(0,4,10)}\nsvm_params={'C':[6,7,8,9,10,11,12],\n            'kernel':['linear','rbf'],\n            'gamma':[0.5,0.2,0.1,0.001,0.0001]\n            }\n\nrfc_params={'criterion':['gini','entropy'],\n            'n_estimators':[10,15,20,25,30],\n            'min_samples_leaf':[1,2,3],\n            'min_samples_split':[3,4,5,6,7],\n            'max_features':['sqrt','auto','log2']\n           }\nknn_params={'n_neighbors':[3,4,5,6,7,8],\n            'leaf_size':[1,2,3,5],\n            'weights':['uniform','distance'],\n            'algorithm':['auto','ball_tree','kd_tree','brute'],\n            \n            \n            }\ndtc_params={'max_features':['auto','sqrt','log2'],\n\n            'min_samples_split':[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n            'min_samples_leaf':[1,2,3]\n            }\ngbc_params={'learning_rate':[.01,.02,.05,.01],\n            'min_samples_split':[2,3,4],\n            'max_depth':[4,6,8],\n            'max_features':[1.0,0.3,0.1]\n            }\nabc_params={'n_estimators':[1, 5, 10, 15, 20, 25, 40, 50, 60, 80, 100, 130, 160, 200, 250, 300],\n            'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]            }\n\netc_params={\n             'n_estimators':[100,300],\n            'max_depth':[None],\n             'max_features':[1,3,10],\n             'min_samples_split':[2,3,10],\n            'min_samples_leaf':[1,3,10],\n            'bootstrap':[False],\n            'criterion':['gini']\n            }","9a8c2441":"from sklearn.model_selection import GridSearchCV\n\n","a01e8ef4":"def hyperParameterTuning(m,params):\n    grid=GridSearchCV(m,params,scoring='accuracy',cv=10,n_jobs=-1)\n    grid.fit(x_train,y_train)\n    best_score=np.round(grid.best_score_*100,2)\n    return best_score","a1f3d4ab":"\ndef bestParameters(m,parameters):\n    grid=GridSearchCV(m,parameters,scoring='accuracy',cv=10,n_jobs=-1)\n    grid.fit(x_train,y_train)\n    best_parameters=grid.best_params_\n    return best_parameters\n\n\nbestParameters(logReg,logReg_params)\nbestParameters(rfc,rfc_params)\nbestParameters(knn,knn_params)\nbestParameters(dtc,dtc_params)\nbestParameters(gbc,gbc_params)\nbestParameters(abc,abc_params)\nbestParameters(Svm,svm_params)","2244a266":"#now lets comapre all of our accuracies\n'''pureAccuracy=pd.DataFrame({'pureModel':[\n                                   etc_accuracy,\n                                   abc_accuracy,\n                                   gbc_accuracy,\n                                   dtc_accuracy,\n                                   knn_accuracy,\n                                   rfc_accuracy,\n                                   svm_accuracy,\n                                   logReg_accuracy,\n                                   gnb_accuracy\n                             ]})\n    \npureAccuracy.index=[  \n                                   'ExtraTreesClassifier',  \n                                   'AdaBoostClassifier', \n                                   'GradientBoostingClassifier',\n                                   'DecisionTreeClassifier',\n                                   'KNeighborsClassifier',\n                                   'RandomForestClassifier',\n                                   'Support vector machine',\n                                   'LogisticRegression',\n                                   'GaussianNB'\n]\npure_accuracy = pureAccuracy.sort_values(by = 'pureModel', ascending = False)\n \nCV_accuracy=pd.DataFrame({ 'crossValidation':[ \n                                   cross_val_accuracy(etc),\n                                   cross_val_accuracy(abc),\n                                   cross_val_accuracy(gbc),\n                                   cross_val_accuracy(dtc),\n                                   cross_val_accuracy(knn),\n                                   cross_val_accuracy(rfc),\n                                   cross_val_accuracy(Svm),\n                                   cross_val_accuracy(logReg),\n                                   cross_val_accuracy(gnb)\n                                   ]})\nCV_accuracy.index=[\n                                   'ExtraTreesClassifier',  \n                                   'AdaBoostClassifier', \n                                   'GradientBoostingClassifier',\n                                   'DecisionTreeClassifier',\n                                   'KNeighborsClassifier',\n                                   'RandomForestClassifier',\n                                   'Support vector machine',\n                                   'LogisticRegression',\n                                   'GaussianNB'] \ncrossValidation_accuracy = CV_accuracy.sort_values(by = 'crossValidation', ascending = False)\n\nHPT_accuracy=pd.DataFrame({ 'hyperparameterTuning':[\n                                    hyperParameterTuning(etc,etc_params),\n                                    hyperParameterTuning(abc,abc_params),\n                                    hyperParameterTuning(gbc,gbc_params),\n                                    hyperParameterTuning(dtc,dtc_params),\n                                    hyperParameterTuning(knn,knn_params),\n                                    hyperParameterTuning(rfc,rfc_params),\n                                    hyperParameterTuning(Svm,svm_params),\n                                    hyperParameterTuning(logReg,logReg_params),\n                                    0]})#0 for gnb\n                            \n       \nHPT_accuracy.index=[  \n                                   'ExtraTreesClassifier',  \n                                   'AdaBoostClassifier', \n                                   'GradientBoostingClassifier',\n                                   'DecisionTreeClassifier',\n                                   'KNeighborsClassifier',\n                                   'RandomForestClassifier',\n                                   'Support vector machine',\n                                   'LogisticRegression',\n                                   'GaussianNB'\n]\n\n\ntuning_accuracy = HPT_accuracy.sort_values(by = 'hyperparameterTuning', ascending = False)\n'''","c29aa5bd":"#Ensembling voting classifier\nfrom sklearn.ensemble import VotingClassifier\ndtc_voted=DecisionTreeClassifier(max_features='auto',min_samples_leaf=2,min_samples_split=9)\n \nlogReg_voted=LogisticRegression(penalty='l1',C=2.7825594022071245)\n \ngnb_voted=GaussianNB()#no argumenets\n \nsvm_voted=svm.SVC(C=7,gamma=0.2,kernel='rbf',probability=True)\n \nknn_voted=KNeighborsClassifier(algorithm= 'brute', leaf_size=1,n_neighbors= 8, weights= 'uniform')\nvotingEnsembling=VotingClassifier(estimators=[('logReg',logReg_voted),\n                                                 ('gnb',gnb_voted),\n                                                 ('svm',svm_voted),\n                                                 ('knn',knn_voted),\n                                                 ('dtc',dtc_voted)],voting='soft')\nvotingEnsembling.fit(x_train,y_train)\nvotingEnsembling.score(x_test,y_test)\ncrossvalscore=cross_val_score(votingEnsembling,x_train,y_train, cv = 10,scoring = \"accuracy\").mean()\naccuracypercent=np.round(crossvalscore*100,2)","ce676596":"#you can submit etc ,gbc ,rfc\ngrid_rfc=GridSearchCV(rfc,rfc_params,scoring='accuracy',cv=10,n_jobs=-1)\ngrid_rfc.fit(x_train,y_train)\nbest_score=np.round(grid_rfc.best_score_*100,2)\nprediction=grid_rfc.predict(x_testing)\nprint(best_score)","345860ea":"submission=pd.DataFrame({\n                        'PassengerId' :ids ,#see up codes\n                        'Survived' : prediction})\nsubmission.Survived=submission.Survived.astype(int)\nsubmission.to_csv('prediction.csv',index=False)","64707531":"**Modelling\n**","2ac11d80":"**Submission**","240c16da":"**Encoding categorical variables** \n\nconcatination between train and test data must be performed ; since on encoding of train and test data separately , there will be specified number of features in the train data .some features may be lost and upon  making . predict it will show you an error(if features are not equal in train and test data). I will show it at the end.","368e3929":"**1- Accuracy of the pure model**","b8cdce65":"2-drop features for test data\n**Note :** \nbefore droping featues , we  extract PassengerId feature because we need it for submission","32442311":"it is time consuming but you must do it by your own to compare all of your results","02e7fdc2":"**Hyper Parameter tuning**","78c67f0d":"**detecting best parameters**","b158e92b":"Name :","309707a9":"**Understanding our files before sinking**\nso we have 3 files here :\n1 for train data , 1 for test data and another one as mirror image of our submission file that we must submit at the end. I saw alot of kernel join train with test but it is fatal mistake as there is no survival in the test thats will ulter our results and gives fake results . so to get a test set we must split our *train* file into *training* and *testing* and the test file given to us **is not the test data of our model**. ","aca4ba33":"2-binning Fare\n","89e663af":"preprocessing must be done for the train (allData) and test for each feature","523cee69":"1- for train","c07eeb69":"SibSp - Parch  :","6d2bc6f2":"Importing our necessary files and libraries","7f55e67f":"**Organize our accuracies in tables for comparison**","fdd356cb":"Comparing the highiest accuracies from the threee techniques :\n\netc>>85.03 ,gbc ,rfc\n\n83.98>>>gradient boosting after cross validation\n\n81.17>>> logistic regression pure accuracy","37a3e1fb":"Imputing missing values for train and test data\n","0e43df66":"**2- Accuracy after cross validation**","8696c149":"**Features  processing **","93e70fa1":"   1- binning ages","06ac3d6e":"**splitting our train data into train and test split**","6dbd959e":"**Data Transformation**","41fa6fe8":"**Correcting data types of train data**","8b447d15":"2-for test","4cc1967b":"**Note**: for computational purposes . use this code to calculate how many seconds spent to execute code","9c8bc91e":"Thats all for now , I  welcome upvoting and commenting , thanks","9cca054b":"In this tutorial I will show how to make preprocessing and machine learning of titanic data set to achieve as I did 85.03 5 accuracy by using machine learning and preprocessing.\nfor me I saw alot of kaggler submit beatiful EDA better than me so I won't concentrate on EDA because I wan't to make the eyes on preprocessing and machine learning. \n\n\n\n","dc3dbeea":"**Dropping Features**\n\n1- remover age :during survival , they ordered to safe first women and children\nwhatever their ages.\n2- remove name : death won't choose mr and leave sir ,will  it?\n3- Fare : there are alot of things more important than that","9227ae00":"1- drop features for train data","e96b86d4":"**Ensembling**"}}