{"cell_type":{"cf419036":"code","0d1ad7f2":"code","f4d657f8":"code","93d6ee23":"code","e2ae4fbf":"code","cfe9981d":"code","73289e34":"code","91667943":"code","4897ea90":"code","f00e7b93":"code","9380ba16":"code","f6f27bcb":"code","c678dc97":"code","85ac9bb5":"code","6c1a0e7e":"code","8a1916cc":"code","3e05f9d2":"markdown","b68f0c81":"markdown","a316b5a1":"markdown","08654bf0":"markdown","bfe72320":"markdown","9daa8258":"markdown","aeb20422":"markdown","217422a9":"markdown"},"source":{"cf419036":"import pandas as pd","0d1ad7f2":"train = pd.read_csv(\"\/kaggle\/input\/interbank-internacional-2019\/ib_base_inicial_train\/ib_base_inicial_train.csv\")\nX_test = pd.read_csv(\"\/kaggle\/input\/interbank-internacional-2019\/ib_base_inicial_test\/ib_base_inicial_test.csv\")\n\nsunat = pd.read_csv(\"\/kaggle\/input\/interbank-internacional-2019\/ib_base_sunat\/ib_base_sunat.csv\")\nreniec = pd.read_csv(\"\/kaggle\/input\/interbank-internacional-2019\/ib_base_reniec\/ib_base_reniec.csv\")\nvehicular = pd.read_csv(\"\/kaggle\/input\/interbank-internacional-2019\/ib_base_vehicular\/ib_base_vehicular.csv\")\ncampanias = pd.read_csv(\"\/kaggle\/input\/interbank-internacional-2019\/ib_base_campanias\/ib_base_campanias.csv\")","f4d657f8":"y_train = train[['codmes', 'id_persona', 'margen']].copy()\ny_train[\"prediction_id\"] = y_train[\"id_persona\"].astype(str) + \"_\" + y_train[\"codmes\"].astype(str)\ny_train[\"target\"] = y_train[\"margen\"].astype(\"float32\")\ny_train = y_train.set_index(\"prediction_id\")\nX_train = train.drop([\"codtarget\", \"margen\"], axis=1)\nX_train[\"prediction_id\"] = X_train[\"id_persona\"].astype(str) + \"_\" + X_train[\"codmes\"].astype(str)\ndel train","93d6ee23":"sunat = sunat.groupby([\"id_persona\", \"activ_econo\"]).meses_alta.sum().unstack(level=1, fill_value=0).astype(\"int32\")\nvehicular1 = vehicular.groupby([\"id_persona\", \"marca\"]).veh_var1.sum().unstack(level=1, fill_value=0).astype(\"float32\")\nvehicular2 = vehicular.groupby([\"id_persona\", \"marca\"]).veh_var2.sum().unstack(level=1, fill_value=0).astype(\"float32\")\nreniec = reniec.set_index(\"id_persona\").astype(\"float32\")\ndel vehicular","e2ae4fbf":"vehicular1.columns = [c + \"_v1\" for c in vehicular1.columns]\nvehicular2.columns = [c + \"_v2\" for c in vehicular2.columns]","cfe9981d":"X_train = X_train.set_index(\"prediction_id\").astype(\"int32\").reset_index().set_index(\"id_persona\").join(vehicular1).join(vehicular2).join(reniec).join(sunat)\nX_test = X_test.set_index(\"prediction_id\").astype(\"int32\").reset_index().set_index(\"id_persona\").join(vehicular1).join(vehicular2).join(reniec).join(sunat)\ndel vehicular1, vehicular2, reniec, sunat","73289e34":"camp_canal = campanias.groupby([\"codmes\", \"id_persona\", \"canal_asignado\"]).size().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(\"int32\")\ncamp_prod = campanias.groupby([\"codmes\", \"id_persona\", \"producto\"]).size().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(\"int32\")\ndel campanias","91667943":"import gc\ngc.collect()","4897ea90":"meses = {\n    201901: slice(201808, 201810),\n    201902: slice(201809, 201811),\n    201903: slice(201810, 201812),\n    201904: slice(201811, 201901),\n    201905: slice(201812, 201902),\n    201906: slice(201901, 201903),\n    201907: slice(201902, 201904)\n}\n\ncomplementos = []\nfor mes in meses.keys():\n    print(\"*\"*10, mes, \"*\"*10)\n    res = pd.concat([\n        camp_canal.loc[meses[mes]].groupby(\"id_persona\").sum(),\n        camp_prod.loc[meses[mes]].groupby(\"id_persona\").sum()\n        \n    ], axis=1)\n    res[\"codmes\"] = mes\n    res = res.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n    complementos.append(res)\n\ngc.collect()\nprint(\"contatenando complementos\")\ncomplementos = pd.concat(complementos)\ngc.collect()\nprint(\"X_train join\")\nX_train = X_train.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\ngc.collect()\nprint(\"X_test join\")\nX_test = X_test.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\ngc.collect()\n\ndel camp_canal, camp_prod, complementos,res\ngc.collect()","f00e7b93":"non_ascii = X_train.columns[[not all(ord(c) < 128 for c in s) for s in X_train.columns]].tolist()\nnon_ascii","9380ba16":"for i, c in enumerate(non_ascii):\n    X_train[\"non_ascii_\" + str(i)] = X_train[c]\n    X_train = X_train.drop(c, axis= 1)\n    X_test[\"non_ascii_\" + str(i)] = X_test[c]\n    X_test = X_test.drop(c, axis= 1)","f6f27bcb":"from lightgbm import LGBMRegressor\ngc.collect()","c678dc97":"drop_cols = [\"codmes\"]\nfi = []\ntest_probs = []\ntrain_probs = []\nfor mes in X_train.codmes.unique():\n    print(\"*\"*10, mes, \"*\"*10)\n    Xt = X_train[X_train.codmes != mes]\n    yt = y_train.loc[Xt.index, \"target\"]\n    Xt = Xt.drop(drop_cols, axis=1)\n\n    Xv = X_train[X_train.codmes == mes]\n    yv = y_train.loc[Xv.index, \"target\"]\n    \n    learner = LGBMRegressor(n_estimators=1000)\n    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"mae\",\n                eval_set=[(Xt, yt), (Xv.drop(drop_cols, axis=1), yv)], verbose=50)\n    gc.collect()\n    test_probs.append(pd.Series(learner.predict(X_test.drop(drop_cols, axis=1)),\n                                index=X_test.index, name=\"fold_\" + str(mes)))\n    train_probs.append(pd.Series(learner.predict(Xv.drop(drop_cols, axis=1)),\n                                index=Xv.index, name=\"probs\"))\n    fi.append(pd.Series(learner.feature_importances_ \/ learner.feature_importances_.sum(), index=Xt.columns))\n    gc.collect()\n\ntest_probs = pd.concat(test_probs, axis=1).mean(axis=1)\ntrain_probs = pd.concat(train_probs)\nfi = pd.concat(fi, axis=1).mean(axis=1)","85ac9bb5":"fi.sort_values().tail(50).to_frame()","6c1a0e7e":"from scipy.optimize import differential_evolution\n\nres = y_train.join(train_probs.rename(\"probs\"))\noptimization = differential_evolution(lambda c: -((res.probs > c[0]) * res.margen \/ res.margen.sum()).sum(), [(0, 1)])\noptimization","8a1916cc":"test_preds = (test_probs > optimization[\"x\"][0]).astype(int)\ntest_preds.index.name=\"prediction_id\"\ntest_preds.name=\"class\"\ntest_preds.to_csv(\"benchmark_regbresion.csv\", header=True)","3e05f9d2":"## Creaci\u00f3n del Target de predicci\u00f3n\n\nEsta vez, el target ser\u00e1 real en lugar de binario, por tanto, requerir\u00e1 de un algoritmo de regresi\u00f3n.","b68f0c81":"## Guardado del modelo para hacer la presentaci\u00f3n","a316b5a1":"## Optimizaci\u00f3n de punto de corte\n\nCon las probabilidades calculadas en validaci\u00f3n, calcularmos el punto de corte optimo para maximizar la ecuaci\u00f3n econ\u00f3mica de la empresa","08654bf0":"## Consolidaci\u00f3n de Bases\n\nSe unene todas las bases por id_persona","bfe72320":"## Entrenamiento del Modelo\n\nSe entrena un modelo con valores en default, pero optimizando en nro de estimadores inferiores, con validaci\u00f3n basada en meses.","9daa8258":"## Importancia de Variables\n\nObservamos la importancia media que le dieron los modelos a cada variables","aeb20422":"## Renombrado de Variables con nombre no ascii\n\nEl algoritmo que usamos no se lleva bien con cadenas de texto con caracteres especiales, las renombramos.","217422a9":"## Lectura de Datos\n\nSe leer\u00e1n las bases b\u00e1sicas, que solo tienen registros a nivel de usuario. S\u00f3lo se leer\u00e1 la informaci\u00f3n de campa\u00f1a que dependa del tiempo. Queda para mejorar, la incorporacion de m\u00e1s informaici\u00f3n temporal. "}}