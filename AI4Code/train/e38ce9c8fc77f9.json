{"cell_type":{"cd2e592e":"code","48c0c3e5":"code","73d77ef8":"code","bf832fbb":"code","323b4b8f":"code","452e78e8":"code","6c1a96ea":"code","b76441b9":"code","4794f313":"code","04441cfc":"code","d8cd1f04":"code","a0032edd":"code","d71168d3":"code","2d1d33d1":"code","a96c53f6":"code","eceff211":"code","6dfef449":"code","622ba8b4":"code","903eff63":"code","431f61c9":"code","721a82ea":"code","e3cb4d53":"markdown","dc0136a4":"markdown","0cd0b3ee":"markdown","fe6b48e5":"markdown","23e08378":"markdown","0c7bca31":"markdown","47f73990":"markdown","b7ed94d5":"markdown","4024ba74":"markdown","90c1c5cf":"markdown","9af7864d":"markdown","ee171103":"markdown","45729164":"markdown","70ad1f41":"markdown","37da0d9d":"markdown","888cb899":"markdown"},"source":{"cd2e592e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for Data Visualization\nfrom sklearn.model_selection import train_test_split, GridSearchCV # for test and train dataset split and Gridsearch for hypertuning of models\nimport statsmodels.api as sm # for Feature Perfomances\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nimport numpy as np\nimport seaborn as sns","48c0c3e5":"model_path = r'\/kaggle\/working\/best_model.hdf5'","73d77ef8":"data_test = pd.read_csv('..\/input\/titanic\/test.csv', skipinitialspace=True)\ndata_train = pd.read_csv('..\/input\/titanic\/train.csv', skipinitialspace=True)\ndata = data_train.drop(columns=['PassengerId', 'Ticket']) # dropping the columns which are cumbersome\/ not useful to create features out of them\ndata_test_raw = data_test.copy()\ndata.info()\nprint('\\n\\n\\n')\ndata_test.info()\n","bf832fbb":"sns.FacetGrid(data, col='Survived', row='Pclass',  sharex=False, sharey=False).map(plt.hist, 'Age', alpha=0.75).add_legend()\nplt.show()\n\n# 1st class passenger aged between 20-40 had better chance of survival","323b4b8f":"data['Relative'] = data.SibSp + data.Parch # consolidating similar type of data to create new feature\n\nsns.catplot('Relative', 'Survived', data=data, kind='point' )\nplt.show()\n\n# If the passenger had 1-3 relatives, he had good chances of survival. ","452e78e8":"sns.FacetGrid(data, row='Sex', col='Survived', sharex=False, sharey=False).map(plt.hist, 'Age', alpha=0.75, bins=8).add_legend()\nplt.show()\n\n# If the passenger is aged between 20-40, he had good chances of survival\n","6c1a96ea":"data.Sex = data.Sex.map({'male': 1, 'female': 0}) # Converting categorical values into numerical values for feature prep\ndata.Embarked.fillna('S', inplace=True) # Two values were NaN, hence filled with most frequent occuring value\ndata.Age.fillna(data.groupby(by=['Survived', 'Sex', 'Pclass', 'Embarked']).Age.transform('mean'), inplace=True) #Filled the missing Age as per the group age\ndata['DOB'] = 1912-data.Age.round().astype('int64') # Creating Date Of Birth Feature out of age considering the 1912 as the death year.\ndata.Age = pd.qcut(data.Age, q=8, labels=[0, 1, 2, 3, 4, 5, 6, 7]) # binning the age on equal proportion\ndata.DOB = pd.qcut(data.DOB, q=8, labels=[0, 1, 2, 3, 4, 5, 6, 7]) # binning the DOB on equal proportion\ndata.Fare = pd.qcut(data.Fare, q=5, labels=[0, 1, 2, 3, 4]) # Converting the fare value into range of values for feature prep using binning\ndata.Embarked = data.Embarked.map({'S': 0, 'C': 1, 'Q': 2}) # Converting the fare value into range of values for feature prep\ndata['Alone'] = pd.Series(np.where(data.Relative==0, 1, 0))\ndata.Cabin = data.Cabin.astype('str').apply(lambda x: x[0])\ndata.Cabin = data.Cabin.map({'n': 0, 'C': 1, 'E': 2, 'G': 3, 'D': 4, 'A': 5, 'B': 6, 'F': 7, 'T': 8 })\ndata.Name = data.Name = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndata.Name = data.Name.str.replace('Mme', 'Mrs')\ndata.Name = data.Name.str.replace('Ms', 'Miss')\ndata.Name = data.Name.str.replace('Mlle', 'Miss')\ndata.Name = data.Name.str.replace('Dr', 'rare')\ndata.Name = data.Name.str.replace('Rev', 'rare')\ndata.Name = data.Name.str.replace('Col', 'rare')\ndata.Name = data.Name.str.replace('Major', 'rare')\ndata.Name = data.Name.str.replace('Don', 'rare')\ndata.Name = data.Name.str.replace('Jonkheer', 'rare')\ndata.Name = data.Name.str.replace('Countess', 'rare')\ndata.Name = data.Name.str.replace('Lady', 'rare')\ndata.Name = data.Name.str.replace('Capt', 'rare')\ndata.Name = data.Name.str.replace('Sir', 'rare')\ndata.Name = data.Name.map({'np.nan': 0, 'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'rare': 5})\ndata['Fare_per_person'] = (data.Fare.astype('int64')\/(data.Relative.astype('int64') + 1)).astype('int64')\ndata.info() #checking if there's any null value present after data cleansing","b76441b9":"x = data.iloc[:, 1:]\ny = data.iloc[:, 0]\nx1 = sm.add_constant(x)\nresults = sm.OLS(y, x1.astype('float')).fit()\n\n# P-values helps to see whether the feature is contributing positively towards better prediction. More the values towards 0, better the prediction. \npd.DataFrame({'Features': x.columns, 'Weights': round(results.pvalues[1:], 2)}).reset_index().drop(columns='index') ","4794f313":"x_train, x_test, y_train, y_test =train_test_split(x, y, test_size=0.2, random_state=0)\nx_train","04441cfc":"rand_class = RandomForestClassifier(n_estimators=90,random_state=0, n_jobs=-1).fit(x_train, y_train)\nrand_score = (rand_class.score(x_train, y_train)*100).round(2)\n","d8cd1f04":"dec_class = DecisionTreeClassifier(random_state=0).fit(x_train, y_train)\ndec_score = (dec_class.score(x_train, y_train)*100).round(2)","a0032edd":"svm = SVC(probability=True, random_state=0).fit(x_train, y_train)\nsvm_score = (svm.score(x_train, y_train)*100).round(2)","d71168d3":"nb = GaussianNB().fit(x_train, y_train)\nnb_score = (nb.score(x_train, y_train)*100).round(2)","2d1d33d1":"knn = KNeighborsClassifier().fit(x_train, y_train)\nknn_score = (knn.score(x_train, y_train)*100).round(2)","a96c53f6":"log_reg = LogisticRegression(random_state=0).fit(x_train, y_train)\nlog_score = (log_reg.score(x_train, y_train)*100).round(2)","eceff211":"score_table = pd.DataFrame({'Model': ['Random_Forest', 'Decision_Tree', 'SVM', 'Naive Bayes', 'KNN', 'Logistic_Reg'], \n                            'Score': [rand_score, dec_score, svm_score, nb_score, knn_score, log_score]})\nscore_table","6dfef449":"plt.figure(figsize=(20,12))\nplt.plot([0, 1], [0, 1], 'r--')\n\n# Calculating the probablity of prediction\nrand_prob = rand_class.predict_proba(x_test).round(2)\ndec_prob = dec_class.predict_proba(x_test).round(2)\nsvm_prob = svm.predict_proba(x_test).round(2)\nnb_prob = nb.predict_proba(x_test).round(2)\nknn_prob = knn.predict_proba(x_test).round(2)\nlog_reg_prob = log_reg.predict_proba(x_test).round(2)\n\n# Calcuating the FPR, TPR for all the models. \nrand_fpr, rand_tpr, rand_thres = roc_curve(rand_class.predict(x_test), rand_prob[:, 1])\ndec_fpr, dec_tpr, dec_thres = roc_curve(dec_class.predict(x_test), dec_prob[:, 1])\nsvm_fpr, svm_tpr, svm_thres = roc_curve(svm.predict(x_test), svm_prob[:, 1])\nnb_fpr, nb_tpr, nb_thres = roc_curve(nb.predict(x_test), nb_prob[:, 1])\nknn_fpr, knn_tpr, knn_thres = roc_curve(knn.predict(x_test), knn_prob[:, 1])\nlog_reg_fpr, log_reg_tpr, log_reg_thres = roc_curve(log_reg.predict(x_test), log_reg_prob[:, 1])\n\n# Calculating the AUC for all the models\nrand_roc_auc = auc(rand_fpr, rand_tpr)\ndec_roc_auc = auc(dec_fpr, dec_tpr)\nsvm_roc_auc = auc(svm_fpr, svm_tpr)\nnb_roc_auc = auc(nb_fpr, nb_tpr)\nknn_roc_auc = auc(knn_fpr, knn_tpr)\nlog_reg_roc_auc = auc(log_reg_fpr, log_reg_tpr)\n\n# Plotting the ROC of each model. \nplt.plot(rand_fpr, rand_tpr, c='b', label='Random Forest AUC: ' + '{0:.2f}'.format(rand_roc_auc))\nplt.plot(dec_fpr, dec_tpr, c='g', label='Decision Tree AUC: ' + '{0:.2f}'.format(dec_roc_auc))\nplt.plot(svm_fpr, svm_tpr, c='c', label='SVM AUC: ' + '{0:.2f}'.format(svm_roc_auc))\nplt.plot(nb_fpr, nb_tpr, c='m', label='Naive Bayes AUC: ' + '{0:.2f}'.format(nb_roc_auc))\nplt.plot(knn_fpr, knn_tpr, c='y', label='KNN AUC: ' + '{0:.2f}'.format(knn_roc_auc))\nplt.plot(log_reg_fpr, log_reg_tpr, c='k', label='Logistic Regression AUC: ' + '{0:.2f}'.format(log_reg_roc_auc))\nplt.xlabel('FPR', fontsize=16)\nplt.ylabel('TPR', fontsize=16)\nplt.title('ROC Curve', fontsize=16)\nplt.legend(fontsize=16)\nplt.show()\n\n# Below ROC graph suggest, all the models are excellent in classifying people who survived and who couldn't. ","622ba8b4":"count_1 = y_test.sum() # Counting total people survived from the training set\ncount_0 = len(y_test) - count_1 # counting total couldn't survive from training set\narea = auc([0, len(y_test)], [0, count_1]) # Random model AUC\naP = auc([0, count_1, len(y_test)], [0, count_1, count_1]) - area # Perfect model AUC","903eff63":"rand_model = [z for _, z in sorted(zip(rand_prob[:, 1], y_test), reverse=True)]\ndec_model = [z for _, z in sorted(zip(dec_prob[:, 1], y_test), reverse=True)]\nsvm_model = [z for _, z in sorted(zip(svm_prob[:, 1], y_test), reverse=True)]\nnb_model = [z for _, z in sorted(zip(nb_prob[:, 1], y_test), reverse=True)]\nknn_model = [z for _, z in sorted(zip(knn_prob[:, 1], y_test), reverse=True)]\nlog_reg_model = [z for _, z in sorted(zip(log_reg_prob[:, 1], y_test), reverse=True)]\n\n'''Extracting the survivals array and buliding up y- coordinate of the ramp curve by making the cummulative sum of survival prediction'''\nrand_y = np.append([0], np.cumsum(rand_model))\ndec_y = np.append([0], np.cumsum(dec_model))\nsvm_y = np.append([0], np.cumsum(svm_model))\nnb_y = np.append([0], np.cumsum(nb_model))\nknn_y = np.append([0], np.cumsum(knn_model))\nlog_reg_y = np.append([0], np.cumsum(log_reg_model))\n\n'''Creating x-axis value for each model'''\nrand_x = np.arange(0, len(y_test) + 1)\ndec_x = rand_x\nsvm_x = dec_x\nnb_x = rand_x\nknn_x = dec_x\nlog_reg_x = knn_x\n\n'''Calculating Accuracy Rate'''\nrand_aR = (auc(rand_x, rand_y) - area)\/aP\ndec_aR = (auc(dec_x, dec_y) - area)\/aP\nsvm_aR = (auc(svm_x, svm_y) - area)\/aP\nnb_aR = (auc(nb_x, nb_y) - area)\/aP\nknn_aR = (auc(knn_x, knn_y) - area)\/aP\nlog_reg_aR = (auc(log_reg_x, log_reg_y) - area)\/aP\n\n\n# Plotting all models' CAP Curve\nplt.figure(figsize=(20,12))\nplt.plot([0, len(y_test)], [0, count_1], 'r--', label='Random Model')\nplt.plot([0, count_1, len(y_test)], [0, count_1, count_1], c='grey', label='Perfect Model')\nplt.plot(rand_x, rand_y, c='b', label='Random Forest CAP: ' + '{0:.2f}'.format(rand_aR*100) + ', Score: ' + '{0:.2f}%'.format(rand_class.score(x_test, y_test)*100))\nplt.plot(dec_x, dec_y, c='g', label='Decsion Tree CAP: ' + '{0:.2f}'.format(dec_aR*100) + ', Score: ' + '{0:.2f}%'.format(dec_class.score(x_test, y_test)*100))\nplt.plot(svm_x, svm_y, c='c', label='SVM CAP: ' + '{0:.2f}'.format(svm_aR*100) + ', Score: ' + '{0:.2f}%'.format(svm.score(x_test, y_test)*100))\nplt.plot(nb_x, nb_y, c='m', label='Naive Bayes CAP: ' + '{0:.2f}'.format(nb_aR*100) + ', Score: ' + '{0:.2f}%'.format(nb.score(x_test, y_test)*100))\nplt.plot(knn_x, knn_y, c='y', label='KNN CAP: ' + '{0:.2f}'.format(knn_aR*100) + ', Score: ' + '{0:.2f}%'.format(knn.score(x_test, y_test)*100))\nplt.plot(log_reg_x, log_reg_y, c='orange', label='Logistic Regression CAP: ' + '{0:.2f}'.format(log_reg_aR*100) \n         + ', Score: ' + '{0:.2f}%'.format(log_reg.score(x_test, y_test)*100))\nplt.xlabel('Total Testset Population', fontsize=16)\nplt.ylabel('Total Testset Survived', fontsize=16)\nplt.legend(fontsize=16)\nplt.title('CAP CURVE', fontsize=16)\nplt.show()\n\n\n# Below CAP CURVE with Accuracy score shows that Random Forest classifier performs well with great accuracy with least number of tries. \n# Hence I am going to select the same'''","431f61c9":"data_test.drop(columns=['PassengerId', 'Ticket'], inplace=True)\ndata_test.Sex = data_test.Sex.map({'male': 1, 'female': 0}) # Converting categorical values into numerical values for feature prep\ndata_test.Embarked.fillna('S', inplace=True) # Two values were NaN, hence filled with most frequent occuring value\ndata_test.Age.fillna(data_test.groupby(by=['Sex', 'Pclass', 'Embarked']).Age.transform('mean'), inplace=True) #Filled the missing Age as per the group age\ndata_test['DOB'] = 1912-data_test.Age.round().astype('int64') # Creating Date Of Birth Feature out of age considering the 1912 as the death year.\ndata_test['Relative'] = data_test.SibSp + data_test.Parch # consolidating similar type of data to create new feature\ndata_test.Age = pd.qcut(data_test.Age, q=8, labels=[0, 1, 2, 3, 4, 5, 6, 7]) # binning the age on equal proportion\ndata_test.DOB = pd.qcut(data_test.DOB, q=8, labels=[0, 1, 2, 3, 4, 5, 6, 7]) # binning the DOB on equal proportion\ndata_test.Fare = pd.qcut(data_test.Fare, q=5, labels=[0, 1, 2, 3, 4]) # Converting the fare value into range of values for feature prep using binning\ndata_test.Fare = data_test.Fare.fillna(0)\ndata_test.Embarked = data_test.Embarked.map({'S': 0, 'C': 1, 'Q': 2}) # Converting the fare value into range of values for feature prep\ndata_test['Alone'] = pd.Series(np.where(data_test.Relative==0, 1, 0))\ndata_test.Cabin = data_test.Cabin.astype('str').apply(lambda x: x[0])\ndata_test.Cabin = data_test.Cabin.map({'n': 0, 'C': 1, 'E': 2, 'G': 3, 'D': 4, 'A': 5, 'B': 6, 'F': 7, 'T': 8 })\ndata_test.Name = data_test.Name = data_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndata_test.Name = data_test.Name.str.replace('Mme', 'Mrs')\ndata_test.Name = data_test.Name.str.replace('Ms', 'Miss')\ndata_test.Name = data_test.Name.str.replace('Mlle', 'Miss')\ndata_test.Name = data_test.Name.str.replace('Dr', 'rare')\ndata_test.Name = data_test.Name.str.replace('Rev', 'rare')\ndata_test.Name = data_test.Name.str.replace('Col', 'rare')\ndata_test.Name = data_test.Name.str.replace('Major', 'rare')\ndata_test.Name = data_test.Name.str.replace('Don', 'rare')\ndata_test.Name = data_test.Name.str.replace('Jonkheer', 'rare')\ndata_test.Name = data_test.Name.str.replace('Countess', 'rare')\ndata_test.Name = data_test.Name.str.replace('Lady', 'rare')\ndata_test.Name = data_test.Name.str.replace('Capt', 'rare')\ndata_test.Name = data_test.Name.str.replace('Sir', 'rare')\ndata_test.Name = data_test.Name.map({'np.nan': 0, 'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'rare': 5})\ndata_test.Name = data_test.Name.fillna(0)\ndata_test['Fare_per_person'] = (data_test.Fare.astype('int64')\/(data_test.Relative.astype('int64') + 1)).astype('int64')\ndata_test.info() #checking if there's any null value present after data_test cleansing \n\n# No more missing values, hence I can go for prediction","721a82ea":"pd.DataFrame({'PassengerId': data_test_raw.PassengerId, 'Survived': rand_class.predict(data_test)}).to_csv('Titanic_Prediction_ML.csv', index=False)","e3cb4d53":"# Random Forest Classifier","dc0136a4":"# SVM","0cd0b3ee":"# Analysis","fe6b48e5":"# Model Training Performance","23e08378":"# Preprocessing","0c7bca31":"# Train_Test Split","47f73990":"# LogisticRegression","b7ed94d5":"# Loading","4024ba74":"# Perfomance of each Features","90c1c5cf":"# Cummulative CAP Curve","9af7864d":"# Naive Bayes","ee171103":"# Preprocessing testset","45729164":"# Traditional ML prediction","70ad1f41":"# DecisionTreeClassifier","37da0d9d":"# ROC vs AUC Curve plotting","888cb899":"# KNN"}}