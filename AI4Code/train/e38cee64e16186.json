{"cell_type":{"cff1816d":"code","518ba945":"code","7dadb5bc":"code","c498dc67":"code","e2f41cc6":"code","ee18e277":"code","e2ef83fb":"code","fb0e919d":"code","c616f308":"code","beef8d6e":"code","d5c5f65b":"code","a5060205":"code","743717bf":"code","facec1c8":"code","2799cfec":"code","2c3baabf":"code","e48ee527":"code","4626eb42":"code","9ae548c1":"markdown","2138bdf7":"markdown","9d902202":"markdown","6428f8b8":"markdown"},"source":{"cff1816d":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split","518ba945":"train_path = os.path.join(\"..\/input\/\", \"train.csv\")\nraw_data = pd.read_csv(train_path)\n\nraw_data.head(5)","7dadb5bc":"raw_data.info()","c498dc67":"passengers = raw_data.drop(['PassengerId','Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n\nX = passengers.drop('Survived', axis=1)\ny = passengers['Survived'].copy()\n\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, random_state=0)","e2f41cc6":"X_train.head(5)","ee18e277":"X_train.describe()","e2ef83fb":"X_train_dummies = pd.get_dummies(X_train)\nX_train_dummies = X_train_dummies.drop('Sex_male', axis=1)\nX_train_dummies.head(5)","fb0e919d":"passengers.info()","c616f308":"X_train_dummies.info()","beef8d6e":"from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy=\"median\")\nimputer.fit(X_train_dummies)\nX_train_filled = pd.DataFrame(imputer.transform(X_train_dummies), columns=X_train_dummies.columns)\nX_train_filled.info()","d5c5f65b":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X_train_filled)\nX_train_pca = pca.transform(X_train_filled)","a5060205":"Surviver_f = np.zeros((len(y_train), 2))\nSurviver_m = np.zeros((len(y_train), 2))\nNot_Surviver_f = np.zeros((len(X_train_filled), 2))\nNot_Surviver_m = np.zeros((len(X_train_filled), 2))\n\ni, j, l, m = 0, 0, 0, 0\nfor k, data, f in zip(y_train, X_train_pca, X_train_filled['Sex_female']):\n    if k == 1:\n        if f == 1:\n            Surviver_f[i] = data\n            i = i + 1\n        else:\n            Surviver_m[l] = data\n            l = l + 1\n    else:\n        if f == 1:\n            Not_Surviver_f[j] = data\n            j = j + 1\n        else:    \n            Not_Surviver_m[m] = data\n            m = m + 1\n            \n\nplt.scatter(Surviver_f[:, 0], Surviver_f[:, 1], color='green', alpha=.4)\nplt.scatter(Surviver_m[:, 0], Surviver_m[:, 1], color='blue', alpha=.4)\n\nplt.scatter(Not_Surviver_m[:, 0], Not_Surviver_m[:, 1], color='red', alpha=.4)\nplt.scatter(Not_Surviver_f[:, 0], Not_Surviver_f[:, 1], color='orange', alpha=.4)","743717bf":"plt.matshow(pca.components_, cmap='viridis')\nplt.xticks(range(len(X_train_filled.columns)), X_train_filled.columns, rotation=60, ha='left')","facec1c8":"Sample = X_train_filled[['Age', 'Sex_female', 'Pclass', 'Fare']]","2799cfec":"from mpl_toolkits.mplot3d import Axes3D\nimport math\n\nTmp1 = np.zeros((len(Sample), 4))\nTmp2 = np.zeros((len(Sample), 4))\nTmp3 = np.zeros((len(Sample), 4))\nTmp4 = np.zeros((len(Sample), 4))\ni, j, k, l = 0, 0, 0, 0\nfor data, s, f in zip(Sample.values, y_train, Sample['Sex_female']):\n    if s == 1:\n        if f == 1:\n            Tmp1[i] = data\n            i += 1\n        else:\n            Tmp2[j] = data\n            j += 1\n    else:\n        if f == 1:\n            Tmp3[k] = data\n            k += 1\n        else:\n            Tmp4[l] = data\n            l += 1\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.set_xlabel('Age')\nax.set_ylabel('Pclass')\nax.set_zlabel('Fare')\nax.set_yticks([0, 1, 2, 3])\n\nax.scatter(Tmp1[:,0], Tmp1[:,2], Tmp1[:,3], color='blue', alpha=.4)\nax.scatter(Tmp2[:,0], Tmp2[:,2], Tmp2[:,3], color='green', alpha=.4)\nax.scatter(Tmp3[:,0], Tmp3[:,2], Tmp3[:,3], color='red', alpha=.4)\nax.scatter(Tmp4[:,0], Tmp4[:,2], Tmp4[:,3], color='orange', alpha=.4)","2c3baabf":"from sklearn.linear_model import LogisticRegression\n\nX_val_dummies = pd.get_dummies(X_validate).drop('Sex_male', axis=1)\nX_val_filled = pd.DataFrame(imputer.transform(X_val_dummies), columns=X_val_dummies.columns)\nX_val = X_val_filled[['Age', 'Sex_female', 'Pclass', 'Fare']]\n\nclf = LogisticRegression().fit(Sample, y_train)\nprint(\"Test set score:{}\".format(clf.score(Sample, y_train)))\nprint(\"Validation set score:{}\".format(clf.score(X_val, y_validate)))","e48ee527":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(Sample, y_train)\nprint(\"Test set score:{}\".format(tree.score(Sample, y_train)))\nprint(\"Validation set score:{}\".format(tree.score(X_val, y_validate)))","4626eb42":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=5, random_state=2)\nforest.fit(Sample, y_train)\nprint(\"Test set score:{}\".format(forest.score(Sample, y_train)))\nprint(\"Validation set score:{}\".format(forest.score(X_val, y_validate)))","9ae548c1":"Kev features are Age, Sex, Pclass and Fare.","2138bdf7":"In this problem, gender is important feature. I will one hot encode it.","9d902202":"Load train.csv and split train set and validation set.","6428f8b8":"Prepare environment."}}