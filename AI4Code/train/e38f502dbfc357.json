{"cell_type":{"8f95eded":"code","e34c8358":"code","d1ae1334":"code","1cb6f8d5":"code","bdf66d69":"code","f2eabca6":"code","afebacc4":"code","c32f9996":"code","ca0a30bb":"code","201cfc5d":"code","8a096bf6":"code","134f9c85":"code","729ba346":"code","ea5b8106":"code","4476f6a9":"code","7fc40f80":"code","5f31cb9c":"code","223ce7c5":"code","06e52e15":"code","e6ab4c45":"code","1fbde60c":"code","1c96dd7e":"code","105a23bc":"code","ca0ea876":"code","3444deee":"code","bb73c9fe":"code","653a679f":"code","27e31436":"code","e7c9c16f":"code","b3c1314b":"code","24109ec4":"code","11a9f67d":"code","9817db87":"code","b1c20773":"code","ed13dd13":"code","23078567":"code","e40aaa60":"code","c373f78c":"code","4fa5cb7c":"code","02ba682e":"code","33ac6f91":"code","7b85be1a":"code","3a892ecf":"code","91a87177":"code","30ee4b99":"code","be5becbc":"code","3c02c70a":"code","251f1b9e":"code","f8192ea1":"code","cb141f8a":"markdown","f84e19b6":"markdown","684e1119":"markdown","4f996ada":"markdown","fcbfd39e":"markdown","1d957ef0":"markdown","e571d03f":"markdown","0d5cb071":"markdown","e2bad62c":"markdown","cbb215a2":"markdown","9f611482":"markdown","a8f983fb":"markdown","9efb97c5":"markdown","1756bf17":"markdown","4ad27cb5":"markdown","274506f4":"markdown","68b1c3d0":"markdown","3a43532c":"markdown","f57f6812":"markdown","c2aaf719":"markdown","cccb635b":"markdown","548f314f":"markdown","6beb21f1":"markdown","c4165ba8":"markdown","9fc6f4d1":"markdown"},"source":{"8f95eded":"#loading tools for data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# data processing\nimport pandas as pd\n#linear algebra\nimport numpy as np","e34c8358":"#Create dataframes for train and test set\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d1ae1334":"#overview features\ntrain.columns","1cb6f8d5":"#shape\nprint(\"Shape of train set: {}\".format(train.shape))\nprint(\"Shape of test set: {}\".format(test.shape))","bdf66d69":"#first look at data\ntrain.head()","f2eabca6":"#data visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#target\nplt.style.use(\"seaborn-darkgrid\")\nplt.figure(figsize=(10,6))\nsns.histplot(train[\"SalePrice\"])\nplt.show()","afebacc4":"#GrLivArea\ndata = pd.concat([train['SalePrice'], train[\"GrLivArea\"]], axis=1)\nplt.figure(figsize=(12,6))\nsns.scatterplot(x=\"GrLivArea\", y='SalePrice', data=train)\nplt.show()","c32f9996":"#deleting outliers\ntrain.sort_values(by = 'GrLivArea', ascending = False)[:2]\ntrain = train.drop(train[train['Id'] == 1299].index)\ntrain = train.drop(train[train['Id'] == 524].index)","ca0a30bb":"#YearBuilt\nplt.figure(figsize=(24,8))\nsns.boxplot(x=\"YearBuilt\", y=\"SalePrice\", data=train)\nplt.xticks(rotation=90);","201cfc5d":"#Neighborhood\nlist_neighborhood = list(train[[\"Neighborhood\", 'SalePrice']].groupby([\"Neighborhood\"], as_index=False).median().sort_values(by=\"SalePrice\").loc[:, \"Neighborhood\"])\nplt.figure(figsize=(14,8))\nsns.boxplot(x=\"Neighborhood\", y=\"SalePrice\", data=train, order=list_neighborhood)\nplt.xticks(rotation=30);","8a096bf6":"#remove Id column as it does not matter for now\ntrain_Id = train.pop(\"Id\")\ntest_Id = test.pop(\"Id\")","134f9c85":"#missing data in percent and total of each feature\ntrain_total = train.isnull().sum().sort_values(ascending=False)\ntest_total = test.isnull().sum().sort_values(ascending=False)\n\ntrain_percent = (train.isnull().sum() \/ train.isnull().count().sort_values(ascending=False))\ntest_percent = (test.isnull().sum() \/ test.isnull().count().sort_values(ascending=False))\n\ntrain_missing_values = pd.concat([train_total, train_percent], axis=1, keys=[\"Total\", \"Percent\"])\ntest_missing_values = pd.concat([test_total, test_percent], axis=1, keys=[\"Total\", \"Percent\"])","729ba346":"#train set\ntrain_missing_values.head(20)","ea5b8106":"#test set\ntest_missing_values.head(35)","4476f6a9":"#create combined list of train and test set to operate adjustments as a batch\ncombine = [train, test]","7fc40f80":"for dataset in combine:\n    dataset[\"PoolQC\"].fillna(\"None\", inplace=True)\n    dataset[\"MiscFeature\"].fillna(\"None\", inplace=True)\n    dataset[\"Alley\"].fillna(\"None\", inplace=True)\n    dataset[\"FireplaceQu\"].fillna(\"None\", inplace=True)\n    dataset[\"Fence\"].fillna(\"None\", inplace=True)\n    dataset[\"GarageYrBlt\"].fillna(\"None\", inplace=True)\n    dataset[\"GarageQual\"].fillna(\"None\", inplace=True)\n    dataset[\"GarageFinish\"].fillna(\"None\", inplace=True)\n    dataset[\"GarageCond\"].fillna(\"None\", inplace=True)\n    dataset[\"GarageType\"].fillna(\"None\", inplace=True)\n    dataset[\"BsmtCond\"].fillna(\"None\", inplace=True)\n    dataset[\"BsmtQual\"].fillna(\"None\", inplace=True)\n    dataset[\"BsmtExposure\"].fillna(\"None\", inplace=True)\n    dataset[\"BsmtFinType1\"].fillna(\"None\", inplace=True)\n    dataset[\"BsmtFinType2\"].fillna(\"None\", inplace=True)\n    dataset[\"MasVnrType\"].fillna(\"None\", inplace=True)\n","5f31cb9c":"for dataset in combine:\n    dataset[\"LotFrontage\"].fillna(dataset[\"LotFrontage\"].mean(), inplace=True)\n    dataset[\"BsmtFinSF1\"].fillna(dataset[\"BsmtFinSF1\"].median(), inplace=True)\n    dataset[\"TotalBsmtSF\"].fillna(dataset[\"TotalBsmtSF\"].median(), inplace=True)\n    dataset[\"BsmtUnfSF\"].fillna(dataset[\"BsmtUnfSF\"].median(), inplace=True)\n    dataset[\"BsmtFinSF2\"].fillna(dataset[\"BsmtFinSF2\"].median(), inplace=True)\n    dataset[\"GarageArea\"].fillna(dataset[\"GarageArea\"].median(), inplace=True)","223ce7c5":"train.MSZoning.value_counts()","06e52e15":"train.BsmtHalfBath.value_counts()","e6ab4c45":"train.Utilities.value_counts()","1fbde60c":"train.Functional.value_counts()","1c96dd7e":"train.BsmtFullBath.value_counts()","105a23bc":"train.Electrical.value_counts()","ca0ea876":"train.Exterior2nd.value_counts()","3444deee":"train.KitchenQual.value_counts()","bb73c9fe":"train.SaleType.value_counts()","653a679f":"train.GarageCars.value_counts()","27e31436":"train.Exterior1st.value_counts()","e7c9c16f":"for dataset in combine:\n    dataset[\"MSZoning\"].fillna(\"RL\", inplace=True)\n    dataset[\"BsmtHalfBath\"].fillna(0, inplace=True)\n    dataset[\"Utilities\"].fillna(\"AllPub\", inplace=True)\n    dataset[\"Functional\"].fillna(\"Typ\", inplace=True)\n    dataset[\"BsmtFullBath\"].fillna(0, inplace=True)\n    dataset[\"Electrical\"].fillna(\"SBrkr\", inplace=True)\n    dataset[\"Exterior1st\"].fillna(\"VinylSd\", inplace=True)\n    dataset[\"Exterior2nd\"].fillna(\"VinylSd\", inplace=True)\n    dataset[\"KitchenQual\"].fillna(\"TA\", inplace=True)\n    dataset[\"SaleType\"].fillna(\"WD\", inplace=True)\n    dataset[\"GarageCars\"].fillna(2, inplace=True)\n    dataset[\"MasVnrArea\"].fillna(0, inplace=True)","b3c1314b":"#control check for missing data\ntrain_total = train.isnull().sum().sort_values(ascending=False)\ntest_total = test.isnull().sum().sort_values(ascending=False)\n\ntrain_percent = (train.isnull().sum() \/ train.isnull().count().sort_values(ascending=False))\ntest_percent = (test.isnull().sum() \/ test.isnull().count().sort_values(ascending=False))\n\ntrain_missing_values = pd.concat([train_total, train_percent], axis=1, keys=[\"Total\", \"Percent\"])\ntest_missing_values = pd.concat([test_total, test_percent], axis=1, keys=[\"Total\", \"Percent\"])","24109ec4":"train_missing_values.head()","11a9f67d":"test_missing_values.head()","9817db87":"#list of numeric features\nnumeric_features = [cname for cname in train.columns if train[cname].dtype  not in ['object']]\nnumeric_features","b1c20773":"#numeric features that are actually categorical\nnot_numeric = [\"OverallQual\",\"OverallCond\",\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\",\"KitchenAbvGr\",\"BedroomAbvGr\",\"TotRmsAbvGrd\",\"Fireplaces\",\n                       \"GarageCars\",\"MoSold\",\"YrSold\"]","ed13dd13":"#transform not_numeric features into real categorical features \nfor dataset in combine:\n    for feature in not_numeric:\n        dataset[feature] = dataset[feature].astype(str)","23078567":"#list of real numeric features\nnumeric_features = [cname for cname in train.columns if train[cname].dtype  not in ['object']]\nnumeric_features","e40aaa60":"#heatmap for overview of numerical features\ncorrmat = train.corr()\nx = corrmat.loc[abs(corrmat.SalePrice) > 0.3].index\nh_map = train[x].corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(h_map, annot=True)","c373f78c":"#statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n#skewness of each feature\nskewed_feats = train[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = pd.DataFrame({\"Skew\":skewed_feats})\nskewness","4fa5cb7c":"#apply log(1+x) with threshold of skewness < 1.0\nfor feature in skewness.index[0:-5]:\n    train[feature] = np.log1p(train[feature])\nfor feature in skewness.index[0:-5].drop(\"SalePrice\"):\n    test[feature] = np.log1p(test[feature])","02ba682e":"#concat train and test set to get dummie variables\nntrain = train.shape[0]\nntest = test.shape[0]\nall_data = pd.concat((train, test)).reset_index(drop=True)\n#get dummies\nall_data = pd.get_dummies(all_data)\n#split again\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","33ac6f91":"#seperate target\ntarget = train.pop(\"SalePrice\")\ntest.drop(\"SalePrice\", axis=1, inplace=True)","7b85be1a":"#check shape\nprint(train.shape, test.shape)","3a892ecf":"#Cross validation via KFold\nfrom sklearn.model_selection import KFold\nrandom_state = 2021\nn_folds = 10\nk_fold = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)","91a87177":"#Arrays for test and train predictions\nenet_train_preds = np.zeros(len(train.index), )\nenet_test_preds = np.zeros(len(test.index), )\n\nsvr_train_preds = np.zeros(len(train.index), )\nsvr_test_preds = np.zeros(len(test.index), )\n\nlasso_train_preds = np.zeros(len(train.index), )\nlasso_test_preds = np.zeros(len(test.index), )","30ee4b99":"#set up base models\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.svm import SVR\n# Support Vector Regressor\nsvr_model = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n# Elastic Net Regression   \nenet_model = ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n#Lasso Regression\nlasso_model = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","be5becbc":"#Base model 10x fold with SVR, lasso and elasticnet regression\nfrom sklearn.metrics import mean_squared_error\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n    \n    x_train = pd.DataFrame(train.iloc[train_index])\n    x_valid = pd.DataFrame(train.iloc[test_index])\n\n    enet_model.fit(x_train,y_train,)\n    \n    train_oof_preds = enet_model.predict(x_valid)\n    enet_test_preds = enet_model.predict(test)\n    enet_train_preds[test_index] = train_oof_preds\n    print(\": ENET - Score = {}\".format(mean_squared_error(y_valid, train_oof_preds)))\n      \n    svr_model.fit(x_train,y_train)\n    \n    train_oof_preds = svr_model.predict(x_valid)\n    test_oof_preds = svr_model.predict(test)\n    svr_train_preds[test_index] = train_oof_preds\n    svr_test_preds = test_oof_preds\n    print(\": SVR - Score = {}\".format(mean_squared_error(y_valid, train_oof_preds)))\n    \n    lasso_model.fit(x_train,y_train)\n    \n    train_oof_preds = lasso_model.predict(x_valid)\n    lasso_test_preds = lasso_model.predict(test)\n    lasso_train_preds[test_index] = train_oof_preds\n    print(\": LASSO - Score = {}\".format(mean_squared_error(y_valid, train_oof_preds)))\n        \nprint(\"--> Overall metrics\")\n#print(\": XGB Score = {}\".format(mean_squared_error(target, xgb_train_preds)))\nprint(\": ENET Score = {}\".format(mean_squared_error(target, enet_train_preds)))\nprint(\": SVR Score = {}\".format(mean_squared_error(target, svr_train_preds)))\nprint(\": LASSO Score = {}\".format(mean_squared_error(target, lasso_train_preds)))","3c02c70a":"#Stacked Model with XGBRegressor\nfrom xgboost import XGBRegressor\n\nrandom_state = 2021\nn_folds = 10\nk_fold = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pd.DataFrame(data={\n#    \"xgb\": xgb_train_preds.tolist(),\n    \"enet\": enet_train_preds.tolist(),\n    \"svr\": svr_train_preds.tolist(),\n    \"lasso\": lasso_train_preds.tolist(),\n    \"target\": target.tolist()\n})\nl1_test = pd.DataFrame(data={\n#    \"xgb\": xgb_test_preds.tolist(),\n    \"enet\": enet_test_preds.tolist(),\n    \"svr\": svr_test_preds.tolist(),\n    \"lasso\": lasso_test_preds.tolist(),\n})\n\ntrain_preds = np.zeros(len(l1_train.index), )\ntest_preds = np.zeros(len(l1_test.index), )\nfeatures = [\"enet\", \"svr\", \"lasso\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = pd.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pd.DataFrame(l1_train[features].iloc[test_index])\n    \n    model = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.075, max_depth=6, \n                             min_child_weight=1.5, n_estimators=6000,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213,\n                             random_state =7, nthread = -1)\n    model.fit(\n        x_train,\n        y_train, early_stopping_rounds=300, \n             eval_set=[(x_valid, y_valid)], verbose=False\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    test_oof_preds = model.predict(l1_test[features])\n    train_preds[test_index] = train_oof_preds\n    test_preds = test_oof_preds\n    print(\": Stacked Model Score = {}\".format(mean_squared_error(y_valid, train_oof_preds)))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": Stacked Model Score = {}\".format(mean_squared_error(target, train_preds)))","251f1b9e":"#revert log transformation of target predictions\nfinal_test_preds =np.expm1(test_preds)","f8192ea1":"#Final Submission\nsubmission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission[\"SalePrice\"] = final_test_preds.tolist()\nsubmission.to_csv(\"submission.csv\", index=False)","cb141f8a":"Here we have a correlation heatmap for all numerical features. Our initial guesses weren't too bad. Interesting to see how important GarageArea seems to be.","f84e19b6":"Correlation between \"Neighborhood\" and \"SalePrice\" also visible.","684e1119":"Fortunatly for most of data the \"nan\"-values just symbolize the absence of the feature (e.g. pool, garage...). For these cases we just impute the \"nan\"-value with an equivilant \"None\"-placeholder.","4f996ada":"As already said one of my main task for this kernel was to use all available features. As our first task we distinguish between numerical and categorical data. ","fcbfd39e":"\"Dummie-variables\" for categorical features.","1d957ef0":"At the beginning we just want to take a look at the more important features. As a data scientist I took an initial guess about what features are the most important. Important to me seemed GrLivArea, YearBuilt and Neighborhood. Nevertheless we still go through the other features at a later point.","e571d03f":"# 2. Data Cleaning\ud83e\uddf9\n\nFor this kernel I made the decision to try to use all avaiable features we have. That being said we have to impute all \"Nan\"-values.","0d5cb071":"That's a lot of missing data! ","e2bad62c":"# 5. Data Preprocessing for Model\ud83d\udd29","cbb215a2":"Next we take a look at the \"YearBuilt\" feature. Although the trend seems to be \"the newer the better\" there are some years with higher sale price tendencies.","9f611482":"# 1. First look\ud83d\udd0e","a8f983fb":"The houses\/apartments seem very cheap with round about 200000$ in average.","9efb97c5":"Linear correlation between LivingArea and SalePrice - check.","1756bf17":"RobustScalar makes our models more resistant to outliers in our data.","4ad27cb5":"For most of our models it's benifitial to log-transform numerical features in case they are skewed. ","274506f4":"As we are done with preparing the data we are able to focus on the model-building-part. For this kernel we will go forward with a 10x cross validation, a baseline model consisting of SVR, Lasso and ElasticNet Regressor and a stacked xgboost model.","68b1c3d0":"# 6. Submission\ud83d\udd11","3a43532c":"# Boston House Pricing \ud83c\udfe1\n\n![image.png](attachment:e38da2ad-1f18-40e3-a7c4-d00e125b317a.png)\n\nIn this notebook we are going to predict the price values of houses in Boston with the famous Boston House Pricing Dataset.\n\nBest results : standard deviation of 13379$ (75% accuracy)\n\n# Agenda\ud83d\udcc3\n1. First look\n2. Data Cleaning\n3. Data Exploration & Feature Engineering\n4. Data Preprocessing for Models\n5. Basic Model Building and Stacked Model\n6. Submission","f57f6812":"Puh, we have done it. No more missing data.","c2aaf719":"# 6. Baseline Model and Stacked Model\u2699\ufe0f","cccb635b":"# 3. Data Exploration and Feature Engineering\ud83d\udd27","548f314f":"As we can see we have a lot of features available. That being said we will just focus on a subnet of features in our EDA.","6beb21f1":"For missing numerical data we use the median in case we have more than 85% of the data.","c4165ba8":"For missing categorical data we use the most common category.","9fc6f4d1":"At this point in time I just want to save time by deleting the 2 low priced outliers with 3500 and 5000SF immediatly. I know this is part of the \"Feature Engineering\" part."}}