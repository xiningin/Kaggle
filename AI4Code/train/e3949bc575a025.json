{"cell_type":{"a2ea362e":"code","9ba86242":"code","e271a894":"code","0ecdbc37":"code","19c33949":"code","8b87d848":"code","6afd3ba4":"code","160af420":"code","4ce00ab0":"code","6c286bd9":"code","e4fab320":"code","dd2f702d":"code","097376d2":"code","6282cbac":"code","2236b4fa":"code","baeea05e":"code","4a9cb14a":"code","0f8e5182":"code","2e2dac99":"code","e532f132":"code","b2bb8680":"code","c2637685":"code","dc1f5abd":"code","2531c54d":"code","da3fe6b8":"code","4dbfaa7f":"code","969cf0e2":"code","cca6a417":"code","ea285d87":"code","2fe5aa25":"code","d0253f38":"code","1cc8de30":"code","870152d2":"code","6577bf72":"markdown","b066b705":"markdown","a3bc370f":"markdown","cdd7e95c":"markdown","9464c1ce":"markdown","25d35c98":"markdown","d6143a4c":"markdown","bab1a3b8":"markdown","c3c9eb0a":"markdown","0c52e7da":"markdown","beabdd28":"markdown","951e3f5e":"markdown","da2656e2":"markdown"},"source":{"a2ea362e":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n\nfrom sklearn.metrics import r2_score, mean_squared_log_error, mean_absolute_error  \n\nplt.style.use(\"default\")","9ba86242":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e271a894":"pizza1 = pd.read_csv('\/kaggle\/input\/pizza-price-prediction\/pizza_v2.csv')","0ecdbc37":"pizza1.head()","19c33949":"pizza1.info()","8b87d848":"# Checking for missing values\n\npizza1.isnull().sum()","6afd3ba4":"# Checking the count of each category\n\nprint((\"Company: \\n\"),pizza1[\"company\"].value_counts(),(\"\\n\"))\nprint((\"Toppings: \\n\"),pizza1[\"topping\"].value_counts(),(\"\\n\"))\nprint((\"Variants: \\n\"),pizza1[\"variant\"].value_counts(),(\"\\n\"))\nprint((\"Size: \\n\"),pizza1[\"size\"].value_counts())","160af420":"# Changing price and diameter column into numeric format\n\npizza1['price'] = pizza1['price_rupiah'].str.replace(\"Rp\",\"\").str.replace(\",\",\"\").astype(int)\npizza1['diameter'] = pizza1['diameter'].str.replace(\" inch\",\"\").astype(float)\npizza1.head()","4ce00ab0":"# Checking the datatypes now\npizza1.dtypes","6c286bd9":"# Dropping the price_rupiah column\n\npizza1.drop(\"price_rupiah\",axis=1,inplace=True)\npizza1.head()","e4fab320":"# Company share\n\nfig = plt.figure(figsize =(10, 7))\ncompany=pizza1[\"company\"].value_counts()\nax1=plt.pie(pizza1[\"company\"].value_counts(), labels = company.keys(),autopct=\"%.2f%%\"); \n# autopct is to show percentage\nplt.legend(company.keys());","dd2f702d":"# Size share\n\nsize=pizza1[\"size\"].value_counts()\nfig = plt.figure(figsize =(10, 7))\nplt.pie(pizza1[\"size\"].value_counts(), labels = size.keys(),autopct=\"%.2f%%\")\n\nplt.legend(size.keys());","097376d2":"# Topping share\n\npizza1[\"topping\"].value_counts().plot(kind=\"bar\");","6282cbac":"# Variant share\n\npizza1[\"variant\"].value_counts().plot(kind='bar');","2236b4fa":"# This will turn all of the string value into category values\n\nfor label, content in pizza1.items():\n    if pd.api.types.is_string_dtype(content):\n        pizza1[label] = content.astype(\"category\").cat.as_ordered()\n        \npizza1.dtypes","baeea05e":"# Turn categorical variables into numbers\n\nfor label,content in pizza1.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        pizza1[label] = pd.Categorical(pizza1[label]).codes\n        \n        \npizza1.dtypes","4a9cb14a":"sns.heatmap(pizza1.corr(),annot=True,cbar=False)\nplt.show()\n\n# We can see that there is high correlation between price & diameter","0f8e5182":"X = pizza1.drop(\"price\",axis=1)\ny= pizza1[\"price\"]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","2e2dac99":"np.random.seed(42)\nmodels= {\n    \"Linear Regression\": LinearRegression(),\n    \"KNeighborsRegressor\": KNeighborsRegressor(),\n    \"DecisonTreeRegressor\": DecisionTreeRegressor(),\n    \"RandomForestRegressor\": RandomForestRegressor(),\n    \"BaggingRegressor\":BaggingRegressor(),\n    \n}\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through Models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","e532f132":"model_scores= fit_and_score(models,X_train,X_test,y_train,y_test)\nprint(model_scores)","b2bb8680":"np.random.seed(0)\n\nrsgrid_rf={\n 'max_depth': np.arange(10,100,100),\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': np.arange(200,1800,200)}\n\nrsgrid_dec={\"criterion\": [\"mse\", \"mae\"],\n              \"min_samples_split\": [10, 20, 40],\n              \"max_depth\": [2, 6, 8],\n              \"min_samples_leaf\": [20, 40, 100],\n              \"max_leaf_nodes\": [5, 20, 100]}","c2637685":"%%time\n\n# Tune RandomForestRegressor\n\nnp.random.seed(0)\n\n# Setup random hyperparameter search for RandomForest\nrs_rfmodel = RandomizedSearchCV(RandomForestRegressor(),param_distributions=rsgrid_rf,\n                                cv=5,n_iter=20,verbose=True)\n\n# Fit random hyperparameter search model for RandomForest\nrs_rfmodel.fit(X_train,y_train)","dc1f5abd":"%%time\n\n# Tune DecisionTreeRegressor\nnp.random.seed(0)\n\n# Setup random hyperparameter search for DecisonTree\nrs_decmodel= RandomizedSearchCV(DecisionTreeRegressor(),param_distributions=rsgrid_dec,\n                                cv=5,n_iter=20,verbose=True)\n\n# Fit random hyperparameter search model for DecisionTree\nrs_decmodel.fit(X_train,y_train)","2531c54d":"# Scoring RandomForestRegressor\n\ny_preds_rf = rs_rfmodel.predict(X_test)\nr2_score(y_test,y_preds_rf)","da3fe6b8":"# Scoring DecisionTreeRegressor\n\ny_preds_dec = rs_decmodel.predict(X_test)\nr2_score(y_test,y_preds_dec)","4dbfaa7f":"# Scoring RandomForestRegressor train data\n\nnp.random.seed(0)\n\nrf = RandomForestRegressor()\nrf.fit(X_train,y_train)\ny_preds_train = rf.predict(X_train)\ntrain_r2=r2_score(y_train,y_preds_train)\nprint(\"Training R2 score:\",train_r2)","969cf0e2":"# Scoring RandomForestRegressor test data\n\nnp.random.seed(0)\n\nrf = RandomForestRegressor()\nrf.fit(X_train,y_train)\ny_preds_test = rf.predict(X_test)\ntest_r2=r2_score(y_test,y_preds_test)\nprint(\"Test R2 score:\",test_r2)","cca6a417":"# Create function to evaluate model on a few different levels\n\ndef show_scores(model):\n    y_train_preds = model.predict(X_train)\n    y_preds_test = model.predict(X_test)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, y_train_preds),\n              \"Test MAE\": mean_absolute_error(y_test, y_preds_test),\n              \"Training MSLE\": mean_squared_log_error(y_train, y_train_preds),\n              \"Test MSLE\": mean_squared_log_error(y_test, y_preds_test),\n              \"Training R^2\": r2_score(y_train, y_train_preds),\n              \"Test R^2\": r2_score(y_test, y_preds_test)}\n    return scores","ea285d87":"show_scores(rf)","2fe5aa25":"rf.feature_importances_","d0253f38":"# Match features to columns\nfeature_dict = dict(zip(pizza1.columns,list(rf.feature_importances_))) \nfeature_dict","1cc8de30":"# Helper function for plotting feature importance\ndef plot_features(columns, importances):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importances\": importances})\n          .sort_values(\"feature_importances\", ascending=False)\n          .reset_index(drop=True))\n    \n    # Plot the dataframe\n    fig, ax = plt.subplots()\n    ax.barh(df[\"features\"], df[\"feature_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature importance\")\n    ax.invert_yaxis()","870152d2":"plot_features(X_train.columns, rf.feature_importances_)","6577bf72":"## Tuning the hyperparameters","b066b705":"**We see that DecisionTreeRegressor, RandomForestRegressor are the best models,so we will go forward with them**","a3bc370f":"## Visualizing the data","cdd7e95c":"## Feature Importance","9464c1ce":"**We see that RandomForestRegressor performed better out of the box,so we will go forward with it**","25d35c98":"## Pizza Land!!\n\n<img src = \"https:\/\/3.bp.blogspot.com\/-DYDzOymPQSA\/XIW4TsE7ZQI\/AAAAAAAAHzw\/mulRYiNTrvov-gN7Eu-45K4UDBltRvmsQCHMYCw\/s1600\/italian-food-images-pizza-pie-hd-wallpaper-and-background-photos.jpg\" width=600\/>","d6143a4c":"## Conclusion\n\nTherefore, we see that Random Forest Classifier is the best model.\n\nIf you found this helpful, do consider upvoting \ud83d\ude4c.","bab1a3b8":"## Import Data and libraries","c3c9eb0a":"## Data preprocessing","0c52e7da":"## Evaluation","beabdd28":"### RandomizedSearchCV","951e3f5e":"## Modelling","da2656e2":"## Encoding the data"}}