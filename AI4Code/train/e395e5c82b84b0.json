{"cell_type":{"bf3b0c4e":"code","02db26af":"code","054c2c28":"code","be9d0bf7":"code","8f04d5cf":"code","ff7b8364":"code","c950fe81":"code","9f6aa669":"code","0a230265":"code","c97705d6":"code","f714b78f":"code","2c21ca07":"code","6b0dfe11":"code","9a967a0f":"code","577b1564":"code","7b7009f9":"code","8bc18261":"code","ad930166":"code","a9131ed1":"code","d1684e31":"code","336f0aba":"code","d6c804ae":"code","eadd5dc2":"code","448c7b49":"code","8e000bd1":"code","b0250f68":"code","2267b51c":"code","10c67625":"code","8c78816c":"code","a5be3712":"code","f9ac252b":"code","52f2eddb":"code","d177eb36":"code","d5ba2673":"code","70996363":"code","5f4d48d1":"code","2ef0c99a":"code","e39799e6":"code","b1d373c2":"code","77933065":"code","5e5a67b9":"code","e777f632":"code","415eaebc":"code","6284f683":"code","2b788914":"code","766f1ec6":"code","6d4cd7e9":"code","bbb2b534":"code","d3de9bab":"code","576f85e1":"code","492414a4":"code","fb1060fc":"code","462c85b4":"code","e2d777cf":"code","ddc253d2":"code","8395e22a":"markdown","4c13a289":"markdown","a35d2229":"markdown","d1aa9a44":"markdown","d03b961f":"markdown","5b2d8169":"markdown","257a2e69":"markdown","f6d56958":"markdown"},"source":{"bf3b0c4e":"# Standard Libraries\nimport pandas as pd\nimport numpy as np\n\n# Neural Network Model\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n\n# Data Preprocessing\nimport re\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Create test and train\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Machine Learning Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Accuracy \nfrom sklearn.metrics import accuracy_score, make_scorer, roc_curve, roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings('ignore')","02db26af":"news_df = pd.read_csv(\"..\/input\/bbcnewsarchive\/bbc-news-data.csv\",sep='\\t')","054c2c28":"news_df.drop('filename',axis=1,inplace=True)\nnews_df.head()","be9d0bf7":"px.pie(news_df.category.value_counts().to_frame().reset_index(),\n       values='category',names='index',\n       color_discrete_sequence=px.colors.sequential.Darkmint,\n      title = 'Count and frequency news category in DataFrame',\n      labels={'category':'count','index':'category'})","8f04d5cf":"news_df.info()","ff7b8364":"news_df['category_id'] = news_df.category.factorize()[0]","c950fe81":"category_id_df = news_df[['category','category_id']].drop_duplicates().sort_values('category_id')","9f6aa669":"category_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id','category']].values)","0a230265":"pd.DataFrame(news_df.category.value_counts()).rename(columns={'category':'Count'}).style.bar(color='#137a63')","c97705d6":"# Drop duplicate data\nnews_df.drop_duplicates(subset=['category','title'],inplace=True)","f714b78f":"# join title and content\nnews_df['text'] = news_df['title']+ ' ' + news_df['content']","2c21ca07":"# Data Cleaning\ndef clean_text(text):\n    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n    text = ' '.join(text.split())\n    text = text.lower()\n    return text\nnews_df['clean_text'] = news_df['text'].apply(clean_text)","6b0dfe11":"stop_words = set(stopwords.words('english'))\n\n# function to remove stopwords\ndef remove_stopwords(text):\n    no_stopword_text = [w for w in text.split() if not w in stop_words]\n    return ' '.join(no_stopword_text)\nnews_df['clean_text'] = news_df['clean_text'].apply(remove_stopwords)","9a967a0f":"# Word lemmatization\nlemmatizer = WordNetLemmatizer()\ndef lemmatization(text):\n    lem = [lemmatizer.lemmatize(w) for w in text.split()]\n    return ' '.join(lem)\nnews_df['clean_text'] = news_df['clean_text'].apply(lemmatization)","577b1564":"stopwords = set(STOPWORDS)\ntotalTarget = ' '.join(news_df['clean_text'])\nwordCloud = WordCloud(width=1000,height=500,stopwords=stopwords).generate(totalTarget)","7b7009f9":"fig = px.imshow(wordCloud,color_continuous_scale='gray')\nfig.update_layout(coloraxis_showscale=False)\nfig.update_xaxes(showticklabels=False)\nfig.update_yaxes(showticklabels=False)\nfig.show()","8bc18261":"totalTarget = totalTarget.split()\nfreq_dist = nltk.FreqDist(totalTarget)","ad930166":"freq_dist = pd.DataFrame({'word':list(freq_dist.keys()),'count':list(freq_dist.values())})","a9131ed1":"px.bar(freq_dist.nlargest(20,'count').sort_values('count'),y='word',x='count',\n       title='Frequency of Words',color_discrete_sequence=['#137a63'])","d1684e31":"tokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(news_df.clean_text)\nsequences = tokenizer.texts_to_sequences(news_df.clean_text)\ntext = pad_sequences(sequences)\nlabels = news_df.category_id.astype('float32')","336f0aba":"tokenizer.word_index['said']","d6c804ae":"reverse_word_index = dict([(value,key) for (key,value) in tokenizer.word_index.items()])","eadd5dc2":"reverse_word_index[1]","448c7b49":"' '.join([reverse_word_index.get(i,'') for i in text[0]])","8e000bd1":"news_df['clean_text'][0]","b0250f68":"def vectorize_sequence(sequnce,dimension=10000):\n    results = np.zeros((len(sequnce),dimension))\n    for i,sequnce in enumerate(sequnce):\n        results[i,sequnce] = 1\n    return results\nvectorizer = vectorize_sequence(text)","2267b51c":"vectorizer[0]","10c67625":"vec = TfidfVectorizer(max_features=10000)\nfeatures = vec.fit_transform(news_df.clean_text).toarray()\nlabels = news_df.category_id\nprint(type(features),features.shape)","8c78816c":"from sklearn.feature_selection import chi2\nN = 3\nfor category,category_id in sorted(category_to_id.items()):\n    features_chi2 = chi2(features,labels == category_id)\n    indices = np.argsort(features_chi2[0])\n    feature_names = np.array(vec.get_feature_names())[indices]\n    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]         \n    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]          \n    print(\"# '{}':\".format(category))\n    print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:]))) \n    print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:]))) ","a5be3712":"split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_index,test_index in split.split(news_df,news_df['category_id']):\n    strat_train_set = news_df.iloc[train_index]\n    strat_test_set = news_df.iloc[test_index]","f9ac252b":"X_train = strat_train_set['clean_text']\ny_train = strat_train_set['category_id']","52f2eddb":"X_test = strat_test_set['clean_text']\ny_test = strat_test_set['category_id']","d177eb36":"one_hot_train_labels = to_categorical(y_train)\none_hot_test_labels = to_categorical(y_test)","d5ba2673":"Xtrain_tfidf = vec.fit_transform(X_train)\nXtest_tfidf = vec.fit_transform(X_test)","70996363":"Xtrain_vectorize = vectorize_sequence(text[train_index])\nXtest_vectorize = vectorize_sequence(text[test_index])","5f4d48d1":"perform_list = []\ndef run_model(model_name,X_train,X_test,process):\n    if model_name == 'Logistic Regression':\n        mdl = LogisticRegression()\n    elif model_name == 'Linear SVC':\n        mdl = LinearSVC()\n    elif model_name == 'Random Forest':\n        mdl = RandomForestClassifier(n_estimators=100,criterion='entropy')\n    elif model_name == 'Multinomial Naive Bayes':\n        mdl = MultinomialNB()\n    mdl.fit(X_train,y_train)\n    y_pred = mdl.predict(X_test)\n    \n    # Performance metrics\n    acc = round(accuracy_score(y_test,y_pred) * 100,2)\n    precision,recall,f1score,support = score(y_test,y_pred,average='micro')\n    \n    print(f'Test Accuracy Score of Basic {model_name}: % {acc}')\n    print(f'Precision : {precision}')\n    print(f'Recall    : {recall}')\n    print(f'F1-score   : {f1score}')\n\n    # Add performance parameters to list\n    \n    perform_list.append(dict([\n        ('Model', model_name),\n        ('Test Accuracy', round(acc, 2)),\n        ('Precision', round(precision, 2)),\n        ('Recall', round(recall, 2)),\n        ('F1', round(f1score, 2)),\n        ('process',process)\n            ]))\n\n        ","2ef0c99a":"run_model('Random Forest',Xtrain_tfidf,Xtest_tfidf,'tfidf')","e39799e6":"run_model('Random Forest',Xtrain_vectorize,Xtest_vectorize,'vectorize')","b1d373c2":"run_model('Logistic Regression',Xtrain_tfidf,Xtest_tfidf,'tfidf')","77933065":"run_model('Logistic Regression',Xtrain_vectorize,Xtest_vectorize,'vectorize')","5e5a67b9":"run_model('Multinomial Naive Bayes',Xtrain_tfidf,Xtest_tfidf,'tfidf')","e777f632":"run_model('Multinomial Naive Bayes',Xtrain_vectorize,Xtest_vectorize,'vectorize')","415eaebc":"run_model('Linear SVC',Xtrain_tfidf,Xtest_tfidf,'tfidf')","6284f683":"run_model('Linear SVC',Xtrain_vectorize,Xtest_vectorize,'vectorize')","2b788914":"model_performance = pd.DataFrame(data=perform_list)\nmodel_performance = model_performance[['Model', 'Test Accuracy', 'Precision', 'Recall', 'F1','process']]\nmodel_performance","766f1ec6":"model = models.Sequential()\nmodel.add(layers.Dense(16,activation='relu',input_shape=(10000,)))\nmodel.add(layers.Dense(16,activation='relu'))\nmodel.add(layers.Dense(5,activation='softmax'))","6d4cd7e9":"model.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['acc'])","bbb2b534":"history = model.fit(Xtrain_vectorize,one_hot_train_labels,\n                   epochs=20,\n                   batch_size=64,\n                   validation_data=(Xtest_vectorize,one_hot_test_labels))","d3de9bab":"model.predict(Xtest_vectorize)","576f85e1":"X_test[1713]","492414a4":"y_test","fb1060fc":"id_to_category","462c85b4":"history.history.keys()","e2d777cf":"acc = history.history['acc']\nacc_val = history.history['val_acc']\nepochs = range(1,len(acc)+1)","ddc253d2":"plt.plot(epochs,acc,'bo')\nplt.plot(epochs,acc_val,'r')","8395e22a":"# Prepare Data for Models","4c13a289":"## Preprocessing Second Step\nApproach for text preprocessing  \n1. lower casing\n2. removal of punctuations and numbers\n3. remove white space\n4. remove stops\n5. lemmitization word","a35d2229":"## :)","d1aa9a44":"# Machine Learning Model","d03b961f":"## Work in progress","5b2d8169":"## Preprocessing First Step\nOrdinal encoding : *category* column","257a2e69":"# Neural Network Model","f6d56958":"# Visualization"}}