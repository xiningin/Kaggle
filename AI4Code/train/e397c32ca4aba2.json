{"cell_type":{"eb654f85":"code","59b5db0f":"code","0c397448":"code","8a6be778":"code","7fcb2988":"code","c3915615":"code","74cb21c7":"code","e3df2d45":"code","48352f76":"code","3cd2eaed":"code","fbb6bbd4":"code","be3db0c3":"markdown","24b3e3d0":"markdown","03f7295d":"markdown"},"source":{"eb654f85":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nfrom numpy.linalg import det,inv\nimport pystan as ps\nfrom scipy.stats import multivariate_normal","59b5db0f":"def gmm(X,k,n_iter = 20):\n    # Initial approximation\n    N = X.shape[0] # Number of data\n    m = X.shape[1] # Numper of features\n    w = np.ones(k)\/k # Coef of distributions\n    \u00b5 = np.ones([k,m]) # Mathematical expectation\n    S = np.array([np.eye(m) for _ in range(k)]) # Matrix of covariation\n    G = np.ones([N,k]) \n    \n    for j in range(k):\n        \u00b5[j] = X[np.random.choice(N)]\n    \n    \n    # E-step\n    for _ in range(n_iter):\n        for j in range(k):\n            G[:,j] = w[j]*multivariate_normal.pdf(X,\u00b5[j],S[j])\n        G = G\/(G.sum(axis=1,keepdims=True))\n        n = G.sum(axis = 0)\n\n    # M-step\n        w = n\/N\n        \u00b5 = G.T@X\/n[:,None]\n        for j in range(k):\n            delta = X - \u00b5[j,:] # N x m\n            Rdelta = np.expand_dims(G[:,j], -1) * delta\n            S[j] = Rdelta.T.dot(delta) \/ n[j]\n    return(\u00b5,S,w)\n\ndef gmm_predict(X,k,n_iter = 20):\n    N = len(X)\n    Class = np.zeros(N)\n    \u00b5,S,w = gmm(X,k,n_iter)\n    G = np.ones([N,k])\n    \n    for j in range(k):\n        G[:,j] = w[j]*multivariate_normal.pdf(X,\u00b5[j],S[j])\n    \n    Class = G.argmax(axis=1)\n    return(Class)","0c397448":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\nimport plotly.graph_objects as go\n\n# Making data\nX, y = datasets.make_blobs(n_samples=2000)\n\n# \u0421lustering\ncolor = gmm_predict(X,3,20)\n\n# \u0421hart\nfig = go.Figure(data=go.Scatter(\n    x=X.T[0],\n    y=X.T[1],\n    mode='markers',\n    marker=dict(color=color)))\n\nfig.show()","8a6be778":"# Toy data\nX = np.concatenate([np.random.randn(100)*2+12,np.random.randn(300)*5-5])","7fcb2988":"import plotly.express as px\nfig = go.Figure(data=[go.Histogram(x=X,histnorm='probability',opacity=0.5)])\nfig.show()","c3915615":"eccmodel = \"\"\"\ndata {\n    int<lower=1> m; \/\/ number of mixture components\n    int<lower=1> N; \/\/ number of data points\n    real y[N]; \/\/ observations\n}\nparameters {\n    vector[m] mu;\n    vector<lower=0>[m] o;\n    simplex[m] w;\n    vector[m] lambda;\n}\n\nmodel {\n    o ~ cauchy(0, 5);\n    w ~ dirichlet(lambda);\n    mu ~ normal(0, 20);\n    for (n in 1:N) {\n        target += log_mix(w[1],\n            normal_lpdf(y[n] | mu[1], o[1]),\n            normal_lpdf(y[n] | mu[2], o[2]));\n    }\n}\n\"\"\"\nsm1 = ps.StanModel(model_code=eccmodel)","74cb21c7":"data = {\"m\":2,\"N\":len(X),\"y\":X}\nfit = sm1.sampling(data=data, iter=1000, chains=1)","e3df2d45":"fit.plot()","48352f76":"def stan_predict(X,fit):\n    N = len(X)\n    Class = np.zeros(N)\n    \u00b5,S,w = fit[\"mu\"].mean(axis=0),fit[\"o\"].mean(axis=0),fit[\"w\"].mean(axis=0)\n    G = np.ones([N,2])\n    for j in range(2):\n        G[:,j] = w[j]*multivariate_normal.pdf(X,\u00b5[j],S[j])\n    \n    Class = G.argmax(axis=1)\n    return(Class)","3cd2eaed":"color = stan_predict(X,fit);","fbb6bbd4":"df = pd.DataFrame([X,color]).T\nfig = px.histogram(df, x=0, color=1)\nfig.show()","be3db0c3":"# Cluster data with Gaussian Mixture Model\n\nEM is algorithm which allows to compute maximum likelihood or maximum a posteriori (MAP) estimates of parameters. Basic example is separation of the Gaussian mixture. \nYou can read about EM [here](https:\/\/www.nowpublishers.com\/article\/Details\/SIG-034)\nAbout Gaussian Mixture Model [here](https:\/\/towardsdatascience.com\/gaussian-mixture-models-explained-6986aaf5a95)\n\n","24b3e3d0":"# Realization GMM with x\u2208\u211d at Stan","03f7295d":"**We are looking for a distribution on x as a normalized sum of Gaussians**\n\n\n\\begin{align}\n& \\text{p(x) =} \\sum_{i=1}^kw_i \\cdot N(x|\u00b5_{i},S_{i})  & \\\\\n\\end{align}\n"}}