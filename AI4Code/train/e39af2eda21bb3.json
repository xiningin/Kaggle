{"cell_type":{"16942316":"code","853f41f0":"code","1c03e0ba":"code","1cc472af":"code","f1d33545":"code","57cd3aca":"code","43a3b551":"code","784b316b":"code","289ce3e9":"code","4ab0cf78":"code","ba3d70db":"code","405e4e7b":"code","282e9bc6":"code","5e12006d":"code","473e9cd7":"code","e16cb92c":"code","b68df8b2":"code","b4ece6d8":"code","9c50425a":"code","a0178cf7":"code","f082dc9f":"code","24e26bb8":"code","3b7a1c8e":"code","f057c89a":"code","279331d4":"code","ed52a4fe":"code","269db038":"code","fe9195cf":"code","9062deee":"markdown","471194bd":"markdown","d378a721":"markdown","f8140323":"markdown","9407fd1c":"markdown","708692a1":"markdown","31d84d20":"markdown","d40e57ec":"markdown","4dfff0a4":"markdown","c7f8d406":"markdown","8bda2ffc":"markdown","42aa8111":"markdown","877b22bd":"markdown","1d357d53":"markdown"},"source":{"16942316":"import numpy as np \nimport pandas as pd \nimport statsmodels.api as sm\nfrom statsmodels.tools import add_constant\nfrom itertools import combinations\nfrom sklearn.model_selection import train_test_split","853f41f0":"df = pd.read_csv('\/kaggle\/input\/coronary-heart-disease\/CHDdata.csv')\ndf[\"famhist\"] = (df[\"famhist\"] == \"Present\")*1 # converts the famhit to 0 (no hist) and 1 (has hist)\n#df = df.drop([\"famhist\"], axis=1)\ndf.head()","1c03e0ba":"X = df.drop([\"chd\"], axis=1)\ny = df[\"chd\"]\n# building the model and fitting the data \nlog_reg = sm.Logit(y, add_constant(X)).fit()\nlog_reg.summary()","1cc472af":"# Seaborn visualization library\nimport seaborn as sns\n# Create the default pairplot\ng = sns.pairplot(df, hue=\"chd\", palette=\"tab10\", markers=[\"o\", \"D\"])","f1d33545":"from mpmath import mp\nmp.dps = 50\nclass BMA:\n    \n    def __init__(self, y, X, **kwargs):\n        # Setup the basic variables.\n        self.y = y\n        self.X = X\n        self.names = list(X.columns)\n        self.nRows, self.nCols = np.shape(X)\n        self.likelihoods = mp.zeros(self.nCols,1)\n        self.likelihoods_all = {}\n        self.coefficients_mp = mp.zeros(self.nCols,1)\n        self.coefficients = np.zeros(self.nCols)\n        self.probabilities = np.zeros(self.nCols)\n        # Check the max model size. (Max number of predictor variables to use in a model.)\n        # This can be used to reduce the runtime but not doing an exhaustive sampling.\n        if 'MaxVars' in kwargs.keys():\n            self.MaxVars = kwargs['MaxVars']\n        else:\n            self.MaxVars = self.nCols  \n        # Prepare the priors if they are provided.\n        # The priors are provided for the individual regressor variables.\n        # The prior for a model is the product of the priors on the variables in the model.\n        if 'Priors' in kwargs.keys():\n            if np.size(kwargs['Priors']) == self.nCols:\n                self.Priors = kwargs['Priors']\n            else:\n                print(\"WARNING: Provided priors error.  Using equal priors instead.\")\n                print(\"The priors should be a numpy array of length equal tot he number of regressor variables.\")\n                self.Priors = np.ones(self.nCols)  \n        else:\n            self.Priors = np.ones(self.nCols)  \n        if 'Verbose' in kwargs.keys():\n            self.Verbose = kwargs['Verbose'] \n        else:\n            self.Verbose = False \n        if 'RegType' in kwargs.keys():\n            self.RegType = kwargs['RegType'] \n        else:\n            self.RegType = 'LS' \n        \n    def fit(self):\n        # Perform the Bayesian Model Averaging\n        \n        # Initialize the sum of the likelihoods for all the models to zero.  \n        # This will be the 'normalization' denominator in Bayes Theorem.\n        likelighood_sum = 0\n        \n        # To facilitate iterating through all possible models, we start by iterating thorugh\n        # the number of elements in the model.  \n        max_likelihood = 0\n        for num_elements in range(1,self.MaxVars+1): \n            \n            if self.Verbose == True:\n                print(\"Computing BMA for models of size: \", num_elements)\n            \n            # Make a list of all index sets of models of this size.\n            Models_next = list(combinations(list(range(self.nCols)), num_elements)) \n             \n            # Occam's window - compute the candidate models to use for the next iteration\n            # Models_previous: the set of models from the previous iteration that satisfy (likelihhod > max_likelihhod\/20)\n            # Models_next:     the set of candidate models for the next iteration\n            # Models_current:  the set of models from Models_next that can be consturcted by adding one new variable\n            #                    to a model from Models_previous\n            if num_elements == 1:\n                Models_current = Models_next\n                Models_previous = []\n            else:\n                idx_keep = np.zeros(len(Models_next))\n                for M_new,idx in zip(Models_next,range(len(Models_next))):\n                    for M_good in Models_previous:\n                        if(all(x in M_new for x in M_good)):\n                            idx_keep[idx] = 1\n                            break\n                        else:\n                            pass\n                Models_current = np.asarray(Models_next)[np.where(idx_keep==1)].tolist()\n                Models_previous = []\n                        \n            \n            # Iterate through all possible models of the given size.\n            for model_index_set in Models_current:\n                \n                # Compute the linear regression for this given model. \n                model_X = self.X.iloc[:,list(model_index_set)]\n                if self.RegType == 'Logit':\n                    model_regr = sm.Logit(self.y, model_X).fit(disp=0)\n                else:\n                    model_regr = OLS(self.y, model_X).fit()\n                \n                # Compute the likelihood (times the prior) for the model. \n                model_likelihood = mp.exp(-model_regr.bic\/2)*np.prod(self.Priors[list(model_index_set)])\n                    \n                if (model_likelihood > max_likelihood\/20):\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"likelihood=\",model_likelihood)\n                    self.likelihoods_all[str(model_index_set)] = model_likelihood\n                    \n                    # Add this likelihood to the running tally of likelihoods.\n                    likelighood_sum = mp.fadd(likelighood_sum, model_likelihood)\n\n                    # Add this likelihood (times the priors) to the running tally\n                    # of likelihoods for each variable in the model.\n                    for idx, i in zip(model_index_set, range(num_elements)):\n                        self.likelihoods[idx] = mp.fadd(self.likelihoods[idx], model_likelihood, prec=1000)\n                        self.coefficients_mp[idx] = mp.fadd(self.coefficients_mp[idx], model_regr.params[i]*model_likelihood, prec=1000)\n                    Models_previous.append(model_index_set) # add this model to the list of good models\n                    max_likelihood = np.max([max_likelihood,model_likelihood]) # get the new max likelihood if it is this model\n                else:\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"rejected by Occam's window\")\n                    \n\n        # Divide by the denominator in Bayes theorem to normalize the probabilities \n        # sum to one.\n        self.likelighood_sum = likelighood_sum\n        for idx in range(self.nCols):\n            self.probabilities[idx] = mp.fdiv(self.likelihoods[idx],likelighood_sum, prec=1000)\n            self.coefficients[idx] = mp.fdiv(self.coefficients_mp[idx],likelighood_sum, prec=1000)\n        \n        # Return the new BMA object as an output.\n        return self\n    \n    def predict(self, data):\n        data = np.asarray(data)\n        if self.RegType == 'Logit':\n            try:\n                result = 1\/(1+np.exp(-1*np.dot(self.coefficients,data)))\n            except:\n                result = 1\/(1+np.exp(-1*np.dot(self.coefficients,data.T)))\n        else:\n            try:\n                result = np.dot(self.coefficients,data)\n            except:\n                result = np.dot(self.coefficients,data.T)\n        \n        return result  \n        \n    def summary(self):\n        # Return the BMA results as a data frame for easy viewing.\n        df = pd.DataFrame([self.names, list(self.probabilities), list(self.coefficients)], \n             [\"Variable Name\", \"Probability\", \"Avg. Coefficient\"]).T\n        return df  ","57cd3aca":"result = BMA(y,add_constant(X), RegType = 'Logit', Verbose=True).fit()","43a3b551":"result.summary()\n","784b316b":"result.likelihoods_all","289ce3e9":"# predict the y-values from training input data\npred_BMA = result.predict(add_constant(X))\npred_Logit = log_reg.predict(add_constant(X))","4ab0cf78":"# plot the predictions with the actual values\nimport matplotlib.pyplot as plt\nplt.scatter(pred_BMA,y-0.05)\nplt.scatter(pred_Logit,y)\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Coronary Heart Disease \\n(0=Not Present, 1=Present)\")\nplt.legend(['pred_BMA','pred_Logit'])","ba3d70db":"# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == y)\/len(y))\nprint(\"Logit Accuracy: \", np.sum((pred_Logit > 0.5) == y)\/len(y))","405e4e7b":"results = result.summary().sort_values('Probability', ascending=False)\n#results\n\nvar_name = results['Variable Name']\nprob = results['Probability']\n\n# Change fig size\nplt.figure(figsize=(12, 8))\n\n# Create bars\nplt.bar(var_name, prob)\n\n# Create names on the x-axis\nplt.xticks(var_name)\n\n# Show graphic\nplt.show()","282e9bc6":"results = result.summary().sort_values('Avg. Coefficient', ascending=False)\n#results\n\nvar_name = results['Variable Name']\navg_coef = results['Avg. Coefficient']\n\n# Change fig size\nplt.figure(figsize=(12, 8))\n\n# Create bars\nplt.bar(var_name, avg_coef)\n\n# Create names on the x-axis\nplt.xticks(var_name)\n\n# Show graphic\nplt.show()","5e12006d":"df = pd.read_csv('\/kaggle\/input\/coronary-heart-disease\/CHDdata.csv')\ndf[\"famhist\"] = (df[\"famhist\"] == \"Present\")*1 # converts the famhit to 0 (no hist) and 1 (has hist)\n#df = df.drop([\"famhist\"], axis=1)\ndf.head()\n\nX = df.drop([\"chd\"], axis=1)\ny = df[\"chd\"]\nfam_hist = df['famhist']\n\n# Calc mean and std for each column \nstd_scale = pd.DataFrame(columns = ['mean', 'sd'])\nstd_scale['mean'] = X.mean()\nstd_scale['sd'] = X.std()\n#std_scale = std_scale.drop(\"chd\", axis=0)\nprint(std_scale)\n\n","473e9cd7":"# Standard Scale \ndf_scaled = pd.DataFrame(columns = ['sbp', 'tobacco', 'ldl', 'adiposity', 'typea', 'obesity', 'alcohol', 'age']) #'famhist',\ndf_scaled = (X-std_scale['mean'])\/std_scale['sd']\ndf_scaled['famhist'] = fam_hist\n#df_scaled\n\n\n# building the model and fitting the data \nlog_reg = sm.Logit(y, add_constant(df_scaled)).fit()\nlog_reg.summary()","e16cb92c":"result = BMA(y,add_constant(df_scaled), RegType = 'Logit', Verbose=True).fit()","b68df8b2":"result.summary()","b4ece6d8":"results = result.summary().sort_values('Probability', ascending=False)\n#results\n\nvar_name = results['Variable Name']\nprob = results['Probability']\n\n# Change fig size\nplt.figure(figsize=(12, 8))\n\n# Create bars\nplt.bar(var_name, prob)\n\n# Create names on the x-axis\nplt.xticks(var_name)\n\n# Show graphic\nplt.show()","9c50425a":"results = result.summary().sort_values('Avg. Coefficient', ascending=False)\n#results\n\nvar_name = results['Variable Name']\navg_coef = results['Avg. Coefficient']\n\n# Change fig size\nplt.figure(figsize=(12, 8))\n\n# Create bars\nplt.bar(var_name, avg_coef)\n\n# Create names on the x-axis\nplt.xticks(var_name)\n\n# Show graphic\nplt.show()","a0178cf7":"# predict the y-values from training input data\npred_BMA = result.predict(add_constant(df_scaled))\npred_Logit = log_reg.predict(add_constant(df_scaled))","f082dc9f":"# plot the predictions with the actual values\nimport matplotlib.pyplot as plt\nplt.scatter(pred_BMA,y-0.05)\nplt.scatter(pred_Logit,y)\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Coronary Heart Disease \\n(0=Not Present, 1=Present)\")\nplt.legend(['pred_BMA','pred_Logit'])","24e26bb8":"# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == y)\/len(y))\nprint(\"Logit Accuracy: \", np.sum((pred_Logit > 0.5) == y)\/len(y))","3b7a1c8e":"X_train, X_test, y_train, y_test = train_test_split(df_scaled, y, test_size=0.2, random_state=42)\nX_train","f057c89a":"result = BMA(y_train,add_constant(X_train), RegType = 'Logit', Verbose=True).fit()","279331d4":"result.summary()","ed52a4fe":"# predict the y-values from test input data\npred_BMA = result.predict(add_constant(X_test))\npred_Logit = log_reg.predict(add_constant(X_test))","269db038":"# plot the predictions with the actual values\nimport matplotlib.pyplot as plt\nplt.scatter(pred_BMA,y_test-0.05)\nplt.scatter(pred_Logit,y_test)\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Coronary Heart Disease \\n(0=Not Present, 1=Present)\")\nplt.legend(['pred_BMA','pred_Logit'])","fe9195cf":"# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == y_test)\/len(y_test))\nprint(\"Logit Accuracy: \", np.sum((pred_Logit > 0.5) == y_test)\/len(y_test))","9062deee":"### Full Dataset","471194bd":"Load the data, and check the head.","d378a721":"### Proper Subset","f8140323":"Now we split our data into input X dataframe and an output y datafram, and run our BMA analysis.","9407fd1c":"# Bayesian Model Averaging\nHere we define the class that will perform our BMA analysis.\n\nFor any model $M_i$ (each model is defined by the set of predictor varialbes being used in the model), Bayes theorem tells us that the probability for $M_i$ is\n\\begin{equation}\np(M_i|X,y)=\\frac{p(X,y|M_i)p(M_i)}{p(X,y)}.\n\\end{equation}\n\nUsing our previous formulas, this becomes,\n\\begin{equation}\np(M_i|X,y)=\\frac{e^{\u2212\\text{BIC}_i\/2}p(M_i)}{\\sum_k e^{\u2212\\text{BIC}_k\/2}p(M_k)}.\n\\end{equation}\n\nSo far, we have just done Bayesian analysis to compute a posterior probability distribution on the parameters.  But now we can do more with the 'averaging' part of BMA.\n\nThe probability for any predictor variable is the sum of the probabilities for all models contiaining that predictor variable, and the expected value for the coefficient of the predictor variable is the average value of the coefficient over all models containing the variable, weighted by the probability of each model.  That is,\n\\begin{equation}\np(X_k) = \\sum_{M_i \\text{such that } X_k\\in M_i} p(M_i|X,y),\n\\end{equation}\nand\n\\begin{equation}\nE[\\beta_k] = \\sum_{M_i \\text{such that } X_k\\in M_i} p(M_i|X,y)\\times \\beta_k^{(i)},\n\\end{equation}\nwhere $\\beta_k^{(i)}$ is the coefficient of $X_k$ in model $M_i$.","708692a1":"## Part 3\nStandardize all of the numeric, continuous predictors using the mean and standard deviation, and create the previous two charts using the normalized variables. Do the results change?","31d84d20":"# HW 3 Problem 2 \n\n(33) With the CHD dataset(CHDdata.csv) from the previous homework, use Bayeisan model averaging with logistic regression in the Kaggle note-book (https:\/\/www.kaggle.com\/billbasener\/bayesian-model-averaging-logistic-regression) to determine the probability of inclusions of each of the factors.\n\n1. Provide a bar chart using matplotlib.pyplot.bar to show the probabilities for each factor, and sort the factors by probability.\n2. Create a bar chart sowing the model averaged coe\u000ecients for the predictor variables.\n3. Standardize all of the numeric, continuous predictors using the mean and standard deviation, and create the previous two charts using the normalized variables. Do the results change?\n4. Use the BMA (Bayesian Model Averaging) and OLS (Ordinary Least Squares) regression prediction methods at the bottom of the notebook to compare the accuracy of BMA to OLS. (Note that the full dataset is used for training and testing here.) Repeat the comparison using a proper test\/train split and determine the prediction accuracy of BMA vs OLS. Do you see any e\u000bects of regularization when the size of the training set is small?","d40e57ec":"### Results \n\nComparing the probabilities of standardized and non-standardized variables we see no sigificant change in the probabilities of each variable. The coefficients do change, which we expect due to the standardization. Age has a larger coefficient than ldl which is different compared to the non-standardized results. This indicates age has more effect on the model than ldl though both do have an effect.  ","4dfff0a4":"# Bayesian Model Averaging Logistic Regression\n\nIn this notebook we will use Bayesian Model Averaging (BMA) to understand a logistic regression problem.  The data coronary heart disease (0 = does not have CHD, 1 = has CHD), depending on a number of medical predictor variables.\n\nCamille Leonard cvl7qu","c7f8d406":"## Part 4\n\nUse the BMA (Bayesian Model Averaging) and OLS (Ordinary Least Squares) regression prediction methods at the bottom of the notebook to compare the accuracy of BMA to OLS. (Note that the full dataset is used for training and testing here.) Repeat the comparison using a proper test\/train split and determine the prediction accuracy of BMA vs OLS. Do you see any e\u000bects of regularization when the size of the training set is small?","8bda2ffc":"Here is code for a BMA class that will do the Bayeisan Model Averaging.  This is the same as the code from the Bayesian Model Averaging notebook https:\/\/www.kaggle.com\/billbasener\/bayesian-model-averaging-regression-tutorial-pt-2, but with that added capability to do logistic regression via the keyword RegType to \"Logit\".","42aa8111":"## Part 1\nProvide a bar chart using matplotlib.pyplot.bar to show the probabilities for each factor, and sort the factors by probability.","877b22bd":"## Part 2\n Create a bar chart sowing the model averaged coe\u000ecients for the predictor variables.","1d357d53":"### Results \n\nUsing a test-train split improves the accuracy of both the BMA and OLS models. OLS has a higher accuracy than BMA. I believe the reason we are seeing the difference in the performance is due to the small sample size combined with the averaging methods that occur in BMA and perform regularization. The weighted averaging needs a larger sample size to get accurate weighted averages for the coefficients. OLS is not as sensistive to the sample size. "}}