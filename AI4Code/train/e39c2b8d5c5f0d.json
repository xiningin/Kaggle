{"cell_type":{"d684ac8b":"code","12ded9fc":"code","4e826368":"code","ab092bcc":"code","d512c402":"code","e6a40b77":"code","346997c8":"code","05a30b84":"code","edc63740":"code","b214918d":"code","4090906b":"code","ecb1b97b":"code","89de5d2e":"code","5bc626b4":"code","4d4aca06":"code","801a2232":"code","b028197a":"code","6e82339d":"markdown","18ee8fd0":"markdown","f450423a":"markdown","68d11bde":"markdown","ebfe8d8f":"markdown"},"source":{"d684ac8b":"import os\nimport json\nimport gc\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Masking\nfrom tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tqdm import tqdm_notebook as tqdm\nimport fasttext","12ded9fc":"def build_train(train_path, n_rows=300000, sampling_rate=20):\n    with open(train_path) as f:\n        processed_rows = []\n\n        for i in tqdm(range(n_rows)):\n            line = f.readline()\n            if not line:\n                break\n\n            line = json.loads(line)\n\n            text = line['document_text'].split(' ')\n            question = line['question_text']\n            annotations = line['annotations'][0]\n            example_id = line['example_id']\n\n            for i, candidate in enumerate(line['long_answer_candidates']):\n                label = i == annotations['long_answer']['candidate_index']\n\n                start = candidate['start_token']\n                end = candidate['end_token']\n\n                if label or (i % sampling_rate == 0):\n                    processed_rows.append({\n                        'text': \" \".join(text[start:end]),\n                        'is_long_answer': label,\n                        'question': question,\n                        'example_id': example_id,\n                        'annotation_id': annotations['annotation_id']\n                    })\n\n        train = pd.DataFrame(processed_rows)\n        \n        return train","4e826368":"def build_test(test_path):\n    with open(test_path) as f:\n        processed_rows = []\n\n        for line in tqdm(f):\n            line = json.loads(line)\n\n            text = line['document_text'].split(' ')\n            question = line['question_text']\n            example_id = line['example_id']\n\n            for candidate in line['long_answer_candidates']:\n                start = candidate['start_token']\n                end = candidate['end_token']\n\n                processed_rows.append({\n                    'text': \" \".join(text[start:end]),\n                    'question': question,\n                    'example_id': example_id,\n                    'sequence': f'{start}:{end}'\n\n                })\n\n        test = pd.DataFrame(processed_rows)\n    \n    return test","ab092bcc":"directory = '\/kaggle\/input\/tensorflow2-question-answering\/'\ntrain_path = directory + 'simplified-nq-train.jsonl'\ntest_path = directory + 'simplified-nq-test.jsonl'\n\ntrain = build_train(train_path)\ntest = build_test(test_path)","d512c402":"print(train.shape)\nprint(\"Total number of positive labels:\", train.is_long_answer.sum())\nprint(\"Number of anno:\", train.is_long_answer.sum())\ntrain.head()","e6a40b77":"print(test.shape)\ntest.head()","346997c8":"def compute_text_and_questions(train, test, tokenizer):\n    train_text = tokenizer.texts_to_sequences(train.text.values)\n    train_questions = tokenizer.texts_to_sequences(train.question.values)\n    test_text = tokenizer.texts_to_sequences(test.text.values)\n    test_questions = tokenizer.texts_to_sequences(test.question.values)\n    \n    train_text = sequence.pad_sequences(train_text, maxlen=300)\n    train_questions = sequence.pad_sequences(train_questions)\n    test_text = sequence.pad_sequences(test_text, maxlen=300)\n    test_questions = sequence.pad_sequences(test_questions)\n    \n    return train_text, train_questions, test_text, test_questions","05a30b84":"tokenizer = text.Tokenizer(lower=False, num_words=80000)\n\nfor text in tqdm([train.text, test.text, train.question, test.question]):\n    tokenizer.fit_on_texts(text.values)","edc63740":"train_target = train.is_long_answer.astype(int).values","b214918d":"train_text, train_questions, test_text, test_questions = compute_text_and_questions(train, test, tokenizer)\ndel train","4090906b":"def build_embedding_matrix(tokenizer, path):\n    embedding_matrix = np.zeros((tokenizer.num_words + 1, 300))\n    ft_model = fasttext.load_model(path)\n\n    for word, i in tokenizer.word_index.items():\n        if i >= tokenizer.num_words - 1:\n            break\n        embedding_matrix[i] = ft_model.get_word_vector(word)\n    \n    return embedding_matrix","ecb1b97b":"def build_model(embedding_matrix):\n    embedding = Embedding(\n        *embedding_matrix.shape, \n        weights=[embedding_matrix], \n        trainable=False, \n        mask_zero=True\n    )\n    \n    q_in = Input(shape=(None,))\n    q = embedding(q_in)\n    q = SpatialDropout1D(0.2)(q)\n    q = Bidirectional(LSTM(100, return_sequences=True))(q)\n    q = GlobalMaxPooling1D()(q)\n    \n    \n    t_in = Input(shape=(None,))\n    t = embedding(t_in)\n    t = SpatialDropout1D(0.2)(t)\n    t = Bidirectional(LSTM(150, return_sequences=True))(t)\n    t = GlobalMaxPooling1D()(t)\n    \n    hidden = concatenate([q, t])\n    hidden = Dense(300, activation='relu')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(300, activation='relu')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    \n    out1 = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[t_in, q_in], outputs=out1)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","89de5d2e":"path = '\/kaggle\/input\/fasttext-crawl-300d-2m-with-subword\/crawl-300d-2m-subword\/crawl-300d-2M-subword.bin'\nembedding_matrix = build_embedding_matrix(tokenizer, path)","5bc626b4":"model = build_model(embedding_matrix)\nmodel.summary()","4d4aca06":"train_history = model.fit(\n    [train_text, train_questions], \n    train_target,\n    epochs=5,\n    validation_split=0.2,\n    batch_size=1024\n)","801a2232":"# saving\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","b028197a":"model.save('model.h5')","6e82339d":"# About This Kernel\n\nThis is the training kernel for the [Tensorflow 2.0 Question Answering](https:\/\/www.kaggle.com\/c\/tensorflow2-question-answering) Competition. I also created [an inference kernel](https:\/\/www.kaggle.com\/xhlulu\/tf2-qa-lstm-inference-kernel\/) that shows how to use the trained model to make a submission.\n\nMy approach is heavily inspired from [Oleg's kernel](https:\/\/www.kaggle.com\/opanichev\/tf2-0-qa-binary-classification-baseline), i.e. I break down each document into parts corresponding to each of the candidate long answers. Then, I label each of the long answer to be `1` if it is the true long answer, or `0` if not. For each of row, I also include the question and the `example_id`. Then, I train a LSTM to predict that label.\n\nThe sections are broken down in the following way:\n\n1. **Build Dataset:** Two different utility functions for creating ready-to-use dataframes for both training and testing data. The `build_train` function only loads a subset of all training data, and set a sampling rate `S`, such that we only keep `1\/S` of all the negative-labelled data (and keep all positive-labelled data).\n\n2. **Preprocessing:** Train a keras `Tokenizer` to encode the text and questions into list of integers (tokenization), then pad them to a fixed length to form a single numpy array for text and one for questions.\n\n3. **Modelling:**\n    * Generate a fasttext embedding (directly using [FAIR's Official Python API](https:\/\/github.com\/facebookresearch\/fastText\/tree\/master\/python)) based on the index of the tokenizer. \n    * Build two 2-layer bidirectional LSTM; one to read the questions, and one to read the text. \n    * Concatenate the output of the LSTM and feed in 2-layer fully-connected neural networks.\n    * Predict the binary output using Sigmoid activation.\n    * Optimize using Adam and binary cross-entropy loss.\n\n4. **Save Model:** Due to the submission time limit, it is better to import the model we just trained in a separate kernel to infer and submit. In the inference kernel, we first remove all the rows with less than 0.5 confidence, then for each `example_id` we only keep the one with the highest confidence to be the output.","18ee8fd0":"# Preprocessing","f450423a":"# Build Dataset","68d11bde":"# Modelling","ebfe8d8f":"# Save Model"}}