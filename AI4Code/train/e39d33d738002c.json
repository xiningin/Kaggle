{"cell_type":{"1ca61b4c":"code","dd28f7b0":"code","033beb3a":"code","a9f663ed":"code","23515e11":"code","459b88e9":"code","af9ca073":"code","fe192571":"code","1913f540":"code","455560c3":"code","ef9138d1":"code","579c35e4":"code","68ad069c":"code","37ed01c6":"code","9fa5f626":"code","4f297278":"code","dbc685ec":"code","6bb60a1a":"code","a61644da":"code","c5dbcfb5":"code","c60d9d57":"code","314ee28c":"code","fe8b12a8":"code","0c7db1db":"code","5f483466":"code","9226ed3c":"markdown","3c53b949":"markdown","87002134":"markdown","a02a65db":"markdown","476d374a":"markdown","dbce759c":"markdown","a8e07ed2":"markdown","0da9d491":"markdown","ae3126f4":"markdown","a5d53cac":"markdown","49922867":"markdown","44c598cc":"markdown","bd0b4df7":"markdown","4ece7b41":"markdown","03dd964a":"markdown","3cf3339f":"markdown","a6fc0a40":"markdown","23835b0b":"markdown","cdd2364e":"markdown","67b257bd":"markdown","d3b59412":"markdown","262f99eb":"markdown","283f5ac1":"markdown","5de3c039":"markdown","27e9709a":"markdown","eccf4dbd":"markdown","5aad5f00":"markdown","2e3ce5f7":"markdown"},"source":{"1ca61b4c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dd28f7b0":"import pandas as pd\nimport numpy as np","033beb3a":"f = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv', delimiter = ',')\nt = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv', delimiter = ',')","a9f663ed":"f.head()","23515e11":"t.head()","459b88e9":"f['temp']= 0\nt['temp']= 1\n\ndatas = pd.DataFrame()\ndatas = t.append(f)","af9ca073":"datas.head()","fe192571":"print(datas.shape)","1913f540":"column = ['date','subject']\ndatas = datas.drop(columns=column)","455560c3":"print(datas.shape)","ef9138d1":"datas.info()","579c35e4":"input_arr=np.array(datas['title'])","68ad069c":"import re\nimport nltk\nnltk.download('stopwords')","37ed01c6":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\ncorpus = []\n\nfor i in range(0, 40000):\n    newArr = re.sub('[^a-zA-Z]', ' ', input_arr[i])\n    #The sub() function replaces the matches with the text of your choice,\n    #in this case \"[a-zA-Z]\" is getting replaced with blank space\n    #input_arr[i] is the array of 'title' column.\n\n    #[a-zA-Z]: Returns a match for any character alphabetically between a and z, lower case OR upper case\n\n    newArr = newArr.lower()\n    #Converting into lowercase\n\n    newArr = newArr.split()\n    #The split() method splits a string into a list.\n\n    ps = PorterStemmer()\n    newArr = [ps.stem(word) for word in newArr if not word in set(stopwords.words('english'))]\n    #ps.stem(word) is stemming the words\n    #the word will be considered if the word is not a stopword\n    #set(stopwords.words('english')= checks any kind of stopwords in English language\n\n    newArr = ' '.join(newArr)\n    #joins the string with blank spaces\n\n    corpus.append(newArr)\n    #adding strings into the corpus list","9fa5f626":"datas","4f297278":"from sklearn.feature_extraction.text import CountVectorizer\ncountv = CountVectorizer(max_features = 5000)\n#max_features: The CountVectorizer will select the\n#words\/features\/terms which occur the most frequently.\n#It takes absolute values so if you set the \u2018max_features = 3\u2019,\n#it will select the 3 most common words in the data.\n\nX = countv.fit_transform(corpus).toarray()\ny = datas.iloc[0:40000, 2].values","dbc685ec":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","6bb60a1a":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","a61644da":"y_pred = classifier.predict(X_test)","c5dbcfb5":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","c60d9d57":"print(cm)","314ee28c":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","fe8b12a8":"from sklearn.metrics import accuracy_score\nprint(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))","0c7db1db":"import seaborn as sns\nimport matplotlib.pyplot as plt","5f483466":"new_cm = pd.DataFrame(cm , index = ['Fake','Not Fake'] , columns = ['Fake','Not Fake'])\nsns.heatmap(new_cm,cmap= 'Blues', annot = True, fmt='',xticklabels = ['Fake','Not Fake'], yticklabels = ['Fake','Not Fake'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title('Confusion matrix On Test Data')\nplt.show()","9226ed3c":"**Observing the top 5 rows of fake and true data**","3c53b949":"*Downloading Stopwords*","87002134":"**Using nltk and importing Stopwords(For dealing with stopwords) and PortStemmer(For stemming)**","a02a65db":"# Thanks A Lot For Your Time ! Please Upvote if you Like it.\n\n## PS: It took alot of time to explain each line of code :\")\n\n<p align=\"center\">\n  <img width=\"460\" height=\"300\" src=\"https:\/\/media.giphy.com\/media\/36otikFtpDJq8\/giphy.gif\">\n<\/p>","476d374a":"**Created array of 'title' column as input_array for preprocessing**","dbce759c":"**Fitting Logistic Regression to the Training set**","a8e07ed2":"**Splitting the dataset into the Training set and Test set in 80-20 ratio**","0da9d491":"**Exploring the data**","ae3126f4":"**Getting more details**","a5d53cac":"# Natural Language Processing","49922867":"***CountVectorizer*** : *CountVectorizer is a tool provided by the scikit-learn library. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.* \nhttps:\/\/www.geeksforgeeks.org\/using-countvectorizer-to-extracting-features-from-text\/","44c598cc":"# Modelling","bd0b4df7":"**Making the Confusion Matrix**","4ece7b41":"**Predicting the Test set results**","03dd964a":"**Confusion matrix in a well plotted chart**","3cf3339f":"TP \u2013 True Positives\nTN - True Negatives\nFP \u2013 False Positives\nFN \u2013 False Negatives\n\nPrecision: Accuracy of positive predictions.\nPrecision = TP\/(TP + FP)\n\nRecall: Fraction of positives that were correctly identified.\nRecall = TP\/(TP+FN)\n\nF1 score \u2013 Percent of positive predictions were correct.\nF1 Score = 2*(Recall * Precision) \/ (Recall + Precision)","a6fc0a40":"**What Is Fake News ?**\n\nFalse news, also known as unwanted news, false stories, misconceptions or fraudulent stories, types of stories that contain deliberate information or frauds that are spread through traditional media (print and broadcast) or online media. Digital news has revived and increased the use of fake news, or yellow journalism. These stories are often referred to as information that is not in the media but sometimes finds its way into the mainstream media as well.\n\n\n*   It causes panic\n*   Damaging the reputation of public and private organizations\n*   It deceives the people, for the benefit of the deceivers\n*   Motivated by a personal vendetta, some people support such things.\n\n<br>\n<br>\n\n**PROBLEM :** \n<br>\nHow to distinguish between a real news and a fake news?\n\n<br>\n<br>\n\n**SOLUTION :**\n\nWe can show an algorithm that has a large number of false and real news stories to learn to distinguish between them automatically, and then give a possible score or percentage of confidence such as a given news release is true or false.","23835b0b":"**Observing the top 5 rows of dataset**","cdd2364e":"**Column 'Date' and 'Subject' are important to Descriptive analysis but here for prediction they are less important so dropping these columns.**","67b257bd":"Now combining these two dataset to one dataset to simplify processing.\n\nAlso to combine we need to add an extra column as 'temp' to differtiate news as 1=true_news 0=fake_news","d3b59412":"**Cheking the dimensions after dropping**","262f99eb":"***Stopwords***: *A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.*\nhttps:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n\n***PortStemmer***: *A PortStemmer is an algorithm used for removing the commoner morphological and inflexional endings from words in English.  For example: words such as \u201cLikes\u201d, \u201dliked\u201d, \u201dlikely\u201d and \u201dliking\u201d will be reduced to \u201clike\u201d after stemming.*\nhttps:\/\/www.geeksforgeeks.org\/python-stemming-words-with-nltk\/","283f5ac1":"# Exploratory Data Analysis","5de3c039":"# Model Evaluation","27e9709a":"**Loading The Dataset**","eccf4dbd":"**Checking the dimensions**","5aad5f00":"X=independent_variable(title)\n<\/br> y=dependent_variable(sentiment 0 or 1)\n\n<\/br>\nBasically what I did here is created a bag of words (https:\/\/machinelearningmastery.com\/gentle-introduction-bag-words-model\/)i.e, X will have top 5000 most common words in array form","2e3ce5f7":"**Getting Accuracy**"}}