{"cell_type":{"8cd3a72e":"code","4335a932":"code","cdc20e19":"code","999b31ca":"code","720d91a5":"code","165d883d":"code","7f2fd5f7":"code","3f655323":"code","595c9a8e":"code","a14dca5f":"code","3cf2a289":"code","4beb4444":"code","2c1eb1d3":"code","10d7fa21":"code","441e89a5":"code","ff1311b6":"code","5ad91aa9":"code","e983e656":"code","f307d9f2":"markdown","fa94fdd8":"markdown","613645b3":"markdown","6b38fd8a":"markdown","1f42aaac":"markdown","9b07030b":"markdown","b79e3686":"markdown","e4c57020":"markdown","e9fe60b1":"markdown","055fe414":"markdown"},"source":{"8cd3a72e":"import warnings\nwarnings.filterwarnings(\"ignore\")","4335a932":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 20, 6\nrcParams['axes.grid'] = True","cdc20e19":"data = pd.read_csv('..\/input\/delhi-weather-data\/testset.csv')\nprint(data.shape)\ndata.head()","999b31ca":"data['datetime_utc'] = pd.to_datetime(data['datetime_utc'])\ndata.set_index('datetime_utc', inplace= True)\ndata =data.resample('H').mean()\ndata = data[[' _tempm' ]]\ndata.columns = ['Temp']\nprint(data.shape)\ndata.head()","720d91a5":"fig, ax = plt.subplots()\nsns.heatmap(data.isnull(), cbar=False, yticklabels=False)\nfig.suptitle('Missing Values', fontsize=18);","165d883d":"data.interpolate(method='time', inplace=True)\nax = data.plot()\nax.set_title('Temperature Time Series', fontsize=18)\nax.set_xlabel('');","7f2fd5f7":"from statsmodels.tsa.seasonal import seasonal_decompose\nseasonal_decompose(data['Temp']).plot();","3f655323":"train = data.iloc[:-(24*7)]\ntest = data.iloc[-(24*7):]","595c9a8e":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(train)\nscaled_train = scaler.transform(train)\nscaled_test = scaler.transform(test)","a14dca5f":"from keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import SimpleRNN, LSTM, GRU, Dense \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.utils import plot_model","3cf2a289":"n_input = 24*7\nn_feature = 1\n\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=32)","4beb4444":"es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nrlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)","2c1eb1d3":"model = Sequential()\n\nmodel.add(LSTM(64, input_shape= (n_input, n_feature), return_sequences=True))\nmodel.add(LSTM(32, return_sequences=True))\nmodel.add(LSTM(16, return_sequences=True))\nmodel.add(LSTM(8, return_sequences=False))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nmodel.summary()\nplot_model(model, show_shapes=True)","10d7fa21":"history = model.fit_generator(train_generator, callbacks=[es, ckpt, rlp], epochs=100)\npd.DataFrame(history.history)[['loss']].plot();","441e89a5":"model = load_model('model.h5')\n\ntest_predictions = []\ncurrent_batch = scaled_train[-n_input:].reshape((1,n_input,n_feature))\n\nfor i in range(len(test)):\n    current_pred = model.predict(current_batch)[0]\n    test_predictions.append(current_pred)\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]], axis= 1)","ff1311b6":"test['Predictions'] = scaler.inverse_transform(test_predictions)\ntest.plot();","5ad91aa9":"test['Residuals'] = test['Temp'] - test['Predictions']\nax = test['Residuals'].plot()\nax.set_title('Residuals', fontsize=18);","e983e656":"fig, ax = plt.subplots()\nax.plot(test['Temp'].values, test['Residuals'].values, '.')\nax.set_title('Residual Plot', fontsize=18)\nax.legend();","f307d9f2":"An important constructor argument for all keras RNN layers is the return_sequences argument. This setting can configure the layer in one of two ways.\n\n* If False, the default, the layer only returns the output of the final timestep, giving the model time to warm up its internal state before making a single prediction:\n\n![](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/images\/lstm_1_window.png)\n\n* If True the layer returns an output for each input. This is useful for:\n  * Stacking RNN layers.\n  * Training a model on multiple timesteps simultaneously.\n\n\n![](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/images\/lstm_many_window.png)","fa94fdd8":"## Resampling (Hourly)","613645b3":"## Normalize the data\nIt is important to scale features before training a neural network. Normalization is a common way of doing this scaling. Subtract the mean and divide by the standard deviation of each feature.\n\nThe mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.\n\nIt's also arguable that the model shouldn't have access to future values in the training set when training, and that this normalization should be done using moving averages.","6b38fd8a":"# Recurrent neural network\nA Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time-step to time-step.","1f42aaac":"## Missing Treatment","9b07030b":"## Seasonal Decomposition","b79e3686":"Although we used one week data as test set, but we would like to predict only one day into future, therefore from that perspective our model is performing very well","e4c57020":"# Dataset\n\nThis dataset contains weather data for New Delhi, India. It was taken out from wunderground with the help of their easy to use api. It contains various features such as temperature, pressure, humidity, rain, precipitation,etc.\nThe main target is to develop a prediction model accurate enough for predicting the weather. We can try something like predicting the weather in the next 24 hours","e9fe60b1":"# Splitting the data\n\nWe will train the model on data till 2015 and will evaluate the model on the data for 2016","055fe414":"# Model Evaluation"}}