{"cell_type":{"2f5d2c11":"code","f44619bf":"code","8c8266b1":"code","10edfbb5":"code","a2b22956":"code","5b943ea4":"code","c7ae711a":"code","3dce2dfb":"code","dfd1c03d":"code","327021fc":"code","af1ec2da":"code","a2cf6bd6":"code","9dc79051":"code","7c3e7789":"code","6d2b5cfa":"code","bfc331f9":"code","bbd0df3a":"code","142f4231":"code","a0c34c93":"code","39b06065":"code","4a71eb8d":"code","fbf9e4c5":"code","ceb06a2a":"code","f5e84f4b":"code","f18c6ac1":"code","c91c17ce":"code","e0278836":"code","57c7b512":"code","8748a4e2":"code","261471c1":"code","74942e5a":"code","04504456":"code","e039d361":"code","eef0013a":"code","88b3d1b7":"code","82aabd48":"code","a169c955":"code","8a126a5c":"code","ba46c614":"code","f73cb871":"code","ee1a063a":"code","9a4f6f40":"code","c974626f":"code","7db21313":"code","7fa6aa72":"code","9835df99":"code","f96f4359":"code","0e4f6aab":"code","fb29c297":"code","73083206":"code","52572658":"code","51212afc":"code","bf936e49":"code","224064d8":"code","67dff43e":"code","52527ee2":"code","32289fbf":"code","ef79390e":"code","ac25a399":"code","1196e771":"code","dfdf8141":"code","9326173b":"code","cb81b9c6":"code","aeae4db2":"code","d282eb9d":"code","08eb000a":"code","5c31d546":"code","6d8cba07":"code","1bc54536":"code","abb856c5":"code","ec1d22c0":"code","ce641a99":"code","ce29cb7e":"code","ff2e18f9":"code","6e2a892d":"code","ef057dae":"code","1abc0f6d":"code","43d54b83":"code","48477bb6":"code","f338f28c":"code","adaf5646":"markdown","a169770f":"markdown","fe25cd5f":"markdown","8f64254b":"markdown","ae48193a":"markdown","69252991":"markdown","e8669ec9":"markdown","829e0358":"markdown","29c95195":"markdown","10882293":"markdown","90d6c8cd":"markdown","4e72bd47":"markdown","9f9b0f74":"markdown","57f1b5ee":"markdown","0c495340":"markdown"},"source":{"2f5d2c11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f44619bf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split  \nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","8c8266b1":"train = pd.read_csv(\"..\/input\/black-friday\/train.csv\")\ntrain.head(10)","10edfbb5":"train[\"Product_Category_1\"].isna().sum(), train[\"Product_Category_2\"].isna().sum(), train[\"Product_Category_3\"].isna().sum()","a2b22956":"train[\"Product_Category_3\"].isna().sum()\/550068*100, train[\"Product_Category_2\"].isna().sum()\/550068*100, train[\"Product_Category_1\"].isna().sum()\/550068*100","5b943ea4":"### it looks like product category 3 has more null values which is close to 70 percent of the data, so we delete the feature.\n### keep product category 2 and 1.","c7ae711a":"train.drop([\"Product_Category_3\"],  axis=1, inplace=True)","3dce2dfb":"train.head(10)","dfd1c03d":"train[\"Product_Category_2\"].fillna(train[\"Product_Category_2\"].median(), inplace = True)","327021fc":"train[\"Product_Category_2\"].isna().sum(), train[\"Product_Category_1\"].isna().sum()","af1ec2da":"train.Age.unique()","a2cf6bd6":"Label_Encoder = LabelEncoder()\n\ntrain[\"Gender\"] = Label_Encoder.fit_transform(train[\"Gender\"])\ntrain[\"City_Category\"] = Label_Encoder.fit_transform(train[\"City_Category\"])\ntrain[\"Age_Category\"] = Label_Encoder.fit_transform(train[\"Age\"]) #then we drop the Age variable. lets delete after visualization","9dc79051":"# # we can also use fn to create a category of the above variables \n\n# def Age_Category(Age):\n#     if Age == '0-17':\n#         return 1\n#     elif Age == '18-25':\n#         return 2\n#     elif Age == '26-35':\n#         return 3\n#     elif Age == '36-45':\n#         return 4\n#     elif Age == '46-50':\n#         return 5\n#     elif Age == '51-55':\n#         return 6\n#     else :\n#         return 7\n# train[\"Age_Category\"] = train[\"Age\"].apply(Age_Category)        # now we can delete Age .. but let us do that after visualization","7c3e7789":"train.head()","6d2b5cfa":"train.Gender.unique(), train.Age.unique(),   train.City_Category.unique() \n\n    # make sure no missing values    \n    # also make sure 0 is for gender and 1 is form menA=0, B = 1, C = 2 , ","bfc331f9":"train.head(5)     ","bbd0df3a":"sns.boxplot(x =train[\"Gender\"], y = train[\"Purchase\"])","142f4231":"sns.boxplot(x =train[\"City_Category\"], y = train[\"Purchase\"], hue=train[\"Gender\"])","a0c34c93":"fig, ax = plt.subplots(figsize = (12,8))\n    \nax = sns.boxplot(x =train[\"Age\"], y = train[\"Purchase\"], hue= train[\"Gender\"], palette='bright')","39b06065":"train.head(10)","4a71eb8d":"sns.violinplot(\"Marital_Status\", y=\"Purchase\", data = train)","fbf9e4c5":"fig = plt.subplots(figsize = (12, 8))\nsns.boxplot(x=\"Occupation\", y = \"Purchase\", hue=\"Gender\", data = train)","ceb06a2a":"train[\"Marital_Status\"].value_counts()","f5e84f4b":"fig = plt.subplots(figsize= (12,8))\ntrain[\"Purchase\"].value_counts().hist()","f18c6ac1":"# no significnt change in purchasing behavior between married and non-married.","c91c17ce":"train.Purchase[1:100].sort_values(ascending = False)","e0278836":"train.head(2)","57c7b512":"forget = train[[\"User_ID\", \"Product_ID\", \"Age\"]]\n\ntrain.drop(forget, axis =1, inplace = True )","8748a4e2":"train.head()","261471c1":"train.Stay_In_Current_City_Years.unique()","74942e5a":"def stay(Stay_In_Current_City_Years):\n        if Stay_In_Current_City_Years == '4+':\n            return 4\n        else:\n            return Stay_In_Current_City_Years\ntrain['Stay_In_Current_City_Years'] = train['Stay_In_Current_City_Years'].apply(stay).astype(int)    ","04504456":"train.dtypes","e039d361":"fig, ax = plt.subplots(figsize = (15,8))\nax = sns.heatmap(train.corr(), annot=True, cmap=\"YlGnBu\" )","eef0013a":"X = train.drop(\"Purchase\", axis=1)\ny = train[\"Purchase\"]","88b3d1b7":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.2)","82aabd48":"train.dtypes","a169c955":"#model\n%time\n#cutting down on the max number of samples each estimator can see improves training time\n\n# rf_regressor = RandomForestRegressor(n_estimators=100)\n\n\nrf_regressor = RandomForestRegressor(n_jobs=-1, \n                              random_state=42)\n\nrf_regressor.fit(X_train, y_train)","8a126a5c":"rf_regressor.score(X_test, y_test)","ba46c614":"y_pred = rf_regressor.predict(X_test)","f73cb871":"r2 = r2_score(y_test, y_pred)\nr2","ee1a063a":"MAE = mean_absolute_error(y_test, y_pred)\nMAE","9a4f6f40":"MSE = mean_squared_error(y_test, y_pred)\nMSE","c974626f":"fig, ax = plt.subplots(figsize=(12,5))\nax = plt.scatter(y_test, y_pred, c=\"blue\")","7db21313":"# 1. RandomSearchCV \n\ngrid =  {\"n_estimators\": [10,50,100],\n       \"max_depth\": [None,10,20,30,40,50,],\n       \"max_features\": [\"auto\", \"sqrt\"],\n       \"min_samples_leaf\": [2,10,15],\n       \"min_samples_split\": [2,5,20]}\n","7fa6aa72":"randomsearchCV = RandomizedSearchCV(rf_regressor, param_distributions = grid, n_iter = 5, cv=5,  verbose = True, n_jobs=2 )\n     \n         #\"Verbose is a general programming term for produce lots of logging output. You can think of it as asking the program to \"tell me everything about what you are doing all the time\". \n          #Just set it to true and see what happens.\"\n            \n            \n        # if you specify n_jobs to -1, it will use all cores in the CPU (100% CPU). \n          #If it is set to 1 or 2, it will use one or two cores only ","9835df99":"%time\n\nrandomsearchCV.fit(X_train, y_train)\n","f96f4359":"randomsearchCV.best_params_","0e4f6aab":"rf_regressor_tune = RandomForestRegressor(n_estimators=100, max_depth = 40, max_features = 'auto', min_samples_leaf =10,\n                                     min_samples_split=2 )","fb29c297":"rf_regressor_tune.fit(X_train, y_train) ","73083206":"y_pred_tune = rf_regressor_tune.predict(X_test)\ny_pred_tune","52572658":"# rf_regressor.score(X_test, y_test)\n\nr2_tune = r2_score(y_test, y_pred_tune)  \nr2_tune","51212afc":"MAE_tune = mean_absolute_error(y_test, y_pred_tune)\nMAE_tune","bf936e49":"MSE_tune = mean_squared_error(y_test, y_pred_tune)\nMSE_tune","224064d8":"# compare prediction before and after Tunning\n\ncompare = {\"R^2_score\":[r2_tune, r2],\n            \"Mean Squared Error\": [MSE_tune, MSE],\n            \"Mean Absolute Error\": [MAE_tune, MAE]}\n\ncompare","67dff43e":"Compare = pd.DataFrame(compare, index=[[\"After_tune\", \"Before Tune\"]])\nCompare","52527ee2":"fig, ax = plt.subplots(figsize=(12,5))\nax = plt.scatter(y_test, y_pred_tune, c=\"blue\")","32289fbf":"test = pd.read_csv(\"..\/input\/black-friday\/test.csv\")","ef79390e":"test.head()","ac25a399":"test[\"Product_Category_1\"].isna().sum(), test[\"Product_Category_2\"].isna().sum(), test[\"Product_Category_3\"].isna().sum()","1196e771":"### Lets do same data preprocessing as we do in the train data\ntest[\"Product_Category_1\"].isna().sum()\/550068*100, test[\"Product_Category_2\"].isna().sum()\/550068*100, test[\"Product_Category_3\"].isna().sum()\/550068*100","dfdf8141":"test[\"Product_Category_2\"].fillna(test[\"Product_Category_2\"].median(), inplace = True)\ntest[\"Product_Category_3\"].fillna(test[\"Product_Category_3\"].median(), inplace = True)","9326173b":"test.head()","cb81b9c6":"Label_Encoder = LabelEncoder()\n\n\ntest[\"Gender\"] = Label_Encoder.fit_transform(test[\"Gender\"])\ntest[\"City_Category\"] = Label_Encoder.fit_transform(test[\"City_Category\"])\ntest[\"Age_Category\"] = Label_Encoder.fit_transform(test[\"Age\"])","aeae4db2":"test[\"Stay_In_Current_City_Years\"].unique()","d282eb9d":"def stay(Stay_In_Current_City_Years):\n        if Stay_In_Current_City_Years == '4+':\n            return 4\n        else:\n            return Stay_In_Current_City_Years\ntest['Stay_In_Current_City_Years'] = test['Stay_In_Current_City_Years'].apply(stay).astype(int) ","08eb000a":"forget = test[[\"User_ID\", \"Product_ID\", \"Age\"]]\n\ntest.drop(forget, axis =1, inplace = True )","5c31d546":"test.head()","6d8cba07":"test.drop(\"Product_Category_3\", axis = 1, inplace = True)","1bc54536":"test_pred = rf_regressor_tune.predict(test)\ntest_pred","abb856c5":"test.columns","ec1d22c0":"len(y_pred)","ce641a99":"# test = pd.read_csv(\"..\/input\/black-friday\/test.csv\")","ce29cb7e":"Prediction = pd.DataFrame()\nPrediction[\"User_ID\"] = test[\"User_ID\"]\nPrediction[\"Purchase Prediction\"] = test_pred","ff2e18f9":"Prediction","6e2a892d":"print(rf_regressor_tune.feature_importances_)","ef057dae":"test.head(1)","1abc0f6d":"# test.drop([\"User_ID\", \"Product_ID\", \"Age\"], axis = 1, inplace=True)","43d54b83":"columns = pd.DataFrame({\"Features\": test.columns, \n                        \"Feature Importance\" :rf_regressor_tune.feature_importances_})","48477bb6":"columns.sort_values(\"Feature Importance\", ascending = False).reset_index(drop=True)","f338f28c":"sns.barplot(y=\"Features\", x = \"Feature Importance\", data = columns)","adaf5646":"Product_Category_1 feature has by far the highest regression coefficient and is very important feature in explaining, positively, why people purchase in mass on Black Friday.","a169770f":"## Modelling","fe25cd5f":"## Hyperparameter tuning  ","8f64254b":"## 3. Evaluation\n  * We use three evaluation matrices: Mean Squared error (MSE), Mean Absolute Error(MAE), and R^2 \/ coeeficient of determination.","ae48193a":"## 1. Problem\n\n   * We have a regression prblem: to Predict Black-Friday purchase.","69252991":"#### In all the three cases our model performed good while tuning hyperparameters: We have got higher R^2, and lower MSE & MAE compared to same values before tuning the hyperparameters","e8669ec9":"## 4. Modelling\n  *Since we have a regression problem we are going to apply a regression model in this case Random Forest Regressor (you can check the cheat sheet for deatil model selection)","829e0358":"## Make predictions with the model whose hyperparameter are tuned\n\nrf_regressor_tune.predict(test)\n\n    We get the ff error message: \n         \"3ValueError: Number of features of the model must match the input. Model n_features is 8 and input n_features is 9 \"\n    it is because we have Product_category_3 dropped in train data but retained here. so we need to handle this \n    ","29c95195":"## 2. Data\n  * Dataset comprises of sales transactions captured at a retail store. It\u2019s a classic dataset to explore and expand your feature engineering skills and day to day understanding from multiple shopping experiences. \nThis is a regression problem. \nThe dataset has 550,069 rows and 12 columns.\n\n\n\nSource: https:\/\/datahack.analyticsvidhya.com\/contest\/black-friday\/","10882293":"## 5. Model improvement \n  * we apply hyperparameter tuning to improve our model, we use RandomSearchCV accordingly.","90d6c8cd":"## Prediction and metrics ","4e72bd47":"Optimizing hyperparameters for machine learning models is a key step in making accurate predictions. Hyperparameters define characteristics of the model that can impact model accuracy and computational efficiency. They are typically set prior to fitting the model to the data. In contrast, parameters are values estimated during the training process that allow the model to fit the data. Hyperparameters are often optimized through trial and error; multiple models are fit with a variety of hyperparameter values, and their performance is compared.","9f9b0f74":"Cross-validation is often used to determine the optimal values for hyperparameters; we want to identify a model structure that performs the best on records it has not been trained on. A variety of hyperparameter values should be considered. For example, below are some candidate hyperparameters.\n![image.png](attachment:image.png)\n\nsource: https:\/\/medium.com\/@ODSC\/optimizing-hyperparameters-for-random-forest-algorithms-in-scikit-learn-d60b7aa07ead","57f1b5ee":"### Test data","0c495340":"fiting the data with the best hyperparamters"}}