{"cell_type":{"f7be8622":"code","3f280347":"code","9204658e":"code","a9185bb1":"code","b512c4c3":"code","b0fb1cb7":"code","4c159c18":"code","26520c55":"code","b107ef3e":"code","7e32a2f5":"code","04d93c22":"code","690ab17a":"code","ae0bc4a9":"code","2fea6c20":"code","ff6cc3a1":"code","2452cafe":"code","d9e86470":"code","482ba274":"code","9d293ef8":"code","037c8c6c":"code","e4ba97e5":"code","76f6ab54":"code","8cb7bbac":"code","32f22870":"code","967dc9ed":"code","f8d57421":"code","3abbda13":"code","4dc32036":"code","25ba4b65":"code","7cad9beb":"code","6151f889":"code","45826447":"code","a693d83f":"markdown","52802ae6":"markdown","64faf0b4":"markdown","b9148b96":"markdown"},"source":{"f7be8622":"import math, re, os\nimport numpy as np\nimport tensorflow as tf\n\n\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras import Sequential\nfrom keras.layers import *\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport seaborn as sns\ntf.random.set_seed(20)\nnp.random.seed(20)\nprint(\"Tensorflow version \" + tf.__version__)","3f280347":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","9204658e":"from kaggle_datasets import KaggleDatasets\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\nprint(GCS_DS_PATH) # what do gcs paths look like?","a9185bb1":"\nIMAGE_SIZE = [192, 192]\nGCS_PATH = GCS_DS_PATH + '\/tfrecords-jpeg-192x192'\nAUTO = tf.data.experimental.AUTOTUNE\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/val\/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test\/*.tfrec') \n\nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","b512c4c3":"TRAINING_FILENAMES","b0fb1cb7":"mydataset = load_dataset(TRAINING_FILENAMES, labeled=True)\nmydataset","4c159c18":"only10classes = ['pink primrose', \n                 'snapdragon', \n                 'purple coneflower', \n                 'king protea', \n                 'wild geranium',    \n                 'tiger lily',         \n                'peruvian lily',   \n                 'bird of paradise',\n                 'monkshood',        \n                 'globe thistle', \n           ]     \nlen(only10classes)","26520c55":"def display_20_image(images):\n    \n    w = 10\n    h = 10\n    fig = plt.figure(figsize=(9, 13))\n    columns = 4\n    rows = 5\n    # prep (x,y) for extra plotting\n    xs = np.linspace(0, 2*np.pi, 60)  # from 0 to 2pi\n    ys = np.abs(np.sin(xs))           # absolute of sine\n    ax = []\n\n    for i in range(columns*rows):\n        img = np.random.randint(10, size=(h,w))\n        ax.append( fig.add_subplot(rows, columns, i+1) )\n        plt.imshow(images[i].reshape(images[i].shape))\n\n    # do extra plots on selected axes\/subplots\n    # note: index starts with 0\n    ax[2].plot(xs, 3*ys)\n    ax[19].plot(ys**2, xs)\n\n    plt.show()  # finally, render the plot\n","b107ef3e":"mydataset","7e32a2f5":"\nclass0images = []\nclass1images = []\nclass2images = []\nclass3images = []\nclass4images = []\nclass5images = []\nclass6images = []\nclass7images = []\nclass8images = []\nclass9images = []\n\nfor images, labels in mydataset:  # only take first element of dataset\n    #print(images.numpy().shape)\n    if int(labels.numpy()) == 0:\n        class0images.append(images.numpy())\n    if int(labels.numpy()) == 10:\n        class1images.append(images.numpy())\n    if int(labels.numpy()) == 16:\n        class2images.append(images.numpy())\n    if int(labels.numpy()) == 12:\n        class3images.append(images.numpy())\n    if int(labels.numpy()) == 4:\n        class4images.append(images.numpy())\n    if int(labels.numpy()) == 5:\n        class5images.append(images.numpy())\n    if int(labels.numpy()) == 17:\n        class6images.append(images.numpy())\n    if int(labels.numpy()) == 7:\n        class7images.append(images.numpy())\n    if int(labels.numpy()) == 8:\n        class8images.append(images.numpy())\n    if int(labels.numpy()) == 9:\n        class9images.append(images.numpy())\nprint(\"done\")","04d93c22":"print(len(class0images))\nprint(len(class1images))\nprint(len(class2images))\nprint(len(class3images))\nprint(len(class4images))\nprint(len(class5images))\nprint(len(class6images))\nprint(len(class7images))\nprint(len(class8images))\nprint(len(class9images))\n","690ab17a":"train=np.concatenate([class0images[:50],class1images[:50],class2images[:50],class3images[:50],class4images[:50],class5images[:50],class6images[:50],class7images[:50],class8images[:50],class9images[:50]])","ae0bc4a9":"train.shape","2fea6c20":"label0 =(len(class0images)*\"pink_primrose \").split(' ')[:-1]\nlabel1 =(len(class1images)*\"snapdragon \").split(' ')[:-1]\nlabel2 =(len(class2images)*\"purple_coneflower \").split(' ')[:-1]\nlabel3 =(len(class3images)*\"king_protea \").split(' ')[:-1]\nlabel4 =(len(class4images)*\"wild_geranium \").split(' ')[:-1]\nlabel5 =(len(class5images)*\"tiger_lily \").split(' ')[:-1]\nlabel6 =(len(class6images)*\"peruvian_lily \").split(' ')[:-1]\nlabel7 =(len(class7images)*\"birdof_p_aradise \").split(' ')[:-1]\nlabel8 =(len(class8images)*\"monkshood \").split(' ')[:-1]\nlabel9 =(len(class9images)*\"globe_thistle \").split(' ')[:-1]","ff6cc3a1":"label0 =(len(class0images)*\"0 \").split(' ')[:-1]\nlabel1 =(len(class1images)*\"1 \").split(' ')[:-1]\nlabel2 =(len(class2images)*\"2 \").split(' ')[:-1]\nlabel3 =(len(class3images)*\"3 \").split(' ')[:-1]\nlabel4 =(len(class4images)*\"4 \").split(' ')[:-1]\nlabel5 =(len(class5images)*\"5 \").split(' ')[:-1]\nlabel6 =(len(class6images)*\"6 \").split(' ')[:-1]\nlabel7 =(len(class7images)*\"7 \").split(' ')[:-1]\nlabel8 =(len(class8images)*\"8 \").split(' ')[:-1]\nlabel9 =(len(class9images)*\"9 \").split(' ')[:-1]","2452cafe":"labels=np.concatenate([label0[:50],label1[:50],label2[:50],label3[:50],label4[:50],label5[:50],label6[:50],label7[:50],label8[:50],label9[:50]])\nlabels","d9e86470":"labels.shape","482ba274":"display_20_image(train)","9d293ef8":"from skimage.exposure import equalize_adapthist\nfrom skimage.transform import resize\nnew_train=[]\nfor i in train:\n    new_train.append((equalize_adapthist(i)))\n    \ndisplay_20_image(new_train)\nnew_train=np.array(new_train)","037c8c6c":"mydataset=[]","e4ba97e5":"X_train, X_test, y_train, y_test = train_test_split(new_train, labels, test_size=0.2, random_state=42)\n","76f6ab54":"Base_model = VGG16(include_top= False, weights='imagenet',input_shape=(192,192,3), pooling='avg')\nBase_model.trainable = False","8cb7bbac":"data_augmentation = tf.keras.Sequential([\n  RandomFlip(\"horizontal_and_vertical\"),\n  RandomRotation(0.1),\n])\n\nnew_X=[]\nnew_y=[]\nfor i in range(len(X_train)):\n    for j in range(8):\n        new_X.append(data_augmentation(X_train[i]))\n        new_y.append(y_train[i])\nnew_X=np.array(new_X)\nnew_X.shape","32f22870":"es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n","967dc9ed":"\nmodel = Sequential(Base_model.layers)\n\n\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1000,activation='relu'))\n\n\n# adding prediction(softmax) layer\nmodel.add(Dense(10,activation=\"softmax\"))\nmodel.summary()\n\n\nmodel.compile(optimizer='adam', loss= 'categorical_crossentropy', metrics=['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\ny_tr=tf.keras.utils.to_categorical(new_y,num_classes=10)\ny_ts=tf.keras.utils.to_categorical(y_test,num_classes=10)\n\nhistory=model.fit(new_X,y_tr,epochs=20,batch_size=256,validation_split=0.2,callbacks=[es])\n\ny_pred=model.predict(X_test)\n\nprint(classification_report(y_test.astype('int'),np.argmax(y_pred,axis=1)))","f8d57421":"y_pred=model.predict(new_X)\n\nprint(classification_report(np.array(new_y).astype('int'),np.argmax(y_pred,axis=1)))","3abbda13":"import matplotlib.pyplot as plt\n \nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\naccuracy = history_dict['accuracy']\nval_accuracy = history_dict['val_accuracy']\n \nepochs = range(1, len(loss_values) + 1)\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))\n#\n# Plot the model accuracy vs Epochs\n#\nax[0].plot(epochs, accuracy, 'r', label='Training accuracy')\nax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nax[0].set_title('Training & Validation Accuracy', fontsize=16)\nax[0].set_xlabel('Epochs', fontsize=16)\nax[0].set_ylabel('Accuracy', fontsize=16)\nax[0].legend()\n#\n# Plot the loss vs Epochs\n#\nax[1].plot(epochs, loss_values, 'r', label='Training loss')\nax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')\nax[1].set_title('Training & Validation Loss', fontsize=16)\nax[1].set_xlabel('Epochs', fontsize=16)\nax[1].set_ylabel('Loss', fontsize=16)\nax[1].legend()","4dc32036":"import matplotlib.pyplot as plt\n \nhistory_dict = history.history\nloss_values = history_dict['recall']\nval_loss_values = history_dict['val_recall']\naccuracy = history_dict['precision']\nval_accuracy = history_dict['val_precision']\n \nepochs = range(1, len(loss_values) + 1)\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))\n#\n# Plot the model accuracy vs Epochs\n#\nax[0].plot(epochs, accuracy, 'r', label='Training Recall')\nax[0].plot(epochs, val_accuracy, 'b', label='Validation Recall')\nax[0].set_title('Training & Validation Recall', fontsize=16)\nax[0].set_xlabel('Epochs', fontsize=16)\nax[0].set_ylabel('Recall', fontsize=16)\nax[0].legend()\n#\n# Plot the loss vs Epochs\n#\nax[1].plot(epochs, loss_values, 'r', label='Training precision')\nax[1].plot(epochs, val_loss_values, 'b', label='Validation precision')\nax[1].set_title('Training & Validation precision', fontsize=16)\nax[1].set_xlabel('Epochs', fontsize=16)\nax[1].set_ylabel('precision', fontsize=16)\nax[1].legend()","25ba4b65":"cm=confusion_matrix(np.array(new_y).astype('int'),np.argmax(y_pred,axis=1))\nsns.heatmap(cm,annot=True)","7cad9beb":"X_train, X_test, y_train, y_test = train_test_split(new_train, labels, test_size=0.2, random_state=42)\n\ntf.random.set_seed(20)\nnp.random.seed(20)\nmodel = Sequential(Base_model.layers)\n\n\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1000,activation='relu'))\n\n\n# adding prediction(softmax) layer\nmodel.add(Dense(10,activation=\"softmax\"))\nmodel.summary()\n\n\nmodel.compile(optimizer='adam', loss= 'categorical_crossentropy', metrics=['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\ny_tr=tf.keras.utils.to_categorical(y_train,num_classes=10)\ny_ts=tf.keras.utils.to_categorical(y_test,num_classes=10)\n\nhistory=model.fit(X_train,y_tr,epochs=20,batch_size=256,validation_split=0.2,callbacks=[es])\n\ny_pred=model.predict(X_test)\n\nprint(classification_report(y_test.astype('int'),np.argmax(y_pred,axis=1)))","6151f889":"cm=confusion_matrix(y_test.astype('int'),np.argmax(y_pred,axis=1))\nsns.heatmap(cm,annot=True)","45826447":"X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2, random_state=42)\n\ntf.random.set_seed(20)\nnp.random.seed(20)\nmodel = Sequential(Base_model.layers)\n\n\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1000,activation='relu'))\n\n\n# adding prediction(softmax) layer\nmodel.add(Dense(10,activation=\"softmax\"))\nmodel.summary()\n\n\nmodel.compile(optimizer='adam', loss= 'categorical_crossentropy', metrics=['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\ny_tr=tf.keras.utils.to_categorical(y_train,num_classes=10)\ny_ts=tf.keras.utils.to_categorical(y_test,num_classes=10)\n\nhistory=model.fit(X_train,y_tr,epochs=20,batch_size=256,validation_split=0.2,callbacks=[es])\n\ny_pred=model.predict(X_test)\n\nprint(classification_report(y_test.astype('int'),np.argmax(y_pred,axis=1)))","a693d83f":"\n\n\n\n\n\n# Step 2: Distribution Strategy #\n\nA TPU has eight different *cores* and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a **distribution strategy**. Run the following cell to create the distribution strategy that we'll later apply to our model.","52802ae6":"We'll use the distribution strategy when we create our neural network model. Then, TensorFlow will distribute the training among the eight TPU cores by creating eight different *replicas* of the model, one for each core.\n\n# Step 3: Loading the Competition Data #\n\n## Get GCS Path ##\n\nWhen used with TPUs, datasets need to be stored in a [Google Cloud Storage bucket](https:\/\/cloud.google.com\/storage\/). You can use data from any public GCS bucket by giving its path just like you would data from `'\/kaggle\/input'`. The following will retrieve the GCS path for this competition's dataset.","64faf0b4":"# Step 1: Imports #\n\nWe begin by importing several Python packages.","b9148b96":"You can use data from any public dataset here on Kaggle in just the same way. If you'd like to use data from one of your private datasets, see [here](https:\/\/www.kaggle.com\/docs\/tpu#tpu3pt5).\n\n## Load Data ##\n\nWhen used with TPUs, datasets are often serialized into [TFRecords](https:\/\/www.kaggle.com\/ryanholbrook\/tfrecords-basics). This is a format convenient for distributing data to each of the TPUs cores. We've hidden the cell that reads the TFRecords for our dataset since the process is a bit long. You could come back to it later for some guidance on using your own datasets with TPUs."}}