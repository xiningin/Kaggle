{"cell_type":{"1febf88f":"code","34ce7fd0":"code","10c039bb":"code","147e051a":"code","9bf52cb9":"code","3df9c7c9":"code","47cb3a7d":"code","6bc1065a":"code","1e8f5e3f":"code","eadf386e":"code","46fb2a90":"code","f93e35bb":"markdown","1fd9ebfd":"markdown","6018aa55":"markdown","66454731":"markdown","48ea884f":"markdown","69b74fdc":"markdown"},"source":{"1febf88f":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import LinearSVR, SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","34ce7fd0":"data = pd.read_csv('..\/input\/housing-in-london\/housing_in_london_monthly_variables.csv')","10c039bb":"data","147e051a":"data.info()","9bf52cb9":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop redundant columns\n    df = df.drop('code', axis=1)\n    \n    # Drop columns with too many missing values\n    df = df.drop('no_of_crimes', axis=1)\n    \n    # Drop rows with missing target values\n    missing_target_rows = df[df['houses_sold'].isna()].index\n    df = df.drop(missing_target_rows, axis=0).reset_index(drop=True)\n    \n    # Extract date features\n    df['date'] = pd.to_datetime(df['date'])\n    df['year'] = df['date'].apply(lambda x: x.year)\n    df['month'] = df['date'].apply(lambda x: x.month)\n    df = df.drop('date', axis=1)\n    \n    # One-hot encode the area column\n    area_dummies = pd.get_dummies(df['area'], prefix='area')\n    df = pd.concat([df, area_dummies], axis=1)\n    df = df.drop('area', axis=1)\n    \n    # Split df into X and y\n    y = df['houses_sold']\n    X = df.drop('houses_sold', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n    \n    return X_train, X_test, y_train, y_test","3df9c7c9":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","47cb3a7d":"X_train","6bc1065a":"y_train","1e8f5e3f":"models = {\n    \"                     Linear Regression\": LinearRegression(),\n    \" Linear Regression (L2 Regularization)\": Ridge(),\n    \" Linear Regression (L1 Regularization)\": Lasso(),\n    \"                   K-Nearest Neighbors\": KNeighborsRegressor(),\n    \"                        Neural Network\": MLPRegressor(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVR(),\n    \"   Support Vector Machine (RBF Kernel)\": SVR(),\n    \"                         Decision Tree\": DecisionTreeRegressor(),\n    \"                         Random Forest\": RandomForestRegressor(),\n    \"                     Gradient Boosting\": GradientBoostingRegressor(),\n    \"                               XGBoost\": XGBRegressor(),\n    \"                              LightGBM\": LGBMRegressor(),\n    \"                              CatBoost\": CatBoostRegressor(verbose=0)\n}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    print(name + \" trained.\")","eadf386e":"for name, model in models.items():\n    y_pred = model.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    print(name + \" RMSE: {:.4f}\".format(rmse))","46fb2a90":"for name, model in models.items():\n    print(name + \" R^2: {:.4f}\".format(model.score(X_test, y_test)))","f93e35bb":"# Preprocessing","1fd9ebfd":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/iPJ0ZgO04m4","6018aa55":"# Getting Started","66454731":"# Training","48ea884f":"# Results","69b74fdc":"# Task for Today  \n\n***\n\n## London House Sales Prediction  \n\nGiven *data about houses in London*, let's try to predict how many **houses will be sold** in a given month and area.\n\nWe will use a variety of regression models to make our predictions."}}