{"cell_type":{"5156b4b0":"code","15de7839":"code","eb5fdbab":"code","ff393b4a":"code","fb0919a6":"code","181cef15":"code","3907e9bc":"code","2c67924c":"code","0812ab4f":"code","bfc198f5":"code","e729de19":"code","946a0c2c":"code","71680831":"code","4e897781":"code","30465445":"code","50bbc96c":"code","e1231cb8":"code","b9600c53":"code","9ec22123":"code","4d9ec60f":"code","2986e990":"code","2e08061b":"code","11a67d81":"code","47734269":"code","47d4d5e5":"code","640a8b8f":"code","04ff6f4d":"code","109a691e":"code","df158aea":"code","65a149d3":"code","4a281d9f":"code","81b7fc7c":"code","0b7bd6cb":"code","13112002":"code","2981ee07":"code","885e8344":"code","ee078336":"code","2244b595":"code","075a0db0":"code","96d034f6":"code","8c85f528":"code","ad049239":"code","e7d9079c":"code","6067091b":"code","e746cc6a":"code","e0a71a10":"code","45e3eeba":"code","ffd3ffbb":"markdown","702edd76":"markdown","65aec11f":"markdown","4a441d56":"markdown","8693fd6f":"markdown","0465178b":"markdown","0a0da00b":"markdown","703bea4d":"markdown","28701b54":"markdown","b7f47cd3":"markdown","90b401bc":"markdown","840a7cfe":"markdown","cbccfe62":"markdown","982b49ef":"markdown","86eddb0e":"markdown","f06b912b":"markdown","7b166e3a":"markdown","989ce14d":"markdown","71341f85":"markdown","fc78cff2":"markdown","a1873582":"markdown","e80a5b38":"markdown","23e0c525":"markdown","941b816b":"markdown","bd5d3673":"markdown","a0d240c9":"markdown","a9eea2a1":"markdown","6b1b1b2f":"markdown","1fd55724":"markdown","38424529":"markdown","d2a3a552":"markdown","fe323941":"markdown","086152cb":"markdown","bbe0ab5e":"markdown","d0bc83ae":"markdown","fb0de91c":"markdown","08b3c302":"markdown","d62178fd":"markdown","cfc5d20d":"markdown","39293b16":"markdown","fac4f00e":"markdown","81551b79":"markdown"},"source":{"5156b4b0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","15de7839":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')","eb5fdbab":"#loading the libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import ElasticNetCV, ElasticNet\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom xgboost import  XGBRegressor\nfrom numpy import arange\nfrom pandas import read_csv\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = None","ff393b4a":"df.head()","fb0919a6":"df.dtypes","181cef15":"df.columns","3907e9bc":"#to facilitate theiteration prcocess I will standardize columns names with _\ndf.columns = df.columns.str.strip()\ndf.columns = df.columns.str.replace(' ', '_')","2c67924c":"df.columns","0812ab4f":"df.shape","bfc198f5":"df.info()","e729de19":"df.describe()","946a0c2c":"df.columns","71680831":"#Lets take a look at the pair plot between the target feature and the independent variables in our dataset\npp = sns.pairplot(data=df,\n                  y_vars=df.columns[:-1],\n                  x_vars=['Chance_of_Admit'], height=5)","4e897781":"#Frequency of each feature label\nplt.figure(figsize=[15,18])\nfeafures = df.columns[:-1]\n#take all columns except the target Selling_Price\nn=1\nfor f in feafures:\n    plt.subplot(5,2,n)\n    sns.barplot(x=f,  y= df['Chance_of_Admit'], data=df, palette='Dark2')\n    plt.title(\"Countplot of {}  by Chance_of_Admit\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()","30465445":"#Using Pearson Correlation\nplt.figure(figsize=(13,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","50bbc96c":"plt.figure(figsize=(10, 8))\nsns.heatmap(df.corr()[['Chance_of_Admit']].sort_values(by='Chance_of_Admit'), \n                    annot=True, \n                    cmap='inferno', \n                    vmin=-1,\n                    vmax=1) ","e1231cb8":"#Some algorithm could not process the continious output variable, so I have decide to convert the\n#admission_rate to integer values\ndf['Chance_of_Admit'] = df['Chance_of_Admit']*100\ndf['Chance_of_Admit'] = df['Chance_of_Admit'].astype('int64')","b9600c53":"#To select features I need to split the dataset to independent and dependent variables\nX = df.drop('Chance_of_Admit', axis =1)\nY = df['Chance_of_Admit']","9ec22123":"#train_tets_split to ensure no data would leak to test data  \nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=1)\n\n\n#Some of the feature selection algorithms require the scaling: Lasso & Ridge regressions\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)","4d9ec60f":"print('Filter methods')\n\n# Anova\ndef select_features(X_train, y_train, X_test):\n\t# configure to select all features\n\tfs = SelectKBest(score_func=f_classif, k='all')\n\t# learn relationship from training data\n\tfs.fit(X_train, y_train)\n\t# transform train input data\n\tX_train_fs = fs.transform(X_train)\n\t# transform test input data\n\tX_test_fs = fs.transform(X_test)\n\treturn X_train_fs, X_test_fs, fs\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=1)\n# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\nanova = pd.Series(abs(fs.scores_), index=df.columns[:-1])\nanova.sort_values(ascending=True).plot(kind='barh')\nplt.title('Anova regression')\nplt.show()\n\n#mutual info\nfrom sklearn.feature_selection import mutual_info_regression\n# determine the mutual information\nmutual_info = mutual_info_regression(X_train, y_train)\nmutual_info_series = pd.Series(mutual_info)\nmutual_info_series.index = X_train.columns\nmutual_info_series.sort_values(ascending=True).plot(kind='barh')\nplt.title('Mutual info')\nplt.show()\n\n#Correlation\ncorrelation=abs(df.corr()['Chance_of_Admit'])\ndel correlation['Chance_of_Admit']\ncorrelation.sort_values(ascending=True).plot(kind='barh')\nplt.title('Correlation')\nplt.show()\n\n\n\nprint('Wrapper methods')\n\n#Recursive feature elimination\nfrom sklearn.feature_selection import RFE\nrfc = RandomForestClassifier(random_state=101)\nselector = RFE(rfc, n_features_to_select=8, step=1)\nselector.fit(X_train_std, y_train)\nrfecvccoeff = pd.Series((selector.estimator_.feature_importances_), index=df.columns[:-1])\nrfecvccoeff.sort_values(ascending=True).plot(kind='barh')\nplt.title('RFECV regression')\nplt.show()\n\n\n\nprint('Embeded methods')\n\n#Lasso regression\nlasso=Lasso()\nlasso.fit(X_train_std,y_train)\nlassocoeff = pd.Series((lasso.coef_), index=df.columns[:-1])\nlassocoeff.sort_values(ascending=True).plot(kind='barh')\nplt.title('Lasso regression')\n#plt.show()\nnormLasso = lassocoeff\/np.linalg.norm(lassocoeff)\nnormLasso = pd.Series(abs(normLasso), index=df.columns[:-1])\nnormLasso.sort_values(ascending=True).plot(kind='barh')\nplt.title('Lasso regression')\nplt.show()\n\n\n#Ridge regression\nridge=Ridge()\nridge.fit(X_train_std,y_train)\nridgecoeff = pd.Series((ridge.coef_), index=X.columns)\nridgecoeff.sort_values(ascending=True).plot(kind='barh')\n\nnormRidge = ridgecoeff\/np.linalg.norm(ridgecoeff)\nnormRidge = pd.Series(abs(normRidge), index=X.columns)\nnormRidge.sort_values(ascending=True).plot(kind='barh')\nplt.title('Ridge regression')\nplt.show()\n\n\n#Elastic Net regression\nelastic=ElasticNetCV()\nelastic.fit(X_train_std,y_train)\nelasticcoeff = pd.Series((elastic.coef_), index=df.columns[:-1])\nelasticcoeff.sort_values(ascending=True).plot(kind='barh')\nplt.title('Elastic regression')\nnormelastico = elasticcoeff\/np.linalg.norm(elasticcoeff)\nnormelastico = pd.Series(abs(normelastico), index=df.columns[:-1])\nnormelastico.sort_values(ascending=True).plot(kind='barh')\nplt.title('Elastic regression')\nplt.show()\n\n# Random forest regression\nrf=RandomForestRegressor()\nrf.fit(X_train_std ,y_train)\nfeat_importances2 = pd.Series(rf.feature_importances_, index=df.columns[:-1])\nfeat_importances2.sort_values(ascending=True).plot(kind='barh')\nplt.title('Random forest regression')\nplt.show()\n\n#Extra Trees regression\net=ExtraTreesRegressor()\net.fit(X_train_std ,y_train)\nfeat_importances3 = pd.Series(et.feature_importances_, index=X.columns)\nfeat_importances3.sort_values(ascending=True).plot(kind='barh')\nplt.title('Extra Trees regression')\nplt.show()\n\n#Gradient boostung regression\ngbr=GradientBoostingRegressor()\ngbr.fit(X_train_std,y_train)\nfeat_importances4 = pd.Series(gbr.feature_importances_, index=X.columns)\nfeat_importances4.sort_values(ascending=True).plot(kind='barh')\nplt.title('Gradient boostung regression')\nplt.show()","2986e990":"# Here we take the average of these 10 regressors\nimport random\n\nweight1= np.random.randint(1,10)\/10\nweight2= np.random.randint(1,10)\/10\nweight3= np.random.randint(1,10)\/10\nweight4= np.random.randint(1,10)\/10\nweight5= np.random.randint(1,10)\/10\nweight6= np.random.randint(1,10)\/10\nweight7= np.random.randint(1,10)\/10\nweight8= np.random.randint(1,10)\/10\nweight9= np.random.randint(1,10)\/10\nweight10= np.random.randint(1,10)\/10\n\nweightSum=weight1+weight2+weight3+weight4+weight5+weight6+weight7+weight8+weight9+weight10\n\n#Compute the average\nfeat=(weight1 * abs(fs.scores_) +weight2*abs(correlation) + weight3*mutual_info +\n      weight4*rfecvccoeff + weight5*feat_importances2 + weight6*feat_importances3 + \n      weight5*feat_importances4 + weight8*normLasso + weight9*normRidge + \n      weight10*normelastico)\/(weightSum)\n\n#plot the results of the average sum\nfeat.sort_values(ascending=True).plot(kind='barh')\nplt.show()","2e08061b":"#Using Pearson Correlation\nplt.figure(figsize=(10,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","11a67d81":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","47734269":"correlation(df, 0.7)","47d4d5e5":"# read the dataset \nimport pandas as pd \n\n  \n# Remove the price from the dataset \n#Y = df[\"Admit\"] \n\niv = df.columns\n\n  \n# calculate the varience inflation factor \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif \n  \n# compare with each columns \n[vif(df[iv].values, index) for index in range(len(iv))]  \n  \n# Removing multicollinearity from the datset using vif  \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif \n  \n# compare with each columns \nfor i in range(len(iv)): \n    vif_list = [vif(df[iv].values, index) for index in range(len(iv))]  \n    maxvif = max(vif_list) \n    print(\"Max VIF value is \", maxvif)                       \n    drop_index = vif_list.index(maxvif) \n    print(\"For Independent variable\", iv[drop_index]) \n     \n    if maxvif > 40: \n         \n        print(\"Deleting\", iv[drop_index]) \n        iv = iv.delete(drop_index) \n        print(\"Final Independent_variables \", iv) ","640a8b8f":"df_final = df[['TOEFL_Score', 'University_Rating', 'Research', 'Chance_of_Admit', 'LOR', 'SOP']]","04ff6f4d":"#checking for the outliers for the selected features\nfor i in range(len(df_final.columns)):\n    fig = plt.figure(figsize=[10,3])\n\n    sns.boxplot(x=df_final.columns[i], data=df_final)\n\n  \n    \n    plt.show()","109a691e":"df_final.columns","df158aea":"#calculating the Interquartile range of Chance_of_Admit\nIQR=df_final.Chance_of_Admit.quantile(0.75)-df_final.Chance_of_Admit.quantile(0.25)\n\n#calculating the borders of the normal distribution of Chance_of_Admit\nlower_bridge=df_final['Chance_of_Admit'].quantile(0.25)-(IQR*1.5)\nupper_bridge=df_final['Chance_of_Admit'].quantile(0.75)+(IQR*1.5)\n\n#Inpute the outliers with the min values that are present for the normal distribution of Chance_of_Admit\ndf_final.loc[df_final['Chance_of_Admit']<=lower_bridge,'Chance_of_Admit']=lower_bridge","65a149d3":"#calculating the Interquartile range of Chance_of_Admit\nIQR=df_final.LOR.quantile(0.75)-df_final.LOR.quantile(0.25)\n\n#calculating the borders of the normal distribution of Chance_of_Admit\nlower_bridge=df_final['LOR'].quantile(0.25)-(IQR*1.5)\nupper_bridge=df_final['LOR'].quantile(0.75)+(IQR*1.5)\n\n#Inpute the outliers with the min values that are present for the normal distribution of Chance_of_Admit\ndf_final.loc[df_final['LOR']<=lower_bridge,'LOR']=lower_bridge","4a281d9f":"#checking for the outliers in LOR and Chance_of_Admit columns\n\nfig = plt.figure(figsize=[10,3])\nsns.boxplot(x=df_final['LOR'], data=df_final)\n\nfig = plt.figure(figsize=[10,3])\nsns.boxplot(x=df_final['Chance_of_Admit'], data=df_final)\n  ","81b7fc7c":"# Separating target variable and its features\ny = df_final['Chance_of_Admit']\nX = df_final.drop('Chance_of_Admit',axis=1)\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nprint(\"x train: \",X_train.shape)\nprint(\"x test: \",X_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","0b7bd6cb":"#uploading Standard Scaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_st = sc.fit_transform(X_train)\nX_test_st = sc.transform(X_test)","13112002":"#creating a dictionary of the models(estimators) \nestimators = {\n    'Linear Regression': [LinearRegression()],\n    'Lasso' :[Lasso()],\n    'Ridge' :[Ridge()],\n    'KNN' :[KNeighborsRegressor()],\n    'Decision Tree' :[DecisionTreeRegressor()],\n    'Ransom Forest' :[RandomForestRegressor()],\n    'XG Boost': [XGBRegressor()],\n}\n\n#writing a function to fit models above to the train dataset\ndef mfit(estimators, X_train_st, y_train):\n    for m in estimators:\n        estimators[m][0].fit(X_train_st, y_train)\n        print(m+' fitted')\n\nmfit(estimators, X_train_st, y_train)","2981ee07":"import sklearn.metrics as metrics\n\n#applying the fitted models to the test dataset\ndef mpredict(estimators, X_test_st, y_test):\n    outcome = dict()\n    for m in estimators:\n        y_pred = estimators[m][0].predict(X_test_st)\n        outcome[m] = [round(metrics.r2_score(y_test, y_pred), 2), \n                      metrics.mean_absolute_error(y_test, y_pred),\n                     \n                     metrics.mean_squared_error(y_test, y_pred),\n                     np.sqrt(metrics.mean_squared_error(y_test, y_pred))]\n    return outcome\n\noutcome = mpredict(estimators, X_test_st, y_test)","885e8344":"#printing the regression errors as metrics for the model evaluation\nfor m in outcome:\n    print('------------------------'+m+'------------------------')\n    print('R2 score', round(outcome[m][0],2))\n    print('MAE', round(outcome[m][1],2))\n    print('MSE', round(outcome[m][2],2))\n    print('RMSE', round(outcome[m][3],2))","ee078336":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nmodel = KNeighborsRegressor()\n\n# Select an algorithm\nalgorithm = KNeighborsRegressor()\n\n# Create 3 folds. We will use 3 fold cross validation\nseed = 13\nkfold = KFold(n_splits=3, shuffle=True, random_state=seed)\n\n\n# Define our candidate hyperparameters\nhp_candidates = [{'n_neighbors': [2,3,4,5], 'weights': ['uniform','distance']}]\n\n\n# Search for best hyperparameters\ngrid = GridSearchCV(estimator=algorithm, param_grid=hp_candidates, cv=kfold, scoring='r2')\ngrid.fit(X_train_st, y_train)\n\n\n# Get the results\nprint(grid.best_estimator_)\nprint(grid.best_params_)","2244b595":"KNN = KNeighborsRegressor(n_neighbors = 5, weights = 'uniform')\nKNN.fit(X_train_st, y_train)\n\npred = KNN.predict(X_test_st)\n\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\nprint(\"R2 score =\", round(metrics.r2_score(y_test, pred), 2))\n","075a0db0":"import matplotlib.pyplot as plt\n\nplt.style.use('default')\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(figsize=(7, 3.5))\n\nax.scatter(X_test_st[:,0], y_test, color='green', label='Regression model')\nax.scatter(X_train_st[:,0], y_train, edgecolor='k', facecolor='yellow', alpha=0.7, label='Sample data')\nax.set_ylabel('Chance_of_Admit', fontsize=12)\nax.set_xlabel('Factors = f(TOEFL_Score + University_Rating + Research + LOR + SOP)', fontsize=12)\nax.legend(facecolor='white', fontsize=11)\n\nfig.tight_layout()\n\nfig.tight_layout()","96d034f6":"df_final","8c85f528":"import statsmodels.formula.api as sm\nmodel=sm.ols(formula = 'Chance_of_Admit ~ TOEFL_Score + University_Rating + Research + LOR + SOP', \n             data=df_final)\nfitted = model.fit()\nprint(fitted.summary())","ad049239":"#Checking for the linearity \nfor c in df_final.drop('Chance_of_Admit', axis = 1):\n    plt.figure(figsize=(8,5))\n    plt.title(\"{} vs. \\nChance_of_Admit\".format(c),fontsize=16)\n    plt.scatter(x=df_final[c],y=df_final['Chance_of_Admit'],color='black',edgecolor='k')\n    plt.grid(True)\n    plt.xlabel(c,fontsize=14)\n    plt.ylabel('Chance_of_Admit',fontsize=14)\n\n    plt.show()","e7d9079c":"df.columns","6067091b":"#Checking for Normality\nfrom statsmodels.graphics.gofplots import qqplot\nfig=qqplot(fitted.resid_pearson,line='45',fit = 'True')\nplt.xlabel('Theoretical quantiles')\nplt.ylabel('Sample quantiles')\nplt.show()","e746cc6a":"#Checking for Normality\nfrom scipy.stats import shapiro\n_,p=shapiro(fitted.resid)\nif p>0.05:\n    print(\"The residuals seem to come from Gaussian process\")\nelse:\n    print(\"The normality assumption may not hold\")\n    \nplt.figure(figsize=(8,5))\nplt.hist(fitted.resid_pearson,bins=20,edgecolor='k')\nplt.ylabel('Count',fontsize=15)\nplt.xlabel('Normalized residuals',fontsize=15)\nplt.title(\"Histogram of normalized residuals\",fontsize=18)\nplt.show()","e0a71a10":"#Checking for Homoscedasticity\nplt.figure(figsize=(8,5))\np=plt.scatter(x=fitted.fittedvalues,y=fitted.resid,edgecolor='k')\nxmin=min(fitted.fittedvalues)\nxmax = max(fitted.fittedvalues)\nplt.hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)\nplt.xlabel(\"Fitted values\",fontsize=15)\nplt.ylabel(\"Residuals\",fontsize=15)\nplt.title(\"Fitted vs. residuals plot\",fontsize=18)\nplt.grid(True)\nplt.show()","45e3eeba":"#Autocorrelation\nfrom statsmodels.stats.stattools import durbin_watson\ndurbinWatson = durbin_watson(fitted.resid)\nprint(durbinWatson)","ffd3ffbb":"All features except Serial No do influence the model predictions with max of 0.88 for CGPA and min of 0.55 for Research","702edd76":"# Feature Selection","65aec11f":"Here is the deployment [link](https:\/\/admisisonusa.herokuapp.com\/)!","4a441d56":"# Deployment","8693fd6f":"# Quick glance at Data","0465178b":"<a class = 'anchor' id = 'EDA'><\/a>","0a0da00b":"Looks like GRE_Score, TOEFL_Score, University_rating, CGPA, SOP, LOR have a sort of linear relationship with the target feature: Chance_of_Admit.","703bea4d":"We can see that some features have relatively high collinearity with other independent variables. This will cause a multicollinearity issue, which will lead to instability during the prediction process. Thus, we need to carefully select those features that are the most relevant and remove those that have high collinearity index: like GRE vs TOEFL with 0.83. We need to choose which one to keep and which to delete. We cannot use them both in our model.","28701b54":"A good model displays the constant variance of the residuals vs their fitted values. In our case we have two major clusters of the residuals as displayed on the X-axis: 1st cluster (50-80), 2nd cluster (80-95) meaning that the variance of Chance_of_Admit is increasing with the values of predictors meaning that our model might not work well at the lower values of Chance_of_Admit.","b7f47cd3":"**Summary of EDA analysis: how the independent features influence the dependent feature (Chance_of_Admit)**\n\n1. Looks like Serial No. is just a unique applicant identifier, so we can delete it\n2. The higher the GRE score, the higher chances of admission.\n3. Similar with GRE, the higher the TOEFL score, the higer chances of admission.\n3. University rating: the higher the University rating, the higher the chances of admission. Same goes for SOP, LOR and CGPA.\n4. Research does play a role: those with research experience have higher chances to get admitted to the university than those without this experience.","90b401bc":"**The best model is the KNN!**\n","840a7cfe":"**There are four key assumptions that need to be tested for a linear regression model:**\n\n* Linearity: The expected value of the dependent variable is a linear function of each independent variable, holding the others fixed (note this does not restrict you to use a nonlinear transformation of the independent variables i.e. you can still model f(x) = ax\u00b2 + bx + c, using both x\u00b2 and x as predicting variables.\n\n* Independence: The errors (residuals of the fitted model) are independent of each other.\n\n* Homoscedasticity (constant variance): The variance of the errors is constant with respect to the predicting variables or the response.\n\n* Normality: The errors are generated from a Normal distribution (of unknown mean and variance, which can be estimated from the data). Note, this is not a necessary condition to perform linear regression unlike the top three above. However, without this assumption being satisfied, you cannot calculate the so-called \u2018confidence\u2019 or \u2018prediction\u2019 intervals easily as the well-known analytical expressions corresponding to Gaussian distribution cannot be used. Ref","cbccfe62":"We do not have null values in this dataset","982b49ef":"<a class = 'anchor' id = 'Model_valuation'><\/a>\n","86eddb0e":"So, we have the outliers at the Chance_of_Admit and LOR columns, lets impute them!","f06b912b":"In this section we will apply various feature selection models to extract the most usefull features from the overall pool. For recursive feature elemination (RFE), we need to convert Chance_of_Admit fro float to int, otherwise, we will get this error: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.","7b166e3a":"<a class = 'anchor' id = 'De'><\/a>","989ce14d":"# Hyperparameter tuning","71341f85":"# Will you get admitted to the USA university?","fc78cff2":"<a class = 'anchor' id = 'Model_Building'><\/a>\n\n","a1873582":"Lets check the feature correlation of independent features with the dependent variable.","e80a5b38":"* [Filter methods](https:\/\/towardsdatascience.com\/feature-selection-for-machine-learning-in-python-filter-methods-6071c5d267d5) use statistical calculation to evaluate the relevance of the predictors outside of the predictive models and keep only the predictors that pass some criterion. Filter methods are usualy performed without any predictive model. These are faster and usually the better choice for large number of features. Allow to avoid overfitting, but sometimes may not always select the best features.\n    1. [Anova F-score](https:\/\/chrisalbon.com\/machine_learning\/feature_selection\/anova_f-value_for_feature_selection\/)\n\n    2. [Mutual Information for regression](https:\/\/machinelearningmastery.com\/information-gain-and-mutual-information\/)\n\n    3. [Pearson Correlation](https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/)\n\n\n* [Wrapper methods](https:\/\/towardsdatascience.com\/feature-selection-for-machine-learning-in-python-wrapper-methods-2b5e27d2db31) evaluate multiple models using procedures that add and\/or remove predictors to find the optimal combination that maximizes model performance. Though computationally expensive and tend to overfitting, give better performance.\n\n    1. [Recursive feature elimination](https:\/\/machinelearningmastery.com\/rfe-feature-selection-in-python\/)\n\n\n* [Embedded methods](https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/feature-selection-techniques-in-machine-learning\/) are models where the feature selection procedure occurs naturally in the course of the model fitting process integrating the feature selection algorithm as part of the machine learning algorithm. The most typical embedded technique is tree based algorithm, which includes decision tree and random forest. The general idea of feature selection is decided at the splitting node based on information gain. Other exemplars of embedded methods are the LASSO with the L1 penalty and Ridge with the L2 penalty for constructing a linear model.\n\n    1. [Lasso regression](https:\/\/medium.com\/@23.sargam\/lasso-regression-for-feature-selection-8ac2287e25fa)\n    2. [Ridge regression](https:\/\/towardsdatascience.com\/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b)\n    3. [Elastic Net regression](https:\/\/machinelearningmastery.com\/elastic-net-regression-in-python\/)\n    4. [Random forest regression](https:\/\/chrisalbon.com\/machine_learning\/trees_and_forests\/feature_selection_using_random_forest\/)\n    5. [Extra Trees regression](https:\/\/machinelearningmastery.com\/extra-trees-ensemble-with-python\/)\n    6. [Gradient boostung regression](https:\/\/towardsdatascience.com\/feature-selection-in-machine-learning-d5af31f7276)\n","23e0c525":"Summary of Feature Selection:\nAt the first sight CGPA, TOEFL and GRE are among the most important features. However, we need to check wether there are no multicollinearity between these three features, so we can safely use all of the m in our model.","941b816b":"<a class = 'anchor' id = 'Model_Building'><\/a>","bd5d3673":"We can observe an R-squared value of 0.74 demonstrating a decent correlation of TOEFL_Score, University_Rating, Research, LOR, SOP with the Chance_of_Admit. The most important part of this table are p-values that show the importance of the individual predictors for the overall model corresponding to the significance level providing that 0,000 < 0.5.","a0d240c9":"<a class = 'anchor' id = 'Feature_Selection'><\/a>","a9eea2a1":"Columns:\n1. Serial No is the unique number of each applicant.\n2. GRE sore is the score of GRE exam (graduate examination).\n3. TOEFL score is an english profficiencey language score.\n4. University rating is the rating of the university to which a student apply (ranging from 0 to 5).\n5. SOP is a statement of purpose score (ranging from 1 to 5).\n6. LOR is a letters of recommendation score (ranging from 1 to 5).\n7. Research score is whether a candidate have conducted the research (1) or not (0) during their undergraduate times.\n8. Chances of Admit are the admission chances (ranging from 0 to 1)","6b1b1b2f":"<a class = 'anchor' id = 'Quick-glance-at-the-Data'><\/a>","1fd55724":"The developed model shows a linear relationship between the TOEFL vs Chance_of_Admit, LOR vs Chance_of_Admit, and SOP vs Chance_of_Admit. Lets take a look at the most influencial independent feature: TOEFL. At lower TOEFL the variance of Chance_of_Admit larger than that at higher TOEFL values meaning that the model might not perormed well at the lowest TOEFL scores.\n","38424529":"Now using the corr matrix and the threshold we will check which features need to be examined closely","d2a3a552":"dtypes appear to be numeric ","fe323941":"**The logic behind the creation of df_final is the following:**\n\nGRE has crazy high VIF, this feature correlates with TOEFL, thus we can just delete GRE. CGPA has the second highest VIF and correlates with TOEFL, thus I delete CGPA as well. We delete Serial No as this is just the unique identifier of applicants. \n","086152cb":"The Normality assumption is violated as the left side of the Residual distribution does not follow the normal distribution, thus we can anticipate that the model might not predict well the lower values of Chance_of_Admit.","bbe0ab5e":"# Model_Evaluation","d0bc83ae":"**Introduction**\n\nAs a Florida International University Ph.D. Candidate I understand how difficult it is to get admitted to USA universities. This project aims to help undergraduate from the developing and developed countries to estimate their chances during the applications cycle for USA graduate schools.\nWhen I was applying for this programs, I have sent my documents to more then 20 USA schools: on average that costed me 2000$ in total for the online apllications submission process and hard copy documents sending. \n\n**Thus, I decided to design a model that can help students to save up the money by deciding whether they should apply to this particular school or choose the other one**.\n\n\n* I have compared the most common regression models and found that KNN regressor resulted in the highest r2 score of 0.76 giving the minimum errors (MSE, MAE)\n\n* Please do UpVote this Notebook if you find it helpful!","fb0de91c":"# Model Building ","08b3c302":"In addition to this, we need to check for the Variance Inflation Factor, as multicorrelated features  might heavily impact the overall model performance","d62178fd":"The autocorrelation assumption is violated. The Durbin-Watson value must be outside of the 0-2 range Values from 0 to 2 show positive autocorrelation (common in time series data).","cfc5d20d":"<a class = 'anchor' id = 'Model_Evaluation'><\/a>","39293b16":" <a><\/a>\n# Table of contents\n\n1. [Quick glance at Data](#Quick-glance-at-the-Data)\n\n2. [Exploratory Data Analysis](#EDA)\n\n3. [Feature Selection](#Feature_Selection)\n\n4. [Model Building](#Model_Building)\n\n5. [Hyperparameter Tuning](#Hyperparameter_tuning)\n\n6. [Model Evaluation](#Model_valuation)\n\n7. [Deployment](#De)","fac4f00e":"<a class = 'anchor' id = 'Hyperparameter_tuning'><\/a>","81551b79":"# Exploratory Data Analysis"}}