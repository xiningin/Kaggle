{"cell_type":{"193f6deb":"code","221b0e5a":"code","2c30c615":"code","16202341":"code","b63bb15e":"code","57bf8714":"code","c5a2b956":"code","2052933c":"code","71218af5":"code","405a86fd":"code","c0601751":"code","8e1f30b0":"code","d5e4cd51":"code","ec9e3774":"code","c6e02baa":"code","72d64b98":"code","29e710d0":"code","6cb2e4fd":"code","ecd2e5f1":"code","dd598190":"code","974eb039":"code","dd3028b8":"code","a40761b3":"code","cacdc53a":"code","2b4be239":"code","09790347":"code","2b9aa495":"code","c58c8ab6":"code","a37e7119":"code","4073a939":"code","f63d1654":"code","8749d492":"code","f063e1e5":"code","59a3f753":"markdown","b8057c13":"markdown","daf7068e":"markdown","46bcd0b9":"markdown","d8bfee9d":"markdown","a805dc02":"markdown","2047f2cd":"markdown","3e66659e":"markdown","94495fed":"markdown","16edf454":"markdown","0677d8c9":"markdown","a8c3bfc2":"markdown","cd405bd1":"markdown","a1baad2c":"markdown","da6ce2cc":"markdown","13c3c4c7":"markdown","b23d7a1d":"markdown","b7740d54":"markdown","80c47b39":"markdown","d97a97ff":"markdown","28055eda":"markdown","7867c788":"markdown","316d0c89":"markdown","be45f3d7":"markdown","fadeae32":"markdown","8b243c50":"markdown","06668d67":"markdown","bac04e2f":"markdown","27a651d7":"markdown","3b32fd39":"markdown","b19ac53e":"markdown","8f6b033c":"markdown"},"source":{"193f6deb":"import pandas as pd\nimport numpy as np\nimport random as rnd\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\nimport category_encoders as ce\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","221b0e5a":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","2c30c615":"train_df","16202341":"target = 'Survived'\ny = train_df[target]\ntrain_df = train_df.drop([target], axis = 1)","b63bb15e":"train_df.info()","57bf8714":"numerical_cols = [col for col in train_df.columns if train_df[col].dtype in ['int64', 'float64']]\ncategorical_cols = [col for col in train_df.columns if train_df[col].dtype == \"object\"]","c5a2b956":"categorical_cols","2052933c":"numerical_cols","71218af5":"train_df.isnull().any()","405a86fd":"train_df['Cabin'] = train_df['Cabin'].fillna(value = 'N\/A')\ntrain_df['Age']= train_df['Age'].fillna(train_df['Age'].mean())\ntrain_df['Embarked'] = train_df['Embarked'].fillna(train_df['Embarked'].mode().iloc[0])\n","c0601751":"train_df.isnull().any()","8e1f30b0":"train_df['Name'].value_counts()","d5e4cd51":"train_df['Sex'].value_counts()","ec9e3774":"train_df['Ticket'].value_counts()","c6e02baa":"train_df['Cabin'].value_counts()","72d64b98":"train_df['Embarked'].value_counts()","29e710d0":"#df_cat = train_df[categorical_cols]","6cb2e4fd":"X_train, X_valid, y_train, y_valid = train_test_split(train_df, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","ecd2e5f1":"X_train","dd598190":"categorical_transformer = Pipeline(steps=[\n    ('label', ce.ordinal.OrdinalEncoder())\n])","974eb039":"preprocess = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])","dd3028b8":"clf = LogisticRegression(class_weight='balanced', random_state = 777)","a40761b3":"pipe = Pipeline(steps = [('preprocess', preprocess),\n                               ('clf', clf )])","cacdc53a":"cv_score = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-1, scoring='accuracy')","2b4be239":"pipe.fit(X_train, y_train)","09790347":"y_preds = pipe.predict(X_valid)","2b9aa495":"acc = accuracy_score(y_valid, y_preds)\nprint('CV Mean Accuracy :', cv_score.mean())\nprint('CV Standard Deviation:', cv_score.std())\nprint('Validation Accuracy:', acc)\n","c58c8ab6":"categorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\nclf = LogisticRegression(class_weight='balanced', n_jobs=1, random_state = 777)\n\npipe = Pipeline(steps = [('preprocess', preprocess),\n                               ('clf', clf )])\n\ncv_score = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-1, scoring='accuracy')\n\npipe.fit(X_train, y_train)\n\npreds = pipe.predict(X_valid)\n\nacc = accuracy_score(y_valid, preds)\nprint('CV Mean Accuracy :', cv_score.mean())\nprint('CV Standard Deviation:', cv_score.std())\nprint('Validation Accuracy:', acc)","a37e7119":"categorical_transformer = Pipeline(steps=[\n    ('target', ce.target_encoder.TargetEncoder(handle_unknown='value'))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\nclf = LogisticRegression(class_weight='balanced', n_jobs=1, random_state = 777)\n\npipe = Pipeline(steps = [('preprocess', preprocess),\n                               ('clf', clf )])\n\ncv_score = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-1, scoring='accuracy')\n\npipe.fit(X_train, y_train)\n\npreds = pipe.predict(X_valid)\n\nacc = accuracy_score(y_valid, preds)\nprint('CV Mean Accuracy :', cv_score.mean())\nprint('CV Standard Deviation:', cv_score.std())\nprint('Validation Accuracy:', acc)","4073a939":"categorical_transformer = Pipeline(steps=[\n    ('woe', ce.woe.WOEEncoder(handle_unknown='value'))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\nclf = LogisticRegression(class_weight='balanced', n_jobs=1, random_state = 777)\n\npipe = Pipeline(steps = [('preprocess', preprocess),\n                               ('clf', clf )])\n\ncv_score = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-1, scoring='accuracy')\n\npipe.fit(X_train, y_train)\n\npreds = pipe.predict(X_valid)\n\nacc = accuracy_score(y_valid, preds)\nprint('CV Mean Accuracy :', cv_score.mean())\nprint('CV Standard Deviation:', cv_score.std())\nprint('Validation Accuracy:', acc)","f63d1654":"categorical_transformer = Pipeline(steps=[\n    ('james-stein', ce.james_stein.JamesSteinEncoder(handle_unknown='value'))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\nclf = LogisticRegression(class_weight='balanced', n_jobs=1, random_state = 777)\n\npipe = Pipeline(steps = [('preprocess', preprocess),\n                               ('clf', clf )])\n\ncv_score = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-1, scoring='accuracy')\n\npipe.fit(X_train, y_train)\n\npreds = pipe.predict(X_valid)\n\nacc = accuracy_score(y_valid, preds)\nprint('CV Mean Accuracy :', cv_score.mean())\nprint('CV Standard Deviation:', cv_score.std())\nprint('Validation Accuracy:', acc)","8749d492":"categorical_transformer = Pipeline(steps=[\n    ('james-stein', ce.leave_one_out.LeaveOneOutEncoder(handle_unknown='value'))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\nclf = LogisticRegression(class_weight='balanced', n_jobs=1, random_state = 777)\n\npipe = Pipeline(steps = [('preprocess', preprocess),\n                               ('clf', clf )])\n\ncv_score = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-1, scoring='accuracy')\n\npipe.fit(X_train, y_train)\n\npreds = pipe.predict(X_valid)\n\nacc = accuracy_score(y_valid, preds)\nprint('CV Mean Accuracy :', cv_score.mean())\nprint('CV Standard Deviation:', cv_score.std())\nprint('Validation Accuracy:', acc)","f063e1e5":"categorical_transformer = Pipeline(steps=[\n    ('james-stein', ce.cat_boost.CatBoostEncoder(handle_unknown='value'))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\nclf = LogisticRegression(class_weight='balanced', n_jobs=1, random_state = 777)\n\npipe = Pipeline(steps = [('preprocess', preprocess),\n                               ('clf', clf )])\n\ncv_score = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-1, scoring='accuracy')\n\npipe.fit(X_train, y_train)\n\npreds = pipe.predict(X_valid)\n\nacc = accuracy_score(y_valid, preds)\nprint('CV Mean Accuracy :', cv_score.mean())\nprint('CV Standard Deviation:', cv_score.std())\nprint('Validation Accuracy:', acc)","59a3f753":"The chart below shows a comparison of the different encoders for this set of data. However, it is not fair to say that one is better than the other. They are useful for different things. There are a few important considerations to have about your data before you apply an encoder:\n\n- Have I cleaned my data in a helpful way?\n- What is the cardinality of my features?\n- Which methods are prone to overfitting?\n- How much target leakage is there in my model?\n\nFinally, it is very important to always cross-validate. A model may score well but it can be overfit and thus do very poorly against new data. One-hot and Target encoding have high accuracies here but when cross-validated, they do much more poorly. ","b8057c13":"We have a mix of numerical and categorical columns. We will have to encode the categorical features before putting them in the model.","daf7068e":"Use the model fit to predict on the validation set","46bcd0b9":"## 7. CatBoost Encoding\n\n\"CatBoost coding for categorical features.\n\nThis is very similar to leave-one-out encoding, but calculates the values \u201con-the-fly\u201d. Consequently, the values naturally vary during the training phase and it is not necessary to add random noise.\n\nBeware, the training data have to be randomly permutated. E.g.:\n\n#### Random permutation perm = np.random.permutation(len(X)) X = X.iloc[perm].reset_index(drop=True) y = y.iloc[perm].reset_index(drop=True)\nThis is necessary because some data sets are sorted based on the target value and this coder encodes the features on-the-fly in a single pass.\"\n\nhttps:\/\/contrib.scikit-learn.org\/categorical-encoding\/catboost.html","d8bfee9d":"Cabin has more null values than actual values so I am filling with a new value of N\/A. For Age, I will fill with the mean. For Embarked, I will fill with the most common value.","a805dc02":"5 fold cross-validation on the data.","2047f2cd":"Define the model. Here I am using Logistic Regression that accounts for imbalanced class weights.","3e66659e":"## Import packages \n\nI will be using a mix of the sklearn built in encoders and the encoders in Category Encoders which has implemented additional encoders that can be used easily in sklearn pipelines. \nhttps:\/\/contrib.scikit-learn.org\/categorical-encoding\/index.html","94495fed":"Fit the transforms on the data","16edf454":"We have 3 features with NaNs, (Age, Cabin, Embarked). We will have to deal with these for the model but I will be using simple imputation schemes for this notebook.","0677d8c9":"## 6. Leave One Out(LOO) Encoding \n\n\"Leave one out coding for categorical features.\n\nThis is very similar to target encoding but excludes the current row\u2019s target when calculating the mean target for a level to reduce the effect of outliers.\"\n\nhttps:\/\/contrib.scikit-learn.org\/categorical-encoding\/leaveoneout.html","a8c3bfc2":"Drop the target from the training df","cd405bd1":"This is the set up of the pipeline steps for the encoder. I am not focusing on the numerical values and I have already imputed the null values, so I will pass them in as is.\nThe categorical encoding steps is what I will be changing with the type of encoder specified.","a1baad2c":"## 5. James-Stein Encoding \n\n\"James-Stein estimator.\n\nFor feature value i, James-Stein estimator returns a weighted average of:\n\nThe mean target value for the observed feature value i.\nThe mean target value (regardless of the feature value).\"\n\nhttps:\/\/contrib.scikit-learn.org\/categorical-encoding\/jamesstein.html\n\nThe method essentially is target encoding with smoothing.","da6ce2cc":"# Conclusion","13c3c4c7":"# Set up for encoding and modeling\n\nI will be doing doing a train-test split on the training data (80% for training and 20% for validation). Later I will be doing a 5 fold CV on the training set as well as fitting a model a testing on the validation set. \n\nAlso, I am using sklearn pipelines to streamline the encoding and model fitting process. It is a useful way to use different encodings and also allows for easy application of different encodings to different parts of the data. \n\nFor the model I am using a single Logistic Regression. I will step through the code of the first example to show the flow of the pipeline steps and then combine everything for the other encoders.","b23d7a1d":"### Read in the data","b7740d54":"Set up the pipeline so that it applies the preprocessing\/encoding to the data and then passes it to the model.","80c47b39":"Create lists of numerical and categorical columns that we will need for our later encoding. Our categorical columns are ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']. ","d97a97ff":"## 2. One Hot Encoding \n\nThis method creates a binary feature for each unique value in a feature column. For the example of Sex, it would create one column for Male and one column for Female. This will greatly expand the number of features in your model for high cardinality features. Can be very useful for low cardinality categorical features but will be prone to overfitting for higher cardinality features. This can be seen in the validation score increasing greatly but the CV score decreasing. ","28055eda":"| Encoder | Validation Accuracy|\n| ----------- | ----------- |\n| Label | 0.793296|\n| One-Hot | 0.832402 |\n| Target | 0.826815 |\n| WOE | 0.832402 |\n| James-Stein | 0.770949|\n| Leave-One-Out | 0.793296 |\n| CatBoost | 0.787709 |\n","7867c788":"## 1. Label Encoding\n\nThe first method is the most simple and is very common. You are replacing each category with an integer. This is selected at random by default and is useful for categorical variables with no natural ordering. The encoder used here from Category Encoders can also be used with ordinal features by passing a list with the order of the classes.","316d0c89":"---","be45f3d7":"# What is the big deal with encoding?","fadeae32":"## 4. WOE Encoding \n\nFrom the credit scoring world and another powerful encoding scheme. A calculation of the separation of \"goods\" vs. \"bads\"\n\nWOE = Ln(% of non-events \/ % of events)","8b243c50":"Specifying the column lists used for each transformer. The pipeline will apply the specified transforms to the specified columns in the dataframe. You can adjust this block to add additional encoders to different parts of the data if you wish to encode different types of categorical data differently.","06668d67":"Compare predictions of validation set against the true y-values with accuracy score. This is shown next to mean CV accuracy as well as the CV standard deviation.","bac04e2f":"While a lot of attention seems to be given to the beefiest modeling techniques to tackle datasets in machine learning, there seems to be less emphasis on how to prepare and preprocess categorical data for modeling. Numerical features can be thrown directly into a model as is (for better or for worse) but categorical variables must be encoded into numerical values before the modeling step. Data cleaning, feature engineering, and encoding are all important steps that should undertaken before model selection occurs.\n\nI got the inspiration from this notebook https:\/\/www.kaggle.com\/subinium\/11-categorical-encoders-and-benchmark written by Subin An.\n\nIn this notebook, I will be focusing on different encoding schemes and testing them in a simple model using the Titanic dataset. I will be doing a simple imputation of null values. I will not be doing any additional preprocessing or feature engineering and will be leaving the numerical data as is to focus on the encoding and the effect on model performance.","27a651d7":"It is important to see the cardinality(number of unique values) of each of our categorical variables. This will greatly affect the effectiveness of some of our encoding schemes as we go forward. We can see in the next few cells that Name is unique for every row (891 unique values), Ticket is a high cardinality feature with 681 unique values. Cabin is also high cardinality feature with 147 unique values. Embarked and sex are low cardinality features with 3 and 2 unique values respectively.","3b32fd39":"train-test split ","b19ac53e":"## 3. Target Encoding \n\nA method that calculates values based on prior probablities of the target occuring across the training set. Can be a very good method for higher cardinality features. However this method uses the target in calculation and thus has target leakage built in. This can lead to overfitting and the model performance sinking when compared against new data.","8f6b033c":"## First Look at the data\nWe have 891 rows of data. Broken into 11 columns and a binary target column ('Survived').\n\n"}}