{"cell_type":{"a5480d4a":"code","5b127279":"code","683a7fe3":"code","46454174":"code","aa7b1453":"code","47e34645":"code","07bdf1ed":"code","998d44ee":"code","11e2b984":"code","d460f11d":"code","f80af678":"code","87e995d0":"code","9b50bf5b":"code","b980fcfb":"code","0039633d":"code","5992792b":"code","bf655b8e":"markdown","bbc67644":"markdown","011c5d64":"markdown","f574407b":"markdown","5e3a9241":"markdown","751e74d7":"markdown","fee270b1":"markdown","3aa8ef9f":"markdown","1bb33170":"markdown","378dcdff":"markdown","3139e50b":"markdown","2837f163":"markdown","293409ba":"markdown","47ab4a3a":"markdown","c951a11f":"markdown","a1820e35":"markdown","60610809":"markdown","ef863a37":"markdown","51d1ceb7":"markdown"},"source":{"a5480d4a":"!pip install -q timm pytorch-metric-learning","5b127279":"import os\nimport cv2\nimport copy\nimport time\nimport random\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda import amp\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold\nfrom sklearn.metrics import roc_auc_score, f1_score\n\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport timm\nfrom pytorch_metric_learning import losses","683a7fe3":"class CFG:\n    seed = 42\n    model_name = 'tf_efficientnet_b4_ns'\n    img_size = 512\n    scheduler = 'CosineAnnealingLR'\n    T_max = 10\n    lr = 1e-5\n    min_lr = 1e-6\n    batch_size = 16\n    weight_decay = 1e-6\n    num_epochs = 10\n    num_classes = 11014\n    embedding_size = 512\n    n_fold = 5\n    n_accumulate = 4\n    temperature = 0.1\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","46454174":"TRAIN_DIR = '..\/input\/shopee-product-matching\/train_images\/'\nTEST_DIR = '..\/input\/shopee-product-matching\/test_images\/'","aa7b1453":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n\nset_seed(CFG.seed)","47e34645":"df_train = pd.read_csv('..\/input\/shopee-folds\/folds.csv')\ndf_train['file_path'] = df_train.image.apply(lambda x: os.path.join(TRAIN_DIR, x))\ndf_train.head(5)","07bdf1ed":"le = LabelEncoder()\ndf_train.label_group = le.fit_transform(df_train.label_group)","998d44ee":"class ShopeeDataset(Dataset):\n    def __init__(self, root_dir, df, transforms=None):\n        self.root_dir = root_dir\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.df.iloc[index, -1]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.df.iloc[index, -3]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return img, label","11e2b984":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CFG.img_size, CFG.img_size),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(\n                brightness_limit=(-0.1,0.1), \n                contrast_limit=(-0.1, 0.1), \n                p=0.5\n            ),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CFG.img_size, CFG.img_size),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","d460f11d":"def train_model(model, criterion, optimizer, scheduler, num_epochs, dataloaders, dataset_sizes, device, fold):\n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n    history = defaultdict(list)\n    scaler = amp.GradScaler()\n\n    for step, epoch in enumerate(range(1,num_epochs+1)):\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train','valid']:\n            if(phase == 'train'):\n                model.train() # Set model to training mode\n            else:\n                model.eval() # Set model to evaluation mode\n            \n            running_loss = 0.0\n            \n            # Iterate over data\n            for inputs,labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(CFG.device)\n                labels = labels.to(CFG.device)\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    with amp.autocast(enabled=True):\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n                        loss = loss \/ CFG.n_accumulate\n                    \n                    # backward only if in training phase\n                    if phase == 'train':\n                        scaler.scale(loss).backward()\n\n                    # optimize only if in training phase\n                    if phase == 'train' and (step + 1) % CFG.n_accumulate == 0:\n                        scaler.step(optimizer)\n                        scaler.update()\n                        scheduler.step()\n                        \n                        # zero the parameter gradients\n                        optimizer.zero_grad()\n\n\n                running_loss += loss.item()*inputs.size(0)\n            \n            epoch_loss = running_loss\/dataset_sizes[phase]            \n            history[phase + ' loss'].append(epoch_loss)\n\n            print('{} Loss: {:.4f}'.format(\n                phase, epoch_loss))\n            \n            # deep copy the model\n            if phase=='valid' and epoch_loss <= best_loss:\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                PATH = f\"Fold{fold}_{best_loss}_epoch_{epoch}.bin\"\n                torch.save(model.state_dict(), PATH)\n\n        print()\n\n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss \",best_loss)\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, history","f80af678":"def run_fold(model, criterion, optimizer, scheduler, device, fold, num_epochs=10):\n    valid_df = df_train[df_train.fold == fold]\n    train_df = df_train[df_train.fold != fold]\n    \n    train_data = ShopeeDataset(TRAIN_DIR, train_df, transforms=data_transforms[\"train\"])\n    valid_data = ShopeeDataset(TRAIN_DIR, valid_df, transforms=data_transforms[\"valid\"])\n    \n    dataset_sizes = {\n        'train' : len(train_data),\n        'valid' : len(valid_data)\n    }\n    \n    train_loader = DataLoader(dataset=train_data, batch_size=CFG.batch_size, num_workers=4, pin_memory=True, shuffle=True)\n    valid_loader = DataLoader(dataset=valid_data, batch_size=CFG.batch_size, num_workers=4, pin_memory=True, shuffle=False)\n    \n    dataloaders = {\n        'train' : train_loader,\n        'valid' : valid_loader\n    }\n\n    model, history = train_model(model, criterion, optimizer, scheduler, num_epochs, dataloaders, dataset_sizes, device, fold)\n    \n    return model, history","87e995d0":"model = timm.create_model(CFG.model_name, pretrained=True)\nin_features = model.classifier.in_features\nmodel.classifier = nn.Linear(in_features, CFG.embedding_size)\n\nout = model(torch.randn(1, 3, CFG.img_size, CFG.img_size))\nprint(f'Embedding shape: {out.shape}')\n\nmodel.to(CFG.device);","9b50bf5b":"class SupervisedContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.1):\n        super(SupervisedContrastiveLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, feature_vectors, labels):\n        # Normalize feature vectors\n        feature_vectors_normalized = F.normalize(feature_vectors, p=2, dim=1)\n        # Compute logits\n        logits = torch.div(\n            torch.matmul(\n                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n            ),\n            self.temperature,\n        )\n        return losses.NTXentLoss(temperature=0.5)(logits, torch.squeeze(labels))","b980fcfb":"# Custom Implementation\ncriterion = SupervisedContrastiveLoss(temperature=CFG.temperature).to(CFG.device)\n# criterion = losses.SupConLoss(temperature=CFG.temperature).to(CFG.device)\noptimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr)","0039633d":"model, history = run_fold(model, criterion, optimizer, scheduler, device=CFG.device, fold=0, num_epochs=CFG.num_epochs)","5992792b":"plt.style.use('fivethirtyeight')\nplt.rcParams[\"font.size\"] = \"20\"\nfig = plt.figure(figsize=(22,8))\nepochs = list(range(CFG.num_epochs))\nplt.plot(epochs, history['train loss'], label='train loss')\nplt.plot(epochs, history['valid loss'], label='valid loss')\nplt.ylabel('Loss', fontsize=20)\nplt.xlabel('Epoch', fontsize=20)\nplt.legend()\nplt.title('Loss Curve');","bf655b8e":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Custom Implementation<\/span>","bbc67644":"Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. <strong>Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes.<\/strong> We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations\n\n*Supervised Contrastive Learning*: https:\/\/arxiv.org\/abs\/2004.11362","011c5d64":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Install Libraries<\/span>","f574407b":"Version 5: Loss from Pytorch Metric Learning Library <br>\nVersion 6: Custom Implementation with temperature for NTXentLoss 0.07 <br>\nVersion 7: Custom Implementation with temperature for NTXentLoss 0.5","5e3a9241":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Run Fold 0<\/span>","751e74d7":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Dataset Class<\/span>","fee270b1":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Implementation converted to Pytorch from <a href=\"https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-supervised-contrastive-learning\">this<\/a> amazing notebook<\/span>","3aa8ef9f":"<p style='text-align: center;'><span style=\"color: #000508; font-family: Segoe UI; font-size: 2.6em; font-weight: 300;\">SUPERVISED CONTRASTIVE LEARNING<\/span><\/p>","1bb33170":"![](https:\/\/paperswithcode.com\/media\/methods\/Screen_Shot_2020-06-12_at_12.43.14_PM.png)","378dcdff":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Visualize Training & Validation Metrics<\/span>","3139e50b":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Set Seed for Reproducibility<\/span>","2837f163":"<span style=\"text-align: center; color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Research Paper Walkthrough<\/span>","293409ba":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Training Function<\/span>\n\n<p> Uses Automatic Mixed Precision to speed up training process and Gradient Accumulation to increase batch size<br>\nRefer this <a href=\"https:\/\/www.kaggle.com\/c\/cassava-leaf-disease-classification\/discussion\/199631\">Discussion<\/a> to know more about mixed precision training <br>\nRefer this <a href=\"https:\/\/www.kaggle.com\/c\/cassava-leaf-disease-classification\/discussion\/217133\">Discussion<\/a> to know more about gradient accumulation<\/p>","47ab4a3a":"<style>\n    iframe {display: block; margin: 0 auto;}\n<\/style>\n\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/MpdbFLXOOIw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>","c951a11f":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Import Packages<\/span>","a1820e35":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Training Configuration<\/span>","60610809":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Load Model<\/span>","ef863a37":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Augmentations & Transforms<\/span>","51d1ceb7":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)"}}