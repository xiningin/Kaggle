{"cell_type":{"dcf0b032":"code","362d9cb2":"code","22fbf86c":"code","4f2bb872":"code","c9fceeae":"code","bba1e7ed":"code","eb2755c1":"code","45d47ea6":"code","67018ce8":"code","dbfc7d59":"code","dac62133":"code","a10ee008":"code","0c205d25":"code","ae495c43":"code","94aaa2a7":"code","64163b76":"code","045bbe43":"code","89399d69":"code","f1c910fd":"code","4a3bcf11":"code","90237ecc":"code","4ed9294f":"code","c2fa5a7f":"code","4d4d4735":"code","cbb80ee5":"code","cd30d8a8":"code","f771751b":"code","364b60e2":"code","8bf51233":"code","290a5e23":"code","dd998c43":"code","01bbea43":"code","592ee85f":"code","a676ce1f":"code","fc06d8e2":"code","a5d341c4":"code","404e19d4":"code","ccbe5e44":"code","13855da5":"code","e2fd94e1":"code","fb919be9":"code","93ef6112":"code","1418d37c":"code","71849200":"code","a3019670":"code","a452028f":"code","247e493c":"code","9ab5a772":"markdown","2233f983":"markdown","655d5b1c":"markdown","7a594de4":"markdown","e9bb579c":"markdown","92d5130e":"markdown","6ed96c2a":"markdown","0b8aa492":"markdown","2be230b9":"markdown","c0848184":"markdown","e9582674":"markdown","04f6a41c":"markdown","6dbcaf35":"markdown","0778e3e8":"markdown","cd6695c6":"markdown"},"source":{"dcf0b032":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","362d9cb2":"import seaborn as sns\nimport math","22fbf86c":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","4f2bb872":"train.head()","c9fceeae":"test.head()","bba1e7ed":"train.info()","eb2755c1":"#starting values\nstartprobyes=len(train[train['Survived']==1].index)\/len(train.index)\nstartprobno=len(train[train['Survived']==0].index)\/len(train.index)\nEntropyStart=-startprobyes*math.log2(startprobyes)-startprobno*math.log2(startprobno)\nEntropyStart","45d47ea6":"women=train.loc[train.Sex =='female']['Survived']\nrate_women=sum(women)\/len(women)\nprint(\"% of women who survived:\", rate_women)","67018ce8":"men = train.loc[train.Sex == 'male']['Survived']\nrate_men = sum(men)\/len(men)\nprint(\"% of men who survived:\", rate_men)","dbfc7d59":"sns.countplot(x='Survived',data=train,hue='Sex')","dac62133":"train=train.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\ntest=test.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)","a10ee008":"#look for missing data in train set\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","0c205d25":"isnull=train.isnull()\nisnull.value_counts()","ae495c43":"train['Embarked'].value_counts()","94aaa2a7":"#fill embarked isnull values with the mode which is 'S'\ntrain=train.fillna({'Embarked':'S'})","64163b76":"train=train.fillna(train.mean())\ntest=test.fillna(test.mean())","045bbe43":"train.info()","89399d69":"test.info()","f1c910fd":"#determine minimum and maximum age\nprint(train['Age'].min())\nprint(train['Age'].max())","4a3bcf11":"splits=pd.DataFrame(columns=['Split Location', 'Entropy Split', 'IG'])\ntrainyes=train[train['Survived']==1]\ntrainno=train[train['Survived']==0]\nfor i in range(1,80):\n    ES=0\n    IG=0\n    countLT=train[train['Age']<=i].shape[0]\n    weightedcountLT=countLT\/train.shape[0]\n    countGT=train[train['Age']>i].shape[0]\n    weightedcountGT=countGT\/train.shape[0]\n    pSurviveLT=len(trainyes[trainyes['Age']<=i].index)\/len(train[train['Age']<=i])\n    pNoSurviveLT=len(trainno[trainno['Age']<=i].index)\/len(train[train['Age']<=i])\n    pSurviveGT=len(trainyes[trainyes['Age']>i].index)\/len(train[train['Age']>i])\n    pNoSurviveGT=len(trainno[trainno['Age']>i].index)\/len(train[train['Age']>i])\n    \n    #calculate entropy for the less than values\n    if pSurviveLT!=0 and pNoSurviveLT!=0:\n        ESLT=-pSurviveLT*math.log2(pSurviveLT)-pNoSurviveLT*math.log2(pNoSurviveLT)\n    elif pSurviveLT==0:\n        ESLT=-pNoSurviveLT*math.log2(pNoSurviveLT)\n    elif pNoSurviveLT==0:\n        ESLT=-pSurviveLT*math.log2(pSurviveLT)\n        \n     #calculate entropy for greater than values\n    if pSurviveGT!=0 and pNoSurviveGT!=0:\n        ESGT=-pSurviveGT*math.log2(pSurviveGT)-pNoSurviveGT*math.log2(pNoSurviveGT)\n    elif pSurviveGT==0:\n        ESGT=-pNoSurviveGT*math.log2(pNoSurviveGT)\n    elif pNoSurviveGT==0:\n        ESGT=-pSurviveGT*math.log2(pSurviveGT)\n    \n    #total entropy of split\n    ES=weightedcountLT*ESLT+weightedcountGT*ESGT\n    \n    IG=EntropyStart-ES\n    splits=splits.append({'Split Location':i,'Entropy Split': ES, 'IG':IG},ignore_index=True)","90237ecc":"splits[splits['IG']==splits['IG'].max()]","4ed9294f":"#create a function to change \"Age\" to \">6\" or \"<=6\"\ndef Age(x):\n    if x>6:\n        return('>6')\n    elif x<=6:\n        return('<=6')","c2fa5a7f":"train['Age']=train['Age'].apply(lambda x: Age(x))\ntest['Age']=test['Age'].apply(lambda x: Age(x))","4d4d4735":"#verify function worked\ntrain['Age'].value_counts()","cbb80ee5":"sns.countplot(x='Survived',data=train,hue='Age')","cd30d8a8":"#determine the range for Fare\nprint(train['Fare'].min())\nprint(train['Fare'].max())","f771751b":"splits=pd.DataFrame(columns=['Split Location', 'Entropy Split', 'IG'])\ntrainyes=train[train['Survived']==1]\ntrainno=train[train['Survived']==0]\nfor i in range(1,512):\n    ES=0\n    IG=0\n    countLT=train[train['Fare']<=i].shape[0]\n    weightedcountLT=countLT\/train.shape[0]\n    countGT=train[train['Fare']>i].shape[0]\n    weightedcountGT=countGT\/train.shape[0]\n    pSurviveLT=len(trainyes[trainyes['Fare']<=i].index)\/len(train[train['Fare']<=i])\n    pNoSurviveLT=len(trainno[trainno['Fare']<=i].index)\/len(train[train['Fare']<=i])\n    pSurviveGT=len(trainyes[trainyes['Fare']>i].index)\/len(train[train['Fare']>i])\n    pNoSurviveGT=len(trainno[trainno['Fare']>i].index)\/len(train[train['Fare']>i])\n    \n    #calculate entropy for the less than values\n    if pSurviveLT!=0 and pNoSurviveLT!=0:\n        ESLT=-pSurviveLT*math.log2(pSurviveLT)-pNoSurviveLT*math.log2(pNoSurviveLT)\n    elif pSurviveLT==0:\n        ESLT=-pNoSurviveLT*math.log2(pNoSurviveLT)\n    elif pNoSurviveLT==0:\n        ESLT=-pSurviveLT*math.log2(pSurviveLT)\n        \n     #calculate entropy for greater than values\n    if pSurviveGT!=0 and pNoSurviveGT!=0:\n        ESGT=-pSurviveGT*math.log2(pSurviveGT)-pNoSurviveGT*math.log2(pNoSurviveGT)\n    elif pSurviveGT==0:\n        ESGT=-pNoSurviveGT*math.log2(pNoSurviveGT)\n    elif pNoSurviveGT==0:\n        ESGT=-pSurviveGT*math.log2(pSurviveGT)\n    \n    #total entropy of split\n    ES=weightedcountLT*ESLT+weightedcountGT*ESGT\n    \n    IG=EntropyStart-ES\n    splits=splits.append({'Split Location':i,'Entropy Split': ES, 'IG':IG},ignore_index=True)","364b60e2":"splits[splits['IG']==splits['IG'].max()]","8bf51233":"#change \"Fare\" to \">11\" or \"<=11\"\ndef Fare(x):\n    if x>11:\n        return('>11')\n    elif x<=11:\n        return('<=11')","290a5e23":"train['Fare']=train['Fare'].apply(lambda x: Fare(x))\ntest['Fare']=test['Fare'].apply(lambda x: Fare(x))","dd998c43":"#determine function worked\ntrain['Fare'].value_counts()","01bbea43":"sns.countplot(x='Fare',data=train,hue='Survived')","592ee85f":"for col in train.columns:\n    print(col)\n    print(train[col].value_counts())","a676ce1f":"def SibSpParch(x):\n    if x>1:\n        return ('>1')\n    elif x==1:\n        return('1')\n    else:\n        return('0')","fc06d8e2":"train['SibSp']=train['SibSp'].apply(lambda x:SibSpParch(x))\ntrain['Parch']=train['Parch'].apply(lambda x:SibSpParch(x))\ntest['SibSp']=test['SibSp'].apply(lambda x:SibSpParch(x))\ntest['Parch']=test['Parch'].apply(lambda x:SibSpParch(x))","a5d341c4":"#check out the head to see what dataframe looks like now\ntrain.head(2)","404e19d4":"def fun(col,dataframe):\n    i=0\n    ES=0\n    IG=0\n    dfyes=dataframe[dataframe['Survived']==1]\n    dfno=dataframe[dataframe['Survived']==0]\n    weightedcount=0\n    for x in dataframe[col].unique():\n        weightedcount=len(dataframe[dataframe[col]==x].index)\/len(dataframe.index)\n        probabilityyes=len(dfyes[dfyes[col]==x].index)\/len(dataframe[dataframe[col]==x].index)\n        probabilityno=len(dfno[dfno[col]==x].index)\/len(dataframe[dataframe[col]==x].index)\n        if probabilityyes!=0 and probabilityno!=0:\n                ES=ES-weightedcount*(probabilityyes*math.log2(probabilityyes)+probabilityno*math.log2(probabilityno))\n        elif probabilityyes==0:\n                ES=ES-weightedcount*(0+probabilityno*math.log2(probabilityno))\n        elif probabilityno==0:\n                ES=ES-weightedcount*(probabilityyes*math.log2(probabilityyes)+0)\n    IG=EntropyStart-ES\n    return (ES,IG)","ccbe5e44":"decisiontree=pd.DataFrame(columns=['Split #','Split Location','TreeLoc','NumBranch','LHS Split','LHS Split Answer', 'LHS Split #Entries', 'MidSplit1','MidSplit1 Answer', 'MidSplit1 #Entries','RHS Split','RHS Split Answer','RHS Split #Entries'])","13855da5":"for i in range(1,4):   \n    splits=pd.DataFrame(columns=['Split Location', 'Entropy Split', 'IG'])\n    if i==1:\n        dataframe=train\n    elif i==2:\n        dataframe=trainleft\n    else:\n        dataframe=trainright\n    for col in dataframe.drop('Survived',axis=1).columns:\n        splits=splits.append({'Split Location':col,'Entropy Split':fun(col,dataframe)[0], 'IG': fun(col,dataframe)[1]},ignore_index=True)\n    SplitLoc=splits[splits['IG']==splits['IG'].max()]['Split Location'].item()    \n    #Add a row to decision tree classifier DF\n    numbranch=dataframe[SplitLoc].nunique()\n    if numbranch==2:\n        LHSSplit=dataframe[SplitLoc].unique()[0]\n        RHSSplit=dataframe[SplitLoc].unique()[1]\n        MidSplit=None\n        midAnswer=None\n        midEntries=None\n    elif numbranch==3:\n        LHSSplit=dataframe[SplitLoc].unique()[0]\n        MidSplit=dataframe[SplitLoc].unique()[1]\n        RHSSplit=dataframe[SplitLoc].unique()[2]\n        #only need mid answer and mid entries if numbranch==3\n        if i==3:\n            #special case where there is an error on the mode, half the people survived in class 3 and half do not, we will say in third class they do not survive\n            midAnswer=0\n        else:\n            midAnswer=dataframe[dataframe[SplitLoc]==MidSplit]['Survived'].mode().item()\n        midEntries=dataframe[dataframe[SplitLoc]==MidSplit]['Survived'].count().item()\n    \n    LHSAnswer=dataframe[dataframe[SplitLoc]==LHSSplit]['Survived'].mode().item()\n    RHSAnswer=dataframe[dataframe[SplitLoc]==RHSSplit]['Survived'].mode().item()\n    LHSEntries=dataframe[dataframe[SplitLoc]==LHSSplit]['Survived'].count().item()\n    RHSEntries=dataframe[dataframe[SplitLoc]==RHSSplit]['Survived'].count().item()\n    \n    #determine location of tree split\n    if i==1:\n        TreeLoc='First Split'\n        #setting up for next 2 rounds\n        # it is okay to use df and not dataframe because this is the first split\n        trainleft=train[train[SplitLoc]==LHSSplit].drop(SplitLoc,axis=1) \n        trainright=train[train[SplitLoc]==RHSSplit].drop(SplitLoc,axis=1)\n    elif i==2:\n        TreeLoc='LHS Split'\n    else:\n        TreeLoc='RHS Split'\n    \n    decisiontree=decisiontree.append({'Split #':i, 'Split Location':SplitLoc,'TreeLoc':TreeLoc,\n                                      'NumBranch': numbranch,'LHS Split':LHSSplit,'LHS Split Answer':LHSAnswer,\n                                      'LHS Split #Entries':LHSEntries,'MidSplit1':MidSplit, \n                                      'MidSplit1 Answer': midAnswer,'MidSplit1 #Entries':midEntries,\n                                      'RHS Split':RHSSplit,'RHS Split Answer':RHSAnswer,\n                                      'RHS Split #Entries':RHSEntries},ignore_index=True)","e2fd94e1":"decisiontree","fb919be9":"#create a function to classify\ndef classify(test_df):\n    firstsplit=decisiontree.iloc[0,1]\n    firstsplitLHS=decisiontree.iloc[0,4]\n    firstsplitmid=decisiontree.iloc[0,7]\n    firstsplitRHS=decisiontree.iloc[0,10]\n    LHSSplit=decisiontree.iloc[1,1]\n    LHSSplitLHS=decisiontree.iloc[1,4]\n    LHSSplitmid=decisiontree.iloc[1,7]\n    LHSSplitRHS=decisiontree.iloc[1,10]\n    RHSSplit=decisiontree.iloc[2,1]\n    RHSSplitLHS=decisiontree.iloc[2,4]\n    RHSSplitmid=decisiontree.iloc[2,7]\n    RHSSplitRHS=decisiontree.iloc[2,10]\n    \n    #split one\n    df1=test_df[(test_df[firstsplit]==firstsplitLHS) & (test_df[LHSSplit]==LHSSplitLHS)]\n    df2=test_df[(test_df[firstsplit]==firstsplitLHS) & (test_df[LHSSplit]==LHSSplitmid)]\n    df3=test_df[(test_df[firstsplit]==firstsplitLHS) & (test_df[LHSSplit]==LHSSplitRHS)]\n    df4=test_df[(test_df[firstsplit]==firstsplitRHS) & (test_df[RHSSplit]==RHSSplitLHS)]\n    df5=test_df[(test_df[firstsplit]==firstsplitRHS) & (test_df[RHSSplit]==RHSSplitmid)]\n    df6=test_df[(test_df[firstsplit]==firstsplitRHS) & (test_df[RHSSplit]==RHSSplitRHS)]\n    \n    \n    Answerdf=pd.DataFrame(columns=['Answer'])\n    Answer1=decisiontree.iloc[1,5]\n    Answer2=decisiontree.iloc[1,8]\n    Answer3=decisiontree.iloc[1,11]\n    Answer4=decisiontree.iloc[2,5]\n    Answer5=decisiontree.iloc[2,8]\n    Answer6=decisiontree.iloc[2,11]\n    #initialize new column\n    \n    if len(df1.index)!=0:\n        df1.loc[:,'Answer']=\"\"\n        df1.loc[:,'Answer']=Answer1\n    \n    if len(df2.index)!=0:\n        df2.loc[:,'Answer']=\"\"\n        df2.loc[:,'Answer']=Answer2\n    if len (df3.index)!=0:\n        df3.loc[:,'Answer']=\"\"\n        df3.loc[:,'Answer']=Answer3\n    if len(df4.index)!=0:\n        \n        df4.loc[:,'Answer']=\"\"\n        df4.loc[:,'Answer']=Answer4\n    if len(df5.index)!=0:\n        df5.loc[:,'Answer']=\"\"\n        df5.loc[:,'Answer']=Answer5\n        \n        \n    if len(df6.index)!=0:\n        df6.loc[:,'Answer']=\"\"\n        df6.loc[:,'Answer']=Answer6\n    \n    Answerdf=pd.concat([df1,df2,df3,df4,df5,df6],axis=0)\n    \n    Answerdf=Answerdf['Answer']\n    Answerdf=Answerdf.sort_index(axis=0)\n    return Answerdf","93ef6112":"classified=classify(test)","1418d37c":"#redefine test to get passengerID back\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","71849200":"output=pd.DataFrame({'PassengerID':test.PassengerId,'Survived':classified})","a3019670":"output.head()","a452028f":"output['Survived'].value_counts()","247e493c":"output.to_csv('my_submission.csv',index=False)\nprint(\"Your submission was successfully saved!\")","9ab5a772":"Splitting fare on fare=11 gives us the greatest infomation gain","2233f983":"find age where it makes the most sense to split based off of information gain","655d5b1c":"Data exploration for new Age variable","7a594de4":"Classify test data","e9bb579c":"Dividing age into whether or not the child is over 6 gives us the greatest information gain","92d5130e":"Data Exploration","6ed96c2a":"Create decision tree\n","0b8aa492":"Repeat the same process for fare","2be230b9":"Verify there is no remaining null objects in train or test set","c0848184":"Hello All! I am new to data science, this is an attempt to create a decision tree from scratch without using the tools from Scikit learn. I did this mostly to improve my own understanding. Any insight for ways to improve my decision tree or additional resources would be helpful. Thanks!","e9582674":"Data exploration for new fare variable","04f6a41c":"Create a function to return ES,IG","6dbcaf35":"Obtain starting probability of survival, starting probability of not surviving, and the starting entropy of training dataset","0778e3e8":"Remove irrelevant columns, look for missing data","cd6695c6":"Decision tree can only work with a maximum of three variables per column\nSplit SibSp and Parch into categories \"0\",\"1\" or \">1\""}}