{"cell_type":{"b397f597":"code","26582ecc":"code","848a7820":"code","07445264":"code","aa725f61":"code","2e57a0ba":"code","63feec68":"code","51717919":"code","c0c56217":"code","4b76a31f":"code","dbdb2658":"code","884c3ac5":"code","75bd7b62":"code","38245323":"code","cdc3c7c7":"code","cd859e43":"code","d0db535a":"code","04fb52a5":"code","3191f393":"code","3de472d2":"markdown","5f2eb0a0":"markdown"},"source":{"b397f597":"from tensorflow.keras.preprocessing.text import Tokenizer\n\nsentence_list = [\n                 \"I Love machine learning\",\n                 \"I love deep learning\",\n                 \"We are learning deep learning\"\n]","26582ecc":"tokenizer = Tokenizer(num_words = 50)\ntokenizer.fit_on_texts(sentence_list)\nword_index = tokenizer.word_index\nprint(word_index)","848a7820":"sentence_list = [\n                 \"I Love machine learning\",\n                 \"I love deep learning\",\n                 \"We are learning deep learning\",\n                 \"We are learning from best trainer\"\n]\n\ntokenizer = Tokenizer(num_words = 50)\ntokenizer.fit_on_texts(sentence_list)\nword_index = tokenizer.word_index\nprint(word_index)\n\nsequences = tokenizer.texts_to_sequences(sentence_list)\nfor s in sequences : print(sequences)","07445264":"new = [\"The world is screwed, so are we\"]\nseq = tokenizer.texts_to_sequences(new)\nprint(seq)","aa725f61":"sentence_list = [\n                 \"I Love machine learning\",\n                 \"I love deep learning\",\n                 \"We are learning deep learning\",\n                 \"We are learning from best trainer\"\n]\n\ntokenizer = Tokenizer(num_words = 50,oov_token = \"#OOv\")\ntokenizer.fit_on_texts(sentence_list)\nword_index = tokenizer.word_index\nprint(word_index)\n\nsequences = tokenizer.texts_to_sequences(sentence_list)\nfor s in sequences : print(s)","2e57a0ba":"new = [\"The world is screwed, so are we\"]\nseq = tokenizer.texts_to_sequences(new)\nprint(s)","63feec68":"from tensorflow.keras.preprocessing.sequence import pad_sequences\npadded_seq = pad_sequences(sequences, padding = 'post')\nfor s in padded_seq : print(s)","51717919":"import tensorflow_datasets as tfds\nimdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)","c0c56217":"import numpy as np\n\ntrain_data, test_data = imdb['train'], imdb['test']\n\ntraining_sentences = []\ntraining_labels = []\n\ntesting_sentences = []\ntesting_labels = []\n\nfor s,l in train_data:\n  training_sentences.append(s.numpy().decode('utf8'))\n  training_labels.append(l.numpy())\n\nfor s,l in test_data:\n  testing_sentences.append(s.numpy().decode('utf8'))\n  testing_labels.append(l.numpy())\n\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","4b76a31f":"count = 1\nfor s in training_sentences:\n  if count>5 : break\n  print(s,end = '\\n\\n\\n')\n  count +=1","dbdb2658":"count = 1\nfor l in training_labels:\n  if count>5 : break\n  print(l,end = '\\n\\n')\n  count +=1","884c3ac5":"print(len(training_sentences))\nprint(len(training_labels))","75bd7b62":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 120\ntruc_type = 'post'\noov_token = \"#OOV\"","38245323":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\n#print(sequences)\npadded = pad_sequences(sequences,maxlen=max_length,truncating=truc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen = max_length)","cdc3c7c7":"model = tf.keras.Sequential([\n                             tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length = max_length),\n                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n                             tf.keras.layers.Dense(16,activation='relu'),\n                             tf.keras.layers.Dense(1,activation='sigmoid')\n\n])","cd859e43":"model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","d0db535a":"history = model.fit(padded,training_labels_final,epochs = 6,validation_data = (testing_padded,testing_labels_final))","04fb52a5":"import matplotlib.pyplot as plt\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel(\"Epochs\")\nplt.ylabel('accuracy')\nplt.legend(['accuracy', 'val_accuracy'])\nplt.show()","3191f393":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel(\"Epochs\")\nplt.ylabel('loss')\nplt.legend(['loss', 'val_loss'])\nplt.show()","3de472d2":"# Tokenization in NLP\n### Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n\n\n## Why is Tokenization required in NLP?\n\n### Before processing a natural language, we need to identify the words that constitute a string of characters. That\u2019s why tokenization is the most basic step to proceed with NLP (text data). This is important because the meaning of the text could easily be interpreted by analyzing the words present in the text.\n\n### Let\u2019s take an example. Consider the below string:\n### \u201cThis is a cat.\u201d\n\n### What do you think will happen after we perform tokenization on this string? We get [\u2018This\u2019, \u2018is\u2019, \u2018a\u2019, cat\u2019].\n\n### There are numerous uses of doing this. We can use this tokenized form to:\n### Count the number of words in the text\n### Count the frequency of the word, that is, the number of times a particular word is present\n### And so on. We can extract a lot more information which we\u2019ll discuss in detail in future articles. For now, it\u2019s time to dive into the meat of this article \u2013 the different methods of performing tokenization in NLP.","5f2eb0a0":"# Training RNN with IMDB dataset\n### Recurrent Neural Networks (RNNs) are a form of machine learning algorithm that are ideal for sequential data such as text, time series, financial data, speech, audio, video among others.\n\n### An RNNs is essentially a fully connected neural network that contains a refactoring of some of its layers into a loop. That loop is typically an iteration over the addition or concatenation of two inputs, a matrix multiplication and a non-linear function."}}