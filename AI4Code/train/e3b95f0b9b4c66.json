{"cell_type":{"364358b4":"code","610a2e59":"code","51d69b0b":"code","4c4160b7":"code","0f19efbb":"code","cd5bf072":"code","68b2c35a":"code","09f8247e":"code","015198af":"code","a0e2229d":"code","65010b56":"code","57d3f01f":"code","f74e42af":"code","5e533926":"code","e38c2c0a":"code","236e0267":"code","4ae660ac":"code","53f563e3":"code","00ed018d":"code","51fc9a1b":"code","f0ae9546":"code","57617db2":"code","5bca03c4":"code","9bb3b91f":"markdown","af84e51a":"markdown","b19080e1":"markdown","00da674d":"markdown","eff81671":"markdown","80ce35a9":"markdown"},"source":{"364358b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# importing neccesary libraries\n\n%matplotlib inline\nimport sqlite3\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.preprocessing.text import Tokenizer\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc,accuracy_score,f1_score\nfrom sklearn import metrics\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nfrom nltk.stem.porter import PorterStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nimport os\nfile_names = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_names.append(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","610a2e59":"# using the sqlite table to read data\ncon = sqlite3.connect([file for file in file_names if file.endswith('sqlite')][0])\n\n# filtering only positive and negative reviews i.e\n# not taking into consideration those reviews with score =3\nfiltered_data = pd.read_sql_query(\"select * from Reviews where Score !=3\",con)\n\n# give reviews with score>3 a positive rating, and <3 a negative\ndef partition(x):\n    if x<3:\n        return 'negative'\n    return 'positive'\n# changing reviews with score<3 to be positive and vice versa\nactualScore=filtered_data['Score']\npositiveNegative = actualScore.map(partition)\nfiltered_data['Score'] = positiveNegative","51d69b0b":"filtered_data.shape \nfiltered_data.head()","4c4160b7":"# checking the if \n# display = pd.read_sql_query('select * from Reviews group by UserId having count(distinct ProductId)>1', con)\n# display\nfiltered_data[filtered_data.duplicated(subset={'UserId', 'ProfileName', 'Time', 'Text'}, keep='first')]","0f19efbb":"# sorting data according to ProductId in ascending order\nsorted_data = filtered_data.sort_values('ProductId', axis=0, ascending=True)","cd5bf072":"# deduplication of entries\nfinal = sorted_data.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Text'}, keep='first',inplace=False)\nfinal.shape","68b2c35a":"# checking to see how much % of data still remains\n(final['Id'].size)\/(filtered_data['Id'].size)*100","09f8247e":"display = pd.read_sql_query('select * from Reviews where Score!=3 and HelpfulnessNumerator>HelpfulnessDenominator', con)\ndisplay","015198af":"final = final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\nfinal","a0e2229d":"print(final.shape)\n\n# how many positive and negative reviews are present in our dataset?\nfinal['Score'].value_counts()","65010b56":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = set(stopwords.words('english')) # set of stopwords\nsno = nltk.stem.SnowballStemmer('english') # initializing the snowball stemmer\n\n# function to clean teh word in html tags\ndef cleanhtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr,' ',sentence)\n    return cleantext\n# function to clean the word of any punctuation\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]', r' ',cleaned)\n    return cleaned\nprint(stop)\nprint('***********************************************************************************************************')\nprint(sno.stem('tasty'))","57d3f01f":"# code for implementing step by step the checks mentioned in the preprocessing \n# this code takes a while to run as it needs to run on 500k sentences.\ni=0\nstr1= ' '\nfinal_string = []\nall_positive_words = []\nall_negative_words =[]\nfor sent in final['Text'].values:\n    filtered_sentence = []\n    sent = cleanhtml(sent)\n    # remove html tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if ((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n                if (cleaned_words.lower() not in stop):\n                    s = (sno.stem(cleaned_words.lower()))#.encode('utf-8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(s)\n                    if (final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s)\n                else:\n                    continue\n            else:\n                continue\n    str1 = \" \".join(filtered_sentence) # final string of cleaned words\n    \n    final_string.append(str1)\n    i+=1\n    ","f74e42af":"filtered_sentence","5e533926":"final['Cleaned_text'] = final_string # adding the new column after cleaning the text","e38c2c0a":"final.head(3)\n\n# store the final table into a SQLite table for future\nconn = sqlite3.connect('final.sqlite')\nc = conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews', conn, schema=None, if_exists='replace')","236e0267":"final['Score'] = final['Score'].apply(lambda x:1 if x=='positive' else 0)","4ae660ac":"train_df, test_df = train_test_split(final, test_size = 0.2, random_state = 42)\nprint(\"Training data size : \", train_df.shape)\nprint(\"Test data size : \", test_df.shape)","53f563e3":"type(train_df['Cleaned_text'][0])","00ed018d":"top_words = 5000\ntokenizer = Tokenizer(num_words=top_words)\ntokenizer.fit_on_texts(train_df['Cleaned_text'])\nlist_tokenized_train = tokenizer.texts_to_sequences(train_df['Cleaned_text'])\n\nmax_review_length = 200\nX_train = pad_sequences(list_tokenized_train, maxlen=max_review_length)\ny_train = train_df['Score']","51fc9a1b":"# create the model\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(GRU(100))\n# model.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","f0ae9546":"\n\nmodel.fit(X_train,y_train, nb_epoch=10, batch_size=64, validation_split=0.2)\n\n","57617db2":"list_tokenized_test = tokenizer.texts_to_sequences(test_df['Text'])\nX_test = pad_sequences(list_tokenized_test, maxlen=max_review_length)\ny_test = test_df['Score']\nprediction = model.predict(X_test)\ny_pred = (prediction > 0.5)\nprint(\"Accuracy of the model : \", accuracy_score(y_pred, y_test))\nprint('F1-score: ', f1_score(y_pred, y_test))\nprint('Confusion matrix:')\nconfusion_matrix(y_test,y_pred)\n","5bca03c4":"prediction","9bb3b91f":"# LSTSM APPLY","af84e51a":"# Cleaning the text ","b19080e1":"# Data cleaning : Deduplication","00da674d":"# Loading Data ","eff81671":"# Text Preprocessing: Stemming, Stop-Words removal and Lemmatization.\n### Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.","80ce35a9":"## ***Observation***: it was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not pratically possible hence these two rows too are removed from calculations"}}