{"cell_type":{"b4165adc":"code","b3b0bb05":"code","690381ec":"code","26c2ce3c":"code","63b0eeba":"code","af62b6f9":"code","7fc99f39":"code","ce5045ed":"code","8c683e46":"code","e9ab6a61":"code","872a4c50":"code","90b37b2b":"code","c31c1863":"code","39dd19ae":"code","888c119e":"code","06c984c7":"code","ff75d744":"code","4da911e0":"code","3f443f36":"code","ab0e05a6":"code","f5e7bdf9":"code","a0cd84c8":"code","93876f9c":"code","abbd4812":"code","58345a9e":"code","7c99acbb":"code","3f431137":"code","6ccf4065":"markdown","e44490b3":"markdown","a8b404a8":"markdown","c49c274d":"markdown","56079bd4":"markdown","facaf34b":"markdown","0d4846aa":"markdown","ccb6fc6e":"markdown","66336a3f":"markdown","d000bbc6":"markdown","4a5bd29d":"markdown","a436ced1":"markdown","f055fa60":"markdown","f91747d5":"markdown"},"source":{"b4165adc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b3b0bb05":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport re as re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor, XGBClassifier\nimport shap\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv', header = 0, dtype={'Age': np.float64})\ntest  = pd.read_csv('..\/input\/titanic\/test.csv' , header = 0, dtype={'Age': np.float64})\nfull_data = [train, test]\n\nprint (train.info())\ntrain.head()","690381ec":"def feat_analysis(feat_name, train):\n    df_survived = train[[feat_name, 'Survived']].groupby([feat_name], as_index=True).mean()\n    df_count = train[[feat_name, 'Survived']].groupby([feat_name], as_index=True).count()\n    df_count['Survived'] = df_count['Survived']\/df_count['Survived'].sum()\n    df_count.columns = ['count']\n\n    df_plot = pd.merge(df_survived, df_count, left_index=True, right_index=True)\n    df_plot = df_plot*100\n#     print(df_plot)\n#     plt.figure()\n    df_plot.plot(kind='bar')\n    plt.legend(['survival rate', 'proportion'])\n    plt.xticks(rotation=0)\n    plt.xlabel(df_plot.columns[0])\n    plt.xlabel('')\n    plt.ylabel('%')\n    plt.title(feat_name)\n    plt.show()","26c2ce3c":"# Class\nfeat_name = 'Pclass'\nfeat_analysis(feat_name, train)\n\nfor dataset in full_data:\n    dataset['Has_Cabin'] = (dataset['Cabin']==dataset['Cabin'])*1\n    \nfeat_name = 'Has_Cabin'\nfeat_analysis(feat_name, train)","63b0eeba":"# Sex\nfor dataset in full_data:\n    dataset['Sex_code'] = 1*(dataset['Sex']=='female')\n\n    \nfeat_name = 'Sex'\nfeat_analysis(feat_name, train)","af62b6f9":"## Show the categories of Age and Fare\nprint(pd.qcut(train['Fare'].fillna(dataset['Fare'].median()), 5).unique())\nprint(pd.qcut(train['Age'].fillna(dataset['Age'].median()), 5).unique())","7fc99f39":"# Age & Fare\nfor dataset in full_data:\n    dataset['Age'] = dataset['Age'].fillna(-10)\n    dataset['Age_code'] = 0\n    dataset['Age_code'].loc[(dataset['Age']>=0) & (dataset['Age']<16)] = 1\n    dataset['Age_code'].loc[(dataset['Age']>=16) & (dataset['Age']<32)] = 2\n    dataset['Age_code'].loc[(dataset['Age']>=32) & (dataset['Age']<48)] = 3\n    dataset['Age_code'].loc[(dataset['Age']>=48) & (dataset['Age']<64)] = 4\n    dataset['Age_code'].loc[dataset['Age']>=64] = 5\n    \n    \n    dataset['Fare'] = dataset['Fare'].fillna(-10)\n    dataset['Fare_code'] = 0\n    dataset['Fare_code'].loc[(dataset['Fare']>=0) & (dataset['Fare']<7.85)] = 1\n    dataset['Fare_code'].loc[(dataset['Fare']>=7.85) & (dataset['Fare']<10.5)] = 2\n    dataset['Fare_code'].loc[(dataset['Fare']>=10.5) & (dataset['Fare']<21.67)] = 3\n    dataset['Fare_code'].loc[(dataset['Fare']>=21.67) & (dataset['Fare']<39.68)] = 4\n    dataset['Fare_code'].loc[dataset['Fare']>=39.68] = 5\n    \nfeat_name = 'Age_code'\nfeat_analysis(feat_name, train)\n\nfeat_name = 'Fare_code'\nfeat_analysis(feat_name, train)","ce5045ed":"# SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset.loc[dataset['FamilySize']>=8, 'FamilySize'] = 8\n    \n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n    \nfeat_name = 'FamilySize'\nfeat_analysis(feat_name, train)\n\nfeat_name = 'IsAlone'\nfeat_analysis(feat_name, train)","8c683e46":"# Embarked port\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    dataset['Embarked_code'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\nfeat_name = 'Embarked'\nfeat_analysis(feat_name, train)","e9ab6a61":"# Name\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in full_data:\n    dataset['Name_length'] = dataset['Name'].apply(len)    \n    dataset['Name_length_code'] = 0\n    dataset['Name_length_code'].loc[(dataset['Name_length']>=0) & (dataset['Name_length']<20)] = 1\n    dataset['Name_length_code'].loc[(dataset['Name_length']>=20) & (dataset['Name_length']<24)] = 2\n    dataset['Name_length_code'].loc[(dataset['Name_length']>=24) & (dataset['Name_length']<27)] = 3\n    dataset['Name_length_code'].loc[(dataset['Name_length']>=27) & (dataset['Name_length']<33)] = 4\n    dataset['Name_length_code'].loc[dataset['Name_length']>=33] = 5\n    \n    \n    dataset['Title'] = dataset['Name'].apply(get_title)\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \n    dataset['Title_code'] = 0\n    dataset['Title_code'].loc[dataset['Title']=='Master']=1\n    dataset['Title_code'].loc[dataset['Title']=='Miss']=2\n    dataset['Title_code'].loc[dataset['Title']=='Mr']=3\n    dataset['Title_code'].loc[dataset['Title']=='Mrs']=4\n    dataset['Title_code'].loc[dataset['Title']=='Rare']=5\n\n    \nfeat_name = 'Title_code'\nfeat_analysis(feat_name, train)\n\nfeat_name = 'Name_length_code'\nfeat_analysis(feat_name, train)\n\n# print(pd.qcut(train['Name_length'],5).unique())","872a4c50":"target_name = 'Survived'\nfeature_list = ['Pclass', 'Sex_code', 'Age_code',  'Fare_code','Embarked_code', \n                'Name_length_code', 'Has_Cabin', 'FamilySize', 'IsAlone', 'Title_code']","90b37b2b":"from sklearn.model_selection import cross_val_score\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n\nX = train[feature_list]\ny = train[target_name]\ndef hyperopt_train_test(params):\n    clf = XGBClassifier(**params)\n    return cross_val_score(clf, X, y, cv=5).mean()\n\nxgb_reg_params = {\n    'learning_rate':    hp.choice('learning_rate',    np.arange(0.002, 0.1, 0.002)),\n    'max_depth':        hp.choice('max_depth',        np.arange(1, 8, 1, dtype=int)),\n    'min_child_weight': hp.choice('min_child_weight', np.arange(0, 8, 1, dtype=int)),\n    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n    'subsample':        hp.uniform('subsample', 0.7, 1),\n    'n_estimators':     hp.choice('n_estimators', np.arange(50, 600, 10, dtype=int)),\n    'objective':        'reg:squarederror',\n    'seed':             24\n}\n\ndef objective_f(params):\n    acc = hyperopt_train_test(params)\n    return {'loss': -acc, 'status': STATUS_OK}\ntrials = Trials()\n# You can use increase max_evals for more iteration\nbest = fmin(objective_f, xgb_reg_params, algo=tpe.suggest, max_evals=50, trials=trials)\nprint(f'Best XGBoost:\\n{space_eval(xgb_reg_params, best)}')","c31c1863":"X, y = train[feature_list], train[target_name]\nxgb_reg_params = {'colsample_bytree': 0.5, 'learning_rate': 0.068, 'max_depth': 3, \n                  'min_child_weight': 3, 'n_estimators': 440, 'objective': 'reg:squarederror', \n                  'seed': 24, 'subsample': 0.8163574781326463}\npred_model = XGBRegressor(**xgb_reg_params)\n# pred_model = xgb.XGBClassifier(**xgb_reg_params)\n\nmodel_name = 'XGBoost'\npred_model.fit(X, y)\ny_pred = pred_model.predict(X)","39dd19ae":"# margin means real contribution to the predition\nexplainer = shap.TreeExplainer(pred_model, model_output='margin')\nshap_values = explainer.shap_values(X)\ndf_shap = X.copy()\ndf_shap.loc[:,:] = shap_values\n\nshap.summary_plot(shap_values, X)","888c119e":"shap.summary_plot(shap_values, X, plot_type=\"bar\")","06c984c7":"shap.dependence_plot(\"Pclass\", shap_values, X, interaction_index=\"Sex_code\")","ff75d744":"for feat in feature_list:\n    shap.dependence_plot(feat, shap_values, X, interaction_index='auto')\n","4da911e0":"shap_interaction_values = shap.TreeExplainer(pred_model).shap_interaction_values(X)\nshap.summary_plot(shap_interaction_values, X)","3f443f36":"from shap.common import hclust_ordering\nfrom sklearn.manifold import TSNE, MDS\nimport seaborn as sns\n\nnum_start_end = [200, 290] # Orange line position\n\nhierarchical_order = hclust_ordering(shap_values, metric=\"sqeuclidean\")\nid_to_study = hierarchical_order[num_start_end[0]:num_start_end[1]]\n\nplt.figure(figsize=(10,3))\nplt.plot(y_pred[hierarchical_order])\nplt.axvline(x=num_start_end[0], color='orangered')\nplt.axvline(x=num_start_end[1], color='orangered')\nplt.title(f'Prediction sorted by hierarchical similarity of SHAP values', fontsize=20)","ab0e05a6":"shap_TSNE = TSNE(n_components=2).fit_transform(df_shap)\n\n\ndf_shap_Clustering = np.zeros(len(X))\ndf_shap_Clustering[id_to_study] = 1\n\nfor i in np.unique(df_shap_Clustering):\n    plt.scatter(shap_TSNE[:,0][df_shap_Clustering==i],shap_TSNE[:,1][df_shap_Clustering==i], s=10)\nplt.title('TSNE')\nplt.show()","f5e7bdf9":"from plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=False)\nimport plotly.graph_objs as go\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\ndf=X\ndf_scaled = StandardScaler().fit_transform(df)\n\n\ndf = train[feature_list]\ncategories = df_shap.columns\n\nlist_color = [\"#0058b7\", \"#fc8500\", \"green\", \"red\"]\nlist_mode = ['lines', 'lines', 'markers', 'markers']\n\ndata = []\n#for i in range(len(list_name)):\ntrace = go.Scatterpolar(\n    r=df_scaled.mean(axis=0),\n    theta=categories,\n    fill='toself',\n    name=f'''Total ({len(df_shap_Clustering)} \/ 100% people): \\\n        {train[target_name].mean(): .2} \\\n        ''',\n    opacity=1)\ndata.append(trace)\n\nfor i in range(1,2):\n    trace = go.Scatterpolar(\n        r=df_scaled[df_shap_Clustering==i].mean(axis=0),\n        theta=categories,\n        fill='toself',\n#         name= f'''Type {i} ({(df_shap_Clustering==i).sum()} \/{(df_shap_Clustering==i).mean()*100: .3}% people): \\\n#         {train[target_name][df_shap_Clustering==i].mean(): .2} \\\n#         ''',\n        name= f'''Orange passengers ({(df_shap_Clustering==i).sum()} \/{(df_shap_Clustering==i).mean()*100: .3}% people): \\\n        {train[target_name][df_shap_Clustering==i].mean(): .2} \\\n        ''',\n        opacity=0.8)\n    data.append(trace)\n\n\nlayout = {\n    \n}\n\nfigure = go.Figure(\n    data = data,\n    layout = layout\n)\niplot(figure)","a0cd84c8":"## See orange paseengers\ntrain.iloc[sorted(id_to_study),:13]","93876f9c":"personal_info = np.array([\n    [1  , 1  , 2  , 5  , 0  , 4  , 1  , 4  , 0  , 2],\n    [3  , 0  , 2  , 1  , 0  , 4  , 0  , 1  , 1  , 3],\n    [1  , 0  , 2  , 5  , 0  , 4  , 1  , 4  , 0  , 3],\n    [2  , 0  , 2  , 2  , 0  , 4  , 1  , 2  , 0  , 3],\n                       ])\n\ndf_test = pd.DataFrame(personal_info, index=['Rose','Jack', \"Fiance\", \"Jack_test\"], columns=feature_list)\ndf_shap_test = df_test.copy()\ndf_shap_test[feature_list] = explainer.shap_values(df_test)","abbd4812":"shap.initjs()\nname = 'Rose'\nshap.force_plot(explainer.expected_value, \n                df_shap_test.loc[name,:].values,\n                df_test.loc[name,:],\n               )","58345a9e":"name = 'Jack'\nshap.force_plot(explainer.expected_value, \n                df_shap_test.loc[name,:].values,\n                df_test.loc[name,:],\n               )","7c99acbb":"name = 'Fiance'\nshap.force_plot(explainer.expected_value, \n                df_shap_test.loc[name,:].values,\n                df_test.loc[name,:],\n               )","3f431137":"shap.initjs()\nname = 'Jack_test'\nshap.force_plot(explainer.expected_value, \n                df_shap_test.loc[name,:].values,\n                df_test.loc[name,:],\n               )","6ccf4065":"There is also an interaction shap value, which is less intuitive, if you are interested, you can read this [paper](https:\/\/arxiv.org\/pdf\/1802.03888.pdf).","e44490b3":"### 3.4.2 How to save Jack?\nIs there any solution could help Jack survive? According to the above study, higher class and fare can help survive, we could expect that Jack have a better chance and won a 2nd class ticket and his fare level is 2 (not the lowest level), moreover, we should consider that he is not alone, because he has a friend in the movie. We make a prediction with his new profile.\n\n* We find that Jack\u2019s survival chance increased a little, although this is still a very low probability, facing a disaster, any improvement in survival probability is invaluable. ","a8b404a8":"### 3.4.2 Jack\nConversely, the XGBoost model has an extremely negative prediction for Jack because he is male (Mr), in third class, and he is 20 years old. ","c49c274d":"## 3.4 Case explanation -- why Rose is survival but Jack is dead?\nFinally, to demonstrate how the SHAP explicate the prediction for the individual cases, we reproduce personal information about the Rose, Jack and Rose's fianc\u00e9 based on the movie Titanic (1997)\n\n**Personal information:**\n\nrole        | Rose | Jack | Fianc\u00e9 |\n------      |------|------|--------|\nTitle       | Miss | Mr   | Mr     |\nSex         | F    | M    | M      |\nPclass      | 1    | 3    | 1      |\nAge         | 17   | 20   | 24     |\nFare        | highest | lowest   | highest    |\nFamily size | 4    | 1    | 4      |\nEmbarked    | S    | S    | S      |\n\n**Feature table:**\n\nName | Pclass | Sex_code | Age_code | Fare_code | Embarked_code | Name_length_code | Has_Cabin | FamilySize | IsAlone | Title_code\n------ | ------ | ------ | ------ | ------ | ------ | -------- | ------ | ------ | -------- | -------- | \nRose | 1 | 1 | 2 | 5 | 0 | 4 | 1 | 4 | 0 | 2\nJack | 3 | 0 | 2 | 1 | 0 | 4 | 0 | 1 | 1 | 3\nFinanc\u00e9 | 1 | 0 | 2 | 5 | 0 | 4 | 1 | 4 | 0 | 3","56079bd4":"# Step1 --  Data cleaning and feature engineering\nThe data include the surviving results of 891 passengers and their personal information. After the data cleaning, we choose the gender, class, fare, age, family size, special title of the name, and embarked port as our features. ","facaf34b":"# Context\n\n### In this notebook we will use Shap to explore the data from the Titanic, then explain why Rose is alive from the disaster but Jack is not, and finally we will try to find a solution to increase Jack's survival chnace.\n\nIn Titanic\u2019s film (1997), 1st class passenger Rose, her fianc\u00e9 and mother embarked at Southampton, began their journey to New York. At the same time, a penniless young artist Jack won a 3rd class ticket in a poker game and boarded the ship at the last minute. At the end of this love story, Rose and her fianc\u00e9 survive, but Jack doesn't. Why it happened, let's explain the reason using the SHAP model.\n\n[SHAP (SHapley Additive exPlanations) model](https:\/\/github.com\/slundberg\/shap) was proposed by Scott Lundberg which is a powerful model can explain any machine learning model. The core of SHAP model is to calculate SHAP value. The SHAP value, in simple terms, is the contribution of one feature for making the one prediction.\n\nTo facilitate this interpretation demo, we will train an XGBoost regressor to predict the survival probability of passengers, the prediction results are between 0 and 1, 0 means dead and 1 means survival.\n\nFor a more detailed analysis, please see: [Why Rose survived from Titanic but Jack did not\u2014\u2014an explanation given by SHAP](https:\/\/medium.com\/@leoclementliao\/why-rose-survived-from-titanic-but-jack-did-not-an-explanation-given-by-shap-5519b2b5cbdd)\n","0d4846aa":"## 3.2 Feature dependence explication\nShap also support plot cross-influence from two features. From the Following dependence figure, we find that from the first class to the third class, both males (blue) and females (red), their survival advantages have been weakened, but this weakening is particularly evident in women. We can think that because the male survival rate is already low, the impact of class on male survival is not so sensible. However, the female survival rate is much higher, therefore, whether the woman is in the third-class becomes an important condition for females' survival. So we can say that the surviving bias due to the class is more pronounced in females.","ccb6fc6e":"You can also try other features couple","66336a3f":"# Step2 -- Fine tuning XGBoost parameters and train\n\nWe can use hyperopt to fine tune the prediction model, if you have better model for this prediciton you can also use your own model or parameters","d000bbc6":"### 3.4.1 Rose\nFor Rose, the model predicts that its survival is very high. SHAP tells us that the result is based on the fact that she is a woman (Miss), has a cabin in the first class, her fare is high.","4a5bd29d":"### 3.4.3 Finac\u00e9\nThe prediction of fianc\u00e9\u2019s survival chance is 2 times higher than Jack, because although he is a male with the age of 24 years which are disadvantages for surviving, his cabine in the first-class, high fare compensating these negative effects.","a436ced1":"## 3.3 Supervised clustering using shap value (hierarchical agglomerative)\nWe can also cluster the shap values to find the latent information in the data.","f055fa60":"The follwing figures show the prediction results (output value) from the XGBoost model and the explanations from SHAP. The base value is the overall average survival chance. (It means the prediction that we can make when we do not know any feature values of one passenger. ) The red feature values push the prediction to the positive direction, and the blue features do the inverse.","f91747d5":"# Step3 -- Shap explainer\n## 3.1 Global explication\n\nIn the following figure, each point represents a passenger, the horizontal axis is SHAP value (the contribution of this feature to the survival probability). The color of the point indicates the feature value across the entire feature range. For example, the blue points in age represent children and the red point is elder. The features are sorted from top to bottom in order of decreasing importance. From the figure we can see some results that match the intuition such as females (red) have survival advantage; the higher the class, the more survival advantage; children (blue) have more survival advantage. We find also some interesting phenomena like: people with family members have a survival advantage over alone, but if there are too many family members, it is worse than alone."}}