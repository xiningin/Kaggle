{"cell_type":{"4c7bdc3b":"code","d747799a":"code","ebe9c864":"code","43ca4171":"code","c6e646ea":"code","97fd9cd4":"code","f6479266":"code","c36434e6":"code","51b1f973":"code","a6ebc011":"code","2748c35c":"code","dbc1320e":"code","b05ff354":"code","26a7ab11":"code","ab51d4e9":"code","366fc179":"code","73088102":"code","3eb85d04":"markdown","3f191d9c":"markdown","2cb1b9da":"markdown","c87b2619":"markdown","e5d1832c":"markdown","51b41240":"markdown","65b4b307":"markdown","3c866014":"markdown","c62e5905":"markdown","89138d16":"markdown","48d79c84":"markdown","108554bc":"markdown","17c77484":"markdown","06e5c0c0":"markdown","5fd152bc":"markdown","1d05e88d":"markdown","d7e7e4ba":"markdown","3ada9a4d":"markdown","78a968cb":"markdown"},"source":{"4c7bdc3b":"# Install custom library from Github\n!pip install -q --no-warn-conflicts git+https:\/\/github.com\/MrMimic\/covid-19-kaggle\n\nfrom c19 import parameters, database_utilities, text_preprocessing, embedding, query_matching, clusterise_sentences, plot_clusters, display_output\n\n# Ugly dependencies warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d747799a":"import os\n\nparams = parameters.Parameters(\n    first_launch=True,\n    database=parameters.Database(\n        local_path=\"local_database.sqlite\",\n        kaggle_data_path=os.path.join(os.sep, \"kaggle\", \"input\", \"CORD-19-research-challenge\"),\n        only_newest=True,\n        only_covid=True\n    ),\n    preprocessing=parameters.PreProcessing(\n        max_body_sentences=0,\n        stem_words=False\n    ),\n    query=parameters.Query(\n        cosine_similarity_threshold=0.8,\n        minimum_sentences_kept=500,\n        number_of_clusters=\"auto\",\n        k_min=3,\n        k_max=10,\n        min_feature_per_cluster=100\n    )\n)","ebe9c864":"database_utilities.create_db_and_load_articles(\n    db_path=params.database.local_path,\n    kaggle_data_path=params.database.kaggle_data_path,\n    first_launch=params.first_launch,\n    only_newest=params.database.only_newest,\n    only_covid=params.database.only_covid,\n    enable_data_cleaner=params.database.enable_data_cleaner)","43ca4171":"embedding_model = embedding.Embedding(\n    parquet_embedding_path=params.embedding.local_path,\n    embeddings_dimension=params.embedding.dimension,\n    sentence_embedding_method=params.embedding.word_aggregation_method,\n    weight_vectors=params.embedding.weight_with_tfidf)","c6e646ea":"text_preprocessing.pre_process_and_vectorize_texts(\n    embedding_model=embedding_model,\n    db_path=params.database.local_path,\n    first_launch=params.first_launch,\n    stem_words=params.preprocessing.stem_words,\n    remove_num=params.preprocessing.remove_numeric,\n    batch_size=params.preprocessing.batch_size,\n    max_body_sentences=params.preprocessing.max_body_sentences)","97fd9cd4":"full_sentences_db = query_matching.get_sentences_data(\n    db_path=params.database.local_path)","f6479266":"query = \"What do we know about hydroxychloroquine to treat covid-19 disease?\"","c36434e6":"closest_sentences_df = query_matching.get_k_closest_sentences(\n    query=query,\n    all_sentences=full_sentences_db,\n    embedding_model=embedding_model,\n    minimal_number_of_sentences=params.query.minimum_sentences_kept,\n    similarity_threshold=params.query.cosine_similarity_threshold)","51b1f973":"closest_sentences_df = database_utilities.get_df_pagerank_by_doi(\n    db_path=params.database.local_path, df=closest_sentences_df)","a6ebc011":"closest_sentences_df = clusterise_sentences.perform_kmean(\n    k_closest_sentences_df=closest_sentences_df,\n    number_of_clusters=params.query.number_of_clusters,\n    k_min=params.query.k_min,\n    k_max=params.query.k_max,\n    min_feature_per_cluster=params.query.min_feature_per_cluster\n)","2748c35c":"plot_clusters.scatter_plot(\n    closest_sentences_df=closest_sentences_df,\n    query=query)","dbc1320e":"display_output.create_html_report(\n    query=query,\n    closest_sentences_df=closest_sentences_df,\n    top_x=2,\n    db_path=params.database.local_path)","b05ff354":"query = \"How neonates and pregnant women are susceptible of developing covid-19?\"\n\nclosest_sentences_df = query_matching.get_k_closest_sentences(\n    query=query,\n    all_sentences=full_sentences_db,\n    embedding_model=embedding_model,\n    minimal_number_of_sentences=params.query.minimum_sentences_kept,\n    similarity_threshold=params.query.cosine_similarity_threshold)\n\nclosest_sentences_df = database_utilities.get_df_pagerank_by_doi(\n    db_path=params.database.local_path, df=closest_sentences_df)\n\nclosest_sentences_df = clusterise_sentences.perform_kmean(\n    k_closest_sentences_df=closest_sentences_df,\n    number_of_clusters=params.query.number_of_clusters,\n    k_min=params.query.k_min,\n    k_max=params.query.k_max,\n    min_feature_per_cluster=params.query.min_feature_per_cluster\n)\n\nplot_clusters.scatter_plot(\n    closest_sentences_df=closest_sentences_df,\n    query=query)","26a7ab11":"display_output.create_html_report(\n    query=query,\n    closest_sentences_df=closest_sentences_df,\n    top_x=2,\n    db_path=params.database.local_path)","ab51d4e9":"query = \"Are smoking or pre-existing pulmonary disease (lung) risk factors for developing covid-19?\"\n\nclosest_sentences_df = query_matching.get_k_closest_sentences(\n    query=query,\n    all_sentences=full_sentences_db,\n    embedding_model=embedding_model,\n    minimal_number_of_sentences=params.query.minimum_sentences_kept,\n    similarity_threshold=params.query.cosine_similarity_threshold)\n\nclosest_sentences_df = database_utilities.get_df_pagerank_by_doi(\n    db_path=params.database.local_path, df=closest_sentences_df)\n\nclosest_sentences_df = clusterise_sentences.perform_kmean(\n    k_closest_sentences_df=closest_sentences_df,\n    number_of_clusters=params.query.number_of_clusters,\n    k_min=params.query.k_min,\n    k_max=params.query.k_max,\n    min_feature_per_cluster=params.query.min_feature_per_cluster\n)\n\nplot_clusters.scatter_plot(\n    closest_sentences_df=closest_sentences_df,\n    query=query)","366fc179":"display_output.create_html_report(\n    query=query,\n    closest_sentences_df=closest_sentences_df,\n    top_x=2,\n    db_path=params.database.local_path)","73088102":"query = \"Which is the cell entry receptor for SARS-cov-2?\"\n\nparams.query.number_of_clusters = 1\n\nclosest_sentences_df = query_matching.get_k_closest_sentences(\n    query=query,\n    all_sentences=full_sentences_db,\n    embedding_model=embedding_model,\n    minimal_number_of_sentences=params.query.minimum_sentences_kept,\n    similarity_threshold=params.query.cosine_similarity_threshold)\n\nclosest_sentences_df = database_utilities.get_df_pagerank_by_doi(\n    db_path=params.database.local_path, df=closest_sentences_df)\n\nclosest_sentences_df = clusterise_sentences.perform_kmean(\n    k_closest_sentences_df=closest_sentences_df,\n    number_of_clusters=params.query.number_of_clusters,\n    k_min=params.query.k_min,\n    k_max=params.query.k_max,\n    min_feature_per_cluster=params.query.min_feature_per_cluster\n)\n\nprint(f\"\\nAnswer to the query: {query}\")\nprint(closest_sentences_df.sort_values(by=\"distance\", ascending=False).head(1)[\"raw_sentence\"][0])","3eb85d04":"### Simple question\n\nThis tool can also be used to answer simple question, wich do not need any opinion.\n\nJust set the number of cluster to one and get the first line of the sorted resulting dataframe.","3f191d9c":"\n\n## Intro\n\nThis notebook is the result of the collaborative work of a group of engineers at Atos\/Bull.\n\nOur goal was to **overcomes the problem of quickly finding different opinions** about a given subjet. In fact, it can be very difficult to quickly get reliable information: many different points of view are represented in the medias as well as in the scientific litterature.\n\nInstead of simply returning the most closest sentences to the query, we chose to **extract the diferent opinions**, which can be shared by the different groups of people working on a subject.\n\nWe wanted to provide a tool easily reusable, reproducible and easy to understand as a Python library installable from Github.\n\n![Overview](https:\/\/raw.githubusercontent.com\/MrMimic\/covid-19-kaggle\/master\/images\/kaggle_covid.png)\n\n## How it works \n\n### Database creation\n\nAll titles, abstracts and body texts of the dataset are [inserted into an SQLite DB](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/src\/main\/python\/c19\/database_utilities.py#L186) (only english articles for the moment).\n\nThey are preprocessed by using the [method we developed](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/src\/main\/python\/c19\/text_preprocessing.py#L21). It will lower and stem the text, remove stopwords, remove numeric values, split texts into sentences and sentences into words.\n\nA [word2vec](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html) embedding and a [TF-IDF](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html) models have been trained on this pre-processed corpus. Briefly, these models allow to get a fixed-length vector of numeric values to represent each word of the corpus (word2vec) and to weight each word regarding it's frequency among all the corpus and in each document (TF-IDF). The result is a parquet table, [stored on Github](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/resources\/global_df_w2v_tfidf.parquet), containing for each word a float vector and a TF-IDF score.\n\n![Table header](https:\/\/raw.githubusercontent.com\/MrMimic\/covid-19-kaggle\/master\/images\/header_w2v_tfidf.jpg \"Table header\")\n\nThis embedding can be re-generated in more or less 30 minutes on a 8 vCPU machine by using [this script](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/src\/main\/scripts\/train_w2v.py).\n\nEach sentence from the corpus is pre-processed and [vectorized](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/src\/main\/python\/c19\/embedding.py#L65). To do so, each pre-processed word from a sentence is represented by its vector and weithed by the TF-IDF score. Then, all vectors from the different words composing the sentence are averaged ([Mean of Word Embeddings](https:\/\/books.google.fr\/books?id=tBxrDwAAQBAJ&pg=PA95&lpg=PA95&dq=mean+of+word+embedding+MOWE&source=bl&ots=7laX_HWKS0&sig=ACfU3U2DvGwGI6Bs4HTkX0_oP7Nf3UTP2A&hl=en&sa=X&ved=2ahUKEwiXguOJ9tjoAhX3D2MBHS6mAzoQ6AEwCnoECA0QKA#v=onepage&q=mean%20of%20word%20embedding%20MOWE&f=false)). All these pre-processed sentences are [stored in base](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/src\/main\/python\/c19\/text_preprocessing.py#L151).\n\n### Query matching\n\nThe query is first [vectorised](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/src\/main\/python\/c19\/query_matching.py#L57) by using the same strategy and tool as explained above. The cosine similarity of this sentence [versus all stored sentences](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/src\/main\/python\/c19\/query_matching.py#L127) vectors is then computed. Briefly, it allows to check how each sentence of the dataset is close from the query. Only the top-k sentences are returned (filtered by minimal distance).\n\nAll these top-k closest sentences are then clusterised by a Kmean algorithm. These clusters will represent the different *opinions* found about the query.\n\nAll the closest sentence from each centroid is highlited (*ie*, the sentence reflecting the most the opinion on this subject) and an HTML (or markdown) report about closest sentences and most relevant papers is written.\n\n## What's cool\n\n- Code is documented, cleaned, PEP8 complient and installable as a Python library. It is hosted on a public Github repository, allowing people to collaborate to the devlopment.\n- The trained embedding is not generic. Even if pre-trained models found on the Internet work well, the context of covid-19 and the kind of sentences to be processed make a locally trained embedding better.\n- Code is optimisez for RAM and rapid processing. Only the resulting DB built on all articles weights gigabytes.\n- The solution is highly portable (even on mobile with less sentences for example) due to the usage of SQLite.\n- Scripts to reproduce experiments (create the DB, perform a query, retrain W2V, generate pagerank score or perform multi queries) are published on the Github and usable out-of-the-box.\n\n## What's not\n\n- The database containing all sentences weights more than 20Go. It is thus unusable on Kaggle. To overcome this issue, we first randomly selected 10 sentences from the body of all articles. Then, we add two parameters instead *only_newest* and *only_covid* instead which select respectively only articles published since 2019 or with an abstract containing a synonym of \"covid\".\n- Clusters quelity is highly dependent of the number of cluster. Once automatically estimated, it sometimes \"split an opinion\" into two distinct clusters which should have been merged.\n- The highly-specific embedding also has cons. Many words from queries have not been found enough in the corpus to be kept by the embedding model. Maybe the solution is to use a generic pre-trained embedding and to update it on our data.\n\n## What's next\n\nVersion 2.0 of this work will be released before the April, 15th. To come:\n\n- ~~Auto-estimate K for the number of opinions.~~\n- ~~Maybe some interactive figures.~~\n- ~~Ranking best papers from opinions clusters regarding the authors and their background.~~\n- ~~Uploading the citation network as a directed graphe in Kaggle datasets.~~\n\n**And during the round #2, we would to develop:**\n\n- Create an interactive tool, hosted online for instant query.\n- Auto-test the code on Github with unitary tests on the methods to ensure quelity of the code.\n- A multi-lingual search (maybe with trained embedding on different languages instead of just translating the query).\n- Use a larger pre-trained embedding (on the same corpus but maybe with some data augmentation from PubMed on the given subjects).\n- Auto update of the newly published scientific litterature with a link to the Pubmed API.\n\n## List of tricks\n\n- Only newest or covid-related articles are used in Kaggle, the resulting database is way too large.\n- Two regexs are applied on the texts: one transforming every possible way of writing \"Coronavirus-2019\" into \"COVID-19\", the other one transforming \"Corona Virus\" into \"Coronavirus\".\n- We filter non-english articles for the moment, so they are not in the dabatase.\n\n## Usage\n\nQueries from the different tasks have been reformulated (or raw task if not) and [stored in Github](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/resources\/queries.json). All of them have been sent to the pipeline and the result are stored in markdown format [here](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/resources\/executed_queries.md) (queries have been matched versus the corpus released on April 3th which was not updated since on our devlopment workstations).\n\nBut we think a manual analyse of the refults reflects the most how this tool can be used.\n\nFor this notebook, we will focus on several questions and try to answer them by using our tool.","2cb1b9da":"Keep in mind that these results are coming from a very limited number of papers (published since 01\/01\/2019) and the entire set will run on the web app.\n\nMore analyses to come before the end of round 1, with nice output and a ranking based on the citation network of the papers from each cluster, and other cool features! Check the git!\n\nDon't hesitate to try the tool !\n\nStay safe !","c87b2619":"All sentences are finally loaded in RAM to be matched versus the user query.","e5d1832c":"After filtering the closest sentences, we retrieve from our database the pagerank score of their corresponding papers. This will add a column pagerank in the dataframe.","51b41240":" **Bigger symbols correspond to the nearest sentence for each cluster's centroid**.\n\nIt can be seen as the most representative sentence for this given opinion.\n\n> Note that the legend is interactive and you can show\/hide clusters or sections\n\nAnd finally, let's output a pretty HTML report about these clusters.","65b4b307":"Now, let's plot these clusters as an interactive scatter plot.","3c866014":"We construct the database by loading all titles, abstracts and bodies (please remember that articles are filtered on Kaggle and they are not all loaded into the database).\n\nWe also build up a knowledge graph using each article and it's citations, to build up a citation network of the dataset. Then we applied the [pagerank](https:\/\/en.wikipedia.org\/wiki\/PageRank) algorithm on the network to get a score for each article. The higher the pagerank score is, the more important the article is in the network (regular citations by others, etc). This is used later to return the most imporant papers of an opinion cluster. **The ones that might be the most useful for a scientist to read**.","c62e5905":"# Opinions extraction tool: several examples","89138d16":"Results are clustered, with the number of clusters *K* being auto-computed by [Silhouette Score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.silhouette_score.html). Then, sentences are ranked to extract the most representative sentence for each cluster.\n\n> Please note that for this example, the number of cluster is set to \"auto\". If the query you want to run has an expected number of \"opinions\", you can replace the \"auto\" by the desired number of cluster. For example, in a yes\/no question, you can set the number of cluster to two.\n\nThe \"auto\" argument of the number of clusters for the clustering algorithm can add computing time to the query. If too long, tweak the following parameters to get less close sentences or define a number of clusters manually.\n\n- params.query.minimum_sentences_kept\n- query.cosine_similarity_threshold\n\nThis algorithm can sometimes design clusters of one or two sentences. To prevent this to happen, use the **min_feature_per_cluster** argument. The number of clusters will decrease gradually until all clusters contain this minimal amount of sentences (until it reach *min_cluster* argument value).","48d79c84":"The database is ready to be used.\n\n### Analyse: opinions about Chloroquine\n\nFor now, we will just illustrate how our tool can be use for the chloroquine case study.\n\nIf you forked the notebook, you can just relaunch from this cell when you change the query.","108554bc":"************************\n\n## Results example\n\n> *What do we know about Chloroquine to treat covid-19?*\n\n#### Opinions found in the article corpus:\n\n- (47 sentences) claiming that it is under study and we still need to investigate.\n- (91 sentences) saying that indeed, Chloroquine has a proven effect.\n- (118 sentences) providing another treatment or advise against taking chloroquine at the moment.\n\n************************","17c77484":"Then, the parameters are loaded ([full explaination of the parameters](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/src\/main\/python\/c19\/parameters.py)). \n\n*Parameters* class returns default configuration which can be customised as follow.\n\nThe *first_launch* parameter allows to create the database instead of just loading it.","06e5c0c0":"#### Analyses results\n\nHere, we could say that:\n\n- A cluster <span style=\"color:red\"><b>represents sentences claiming that it is under study and we still need to investigate<\/b><\/span>.\n- Another cluster would <span style=\"color:red\"><b>represent sentences saying that indeed, Chloroquine has a proven effect<\/b><\/span>. \n- A last cluster <span style=\"color:red\"><b>represents sentences providing another treatment or advise against taking chloroquine at the moment<\/b><\/span>.\n\nAssesing clusters in advance (pre-commit) is quite hard on Kaggle due to the ID of the cluster which can change for each launch. However, seeds have been fixed internally for reproducible KMeans.\n\n### Task neonates \/ pregnancy risk factor\n\nLet's analyse another question.","5fd152bc":"The sentences are pre-processed, vectorised and inserted into the SQLite database.","1d05e88d":"The cosine similarity between each sentence and the query will be computed.\n\nThe minimal number of sentences to keep is defined by the **minimal_number_of_sentences** parameters.\n\nThis number will influence the **similarity_threshold** if this limit to not return enough sentences. It will be incidcated by this kind of log:\n\n> Similarity threshold lowered from 0.8 to 0.79 due to minimal number of sentence constraint.","d7e7e4ba":"The pre-trained embeddings are loaded from Github (scipt file to re-train it available [here](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/83747070e1f63777c542f25df3a46b9a248ff68f\/src\/main\/scripts\/train_w2v.py)).\n\nThe embedding model can now return words vectors (which can be weighted by TF-IDF scores).","3ada9a4d":"### Setup\n\nThe library can be easily [installed from github](https:\/\/github.com\/MrMimic\/covid-19-kaggle\/blob\/master\/setup.py) by using [pip](https:\/\/pypi.org\/project\/pip\/).","78a968cb":"<span style=\"color:red\"><b>Here, we see two clusters, one not categorical about risk, other saying that there is no risk<\/b><\/span>.\n\n### Task tobacco as a risk factor"}}