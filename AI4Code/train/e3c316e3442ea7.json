{"cell_type":{"920b77d4":"code","c0ec0fb9":"code","7db21d6f":"code","11830603":"code","f54c93ad":"code","d7b3fc9d":"code","0339ad4f":"code","898688bc":"code","7a7ff7c0":"code","0e4a5d91":"code","fc8d6005":"code","110373e2":"code","6ad7912c":"code","41043a2e":"code","bca22285":"code","aa751979":"code","2b612b81":"code","9108b6be":"code","a8ba5881":"code","bbd6732e":"code","dc07d8f1":"code","10c6272c":"code","b637e223":"code","797d60cd":"code","ebbc3a4f":"code","5c1658de":"code","2fc65ed9":"code","a6348e71":"code","e1cf30a2":"code","78ab0bba":"code","d42d111c":"code","751b35dd":"code","08f24e22":"code","17f144f4":"code","8a302e13":"code","ad4e467b":"code","3b13b5a9":"code","7ffce17f":"code","0920d20a":"code","207c8ab7":"code","208a4794":"code","69b3b254":"code","08fbf150":"code","71649380":"code","dcb09ef8":"code","6030b28c":"code","93d53eae":"code","4aacc983":"code","b08fb982":"code","471d7554":"code","9066c876":"code","c71207b9":"code","f80652cc":"code","62b4bc25":"code","84fdd7a9":"code","afc64697":"code","61af06e5":"code","b2b0c00b":"code","3e0586b0":"code","d7bc83a0":"code","96453999":"code","111a66c8":"code","963ba3c1":"code","7e2acc28":"code","982b472a":"code","cb64a100":"markdown","ce56e08e":"markdown","72a93866":"markdown","b604ed7b":"markdown","3b12d4b8":"markdown","0c1092a5":"markdown","3d1a457d":"markdown","1a55abe0":"markdown","94741708":"markdown","85aeb258":"markdown","ee71c51a":"markdown","41956e99":"markdown","c5b3edea":"markdown","dca7fd06":"markdown","62b6f64e":"markdown","8f850d63":"markdown","ae7a56d1":"markdown","f7325f25":"markdown","f3fa0e49":"markdown","ad96ac08":"markdown","75fba81a":"markdown","c84420ac":"markdown","75c6f7a1":"markdown","b7d8fff4":"markdown","00d52cb5":"markdown","5b569a37":"markdown","53ae3c25":"markdown","bbfdec67":"markdown","521f47a2":"markdown","611de34a":"markdown","5eb0f4d2":"markdown","34f7325c":"markdown","ac6e481f":"markdown","98bca4ff":"markdown","ee4cc418":"markdown","c32b7322":"markdown","646d90fa":"markdown","b3b2af3b":"markdown","5971ea4a":"markdown","bf59e1b3":"markdown","2708e076":"markdown","3ce19a0e":"markdown","cf0614dd":"markdown","415eb0f5":"markdown","6632af45":"markdown","834900ef":"markdown","19229a7b":"markdown","352e7e1c":"markdown","b95bda2b":"markdown","948f96e8":"markdown","d86107b0":"markdown","6dcf4bb5":"markdown","1a324f0c":"markdown","087befeb":"markdown","809bbd85":"markdown","648a5dc9":"markdown","5b19c580":"markdown","0efaeb51":"markdown","eb33b966":"markdown","f1c0ceae":"markdown","77e96878":"markdown","15790106":"markdown","e9542963":"markdown","c9713599":"markdown","41d42947":"markdown","625546f9":"markdown","08d2696d":"markdown","bd1c0dba":"markdown"},"source":{"920b77d4":"#Algebra Linear\nimport numpy as np \n\n# Processamento\/manipula\u00e7\u00e3o dos dados\nimport pandas as pd \n\n# Visualiza\u00e7\u00e3o dos dados\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n#Algoritimos Machine Learning\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","c0ec0fb9":"test_df = pd.read_csv(\"..\/input\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")","7db21d6f":"train_df.info()","11830603":"train_df.describe()","f54c93ad":"train_df.head(15)","d7b3fc9d":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","0339ad4f":"train_df.columns.values","898688bc":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","7a7ff7c0":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=\"muted\",  order=None, hue_order=None )\nFacetGrid.add_legend()","0e4a5d91":"sns.barplot(x='Pclass', y='Survived', data=train_df)","fc8d6005":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","110373e2":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)","6ad7912c":"train_df['not_alone'].value_counts()","41043a2e":"axes = sns.factorplot('relatives','Survived', \n                      data=train_df, aspect = 2.5, )","bca22285":"train_df = train_df.drop(['PassengerId'], axis=1)","aa751979":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) ","2b612b81":"# Agora podemos excluir a variavel cabine\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","9108b6be":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # computar n\u00fameros aleat\u00f3rios entre a m\u00e9dia, std e is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # preencher valores NaN na coluna Idade com valores aleat\u00f3rios gerados\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)","a8ba5881":"train_df[\"Age\"].isnull().sum()","bbd6732e":"train_df['Embarked'].describe()","dc07d8f1":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","10c6272c":"train_df.info()","b637e223":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","797d60cd":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extraindo os titulos\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # substituir t\u00edtulos com um t\u00edtulo mais comum ou como Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # converter t\u00edtulos em n\u00fameros\n    dataset['Title'] = dataset['Title'].map(titles)\n    # Inserindo o 0 nos campos NaN \n    dataset['Title'] = dataset['Title'].fillna(0)","ebbc3a4f":"train_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","5c1658de":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","2fc65ed9":"train_df['Ticket'].describe()","a6348e71":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","e1cf30a2":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","78ab0bba":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","d42d111c":"# Verificando a distribui\u00e7\u00e3o\ntrain_df['Age'].value_counts()","751b35dd":"train_df.head(10)","08f24e22":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","17f144f4":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","8a302e13":"for dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)","ad4e467b":"# Vamos dar uma \u00faltima olhada no conjunto de treinamento, antes de come\u00e7armos a treinar os modelos.\ntrain_df.head(20)","3b13b5a9":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()","7ffce17f":"# stochastic gradient descent (SGD) learning\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\n\nprint(round(acc_sgd,2,), \"%\")","0920d20a":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","207c8ab7":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","208a4794":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(round(acc_knn,2,), \"%\")","69b3b254":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(round(acc_gaussian,2,), \"%\")","08fbf150":"# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(round(acc_perceptron,2,), \"%\")","71649380":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(round(acc_linear_svc,2,), \"%\")","dcb09ef8":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")","6030b28c":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","93d53eae":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")","4aacc983":"print(\"Score:\", scores)\nprint(\"M\u00e9dia:\", scores.mean())\nprint(\"Desvio Padr\u00e3o:\", scores.std())","b08fb982":"parametros = pd.DataFrame({'feature':X_train.columns,'Parametros':np.round(random_forest.feature_importances_,3)})\nparametros = parametros.sort_values('Parametros',ascending=False).set_index('feature')","471d7554":"parametros.head(15)","9066c876":"parametros.plot.bar()","c71207b9":"train_df  = train_df.drop(\"not_alone\", axis=1)\ntest_df  = test_df.drop(\"not_alone\", axis=1)\n\ntrain_df  = train_df.drop(\"Parch\", axis=1)\ntest_df  = test_df.drop(\"Parch\", axis=1)","f80652cc":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","62b4bc25":"print(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","84fdd7a9":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","afc64697":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","61af06e5":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","b2b0c00b":"from sklearn.metrics import f1_score\nf1_score(Y_train, predictions)","3e0586b0":"from sklearn.metrics import precision_recall_curve\n\n# Obtendo as probabilidades de nossas previs\u00f5es\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)","d7bc83a0":"def plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"Precis\u00e3o (precision)\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"Recupera\u00e7\u00e3o (recall)\", linewidth=5)\n    plt.xlabel(\"Limite\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(17, 9))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","96453999":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()","111a66c8":"from sklearn.metrics import roc_curve\n# calcular taxa positiva verdadeira e taxa de falsos positivos\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)","963ba3c1":"# plotando  a Curva ROC AUC\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('Taxa de Falsos Positivos', fontsize=16)\n    plt.ylabel('Taxa de Verdaeiros Positivos', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","7e2acc28":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","982b472a":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)","cb64a100":"## Ajuste dos Hiperpar\u00e2metros\n\n\nAbaixo veremos o c\u00f3digo do ajuste dos hiperpar\u00e2metros para o crit\u00e9rio dos par\u00e2metros, **min_samples_leaf, min_samples_split e n_estimators.**\n\nEu coloquei este c\u00f3digo em uma c\u00e9lula de remarca\u00e7\u00e3o e n\u00e3o em uma c\u00e9lula de c\u00f3digo, porque leva muito tempo para execut\u00e1-lo. Diretamente por baixo, coloquei uma captura de tela da sa\u00edda da grade.","ce56e08e":"## ROC AUC Score\nA ROC AUC Score \u00e9 a pontua\u00e7\u00e3o correspondente \u00e0 Curva ROC AUC. \u00c9 calculado simplesmente medindo a \u00e1rea sob a curva, que \u00e9 chamada de AUC.\n\nUm classificador 100% correto teria uma pontua\u00e7\u00e3o AUC ROC de 1 e um classificador completamente aleat\u00f3rio teria uma pontua\u00e7\u00e3o de 0,5.","72a93866":"**Teste novos parametros:**","b604ed7b":"A linha vermelha no meio representa um classificador puramente aleat\u00f3rio (por exemplo, um coin flip) e, portanto, seu classificador deve estar o mais longe poss\u00edvel. Nosso modelo Random Forest parece ter feito um bom trabalho.\n\n\u00c9 claro que tamb\u00e9m temos uma desvantagem aqui, porque o classificador produz mais falsos positivos, quanto maior a taxa positiva verdadeira.","3b12d4b8":"O gr\u00e1fico acima confirma nossa suposi\u00e7\u00e3o sobre a **PClass** 1, mas tamb\u00e9m podemos identificar uma alta probabilidade de que uma pessoa na pclasse 3 n\u00e3o sobreviva.","0c1092a5":"**4. Pclass:**","3d1a457d":"Como o atributo **Ticket** temos 681 tickets \u00fanicos, ser\u00e1 um pouco complicado convert\u00ea-los em categorias \u00fateis. Ent\u00e3o vamos remove-los do conjunto de dados.","1a55abe0":"**3. Embarca\u00e7\u00e3o, Classes de bilhetes e Sexo:**","94741708":"### 2.  Fare per Person","85aeb258":"## Convertendo os Parametros:","ee71c51a":"**Treinando nosso modelo novamente:**","41956e99":"Em nosso dataset acima, podemos notar algumas coisas. Em primeiro lugar, precisamos **converter muitos recursos em num\u00e9ricos**, mais tarde, para que os algoritmos de aprendizado de m\u00e1quina possam process\u00e1-los. Al\u00e9m disso, podemos ver que os recursos do **t\u00eam intervalos muito diferentes**, que precisaremos converter em aproximadamente na mesma escala. Tamb\u00e9m podemos detectar mais alguns recursos, que cont\u00eam valores ausentes (NaN = n\u00e3o um n\u00famero), que precisam ser tratados.\n\n**Vamos dar uma olhada mais detalhada em quais dados est\u00e3o realmente ausentes:**","c5b3edea":"A primeira linha \u00e9 sobre as predi\u00e7\u00f5es dos n\u00e3o sobreviventes: **493 passageiros foram classificados corretamente como n\u00e3o sobreviventes** (chamados de verdadeiros negativos) e **56 foram erroneamente classificados como n\u00e3o sobreviventes** (falsos negativos).\n\nA segunda linha \u00e9 sobre as previs\u00f5es dos sobreviventes: **97 passageiros classificados erroneamente como sobreviventes** (falsos positivos) e **245 onde corretamente classificados como sobreviventes** (positivos verdadeiros).\n\nUma matriz de confus\u00e3o fornece muitas informa\u00e7\u00f5es sobre o desempenho do nosso modelo, mas h\u00e1 uma maneira de obter ainda mais informa\u00e7\u00f5es, por exemplo como calcular a precis\u00e3o dos classificadores.","dca7fd06":"# **O RMS Titanic**\n\nO RMS Titanic era um navio de passageiros brit\u00e2nico que afundou no Oceano Atl\u00e2ntico Norte nas primeiras horas da manh\u00e3 de 15 de abril de 1912, ap\u00f3s colidir com um iceberg durante sua viagem inaugural de Southampton para a cidade de Nova York. Havia um n\u00famero estimado de 2.224 passageiros e tripulantes a bordo do navio, e mais de 1.500 morreram, tornando-se um dos mais mortais desastres mar\u00edtimos comerciais em tempos de paz da hist\u00f3ria moderna. O RMS Titanic foi o maior navio \u00e0 tona na \u00e9poca em que entrou em opera\u00e7\u00e3o e foi o segundo de tr\u00eas transatl\u00e2nticos de classe ol\u00edmpica operados pela White Star Line. O Titanic foi constru\u00eddo pelo estaleiro Harland and Wolff em Belfast. Thomas Andrews, seu arquiteto, morreu no desastre.","62b6f64e":"## K-Fold Cross Validation:\n\nK-Fold Cross Validation divide aleatoriamente os dados de treinamento em **K subsets called folds**. Vamos imaginar que dividir\u00edamos nossos dados em 4 dobras (K = 4). Nosso modelo Random Forest seria treinado e avaliado 4 vezes, usando uma dobra diferente para avalia\u00e7\u00e3o toda vez, enquanto seria treinado nas 3 dobras restantes.\n\nA imagem abaixo mostra o processo, usando 4 dobras (K = 4). Cada linha representa um processo de treinamento + avalia\u00e7\u00e3o. Na primeira linha, o modelo \u00e9 treinado no primeiro, segundo e terceiro subconjunto e avaliado no quarto. Na segunda linha, o modelo \u00e9 treinado no segundo, terceiro e quarto subconjunto e avaliado no primeiro. O K-Fold Cross Validation repete este processo at\u00e9 que cada dobra tenha atuado uma vez como uma dobra de avalia\u00e7\u00e3o.\n\n![cross-v.](https:\/\/img3.picload.org\/image\/ddwrppcl\/bildschirmfoto2018-02-02um10.0.png)\n\nO resultado do nosso exemplo **K-Fold Cross Validation** seria uma matriz que cont\u00e9m 4 pontua\u00e7\u00f5es diferentes. Ent\u00e3o, precisamos calcular a m\u00e9dia e o desvio padr\u00e3o para essas pontua\u00e7\u00f5es.\n\nO c\u00f3digo abaixo executa a  K-Fold Cross Validation em nosso modelo Random Forest, usando 10 dobras (K = 10). Portanto, gera uma matriz com 10 pontua\u00e7\u00f5es diferentes.","8f850d63":"# **Cria\u00e7\u00e3o dos Modelos de Machine Learning**","ae7a56d1":"Acima voc\u00ea pode ver que **'Fare'** \u00e9 um float e n\u00f3s temos que lidar com 4 caracter\u00edsticas categ\u00f3ricas: Nome, Sexo, Ticket e Embarked. Vamos investigar e transformar um ap\u00f3s o outro.","f7325f25":"Notebook created by: **Samuel da Silva Oliveira**\n\n<a href=\"https:\/\/www.linkedin.com\/in\/samuel-oliveira-74278873\/\">Linkedin<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/samukaunt\">Kaggle<\/a>\n\n<a href=\"https:\/\/github.com\/samukaunt\">Github<\/a>\n","f3fa0e49":"## Precision Recall Curve\n\nPara cada pessoa que nosso algoritmo Random Forest precisa classificar, ele calcula uma probabilidade baseada em uma fun\u00e7\u00e3o e classifica a pessoa como sobrevivente (quando a pontua\u00e7\u00e3o \u00e9 maior do que o limite) ou como n\u00e3o sobrevivente (quando a pontua\u00e7\u00e3o \u00e9 menor que o limite). \u00c9 por isso que o limitar desempenha um papel importante.\n\nVamos plotar a precis\u00e3o e recuperar com o limite usando o matplotlib:","ad96ac08":"## Matriz de Confus\u00e3o:","75fba81a":"Excelente! Temos 94% acho que \u00e9 pontua\u00e7\u00e3o boa o suficiente para enviar as previs\u00f5es para o conjunto de teste para o ranking de Kaggle.","c84420ac":"Aqui podemos ver que temos uma alta probabilidade de sobreviv\u00eancia com 1 a 3 parentes, mas uma menor probabilidade se voc\u00ea tem menos de 1 ou mais de 3 (exceto em alguns casos com 6 parentes).","75c6f7a1":"Acima podemos ver que **38% do conjunto de treinamento sobreviveu ao Titanic**. Tamb\u00e9m podemos ver que as idades dos passageiros variam de 0,4 a 80. Al\u00e9m disso, j\u00e1 podemos detectar alguns recursos que cont\u00eam valores ausentes, como o recurso **\"Age\".**","b7d8fff4":"Aqui vemos claramente que a **Pclass** est\u00e1 contribuindo para a chance de sobreviv\u00eancia de uma pessoa, especialmente se essa pessoa est\u00e1 na classe 1. Vamos criar outra plotagem de **Pclass** abaixo.","00d52cb5":"# **Random Forest**\n\n## O que \u00e9 Random Forest ?\n\nRandom Forest \u00e9 um algoritmo de aprendizado supervisionado. Como voc\u00ea v\u00ea pelo nome, o algoritimo cria uma floresta e torna aleat\u00f3ria. A floresta que constr\u00f3i, \u00e9 um conjunto de \u00c1rvores de Decis\u00e3o, na maioria das vezes treinadas com o m\u00e9todo de **\u201censacamento\u201d**. A ideia geral do m\u00e9todo de ensacamento \u00e9 que uma combina\u00e7\u00e3o de modelos de aprendizagem aumenta o resultado geral.\n\nPara dizer isso em palavras simples: O **algoritimo Random Forest** cria v\u00e1rias \u00e1rvores de decis\u00e3o e as mescla para obter uma previs\u00e3o mais precisa e est\u00e1vel.\n\nUma grande vantagem do Random Forest \u00e9 que ele pode ser usada para problemas de classifica\u00e7\u00e3o e regress\u00e3o, que formam a maioria dos sistemas atuais de aprendizado de m\u00e1quina. Com poucas exce\u00e7\u00f5es, um classificador com Random Forest tem todos os hiperpar\u00e2metros de um classificador de uma \u00e1rvore de decis\u00e3o e tamb\u00e9m todos os hiperpar\u00e2metros de um classificador de ensacamento, para controlar o conjunto em si.\n\nO algoritmo Random Forest traz aleatoriedade extra para o modelo, quando as \u00e1rvores est\u00e3o crescendo. Em vez de procurar o melhor recurso ao dividir um n\u00f3, ele procura o melhor recurso entre um subconjunto aleat\u00f3rio de recursos. Esse processo cria uma ampla diversidade, o que geralmente resulta em um modelo melhor. Portanto, quando voc\u00ea est\u00e1 desenvolvendo uma \u00e1rvore com Random Forest, apenas um subconjunto aleat\u00f3rio dos recursos \u00e9 considerado para dividir um n\u00f3. Voc\u00ea pode at\u00e9 tornar as \u00e1rvores mais aleat\u00f3rias, usando limites aleat\u00f3rios, para cada recurso, em vez de procurar os melhores limites poss\u00edveis (como acontece com uma \u00e1rvore de decis\u00e3o normal).\n\nAbaixo est\u00e1 um exemplo usando Random Forest com duas \u00e1rvores:\n\n![picture](https:\/\/img3.picload.org\/image\/dagpgdpw\/bildschirmfoto-2018-02-06-um-1.png)","5b569a37":"Isso parece muito mais realista do que antes. Nosso modelo tem uma precis\u00e3o m\u00e9dia de 82% com um desvio padr\u00e3o de 4%. O desvio padr\u00e3o nos mostra qu\u00e3o precisas s\u00e3o as estimativas.\n\nIsso significa, no nosso caso, que a precis\u00e3o do nosso modelo pode diferir **mais ou menos 4%.**\n\nAcho que a precis\u00e3o ainda \u00e9 muito boa e, como o Random Forest \u00e9 um modelo f\u00e1cil de usar, tentaremos aumentar ainda mais o desempenho na se\u00e7\u00e3o seguinte.","53ae3c25":"## **\u00cdndice:**\n* Introdu\u00e7\u00e3o\n* O RMS Titanic\n* Importando as bibliotecas\n* Obtendo os dados\n* Explora\u00e7\u00e3o\/An\u00e1lise de Dados\n* Pr\u00e9-processamento dos dados\n    - Dados ausentes\n    - Convertendo os parametros\n    - Criando Categorias\n    - Criando novos parametros\n* Cria\u00e7\u00e3o dos Modelos de Machine Learning\n    - Treinando 8 modelos diferentes\n    - Qual \u00e9 o melhor modelo?\n    - K-Fold Cross Validation\n* Random Forest \n    - O que \u00e9 Random Forest ?\n    - Parametros importantes\n    - Ajuste de hiperpar\u00e2metros   \n* Avalia\u00e7\u00e3o adicional \n    - Matriz de Confus\u00e3o\n    - Precis\u00e3o e Recall \n    - F-Score\n    - Curva de Rechamada de Precis\u00e3o\n    - Curva ROC AUC\n    - ROC AUC Score\n* Submiss\u00e3o\n* Resumo","bbfdec67":"**1. Idade e Sexo:**","521f47a2":"## F-Score\n\nVoc\u00ea pode combinar precis\u00e3o e recupera\u00e7\u00e3o em uma pontua\u00e7\u00e3o, chamada de **F-Score**. O F-Score \u00e9 calculado com a m\u00e9dia harm\u00f4nica de precis\u00e3o e recupera\u00e7\u00e3o. Observe que ele atribui muito mais peso a valores baixos. Com o resultado disso, o classificador s\u00f3 obter\u00e1 um valor F alto, se tanto a recupera\u00e7\u00e3o quanto a precis\u00e3o forem altas.","611de34a":"# Criando novos parametros\n\nEu adicionei dois novos recursos ao conjunto de dados, que eu computo de outros recursos.\n\n### 1. Age times Class","5eb0f4d2":"### Age:\n\nAgora podemos resolver o problema com os valores ausentes da faixa et\u00e1ria. Vou criar uma matriz que cont\u00e9m n\u00fameros aleat\u00f3rios, que s\u00e3o calculados com base no valor m\u00e9dio de idade em rela\u00e7\u00e3o ao **desvio padr\u00e3o e is_null**.","34f7325c":"# **Importando as bibliotecas**","ac6e481f":"## Criando as Categorias:\n\nVamos agora criar categorias dentro dos seguintes recursos:\n\n### Age:\nAgora precisamos converter o recurso **'Age'**. Primeiro vamos convert\u00ea-lo de float para inteiro. Em seguida, criaremos a nova vari\u00e1vel **\"AgeGroup\"**, categorizando cada idade em um grupo. Observe que \u00e9 importante prestar aten\u00e7\u00e3o em como voc\u00ea forma esses grupos, pois voc\u00ea n\u00e3o deseja, por exemplo, que 80% dos seus dados caiam grupo 1.","98bca4ff":"## Dados Ausentes:\n### Cabine:\nComo lembrete, temos que lidar com a Cabine (687), Embarcou (2) e Idade (177).\n\nPrimeiro eu lembrei, que deveriamos deletar a vari\u00e1vel **'Cabin'** mas ent\u00e3o eu achei algo interessante. Um n\u00famero de cabine que se parece com \"C123\" e a letra \"** refere-se ao conv\u00e9s **.\n\nPortanto, vamos extra\u00ed-los e criar um novo recurso que contenha um deck de pessoas. Depois, converteremos o recurso em uma vari\u00e1vel num\u00e9rica. Os valores ausentes ser\u00e3o convertidos em zero.\n\nNa foto abaixo voc\u00ea pode ver os decks atuais do Titanic, variando de A a G.\n![titanic decks](http:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/84\/Titanic_cutaway_diagram.png\/687px-Titanic_cutaway_diagram.png)","ee4cc418":"# **Explora\u00e7\u00e3o\/An\u00e1lise de Dados**","c32b7322":"Primeiro, vamos remover **'PassengerId'** do conjunto de treino, porque isso n\u00e3o contribui para a probabilidade de sobreviv\u00eancia de uma pessoa. Eu n\u00e3o vou solt\u00e1-lo do conjunto de testes, j\u00e1 que \u00e9 necess\u00e1rio para o envio","646d90fa":"Agora que temos um modelo adequado, podemos come\u00e7ar a avaliar sua performace de maneira mais precisa. Anteriormente, usamos apenas a precis\u00e3o e o score de oob, que \u00e9 apenas outra forma de precis\u00e3o. O problema \u00e9 que \u00e9 mais complicado avaliar um modelo de classifica\u00e7\u00e3o do que um modelo de regress\u00e3o. Vamos falar sobre isso na se\u00e7\u00e3o seguinte.","b3b2af3b":"Como podemos ver, o Random Forest fica em primeiro lugar. Mas antes, vamos verificar como funciona o Random Forest quando usamos Cross Validation.","5971ea4a":"![Titanic](https:\/\/www.cruisemapper.com\/images\/news\/3956-21b7ab89728.jpg)","bf59e1b3":"# **Pr\u00e9-Processamento de Dados**","2708e076":"O recurso Embarcado tem apenas dois valores ausentes, que podem ser facilmente preenchidos. Ser\u00e1 muito mais complicado lidar com o recurso 'Age', que tem 177 valores em falta. O recurso 'Cabin' precisa de uma investiga\u00e7\u00e3o mais aprofundada, mas provavelmente removeremos do conjunto de dados, j\u00e1 que 77% dele est\u00e1 faltando.","3ce19a0e":"### Sex:\n\nConvertendo **'Sex'** em num\u00e9rico.","cf0614dd":"### Ticket:","415eb0f5":"Agora podemos come\u00e7ar a ajustar os hiperpar\u00e2metros  do nosso algoritimo Random Forest","6632af45":"## Parametros Importantes\n\nOutra grande qualidade do Random Forest \u00e9 que ele facilita muito a medi\u00e7\u00e3o da import\u00e2ncia relativa de cada recurso. O Sklearn mede a import\u00e2ncia de um recurso observando quanto n\u00f3s da arvore, usam esse recurso, reduzem a impureza em m\u00e9dia (em todas as vezes que utilizamos o algoritimo Random Forest). Ele calcula essa pontua\u00e7\u00e3o automaticamente para cada recurso ap\u00f3s o treinamento e dimensiona os resultados para que a soma de todas os parametros seja igual a 1. Veremos abaixo:","834900ef":"# **Avalia\u00e7\u00e3o Adicional**\n\n","19229a7b":"## Precis\u00e3o e Recupera\u00e7\u00e3o:","352e7e1c":"## Curva ROC AUC\n\nOutra maneira de avaliar e comparar nosso modelo de classifica\u00e7\u00e3o \u00e9 utilizar **Curva ROC AUC**. Esta curva representa a taxa positiva real (tamb\u00e9m chamada de recall) contra a taxa de falsos positivos (propor\u00e7\u00e3o de inst\u00e2ncias negativas incorretamente classificadas), em vez de tra\u00e7ar a precis\u00e3o versus a recupera\u00e7\u00e3o.","b95bda2b":"Acima podemos ver claramente que o **Recupera\u00e7\u00e3o(Recall)** est\u00e1 caindo rapidamente com uma precis\u00e3o de cerca de 85%. Por causa disso, voc\u00ea pode querer selecionar a troca de precis\u00e3o \/ recupera\u00e7\u00e3o antes disso - talvez em torno de 75%.\n\nAgora voc\u00ea pode escolher um limite, que oferece a melhor compensa\u00e7\u00e3o de precis\u00e3o \/ recupera\u00e7\u00e3o para seu problema atual de aprendizado de m\u00e1quina. Se voc\u00ea quiser, por exemplo, uma precis\u00e3o de 80%, voc\u00ea pode facilmente olhar para os gr\u00e1ficos e ver que voc\u00ea precisaria de um limite de cerca de 0,4. Ent\u00e3o voc\u00ea poderia treinar um modelo com exatamente esse limite e obteria a precis\u00e3o desejada.\n\n\nOutra maneira \u00e9 tra\u00e7ar a precis\u00e3o e recordar uns contra os outros:","948f96e8":"** O conjunto de treinamento tem 891 exemplos e 11 caracter\u00edsticas + a vari\u00e1vel de destino (Survived) **. 2 dos recursos s\u00e3o floats, 5 s\u00e3o inteiros e 5 s\u00e3o objetos. Abaixo listei os recursos com uma breve descri\u00e7\u00e3o:\n\n    survival:\tSobrevivente\n    PassengerId: ID \u00fanico de um passageiro\n    pclass:\tClasse de bilhetes\t\n    sex:\tSexo\n    Age:\tIdade em anos\t\n    sibsp:\tN\u00famero de irm\u00e3os \/ c\u00f4njuges a bordo do Titanic\t\n    parch:\tN\u00famero de pais \/ filhos a bordo do Titanic\t\n    ticket:\tNumero do bilhete\t\n    fare:\tTarifa de passageiros\t\n    cabin:\tN\u00famero de cabine\t\n    embarked:\tPorto de embarca\u00e7\u00e3o","d86107b0":"# **Introdu\u00e7\u00e3o**\n\nNeste notebook eu vou passar por todo o processo de cria\u00e7\u00e3o de um modelo de aprendizado de m\u00e1quina no famoso conjunto de dados Titanic, que \u00e9 usado por muitas pessoas em todo o mundo. Ele fornece informa\u00e7\u00f5es sobre o destino dos passageiros no Titanic, resumidos de acordo com o status econ\u00f4mico (classe), sexo, idade e sobreviv\u00eancia. Neste desafio, somos solicitados a prever se um passageiro do Titanic teria sobrevivido ou n\u00e3o.","6dcf4bb5":"### Embarked:\nConvertendo **'Embarked'** em numerico.","1a324f0c":"# **Submiss\u00e3o**","087befeb":"##  Qual \u00e9 o melhor modelo ?","809bbd85":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n              \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n              \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \n              \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nclf = GridSearchCV(estimator=rf, param_grid=param_grid,\n                    n_jobs=-1)\n\nclf.fit(X_train, Y_train)        \n\nclf.best_params_\n\n![GridSearch Output](https:\/\/img2.picload.org\/image\/ddwglili\/bildschirmfoto2018-02-01um15.4.png)","648a5dc9":"Nosso modelo prev\u00ea 81% do tempo, uma sobreviv\u00eancia de passageiros corretamente (precis\u00e3o). A (Recupera\u00e7\u00e3o) nos diz que o modelo previu a sobreviv\u00eancia de 71% das pessoas que realmente sobreviveram.","5b19c580":"Nosso modelo teve o mesmo score de antes. Uma regra geral \u00e9 que, quanto mais parametros\/variaveis tivermos, **mais prov\u00e1vel ser\u00e1 que o modelo tenha um overfitting** e vice-versa. Mas acho que os dados do modelo est\u00e3o bons por enquanto e n\u00e3o temos muitos parametros.\n\nH\u00e1 tamb\u00e9m outra maneira de avaliar nosso algoritimo Random Forest, que \u00e9 provavelmente muito mais preciso do que a pontua\u00e7\u00e3o usada anteriormente. Estou falando de usarmos a t\u00e9cnica **Out-of-bag (OOB)** para estimar a precis\u00e3o da generaliza\u00e7\u00e3o. N\u00e3o vou entrar em detalhes de como funciona o **OOB**, mas clicando <a href=\"https:\/\/en.wikipedia.org\/wiki\/Out-of-bag_error\">aqui<\/a> voc\u00ea tera mais informa\u00e7\u00f5es. Mas observando a estimativa o **Out-of-bag (OOB)** \u00e9 t\u00e3o preciso quanto usar um conjunto de teste do mesmo tamanho que o conjunto de treinamento. Portanto, usar o **Out-of-bag (OOB)**  elimina a necessidade de um conjunto de teste reservado.","0efaeb51":"**Conclus\u00e3o:**\n\n**not_alone e Parch** n\u00e3o desempenha um papel significativo em nosso modelo classifica\u00e7\u00e3o utilizanbdo **Random Forest**. Por causa disso, vamos remove-los do conjunto de dados e treinar novamente nosso modelo. Tamb\u00e9m podemos remover mais parametros, mas isso exigiria uma investiga\u00e7\u00e3o mais detalhada do efeito dos parametros em nosso modelo.","eb33b966":"Voc\u00ea pode ver que os homens t\u00eam uma alta probabilidade de sobreviv\u00eancia quando t\u00eam entre 18 e 30 anos, o que tamb\u00e9m \u00e9 um pouco verdadeiro para as mulheres, mas n\u00e3o totalmente. Para as mulheres, as chances de sobreviv\u00eancia s\u00e3o maiores entre 14 e 40 anos.\n\nPara os homens, a probabilidade de sobreviv\u00eancia \u00e9 muito baixa entre os 5 e os 18 anos, mas isso n\u00e3o \u00e9 verdade para as mulheres. Outra coisa a notar \u00e9 que os beb\u00eas tamb\u00e9m t\u00eam uma probabilidade um pouco maior de sobreviv\u00eancia.\n\nUma vez que parece haver **algumas idades, que aumentaram as chances de sobreviv\u00eancia** e porque eu quero que todos os recursos estejam mais ou menos na mesma escala, criaremos grupos et\u00e1rios mais tarde.","f1c0ceae":"### Fare:\nPara o recurso **\"Fare\"**, precisamos fazer o mesmo com o recurso **\"Age\"**. Mas n\u00e3o \u00e9 t\u00e3o f\u00e1cil, porque se cortarmos o intervalo dos valores de tarifa em algumas categorias igualmente grandes, 80% dos valores cairiam na primeira categoria. Felizmente, podemos usar a fun\u00e7\u00e3o **\"qcut()\"**, que podemos usar para ver como podemos formar as categorias.","77e96878":"### Embarked:\n\nComo a coluna **Embarked** tem apenas dois valores ausentes, apenas os preencheremos com o mais comum.","15790106":"**5.  SibSp and Parch:**\n\nSibSp e Parch faria mais sentido como uma caracter\u00edstica combinada, que mostra o n\u00famero total de parentes, uma pessoa tem no Titanic. Vou cri\u00e1-lo abaixo e tamb\u00e9m um recurso que semeia se algu\u00e9m n\u00e3o est\u00e1 sozinho.","e9542963":"# **Obtendos os dados**","c9713599":"### Name:\nUsaremos o recurso **Name** para extrair os T\u00edtulos do Nome, para que possamos criar um novo recurso a partir disso.","41d42947":"**Embarked** parece estar correlacionada com a sobreviv\u00eancia, dependendo do sexo.\n\nAs mulheres na porta Q e na porta S t\u00eam maior chance de sobreviv\u00eancia. O inverso \u00e9 verdadeiro, se estiverem na porta C. Os homens t\u00eam uma alta probabilidade de sobreviv\u00eancia se estiverem na porta C, mas uma probabilidade baixa se estiverem na porta Q ou S.\n\n**Pclass** tamb\u00e9m parece estar correlacionado com a sobreviv\u00eancia. Vamos gerar outro gr\u00e1fico abaixo.","625546f9":"### Fare:\n\nConvertendo **\"Fare\"** de float para int64, usando a fun\u00e7\u00e3o \"astype ()\", que a biblioteca pandas fornece:","08d2696d":"Acima voc\u00ea pode ver os 11 recursos + a vari\u00e1vel de destino (sobreviveu). ** Quais recursos podem contribuir para uma alta taxa de sobrevida? **\n\nPara mim, faria sentido se tudo, exceto 'PassengerId', 'Ticket' e 'Name', fosse correlacionado com uma alta taxa de sobreviv\u00eancia.","bd1c0dba":"Nosso resultado foi um F-Score de 76%. A pontua\u00e7\u00e3o n\u00e3o \u00e9 t\u00e3o alta, porque o resultado da nossa recupera\u00e7\u00e3o foi de 71%.\n\nMas infelizmente o F-score n\u00e3o \u00e9 perfeito, porque favorece os classificadores que t\u00eam uma precis\u00e3o e uma recupera\u00e7\u00e3o semelhantes. Isso \u00e9 um problema, porque \u00e0s vezes voc\u00ea quer uma alta precis\u00e3o e, \u00e0s vezes, uma alta recupera\u00e7\u00e3o. A coisa \u00e9 que uma precis\u00e3o crescente, \u00e0s vezes, resulta em um recupera\u00e7\u00e3o decrescente e vice-versa (dependendo do limite). Isso \u00e9 chamado de troca de precis\u00e3o \/ recupera\u00e7\u00e3o. Vamos ver isso abaixo."}}