{"cell_type":{"7697d88e":"code","989628ff":"code","90edf369":"code","626d7563":"code","a0055af9":"code","0d0cd87a":"code","8ab11e17":"code","1081a2de":"code","85efed60":"code","20d2453f":"code","4526ca5b":"code","6e02fc84":"code","2dbe00e5":"code","7f469644":"code","53b96b68":"code","1ba22fe4":"code","78f3e858":"code","360b5adc":"code","4ad5a88a":"code","ad1c7d75":"code","e64c3105":"code","e78a70eb":"code","5f4278de":"code","6bc77ddd":"code","6c8a5137":"code","12e2644d":"code","34396a2c":"code","0d57ea12":"code","d1c0affc":"code","4389e3a9":"code","d472efdb":"code","25571d85":"code","fced1f9e":"code","1688ad3a":"code","34add1cc":"code","7ed6d15e":"code","bada015d":"code","fb5df317":"markdown","07987609":"markdown","9ae69be1":"markdown","4e86721c":"markdown","d424872f":"markdown","d0f2aa37":"markdown","d8a49eea":"markdown","a9fe4977":"markdown","492d8a91":"markdown","9143a526":"markdown","f0ab6fb1":"markdown","3bd28105":"markdown","dd2034f0":"markdown","23c82de7":"markdown","c2112db0":"markdown","602ac18e":"markdown","c8ca7f20":"markdown","4b6f5e14":"markdown","54edee74":"markdown","a8f73412":"markdown","cba6422b":"markdown"},"source":{"7697d88e":"import pandas as pd\nfeatures = pd.read_csv(\"..\/input\/temperature-data-seattle\/temps.csv\")","989628ff":"features.head(5)","90edf369":"print(features.shape)","626d7563":"# Descriptive statistics for each column\nfeatures.describe()","a0055af9":"import matplotlib.pyplot as plt","0d0cd87a":"with plt.style.context('ggplot'):\n    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True)\n    axes[0][0].plot(features['actual'])\n    axes[0][0].set_title('Max Temp')\n    axes[0][1].plot(features['temp_1'])\n    axes[0][1].set_title('Previous Max Temp')\n    axes[1][0].plot(features['temp_2'])\n    axes[1][0].set_title('Prior Two Days Max Temp')\n    axes[1][1].plot(features['friend'])\n    axes[1][1].set_title('Friend Estimate')\n    plt.plot()","8ab11e17":"# One-hot encode the data using pandas get_dummies\nfeatures = pd.get_dummies(features)","1081a2de":"features.head()","85efed60":"import numpy as np\n# Access the targets\nlabels = np.array(features['actual'])\n# remove targets from the features\n# axis 1 refers to the columns\nfeatures=features.drop('actual', axis=1)","20d2453f":"# saving feature names for later use\nfeature_list = list(features.columns)","4526ca5b":"# Convert to numpy array\nfeatures = np.array(features)","6e02fc84":"# Suing Scikit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training adn testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)","2dbe00e5":"print(f'Training Features Shape: {train_features.shape}')\nprint(f'Testing Features Shape: {test_features.shape}')\nprint(f'Training Labels Shape: {train_labels.shape}')\nprint(f'Testing Labels Shape: {test_labels.shape}')","7f469644":"# The baseline predictions ar eth historical averages\nbaseline_preds = test_features[:, feature_list.index('average')]\n# Baseline errors, and display average baseline error\nbaseline_errors = abs(test_labels - baseline_preds)\n\nprint(f'Average baseline error (MAE): {round(np.mean(baseline_errors), 2)}')","53b96b68":"# Improt the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators=1000, random_state=42)\n\n# Train the model on training data\nrf.fit(train_features, train_labels)","1ba22fe4":"# Use the forests predict method on the test data\npredictions = rf.predict(test_features)\n\n# Calcuate the absoulte errors\nerrors = abs(test_labels - predictions)\n\n# Print out the mean absolute error (MAE)\nprint(f'Mean Absolute Error: {round(np.mean(errors), 2)}')","78f3e858":"# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors\/test_labels)\n# Calcualte and display accuracy\naccuracy = 100 - np.mean(mape)\nprint(f'Accuracy: {round(accuracy, 2)}%.')","360b5adc":"# Import tools needed for visualization\nfrom sklearn.tree import export_graphviz\nimport pydot\n\n# Pull out one tree from the forest\ntree = rf.estimators_[5]\n\n# Export the image to a dot file\nexport_graphviz(tree, out_file = 'tree.dot', feature_names=feature_list, rounded=True, precision = 1)","4ad5a88a":"# Use dot file to create a graph\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\n\n# Write graph to a png file\ngraph.write_png('tree.png')","ad1c7d75":"from IPython.display import Image\nImage(filename='tree.png')","e64c3105":"# Limit the depth of the tree to 3 levels\nrf_small = RandomForestRegressor(n_estimators=10, max_depth=3)\nrf_small.fit(train_features, train_labels)\n# Extract the small tree\ntree_small = rf_small.estimators_[5]\n\n# Svae thre tree as a png image\nexport_graphviz(tree_small, out_file='small_tree.dot', feature_names=feature_list, rounded=True, precision=1)\n\n(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\ngraph.write_png(\"small_tree.png\")","e78a70eb":"Image(filename='small_tree.png')","5f4278de":"# Get the numerical fature importances\nimportances = list(rf.feature_importances_)","6bc77ddd":"list(zip(feature_list, importances))","6c8a5137":"# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]","12e2644d":"feature_importances","34396a2c":"# Sort hte feature importances by most important first\nfeature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)","0d57ea12":"feature_importances","d1c0affc":"# Print out\n_ = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]","4389e3a9":"# New random forest with only the two most important variables\nrf_most_important = RandomForestRegressor(n_estimators = 1000, random_state=42)","d472efdb":"# Extract the two most important features\nimportant_indices = [feature_list.index('temp_1'), feature_list.index('average')]\ntrain_important = train_features[:, important_indices]\ntest_important = test_features[:, important_indices]\n\n# Train the random forest\nrf_most_important.fit(train_important, train_labels)\n\n# Make predictions and determine the error\npredictions = rf_most_important.predict(test_important)\n\nerrors = abs(predictions - test_labels)\n\n# Display the performance metrics\nprint('Mean Absolute Error: ', round(np.mean(errors), 2), ' degrees.')\n\nmape = np.mean(100*(errors\/test_labels))\naccuracy = 100 - mape\n\nprint('Accuracy: ', round(accuracy, 2), '%.')","25571d85":"# Set the style\nplt.style.use('fivethirtyeight')\n# List of x locations for plotting\nx_values = list(range(len(importances)))\n\n# Make a bar chart\nplt.bar(x_values, importances, orientation='vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n\n# Axis labels and title\nplt.ylabel('Importance')\nplt.xlabel('Variable')\nplt.title('Variable Importances')","fced1f9e":"# Use datetime for creating data objects for plotting\nimport datetime\n\n# Dates of training values\nmonths = features[:, feature_list.index('month')]\ndays =features[:, feature_list.index('day')]\nyears = features[:, feature_list.index('year')]\n\n# List and then convert to datetime object\ndates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\ndates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n","1688ad3a":"true_data = pd.DataFrame(data={'date': dates, 'actual': labels})","34add1cc":"# Dates of predictions\nmonths = test_features[:, feature_list.index('month')]\ndays = test_features[:, feature_list.index('day')]\nyears = test_features[:, feature_list.index('year')]\ntest_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n\n# Convert to datetime objects\ntest_dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in test_dates]\n\n# Dataframe with predictions and dates\npredictions_data = pd.DataFrame(data={'date': test_dates, 'prediction': predictions})","7ed6d15e":"# Plot the actual values\nplt.plot(true_data['date'], true_data['actual'], 'b-', label='actual')\n# Plot the predicted values\nplt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label='prediction')\nplt.xticks(rotation='60')\nplt.legend()\n# Graph labels\nplt.xlabel('Date')\nplt.ylabel('Maximum temperature (F)')\nplt.title('Actual and Predicted Values')","bada015d":"# Make the data accessible for plotting\ntrue_data['temp_1'] = features[:, feature_list.index('temp_1')]\ntrue_data['average'] = features[:, feature_list.index('average')]\ntrue_data['friend'] = features[:, feature_list.index('friend')]\n\n# Plot all the data as lines\nplt.plot(true_data['date'], true_data['actual'], 'b-', label='actual', alpha = 1.0)\nplt.plot(true_data['date'], true_data['temp_1'], 'y-', label='temp_1', alpha=1.0)\nplt.plot(true_data['date'], true_data['average'], 'y-', label='average', alpha = 0.8)\nplt.plot(true_data['date'], true_data['friend'], 'r-', label='friend', alpha =0.3)\nplt.legend()\nplt.xticks(rotation='60')\nplt.xlabel('Date')\nplt.ylabel('Maximum Tewmperature (F)')\nplt.title('Actual Max Temp and Variables')","fb5df317":"Data description\nyear: 2016 for all\nmonth: number for month of the year\nday: number for day of the year\nweek: day of thwe week as a character string\ntemp_2: max temperature 2 days prior\ntemp_1: max temperature 1 day prior\naverage: historical average max temperature\nactual: max temperature measurement\nfriend: your friend's prediction, a random number between 20 below the average and 20 above the average","07987609":"There are not any data points that immediately appear as anomalous and no zeros in any of the measurement columns. Another method to veryfing the quality of the data is make basic plots. Often it is easier to spot anomalies in a graph than in numbers.","9ae69be1":"# Establish Baseline\nBefore we can make and evaluate predictions, we need to establish a baseline, a sensible measure that we hope to beat with our model. If our model cannot improve upon the baseline, then it will be a failure and we should try a different mode, or admit that machine learning is not right for our problem.\n\nOne simple baseline prediction for our case can be the historical max temperature averages.","4e86721c":"Summarize statistics to identify anomalies","d424872f":"## Training and Testing Sets","d0f2aa37":"## Features and Targets and Convert Data to Arrays","d8a49eea":"We may limit the depth of the trees in the forest to preoduce an understandable image","a9fe4977":"Though it is not very good result, it is 25% better than the baseline","492d8a91":"# Make Predictions on the Test Set","9143a526":"Based solely on this tree, we can make a prediction for any new data point. Let's take an example of making a prediction for Wednesday, December 27, 2017. The (actual) variables are: temp_2 = 39, temp_1 = 35, average = 44, a friend = 30.\n\nWe start at the root node and the first answer is True because $temp\\_1 \\leq 59.5$ so on and so forth. Finally, we come to a leave node with a value of 41.0 as the value of the leaf node.\n\nAn interesting observation is that in the root node, there are only 162 samples, despite there being 261 training data points. This is because each tree in the forest is trained on a random subset of the data points with replacement (called bagging, short for bootstrap aggregating). (We can turn off the sampling with replacement and use all the data points by setting bootstrap = False when making the forest). Random sampling of **data points**, combined with random sampling of a **subset of the features** at each node fo the tree, is why the model is called a `random` forest.\n\nFurthermore, notice that in our tree, there are only 3 variables we actually used to make a prediction! According to this particular decision tree, the rest of the features are not important for making a prediction. Month of the year, day of the month, and our friends prediction are utterly useless for predicting the maximum temperature tomorrow! The only important information according to our simple tree is the temperature 1 day priior and the historical average, and one other. ","f0ab6fb1":"## Identify Anomalies\/Missing Data\nThere are 348 rows in the data (not 366 days in 2016),so there are several missing days.","3bd28105":"# Variable Importances\nIn order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables. The importances returned in Scikit-learn represent how much including a particular variable improves the prediction. The actual calculation of the importance is beyond the scope of this post, but we can use the numbers to make relative comparisons between variables.\n\nThe code here takes advantage of a number of tricks in the Python language, namely list comprehensive, zip, sorting, and argument unpacking. It's not that important to understand these at the moment, but if you want to become skilled at Python, these are tools you should have in your arsenal!","dd2034f0":"At the top of the list is temp_1, the max temperature of the day before. This tells us the best predictor of the max temperature for a day is the max temperature of the day before, a rather intuitive finding. The second most important factor is the historical aveage max tempterature, also not that surprising. Your friend turns out to not be very helpful, along with the day of the week, the year, the month, and the temperature 2 days prior. These importances all make sense as we would nto expect the day of the week to be a predictor of maximum temperatures as it has nothing to do with weathre. Moreover, the year is the same for all data points and hence provides us with no infomration for predicting the max temperature.\n\nIn the future implementation of the model, we can remove those variables that have no importance and the performance will not suffer. <font color = red>Additionally, if we are using a different model, say a support vector machine, we could use random forest feature importances as kind of feature selection method. <\/font> Let's quickly make a randomf orest with only the two most important variables, the max temperature 1 day prior and the historical averager and see how the performance compares.","23c82de7":"# Determine Performance Metrics\nTo put our predictions in perspective, we can calculate an accuracy using the mean average percentage error substracted from 100%.","c2112db0":"This tells us that we actually do not need all the data we collected to make accurate predictions! If we were continue using this model, we could only collect the two variables and achieve nearly the same performance. In a production setting, we would need to weigh the decrease in accuracy versus extra time required to obtain more information. Knowing how to find the right balance between performance and cost is an essential skill for a machine learning engineer and will ultimately depend on the problem!","602ac18e":"Next, we can plot the entire dataset with predictions highlighted. This requires a little data manipulation, but it is not too difficult. We can use this plot to determine if there are any outliers in either the data or our predictions.","c8ca7f20":"# Data Preparation\n## One-Hot Encoding\nThis step takes categorical variables (e.g., days of the week) and converts it to numerical representation without an arbitrary ordering. Days of the week are intuitive to us because we use them all the time.\nOne solution is to put numbers 1-7 for the days, but this might lead to the algorithm placing more importance on Sunday because it has a higher numerical value. Instead, we change the single column of weekdays into seven columns of binary data. this is best illustrated pictorially. Contvering\n<img src='https:\/\/miro.medium.com\/max\/431\/1*lw3v5DrfjwlAUXJb-P06IA.png'\/>\n\nTo\n<img src='https:\/\/miro.medium.com\/max\/994\/1*dYu_qkF2OKwnyS2YPr18iA.png'\/>","4b6f5e14":"# Train Model\nWe import the random forest regression model from scikit learn, instantitate the model, and fit (scikit-learns name for training) the model on the trasining data. ","54edee74":"# Improve Model if Necessary\nIn the usual machine learning workflow, this would be when start hyperparameter tuning. This is a complicated phrase that means \"adjust the settings to improve performance\" (The settings are known as hyperparameters to distinguish them from model parameters learned during training). The most common way to do this is simply make a bunch of model with different settings, evaluate them all on the same validations et, and see which one does best. Of cours,e this would be a tedious process to do by hand, and there are automated methods to do this process in Scikit learn. Hyperparameter tuning is often more engineering than theory-based.\n\n# Interpret Model and Report Results\nAt this point, we know our model is good, but it's pretty much a black box. We feed in some Numpy arrays for training, ask it to make a prediction, evaluate the predictions, and see that they are reasonable. The question is: how does this model arrive at the values? There are two approaches to get under the hood of the random forest: first, we can look at a single tree in the forest, and second, we can look at the feature importances of our explanatory variables.\n\n# Visualizing a Single Decision Tree\nOne of the coolest parts of Random Forest implementation in Scikit learn is we can actually examine any of the trees in the forest. We will select one tree, and save the whole tree as an image.\n\nThe following code takes one tree form the forest and saves it as an image.","a8f73412":"# Visualizations\nThe first chart I'll make a simple bar plot of the feature importances to illustrate the disparities in the relative significance of the variables. ","cba6422b":"# Random Forest\nLearned from [here](https:\/\/towardsdatascience.com\/random-forest-in-python-24d0893d51c0)\n## Data Acquisition\nThis is the weather data for Seattle, WA from 2016 using NOAA Climate Data Online tool."}}