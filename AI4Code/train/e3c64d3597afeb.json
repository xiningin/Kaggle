{"cell_type":{"e6b5de5a":"code","2f1dbceb":"code","c71450f0":"code","18b04b9d":"code","efa0ee83":"code","97c5f04d":"code","ccf4c66b":"code","1c2fb1d0":"code","206b4022":"code","1779aaa9":"code","ea74cd18":"code","bf5c42fc":"code","8c55aeb1":"code","beaed4c5":"code","a6e38444":"code","6aeaf071":"code","e6e5e52f":"code","3e2ae662":"code","986ae8d6":"code","b3c93cc3":"code","c3b484a7":"code","f52d955c":"code","7202380f":"code","27c03335":"code","8b3311e4":"code","03cfd507":"code","94adb30f":"code","4e668053":"code","4b33d17f":"code","fa835d04":"code","7939356d":"code","1b5dbda1":"code","e10a9ba6":"code","45a62b9c":"code","9e1c5f84":"code","a10443f6":"code","cc94867e":"code","a404459b":"code","2f1e71fd":"code","6a3f3f10":"code","e4a2dadd":"code","ff5a9b10":"code","3ac163c0":"code","7ff35a3b":"code","0a26fa60":"code","84318937":"code","a8fb9760":"code","91fd8c16":"code","d1b9f38c":"code","eb2f3b50":"code","24d8e43f":"code","5e930b83":"code","163fab78":"code","113fa834":"code","1ff5091a":"code","0cb99333":"code","0e15dd99":"code","63e566ff":"code","a0c8fb2b":"code","2f19cbe5":"code","18a2f5ec":"code","5b8e0675":"code","315ea2f0":"code","46026283":"code","cd92eef6":"code","03f9dad3":"code","cb6f2b67":"code","1b630a47":"code","46ca60f7":"code","b292b8f4":"code","ebfdfa16":"code","cc9fc9cb":"markdown","85e6e6ef":"markdown","72b78df2":"markdown","b83ed4fa":"markdown","4ae8a65e":"markdown","1b95a712":"markdown","0cddc76a":"markdown","0b51d2b5":"markdown","a9bebfff":"markdown","9d9398ad":"markdown","7366a093":"markdown","ef566873":"markdown","560a8ad7":"markdown","ea0763de":"markdown","378f2395":"markdown","cd24fdf1":"markdown","b9570c3f":"markdown","350f0b14":"markdown","57771ef1":"markdown","d09ae0cb":"markdown","14c539b4":"markdown","79f3fdc7":"markdown","59747b8e":"markdown","4f02bab5":"markdown","f1b53fa6":"markdown","17f8df76":"markdown","cd1149cf":"markdown","638bc1a5":"markdown","f62c5e2c":"markdown","67eb577d":"markdown","fef2d20f":"markdown","357bd788":"markdown","53868b64":"markdown","6e590e7f":"markdown","3f0ce97b":"markdown","923625a4":"markdown","a68fb7fc":"markdown","a16f5e52":"markdown","4cb13df7":"markdown","5e8a8e2d":"markdown","5e7f75fc":"markdown","9c4a5b85":"markdown","08556e3e":"markdown","3b08099e":"markdown","b5f088f3":"markdown","e66789f6":"markdown","8a01d6b7":"markdown","d6f5138b":"markdown","4d2a3ab2":"markdown","d9c5d5af":"markdown","f1eaea73":"markdown","968a9970":"markdown","85ff25cc":"markdown","1b4bbc17":"markdown","1f6c2b5e":"markdown","5890da8b":"markdown","e0f45d93":"markdown","3396035a":"markdown","05fda785":"markdown","ede5e8b2":"markdown","bcf2421f":"markdown","a606bbe8":"markdown","fd711370":"markdown","dbda103e":"markdown","716d8949":"markdown","50c90e17":"markdown","ccc3c464":"markdown","4516b5d1":"markdown","168db5c4":"markdown","e911d09b":"markdown","97ee9c5a":"markdown","e9eb6df8":"markdown","ca71d6ae":"markdown","3ec11f65":"markdown","de37b1b2":"markdown","680bf751":"markdown","aa5257bb":"markdown","634b58e8":"markdown","f1a3441f":"markdown","a7c9506f":"markdown","9cfa1b12":"markdown","be209a81":"markdown","cbd7d2f5":"markdown","666a41a2":"markdown","fcef3496":"markdown","ba5309a4":"markdown","0a6e5c1c":"markdown"},"source":{"e6b5de5a":"## Something went wrong when importing fastai.structured.\n## We fixed this by put the whole source code of fastai.structured in the notebook.\n## This was copied from: https:\/\/github.com\/anandsaha\/fastai.part1.v2\/blob\/master\/fastai\/structured.py\n\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nfrom sklearn.ensemble import forest\nfrom sklearn.tree import export_graphviz\n\ndef get_sample(df,n):\n    \"\"\" Gets a random sample of n rows from df, without replacement.\n    Parameters:\n    -----------\n    df: A pandas data frame, that you wish to sample from.\n    n: The number of rows you wish to sample.\n    Returns:\n    --------\n    return value: A random sample of n rows of df.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    >>> get_sample(df, 2)\n       col1 col2\n    2     3    a\n    1     2    b\n    \"\"\"\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()\n\ndef proc_df(df, y_fld, skip_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n\n    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n    changes the df into an entirely numeric dataframe.\n    Parameters:\n    -----------\n    df: The data frame you wish to process.\n    y_fld: The name of the response variable\n    skip_flds: A list of fields that dropped from df.\n    do_scale: Standardizes each column in df,Takes Boolean Values(True,False)\n    na_dict: a dictionary of na columns to add. Na columns are also added if there\n        are any missing values.\n    preproc_fn: A function that gets applied to df.\n    max_n_cat: The maximum number of categories to break into dummy values, instead\n        of integer codes.\n    subset: Takes a random subset of size subset from df.\n    mapper: If do_scale is set as True, the mapper variable\n        calculates the values used for scaling of variables during training time(mean and standard deviation).\n    Returns:\n    --------\n    [x, y, nas, mapper(optional)]:\n        x: x is the transformed version of df. x will not have the response variable\n            and is entirely numeric.\n        y: y is the response variable\n        nas: returns a dictionary of which nas it created, and the associated median.\n        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continous\n        variables which is then used for scaling of during test-time.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category { a : 1, b : 2}\n    >>> x, y, nas = proc_df(df, 'col1')\n    >>> x\n       col2\n    0     1\n    1     2\n    2     1\n    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n                          ([:children], StandardScaler())])\n    >>>round(fit_transform!(mapper, copy(data)), 2)\n    8x4 Array{Float64,2}:\n    1.0  0.0  0.0   0.21\n    0.0  1.0  0.0   1.88\n    0.0  1.0  0.0  -0.63\n    0.0  0.0  1.0  -0.63\n    1.0  0.0  0.0  -1.46\n    0.0  1.0  0.0  -0.63\n    1.0  0.0  0.0   1.04\n    0.0  0.0  1.0   0.21\n    \"\"\"\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    df = df.copy()\n    if preproc_fn: preproc_fn(df)\n    y = df[y_fld].values\n    df.drop(skip_flds+[y_fld], axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    res = [pd.get_dummies(df, dummy_na=True), y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\ndef set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n\ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n_samples))","2f1dbceb":"# For autoreloading modules\n%load_ext autoreload\n%autoreload 2\n# For notebook plotting\n%matplotlib inline\n\n# Standard libraries\nimport os\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pdpbox import pdp\nfrom plotnine import *\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor\nfrom IPython.display import display\n\n# Machine Learning\nimport sklearn\nfrom sklearn import metrics\nfrom scipy.cluster import hierarchy as hc\nfrom fastai.imports import *\n\n# Directories\nKAGGLE_DIR = '..\/input\/'\n\n# Info about dataset\nprint('Files and directories: \\n{}\\n'.format(os.listdir(\"..\/input\")))\n\nprint('\\n# File sizes')\nfor file in os.listdir(KAGGLE_DIR):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(KAGGLE_DIR + file) \/ 1000000, 2))))","c71450f0":"# Import dataset\ntrain = pd.read_csv(KAGGLE_DIR + 'train_V2.csv')\ntest = pd.read_csv(KAGGLE_DIR + 'test_V2.csv')","18b04b9d":"# First five rows (From Head)\nprint('First 5 rows: ')\ndisplay(train.head())\n\n# Last five rows (To Tail)\nprint('Last 5 rows: ')\ndisplay(train.tail())","efa0ee83":"# Stats\ntrain.describe()","97c5f04d":"# Types, Data points, memory usage, etc.\ntrain.info()\n\n# Check dataframe's shape\nprint('Shape of training set: ', train.shape)","ccf4c66b":"# Use this code if you want to store and read DataFrames in a feather format\n# os.makedirs('tmp', exist_ok=True)\n# train.to_feather('tmp\/PUBG')\n# df_raw = pd.read_feather('tmp\/PUBG')","1c2fb1d0":"# Check row with NaN value\ntrain[train['winPlacePerc'].isnull()]","206b4022":"# Drop row with NaN 'winPlacePerc' value\ntrain.drop(2744604, inplace=True)","1779aaa9":"# The row at index 2744604 will be gone\ntrain[train['winPlacePerc'].isnull()]","ea74cd18":"# playersJoined\ntrain['playersJoined'] = train.groupby('matchId')['matchId'].transform('count')\nplt.figure(figsize=(15,10))\nsns.countplot(train[train['playersJoined']>=75]['playersJoined'])\nplt.title('playersJoined')\nplt.show()","bf5c42fc":"# Create normalized features\ntrain['killsNorm'] = train['kills']*((100-train['playersJoined'])\/100 + 1)\ntrain['damageDealtNorm'] = train['damageDealt']*((100-train['playersJoined'])\/100 + 1)\ntrain['maxPlaceNorm'] = train['maxPlace']*((100-train['playersJoined'])\/100 + 1)\ntrain['matchDurationNorm'] = train['matchDuration']*((100-train['playersJoined'])\/100 + 1)\n# Compare standard features and normalized features\nto_show = ['Id', 'kills','killsNorm','damageDealt', 'damageDealtNorm', 'maxPlace', 'maxPlaceNorm', 'matchDuration', 'matchDurationNorm']\ntrain[to_show][0:11]","8c55aeb1":"# Create new feature healsandboosts\ntrain['healsandboosts'] = train['heals'] + train['boosts']\ntrain[['heals', 'boosts', 'healsandboosts']].tail()","beaed4c5":"# Create feature totalDistance\ntrain['totalDistance'] = train['rideDistance'] + train['walkDistance'] + train['swimDistance']\n# Create feature killsWithoutMoving\ntrain['killsWithoutMoving'] = ((train['kills'] > 0) & (train['totalDistance'] == 0))","a6e38444":"# Create headshot_rate feature\ntrain['headshot_rate'] = train['headshotKills'] \/ train['kills']\ntrain['headshot_rate'] = train['headshot_rate'].fillna(0)","6aeaf071":"# Check players who kills without moving\ndisplay(train[train['killsWithoutMoving'] == True].shape)\ntrain[train['killsWithoutMoving'] == True].head(10)","e6e5e52f":"# Remove outliers\ntrain.drop(train[train['killsWithoutMoving'] == True].index, inplace=True)","3e2ae662":"# Players who got more than 10 roadKills\ntrain[train['roadKills'] > 10]","986ae8d6":"# Drop roadKill 'cheaters'\ntrain.drop(train[train['roadKills'] > 10].index, inplace=True)","b3c93cc3":"# Plot the distribution of kills\nplt.figure(figsize=(12,4))\nsns.countplot(data=train, x=train['kills']).set_title('Kills')\nplt.show()","c3b484a7":"# Players who got more than 30 kills\ndisplay(train[train['kills'] > 30].shape)\ntrain[train['kills'] > 30].head(10)","f52d955c":"# Remove outliers\ntrain.drop(train[train['kills'] > 30].index, inplace=True)","7202380f":"# Plot the distribution of headshot_rate\nplt.figure(figsize=(12,4))\nsns.distplot(train['headshot_rate'], bins=10)\nplt.show()","27c03335":"# Players who made a minimum of 10 kills and have a headshot_rate of 100%\ndisplay(train[(train['headshot_rate'] == 1) & (train['kills'] > 9)].shape)\ntrain[(train['headshot_rate'] == 1) & (train['kills'] > 9)].head(10)","8b3311e4":"# Plot the distribution of longestKill\nplt.figure(figsize=(12,4))\nsns.distplot(train['longestKill'], bins=10)\nplt.show()","03cfd507":"# Check out players who made kills with a distance of more than 1 km\ndisplay(train[train['longestKill'] >= 1000].shape)\ntrain[train['longestKill'] >= 1000].head(10)","94adb30f":"# Remove outliers\ntrain.drop(train[train['longestKill'] >= 1000].index, inplace=True)","4e668053":"# Summary statistics for the Distance features\ntrain[['walkDistance', 'rideDistance', 'swimDistance', 'totalDistance']].describe()","4b33d17f":"# Plot the distribution of walkDistance\nplt.figure(figsize=(12,4))\nsns.distplot(train['walkDistance'], bins=10)\nplt.show()","fa835d04":"# walkDistance anomalies\ndisplay(train[train['walkDistance'] >= 10000].shape)\ntrain[train['walkDistance'] >= 10000].head(10)","7939356d":"# Remove outliers\ntrain.drop(train[train['walkDistance'] >= 10000].index, inplace=True)","1b5dbda1":"# Plot the distribution of rideDistance\nplt.figure(figsize=(12,4))\nsns.distplot(train['rideDistance'], bins=10)\nplt.show()","e10a9ba6":"# rideDistance anomalies\ndisplay(train[train['rideDistance'] >= 20000].shape)\ntrain[train['rideDistance'] >= 20000].head(10)","45a62b9c":"# Remove outliers\ntrain.drop(train[train['rideDistance'] >= 20000].index, inplace=True)","9e1c5f84":"# Plot the distribution of swimDistance\nplt.figure(figsize=(12,4))\nsns.distplot(train['swimDistance'], bins=10)\nplt.show()","a10443f6":"# Players who swam more than 2 km\ntrain[train['swimDistance'] >= 2000]","cc94867e":"# Remove outliers\ntrain.drop(train[train['swimDistance'] >= 2000].index, inplace=True)","a404459b":"# Plot the distribution of weaponsAcquired\nplt.figure(figsize=(12,4))\nsns.distplot(train['weaponsAcquired'], bins=100)\nplt.show()","2f1e71fd":"# Players who acquired more than 80 weapons\ndisplay(train[train['weaponsAcquired'] >= 80].shape)\ntrain[train['weaponsAcquired'] >= 80].head()","6a3f3f10":"# Remove outliers\ntrain.drop(train[train['weaponsAcquired'] >= 80].index, inplace=True)","e4a2dadd":"# Distribution of heals\nplt.figure(figsize=(12,4))\nsns.distplot(train['heals'], bins=10)\nplt.show()","ff5a9b10":"# 40 or more healing items used\ndisplay(train[train['heals'] >= 40].shape)\ntrain[train['heals'] >= 40].head(10)","3ac163c0":"# Remove outliers\ntrain.drop(train[train['heals'] >= 40].index, inplace=True)","7ff35a3b":"# Remaining players in the training set\ntrain.shape","0a26fa60":"print('There are {} different Match types in the dataset.'.format(train['matchType'].nunique()))","84318937":"# One hot encode matchType\ntrain = pd.get_dummies(train, columns=['matchType'])\n\n# Take a look at the encoding\nmatchType_encoding = train.filter(regex='matchType')\nmatchType_encoding.head()","a8fb9760":"# Turn groupId and match Id into categorical types\ntrain['groupId'] = train['groupId'].astype('category')\ntrain['matchId'] = train['matchId'].astype('category')\n\n# Get category coding for groupId and matchID\ntrain['groupId_cat'] = train['groupId'].cat.codes\ntrain['matchId_cat'] = train['matchId'].cat.codes\n\n# Get rid of old columns\ntrain.drop(columns=['groupId', 'matchId'], inplace=True)\n\n# Lets take a look at our newly created features\ntrain[['groupId_cat', 'matchId_cat']].head()","91fd8c16":"# Drop Id column, because it probably won't be useful for our Machine Learning algorithm,\n# because the test set contains different Id's\ntrain.drop(columns = ['Id'], inplace=True)","d1b9f38c":"# Take sample for debugging and exploration\nsample = 500000\ndf_sample = train.sample(sample)","eb2f3b50":"# Split sample into training data and target variable\ndf = df_sample.drop(columns = ['winPlacePerc']) #all columns except target\ny = df_sample['winPlacePerc'] # Only target variable","24d8e43f":"# Function for splitting training and validation data\ndef split_vals(a, n : int): \n    return a[:n].copy(), a[n:].copy()\nval_perc = 0.12 # % to use for validation set\nn_valid = int(val_perc * sample) \nn_trn = len(df)-n_valid\n# Split data\nraw_train, raw_valid = split_vals(df_sample, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\n# Check dimensions of samples\nprint('Sample train shape: ', X_train.shape, \n      'Sample target shape: ', y_train.shape, \n      'Sample validation shape: ', X_valid.shape)","5e930b83":"# Metric used for the PUBG competition (Mean Absolute Error (MAE))\nfrom sklearn.metrics import mean_absolute_error\n\n# Function to print the MAE (Mean Absolute Error) score\n# This is the metric used by Kaggle in this competition\ndef print_score(m : RandomForestRegressor):\n    res = ['mae train: ', mean_absolute_error(m.predict(X_train), y_train), \n           'mae val: ', mean_absolute_error(m.predict(X_valid), y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","163fab78":"# Train basic model\nm1 = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features='sqrt',\n                          n_jobs=-1)\nm1.fit(X_train, y_train)\nprint_score(m1)","113fa834":"# What are the most predictive features according to our basic random forest model\nfi = rf_feat_importance(m1, df); fi[:10]","1ff5091a":"# Plot a feature importance graph for the 20 most important features\nplot1 = fi[:20].plot('cols', 'imp', figsize=(14,6), legend=False, kind = 'barh')\nplot1","0cb99333":"# Use this code if you want to save the figure\n#fig = plot1.get_figure()\n#fig.savefig(\"Feature_importances(AllFeatures).png\")","0e15dd99":"# Keep only significant features\nto_keep = fi[fi.imp>0.005].cols\nprint('Significant features: ', len(to_keep))\nto_keep","63e566ff":"# Make a DataFrame with only significant features\ndf_keep = df[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","a0c8fb2b":"# Train model on top features\nm2 = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features='sqrt',\n                          n_jobs=-1)\nm2.fit(X_train, y_train)\nprint_score(m2)","2f19cbe5":"# Get feature importances of our top features\nfi_to_keep = rf_feat_importance(m2, df_keep)\nplot2 = fi_to_keep.plot('cols', 'imp', figsize=(14,6), legend=False, kind = 'barh')\nplot2\n\n# Use this code if you want to save the figure\n#fig = plot2.get_figure()\n#fig.savefig(\"Feature_importances(TopFeatures).png\")","18a2f5ec":"# Create a Dendrogram to view highly correlated features\ncorr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(14,10))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.plot()","5b8e0675":"# Use this code if you want to save the figure\n#plt.savefig('Dendrogram.png')","315ea2f0":"# Correlation heatmap\ncorr = df_keep.corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Create heatmap\nheatmap = sns.heatmap(corr)","46026283":"# Use this code if you want to save the figure\n#fig = heatmap.get_figure()\n#fig.savefig(\"Heatmap(TopFeatures).png\")","cd92eef6":"# Plot the predictive quality of kills \nx_all = get_sample(train, 100000)\nggplot(x_all, aes('kills','winPlacePerc'))+stat_smooth(se=True, colour='red', method='mavg')","03f9dad3":"# Plot the predictive quality of walkDistance\nx_all = get_sample(train, 100000)\nggplot(x_all, aes('walkDistance','winPlacePerc'))+stat_smooth(se=True, colour='red', method='mavg')","cb6f2b67":"# Prepare data\nval_perc_full = 0.12 # % to use for validation set\nn_valid_full = int(val_perc_full * len(train)) \nn_trn_full = len(train)-n_valid_full\ndf_full = train.drop(columns = ['winPlacePerc']) # all columns except target\ny = train['winPlacePerc'] # target variable\ndf_full = df_full[to_keep] # Keep only relevant features\nX_train, X_valid = split_vals(df_full, n_trn_full)\ny_train, y_valid = split_vals(y, n_trn_full)\n\n# Check dimensions of data\nprint('Sample train shape: ', X_train.shape, \n      'Sample target shape: ', y_train.shape, \n      'Sample validation shape: ', X_valid.shape)","1b630a47":"# Train final model\n# You should get better results by increasing n_estimators\n# and by playing around with the parameters\nm3 = RandomForestRegressor(n_estimators=70, min_samples_leaf=3, max_features=0.5,\n                          n_jobs=-1)\nm3.fit(X_train, y_train)\nprint_score(m3)","46ca60f7":"# Add engineered features to the test set\ntest['headshot_rate'] = test['headshotKills'] \/ test['kills']\ntest['headshot_rate'] = test['headshot_rate'].fillna(0)\ntest['totalDistance'] = test['rideDistance'] + test['walkDistance'] + test['swimDistance']\ntest['playersJoined'] = test.groupby('matchId')['matchId'].transform('count')\ntest['killsNorm'] = test['kills']*((100-test['playersJoined'])\/100 + 1)\ntest['damageDealtNorm'] = test['damageDealt']*((100-test['playersJoined'])\/100 + 1)\ntest['maxPlaceNorm'] = test['maxPlace']*((100-train['playersJoined'])\/100 + 1)\ntest['matchDurationNorm'] = test['matchDuration']*((100-test['playersJoined'])\/100 + 1)\ntest['healsandboosts'] = test['heals'] + test['boosts']\ntest['killsWithoutMoving'] = ((test['kills'] > 0) & (test['totalDistance'] == 0))\n\n# Turn groupId and match Id into categorical types\ntest['groupId'] = test['groupId'].astype('category')\ntest['matchId'] = test['matchId'].astype('category')\n\n# Get category coding for groupId and matchID\ntest['groupId_cat'] = test['groupId'].cat.codes\ntest['matchId_cat'] = test['matchId'].cat.codes\n\n# Remove irrelevant features from the test set\ntest_pred = test[to_keep].copy()\n\n# Fill NaN with 0 (temporary)\ntest_pred.fillna(0, inplace=True)\ntest_pred.head()","b292b8f4":"# Make submission ready for Kaggle\n# We use our final Random Forest model (m3) to get the predictions\npredictions = np.clip(a = m3.predict(test_pred), a_min = 0.0, a_max = 1.0)\npred_df = pd.DataFrame({'Id' : test['Id'], 'winPlacePerc' : predictions})\n\n# Create submission file\npred_df.to_csv(\"submission.csv\", index=False)","ebfdfa16":"# Last check of submission\nprint('Head of submission: ')\ndisplay(pred_df.head())\nprint('Tail of submission: ')\ndisplay(pred_df.tail())","cc9fc9cb":"We create a feature called 'healsandboosts' by adding heals and boosts. (duh!) We are not sure if this has additional predictive value, but we can always delete it later if the feature importance according to our random forest model is too low.","85e6e6ef":"There are a few matches with fewer than 75 players that are not displayed here. As you can see most of the matches are nearly packed a have nearly 100 players. It is nevertheless interesting to take these features into our analysis.","72b78df2":"[](http:\/\/)Hi fellow Kagglers! \n\nIn this Kernel we ([Dalton Harmsen](https:\/\/www.kaggle.com\/daltonharmsen), [Lourens Touwen](https:\/\/www.kaggle.com\/lourenst) and [Carlo Lepelaars](https:\/\/www.kaggle.com\/carlolepelaars)) will show you how we explore the [PUBG dataset](https:\/\/www.kaggle.com\/c\/pubg-finish-placement-prediction\/data), detect outliers and recognize important features. We also implement a random forest model and optimize it.\n\nIf you like this Kaggle kernel, feel free to give an upvote and leave a comment.\n\nA lot of inspiration for this kernel came from [fast.ai](https:\/\/www.fast.ai\/)'s \"[Machine Learning for Coders](https:\/\/course.fast.ai\/ml)\" course.\n\n![alt text](https:\/\/o.aolcdn.com\/images\/dims?quality=100&image_uri=http%3A%2F%2Fo.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2Fb0be09f425cc5175fb413bc03c32dd0d%2F206235889%2Fpubg-ed.jpg&client=amp-blogside-v2&signature=88c6b77342cbeb0d25c0dc9d909018136aec1971 \"Logo Title Text 1\")","b83ed4fa":"[Mean Absolute Error (MAE)](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error) is the metric that is used for this competition. The scikit-learn library already programmed this metric for us so we don't have to implement it from scratch.","4ae8a65e":"**Correlation Heatmap**","1b95a712":"# Preparation for Machine Learning <a id=\"8\"><\/a>","0cddc76a":"**Anomalies in travelling (rideDistance, walkDistance and swimDistance)**\n\nLet's check out anomalies in Distance travelled.","0b51d2b5":"And of course, we import our data from the Kaggle kernel directory and load it into two different DataFrames. one for the training data and one for the test data.","a9bebfff":"There is something fishy going on with these players. We are probably better off removing them from our dataset.\n\n![Alt Text](https:\/\/media.giphy.com\/media\/RHJkLqcdvMQF4GI3P7\/giphy.gif)","9d9398ad":"# Categorical Variables <a id=\"7\"><\/a>","7366a093":"# Final Random Forest Model <a id=\"10\"><\/a>","ef566873":"We try to identify cheaters by checking if people are getting kills without moving. We first identify the totalDistance travelled by a player and then set a boolean value to True if someone got kills without moving a single inch. We will remove cheaters in our outlier detection section.","560a8ad7":"The feature headshot_rate will also help us to catch cheaters.","ea0763de":"There are a lot of groupId's and matchId's so one-hot encoding them is computational suicide.\nWe will turn them into category codes. That way we can still benefit from correlations between groups and matches in our Random Forest algorithm.","378f2395":"![Alt Text](https:\/\/media.giphy.com\/media\/xT9IgnOQS8e8uKkflK\/giphy.gif)","cd24fdf1":"This is likely a very valuable feature for our model. If we know how many people are in a match we can normalize other features and get stronger predictions on individual players.","b9570c3f":"Let's plot the total kills for every player first. It doesn't look like there are too many outliers.","350f0b14":"**Anomalies in supplies part 2 (heals)**\n\nMost players us 5 healing items or less. We can again recognize some weird anomalies","57771ef1":"Summary Statistics of the training data.","d09ae0cb":"This time we use only the top features to train a random forest model. This often improves results a little bit.","14c539b4":"We will one hot encode the 'matchType' feature to use it in our Random Forest model.","79f3fdc7":"This is perhaps the most obvious sign of cheating in the game. It is already fishy if a player hasn't moved during the whole game, but the player could be AFK and got killed. However, if the player managed to get kills without moving it is most likely a cheater.","59747b8e":"Let's delete this entry:","4f02bab5":"**Feature importance for top features**","f1b53fa6":"**Normalize features**","17f8df76":"Data types, memory usage, shape, etc.","cd1149cf":"Do you think these guys are legit?\n\n![Alt Text](https:\/\/thumbs.gfycat.com\/EvenSpiffyFerret-size_restricted.gif)","638bc1a5":"Now that we have a feature 'playersJoined' we can normalize other features based on the amount of players. Features that can be valuable to normalize are:\n1. kills\n2. damageDealt\n3. maxPlace\n4. matchDuration\n\nLet's try out some things!","f62c5e2c":"Let's take a look at the players who make these shots.","67eb577d":"## Sampling","fef2d20f":"**Anomalies in supplies (weaponsAcquired)**\n\nMost people acquire between 0 and 10 weapons in a game, but you also see some people acquire more than 80 weapons! Let's check these guys out.","357bd788":"Cheaters or do they just like to ride like these guys?\n\n![Alt Text](https:\/\/media.giphy.com\/media\/qlCFjkSruesco\/giphy.gif)","53868b64":"**Anomalies in aim part 3 (Longest kill)**","6e590e7f":"Again, we first take a look at the whole dataset and create a new feature 'headshot_rate'.\nWe see that the most players score in the 0 to 10% region. However, there are a few anomalies that have a headshot_rate of 100% percent with more than 9 kills!","3f0ce97b":"The [fastai](https:\/\/www.fast.ai\/) library gives us an easy way to analyze feature importances from a random forest algorithm with just one line of code!","923625a4":"**Kills without movement**","a68fb7fc":"**voil\u00e0!**","a16f5e52":"# Kaggle Submission <a id=\"11\"><\/a>","4cb13df7":"**Anomalies in roadKills**","5e8a8e2d":"**Anomalies in aim part 2 (100% headshot rate)**","5e7f75fc":"**Dendrogram (to view correlation of features)**","9c4a5b85":"Fellow Kaggler '[averagemn](https:\/\/www.kaggle.com\/donkeys)' brought to our attention that there is one particular player with a 'winPlacePerc' of NaN. The case was that this match had only one player. We will delete this row from our dataset.","08556e3e":"What do you think? Should we remove all these outliers from our dataset?","3b08099e":"First we import the dependencies needed for handling data, visualization and training our model. \n\nImportant dependencies are:\n* [Pandas](https:\/\/pandas.pydata.org) for their dataframe structures and easy visualization.\n* [Matplotlib](https:\/\/matplotlib.org) for visualization.\n* [Scikit-learn](https:\/\/scikit-learn.org\/stable) for machine learning.\n* [fastai](https:\/\/www.fast.ai) for machine learning and feature importance.","b5f088f3":"## First basic Random Forest Model","e66789f6":"# Preparation <a id=\"1\"><\/a>","8a01d6b7":"## Split target variable, validation data, etc.","d6f5138b":"**Predictive quality of walkDistance**","4d2a3ab2":"### Players Joined","d9c5d5af":"Most kills are made from a distance of 100 meters or closer. There are however some outliers who make a kill from more than 1km away. This is probably done by cheaters.","f1eaea73":"# Let's Go!","968a9970":"![Alt Text](https:\/\/media.giphy.com\/media\/OPRbXcsGctvZC\/giphy.gif)","85ff25cc":"## Correlations","1b4bbc17":"Got the suckers! ","1f6c2b5e":"# Initial Exploration <a id=\"3\"><\/a>","5890da8b":"Let's look at the DataFrame from head to tail.","e0f45d93":"**rideDistance**","3396035a":"# Feature Importance <a id=\"9\"><\/a>","05fda785":"# Table of Contents","ede5e8b2":"![API Img](http:\/\/media.comicbook.com\/2018\/03\/pubg-api-1093349.jpeg)\n","bcf2421f":"**Predictive quality of kills**","a606bbe8":"And he's gone!","fd711370":"### Normalized features","dbda103e":"**Outlier conclusions**","716d8949":"### Heals and Boosts","50c90e17":"**Anomalies in aim (More than 45 kills)**","ccc3c464":"# Outlier Detection <a id=\"6\"><\/a>","4516b5d1":"Earlier in this kernel we created the new features ''totalDistance'' and  ''headshot_rate\". In this section we add more interesting features to improve the predictive quality of our machine learning models.\n\nInitial ideas for this section come from [this amazing kernel](https:\/\/www.kaggle.com\/deffro\/eda-is-fun).\n\nNote: It is important with feature engineering that you also add the engineered features to your test set!","168db5c4":"Note that player c3e444f7d1289d drove 5 meters but killed 14 people with it. Sounds insane doesn't it?\n\n![Alt Text](https:\/\/media.giphy.com\/media\/3o7aD85usFbbbrCR3i\/giphy.gif)","e911d09b":"**Check of submission file**","97ee9c5a":"## Set metrics (MAE)","e9eb6df8":"It is always nice to take a look at few of your predictions to make sure that the structure is right for a Kaggle submission.","ca71d6ae":"* [Preparation](#1)\n* [Extra Data (Coming Soon)](#2)\n* [Initial Exploration](#3)\n* [Illegal Match](#4)\n* [Feature Engineering](#5)\n* [Outlier Detection](#6)\n* [Categorical Variables](#7)\n* [Preparation for Machine Learning](#8)\n* [Feature Importance](#9)\n* [Final Random Forest Model](#10)\n* [Kaggle Submission](#11)\n","3ec11f65":"We removed about 2000 players from our dataset. Do you think this is too much? Please let us know in the comments.","de37b1b2":"# Illegal Match <a id=\"4\"><\/a>","680bf751":"### Feature descriptions (From Kaggle)\n\n* DBNOs - Number of enemy players knocked.\n* assists - Number of enemy players this player damaged that were killed by teammates.\n* boosts - Number of boost items used.\n* damageDealt - Total damage dealt. Note: Self inflicted damage is subtracted.\n* headshotKills - Number of enemy players killed with headshots.\n* heals - Number of healing items used.\n* Id - Player\u2019s Id\n* killPlace - Ranking in match of number of enemy players killed.\n* killPoints - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a \u201cNone\u201d.\n* killStreaks - Max number of enemy players killed in a short amount of time.\n* kills - Number of enemy players killed.\n* longestKill - Longest distance between player and player killed at time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat.\n* matchDuration - Duration of match in seconds.\n* matchId - ID to identify match. There are no matches that are in both the training and testing set.\n* matchType - String identifying the game mode that the data comes from. The standard modes are \u201csolo\u201d, \u201cduo\u201d, \u201csquad\u201d, \u201csolo-fpp\u201d, \u201cduo-fpp\u201d, and \u201csquad-fpp\u201d; other modes are from events or custom matches.\n* rankPoints - Elo-like ranking of player. This ranking is inconsistent and is being deprecated in the API\u2019s next version, so use with caution. Value of -1 takes place of \u201cNone\u201d.\n* revives - Number of times this player revived teammates.\n* rideDistance - Total distance traveled in vehicles measured in meters.\n* roadKills - Number of kills while in a vehicle.\n* swimDistance - Total distance traveled by swimming measured in meters.\n* teamKills - Number of times this player killed a teammate.\n* vehicleDestroys - Number of vehicles destroyed.\n* walkDistance - Total distance traveled on foot measured in meters.\n* weaponsAcquired - Number of weapons picked up.\n* winPoints - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a \u201cNone\u201d.\n* groupId - ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.\n* numGroups - Number of groups we have data for in the match.\n* maxPlace - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.\n* winPlacePerc - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.\n\n[Source](https:\/\/www.kaggle.com\/c\/pubg-finish-placement-prediction\/data)","aa5257bb":"We should probably remove these outliers from our model. Do you agree?\n\nNote that player 3f2bcf53b108c4 acquired 236 weapons in one game!\n\n![Alt Text](https:\/\/media.giphy.com\/media\/69lWR6c8Afx9qeg2Tu\/giphy.gif)","634b58e8":"### Killing without moving","f1a3441f":"# Feature Engineering <a id=\"5\"><\/a>","a7c9506f":"**walkDistance**","9cfa1b12":"Let's take a closer look.","be209a81":"It is unclear if these players are cheating so we are probably not deleting these players from the dataset.\nIf they are legitimate players, they are probably really crushing the game!\n\n![Alt Text](https:\/\/media.giphy.com\/media\/l3mZrOajz5VCZf7Hy\/giphy.gif)\n","cbd7d2f5":"**swimDistance**","666a41a2":"## Second Random Forest Model","fcef3496":"Some rows in our dataset have weird characteristics. The players could be cheaters, maniacs or just anomalies. Removing these outliers will likely improve results.\n\nInspiration for this section comes from [this amazing Kaggle Kernel.](https:\/\/www.kaggle.com\/rejasupotaro\/cheaters-and-zombies)","ba5309a4":"# PUBG Data Exploration + Random Forest (+ Funny GIFs)","0a6e5c1c":"We will take a sample of 500000 rows from our training set for easy debugging and exploration."}}