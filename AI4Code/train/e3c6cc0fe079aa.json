{"cell_type":{"302e407e":"code","81c0f36c":"code","40221eb8":"code","da0c9275":"code","2bbe6d76":"code","315dd200":"code","ac2e3483":"code","f6c45f19":"code","63649e07":"code","e3b02b34":"code","b2b6109d":"code","8a9d5412":"code","871fdfc6":"code","f38fc6e5":"code","cd1453c7":"code","831409e8":"code","c0f535ac":"code","ec27c268":"code","c1324d7e":"code","4969b445":"code","4f788450":"code","5df67855":"code","ecee9570":"markdown"},"source":{"302e407e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Imputing missing values\nfrom sklearn.impute import KNNImputer\n\nfrom scipy.stats import chi2_contingency\n# Feature engineering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n# Model processing and testing\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve, precision_score, recall_score\nfrom sklearn.feature_selection import RFE\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier #Decision tree\nfrom sklearn.naive_bayes import GaussianNB #Naive_bayes\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV","81c0f36c":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","40221eb8":"df.isnull().sum()","da0c9275":"#Replacing the special character to nan and then drop the columns\ndf['bmi'] = df['bmi'].replace('?',np.nan)\n#Dropping the NaN rows now \ndf.dropna(how='any',inplace=True)","2bbe6d76":"df.isnull().sum()","315dd200":"#Assigning the numeric values to the string type variables\nnumber = LabelEncoder()\ndf['ever_married'] = number.fit_transform(df['ever_married'])\ndf['work_type'] = number.fit_transform(df['work_type'])\ndf['Residence_type'] = number.fit_transform(df['Residence_type'])\ndf['smoking_status'] = number.fit_transform(df['smoking_status'])\ndf['gender'] = number.fit_transform(df['gender'])","ac2e3483":"df.head()","f6c45f19":"X = df.drop(\"stroke\",1)\ny = df[\"stroke\"]","63649e07":"#Declaring the train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33,random_state=0)","e3b02b34":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)","b2b6109d":"X_train.head()","8a9d5412":"# feature selection\ndef select_features(X_train, y_train, X_test):\n\t# configure to select all features\n\tfs = SelectKBest(score_func=f_classif, k='all')\n\t# learn relationship from training data\n\tfs.fit(X_train, y_train)\n\t# transform train input data\n\tX_train_fs = fs.transform(X_train)\n\t# transform test input data\n\tX_test_fs = fs.transform(X_test)\n\treturn X_train_fs, X_test_fs, fs\n ","871fdfc6":"# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n# what are scores for the features\nfor i in range(len(fs.scores_)):\n\tprint('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()","f38fc6e5":"# Logistic Regression\nLR = LogisticRegression()\nLR.fit(X_train_fs, y_train)\ny_pred = LR.predict(X_test_fs)\nscore_LR = LR.score(X_test_fs,y_test)\nprint('The accuracy of the Logistic Regression model is', score_LR)\ntargets = ['0' , '1']\nprint(classification_report(y_test, y_pred,target_names=targets))","cd1453c7":"# Support Vector Classifier (SVM\/SVC)\nfrom sklearn.svm import SVC\nsvc = SVC(gamma=0.22)\nsvc.fit(X_train_fs, y_train)\ny_pred = svc.predict(X_test_fs)\nscore_svc = svc.score(X_test_fs,y_test)\nprint('The accuracy of SVC model is', score_svc)\ntargets = ['0' , '1']\nprint(classification_report(y_test, y_pred,target_names=targets))","831409e8":"# Random Forest Classifier\nRF = RandomForestClassifier()\nRF.fit(X_train_fs, y_train)\ny_pred = RF.predict(X_test_fs)\nscore_RF = RF.score(X_test_fs,y_test)\nprint('The accuracy of the Random Forest Model is', score_RF)\ntargets = ['0' , '1']\nprint(classification_report(y_test, y_pred,target_names=targets))","c0f535ac":"# Decision Tree\nDT = DecisionTreeClassifier()\nDT.fit(X_train_fs,y_train)\ny_pred = DT.predict(X_test_fs)\nscore_DT = DT.score(X_test_fs,y_test)\nprint(\"The accuracy of the Decision tree model is \",score_DT)\ntargets = ['0' , '1']\nprint(classification_report(y_test, y_pred,target_names=targets))","ec27c268":"# Gaussian Naive Bayes\nGNB = GaussianNB()\nGNB.fit(X_train_fs, y_train)\ny_pred = GNB.predict(X_test_fs)\nscore_GNB = GNB.score(X_test_fs,y_test)\nprint('The accuracy of Gaussian Naive Bayes model is', score_GNB)\ntargets = ['0' , '1']\nprint(classification_report(y_test, y_pred,target_names=targets))","c1324d7e":"# K-Nearest Neighbors\nknn = KNeighborsClassifier()\nknn.fit(X_train_fs, y_train)\ny_pred = knn.predict(X_test_fs)\nscore_knn = knn.score(X_test_fs,y_test)\nprint('The accuracy of the KNN Model is',score_knn)\ntargets = ['0' , '1']\nprint(classification_report(y_test, y_pred,target_names=targets))","4969b445":"# define the evaluation method\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define the pipeline to evaluate\nmodel = LogisticRegression(solver='liblinear')\nfs = SelectKBest(score_func=f_classif)\npipeline = Pipeline(steps=[('anova',fs), ('lr', model)])\n# define the grid\ngrid = dict()\ngrid['anova__k'] = [i+1 for i in range(X.shape[1])]\n# define the grid search\nsearch = GridSearchCV(pipeline, grid, scoring='accuracy', n_jobs=-1, cv=cv)\n# perform the search\nresults = search.fit(X, y)\n# summarize best\nprint('Best Mean Accuracy: %.3f' % results.best_score_)\nprint('Best Config: %s' % results.best_params_)","4f788450":"import numpy as np \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nrandom_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': list(np.linspace(10, 1200, 10, dtype = int)) + [None],\n               'max_features': ['auto', 'sqrt','log2', None],\n               'min_samples_leaf': [4, 6, 8, 12],\n               'min_samples_split': [5, 7, 10, 14],\n               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int))}\n\nclf = RandomForestClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 80, \n                               cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\nmodel.fit(X_train,y_train)","5df67855":"predictionforest = model.best_estimator_.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))\nacc3 = accuracy_score(y_test,predictionforest)","ecee9570":"**ANOVA f-test Feature Selection**"}}