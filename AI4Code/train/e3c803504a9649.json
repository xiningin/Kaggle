{"cell_type":{"fdf5440a":"code","f7716ca3":"code","6b7495b4":"code","35b6bb8c":"code","74f4003e":"code","ab751f69":"code","4bad92f2":"code","abf77828":"code","ffee90b2":"code","1cbfe48d":"code","988abec0":"code","f113833a":"code","5a193e26":"code","04a4f598":"code","7567dc88":"code","5576e0bd":"code","1f85f304":"code","b97b28fb":"code","da21c1a7":"code","ade68537":"code","e29939e6":"code","006beb60":"code","e49d1376":"code","d92949e0":"code","0c46a4f1":"code","a7a7dad0":"code","ccf2204b":"code","2c081134":"code","c49248e9":"code","d305e23a":"code","8a865bc2":"code","43fc687e":"code","6cd31e29":"code","aad4bbbe":"code","f7847c08":"code","4ac09c1f":"code","1420f7f3":"code","5fab16d0":"code","7bbd65fb":"code","e2c8cf87":"code","bba2aa3b":"code","896efa10":"code","2f346e68":"code","db24e8a3":"code","3e746f8f":"code","343bbaba":"code","9fbdcc94":"code","383854cc":"code","639f73f2":"code","d8f7568f":"code","599d0974":"code","9c90529b":"code","df22b1d8":"code","1b9fdc6c":"code","1ae88372":"code","2006a13d":"code","0a6bc9a5":"code","d9375b48":"code","23034b84":"code","e9c9b2c5":"code","ddb2f13b":"code","8be33d59":"code","87be57a8":"code","a4556be1":"code","806197e1":"code","542eed90":"code","d4fb5dc6":"code","692a8b13":"code","1fd5b66d":"code","5210928b":"code","d3fdf16e":"code","7f266017":"code","ffe9de33":"code","dd7360dc":"code","edbabdff":"code","3488a563":"code","baaced4f":"code","780541a1":"code","ba6e4f1d":"code","eef14e83":"code","c4260ff8":"code","6e1b7723":"code","27dedeac":"code","f4bd95a9":"code","6125b4a8":"markdown","58177c66":"markdown","e2ae0745":"markdown","88e150ed":"markdown","1481cb6d":"markdown","d3779e89":"markdown","31761ca0":"markdown","7df78501":"markdown"},"source":{"fdf5440a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7716ca3":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer ","6b7495b4":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","35b6bb8c":"train.shape","74f4003e":"sns.heatmap(train.isnull(),yticklabels=False, cbar=False)","ab751f69":"pd.set_option('display.max_rows',None)","4bad92f2":"train.isnull().sum().sort_values(ascending=False).head(20)","abf77828":"train.isnull().sum().sort_values(ascending=False).index\n# drop 'PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage' \n# replace with mode for 'GarageCond','GarageType','GarageYrBlt','GarageFinish','GarageQual','BsmtExposure','BsmtFinType2','BsmtFinType1',\n# 'BsmtCond', 'BsmtQual', 'MasVnrArea', 'MasVnrType'\n# drop na for Electrical ","ffee90b2":"train.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage'],1,inplace=True)","1cbfe48d":"for c in 'GarageCond','GarageType','GarageYrBlt','GarageFinish','GarageQual','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond', 'BsmtQual', 'MasVnrArea', 'MasVnrType': \n    train[c] = train[c].fillna(train[c].mode()[0])","988abec0":"train.dropna(inplace=True)","f113833a":"train.isnull().sum().sort_values(ascending=False).head(3)","5a193e26":"#do the same for test set \ntest.isnull().sum().sort_values(ascending=False).head(30)","04a4f598":"test.isnull().sum().sort_values(ascending=False).index","7567dc88":"test.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage'],1,inplace=True)","5576e0bd":"for c in 'GarageCond','GarageType','GarageYrBlt','GarageFinish','GarageQual','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond', 'BsmtQual', 'MasVnrArea', 'MasVnrType': \n    test[c] = test[c].fillna(test[c].mode()[0])\n","1f85f304":"# test.dropna(inplace=True)","b97b28fb":"test.isnull().sum().sort_values(ascending=False).head(10)","da21c1a7":"test.shape","ade68537":"train.shape","e29939e6":"#concat and apply one hot encoding\ndf = pd.concat([train, test], axis=0)","006beb60":"df['SalePrice']","e49d1376":"df.shape","d92949e0":"columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n        'SaleCondition','ExterCond',\n         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n         'CentralAir',\n         'Electrical','KitchenQual','Functional',\n         'GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']","0c46a4f1":"df['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x>0 else 0)\ndf['hasBsmt'] = df['TotalBsmtSF'].apply(lambda x:1 if x>0 else 0)\ndf['hasPool'] = df['PoolArea'].apply(lambda x:1 if x>0 else 0)","a7a7dad0":"df['hasPool'].head()","ccf2204b":"df1 = pd.get_dummies(df[columns]);\ndf1.shape","2c081134":"def category_onehot_multcols(multcolumns):\n    df_final=df\n    i=0\n    for fields in multcolumns:\n        \n        print(fields)\n        df12=pd.get_dummies(df[fields],drop_first=True)\n        \n        df.drop([fields],axis=1,inplace=True)\n        if i==0:\n            df_final=df12.copy()\n        else:\n            \n            df_final=pd.concat([df_final,df12],axis=1)\n        i=i+1\n       \n        \n    df_final=pd.concat([df,df_final],axis=1)\n        \n    return df_final","c49248e9":"final_df = category_onehot_multcols(columns)","d305e23a":"final_df.shape","8a865bc2":"final_df = final_df.loc[:,~final_df.columns.duplicated()]\n#next time check duplicated columns ","43fc687e":"final_df.shape","6cd31e29":"df_train = final_df.iloc[:1459,:]\ndf_test = final_df.iloc[1459:,:]","aad4bbbe":"df_test.isnull().sum().max","f7847c08":"df_train.head()","4ac09c1f":"df_test.drop(['SalePrice'],axis=1,inplace=True)","1420f7f3":"#check correlations\ncorr = df_train.corr()\ncols = corr.nlargest(20,'SalePrice')['SalePrice'].index \ncols_T = np.corrcoef(df_train[cols].values.T)\n#plot\nf, ax = plt.subplots(figsize=(14,12))\nsns.heatmap(cols_T, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':10}, xticklabels=cols.values, yticklabels=cols.values)\nplt.show()\n","5fab16d0":"cols.drop('SalePrice')","7bbd65fb":"x_train,x_test,y_train,y_test = train_test_split(df_train.drop('SalePrice',1),df_train['SalePrice'],test_size=0.3,random_state=0)","e2c8cf87":"lm = LinearRegression()\nlm.fit(x_train,y_train)\nlm_pred = lm.predict(x_test)\nsns.distplot(lm_pred,fit=norm)\n\nfrom sklearn.metrics import r2_score\nprint('Coefficient of determination: %.2f'\n      % r2_score(y_test, lm_pred))","bba2aa3b":"#try a sub model \nX20 = df_train[cols]\nx_train,x_test,y_train,y_test = train_test_split(X20.drop('SalePrice',1),X20['SalePrice'],test_size=0.3,random_state=0)\n\nlm.fit(x_train,y_train)\nlm_pred = lm.predict(x_test)\nsns.distplot(lm_pred,fit=norm)\n\nprint('Coefficient of determination: %.2f'\n      % r2_score(y_test, lm_pred))","896efa10":"import pickle\nfilename = 'final_model.pkl'\npickle.dump(lm, open(filename, 'wb'))","2f346e68":"df_test.shape","db24e8a3":"df_train.shape","3e746f8f":"from sklearn.impute import SimpleImputer","343bbaba":"imp_mean = SimpleImputer(strategy='mean')\ntestset = imp_mean.fit(df_test[cols.drop('SalePrice')])\ntestset = imp_mean.transform(df_test[cols.drop('SalePrice')])","9fbdcc94":"test_pred = lm.predict(testset)","383854cc":"sub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","639f73f2":"pred=pd.DataFrame(test_pred)\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\ndatasets.to_csv('sample_submission.csv',index=False)","d8f7568f":"#explore XGB and other models tmr (watch the video)\n#start Angular","599d0974":"import xgboost as xgb","9c90529b":"x1 = X20.drop('SalePrice',1)\ny1 = X20['SalePrice']","df22b1d8":"#basic XGB model\nxgb_model = xgb.XGBRegressor(objective='reg:linear',random_state=42)\nx_train,x_test,y_train,y_test = train_test_split(X20.drop('SalePrice',1),X20['SalePrice'],test_size=0.3,random_state=0)\nxgb_model.fit(x_train,y_train)\n\ny_pred = xgb_model.predict(x_test)\nsns.distplot(y_pred)\nmse = mean_squared_error(y_test,y_pred)\nprint(np.sqrt(mse))\nprint('Coefficient of determination: %.2f'\n      % r2_score(y_test, y_pred))","1b9fdc6c":"#cross validation\nscores = cross_val_score(xgb_model, x1,y1,scoring='neg_mean_squared_error',cv=5)\nscores.mean()\n# display_scores(np.sqrt(-scores))","1ae88372":"import warnings\nwarnings.filterwarnings('ignore')","2006a13d":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","0a6bc9a5":"#hyperparameter tuning\nfrom scipy.stats import uniform, randint\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nsearch = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(x1, y1)\n\n# report_best_scores(search.cv_results_, 1)\n\n#https:\/\/www.kaggle.com\/stuarthallows\/using-xgboost-with-scikit-learn ","d9375b48":"# x = df_train.drop(['SalePrice'],axis=1).select_dtypes(exclude=['object']) \u53ef\u4ee5\u6392\u5217\u7ec4\u5408\u597d\u5389\u5bb3","23034b84":"# xgb_model \nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import make_pipeline #start with a transformer and end with a model \nfrom sklearn.impute import SimpleImputer\n\n# x_train,x_test,y_train,y_test\n# df_test[cols.drop('SalePrice')]","e9c9b2c5":"test20 = df_test[cols.drop('SalePrice')]","ddb2f13b":"test20.shape","8be33d59":"x_train.shape","87be57a8":"my_pipeline = make_pipeline(SimpleImputer(), xgb.XGBRegressor())","a4556be1":"my_pipeline.fit(x_train,y_train)\npred = my_pipeline.predict(x_test)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(pred, y_test)))\n","806197e1":"my_pipeline2 = make_pipeline(SimpleImputer(), xgb.XGBRegressor(n_estimators=1000, learning_rate = 0.1))\nmy_pipeline2.fit(x_train,y_train)\npred = my_pipeline2.predict(x_test)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(pred, y_test)))\n#current best","542eed90":"#get output\nypred = my_pipeline2.predict(testset)\n\nsub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\npred=pd.DataFrame(ypred)\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\ndatasets.to_csv('sample_submission3.csv',index=False)","d4fb5dc6":"def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n                       do_probabilities = False):\n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid, \n        cv=cv, \n        n_jobs=-1, \n        scoring=scoring_fit,\n        verbose=2\n    )\n    fitted_model = gs.fit(X_train_data, y_train_data)\n    \n    if do_probabilities:\n      pred = fitted_model.predict_proba(X_test_data)\n    else:\n      pred = fitted_model.predict(X_test_data)\n    \n    return fitted_model, pred","692a8b13":"# hyperparameter tuning\n# max_depth, min_child_weight, gamma and learning rate\n# https:\/\/mlfromscratch.com\/gridsearch-keras-sklearn\/#\/\n    \n# model = xgb.XGBRegressor()\n# param_grid = {\n#     'n_estimators': [400, 700, 1000],\n#     'learning_rate': [0.05, 0.1, 0.2, 0.3],\n#     'colsample_bytree': [0.7, 0.8],\n#     'max_depth': [15,20,25],\n#     'reg_alpha': [1.1, 1.2, 1.3],\n#     'reg_lambda': [1.1, 1.2, 1.3],\n#     'subsample': [0.7, 0.8, 0.9]\n# }\n\n# model, pred = algorithm_pipeline(x_train, x_test, y_train, y_test, model, \n#                                  param_grid, cv=5)\n\n# # Root Mean Squared Error\n# print(np.sqrt(-model.best_score_))\n# print(model.best_params_)","1fd5b66d":"\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier","5210928b":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","d3fdf16e":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n#         'subsample': [0.6, 0.8, 1.0],\n#         'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","7f266017":"xgb = XGBClassifier(learning_rate=0.1, n_estimators=500, \n                    silent=True, nthread=1)","ffe9de33":"# folds = 5\n# param_comb = 5\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 101)\n\n# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='neg_mean_squared_error', n_jobs=4, cv=skf.split(x1,y1), verbose=3, random_state=101 )\n\n# # Here we go\n# start_time = timer(None) # timing starts from this point for \"start_time\" variable\n# random_search.fit(x1,y1,)\n# timer(start_time) # timing ends here for \"start_time\" variable","dd7360dc":"# print('\\n All results:')\n# print(random_search.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search.best_estimator_)\n# print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n# print(random_search.best_score_)\n# print('\\n Best hyperparameters:')\n# print(random_search.best_params_)\n# results = pd.DataFrame(random_search.cv_results_)\n# results.to_csv('xgb-random-grid-search-results-01.csv', index=False)","edbabdff":"xgb_2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=2, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.1, max_delta_step=0, max_depth=5, min_child_weight=1, n_estimators=500, n_jobs=1, nthread=1, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None)","3488a563":"xgb_2.fit(x_train,y_train)\npred = xgb_2.predict(x_test)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(pred, y_test)))","baaced4f":"#creat partial dependence plot - so easier to measure \nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nmy_plots = plot_partial_dependence(xgb_2,\n                                  features=[0,2],\n                                  X=x_train,\n                                  grid_resolution=10)\n#partial_dependence - a function to get raw data, allowing other plotting stuff to make nicer plots\n#though it may be argued that you cannot derive causal relations from these plots unless from experimental data, \n#it is still effective in explaining your findings to non-technical people","780541a1":"#########","ba6e4f1d":"#get output\nypred = model.predict(testset)\nsns.distplot(ypred,fit=norm)\n\nsub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\npred=pd.DataFrame(ypred)\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\ndatasets.to_csv('sample_submission3.csv',index=False)","eef14e83":"ypred.shape","c4260ff8":"my_pipeline2 = make_pipeline(SimpleImputer(), xgb.XGBRegressor(n_estimators=2000, learning_rate = 0.1))\nmy_pipeline2.fit(x_train,y_train)\npred = my_pipeline2.predict(x_test)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(pred, y_test)))","6e1b7723":"my_model = xgb.XGBRegressor(n_estimators=1000, learning_rate = 0.05)  \nmy_model.fit(x_train,y_train,early_stopping_rounds=5,eval_set=[(x_test,y_test)],verbose=False)\n\ny_pred = my_model.predict(x_test)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(y_pred, y_test)))\n","27dedeac":"# new learning rate \nmy_model = xgb.XGBRegressor(n_estimators=1000, learning_rate = 0.1)  \nmy_model.fit(x_train,y_train,early_stopping_rounds=5,eval_set=[(x_test,y_test)],verbose=False)\n\ny_pred = my_model.predict(x_test)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(y_pred, y_test)))","f4bd95a9":"ypred = my_model.predict(df_test[cols.drop('SalePrice')])\n\nsub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\npred=pd.DataFrame(ypred)\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\ndatasets.to_csv('sample_submission2.csv',index=False)","6125b4a8":"Very comprehensive hyperparameter tuning document -- https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\nEnd notes:\n1. hyperparameter tuning can be time consuming (takes a long time to train xgboost)\n2. can't improve much w\/ simply model change, but feature engineering and ensemble models, like stacking, are more helpful\n3. hp tuning, is it iterative? won't tune regularizaiton parameters in the latter part change the result of max_depth, min_child_weight and gamma in the first place?","58177c66":" All results:\n{'mean_fit_time': array([458.71556292, 451.55773573, 548.13220019, 454.64832773,\n       409.53988581]), 'std_fit_time': array([ 2.47482246,  2.79227067,  3.58496338,  4.64027233, 68.03163166]), 'mean_score_time': array([ 89.32943444,  92.37474055, 100.77147894,  94.52568417,\n        78.44861789]), 'std_score_time': array([ 0.86910983,  3.40912825,  6.76226085,  3.67779347, 16.19733977]), 'param_min_child_weight': masked_array(data=[5, 10, 1, 10, 10],\n             mask=[False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_max_depth': masked_array(data=[3, 5, 5, 4, 4],\n             mask=[False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_gamma': masked_array(data=[0.5, 5, 2, 1, 5],\n             mask=[False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'min_child_weight': 5, 'max_depth': 3, 'gamma': 0.5}, {'min_child_weight': 10, 'max_depth': 5, 'gamma': 5}, {'min_child_weight': 1, 'max_depth': 5, 'gamma': 2}, {'min_child_weight': 10, 'max_depth': 4, 'gamma': 1}, {'min_child_weight': 10, 'max_depth': 4, 'gamma': 5}], 'split0_test_score': array([-3.67274444e+09, -8.00303198e+09, -2.72304281e+09, -5.29052975e+09,\n       -8.00303198e+09]), 'split1_test_score': array([-3.15309269e+09, -7.00798286e+09, -2.64025824e+09, -5.15131909e+09,\n       -7.00798286e+09]), 'split2_test_score': array([-3.86913601e+09, -8.94612569e+09, -2.70174194e+09, -1.11236807e+10,\n       -8.94612569e+09]), 'split3_test_score': array([-2.70297932e+09, -7.62584755e+09, -2.06132268e+09, -5.65088676e+09,\n       -7.62584755e+09]), 'split4_test_score': array([-3.96344541e+09, -8.31969201e+09, -3.02904678e+09, -5.76675662e+09,\n       -8.31969201e+09]), 'mean_test_score': array([-3.47227957e+09, -7.98053602e+09, -2.63108249e+09, -6.59663458e+09,\n       -7.98053602e+09]), 'std_test_score': array([4.75947103e+08, 6.51085222e+08, 3.15130281e+08, 2.27473643e+09,\n       6.51085222e+08]), 'rank_test_score': array([2, 4, 1, 3, 4], dtype=int32)}\n\n Best estimator:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=2, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=500, n_jobs=1, nthread=1, num_parallel_tree=1,\n              objective='multi:softprob', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, silent=True, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n Best normalized gini score for 5-fold search with 5 parameter combinations:\n-2631082493.2326083\n\n Best hyperparameters:\n{'min_child_weight': 1, 'max_depth': 5, 'gamma': 2}","e2ae0745":"Stacked Model","88e150ed":"There are in general two ways that you can control overfitting in XGBoost:\n\nThe first way is to directly control model complexity.\n\nThis includes max_depth, min_child_weight and gamma.\n\nThe second way is to add randomness to make training robust to noise.\n\nThis includes subsample and colsample_bytree.\n\nYou can also reduce stepsize eta. Remember to increase num_round when you do so.","1481cb6d":"set tree_method to hist\/gpu_hist for faster computation;\n","d3779e89":"randomsearchCV","31761ca0":"To do:\nauto hyperparameter tuning - and plot, if possible ","7df78501":"create extra features"}}