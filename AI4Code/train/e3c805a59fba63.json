{"cell_type":{"fea8ca14":"code","52036dcc":"code","3a038f2a":"code","24791bb9":"code","982ccfd3":"code","2047cea9":"code","3962455d":"code","6585dfa5":"code","06d49d11":"code","1cca4eb0":"code","5ee7abd2":"code","927f07ed":"code","509af86e":"code","371aec17":"code","0eabc4c7":"code","c20561e9":"code","cfb66e58":"code","c39e100a":"code","4937d13d":"code","98793f21":"code","486bed3e":"code","a4929c2e":"markdown","7b72e1fe":"markdown","465e50db":"markdown","e6434055":"markdown","6cf7391b":"markdown","968be0d5":"markdown","3354b162":"markdown","696f91c0":"markdown","02cb3c1a":"markdown","e92475bc":"markdown","a22eb1d3":"markdown","c8796058":"markdown","e57f90a1":"markdown","f4f4cb1b":"markdown","0ee5945c":"markdown","4598c434":"markdown","994120fe":"markdown","df85d640":"markdown"},"source":{"fea8ca14":"import numpy as np\nimport os\nfrom os import listdir\nimport pandas as pd\n\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Activation, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow_hub as hub\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import confusion_matrix\n\nimport itertools\nimport seaborn as sns\nfrom tensorflow import keras\n\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nprint('Setup completed!')","52036dcc":"!pip install -q -U tf-hub-nightly\n!pip install -q tfds-nightly\nprint('Tensorflow Hub requirements successfully installed!')","3a038f2a":"train_path = '..\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TRAIN\/'\ntest_path = '..\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TEST\/'\n\nprint('Paths ready!')","24791bb9":"num_classes = len(listdir(train_path))\nnum_classes","982ccfd3":"# Pre-defined functions\ndef key_extractor(dictionary, value):\n    '''\n    Input:\n    - Dictionary of any key,value pair\n    - value to extract\n    \n    Return:\n    - key of that value\n    \n    Example: dict = {'a':4, 'b':6, 'y':9,'z':3}\n    key_extractor(dict, 3) => 'z'\n    \n    Caveat: Works only if all values are unique!\n    '''\n    for k,v in dictionary.items():\n        if value == v:\n            return k","2047cea9":"class_dirs = [(train_path + '\/' + category) for category in listdir(train_path)]\nclass_dirs","3962455d":"num_imgs_per_class = [len(listdir(class_dir)) for class_dir in class_dirs]","6585dfa5":"plt.figure(figsize=(5,5))\nplt.title('Number of Images per Class')\nsns.barplot(x=listdir(train_path), y=num_imgs_per_class)\nplt.ylabel('Number of images per class')","06d49d11":"from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Tensorflow's Keras has an API that already handles converting RAW images into their array form\ndata_generator = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                   validation_split=0.2)\n\nprint('Ready to generate image data!')","1cca4eb0":"image_size = 224\nbatch_size = 32\n\ntrain_generator = data_generator.flow_from_directory(train_path,\n                                                    target_size=(image_size, image_size),\n                                                    class_mode='categorical',\n                                                    batch_size=batch_size,\n                                                    subset='training')\n\nvalid_generator = data_generator.flow_from_directory(train_path,\n                                                    target_size=(image_size, image_size),\n                                                    class_mode='categorical',\n                                                    batch_size=batch_size,\n                                                    subset='validation')\n\n# I turned on the shuffle=False for convenience later when I need to extract the associated filename for the\n# predicted classes\ntest_generator = data_generator.flow_from_directory(test_path,\n                                                    target_size=(image_size, image_size),\n                                                    class_mode='categorical',\n                                                    shuffle=False,\n                                                    batch_size=batch_size)","5ee7abd2":"for image_batch, label_batch in train_generator:\n    print(\"Image batch shape: \", image_batch.shape)\n    print(\"Label batch shape: \", label_batch.shape)\n    break","927f07ed":"train_generator.class_indices","509af86e":"plt.figure(figsize=(20,8))\nplt.subplots_adjust(hspace=0.5)\nshow_num_images = train_generator.batch_size\nrow = 3\ncol = np.ceil(show_num_images\/row)\n\nfor i in range(show_num_images):\n    plt.subplot(row,col,i+1)\n    plt.imshow(image_batch[i])\n#     plt.title(label_batch[i])\n    plt.title(key_extractor(train_generator.class_indices, np.argmax(label_batch[i])))\n    plt.axis('off')\n_ = plt.suptitle(\"One Batch of Training Images (Labeled Accordingly)\")","371aec17":"classifier_url =\"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/classification\/2\"\n\nIMAGE_SHAPE = (224, 224)\n\nclassifier = tf.keras.Sequential([\n    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))\n])\n\nprint(\"TF Hub's classifier successfully loaded!\")","0eabc4c7":"feature_extractor_url = \"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/feature_vector\/2\"\n\n# Create the feature extractor\nfeature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n                                         input_shape=(224,224,3))\n\n# Apply the feature_extractor on the first batch of images generated (for trial purposes only)\nfeature_batch = feature_extractor_layer(image_batch)\nprint(feature_batch.shape)","c20561e9":"feature_extractor_layer.trainable = False\nprint('Feature extraction layer frozen!')","cfb66e58":"model = tf.keras.Sequential([\n  feature_extractor_layer,\n  layers.Dense(train_generator.num_classes)\n])\n\nmodel.summary()","c39e100a":"# Use compile to configure the training process:\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n                metrics=['acc'])\n\nprint('Model compiled! \\nReady for training!')","4937d13d":"!pip install h5py\nprint('Ready to save models in the h5 format.')","98793f21":"# from tf.keras.callbacks import ModelCheckPoint\n# filepath = \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('best_model.hdf5',\n                                                monitor='val_loss',\n                                                verbose=1,\n                                                save_best_only=True,\n                                                mode='min',\n                                                period=1)\n    \nprint('Callback successfully created!')","486bed3e":"steps_per_epoch = np.ceil(train_generator.samples\/train_generator.batch_size)\nvalid_steps_per_epoch = np.ceil(valid_generator.samples\/valid_generator.batch_size)\n\nhistory = model.fit(train_generator,\n                      steps_per_epoch=steps_per_epoch,\n                      validation_data=valid_generator,\n                      validation_steps=valid_steps_per_epoch,\n                      epochs=3,\n                      callbacks = [checkpoint],\n                      verbose=2)\n\nprint('Model trained successfully!')","a4929c2e":"## **2.2. Compile for Training**\n\nSince the last Dense layer was just added with no weight, we retrain the model, where the weights for the first feature extractor layers will no longer be trained and only the last layer's weights will be trained. Hence, the very few number of trainable parameters indicated in the summary.","7b72e1fe":"Now, if you refer back to the output where ```\/kaggle\/working``` directory is defined, you can see a new file **best_model.hdf5** which contains your saved model. Now, we can check if this can be reloaded and continued for training.","465e50db":"For the sake of easier visualization, note that ```ImageDataGenerator``` implements **OneHotEncoding** which means that the labels are in numbers. We formulate a way to convert this back to the string labels, for visualization purposes only!","e6434055":"Since each class has enough size, then proceed as usual in training.","6cf7391b":"## **1.3. Visualize 1 Batch of Training Images**","968be0d5":"# **Goal**\n\nThe goal of this notebook is to illustrate\n* how to create a callback that creates checkpoint on a certain model being trained\n* resume the training process from a checkpoint\n\n## **Context**\nThe following demonstration is using the **Blood Cell Images** which are to be **categorized by blood cell types** and there are 4 cell types to classify it into. Since the above objectives are the main goal of this notebook, I won't be going into much detail as to the choice of the model or the pre-processing methods employed.","3354b162":"# **0. Import Libraries and Dependencies**","696f91c0":"## **2.3. Create Callbacks**\n\nThis is where the juice of this notebook will be. We will use Keras' ```ModelCheckpoint``` to create checkpoints as the training progresses. This will allow us to resume training later if the training process is interrupted.\n\n**WARNING!**\n\nIn Kaggle, **training neural networks** with very long training time exceeding the kernel limit which is I think 9 hours of straight activity will lead to the training being unfinished! So, if you're prototyping your model on Kaggle's resources and it trains more than this, the model's training parameters such as learned weights and current state won't be saved. When you come back to the kernel, you have to restart all over again!\n\n**SOLUTION:**\n\nThe only solutions I found possible are\n* implement a ModelCheckpoint and callback during training. **Don't run yet the kernell cell where training happens**. Instead\n\n> 1) Implement the code on that cell.\n\n> 2) Click on **Save Version** button on the top-right corner.\n\n> 3) Check **Save & Run All (Commit)** option so that the kernel\/notebook's output will be saved!\n\n> 4) Run the entire notebook (with an accelerator if needed).\n\n> 5) Since model training is the last part of your notebook, IF in case the model training gets interrupted, whatever version is committed in the kernel will include the checkpoints created so far!\n\n**Saving the output is important. With the ModelCheckpoint, the kernel's output would INCLUDE the checkpoints created during training. Since these are saved, you can then create another kernel\/notebook and use the PREVIOUS kernel's output as an input, reload the checkpoints in the new notebook, and resume training!**\n\n**Alternative**\n\nThe above methodology will FORCE you to create ANOTHER notebook with a different name which can make \"version control\" very messy and seemingly manual. As I was reviewing this notebook, I was able to find a way to **save the output of a kernel and re-use it as input data on THE SAME kernel.** Take note that we \"save\" the first output the same as described above. Now, **to reload the previous output on the same kernel:**\n> 1) Go to the kernel\/notebook you've saved.\n> 2) Check the an output was indeed saved from the previous commit. Outputs such as checkpoints or an 'h5' version of the best-model depending on your implemented methodology.\n> 3) Click on EDIT on the top-right corner of that notebook.\n> 4) On the kernel\/notebook, click on ADD DATA.\n> 5) Upload\n> 6) Click on the CODE button <\/> on the left panel of the upload box.\n> 7) Choose YOUR WORK on the dropdown to be re-directed to the list of notebooks\/kernels you've saved.\n> 8) Choose the kernel\/notebook (which can be literally the same one you are currently working on right now, a.k.a. previous version of your current notebook). If there were outputs saved, you should be able to see them.\n> 9) Provide a description.\n> 10) Upload!","02cb3c1a":"# **1. Image Preprocessing and Generation**","e92475bc":"### **2.1.3. Freeze the Feature Extractor Layer**\n\nSince we are treating the headless MobileNetV2 as feature extractor, we set its layers to not trainable.","a22eb1d3":"# **3. Train the Model**\n\nIn this part, I demonstrated how to implement a model training with the checkpoint\/callback initiated prior.\n\n## **3.1. Checkpoint Model Improvements**\n\nHere, we illustrate how to **track** the model as it improves over training. Essentially, it will save versions\/checkpoints of the model everytime it detects an improvement in whichever metric you choose. Here we choose to save the best model based on improvements on validation accuracy.","c8796058":"Check first the size of data contained within each class. If the data is too small, then augmentation might be needed. If data is enough, then proceed as usual.","e57f90a1":"### 2.1.1 **Load the Classifier**\n \nThis is similar as reloading a pre-trained model such as ResNet50, MobileNetV2, etc.","f4f4cb1b":"### **2.1.4. Attach a Classification Head**\n\nAs reiterated previously, we remove the original head and attach a new layer for our custom classification of the 4 labels.","0ee5945c":"## **1.1. Initial Inspection**","4598c434":"## **1.2. Generate Image Data**","994120fe":"### **2.1.2. Download the Headless Model**\n\nTo improve the model with respect to the current categorization, we reload the MobileNetV2 without the top prediction layer and retrain that last layer to get the new weights for the prediction layer of 4 classes. In other words, we use MobileNetV2 just as a FEATURE EXTRACTOR and not as the final classifier!","df85d640":"# **2. Build Model**\n\n## **2.1. Model Architecture**\n\nSeveral implementations were already attempted prior to this one. However, the accuracy was incredibly low. Hence, a new model built from Tensorflow Hub's classifiers was opted instead. (For the sake of the main objective of this notebook, I won't focus on possible improvements of this technique in particular)."}}