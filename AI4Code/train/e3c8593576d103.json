{"cell_type":{"3c7ccb2b":"code","98866875":"code","1bc56ae9":"code","a14323bc":"code","cc59b15b":"code","4c1bf58f":"code","1de1cff5":"code","6bd1b21e":"code","df2cfd03":"code","53c885b1":"code","f465a6f4":"code","6f7d7ee8":"code","126c80f1":"code","f97f3164":"code","7a745c31":"code","c5fbe155":"code","9db78b55":"code","965477dd":"code","2f0e8ed7":"code","261a4ff4":"code","d9244f11":"code","5014d4f2":"code","b197e318":"code","b85ed01f":"code","bcd458aa":"code","41f61df8":"code","c048b9d4":"code","f5a51739":"code","f38bd212":"code","23685ecc":"code","c761068e":"code","e6b552b0":"code","955799d4":"code","897e0b05":"code","4fd0bf0b":"code","feb2d71c":"code","5ca8e27c":"code","f5ce003a":"code","556baae2":"code","ef7312bf":"code","58960e0a":"code","58644ade":"code","421aaa45":"code","ea029382":"code","4c778dd1":"code","1a9df192":"code","28602470":"markdown","0ffe8b32":"markdown","841d5ef2":"markdown","03164bef":"markdown","f05297fd":"markdown","33561f4c":"markdown","99dbf069":"markdown","140da184":"markdown","28d23a25":"markdown","154616b3":"markdown","d6f7d2b3":"markdown","2b94b39e":"markdown","c450f7e0":"markdown","79bd9d55":"markdown","f43ca5c9":"markdown","46fcdf66":"markdown","eb0fab90":"markdown"},"source":{"3c7ccb2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats","98866875":"df=pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\nprint(df.shape)\ndf.head()","1bc56ae9":"df=df.drop(columns=[\"Unnamed: 32\"])\nprint(df.columns)","a14323bc":"df.describe()","cc59b15b":"sns.countplot(data=df,x=\"diagnosis\")","4c1bf58f":"X=df.drop(columns=[\"id\"]).copy()\ndef diag_encoder(letter):\n    if letter==\"M\":\n        return 1\n    else:\n        return 0\nX[\"diagnosis\"]=X[\"diagnosis\"].apply(diag_encoder)\nX=X.dropna()\n# calculate the correlation matrix\ncorr = X.corr()\n\n# plot the heatmap\nfig=plt.figure(figsize=[12,9])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\n\nplt.title(\"Correlation lin\u00e9aire entre les variables\",size=18)\nax=sns.heatmap(corr, vmin=-1, vmax=1,cmap=\"bwr\",\n        xticklabels=X.columns,\n        yticklabels=X.columns)","1de1cff5":"import numpy as np\n\n#Pour r\u00e9cuperer les coefficients\ncoef_pearson=[]\ncoef_spearman=[]\n#Pour r\u00e9cup\u00e9rer les labels simplifi\u00e9s\nlabelp=[]\nlabels=[]\n#Pour r\u00e9cuperer les couleurs des bars\ncolorp=[]\ncolors=[]\nbleu=\"#9fb4ff\"\nrouge=\"#ffae9f\"\n\n#Variables \u00e9tudi\u00e9es\nvariables=[ 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\n#Seuil d'affichage de la variable\nseuil=0.35\n#On r\u00e9cup\u00e9re les information\nfor var in variables:\n    cp=stats.pearsonr(X['diagnosis'].values,X[var])[0]\n    cs=stats.spearmanr(X['diagnosis'].values,X[var])[0]\n   #Pour les coef de Pearson\n    if abs(cp)>= seuil:\n        coef_pearson.append(cp)\n        labelp.append(var)\n        if cp>0:\n            colorp.append(bleu)\n        else:\n            colorp.append(rouge)\n    #Pour les coef de Spearman                       \n    if abs(cs)>= seuil:\n        coef_spearman.append(cs)\n        labels.append(var)\n        if cs>0:\n            colors.append(bleu)\n        else:\n            colors.append(rouge)","6bd1b21e":"fig=plt.figure(1,figsize=[18,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"Variables correlated with the diagnosis\",size=16)\n\n# premier barplot des coefficients de corr\u00e9lation de Pearson\nplt.subplot(1,2,1)\nplt.title(\"Pearson correlation \",size=15)\nplt.bar(np.arange(len(labelp))+1,coef_pearson,color=colorp,edgecolor='black')\nplt.xticks(np.arange(len(labelp))+1,labelp,rotation=90,size=14)\nplt.hlines(0,0.5,len(labelp)+0.5,color='black')\nplt.ylabel(\"coefficients value\")\nplt.ylim(-0.1,1)\nplt.grid()\n\n# 2eme barplot des coefficients de corr\u00e9lation de Spearman\nplt.subplot(1,2,2)\nplt.title(\"Spearman correlation.\",size=15)\nplt.bar(np.arange(len(labels))+1,coef_spearman,color=colors,edgecolor='black')\nplt.xticks(np.arange(len(labels))+1,labels,rotation=90, size=14)\nplt.hlines(0,0.5,len(labels)+1,color='black')\nplt.ylim(-0.1,1)\nplt.grid()","df2cfd03":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nss=StandardScaler()\n    \nX=df.drop(columns=[\"diagnosis\"])\ny=df[\"diagnosis\"]\nRF=RandomForestClassifier(random_state=40,class_weight=\"balanced\",criterion='entropy'\n                                        ,n_estimators=300,max_depth=8)\nRF.fit(X,y)\n\nXs=ss.fit_transform(X)\nlr=LogisticRegression(class_weight=\"balanced\")\nlr.fit(Xs,y)","53c885b1":"selected_var={}\ntresh_rf=0.04\ntresh_rl=0.5\nfor name,reglog,randfor in zip(list(X.columns), list(lr.coef_[0]),list(RF.feature_importances_)):\n    if randfor>tresh_rf or abs(reglog)>tresh_rl:\n        dico={'RF':round(randfor,3),\"LR\":round(reglog,2)}\n        selected_var[name]=dico","f465a6f4":"feature1=list(selected_var.keys())","6f7d7ee8":"print(\"the selected variables with their coefficients:\")\nselected_var","126c80f1":"coef_rf=[]\ncoef_lr=[]\nfor n in selected_var:\n    d=selected_var[n]\n    coef_rf.append(d['RF'])\n    coef_lr.append(d['LR'])\ndf_coef=pd.DataFrame({\"Name\":list(selected_var.keys()),\"RF feature importance\":coef_rf,\"LR coef\":coef_lr})\ndf_coef.head()","f97f3164":"n_displayed=8\nd=df_coef.sort_values(by=[\"RF feature importance\"],ascending=False)\nd=d.iloc[:n_displayed,:]\nfig=plt.figure(1,figsize=[18,8])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.subplot(1,2,1)\nplt.title(\"Feature importances of the Random forest .\",size=16)\nplt.bar(range(0,n_displayed), d[\"RF feature importance\"].values,color=\"#28a2b4\",edgecolor='black')\nplt.xticks(range(0,n_displayed),d[\"Name\"],rotation=90,size=13)\nplt.grid()\n\nd=df_coef.copy()\nd[\"LR coef abs\"]=abs(d[\"LR coef\"])\nd=d.sort_values(by=[\"LR coef abs\"],ascending=False)\nd=d.iloc[:n_displayed,:]\nplt.subplot(1,2,2)\nplt.title(\"Coefficients of the logistic regression .\",size=16)\nplt.bar(range(0,n_displayed), d[\"LR coef\"],color=\"#28a2b4\",edgecolor='black')\nplt.xticks(range(0,n_displayed),d[\"Name\"],rotation=90,size=13)\nplt.grid()","7a745c31":"\ndata=df[['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean',\n       'concavity_mean', 'concave points_mean', 'radius_se', 'perimeter_se',\n       'area_se', 'compactness_se', 'fractal_dimension_se', 'radius_worst',\n       'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'concavity_worst', 'concave points_worst', 'symmetry_worst',\n       'fractal_dimension_worst', 'diagnosis']].copy()\ndata.head(2)","c5fbe155":"fig=plt.figure(1,figsize=[16,7])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"Radius\",size=16)\nplt.subplot(1,2,1)\nsns.scatterplot(data=data,x=\"radius_mean\",y=\"radius_worst\",hue=\"diagnosis\",palette=[\"#f65c5c\",\"#92c58a\"])\nplt.subplot(1,2,2)\nplt.title(\"Radius Standard d\u00e9viation\")\nsns.boxplot(data=data,y=\"radius_se\",x=\"diagnosis\",color=\"#afbbd0\",width=0.4,showfliers=False)\n\nplt.xticks([0,1],[\"Malignant\",\"benign\"])","9db78b55":"fig=plt.figure(1,figsize=[16,7])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"Concavity\",size=16)\nplt.subplot(1,2,1)\nsns.scatterplot(data=data,x=\"concavity_mean\",y='concave points_worst',hue=\"diagnosis\",palette=[\"#f65c5c\",\"#92c58a\"])\nplt.subplot(1,2,2)\nplt.title(\"concavity_worst\")\nsns.boxplot(data=data,y='concavity_worst',x=\"diagnosis\",color=\"#afbbd0\",width=0.4,showfliers=False)\nplt.xticks([0,1],[\"Malignant\",\"benign\"])","965477dd":"\nfig=plt.figure(1,figsize=[16,7])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"Perimetre, area and compactness\",size=16)\nplt.subplot(1,2,1)\nsns.scatterplot(data=data,x='perimeter_mean',y='area_mean',hue=\"diagnosis\",palette=[\"#f65c5c\",\"#92c58a\"])\nplt.subplot(1,2,2)\nplt.title(\"compactness_mean\")\nsns.boxplot(data=data,y='compactness_mean',x=\"diagnosis\",color=\"#afbbd0\",width=0.4,showfliers=False)\nplt.xticks([0,1],[\"Malignant\",\"benign\"])","2f0e8ed7":"feature1","261a4ff4":"from sklearn import decomposition\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ndef display_scree_plot(pca):\n#Fonction permettant d'afficher l'\u00e9blouis des valeurs propres\n    scree = pca.explained_variance_ratio_*100\n    fig=plt.figure(figsize=[10,7])\n    fig.patch.set_facecolor('#E0E0E0')\n    fig.patch.set_alpha(0.7)\n    plt.bar(np.arange(len(scree))+1, scree,color=\"#6fd67b\",edgecolor='black')\n    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n    plt.xlabel(\"Axe rank\")\n    plt.ylabel(\"Percent of inertia\")\n    plt.title(\"Screen of explained variance\")\n    plt.grid()\n    plt.show(block=False)","d9244f11":"\n    \ndata=df[['perimeter_mean',\n 'area_mean',\n 'compactness_mean',\n 'concavity_mean',\n 'concave points_mean',\n 'radius_se',\n 'perimeter_se',\n 'area_se',\n 'compactness_se',\n 'fractal_dimension_se',\n 'radius_worst',\n 'texture_worst',\n 'perimeter_worst',\n 'area_worst',\n 'smoothness_worst',\n 'concavity_worst',\n 'concave points_worst',\n 'symmetry_worst',\n 'fractal_dimension_worst',\n 'diagnosis']].copy()    \nX=data.drop(columns=[\"diagnosis\"])\ny=data[\"diagnosis\"]\n    \nscaler=StandardScaler()\nXs=scaler.fit_transform(X)\n\npca = PCA(n_components=X.shape[1])\npca.fit(Xs)\ndisplay_scree_plot(pca)","5014d4f2":"import math\nlab=X.columns\n(fig, ax) = plt.subplots(figsize=(10, 10))\n\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nfor i in range(0, len(pca.components_)):\n    n=len(lab[i])\n    x=pca.components_[0, i]\n    y=pca.components_[1, i]\n    n= x**2+y**2\n    n=math.sqrt(n)\n    if n>0.2:\n        if x>y:\n            ax.arrow(0,0, \n                pca.components_[0, i], #0 for PC1\n                pca.components_[1, i], #1 for PC2\n                head_width=0.02,\n                head_length=0.03)\n                #On va placer les labels de maniere \u00e0 les voir le plus clairement possible\n           # plt.text(pca.components_[0, i]-n\/2 ,\n               # pca.components_[1, i]+0.06,\n                #s=lab[i],c=\"r\",size=11)\n\n            if y<0:\n                    plt.text(pca.components_[0, i]+0.06 ,\n                    pca.components_[1, i]-0.02,\n                    s=lab[i],c=\"r\",size=11)\n            else:\n                    plt.text(pca.components_[0, i]+0.06 ,\n                    pca.components_[1, i],\n                    s=lab[i],c=\"r\",size=11)\n\n        else:\n                ax.arrow(0,0, \n                pca.components_[0, i], #0 for PC1\n                pca.components_[1, i], #1 for PC2\n                head_width=0.02,\n                head_length=0.03)\n\n                plt.text(pca.components_[0, i] ,\n                    pca.components_[1, i]+0.15,\n                    s=lab[i],c=\"g\",size=11)\n\n                \n\nan = np.linspace(0, 2 * np.pi, 100)\nplt.plot(np.cos(an), np.sin(an)) # Add a unit circle for scale\nplt.axis('equal')\nax.set_title('correlation circle')\nplt.grid()\nplt.show()","b197e318":"PC1=[]\nPC2=[]\nfor i in range(0, len(pca.components_)):\n    n=len(lab[i])\n    x=pca.components_[0, i]\n    y=pca.components_[1, i]\n    n= x**2+y**2\n    n=math.sqrt(n)\n    if n>0.2:\n        if x>y:\n            PC1.append(lab[i])\n        else:\n            PC2.append(lab[i])\n            \nprint(\"PCA 1:\")\nprint(PC1)\nprint(\"PCA 2:\")\nprint(PC2)","b85ed01f":"pca = PCA(n_components=2)\n# Fit and transform x to visualise inside a 2D feature space\nd = pca.fit_transform(Xs)\nd=pd.DataFrame(d,columns=[\"PC 1\",\"PC 2\"])\nd[\"diagnosis\"]=data[\"diagnosis\"].values","bcd458aa":"(fig, ax) = plt.subplots(figsize=(10, 6))\n\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nsns.scatterplot(data=d,x=\"PC 1\",y=\"PC 2\",hue=\"diagnosis\")","41f61df8":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\ndef conv_y(y):\n    #to convert the target in 0 and 1\n    y_converted=[]\n    for i in y:\n        if i ==\"M\":\n            y_=1\n        else:\n            y_=0\n        y_converted.append(y_)\n    return pd.Series(y_converted)\n\ndef lets_try(train, y):\n    results = {}\n    ss=StandardScaler()\n    scaled_train=ss.fit_transform(train)\n    \n   \n    def test_model(clf):\n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf, train, conv_y(y), scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n\n    #for the model which needed standardized data \n    def test_model_scaler(clf):\n    \n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf, scaled_train,conv_y(y), scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n    \n    clf = SVC(kernel=\"linear\")\n    results[\"SVC\"] = test_model_scaler(clf)\n    print(\"SVC done\")\n    \n    clf = LogisticRegression()\n    results[\"Logistic Regression\"] = test_model_scaler(clf)\n    print(\"Logistic Regression done\")\n\n    clf = KNeighborsClassifier()\n    results[\"Kneighbors\"] = test_model(clf)\n    print(\"Kneighbors done\")\n\n    clf = SVC(kernel=\"poly\")\n    results[\"SVC poly\"] = test_model_scaler(clf)\n    print(\"SVC poly done.\")\n\n    clf = RandomForestClassifier()\n    results[\"Random Forest Classifier\"] = test_model(clf)\n    print(\"Random Forest Classifier done\")\n\n\n    clf =SVC(kernel='rbf')\n    results[\"SVC RBF\"] = test_model_scaler(clf)\n    print(\"SVC rbf done\")\n\n    clf=GradientBoostingClassifier()\n    results[\"GradientBoosting\"]=test_model(clf)\n    print(\"Grandient boosting done\")\n    \n    clf=AdaBoostClassifier()\n    results[\"AdaBoostClassifier\"]=test_model(clf)\n    print(\"AdaBoostClassifier done\")\n    \n    return results \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX=data.drop(columns=[\"diagnosis\"])\ny=data[\"diagnosis\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","c048b9d4":"results=lets_try(X_train, y_train)","f5a51739":"fig=plt.figure(1,figsize=[10,7])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"CV resultas for each model on the train set \",size=16)\nplt.boxplot(results.values(),labels=results.keys(),showmeans=True)\nplt.ylabel(\"  Scores CV \\n (f1)\",size=14)\nplt.ylim(0.5,1)\nplt.xticks(rotation=90)\n#plt.grid()","f38bd212":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nmodel0=RandomForestClassifier()\nmodel0.fit(X_train, y_train)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model0,X_test, y_test\n                           , cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the Random forest on the testing set \",size=14)\nplt.show()\n","23685ecc":"ss=StandardScaler()\nscaled_train=ss.fit_transform(X_train)\nscaled_test=ss.fit_transform(X_test)","c761068e":"model1=LogisticRegression()\nmodel1.fit(scaled_train, y_train)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model1,scaled_test, y_test\n                           , cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the Logistic regression on the testing set \",size=14)\nplt.grid()\nplt.show()\n","e6b552b0":"model2=SVC(kernel=\"linear\")\nmodel2.fit(scaled_train, y_train)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model2,scaled_test, y_test\n                           , cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the SVC on the testing set \",size=14)\nplt.show()","955799d4":"import shap\n\nshap.initjs()","897e0b05":"explainer = shap.LinearExplainer(model1, scaled_train, model_output = 'probability')\nshap_values = explainer.shap_values(scaled_test)","4fd0bf0b":"plt.style.use('fivethirtyeight')\nshap.summary_plot(shap_values, scaled_test, feature_names=X_test.columns, show=False)\nfig = plt.gcf()\nax = plt.gca()\nax.set_xlabel(\"Benign<===   Target class     ===> malignant \\n SHAP value (impact of the model output)\")\nax.set_title(\"Impact of each feature in the Logistic regression predictions\")","feb2d71c":"scaled_test=pd.DataFrame(scaled_test,columns=X_test.columns)\ndef shap_plot(i):\n    individual=scaled_test.iloc[[i],:]\n    print(\"___________________________________\")\n    print(\"The case n\u00b0 {}\".format(i))\n    print(\"___________________________________\")\n    print(\"Has a  malignancy score of {} %\".format( model1.predict_proba(individual)[0][1].round(2)*100))\n    \n    print('True class :', y_test.iloc[i])\n    return(shap.force_plot( explainer.expected_value, shap_values[i,:], X_test.iloc[i,:],\n    feature_names=X_test.columns))","5ca8e27c":"col=['area_mean', 'texture_worst', 'perimeter_mean','concave points_se',\n       'area_mean','radius_se', 'perimeter_se','smoothness_worst']\n","f5ce003a":"fig=plt.figure(1,figsize=[18,8])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"Distribution\",size=16)\nprint(\"---------\")\nprint(\"MEAN:\")\nprint(\"---------\")\nprint(\"concave_points_mean mean:\",round(df[\"concave points_mean\"].mean(),2))\nfor i in range(0,8):\n    print(col_,\"mean:\",round(df[col_].mean(),2))\n    plt.subplot(2,4,i+1)\n    col_=col[i]\n    plt.title(col_,size=16)\n    plt.hist(df[col_],bins=20, alpha=0.5, label=col_,edgecolor=\"black\")\n   \n    \n    #plt.show()","556baae2":"\nshap_plot(3)\n","ef7312bf":"shap_plot(15)","58960e0a":"for i , n in enumerate(y_test.values):\n    print(i ,n)","58644ade":"from imblearn.over_sampling import ADASYN\nada=ADASYN()\nX_train_res, y_train_res=ada.fit_resample(X_train, y_train)\nadasyn_results=lets_try(X_train_res, y_train_res)","421aaa45":"fig=plt.figure(1,figsize=[15,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"CV results on the train set . 5 folds\",size=16)\n\nplt.subplot(1,2,1)\nplt.title(\"Adasyn oversampling\",size=16)\nplt.boxplot(adasyn_results.values(),labels=adasyn_results.keys(),showmeans=True)\nplt.ylabel(\"  Scores CV \\n (f1)\",size=14)\nplt.ylim(0.7,1)\nplt.xticks(rotation=90)\n#plt.grid()\n\nplt.subplot(1,2,2)\nplt.title(\"With no resampling\",size=16)\nplt.boxplot(results.values(),labels=results.keys(),showmeans=True)\nplt.ylim(0.7,1)\nplt.xticks(rotation=90)\n#plt.grid()","ea029382":" ss=StandardScaler()\nscaled_train_r=ss.fit_transform(X_train_res)\n\nclf =SVC(kernel='rbf',probability=True)\nclf .fit(scaled_train_r,y_train_res)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(clf ,scaled_test, y_test\n                           , cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the SVC RBF (with resampled train set) on the testing set \",size=14)\nplt.grid()\nplt.show()\n","4c778dd1":"explainer = shap.KernelExplainer(clf.predict_proba, scaled_train_r)\nshap_values = explainer.shap_values(scaled_test)","1a9df192":"\nshap.summary_plot(shap_values[1],X_test, show=False)\nfig = plt.gcf()\nax = plt.gca()\nax.set_xlabel(\"Benign<===   Target class     ===> malignant \\n SHAP value (impact of the model output)\")\nax.set_title(\"Impact of each feature in the SVC(rbf) predictions \\n with Adasyn resampling on the trainning set\")","28602470":"This case have low values on each feature, except for the smoothness_worst (0.145) witch is \na little high (mean =0.13) ","0ffe8b32":"This summary_plot is consistent with the previous one.","841d5ef2":"Let's chech the SVC RGF(fit on the resampled set) results on the testing set.","03164bef":"On this projection on the first plan of PCA we can see that the two classes of the target variable are easy to distinguish","f05297fd":"# PCA","33561f4c":"**the case n\u00b0 15 have high values for:**\n\nconcave points_mean (mean=0.05) in this case 0.146\n\ntexture_worst (mean=25.68) in this case,32.72\n\nperimeter_se (mean =2.87) in this case 5.8\n\nperimeter_mean (mean=91.97) in this case 129,1\n\n\n> **This explains the score of 100%**","99dbf069":"# Conclusion:\n\nTwo models obtain comparable results,\n\nThe **logistic regression** on the selected dataset and the **SVC** (with a rgf kernel) on the resampled dataset, this model minimizes the number of undetected malignant tumors . ","140da184":"thanks to the shap.force_plot we can explain the impact of features in **individual predictions**","28d23a25":"# Classification of the breast tumors\n\nThis notebook consist in 2 parts:\n\n**1 EDA:**\n> * visualisation and features selection (thanks to a random forest and logistic regression )\n\n**2 . Prediction:**\n> * 2.1 Comparison of model results and Explaination of the best model with SHAP\n> * 2.2 Resampling with ADASYN, results comparison and Explaination with SHAP","154616b3":"## On the original train dataset.","d6f7d2b3":"# Prediction:","2b94b39e":"## Improve the prediction by using the resampler ADASYN.","c450f7e0":"With this model we have a litte bit more false positif but less false negatif (there the positive class is \"Malignant\")\nin **a medical context**, it's more important to diagnosis correctly the malignants tumors so this model seems to be better.\n\n","79bd9d55":"**Features descriptions:**\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 \/ area - 1.0)\n\ng) concavity (severity of concave portions of the contour)\n\nh) concave points (number of concave portions of the contour)\n\ni) symmetry\n\nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.","f43ca5c9":"We select the feature with a random forest and a logistique regression .","46fcdf66":"As we can see on this graph, a hight value on most of the feature inficated a hight probability of a malignant tumor, exept for the compactness (mean and se). A very compact tumor has a high chance of being benign. \n\nThe features are ranked in descending order of their impact ","eb0fab90":"# 1- EDA"}}