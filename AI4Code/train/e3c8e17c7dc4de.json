{"cell_type":{"44f24b52":"code","4b874559":"code","d99f94c0":"code","9aa280d0":"code","6302784d":"code","8bfc276b":"code","aea46fdc":"code","fdf7c33d":"code","f5b4ba6f":"code","f73a4cb3":"code","ed1bd1b9":"code","296566d2":"code","1049fa9b":"code","00a7e84b":"code","0dbdf0cc":"code","925e7b95":"code","e9310043":"code","e3502ef5":"code","f87494db":"code","8dac994b":"code","d9f65739":"code","cf997a4f":"code","5f896513":"code","db13b5e1":"code","90e65ff6":"code","dd713e29":"code","55d0655e":"code","db93c016":"code","030b86f4":"code","25475f9c":"code","a764138b":"code","8a6e4698":"code","e9628781":"code","336f597c":"code","9d1df6d0":"markdown","1bc68b5d":"markdown","ce39e439":"markdown","8d4ce209":"markdown","aba84938":"markdown","35c12132":"markdown","8e01ed10":"markdown","f43f707c":"markdown","8b9f00e7":"markdown","e2a15184":"markdown","d44d0584":"markdown","9aa66d0c":"markdown","ee29a010":"markdown","1ca51956":"markdown","0a806df7":"markdown","e1ff27cd":"markdown","038bc49d":"markdown"},"source":{"44f24b52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score, roc_curve, auc, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom skopt.space import Real, Integer, Categorical\nimport itertools\nfrom skopt import gp_minimize\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport gc\nimport joblib","4b874559":"p_df_churn = pd.read_excel('\/kaggle\/input\/telco-customer-churn-1113\/Telco_customer_churn.xlsx')\np_df_cust_demo = pd.read_excel('..\/input\/telco-customer-churn-1113\/Telco_customer_churn_demographics.xlsx')\np_df_cust_loc = pd.read_excel('..\/input\/telco-customer-churn-1113\/Telco_customer_churn_location.xlsx')\np_df_cust_popu = pd.read_excel('..\/input\/telco-customer-churn-1113\/Telco_customer_churn_population.xlsx')\np_df_cust_serv = pd.read_excel('..\/input\/telco-customer-churn-1113\/Telco_customer_churn_services.xlsx')\np_df_cust_status = pd.read_excel('..\/input\/telco-customer-churn-1113\/Telco_customer_churn_status.xlsx')","d99f94c0":"p_df_churn.head()","9aa280d0":"p_df_churn.isna().sum()","6302784d":"p_df_cust_status.head()","8bfc276b":"p_df_cust_status.groupby(['Churn Label'])['Customer ID'].count()","aea46fdc":"p_df_cust_demo.isna().sum()","fdf7c33d":"p_df_cust_loc.isna().sum()","f5b4ba6f":"p_df_cust_popu.head()","f73a4cb3":"p_df_cust_serv.isna().sum()","ed1bd1b9":"df_merged = p_df_churn.merge(p_df_cust_demo, left_index=True, right_index=True,\n                 how='outer', suffixes=('', '_y'))\n\ndf_merged = df_merged.merge(p_df_cust_loc, left_index=True, right_index=True,\n                 how='outer', suffixes=('', '_y'))\n\ndf_merged = df_merged.merge(p_df_cust_serv, left_index=True, right_index=True,\n                 how='outer', suffixes=('', '_y'))\n\ndf_merged = df_merged.merge(p_df_cust_status, left_index=True, right_index=True,\n                 how='outer', suffixes=('', '_y'))\n\ndf_merged = df_merged.merge(p_df_cust_popu, left_index=True, right_index=True,\n                 how='outer', suffixes=('', '_y'))\n\n\ndf_merged.drop(df_merged.filter(regex='_y$').columns.tolist(),axis=1, inplace=True)\n","296566d2":"df_merged.info()","1049fa9b":"df_merged.head()","00a7e84b":"#df_merged.Count.unique()\n#df_merged.Country.unique()\n#df_merged.State.unique()\n#df_merged.City.unique()\ndf_merged.Contract.unique()\n#df_merged['Phone Service'].unique()\n#df_merged['Churn Category'].unique()\n#df_merged['Customer Status'].unique()\n#df_merged['Churn Score'].unique()\n#df_merged[['Internet Type','Internet Service']]#.unique()\n#df_merged[df_merged['Churn Label']=='Yes']['Customer Status'].unique()\n","0dbdf0cc":"df_merged['Churn Category'] = df_merged['Churn Category'].fillna('NA')\n\n# use instead of Internet Service as the former has more details, dropping Customer Status columns as it is \n# somehow dubia, also droping Churn Reason as it is coverd by Churn Category. CustomerID is also not needed at this point\n\ndf_data =  df_merged.drop(['Count', 'Country', 'State', 'Lat Long', 'Latitude', 'Churn Reason', 'Customer ID', 'ID', 'CustomerID', 'Population',\n                             'Longitude', 'Churn Label', 'Tenure in Months', 'Internet Service', 'Customer Status' ], axis=1)\n\n### remove spaces within column names, just for ease of accessing\ndf_data.columns = df_data.columns.str.replace(' ', '')","925e7b95":"df_y = df_data['ChurnValue']\ndf_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\ndf_y = df_y.reset_index(drop=True)\n\ndf_X_raw = df_data.drop(['ChurnValue'], axis=1)\n#df_X_raw = p_df_raw_train.drop(['Name'], axis=1)\n\ncolumns=df_X_raw.columns\nfor f in df_X_raw.columns:\n    if df_X_raw[f].dtype == 'object':\n        df_X_raw[f] = LabelEncoder().fit_transform(list(df_X_raw[f]))        ","e9310043":"scaler = PowerTransformer()\nscaled_df = scaler.fit_transform(df_X_raw)\ndf_X = pd.DataFrame(scaled_df, columns=columns)\ndf_X = df_X.reset_index(drop=True)","e3502ef5":"class_labels = np.unique(df_y)\nclass_weights = compute_class_weight('balanced', class_labels, df_y)\nclass_weights = dict(zip(class_labels, class_weights))","f87494db":"df_X_sub, X_test, df_y_sub, y_test = train_test_split(df_X, df_y, test_size=0.33, random_state=42)","8dac994b":"categorical_feature = []\ncolumns = df_data.columns\nfor f in df_data.columns:\n    if df_data[f].dtype == 'object':\n        categorical_feature.append(f)","d9f65739":"df_X.columns","cf997a4f":"space  = [ Integer(3, 30, name='max_depth')\n          ,Integer(6, 50, name='num_leaves')\n          ,Integer(50, 200, name='min_child_samples')\n          ,Real(1, 500,  name='scale_pos_weight')\n          ,Real(0.6, 0.9, name='subsample')\n          ,Real(0.6, 0.9, name='colsample_bytree')\n          ,Real(0.0001, 1,  name='learning_rate', prior='log-uniform')\n          ,Integer(2, 200, name='max_bin')\n          ,Integer(100, 2000, name='num_iterations')\n          ,Integer(20, 100, name='min_child_samples')\n          #,Real(0.0001, 1,  name='lambda_l2', prior='log-uniform')          \n          #,Real(0.0001, 1,  name='lambda_l1', prior='log-uniform')          \n         ]","5f896513":"def sigmoid(x):\n    return (1 \/ (1 + np.exp(-x)))\n\n### for unbalanced classes\ndef focal_loss_lgb_eval_error(y_pred, dtrain, alpha, gamma):\n    a,g = alpha, gamma\n    y_true = dtrain.label\n    p = 1\/(1+np.exp(-y_pred))\n    loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n    # (eval_name, eval_result, is_higher_better)\n    return 'focal_loss', np.mean(loss), False\n\nfocal_loss = lambda x,y: focal_loss_lgb(x, y, alpha=0.25, gamma=1.)\nfocal_loss_eval = lambda x,y: focal_loss_lgb_eval_error(x, y, alpha=0.25, gamma=1.)","db13b5e1":"def objective(values):\n    \n    params = {\n              'max_depth': values[0]\n              ,'num_leaves': values[1]\n              ,'min_child_samples': values[2]\n              ,'scale_pos_weight': values[3]\n              ,'subsample': values[4]\n              ,'colsample_bytree': values[5]\n              ,'learning_rate': values[6]\n              ,'max_bin': values[7]              \n              ,'num_iterations': values[8]\n              ,'min_child_samples': values[9]\n              #,'lambda_l1': values[9]\n              #,'lambda_l2': values[10]\n              ,'metric':'auc'\n              ,'nthread': 20\n              ,'boosting_type': 'gbdt'\n              ,'objective': 'binary'                            \n              ,'min_child_weight': 0\n              ,'min_split_gain': 0\n              ,'subsample_freq': 1\n              }\n    \n    train_auc_list = []\n    valid_auc_list = []\n    early_stopping_rounds = 50\n    num_boost_round       = 100            ### One can increase this number, helps a better fit\n    \n    # Fit model on feature_set and calculate validation AUROC    \n    \n    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n    \n    for train_index, test_index in cv.split(df_X_sub, df_y_sub):\n        X_train, X_valid = df_X_sub.loc[df_X_sub.index[train_index]], df_X_sub.loc[df_X_sub.index[test_index]]\n        y_train, y_valid = df_y_sub.loc[df_y_sub.index[train_index]], df_y_sub.loc[df_y_sub.index[test_index]]\n        \n        train_data = lgb.Dataset(data=X_train, label=y_train, \n                                 categorical_feature=categorical_feature,\n                                 free_raw_data=False)\n        \n        valid_data = lgb.Dataset(X_valid, label=y_valid,\n                                 categorical_feature=categorical_feature,\n                                 free_raw_data=False)\n        \n        evals_results = {}\n        model_lgb     = lgb.train(params,\n                                  train_data,\n                                  valid_sets=valid_data, \n                                  evals_result=evals_results, \n                                  num_boost_round=num_boost_round,\n                                  #early_stopping_rounds=early_stopping_rounds,\n                                  verbose_eval=False,\n                                  feval=focal_loss_eval\n                                 )        \n                \n        train_preds = sigmoid(model_lgb.predict(X_train))\n        train_binary_preds = [int(p>0.5) for p in train_preds]\n        \n        valid_preds = sigmoid(model_lgb.predict(X_valid))\n        valid_binary_preds = [int(p>0.5) for p in valid_preds]\n        \n        train_auc = f1_score(y_train, train_binary_preds)\n        valid_auc = f1_score(y_valid, valid_binary_preds)\n                \n        train_auc_list.append(train_auc)\n        valid_auc_list.append(valid_auc)\n\n        \n    train_valid_diff = np.abs(np.mean(train_auc_list) - np.mean(valid_auc_list))       \n    \n    ### this is a good criteria to control over-fitting\n    if train_valid_diff > 5:\n            train_valid_diff = 0.05\n    else:\n        train_valid_diff = np.mean(train_auc_list)\n    \n    print('TRAIN.....', np.mean(train_auc_list))\n    print('VALID.....', np.mean(valid_auc_list))\n    print('final score ..... ', train_valid_diff)\n    \n    gc.collect()\n    \n    #return  np.mean(train_auc_list)\n    return -train_valid_diff\n    ","90e65ff6":"res_gp = gp_minimize(objective, space, n_calls=50, random_state=0, n_random_starts=10)\n\"Best score=%.4f\" % res_gp.fun","dd713e29":"res_gp.x","55d0655e":"params = {'max_depth': res_gp.x[0]\n              ,'num_leaves': res_gp.x[1]\n              ,'min_child_samples': res_gp.x[2]\n              ,'scale_pos_weight': res_gp.x[3]\n              ,'subsample': res_gp.x[4]\n              ,'colsample_bytree': res_gp.x[5]\n              ,'learning_rate': res_gp.x[6]\n              ,'max_bin':  res_gp.x[7]\n              #,'num_iterations': values[8]\n              ,'min_child_samples': res_gp.x[8]\n              #,'lambda_l1': values[9]\n              #,'lambda_l2': values[10]\n              ,'metric':'auc'\n              ,'nthread': 8\n              ,'boosting_type': 'gbdt'\n              ,'objective': 'binary'                            \n              ,'min_child_weight': 0\n              ,'min_split_gain': 0\n              ,'subsample_freq': 1\n              }","db93c016":"model_lgb = lgb.LGBMClassifier(**params, class_weight = class_weights).fit(df_X_sub, df_y_sub)","030b86f4":"clf_train_pred = model_lgb.predict(df_X_sub)\nclf_test_pred = model_lgb.predict(X_test)","25475f9c":"auc_train = roc_auc_score(df_y_sub, clf_train_pred)\nprint('\\nAUROC.....',auc_train)","a764138b":"auc_test = roc_auc_score(y_test, clf_test_pred)\nprint('\\nTEST AUROC.....', auc_test)","8a6e4698":"fpr, tpr, threshold = roc_curve(y_test, clf_test_pred)\nfig = plt.figure(figsize=(12,8)) \nax = fig.add_subplot(1,1,1)\nplt.plot([0,1],[0,1], 'k--')\nplt.plot(fpr, tpr)","e9628781":"print('Accuracy')\nprint(accuracy_score(y_test, clf_test_pred))\nprint('F1_score')\nprint(f1_score(y_test, clf_test_pred))\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, clf_test_pred))\nprint('Confusion Report')\nprint(classification_report(y_test, clf_test_pred))\n","336f597c":"joblib.dump(model_lgb, 'telco_churn_bayes_Lgbm_F1_91%_.pkl')","9d1df6d0":"#### churn reason is null may be coz those customers did not churn, lets see","1bc68b5d":"# Bayesian Optimization Magic\n### Lets now do the Bayesian Magic. Here first, we need to identify parameter search space.","ce39e439":"### Power transform helps make data look more Gaussian-like. This is to make the search space more convex where local extrema is also global extrema.","8d4ce209":"## So far we see that all expect one data set (p_df_cust_popu) contain information per customer basis. This implies that these data frames can be joined together by removing the duplicate columns.","aba84938":"Always do the train\/test split, we will futher divide the train split into train and validate in further down the analysis","35c12132":"#### see the null values above are due to the fact that those customers did not churn","8e01ed10":"# You cannot do better than above ;). Please like and share","f43f707c":"### Setting the class weigths to better counter the problem of imbalanced classes","8b9f00e7":"## Lets measure model performance","e2a15184":"### To explicitly stating the categorical columns since after transformation, all data is in the float.","d44d0584":"## Data preprocessing\n### we need to normalize data convert strings into integers, calculate class weights since churn use cases always have imbalanced classes","9aa66d0c":"### Lets see what is inside each of these columns","ee29a010":"## save the model.","1ca51956":"### Once the best parameter set is identified, we need to need recreate the model a fit function. ","0a806df7":"### Final columns","e1ff27cd":"### Here we are using a customized evaluation function. We are using focal loss which is very helpful against imbalance classes","038bc49d":"### Best parameters"}}