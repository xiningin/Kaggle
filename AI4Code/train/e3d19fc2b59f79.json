{"cell_type":{"59bc36ea":"code","5ca36464":"code","4392194e":"code","02c46137":"code","5bbece6b":"code","a6ecaf86":"code","fd41aad2":"code","da95cba4":"code","cfedd061":"code","0fa43ce1":"code","c7d911ca":"code","c326b780":"code","8e67f9e4":"code","21394941":"code","4019b985":"code","61cdf435":"code","832db8cd":"code","eb3a3936":"code","86fc2cf6":"code","777bc64b":"code","4728d745":"code","7da08163":"code","2587af03":"code","e7ecc865":"code","c2563c83":"code","f711692c":"code","5b853c18":"markdown","a4639eda":"markdown","810a7fee":"markdown","6418697d":"markdown","a681c14c":"markdown","147a0a7d":"markdown","b5e2d380":"markdown","d99c3606":"markdown","ba31b7dd":"markdown"},"source":{"59bc36ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5ca36464":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","4392194e":"#shape of train and test\nprint(train.shape)\nprint(test.shape)","02c46137":"# Extract the target feature from the train data\n\ntarget = train.label\ntrain = train.drop(train[['label']], axis=1)","5bbece6b":"# Split the train data into train and test\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size= 0.2)","a6ecaf86":"# Convert into numpy arrays\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)","fd41aad2":"#Reshape data\n\nX_train = X_train.reshape(-1, 28, 28, 1)\nX_test = X_test.reshape(-1, 28, 28, 1)","da95cba4":"#Scale the data\n\nX_train = X_train.astype(float) \/ 255.0\nX_test = X_test.astype(float) \/ 255.0","cfedd061":"def cat(df):\n    df = to_categorical(df)\n    return df\n    \ny_train = cat(y_train)\ny_test = cat(y_test)","0fa43ce1":"# Carrying out same data preprocessing on test data\n\ntest = np.array(test)\ntest = test.reshape(28000, 28,28, 1)\ntest = test.astype(float)\/255.0","c7d911ca":"model =  Sequential()\nmodel.add(Conv2D(16, (3,3), padding='same', activation='relu', input_shape=(28, 28, 1)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(32, (3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(64, (3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(Conv2D(64, (3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n          \nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\n\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()","c326b780":"model.compile(optimizer='adam', \n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n              )","8e67f9e4":"history = model.fit(X_train, y_train,\n                   validation_data=(X_test, y_test),\n                    epochs = 30,\n                    batch_size= 42)","21394941":"plt.plot(history.history['loss'], color='b')\nplt.plot(history.history['val_loss'], color='r')\nplt.show()\nplt.plot(history.history['acc'], color='b')\nplt.plot(history.history['val_acc'], color='r')\nplt.show()","4019b985":"pred = model.predict(test)","61cdf435":"finalpred = np.argmax(pred,axis=1)","832db8cd":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.head()","eb3a3936":"subid = sub.ImageId\ndf = pd.DataFrame({'ImageId':subid, 'Label':finalpred})","86fc2cf6":"df.to_csv('digits.csv')","777bc64b":"# For the data augmentation we rotated the image by 30 degrees, adjusted the width and height of the images by 0.15\n# Did a horizontal flip of images and lastly zoomed the image by 50%\n\ndatagen = ImageDataGenerator(\n                    rotation_range=30, \n                    width_shift_range=.15, \n                    height_shift_range=.15, \n                    horizontal_flip=True, \n                    zoom_range=0.5\n                    )","4728d745":"epoch = 30\nbatch_size= 42\n\n\nhistory1 = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                           steps_per_epoch=500,\n                           epochs=epoch,\n                           verbose=2,\n                           validation_data=(X_test, y_test),\n                           )","7da08163":"pred = model.predict(test)","2587af03":"finalpred = np.argmax(pred,axis=1)","e7ecc865":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.head()","c2563c83":"subid = sub.ImageId\ndf = pd.DataFrame({'ImageId':subid, 'Label':finalpred})","f711692c":"df.to_csv('digits1.csv')","5b853c18":"Build Model\n* Model contains an inout layer that passes through a convolution which uses a relu activation function to normalize the output. The layer is also passed through the max pools filter of size 2x2.\n* Then we have 3 hidden layers built on same convolution and eache using a relu activation function.\n* The model is then flattened and dropout of 0.2 is used to reduce overfitting on the training data. The model then passes through a dense layerwith a relu activation function.\n* Finally the output from the hidden layers pass through a dense layer with 10 outputs which are also made to pass throught the softmax activation function.\n* We then print the summary of the model.","a4639eda":"Submission","810a7fee":"Model prediction on test data","6418697d":"Trying out data augmentation to reduce overfitting on the data","a681c14c":"Read Data","147a0a7d":"Submissions","b5e2d380":"Make predictions","d99c3606":"Data Preprocessing","ba31b7dd":"Plot of the training, validation loss, training accuracy and validation accuracy"}}