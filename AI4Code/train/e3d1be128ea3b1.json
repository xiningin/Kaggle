{"cell_type":{"d6cfeb0b":"code","d0b18109":"code","4c275b13":"code","e867df9b":"code","67baa759":"code","ec85fc1f":"code","2a6bd6aa":"code","c37a94eb":"code","f94dec20":"code","85d2bbb5":"code","9e2c342a":"code","9cf49f84":"markdown","9efcb255":"markdown","fec2f7ec":"markdown","e2d8d4fc":"markdown","c6282a14":"markdown","64b6dc6f":"markdown","db00eaf9":"markdown","ddbf2764":"markdown"},"source":{"d6cfeb0b":"!mkdir -p \/tmp\/pip\/cache\/\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/efficientnet_pytorch-0.6.3.xyz \/tmp\/pip\/cache\/efficientnet_pytorch-0.6.3.tar.gz\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/pretrainedmodels-0.7.4.xyz \/tmp\/pip\/cache\/pretrainedmodels-0.7.4.tar.gz\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/segmentation-models-pytorch-0.1.2.xyz \/tmp\/pip\/cache\/segmentation_models_pytorch-0.1.2.tar.gz\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/timm-0.1.20-py3-none-any.whl \/tmp\/pip\/cache\/\n!cp ..\/input\/segmentationmodelspytorch\/segmentation_models\/timm-0.2.1-py3-none-any.whl \/tmp\/pip\/cache\/\n!pip install --no-index --find-links \/tmp\/pip\/cache\/ efficientnet-pytorch\n!pip install --no-index --find-links \/tmp\/pip\/cache\/ segmentation-models-pytorch","d0b18109":"import os\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\n\nfrom skimage.color import label2rgb\n\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\n\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nSEED = 421","4c275b13":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(SEED)","e867df9b":"MODEL_PATH = \"..\/input\/kaggle-hubmap-segmentation-pytorch-training\/unet-se_resnext50-cosineanneal-RES-256-best-FOLD-0-model.pth\"\n\nIMG_SIZE = 256\nTRAIN_IMGS = '..\/input\/hubmap-256x256\/train\/'\nMASKS = '..\/input\/hubmap-256x256\/masks'\nLABELS = '..\/input\/hubmap-kidney-segmentation\/train.csv'\nNUM_WORKERS = 4\n\nMODEL = 'unet-se_resnext50-cosineanneal'\nENCODER = 'se_resnext50_32x4d'\nFOLD = 0\nNFOLDS = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","67baa759":"# https:\/\/www.kaggle.com\/iafoss\/256x256-images\nMEAN = np.array([0.65459856,0.48386562,0.69428385])\nSTD = np.array([0.15167958,0.23584107,0.13146145])","ec85fc1f":"def img2tensor(img,dtype:np.dtype=np.float32):\n    # convert numpy image to Pytorch tensor image\n    if img.ndim==2 : img = np.expand_dims(img,2)\n    img = np.transpose(img,(2,0,1))\n    return torch.from_numpy(img.astype(dtype, copy=False))\n\nclass HuBMAPDataset(Dataset):\n    def __init__(self, fold=FOLD, train=True, preprocess_input=None, transforms=None):\n        ids = pd.read_csv(LABELS).id.values\n        kf = KFold(n_splits=NFOLDS,random_state=SEED,shuffle=True)\n        ids = set(ids[list(kf.split(ids))[fold][0 if train else 1]])\n        self.fnames = [fname for fname in os.listdir(TRAIN_IMGS) if fname.split('_')[0] in ids]\n        self.train = train\n        self.preprocess_input = preprocess_input\n        self.transforms = transforms\n        \n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        img = cv2.cvtColor(cv2.imread(os.path.join(TRAIN_IMGS, fname)), cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(os.path.join(MASKS,fname),cv2.IMREAD_GRAYSCALE)\n        \n        if self.transforms:\n            augmented = self.transforms(image=img,mask=mask)\n            img,mask = augmented['image'],augmented['mask']\n        \n        if self.preprocess_input:\n            # Normalizing the image with the given mean and std corresponding to each channel\n            img = self.preprocess_input(image=img)['image']\n        \n        return img2tensor(img),img2tensor(mask)\n    \n    def __len__(self):\n        return len(self.fnames)","2a6bd6aa":"# https:\/\/www.kaggle.com\/iafoss\/hubmap-pytorch-fast-ai-starter#Data\ndef get_preprocess_fn(encoder_name, pretrained='imagenet'):\n    return A.Lambda(image = get_preprocessing_fn(encoder_name = encoder_name, pretrained = pretrained))\n\n\ndef get_train_transform():\n    return A.Compose([\n        A.HorizontalFlip(),\n        A.VerticalFlip(),\n        A.RandomRotate90(),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, \n                         border_mode=cv2.BORDER_REFLECT),\n        A.OneOf([\n            A.OpticalDistortion(p=0.3),\n            A.GridDistortion(p=.1),\n            A.IAAPiecewiseAffine(p=0.3),\n        ], p=0.3),\n        A.OneOf([\n            A.HueSaturationValue(10,15,10),\n            A.CLAHE(clip_limit=2),\n            A.RandomBrightnessContrast(),            \n        ], p=0.3)\n    ],p=1.)\n\n# def get_val_transform():\n#     return A.Compose([\n#         A.Resize(IMG_SIZE, IMG_SIZE,always_apply=True),\n#     ],p=1.)","c37a94eb":"class HuBMAP(nn.Module):\n    def __init__(self):\n        super(HuBMAP, self).__init__()\n        # since this is a binary segmentation problem FTU or non-FTU so classes = 1\n#         self.cnn_model = smp.Unet(encoder_name='se_resnext50_32x4d', encoder_weights='imagenet', classes=1, activation=None)\n#         self.cnn_model = smp.FPN(encoder_name='se_resnext50_32x4d', encoder_weights='imagenet', classes=1, activation=None)\n\n        self.cnn_model = smp.Unet(encoder_name=ENCODER, encoder_weights=None, classes=1, activation=None)\n#         self.cnn_model = smp.Unet(encoder_name='resnet34', encoder_weights='imagenet', classes=1, activation=None)\n        \n    def forward(self, imgs):\n        img_segs = self.cnn_model(imgs)\n        return img_segs","f94dec20":"def visualise_predictions(image_patches, pred_masks, gt_masks, figsize=(25, 25)):\n    assert image_patches.shape == pred_masks.shape and pred_masks.shape == gt_masks.shape, \"image patches and masks should be of the same shape\"\n    num_patches = image_patches.shape[0]\n    grid_dim = int(sqrt(num_patches))\n    \n    fig, axs = plt.subplots(grid_dim, grid_dim, figsize=figsize)\n    \n    for i, (img_patch, pred_mask, gt_mask) in enumerate(zip(image_patches, pred_masks, gt_masks)):\n        x = i \/\/ ndim\n        y = i % ndim\n        \n        # plot the GT mask on the image\n        img_data = label2rgb(image=img_patch, label=gt_mask, bg_label=0, alpha=0.2)\n        \n        # plot the predicted mask on the image\n        img_data = label2rgb(image=img_data, label=pred_mask, bg_label=0, alpha=0.2)\n        \n        axs[x, y].imshow(img_data)","85d2bbb5":"# create dataloader\n\nds = HuBMAPDataset(preprocess_input=get_preprocess_fn(encoder_name=ENCODER), transforms=None)\ndl = DataLoader(ds,batch_size=25,shuffle=False,num_workers=NUM_WORKERS)\nimgs,masks = next(iter(dl))\n\n# instantiate model and load weights\nmodel = HuBMAP()\nmodel.load_state_dict(torch.load(MODEL_PATH))\nmodel.to(DEVICE)\nmodel.eval()\n\n\npredictions = []\n# select a random batch and make predictions on it\nfor i, img in tqdm(enumerate(imgs), total=imgs.shape[0]):\n    with torch.no_grad():\n        pred_mask = model(img.unsqueeze(0).to(DEVICE))\n    \n    pred_mask = pred_mask.cpu()[0]\n    \n    binary_mask = (pred_mask > 0).int()\n    predictions.append(binary_mask)\n    \n    \npredictions = torch.stack(predictions, dim=0)","9e2c342a":"plt.figure(figsize=(16,16))\nfor i,(img, pred_mask, gt_mask) in enumerate(zip(imgs, predictions, masks)):\n    img = img.permute(1, 2, 0) * STD + MEAN\n    img = (img*255.0).numpy().astype(np.uint8)\n    gt_mask = gt_mask.squeeze().numpy().astype(np.uint8)\n    pred_mask = pred_mask.squeeze().numpy().astype(np.uint8)\n    pred_mask[pred_mask == 1] = 2\n    \n    full_mask = gt_mask + pred_mask\n    # plot the GT mask on the image\n    \n    # red: GT mask, blue: Predicted mask, green: overlap between GT and predicted\n    img_data = label2rgb(image=img, label=full_mask, bg_label=0, alpha=0.2, colors=['red', 'blue', 'green'])\n        \n    # plot the predicted mask on the image\n#     img_data = label2rgb(image=img_data, label=pred_mask, bg_label=0, colors=['blue'], alpha=0.2)\n        \n    plt.subplot(5,5,i+1)\n    plt.imshow(img_data)\n#     plt.imshow(mask.squeeze().numpy(), alpha=0.2)\n    plt.axis('off')\n    plt.subplots_adjust(wspace=None, hspace=None)\n    \n# del ds,dl,imgs,masks","9cf49f84":"# TODO\n1. How to read the data?\n2. How to crop the patches for testing?","9efcb255":"# Necessary imports","fec2f7ec":"# Visualize the predictions","e2d8d4fc":"# Dataset","c6282a14":"# Important variables","64b6dc6f":"# Model\nPut your model definition and loading weights here","db00eaf9":"# Make the prediction","ddbf2764":"# Transformations"}}