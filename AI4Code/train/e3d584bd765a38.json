{"cell_type":{"74af267e":"code","c6097cea":"code","dd3101d9":"code","fcd2b139":"code","e04a5ccc":"code","9528f263":"code","c22adfa4":"code","db05d939":"code","c10f7b21":"code","3959ce86":"code","f6454586":"code","c2d4bd96":"code","fe4e33dc":"code","cdb35f0b":"code","0a3249e3":"code","0c960d3e":"code","71e95995":"code","1f9766ab":"code","b0a28591":"code","cf04c5df":"code","669d01f5":"code","06fdf2bd":"code","7f6fbcbb":"code","06166894":"code","011f4ff3":"code","71ac3e66":"code","5b185881":"code","f885ad76":"markdown","0d0dc382":"markdown","cdf56982":"markdown","e3fe9b95":"markdown","21e02df9":"markdown","12fddc26":"markdown","e8ecf102":"markdown","f6c58689":"markdown","8d873c1c":"markdown","d27bb410":"markdown","fa242999":"markdown","405f1f32":"markdown","75c280bd":"markdown","535cf965":"markdown","1a2277c6":"markdown","72683c5e":"markdown","893baff8":"markdown","95491013":"markdown","dacc930b":"markdown"},"source":{"74af267e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6097cea":"!pip install linearmodels","dd3101d9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import dates\nfrom datetime import datetime\n\nfrom linearmodels import PooledOLS\nfrom linearmodels import PanelOLS\nfrom linearmodels import RandomEffects\nimport statsmodels.api as sm\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nfrom statsmodels.stats.stattools import durbin_watson\n\nfrom scipy import stats\n\n%matplotlib inline","fcd2b139":"df = pd.read_csv('\/kaggle\/input\/retail-analysis-with-walmart-data\/Walmart_Store_sales.csv')\ndf.head()","e04a5ccc":"df.columns","9528f263":"df.info()","c22adfa4":"df.isnull().sum()","db05d939":"df.duplicated().any()","c10f7b21":"years = dates.YearLocator()\nmonths = dates.MonthLocator()\nyears_fmt = dates.DateFormatter('%b %Y')\n\ndf['Date'] = pd.to_datetime(df['Date'], format = '%d-%m-%Y')\n\n#for simplicity, we use 1k unit in sales\ndf['Weekly_Sales'] = round(df['Weekly_Sales'] \/ 1000, 3)\ndf['CPI'] = round(df['CPI'], 2)\n\ndf2 = df.copy()\n\n#build panel data sorting by store and date\ndf2 = df2.sort_values(by = 'Date')\npanel = pd.pivot_table(df2, values = ['Weekly_Sales', 'Holiday_Flag', 'Temperature',\n       'Fuel_Price', 'CPI', 'Unemployment'], index = ['Store', 'Date']).reset_index()\npanel['Month'] = panel['Date'].dt.month\npanel['Year'] = panel['Date'].dt.year\n\npanel.head()","3959ce86":"panel.drop(['Month', 'Year', 'Store'], axis = 1).describe()","f6454586":"#Aggregate Sales for the whole Walmart Store\nholiday_date = pd.to_datetime(['2010-02-12', '2010-09-10',\n       '2010-11-26', '2010-12-31',\n       '2011-02-11', '2011-09-09',\n       '2011-11-25', '2011-12-30',\n       '2012-02-10', '2012-09-07'])\nlocate = pd.to_datetime('2011-05-06')\n\nagg_sales = panel.groupby('Date')['Weekly_Sales'].sum().reset_index()\n\nfig, ax = plt.subplots(1, 1, figsize = (25, 10))\nsns.lineplot(x = agg_sales['Date'], y = agg_sales['Weekly_Sales'], ax = ax, label = 'Data Ori Value')\nsns.lineplot(x = agg_sales['Date'], y = agg_sales['Weekly_Sales'].rolling(12).mean(), ax = ax, \n             label = 'Mean Trend')\n\nfor i in holiday_date:\n    ax.axvline(i, color = 'r', alpha = 0.2, ls='--')\n\nfor s in ['top', 'right']:\n    ax.spines[s].set_visible(False)\n    \nax.text(locate, 87000, 'Walmart Aggregate Sales Trend\\n(In Thousand Dollars)', fontsize = 25, \n        fontweight = 'bold', fontfamily = 'serif', color = 'black', ha = 'center')\nax.set_ylabel('')\nax.set_xlabel('')\nax.xaxis.set_major_formatter(years_fmt)\n\nplt.show()","c2d4bd96":"#Detail about sales in holiday, are thanksgiving holiday have highest sales?\n\n#Defining specific holiday\nsuper_bowl = ['2010-02-12', '2011-02-11', '2012-02-10']\nlabour_day = ['2010-09-10', '2011-09-09', '2012-09-07']\nthanksgiving = ['2010-11-26', '2011-11-25', '2012-11-23']\nchristmas = ['2010-12-31', '2011-12-30', '2012-12-28']\n\nsuper_bowl_sales = df[df['Date'] == '2011-02-11']['Weekly_Sales'].sum()\nlabour_day_sales = df[df['Date'] == '2011-09-09']['Weekly_Sales'].sum()\nthanksgiving_sales = df[df['Date'] == '2011-11-25']['Weekly_Sales'].sum()\nchristmas_sales = df[df['Date'] == '2011-12-30']['Weekly_Sales'].sum()\n\ndata = {'Super Bowl': super_bowl_sales,'Labour Day': labour_day_sales, \n        'Thanksgiving': thanksgiving_sales,'Christmas': christmas_sales}\n\nholiday_sales = round(pd.Series(data),2)\n\nfig, ax = plt.subplots(1, 1, figsize=(13, 8))\nsns.barplot(x = holiday_sales.index, y = holiday_sales, ax = ax, palette = 'viridis')\n\nfor s in ['top', 'right', 'left']:\n    ax.spines[s].set_visible(False)\nax.set_yticks([])\nax.set_ylabel('')\nfor ind, val in enumerate(holiday_sales):\n    ax.text(x = ind, y = val\/2, s = f'{str(val)}k', fontsize = 20, fontweight = 'bold', ha = 'center', \n            fontfamily = 'monospace', color = 'white')\nax.text(1.5, 75000, 'Are Thanksgiving Holiday Contribute Highest Sales?', fontsize = 20, fontweight = 'bold', \n            fontfamily = 'serif', color = 'black', ha = 'center')\n\nplt.show()","fe4e33dc":"pivot_table = pd.pivot_table(panel, values = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment'], \n                             index = 'Date', aggfunc = {'Weekly_Sales': np.sum, 'Temperature': np.mean, 'Fuel_Price': np.mean,\n                                                        'CPI': np.mean, 'Unemployment': np.mean}).reset_index()\npivot_table['Month'] = pivot_table['Date'].dt.month\npivot_table['Year'] = pivot_table['Date'].dt.year\npivot_table.head()","cdb35f0b":"sns.pairplot(pivot_table.drop(['Month', 'Year'], axis = 1))","0a3249e3":"pivot_table.drop(['Month', 'Year'], axis = 1).corr()","0c960d3e":"growth_df = panel[['Date', 'Weekly_Sales']].resample('M', on = 'Date').sum().reset_index()\ngrowth_df['Rolling_Month'] = np.roll(growth_df['Weekly_Sales'], 1)\ngrowth_df['Growth_MoM'] = round((growth_df['Weekly_Sales'] - growth_df['Rolling_Month']) \/ growth_df['Rolling_Month'] * 100, 2)\ngrotwh_df = growth_df.shift(-1)\ngrowth_df['Year'] = growth_df['Date'].dt.year\ngrowth_df['Month'] = growth_df['Date'].dt.month\n\nsuper_bowl = ['2010-02-28', '2011-02-28', '2012-02-28']\nlabour_day = ['2010-09-30', '2011-09-30', '2012-09-30']\nthanksgiving = ['2010-11-30', '2011-11-30', '2012-11-30']\nchristmas = ['2010-12-31', '2011-12-31', '2012-12-31']\n\ng2010 = growth_df[growth_df['Year'] == 2010][['Month', 'Growth_MoM']].max()\ng2011 = growth_df[growth_df['Year'] == 2011][['Month', 'Growth_MoM']].max()\ng2012 = growth_df[growth_df['Year'] == 2012][['Month', 'Growth_MoM']].max()","71e95995":"fig, ax = plt.subplots(1, 1, figsize = (6, 3), dpi = 150)\n\nfor s in ['top','right','left','bottom']:\n    ax.spines[s].set_visible(False)\n    \nax.set_yticklabels('')\nax.set_xticklabels('')\nax.tick_params(axis='both',length=0)\n\nax.text(0.7, 0.85, \"Highest Sales MoM Growth\" , color = 'black', fontsize = 24, fontweight = 'bold', \n         fontfamily = 'sanserif', ha = 'center')\nax.text(0.2, 0.5, \"December\", color = 'blue', fontsize = 25, fontweight = 'bold', fontfamily = 'monospace', ha = 'center')\nax.text(0.2, 0.3, \"42.35%\", color = 'gray', fontsize = 15, fontfamily = 'monospace',ha = 'center', fontweight = 'bold')\nax.text(0.2, 0.1, \"2010\", color = 'gray',fontsize = 15, fontfamily = 'monospace', ha = 'center')\nax.text(0.75, 0.5, \"December\", color = 'blue', fontsize = 25, fontweight = 'bold', fontfamily = 'monospace', ha = 'center')\nax.text(0.75, 0.3, \"37.07%\", color = 'gray', fontsize = 15, fontweight = 'bold', fontfamily = 'monospace', ha = 'center')\nax.text(0.75, 0.1, \"2011\", color = 'gray', fontsize = 15, fontfamily = 'monospace', ha = 'center')\nax.text(1.3, 0.5, \"October\", color = 'blue', fontsize = 25, fontweight = 'bold', fontfamily = 'monospace', ha = 'center')\nax.text(1.3, 0.3, \"27.46%\", color = 'gray', fontsize = 15, fontfamily = 'monospace', ha = 'center', fontweight = 'bold')\nax.text(1.3, 0.1, \"2012\", color = 'gray', fontsize = 15, fontfamily = 'monospace', ha = 'center')","1f9766ab":"# Which store perform better?\n\nsales_store = panel.groupby('Store')['Weekly_Sales'].sum().reset_index()\n\nfig, ax = plt.subplots(1, 1, figsize = (15, 7))\nsns.barplot(x='Store', y='Weekly_Sales', data=sales_store, order=sales_store.sort_values('Weekly_Sales').Store, ax = ax,\n           palette = 'viridis')\n\nfor s in ['top', 'right', 'left']:\n    ax.spines[s].set_visible(False)\nax.set_yticks([])\nax.set_ylabel('')\nax.text(7, 310000, 'Walmart Aggregate Sales by Store \\n(In Thousand Dollars)', fontsize = 18, fontweight = 'bold', \n        fontfamily = 'serif', color = 'black', ha = 'center')\nax.text(-2.5, 230000, '''\nWalmart Store that have higher sales in the period is Store 20 followed by Store 4 and 14. \nThe insight stop here because we dont know exactly where the store location is.\nBut we will defined how is the trend behind it.\n''', fontsize = 14, fontweight = 'light', \n        fontfamily = 'serif', color = 'black')","b0a28591":"#Highest Store\n\nlocate1 = pd.to_datetime(['2011-02'])\n\nstore14 = panel[panel['Store'] == 14][['Date', 'Weekly_Sales']]\nstore14['roll'] = np.roll(store14['Weekly_Sales'], 1)\nstore14['weekly_growth'] = round((store14['Weekly_Sales'] - store14['roll']) \/ store14['roll'] * 100, 2)\nstore14 = store14.shift(-1)\n\nstore4 = panel[panel['Store'] == 4][['Date', 'Weekly_Sales']]\nstore4['roll'] = np.roll(store4['Weekly_Sales'], 1)\nstore4['weekly_growth'] = round((store4['Weekly_Sales'] - store4['roll']) \/ store4['roll'] * 100, 2)\nstore4 = store4.shift(-1)\n\nstore20 = panel[panel['Store'] == 20][['Date', 'Weekly_Sales']]\nstore20['roll'] = np.roll(store20['Weekly_Sales'], 1)\nstore20['weekly_growth'] = round((store20['Weekly_Sales'] - store20['roll']) \/ store20['roll'] * 100, 2)\nstore20 = store20.shift(-1)\n\n#Lowest Store\nstore33 = panel[panel['Store'] == 33][['Date', 'Weekly_Sales']]\nstore33['roll'] = np.roll(store33['Weekly_Sales'], 1)\nstore33['weekly_growth'] = round((store33['Weekly_Sales'] - store33['roll']) \/ store33['roll'] * 100, 2)\nstore33 = store33.shift(-1)\n\nstore44 = panel[panel['Store'] == 44][['Date', 'Weekly_Sales']]\nstore44['roll'] = np.roll(store44['Weekly_Sales'], 1)\nstore44['weekly_growth'] = round((store44['Weekly_Sales'] - store44['roll']) \/ store44['roll'] * 100, 2)\nstore44 = store44.shift(-1)\n\nstore5 = panel[panel['Store'] == 5][['Date', 'Weekly_Sales']]\nstore5['roll'] = np.roll(store5['Weekly_Sales'], 1)\nstore5['weekly_growth'] = round((store5['Weekly_Sales'] - store5['roll']) \/ store5['roll'] * 100, 2)\nstore5 = store5.shift(-1)\n\nfig, ax = plt.subplots(2, 1, figsize = (25, 15))\nsns.lineplot(x =store14['Date'], y = store14['weekly_growth'], ax = ax[0], label = 'Store 14', palette = 'Set2')\nsns.lineplot(x =store4['Date'], y = store4['weekly_growth'], ax = ax[0], label = 'Store 4', palette = 'Set2')\nsns.lineplot(x =store20['Date'], y = store20['weekly_growth'], ax = ax[0], label = 'Store 20', palette = 'Set2')\n#sns.lineplot(x = pivot_table['Date'], y = pivot_table['Temperature'], ax = ax[0], label = 'CPI')\n\nsns.lineplot(x =store33['Date'], y = store33['weekly_growth'], ax = ax[1], label = 'Store 33', palette = 'Set2')\nsns.lineplot(x =store44['Date'], y = store44['weekly_growth'], ax = ax[1], label = 'Store 44', palette = 'Set2')\nsns.lineplot(x =store5['Date'], y = store5['weekly_growth'], ax = ax[1], label = 'Store 5', palette = 'Set2')\n\nfor s in ['top', 'right']:\n    for i in [0,1]:\n        ax[i].spines[s].set_visible(False)\n        ax[i].set_ylabel('')\n        ax[i].set_xlabel('')\n        ax[i].xaxis.set_major_formatter(years_fmt)\n\nax[0].text(locate1, 70, 'Top 3 Sales Stores Growth Trend', color = 'black', fontsize = 20, fontweight = 'bold', \n         fontfamily = 'serif')\nax[1].text(locate1, 70, 'Bottom 3 Sales Stores Growth Trend', color = 'black', fontsize = 20, fontweight = 'bold', \n         fontfamily = 'serif')\n\nplt.show()","cf04c5df":"sales_store = sales_store.sort_values('Weekly_Sales', ascending = False)\nstore_list = list(sales_store['Store'][:4])\n\nsales = panel.groupby(['Store', 'Year'])['Weekly_Sales'].sum()\ntemp = panel.groupby(['Store', 'Year'])['Temperature'].mean()\n#cpi = pivot.groupby(['Store', 'Year'])['CPI'].mean()\n#unem = pivot.groupby(['Store', 'Year'])['Unemployment'].mean()\n#fp = pivot.groupby(['Store', 'Year'])['Fuel_Price'].mean()\n\nfig, ax = plt.subplots(1, 1, figsize=(9, 6))\n\nfor sto in store_list:\n    sns.regplot(x = sales[sto].values, y = temp[sto].values, label = sto)\n    for s in ['top', 'right']:\n        ax.spines[s].set_visible(False)\n    \nlegend_labels, _ = ax.get_legend_handles_labels()\nax.legend(legend_labels, ['Store 20', 'Store 4', 'Store 14', 'Store 13'], ncol = 4, bbox_to_anchor = (0.75, 0.9), \n          loc = 'lower center')\n\nax.text(90000, 150, \"Sales vs Temperature Relationship\" , color = 'black', fontsize = 20, fontweight = 'bold', \n         fontfamily = 'sanserif')\n\nplt.show()","669d01f5":"#building panel dataset format\npanel_data = panel.set_index(['Store', 'Date'])\ndates = panel_data.index.get_level_values('Date').to_list()\npanel_data['Date'] = pd.Categorical(dates)\npanel_data.drop(['Month', 'Year', 'Holiday_Flag'], axis = 1, inplace = True)\npanel_data","06fdf2bd":"#building dependent and independent variable\nindependent = sm.tools.tools.add_constant(panel_data[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']])\ndependent = panel_data['Weekly_Sales']\n\n#fitting variable into panel OLS method\nmodel = PooledOLS(dependent, independent)\npooledOLS_res = model.fit(cov_type='clustered', cluster_entity=True)\n\nresiduals = pooledOLS_res.resids","7f6fbcbb":"pooled_data = pd.concat([panel_data, residuals], axis = 1)\npooled_data = pooled_data.drop(['Date'], axis = 1).fillna(0)\n\n#check regression assumption (heteroskedasticity)\nbreusch_pagan_test = list(het_breuschpagan(pooled_data['residual'], independent))\nlabel_aut = ['LM-Stat', 'LM p-val', 'F-Stat', 'F p-val']\nresult_auto = pd.Series(breusch_pagan_test, index = label_aut)\nprint(result_auto)","06166894":"#check regression assumption (autocorrelation)\ndurbin_watson_test = durbin_watson(pooled_data['residual']) \nprint(durbin_watson_test)","011f4ff3":"#Fixed Effect\nmodel_fe = PanelOLS(dependent, independent, entity_effects = True) \nfe_reg = model_fe.fit() \n\n#Random Effect\nmodel_re = RandomEffects(dependent, independent)\nre_reg = model_re.fit()","71ac3e66":"def hausman(fe, re):\n    b = fe.params\n    B = re.params\n    v_b = fe.cov\n    v_B = re.cov\n    df = b[np.abs(b) < 1e8].size\n    chi2 = np.dot((b - B).T, np.linalg.inv(v_b - v_B).dot(b - B)) \n    pval = stats.chi2.sf(chi2, df)\n    return chi2, df, pval\n\nhausman_results = hausman(fe_reg, re_reg)\nprint('p-Value: ' + str(hausman_results[2]))","5b185881":"#Use Random Effect for interpretation purpose.\n\nprint(re_reg)","f885ad76":"# C. Econometrics Analysis: Does Macroeconomic Variable Affect Weekly Sales?","0d0dc382":"Luckily we have clean dataset that not containing null values, duplicate rows.","cdf56982":"An independent variable is said to have a significant effect on the dependent variable when the p value is less than 0.05.\n\nWe see here that all independent variable (Temperature, CPI, Fuel Price and Unemployemnet) are significant on Weekly Sales. Although these relationship has significant correlation, the slopes\/coeficient\/parameter are small, so do R2.\n\nFor example, temperature has -1.0308: if temperature increase 1 degree, then it could be lower the sales by 1k dollar. ","e3fe9b95":"# Conclusion\n\n1. There is seasonal component in sales through holiday date, so the management have to taking into account the seasonal to stocking, merchandising, marketing etc. decision.\n2. Although Thanksgiving Holiday in November contributed more sales than other holiday, it seems that December has higher MoM growth sales happening before Christmas. \n3. Store 20 indeed has a high sales volume for 3 years. However, it's growth has been dominated by holiday date.\n4. Using econometrics (panel regression method), we found that macroeconomic condition have significant relationship with weekly_sales even though it has small slope and R2. It means that there are external factor outside the model greatly affecting Walmart sales. Possibly it can be the customer buying behaviour, social factor, technology and other issue.","21e02df9":"# Data Description \n\nThis dataset contain 8 available feature:\n1. Store: the store number\n2. Date - the week of sales\n3. Weekly_Sales: sales for the given store\n4. Holiday_Flag: whether the week is a special holiday week 1 \u2013 Holiday week 0 \u2013 Non-holiday week\n5. Temperature: temperature on the day of sale\n6. Fuel_Price: cost of fuel in the region\n7. CPI: prevailing consumer price index\n8. Unemployment: prevailing unemployment rate\n\n\nHoliday Events\n1. Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n2. Labour Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n3. Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n4. Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","12fddc26":"Econometrics is a tool that economist used for knowing how the causal relationship between dependent (target) variable and independent (feature matrix) variable. There are three types of data: 1.) Cross-section, 2.) Time Series, 3.) Pooled Data (Cross-section and Time Series). \n\nBasically, there are three types of regression for panel data:\n1. **PooledOLS** can be described as simple OLS (Ordinary Least Squared) model that is performed on panel data. It ignores time and individual characteristics and focuses only on dependencies between the individuums\n2. **Fixed-Effects (FE) Model**: The FE-model determines individual effects of unobserved, independent variables as constant (\u201cfix\u201c) over time\n3. **Random-Effects (RE) Model**: RE-models determine individual effects of unobserved, independent variables as random variables over time. They are able to \u201cswitch\u201d between OLS and FE and hence, can focus on both, dependencies between and within individuals\n\nHere we will run the data into all the method.","e8ecf102":"# How is the Walmart aggregate sales looks like? Does this sales has seasonal component?","f6c58689":"Here we test the relationship between temperature and sales since this variable has the highest correlation among all feature. As we see that the slope (relative change in dependent variable if independent variable change) are small in all highest sales top 4 store. What we are gonna do next is using econometric to clarify the relationship.\n\n\n\nNote: We better do this econometric domain using SPSS\/Stata software, but for simplicity purpose we use statmodels and linearregression library.","8d873c1c":"If p value less than alpha (0.05), then there is no heteroskedasticity vice versa. Because p value is 3.53 that more than 0.05, so there is violation in heteroskedasticity.","d27bb410":"The graph tell us that this data contain seasonal component on holiday date (red line). Walmart sales seems rapidly increased when holiday date taking place. But in the other side (non-holiday date), sales tend to be in stagnant position. It means we have to be careful interpreting sales volume (Growth sales would be good metrics). Before that, next we'll discuss more little depth which holiday contribute more in Walmart sales aggregately.","fa242999":"Firstly, we run data into pooled OLS regression method. There are 3 classical assumptions that have to be fulfilled. Two of them can help us in choosing between PooledOLS and Fixed Effect and Random Effect Model. These are *heteroskedasticity* and *autocorrelation*. If these two assumptions cant be fulfilled by PooledOLS, then Fixed Effect or Random Effect might be more suitable.","405f1f32":"Although Thanksgiving holiday on November has highest sales than the other holiday, December has the highest growth sales happening before Christmas in 2010 and 2011. It stated that in 2012, October become the highest MoM sales growth because we don't have sales data on December 2012.","75c280bd":"# A. Aggregate Time Series Analysis","535cf965":"# Context\n\n- Walmart is an American multinational retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas (Wikipedia).\n \n- In Retail Industry, sales is the most important metric in their business model that make profit. On this reason, sales analysis will be a good way in determining business operation.\n\n- In this dataset, We'll do analyse how Walmart Sales looks like (Trend and Seasonal) and explain wether available feature (macroeconomic condition) would be affecting it using Econometric Panel Regression (Fixed Effect and Random Effect).","1a2277c6":"# B. Sales Panel Analysis by Store","72683c5e":"It seems that sales have less correlation with macroeconomic condition. Possible reason of this realtionship that because we use weekly data and macroeconomic variables like CPI, unemployment, fuel price have slow\/rigid change in the short run.","893baff8":"In order to choose between Fixed Effect or Random Effect to be the robust model, we will use statistical hausman test. If p value less than 0.05, so Fixed Effect would be a good model, vice versa.","95491013":"If p value less than alpha (0.05), then there is no autocorrelation vice versa. Because p value is 3.53 that more than 0.05, so there is violation in autocorrelation.\n\nWell here we will use Fixed Effect and Random Effect method.","dacc930b":"# References\n\n1. https:\/\/towardsdatascience.com\/a-guide-to-panel-data-regression-theoretics-and-implementation-with-python-4c84c5055cf8"}}