{"cell_type":{"b20263d9":"code","c0f1496b":"code","a7267db4":"code","92041398":"code","5d5b7a13":"code","47658382":"code","aca7c57c":"code","77eb1836":"code","dfea7265":"code","3ef9c541":"code","bd2d6b9d":"code","de853eb3":"code","1910c58d":"code","52d25acd":"code","9718cc33":"code","63788eff":"code","8f8acdfc":"code","e1d13cfd":"code","ce007771":"code","dac3c61e":"code","c1c61d32":"code","6252c66f":"code","c22d1cdf":"code","f52d808c":"code","fda347b0":"code","4b83277f":"code","0a363432":"code","21e839f1":"code","2ed0fe16":"code","6f8350ff":"code","05dc2e27":"code","f80f72b6":"code","c80d64ea":"code","4c9d34bb":"code","beabde5b":"code","becd5196":"code","d74f5995":"code","15899e10":"code","d7fcd9ff":"code","914b764e":"code","70bd99fd":"code","57c63e55":"code","53f700ca":"code","2e10990b":"code","0aa1eb01":"code","c0d1e0c9":"code","da2a07f6":"code","146ed135":"code","efbe5ca7":"code","2190cb3e":"code","800ee075":"code","5f9c243c":"code","72d1079a":"code","e34a6cbb":"code","badfb064":"code","036d2eee":"code","60bb90cf":"code","d956f871":"code","2217471c":"code","8901df0f":"code","8c4dc19b":"code","60c22b64":"code","332197bc":"code","e33e3188":"code","c8b67973":"code","f0b1afa5":"code","34c4c234":"code","52586c70":"code","52ca3208":"code","b617fa8e":"code","ee7b02e2":"code","cfd10dbf":"code","b72df820":"code","1ddb76e8":"code","ad6411f6":"code","8de508a1":"code","a3b763e9":"code","9ce4396f":"code","b912437c":"code","1b8f12e1":"code","3e5d5f97":"code","ff663288":"code","2c4940fb":"code","375a0114":"code","58c82be9":"code","d9c27f12":"code","151ae23a":"code","1936dda0":"code","f571bdb8":"code","8a36aff8":"code","c0a6b593":"code","95f88fed":"code","1b6c525c":"code","39d1c51e":"code","31f07959":"code","9ae4e41a":"code","38b16f97":"code","5cd3b588":"code","b478850d":"code","3d719dcc":"code","77c82387":"code","6d050ddf":"code","5c108165":"code","3b821c7c":"code","a83f7820":"code","182a1f33":"code","0baf4462":"code","046ba030":"code","1ac1df45":"code","18a92bf7":"code","d7c6c4ba":"code","600028e5":"code","42ab008c":"code","d6ee4348":"code","b5110d10":"code","f8d17909":"code","f43712f9":"code","678f0193":"code","99df2222":"code","c06389ca":"code","cb0e50cd":"code","eae6599c":"code","1b45d5a8":"code","b40d84e7":"code","efc721f0":"code","2b0c22cf":"code","2828c9a8":"code","110c24c1":"code","40374eb3":"code","6629448c":"code","10af11be":"code","bd5632ae":"code","0ac950ed":"markdown","95c6b46b":"markdown","eceae42f":"markdown","65787b7f":"markdown","876c888c":"markdown"},"source":{"b20263d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c0f1496b":"\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import file utilities\nimport os\nimport glob\n\n# import charting\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, ArtistAnimation \n%matplotlib inline\n\nfrom IPython.display import HTML\n\n# import computer vision\nimport cv2\n","a7267db4":"TEST_PATH = '..\/input\/deepfake-detection-challenge\/test_videos\/'\nTRAIN_PATH = '..\/input\/deepfake-detection-challenge\/train_sample_videos\/'\n\nmetadata = '..\/input\/deepfake-detection-challenge\/train_sample_videos\/metadata.json'","92041398":"# load the filenames for train videos\ntrain_fns = sorted(glob.glob(TRAIN_PATH + '*.mp4'))\n# load the filenames for test videos\ntest_fns = sorted(glob.glob(TEST_PATH + '*.mp4'))\n\nprint('There are {} samples in the train set.'.format(len(train_fns)))\nprint('There are {} samples in the test set.'.format(len(test_fns)))\n","5d5b7a13":"meta = pd.read_json(metadata).transpose()\nprint(meta.head())\nprint(meta.describe())\n","47658382":"# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = 'FAKE', 'REAL'\nsizes = [meta[meta.label == 'FAKE'].label.count(), meta[meta.label == 'REAL'].label.count()]\n\nfig1, ax1 = plt.subplots(figsize=(10,7))\nax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90, colors=['#f4d53f', '#02a1d8'])\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Labels', fontsize=16)\n\nplt.show()","aca7c57c":"def get_frame(filename):\n    '''\n    Helper function to return the 1st frame of the video by filename\n    INPUT: \n        filename - the filename of the video\n    OUTPUT:\n        image - 1st frame of the video (RGB)\n    '''\n    # Playing video from file\n    cap = cv2.VideoCapture(filename)\n    ret, frame = cap.read()\n\n    # Our operations on the frame come here\n    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    \n    # When everything done, release the capture\n    cap.release()\n    cv2.destroyAllWindows()\n    \n    return image\n\ndef get_label(filename, meta):\n    '''\n    Helper function to get a label from the filepath.\n    INPUT:\n        filename - filename of the video\n        meta - dataframe containing metadata.json\n    OUTPUT:\n        label - label of the video 'FAKE' or 'REAL'\n    '''\n    video_id = filename.split('\/')[-1]\n    return meta.loc[video_id].label\n\ndef get_original_filename(filename, meta):\n    '''\n    Helper function to get the filename of the original image\n    INPUT:\n        filename - filename of the video\n        meta - dataframe containing metadata.json\n    OUTPUT:\n        original_filename - name of the original video\n    '''\n    video_id = filename.split('\/')[-1]\n    original_id = meta.loc[video_id].original\n    \n    return original_id\n\ndef visualize_frame(filename, meta, train = True):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n    image = get_frame(filename)\n\n    # Display the 1st frame of the video\n    fig, axs = plt.subplots(1,3, figsize=(20,7))\n    axs[0].imshow(image) \n    axs[0].axis('off')\n    axs[0].set_title('Original frame')\n    \n    # Extract the face with haar cascades\n    face_cascade = cv2.CascadeClassifier('..\/input\/haarcascades\/haarcascade_frontalface_default.xml')\n\n    # run the detector\n    # the output here is an array of detections; the corners of each detection box\n    # if necessary, modify these parameters until you successfully identify every face in a given image\n    faces = face_cascade.detectMultiScale(image, 1.2, 3)\n\n    # make a copy of the original image to plot detections on\n    image_with_detections = image.copy()\n\n    # loop over the detected faces, mark the image where each face is found\n    for (x,y,w,h) in faces:\n        # draw a rectangle around each detected face\n        # you may also need to change the width of the rectangle drawn depending on image resolution\n        cv2.rectangle(image_with_detections,(x,y),(x+w,y+h),(255,0,0),3)\n\n    axs[1].imshow(image_with_detections)\n    axs[1].axis('off')\n    axs[1].set_title('Highlight faces')\n    \n    # crop out the 1st face\n    crop_img = image.copy()\n    for (x,y,w,h) in faces:\n        crop_img = image[y:y+h, x:x+w]\n        break;\n        \n    # plot the 1st face\n    axs[2].imshow(crop_img)\n    axs[2].axis('off')\n    axs[2].set_title('Zoom-in face')\n    \n    if train:\n        plt.suptitle('Image {image} label: {label}'.format(image = filename.split('\/')[-1], label=get_label(filename, meta)))\n    else:\n        plt.suptitle('Image {image}'.format(image = filename.split('\/')[-1]))\n    plt.show()\n","77eb1836":"print(meta.loc[meta.label=='FAKE'].describe)\nfake_train_sample_video = list(meta.loc[meta.label=='FAKE'].sample(10).index)\nprint(fake_train_sample_video)\n","dfea7265":"def display_image_from_video(video_path):\n    '''\n    input: video_path - path for video\n    process:\n    1. perform a video capture from the video\n    2. read the image\n    3. display the image\n    '''\n    capture_image = cv2.VideoCapture(video_path) \n    ret, frame = capture_image.read()\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    ax.imshow(frame)\n    \nDATA_FOLDER = '..\/input\/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\nfor video_file in fake_train_sample_video:\n    print(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))\n\n","3ef9c541":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/abarnvbtwb.mp4', \n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/aelfnikyqj.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/agrmhtjdlk.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/ahqqqilsxt.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\ni = 0\nwhile i < len(img_list):\n    visualize_frame(img_list[i], meta)\n    i += 1","bd2d6b9d":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/abarnvbtwb.mp4', \n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/aelfnikyqj.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/agrmhtjdlk.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/ahqqqilsxt.mp4']\ni = 0\nwhile i < len(img_list):\n    visualize_frame(img_list[i], meta)\n    i += 1\n","de853eb3":"fake_train_sample_video = list(meta.loc[meta.label=='REAL'].sample(5).index)\nfake_train_sample_video","1910c58d":"def display_image_from_video(video_path):\n    '''\n    input: video_path - path for video\n    process:\n    1. perform a video capture from the video\n    2. read the image\n    3. display the image\n    '''\n    capture_image = cv2.VideoCapture(video_path) \n    ret, frame = capture_image.read()\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    ax.imshow(frame)\n    \nDATA_FOLDER = '..\/input\/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\nfor video_file in fake_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))\n\n","52d25acd":"!pip install dlib","9718cc33":"import cv2,matplotlib.pyplot as plt,dlib","63788eff":"detector = dlib.get_frontal_face_detector()\ncolor_green = (0,255,0)\nline_width = 3","8f8acdfc":"from PIL import Image\nfrom PIL import ImageFilter\n\n\ndef face_extraction(filename, meta, train = True):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n    image = get_frame(filename)\n    \n\n    # Display the 1st frame of the video\n    fig, axs = plt.subplots(1,2, figsize=(20,7))\n    axs[0].imshow(image) \n    axs[0].axis('off')\n    axs[0].set_title('Original frame')\n    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.\n    dets = detector(image, 1)\n    print(dets)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) > 0:\n            roi = image[det.top():det.bottom(),det.left():det.right()]\n            axs[1].imshow(roi)\n            axs[1].axis('off')\n            axs[1].set_title('Highlight faces')\n        else:\n            pass\n    \n\n    plt.show()\n","e1d13cfd":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/abarnvbtwb.mp4', \n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/aelfnikyqj.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/agrmhtjdlk.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/ahqqqilsxt.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\n\ni = 0\nwhile i < len(img_list):\n    face_extraction(img_list[i], meta)\n    i += 1\n","ce007771":"!pip install imutils","dac3c61e":"import cv2\nimport numpy as np\nfrom PIL import Image\nimport dlib","c1c61d32":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/abarnvbtwb.mp4', \n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/aelfnikyqj.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/agrmhtjdlk.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/ahqqqilsxt.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(\"..\/input\/dlibpackage\/shape_predictor_68_face_landmarks.dat\")\n\n\ni = 0\nwhile i < len(img_list):\n    face_extraction(img_list[i], meta)\n    i += 1","6252c66f":"def face_extraction(filename, meta, train = True):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n    image = get_frame(filename)\n#    image = imutils.resize(image, width=800)\n#    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Display the 1st frame of the video\n    fig, axs = plt.subplots(1,2, figsize=(20,7))\n    axs[0].imshow(image) \n    axs[0].axis('off')\n    axs[0].set_title('Original frame')\n    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.\n    dets = detector(image, 1)\n    print(dets)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) > 0:\n            roi = image[det.top():det.bottom(),det.left():det.right()]\n            axs[1].imshow(roi)\n            axs[1].axis('off')\n            axs[1].set_title('Highlight faces')\n        else:\n            pass\n    \n\n    plt.show()\n","c22d1cdf":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/abarnvbtwb.mp4', \n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/aelfnikyqj.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/agrmhtjdlk.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/ahqqqilsxt.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\n\ni = 0\nwhile i < len(img_list):\n    face_extraction(img_list[i], meta)\n    i += 1","f52d808c":"#measure exposure\ndef exposure(image):\n    hist = cv2.calcHist([image],[0],None,[256],[0,256])\n    hist1 = hist[0:64]\n    hist2 = hist[64:128]\n    hist3 = hist[128:192]\n    hist4 = hist[192:256]\n    plt.hist(image.ravel(),256,[0,256]); \n#measure noise\nfrom skimage.restoration import estimate_sigma\n\ndef estimate_noise(image):\n    return estimate_sigma(image, multichannel=True, average_sigmas=True)\n  \n#measure blur\ndef estimate_blur(image):\n    threshold = 80\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    score = cv2.Laplacian(image, cv2.CV_64F).var()\n    if score > threshold:\n        print(\"Not Blur\")\n    else:\n        print(\"Blur\")\n    \n    \nfrom scipy import fftpack\nfrom matplotlib.colors import LogNorm\n\n# Show the results\n\n\ndef plot_spectrum(image):\n    im_fft = fftpack.fft2(image)\n    plt.imshow(im_fft)\n    plt.show()\n\n","fda347b0":"def white_balance(img):\n    result = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    avg_a = np.average(result[:, :, 1])\n    avg_b = np.average(result[:, :, 2])\n    result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] \/ 255.0) * 1.1)\n    result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] \/ 255.0) * 1.1)\n    result = cv2.cvtColor(result, cv2.COLOR_LAB2BGR)\n    return result\n\n\ndef face_extraction_features_extraction(filename, meta, train = True):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n    image = get_frame(filename)\n    #image = white_balance(image)\n\n    # Display the 1st frame of the video\n    fig, axs = plt.subplots(1,2, figsize=(20,7))\n    axs[0].imshow(image) \n    axs[0].axis('off')\n    axs[0].set_title('Original frame')\n    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.\n    dets = detector(image, 1)\n    print(dets)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) > 0:\n            roi = image[det.top():det.bottom(),det.left():det.right()]\n            print('estimate noise: ',estimate_noise(roi))\n            print('estimate blur: ',estimate_blur(roi))\n            axs[1].imshow(roi)\n            axs[1].axis('off')\n            axs[1].set_title('Highlight faces')\n            plt.show()\n            print('exposure')\n            exposure(roi)\n            plt.show()\n        else:\n            pass\n    \n\n    plt.show()\n","4b83277f":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/abarnvbtwb.mp4', \n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/aelfnikyqj.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/agrmhtjdlk.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/ahqqqilsxt.mp4']\n\n\ni = 0\nwhile i < len(img_list):\n    face_extraction_features_extraction(img_list[i], meta)\n    i += 1\n","0a363432":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\n\ni = 0\nwhile i < len(img_list):\n    image = get_frame(img_list[i])\n    new_image = cv2.Laplacian(image,cv2.CV_64F)\n    plt.figure(figsize=(11,6))\n    plt.subplot(131), plt.imshow(image, cmap='gray'),plt.title('Original')\n    plt.xticks([]), plt.yticks([])\n    plt.subplot(132), plt.imshow(new_image, cmap='gray'),plt.title('Laplacian')\n    plt.xticks([]), plt.yticks([])\n    i += 1","21e839f1":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\ni = 0\nwhile i < len(img_list):\n    image = get_frame(img_list[i])\n    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    dft = cv2.dft(np.float32(image),flags = cv2.DFT_COMPLEX_OUTPUT)\n# shift the zero-frequncy component to the center of the spectrum\n    dft_shift = np.fft.fftshift(dft)\n# save image of the image in the fourier domain.\n    magnitude_spectrum = 20*np.log(cv2.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))\n    plt.figure(figsize=(11,6))\n    plt.subplot(121),plt.imshow(image, cmap = 'gray')\n    plt.title('Input Image'), plt.xticks([]), plt.yticks([])\n    plt.subplot(122),plt.imshow(magnitude_spectrum, cmap = 'gray')\n    plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\n    plt.show()\n    i += 1","2ed0fe16":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\n\ni = 0\nwhile i < len(img_list):\n    print(img_list[i])\n    image = get_frame(img_list[i])\n    new_image = cv2.Laplacian(image,cv2.CV_64F)\n    plt.figure(figsize=(11,6))\n    plt.subplot(131), plt.imshow(image, cmap='gray'),plt.title('Original')\n    plt.xticks([]), plt.yticks([])\n    plt.subplot(132), plt.imshow(new_image, cmap='gray'),plt.title('Laplacian')\n    plt.xticks([]), plt.yticks([])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    dft = cv2.dft(np.float32(image),flags = cv2.DFT_COMPLEX_OUTPUT)\n# shift the zero-frequncy component to the center of the spectrum\n    dft_shift = np.fft.fftshift(dft)\n# save image of the image in the fourier domain.\n    magnitude_spectrum = 20*np.log(cv2.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))\n    plt.subplot(133),plt.imshow(magnitude_spectrum, cmap = 'gray')\n    plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\n    plt.show()\n    \n    i += 1","6f8350ff":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/abarnvbtwb.mp4', \n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/aelfnikyqj.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/agrmhtjdlk.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/ahqqqilsxt.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\n\ni = 0\nwhile i < len(img_list):\n        # get the 1st frame of the video\n    image = get_frame(img_list[i])\n    image = white_balance(image)\n    # Display the 1st frame of the video\n    fig, axs = plt.subplots(1,2, figsize=(20,7))\n    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.Q\n    dets = detector(image, 1)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) == 1080:\n            roi = image[det.top():det.bottom(),det.left():det.right()]            \n            image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n            new_image = cv2.Laplacian(image,cv2.CV_64F)\n            plt.figure(figsize=(11,6))\n            plt.subplot(131), plt.imshow(roi, cmap='gray'),plt.title('Original')\n            plt.xticks([]), plt.yticks([])\n            plt.subplot(132), plt.imshow(new_image, cmap='gray'),plt.title('Laplacian')\n            plt.xticks([]), plt.yticks([])\n            dft = cv2.dft(np.float32(image),flags = cv2.DFT_COMPLEX_OUTPUT)\n# shift the zero-frequncy component to the center of the spectrum\n            dft_shift = np.fft.fftshift(dft)\n# save image of the image in the fourier domain.\n            magnitude_spectrum = 20*np.log(cv2.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))\n            print('magnitude shape: ',magnitude_spectrum.shape)\n            plt.subplot(133),plt.imshow(magnitude_spectrum, cmap = 'gray')\n            plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\n            plt.show()\n \n    i += 1","05dc2e27":"from PIL import Image, ImageEnhance \n\ndef feature_extraction(filename, meta, train = True):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n    image = get_frame(filename)\n    image = white_balance(image)\n\n    # Display the 1st frame of the video\n    fig, axs = plt.subplots(1,2, figsize=(20,7))\n    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.\n    dets = detector(image, 1)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) == 1080:\n            roi = image[det.top():det.bottom(),det.left():det.right()]            \n            #image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n            new_image = cv2.Laplacian(roi,cv2.CV_64F)\n            f = np.fft.fft2(new_image)\n            f_shift = np.fft.fftshift(f)\n            f_complex = f_shift\n            f_abs = np.abs(f_complex) + 1 # lie between 1 and 1e6\n            f_bounded = 20 * np.log(f_abs)\n            f_img = 255 * f_bounded \/ np.max(f_bounded)\n            f_img = f_img.astype(np.uint8)\n            \n            #new_image2 = cv2.Laplacian(image,cv2.CV_64F)\n            #new_image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n\n            print('estimate noise: ',estimate_noise(roi))\n            print('estimate blur: ',estimate_blur(roi))\n            print('exposure')\n            exposure(roi)\n            plt.show()\n            plt.figure(figsize=(11,6))\n            plt.subplot(131), plt.imshow(roi, cmap='gray'),plt.title('Original')\n            plt.xticks([]), plt.yticks([])\n            plt.subplot(132), plt.imshow(new_image, cmap='gray'),plt.title('Laplacian')\n            #plt.subplot(132), plt.imshow(new_image2, cmap='gray'),plt.title('Laplacian')\n            plt.xticks([]), plt.yticks([])\n            plt.subplot(133), plt.imshow(f_img, cmap='gray'),plt.title('Spectrum')\n            plt.xticks([]), plt.yticks([])\n            \n            #dft = cv2.dft(np.float32(new_image1),flags = cv2.DFT_COMPLEX_OUTPUT)\n# shift the zero-frequncy component to the center of the spectrum\n            #dft_shift = np.fft.fftshift(dft)\n# save image of the image in the fourier domain.\n            #magnitude_spectrum = 20*np.log(cv2.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))\n            #plt.subplot(133),plt.imshow(magnitude_spectrum, cmap = 'gray')\n            #plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\n            #plt.show()\n        else:\n            pass\n        \n    \n\n    plt.show()\n","f80f72b6":"img_list = ['..\/input\/deepfake-detection-challenge\/train_sample_videos\/awhmfnnjih.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cwbacdwrzo.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/cxttmymlbn.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/eprybmbpba.mp4',\n'..\/input\/deepfake-detection-challenge\/train_sample_videos\/bbvgxeczei.mp4']\n\n\ni = 0\nwhile i < len(img_list):\n    print(img_list[i])\n    feature_extraction(img_list[i], meta)\n    i += 1\n","c80d64ea":"img_list = []\n\nprint(meta.loc[meta.label=='FAKE'].describe)\nfake_train_sample_video = list(meta.loc[meta.label=='FAKE'].index)\ni = 0\nwhile i < len(fake_train_sample_video):\n    img_list.append('..\/input\/deepfake-detection-challenge\/train_sample_videos\/'+str(fake_train_sample_video[i]))\n    i += 1\n","4c9d34bb":"i = 0\nwhile i < 10:\n    print(img_list[i])\n    feature_extraction(img_list[i], meta)\n    i += 1","beabde5b":"from PIL import Image, ImageEnhance \n\ndef data_preparation(path,filename, meta, train = True):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n    image = get_frame(path+filename)\n    image = white_balance(image)\n    # Display the 1st frame of the video    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.\n    dets = detector(image, 1)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        try:\n            if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) == 1080:\n                roi = image[det.top():det.bottom(),det.left():det.right()]    \n            #image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n                new_image = cv2.Laplacian(roi,cv2.CV_64F)\n                f = np.fft.fft2(new_image)\n                f_shift = np.fft.fftshift(f)\n                f_complex = f_shift\n                f_abs = np.abs(f_complex) + 1 # lie between 1 and 1e6\n                f_bounded = 20 * np.log(f_abs)\n                f_img = 255 * f_bounded \/ np.max(f_bounded)\n                f_img = f_img.astype(np.uint8)\n                return(f_img)\n        except:\n            pass\n\n","becd5196":"real_train_sample_video = list(meta.loc[meta.label=='FAKE'].index)\nprint(len(real_train_sample_video))\nfake_train_sample_video = list(meta.loc[meta.label=='REAL'].index)\nprint(len(fake_train_sample_video))","d74f5995":"path = '..\/input\/deepfake-detection-challenge\/train_sample_videos\/'\n\ni = 0\nwhile i < len(real_train_sample_video[0:300]):\n    print(real_train_sample_video[i])\n    laplacian_answer = data_preparation(path,real_train_sample_video[i], meta)\n    try:\n        cv2.imwrite('train_real_'+str(i)+'.jpg', laplacian_answer) \n        plt.imshow(laplacian_answer)\n        plt.show()\n    except:\n        pass\n    i += 1\n    \ni = 0\nwhile i < len(real_train_sample_video[301:323]):\n    print(real_train_sample_video[i])\n    laplacian_answer = data_preparation(path,real_train_sample_video[i], meta)\n    try:\n        cv2.imwrite('test_real_'+str(i)+'.jpg', laplacian_answer)    \n        plt.imshow(laplacian_answer)\n        plt.show()\n    except:\n        pass\n    i += 1\n    \ni = 0\nwhile i < len(fake_train_sample_video):\n    print(fake_train_sample_video[i])\n    laplacian_answer = data_preparation(path,fake_train_sample_video[i], meta)\n    try:\n        cv2.imwrite('test_fake_'+str(i)+'.jpg', laplacian_answer)\n        plt.imshow(laplacian_answer)\n        plt.show()\n    except:\n        pass\n    i += 1","15899e10":"print(os.listdir())","d7fcd9ff":"import os\ndata = os.listdir()\ntrain_pictures = []\ntest_pictures = []\nlabels = []\ndata_type = []\n\ni = 0\nwhile i < len(data):\n    if data[i][0:5] == 'train':\n        train_pictures.append(data[i])\n        labels.append(0)\n        data_type.append('TRAIN')\n    elif data[i][0:4] == 'test':\n        test_pictures.append(data[i])\n        if data[i][5:9] == 'fake':\n            labels.append(0)\n            data_type.append('TEST')\n        elif data[i][5:9] == 'real':\n            labels.append(1)\n            data_type.append('TEST')\n    else:\n        pass\n    i += 1\n","914b764e":"print(len(train_pictures))\nprint(test_pictures[1])\nprint(test_pictures[1][0:5])","70bd99fd":"print(test_pictures)","57c63e55":"from PIL import Image, ImageFile\nfrom matplotlib.pyplot import imshow\nimport requests\nimport numpy as np\nfrom io import BytesIO\n\ndef make_square(img):\n    cols,rows = img.size\n    \n    if rows>cols:\n        pad = (rows-cols)\/2\n        img = img.crop((pad,0,cols,cols))\n    else:\n        pad = (cols-rows)\/2\n        img = img.crop((0,pad,rows,rows))\n    \n    return img\n    \nx = [] \ndsize = (128, 128)\n\nfor pic in train_pictures:\n    img = cv2.imread(pic)\n    img = cv2.resize(img, dsize)\n    plt.imshow(img)\n    img_array = np.asarray(img)\n    img_array = img_array.flatten()\n    img_array = img_array.astype(np.float32)\n    img_array = (img_array-128)\/128\n    x.append(img_array)\n    \n\nx = np.array(x)\nprint(x.shape)","53f700ca":"x = []    \ny = []\nloaded_images = []\n    \nfor pic in train_pictures:\n    \n    img = cv2.imread(pic)\n    img = cv2.resize(img, dsize, Image.ANTIALIAS)\n    y.append(img)\n    plt.imshow(img)\n\n    img_array = np.asarray(img)\n    img_array = img_array.flatten()\n    img_array = img_array.astype(np.float32)\n    img_array = (img_array-128)\/128\n    x.append(img_array)\n    \n    \n\nx = np.array(x)\n\nprint(x.shape)\n","2e10990b":"%matplotlib inline\nfrom PIL import Image, ImageFile\nfrom matplotlib.pyplot import imshow\nimport requests\nfrom io import BytesIO\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom IPython.display import display, HTML\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Fit reg#ression DNN model.\nprint(\"Creating\/Training neural network\")\nmodel = Sequential()\nmodel.add(Dense(100, input_dim=x.shape[1], activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(x.shape[1])) # Multiple output neurons\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x,x,verbose=1,epochs=10000)\n\n\n\nprint(\"Neural network trained\")\n\n","0aa1eb01":"print(model.summary())","c0d1e0c9":"test_pictures = ['test_real_6.jpg','test_real_17.jpg','test_real_0.jpg',  'test_real_11.jpg', 'test_real_20.jpg', 'test_real_4.jpg', 'test_real_8.jpg', 'test_real_3.jpg', 'test_real_16.jpg', 'test_real_21.jpg', 'test_real_13.jpg',  'test_real_10.jpg', 'test_real_15.jpg',  'test_real_18.jpg', 'test_real_12.jpg','test_real_19.jpg',  'test_real_14.jpg','test_real_5.jpg']","da2a07f6":"img = y[0]\nimg = cv2.imread(pic)\nimg = cv2.resize(img, dsize)\nplt.imshow(img)\nimg_array = np.asarray(img)\nprint(img.size)\n\n#Display noisy image\nimg2 = img_array.astype(np.uint8)\nimg2 = Image.fromarray(img2, 'RGB')\nplt.imshow(img2)\nplt.show()\n# Present noisy image to auto encoder\nimg_array = img_array.flatten()\nimg_array = img_array.astype(np.float32)\nimg_array = (img_array-128)\/128\nimg_array = np.array([img_array])\npred = model.predict(img_array)[0]\n\n# Display neural result\nimg_array2 = pred.reshape(128,128,3)\nimg_array2 = (img_array2*128)+128\nimg_array2 = img_array2.astype(np.uint8)\nimg2 = Image.fromarray(img_array2, 'RGB')\n\nplt.imshow(img2)\nplt.show()","146ed135":"y_fake = []\n\n    \nfor pic in test_pictures:\n    img = cv2.imread(pic)\n    img = cv2.resize(img, dsize, Image.ANTIALIAS)\n    y_fake.append(img)\n\nprint(len(y_fake))\nprint(test_pictures)","efbe5ca7":"i = 0\ntt = []\ntt2 = []\nwhile i <len(test_pictures):\n    img = y_fake[i]\n    print(test_pictures[i])\n    img = cv2.imread(test_pictures[i])\n    img = cv2.resize(img, dsize)\n    plt.imshow(img)\n    img_array = np.asarray(img)\n    print(img.size)\n\n    #Display noisy image\n    img2 = img_array.astype(np.uint8)\n    img2 = Image.fromarray(img2, 'RGB')\n    plt.imshow(img2)\n    plt.show()\n    # Present noisy image to auto encoder\n    img_array = img_array.flatten()\n    img_array = img_array.astype(np.float32)\n    img_array = (img_array-128)\/128\n    img_array = np.array([img_array])\n    pred = model.predict(img_array)[0]\n\n    # Display neural result\n    img_array2 = pred.reshape(128,128,3)\n    img_array2 = (img_array2*128)+128\n    img_array2 = img_array2.astype(np.uint8)\n    img2 = Image.fromarray(img_array2, 'RGB')\n\n    plt.imshow(img2)\n    plt.show()\n    \n    score1 = np.sqrt(metrics.mean_squared_error(pred,img_array[0]))\n    print(f\"Out of Sample Score (RMSE): {score1}\")   \n    tt.append(score1)\n    tt2.append(score1)\n    print('=====================')\n    print('=====================')\n    i += 1","2190cb3e":"print(tt2)","800ee075":"data = {'col_1': tt2}\ndf = pd.DataFrame.from_dict(data)\nprint(df.describe())\n\nfig1, ax1 = plt.subplots()\nax1.set_title('box plot')\nax1.boxplot(tt2)","5f9c243c":"plt.scatter(sorted(tt2),range(0,len(tt2)), label='skitscat', color='k', s=25, marker=\"o\")\nplt.show()","72d1079a":"threshold = 0.041602#0.036230 + 2*0.007128","e34a6cbb":"import math\ni = 0\ntt = [0.042558003, 0.030652788, 0.042754516, 0.038435463, 0.038629595, 0.041084316, 0.035615653, 0.04272078, 0.033939373, 0.04175356, 0.041320685, 0.0368206, 0.042760883, 0.039931614, 0.03664131, 0.031103585, 0.038762517, 0.038499016]\nprint(max(tt))\nmean_value = sum(tt)\/len(tt)\nprint(mean_value)\nwhile i < len(tt):\n    tt[i] = (tt[i]-mean_value)**2\n    i += 1\nvalue_std = math.sqrt(sum(tt)\/len(tt))\nprint(value_std)\n\n#mean: 0.03855468094444445\n#std: 0.003723231593256656\n","badfb064":"test_pictures = ['test_fake_23.jpg', 'test_fake_49.jpg', 'test_fake_71.jpg', 'test_fake_41.jpg', 'test_fake_50.jpg', 'test_fake_11.jpg', 'test_fake_27.jpg', 'test_fake_44.jpg', 'test_fake_16.jpg', 'test_fake_30.jpg', 'test_fake_68.jpg', 'test_fake_59.jpg', 'test_fake_70.jpg', 'test_fake_67.jpg', 'test_fake_38.jpg', 'test_fake_55.jpg', 'test_fake_43.jpg', 'test_fake_1.jpg', 'test_fake_22.jpg', 'test_fake_32.jpg', 'test_fake_75.jpg', 'test_fake_63.jpg', 'test_fake_12.jpg', 'test_fake_29.jpg', 'test_fake_8.jpg', 'test_fake_7.jpg', 'test_fake_18.jpg', 'test_fake_0.jpg', 'test_fake_62.jpg', 'test_fake_46.jpg','test_fake_51.jpg', 'test_fake_73.jpg','test_fake_20.jpg', 'test_fake_48.jpg', 'test_fake_4.jpg', 'test_fake_34.jpg','test_fake_58.jpg', 'test_fake_24.jpg', 'test_fake_57.jpg', 'test_fake_5.jpg', 'test_fake_74.jpg', 'test_fake_61.jpg', 'test_fake_15.jpg', 'test_fake_25.jpg', 'test_fake_13.jpg', 'test_fake_33.jpg', 'test_fake_66.jpg', 'test_fake_37.jpg', 'test_fake_65.jpg', 'test_fake_40.jpg', 'test_fake_28.jpg', 'test_fake_64.jpg', 'test_fake_39.jpg', 'test_fake_10.jpg', 'test_fake_35.jpg', 'test_fake_14.jpg', 'test_fake_69.jpg', 'test_fake_26.jpg', 'test_fake_42.jpg', 'test_fake_72.jpg', 'test_fake_47.jpg', 'test_fake_56.jpg', 'test_fake_19.jpg', 'test_fake_36.jpg', 'test_fake_6.jpg', 'test_fake_60.jpg', 'test_fake_54.jpg', 'test_fake_31.jpg']","036d2eee":"y_fake = []\n\n    \nfor pic in test_pictures:\n    img = cv2.imread(pic)\n    img = cv2.resize(img, dsize, Image.ANTIALIAS)\n    y_fake.append(img)\n\nprint(len(y_fake))\nprint(test_pictures)","60bb90cf":"i = 0\ntt = []\ntt2 = []\nwhile i <len(test_pictures):\n    img = y_fake[i]\n    print(test_pictures[i])\n    img = cv2.imread(test_pictures[i])\n    img = cv2.resize(img, dsize)\n    plt.imshow(img)\n    img_array = np.asarray(img)\n    print(img.size)\n\n    #Display noisy image\n    img2 = img_array.astype(np.uint8)\n    img2 = Image.fromarray(img2, 'RGB')\n    plt.imshow(img2)\n    plt.show()\n    # Present noisy image to auto encoder\n    img_array = img_array.flatten()\n    img_array = img_array.astype(np.float32)\n    img_array = (img_array-128)\/128\n    img_array = np.array([img_array])\n    pred = model.predict(img_array)[0]\n\n    # Display neural result\n    img_array2 = pred.reshape(128,128,3)\n    img_array2 = (img_array2*128)+128\n    img_array2 = img_array2.astype(np.uint8)\n    img2 = Image.fromarray(img_array2, 'RGB')\n\n    plt.imshow(img2)\n    plt.show()\n    \n    score1 = np.sqrt(metrics.mean_squared_error(pred,img_array[0]))\n    print(f\"Out of Sample Score (RMSE): {score1}\")   \n    tt.append(score1)\n    tt2.append(score1)\n    print('=====================')\n    print('=====================')\n    i += 1","d956f871":"data = {'col_1': tt2}\ndf = pd.DataFrame.from_dict(data)\nprint(df.describe())\n\nfig1, ax1 = plt.subplots()\nax1.set_title('box plot')\nax1.boxplot(tt2)","2217471c":"plt.scatter(sorted(tt2),range(0,len(tt2)), label='skitscat', color='k', s=25, marker=\"o\")\nplt.show()","8901df0f":"tt2 = [0.042558003, 0.030652788, 0.042754516, 0.038435463, 0.038629595, 0.041084316, 0.035615653, 0.04272078, 0.033939373, 0.04175356, 0.041320685, 0.0368206, 0.042760883, 0.039931614, 0.03664131, 0.031103585, 0.038762517, 0.038499016,0.058573503, 0.066934064, 0.05464456, 0.07709787, 0.08883689, 0.07328239, 0.09117831, 0.07257624, 0.077166654, 0.059338007, 0.10371624, 0.042389914, 0.05010311, 0.07240847, 0.067032576, 0.062947355, 0.079062, 0.087789275, 0.06871186, 0.0634957, 0.052625958, 0.035812557, 0.078799985, 0.06328735, 0.06950112, 0.064118214, 0.060417272, 0.06591671, 0.09110638, 0.08182621, 0.070093654, 0.052168075, 0.05857365, 0.064667016, 0.075556725, 0.05015098, 0.061487824, 0.071132205, 0.07968406, 0.062037375, 0.068701394, 0.052525327, 0.07065824, 0.083880365, 0.09539992, 0.07136654, 0.08247659, 0.06425655, 0.06758966, 0.07027674, 0.05371886, 0.06454329, 0.038022883, 0.07525901, 0.05537062, 0.07471208, 0.05937753, 0.06530632, 0.09115084, 0.047818627, 0.06841827, 0.064744055, 0.09184977, 0.09172562, 0.07337593, 0.08758027, 0.06695, 0.060223594]\n\ndata = {'col_1': tt2}\ndf = pd.DataFrame.from_dict(data)\nprint(df.describe())\n\nfig1, ax1 = plt.subplots()\nax1.set_title('box plot')\nax1.boxplot(tt2)","8c4dc19b":"pictures = []\nreal = ['test_real_6.jpg', 'test_real_17.jpg', 'test_real_0.jpg', 'test_real_11.jpg', 'test_real_20.jpg', 'test_real_4.jpg', 'test_real_8.jpg', 'test_real_3.jpg', 'test_real_16.jpg', 'test_real_21.jpg', 'test_real_13.jpg', 'test_real_10.jpg', 'test_real_15.jpg', 'test_real_18.jpg', 'test_real_12.jpg', 'test_real_19.jpg', 'test_real_14.jpg', 'test_real_5.jpg']\nlabel = []\nfor pic in real:\n    pictures.append(pic)\n    label.append(0)\n    \nfake = ['test_fake_23.jpg', 'test_fake_49.jpg', 'test_fake_71.jpg', 'test_fake_41.jpg', 'test_fake_50.jpg', 'test_fake_11.jpg', 'test_fake_27.jpg', 'test_fake_44.jpg', 'test_fake_16.jpg', 'test_fake_30.jpg', 'test_fake_68.jpg', 'test_fake_59.jpg', 'test_fake_70.jpg', 'test_fake_67.jpg', 'test_fake_38.jpg', 'test_fake_55.jpg', 'test_fake_43.jpg', 'test_fake_1.jpg', 'test_fake_22.jpg', 'test_fake_32.jpg', 'test_fake_75.jpg', 'test_fake_63.jpg', 'test_fake_12.jpg', 'test_fake_29.jpg', 'test_fake_8.jpg', 'test_fake_7.jpg', 'test_fake_18.jpg', 'test_fake_0.jpg', 'test_fake_62.jpg', 'test_fake_46.jpg', 'test_fake_51.jpg', 'test_fake_73.jpg', 'test_fake_20.jpg', 'test_fake_48.jpg', 'test_fake_4.jpg', 'test_fake_34.jpg', 'test_fake_58.jpg', 'test_fake_24.jpg', 'test_fake_57.jpg', 'test_fake_5.jpg', 'test_fake_74.jpg', 'test_fake_61.jpg', 'test_fake_15.jpg', 'test_fake_25.jpg', 'test_fake_13.jpg', 'test_fake_33.jpg', 'test_fake_66.jpg', 'test_fake_37.jpg', 'test_fake_65.jpg', 'test_fake_40.jpg', 'test_fake_28.jpg', 'test_fake_64.jpg', 'test_fake_39.jpg', 'test_fake_10.jpg', 'test_fake_35.jpg', 'test_fake_14.jpg', 'test_fake_69.jpg', 'test_fake_26.jpg', 'test_fake_42.jpg', 'test_fake_72.jpg', 'test_fake_47.jpg', 'test_fake_56.jpg', 'test_fake_19.jpg', 'test_fake_36.jpg', 'test_fake_6.jpg', 'test_fake_60.jpg', 'test_fake_54.jpg', 'test_fake_31.jpg']\nfor pic in fake:\n    pictures.append(pic)\n    label.append(1)\n    \ndata = data = {'name': pictures, 'label':label}\ndf = pd.DataFrame.from_dict(data)\nprint(df.head())\nprint(df.tail())","60c22b64":"y_fake = []\n\n    \nfor pic in df.name.values.tolist():\n    img = cv2.imread(pic)\n    img = cv2.resize(img, dsize, Image.ANTIALIAS)\n    y_fake.append(img)\n\nprint(len(y_fake))\n","332197bc":"i = 0\npredictions = []\nname = []\nscore = []\nthreshold_value = []\nprediction_list = df.name.values.tolist()\nlabel = df.label.values.tolist()\nexpected = []\noutput = []\n\nprint(prediction_list)\nwhile i <len(prediction_list):\n    img = y_fake[i]\n    print(prediction_list[i])\n    img = cv2.imread(prediction_list[i])\n    img = cv2.resize(img, dsize)\n    plt.imshow(img)\n    img_array = np.asarray(img)\n    print(img.size)\n\n    #Display noisy image\n    img2 = img_array.astype(np.uint8)\n    img2 = Image.fromarray(img2, 'RGB')\n    plt.imshow(img2)\n    plt.show()\n    # Present noisy image to auto encoder\n    img_array = img_array.flatten()\n    img_array = img_array.astype(np.float32)\n    img_array = (img_array-128)\/128\n    img_array = np.array([img_array])\n    pred = model.predict(img_array)[0]\n\n    # Display neural result\n    img_array2 = pred.reshape(128,128,3)\n    img_array2 = (img_array2*128)+128\n    img_array2 = img_array2.astype(np.uint8)\n    img2 = Image.fromarray(img_array2, 'RGB')\n\n    plt.imshow(img2)\n    plt.show()\n    \n    score1 = np.sqrt(metrics.mean_squared_error(pred,img_array[0]))\n    print(f\"Out of Sample Score (RMSE): {score1}\")   \n    if score1 > threshold:\n        value = 1\n    else:\n        value = 0\n    if value == 1 and label[i] == 1:\n        output.append(0)\n    elif value == 0 and label[i] == 0:\n        output.append(0)\n    else:\n        output.append(1)\n    predictions.append(value)\n    name.append(prediction_list[i])\n    score.append(score1)\n    threshold_value.append(threshold)\n    expected.append(label[i])\n    print('=====================')\n    print('=====================')\n    i += 1","e33e3188":"print(predictions)\nprint(df.label.values.tolist())","c8b67973":"data = data = {'name': pictures,'threshold':threshold,'score':score,'prediction':predictions,'label':expected, 'output':output}\n\n\ndf = pd.DataFrame.from_dict(data)\nprint(df)","f0b1afa5":"\nerror_df = pd.DataFrame({'reconstruction_error': score,\n                        'true_class': expected})\nerror_df.describe()","34c4c234":"fig = plt.figure()\nax = fig.add_subplot(111)\nnormal_error_df = error_df[(error_df['true_class']== 0)]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)","52586c70":"fig = plt.figure()\nax = fig.add_subplot(111)\nfraud_error_df = error_df[error_df['true_class'] == 1]\n_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)","52ca3208":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)\n","b617fa8e":"fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show();","ee7b02e2":"precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\nplt.plot(recall, precision, 'b', label='Precision-Recall curve')\nplt.title('Recall vs Precision')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","cfd10dbf":"plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\nplt.title('Precision for different threshold values')\nplt.xlabel('Threshold')\nplt.ylabel('Precision')\nplt.show()","b72df820":"plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\nplt.title('Recall for different threshold values')\nplt.xlabel('Reconstruction error')\nplt.ylabel('Recall')\nplt.show()","1ddb76e8":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\ny_pred = df.prediction.values.tolist()\ny_true = df.label.values.tolist()\ny = np.array(y_true)\npred = np.array(y_pred)\n\n\nprint(accuracy_score(y_true, y_pred))\nprint(confusion_matrix(y_true, y_pred))\ntn, fp, fn, tp = confusion_matrix(y_pred, y_true).ravel()\nprint((tn\/len(y_pred), fp\/len(y_pred), fn\/len(y_pred), tp\/len(y_pred)))\nprint(f1_score(y_true, y_pred, average='macro'))\nprint(mean_absolute_error(y_true, y_pred))\nprint(mean_squared_error(y_true, y_pred))\n\n\n\n","ad6411f6":"df = df.loc[df['output'] == 1]\nprint(df)","8de508a1":"print(df.describe())\n\nfig1, ax1 = plt.subplots()\nax1.set_title('box plot')\nax1.boxplot(df.score.values.tolist())","a3b763e9":"import seaborn as sns\n\nLABELS = [\"Normal\", \"FAKE\"]\nconf_matrix = confusion_matrix(error_df.true_class, y_pred)\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","9ce4396f":"print(tt2)\nimport math\ni = 0\nprint(min(tt))\nmean_value = sum(tt)\/len(tt)\nprint(mean_value)\nwhile i < len(tt):\n    tt[i] = (tt[i]-mean_value)**2\n    i += 1\nvalue_std = math.sqrt(sum(tt)\/len(tt))\nprint(value_std)\n\n#mean: 0.070140356581439\n#std: 0.015058747266534056","b912437c":"t2 = []\nt3 = []\n\ni = 0\nwhile i < len(tt):\n    t2.append(t1[i]-2*value_std)\n    t3.append(t1[i]+2*value_std)\n    i += 1\nprint(t2)\nprint(t3)\nprint(min(t2))","1b8f12e1":"TEST_PATH = '..\/input\/deepfake-detection-challenge\/test_videos\/'\nprint(os.listdir(TEST_PATH))","3e5d5f97":"print(path)","ff663288":"from PIL import Image, ImageEnhance \n\ndef data_preparation(path,filename, meta, train = True):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n    image = get_frame(path+filename)\n    image = white_balance(image)\n    # Display the 1st frame of the video    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.\n    dets = detector(image, 1)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        try:\n            if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) == 1080:\n                roi = image[det.top():det.bottom(),det.left():det.right()]    \n            #image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n                new_image = cv2.Laplacian(roi,cv2.CV_64F)\n                f = np.fft.fft2(new_image)\n                f_shift = np.fft.fftshift(f)\n                f_complex = f_shift\n                f_abs = np.abs(f_complex) + 1 # lie between 1 and 1e6\n                f_bounded = 20 * np.log(f_abs)\n                f_img = 255 * f_bounded \/ np.max(f_bounded)\n                f_img = f_img.astype(np.uint8)\n                #prediction_list.append(str(filename))\n                return(f_img)\n        except:\n            pass\n\n","2c4940fb":"i = 0\npath = '..\/input\/deepfake-detection-challenge\/test_videos\/'\nsubmission_path = os.listdir(path)\nprint(submission_path)\nprediction_list = []\n\nwhile i < len(submission_path):\n    print(submission_path[i])\n    laplacian_answer = data_preparation(path,submission_path[i], meta)\n    try:\n        cv2.imwrite('submission_'+submission_path[i]+'.jpg', laplacian_answer) \n        prediction_list.append('submission_'+submission_path[i]+'.jpg')\n        plt.imshow(laplacian_answer)\n        plt.show()\n    except:\n        pass\n    i += 1\n\nprint(prediction_list)","375a0114":"print(prediction_list)","58c82be9":"y_fake = []\n\n    \nfor pic in prediction_list:\n    print(pic)\n    img = cv2.imread(pic)\n    print(img)\n    img = cv2.resize(img, dsize, Image.ANTIALIAS)\n    y_fake.append(img)\n\nprint(len(y_fake))\nprint(prediction_list)","d9c27f12":"i = 0\n\npredictions = []\nwhile i <len(prediction_list):\n    img = y_fake[i]\n    print(prediction_list[i])\n    img = cv2.imread(prediction_list[i])\n    img = cv2.resize(img, dsize)\n    plt.imshow(img)\n    img_array = np.asarray(img)\n    print(img.size)\n\n    #Display noisy image\n    img2 = img_array.astype(np.uint8)\n    img2 = Image.fromarray(img2, 'RGB')\n    plt.imshow(img2)\n    plt.show()\n    # Present noisy image to auto encoder\n    img_array = img_array.flatten()\n    img_array = img_array.astype(np.float32)\n    img_array = (img_array-128)\/128\n    img_array = np.array([img_array])\n    pred = model.predict(img_array)[0]\n\n    # Display neural result\n    img_array2 = pred.reshape(128,128,3)\n    img_array2 = (img_array2*128)+128\n    img_array2 = img_array2.astype(np.uint8)\n    img2 = Image.fromarray(img_array2, 'RGB')\n\n    plt.imshow(img2)\n    plt.show()\n    \n    score1 = np.sqrt(metrics.mean_squared_error(pred,img_array[0]))\n    print(f\"Out of Sample Score (RMSE): {score1}\")   \n    if score1 > threshold:\n        predictions.append(1)\n    else:\n        predictions.append(0)\n    print('=====================')\n    print('=====================')\n    i += 1","151ae23a":"\n\nsubmission_df = pd.DataFrame({\"filename\": prediction_list, \"label\": predictions})\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\n","1936dda0":"print(predictions)\nprint(submission_df.tail(20))","f571bdb8":"t1 = [0.041331466, 0.024498282, 0.04118593, 0.038716756, 0.037078038, 0.03631261, 0.03412535, 0.04222116, 0.039362844, 0.04294556, 0.0386346, 0.03914516, 0.041025102, 0.036734298, 0.025888393, 0.028660871, 0.041659564, 0.039061926]\nt2 = []\nt3 = []\n\ni = 0\nwhile i < len(t1):\n    t2.append(t1[i]-2*0.005346882783381586)\n    t3.append(t1[i]+2*0.005346882783381586)\n    i += 1\nprint(t2)\nprint(t3)\nprint(max(t3))","8a36aff8":"t1 = [0.058573503, 0.066934064, 0.05464456, 0.07709787, 0.08883689, 0.07328239, 0.09117831, 0.07257624, 0.077166654, 0.059338007, 0.10371624, 0.042389914, 0.05010311, 0.07240847, 0.067032576, 0.062947355, 0.079062, 0.087789275, 0.06871186, 0.0634957, 0.052625958, 0.035812557, 0.078799985, 0.06328735, 0.06950112, 0.064118214, 0.060417272, 0.06591671, 0.09110638, 0.08182621, 0.070093654, 0.052168075, 0.05857365, 0.064667016, 0.075556725, 0.05015098, 0.061487824, 0.071132205, 0.07968406, 0.062037375, 0.068701394, 0.052525327, 0.07065824, 0.083880365, 0.09539992, 0.07136654, 0.08247659, 0.06425655, 0.06758966, 0.07027674, 0.05371886, 0.06454329, 0.038022883, 0.07525901, 0.05537062, 0.07471208, 0.05937753, 0.06530632, 0.09115084, 0.047818627, 0.06841827, 0.064744055, 0.09184977, 0.09172562, 0.07337593, 0.08758027, 0.06695, 0.060223594]\nt2 = []\nt3 = []\n\ni = 0\nwhile i < len(t1):\n    t2.append(t1[i]-2*0.013818416343404723)\n    t3.append(t1[i]+2*0.013818416343404723)\n    i += 1\nprint(t2)\nprint(t3)\nprint(min(t2))\nprint(min(t1))","c0a6b593":"img = y_fake[0]\nprint(test_pictures[0])\nimg = cv2.imread(pic)\nimg = cv2.resize(img, dsize)\nplt.imshow(img)\nimg_array = np.asarray(img)\nprint(img.size)\n\n#Display noisy image\nimg2 = img_array.astype(np.uint8)\nimg2 = Image.fromarray(img2, 'RGB')\nplt.imshow(img2)\nplt.show()\n# Present noisy image to auto encoder\nimg_array = img_array.flatten()\nimg_array = img_array.astype(np.float32)\nimg_array = (img_array-128)\/128\nimg_array = np.array([img_array])\npred = model.predict(img_array)[0]\n\n# Display neural result\nimg_array2 = pred.reshape(128,128,3)\nimg_array2 = (img_array2*128)+128\nimg_array2 = img_array2.astype(np.uint8)\nimg2 = Image.fromarray(img_array2, 'RGB')\n\nplt.imshow(img2)\nplt.show()","95f88fed":"img = y[1]\nprint(test_pictures[1])\nimg = cv2.imread(pic)\nimg = cv2.resize(img, dsize)\nplt.imshow(img)\nimg_array = np.asarray(img)\nprint(img.size)\n\n#Display noisy image\nimg2 = img_array.astype(np.uint8)\nimg2 = Image.fromarray(img2, 'RGB')\nplt.imshow(img2)\nplt.show()\n# Present noisy image to auto encoder\nimg_array = img_array.flatten()\nimg_array = img_array.astype(np.float32)\nimg_array = (img_array-128)\/128\nimg_array = np.array([img_array])\npred = model.predict(img_array)[0]\n\n# Display neural result\nimg_array2 = pred.reshape(128,128,3)\nimg_array2 = (img_array2*128)+128\nimg_array2 = img_array2.astype(np.uint8)\nimg2 = Image.fromarray(img_array2, 'RGB')\n\nplt.imshow(img2)\nplt.show()","1b6c525c":"print(x.shape)\npred = model.predict(x[0])\n\npred = pred.reshape(128,128,3)\npred = (pred*128)+128\npred = pred.astype(np.uint8)\npred = Image.fromarray(pred, 'RGB')\n\n\nplt.imshow(pred)\nplt.show()","39d1c51e":"from PIL import Image\n\ndef data_preparation(i,title,filename, meta, train = True):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n\n    image = get_frame(filename)\n    image = white_balance(image)\n\n    # Display the 1st frame of the video\n    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.\n    dets = detector(image, 1)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) == 1080:\n            roi = image[det.top():det.bottom(),det.left():det.right()]            \n            #image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n            new_image = cv2.Laplacian(roi,cv2.CV_64F)\n            f = np.fft.fft2(new_image)\n            f_shift = np.fft.fftshift(f)\n            f_complex = f_shift\n            f_abs = np.abs(f_complex) + 1 # lie between 1 and 1e6\n            f_bounded = 20 * np.log(f_abs)\n            f_img = 255 * f_bounded \/ np.max(f_bounded)\n            f_img = f_img.astype(np.uint8)\n            w, h = 185, 186\n            img = Image.fromarray(f_img, 'RGB')\n            img.save(title+str(i)+'.png')\n            img.show()\n        else:\n            pass","31f07959":"from PIL import Image\n\ndef training_data_preparation(filename):\n    '''\n    Helper function to visualize the 1st frame of the video by filename and metadata\n    INPUT:\n        filename - video filename\n        meta - dataframe containing metadata.json\n        train - indicates that the video is among train samples and the label can be retrived from metadata\n    '''\n    # get the 1st frame of the video\n\n    image = cv2.imread(filename,1)\n    image = white_balance(image)\n\n    # Display the 1st frame of the video\n    \n    # Extract the face with haar cascades\n#    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    dets = detector(image)\n    # The 1 in the second argument indicates that we should upsample the image\n    # 1 time.  This will make everything bigger and allow us to detect more\n    # faces.\n    dets = detector(image, 1)\n    for det in dets:\n        cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)\n        if len(cv2.rectangle(image,(det.left(), det.top()), (det.right(), det.bottom()), color_green, line_width)) == 1080:\n            roi = image[det.top():det.bottom(),det.left():det.right()]            \n            #image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n            new_image = cv2.Laplacian(roi,cv2.CV_64F)\n            f = np.fft.fft2(new_image)\n            f_shift = np.fft.fftshift(f)\n            f_complex = f_shift\n            f_abs = np.abs(f_complex) + 1 # lie between 1 and 1e6\n            f_bounded = 20 * np.log(f_abs)\n            f_img = 255 * f_bounded \/ np.max(f_bounded)\n            f_img = f_img.astype(np.uint8)\n            w, h = 185, 186\n            img = Image.fromarray(f_img, 'RGB')\n            img.save(filename)\n            img.show()\n        else:\n            pass","9ae4e41a":"#preparing the training and testing data\n\nreal_train_sample_video = list(meta.loc[meta.label=='REAL'].index)\nprint(len(real_train_sample_video))\nfake_train_sample_video = list(meta.loc[meta.label=='FAKE'].index)\nprint(len(fake_train_sample_video))","38b16f97":"from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model,Sequential\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adadelta, RMSprop,SGD,Adam\nfrom keras import regularizers\nfrom keras import backend as K\nimport numpy as np\nimport scipy.misc\nimport numpy.random as rng\nimport math\n\nimport cv2\nimport PIL\nimport os\nfrom pathlib import Path\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread, imshow, imsave\nfrom keras.preprocessing.image import load_img, array_to_img, img_to_array\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Input\nfrom keras.optimizers import SGD, Adam, Adadelta, Adagrad\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(111)\nfrom subprocess import check_output\n\nimport cv2\nimport keras\nimport PIL\nimport os\nfrom pathlib import Path\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread, imshow, imsave\nfrom keras.preprocessing.image import load_img, array_to_img, img_to_array\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Input\nfrom keras.optimizers import SGD, Adam, Adadelta, Adagrad\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(111)\nfrom subprocess import check_output\nimport numpy as np\nimport math\nimport tensorflow as tf\nimport nibabel as nib\nimport numpy as np\nfrom keras.layers import Input,Dense,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model,Sequential\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\nimport scipy.misc\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom keras.models import model_from_json\n\nimport numpy as np\nimport math\nimport tensorflow as tf\nimport nibabel as nib\nimport numpy as np\nfrom keras.layers import Input,Dense,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model,Sequential\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\nimport scipy.misc\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom keras.models import model_from_json\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","5cd3b588":"#preparing the training and testing data\n\n\nprint(meta.loc[meta.label=='REAL'].describe)\ni = 0\nwhile i < len(real_train_sample_video):\n    vidcap = cv2.VideoCapture('..\/input\/deepfake-detection-challenge\/train_sample_videos\/'+str(real_train_sample_video[i]))\n    success,image = vidcap.read()\n    count = 0 \n    while success:\n        cv2.imwrite('train_real_'+str(real_train_sample_video[i])+\"_%d.jpg\" % count, image)     # save frame as JPEG file      \n        success,image = vidcap.read()\n        count += 1\n    i += 1\n","b478850d":"print(os.listdir('.'))","3d719dcc":"filename = os.listdir()\nprint(len(filename))\ni = 0\ndefault = []\nwhile i < len(train_sample_video):\n    if filename[i][0:5] == 'train':\n        try:\n            print(filename[i])\n            training_data_preparation(filename[i])\n        except:\n            print(filename[i])\n            default.append(filename[i])\n            pass\n    i += 1\nprint(len(default))","77c82387":"print('train real')\nimg = plt.imread('train_real_cppdvdejkc.mp4_136.jpg')\nimgplot = plt.imshow(img)\nplt.show()\n","6d050ddf":"#preparing the training and testing data\n\n\nprint(meta.loc[meta.label=='FAKE'].describe)\ntrain_sample_video = list(meta.loc[meta.label=='FAKE'].index)\n\ni = 0\nwhile i < len(train_sample_video[0:5]):\n    vidcap = cv2.VideoCapture('..\/input\/deepfake-detection-challenge\/train_sample_videos\/'+str(train_sample_video[i]))\n    success,image = vidcap.read()\n    count = 0 \n    while success:\n        cv2.imwrite('test_fake_'+str(train_sample_video[i])+\"_%d.jpg\" % count, image)     # save frame as JPEG file      \n        success,image = vidcap.read()\n        count += 1\n    i += 1","5c108165":"filename = os.listdir()\nprint(len(filename))\ni = 0\ndefault = []\nwhile i < len(train_sample_video):\n    if filename[i][0:4] == 'test':\n        try:\n            print(filename[i])\n            training_data_preparation(filename[i])\n        except:\n            print(filename[i])\n            default.append(filename[i])\n            pass\n    i += 1\nprint(len(default))","3b821c7c":"print('test fake')\nimg = plt.imread('test_fake_agrmhtjdlk.mp4_213.jpg')\nimgplot = plt.imshow(img)\nplt.show()","a83f7820":"#preparing the training dataset\nfilename = os.listdir()\nprint(len(filename))\ni = 0\nwhile i < len(train_sample_video):\n    print(filename[i])\n    if filename[i][0:5] == 'train':\n        try:\n            print('')\n            #training_data_preparation(filename[i])\n        except:\n            print(filename[i])\n            pass\n    i += 1\n#preparing the testing dataset\ni = 0\nwhile i <  len(fake_test_sample_video):\n    data_preparation(i,'test_real_','..\/input\/deepfake-detection-challenge\/train_sample_videos\/'+str(fake_test_sample_video[i]), meta)    \n    i += 1\n\nfake_train_sample_video = list(meta.loc[meta.label=='FAKE'].index)[0:20]\ni = 0\n#preparing the training dataset\nwhile i < 20:#len(fake_train_sample_video):\n    data_preparation(i,'test_fake_','..\/input\/deepfake-detection-challenge\/train_sample_videos\/'+str(fake_train_sample_video[i]), meta)\n    i += 1\n","182a1f33":"print(os.listdir())","0baf4462":"import os\ndata = os.listdir()\ntrain_pictures = []\ntest_pictures = []\nlabels = []\ndata_type = []\n\ni = 0\nwhile i < len(data):\n    if data[i][0:5] == 'train':\n        train_pictures.append(data[i])\n        labels.append(0)\n        data_type.append('TRAIN')\n    elif data[i][0:4] == 'test':\n        test_pictures.append(data[i])\n        if data[i][5:9] == 'fake':\n            labels.append(0)\n            data_type.append('TEST')\n        elif data[i][5:9] == 'real':\n            labels.append(1)\n            data_type.append('TEST')\n    else:\n        pass\n    i += 1\n\npictures = train_pictures + test_pictures","046ba030":"input_img = Input(shape=(192,192,3))","1ac1df45":"from pathlib import Path\ntrain_images = train_pictures\n\nX = []\nY = []\n\ni = 0\nfor img in train_images:\n    try:\n        img = load_img(img, grayscale=False,target_size=(192, 192, 3))\n        img = img_to_array(img).astype('float32')\/255.\n        X.append(img)\n    except:\n        i += 1\n        pass\nprint(i)\n\nfor img in train_images:\n    try:\n        img = cv2.imread(img,1)\n        img = cv2.resize(img,((192, 192)))\n        img = img_to_array(img).astype('float32')\/255.\n        Y.append(img)\n    except:\n        pass\nX = np.array(X[0:1000])\nY = np.array(Y[0:1000])\n\nprint(\"Size of X : \", X.shape)\n#print(\"Size of Y : \", Y.shape)  ","18a92bf7":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=111)\nx_train = X_train\nx_test = X_test","d7c6c4ba":"# import libraries\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.models import Model\nimport matplotlib.pyplot as plt\nfrom keras.models import load_model\n\n# define input shape\ninput_img = Input(shape=(192, 192, 3))\n\n# encoding dimension\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n# decoding dimension\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((4, 4))(x)\ndecoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n# build model\nautoencoder = Model(input_img, decoded)\n\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\n\n\n#autoencoder = Model(input_img, decoded)\n#utoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')","600028e5":"autoencoder.fit(x_train, x_train,\n                epochs=100,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(x_test, x_test))","42ab008c":"decoded_imgs = autoencoder.predict(x_test)","d6ee4348":"n = 4\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i + 1].reshape(192, 192, 3))\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i + 1].reshape(192, 192,3))\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","b5110d10":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\nimport numpy as np\nfrom pylab import rcParams\n\nimport tensorflow as tf\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.metrics import recall_score, classification_report, auc, roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score","f8d17909":"nb_epoch = 10\nbatch_size = 320\n\n#autoencoder = build_autoencoder()\n\n\nautoencoder.compile(optimizer='sgd', \n                    loss='mean_squared_error', \n                    metrics=['accuracy'])\n\n\n\nhistory = autoencoder.fit(X_train, X_train,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(X_test, X_test),\n                    verbose=1).history","f43712f9":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\ninput_img = Input(shape=(784,))\ninput_img2 = Input(shape=(32,))\nX_train = X_train.reshape(len(X_train), np.prod(X_train.shape[1:]))\nX_test = X_test.reshape(len(X_test), np.prod(X_test.shape[1:]))","678f0193":"encoded = Dense(units=128, activation='relu')(input_img)\nencoded = Dense(units=64, activation='relu')(encoded)\nencoded = Dense(units=32, activation='relu')(encoded)\n\ndecoded = Dense(units=64, activation='relu')(encoded)\ndecoded = Dense(units=128, activation='relu')(decoded)\ndecoded = Dense(units=784, activation='sigmoid')(decoded)\nautoencoder = Model(input_img, decoded)\nautoencoder.summary()","99df2222":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\nautoencoder.fit(X_train, X_train,\n                epochs=50,\n                batch_size=512,\n                shuffle=True,\n                validation_data=(X_test, X_test))","c06389ca":"plt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right');\nplt.show()","cb0e50cd":"predictions = autoencoder.predict(x_test)","eae6599c":"print(predictions)","1b45d5a8":"print(predictions[0])","b40d84e7":"%matplotlib inline \nfrom matplotlib import pyplot as plt\n\n\n\na = np.expand_dims(predictions[0], axis=0)  # or axis=1\nplt.imshow(a)\nplt.show()","efc721f0":"plt.imshow(predictions, interpolation='nearest')\nplt.show()","2b0c22cf":"Y_test = df.loc[df['target_class'] == 0]\ny_test = Y_test['target_class']\n\npredictions = autoencoder.predict(X_test)\nmse = np.mean(np.power(X_test - predictions, 2), axis=1)\nprint(mse[25])\n\nerror_df = pd.DataFrame({'reconstruction_error': mse, 'true_class': y_test})\n\nprint(error_df.describe())","2828c9a8":"Y_test = df.loc[df['target_class'] == 1]\ny_test = Y_test['target_class']\n\npredictions = autoencoder.predict(X_test)\nmse = np.mean(np.power(X_test - predictions, 2), axis=1)\nprint(mse[0])","110c24c1":"nb_epoch = 1000\nbatch_size = 320\n\nautoencoder.compile(optimizer='sgd', \n                    loss='mean_squared_error', \n                    metrics=['accuracy'])\n\ncheckpointer = ModelCheckpoint(filepath=\"model.h5\",\n                               verbose=0,\n                               save_best_only=True)\ntensorboard = TensorBoard(log_dir='.\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\n\nhistory = autoencoder.fit(x_train, x_train,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(x_test, x_test),\n                    verbose=1,\n                    callbacks=[checkpointer, tensorboard]).history","40374eb3":"autoencoder = build_autoencoder()\n\nautoencoder.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\n\n\ntensorboard = TensorBoard(log_dir='.\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\n\nautoencoder.summary()\n\n\nautoencoder = autoencoder.fit(x_train, x_train,\n                    epochs=10,\n                    batch_size=64,\n                    shuffle=True,\n                    validation_data=(x_test, x_test),\n                    verbose=1)","6629448c":"plt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right');\nplt.show()","10af11be":"predictions = autoencoder.predict(X_test)\nprint(type(predictions))\n\n","bd5632ae":"def visualize(img,encoder,decoder):\n    \"\"\"Draws original, encoded and decoded images\"\"\"\n    # img[None] will have shape of (1, 32, 32, 3) which is the same as the model input\n    code = encoder.predict(img[None])[0]\n    reco = decoder.predict(code[None])[0]\n\n    plt.subplot(1,3,1)\n    plt.title(\"Original\")\n    show_image(img)\n\n    plt.subplot(1,3,2)\n    plt.title(\"Code\")\n    plt.imshow(code.reshape([code.shape[-1]\/\/2,-1]))\n\n    plt.subplot(1,3,3)\n    plt.title(\"Reconstructed\")\n    show_image(reco)\n    plt.show()\n\nfor i in range(5):\n    img = X_test[i]\n    visualize(img,encoder,decoder)","0ac950ed":"**Partial Conclusion**\n* The dataset seems very unbalanced. \n* A lot of data are missing, especially in the REAL class.\n* I have been using some haarcascade for the faces extraction. It is not very efficient on\n    * Darker skins\n    * Non frontal pictures    \n","95c6b46b":"**Way Forward**\n* Use dlib for better face extraction\n* Find a way to compensate for missing data\n* Explore features extraction\/analysis for modelisation","eceae42f":"**Next steps**\nIt seems that the combination of the laplacian to highlight the noise due to deepfake and the spectral filtering could give some interesting features. BUt as the dataset is very unbalanced, it could be an issue. Therefore, I'll use an autoencoder on the REAL data. If the reconstruction error is high, then it is a real image","65787b7f":"**Mention**\nPart of the code was originally developped by:\n* https:\/\/www.kaggle.com\/aleksandradeis\/deepfake-challenge-eda\/data\n* https:\/\/www.kaggle.com\/gpreda\/deepfake-starter-kit","876c888c":"Now that the differennt functions are ready, I will try to extract some features in the FAKE data"}}