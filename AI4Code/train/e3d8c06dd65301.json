{"cell_type":{"a750d4e0":"code","15880a34":"code","8cf68e1e":"code","1b0c1712":"code","6b2c28af":"code","38a7c48a":"code","2daf9967":"code","67a8a52a":"code","d9bfbbd2":"code","883e7254":"code","e36ee976":"code","d18d98f9":"code","65a76036":"code","9fed05ab":"code","457fa252":"code","3a209c56":"code","92c974be":"code","e0afd6d0":"code","c91793e2":"code","d2f5f0f7":"code","9d363040":"code","6b45b539":"code","6bc9fffb":"code","d78c513d":"code","93d661df":"code","49a7a960":"code","c9a5ca8c":"code","a6ac3c8e":"code","3bf52478":"code","b8016d4e":"code","ef66b80d":"code","1285eeab":"code","101c268a":"code","de4945c5":"code","4471b6ea":"code","6ef4240e":"code","a210da3e":"code","fec83ae0":"code","7d4f6ec0":"code","d3659061":"code","967a1707":"code","af3c0677":"code","c4698a02":"code","4c30a5dd":"code","c1538ad0":"code","61b6daa5":"code","23ae9cb1":"code","ca255507":"code","683eea48":"code","6d8623f6":"code","cb6e3811":"code","6e3b1cb9":"code","4b6f9898":"code","9a77a721":"code","dfb22dbd":"code","bbe6360c":"code","486ae007":"code","c89078cc":"code","d6a334a0":"code","b1d748fa":"code","360e5e26":"code","b63b1e58":"code","a93b4bcf":"code","7dbac9dd":"code","3b1df021":"code","ddbacd8d":"code","cea32d84":"code","98080ba3":"code","f0d6f61e":"code","2fd6c39c":"code","af91c05c":"code","3abc9803":"code","67ef3397":"code","04b8fe18":"code","6fc2b9b6":"code","a6a02b9d":"code","d38c979d":"code","017782ee":"code","01a51f14":"code","dd7470f3":"code","679da038":"code","0035bc73":"code","ddd05857":"code","a86f6598":"code","39f43607":"code","516e83fc":"code","007e7773":"markdown","e92458bc":"markdown","2ecf5050":"markdown","da4955ca":"markdown","9c360af6":"markdown","8f1e2621":"markdown","c0507ae1":"markdown","a082b24d":"markdown","70e0607f":"markdown","a1ddc916":"markdown","d6354a3e":"markdown","fbe5ff04":"markdown","ed798817":"markdown","36eb43f3":"markdown","d1ecf853":"markdown","936f3c7e":"markdown","efee32f2":"markdown","e57c869e":"markdown","d7b79ef8":"markdown","4086a37d":"markdown","955cbad3":"markdown","3d17d3d7":"markdown","396b055d":"markdown","e9101927":"markdown","d6cba14c":"markdown","aa8f4ff6":"markdown","46b2fe5f":"markdown","e4160e40":"markdown","2b0e4370":"markdown","29008318":"markdown","290c6b82":"markdown","6fd9fae9":"markdown","6e4be8ae":"markdown"},"source":{"a750d4e0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fbprophet import Prophet\nimport matplotlib.pyplot as plt\nimport math as math\n\n%matplotlib inline","15880a34":"# Load the data\ntrain = pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip\", compression=\"zip\")\nkeys = pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/key_1.csv.zip\", compression=\"zip\")\nss = pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/sample_submission_1.csv.zip\", compression=\"zip\")","8cf68e1e":"train.head()","1b0c1712":"# Check the data\nprint(\"Check the number of records\")\nprint(\"Number of records: \", train.shape[0], \"\\n\")\n\nprint(\"Null analysis\")\nempty_sample = train[train.isnull().any(axis=1)]\nprint(\"Number of records contain 1+ null: \", empty_sample.shape[0], \"\\n\")","6b2c28af":"empty_sample.iloc[np.r_[0:10, len(empty_sample)-10:len(empty_sample)]]","38a7c48a":"# plot 3 the time series\ndef plot_time_series(df, row_num, start_col =1, ax=None):\n    if ax is None:\n            fig = plt.figure(facecolor='w', figsize=(10, 6))\n            ax = fig.add_subplot(111)\n    else:\n        fig = ax.get_figure()\n        \n    series_title = df.iloc[row_num, 0]\n    sample_series = df.iloc[row_num, start_col:]\n    sample_series.plot(style=\".\", ax=ax)\n    ax.set_title(\"Series: %s\" % series_title)\n\nfig, axs  = plt.subplots(4,1,figsize=(12,12))\nplot_time_series(empty_sample, 1, ax=axs[0])\nplot_time_series(empty_sample, 10, ax=axs[1])\nplot_time_series(empty_sample, 100, ax=axs[2])\nplot_time_series(empty_sample, 1005, ax=axs[3])\n\nplt.tight_layout()","2daf9967":"# series with all NaN\nempty_sample.iloc[1000:1010]","67a8a52a":"import re\n\ndef breakdown_topic(str):\n    m = re.search('(.*)\\_(.*).wikipedia.org\\_(.*)\\_(.*)', str)\n    if m is not None:\n        return m.group(1), m.group(2), m.group(3), m.group(4)\n    else:\n        return \"\", \"\", \"\", \"\"\n\nprint(breakdown_topic(\"\u0420\u0443\u0434\u043e\u0432\u0430,_\u041d\u0430\u0442\u0430\u043b\u044c\u044f_\u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440\u043e\u0432\u043d\u0430_ru.wikipedia.org_all-access_spider\"))\nprint(breakdown_topic(\"\u53f0\u7063\u707d\u96e3\u5217\u8868_zh.wikipedia.org_all-access_spider\"))\nprint(breakdown_topic(\"File:Memphis_Blues_Tour_2010.jpg_commons.wikimedia.org_mobile-web_all-agents\"))","d9bfbbd2":"page_details = train.Page.str.extract(r'(?P<topic>.*)\\_(?P<lang>.*).wikipedia.org\\_(?P<access>.*)\\_(?P<type>.*)')\n\npage_details[0:10]","883e7254":"unique_topic = page_details[\"topic\"].unique()\nprint(unique_topic)\nprint(\"Number of distinct topics: \", unique_topic.shape[0])","e36ee976":"fig, axs  = plt.subplots(3,1,figsize=(12,12))\n\npage_details[\"lang\"].value_counts().sort_index().plot.bar(ax=axs[0])\naxs[0].set_title('Language - distribution')\n\npage_details[\"access\"].value_counts().sort_index().plot.bar(ax=axs[1])\naxs[1].set_title('Access - distribution')\n\npage_details[\"type\"].value_counts().sort_index().plot.bar(ax=axs[2])\naxs[2].set_title('Type - distribution')\n\nplt.tight_layout()","d18d98f9":"# Generate train and validate dataset\ntrain_df = pd.concat([page_details, train], axis=1)\n\ndef get_train_validate_set(train_df, test_percent):\n    train_end = math.floor((train_df.shape[1]-5) * (1-test_percent))\n    train_ds = train_df.iloc[:, np.r_[0,1,2,3,4,5:train_end]]\n    test_ds = train_df.iloc[:, np.r_[0,1,2,3,4,train_end:train_df.shape[1]]]\n    \n    return train_ds, test_ds\n\nX_train, y_train = get_train_validate_set(train_df, 0.1)\n\nprint(\"The training set sample:\")\nprint(X_train[0:10])\nprint(\"The validation set sample:\")\nprint(y_train[0:10])","65a76036":"def extract_series(df, row_num, start_idx):\n    y = df.iloc[row_num, start_idx:]\n    df = pd.DataFrame({ 'ds': y.index, 'y': y.values})\n    return df","9fed05ab":"def smape(predict, actual, debug=False):\n    '''\n    predict and actual is a panda series.\n    In this implementation I will skip all the datapoint with actual is null\n    '''\n    actual = actual.fillna(0)\n    data = pd.concat([predict, actual], axis=1, keys=['predict', 'actual'])\n    data = data[data.actual.notnull()]\n    if debug:\n        print('debug', data)\n    \n    evals = abs(data.predict - data.actual) * 1.0 \/ (abs(data.predict) + abs(data.actual)) * 2\n    evals[evals.isnull()] = 0\n    #print(np.sum(evals), len(data), np.sum(evals) * 1.0 \/ len(data))\n    \n    result = np.sum(evals) \/ len(data) * 100.0\n    \n    return result\n\n# create testing series\ntesting_series_1 = X_train.iloc[0, 5:494]\ntesting_series_2 = X_train.iloc[0, 5:494].shift(-1)\ntesting_series_3 = X_train.iloc[1, 5:494]\ntesting_series_4 = pd.Series([0,0,0,0])","457fa252":"testing_series_1","3a209c56":"np.repeat(3, 500)","92c974be":"random_series_1 = pd.Series(np.repeat(3, 500))\nrandom_series_2 = pd.Series(np.random.normal(3, 1, 500))\nrandom_series_3 = pd.Series(np.random.normal(500, 20, 500))\nrandom_series_4 = pd.Series(np.repeat(500, 500))\n\n# testing 1 same series\nprint(\"\\nSMAPE score to predict a constant array of 3\")\nprint(\"Score (same series): %.3f\" % smape(random_series_1, random_series_1))\nprint(\"Score (same series - 1) %.3f\" % smape(random_series_1, random_series_1-1))\nprint(\"Score (same series + 1) %.3f\" % smape(random_series_1, random_series_1+1))\n\n# testing 2 same series shift by one\nprint(\"\\nSMAPE score to predict a array of normal distribution around 3\")\nprint(\"Score (random vs mean) %.3f\" % smape(random_series_2, random_series_1))\nprint(\"Score (random vs mean-1) %.3f\" % smape(random_series_2, random_series_2-1))\nprint(\"Score (random vs mean+1) %.3f\" % smape(random_series_2, random_series_2+1))\nprint(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_2, random_series_2*0.9))\nprint(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_2, random_series_2*1.1))\n\n# testing 3 totally different series\nprint(\"\\nSMAPE score to predict a array of normal distribution around 500\")\nprint(\"Score (random vs mean) %.3f\" % smape(random_series_3, random_series_4))\nprint(\"Score (random vs mean-20) %.3f\" % smape(random_series_3, random_series_3-20))\nprint(\"Score (random vs mean+20) %.3f\" % smape(random_series_3, random_series_3+20))\nprint(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_3, random_series_3*0.9))\nprint(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_3, random_series_3*1.1))","e0afd6d0":"y_true_1 = pd.Series(np.random.normal(1, 1, 500))\ny_true_2 = pd.Series(np.random.normal(2, 1, 500))\ny_true_3 = pd.Series(np.random.normal(3, 1, 500))\ny_pred = pd.Series(np.ones(500))\nx = np.linspace(0,10,1000)\nres_1 = list([smape(y_true_1, i * y_pred) for i in x])\nres_2 = list([smape(y_true_2, i * y_pred) for i in x])\nres_3 = list([smape(y_true_3, i * y_pred) for i in x])\nplt.plot(x, res_1, color='b')\nplt.plot(x, res_2, color='r')\nplt.plot(x, res_3, color='g')\nplt.axvline(x=1, color='k')\nplt.axvline(x=2, color='k')\nplt.axvline(x=3, color='k')","c91793e2":"def plot_prediction_and_actual_2(train, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n    fig, ax  = plt.subplots(1,1,figsize=figSize)\n    ax.plot(pd.to_datetime(train.index), train.values, 'k.')\n    ax.plot(pd.to_datetime(actual.index), actual.values, 'r.')\n    ax.plot(pd.to_datetime(forecast.index), forecast.values, 'b-')\n    ax.set_title(title)\n    plt.show()","d2f5f0f7":"def median_model(df_train, df_actual, p, review=False, figSize=(12, 4)):\n    \n    def nanmedian_zero(a):\n        return np.nan_to_num(np.nanmedian(a))\n    \n    df_train['y'] = df_train['y'].astype('float')\n    df_actual['y'] = df_actual['y'].astype('float')\n    visits = nanmedian_zero(df_train['y'].values[-p:])\n    train_series = df_train['y']\n    train_series.index = df_train.ds\n\n    idx = np.arange(p) + np.arange(len(df_train)- p+1)[:,None]\n    b = [row[row>=0] for row in df_train.y.values[idx]]\n    pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\n    pre_forecast.index = df_train.ds\n    \n    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n    forecast_series.index = df_actual.ds\n    \n    forecast_series = pre_forecast.append(forecast_series)\n    \n    actual_series = df_actual.y\n    actual_series.index = df_actual.ds\n    \n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='Median model')\n    \n    return smape(forecast_series, actual_series)","9d363040":"df_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)","6b45b539":"df_train","6bc9fffb":"def nanmedian_zero(a):\n    return np.nan_to_num(np.nanmedian(a))","d78c513d":"p = 15","93d661df":"df_train['y'] = df_train['y'].astype('int')\ndf_actual['y'] = df_actual['y'].astype('int')\nvisits = nanmedian_zero(df_train['y'].values[-p:])\ntrain_series = df_train['y']\ntrain_series.index = df_train.ds","49a7a960":"idx = np.arange(p) + np.arange(len(df_train)- p+1).reshape(-1, 1)","c9a5ca8c":"np.arange(p) + np.arange(len(df_train)- p+1).reshape(-1, 1)","a6ac3c8e":"b = [row[row>=0] for row in df_train.y.values[idx]]","3bf52478":"df_train.y.values[idx]","b8016d4e":"pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))","ef66b80d":"pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\npre_forecast.index = df_train.ds","1285eeab":"pre_forecast","101c268a":"forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\nforecast_series.index = df_actual.ds\n\nforecast_series = pre_forecast.append(forecast_series)","de4945c5":"forecast_series","4471b6ea":"actual_series = df_actual.y\nactual_series.index = df_actual.ds","6ef4240e":"actual_series","a210da3e":"# This is to demo the median model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = median_model(df_train.copy(), df_actual.copy(), 15, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","fec83ae0":"# holiday variable\n#holiday_en = ['2015-01-01', '2015-01-19', '2015-04-03', '2015-05-04', '2015-05-25', '2015-07-01', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-01-18', '2016-03-25', '2016-05-02', '2016-05-30', '2016-07-01', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-16', '2017-04-14', '2017-05-01', '2017-05-29', '2017-07-01', '2017-07-03', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25', '2017-12-26']\n\nholiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25']\nholiday_en_uk = ['2015-01-01', '2015-04-03', '2015-05-04', '2015-05-25', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-03-25', '2016-05-02', '2016-05-30', '2016-12-26', '2016-12-27', '2017-01-01', '2017-04-14', '2017-05-01', '2017-05-29', '2017-12-25', '2017-12-26']\nholiday_en_canada = ['2015-01-01', '2015-07-01', '2015-09-07', '2015-12-25', '2016-01-01', '2016-07-01', '2016-09-05', '2016-12-25', '2017-01-01', '2017-07-01', '2017-07-03', '2017-09-04', '2017-12-25']\n\nholiday_ru_russia = ['2015-01-01', '2015-01-02', '2015-01-05', '2015-01-06', '2015-01-07', '2015-01-08', '2015-01-09', '2015-02-23', '2015-03-09', '2015-05-01', '2015-05-04', '2015-05-09', '2015-05-11', '2015-06-12', '2015-11-04', '2016-01-01', '2016-01-04', '2016-01-05', '2016-01-06', '2016-01-07', '2016-02-22', '2016-02-23', '2016-03-08', '2016-05-01', '2016-05-09', '2016-06-12', '2016-06-13', '2016-11-04', '2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-02-23', '2017-02-24', '2017-03-08', '2017-05-01', '2017-05-08', '2017-05-09', '2017-06-12', '2017-11-04', '2017-11-06']\n#holiday_es = ['2015-01-01', '2015-01-06', '2015-01-12', '2015-02-02', '2015-03-16', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-09-16', '2015-10-12', '2015-11-01', '2015-11-02', '2015-11-16', '2015-12-06', '2015-12-08', '2015-12-12', '2015-12-25', '2016-01-01', '2016-01-06', '2016-01-11', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-09-16', '2016-10-12', '2016-10-17', '2016-11-01', '2016-11-02', '2016-11-07', '2016-11-14', '2016-11-21', '2016-12-06', '2016-12-08', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-06', '2017-01-09', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-09-16', '2017-10-12', '2017-10-16', '2017-11-01', '2017-11-02', '2017-11-06', '2017-11-13', '2017-11-20', '2017-12-06', '2017-12-08', '2017-12-12', '2017-12-25']\n\nholiday_es_mexico = ['2015-01-01', '2015-02-02', '2015-03-16', '2015-04-02', '2015-04-03', '2015-05-01', '2015-09-16', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-12', '2015-12-25', '2016-01-01', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-09-16', '2016-10-12', '2016-11-02', '2016-11-21', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-09-16', '2017-10-12', '2017-11-02', '2017-11-20', '2017-12-12', '2017-12-25']\nholiday_es_spain = ['2017-01-01', '2017-01-06', '2017-04-14', '2017-05-01', '2017-08-15', '2017-10-12', '2017-11-01', '2017-12-06', '2017-12-08', '2017-12-25', '2016-01-01', '2016-01-06', '2016-03-25', '2016-05-01', '2016-08-15', '2016-10-12', '2016-11-01', '2016-12-06', '2016-12-08', '2016-12-25', '2015-01-01', '2015-01-06', '2015-04-03', '2015-05-01', '2015-10-12', '2015-11-01', '2015-12-06', '2015-12-08', '2015-12-25']\nholiday_es_colombia = ['2015-01-01', '2015-01-12', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-08', '2015-12-25', '2016-01-01', '2016-01-11', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-10-17', '2016-11-07', '2016-11-14', '2016-12-08', '2016-12-25', '2017-01-01', '2017-01-09', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-10-16', '2017-11-06', '2017-11-13', '2017-12-08', '2017-12-25']\n\nholiday_fr_france = ['2015-01-01', '2015-04-06', '2015-05-01', '2015-05-08', '2015-05-14', '2015-05-25', '2015-07-14', '2015-08-15', '2015-11-01', '2015-11-11', '2015-12-25', '2016-01-01', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-08', '2016-05-16', '2016-07-14', '2016-08-15', '2016-11-01', '2016-11-11', '2016-12-25', '2017-01-01', '2017-04-17', '2017-05-01', '2017-05-08', '2017-05-25', '2017-06-05', '2017-07-14', '2017-08-15', '2017-11-01', '2017-11-11', '2017-12-25']\nholiday_jp_japan = ['2015-01-01', '2015-01-12', '2015-02-11', '2015-03-21', '2015-04-29', '2015-05-03', '2015-05-04', '2015-05-05', '2015-05-06', '2015-07-20', '2015-09-21', '2015-09-22', '2015-09-23', '2015-10-12', '2015-11-03', '2015-11-23', '2015-12-23', '2016-01-01', '2016-01-11', '2016-02-11', '2016-03-21', '2016-04-29', '2016-05-03', '2016-05-04', '2016-05-05', '2016-07-18', '2016-08-11', '2016-09-19', '2016-09-22', '2016-10-10', '2016-11-03', '2016-11-23', '2016-12-23', '2017-01-01', '2017-01-09', '2017-02-11', '2017-03-20', '2017-04-29', '2017-05-03', '2017-05-04', '2017-05-05', '2017-07-17', '2017-08-11', '2017-09-18', '2017-09-22', '2017-10-09', '2017-11-03', '2017-11-23', '2017-12-23']\n\n#holiday_de = ['2015-01-01', '2015-01-06', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-01', '2015-08-15', '2015-10-03', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-01', '2016-08-15', '2016-10-03', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-01', '2017-08-15', '2017-10-03', '2017-10-26', '2017-10-31', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\n\nholiday_de_germany = ['2015-01-01', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-14', '2015-05-25', '2015-10-03', '2015-12-25', '2015-12-26', '2016-01-01', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-10-03', '2016-12-25', '2016-12-26', '2017-01-01', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-10-03', '2017-10-31', '2017-12-25', '2017-12-26']\nholiday_de_austria = ['2015-01-01', '2015-01-06', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-15', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-15', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-15', '2017-10-26', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\nholiday_de_switzerland = ['2015-01-01', '2015-04-03', '2015-05-14', '2015-08-01', '2015-12-25', '2016-01-01', '2016-03-25', '2016-05-05', '2016-08-01', '2016-12-25', '2017-01-01', '2017-04-14', '2017-05-25', '2017-08-01', '2017-12-25']\n\n#holiday_zh = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-19', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-09', '2015-10-10', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-04-05', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-06-10', '2016-07-01', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-29', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-04', '2017-10-05', '2017-10-09', '2017-10-10', '2017-10-28', '2017-12-25', '2017-12-26']\n\nholiday_zh_hongkong = ['2015-01-01', '2015-02-19', '2015-02-20', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-08', '2016-02-09', '2016-02-10', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-07-01', '2016-09-16', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-28', '2017-01-30', '2017-01-31', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-05', '2017-10-28', '2017-12-25', '2017-12-26']\nholiday_zh_taiwan = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-05', '2015-04-06', '2015-06-19', '2015-06-20', '2015-09-28', '2015-10-09', '2015-10-10', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-04-04', '2016-04-05', '2016-06-09', '2016-06-10', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-10', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-05-01', '2017-05-29', '2017-05-30', '2017-10-04', '2017-10-09', '2017-10-10']\n\nholidays_en_us = pd.DataFrame({\n  'holiday': 'US public holiday',\n  'ds': pd.to_datetime(holiday_en_us),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en_uk = pd.DataFrame({\n  'holiday': 'UK public holiday',\n  'ds': pd.to_datetime(holiday_en_uk),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en_canada = pd.DataFrame({\n  'holiday': 'Canada public holiday',\n  'ds': pd.to_datetime(holiday_en_canada),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en = pd.concat((holidays_en_us, holidays_en_uk, holidays_en_canada))\n\nholidays_ru_russia = pd.DataFrame({\n  'holiday': 'Russia public holiday',\n  'ds': pd.to_datetime(holiday_ru_russia),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_ru = holidays_ru_russia\n\nholidays_es_mexico = pd.DataFrame({\n  'holiday': 'Mexico public holiday',\n  'ds': pd.to_datetime(holiday_es_mexico),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es_spain = pd.DataFrame({\n  'holiday': 'Spain public holiday',\n  'ds': pd.to_datetime(holiday_es_spain),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es_colombia = pd.DataFrame({\n  'holiday': 'Colombia public holiday',\n  'ds': pd.to_datetime(holiday_es_colombia),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es = pd.concat((holidays_es_mexico, holidays_es_spain, holidays_es_colombia))\n\nholidays_fr_france = pd.DataFrame({\n  'holiday': 'France public holiday',\n  'ds': pd.to_datetime(holiday_fr_france),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_fr = holidays_fr_france\n\nholidays_jp_japan = pd.DataFrame({\n  'holiday': 'Japan public holiday',\n  'ds': pd.to_datetime(holiday_jp_japan),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_jp = holidays_jp_japan\n\nholidays_de_germany = pd.DataFrame({\n  'holiday': 'Germany public holiday',\n  'ds': pd.to_datetime(holiday_de_germany),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de_austria = pd.DataFrame({\n  'holiday': 'Austria public holiday',\n  'ds': pd.to_datetime(holiday_de_austria),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de_switzerland = pd.DataFrame({\n  'holiday': 'Switzerland public holiday',\n  'ds': pd.to_datetime(holiday_de_switzerland),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de = pd.concat((holidays_de_germany, holidays_de_austria, holidays_de_switzerland))\n\nholidays_zh_hongkong = pd.DataFrame({\n  'holiday': 'HK public holiday',\n  'ds': pd.to_datetime(holiday_zh_hongkong),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_zh_taiwan = pd.DataFrame({\n  'holiday': 'Taiwan public holiday',\n  'ds': pd.to_datetime(holiday_zh_taiwan),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_zh = pd.concat((holidays_zh_hongkong, holidays_zh_taiwan))\n\nholidays_dict = {\"en\": holidays_en, \n                 \"ru\": holidays_ru, \n                 \"es\": holidays_es, \n                 \"fr\": holidays_fr, \n                 \"ja\": holidays_jp,\n                 \"de\": holidays_de,\n                 \"zh\": holidays_zh}","7d4f6ec0":"df_train['ds'] = pd.to_datetime(df_train['ds'])\ndf_train.ds.dt.dayofweek","d3659061":"df_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]","967a1707":"df_train['ds'] = pd.to_datetime(df_train['ds'])\ndf_actual['ds'] = pd.to_datetime(df_actual['ds'])\ntrain_series = df_train['y']\ntrain_series.index = df_train.ds","af3c0677":"df_train.ds.isin(holidays_dict[lang].ds)","c4698a02":"if(isinstance(lang, float) and math.isnan(lang)):\n    df_train['holiday'] = df_train.ds.dt.dayofweek >=5\n    df_actual['holiday'] = df_actual.ds.dt.dayofweek >=5\nelse:\n    df_train['holiday'] = (df_train.ds.dt.dayofweek >=5) | df_train.ds.isin(holidays_dict[lang].ds)\n    df_actual['holiday'] = (df_actual.ds.dt.dayofweek >=5) | df_actual.ds.isin(holidays_dict[lang].ds)","4c30a5dd":"df_train['y'] = df_train.y.astype(int).values\ndf_actual['y'] = df_actual.y.astype(int).values","c1538ad0":"def nanmedian_zero(a):\n    return np.nan_to_num(np.nanmedian(a))","61b6daa5":"holiday = True\np = 15\nsample = df_train[-p:]\nif(holiday):\n    sample = sample[sample['holiday']]\nelse:\n    sample = sample[~sample['holiday']]","23ae9cb1":"visits = nanmedian_zero(sample['y'])","ca255507":"predict_holiday = median_holiday_helper(df_train, df_actual[df_actual.holiday], p, True)\npredict_non_holiday = median_holiday_helper(df_train, df_actual[~df_actual.holiday], p, False)","683eea48":"predict_holiday","6d8623f6":"predict_non_holiday.combine_first(predict_holiday)","cb6e3811":"def median_holiday_model(df_train, df_actual, p, lang, review=False, figSize=(12, 4)):\n    # Split the train and actual set\n    df_train['ds'] = pd.to_datetime(df_train['ds'])\n    df_actual['ds'] = pd.to_datetime(df_actual['ds'])\n    train_series = df_train['y']\n    train_series.index = df_train.ds\n    \n    if(isinstance(lang, float) and math.isnan(lang)):\n        df_train['holiday'] = df_train.ds.dt.dayofweek >=5\n        df_actual['holiday'] = df_actual.ds.dt.dayofweek >=5\n    else:\n        df_train['holiday'] = (df_train.ds.dt.dayofweek >=5) | df_train.ds.isin(holidays_dict[lang].ds)\n        df_actual['holiday'] = (df_actual.ds.dt.dayofweek >=5) | df_actual.ds.isin(holidays_dict[lang].ds)\n     \n    # Combine the train and actual set\n    predict_holiday = median_holiday_helper(df_train, df_actual[df_actual.holiday], p, True)\n    predict_non_holiday = median_holiday_helper(df_train, df_actual[~df_actual.holiday], p, False)\n\n    forecast_series = predict_non_holiday.combine_first(predict_holiday)\n    \n    actual_series = df_actual.y\n    actual_series.index = df_actual.ds\n    \n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='Median model with holiday')\n    \n    return smape(forecast_series, actual_series)\n\n\ndef median_holiday_helper(df_train, df_actual, p, holiday):\n    def nanmedian_zero(a):\n        return np.nan_to_num(np.nanmedian(a))\n    \n    df_train['y'] = df_train['y'].astype('float').values\n    df_actual['y'] = df_actual['y'].astype('float').values\n    \n    sample = df_train[-p:]\n    if(holiday):\n        sample = sample[sample['holiday']]\n    else:\n        sample = sample[~sample['holiday']]\n\n    visits = nanmedian_zero(sample['y'])\n    \n    idx = np.arange( p) + np.arange(len(df_train)- p+1)[:,None]\n    b = [row[row>=0] for row in df_train.y.values[idx]]\n    pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\n    pre_forecast.index = df_train.ds\n    \n    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n    forecast_series.index = df_actual.ds\n    \n    forecast_series = pre_forecast.append(forecast_series)\n    \n    return forecast_series","6e3b1cb9":"# This is to demo the median model - weekday, weekend and \nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = median_holiday_model(df_train.copy(), df_actual.copy(), 15, lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","4b6f9898":"from statsmodels.tsa.arima_model import ARIMA   \nimport warnings\n\ndef arima_model(df_train, df_actual, p, d, q, figSize=(12, 4), review=False):\n    df_train = df_train.fillna(0)\n    train_series = df_train.y\n    train_series.index = df_train.ds\n\n    result = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        try:\n            arima = ARIMA(train_series ,[p, d, q])\n            result = arima.fit(disp=False)\n        except Exception as e:\n            print('\\tARIMA failed', e)\n                \n    #print(result.params)\n    start_idx = df_train.ds[d]\n    end_idx = df_actual.ds.max()\n    forecast_series = result.predict(start_idx, end_idx, typ='levels')\n    \n    actual_series = df_actual.y\n    actual_series.index = pd.to_datetime(df_actual.ds)\n\n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='ARIMA model')\n    \n    return smape(forecast_series, actual_series)","9a77a721":"df_train = df_train.fillna(0)\ntrain_series = df_train.y\ntrain_series.index = df_train.ds\n\nresult = None\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    try:\n        arima = ARIMA(train_series ,[4, 1, 4])\n        result = arima.fit(disp=False)\n    except Exception as e:\n        print('\\tARIMA failed', e)","dfb22dbd":"print(\"AR params:\", result.arparams, \"MA params:\", result.maparams)","bbe6360c":"start_idx = df_train.ds[1]\nend_idx = df_actual.ds.max()\nforecast_series = result.predict(start_idx, end_idx, typ='levels')","486ae007":"plt.plot(forecast_series.index, forecast_series.values)\nplt.show()","c89078cc":"forecast_series","d6a334a0":"# This is to demo the ARIMA model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","b1d748fa":"def plot_prediction_and_actual(model, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n    fig, ax  = plt.subplots(1,1,figsize=figSize)\n    ax.set_ylim(ylim)\n    ax.plot(pd.to_datetime(actual.ds), actual.y, 'r.')\n    model.plot(forecast, ax=ax);\n    ax.set_title(title)\n    plt.show()","360e5e26":"start_date = df_actual.ds.min()\nend_date = df_actual.ds.max()\n\nactual_series = df_actual.y.copy()\nactual_series.index = df_actual.ds\n\ndf_train['y'] = df_train['y'].astype('float').values\n\ndf_actual['y'] = df_actual['y'].astype('float').values\n\nm = Prophet()\nm.fit(df_train)\nfuture = m.make_future_dataframe(periods=60)\nforecast = m.predict(future)","b63b1e58":"forecast","a93b4bcf":"print(start_date, end_date)","7dbac9dd":"forecast['ds']","3b1df021":"sum((forecast['ds'] >= start_date) & (forecast['ds'] <= end_date))","ddbacd8d":"# simple linear model\ndef normal_model(df_train, df_actual, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float').values\n    \n    df_actual['y'] = df_actual['y'].astype('float').values\n    \n    m = Prophet()\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n        \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n        #\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Normal model')\n    \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = forecast[mask].yhat # filter  predictions\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0 # negative values correction\n\n    return smape(forecast_series, actual_series)\n\ndef holiday_model(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float').values\n    \n    df_actual['y'] = df_actual['y'].astype('float').values\n\n    if(isinstance(lang, float) and math.isnan(lang)):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n\n    m = Prophet(holidays=holidays)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n        \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Holiday model')\n    \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = forecast[mask].yhat\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)\n\ndef yearly_model(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float').values\n    \n    df_actual['y'] = df_actual['y'].astype('float').values\n\n    if(isinstance(lang, float) and math.isnan(lang)):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n\n    m = Prophet(holidays=holidays, yearly_seasonality=True)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n        \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Yealry model')\n    \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = forecast[mask].yhat\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)","cea32d84":"# log model\ndef normal_model_log(df_train, df_actual, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float').values\n    df_train.y = np.log1p(df_train.y)\n    \n    df_actual['y'] = df_actual['y'].astype('float').values\n    df_actual.y = np.log1p(df_actual.y)\n    \n    m = Prophet()\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n    \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Normal model in log')\n        \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)\n\ndef holiday_model_log(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float').values\n    df_train.y = np.log1p(df_train.y)\n    \n    df_actual['y'] = df_actual['y'].astype('float').values\n    df_actual.y = np.log1p(df_actual.y)\n\n    if(isinstance(lang, float) and math.isnan(lang)):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n    m = Prophet(holidays=holidays)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n    \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Holiday model in log')\n        \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n    \n    return smape(forecast_series, actual_series)\n\ndef yearly_model_log(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float').values\n    df_train.y = np.log1p(df_train.y)\n    \n    df_actual['y'] = df_actual['y'].astype('float').values\n    df_actual.y = np.log1p(df_actual.y)\n\n    if(isinstance(lang, float) and math.isnan(lang)):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n        \n    m = Prophet(holidays=holidays, yearly_seasonality=True)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n\n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Yearly model in log')\n        \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n    \n    return smape(forecast_series, actual_series)","98080ba3":"# This is to demo the facebook prophet model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","f0d6f61e":"import warnings\nwarnings.filterwarnings('ignore')","2fd6c39c":"print(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","af91c05c":"print(train.iloc[[4464]])\n\ndf_train = extract_series(X_train, 4464, 5)\ndf_actual = extract_series(y_train, 4464, 5)\nlang = X_train.iloc[4464, 1]\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","3abc9803":"train.iloc[[6245]]\n\ndf_train = extract_series(X_train, 6245, 5)\ndf_actual = extract_series(y_train, 6245, 5)\nlang = X_train.iloc[6245, 1]\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","67ef3397":"train.iloc[[80002]]\n\ndf_train = extract_series(X_train, 80002, 5)\ndf_actual = extract_series(y_train, 80002, 5)\nlang = X_train.iloc[80002, 1]\ntitle = X_train.iloc[80002, 4]\nprint(title)\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\n# Please use this case to check your implementation of SMAPE\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","04b8fe18":"train.iloc[[80009]]\n\ndf_train = extract_series(X_train, 80009, 5)\ndf_actual = extract_series(y_train, 80009, 5)\nlang = X_train.iloc[80009, 1]\ntitle = X_train.iloc[80009, 4]\nprint(title)\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang=lang)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","6fc2b9b6":"train.iloc[[14211]]\n\ndf_train = extract_series(X_train, 14211, 5)\ndf_actual = extract_series(y_train, 14211, 5)\nlang = X_train.iloc[14211, 1]\ntitle = X_train.iloc[14211, 4]\nprint(title)\nscore = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang = lang)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\n# if there is too many zero, just use normal is OK.\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = arima_model(df_train.copy(), df_actual.copy(), 7, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","a6a02b9d":"series_num = 145033\nseries_num = 145057\n\nprint(train.iloc[[series_num]])\n\ndf_train = extract_series(X_train, series_num, 5)\ndf_actual = extract_series(y_train, series_num, 5)\n\nlang = X_train.iloc[series_num, 1]\ntitle = X_train.iloc[series_num, 4]\nprint(title)\n\ntry:\n    score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating median model\", e)\n\ntry:\n    score = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang = lang)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating holiday model\", e)\n\ntry:\n    score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating holiday model in log\", e)\n\ntry:\n    score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating yearly model in log\", e)\n\ntry:\n    score = arima_model(df_train.copy(), df_actual.copy(), 7, 1, 2, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating arima model\", e)","d38c979d":"warnings.resetwarnings()","017782ee":"import glob\n\ndef read_from_folder(path):\n    filenames = glob.glob(path + \"\/*.csv\")\n\n    dfs = []\n    for filename in filenames:\n        dfs.append(pd.read_csv(filename, index_col=0))\n    \n    frame = pd.concat(dfs)\n    return frame.sort_index()","01a51f14":"# TODO: overall validation score in one number.\ndef validation_score(score_series):\n    return score_series.mean()","dd7470f3":"valid_fn = r\"..\/input\/wiktraffictimeseriesforecast\/validation_score.csv\"\nvalid_score_data = pd.read_csv(valid_fn, index_col=0)\n\nprint(valid_score_data[0:10])","679da038":"valid_score_data","0035bc73":"# Check which model is the best\nprint(\"Validation score for median model (7 days) is: %.6f\" % validation_score(valid_score_data['median7']))\nprint(\"Validation score for median model (14 days) is: %.6f\" % validation_score(valid_score_data['median14']))\nprint(\"Validation score for median model (21 days) is: %.6f\" % validation_score(valid_score_data['median21']))\nprint(\"Validation score for median model (28 days) is: %.6f\" % validation_score(valid_score_data['median28']))\nprint(\"Validation score for median model (35 days) is: %.6f\" % validation_score(valid_score_data['median35']))\nprint(\"Validation score for median model (42 days) is: %.6f\" % validation_score(valid_score_data['median42']))\nprint(\"Validation score for median model (49 days) is: %.6f\" % validation_score(valid_score_data['median49']))\n\nfig, axs  = plt.subplots(4,2,figsize=(12,12))\nvalid_score_data['median7'].plot.hist(bins=40, ax=axs[0][0])\nvalid_score_data['median14'].plot.hist(bins=40, ax=axs[0][1])\nvalid_score_data['median21'].plot.hist(bins=40, ax=axs[1][0])\nvalid_score_data['median28'].plot.hist(bins=40, ax=axs[1][1])\nvalid_score_data['median35'].plot.hist(bins=40, ax=axs[2][0])\nvalid_score_data['median42'].plot.hist(bins=40, ax=axs[2][1])\nvalid_score_data['median49'].plot.hist(bins=40, ax=axs[3][0])","ddd05857":"print(\"Validation score for median model w\/holiday (7 days) is: %.6f\" % validation_score(valid_score_data['median7_h']))\nprint(\"Validation score for median model w\/holiday (14 days) is: %.6f\" % validation_score(valid_score_data['median14_h']))\nprint(\"Validation score for median model w\/holiday (21 days) is: %.6f\" % validation_score(valid_score_data['median21_h']))\nprint(\"Validation score for median model w\/holiday (28 days) is: %.6f\" % validation_score(valid_score_data['median28_h']))\nprint(\"Validation score for median model w\/holiday (35 days) is: %.6f\" % validation_score(valid_score_data['median35_h']))\nprint(\"Validation score for median model w\/holiday (42 days) is: %.6f\" % validation_score(valid_score_data['median42_h']))\nprint(\"Validation score for median model w\/holiday (49 days) is: %.6f\" % validation_score(valid_score_data['median49_h']))\n\nfig, axs  = plt.subplots(4,2,figsize=(12,12))\nvalid_score_data['median7_h'].plot.hist(bins=40, ax=axs[0][0])\nvalid_score_data['median14_h'].plot.hist(bins=40, ax=axs[0][1])\nvalid_score_data['median21_h'].plot.hist(bins=40, ax=axs[1][0])\nvalid_score_data['median28_h'].plot.hist(bins=40, ax=axs[1][1])\nvalid_score_data['median35_h'].plot.hist(bins=40, ax=axs[2][0])\nvalid_score_data['median42_h'].plot.hist(bins=40, ax=axs[2][1])\nvalid_score_data['median49_h'].plot.hist(bins=40, ax=axs[3][0])","a86f6598":"print(\"Validation score for holiday model is: %.6f\" % validation_score(valid_score_data['holiday']))\nprint(\"Validation score for holiday model w\/log is: %.6f\" % validation_score(valid_score_data['holiday_log']))\nprint(\"Validation score for yearly model w\/log is: %.6f\" % validation_score(valid_score_data['yearly_log']))\n\nfig, axs  = plt.subplots(3,1,figsize=(12,12))\nvalid_score_data['holiday'].plot.hist(bins=40, ax=axs[0])\naxs[0].set_title(\"Holiday model\")\nvalid_score_data['holiday_log'].plot.hist(bins=40, ax=axs[1])\naxs[1].set_title(\"Holiday model w\/log\")\nvalid_score_data['yearly_log'].plot.hist(bins=40, ax=axs[2])\naxs[2].set_title(\"Yearly model w\/log\")","39f43607":"def model_to_use( median, holiday_log, yearly_log):\n    result = median\n    if(median * 1 > yearly_log):\n        result = yearly_log\n    elif(median * 1 > holiday_log):\n        result = holiday_log\n        \n    return result\n\ndef model_to_use_linear( median, holiday_log, yearly_log):\n    result = median\n    if(median * 1 > yearly_log):\n        result = yearly_log\n    elif(median * 1 > holiday_log):\n        result = holiday_log\n        \n    return result\n\nmodel_score = valid_score_data.apply(lambda x: model_to_use( x['median14'], x['holiday_log'], x['yearly_log']), axis=1)\n\nprint(\"Validation score for a proposed model is: %.6f\" % validation_score(model_score))\nmodel_score.plot.hist(bins=40)","516e83fc":"model_score_2 = valid_score_data.min(axis=1)\nprint(\"Best possible Validation score for a mixed model is: %.6f\" % validation_score(model_score_2))\n\nmodel_score_2.plot.hist(bins=40)","007e7773":"### B. Median model - weekday, weekend, holiday","e92458bc":"# 1. Introduction \n\n## A. Competition details\n\n### First stage\n* Training data from 2015-07-01 to 2016-12-29\n* Testing data from 2017-01-01 to 2017-03-01\n* Length of training vs length of testing = 547 days vs 59 days\n* Predict interval is ~10.7% of the training interval\n\n### Second stage\n* Training data from 2015-07-01 to 2017-09-01\n* Testing data from 2017-09-10 to 2017-11-10\n* Length of training vs length of testing = 793 days vs 61 days\n* Predict interval is ~7.7% of the training interval","2ecf5050":"#### \u043a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u0438\u043f\u043e\u0432 baseline","da4955ca":"### Case ?: Adhoc study","9c360af6":"### Case 2: Yearly model is the best model","8f1e2621":"#### \u0414\u043e\u0441\u0442\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432\u0441\u0435\u0445 wiki-\u0441\u0442\u0440\u0430\u043d\u0438\u0446","c0507ae1":"### Case 4: SMAPE score is too high for all proposed models","a082b24d":"## E. Facebook prophet library\n\nFacebook prophet library is created by facebook and aims to create a human-friendly time series forecasting libary. For details, please refer to https:\/\/facebookincubator.github.io\/prophet\/\n\nThere are several favor, but I will focus on holiday, yearly and log model","70e0607f":"### E. mixed model\n\nIn this section, we will try to mix the model together to give a better prediction model","a1ddc916":"## F. Sample series analysis (For script reconciliation)","d6354a3e":"### Case 5: SMAPE score is too high for all proposed models","fbe5ff04":"## B. Split into train and validation dataset","ed798817":"### Case 6: SMAPE score is too high for all proposed models","36eb43f3":"#### \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u043e \u0442\u0438\u043f\u0430\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","d1ecf853":"# 2. Data transformation and helper functions\n## A. Article names and metadata","936f3c7e":"## C. Missing values","efee32f2":"## E. Extreme data","e57c869e":"### \u041f\u0435\u0440\u0432\u044b\u0435 \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 10 \u0441\u0442\u0440\u043e\u043a \u0441 null-\u0430\u043c\u0438","d7b79ef8":"### Case 1: SMAPE evaluation near zero and SMAPE score is too big","4086a37d":"## A. SMAPE, the measurement\n\nSMAPE is harsh when the series is near zero.\nA notebook https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness give a very good visualization of the SMAPE function.\n\nAfter you find that there is no way to further improve quality of the result, you may consider doing a little bit hacking on SMAPE to give you better score.","955cbad3":"### Case 3: Non-yearly model is better","3d17d3d7":"# 3 Forecast methods\n\n\u0412 \u044d\u0442\u043e\u0439 \u0441\u0435\u043a\u0446\u0438\u0438 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438 \u0440\u044f\u0434\u0430\u043c\u0438","396b055d":"### A. Simple median model\n\nWe will train up the median model using popular choice 7 to 49, with step 7, and compare the overall score.","e9101927":"## D. ARIMA model\n\nThe below use the ARIMA from a Python library statsmodels. Please refer to http:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.arima_model.ARIMA.html for details.\n\nThe model is slow and may throw exception if the model cannot find a solution. (Make the model difficult to build for all series).\n\nWe will further investigate it performance in the later section.","d6cba14c":"#### \u041a\u043e\u043b-\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u0442\u0430\u0442\u0435\u0439","aa8f4ff6":"## C. Median model - weekday, weekend and holiday","46b2fe5f":"## D. Data visualization","e4160e40":"## B. Load libraries and data files, file structure and content","2b0e4370":"## B. Simple median model","29008318":"## 4. Selected model performance (validation score) over train dataset\n\nIn this session, we wil train the model and do prediction over 145000+ series in dataset. \nTo find out the validation score for comparison","290c6b82":"### D. Facebook model\nWe will train up the model using model with yearly and non-yearly model to see the difference","6fd9fae9":"# Web Traffic Time Series Forecasting (Experimenting with different method)\n\nBy Lai Yiu Ming, Tom\n\n1. Introduction\n    1. Competition details\n    2. Load libraries and data files, file structure and content\n    3. Missing values\n    4. Data visualization\n    5. Extreme data\n2. Data transformation and helper functions\n    1. Article names and metadata\n    2. Split into train and validation dataset\n3. Forecast methods\n    1. SMAPE, the measurement\n    2. Simple median model\n    3. Median model - weekday, weekend and holiday\n    4. ARIMA model\n    5. Facebook prophet model\n    6. Sample series analysis (For script reconciliation)\n4. Selected model performance (validation score) over train dataset\n    1. Simple median model\n    2. Median model - weekday, weekend, holiday\n    3. ARIMA model\n    4. Facebook model\n    5. mixed model","6e4be8ae":"### C. ARIMA model\n\nCurrently, it is no promising, and median seems a better base line, so I give up this section."}}