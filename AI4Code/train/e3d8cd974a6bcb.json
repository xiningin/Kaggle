{"cell_type":{"f8d0ccb3":"code","39b5bf2b":"code","46c695ed":"code","f8533d7f":"code","46545a08":"code","55195e5f":"code","657428bd":"code","6e593117":"code","e59c29d6":"code","0f07bee2":"code","69d7ef2c":"code","e42a03ec":"code","8799eff5":"code","9dcaac42":"code","4c29473c":"code","131e717e":"code","f367bbf3":"code","b1c01a71":"code","26b62080":"code","121765d2":"code","594aed72":"code","d6143e86":"code","cd4b47f3":"code","5f07e0c1":"code","ba3572b1":"code","71b9e02c":"code","1827be5b":"code","e468bfda":"code","9e6f0b13":"code","5f559e78":"code","a00529d0":"markdown","814cba66":"markdown","0af53b6c":"markdown","d85220fb":"markdown","ea821cf0":"markdown","3e19d5f4":"markdown","9a2224a8":"markdown","2e89953d":"markdown","a9f2dfcc":"markdown","8fff62e8":"markdown","229703bd":"markdown","8e91a2bc":"markdown","838f0c4d":"markdown","3a674b6b":"markdown","48fe581c":"markdown","f6574fe1":"markdown","b387961e":"markdown","992d9d9e":"markdown","52b832f5":"markdown","a4b20a60":"markdown","98ecc962":"markdown","0f741f60":"markdown"},"source":{"f8d0ccb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39b5bf2b":"df = pd.read_csv('..\/input\/electric-power-consumption-data-set\/household_power_consumption.txt', sep=';', \n                 parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, \n                 low_memory=False, na_values=['nan','?'], index_col='dt')","46c695ed":"df.shape","f8533d7f":"df.isnull().sum()","46545a08":"df = df.fillna(df.mean())\ndf.isnull().sum()","55195e5f":"import matplotlib.pyplot as plt\n\ni = 1\ncols=[0, 1, 3, 4, 5, 6]\nplt.figure(figsize=(20, 10))\nfor col in cols:\n    plt.subplot(len(cols), 1, i)\n    plt.plot(df.resample('M').mean().values[:, col])\n    plt.title(df.columns[col] + ' data resample over month for mean', y=0.75, loc='left')\n    i += 1\nplt.show()","657428bd":"i = 1\ncols=[0, 1, 3, 4, 5, 6]\nplt.figure(figsize=(20, 10))\nfor col in cols:\n    plt.subplot(len(cols), 1, i)\n    plt.plot(df.resample('D').mean().values[:, col])\n    plt.title(df.columns[col] + ' data resample over day for mean', y=0.75, loc='center')\n    i += 1\nplt.show()","6e593117":"i = 1\ncols=[0, 1, 3, 4, 5, 6]\nplt.figure(figsize=(20, 10))\nfor col in cols:\n    plt.subplot(len(cols), 1, i)\n    plt.plot(df.resample('H').mean().values[:, col])\n    plt.title(df.columns[col] + ' data resample over hour for mean', y=0.75, loc='left')\n    i += 1\nplt.show()","e59c29d6":"df.corr()","0f07bee2":"import seaborn as sns\nf= plt.figure(figsize=(21,3))\n\nax=f.add_subplot(131)\ndfm = df.resample('M').mean()\nsns.heatmap(dfm.corr(), vmin=-1, vmax=1, annot=True)\nplt.title('Monthly resampling', size=12)\n\nax=f.add_subplot(132)\ndfd = df.resample('D').mean()\nsns.heatmap(dfd.corr(), vmin=-1, vmax=1, annot=True)\nplt.title('Daily resampling', size=12)\n  \nax=f.add_subplot(133)\ndfh = df.resample('H').mean()\nsns.heatmap(dfh.corr(), vmin=-1, vmax=1, annot=True)\nplt.title('Hourly resampling', size=12)\nplt.show()","69d7ef2c":"df = df[['Global_active_power', 'Global_reactive_power', 'Voltage',\n       'Global_intensity', 'Sub_metering_2', 'Sub_metering_1','Sub_metering_3']]","e42a03ec":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","8799eff5":"df_resample = df.resample('h').mean() \ndf_resample.shape","9dcaac42":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df_resample.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df_resample.shape[1]+1, 2*df_resample.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series. Only 4000 first data points are selected for traing purpose.\nvalues = reframed.values\nn_train_time = 4000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","4c29473c":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=50, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df_resample.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","131e717e":"aa=[x for x in range(500)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:500], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:500], 'r', label=\"prediction\")\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time step for first 500 hours', size=15)\nplt.legend(fontsize=15)\nplt.show()","f367bbf3":"inv_yhat[1]","b1c01a71":"aa=[x for x in range(1000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[20000:21000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[20000:21000], 'r', label=\"prediction\")\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time step for 1000 hours from 20,000 to 21,000', size=15)\nplt.legend(fontsize=15)\nplt.show()","26b62080":"df = df[['Sub_metering_3','Global_active_power', 'Global_reactive_power', 'Voltage',\n       'Global_intensity', 'Sub_metering_2', 'Sub_metering_1']]","121765d2":"df_resample = df.resample('h').mean() \ndf_resample.shape","594aed72":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df_resample.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df_resample.shape[1]+1, 2*df_resample.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series. Only 4000 first data points are selected for traing purpose.\nvalues = reframed.values\nn_train_time = 4000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","d6143e86":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=250, batch_size=100, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df_resample.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","cd4b47f3":"aa=[x for x in range(500)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:500], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:500], 'r', label=\"prediction\")\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time step for first 500 hours', size=15)\nplt.legend(fontsize=15)\nplt.show()","5f07e0c1":"df = df[['Global_active_power', 'Global_reactive_power', 'Voltage',\n       'Global_intensity', 'Sub_metering_2', 'Sub_metering_1','Sub_metering_3']]\ndf = df.drop(['Voltage'],axis=1)","ba3572b1":"df.shape","71b9e02c":"df_resample = df.resample('h').mean() \ndf_resample.shape","1827be5b":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df_resample.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df_resample.shape[1]+1, 2*df_resample.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series. Only 4000 first data points are selected for traing purpose.\nvalues = reframed.values\nn_train_time = 4000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","e468bfda":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=300, batch_size=100, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df_resample.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","9e6f0b13":"aa=[x for x in range(500)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:500], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:500], 'r', label=\"prediction\")\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time step for first 500 hours', size=15)\nplt.legend(fontsize=15)\nplt.show()","5f559e78":"aa=[x for x in range(1000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[25000:26000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[25000:26000], 'r', label=\"prediction\")\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time step for 1000 hours from 25,000 to 26,000', size=15)\nplt.legend(fontsize=15)\nplt.show()","a00529d0":"# Introduction:\n\nIn this Notebook, I try to learn and build the Long Short-Term Memory (LSTM) recurrent neural network to fit one third of data and then predict the rest of data.\n    \nDatabase information:\n    \n(1) date: Date in format dd\/mm\/yyyy\n\n(2) time: time in format hh:mm:ss\n\n(3) global_active_power: household global minute-averaged active power (in kilowatt)\n\n(4) global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n\n(5) voltage: minute-averaged voltage (in volt)\n\n(6) global_intensity: household global minute-averaged current intensity (in ampere)\n\n(7) sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n\n(8) sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n\n(9) sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.","814cba66":"The LSTM nueral network has been selected because:\n\n+ it is best suited for large data, \n+ time-seriers, and \n+ sequential problem.\n\nIn the first step, I will fram the problem to predict the Global_active_power.","0af53b6c":"We also use 4000 first data points to fit the model.","d85220fb":"We resample as the previous procedure, and we have ONLY 6 columns in the database.","ea821cf0":"# Step 3: LSTM Data Preparation and fitting","3e19d5f4":"And compare the actual and prediction on first 500 points of the database.","9a2224a8":"This database have 2,075,259 rows and 7 columns. Let's check the NA data and fill na data with the mean values.","2e89953d":"# Step 2: Data visualizations","a9f2dfcc":"In order to reduce the computation time, and also get a quick result to test the model. We have resampled the data over hour to reduce the size of data from 2075259 to 34589 (data are given in minutes).","8fff62e8":"From above table, it is seen that 'Global_intensity' and 'Global_active_power' correlated. But 'Voltage', 'Global_active_power' are less correlated.","229703bd":"<h3> LSTM model setting <h3>","8e91a2bc":"In this Notebook, I practice to use the LSTM to fit and predict household electric power consumption. \n\n+ Consideration of the amount of input data is important to balance model accuracy and computation cost.\n\n+ The database can be used 30% for fitting purposes and the rest can be used to validate the model.","838f0c4d":"(1) 100 neurons in the first visible layer\n\n(2) dropout 10%\n\n(3) 1 neuron in the output layer for predicting Global_active_power\n\n(4) The input shape will be 1 time step with 7 features\n\n(5) The mean_squared_error loss function and the efficient adam version of stochastic gradient descent\n\n(6) The model will be fit for 50 training epochs with a batch size of 70.","3a674b6b":"Note that, we have resampled the database into hour, so, every time step is one hour. We try first to check the prediction in 500 hours.","48fe581c":"It is seen from above that with resampling techniques one can change the correlations among features.","f6574fe1":"# From above visualizations:\n\n+ the resampling by month, date or hour is very important because it has a large interaction as expected (changing the periodicity of the system). \n+ Therefore, if processing all the original data, the runtime will be very costly, but if processing data with large time-scale samples (e.g. monthly), it will affect the model's predictivity. \n+ From observation, we can see, it is relatively reasonable to resample data by hour.","b387961e":"<h3> Conclusions<h3>","992d9d9e":"So, we will have 7 input series variables and the 1 output variable for 'Global_active_power' at the current time in hour.\n\nWe also splitting the data into: train and validation sets. I select 4000 data over 34,589 dat to train, the rest will be used to test the model.","52b832f5":"Now, let's try to fit and predict the Sub_metering_3","a4b20a60":"Let's try to fit and predict the \"Global_active_power\" parameter, but this time, we do not need to include the \"Voltage\" data series in the database, since this is expected to NOT effect MUCH to the \"Global_active_power\" parameter.","98ecc962":"NOT BAD, let check the comparison of actual and prediction on 1000 data points from the 25,000 to 26,000.","0f741f60":"# Step 1: Import & cleaning data\n\nImporting the txt file takes more time than that of csv file."}}