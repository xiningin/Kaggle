{"cell_type":{"b2f9d185":"code","def46c00":"code","863a64bb":"code","cbde7131":"code","87e25f75":"code","866ae58a":"code","2f62c7ad":"code","b4c96625":"code","b17f10cb":"code","59493855":"code","d812be3d":"code","2dff2fd6":"code","fdf22677":"code","f7033cf4":"markdown","6a0e6e8b":"markdown","08c8d474":"markdown","35ca138f":"markdown"},"source":{"b2f9d185":"## Import libraries \n\nimport cv2\nimport numpy as np\nimport os\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom glob import glob\nfrom albumentations import RandomCrop, HorizontalFlip, VerticalFlip\n\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\n\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate\nfrom tensorflow.keras.models import Model\nfrom keras.utils import plot_model\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback","def46c00":"## Data Augmentation\n## It was chosen the resolution of 1536x1024px keep the ratio of the original images (6000x4000px). \n\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ndef augment_data(images, masks, save_path, augment=True):\n    H = 1024\n    W = 1536\n    for x,y in tqdm(zip(images, masks), total=len(images)):\n        name = x.split(\"\/\")[-1].split(\".\")\n        image_name = name[0]\n        image_extn = name[1]\n\n        name = y.split(\"\/\")[-1].split(\".\")\n        mask_name = name[0]\n        mask_extn = name[1]       \n        \n        x = cv2.imread(x, cv2.IMREAD_COLOR)\n        x = cv2.resize(x, (W, H))\n        y = cv2.imread(y, cv2.IMREAD_COLOR)\n        y = cv2.resize(y, (W, H))\n        \n        if augment == True:\n            \n            aug = RandomCrop(int(2*H\/3), int(2*W\/3), always_apply=False, p=1.0)\n            augmented = aug(image=x, mask=y)\n            x1 = augmented[\"image\"]\n            y1 = augmented[\"mask\"]\n \n            aug = HorizontalFlip(always_apply=False, p=1.0)\n            augmented = aug(image=x, mask=y)\n            x2 = augmented[\"image\"]\n            y2 = augmented[\"mask\"]\n            \n            aug = VerticalFlip(always_apply=False, p=1.0)\n            augmented = aug(image=x, mask=y)\n            x3 = augmented[\"image\"]\n            y3 = augmented[\"mask\"] \n            \n            save_images = [x, x1, x2, x3]\n            save_masks = [y, y1, y2, y3]            \n          \n        else:\n            save_images = [x]\n            save_masks = [y]\n        \n        idx = 0\n        for i, m in zip(save_images, save_masks):\n            i = cv2.resize(i, (W, H))\n            m = cv2.resize(m, (W, H))\n            \n            tmp_img_name = f\"{image_name}_{idx}.{image_extn}\"\n            tmp_msk_name = f\"{mask_name}_{idx}.{mask_extn}\" \n            \n            image_path = os.path.join(save_path, \"images\", tmp_img_name)\n            mask_path = os.path.join(save_path, \"masks\", tmp_msk_name)\n            \n            cv2.imwrite(image_path, i)\n            cv2.imwrite(mask_path, m)\n\n            idx+=1\n\n\npath = \"..\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/\"\nimages = sorted(glob(os.path.join(path, \"original_images\/*\")))\nmasks = sorted(glob(os.path.join(path, \"label_images_semantic\/*\")))\nprint(f\"Original images:  {len(images)} - Original masks: {len(masks)}\")\n\ncreate_dir(\".\/new_data\/images\/\")\ncreate_dir(\".\/new_data\/masks\/\")\n\nsave_path = \".\/new_data\/\"\n\naugment_data(images, masks, save_path, augment=True)\n\nimages = sorted(glob(os.path.join(save_path, \"images\/*\")))\nmasks = sorted(glob(os.path.join(save_path, \"masks\/*\")))\nprint(f\"Augmented images:  {len(images)} - Augmented masks: {len(masks)}\")","863a64bb":"## Create dataframe\n\nimage_path =  os.path.join(save_path, \"images\/\")\nlabel_path = os.path.join(save_path, \"masks\/\")\n\ndef create_dataframe(path):\n    name = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            name.append(filename.split('.')[0])\n    \n    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n\ndf_images = create_dataframe(image_path)\ndf_masks = create_dataframe(label_path)\nprint('Total Images: ', len(df_images))\n#print(df_images)","cbde7131":"## Split data\n\nX_trainval, X_test = train_test_split(df_images['id'], test_size=0.1, random_state=19)\nX_train, X_val = train_test_split(X_trainval, test_size=0.2, random_state=19)\n\nprint(f\"Train Size : {len(X_train)} images\")\nprint(f\"Val Size   :  {len(X_val)} images\")\nprint(f\"Test Size  :  {len(X_test)} images\")\n\ny_train = X_train #the same values for images (X) and labels (y)\ny_test = X_test\ny_val = X_val\n\nimg_train = [os.path.join(image_path, f\"{name}.jpg\") for name in X_train]\nmask_train = [os.path.join(label_path, f\"{name}.png\") for name in y_train]\nimg_val = [os.path.join(image_path, f\"{name}.jpg\") for name in X_val]\nmask_val = [os.path.join(label_path, f\"{name}.png\") for name in y_val]\nimg_test = [os.path.join(image_path, f\"{name}.jpg\") for name in X_test]\nmask_test = [os.path.join(label_path, f\"{name}.png\") for name in y_test]","87e25f75":"## Define U-Net Model\n## In order to minimize the dimension of the model, it is possible to reduce the number of the filters for each layer.\n## To do this, scale by a factor 2 the variables filters_x and filters_b.\n\n\ndef conv_block(inputs, filters, pool=True):\n    x = Conv2D(filters, 3, padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    x = Conv2D(filters, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    if pool == True:\n        p = MaxPool2D((2,2))(x)\n        return x, p\n    else:\n        return x\n    \n    \ndef build_unet(shape, num_classes):   \n    inputs = Input(shape)\n    \n    filters_x = [32,64,96,128,128,96,64,32]\n    filters_b = [256] \n\n    # Encoder\n    x1, p1 = conv_block(inputs, filters_x[0], pool=True)\n    x2, p2 = conv_block(p1, filters_x[1], pool=True)\n    x3, p3 = conv_block(p2, filters_x[2], pool=True)\n    x4, p4 = conv_block(p3, filters_x[3], pool=True)    \n    \n    # Bridge\n    b1 = conv_block(p4, filters_b[0], pool=False)\n    \n    # Decoder\n    u1 = UpSampling2D((2,2), interpolation='bilinear')(b1)\n    c1 = Concatenate()([u1, x4])\n    x5 = conv_block(c1, filters_x[4], pool=False)\n    \n    u2 = UpSampling2D((2,2), interpolation='bilinear')(x5)\n    c2 = Concatenate()([u2, x3])\n    x6 = conv_block(c2, filters_x[5], pool=False)\n    \n    u3 = UpSampling2D((2,2), interpolation='bilinear')(x6)\n    c3 = Concatenate()([u3, x2])\n    x7 = conv_block(c3, filters_x[6], pool=False)\n    \n    u4 = UpSampling2D((2,2), interpolation='bilinear')(x7)\n    c4 = Concatenate()([u4, x1])\n    x8 = conv_block(c4, filters_x[7], pool=False)\n    \n    # Output Layer\n    output = Conv2D(num_classes, 1, padding='same', activation='softmax')(x8)\n\n    return Model(inputs, output)","866ae58a":"## Define the resolution of the images and the number of classes\n\nH = 768   #to keep the original ratio \nW = 1152 \nnum_classes = 23\n\nmodel = build_unet((W, H, 3), num_classes)  ","2f62c7ad":"## Show the summary of the U-Net model and its diagram\n\nmodel.summary()\nplot_model(model,to_file='model.png')","b4c96625":"## Dataset Pipeline used for training the model\n\ndef read_image(x):\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x\/255.0\n    x = x.astype(np.float32)\n    return x\n\n\ndef read_mask(x):\n    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    x = x.astype(np.int32)\n    return x\n\n\ndef tf_dataset(x,y, batch=4):\n    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n    dataset = dataset.shuffle(buffer_size=500)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(2)\n    return dataset\n    \n\ndef preprocess(x,y):\n    def f(x,y):\n        x = x.decode()\n        y = y.decode()\n        image = read_image(x)\n        mask = read_mask(y)\n        return image, mask\n    \n    image, mask = tf.numpy_function(f,[x,y],[tf.float32, tf.int32])\n    mask = tf.one_hot(mask, num_classes, dtype=tf.int32)\n    image.set_shape([H, W, 3])    # In the Images, number of channels = 3. \n    mask.set_shape([H, W, num_classes])    # In the Masks, number of channels = number of classes. \n    return image, mask","b17f10cb":"## Train the model\n\n# Seeding\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Hyperparameters\nshape = (H, W, 3)\nnum_classes = 23  \nlr = 1e-4\nbatch_size = 4 \nepochs = 30\n\n# Model\nmodel = build_unet(shape, num_classes)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr), metrics=['accuracy'])\n\ntrain_dataset = tf_dataset(img_train, mask_train, batch = batch_size)\nvalid_dataset = tf_dataset(img_val, mask_val, batch = batch_size)\n\ntrain_steps = len(img_train)\/\/batch_size\nvalid_steps = len(img_val)\/\/batch_size\n\ncallbacks = [\n    ModelCheckpoint(\"model.h5\", verbose=1, save_best_model=True),\n    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=1, min_lr=1e-6),\n    EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n]\n\nmodel.fit(train_dataset,\n          steps_per_epoch=train_steps,\n          validation_data=valid_dataset,\n          validation_steps=valid_steps,\n          epochs=epochs,\n          callbacks=callbacks\n         )","59493855":"## Plot accuracy and loss\n\ntrain_loss = model.history.history['loss']\nval_loss   = model.history.history['val_loss']\ntrain_acc  = model.history.history['accuracy']\nval_acc    = model.history.history['val_accuracy']\n\n# summarize history for accuracy\nplt.plot(model.history.history['accuracy'])\nplt.plot(model.history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(model.history.history['loss'])\nplt.plot(model.history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","d812be3d":"## Prediction\n\ncreate_dir('.\/results')  #create the folder for the predictions\n\n# Seeding\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Model\nmodel = tf.keras.models.load_model(\"model.h5\")\n\n# Saving the masks\nfor x, y in tqdm(zip(img_test, mask_test), total=len(img_test)):\n    name = x.split(\"\/\")[-1]\n    \n    ## Read image\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x\/255.0\n    x = x.astype(np.float32)\n\n    ## Read mask\n    y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n    y = cv2.resize(y, (W, H))\n    \n    y = np.expand_dims(y, axis=-1) #(384,256,1)\n    \n    y = y * (255\/num_classes)\n    y = y.astype(np.int32)\n    y = np.concatenate([y, y, y], axis=2)\n    \n    ## Prediction\n    p = model.predict(np.expand_dims(x, axis=0))[0]\n    p = np.argmax(p, axis=-1)\n    \n    p = np.expand_dims(p, axis=-1)  \n    \n    p = p * (255\/num_classes)\n    p = p.astype(np.int32)\n    p = np.concatenate([p, p, p], axis=2)\n      \n    cv2.imwrite(f\".\/results\/{name}\", p)\n    ","2dff2fd6":"# From the test set, take only images that represent the ones in the original dataset and not those are obtained from the data augmentation.\n# (they have _0 in the name)\n\nimage_list = []\nmask_list = []\n\nfor x,y in tqdm(zip(img_test, mask_test), total=len(img_test)):\n    name = x.split(\"\/\")[-1]\n    image_name = name[4]\n\n    name = y.split(\"\/\")[-1]\n    mask_name = name[4]\n    \n    if image_name == '0':\n        image_list.append(x)\n        mask_list.append(y)","fdf22677":"## Plot 5 images to verify the accuracy in the predictions\n\nimg_selection = image_list[0:5]\nmask_selection = mask_list[0:5]\n\nfor img, mask in zip(img_selection, mask_selection):\n    name = img.split(\"\/\")[-1]\n    x = cv2.imread(img, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n\n    y = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n    y = cv2.resize(y, (W, H))\n\n\n    p = cv2.imread(f\".\/results\/{name}\", cv2.IMREAD_GRAYSCALE)\n    p = cv2.resize(p, (W, H))\n\n    #Plotto le tre immagini\n    fig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n\n    axs[0].imshow(x, interpolation = 'nearest')\n    axs[0].set_title('image')\n    axs[0].grid(False)\n\n    axs[1].imshow(y, interpolation = 'nearest')\n    axs[1].set_title('mask')\n    axs[1].grid(False)\n\n    axs[2].imshow(p)\n    axs[2].set_title('prediction')\n    axs[2].grid(False)","f7033cf4":"# **U-Net**\n\nU-Net is an end-to-end fully convolutional network (FCN), i.e. it only contains Convolutional layers and does not contain any Dense layer because of which it can accept image of any size. The architecture of a U-Net contains two paths: the first one is the contraction path (also called as the encoder) which is used to capture the context in the image. The encoder is just a traditional stack of convolutional and max pooling layers; the second path is the symmetric expanding path (also called as the decoder) which is used to enable precise localization using transposed convolutions. In the original paper, the U-Net is described as follows:\n\n![](https:\/\/miro.medium.com\/max\/3000\/1*OkUrpDD6I0FpugA_bbYBJQ.png)\n\nIt is important to say that convolution and pooling operations down-sample the image, i.e. convert a high resolution image to a low resolution image.\n\nMax Pooling operation helps to understand \u201cwhat\u201d there is in the image by increasing the receptive field. However it tends to lose the information of \u201cwhere\u201d the objects are.\n\nIn semantic segmentation it is not just important to know \u201cwhat\u201d is present in the image but it is equally important to know \u201cwhere\u201d it is present. Hence we need a way to up-sample the image from low resolution to high resolution which will help us restore the \u201cwhere\u201d information. Transposed Convolution is the most preferred choice to perform up-sampling, which basically learns parameters through back propagation to convert a low resolution image to a high resolution image.\n\nIn the following image there is the U-Net example for an input image of size 128x128x3.\n\n![U-Net architecture](https:\/\/miro.medium.com\/max\/2082\/1*yzbjioOqZDYbO6yHMVpXVQ.jpeg)\n\nTo better explain the image:\n\n- 2@Conv layers means that two consecutive Convolution Layers are applied.\n- c1, c2, ... c9 are the output tensors of Convolutional Layers.\n- p1, p2, p3 and p4 are the output tensors of Max Pooling Layers.\n- u6, u7, u8 and u9 are the output tensors of up-sampling (transposed convolutional) layers.\n- The left hand side is the contraction path (Encoder) where we apply regular convolutions and max pooling layers.\n- In the Encoder, the size of the image gradually reduces while the depth gradually increases. Starting from 128x128x3 to 8x8x256.\n- This basically means the network learns the \u201cWHAT\u201d information in the image, however it has lost the \u201cWHERE\u201d information.\n- The right hand side is the expansion path (Decoder) where we apply transposed convolutions along with regular convolutions.\n- In the decoder, the size of the image gradually increases and the depth gradually decreases. Starting from 8x8x256 to 128x128x1.\n- Intuitively, the Decoder recovers the \u201cWHERE\u201d information (precise localization) by gradually applying up-sampling.\n- To get better precise locations, at every step of the decoder we use skip connections by concatenating the output of the transposed convolution layers with the feature maps from the Encoder at the same level:\n\n    u6 = u6 + c4\n\n    u7 = u7 + c3\n\n    u8 = u8 + c2\n\n    u9 = u9 + c1\n\n    After every concatenation we again apply two consecutive regular convolutions so that the model can learn to assemble a more precise output.\n\n- This is what gives the architecture a symmetric U-shape, hence the name UNET.","6a0e6e8b":"# **About this notebook**\n\nThis notebook tries to provide a solution for the semantic segmentation task for \"Semantic Drone Dataset\", that can be downloaded from [here](https:\/\/www.kaggle.com\/bulentsiyah\/semantic-drone-dataset).\n\nIn this [link](https:\/\/www.tugraz.at\/index.php?id=22387) you can find the original dataset.\n\nI hope this could be a useful work for everyone who approaches ML and Semantic Segmentation for the first time (like me! :) ).\n","08c8d474":"I want to thank \"[Idiot Developer](https:\/\/www.youtube.com\/c\/IdiotDeveloper\/about)\" from YT, for helping me with his videos to write this kernel.","35ca138f":"# **Semantic Segmentation**\n\nSource: \"https:\/\/towardsdatascience.com\/understanding-semantic-segmentation-with-unet-6be4f42d4b47\"\n\n![Semantic Segmentation](https:\/\/miro.medium.com\/max\/875\/1*nXlx7s4wQhVgVId8qkkMMA.png)\n\nSemantic image segmentation is a branch of computer vision and its goal is to label each pixel of an image with a corresponding class of what is being represented. The output in semantic segmentation is a high resolution image (typically of the same size as input image) in which each pixel is classified to a particular class. It is a pixel level image classification.\n\nExamples of the applications of this task are:\n\nAutonomous vehicles, where semantic segmentation provides information about free space on the roads, as well as to detect lane markings and traffic signs.\nBiomedical image diagnosis, helping radiologists improving analysis performed, greatly reducing the time required to run diagnostic tests.\nGeo sensing, to recognize the type of land cover (e.g., areas of urban, agriculture, water, etc.) for each pixel on a satellite image, land cover classification can be regarded as a multi-class semantic segmentation task."}}