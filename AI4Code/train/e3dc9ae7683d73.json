{"cell_type":{"bd7a93c5":"code","21147c83":"code","ea2a711b":"code","d6987c67":"code","fb7cf7df":"code","0152ea1c":"code","8b3a8aa2":"code","73d01e03":"code","13edaa19":"code","c6c7c622":"code","4adf25af":"code","5aa0f2aa":"code","b8730782":"code","5fcb260b":"code","4f359588":"code","586fbe33":"code","5bb61385":"code","6330da0c":"code","00d95bb6":"code","498355ba":"code","f639c49d":"code","480340ee":"code","71fab9e7":"code","22715bf6":"code","54f111f1":"markdown","cf9e8de2":"markdown","171a1754":"markdown","07df6a74":"markdown","20d7b430":"markdown","1c657322":"markdown","b91231c7":"markdown","e062d5a2":"markdown","ce5f733d":"markdown","144f8aa1":"markdown","cb4fa087":"markdown","8b59e22d":"markdown","fba2750d":"markdown","38771b97":"markdown","aaafe947":"markdown","b5867f55":"markdown","77b39f12":"markdown"},"source":{"bd7a93c5":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport itertools\nimport sklearn\nimport numpy as np","21147c83":"train_df = pd.read_csv(\"..\/input\/data_set_ALL_AML_train.csv\")\ntest_df = pd.read_csv(\"..\/input\/data_set_ALL_AML_independent.csv\")\nvalidation_df = pd.read_csv(\"..\/input\/actual.csv\")\ntrain_df.head()","ea2a711b":"# removing all call columns from data frame\ntrain_columns = [col for col in train_df if \"call\" not in col]\ntest_columns = [col for col in test_df if \"call\" not in col]\ntrain_adjusted = train_df[train_columns]\ntest_adjusted = test_df[test_columns]\ntrain_adjusted.head()","d6987c67":"#transposing data frames\ntransposed_train = train_adjusted.T\ntransposed_test = test_adjusted.T\ntransposed_train.head()","fb7cf7df":"predictors = pd.concat([transposed_train, transposed_test], axis = 0)\npredictors = predictors.drop(['Gene Description', 'Gene Accession Number'])\npredictors.columns = transposed_train.iloc[0]","0152ea1c":"#resetting indices of both predictor and validation data frames so they can be combined\nvd = validation_df.reset_index(drop = True)\npr = predictors.reset_index(drop = True)\n#combining validation and predictor data frames\ncombined = pd.concat([pr, vd], axis = 1)\n#finding most expressed genes in combined data dataframe\noutcomes = combined.groupby('cancer').size()\noutcomes.plot(kind = 'bar')","8b3a8aa2":"highest = combined.mean().abs().sort_values(ascending = False)\nplt.figure(figsize=(10, 8))\nhighest.head(10).plot(kind = 'bar')\nplt.title('10 Genes Highest Expression Levels Both Cancer Types')\nplt.ylabel('Expression Levels (au)')","73d01e03":"c_ALL = combined[combined.cancer == 'ALL']\nhighest_ALL = c_ALL.mean().abs().sort_values(ascending = False)\nplt.figure(figsize=(10, 8))\nhighest_ALL.head(10).plot(kind = 'bar')\nplt.title('10 Genes Highest Expression Levels ALL')\nplt.ylabel('Expression Levels (au)')","13edaa19":"c_AML = combined[combined.cancer == 'AML']\nhighest_AML = c_AML.mean().abs().sort_values(ascending = False)\nplt.figure(figsize=(10, 8))\nhighest_AML.head(10).plot(kind = 'bar')\nplt.title('10 Genes Highest Expression Levels AML')\nplt.ylabel('Expression Levels (au)')","c6c7c622":"train_no_acc = transposed_train.drop([\"Gene Accession Number\",\"Gene Description\"]).apply(pd.to_numeric)\ntest_no_acc = transposed_test.drop([\"Gene Accession Number\", \"Gene Description\"]).apply(pd.to_numeric)\npredictors_no_acc = predictors.drop(['Gene Accession Number'], axis = 1).apply(pd.to_numeric)","4adf25af":"#resetting indices for test and train data frames\ntrain_no_acc = train_no_acc.reset_index(drop = True)\ntest_no_acc = test_no_acc.reset_index(drop = True)","5aa0f2aa":"#creating data frames for both test and train data validation\nvalidation_train = validation_df[validation_df.patient <= 38].reset_index(drop = True)\nvalidation_test = validation_df[validation_df.patient > 38].reset_index(drop = True)\nvalidation_test.head()","b8730782":"# combining predictor and validation set data\ntrain = pd.concat([validation_train, train_no_acc], axis = 1)\ntest = pd.concat([validation_test, test_no_acc], axis = 1)","5fcb260b":"#creating sample data frames from original for model creation\ntrain_sample = train.iloc[:,2:].sample(n=200, axis=1)\ntest_sample = test.iloc[:,2:].sample(n=200, axis=1)\ntest_sample.head()","4f359588":"train_sample.plot(kind=\"hist\", legend=None, bins=20, color='k')\ntrain_sample.plot(kind=\"kde\", legend=None);","586fbe33":"from sklearn import preprocessing\nscaled = pd.DataFrame(preprocessing.scale(train_sample))\nscaled.plot(kind=\"hist\", legend=None, bins=20, color='k')\nscaled.plot(kind=\"kde\", legend=None);","5bb61385":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nsample_scaled = StandardScaler().fit_transform(train_sample)\npca = PCA(n_components = 30)\npca.fit(sample_scaled)\n\ncum_sum = pca.explained_variance_ratio_.cumsum()\ncum_sum = cum_sum*100\n\nfix, ax = plt.subplots(figsize = (8, 8))\nplt.bar(range(30), cum_sum, color = 'r',alpha=0.5)\nplt.title('PCA Analysis')\nplt.ylabel('cumulative explained variance')\nplt.xlabel('number of components')\nplt.locator_params(axis='y', nbins=20)\n","6330da0c":"#training and test samples are created and scaled for model creation\nX_train = StandardScaler().fit_transform(train_no_acc)\nX_test = StandardScaler().fit_transform(test_no_acc)\ny_train = validation_train['cancer']\ny_test = validation_test['cancer']","00d95bb6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\n\ndef pipeline_PCA_GLM(components):\n    accuracy_chart = []\n    for i in components:\n        steps = [('pca', PCA(n_components = i)),\n        ('estimator', LogisticRegression())]\n        pipe = Pipeline(steps)\n        pipe.fit(X_train, y_train)\n        predictions = pipe.predict(X_test)\n        accuracy_chart.append(accuracy_score(y_test,predictions))\n    return accuracy_chart\n","498355ba":"n_components = range(1,30)\naccuracy_chart = pipeline_PCA_GLM(n_components)","f639c49d":"plt.figure(figsize=(10, 8))\nplt.bar(n_components, accuracy_chart)\nplt.ylim(0,1)\nplt.xlim(0,30)\nplt.locator_params(axis='y', nbins=20)\nplt.locator_params(axis = 'x', nbins = 30)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Number of Components\")","480340ee":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\ndef knn_pred(train_predictors, train_outcome, k_range, test_predictors):\n    #train_predictors and train_outcome should both be from training split while test_predictors should be from test split\n    y_pred = []\n    for i in k_range:\n        knn = KNeighborsClassifier(n_neighbors = i)\n        knn.fit(train_predictors, train_outcome)\n        y_pred.append(knn.predict(test_predictors))\n    return y_pred\n\n\n","71fab9e7":"#function compares KNN accuracy at different levels of K\ndef knn_accuracy(pred, k_range, test_outcome):\n    #pred represents predicted values while test_outcome represents the values from the test set\n    accuracy_chart = []\n    for i in range(len(k_range)):\n        accuracy_chart.append((sklearn.metrics.accuracy_score(test_outcome, pred[i])))\n    return accuracy_chart\n        ","22715bf6":"train_range = range(2, 20, 2)\nsample_pred = knn_pred(X_train, y_train, train_range, X_test)\naccuracy = knn_accuracy(sample_pred, train_range, y_test)\nplt.figure(figsize=(10, 8))\nplt.bar(train_range, accuracy)\nplt.ylim(0,1)\nplt.xlim(0,20)\nplt.locator_params(axis='y', nbins=20)\nplt.locator_params(axis = 'x', nbins = 10)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Number of Neighborhoods\")","54f111f1":"### Conclusion","cf9e8de2":"Gene expression analysis is a commonly used technique in molecular biology. In this analysis we are looking at the expression levels of multiple genes as determined by a DNA microarray and their roles in predicting cancer types. All of the subjects in the study had one of two types of cancer, ALL (acute lymphoblastic leukemia) or AML (acute myeloid leukemia). The data was used in 1999 by Golub et al. as a proof of concept study.","171a1754":"### K-Nearest Neighbors","07df6a74":"## Data Cleaning","20d7b430":"Below we can see the values of the 10 most highly expressed genes in the study, as the data has been scaled these points represent relative values of the expression of these genes. The most expressed genes in the two samples differ and this may indicate the roles of these genes in causing the different cancer types. Genes encoding the RPL37a Ribosomal protein A are highly expressed in both. If compared to a control population this may possibly be used as a general indicator of either of these leukemia types. However genes encoding proteins such as Globin Beta are only more highly expressed in one of the cancer types and more reasearch into their function may lead to insight with regards to the mechanisms of the different leukemias and what disinguishes them from one another.","1c657322":"### Pairing PCA with Logistic Regression","b91231c7":"## Exploratory Data Analysis","e062d5a2":"In order to test whether the data should be scaled before further model creation it's distribution was analyzed using a histogram as well as a Kernel Density Estimation. The results of this analysis showed that although a large portion of the data was indeed centered at zero it was still right skewed. In order to ramify this a scaled version of the data was used for analysis. ","ce5f733d":"In observing the data frames we can see that the genes accession numbers are listed as rows while the expression levels of each patient are listed as columns. This is not an ideal form for analysis as rows typically represent samples so the data frame was transposed.","144f8aa1":"In order to test the potential value of PCA as a potential dimensionality reduction method before analysis it was performed on a sample and the cummulative variance was compared to different numbers of principle components. What is observed is that within 30 principle components over 90% of the data's variance is captured. This indicates that PCA will be effective in reducing the effects of the data's large feature number and creating a more accurate model.","cb4fa087":"The analysis of this DNA microarray data has shown some interesting results and seems relatively amenable to model creation. During PCA it was shown that the model achieves the highest accuracy when the number of components is two. If one were to research the roles and mechanisms of the main genes involved then it would be possible to find insight into the main factors leading to the two different cancer types as well as what proteins are generally linked to leukemia. The models created achieved accuracy levels within roughly 5% of those observed in the study from which the data was derived. Although modelling wasn't the main goal of this analysis these results indicate that the algorithms used could lead to a high degree of accuracy if fine tuned.","8b59e22d":"The columns labeled call seem to represent probes used during DNA microarray analysis. These probes don't seem to provide any direct value in analysis and were dropped.","fba2750d":"Below I've paired PCA with logistic regression in order to create a regression model using a sample with reduced dimensionality. From the results we see that the ideal number of components is less than 10. Below the accuracy of the regression is plotted compared to the number of components.","38771b97":"### PCA","aaafe947":"## Model Creation \/ Data Preparation","b5867f55":"In order to see if the data was organized in separate gaussian distributions the data was clustered using K-Nearest Neighbors. The results of this clustering was used to create a model that was fit to the test set. We can see that the ideal number of neighbors is less than 10 and seems to lay at roughly 2.","77b39f12":"Below we can see the total proportion of AML to ALL cancer types, we can see  that the data frame has a larger proportion of patients with ALL than AML. We can also see that only slightly over 20 patients have AML so any findings gathered from EDA on the AML set are far from conclusive and should only be used as a guide for possible further investigation."}}