{"cell_type":{"0095f960":"code","61032f59":"code","aab1410a":"code","f549dd83":"code","25fbe606":"code","c263d8dc":"code","8782d782":"code","cbbae06b":"code","92336493":"code","1c052417":"code","a6e0a040":"code","54c51f44":"code","bed768f8":"markdown","a4aa7c57":"markdown","119606f3":"markdown","6606f873":"markdown","7c8fe255":"markdown","9e246071":"markdown"},"source":{"0095f960":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# Normalization tests\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom scipy.stats import shapiro\nfrom scipy.stats import normaltest\n\n# Data normalization \/ standartization\nfrom sklearn.preprocessing import PowerTransformer, MinMaxScaler, StandardScaler\n\n# libs for train tuning\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n\n# Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost.sklearn import XGBClassifier\n\n# Classification\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB, ComplementNB, GaussianNB, MultinomialNB\nfrom sklearn.svm import SVC\n\n# Model parameters fitting\nfrom sklearn.model_selection import GridSearchCV\n\n# Metrics\nfrom sklearn.metrics import accuracy_score,auc, f1_score, confusion_matrix,precision_score, recall_score, roc_auc_score, roc_curve\n\n# Serialize data\nimport pickle\n\nimport random","61032f59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aab1410a":"# Get dataset\ndf = pd.read_csv('\/kaggle\/input\/historical-data-on-the-trading-of-cryptocurrencies\/crypto_tradinds.csv')","f549dd83":"#Check the shape\ndf.shape","25fbe606":"# Raw dataset\ndf.head()","c263d8dc":"# Dataset common info\ndf.info()","8782d782":"# Dataset common stats\ndf.describe(include='all')","cbbae06b":"df['crypto_type'].value_counts(normalize=True)","92336493":"# Check nulls\ndf.isnull().sum()","1c052417":"# Functions for normalization test\ndef draw_hist_for_feature(data):\n  plt.style.use('ggplot')\n  data.hist(bins = 60)\n  plt.show()\n\n\ndef chech_skew(feature):\n  method_name = '\\nSKEW TEST: '\n  skew_ = np.abs(skew(feature))\n  if (skew_ >= 0.75) and (skew_ < 1.0):\n    print(method_name + 'Use logarithm method for data\\n')\n  elif skew_ >= 1:\n    print(method_name + 'Use normalization method for data\\n')\n  else:\n    print(method_name + 'Use standartization method for data\\n')\n\n\ndef check_shapiro(feature):\n  method_name = '\\nSHAPIRO TEST: '\n  shapiro_ = np.abs(shapiro(feature))\n  if (shapiro_[1] < 0.50):\n    print(method_name + 'Use normalization method for data\\n')\n  else:\n    print(method_name + 'Use standartization method for data\\n')\n\n\ndef print_stats(data, need_hist = True):\n  if (need_hist == True):\n    draw_hist_for_feature(data)\n\n  print(\"mean : \", np.mean(data))\n  print(\"var  : \", np.var(data))\n  print(\"skew : \", skew(data))\n  print(\"kurt : \", kurtosis(data))\n  print(\"shapiro : \", shapiro(data))\n  print(\"normaltest : \", normaltest(data))\n\n\ndef print_stats_all(df, need_hist = True):\n  n = 1\n  for feature_name in df.columns:\n    print(f'\\n\\n{n}. {feature_name}')\n    print_stats(df[feature_name], need_hist)\n    chech_skew(df[feature_name])\n    check_shapiro(df[feature_name])\n    n += 1","a6e0a040":"# Divide numerical and categorical data\nX_num = df.drop(['trade_date', 'crypto_name', 'crypto_type', 'ticker', 'site_url', 'github_url', 'minable', 'platform_name', 'industry_name'], axis = 1)\nX_cat = df[['trade_date', 'crypto_name', 'crypto_type', 'ticker', 'site_url', 'github_url', 'minable', 'platform_name', 'industry_name']]","54c51f44":"# Check normalization\nprint_stats_all(X_num)","bed768f8":"We have a dataset with cryptocurrencies prices.\n\nLet's have a look at the data and deal with some EDA.","a4aa7c57":"1. Let's define some testing functions to check our data.","119606f3":"So, we have 17 columns and 1996554 rows in dataset.","6606f873":"Here we see, that the balance between two types of currencies (tokens and coins) is rather equal.","7c8fe255":"Let's check some distrubution of the data","9e246071":"We have nulls in:\n* crypto_name                     149087\n* crypto_type                     149087\n* ticker                          149087\n* max_supply                     1545244\n* site_url                        177958\n* github_url                      705135\n* minable                         149087\n* platform_name                   784656\n* industry_name                   524799\n\nThat is not so good for us and we should decide what to do with that."}}