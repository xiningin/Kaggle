{"cell_type":{"87c522a1":"code","0bde03ba":"code","62a32ea6":"code","deeb64fe":"code","d5e39b40":"code","e08f0bae":"code","e6127348":"code","e26df3fe":"code","c776210b":"code","51e3a5cb":"code","c86540f5":"code","fdf7d2f5":"code","93a55913":"code","c293bcca":"code","7859b9c5":"code","6e22a0bd":"code","c27afd5d":"code","db26c9cb":"code","ff194b48":"code","d61ff7f9":"code","13861167":"code","b1b480ac":"code","9742de29":"code","3d04ed97":"code","a395ce43":"code","70084ba9":"code","2d9fba50":"code","4d2cef09":"code","e583bf6c":"code","1ced8420":"code","696c2e0b":"code","527e55e3":"code","dd56f842":"code","403fafe3":"code","dc207f11":"code","38b648a1":"code","698ee5d5":"code","d6e56edb":"code","c8f9c5ce":"code","08fd2000":"code","67d1a1c4":"code","11abce40":"code","6aa22a8a":"code","9c629166":"code","9389be1a":"code","094a4450":"code","3caba64f":"code","ae761fff":"code","1efeca6b":"code","d3d69909":"code","efea6193":"code","22053743":"code","830b0973":"code","025f9f41":"code","6bfabe0a":"code","53832320":"code","4e2d1ca6":"markdown","4915e0d8":"markdown","21b80f8b":"markdown","bfb5e541":"markdown","a1134175":"markdown"},"source":{"87c522a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0bde03ba":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport pandas_profiling\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","62a32ea6":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","deeb64fe":"#Numberical column analysis\ntrain_df.describe()","d5e39b40":"#String describe\ntrain_df.describe(include=['O'])","e08f0bae":"train_df[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived', ascending=False)","e6127348":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e26df3fe":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c776210b":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","51e3a5cb":"# Data visualization by age\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)\n# consider age in model training.\n# complete age feature for null values.\n# band age groups.","c86540f5":"#Infants (Age <=4) had high survival rate = 67.5%\ntf1 = train_df[['Age','Survived']]\ntf2 = tf1.loc[tf1['Age']<=4]\ntf2['Survived'].mean()","fdf7d2f5":"# Observations\n# Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n# Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n# Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\ngrid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();\n#consider Pclass for model traning.","93a55913":"# Correlating categorical features\n\n# Observations\n# Female passengers had much better survival rate than males. Confirms classifying (#1).\n# Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n# Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n# Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\ngrid = sns.FacetGrid(train_df, col='Embarked')\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()\n# add sex feature to model training\n# Complete and add embarked feature to model training.","c293bcca":"#Correlating categorical and numerical features(not-num:Embarded,Sex; continuous#: Fare)\n\n#Observation\n#Higher fare paying passengers had better survival. \n#Port of embarkation correlates with survival rates.\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()\n#Consider banding fare feature","7859b9c5":"#Wrangle data (dropping features)\nprint(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\nprint(\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)","6e22a0bd":"# Creating new feature extracting from existing (remove name,id not related replace by title)\n# Most titils band age groups accurately. Master title has age mean of 5 years.\n# Survival among Title age bands varies slightly.\n# Ceatin titles mostly survived(Mme, Lady, Sir) not(Don,Rev,Jonkheer)\n## We decide to retain new title feature for model training.\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(train_df['Title'],train_df['Sex']) #(index, columns)","c27afd5d":"# Master title has age mean of 5 years.\ntrain_df.loc[train_df['Title']=='Master']['Age'].mean()","db26c9cb":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","ff194b48":"# convert the categorical titles to ordinal.\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df.head()","d61ff7f9":"# drop name,id from train; drop name from test\ntrain_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","13861167":"#Converting a categorical feature Sex:String->int;'female': 1, 'male': 0\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntrain_df.head(7)","b1b480ac":"guess_ages = np.zeros((2,3))\nguess_ages","9742de29":"#Guess age == NaN based on Pclass x Gender\n#We assign median of the same class on (Sex,Pclass) to age==NaN column\n#Calculate(six-mean)\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n            #age_guess = guess_df.median()\n            age_guess = guess_df.median()\n            guess_ages[i,j] = int(age_guess\/0.5 + 0.5 ) * 0.5 #transfer it to int by .5 age\n    #assgin(six-mean) to approperiate age=NaN column\n    for i in range(0, 2):\n        for j in range(0, 3):            \n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain_df.head(6) #We predict an age based on this","3d04ed97":"#Turns continuous Age to 5 ordinal classes based bands\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","a395ce43":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain_df.head()","70084ba9":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","2d9fba50":"# Create new feature combining existing features Familysize=Parch+SibSp-> isalone(0,1)\n# Group of 2-4 are easier to survive\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4d2cef09":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","e583bf6c":"train_df = train_df.drop(['Parch', 'SibSp','FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp','FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","1ced8420":"# Fill the null embarked with 'S' and map( {'S': 0, 'C': 1, 'Q': 2} \nfreq_port = train_df.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","696c2e0b":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","527e55e3":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","dd56f842":"# Turns the continuous fees to discrete classes (Create FareBand)\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","403fafe3":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","dc207f11":"train_df.head(10)","38b648a1":"test_df.head(10)","698ee5d5":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape\n","d6e56edb":"X_train.head()","c8f9c5ce":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","08fd2000":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","67d1a1c4":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_LR.csv', index=False)","11abce40":"# Support Vector Machines\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","6aa22a8a":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_SVM.csv', index=False)","9c629166":"# 3-nn\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","9389be1a":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_3nn.csv', index=False)","094a4450":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","3caba64f":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_GNB.csv', index=False)","ae761fff":"# Perceptron\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","1efeca6b":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_Percep.csv', index=False)","d3d69909":"# Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","efea6193":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_SGD.csv', index=False)","22053743":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","830b0973":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_DT.csv', index=False)","025f9f41":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100,max_depth=5)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","6bfabe0a":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission_RFC.csv', index=False)","53832320":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN','Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","4e2d1ca6":"### Correlating.\n1. We want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n### Completing.\n1. We may want to complete Age feature as it is definitely correlated to survival.\n2. We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n### Correcting\n1. Ticket feature be dropped.(duplicated)\n2. Cabin feature droppped (imcompleted)\n3. PassengerId be dropped (not contributed)\n4. Name feature be dropped (not contributed)\n### Creating (3.4 prevent overfiting)\n1. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n2. We may want to engineer the Name feature to extract Title as a new feature.\n3. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n4. We may also want to create a Fare range feature if it helps our analysis.\n### Classifying(assumption)\n1. Women (Sex=female) were more likely to have survived.\n2. Children (Age<*) were more likely to have survived.\n3. The upper-class passengers (Pclass=1) were more likely to have survived.","4915e0d8":"### Model, predict and solve\n* Logistic Regression \n* Support Vector Machines (parametic)\n* KNN or k-Nearest Neighbors (non-parametic)\n* Naive Bayes classifier (scalable)\n* Perceptron: supervised learning of binary classifiers, classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. **online learning**(processes elements in the training set one at a time) xxx\n* Stochastic Gradient Descent xxx\n* Decision Tree \n* * target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. (If age is continuous)\n* Random Forest (an ensemble learning method for \n* * classification(outputting the class that is the mode of the classes) \n* * regression (mean prediction of the individual trees) multitude of decision trees (n_estimators=100)","21b80f8b":"We replace many titles with more common name and classify them as Rare.","bfb5e541":"### Analyze by visualizing data\n* Correlating numerical features(Age)\n* Correlating numerical and ordinal features(Age, Pclass)\n* Correlating categorical and numberical features(Pclass,sex,emarked)\n> *Categorical:survived,sex,embarked(port you enter); Ordinal:Pclass(ticket class); \n> *Numberial:continuous:age,fare(fee);discrete:sibsp(siling\/spouse),parch","a1134175":"### Analyze by pivoting features\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n* **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\/\/\n*  **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n*  **SibSp** and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1)."}}