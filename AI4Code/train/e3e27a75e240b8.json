{"cell_type":{"d6cdb2b9":"code","59af986d":"code","e22562ff":"code","1e4bc50b":"code","3457ee01":"code","8c96a4c6":"code","3172f527":"code","bd2ff12a":"code","54cab530":"code","c91d6fee":"code","d76266b4":"code","9822b016":"code","32f330e9":"code","9e36424f":"code","952beffb":"code","0a6535bf":"code","403e43b8":"code","82148b67":"code","a603728c":"code","daa5bd3a":"code","89ea4f77":"code","7216c2f0":"code","982e5ae0":"code","32ed7ce0":"code","4f15a983":"code","6802310e":"code","5ff9ed07":"code","1819e0cb":"code","07c3003c":"code","c01fe598":"code","80a9a7c5":"code","d1680bfd":"code","bf715695":"code","4baacaf7":"code","1556df34":"code","60a2f32a":"code","3267cf8a":"code","3ca0c65b":"code","a7d6d60b":"code","f221a97f":"code","451c8318":"markdown","892160b0":"markdown","126b2e4d":"markdown","c4ec2996":"markdown","f21a5998":"markdown","8012d9e0":"markdown","844f9b29":"markdown","6116df09":"markdown","64e4eefd":"markdown","b1d45ab1":"markdown","8b807655":"markdown","949316da":"markdown","26c374e8":"markdown","659e6483":"markdown","b556c771":"markdown","7f93aaf1":"markdown","86bc221e":"markdown","b8c27f23":"markdown","c8247fb3":"markdown","b4b6130b":"markdown","6cdaa20d":"markdown","d95d4eca":"markdown","d903a92e":"markdown","0ec397cb":"markdown","7b3e0731":"markdown","2179aa96":"markdown","cc1acd13":"markdown"},"source":{"d6cdb2b9":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","59af986d":"train = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","e22562ff":"train.head()","1e4bc50b":"import torch","3457ee01":"\ndef model(t_u, w, b):\n    return w * t_u + b","8c96a4c6":"\nparams = torch.tensor([1.0, 0.0], requires_grad=True)","3172f527":"\nt_u = train.drop('quality', axis=1)\nt_c = train.quality","bd2ff12a":"t_u","54cab530":"t_c ","c91d6fee":"t_u.describe()","d76266b4":"t_un =(t_u-t_u.mean())\/t_u.std()","9822b016":"t_un","32f330e9":"t_un.describe()","9e36424f":"t_un2 = (t_u-t_u.min())\/(t_u.max()-t_u.min())","952beffb":"t_un2.describe()","0a6535bf":"t_un2","403e43b8":"t_un2 = torch.tensor(t_un2.values)\nt_c = torch.tensor(t_c.values)","82148b67":"n_samples = t_un2.shape[0]\nn_val = int(0.2 * n_samples)\n\nshuffled_indices = torch.randperm(n_samples)\n\ntrain_indices = shuffled_indices[:-n_val]\nval_indices = shuffled_indices[-n_val:]\n\ntrain_indices[:5], val_indices[:5]","a603728c":"t_u_train = t_un2[train_indices]\nt_c_train = t_c[train_indices]\n\nt_u_val = t_un2[val_indices]\nt_c_val = t_c[val_indices]","daa5bd3a":"\nt_u_train = t_u_train.unsqueeze(1)\nt_c_train = t_c_train.unsqueeze(1)\n\nt_u_val = t_u_val.unsqueeze(1)\nt_c_val = t_c_val.unsqueeze(1)","89ea4f77":"t_u_train, t_u_val = t_u_train.type(torch.FloatTensor), t_u_val.type(torch.FloatTensor)\nt_c_train, t_c_val = t_c_train.type(torch.FloatTensor), t_c_val.type(torch.FloatTensor)","7216c2f0":"t_u_train.shape, t_u_val.shape","982e5ae0":"t_c_train.shape, t_c_val.shape","32ed7ce0":"params = torch.tensor([1.0, 0.0], requires_grad=True)\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD([params], lr=learning_rate)","4f15a983":"def training_loop(n_epochs=3000, optimizer= optimizer, params=params,train_t_u= t_u_train, val_t_u=t_u_val, train_t_c=t_c_train, val_t_c=t_c_val, loss_fn=torch.nn.MSELoss(),model=model):\n    for epoch in range(1, n_epochs+1):\n        train_t_p = model(train_t_u, *params)\n        train_loss = loss_fn(train_t_p, train_t_c)\n        \n        val_t_p = model(val_t_u, *params)\n        val_loss = loss_fn(val_t_p, val_t_c)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        if epoch <= 3 or epoch % 500 == 0:\n            print(\"Epoch: \", epoch, \" Training loss: \", train_loss.item(), \" Val loss \", val_loss.item())\n    return params\n    ","6802310e":"optimal_params = training_loop()\n%time","5ff9ed07":"seq_model = torch.nn.Sequential(\n    torch.nn.Linear(11,13),\n    torch.nn.Tanh(),\n    torch.nn.Linear(13,1),\n)\n\nseq_model","1819e0cb":"[param.shape for param in seq_model.parameters()]","07c3003c":"t_u_train.shape, t_c_train.shape, t_u_val.shape, t_c_val.shape","c01fe598":"def training_loop(n_epochs=3000, optimizer= optimizer, params=params,train_t_u= t_u_train, val_t_u=t_u_val, train_t_c=t_c_train, val_t_c=t_c_val, loss_fn=torch.nn.MSELoss(),model=model):\n    for epoch in range(1, n_epochs+1):\n#         train_t_p = model(train_t_u, *params)\n        train_t_p = model(train_t_u)\n        train_loss = loss_fn(train_t_p, train_t_c)\n        \n#         val_t_p = model(val_t_u, *params)\n        val_t_p = model(val_t_u)\n        val_loss = loss_fn(val_t_p, val_t_c)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        if epoch <= 3 or epoch % 500 == 0:\n            print(\"Epoch: \", epoch, \" Training loss: \", train_loss.item(), \" Val loss \", val_loss.item())\n#     return params\n    ","80a9a7c5":"optimizer = torch.optim.SGD(seq_model.parameters(), lr=1e-3)\n\ntraining_loop(\n    n_epochs=5000,\n    optimizer=optimizer,\n    model = seq_model,\n    loss_fn=torch.nn.MSELoss(),\n)\n\n%time","d1680bfd":"print('output', seq_model(t_u_val))","bf715695":"print('answer', t_c_val)","4baacaf7":"from matplotlib import pyplot as plt\n\nt_range = torch.arange(20., 90.).unsqueeze(1)\nfig = plt.figure(dpi=600)\n\ntry: \n#     plt.xlabel(\"X\")\n#     plt.ylabel(\"y\")\n#     plt.plot(t_u_train.numpy(), t_c_train.numpy(), 'o')\n    plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\n    plt.plot(t_u_train.numpy(), seq_model(0.1 * t_u_train).detach().numpy(), 'kx')\nexcept Exception as e:\n    print(e)","1556df34":"len(t_u_val)","60a2f32a":"t_u_val[289]","3267cf8a":"optimal_params","3ca0c65b":"model(t_c_val[289], *optimal_params)","a7d6d60b":"seq_model(t_u_val[289])","f221a97f":"t_c_val[289]","451c8318":"using mean normalisation","892160b0":"these are our x'es - input","126b2e4d":"lets look at the parameters","c4ec2996":"lets ensure all of the tensors are float now.","f21a5998":"lets get the tensors out of training data","8012d9e0":"We see that that the columns are not in the same mean range so we need to normalise the data first.","844f9b29":"### Lets start with a linear model","6116df09":"### preprocessing","64e4eefd":"So our preprocessing of tensors is complete.","b1d45ab1":"Lets compare the two models","8b807655":"Lets see what linear model gives","949316da":"`describe` function helps us to know more about our dataset. look at the mean column, we want it to be in the similar range","26c374e8":"It will be interesting if we finda way to plot all 11 values of x. Lets make some predictions.","659e6483":"creating the simplest linear model","b556c771":"initialising w and b","7f93aaf1":"### Now comes the training loop.","86bc221e":"using min max normalisation****","b8c27f23":"Now the neural net model","c8247fb3":"### creating a validation set\n\nwe need to do this before we get into the training loop","b4b6130b":"I personally like min max normalisation.\nNote that we are not normalising `t_c`. (should we?)\n ","6cdaa20d":"one last thing would be that the input dimension is different from the target dimension. usually pytorch will give a broadcasting warning for this, but lets decide to use the tensor as this.","d95d4eca":"### getting our data","d903a92e":"`t_c` is our target","0ec397cb":"Wine quality through hand made simple models.","7b3e0731":"### creating tensors\n\nNext step is to convert `t_un2` and `t_c` into tensors of right format to be used as input to the linear model.\n","2179aa96":"It does take a lot of time. But lets try using a neural network next.","cc1acd13":"Lets check all the sizes of parameters first."}}