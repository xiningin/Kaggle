{"cell_type":{"c34e26a3":"code","294c0176":"code","02e1d7d8":"code","247ec299":"code","16c07a39":"code","3f095d6f":"code","4ca4def6":"code","fad77c24":"code","eaaaa092":"code","35d71d9c":"code","f528f423":"code","6079a335":"code","6dba4388":"code","e75682e1":"code","7d39f510":"code","6c6722a8":"code","d0f2c2f6":"code","7a129441":"code","2546440a":"code","cfe78b79":"code","74a62f9e":"code","7da09aa4":"code","8645fcbf":"code","bf2bd17b":"code","c0edab01":"code","c302a81e":"code","ee260187":"code","fff2f42b":"code","bed67659":"code","932a77ad":"code","2583290d":"code","6aa1ed6e":"code","6dd634ce":"code","e5f3eaf9":"code","1d68bb53":"code","e87c71ef":"code","3e3330be":"code","c0995753":"code","2c6d43f9":"code","6744ca3f":"code","48367382":"code","cff36f76":"code","fe2b21ac":"code","f279600d":"code","37895d03":"code","036ed2b8":"code","67380d39":"code","db31ce9b":"code","a06eea87":"code","921f6150":"code","87c9f88e":"code","d4f3b929":"code","46c06182":"code","cf5a7953":"code","12704bd9":"code","231f7bed":"code","839a78fc":"code","e49cb720":"code","72410693":"code","3186d368":"code","ab9abf3b":"code","8914adb3":"code","8fa29b61":"code","e5ea1361":"code","5a6a5f48":"code","f57674e5":"code","4d2cc5a3":"code","30820358":"code","088bd443":"code","b7cd8cc9":"code","db78ff13":"code","4fe56a0e":"code","31db7173":"code","6c0bd4c9":"code","957e90a3":"code","f88d014e":"code","81cfa0c7":"code","71af2695":"code","96732622":"code","5a687c85":"code","14f8dbe7":"code","676bb21d":"code","1fc9efa7":"code","96ed4114":"code","0d9fd541":"code","df8033e0":"code","c8d9f97b":"code","b57ae7c5":"code","aae880ba":"code","8f4666e4":"code","ff27fc6d":"code","414ad7ec":"code","c1765480":"code","c4486685":"markdown","9ad5e69e":"markdown","cbbcc5a7":"markdown","b11b6100":"markdown","008fcb25":"markdown","64bfdadd":"markdown","6d491429":"markdown","feb7f283":"markdown","4abd2b27":"markdown","cf9ff084":"markdown","44c7790c":"markdown","a9a1c6c3":"markdown","a4d6712a":"markdown","586be351":"markdown","a312bcc9":"markdown","34529ca6":"markdown","fd629ab0":"markdown","a929a7a2":"markdown","7b8246c0":"markdown","ffc7c57d":"markdown","026cb9b3":"markdown","f183eb35":"markdown","a0332be3":"markdown","4c02592e":"markdown","01479731":"markdown","d3eac72b":"markdown","6fc3e4fa":"markdown","9f581d25":"markdown","2f3f355f":"markdown","864963f1":"markdown","da2142a6":"markdown","fdccc6f0":"markdown","3ae47148":"markdown","5dc81b74":"markdown","79d601e1":"markdown","c8cda8db":"markdown","d6469dbe":"markdown","b6580104":"markdown","b3a57369":"markdown","b461226e":"markdown","e8c134e8":"markdown","05d9d8e5":"markdown","04adda33":"markdown","700f2787":"markdown","0f2c1e59":"markdown","b9bc79f6":"markdown","5a10b547":"markdown","d194a1a4":"markdown","57a423a6":"markdown"},"source":{"c34e26a3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os","294c0176":"import os, seaborn as sns, pandas as pd, numpy as np\n#os.chdir('\/Utenti\/marionascitini\/Download\/')\n\nfilepath_train = '..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv'\nfilepath_test = '..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv'\nfilepath_submission = '..\/input\/hr-analytics-job-change-of-data-scientists\/sample_submission.csv'\n\noutput_file='..\/output\/kaggle\/working\/test_data_predictions.csv'\ndf_train = pd.read_csv(filepath_train, sep=',')\ndf_test = pd.read_csv(filepath_test, sep=',')\ndf_submission=pd.read_csv(filepath_submission,sep=',')","02e1d7d8":"orig_test=df_test.copy()","247ec299":"df_test.head()","16c07a39":"df_train.info()","3f095d6f":"df_test.info()","4ca4def6":"df_train.dtypes.value_counts()","fad77c24":"sns.countplot(x=df_train['target']);\nprint(df_train.target.value_counts(normalize=True))\n","eaaaa092":"object_features = df_train.select_dtypes(include=['object', 'bool']).columns.values\n\ntarget_feature=['target']\n\n\nbinary_features=list() \nfor col in object_features:\n    if len(df_train[col].unique())==2:\n      binary_features.append(col)\nbinary_features=list(set(binary_features)-set(target_feature))\n\ncategorical_features=list(set(object_features)-set(binary_features)-set(target_feature))    \n\nnumerical_features=df_train.select_dtypes(include=['int64','float64']).columns.values\nnumerical_features=list(set(numerical_features)-set(target_feature))\n\nordinal_features=['education_level','experience','last_new_job','company_size','enrolled_university']\nnominal_features=list(set(categorical_features)-set(ordinal_features))","35d71d9c":"print('numerical features:',numerical_features)\nprint('\\n')\nprint('categorical binary features:',binary_features)\nprint('\\n')\nprint('categorical nominal features:',nominal_features)\nprint('\\n')\nprint('categorical ordinal features:',ordinal_features)\nprint('\\n')\nprint('target feature:',target_feature)","f528f423":"df_uniques = pd.DataFrame([[i, len(df_train[i].unique())] for i in df_train.columns], columns=['Variable', 'Unique Values']).set_index('Variable')\ndf_uniques","6079a335":"for col in numerical_features:\n    print(col, \"(\", len(df_train[col].unique()) , \"values):\\n\", df_train[col].unique())\n    ","6dba4388":"for col in binary_features:\n    print(col, \"(\", len(df_train[col].unique()) , \"values):\\n\", df_train[col].unique())","e75682e1":"for col in nominal_features:\n    print(col, \"(\", len(df_train[col].unique()) , \"values):\\n\", df_train[col].unique())\n\n","7d39f510":"for col in ordinal_features:\n    print(col, \"(\", len(df_train[col].unique()) , \"values):\\n\", df_train[col].unique())\n","6c6722a8":"print('# of numerical features with missing values:\\n',df_train[numerical_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of binary features with missing values:\\n',df_train[binary_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of categorical features with missing values:\\n',df_train[categorical_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of target feature with missing values:\\n',df_train[target_feature].isnull().sum().sort_values(ascending=False))","d0f2c2f6":"print('# of numerical features with missing values:\\n',df_test[numerical_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of binary features with missing values:\\n',df_test[binary_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of categorical features with missing values:\\n',df_test[categorical_features].isnull().sum().sort_values(ascending=False))\n","7a129441":"df_train.dropna(axis=0,inplace=True)\n#df_test.dropna(axis=0,inplace=True)","2546440a":"df_test.isnull().sum()","cfe78b79":"pip install feature_engine","74a62f9e":"\n#Using feature_engine  RandomSampleImputer\nfrom feature_engine.imputation import RandomSampleImputer\nimputer = RandomSampleImputer(\n        random_state=101,\n        seed='general',\n        seeding_method='add'\n    )\n\n# fit the imputer\n#imputer.fit(df_train)\n# transform the data\n#df_train = imputer.transform(df_train)\n\nimputer.fit(df_test)\ndf_test = imputer.transform(df_test)\n\n\n#Using sklearn  SimpleImputer\n#from sklearn.impute import SimpleImputer\n#si=SimpleImputer(strategy=\"most_frequent\")\n#df_train[categorical_features]=si.fit_transform(df_train[categorical_features])\n#df_test[categorical_features]=si.fit_transform(df_test[categorical_features])\n\n#from sklearn.impute import SimpleImputer\n#si=SimpleImputer(strategy='constant',fill_value='Unknown')\n#df_train[categorical_features]=si.fit_transform(df_train[categorical_features])\n\n# Coding manually\n#def fill_with_mode(data,features):\n#  for col in features:\n#    if data[col].isnull().sum()>0:\n#      data[col]=data[col].fillna(data[col].value_counts().index[0])\n\n#def fill_with_unknown(data,features):\n#    for col in features:\n#        if data[col].isnull().sum()>0:\n#            data[col]=data[col].fillna('Unknown')\n\n#fill_with_mode(df_train,categorical_features)\n#fill_with_mode(df_test,categorical_features)\n#fill_with_unknown(df_train,categorical_features)\n#fill_with_unknown(df_test,categorical_features)","7da09aa4":"df_test.isnull().sum()","8645fcbf":"print('# of numerical features with missing values:\\n',df_train[numerical_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of binary features with missing values:\\n',df_train[binary_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of categorical features with missing values:\\n',df_train[categorical_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of target feature with missing values:\\n',df_train[target_feature].isnull().sum().sort_values(ascending=False))","bf2bd17b":"print('# of numerical features with missing values:\\n',df_test[numerical_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of binary features with missing values:\\n',df_test[binary_features].isnull().sum().sort_values(ascending=False))\nprint('\\n # of categorical features with missing values:\\n',df_test[categorical_features].isnull().sum().sort_values(ascending=False))\n","c0edab01":"df_test['gender'].value_counts()","c302a81e":"index_of_other_gender=df_train[df_train['gender']=='Other'].index \ndf_train.drop(axis=0,index=index_of_other_gender,inplace=True)\n#index_of_other_gender=df_test[df_test['gender']=='Other'].index \n#df_test.drop(axis=0,index=index_of_other_gender,inplace=True)\n#df_test[df_test['gender']=='Other']='Male'\n#df_train['gender'].value_counts()","ee260187":"df_test['gender'].value_counts()","fff2f42b":"df_test[\"gender\"].replace({\"Other\":\"Male\"},inplace=True)\n","bed67659":"df_test['gender'].value_counts()","932a77ad":"binary_features=list(set.union(set(binary_features),set(['gender'])))","2583290d":"binary_features","6aa1ed6e":"nominal_features=list(set(nominal_features)-set(['gender']))\nnominal_features","6dd634ce":"for col in binary_features:\n    plt.figure(figsize=(20,50))    \n    sns.catplot(x=col, kind=\"count\", data=df_train)    \n    plt.title(col)    \n    plt.tight_layout()","e5f3eaf9":"#sns.histplot(data=df, x=\"marital\", color=\"lime\",hue='subscribe')\nfor col in nominal_features:\n    plt.figure(figsize=(20,50))    \n    sns.catplot(x=col, kind=\"count\", data=df_train)    \n    plt.title(col)    \n    plt.tight_layout()","1d68bb53":"#sns.histplot(data=df, x=\"marital\", color=\"lime\",hue='subscribe')\nfor col in ordinal_features:\n    plt.figure(figsize=(20,50))    \n    sns.catplot(x=col, kind=\"count\", data=df_train)    \n    plt.title(col)    \n    plt.tight_layout()","e87c71ef":"#for col in ordinal_features:\n#    df_train = pd.concat([df_train,pd.get_dummies(df_train[col], prefix=col)],axis=1)\n#    df_train.drop([col],axis=1, inplace=True)","3e3330be":"df_train['education_level']=df_train['education_level'].map({'Primary School': 1, 'High School': 2,'Graduate': 3, 'Masters':4, 'Phd': 5})\ndf_test['education_level']=df_test['education_level'].map({'Primary School': 1, 'High School': 2,'Graduate': 3, 'Masters':4, 'Phd': 5})\ndf_train['last_new_job']=df_train['last_new_job'].map({'1': 1, '2': 2,'3': 3, '4':4, '>4': 5, 'never': 0})\ndf_test['last_new_job']=df_test['last_new_job'].map({'1': 1, '2': 2,'3': 3, '4':4, '>4': 5, 'never': 0})\ndf_train['experience']=df_train['experience'].map({'<1': 0, '1': 1,'2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'10': 10,'11': 11,'12': 12,'13': 13,'14': 14,'15': 15,'16': 16,'17': 17,'18': 18,'19': 19,'20': 20,'>20': 21})\ndf_test['experience']=df_test['experience'].map({'<1': 0, '1': 1,'2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'10': 10,'11': 11,'12': 12,'13': 13,'14': 14,'15': 15,'16': 16,'17': 17,'18': 18,'19': 19,'20': 20,'>20': 21})\ndf_train['company_size']=df_train['company_size'].map({'<10': 1, '10\/49': 2,'50-99': 3,'100-500': 4,'500-999': 5,'1000-4999': 6,'5000-9999': 7,'10000+': 8})\ndf_test['company_size']=df_test['company_size'].map({'<10': 1, '10\/49': 2,'50-99': 3,'100-500': 4,'500-999': 5,'1000-4999': 6,'5000-9999': 7,'10000+': 8})\ndf_train['enrolled_university']=df_train['enrolled_university'].map({'no_enrollment': 1, 'Part time course': 2,'Full time course': 3})\ndf_test['enrolled_university']=df_test['enrolled_university'].map({'no_enrollment': 1, 'Part time course': 2,'Full time course': 3})\n","c0995753":"df_train[\"relevent_experience\"].replace({\"Has relevent experience\":\"YES\",\"No relevent experience\":\"NO\"},inplace=True)","2c6d43f9":"df_test[\"relevent_experience\"].replace({\"Has relevent experience\":\"YES\",\"No relevent experience\":\"NO\"},inplace=True)","6744ca3f":"df_test","48367382":"binary_features","cff36f76":"from sklearn.preprocessing import LabelEncoder,LabelBinarizer\nle = LabelEncoder()\nfor col in binary_features:\n    le.fit(df_train[col])\n    df_train[col]=le.transform(df_train[col])\n    #le.fit(df_test[col])\n    df_test[col]=le.transform(df_test[col])\n","fe2b21ac":"nominal_features","f279600d":"df_test","37895d03":"id_test=df_test['enrollee_id']","036ed2b8":"df_train.drop(columns=['enrollee_id','city'],axis=1,inplace=True)\ndf_test.drop(columns=['enrollee_id','city'],axis=1,inplace=True)\n","67380d39":"nominal_features=['major_discipline','company_type']\nnominal_features","db31ce9b":"print(df_train['major_discipline'].value_counts())\nprint(df_train['major_discipline'].value_counts().isnull().sum())","a06eea87":"print(df_train['company_type'].value_counts())\nprint(df_train['company_type'].value_counts().isnull().sum())","921f6150":"df_train=pd.get_dummies(df_train,columns=nominal_features,drop_first=True)","87c9f88e":"df_test=pd.get_dummies(df_test,columns=nominal_features,drop_first=True)","d4f3b929":"#from sklearn.utils import resample\n\n#def df_sample(data_frame,num_samples):\n#    dataset_majority = data_frame[data_frame.target == 0]\n#    dataset_minority = data_frame[data_frame.target == 1]\n    # Downsample majority class\n#    df_majority_downsampled = resample(dataset_majority, replace=False,\n#                                   n_samples=num_samples, random_state=123)\n\n#    data_frame_downsampled = pd.concat([df_majority_downsampled, dataset_minority])\n#    return data_frame_downsampled\n\n#dataset_majority = y_train[y_train == 0]\n#dataset_minority = y_train[y_train == 1]\n\n#Downsample majority class\n#df_majority_downsampled = resample(dataset_majority, replace=False,\n#                                   n_samples=4198, random_state=123)\n\n#y_train_downsampled = pd.concat([df_majority_downsampled, dataset_minority])\n","46c06182":"#len_min_class=len(df_train[df_train['target']==1])\n#len_maj_class=len(df_train[df_train['target']==0])\n\n#print('length of minority class:', len_min_class)\n#print('length of majority class:', len_maj_class)\n","cf5a7953":"#pip install imblearn","12704bd9":"#df_train_resampled=df_sample(df_train,len_min_class)\n#df_train_resampled.target.value_counts()\n#df_train_resampled","231f7bed":"#X=df_train_resampled.drop(['target'],axis=1)\n#y=df_train_resampled['target']\nX=df_train.drop(['target'],axis=1)\ny=df_train['target']\ncolumns=X.columns\n","839a78fc":"X.shape","e49cb720":"from imblearn.over_sampling import SMOTE, ADASYN\nX_resampled, y_resampled = SMOTE().fit_resample(X, y)\n","72410693":"X_resampled.shape","3186d368":"y_resampled.value_counts()","ab9abf3b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)","8914adb3":"y_train.value_counts()","8fa29b61":"from sklearn.preprocessing import MinMaxScaler\nmm=MinMaxScaler(copy=False)\ncols=X_train.columns\nX_train[cols] = mm.fit_transform(X_train[cols])\nX_test[cols]=mm.transform(X_test[cols])\n#X_train_scaled=pd.DataFrame(mm.fit_transform(X_train),columns=X_train.columns) \n#X_test_scaled=pd.DataFrame(mm.transform(X_test),columns=X_test.columns) \n#X_train_scaled=pd.DataFrame(X_train_scaled,columns=X.columns)\n#X_test_scaled=pd.DataFrame(X_test_scaled,columns=X.columns)","e5ea1361":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import classification_report\n\ndef view_scores(test,pred):\n  print(\"Accuracy score:\", accuracy_score(test,pred))\n  print(\"Classification report\")\n  print(classification_report(test,pred))\n\n  # Confusion Matrix\n  print(\"Confusion Matrix:\")\n  print(confusion_matrix(test,pred))\n\n  conf_mat = confusion_matrix(test,pred)\n  ax = plt.subplot()\n  sns.heatmap(conf_mat, annot=True, ax=ax, fmt='d')\n  #labels, title and ticks\n  ax.set_xlabel('Predicted labels')\n  ax.set_ylabel('True labels')\n  ax.set_title('Confusion Matrix')\n  ax.xaxis.set_ticklabels(['no', 'yes'])\n  ax.yaxis.set_ticklabels(['no', 'yes'])\n  plt.show()\n","5a6a5f48":"# Standard logistic regression without downsampling\nlr = LogisticRegression(C=1,solver='liblinear',penalty='l2',class_weight='balanced',random_state=31) \nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\ny_proba_lr=lr.predict_proba(X_test)\nview_scores(y_test,y_pred_lr)","f57674e5":"df_test.shape","4d2cc5a3":"df_train.columns","30820358":"from sklearn.linear_model import LogisticRegressionCV\nlr_l1 = LogisticRegressionCV(Cs=10, cv=5, penalty='l1', solver='liblinear').fit(X_train, y_train)\nlr_l2 = LogisticRegressionCV(Cs=10, cv=5, penalty='l2', solver='liblinear').fit(X_train, y_train)\ny_pred_lr_l1=lr_l1.predict(X_test)\ny_pred_lr_l2=lr_l2.predict(X_test)\nview_scores(y_test,y_pred_lr_l1)\nview_scores(y_test,y_pred_lr_l2)\n","088bd443":"# Decision Tree Classifier model\ndt = DecisionTreeClassifier(criterion='entropy',splitter='best',max_depth=50)\ndt=dt.fit(X_train,y_train)\ny_pred_dt=dt.predict(X_test)\nview_scores(y_test,y_pred_dt)","b7cd8cc9":"dt.tree_.node_count, dt.tree_.max_depth","db78ff13":"def measure_error(y_true, y_pred, label):\n    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred),\n                      'recall': recall_score(y_true, y_pred),\n                      'f1': f1_score(y_true, y_pred)},\n                      name=label)","4fe56a0e":"y_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)\n\ntrain_test_full_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n                              measure_error(y_test, y_test_pred, 'test')],\n                              axis=1)\n\ntrain_test_full_error\n","31db7173":"dt.feature_importances_","6c0bd4c9":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth':range(1, dt.tree_.max_depth+1, 2),\n              'max_features': range(1, len(dt.feature_importances_)+1)}\n\nGR = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  n_jobs=-1)\n\nGR = GR.fit(X_train, y_train)","957e90a3":"y_pred_gr = GR.predict(X_test)\nview_scores(y_test,y_pred_gr)","f88d014e":"from sklearn.svm import LinearSVC\n\nLSVC = LinearSVC()\nLSVC.fit(X_train, y_train)\ny_pred_svc=LSVC.predict(X_test)\nview_scores(y_test,y_pred_svc)","81cfa0c7":"knn = KNeighborsClassifier(n_neighbors=10,weights='distance')\nknn = knn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\nview_scores(y_test,y_pred_knn)","71af2695":"from sklearn import metrics\nmetrics.plot_roc_curve(lr, X_test, y_test)  \nplt.show()                                  \n\nmetrics.plot_precision_recall_curve(lr, X_test, y_test)  \nplt.show()                                  \n\n","96732622":"metrics.plot_roc_curve(dt, X_test, y_test)  \nplt.show()      \n\nmetrics.plot_precision_recall_curve(dt, X_test, y_test)  \nplt.show()                                  \n","5a687c85":"from sklearn import metrics\nmetrics.plot_roc_curve(knn, X_test, y_test)  \nplt.show()           \nmetrics.plot_precision_recall_curve(knn, X_test, y_test)  \nplt.show()                                  \n","14f8dbe7":"from sklearn.ensemble import GradientBoostingClassifier","676bb21d":"error_list = list()\n\ntree_list = [200, 400,500,600,700,800,900,1000]\nfor n_trees in tree_list:\n    \n    # Initialize the gradient boost classifier\n    GBC = GradientBoostingClassifier(max_features=5,n_estimators=n_trees, random_state=42)\n\n    # Fit the model\n    print(f'Fitting model with {n_trees} trees')\n    GBC.fit(X_train.values, y_train.values)\n    y_pred_gbc = GBC.predict(X_test)\n\n    # Get the error\n    error = 1.0 - accuracy_score(y_test, y_pred_gbc)\n    \n    # Store it\n    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n\nerror_df = pd.concat(error_list, axis=1).T.set_index('n_trees')\n\nerror_df","1fc9efa7":"view_scores(y_test,y_pred_gbc)","96ed4114":"metrics.plot_roc_curve(GBC, X_test, y_test)  \nplt.show()           \nmetrics.plot_precision_recall_curve(GBC, X_test, y_test)  \nplt.show()                                  \n","0d9fd541":"cols=df_test.columns\ndf_test[cols] = mm.fit_transform(df_test[cols])","df8033e0":"test_predictions=GBC.predict(df_test)","c8d9f97b":"df_test_pred=pd.DataFrame(test_predictions,columns=['target'])","b57ae7c5":"df_id=pd.DataFrame(id_test)\ndf_id_new=df_id.reset_index()\ndf_id_new.drop('index',axis=1,inplace=True)","aae880ba":"df_pred_final=pd.concat([df_id_new,df_test_pred],axis=1)","8f4666e4":"df_pred_final.target.value_counts()","ff27fc6d":"sns.countplot(x=df_pred_final['target']);\nprint(df_pred_final.target.value_counts(normalize=True))\n","414ad7ec":"df_pred_final","c1765480":"df_pred_final.to_csv('test_data_predictions.csv')","c4486685":"# Decision Tree Classifier","9ad5e69e":"**Now we have to solve the problem of unbalanced data on target variable.We can \"downsample\" the majority class or \"oversample\" the minority class.\nWe tried both but the oversample gives better results on model scoring**","cbbcc5a7":"# Let's plot ordinal features distribution of values","b11b6100":"# **Importing libraries**","008fcb25":"# Decision Tree Classifier with GridSearchCV","64bfdadd":"# Prepare modeling: import libraries and define score function ","6d491429":"**Define the numerical and categorical features**","feb7f283":"# Build Model and Predict\n","4abd2b27":"# **Dataset description**","cf9ff084":"We have:\n\n**3 quantitative (numerical) indipendent features**\n\n**10 qualitative (categorical) indipendent features**\n\n**1 output variable (binary: 1\/0)**","44c7790c":"**Analyze \"gender\" feature**","a9a1c6c3":"# Let's encode the binary features","a4d6712a":"# Save the \"enrollee_id\" for finale test predictions","586be351":"**Choose \"Dummy encoding\"**","a312bcc9":"# KNN","34529ca6":"**Must scale the final test data too.**","fd629ab0":"# Simple Logistic Regression","a929a7a2":"# Linear SVC","7b8246c0":"# Oversampling minority class","ffc7c57d":"**One Hot Encoding**","026cb9b3":"> Note that the **dataset is unbalanced on the target variable**.\n\n> **75%** of enrollees **did not look a job change** \n> \n> **25%** of enrollees **look a job change**","f183eb35":"**Nominal features unique values**","a0332be3":"# **Loading data**","4c02592e":"**Ordinal features unique values**","01479731":"# Predict  test data contained in \"aug_test.csv\" ","d3eac72b":"**For final test data we do not delete missing values but we use a RandomSampleImputer**","6fc3e4fa":"**Manual Mapping**","9f581d25":"# Prepare out dataset for train\/test split","2f3f355f":"**Count of the missing values for each variable of the train set**","864963f1":"We have two choices: \n1) to do a one hot encoding \n\n2) **to implement a manual mapping on ordered values (choose this one)**","da2142a6":"# Let's plot nominal features distribution of values","fdccc6f0":"**Unique values for numerical features**","3ae47148":"# Let's drop 'enrollee_id' and 'city'features","5dc81b74":"# Train\/Test Splitting","79d601e1":"Now let's compare the ROC-AUC Curve for these 3 models.","c8cda8db":"We have missing values only on categorical features. \n4 categorical features with many missing values.\nWe can: replace these missing values in three ways: \n\n1) replace missing values with the \"mode\" value for each feature (see \"fill_with_mode\" function)\n \n2) replace missing values with a specific \"unknown\" value for each feature (see \"fill_with_unknown\" function)\n\n3) replace missing values with random sampling on other values (see feature_engine RandomSampler code)\n \n**4) delete all rows with missing values (choose this one)**","d6469dbe":"# Let's encode the ordinal categorical features","b6580104":"# Scaling our train\/test data","b3a57369":"**Binary features unique values**","b461226e":"# Gradient Boost Classifier","e8c134e8":"# **HR Analytics Project** \n\n**by Mario Nascitini**","05d9d8e5":"# Let's plot binary features distribution of values","04adda33":"# **Exploratory Data Analysis**","700f2787":"**Output variable (desired target for prediction):**\n\ntarget - Looking for job change? (binary: 1:\"yes\",0:\"no\")","0f2c1e59":"The value \"Other\" for gender is negligible. Moreover the target values for these records respect the same proportion of the entire dataset.\nSo we can drop rows with gender=\"Other\"\nFor final test data we set value of \"Other\" with the most frequent \"Male\"","b9bc79f6":"# Write to submission csv file","5a10b547":"# Let's encode nominal features","d194a1a4":"**Context and Content**\n\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n\nThis dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials,demographics,experience data you will predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision.\n\nThe whole data divided to train and test . Target isn't included in test but the test target values data file is in hands for related tasks. A sample submission correspond to enrollee_id of test set provided too with columns : enrollee _id , target\n\nNote:\n\nThe dataset is imbalanced.\nMost features are categorical (Nominal, Ordinal, Binary), some with high cardinality.\nMissing imputation can be a part of your pipeline as well.\nFeatures\n\nenrollee_id : Unique ID for candidate\n\ncity: City code\n\ncity_ development _index : Developement index of the city (scaled)\n\ngender: Gender of candidate\n\nrelevent_experience: Relevant experience of candidate\n\nenrolled_university: Type of University course enrolled if any\n\neducation_level: Education level of candidate\n\nmajor_discipline :Education major discipline of candidate\n\nexperience: Candidate total experience in years\n\ncompany_size: No of employees in current employer's company\n\ncompany_type : Type of current employer\n\nlastnewjob: Difference in years between previous job and current job\n\ntraining_hours: training hours completed\n\ntarget: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change\n\n**Inspiration**\n\n**Predict** the probability of a candidate will work for the company\n\n**Interpret** model(s) such a way that illustrate which features affect candidate decision\n\nPlease refer to the following task for more details:\nhttps:\/\/www.kaggle.com\/arashnic\/hr-analytics-job-change-of-data-scientists\/tasks?taskId=3015","57a423a6":"# Logistic Regression with cross validation"}}