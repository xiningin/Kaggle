{"cell_type":{"0646ec25":"code","c42904f1":"code","ae8a244e":"code","c0d5b0c5":"code","1fd46a01":"code","2d90d591":"code","9278e63c":"code","f45f913f":"code","0948c0b3":"code","5bd880b6":"code","a9e000d8":"code","7d5f4ab2":"code","9c7737ae":"code","1993435b":"code","22bd7232":"code","9a5c3878":"code","1bd3e690":"code","6db83648":"code","0a2450a7":"code","df8859ed":"code","795df725":"code","4efb4407":"code","efb1ea79":"code","ca3cccde":"code","59a8d3ba":"code","746f0ab5":"code","189c693f":"code","44d4b0cb":"code","a2734f1c":"code","16b45ecb":"code","b9f307bc":"code","24a74499":"code","db521aba":"code","747f348d":"code","2731944d":"code","a90a467c":"code","82ff4ad1":"code","e5cbf784":"code","51409063":"code","a5104e64":"code","d141f3fb":"code","94e4ba7d":"code","edf2d783":"code","83e2880d":"code","abdf5e5e":"code","f34596d2":"code","2d285c1a":"code","1c32a9b7":"code","e071100f":"code","1d23bc03":"code","7dacbff6":"code","7da13b2d":"code","fb82f951":"code","8236da02":"code","228586a9":"code","7e8c0712":"code","ba38b9ea":"code","565e336e":"code","5098af7e":"code","e449a192":"code","c2bb2e8b":"code","0193e50e":"code","13883d11":"code","83e11942":"code","71164d78":"code","8094bd72":"code","bd15f251":"code","956c02e6":"code","ab6a974d":"code","876f7335":"code","313e9ab0":"code","59dfedce":"code","85fee4ef":"code","2f99b24b":"code","a1c663c4":"code","bdb1ef45":"code","6282434e":"code","97654687":"code","5ef90051":"code","17cd85d1":"code","48830886":"code","18e87d9d":"code","2663703d":"code","8121e231":"code","acddb96d":"code","a412a076":"code","2214d4be":"code","62c63a37":"code","8ca383e2":"markdown","84263ecc":"markdown","dd642ebf":"markdown","bd276325":"markdown","cbae82ed":"markdown","a4479a6b":"markdown","29ee0d3f":"markdown","639ac540":"markdown","1275d41a":"markdown","69442b35":"markdown","d8d05d74":"markdown","eb2cd0d9":"markdown","9647767c":"markdown","03064ca9":"markdown","b4354289":"markdown","9b5af4af":"markdown","35a33170":"markdown","184b5d33":"markdown","a3e94957":"markdown","568376ac":"markdown","56adc9f8":"markdown","796bb341":"markdown","0bcfd834":"markdown","be5da6b6":"markdown","5855c77f":"markdown","7caa90f2":"markdown","73768639":"markdown","fe6057da":"markdown","ca600973":"markdown","451b5fec":"markdown","84ef40f7":"markdown","b25b6350":"markdown","8f5cfc91":"markdown","b1e819fc":"markdown","068a9bb0":"markdown","0d819c4d":"markdown","9494b3cb":"markdown","c2547e44":"markdown","11d91a18":"markdown","878e0da9":"markdown","7ec7a413":"markdown","f0836c31":"markdown","335c193f":"markdown","106902a4":"markdown","2b225f47":"markdown","4a072a84":"markdown","afd47dce":"markdown","404e2e30":"markdown","b3afd438":"markdown","eb75e883":"markdown","3d7b9ffa":"markdown","67dd97e4":"markdown","c8a20248":"markdown","3c2ef684":"markdown","c219d77e":"markdown","e05a27ea":"markdown","736465b5":"markdown","3d14dd89":"markdown","01de8355":"markdown"},"source":{"0646ec25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Using Matplotlib and Seaborn for visualisation.\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c42904f1":"# Import train and test data, and set them as df_train and df_test, accordingly.\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_Passenger_id = df_test[\"PassengerId\"]","ae8a244e":"# Let's look at colums of the train dataframe quickly.\ndf_train.columns","c0d5b0c5":"# It shows the train columns and their types\ndf_train.info()","1fd46a01":"# That gives the F\u0130RST 5 rows of the df_train \ndf_train.head()","2d90d591":"# That gives the LAST 5 rows of the df_train \ndf_train.tail()","9278e63c":"# shows the statistical information of the train columns\ndf_train.describe().T","f45f913f":"# This shows the correlations between numerical train columns \ndf_train.corr()","0948c0b3":"# Let's visualize the correlations between features of the train set.\nfig, ax = plt.subplots(figsize=(8,5)) \nsns.heatmap(df_train.corr(), annot = True, fmt = \".2f\", linewidths=0.5, ax=ax) \nplt.show()","5bd880b6":"# We define a function that choose the variables and their own values and plot them. \n\ndef bar_plot(variable):\n    \n    var = df_train[variable]\n    \n    varValue = var.value_counts()\n    \n    \n    \n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show\n    print(\"{}: \\n {}\".format(variable, varValue))\n    \n","a9e000d8":"# We are plotting the categorical variables \ncategory1=[\"Survived\", \"Sex\", \"Pclass\", \"Embarked\", \"SibSp\", \"Parch\"]\nlist(map(lambda x:bar_plot(x), category1))\nplt.show()","7d5f4ab2":"# They have lots of different categories, therefore we donot plot them. \ncategory2=[\"Cabin\", \"Name\", \"Ticket\"]\nlist(map(lambda x:print(\"{} \\n\".format(df_train[x].value_counts())), category2))\nplt.show()","9c7737ae":"# We will use histogram to plot values of the numerical variables.\n\ndef hist_plot(variable):\n    \n    plt.figure(figsize=(9,3))\n    plt.hist(df_train[variable],bins = 50)\n    \n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with histogram\". format(variable))\n    plt.show","1993435b":"numericalVariables=[\"Fare\", \"Age\", \"PassengerId\"]\nlist(map(lambda x: hist_plot(x), numericalVariables))\nplt.show()","22bd7232":" # we will check whether it is relation between Pclass and Survived\n    \ndf_train[[\"Pclass\",\"Survived\"]].groupby(\"Pclass\", as_index=False).mean().sort_values(by=\"Survived\", ascending = False)\n\n","9a5c3878":"# we will check whether it is relation between Sex and Survived \n    \ndf_train[[\"Sex\",\"Survived\"]].groupby(\"Sex\", as_index=False).mean().sort_values(by=\"Survived\", ascending = False)\n","1bd3e690":" # we will check whether it is relation between SibSp and Survived\n    \ndf_train[[\"SibSp\",\"Survived\"]].groupby(\"SibSp\", as_index=False).mean().sort_values(by=\"Survived\", ascending = False)\n\n","6db83648":" # we will check whether it is relation between Parch and Survived\n    \ndf_train[[\"Parch\",\"Survived\"]].groupby(\"Parch\", as_index=False).mean().sort_values(by=\"Survived\", ascending = False)\n","0a2450a7":"def outlier_detection(df, features):\n    outlier_indices = []\n    \n    for i in features:\n        # 1st quirtile\n        q1 = np.percentile(df[i],25)\n        \n        # 3rd quirtile\n        q3 = np.percentile(df[i],75)\n        \n        # IQR\n        IQR = q3 - q1\n        \n        # Outlier step\n        \n        outlier_step = IQR * 1.5\n        \n        # detect outliers and their indices \n        \n        outliers_list = df[(df[i] < q1-outlier_step) | (df[i]> q3 + outlier_step)].index\n        \n        \n        # Storing indices\n        \n        outlier_indices.extend(outliers_list)\n        \n        # Here we mean that if a feature has more than two outliers, we will store the indices of the outliers. \n        # Otherwise we are not intersted in outliers of a feature \n        outlier_indices1 = Counter(outlier_indices)\n        multiple_outliers = list(j for j,v in outlier_indices1.items() if v>2 )\n        \n    return multiple_outliers","df8859ed":"# Here we are running the outlier_detection function.\ndf_train.loc[outlier_detection(df_train, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])]","795df725":"# Now we are dropping the outliers and reseting the index\ndf_train = df_train.drop(outlier_detection(df_train, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]), axis = 0).reset_index(drop = True)","4efb4407":"# In order not to damage our original data, we assigned train and test sets to new variables by the copy method during the concatenating processes.\ndf_train_copy = df_train.copy()\ndf_test_copy = df_test.copy()\n\n# concatenating train and test sets\ndf_new_train = pd.concat([df_train_copy,df_test_copy], axis = 0).reset_index(drop = True)\ndf_new_train ","efb1ea79":"# It shows all columns in the data set that have missing values.\ndf_new_train.columns[df_new_train.isnull().any()]","ca3cccde":"# It shows the sumation of the missing values in each variable.\ndf_new_train.isnull().sum()","59a8d3ba":"# Let's see which indices of \"Embarked\" variable has missing values.\ndf_new_train[df_new_train[\"Embarked\"].isnull()]","746f0ab5":"# Boxplot of \"Age\" grouped by \"Embarked\" variable. \ndf_new_train.boxplot(column = \"Age\", by = \"Embarked\")\nplt.show()","189c693f":"# Boxplot of \"Fare\" grouped by \"Embarked\" variable. \ndf_new_train.boxplot(column = \"Fare\", by = \"Embarked\")\nplt.show()","44d4b0cb":"# Filling the missing values in \"Embarked\" variable\ndf_new_train[\"Embarked\"] = df_new_train[\"Embarked\"].fillna(\"C\")","a2734f1c":"# Let's see which indices of \"Fare\" variable has missing values.\ndf_new_train[df_new_train[\"Fare\"].isnull()]","16b45ecb":"# Boxplot of \"Fare\" grouped by \"Embarked\" variable. \ndf_new_train.boxplot(column = \"Fare\", by = \"Pclass\")\nplt.show()","b9f307bc":"# Median values of the \"Fare\" column grouped by \"Pclass\"\ndf_new_train[[\"Pclass\",\"Fare\"]].groupby(\"Pclass\", as_index=False).median()","24a74499":"# Filling the missing values in \"Fare\" variable by  the median value of  3rd class passengers' \"Fare\" . \ndf_new_train[\"Fare\"] = df_new_train[\"Fare\"].fillna(df_new_train[[\"Pclass\",\"Fare\"]].groupby(\"Pclass\", as_index=False).median()[\"Fare\"][2])","db521aba":"# This shows the correlations between numerical train columns \ndf_new_train.corr()","747f348d":"# Let's visualize the correlations between features of the train set.\n%config InlineBackend.figure_format ='retina'\nfig, ax = plt.subplots(figsize=(8,5)) \nsns.heatmap(df_new_train.corr(), annot = True, fmt = \".2f\", linewidths=0.5, ax=ax) \nplt.show()\n","2731944d":"g= sns.factorplot(x = \"SibSp\", y = \"Survived\", data = df_new_train, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","a90a467c":"g= sns.factorplot(x = \"Parch\", y = \"Survived\", data = df_new_train, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","82ff4ad1":"g= sns.factorplot(x = \"Pclass\", y = \"Survived\", data = df_new_train, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","e5cbf784":"g= sns.FacetGrid(df_new_train, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","51409063":"g = sns.FacetGrid(df_new_train, col=\"Survived\", row = \"Pclass\", size = 2)\ng.map(plt.hist, \"Age\", bins = 25)\ng.set_ylabels(\"The number of Survived\")\nplt.show()","a5104e64":"g = sns.FacetGrid(df_new_train, row = \"Embarked\", size = 2)\ng.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\")\ng.add_legend()\nplt.show()","d141f3fb":"g = sns.FacetGrid(df_new_train, row = \"Embarked\", col=\"Survived\", size = 2.3)\ng.map(sns.barplot, \"Sex\", \"Fare\")\ng.add_legend()\nplt.show()","94e4ba7d":"g = sns.FacetGrid(df_new_train, col=\"Pclass\", height=4, aspect=.5)\ng.map(sns.barplot, \"Sex\", \"Survived\")\nplt.show()","edf2d783":"# This code is giving all variables whose Age number is  NaN. \ndf_new_train[df_new_train[\"Age\"].isnull()]","83e2880d":"# let's look at relationship between Age and Sex variables.\nsns.factorplot(x = \"Sex\", y = \"Age\", data = df_new_train, kind = \"box\");","abdf5e5e":"# let's look at relationship between Pclass, Age and Sex variables.\nsns.factorplot(x = \"Sex\", y = \"Age\", hue = \"Pclass\", data = df_new_train, kind = \"box\");","f34596d2":"# let's look at relationship between Age and Parch variables and between Age and SibSp variables.\n\nsns.factorplot(x = \"Parch\", y = \"Age\", data = df_new_train, kind = \"box\");\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = df_new_train, kind = \"box\");","2d285c1a":"# We use list comprehension to tranform \"Sex\" variable from Object to numerical variable...\n# because \u0131 want to see correlation with Sex and other variables.\ndf_new_train[\"Sex\"] = [1 if i == \"male\" else 0 for i in df_new_train[\"Sex\"]]\ndf_new_train[\"Sex\"]","1c32a9b7":"# Correlation Matrix\nvar_list = [\"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Pclass\"]\nsns.set(font_scale=0.9)\nfig, ax = plt.subplots(figsize=(10,5)) \nsns.heatmap(df_new_train[var_list].corr(), annot = True, fmt = \".2f\", linewidths=0.5, ax=ax) \nplt.show()","e071100f":"# Finding the indices of the missing values in Age variable.\nAge_index= df_new_train[\"Age\"][df_new_train[\"Age\"].isnull()].index\nAge_index","1d23bc03":"# Let's filling missing values of Age \nfor i in Age_index:\n    predicted_Age = df_new_train[\"Age\"][((df_new_train[\"SibSp\"] == df_new_train.iloc[i][\"SibSp\"])&(df_new_train[\"Pclass\"] == df_new_train.iloc[i][\"Pclass\"])&(df_new_train[\"Parch\"] == df_new_train.iloc[i][\"Parch\"]))].median()\n    Age_med = df_new_train[\"Age\"].median()\n    \n    if not np.isnan(predicted_Age):\n        df_new_train[\"Age\"].iloc[i] = predicted_Age\n    else:\n         df_new_train[\"Age\"].iloc[i] = Age_med\n    ","7dacbff6":"# Looking at Name variable\ndf_new_train[\"Name\"].head(10)","7da13b2d":"# We are scraping titles from name and assign them into new variable as \"Title\".\nname = df_new_train[\"Name\"]\ndf_new_train[\"Title\"] = [((i.split('.')[0]).split(',')[-1]).strip() for i in name]\ndf_new_train[\"Title\"] ","fb82f951":"# showing the frequiencies of each titel. \ndf_new_train[\"Title\"].value_counts()","8236da02":"# Plotting the frequiencies of each titel.\nsns.countplot(x = \"Title\", data = df_new_train)\nplt.xticks(rotation = 60)\n","228586a9":"# Creating new title name as other.\nother_list = [\"Rev\",\"Dr\",\"Col\",\"Mlle\",\"Ms\",\"Major\",\"Dona\",\"Capt\",\"Jonkheer\",\"Lady\",\"Mme\",\"Sir\",\"the Countess\", \"Don\"]\ndf_new_train[\"Title\"] = df_new_train[\"Title\"].replace(other_list, \"other\")","7e8c0712":"df_new_train[\"Title\"].value_counts()","ba38b9ea":"# Converting the Title to categorical \ndf_new_train[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" else 2 if i == \"Mrs\" else 3 if i == \"Mr\" else 4  for i in df_new_train[\"Title\"]]\ndf_new_train[\"Title\"] = df_new_train[\"Title\"].astype('category', inplace = True)\ndf_new_train[\"Title\"].value_counts()","565e336e":"# Plotting the frequiencies of each title.\nsns.countplot(x = \"Title\", data = df_new_train)\nplt.xticks(rotation = 60)","5098af7e":"g = sns.factorplot(x = \"Title\", y = \"Survived\", data = df_new_train, kind = \"bar\");\ng.set_xticklabels([\"Master\", \"Miss\", \"Mrs\", \"Mr\", \"other\"])\ng.set_ylabels(\"Survival probability\");","e449a192":"# Droping variable \"Name\" from the data.\ndf_new_train.drop(\"Name\", axis = 1, inplace = True)\ndf_new_train.head()","c2bb2e8b":"# Convert \"Title\" categorical variable to dummy variables. \ndf_new_train = pd.get_dummies(df_new_train, columns = [\"Title\"])\ndf_new_train","0193e50e":"# creating new feature as FamilySize. Here 1 refers to passenger itself \ndf_new_train[\"FamilySize\"] = df_new_train[\"SibSp\"] + df_new_train[\"Parch\"] + 1\ndf_new_train","13883d11":"# Let's see what the relationship between survival rate and FamilySize is. \ng = sns.factorplot(x = \"FamilySize\", y = \"Survived\", data = df_new_train, kind = \"bar\");\ng.set_ylabels(\"Survival probability\");","83e11942":"# As it can be seen from the plot that we can categorize family size as 1 if FamilySize is less than 5 or 0 if FamilySize is equal to and greater  than 5. \ndf_new_train[\"FamilySize\"] = [1 if i < 5 else 0 for i in df_new_train[\"FamilySize\"] ]","71164d78":"df_new_train.head(10)","8094bd72":"# The frequincies of the categories in FamilySize. \ndf_new_train[\"FamilySize\"].value_counts()","bd15f251":"# Plotting the frequiencies of each Categories in FamilySize.\nsns.countplot(x = \"FamilySize\", data = df_new_train);","956c02e6":"# Let's see what the relationship between survival rate and FamilySize is. \ng = sns.factorplot(x = \"FamilySize\", y = \"Survived\", data = df_new_train, kind = \"bar\");\ng.set_ylabels(\"Survival probability\");","ab6a974d":"# Convert \"Title\" categorical variable to dummy variables. \ndf_new_train = pd.get_dummies(df_new_train, columns = [\"FamilySize\"])\ndf_new_train","876f7335":"df_new_train.head(10)","313e9ab0":"# Convert \"Title\" categorical variable to dummy variables. \ndf_new_train = pd.get_dummies(df_new_train, columns = [\"Embarked\"])\ndf_new_train","59dfedce":"# Convert \"Pclass\" categorical variable to dummy variables. \ndf_new_train[\"Pclass\"] = df_new_train[\"Pclass\"].astype('category', inplace = True)\ndf_new_train = pd.get_dummies(df_new_train, columns = [\"Pclass\"])\n","85fee4ef":"df_new_train","2f99b24b":"# Convert \"Sex\" categorical variable to dummy variables. \ndf_new_train[\"Sex\"] = df_new_train[\"Sex\"].astype('category', inplace = True)\ndf_new_train = pd.get_dummies(df_new_train, columns = [\"Sex\"])\n","a1c663c4":"df_new_train","bdb1ef45":"# Droping PassengerId and Cabin\ndf_new_train.drop([\"PassengerId\", \"Cabin\", \"Ticket\"], axis = 1, inplace = True)\n","6282434e":"df_new_train","97654687":"# Importin libraries\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","5ef90051":"# As you remember we concatenated test and train values. Now first of all we are splitting data into train and test \nlen(df_test) # this is the length of the test data before concatenating.\nlen(df_train)# this is the length of the train data before concatenating.\n\n# We will use these lengths of the train and test data for splitting final data as train and test set\n","17cd85d1":"# we are splitting test set and dropping Survived feature because test_set didnot have Survived column before concatenating\ntest = df_new_train[len(df_train):] \ntest.drop(labels =[\"Survived\"], axis = 1, inplace = True)\ntest","48830886":"# we are splitting train set into x_train and y_train. Furthermore we use train_test_split method to create X_train, X_test, Y_train, Y_test\ntrain = df_new_train[:len(df_train)]\nx_train = train.drop(labels = [\"Survived\"], axis = 1)\ny_train = train[\"Survived\"]\nX_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size = 0.3, random_state = 42)\n","18e87d9d":"# Logistic regression model\nlog_Reg = LogisticRegression()\nlog_Reg.fit(X_train, Y_train)\nlog_Reg_train_Accuracy = round(log_Reg.score(X_train, Y_train)*100,3)\nlog_Reg_test_Accuracy = round(log_Reg.score(X_test, Y_test)*100,3)\nprint(\"Training accuracy: %{}\".format(log_Reg_train_Accuracy))\nprint(\"Testing accuracy: %{}\".format(log_Reg_test_Accuracy))","2663703d":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n              SVC(random_state = random_state),\n              RandomForestClassifier(random_state = random_state),\n              LogisticRegression(random_state = random_state),\n              KNeighborsClassifier()]","8121e231":"# we are tuning the hyper parameters\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[500,1000],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","acddb96d":"# Cross Validation\ncv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,Y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","a412a076":"# Visualazing the results\ncv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\");","2214d4be":"# Ensembling of Decision Tree, Random Forest classifiers and Logistic Regression\nvotingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\",weights=[1,1,9], n_jobs = -1)\nvotingC = votingC.fit(X_train, Y_train)\nprint(accuracy_score(votingC.predict(X_test),Y_test))","62c63a37":"test_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_Passenger_id, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","8ca383e2":"# Introduction\n\nTitanic shipwreck is one of the biggest accidents in the world shipping history. The Titanic ship was such a big ship that it was named as \"unsinkable\". The ship hit a large iceberg  and 1502 of the 2224 passengers carried by Titanic drowned. \n\nIn this dataset we will invesitgate the features and try to find which passengers are more likely to survive. We will use Machine Learning Algorithms to predict the survived passengers.In the end we will compare the performance of the various Machine Learning Algorithms. \n\n<font color = 'blue'>\n Content:\n   \n   1. [Load and check data](#1)\n   2. [Information about variables](#2)\n       * 2.1 [Univariate variable analysis](#3)\n           * 2.1.1 [Categorical variable analysis](#4)\n           * 2.1.2 [Numerical variable analysis](#5)   \n   3. [Basic data analysis](#6)\n   4. [Detection of outliers](#7)\n   5. [Missing values](#8)\n       * 5.1 [Detecting missing values](#9)\n       * 5.2 [Filling missing values](#10)\n           * 5.2.1 [Filling missing values in \"Embarked\" variable](#11)\n           * 5.2.2 [Filling missing values in \"Fare\" variable](#12)\n           \n   6. [Visualization](#13)\n       * 6.1 [Correlation matrix](#14)\n       * 6.2 [SibSp and Survived](#15)\n       * 6.3 [Parch and Survived](#16)\n       * 6.4 [Pclass and Survived](#17)\n       * 6.5 [Age and Survived](#18)\n       * 6.6 [Pclass, Age and Survived](#19)\n       * 6.7 [Embarked, Sex, Pclass and Survived](#20)\n       * 6.8 [Embarked, Sex, Fare and Survived](#21)\n       * 6.9 [Pclass, Sex and Survived](#22)\n       \n   7. [Filling the missing values in Age variable](#23)\n   8. [Feature engineering](#24)\n        * [8.1 Name - Title](#25)\n        * [8.2 Family size](#26)\n        * [8.3 Embarked](#27)\n        * [8.4 Pclass](#28) \n        * [8.5 Sex](#29) \n        * [8.6  Droping PassengerId and Cabin](#30)\n   9. [Modelling](#31)\n        * [9.1 Train and test split](#32)\n        * [9.2 Simple Logistic Regression Model](#33)\n        * [9.3 Hyperparameter Tuning - Grid Search - Cross Validation](#34)\n        * [9.4 Ensembling](#35)\n        * [9.5 Prediction](#36)","84263ecc":"The graph on the left shows the disturibition of the died passengers while The graph on the right demostrates the disturibition of the survived passengers. \n* As seen in the chart on the right, the rate of survival of children aged 0-8 is very high, which indicates that **children were given priority**\n*  The rate of survival of elderly passengers aged over 70 is very high, which indicates that **old passengers were also given priority**\n* Most of the passengers in the Titanic were between the ages of 15-35.\n* Most of the **died passengers** in the Titanic were between the ages of **15-30.**\n* Most of the **survived passengers** in the Titanic were between the ages of **20-35.**\n* Both the left and right graphs have **gaussian distribution**.\n* **We can use the Age distributions for filling the missing values in \"Age\" variable.** ","dd642ebf":"<a id = '9'><\/a><br>\n### 5.1 Detecting missing values\n","bd276325":"**First class** passengers have the highest survival rate with 62.9% while 3rd class passengers have the lowest survival rate with 24.2 .","cbae82ed":"<a id = '33'><\/a><br>\n## 9.2 Simple Logistic Regression Model","a4479a6b":"<a id = '3'><\/a><br>\n##  2.1 Univariate variable analysis\n In this section we will analyse the variables individually which means that we will ignore the relations between variables. Univariate variable analysis can be divided into 2 groups as follow:\n \n   * Categorical variable analysis\n   * Numerical variable analysis","29ee0d3f":"<a id = '18'><\/a><br>\n## 6.5 Age and Survived\n       ","639ac540":"<a id = '30'><\/a><br>\n## 8.6  Droping PassengerId and Cabin\n\nWe will not use the PassengerID and Cabin variable to train ML models because they are not meaningful variables. ","1275d41a":"**At first glance** at the above plotting,  we can say that Passengers with fewer family members are more likely to survive, and Parch with 3 has the highest survival probability. However black vertical line refers to standard deviation. Parch with 3 has very high standard deviation, therfore this relation is not enough to extract new feature, **but SibSp and Parch can be used together for extracting new feature.** \n  ","69442b35":"<a id = '25'><\/a><br>\n## 8.1 Name - Title","d8d05d74":"<a id = '31'><\/a><br>\n# 9.  Modelling\n","eb2cd0d9":" **From the relationships of variables, the following can be understood:**\n* First class passengers have the highest survival rate with 62.9% while 3rd class passengers have the lowest survival rate with 24.2\n* 74% of women survived, while 18% of men survived\n* The survival rate of passengers with one or two siblings is significantly higher than others.\n* The survival rate of people with 3 family members on board is the highest with 60%, but the survival rates of people with 1 and 2 family members are also quite high. It is 55% and 50% respectively.\n\n\n\nFrom here, we can clearly conclude that if a person is **a woman** and has **a first-class ticket**, the probability of survival is **very high**","9647767c":"<a id = '13'><\/a><br>\n# 6. Visualization\n\nIn this section we will illustrate the relationship between variables by using visualization tools. we will visualize relation between :\n*  SibSp and Survived\n*  Parch and Survived\n*  Pclass and Survived\n*  Age and Survived\n*  Pclass, Age and Survived\n*  SibSp and Survived\n*  Embarked, Sex, Pclass and Survived\n*  Embarked, Sex, Pclass and Survived\n*  correlation matrix","03064ca9":"**Pclass, Parch and SibSp have obvious correlation with Age while Sex doesn't have. Therefore it can be logical if we use Pclass, Parch and SibSp variables for filling the missing values in Age variable.**","b4354289":"<a id = '35'><\/a><br>\n## 9.4 Ensembling\n\nAs it can be seen from cross validation scores that Decision Tree , Logistic Regression and Random Forest classifiers have higher scores than 80%. We set 80% as treshold and will ensemble these 3 classifiers. we will use VotingClassifier to ensemble them.","9b5af4af":"We cannot make a prediction about survival condition by using passengers' names, but there might be relationship between survival rate and titles (such as Mr., Miss or Mrs). ","35a33170":"It can be seen that this realtionship give us much more valuable perspective. For example:\n* we might say that if a person is male and on first class, his age can be 42 which is the median value of men on first class.\n* we might say that if a person is female and on third class, her age can be filled by 22 which is the median value of females on third class.\n","184b5d33":"It is clear that the number of ladies survived is much more higher than the number of survived men.\n\n**We have usen \"Name\" variable to create new feature as Title. Therefore we donot need variable \"Name\" anymore, and we will drop it from the data.**","a3e94957":"<a id = '14'><\/a><br>\n## 6.1 Correlation matrix","568376ac":"<a id = '15'><\/a><br>\n## 6.2 SibSp and Survived\n       ","56adc9f8":"<a id = '5'><\/a><br>\n\n## 2.1.2 Numerical variable analysis","796bb341":"<a id = '11'><\/a><br>\n### 5.2.1 Filling missing values in \"Embarked\" variable\n","0bcfd834":"<a id = '32'><\/a><br>\n## 9.1 Train and test split","be5da6b6":"We can say that:\n* The majority of passengers belong to 3rd class. The great number of 3rd class passengers didnot survive. From this, we can deduce that there is an inverse proportion between the number of passengers in a class and the rate of living.\n* While the survival rates of the first class passengers are significantly higher, there is no significant difference between the survival and death rates of the second class passengers.","5855c77f":"<a id = '17'><\/a><br>\n## 6.4 Pclass and Survived\n       ","7caa90f2":"**We can say:**\n* Age of passengers with parch equal to and less than 2 can be filled by 22 which is the avarage median value of Parch 0, 1 and 2.\n* Age of passengers with parch equal to and greater than 3 can be filled by 40 which is the avarage median value of Parch 3 and above.\n\n**We can divide SibSp into two groups as follow:**\n* Age of passengers with SibSp equal to and less than 2 can be filled by 25 which is the avarage median value of SibSp 0, 1 and 2.\n* Age of passengers with SibSp equal to and greater than 3 can be filled by 10 which is the avarage median value of SibSp 3 and above.","73768639":"<a id = '21'><\/a><br>\n## 6.8 Embarked, Sex, Fare and Survived","fe6057da":"**It is very clear** that distribution and median values of both men and females are almost similiar. Therefore we cannot use gender variables directly for filling missing values.  ","ca600973":"Survival probability of small families is almost three times higher than big families. ","451b5fec":"<a id = '22'><\/a><br>\n## 6.9 Pclass, Sex and Survived","84ef40f7":"<a id = '2'><\/a><br>\n## 2. Information about variables\n\n\nDescription of the variables in dataset:\n\n* PassengerId : refers to passenger's id, which is unique\n* Survived : if passenger was survived, it takes 1, otherwise it is 0.\n* Pclass : refers to ticket's class. 1= 1st, 2= 2nd , 3 = 3rd (1st is the highest class)\n* Name : name of passenger\n* Sex : gender of passenger\n* Age : age of passenger\n* SibSp : the number of siblings \/ spouses aboard the Titanic (mistresses and fianc\u00e9s were ignored)\n* Parch : defines family relations such as mother, father,daughter, son, stepdaughter, stepson (Some children travelled only with a nanny, therefore parch=0 for them.)\n* Ticket : Ticket number\n* Fare : cost written on the pessenger's ticket\n* Cabin : cabin number\n* Embarked : defines which passenger embarked on the Titanic from which port (C = Cherbourg, Q = Queenstown, S = Southampton )","b25b6350":"<a id = '26'><\/a><br>\n## 8.2 Family size\n\nVariables \"SibSp\" and \"Parch\" give the information about passengers family. Maybe we can use them to create new feature.\n\nSibSp refers to sibling and spouse while Parch indicates parents or children.","8f5cfc91":"<a id = '27'><\/a><br>\n## 8.3  Embarked\nWe are Converting \"Embarked\" categorical variable to dummy variables. We will use Embarked as it is to train our Machine Learning Model.\n\n","b1e819fc":"<a id = '28'><\/a><br>\n## 8.4  Pclass\n\nWe are Converting \"Pclass\" categorical variable to dummy variables. We will use Pclass as it is to train our Machine Learning Model.","068a9bb0":"<a id = '36'><\/a><br>\n## 9.5 Prediction ","0d819c4d":"<a id = '20'><\/a><br>\n## 6.7 Embarked, Sex, Pclass and Survived\n      ","9494b3cb":"<a id = '7'><\/a><br>\n## 4. Detection of outliers","c2547e44":"**Training accuracy: %83.117 and Testing accuracy: %83.019 are almost same.** ","11d91a18":"**We can see that** when the number of siblings and spouse (SibSp) is more than 2, the survival probability decreases sharply. We can use this plot in order to extract the new feature.  **For example**, we can define new variable as SibSp2 and set 2 as treshold. if SibSp is equal to or less than 2, the value of SibSp2 equals to 1, otherwise 0.    ","878e0da9":"As we can see from above plotting:\n* People who paid more money and embarked from Cherbourg and Southampton ports have more chance to survive than people who embarked from same ports but paid less. \n* Fare does not have effect of the survival probability of people who embarked from Quuenstown. \n\n","7ec7a413":"It can be concluded from the boxplots that median value of Passengers' age which embarked on the board from Cherbourg (C) is the closest to the  age values of the columns with missing data.\nIn terms of the median values of \"Fare\" variable,  passenger embarked on the board from Cherbourg paid the closest fare to the  fare values of the columns with missing data.\n\n**It can be said that missing values of \"Embarked\" variable can be filled by Cherbourg (C)**","f0836c31":"<a id = '6'><\/a><br>\n## 3. Basic data analysis\n\nIn this section we will analyse the relationships between the variables and show the pattern\nFollowing variables pair will be analysed:\n\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parch - Survived","335c193f":"<a id = '23'><\/a><br>\n# 7. Filling the missing values in Age variable\n\n\nI prefer to handle missing values in Age variable because of its complexity. As we see in missing value section that variable Age has 256 missing values. We need to look depth inside the Age variable and its relationship with other varables. ","106902a4":"<a id = '4'><\/a><br>\n## 2.1.1 Categorical variable analysis\n","2b225f47":"<a id = '34'><\/a><br>\n## 9.3 Hyperparameter Tuning - Grid Search - Cross Validation\n\nIn this section we will compare the performance of the 5 different machine learning classifiers such as Decision Tree, SVM, Random Forest, KNN and Logistic Regression. we will use Grid Search technique to investigate the hyper parameters of the models.It means that we will utilize Cross Validation technique to compare the parameters values and find the optimal values of the parameters in the ML models. ","4a072a84":"<a id = '16'><\/a><br>\n## 6.3 Parch and Survived\n       ","afd47dce":"<a id = '8'><\/a><br>\n## 5. Missing values\n       \nMissing values that are unusable values (such as #, ?, - and NaN (not a number)), have to get handled in order to create machine learning model with high accuracy. \nIn this section we will detect the missing values and replace them with meaningful numbers.","404e2e30":"<a id = '29'><\/a><br>\n## 8.5  Sex\n\nWe are Converting \"Sex\" categorical variable to dummy variables. We will use Sex as it is to train our Machine Learning Model.","b3afd438":"<a id = '10'><\/a><br>\n### 5.2 Filling missing values\n\n* Column \"Embarked\" has 2 missing values\n* Column \"Fare\" has just one missing value\n ","eb75e883":"It can be seen that the row above with missing value belongs to 3rd class passenger. Therfore we can use the fare of the 3rd class passengers to fill the missing value. As we see from the boxplot that \"Fare\" values of 3rd class have lots of outliers, therefore it can be better if we use median value of  3rd class passengers' \"Fare\". \n","3d7b9ffa":"<a id = '19'><\/a><br>\n## 6.6 Pclass, Age and Survived\n      ","67dd97e4":"<a id = '24'><\/a><br>\n# 8. Feature engineering","c8a20248":"<a id = '1'><\/a><br>\n## 1. Load and check data","3c2ef684":"**As we can see,** we do not have information about \n- ages og 256 passengers, \n- 1007 passengers' cabin numbers, \n- embarking port of 2 passengers and \n- ticket price of a passenger. \n\nThere is also 418 missing values in \"survived\" column that comes from test_set, because the test_set does not have \"survived\" column.   ","c219d77e":"**It can be concluded** from the heatmaps that \"Survived\" variable has the highest correlation with \"Fare\" variable. **We can say:**\n* the more money passengers pay,  the higher  probability of survival they have\n* the lower class passengers belongs to, the lowest probability of survival they have\n","e05a27ea":"As we can see from the frequiencies of each titel that Mr, Mrs, Miss and Master account for majority of the \"Titles\", therefore we can sum rest of the Titles and named as other. Now we can transform \"Titles\" to categorical variable.","736465b5":"<a id = '12'><\/a><br>\n### 5.2.2 Filling missing values in \"Fare\" variable","3d14dd89":"**It can be concluded** from the heatmaps that \"Survived\" variable has the highest correlation with \"Fare\" variable. **We can say:**\n* the more money passengers pay,  the higher  probability of survival they have\n* the lower class passengers belongs to, the lowest probability of survival they have\n","01de8355":"It can be said:\n* All females on first and second classes, which embarked from Queenstown,survived while all men on first and second classes, which embarked from Queenstown,died.\n* Almost all females on first and second classes, which embarked from Southampton,survived while just %30 of women which is 3rd class and embarked from Southampton, survived.\n* Men embared from Cherbourg, have the highest survival probability if ther are compared with other men embarked from Southampton and Quuenstown.\n "}}