{"cell_type":{"a193f207":"code","e6df313f":"code","42248e00":"code","fa4eeeec":"code","3675c504":"code","e93a6f62":"code","83e5bec4":"code","e88a8527":"code","06171960":"code","49b2bb70":"code","ba521663":"code","1907b63c":"code","15e6c423":"code","39872cea":"code","bf287855":"code","86afd9d8":"code","dc99c24e":"code","3a64f22f":"code","6ccbbe4a":"code","e9bf9f43":"code","2b7186b0":"code","62203067":"code","a4a15814":"code","315c070f":"code","cdf4372e":"code","bf7069ca":"code","de0d6e2f":"code","bc7d9732":"code","813e0f62":"code","8d5d4ef2":"code","53501ac6":"code","e16353c0":"code","bb755df8":"code","6d36e321":"code","b0a88622":"code","406f0691":"code","dc57b0ac":"code","d17cb5d7":"code","e4b2a3f6":"code","8325a09f":"code","db5a9949":"code","19ef3316":"code","bb1a014a":"code","b5b89cac":"code","f455909a":"code","44d3f0a0":"code","b7ca52a8":"code","3bcddfa4":"code","81890376":"code","a378d597":"code","7be70544":"code","e1d149a5":"code","a9955eca":"code","2f27abbd":"code","6c1cb29f":"code","3008efd4":"code","6b438653":"code","e717b422":"code","b0e3b561":"code","cce19edb":"code","0461c77c":"code","f6f25581":"code","d73f02fd":"code","e5140d29":"code","a1b6e8b4":"code","dfcfdb20":"code","617c00a2":"code","ce2b8651":"code","dba48e7a":"code","dd2456f8":"code","732c00d0":"code","bd654d19":"code","d1a27d2d":"code","d90fce58":"code","a81658fe":"code","4f33f05f":"code","ca6ee435":"code","8e3ec8cd":"code","5db71b86":"code","2c490745":"code","abe305a1":"code","1013369e":"code","2d09d881":"code","0c5992b9":"code","e2ec94c9":"code","96bce441":"code","cda90c42":"code","512def45":"code","86550630":"code","1e34d080":"code","9f5208c1":"code","f0821063":"code","a068c879":"code","92358d11":"code","4cc85f9a":"code","6f4ee59f":"code","2f292d9e":"code","99554643":"code","9bae12dc":"code","eef7d161":"code","e18c4714":"code","81091fdf":"code","c0921471":"code","330fbd6a":"code","12436c19":"code","38d419ce":"code","f1c5f669":"code","8d213d2f":"code","78b07da4":"code","30e72dd6":"code","49ae5e24":"code","c379da2c":"code","a85b4093":"code","072f1828":"code","b9d56aa7":"markdown","e45106c7":"markdown","452ed6ed":"markdown","52c54b61":"markdown","2fbc033a":"markdown","edbe5000":"markdown","c2167012":"markdown","fdec02c7":"markdown","6a93c75a":"markdown","b272d0f2":"markdown","41af542c":"markdown","0ca47290":"markdown","db016029":"markdown","165e0027":"markdown","7b721905":"markdown","74e34fa5":"markdown","008a5137":"markdown","40772a69":"markdown","29ebd544":"markdown","553791e1":"markdown","32f66133":"markdown","af50ed5e":"markdown","34e38c97":"markdown","c33c03dd":"markdown","3508a1ba":"markdown","0c14485b":"markdown","82990601":"markdown","3a5dd0bd":"markdown","4df1f38b":"markdown","e72d63bd":"markdown","eed0f792":"markdown","b488df78":"markdown","51491571":"markdown","dcb42b1a":"markdown","bf0c8be4":"markdown","293b98dc":"markdown","268a1107":"markdown","023bd966":"markdown","9faa19aa":"markdown","28793fd6":"markdown","d80cd5ae":"markdown","afc027da":"markdown","603f9764":"markdown","71c607f9":"markdown","5db3b6ee":"markdown","7d6e055e":"markdown","30ddaeb3":"markdown","c3718533":"markdown","eb840f14":"markdown","d640e2a1":"markdown","943323a0":"markdown","ce51fcc6":"markdown","4b9e4861":"markdown","cf661493":"markdown","91bf6e18":"markdown","50e5c081":"markdown","d1e0e338":"markdown","3b6f71a8":"markdown","be88c9b2":"markdown","d2319081":"markdown","199e2e3f":"markdown","50155cb2":"markdown","6a52feea":"markdown","898356df":"markdown","a42c0250":"markdown","1d6be13c":"markdown","92874914":"markdown","f8135f5f":"markdown","4534092d":"markdown","333538bc":"markdown","5bb43e3f":"markdown","689a81f7":"markdown","9b20b850":"markdown","aba253b0":"markdown","9da63fad":"markdown","dc4da0c9":"markdown","c7d02dd3":"markdown","b5e79ffa":"markdown","1f8bd2dd":"markdown","3957f905":"markdown","ea38b97a":"markdown","23de1a11":"markdown","67077e56":"markdown","5dc2f9ed":"markdown","302db7f6":"markdown","88ebe806":"markdown","36907f1c":"markdown","9054be63":"markdown","75a6345e":"markdown","73e17b75":"markdown"},"source":{"a193f207":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","e6df313f":"import pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport spacy\nfrom spacy.matcher import Matcher\nimport fasttext\nimport json\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.models import CoherenceModel\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.colors as mcolors\nfrom bokeh.models import HoverTool\nfrom bokeh.models import ColumnDataSource\nfrom wordcloud import WordCloud\n\nwarnings.simplefilter('ignore')\npd.options.mode.chained_assignment = None  # default='warn'","42248e00":"data_path = '\/kaggle\/input\/cord19researchchallenge-old\/CORD-19-research-challenge\/' # path for Kaggle\n#data_path = os.getcwd() + '\/data\/CORD-19-research-challenge\/'\nmeta_df = pd.read_csv(data_path + 'metadata.csv',\n                      dtype={'pubmed_id': str, 'Microsoft Academic Paper ID': str, 'doi': str})","fa4eeeec":"all_json = glob.glob(f'{data_path}\/**\/*.json', recursive=True)\n\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.body_text = []\n            self.paragraphs = []\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            # Paragraps\n            for entry in content['body_text']:\n                self.paragraphs.append((entry['section'].capitalize().replace('\\\\', ''), entry['text']))\n            self.body_text = '\\n'.join(self.body_text)\n\n    def __repr__(self):\n        return f'{self.paper_id}: \\n\\n{self.abstract[:200]}... \\n\\n{self.body_text[:200]}...'\n\n    \ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'meta_abstract': [], 'publication_date': [], 'paragraphs': []}\n\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 20) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n    dict_['paragraphs'].append(content.paragraphs)\n\n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    dict_['authors'].append(np.nan if meta_data['authors'].empty else meta_data['authors'][0:1].item())\n    dict_['title'].append(np.nan if meta_data['title'].empty else meta_data['title'][0:1].item())\n    dict_['journal'].append(np.nan if meta_data['journal'].empty else meta_data['journal'][0:1].item())\n    dict_['publication_date'].append(np.nan if meta_data['publish_time'].empty else meta_data['publish_time'][0:1].item())\n    abstract_text = np.nan if meta_data['abstract'].empty else meta_data['abstract'][0:1].item()\n    dict_['abstract'].append(abstract_text if pd.isnull(abstract_text) or abstract_text.partition(' ')[0].lower() != 'abstract' else abstract_text.partition(' ')[2])\n\ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'publication_date', 'paragraphs'])","3675c504":"df_covid.abstract.fillna('', inplace=True)\ndf_covid.title.fillna('', inplace=True)","e93a6f62":"df_covid.head()","83e5bec4":"def merge_section_texts(paragraphs_list):\n    \"\"\"\n    Concatenate paragraph texts of the same section\n    :param paragraphs_list:\n    :return:\n    \"\"\"\n    merged_index = 0\n    merged_paragraphs = [paragraphs_list[0]]\n    for index in range(1, len(paragraphs_list)):\n        if merged_paragraphs[merged_index][0] == paragraphs_list[index][0]:\n            merged_paragraphs[merged_index] = (merged_paragraphs[merged_index][0], ' '.join(\n                [merged_paragraphs[merged_index][1], paragraphs_list[index][1]]))\n        else:\n            merged_index += 1\n            merged_paragraphs.append(paragraphs_list[index])\n    return merged_paragraphs\n\n\ndf_covid['sections'] = df_covid.paragraphs.apply(merge_section_texts)\ndf_covid.drop('paragraphs', axis=1, inplace=True)","e88a8527":"lang_detect_model = fasttext.load_model('\/kaggle\/input\/ft-model\/lid.176.bin') #https:\/\/fasttext.cc\/docs\/en\/crawl-vectors.html\n\n\ndef get_language(content):\n    \"\"\"\n    Function to detect the language of a string\n    :param content: text\n    :return: language\n    \"\"\"\n    return lang_detect_model.predict(content.replace('\\n', ' '))[0][0][-2:]\n\n\ndf_covid['paper_language'] = df_covid['body_text'].apply(get_language)","06171960":"df_covid.head()","49b2bb70":"nlp = spacy.load('en_core_web_lg') #https:\/\/spacy.io\/models\/en","ba521663":"abstracts = list(nlp.pipe(df_covid['abstract'], batch_size=50, n_process=3))","1907b63c":"df_abstract = df_covid.copy()\ndf_abstract['token'] = None\ndf_abstract['lemma'] = None\ndf_abstract['lemma_lower'] = None\ndf_abstract['token_pos'] = None\ndf_abstract['token_tag'] = None\ndf_abstract['token_dep'] = None\ndf_abstract['token_entity'] = None\n\ndf_abstract['token'] = df_abstract['token'].astype('object')\ndf_abstract['lemma'] = df_abstract['lemma'].astype('object')\ndf_abstract['lemma_lower'] = df_abstract['lemma_lower'].astype('object')\ndf_abstract['token_pos'] = df_abstract['token_pos'].astype('object')\ndf_abstract['token_tag'] = df_abstract['token_tag'].astype('object')\ndf_abstract['token_dep'] = df_abstract['token_dep'].astype('object')\ndf_abstract['token_entity'] = df_abstract['token_entity'].astype('object')\n\n\nfor i in range(len(abstracts)):\n\n    l_token = list()\n    l_lemma = list()\n    l_lemma_lower = list()\n    l_token_pos = list()\n    l_token_tag = list()\n    l_token_dep = list()\n    l_token_entity = list()\n\n    for token in abstracts[i]:\n        if not token.is_stop and not token.is_punct and token.is_alpha:\n            l_token.append(token.text)\n            l_lemma.append(token.lemma_)\n            l_lemma_lower.append(token.lemma_.lower())\n            l_token_pos.append(token.pos_)\n            l_token_tag.append(token.tag_)\n            l_token_dep.append(token.dep_)\n            l_token_entity.append(token.ent_type_)\n\n    df_abstract.loc[i,'token'] = l_token\n    df_abstract.loc[i,'lemma'] = l_lemma\n    df_abstract.loc[i,'lemma_lower'] = l_lemma_lower\n    df_abstract.loc[i,'token_pos'] = l_token_pos\n    df_abstract.loc[i,'token_tag'] = l_token_tag\n    df_abstract.loc[i,'token_dep'] = l_token_dep\n    df_abstract.loc[i,'token_entity'] = l_token_entity","15e6c423":"df_abstract = pd.read_pickle('\/kaggle\/input\/pickle\/df_abstract.pickle')","39872cea":"df_abstract.abstract = df_abstract['abstract'].replace('', np.nan)\ndf = df_abstract.dropna()","bf287855":"for i in range(len(df)):\n    for j in range(len(df.lemma.iloc[i])):\n        if df.token_pos.iloc[i][j] not in ['NOUN','PROPN']:\n            df.lemma.iloc[i][j] = \"\"    ","86afd9d8":"for i in range(len(df)):\n    df.lemma.iloc[i] = [string for string in df.lemma.iloc[i] if not len(string) < 3]","dc99c24e":"word_list = list()\nfor i in range(len(df)):\n    for j in range(len(df.lemma.iloc[i])):\n        word_list.append(df.lemma.iloc[i][j])","3a64f22f":"string_word_list = str(word_list)\n\nstring_word_list = string_word_list.replace(\"'\",\"\")","6ccbbe4a":"wordcloud = WordCloud(width = 800, height = 800,background_color ='white').generate(string_word_list)","e9bf9f43":"# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","2b7186b0":"def get_tf_idf(document_corpus):\n# TFIDF -----------------------------------------------------------------------\n    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), min_df=0.001, max_df=0.85, strip_accents='ascii',\n                                       tokenizer=lambda x: x,\n                                       preprocessor=lambda x: x)\n    tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(document_corpus.lemma).toarray()\n    # place tf-idf values in a pandas data frame\n    df_tfidf = pd.DataFrame(tfidf_vectorizer_vectors, columns=tfidf_vectorizer.get_feature_names())\n    return df_tfidf","62203067":"df_tfidf = get_tf_idf(df)","a4a15814":"list(df_tfidf.columns)","315c070f":"len(df_tfidf)","cdf4372e":"#use a dictionary comprehension to generate the largest_n values in each row of the dataframe. \n#transpose the dataframe and then applied nlargest to each of the columns. \n#use .index.tolist() to extract the desired top_n columns. \n#transposed this result to get the dataframe back into the desired shape.\n\ntop_n = 10\ndf_tfidf_top10 = pd.DataFrame({n: df_tfidf.T[col].nlargest(top_n).index.tolist() \n                  for n, col in enumerate(df_tfidf.T)}).T","bf7069ca":"df_tfidf_top10.head(30)","de0d6e2f":"len(df_tfidf_top10)","bc7d9732":"# LDA model -------------------------------------------------------------------\n\n# Creates dictionary, which is a mapping of word IDs to words.\nwords = corpora.Dictionary(df.lemma)\n# Turns each document into a bag of words.\ncorpus = [words.doc2bow(doc) for doc in df.lemma]","813e0f62":"n_topics = 10\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                            id2word=words,\n                                            num_topics=n_topics,\n                                            random_state=2,\n                                            update_every=1,\n                                            passes=10,\n                                            alpha=0.001,\n                                            eta=0.001,\n                                            per_word_topics=True,\n                                            minimum_probability=0)","8d5d4ef2":"print(lda_model.print_topics())","53501ac6":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=df.lemma, dictionary=words, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","e16353c0":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\nvis","bb755df8":"# Get topic weights and dominant topics ------------\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 4\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=900, plot_height=700)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])","6d36e321":"show(plot)","b0a88622":"# Read the data\n#df_covid = pd.read_pickle(os.getcwd() + '\/data\/processed\/df_covid.pickle')\ndf_covid = pd.read_pickle(\"df_covid.pickle\")","406f0691":"# Initialize the matcher\nmatcher = Matcher(nlp.vocab)","dc57b0ac":"# Keep only the English papers\ndf_covid_en = df_covid[df_covid['paper_language']=='en']\nprint('Number of English papers in the dataset: ' + str(df_covid_en.shape[0]))","d17cb5d7":"covid19_synonyms = ['covid',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'coronavirus 2019',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b']\n\n# Helper functions\ndef abstract_title_filter(df, search_string):\n    return (df.abstract.str.lower().str.replace('-', ' ').str.contains(search_string, na=False) |\n            df.title.str.lower().str.replace('-', ' ').str.contains(search_string, na=False))\n\n\ndef tagger(df, synonym_list, tag_suffix):\n    df[f'tag_{tag_suffix}'] = False\n    for synonym in synonym_list:\n        synonym_filter = abstract_title_filter(df, synonym)\n        df.loc[synonym_filter, f'tag_{tag_suffix}'] = True\n    return df","e4b2a3f6":"# Filter the relevant papers\ndf_covid_en = tagger(df_covid_en, covid19_synonyms, 'covid19')\ndf_covid19 = df_covid_en[df_covid_en['tag_covid19']]","8325a09f":"print('Number of papers about Covid-19: ' + str(df_covid19.shape[0]))","db5a9949":"# https:\/\/github.com\/explosion\/spaCy\/blob\/master\/examples\/pipeline\/custom_sentence_segmentation.py\ndef prevent_sentence_boundaries(doc):\n    for token in doc:\n        if not can_be_sentence_start(token):\n            token.is_sent_start = False\n    return doc\n\n\ndef can_be_sentence_start(token):\n    if token.i > 1 and token.nbor(-2).text == 'al':\n        return False\n    return True\n\nnlp.add_pipe(prevent_sentence_boundaries, before='parser')","19ef3316":"# Helper functions for pattern matching\ndef get_section_title(span, section_list):\n    \"\"\"\n    Function to search in which section of the paper\n    the detected pattern match (span) is found.\n    \"\"\"\n    section_title = ''\n    for section in section_list:\n        if span.sent.text.strip() in section[1]:\n            section_title = section[0]\n    return section_title\n\n\ndef get_paragraph(span, doc):\n    l_sents = []\n    sentences = list(doc.sents)\n    for i in range(len(sentences)):\n        if sentences[i].text == span.sent.text:\n            if i+1 <= len(sentences)-1 and i-1>=0: \n                l_sents = [sentences[i - 1].text, sentences[i].text, sentences[i + 1].text]\n            else:\n                l_sents = [sentences[i].text]\n    return ' '.join(l_sents)\n\n\ndef get_matches(doc, matcher, matcher_string_id):\n    \"\"\"\n    Function to search for all relevant mathes in a document (doc).\n    We only match on the requested match patterns \n    (matcher_string_id: list of IDs referring to the matching pattern you want to use)\n    \"\"\"\n    matches = []\n    raw_matches = matcher(doc)\n    matcher_id = []\n    for id_ in matcher_string_id:\n        matcher_id.append(nlp.vocab.strings[id_])\n    for match_id, start, end in raw_matches:\n        if end - start < 10 and match_id in matcher_id:\n            span = doc[start:end]\n            matches.append({'id': match_id, 'span': span})\n    return matches\n\n\ndef matched_paragraphs(doc, matcher, matcher_id, section_list):\n    matches, l_matched_paragraphs, section_titles = [], [], []\n    matches = get_matches(doc, matcher, matcher_id)\n    for match in matches:\n        section_titles.append(get_section_title(match['span'], section_list))\n        l_matched_paragraphs.append(get_paragraph(match['span'], doc))\n    matched_paragraphs_tuples = list(zip(section_titles, l_matched_paragraphs))\n    unique_matched_paragraphs_tuples = list(dict.fromkeys(matched_paragraphs_tuples).keys())\n    return unique_matched_paragraphs_tuples\n\n\ndef create_text_from_paragraph_list(paragraph_tuples):\n    tuple_text = []\n    for tuple_ in paragraph_tuples:\n        tuple_text.append('[{title}] {text}'.format(title=tuple_[0], text=tuple_[1]))\n    return ' [...] '.join(tuple_text)","bb1a014a":"# docs = list(nlp.pipe(df_covid19['body_text'], batch_size=50, n_process=3))\ndocs = list(nlp.pipe(df_covid19['body_text'], batch_size=50, n_process=1))\n\n# Add spacy doc objects to our covid-19 df\ndf_covid19['spacy_doc'] = docs","b5b89cac":"matcher = Matcher(nlp.vocab)\nincubation_pattern = [{\"LOWER\": {'IN': ['incubation', 'latency', 'latent','window']}}, \n                      {\"LOWER\": 'period'},\n                      {'OP':'*'},\n                      {'POS': 'NUM'},\n                      {'OP':'*'},\n                      {\"LOWER\": {'IN': ['week', 'weeks', 'days','day','months','month']}}]\n\nincubation_pattern2 = [{\"LOWER\": 'period'},\n                       {\"LOWER\": {'IN': ['of']}},\n                       {\"LOWER\": {'IN': ['incubation', 'latency']}},\n                       {'OP':'*'},\n                       {'POS': 'NUM'},\n                       {'OP':'*'},\n                       {\"LOWER\": {'IN': ['week', 'weeks', 'days','day','month','month']}}]\n\nmatcher.add(\"incubation_matcher\", None, incubation_pattern)\nmatcher.add(\"incubation_matcher2\", None, incubation_pattern2)","f455909a":"incubation_matchers = ['incubation_matcher', 'incubation_matcher2']","44d3f0a0":"relevant_extract = []\nfor index, row in df_covid19.iterrows():\n    mp_tuples = matched_paragraphs(row['spacy_doc'], matcher, incubation_matchers, row['sections'])\n    relevant_extract.append(create_text_from_paragraph_list(mp_tuples))\n\ndf_covid19['incubation_extract'] = relevant_extract\ndf_covid19['incubation_extract'] = df_covid19['incubation_extract'].apply(lambda x: np.nan if len(x) == 0 else x)\n\n# Create a dataframe with the results\ndf_incubation = df_covid19.loc[df_covid19['incubation_extract'].notna(), ['paper_id','title','authors', 'incubation_extract']]","b7ca52a8":"print('Number of papers related to Covid-19 that mention the incubation period: ' + str(df_incubation.shape[0]))","3bcddfa4":"df_incubation.head()","81890376":"get_matches(df_covid19.loc[129, 'spacy_doc'], matcher, incubation_matchers)","a378d597":"df_incubation.loc[129, 'incubation_extract']","7be70544":"matcher = Matcher(nlp.vocab)\ntransmission_pattern = [{\"LOWER\": {'IN': ['transmission', 'transmissibility','reproduction']}},\n                        {\"LOWER\":{'IN' : ['number','rate']}},\n                       {'OP':'*'},\n                       {'POS': 'NUM'}\n                       #,{\"LOWER\": {'IN': ['percent', '%']}}\n                       ]\n\ntransmission_pattern2 = [{\"LOWER\": 'reproductive'}, \n                         {\"LOWER\": 'rate'},\n                         {'OP':'*'},\n                         {'POS': 'NUM'}\n                         #,{\"LOWER\": {'IN': ['percent', '%']}}\n                        ]\n\n\ntransmission_pattern3 = [{\"LOWER\": 'environmental'}, \n                         {\"LOWER\": 'stability'},\n                         {'OP':'*'},\n                         {'POS': 'NUM'}]\n\n\nmatcher.add(\"transmission_matcher\", None, transmission_pattern)\nmatcher.add(\"transmission_matcher2\", None, transmission_pattern2)\nmatcher.add(\"transmission_matcher3\", None, transmission_pattern3)","e1d149a5":"transmission_matchers = ['transmission_matcher', 'transmission_matcher2','transmission_matcher3']","a9955eca":"# Extract relevant paragraphs\n\nrelevant_extract = []\nfor index, row in df_covid19.iterrows():\n    mp_tuples = matched_paragraphs(row['spacy_doc'], matcher, transmission_matchers, row['sections'])\n    relevant_extract.append(create_text_from_paragraph_list(mp_tuples))\n\ndf_covid19['transmission_extract'] = relevant_extract\ndf_covid19['transmission_extract'] = df_covid19['transmission_extract'].apply(lambda x: np.nan if len(x) == 0 else x)\n\n# Create a dataframe with the results\ndf_transmission = df_covid19.loc[df_covid19['transmission_extract'].notna(), ['paper_id','title','authors', 'transmission_extract']]\n\n","2f27abbd":"print('Number of papers related to Covid-19 that mention the transmission rate: ' + str(df_transmission.shape[0]))","6c1cb29f":"df_transmission.head()","3008efd4":"get_matches(df_covid19.loc[2164, 'spacy_doc'], matcher, transmission_matchers)","6b438653":"df_transmission.loc[2164, 'transmission_extract']","e717b422":"# Pattern for which you want to find matches\n\nmortality_pattern = [{\"LOWER\": {'IN': ['mortality', 'fatality']}},\n                     #{'LOWER': {'IN':['rate','rates']}},\n                     {'OP': '*'},\n                     {'POS': 'NUM'},\n                     {\"LOWER\": {'IN': ['percent', '%']}}]\n\n\n\nmatcher.add(\"mortality_matcher\", None, mortality_pattern)","b0e3b561":"relevant_extract = []\nfor index, row in df_covid19.iterrows():\n    mp_tuples = matched_paragraphs(row['spacy_doc'], matcher, ['mortality_matcher'], row['sections'])\n    relevant_extract.append(create_text_from_paragraph_list(mp_tuples))\n\ndf_covid19['mortality_extract'] = relevant_extract\ndf_covid19['mortality_extract'] = df_covid19['mortality_extract'].apply(lambda x: np.nan if len(x) == 0 else x)\n\n# Create a dataframe with the results\ndf_mortality = df_covid19.loc[df_covid19['mortality_extract'].notna(), ['paper_id','title','authors', 'mortality_extract']]","cce19edb":"print('Number of papers related to Covid-19 that mention the mortality rate: ' + str(df_mortality.shape[0]))","0461c77c":"df_mortality.head()","f6f25581":"get_matches(df_covid19.loc[3047, 'spacy_doc'], matcher, ['mortality_matcher'])","d73f02fd":"# Inspect some results\ndf_mortality.loc[3047, 'mortality_extract']","e5140d29":"columns_to_include = ['paper_id','abstract', 'title','publication_date']\ndf = df_covid19[columns_to_include]","a1b6e8b4":"for i in range(len(df)):\n    if df['abstract'].iloc[i] == \"\":\n        df['abstract'].iloc[i] = np.nan\n    if df['title'].iloc[i] == \"\":\n        df['title'].iloc[i] = np.nan","dfcfdb20":"sars_synonyms = [r'\\bsars\\b',\n                 'severe acute respiratory syndrome']","617c00a2":"df = tagger(df, sars_synonyms, 'sars')","ce2b8651":"mers_synonyms = [r'\\bmers\\b',\n                 'middle east respiratory syndrome']","dba48e7a":"df = tagger(df, mers_synonyms, 'mers')","dd2456f8":"df = tagger(df, corona_synonyms, 'corona')","732c00d0":"corona_synonyms = ['corona', r'\\bcov\\b']","bd654d19":"ards_synonyms = ['acute respiratory distress syndrome',\n                 r'\\bards\\b']","d1a27d2d":"df = tagger(df, ards_synonyms, 'ards')","d90fce58":"riskfac_synonyms = [\n    'risk factor analysis',\n    'cross sectional case control',\n    'prospective case control',\n    'matched case control',\n    'medical records review',\n    'seroprevalence survey',\n    'syndromic surveillance'\n]","a81658fe":"df = tagger(df, riskfac_synonyms, 'design_riskfac')","4f33f05f":"risk_factor_synonyms = ['risk factor',\n                        'risk model',\n                        'risk by',\n                        'comorbidity',\n                        'comorbidities',\n                        'coexisting condition',\n                        'co existing condition',\n                        'clinical characteristics',\n                        'clinical features',\n                        'demographic characteristics',\n                        'demographic features',\n                        'behavioural characteristics',\n                        'behavioural features',\n                        'behavioral characteristics',\n                        'behavioral features',\n                        'predictive model',\n                        'prediction model',\n                        'univariate', # implies analysis of risk factors\n                        'multivariate', # implies analysis of risk factors\n                        'multivariable',\n                        'univariable',\n                        'odds ratio', # typically mentioned in model report\n                        'confidence interval', # typically mentioned in model report\n                        'logistic regression',\n                        'regression model',\n                        'factors predict',\n                        'factors which predict',\n                        'factors that predict',\n                        'factors associated with',\n                        'underlying disease',\n                        'underlying condition']","ca6ee435":"df = tagger(df, risk_factor_synonyms, 'generic_risk_factors')","8e3ec8cd":"age_synonyms = ['median age',\n                'mean age',\n                'average age',\n                'elderly',\n                r'\\baged\\b',\n                r'\\bold',\n                'young',\n                'teenager',\n                'adult',\n                'child'\n               ]","5db71b86":"df = tagger(df, age_synonyms, 'risk_age')","2c490745":"sex_synonyms = ['sex',\n                'gender',\n                r'\\bmale\\b',\n                r'\\bfemale\\b',\n                r'\\bmales\\b',\n                r'\\bfemales\\b',\n                r'\\bmen\\b',\n                r'\\bwomen\\b'\n               ]","abe305a1":"df = tagger(df, sex_synonyms, 'risk_gender')","1013369e":"bodyweight_synonyms = [\n    'overweight',\n    'over weight',\n    'obese',\n    'obesity',\n    'bodyweight',\n    'body weight',\n    r'\\bbmi\\b',\n    'body mass',\n    'body fat',\n    'bodyfat',\n    'kilograms',\n    r'\\bkg\\b', # e.g. 70 kg\n    r'\\dkg\\b'  # e.g. 70kg\n]","2d09d881":"df = tagger(df, bodyweight_synonyms, 'risk_bodyweight')","0c5992b9":"smoking_synonyms = ['smoking',\n                    'smoke',\n                    'cigar', # this picks up cigar, cigarette, e-cigarette, etc.\n                    'nicotine',\n                    'cannabis',\n                    'marijuana']","e2ec94c9":"df = tagger(df, smoking_synonyms, 'risk_smoking')","96bce441":"diabetes_synonyms = [\n    'diabet', # picks up diabetes, diabetic, etc.\n    'insulin', # any paper mentioning insulin likely to be relevant\n    'blood sugar',\n    'blood glucose',\n    'ketoacidosis',\n    'hyperglycemi', # picks up hyperglycemia and hyperglycemic\n]","cda90c42":"df = tagger(df, diabetes_synonyms, 'risk_diabetes')","512def45":"hypertension_synonyms = [\n    'hypertension',\n    'blood pressure',\n    r'\\bhbp\\b', # HBP = high blood pressure\n    r'\\bhtn\\b' # HTN = hypertension\n]","86550630":"df = tagger(df, hypertension_synonyms, 'risk_hypertension')","1e34d080":"immunodeficiency_synonyms = [\n    'immune deficiency',\n    'immunodeficiency',\n    r'\\bhiv\\b',\n    r'\\baids\\b'\n    'granulocyte deficiency',\n    'hypogammaglobulinemia',\n    'asplenia',\n    'dysfunction of the spleen',\n    'spleen dysfunction',\n    'complement deficiency',\n    'neutropenia',\n    'neutropaenia', # alternate spelling\n    'cell deficiency' # e.g. T cell deficiency, B cell deficiency\n]","9f5208c1":"df = tagger(df, immunodeficiency_synonyms, 'risk_immunodeficiency')","f0821063":"cancer_synonyms = [\n    'cancer',\n    'malignant tumour',\n    'malignant tumor',\n    'melanoma',\n    'leukemia',\n    'leukaemia',\n    'chemotherapy',\n    'radiotherapy',\n    'radiation therapy',\n    'lymphoma',\n    'sarcoma',\n    'carcinoma',\n    'blastoma',\n    'oncolog'\n]","a068c879":"df = tagger(df, cancer_synonyms, 'risk_cancer')","92358d11":"chronicresp_synonyms = [\n    'chronic respiratory disease',\n    'asthma',\n    'chronic obstructive pulmonary disease',\n    r'\\bcopd',\n    'chronic bronchitis',\n    'emphysema'\n]","4cc85f9a":"df = tagger(df, chronicresp_synonyms, 'risk_chronic_respiratory_disease')","6f4ee59f":"asthma_synonyms = ['asthma']","2f292d9e":"df = tagger(df, asthma_synonyms, 'risk_asthma')","99554643":"immunity_synonyms = [\n    'immunity',\n    r'\\bvaccin',\n    'innoculat'\n]","9bae12dc":"df = tagger(df, immunity_synonyms,'immunity_generic')","eef7d161":"climate_synonyms = [\n    'climate',\n    'weather',\n    'humid',\n    'sunlight',\n    'air temperature',\n    'meteorolog', # picks up meteorology, meteorological, meteorologist\n    'climatolog', # as above\n    'dry environment',\n    'damp environment',\n    'moist environment',\n    'wet environment',\n    'hot environment',\n    'cold environment',\n    'cool environment'\n]","e18c4714":"df = tagger(df, climate_synonyms,'climate_generic')","81091fdf":"df['publication_date']= pd.to_datetime(df['publication_date'])","c0921471":"date_sars = pd.to_datetime('2003-04-01')\ndate_mers = pd.to_datetime('2012-09-01')\ndate_covid_19 = pd.to_datetime('2019-12-31')","330fbd6a":"def tagger_date(row):\n    date_sars = pd.to_datetime('2003-04-01')\n    date_mers = pd.to_datetime('2012-09-01')\n    date_covid_19 = pd.to_datetime('2019-12-31')\n    \n    if row['publication_date'] < date_sars :\n      return 'publication date before SARS'\n    if ((row['publication_date'] >= date_sars) & (row['publication_date'] < date_mers)):\n      return 'publication date after SARS (but before MERS)'\n    if ((row['publication_date'] >= date_mers) & (row['publication_date'] < date_covid_19)):\n      return 'publication date after MERS (but before COVID 19)'\n    if row['publication_date'] >= date_covid_19:\n      return 'publication date after COVID_19'","12436c19":"df['tag_publication_date'] = df.apply (lambda row: tagger_date(row), axis=1)","38d419ce":"df = df.drop(columns=['abstract', 'title','publication_date'])","f1c5f669":"df_covid19 = df_covid19.merge(df,on='paper_id',how='left')","8d213d2f":"df_mortality = df_mortality.merge(df,on='paper_id',how='left')","78b07da4":"df_incubation = df_incubation.merge(df,on='paper_id',how='left')","30e72dd6":"df_transmission = df_transmission.merge(df,on='paper_id',how='left')","49ae5e24":"df_covid19.head()","c379da2c":"df_incubation.head()","a85b4093":"df_transmission.head()","072f1828":"df_mortality.head()","b9d56aa7":"### Dataframe of papers with relevant transmission extracts (and additonal meta-data)","e45106c7":"## Demografic risk factors","452ed6ed":"### Remove words of certain type","52c54b61":"For each paper we add lists of abstract tokens, lemmas, POS tag of tokens, etc. to our dataframe","2fbc033a":"### Example: matches found in paper with index 2164","edbe5000":"### Define patterns\n\nWe define the pattern we want to match on and add it to our matcher","c2167012":"# Topic modelling (first iteration)","fdec02c7":"### Define patterns\n\nTo find all interesting passages about the transmission rate we define two patterns to match on. We add both to our matcher. ","6a93c75a":"### Example: matches found in paper with index 129","b272d0f2":"Below an example of each of the created dataframes is given. ","41af542c":"### remove elements which length < 3","0ca47290":"First we load the spacy model","db016029":"### Evaluation model","165e0027":"### Cancer","7b721905":"### Inspect some results","74e34fa5":"### Diabetes","008a5137":"## Incubation period","40772a69":"### Example: matches found in paper with index 3265","29ebd544":"We send the abstracts through the spaCy pipeline and save the output as a list of Doc objects","553791e1":"## Some initial cleaning steps\n\n- fill the NA-values in the title and abstract with an empty string (necessary for processing with fastText and spaCy)\n- concatenate the text paragraphs of the same sections (we will use this to locate our relevant extracts in a paper)","32f66133":"## Add additional meta data tags","af50ed5e":"### Immunodeficiency","34e38c97":"We merge the extra meta data tags with the previously created dataframes (df_covid19, df_mortality, df_incubation and df_transmission). ","c33c03dd":"# Python libraries","3508a1ba":"### Asthma","0c14485b":"## Keep only desired columns from covid 19 df","82990601":"### Inspect some results","3a5dd0bd":"\nTo find all interesting passages about the incubation period we define two patterns to match on. We add them both to our matcher. ","4df1f38b":" Following pre-processing steps were executed: \n - remove words of certain type (we only include nouns and proper nouns)\n - remove elements with less than 3 characters (in order to exclude typos and meaningless words)","e72d63bd":"In this chapter, we apply topic modelling on the abstract text of each paper to find related articles. After some extra pre-processing steps, a wordcloud was created in order to get already some first insights. Afterwards we used tf-idf and LDA to uncover keywords and hidden topics within the COVID-19 literature. ","eed0f792":"### Send the papers through the spacy pipeline","b488df78":"We used LDAvis, an interactive visualization system, that enables deep inspection of topics and keywords relationships in a LDA analytical model.  \nBasically, the visualization has two main pieces. On the left panel, a global view of the discovered topics are displayed. \nEach circle represents a topic and the area of the circles indicate the overall prevalence of the topics among the research papers. \nThe closer the topics are located to each other, the more they are related to each other. On the right panel on the other hand, the individual keywords are listed which are most useful for interpreting the currently selected topic. \nSo selecting a topic in the left panel reveals the most useful keywords in the right panel for interpreting that selected topic. \nNote that it is also possible to click on individual keywords to get more insights in which topics they occur. \n\nFrom this visualisation we derived the following insights. We set the relevance metrix to 0.48 in order to search for relevant topics which are rather unique for a specific topic. \n\n- **Cluster 1**: describes epidemic diseases and how healthcare and health policy respond to it\n- **Cluster 2**: describes reaction of immune system \n- **Cluster 3**: describes patients\n- **Cluster 4**: search for antibodies and antigens (also describes a part of the viruses and how they enter our body: cell, virus, peptide, fusion, epitope, entry, membrane, glycoprotein, vector, envelope ...)\n- **Cluster 5**: also investigates gene sequences\n- **Cluster 6**: Describes other viruses  (influenza, RSV, IBV HRV, HMPV, adenorvirus, rhinovirus, PEDV) \n- **Cluster 7**: Techniques to detect viruses (assay, detection, PCR, method, pcr, test, sequencing, ELISA, microarray)\n- **Cluster 8**: describes viruses on animals (What is the reason that this cluster is close to cluster 3?) \n- **Cluster 9**: describes spread of the virus\n- **Cluster 10**: describes the viruses 'SARS', 'MERS', 'COV', 'Corona' (on first sight, it seems that this cluster describes the - - diseases that we are interested in)","51491571":"### prevent sentence splitting on 'et al.'\n\nWe define some additional functions to prevent splitting a sentence in two separate sentences if 'et al.' occurs within it. ","dcb42b1a":"We use spaCy's Token Matcher functionality to match sequences of tokens, based on pattern rules. Compared to using regular expressions on raw text, spaCy\u2019s rule-based matcher engines and components not only let you find the words and phrases you\u2019re looking for \u2013 they also give you access to the tokens within the document and their relationships.\n\n- More information about rule-based matching: https:\/\/spacy.io\/usage\/rule-based-matching\n- Inspiration: https:\/\/www.kaggle.com\/cstefanache\/nlp-text-mining-disease-behavior\n\nWe created patterns for the following three items:\n- Incubation period\n- Transmission rate and environmental stability\n- Mortality rate\n\nFor each of these items we searched for relevant papers and passages according to predefined matchers. \nFor every item we created a dataframe which contains only papers which deal with one of the items 'incubation period', 'transmission rate' or 'mortality rate'. Next to it, we included the sentences in which relevant information (= matched patterns) were found. So if a certain sentence matches with our predefined patterns, this sentence is included together with the previous and next sentence (to provide some additional context). In order to help researchers to find these sentences quickly within a given paper, we also included the section title in which the corresponding sencence appears. \n","bf0c8be4":"### Gender","293b98dc":"Based on the outcome of the tf-idf model, the 10 most frequent terms are derived and listed for every research paper. ","268a1107":"## Word cloud","023bd966":"### Chronic respiratory disease","9faa19aa":"### Top 10 key words","28793fd6":"We used a tf-idf model to reflect how important a word is to a specific paper within the covid-19 literature. ","d80cd5ae":"## tfidf","afc027da":"### Define patterns","603f9764":"# Data pre-processing\nIn this chapter we:\n - read the metadata on the CORD-19 papers from the metadata.csv\n - import the body texts and the sections (section title + text) of papers from json files\n - perform some data cleaning\n - detect the paper's language with a fastText model\n - Send the abstract paper text through a SpaCy pipeline (for each paper we add lists of abstract tokens, lemmas, POS tag of tokens, etc. to our dataframe)","71c607f9":"### Hypertension","5db3b6ee":"### generic risk factors","7d6e055e":"### Model","30ddaeb3":"### Add tags based on release date","c3718533":"## Detection of the language of the papers using fasttext","eb840f14":"### Define additional helper functions for pattern matching","d640e2a1":"We used following python libraries:","943323a0":"### Inspect some results","ce51fcc6":"## add additional tags","4b9e4861":"Next to pyLDAvis, we also used T-SNE to plot the derived topic clusters in a clustering chart. ","cf661493":"## LDA","91bf6e18":"## Import the json files","50e5c081":"## Import the metadata file","d1e0e338":"## Example dataframes","3b6f71a8":"### Severe Acute Respiratory Syndrome (SARS)","be88c9b2":"Within this notebook we want to address the question: \n\n**\"What is known about incubation, transmission, environmental stability and mortality within the COVID-19 literature?\"**\n\nIn a first iteration, we explored the medical\/scientific articles by applying Topic Modelling. We built an LDA and tf-idf model to discover hidden topics and keywords within the medical\/scientific articles. We used the gained knowledge to identify common terms within the COVID-19 literature. Next to it, we created an interactive visualization which allow researchers to flexibly explore the topics within the range of available papers.\n\nIn a second iteration we used SpaCy's Token Matcher functionality to match sequences of tokens, based on pattern rules. \nWe created patterns for the following three items: \n\n<ul>\n<li>Incubation period<\/li>\n<li>Transmission rate and environmental stability<\/li>\n<li>Mortality rate<\/li>\n<\/ul>\n\nFor each of these items we created a dataframe which contains the relevant papers together with the relevant sentences which contain information about incubation periods, transmission rates and mortality rates respectively. In order to help researchers to find these sentences quickly within a given paper, we also included the section title in which the corresponding sencence appears.\n\nNote that we also added extra meta data tags for each of these papers (e.g. if a paper deals with specific risk factor) to provide additional insights. ","d2319081":"### corona","199e2e3f":"### Acute Respiratory Distress Syndrome (ARDS)","50155cb2":"A word cloud is created to highlight popular or trending terms based on frequency of use and prominence. ","6a52feea":"Additional helper functions are defined in order to retrieve a section title from a certain sentence, paragrahps and matches. ","898356df":"## SpaCy pipeline","a42c0250":"In order to enrich the created dataframes with some additional meta-data, we tagged every paper with relevant themes (e.g. tag_risk_diabetis, tag_risk_mers) using rules based on synonyms and related terms. We apply these rules to the title and abstract of every paper in order to assign a certain tag to it. \n\nInspiration: https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions#Geographies","1d6be13c":"The LDA is evaluated based on the following metrics: \n\n- Perplexity (\"commonly used measurement in information theory to evaluate how well a statistical model describes a dataset, with lower perplexity denoting a better probabilistic model.\")\n- Coherence (\"Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic.)","92874914":"### Research design","f8135f5f":"# Covid-19 Challenge: Topic modelling and rule-based matching\n\nCOVID-19 Open Research Dataset Challenge (CORD-19)\n","4534092d":"## Exra pre-processing","333538bc":"# Rule-based matching with SpaCy","5bb43e3f":"### Extract relevant passages from all papers","689a81f7":"### Immunity","9b20b850":"### bodyweight","aba253b0":"### Middle East Respiratory Syndrome (MERS)","9da63fad":"### Dataframe of papers filtered on covid19 synonyms (and additional meta-data)","dc4da0c9":"### Dataframe of papers with relevant incubation extracts (and additional meta-data)","c7d02dd3":"An LDA model is trained to discover hidden topics within the COVID-19 literature. This allows users to view each article as a mixture of these topics. Next to it, we clustered all the documents based on the most dominant topic in each document. \nBy mapping a specific article into the topic space, we can find related articles. ","b5e79ffa":"### t-SNE Clustering Chart","1f8bd2dd":"### Extract relevant passages from all papers","3957f905":"### Extract relevant passages from all papers","ea38b97a":"### Filtering of relevant papers\nWe search on a synonym list of Covid-19 and SARS-CoV-2 to determine which papers are about the novel corona virus or the disease it causes. In this way we exlude irrelevant papers, since a lot of the available papers are not about COVID-19.\n\n*Inspiration: https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions#Geographies*","23de1a11":"## Merge with dataframes","67077e56":"## Mortality ","5dc2f9ed":"### Smoking","302db7f6":"### Dateframe of papers with relevant mortality extracts (and additional meta-data)","88ebe806":"## Pre-processing steps","36907f1c":"### Model","9054be63":"## Transmission Rate","75a6345e":"### Climate","73e17b75":"### pyLDAvis"}}