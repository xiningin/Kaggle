{"cell_type":{"1d40905a":"code","9e3febb4":"code","44d816d9":"code","a69fb1e6":"code","38b9249f":"code","53bb496e":"code","f2b69c3e":"code","96d4c2eb":"code","3ba94242":"code","0a8abe98":"code","36b0a666":"code","3307f8c6":"code","2c42ad46":"code","cf78c318":"code","aead4c52":"code","fd1d0b50":"code","69c5b61c":"code","8d8c8fd0":"code","3fc91c19":"code","370d87f9":"code","b243e44d":"code","d0937859":"code","d0202347":"code","b7bbb0b7":"code","9345344a":"code","f6eb7e58":"code","5dd48d28":"code","c09b6942":"code","3b40a82b":"code","71b1c662":"code","28b61d55":"code","90d492a9":"code","5b3e796e":"markdown","03b06aa4":"markdown","66317487":"markdown","3e6e3fc5":"markdown","cd67a9e0":"markdown","e5c6f2ec":"markdown","928a5bd8":"markdown","61d54fc4":"markdown","ae28e20b":"markdown","36e650fe":"markdown","9a234d48":"markdown","5e2a0a05":"markdown","3a3fbd88":"markdown","7df8a618":"markdown","afa3d9fe":"markdown","5b30934e":"markdown","1bd788c3":"markdown","3eb6986c":"markdown"},"source":{"1d40905a":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ntorch.manual_seed(42)","9e3febb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport sys\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport zipfile\nimport matplotlib.pyplot as plt\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\n\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \nseed = 42\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44d816d9":"with zipfile.ZipFile('..\/input\/data-science-bowl-2018\/stage1_train.zip') as z:\n    z.extractall('stage1_train')\n\nwith zipfile.ZipFile('..\/input\/data-science-bowl-2018\/stage1_test.zip') as z:\n    z.extractall('stage1_test')\n    \nwith zipfile.ZipFile('..\/input\/data-science-bowl-2018\/stage1_test.zip') as z:\n    z.extractall('stage2_test_final')    ","a69fb1e6":"TRAIN_PATH = '\/kaggle\/working\/stage1_train\/'\nTEST_PATH = '\/kaggle\/working\/stage1_test\/'\nTEST_PATH_2 = '\/kaggle\/working\/stage2_test_final\/'\n\ntrain_files = next(os.walk(TRAIN_PATH))[1]\ntest_files = next(os.walk(TEST_PATH))[1]\ntest_files_2 = next(os.walk(TEST_PATH_2))[1]\ntest_files_final = test_files + test_files_2","38b9249f":"X_train = np.zeros((len(train_files), 128, 128, 3), dtype = np.uint8)\nY_train = np.zeros((len(train_files), 128, 128, 1), dtype = np.bool)\nX_test = np.zeros((len(test_files), 128, 128, 3), dtype = np.uint8)\nX_test_2 = np.zeros((len(test_files), 128, 128, 3), dtype = np.uint8)\n\nprint('Getting training data...')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_files), total = len(train_files)):\n    img_path = TRAIN_PATH + id_ + '\/images\/' + id_ + '.png'\n    img = imread(img_path)[:,:,:3]\n    img = resize(img, (128, 128), mode='constant', preserve_range=True)\n    X_train[n] = img\n    \n    masks_path = TRAIN_PATH + id_ + '\/masks\/'\n    mask = np.zeros((128, 128, 1))\n    mask_images = next(os.walk(masks_path))[2]\n    for mask_id in mask_images:\n        mask_path = masks_path + mask_id\n        mask_ = imread(mask_path)\n        mask_ = np.expand_dims(resize(mask_, (128, 128), mode='constant', preserve_range=True), axis=-1)\n        mask = np.maximum(mask, mask_)\n    Y_train[n] = mask\n\nprint('Getting testing data for stage 1...')\nsys.stdout.flush()\n\nsizes_test = []\nfor n, id_ in tqdm(enumerate(test_files), total = len(test_files)):\n    img_path = TEST_PATH + id_ + '\/images\/' + id_ + '.png'\n    img = imread(img_path)[:,:,:3]\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (128, 128), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Getting testing data for stage 2...')\nsys.stdout.flush()\n\nsizes_test_2 = []\nfor n, id_ in tqdm(enumerate(test_files_2), total = len(test_files_2)):\n    img_path = TEST_PATH + id_ + '\/images\/' + id_ + '.png'\n    img = imread(img_path)[:,:,:3]\n    sizes_test_2.append([img.shape[0], img.shape[1]])\n    img = resize(img, (128, 128), mode='constant', preserve_range=True)\n    X_test_2[n] = img\n\nprint('Done!')","53bb496e":"X_test_final = np.concatenate((X_test, X_test_2), axis = 0)\nsizes_test_final = sizes_test + sizes_test_2\nprint(X_test_final.shape)","f2b69c3e":"class Nuc_Seg(Dataset):\n    def __init__(self, images_np, masks_np):\n        self.images_np = images_np\n        self.masks_np = masks_np\n    \n    def transform(self, image_np, mask_np):\n        ToPILImage = transforms.ToPILImage()\n        image = ToPILImage(image_np)\n        mask = ToPILImage(mask_np.astype(np.int32))\n        \n        image = TF.pad(image, padding = 20, padding_mode = 'reflect')\n        mask = TF.pad(mask, padding = 20, padding_mode = 'reflect')\n        \n        angle = random.uniform(-10, 10)\n        width, height = image.size\n        max_dx = 0.1 * width\n        max_dy = 0.1 * height\n        translations = (np.round(random.uniform(-max_dx, max_dx)), np.round(random.uniform(-max_dy, max_dy)))\n        scale = random.uniform(0.8, 1.2)\n        shear = random.uniform(-0.5, 0.5)\n        image = TF.affine(image, angle = angle, translate = translations, scale = scale, shear = shear)\n        mask = TF.affine(mask, angle = angle, translate = translations, scale = scale, shear = shear)\n        \n        image = TF.center_crop(image, (128, 128))\n        mask = TF.center_crop(mask, (128, 128))\n        \n        image = TF.to_tensor(image)\n        mask = TF.to_tensor(mask)\n        return image, mask\n        \n    def __len__(self):\n        return len(self.images_np)\n    \n    def __getitem__(self, idx):\n        image_np = self.images_np[idx]\n        mask_np = self.masks_np[idx]\n        image, mask = self.transform(image_np, mask_np)\n        \n        return image, mask    ","96d4c2eb":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = seed)\n\ntrain_dataset = Nuc_Seg(X_train, Y_train)\ntrain_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\nvalid_dataset = Nuc_Seg(X_val, Y_val)\nvalid_loader = DataLoader(valid_dataset, batch_size = 16, shuffle = True)","3ba94242":"fig, axis = plt.subplots(2, 2)\naxis[0][0].imshow(X_train[0].astype(np.uint8))\naxis[0][1].imshow(np.squeeze(Y_train[0]).astype(np.uint8))\naxis[1][0].imshow(X_val[0].astype(np.uint8))\naxis[1][1].imshow(np.squeeze(Y_val[0]).astype(np.uint8))","0a8abe98":"%matplotlib inline\n\nfor ex_img, ex_mask in train_loader:\n    \n    img = np.array(TF.to_pil_image(ex_img[0]))\n    mask = np.array(TF.to_pil_image(ex_mask[0]))\n    \n    fig, (axis_1, axis_2) = plt.subplots(1, 2)\n    axis_1.imshow(img.astype(np.uint8))\n    axis_2.imshow(mask.astype(np.uint8))\n    \n    break","36b0a666":"def iou(pred, target, n_classes = 2):\n    \n    iou = []\n    pred = pred.view(-1)\n    target = target.view(-1)\n\n    # Ignore IoU for background class (\"0\")\n    for cls in range(1, n_classes):  # This goes from 1:n_classes-1 -> class \"0\" is ignored\n      pred_inds = pred == cls\n      target_inds = target == cls\n      intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()  # Cast to long to prevent overflows\n      union = pred_inds.long().sum().data.cpu().item() + target_inds.long().sum().data.cpu().item() - intersection\n    \n      if union == 0:\n        iou.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n      else:\n        iou.append(float(intersection) \/ float(max(union, 1)))\n     \n    return sum(iou)","3307f8c6":"def iou_metric(y_pred, y_true, n_classes = 2):\n    miou = []\n    for i in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = (y_pred > i)\n        iou_init = iou(y_pred_, y_true, n_classes = n_classes)\n        miou.append(iou_init)\n    \n    return sum(miou)\/len(miou)","2c42ad46":"class UNet(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.conv1_1 = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n        self.drop1_1 = nn.Dropout2d(0.1)\n        self.conv1_2 = nn.Conv2d(16, 16, kernel_size = 3, padding = 1)\n        \n        self.conv2_1 = nn.Conv2d(16, 32, kernel_size = 3, padding = 1)\n        self.drop2_1 = nn.Dropout2d(0.1)\n        self.conv2_2 = nn.Conv2d(32, 32, kernel_size = 3, padding = 1)\n        \n        self.conv3_1 = nn.Conv2d(32, 64, kernel_size = 3, padding = 1)\n        self.drop3_1 = nn.Dropout2d(0.2)\n        self.conv3_2 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n        \n        self.conv4_1 = nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n        self.drop4_1 = nn.Dropout2d(0.2)\n        self.conv4_2 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n        \n        self.conv5_1 = nn.Conv2d(128, 256, kernel_size = 3, padding = 1)\n        self.drop5_1 = nn.Dropout2d(0.3)\n        self.conv5_2 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n        \n        self.conv_trans6_1 = nn.ConvTranspose2d(256, 128, kernel_size = 2, stride = (2, 2))\n        self.conv6_1 = nn.Conv2d(256, 128, kernel_size = 3, padding = 1)\n        self.drop6_1 = nn.Dropout2d(0.2)\n        self.conv6_2 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n        \n        self.conv_trans7_1 = nn.ConvTranspose2d(128, 64, kernel_size = 2, stride = (2, 2))\n        self.conv7_1 = nn.Conv2d(128, 64, kernel_size = 3, padding = 1)\n        self.drop7_1 = nn.Dropout2d(0.2)\n        self.conv7_2 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n        \n        self.conv_trans8_1 = nn.ConvTranspose2d(64, 32, kernel_size = 2, stride = (2, 2))\n        self.conv8_1 = nn.Conv2d(64, 32, kernel_size = 3, padding = 1)\n        self.drop8_1 = nn.Dropout2d(0.1)\n        self.conv8_2 = nn.Conv2d(32, 32, kernel_size = 3, padding = 1)\n        \n        self.conv_trans9_1 = nn.ConvTranspose2d(32, 16, kernel_size = 2, stride = (2, 2))\n        self.conv9_1 = nn.Conv2d(32, 16, kernel_size = 3, padding = 1)\n        self.drop9_1 = nn.Dropout2d(0.1)\n        self.conv9_2 = nn.Conv2d(16, 16, kernel_size = 3, padding = 1)\n        \n        self.conv10 = nn.Conv2d(16, 1, kernel_size = 3, padding = 1)\n    \n    def forward(self, s):\n        \n        c1 = F.elu(self.conv1_1(s))\n        c1 = self.drop1_1(c1)\n        c1 = F.elu(self.conv1_2(c1))\n        p1 = F.max_pool2d(c1, kernel_size = (2, 2), stride = 2)\n        \n        c2 = F.elu(self.conv2_1(p1))\n        c2 = self.drop2_1(c2)\n        c2 = F.elu(self.conv2_2(c2))\n        p2 = F.max_pool2d(c2, kernel_size = (2, 2), stride = 2)\n        \n        c3 = F.elu(self.conv3_1(p2))\n        c3 = self.drop3_1(c3)\n        c3 = F.elu(self.conv3_2(c3))\n        p3 = F.max_pool2d(c3, kernel_size = (2, 2), stride = 2)\n        \n        c4 = F.elu(self.conv4_1(p3))\n        c4 = self.drop4_1(c4)\n        c4 = F.elu(self.conv4_2(c4))\n        p4 = F.max_pool2d(c4, kernel_size = (2, 2), stride = 2)\n        \n        c5 = F.elu(self.conv5_1(p4))\n        c5 = self.drop5_1(c5)\n        c5 = F.elu(self.conv5_2(c5))\n        \n        u6 = self.conv_trans6_1(c5)\n        u6 = torch.cat((u6, c4), axis = 1)\n        c6 = F.elu(self.conv6_1(u6))\n        c6 = self.drop6_1(c6)\n        c6 = F.elu(self.conv6_2(c6))\n        \n        u7 = self.conv_trans7_1(c6)\n        u7 = torch.cat((u7, c3), axis = 1)\n        c7 = F.elu(self.conv7_1(u7))\n        c7 = self.drop7_1(c7)\n        c7 = F.elu(self.conv7_2(c7))\n    \n        u8 = self.conv_trans8_1(c7)\n        u8 = torch.cat((u8, c2), axis = 1)\n        c8 = F.elu(self.conv8_1(u8))\n        c8 = self.drop8_1(c8)\n        c8 = F.elu(self.conv8_2(c8))\n        \n        u9 = self.conv_trans9_1(c8)\n        u9 = torch.cat((u9, c1), axis = 1)\n        c9 = F.elu(self.conv9_1(u9))\n        c9 = self.drop9_1(c9)\n        c9 = F.elu(self.conv9_2(c9))\n        \n        output = torch.sigmoid(self.conv10(c9))\n        \n        return output        ","cf78c318":"model = UNet()\nmodel = model.float()\nmodel = model.to(device)\nprint(model)","aead4c52":"total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(total_params)","fd1d0b50":"opt = optim.Adam(model.parameters(), lr = 0.001)\nloss_func = nn.BCELoss()\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, patience = 3, verbose = 1)","69c5b61c":"def fit(model, epochs, opt, loss_func, train_loader, valid_loader, alpha):\n\n    for epoch in range(epochs):\n        \n        #Going into training mode\n        model.train()\n        \n        train_loss = 0 \n        iou = 0\n        \n        for image, mask in train_loader:\n            image = image.to(device)   #Passing the input mini-batch to the GPU\n            mask = mask.to(device)   #Passing the label mini-batch to the GPU\n            opt.zero_grad()      #Setting the grads to zero to avoid accumulation of gradients\n            out = model(image.float())\n            loss = loss_func(out.float(), mask.float())    \n            train_loss += loss\n            \n            iou += iou_metric(out, mask)\n            iou_rev = 16 - iou_metric(out, mask)\n            loss += alpha * iou_rev\n            \n            loss.backward()\n            opt.step()\n        \n        lr_scheduler.step(train_loss\/len(train_loader))   #Setting up lr decay  \n        \n        model.eval()            #Going into eval mode                            \n        with torch.no_grad():   #No backprop\n            valid_loss = 0\n            valid_iou = 0\n            \n            for image_val, mask_val in valid_loader:\n                image_val = image_val.to(device)  \n                mask_val = mask_val.to(device)\n                out_val = model(image_val.float())\n                valid_loss += loss_func(out_val.float(), mask_val.float())\n                \n                valid_iou += iou_metric(out_val, mask_val)\n        \n        print(\"Epoch \", epoch + 1, \" Training Loss: \", train_loss\/len(train_loader), \"CV Loss: \", valid_loss\/len(valid_loader))\n        print(\"Training IoU: \", iou\/len(train_loader), \"CV IoU: \", valid_iou\/len(valid_loader))","8d8c8fd0":"fit(model, 30, opt, loss_func, train_loader, valid_loader, 5)","3fc91c19":"%matplotlib inline\n\nfor ex_img, ex_mask in train_loader:\n    \n    img = ex_img[1].to(device)\n    img.unsqueeze_(0)\n    mask_pred = model(img.float())\n    mask_pred = mask_pred.cpu()\n    mask_pred = (mask_pred > 0.75)\n    mask_true = ex_mask[1]\n    \n    img = TF.to_pil_image(mask_pred.float().squeeze(0))\n    mask = TF.to_pil_image(mask_true)\n    \n    img = np.array(img)\n    mask = np.array(mask)\n    \n    fig, (axis_1, axis_2) = plt.subplots(1, 2)\n    axis_1.imshow(img.astype(np.uint8))\n    axis_2.imshow(mask.astype(np.uint8))\n    \n    break","370d87f9":"# Run-length encoding stolen from https:\/\/www.kaggle.com\/rakhlin\/fast-run-length-encoding-python\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef prob_to_rles(x, cutoff=0.5):\n    lab_img = label(x > cutoff)\n    for i in range(1, lab_img.max() + 1):\n        yield rle_encoding(lab_img == i)","b243e44d":"with torch.no_grad():\n    \n    test = torch.from_numpy(X_test)\n    test = test\/255.0\n    test = test.permute(0, 3, 1, 2)\n    test = test.to(device)\n    preds = model(test.float())\n    preds = preds.permute(0, 2, 3, 1) \n    print(preds.shape)\n\n    preds = preds*255.0\n    preds = preds.cpu().numpy()\n\nprint(preds.shape)","d0937859":"preds_t = (preds > 0.5).astype(np.uint8)\n\npreds_upsampled = []\nfor i in range(len(preds)):\n    preds_upsampled.append(resize(np.squeeze(preds[i]), \n                                       (sizes_test[i][0], sizes_test[i][1]), \n                                       mode='constant', preserve_range=True))    ","d0202347":"new_test_ids = []\nrles = []\nfor n, id_ in enumerate(test_files):\n    rle = list(prob_to_rles(preds_upsampled[n]))\n    rles.extend(rle)\n    new_test_ids.extend([id_] * len(rle))","b7bbb0b7":"sub = pd.DataFrame()\nsub['ImageId'] = new_test_ids\nsub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsub.to_csv('sub-dsbowl2018-1.csv', index=False)","9345344a":"with torch.no_grad():\n    \n    test = torch.from_numpy(X_test_2)\n    test = test\/255.0\n    test = test.permute(0, 3, 1, 2)\n    test = test.to(device)\n    preds_2 = model(test.float())\n    preds_2 = preds_2.permute(0, 2, 3, 1) \n    print(preds_2.shape)\n\n    preds_2 = preds_2*255.0\n    preds_2 = preds_2.cpu().numpy()\n\nprint(preds_2.shape)","f6eb7e58":"preds_t_2 = (preds_2 > 0.5).astype(np.uint8)\n\npreds_upsampled_2 = []\nfor i in range(len(preds_2)):\n    preds_upsampled_2.append(resize(np.squeeze(preds_2[i]), \n                                       (sizes_test_2[i][0], sizes_test_2[i][1]), \n                                       mode='constant', preserve_range=True))    ","5dd48d28":"new_test_ids_2 = []\nrles_2 = []\nfor n, id_ in enumerate(test_files_2):\n    rle = list(prob_to_rles(preds_upsampled_2[n]))\n    rles_2.extend(rle)\n    new_test_ids_2.extend([id_] * len(rle))","c09b6942":"sub_2 = pd.DataFrame()\nsub_2['ImageId'] = new_test_ids_2\nsub_2['EncodedPixels'] = pd.Series(rles_2).apply(lambda x: ' '.join(str(y) for y in x))\nsub_2.to_csv('sub-dsbowl2018-2.csv', index=False)","3b40a82b":"with torch.no_grad():\n    \n    test = torch.from_numpy(X_test_final)\n    test = test\/255.0\n    test = test.permute(0, 3, 1, 2)\n    test = test.to(device)\n    preds_final = model(test.float())\n    preds_final = preds_final.permute(0, 2, 3, 1) \n    print(preds_final.shape)\n\n    preds_final = preds_final*255.0\n    preds_final = preds_final.cpu().numpy()\n\nprint(preds_final.shape)","71b1c662":"preds_t_final = (preds_final > 0.5).astype(np.uint8)\n\npreds_upsampled_final = []\nfor i in range(len(preds_final)):\n    preds_upsampled_final.append(resize(np.squeeze(preds_final[i]), \n                                       (sizes_test_final[i][0], sizes_test_final[i][1]), \n                                       mode='constant', preserve_range=True))    ","28b61d55":"new_test_ids_final = []\nrles_final = []\nfor n, id_ in enumerate(test_files_final):\n    rle = list(prob_to_rles(preds_upsampled_final[n]))\n    rles_final.extend(rle)\n    new_test_ids_final.extend([id_] * len(rle))","90d492a9":"sub_final = pd.DataFrame()\nsub_final['ImageId'] = new_test_ids_2\nsub_final['EncodedPixels'] = pd.Series(rles_final).apply(lambda x: ' '.join(str(y) for y in x))\nsub_final.to_csv('sub-dsbowl2018-final.csv', index=False)","5b3e796e":"# Checking One Random Training & Validation Image","03b06aa4":"# Defining Train & Test Images ","66317487":"# Defining DataLoader","3e6e3fc5":"# Combined Stage 1 & 2 Predictions","cd67a9e0":"# Combined Stage 1 & 2 File Submission","e5c6f2ec":"# Comparing Random Predicted and True Masks","928a5bd8":"# Defining Class for Data Augmentation","61d54fc4":"# Predictions for Stage 2 Testing Images ","ae28e20b":"# Predictions for Stage 1 Testing Images","36e650fe":"# Stage 1 Submission File","9a234d48":"# Checking Random Augmented Image","5e2a0a05":"# Defining Optimizer, Callback and Loss Function","3a3fbd88":"# Defining UNet Model","7df8a618":"# Stage 2 Submission File","afa3d9fe":"# Extract Relevant Files","5b30934e":"# Defining IoU Metric","1bd788c3":"# Defining Accelerator","3eb6986c":"# Defining RLE Encoding Functions"}}