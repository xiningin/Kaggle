{"cell_type":{"a87a2aca":"code","de412e99":"code","5ee55b5a":"code","2a9263eb":"code","7ef7ca8a":"code","9411969d":"code","045f4da6":"code","c5612362":"code","fddaf1a7":"code","66357196":"code","ab32be2d":"code","792da983":"code","644bca62":"code","912d8b7d":"code","31ae6f00":"markdown","693559aa":"markdown","f19bbcae":"markdown","5be1270d":"markdown","8129061a":"markdown","d10f71df":"markdown","f1dd5755":"markdown","cf513d60":"markdown","6337f23c":"markdown"},"source":{"a87a2aca":"import torch\nfrom torch import nn\nfrom torchvision import transforms as T\n\nimport time, datetime\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\nfrom collections import deque\nimport random, datetime, os, copy\n\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\n\n# change directory\nimport os\nos.chdir('\/kaggle\/input\/gym-super-mario-bros')\n\n# install controller\n!pip --disable-pip-version-check install -q nes_py\nfrom nes_py.wrappers import JoypadSpace\n\nimport gym_super_mario_bros","de412e99":"# Initialize Super Mario environment\nenv = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n\n# Limit the action-space to\n#   0. walk right\n#   1. jump right\nenv = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n\nenv.reset()\nnext_state, reward, done, info = env.step(action=0)\nprint(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")","5ee55b5a":"class SkipFrame(gym.Wrapper):\n    '''\n        Custom wrapper that inherits from gy.Wrapper and implements the step() function.\n        Use it to return only every skip nth frame\n    '''\n    def __init__(self, env, skip):\n        super().__init__(env)\n        self._skip = skip\n        \n    def step(self, action):\n        total_reward = 0.0\n        done = False\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n        return obs, total_reward, done, info","2a9263eb":"class GrayScaleObservation(gym.ObservationWrapper):\n    '''\n        Common wrapper to transform an RGB image to grayscale; doing so reduces the size of the state representation without losing useful information.\n        Now the size of each state: [1, 240, 256]\n    '''\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape[:2]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n        self.transform = T.Grayscale()\n    \n    def permute_orientation(self, observation):\n        observation = np.transpose(observation, (2, 0, 1))\n        return torch.tensor(observation.copy(), dtype=torch.float)\n    \n    def observation(self, observation):\n        observation = self.permute_orientation(observation)\n        return self.transform(observation)","7ef7ca8a":"class ResizeObservation(gym.ObservationWrapper):\n    '''\n        Downsamples each observation into a square image.\n        New size: [1, 84, 84]\n    '''\n    def __init__(self, env, shape):\n        super().__init__(env)\n        if isinstance(shape, int): self.shape = (shape, shape)\n        else: self.shape = tuple(shape)\n\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n        \n    def observation(self, observation):\n        transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n        return transforms(observation).squeeze(0)","9411969d":"# preprocess environment\nenv = SkipFrame(env, skip=4)\nenv = GrayScaleObservation(env)\nenv = ResizeObservation(env, shape=84)\nenv = FrameStack(env, num_stack=4)","045f4da6":"class MarioNet(nn.Module):\n    \"\"\"\n        input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n    \"\"\"\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, w, h = input_dim\n        \n        if h != 84: raise ValueError(f\"Expecting input height: 84, got: {h}\")\n        if w != 84: raise ValueError(f\"Expecting input width: 84, got: {w}\")\n            \n        self.online = nn.Sequential(\n            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3136, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim)\n        )\n        \n        self.target = copy.deepcopy(self.online)\n        \n        # freeze Q-target parameters\n        for p in self.target.parameters():\n            p.requires_grad = False\n            \n    def forward(self, input, model):\n        if model == \"online\":\n            return self.online(input)\n        elif model == \"target\":\n            return self.target(input)","c5612362":"class Mario:\n    '''\n        Mario randomly explores with a chance of self.exploration_rate.\n        When he chooses to exploit, he relies on MarioNet to provide the most optimal action.\n    '''\n    def __init__(self, state_dim, action_dim, use_cuda):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.use_cuda = use_cuda\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 32\n        \n        self.net = MarioNet(self.state_dim, self.action_dim).float()\n        if self.use_cuda:\n            self.net = self.net.to(device=\"cuda\")\n        \n        self.exploration_rate = 1\n        self.exploration_rate_decay = 0.99999975\n        self.exploration_rate_min = 0.1\n        self.curr_step = 0\n\n        self.save_every = 5e5\n    \n    \n    def act(self, state):\n        '''\n            Given a state, choose an epsilon-greedy action and update value of step.\n        '''\n        # exploration\n        if np.random.rand() < self.exploration_rate:\n            action_idx = np.random.randint(self.action_dim)\n            \n        # exploitation\n        else:\n            state = state.__array__()\n            \n            if self.use_cuda: state = torch.tensor(state).cuda()\n            else: state = torch.tensor(state)\n                \n            state = state.unsqueeze(0)\n            action_values = self.net(state, model=\"online\")\n            action_idx = torch.argmax(action_values, axis=1).item()\n            \n        # decrease exploration_rate\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n        \n        # increment step\n        self.curr_step += 1\n        return action_idx\n    \n    \n    def cache(self, state, next_state, action, reward, done):\n        \"\"\"\n            Store the experience to self.memory (replay buffer).\n        \"\"\"\n        state = state.__array__()\n        next_state = next_state.__array__()\n        \n        state = torch.tensor(state)\n        next_state = torch.tensor(next_state)\n        action = torch.tensor([action])\n        reward = torch.tensor([reward])\n        done = torch.tensor([done])\n        \n        if self.use_cuda:\n            state = state.cuda()\n            next_state = next_state.cuda()\n            action = action.cuda()\n            reward = reward.cuda()\n            done = done.cuda()\n            \n        self.memory.append((state, next_state, action, reward, done,))\n        \n        \n    def recall(self):\n        \"\"\"\n            Retrieve a batch of experiences from memory\n        \"\"\"\n        batch = random.sample(self.memory, self.batch_size)\n        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()","fddaf1a7":"class Mario(Mario):\n    def __init__(self, state_dim, action_dim, use_cuda):\n        super().__init__(state_dim, action_dim, use_cuda)\n        self.gamma = 0.9\n        self.burnin = 1e4  # min. experiences before training\n        self.learn_every = 3  # no. of experiences between updates to Q_online\n        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n        self.loss_fn = torch.nn.SmoothL1Loss()\n\n        \n    def td_estimate(self, state, action):\n        current_Q = self.net(state, model=\"online\")[\n            np.arange(0, self.batch_size), action\n        ]\n        return current_Q\n\n    \n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q = self.net(next_state, model=\"online\")\n        best_action = torch.argmax(next_state_Q, axis=1)\n        next_Q = self.net(next_state, model=\"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n    \n    \n    def update_Q_online(self, td_estimate, td_target):\n        loss = self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    \n    def sync_Q_target(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())\n        \n    \n    def learn(self):\n        if self.curr_step % self.sync_every == 0: self.sync_Q_target()\n        if self.curr_step % self.save_every == 0: self.save()\n        if self.curr_step < self.burnin: return None, None\n        if self.curr_step % self.learn_every != 0: return None, None\n\n        # Sample from memory\n        state, next_state, action, reward, done = self.recall()\n\n        # Get TD Estimate\n        td_est = self.td_estimate(state, action)\n\n        # Get TD Target\n        td_tgt = self.td_target(reward, next_state, done)\n\n        # Backpropagate loss through Q_online\n        loss = self.update_Q_online(td_est, td_tgt)\n\n        return (td_est.mean().item(), loss)","66357196":"class MetricLogger:\n    def __init__(self):\n        # history metrics\n        self.ep_rewards = []\n        self.ep_lengths = []\n        self.ep_avg_losses = []\n        self.ep_avg_qs = []\n\n        # moving averages, added for every call to record()\n        self.moving_avg_ep_rewards = []\n        self.moving_avg_ep_lengths = []\n        self.moving_avg_ep_avg_losses = []\n        self.moving_avg_ep_avg_qs = []\n\n        # current episode metric\n        self.init_episode()\n\n        # timing\n        self.record_time = time.time()\n\n    def log_step(self, reward, loss, q):\n        self.curr_ep_reward += reward\n        self.curr_ep_length += 1\n        if loss:\n            self.curr_ep_loss += loss\n            self.curr_ep_q += q\n            self.curr_ep_loss_length += 1\n\n    def log_episode(self):\n        self.ep_rewards.append(self.curr_ep_reward)\n        self.ep_lengths.append(self.curr_ep_length)\n        if self.curr_ep_loss_length == 0:\n            ep_avg_loss = 0\n            ep_avg_q = 0\n        else:\n            ep_avg_loss = np.round(self.curr_ep_loss \/ self.curr_ep_loss_length, 5)\n            ep_avg_q = np.round(self.curr_ep_q \/ self.curr_ep_loss_length, 5)\n        self.ep_avg_losses.append(ep_avg_loss)\n        self.ep_avg_qs.append(ep_avg_q)\n\n        self.init_episode()\n\n    def init_episode(self):\n        self.curr_ep_reward = 0.0\n        self.curr_ep_length = 0\n        self.curr_ep_loss = 0.0\n        self.curr_ep_q = 0.0\n        self.curr_ep_loss_length = 0\n\n    def record(self, episode, epsilon, step):\n        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n        self.moving_avg_ep_rewards.append(mean_ep_reward)\n        self.moving_avg_ep_lengths.append(mean_ep_length)\n        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n\n        last_record_time = self.record_time\n        self.record_time = time.time()\n        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n\n        print(\n            \"Episode:{:4d}  :: Step:{:5d}  :: Epsilon:{:8.3f}  :: Mean_Reward:{:8.3f}  :: \" \\\n            \"Mean_Length:{:8.3f}  :: Mean_Loss:{:4.3f}  :: Mean_Q_Value:{:8.3f}  :: \" \\\n            \"Time_Delta:{:8.3f} \"\n            .format(episode, step, epsilon, mean_ep_reward, mean_ep_length, \n                    mean_ep_loss, mean_ep_q, time_since_last_record)\n        )","ab32be2d":"use_cuda = torch.cuda.is_available()\nprint(f\"Using CUDA: {use_cuda}\")\nprint()\n\nmario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, use_cuda=use_cuda)\n\nlogger = MetricLogger()\n\nepisodes = 301\nfor e in range(episodes):\n\n    state = env.reset()\n\n    # Play the game!\n    while True:\n\n        # Run agent on the state\n        action = mario.act(state)\n\n        # Agent performs action\n        next_state, reward, done, info = env.step(action)\n\n        # Remember\n        mario.cache(state, next_state, action, reward, done)\n\n        # Learn\n        q, loss = mario.learn()\n\n        # Logging\n        logger.log_step(reward, loss, q)\n\n        # Update state\n        state = next_state\n\n        # Check if end of game\n        if done or info[\"flag_get\"]:\n            break\n\n    logger.log_episode()\n\n    if e % 25 == 0:\n        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)","792da983":"!pip --disable-pip-version-check install -q pyvirtualdisplay\n\nfrom pyvirtualdisplay import Display\n\nfrom IPython import display as ipythondisplay\nfrom IPython.display import HTML\n\nfrom gym.wrappers import Monitor\nfrom glob import glob\n\nimport base64\nimport io\n\ndisplay = Display(visible=0, size=(600, 300))\ndisplay.start()","644bca62":"\"\"\"\nUtility functions to enable video recording of gym environment and displaying it\nTo enable video, just do \"env = wrap_env(env)\"\"\n\"\"\"\n\ndef show_video():\n    mp4list = glob('\/kaggle\/working\/video\/*.mp4')\n    if len(mp4list) > 0:\n        mp4 = mp4list[0]\n        video = io.open(mp4, 'r+b').read()\n        encoded = base64.b64encode(video)\n        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                    loop controls style=\"height: 400px;\">\n                    <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n                 <\/video>'''.format(encoded.decode('ascii'))))\n    else: \n        print(\"Could not find video\")\n    \n\ndef wrap_env(env):\n    env = Monitor(env, '\/kaggle\/working\/video', force=True)\n    return env\n\nenv = wrap_env(env)","912d8b7d":"state = env.reset()\n\n# Play the game!\nwhile True:\n\n    # Run agent on the state\n    action = mario.act(state)\n\n    # Agent performs action\n    next_state, reward, done, info = env.step(action)\n\n    # Update state\n    state = next_state\n    \n    # Check if end of game\n    if done or info[\"flag_get\"]:\n        break\n\nenv.close()\nshow_video()","31ae6f00":"# TD Estimate & TD Target","693559aa":"# Logging","f19bbcae":"# Initialize Environment","5be1270d":"<pre>\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2500\u2500\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\n\u2500\u2500\u2500\u2500\u2500\u2500\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2588\u2588\n\u2500\u2500\u2500\u2500\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2588\u2588\n\u2500\u2500\u2500\u2500\u2588\u2588\u2593\u2593\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\n\u2500\u2500\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2591\u2591\u2588\u2588\u2591\u2591\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\n\u2500\u2500\u2588\u2588\u2591\u2591\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2591\u2591\u2588\u2588\u2591\u2591\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\n\u2588\u2588\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2593\u2593\u2588\u2588\n\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2591\u2591\u2591\u2591\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2593\u2593\u2588\u2588\n\u2500\u2500\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\n\u2500\u2500\u2500\u2500\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2588\u2588\n\u2500\u2500\u2500\u2500\u2500\u2500\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\n\u2500\u2500\u2500\u2500\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2588\u2588\n\u2500\u2500\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\n\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\n\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\n\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n\u2500\u2500\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n\u2500\u2500\u2500\u2500\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\n\u2500\u2500\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\n\u2588\u2588\u2588\u2588\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\n\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2588\n\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2500\u2500\u2500\u2500\u2500\u2500\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\n\u2588\u2588\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2588\u2588\u2588\u2588\u2588\u2588 \n\u2500\u2500\u2588\u2588\u2588\u2588\n<\/pre>","8129061a":"# Training","d10f71df":"# Double Deep Q-Networks","f1dd5755":"# Preprocess Environment","cf513d60":"# Agent","6337f23c":"# Play a game"}}