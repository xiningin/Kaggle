{"cell_type":{"8c2c4998":"code","28f0da5e":"code","b47be56f":"code","c392f068":"code","a09db380":"code","9e1a1708":"code","f688615a":"code","22193eb8":"code","66432ed6":"code","79b8a463":"code","754cc0ae":"code","4b3e7cd8":"code","6ec70c27":"code","6da18929":"code","7bfc1d44":"code","47a67994":"code","702eb119":"code","085aa2db":"code","c13961a1":"markdown","1f1b765b":"markdown","5a60eebd":"markdown","544d8e86":"markdown","a6696689":"markdown","f1d9caee":"markdown","87502721":"markdown","e23b410e":"markdown","aeae0be1":"markdown"},"source":{"8c2c4998":"import os, sys, shutil\nimport time\nimport gc\nfrom contextlib import contextmanager\nfrom pathlib import Path\nimport random\nimport numpy as np, pandas as pd\nfrom tqdm import tqdm, tqdm_notebook\n\nfrom sklearn.metrics import f1_score, roc_auc_score, classification_report\nfrom sklearn.preprocessing import LabelBinarizer\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_formats = ['retina']","28f0da5e":"MAX_SEQUENCE_LENGTH = 512    # maximal possible sequence length is 512. Generally, the higher, the better, but more GPU memory consumed\nBATCH_SIZE = 16              # refer to the table here https:\/\/github.com\/google-research\/bert to adjust batch size to seq length\n\nSEED = 1234\nEPOCHS = 20                                   \nPATH_TO_DATA = Path(\"..\/input\/exploring-transfer-learning-for-nlp\/\")\nWORK_DIR = Path(\"..\/working\/\")\n\nLRATE = 2e-5             # hard to tune with BERT, but this shall be fine (improvements: LR schedules, fast.ai wrapper for LR adjustment)\nACCUM_STEPS = 2          # wait for several backward steps, then one optimization step, this allows to use larger batch size\n                         # well explained here https:\/\/medium.com\/huggingface\/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255\nWARMUP = 5e-2            # warmup helps to tackle instability in the initial phase of training with large learning rates. \n                         # During warmup, learning rate is gradually increased from 0 to LRATE.\n                         # WARMUP is a proportion of total weight updates for which warmup is done. By default, it's linear warmup\nUSE_APEX = True         # using APEX shall speedup training (here we use mixed precision training), https:\/\/github.com\/NVIDIA\/apex","b47be56f":"# nice way to report running times\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')","c392f068":"# make results fully reproducible\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","a09db380":"if USE_APEX:\n    with timer('install Nvidia apex'):\n        # Installing Nvidia Apex\n        os.system('git clone https:\/\/github.com\/NVIDIA\/apex; cd apex; pip install -v --no-cache-dir' + \n                  ' --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/')\n        os.system('rm -rf apex\/.git') # too many files, Kaggle fails\n        from apex import amp","9e1a1708":"device = torch.device('cuda')","f688615a":"!wget -q https:\/\/storage.googleapis.com\/bert_models\/2018_11_23\/multi_cased_L-12_H-768_A-12.zip\n!unzip multi_cased_L-12_H-768_A-12.zip\nBERT_MODEL_PATH = Path('multi_cased_L-12_H-768_A-12')","22193eb8":"%%capture  \n# suppresses output\n\n# Add the Bert Pytorch repo to the PATH using files from: https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\npackage_dir_a = \"..\/input\/ppbert\/pytorch-pretrained-bert\/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, BertConfig\n\n# Translate model from tensorflow to pytorch\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    str(BERT_MODEL_PATH \/ 'bert_model.ckpt'),\n    str(BERT_MODEL_PATH \/ 'bert_config.json'),\n    str(WORK_DIR \/ 'pytorch_model.bin')\n)\n\nshutil.copyfile(BERT_MODEL_PATH \/ 'bert_config.json', WORK_DIR \/ 'bert_config.json')\nbert_config = BertConfig(str(BERT_MODEL_PATH \/ 'bert_config.json'))","66432ed6":"# Converting the lines to BERT format\ndef convert_lines(example, max_seq_length, tokenizer):\n    max_seq_length -= 2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a) > max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"]) + [0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(f\"There are {longer} lines longer than {max_seq_length}\")\n    return np.array(all_tokens)","79b8a463":"with timer('Read data and convert to BERT format'):\n    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,\n                                              do_lower_case=False)\n    train_df = pd.read_csv(PATH_TO_DATA \/ \"dutch_book_reviews_train_14k.csv\")\n    \n    val_df = pd.read_csv(PATH_TO_DATA \/ \"dutch_book_reviews_valid_6k.csv\")\n    \n    print('loaded {} train and {} validation records'.format(\n        len(train_df), len(val_df)))\n\n    # Make sure all text values are strings\n    train_df['text'] = train_df['text'].astype(str).fillna(\"DUMMY_VALUE\") \n    val_df['text'] = val_df['text'].astype(str).fillna(\"DUMMY_VALUE\") \n    \n    X_train = convert_lines(train_df[\"text\"], MAX_SEQUENCE_LENGTH, tokenizer)\n    X_val = convert_lines(val_df[\"text\"], MAX_SEQUENCE_LENGTH, tokenizer)\n    \n    y_train = train_df['label'].values.reshape(-1, 1)\n    y_val = val_df['label'].values.reshape(-1, 1)\n    \n    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.long), \n                                               torch.tensor(y_train, dtype=torch.float))\n    val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.long))\n    \n    del train_df, val_df; gc.collect()","754cc0ae":"with timer('Setting up BERT'):\n    output_model_file = \"bert_pytorch.bin\"\n    seed_everything(SEED)\n    model = BertForSequenceClassification.from_pretrained(WORK_DIR,\n                                                          cache_dir=None,\n                                                          num_labels=1)\n    model.zero_grad()\n    model = model.to(device)\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n         'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n         'weight_decay': 0.0}\n        ]\n\n    num_train_optimization_steps = int(EPOCHS * len(train_dataset) \/ BATCH_SIZE \/ ACCUM_STEPS)\n\n    optimizer = BertAdam(optimizer_grouped_parameters,\n                         lr=LRATE, warmup=WARMUP,\n                         t_total=num_train_optimization_steps)\n    if USE_APEX:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n    model = model.train()","4b3e7cd8":"with timer('Training'):\n    loss_history = []\n    tq = tqdm_notebook(range(EPOCHS))\n    for epoch in tq:\n        train_loader = torch.utils.data.DataLoader(train_dataset, \n                                                   batch_size=BATCH_SIZE, \n                                                   shuffle=True)\n        avg_loss = 0.\n        lossf = None\n        tk0 = tqdm_notebook(enumerate(train_loader), total=len(train_loader), leave=False)\n        optimizer.zero_grad()   \n        for i,(x_batch, y_batch) in tk0:\n            y_pred = model(x_batch.to(device), \n                           attention_mask=(x_batch>0).to(device), labels=None)\n            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch.to(device))\n            if USE_APEX:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            if (i+1) % ACCUM_STEPS == 0:                     # Wait for several backward steps\n                optimizer.step()                             # Now we can do an optimizer step\n                optimizer.zero_grad()\n            if lossf:\n                lossf = 0.98 * lossf + 0.02 * loss.item()\n            else:\n                lossf = loss.item()\n            tk0.set_postfix(loss = lossf)\n            avg_loss += loss.item() \/ len(train_loader)\n            \n            loss_history.append(loss.item())\n            \n        tq.set_postfix(avg_loss=avg_loss)\n        \n# torch.save(model.state_dict(), output_model_file)","6ec70c27":"plt.plot(range(len(loss_history)), loss_history);\nplt.ylabel('Loss'); plt.xlabel('Batch number');","6da18929":"!nvidia-smi","7bfc1d44":"with timer('Validation predictions'):\n    # The following 3 lines are not needed but show how to download the model for prediction\n#     model = BertForSequenceClassification(bert_config, num_labels=y_train.shape[1])\n#     model.load_state_dict(torch.load(output_model_file))\n#     model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval();\n    \n    val_pred_probs = np.zeros_like(y_val)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, \n                                             shuffle=False)\n        \n    for i,(x_batch,) in enumerate(tqdm_notebook(val_loader)):\n        pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n        val_pred_probs[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = pred.detach().cpu().numpy()","47a67994":"roc_auc_score(y_val, val_pred_probs)","702eb119":"f1_score(y_val, val_pred_probs > 0.5, average='micro')","085aa2db":"print(classification_report(y_val, val_pred_probs > 0.5))","c13961a1":"#### Training","1f1b765b":"#### Read and convert data \nKudos to [this Kernel](https:\/\/www.kaggle.com\/httpwwwfszyc\/bert-in-keras-taming). ","5a60eebd":"#### Settings","544d8e86":"#### Validation metrics","a6696689":"#### Setting up BERT","f1d9caee":"#### Helper functions","87502721":"Here we walk through PyTorch BERT fine-tuning for classification, mostly based on [this Kernel](https:\/\/www.kaggle.com\/yuval6967\/toxic-bert-plain-vanila). \nSome comments on the used heurisitcs (batch accumulation, warmup) are added. \n\nWe are using BERT-Base, Multilingual Cased from official BERT [repository](https:\/\/github.com\/google-research\/bert\/blob\/master\/multilingual.md). \n\n#### Imports","e23b410e":"#### Test predictions","aeae0be1":"#### Download BERT model and convert it to PyTorch format"}}