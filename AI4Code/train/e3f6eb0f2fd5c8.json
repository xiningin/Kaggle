{"cell_type":{"73cbb339":"code","f052d2f4":"code","36c43ac3":"code","f113b4f0":"code","54ea0562":"code","91163a1a":"code","c2c3b9a3":"code","6ce9997a":"code","44f446ca":"code","0ec813b9":"code","c61bbd23":"code","f00cfc2c":"code","feeda24a":"code","74ca078b":"code","99c30650":"code","585a5735":"code","fd0f976d":"code","af86158f":"code","f0905404":"code","adc9c8f0":"code","8df5577f":"code","0dab35e6":"code","0f7151d5":"code","251d1d7d":"code","5c1b151b":"markdown"},"source":{"73cbb339":"import pandas as pd\nimport pyarrow.parquet as pq\nimport os\nimport numpy as np\nfrom keras.layers import *\nfrom keras.models import Model\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nprint(os.listdir(\"..\/input\"))","f052d2f4":"def matthews_correlation(y_true, y_pred):\n    '''Calculates the Matthews correlation coefficient measure for quality\n    of binary classification problems.\n    '''\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n\n    tp = K.sum(y_pos * y_pred_pos)\n    tn = K.sum(y_neg * y_pred_neg)\n\n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg)\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator \/ (denominator + K.epsilon())","36c43ac3":"# https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-652-lb\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","f113b4f0":"df_train = pd.read_csv('..\/input\/metadata_train.csv')\ndf_train = df_train.set_index(['id_measurement', 'phase'])\ndf_train.head()","54ea0562":"max_num = 127\nmin_num = -128","91163a1a":"def min_max_transf(ts, min_data, max_data, range_needed=(-1,1)):\n    if min_data < 0:\n        ts_std = (ts + abs(min_data)) \/ (max_data + abs(min_data))\n    else:\n        ts_std = (ts - min_data) \/ (max_data - min_data)\n    if range_needed[0] < 0:    \n        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n    else:\n        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]","c2c3b9a3":"def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n    sample_size = 800000\n    bucket_size = int(sample_size \/ n_dim)\n    new_ts = []\n    for i in range(0, sample_size, bucket_size):\n        ts_range = ts_std[i:i + bucket_size]\n        mean = ts_range.mean()\n        std = ts_range.std()\n        std_top = mean + std\n        std_bot = mean - std\n        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100])\n        max_range = percentil_calc[-1] - percentil_calc[0]\n        covar = std \/ mean\n        asymmetry = mean - percentil_calc[4]\n        new_ts.append(np.concatenate([np.asarray([mean, std_top, std_bot, max_range, covar, asymmetry]),percentil_calc]))\n    return np.asarray(new_ts)","6ce9997a":"def prep_data(start, end):\n    #praq_train = pq.read_pandas('..\/input\/train.parquet').to_pandas()\n    praq_train = pq.read_pandas('..\/input\/train.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n    X = []\n    y = []\n    #for id_measurement in tqdm(df_train.index.levels[0].unique()):\n    for id_measurement in df_train.index.levels[0].unique()[int(start\/3):int(end\/3)]:\n        X_signal = []\n        for phase in [0,1,2]:\n            signal_id, target = df_train.loc[id_measurement].loc[phase]\n            if phase == 0:\n                y.append(target)\n            X_signal.append(transform_ts(praq_train[str(signal_id)]))\n        X_signal = np.concatenate(X_signal, axis=1)\n        X.append(X_signal)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    return X, y","44f446ca":"X = []\ny = []\ndef load_all():\n    total_size = len(df_train)\n    for ini, end in [(0, int(total_size\/2)), (int(total_size\/2), total_size)]:\n        X_temp, y_temp = prep_data(ini, end)\n        X.append(X_temp)\n        y.append(y_temp)\nload_all()\nX = np.concatenate(X)\ny = np.concatenate(y)","0ec813b9":"print(X.shape, y.shape)","c61bbd23":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","f00cfc2c":"def model_lstm(input_shape):\n    inp = Input(shape=(input_shape[1], input_shape[2],))\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = Attention(input_shape[1])(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n    \n    return model","feeda24a":"model = model_lstm(X_train.shape)\nprint(model.metrics_names)\nmodel.summary()","74ca078b":"ckp = ModelCheckpoint('weights.h5', save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\nmodel.fit(X_train, y_train, batch_size=100, epochs=100, validation_data=[X_valid, y_valid], callbacks=[ckp])","99c30650":"%%time\n# 25ms in Kernel\nmeta_test = pd.read_csv('..\/input\/metadata_test.csv')","585a5735":"meta_test = meta_test.set_index(['signal_id'])\nmeta_test.head()","fd0f976d":"%%time\n# About 10min in Kernel\nfirst_sig = meta_test.index[0]\nn_parts = 10\nmax_line = len(meta_test)\npart_size = int(max_line \/ n_parts)\nlast_part = max_line % n_parts\nprint(first_sig, n_parts, max_line, part_size, last_part, n_parts * part_size + last_part)\nstart_end = [[x, x+part_size] for x in range(first_sig, max_line + first_sig, part_size)]\nstart_end = start_end[:-1] + [[start_end[-1][0], start_end[-1][0] + last_part]]\nprint(start_end)\nX_test = []\nfor start, end in start_end:\n    subset_test = pq.read_pandas('..\/input\/test.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n    for i in tqdm(subset_test.columns):\n        id_measurement, phase = meta_test.loc[int(i)]\n        subset_test_col = subset_test[i]\n        subset_trans = transform_ts(subset_test_col)\n        X_test.append([i, id_measurement, phase, subset_trans])","af86158f":"X_test_input = np.asarray([np.concatenate([X_test[i][3],X_test[i+1][3], X_test[i+2][3]], axis=1) for i in range(0,len(X_test), 3)])\nX_test_input.shape","f0905404":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nprint(len(submission))\nsubmission.head()","adc9c8f0":"model.load_weights('weights.h5')","8df5577f":"pred = model.predict(X_test_input, batch_size=300)","0dab35e6":"pred_3 = []\nfor pred_scalar in pred:\n    for i in range(3):\n        pred_3.append(int(pred_scalar > 0.4))","0f7151d5":"submission['target'] = pred_3","251d1d7d":"submission.to_csv('submission.csv', index=False)","5c1b151b":"This notebook was made from scratch by me.  \nThe idea is reducing the 800,000 long mesurements to some shorter vector that is more fitted to a LSTM.  \nThis code below transform the 800.000 mesurements of the 3 diferent phases in a unique (80, 39) matrix.  \nThe matthews ocrrelation and attention functions are not mine.  "}}