{"cell_type":{"9bc1ac94":"code","74cdc518":"code","58caefca":"code","603e9460":"code","d486f9a6":"code","14ca8c52":"code","fe1f9573":"code","aaeefdb1":"code","bc9b966b":"code","daf65d3b":"code","53fe0864":"markdown","9e9dde9d":"markdown","1de89689":"markdown","118c31ca":"markdown","5cfd1986":"markdown","6c3aa1bf":"markdown","71152533":"markdown"},"source":{"9bc1ac94":"import numpy as np\nimport pandas as pd\nimport json\nfrom random import choice\nimport tensorflow as tf\nimport sklearn\n\nfrom tensorflow.keras.models import model_from_json\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalMaxPool2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping","74cdc518":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\n# Strangely, the sample submission file has an index column, but the other .csvs do not\npredictions = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv', index_col=0)\ntrain_labels = train.pop('label')\n\nprint(train.values.shape)","58caefca":"img_width = 28\nimg_height = 28\n\n# Index,y,x,channel\ntrain_X = train.values.reshape(-1, img_height, img_width, 1) \/ 255\ntest_X = test.values.reshape(-1, img_height, img_width, 1) \/ 255\ntrain_Y = to_categorical(train_labels, num_classes = 10)\n\n","603e9460":"def build_cnn_model(filters=(30,), kernel_size=(5,), neurons=(30,), augmentation=0.1, whiten=0., dropout=0.33, dilation_rate=1):\n    \"\"\"\n    :param filters: tuple of number filters for each convolutional layer.\n    :param kernel_size: tuple of kernel size for each convolutional layer. (Each kernel will be square)\n    :param neurons: tuple of neuron size ofr each densely-connected layer (except the sigmoid at the top)\n    :param augmentation: float from 0. to 1.0 of image augmentation intensity\n    :param whiten: epsilon parameter for whitening. 0 means no whitening (recommended)\n    :param dropout: float from  0. to 1.0 of dropout fraction. There is a dropout layer before every densely-connected layer\n    :dilation_rate: int. Dilation rate for first convolutional layer\n    \"\"\"\n\n    model = Sequential()\n    \n    # Add convolutional layers\n    for i, f in enumerate(filters):\n        if i==0:\n            dr=dilation_rate\n        else:\n            dr=1\n        kernel = (kernel_size[i],kernel_size[i])\n        model.add(Conv2D(filters=f, kernel_size=kernel, padding = 'Same', \n                     activation ='relu', dilation_rate=dr))\n        if i < (len(filters)-1):\n            model.add(MaxPool2D(pool_size=kernel))\n    \n    model.add(Flatten())\n    model.add(Dropout(dropout))\n    \n    for n in neurons:\n        model.add(Dense(n, activation = \"relu\",  kernel_initializer='glorot_normal'))\n        model.add(Dropout(dropout))\n    \n    model.add(Dense(10, activation = \"softmax\",  kernel_initializer='glorot_normal'))\n    model.compile(optimizer='adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    datagen = ImageDataGenerator(\n            validation_split = 0.2,\n            featurewise_center=True,  # set input mean to 0 over the dataset\n            samplewise_center=False,  # set each sample mean to 0\n            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n            samplewise_std_normalization=False,  # divide each input by its std\n            zca_whitening=bool(whiten),  # apply ZCA whitening\n            zca_epsilon=whiten,   # Whitening \n            rotation_range=augmentation*90,  # randomly rotate images in the range (degrees, 0 to 180)\n            zoom_range=augmentation, # Randomly zoom image \n            width_shift_range=augmentation,  # randomly shift images horizontally (fraction of total width)\n            height_shift_range=augmentation,  # randomly shift images vertically (fraction of total height)\n            shear_range=augmentation, # Randomly shear images (degrees, 0 to 180)\n            horizontal_flip=False,  # randomly flip images\n            vertical_flip=False)  # randomly flip images\n    return model, datagen","d486f9a6":"param_grid = {'filters': [(60,30), (60,60), (30,30), (30, 60)],\n          'neurons': [(80,), (30,), (80,30), (80,80), (30,30)],\n          'dropout': [0.15, 0.25, 0.33, 0.5],\n           'augmentation': [0.1, 0.15, 0.33],\n             'whiten': [0],  # Does not help\n             'kernel_size': [(3,3), (3,6), (6,6), (6,12)],\n             'dilation_rate': [1, 2, 3]}\n\nbest_accuracy = 0\nparam_scan = pd.DataFrame()","14ca8c52":"callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=2, cooldown=2, verbose=1, factor=0.25),\n             tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)]","fe1f9573":"for i in range(20):\n    params = {}\n    for key, value in param_grid.items():\n        params[key] = param_grid[key][np.random.choice(len(value))]\n   \n    model, datagen = build_cnn_model(**params)\n\n    datagen.fit(train_X)\n\n    print(f'Fitting {params}')\n    model_history = model.fit(datagen.flow(train_X, train_Y, batch_size=32, subset='training'), callbacks=callbacks, epochs=30, \n              validation_data =datagen.flow(train_X, train_Y, batch_size=32, subset='validation'))\n    \n    params['val_accuracy'] = max(model_history.history['val_accuracy'])\n    param_scan = param_scan.append(params, ignore_index=True)\n    param_scan.to_csv('\/kaggle\/working\/param_scan.csv')\n    if params['val_accuracy'] > best_accuracy:\n        best_params = params\n        best_model = model\n        best_datagen  = datagen\n        best_accuracy = params['val_accuracy']\n    print(f'Best accuracy: {best_accuracy:0.3f}')\n    print(f'Best params: {best_params}')","aaeefdb1":"predictions['Label'] = np.argmax(best_model.predict(best_datagen.standardize(test_X)), axis=1)","bc9b966b":"predictions.to_csv('submission.csv')","daf65d3b":"best_model.summary()","53fe0864":"Now we train twenty random models and keep the best one.","9e9dde9d":"Here are the callbacks we will use - learning rate reduction will help our model converge a bit better by lowering the learning rate if loss does not improve, and early stopping will cease stop training a model and restore the best weights if the val loss still doesn't improve.","1de89689":"Here we define our model function. It will be a sequential model with a series of convolutional layers, followed by a series of densely-connected layers","118c31ca":"Our task is to recognize handwritten black-and-white numbers. We are going to do a random hyperparameter search to find the best simple neural network for this problem.\n\nFirst, we import the packages we need and the data.","5cfd1986":"Unusually for computer vision problems, the training data is provided in .csv files","6c3aa1bf":"Here is our hyperparameter grid","71152533":"We reshape the train and test data into 28x28 images"}}