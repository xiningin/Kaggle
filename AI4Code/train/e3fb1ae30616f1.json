{"cell_type":{"e78b60c5":"code","0c93b6c1":"code","ace5f1f0":"code","1ef26843":"code","8c668fab":"code","f8ca995a":"code","ab4354c6":"code","2784cb8f":"code","213afb05":"code","c50a24b7":"code","1519d415":"code","fafe5c06":"code","d9d2521e":"code","ad31f70e":"code","1465d09b":"code","6c52ef51":"code","f67e151a":"code","4fc1ba00":"code","e26e04cd":"code","4d9f12c7":"code","a0d786b0":"code","6176c733":"code","c07afcce":"code","555aed7a":"code","f816d954":"markdown","e02bc981":"markdown","bebf6c7e":"markdown","d123702c":"markdown","d2e2d955":"markdown","d2d9564c":"markdown","234e6e9f":"markdown","b06da5e0":"markdown","8afa47c5":"markdown","3267aff1":"markdown","2a01a726":"markdown","8d541c3f":"markdown"},"source":{"e78b60c5":"import matplotlib.pyplot as plt\nimport pandas as p\n\n#Scaling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n#ML Models\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN","0c93b6c1":"y=[4, 13, 25, 37, 49, 61, 73, 85, 97,2, 6, 11, 16, 21, 26, 31, 36, 41, 46, 109, 121, 133, 145, 157, 169, 181, 193, 205, 217, 229, 241, 253, 265, 277, 289, 301, 313, 325, 337, 349, 361, 373,100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 1500.0, 1513.5, 1527.0, 1540.5, 1554.0, 1567.5, 1581.0, 1594.5, 1608.0, 1621.5, 1635.0, 1648.5, 1662.0, 1675.5, 1689.0, 1702.5, 1729.5, 1743.0, 1756.5, 1770.0, 1783.5, 1797.0, 1810.5, 1824.0, 1837.5, 1851.0, 1864.5, 1878.0, 1891.5, 1905.0, 1918.5, 1932.0, 1945.5, 1959.0, 1972.5, 1986.0, 1999.5, 2013.0, 2026.5, 2040.0, 2053.5, 2067.0, 2080.5, 2094.0, 2107.5, 2121.0, 2134.5, 2148.0, 2161.5, 2175.0, 2188.5, 2202.0, 2215.5, 2229.0, 2242.5,195, 385, 397, 409, 421, 433, 445, 457, 469, 481, 493, 505, 517, 529, 541, 553, 565, 577, 589, 601, 613, 625, 637, 649, 661, 673, 685, 697, 709, 721, 733, 745, 757, 769, 781, 793, 805, 817, 829, 841, 853, 865, 877, 889, 901, 913, 925, 937, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95,949, 961, 973, 985,-1200.0, -1170.0, -1140.0, -1110.0, -1080.0, -1050.0, -1020.0, -990.0, -960.0, -930.0, -900.0, -870.0, -840.0, -810.0, -780.0, -750.0, -720.0, -690.0, -660.0, -630.0, -600.0, -570.0, -540.0, -510.0, -480.0, -450.0, -420.0, -390.0, -360.0, -330.0, -300.0, -270.0, -240.0, -210.0, -180.0, -150.0, -120.0, -90.0, -60.0, -30.0, 0.0, 30.0, 997]\nx=[1652, 1640, 1628, 1616, 1604, 1592, 1580, 1568, 1556, 1544, 1532, 1520, 1508, 1496, 1484, 1472, 1460, 1448, 1436, 1424, 1412, 1400, 1388, 1376, 1364, 1352, 1340, 1328, 1316, 1304, 1292, 1280, 1268, 1256, 1244, 1232, 1220, 1208, 1196, 1184, 1172, 1160, 1148, 1136, 1124, 1112, 1100, 1088, 1076, 1064, 1052, 1040, 1028, 1016, 1004, 992, 980, 968, 956, 944, 932, 920, 908, 896, 884, 872, 860, 848, 836, 824, 812, 800, 788, 776, 764, 752, 740, 728, 716, 704, 692, 680, 668, 656, 644, 632, 620, 608, 596, 584, 572, 560, 548, 536, 524, 512, 500, 488, 476, 464, 452, 440, 428, 416, 404, 392, 380, 368, 356, 344, 332, 320, 308, 296, 284, 272, 260, 248, 236, 224, 212, 200, 188, 176, 164, 152, 140, 128, 116, 104, 92, 80, 68, 56, 44, 32, 20, 8, -4, -16, -28, -40, -52, -64, -76, -88, -100, -112, -124, -136, -148, -160, -172, -184, -196, -208, -220, -232, -244, -256, -268, -280, -292, -304, -316, -328, -340, -352, -364, -376, -388, -400, -412, -424, -436, -448, -460, -472, -484, -496, -508, -520, -532, -544, -556, -568, -580, -592, -604, -616, -628, -640, -652, -664, -676, -688, -700, -712, -724, -736, -748, -760, -772, -784, -796, -808, -820, -832, -844, -856, -868, -880, -892, -904, -916, -928, -940, -952, -964, -976, -988]\n\nlen(y),len(x)","ace5f1f0":"data = p.DataFrame({\"x\":x,\"y\":y})\ndata.head()","1ef26843":"plt.figure(figsize = (15, 7))\nplt.scatter(data[\"x\"], data[\"y\"], cmap=\"winter\", s=200, alpha=0.8)","8c668fab":"sc_X = StandardScaler()\nX = sc_X.fit_transform(data)","f8ca995a":"inerties=[]\nfor k in range(2, 6):\n    kmeans=KMeans(n_clusters=k)\n    kmeans.fit(X)\n    inerties.append(kmeans.inertia_)","ab4354c6":"# on repr\u00e9sente le graphique\nfig=plt.figure(figsize=(10, 5))\nplt.plot(range(2, 6), inerties)\nplt.xlabel(\"Nombre de clusters\")\nplt.ylabel(\"Inertie\")\nplt.title('Inertie vs nombre de classes')","2784cb8f":"kmeans_4_cluster = KMeans(n_clusters=4) ","213afb05":"kmeans_4_cluster.fit(X)","c50a24b7":"kmeans_ped=kmeans_4_cluster.predict(X)","1519d415":"kmeans_4_cluster_center = kmeans_4_cluster.cluster_centers_","fafe5c06":"kmeans_ped","d9d2521e":"plt.figure(figsize=(15,10))\nplt.scatter(X[:,0], X[:,1],c=kmeans_ped, cmap=\"plasma\", s=200, alpha=0.8)\nplt.scatter(kmeans_4_cluster_center[:,0], kmeans_4_cluster_center[:,1],c=\"black\", s=300, alpha=0.8)","ad31f70e":"kmeans_5_cluster = KMeans(n_clusters=5)","1465d09b":"kmeans_5_cluster.fit(X)","6c52ef51":"kmeans_ped = kmeans_5_cluster.predict(X)","f67e151a":"kmeans_5_cluster_center = kmeans_5_cluster.cluster_centers_","4fc1ba00":"plt.figure(figsize=(15,10))\nplt.scatter(X[:,0], X[:,1],c=kmeans_ped, cmap=\"plasma\", s=200, alpha=0.8)\nplt.scatter(kmeans_5_cluster_center[:,0], kmeans_5_cluster_center[:,1], c=\"black\", s=300, alpha=0.8)","e26e04cd":"from sklearn import metrics\neps_range = [0.1,0.2,0.3,0.4,0.5,0.6]\nfor eps in eps_range:\n    # pour chaque k, on cr\u00e9e un mod\u00e8le et on l\u2019ajuste\n    dbscan=DBSCAN(eps=eps)\n    dbscan.fit(X)\n    clusters = dbscan.labels_\n    print('Silhouette index for {0} epsilon is {1:3f}'.format(eps,metrics.silhouette_score(X,  clusters,metric='euclidean')))","4d9f12c7":"dbscan = DBSCAN(eps=0.3)","a0d786b0":"dbscan.fit(X)","6176c733":"clusters = dbscan.labels_","c07afcce":"clusters","555aed7a":"plt.figure(figsize = (15,10))\nplt.scatter(X[:,0], X[:,1], c=clusters, cmap=\"plasma\", s=200, alpha=0.8)","f816d954":"**kmeans sensitive  to outliers** <br>\n* It considers the (-1, 0.5) sample as a member of a cluster, even if it is a outlier.<br>\n* The Number of cluster is actually not that good, we'll try k=5.","e02bc981":"# DBSCAN\n\nThe full name of the DBSCAN algorithm is Density-based Spatial Clustering of Applications with Noise. From the name, it is clear that the algorithm uses density to cluster the data points and it has something to do with the noise\n\n**What is density?**\n\nWell from Physics we know that density is just the amount of matter present in a unit volume. We can easily extend this idea of volume into higher dimensions or even in a lower dimension.\n\nFor example, we have this region.\n\nWe have some data points in this region. And we have another region of the same area we have got these many data points here.\n\n![](https:\/\/editor.analyticsvidhya.com\/uploads\/71980db11.png)\n\nSo, from the idea of density, the density of the first region is greater than the second region. Because, there are more data points, more matter in the first region. DBSCAN uses this concept of density to cluster the dataset. Now to understand the DBSCAN algorithm clearly, we need to know some important parameters.\n\n\n**Important parameters of the DBSCAN algorithm**\n\n* *Epsilon*\n\n> It is a measure of the neighborhood.\n\n* *Neighbourhood*\n\n> Suppose, this is the point we are considering right now, and let me draw a circle around this point making this as a center and add a distance Epsilon.\n> \n> So, we are gonna say this circle as this point\u2019s neighborhood. So, epsilon is just a number that represents the radius of the circle around a particular point that we are going to consider the neighborhood of that point.\n\n* *min_sample*\n\n> This is a threshold on the least number of points that we want to see in a point\u2019s neighborhood. Suppose we are\n> taking z = 3.\n\n![](https:\/\/editor.analyticsvidhya.com\/uploads\/81523db12.jpg)\n\n> If we have 4 points in our neighborhood, this will also satisfy our epsilon eps = 3. Because this epsilon represents the minimum number of samples in a neighborhood.\n\n\n**DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means**\n\n\nFor more Informations: [analyticsvidhya](http:\/\/https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/understand-the-dbscan-clustering-algorithm\/)","bebf6c7e":"Let's chose 0.3 for epsilon !!","d123702c":"**We will use 4 clusters**","d2e2d955":"**Chossing the best number of n_clusters**","d2d9564c":"![](https:\/\/community.alteryx.com\/t5\/image\/serverpage\/image-id\/71501i2E7B9711E7608452?v=v2)","234e6e9f":"* As you can see the clusters in DBSACN model is perfect in this kind of cases.\n* It's not sensitive to outlires it can detect it them and give them -1 as you can see in **clusters** variable, and in the plots it represents with a blue point (-1, 0.5)","b06da5e0":"* We will try to find the best epsilon for ower DBSCAN model","8afa47c5":"![](https:\/\/miro.medium.com\/max\/1200\/1*KqWII7sFp1JL0EXwJGpqFw.png)","3267aff1":"**For k = 5 clusters are not good at all, and it is actually normal for k_means in this cases<br>**","2a01a726":"We will first scale the data using StandardScaler","8d541c3f":"# K-means\n\nA K-means clustering algorithm tries to group similar items in the form of clusters. The number of groups is represented by K.\n\nk-means clustering tries to group similar kinds of items in form of clusters. It finds the similarity between the items and groups them into the clusters. K-means clustering algorithm works in three steps. \n\n\n1. Select the k values.\n1. Initialize the centroids.\n2. Select the group and find the average.\n\n![](https:\/\/i.stack.imgur.com\/ibYKU.png)"}}