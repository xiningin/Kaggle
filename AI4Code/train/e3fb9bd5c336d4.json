{"cell_type":{"b5a79d6e":"code","0e418eb7":"code","1680f0c4":"code","a5438fbd":"code","cc4917e9":"code","8f9ca1fd":"code","42f02e6c":"code","164901bf":"code","af555ae3":"code","719707f5":"code","680dd3c4":"code","1fdfa17f":"code","8dd31dd8":"code","4b2a4d78":"code","49d05a41":"code","0299c699":"code","dc7b4e82":"code","ef6a2a71":"code","c04f8ea0":"markdown","16be4720":"markdown","ef89ddfe":"markdown","9b45bcd5":"markdown","f2e3dcd2":"markdown","1e9e4fae":"markdown","7808b8ca":"markdown"},"source":{"b5a79d6e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","0e418eb7":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = pd.concat([train.drop('Survived', axis=1), test])\n\nX_train = train.drop(['Survived'], axis=1)\ny_train = train['Survived'].copy()","1680f0c4":"# Custom transformer for creating new feature `CabinMissing`\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_cabin_missing = True): # hyperparameter for tuning later\n        self.add_cabin_missing = add_cabin_missing\n    def fit(self, X, y=None):\n        return self # nothing else to do\n    def transform(self, X, y=None):\n        if self.add_cabin_missing:\n            X = X.to_numpy()\n            cabin_array = X[:, 0]\n            CabinMissing = pd.isnull(cabin_array)\n            return np.c_[CabinMissing]\n        else:\n            return np.c_[X]","a5438fbd":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')), # impute missing values with median\n    ('minmax_scaler', MinMaxScaler()),             # scale features\n])","cc4917e9":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')), # impute missing values with mode\n    ('cat_encoder', OneHotEncoder()),                     # convert text to numbers\n])","8f9ca1fd":"from sklearn.compose import ColumnTransformer\n\nnum_attribs_all = combine.select_dtypes(['float64', 'int64']).columns\nnum_attribs = num_attribs_all.drop(['PassengerId'])\n\ncat_attribs_all = combine.select_dtypes('object').columns\ncat_attribs = cat_attribs_all.drop(['Ticket', 'Cabin', 'Name'])\ncabin = ['Cabin']\n\nfull_pipeline = ColumnTransformer([\n    # ('attribs_adder', CombinedAttributesAdder(add_cabin_missing=True), cabin), # create new feature `CabinMissing`\n    ('num', num_pipeline, num_attribs),\n    ('cat', cat_pipeline, cat_attribs),\n    # ('drop', \"drop\", cabin),\n])","42f02e6c":"X_train_prepared = full_pipeline.fit_transform(X_train)","164901bf":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb_clf = GradientBoostingClassifier()\ngb_clf.fit(X_train_prepared, y_train);","af555ae3":"gb_clf.score(X_train_prepared, y_train)","719707f5":"from sklearn.model_selection import cross_val_score\n\naccuracy_cv = cross_val_score(gb_clf, X_train_prepared, y_train, cv=5, scoring='accuracy')\nnp.mean(accuracy_cv)","680dd3c4":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30, 100, 150, 200], 'max_depth': [3, 10, 30], 'max_features': [2, 4, 6, 8, 10]},\n]\n\ngrid_search = GridSearchCV(gb_clf, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n\ngrid_search.fit(X_train_prepared, y_train)","1fdfa17f":"grid_search.best_params_","8dd31dd8":"grid_search.best_estimator_","4b2a4d78":"final_model = grid_search.best_estimator_\n\nX_test = test.copy()\nX_test_prepared = full_pipeline.transform(X_test)","49d05a41":"# final_predictions = final_model.predict(X_test_prepared) # fine-tuned model\nfinal_predictions = gb_clf.predict(X_test_prepared)        # model with default hyperparameter values","0299c699":"submit = test.copy()\nsubmit['Survived'] = final_predictions\nsubmit = submit[['PassengerId', 'Survived']]\nsubmit.shape","dc7b4e82":"submit.head()","ef6a2a71":"submit.to_csv(\"..\/working\/submit.csv\", index=False)","c04f8ea0":"# A work-in-progress\n\nWhat is the best way to fine-tune my model while minimising the amount of manual work required for experimenting with features and hyperparameters? There are so many potential feature combinations and including them all during training might induce noise and lead to overfitting, resulting in poorer model accuracy. Strong correlations between two features would suggest only one is necessary for our model.\n\n**Problem: How do I find out the best combination of features and hyperparameters for predicting passenger survival?**\n\nI know I can experiment with many different hyperparameter values easily using something like grid search. But what about different combinations of features?\n\nI have a list of promising transformations at the bottom of this notebook [here](https:\/\/www.kaggle.com\/benherbertson\/titanic-eda).\n\nBased on [my previous shortlist of models](https:\/\/www.kaggle.com\/benherbertson\/titanic-model-selection), I have decided to use the gradient boosting classifier. I will try to automate my transformation steps using `Pipeline` from scikit-learn as much as possible.\n\nData preparation steps can be treated as hyperparameters. Below, I have created a new feature called `CabinMissing` which indicates whether or not a passenger has a cabin assigned. Using a grid search, I want to find out whether or not to use this derived feature. I can then hopefully use this pipeline to assess any number of derived features.\n\nAt the moment, I'm not sure how to implement this. Apparently the hyperparameter should be specified in the grid search `param_grid`. Maybe I need to retrieve the feature importances and use them some how?","16be4720":"## Summary\n\n* The model with the default hyperparameter values has an accuracy of 0.80143 on the test set.\n* Fine-tuning the model using `GridSearchCV` results in a lower accuracy: 0.77272.\n* Including `CabinMissing` in the model makes it even worse: 0.76076.","ef89ddfe":"## Fine-tune the model\n\nIs `CabinMissing` useful for predicting passenger survival? I should be able to check this using the `add_cabin_missing` hyperparameter from my `CombinedAttributesAdder` transformer. However, I'm not sure how to implement this yet.\n\nLet's find the best combination of hyperparameter values.","9b45bcd5":"## Evaluate the final model on the test set","f2e3dcd2":"## Prepare the data","1e9e4fae":"## Train the model","7808b8ca":"## Get the data"}}