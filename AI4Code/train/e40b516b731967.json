{"cell_type":{"f59720f9":"code","b2d8ae02":"code","5ad02471":"code","73777558":"code","315fe7a8":"code","81bdf806":"code","9bf65720":"code","a692d5ec":"code","05779736":"code","4df7380f":"code","64614f15":"code","e2dcfc26":"code","0772b10a":"code","f9aae51d":"code","a2d55c15":"code","0773a93f":"code","cc319979":"code","e590f27c":"code","b5193937":"code","1a9b2243":"code","d648ca63":"code","6fe29245":"markdown","5ece79f3":"markdown","70e31717":"markdown","99f05d4d":"markdown","bd74a7b9":"markdown","da379f7a":"markdown","83176300":"markdown","fd7d6505":"markdown","a87fd239":"markdown","b207fd27":"markdown","92f54683":"markdown","41d87678":"markdown","ee616ab5":"markdown","461bf50f":"markdown","65bc9fa6":"markdown","1bda9350":"markdown","95f1e616":"markdown"},"source":{"f59720f9":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\n\nwarnings.filterwarnings(\"ignore\")\nNUM_WORKERS = 4","b2d8ae02":"DATA_PATH = \"\/kaggle\/input\/ventilator-pressure-prediction\/\"\n\nsub = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ndf_train = pd.read_csv(DATA_PATH + 'train.csv')\ndf_test = pd.read_csv(DATA_PATH + 'test.csv')\n\n\ndf = df_train[df_train['breath_id'] < 5].reset_index(drop=True)","5ad02471":"df.head()","73777558":"def plot_sample(sample_id, df):\n    df_breath = df[df['breath_id'] == sample_id]\n    r, c  = df_breath[['R', 'C']].values[0]\n\n    cols = ['u_in', 'u_out', 'pressure'] if 'pressure' in df.columns else ['u_in', 'u_out']\n    \n    plt.figure(figsize=(12, 4))\n    for col in ['u_in', 'u_out', 'pressure']:\n        plt.plot(df_breath['time_step'], df_breath[col], label=col)\n        \n    plt.legend()\n    plt.title(f'Sample {sample_id} - R={r}, C={c}')","315fe7a8":"for i in df['breath_id'].unique():\n    plot_sample(i, df_train)","81bdf806":"import torch\nfrom torch.utils.data import Dataset\n\nclass VentilatorDataset(Dataset):\n    def __init__(self, df):\n        if \"pressure\" not in df.columns:\n            df['pressure'] = 0\n\n        self.df = df.groupby('breath_id').agg(list).reset_index()\n        \n        self.prepare_data()\n                \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def prepare_data(self):\n        self.pressures = np.array(self.df['pressure'].values.tolist())\n        \n        rs = np.array(self.df['R'].values.tolist())\n        cs = np.array(self.df['C'].values.tolist())\n        u_ins = np.array(self.df['u_in'].values.tolist())\n        \n        self.u_outs = np.array(self.df['u_out'].values.tolist())\n        \n        self.inputs = np.concatenate([\n            rs[:, None], \n            cs[:, None], \n            u_ins[:, None], \n            np.cumsum(u_ins, 1)[:, None],\n            self.u_outs[:, None]\n        ], 1).transpose(0, 2, 1)\n\n    def __getitem__(self, idx):\n        data = {\n            \"input\": torch.tensor(self.inputs[idx], dtype=torch.float),\n            \"u_out\": torch.tensor(self.u_outs[idx], dtype=torch.float),\n            \"p\": torch.tensor(self.pressures[idx], dtype=torch.float),\n        }\n        \n        return data","9bf65720":"dataset = VentilatorDataset(df)\ndataset[0]","a692d5ec":"import torch\nimport torch.nn as nn\n\n\nclass RNNModel(nn.Module):\n    def __init__(\n        self,\n        input_dim=4,\n        lstm_dim=256,\n        dense_dim=256,\n        logit_dim=256,\n        num_classes=1,\n    ):\n        super().__init__()\n\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, dense_dim \/\/ 2),\n            nn.ReLU(),\n            nn.Linear(dense_dim \/\/ 2, dense_dim),\n            nn.ReLU(),\n        )\n\n        self.lstm = nn.LSTM(dense_dim, lstm_dim, batch_first=True, bidirectional=True)\n\n        self.logits = nn.Sequential(\n            nn.Linear(lstm_dim * 2, logit_dim),\n            nn.ReLU(),\n            nn.Linear(logit_dim, num_classes),\n        )\n\n    def forward(self, x):\n        features = self.mlp(x)\n        features, _ = self.lstm(features)\n        pred = self.logits(features)\n        return pred","05779736":"import os\nimport torch\nimport random\nimport numpy as np\n\n\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n\n    Args:\n        seed (int): Number of the seed.\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    \ndef count_parameters(model, all=False):\n    \"\"\"\n    Counts the parameters of a model.\n\n    Args:\n        model (torch model): Model to count the parameters of.\n        all (bool, optional):  Whether to count not trainable parameters. Defaults to False.\n\n    Returns:\n        int: Number of parameters.\n    \"\"\"\n    if all:\n        return sum(p.numel() for p in model.parameters())\n    else:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    \ndef worker_init_fn(worker_id):\n    \"\"\"\n    Handles PyTorch x Numpy seeding issues.\n\n    Args:\n        worker_id (int): Id of the worker.\n    \"\"\"\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n    \n\ndef save_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    Saves the weights of a PyTorch model.\n\n    Args:\n        model (torch model): Model to save the weights of.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to save to. Defaults to \"\".\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Saving weights to {os.path.join(cp_folder, filename)}\\n\")\n    torch.save(model.state_dict(), os.path.join(cp_folder, filename))","4df7380f":"def compute_metric(df, preds):\n    \"\"\"\n    Metric for the problem, as I understood it.\n    \"\"\"\n    \n    y = np.array(df['pressure'].values.tolist())\n    w = 1 - np.array(df['u_out'].values.tolist())\n    \n    assert y.shape == preds.shape and w.shape == y.shape, (y.shape, preds.shape, w.shape)\n    \n    mae = w * np.abs(y - preds)\n    mae = mae.sum() \/ w.sum()\n    \n    return mae\n\n\nclass VentilatorLoss(nn.Module):\n    \"\"\"\n    Directly optimizes the competition metric\n    \"\"\"\n    def __call__(self, preds, y, u_out):\n        w = 1 - u_out\n        mae = w * (y - preds).abs()\n        mae = mae.sum(-1) \/ w.sum(-1)\n\n        return mae","64614f15":"import gc\nimport time\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\n\n\ndef fit(\n    model,\n    train_dataset,\n    val_dataset,\n    loss_name=\"L1Loss\",\n    optimizer=\"Adam\",\n    epochs=50,\n    batch_size=32,\n    val_bs=32,\n    warmup_prop=0.1,\n    lr=1e-3,\n    num_classes=1,\n    verbose=1,\n    first_epoch_eval=0,\n    device=\"cuda\"\n):\n    avg_val_loss = 0.\n\n    # Optimizer\n    optimizer = getattr(torch.optim, optimizer)(model.parameters(), lr=lr)\n\n    # Data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=val_bs,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    # Loss\n#     loss_fct = getattr(torch.nn, loss_name)(reduction=\"none\")\n    loss_fct = VentilatorLoss()\n\n    # Scheduler\n    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n    num_training_steps = int(epochs * len(train_loader))\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps, num_training_steps\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        model.zero_grad()\n        start_time = time.time()\n\n        avg_loss = 0\n        for data in train_loader:\n            pred = model(data['input'].to(device)).squeeze(-1)\n\n            loss = loss_fct(\n                pred,\n                data['p'].to(device),\n                data['u_out'].to(device),\n            ).mean()\n            loss.backward()\n            avg_loss += loss.item() \/ len(train_loader)\n\n            optimizer.step()\n            scheduler.step()\n\n            for param in model.parameters():\n                param.grad = None\n\n        model.eval()\n        mae, avg_val_loss = 0, 0\n        preds = []\n\n        with torch.no_grad():\n            for data in val_loader:\n                pred = model(data['input'].to(device)).squeeze(-1)\n\n                loss = loss_fct(\n                    pred.detach(), \n                    data['p'].to(device),\n                    data['u_out'].to(device),\n                ).mean()\n                avg_val_loss += loss.item() \/ len(val_loader)\n\n                preds.append(pred.detach().cpu().numpy())\n        \n        preds = np.concatenate(preds, 0)\n        mae = compute_metric(val_dataset.df, preds)\n\n        elapsed_time = time.time() - start_time\n        if (epoch + 1) % verbose == 0:\n            elapsed_time = elapsed_time * verbose\n            lr = scheduler.get_last_lr()[0]\n            print(\n                f\"Epoch {epoch + 1:02d}\/{epochs:02d} \\t lr={lr:.1e}\\t t={elapsed_time:.0f}s \\t\"\n                f\"loss={avg_loss:.3f}\",\n                end=\"\\t\",\n            )\n\n            if (epoch + 1 >= first_epoch_eval) or (epoch + 1 == epochs):\n                print(f\"val_loss={avg_val_loss:.3f}\\tmae={mae:.3f}\")\n            else:\n                print(\"\")\n\n    del (val_loader, train_loader, loss, data, pred)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return preds\n","e2dcfc26":"def predict(\n    model,\n    dataset,\n    batch_size=64,\n    device=\"cuda\"\n):\n    \"\"\"\n    Usual torch predict function. Supports sigmoid and softmax activations.\n    Args:\n        model (torch model): Model to predict with.\n        dataset (PathologyDataset): Dataset to predict on.\n        batch_size (int, optional): Batch size. Defaults to 64.\n        device (str, optional): Device for torch. Defaults to \"cuda\".\n\n    Returns:\n        numpy array [len(dataset) x num_classes]: Predictions.\n    \"\"\"\n    model.eval()\n\n    loader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n    )\n    \n    preds = []\n    with torch.no_grad():\n        for data in loader:\n            pred = model(data['input'].to(device)).squeeze(-1)\n            preds.append(pred.detach().cpu().numpy())\n\n    preds = np.concatenate(preds, 0)\n    return preds","0772b10a":"def train(config, df_train, df_val, df_test, fold):\n    \"\"\"\n    Trains and validate a model.\n\n    Args:\n        config (Config): Parameters.\n        df_train (pandas dataframe): Training metadata.\n        df_val (pandas dataframe): Validation metadata.\n        df_test (pandas dataframe): Test metadata.\n        fold (int): Selected fold.\n\n    Returns:\n        np array: Study validation predictions.\n    \"\"\"\n\n    seed_everything(config.seed)\n\n    model = RNNModel(\n        input_dim=config.input_dim,\n        lstm_dim=config.lstm_dim,\n        dense_dim=config.dense_dim,\n        logit_dim=config.logit_dim,\n        num_classes=config.num_classes,\n    ).to(config.device)\n    model.zero_grad()\n\n    train_dataset = VentilatorDataset(df_train)\n    val_dataset = VentilatorDataset(df_val)\n    test_dataset = VentilatorDataset(df_test)\n\n    n_parameters = count_parameters(model)\n\n    print(f\"    -> {len(train_dataset)} training breathes\")\n    print(f\"    -> {len(val_dataset)} validation breathes\")\n    print(f\"    -> {n_parameters} trainable parameters\\n\")\n\n    pred_val = fit(\n        model,\n        train_dataset,\n        val_dataset,\n        loss_name=config.loss,\n        optimizer=config.optimizer,\n        epochs=config.epochs,\n        batch_size=config.batch_size,\n        val_bs=config.val_bs,\n        lr=config.lr,\n        warmup_prop=config.warmup_prop,\n        verbose=config.verbose,\n        first_epoch_eval=config.first_epoch_eval,\n        device=config.device,\n    )\n    \n    pred_test = predict(\n        model, \n        test_dataset, \n        batch_size=config.val_bs, \n        device=config.device\n    )\n\n    if config.save_weights:\n        save_model_weights(\n            model,\n            f\"{config.selected_model}_{fold}.pt\",\n            cp_folder=\"\",\n        )\n\n    del (model, train_dataset, val_dataset, test_dataset)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return pred_val, pred_test","f9aae51d":"from sklearn.model_selection import GroupKFold\n\ndef k_fold(config, df, df_test):\n    \"\"\"\n    Performs a patient grouped k-fold cross validation.\n    \"\"\"\n\n    pred_oof = np.zeros(len(df))\n    preds_test = []\n    \n    gkf = GroupKFold(n_splits=config.k)\n    splits = list(gkf.split(X=df, y=df, groups=df[\"breath_id\"]))\n\n    for i, (train_idx, val_idx) in enumerate(splits):\n        if i in config.selected_folds:\n            print(f\"\\n-------------   Fold {i + 1} \/ {config.k}  -------------\\n\")\n\n            df_train = df.iloc[train_idx].copy().reset_index(drop=True)\n            df_val = df.iloc[val_idx].copy().reset_index(drop=True)\n\n            pred_val, pred_test = train(config, df_train, df_val, df_test, i)\n            \n            pred_oof[val_idx] = pred_val.flatten()\n            preds_test.append(pred_test.flatten())\n\n    print(f'\\n -> CV MAE : {compute_metric(df, pred_oof) :.3f}')\n\n    return pred_oof, np.mean(preds_test, 0)","a2d55c15":"class Config:\n    \"\"\"\n    Parameters used for training\n    \"\"\"\n    # General\n    seed = 42\n    verbose = 1\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    save_weights = True\n\n    # k-fold\n    k = 5\n    selected_folds = [0, 1, 2, 3, 4]\n    \n    # Model\n    selected_model = 'rnn'\n    input_dim = 5\n\n    dense_dim = 512\n    lstm_dim = 512\n    logit_dim = 512\n    num_classes = 1\n\n    # Training\n    loss = \"L1Loss\"  # not used\n    optimizer = \"Adam\"\n    batch_size = 128\n    epochs = 200\n\n    lr = 1e-3\n    warmup_prop = 0\n\n    val_bs = 256\n    first_epoch_eval = 0","0773a93f":"pred_oof, pred_test = k_fold(\n    Config, \n    df_train,\n    df_test,\n)","cc319979":"def plot_prediction(sample_id, df):\n    df_breath = df[df['breath_id'] == sample_id]\n\n    cols = ['u_in', 'u_out', 'pressure'] if 'pressure' in df.columns else ['u_in', 'u_out']\n    \n    plt.figure(figsize=(12, 4))\n    for col in ['pred', 'pressure', 'u_out']:\n        plt.plot(df_breath['time_step'], df_breath[col], label=col)\n        \n    metric = compute_metric(df_breath, df_breath['pred'])\n        \n    plt.legend()\n    plt.title(f'Sample {sample_id} - MAE={metric:.3f}')","e590f27c":"df_train[\"pred\"] = pred_oof","b5193937":"for i in df_train['breath_id'].unique()[:5]:\n    plot_prediction(i, df_train)","1a9b2243":"df_test['pred'] = pred_test\n\nfor i in df_test['breath_id'].unique()[:5]:\n    plot_prediction(i, df_test)","d648ca63":"sub['pressure'] = pred_test\nsub.to_csv('submission.csv', index=False)","6fe29245":"### Load","5ece79f3":"### Predictions","70e31717":"## Training","99f05d4d":"### Dataset","bd74a7b9":"## Main","da379f7a":"## Sub","83176300":"### Metric & Loss\n> The competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored.","fd7d6505":"## Data","a87fd239":"### Viz","b207fd27":"## Train","92f54683":"### Utils","41d87678":"### Fit","ee616ab5":"### $k$-fold","461bf50f":"### Predict","65bc9fa6":"**Thanks for reading !**","1bda9350":"## Model\n- 2 Layer MLP\n- Bidirectional LSTM\n- Prediction dense layer","95f1e616":"# Deep Learning Starter : Simple LSTM\n\nThis notebook leverages the time series structure of the data.\n\nI expect sequential Deep Learning models to dominate in this competition, so here's a simple LSTM architecture.\n\nParameters were not really tweaked so the baseline is easily improvable.\n\nCode is taken from previous work, some functions are documented but the doc may be outdated.\n\n\n**Don't fork without upvoting ^^**"}}