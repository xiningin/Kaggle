{"cell_type":{"ace47f34":"code","df6f7798":"code","2bacb127":"code","29167aa5":"code","29d86e4a":"code","e85ae6e5":"code","695bb083":"code","9d0132fb":"code","b2fb1a6a":"code","187572dd":"code","3aa2d8dc":"code","9264e448":"markdown"},"source":{"ace47f34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df6f7798":"# !cp -r \/kaggle\/input\/pylibtiff\/pylibtiff-master\/* .\/\n# import libtiff","2bacb127":"# \/kaggle\/input\/eye-in\/eye-in-the-sky-master\/","29167aa5":"!cp -r \/kaggle\/input\/eye-in\/eye-in-the-sky-master\/* .\/\n# import libtiff","29d86e4a":"%cd eye-in-the-sky\n%mkdir train_predictions\n%mkdir test_outputs","e85ae6e5":"!pip install libtiff\n!pip install matplotlib==3.1.0\n\n!pip install glob2\n\n!pip install numpy==1.16.4\n\n!pip install sympy==1.4\n\n!pip install scipy==1.2.0\n\n!pip install glob3\n\n!pip install pandas==0.24.2\n\n!pip install Pillow==6.0.0\n!pip install opencv-python==4.1.0.25\n\n!pip install scikit-learn\n\n!pip install scikit-image==0.15.0\n\n!pip install tensorflow-gpu==1.4.0\n!pip install Keras==2.1.3\n","695bb083":"# !python3 main_unet.py","9d0132fb":"import PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n#from libtiff import TIFF\n#from libtiff import TIFFfile, TIFFimage\nfrom scipy.misc import imresize\nimport numpy as np\nimport math\nimport glob\nimport cv2\nimport os\nimport skimage.io as io\nimport skimage.transform as trans\nfrom tensorflow import keras\n\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import backend as keras\n#%matplotlib inline\nimport tifffile\n\ndef iou(y_true, y_pred, smooth = 100):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n    #sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n    iou_acc = (intersection + smooth) \/ (union + smooth)\n    return iou_acc\n\n\ndef as_keras_metric(method):\n    import functools\n    from keras import backend as K\n    import tensorflow as tf\n    @functools.wraps(method)\n    def wrapper(self, args, **kwargs):\n        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n        value, update_op = method(self, args, **kwargs)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([update_op]):\n            value = tf.identity(value)\n        return value\n    return wrapper\n\ndef tf_mean_iou(y_true, y_pred, num_classes=8):\n    return tf.metrics.mean_iou(y_true, y_pred, num_classes)\n\n\nmean_iou = as_keras_metric(tf_mean_iou)\n\n\n# To read the images in numerical order\nimport re\nnumbers = re.compile(r'(\\d+)')\ndef numericalSort(value):\n    parts = numbers.split(value)\n    parts[1::2] = map(int, parts[1::2])\n    return parts\n\n# List of file names of actual Satellite images for traininig \nfilelist_trainx = sorted(glob.glob('Inter-IIT-CSRE\/The-Eye-in-the-Sky-dataset\/sat\/*.tif'), key=numericalSort)\n# List of file names of classified images for traininig \nfilelist_trainy = sorted(glob.glob('Inter-IIT-CSRE\/The-Eye-in-the-Sky-dataset\/gt\/*.tif'), key=numericalSort)\n\n# List of file names of actual Satellite images for testing \nfilelist_testx = sorted(glob.glob('Inter-IIT-CSRE\/The-Eye-in-the-Sky-test-data\/sat_test\/*.tif'), key=numericalSort)\n                                        \n\n\n# Not useful, messes up with the 4 dimentions of sat images\n\n# Resizing the image to nearest dimensions multipls of 'stride'\n\ndef resize(img, stride, n_h, n_w):\n    #h,l,_ = img.shape\n    ne_h = (n_h*stride) + stride\n    ne_w = (n_w*stride) + stride\n    \n    img_resized = imresize(img, (ne_h,ne_w))\n    return img_resized\n\n# Padding at the bottem and at the left of images to be able to crop them into 128*128 images for training\n\ndef padding(img, w, h, c, crop_size, stride, n_h, n_w):\n    \n    w_extra = w - ((n_w-1)*stride)\n    w_toadd = crop_size - w_extra\n    \n    h_extra = h - ((n_h-1)*stride)\n    h_toadd = crop_size - h_extra\n    \n    img_pad = np.zeros(((h+h_toadd), (w+w_toadd), c))\n    #img_pad[:h, :w,:] = img\n    #img_pad = img_pad+img\n    img_pad = np.pad(img, [(0, h_toadd), (0, w_toadd), (0,0)], mode='constant')\n    \n    return img_pad\n    \n\n\n# Adding pixels to make the image with shape in multiples of stride\n\ndef add_pixals(img, h, w, c, n_h, n_w, crop_size, stride):\n        \n    w_extra = w - ((n_w-1)*stride)\n    w_toadd = crop_size - w_extra\n\n    h_extra = h - ((n_h-1)*stride)\n    h_toadd = crop_size - h_extra\n\n    img_add = np.zeros(((h+h_toadd), (w+w_toadd), c))\n        \n    img_add[:h, :w,:] = img\n    img_add[h:, :w,:] = img[:h_toadd,:, :]\n    img_add[:h,w:,:] = img[:,:w_toadd,:]\n    img_add[h:,w:,:] = img[h-h_toadd:h,w-w_toadd:w,:]\n            \n    return img_add    \n\n\n\n# Adding pixels to make the image with shape in multiples of stride\n\ndef add_pixals(img, h, w, c, n_h, n_w, crop_size, stride):\n    \n    w_extra = w - ((n_w-1)*stride)\n    w_toadd = crop_size - w_extra\n    \n    h_extra = h - ((n_h-1)*stride)\n    h_toadd = crop_size - h_extra\n    \n    img_add = np.zeros(((h+h_toadd), (w+w_toadd), c))\n    \n    img_add[:h, :w,:] = img\n    img_add[h:, :w,:] = img[:h_toadd,:, :]\n    img_add[:h,w:,:] = img[:,:w_toadd,:]\n    img_add[h:,w:,:] = img[h-h_toadd:h,w-w_toadd:w,:]\n    \n    return img_add    \n\n\n# Slicing the image into crop_size*crop_size crops with a stride of crop_size\/2 and makking list out of them\n\ndef crops(a, crop_size = 128):\n    \n    #stride = int(crop_size\/2)\n    stride = 32\n\n    croped_images = []\n    h, w, c = a.shape\n    \n    n_h = int(int(h\/stride))\n    n_w = int(int(w\/stride))\n    \n    # Padding using the padding function we wrote\n    a = padding(a, w, h, c, crop_size, stride, n_h, n_w)\n    \n    # Resizing as required\n    ##a = resize(a, stride, n_h, n_w)\n    \n    # Adding pixals as required\n    #a = add_pixals(a, h, w, c, n_h, n_w, crop_size, stride)\n    \n    # Slicing the image into 128*128 crops with a stride of 64\n    for i in range(n_h-1):\n        for j in range(n_w-1):\n            crop_x = a[(i*stride):((i*stride)+crop_size), (j*stride):((j*stride)+crop_size), :]\n            croped_images.append(crop_x)\n    return croped_images\n\n\n# Another type of cropping\n\ndef new_crops(img, crop_size = 512):\n    stride = crop_size \n    \n    croped_images = []\n    h, w, c = img.shape\n    \n    n_h = math.ceil(h\/stride)\n    n_w = math.ceil(w\/stride)\n    \n    for i in range(n_h):\n        \n        if (h - i*crop_size) >= crop_size:\n            stride = crop_size\n        elif (h - i*crop_size) <= crop_size:\n            stride = (crop_size - (w - i*crop_size))\n        for j in range(n_w):\n            if (w - i*crop_size) >= crop_size:\n                stride = crop_size\n            elif (w - i*crop_size) <= crop_size:\n                stride = (crop_size - (w - i*crop_size))\n                \n            crop_x = img[(i*stride):((i*stride)+crop_size), (j*stride):((j*stride)+crop_size), :]\n            croped_images.append(crop_x)\n    return croped_images\n\n\n# Reading, padding, cropping and making array of all the cropped images of all the trainig sat images\ntrainx_list = []\n\nfor fname in filelist_trainx[:13]:\n    \n    # Reading the image\n    tif = tifffile.imread(fname)\n    image = np.array(tif)\n    \n    # Padding as required and cropping\n    crops_list = crops(image)\n    #print(len(crops_list))\n    trainx_list = trainx_list + crops_list\n    \n# Array of all the cropped Training sat Images    \ntrainx = np.asarray(trainx_list)\n\n\n# Reading, padding, cropping and making array of all the cropped images of all the trainig gt images\ntrainy_list = []\n\nfor fname in filelist_trainy[:13]:\n    \n    # Reading the image\n    tif = tifffile.imread(fname)\n    image = np.array(tif)\n    \n    # Padding as required and cropping\n    crops_list =crops(image)\n    \n    trainy_list = trainy_list + crops_list\n    \n# Array of all the cropped Training gt Images    \ntrainy = np.asarray(trainy_list)\n\n\n# Reading, padding, cropping and making array of all the cropped images of all the testing sat images\ntestx_list = []\n\n#for fname in filelist_trainx[13]:\n    \n    # Reading the image\ntif = tifffile.imread(filelist_trainx[13])\nimage = np.array(tif)\n    \n# Padding as required and cropping\ncrops_list = crops(image)\n    \ntestx_list = testx_list + crops_list\n    \n# Array of all the cropped Testing sat Images  \ntestx = np.asarray(testx_list)\n\n\n# Reading, padding, cropping and making array of all the cropped images of all the testing sat images\ntesty_list = []\n\n#for fname in filelist_trainx[13]:\n    \n# Reading the image\ntif = tifffile.imread(filelist_trainy[13])\nimage = np.array(tif)\n    \n# Padding as required and cropping\ncrops_list = crops(image)\n    \ntesty_list = testy_list + crops_list\n    \n# Array of all the cropped Testing sat Images  \ntesty = np.asarray(testy_list)\n\n# Making array of all the training sat images as it is without any cropping\n\nxtrain_list = []\n\nfor fname in filelist_trainx:\n    \n    # Reading the image\n    tif = tifffile.imread(fname)\n    image = np.array(tif)\n    \n    crop_size = 128\n    \n    stride = 64\n    \n    h, w, c = image.shape\n    \n    n_h = int(int(h\/stride))\n    n_w = int(int(w\/stride))\n    \n    \n    image = padding(image, w, h, c, crop_size, stride, n_h, n_w)\n    \n    xtrain_list.append(image)\n\nx_train = np.asarray(xtrain_list)\ntif = tifffile.imread('Inter-IIT-CSRE\/The-Eye-in-the-Sky-dataset\/sat\/14.tif')\nimage = np.array(tif)\ncrop_size = 128\n    \nstride = 64\nh, w, c = image.shape\n    \nn_h = int(int(h\/stride))\nn_w = int(int(w\/stride))\n    \n    \nimage = padding(image, w, h, c, crop_size, stride, n_h, n_w)\nx_train = image\n# Making array of all the training gt images as it is without any cropping\n\nytrain_list = []\n\nfor fname in filelist_trainy:\n    \n    # Reading the image\n    tif = tifffile.imread(fname)\n    image = np.array(tif)\n    \n    crop_size = 128\n    \n    stride = 64\n    \n    h, w, c = image.shape\n    \n    n_h = int(int(h\/stride))\n    n_w = int(int(w\/stride))\n    \n    \n    image = padding(image, w, h, c, crop_size, stride, n_h, n_w)\n    \n    ytrain_list.append(image)\n\ny_train = np.asarray(ytrain_list)\n\n\ntif = tifffile.imread('Inter-IIT-CSRE\/The-Eye-in-the-Sky-dataset\/gt\/14.tif')\nimage = np.array(tif)\ncrop_size = 128\n    \nstride = 64\n    \nh, w, c = image.shape\n    \nn_h = int(int(h\/stride))\nn_w = int(int(w\/stride))\n    \n    \nimage = padding(image, w, h, c, crop_size, stride, n_h, n_w)\ny_train = image\n\ndef unet(shape = (None,None,4)):\n    \n    # Left side of the U-Net\n    inputs = Input(shape)\n#    in_shape = inputs.shape\n#    print(in_shape)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(inputs)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv1)\n    conv1 = BatchNormalization()(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv2)\n    conv2 = BatchNormalization()(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv3)\n    conv3 = BatchNormalization()(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv4)\n    conv4 = BatchNormalization()(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n    \n    # Bottom of the U-Net\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv5)\n    conv5 = BatchNormalization()(conv5)\n    drop5 = Dropout(0.5)(conv5)\n    \n    # Upsampling Starts, right side of the U-Net\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(UpSampling2D(size = (2,2))(drop5))\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv6)\n    conv6 = BatchNormalization()(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(UpSampling2D(size = (2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv7)\n    conv7 = BatchNormalization()(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv8)\n    conv8 = BatchNormalization()(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv9)\n    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv9)\n    conv9 = BatchNormalization()(conv9)\n\n    # Output layer of the U-Net with a softmax activation\n    conv10 = Conv2D(9, 1, activation = 'softmax')(conv9)\n\n    model = Model(inputs = inputs, outputs = conv10)\n\n    model.compile(optimizer = Adam(lr = 0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    model.summary()\n    \n    #filelist_modelweights = sorted(glob.glob('*.h5'), key=numericalSort)\n    \n    #if 'model_nocropping.h5' in filelist_modelweights:\n     #   model.load_weights('model_nocropping.h5')\n    ##model.load_weights(\"model_onehot.h5\")\n    return model\n\n\nmodel = unet()\n\n\ncolor_dict = {0: (0, 0, 0),\n              1: (0, 125, 0),\n              2: (150, 80, 0),\n              3: (255, 255, 0),\n              4: (100, 100, 100),\n              5: (0, 255, 0),\n              6: (0, 0, 150),\n              7: (150, 150, 255),\n              8: (255, 255, 255)}\n\ndef rgb_to_onehot(rgb_arr, color_dict):\n    num_classes = len(color_dict)\n    shape = rgb_arr.shape[:2]+(num_classes,)\n    #print(shape)\n    arr = np.zeros( shape, dtype=np.int8 )\n    for i, cls in enumerate(color_dict):\n        arr[:,:,i] = np.all(rgb_arr.reshape( (-1,3) ) == color_dict[i], axis=1).reshape(shape[:2])\n    return arr\n\ndef onehot_to_rgb(onehot, color_dict):\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros( onehot.shape[:2]+(3,) )\n    for k in color_dict.keys():\n        output[single_layer==k] = color_dict[k]\n    return np.uint8(output)\n\n\n# Convert trainy and testy into one hot encode\n\ntrainy_hot = []\n\nfor i in range(trainy.shape[0]):\n    \n    hot_img = rgb_to_onehot(trainy[i], color_dict)\n    \n    trainy_hot.append(hot_img)\n    \ntrainy_hot = np.asarray(trainy_hot)\n\ntesty_hot = []\n\nfor i in range(testy.shape[0]):\n    \n    hot_img = rgb_to_onehot(testy[i], color_dict)\n    \n    testy_hot.append(hot_img)\n    \ntesty_hot = np.asarray(testy_hot)\n\n\n#trainx = trainx\/np.max(trainx)\ntrainy = trainy\/np.max(trainy)\n\n#testx = testx\/np.max(testx)\ntesty = testy\/np.max(testy)\n\n# Data Augmentation\n\ndatagen_args = dict(rotation_range=45.,\n                         width_shift_range=0.1,\n                         height_shift_range=0.1,\n                         shear_range=0.2,\n                         zoom_range=0.2,\n                         horizontal_flip=True,\n                         vertical_flip=True,\n                         fill_mode='reflect')\n\nx_datagen = ImageDataGenerator(**datagen_args)\ny_datagen = ImageDataGenerator(**datagen_args)\n\nseed = 1\nbatch_size = 2\nx_datagen.fit(trainx, augment=True, seed = seed)\ny_datagen.fit(trainy, augment=True, seed = seed)\n\nx_generator = x_datagen.flow(trainx, batch_size = 2, seed=seed)\n\ny_generator = y_datagen.flow(trainy, batch_size = 2, seed=seed)\n\ntrain_generator = zip(x_generator, y_generator)\n\nX_datagen_val = ImageDataGenerator()\nY_datagen_val = ImageDataGenerator()\nX_datagen_val.fit(testx, augment=True, seed=seed)\nY_datagen_val.fit(testy, augment=True, seed=seed)\nX_test_augmented = X_datagen_val.flow(testx, batch_size=batch_size, seed=seed)\nY_test_augmented = Y_datagen_val.flow(testy, batch_size=batch_size, seed=seed)\n\ntest_generator = zip(X_test_augmented, Y_test_augmented)\n\nhistory=model.fit_generator(train_generator, validation_data=test_generator, validation_steps=batch_size\/2, epochs = 10, steps_per_epoch=len(x_generator))\nmodel.save(\"model_augment.h5\")\n\n\n#trainx = trainx\/np.max(trainx)\n#trainy = trainy\/np.max(trainy)\n\n#testx = testx\/np.max(testx)\n#testy = testy\/np.max(testy)\n\n# history = model.fit(trainx, trainy_hot, epochs=20, validation_data = (testx, testy_hot),batch_size=2, verbose=1)\n# model.save(\"model_onehot.h5\")\n\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.savefig('acc_plot.png')\nplt.show()\nplt.close()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.savefig('loss_plot.png')\nplt.show()\nplt.close()\n\n#epochs = 20\n#for e in range(epochs):\n#        print(\"epoch %d\" % e)\n#        #for X_train, Y_train in zip(x_train, y_train): # these are chunks of ~10k pictures\n#        h,w,c = x_train.shape\n#        X_train = np.reshape(x_train,(1,h,w,c))\n#        h,w,c = y_train.shape\n#        Y_train = np.reshape(y_train,(1,h,w,c))\n#        model.fit(X_train, Y_train, batch_size=1, nb_epoch=1)\n\t\n#        model.save(\"model_nocropping.h5\")        \n#print(X_train.shape, Y_train.shape)\n\n\n#model.save(\"model_nocropping.h5\")\n\n#epochs = 10\n#for e in range(epochs):\n#\tprint(\"epoch %d\" % e)\n#\tfor X_train, Y_train in zip(x_train, y_train): # these are chunks of ~10k pictures\n#\t\th,w,c = X_train.shape\n#\t\tX_train = np.reshape(X_train,(1,h,w,c))\n#\t\th,w,c = Y_train.shape\n#\t\tY_train = np.reshape(Y_train,(1,h,w,c))\n#\t\tmodel.fit(X_train, Y_train, batch_size=1, nb_epoch=1)\n        #print(X_train.shape, Y_train.shape)\n\n\n#model.save(\"model_nocropping.h5\")\n\n#accuracy = model.evaluate(x=x_test,y=y_test,batch_size=16)\n#print(\"Accuracy: \",accuracy[1])\n","b2fb1a6a":"p  =os.getcwd()\np=''\np=os.path.join(p, 'Inter-IIT-CSRE\/The-Eye-in-the-Sky-dataset\/sat\/2.tif')","187572dd":"os.listdir('Inter-IIT-CSRE\/The-Eye-in-the-Sky-dataset\/sat\/')","3aa2d8dc":"os.rename(p,  p+'f')","9264e448":"main_unet :"}}