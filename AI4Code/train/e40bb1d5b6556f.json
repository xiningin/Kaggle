{"cell_type":{"f628abf9":"code","1d0533a6":"code","31ae33d9":"code","b49dbf6c":"code","007fe3f8":"code","78d6e2bf":"code","07f69282":"code","5b84b688":"code","8f694321":"code","aeb8ceb7":"code","b0ab81e9":"code","e429b9b0":"code","c50cf414":"code","93bac352":"code","750126a2":"code","b2b60637":"code","34c67d03":"code","916cd91e":"code","78bee4a7":"code","da2817d9":"code","2e726115":"code","d2b0c77c":"code","246ae0b1":"code","9bdee515":"code","d8948067":"code","301a2ab8":"code","9600c242":"markdown","a7ffb7a8":"markdown","ad1a6cca":"markdown","92889a76":"markdown","c57b4f9b":"markdown","7d0a94a0":"markdown","3ea390ee":"markdown","e9fdce03":"markdown","21523e2d":"markdown","bd994a21":"markdown","24a060d9":"markdown","ccf36fe8":"markdown","f35ddc3d":"markdown","c5322141":"markdown","417fcb50":"markdown","d495ed0f":"markdown","2d93e192":"markdown","f4e16194":"markdown","c99b3f1d":"markdown","75775d79":"markdown","aec3d55b":"markdown","bdf85a52":"markdown","8ec97a6a":"markdown","af20e617":"markdown","229e45cc":"markdown","4ce0618d":"markdown","734c68f8":"markdown","0c677bfb":"markdown","b9f83a6b":"markdown","7984dcdc":"markdown","d612fd1b":"markdown","0ce4095a":"markdown","a6e1f157":"markdown","824e2ab9":"markdown","b08a335c":"markdown","222dfdaf":"markdown","bf289641":"markdown"},"source":{"f628abf9":"# Core\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport math\nimport statistics\nimport scipy.stats\nfrom scipy.stats import pearsonr\nimport time\nfrom datetime import datetime\nimport matplotlib.dates as mdates\nimport dateutil.easter as easter\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n# Models\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","1d0533a6":"# Save to df\ntrain_data=pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv', index_col='row_id')\ntest_data=pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv', index_col='row_id')\n\n# Shape and preview\nprint('Training data df shape:',train_data.shape)\nprint('Test data df shape:',test_data.shape)\ntrain_data.head()","31ae33d9":"# Convert date to datetime\ntrain_data.date=pd.to_datetime(train_data.date)\ntest_data.date=pd.to_datetime(test_data.date)\n\n# drop 29th Feb\ntrain_data.drop(train_data[(train_data.date.dt.month==2) & (train_data.date.dt.day==29)].index, axis=0, inplace=True)","b49dbf6c":"# Figure\nplt.figure(figsize=(12,5))\n\n# Groupby\naa=train_data.groupby(['date','store']).agg(num_sold=('num_sold','sum'))\n\n# Lineplot\nsns.lineplot(data=aa, x='date', y='num_sold', hue='store')\n\n# Aesthetics\nplt.title('num_sold by store')","007fe3f8":"# Subplots\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Groupby\nKR=train_data[train_data.store=='KaggleRama']\nKM=train_data[train_data.store=='KaggleMart']\nbb=KR.groupby(['date','product']).agg(num_sold=('num_sold','sum'))\ncc=KM.groupby(['date','product']).agg(num_sold=('num_sold','sum'))\n\n# Lineplots\nax1=sns.lineplot(ax=axes[0], data=bb, x='date', y='num_sold', hue='product')\nax2=sns.lineplot(ax=axes[1], data=cc, x='date', y='num_sold', hue='product')\n\n# Aesthetics\nax1.title.set_text('KaggleRama')\nax2.title.set_text('KaggleMart')","78d6e2bf":"# Subplots\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Groupby\ndd=KR.groupby(['date','country']).agg(num_sold=('num_sold','sum'))\nee=KM.groupby(['date','country']).agg(num_sold=('num_sold','sum'))\n\n# Lineplots\nax1=sns.lineplot(ax=axes[0], data=dd, x='date', y='num_sold', hue='country')\nax2=sns.lineplot(ax=axes[1], data=ee, x='date', y='num_sold', hue='country')\n\n# Aesthetics\nax1.title.set_text('KaggleRama')\nax2.title.set_text('KaggleMart')","07f69282":"# Labels\ny=train_data.num_sold\n\n# Features\nX=train_data.drop('num_sold', axis=1)","5b84b688":"# From https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298990\ndef unofficial_hol(df):\n    countries = {'Finland': 1, 'Norway': 2, 'Sweden': 3}\n    stores = {'KaggleMart': 1, 'KaggleRama': 2}\n    products = {'Kaggle Mug': 1,'Kaggle Hat': 2, 'Kaggle Sticker': 3}\n    \n    # load holiday info.\n    hol_path = '..\/input\/public-and-unofficial-holidays-nor-fin-swe-201519\/holidays.csv'\n    holiday = pd.read_csv(hol_path)\n    \n    fin_holiday = holiday.loc[holiday.country == 'Finland']\n    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    return df","8f694321":"def get_holidays(df):\n    # End of year\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                      for d in range(24, 32)}),\n        pd.DataFrame({f\"n-dec{d}\":\n                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in range(24, 32)}),\n        pd.DataFrame({f\"f-jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                      for d in range(1, 14)}),\n        pd.DataFrame({f\"jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in range(1, 10)}),\n        pd.DataFrame({f\"s-jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                      for d in range(1, 15)})], axis=1)\n    \n    # May\n    df = pd.concat([df, pd.DataFrame({f\"may{d}\":\n                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                      for d in list(range(1, 10))}),\n        pd.DataFrame({f\"may{d}\":\n                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in list(range(19, 26))})], axis=1)\n    \n    # June and July\n    df = pd.concat([df, pd.DataFrame({f\"june{d}\":\n                   (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                   for d in list(range(8, 14))})], axis=1)\n    \n    #Swedish Rock Concert\n    #Jun 3, 2015 \u2013 Jun 6, 2015\n    #Jun 8, 2016 \u2013 Jun 11, 2016\n    #Jun 7, 2017 \u2013 Jun 10, 2017\n    #Jun 6, 2018 \u2013 Jun 10, 2018\n    #Jun 5, 2019 \u2013 Jun 8, 2019\n    swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),\n                                         2016: pd.Timestamp(('2016-06-11')),\n                                         2017: pd.Timestamp(('2017-06-10')),\n                                         2018: pd.Timestamp(('2018-06-10')),\n                                         2019: pd.Timestamp(('2019-06-8'))})\n\n    df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n                                      (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n                                      for d in list(range(-3, 3))})], axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"wed_june{d}\": \n                   (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                   for d in list(range(-4, 6))})], axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\": \n                   (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                   for d in list(range(0, 9))})], axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                   (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                   for d in list(range(6, 14))})], axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n                   (df.date - easter_date == np.timedelta64(d, \"D\"))\n                   for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n    \n    return df","aeb8ceb7":"def date_feat_eng_X1(df):\n    df['year']=df['date'].dt.year                   # 2015 to 2019\n    return df\n\ndef date_feat_eng_X2(df):\n    df['day_of_week']=df['date'].dt.dayofweek       # 0 to 6\n    df['day_of_month']=df['date'].dt.day            # 1 to 31\n    df['dayofyear'] = df['date'].dt.dayofyear       # 1 to 366\n    df.loc[(df.date.dt.year==2016) & (df.dayofyear>60), 'dayofyear'] -= 1   # 1 to 365\n    df['week']=df['date'].dt.isocalendar().week     # 1 to 53\n    df['week']=df['week'].astype('int')             # int64\n    df['month']=df['date'].dt.month                 # 1 to 12\n    return df","b0ab81e9":"def get_GDP(df):\n\n    # Load data\n    GDP_data = pd.read_csv(\"..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n\n    # Rename the columns in GDP df \n    GDP_data.columns = ['Finland', 'Norway', 'Sweden']\n\n    # Create a dictionary\n    GDP_dictionary = GDP_data.unstack().to_dict()\n    \n    # Create GDP column\n    df['GDP']=df.set_index(['country', 'year']).index.map(GDP_dictionary.get)\n    \n    # Log transform (only if the target is log-transformed too)\n    df['GDP']=np.log(df['GDP'])\n    \n    # Split GDP by country (for linear model)\n    df['GDP_Finland']=df['GDP'] * (df['country']=='Finland')\n    df['GDP_Norway']=df['GDP'] * (df['country']=='Norway')\n    df['GDP_Sweden']=df['GDP'] * (df['country']=='Sweden')\n    \n    # Drop column\n    df=df.drop(['GDP','year'],axis=1)\n    \n    return df","e429b9b0":"def GDP_PC(df):\n    # Load data\n    GDP_PC_data = pd.read_csv(\"..\/input\/gdp-per-capita-finland-norway-sweden-201519\/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n    \n    # Create a dictionary\n    GDP_PC_dictionary = GDP_PC_data.unstack().to_dict()\n\n    # Create new GDP_PC column\n    df['GDP_PC'] = df.set_index(['country', 'year']).index.map(GDP_PC_dictionary.get)\n    \n    return df","c50cf414":"def GDP_corr(df):\n    \n    # Load data\n    GDP_data = pd.read_csv(\"..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n    GDP_PC_data = pd.read_csv(\"..\/input\/gdp-per-capita-finland-norway-sweden-201519\/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n\n    # Rename the columns\n    GDP_data.columns = ['Finland', 'Norway', 'Sweden']\n    \n    # Create dictionary\n    GDP_dictionary = GDP_data.unstack().to_dict()\n    GDP_PC_dictionary = GDP_PC_data.unstack().to_dict()\n    \n    # Add year column\n    df['year']=df.date.dt.year\n    \n    # Make new column\n    df['GDP']=df.set_index(['country', 'year']).index.map(GDP_dictionary.get)\n    df['GDP_PC'] = df.set_index(['country', 'year']).index.map(GDP_PC_dictionary.get)\n\n    # Initialise output\n    feat_corr=[]\n    \n    # Compute pairwise correlations\n    for SS in ['KaggleMart', 'KaggleRama']:\n        for CC in ['Finland', 'Norway', 'Sweden']:\n            for PP in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n                subset=df[(df.store==SS)&(df.country==CC)&(df['product']==PP)].groupby(['year']).agg(num_sold=('num_sold','sum'), GDP=('GDP','mean'), GDP_PC=('GDP_PC','mean'))\n                v1=subset.num_sold\n                v2=subset.GDP\n                v3=subset.GDP_PC\n                \n                r1, _ = pearsonr(v1,v2)\n                r2, _ = pearsonr(v1,v3)\n                \n                feat_corr.append([f'{SS}, {CC}, {PP}', r1, r2])\n\n    return pd.DataFrame(feat_corr, columns=['Features', 'GDP_corr', 'GDP_PC_corr'])\n    \ncorr_df=GDP_corr(train_data)\ncorr_df.head()","93bac352":"# From https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model#Simple-feature-engineering-(without-holidays)\ndef FourierFeatures(df):\n    # temporary one hot encoding\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 2):\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        df=df.drop([f'sin{k}', f'cos{k}'], axis=1)\n    \n    # drop temporary one hot encoding\n    df=df.drop(['Kaggle Mug', 'Kaggle Hat'], axis=1)\n    \n    return df","750126a2":"# Help linear model find the right height of trends for each combination of features\ndef get_interactions(df):\n    df['KR_Sweden_Mug']=(df.country=='Sweden')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Sweden_Hat']=(df.country=='Sweden')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Sweden_Sticker']=(df.country=='Sweden')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    df['KR_Norway_Mug']=(df.country=='Norway')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Norway_Hat']=(df.country=='Norway')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Norway_Sticker']=(df.country=='Norway')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    df['KR_Finland_Mug']=(df.country=='Finland')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Finland_Hat']=(df.country=='Finland')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Finland_Sticker']=(df.country=='Finland')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    \n    df['KM_Sweden_Mug']=(df.country=='Sweden')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Sweden_Hat']=(df.country=='Sweden')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Sweden_Sticker']=(df.country=='Sweden')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    df['KM_Norway_Mug']=(df.country=='Norway')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Norway_Hat']=(df.country=='Norway')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Norway_Sticker']=(df.country=='Norway')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    df['KM_Finland_Mug']=(df.country=='Finland')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Finland_Hat']=(df.country=='Finland')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Finland_Sticker']=(df.country=='Finland')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    \n    return df","b2b60637":"def dropdate(df):\n    df=df.drop('date',axis=1)\n    return df\n\ndef onehot(df,columns):\n    df=pd.get_dummies(df, columns)\n    return df","34c67d03":"# Feature set for trend model\ndef FeatEng_X1(df):\n    df=date_feat_eng_X1(df)\n    df=get_GDP(df)\n    df=FourierFeatures(df)\n    df=get_interactions(df)\n    df=dropdate(df)\n    df=onehot(df,['store', 'product', 'country'])\n    return df\n\n# Feature set for interactions model\ndef FeatEng_X2(df):\n    df=date_feat_eng_X2(df)\n    df=unofficial_hol(df)\n    df=get_holidays(df)\n    df=dropdate(df)\n    df=onehot(df,['store', 'product', 'country'])\n    return df\n\n# Apply feature engineering\nX_train_1=FeatEng_X1(X)\nX_train_2=FeatEng_X2(X)\nX_test_1=FeatEng_X1(test_data)\nX_test_2=FeatEng_X2(test_data)","916cd91e":"# A class is a collection of properties and methods (like models from Sklearn)\nclass HybridModel:\n    def __init__(self, model_1, model_2, grid=None):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.grid=grid\n        \n    def fit(self, X_train_1, X_train_2, y):\n        # Train model 1\n        self.model_1.fit(X_train_1, y)\n        \n        # Predictions from model 1 (trend)\n        y_trend = self.model_1.predict(X_train_1)\n\n        if self.grid:\n            # Grid search\n            tscv = TimeSeriesSplit(n_splits=3)\n            grid_model = GridSearchCV(estimator=self.model_2, cv=tscv, param_grid=self.grid)\n        \n            # Train model 2 on detrended series\n            grid_model.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = grid_model.predict(X_train_2)\n            \n            # Save model\n            self.grid_model=grid_model\n        else:\n            # Train model 2 on residuals\n            self.model_2.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = self.model_2.predict(X_train_2)\n        \n        # Save data\n        self.y_train_trend = y_trend\n        self.y_train_resid = y_resid\n        \n    def predict(self, X_test_1, X_test_2):\n        # Predict trend using model 1\n        y_trend = self.model_1.predict(X_test_1)\n        \n        if self.grid:\n            # Grid model predictions\n            y_resid = self.grid_model.predict(X_test_2)\n        else:\n            # Model 2 predictions\n            y_resid = self.model_2.predict(X_test_2)\n        \n        # Add predictions together\n        y_pred = y_trend + y_resid\n        \n        # Save data\n        self.y_test_trend = y_trend\n        self.y_test_resid = y_resid\n        \n        return y_pred","78bee4a7":"# Choose models\nmodel_1=LinearRegression()\nmodels_2=[LGBMRegressor(random_state=0), CatBoostRegressor(random_state=0, verbose=False), XGBRegressor(random_state=0)]\n\n# Parameter grid\nparam_grid = {'n_estimators': [100, 150, 200, 225, 250, 275, 300],\n        'max_depth': [4, 5, 6, 7],\n        'learning_rate': [0.1, 0.12, 0.13, 0.14, 0.15]}\n\n# Initialise output vectors\ny_pred=np.zeros(len(test_data))\ntrain_preds=np.zeros(len(y))\n\n# Ensemble predictions\nfor model_2 in models_2:\n    # Start timer\n    start = time.time()\n    \n    # Construct hybrid model\n    model = HybridModel(model_1, model_2, grid=param_grid)\n\n    # Train model\n    model.fit(X_train_1, X_train_2, np.log(y))\n\n    # Save predictions\n    y_pred += np.exp(model.predict(X_test_1,X_test_2))\n    \n    # Training set predictions (for residual analysis)\n    train_preds += np.exp(model.y_train_trend+model.y_train_resid)\n    \n    # Stop timer\n    stop = time.time()\n    \n    print(f'Model_2:{model_2} -- time:{round((stop-start)\/60,2)} mins')\n    \n    if model.grid:\n        print('Best parameters:',model.grid_model.best_params_,'\\n')\n    \n# Scale\ny_pred = y_pred\/len(models_2)\ntrain_preds = train_preds\/len(models_2)","da2817d9":"# From https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    return result_array\n\ny_pred=geometric_round(y_pred)\n\n# Save predictions to file\noutput = pd.DataFrame({'row_id': test_data.index, 'num_sold': y_pred})\n\n# Check format\noutput.head()","2e726115":"output.to_csv('submission.csv', index=False)","d2b0c77c":"def plot_predictions(SS, CC, PP, series=output):\n    '''\n    SS=store\n    CC=country\n    PP=product\n    '''\n    \n    # uncomment if your dataframes have different names\n    #train_data=train_df\n    #test_data=test_df\n    \n    # Training set target\n    train_subset=train_data[(train_data.store==SS)&(train_data.country==CC)&(train_data['product']==PP)]\n    \n    # Predictions\n    plot_index=test_data[(test_data.store==SS)&(test_data.country==CC)&(test_data['product']==PP)].index\n    pred_subset=series[series.row_id.isin(plot_index)].reset_index(drop=True)\n    \n    # Plot\n    plt.figure(figsize=(12,5))\n    n1=len(train_subset['num_sold'])\n    n2=len(pred_subset['num_sold'])\n    plt.plot(np.arange(n1),train_subset['num_sold'], label='Training')\n    plt.plot(np.arange(n1,n1+n2),pred_subset['num_sold'], label='Predictions')\n    plt.title('\\n'+f'Store:{SS}, Country:{CC}, Product:{PP}')\n    plt.legend()\n    plt.xlabel('Days since 2015-01-01')\n    plt.ylabel('num_sold')","246ae0b1":"# Put into dataframes\ny_trend=pd.DataFrame({'row_id': test_data.index, 'num_sold': np.exp(model.y_test_trend)})\ny_resid=pd.DataFrame({'row_id': test_data.index, 'num_sold': np.exp(model.y_test_resid)})\ny_pred=pd.DataFrame({'row_id': test_data.index, 'num_sold': np.exp(model.y_test_trend+model.y_test_resid)})\n\n# Choose parameters\nSS='KaggleMart'\nCC='Norway'\n\n# Plot trends (model 1 predictions)\nplot_predictions(SS, CC, 'Kaggle Hat', series=y_trend)\nplot_predictions(SS, CC, 'Kaggle Mug', series=y_trend)\nplot_predictions(SS, CC, 'Kaggle Sticker', series=y_trend)","9bdee515":"for SS in ['KaggleMart','KaggleRama']:\n    for CC in ['Finland', 'Norway', 'Sweden']:\n        for PP in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n            plot_predictions(SS, CC, PP)","d8948067":"# need to ensemble\ntrain_preds = np.exp(model.y_train_trend+model.y_train_resid)\n\n# Residuals on training set (SMAPE)\nresiduals = 200 * (train_preds - y) \/ (train_preds + y)\n\n# Plot residuals\nplt.figure(figsize=(12,4))\nplt.scatter(np.arange(len(residuals)),residuals, s=1)\nplt.hlines([0], 0, residuals.index.max(), color='k')\nplt.title('Residuals on training set')\nplt.xlabel('Sample')\nplt.ylabel('SMAPE')","301a2ab8":"mu, std = scipy.stats.norm.fit(residuals)\n\nplt.figure(figsize=(12,4))\nplt.hist(residuals, bins=100, density=True)\nx = np.linspace(plt.xlim()[0], plt.xlim()[1], 200)\nplt.plot(x, scipy.stats.norm.pdf(x, mu, std), 'r', linewidth=2)\nplt.title(f'Histogram of residuals; mean = {residuals.mean():.4f}, '\n          f'$\\sigma = {residuals.std():.1f}$, SMAPE = {residuals.abs().mean():.5f}')\nplt.xlabel('Residual (percent)')\nplt.ylabel('Density')\nplt.show()","9600c242":"# Residual Analysis","a7ffb7a8":"**Plot residuals**","ad1a6cca":"**All predictions**","92889a76":"# Quick EDA","c57b4f9b":"**Plot trends**","7d0a94a0":"**GDP**","3ea390ee":"**Include day of week, month, year etc**","e9fdce03":"**GDP vs GDP per capita**","21523e2d":"**Datetime and drop 29th Feb**","bd994a21":"**num_sold by country**","24a060d9":"**Fourier features**","ccf36fe8":"# Introduction","f35ddc3d":"* The Hat and Mug show strong yearly seasonal trends whereas the Sticker remains fairly constant. We can use Fourier Features to model these trends.\n* Hats sells the most, then Mugs and finally Stickers.","c5322141":"The idea is as follows. Linear interpolation is good at extrapolating trends but poor at learning interactions. Conversely, decision tree algorithms like XGBoost are very good at learning interactions but can't extrapolate trends. A hybrid model tries to take the best of both worlds by first learning the trend with linear interpolation and then learning the interactions on the detrended time series.","417fcb50":"**Ensembling**","d495ed0f":"**Interactions**","2d93e192":"* Most products are sold in Norway, then Sweden followed by Finland.\n* Some spikes accur at different times for each country, perhaps because of different holidays.","f4e16194":"# Data","c99b3f1d":"* In general, both GDP and GDP_PC are very highly correlated to the num_sold aggregate each year. \n* GDP tends to have a slightly higher correlation than GDP_PC.","75775d79":"**Plot histogram of residuals**","aec3d55b":"**Holidays (from AmbrosM)**","bdf85a52":"# Prediction","8ec97a6a":"# Hybrid model","af20e617":"**GDP per capita**","229e45cc":"* KaggleRama consistently sells more products than KaggleMart. \n* There are big spikes towards the end of each year.","4ce0618d":"# Libraries","734c68f8":"**num_sold by product**","0c677bfb":"**Post-processing**","b9f83a6b":"**Public holidays (including unofficial ones)**","7984dcdc":"**Labels and features**","d612fd1b":"**num_sold by store**","0ce4095a":"See my previous notebook for EDA and a LGBM model: [tps-jan-22-eda-modelling](https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-eda-modelling). This notebook improves on my previous one and attempts a hybrid model. \n\n**Acknowledgements:**\n* Kaggle's [time series course](https:\/\/www.kaggle.com\/learn\/time-series).\n* This [notebook](https:\/\/www.kaggle.com\/teckmengwong\/tps2201-hybrid-time-series\/notebook#Data\/Feature-Engineering) by [Teck Meng Wong](https:\/\/www.kaggle.com\/teckmengwong).\n* Many of [AmbrosM's](https:\/\/www.kaggle.com\/ambrosm) great notebooks.\n* This [notebook](https:\/\/www.kaggle.com\/lucamassaron\/kaggle-merchandise-eda-with-baseline-linear-model\/notebook) by [Luca Massaron](https:\/\/www.kaggle.com\/lucamassaron).","a6e1f157":"**Put pieces together**","824e2ab9":"# Plot predictions","b08a335c":"**Drop date and one hot encoding**","222dfdaf":"**Load data**","bf289641":"# Feature Engineering"}}