{"cell_type":{"44ffbb41":"code","b0260310":"code","a3624012":"code","cab8c18b":"code","00a9f546":"code","a7b918b5":"code","43b38d93":"code","1d0de345":"code","4d8f1431":"code","81afcea0":"code","86ef4547":"code","0a49e591":"code","77b04cf6":"code","4cc10819":"code","f20671fd":"code","4b50b222":"code","dcf4ac1a":"code","7fe2836b":"code","563d866b":"code","9ffb5a1d":"code","d68494a8":"code","3936ad4e":"code","2ac0559e":"code","dce55af4":"code","873a4123":"code","4d0a6b4e":"code","63f9d3ba":"code","636c7520":"code","3aec5b5a":"code","f32f1353":"markdown","bb81a357":"markdown","30200ee4":"markdown","6aa8fbc0":"markdown","5197b57a":"markdown","b73844c6":"markdown","2240a07f":"markdown","d0be597c":"markdown","ea81e756":"markdown","c6108d25":"markdown","4c912019":"markdown","b4da0b3e":"markdown"},"source":{"44ffbb41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+En8*ter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0260310":"import gresearch_crypto\nfrom datetime import datetime\nimport time\nimport os \n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt","a3624012":"def corr(a, b, w):\n    cov = lambda x, y: np.sum(w * (x - np.average(x, weights=w)) * (y - np.average(y, weights=w))) \/ np.sum(w)\n    return cov(a, b) \/ np.sqrt(cov(a, a) * cov(b, b))","cab8c18b":"data_folder = \"\/kaggle\/input\/g-research-crypto-forecasting\/\"\n\ntotimestamp = lambda s: np.int32(time.mktime(datetime.strptime(s, \"%d\/%m\/%Y\").timetuple()))\nstart_time = totimestamp('01\/01\/2021')\nend_time = totimestamp('21\/09\/2021')\ntrain_index = slice(start_time,totimestamp('01\/07\/2021')-60)\ntest_index = slice(totimestamp('01\/07\/2021'),end_time)\n\nall_train = pd.read_csv(data_folder+'train.csv').set_index(\"timestamp\").loc[start_time : end_time]\n\nasset_details = pd.read_csv(data_folder+'asset_details.csv')\nasset_details['w'] = asset_details['Weight']\/asset_details['Weight'].sum()\nasset_details = asset_details.set_index('Asset_ID').sort_index()","00a9f546":"all_train","a7b918b5":"# pd.pivot_table(all_train[['Asset_ID','Open']],values = 'Open',index =all_train.index ,columns = ['Asset_ID'])\n","43b38d93":"# del t1","1d0de345":"features = {}\nfor col in all_train.columns[1:]:\n#     features[col] = pd.pivot_table(all_train[['timestamp','Asset_ID',col]],values = col,index ='timestamp' ,columns = ['Asset_ID'])\n    features[col] = pd.pivot_table(all_train[['Asset_ID',col]],values = col,index =all_train.index ,columns = ['Asset_ID'])\n    features[col] = features[col].reindex(range(start_time,end_time+60,60),method='pad')\n    print(f'finish {col}')\n    \nidx = features['Open'].index\ncols = features['Open'].columns","4d8f1431":"# source\nprint('source',features.keys())\n\n# \u7f3a\u5931\u503c\u6bd4\u7387\nprint('\u7f3a\u5931\u503c\u6bd4\u7387')\nfor k,v in features.items():\n    print(k,(v.isnull().sum() \/ v.shape[0]).mean()  )\n","81afcea0":"features['Open'].head()","86ef4547":"# def ts_corr(df):\n#     return  np.average(df.corrwith(features['Target']),weights = asset_details['w'] ) \n\ndef corr(a, b, w):\n    cov = lambda x, y: np.sum(w * (x - np.average(x, weights=w)) * (y - np.average(y, weights=w))) \/ np.sum(w)\n    return cov(a, b) \/ np.sqrt(cov(a, a) * cov(b, b))\n\ndef log_return(series, periods):\n    return np.log(series).diff(periods=periods)\n\ndef get_mkt_and_residual_ret(ret):\n    market_return = pd.Series(np.nanmean(ret.mul(asset_details['w'],axis=1) ,axis=1),index = idx)\n    a = ret.mul(market_return,axis=0).rolling(3750,min_periods=3750).mean()\n    b = (market_return*market_return).rolling(3750,min_periods=3750).mean()\n    beta = a.div(b,axis=0)\n    re_target = ret - beta.mul(market_return,axis=0)\n    return market_return,re_target\n\nupper_shadow = (features['High'] - np.maximum(features['Close'],features['Open']))\/features['Close']\nlower_shadow = (np.minimum(features['Close'],features['Open']) - features['Low'])\/features['Close']\n\nlogret15 = log_return(features['Close'],15)\nlogret15_mkt,logret15_re = get_mkt_and_residual_ret(log_return(features['Close'],15))\nlogret15_mkt= pd.DataFrame(np.tile(logret15_mkt.values,(14,1)).T,index=idx, columns = cols)\n\nlogret60 = log_return(features['Close'],60)\nlogret60_mkt,logret60_re = get_mkt_and_residual_ret(log_return(features['Close'],60))\nlogret60_mkt= pd.DataFrame(np.tile(logret60_mkt.values,(14,1)).T,index=idx, columns = cols)\n\nlogret100 = log_return(features['Close'],100)\nlogret100_mkt,logret100_re = get_mkt_and_residual_ret(log_return(features['Close'],100))\nlogret100_mkt= pd.DataFrame(np.tile(logret100_mkt.values,(14,1)).T,index=idx, columns = cols)","0a49e591":"# feats = [(features['High'] - np.maximum(features['Close'],features['Open'])),\n#         (np.minimum(features['Close'],features['Open']) - features['Low']),\n#          logret15] #\u6539\u8fdb\u524d\u7684\u56e0\u5b50\n\nfeats = [upper_shadow,lower_shadow,\n         logret15_mkt, logret60_mkt, logret100_mkt,\n         logret15_re, logret60_re, logret100_re,\n        logret15, logret60, logret100]\n\nfeats = [feat.unstack() for feat in feats]\nX = pd.concat(feats,axis=1)\ny = features['Target'].unstack()\n\n\nX_train = X.loc[(slice(0,13),train_index),:]\ny_train = y.loc[slice(0,13),train_index]\nX_test = X.loc[(slice(0,13),test_index),:]\ny_test = y.loc[slice(0,13),test_index]\n\n# def split(X,y,train_ratio):\n#     train_num = int(X.shape[0]*train_ratio)\n#     return X.iloc[:train_num],X.iloc[train_num:], y.iloc[:train_num],y.iloc[train_num:]\n\n# X_train, X_valid, y_train, y_valid = split(X_train,y_train,0.8)","77b04cf6":"X_train.shape[0]+X_test.shape[0] == X.shape[0]","4cc10819":"y_train.shape[0]+y_test.shape[0] == y.shape[0]","f20671fd":"X.shape[0] == y.shape[0]","4b50b222":"def corr(a, b, w):\n    cov = lambda x, y: np.sum(w * (x - np.average(x, weights=w)) * (y - np.average(y, weights=w))) \/ np.sum(w)\n    return cov(a, b) \/ np.sqrt(cov(a, a) * cov(b, b))","dcf4ac1a":"w = pd.Series(index = X_test.index)\nfor i in asset_details['Weight'].index:\n    w.loc[i] = asset_details['Weight'][i]\nw","7fe2836b":"from sklearn.linear_model import LinearRegression\n\n# implement basic ML baseline (one per asset)\nlr = LinearRegression()\nlr.fit(X_train.ffill().fillna(0),y_train.ffill().fillna(0))\ny_lr_pred = lr.predict(X_test.ffill().fillna(0))\nprint(\"y_lr_pred : \",corr(np.nan_to_num(y_lr_pred),np.nan_to_num(y_test.values),np.nan_to_num(np.array(w.to_list()))) )","563d866b":"import lightgbm as lgb\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n# specify your configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2', 'l1'},\n    'num_leaves': 31,\n    'learning_rate': 0.2,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\nprint('Starting training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=500,\n                valid_sets=[lgb_train,lgb_eval],\n                early_stopping_rounds=50,\n               verbose_eval=50)\n\ny_lgb_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\nprint(\"y_lr_pred : \",corr(np.nan_to_num(y_lgb_pred),np.nan_to_num(y_test.values),np.nan_to_num(np.array(w.to_list()))) )","9ffb5a1d":"X_trains = {}\ny_trains = {}\nX_tests = {}\ny_tests = {}\nfor i in range(14):\n    X_trains[i] = X_train.loc[(i),:]\n    y_trains[i] = y_train.loc[(i),:]\n    X_tests[i] = X_test.loc[(i),:]\n    y_tests[i] = y_test.loc[(i),:]","d68494a8":"from sklearn.linear_model import LinearRegression\n\nlr_models = {}\n\ndef get_lr_model(X_train,y_train):\n    lr = LinearRegression()\n    lr.fit(X_train.ffill().fillna(0),y_train.ffill().fillna(0))\n    return lr\n\nfor (i,X_train),(_,y_train) in zip(X_trains.items(),y_trains.items()):\n    lr_models[i] = get_lr_model(X_train,y_train)\n\ny_lr_preds = []\nfor i,lr_model in lr_models.items():\n    y_lr_preds.append(lr_model.predict(X_tests[i].ffill().fillna(0)))\n\nprint(\"small seperate model , y_lr_preds : \",corr(np.nan_to_num(np.array(y_lr_preds).flatten()),np.nan_to_num(y_test.values),np.nan_to_num(np.array(w.to_list()))) )","3936ad4e":"import lightgbm as lgb\nlgb_models = {}\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2', 'l1'},\n    'num_leaves': 31,\n    'learning_rate': 0.2,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\ndef get_lgb_model(X_train,y_train):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    print('Starting training...')\n    # train\n    gbm = lgb.train(params,\n                    lgb_train,\n                    num_boost_round=500,\n                    valid_sets=[lgb_train],\n                    early_stopping_rounds=50,\n                   verbose_eval=50)\n    return gbm\n\nfor (i,X_train),(_,y_train) in zip(X_trains.items(),y_trains.items()):\n    lgb_models[i] = get_lgb_model(X_train,y_train)\n\ny_lgb_preds = []\nfor i,lgb_model in lgb_models.items():\n    y_lgb_preds.append(lgb_model.predict(X_tests[i], num_iteration=lgb_model.best_iteration))\n    \nprint(\"small seperate model , y_lgb_preds : \",corr(np.nan_to_num(np.array(y_lgb_preds).flatten()),np.nan_to_num(y_test.values),np.nan_to_num(np.array(w.to_list()))) )","2ac0559e":"print(\"small seperate model , y_lgb_preds : \",corr(np.nan_to_num(np.array(y_lgb_preds).flatten()),np.nan_to_num(y_test.values),np.nan_to_num(np.array(w.to_list()))) )","dce55af4":"print(\"lgb single model for asset 0 \")\nnp.corrcoef(np.nan_to_num(y_lgb_preds[0]),np.nan_to_num(y_tests[0].values))","873a4123":"print(\"linear single model for asset 0 \")\nnp.corrcoef(np.nan_to_num(y_lr_preds[0]),np.nan_to_num(y_tests[0].values))","4d0a6b4e":"# import gresearch_crypto\n# import traceback\n\n# env = gresearch_crypto.make_env()\n# iter_test = env.iter_test()\n# # model = lr\n\n# for i, (df_test, df_pred) in enumerate(iter_test):\n#     for j , row in df_test.iterrows():        \n#         try:\n#             print(row)\n#             x_test = get_features(row)\n#             y_pred = model.predict(x_test.ffill.fillna(0))\n#             df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n#         except:\n#             df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n#             traceback.print_exc()\n    \n#     env.predict(df_pred)","63f9d3ba":"# # r \u6839\u636e\u9898\u76ee\u5b9a\u4e49\u4e25\u683c\u5b9e\u73b0 Ra(t)=log(Pa(t+16) \/ Pa(t+1))\n# ret =  np.log(features['Close'].shift(-16)\/features['Close'].shift(-1))\n# market_return = pd.Series(np.nanmean(ret.mul(asset_details['w'],axis=1)\n#                                      ,axis=1),index = idx)\n\n# # beta \u6839\u636e\u9898\u76ee\u5b9a\u4e49\u5b9e\u73b0 beta = m*r \/ m*m\n# a = ret.mul(market_return,axis=0).rolling(3750,min_periods=3750).mean()\n# b = (market_return*market_return).rolling(3750,min_periods=3750).mean()\n# beta = a.div(b,axis=0)\n\n# #\n# re_target = ret - beta.mul(market_return,axis=0)\n# print(\"mean mae\",(re_target - features['Target']).abs().mean())\n# print(\"max mae\",(re_target - features['Target']).abs().max())\n# print(\"min mae\",(re_target - features['Target']).abs().min())","636c7520":"# from sklearn.model_selection import GridSearchCV\n\n# def get_model(X,y):\n#     model = LGBMRegressor()\n#     model.fit(X, y)\n#     return X, y, model\n\n# parameters = {\n# #     'max_depth': range (2, 10, 1),\n#     'num_leaves': range(21, 161, 10),\n#     'learning_rate': [0.1, 0.01, 0.05]\n# }\n\n# grid_search = GridSearchCV(\n#         estimator=get_model(X_btc_train_scaled,y_btc_train)[2], # bitcoin\n#         param_grid=parameters,\n#         n_jobs = -1,\n#         cv = 5,\n#         verbose=True\n#     )\n# grid_search.fit(X_btc_train_scaled, y_btc_train)\n# new_model = grid_search.best_estimator_\n# grid_search.best_estimator_\n\n# y_pred_lgbm_tune_btc = new_model.predict(X_btc_test_scaled)\n# print('Test score for LGBM tune model: BTC', f\"{np.corrcoef(y_pred_lgbm_tune_btc, y_btc_test)[0,1]:.4f}\")","3aec5b5a":"# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import LSTM\n# from tensorflow.keras.layers import Dense\n# from tensorflow.keras.layers import Dropout\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# tf.__version__\n\n# model1 = tf.keras.models.Sequential()\n# model1.add(tf.keras.Input(shape=(4,)))\n# model1.add(tf.keras.layers.Dense(100, activation='relu'))\n# model1.add(tf.keras.layers.Dense(100, activation='relu'))\n# model1.add(tf.keras.layers.Dense(100, activation='relu'))\n# model1.add(tf.keras.layers.Dense(32, activation='relu'))\n# model1.add(tf.keras.layers.Dense(16, activation='relu'))\n# model1.add(tf.keras.layers.Dense(1))\n# model1.output_shape\n# model1.compile(loss='mse',optimizer='adam')\n# print(model1.summary())\n# model1.fit(X_train, y_train.ffill(), batch_size=3000, epochs=5)\n\n# y_pred = model1.predict(X_valid)\n# np.corrcoef(y_pred.flatten(),y_valid.ffill())[0,1]","f32f1353":"## weighter correlation","bb81a357":"\u7f3a\u5931\u503c\u4e3b\u8981\u56e0\u4e3a\u5f88\u591a\u5c0f\u5e01\u5728\u65e9\u671f\u6ca1\u6709\u6570\u636e","30200ee4":"## big model","6aa8fbc0":"## simple nn","5197b57a":"## lgbm tune","b73844c6":"# model","2240a07f":"# small model","d0be597c":"# load data & data preprocessing","ea81e756":"### lightgbm","c6108d25":"# submission","4c912019":"### recreating target","b4da0b3e":"### linear "}}