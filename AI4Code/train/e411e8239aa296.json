{"cell_type":{"ce42202b":"code","40937da0":"code","7e19755e":"code","0ca59f35":"code","b22544a1":"code","63a3b0f3":"code","6e9a8126":"code","78bde294":"code","d13f8247":"code","c25bec76":"code","fc11080f":"code","066f7546":"code","d1319755":"markdown","21a79878":"markdown","2cff8f7a":"markdown","75d3217a":"markdown","b912d4d9":"markdown","6b9cae98":"markdown","0afe7ce2":"markdown","1138c806":"markdown"},"source":{"ce42202b":"from tensorflow.keras.datasets import imdb\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt","40937da0":"DICTIONARY_WORD_SIZE = 500 \nTEST_SIZE = 0.2\nSEED = 2021\nONE_REVIEW_WORD_MAXLEN = 200","7e19755e":"(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=DICTIONARY_WORD_SIZE)","0ca59f35":"train_input, val_input, train_target, val_target = train_test_split(\n    train_input, train_target, test_size=TEST_SIZE, random_state=SEED)","b22544a1":"train_seq = pad_sequences(train_input, maxlen=ONE_REVIEW_WORD_MAXLEN)\nval_seq = pad_sequences(val_input, maxlen=ONE_REVIEW_WORD_MAXLEN)","63a3b0f3":"EMBEDDING_INPUT_DIM = DICTIONARY_WORD_SIZE\nEMBEDDING_OUTPUT_DIM = 16\nEMBEDDING_INPUT_LENGTH = ONE_REVIEW_WORD_MAXLEN\n\nLSTM_UNITS= 8\nLSTM_DROP_OUT = 0.3\n\nMODEL_TARGET_NUM = 1\nMODEL_ACTIVATION = 'sigmoid'","6e9a8126":"model = keras.Sequential()\n\nmodel.add(keras.layers.Embedding(EMBEDDING_INPUT_DIM, EMBEDDING_OUTPUT_DIM,\n                                 input_length=EMBEDDING_INPUT_LENGTH))\nmodel.add(keras.layers.LSTM(LSTM_UNITS,dropout=LSTM_DROP_OUT, return_sequences=True))\nmodel.add(keras.layers.LSTM(LSTM_UNITS, dropout=LSTM_DROP_OUT))\nmodel.add(keras.layers.Dense(MODEL_TARGET_NUM, activation=MODEL_ACTIVATION))\n\nmodel.summary()","78bde294":"MODEL_LR = 1e-4\nMODEL_LOSS = 'binary_crossentropy'\nMODEL_METRICS = ['accuracy']\n\nBEST_MODEL_FILE_NAME = 'best-lstm-model.h5'\nES_PATIENCE = 2\n\nEPOCHS = 100\nBATCH_SIZE = 64","d13f8247":"rmsprop = keras.optimizers.RMSprop(learning_rate=MODEL_LR)\nmodel.compile(optimizer=rmsprop, loss=MODEL_LOSS, \n              metrics=MODEL_METRICS)\n\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(BEST_MODEL_FILE_NAME, save_best_only=True)\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=ES_PATIENCE,\n                                                  restore_best_weights=True)\n\nhistory = model.fit(train_seq, train_target, epochs=EPOCHS, batch_size=BATCH_SIZE,\n                    validation_data=(val_seq, val_target),\n                    callbacks=[checkpoint_cb, early_stopping_cb])","c25bec76":"TRAIN_LOSS = 'loss'\nTEST_LOSS = 'val_loss'\nX_LABEL = 'epoch'\nY_LABEL = 'loss'\nLEGEND = ['train', 'val']","fc11080f":"plt.plot(history.history[TRAIN_LOSS])\nplt.plot(history.history[TEST_LOSS])\nplt.xlabel(X_LABEL)\nplt.ylabel(Y_LABEL)\nplt.legend(LEGEND)\nplt.show()","066f7546":"test_seq = pad_sequences(test_input, maxlen=ONE_REVIEW_WORD_MAXLEN)\n\nrnn_model = keras.models.load_model(BEST_MODEL_FILE_NAME)\n\nrnn_model.evaluate(test_seq, test_target)","d1319755":"# preprocess data (sequence padding)","21a79878":"# load data ","2cff8f7a":"# LSTM: Long Short-Term Memory layer","75d3217a":"# define model","b912d4d9":"# plot train history","6b9cae98":"# split data","0afe7ce2":"# evaluate model","1138c806":"# build model"}}