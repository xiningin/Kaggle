{"cell_type":{"b3720ea3":"code","055d2d7e":"code","573052ef":"code","ef9edf8c":"code","00fe2164":"code","c104d057":"code","dd28baa1":"code","e249a15e":"code","ad4c8e28":"code","be08b18b":"code","a86628a2":"code","a8788f9f":"code","2f247d0f":"code","004a3047":"code","0d182197":"code","217b4a36":"code","f92a3482":"code","dc1761ae":"code","c75b4b8e":"code","1d3c1e6d":"code","ca4a5668":"code","e8ccdb16":"code","0a436e27":"code","70d5c336":"code","49f05fde":"code","c832ead7":"code","411fe067":"code","5ca298d7":"code","67eb8901":"code","244d7f1b":"code","0bd3a231":"code","f49a8ae2":"code","a36ff74c":"code","3ee36b92":"code","d273eb6b":"code","44c4a4e1":"code","367d1353":"code","21f821fc":"code","c27d7928":"code","8249ec10":"code","d70705ab":"code","d974bcab":"code","93e9b659":"code","0a6a3ef3":"code","d075786c":"code","9eaaed71":"code","3ed84894":"code","e54ef380":"code","591d0b2e":"code","90424543":"code","141ead12":"code","a9371578":"code","7a62ca6d":"code","54c44ea5":"code","3a460aad":"code","e217aae9":"code","a10f0356":"code","0b6b7852":"code","bf1ebb64":"code","11da97d0":"code","30772919":"code","14c31e07":"code","2ea5ad70":"code","92fe7c9a":"code","14e4f177":"code","08ea5f27":"code","c72c4ddd":"code","5807d29c":"code","3ace33e2":"code","f7f5a1dc":"code","ca32f1d7":"code","95f26b97":"code","2ce25d86":"code","65070a7a":"code","7d12e3d4":"code","98689182":"code","94828d60":"code","848ff592":"code","3d52b96e":"code","9f8d40b8":"code","b1e3ac3b":"code","a5972a5e":"code","405b0566":"code","0c03325d":"code","836e5298":"code","2336c127":"code","593a3487":"code","972d9f27":"code","52954ae1":"code","73e67701":"code","aba4aa24":"code","fd561824":"code","35df454d":"code","b25d88c5":"code","f5784eb1":"code","6f005feb":"code","c030d634":"code","d611dbc1":"code","985bee7a":"code","5fb14747":"code","2d65b9dc":"code","5145fd00":"code","81a9ba56":"code","142bd01e":"code","885e7354":"code","942b1d49":"code","eff31cdc":"code","9c49f772":"code","6a351445":"code","7eb8ffe8":"code","a4d23163":"code","16957d34":"code","7c50ed1a":"code","6a11daa3":"code","5ac1c489":"code","d0d74d89":"code","fd1a8fa1":"code","dffd91d9":"code","246cfe1d":"code","f7155d70":"code","9244f20f":"code","53bde711":"code","68f7d983":"code","2796fd19":"code","405e2420":"code","f7577ada":"code","06dc9df1":"code","9e221731":"code","fad1cecb":"code","99d7dc54":"markdown","ea43d365":"markdown","63209aaa":"markdown","2ae61ebf":"markdown","9cc943eb":"markdown","4b358cd8":"markdown","8d92473d":"markdown","fc1858d2":"markdown","d3c49fae":"markdown","8a6f2876":"markdown","3e15466c":"markdown","509ef9d8":"markdown","682660ce":"markdown","d698d9d8":"markdown","3d64f74b":"markdown","a586e984":"markdown","6684c85d":"markdown","b79141c1":"markdown","8365b670":"markdown","76f5664b":"markdown","5e7ae126":"markdown","ad73af37":"markdown","002c8eee":"markdown","b89a8264":"markdown","58665d66":"markdown","122274cb":"markdown","c3663053":"markdown","0ab20160":"markdown","18ad53f1":"markdown","3e462db4":"markdown","9e6a7324":"markdown","44698dc7":"markdown","f74d35f6":"markdown","4bea1a2f":"markdown","d3730864":"markdown","cbd73b84":"markdown","915a592e":"markdown","6a7709f4":"markdown","7d246582":"markdown","9ffcddc7":"markdown","222cc024":"markdown","1a493e79":"markdown","21bb04a8":"markdown","eee2b5fa":"markdown","00debff8":"markdown","f06b400f":"markdown","57097d5e":"markdown","a06f6a88":"markdown","d1bcc0c7":"markdown","13ee5e92":"markdown","180d780a":"markdown","134d7f93":"markdown","a5b2d0fc":"markdown","945f612c":"markdown","c86c6d62":"markdown","a4101a8a":"markdown","b354c3f9":"markdown","a240e9ef":"markdown","a4237100":"markdown","93b62b2b":"markdown","3ccdf1b9":"markdown","de871c35":"markdown","cd44d5cf":"markdown","63f68b07":"markdown","2b9402f6":"markdown","434f7cf5":"markdown","458777e8":"markdown","dd608ed9":"markdown","b51c8cee":"markdown","dd0022eb":"markdown","f6befa6d":"markdown"},"source":{"b3720ea3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport emoji\n\n#Count vectorizer for N grams\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n# Nltk for tekenize and stopwords\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\n#Loading data\ndf=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndf.head()","055d2d7e":"def missing_value_of_data(data):\n    total=data.isnull().sum().sort_values(ascending=False)\n    percentage=round(total\/data.shape[0]*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n","573052ef":"missing_value_of_data(df)\n#  Reason for 0 percentage value = Round of 1 divided by 27481 will be 0","ef9edf8c":"df=df.dropna()","00fe2164":"def count_values_in_column(data,feature):\n    total=data.loc[:,feature].value_counts(dropna=False)\n    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])","c104d057":"count_values_in_column(df,'sentiment')","dd28baa1":"def unique_values_in_column(data,feature):\n    unique_val=pd.Series(data.loc[:,feature].unique())\n    return pd.concat([unique_val],axis=1,keys=['Unique Values'])\n","e249a15e":"unique_values_in_column(df,'sentiment')","ad4c8e28":"    \ndef duplicated_values_data(data):\n    dup=[]\n    columns=data.columns\n    for i in data.columns:\n        dup.append(sum(data[i].duplicated()))\n    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])","be08b18b":"duplicated_values_data(df)","a86628a2":"df.describe()","a8788f9f":"def find_url(string): \n    text = re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n    return \"\".join(text) # converting return value from list to string","2f247d0f":"sentence=\"I love spending time at https:\/\/www.kaggle.com\/\"\nfind_url(sentence)","004a3047":"df['url']=df['text'].apply(lambda x:find_url(x))","0d182197":"def find_emoji(text):\n    emo_text=emoji.demojize(text)\n    line=re.findall(r'\\:(.*?)\\:',emo_text)\n    return line","217b4a36":"sentence=\"I love \u26bd very much \ud83d\ude01\"\nfind_emoji(sentence)\n\n# Emoji cheat sheet - https:\/\/www.webfx.com\/tools\/emoji-cheat-sheet\/\n# Uniceode for all emoji : https:\/\/unicode.org\/emoji\/charts\/full-emoji-list.html","f92a3482":"df['emoji']=df['text'].apply(lambda x: find_emoji(x))","dc1761ae":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n","c75b4b8e":"sentence=\"Its all about \\U0001F600 face\"\nprint(sentence)\nremove_emoji(sentence)","1d3c1e6d":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","ca4a5668":"def find_email(text):\n    line = re.findall(r'[\\w\\.-]+@[\\w\\.-]+',str(text))\n    return \",\".join(line)","e8ccdb16":"sentence=\"My gmail is abc99@gmail.com\"\nfind_email(sentence)","0a436e27":"df['email']=df['text'].apply(lambda x: find_email(x))","70d5c336":"def find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)","49f05fde":"sentence=\"#Corona is trending now in the world\" \nfind_hash(sentence)","c832ead7":"df['hash']=df['text'].apply(lambda x: find_hash(x))","411fe067":"def find_at(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)","5ca298d7":"sentence=\"@David,can you help me out\"\nfind_at(sentence)","67eb8901":"df['at_mention']=df['text'].apply(lambda x: find_at(x))","244d7f1b":"def find_number(text):\n    line=re.findall(r'[0-9]+',text)\n    return \" \".join(line)","0bd3a231":"sentence=\"2833047 people are affected by corona now\"\nfind_number(sentence)","f49a8ae2":"df['number']=df['text'].apply(lambda x: find_number(x))","a36ff74c":"def find_phone_number(text):\n    line=re.findall(r\"\\b\\d{10}\\b\",text)\n    return \"\".join(line)","3ee36b92":"find_phone_number(\"9998887776 is a phone number of Mark from 210,North Avenue\")","d273eb6b":"df['phone_number']=df['text'].apply(lambda x: find_phone_number(x))","44c4a4e1":"def find_year(text):\n    line=re.findall(r\"\\b(19[40][0-9]|20[0-1][0-9]|2020)\\b\",text)\n    return line","367d1353":"sentence=\"India got independence on 1947.\"\nfind_year(sentence)","21f821fc":"df['year']=df['text'].apply(lambda x: find_year(x))","c27d7928":"def find_nonalp(text):\n    line = re.findall(\"[^A-Za-z0-9 ]\",text)\n    return line","8249ec10":"sentence=\"Twitter has lots of @ and # in posts.(general tweet)\"\nfind_nonalp(sentence)","d70705ab":"df['non_alp']=df['text'].apply(lambda x: find_nonalp(x))","d974bcab":"def find_punct(text):\n    line = re.findall(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*', text)\n    string=\"\".join(line)\n    return list(string)","93e9b659":"example=\"Corona virus have kiled #24506 confirmed cases now.#Corona is un(tolerable)\"\nprint(find_punct(example))","0a6a3ef3":"df['punctuation']=df['text'].apply(lambda x : find_punct(x))","d075786c":"def stop_word_fn(text):\n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(text) \n    non_stop_words = [w for w in word_tokens if not w in stop_words] \n    stop_words= [w for w in word_tokens if w in stop_words] \n    return stop_words","9eaaed71":"example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\nstop_word_fn(example_sent)","3ed84894":"df['stop_words']=df['text'].apply(lambda x : stop_word_fn(x))","e54ef380":"def ngrams_top(corpus,ngram_range,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df","591d0b2e":"ngrams_top(df['text'],(1,1),n=10)","90424543":"ngrams_top(df['text'],(2,2),n=10)","141ead12":"ngrams_top(df['text'],(3,3),n=10)","a9371578":"def rep(text):\n    grp = text.group(0)\n    if len(grp) > 1:\n        return grp[0:1] # can change the value here on repetition\ndef unique_char(rep,sentence):\n    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n    return convert","7a62ca6d":"sentence=\"heyyy this is loong textttt sooon\"\nunique_char(rep,sentence)","54c44ea5":"df['unique_char']=df['text'].apply(lambda x : unique_char(rep,x))","3a460aad":"def find_dollar(text):\n    line=re.findall(r'\\$\\d+(?:\\.\\d+)?',text)\n    return \" \".join(line)\n\n# \\$ - dollar sign followed by\n# \\d+ one or more digits\n# (?:\\.\\d+)? - decimal which is optional","e217aae9":"sentence=\"this shirt costs $20.56\"\nfind_dollar(sentence)","a10f0356":"df['dollar']=df['text'].apply(lambda x : find_dollar(x))","0b6b7852":"#Number greater than 930\ndef num_great(text): \n    line=re.findall(r'9[3-9][0-9]|[1-9]\\d{3,}',text)\n    return \" \".join(line)","bf1ebb64":"sentence=\"It is expected to be more than 935 corona death and 29974 observation cases across 29 states in india\"\nnum_great(sentence)","11da97d0":"#Number greater than 930 (Just part of example)\ndf['num_great']=df['text'].apply(lambda x : num_great(x))","30772919":"# Number less than 930\ndef num_less(text):\n    only_num=[]\n    for i in text.split():\n        line=re.findall(r'^(9[0-2][0-0]|[1-8][0-9][0-9]|[1-9][0-9]|[0-9])$',i) # 5 500\n        only_num.append(line)\n        all_num=[\",\".join(x) for x in only_num if x != []]\n    return \" \".join(all_num)","14c31e07":"sentence=\"There are some countries where less than 920 cases exist with 1100 observations\"\nnum_less(sentence)","2ea5ad70":"#Number greater than 930 (Just part of example)\ndf['num_less']=df['text'].apply(lambda x : num_less(x))","92fe7c9a":"def or_cond(text,key1,key2):\n    line=re.findall(r\"{}|{}\".format(key1,key2), text) \n    return \" \".join(line)","14e4f177":"sentence=\"sad and sorrow displays emotions\"\nor_cond(sentence,'sad','sorrow')","08ea5f27":"# Looks for sorrow or sad word\ndf['sad_or_sorrow']=df['text'].apply(lambda x : or_cond(x,'sad','sorrow'))","c72c4ddd":"def and_cond(text):\n    line=re.findall(r'(?=.*do)(?=.*die).*', text) \n    return \" \".join(line)","5807d29c":"print(\"Both string present:\",and_cond(\"do or die is a motivating phrase\"))\nprint(\"Only one string present :\",and_cond('die word is other side of emotion'))","3ace33e2":"# Looks for do and die both else empty\ndf['do_and_die']=df['text'].apply(lambda x : and_cond(x))","f7f5a1dc":"# mm-dd-yyyy format \ndef find_dates(text):\n    line=re.findall(r'\\b(1[0-2]|0[1-9])\/(3[01]|[12][0-9]|0[1-9])\/([0-9]{4})\\b',text)\n    return line\n","ca32f1d7":"sentence=\"Todays date is 04\/28\/2020 for format mm\/dd\/yyyy, not 28\/04\/2020\"\nfind_dates(sentence)","95f26b97":"df['dates']=df['text'].apply(lambda x : find_dates(x))","2ce25d86":"def only_words(text):\n    line=re.findall(r'\\b[^\\d\\W]+\\b', text)\n    return \" \".join(line)\n","65070a7a":"sentence=\"the world population has grown from 1650 million to 6000 million\"\nonly_words(sentence)","7d12e3d4":"df['only_words']=df['text'].apply(lambda x : only_words(x))","98689182":"def only_numbers(text):\n    line=re.findall(r'\\b\\d+\\b', text)\n    return \" \".join(line)","94828d60":"sentence=\"the world population has grown from 1650 million to 6000 million\"\nonly_numbers(sentence)","848ff592":"df['only_num']=df['text'].apply(lambda x : only_numbers(x))","3d52b96e":"# Extracting word with boundary\ndef boundary(text):\n    line=re.findall(r'\\bneutral\\b', text)\n    return \" \".join(line)","9f8d40b8":"sentence=\"Most tweets are neutral in twitter\"\nboundary(sentence)","b1e3ac3b":"df['bound']=df['text'].apply(lambda x : boundary(x))","a5972a5e":"def search_string(text,key):\n    return bool(re.search(r''+key+'', text))","405b0566":"sentence=\"Happy Mothers day to all Moms\"\nsearch_string(sentence,'day')","0c03325d":"df['search_day']=df['text'].apply(lambda x : search_string(x,'day'))","836e5298":"def pick_only_key_sentence(text,keyword):\n    line=re.findall(r'([^.]*'+keyword+'[^.]*)', text)\n    return line","2336c127":"sentence=\"People are fighting with covid these days.Economy has fallen down.How will we survice covid\"\npick_only_key_sentence(sentence,'covid')","593a3487":"df['pick_senence']=df['text'].apply(lambda x : pick_only_key_sentence(x,'covid'))","972d9f27":"def pick_unique_sentence(text):\n    line=re.findall(r'(?sm)(^[^\\r\\n]+$)(?!.*^\\1$)', text)\n    return line","52954ae1":"sentence=\"I thank doctors\\nDoctors are working very hard in this pandemic situation\\nI thank doctors\"\npick_unique_sentence(sentence)","73e67701":"df['pick_unique']=df['text'].apply(lambda x : pick_unique_sentence(x))","aba4aa24":"def find_capital(text):\n    line=re.findall(r'\\b[A-Z]\\w+', text)\n    return line","fd561824":"sentence=\"World is affected by corona crisis.No one other than God can save us from it\"\nfind_capital(sentence)","35df454d":"df['caps_word']=df['text'].apply(lambda x : find_capital(x))","b25d88c5":"df['text_length']=df['text'].str.split().map(lambda x: len(x))\ndf[['text','text_length']].sample(3)","f5784eb1":"df['char_length']=df['text'].str.len()\ndf[['text','char_length']].sample(3)","6f005feb":"def find_id(text):\n    line=re.findall(r'\\bIND(\\d+)', text)\n    return line","c030d634":"sentence=\"My company id is IND50120.And I work under Asia region\"\nfind_id(sentence)","d611dbc1":"df['get_id']=df['text'].apply(lambda x : find_id(x))","985bee7a":"my_string_rows = df[df['text'].str.contains(\"good\")]\nmy_string_rows[['text']].sample(3)","5fb14747":"!pip install webcolors\nimport webcolors","2d65b9dc":"def find_color(string): \n    text = re.findall('\\#(?:[0-9a-fA-F]{3}){1,2}',string)\n    conv_name=[]\n    for i in text:\n        conv_name.append(webcolors.hex_to_name(i))\n    return conv_name","5145fd00":"sentence=\"Find the color of #00FF00 and #FF4500\"\nfind_color(sentence)","81a9ba56":"def remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text","142bd01e":"sentence=\"Markdown sentences can use <br> for breaks and <i><\/i> for italics\"\nremove_tag(sentence)","885e7354":"def ip_add(string):\n    text=re.findall('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}',string)\n    return text","942b1d49":"sentence=\"An example of ip address is 125.16.100.1\"\nip_add(sentence)","eff31cdc":"def mac_add(string):\n    text=re.findall('(?:[0-9a-fA-F]:?){12}',string)\n    return text\n#https:\/\/stackoverflow.com\/questions\/26891833\/python-regex-extract-mac-addresses-from-string\/26892371","9c49f772":"sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc .Other details will be mentioned\"\nmac_add(sentence)","6a351445":"def subword(string,sub): \n    text=re.findall(sub,string)\n    return len(text)","7eb8ffe8":"sentence = 'Fundamentalism and constructivism are important skills'\nsubword(sentence,'ism') # change subword and try for others","a4d23163":"def lat_lon(string):\n    text=re.findall(r'^[-+]?([1-8]?\\d(\\.\\d+)?|90(\\.0+)?),\\s*[-+]?(180(\\.0+)?|((1[0-7]\\d)|([1-9]?\\d))(\\.\\d+)?)$',string)\n    if text!=[]:\n        print(\"[{}] is valid latitude & longitude\".format(string))\n    else:\n        print(\"[{}] is not a valid latitude & longitude\".format(string))","16957d34":"lat_lon('28.6466772,76.8130649')\nlat_lon('2324.3244,3423.432423')","7c50ed1a":"def valid_pan(string):\n    text=re.findall(r'^([A-Z]){5}([0-9]){4}([A-Z]){1}$',string)\n    if text!=[]:\n        print(\"{} is valid PAN number\".format(string))\n    else:\n        print(\"{} is not a valid PAN number\".format(string))","6a11daa3":"valid_pan(\"ABCSD0123K\")\nvalid_pan(\"LEcGD012eg\")","5ac1c489":"def valid_phone_code(string):\n    text=re.findall(r'^([0-9]){2}(-)([0-9]){2}(-)(\\d+)$',string)\n    if text!=[]:\n        print(\"{} is valid Indian Phone number wth country code\".format(string))\n    else:\n        print(\"{} is not a valid Indian Phone number wth country code\".format(string))","d0d74d89":"valid_phone_code('91-44-23413627')\nvalid_phone_code('291-4456-23413627')","fd1a8fa1":"def pos_look_ahead(string,A,B):\n    pattern = re.compile(''+A+'(?=\\s'+B+')')\n    match = pattern.search(string)\n    print(\"position:{} Matched word:{}\".format(match.span(),match.group()))","dffd91d9":"pos_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\")","246cfe1d":"def neg_look_ahead(string,A,B):\n    pattern = re.compile(''+A+'(?!\\s'+B+')')\n    match = pattern.search(string)\n    print(\"position:{} Matched word:{}\".format(match.span(),match.group()))","f7155d70":"neg_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\")","9244f20f":"def pos_look_behind(string,A,B):\n    pattern = re.compile(\"(?<=\"+A+\"\\s)\"+B+\"\")\n    match = pattern.search(string)\n    print(\"position:{} Matched word: {}\".format(match.span(),match.group()))","53bde711":"pos_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\")\n# the word \"nlp\" that do come after \"love\"","68f7d983":"def neg_look_behind(string,A,B):\n    pattern = re.compile(\"(?<!\"+A+\"\\s)\"+B+\"\")\n    match = pattern.search(string)\n    print(\"position:{} Matched word: {}\".format(match.span(),match.group()))","2796fd19":"neg_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\")\n# the word \"nlp\" that doesnt come after \"love\"","405e2420":"def find_domain(string): \n    text = re.findall(r'\\b(\\w+[.]\\w+)',string)\n    return text","f7577ada":"sentence=\"WHO provides valid information about covid in their site who.int . UNICEF supports disadvantageous childrens. know more in unicef.org\"\nfind_domain(sentence)","06dc9df1":"def find_percent(string): \n    text = re.findall(r'\\b(100|[1-9][0-9]|[0-9])\\%',string)\n    return text","9e221731":"sentence=\"COVID recovery rate has been increased to 76%.And death rate drops to 2% from 3%\"\nfind_percent(sentence)","fad1cecb":"df.sample(5)\n# We will see empty values too as most of text may not have related feature.You can filter and check.","99d7dc54":"If you want to change match repetitive characters to n numbers,**chage the return line in the *rep function* to grp[0:n]**.","ea43d365":"<font size=\"+2\" color=\"indigo\"><b>4.3 Email<\/b><\/font><br><a id=\"4.3\"><\/a>\n\nExtract email from text","63209aaa":"<font size=\"+2\" color=\"indigo\"><b>4.12 N-grams<\/b><\/font><br><a id=\"4.12\"><\/a>","2ae61ebf":"<font size=\"+2\" color=\"indigo\"><b>4.8 Year<\/b><\/font><br><a id=\"4.8\"><\/a>\n\nExtract year from 1940 till 2020","9cc943eb":"<a id=\"4.23\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.23 Search<\/b><\/font><br><br>\nIs the key word present in the sentence?","4b358cd8":"The aim of this kernel is to provide helper function for basic text processing.This functions can aid you to understand the data much better and perform EDA.\nMy major focus will be on apply regex on text but still i have mentioned few basic starter codes on the 3rd part of kernel.Rest all sections will deal with text preprocessing.The whole kernel can be useful for everyone especially **beginners**.\n\n<font size=\"+1\"><i>Readers,I am making you lazy to code but at same time I am helping you out.Do utilize this kernel for any of your text oriented competitions.<\/i><\/font><br><br>\n    \n<font size=\"+1\" color=chocolate ><b>Please appreciate me through your Upvote.<\/b><\/font>","8d92473d":"<font size=\"+2\" color=\"indigo\"><b>4.19 Dates<\/b><\/font><br><a id=\"4.19\"><\/a>","fc1858d2":"<font size=\"+2\" color=\"indigo\"><b>4.16 Number Lesser<\/b><\/font><br><a id=\"4.16\"><\/a>","d3c49fae":"<a id=\"4.25\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.25 Duplicate Sentence<\/b><\/font><br><br>\nMost webscrapped data contains duplicated sentence.This function could retrieve unique ones.","8a6f2876":"<a id=\"4.35\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.35 Subword<\/b><\/font><br><br>\nExtract number of subwords from sentences and words.","3e15466c":"<font size=\"+2\" color=\"indigo\"><b>4.11 Stopwords<\/b><\/font><br><a id=\"4.11\"><\/a>","509ef9d8":"<font size=\"+2\" color=\"indigo\"><b>3.5 Stat<\/b><\/font><br><a id=\"3.5\"><\/a>","682660ce":"<a id=\"4.33\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.33 IP Address<\/b><\/font><br><br>\nExtract IP address from text.","d698d9d8":"Since these kind of basic python functions are pretty much well known to all python users.We dont need much focus here.Still I will elaborate this section in upcoming versions.","3d64f74b":"<a id=\"4.42\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.42 Negative Look Behind<\/b><\/font><br><br>\nPositive look behind will succeed if passed non-consuming expression **does not match** against the forthcoming input.<br>\nThe syntax is \"A(?<!=B)\" where \"A\"is actual expression and \"B\" is non-consuming expression.","a586e984":"<a id=\"4.24\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.24 Pick Sentence<\/b><\/font><br><br>\nIf we want to get all sentence with particular keyword.We can use below function","6684c85d":"<font size=\"+2\" color=\"indigo\"><b>4.10 Punctuations<\/b><\/font><br><a id=\"4.10\"><\/a>\n\nRetrieve punctuations from sentence.","b79141c1":"Try more hex codes:https:\/\/www.rapidtables.com\/web\/css\/css-color.html","8365b670":"<font size=\"+2\" color=\"indigo\"><b>4.5 Mention<\/b><\/font><br><a id=\"4.5\"><\/a>\n\n@ - Used to mention someone in tweets","76f5664b":"## *Data with new Features*","5e7ae126":"<a id=\"4.27\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.27 Length of words<\/b><\/font><br><br>\nNo regex but added one liner to identify length of words in a sentence","ad73af37":"**Version 4**  :  First Run <br>\n**Version 6**  :  Added 9 new regex function required for text processing.<br>\n**Version 7**  :  Added few functions and modified scripts.<br>\n**Version 8**  :  Updated kernel with few more functions and modified scripts.<br>\n**Version 9**  :  Modified script <br>\n**Version 10** :  Added description and modified script in 4.13 <br>\n**Version 11** :  Added few more helpers<br>\n**Version 12** :  Few modifications and added few more helpers<br>\n**Version 13** :  Added two more functions.<br>\n**Version 14** :  Small modification<br>\n**Version 15** :  Added more function<br>\n**Version 16** :  Added tag function<br>\n**Version 17** :  Fixed bug from V16<br>\n**Version 18** :  Added two more functions- ip and mac address<br>\n**Version 19** :  Added subword and latitude & longitude<br>\n**Version 20** :  Added PAN validation <br>\n**Version 21** :  Added Phone Number Country code<br>\n**Version 22** :  Added Positive and Negative look ahead<br>\n**Version 23** :  Added Positive and Negative look behind<br>\n**Version 24** :  Bug fix from V23<br>\n**Version 25** :  Added Domain<br>\n**Version 26** :  (Current) Added Percentage<br>\n**Version 27** :  *Loading...*","002c8eee":"<font size=\"+2\" color=\"indigo\"><b>4.4 Hash<\/b><\/font><br><a id=\"4.4\"><\/a>\n\nThis value is especially to denote trends in twitter.","b89a8264":"<font size=\"+2\" color=\"indigo\"><b>4.7 Phone Number<\/b><\/font><br><a id=\"4.7\"><\/a>\n\nIndian Mobile numbers have ten digit.I will write that pattern below","58665d66":"<a id=\"4.21\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.21 Only Numbers<\/b><\/font>","122274cb":"<font size=\"+2\" color=\"indigo\"><b>4.20 Only Words<\/b><\/font><br><a id=\"4.20\"><\/a>","c3663053":"<a id=\"4.22\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.22 Boundaries<\/b><\/font><br><br>\nPicking up the words with boundaries","0ab20160":"<font size=\"+2\" color=\"indigo\"><b>4.18 AND<\/b><\/font><br><a id=\"4.18\"><\/a>","18ad53f1":"<a id=\"4.44\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.44 Percentage<\/b><\/font><br><br>","3e462db4":"<a id=\"4.30\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.30 Specific String Rows<\/b><\/font><br><br>\nQuering for specific string can also be done by directly applying *\"str.contains(\"XXXX\")\"* to a series\/column of a dataframe","9e6a7324":"**Version 27** : <font size=\"+3\" color=\"red\"><b><i>Loading...<\/i><\/b><\/font><br><br>\n<a href=\"#top\" class=\"btn btn-success btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP<\/a>","44698dc7":"<font size=\"+3\" color=\"blue\"><b>2. Data<\/b><\/font><br><a id=\"2\"><\/a>\n\nI will be using data from [Twitter Sentiment Analysis](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/data) competiton","f74d35f6":"<font size=\"+2\" color=\"chocolate\"><b>My Other Kernels<\/b><\/font><br>\n\nClick on the button to view kernels...\n\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/kaggle-notebooks-eda\" class=\"btn btn-primary\" style=\"color:white;\">Kaggle Notebooks EDA<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/kaggle-users-competitions-stats-eda\/\" class=\"btn btn-primary\" style=\"color:white;\">Kaggle Users & Competition EDA<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/covid19-tweets-interactive-eda\" class=\"btn btn-primary\" style=\"color:white;\">#COVID Tweets<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/self-evaluation-kaggle-profiler\" class=\"btn btn-primary\" style=\"color:white;\">Kaggle Profiler<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/cheatsheet-100-plotly-part-1-basic\" class=\"btn btn-primary\" style=\"color:white;\">100+ Plotly Basic<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/cheatsheet-100-plotly-part-2-advanced\" class=\"btn btn-primary\" style=\"color:white;\">100+ Plotly Advanced<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/don-t-shoot\/\" class=\"btn btn-primary\" style=\"color:white;\">Don't Shoot<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/become-grandmaster\/\" class=\"btn btn-primary\" style=\"color:white;\">Become GrandMaster<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/cheatsheet-date-helpers\/\" class=\"btn btn-primary\" style=\"color:white;\">Cheatsheet Date Helpers<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/raenish\/tweet-sentiment-insight-eda\/\" class=\"btn btn-primary\" style=\"color:white;\">Tweet Sentiment Extraction<\/a>\n<br>\n<br>\n### If these kernels impress you,give them an <font size=\"+2\" color=\"red\"><b>Upvote<\/b><\/font>.<br>\n\n","4bea1a2f":"<a id=\"4.39\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.39 Positive Look Ahead<\/b><\/font><br><br>\nPositive look ahead will succeed if passed non-consuming expression **does match** against the forthcoming input.<br>\nThe syntax is <code>A(?=B)<\/code> where <code>A<\/code> is actual expression and <code>B<\/code> is non-consuming expression.\n\nScripts utlized from [here](https:\/\/github.com\/nikhilkumarsingh\/RegEx-In-Python\/blob\/master\/16.%20Look%20ahead.ipynb)","d3730864":"<font size=\"+2\" color=\"indigo\"><b>3.2 Count Values<\/b><\/font><br><a id=\"3.2\"><\/a>","cbd73b84":"<font size=\"+2\" color=\"indigo\"><b>3.4 Duplicate Values<\/b><\/font><br><a id=\"3.4\"><\/a>","915a592e":"### Remove Emoji from text","6a7709f4":"<a id=\"4.40\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.40 Negative Look Ahead<\/b><\/font><br><br>\nNegative look ahead will succeed if passed non-consuming expression **does not match** against the forthcoming input.<br>\nThe syntax is <code>A(?!B)<\/code> where <code>A<\/code> is actual expression and <code>B<\/code> is non-consuming expression.","7d246582":"<a id=\"4.26\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.26 Caps Words<\/b><\/font><br><br>\nExtract words starting with capital letter.Some words like names,place or universal object are usually mentioned in a text starting with CAPS.","9ffcddc7":"<font size=\"+2\" color=\"indigo\"><b>3.3 Unique Values<\/b><\/font><br><a id=\"3.3\"><\/a>","222cc024":"<font size=\"+2\" color=\"indigo\"><b>4.17 OR<\/b><\/font><br><a id=\"4.17\"><\/a>","1a493e79":"<font size=\"+2\" color=\"brown\"><i><b><center>\"With hard work, you can get fire out of a stone.\"<\/center><\/b><\/i><\/font>","21bb04a8":"<a id=\"4.37\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.37 PAN<\/b><\/font><br><br>\n\nPAN Validation:\n\n[First 5 letters in CAPS+4 didgits+Last letter in CAPS]","eee2b5fa":"We will drop NA values","00debff8":"<font size=\"+2\" color=\"indigo\"><b>4.14 Dollar<\/b><\/font><br><a id=\"4.14\"><\/a>","f06b400f":"<font size=\"+3\" color=\"blue\"><b>4. Regex Helpers<\/b><\/font><br> \n<a id=\"4\"><\/a>","57097d5e":"<font size=\"+3\" color=\"blue\"><b>3. Basic Data Explorers<\/b><\/font><br> <a id=\"3\"><\/a>","a06f6a88":"### Find and convert emoji to text","d1bcc0c7":"<a id=\"4.34\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.34 Mac Address<\/b><\/font><br><br>\nExtract Mac address from text.","13ee5e92":"<a id=\"4.36\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.36 Latitude & Longitude<\/b><\/font><br><br>\nExtract number of subwords from sentences and words.","180d780a":"<font size=\"+2\" color=\"indigo\"><b>3.1 Missing values<\/b><\/font><br><a id=\"3.1\"><\/a>","134d7f93":"<a id=\"4.29\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.29 Get ID<\/b><\/font><br><br>\nMost data has IDs in it with some prefix.So if we want to pick only numbers in ID leaving the prefix out,we can apply below function.","a5b2d0fc":"<a id=\"4.28\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.28 Length of characters<\/b><\/font><br><br>\nNo regex but added one liner to identify length of characters in a sentence including space","945f612c":"<a id=\"4.38\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.38 Phone number Code<\/b><\/font><br><br>\n**Format**: [Country code]-[Local Area Code]-[Number]  ","c86c6d62":"<font size=\"+2\" color=\"indigo\"><b>4.13 Repetitive Character<\/b><\/font><br><a id=\"4.13\"><\/a>","a4101a8a":"<font size=\"+2\" color=\"chocolate\"><b>Reference<\/b><\/font><br>\n* https:\/\/www.guru99.com\/python-regular-expressions-complete-tutorial.html\n* https:\/\/docs.python.org\/3.4\/library\/re.html\n* https:\/\/www3.ntu.edu.sg\/home\/ehchua\/programming\/howto\/Regexe.html#zz-1.9\n* https:\/\/www.debuggex.com\/cheatsheet\/regex\/python\n* https:\/\/blog.finxter.com\/python-regex-and-operator-tutorial-video\/\n* https:\/\/www.oreilly.com\/library\/view\/regular-expressions-cookbook\/9781449327453\/ch04s04.html\n* https:\/\/www.webfx.com\/tools\/emoji-cheat-sheet\/\n* https:\/\/github.com\/nikhilkumarsingh\/RegEx-In-Python\/blob\/master\/17.%20Look%20behind.ipynb\n* https:\/\/github.com\/nikhilkumarsingh\/RegEx-In-Python\/blob\/master\/16.%20Look%20ahead.ipynb","b354c3f9":"<font size=\"+2\" color=\"indigo\"><b>4.6 Number<\/b><\/font><br><a id=\"4.6\"><\/a>\n\nPick only number from sentence","a240e9ef":"<font size=\"+3\" color=purple ><b> <center><u>Text Helper Functions<\/u><\/center><\/b><\/font>","a4237100":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content<\/h3>\n\n* [1. Objective](#1)   \n* [2. Data](#2)\n* [3. Basic Data Explorers](#3)\n    - [3.1 Missing values](#3.1)\n    - [3.2 Count values](#3.2)\n    - [3.3 Unique values](#3.3)\n    - [3.4 Duplicate values](#3.4)\n    - [3.5 Stat](#3.5)\n* [4. Regex Helpers](#4)\n    - [4.1 URL](#4.1)\n    - [4.2 Emoticons](#4.2)\n    - [4.3 Email](#4.3)\n    - [4.4 Hash](#4.4)\n    - [4.5 Mention](#4.5)\n    - [4.6 Number](#4.6)\n    - [4.7 Phone Number](#4.7)\n    - [4.8 Year](#4.8)\n    - [4.9 Non Alphanumeric](#4.9)\n    - [4.10 Punctuations](#4.10)\n    - [4.11 Stopwords](#4.11)\n    - [4.12 N-grams](#4.12)\n    - [4.13 Repetitive Character](#4.13)\n    - [4.14 Dollar](#4.14)\n    - [4.15 Number-Greater](#4.15)\n    - [4.16 Number- Lesser](#4.16)\n    - [4.17 OR](#4.17)\n    - [4.18 AND](#4.18)\n    - [4.19 Dates](#4.19)\n    - [4.20 Only Words](#4.20)\n    - [4.21 Only Numbers](#4.21)\n    - [4.22 Boundaries](#4.22)\n    - [4.23 Search](#4.23)\n    - [4.24 Pick Sentence](#4.24)\n    - [4.25 Duplicate Sentence](#4.25)\n    - [4.26 Caps Words](#4.26)\n    - [4.27 Length of Words](#4.27)\n    - [4.28 Length of Characters](#4.28)\n    - [4.29 Get ID](#4.29)\n    - [4.30 Specific String Rows](#4.30)\n    - [4.31 Hex code to Color](#4.31)\n    - [4.32 Tags](#4.32)\n    - [4.33 IP Address](#4.33)\n    - [4.34 Mac Address](#4.34)\n    - [4.35 Subword](#4.35)\n    - [4.36 Latitude & Longitude](#4.36)\n    - [4.37 PAN](#4.37)\n    - [4.38 Phone Number Country Code](#4.38)\n    - [4.39 Positive Look Ahead](#4.39)\n    - [4.40 Negative Look Ahead](#4.40)\n    - [4.41 Positive Look Behind](#4.41)\n    - [4.42 Negative Look Behind](#4.42)\n    - [4.43 Domain](#4.43)\n    - [4.44 Percentage](#4.44)\n    \n* [5. End Notes](#5)\n    ","93b62b2b":"<a id=\"4.31\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.31 Hex code to Color<\/b><\/font><br><br>\nConverting hex color codes to color names.We will install and import webcolors. (only for CSS3 colors)","3ccdf1b9":"<font size=\"+2\" color=\"indigo\"><b>4.9 Non Alphanumeric<\/b><\/font><br><a id=\"4.9\"><\/a>","de871c35":"<font size=\"+2\" color=\"indigo\"><b>4.2 Emoticons<\/b><\/font><br><a id=\"4.2\"><\/a>","cd44d5cf":"<a id=\"4.41\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.41 Positive Look Behind<\/b><\/font><br><br>\nPositive look behind will succeed if passed non-consuming expression **does match** against the forthcoming input.<br>\nThe syntax is <code>A(?<=B)<\/code> where <code>A<\/code> is actual expression and <code>B<\/code> is non-consuming expression.\n    \nScripts utilized from [here](https:\/\/github.com\/nikhilkumarsingh\/RegEx-In-Python\/blob\/master\/17.%20Look%20behind.ipynb)","63f68b07":"<font size=\"+2\" color=\"indigo\"><b>4.1 URL<\/b><\/font><br><a id=\"4.1\"><\/a>","2b9402f6":"<font size=\"+2\" color=\"indigo\"><b>4.15 Number-Greater<\/b><\/font><br><a id=\"4.15\"><\/a>","434f7cf5":"Major RE functions\n\n* **re.findall**   - Module is used to search for \u201call\u201d occurrences that match a given pattern.\n* **re.sub**       - Substitute the matched RE patter with given text\n* **re.match**     - The match function is used to match the RE pattern to string with optional flags\n* **re.search**    - This method takes a regular expression pattern and a string and searches for that pattern with the string.\n\n\nWe will be mostly using re.findall to detect patterns.","458777e8":"No worries.This is not the end of kernel.**I have updated this kernel with few more functions (Now version 26 is live)**.Going forward i will add more helper functions in upcoming versions.I would like to get appreciation from you with an \ud83d\udc4d .Please <font size=\"+1\" color=\"red\"><b>Upvote<\/b><\/font> and keep it in your favourite list.\n\nThanks for your patience.\n\n\n*Happy Kaggling!!!*.\n","dd608ed9":"<font size=\"+3\" color=\"blue\"><b>1. Objective<\/b><\/font><br><a id=\"1\"><\/a>","b51c8cee":"<a id=\"4.43\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.43 Domain<\/b><\/font><br><br>","dd0022eb":"<font size=\"+3\" color=\"blue\"><b>5. End Notes<\/b><\/font><br> <a id=\"5\"><\/a>","f6befa6d":"<a id=\"4.32\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.32 Tags<\/b><\/font><br><br>\nMost of web scrapped data contains html tags.It can be removed from below re script"}}