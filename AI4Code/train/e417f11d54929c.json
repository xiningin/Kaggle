{"cell_type":{"b9dadea6":"code","9ef7d657":"code","c19c6e8c":"code","dfdb8be0":"code","1088d625":"code","a14afebf":"code","00dc6740":"code","bf1e253d":"code","9c9836e9":"code","90db96ef":"code","1a872bcf":"code","03be3d2c":"code","bcdb6ecb":"code","4b49483b":"code","cc525f42":"code","1544429a":"code","1fcb13b5":"code","db778688":"code","53875fab":"code","0fd9a30a":"code","27518c59":"code","b1b6d975":"code","2555768d":"code","643bf108":"code","17da8bfc":"code","04ed9027":"code","bbda8943":"code","d3f223e5":"code","6e639930":"code","5baf3645":"code","61d93d94":"code","b06dcfe2":"code","3a711960":"code","05dacb70":"code","30801490":"code","81c8891c":"code","e9ff0ac9":"code","752b7c2c":"code","e9fb6825":"code","b04cf97b":"code","ad64c3d7":"code","50f78535":"markdown","d4ce1c20":"markdown"},"source":{"b9dadea6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ef7d657":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","c19c6e8c":"df = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\npd.set_option('display.max_rows', 10)\npd.set_option('display.max_columns', len(df.columns))","dfdb8be0":"df.head()","1088d625":"df.shape","a14afebf":"df.info()","00dc6740":"df.describe()","bf1e253d":"for i in df.columns:\n    print('{}:- {}\\n{}\\n\\n'.format(i ,df[i].nunique(), df[i].unique()))","9c9836e9":"sns.set(style='whitegrid')","90db96ef":"def distribution(df, cols=4, display_num=len(df.columns)-1,figsize=(12,10)):\n    rows = (display_num - 1) \/\/ cols + 1\n    \n    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n    for i in range(display_num):\n        row_i = i \/\/ cols\n        col_i = i % cols\n        title = df.columns[i]\n        sns.histplot(x=df.columns[i], data=df, alpha=0.8, ax=axes[row_i, col_i])\n        axes[row_i, col_i].set_title(title)\n    \n    fig.tight_layout()\n    ","1a872bcf":"distribution(df)","03be3d2c":"class Outliers(object):\n    def __init__(self, df, col):\n        self.df = df\n        self.col = col\n        \n        self.mean = df[col].mean()\n        self.median = df[col].median()\n        self.min = df[col].min()\n        self.max = df[col].max()\n        self.std = df[col].std()\n        self.quantile_25 = self.df[self.col].quantile(0.25)\n        self.quantile_75 = self.df[self.col].quantile(0.75)\n        \n    def info(self):\n        print('Mean: {}'.format(self.mean))\n        print('Median: {}'.format(self.median))\n        print('Standard Deviation: {}'.format(self.std))\n        print('Minimun value: {}'.format(self.min))\n        print('Maximum value: {}'.format(self.max))\n        print('25th quantile: {}'.format(self.quantile_25))\n        print('75th quantile: {}'.format(self.quantile_75))\n        \n        des = '*' * 20\n        return des\n\nclass IQR(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col)\n        \n    def iqr_calc(self):\n        \n        IQR = self.quantile_75 - self.quantile_25\n        \n        lower_bound = self.quantile_25 - (1.5 * IQR)\n        upper_bound = self.quantile_75 + (1.5 * IQR)\n        \n        return lower_bound, upper_bound\n    \n    def iqr_outliers(self):\n        \n        lower_bound, upper_bound = self.iqr_calc()\n        \n        return self.df.loc[(self.df[self.col] < lower_bound) | (self.df[self.col] > upper_bound), self.col]\n    \n    def iqr_remove(self):\n        \n        lower_bound, upper_bound = self.iqr_calc()\n        \n        return self.df.loc[(self.df[self.col] > lower_bound) & (self.df[self.col] < upper_bound)]\n    \nclass Z_score(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col)\n        \n    def z_score_outliers(self):\n        outlier = []\n        for i in self.df[self.col]:\n            z = (i - self.mean) \/ self.std\n            if abs(z) > 3:\n                outlier.append(i)\n                \n        return outlier\n    \n    def z_score_remove(self):\n        \n        df_copy = self.df.copy()\n        for i in self.z_score_outliers():\n            df_copy = df_copy.loc[df_copy[self.col] != i]\n            \n        return df_copy\n    \nclass StandardDeviation(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col)\n        \n    def std_calc(self):\n        \n        lower_std = self.mean - (3 * self.std)\n        upper_std = self.mean + (3 * self.std)\n        \n        return lower_std, upper_std\n    \n    def std_outliers(self):\n        \n        lower_std, upper_std = self.std_calc()\n        return self.df.loc[(self.df[self.col] < lower_std) | (self.df[self.col] > upper_std), self.col]\n\n    def std_remove(self):\n        \n        lower_std, upper_std = self.std_calc()\n        return self.df.loc[(self.df[self.col] > lower_std) & (self.df[self.col] < upper_std)]\n","bcdb6ecb":"columns = ['chol', 'trtbps', 'thalachh', 'oldpeak']\n\nfor i in columns:\n    out = Outliers(df, i)\n    iqr = IQR(df, i)\n    z_score = Z_score(df, i)\n    std = StandardDeviation(df, i)\n    \n    print('Outliers:- {}\\n'.format(i))\n    \n    print('Info:- {}\\n')\n    out.info()\n    print('*' * 40)\n    print('\\n')\n          \n    print('IQR Outliers:- \\n{}\\n'.format(iqr.iqr_outliers()))\n    print('Shape of df if removed outliers with IQR:- {}'.format(iqr.iqr_remove().shape))\n    print('*' * 20)\n    \n    print('Z-score Outliers:- \\n{}\\n'.format(z_score.z_score_outliers())) \n    print('Shape of df if removed outliers with Z-score:- {}'.format(z_score.z_score_remove().shape))\n    print('*' * 20)\n          \n    print('StandardDeviation Outliers:- \\n{}\\n'.format(std.std_outliers()))\n    print('Shape of df if removed outliers with StandardDeviation:- {}'.format(std.std_remove().shape))\n          \n    print('\\n', '*'*100, '\\n', '*'*100, '\\n')","4b49483b":"# Treating Outliers of chol\nz_score_chol = Z_score(df, 'chol')\ndf = z_score_chol.z_score_remove()\ndf.shape","cc525f42":"# Treating Outliers of trtbps\nstd_trtbps = StandardDeviation(df, 'trtbps')\ndf = std_trtbps.std_remove()\ndf.shape","1544429a":"# Treating Outliers of thalachh\niqr_thalachh = IQR(df, 'thalachh')\ndf = iqr_thalachh.iqr_remove()\ndf.shape","1fcb13b5":"# Treating Outliers of oldpeak\nz_score_oldpeak = Z_score(df, 'oldpeak')\ndf = z_score_oldpeak.z_score_remove()\ndf.shape","db778688":"# Distributions of columns after treating outliers\ndistribution(df)","53875fab":"sex_out = df.groupby(['sex', 'output'])[['output']].count()\nsex_atk = df.groupby('sex')[['output']].mean()\n\nfig, axes = plt.subplots(2,1, figsize=(8,10))\n\nindx = np.arange(len(sex_out.unstack().output.index))\nwidth = 0.25\n\naxes[0].bar(indx - width\/2, sex_out.unstack().output[0].values,\n       width = width, alpha = 0.7, label = 'less chance of heart attack')\naxes[0].bar(indx + width\/2, sex_out.unstack().output[1].values,\n       width=width, alpha=0.7, label= 'more chance of heart attack')\n\naxes[0].set_title('Frequency of people with chance of heart attack \\n(gender wise)\\n', fontsize=25)\naxes[0].set_xlabel('Sex', fontsize=20)\naxes[0].set_ylabel('Frequency', fontsize=20)\n\naxes[0].set_xticks([0,1])\naxes[0].set_xticklabels(('Female', 'Male'))\naxes[0].legend()\n\n\n\naxes[1].bar(sex_atk.index.map({0: 'Female', 1: 'Male'}), sex_atk.output, color = 'red', \n        alpha = 0.5, edgecolor = 'black', width = 0.5)\n\naxes[1].set_title('Chance of getting heart attack of each gender type\\n', fontsize = 25)\naxes[1].set_xlabel('\\nChance of getting heart attack (in %)', fontsize = 20)\naxes[1].set_ylabel('Genders\\n', fontsize = 20)\n\nfig.tight_layout()\nplt.show()\n","0fd9a30a":"sex_hrt = df.groupby('sex')[['output']].mean()\nsex_info = df.groupby('sex')[['chol', 'trtbps', 'thalachh', 'oldpeak']].aggregate(np.mean)\nage_dis = pd.qcut(df.age, 5)\nage_hrt = df.pivot_table('output', index=age_dis, columns='sex')\n\nfig, axes = plt.subplots(1, 3, figsize=(15,9))\n\naxes[0].pie(sex_hrt.output, labels=sex_hrt.index.map({0: 'Female', 1: 'Male'}),\n           wedgeprops={'edgecolor': 'black'},\n            shadow=True, textprops={'fontsize': 15}, autopct='%1.2f%%')\naxes[0].set_title('Chance of getting \\n(if compared according to genders)', fontsize=20)\n\nindx_1 = np.arange(len(sex_info.columns))\nwidth_1 = 0.2\n\naxes[1].bar(indx_1 - width_1\/2, sex_info.loc[0,:], width=width_1, alpha=0.5,\n           color='pink', label='Female', edgecolor='black')\naxes[1].bar(indx_1 + width_1\/2, sex_info.loc[1,:], width=width_1, alpha=0.5,\n           color='blue', label='Male', edgecolor='black')\n\naxes[1].set_title('Comparing genders on avg. \\nvalues of differenta categories\\n', fontsize=20)\naxes[1].set_xlabel('\\nCategories', fontsize=15)\naxes[1].set_ylabel('Average values', fontsize=15)\naxes[1].set_yscale('log')\naxes[1].set_xticks([0,1, 2, 3])\naxes[1].set_xticklabels(('chol', 'trtbps', 'thalachh', 'oldpeak'))\naxes[1].legend()\n\nindx_2 = np.arange(len(age_hrt.index))\nwidth_2 = 0.4\n\naxes[2].bar(indx_2 - width_2\/2, age_hrt[0], width=width_2, alpha=0.7, label='Female')\naxes[2].bar(indx_2 + width_2\/2, age_hrt[1], width=width_2, alpha=0.7, label='Male')\n\naxes[2].set_title('Chances of having heart attack\\n as pereach age group\\n', fontsize=20)\naxes[2].set_xlabel('\\nAge Group', fontsize=15)\naxes[2].set_ylabel('Chances of heart attack (in %)', fontsize=15)\n\naxes[2].set_xticks(indx_2)\naxes[2].set_xticklabels(age_hrt.index, rotation=90)\naxes[2].legend()\n\nfig.tight_layout()\nplt.show()","27518c59":"df.head()","b1b6d975":"for i in df.columns:\n    print(i, ':- ', df[i].nunique())","2555768d":"X = df.drop('output', axis=1).copy()\ny = df['output']","643bf108":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)","17da8bfc":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","04ed9027":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler","bbda8943":"cat1 = ['sex', 'fbs', 'exng']\ncat2 = ['cp', 'restecg', 'caa', 'thall']\nnum = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']","d3f223e5":"trans_col = make_column_transformer((OneHotEncoder(handle_unknown='ignore'), cat2),\n                                   (OrdinalEncoder(), cat1),\n                                   (StandardScaler(), num),\n                                   remainder='passthrough')","6e639930":"from sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier","5baf3645":"pipe_lr = make_pipeline(trans_col, LogisticRegression(solver = 'liblinear'))\npipe_svm = make_pipeline(trans_col, SVC())\npipe_rf = make_pipeline(trans_col, RandomForestClassifier())","61d93d94":"from sklearn.model_selection import GridSearchCV","b06dcfe2":"params_lr = {\n    'logisticregression__penalty': ['l1', 'l2'],\n    'logisticregression__tol': [0.01, 0.001, 0.0001, 0.00001]\n}\n\ngrid_lr = GridSearchCV(estimator=pipe_lr, param_grid=params_lr, cv=10, verbose=10)\n","3a711960":"params_svm = {\n    'svc__kernel': ['linear', 'rbf'],\n    'svc__C': [1, 10, 100, 1000, 10000]\n}\n\ngrid_svm = GridSearchCV(estimator=pipe_svm, param_grid=params_svm, cv=10, verbose=10)","05dacb70":"params_rf = {\n    'randomforestclassifier__criterion': ['entropy', 'gini'],\n    'randomforestclassifier__max_depth': [2, 4, 6, 8, 10],\n    'randomforestclassifier__min_samples_split': [2, 4, 6, 8, 10],\n    'randomforestclassifier__min_samples_leaf': [2, 3, 4, 5],\n}\n\ngrid_rf = GridSearchCV(estimator = pipe_rf, param_grid = params_rf, cv=10, verbose=10, n_jobs=-1)","30801490":"grid_lr.fit(X_train, y_train)","81c8891c":"grid_svm.fit(X_train, y_train)","e9ff0ac9":"grid_rf.fit(X_train, y_train)","752b7c2c":"print(grid_lr.best_params_)\nprint(grid_lr.score(X_train, y_train))\nprint(grid_lr.score(X_test, y_test))\n","e9fb6825":"print(grid_svm.best_params_)\nprint(grid_svm.score(X_train, y_train))\nprint(grid_svm.score(X_test, y_test))","b04cf97b":"print(grid_rf.best_params_)\nprint(grid_rf.score(X_train, y_train))\nprint(grid_rf.score(X_test, y_test))","ad64c3d7":"from sklearn.metrics import f1_score\nprint(f1_score(y_test, grid_lr.predict(X_test)))\nprint(f1_score(y_test, grid_svm.predict(X_test)))\nprint(f1_score(y_test, grid_rf.predict(X_test)))","50f78535":"### EDA","d4ce1c20":"### Creating Model"}}