{"cell_type":{"5e640f99":"code","994d67f4":"code","d2583f16":"code","56b6f84e":"code","c9720bfb":"code","2ef77690":"code","c281c287":"code","05fbc2d5":"code","7d6892a6":"code","8aa218fd":"code","2da7d67a":"code","c5739769":"code","fdc51898":"code","238519b7":"code","57f02d80":"code","97b0937e":"code","2691ed98":"code","6a66520a":"code","b9bdc180":"code","590a5775":"code","db34536d":"code","82f76538":"code","f895f66a":"code","d05ad100":"code","70248a56":"markdown","5f9bb43c":"markdown","6c63b16d":"markdown","242e818a":"markdown","ebb135bc":"markdown","00d9dea3":"markdown","b36f515c":"markdown","0b2e6579":"markdown","ed4d9cd6":"markdown","6e160a41":"markdown","862f1b6b":"markdown","61c23391":"markdown","bacc9122":"markdown","a703b211":"markdown","f7d72c38":"markdown","576563a7":"markdown","fa4b68a1":"markdown","0a8e98c6":"markdown","c05ad6fa":"markdown","8e335464":"markdown","88c6faea":"markdown","904d8e5b":"markdown","e078caa7":"markdown","958b7ed9":"markdown"},"source":{"5e640f99":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.utils.multiclass import unique_labels\nfrom xgboost import XGBClassifier\n\nimport time\nimport pickle\n\nfrom lwoku import get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search","994d67f4":"N_ESTIMATORS = 2000\nMIN_SAMPLE_LEAFS = 100","d2583f16":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","56b6f84e":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_liblinear.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nlr_li_clf = clf.best_estimator_\nlr_li_clf","c9720bfb":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_saga.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nlr_sa_clf = clf.best_estimator_\nlr_sa_clf","2ef77690":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_sag.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nlr_sg_clf = clf.best_estimator_\nlr_sg_clf","c281c287":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_lbfgs.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nlr_lb_clf = clf.best_estimator_\nlr_lb_clf","05fbc2d5":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_newton-cg.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nlr_clf = clf.best_estimator_\nlr_clf","7d6892a6":"with open('..\/input\/tactic-03-hyperparameter-optimization-lda\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nlda_clf = clf.best_estimator_\nlda_clf","8aa218fd":"with open('..\/input\/tactic-03-hyperparameter-optimization-knn\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nknn_clf = clf.best_estimator_\nknn_clf","2da7d67a":"with open('..\/input\/tactic-03-hyperparameter-optimization-gnb\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\ngnb_clf = clf.best_estimator_\ngnb_clf","c5739769":"svc_clf = SVC(random_state=42,\n              verbose=True)","fdc51898":"with open('..\/input\/tactic-03-hyperparameter-optimization-bagging\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nbg_clf = clf.best_estimator_\nbg_clf","238519b7":"with open('..\/input\/tactic-03-hyperparameter-optimization-xtra-trees\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nxt_clf = clf.best_estimator_\nxt_clf","57f02d80":"with open('..\/input\/tactic-03-hyperparameter-optimization-rf\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nrf_clf = clf.best_estimator_\nrf_clf","97b0937e":"with open('..\/input\/tactic-03-hyperparameter-optimization-adaboost\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nab_clf = clf.best_estimator_\nab_clf","2691ed98":"with open('..\/input\/tactic-03-hyperparameter-optimization-gb\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\ngb_clf = clf.best_estimator_\ngb_clf","6a66520a":"with open('..\/input\/tactic-03-hyperparameter-optimization-lightgbm\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nlg_clf = clf.best_estimator_\nlg_clf","b9bdc180":"with open('..\/input\/tactic-03-hyperparameter-optimization-xgboost\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nplot_grid_search(clf)\ntable_grid_search(clf)\nxg_clf = clf.best_estimator_\nxg_clf","590a5775":"models = [\n          ('lr_li', lr_li_clf),\n          ('lr_sa', lr_sa_clf),\n          ('lr_sg', lr_sg_clf),\n          ('lr_lb', lr_lb_clf),\n          ('lr', lr_clf),\n          ('lda', lda_clf),\n          ('knn', knn_clf),\n          ('gnb', gnb_clf),\n#           ('svc', svc_clf),\n          ('bg', bg_clf),\n          ('xt', xt_clf),\n          ('rf', rf_clf),\n          ('ab', ab_clf),\n          ('gb', gb_clf),\n          ('lg', lg_clf),\n          ('xg', xg_clf)\n]","db34536d":"results = pd.DataFrame(columns = ['Model',\n                                  'Accuracy',\n                                  'Fit time',\n                                  'Predict test set time',\n                                  'Predict train set time'])\n\nfor name, model in models:\n\n    # Fit\n    t0 = time.time()\n    model.fit(X_train, y_train)\n    t1 = time.time()\n    t_fit = (t1 - t0)\n    \n    # Predict test set\n    t0 = time.time()\n    y_test_pred = pd.Series(model.predict(X_test), index=X_test.index)\n    t1 = time.time()\n    t_test_pred = (t1 - t0)\n\n    # Predict train set\n    t0 = time.time()\n    y_train_pred = pd.Series(get_prediction(model, X_train, y_train), index=X_train.index)\n    accuracy = accuracy_score(y_train, y_train_pred)\n    t1 = time.time()\n    t_train_pred = (t1 - t0)\n\n    # Submit\n    y_train_pred.to_csv('train_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')\n    y_test_pred.to_csv('submission_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')\n    print('\\n')\n    \n    results = results.append({\n        'Model': name,\n        'Accuracy': accuracy,\n        'Fit time': t_fit,\n        'Predict test set time': t_test_pred,\n        'Predict train set time': t_train_pred\n    }, ignore_index = True)","82f76538":"results = results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults.to_csv('results.csv', index=True, index_label='Id')\nresults","f895f66a":"tactic_01_results = pd.read_csv('..\/input\/tactic-01-test-classifiers\/results.csv', index_col='Id', engine='python')\ntactic_01_results","d05ad100":"comparison = pd.DataFrame(columns = ['Model',\n                                     'Accuracy',\n                                     'Fit time',\n                                     'Predict test set time',\n                                     'Predict train set time'])\n\ndef get_increment(df1, df2, model, column):\n    model1 = model.split('_', 1)[0]\n    v1 = float(df1[df1['Model'] == model1][column])\n    v2 = float(df2[df2['Model'] == model][column])\n    return '{:.2%}'.format((v2 - v1) \/ v1)\n\nfor model in results['Model']:\n    accuracy = get_increment(tactic_01_results, results, model, 'Accuracy')\n    fit_time = get_increment(tactic_01_results, results, model, 'Fit time')\n    predict_test_set_time = get_increment(tactic_01_results, results, model, 'Predict test set time')\n    predict_train_set_time = get_increment(tactic_01_results, results, model, 'Predict train set time')\n    comparison = comparison.append({\n        'Model': model,\n        'Accuracy': accuracy,\n        'Fit time': fit_time,\n        'Predict test set time': predict_test_set_time,\n        'Predict train set time': predict_train_set_time\n    }, ignore_index = True)    \n\ncomparison","70248a56":"### Boosting","5f9bb43c":"# Random forest classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. RF](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-rf)","6c63b16d":"# Extra-trees classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. Xtra-trees](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-xtra-trees)","242e818a":"# Introduction\n\nThe aim of this notebook is to optimize all the models of the notebook: [Tactic 01. Test classifiers\n](https:\/\/www.kaggle.com\/juanmah\/tactic-01-test-classifiers).\n\nThe models are fitted and predicted with the optimized parameters.\nThe results are collected at [Tactic 99. Summary](https:\/\/www.kaggle.com\/juanmah\/tactic-99-summary).","ebb135bc":"## [Ensemble Methods](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.ensemble)","00d9dea3":"## [Nearest Neighbors](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.neighbors)","b36f515c":"# Compare","0b2e6579":"## [Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.naive_bayes)","ed4d9cd6":"## [Generalized Linear Models](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model)","6e160a41":"# Prepare data","862f1b6b":"# Gaussian Naive Bayes\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. GNB](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-gnb)","61c23391":"# AdaBoost classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. Adaboost](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-adaboost)","bacc9122":"# C-Support Vector Classification\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. SVC](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-svc)","a703b211":"## [Discriminant Analysis](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.discriminant_analysis)","f7d72c38":"## [Support Vector Machines](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.svm)","576563a7":"# Gradient Boosting for classification\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. GB](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-gb)","fa4b68a1":"# k-nearest neighbors classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. KNN](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-knn)","0a8e98c6":"# Logistic Regression classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LR](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lr).","c05ad6fa":"\n### Bagging\n","8e335464":"# Linear Discriminant Analysis\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LDA](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lda).","88c6faea":"### Model list","904d8e5b":"# LightGBM\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LightGBM](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lightgbm)","e078caa7":"# Bagging classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. Bagging](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-bagging)","958b7ed9":"# XGBoost\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. XGBoost](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-xgboost)"}}