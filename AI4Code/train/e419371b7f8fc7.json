{"cell_type":{"fb682670":"code","3cea1d8a":"code","684d3e69":"code","6d90ee29":"code","45301931":"code","b8132afe":"code","e59724ff":"code","0c72f9a1":"code","709a6f1d":"markdown"},"source":{"fb682670":"import numpy as np\nimport matplotlib.pyplot as plt\nimport random as rm\nfrom scipy.stats import norm","3cea1d8a":"\n# 9-Armed Bandit \nmu = [10, 8, 5, 2, 6,3.4, 9, 9.4, 10.5]\nsd = [2, 4, 5, 3, 1.5, 2.3, 3, 1.2, 2.3]\n\n# Histogram of Distribution\nxp = np.linspace(-10, 30, 500)\nyp = np.array([norm.pdf(xp, mu[0], sd[0]), \n               norm.pdf(xp, mu[1], sd[1]), \n               norm.pdf(xp, mu[2], sd[2]), \n               norm.pdf(xp, mu[3], sd[3]), \n               norm.pdf(xp, mu[4], sd[4]), \n               norm.pdf(xp, mu[5], sd[5]), \n               norm.pdf(xp, mu[6], sd[6]), \n               norm.pdf(xp, mu[7], sd[7]), \n               norm.pdf(xp, mu[8], sd[8])])\n\ncolor = ['green', 'red','blue', 'black', 'darkorange', 'black', 'darkviolet', 'lawngreen', 'crimson']\nplt.figure(figsize = (10, 6))\nfor i in range(len(mu)):\n    plt.plot(xp, yp[i, :], color = color[i], alpha = 0.4, label = f'$\\mu$ : {mu[i]}, $\\sigma$: {sd[i]}')\n    plt.axvline(mu[i], color = color[i], linestyle = ':', alpha = 0.4)\n    \nplt.legend()\n","684d3e69":"# Only Exploration Strategy\n\ndef only_exploration(mu = mu, sd = sd, N = 1000):\n    d = len(mu)\n    dist_selected = []\n    numbers_of_selections = [0] * d\n    sum_of_rewards = [0] * d\n    cum_reward = 0\n    cum_reward_hist = []\n    \n    for i in range(N):\n        \n        dist_selected.append(rm.choice(list(range(d))))\n        reward = np.random.normal(loc = mu[dist_selected[i]], scale = sd[dist_selected[i]], size = 1)[0]\n        sum_of_rewards[dist_selected[i]] += reward\n        cum_reward += reward\n        cum_reward_hist.append(cum_reward)\n    \n    ave_reward = sum_of_rewards \/ np.unique(dist_selected, return_counts=True)[1]\n    return(dist_selected, sum_of_rewards, cum_reward, cum_reward_hist, ave_reward)","6d90ee29":"res1 = only_exploration(mu = mu, sd = sd, N = 1000)","45301931":"res1[1]","b8132afe":"#Distribution of dist_selected\nnp.unique(res1[0], return_counts = True)","e59724ff":"#Regret\nN = 1000\noptimal_reward = max(mu) * N\nregret = (optimal_reward - res1[2]) \/ optimal_reward * 100\nprint('The Regret for Only Exploration Strategy is = ', format(regret, '0.2f'))","0c72f9a1":"#Hisogram of Cumulitive Reward\nplt.axhline(optimal_reward, color = 'red', linewidth = 1.5, linestyle = '--')\nplt.plot(res1[3])\nplt.title('Only Exploration Strategy')\nplt.xlabel('Iteration')\nplt.ylabel('Cumulitive Reward')","709a6f1d":"## Multi-Armed Bandit Problem"}}