{"cell_type":{"417d054a":"code","cc3f4ba2":"code","86196862":"code","08f417bd":"code","25e9449b":"code","f57860a2":"code","33e5560a":"code","d40712ef":"code","25f8b2cb":"code","e035929c":"code","adb5d93b":"code","dd4fd6da":"code","1a03bcc6":"code","bbbeba0a":"code","8e104f91":"code","653c2637":"code","ea38d7ec":"code","b525a7d4":"code","881a55a8":"code","49e16c58":"code","ea87861a":"code","025ec0ec":"code","04cf99c6":"code","82b7c59b":"code","7e5c2389":"code","7d98a2e9":"code","320cb6b1":"code","122980c5":"code","c79316ca":"code","edbbae2e":"markdown","65a7cda7":"markdown","f09fb2af":"markdown","574f6293":"markdown","f06c9eac":"markdown","8e78aec8":"markdown","d5a7968f":"markdown","8a85902b":"markdown","5808a1e7":"markdown","692fb5ec":"markdown","cf703fc6":"markdown","5dd978a1":"markdown","62744182":"markdown","6e9cdbab":"markdown","7dcd3bec":"markdown"},"source":{"417d054a":"# This is a summary of the articles on the links bellow:\n\n#Link1: https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/\n#Link2: https:\/\/www.analyticsvidhya.com\/blog\/2015\/12\/complete-tutorial-time-series-modeling\/\n\nimport numpy as np \nimport pandas as pd \nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA\n\nfrom pylab import rcParams\n\n#Size of plt.plot fig\nrcParams['figure.figsize'] = 15, 5\n\ndf = pd.read_csv('..\/input\/airpassengers\/AirPassengers.csv', \n                 parse_dates = True, \n                 index_col = 'Month')","cc3f4ba2":"df.head()","86196862":"df.index","08f417bd":"ts = df['#Passengers']\nts","25e9449b":"ts['1949-01-01']\nts[datetime(1949,1,1)]","f57860a2":"ts[:'1949-05-01']","33e5560a":"ts['1949']","d40712ef":"plt.plot(ts)","25f8b2cb":"def test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=12).mean()\n    rolstd = timeseries.rolling(window=12).std()\n\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","e035929c":"test_stationarity(ts)","adb5d93b":"ts_log = np.log(ts)\nplt.plot(ts_log)","dd4fd6da":"moving_avg = ts_log.rolling(12).mean()\nplt.plot(ts_log)\nplt.plot(moving_avg, color='red')","1a03bcc6":"ts_moving_avg_diff = ts_log - moving_avg\nts_moving_avg_diff.head(12)","bbbeba0a":"ts_moving_avg_diff.dropna(inplace=True)\ntest_stationarity(ts_moving_avg_diff)","8e104f91":"expweighted_mavg = ts_log.ewm(halflife = 12).mean()\nplt.plot(ts_log)\nplt.plot(expweighted_mavg, color='red')","653c2637":"ts_log_ewma_diff = ts_log - expweighted_mavg\ntest_stationarity(ts_log_ewma_diff)","ea38d7ec":"ts_log_diff = ts_log - ts_log.shift()\nplt.plot(ts_log_diff)","b525a7d4":"ts_log_diff.dropna(inplace=True)\ntest_stationarity(ts_log_diff)","881a55a8":"decomposing = seasonal_decompose(ts_log)\n\ntrend = decomposing.trend\nseasonal = decomposing.seasonal\nresidual = decomposing.resid\n\nplt.subplot(411)\nplt.plot(ts_log, label='Original')\nplt.legend(loc='upper left')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='upper left')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='upper left')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='upper left')\nplt.tight_layout()","49e16c58":"ts_log_decompose = residual\nts_log_decompose.dropna(inplace=True)\ntest_stationarity(ts_log_decompose)","ea87861a":"lag_acf = acf(ts_log_diff, nlags = 20)\nlag_pacf = pacf(ts_log_diff, nlags=20, method = 'ols')","025ec0ec":"plt.plot(lag_acf, label='acf')\nplt.plot(lag_pacf, label='pacf')\nplt.legend(loc='upper right')\nplt.axhline(y=-1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n\n# We set the range from 0 to 5 after analising the full chart, to find the exact number \n#for acf and pacf where it crosses the confidence limits\nplt.xlim([0, 5]) ","04cf99c6":"model = ARIMA(ts_log, order = (2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(ts_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_log_diff)**2))","82b7c59b":"predicted_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy = True)\npredicted_ARIMA_diff.head()","7e5c2389":"predicted_ARIMA_diff_cumsum = predicted_ARIMA_diff.cumsum()\npredicted_ARIMA_diff_cumsum.head()","7d98a2e9":"predictions_ARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predicted_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA_log.head()","320cb6b1":"predictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(ts)\nplt.plot(predictions_ARIMA)\nplt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)\/len(ts)))","122980c5":"# Now let's forecast the next year\nresults_ARIMA.plot_predict(1,156)\nplt.legend(loc='upper left')","c79316ca":"# Some extra info just in case\n\n#x=results_ARIMA.forecast(steps=12)\n#print(x[1])\n#print(len(x[1]))\n#print(np.exp(x[1]))","edbbae2e":"#### **Decomposing**","65a7cda7":"***\n# **Modelling**","f09fb2af":"The red line shows the rolling mean. Lets subtract this from the original series. Note that since we are taking average of last 12 values, rolling mean is not defined for first 11 values. This can be observed as:","574f6293":"# **Estimating & Eliminating Trend**\n***\nOne of the first tricks to reduce trend can be transformation. For example, in this case we can clearly see that the there is a significant positive trend. So we can apply transformation which penalize higher values more than smaller values. These can be taking a **log, square root, cube root, etc**. Lets take a log transform here for simplicity:\n***","f06c9eac":"**********************\n# **How to Check Stationarity of a Time Series?**\n\n#### **Stationarity is defined using very strict criterion. However, for practical purposes we can assume the series to be stationary if it has constant statistical properties over time, ie. the following:**\n\n* Constant mean\n* Constant variance\n* An autocovariance that does not depend on time.\n********************","8e78aec8":"In this simpler case, it is easy to see a forward trend in the data. But its not very intuitive in presence of noise. So we can use some techniques to estimate or model this trend and then remove it from the series. There can be many ways of doing it and some of most commonly used are:\n\n* **Aggregation \u2013** taking average for a time period like monthly\/weekly averages\n* **Smoothing \u2013** taking rolling averages\n* **Polynomial Fitting \u2013** fit a regression model","d5a7968f":"# **More formally, we can check stationarity using the following:**\n*****\n* **Plotting Rolling Statistics:** We can plot the moving average or moving variance and see if it varies with time. By moving average\/variance I mean that at any instant \u2018t\u2019, we\u2019ll take the average\/variance of the last year, i.e. last 12 months. But again this is more of a visual technique.\n* **Dickey-Fuller Test:** This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the \u2018Test Statistic\u2019 is less than the \u2018Critical Value\u2019, we can reject the null hypothesis and say that the series is stationary. \n*****","8a85902b":"# **How to make a Time Series Stationary?**\n***\n**Lets understand what is making a TS non-stationary. There are 2 major reasons behind non-stationaruty of a TS:**\n* **Trend \u2013** varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.\n* **Seasonality \u2013** variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.\n***","5808a1e7":"### **Exponentially weighted moving average**\n\nNote that here the parameter \u2018halflife\u2019 is used to define the amount of exponential decay. This is just an assumption here and would depend largely on the business domain. Other parameters like span and center of mass can also be used to define decay which are discussed in the link shared above. Now, let\u2019s remove this from series and check stationarity:","692fb5ec":"\nLet me give you a brief introduction to ARIMA. I won\u2019t go into the technical details but you should understand these concepts in detail if you wish to apply them more effectively. ARIMA stands for **Auto-Regressive Integrated Moving Averages**. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n\n* **Number of AR (Auto-Regressive) terms (p):** AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)\u2026.x(t-5).\n\n* **Number of MA (Moving Average) terms (q):** MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)\u2026.e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n\n* **Number of Differences (d):** These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.\n\nAn importance concern here is how to determine the value of \u2018p\u2019 and \u2018q\u2019. We use two plots to determine these numbers. Lets discuss them first.\n\n**Autocorrelation Function (ACF):** It is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant \u2018t1\u2019\u2026\u2019t2\u2019 with series at instant \u2018t1-5\u2019\u2026\u2019t2-5\u2019 (t1-5 and t2 being end points).\n\n**Partial Autocorrelation Function (PACF):** This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.\n***","cf703fc6":"The p,d,q values can be specified using the order argument of ARIMA which take a tuple (p,d,q).\n\n* **p \u2013** The lag value where the **PACF** chart crosses the upper confidence interval for the first time. If you notice closely, in this case p=2.\n\n* **q \u2013** The lag value where the **ACF** chart crosses the upper confidence interval for the first time. If you notice closely, in this case q=3.","5dd978a1":"#### **Differencing**","62744182":"***\n# **Re-Scaling**","6e9cdbab":"# **Eliminating Trend and Seasonality**\n****\n\nThe simple trend reduction techniques discussed before don\u2019t work in all cases, particularly the ones with high seasonality. Lets discuss two ways of removing trend and seasonality:\n\n* **Differencing** \u2013 taking the differece with a particular time lag\n* **Decomposition** \u2013 modeling both trend and seasonality and removing them from the model.\n****","7dcd3bec":"***\n# **Forecasting**"}}