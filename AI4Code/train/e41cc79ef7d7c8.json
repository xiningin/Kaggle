{"cell_type":{"2688fb18":"code","3ffc0987":"code","5eade285":"code","37c5fe4d":"code","6d269283":"code","f7df2fe2":"code","52804499":"code","3a7f9176":"code","91203dd2":"code","2bfa9c1c":"code","ea82793c":"code","16956c60":"code","49990ca8":"code","817b4668":"code","220826c5":"code","ab0c8d13":"markdown","39f42283":"markdown","8af47266":"markdown","b41c1964":"markdown","9543316f":"markdown","386e7a36":"markdown","b7a568ea":"markdown","5bd6401f":"markdown","0ac0e5b5":"markdown","7ac6a848":"markdown","042f7625":"markdown","491fa5db":"markdown","2191e544":"markdown","89837596":"markdown","66163ea1":"markdown","4a877555":"markdown","02a14c06":"markdown","d04cfc6a":"markdown","5288a875":"markdown"},"source":{"2688fb18":"!pip install pycaret","3ffc0987":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","5eade285":"# Import the data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","37c5fe4d":"train.shape, test.shape","6d269283":"train.head()","f7df2fe2":"import plotly.graph_objects as go\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=train.target, hole=.3)])\nfig.show()","52804499":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","3a7f9176":"from pycaret.classification import *\nexp_mclf = setup(data = train, target = 'target', fold = 9, session_id=2021,silent = True)","91203dd2":"from sklearn.metrics import log_loss\nadd_metric('logloss', 'LogLoss', log_loss, greater_is_better=False, target=\"pred_proba\")","2bfa9c1c":"models()","ea82793c":"lgb = create_model('lightgbm',learning_rate= 0.0321)","16956c60":"plot_model(lgb,plot = 'auc')","49990ca8":"prep_pipe = get_config(\"prep_pipe\")\nprep_pipe.steps.append(['trained_model', lgb])\npredictions = prep_pipe.predict_proba(test)\npredictions","817b4668":"#pred_lgb = pred(lgb)\n#pred_xgb = pred(xgb)\n#pred_blend = pred(blended)\n#pred_stacked = pred(stacked)","220826c5":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] = predictions\nsample_submission.to_csv(f'lgb.csv',index=False)\n\n#sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] = pred_xgb\n#sample_submission.to_csv(f'xgb.csv',index=False)\n","ab0c8d13":"### Creating Models\n\n* create_model is the most granular function in PyCaret and is often the foundation behind most of the PyCaret functionalities. As the name suggests this function trains and evaluates a model using cross validation that can be set with fold parameter. The output prints a score grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa, MCC and logloss by fold.I will work with the below models.\n  * LGBMClassifier ('lightgbm')\n  * XGBClassifier ('xgboost')","39f42283":"> plot_model(xgb,plot = 'auc')","8af47266":"> stacked = stack_models(estimator_list = [lgb, xgb], optimize = 'logloss', method = 'predict_proba')","b41c1964":"> xgb = create_model('xgboost',max_depth= 8)","9543316f":"### Stack Models\n* Stacking models is method of ensembling that uses meta learning. The idea behind stacking is to build a meta model that generates the final prediction using the prediction of multiple base estimators. Stacking models in PyCaret is as simple as writing stack_models.","386e7a36":"### Blend Models\n* Blending models is a method of ensembling which uses consensus among estimators to generate final predictions. The idea behind blending is to combine different machine learning algorithms and use a majority vote or the average predicted probabilities in case of classification to predict the final outcome. Blending models in PyCaret is as simple as writing blend_models.","b7a568ea":"> tuned_lgb = tune_model(lgb,\n                      optimize='LogLoss')","5bd6401f":"* I choose **lightgbm** and **xgboost**  for prediction.","0ac0e5b5":"### Models in PyCaret\n* There are 18 classifiers available in the model library of PyCaret. To see list of all classifiers either check the docstring or use models function to see the library.","7ac6a848":"\u2699\ufe0f Install PyCaret & Import Libraries\nKaggle notebooks do not provide pycaret by default. So, you can install it with the following command :\n\n> !pip install pycaret","042f7625":"### Tune a Model\n* When a model is created using the create_model() function it uses the default hyperparameters to train the model. In order to tune hyperparameters, the tune_model() function is used. This function automatically tunes the hyperparameters of a model using Random Grid Search on a pre-defined search space. The output prints a score grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa, and MCC by fold for the best model. To use the custom search grid, you can pass custom_grid parameter in the tune_model function.","491fa5db":"### Setting up Environment in PyCaret\nThe setup() function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. setup() must be called before executing any other function in pycaret. It takes two mandatory parameters: a pandas dataframe and the name of the target column. All other parameters are optional and are used to customize the pre-processing pipeline.","2191e544":"### Predict on test","89837596":"### Plot a Model\nBefore model finalization, the plot_model() function can be used to analyze the performance across different aspects such as AUC, confusion_matrix, decision boundary etc. This function takes a trained model object and returns a plot based on the test \/ hold-out set.\n\nThere are 15 different plots available, please see the plot_model() docstring for the list of available plots.\n\n#### auc Plot","66163ea1":"* the dataset looks same as previous TPS.\n* The id value is meaningless, so I will leave it out in advance.","4a877555":"![](https:\/\/i1.wp.com\/pycaret.org\/wp-content\/uploads\/2020\/04\/thumbnail.png?fit=1166%2C656&ssl=1)","02a14c06":"> blended = blend_models(estimator_list = [lgb, xgb], optimize = 'logloss')","d04cfc6a":"#### using add_metric to apply logloss and submit with predict_proba","5288a875":"#### if you like this notebook plz upvote :)\n#### thank you"}}