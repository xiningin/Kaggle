{"cell_type":{"cc1703fe":"code","26398d65":"code","99479fab":"code","7ce65d48":"code","650c10d7":"code","1137b850":"code","91ed0dfd":"code","b640e164":"code","9f1e01c4":"code","e22bee58":"code","58162eeb":"code","88605850":"code","5fe71fa0":"code","85d3db99":"code","fcdf4305":"code","e8717fc8":"code","b8bccbcd":"code","eb5c84fe":"code","963e1913":"code","b955c653":"code","7a622132":"code","458d38f2":"code","2667ab07":"code","e4c27bca":"code","6357e86d":"code","cffc4c0b":"code","edffb87a":"code","64768a66":"code","8289cc70":"code","f0e26626":"code","c555e9d9":"code","b53b0359":"code","d3f30a47":"code","b6594d36":"code","9ba67d50":"code","f7c37901":"markdown","807ded3d":"markdown","2858b585":"markdown","4af7d331":"markdown","a24aecb7":"markdown","f9205e07":"markdown"},"source":{"cc1703fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26398d65":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error","99479fab":"X_full = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\", index_col= 'Id')\nX_test_full = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\", index_col = 'Id')","7ce65d48":"X_full","650c10d7":"X_test_full","1137b850":"X_full.shape, X_test_full.shape","91ed0dfd":"X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)","b640e164":"X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","9f1e01c4":"X_train_full.shape, X_valid_full.shape, y_train.shape, y_valid.shape","e22bee58":"categorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]","58162eeb":"categorical_cols","88605850":"numerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]","5fe71fa0":"numerical_cols","85d3db99":"my_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","fcdf4305":"my_cols","e8717fc8":"numerical_transformer = SimpleImputer(strategy='constant')","b8bccbcd":"categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","eb5c84fe":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","963e1913":"# Define model1\nmodel1 = RandomForestRegressor(n_estimators=800,random_state=20)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline1 = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model1)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline1.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds1 = my_pipeline1.predict(X_valid)","b955c653":"\nfrom sklearn.model_selection import RandomizedSearchCV","7a622132":"\n#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","458d38f2":"\n# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","2667ab07":"rf_random = RandomizedSearchCV(estimator = model1, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","e4c27bca":"randomized = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('random', rf_random)\n                             ])","6357e86d":"randomized.fit(X_train, y_train)","cffc4c0b":"rf_random.best_params_","edffb87a":"# Define model1\ntuned_rand = RandomForestRegressor(n_estimators = 1000,\n min_samples_split= 2,\n min_samples_leaf= 1,\n max_features= 'sqrt',\n max_depth= 25)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline2= Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', tuned_rand)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline2.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds2= my_pipeline2.predict(X_valid)","64768a66":"from xgboost import XGBRegressor","8289cc70":"# Define model1\nxgboost = XGBRegressor()\n\n# Bundle preprocessing and modeling code in a pipeline\nxgpipe= Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', xgboost)\n                             ])\n\n# Preprocessing of training data, fit model \nxgpipe.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds3 = xgpipe.predict(X_valid)","f0e26626":"# Define model2\nfrom sklearn.ensemble import GradientBoostingRegressor\ngrad = GradientBoostingRegressor(n_estimators=600, random_state=32)\n\n# Bundle preprocessing and modeling code in a pipeline\ngrad_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', grad)\n                             ])\n\n# Preprocessing of training data, fit model \ngrad_pipe.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds4 = grad_pipe.predict(X_valid)","c555e9d9":"# Define model2\nfrom sklearn.ensemble import GradientBoostingRegressor\ngrad_tuned = GradientBoostingRegressor(n_estimators = 1000,\n min_samples_split= 2,\n min_samples_leaf= 1,\n max_features= 'sqrt',\n max_depth= 25)\n\n# Bundle preprocessing and modeling code in a pipeline\ngrad_pipe1 = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', grad_tuned)\n                             ])\n\n# Preprocessing of training data, fit model \ngrad_pipe1.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds5 = grad_pipe1.predict(X_valid)","b53b0359":"score = mean_absolute_error(y_valid, preds1)\nprint('1 - MAE:', score)\nscore = mean_absolute_error(y_valid, preds2)\nprint('2 - MAE:', score)\nscore = mean_absolute_error(y_valid, preds3)\nprint('3 - MAE:', score)\nscore = mean_absolute_error(y_valid, preds4)\nprint('4 - MAE:', score)\nscore = mean_absolute_error(y_valid, preds5)\nprint('5 - MAE:', score)\n","d3f30a47":"preds= (preds1+ preds2+ preds3+ preds4 + preds5)\/5\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)","b6594d36":"preds_test1 = my_pipeline1.predict(X_test)\npreds_test2 = my_pipeline2.predict(X_test)\npreds_test3 = xgpipe.predict(X_test)\npreds_test4 = grad_pipe.predict(X_test)\npreds_test5 = grad_pipe1.predict(X_test)\npreds_test = (preds_test1+preds_test2+ preds_test3+ preds_test4 + preds_test5  )\/5","9ba67d50":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False) ","f7c37901":"# Randomized CV for Hyperparameter optimization","807ded3d":"# Xg-Boost","2858b585":"# Gradient Boosting Regressor","4af7d331":"# Random Forest Regressor After using Best Parameters","a24aecb7":"# Tuned Gradient Boosting","f9205e07":"# Random Forest Regressor"}}