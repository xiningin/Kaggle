{"cell_type":{"3d7b7340":"code","93756bd2":"code","c6b861f0":"code","34341c27":"code","b67063d0":"code","a8950c41":"code","5d1122b9":"code","670405df":"code","765e4038":"code","b2dee9c7":"markdown","d0b87c31":"markdown","35206695":"markdown"},"source":{"3d7b7340":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers","93756bd2":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n    \n    return np.array(all_tokens)","c6b861f0":"def build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","34341c27":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","b67063d0":"%%time\ntransformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","a8950c41":"model = build_model(transformer_layer, max_len=160)\nmodel.summary()","5d1122b9":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","670405df":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=4,\n    batch_size=32\n)","765e4038":"test_pred = model.predict(test_input, verbose=1)\n\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","b2dee9c7":"# Helper Functions","d0b87c31":"# Train Model","35206695":"# Load Data"}}