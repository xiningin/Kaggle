{"cell_type":{"abb1657a":"code","e076ddf6":"code","7736aca6":"code","67eb97f3":"code","b1fc3eda":"code","81f0499a":"code","3f340373":"code","89cdebd3":"code","82a8f450":"code","17723735":"code","7fa62204":"code","97317827":"code","b82afdaf":"code","f56c1b27":"code","eb76ecea":"code","838e4156":"code","dd6f3fc0":"markdown","fbf9aded":"markdown","ed8afb44":"markdown"},"source":{"abb1657a":"import numpy as np\nimport pandas as pd \nimport os\nprint(os.listdir(\"..\/input\"))","e076ddf6":"# Step 1 is to import both data sets\ntraining_data = pd.read_csv(\"..\/input\/train.csv\")\ntesting_data = pd.read_csv(\"..\/input\/test.csv\")\n\n# Step two is to create columns which I will add to the respective datasets, in order to know which row came from which dataset when I combine the datasets\ntraining_column = pd.Series([1] * len(training_data))\ntesting_column = pd.Series([0] * len(testing_data))\n\n# Now we append them by creating new columns in the original data. We use the same column name\ntraining_data['is_training_data'] = training_column\ntesting_data['is_training_data'] = testing_column","7736aca6":"# Now we can merge the datasets while retaining the key to split them later\ncombined_data = training_data.append(testing_data, ignore_index=True, sort=False)\n\n# Encode gender (if == female, True)\ncombined_data['female'] = combined_data.Sex == 'female'\n\n# Split out Title\ntitle = []\nfor i in combined_data['Name']:\n    period = i.find(\".\")\n    comma = i.find(\",\")\n    title_value = i[comma+2:period]\n    title.append(title_value)\ncombined_data['title'] = title\n\n# Replace the title values with an aliased dictionary\ntitle_arr = pd.Series(title)\ntitle_dict = {\n    \"Capt\" : \"Officer\",\n    \"Col\" : \"Officer\",\n    \"Major\" : \"Officer\",\n    \"Jonkheer\" : \"Royalty\",\n    \"Don\" : \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\" : \"Officer\",\n    \"Rev\" : \"Officer\",\n    \"the Countess\" : \"Royalty\",\n    \"Dona\" : \"Royalty\",\n    \"Mme\" : \"Mrs\",\n    \"Mlle\" : \"Miss\",\n    \"Ms\" : \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}\ncleaned_title = title_arr.map(title_dict)\ncombined_data['cleaned_title'] = cleaned_title\n\n# Fill NaN of Age - first create groups to find better medians than just the overall median. \ngrouped = combined_data.groupby(['female','Pclass', 'cleaned_title'])  \n\n# And now fill NaN with the grouped medians\ncombined_data['Age'] = grouped.Age.apply(lambda x: x.fillna(x.median()))\n\n# Fill NaN of Embarked\ncombined_data['Embarked'] = combined_data['Embarked'].fillna(\"S\") \n\n# Fill NaN of Fare\ncombined_data['Fare'] = combined_data['Fare'].fillna(combined_data['Fare'].mode()[0]) \n\n# Fill NaN of Cabin with a U for unknown. Not sure cabin will help.\ncombined_data['Cabin'] = combined_data['Cabin'].fillna(\"U\") \n\n# Finding cabin group\ncabin_group = []\nfor i in combined_data['Cabin']:\n    cabin_group.append(i[0])\ncombined_data['cabin_group'] = cabin_group\n\n# Adding a family_size feature as it may have an inverse relationship to either of its parts\ncombined_data['family_size'] = combined_data.Parch + combined_data.SibSp + 1\n\n# Mapping ports to passenger pickup order\nport = {\n    'S' : 1,\n    'C' : 2,\n    'Q' : 3\n}\ncombined_data['pickup_order'] = combined_data['Embarked'].map(port)\n\n# Encode childhood\ncombined_data['child'] = combined_data.Age < 16\n\n# One-Hot Encoding the titles\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cleaned_title'], prefix=\"C_T\")], axis = 1)\n\n# One-Hot Encoding the Pclass\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Pclass'], prefix=\"PClass\")], axis = 1)\n\n# One-Hot Encoding the  cabin group\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cabin_group'], prefix=\"C_G\")], axis = 1)\n\n# One-Hot Encoding the ports\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Embarked'], prefix=\"Embarked\")], axis = 1)","67eb97f3":"combined_data.dtypes.index","b1fc3eda":"new_train_data=combined_data.loc[combined_data['is_training_data']==1]\nnew_test_data=combined_data.loc[combined_data['is_training_data']==0]","81f0499a":"# here is the expanded model set and metric tools\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits = 10, shuffle=True, random_state=0)\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix","3f340373":"# Here are the features\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'child', 'Embarked_S', 'Embarked_C', \n            'Embarked_Q', 'pickup_order', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', 'C_T_Mrs',\n            'C_T_Officer', 'C_T_Royalty', 'C_G_A', 'C_G_B', 'C_G_C', 'C_G_D', 'C_G_E', 'C_G_F', 'C_G_G', \n            'C_G_T', 'C_G_U', 'family_size', 'PClass_1', 'PClass_2', 'PClass_3']\ntarget = 'Survived'","89cdebd3":"cvs_train_data = new_train_data[features]\ncvs_test_data = new_test_data[features]\ncvs_target = new_train_data['Survived']","82a8f450":"# Define the models\n################################################################################################### NOTE - I cannot justify setting random_state=0 for any reason other than reproducability of results\nmodel01 = RandomForestClassifier(n_estimators=10, random_state=0);\nmodel02 = DecisionTreeClassifier(random_state=0);\nmodel03 = LogisticRegression(solver='liblinear');\nmodel04 = KNeighborsClassifier(n_neighbors=15)\nmodel05 = GaussianNB()\nmodel06 = SVC(gamma='auto')\n\n# Define a function to make reading recall score easier\ndef printCVPRAF(model_number):\n    print(\"CrossVal Precision: \", round(np.mean(cross_val_score(model_number, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='precision'))*100,2), \"%\")\n    print(\"CrossVal Recall: \", round(np.mean(cross_val_score(model_number, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='recall'))*100,2), \"%\")\n    print(\"CrossVal Accuracy: \", round(np.mean(cross_val_score(model_number, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2), \"%\")\n    print(\"CrossVal F1-Score: \", round(np.mean(cross_val_score(model_number, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='f1_macro'))*100,2), \"%\")\n    y_pred = cross_val_predict(model_number, cvs_train_data, cvs_target, cv=k_fold)\n    conf_mat = confusion_matrix(cvs_target, y_pred)\n    rows = ['Actually Died', 'Actually Lived']\n    cols = ['Predicted Dead','Predicted Lived']\n    print(\"\\n\",pd.DataFrame(conf_mat, rows, cols))\n    \n# Print results\nprint(\"Random Forest Classifier\")\nprintCVPRAF(model01);\nprint(\"\\n\\nDecision Tree Classifier\")\nprintCVPRAF(model02);\nprint(\"\\n\\nLogistic Regression\")\nprintCVPRAF(model03);\nprint(\"\\n\\nKNeighborsClassifier\")\nprintCVPRAF(model04);\nprint(\"\\n\\nGaussianNB\")\nprintCVPRAF(model05);\nprint(\"\\n\\nSVC\")\nprintCVPRAF(model06);","17723735":"# First we fit the model\nmodel03.fit(cvs_train_data, cvs_target)\nprediction = model03.predict(cvs_test_data)\nsubmission = pd.DataFrame({\n    \"PassengerId\" : new_test_data['PassengerId'],\n    \"Survived\" : prediction.astype(int)\n})\nsubmission.to_csv('submission.csv', index=False)","7fa62204":"from sklearn.model_selection import GridSearchCV\nmodel07 = RandomForestClassifier(random_state=0);","97317827":"# The first thing to do is to create a dictionary of parameters\nforest_params = dict(     \n    max_depth = [n for n in range(5, 20)],     \n    min_samples_split = [n for n in range(3, 13)], \n    min_samples_leaf = [n for n in range(1, 7)],     \n    n_estimators = [n for n in range(10, 60, 2)],\n)\n\n# # Going to reduce the number of attempts for a while to see if this works better\n# forest_params = dict(     \n#     max_depth = [n for n in range(8, 12)],     \n#     min_samples_split = [n for n in range(5, 9)], \n#     min_samples_leaf = [n for n in range(2, 4)],     \n#     n_estimators = [n for n in range(10, 60, 25)],\n# )\n\n\nforest_gscv = GridSearchCV(estimator=model07, param_grid=forest_params, cv=k_fold) \nforest_gscv.fit(cvs_train_data, cvs_target)","b82afdaf":"print(\"Best score: {}\".format(forest_gscv.best_score_))\nprint(\"Optimal params: {}\".format(forest_gscv.best_estimator_))","f56c1b27":"forest_gscv_predictions = forest_gscv.predict(cvs_test_data)","eb76ecea":"forest_gscv_predictions.shape","838e4156":"# Export \nsubmission3 = pd.DataFrame({\n    \"PassengerId\" : new_test_data['PassengerId'],\n    \"Survived\" : forest_gscv_predictions.astype(int)\n})\nsubmission3.to_csv('submission3.csv', index=False)","dd6f3fc0":"This appears to very slightly increase the accuracy of the logistic regressor, so let's see if that's enough to jump in the rankings","fbf9aded":"Well that did nothing. Okay, what else can I change?\n# Trying out GridSearchCV\nis one thing I wanted to try","ed8afb44":"Now I am going to quickly try just replacing the titles with a more common map I've seen online."}}