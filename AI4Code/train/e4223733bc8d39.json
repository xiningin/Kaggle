{"cell_type":{"b5af7e24":"code","c320777b":"code","4a2f1233":"code","d8023122":"code","327c7400":"code","a6931fa0":"code","4f77255c":"code","2a737f3f":"code","983957ab":"code","0f6668c2":"code","b03073d3":"code","1faf3e69":"code","bb22275e":"code","64861223":"code","e9f2c04b":"code","c81f670b":"code","9509cf2b":"code","05ded459":"code","3333b91e":"code","a37fadd9":"code","a0fc8a9e":"code","6e82ab0e":"code","9dc3a3cb":"code","c4ccdf93":"code","cf577080":"code","6529a51a":"code","7ec47ae1":"code","e4b2bbb2":"code","5bb95388":"code","66f643d3":"code","dbd2883c":"code","03c41311":"code","50be176b":"code","15fafd1a":"code","b7d8663d":"code","cded901e":"code","6b6599fc":"markdown","eaffbdf3":"markdown","c66ec05e":"markdown","9ff21146":"markdown","302890ef":"markdown","66d16970":"markdown","7cc514ae":"markdown","e7b20ee4":"markdown","63b986c0":"markdown","7793fde1":"markdown","a0743c1c":"markdown","acf0310c":"markdown","0353fb5f":"markdown","dce38053":"markdown","839cd50d":"markdown","bbab744c":"markdown","39636478":"markdown","6100f614":"markdown","5108a3ad":"markdown","da9a7ef4":"markdown","eeffcf88":"markdown","a3066adb":"markdown","41822448":"markdown","8592435c":"markdown","8d54a38c":"markdown","6edf1c8d":"markdown","7617c3a1":"markdown","84f439ad":"markdown","d1db4372":"markdown","661d7494":"markdown","d69061a6":"markdown","9ddd3d87":"markdown","35054a16":"markdown","72867e21":"markdown","08bed7ad":"markdown","dfdb5fe7":"markdown","aab2707b":"markdown","2d5315b5":"markdown","b75d6f55":"markdown","7e918bd9":"markdown","fc25698f":"markdown","6d7259da":"markdown","92c13e43":"markdown","39482164":"markdown","e36965d2":"markdown","145b43d6":"markdown","6f415acb":"markdown","d0efffd4":"markdown","8634f780":"markdown","d4f1d775":"markdown","8baa64b1":"markdown","68451ad0":"markdown","4a3e0eaa":"markdown","71f611e3":"markdown","692594d1":"markdown","23bd2f46":"markdown"},"source":{"b5af7e24":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:, 0:2]  # we only take the first two features for visualization\ny = iris.target\n\nn_features = X.shape[1]\n\nrandom_state=0\nseed = 1111\n\nC = 10\nkernel = 1.0 * RBF([1.0, 1.0])  # for GPC\n\n# Create different classifiers.\nclassifiers = {\n    'L1 logistic': LogisticRegression(C=C, penalty='l1',\n                                      solver='saga',\n                                      multi_class='multinomial',\n                                      max_iter=10000),\n    'L1 logistic (Multinomial)': LogisticRegression(C=C, penalty='l1',\n                                                    solver='saga',\n                                                    multi_class='multinomial',\n                                                    max_iter=10000),\n    'L2 logistic (Multinomial)': LogisticRegression(C=C, penalty='l2',\n                                                    solver='saga',\n                                                    multi_class='multinomial',\n                                                    max_iter=10000),\n    'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2',\n                                            solver='saga',\n                                            multi_class='ovr',\n                                            max_iter=10000),\n    'Linear SVC': SVC(kernel='linear', C=C, probability=True,\n                      random_state=0),\n    'GPC': GaussianProcessClassifier(kernel)\n}\n\nn_classifiers = len(classifiers)\n\nplt.figure(figsize=(3 * 2, n_classifiers * 2))\nplt.subplots_adjust(bottom=.2, top=.95)\n\n\nxx = np.linspace(3, 9, 100)\nyy = np.linspace(1, 5, 100).T\nxx, yy = np.meshgrid(xx, yy)\nXfull = np.c_[xx.ravel(), yy.ravel()]\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classifier.fit(X, y)\n\n    y_pred = classifier.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\n\n    # View probabilities:\n    probas = classifier.predict_proba(Xfull)\n    n_classes = np.unique(y_pred).size\n    for k in range(n_classes):\n        plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)\n        plt.title(\"Class %d\" % k)\n        if k == 0:\n            plt.ylabel(name)\n        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),\n                                   extent=(3, 9, 1, 5), origin='lower')\n        plt.xticks(())\n        plt.yticks(())\n        idx = (y_pred == k)\n        if idx.any():\n            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='w', edgecolor='k')\n\nax = plt.axes([0.15, 0.04, 0.7, 0.05])\nplt.title(\"Probability\")\nplt.colorbar(imshow_handle, cax=ax, orientation='horizontal')\n\nplt.show()","c320777b":"import matplotlib.pyplot as plt\nimport numpy as np\nimport time # Added this as original code was giving error\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import check_random_state\n\n\n# Author: Arthur Mensch <arthur.mensch@m4x.org>\n# License: BSD 3 clause\n\n# Turn down for faster convergence\nt0 = time.time()\ntrain_samples = 5000\n\n# Load data from https:\/\/www.openml.org\/d\/554\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n\nrandom_state = check_random_state(0)\npermutation = random_state.permutation(X.shape[0])\nX = X[permutation]\ny = y[permutation]\nX = X.reshape((X.shape[0], -1))\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=train_samples, test_size=10000)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Turn up tolerance for faster convergence\nclf = LogisticRegression(\n    C=50. \/ train_samples, penalty='l2', solver='saga', tol=0.1\n)\nclf.fit(X_train, y_train)\nsparsity = np.mean(clf.coef_ == 0) * 100\nscore = clf.score(X_test, y_test)\n# print('Best C % .4f' % clf.C_)\nprint(\"Sparsity with L2 penalty: %.2f%%\" % sparsity)\nprint(\"Test score with L2 penalty: %.4f\" % score)\n\ncoef = clf.coef_.copy()\nplt.figure(figsize=(10, 5))\nscale = np.abs(coef).max()\nfor i in range(10):\n    l2_plot = plt.subplot(2, 5, i + 1)\n    l2_plot.imshow(coef[i].reshape(28, 28), interpolation='spline36',\n                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n    l2_plot.set_xticks(())\n    l2_plot.set_yticks(())\n    l2_plot.set_xlabel('Class %i' % i)\nplt.suptitle('Classification vector for...')\n\nrun_time = time.time() - t0\nprint('Example run in %.3f s' % run_time)\nplt.show()","4a2f1233":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\n\n# make 3-class dataset for classification\ncenters = [[-5, 0], [0, 1.5], [5, -1]]\nX, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\ntransformation = [[0.4, 0.2], [-0.4, 1.2]]\nX = np.dot(X, transformation)\n\nfor multi_class in ('multinomial', 'ovr'):\n    clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,\n                             multi_class=multi_class).fit(X, y)\n\n    # print the training scores\n    print(\"training score : %.3f (%s)\" % (clf.score(X, y), multi_class))\n\n    # create a mesh to plot in\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n    plt.axis('tight')\n\n    # Plot also the training points\n    colors = \"bry\"\n    for i, color in zip(clf.classes_, colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,\n                    edgecolor='black', s=20)\n\n    # Plot the three one-against-all classifiers\n    xmin, xmax = plt.xlim()\n    ymin, ymax = plt.ylim()\n    coef = clf.coef_\n    intercept = clf.intercept_\n\n    def plot_hyperplane(c, color):\n        def line(x0):\n            return (-(x0 * coef[c, 0]) - intercept[c]) \/ coef[c, 1]\n        plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n                 ls=\"--\", color=color)\n\n    for i, color in zip(clf.classes_, colors):\n        plot_hyperplane(i, color)\n\nplt.show()","d8023122":"import urllib.request\nimport pandas as pd\nimport requests\nimport io\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot","327c7400":"\nlink = \"https:\/\/stats.idre.ucla.edu\/stat\/data\/hsb2.csv\"\nwebUrl = urllib.request.urlopen(link)\nif webUrl.getcode() == 200:\n    print(\"URL read successfully\")\nelse:\n    print(\"URL not read successfully, check url link\/internet connection and try again\")","a6931fa0":"s = requests.get(link).content\nhsb2 = pd.read_csv(io.StringIO(s.decode('utf-8')))","4f77255c":"hsb2.head()","2a737f3f":"hsb2.dtypes","983957ab":"hsb2[\"race\"] = hsb2[\"race\"].astype('category')\nhsb2[\"female\"] = hsb2[\"female\"].astype('category')\nhsb2[\"ses\"] = hsb2[\"ses\"].astype('category')","0f6668c2":"hsb2.dtypes","b03073d3":"sns.countplot(hsb2['prog']);","1faf3e69":"sns.countplot(hsb2['ses']);","bb22275e":"sns.countplot(hsb2['race']);","64861223":"sns.countplot(hsb2['female']);","e9f2c04b":"sns.countplot(hsb2['schtyp']);","c81f670b":"sns.boxplot(hue=hsb2[\"female\"],y=hsb2[\"read\"],x=hsb2[\"prog\"]);","9509cf2b":"sns.boxplot(hue=hsb2[\"female\"],y=hsb2[\"write\"],x=hsb2[\"prog\"]);","05ded459":"sns.boxplot(hue=hsb2[\"female\"],y=hsb2[\"math\"],x=hsb2[\"prog\"]);","3333b91e":"sns.boxplot(hue=hsb2[\"female\"],y=hsb2[\"science\"],x=hsb2[\"prog\"]);","a37fadd9":"sns.boxplot(hue=hsb2[\"female\"],y=hsb2[\"socst\"],x=hsb2[\"prog\"]);","a0fc8a9e":"sns.boxplot(hue=hsb2[\"schtyp\"],y=hsb2[\"read\"],x=hsb2[\"prog\"]);","6e82ab0e":"sns.boxplot(hue=hsb2[\"schtyp\"],y=hsb2[\"write\"],x=hsb2[\"prog\"]);","9dc3a3cb":"sns.boxplot(hue=hsb2[\"schtyp\"],y=hsb2[\"math\"],x=hsb2[\"prog\"]);","c4ccdf93":"sns.boxplot(hue=hsb2[\"schtyp\"],y=hsb2[\"science\"],x=hsb2[\"prog\"]);","cf577080":"sns.boxplot(hue=hsb2[\"schtyp\"],y=hsb2[\"socst\"],x=hsb2[\"prog\"]);","6529a51a":"sns.pairplot(hsb2.drop(['id'],axis=1),hue=\"prog\", diag_kind=\"hist\",markers=[\"o\", \"s\", \"D\"], height=1.5);","7ec47ae1":"race = pd.get_dummies(hsb2['race'],drop_first=True,prefix='race')\nses = pd.get_dummies(hsb2['ses'],drop_first=True,prefix='ses')\nhsb3 = hsb2\nhsb3.drop(['race','ses'],axis=1,inplace=True)\nhsb3 = pd.concat([hsb3,race,ses],axis=1)","e4b2bbb2":"y = hsb3['prog']\nX = hsb3.drop(['prog','id'],axis=1)","5bb95388":"# define the multinomial logistic regression model with a default penalty\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', \n                           C=1.0, max_iter = 1000000)\n# define the model evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model and collect the scores\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report the model performance\nprint('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","66f643d3":"result = model.fit(X, y)","dbd2883c":"row = X.iloc[0:1, :]\n# predict a multinomial probability distribution\nyhat = model.predict_proba(row)\n# summarize the predicted probabilities\nprint('Predicted Probabilities: %s' % yhat[0])","03c41311":"# predict the class label\nyhat = model.predict(row)\n# summarize the predicted class\nprint('Predicted Class: %d' % yhat[0])","50be176b":"# get a list of models to evaluate\ndef get_models():\n\tmodels = dict()\n\tfor p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n\t\t# create name for model\n\t\tkey = '%.4f' % p\n\t\t# turn off penalty in some cases\n\t\tif p == 0.0:\n\t\t\t# no penalty in this case\n\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n\t\telse:\n\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n\treturn models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n\t# define the evaluation procedure\n\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\t# evaluate the model\n\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n\treturn scores\n \n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n\t# evaluate the model and collect the scores\n\tscores = evaluate_model(model, X, y)\n\t# store the results\n\tresults.append(scores)\n\tnames.append(name)\n\t# summarize progress along the way\n\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show();","15fafd1a":"print(result.intercept_)\nprint(result.coef_)","b7d8663d":"summary = pd.DataFrame(zip(X.columns, np.transpose(result.coef_.tolist()[0])), \n                       columns=['features', 'coef'])","cded901e":"print(summary)","6b6599fc":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.1  Data Description and import necessary libraries<\/h3>","eaffbdf3":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.5  Exploratory data analysis (EDA)<\/h3>","c66ec05e":"Females have scored lesser marks than males in read except program 3","9ff21146":"![image.png](attachment:image.png)","302890ef":"<h1 style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">1.1 &nbsp;&nbsp;MLR example through code - IRIS dataset<\/h1>","66d16970":"The table below provides a quick reference on the differences between problem types. More detailed explanations can be found in subsequent sections of this guide.","7cc514ae":"The predicted program class is one which we know is true from the initial data display","e7b20ee4":"Females have scored better marks than males in write for all programs","63b986c0":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">2&nbsp;&nbsp;MLR Equation<\/a>","7793fde1":"Student of school type 1 perform well in program 1 and 2 compared to 3 in maths as well","a0743c1c":"MNIST dataset is very famous in computer vision for building basis neural networks.\nL2 logistic has a very good accuracy of ~ 87% comparable to other more complicated algorithms like neural networks on the test dataset. So we can see that multinomial logistic regression (MLR) extends well for computer vision problems as well. It definetly whets our appetite to learn more about this method.","acf0310c":"Student of school type 1 perform well in program 1 and 2 compared to 3 in social studies as well","0353fb5f":"Student of school type 1 perform well in program 1 and 2 compared to 3 in science as well","dce38053":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">4&nbsp;&nbsp;MLR SKLEARN Usage<\/a>","839cd50d":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.9  Print the coefficients and intercept for the model<\/h3>","bbab744c":"Students of school type 1 are better performers in program 1 and 2 but not in 3","39636478":"Let us first understand the equation of logistic regression which will then help us understand multinomial logistic regression (MLR). Logistic regression is an instance of classification technique that we can use to predict a qualitative response. More specifically, logistic regression models the probability for example if a person will buy a certain product or not in a shopping mall.\n\nThat means that, if we are trying to do shopping classification, where the response variable falls into one of the two categories, buy or not-buy, we will use logistic regression models to estimate the probability that the particular person will buy or not.\n\nFor example, the probability of buying given gender can be written as:\n\n$$ Pr(buy=yes|gender) $$\n\nThe values of $ Pr(buy=yes|gender) $ (abbreviated as $ p(gender) $) will range between 0 and 1. Then, for any given value of gender (male or female), a prediction can be made wether the person will buy or not.\n\nGiven X as the explanatory or dependent variable and Y as the response or independent variable, how should we then model the relationship between \n\n$$ p(X)=Pr(Y=1|X) and X ? $$\n\nThe linear regression model represents these probabilities as:\n\n$$ p(X)=\u03b2_0\u2005+\u2005\u03b2_1X $$\n\nThe problem with this approach is that, any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict $ p(X)<0 $ for some values of $ X $ and $ p(X)>1 $ for others.\n\nTo avoid this problem, you can use the logistic function to model $ p(X) $ that gives outputs between 0 and 1 for all values of $ X $:\n\n$$ p(X) = {e^{(\u03b2_0 + \u03b2_1X_1)}\\over 1+e^{(\u03b2_0+\u03b2_1X)}} $$\n\nThe logistic function will always produce an S-shaped curve, so regardless of the value of $ X $, we will obtain a sensible prediction.\n\nThe above equation can also be reframed as:\n\n$$ {p(X)\\over 1\u2212p(X)} = e^{\u03b2_0+\u03b2_1X} $$\n\nThe quantity $ {p(X)\\over 1\u2212p(X)}$ is called the odds ratio, and can take on any value between 0 and $ {\\infty} $. Values of the odds ratio close to 0 and ${\\infty}$ indicate very low and very high probabilities of $ p(X) $, respectively.\n\nBy taking the logarithm of both sides from the equation above, you get:\n\n$$ log({p(X)\\over1\u2212p(X)})=\u03b2_0+\u03b2_1X $$\n\nThe left-hand side is called the logit. In a logistic regression model, increasing $ X $ by one unit changes the logit by $ \u03b2_0 $. The amount that $ p(X) $ changes due to a one-unit change in $ X $ will depend on the current value of $ X $. But regardless of the value of $ X $, if $ \u03b2_1 $ is positive then increasing $ X $ will be associated with increasing $ p(X) $, and if $ \u03b2_1 $ is negative then increasing $ X $ will be associated with decreasing $ p(X) $.\n\nThe coefficients $ \u03b2_0 $ and $ \u03b2_1 $ are unknown, and must be estimated based on the available training data. For logistic regression, you can use maximum likelihood, a powerful statistical technique. Let's refer back to your gender classification example.\n\nYou seek estimates for $ \u03b2_0 $ and $ \u03b2_1 $ such that plugging these estimates into the model for $ p(X) $ yields a number close to 1 for all individuals who are female, and a number close to 0 for all individuals who are not.\n\nThis intuition can be formalized using a mathematical equation called a likelihood function:\n\n$$ l(\u03b2_0,\u2006\u03b2_1)=p(X)(1\u2005\u2212\u2005p(X)) $$\n\nThe estimates $ \u03b2_0 $ and $ \u03b2_1 $ are chosen to maximize this likelihood function. Once the coefficients have been estimated, you can simply compute the probability of being female given any instance of having longhair. Overall, maximum likelihood is a very good approach to fit non-linear models.\n\nSo far, we have only focused on Binomial Logistic Regression, since we were classifying as buyers or non-buyers. Multinomial Logistic Regression model is a simple extension of the binomial logistic regression model, which you use when the exploratory variable has more than two nominal (unordered) categories.\n\nIn multinomial logistic regression, the exploratory variable is dummy coded into multiple 1\/0 variables. There is a variable for all categories but one, so if there are $ N $ categories, there will be $ N\u22121 $ dummy variables. Each category\u2019s dummy variable has a value of 1 for its category and a 0 for all others. One category, the reference category, doesn\u2019t need its own dummy variable, as it is uniquely identified by all the other variables being 0.\n\nThe mulitnomial logistic regression then estimates a separate binary logistic regression model for each of those dummy variables. The result is $ N\u22121 $ binary logistic regression models. Each model conveys the effect of predictors on the probability of success in that category, in comparison to the reference category.","6100f614":"Multinomial logistic regression is the generalization of logistic regression algorithm. If the logistic regression algorithm used for the multi-classification task, then the same logistic regression algorithm called as the multinomial logistic regression.\n\nThe difference in the normal logistic regression algorithm and the multinomial logistic regression in not only about using for different tasks like binary classification or multi-classification task. It is all about using the different functions.\n\nIn the logistic regression, the black-box function which takes the input features and calculates the probabilities of the possible two outcomes is the Sigmoid Function. Later the high probabilities target class is the final predicted class from the logistic regression classifier.\n\nWhen it comes to the multinomial logistic regression the function is the Softmax Function. We will see what is the difference between the two functions in subsequent sections","5108a3ad":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.6  Define the dependent and independent variables<\/h3>","da9a7ef4":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.3  Display few rows of the data<\/h3>","eeffcf88":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">5&nbsp;Multinomial logistic regression (MLR) in R\/Python\/STATA<\/a>","a3066adb":"Majority of the students are from school type 1 compared to 2","41822448":"<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: #7b4f88; background-color: #ffffff;\">Multinomial Logistic Regression<\/h1>\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">Exploratory Data Analysis (EDA) and deep dive<\/h2>\n\n","8592435c":"* A one-unit increase in the variable write is associated with a .005 decrease in the relative log odds of being in program = 0 vs.program = 1.\n\n* A one-unit increase in the variable write is associated with a .010 (0.005*2) decrease in the relative log odds of being in program = 2 vs. program = 1\n\n* The relative log odds of being in program = 0 vs.program = 1 will decrease by 0.47 if moving from the lowest level of ses (ses==1) to the middle level of ses (ses==2) and by 0.49 if moving from the middle level of ses (ses==2) to (ses==3). Here ses stands for socio economic structure","8d54a38c":"We use a 3 class dataset here which is the IRIS dataset, and we classify it with a Support Vector classifier (SVC) , L1 and L2 penalized logistic regression with either a One-Vs-Rest or multinomial setting, and Gaussian process classification. We see that mutinomial logistic regression (MLR) performs quite well compared to other complicated alogrithms.\n\nLinear SVC is not a probabilistic classifier by default but it has a built-in calibration option enabled in this example (probability=True).\n\nThe logistic regression with One-Vs-Rest is not a multiclass classifier out of the box. As a result it has more trouble in separating class 2 and 3 than the other estimators.","6edf1c8d":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">0&nbsp;&nbsp;What is Multinomial Logistic Regression (MLR)?<\/a>","7617c3a1":"Softmax is used for Multi-Class Classification Problem that has only one right answer in other words outputs are mutually exclusive outputs (e.g. handwritten digits, irises)\n\n* When we are building a classifier for problems with only one right answer, we apply a softmax to the raw outputs.\n* Applying a softmax takes into account all of the elements of the raw output, in the denominator, which means that the different probabilities produced by the softmax function are interrelated.\n* The softmax function looks like this:\n\n$$softmax(z_j) = {e^{z_j}\\over\\sum \\limits_{k=1}^k e^{z_k}} for j = 1,2,3...k$$\n\nThis is similar to the sigmoid function, except in the denominator we sum together $e^$ thing for all of the things in our raw output. In other words, when calculating the value of softmax on a single raw output (e.g. $z_1$) we can\u2019t just look at $z_1$ alone: we have to take into account $z_1$, $z_2$, $z_3$, and $z_4$ in the denominator, like this:\n\n$$softmax(z_j) = {e^{z_j}\\over\\sum \\limits_{k=1}^k e^{z_k}} for j = 1,2,3...k$$\n\n$$ softmax(z_j) = {e^{z_1} \\over e^{z_1} + e^{z_2} + e^{z_3} + e^{z_4}} $$\n\n$$ softmax(z_j) = {e^{-0.5} \\over e^{-0.5} + e^{1.2} + e^{-0.1} + e^{2.4}} = 0.0383 $$\n\nThe softmax function is cool because it ensures that the sum of all our output probabilities will be equal to one.\n\nThat means if we are classifying handwritten digits and applying a softmax to our raw outputs, in order for the network to increase the probability that a particular example is classified as an \u201c6\u201d it needs to decrease the probabilities that the example is classified as some other number(s) (0, 1, 2, 3, 4, 5, 7, 8 and\/or 9).\n\n","84f439ad":"![image.png](attachment:image.png)","d1db4372":"Plot decision surface of multinomial and One-vs-Rest Logistic Regression. The hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers are represented by the dashed lines.","661d7494":"Most of the students are from socio economic status 2, while 1 and 3 are comparable","d69061a6":"![image.png](attachment:image.png)","9ddd3d87":"In statistics, *multinomial logistic regression* (MLR) is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.).\nThere are different synonyms of MLR as follows\n\n* Polytomous LR\n* Multiclass LR\n* Softmax regression\n* Multinomial logit\n* Maximum entropy classifier\n* Conditional maximum entropy model","35054a16":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.7  Run the logistic regression<\/h3>","72867e21":"We can similarly execute the model in STATA and python which can be followed from below links\n\n* STATA -> (https:\/\/stats.idre.ucla.edu\/stata\/dae\/multinomiallogistic-regression\/)\n* R -> described in detail in section below","08bed7ad":"Here, the sigma symbol $\\sigma$ indicates the sigmoid function. The expression $\\sigma(z_j)$ indicates that we are applying the sigmoid function to the number $z_j$. $ z_j $ indicates a single raw output value, e.g. $ -0.5 $. What is the $ j $ for? It tells us which of the output values we are using. If we have four output values, we have $ j = 1, 2, 3, 4 $. So where our raw outputs were $ [-0.5, 1.2, -0.1, 2.4] $, we have $ z_1 = -0.5, z_2 = 1.2, z_3 = -0.1, z_4 = 2.4 $\n\nThus we have for $ z_1 = -0.5 $","dfdb5fe7":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.8  Fit the model and predict class for first observation<\/h3>","aab2707b":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.2  Read the data<\/h3>","2d5315b5":"For social studies females have performed better than males in program 2 and 3","b75d6f55":"<h1 style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">1.2 &nbsp;&nbsp;MLR example through code - MNIST dataset<\/h1>","7e918bd9":"Most of the students have opted for program 2, program 1 and 3 are fairly balanced","fc25698f":"For science males have in general performed better than females across programs","6d7259da":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">1&nbsp;&nbsp;MLR Example<\/a>","92c13e43":"Majority of the students are females however there is not too much imbalance between the genders","39482164":"<h2 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\">TABLE OF CONTENTS<\/h2>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;What is multinomial logistic regression?<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;MLR Example<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;MLR Equation<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">3&nbsp;&nbsp;&nbsp;&nbsp;Sigmoid versus softmax function<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#combining_annotations\">4&nbsp;&nbsp;&nbsp;&nbsp;MLR SKLEARN usage<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">5&nbsp;&nbsp;&nbsp;&nbsp;Multinomial logistic regression (MLR) in R\/Python\/STATA<\/a><\/h3>\n\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#image_data\">6&nbsp;&nbsp;&nbsp;&nbsp;MLR Interpretation<\/a><\/h3>\n\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#combining_annotations\">7&nbsp;&nbsp;&nbsp;&nbsp;References and further reads<\/a><\/h3>\n","e36965d2":"L1 logistic has a very good accuracy of ~ 83% compared to other more complicated algorithms like support vector classifier (SVC) or Gaussian process classifier (GPC). ","145b43d6":"Most of the students are from race type 4, while other races are minority","6f415acb":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">3&nbsp;&nbsp;Sigmoid versus Softmax function<\/a>","d0efffd4":"For maths both genders are almost neck to neck to all the programs","8634f780":"We observe that the model accuracy dips with increase in value of C (penalty term). Optimal value of c appears to be 0.0001 as per above box plot","d4f1d775":"Logistic regression, despite its name, is a linear model for classification rather than regression. \n\nLogistic regression is implemented in [LogisticRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression). \nThis implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional $l_1$, $l_2$ or Elastic-Net regularization.\n\n****Note**** : Regularization is applied by default, which is common in machine learning but not in statistics. Another advantage of regularization is that it improves numerical stability. No regularization amounts to setting C to a very high value.\n","8baa64b1":"Sigmoid is used for Multi-Label Classification Problem. It means that it has more than one right answer and the output is non-exclusive (e.g. chest x-rays, hospital admission)\n\n* When we are building  a classifier for a problem with more than one right answer, we apply a sigmoid function to each element of the raw output independently.\n* The sigmoid function looks like this (notice the number e in there):\n\n$$ \\sigma (z_j) = {e^{z_j} \\over (1+e^{z_j})} $$","68451ad0":"* Which subject will a college student choose, given their grades, stated likes and dislikes, gender etc.?\n* Which blood type does a person have, given the results of various diagnostic tests?\n* Which candidate will a person vote for, given particular demographic characteristics?\n* Which country will a firm locate an office in, given the characteristics of the firm and of the various candidate countries?\n* Which activity is the person currently in out of the following, based on body sensors?\n> 1. Standing\n> 2. Walking\n> 3. Running\n> 4. Laying\n> 5. Sitting\n* The plant belongs to which class based on IRIS dataset? \n> 1. Setosa\n> 2. Versicolor\n> 3. Virginica\n* Classification of handwritten digits (0-9) into one of the 10 classes (MNIST dataset)","4a3e0eaa":"$$ \\sigma(z_1) = \\sigma(-0.5) = {e^{-0.5} \\over (1 + e^{-0.5})} = 0.3775 $$","71f611e3":"For our data analysis example, we will be using the hsbdemo data set. The data is available [here](https:\/\/stats.idre.ucla.edu\/stat\/data\/hsb2.csv)\n\nThe data set contains variables on 200 students. The outcome variable is prog that is program type. The predictor variables are social economic status, ses,  a three-level categorical variable and writing score, write, a continuous variable. Let\u2019s start with getting some descriptive statistics of the variables of interest.","692594d1":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">5.4  Convert Race, gender and socio economic status (ses) as categorical variables<\/h3>","23bd2f46":"Students of school type 1 have better read scores for program 2 and 3"}}