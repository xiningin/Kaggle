{"cell_type":{"1ce98d4b":"code","f8624cef":"code","93d42103":"code","d8541c34":"code","76b7b4fb":"code","f0681ad6":"code","732a71a7":"code","8f8aa6e2":"code","4f84326d":"code","fbd2b083":"code","c5207cbe":"code","3ffa2584":"code","822ae45c":"code","978e6b72":"code","0f7fc351":"code","b860c520":"code","59220bb2":"code","0e9f15e2":"code","af99dfd9":"code","6ec48902":"code","10686ca6":"code","67c79b68":"code","94f7ad8b":"code","de659564":"code","4eca591e":"code","70700e16":"code","117b5bc7":"code","f8501e93":"code","08ef6420":"code","bcbff220":"code","afccb974":"code","b869530b":"code","57ec8e1b":"code","def16488":"code","8f3a33fa":"code","c085b01e":"code","a156804a":"code","9ae566af":"code","90775213":"code","6452a79a":"code","b78765bc":"code","953f890d":"code","ffe09881":"code","99c05481":"code","5af153e5":"code","4b252564":"code","eb5e7446":"code","2a4fa37d":"code","c847ade5":"code","82d12236":"code","905231ca":"code","cab19b8d":"code","02693e27":"code","552a176c":"code","f4a006f3":"code","e0368796":"markdown","af7ae4a8":"markdown","695c2d09":"markdown","6e9370a7":"markdown","319f967a":"markdown","fb2c0a04":"markdown","2c7b5090":"markdown","c10c226d":"markdown","6288d5b9":"markdown","fcdae354":"markdown","4d9816cf":"markdown","2f5f6163":"markdown","951084bb":"markdown","93e895a2":"markdown","1e774630":"markdown","3f6ab958":"markdown","e4ed3f68":"markdown","ae5a1c21":"markdown","f7b12e38":"markdown","a61eb98e":"markdown","a7e4f1f4":"markdown","d2171d2f":"markdown","ae36e30a":"markdown","7d1df445":"markdown","424f388d":"markdown","4efc522b":"markdown","b3479af4":"markdown","7fa823da":"markdown","89728898":"markdown","0da924b4":"markdown","0b9aff18":"markdown","e8fc55d9":"markdown","d9ea5fe9":"markdown","28b92a76":"markdown","7cdc5824":"markdown"},"source":{"1ce98d4b":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 12\nMAX_TREES = 2000\nEARLY_STOP = 50\nNUM_TRIALS = 100\nSUBMIT = True","f8624cef":"# Essentials\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport time\nfrom collections import defaultdict\n\n# Models\nfrom sklearn.base import clone\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Model Evaluation\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessing\nfrom functools import partial, reduce\nfrom sklearn.impute import SimpleImputer\nfrom category_encoders import OrdinalEncoder, OneHotEncoder\n\n# Feature Engineering\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom category_encoders import MEstimateEncoder\n\n# Hyperparameter Tuning\nimport optuna\nfrom optuna.visualization import plot_param_importances, plot_parallel_coordinate\nfrom optuna.pruners import PercentilePruner\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","93d42103":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")\ntest = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/home-data-for-ml-course\/sample_submission.csv\")\n\n# Remove rows with missing target\ntrain.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# Columns of interest\nfeatures = [x for x in train.columns if x not in ['SalePrice','Id']]\ncategorical = [x for x in features if train[x].dtype == \"object\"]\nnumerical = [x for x in features if  train[x].dtype in ['int64', 'float64']]\n\n# Bin target for stratified cross-validation\nbinner = KBinsDiscretizer(n_bins = 45, encode = 'ordinal', strategy = 'quantile')\ny_bins = binner.fit_transform(pd.DataFrame(data=train['SalePrice']))","d8541c34":"def data_cleaning(input_df):\n    df = input_df.copy()\n    \n    # Data cleaning: fix typos and bad values\n    df['MSZoning'] =  df['MSZoning'].replace({'C (all)': 'C'})\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\":\"BrkComm\",\"Wd Shng\": \"WdShing\"})\n    df['Neighborhood'] = df['Neighborhood'].replace({'NAmes':'Names'})\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    #df[\"MSClass\"] = df['MSZoning'].map({'A': 'A','C': 'C',\"FV\": 'R','I': 'I',\"RH\": 'R',\"RL\": 'R',\"RP\": 'R',\"RM\": 'R', np.nan:np.nan})\n    \n    return df","76b7b4fb":"# Transformations that depend only on the input data (no fear of leakage)\ndef imputation(X_train, X_valid, X_test = None, num_strategy = 'mean', cat_strategy = 'constant'):\n\n    X_train, X_valid = X_train.copy(), X_valid.copy()\n    if X_test is not None: \n        X_test = X_test.copy()\n    \n    # 1. impute numerical data\n    assert num_strategy in ['median','mean']\n    columns = [col for col in X_train.columns if X_train[col].dtype != \"object\"]\n    num_imputer = SimpleImputer(strategy = num_strategy)\n    X_train[columns] = num_imputer.fit_transform(X_train[columns])\n    X_valid[columns] = num_imputer.transform(X_valid[columns])\n    if X_test is not None:\n        X_test[columns] = num_imputer.transform(X_test[columns])\n    \n    # 2. impute categorical data\n    assert cat_strategy in ['constant','most_frequent']\n    cat_imputer = SimpleImputer(strategy = cat_strategy, fill_value = 'None')\n    columns = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n    X_train[columns] = cat_imputer.fit_transform(X_train[columns])\n    X_valid[columns] = cat_imputer.transform(X_valid[columns])\n    if X_test is not None:\n        X_test[columns] = cat_imputer.transform(X_test[columns])\n        \n    return X_train, X_valid, X_test","f0681ad6":"def ordinal_encoding(X_train, X_valid, X_test = None):\n    \n    X_train, X_valid = X_train.copy(), X_valid.copy()\n    if X_test is not None: \n        X_test = X_test.copy()\n    \n    # 1. Encode 1-10 ratings\n    cols = [\"OverallQual\",\"OverallCond\"]\n    cols = [x for x in cols if x in X_train.columns]\n    ratings = {float(a):b for b,a in enumerate(range(1,11))}\n    mapping = [{'col':x, 'mapping': ratings} for x in cols]\n    \n    encoder = OrdinalEncoder(cols = cols, mapping = mapping, handle_missing = 'return_nan')\n    X_train = encoder.fit_transform(X_train)\n    X_valid = encoder.transform(X_valid)\n    if X_test is not None: \n        X_test = encoder.transform(X_test)\n    \n    # 2. Encode Poor, Fair, Avg, Good, Ex ratings\n    cols = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"HeatingQC\", \"KitchenQual\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",'PoolQC']\n    cols = [x for x in cols if x in X_train.columns]\n    ratings = {\"Po\":0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4}\n    mapping = [{'col':x, 'mapping': ratings} for x in cols]\n    \n    encoder = OrdinalEncoder(cols = cols, mapping = mapping, handle_missing = 'return_nan')\n    X_train = encoder.fit_transform(X_train)\n    X_valid = encoder.transform(X_valid)\n    if X_test is not None: \n        X_test = encoder.transform(X_test)\n    \n    # 3. Encode remaining ordinal data\n    cols = [\"LotShape\",\"LandSlope\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\n    \"Functional\",\"GarageFinish\",\"PavedDrive\",\"Utilities\",\"CentralAir\",\"Electrical\",\n    \"Fence\"]\n    cols = [x for x in cols if x in X_train.columns]\n    mapping = [{'col':\"LotShape\",\n                'mapping': {\"Reg\":0, \"IR1\":1, \"IR2\":2, \"IR3\":3}},\n               {'col':\"LandSlope\",\n                'mapping': {\"Sev\":0, \"Mod\":1, \"Gtl\":2}},\n               {'col':\"BsmtExposure\",\n                'mapping': {\"No\":0, \"Mn\":1, \"Av\":2, \"Gd\":3}},\n               {'col':\"BsmtFinType1\",\n                'mapping': {\"Unf\":0, \"LwQ\":1, \"Rec\":2, \"BLQ\":3, \"ALQ\":4, \"GLQ\":5}},\n               {'col':\"BsmtFinType2\",\n                'mapping': {\"Unf\":0, \"LwQ\":1, \"Rec\":2, \"BLQ\":3, \"ALQ\":4, \"GLQ\":5}},\n               {'col':\"Functional\",\n                'mapping': {\"Sal\":0, \"Sev\":1, \"Maj1\":2, \"Maj2\":3, \"Mod\":4, \"Min2\":5, \"Min1\":6, \"Typ\":7}},\n               {'col':\"GarageFinish\",\n                'mapping': {\"Unf\":0, \"RFn\":1, \"Fin\":2}},\n               {'col':\"PavedDrive\",\n                'mapping': {\"N\":0, \"P\":1, \"Y\":2}},\n               {'col':\"Utilities\",\n                'mapping': {\"NoSeWa\":0, \"NoSewr\":1, \"AllPub\":2}},\n               {'col':\"CentralAir\",\n                'mapping': {\"N\":0, \"Y\":1}},\n               {'col':\"Electrical\",\n                'mapping': {\"Mix\":0, \"FuseP\":1, \"FuseF\":2, \"FuseA\":3, \"SBrkr\":4}},\n               {'col':\"Fence\",\n                'mapping': {\"MnWw\":0, \"GdWo\":1, \"MnPrv\":2, \"GdPrv\":3}}]\n    mapping = [x for x in mapping if x['col'] in X_train.columns]\n    \n    encoder = OrdinalEncoder(cols = cols, mapping = mapping, handle_missing = 'return_nan')\n    X_train = encoder.fit_transform(X_train)\n    X_valid = encoder.transform(X_valid)\n    if X_test is not None: \n        X_test = encoder.transform(X_test)\n        \n    return X_train, X_valid, X_test","732a71a7":"# Not ordinal categorical data\n#columns = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \n#           \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \n#           \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \n#           \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\ndef nominal_encoding(X_train, X_valid, X_test = None, threshold = 10):\n    \n    X_train, X_valid = X_train.copy(), X_valid.copy()\n    if X_test is not None: \n        X_test = X_test.copy()\n    \n    # 1. Determine high and low cardinality data\n    columns = [col for col in X_train.columns if X_train[col].dtype == 'object']\n    high_cols = [col for col in columns if X_train[col].nunique() >= threshold]\n    low_cols = [col for col in columns if X_train[col].nunique() < threshold]\n    \n    # label encode high cardinality data\n    if high_cols:\n        encoder = OrdinalEncoder(cols = high_cols, handle_missing = 'return_nan')\n        X_train = encoder.fit_transform(X_train)\n        X_valid = encoder.transform(X_valid)\n        if X_test is not None: \n            X_test = encoder.transform(X_test)\n    \n    if low_cols:\n        encoder = OneHotEncoder(cols = low_cols, use_cat_names = True, handle_missing = 'return_nan')\n        X_train = encoder.fit_transform(X_train)\n        X_valid = encoder.transform(X_valid)\n        if X_test is not None: \n            X_test = encoder.transform(X_test)\n        \n    return X_train, X_valid, X_test","8f8aa6e2":"def preprocessing(X_train, X_valid, X_test = None):\n    \n    # 1. Data cleaning\n    X_train = data_cleaning(X_train)\n    X_valid = data_cleaning(X_valid)\n    if X_test is not None: \n        X_test = data_cleaning(X_test)\n        \n    # 2. Imputation\n    X_train, X_valid, X_test = imputation(X_train, X_valid, X_test)\n        \n    # 3. Ordinal Encoding\n    X_train, X_valid, X_test = ordinal_encoding(X_train, X_valid, X_test)\n    \n    # 4. Nominal Encoding\n    X_train, X_valid, X_test = nominal_encoding(X_train, X_valid, X_test)\n        \n    return X_train, X_valid, X_test","4f84326d":"def score_xgboost(xgb_model = XGBRegressor(random_state = RANDOM_SEED, n_estimators = 500, learning_rate = 0.05), processing = preprocessing, trial = None, verbose = True):\n    \n    # Drop high cardinality categorical variables\n    features = [x for x in train.columns if x not in ['Id','SalePrice']]\n    X_temp = train[features].copy()\n    y_temp = train['SalePrice'].copy()\n    \n    # Data structure for storing scores and times\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    kfold = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X_temp, y_bins)):\n        \n        # Training and Validation Sets\n        X_train, X_valid = X_temp.iloc[train_idx], X_temp.iloc[valid_idx]\n        y_train, y_valid = y_temp.iloc[train_idx], y_temp.iloc[valid_idx]\n        \n        # Preprocessing\n        start = time.time()\n        X_train, X_valid, _ = processing(X_train, X_valid)\n        \n        # Create model\n        model = clone(xgb_model)\n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds=EARLY_STOP,\n            eval_set=[(X_valid, y_valid)], \n            verbose=False\n        )\n        \n        # validation predictions\n        valid_preds = np.ravel(model.predict(X_valid))\n        scores[fold] = mean_absolute_error(y_valid, valid_preds)\n        end = time.time()\n        times[fold] = end - start\n        time.sleep(0.5)\n        \n        if trial:\n            # Use pruning on fold AUC\n            trial.report(\n                value = scores[fold],\n                step = fold\n            )\n            # prune slow trials and bad fold AUCs\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n    \n    if verbose: print(f'\\n{NUM_FOLDS}-Fold Average MAE: {round(scores.mean(), 5)} in {round(times.sum(),2)}s.\\n')\n\n    \n    return scores.mean()","fbd2b083":"# Data structure for saving our training scores\nbenchmarks = defaultdict(list)\n\n# Baseline score\nscore = score_xgboost()\n\n# Save scores\nbenchmarks['feature'].append('Baseline')\nbenchmarks['score'].append(score)","c5207cbe":"def get_mi_scores():\n    \n    # Preprocessing\n    columns = [x for x in train.columns if x not in ['Id','SalePrice']]\n    X_train, X_test, _ = preprocessing(train[columns], test[columns])\n    discrete = [i for i,x in enumerate(X_train.columns) if x not in numerical]\n    y_train = train['SalePrice']\n    \n    # Get Score\n    scores = mutual_info_regression(\n        X_train, y_train, discrete_features = discrete\n    )\n    scores = pd.Series(scores, name = \"MI Scores\", index = X_train.columns)\n    return scores.sort_values(ascending=False)","3ffa2584":"# Get sorted list of features\nget_mi_scores()","822ae45c":"def plot_informative(threshold = 0.1):\n    scores = get_mi_scores()\n    scores.drop(scores[scores <= threshold].index, inplace = True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.figure(dpi=100, figsize=(8,6))\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","978e6b72":"plot_informative(threshold = 0.1)","0f7fc351":"# X_train must have no NA values\ndef remove_uninformative(X_train, X_valid, X_test = None, threshold = 1e-5, verbose = False):\n    \n    # 0. Preprocessing\n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    \n    # 1. Get discrete columns and target\n    discrete = [i for i,x in enumerate(X_train.columns) if x not in numerical]\n    y_train = train['SalePrice'].iloc[X_train.index]\n    \n    # 2. Get mutual information scores\n    scores = mutual_info_regression(X_train, y_train, discrete_features = discrete)\n    cols = [x for i, x in enumerate(X_train.columns) if scores[i] < threshold]\n    \n    # 3. Drop the uninformative columns\n    X_train.drop(cols, axis = 1, inplace = True)\n    X_valid.drop(cols, axis = 1, inplace = True)\n    if X_test is not None: X_test.drop(cols, axis = 1, inplace = True)\n    \n    if verbose:\n        print(\"Dropped columns:\", *cols)\n    \n    return X_train, X_valid, X_test","b860c520":"# Drop uninformative\nscore = score_xgboost(processing = remove_uninformative)\n\n# Save scores\nbenchmarks['feature'].append('Mutual_Info')\nbenchmarks['score'].append(score)","59220bb2":"def transformations(input_df, test_data = False):\n    \n    df = input_df.copy()\n    temp = train.iloc[df.index]\n    if test_data: temp = test.copy()\n        \n    df[\"LivLotRatio\"] = temp[\"GrLivArea\"] \/ temp[\"LotArea\"]\n    df[\"Spaciousness\"] = (temp[\"1stFlrSF\"]+temp[\"2ndFlrSF\"]) \/ temp[\"TotRmsAbvGrd\"]\n    df[\"TotalOutsideSF\"] = temp[\"WoodDeckSF\"] + temp[\"OpenPorchSF\"] + temp[\"EnclosedPorch\"] + temp[\"3SsnPorch\"] + temp[\"ScreenPorch\"]\n    df['TotalLot'] = temp['LotFrontage'] + temp['LotArea']\n    df['TotalBsmtFin'] = temp['BsmtFinSF1'] + temp['BsmtFinSF2']\n    df['TotalSF'] = temp['TotalBsmtSF'] + temp['2ndFlrSF'] + temp['1stFlrSF']\n    df['TotalBath'] = temp['FullBath'] + temp['HalfBath'] * 0.5 + temp['BsmtFullBath'] + temp['BsmtHalfBath'] * 0.5\n    df['TotalPorch'] = temp['OpenPorchSF'] + temp['EnclosedPorch'] + temp['ScreenPorch'] + temp['WoodDeckSF']\n    \n    return df\n    \ndef mathematical_transformations(X_train, X_valid, X_test = None):\n    \n    # 0. Preprocessing\n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    \n    X_train = transformations(X_train)\n    X_valid = transformations(X_valid)\n    if X_test is not None: X_test = transformations(X_test)\n    \n    return X_train, X_valid, X_test","0e9f15e2":"# Mathematical transformations\nscore = score_xgboost(processing = mathematical_transformations)\n\n# Save scores\nbenchmarks['feature'].append('Transformations')\nbenchmarks['score'].append(score)","af99dfd9":"def interaction(input_df, cat_col = \"BldgType\", num_col = \"GrLivArea\"):\n    \n    df = input_df.copy()\n    try:\n        # will fail if column already one-hot encoded\n        X = pd.get_dummies(df[cat_col], prefix=cat_col)\n        for col in X.columns:\n            df[col+\"_\"+num_col] = X[col]*df[num_col]\n    except:\n        # if column already one-hot encoded\n        for col in df.columns:\n            if col.startswith(cat_col):\n                df[col+\"_\"+num_col] = df[col]*df[num_col]\n    return df\n\ndef encode_interaction(X_train, X_valid, X_test = None, cat_col = \"BldgType\", num_col = \"GrLivArea\"):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n\n    X_train = interaction(X_train, cat_col, num_col)\n    X_valid = interaction(X_valid, cat_col, num_col)\n    if X_test is not None: X_test = interaction(X_test, cat_col, num_col)\n    \n    return X_train, X_valid, X_test","6ec48902":"# Categorical interactions\nscore = score_xgboost(processing = encode_interaction)\n\n# Save scores\nbenchmarks['feature'].append('Interactions')\nbenchmarks['score'].append(score)","10686ca6":"def count_features(X_train, X_valid, X_test = None, features = [\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"]):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    \n    X_train[\"PorchTypes\"] = X_train[features].gt(0).sum(axis=1)\n    X_valid[\"PorchTypes\"] = X_valid[features].gt(0).sum(axis=1)\n    if X_test is not None: X_test[\"PorchTypes\"] = X_test[features].gt(0).sum(axis=1)\n        \n    return X_train, X_valid, X_test","67c79b68":"# New count features\nscore = score_xgboost(processing = count_features)\n\n# Save scores\nbenchmarks['feature'].append('Count')\nbenchmarks['score'].append(score)","94f7ad8b":"def breakdown_zoning(X_train, X_valid, X_test = None):\n    \n    mapping = {'A': 'A','C': 'C',\"FV\": 'R','I': 'I',\"RH\": 'R',\"RL\": 'R',\"RP\": 'R',\"RM\": 'R', np.nan:np.nan}\n    \n    X_train[\"MSClass\"] = train['MSZoning'].iloc[X_train.index].map(mapping)\n    X_valid[\"MSClass\"] = train['MSZoning'].iloc[X_valid.index].map(mapping)\n    if X_test is not None: X_test[\"MSClass\"] = test['MSZoning'].map(mapping)\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    \n    return X_train, X_valid, X_test","de659564":"# New count features\nscore = score_xgboost(processing = breakdown_zoning)\n\n# Save scores\nbenchmarks['feature'].append('MSZoning')\nbenchmarks['score'].append(score)","4eca591e":"def group_transformation(X_train, X_valid, X_test = None):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    X_train[\"MedNhbdLvArea\"] = X_train.groupby(\"Neighborhood\")[\"GrLivArea\"].transform('median')\n    \n    # we use the medians from the training data to impute the test data\n    mapping = {y:x for x,y in zip(X_train[\"MedNhbdLvArea\"].values, X_train['Neighborhood'].values)}\n    \n    X_valid[\"MedNhbdLvArea\"] = X_valid['Neighborhood'].map(mapping)\n    if X_test is not None: X_test[\"MedNhbdLvArea\"] = X_test['Neighborhood'].map(mapping)\n    \n    return X_train, X_valid, X_test","70700e16":"# New count features\nscore = score_xgboost(processing = group_transformation)\n\n# Save scores\nbenchmarks['feature'].append('Group')\nbenchmarks['score'].append(score)","117b5bc7":"def cluster_labels(X_train, X_valid, X_test = None, name = \"Area\", features = ['LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF','GrLivArea']):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    \n    # 1. normalize based on training data\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_train[features])\n    X_valid_scaled = scaler.transform(X_valid[features])\n    if X_test is not None: X_test_scaled = scaler.transform(X_test[features])\n    \n    # 2. create cluster labels (use predict)\n    kmeans = KMeans(n_clusters = 10, n_init = 10, random_state=0)\n    X_train[name + \"_Cluster\"] = kmeans.fit_predict(X_scaled)\n    X_valid[name + \"_Cluster\"] = kmeans.predict(X_valid_scaled)\n    if X_test is not None: X_test[name + \"_Cluster\"] = kmeans.predict(X_test_scaled)\n         \n    return X_train, X_valid, X_test","f8501e93":"# Cluster label features\nscore = score_xgboost(processing = cluster_labels)\n\n# Save scores\nbenchmarks['feature'].append('Cluster_Labels')\nbenchmarks['score'].append(score)","08ef6420":"def cluster_distances(X_train, X_valid, X_test = None, name = \"Area\", features = ['LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF','GrLivArea']):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    \n    # 1. normalize based on training data\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_train[features])\n    X_valid_scaled = scaler.transform(X_valid[features])\n    if X_test is not None: X_test_scaled = scaler.transform(X_test[features])\n    \n    # 2. generate cluster distances (use transform)\n    kmeans = KMeans(n_clusters = 10, n_init = 10, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    X_valid_cd = kmeans.transform(X_valid_scaled)\n    if X_test is not None: X_test_cd = kmeans.transform(X_test_scaled)\n    \n    # 3. column labels\n    X_cd = pd.DataFrame(X_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_cd.shape[1])])\n    X_valid_cd = pd.DataFrame(X_valid_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_valid_cd.shape[1])])\n    if X_test is not None: X_test_cd = pd.DataFrame(X_test_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_test_cd.shape[1])])    \n    \n    if X_test is not None:\n        return X_train.join(X_cd), X_valid.join(X_valid_cd), X_test.join(X_test_cd)\n    \n    return X_train.join(X_cd), X_valid.join(X_valid_cd), X_test","bcbff220":"# Cluster distance features\nscore = score_xgboost(processing = cluster_distances)\n\n# Save scores\nbenchmarks['feature'].append('Cluster_Dist')\nbenchmarks['score'].append(score)","afccb974":"# Assumes data is standardized\ndef apply_pca(X):\n    \n    # Standardize input\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_scaled = pd.DataFrame(X_scaled, columns = X.columns)\n    \n    pca = PCA()\n    X_pca = pca.fit_transform(X_scaled)\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X_scaled.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings","b869530b":"def plot_variance(X, width=8, dpi=100):\n\n    pca, _, _ = apply_pca(X)\n    \n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    \n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(-0.1, 1.1)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(-0.1, 1.1)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)","57ec8e1b":"plot_variance(\n    train[[\"GarageArea\",\"YearRemodAdd\",\"TotalBsmtSF\",\"GrLivArea\"]]\n)","def16488":"pca, X_pca, loadings = apply_pca(\n    train[[\"GarageArea\",\"YearRemodAdd\",\"TotalBsmtSF\",\"GrLivArea\"]]\n)\nloadings","8f3a33fa":"def check_outliers(X_pca, component = \"PC1\"):\n    idx = X_pca[component].sort_values(ascending=False).index\n    return train.loc[idx, [\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + features].head(10)","c085b01e":"check_outliers(X_pca, component = \"PC1\")","a156804a":"# Performs PCA on the whole dataframe\ndef pca_transform(X_train, X_valid, X_test = None, features = [\"GarageArea\",\"YearRemodAdd\",\"TotalBsmtSF\",\"GrLivArea\"], n_components = 2):\n    \n    assert n_components <= len(features)\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    \n    # Normalize based on training data\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_train[features])\n    X_valid_scaled = scaler.transform(X_valid[features])\n    if X_test is not None: X_test_scaled = scaler.transform(X_test[features])\n    \n    # Create principal components\n    pca = PCA(n_components)\n    X_pca = pca.fit_transform(X_scaled)\n    X_valid_pca = pca.transform(X_valid_scaled)\n    if X_test is not None: X_test_pca = pca.transform(X_test_scaled)\n    \n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    X_valid_pca = pd.DataFrame(X_valid_pca, columns=component_names)\n    if X_test is not None: X_test_pca = pd.DataFrame(X_test_pca, columns=component_names)\n    \n    if X_test is not None: \n        return X_train.join(X_pca), X_valid.join(X_valid_pca), X_test.join(X_test_pca)\n\n    return X_train.join(X_pca), X_valid.join(X_valid_pca), X_test","9ae566af":"# PCA components\nscore = score_xgboost(processing = pca_transform)\n\n# Save scores\nbenchmarks['feature'].append('PCA_Comps')\nbenchmarks['score'].append(score)","90775213":"def pca_inspired(X_train, X_valid, X_test = None):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    \n    X_train[\"Feature1\"] = X_train.GrLivArea + X_train.TotalBsmtSF\n    X_train[\"Feature2\"] = X_train.YearRemodAdd * X_train.TotalBsmtSF\n    X_valid[\"Feature1\"] = X_valid.GrLivArea + X_valid.TotalBsmtSF\n    X_valid[\"Feature2\"] = X_valid.YearRemodAdd * X_valid.TotalBsmtSF\n    if X_test is not None:\n        X_test[\"Feature1\"] = X_test.GrLivArea + X_test.TotalBsmtSF\n        X_test[\"Feature2\"] = X_test.YearRemodAdd * X_test.TotalBsmtSF\n    \n    return X_train, X_valid, X_test","6452a79a":"# PCA inspired features\nscore = score_xgboost(processing = pca_inspired)\n\n# Save scores\nbenchmarks['feature'].append('PCA_Inspired')\nbenchmarks['score'].append(score)","b78765bc":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","953f890d":"def encode_neighborhood(X_train, X_valid, X_test = None):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    y_train = train['SalePrice'].iloc[X_train.index]\n    \n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X1_train = encoder.fit_transform(X_train, y_train, cols=[\"Neighborhood\"])\n    X1_valid = encoder.transform(X_valid)\n    if X_test is not None: X1_test = encoder.transform(X_test)\n    \n    if X_test is not None:\n        return X_train.join(X1_train), X_valid.join(X1_valid), X_test.join(X1_test)\n\n    return X_train.join(X1_train), X_valid.join(X1_valid), X_test","ffe09881":"score = score_xgboost(processing = encode_neighborhood)\n\nbenchmarks['feature'].append('Encode_Neighborhood')\nbenchmarks['score'].append(score)","99c05481":"def encode_subclass(X_train, X_valid, X_test = None):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    y_train = train['SalePrice'].iloc[X_train.index]\n    \n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X1_train = encoder.fit_transform(X_train, y_train, cols=[\"MSSubClass\"])\n    X1_valid = encoder.transform(X_valid)\n    if X_test is not None: X1_test = encoder.transform(X_test)\n        \n    if X_test is not None:\n        return X_train.join(X1_train), X_valid.join(X1_valid), X_test.join(X1_test)\n\n    return X_train.join(X1_train), X_valid.join(X1_valid), X_test","5af153e5":"score = score_xgboost(processing = encode_subclass)\n\nbenchmarks['feature'].append('Encode_Subclass')\nbenchmarks['score'].append(score)","4b252564":"pd.DataFrame(benchmarks).sort_values('score', ascending = False)","eb5e7446":"def feature_engineering(X_train, X_valid, X_test = None):\n    \n    X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n    y_train = train['SalePrice'].iloc[X_train.index]\n    og_columns = [x for x in X_train.columns]\n    \n    # Drop the uninformative columns\n    discrete = [i for i,x in enumerate(X_train.columns) if x not in numerical]\n    scores = mutual_info_regression(X_train, y_train, discrete_features = discrete)\n    cols = [x for i, x in enumerate(X_train.columns) if scores[i] < 1e-5]\n    X_train.drop(cols, axis = 1, inplace = True)\n    X_valid.drop(cols, axis = 1, inplace = True)\n    if X_test is not None: \n        X_test.drop(cols, axis = 1, inplace = True)\n    \n    # Cluster Labels\n    features = ['LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF','GrLivArea']\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_train[features])\n    X_valid_scaled = scaler.transform(X_valid[features])\n    if X_test is not None: \n        X_test_scaled = scaler.transform(X_test[features])\n    kmeans = KMeans(n_clusters = 10, n_init = 10, random_state=0)\n    X_train[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    X_valid[\"Cluster\"] = kmeans.predict(X_valid_scaled)\n    if X_test is not None: \n        X_test[\"Cluster\"] = kmeans.predict(X_test_scaled)\n    \n    # Group Transformation\n    X_train[\"MedNhbdLvArea\"] = X_train.groupby(\"Neighborhood\")[\"GrLivArea\"].transform('median')\n    mapping = {y:x for x,y in zip(X_train[\"MedNhbdLvArea\"].values, X_train['Neighborhood'].values)}\n    X_valid[\"MedNhbdLvArea\"] = X_valid['Neighborhood'].map(mapping)\n    if X_test is not None: \n        X_test[\"MedNhbdLvArea\"] = X_test['Neighborhood'].map(mapping)\n    \n    # PCA Inspired\n    X_train[\"Feature1\"] = X_train.GrLivArea + X_train.TotalBsmtSF\n    X_train[\"Feature2\"] = X_train.YearRemodAdd * X_train.TotalBsmtSF\n    X_valid[\"Feature1\"] = X_valid.GrLivArea + X_valid.TotalBsmtSF\n    X_valid[\"Feature2\"] = X_valid.YearRemodAdd * X_valid.TotalBsmtSF\n    if X_test is not None:\n        X_test[\"Feature1\"] = X_test.GrLivArea + X_test.TotalBsmtSF\n        X_test[\"Feature2\"] = X_test.YearRemodAdd * X_test.TotalBsmtSF\n    \n    # Transformations\n    X_train = transformations(X_train)\n    X_valid = transformations(X_valid)\n    if X_test is not None: \n        X_test = transformations(X_test, test_data = True)\n        \n    \n    # Target Encode Subclass\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X1_train = encoder.fit_transform(X_train, y_train, cols=[\"MSSubClass\"])\n    X1_valid = encoder.transform(X_valid)\n    if X_test is not None: \n        X1_test = encoder.transform(X_test)\n        \n    if X_test is not None:\n        X_train, X_valid, X_test = X_train.join(X1_train), X_valid.join(X1_valid), X_test.join(X1_test)\n    else:\n        X_train, X_valid = X_train.join(X1_train), X_valid.join(X1_valid)\n        \n    # Target Encode Neighborhood\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X2_train = encoder.fit_transform(X_train, y_train, cols=[\"Neighborhood\"])\n    X2_valid = encoder.transform(X_valid)\n    if X_test is not None: \n        X2_test = encoder.transform(X_test)\n        \n    if X_test is not None:\n        return X_train.join(X2_train), X_valid.join(X2_valid), X_test.join(X2_test)\n\n    return X_train.join(X2_train), X_valid.join(X2_valid), X_test","2a4fa37d":"_ = score_xgboost(processing = feature_engineering)","c847ade5":"# Tweak Pruner settings\npruner = PercentilePruner(\n    percentile = 33,\n    n_startup_trials = 10,\n    n_warmup_steps = 0,\n    interval_steps = 1,\n    n_min_trials = 10,\n)","82d12236":"def parameter_search(trials):\n    \n    # Optuna objective function\n    def objective(trial):\n        \n        model_params = dict( \n            # default 6\n            max_depth = trial.suggest_int(\n                \"max_depth\", 2, 12\n            ), \n            # default 0.3\n            learning_rate = trial.suggest_loguniform(\n                \"learning_rate\", 0.01, 0.3\n            ),\n            # default 0\n            gamma = trial.suggest_loguniform(\n                \"gamma\", 1e-10, 100\n            ), \n            # default 1\n            min_child_weight = trial.suggest_loguniform(\n                \"min_child_weight\", 1e-2, 1e2\n            ),\n            # default 1\n            subsample = trial.suggest_discrete_uniform(\n                \"subsample\", 0.2, 1.0, 0.01\n            ),\n            # default 1\n            colsample_bytree = trial.suggest_discrete_uniform(\n                \"colsample_bytree\",  0.2, 1.0, 0.01\n            ),\n            # default 1\n            colsample_bylevel = trial.suggest_discrete_uniform(\n                \"colsample_bylevel\",  0.2, 1.0, 0.01\n            ),\n            # default 1\n            reg_lambda = trial.suggest_loguniform(\n                \"reg_lambda\", 1e-10, 100\n            ),\n            # default 0\n            reg_alpha = trial.suggest_loguniform(\n                \"reg_alpha\", 1e-10, 100\n            ),\n        )\n        \n        return score_xgboost(\n            xgb_model = XGBRegressor(\n                random_state = RANDOM_SEED, \n                n_estimators = MAX_TREES,\n                n_jobs = 4,\n                **model_params\n            ),\n            processing = feature_engineering,\n            trial = trial\n        )\n    \n    \n    optuna.logging.set_verbosity(optuna.logging.DEBUG)\n    study = optuna.create_study(\n        pruner = pruner,\n        direction = \"minimize\"\n    )\n    # close to default parameters\n    study.enqueue_trial({\n        'max_depth': 6, \n        'learning_rate': 0.05, \n        'gamma': 1e-10,\n        'min_child_weight': 1, \n        'subsample': 1, \n        'colsample_bytree': 1, \n        'colsample_bylevel': 1,\n        'reg_lambda': 1,\n        'reg_alpha': 1,\n    })\n    study.optimize(objective, n_trials=trials)\n    return study","905231ca":"study = parameter_search(NUM_TRIALS)","cab19b8d":"print(\"Best Parameters:\", study.best_params)","02693e27":"plot_parallel_coordinate(study)","552a176c":"def make_submission(model_params):\n    \n    # Features\n    features = [x for x in train.columns if x not in ['Id','SalePrice']]\n    X_temp = train[features].copy()\n    y_temp = train['SalePrice'].copy()\n    \n    # Data structure for storing scores and times\n    test_preds = np.zeros((test.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    kfold = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X_temp, y_bins)):\n        \n        # Training and Validation Sets\n        X_train, X_valid = X_temp.iloc[train_idx], X_temp.iloc[valid_idx]\n        y_train, y_valid = y_temp.iloc[train_idx], y_temp.iloc[valid_idx]\n        X_test = test[features].copy()\n        \n        # Preprocessing\n        X_train, X_valid, X_test = feature_engineering(X_train, X_valid, X_test)\n        \n        # Create model\n        start = time.time()\n        model = XGBRegressor(\n            random_state = RANDOM_SEED, \n            n_estimators = MAX_TREES,\n            n_jobs = 4,\n            **model_params\n        )\n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds=EARLY_STOP,\n            eval_set=[(X_valid, y_valid)], \n            verbose=False\n        )\n        \n        # validation predictions\n        valid_preds = np.ravel(model.predict(X_valid))\n        test_preds += model.predict(X_test) \/ NUM_FOLDS\n        scores[fold] = mean_absolute_error(y_valid, valid_preds)\n        end = time.time()\n        times[fold] = end - start\n        print(f'Fold {fold} MAE: {round(scores[fold], 5)} in {round(end-start,2)}s.')\n        time.sleep(0.5)\n        \n    print(f'\\n{NUM_FOLDS}-Fold Average MAE: {round(scores.mean(), 5)} in {round(times.sum(),2)}s.')\n\n    output = pd.DataFrame({'Id': test.Id,'SalePrice': test_preds})\n    output.to_csv('new_submission.csv', index=False)","f4a006f3":"# Make final submission\nmake_submission(study.best_params)","e0368796":"## 2. XGBoost Baseline\n\nThis model performs no feature engineering other than the preprocessing defined in the previous section.","af7ae4a8":"## 5.4 PCA Features\n\nThe following function adds the first two principal components as features to our original data:","695c2d09":"# Lesson 5: Principal Component Analysis\n\nFollowing the bonus notebook, we use PCA to examine the following four features:  `GarageArea`, `YearRemodAdd`, `TotalBsmtSF`, and `GrLivArea`. ","6e9370a7":"Thanks for reading, I hope you found this useful!","319f967a":"## 3.3 Generate a Count Feature\n\nWe combine several related features into an aggregate feature counting the presence of the components.","fb2c0a04":"## Final Feature Engineering\n\nWe include all of the techniques which resulted in improvements over the baseline.","2c7b5090":"## Hyperparameter Evaluation","c10c226d":"# Feature Engineering\n\nIn this section we consider the feature engineering techniques covered in the course and benchmark.\n\n## 1. Scoring Function\n\nThis function takes a function which transforms our data (e.g. preprocessing or feature engineering) and scores it using cross-validation and returns the mean absolute error (MAE).","6288d5b9":"## 3. Encoding Ordinal Variables\n\nA few of our variables are based on ratings (e.g. poor, fair, good) of various qualities of the property. We hard code this ordering:","fcdae354":"## 3.4 Break Down a Categorical Feature\n\nNote: The column `HouseStyle` already serves the purpose of the `MSClass` feature in the notes, so we create `MSClass` use the `Zoning` column instead:","4d9816cf":"# Generate Submission","2f5f6163":"## 2.2 Plot Informative Features\n\nThe following function plots all the features with mutual information scores above a threshold (0.1 by default):","951084bb":"## 5.1 Plot Variance\n\nThe following function adapted from the notes plots the explained variance and culmulative variance of each component.","93e895a2":"# Determining Feature Engineering Strategy\n\nIn this section we compare all the above strategies versus the baseline and choose which we will use in our final model.","1e774630":"## 5.5 PCA Inspired Features\n\nFollowing the bonus notebook, adds two new features inspired by the principal component analysis:","3f6ab958":"## Load Data","e4ed3f68":"# Lesson 4: K-means Clustering\n\nIn this section we use unsupervised clustering techniques to build new features for our model.\n\n## 4.1 Cluster Label Features\n\nWe create a new feature by clustering a subset of the numerical data and using the resulting cluster labels to create a new categorical feature.","ae5a1c21":"## Search Function\n\nFunction which actually performs the hyperparameter search, based on [optuna](https:\/\/optuna.org\/):","f7b12e38":"## 3.5 Use a Grouped Transform\n\nWe create a feature from a statistic calculated on a group. In this case the above ground living area per neighborhood. Note that the statistic is calculated on the training data only.","a61eb98e":"## 10.2 Encode Subclass\n\nThe example in the notes uses `MSSubClass` for target encoding so we test it out here for sake of completeness.","a7e4f1f4":"## 6.1 Encode Neighborhood\n\nWe use `Neighborhood` to target encode since it is a high cardinality nominal feature which is likely very important for determining the target `SalePrice`.","d2171d2f":"# Lesson 6: Target Encoding\n\nWe use the wrapper from the [bonus notebook](https:\/\/www.kaggle.com\/ryanholbrook\/feature-engineering-for-house-prices) to target encode a few categorical variables.","ae36e30a":"## 3.2 Encode Feature Interactions\n\nAttempt to encode an interaction between a categorical variable and a numerical variable","7d1df445":"## 2.3 Drop Uninformative Features\n\nA function which transforms our data by removing features with low mutual information scores:","424f388d":"## 5.3 Outlier Detection\n\nFind observations with outlier values in each component, this can be useful for finding outlier values that might not be readily apparent in the original feature space:","4efc522b":"## 2. Imputation\n\nWe replace numerical NA values with the column mean and categorical NAs with a placeholder value.","b3479af4":"## 4.2 Cluster Distance Features\n\nWe create new numerical features by clustering a subset of the numerical data and using the distances to each respective cluster centroid.","7fa823da":"## 5.2 Loadings\n\nView the loadings for each feature\/component.","89728898":"# Preliminaries: Preprocessing \n\nThis section involves preprocessing our data prior to feature engineering, in particular, dealing with missing values and encoding categorical variables. This content is covered in the [Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) course, and is the subject of my [previous notebook](https:\/\/www.kaggle.com\/rsizem2\/kaggle-learn-reference-intermediate-ml). In this section we will do the following:\n\n1. Clean data by fixing typos and erroneous values\n2. Impute numerical data with the column mean\n3. Ordinally encode the *ordinal* variables\n4. Use a mix of ordinal and one-hot encoding for the *nominal* variables\n\n## 1. Data Cleaning\n\nWe fix some typos and bad values in our raw data.","0da924b4":"# Hyperparameter Search\n\nNow that we have established our preprocessing and feature engineering strategies we want to optimize our model parameters.\n\n## Pruning\n\nWe use a pruner to skip unpromising trials (in the lower 33% of scores for that fold).","0b9aff18":"# Feature Engineering\n\nThis notebook is an attempt to summarize the feature engineering techniques covered in the [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/feature-engineering) course (the notes, the exercises and the [bonus notebook](https:\/\/www.kaggle.com\/ryanholbrook\/feature-engineering-for-house-prices)) into one notebook for easier reference. We conclude with a sample competition submission to the [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course).","e8fc55d9":"## 4. Encoding Nominal Data\n\nFor the remaining categorical data we one-hot encode the low cardinality variables and ordinally encode the high cardinality variables. Recall the *cardinality* refers to the number of unique values.","d9ea5fe9":"# Lesson 2: Mutual Information\n\nWe use mutual information to perform feature selection, discarding features with low\/no mutual information score. We use the [mutual_info_regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.mutual_info_regression.html) function from scikit-learn.\n\n## 2.1 Most Informative Features\n\nThe following function returns a sorted series of features and their mutual information scores:","28b92a76":"# Lesson 3: Creating Features\n\nIn this section we create features using static transformations of existing features:\n\n1. Mathematical Transformations\n2. Feature Interactions\n3. Count Features\n4. Building and breaking down features\n5. Group transformations\n\n\n## 3.1 Mathematical Transformations\n\nCreate features by combining and\/or transforming already existing features","7cdc5824":"## Full Preprocessing \n\nThe following function combines all of the above preprocessing steps. We will use this preprocessing for every model we test."}}