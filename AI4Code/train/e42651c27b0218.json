{"cell_type":{"320d30ba":"code","297c95c7":"code","830b03e7":"code","4964214c":"code","6166693e":"code","8566e89f":"code","fa06f7f3":"code","06888fe3":"code","309ea1e9":"code","0cea5b54":"code","9dde45a7":"code","4bd52831":"code","3789ad89":"code","43e2b348":"code","905565c3":"code","dae4b1f5":"code","a3a6b93a":"code","2181ee19":"code","ae958c10":"code","9174f568":"code","16ddf7dd":"code","42326629":"code","4bfa1875":"code","3ae4ba2f":"code","261019ee":"code","b55bafae":"code","16080289":"code","103bb533":"code","234662fa":"code","b6d7cf88":"code","55f34259":"code","57c24703":"code","6ec623e8":"code","4d23274b":"code","b6229917":"code","117c6d79":"code","096da704":"code","9d42cd58":"code","e9265527":"code","7e71d5b9":"markdown","606caf9a":"markdown","d1b9cfc4":"markdown","601a5c52":"markdown","c46bf3aa":"markdown","4468b926":"markdown","bee3d0cb":"markdown","89fc4a8b":"markdown","f60445b2":"markdown","b6450e09":"markdown","bb5bb2bf":"markdown","7418ed07":"markdown","e598dac0":"markdown","56cb710c":"markdown","02c53224":"markdown"},"source":{"320d30ba":"# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport re","297c95c7":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\nfull_data = [train_data, test_data]\ntrain_data.head(3)","830b03e7":"test_data.head(3)","4964214c":"train_data.info()","6166693e":"test_data.info()","8566e89f":"train_data.columns","fa06f7f3":"# Pclass \ntrain_data[['Pclass','Survived']].groupby('Pclass', as_index=False).mean()","06888fe3":"# Name is text data We can extract information like name-initials (like Mr. Sir. Mrs. Miss. ) \n# May be this can be used as Class between ppl or to show Respect (Experimenting;)\ndef get_title(name):\n    title_search = re.search('([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return ''\nfor df in full_data:\n    df['Title'] = df['Name'].apply(get_title)\n\ntrain_data[['Title','Survived']].groupby('Title', as_index=False).mean()","309ea1e9":"# Sex\ntrain_data[['Sex','Survived']].groupby('Sex', as_index=False).mean()","0cea5b54":"# Age - Lets Make Age-Groups and then Find Survival Rate\n\n# Getting Rid of Missing Values from Age column (Part of Cleaning Data Step)\nfor df in full_data:\n    age_avg = df['Age'].mean()\n    age_std = df['Age'].std()\n    age_null_count = df['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    df['Age'][np.isnan(df['Age'])] = age_null_random_list\n    df['Age'] = df['Age'].astype(int)\n\n\n# Mapping Age\nfor dataset in full_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n\n\n\ntrain_data[['Age','Survived']].groupby('Age', as_index=False).mean()","9dde45a7":"# SibSp & Parch\n# With the number of siblings\/spouse and the number of children\/parents we can create new feature called Family Size.\n# Since we are Making New Column we can Delete these Columns in Data Cleaning Step\nfor df in full_data:\n    df['FamSize'] = df['SibSp'] + df['Parch'] + 1\n\ntrain_data[['FamSize','Survived']].groupby('FamSize', as_index=False).mean()","4bd52831":"# Fare Missing in Testing Data\nfor df in full_data:\n    \n    df['Fare'] = df['Fare'].fillna(train_data['Fare'].median())\n    \n    # Mapping Fare (Part of Data Cleaning Data)\n    df.loc[ df['Fare'] <= 7.91, 'Fare'] = 0\n    df.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare'] = 1\n    df.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare']   = 2\n    df.loc[ df['Fare'] > 31, 'Fare'] = 4\n\ntrain_data[['Fare','Survived']].groupby('Fare', as_index=False).mean()","3789ad89":"# Cabin\nfor df in full_data:\n    # Those with Cabin Seats be 1 and others be 0 (Nan)\n    df['Cabin'].fillna(0, inplace =True)\n    #df.loc[df['Cabin']==np.nan, 'Cabin'] = 0\n    df.loc[df['Cabin']!= 0, 'Cabin'] = 1\n\ntrain_data[['Cabin','Survived']].groupby('Cabin', as_index=False).mean()","43e2b348":"# Embarked\nfor df in full_data:\n    df['Embarked'] = df['Embarked'].fillna('S')\n\ntrain_data[['Embarked','Survived']].groupby('Embarked', as_index=False).mean()","905565c3":"for df in full_data:\n    \n    # Getting Rid of Useless Columns\n    df.drop(['Name','PassengerId','SibSp','Parch','Ticket'], axis=1, inplace=True)\n    \n    # Converting into desired Datatypes\n    df['Sex'] = df['Sex'].astype('category').cat.codes\n    df['Title'] = df['Title'].astype('category').cat.codes\n    df['Embarked'] = df['Embarked'].astype('category').cat.codes","dae4b1f5":"train_data.head(3)","a3a6b93a":"test_data.tail(3)","2181ee19":"train_data.info()","ae958c10":"test_data.info()","9174f568":"plt.figure(figsize=(15,8))\nplt.imshow(train_data.corr(), cmap=plt.cm.Blues, interpolation='nearest')  # or plt.cm.RdBu\nplt.colorbar( )\ntick_marks = [i for i in range(len(train_data.columns))]\nplt.xticks(tick_marks, train_data.columns, rotation='vertical')\nplt.yticks(tick_marks, train_data.columns)\n\nplt.show()","16ddf7dd":"y = train_data['Survived']\nX = train_data.drop(['Survived'],axis=1)","42326629":"from sklearn.tree import DecisionTreeClassifier as dtc\nmodel1 = dtc()\nmodel1.fit(X, y)\n\nfrom sklearn.neighbors import KNeighborsClassifier as knc\nmodel2 = knc(n_neighbors=5)\nmodel2.fit(X, y)\n\nfrom sklearn.svm import SVC\nmodel4 = SVC(C=1.0, kernel='rbf', degree=3)\nmodel4.fit(X, y)\n\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nmodel3 = rfc(n_estimators=100, max_depth=3, max_features=0.5, min_samples_leaf = 32)\nmodel3.fit(X, y)\n\nmodelx = rfc(n_estimators=100, max_depth=3, max_features=0.5, min_samples_leaf = 50)\nmodelx.fit(X, y)","4bfa1875":"#DTC\nscore01 = model1.score(X, y)\n\n#KNC\nscore02 = model2.score(X, y)\n\n#RFC\nscore03 = model3.score(X, y)\n\n#SVC\nscore04 = model4.score(X, y)\n\n\nscore_x = modelx.score(X, y)","3ae4ba2f":"print(\"Training Scores: \")\nprint(\"Decision Tree Classifier : \", score01*100)\nprint(\"K Neighbors Classifier   : \", score02*100)\nprint(\"Support Vector Classifier: \", score04*100)\nprint(\"Random Forest Classifier : \", score03*100)\n\nprint(\"Trial & Error RFC model  : \", score_x*100)","261019ee":"Xt = test_data\nyt = pd.read_csv('..\/input\/gender_submission.csv')['Survived']","b55bafae":"#DTC\nscore1 = model1.score(Xt, yt)\n\n#KNC\nscore2 = model2.score(Xt, yt)\n\n#SVC\nscore4 = model4.score(Xt, yt)\n\n# The following Random Forest Classifier Test score is the best result\n#RFC\nscore3 = model3.score(Xt, yt)\n\nscorex = modelx.score(Xt, yt)","16080289":"print(\"Testing Scores: \")\nprint(\"Decision Tree Classifier : \", score1*100)\nprint(\"K Neighbors Classifier   : \", score2*100)\nprint(\"Support Vector Classifier: \", score4*100)\nprint(\"Random Forest Classifier : \", score3*100)\n\nprint(\"Trial & Error RFC model  : \", scorex*100)","103bb533":"print(\"Random Forest Classifier train : \", score03*100)\nprint(\"Random Forest Classifier test  : \", score3*100)\nprint(\"Trial & Error RFC model  : \", scorex*100)","234662fa":"test_file = pd.read_csv('..\/input\/test.csv')\npred = modelx.predict(Xt)\nsub_df = pd.DataFrame({'PassengerId':test_file['PassengerId'], 'Survived':pred})\nsub_df.head()","b6d7cf88":"sub_df.to_csv('RF_result.csv', index = False)","55f34259":"from tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense,Flatten,Dropout","57c24703":"print('Shape of X is : ', X.shape)\nprint('Shape of y is : ', y.shape)","6ec623e8":"# I picked this Layer architecture by Trail and Error\ndmodel = Sequential()\ndmodel.add(Dense(200, activation = 'relu', input_shape = (8,)))\ndmodel.add(Dense(150, activation = 'relu'))\ndmodel.add(Dense(50, activation = 'relu'))\ndmodel.add(Dense(25, activation = 'relu'))\ndmodel.add(Dense(50, activation = 'relu'))\ndmodel.add(Dense(150, activation = 'relu'))\ndmodel.add(Dense(1, activation = 'sigmoid'))","4d23274b":"dmodel.summary()","b6229917":"dmodel.compile(loss = keras.losses.binary_crossentropy, optimizer='adam', metrics=['accuracy'])","117c6d79":"# Training data\ndmodel.fit(X, y, batch_size=50, epochs=6, validation_split=0.2)","096da704":"# Score on Testing Data\ndmodel.evaluate(Xt, yt)[1]","9d42cd58":"# predict against test set\npredictions = dmodel.predict(Xt)\npredictions = predictions.ravel().round() # To reduce ND-array to 1D-array\n\n# output results to DeepNet_result.csv\n\ntest_file = pd.read_csv('..\/input\/test.csv')\nsubmit_df = pd.DataFrame({'PassengerId':test_file['PassengerId'], 'Survived':predictions})\nsubmit_df.head(10)","e9265527":"submit_df.to_csv('DeepNet_result.csv', index = False)","7e71d5b9":"## *Without Neural network; Random Forest gives Best Result for this Dataset *\n**Now Lets Dive Deep**","606caf9a":"# ML","d1b9cfc4":"# Submitting","601a5c52":"# Cleaning Data","c46bf3aa":"# Successfully Converted All Columns into Numeric Data","4468b926":"## $To$ $be$ $Continued...$\nI have used all ML algos with Default Parameters and Visualization stuff has not been Incorporated\n\nAny suggestion would be appreciated.","bee3d0cb":"- So there are Missing Values in Both Training Data and Testing Data\n- So we will need to Fill both Data","89fc4a8b":"Self Note: After this Submission, Go to Output and then click on Submit predictions which will upload the result.","f60445b2":"There are 891 entries so every column must contain 891 but as we can see above **Age**, **Cabin** and **Embarked** doesn't have 891 entries so they must have some Missing Values","b6450e09":"# Going Deep","bb5bb2bf":"Here **Age**, **Cabin**, **Fare** have Missing Values","7418ed07":"When I set min_samples_leaf = 50; I get Test Score of 100 and train score of 81.6;\nAny plausible explanation will be appreciated.","e598dac0":"# Skipping the Data Visualization Step (for now)","56cb710c":"## Scores on Testing Data\nThe Scores that Really matter","02c53224":"## Feature Engineering\n- Go through all The Features One by one against  **Survived**\n- Since there is No Survived Column for Test Data , we wil only Deal with training data for this Section"}}