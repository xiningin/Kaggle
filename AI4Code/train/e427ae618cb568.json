{"cell_type":{"12cf226b":"code","06163a2e":"code","2c996556":"code","388354cb":"code","04195372":"code","7d2d972f":"code","3d05e316":"code","05a4bb43":"code","73fedc40":"code","9ab64b0e":"code","ab308a03":"code","3bf26f46":"code","91d82b7b":"code","9c38f038":"code","090b620f":"code","a59e96ed":"code","212833ec":"code","f3c08cf2":"code","94945fc6":"code","3946196a":"code","76513f15":"code","57d486fe":"code","57e44210":"code","757a227c":"code","83919557":"code","210cdb3d":"markdown","d6d02319":"markdown","8425e059":"markdown","fcf92782":"markdown","f6d2c6ab":"markdown","69d7b531":"markdown","07113641":"markdown","52f4d697":"markdown","c991a083":"markdown","ef6ae338":"markdown","3f63c1b4":"markdown","7f674974":"markdown","6d055348":"markdown","c0e19dd5":"markdown","a12238b9":"markdown","ed048837":"markdown","a302c48e":"markdown","ccb05d79":"markdown","97e070ab":"markdown","6590bb74":"markdown","98009bff":"markdown","b1fcf6e2":"markdown","e73387e3":"markdown","72fc3d82":"markdown","f0330099":"markdown","f86da669":"markdown","77e44052":"markdown"},"source":{"12cf226b":"!pip install dataprep","06163a2e":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter(action='ignore')\nfrom dataprep.eda import plot, plot_correlation, create_report, plot_missing\n","2c996556":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","388354cb":"df.shape","04195372":"df.duplicated().any()","7d2d972f":"df = df.drop_duplicates()","3d05e316":"df.shape","05a4bb43":"plot(df)","73fedc40":"create_report(df)","9ab64b0e":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]\ny.value_counts()","ab308a03":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 59)","3bf26f46":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n\nX_train['Time'] = scaler.fit_transform(X_train['Time'].values.reshape(-1,1))\nX_train['Amount'] = scaler.fit_transform(X_train['Amount'].values.reshape(-1,1))\n\nX_test['Time'] = scaler.fit_transform(X_test['Time'].values.reshape(-1,1))\nX_test['Amount'] = scaler.fit_transform(X_test['Amount'].values.reshape(-1,1))\n","91d82b7b":"from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\ngrids = []\n\nfor i in np.linspace(0.1,0.5,num=5):\n    oversample = SMOTE(sampling_strategy=i)\n    \n    undersample = RandomUnderSampler(sampling_strategy=1)\n    \n    model = LogisticRegression()\n    \n    param_grid = {'model__C': np.logspace(-3,3, num=7)}\n    \n    steps = [('over',oversample),('under',undersample),('model',model)]\n    \n    pipeline = Pipeline(steps=steps)\n    \n    cv = StratifiedKFold(n_splits=5)\n    \n    grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, n_jobs=-1, scoring = 'average_precision')\n    \n    grid.fit(X_train, y_train)\n    \n    grids.append(grid)\n    #Percentage of the majority class samples, e.g The majority '0' class has 1000 samples, then we will have 100 '1' class samples\n    print(\"SMOTE sampling_strategy: {}\".format(i))\n    #Ratio of minority and majority sample, 1 means same number of '0' and '1' samples\n    print(\"Random undersampling sampling_strategy: 1\")     \n    print(\"Best Average Precision scores:\\n{}\".format(grid.best_score_))\n    print(\"Best parameter(C) for Logistic Regression:\\n{}\".format(grid.best_params_))\n    print(\"\\n\")","9c38f038":"# Select the model with best Average Precision score from the five trained grid models.\nmodel = grids[0]\nfor grid in grids:\n    if grid.best_score_ > model.best_score_:\n        model = grid","090b620f":"import itertools\nimport matplotlib.pyplot as plt\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1#print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a59e96ed":"from sklearn.metrics import confusion_matrix\nytrain_pred = model.predict(X_train)\nconf_matrix = confusion_matrix(y_train,ytrain_pred)\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(conf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix of training set')\nplt.show()","212833ec":"from sklearn.metrics import classification_report\n\nytrain_pred = model.predict(X_train)\n\nprint(classification_report(y_train, ytrain_pred))","f3c08cf2":"from sklearn.metrics import precision_recall_curve, average_precision_score\nfrom numpy import argmax\n\ndef plot_PRC(mode):\n    \n    if mode == 'train':\n        a = X_train\n        b = y_train\n        print(\"PR-Curve of training set\")\n    elif mode == 'test':\n        a = X_test\n        b = y_test\n        print(\"PR-Curve of test set\")\n    else:\n        print(\"Please insert 'train' or 'test' to plot PR-Curve.\")\n        \n    y_score = model.predict_proba(a)\n\n    y1_proba = y_score[:,1]\n\n    fig = plt.figure(figsize=(12,6))\n\n    precision, recall, threshold = precision_recall_curve(b, y1_proba)\n\n    f2_score = (5 * precision * recall) \/ ((4 * precision) + recall)\n\n    max_f_index = argmax(f2_score)\n\n    aps = average_precision_score(b, y1_proba)\n\n    print('Best Threshold=%f, F2-Score=%.3f\\n' % (threshold[max_f_index], f2_score[max_f_index]))\n\n    best_threshold = threshold[max_f_index]\n\n    no_skill = len(b[b==1]) \/ len(b)\n\n    plt.step(recall, precision, color='#004a93', alpha=0.2,\n         where='post')\n\n    plt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#48a6ff')\n\n    plt.scatter(recall[max_f_index], precision[max_f_index], marker='o', color='black', label='Best')\n\n    plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([-0.05, 1.05])\n    plt.xlim([-0.05, 1.05])\n    plt.legend()\n    plt.title('UnderSampling Precision-Recall curve: \\n Average Precision Score ={0:0.2f}'.format(aps),\n          fontsize=16)\n    return best_threshold","94945fc6":"best_threshold = plot_PRC('train')","3946196a":"y_pred_new_threshold = (model.predict_proba(X_test)[:,1] >= best_threshold).astype(int)","76513f15":"conf_matrix = confusion_matrix(y_test, y_pred_new_threshold)\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(conf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix of test set')\nplt.show()","57d486fe":"print(classification_report(y_test, y_pred_new_threshold))","57e44210":"plot_PRC('test')","757a227c":"from sklearn.metrics import roc_auc_score,roc_curve, precision_recall_curve, average_precision_score\n\ny_score = model.predict_proba(X_test)\n\ny1_proba_T = y_score[:,1]\n\n#Creating new model to illustrate the difference of ROC Curve and PR Curve.\nmodel1 = LogisticRegression(class_weight={0:1,1:1})\nmodel2 = LogisticRegression(class_weight={0:1,1:10000})\nmodel1.fit(X_train, y_train)\nmodel2.fit(X_train, y_train)\n\ny_score_1 = model1.predict_proba(X_test)\ny1_proba_T1 = y_score_1[:,1]\n\ny_score_2 = model2.predict_proba(X_test)\ny1_proba_T2 = y_score_2[:,1]","83919557":"precision, recall, threshold = precision_recall_curve(y_test, y1_proba_T)\nprecision1, recall1, threshold1 = precision_recall_curve(y_test, y1_proba_T1)\nprecision2, recall2, threshold2 = precision_recall_curve(y_test, y1_proba_T2)\n\nlog_fpr, log_tpr, log_thresold = roc_curve(y_test, y1_proba_T)\nlog1_fpr, log1_tpr, log1_thresold = roc_curve(y_test, y1_proba_T1)\nlog2_fpr, log2_tpr, log2_thresold = roc_curve(y_test, y1_proba_T2)\n\n\nfig = plt.figure(figsize=(15,8))\nax1 = fig.add_subplot(1,2,1)\nax1.set_xlim([-0.05,1.05])\nax1.set_ylim([-0.05,1.05])\nax1.set_xlabel('Recall')\nax1.set_ylabel('Precision')\nax1.set_title('PR Curve')\n\nax2 = fig.add_subplot(1,2,2)\nax2.set_xlim([-0.05,1.05])\nax2.set_ylim([-0.05,1.05])\nax2.set_xlabel('False Positive Rate')\nax2.set_ylabel('True Positive Rate')\nax2.set_title('ROC Curve')\n\n\nax1.plot(recall, precision, label='Originally trained model AP score: {:.4f}'.format(average_precision_score(y_test, y1_proba_T)))\nax1.plot(recall1, precision1, label='Logistic Regression(class_weight = 1) AP score: {:.4f}'.format(average_precision_score(y_test, y1_proba_T1)))\nax1.plot(recall2, precision2, label='Logistic Regression(class_weight = 10000) AP score: {:.4f}'.format(average_precision_score(y_test, y1_proba_T2)))\n\nax2.plot(log_fpr, log_tpr, label='Originally trained model score: {:.4f}'.format(roc_auc_score(y_test, y1_proba_T)))\nax2.plot(log1_fpr, log1_tpr, label='Logistic Regression(class_weight = 1) score: {:.4f}'.format(roc_auc_score(y_test, y1_proba_T1)))\nax2.plot(log2_fpr, log2_tpr, label='Logistic Regression(class_weight = 10000) score: {:.4f}'.format(roc_auc_score(y_test, y1_proba_T2)))\n\n\nax1.legend(loc='lower left')\nax2.legend()\n    ","210cdb3d":"__After choosing the optimal threshold from the training data to produce the best trade-off between precision and recall, we set the new threshold for our model and use it to classify the test data.__","d6d02319":"## Data Visualization","8425e059":"### Important notes:\n1. __To deal with imbalanced data, I will resample the training set with a combination of oversampling and undersampling technique(SMOTE + RandomUnderSampling). I will oversample(SMOTE) the minority class '1' to 10%, 20%, 30%, 40% and 50% of the amount of majority class '0' samples in. Then, I will undersample(RandomUnderSampler) the majority class '0' to the same amount of samples as the oversampled '1' class. Finally, after five iterations of different SMOTE level, I will choose the one which creates the best model.__<br>\n2. __The reason I choose to perform both oversampling(SMOTE) and undersampling is based on certain research papers suggesting the combination will yield better result.__<br>\n3. __It is also important to note that we should only resample our training dataset and not the validation set and test set. This is because resampling validation and test set will cause data leakage and create an over-optimistic model. In short, we only resample the training set for fitting our classifier(in here, Logistic Regression).__<br>\n4. __This specific process must be done using a pipeline. We have to chain together the resampling technique and our classifier in order to make sure we don't resample the validation set during cross-validation which is the ultimate process to determine if we have create a model which generalizes well.__<br>\n5. __I will also combine the cross-validation process with GridSearchCV to select the best hyperparameter for the Logistic Regression model.__<br>\n6. __For cross-validation method, remember to use 'Stratified' version to address the imbalanced datasets(Ultimately, the data is still imbalanced, remember that the validation set is imbalanced while only the training datas will be resampled to fit a model.) Stratified version splits the datas into training and validation set so that they have the same proportion of majority to minority class.__<br>\n7. __I have used 'average precision' as the evaluation metric to pick the best model. We will see why in the analysis later.__","fcf92782":"## Conclusion\n\nWe should always think carefully about the evaluation metrics we use for training our models and evaluating their performance. For binary classification problem, if our dataset is highly imbalanced and our main focus is on the positive class '1', we should always use PR-Curve instead of ROC-curve to as our evaluation metric. Confusion matrix and the classification report(which includes Precision, Recall and F-1 Score) from sklearn are not good enough to make conclusive statement about the performance of our model due to the lack of flexibility around the usage of prediction probability of the model.\n\nThank you very much for reading until the end! I hope you have learnt a lot from this notebook and please upvote if you find it helpful so that more people can notice this notebook! Cheers!","f6d2c6ab":"# Credit card fraud detector","69d7b531":"## Introduction\nCredit card is a very common tool for purchasing stuffs online and offline nowadays. The popularity and convenience of credit card lead to increasing usage of credit card especially in this digital era. Hence, the security and trusthworthiness of credit card company(banks,etc) can be seen as one of the most important aspect of the credit card business. So, to be able to detect credit card fraud is very important for the survival and success of credit cards company.\n\nIn this notebook, we will investigate a dataset of credit card transactions which are also labelled whether each transaction is fraud or non-fraud. We will then build a machine learning model(logistic regression) to classify the transactions. One of the most important aspect to create a good model is to make sure the choice of evaluation metric is parallel to our business goal, so, we will investigate some of the pros and cons of several commonly used metric for classification problems(in this case, binary classification). Let's dive in and try your best to read till the end because the important concepts are in the Model Evaluation section!\n\n__Goals__ of this notebook:\n1. Tackle the imbalanced dataset with resampling technique like SMOTE and Random Undersampling for Cross-validation.\n2. Compare different evaluation metrics for the machine learning model and select the best metric for this problem.\n3. Provide intuition to readers in the evaluation of model specifically in imbalanced binary classification problems.\n\nFor beginners who are interested in Data Visualization(EDA) and basic data cleaning, please visit the link below where I analyse the netflix movies and tv shows data:<br>\nhttps:\/\/www.kaggle.com\/oscarliujun\/netflix-data-preprocessing-eda-beginner-friendly <br>\nFor more data cleaning and basic feature selection + engineering, look at the regression model I have to predict used vehicle prices:<br>\nhttps:\/\/www.kaggle.com\/oscarliujun\/car-price-prediction-feature-engineering-91-cv","07113641":"__Now, we can confirm that by setting a new threshold at (P=0.9999), we greatly increases our trade-off between precision and recall , where we achieve 74% increase in precision while only have a decrease of 11% in recall.__","52f4d697":"### 4. ROC Curve","c991a083":"### Some conclusions from the graphs and details above:\n- 'Class' which is the labels for our transaction, '0' for non-fraud, '1' for fraud, is highly imbalanced where '0' labels constitute 99.83% while '1' only accounts for 0.17% of the transactions.\n- According to the Pearson correlation matrix, there is no particular highly correlated features with respect to 'Class', the highest being V17(-0.31).\n- There are no missing values in the dataset.\n- As we can see from the visualization above, the components from PCA V1 - V28 are all scaled, which is the usual case of PCA. However, 'Amount' and 'Time' are not scaled, so we need to scale it to ensure the two features do not overshadow the other features. This is particularly important to create a model which is sensitive to the scaling of the features. Some of the most used scalers are: 1. Standard scaler 2. Min-max scaler 3. Robust scaler and etc.","ef6ae338":"## Building model","3f63c1b4":"__I would like to emphasize that we should always split our data first into training set and test set before scaling any of the features to prevent data leakage. When we scale the orignal dataset before splitting, the information from our test set will be leaked to the training set. The test set should be left to one corner until the whole modelling process is done. It should only be used to test the performance of the final model. We should also not tune our model based on our result with test set.__","7f674974":"__One of the most widely used evaluation metric for binary classification is the ROC-AUC score which is the area under the ROC curve. However, this metric is not good and suitable for highly imbalanced data because it will produce an overly optimistic result which may misguide us into believing we have a model that perform and generalize well.__","6d055348":"__I have chosen RobustScaler since it is robust to outliers.__","c0e19dd5":"## Model Evaluation","a12238b9":"I will use an automatic EDA library called DataPrep to help me generate all the relevant plots for this dataset and then analyze them. This automatic EDA helps to save me a lot of time and effort where I can focus more on the imbalanced datasets, modelling and evaluation.","ed048837":"- __Now, here is why Precision-Recall Curve is the best choice since it plot all the Precision and Recall values for all the classification threshold(from 0 to 1) based on the predicted probability(predict_proba method of logistic regression) of each samples.__\n- __The higher the AUC(Area Under Curve) of the PR Curve, the better the model is in maximizing the trade-off between precision and recall(higher recall and precision in total compare to lower PR-AUC model). Average Precision is equal to the AUC of PR Curve.__\n- __The black dot is the best point(highest recall and precision) when we set the threshold at 0.999984, which is obtained by calculating the maximum F-2 score using precision and recall. F-2 score is used instead of F-1 score because we prioritize recall more than precision, so using this metric, we will find the best point which gives us higher recall by trading off precision.__\n- __The F2-score for the model based on the imbalanced training data is 0.79.__\n- __As a business problem(credit card business), business stakeholders can discuss and assign cost function to each fault (missed fraud detection or false alarm of fraud) and choose the best operating point that minimize the cost function based on the above PR-Curve. For example, if we prioritize catching all the frauds, we can set our operating point closer to the lower right of the curve, which yield high recall(good ability to catch fraud) but in the expense of low precision(creating many false alarm of fraud).__","a302c48e":"- __As we can see from the confusion matrix heatmap above, the numbers of TN(220761) dominates the others. The TP (346) constitutes a very small amount compare to the FP(5841).__\n- __Keep in my that our focus should be on positive cases(TP,FP,FN) only, since we are trying to deal with 'Fraud' cases only and true non-fraud case(TN) is not interesting for us.__\n- __It is very obvious due to the imbalanced data, we can hardly tell if our model can detect all the frauds by just looking at confusion matrix because it only look at the 'class' labels without looking at the uncertainty probability of each predicted class value.__\n- __However, you can calculate the Precision, Recall(Sensitivity), Specificity, TNR, etc using the values in the confusion matrix.__","ccb05d79":"__As we can see from the above confusion matrix, the number of False Positive(top right square) greatly reduced relative to the previous confusion matrix where we use the default threshold(P=0.5).__","97e070ab":"### 2. Classification report from sklearn","6590bb74":"## Understanding our data","98009bff":"- __Two additional logistic regression model with different class_weight parameters are created to illustrate the misguiding ROC-AUC.__\n- __We can see that on the ROC curve, all three models perform very having a AUC score of 0.9859(lowest) and 0.9931(highest). The difference of performance is almost undifferentiable for all the models. Consequently, if we used ROC curve and ROC-AUC to determine the performance of our model, we would be misguided into thinking our models perform very well where in fact they are not. Hence, we can say that the ROC-AUC provide an overly-optimistic evaluation of our model in classifying highly imbalanced datasets.__\n- __For the Precision-recall curve, we can clearly see the differences in performances of the three models. So, this is why the Average Precision(PR-AUC) is a better metric for a highly imbalanced dataset and especially useful when our main focus is on the positive class '1' because precision and recall inform about all the positive class '1'.__","b1fcf6e2":"There are 30 features in this dataset, where 28 features are components from Principal Component Analysis(PCA) on the unknown features. The dataset provider has mentioned that due to privacy and security factor, only the components from PCA are provided. The other two features are Time(seconds from the first transaction) and Amount. ","e73387e3":"### 1. Confusion Matrix\n- __One of the most common tool to evaluate binary classifcation is a confusion matrix. Confusion matrix shows us the TP(True Positive), TN(True Negative) , FP(False Positive), FN(False Negative) values.__","72fc3d82":"### 3. Precision-recall Curve","f0330099":"- __The sklearn metrics class provide a very useful function 'classification_report' which provides all the calculated Precision,Recall, f1-score(which we don't need to calculate manually based on the confusion matrix values).__\n- __By focusing on class label '1' row only, we have all our interested details(TP,FP,FN), remember that Precision = (TP \/ TP + FP) and Recall = (TP \/ TP + FN).__\n- __We have a good recall score(0.92) and very bad precision score(0.06) and f1-score(0.11). This ultimately means that, we can catch 92% of the frauds, however, we can only be sure that 6% of the frauds that we caught are really a fraud. In other words, we may falsely reject too many transactions, while be able reject most of the fraud transactions.__ \n- __Essentially, we have to make a trade-off between the ability to catch fraud transactions and the ability to make correct rejection to transactions we think are fraud.(Trade-off of Recall vs Precision)__ \n- __The precision and recall score provided in this report is calculated when the classification threshold is set at default value which is P=0.5(which means, if the predicted probability of being class '1' is more than or equal to 0.5, the sample will be predicted and labeled as '1' by the classifier).__\n- __The above point is the main reason that this report is not the best metric to judge whether our model is good enough because the high majority class '0' of the dataset will skew the model while it is being fitted.For example, imagine you train your dog to 'sit' 10000 times while only training it to jump for '10' times. Surely, the dog will have the greater tendency to 'sit' rather than 'jump' when you give it some instructions. When you set the threshold of the classifier to classify the action of dog to be P=0.5, 50% of the action will be classified as 'sit' and 50% will be classified as 'jump'. However, the probability of the dog to 'sit' when given an instruction is definitely higher than 50%(P>>0.5) since it is trained 1000 times more to 'sit' than 'jump', which explains why there are so many false positive cases(low precision) when we set threshold at P=0.5 for the classification process(the dogs are classified as 'jumping' where in reality, it is 'sitting').__\n- __Similarly for recall, when we have classify so many dogs as 'jump'(50% in this case) when given an instruction where in fact we are not so sure if the dog really 'jumped' since the dogs are rarely trained to 'jump'(P<<0.5), this will inflate the chance that we correctly predicted that the dog really 'jumped' since when we are not sure, we just classify it as 'jumped' and this technique can reduce the false negative('jumped' but didn't get classified as 'jumped') simply by guessing more 'jumped' dogs. Hence, recall is high(low FN).__\n- __In conclusion, the precision is lower that it is supposed to be and the recall is higher than it should be when we set threshold at P=0.5(default). This shows the inflexibility of the classification report where we cannot observe the Precision and Recall value at other threshold level.__","f86da669":"__Splitting the data into X(input features) and y(output class)__","77e44052":"- __The Average Precision score(PR-AUC) of the test set is the same as the training sets, which tells us that our model works equally well in the training set and the test set. We also achieve a higher F2-score(0.817) in the test set at the best point where the threshold is really close to 1(0.9999999). This might mean that our model has achieved its best performance possible and we might need to retune the model or use a more complex classifier(such as Random Forest\/SVM) to achieve an even higher performance(Average Precision score).__\n- __To reiterate, we can set our desired recall\/precision level using the PR Curve with the trade-off between precision and recall based on the curve. The best point represents the best trade-off point for precision and recall.__"}}