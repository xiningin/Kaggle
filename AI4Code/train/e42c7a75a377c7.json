{"cell_type":{"89143c12":"code","ab898d40":"code","37abbce8":"code","0ada6b6c":"code","067e6c3d":"code","7163dc80":"code","5874b328":"code","5e349e11":"code","93c8dc90":"code","f869c9f2":"code","608de474":"code","9ece57d8":"code","670fab7b":"code","359f0209":"code","53666457":"code","5b4d302c":"code","6ed6fe26":"code","a12bf55a":"code","424090ad":"code","aaad2c21":"code","29922b58":"code","58163958":"code","987d5323":"code","ff801751":"code","d7d539df":"code","71fc1235":"code","747118eb":"code","da0299f6":"code","a335903d":"code","31e6961a":"code","4d49af8c":"code","34f0582c":"code","f24326fb":"code","5b4f702e":"code","9835301f":"code","33c831e5":"code","1126ea27":"code","6639a194":"code","ea2e6ff6":"code","afe25414":"code","a126b537":"code","2c30481f":"code","25b8d1fa":"code","3f9fd33e":"code","e86afe4c":"code","afa36938":"code","30e0255e":"code","eaaf4c00":"code","9c9c85df":"code","d3261243":"code","e600a3b5":"code","603edfde":"code","b8f5d465":"code","ecc618c2":"code","2674c7c4":"code","7e1c5c3f":"markdown","b833a66f":"markdown","a4190f0f":"markdown","4c0cf495":"markdown","34644e39":"markdown","70d4ca42":"markdown","2395cb4b":"markdown","fe15b54b":"markdown","365aacf6":"markdown","5cc3839f":"markdown","5f6d7329":"markdown","e3aa3018":"markdown","30681221":"markdown","e98c1ff7":"markdown","9720e0dc":"markdown","3142a988":"markdown","d0c21e0d":"markdown","2966324e":"markdown","8e07ce7c":"markdown","055fc7cf":"markdown","eb54c1b6":"markdown","534a061e":"markdown","0f5e253e":"markdown","767c0b9a":"markdown","fb1808b4":"markdown","43db5fac":"markdown","5bbdf00a":"markdown","8bd25ad3":"markdown","90a675d3":"markdown","a2b8b3ec":"markdown","8252a6f2":"markdown","73acccfc":"markdown"},"source":{"89143c12":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ab898d40":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = train_data.copy()\ntest = test_data.copy()","37abbce8":"train.head()","0ada6b6c":"train.drop(['PassengerId'], axis=1, inplace=True)\ntest.drop(['PassengerId'], axis=1, inplace=True)\npred = train['Survived']","067e6c3d":"train.head()","7163dc80":"train.isnull().sum()","5874b328":"test.isnull().sum()","5e349e11":"train['Age'].describe()","93c8dc90":"train['Age'].fillna(train['Age'].quantile(0.5), inplace=True)\ntest['Age'].fillna(test['Age'].quantile(0.5), inplace=True)","f869c9f2":"train['Embarked'].fillna('S', inplace=True)\ntest['Embarked'].fillna('S', inplace=True)","608de474":"test['Fare'].fillna(test['Fare'].quantile(0.5), inplace=True)","9ece57d8":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Sex',hue='Survived',data=train_data)\nplt.show()","670fab7b":"sex1 = pd.get_dummies(train['Sex'])\nsex2 = pd.get_dummies(test['Sex'])","359f0209":"train.drop(['Sex'], axis=1, inplace=True)\ntest.drop(['Sex'], axis=1, inplace=True)\n\ntrain = pd.concat([train, sex1], axis=1)\ntest = pd.concat([test, sex2], axis=1)","53666457":"train.drop(['female'], axis=1, inplace=True)\ntest.drop(['female'], axis=1, inplace=True)","5b4d302c":"plt.figure(figsize=(8, 5))\nsns.boxplot(x='Pclass', y='Age', data=train_data)\nplt.show()","6ed6fe26":"plt.figure(figsize=(8, 5))\nsns.countplot(x='Pclass', hue='Survived', data=train_data)\nplt.show()","a12bf55a":"plt.figure(figsize=(8, 5))\nfor x in [1,2,3]:\n    sns.kdeplot(data=train_data.Age[train_data.Pclass == x])\nplt.title('Age vs Pclass')\nplt.legend(['1st','2nd','3rd'])\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.show()","424090ad":"plt.figure(figsize=(12, 3))\nsns.set_style('whitegrid')\nsns.distplot(train['Fare'], color='orange')\nplt.title('Fare distribution for all people')\nplt.show()","aaad2c21":"plt.figure(figsize=(12, 3))\nsns.distplot(train[train['Survived'] == 0]['Fare'], color='red')\nplt.title('Fare distribution for not survived people')\nplt.show()","29922b58":"plt.figure(figsize=(12, 3))\nsns.distplot(train[train['Survived'] == 1]['Fare'])\nplt.title('Fare distribution for survived people')\nplt.show()","58163958":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x='Embarked', hue='Survived', data=train)\nplt.show()","987d5323":"embark1 = pd.get_dummies(train['Embarked'])\nembark2 = pd.get_dummies(test['Embarked'])\n\ntrain.drop(['Embarked'], axis=1, inplace=True)\ntest.drop(['Embarked'], axis=1, inplace=True)\n\ntrain = pd.concat([train, embark1], axis=1)\ntest = pd.concat([test, embark2], axis=1)","ff801751":"plt.figure(figsize=(10,5))\nsns.countplot(y='SibSp', hue='Survived', data=train, color='orange')\nplt.show()","d7d539df":"plt.figure(figsize=(10,5))\nsns.countplot(y='Parch', hue='Survived', data=train, color='green')\nplt.show()","71fc1235":"def family(x):\n    if x['SibSp'] + x['Parch'] > 1:\n        return 1\n    else:\n        return 0\ntrain['Family'] = train.apply(family, axis=1)\ntest['Family'] = test.apply(family, axis=1)","747118eb":"train.drop(['SibSp','Parch'], axis=1, inplace=True)\ntest.drop(['SibSp','Parch'], axis=1, inplace=True)","da0299f6":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x='Family', hue='Survived', data=train)\nplt.show()","a335903d":"train['Cabin'] = pd.Series(i[0] if not pd.isnull(i) else 'X' for i in train['Cabin'])\ntest['Cabin'] = pd.Series(i[0] if not pd.isnull(i) else 'X' for i in test['Cabin'])","31e6961a":"plt.figure(figsize=(8, 5))\nsns.catplot(y='Survived', x='Cabin', data=train, kind='bar', order=['A','B','C','D','E','F','G','X'])\nplt.show()","4d49af8c":"train['Cabin'] = train['Cabin'].map({\n    'X': 0,\n    'A': 1,\n    'B': 2,\n    'C': 3,\n    'D': 4,\n    'E': 5,\n    'F': 6,\n    'G': 7,\n    'T': 0\n})\ntrain['Cabin'] = train['Cabin'].astype(int)\ntest['Cabin'] = test['Cabin'].map({\n    'X': 0,\n    'A': 1,\n    'B': 2,\n    'C': 3,\n    'D': 4,\n    'E': 5,\n    'F': 6,\n    'G': 7,\n    'T': 0\n})\ntest['Cabin'] = test['Cabin'].astype(int)","34f0582c":"train_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train[\"Name\"]]\ntrain[\"Title\"] = pd.Series(train_title)\ntest_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in test[\"Name\"]]\ntest[\"Title\"] = pd.Series(test_title)","f24326fb":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","5b4f702e":"train[\"Title\"] = train[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain[\"Title\"] = train[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntrain[\"Title\"] = train[\"Title\"].astype(int)\ntest[\"Title\"] = test[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest[\"Title\"] = test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntest[\"Title\"] = test[\"Title\"].astype(int)","9835301f":"Ticket1 = []\nfor i in list(train.Ticket):\n    if not i.isdigit() :\n        Ticket1.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket1.append(\"X\")\ntrain[\"Ticket\"] = Ticket1\n\nTicket2 = []\nfor j in list(test.Ticket):\n    if not j.isdigit() :\n        Ticket2.append(j.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket2.append(\"X\")\ntest[\"Ticket\"] = Ticket2","33c831e5":"train= pd.get_dummies(train, columns = [\"Ticket\"], prefix=\"T\")\ntest = pd.get_dummies(test, columns = [\"Ticket\"], prefix=\"T\")","1126ea27":"train = train.drop(['T_SP','T_SOP','T_Fa','T_LINE','T_SWPP','T_SCOW','T_PPP','T_AS','T_CASOTON'],axis = 1)\ntest = test.drop(['T_SCA3','T_STONOQ','T_AQ4','T_A','T_LP','T_AQ3'],axis = 1)","6639a194":"train.drop(['Survived'],axis=1,inplace=True)","ea2e6ff6":"train.head(5)","afe25414":"test.head()","a126b537":"print('Train:')\nprint(train.isnull().sum())\nprint('Test:')\nprint(test.isnull().sum())","2c30481f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain2 = scaler.fit_transform(train)\ntest2 = scaler.fit_transform(test)","25b8d1fa":"from sklearn.pipeline import Pipeline\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score\n\nKFold_Score = pd.DataFrame()\nclassifiers = ['Linear SVM', 'Radial SVM', 'LogisticRegression', \n               'RandomForestClassifier', 'AdaBoostClassifier', \n               'XGBoostClassifier', 'KNeighborsClassifier','GradientBoostingClassifier']\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          xgb.XGBClassifier(n_estimators=100),\n          KNeighborsClassifier(),\n          GradientBoostingClassifier(random_state=0)\n         ]\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score[classifiers[j]] = (cross_val_score(model, train2, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    j = j+1","3f9fd33e":"mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\nKFold_Score = pd.concat([KFold_Score,mean.T])\nKFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score.T.sort_values(by=['Mean'], ascending = False)","e86afe4c":"col_name1 = list(train.columns)\ncol_name2 = list(test.columns)","afa36938":"col_name1[0],col_name1[2] = col_name1[2],col_name1[0]\ncol_name2[0],col_name2[2] = col_name2[2],col_name2[0]","30e0255e":"train_new = train[col_name1]\ntest_new = test[col_name2]","eaaf4c00":"train_new = train_new.drop(['Cabin'],axis = 1)\ntest_new = test_new.drop(['Cabin'],axis = 1)","9c9c85df":"sc = StandardScaler()\ntrain3 = sc.fit_transform(train_new)\ntest3 = sc.transform(test_new)","d3261243":"clf = RandomForestClassifier(random_state=0)","e600a3b5":"param_grid={\n    'n_estimators': [200,300],\n    'max_features': ['auto', 'sqrt'],\n    'max_depth': [6,7,8],\n    'criterion':['gini','entropy']\n}","603edfde":"from sklearn.model_selection import GridSearchCV\nCV_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\nCV_clf.fit(train3, pred)\nCV_clf.best_params_","b8f5d465":"clf1 = RandomForestClassifier(random_state=0, n_estimators=200, criterion='gini', max_features='auto', max_depth=8)\nclf1.fit(train3, pred)","ecc618c2":"pred3 = clf1.predict(test3)","2674c7c4":"pred_test = pred3\noutput = pd.DataFrame({\n    'PassengerId': test_data.PassengerId,\n    'Survived': pred_test\n})\noutput.to_csv('.\/submission.csv', index=False)","7e1c5c3f":"### 3. Age","b833a66f":"### Applying standardization as part of our data normalization","a4190f0f":"### 4. Fare","4c0cf495":"### Here the missing values are replaced with 'X'.","34644e39":"### 7. Cabin","70d4ca42":"### map with numeric value.","2395cb4b":"### Here, '.get_dummies' will convert this column into 2 dummy columns of male and female.","fe15b54b":"### 2. Pclass","365aacf6":"#### In this dataset we have a \"Name\" column mentioning the name of every passenger. These names also have a title along with them which can be useful.\n#### We will be splitting the title from the name.","5cc3839f":"### fill the null of Age using 50% median value","5f6d7329":"#### Here is a tricky part. The training set and test set have a few tickets which are unique to themselves.","e3aa3018":"#### Here we have our titles mapped with numeric values.\u00b6","30681221":"### We can see that people from higher class has a better survived rate.","e98c1ff7":"### 'Fare' column tells us the amount of money paied by passengers. Here we can see that the passengers will have a higher probability of surviving if they had payed more.","9720e0dc":"### 9. Ticket\n#### This column has the ticket number of all the passengers. Here we will be taking the ticket prefix.","3142a988":"### It is the end of data perprocessing.\n>Note: In the above feature preprocessing, the values that I have used for filling missing values were chosen after experimenting with different values. I took these values as they gave me the best result. Median values are best suited for missing values in most of the Machine-Learning models.","d0c21e0d":"### Name & Titles ","2966324e":"## Model Training","8e07ce7c":"### Here we creat a new column combining SibSp and Parch column and determine whether the passenger has a family or not.","055fc7cf":"### 5. Embarked","eb54c1b6":"### We can delete one column as we get the necessary information from other one.","534a061e":"#### It is the list of missing values from train and test set.","0f5e253e":"## Feature Scaling with Standardization","767c0b9a":"### 1. Sex","fb1808b4":"### 6. SibSp and Parch\n#### SibSp tell us about the passengers' siblings and spouse.\n#### Parch tell us about the passengers' parents and children.","43db5fac":"# Data Analysis\n### We will looking at the folliowing things:\n* Filling the missing values\n* Data visualization\n* Data Preprocessing\n* Encoding","5bbdf00a":"## Missing Values","8bd25ad3":"## Features\n### In this section we will look at all the features from the dataset. There will be visualization of data by using different graphs and preprocessing on the training and test set.\n> Note: Similar preprocessing will be done on training and test set.","90a675d3":"### fill the null of Embarked using the most frequently occuring value(S).","a2b8b3ec":"### Here we can see that they almost have the same distribution.","8252a6f2":"### Here we can see that a person having no family had a less chance of survival.","73acccfc":"### Check for the missing values"}}