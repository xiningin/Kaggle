{"cell_type":{"85f1d6a1":"code","52935576":"code","d5365f48":"code","98bf9081":"code","28584d30":"code","f79a586a":"code","0e9f0c84":"code","262b0352":"code","0100fb77":"code","af223a68":"code","f42fe8ad":"code","8245b374":"code","750e38ab":"code","b9e8e2eb":"code","cd020f91":"code","03a033fa":"code","6917ba4f":"code","c8927731":"code","934d19cf":"code","6b74aff0":"code","ffbf94e0":"code","2445edc1":"code","33adc92f":"code","a22d4aa0":"code","109eb4a5":"code","8f4327ad":"code","7de6b6b1":"code","dfc553e6":"code","5737d684":"code","4bbbd6d8":"code","32450d46":"code","df5d8749":"code","34f21362":"code","af421515":"code","d1e6f1d9":"code","3a37bbe8":"code","94491f11":"code","9e3487be":"code","b6b9b9fc":"code","2337325c":"code","c2a1f78a":"code","2e377be9":"code","2ff938aa":"code","c24e5d85":"code","a471201b":"code","598cb874":"code","44013e26":"code","71fc2f64":"code","0675eb3c":"code","2be1b627":"code","115fd74d":"code","e6e07a5a":"code","201358bd":"code","d658e07e":"code","969f1b4a":"code","113a83ea":"code","0e2c16ed":"code","738812f4":"code","27d8dcf8":"code","0f9558c8":"code","77747dee":"code","4406780f":"code","50f61310":"code","01de9faa":"code","6959cff3":"code","ea78350d":"code","946a0afe":"code","e20cd387":"code","496d97d6":"code","85257f94":"code","bad3c8cb":"markdown","dcdf92fd":"markdown","2b2faba3":"markdown","b21536c9":"markdown","25f34a62":"markdown","d31e32bf":"markdown","5caed983":"markdown","b8fb5022":"markdown","c8ce4393":"markdown","fd300aaf":"markdown","c4234a37":"markdown","e1ce40ae":"markdown","6df4b88c":"markdown","c67bb9b0":"markdown","11d67366":"markdown"},"source":{"85f1d6a1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sbn\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nimport pandas_profiling\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","52935576":"data  = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndata.head()","d5365f48":"data.profile_report()\n","98bf9081":"data.isnull().sum()","28584d30":"data.corr()","f79a586a":"data.describe()","0e9f0c84":"s = data.groupby(\"Sex\")","262b0352":"s = s.size().sort_values()","0100fb77":"plt.pie(s, labels = [\"F\", \"M\"],autopct='%1.1f%%',radius = 2, textprops = {\"fontsize\" : 14})","af223a68":"c = data.groupby(\"ChestPainType\")","f42fe8ad":"c = c.size().sort_values()","8245b374":"plt.pie(c, labels = [\"TA\", \"ATA\", \"NAP\", \"ASY\"], autopct = '%1.1f%%', radius = 2, textprops = {\"fontsize\" : 14 }  )","750e38ab":"r = data.groupby(\"RestingECG\")","b9e8e2eb":"r = r.size().sort_values()","cd020f91":"r","03a033fa":"plt.pie(r, labels = [\"ST\", \"LVH\", \"Normal\"], autopct = '%1.1f%%', radius = 2, textprops = {\"fontsize\" : 14 }  )","6917ba4f":"ea = data.groupby(\"ExerciseAngina\")","c8927731":"ea = ea.size().sort_values()","934d19cf":"ea","6b74aff0":"plt.pie(ea, labels = [\"Y\", \"N\"], autopct = '%1.1f%%', radius = 2, textprops = {\"fontsize\" : 14 }  )","ffbf94e0":"plt.figure(figsize = (7,5))\nsbn.distplot(data[\"HeartDisease\"])","2445edc1":"sbn.countplot(data[\"HeartDisease\"])","33adc92f":"sbn.scatterplot(x = \"HeartDisease\", y=\"Cholesterol\", data = data)","a22d4aa0":"st = data.groupby(\"ST_Slope\")","109eb4a5":"st = st.size().sort_values()","8f4327ad":"st","7de6b6b1":"plt.pie(st, labels = [\"Down\", \"Up\", \"Flat\"], autopct = '%1.1f%%', radius = 2, textprops = {\"fontsize\" : 14 }  )","dfc553e6":"chest = data.iloc[:,2:3].values\nchest","5737d684":"le = preprocessing.LabelEncoder()\nchest[:,0]  = le.fit_transform(data.iloc[:,2:3])","4bbbd6d8":"chest","32450d46":"ohe = preprocessing.OneHotEncoder()\nchest = ohe.fit_transform(chest).toarray()","df5d8749":"chest","34f21362":"chestt = pd.DataFrame(data = chest, index = range(918), columns = [\"cpt_asy\",\"cpt_ata\", \"cpt_nap\", \"cpt_ta\" ])","af421515":"chestt","d1e6f1d9":"sex = data.iloc[:,1:2].values\nle1 = preprocessing.LabelEncoder()\nsex[:,0] = le1.fit_transform(data.iloc[:,1:2])","3a37bbe8":"ohe1 = preprocessing.OneHotEncoder()\nsex = ohe1.fit_transform(sex).toarray()\nsex","94491f11":"sex = pd.DataFrame(data = sex, index = range(918), columns = [\"F\", \"M\"] )\nsex","9e3487be":"data","b6b9b9fc":"res = data.iloc[:,6:7].values\nres.shape","2337325c":"le2 = preprocessing.LabelEncoder()\nres[:,0] = le2.fit_transform(data.iloc[:,6:7])\nres","c2a1f78a":"ohe2 = preprocessing.OneHotEncoder()\nres = ohe2.fit_transform(res).toarray()\nres","2e377be9":"res = pd.DataFrame(data = res, index = range(918), columns = [\"re_lvh\", \"re_normal\", \"le_st\"] )","2ff938aa":"res","c24e5d85":"ex = data.iloc[:,8:9].values\nle3 = preprocessing.LabelEncoder()\nex[:,0] = le3.fit_transform(data.iloc[:,8:9])","a471201b":"ohe3 = preprocessing.OneHotEncoder()\nex = ohe3.fit_transform(ex).toarray()\nex","598cb874":"ex = pd.DataFrame(data = ex, index = range(918), columns = [\"ex_no\", \"ex_yes\"] )","44013e26":"ex","71fc2f64":"st = data.iloc[:,10:11].values\nle4 = preprocessing.LabelEncoder()\nst[:,0] = le4.fit_transform(data.iloc[:,10:11])","0675eb3c":"ohe4 = preprocessing.OneHotEncoder()\nst = ohe3.fit_transform(st).toarray()\nst","2be1b627":"st = pd.DataFrame(data = st, index = range(918), columns = [\"ST_S_down\", \"ST_S_flat\", \"ST_S_up\"] )","115fd74d":"st","e6e07a5a":"data1 = pd.concat([sex,chestt,res,ex,st], axis = 1)","201358bd":"data1","d658e07e":"data.drop([\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"], axis = 1, inplace = True)","969f1b4a":"data","113a83ea":"data = pd.concat([data1, data], axis = 1)","0e2c16ed":"data","738812f4":"data.columns","27d8dcf8":"x = data.iloc[:,0:20].values","0f9558c8":"y = data.iloc[:,20:].values","77747dee":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)\nsc = preprocessing.StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)","4406780f":"myList = []\nclass Logistic():\n    def __init__(self,y_pred, cm, accuracy, report):\n        self.y_pred = y_pred \n        self.cm = cm\n        self.accuracy = accuracy\n        self.report = report\n        logr = LogisticRegression()\n        logr.fit(X_train, y_train)\n        y_pred = logr.predict(X_test)\n        print(f' Logistic Regression : \\n{y_pred}')\n        print(\"------------------------------------\")\n        cm = confusion_matrix(y_test, y_pred)\n        print(f'confusion_ matrix: \\n{cm}') \n        print(\"------------------------------------\")\n        accuracy = accuracy_score(y_pred, y_test)\n        print(f'accuracy score: {accuracy}')\n        print(\"------------------------------------\")\n        myList.append(accuracy)\n        report = classification_report(y_test, y_pred)\n        print(f'classification_report: {report}')\nLogistic = Logistic(\"y_pred\",\"cm\", \"accuracy\",\"report\")   \nprint(Logistic)    \n","50f61310":"myList1 = []\nclass Svc():\n    def __init__(self,y_pred1, cm1, accuracy1, report1, success1):\n        self.y_pred1 = y_pred1\n        self.cm1 = cm1\n        self.accuracy1 = accuracy1\n        self.report1 = report1\n        self.success1 = success1\n        svc = SVC(C = 2, kernel = \"rbf\", gamma = 0.01, random_state = 0)\n        svc.fit(X_train, y_train)\n        y_pred1 = svc.predict(X_test)\n        print(f' SVC : \\n{y_pred1}')\n        print(\"------------------------------------\")\n        cm1 = confusion_matrix(y_test, y_pred1)\n        print(f'confusion_ matrix: \\n{cm1}') \n        print(\"------------------------------------\")\n        accuracy1 = accuracy_score(y_pred1, y_test)\n        print(f'accuracy score: {accuracy1}')\n        print(\"------------------------------------\")\n        myList1.append(accuracy1)\n        report1 = classification_report(y_test, y_pred1)\n        print(f'classification_report: {report1}')\n        print(\"------------------------------------\")\n        success1 = cross_val_score(estimator = svc, X= X_train, y=y_train, cv=4)\n        print(f'success: {success1}')\n        p = [{'C': [1,2,3,4,5,6,7,8,9,10], 'kernel': ['linear']}, {'C': [1,2,3,4,5,6,7,8,9,10], 'kernel': ['rbf']}, {'gamma': [1,0.5,0.1,0.01, 0.001]}]\n        gs = GridSearchCV(estimator = svc, param_grid = p, scoring=\"accuracy\", cv = 10, n_jobs = -1)\n        grid_search = gs.fit(X_train, y_train)\n        bestresult = grid_search.best_score_\n        bestparameters = grid_search.best_params_\n        print(f'best result: {bestresult}')\n        print(f'best parameters: {bestparameters}')\nSvc = Svc(\"y_pred1\",\"cm1\", \"accuracy1\",\"report1\",\"success1\")\n         ","01de9faa":"myList2 = []\nclass Knn():\n    def __init__(self,y_pred2, cm2, accuracy2, report2,success2):\n        self.y_pred2 = y_pred2\n        self.cm2 = cm2\n        self.accuracy2 = accuracy2\n        self.report2 = report2\n        self.success2 = success2\n        knn = KNeighborsClassifier(n_neighbors=19, metric = \"manhattan\", weights = \"distance\")\n        knn.fit(X_train, y_train)\n        y_pred2 = knn.predict(X_test)\n        print(f' KNN : \\n{y_pred2}')\n        print(\"------------------------------------\")\n        cm2 = confusion_matrix(y_test, y_pred2)\n        print(f'confusion_ matrix: \\n{cm2}') \n        print(\"------------------------------------\")\n        accuracy2 = accuracy_score(y_test, y_pred2)\n        print(f'accuracy score: {accuracy2}')\n        print(\"------------------------------------\")\n        myList2.append(accuracy2)\n        report2 = classification_report(y_test, y_pred2)\n        print(f'classification_report: {report2}')\n        print(\"------------------------------------\")\n        success2 = cross_val_score(estimator = knn, X= X_train, y=y_train, cv=4)\n        print(f'success : {success2}')\n        p1 = [{\"n_neighbors\": range(20), \"weights\": ['uniform' ,'distance'], \"metric\": ('minkowski', 'euclidean', 'manhattan')} ]\n        gs = GridSearchCV(estimator = knn, param_grid = p1, scoring=\"accuracy\", cv = 30, n_jobs = -1)\n        grid_search = gs.fit(X_train, y_train)\n        bestresult = grid_search.best_score_\n        bestparameters = grid_search.best_params_\n        print(f'best result: {bestresult}')\n        print(f'best parameters: {bestparameters}')\nKnn(\"y_pred2\",\"cm2\", \"accuracy2\",\"report2\", \"success2\")\n         ","6959cff3":"myList3 = []\nclass Decisiontree():\n    def __init__(self,y_pred3, cm3, accuracy3, report3):\n        self.y_pred3 = y_pred3\n        self.cm3 = cm3\n        self.accuracy3 = accuracy3\n        self.report3 = report3\n        dt = DecisionTreeClassifier(criterion = 'entropy')\n        dt.fit(X_train, y_train)\n        y_pred3 = dt.predict(X_test)\n        print(f' Decision Tree : \\n{y_pred3}')\n        print(\"------------------------------------\")\n        cm = confusion_matrix(y_test, y_pred3)\n        print(f'confusion_ matrix: \\n{cm}') \n        print(\"------------------------------------\")\n        accuracy3 = accuracy_score(y_pred3, y_test)\n        print(f'accuracy score: {accuracy3}')\n        print(\"------------------------------------\")\n        myList3.append(accuracy3)\n        report3 = classification_report(y_test, y_pred3)\n        print(f'classification_report: {report3}')\n        \nDecisiontree(\"y_pred3\", \"cm3\", \"accuracy3\", \"report3\")\n         ","ea78350d":"myList4 = []\nclass Randomforest():\n    def __init__(self,y_pred4, cm4, accuracy4, report4,success4):\n        self.y_pred4 = y_pred4\n        self.cm4= cm4\n        self.accuracy4= accuracy4\n        self.report4= report4\n        self.success4 = success4\n        rfc = RandomForestClassifier(n_estimators=95, criterion = 'gini', random_state = 0)\n        rfc.fit(X_train, y_train)\n        y_pred4 = rfc.predict(X_test)\n        print(f'Random Forest : \\n{y_pred4}')\n        print(\"------------------------------------\")\n        cm4 = confusion_matrix(y_test, y_pred4)\n        print(f'confusion_ matrix: \\n{cm4}') \n        print(\"------------------------------------\")\n        accuracy4 = accuracy_score(y_pred4, y_test)\n        print(f'accuracy score: {accuracy4}')\n        print(\"------------------------------------\")\n        myList4.append(accuracy4)\n        report4 = classification_report(y_test, y_pred4)\n        print(f'classification_report: {report4}')\n        print(\"------------------------------------\")\n        success4 = cross_val_score(estimator = rfc, X= X_train, y=y_train, cv=4)\n        print(f'success : {success4}')\n        p2 = {'n_estimators': range(100), 'criterion' :['gini', 'entropy']}\n        gs = GridSearchCV(estimator=rfc, param_grid=p2, scoring = \"accuracy\", cv= 4, n_jobs = -1)\n        grid_search = gs.fit(X_train, y_train)\n        bestresult = grid_search.best_score_\n        bestparameters = grid_search.best_params_\n        print(f'best result: {bestresult}')\n        print(f'best parameters: {bestparameters}')\n\nRandomforest(\"y_pred4\",\"cm4\", \"accuracy4\", \"report4\",\"success4\")\n         \n    ","946a0afe":"myList5 = []\nclass Naivebayes():\n    def __init__(self,y_pred5, cm5, accuracy5, report5):\n        self.y_pred5 = y_pred5\n        self.cm5= cm5\n        self.accuracy5= accuracy5\n        self.report5= report5\n        gnb = GaussianNB(var_smoothing = 10.0)\n        gnb.fit(X_train, y_train)\n        y_pred5 = gnb.predict(X_test)\n        print(f'Gaussian NB : \\n{y_pred5}')\n        print(\"------------------------------------\")\n        cm5 = confusion_matrix(y_test, y_pred5)\n        print(f'confusion_ matrix: \\n{cm5}') \n        print(\"------------------------------------\")\n        accuracy5 = accuracy_score(y_pred5, y_test)\n        print(f'accuracy score: {accuracy5}')\n        print(\"------------------------------------\")\n        myList5.append(accuracy5)\n        report5 = classification_report(y_test, y_pred5)\n        print(f'classification_report: {report5}')\n        print(\"---------------------------------\")\n        success5 = cross_val_score(estimator = gnb, X= X_train, y=y_train, cv=4)\n        print(f'success : {success5}')\n        p4 = {'var_smoothing': np.logspace(0,9, num=100)}\n        gs = GridSearchCV(estimator=gnb, \n                 param_grid=p4, \n                 cv=10,  \n                 verbose=1, \n                 scoring='accuracy')\n        grid_search = gs.fit(X_train, y_train)\n        bestresult = grid_search.best_score_\n        bestparameters = grid_search.best_params_\n        print(f'best result: {bestresult}')\n        print(f'best parameters: {bestparameters}')\nNaivebayes(\"y_pred5\",\"cm5\", \"accuracy5\", \"report5\")\n         ","e20cd387":"myList6 = []\nclass XGBoost():\n    def __init__(self,y_pred, cm, accuracy, report):\n        self.y_pred = y_pred \n        self.cm = cm\n        self.accuracy = accuracy\n        self.report = report\n        xgb = XGBClassifier()\n        xgb.fit(X_train, y_train)\n        y_pred = xgb.predict(X_test)\n        print(f' XGB : \\n{y_pred}')\n        print(\"------------------------------------\")\n        cm = confusion_matrix(y_test, y_pred)\n        print(f'confusion_ matrix: \\n{cm}') \n        print(\"------------------------------------\")\n        accuracy = accuracy_score(y_pred, y_test)\n        print(f'accuracy score: {accuracy}')\n        print(\"------------------------------------\")\n        myList6.append(accuracy)\n        report = classification_report(y_test, y_pred)\n        print(f'classification_report: {report}')\nXGBoost = XGBoost(\"y_pred\",\"cm\", \"accuracy\",\"report\")   \nprint(XGBoost)    \n","496d97d6":"models = myList + myList1 + myList2 + myList3 + myList4 + myList5 + myList6\naccuracy_scores = []\nfor model in models:\n    accuracy_scores.append(model)\nprint(accuracy_scores)    ","85257f94":"plt.bar(['LG', 'SVC', 'KNN', 'DTC', 'RFC', 'GB', 'XGB'], accuracy_scores)\nplt.ylim(0.6,0.95)\nplt.title('Accuracy comparison for various models', fontsize=15, color='r')\nplt.xlabel('Models', fontsize=15, color='r')\nplt.ylabel('Accuracy Score', fontsize=15, color='r')\nplt.tight_layout()\nplt.show()","bad3c8cb":"# b) SVC","dcdf92fd":"# Heart Failure Prediction Dataset","2b2faba3":"****Import Libraries****","b21536c9":"# g) XGBoost","25f34a62":"# d) DecisionTree","d31e32bf":"![kkkkkkkkkkkkkkkkkkkkkkkkkkk.jpg](attachment:f828160d-675b-4e53-8729-4e1ff6d604bb.jpg)","5caed983":"# 7 Different Models","b8fb5022":"# f) Naive Bayes","c8ce4393":"# Data Preprocessing","fd300aaf":"****In this dataset, I will use 7 different models and GridSearchCV to improve some of these models. We will review the results together...****","c4234a37":"# a) Logistic","e1ce40ae":"# Comparison","6df4b88c":"# c) KNN","c67bb9b0":"# Data Visualization\n","11d67366":"# e)Random Forest"}}