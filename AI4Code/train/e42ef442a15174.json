{"cell_type":{"eef343c9":"code","6c27c46f":"code","560aacce":"code","c63c8fd9":"code","9514ee2c":"code","fc939d19":"code","bb715aae":"code","07837e17":"code","e1ccb0c5":"code","4821df8f":"code","7c2c9e8e":"code","87cd4781":"code","a23bd948":"code","e7ef17c3":"code","030a7b47":"code","ce28b770":"code","862c30fc":"code","7c042845":"code","75c2942d":"code","96c9c617":"code","a491794c":"code","4c4d244a":"code","afc6b228":"code","e712f3de":"code","f860756d":"code","5c46c55f":"code","8b8744e3":"code","b9c0c934":"code","d7d59910":"code","ece55ea3":"code","dbad7917":"code","320ac51f":"code","74e555be":"markdown","15ea2450":"markdown","31e1a3d2":"markdown","455d3716":"markdown","aab6ad00":"markdown","f164ae17":"markdown","984ffeda":"markdown","08dc7219":"markdown","378a8a39":"markdown","ea4d5b2d":"markdown","2a3cab80":"markdown","82c84185":"markdown","1f9e7ed8":"markdown","0795b38b":"markdown"},"source":{"eef343c9":"# For basic data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd  # data preprocessing","6c27c46f":"#For Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')","560aacce":"# reading the data\ndata = pd.read_csv('..\/input\/data.csv')","c63c8fd9":"# Shapre of data\ndata.shape","9514ee2c":"# printing few lines\ndata.head(10) # first 10","fc939d19":"# describing the data\ndata.describe()","bb715aae":"# getting the info\ndata.info()","07837e17":"# As we can see the last column contains all null values, so we can remove it\ndata = data.drop('Unnamed: 32', axis=1)","e1ccb0c5":"data.shape","4821df8f":"# check if dataset contains any null value\ndata.isnull().sum().sum()","7c2c9e8e":"# the id values are all unique and won't be required in computation\n# so we can remove it\n\ndata = data.drop('id', axis = 1)","87cd4781":"# Some Data Visualization for better understanding\n# Here B refers to Benign which means cells are safe from cancer and M means Malignant \n# which means cells are poisonous and can lead to cancer\n\n\n# 1. Bar Chart\nd = data.diagnosis\nax = sns.countplot(d, label='Count')\nB, M = d.value_counts()\nprint('Number of Benign:', B)\nprint('Number of Malignant:', M)","a23bd948":"# 2. Percentage \n# plotting a pie chart \n\nlabels = 'Benign', 'Malignant'\ncolors = ['Red', 'Green']\nexplode = [0, 0.1]\nplt.rcParams['figure.figsize'] = (6,6)\nplt.pie(d.value_counts(), colors = colors, labels = labels,explode = explode, autopct = '%.1f%%')\nplt.title('Cell Types', fontsize = 18)\nplt.show()","e7ef17c3":"# Correlation Matrix\n\ncorrelation = data.corr()\n\n# tick labels\nmatrix_cols = correlation.columns.tolist()\n\n# convert to array\ncorr_array = np.array(correlation)","030a7b47":"# Plotting Correlation heatmap\n\nf, ax = plt.subplots(figsize =(20, 15))\nsns.heatmap(correlation, mask=np.zeros_like(correlation, dtype=np.bool), cmap=sns.diverging_palette(220,20,as_cmap=True), square=True, ax=ax)","ce28b770":"sns.pairplot(data)","862c30fc":"# Box Plots for mean\n\n# box plots are useful for seeing outliers\n\nplt.rcParams['figure.figsize'] = (18,16)\n\nplt.subplot(2,2,1)\nsns.boxplot(x = 'diagnosis', y='radius_mean', data=data, palette='Blues')\nplt.title('Diagnosis vs radius_mean', fontsize=16)\n\n\nplt.subplot(2,2,2)\nsns.boxplot(x = 'diagnosis', y='texture_mean', data=data, palette='bright')\nplt.title('Diagnosis vs texture_mean', fontsize=16)\n\n\nplt.subplot(2,2,3)\nsns.boxplot(x = 'diagnosis', y='perimeter_mean', data=data, palette='spring')\nplt.title('Diagnosis vs perimeter_mean', fontsize=16)\n\n\nplt.subplot(2,2,4)\nsns.boxplot(x = 'diagnosis', y='area_mean', data=data, palette='deep')\nplt.title('Diagnosis vs area_mean', fontsize=16)\n\n\nplt.show()","7c042845":"# Boxen Plots for Smoothness\n\n# box plots are useful for seeing outliers\n\nplt.rcParams['figure.figsize'] = (18,16)\n\nplt.subplot(2,2,1)\nsns.boxenplot(x = 'diagnosis', y='smoothness_mean', data=data, palette='Blues')\nplt.title('Diagnosis vs smoothness_mean', fontsize=16)\n\n\nplt.subplot(2,2,2)\nsns.boxenplot(x = 'diagnosis', y='compactness_mean', data=data, palette='bright')\nplt.title('Diagnosis vs compactness_mean', fontsize=16)\n\n\nplt.subplot(2,2,3)\nsns.boxenplot(x = 'diagnosis', y='concavity_mean', data=data, palette='spring')\nplt.title('Diagnosis vs concavity_mean', fontsize=16)\n\n\nplt.subplot(2,2,4)\nsns.boxenplot(x = 'diagnosis', y='concave points_mean', data=data, palette='deep')\nplt.title('Diagnosis vs concave points_mean', fontsize=16)\n\nplt.show()","75c2942d":"# Strip Plots\n\nplt.rcParams['figure.figsize'] = (18,16)\n\nplt.subplot(2,2,1)\nsns.stripplot(x = 'diagnosis', y='concavity_se', data=data, palette='Blues')\nplt.title('Diagnosis vs concavity_se', fontsize=16)\n\n\nplt.rcParams['figure.figsize'] = (18,16)\nplt.subplot(2,2,2)\nsns.stripplot(x = 'diagnosis', y='concave points_se', data=data, palette='bright')\nplt.title('Diagnosis vs concave points_se', fontsize=16)\n\n\nplt.rcParams['figure.figsize'] = (18,16)\nplt.subplot(2,2,3)\nsns.stripplot(x = 'diagnosis', y='symmetry_se', data=data, palette='spring')\nplt.title('Diagnosis vs symmetry_se', fontsize=16)\n\n\nplt.rcParams['figure.figsize'] = (18,16)\nplt.subplot(2,2,4)\nsns.stripplot(x = 'diagnosis', y='fractal_dimension_se', data=data, palette='deep')\nplt.title('Diagnosis vs fractal_dimension_se', fontsize=16)\n\nplt.show()","96c9c617":"# Label encoding for dependent variable\n\n# importing label encoder\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# performing label encoding\n\nle = LabelEncoder()\ndata['diagnosis'] = le.fit_transform(data['diagnosis'])","a491794c":"data['diagnosis'].value_counts()","4c4d244a":"# splitting the dependent and independent variables from the dataset\n\nx = data.iloc[:, 1:]\ny = data.iloc[:, 0]","afc6b228":"print(x.shape)\nprint(y.shape)","e712f3de":"# splitting the dataset into training and testing dataset\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 8)","f860756d":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","5c46c55f":"# standard Scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n                   \nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)","8b8744e3":"# importing libraries for calculating prediction scores\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, cross_val_score","b9c0c934":"# Logistic Regression\n\nfrom sklearn.tree import DecisionTreeClassifier\n# create the model\nmodel = DecisionTreeClassifier()\n\n# feed the training data into model\nmodel.fit(x_train, y_train)\n\n# predict the test result \ny_pred = model.predict(x_test)\n\n# Calculating the accuracy\nprint('Training accuracy: ', model.score(x_train, y_train))\nprint('Test Accuracy: ', model.score(x_test, y_test))\n\n# classification report\ncr = classification_report(y_test, y_pred)\nprint(cr)\n\n# confusion matrix\nplt.rcParams['figure.figsize']= (5, 5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True)","d7d59910":"# Feature importance of decision tree\n\nfeatures = data.columns\nimp = model.feature_importances_\nindices = np.argsort(imp)\n\nplt.rcParams['figure.figsize']=(15,15)\nplt.barh(range(len(indices)), imp[indices])\nplt.yticks(range(len(indices)), features[indices])\nplt.title('Feature importance for Decision Tree', fontsize=25)\nplt.grid()\nplt.tight_layout()\nplt.show()","ece55ea3":"# Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# creating a model\nmodel = RandomForestClassifier()\n\n# feeding the training data\nmodel.fit(x_train, y_train)\n\n# predcit the test results\ny_pred = model.predict(x_test)\n\n# Calculating the accuracy\nprint('Training accuracy: ', model.score(x_train, y_train))\nprint('Test accuracy: ', model.score(x_test, y_test))\n\n# Classification report\ncr = classification_report(y_test, y_pred)\nprint(cr)\n\n# confusion matrix\nplt.rcParams['figure.figsize'] = (5, 5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, cmap='Blues')","dbad7917":"# Feature importance for random forest\nfeatures = data.columns\nimportance = model.feature_importances_\nindices = np.argsort(importance)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.barh(range(len(indices)), importance[indices])\nplt.yticks(range(len(indices)), features[indices])\nplt.title('Feature Importance for Random Forest', fontsize = 30)\nplt.grid()\nplt.tight_layout()\nplt.show()","320ac51f":"# Support Vector classfier\n\nfrom sklearn.svm import SVC\n\n# Create a model\nmodel = SVC()\n\n# feed the training data\nmodel.fit(x_train, y_train)\n\n# predicting test results\ny_pred = model.predict(x_test)\n\n# Claculating the accuracy\nprint('Training accuracy: ', model.score(x_train, y_train))\nprint('Test accuracy: ', model.score(x_test, y_test))\n\n# Classification repot\ncr = classification_report(y_test, y_pred)\nprint(cr)\n\n# confusion matrix\nplt.rcParams['figure.figsize'] = (5,5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, cmap = 'Greens')","74e555be":"Decision Tree","15ea2450":"Pairplot","31e1a3d2":"Box Plots","455d3716":"Thanks for reading the kernel.","aab6ad00":"Support Vector Classifier","f164ae17":"Random Forest Classifier","984ffeda":"Importing necessary libraries","08dc7219":"Data Visualizations","378a8a39":"Data Preprocessing","ea4d5b2d":"Reading Data","2a3cab80":"Support Vector Classifiers show the best results.","82c84185":"Strip Plots","1f9e7ed8":"Boxen Plots","0795b38b":"Modelling"}}