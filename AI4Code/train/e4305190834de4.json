{"cell_type":{"73b37b97":"code","9693a181":"code","210aab60":"code","6fff2c60":"code","12da318c":"code","65876a54":"code","2042209f":"code","b83c6e30":"code","2a946cd4":"code","af24715f":"code","fb81567d":"code","4f71c20a":"code","314a0954":"code","b84a03bb":"code","bc4d95ea":"code","7fdd67aa":"code","6787bedd":"code","6257f367":"code","21002841":"code","1f7a5d41":"code","0f05f512":"code","be6d940e":"code","9e5bb9f7":"code","252f1533":"code","f5e9931d":"code","140c8e74":"code","77f834ae":"code","cce33ab9":"code","7623db17":"markdown","0c30e06a":"markdown","807ef3ab":"markdown","2bf8e17f":"markdown","a203c91a":"markdown","ef85686b":"markdown","d760ef95":"markdown","804849e3":"markdown","4cac5d66":"markdown","6cb12f2a":"markdown","f0abcfe8":"markdown","0ab3a603":"markdown"},"source":{"73b37b97":"# Libs to deal with tabular data\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\n\n# Statistics\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats.contingency import expected_freq\n\n# Plotting packages\nimport seaborn as sns\nsns.axes_style(\"darkgrid\")\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# Machine Learning\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.feature_selection import mutual_info_classif\nfrom boruta import BorutaPy\n\nfrom lightgbm import LGBMRegressor\n\n# Optimization\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour, plot_optimization_history\nfrom optuna.visualization import plot_param_importances, plot_slice\n\n# To display stuff in notebook\nfrom IPython.display import display, Markdown\n\n# Misc \nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nimport time\nimport os\nimport glob","9693a181":"# data directory\nDATA_DIR = '..\/input\/optiver-realized-volatility-prediction\/'","210aab60":"def compute_wap(df, index):\n    numerator = df[f'bid_price{index}'] * df[f'ask_size{index}'] + df[f'ask_price{index}'] * df[f'bid_size{index}'] \n    wap = numerator \/ (df[f'bid_size{index}'] + df[f'ask_size{index}'])\n    return wap\n\ndef compute_realized_volatility(returns):\n    return np.sqrt(np.sum(returns**2))","6fff2c60":"def create_book_features(df, windows = [600]):\n    # compute prices and returns\n    for idx in [1,2]:\n        df[f'wap{idx}'] = compute_wap(df, idx)\n        df[f'log_wap{idx}'] = np.log(df[f'wap{idx}'])\n        df[f'log_return{idx}'] = df.groupby('time_id')[f'log_wap{idx}'].diff()\n    \n    # compute general book features\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # compute aggregations over the features created above for different time windows\n    feature_creation_dict = {\n        'log_return1':[compute_realized_volatility],\n        'log_return2':[compute_realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap1':[np.mean]\n    }\n    \n    window_features = []\n    for seconds in windows:\n        df_window = df.loc[df['seconds_in_bucket'] >= (600 - seconds), :] if seconds != 600 else df\n        df_features = df_window.groupby(['time_id']).agg(feature_creation_dict)\n        df_features.columns = ['_'.join(col) + f'_l{seconds}' for col in df_features.columns] # join multi-index column names\n        window_features.append(df_features)\n        \n    df_features = pd.concat(window_features, axis=1, copy=False)     \n    return df_features","12da318c":"%%time\nstock_id = 0\nfile_path = DATA_DIR + f\"book_train.parquet\/stock_id={stock_id}\"\ndf = pd.read_parquet(file_path)\nbook_features = create_book_features(df, [600, 300])","65876a54":"book_features.head()","2042209f":"def create_trade_features(df, windows = [600]):\n    # compute return\n    df['log_price'] = np.log(df['price'])\n    df['log_return'] = df.groupby('time_id')['log_price'].diff()\n    \n    # compute aggregations for different time windows\n    feature_creation_dict = {\n        'log_return':[compute_realized_volatility],\n        'seconds_in_bucket':'nunique',\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    window_features = []\n    for seconds in windows:\n        df_window = df.loc[df['seconds_in_bucket'] >= (600 - seconds), :] if seconds != 600 else df\n        df_features = df_window.groupby(['time_id']).agg(feature_creation_dict)\n        df_features.columns = ['_'.join(col) + f'_l{seconds}' for col in df_features.columns] # join multi-index column names\n        window_features.append(df_features)\n    \n    df_features = pd.concat(window_features, axis=1, copy=False)   \n    return df_features","b83c6e30":"%%time\nstock_id = 0\nfile_path = DATA_DIR + f\"trade_train.parquet\/stock_id={stock_id}\"\ndf = pd.read_parquet(file_path)\ntrade_features = create_trade_features(df, [600, 300])","2a946cd4":"trade_features.head()","af24715f":"def preprocessor(stock_id_list, mode = 'train', windows = [600, 300]):\n    # the function above will be parallelized\n    def create_stock_features(stock_id):\n        book = pd.read_parquet(f\"{DATA_DIR}book_{mode}.parquet\/stock_id={stock_id}\")\n        trade = pd.read_parquet(f\"{DATA_DIR}trade_{mode}.parquet\/stock_id={stock_id}\")\n\n        features = create_book_features(book, windows).join(\n            create_trade_features(trade, windows), \n            how='outer'\n        )\n        \n        # create row_id\n        features['row_id'] = features.index.map(lambda x: f'{stock_id}-{x}')\n        features = features.reset_index(drop=True)\n\n        return features\n    \n    features_list = Parallel(n_jobs=-1, verbose=1)(\n        delayed(create_stock_features)(stock_id) for stock_id in stock_id_list\n    )\n    features = pd.concat(features_list, ignore_index = True)\n    return features","fb81567d":"list_stock_ids = [0,1]\nfeatures = preprocessor(list_stock_ids, 'train')\nfeatures.head()","4f71c20a":"# Reading train file, which maps stock_id and time_it to the target\ntrain = pd.read_csv(DATA_DIR + 'train.csv')\ntrain_stock_ids = train.stock_id.unique()\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\n\n# Creating train dataset\ndf_train = preprocessor(train_stock_ids, 'train')\ndf_train = train.merge(df_train, on=['row_id'], how='left')\n\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x: x.split('-')[0]).astype(int)","314a0954":"df_train.shape","b84a03bb":"X = df_train.drop(['row_id','target'],axis=1)\ny = df_train['target']","bc4d95ea":"def rmspe(y_true, y_pred):\n    metric_val = (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    return  'rmspe', metric_val, False","7fdd67aa":"class Light_GBM_CV:\n    def __init__(self, X, y, folds=5, random_state=42):\n        self.X = X\n        self.y = y\n        self.folds = folds\n        self.random_state = random_state\n\n    def __call__(self, trial):\n        cv = KFold(\n            self.folds, \n            random_state = self.random_state, \n            shuffle=True\n        )\n        \n        clf = LGBMRegressor(\n            boosting_type = 'gbdt',\n            objective = 'rmse',\n            random_state = self.random_state,\n            first_metric_only = True,\n            num_leaves = trial.suggest_int('num_leaves', 16, 256),\n            max_depth = trial.suggest_int('max_depth', 4, 8),\n            learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1.0),\n            min_child_samples = trial.suggest_int('min_child_samples', 5, 100),\n            n_estimators = trial.suggest_int('n_estimators', 10, 1000),\n            lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-5, 1.0),\n            lambda_l2 = trial.suggest_loguniform('lambda_l2', 1e-5, 1.0),\n            max_bin = trial.suggest_int('max_bin', 10, 256),\n            feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1),\n            bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1),\n            categorical_feature = ['stock_id']\n        )\n        \n        cv_scores = []\n\n        for array_idxs in cv.split(self.X):\n            train_index, val_index = array_idxs[0], array_idxs[1]\n            X_train, X_val = self.X.loc[train_index], self.X.loc[val_index]\n            y_train, y_val = self.y.loc[train_index], self.y.loc[val_index]\n            \n            clf.fit(\n                X_train, y_train,\n                sample_weight = 1 \/ np.square(y_train),\n                eval_set = [(X_val, y_val), (X_train, y_train)],\n                eval_metric = rmspe,\n                early_stopping_rounds = 10,\n                verbose = False,\n                categorical_feature = ['stock_id']\n            )\n            cv_scores.append(clf.best_score_['valid_0']['rmspe'])\n\n        return sum(cv_scores) \/ len(cv_scores)","6787bedd":"lgbm_cv = Light_GBM_CV(X, y)\nstudy = optuna.create_study(sampler=TPESampler(seed = 42), direction='minimize')\nstudy.optimize(lgbm_cv, n_trials=100)","6257f367":"print('Best model')\nprint('Mean validation RMSPE: ', study.best_value, '\\n')","21002841":"study.best_params","1f7a5d41":"%%time\n\nmodels_list = []\ncv_scores = []\n\nkf = KFold(n_splits=5, random_state=19901028, shuffle=True)\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print(\"Fold :\", fold+1)\n    \n    # Dataset creation\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    # Modelling\n    model = LGBMRegressor(\n        objective = \"rmse\",\n        boosting_type = \"gbdt\",\n        importance_type = 'gain',\n        first_metric_only = True,\n        random_state = 42,\n        categorical_feature = ['stock_id'],\n        **study.best_params\n    )\n    \n    model.fit(\n        X_train, y_train,\n        sample_weight = 1 \/ np.square(y_train),\n        eval_set = [(X_valid, y_valid), (X_train, y_train)],\n        eval_metric = rmspe,\n        early_stopping_rounds = 30,\n        verbose = 100,\n        categorical_feature = ['stock_id']\n    )\n    \n    # validation\n    rmspe_val = model.best_score_['valid_0']['rmspe']\n    print(f'Performance fold #{fold+1}: {rmspe_val}')\n\n    #keep scores and models\n    cv_scores.append(rmspe_val)\n    models_list.append(model)\n    print(\"*\" * 100)","0f05f512":"print(f'CV score:', pd.Series(cv_scores).mean())\ncv_scores","be6d940e":"raw_imp_vetors = [model.feature_importances_.reshape(1, -1) for model in models_list]\nraw_imp_matrix = np.concatenate(raw_imp_vetors, axis=0)\nnorm_imp = raw_imp_matrix \/ raw_imp_matrix.sum(1).reshape(-1, 1)\nmean_imp = norm_imp.mean(0)\nimp_series = pd.Series(mean_imp, index=X.columns).sort_values(ascending=False)","9e5bb9f7":"imp_series","252f1533":"plt.figure(figsize=(10, 8))\nsns.barplot(x = imp_series.values, y = imp_series.index, color='lightblue')\nplt.title('Normalized CV feature importance (gain)', fontsize=16)\nplt.show()","f5e9931d":"# Reading test file\ntest = pd.read_csv(DATA_DIR + 'test.csv')\ntest_stock_ids = test.stock_id.unique()\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\ntest = test[['row_id']]\n\n# Creating train dataset\ndf_test = preprocessor(test_stock_ids, 'test')\ndf_test = test.merge(df_test, on=['row_id'], how='left')\n\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x: x.split('-')[0]).astype(int)","140c8e74":"submission = df_test[['row_id']]\nX_test = df_test.drop(columns=['row_id'])\n\n# Scoring ensemble\ntarget = np.zeros(len(X_test))\nfor model in models_list:\n    pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    target += pred \/ len(models_list)\n\nsubmission = submission.assign(target = target)    ","77f834ae":"submission.head()","cce33ab9":"submission.to_csv('submission.csv', index = False)","7623db17":"## Combined preprocessor function","0c30e06a":"# Test set","807ef3ab":"### Cross validation","2bf8e17f":"## Training set","a203c91a":"## Feature importance","ef85686b":"## Main function for preprocessing book data","d760ef95":"## Main function for preprocessing trade data","804849e3":"# Feature engineering pipeline with optimized LightGBM\n\nThis notebook is an adaptation of [this notebook](https:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea) with improvements in organization, performance, code generalization and readability. I also changed the modelling and feature importance parts to use the scikit-learn API and focus on normalized gain. A lot more could be done in the feature engineering but I'm going to leave it this way, since for me now it has a good organization to keep progressing. Hyperparameter optimization with Optuna was implemented as well.\n\n**If you think this is relevant or helped you, please give it an upvote. Thanks!**","4cac5d66":"## Preparation","6cb12f2a":"## Simple feature creation functions","f0abcfe8":"## LightGBM","0ab3a603":"### Hyperparameter optimization"}}