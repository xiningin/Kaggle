{"cell_type":{"5073ecc9":"code","22b10c8b":"code","d4d11916":"code","050d6660":"code","408c3064":"code","e7c9cc6e":"code","3437e719":"code","8956257b":"code","51fad7b5":"code","8012a066":"code","51efe3a6":"code","68fbe4b0":"code","5c26bbbf":"code","b9464bff":"code","4184e057":"code","05aab8b2":"code","ea848d7a":"code","932a46a8":"code","c93cc423":"code","a348f4c9":"code","061694fc":"code","a437beb6":"code","747a1828":"code","33678d10":"code","0fd470f8":"code","31300870":"code","ebf0eeed":"code","565e3b04":"code","b27a7278":"code","922354ab":"code","38033005":"code","94e9fbc2":"code","5e0a2970":"code","8a0ee31c":"code","62303fe5":"code","2690b7f8":"code","fddbe0a0":"code","eb9a65ed":"code","221a9db5":"code","5a61b1fa":"code","24d22783":"code","21cbdc22":"code","3bba93f1":"code","1656b0a6":"code","e1bc4b5f":"code","3be43ef7":"code","08479682":"code","be4be39d":"code","f9bac65a":"code","797a676c":"code","4b3f0e55":"code","9cccc8bd":"code","cd02db07":"code","f3e01ed6":"code","b6ff60c2":"code","7e8fd02c":"code","1ea03d70":"code","cf39322f":"code","6269b56c":"code","1d738727":"code","966dc5d5":"code","ce60634a":"code","9babb982":"code","1a03185e":"code","a3915d99":"code","5fc64ab8":"code","a7e3a875":"code","865ea50c":"code","57f70dfa":"code","0629a360":"code","34943436":"code","88ac376e":"code","b9aa3ce8":"code","e7a2aef9":"markdown","1cf93cd2":"markdown","fadea918":"markdown","595308f5":"markdown","56303dd4":"markdown","aba30f9e":"markdown","d42c78eb":"markdown","bd9cceca":"markdown","f6766b5a":"markdown","70332d82":"markdown","c74b69e2":"markdown","0c534bdc":"markdown","962bef31":"markdown","8c2d85b0":"markdown","ad855fd1":"markdown"},"source":{"5073ecc9":"#!pip -q install \"dask[complete]\"","22b10c8b":"#!pip -q install \"dask-ml[complete]\"","d4d11916":"#!pip -q install --upgrade --ignore-installed numpy pandas scipy sklearn","050d6660":"## https:\/\/stackoverflow.com\/questions\/49853303\/how-to-install-pydot-graphviz-on-google-colab?rq=1\n#!pip -q install graphviz \n#!apt-get install graphviz -qq\n#!pip -q install pydot","408c3064":"#!pip -q install bokeh","e7c9cc6e":"import numpy as np\nimport pandas as pd\nimport dask.array as da\nimport graphviz","3437e719":"import matplotlib.pyplot as plt","8956257b":"arr = np.random.randint(1, 1000, (1000, 1000))\ndarr = da.from_array(arr, chunks=(250, 250))\ndarr","51fad7b5":"darr.visualize(color=\"order\", size=\"9,10!\")","8012a066":"darr.chunks, darr.chunksize, darr.npartitions","51efe3a6":"res = darr.sum(axis=0)","68fbe4b0":"res.visualize(rankdir=\"LR\", size=\"3,20!\") # Graph of methods we applied\n# If we have a graph structure with many independent nodes per level in our implementation, Dask will be able to \n# parallelize it and we will get speedup, if our problem is sufficiently large.","5c26bbbf":"res.compute().shape","b9464bff":"def numpy_mean(size=(10, 10)):\n  arr = np.random.random(size=size)\n  return arr.mean()\n\ndef dask_mean(size=(10, 10)):\n  if size[0] > 10000: chunks = (1000, 1000)\n  else: chunks = (int(size[0]\/10), int(size[1]\/10))\n  \n  arr = da.random.random(size=size, chunks=chunks)\n  y = arr.mean()\n  return y.compute()","4184e057":"import time\n\ndef dask_arr_chk():\n  sizes = []\n  times = []\n  size = 10\n  for i in range(5):\n    dim1 = size ** (i+1)\n    for j in range(4):\n      dim2 = size ** (j+1)\n      if dim1*dim2 in sizes: continue\n      st = time.time()\n      dask_mean(size=(dim1, dim2))\n      en = time.time()\n      sizes.append(dim1*dim2)\n      times.append(en-st)\n  return sizes, times\n\ndef numpy_arr_chk():\n  sizes = []\n  times = []\n  size = 10\n  for i in range(4):\n    dim1 = size ** (i+1)\n    for j in range(4):\n      dim2 = size ** (j+1)\n      if dim1*dim2 in sizes: continue\n      st = time.time()\n      numpy_mean(size=(dim1, dim2))\n      en = time.time()\n      sizes.append(dim1*dim2)\n      times.append(en-st)\n  return sizes, times","05aab8b2":"%%time\nx1, y1 = numpy_arr_chk()\nx2, y2 = dask_arr_chk()","ea848d7a":"fig, axs = plt.subplots(1, 3, figsize=(23, 5))\naxs[0].plot(x1[:-1], y1[:-1], \"o-\", label=\"Numpy\")\naxs[0].plot(x2[:-2], y2[:-2], \"o-\", label=\"Dask\")\naxs[0].set_xlabel(\"Array elements:\")\naxs[0].set_ylabel(\"Time Taken (sec):\")\naxs[0].legend()\n\naxs[1].plot(x1, y1, \"o-\", label=\"Numpy\")\naxs[1].plot(x2[:-1], y2[:-1], \"o-\", label=\"Dask\")\naxs[1].set_xlabel(\"Array elements:\")\naxs[1].set_ylabel(\"Time Taken (sec):\")\naxs[1].legend()\n\naxs[2].plot(x1, y1, \"o-\", label=\"Numpy\")\naxs[2].plot(x2, y2, \"o-\", label=\"Dask\")\naxs[2].set_xlabel(\"Array elements:\")\naxs[2].set_ylabel(\"Time Taken (sec):\")\naxs[2].legend()","932a46a8":"import dask.dataframe as dd\nimport numpy as np\nimport gc\ngc.enable()","c93cc423":"arr = np.random.normal(0.0, 1.0, size=(1000000, 10))\ndf = dd.from_array(arr, chunksize=50000, columns=[f\"col-{i+1}\" for i in range(10)])\ndel arr\ngc.collect()","a348f4c9":"df","061694fc":"df.visualize(size=\"14,16!\")","a437beb6":"df.head() # Not lazy beacuse it doesn't take much computation","747a1828":"df.tail()","33678d10":"df[\"col-1\"] = (df[\"col-1\"]*10).astype(int)","0fd470f8":"agg = df.groupby(by=[\"col-1\"]).aggregate([\"sum\", \"std\", \"max\", \"min\", \"mean\"])","31300870":"agg.head(2)","ebf0eeed":"columns = []\nfor col in agg.columns.levels[0]:\n  for a in agg.columns.levels[1]:\n    columns.append(f\"{col}.{a}\")\n\nagg.columns = columns\nagg.head(2)","565e3b04":"df_new = df.merge(agg.reset_index(), how=\"left\", on=\"col-1\")","b27a7278":"df_new","922354ab":"df_new.visualize(rankdir=\"LR\", size=\"20, 15!\")","38033005":"df_new.compute().head()","94e9fbc2":"df_new.shape[0].compute(), df_new.shape[1]","5e0a2970":"import dask.bag as db\n\nlst = []\nfor i in range(5):\n  lst.append({f\"Name.{name}\": value for name, value in np.random.randint(1, 10, (5, 2))})\n  lst.append(np.random.randint(2, 5, (2, 4)))\n  lst.append(np.random.randint(1, 1000, (1,)))\n  lst.append([i for i in range(100, 200, 10)])\n  \nb = db.from_sequence(lst)\nb.take(1)","8a0ee31c":"def fil(el):\n  if type(el)!=dict and type(el)!=list: return True\n  else: return False\n\nfilmap = b.filter(fil).map(lambda x: x**2)","62303fe5":"filmap.visualize(size=\"15,10!\")","2690b7f8":"filmap.compute()","fddbe0a0":"comp = filmap.flatten().mean()","eb9a65ed":"comp.visualize(size=\"15, 15!\")","221a9db5":"comp.compute()","5a61b1fa":"import dask.delayed as delay\n\n@delay\ndef sq(x):\n  return x**2\n\n@delay\ndef add(x, y):\n  return x+y\n\n@delay\ndef sum(arr):\n  sum = 0\n  for i in range(len(arr)): sum+=arr[i]\n  return sum","24d22783":"# Adding tasks here is like adding nodes to graphs.\n# You can add new taks based on results of prevoius tasks.\n# Dask won't compute them right away. It will make a graph as\n# you call them. And then COmpute the whole graph parallely.\nlst = list(np.arange(1, 11))\n\nfor i in range(3):\n  temp = []\n  if i == 0:\n    for j in range(0, len(lst)):\n      temp.append(sq(lst[j]))\n  elif i == 1:\n    for j in range(0, len(lst)-1, 2):\n      temp.append(add(lst[j], lst[j+1]))\n  else:\n    temp.append(sum(lst))\n  lst = temp # New functions will be computed on last results\n  \nlst","21cbdc22":"lst[0].visualize(size=\"7,10!\")","3bba93f1":"lst[0].compute()","1656b0a6":"from dask.distributed import Client, LocalCluster # Look into parameters of LocalCluster for arguments used","e1bc4b5f":"client = Client(processes=False, threads_per_worker=4, n_workers=4, memory_limit='8GB')\nclient","3be43ef7":"def sq(x):\n  return x**2\n\ninputs = np.arange(0, 10000000)","08479682":"sent = client.submit(sq, 1000000)\nsent # Pending: Not Complete","be4be39d":"sent # Finished (after a few sec): Complete ","f9bac65a":"result = sent.result()\nresult","797a676c":"sent = client.submit(sq, inputs,)\nsent","4b3f0e55":"sent","9cccc8bd":"sent.result()","cd02db07":"from dask_ml.datasets import make_regression\nimport dask.dataframe as dd\n\nX, y = make_regression(n_samples=1e6, chunks=50000)","f3e01ed6":"df = dd.from_dask_array(X)\ndf.head()","b6ff60c2":"from dask_ml.model_selection import train_test_split, GridSearchCV\n\nxtr, ytr, xval, yval = train_test_split(X, y)","7e8fd02c":"from sklearn.linear_model import ElasticNet\n\nsearch_params = {\n    \"alpha\": [.01, .005],\n    \"l1_ratio\": [0.6, 0.8],\n    \"normalize\": [True, False],\n}","1ea03d70":"gsearch = GridSearchCV(ElasticNet(), search_params, cv=10)\n#gsearch.fit(X, y)","cf39322f":"#gsearch.best_params_","6269b56c":"#gsearch.best_score_","1d738727":"from dask_ml.datasets import make_classification\nimport dask.dataframe as dd\n\nX, y = make_classification(n_samples=1e6, chunks=50000) # number of classes here are 2","966dc5d5":"from dask_ml.model_selection import train_test_split, GridSearchCV\n\nxtr, xval, ytr, yval = train_test_split(X, y)","ce60634a":"from sklearn.linear_model import LogisticRegression\n\nsearch_params = {\n    \"C\": [.05, .005],\n    #\"penalty\": [\"l2\", \"l1\"],\n    \"class_weight\": [None, \"balanced\"],\n    \"solver\": [\"lbfgs\"]\n}","9babb982":"gsearch = GridSearchCV(LogisticRegression(), search_params, cv=10)\n#gsearch.fit(X, y)","1a03185e":"#gsearch.best_params_","a3915d99":"#gsearch.best_score_","5fc64ab8":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","a7e3a875":"from sklearn.externals import joblib\n\n#client = Client() # Not able to make client here\nLR = LogisticRegression(C=0.01, class_weight=\"balanced\", penalty=\"l2\", solver=\"lbfgs\")","865ea50c":"#with joblib.parallel_backend('dask'):\n#    LR.fit(xtr, ytr)\n#    preds = LR.predict(xval)\n#    \n#preds[0:5], yval[0:5]","57f70dfa":"#preds[0:5], yval.compute()[0:5]","0629a360":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","34943436":"xtr.shape, ytr.shape, xval.shape, yval.shape","88ac376e":"from dask_ml.cluster import KMeans\n\nKM = KMeans(n_clusters=2)\n\nKM.fit(xtr, ytr)\npreds = KM.predict(xval)\n\npreds[0:5], yval[0:5]","b9aa3ce8":"preds.compute()[0:5], yval.compute()[0:5]","e7a2aef9":"### a) Array: <a id=\"arr\"><\/a>","1cf93cd2":"## Using Sklearn's estimators:","fadea918":"# 2. Data Types  <a id=\"dtypes\"><\/a>","595308f5":"### a) Regression: ","56303dd4":"This goes along with my Medium post. All posts are here:\n1. [Speed Up your Algorithms Part 1\u200a-\u200aPyTorch](https:\/\/medium.com\/r\/?url=https%3A%2F%2Ftowardsdatascience.com%2Fspeed-up-your-algorithms-part-1-pytorch-56d8a4ae7051)\n1. [Speed Up your Algorithms Part 2\u200a-\u200aNumba](https:\/\/medium.com\/r\/?url=https%3A%2F%2Ftowardsdatascience.com%2Fspeed-up-your-algorithms-part-2-numba-293e554c5cc1)\n1. [Speed Up your Algorithms Part 3\u200a-\u200aParallelization](https:\/\/towardsdatascience.com\/speed-up-your-algorithms-part-3-parallelization-4d95c0888748)\n1. [Speed Up your Algorithms Part 4\u200a-\u200aDask](https:\/\/towardsdatascience.com\/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef)\n\n## Index\n2. [Data Types](#dtypes)\n    * [Array](#arr)\n    * [Data Frame](#dataframe)\n    * [Bag](#bag)\n3. [Delayed](#delayed)\n4. [Distributed](#distributed)\n5. [Machine Learning](#ml)\n\n\n# Initial","aba30f9e":"In low dimensions, numpy is taking less time than Dask because Dask has to create many processes for the number of workers we set during definition of *Client* declaration (Fig 1). But as number of array elements increases we see that Dask takes less time than Numpy (Fig 2). Beyond that numpy is not able to compute because it is not able to bring whole array into memory, but Dask is able to by computings many blocks in sequential order. (Fig 3).","d42c78eb":"### c) Bag: <a id=\"bag\"><\/a>","bd9cceca":"# Import","f6766b5a":"### b) DataFrame: <a id=\"dataframe\"><\/a>","70332d82":"## Using Dask's Inbuilt Estimators:\n","c74b69e2":"# 3. Delayed <a id=\"delayed\"><\/a>","0c534bdc":"### c) Using Dask and Joblib:","962bef31":"# 5. Machine Learning <a id=\"ml\"><\/a>","8c2d85b0":"### b) Classification:","ad855fd1":"# 4. Distributed <a id=\"distributed\"><\/a>"}}