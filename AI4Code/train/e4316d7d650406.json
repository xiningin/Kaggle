{"cell_type":{"75540264":"code","69c37580":"code","90799115":"code","151fb054":"code","12e336fd":"code","15e5c14f":"code","c40f2bad":"code","e3cbfbea":"code","dab104b8":"code","ed4395ee":"code","b3e58deb":"code","8e226fd3":"code","0a2ed11f":"code","44258a3c":"code","cd0f5611":"code","68ea7b94":"code","a844d771":"code","ac2bb75a":"code","1027dd8e":"code","2d5a7008":"code","b2ffbadc":"code","2ce70672":"code","149f18a9":"code","2af3fb99":"code","9c355a36":"code","2aa1114a":"code","9ba5080e":"code","bcaf63d1":"code","11aab228":"code","a22683a6":"code","4c92fe58":"code","fffe18fb":"code","6133898e":"markdown","5e6ed118":"markdown","f9fd2553":"markdown","577c62d0":"markdown","f8e64b40":"markdown","5bfe7054":"markdown","3ad98916":"markdown","3cd9ee41":"markdown","0190a729":"markdown","e5af1450":"markdown","99b8c621":"markdown","caec481a":"markdown","20da2207":"markdown","50764ea6":"markdown","e48295ea":"markdown"},"source":{"75540264":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","69c37580":"\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb","90799115":"path = '..\/input\/'\ntrain_data = pd.read_csv(path+'train.csv',index_col=0)\ntest_data = pd.read_csv(path+'test.csv',index_col=0)\ntest_sub = pd.read_csv(path+'sample_submission.csv',index_col=0)\n\ntrain_y = train_data['SalePrice']\ntrain_data.drop(['SalePrice'],axis=1,inplace=True)\n\ndata = train_data.append(test_data,sort=False)\nfeatures = data.columns\nsns.set_style('whitegrid')","151fb054":"#1.\u7279\u5f81\u7f3a\u5931\u6bd4\u7387\ndef Na_rate(data):\n    #\u7279\u5f81\u7f3a\u5931\u6bd4\u7387\n    na_df = pd.DataFrame(columns=['feature_name','na_rate'])\n    na_rate = []\n    for i in data.columns:\n        rate = data[i].isnull().sum()*1.0\/len(data[i])\n        na_rate.append(rate)\n        \n    na_df['feature_name'] = features\n    na_df['na_rate'] = na_rate\n    na_df = na_df[na_df['na_rate']!=0].sort_values(by='na_rate',ascending=False)\n    print('miss_features_num',na_df.shape[0])\n    return na_df\n\n\n#2.\u533a\u5206\u7c7b\u522b\u548c\u6570\u503c\u578b\u53d8\u91cf\ndef Dis_features(data):\n    #\u533a\u5206\u7c7b\u522b\u548c\u6570\u503c\u578b\u53d8\u91cf\n    cat_features = []\n    num_features = []\n    for i in data.columns:\n        if data[i].dtype == 'object':\n            cat_features.append(i)\n        else:\n            num_features.append(i)\n    if 'Id' in num_features:\n        num_features.remove('Id')\n    print('\u7c7b\u522b\u578b\u53d8\u91cf\u5171\u6709\uff1a',len(cat_features))\n    print('\u6570\u503c\u578b\u53d8\u91cf\u6709\uff1a',len(num_features))\n    \n    return cat_features,num_features\n\n#3.\u5bf9\u6709\u539f\u56e0\u7f3a\u5931\u5206-1\u548c1\u586b\u5145\ndef Fill_reason_missing(data,features):\n    #\u7ed9\u6570\u636e\u96c6\u3001\u9700\u8981\u5904\u7406\u7684\u7279\u5f81\n    data1 = data.copy()\n    for i in features:\n        data1[i] = data1[i].fillna(-1)\n        data1[i] = data1[i].map(lambda x:x if x==-1 else 1)\n    return data1\n\n#4.\u5bf9\u6570\u503c\u578b\u53d8\u91cf\u7f3a\u5931\u503c\u8fdb\u884c\u7b80\u5355\u586b\u5145\ndef Fill_num_miss(data,col):\n    data1 = data.copy()\n    for i in col:\n        data1[i] = SimpleImputer().fit_transform(data1[i].values.reshape(-1,1))\n    return data1\n\n#5.\u5bf9\u7c7b\u522b\u578b\u53d8\u91cf\u8fdb\u884c\u4f17\u6570\u586b\u5145\ndef Fill_cat_miss(data,col):\n    data1 = data.copy()\n    for i in col:\n        data1[i] = data1[i].fillna(data1[i].dropna().mode()[0])\n    return data1\n\n#6.\u5bf9\u7c7b\u522b\u578b\u53d8\u91cf\u8fdb\u884c\u7f16\u7801\ndef Cat_encoder(data,col):\n    data1 = data.copy()\n    for i in col:\n        data1[i] = pd.factorize(data1[i])[0]\n    return data1\n\n#7.\u5bf9\u6570\u503c\u578b\u53d8\u91cf\u8fdb\u884c\u5f52\u4e00\u5316\ndef Standard_num(data,col):\n    data1 = data.copy()\n    for i in col:\n        data1[i] = StandardScaler().fit_transform(data1[i].values.reshape(-1,1))\n    return data1","12e336fd":"#\u56de\u5f52\u95ee\u9898\nprint('\u504f\u5ea6\uff1a',train_y.skew())\nprint('\u5cf0\u5ea6\uff1a',train_y.kurt())\ntrain_y.hist(bins=50)\nplt.show()\n#\u76ee\u6807\u53d8\u91cf\u5927\u81f4\u662f\u4e00\u4e2a\u53f3\u504f\u7684\u6b63\u6001\u5206\u5e03","15e5c14f":"#\u7531\u4e8e\u76ee\u6807\u53d8\u91cf\u53f3\u504f\uff0c\u56e0\u6b64\u9009\u62e9\u03bb=0\u7684box-cox\u53d8\u6362\uff0c\u53ef\u4ee5\u51cf\u5c0f\u4e0d\u53ef\u89c2\u6d4b\u8bef\u5dee\u548c\u53d8\u91cf\u7684\u76f8\u5173\u6027\uff0c\u964d\u4f4e\u6570\u636e\u5206\u5e03\u7684\u504f\u5ea6\uff0c\u4f7f\u5f97\u6570\u636e\u66f4\u63a5\u8fd1\u6b63\u6001\u5206\u5e03\nlog_train_y = np.log(1+train_y)\nprint('\u504f\u5ea6\uff1a',log_train_y.skew())\nprint('\u5cf0\u5ea6\uff1a',log_train_y.kurt())\nlog_train_y.hist(bins=50)\nplt.show()","c40f2bad":"cat_all, num_all = Dis_features(data)","e3cbfbea":"na_train = Na_rate(train_data)\nprint(na_train)\n","dab104b8":"#\u5408\u5e76\u603b\u6570\u636e\u7684\u7f3a\u5931\u60c5\u51b5\nna_all = Na_rate(data)\nprint(na_all)\n#\u5bf9\u4e8e\u8d85\u8fc740%\u7f3a\u5931\u7684\u8fd9\u51e0\u4e2a\u53d8\u91cf\u8fdb\u884c\u89c2\u5bdf\uff0c\u53d1\u73b0\u90fd\u662f\u623f\u5b50\u7684\u67d0\u79cd\u8bbe\u65bd\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\uff0c\u7f3a\u5931\u662f\u6ca1\u6709\u8be5\u8bbe\u65bd\uff0c\u662f\u6709\u610f\u4e49\u7684\u7f3a\u5931\uff0c\u56e0\u6b64\u51b3\u5b9a\u5c06\u8fd9\u51e0\u4e2a\u7279\u5f81\u6309\u7167\u662f\u5426\u7f3a\u5931\u5206\u6210\u4e24\u7c7b\u6765\u5904\u7406\nmiss_40_features = na_all[na_all['na_rate']>0.4]['feature_name'].values","ed4395ee":"#\u5bf9\u7f3a\u5931\u8d85\u8fc74\u6210\u7684\u53d8\u91cf\u8fdb\u884c\u662f\u5426\u7f3a\u5931\u7684\uff08-1,1\uff09\u7684\u7f16\u7801\ndata = Fill_reason_missing(data,miss_40_features)\n#\u6b64\u65f6\u7684Na_rate\nna_all1 = Na_rate(data)\nprint(na_all1)","b3e58deb":"#\u5269\u4e0b\u7684\u7f3a\u5931\u6570\u503c\u578b\u53d8\u91cf\nprint('\u8bad\u7ec3\u96c6\uff1a')\nmissing_data_train = train_data[na_train['feature_name'].values]\ncat_miss_train, num_miss_train = Dis_features(missing_data_train)\nprint(num_miss_train)\n\nprint('\u5168\u90e8\u6570\u636e\u96c6\uff1a')\nmissing_data = data[na_all1['feature_name'].values]\ncat_miss, num_miss = Dis_features(missing_data)#\u5f97\u5230\u6570\u503c\u578b\u7f3a\u5931\u53d8\u91cf\nprint(num_miss)","8e226fd3":"#\u5bf9\u4e8e\u4e0a\u9762\u8bad\u7ec3\u96c6\u548c\u6240\u6709\u6570\u636e\u96c6\u5171\u540c\u7f3a\u5931\u7684\u6570\u636e\uff0c\u4e09\u4e2a\u6570\u503c\u578b\u7f3a\u5931\u53d8\u91cf\uff0c\n#1.LotFrontage\u8868\u793a\u623f\u4ea7\u8ddd\u79bb\u8857\u8fb9\u7684\u8ddd\u79bb\uff0c\u6b64\u53d8\u91cf\u7528\u7b80\u5355\u586b\u5145\ndata['LotFrontage'] = SimpleImputer().fit_transform(data['LotFrontage'].values.reshape(-1,1))\n#2.GarageYrBlt\u8f66\u5e93\u4fee\u5efa\u5e74\u6570\uff0cMasVnrArea\u8d34\u7816\u9762\u79ef\uff0c\u7f3a\u5931\u662f\u56e0\u4e3a\u6ca1\u6709\u8f66\u5e93\uff0c\u6ca1\u6709\u8d34\u7816\uff0c\u4ee5\u53ca\u5176\u4ed6\u53d8\u91cf\u56e0\u4e3a\u6ca1\u6709\u8f66\u5e93\u548c\u5730\u4e0b\u5ba4\u7684\u7f3a\u5931\u9020\u6210\uff0c\u56e0\u6b64\u5355\u72ec\u7f16\u7801\u4e3a-1\nfor i in num_miss:\n    data[i] = data[i].fillna(-1)\n    \n#\u6b64\u65f6\u7684Na_rate\nna_all2 = Na_rate(data)\nprint(na_all2)\nprint('\u5168\u90e8\u6570\u636e\u96c6\uff1a')\nmissing_data = data[na_all2['feature_name'].values]\nDis_features(missing_data)#\u5f97\u5230\u7f3a\u5931\u53d8\u91cf\n","0a2ed11f":"#\u6b64\u65f6\u5c31\u5269\u8fd918\u4e2a\u7c7b\u522b\u578b\u53d8\u91cf\uff0c\u8f66\u5e93\u548c\u5730\u4e0b\u5ba4\u76f8\u5173\u7684\u53d8\u91cf\u7f3a\u5931\u90fd\u662f\u6709\u539f\u56e0\uff0cMasVnrType\u8868\u9762\u780c\u4f53\u8fd9\u4e2a\u53d8\u91cf\uff0c\u6ca1\u6709\u5bf9\u7f3a\u5931\u89e3\u91ca\uff0c\u56e0\u6b64\u7528\u4f17\u6570\u586b\u5145,\u540e\u9762\u7684\u53d8\u91cf\u90fd\u662f\u8fd9\u79cd\u60c5\u51b5\n#\u9700\u8981\u8865\u7f3a\u7684\u7c7b\u522b\u578b\u53d8\u91cf\u6709\uff1a['MasVnrType','MSZoning','Functional','Utilities','KitchenQual','Electrical','Exterior2nd','Exterior1st','SaleType']\nfill_cat = na_all2[na_all2['na_rate']<0.02]['feature_name'].values\nprint(data['MasVnrType'].value_counts())\nprint('==========')\nprint(fill_cat)\ndata = Fill_cat_miss(data,fill_cat)#\u7c7b\u522b\u578b\u53d8\u91cf\u8865\u7f3a\nna_all3 = Na_rate(data)\nprint('==========')\nprint(na_all3)","44258a3c":"#\u5bf9\u4e8e\u5269\u4e0b\u7684\u53d8\u91cf\uff0c\u56e0\u4e3a\u662f\u6709\u539f\u56e0\u7f3a\u5931\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fdb\u884cpd.factorize\u7f16\u7801\uff0c\u7f3a\u5931\u4f1a\u81ea\u52a8\u586b\u4e3a-1\n\nprint('cat\uff1a',cat_all)\nprint('='*50)\nprint('num:',num_all)\ndata = Cat_encoder(data,cat_all)#\u5bf9\u6240\u6709\u7c7b\u522b\u578b\u53d8\u91cf\u8fdb\u884c\u7f16\u7801","cd0f5611":"#\u67e5\u770b\u73b0\u5728\u5f97\u7f3a\u5931\u60c5\u51b5,\u53d1\u73b0\u6ca1\u6709\u7f3a\u5931\u4e86\nNa_rate(data)","68ea7b94":"data = Standard_num(data,num_all)","a844d771":"#\u5728\u8003\u8651\u8981\u4e0d\u8981\u628a\u53d6\u503c\u8fc7\u591a\u7684\u7c7b\u522b\u578b\u53d8\u91cf\u8fdb\u884c\u5206\u7bb1\nfor i in cat_all:\n    length = len(set(data[i].values))\n    print(i,length)","ac2bb75a":"train_x = data.loc[train_data.index]\ntest_x = data.loc[test_data.index]","1027dd8e":"from sklearn import model_selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_top_n_features(titanic_train_data_X, titanic_train_data_Y, top_n_features):\n\n    # random forest\n    rf_est = RandomForestRegressor(random_state=0)\n    rf_param_grid = {'n_estimators': [500], 'min_samples_split': [2, 3], 'max_depth': [20]}\n    rf_grid = model_selection.GridSearchCV(rf_est, rf_param_grid, n_jobs=25, cv=10, verbose=1)\n    rf_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n    print('Top N Features Best RF Params:' + str(rf_grid.best_params_))\n    print('Top N Features Best RF Score:' + str(rf_grid.best_score_))\n    print('Top N Features RF Train Score:' + str(rf_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n    feature_imp_sorted_rf = pd.DataFrame({'feature': list(titanic_train_data_X),\n                                          'importance': rf_grid.best_estimator_.feature_importances_}).sort_values('importance', ascending=False)\n    features_top_n_rf = feature_imp_sorted_rf.head(top_n_features)['feature']\n    print('Sample 10 Features from RF')\n    print(str(features_top_n_rf[:10]))\n\n    # AdaBoost\n    ada_est =AdaBoostRegressor(random_state=0)\n    ada_param_grid = {'n_estimators': [500], 'learning_rate': [0.01, 0.1]}\n    ada_grid = model_selection.GridSearchCV(ada_est, ada_param_grid, n_jobs=25, cv=10, verbose=1)\n    ada_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n    print('Top N Features Best Ada Params:' + str(ada_grid.best_params_))\n    print('Top N Features Best Ada Score:' + str(ada_grid.best_score_))\n    print('Top N Features Ada Train Score:' + str(ada_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n    feature_imp_sorted_ada = pd.DataFrame({'feature': list(titanic_train_data_X),\n                                           'importance': ada_grid.best_estimator_.feature_importances_}).sort_values('importance', ascending=False)\n    features_top_n_ada = feature_imp_sorted_ada.head(top_n_features)['feature']\n    print('Sample 10 Feature from Ada:')\n    print(str(features_top_n_ada[:10]))\n\n    # ExtraTree\n    et_est = ExtraTreesRegressor(random_state=0)\n    et_param_grid = {'n_estimators': [500], 'min_samples_split': [3, 4], 'max_depth': [20]}\n    et_grid = model_selection.GridSearchCV(et_est, et_param_grid, n_jobs=25, cv=10, verbose=1)\n    et_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n    print('Top N Features Best ET Params:' + str(et_grid.best_params_))\n    print('Top N Features Best ET Score:' + str(et_grid.best_score_))\n    print('Top N Features ET Train Score:' + str(et_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n    feature_imp_sorted_et = pd.DataFrame({'feature': list(titanic_train_data_X),\n                                          'importance': et_grid.best_estimator_.feature_importances_}).sort_values('importance', ascending=False)\n    features_top_n_et = feature_imp_sorted_et.head(top_n_features)['feature']\n    print('Sample 10 Features from ET:')\n    print(str(features_top_n_et[:10]))\n    \n    # GradientBoosting\n    gb_est =GradientBoostingRegressor(random_state=0)\n    gb_param_grid = {'n_estimators': [500], 'learning_rate': [0.01, 0.1], 'max_depth': [20]}\n    gb_grid = model_selection.GridSearchCV(gb_est, gb_param_grid, n_jobs=25, cv=10, verbose=1)\n    gb_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n    print('Top N Features Best GB Params:' + str(gb_grid.best_params_))\n    print('Top N Features Best GB Score:' + str(gb_grid.best_score_))\n    print('Top N Features GB Train Score:' + str(gb_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n    feature_imp_sorted_gb = pd.DataFrame({'feature': list(titanic_train_data_X),\n                                           'importance': gb_grid.best_estimator_.feature_importances_}).sort_values('importance', ascending=False)\n    features_top_n_gb = feature_imp_sorted_gb.head(top_n_features)['feature']\n    print('Sample 10 Feature from GB:')\n    print(str(features_top_n_gb[:10]))\n    \n    # DecisionTree\n    dt_est = DecisionTreeRegressor(random_state=0)\n    dt_param_grid = {'min_samples_split': [2, 4], 'max_depth': [20]}\n    dt_grid = model_selection.GridSearchCV(dt_est, dt_param_grid, n_jobs=25, cv=10, verbose=1)\n    dt_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n    print('Top N Features Best DT Params:' + str(dt_grid.best_params_))\n    print('Top N Features Best DT Score:' + str(dt_grid.best_score_))\n    print('Top N Features DT Train Score:' + str(dt_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n    feature_imp_sorted_dt = pd.DataFrame({'feature': list(titanic_train_data_X),\n                                          'importance': dt_grid.best_estimator_.feature_importances_}).sort_values('importance', ascending=False)\n    features_top_n_dt = feature_imp_sorted_dt.head(top_n_features)['feature']\n    print('Sample 10 Features from DT:')\n    print(str(features_top_n_dt[:10]))\n    \n    # merge the three models\n    features_top_n = pd.concat([features_top_n_rf, features_top_n_ada, features_top_n_et, features_top_n_gb, features_top_n_dt], \n                               ignore_index=True).drop_duplicates()\n    \n    features_importance = pd.concat([feature_imp_sorted_rf, feature_imp_sorted_ada, feature_imp_sorted_et, \n                                   feature_imp_sorted_gb, feature_imp_sorted_dt],ignore_index=True)\n    \n    return features_top_n , features_importance\n","2d5a7008":"feature_to_pick = 30\nfeature_top_n, feature_importance = get_top_n_features(train_x, log_train_y, feature_to_pick)\ntrain_x = pd.DataFrame(train_x[feature_top_n])\ntest_x = pd.DataFrame(test_x[feature_top_n])\n\n# rf.fit(train_x,log_train_y)\n# log_y_hat = rf.predict(test_x)\n# # rf.fit(train_x,train_y)\n# # y_hat = rf.predict(test_x)\n","b2ffbadc":"rf_feature_imp = feature_importance[:10]\nAda_feature_imp = feature_importance[32:32+10].reset_index(drop=True)\n\n# make importances relative to max importance\nrf_feature_importance = 100.0 * (rf_feature_imp['importance'] \/ rf_feature_imp['importance'].max())\nAda_feature_importance = 100.0 * (Ada_feature_imp['importance'] \/ Ada_feature_imp['importance'].max())\n\n# Get the indexes of all features over the importance threshold\nrf_important_idx = np.where(rf_feature_importance)[0]\nAda_important_idx = np.where(Ada_feature_importance)[0]\n\n# Adapted from http:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regression.html\npos = np.arange(rf_important_idx.shape[0]) + .5\n\nplt.figure(1, figsize = (18, 8))\n\nplt.subplot(121)\nplt.barh(pos, rf_feature_importance[rf_important_idx][::-1])\nplt.yticks(pos, rf_feature_imp['feature'][::-1])\nplt.xlabel('Relative Importance')\nplt.title('RandomForest Feature Importance')\n\nplt.subplot(122)\nplt.barh(pos, Ada_feature_importance[Ada_important_idx][::-1])\nplt.yticks(pos, Ada_feature_imp['feature'][::-1])\nplt.xlabel('Relative Importance')\nplt.title('AdaBoost Feature Importance')\n\nplt.show()","2ce70672":"from sklearn.model_selection import KFold\n\n# Some useful parameters which will come in handy later on\nntrain = train_x.shape[0]\nntest = test_x.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 7 # set folds for out-of-fold prediction\nkf = KFold(n_splits = NFOLDS, random_state=SEED, shuffle=False)\n\ndef get_out_fold(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.fit(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","149f18a9":"\nfrom sklearn.svm import SVR\n\nrf = RandomForestRegressor(n_estimators=500, warm_start=True, max_features='sqrt',max_depth=6, \n                            min_samples_split=3, min_samples_leaf=2, n_jobs=-1, verbose=0)\n\nada = AdaBoostRegressor(n_estimators=500, learning_rate=0.1)\n\net = ExtraTreesRegressor(n_estimators=500, n_jobs=-1, max_depth=8, min_samples_leaf=2, verbose=0)\n\ngb = GradientBoostingRegressor(n_estimators=500, learning_rate=0.008, min_samples_split=3, min_samples_leaf=2, max_depth=5, verbose=0)\n\ndt = DecisionTreeRegressor(max_depth=8)\n\nsvm = SVR(kernel='linear', C=0.025)","2af3fb99":"x_train = train_x.values # Creates an array of the train data\nx_test = test_x.values # Creats an array of the test data\ny_train = log_train_y.values","9c355a36":"rf_oof_train, rf_oof_test = get_out_fold(rf, x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_out_fold(ada, x_train, y_train, x_test) # AdaBoost \net_oof_train, et_oof_test = get_out_fold(et, x_train, y_train, x_test) # Extra Trees\ngb_oof_train, gb_oof_test = get_out_fold(gb, x_train, y_train, x_test) # Gradient Boost\ndt_oof_train, dt_oof_test = get_out_fold(dt, x_train, y_train, x_test) # Decision Tree\nsvm_oof_train, svm_oof_test = get_out_fold(svm, x_train, y_train, x_test) # Support Vector\n\nprint(\"Training is complete\")","2aa1114a":"x_train = np.concatenate((rf_oof_train, ada_oof_train, et_oof_train, gb_oof_train, dt_oof_train, svm_oof_train), axis=1)\nx_test = np.concatenate((rf_oof_test, ada_oof_test, et_oof_test, gb_oof_test, dt_oof_test, svm_oof_test), axis=1)","9ba5080e":"from xgboost import XGBRegressor\n\ngbm = XGBRegressor( n_estimators= 2000, max_depth= 4, min_child_weight= 2, gamma=0.9, subsample=0.8, \n                        colsample_bytree=0.8, nthread= -1, scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n","bcaf63d1":"y_hat = np.expm1(predictions)\nsubmission = test_sub.copy()\nsubmission['SalePrice'] = y_hat\nsubmission.to_csv('submission.csv')","11aab228":"submission","a22683a6":"# y_hat = np.expm1(log_y_hat)","4c92fe58":"# test_sub['SalePrice']\n","fffe18fb":"# submission = test_sub.copy()\n# submission['SalePrice'] = y_hat\n# submission.to_csv('submission.csv')\n","6133898e":"1.\u76ee\u6807\u53d8\u91cf\u7684\u63a2\u7d22","5e6ed118":"2.\u8f93\u51fa\u7279\u5f81\u91cd\u8981\u6027","f9fd2553":"LEVEL1:","577c62d0":"4.\u6570\u503c\u578b\u53d8\u91cf\u7684\u5f52\u4e00\u5316","f8e64b40":"\u51fd\u6570\u96c6\u5408","5bfe7054":"### \u4e8c\u3001\u6570\u636e\u6e05\u6d17","3ad98916":"# \u4e00\u3001\u76ee\u6807\u53d8\u91cf","3cd9ee41":"3.tacking\u6846\u67b6\u878d\u5408:\n\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u5c42\u7684\u6a21\u578b\u878d\u5408\uff0cLevel 1\u4f7f\u7528\u4e86\uff1aRandomForest\u3001AdaBoost\u3001ExtraTrees\u3001GBDT\u3001DecisionTree\u3001SVM \uff0c\u4e00\u51716\u4e2a\u6a21\u578b\uff0cLevel 2\u4f7f\u7528\u4e86XGBoost\u4f7f\u7528\u7b2c\u4e00\u5c42\u9884\u6d4b\u7684\u7ed3\u679c\u4f5c\u4e3a\u7279\u5f81\u5bf9\u6700\u7ec8\u7684\u7ed3\u679c\u8fdb\u884c\u9884\u6d4b\u3002","0190a729":"1.\u8bad\u7ec3\u96c6\u7279\u5f81\u7684\u7f3a\u5931\u60c5\u51b5","e5af1450":"1.\u786e\u5b9a\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6\u4ee5\u53ca\u5bf9\u5e94\u7684x\uff0cy","99b8c621":"2.\u7f3a\u5931\u503c\u7684\u5904\u7406","caec481a":"3.\u7c7b\u522b\u578b\u53d8\u91cf\u7684\u7f16\u7801","20da2207":"# \u4e09\u3001\u7279\u5f81\u6311\u9009","50764ea6":"# \u56db\u3001\u5efa\u7acb\u6a21\u578b","e48295ea":"LEVEL2:"}}