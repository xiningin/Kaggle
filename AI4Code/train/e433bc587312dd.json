{"cell_type":{"4ac359ea":"code","1809e815":"code","e325fd2f":"code","dedb65cc":"code","864e39fd":"code","80c7cc8c":"code","a827d033":"code","140a13a5":"code","c194a148":"code","90301c2d":"code","ef489e63":"code","e6c5613a":"code","d4f3a56c":"code","fb5e4346":"code","5ed3504d":"code","66cdb37f":"code","4c81163e":"code","66abe51b":"code","3acc979b":"code","e225b0b8":"code","76facc61":"code","eadee0df":"code","d53ecae8":"markdown","c39d1a93":"markdown","0a26a864":"markdown","0dda1763":"markdown","e28e6eab":"markdown","4b7e5f96":"markdown"},"source":{"4ac359ea":"import numpy as np \nimport pandas as pd \nimport json\nimport bq_helper\nfrom pandas.io.json import json_normalize\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nfrom sklearn.metrics import mean_squared_error\nimport time","1809e815":"# https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\n\ndef load_df(csv_path='..\/input\/train.csv', JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']):\n\n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'})\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n\n    return df","e325fd2f":"%%time\ntrain = load_df(\"..\/input\/train.csv\")","dedb65cc":"%%time\ntest = load_df(\"..\/input\/test.csv\")","864e39fd":"cols_to_drop = [col for col in train.columns if train[col].nunique(dropna=False) == 1]\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop([col for col in cols_to_drop if col in test.columns], axis=1, inplace=True)\n\n#only one not null value\ntrain.drop(['trafficSource.campaignCode'], axis=1, inplace=True)\n\nprint(f'Dropped {len(cols_to_drop)} columns.')","80c7cc8c":"train['totals.transactionRevenue'] = train['totals.transactionRevenue'].fillna(0).astype(int)\ntrain['totals.transactionRevenue'] = np.log1p(train['totals.transactionRevenue'])","a827d033":"def process_df(df):\n    \"\"\"Process df and create new features.\"\"\"\n    \n    for col in ['visitNumber', 'totals.hits', 'totals.pageviews']:\n        df[col] = df[col].astype(float)\n    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\n    df['trafficSource.isTrueDirect'].fillna(False, inplace=True)\n    \n    df['date'] = pd.to_datetime(df['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))\n    \n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear\n\n    df['month_unique_user_count'] = df.groupby('month')['fullVisitorId'].transform('nunique')\n    df['day_unique_user_count'] = df.groupby('day')['fullVisitorId'].transform('nunique')\n    df['weekday_unique_user_count'] = df.groupby('weekday')['fullVisitorId'].transform('nunique')\n    df['weekofyear_unique_user_count'] = df.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n    \n    df['browser_category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n    df['browser_operatingSystem'] = df['device.browser'] + '_' + df['device.operatingSystem']\n    df['source_country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n    \n    df['visitNumber'] = np.log1p(df['visitNumber'])\n    df['totals.hits'] = np.log1p(df['totals.hits'])\n    df['totals.pageviews'] = np.log1p(df['totals.pageviews'].fillna(0))\n\n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\n    df['mean_hits_per_day'] = df.groupby(['day'])['totals.hits'].transform('mean')\n    df['sum_hits_per_day'] = df.groupby(['day'])['totals.hits'].transform('sum')\n    \n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n\n    df['sum_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('count')\n    df['mean_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('mean')\n    \n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\n    df['sum_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('sum')\n    df['count_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('count')\n    df['mean_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('mean')\n\n    df['sum_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('sum')\n    df['count_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('count')\n    df['mean_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('mean')\n\n    df['user_pageviews_sum'] = df.groupby('fullVisitorId')['totals.pageviews'].transform('sum')\n    df['user_hits_sum'] = df.groupby('fullVisitorId')['totals.hits'].transform('sum')\n\n    df['user_pageviews_count'] = df.groupby('fullVisitorId')['totals.pageviews'].transform('count')\n    df['user_hits_count'] = df.groupby('fullVisitorId')['totals.hits'].transform('count')\n\n    df['user_pageviews_sum_to_mean'] = df['user_pageviews_sum'] \/ df['user_pageviews_sum'].mean()\n    df['user_hits_sum_to_mean'] = df['user_hits_sum'] \/ df['user_hits_sum'].mean()\n    df['user_pageviews_sum_to_mean'] = df['user_pageviews_sum'] \/ df['user_pageviews_sum'].mean()\n    df['user_hits_sum_to_mean'] = df['user_hits_sum'] \/ df['user_hits_sum'].mean()\n    \n    df['user_pageviews_to_region'] = df['user_pageviews_sum'] \/ df['mean_pageviews_per_region']\n    df['user_hits_to_region'] = df['user_hits_sum'] \/ df['mean_hits_per_region']\n    \n    return df","140a13a5":"%%time\ntrain = process_df(train)","c194a148":"%%time\ntest = process_df(test)","90301c2d":"not_num_cols = ['visitId', 'totals.transactionRevenue', 'month', 'day', 'weekday', 'weekofyear']\nnum_cols = [col for col in train.columns if train[col].dtype in ['float64', 'int64'] and col not in not_num_cols]\n\nnot_cat_cols = ['fullVisitorId', 'sessionId', 'trafficSource.referralPath']\ncat_cols = [col for col in train.columns if train[col].dtype == 'object' and col not in not_cat_cols] + ['month', 'day', 'weekday', 'weekofyear']\n\nno_use = ['visitStartTime', \"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", 'totals.transactionRevenue', 'trafficSource.referralPath']","ef489e63":"def generate_more_features(df):\n    for col in num_cols:\n        df[col + '_root'] = df[col] ** 0.5\n        \n    return df","e6c5613a":"%%time\ntrain = generate_more_features(train)\ntest = generate_more_features(test)","d4f3a56c":"def generate_more_features(df, features_slice=[]):\n    \"\"\"\n    Generate more features by multiplying all numerical columns by each other.\n    But can't do it for all columns due to memory limitations\n    \"\"\"\n    for col1 in num_cols[features_slice[0] : features_slice[1]]:\n        for col2 in num_cols:\n            if col1 != col2:\n                # print(col1, col2)\n                df[col1 + '_' + col2] = df[col1] * df[col2]\n        \n    return df","fb5e4346":"%%time\ntrain = generate_more_features(train, [0, 3])\ntest = generate_more_features(test, [0, 3])","5ed3504d":"for col in cat_cols:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","66cdb37f":"train = train.sort_values('date')\nX = train.drop(no_use, axis=1)\ny = train['totals.transactionRevenue']\nX_test = test.drop([col for col in no_use if col in test.columns], axis=1)","4c81163e":"params = {\"objective\" : \"regression\",\n          \"metric\" : \"rmse\", \n          #\"max_depth\": 6,\n          \"min_child_samples\": 20, \n          \"reg_alpha\": 0.033948965191129526, \n          \"reg_lambda\": 0.06490202783578762,\n          \"num_leaves\" : 34,\n          \"learning_rate\" : 0.019732018807662323,\n          \"subsample\" : 0.876,\n          \"colsample_bytree\" : 0.85,\n          \"subsample_freq \": 5,\n          #'min_split_gain': 0.024728814179385473,\n          #'min_child_weight': 39.40511524645848\n         }\n\nn_fold = 5\nfolds = KFold(n_splits=n_fold, random_state=42)\n# Cleaning and defining parameters for LGBM\nmodel = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)","66abe51b":"oof = np.zeros(len(train))\noof_1 = np.zeros(len(train))\nprediction = np.zeros(len(test))\nscores = []\nscores_1 = []\nfeature_importance = pd.DataFrame()\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n            verbose=500, early_stopping_rounds=100)\n    \n    y_pred_valid = model.predict(X_valid)\n    oof[valid_index] = y_pred_valid.reshape(-1,)\n    scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n    \n    y_val = train.iloc[valid_index][['fullVisitorId', 'totals.transactionRevenue']]\n    y_val[\"totals.transactionRevenue\"] = y_val[\"totals.transactionRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\n    y_val[\"totals.transactionRevenue\"] = y_val[\"totals.transactionRevenue\"].fillna(0.0)\n    y_val['totals.transactionRevenue'] = np.expm1(y_val['totals.transactionRevenue'])\n    y_val_sum_true = y_val.groupby('fullVisitorId').sum().reset_index()['totals.transactionRevenue']\n    y_val_sum_true = np.log1p(y_val_sum_true)\n    \n    oof_1[valid_index] = np.expm1(oof[valid_index])\n    \n    val_df = train.iloc[valid_index][['fullVisitorId']]\n    val_df['totals.transactionRevenue'] = oof[valid_index]\n    val_df[\"totals.transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\n    val_df[\"totals.transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].fillna(0.0)\n    y_val_sum_pred = val_df.groupby('fullVisitorId').sum().reset_index()['totals.transactionRevenue']\n    y_val_sum_pred = np.log1p(y_val_sum_pred)\n    scores_1.append(mean_squared_error(y_val_sum_true, y_val_sum_pred) ** 0.5)\n    \n       \n    y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    prediction += y_pred\n    \n    # feature importance\n    fold_importance = pd.DataFrame()\n    fold_importance[\"feature\"] = X.columns\n    fold_importance[\"importance\"] = model.feature_importances_\n    fold_importance[\"fold\"] = fold_n + 1\n    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n    \n    print('')\n    print(f'Fold {fold_n}. RMSE: {scores[-1]:.4f}.')\n    print('')\n\nprediction \/= n_fold\nfeature_importance[\"importance\"] \/= n_fold","3acc979b":"print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\nprint('CV new mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores_1), np.std(scores_1)))","e225b0b8":"#lgb.plot_importance(model, max_num_features=30);\n#feature_importance_lgb.sort_values('importance', ascending=False).set_index('features').plot('')\ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('LGB Features (avg over folds)');","76facc61":"submission = test[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = prediction\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].fillna(0.0)\ngrouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test.to_csv(f'lgb_cv{np.mean(scores):.4f}_std_{np.std(scores):.4f}_prediction_old.csv', index=False)\noof_df = pd.DataFrame({\"fullVisitorId\": train[\"fullVisitorId\"], \"PredictedLogRevenue\": oof})\noof_df.to_csv(f'lgb_cv{np.mean(scores):.4f}_std_{np.std(scores):.4f}_oof_old.csv', index=False)","eadee0df":"submission = test[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = np.expm1(prediction)\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].fillna(0.0)\ngrouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test[\"PredictedLogRevenue\"] = np.log1p(grouped_test[\"PredictedLogRevenue\"])\ngrouped_test.to_csv(f'lgb_cv{np.mean(scores_1):.4f}_std_{np.std(scores_1):.4f}_prediction_new.csv', index=False)\noof_df = pd.DataFrame({\"fullVisitorId\": train[\"fullVisitorId\"], \"PredictedLogRevenue\": oof_1})\noof_df.to_csv(f'lgb_cv{np.mean(scores_1):.4f}_std_{np.std(scores_1):.4f}_oof_new.csv', index=False)","d53ecae8":"Some of columns aren't available in this dataset, let's drop them.","c39d1a93":"### Feature processing","0a26a864":"    In fact it seems that it will take some time to find a good validation - TimeSeriesSplit gives a high variance in scores, so I'll try kfold for now.","0dda1763":"## Data processing","e28e6eab":"### More features\n\nFor now only several features are calculated.","4b7e5f96":"## General information\n\nThis kernel continues my [previous one](https:\/\/www.kaggle.com\/artgor\/fork-of-eda-on-basic-data-and-lgb-in-progress) - you can see EDA and other things there.\n\nThis kernel is dedicated to feature generation. I'll generate features step by step and try to increase CV."}}