{"cell_type":{"d2b18ba3":"code","9b36fd4b":"code","741fcfda":"code","044d9002":"code","36b7d340":"code","67b8f209":"code","8edaafdd":"code","849d5243":"code","37518ac4":"code","5958ae51":"code","164f5da6":"code","6412f258":"code","f2f76998":"code","9d8fd972":"code","aba7651d":"code","1e7b9231":"code","e529d959":"code","d813dc4c":"code","2f2f93ab":"code","fd4e3c79":"code","32dd86bf":"code","24c29ec5":"code","204bf043":"code","aa9d5f39":"code","8b67b5ce":"code","f4b8b2b8":"code","36f009a2":"code","1f28966a":"code","1e51981c":"code","be1cbb56":"code","ca940567":"markdown","033c60eb":"markdown","d253ff4f":"markdown","8450428c":"markdown","b3261cf3":"markdown","c965609d":"markdown","a6d60902":"markdown","e769ba04":"markdown","5a2b18d7":"markdown","2e31a491":"markdown","cb440d19":"markdown","ea2ff3d1":"markdown","6a1f20bd":"markdown","b097d16b":"markdown","ba027fd3":"markdown","dc031c26":"markdown","6a828078":"markdown","cd79b14b":"markdown","3972080a":"markdown","a117ba8c":"markdown","f354409f":"markdown","ed731b25":"markdown","850299a7":"markdown","eb24705b":"markdown","e074fdb4":"markdown"},"source":{"d2b18ba3":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\n\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport seaborn as sb\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n\n%matplotlib inline","9b36fd4b":"datafile = '..\/input\/abcnews-date-text.csv'\nraw_data = pd.read_csv(datafile, parse_dates=[0], infer_datetime_format=True)\n\nreindexed_data = raw_data['headline_text']\nreindexed_data.index = raw_data['publish_date']\n\nraw_data.head()","741fcfda":"# Define helper functions\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","044d9002":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=15,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=reindexed_data)\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in headlines dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","36b7d340":"tagged_headlines = [TextBlob(reindexed_data[i]).pos_tags for i in range(reindexed_data.shape[0])]","67b8f209":"tagged_headlines_df = pd.DataFrame({'tags':tagged_headlines})\n\nword_counts = [] \npos_counts = {}\n\nfor headline in tagged_headlines_df[u'tags']:\n    word_counts.append(len(headline))\n    for tag in headline:\n        if tag[1] in pos_counts:\n            pos_counts[tag[1]] += 1\n        else:\n            pos_counts[tag[1]] = 1\n            \nprint('Total number of words: ', np.sum(word_counts))\nprint('Mean number of words per headline: ', np.mean(word_counts))","8edaafdd":"y = stats.norm.pdf(np.linspace(0,14,50), np.mean(word_counts), np.std(word_counts))\n\nfig, ax = plt.subplots(figsize=(18,8))\nax.hist(word_counts, bins=range(1,14), density=True);\nax.plot(np.linspace(0,14,50), y, 'r--', linewidth=1);\nax.set_title('Headline word lengths');\nax.set_xticks(range(1,14));\nax.set_xlabel('Number of words');\nplt.show()","849d5243":"pos_sorted_types = sorted(pos_counts, key=pos_counts.__getitem__, reverse=True)\npos_sorted_counts = sorted(pos_counts.values(), reverse=True)\n\nfig, ax = plt.subplots(figsize=(18,8))\nax.bar(range(len(pos_counts)), pos_sorted_counts);\nax.set_xticks(range(len(pos_counts)));\nax.set_xticklabels(pos_sorted_types);\nax.set_title('Part-of-Speech Tagging for Headlines Corpus');\nax.set_xlabel('Type of Word');","37518ac4":"monthly_counts = reindexed_data.resample('M').count()\nyearly_counts = reindexed_data.resample('A').count()\ndaily_counts = reindexed_data.resample('D').count()\n\nfig, ax = plt.subplots(3, figsize=(18,16))\nax[0].plot(daily_counts);\nax[0].set_title('Daily Counts');\nax[1].plot(monthly_counts);\nax[1].set_title('Monthly Counts');\nax[2].plot(yearly_counts);\nax[2].set_title('Yearly Counts');\nplt.show()","5958ae51":"small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\nsmall_text_sample = reindexed_data.sample(n=10000, random_state=0).values\n\nprint('Headline before vectorization: {}'.format(small_text_sample[123]))\n\nsmall_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n\nprint('Headline after vectorization: \\n{}'.format(small_document_term_matrix[123]))","164f5da6":"n_topics = 8","6412f258":"lsa_model = TruncatedSVD(n_components=n_topics)\nlsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)","f2f76998":"# Define helper functions\ndef get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)","9d8fd972":"lsa_keys = get_keys(lsa_topic_matrix)\nlsa_categories, lsa_counts = keys_to_counts(lsa_keys)","aba7651d":"# Define helper functions\ndef get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n    '''\n    returns a list of n_topic strings, where each string contains the n most common \n    words in a predicted category, in order\n    '''\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","1e7b9231":"top_n_words_lsa = get_top_n_words(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lsa)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])","e529d959":"top_3_words = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lsa_categories, lsa_counts);\nax.set_xticks(lsa_categories);\nax.set_xticklabels(labels);\nax.set_ylabel('Number of headlines');\nax.set_title('LSA topic counts');\nplt.show()","d813dc4c":"tsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_matrix)","2f2f93ab":"# Define helper functions\ndef get_mean_topic_vectors(keys, two_dim_vectors):\n    '''\n    returns a list of centroid vectors from each predicted topic category\n    '''\n    mean_topic_vectors = []\n    for t in range(n_topics):\n        articles_in_that_topic = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles_in_that_topic.append(two_dim_vectors[i])    \n        \n        articles_in_that_topic = np.vstack(articles_in_that_topic)\n        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n        mean_topic_vectors.append(mean_article_in_that_topic)\n    return mean_topic_vectors","fd4e3c79":"colormap = np.array([\n    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\ncolormap = colormap[:n_topics]","32dd86bf":"top_3_words_lsa = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\nlsa_mean_topic_vectors = get_mean_topic_vectors(lsa_keys, tsne_lsa_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LSA Topics\".format(n_topics), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lsa_vectors[:,0], y=tsne_lsa_vectors[:,1], color=colormap[lsa_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lsa_mean_topic_vectors[t][0], y=lsa_mean_topic_vectors[t][1], \n                  text=top_3_words_lsa[t], text_color=colormap[t])\n    plot.add_layout(label)\n    \nshow(plot)","24c29ec5":"lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n                                          random_state=0, verbose=0)\nlda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)","204bf043":"lda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)","aa9d5f39":"top_n_words_lda = get_top_n_words(10, lda_keys, small_document_term_matrix, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lda)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])","8b67b5ce":"top_3_words = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lda_categories, lda_counts);\nax.set_xticks(lda_categories);\nax.set_xticklabels(labels);\nax.set_title('LDA topic counts');\nax.set_ylabel('Number of headlines');","f4b8b2b8":"tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)","36f009a2":"top_3_words_lda = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)","1f28966a":"big_sample_size = 100000 \n\nbig_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\nbig_text_sample = reindexed_data.sample(n=big_sample_size, random_state=0).values\nbig_document_term_matrix = big_count_vectorizer.fit_transform(big_text_sample)\n\nbig_lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online')\nbig_lda_model.fit(big_document_term_matrix);","1e51981c":"yearly_data = []\nfor i in range(2003,2017+1):\n    yearly_data.append(reindexed_data['{}'.format(i)].values)\n\nyearly_topic_matrices = []\nfor year in yearly_data:\n    document_term_matrix = big_count_vectorizer.transform(year)\n    topic_matrix = big_lda_model.transform(document_term_matrix)\n    yearly_topic_matrices.append(topic_matrix)\n\nyearly_keys = []\nfor topic_matrix in yearly_topic_matrices:\n    yearly_keys.append(get_keys(topic_matrix))\n    \nyearly_counts = []\nfor keys in yearly_keys:\n    categories, counts = keys_to_counts(keys)\n    yearly_counts.append(counts)\n\nyearly_topic_counts = pd.DataFrame(np.array(yearly_counts), index=range(2003,2017+1))\nyearly_topic_counts.columns = ['Topic {}'.format(i+1) for i in range(n_topics)]\n\nprint(yearly_topic_counts)","be1cbb56":"fig, ax = plt.subplots(figsize=(14,10))\nsb.heatmap(yearly_topic_counts, cmap=\"YlGnBu\", ax=ax);\nplt.show()","ca940567":"However, this does not provide a great point of comparison with other clustering algorithms. In order to properly contrast LSA with LDA we instead use a dimensionality-reduction technique called $t$-SNE, which will also serve to better illuminate the success of the clustering process.","033c60eb":"Thus we have our (very high-rank and sparse) training data,  ```small_document_term_matrix```, and can now actually implement a clustering algorithm. Our choice will be either Latent Semantic Analysis or Latent Dirichilet Allocation. Both will take our document-term matrix as input and yield an $n \\times N$ topic matrix as output, where $N$ is the number of topic categories (which we supply as a parameter). For the moment, we shall take this to be 8.","d253ff4f":"First we develop a list of the top words used across all one million headlines, giving us a glimpse into the core vocabulary of the source data. Stop words are omitted here to avoid any trivial conjunctions, prepositions, etc.","8450428c":"Now that we have reduced these ```n_topics```-dimensional vectors to two-dimensional representations, we can then plot the clusters using Bokeh. Before doing so however, it will be useful to derive the centroid location of each topic, so as to better contextualise our visualisation.","b3261cf3":"Taking the $\\arg \\max$ of each headline in this topic matrix will give the predicted topics of each headline in the sample. We can then sort these into counts of each topic.","c965609d":"By plotting the number of headlines published per day, per month and per year, we can also get a sense of the sample density.","a6d60902":"This is a much better result! Controlling for $t$-SNE, it would seem that LDA has had much more succcess than LSA in separating out the topic categories. For this reason, LDA appears the more appropriate algorithm when we scale up the clustering process in the next section.","e769ba04":"We thus have our big topic model. The next step is to pass the entire dataset of one million headlines through this model, and sort by years. This will allow us to develop a table of topic counts per year.","5a2b18d7":"### Latent Semantic Analysis\nLet's start by experimenting with LSA. This is effectively just a truncated singular value decomposition of a (very high-rank and sparse) document-term matrix, with only the $r=$```n_topics``` largest singular values preserved.","2e31a491":"Next we generate a histogram of headline word lengths, and use part-of-speech tagging to understand the types of words used across the corpus. This requires first converting all headline strings to TextBlobs and calling the ```pos_tags``` method on each, yielding a list of tagged words for each headline. A complete list of such word tags is available [here](https:\/\/www.clips.uantwerpen.be\/pages\/MBSP-tags).","cb440d19":"The relative topic compositions of the sample are then illustated with a barchart.","ea2ff3d1":"Evidently, this is a bit a of a failed result. We have failed to reach any great degree of separation across the topic categories, and it is difficult to tell whether this can be attributed to the LSA decomposition or instead the $t$-SNE dimensionality reduction process. Let's move forward and try another clustering technique.","6a1f20bd":"Once again, we take the $\\arg \\max$ of each entry in the topic matrix to obtain the predicted topic category for each headline. These topic categories can then be characterised by their most frequent words.\n","b097d16b":"### Latent Dirichilet Allocation\nWe now repeat this process using LDA instead of LSA. LDA is instead a generative probabilistic process, designed with the specific goal of uncovering latent topic structure in text corpora.","ba027fd3":"Several distinct trends are apparent here, though we can only hypothesise at their cause.","dc031c26":"However, these topic categories are in and of themselves a little meaningless. In order to better characterise them, it will be helpful to find the most frequent words in each.","6a828078":"# Topic Modelling with LSA and LDA\nIn this kernel, two topic modelling algorithms are explored: LSA and LDA. These techniques are applied to the 'A Million News Headlines' dataset, which is a corpus of over one million news article headlines published by the ABC. ","cd79b14b":"However, in order to properly compare LDA with LSA, we again take this topic matrix and project it into two dimensions with $t$-SNE.\n","3972080a":"## Exploratory Data Analysis\nAs usual, it is prudent to begin with some basic exploratory analysis.","a117ba8c":"Thus we have converted our initial small sample of headlines into a list of predicted topic categories, where each category is characterised by its most frequent words. The relative magnitudes of each of these categories can then be easily visualised though use of a bar chart.","f354409f":"## Topic Modelling\nWe now apply a clustering algorithm to the headlines corpus in order to study the topic focus of ABC News, as well as how it has evolved through time. To do so, we first experiment with a small subsample of the dataset in order to determine which of the two potential clustering algorithms is most appropriate \u2013 once this has been ascertained, we then scale up to a larger portion of the available data.","ed731b25":"We now have a dataframe of the yearly counts of each of the $N$ topic categories, and can best visualise their evolution across time through use of a heatmap.","850299a7":"All that remains is to plot the clustered headlines. Also included are the top three words in each cluster, which are placed at the centroid for that topic.","eb24705b":"### Preprocessing\nThe only preprocessing step required in our case is feature construction, where we take the sample of text headlines and represent them in some tractable feature space. In practice, this simply means converting each string to a numerical vector. This can be done using the ```CountVectorizer``` object from SKLearn, which yields an $n\u00d7K$ document-term matrix where $K$ is the number of distinct words  across the $n$ headlines in our sample (less stop words and with a limit of ```max_features```).","e074fdb4":"### Scaling Up\nNow that we have found an effective clustering algorithm, we can scale it up. We still have to work with a subsample, as it is infeasible to train an LDA model one million observations, but we can significantly increase our scope to $100,000$ headlines.\n"}}