{"cell_type":{"bbde9730":"code","4c284429":"code","8a97fd97":"code","359cdfdb":"code","e9e95ed1":"code","cb2a4c54":"code","df565218":"code","68d9e057":"code","75a25ab8":"code","e965f6de":"code","c5ffb927":"code","8be93ed7":"code","df378694":"code","85ca9b53":"code","8cc90a63":"code","11670a7b":"code","ac39deee":"code","0e1738d8":"code","289a6fe1":"code","30a78323":"code","fb718d64":"code","f2f19f7b":"code","94ad9098":"code","7d547d28":"code","760ed24b":"code","575ea959":"code","d8a37abc":"code","4d47d8b3":"code","a29866e9":"code","6d547021":"code","f818e694":"code","bf50d144":"code","5007e319":"code","25cb7791":"code","f3892324":"code","47da565e":"code","cd259b30":"code","b4d87b3b":"code","d23f5bfe":"code","8356b05e":"code","743ec15d":"code","1a9e1a46":"markdown","cb9456f8":"markdown","80bdc36a":"markdown","c55ba20f":"markdown","e15934a4":"markdown","31c4589d":"markdown","d0a0dfff":"markdown","077c027f":"markdown","58b7900b":"markdown","7e5ef981":"markdown","614ee7a2":"markdown","5b093236":"markdown","ed7eaab4":"markdown","8c818b7e":"markdown","827cdab3":"markdown","4d776225":"markdown"},"source":{"bbde9730":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c284429":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB,CategoricalNB\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn import svm\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import accuracy_score\nfrom time import time\nfrom sklearn.model_selection import StratifiedKFold","8a97fd97":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","359cdfdb":"print(train.shape,test.shape)","e9e95ed1":"train.head(3)","cb2a4c54":"#Check if ther'is null values\ntrain.isnull().sum()","df565218":"#Remove redundant samples\ntrain=train.drop_duplicates(subset=['text', 'target'], keep='first')\ntrain.shape","68d9e057":"train.target.value_counts()","75a25ab8":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\ntrain.groupby('target').id.count().plot.bar(ylim=0)\nplt.show()","e965f6de":"# Numbers of word for each sapmle in train & test data\ntrain['text_length'] = train.text.apply(lambda x: len(x.split()))\ntest['text_length'] = test.text.apply(lambda x: len(x.split()))","c5ffb927":"train['text_length'].describe()","8be93ed7":"test['text_length'].describe()","df378694":"def plot_word_count(df, data_name):\n  sns.distplot(df['text_length'].values)\n  plt.title(f'Sequence char count: {data_name}')\n  plt.grid(True)","85ca9b53":"#ig = plt.figure(figsize=(16,6))\n#plt.hist(train[\"text_length\"], bins = 30)\n#plt.show()\nplt.subplot(1, 2, 1)\nplot_word_count(train, 'Train')\n\nplt.subplot(1, 2, 2)\nplot_word_count(test, 'Test')\n\nplt.subplots_adjust(right=3.0)\nplt.show()","8cc90a63":"# collecting all words in single list\nlist_= []\nfor i in train.text:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)","11670a7b":"len(vocabulary)","ac39deee":"def create_corpus(df,target):\n    corpus=[]\n    \n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","0e1738d8":"#most frequent 20 words when label == 0 \nimport collections\nallWords=create_corpus(train,target=0)\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(16,5))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","289a6fe1":"#most frequent 20 words when label == 1 \nimport collections\nallWords=create_corpus(train,target=1)\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(16,5))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","30a78323":"#List of punctuations and we will remove them from our corpus\nstring.punctuation","fb718d64":"#for  example\ntext='hey # how are !you doing ?'\n\"\".join([char for char in text if char not in string.punctuation])","f2f19f7b":"#for example \ntext='hey 4 look 333 at me0'\nre.sub('[0-9]', '', text)","94ad9098":"#list of stopwords\nstopwords.words('english')","7d547d28":"#for example\ntext='hey this is me and I am here to help you  '\ntokens = word_tokenize(text)\ntokens=[word for word in tokens if word not in stopwords.words('english')]\n' '.join(tokens)","760ed24b":"pstem = PorterStemmer()\ndef clean_text(text):\n    text= text.lower()\n    text= re.sub('[0-9]', '', text)\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens=[pstem.stem(word) for word in tokens]\n    #tokens=[word for word in tokens if word not in stopwords.words('english')]\n    text = ' '.join(tokens)\n    return text","575ea959":"clean_text(\"hey I am here # ! looks 4 GOOD can't see you!\")","d8a37abc":"train[\"clean\"]=train[\"text\"].apply(clean_text)\ntest[\"clean\"]=test[\"text\"].apply(clean_text)","4d47d8b3":"#Let's see the effect of cleaning\ntrain[[\"text\",\"clean\"]].head(4)","a29866e9":"# collecting all words in single list\nlist_= []\nfor i in train.clean:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)\nlen(vocabulary)","6d547021":"tfidf = TfidfVectorizer(sublinear_tf=True,max_features=60000, min_df=1, norm='l2',  ngram_range=(1,2))\nfeatures = tfidf.fit_transform(train.clean).toarray()\nfeatures.shape","f818e694":"features_test = tfidf.transform(test.clean).toarray()","bf50d144":"#split data into 4 parts with same distribution of classes.\nskf = StratifiedKFold(n_splits=4, random_state=48, shuffle=True)\naccuracy=[] # list contains the accuracy for each fold\nn=1\ny=train['target']","5007e319":"for trn_idx, test_idx in skf.split(features, y):\n  start_time = time()\n  X_tr,X_val=features[trn_idx],features[test_idx]\n  y_tr,y_val=y.iloc[trn_idx],y.iloc[test_idx]\n  model= LogisticRegression(max_iter=1000,C=3)\n  #model=MultinomialNB(alpha=0.5)\n  #model=svm.SVC(max_iter=1000)\n  model.fit(X_tr,y_tr)\n  s = model.predict(X_val)\n  sub[str(n)]= model.predict(features_test) \n  \n  accuracy.append(accuracy_score(y_val, s))\n  print((time() - start_time)\/60,accuracy[n-1])\n  n+=1","25cb7791":"accuracy","f3892324":"np.mean(accuracy)*100","47da565e":"from sklearn.metrics import confusion_matrix, classification_report","cd259b30":"pred_valid_y = model.predict(X_val)\nprint(classification_report(y_val, pred_valid_y ))","b4d87b3b":"print(confusion_matrix(y_val, pred_valid_y ))","d23f5bfe":"sub","8356b05e":"df=sub[['1','2','3','4']].mode(axis=1)# select the most frequent predicted class by our model\nsub['target']=df[0]    \nsub=sub[['id','target']]\nsub['target']=sub['target'].apply(lambda x : int(x))","743ec15d":"sub.to_csv('submission.csv',index=False)","1a9e1a46":"## 2-Removing Numbers","cb9456f8":"* we reduced our data from 31480 unique words to 19920","80bdc36a":"# Let's use some machine leaning algorithm","c55ba20f":"* Max number of words in all data is 31 and min is 1!","e15934a4":"## 3-Removing Stopwords","31c4589d":"* We have 92 redundants sapmles in our dataset","d0a0dfff":"## Now let's Build a function that clean our data","077c027f":"# Data Cleaning","58b7900b":"# Submission","7e5ef981":"# Please If you find this kernel helpful, upvote it to help others see it \ud83d\ude0a","614ee7a2":" * We have 31480 different words in our train data","5b093236":"# Evaluating Model on Validation Set","ed7eaab4":"# Let's start with EDA","8c818b7e":"* I just added lower function in order to lowercase all words and stemming","827cdab3":"## 1-Removing Punctuations","4d776225":"* labels are not balanced"}}