{"cell_type":{"668f6631":"code","c4139004":"code","569c469e":"code","f2b639ee":"code","808e474f":"code","10cd85c2":"code","271efc3c":"code","ec56ed79":"code","2400e7fb":"code","cf5b0cdc":"code","ed2d5413":"code","a5721048":"code","0c7d29ff":"code","2812a497":"code","6adc36d0":"code","5ac25c1a":"code","4f242ac2":"markdown","5388a08e":"markdown"},"source":{"668f6631":"#import libraries\nimport os\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import  Flatten, Dense, Dropout\nfrom tensorflow.keras import Model\nimport matplotlib.pyplot as plt","c4139004":"#importing train and test dataset\nTRAIN_PATH = '\/kaggle\/input\/walk-or-run\/walk_or_run_train\/train'\nTEST_PATH = '\/kaggle\/input\/walk-or-run\/walk_or_run_test\/test'","569c469e":"# -- Global Variables -- \nBATCH_SIZE = 128\nCOLOR_MODE = 'rgb'\nTARGET_SIZE = (224, 224)\nGRAY_SCALL = (3,)\nINPUT_SIZE = TARGET_SIZE + GRAY_SCALL\nEPOCHS = 10\nCLASSES = ['Run','Walk']","f2b639ee":"#show an train image\nimg = tf.keras.preprocessing.image.load_img(path=\"..\/input\/walk-or-run\/walk_or_run_train\/train\/walk\/walk_00e3d982.png\", target_size=(224,224))\nimg = np.asarray(img)\nplt.imshow(img)\nplt.show()","808e474f":"# -- Data Normalization --\nidg=tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True, rotation_range=30,\n                                                    rescale=1\/255,validation_split=0.1,\n                                                    preprocessing_function=tf.keras.applications.vgg16.preprocess_input)","10cd85c2":"# -- Data iterators -- \ntrain_data = idg.flow_from_directory(directory=TRAIN_PATH, \n                                                    target_size=TARGET_SIZE, \n                                                    batch_size=BATCH_SIZE, \n                                                    class_mode='categorical', \n                                                    color_mode=COLOR_MODE, \n                                                    subset='training')         \n    \nvalidation_data =idg.flow_from_directory(directory=TRAIN_PATH, \n                                                    target_size=TARGET_SIZE, \n                                                    batch_size=BATCH_SIZE, \n                                                    class_mode='categorical', \n                                                    color_mode=COLOR_MODE, \n                                                    subset='validation')             \n\ntest_data = idg.flow_from_directory(directory=TEST_PATH, \n                                                   target_size=TARGET_SIZE, \n                                                   batch_size=BATCH_SIZE, \n                                                   class_mode='categorical', \n                                                   color_mode=COLOR_MODE)","271efc3c":"# -- plot random batch -- \nimages, labels = train_data.next()\nclasses = np.asarray(CLASSES)\n\n_, axs = plt.subplots(4, 4, figsize=(12, 12))\naxs = axs.flatten()\n\nfor img, l, ax in zip(images, labels, axs):\n    ax.imshow(img)\n    ax.axis('off')\n    l = l.astype(int)\n    ax.set_title(classes[l == 1])\n\nplt.show()","ec56ed79":"# -- Define model -- \n# since we have a small amount of data I'll use the trained weights of VGG16, \n# but I will change the last few layers, and train them.\n\ndef my_model():\n    #Load vgg16 model without classifier layers\n    vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=INPUT_SIZE)\n\n    #Freeze the convolutional base\n    vgg16_model.trainable = False\n    \n    #Add new classifier layers\n    flatten = Flatten()(vgg16_model.layers[-1].output)\n    fc1 = Dense(units=4096, activation='relu')(flatten)\n    dropout = Dropout(0.2)(fc1) \n    fc2 = Dense(units=1024,activation='relu')(dropout)\n    output = Dense(2, activation='softmax')(fc2)\n   \n    #Define a new modol\n    model = Model(inputs = vgg16_model.input, outputs=output)\n    \n    #Model summary\n    model.summary()\n    \n    return model","2400e7fb":"model = my_model()","cf5b0cdc":"# -- Define optimizer and loss --\nopt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nloss = tf.keras.losses.CategoricalCrossentropy()","ed2d5413":"# -- Compile model --\nmodel.compile(optimizer=opt, loss=loss, metrics=['accuracy'])","a5721048":" # -- Callbacks --\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath='my_model.h5', \n                                                    monitor='accuracy', verbose=1, \n                                                    save_best_only=True, \n                                                    save_weights_only=False, \n                                                    mode='auto', \n                                                    save_freq='epoch')\n    \nearlystoping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', \n                                                    min_delta=0, \n                                                    patience=5,  #Number of epochs with no improvement after which training will be stopped.\n                                                    verbose=1, \n                                                    mode='auto')\n    \nlog_dir = '.\/logs\/fit\/' + datetime.datetime.now().strftime('%m.%d.%Y--%H-%M-%S')\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n                                                 histogram_freq=1, \n                                                 write_graph=True,\n                                                 write_images=False, \n                                                 update_freq='epoch')\n","0c7d29ff":"# -- Train model --\nhistory = model.fit(x=train_data, \n                        epochs=EPOCHS, \n                        steps_per_epoch=len(train_data), \n                        verbose=1, \n                        validation_data=validation_data, \n                        validation_steps=1, \n                        callbacks=[checkpoint, earlystoping, tensorboard])\n    \n# -- Save model -- \nmodel.save('my_model.h5')","2812a497":"def run_or_walk(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224,224,3))\n    plt.imshow(img)\n    img = np.asarray(img)\n    img = np.expand_dims(img, axis=0)\n    model = tf.keras.models.load_model('my_model.h5')\n    output = model.predict(img)\n    print(classes[output[0]==1])","6adc36d0":"run_or_walk('..\/input\/walk-or-run\/walk_or_run_test\/test\/walk\/walk_b37fa854.png')","5ac25c1a":"run_or_walk('..\/input\/walk-or-run\/walk_or_run_test\/test\/walk\/walk_0c96b662.png')","4f242ac2":"Thanks for Reading.\n\nPlease upvote me if you like this kernel.","5388a08e":"Resource: https:\/\/www.kaggle.com\/michalashkenazi\/run-or-walk-vgg16-transfer-learning"}}