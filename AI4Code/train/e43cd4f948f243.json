{"cell_type":{"db47059a":"code","8f7cfe62":"code","46f36fa5":"code","3441e872":"code","3f6cb3e1":"code","8a5dddfd":"code","71af3e35":"code","30cdb45b":"code","c9062843":"code","c511ecb1":"code","e25fb542":"code","f358bfe8":"code","0364d50d":"code","098e4865":"code","80175270":"code","48ecf4dc":"code","8461ef6b":"code","0febc8f6":"code","a46015f7":"code","92ba970c":"code","98dac323":"code","3dfb253a":"code","a39cbffe":"code","c2cdd824":"code","c337afa6":"code","de05375e":"code","19178323":"code","7f25ce21":"code","a063f160":"code","eb06f20f":"code","692a56e2":"code","6e1195dc":"code","3292b741":"markdown","ab9f0c9c":"markdown","e4c4c5f0":"markdown","f5ea53bf":"markdown","50fc3fd3":"markdown","b8b8c503":"markdown","427129b5":"markdown","e3916fb6":"markdown","18f6f9a0":"markdown","9b229260":"markdown","cb2ff6bd":"markdown","f30bf0e8":"markdown","bf653930":"markdown","4fa3511b":"markdown","cf16c85c":"markdown","6b56e097":"markdown","0f6fe40f":"markdown","9362fa4e":"markdown","1083ff0b":"markdown","93062372":"markdown","aa972a1c":"markdown","62e85ab2":"markdown","df914c5f":"markdown","93d3990d":"markdown","3c0b1835":"markdown","3027bb2e":"markdown"},"source":{"db47059a":"from glob import glob\nPATH='..\/input\/10-monkey-species\/'\ntraining_file_count=len(glob(f'{PATH}\/training\/\/training\/\/**\/\/*.*'))\nval_file_count=len(glob(f'{PATH}\/validation\/\/validation\/\/**\/\/*.*'))\nprint(f'Images for Training : {training_file_count}')\nprint(f'Images for Validation : {val_file_count}')","8f7cfe62":"! cat '{PATH}monkey_labels.txt'","46f36fa5":"from fastai.imports import *\nfrom fastai.conv_learner import * \nfrom fastai.dataset import * \nfrom fastai.sgdr import *\nfrom fastai.plots import * \nfrom fastai.transforms import *","3441e872":"from PIL import Image\nimport pandas as pd","3f6cb3e1":"train_files_path=glob(f'{PATH}\/training\/\/**\/\/**\/*.*')\nval_files_path=glob(f'{PATH}\/validation\/\/**\/\/**\/*.*')","8a5dddfd":"draw=np.random.choice(train_files_path,3)","71af3e35":"for each in draw:\n    im=Image.open(each)\n    plt.figure()\n    plt.imshow(np.asarray(im))","30cdb45b":"train_img_sz=pd.DataFrame([ Image.open(img).size for img in train_files_path],columns=['x','y'])\nval_im_sz=pd.DataFrame([ Image.open(img).size for img in val_files_path],columns=['x','y'])","c9062843":"train_img_sz.mean(),train_img_sz.std()","c511ecb1":"val_im_sz.mean(),val_im_sz.mean()","e25fb542":"train_img_sz.boxplot(figsize=(20,10))","f358bfe8":"val_im_sz.boxplot(figsize=(20,10))","0364d50d":"sz=256\nbs=64","098e4865":"arch=resnet34","80175270":"def get_data(sz,bs):\n    tfms=tfms_from_model(arch,sz,aug_tfms=transforms_side_on,max_zoom=1.2)\n    return ImageClassifierData.from_paths('.',bs=bs,tfms=tfms,trn_name='..\/input\/10-monkey-species\/training\/training',val_name='..\/input\/10-monkey-species\/validation\/validation')","48ecf4dc":"data=get_data(sz,bs)","8461ef6b":"import random\ndef draw_random_from_ds(data,n):\n    x=random.sample(list(iter(data.trn_ds)),n)\n    for each in x:\n        a=each[0]\n        for i,v in enumerate(a):\n            a[i]=(a[i]-a[i].min())\/(a[i].max()-a[i].min())\n        a=a.reshape((a.shape[1],a.shape[2],3))\n        plt.figure()\n        plt.imshow(a)\n    ","0febc8f6":"draw_random_from_ds(data,3)","a46015f7":"learn=ConvLearner.pretrained(arch,data,ps=.5,precompute=True)","92ba970c":"learn.lr_find()\nlearn.sched.plot()","98dac323":"lr=6e-2","3dfb253a":"learn.fit(lr,3)","a39cbffe":"learn.fit(lr,3)","c2cdd824":"learn.precompute=False","c337afa6":"learn.fit(lr,3)","de05375e":"learn.set_data(get_data(399,64))","19178323":"learn.lr_find()\nlearn.sched.plot()","7f25ce21":"learn.fit(lr,3)","a063f160":"learn.set_data(get_data(512,64))","eb06f20f":"data=get_data(512,64)","692a56e2":"draw_random_from_ds(data,3)","6e1195dc":"learn.fit(lr,3)","3292b741":"Approx. 75% of images have resolution below 1100X900","ab9f0c9c":"Let us build a model next.\nWe will start with image size of 256 first\n","e4c4c5f0":"6e-2 looks like a good learning rate. We will use that and start training our model.","f5ea53bf":"It is a balanced set with each class having similar number of images for train and validation","50fc3fd3":"Let us look at the size of the images that we have.","b8b8c503":"Let us write a function to randomly draw imaged from our data object.","427129b5":"The Accuracy during our first epoch was 100%. It went down to 99.37 during the next epochs. Let us further increase our image size. We will set it to 512.","e3916fb6":"Define a function to get data. We will be using image augmentation to compensate for the small dataset.","18f6f9a0":"In this notebook I will be using fastai library to build a classifier that can identify 10 different monkey species","9b229260":"## Choose the Architecture and Build the Model","cb2ff6bd":"We will now set precompute to False and take advantage of data augmentation.","f30bf0e8":"## Increase Image Size Train Some More","bf653930":"Congratulations! We have reached 100% accuracy.","4fa3511b":"Run lr_find again","cf16c85c":"Let us find the best learning rate for our model using the lr_find function","6b56e097":"## Start Training","0f6fe40f":"I will draw few randomly selected images.","9362fa4e":"Since the dataset is relatively small, we will start with a pre-trained resnet34 architecture and weights.","1083ff0b":"Our accuracy went down slightly. It\u2019s time to get a new data object with increased image size and see if we can improve the accuracy.","93062372":"This data object will now pass batches of 64 images of size 256X256X3 to our model.","aa972a1c":"Let us import our libraries. I would be using fastai and a pre trained model.","62e85ab2":"## Analyze The Data","df914c5f":"Let us See whats in our training and validation directories ","93d3990d":"# **Import Libraries**","3c0b1835":"## Increase Image Size Train Some More","3027bb2e":"The Accuracy of the model is already at 99.6% with an image size of 224.  And we are nowhere near overfitting. We will run few more epochs."}}