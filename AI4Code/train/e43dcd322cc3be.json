{"cell_type":{"5dac7348":"code","23c31b7f":"code","c651ee5d":"code","154b2590":"code","7502c54e":"code","eaec3af3":"code","cec0a19c":"code","a38bb029":"code","e1c20710":"code","77d9af63":"code","6ac5e865":"code","9cff5862":"code","99f0435c":"code","d3617ba9":"code","36e396c3":"markdown","0acd7ee0":"markdown","62f24a22":"markdown","15f6a3dd":"markdown","449bb1dd":"markdown","dc97a877":"markdown","66d112f6":"markdown"},"source":{"5dac7348":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","23c31b7f":"import pandas as pd\nimport matplotlib.pyplot as plt\ndf= pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv')\nplt.figure(figsize=[10,10])\nplt.scatter(df.pelvic_incidence,df.sacral_slope)\nplt.xlabel(\"pelvic incidence\")\nplt.ylabel(\"sacral slope\")\n","c651ee5d":"from sklearn.linear_model import LinearRegression\nlinear_reg=LinearRegression()\nx=df.pelvic_incidence.values.reshape(-1,1)\ny=df.sacral_slope.values.reshape(-1,1)\nlinear_reg.fit(x,y)\ny_head=linear_reg.predict(x)","154b2590":"#%% # Plot regression line and scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x,y)\nplt.xlabel(\"pelvic incidence\")\nplt.ylabel(\"sacral slope\")\nplt.plot(x,y_head,color=\"red\")\nplt.show()","7502c54e":"#%% prediction\nb0=linear_reg.predict([[0]])\nb1=linear_reg.coef_\nprint(\"b0: \",b0)\nprint(\"b1: \",b1)\n#pelvic incidence = 80 sacral slope=?\nprint(\"Predict: \",b1*80+b0)","eaec3af3":"#%% R^2 score\nfrom sklearn.metrics import r2_score\nprint(\"R^2 : \",r2_score(y,y_head))\n","cec0a19c":"\nfrom sklearn.linear_model import LinearRegression\n\nx=df.iloc[:,[1,2,3,4,5]].values\ny=df.pelvic_incidence.values.reshape(-1,1)\n\nmultiple_reg=LinearRegression()\nmultiple_reg.fit(x,y)\nprint(\"b0: \",multiple_reg.intercept_) # or print(\"b0: \",multiple_reg.predict(0))\nprint(\"b1,b2,b3,b4,b5 : \", multiple_reg.coef_)","a38bb029":"y_head=multiple_reg.predict(x)\n#%% R^2 score\nfrom sklearn.metrics import r2_score\nprint(\"R^2 : \",r2_score(y,y_head))","e1c20710":"#%% R^2 score\nprint(\"R^2 :\",multiple_reg.score(x,y))","77d9af63":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 4)\nx_polynomial = polynomial_regression.fit_transform(x)","6ac5e865":"from sklearn.linear_model import LinearRegression\nplt.figure(figsize=[20,20])\npoly_reg=LinearRegression()\npoly_reg.fit(x_polynomial,y)\ny_head=poly_reg.predict(x_polynomial)\nplt.plot(x,y_head,color=\"orange\")\nplt.show()","9cff5862":"#%% R^2 score\nfrom sklearn.metrics import r2_score\nprint(\"R^2 : \",r2_score(y,y_head))","99f0435c":"from sklearn.tree import DecisionTreeRegressor\nimport numpy as np\nx=df.pelvic_incidence.values.reshape(-1,1)\ny=df.sacral_slope.values.reshape(-1,1)\ntree=DecisionTreeRegressor()\ntree.fit(x,y)\n\nx_=np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head=tree.predict(x_)\nplt.plot(x_,y_head,color=\"red\")\nplt.show()","d3617ba9":"x=df.pelvic_incidence.values.reshape(-1,1)\ny=df.sacral_slope.values.reshape(-1,1)\nplt.figure(figsize=[20,20])\nplt.scatter(x,y)\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrandom_reg=RandomForestRegressor(n_estimators=100,random_state=42)\n\nrandom_reg.fit(x,y)\n\nx_=np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head=random_reg.predict(x_)\nplt.plot(x_,y_head,color=\"red\")\nplt.show()","36e396c3":"# > Polynomial Linear Regression\n\ny=b0+b1*x+b2*x^2+...+bn*x^n","0acd7ee0":"R^2 score different way to show R^2 score","62f24a22":"# Introduction\n\nContent\n1. Linear Regression\n2. Multiple Linear Regression\n3. Polynomial Linear Regression\n4. Decision Tree Regression\n5. Random Forest Regression\n","15f6a3dd":"# > Random Forest Regression\n\nUsing more than one machine language algorithm in one algorithm -->Ensemble Learning\n\nRandom forest is a mamber of ensemble learning.","449bb1dd":"# > Multiple Linear Regression\n\ny=b0+b1*x1+b2*x2+...+bn*xn\nMultiple linear regression is similar to linear regression. Multiple linear regression required more than one features.\nI used all dataset's feature.","dc97a877":"# > Decision Tree Regression","66d112f6":"# > Linear Regression\n \ny=b1*x+b0 \n\nb1=constant(bias) \n\nb0=coefficient[](http:\/\/)\n\nA feature (x) is required for linear regression so I used pelvic_incidence column for this regression. And target is sacral slope column."}}