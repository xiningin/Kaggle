{"cell_type":{"17df5d85":"code","9cec3ac9":"code","c3ddbc0f":"code","17a429d0":"code","de1e169e":"code","9c2ce734":"code","c7597a91":"code","5afc72f1":"code","77146085":"code","21e775ee":"code","e389dc66":"code","4d25c2f5":"code","ec4b2f0b":"code","bbea6140":"code","65ea4c1a":"code","08f0bc50":"code","a75bec3e":"code","d3512f61":"code","dca082c1":"code","b169231f":"code","37c9e075":"code","94b17bef":"code","065724a0":"markdown","2e049071":"markdown","15c5b51a":"markdown","c16a0690":"markdown","d55f5288":"markdown","f54e326e":"markdown","a26a1d10":"markdown","431aa6df":"markdown","b3888704":"markdown","b2e090f7":"markdown","c6ccbdf0":"markdown","dacb9202":"markdown","b75240d5":"markdown","5c0d80fd":"markdown","1f743eaa":"markdown","603a42ab":"markdown","6aaf06cb":"markdown","8a9703e0":"markdown"},"source":{"17df5d85":"# Import libraries\n\n# Data handling\nimport pandas as pd\nimport numpy as np\n\n# NLP\n## Stopwords and Stemming\nimport re, nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n## Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nimport re\n# TF-IDF vectors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Data preparation\n## Splitting train and test\nfrom sklearn.model_selection import train_test_split\n## Combining SMOTE and Edited Nearest Neighbors sampling for imbalanced classification\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\n# Classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\n\n# Model performance\n## Performance metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n## ROC curve\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n%matplotlib inline","9cec3ac9":"# Set file path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c3ddbc0f":"# Load the dataset\ndf = pd.read_csv(\"\/kaggle\/input\/ecommerce-reviews-for-women-clothings\/Womens-Clothing-E-Commerce-Reviews.csv\", encoding=\"utf-8\")\n\n# Show the first five rows\ndf.head()","17a429d0":"# Get information of the data\ndf.info()","de1e169e":"# Check the value of recommendation index\ndf['Recommended IND'].value_counts()","9c2ce734":"# Set stopwords and lemmatizer\nstop_words = set(stopwords.words('english'))\nwordnet_lemmatizer = WordNetLemmatizer()","c7597a91":"# Define a function to delete stopwords\ndef review_to_words(raw_review):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",str(raw_review)) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words )) ","5afc72f1":"# Apply the function to delete stopwords from product review text\ndf['Cleaned_Review'] = df['Review Text'].apply(lambda x: review_to_words(x))\n\n# Show the first five rows\ndf[['Review Text', 'Cleaned_Review']].head()","77146085":"# Initiate stemmers\nporter = PorterStemmer()\nlancaster=LancasterStemmer()","21e775ee":"# Define a function to apply porter stemming\ndef stemSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    stem_sentence=[]\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n        stem_sentence.append(\" \")\n    return \"\".join(stem_sentence)","e389dc66":"# Apply the porter stemming to an example sentence.\nstemSentence(\"I was born in Japan on September 6th, and it is also my friend's birthday.\")","4d25c2f5":"# Initiate lemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()","ec4b2f0b":"# Define a function to perform both tokenization and stemming\ndef tokenize_and_lemmatize(text):\n    \n    # Tokenize by sentence, then by word\n    tokens = [y for x in nltk.sent_tokenize(text) for y in nltk.word_tokenize(x)]\n    \n    # Filter out raw tokens to remove noise\n    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n    \n    # Stem the filtered_tokens\n    lemmas = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in filtered_tokens]\n    \n    return lemmas","bbea6140":"# Apply tokenization and lemmatization to an example sentence.\nwords_lemmatized = tokenize_and_lemmatize(\"I was born in Japan on September 6th, and it is also my friend's birthday.\")\nprint(words_lemmatized)","65ea4c1a":"# Instantiate TfidfVectorizer object with stopwords and tokenizer\n# parameters for efficient processing of text\nvectorizer = TfidfVectorizer(max_df=0.9, max_features=200000,\n                             min_df=0.1, stop_words='english',\n                             use_idf=True, tokenizer=tokenize_and_lemmatize,\n                             ngram_range=(1,3))","08f0bc50":"# Split data into train data and test data\ntrain_data, test_data = train_test_split(df, train_size=0.7, random_state=0)\n\n# Split train data into vectorized X and y\nX_train = vectorizer.fit_transform(train_data['Cleaned_Review']).toarray()\ny_train = train_data['Recommended IND']\n\n# Split test data into vectorized X and y\nX_test = vectorizer.transform(test_data['Cleaned_Review'])\ny_test = test_data['Recommended IND']","a75bec3e":"# Rebalance the Data with SMOTE-ENN\nX_train, y_train=SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority')).fit_sample(X_train,y_train)\n\ny_train.value_counts()","d3512f61":"# Initiate and train the model\nlr = LogisticRegression().fit(X_train, y_train)\n\n# Make prediction\nlr_pred = lr.predict(X_test)\n\n# Show the model performance\nprint(\"Accuracy:{:.3f}\". format(accuracy_score(y_test, lr_pred)))\nprint(\"Precision:{:.3f}\". format(precision_score(y_test, lr_pred)))\nprint(\"Recall:{:.3f}\". format(recall_score(y_test, lr_pred)))\nprint(\"F1 Score:{:.3f}\". format(f1_score(y_test, lr_pred)))\nprint(\"ROC AUC Score:{:.3f}\". format(roc_auc_score(y_test, lr_pred)))\nconfusion_matrix(y_test, lr_pred)","dca082c1":"# Initiate and train the model\nnb = MultinomialNB().fit(X_train, y_train)\n\n# Make Prediction\nnb_pred = nb.predict(X_test)\n\n# Show model performance\nprint(\"Accuracy:{:.3f}\". format(accuracy_score(y_test, nb_pred)))\nprint(\"Precision:{:.3f}\". format(precision_score(y_test, nb_pred)))\nprint(\"Recall:{:.3f}\". format(recall_score(y_test, nb_pred)))\nprint(\"F1 Score:{:.3f}\". format(f1_score(y_test, nb_pred)))\nprint(\"ROC AUC Score:{:.3f}\". format(roc_auc_score(y_test, nb_pred)))\nconfusion_matrix(y_test, nb_pred)","b169231f":"# Initiate and train the model\nsvm = LinearSVC().fit(X_train, y_train)\n\n# Make prediction\nsvm_pred = svm.predict(X_test)\n\n# Show model performance\nprint(\"Accuracy:{:.3f}\". format(accuracy_score(y_test, svm_pred)))\nprint(\"Precision:{:.3f}\". format(precision_score(y_test, svm_pred)))\nprint(\"Recall:{:.3f}\". format(recall_score(y_test, svm_pred)))\nprint(\"F1 Score:{:.3f}\". format(f1_score(y_test, svm_pred)))\nprint(\"ROC AUC Score:{:.3f}\". format(roc_auc_score(y_test, svm_pred)))\nconfusion_matrix(y_test, svm_pred)","37c9e075":"# Create a dictionary of all models.\nmodels = [{'label': 'Logistic Regression','model': lr,},\n          {'label': 'Naive Bayes','model': nb,},\n          {'label': 'SVM','model': svm,}\n         ]","94b17bef":"# Plot the ROC curve.\nfor m in models:\n    model = m['model']\n    model.fit(X_train, y_train)\n    y_pred=model.predict(X_test)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='%s AUC = %0.2f' % (m['label'], roc_auc))\n\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","065724a0":"## 1.1. Create Binary Variable \"Sentiment\"","2e049071":"# 3. ROC Curve","15c5b51a":"## 2.3. SVM","c16a0690":"# Sentiment Analysis of E-Commerce Review Data","d55f5288":"## 2.1. Logistic Regression","f54e326e":"- Let's do sentiment analysis as a binary classification problem\n  - The outcome variable is operatinalized as \"recommend\" or \"not recommend\"\n- The analysis contains Natural Language Processing\n  - Stemming\n  - Lemmatizing\n  - TF-IDF\n- Classification algorithms: known to be effective in NLP \n  - Logistic regression\n  - Suppor Vector Machine\n  - Naive Bayes \n- Performance metric\n  - AUC","a26a1d10":"- SVM achieved the best performance.","431aa6df":"### 1.2.2. Stemming","b3888704":"- Tokenization and lemmatization leds to separated but meaningful words.","b2e090f7":"### 1.2.1. Delete Stopwords","c6ccbdf0":"## 2.2. Naive Bayes Classification","dacb9202":"## 1.2. Preprocessing of Text Data","b75240d5":"## 1.3. Create vectorized training data and test data","5c0d80fd":"- The variable \"Recommended IND\" is recorded with 0 or 1. So we can use it as the output variable for a binary classification.\n- There are much more positive cases than negative cases. The data is imbalanced.   ","1f743eaa":"### 1.2.3. Tokenization and Lemmatization","603a42ab":"- Porter stemming made some words meaningless.","6aaf06cb":"# 1. Load and Explore the Dataset","8a9703e0":"# 2. Classification"}}