{"cell_type":{"886f3617":"code","99043924":"code","a40b3028":"code","16b44316":"code","83426a01":"code","c93cff57":"code","7fec2854":"code","da344ff2":"code","8963b408":"code","b6ac2a45":"code","7c0b3f76":"code","a0751618":"code","26443a46":"code","7e9a513b":"code","14259990":"code","70aab0ae":"code","5cd613dc":"code","9b4a0434":"code","bef37416":"code","dd3aeb4e":"code","03c26285":"code","a1cc7fa4":"code","35378f90":"code","24ef6da8":"code","7cf044d2":"code","2a637103":"code","3c061936":"code","e9e7d91e":"code","e2cb953d":"code","05f17626":"code","a030b8cf":"code","d3e7ee44":"code","e2c5aa5e":"code","d9df9f83":"code","76b25663":"code","e91a3216":"code","8e723f63":"code","e0902ab2":"code","1e54042a":"code","b72a2254":"code","f2b7a5af":"code","e0e2b452":"code","cc83ed9a":"code","8b3f96f0":"code","e15fd9bd":"code","c5e70e2e":"code","62564c09":"code","44afdb57":"code","6a579bd9":"code","311af2b7":"code","a470b3d3":"code","5b9168a0":"code","01cd71ad":"code","f0b16df0":"code","7cb1015e":"code","e081a26e":"code","94708823":"code","0b6b85ed":"code","330358b8":"code","4d7839f5":"code","c99a4d5a":"code","6195f6d2":"code","d854eadc":"code","7f03e17b":"code","fd48f2a6":"code","261143db":"code","54becbe0":"code","ba3cd664":"code","445cbadf":"code","8f5ffd36":"code","88fed63c":"code","3413e19a":"code","298a6bfa":"code","75c8b2f2":"code","22dfa484":"code","3f9f0a66":"code","9266d706":"code","2a5f605e":"code","477c3859":"code","acdbf153":"code","266fd903":"code","22ed085b":"code","5e67145b":"code","68d1d476":"code","9b7185d8":"code","047df857":"code","0e382140":"code","e80f8926":"code","88d98a19":"code","e9441b1d":"code","eb5e50f1":"code","7792842f":"code","da9a916f":"code","79bedc3e":"code","f5511397":"code","9236e48e":"code","b514f8a4":"code","fef8fac2":"code","2102a46b":"code","53c47e59":"code","edef38be":"code","91289556":"code","f042a74b":"markdown","f6d9aacf":"markdown","49117a9a":"markdown","1300b2c5":"markdown","09764e2b":"markdown","e22a0d54":"markdown","ffd03027":"markdown","123abb4d":"markdown","024674d2":"markdown","2685cd71":"markdown","a1b2d151":"markdown","35951dc8":"markdown","a4ae94bd":"markdown","78b7c661":"markdown","15b18e74":"markdown","8a685722":"markdown","3204cc92":"markdown","48ea1e49":"markdown","45585f36":"markdown","7bd4a22e":"markdown","934b8919":"markdown","33f8108e":"markdown","6efc6130":"markdown","f119a122":"markdown","4f26540f":"markdown","3cd26927":"markdown","6ff44f39":"markdown","a10fedd3":"markdown","b77f18d1":"markdown","2d58be2c":"markdown","f66548e0":"markdown","48dc1f96":"markdown","c20ab4c9":"markdown","235c0992":"markdown","ccad704b":"markdown","74956a1e":"markdown","93ceaec4":"markdown","bcbd81f0":"markdown","f844b4b7":"markdown","3fa68510":"markdown","f7ec30eb":"markdown","af12633d":"markdown","7da2cbf4":"markdown","0d225914":"markdown","4127c0dd":"markdown","f7b0b7dc":"markdown","7edc3b31":"markdown","842da802":"markdown","8d677069":"markdown","8719aaa8":"markdown","97883ac3":"markdown","5d4b1658":"markdown","3cbb658b":"markdown","f0f78f3d":"markdown","3baa384e":"markdown","285c3c0f":"markdown","7b93224c":"markdown","b7726475":"markdown","dd51ef24":"markdown","e27ba264":"markdown","8389ce2c":"markdown","fef7f648":"markdown","444a4b5e":"markdown","fdaa0e39":"markdown","b80bea3e":"markdown","52d84095":"markdown","5141b3ff":"markdown","02b60a6e":"markdown","bb84aa31":"markdown","2e9beefc":"markdown","1c13017a":"markdown","95ede74c":"markdown","8b787c68":"markdown","2400c353":"markdown","228d77e8":"markdown","b6142a7e":"markdown","03dc2679":"markdown","158c9507":"markdown","5fd0b5a1":"markdown","a9f222fb":"markdown","0574c1a0":"markdown","14ed7447":"markdown","065618f1":"markdown","1569d352":"markdown","d1c04cc3":"markdown","af771fe2":"markdown","ef46cf89":"markdown","7c5d4c56":"markdown","78af6418":"markdown","6788e775":"markdown","9e29f671":"markdown","914b6fe6":"markdown","74cc2078":"markdown","63ed6218":"markdown","2e6f90b5":"markdown","b3377643":"markdown","4dd57d2b":"markdown","755f3373":"markdown","7a42dde6":"markdown","ffa11083":"markdown","62932aa2":"markdown","244d864c":"markdown","c744263e":"markdown","d8a0f53d":"markdown"},"source":{"886f3617":"# basic libraries\nimport os\nimport numpy as np\nimport pandas as pd\n\n# this will allow us to print all the files as we generate more in the kernel\ndef print_files():\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n# check Trick 91 for an example\ndef generate_sample_data(): # creates a fake df for testing\n    number_or_rows = 20\n    num_cols = 7\n    cols = list(\"ABCDEFG\")\n    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n    df.index = pd.util.testing.makeIntIndex(number_or_rows)\n    return df\n\n# check Trick 91 for an example\ndef generate_sample_data_datetime(): # creates a fake df for testing\n    number_or_rows = 365*24\n    num_cols = 2\n    cols = [\"sales\", \"customers\"]\n    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n    df.index = pd.util.testing.makeDateIndex(number_or_rows, freq=\"H\")\n    return df\n\n# show several prints in one cell. This will allow us to condence every trick in one cell.\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nprint_files()","99043924":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\nprint(\"The shape of the df is {}\".format(df.shape))\n\ndel df\n\ndf = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\", skiprows = lambda x: x>0 and np.random.rand() > 0.01)\nprint(\"The shape of the df is {}. It has been reduced 10 times!\".format(df.shape))\n\n\n'''\nHow it works:\nskiprows accepts a function that is evaluated against the integer index.\nx > 0 makes sure that the headers is not skipped\nnp.random.rand() > 0.01 returns True 99% of the tie, thus skipping 99% of the time.\nNote that we are using skiprows\n'''","a40b3028":"d = {\\\n\"zip_code\": [12345, 56789, 101112, 131415],\n\"factory\": [100, 400, 500, 600],\n\"warehouse\": [200, 300, 400, 500],\n\"retail\": [1, 2, 3, 4]\n}\n\ndf = pd.DataFrame(d)\ndf\n\n# save to csv\ndf.to_csv(\"trick99data.csv\")\n\ndf = pd.read_csv(\"trick99data.csv\")\ndf\n# To avoid Unnamed: 0\n\ndf = pd.read_csv(\"trick99data.csv\", index_col=0)\n# or when saving df = pd.read_csv(\"trick99data.csv\", index = False)\ndf","16b44316":"d = {\\\n\"zip_code\": [12345, 56789, 101112, 131415],\n\"factory\": [100, 400, 500, 600],\n\"warehouse\": [200, 300, 400, 500],\n\"retail\": [1, 2, 3, 4]\n}\n\ndf = pd.DataFrame(d)\ndf\n\n# we have to reassing\n\n# location_type is generated automatically from the columns left after specifying id_vars (you can pass a list also)\ndf = df.melt(id_vars = \"zip_code\", var_name = \"location_type\", value_name = \"distance\")\ndf","83426a01":"# Trick 97\n# Convert\nd = {\\\n\"year\": [2019, 2019, 2020],\n\"day_of_year\": [350, 365, 1]\n}\n\ndf = pd.DataFrame(d)\ndf\n\n# Step 1: create a combined column\ndf[\"combined\"] = df[\"year\"]*1000 + df[\"day_of_year\"]\ndf\n\n# Step 2: convert to datetime\ndf[\"date\"] = pd.to_datetime(df[\"combined\"], format = \"%Y%j\")\ndf","c93cff57":"print(pd.__version__)\n# Pandas version 0.25 or higher requiered and you need hvplot\n\nimport pandas as pd\ndf = pd.read_csv(\"..\/input\/drinks-by-country\/drinksbycountry.csv\")\ndf\n\n# this one is not interactve\ndf.plot(kind = \"scatter\", x = \"spirit_servings\", y = \"wine_servings\")\n\n# run !pip install hvplot\n#pd.options.plotting.backend = \"hvplot\"\n#df.plot(kind = \"scatter\", x = \"spirit_servings\", y = \"wine_servings\", c = \"continent\")","7fec2854":"d = {\\\n\"col1\": [2019, 2019, 2020],\n\"col2\": [350, 365, 1],\n\"col3\": [np.nan, 365, None]\n}\n\ndf = pd.DataFrame(d)\ndf\n\n# Solution 1\ndf.isnull().sum().sum()\n\n# Solution 2\ndf.isna().sum()\n\n# Solution 3\ndf.isna().any()\n\n# Solution 4:\ndf.isna().any(axis = None)","da344ff2":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\", usecols = [\"Pclass\", \"Sex\", \"Parch\", \"Cabin\"])\ndf\n\n# let's see how much our df occupies in memory\ndf.memory_usage(deep = True)\n\n# convert to smaller datatypes\ndf = df.astype({\"Pclass\":\"int8\",\n                \"Sex\":\"category\", \n                \"Parch\": \"Sparse[int]\", # most values are 0\n                \"Cabin\":\"Sparse[str]\"}) # most values are NaN\n\ndf.memory_usage(deep = True)","8963b408":"d = {\"genre\": [\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"D\", \"E\", \"F\"]}\ndf = pd.DataFrame(d)\ndf\n\n# Step 1: count the frequencies\nfrequencies = df[\"genre\"].value_counts(normalize = True)\nfrequencies\n\n# Step 2: establish your threshold and filter the smaller categories\nthreshold = 0.1\nsmall_categories = frequencies[frequencies < threshold].index\nsmall_categories\n\n# Step 3: replace the values\ndf[\"genre\"] = df[\"genre\"].replace(small_categories, \"Other\")\ndf[\"genre\"].value_counts(normalize = True)","b6ac2a45":"d = {\"customer\": [\"A\", \"B\", \"C\", \"D\"], \"sales\":[1100, 950.75, \"$400\", \"$1250.35\"]}\ndf = pd.DataFrame(d)\ndf\n\n# Step 1: check the data types\ndf[\"sales\"].apply(type)\n\n# Step 2: use regex\ndf[\"sales\"] = df[\"sales\"].replace(\"[$,]\", \"\", regex = True).astype(\"float\")\ndf\ndf[\"sales\"].apply(type)","7c0b3f76":"# Solution 1\nnumber_or_rows = 365*24 # hours in a year\npd.util.testing.makeTimeDataFrame(number_or_rows, freq=\"H\")\n\n# Solution 2\nnum_cols = 2\ncols = [\"sales\", \"customers\"]\ndf = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\ndf.index = pd.util.testing.makeDateIndex(number_or_rows, freq=\"H\")\ndf","a0751618":"d = {\"A\":[15, 20], \"B\":[20, 25], \"C\":[30 ,40], \"D\":[50, 60]}\ndf = pd.DataFrame(d)\ndf\n\n# Using insert\ndf.insert(3, \"C2\", df[\"C\"]*2)\ndf\n\n# Other solution\ndf[\"C3\"] = df[\"C\"]*3 # create a new columns, it will be at the end\ncolumns = df.columns.to_list() # create a list with all columns\nlocation = 4 # specify the location where you want your new column\ncolumns = columns[:location] + [\"C3\"] + columns[location:-1] # reaarange the list\ndf = df[columns] # create te dataframe in with the order of columns you like\ndf\n","26443a46":"df = pd.Series([\"Geordi La Forge\", \"Deanna Troi\", \"Data\"]).to_frame()\ndf.rename({0:\"names\"}, inplace = True, axis = 1)\ndf\n#                              split on first space  \ndf[\"first_name\"] = df[\"names\"].str.split(n = 1).str[0]\ndf[\"last_name\"] = df[\"names\"].str.split(n = 1).str[1]\ndf","7e9a513b":"df = generate_sample_data()\ndf.head()\n\n# Solution 1\ndf[[\"A\", \"C\", \"D\", \"F\", \"E\", \"G\", \"B\"]].head() # doesn't modify in place\n\n# Solution 2\ncols_to_move = [\"A\", \"G\", \"B\"]\n\nnew_order = cols_to_move + [c for c in df.columns if c not in cols_to_move] # generate your new order\ndf[new_order].head()\n\n# Solutin 3: using index\ncols = df.columns[[0, 5 , 3, 4, 2, 1, 6]] # df.columns returns a series with index, we use the list to iorder the index as we like --> this way we order the columns\ndf[cols].head()\n","14259990":"df = generate_sample_data_datetime()\ndf.shape\ndf.head()\n\n# Step 1: resample by D. Basically agregate by day and use to_frame() to convert it to frame\ndaily_sales = df.resample(\"D\")[\"sales\"].sum().to_frame()\ndaily_sales\n\n# Step 2: filter weekends\nweekends_sales = daily_sales[daily_sales.index.dayofweek.isin([5, 6])]\nweekends_sales\n\n'''\ndayofweek day\n0         Monday\n1         Tuesday\n2         Wednesday\n3         Thursday\n4         Friday\n5         Saturday\n6         Sunday\n'''","70aab0ae":"print_files()\n\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.head()\n\n# Problem 1\nprint(\"The Problem relies on that we don't know the column name\")\ndf.groupby(\"Pclass\")[\"Age\"].agg([\"mean\", \"max\"])\n\n# Problem 2\nprint(\"The Problem relies on that we have multiindex\")\ndf.groupby(\"Pclass\").agg({\"Age\":[\"mean\", \"max\"]})\n\n# Solution new in pandas 0.25 and higher\nprint(\"Now we have solved the previous problems by specifyig the column final name we want.\")\nprint(\"BUT IT ONLY WORKS WITH A COLUMN. TO THIS KIND OF OPERATIONS ON MULTIPLE COLUMNS CHECK THE NEXT CELL\")\ndf.groupby(\"Pclass\")[\"Age\"].agg(age_mean = \"mean\", age_max = \"max\")\n","5cd613dc":"def my_agg(x):\n    names = {\n        'age_mean': x['Age'].mean(),\n        'age_max':  x['Age'].max(),\n        'fare_mean': x['Fare'].mean(),\n        'fare_max': x['Fare'].max()\n    } # define you custom colum names and operations\n\n    return pd.Series(names, index=[ key for key in names.keys()]) # all the columns you create in the previous dictionary will be in this list comprehension\n\ndf.groupby('Pclass').apply(my_agg)\n\n# reference\n# https:\/\/stackoverflow.com\/questions\/44635626\/rename-result-columns-from-pandas-aggregation-futurewarning-using-a-dict-with\n","9b4a0434":"# Do some fast feature eng on the DF\nd = {\"gender\":[\"male\", \"female\", \"male\"], \"color\":[\"red\", \"green\", \"blue\"], \"age\":[25, 30, 15]}\ndf = pd.DataFrame(d)\ndf\n\n# Solution\nmap_dict = {\"male\":\"M\", \"female\":\"F\"}\ndf[\"gender_mapped\"] = df[\"gender\"].map(map_dict) # using dictionaries to map values\ndf[\"color_factorized\"] = df[\"color\"].factorize()[0] # using factorize: returns a tuple of arrays (array([0, 1, 2]), Index(['red', 'green', 'blue'], dtype='object')) that's why we select [0]\ndf[\"age_compared_boolean\"] = df[\"age\"] < 18 # return a True False boolean value\n\ndf","bef37416":"print(\"This df occupies way too much space\")\ndf = generate_sample_data()\ndf\n\nprint(\"using set_option to save some screen space\")\npd.set_option(\"display.max_rows\", 6)\ndf\n\nprint(\"use reset_option all to reset to default\")\npd.reset_option(\"all\")\ndf","dd3aeb4e":"print_files()\ndf = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\")\n\n# Step 1: Let's the datetype of the columns\ncol_types = df.dtypes.to_frame()\ncol_types.rename({0:\"type\"}, inplace = True, axis = 1)\ncol_types\ncol_types.to_csv(\"trick83data.csv\")\n\n# Step 2: Let's import the previous data and convert it to a dictionary\ncol_dict = pd.read_csv(\"trick83data.csv\", index_col = 0)[\"type\"].to_dict()\n\n# Step 3: Edit the dictionary with the correct data types\nprint(\"Original dictionary\")\ncol_dict\ncol_dict[\"country\"] = \"category\"\ncol_dict[\"continent\"] = \"category\"\nprint(\"Modified dictionary\")\ncol_dict\n\n# Step 4: Use the dictionary to import the data\ndf = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\", dtype=col_dict)\ndf.dtypes\n\n# Note: please note that you can use the dict from step1 and paste in like this\ndf = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\", \\\ndtype=\n{'country': 'category',\n 'beer_servings': 'int64',\n 'spirit_servings': 'int64',\n 'wine_servings': 'int64',\n 'total_litres_of_pure_alcohol': 'float64',\n 'continent': 'category'})\n# However, if you have many colums, this can be confusing\ndf.dtypes","03c26285":"df = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\", index_col=\"country\")\ndf.iloc[15:20, :].loc[:, \"beer_servings\":\"wine_servings\"]\n# iloc is used to filter the rows and loc the columns","a1cc7fa4":"d = {\"customer\":[\"A\", \"B\", \"C\", \"D\", \"E\"], \"sales\":[100, \"100\", 50, 550.20, \"375.25\"]}\ndf = pd.DataFrame(d)\n# everything seems  but this operation crashes df[\"sales\"].sum(). We have mixed data types\ndf.dtypes\ndf[\"sales\"].apply(type) # Wow we can see that we have int, str, floats inn one column\ndf[\"sales\"].apply(type).value_counts() # See the number of each value\n\ndf[\"sales\"] = df[\"sales\"].astype(float) # convert the data to float\ndf[\"sales\"].sum()\ndf[\"sales\"].apply(type).value_counts()","35378f90":"df = generate_sample_data().T\ncols_str = list(map(str, list(df.columns))) # so that we can do df[\"0\"] as string for the example\ndf.columns = cols_str\n\n# Using pandas concatenation\n# if you are ever confused about axis = 1 or axis = 0, just put axis = \"columns\" or axis = \"rows\"\npd.concat([df.loc[:, \"0\":\"2\"], df.loc[:, \"6\":\"10\"], df.loc[:, \"16\":\"19\"]], axis = \"columns\") # ------------------> here we are selecting columns converted to strings\n\n# Using lists\n# please ntoe that df.columns is a series with index, so we are using index to filter # -------------------------> here we are selecting the index of columns\ndf[list(df.columns[0:3]) + list(df.columns[6:11]) + list(df.columns[16:20])]\n\n# Using numpy\ndf.iloc[:, np.r_[0:3, 6:11, 16:20]] # probably the most beautiful solution","24ef6da8":"df = generate_sample_data()\ndf.head()\ndf.shape\n\n# absolute values\n(df[\"A\"] < 5).sum()\nprint(\"In the columns A we have {} of rows that are below 5\".format((df[\"A\"] < 5).sum()))\n\n# percentage\n(df[\"A\"] < 5).mean()\nprint(\"In the columns A the values that are below 5 represent {}%\".format((df[\"A\"] < 5).mean()))","7cf044d2":"# let's generate some fake data\ndf1 = generate_sample_data()\ndf2 = generate_sample_data()\ndf3 = generate_sample_data()\n# df1.head()\n# df2.head()\n# df3.head()\ndf1.to_csv(\"trick78data1.csv\")\ndf2.to_csv(\"trick78data2.csv\")\ndf3.to_csv(\"trick78data3.csv\")\n\n# Step 1 generate list with the file name\nlf = []\nfor _,_, files in os.walk(\"\/kaggle\/working\/\"):\n    for f in files:\n        if \"trick78\" in f:\n            lf.append(f)\n            \nlf\n\n# You can use this on your local machine\n#from glob import glob\n#files = glob(\"trick78.csv\")\n\n# Step 2: assing create a new column named filename and the value is file\n# Other than this we are just concatinating the different dataframes\ndf = pd.concat((pd.read_csv(file).assign(filename = file) for file in lf), ignore_index = True)\ndf.sample(10)","2a637103":"d = {\"genre\": [\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"D\", \"E\", \"F\"]}\ndf = pd.DataFrame(d)\ndf[\"genre\"].value_counts()\n\n# Step 1: count the frequencies\ntop_four = df[\"genre\"].value_counts().nlargest(4).index\ntop_four\n\n# Step 2: update the df\ndf_updated = df.where(df[\"genre\"].isin(top_four), other = \"Other\")\ndf_updated[\"genre\"].value_counts()","3c061936":"df = pd.read_csv(\"..\/input\/imdb-data\/IMDB-Movie-Data.csv\")\ndf.columns = map(str.lower, list(df.columns)) # convert headers to lower type\ndf.shape\n# select top 3 genre\ntop_genre = df[\"genre\"].value_counts().to_frame()[0:3].index\n\n# now let's filter the df with the top genre\ndf_top = df[df[\"genre\"].isin(top_genre)]\ndf_top\ndf_top.shape\ndf_top[\"genre\"].unique()","e9e7d91e":"df = pd.read_csv(\"..\/input\/imdb-data\/IMDB-Movie-Data.csv\", usecols=[\"Title\"])\ndf[\"Words\"] = df[\"Title\"].str.count(\" \") + 1\ndf","e2cb953d":"# Run this on you local machine\n# url = \"https:\/\/es.wikipedia.org\/wiki\/Twitter\"\n# tables = pd.read_html(url)\n# len(tables)\n\n# matching_tables = pd.read_html(url, match = \"Followers\")\n# matching_tables[0]","05f17626":"df = pd.read_csv(\"..\/input\/imdb-data\/IMDB-Movie-Data.csv\")\ndf.head()\n\nmeta = df.pop(\"Metascore\").to_frame()\ndf.head()\nmeta.head()","a030b8cf":"df = pd.read_csv(\"..\/input\/imdb-data\/IMDB-Movie-Data.csv\")\ndf.head()\n\n# Using cut you can specify the bin edges\npd.cut(df[\"Metascore\"], bins = [0, 25, 50, 75, 99]).head()\n\n# Using qcut you can specify the number of bins and it fill generate of aproximate equal size\npd.qcut(df[\"Metascore\"], q = 3).head()\n\n# cut and qcut accept label bin size\npd.qcut(df[\"Metascore\"], q = 4, labels = [\"awful\", \"bad\", \"average\", \"good\"]).head()","d3e7ee44":"# you will have to run on your local machine\n#from tabula import read_pdf\n# df = read_pdf(\"test.pdf\", pages = \"all\")","e2c5aa5e":"print(pd.__version__)\nprint(pd.show_versions())","d9df9f83":"d = {\"A\":[1, 2, 3, 4,], \"B\":[1.0, 2.0, 3.0, 4.0], \"C\":[1.00000, 2.00000, 3.00000, 4.000003], \"D\":[1.0, 2.0, 3.0, 4.0], \"E\":[4.0, 2.0, 3.0, 1.0]}\ndf = pd.DataFrame(d)\ndf\n\ndf[\"A\"].equals(df[\"B\"]) # they requiere identical datatypes\ndf[\"B\"].equals(df[\"C\"])\ndf[\"B\"].equals(df[\"D\"])\ndf[\"B\"].equals(df[\"E\"]) # and the same order\n\nprint(pd.testing.assert_series_equal(df[\"A\"], df[\"B\"], check_names=False, check_dtype=False)) # assertion passes","76b25663":"# You will have to run this on you local machine\n#apple_stocks = pd.read_html(\"https:\/\/finance.yahoo.com\/quote\/AAPL\/history?p=AAPL\")\n#pd.concat([apple_stocks[0], apple_stocks[1]])","e91a3216":"print_files()\n\ndf = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\", usecols=[\"continent\", \"beer_servings\"])\ndf.head()\n\n(df.assign(continent = df[\"continent\"].str.title(),\n           beer_ounces = df[\"beer_servings\"]*12,#                                     this will allow yo set a title\n           beer_galons = lambda df: df[\"beer_ounces\"]\/128).query(\"beer_galons > 30\").style.set_caption(\"Average beer consumption\"))","8e723f63":"d = {\"state\":[\"ny\", \"CA\", \"Tx\", \"FI\"], \"country\":[\"USA\", \"usa\", \"UsA\", \"uSa\"], \"pop\":[1000000, 2000000, 30000, 40000]}\ndf = pd.DataFrame(d)\ndf\n\nint_types = [\"int64\"]\n# creating new columns\nfor col in df.columns:\n    ctype = str(df[col].dtype)\n    if ctype in int_types:\n        df[f'{col}_millions'] = df[col]\/1000000\n    elif ctype == \"object\":\n        df[f'{col}_new'] = df[col].str.upper()\n        # you can also drop the columns\n        df.drop(col, inplace = True, axis = \"columns\")\n        \ndf","e0902ab2":"df = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\")\ndf\n\ndrink = \"wine\"\n\n# allows us to iterate fast over columns\ndf[f'{drink}_servings'].to_frame()","1e54042a":"df = pd.DataFrame({\"gender\":[\"Male\", \"Female\", \"Female\", \"Male\"]})\ndf\n\n# Getting this nasty warning\nmales = df[df[\"gender\"] == \"Male\"]\nmales[\"abbreviation\"] = \"M\"\n\n# Fixing the error\nprint(\"Fixing the warning with print\")\nmales = df[df[\"gender\"] == \"Male\"].copy()\nmales[\"abbreviation\"] = \"M\"\nmales","b72a2254":"d = {\"salesperson\":[\"Nico\", \"Carlos\", \"Juan\", \"Nico\", \"Nico\", \"Juan\", \"Maria\", \"Carlos\"], \"item\":[\"Car\", \"Truck\", \"Car\", \"Truck\", \"cAr\", \"Car\", \"Truck\", \"Moto\"]}\ndf = pd.DataFrame(d)\ndf\n\n# Fixing columns\ndf[\"salesperson\"] = df[\"salesperson\"].str.title()\ndf[\"item\"] = df[\"item\"].str.title()\n\ndf[\"count_by_person\"] = df.groupby(\"salesperson\").cumcount() + 1\ndf[\"count_by_item\"] = df.groupby(\"item\").cumcount() + 1\ndf[\"count_by_both\"] = df.groupby([\"salesperson\",\"item\"]).cumcount() + 1\ndf","f2b7a5af":"df = pd.DataFrame({\"gender\":[\"Male\", \"Female\", \"Female\", \"Male\"]})\ndf\n\n# Getting this nasty warning\ndf[df[\"gender\"] == \"Male\"][\"gender\"] = 1\ndf[df[\"gender\"] == \"Female\"][\"gender\"] = 0\n\n\nprint(\"Fix using loc\")\ndf.loc[df[\"gender\"] == \"Male\", \"gender\"] = 1\ndf.loc[df[\"gender\"] == \"Female\", \"gender\"] = 0\ndf","e0e2b452":"url = \"https:\/\/github.com\/justmarkham?tab=repositories\"\n\n# run it on your local machine\n# df = pd.read_json(url)\n# df = df[df[\"fork\"] == False]\n# df.shape\n# df.head()\n\n# lc = [\"name\", \"stargazers_count\", \"forks_count\"]\n# df[lc].sort_values(\"stargazers_count\", asending = False).head(10)","cc83ed9a":"d = {\"salesperson\":[\"Nico\", \"Carlos\", \"Juan\", \"Nico\", \"Nico\", \"Juan\", \"Maria\", \"Carlos\"], \"item\":[10, 120, 130, 200, 300, 550, 12.3, 200]}\ndf = pd.DataFrame(d)\ndf\n\ndf[\"running_total\"] = df[\"item\"].cumsum()\ndf[\"running_total_by_person\"] = df.groupby(\"salesperson\")[\"item\"].cumsum()\ndf\n\n# other useful functions are cummax(), cummin(), cumprod()","8b3f96f0":"d = {\"orderid\":[1, 1, 1, 2, 2, 3, 4, 5], \"item\":[10, 120, 130, 200, 300, 550, 12.3, 200]}\ndf = pd.DataFrame(d)\ndf\n\nprint(\"This is the output we want to aggregate to the original df\")\ndf.groupby(\"orderid\")[\"item\"].sum().to_frame()\n\ndf[\"total_items_sold\"] = df.groupby(\"orderid\")[\"item\"].transform(sum)\ndf","e15fd9bd":"# we have empty rows and bad data\ndf = pd.read_csv(\"\/kaggle\/input\/trick58data\/trick58data.csv\")\ndf\n\n# importing correct data\ndf = pd.read_csv(\"\/kaggle\/input\/trick58data\/trick58data.csv\", header = 2, skiprows = [3,4])\ndf","c5e70e2e":"print_files()\n\ndf = pd.read_csv(\"\/kaggle\/input\/imdb-data\/IMDB-Movie-Data.csv\")\ndf\n\ngbdf = df.groupby(\"Genre\")\ngbdf.get_group(\"Horror\")","62564c09":"df = pd.DataFrame({\"A\":[\"Male\", \"Female\", \"Female\", \"Male\"], \"B\":[\"x\", \"y\", \"z\", \"A\"], \"C\":[\"male\", \"female\", \"male\", \"female\"], \"D\":[1, 2, 3, 4]})\ndf\n\n# first let's use applymap to convert to standarize the text\ndf = df.applymap(lambda x: x.lower() if type(x) == str else x)\n\nmapping = {\"male\":0, \"female\":1}\n\nprint(\"PROBLEM: Applies to the whole df but retruns None\")\ndf.applymap(mapping.get)\n\nprint(\"Get the correct result but you have to specify the colums. If you don't want to do this, check the next result\")\ndf[[\"A\", \"C\"]].applymap(mapping.get)\n\nprint(\"Condtional apply map: if can map --> map else return the same value\")\ndf = df.applymap(lambda x: mapping[x] if x in mapping.keys() else x)\ndf","44afdb57":"df = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\")\ndf\n\nprint(\"Classical filter hard to read and mantain.\")\ndf[(df[\"continent\"] == \"Europe\") & (df[\"beer_servings\"] > 150) & (df[\"wine_servings\"] > 50) & (df[\"spirit_servings\"] < 60)]\n\nprint(\"You can split it across multiple lines to make it more readable. But it's still hard to read.\")\ndf[\n    (df[\"continent\"] == \"Europe\") & \n    (df[\"beer_servings\"] > 150) & \n    (df[\"wine_servings\"] > 50) & \n    (df[\"spirit_servings\"] < 60)\n]\n\nprint(\"Solution saving criteria as objects\")\n\ncr1 = df[\"continent\"] == \"Europe\"\ncr2 = df[\"beer_servings\"] > 150\ncr3 = df[\"wine_servings\"] > 50\ncr4 = df[\"spirit_servings\"] < 60\n\ndf[cr1 & cr2 & cr3 & cr4]\n\nprint(\"Solution using reduce\")\nfrom functools import reduce\n\n# creates our criteria usings lambda\n# lambda takes 2 parameters, x and y\n# reduce combines them & for every cr in the (cr1, cr2, cr3, cr4)\ncriteria = reduce(lambda x, y: x & y, (cr1, cr2, cr3, cr4))\ndf[criteria]\n","6a579bd9":"df = generate_sample_data()\ndf[\"A_diff\"] = df[\"A\"].diff() # calculate the difference between 2 rows\ndf[\"A_diff_pct\"] = df[\"A\"].pct_change()*100 # calculates the porcentual variation between 2 rows\n\n# add some style\ndf.style.format({\"A_diff_pct\":'{:.2f}%'})\n","311af2b7":"df = generate_sample_data()\n\ndf.sample(frac = 0.5, random_state = 2)\ndf.sample(frac = 0.5, random_state = 2).reset_index(drop = True) # reset index after shuffeling\n","a470b3d3":"df = generate_sample_data()\n\ndf.plot(kind = \"line\")\ndf.plot(kind = \"bar\")\ndf.plot(kind = \"barh\")\ndf.plot(kind = \"hist\")\ndf.plot(kind = \"box\")\ndf.plot(kind = \"kde\")\ndf.plot(kind = \"area\")\n\n# the following plots requiere x and y\ndf.plot(x = \"A\", y = \"B\", kind = \"scatter\")\ndf.plot(x = \"A\", y = \"B\", kind = \"hexbin\")\ndf.plot(x = \"A\", y = \"B\", kind = \"pie\") # here you can pass only x but you need to add subplots = True\n\n# other plots are available through pd.plotting\n# more about plotting https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/visualization.html","5b9168a0":"print_files()\n\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\n# Solution 1: using str.cat \ndf[\"Name\"].str.cat(df[\"Sex\"], sep = \", \").head()\n\n# using + sign\ndf[\"Name\"] + \", \" + df[\"Sex\"].head()","01cd71ad":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\n# Typical groupby\nprint(\"Problem: MultiIndex\")\ndf.groupby(\"Pclass\").agg({\"Age\":[\"mean\", \"max\"], \"Survived\": \"mean\"})\n\n# Please note that this has been covered in 86 and 86 bis.\n# This is just one more way to do it.\nprint(\"Named aggregation\")\ndf.groupby(\"Pclass\").agg(avg_age = (\"Age\", \"mean\"),\n                        max_age = (\"Age\", \"max\"), \n                        survival_rate = (\"Survived\", \"mean\"))","f0b16df0":"d = {\"A\": [100, 200, 300, 400, 100], \"W\":[10, 5, 0, 3, 8]}\ndf = pd.DataFrame(d)\ndf\n\n# with replacement\ndf.sample(n = 5, replace = True, random_state = 2)\n\n# adding weights\ndf.sample(n = 5, replace = True, random_state = 2, weights = \"W\")\n","7cb1015e":"\ndf = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\")\ndf.head()\ndf.dtypes\n\n# Let's import the country and beer_servings columns, convert them to string and float64 respectevly\n# Import only the first 5 rows and thread 0 as nans\ndf = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\",\n                    usecols=[\"country\", \"beer_servings\"],\n                    dtype={\"country\":\"category\", \"beer_servings\":\"float64\"},\n                    nrows = 5,\n                    na_values = 0.0)\ndf.head()\ndf.dtypes\n\n# more about read_csv on https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html","e081a26e":"d = {\"Team\":[\"FC Barcelona\", \"FC Real Madrid\"], \n    \"Players\":[[\"Ter Stegen\", \"Semedo\", \"Piqu\u00e9\", \"Lenglet\", \"Alba\", \"Rakitic\", \"De Jong\", \"Sergi Roberto\", \"Messi\", \"Su\u00e1rez\", \"Griezmann\"], \\\n               [\"Courtois\", \"Carvajal\", \"Varane\", \"Sergio Ramos\", \"Mendy\", \"Kroos\", \"Valverde\", \"Casemiro\", \"Isco\", \"Benzema\", \"Bale\"]]}\n\nprint(\"Notice that we have a list of players for each team. Let's generate a row for each player.\")\ndf = pd.DataFrame(d)\ndf\n\nprint(\"Using explode to generate new rows for each player.\")\ndf1 = df.explode(\"Players\")\ndf1\n\nprint(\"Reverse this operation with groupby and agg\")\ndf[\"Imploded\"] = df1.groupby(df1.index)[\"Players\"].agg(list)\ndf","94708823":"print(\"Default series\")\nser1 = pd.Series([10, 20])\nser1\n\nprint(\"Let's add a NaN to an int64 series\")\nser1 = pd.Series([10, 20, np.nan])\nser1 # Notice it has been converted to float64\n\nprint(\"But if we use Int64 than everything will work\")\nser1 = pd.Series([10, 20, np.nan], dtype = \"Int64\")\nser1","0b6b85ed":"d = {\"Team\":[\"FC Barcelona\", \"FC Real Madrid\"], \n    \"Players\":[\"Ter Stegen, Semedo, Piqu\u00e9, Lenglet, Alba, Rakitic, De Jong, Sergi Roberto, Messi, Su\u00e1rez, Griezmann\",\n               \"Courtois, Carvajal, Varane, Sergio Ramos, Mendy, Kroos, Valverde, Casemiro, Isco, Benzema, Bale\"]}\n\nprint(\"Notice that we have a list of players for each team separated by commas. Let's generate a row for each player.\")\ndf = pd.DataFrame(d)\ndf\n\nprint(\"Notice that we have converted to something similar seen in example 47.\")\ndf.assign(Players = df[\"Players\"].str.split(\",\"))\n\nprint(\"Now add explode and done.\")\ndf.assign(Players = df[\"Players\"].str.split(\",\")).explode(\"Players\")","330358b8":"df = generate_sample_data()\ndf\n\n# create a local variable mean\nmean = df[\"A\"].mean()\n\n# now let's use in inside a query of pandas using @\ndf.query(\"A > @mean\")\n","4d7839f5":"# It seems that this trick is duplicated, skip to the next one\n# I decided to keep in, so in the future there will be no confusion if you consult the original material\n# and this kernel\nd = {\"Team\":[\"FC Barcelona\", \"FC Real Madrid\"], \n    \"Players\":[[\"Ter Stegen\", \"Semedo\", \"Piqu\u00e9\", \"Lenglet\", \"Alba\", \"Rakitic\", \"De Jong\", \"Sergi Roberto\", \"Messi\", \"Su\u00e1rez\", \"Griezmann\"], \\\n               [\"Courtois\", \"Carvajal\", \"Varane\", \"Sergio Ramos\", \"Mendy\", \"Kroos\", \"Valverde\", \"Casemiro\", \"Isco\", \"Benzema\", \"Bale\"]]}\n\nprint(\"Notice that we have a list of players for each team. Let's generate a row for each player.\")\ndf = pd.DataFrame(d)\ndf\n\nprint(\"Using explode to generate new rows for each player.\")\ndf1 = df.explode(\"Players\")\ndf1\n\nprint(\"Reverse this operation with groupby and agg\")\ndf[\"Imploded\"] = df1.groupby(df1.index)[\"Players\"].agg(list)\ndf","c99a4d5a":"d = {\"patient\":[1, 2, 3, 1, 1, 2], \"visit\":[2015, 2016, 2014, 2016, 2017, 2020]}\ndf = pd.DataFrame(d)\ndf.sort_values(\"visit\")\n\nprint(\"Let's get the last visit for each patient\")\ndf.groupby(\"patient\")[\"visit\"].last().to_frame()","6195f6d2":"import pandas as pd\nfrom pandas.api.types import CategoricalDtype\nd = {\"ID\":[100, 101, 102, 103], \"quality\":[\"bad\", \"very good\", \"good\", \"excellent\"]}\ndf = pd.DataFrame(d)\ndf\n\nprint(\"Let's create our own categorical order.\")\ncat_type = CategoricalDtype([\"bad\", \"good\", \"very good\", \"excellent\"], ordered = True)\ndf[\"quality\"] = df[\"quality\"].astype(cat_type)\ndf\n\nprint(\"Now we can use logical sorting.\")\ndf = df.sort_values(\"quality\", ascending = True)\ndf\n\nprint(\"We can also filter this as if they are numbers. AMAZING.\")\ndf[df[\"quality\"] > \"bad\"]","d854eadc":"df = generate_sample_data()\nprint(\"Original df\")\ndf\n\ndf.style.hide_index().set_caption(\"Styled df with no index and a caption\")","7f03e17b":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", usecols = [2, 4, 5, 11], nrows = 10)\ndf\n\npd.get_dummies(df) # Notice that we can eliminate one column of each since this information is contained in the others\n\npd.get_dummies(df, drop_first=True)","fd48f2a6":"df = generate_sample_data_datetime().reset_index()\ndf = df.sample(500)\ndf[\"Year\"] = df[\"index\"].dt.year\ndf[\"Month\"] = df[\"index\"].dt.month\ndf[\"Day\"] = df[\"index\"].dt.day\ndf[\"Hour\"] = df[\"index\"].dt.hour\ndf[\"Minute\"] = df[\"index\"].dt.minute\ndf[\"Second\"] = df[\"index\"].dt.second\ndf[\"Nanosecond\"] = df[\"index\"].dt.nanosecond\ndf[\"Date\"] = df[\"index\"].dt.date\ndf[\"Time\"] = df[\"index\"].dt.time\ndf[\"Time_Time_Zone\"] = df[\"index\"].dt.timetz\ndf[\"Day_Of_Year\"] = df[\"index\"].dt.dayofyear\ndf[\"Week_Of_Year\"] = df[\"index\"].dt.weekofyear\ndf[\"Week\"] = df[\"index\"].dt.week\ndf[\"Day_Of_week\"] = df[\"index\"].dt.dayofweek\ndf[\"Week_Day\"] = df[\"index\"].dt.weekday\ndf[\"Week_Day_Name\"] = df[\"index\"].dt.weekday_name\ndf[\"Quarter\"] = df[\"index\"].dt.quarter\ndf[\"Days_In_Month\"] = df[\"index\"].dt.days_in_month\ndf[\"Is_Month_Start\"] = df[\"index\"].dt.is_month_start\ndf[\"Is_Month_End\"] = df[\"index\"].dt.is_month_end\ndf[\"Is_Quarter_Start\"] = df[\"index\"].dt.is_quarter_start\ndf[\"Is_Quarter_End\"] = df[\"index\"].dt.is_quarter_end\ndf[\"Is_Leap_Year\"] = df[\"index\"].dt.is_leap_year\ndf","261143db":"df = generate_sample_data()\ndf\n\n# using loc --> labels\ndf.loc[0, \"A\"]\n\n# using iloc --> position\ndf.iloc[0, 0]\n\n# mixing labels and position with loc\ndf.loc[0, df.columns[0]]\n\n# mixing labels and position with loc\ndf.loc[df.index[0], \"A\"]\n\n# mixing labels and position with iloc\ndf.iloc[0, df.columns.get_loc(\"A\")]\n\n# mixing labels and position with iloc\ndf.iloc[df.index.get_loc(0), 0]","54becbe0":"s = pd.Series(range(1552194000, 1552212001, 3600))\ns = pd.to_datetime(s, unit = \"s\")\ns\n\n# set timezome to current time zone (UTC)\ns = s.dt.tz_localize(\"UTC\")\ns\n\n# set timezome to another time zone (Chicago)\ns = s.dt.tz_convert(\"America\/Chicago\")\ns","ba3cd664":"d = {\"colum_without_space\":np.array([1, 2, 3, 4, 5, 6]), \"column with space\":np.array([1, 2, 3, 4, 5, 6])*2}\ndf = pd.DataFrame(d)\ndf\n\nprint(\"Query a column without space\")\ndf.query(\"colum_without_space > 4\")\nprint(\"Query a column with space using backticks ``\")\nprint(\"This is a backtick ``\")\ndf.query(\"`column with space` > 8\")","445cbadf":"import pandas_profiling\n\ndf = generate_sample_data()\n\ndf\n\nprint(\"Generating report with pandas profiling\")\ndf.profile_report()\n","8f5ffd36":"# use pd.describe_option() to see all\n# max_rows\n# max_columns\n# max_colwidth\n# precision\n# date_dayfirst\n# date_yearfirst\n\ndf = generate_sample_data_datetime()[:10].reset_index()\ndf[\"sales\"] = df[\"sales\"].astype(\"float\")\ndf\n\npd.set_option(\"display.max_rows\",5)\npd.set_option(\"display.max_columns\",3)\npd.set_option('display.width', 1000)\npd.set_option('display.date_dayfirst', True)\npd.describe_option()\n\npd.reset_option('^display.', silent=True) # restore to default\n#pd.reset_option('display.width') # restore one by one","88fed63c":"df = generate_sample_data()[:10]\ndf[\"A\"] = pd.Series([\"APP\", \"GOO\", \"APP\", \"GOO\", \"MIC\", \"MIC\", \"APP\", \"GOO\", \"MIC\", \"APP\"])\ndf.rename(columns = {\"A\":\"stock\"}, inplace = True)\nprint(\"Original df\")\ndf\n\nprint(\"Filter data using intermediate variables\")\ntemp = df.groupby(\"stock\").mean()\ntemp \n\nfv = temp[\"B\"].sort_values(ascending = False)[1] # filter by the second greates. This way every time we generate sample data we will have a result\ntemp[temp[\"B\"] < fv]\n\nprint(\"Filter using query\")\ndf.groupby(\"stock\").mean().query(\"B < {}\".format(fv))\ndf.groupby(\"stock\").mean().query(\"B < @fv\")\ndf.groupby(\"stock\").mean().query(\"B < 10\")","3413e19a":"pd.reset_option('^display.', silent=True) # restore to default\n\ndf = generate_sample_data()\ndf1 = df.copy(deep = True)\ndf = df.append(df1)\n\nprint(\"Imagine we have a big df where we can see all the columns ...\")\ndf.T.head() # we are trasposing JUST TO CREATE A GIANT DF\n\n# Solution 1\nprint(\"Solution 1 using pd.set_option display.max_columns\")\npd.set_option(\"display.max_columns\", None)\ndf.T.head()\npd.reset_option('^display.', silent=True) # restore to default\n\n# Solution 2\nprint(\"Another clever solution using Traspose\")\ndf.T.head().T","298a6bfa":"df = generate_sample_data()\ndf1 = df.copy(deep = True)\ndf1 = df1.drop([0, 1, 2], axis = \"rows\") # drop some index just to see the example workings\ndf.head()\ndf1.head()\n\npd.merge(df, df1, how = \"left\", indicator = True)\n","75c8b2f2":"# Pandas is built upon numpy, so we can acess all numpy functionality from pandas\npd.np.random.rand(2, 3)\npd.np.nan","22dfa484":"df = pd.read_csv(\"\/kaggle\/input\/drinks-by-country\/drinksbycountry.csv\")\nprint(\"Original df\")\ndf\n\nprint(\"Groupby continent beer_servings\")\ndf.groupby(\"continent\")[\"beer_servings\"].mean()\n\nprint(\"Using agg to pass multiple functions\")\ndf.groupby(\"continent\")[\"beer_servings\"].agg([\"mean\", \"count\"])\n\nprint(\"Using describe over a groupby object\")\ndf.groupby(\"continent\")[\"beer_servings\"].describe()","3f9f0a66":"df = generate_sample_data_datetime()\n\nprint(\"Original df\")\ndf\nprint(\"Let's resample\/groupby by month\")\ndf.resample(\"M\")[\"sales\"].sum()\n\nprint(\"Let's resample\/groupby by day\")\ndf.resample(\"D\")[\"sales\"].sum()","9266d706":"df = generate_sample_data_datetime().reset_index()[:10]\ndf.rename(columns = {\"index\":\"time\"}, inplace = True)\ndf[\"sales_100\"] = df[\"sales\"]*100\nprint(\"Original df\")\ndf.head()\n\n# declare a formatting dict: individual for each column\nfd = {\"time\":\"{:%d\/%m\/%y}\", \"sales\":\"${:.2f}\", \"customers\":\"{:,}\"}\ndf.style.format(fd)\ndf\n\n# add some more formattin\n(df.style.format(fd)\n .hide_index()\n .highlight_min(\"sales\", color =\"red\")\n .highlight_max(\"sales\", color =\"green\")\n .background_gradient(subset = \"sales_100\", cmap =\"Blues\")\n .bar(\"customers\", color = \"lightblue\", align = \"zero\")\n .set_caption(\"A df with different stylings\")\n)","2a5f605e":"df = generate_sample_data()\ndf.head(2)\n\n# Solution 1\ndf.rename({\"A\":\"col_1\", \"B\":\"col_2\"}, axis = \"columns\", inplace = True)\ndf.head(2)\n\n# Solution 2\ndf.columns = [\"col1\", \"col2\", \"col3\", \"col4\",\"col5\", \"col6\", \"col7\"] # list must be equal to the columns number\ndf.head(2)\n\n# Solution 3\ndf.columns = df.columns.str.title() # apply any string method to the columns names\ndf.head(2)","477c3859":"# You will have to check this on your local machine\n# Useful for fast importing\n# Step 1: copy a table from excel sheet using ctrl + c (to the clipboard)\n# Step 2: run this command\n# df = pd.read_clipboard()","acdbf153":"d = {\"col1\":[100, 120 ,140, np.nan, 160], \"col2\":[9, 10, np.nan, 7.5, 6.5]}\ndf = pd.DataFrame(d)\ndf.index = pd.util.testing.makeDateIndex()[0:5]\nprint(\"Original df\")\ndf\nprint(\"DataFrame after interpolate\")\ndf.interpolate()","266fd903":"print(\"Contains random values\")\ndf1 = pd.util.testing.makeDataFrame() # contains random values\ndf1\nprint(\"Contains missing values\")\ndf2 = pd.util.testing.makeMissingDataframe() # contains missing values\ndf2\nprint(\"Contains datetime values\")\ndf3 = pd.util.testing.makeTimeDataFrame() # contains datetime values\ndf3\nprint(\"Contains mixed values\")\ndf4 = pd.util.testing.makeMixedDataFrame() # contains mixed values\ndf4","22ed085b":"d = {\"name\":[\"John Artur Doe\", \"Jane Ann Smith\", \"Nico P\"], \"location\":[\"Los Angeles, CA\", \"Washington, DC\", \"Barcelona, Spain\"]}\ndf = pd.DataFrame(d)\ndf\n\ndf[[\"first\", \"middle\", \"last\"]] = df[\"name\"].str.split(\" \", expand = True)\ndf[\"city\"] = df[\"location\"].str.split(\",\", expand = True)[0]\ndf","5e67145b":"d = {\"day\":[1, 2, 10 ,25, 12], \"month\":[1, 2, 4, 5, 6], \"year\":[2000, 2001, 2010, 2015, 2020]}\ndf = pd.DataFrame(d)\ndf[\"date\"] = pd.to_datetime(df[[\"day\", \"month\", \"year\"]])\ndf\ndf.dtypes","68d1d476":"df = generate_sample_data_datetime().reset_index()\ndf.columns = [\"date\", \"sales\", \"customers\"]\ndf\n\nprint(\"Show the global usage of memory of the df\")\ndf.info(memory_usage = \"deep\")\nprint()\nprint(\"Show the usage of memory of every column\")\ndf.memory_usage(deep = True)","9b7185d8":"df = generate_sample_data()\ndf.head()\n\nprint(\"Writing data to a csv.zip file\")\ndf.to_csv(\"trick18data.csv.zip\")\n\nprint(\"Deleting df\")\ndel df\n\nprint(\"Importing data from a csv.zip file\")\ndf = pd.read_csv(\"\/kaggle\/working\/trick18data.csv.zip\", index_col=0)\ndf.head()\n\n# other compression files supported .gz, .bz2, .xz","047df857":"df = generate_sample_data()\nprint(\"Original df\")\ndf\n\nprint(\"Using a slice (inclusive)\")\ndf.loc[0:4, \"A\":\"E\"]\n\nprint(\"Using a list\")\ndf.loc[[0,4], [\"A\",\"E\"]]\n\nprint(\"Using a condition\")\ndf.loc[df[\"A\"] > 10, [\"A\",\"E\"]]","0e382140":"df = generate_sample_data()\ndf[\"A\"] = df[\"A\"] + 5\ndf.rename(columns = {\"A\":\"age\"}, inplace = True)\ndf.sample(5)\n\ndf[\"age_groups\"] = pd.cut(df[\"age\"], bins = [0, 18, 65, 99], labels = [\"kids\", \"adult\", \"elderly\"])\ndf","e80f8926":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nprint(\"Original df\")\ndf.head()\n\nprint(\"Groupby and create a MultiIndex df\")\nprint(\"Notice we have a df with MultiIndex (Sex and Pclass)\")\ndf.groupby([\"Sex\", \"Pclass\"])[\"Survived\"].mean().to_frame()\n\nprint(\"Reshaping using unstack\")\nprint(\"Now we can interact with it like with a normal df\")\ndf.groupby([\"Sex\", \"Pclass\"])[\"Survived\"].mean().unstack()","88d98a19":"# Method 1: from a dict\npd.DataFrame({\"A\":[10 ,20], \"B\":[30, 40]})\n\n# Method 2: using numpy\npd.DataFrame(np.random.rand(2, 3), columns = list(\"ABC\"))\n\n# Method 3: using pandas builtin functionalities\npd.util.testing.makeMixedDataFrame()","e9441b1d":"d = {\"A\":[1, 2, 3], \"B\":[[10, 20], [40, 50], [60, 70]]}\ndf = pd.DataFrame(d)\nprint(\"Notice that the column B has as values lists\")\ndf\nprint(\"Convert it to normal series\")\ndf_ = df[\"B\"].apply(pd.Series)\ndf_\n\nprint(\"Join the 2 df\")\npd.merge(df, df_, left_index = True, right_index = True)\n","eb5e50f1":"df = generate_sample_data()[:10]\ndf1 = df.copy(deep = True)\ndf = df.drop([0, 1, 2])\ndf1 = df1.drop([8, 9])\ndf\ndf1\n\ndf_one_to_one = pd.merge(df, df1, validate = \"one_to_one\")\ndf_one_to_one\n\ndf_one_to_many = pd.merge(df, df1, validate = \"one_to_many\")\ndf_one_to_many\n\ndf_many_to_one = pd.merge(df, df1, validate = \"many_to_one\")\ndf_many_to_one\n","7792842f":"print_files()\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.columns = [\"Passenger ID\", \"Survived\", \"Pclass\", \"Name         \", \"Sex\", \"Age\", \"Sib SP\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"] # creating column names for the example\ndf\ndf1 = df.copy(deep = True)\n\nprint(\"Replace all spaces with undescore and convert to lower\")\nprint(\"Notice the Passenger and Sib SP column now has underscore\")\ndf.columns = df.columns.str.replace(\" \", \"_\").str.lower()\ndf.head()\n\nprint(\"Remove trailing (at the end) whitesapce and convert to lower\")\nprint(\"Notice the Passenger and Sib SP column now has underscore\")\ndf1.columns = df1.columns.str.lower().str.rstrip()\ndf1.head()","da9a916f":"df = generate_sample_data()[[\"A\", \"B\"]][:5]\ndf[\"A\"] = pd.Series([15, 15, 18, np.nan, 12])\ndf[\"B\"] = pd.Series([15, 15, 18, np.nan, 12])\ndf\n\nprint(\"Don't use ==, it does not handle NaN properly\")\nprint(\"Notice that element 4 of each list is np.nan but == still returns False\")\ndf[\"A\"] == df[\"B\"]\n\nprint(\"Using equals. Now we get True, so the 2 series are equal\")\ndf[\"A\"].equals(df[\"B\"])\n\nprint(\"Equals also works for df\")\ndf1 = df.copy(deep = True)\ndf.equals(df1)\n\nprint(\"== of df has the same issue as for series\")\ndf == df1","79bedc3e":"print_files()\n\ndf = pd.read_csv(\"\/kaggle\/input\/imdb-data\/IMDB-Movie-Data.csv\", \\\n                 usecols = [\"Title\", \"Genre\", \"Year\", \"Metascore\", \"Revenue (Millions)\"])\ndf.dtypes\ndf.memory_usage(deep = True)\n\nprint(\"Importing only a few columns and converting to proper dtype\")\ndf = pd.read_csv(\"\/kaggle\/input\/imdb-data\/IMDB-Movie-Data.csv\", \\\n                 usecols = [\"Title\", \"Genre\", \"Year\", \"Metascore\", \"Revenue (Millions)\"], \\\n                dtype = {\"Genre\":\"category\", \"Metascore\":\"Int64\", \"Year\":\"int8\"})\ndf.dtypes\ndf.memory_usage(deep = True) # notice how Genre and Year are consuming now less memory","f5511397":"# let's generate some fake data\ndf1 = generate_sample_data()\ndf2 = generate_sample_data()\ndf3 = generate_sample_data()\n# df1.head()\n# df2.head()\n# df3.head()\ndf1.to_csv(\"trick8data1.csv\", index = False)\ndf2.to_csv(\"trick8data2.csv\", index = False)\ndf3.to_csv(\"trick8data3.csv\", index = False)\n\n# Step 1 generate list with the file name\nlf = []\nfor _,_, files in os.walk(\"\/kaggle\/working\/\"):\n    for f in files:\n        if \"trick8data\" in f:\n            lf.append(f)\n            \nlf\n\n# You can use this on your local machine\n#from glob import glob\n#files = glob(\"trick8.csv\")\n\n# Step 2: we do the same as in trick 78 except we don't create a new column of the rows origin (file they came from)\ndf = pd.concat((pd.read_csv(file) for file in lf), ignore_index = True)\ndf","9236e48e":"df = pd.util.testing.makeMissingDataframe().reset_index() # contains missing values\ndf.rename(columns = {\"index\":\"A\"})\ndf1 = df.copy(deep = True)\ndf\n\nprint(\"Calculate the % of missing values in each row\")\ndf.isna().mean() # calculate the % of missing values in each row\nprint(\"Droping any columns that have missing values. Only column A wil remain\")\ndf.dropna(axis = \"columns\") # drop any column that has missing values\nprint(\"Droping any rows that have missing values.\")\ndf1.dropna(axis = \"rows\") # drop any row that has missing values\nprint(\"Droping column where missing values are above a threshold\")\ndf.dropna(thresh = len(df)*0.95, axis = \"columns\") # drop any row that has missing values","b514f8a4":"df = generate_sample_data()\ndf_1 = df.sample(frac = 0.7)\ndf_2 = df.drop(df_1.index) # only works if the df index is unique\n\ndf.shape\ndf_1.shape\ndf_2.shape","fef8fac2":"d = {\"col1\":[\"1\", \"2\", \"3\", \"stuff\"], \"col2\":[\"1\", \"2\", \"3\", \"4\"]}\ndf = pd.DataFrame(d)\ndf.astype({\"col2\":\"int\"}) # this will fail for col1 --> ValueError: invalid literal for int() with base 10: 'stuff'\n\nprint(\"Notice that now stuff got converted to NaN\")\ndf.apply(pd.to_numeric, errors = \"coerce\")","2102a46b":"df = generate_sample_data_datetime()[:10].reset_index()\ndf[\"string_col\"] = list(\"ABCDEABCDE\")\ndf[\"sales\"] = df[\"sales\"].astype(\"float\")\nprint(\"Original df\")\ndf\n\nprint(\"Select numerical columns\")\ndf.select_dtypes(include = \"number\")\n\nprint(\"Select string columns\")\ndf.select_dtypes(include = \"object\")\n\nprint(\"Select datetime columns\")\ndf.select_dtypes(include = [\"datetime\", \"timedelta\"])\n\nprint(\"Select miscelaneous\")\ndf.select_dtypes(include = [\"number\", \"object\", \"datetime\", \"timedelta\"])\n\nprint(\"Select by passing the dtypes you need\")\ndf.select_dtypes(include = [\"int8\", \"int16\", \"int32\", \"int64\", \"float\"])","53c47e59":"df = generate_sample_data()[:5]\ndf[\"A\"] = [1, 2, 3, 4, 5]\n\nprint(\"Filter using multiple |\")\ndf[(df[\"A\"] == 1) | (df[\"A\"] == 3)]\n\nprint(\"Filter using isin\")\ndf[df[\"A\"].isin([1, 3])]\n\nprint(\"Invert using ~ (ctrl + alt + 4)\")\ndf[~df[\"A\"].isin([1, 3])]","edef38be":"df = generate_sample_data()[:5]\ndf\n\nprint(\"Reverse column order\")\ndf.loc[:, ::-1]\n\nprint(\"Reverse row order\")\ndf.loc[::-1]\n\nprint(\"Reverse row order and reset index\")\ndf.loc[::-1].reset_index(drop = True)","91289556":"df = generate_sample_data()[:5]\nprint(\"Original df\")\ndf\n\nprint(\"Add prefix\")\ndf.add_prefix(\"1_\")\n\nprint(\"Add suffix\")\ndf.add_suffix(\"_Z\")\n","f042a74b":"<a id = \"Imports\"><\/a>\n# Importing libraries and setting some helper functions\n[Go back to the Table of Contents](#table_of_contents)","f6d9aacf":"<a id = \"trick89\"><\/a>\n# Trick 89: Split names into first and last name\n[Go back to the Table of Contents](#table_of_contents)","49117a9a":"<a id = \"trick56\"><\/a>\n# Trick 56: Apply a mappings or functions to the whole df (applymap)\n[Go back to the Table of Contents](#table_of_contents)","1300b2c5":"<a id = \"trick15\"><\/a>\n# Trick 15: Reshape a MultiIndex df (unstack())\n[Go back to the Table of Contents](#table_of_contents)","09764e2b":"<a id = \"trick50\"><\/a>\n# Trick 50: Named aggregation with multiple columns passing tupples (new in pandas 0.25)\n[Go back to the Table of Contents](#table_of_contents)","e22a0d54":"<a id = \"trick88\"><\/a>\n# Trick 88: Rearrange columns in a df\n[Go back to the Table of Contents](#table_of_contents)","ffd03027":"<a id = \"trick34\"><\/a>\n# Trick 34: Explore a dataset with profiling\n[Go back to the Table of Contents](#table_of_contents)","123abb4d":"<a id = \"trick12\"><\/a>\n# Trick 12: Merging datasets and check uniqueness\n[Go back to the Table of Contents](#table_of_contents)","024674d2":"<a id = \"trick40\"><\/a>\n# Trick 40: Style you df fast with hide_index() and set_caption()\n[Go back to the Table of Contents](#table_of_contents)","2685cd71":"<a id = \"trick83\"><\/a>\n# Trick 83: Correct the data types while importing the df\n[Go back to the Table of Contents](#table_of_contents)","a1b2d151":"<a id = \"trick27\"><\/a>\n# Trick 27: Aggregation over timeseries (resample)\n[Go back to the Table of Contents](#table_of_contents)","35951dc8":"<a id = \"trick96\"><\/a>\n# Trick 96: Interactive plots out of the box in pandas\n[Go back to the Table of Contents](#table_of_contents)","a4ae94bd":"<a id = \"trick72\"><\/a>\n# Trick 72: Convert continuos variable to categorical (cut and qcut)\n[Go back to the Table of Contents](#table_of_contents)","78b7c661":"<a id = \"trick90\"><\/a>\n# Trick 90: Moving columns to a specific location\n[Go back to the Table of Contents](#table_of_contents)","15b18e74":"<a id = \"trick47\"><\/a>\n# Trick 47: Create one row for each item in a list (explode)\n[Go back to the Table of Contents](#table_of_contents)","8a685722":"<a id = \"trick17\"><\/a>\n# Trick 17: Select multiple rows and columns with loc\n[Go back to the Table of Contents](#table_of_contents)","3204cc92":"<a id = \"trick57\"><\/a>\n# Trick 57: Accesing the groups of a groupby object (get_group())\n[Go back to the Table of Contents](#table_of_contents)","48ea1e49":"<a id = \"trick20\"><\/a>\n# Trick 20: Create a datetime columns from multiple columns\n[Go back to the Table of Contents](#table_of_contents)","45585f36":"<a id = \"trick91\"><\/a>\n# Trick 91: Creating a time series dataset for testing\n[Go back to the Table of Contents](#table_of_contents)","7bd4a22e":"<a id = \"trick1\"><\/a>\n# Trick 1: Add a prefix or suffix to all columns\n[Go back to the Table of Contents](#table_of_contents)","934b8919":"<a id = \"trick84\"><\/a>\n# Trick 84: Show fewer rows in a df\n[Go back to the Table of Contents](#table_of_contents)","33f8108e":"<a id = \"trick86bis\"><\/a>\n# Trick 86bis: Named aggregations on multiple columns- avoids multiindex\n[Go back to the Table of Contents](#table_of_contents)","6efc6130":"<a id = \"trick85\"><\/a>\n# Trick 85: Convert one type of values to others\n[Go back to the Table of Contents](#table_of_contents)","f119a122":"<a id = \"trick3\"><\/a>\n# Trick 3: Filter a df by multiple conditions (isin and inverse using ~)\n[Go back to the Table of Contents](#table_of_contents)","4f26540f":"<a id = \"trick92\"><\/a>\n# Trick 92: Clean Object column with mixed data using regex\n[Go back to the Table of Contents](#table_of_contents)","3cd26927":"<a id = \"trick31\"><\/a>\n# Trick 31: See all the columns of a big df\n[Go back to the Table of Contents](#table_of_contents)","6ff44f39":"<a id = \"trick58\"><\/a>\n# Trick 58: Use header and skiprows to get rid of bad data or empty rows while importing\n[Go back to the Table of Contents](#table_of_contents)","a10fedd3":"<a id = \"trick76\"><\/a>\n# Trick 76: Filter in pandas only the largest categories.\n[Go back to the Table of Contents](#table_of_contents)","b77f18d1":"<a id = \"trick71\"><\/a>\n# Trick 71: Read data from a PDF (tabula py)\n[Go back to the Table of Contents](#table_of_contents)","2d58be2c":"<a id = \"trick18\"><\/a>\n# Trick 18: Read and write to a compressed file (csv.zip)\n[Go back to the Table of Contents](#table_of_contents)","f66548e0":"<a id = \"trick32\"><\/a>\n# Trick 32: Filter a df with query and avoid intermediate variables\n[Go back to the Table of Contents](#table_of_contents)","48dc1f96":"<a id = \"trick29\"><\/a>\n# Trick 29: Access numpy within pandas (without importing numpy as np)\n[Go back to the Table of Contents](#table_of_contents)","c20ab4c9":"<a id = \"trick36\"><\/a>\n# Trick 36: Convert from UTC to another timezone\n[Go back to the Table of Contents](#table_of_contents)","235c0992":"<a id = \"trick78\"><\/a>\n# Trick 78: Keep track of where your data is coming when you are using multiple sources\n[Go back to the Table of Contents](#table_of_contents)","ccad704b":"<a id = \"trick54\"><\/a>\n# Trick 54: Calculate the difference between each row and the previous (diff())\n[Go back to the Table of Contents](#table_of_contents)","74956a1e":"<a id = \"trick79\"><\/a>\n# Trick 79: Count of rows that match a condition\n[Go back to the Table of Contents](#table_of_contents)","93ceaec4":"<a id = \"trick28\"><\/a>\n# Trick 28: Aggregating by multiple columns (using agg)\n[Go back to the Table of Contents](#table_of_contents)","bcbd81f0":"<a id = \"trick39\"><\/a>\n# Trick 39: One hot encoding (get_dummies())\n[Go back to the Table of Contents](#table_of_contents)","f844b4b7":"<a id = \"trick49\"><\/a>\n# Trick 49: Sampling with pandas (with replacement and weights)\n[Go back to the Table of Contents](#table_of_contents)","3fa68510":"<a id = \"trick24\"><\/a>\n# Trick 24: Copy data from Excel into pandas quick (read_clipboard())\n[Go back to the Table of Contents](#table_of_contents)","f7ec30eb":"<a id = \"trick63\"><\/a>\n# Trick 63: Calculate running count with groups using cumcount() + 1\n[Go back to the Table of Contents](#table_of_contents)","af12633d":"<a id = \"trick52\"><\/a>\n# Trick 52: Making plots with pandas\n[Go back to the Table of Contents](#table_of_contents)","7da2cbf4":"<a id = \"trick14\"><\/a>\n# Trick 14: Creating toy df (3 methods)\n[Go back to the Table of Contents](#table_of_contents)","0d225914":"<a id = \"trick5\"><\/a>\n# Trick 5: Convert numbers stored as strings (coerce)\n[Go back to the Table of Contents](#table_of_contents)","4127c0dd":"<a id = \"trick8\"><\/a>\n# Trick 8: Using glob to generate a df from multiple files !!!duplicated Trick 78!!!\n[Go back to the Table of Contents](#table_of_contents)","f7b0b7dc":"<a id = \"trick86\"><\/a>\n# Trick 86: Named aggregations - avoids multiindex\n[Go back to the Table of Contents](#table_of_contents)","7edc3b31":"<a id = \"trick93\"><\/a>\n# Trick 93: Combine the small categories into a single category named \"Others\" (using frequencies)\n[Go back to the Table of Contents](#table_of_contents)","842da802":"<a id = \"trick11\"><\/a>\n# Trick 11: Rename all columns with the same pattern\n[Go back to the Table of Contents](#table_of_contents)","8d677069":"<a id = \"trick60\"><\/a>\n# Trick 60: Creating running totals with cumsum function\n[Go back to the Table of Contents](#table_of_contents)","8719aaa8":"<a id = \"trick9\"><\/a>\n# Trick 9: Reduce memory usage of a df while importing !!!duplicated Trick 83!!!\n[Go back to the Table of Contents](#table_of_contents)","97883ac3":"<a id = \"trick43\"><\/a>\n# Trick 43: Create one row for each item in a list (explode) !!!duplicated Trick 47!!!\n[Go back to the Table of Contents](#table_of_contents)","5d4b1658":"<a id = \"trick70\"><\/a>\n# Trick 70: Print current version of pandas and it's dependencies\n[Go back to the Table of Contents](#table_of_contents)","3cbb658b":"<a id = \"trick2\"><\/a>\n# Trick 2: Reverse order of a df\n[Go back to the Table of Contents](#table_of_contents)","f0f78f3d":"<a id = \"trick87\"><\/a>\n# Trick 87: Aggregate you datetime by by and filter weekends\n[Go back to the Table of Contents](#table_of_contents)","3baa384e":"<a id = \"trick38\"><\/a>\n# Trick 38: Pandas datetime (lot's of examples)\n[Go back to the Table of Contents](#table_of_contents)","285c3c0f":"<a id = \"trick80\"><\/a>\n# Trick 80: Select multiple slices of columns from a df\n[Go back to the Table of Contents](#table_of_contents)","7b93224c":"<a id = \"trick100\"><\/a>\n# Trick 100: Loading sample of big data\n[Go back to the Table of Contents](#table_of_contents)","b7726475":"<a id = \"trick81\"><\/a>\n# Trick 81: Use apply(type) to see if you have mixed data types\n[Go back to the Table of Contents](#table_of_contents)","dd51ef24":"<a id = \"trick94\"><\/a>\n# Trick 94: Save memory by fixing your date\n[Go back to the Table of Contents](#table_of_contents)","e27ba264":"<a id = \"trick19\"><\/a>\n# Trick 19: Show memory usage of a df and every column\n[Go back to the Table of Contents](#table_of_contents)","8389ce2c":"<a id = \"trick97\"><\/a>\n# Trick 97: Convert year and day of year into a single datetime column\n[Go back to the Table of Contents](#table_of_contents)","fef7f648":"<a id = \"trick48\"><\/a>\n# Trick 48: Useful parameters when using pd.read_csv()\n[Go back to the Table of Contents](#table_of_contents)","444a4b5e":"<a id = \"trick7\"><\/a>\n# Trick 7: Dealing with missing values (NaN)\n[Go back to the Table of Contents](#table_of_contents)","fdaa0e39":"<a id = \"trick69\"><\/a>\n# Trick 69: Check if 2 series are \"similar\"\n[Go back to the Table of Contents](#table_of_contents)","b80bea3e":"<a id = \"trick77\"><\/a>\n# Trick 77: Combine the small categories into a single category named \"Others\" (using where)\n[Go back to the Table of Contents](#table_of_contents)","52d84095":"<a id = \"trick51\"><\/a>\n# Trick 51: Concatenate 2 column strings\n[Go back to the Table of Contents](#table_of_contents)","5141b3ff":"<a id = \"trick23\"><\/a>\n# Trick 23: Fill missing values in time series data (interpolate())\n[Go back to the Table of Contents](#table_of_contents)","02b60a6e":"<a id = \"trick16\"><\/a>\n# Trick 16: Convert continuos values to categorical (cut())\n[Go back to the Table of Contents](#table_of_contents)","bb84aa31":"<a id = \"trick13\"><\/a>\n# Trick 13: Avoid the series of lists TRAP\n[Go back to the Table of Contents](#table_of_contents)","2e9beefc":"<a id = \"trick21\"><\/a>\n# Trick 21: Split a string column into multiple columns\n[Go back to the Table of Contents](#table_of_contents)","1c13017a":"<a id = \"trick46\"><\/a>\n# Trick 46: Store NaN in an integer type with Int64 (not int64)\n[Go back to the Table of Contents](#table_of_contents)","95ede74c":"<a id = \"table_of_contents\"><\/a>\n# <font size=\"+3\" color=red >Table of contents<\/font>\n\n[Importing libraries and setting some helper functions](#Imports)\n\n[Trick 100: Loading sample of a big data file](#trick100)\n\n[Trick 99: How to avoid Unnamed: 0 columns](#trick99)\n\n[Trick 98: Convert a wide DF into a long one](#trick98)\n\n[Trick 97: Convert year and day of year into a single datetime column](#trick97)\n\n[Trick 96: Interactive plots out of the box in pandas](#trick96)\n\n[Trick 95: Count the missing values](#trick95)\n\n[Trick 94: Save memory by fixing your datetypes](#trick94)\n\n[Trick 93: Combine the small categories into a single category named \"Others\" (using frequencies)](#trick93)\n\n[Trick 92: Clean Object column with mixed data using regex](#trick92)\n\n[Trick 91: Creating a time series dataset for testing](#trick91)\n\n[Trick 90: Moving columns to a specific location](#trick90)\n\n[Trick 89: Split names into first and last name](#trick89)\n\n[Trick 88: Rearange columns in a DF](#trick88)\n\n[Trick 87: Aggregate you datetime by by and filter weekends](#trick87)\n\n[Trick 86: Named aggregations - avoids multiindex](#trick86)\n\n[Trick 86bis: Named aggregations on multiple columns- avoids multiindex](#trick86bis)\n\n[Trick 85: Convert one type of values to others](#trick85)\n\n[Trick 84: Show fewer rows in a df](#trick84)\n\n[Trick 83: Correct the data types while importing the df](#trick83)\n\n[Trick 82: Select data by label and position (chained iloc and loc)](#trick82)\n\n[Trick 81: Use apply(type) to see if you have mixed data types](#trick81)\n\n[Trick 80: Select multiple slices of columns from a df](#trick80)\n\n[Trick 79: Count of rows that match a condition](#trick79)\n\n[Trick 78: Keep track of where your data is coming when you are using multiple sources](#trick78)\n\n[Trick 77: Combine the small categories into a single category named \"Others\" (using where)](#trick77)\n\n[Trick 76: Filter in pandas only the largest categories.](#trick76)\n\n[Trick 75: Count the number of words in a pandas series](#trick75)\n\n[Trick 74: Webscraping using read_html() and match parameter](#trick74)\n\n[Trick 73: Remove a column and store it as a separate series](#trick73)\n\n[Trick 72: Convert continuos variable to categorical](#trick72)\n\n[Trick 71: Read data from a PDF (tabula py)](#trick71)\n\n[Trick 70: Print current version of pandas and it's dependencies](#trick70)\n\n[Trick 69: Check if 2 series are \"similar\"](#trick69)\n\n[Trick 68: Webscraping using read_html()](#trick68)\n\n[Trick 67: Create new columns or overwrite using assing](#trick67)\n\n[Trick 66: Create a bunch of new columns using a for loop and f-strings df[f'{col}_new']](#trick66)\n\n[Trick 65: Select columns using f-strings (new in pandas 3.6+)](#trick65)\n\n[Trick 64: Fixing \"SettingWithCopyWarning\" when creating a new columns](#trick64)\n\n[Trick 63: Calculate running count with groups using cumcount() + 1](#trick63)\n\n[Trick 62: Fixing \"SettingWithCopyWarning\" when changing columns using loc](#trick62)\n\n[Trick 61: Reading JSON from the web into a df](#trick61)\n\n[Trick 60: Creating running totals with cumsum function](#trick60)\n\n[Trick 59: Combine the output of an aggregation with the original df using transform](#trick59)\n\n[Trick 58: Use header and skiprows to get rid of bad data or empty rows while importing](#trick58)\n\n[Trick 57: Accesing the groups of a groupby object (get_group())](#trick57)\n\n[Trick 56: Apply a mappings to the whole df (applymap)](#trick56)\n\n[Trick 55: Filtering a df with multiple criteria using reduce](#trick55)\n\n[Trick 54: Calculate the difference between each row and the previous (diff())](#trick54)\n\n[Trick 53: Shuffle rows of a df (df.sample())](#trick53)\n\n[Trick 52: Making plots with pandas](#trick52)\n\n[Trick 51: Concatenate 2 column strings](#trick51)\n\n[Trick 50: Named aggregation with multiple columns passing tupples (new in pandas 0.25)](#trick50)\n\n[Trick 49: Sampling with pandas (with replacement and weights)](#trick49)\n\n[Trick 48: Useful parameters when using pd.read_csv()](#trick48)\n\n[Trick 47: Create one row for each item in a list (explode)](#trick47)\n\n[Trick 46: Store NaN in an integer type with Int64](#trick46)\n\n[Trick 45: Create rows for values separated by commas in a cell (assing and explode)](#trick45)\n\n[Trick 44: Use a local variable within a query in pandas (using @)](#trick44)\n\n[Trick 43: Create one row for each item in a list (explode) !!!duplicated Trick 47!!!](#trick43)\n\n[Trick 42: New aggregation function --> last()](#trick42)\n\n[Trick 41: Ordered categories (from pandas.api.types import CategoricalDtypee)](#trick41)\n\n[Trick 40: Style you df fast with hide_index() and set_caption()](#trick40)\n\n[Trick 39: One hot encoding (get_dummies())](#trick39)\n\n[Trick 38: Pandas datetime (lot's of examples)](#trick38)\n\n[Trick 37: Pandas slicing loc and iloc (6 examples)](#trick37)\n\n[Trick 36: Convert from UTC to another timezone](#trick36)\n\n[Trick 35: Query a column that has spaces in the name (using backticks)](#trick35)\n\n[Trick 34: Explore a dataset with profiling](#trick34)\n\n[Trick 33: Pandas display options](#trick33)\n\n[Trick 32: Filter a df with query and avoid intermediate variables](#trick32)\n\n[Trick 31: See all the columns of a big df](#trick31)\n\n[Trick 30: Pandas merge --> see where the columns are coming from (indicator = True)](#trick30)\n\n[Trick 29: Access numpy within pandas (without importing numpy as np)](#trick29)\n\n[Trick 28: Aggregating by multiple columns (using agg)](#trick28)\n\n[Trick 27: Aggregation over timeseries (resample)](#trick27)\n\n[Trick 26: Formatting different columns of a df (using dictionaries)](#trick26)\n\n[Trick 25: 3 ways of renaming columns names](#trick25)\n\n[Trick 24: Copy data from Excel into pandas quick (read_clipboard())](#trick24)\n\n[Trick 23: Fill missing values in time series data (interpolate())](#trick23)\n\n[Trick 22: Create DataFrames for testing](#trick22)\n\n[Trick 21: Split a string column into multiple columns](#trick21)\n\n[Trick 20: Create a datetime columns from multiple columns](#trick20)\n\n[Trick 19: Show memory usage of a df and every column](#trick19)\n\n[Trick 18: Read and write to a compressed file (csv.zip)](#trick18)\n\n[Trick 17: Select multiple rows\/columns with loc](#trick17)\n\n[Trick 16: Convert continuos values to categorical (cut())](#trick16)\n\n[Trick 15: Reshape a MultiIndex df (unstack())](#trick15)\n\n[Trick 14: Creating toy df (3 methods)](#trick14)\n\n[Trick 13: Avoid the series of lists TRAP](#trick13)\n\n[Trick 12: Merging datasets and check uniqueness](#trick12)\n\n[Trick 11: Rename all columns with the same pattern](#trick11)\n\n[Trick 10: Check the equality of 2 series](#trick10)\n\n[Trick 9: Reduce memory usage of a df while importing](#trick9)\n\n[Trick 8: Using glob to generate a df from multiple files !!!duplicated Trick 78!!!](#trick8)\n\n[Trick 7: Dealing with missing values (NaN)](#trick7)\n\n[Trick 6: Split a df into 2 random subsets](#trick6)\n\n[Trick 5: Convert numbers stored as strings (coerce)](#trick5)\n\n[Trick 4: Select columns by dtype](#trick4)\n\n[Trick 3: Filter a df by multiple conditions (isin and inverse using ~)](#trick3)\n\n[Trick 2: Reverse order of a df](#trick2)\n\n[Trick 1: Add a prefix or suffix to all columns](#trick1)\n","8b787c68":"<a id = \"trick55\"><\/a>\n# Trick 55: Filtering a df with multiple criteria using reduce\n[Go back to the Table of Contents](#table_of_contents)","2400c353":"<a id = \"trick45\"><\/a>\n# Trick 45: Create rows for values separated by commas in a cell (assing and explode)\n[Go back to the Table of Contents](#table_of_contents)","228d77e8":"# Welcome to this Kernel\n\nThis kernel is a compilation of tricks of pandas published by Kevin Markham weekly.\n\nYou can find the the original 100 pandas tricks (created by [Kevin Markham](https:\/\/www.linkedin.com\/in\/justmarkham\/) from Data School) on this page: \n\nhttps:\/\/www.dataschool.io\/python-pandas-tips-and-tricks\/\n\n### In the future I plan to release a similar kernel on numpy, matplotlib, seaborn & plotly.\n\n# <font size=\"+3\" color=blue ><center>Upvote if you found it useful. This will help others to discover and learn Pandas.<\/center><\/font><br><a id=\"top\"><\/a>\n\n","b6142a7e":"<a id = \"trick4\"><\/a>\n# Trick 4: Select columns by dtype\n[Go back to the Table of Contents](#table_of_contents)","03dc2679":"<a id = \"trick98\"><\/a>\n# Trick 98: Convert a wide DF into a long one\n[Go back to the Table of Contents](#table_of_contents)","158c9507":"<a id = \"trick73\"><\/a>\n# Trick 73: Remove a column and store it as a separate series\n[Go back to the Table of Contents](#table_of_contents)","5fd0b5a1":"<a id = \"trick22\"><\/a>\n# Trick 22: Create DataFrames for testing\n[Go back to the Table of Contents](#table_of_contents)","a9f222fb":"<a id = \"trick65\"><\/a>\n# Trick 65: Select columns using f-strings (new in pandas 3.6+)\n[Go back to the Table of Contents](#table_of_contents)","0574c1a0":"<a id = \"trick64\"><\/a>\n# Trick 64: Fixing \"SettingWithCopyWarning\" when creating a new columns\n[Go back to the Table of Contents](#table_of_contents)","14ed7447":"<a id = \"trick30\"><\/a>\n# Trick 30: Pandas merge --> see where the columns are coming from (indicator = True)\n[Go back to the Table of Contents](#table_of_contents)","065618f1":"<a id = \"trick37\"><\/a>\n# Trick 37: Pandas slicing loc and iloc (6 examples)\n[Go back to the Table of Contents](#table_of_contents)","1569d352":"<a id = \"trick33\"><\/a>\n# Trick 33: Pandas display options\n[Go back to the Table of Contents](#table_of_contents)","d1c04cc3":"<a id = \"trick35\"><\/a>\n# Trick 35: Query a column that has spaces in the name (using backticks)\n[Go back to the Table of Contents](#table_of_contents)","af771fe2":"<a id = \"trick68\"><\/a>\n# Trick 68: Webscraping using read_html()\n[Go back to the Table of Contents](#table_of_contents)","ef46cf89":"<a id = \"trick59\"><\/a>\n# Trick 59: Combine the output of an aggregation with the original df using transform\n[Go back to the Table of Contents](#table_of_contents)","7c5d4c56":"<a id = \"trick44\"><\/a>\n# Trick 44: Use a local variable within a query in pandas (using @)\n[Go back to the Table of Contents](#table_of_contents)","78af6418":"<a id = \"trick42\"><\/a>\n# Trick 42: New aggregation function --> last()\n[Go back to the Table of Contents](#table_of_contents)","6788e775":"<a id = \"trick53\"><\/a>\n# Trick 53: Shuffle rows of a df (df.sample())\n[Go back to the Table of Contents](#table_of_contents)","9e29f671":"<a id = \"trick6\"><\/a>\n# Trick 6: Split a df into 2 random subsets\n[Go back to the Table of Contents](#table_of_contents)","914b6fe6":"<a id = \"trick62\"><\/a>\n# Trick 62: Fixing \"SettingWithCopyWarning\" when changing columns using loc\n[Go back to the Table of Contents](#table_of_contents)","74cc2078":"<a id = \"trick99\"><\/a>\n# Trick 99: How to avoid Unnamed: 0 columns\n[Go back to the Table of Contents](#table_of_contents)","63ed6218":"<a id = \"trick10\"><\/a>\n# Trick 10: Check the equality of 2 series\n[Go back to the Table of Contents](#table_of_contents)","2e6f90b5":"<a id = \"trick66\"><\/a>\n# Trick 66: Create a bunch of new columns using a for loop and f-strings df[f'{col}_new']\n[Go back to the Table of Contents](#table_of_contents)","b3377643":"<a id = \"trick26\"><\/a>\n# Trick 26: Formatting different columns of a df (using dictionaries)\n[Go back to the Table of Contents](#table_of_contents)","4dd57d2b":"<a id = \"trick67\"><\/a>\n# Trick 67: Create new columns or overwrite using assing and set a title for the df\n[Go back to the Table of Contents](#table_of_contents)","755f3373":"<a id = \"trick74\"><\/a>\n# Trick 74: Webscraping using read_html() and match parameter\n[Go back to the Table of Contents](#table_of_contents)","7a42dde6":"<a id = \"trick82\"><\/a>\n# Trick 82: Select data by label and position (chained iloc and loc)\n[Go back to the Table of Contents](#table_of_contents)","ffa11083":"<a id = \"trick41\"><\/a>\n# Trick 41: Ordered categories (from pandas.api.types import CategoricalDtypee)\n[Go back to the Table of Contents](#table_of_contents)","62932aa2":"<a id = \"trick95\"><\/a>\n# Trick 95: Count the missing values\n[Go back to the Table of Contents](#table_of_contents)","244d864c":"<a id = \"trick25\"><\/a>\n# Trick 25: 3 ways of renaming columns names\n[Go back to the Table of Contents](#table_of_contents)","c744263e":"<a id = \"trick75\"><\/a>\n# Trick 75: Count the number of words in a pandas series\n[Go back to the Table of Contents](#table_of_contents)","d8a0f53d":"<a id = \"trick61\"><\/a>\n# Trick 61: Reading JSON from the web into a df\n[Go back to the Table of Contents](#table_of_contents)"}}