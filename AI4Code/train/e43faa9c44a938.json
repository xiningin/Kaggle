{"cell_type":{"ecf49938":"code","64bdb0f9":"code","d9bcfd7e":"code","190ff35f":"code","7d25ab06":"code","31b8396c":"code","345ecee2":"code","dd856ed8":"code","d3048d1a":"code","a2e0d27f":"code","318ab226":"code","d71318c2":"code","74fd4e43":"code","f6615a7d":"code","86bae8c4":"code","0afa8c1c":"code","a1f67aa6":"code","6e8564fd":"markdown","a8fa3509":"markdown","f29145ad":"markdown","d4b1d5ff":"markdown","6e077774":"markdown","35961b7c":"markdown","8e1f5f7a":"markdown","40e6683a":"markdown","d08f861a":"markdown","e5c01bc5":"markdown","f9dda073":"markdown","ff295d88":"markdown"},"source":{"ecf49938":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plot your data\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\nos.chdir(\"..\/input\")\n%matplotlib inline","64bdb0f9":"df= pd.read_csv('column_2C_weka.csv')","d9bcfd7e":"df.head()","190ff35f":"df.shape","7d25ab06":"df['class'].unique()","31b8396c":"df['class'].value_counts()","345ecee2":"df.hist()","dd856ed8":"df.columns","d3048d1a":"X=df.iloc[:,0:6].values\ny=df['class'].values\nX","a2e0d27f":"from sklearn import preprocessing","318ab226":"X=preprocessing.StandardScaler().fit(X).transform(X.astype(float))\n#X=(X-np.min(X))\/(np.max(X)\/np.min(X))\nX","d71318c2":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=1)","74fd4e43":"from sklearn.neighbors import KNeighborsClassifier","f6615a7d":"neigh= KNeighborsClassifier(n_neighbors=3).fit(X_train,y_train) \n# our distance metric is Euclidean metric. more: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html","86bae8c4":"yhat = neigh.predict(X_test)\nyhat","0afa8c1c":"from sklearn import metrics\nprint(\"Train Set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test Set Accuracy: \",metrics.accuracy_score(y_test,yhat))","a1f67aa6":"accuracy_list= []\nfor each in range(1,25):\n    neigh_fit= KNeighborsClassifier(n_neighbors=each).fit(X_train,y_train)\n    yhat=neigh_fit.predict(X_test)\n    accuracy_list.append(metrics.accuracy_score(y_test,yhat))\n\nplt.title('k-NN Varying number of neighbors')\nplt.plot(range(1,25),accuracy_list)\nplt.legend(loc='best',title=\"ACCURACY\")\nplt.xlabel('k values')\nplt.ylabel('accuracy')\nplt.show()","6e8564fd":"**CONCLUSION**\n\nAs you see, we catch best accuracy scrore, if k value equals to 4. Let's calculate it and it's your homework :)","a8fa3509":"**DATA VISUALIZATION **","f29145ad":"** EXPLOTARY DATA ANALYSIS (EDA)**","d4b1d5ff":"**NORMALIZE DATA**\n\nThe basic principle of k nearest neighbour is that it is a distance based algorithm. If you have different variables with varied scale ( one variable which ranges from 1 to 100 and another variable ranges from 1 to 1,00,000), it would be difficult for the model to calculate distance for each and every point. Even if there is some distance calculated in training data, when you are fitting the same model in test data it will give incorrect predictions.","6e077774":"**TRAINING**","35961b7c":"**ABOUT DATA**\n\nIn this dataset, we have Biomechanical features of orthopedic patients. We try  to classify patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 patients).\n\n\n**WHAT IS KNN ALGORITHM?**\n\nK Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN algorithm used for both classification and regression problems. KNN algorithm based on feature similarity approach.\n\n\n**KNN IN ACTION?**\n\nKNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. In Credit ratings, financial institutes will predict the credit rating of customers. In loan disbursement, banking institutes will predict whether the loan is safe or risky. In political science, classifying potential voters in two classes will vote or won\u2019t vote.\n\n\nfor more:\nhttps:\/\/www.datacamp.com\/community\/tutorials\/k-nearest-neighbor-classification-scikit-learn\n\n**AND STEPS?**\n\n* Import Data\n* Data Visualization\n* Feature Set\n* Normalize Data\n* Train Test Split\n* Classification\n* Training\n* Prediction\n* Accuracy Evaluation\n* How do we choose right K value?\n\n","8e1f5f7a":"**How we choose right K value?**\n\nK is the nearest neighbors to examine. The general solution is to choose k=1, use the training part of modeling, and calculate the accuracy of prediction using all samples in your dataset. Repeat this process and see which k is the best for your model.\n","40e6683a":"**PREDICTION**","d08f861a":"**CLASSIFICATION**","e5c01bc5":"**TRAIN TEST SPLIT**","f9dda073":"**ACCURACY EVALUATION**","ff295d88":"**FUTURE SET**"}}