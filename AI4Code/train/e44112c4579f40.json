{"cell_type":{"2262b067":"code","d45c2235":"code","7a5afce1":"code","dfea5235":"code","b34a020a":"code","20eebaec":"code","34b0b129":"code","6e5f94e0":"markdown","8469a9cc":"markdown","918a9e81":"markdown","9158501c":"markdown","59380a8b":"markdown"},"source":{"2262b067":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold\n\nimport optuna\nfrom optuna.samplers import RandomSampler\n","d45c2235":"df = pd.read_csv(\"\/kaggle\/input\/beginners-classification-dataset\/classification.csv\")","7a5afce1":"import math\nX = np.array([np.array((x1, x2, x1\/x2, x1*x2, math.log(max(x1, 0.0001)), math.log(max(x2, 0.0001)))) for x1, x2 in zip(df['age'], df['interest'])])\n\ny = np.array(df['success'])\n\ndef objective(trial):\n    tree_criterion = trial.suggest_categorical(\"tree_criterion\", ['gini', 'entropy'])\n    tree_max_depth = trial.suggest_int(\"tree_max_depth\", 50, 300)\n    tree_min_samples_leaf = trial.suggest_int(\"tree_min_samples\", 1, 10)\n    tree_split = trial.suggest_int(\"tree_split\", 2, 10)\n    classifier = DecisionTreeClassifier(criterion=tree_criterion,random_state=42,\n                                 max_depth=tree_max_depth, \n                                 min_samples_leaf=tree_min_samples_leaf,\n                                 min_samples_split=tree_split, \n                                 )    \n    score = cross_val_score(classifier, X, y, cv=10)\n    return np.mean(score)\n\n","dfea5235":"# set a sampler so that the run is deterministic, 42 is the answer to the universe etc. \nstudy = optuna.create_study(direction='maximize', sampler=RandomSampler(seed=42))\nstudy.optimize(objective, n_trials=200)\n","b34a020a":"study.best_params","20eebaec":"cv = KFold(n_splits=10)\nmislabelled_idx = []\ncorrect_idx = []\nclassifier = DecisionTreeClassifier(criterion='gini',random_state=42,\n                                     max_depth=52, \n                                     min_samples_leaf=3,\n                                     min_samples_split=2)\nfor train_index, test_index in cv.split(X):\n    classifier.fit(X[train_index], y[train_index])\n    ypred = classifier.predict(X[test_index])\n    test_index[np.argmin(ypred==y[test_index])]\n    mislabelled_idx.append(test_index[np.argmin(ypred==y[test_index])])\n        ","34b0b129":"plt.figure(figsize=(9, 7))\nsuccess2col = {0: 'y', 1: 'b'}\nfor idx, row in enumerate(df.values):\n    if idx in mislabelled_idx:\n        edgecolor = 'k'\n    else:\n        edgecolor=success2col[row[2]]\n        \n    plt.plot(row[0], row[1], 'o', color=success2col[row[2]], markeredgecolor=edgecolor, \n             alpha=0.7, markersize=10, markeredgewidth=2)    \nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Interest\", fontsize=12)\nplt.xticks(fontsize=11)\nplt.yticks(fontsize=11)","6e5f94e0":"#### I wanted to try display which points are being mislabelled.","8469a9cc":"#### I found a ~2% improvement by adding the quotient and product of the features to the input matrix.\n\n#### Here, I am using ***optuna*** which is a great and simple way to search for and tune hyperparameters, see how we can define integer-valued and categorical variables, which define a search space over which we search for the best combination. ","918a9e81":"#### It is interesting to note the points which are nestled in the 'wrong' place despite being labelled correctly.","9158501c":"#### In this notebook I add features and use optuna for a DecisionTreeClassifier to get 95% accuracy using 10-fold CV.","59380a8b":"#### Imports..."}}