{"cell_type":{"938314f5":"code","9c723fd1":"code","c89f0e78":"code","fd5b7243":"code","92db2b1b":"code","99f464b0":"code","4e5e1190":"code","d6795fdd":"code","3af802cb":"code","3171369a":"code","0cd3a792":"code","a15ff4e7":"code","5ed93853":"code","e3172969":"code","ddcca941":"code","bd82526c":"code","c7e96dd7":"code","f308e046":"code","47771b45":"code","1ee63838":"code","5afe5770":"code","14337414":"code","48b921f4":"code","fcf9297e":"markdown","ae1376ea":"markdown","e9bed418":"markdown","0f808368":"markdown","7945e482":"markdown","7fc19e6b":"markdown","30aa56a3":"markdown","5a8544a9":"markdown","5f29d6df":"markdown","b1619afa":"markdown","2ea34a1a":"markdown","24187e3c":"markdown","75dbcb3c":"markdown","b4a8496d":"markdown","4f77ff70":"markdown","ce0f7117":"markdown","fbb3f41c":"markdown","6a8e1747":"markdown"},"source":{"938314f5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable\n\n# datatable reads large csv files faster than pandas\ntrain_df = datatable.fread('\/kaggle\/input\/jane-street-market-prediction\/train.csv').to_pandas()","9c723fd1":"print(train_df.info())\ntrain_df.head()","c89f0e78":"f_mean = train_df.mean().drop(['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'date', 'ts_id']) # Will be used later","fd5b7243":"isna_df = train_df.isnull().sum()\nisna_df[isna_df > 0]","92db2b1b":"print(\"Max:\", isna_df.max())\nprint(isna_df[isna_df == isna_df.max()])\nisna_df.max()\/train_df.size","99f464b0":"discrete_features = ['feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45']\n\nisna_df = train_df[discrete_features].isnull().sum()\nisna_df[isna_df > 0]","4e5e1190":"# Filling with mean for discrete data\ndef fill_na_mean_discrete(df, discrete_features):\n    df[discrete_features] = df[discrete_features].fillna(value=df[discrete_features].mean())\n    return df\n\n# Splitting into validation and training datasets to prevent data leakage\nvalid_ratio = 0.1 # 90% training data, 10% validation data\nvalid_index = int(len(train_df.index) * (1 - valid_ratio))\n\nvalid_df = fill_na_mean_discrete(train_df[valid_index:], discrete_features)\ntrain_df = fill_na_mean_discrete(train_df[0:valid_index], discrete_features)\n\n# Re-concatenating both datasets\ntrain_df = pd.concat([train_df, valid_df], axis=0)\ntrain_df.head()","d6795fdd":"isna_df = train_df[discrete_features].isnull().sum()\nisna_df[isna_df > 0]","3af802cb":"# Forward-filling\ntrain_df.fillna(method=\"ffill\", inplace=True)\ntrain_df.head()","3171369a":"isna_df = train_df.isnull().sum()\nisna_df[isna_df > 0]","0cd3a792":"train_df.fillna(method=\"bfill\", inplace=True)\ntrain_df.head()","a15ff4e7":"isna_df = train_df.isnull().sum()\nisna_df[isna_df > 0]","5ed93853":"def reduce_memory_usage(df):\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            cmin = df[col].min()\n            cmax = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                    \n        else:\n            df[col] = df[col].astype('category')\n            \n    return df\n\ntrain_df = reduce_memory_usage(train_df)\ntrain_df.info()","e3172969":"train_df.set_index(\"ts_id\", drop=True)","ddcca941":"# valid_index established above\nvalid_df = train_df[valid_index:]\ntrain_df = train_df[0:valid_index]\n\nprint(len(train_df.index))\nprint(len(valid_df.index))","bd82526c":"train_Y = (train_df[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]] > 0).astype(int) # The model just has to predict whether the 'resp' value is positive or negative\ntrain_X = train_df.drop([\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"date\", \"ts_id\"], axis=1)\n\nvalid_Y = (valid_df[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]] > 0).astype(int)\nvalid_X = valid_df.drop([\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"date\", \"ts_id\"], axis=1)\n\nprint(train_X.head())\nprint(train_Y.head())","c7e96dd7":"import tensorflow as tf\n\n# returns a tf.data.Dataset object\ndef get_windowed_dataset(x_data, y_data, window_size, batch_size=4096, mode='train'):\n    x_ds = tf.data.Dataset.from_tensor_slices(x_data) # converting pandas Dataframe into tf.data.Dataset object\n    \n    x_ds = x_ds.window(window_size, shift=1)\n    x_ds = x_ds.flat_map(lambda window : window.batch(window_size, drop_remainder=True))\n    \n    if mode == 'train':\n        y_ds = tf.data.Dataset.from_tensor_slices(y_data[window_size:])\n        ds = tf.data.Dataset.zip((x_ds, y_ds))\n        ds = ds.shuffle(10000).batch(batch_size)\n    elif mode == 'predict':\n        ds = x_ds\n        ds = ds.batch(batch_size)\n        \n    ds = ds.prefetch(tf.data.AUTOTUNE)    \n    return ds","f308e046":"lookback = 15 # The window_size is the lookback of the model\n\ntrain_ds = get_windowed_dataset(train_X, train_Y, lookback)\nvalid_ds = get_windowed_dataset(valid_X, valid_Y, lookback)","47771b45":"for line in train_ds.take(5):\n    print(line)","1ee63838":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef build_lstm(lookback, num_columns, num_labels, lstm_units, dense_units, dropout_rate, learning_rate, label_smoothing):\n    inp = layers.Input(shape=(lookback, num_columns, ))\n    x = layers.BatchNormalization()(inp)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    for i in range(len(lstm_units)):\n        x = layers.LSTM(lstm_units[i], return_state=False, return_sequences=(False if i==len(lstm_units)-1 else True))(x)\n        x = layers.Dropout(dropout_rate)(x)\n        \n    for j in range(len(dense_units)):\n        x = layers.Dense(dense_units[j])(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(tf.keras.activations.swish)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        \n    x = layers.Dense(num_labels)(x)\n    out = layers.Activation(\"sigmoid\")(x)\n    \n    model = keras.Model(inp, out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  loss=keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n                  metrics=['AUC', 'accuracy'])\n    print(model.summary())\n    return model","5afe5770":"# This model has been tuned\nnum_epochs = 20\n\nnum_columns = len(train_X.columns)\nnum_labels = len(train_Y.columns)\nlstm_units = [64, 64]\ndense_units = [512, 256]\ndropout_rate = 0.2\nlearning_rate = 0.001\nlabel_smoothing = 0.01\n\n# Early stopping\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n\nlstm_model = build_lstm(lookback, num_columns, num_labels, lstm_units, dense_units, dropout_rate, learning_rate, label_smoothing)\nlstm_model.fit(train_ds, validation_data=(valid_ds), epochs=num_epochs, callbacks=callback)","14337414":"# Object for keeping track of the windowed data\nclass DataWindower:\n    def __init__(self, lookback, discrete_features):\n        self.data = pd.DataFrame()\n        self.cols = None\n        self.lookback = lookback\n        self.discrete_features = discrete_features\n        \n    def add_data(self, data):\n        if self.data.empty:\n            data = np.nan_to_num(data) + np.isnan(data) * f_mean # Dealing with NaN entries\n            self.data = pd.concat([data for _ in range(self.lookback)], axis=0) # Filling all rows with copies of the first data entry\n            self.cols = self.data.columns\n            self.data.reset_index(drop=True, inplace=True)\n        else:\n            data = self.__fill_na_mean_discrete(data) # Dealing with discrete NaN entries\n            data = np.nan_to_num(data) + np.isnan(data) * self.data.loc[len(self.data)-1] # Dealing with continuous NaN entries\n            self.data = pd.concat([self.data, data], axis=0)\n            self.data.drop(0, axis=0, inplace=True) # Ensuring that the data window is always of lookback length\n            self.data.reset_index(drop=True, inplace=True)\n            \n    def __fill_na_mean_discrete(self, data):\n        data[self.discrete_features] = data[self.discrete_features].fillna(value=f_mean[self.discrete_features])\n        return data\n    \n    def get_data(self):\n        return self.data.values.reshape(1, self.data.shape[0], self.data.shape[1])","48b921f4":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\ndata_w = DataWindower(lookback, discrete_features)\nthreshold = 0.500\n\nfor (test_df, sample_prediction_df) in iter_test:\n    data_w.add_data(test_df.drop('date', axis=1))\n    \n    if test_df['weight'].values > 0:\n        prediction = lstm_model.predict(data_w.get_data())\n        avg = np.sum(prediction) \/ prediction.size\n        sample_prediction_df.action = 1 if avg > threshold else 0\n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)","fcf9297e":"## 2. Transforming the Dataset\n\nNow that the data is clean, we can start to prepare the data for the model. We first split the data into training and validation data (because this is Time Series, the last 10% of data will be taken as validation).","ae1376ea":"Next, we can use forward-filling to fill the rest of the data.","e9bed418":"Let's inspect the data:","0f808368":"### Reducing Memory Usage\n\nBefore we continue, we should return to the memory usage of the dataset, as seen above. At 2.4GB, the training dataset takes up quite a lot of memory. Let's try to reduce the memory usage by optimizing the data types.\n\n(Note: if done before we fill the NaN entries, the pandas.fillna method will not work)","7945e482":"The `date` is the day on which the trading opportunity occurs. This goes from Day 0-499.\n\nThe `weight` and `resp` together represent the value of each trade. `resp_1` to `resp_4` are 'resp' values over different time horizons. **The five 'resp' values will be the dependent variables, and hence the targets of prediction.**\n\n`feature_0` to `feature_129` represent stock market data.\n\nThe `ts_id` is the index of each row. It is the number representing the time of the trading opportunity.","7fc19e6b":"Since there are discrete features with NaN entries, we need to take two different approaches to filling in the data: forward-filling the continuous data and filling with mean for the discrete data.\n\nWe deal with the discrete data first.\n\n*Note: For the sake of simplicity in this notebook, I split the data into training and validation datasets and fill with the mean before concatenating again. This is to prevent **data leakage** that occurs when the mean used to fill in the values is the mean of the whole dataset, rather than just the training set.*","30aa56a3":"### Dealing with NaN entries\n\nRight away we see that we will have to deal with numerous NaN entries, as seen in feature_121. Let's dig a little deeper:","5a8544a9":"## 1. Cleaning the Dataset\n\nWe first have to import the dataset from Kaggle.","5f29d6df":"We separate the features and our dependent variables, which are \"resp\" and the other \"resp\" over the various time frames.","b1619afa":"### Re-indexing the Data\n\nLastly, we should set the index of train_df to \"ts_id\".","2ea34a1a":"## 4. Submission\n\nUsing the Jane Street Time-series API, we set up our notebook for submission to the competition.","24187e3c":"### Data Windowing\n\nNext, we have to reshape our data for our model. Our model expects us to **window** our data for Time Series analysis. The final shape should be 3D, of the format **(batch_size, time_steps, feature_count)**.","75dbcb3c":"## 3. Building and Training the Model\n\nWe will then start building the model. I use Keras to build a LSTM model, using Adam as the optimizer, Binary-Crossentropy as the loss, and AUC-ROC and accuracy as the metrics.","b4a8496d":"We can see that the number of NaN entries has been drastically reduced, but there are still many entries with NaN values. This is likely because many NaN values start at index 0 (as can be seen from `feature_121` above) and hence do not have a last valid observation to fill from.\n\nAlthough this is not ideal since in actual use we will not have future data on hand, for training purposes we can fill in the last few NaN entries with the next valid observation instead.","4f77ff70":"## 5. Notes and Observations\n\nDespite my initial optimism that an LSTM will be an improvement over simply using a standard multi-layer perceptron (MLP), the model did not perform well. The AUC-ROC was very close to 0.5, indicating that the model had little to no distinguishing power, even on the training data. The accuracy was also low, hovering around 15-20%.\n\nThe poor performance might be due to the very short time between each data point. A lookback of 10, 50 or even 100 will only retain data from a short period of time into the past. In contrast, Fundamental Analysis tends to look at data going back hours, days or weeks. With such a short lookback, the data is also likely very noisy.\n\nThis LSTM approach was also much more resource-intensive than simpler approaches, due to the windowing of the data increasing the size of the data processed by a factor of the lookback value and the complexity of an LSTM model relative to a MLP. This limited the amount of tuning and epochs I could run due to Kaggle Notebooks' computing limitations.\n \nUltimately, I conclude that the model, as it is, is ill-suited for this problem.","ce0f7117":"We can see that there are 88 columns with NaN entries, with the a maximum of 395535 NaN entries in a single column. However, this is 0.1% of the whole dataset, so it should be okay to fill in the NaN entries.\n\nAn analysis by Tom Warrens strongly suggests that most NaN values occur at the start of the day and during midday, which corresponds to the market opening and lunch breaks. With this information, it makes sense to fill in the NaN values with the last valid observation.\n\nHowever, this only holds true if data is at least generally continuous. Carl McBride's Day 0 Exploratory Data Analysis workbook shows that this is not always the case. `feature_41` to `feature_45` comprise of discrete value. For these features, it makes more sense to fill in NaN values with the mean.\n\n*Tom Warrens' analysis can be found here: https:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day*\n\n*Carl McBride's Day 0 EDA can be found here: https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance*","fbb3f41c":"### References:\n\nhttps:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance\n\nhttps:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\n\nhttps:\/\/www.kaggle.com\/manavtrivedi\/lstm-rnn-classifier\n\nhttps:\/\/www.kaggle.com\/rajkumarl\/jane-tf-keras-lstm\n\nhttps:\/\/www.kaggle.com\/tarlannazarov\/own-jane-street-with-keras-nn","6a8e1747":"# Jane Street Market Prediction - An LSTM Approach\n*This notebook is a response to the problem posed by the \"Jane Street Market Prediction\" Kaggle Competition (Nov 2020 - Feb 2021).*\n\nThe applications of Deep Learning in financial markets has always been one of the hot topics of the field. The Jane Street Market Prediction competition challenges us to create a quantitative trading model, one that utilizes real-time market data to help make trading decisions and maximise returns.\n\n### Framing the Problem\n\nThe goal of the model is to **predict whether it is better to make a trade or pass on it** at a certain point in time, given an anonymized set of features representing stock market data at that point.\n\nI opted to use a **Long Short-Term Memory (LSTM)** model because market data is a Time Series. Analysing past patterns to predict future performance is already established in Fundamental market analysis, so I decided to have the model take into account past data in addition to current data.\n\nBelow, I go through the preparation of data, model creation and finally prediction."}}