{"cell_type":{"ddd54847":"code","d9dff4aa":"code","888801ca":"code","946260b7":"code","5d144b17":"code","2f0e3088":"code","e14de4a1":"code","9950f840":"code","56936281":"code","5bfbcdea":"code","a00c47bb":"code","980e407f":"code","b7bd5020":"code","93cfec5a":"code","9986f368":"markdown","27d72540":"markdown","f97ad841":"markdown","7610ebf1":"markdown","dc271b1e":"markdown","a2148c78":"markdown","8cb01338":"markdown","666c3b3d":"markdown","82d3975c":"markdown"},"source":{"ddd54847":"import sys","d9dff4aa":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data.dataset import random_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","888801ca":"from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n        AdamW, get_scheduler)","946260b7":"TRAIN_CSV = \"..\/input\/nlp-getting-started\/train.csv\"\nTEST_CSV = \"..\/input\/nlp-getting-started\/test.csv\"\nSAMPLE_CSV = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\nOUT_CSV = \"..\/working\/sub-nlp-getting-started_bert.csv\"","5d144b17":"torch.manual_seed(9)\ntorch.cuda.manual_seed(9)\ntorch.cuda.manual_seed_all(9)","2f0e3088":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","e14de4a1":"n_epochs = 3\nlearning_rate = 5e-5\npretrained_model = \"bert-base-uncased\"","9950f840":"\ndef main():\n\n    # train\n\n    print(\"Prepare\")\n\n    df = pd.read_csv(TRAIN_CSV)\n\n    df = cleanup_input_data(df)\n\n    train_dl, val_dl = build_train_data_loaders(df, 64, 0.1)\n\n    model, optimizer, scheduler = create_model(n_epochs, train_dl)\n\n    print(\"Train\")\n\n    for epoch in range(n_epochs):\n\n        loss, acc = train(model, optimizer, scheduler, train_dl)\n        loss_val, acc_val = evaluate(model, val_dl)\n\n        print (f\"Epoch {epoch:3d} | Train loss {loss:.6f} acc {acc:.4f} | \"\n                f\"Validation loss {loss_val:.6f} acc {acc_val:.4}\")\n\n    print(\"Create submission\")\n\n    df = pd.read_csv(TEST_CSV)\n\n    df = cleanup_input_data(df)\n\n    dl = build_test_data_loader(df, 64, 0.1)\n\n    create_submission(pd.read_csv(SAMPLE_CSV), OUT_CSV, model, dl)\n\n    print(\"Done.\")\n","56936281":"\ndef cleanup_input_data(dataframe):\n\n    return dataframe\n","5bfbcdea":"\ndef build_train_data_loaders(df, batch_size=64, val_size=0.1):\n\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n\n    x = tokenizer(df[\"text\"].to_numpy().tolist(), padding=\"longest\",\n            return_token_type_ids=False)\n\n    ds = TensorDataset(torch.tensor(x.input_ids).to(device),\n            torch.tensor(x.attention_mask).to(device),\n            torch.tensor(df.target).float().to(device))\n\n    # split\n    val_ds_size = int(len(ds) * val_size)\n    sizes = [len(ds) - val_ds_size, val_ds_size]\n    train_ds, val_ds = random_split(ds, sizes)\n\n    train_dl = DataLoader(train_ds, shuffle=True, batch_size=8)\n    val_dl = DataLoader(val_ds, shuffle=True, batch_size=8)\n\n    return train_dl, val_dl\n\n\ndef build_test_data_loader(df, batch_size=64, val_size=0.1):\n\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n\n    x = tokenizer(df[\"text\"].to_numpy().tolist(), padding=\"longest\",\n            return_token_type_ids=False)\n\n    ds = TensorDataset(torch.tensor(x.input_ids).to(device),\n            torch.tensor(x.attention_mask).to(device))\n\n    return DataLoader(ds, shuffle=False, batch_size=batch_size)\n","a00c47bb":"\ndef create_model(n_epochs, train_dl):\n\n    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model, num_labels=1)\n    model.to(device)\n\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n\n    num_training_steps = n_epochs * len(train_dl)\n\n    scheduler = get_scheduler(\n            \"linear\",\n            optimizer=optimizer,\n            num_warmup_steps=0,\n            num_training_steps=num_training_steps\n            )\n\n    return model, optimizer, scheduler\n","980e407f":"\ndef train(model, optimizer, scheduler, data_loader):\n\n    model.train()\n\n    steps = len(data_loader)\n\n    loss_cnt = 0\n    total_loss = 0\n    acc_cnt = 0\n    total_acc = 0\n\n    for batch in data_loader:\n\n        # transform into format expected by transformer model.\n        # Note, we could define own DataSet so loaders give data already\n        # prepared.\n        batch = {'input_ids':      batch[0],\n                'attention_mask':  batch[1],\n                'labels':          batch[2]}\n\n        out = model(**batch)\n\n        loss = out.loss\n        acc = torch.sum(batch[\"labels\"] == (out.logits.squeeze() >= 0.5))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        acc_cnt += len(batch[\"labels\"])\n        total_acc += acc\n        loss_cnt += 1\n        total_loss += loss.detach().item()\n\n        scheduler.step()\n\n\n    return total_loss \/ loss_cnt, total_acc \/ acc_cnt\n\n\ndef evaluate(model, data_loader):\n\n    model.eval()\n\n    loss_cnt = 0\n    total_loss = 0\n    acc_cnt = 0\n    total_acc = 0\n\n    for batch in data_loader:\n\n        batch = {'input_ids':      batch[0],\n                'attention_mask':  batch[1],\n                'labels':          batch[2]}\n\n        with torch.no_grad():\n            out = model(**batch)\n\n        loss = out.loss\n        acc = torch.sum(batch[\"labels\"] == (out.logits.squeeze() >= 0.5))\n\n        acc_cnt += len(batch[\"labels\"])\n        total_acc += acc\n        loss_cnt += 1\n        total_loss += loss.detach().item()\n\n    return total_loss \/ loss_cnt, total_acc \/ acc_cnt\n","b7bd5020":"\ndef create_submission(sample_cvs, out_filename, model, data_loader):\n\n    model.eval()\n\n    pred = []\n\n    with torch.no_grad():\n\n        for batch in data_loader:\n\n            batch = {'input_ids':      batch[0],\n                    'attention_mask':  batch[1]}\n\n            out = model(**batch)\n            out = (out.logits.squeeze() >= 0.5).int()\n            sys.stdout.flush()\n\n            pred.extend(out.cpu().numpy().tolist())\n\n    sample_cvs[\"target\"] = pred\n    sample_cvs.to_csv(out_filename, index=False)\n","93cfec5a":"\nif __name__ == \"__main__\":\n\n    main()\n","9986f368":"### Main\nDefine main control flow that calls other functions below it.","27d72540":"# BERT with transformers library\n\nThis uses the transformers library, preloading BERT and training it further\nfor this task as explained here\nhttps:\/\/huggingface.co\/transformers\/training.html.\n\nPerformance is much better than previous attempt where I was using pytorch\nwith an EmbeddingBag layer\nhttps:\/\/www.kaggle.com\/pinkaxe\/pytorch-with-embeddingbag-layer.\n\nOne doesn't have to understand transformers to use the transformers library\nbut found this a good explanation to understand better what is going on.\nhttp:\/\/jalammar.github.io\/illustrated-transformer\/\n+","f97ad841":"### Submission\nDefine function to create submission using trained model","7610ebf1":"### Run main","dc271b1e":"Define train\/validation functions.","a2148c78":"### Cleanup input data\nTODO: For now do no cleanup of the text field. If done it might give accuracy\nimprovements.","8cb01338":"### Model\nDefine function to return transformer pretrained model.","666c3b3d":"### DataLoaders\nReturn pytorch Dataloaders given panda Dataframes. Note, these also tokenize\nthe text using the BERT tokenizer.","82d3975c":"-"}}