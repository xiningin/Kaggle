{"cell_type":{"e938a41b":"code","16f0e922":"code","718439ed":"code","a0fb2366":"code","7896f296":"code","4fddef1a":"code","20fc1b3b":"code","737bf9f4":"code","7af0a1f2":"code","ef1e9bce":"code","967f5597":"code","c985c756":"code","103b5dc7":"code","02227891":"code","ccb40fd8":"code","849ef65d":"code","455b7cd7":"code","5bc27107":"code","6cfc5ba8":"code","ff368389":"code","0f4cce63":"code","4aab1e25":"code","e33b4942":"code","5677aef6":"markdown","79dcdcf4":"markdown","1a3e509f":"markdown","a9c577b7":"markdown","b396eaee":"markdown","af1fdf9c":"markdown","2f5f4533":"markdown","2bff9e8e":"markdown","62e5584a":"markdown","d5bbedbe":"markdown","b1cfde02":"markdown","6027f93a":"markdown","723cc85d":"markdown","71bf9dbb":"markdown","f0a9870d":"markdown","f1996741":"markdown","db299384":"markdown","905a9ec8":"markdown","fbea22c8":"markdown","10e40d52":"markdown","c9af7b6b":"markdown","0c74a274":"markdown","f7b41385":"markdown","39ddf706":"markdown","bdae716e":"markdown","8c4a359b":"markdown","6a88e020":"markdown","670786f5":"markdown","485ec5ea":"markdown","a44afbc1":"markdown","1904fbea":"markdown","d87cdd0c":"markdown"},"source":{"e938a41b":"# NumPy - math library for handling matrices and arrays\nimport numpy as np\n\n# Matplotlib's PyPlot - visualization library\nimport matplotlib.pyplot as plt\n\n# Seaborn - another vizualization library (based on matplotlib)\nimport seaborn as sns\n\n# Pandas - Data Analysis library\nimport pandas as pd\n\n# Tensorflow\nimport tensorflow as tf\n\n#  Keras is Tensorflow's high level API for Deep Learning\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Flatten\n\n# Util for One Hot Encoding\nfrom tensorflow.keras.utils import to_categorical","16f0e922":"# Run this twice, to get rid of Tensorflow info message\n\nSequential()","718439ed":"path = '\/kaggle\/input\/mnistimages\/mnist.npz'\n\nimport numpy as np\n\nwith np.load(path, allow_pickle=True) as f:\n    X_train, y_train = f['x_train'], f['y_train']\n    X_test, y_test = f['x_test'], f['y_test']","a0fb2366":"def visualize_distribution(data):\n    dist = pd.Series(data).value_counts()\n    sns.barplot(x=dist.index.values, y=dist.values)","7896f296":"visualize_distribution(y_train)","4fddef1a":"visualize_distribution(y_test)","20fc1b3b":"for i in range(10):\n    ith_number = np.where(y_train == i)[0]\n    random_index_number = ith_number[np.random.randint(0, (len(ith_number)))]\n    plt.subplot(2, 5, i+1)\n    plt.title(f\"{y_train[random_index_number]}\", y=-0.4, fontdict={'fontsize':14, 'color': 'green'})\n    plt.axis('off')\n    plt.imshow(X_train[random_index_number], cmap='gray')\n\nplt.show()","737bf9f4":"print(f\"y_train:\\n{y_train[0]}\")\nprint('-'*100)\nprint(f\"x_train:\\n{X_train[0]}\")","7af0a1f2":"X_train.shape","ef1e9bce":"def normalize(data):\n    data = data.astype('float32')\n    data \/= 255\n    \n    return data","967f5597":"X_train = normalize(X_train)\nX_test = normalize(X_test)","c985c756":"y_train.shape","103b5dc7":"y_train = to_categorical(y_train, num_classes=10)\ny_test = to_categorical(y_test, num_classes=10)","02227891":"y_train.shape","ccb40fd8":"y_train[0]","849ef65d":"def build_model():\n    model = Sequential() \n    model.add(Flatten(input_shape=(28, 28, 1)))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(10, activation='softmax'))\n    return model","455b7cd7":"classifier = build_model()","5bc27107":"def compile_model(model):\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )","6cfc5ba8":"compile_model(classifier)","ff368389":"history = classifier.fit(\n    X_train,\n    y_train,\n    epochs=4,\n    batch_size=32,\n    verbose=1,\n    validation_split=0.2\n)\n\n# Visualize loss during training\nplt.plot(history.history['accuracy'])\nplt.title('Training')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()\n","0f4cce63":"loss, accuracy  = classifier.evaluate(X_test, y_test, verbose=True)\n\nprint(f'Test loss: {loss:.3}')\nprint(f'Test accuracy: {accuracy:.3}')","4aab1e25":"model = build_model()                           # get new model\ncompile_model(model)                            # compile\n\nmodel.fit(                                      # train model\n    X_train[:1000],\n    y_train[:1000],\n    epochs=2,\n    batch_size=32,\n    verbose=1\n)\n\npreds = model(X_test)                           # get predictions for X_test. preds.shape = (10000, 10).","e33b4942":"rand_num = np.random.randint(0, len(X_test))\nimage = X_test[rand_num]                        # get random image\n\n\nplt.axis('off')                                 # plot image\nplt.imshow(image, cmap='gray')\nplt.show()\n\nsns.barplot(                                    # plot network output (prediction)\n    x=[i for i in range(10)], \n    y=preds[rand_num].numpy()\n)","5677aef6":"##### What is the shape of the labels before One Hot Encoding?","79dcdcf4":"## 5. Evaluate our model\nIn order to evaluate our model, we evaluate it on the test data","1a3e509f":"## 1. Now some quick data exploration","a9c577b7":"#### One-Hot Encoding\nMost machine learning algorithms are not able to train on labeled data, e.g. \"cat\", \"dog\". This needs to be converted into a numerical representation. This step is called **integer encoding**. However, by just converting \"cat\" to 1, and \"dog\" to 2, we are making an unjustified assumption that there exists a **natural ordinal ordering**.\n\nTherefore, we need to **one-hot encode** our data. This way we can represent categorical data, in a numerical way, without assuming anything about its ordering.\n![image.png](attachment:dd4dd6b3-6399-4268-8485-301bb56c8e45.png)\n[[Image source](https:\/\/medium.com\/@michaeldelsole\/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179)]","b396eaee":"## 6. Let's take a closer look at the output","af1fdf9c":"<details><summary>Hint 2: <\/summary>\n<p>\n    \n```python\nmodel.add(Dense(128, activation='relu'))\n```\n<\/p>","2f5f4533":"<details><summary>Hint 2: <\/summary>\n<p>\n    \n```python\n    data = to_categorical(data, num_classes=NUM_CLASSES)\n```\n<\/p>","2bff9e8e":"## 2. Feature engineering","62e5584a":"### Import all the libraries and modules needed","d5bbedbe":"#### Pipeline:","b1cfde02":"##### Let's visualize an example of every digit (0-9) from the training set  ","6027f93a":"##### The cell below will pick a random image and visualize it along with corresponding network output","723cc85d":"<details><summary> Click here for a more clear example  <\/summary>\n<p>\n\n![image.png](attachment:e7125604-4cb2-43f6-917e-bd20328a44a7.png)\n<a href=\"https:\/\/towardsdatascience.com\/how-to-teach-a-computer-to-see-with-convolutional-neural-networks-96c120827cd1\">[Image Source]<\/a>\n<\/p>","71bf9dbb":"<div>\n    <img src=\"attachment:0febcd7a-4ee2-45e9-a985-e747bb934c4d.png\" width=\"500\">\n<\/div>","f0a9870d":"## 4. Train model","f1996741":"#### Architecture:","db299384":"<details><summary>Hint: <\/summary>\n<p>\n    \nhttps:\/\/keras.io\/api\/utils\/python_utils\/#to_categorical-function\n    \n<\/p>","905a9ec8":"### Compiling the model\nIn order to use our defined model, we need to _compile_ it. This will create a [computational graph](https:\/\/medium.com\/tebs-lab\/deep-neural-networks-as-computational-graphs-867fcaa56c9). We will need to choose a optimizer, and a loss function. In our case, `adam` is a good fit. You can read more about the [Adam Optimizer here](https:\/\/towardsdatascience.com\/adam-latest-trends-in-deep-learning-optimization-6be9a291375c). As we have categorical data, we will choose `categorical_crossentropy`as our loss function. Read more on [loss functions here](https:\/\/towardsdatascience.com\/what-is-loss-function-1e2605aeb904).","fbea22c8":"#### Normalize the data:\nThe pixel intensity in a images is in the range between `[0, 255]`. We would like to normalize this to `[0.0, 1.0]`.\n> Artificial Neural Network processes the input data by updating its so called weights. This process can be disrupted by large integer values, and the training process can be slowed down.","10e40d52":"**What does our data look like, raw?**","c9af7b6b":"### Load data","0c74a274":"<details><summary>Hint 3: <\/summary>\n<p>\n    \n```python\nmodel.add(Dense(10, activation='softmax'))\n```\n<\/p>","f7b41385":"<details><summary>Hint 1: <\/summary>\n<p>\n    \n```python\nmodel.add(Flatten(input_shape=(28, 28, 1)))\n```\n<\/p>","39ddf706":"**Discuss with your partner**:\n\nwhat _could_ be the result of different distribution in training- and test data?","bdae716e":"##### Now, apply One Hot Encoding\n\nOnly on labels, not on features!","8c4a359b":"# Handwritten Digit Recognition\n### Using MNIST dataset and Keras\n---\n\n**Dataset**: [MNIST](http:\/\/yann.lecun.com\/exdb\/mnist\/)\n* `x_train, y_train`: contains  60 000 training images, and labels\n* `x_test, y_test`: contains  10 000 test images, and labels\n\nMade by [William Kvaale](https:\/\/github.com\/wQuole), audited by [Markus Malum Kim](https:\/\/github.com\/markusmkim)\n","6a88e020":"### Interactive Node-Link Visualization of:\n#### [Fully Connected Neural Networks](https:\/\/www.cs.ryerson.ca\/~aharley\/vis\/fc\/)\n\n#### [Convolutional Neural Networks](https:\/\/www.cs.ryerson.ca\/~aharley\/vis\/conv\/)","670786f5":"<details><summary>Hint: <\/summary>\n<p>\n    \n```python\nloss, score = model.evaluate(X_TESTING_DATA, Y_TESTING_DATA, verbose=True)\n```\n<\/p>","485ec5ea":"<details><summary>Hint: <\/summary>\n<p>\n    \n```python\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n```\n<\/p>","a44afbc1":"##### How does the labels look now?","1904fbea":"**What is the shape (dimensions) of our data?**\n\nWhat does this shape mean?","d87cdd0c":"## 3. Let's define a model\n\nWe will define a Vanilla Fully Connected Feed Forward Neural Network. This might sound very complicated, but in should not discourage the upcomming machine learning apprentice.\n\nIn order for the network to find non-linear relationships in the data, we use **activation functions**. One activation function that is quite common these day are the ReLu function.\n$$ ReLu = max(0, x)$$\n\n\nOur network will output a `1x10 vector` representing the _probability_ for each number. This is done using **softmax**. \n$$ S(y_i) = \\frac{e^{y_i}}{\\sum_{i=0}^{n=9}{e^{y_i}}}$$\n\nAll the probabilities inside the outputted vector will sum to 1.\nYou can read more about Softmax [here](https:\/\/medium.com\/data-science-bootcamp\/understand-the-softmax-function-in-minutes-f3a59641e86d).\n\n### Our model\nIs composed using the [Sequential class from Keras](https:\/\/keras.io\/api\/models\/sequential\/), and we will add three layers:\n1. a [Flatten](https:\/\/keras.io\/api\/layers\/reshaping_layers\/flatten\/) layer with input_shape=(28, 28, 1)\n2. a [Dense](https:\/\/keras.io\/api\/layers\/core_layers\/dense\/) layer with 128 units and **relu** as our activation function\n3. another Dense layer, with 10 units and **softmax** activation\n"}}