{"cell_type":{"7e28286a":"code","ef4c6f80":"code","7af58841":"code","5384e836":"code","8a14c8f0":"code","c2eb0d17":"code","10461dfc":"code","60937bf3":"code","91097d7c":"code","5e971283":"code","934be350":"code","30a1ce88":"markdown","984e7b60":"markdown","3385fd02":"markdown","40787888":"markdown","042a69c8":"markdown"},"source":{"7e28286a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\n# nltk.download('wordnet')\nstemmer = SnowballStemmer('english')\nimport unicodedata\nfrom numpy import dot\nfrom numpy.linalg import norm\n# Lemmatize with POS Tag\nfrom nltk.corpus import wordnet\nimport gc\nimport cudf\nimport cupy\nimport torch\n%matplotlib inline\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt","ef4c6f80":"train_df = pd.read_csv('\/kaggle\/input\/shopee-product-matching\/train.csv')\nDATA_PATH = '\/kaggle\/input\/shopee-product-matching\/'","7af58841":"test_df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\ndf_cu = cudf.DataFrame(test_df)","5384e836":"def cleanData(dataParse):\n    data = unicodedata.normalize('NFKC', dataParse)\n    data = re.sub(r'\u3010.*\u3011', '', data)\n    data = re.sub(r'\\[.*\\]', '', data)\n    data = re.sub(r'\u300c.*\u300d', '', data)\n    data = re.sub(r'\\(.*\\)', '', data)\n    data = re.sub(r'\\<.*\\>', '', data)\n    data = re.sub(r'[\u203b@\u25ce].*$', '', data)\n    return data.lower() #Returns the parsed tweets\n\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(cleanData(text), pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return result","8a14c8f0":"# Load word2vec model\n\nw2v_model = Word2Vec.load(\"..\/input\/word2vec-model\/word2vec_model\")","c2eb0d17":"# Generate the average word2vec for the each title description\nnon_dup_train_df=test_df.drop_duplicates(subset=['title'])\nnon_dup_train_df=non_dup_train_df.reset_index()\ndef vectors_test(): #test_df\n    \n    # Creating a list for storing the vectors (description into vectors)\n    word_embeddings_test = []\n    test_processed_docs = non_dup_train_df['title'].map(preprocess)\n    \n    # Reading the each book description \n    for line in test_processed_docs:\n        avgword2vec = None\n        count = 0\n        for word in line:\n            if word in w2v_model.wv:\n                count += 1\n                if avgword2vec is None:\n                    avgword2vec = w2v_model.wv[word]\n                else:\n                    avgword2vec = avgword2vec + w2v_model.wv[word]\n                \n        if avgword2vec is not None:\n            avgword2vec = avgword2vec \/ count\n            word_embeddings_test.append(avgword2vec)\n        else:\n            word_embeddings_test.append(np.array([0]*50, dtype='float32'))\n    return word_embeddings_test","10461dfc":"word_embeddings_test = cupy.array(vectors_test(), dtype=cupy.float32)#test_df","60937bf3":"def similarity1():\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(test_df)\/\/CHUNK\n    if len(test_df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(test_df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( word_embeddings_test, word_embeddings_test[a:b].T).T        \n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>9)[0]\n            o = test_df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n\n    \n    return preds","91097d7c":"def combine_predictions(row):\n    return ' '.join( np.unique(row))","5e971283":"non_dup_train_df['test_matches']=similarity1()\nnon_dup_train_df['matches'] = non_dup_train_df['test_matches'].apply(combine_predictions)\ndel word_embeddings_test\ndel w2v_model\ngc.collect()\nnon_dup_train_df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","934be350":"non_dup_train_df[['posting_id', 'matches']].head()","30a1ce88":"# Finding the similarity between title and product text using word2vec and cosine similarity","984e7b60":"# Data preprocessing","3385fd02":"# Load model parameter for traing","40787888":"# keep learning and enrich your intuition.Don't forgot to upvote !! :)","042a69c8":"# Find the similarity between two vector"}}