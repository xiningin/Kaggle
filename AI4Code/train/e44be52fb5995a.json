{"cell_type":{"45be9d46":"code","e117bad2":"code","b3ac0e53":"code","b07418de":"code","eabc2b37":"code","53822998":"code","5f3ce14b":"code","46a999e8":"code","fbaf4fe2":"code","07278161":"code","511a835b":"code","5d2bd14f":"code","d189cb9b":"code","c353b35a":"code","66886ea3":"code","e4145825":"code","45c5af78":"code","8d6db8ad":"code","5e88c213":"code","89311240":"code","b6efe5a8":"code","720cea5f":"code","eb5dd95d":"code","5940415d":"code","9dd74bf4":"code","448b4133":"code","4acd2f2b":"code","8f04c412":"code","91902d24":"code","d1699075":"code","696f0f9d":"code","611186f2":"code","26e6fbf5":"code","93651538":"code","a96a8967":"code","9ddadcd1":"code","2ccc33a3":"code","2413bc82":"code","61906e72":"code","6953e5a5":"code","72a0046c":"code","c2e410be":"code","3dd84e87":"code","3128dd1f":"code","ba8b436e":"code","b37905dc":"code","6a2602b3":"code","754e8d06":"code","af5bfee9":"code","81664bae":"code","b6a48e62":"code","05420aea":"code","d7cc1587":"code","1528e33b":"code","6f17fe3d":"code","5d8dda32":"code","4592fbb1":"code","d14df274":"code","aba0f9f2":"code","019a6713":"code","342235eb":"code","85833908":"code","d3984b29":"code","d0319cc9":"code","718846dd":"code","1c8036d8":"code","0939afe7":"code","617a6339":"code","7eec83e6":"code","6a24c58e":"code","506518e2":"code","044220da":"code","54b80ed9":"code","c1dc023d":"code","dc9977ef":"code","ff1666c2":"code","04d1876b":"code","2d29f265":"code","30aff6d7":"code","6b0e4985":"code","0bdbfe2f":"code","c6196491":"code","2648190e":"code","a5869e60":"code","74894adc":"code","b242f2bf":"code","6b24938e":"code","b0ffbe44":"code","dc203b0d":"code","c9f82f22":"markdown","5595322c":"markdown","5beb4e3f":"markdown","25c180b4":"markdown","704e3ffe":"markdown","84d27597":"markdown","c89ca16f":"markdown","02099cd9":"markdown","bab2b2f3":"markdown","c62b02ef":"markdown","bb45c005":"markdown","d448629d":"markdown","214018ee":"markdown","6f74c3a7":"markdown","70ab8f64":"markdown","7e8efce7":"markdown","211e0976":"markdown","110d1f99":"markdown","79148402":"markdown","e9c58534":"markdown","04dcee8f":"markdown","5a5064b1":"markdown","7d561475":"markdown","95425596":"markdown","e0a6f287":"markdown","84ff7e4a":"markdown","40b94ea5":"markdown","e694a4a7":"markdown","ac6e93a2":"markdown","57bc02a0":"markdown","1d00a2d6":"markdown","eca1e98d":"markdown","40d35e5b":"markdown","9c491753":"markdown","59a2416d":"markdown","2afecd01":"markdown","8b135a92":"markdown","dd76bdea":"markdown","dfa8e550":"markdown","d1effe71":"markdown","62521e82":"markdown","f6155852":"markdown","538b75dd":"markdown","a4b587f0":"markdown","f29f30d4":"markdown","c43865a3":"markdown","c7f7a2ff":"markdown","f14b339c":"markdown","6c8b87ad":"markdown","aa285ce8":"markdown","b8273ab7":"markdown","4a2838d9":"markdown","dfdd4343":"markdown","8b2175ec":"markdown","34c83496":"markdown","e3442776":"markdown","a774ab81":"markdown","60e8f85f":"markdown","dc1f2af7":"markdown","3a6d7a51":"markdown","829bd700":"markdown","93a75e1a":"markdown","eacdcfe1":"markdown","a1be8e17":"markdown","85232263":"markdown","4d4b2bad":"markdown","13be27c5":"markdown","94105737":"markdown","de6fbe78":"markdown","9a3395ae":"markdown","ea5535d1":"markdown","c5168f2f":"markdown","fce41384":"markdown","219ce254":"markdown","c62a35c9":"markdown","e4a6962b":"markdown","ac030f43":"markdown","a1b40aac":"markdown","492b171f":"markdown","632629db":"markdown","1fb01384":"markdown"},"source":{"45be9d46":"import zipfile\n\n# Download zip file of pizza_steak images\n!wget https:\/\/storage.googleapis.com\/ztm_tf_course\/food_vision\/pizza_steak.zip \n\n# Unzip the downloaded file\nzip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\")\nzip_ref.extractall()\nzip_ref.close()","e117bad2":"!ls pizza_steak","b3ac0e53":"!ls pizza_steak\/train\/","b07418de":"#!ls pizza_steak\/train\/steak\/","eabc2b37":"import os\n\n# Walk through pizza_steak directory and list number of files\nfor dirpath, dirnames, filenames in os.walk(\"pizza_steak\"):\n  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")","53822998":"# Another way to find out how many images are in a file\nnum_steak_images_train = len(os.listdir(\"pizza_steak\/train\/steak\"))\n\nnum_steak_images_train","5f3ce14b":"# Get the class names (programmatically, this is much more helpful with a longer list of classes)\nimport pathlib\nimport numpy as np\ndata_dir = pathlib.Path(\"pizza_steak\/train\/\") # turn our training path into a Python path\nclass_names = np.array(sorted([item.name for item in data_dir.glob('*')])) # created a list of class_names from the subdirectories\nprint(class_names)","46a999e8":"#View an image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport random\n\ndef view_random_image(target_dir, target_class):\n  # Setup target directory (we'll view images from here)\n  target_folder = target_dir+target_class\n\n  # Get a random image path\n  random_image = random.sample(os.listdir(target_folder), 1)\n\n  # Read in the image and plot it using matplotlib\n  img = mpimg.imread(target_folder + \"\/\" + random_image[0])\n  plt.imshow(img)\n  plt.title(target_class)\n  plt.axis(\"off\");\n\n  print(f\"Image shape: {img.shape}\") # show the shape of the image\n\n  return img","fbaf4fe2":"# View a random image from the training dataset\nimg = view_random_image(target_dir=\"pizza_steak\/train\/\",\n                        target_class=\"steak\")","07278161":"# View the img (actually just a big array\/tensor)\nimg","511a835b":"# View the image shape\nimg.shape # returns (width, height, colour channels)","5d2bd14f":"# Get all the pixel values between 0 & 1\nimg\/255","d189cb9b":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Set the seed\ntf.random.set_seed(42)\n\n# Preprocess data (get all of the pixel values between 1 and 0, also called scaling\/normalization)\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\nvalid_datagen = ImageDataGenerator(rescale=1.\/255)\n\n# Setup the train and test directories\ntrain_dir = \"pizza_steak\/train\/\"\ntest_dir = \"pizza_steak\/test\/\"\n\n# Import data from directories and turn it into batches\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                               batch_size=32, # number of images to process at a time \n                                               target_size=(224, 224), # convert all images to be 224 x 224\n                                               class_mode=\"binary\", # type of problem we're working on\n                                               seed=42)\n\nvalid_data = valid_datagen.flow_from_directory(test_dir,\n                                               batch_size=32,\n                                               target_size=(224, 224),\n                                               class_mode=\"binary\",\n                                               seed=42)\n\n# Create a CNN model (same as Tiny VGG - https:\/\/poloclub.github.io\/cnn-explainer\/)\nmodel_1 = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(filters=10, \n                         kernel_size=3, # can also be (3, 3)\n                         activation=\"relu\", \n                         input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels)\n  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n  tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n                            padding=\"valid\"), # padding can also be 'same'\n  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"), # activation='relu' == tf.keras.layers.Activations(tf.nn.relu)\n  tf.keras.layers.MaxPool2D(2),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\") # binary activation output\n])\n\n# Compile the model\nmodel_1.compile(loss=\"binary_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_1 = model_1.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=valid_data,\n                        validation_steps=len(valid_data))","c353b35a":"# Check out the layers in our model\nmodel_1.summary()","66886ea3":"# Set random seed\ntf.random.set_seed(42)\n\n# Create a model to replicate the TensorFlow Playground model\nmodel_2 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input\n  tf.keras.layers.Dense(4, activation='relu'),\n  tf.keras.layers.Dense(4, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_2.compile(loss='binary_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_2 = model_2.fit(train_data, # use same training data created above\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=valid_data, # use same validation data created above\n                        validation_steps=len(valid_data))","e4145825":"# Check out our second model's architecture\nmodel_2.summary()","45c5af78":"# Set random seed\ntf.random.set_seed(42)\n\n# Create a model similar to model_1 but add an extra layer and increase the number of hidden units in each layer\nmodel_3 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input\n  tf.keras.layers.Dense(100, activation='relu'), # increase number of neurons from 4 to 100 (for each layer)\n  tf.keras.layers.Dense(100, activation='relu'),\n  tf.keras.layers.Dense(100, activation='relu'), # add an extra layer\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_3.compile(loss='binary_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_3 = model_3.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=valid_data,\n                        validation_steps=len(valid_data))","8d6db8ad":"# Check out model_3 architecture\nmodel_3.summary()","5e88c213":"# import zipfile\n\n# # Download zip file of pizza_steak images\n# !wget https:\/\/storage.googleapis.com\/ztm_tf_course\/food_vision\/pizza_steak.zip\n\n# # Unzip the downloaded file\n# zip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\")\n# zip_ref.extractall()\n# zip_ref.close()","89311240":"# Visualize data (requires function 'view_random_image' above)\nplt.figure()\nplt.subplot(1, 2, 1)\nsteak_img = view_random_image(\"pizza_steak\/train\/\", \"steak\")\nplt.subplot(1, 2, 2)\npizza_img = view_random_image(\"pizza_steak\/train\/\", \"pizza\")","b6efe5a8":"# Define training and test directory paths\ntrain_dir = \"pizza_steak\/train\/\"\ntest_dir = \"pizza_steak\/test\/\"","720cea5f":"# Create train and test data generators and rescale the data \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale=1\/255.)\ntest_datagen = ImageDataGenerator(rescale=1\/255.)","eb5dd95d":"# Turn it into batches\ntrain_data = train_datagen.flow_from_directory(directory=train_dir,\n                                               target_size=(224, 224),\n                                               class_mode='binary',\n                                               batch_size=32)\n\ntest_data = test_datagen.flow_from_directory(directory=test_dir,\n                                             target_size=(224, 224),\n                                             class_mode='binary',\n                                             batch_size=32)","5940415d":"# Get a sample of the training data batch \nimages, labels = train_data.next() # get the 'next' batch of images\/labels\nlen(images), len(labels)","9dd74bf4":"# Get the first two images\nimages[:2], images[0].shape","448b4133":"# View the first batch of labels\nlabels","4acd2f2b":"# Make the creating of our model a little easier\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation\nfrom tensorflow.keras import Sequential","8f04c412":"# Create the model (this can be our baseline, a 3 layer Convolutional Neural Network)\nmodel_4 = Sequential([\n  Conv2D(filters=10, \n         kernel_size=3, \n         strides=1,\n         padding='valid',\n         activation='relu', \n         input_shape=(224, 224, 3)), # input layer (specify input shape)\n  Conv2D(10, 3, activation='relu'),\n  Conv2D(10, 3, activation='relu'),\n  Flatten(),\n  Dense(1, activation='sigmoid') # output layer (specify output shape)\n])","91902d24":"# Compile the model\nmodel_4.compile(loss='binary_crossentropy',\n                optimizer=Adam(),\n                metrics=['accuracy'])","d1699075":"# Check lengths of training and test data generators\nlen(train_data), len(test_data)","696f0f9d":"# Fit the model\nhistory_4 = model_4.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))","611186f2":"# Plot the training curves\nimport pandas as pd\npd.DataFrame(history_4.history).plot(figsize=(10, 7));","26e6fbf5":"# Plot the validation and training data separately\ndef plot_loss_curves(history):\n  \"\"\"\n  Returns separate loss curves for training and validation metrics.\n  \"\"\" \n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  accuracy = history.history['accuracy']\n  val_accuracy = history.history['val_accuracy']\n\n  epochs = range(len(history.history['loss']))\n\n  # Plot loss\n  plt.plot(epochs, loss, label='training_loss')\n  plt.plot(epochs, val_loss, label='val_loss')\n  plt.title('Loss')\n  plt.xlabel('Epochs')\n  plt.legend()\n\n  # Plot accuracy\n  plt.figure()\n  plt.plot(epochs, accuracy, label='training_accuracy')\n  plt.plot(epochs, val_accuracy, label='val_accuracy')\n  plt.title('Accuracy')\n  plt.xlabel('Epochs')\n  plt.legend();","93651538":"# Check out the loss curves of model_4\nplot_loss_curves(history_4)","a96a8967":"# Check out our model's architecture\nmodel_4.summary()","9ddadcd1":"# Create the model (this can be our baseline, a 3 layer Convolutional Neural Network)\nmodel_5 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  MaxPool2D(pool_size=2), # reduce number of features by half\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(1, activation='sigmoid')\n])\n# Compile model (same as model_4)\nmodel_5.compile(loss='binary_crossentropy',\n                optimizer=Adam(),\n                metrics=['accuracy'])\n# Fit the model\nhistory_5 = model_5.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))","2ccc33a3":"# Check out the model architecture\nmodel_5.summary()","2413bc82":"# Plot loss curves of model_5 results\nplot_loss_curves(history_5)","61906e72":"# Create ImageDataGenerator training instance with data augmentation\ntrain_datagen_augmented = ImageDataGenerator(rescale=1\/255.,\n                                             rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n                                             shear_range=0.2, # shear the image\n                                             zoom_range=0.2, # zoom into the image\n                                             width_shift_range=0.2, # shift the image width ways\n                                             height_shift_range=0.2, # shift the image height ways\n                                             horizontal_flip=True) # flip the image on the horizontal axis\n\n# Create ImageDataGenerator training instance without data augmentation\ntrain_datagen = ImageDataGenerator(rescale=1\/255.) \n\n# Create ImageDataGenerator test instance without data augmentation\ntest_datagen = ImageDataGenerator(rescale=1\/255.)","6953e5a5":"# Import data and augment it from training directory\nprint(\"Augmented training images:\")\ntrain_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n                                                                   target_size=(224, 224),\n                                                                   batch_size=32,\n                                                                   class_mode='binary',\n                                                                   shuffle=False) # Don't shuffle for demonstration purposes, usually a good thing to shuffle\n\n# Create non-augmented data batches\nprint(\"Non-augmented training images:\")\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                               target_size=(224, 224),\n                                               batch_size=32,\n                                               class_mode='binary',\n                                               shuffle=False) # Don't shuffle for demonstration purposes\n\nprint(\"Unchanged test images:\")\ntest_data = test_datagen.flow_from_directory(test_dir,\n                                             target_size=(224, 224),\n                                             batch_size=32,\n                                             class_mode='binary')","72a0046c":"# Get data batch samples\nimages, labels = train_data.next()\naugmented_images, augmented_labels = train_data_augmented.next() # Note: labels aren't augmented, they stay the same","c2e410be":"# Show original image and augmented image\nrandom_number = random.randint(0, 32) # we're making batches of size 32, so we'll get a random instance\nplt.imshow(images[random_number])\nplt.title(f\"Original image\")\nplt.axis(False)\nplt.figure()\nplt.imshow(augmented_images[random_number])\nplt.title(f\"Augmented image\")\nplt.axis(False);","3dd84e87":"# Create the model (same as model_5)\nmodel_6 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  MaxPool2D(pool_size=2), # reduce number of features by half\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_6.compile(loss='binary_crossentropy',\n                optimizer=Adam(),\n                metrics=['accuracy'])\n\n# Fit the model\nhistory_6 = model_6.fit(train_data_augmented, # changed to augmented training data\n                        epochs=5,\n                        steps_per_epoch=len(train_data_augmented),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))","3128dd1f":"# Check model's performance history training on augmented data\nplot_loss_curves(history_6)","ba8b436e":"# Import data and augment it from directories\ntrain_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir,\n                                                                            target_size=(224, 224),\n                                                                            batch_size=32,\n                                                                            class_mode='binary',\n                                                                            shuffle=True) # Shuffle data (default)","b37905dc":"# Create the model (same as model_5 and model_6)\nmodel_7 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_7.compile(loss='binary_crossentropy',\n                optimizer=Adam(),\n                metrics=['accuracy'])\n\n# Fit the model\nhistory_7 = model_7.fit(train_data_augmented_shuffled, # now the augmented data is shuffled\n                        epochs=5,\n                        steps_per_epoch=len(train_data_augmented_shuffled),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))","6a2602b3":"# Create a CNN model (same as Tiny VGG but for binary classification - https:\/\/poloclub.github.io\/cnn-explainer\/ )\nmodel_8 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)), # same input shape as our images\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_8.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_8 = model_8.fit(train_data_augmented_shuffled,\n                        epochs=5,\n                        steps_per_epoch=len(train_data_augmented_shuffled),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))","754e8d06":"# Check model_1 architecture (same as model_8)\nmodel_1.summary()","af5bfee9":"# Check model_8 architecture (same as model_1)\nmodel_8.summary()","81664bae":"# How does this training curve look compared to the one above?\nplot_loss_curves(history_1)","b6a48e62":"# Classes we're working with\nprint(class_names)","05420aea":"# View our example image\n!wget https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/03-steak.jpeg \nsteak = mpimg.imread(\"03-steak.jpeg\")\nplt.imshow(steak)\nplt.axis(False);","d7cc1587":"# Check the shape of our image\nsteak.shape","1528e33b":"# Create a function to import an image and resize it to be able to be used with our model\ndef load_and_prep_image(filename, img_shape=224):\n  \"\"\"\n  Reads an image from filename, turns it into a tensor\n  and reshapes it to (img_shape, img_shape, colour_channel).\n  \"\"\"\n  # Read in target file (an image)\n  img = tf.io.read_file(filename)\n\n  # Decode the read file into a tensor & ensure 3 colour channels \n  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n  img = tf.image.decode_image(img, channels=3)\n\n  # Resize the image (to the same size our model was trained on)\n  img = tf.image.resize(img, size = [img_shape, img_shape])\n\n  # Rescale the image (get all values between 0 and 1)\n  img = img\/255.\n  return img","6f17fe3d":"# Load in and preprocess our custom image\nsteak = load_and_prep_image(\"03-steak.jpeg\")\nsteak","5d8dda32":"# Add an extra axis\nprint(f\"Shape before new dimension: {steak.shape}\")\nsteak = tf.expand_dims(steak, axis=0) # add an extra dimension at axis 0\n#steak = steak[tf.newaxis, ...] # alternative to the above, '...' is short for 'every other dimension'\nprint(f\"Shape after new dimension: {steak.shape}\")\nsteak","4592fbb1":"# Make a prediction on custom image tensor\npred = model_8.predict(steak)\npred","d14df274":"# Remind ourselves of our class names\nclass_names","aba0f9f2":"# We can index the predicted class by rounding the prediction probability\npred_class = class_names[int(tf.round(pred)[0][0])]\npred_class","019a6713":"def pred_and_plot(model, filename, class_names):\n  \"\"\"\n  Imports an image located at filename, makes a prediction on it with\n  a trained model and plots the image with the predicted class as the title.\n  \"\"\"\n  # Import the target image and preprocess it\n  img = load_and_prep_image(filename)\n\n  # Make a prediction\n  pred = model.predict(tf.expand_dims(img, axis=0))\n\n  # Get the predicted class\n  pred_class = class_names[int(tf.round(pred)[0][0])]\n\n  # Plot the image and predicted class\n  plt.imshow(img)\n  plt.title(f\"Prediction: {pred_class}\")\n  plt.axis(False);","342235eb":"# Test our model on a custom image\npred_and_plot(model_8, \"03-steak.jpeg\", class_names)","85833908":"# Download another test image and make a prediction on it\n!wget https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/03-pizza-dad.jpeg \npred_and_plot(model_8, \"03-pizza-dad.jpeg\", class_names)","d3984b29":"import zipfile\n\n# Download zip file of 10_food_classes images\n# See how this data was created - https:\/\/github.com\/mrdbourke\/tensorflow-deep-learning\/blob\/main\/extras\/image_data_modification.ipynb\n!wget https:\/\/storage.googleapis.com\/ztm_tf_course\/food_vision\/10_food_classes_all_data.zip \n\n# Unzip the downloaded file\nzip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\")\nzip_ref.extractall()\nzip_ref.close()","d0319cc9":"import os\n\n# Walk through 10_food_classes directory and list number of files\nfor dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):\n  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")","718846dd":"train_dir = \"10_food_classes_all_data\/train\/\"\ntest_dir = \"10_food_classes_all_data\/test\/\"","1c8036d8":"# Get the class names for our multi-class dataset\nimport pathlib\nimport numpy as np\ndata_dir = pathlib.Path(train_dir)\nclass_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\nprint(class_names)","0939afe7":"# View a random image from the training dataset\nimport random\nimg = view_random_image(target_dir=train_dir,\n                        target_class=random.choice(class_names)) # get a random class name","617a6339":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Rescale the data and create data generator instances\ntrain_datagen = ImageDataGenerator(rescale=1\/255.)\ntest_datagen = ImageDataGenerator(rescale=1\/255.)\n\n# Load data in from directories and turn it into batches\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                               target_size=(224, 224),\n                                               batch_size=32,\n                                               class_mode='categorical') # changed to categorical\n\ntest_data = train_datagen.flow_from_directory(test_dir,\n                                              target_size=(224, 224),\n                                              batch_size=32,\n                                              class_mode='categorical')","7eec83e6":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n\n# Create our model (a clone of model_8, except to be multi-class)\nmodel_9 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(10, activation='softmax') # changed to have 10 neurons (same as number of classes) and 'softmax' activation\n])\n\n# Compile the model\nmodel_9.compile(loss=\"categorical_crossentropy\", # changed to categorical_crossentropy\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","6a24c58e":"# Fit the model\nhistory_9 = model_9.fit(train_data, # now 10 different classes \n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))","506518e2":"# Evaluate on the test data\nmodel_9.evaluate(test_data)","044220da":"# Check out the model's loss curves on the 10 classes of data (note: this function comes from above in the notebook)\nplot_loss_curves(history_9)","54b80ed9":"# Try a simplified model (removed two layers)\nmodel_10 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(10, activation='softmax')\n])\n\nmodel_10.compile(loss='categorical_crossentropy',\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=['accuracy'])\n\nhistory_10 = model_10.fit(train_data,\n                          epochs=5,\n                          steps_per_epoch=len(train_data),\n                          validation_data=test_data,\n                          validation_steps=len(test_data))","c1dc023d":"# Check out the loss curves of model_10\nplot_loss_curves(history_10)","dc9977ef":"# Create augmented data generator instance\ntrain_datagen_augmented = ImageDataGenerator(rescale=1\/255.,\n                                             rotation_range=20, # note: this is an int not a float\n                                             width_shift_range=0.2,\n                                             height_shift_range=0.2,\n                                             zoom_range=0.2,\n                                             horizontal_flip=True)\n\ntrain_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n                                                                  target_size=(224, 224),\n                                                                  batch_size=32,\n                                                                  class_mode='categorical')","ff1666c2":"# Clone the model (use the same architecture)\nmodel_11 = tf.keras.models.clone_model(model_10)\n\n# Compile the cloned model (same setup as used for model_10)\nmodel_11.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_11 = model_11.fit(train_data_augmented, # use augmented data\n                          epochs=5,\n                          steps_per_epoch=len(train_data_augmented),\n                          validation_data=test_data,\n                          validation_steps=len(test_data))","04d1876b":"# Check out our model's performance with augmented data\nplot_loss_curves(history_11)","2d29f265":"# What classes has our model been trained on?\nclass_names","30aff6d7":"# -q is for \"quiet\"\n!wget -q https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/03-pizza-dad.jpeg\n!wget -q https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/03-steak.jpeg\n!wget -q https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/03-hamburger.jpeg\n!wget -q https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/03-sushi.jpeg","6b0e4985":"# Make a prediction using model_11\npred_and_plot(model=model_11, \n              filename=\"03-steak.jpeg\", \n              class_names=class_names)","0bdbfe2f":"pred_and_plot(model_11, \"03-sushi.jpeg\", class_names)","c6196491":"pred_and_plot(model_11, \"03-pizza-dad.jpeg\", class_names)","2648190e":"# Load in and preprocess our custom image\nimg = load_and_prep_image(\"03-steak.jpeg\")\n\n# Make a prediction\npred = model_11.predict(tf.expand_dims(img, axis=0))\n\n# Match the prediction class to the highest prediction probability\npred_class = class_names[pred.argmax()]\nplt.imshow(img)\nplt.title(pred_class)\nplt.axis(False);","a5869e60":"# Check the output of the predict function\npred = model_11.predict(tf.expand_dims(img, axis=0))\npred","74894adc":"# Find the predicted class name\nclass_names[pred.argmax()]","b242f2bf":"# Adjust function to work with multi-class\ndef pred_and_plot(model, filename, class_names):\n  \"\"\"\n  Imports an image located at filename, makes a prediction on it with\n  a trained model and plots the image with the predicted class as the title.\n  \"\"\"\n  # Import the target image and preprocess it\n  img = load_and_prep_image(filename)\n\n  # Make a prediction\n  pred = model.predict(tf.expand_dims(img, axis=0))\n\n  # Get the predicted class\n  if len(pred[0]) > 1: # check for multi-class\n    pred_class = class_names[pred.argmax()] # if more than one output, take the max\n  else:\n    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n\n  # Plot the image and predicted class\n  plt.imshow(img)\n  plt.title(f\"Prediction: {pred_class}\")\n  plt.axis(False);","6b24938e":"pred_and_plot(model_11, \"03-steak.jpeg\", class_names)","b0ffbe44":"pred_and_plot(model_11, \"03-sushi.jpeg\", class_names)","dc203b0d":"pred_and_plot(model_11, \"03-pizza-dad.jpeg\", class_names)","c9f82f22":"Okay, so we've got a collection of 750 training images and 250 testing images of pizza and steak.\n\nLet's look at some.\n\n> \ud83e\udd14 **Note:** Whenever you're working with data, it's always good to visualize it as much as possible. Treat your first couple of steps of a project as becoming one with the data. **Visualize, visualize, visualize.**","5595322c":"### 3. Create a model (start with a baseline)\n\nWe can use the same model (TinyVGG) we used for the binary classification problem for our multi-class classification problem with a couple of small tweaks.\n\nNamely:\n* Changing the output layer to use have 10 ouput neurons (the same number as the number of classes we have).\n* Changing the output layer to use `'softmax'` activation instead of `'sigmoid'` activation.\n* Changing the loss function to be `'categorical_crossentropy'` instead of `'binary_crossentropy'`.","5beb4e3f":"Woah, that's quite the gap between the training and validation loss curves.\n\nWhat does this tell us?\n\nIt seems our model is **overfitting** the training set quite badly. In other words, it's getting great results on the training data but fails to generalize well to unseen data and performs poorly on the test data.","25c180b4":"## Binary classification: Let's break it down\n\nWe just went through a whirlwind of steps:\n\n1. Become one with the data (visualize, visualize, visualize...)\n2. Preprocess the data (prepare it for a model)\n3. Create a model (start with a baseline)\n4. Fit the model\n5. Evaluate the model\n6. Adjust different parameters and improve model (try to beat your baseline)\n7. Repeat until satisfied\n\nLet's step through each.","704e3ffe":"> \ud83d\udd11 **Note:** You might've noticed we used some slightly different code to build `model_8` as compared to `model_1`. This is because of the imports we did before, such as `from tensorflow.keras.layers import Conv2D` reduce the amount of code we had to write. Although the code is different, the architectures are the same.","84d27597":"### 5. Evaluate the model\nOh yeah! Looks like our model is learning something.\n\nLet's check out its training curves.","c89ca16f":"It seems our validation loss curve is heading in the right direction but it's a bit jumpy (the most ideal loss curve isn't too spiky but a smooth descent, however, a perfectly smooth loss curve is the equivalent of a fairytale).\n\nLet's see what happens when we shuffle the augmented training data.","02099cd9":"### 6. Adjust the model parameters\n\nFitting a machine learning model  comes in 3 steps:\n0. Create a basline.\n1. Beat the baseline by overfitting a larger model.\n2. Reduce overfitting.\n\nSo far we've gone through steps 0 and 1.\n\nAnd there are even a few more things we could try to further overfit our model:\n* Increase the number of convolutional layers.\n* Increase the number of convolutional filters.\n* Add another dense layer to the output of our flattened layer.\n\nBut what we'll do instead is focus on getting our model's training curves to better align with eachother, in other words, we'll take on step 2.\n\nWhy is reducing overfitting important?\n\nWhen a model performs too well on training data and poorly on unseen data, it's not much use to us if we wanted to use it in the real world.\n\nSay we were building a pizza vs. steak food classifier app, and our model performs very well on our training data but when users tried it out, they didn't get very good results on their own food images, is that a good experience?\n\nNot really...\n\nSo for the next few models we build, we're going to adjust a number of parameters and inspect the training curves along the way.\n\nNamely, we'll build 2 more models:\n* A ConvNet with [max pooling](https:\/\/deeplizard.com\/learn\/video\/ZjM_XQa5s6s)\n* A ConvNet with max pooling and data augmentation\n\nFor the first model, we'll follow the modified basic CNN structure:\n\n```\nInput -> Conv layers + ReLU layers (non-linearities) + Max Pooling layers -> Fully connected (dense layer) as Output\n```\n\nLet's built it. It'll have the same structure as `model_4` but with a [`MaxPool2D()`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/MaxPool2D) layer after each convolutional layer.","bab2b2f3":"Hmm, judging by our loss curves, it looks like our model is **overfitting** the training dataset.\n\n> \ud83d\udd11 **Note:** When a model's **validation loss starts to increase**, it's likely that it's overfitting the training dataset. This means, it's learning the patterns in the training dataset *too well* and thus its ability to generalize to unseen data will be diminished.\n\nTo further inspect our model's training performance, let's separate the accuracy and loss curves.","c62b02ef":"### 5. Evaluate the model\n\nWoohoo! We've just trained a model on 10 different classes of food images, let's see how it went.","bb45c005":"After going through a sample of original and augmented images, you can start to see some of the example transformations on the training images.\n\nNotice how some of the augmented images look like slightly warped versions of the original image. This means our model will be forced to try and learn patterns in less-than-perfect images, which is often the case when using real-world images.\n\n> \ud83e\udd14 **Question:** Should I use data augmentation? And how much should I augment?\n\nData augmentation is a way to try and prevent a model overfitting. If your model is overfiting (e.g. the validation loss keeps increasing), you may want to try using data augmentation.\n\nAs for how much to data augment, there's no set practice for this. Best to check out the options in the `ImageDataGenerator` class and think about how a model in your use case might benefit from some data augmentation.\n\nNow we've got augmented data, let's try and refit a model on it and see how it affects training.\n\nWe'll use the same model as `model_5`.","d448629d":"### 4. Fit a model\n\nOur model is compiled, time to fit it.\n\nYou'll notice two new parameters here:\n* `steps_per_epoch` - this is the number of batches a model will go through per epoch, in our case, we want our model to go through all batches so it's equal to the length of `train_data` (1500 images in batches of 32 = 1500\/32 = ~47 steps)\n* `validation_steps` - same as above, except for the `validation_data` parameter (500 test images in batches of 32 = 500\/32 = ~16 steps)","214018ee":"### 3. Create a model (start with a baseline)\n\nYou might be wondering what your default model architecture should be.\n\nAnd the truth is, there's many possible answers to this question.\n\nA simple heuristic for computer vision models is to use the model architecture which is performing best on [ImageNet](https:\/\/www.image-net.org\/) (a large collection of diverse images to benchmark different computer vision models).\n\nHowever, to begin with, it's good to build a smaller model to acquire a baseline result which you try to improve upon.\n\n> \ud83d\udd11 **Note:** In deep learning a smaller model often refers to a model with less layers than the state of the art (SOTA). For example, a smaller model might have 3-4 layers where as a state of the art model, such as, ResNet50 might have 50+ layers.\n\nIn our case, let's take a smaller version of the model that can be found on the [CNN explainer website](https:\/\/poloclub.github.io\/cnn-explainer\/) (`model_1` from above) and build a 3 layer convolutional neural network.","6f74c3a7":"Wonderful! Looks like our training dataset has 1500 images belonging to 2 classes (pizza and steak) and our test dataset has 500 images also belonging to 2 classes.\n\nSome things to here:\n* Due to how our directories are structured, the classes get inferred by the subdirectory names in `train_dir` and `test_dir`.\n* The `target_size` parameter defines the input size of our images in `(height, width)` format.\n* The `class_mode` value of `'binary'` defines our classification problem type. If we had more than two classes, we would use `'categorical'`.\n* The `batch_size` defines how many images will be in each batch, we've used 32 which is the same as the default.\n\nWe can take a look at our batched images and labels by inspecting the `train_data` object.","70ab8f64":"Wow. One of the most noticeable things here is the much larger number of parameters in `model_2` versus `model_1`.\n\n`model_2` has 602,141 trainable parameters where as `model_1` has only 31,101. And despite this difference, `model_1` still far and large out performs `model_2`.\n\n> \ud83d\udd11 **Note:** You can think of trainable parameters as *patterns a model can learn from data*. Intuitiely, you might think more is better. And in some cases it is. But in this case, the difference here is in the two different styles of model we're using. Where a series of dense layers have a number of different learnable parameters connected to each other and hence a higher number of possible learnable patterns, **a convolutional neural network seeks to sort out and learn the most important patterns in an image**. So even though there are less learnable parameters in our convolutional neural network, these are often more helpful in decphering between different **features** in an image.\n\nSince our previous model didn't work, do you have any ideas of how we might make it work?\n\nHow about we increase the number of layers? \n\nAnd maybe even increase the number of neurons in each layer?\n\nMore specifically, we'll increase the number of neurons (also called hidden units) in each dense layer from 4 to 100 and add an extra layer.\n\n> \ud83d\udd11 **Note:** Adding extra layers or increasing the number of neurons in each layer is often referred to as increasing the **complexity** of your model.","7e8efce7":"### Making a prediction with our trained model\n\nWhat good is a trained model if you can't make predictions with it?\n\nTo really test it out, we'll upload a couple of our own images and see how the model goes.\n\nFirst, let's remind ourselves of the classnames and view the image we're going to test on.","211e0976":"Now we've got a function to load our custom image, let's load it in.","110d1f99":"Ahh, the predictions come out in **prediction probability** form. In other words, this means how likely the image is to be one class or another.\n\nSince we're working with a binary classification problem, if the prediction probability is over 0.5, according to the model, the prediction is most likely to be the **postive class** (class 1).\n\nAnd if the prediction probability is under 0.5, according to the model, the predicted class is most likely to be the **negative class** (class 0).\n\n> \ud83d\udd11 **Note:** The 0.5 cutoff can be adjusted to your liking. For example, you could set the limit to be 0.8 and over for the positive class and 0.2 for the negative class. However, doing this will almost always change your model's performance metrics so be sure to make sure they change in the right direction.\n\nBut saying positive and negative class doesn't make much sense when we're working with pizza \ud83c\udf55 and steak \ud83e\udd69...\n\nSo let's write a little function to convert predictions into their class names and then plot the target image.","79148402":"## A (typical) architecture of a convolutional neural network\n\nConvolutional neural networks are no different to other kinds of deep learning neural networks in the fact they can be created in many different ways. What you see below are some components you'd expect to find in a traditional CNN.","e9c58534":"You can see it each epoch takes longer than the previous model. This is because our data is being augmented on the fly on the CPU as it gets loaded onto the GPU, in turn, increasing the amount of time between each epoch.\n\nHow do our model's training curves look?","04dcee8f":"After going through a dozen or so images from the different classes, you can start to get an idea of what we're working with.\n\nThe entire Food101 dataset comprises of similar images from 101 different classes.\n\nYou might've noticed we've been printing the image shape alongside the plotted image.\n\nThis is because the way our computer sees the image is in the form of a big array (tensor).","5a5064b1":"Due to the `class_mode` parameter being `'binary'` our labels are either `0` (pizza) or `1` (steak).\n\nNow that our data is ready, our model is going to try and figure out the patterns between the image tensors and the labels.","7d561475":"### 2. Preprocess the data (prepare it for a model)\n\nOne of the most important steps for a machine learning project is creating a training and test set.\n\nIn our case, our data is already split into training and test sets. Another option here might be to create a validation set as well, but we'll leave that for now.\n\nFor an image classification project, it's standard to have your data seperated into `train` and `test` directories with subfolders in each for each class.\n\nTo start we define the training and test directory paths.","95425596":"\nComponents of a convolutional neural network:\n\n| **Hyperparameter\/Layer type** | **What does it do?** | **Typical values** |\n| ----- | ----- | ----- |\n| Input image(s) | Target images you'd like to discover patterns in| Whatever you can take a photo (or video) of |\n| Input layer | Takes in target images and preprocesses them for further layers | `input_shape = [batch_size, image_height, image_width, color_channels]` |\n| Convolution layer | Extracts\/learns the most important features from target images | Multiple, can create with [`tf.keras.layers.ConvXD`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Conv2D) (X can be multiple values) |\n| Hidden activation | Adds non-linearity to learned features (non-straight lines) | Usually ReLU ([`tf.keras.activations.relu`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations\/relu)) |\n| Pooling layer | Reduces the dimensionality of learned image features | Average ([`tf.keras.layers.AvgPool2D`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/AveragePooling2D)) or Max ([`tf.keras.layers.MaxPool2D`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/MaxPool2D)) |\n| Fully connected layer | Further refines learned features from convolution layers | [`tf.keras.layers.Dense`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dense) |\n| Output layer | Takes learned features and outputs them in shape of target labels | `output_shape = [number_of_classes]` (e.g. 3 for pizza, steak or sushi)|\n| Output activation | Adds non-linearities to output layer | [`tf.keras.activations.sigmoid`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations\/sigmoid) (binary classification) or [`tf.keras.activations.softmax`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations\/softmax) |\n\nHow they stack together:\n\n![](https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/03-simple-convnet.png)\n*A simple example of how you might stack together the above layers into a convolutional neural network. Note the convolutional and pooling layers can often be arranged and rearranged into many different formations.*","e0a6f287":"## Using the same model as before\n\nTo examplify how neural networks can be adapted to many different problems, let's see how a binary classification model we've previously built might work with our data.\n\n> \ud83d\udd11 **Note:** If you haven't gone through the previous classification notebook, no troubles, we'll be bringing in the a simple 4 layer architecture used to separate dots replicated from the [TensorFlow Playground environment](https:\/\/playground.tensorflow.org\/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=4,4&seed=0.75075&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n\nWe can use all of the same parameters in our previous model except for changing two things:\n* **The data** - we're now working with images instead of dots.\n* **The input shape** - we have to tell our neural network the shape of the images we're working with.\n  * A common practice is to reshape images all to one size. In our case, we'll resize the images to `(224, 224, 3)`, meaning a height and width of 224 pixels and a depth of 3 for the red, green, blue colour channels.","84ff7e4a":"Woah! Looks like our model is learning again. It got ~70% accuracy on the training set and ~70% accuracy on the validation set.\n\nHow does the architecute look?","40b94ea5":"### 7. Repeat until satisfied\n\nWe could keep going here. Restructuring our model's architecture, adding more layers, trying it out, adjusting the learning rate, trying it out, trying different methods of data augmentation, training for longer. But as you could image, this could take a fairly long time.\n\nGood thing there's still one trick we haven't tried yet and that's **transfer learning**.\n\nHowever, we'll save that for the next notebook where you'll see how rather than design our own models from scratch we leverage the patterns another model has learned for our own task.\n\nIn the meantime, let's make a prediction with our trained multi-class model.","e694a4a7":"Now let's check out all of the different directories and sub-directories in the `10_food_classes` file.","ac6e93a2":"Wonderful, it seems our images and labels are in batches of 32.\n\nLet's see what the images look like.","57bc02a0":"Okay, we've got some custom images to try, let's use the `pred_and_plot` function to make a prediction with `model_11` on one of the images and plot it.","1d00a2d6":"Due to our `rescale` parameter, the images are now in `(224, 224, 3)` shape tensors with values between 0 and 1.\n\nHow about the labels?","eca1e98d":"Knowing this, we can readjust our `pred_and_plot` function to work with multiple classes as well as binary classes.","40d35e5b":"The first test image we're going to use is [a delicious steak](https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/03-steak.jpeg) I cooked the other day.","9c491753":"As with binary classifcation, we've creator image generators. The main change this time is that we've changed the `class_mode` parameter to `'categorical'` because we're dealing with 10 classes of food images.\n\nEverything else like rescaling the images, creating the batch size and target image size stay the same.\n\n> \ud83e\udd14 **Question:** Why is the image size 224x224? This could actually be any size we wanted, however, 224x224 is a very common size for preprocessing images to. Depending on your problem you might want to use larger or smaller images.","59a2416d":"## An end-to-end example\n\nWe've checked out our data and found there's 750 training images, as well as 250 test images per class and they're all of various different shapes. \n\nIt's time to jump straight in the deep end.\n\nReading the [original dataset authors paper](https:\/\/data.vision.ee.ethz.ch\/cvl\/datasets_extra\/food-101\/static\/bossard_eccv14_food-101.pdf), we see they used a [Random Forest machine learning model](https:\/\/medium.com\/@williamkoehrsen\/random-forest-simple-explanation-377895a60d2d) and averaged 50.76% accuracy at predicting what different foods different images had in them.\n\nFrom now on, that 50.76% will be our baseline.\n\n> \ud83d\udd11 **Note:** A **baseline** is a score or evaluation metric you want to try and beat. Usually you'll start with a simple model, create a baseline and try to beat it by increasing the complexity of the model. A really fun way to learn machine learning is to find some kind of modelling paper with a published result and try to beat it.\n\nThe code in the following cell replicates and end-to-end way to model our `pizza_steak` dataset with a convolutional neural network (CNN) using the components listed above.\n\nThere will be a bunch of things you might not recognize but step through the code yourself and see if you can figure out what it's doing.\n\nWe'll go through each of the steps later on in the notebook.\n\nFor reference, the model we're using replicates TinyVGG, the computer vision architecture which fuels the [CNN explainer webpage](https:\/\/poloclub.github.io\/cnn-explainer\/).\n\n> \ud83d\udcd6 **Resource:** The architecture we're using below is a scaled-down version of [VGG-16](https:\/\/arxiv.org\/abs\/1505.06798), a convolutional neural network which came 2nd in the 2014 [ImageNet classification competition](http:\/\/image-net.org\/).","2afecd01":"Woah! That's looking much better, the loss curves are much closer to eachother. Although our model didn't perform as well on the augmented training set, it performed much better on the validation dataset.\n\nIt even looks like if we kept it training for longer (more epochs) the evaluation metrics might continue to improve.","8b135a92":"Our next step is to turn our data into **batches**.\n\nA **batch** is a small subset of the dataset a model looks at during training. For example, rather than looking at 10,000 images at one time and trying to figure out the patterns, a model might only look at 32 images at a time.\n\nIt does this for a couple of reasons:\n* 10,000 images (or more) might not fit into the memory of your processor (GPU).\n* Trying to learn the patterns in 10,000 images in one hit could result in the model not being able to learn very well.\n\nWhy 32?\n\nA [batch size of 32 is good for your health](https:\/\/twitter.com\/ylecun\/status\/989610208497360896?s=20). \n\nNo seriously, there are many different batch sizes you could use but 32 has proven to be very effective in many different use cases and is often the default for many data preprocessing functions.\n\nTo turn our data into batches, we'll first create an instance of [`ImageDataGenerator`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator) for each of our datasets.\n","dd76bdea":"Let's try it out. If we've done it right, using different images should lead to different outputs (rather than `chicken_curry` every time).","dfa8e550":"## Multi-class Classification\n\nWe've referenced the TinyVGG architecture from the CNN Explainer website multiple times through this notebook, however, the CNN Explainer website works with 10 different image classes, where as our current model only works with two classes (pizza and steak).\n\n> \ud83d\udee0 **Practice:** Before scrolling down, how do you think we might change our model to work with 10 classes of the same kind of images? Assume the data is in the same style as our two class problem.\n\nRemember the steps we took before to build our pizza \ud83c\udf55 vs. steak \ud83e\udd69 classifier?\n\nHow about we go through those steps again, except this time, we'll work with 10 different types of food.\n\n1. Become one with the data (visualize, visualize, visualize...)\n2. Preprocess the data (prepare it for a model)\n3. Create a model (start with a baseline)\n4. Fit the model\n5. Evaluate the model\n6. Adjust different parameters and improve model (try to beat your baseline)\n7. Repeat until satisfied\n\n![](https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/images\/misc-tensorflow-workflow-outline.png)\n*The workflow we're about to go through is a slightly modified version of the above image. As you keep going through deep learning problems, you'll find the workflow above is more of an outline than a step-by-step guide.*","d1effe71":"### 2. Preprocess the data (prepare it for a model)\n\nAfter going through a handful of images (it's good to visualize at least 10-100 different examples), it looks like our data directories are setup correctly.\n\nTime to preprocess the data.","62521e82":"Nice! Our model got the prediction right.\n\nThe only downside of working with food is this is making me hungry.\n\nLet's try one more image","f6155852":"What do you notice about the names of `model_1`'s layers and the layer names at the top of the [CNN explainer website](https:\/\/poloclub.github.io\/cnn-explainer\/)?\n\nI'll let you in on a little secret: we've replicated the exact architecture they use for their model demo.\n\nLook at you go! **You're already starting to replicate models you find in the wild.**\n\nNow there are a few new things here we haven't discussed, namely: \n* The [`ImageDataGenerator`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator) class and the `rescale` parameter\n* The [`flow_from_directory()`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator#flow_from_directory) method\n  * The `batch_size` parameter\n  * The `target_size` parameter\n* [`Conv2D`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Conv2D) layers (and the parameters which come with them)\n* [`MaxPool2D`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/MaxPool2D) layers (and their parameters).\n* The `steps_per_epoch` and `validation_steps` parameters in the `fit()` function\n\nBefore we dive into each of these, let's see what happens if we try to fit a model we've worked with previously to our data.\n","538b75dd":"Hmm... even with a simplifed model, it looks like our model is still dramatically overfitting the training data.\n\nWhat else could we try?\n\nHow about **data augmentation**?\n\nData augmentation makes it harder for the model to learn on the training data and in turn, hopefully making the patterns it learns more generalizable to unseen data.\n\nTo create augmented data, we'll recreate a new [`ImageDataGenerator`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator) instance, this time adding some parameters such as `rotation_range` and `horizontal_flip` to manipulate our images.","a4b587f0":"## Get the data\n\nBecause convolutional neural networks work so well with images, to learn more about them, we're going to start with a dataset of images.\n\nThe images we're going to work with are from the [Food-101 dataset](https:\/\/data.vision.ee.ethz.ch\/cvl\/datasets_extra\/food-101\/), a collection of 101 different categories of 101,000 (1000 images per category) real-world images of food dishes. \n\nTo begin, we're only going to use two of the categories, pizza \ud83c\udf55 and steak \ud83e\udd69 and build a binary classifier.\n\n> \ud83d\udd11 **Note:** To prepare the data we're using, preprocessing steps such as, moving the images into different subset folders, have been done. To see these preprocessing steps check out [the preprocessing notebook](https:\/\/github.com\/mrdbourke\/tensorflow-deep-learning\/blob\/main\/extras\/image_data_modification.ipynb).\n\nWe'll download the `pizza_steak` subset .zip file and unzip it.","f29f30d4":"### 7. Repeat until satisified\n\nWe've trained a few model's on our dataset already and so far they're performing pretty good.\n\nSince we've already beaten our baseline, there are a few things we could try to continue to improve our model:\n* Increase the number of model layers (e.g. add more convolutional layers).\n* Increase the number of filters in each convolutional layer (e.g. from 10 to 32, 64, or 128, these numbers aren't set in stone either, they are usually found through trial and error).\n* Train for longer (more epochs).\n* Finding an ideal learning rate.\n* Get more data (give the model more opportunities to learn).\n* Use **transfer learning** to leverage what another image model has learned and adjust it for our own use case.\n\nAdjusting each of these settings (except for the last two) during model development is usually referred to as **hyperparameter tuning**.\n\nYou can think of hyperparameter tuning as simialr to adjusting the settings on your oven to cook your favourite dish. Although your oven does most of the cooking for you, you can help it by tweaking the dials.\n\nLet's go back to right where we started and try our original model (`model_1` or the TinyVGG architecture from [CNN explainer](https:\/\/poloclub.github.io\/cnn-explainer\/)).\n","c43865a3":"Hmm, our training curves are looking good, but our model's performance on the training and test sets didn't improve much compared to the previous model.\n\nTaking another loook at the training curves, it looks like our model's performance might improve if we trained it a little longer (more epochs).\n\nPerhaps that's something you like to try?","c7f7a2ff":"> \ud83e\udd14 **Question:** Why didn't our model get very good results on the training set to begin with?\n\nIt's because when we created `train_data_augmented` we turned off data shuffling using `shuffle=False` which means our model only sees a batch of a single kind of images at a time. \n\nFor example, the pizza class gets loaded in first because it's the first class. Thus it's performance is measured on only a single class rather than both classes. The validation data performance improves steadily because it contains shuffled data.\n\nSince we only set `shuffle=False` for demonstration purposes (so we could plot the same augmented and non-augmented image), we can fix this by setting `shuffle=True` on future data generators.\n\nYou may have also noticed each epoch taking longer when training with augmented data compared to when training with non-augmented data (~25s per epoch vs. ~10s per epoch).\n\nThis is because the `ImageDataGenerator` instance augments the data as it's loaded into the model. The benefit of this is that it leaves the original images unchanged. The downside is that it takes longer to load them in.\n\n> \ud83d\udd11 **Note:** One possible method to speed up dataset manipulation would be to look into [TensorFlow's parrallel reads and buffered prefecting options](https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation).","f14b339c":"`chicken_curry` again? There must be something wrong...\n\nI think it might have to do with our `pred_and_plot` function.\n\nLet's makes a prediction without using the function and see where it might be going wrong.","6c8b87ad":"### 1. Import and become one with the data\n\nAgain, we've got a subset of the [Food101 dataset](https:\/\/www.kaggle.com\/dansbecker\/food-101). In addition to the pizza and steak images, we've pulled out another eight classes.\n","aa285ce8":"Looking at the image shape more closely, you'll see it's in the form `(Width, Height, Colour Channels)`.\n\nIn our case, the width and height vary but because we're dealing with colour images, the colour channels value is always 3. This is for different values of [red, green and blue (RGB) pixels](https:\/\/en.wikipedia.org\/wiki\/RGB_color_model).\n\nYou'll notice all of the values in the `img` array are between 0 and 255. This is because that's the possible range for red, green and blue values.\n\nFor example, a pixel with a value `red=0, green=0, blue=255` will look very blue.\n\nSo when we build a model to differentiate between our images of `pizza` and `steak`, it will be finding patterns in these different pixel values which determine what each class looks like.\n\n> \ud83d\udd11 **Note:** As we've discussed before, many machine learning models, including neural networks prefer the values they work with to be between 0 and 1. Knowing this, one of the most common preprocessing steps for working with images is to **scale** (also referred to as **normalize**) their pixel values by dividing the image arrays by 255.","b8273ab7":"# Convolutional Neural Networks and Computer Vision with TensorFlow\n\nSo far we've covered the basics of TensorFlow and built a handful of models to work across different problems.\n\nNow we're going to get specific and see how a special kind of neural network, [convolutional neural networks (CNNs)](https:\/\/en.wikipedia.org\/wiki\/Convolutional_neural_network) can be used for computer vision (detecting patterns in visual data).\n\n> \ud83d\udd11 **Note:** In deep learning, many different kinds of model architectures can be used for different problems. For example, you could use a convolutional neural network for making predictions on image data and\/or text data. However, in practice some architectures typically work better than others.\n\nFor example, you might want to:\n* Classify whether a picture of food contains pizza \ud83c\udf55 or steak \ud83e\udd69 (we're going to do this)\n* Detect whether or not an object appears in an image (e.g. did a specific car pass through a security camera?)\n\nIn this notebook, we're going to follow the TensorFlow modelling workflow we've been following so far whilst learning about how to build and use CNNs.\n\n## What we're going to cover\n\nSpecifically, we're going to go through the follow with TensorFlow:\n\n- Getting a dataset to work with\n- Architecture of a convolutional neural network\n- A quick end-to-end example (what we're working towards)\n- Steps in modelling for binary image classification with CNNs\n  - Becoming one with the data\n  - Preparing data for modelling\n  - Creating a CNN model (starting with a baseline)\n  - Fitting a model (getting it to find patterns in our data)\n  - Evaluating a model\n  - Improving a model\n  - Making a prediction with a trained model\n- Steps in modelling for multi-class image classification with CNNs\n - Same as above (but this time with a different dataset)\n\n## How you can use this notebook\n\nYou can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.\n\nWrite all of the code yourself.\n\nYes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?\n\nYou don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n\nDon't worry if you make mistakes, we all do. The way to get better and make less mistakes is to **write more code**.","4a2838d9":"Hmmm... our model ran but it doesn't seem like it learned anything. It only reaches 50% accuracy on the training and test sets which in a binary classification problem is as good as guessing.\n\nLet's see the architecture.","dfdd4343":"Since our model takes in images of shapes `(224, 224, 3)`, we've got to reshape our custom image to use it with our model.\n\nTo do so, we can import and decode our image using [`tf.io.read_file`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/read_file) (for readining files) and [`tf.image`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/image) (for resizing our image and turning it into a tensor).\n\n> \ud83d\udd11 **Note:** For your model to make predictions on unseen data, for example, your own custom images, the custom image has to be in the same shape as your model has been trained on. In more general terms, to make predictions on custom data it has to be in the same form that your model has been trained on.","8b2175ec":"Do you notice what's going on here with the output shape in each `MaxPooling2D` layer?\n\nIt gets halved each time. This is effectively the `MaxPooling2D` layer taking the outputs of each Conv2D layer and saying \"I only want the most important features, get rid of the rest\".\n\nThe bigger the `pool_size` parameter, the more the max pooling layer will squeeze the features out of the image. However, too big and the model might not be able to learn anything.\n\nThe results of this pooling are seen in a major reduction of total trainable parameters (8,861 in `model_5` and 477,431 in `model_4`).\n\nTime to check out the loss curves.","34c83496":"And get the class names from the subdirectories.","e3442776":"## Making a prediction with our trained model\n\nWhat good is a model if you can't make predictions with it?\n\nLet's first remind ourselves of the classes our multi-class model has been trained on and then we'll download some of own custom images to work with.","a774ab81":"Looking good!\n\nWe'll now setup the training and test directory paths.","60e8f85f":"Now let's check out our TinyVGG model's performance.","dc1f2af7":"My gosh, the number of trainable parameters has increased even more than `model_2`. And even with close to 500x (~15,000,000 vs. ~31,000) more trainable parameters, `model_3` still doesn't out perform `model_1`.\n\nThis goes to show the power of convolutional neural networks and their ability to learn patterns despite using less parameters.","3a6d7a51":"Better than talk about data augmentation, how about we see it?\n\n(remember our motto? visualize, visualize, visualize...)","829bd700":"Great! We've got a simple convolutional neural network architecture ready to go.\n\nAnd it follows the typical CNN structure of:\n\n```\nInput -> Conv + ReLU layers (non-linearities) -> Pooling layer -> Fully connected (dense layer) as Output\n```\n\nLet's discuss some of the components of the `Conv2D` layer:\n\n* The \"`2D`\" means our inputs are two dimensional (height and width), even though they have 3 colour channels, the convolutions are run on each channel invididually.\n* `filters` - these are the number of \"feature extractors\" that will be moving over our images.\n* `kernel_size` - the size of our filters, for example, a `kernel_size` of `(3, 3)` (or just 3) will mean each filter will have the size 3x3, meaning it will look at a space of 3x3 pixels each time. The smaller the kernel, the more fine-grained features it will extract.\n* `stride` - the number of pixels a `filter` will move across as it covers the image. A `stride` of 1 means the filter moves across each pixel 1 by 1. A `stride` of 2 means it moves 2 pixels at a time.\n* `padding` - this can be either `'same'` or `'valid'`, `'same'` adds zeros the to outside of the image so the resulting output of the convolutional layer is the same as the input, where as `'valid'` (default) cuts off excess pixels where the `filter` doesn't fit (e.g. 224 pixels wide divided by a kernel size of 3 (224\/3 = 74.6) means a single pixel will get cut off the end.\n\nWhat's a \"feature\"?\n\nA **feature** can be considered any significant part of an image. For example, in our case, a feature might be the circular shape of pizza. Or the rough edges on the outside of a steak.\n\nIt's important to note that these **features** are not defined by us, instead, the model learns them as it applies different filters across the image.\n\n> \ud83d\udcd6 **Resources:** For a great demonstration of these in action, be sure to spend some time going through the following:\n  * [CNN Explainer Webpage](https:\/\/poloclub.github.io\/cnn-explainer\/) - a great visual overview of many of the concepts we're replicating here with code.\n  * [A guide to convolutional arithmetic for deep learning](https:\/\/arxiv.org\/pdf\/1603.07285.pdf) - a phenomenal introduction to the math going on behind the scenes of a convolutional neural network.\n  * For a great explanation of padding, see this [Stack Overflow answer](https:\/\/stackoverflow.com\/a\/39371113\/7900723).\n\nNow our model is ready, let's compile it.","93a75e1a":"## Inspect the data (become one with it)\n\nA very crucial step at the beginning of any machine learning project is becoming one with the data. This usually means plenty of visualizing and folder scanning to understand the data you're working with.\n\nWtih this being said, let's inspect the data we just downloaded.\n\nThe file structure has been formatted to be in a typical format you might use for working with images.\n\nMore specifically:\n* A `train` directory which contains all of the images in the training dataset with subdirectories each named after a certain class containing images of that class.\n* A `test` directory with the same structure as the `train` directory.\n\n```\nExample of file structure\n\npizza_steak <- top level folder\n\u2514\u2500\u2500\u2500train <- training images\n\u2502   \u2514\u2500\u2500\u2500pizza\n\u2502   \u2502   \u2502   1008104.jpg\n\u2502   \u2502   \u2502   1638227.jpg\n\u2502   \u2502   \u2502   ...      \n\u2502   \u2514\u2500\u2500\u2500steak\n\u2502       \u2502   1000205.jpg\n\u2502       \u2502   1647351.jpg\n\u2502       \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500test <- testing images\n\u2502   \u2514\u2500\u2500\u2500pizza\n\u2502   \u2502   \u2502   1001116.jpg\n\u2502   \u2502   \u2502   1507019.jpg\n\u2502   \u2502   \u2502   ...      \n\u2502   \u2514\u2500\u2500\u2500steak\n\u2502       \u2502   100274.jpg\n\u2502       \u2502   1653815.jpg\n\u2502       \u2502   ...    \n ```\n\nLet's inspect each of the directories we've downloaded.\n\nTo so do, we can use the command `ls` which stands for list.","eacdcfe1":"Hmm... it looks like our model got the prediction wrong, how about we try another?","a1be8e17":"> \ud83e\udd14 **Question:** What's data augmentation?\n\n**Data augmentation** is the process of altering our training data, leading to it having more diversity and in turn allowing our models to learn more generalizable patterns. Altering might mean adjusting the rotation of an image, flipping it, cropping it or something similar.\n\nDoing this simulates the kind of data a model might be used on in the real world.\n\nIf we're building a pizza vs. steak application, not all of the images our users take might be in similar setups to our training data. Using data augmentation gives us another way to prevent overfitting and in turn make our model more generalizable.\n\n> \ud83d\udd11 **Note:** Data augmentation is usally only performed on the training data. Using the `ImageDataGenerator` built-in data augmentation parameters our images are left as they are in the directories but are randomly manipulated when loaded into the model.","85232263":"The `ImageDataGenerator` class helps us prepare our images into batches as well as perform transformations on them as they get loaded into the model.\n\nYou might've noticed the `rescale` parameter. This is one example of the transformations we're doing.\n\nRemember from before how we imported an image and it's pixel values were between 0 and 255?\n\nThe `rescale` parameter, along with `1\/255.` is like saying \"divide all of the pixel values by 255\". This results in all of the image being imported and their pixel values being normalized (converted to be between 0 and 1).\n\n> \ud83d\udd11 **Note:** For more transformation options such as data augmentation (we'll see this later), refer to the [`ImageDataGenerator` documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator).\n\nNow we've got a couple of ImageDataGenerator instances, we can load our images from their respective directories using the [`flow_from_directory`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator#flow_from_directory) method.","4d4b2bad":"The ideal position for these two curves is to follow each other. If anything, the validation curve should be slightly under the training curve. If there's a large gap between the training curve and validation curve, it means your model is probably overfitting.","13be27c5":"Beautiful, now let's get some of our custom images.\n\nIf you're using Google Colab, you could also upload some of your own images via the files tab.","94105737":"And again, it's predicting `chicken_curry` for some reason.\n\nHow about one more?","de6fbe78":"### 4. Fit a model\n\nNow we've got a model suited for working with multiple classes, let's fit it to our data.","9a3395ae":"Now we've got augmented data, let's see how it works with the same model as before (`model_10`).\n\nRather than rewrite the model from scratch, we can clone it using a handy function in TensorFlow called [`clone_model`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/models\/clone_model) which can take an existing model and rebuild it in the same format. \n\nThe cloned version will not include any of the weights (patterns) the original model has learned. So when we train it, it'll be like training a model from scratch.\n\n> \ud83d\udd11 **Note:** One of the key practices in deep learning and machine learning in general is to **be a serial experimenter**. That's what we're doing here. Trying something, seeing if it works, then trying something else. A good experiment setup also keeps track of the things you change, for example, that's why we're using the same model as before but with different data. The model stays the same but the data changes, this will let us know if augmented training data has any influence over performance.","ea5535d1":"Much better! There must be something up with our `pred_and_plot` function.\n\nAnd I think I know what it is.\n\nThe `pred_and_plot` function was designed to be used with binary classification models where as our current model is a multi-class classification model.\n\nThe main difference lies in the output of the `predict` function.","c5168f2f":"How about we visualize an image from the training set?","fce41384":"Since our model has a `'softmax'` activation function and 10 output neurons, it outputs a prediction probability for each of the classes in our model.\n\nThe class with the highest probability is what the model believes the image contains.\n\nWe can find the maximum value index using [`argmax`](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.argmax.html) and then use that to index our `class_names` list to output the predicted class.","219ce254":"\n\nNice! After 5 epochs, our model beat the baseline score of 50.76% accuracy (our model got ~85% accuaracy on the training set and ~85% accuracy on the test set).\n\nHowever, our model only went through a binary classificaiton problem rather than all of the 101 classes in the Food101 dataset, so we can't directly compare these metrics. That being said, the results so far show that our model is learning something.\n\n> \ud83d\udee0 **Practice:** Step through each of the main blocks of code in the cell above, what do you think each is doing? It's okay if you're not sure, we'll go through this soon. In the meantime, spend 10-minutes playing around the incredible [CNN explainer website](https:\/\/poloclub.github.io\/cnn-explainer\/). What do you notice about the layer names at the top of the webpage?\n\n\nSince we've already fit a model, let's check out its architecture.","c62a35c9":"Notice with `model_7` how the performance on the training dataset improves almost immediately compared to `model_6`. This is because we shuffled the training data as we passed it to the model using the parameter `shuffle=True` in the `flow_from_directory` method.\n\nThis means the model was able to see examples of both pizza and steak images in each batch and in turn be evaluated on what it learned from both images rather than just one kind.\n\nAlso, our loss curves look a little bit smoother with shuffled data (comparing `history_6` to `history_7`).","e4a6962b":"### 6. Adjust the model parameters\n\nDue to its performance on the training data, it's clear our model is learning something. However, performing well on the training data is like going well in the classroom but failing to use your skills in real life.\n\nIdeally, we'd like our model to perform as well on the test data as it does on the training data.\n\nSo our next steps will be to try and prevent our model overfitting. A couple of ways to prevent overfitting include:\n\n- **Get more data** - Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples.\n- **Simplify model** - If the current model is already overfitting the training data, it may be too complicated of a model. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer.\n- **Use data augmentation** - Data augmentation manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data.\n- **Use transfer learning** - Transfer learning involves leverages the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images.\n\n> \ud83d\udd11 **Note:** Preventing overfitting is also referred to as **regularization**.\n\nIf you've already got an existing dataset, you're probably most likely to try one or a combination of the last three above options first.\n\nSince collecting more data would involve us manually taking more images of food, let's try the ones we can do from right within the notebook.\n\nHow about we simplify our model first?\n\nTo do so, we'll remove two of the convolutional layers, taking the total number of convolutional layers from four to two.","ac030f43":"> \ud83e\udd14 **Note:** If the cell above takes more than ~12 seconds per epoch to run, you might not be using a GPU accelerator. If you're using a Colab notebook, you can access a GPU accelerator by going to Runtime -> Change Runtime Type -> Hardware Accelerator and select \"GPU\". After doing so, you might have to rerun all of the above cells as changing the runtime type causes Colab to have to reset.\n\n","a1b40aac":"Okay, it looks like our model with max pooling (model_5) is performing worse on the training set but better on the validation set.\n\nBefore we checkout its training curves, let's check out its architecture.\n\n","492b171f":"Since we're working on a binary classification problem (pizza vs. steak), the `loss` function we're using is `'binary_crossentropy'`, if it was mult-iclass, we might use something like `'categorical_crossentropy'`.\n\nAdam with all the default settings is our optimizer and our evaluation metric is accuracy.","632629db":"Why do you think each epoch takes longer than when working with only two classes of images?\n\nIt's because we're now dealing with more images than we were before. We've got 10 classes with 750 training images and 250 validation images each totalling 10,000 images. Where as when we had two classes, we had 1500 training images and 500 validation images, totalling 2000.\n\nThe intuitive reasoning here is the more data you have, the longer a model will take to find patterns.","1fb01384":"Nice! We can see the training curves get a lot closer to eachother. However, our the validation loss looks to start increasing towards the end and in turn potentially leading to overfitting.\n\nTime to dig into our bag of tricks and try another method of overfitting prevention, data augmentation.\n\nFirst, we'll see how it's done with code then we'll discuss what it's doing.\n\nTo implement data augmentation, we'll have to reinstantiate our [`ImageDataGenerator`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator) instances."}}