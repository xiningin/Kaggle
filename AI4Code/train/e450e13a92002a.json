{"cell_type":{"5c90a194":"code","039c4894":"code","9571b3b0":"code","a210999a":"code","20a0b310":"code","0df8cf97":"code","d8371f1d":"code","66b4b822":"code","a89d62f0":"code","912066e2":"code","41be03bc":"code","4620c96f":"code","d23ef6ad":"code","32b3a210":"code","073e4b7d":"code","9c3979e2":"code","49d9c53a":"code","cc7666ea":"code","f8c6520a":"code","c4829369":"code","a67b764d":"code","9756a8b2":"code","98ddbadd":"code","52a5485a":"code","6406561e":"code","3ca65604":"code","90549bb8":"code","06731513":"code","6f337bfa":"markdown","751ea950":"markdown","544cca84":"markdown","666d5b99":"markdown","90e2e048":"markdown","e550337d":"markdown","5d7a329d":"markdown","28ca0b88":"markdown","c8b362d1":"markdown","01ebec74":"markdown","239586d5":"markdown","f2e5085c":"markdown","d030748b":"markdown","5f4f3b70":"markdown","a07e3b93":"markdown","014f4a9a":"markdown","aee99f5f":"markdown","f4fa047d":"markdown","8c82f9b0":"markdown","cf361ac0":"markdown","c7965841":"markdown","ae46d9de":"markdown","f0eede9d":"markdown","72159322":"markdown","8ffac2e9":"markdown","7abd8f56":"markdown","c1c858cc":"markdown","eeff763c":"markdown","753ae88d":"markdown","27ec0da8":"markdown","67cbeb6c":"markdown"},"source":{"5c90a194":"import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\n#import cufflinks and offline mode\nimport cufflinks as cf\ncf.go_offline()\nimport nibabel\nfrom nibabel.testing import data_path\n\n# Venn diagram\nfrom matplotlib_venn import venn2\nimport re\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = stopwords.words('english')\nimport gc\nimport h5py\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","039c4894":"import os\nbp = '\/kaggle\/input\/trends-assessment-prediction'\nprint(os.listdir(bp))","9571b3b0":"print('Reading data...')\nloading_data = pd.read_csv(bp+'\/loading.csv')\ntrain_data = pd.read_csv(bp+'\/train_scores.csv')\nsample_submission = pd.read_csv(bp+'\/sample_submission.csv')\nfnc_df=pd.read_csv(bp+'\/fnc.csv')\nprint('Reading data completed')\nloading_data.head(5)","a210999a":"train_data.head()","20a0b310":"sample_submission.head()","0df8cf97":"fnc_df.head()","d8371f1d":"print('Size of loading_data', loading_data.shape)\nprint('Size of train_data', train_data.shape)\nprint('Size of sample_submission', sample_submission.shape)\nprint('test size:', len(sample_submission)\/5)","66b4b822":"fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n\nsns.distplot(train_data['age'], ax=ax[0])\nax[0].set_title('Age')\n\nsns.distplot(train_data['domain1_var1'], ax=ax[1])\nax[1].set_title('Domain 1 - Var 1')\n\nsns.distplot(train_data['domain1_var2'], ax=ax[2])\nax[2].set_title('Domain 1 - Var 2')\n\nsns.distplot(train_data['domain2_var1'], ax=ax[3])\nax[3].set_title('Domain 2 - Var 1')\n\nsns.distplot(train_data['domain2_var2'], ax=ax[4])\nax[4].set_title('Domain 2 - Var 2')\n\nfig.suptitle('Target distributions', fontsize=14)","a89d62f0":"display(loading_data.head())\ndisplay(loading_data.describe())","912066e2":"display(train_data.head())\ndisplay(train_data.describe())","41be03bc":"from scipy.stats import skew, kurtosis\n\nprint(\"Kurtosis (Fisher's definition)\")\ntrain_data.kurtosis()","4620c96f":"fig, ax = plt.subplots(9, 3, figsize=(20, 25))\n\nfor row in range(9):\n    for col in range(3):\n        sns.distplot(loading_data.iloc[:, row+col+1], ax=ax[row][col])\n\nfig.suptitle('Source-based morphometry loadings distribution', fontsize=14)\nfig.tight_layout(rect=[0, 0.03, 1, 0.95], pad=3.0)\nfig.show()","d23ef6ad":"targets = list(train_data.columns[1:])\ntargets","32b3a210":"# checking missing data\ntotal = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()\/train_data.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","073e4b7d":"# checking missing data\ntotal = loading_data.isnull().sum().sort_values(ascending = False)\npercent = (loading_data.isnull().sum()\/loading_data.isnull().count()*100).sort_values(ascending = False)\nmissing_test_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test_data.head()","9c3979e2":"targets= loading_data.columns[1:]\nfig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(-0.05, 0.05, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(loading_data[col], label=col, kde=False, bins=bins, ax=ax)\n    # ax.set_title(col)\n#     ax.set_xlim([0, 1])\n#     ax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","49d9c53a":"targets= train_data.columns[1:]\nfig, axes = plt.subplots(1, 5, figsize=(18, 4))\naxes = axes.ravel()\nbins = np.linspace(0, 100, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train_data[col], label=col, kde=False, bins=bins, ax=ax)\n    # ax.set_title(col)\n#     ax.set_xlim([0, 1])\n#     ax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","cc7666ea":"fig, ax = plt.subplots(figsize=(8, 6))\ncols = loading_data.columns[1:]\nsns.heatmap(loading_data[cols].corr(), ax=ax)","f8c6520a":"merged = train_data.merge(loading_data)","c4829369":"from scipy.spatial.distance import cdist\n\ndef calc_corr(df, x_cols, y_cols):\n    arr1 = df[x_cols].T.values\n    arr2 = df[y_cols].T.values\n    corr_df = pd.DataFrame(1 - cdist(arr2, arr1, metric='correlation'), index=y_cols, columns=x_cols)\n    return corr_df\n\ninput_cols = merged.columns[6:]\noutput_cols = merged.columns[1:6]\n\ncorr_df = calc_corr(merged, input_cols, output_cols)\nfig, ax = plt.subplots(figsize=(10, 2))\nsns.heatmap(corr_df, ax=ax)","a67b764d":"features_df = pd.merge(train_data, loading_data, on=['Id'], how='left')\nfeatures_df.head()","9756a8b2":"features_df = pd.merge(features_df, fnc_df, how='left', on='Id')\nfeatures_df.head()","98ddbadd":"!wget https:\/\/github.com\/Chaogan-Yan\/DPABI\/raw\/master\/Templates\/ch2better.nii","52a5485a":"import nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\nimport random\n\nmask_filename = '..\/input\/trends-assessment-prediction\/fMRI_mask.nii'\nsmri_filename = 'ch2better.nii'\n\nmask_niimg = nl.image.load_img(mask_filename)\n\ndef load_subject(filename, mask_niimg):\n    subject_data = None\n    \n    with h5py.File(subject_filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n        \n    subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n    subject_niimg = nl.image.new_img_like(mask_niimg, subject_data, affine=mask_niimg.affine, copy_header=True)\n    \n    return subject_niimg ","6406561e":"fMRI_train_data_path = '..\/input\/trends-assessment-prediction\/fMRI_train\/'\nfilenames = random.choices(os.listdir(fMRI_train_data_path), k=4)","3ca65604":"for filename in filenames:\n    subject_filename = os.path.join(fMRI_train_data_path, filename)\n    subject_niimg = load_subject(subject_filename, mask_niimg)\n\n    print(\"Image shape is %s\" % (str(subject_niimg.shape)))\n    num_components = subject_niimg.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n\n    nlplt.plot_prob_atlas(subject_niimg, \n                          bg_img=smri_filename,\n                          view_type='filled_contours',\n                          draw_cross=False,\n                          title='All %d spatial maps' % num_components,\n                          threshold='auto')","90549bb8":"!ls \/kaggle\/input","06731513":"sub = pd.read_csv('..\/input\/trends-assessment-prediction\/sample_submission.csv')\nsub.to_csv('sub.csv', index=False)","6f337bfa":"Let's start with fnc data","751ea950":"### There are 5877 training data and 5877 test data. \n\n### For submission, we need to fill five rows for each entry (5877x5 rows):\n\n> age\t\n\n> domain1_var1\n\n> domain1_var2\n\n> domain2_var1\n\n> domain2_var2","544cca84":"# More To Come. Stay Tuned. !!","666d5b99":"\n# More To Come. Stay Tuned. !!\nIf there are any suggestions\/changes you would like to see in the Kernel please let me know :). Appreciate every ounce of help!\n\n**This notebook will always be a work in progress.** Please leave any comments about further improvements to the notebook! Any feedback or constructive criticism is greatly appreciated!. **If you like it or it helps you , you can upvote and\/or leave a comment :).**\n","90e2e048":"# <a id='6-2'>6.2 Mean submission<\/a>","e550337d":"**checking missing data in loading_data **","5d7a329d":"# <a id='5-2'>5.2 Distribution of target variables in train data<\/a>","28ca0b88":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1095143%2F47c74960e6540f11287e1e271438e029%2FTReNDS.png?generation=1587603283379241&alt=media)","c8b362d1":"Refrences\n1. https:\/\/www.kaggle.com\/moradnejad\/trends-eda-fe-submission#-4-Check-for-missing-data\n2. https:\/\/www.kaggle.com\/nischaydnk\/beginners-trends-neuroimaging-decent-score\/comments#822577","01ebec74":"Now let us estimate the correlation among these","239586d5":"# <a id='3'>3. Glimpse of Data<\/a>","f2e5085c":"# <a id='4'> 4 Check for missing data<\/a>","d030748b":"# <a id='6-1'>6.1 Baseline submission<\/a>","5f4f3b70":"In this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward.","a07e3b93":"**sample_submission**","014f4a9a":" ## <a id='2-1'>2.1 Load libraries<\/a>","aee99f5f":"# Visualizing 3D maps","f4fa047d":"# <a id='2-2'>2.2 Reading Data<\/a>","8c82f9b0":"# <a id='1'>1. Introduction<\/a>","cf361ac0":"Lets do some statistical analysis here, I have used kurtosis","c7965841":"\n- <a href='#1'>1. Introduction<\/a>  \n- <a href='#2'>2. Retrieving the Data<\/a>\n     - <a href='#2-1'>2.1 Load libraries<\/a>\n     - <a href='#2-2'>2.2 Read the Data<\/a>\n- <a href='#3'>3. Glimpse of Data<\/a>\n     - <a href='#3-1'>3.1 Overview of tables<\/a>\n     - <a href='#3-2'>3.2 Statistical overview of the Data<\/a>\n- <a href='#4'>4. Check for missing data<\/a>\n- <a href='#5'>5. Data Exploration<\/a>\n    - <a href='#5-1'>5.1 Distribution of input variables in loading_data<\/a>\n    - <a href='#5-2'>5.2 Distribution of target variables in train data<\/a>\n- <a href='#6'>6. Sample submissions<\/a>\n    - <a href='#6-1'>6.1 Baseline submission<\/a>\n    - <a href='#6-2'>6.2 Mean submission<\/a>","ae46d9de":"**train_data**\n\n\n##### This is actually only the target values for the training data set. The real training data is in loading data (partially). So, I will only focus on that file.","f0eede9d":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1537731%2Fa5fdbe17ca91e6713d2880887232c81a%2FScreen%20Shot%202019-12-09%20at%2011.25.31%20AM.png?generation=1575920121028151&alt=media)","72159322":"## <a id='3-1'>3.1 Overview of tables<\/a>","8ffac2e9":"**loading_data**","7abd8f56":"**checking missing data in train_data **","c1c858cc":"**Target variables**","eeff763c":"first we merge the training and loading data for features","753ae88d":"# <a id='6'>6. Sample submissions<\/a>","27ec0da8":" # <a id='2'>2. Retrieving the Data<\/a>","67cbeb6c":"# Plotting the Data\n"}}