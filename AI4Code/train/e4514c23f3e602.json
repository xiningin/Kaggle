{"cell_type":{"d2f6c08b":"code","1312ff42":"code","bdb7de0e":"code","9d0ce864":"code","8738c98b":"code","e724160b":"code","c9fd6736":"code","4fc73203":"code","a53eec97":"code","714daf8b":"code","f8a6437b":"code","0ec873ff":"code","c651c68e":"code","82b9e8ba":"code","63eb05ee":"code","1785a50c":"code","77c41081":"code","b48d6d31":"code","7fa0d2e0":"code","c04db2f8":"code","dc7b11d8":"code","7f6d9081":"code","1cfbe4cf":"code","bd9f9891":"code","4e933a3b":"markdown","15a8b279":"markdown","f38d03b9":"markdown","c9a1ca92":"markdown","5cbfe41a":"markdown"},"source":{"d2f6c08b":"import numpy as np\nimport pandas as pd\nimport os\nfrom keras.models import Model, Sequential, load_model\nfrom keras.layers import Dense, Input\nfrom keras.optimizers import Adam\nfrom tqdm import tqdm_notebook\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/trackml\/\"))\nprefix='..\/input\/trackml-particle-identification\/'\nimport zipfile","1312ff42":"def init_model(fs = 10):\n    model = Sequential()\n    model.add(Dense(800, activation='selu', input_shape=(fs,)))\n    model.add(Dense(400, activation='selu'))\n    model.add(Dense(400, activation='selu'))\n    model.add(Dense(400, activation='selu'))\n    model.add(Dense(200, activation='selu'))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n\ndef get_event(event):\n    zf = zipfile.ZipFile('..\/input\/trackml-particle-identification\/train_1.zip')\n    hits= pd.read_csv(zf.open('train_1\/%s-hits.csv'%event))\n    cells= pd.read_csv(zf.open('train_1\/%s-cells.csv'%event))\n    truth= pd.read_csv(zf.open('train_1\/%s-truth.csv'%event))\n    particles= pd.read_csv(zf.open('train_1\/%s-particles.csv'%event))\n    return hits, cells, truth, particles","bdb7de0e":"# you can jump to step4 for test only.\ntrain = False\nif train:\n    Train = []\n    for i in tqdm_notebook(range(10,20)):\n        event = 'event0000010%02d'%i\n        hits, cells, truth, particles = get_event(event)\n        hit_cells = cells.groupby(['hit_id']).value.count().values\n        hit_value = cells.groupby(['hit_id']).value.sum().values\n        features = np.hstack((hits[['x','y','z']]\/1000, hit_cells.reshape(len(hit_cells),1)\/10,hit_value.reshape(len(hit_cells),1)))\n        particle_ids = truth.particle_id.unique()\n        particle_ids = particle_ids[np.where(particle_ids!=0)[0]]\n\n        pair = []\n        for particle_id in particle_ids:\n            hit_ids = truth[truth.particle_id == particle_id].hit_id.values-1\n            for i in hit_ids:\n                for j in hit_ids:\n                    if i != j:\n                        pair.append([i,j])\n        pair = np.array(pair)   \n        Train1 = np.hstack((features[pair[:,0]], features[pair[:,1]], np.ones((len(pair),1))))\n\n        if len(Train) == 0:\n            Train = Train1\n        else:\n            Train = np.vstack((Train,Train1))\n\n        n = len(hits)\n        size = len(Train1)*3\n        p_id = truth.particle_id.values\n        i =np.random.randint(n, size=size)\n        j =np.random.randint(n, size=size)\n        pair = np.hstack((i.reshape(size,1),j.reshape(size,1)))\n        pair = pair[((p_id[i]==0) | (p_id[i]!=p_id[j]))]\n\n        Train0 = np.hstack((features[pair[:,0]], features[pair[:,1]], np.zeros((len(pair),1))))\n\n        print(event, Train1.shape)\n\n        Train = np.vstack((Train,Train0))\n    del Train0, Train1\n\n    np.random.shuffle(Train)\n    print(Train.shape)","9d0ce864":"if train:\n    model = init_model()","8738c98b":"if train:\n    lr=-5\n    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=1, verbose=2, validation_split=0.05, shuffle=True)","e724160b":"if train:\n    lr=-4\n    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=20, verbose=2, validation_split=0.05, shuffle=True)","c9fd6736":"if train:\n    lr=-5\n    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=3, verbose=2, validation_split=0.05, shuffle=True)","4fc73203":"# if you skip step2, you still need to run step1 to get training data.\nif train:\n    try:\n        model\n    except NameError:\n        print('load model')\n        model = load_model('..\/input\/trackml\/my_model.h5')","a53eec97":"if train:\n    Train_hard = []\n\n    for i in tqdm_notebook(range(10,20)):\n\n        event = 'event0000010%02d'%i\n        hits, cells, truth, particles = get_event(event)\n        hit_cells = cells.groupby(['hit_id']).value.count().values\n        hit_value = cells.groupby(['hit_id']).value.sum().values\n        features = np.hstack((hits[['x','y','z']]\/1000, hit_cells.reshape(len(hit_cells),1)\/10,hit_value.reshape(len(hit_cells),1)))\n\n        size=30000000\n        n = len(truth)\n        i =np.random.randint(n, size=size)\n        j =np.random.randint(n, size=size)\n        p_id = truth.particle_id.values\n        pair = np.hstack((i.reshape(size,1),j.reshape(size,1)))\n        pair = pair[((p_id[i]==0) | (p_id[i]!=p_id[j]))]\n\n        Train0 = np.hstack((features[pair[:,0]], features[pair[:,1]], np.zeros((len(pair),1))))\n\n        pred = model.predict(Train0[:,:-1], batch_size=20000)\n        s = np.where(pred>0.5)[0]\n\n        print(event, len(Train0), len(s))\n\n        if len(Train_hard) == 0:\n            Train_hard = Train0[s]\n        else:\n            Train_hard = np.vstack((Train_hard,Train0[s]))\n    del Train0\n    print(Train_hard.shape)","714daf8b":"if train:\n    Train = np.vstack((Train,Train_hard))\n    np.random.shuffle(Train)\n    print(Train.shape)","f8a6437b":"if train:\n    lr=-4\n    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=30, verbose=2, validation_split=0.05, shuffle=True)","0ec873ff":"if train:\n    lr=-5\n    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=10, verbose=2, validation_split=0.05, shuffle=True)","c651c68e":"if train:\n    lr=-6\n    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=2, verbose=2, validation_split=0.05, shuffle=True)","82b9e8ba":"try:\n    model\nexcept NameError:\n    print('load model')\n    model = load_model('..\/input\/trackml\/my_model_h.h5')","63eb05ee":"event = 'event000001001'\nhits, cells, truth, particles = get_event(event)\nhit_cells = cells.groupby(['hit_id']).value.count().values\nhit_value = cells.groupby(['hit_id']).value.sum().values\nfeatures = np.hstack((hits[['x','y','z']]\/1000, hit_cells.reshape(len(hit_cells),1)\/10,hit_value.reshape(len(hit_cells),1)))\ncount = hits.groupby(['volume_id','layer_id','module_id'])['hit_id'].count().values\nmodule_id = np.zeros(len(hits), dtype='int32')\n\nfor i in range(len(count)):\n    si = np.sum(count[:i])\n    module_id[si:si+count[i]] = i","1785a50c":"def get_path(hit, mask, thr):\n    path = [hit]\n    a = 0\n    while True:\n        c = get_predict(path[-1], thr\/2)\n        mask = (c > thr)*mask\n        mask[path[-1]] = 0\n        \n        if 1:\n            cand = np.where(c>thr)[0]\n            if len(cand)>0:\n                mask[cand[np.isin(module_id[cand], module_id[path])]]=0\n                \n        a = (c + a)*mask\n        if a.max() < thr*len(path):\n            break\n        path.append(a.argmax())\n    return path\n\ndef get_predict(hit, thr=0.5):\n    Tx = np.zeros((len(truth),10))\n    Tx[:,5:] = features\n    Tx[:,:5] = np.tile(features[hit], (len(Tx), 1))\n    pred = model.predict(Tx, batch_size=len(Tx))[:,0]\n    # TTA\n    idx = np.where(pred > thr)[0]\n    Tx2 = np.zeros((len(idx),10))\n    Tx2[:,5:] = Tx[idx,:5]\n    Tx2[:,:5] = Tx[idx,5:]    \n    pred1 = model.predict(Tx2, batch_size=len(idx))[:,0]\n    pred[idx] = (pred[idx] + pred1)\/2\n    return pred","77c41081":"# select one hit to construct a track\nfor hit in range(3):\n    path = get_path(hit, np.ones(len(truth)), 0.95)\n    gt = np.where(truth.particle_id==truth.particle_id[hit])[0]\n    print('hit_id = ', hit+1)\n    print('reconstruct :', path)\n    print('ground truth:', gt.tolist())","b48d6d31":"# Predict all pairs for reconstruct by all hits. (takes 2.5hr but can skip)\nskip_predict = True\n\nif skip_predict == False:\n    TestX = np.zeros((len(features), 10))\n    TestX[:,5:] = features\n\n    # for TTA\n    TestX1 = np.zeros((len(features), 10))\n    TestX1[:,:5] = features\n\n    preds = []\n\n    for i in tqdm_notebook(range(len(features)-1)):\n        TestX[i+1:,:5] = np.tile(features[i], (len(TestX)-i-1, 1))\n\n        pred = model.predict(TestX[i+1:], batch_size=20000)[:,0]                \n        idx = np.where(pred>0.2)[0]\n\n        if len(idx) > 0:\n            TestX1[idx+i+1,5:] = TestX[idx+i+1,:5]\n            pred1 = model.predict(TestX1[idx+i+1], batch_size=20000)[:,0]\n            pred[idx] = (pred[idx]+pred1)\/2\n\n        idx = np.where(pred>0.5)[0]\n\n        preds.append([idx+i+1, pred[idx]])\n\n        #if i==0: print(preds[-1])\n\n    preds.append([np.array([], dtype='int64'), np.array([], dtype='float32')])\n\n    # rebuild to NxN\n    for i in range(len(preds)):\n        ii = len(preds)-i-1\n        for j in range(len(preds[ii][0])):\n            jj = preds[ii][0][j]\n            preds[jj][0] = np.insert(preds[jj][0], 0 ,ii)\n            preds[jj][1] = np.insert(preds[jj][1], 0 ,preds[ii][1][j])\n\n    #np.save('my_%s.npy'%event, preds)\nelse:\n    print('load predicts')\n    preds = np.load('..\/input\/trackml\/my_%s.npy'%event)","7fa0d2e0":"def get_path2(hit, mask, thr):\n    path = [hit]\n    a = 0\n    while True:\n        c = get_predict2(path[-1])\n        mask = (c > thr)*mask\n        mask[path[-1]] = 0\n        \n        if 1:\n            cand = np.where(c>thr)[0]\n            if len(cand)>0:\n                mask[cand[np.isin(module_id[cand], module_id[path])]]=0\n                \n        a = (c + a)*mask\n        if a.max() < thr*len(path):\n            break\n        path.append(a.argmax())\n    return path\n\ndef get_predict2(p):\n    c = np.zeros(len(preds))\n    c[preds[p, 0]] = preds[p, 1]          \n    return c","c04db2f8":"# reconstruct by all hits. (takes 0.6hr but can skip)\nskip_reconstruct = True\n\nif skip_reconstruct == False:\n    tracks_all = []\n    thr = 0.85\n    x4 = True\n    for hit in tqdm_notebook(range(len(preds))):\n        m = np.ones(len(truth))\n        path  = get_path2(hit, m, thr)\n        if x4 and len(path) > 1:\n            m[path[1]]=0\n            path2  = get_path2(hit, m, thr)\n            if len(path) < len(path2):\n                path = path2\n                m[path[1]]=0\n                path2  = get_path2(hit, m, thr)\n                if len(path) < len(path2):\n                    path = path2\n            elif len(path2) > 1:\n                m[path[1]]=1\n                m[path2[1]]=0\n                path2  = get_path2(hit, m, thr)\n                if len(path) < len(path2):\n                    path = path2\n        tracks_all.append(path)\n    #np.save('my_tracks_all', tracks_all)\nelse:\n    print('load tracks')\n    tracks_all = np.load('..\/input\/trackml\/my_tracks_all.npy')","dc7b11d8":"def get_track_score(tracks_all, n=4):\n    scores = np.zeros(len(tracks_all))\n    for i, path in enumerate(tracks_all):\n        count = len(path)\n\n        if count > 1:\n            tp=0\n            fp=0\n            for p in path:\n                tp = tp + np.sum(np.isin(tracks_all[p], path, assume_unique=True))\n                fp = fp + np.sum(np.isin(tracks_all[p], path, assume_unique=True, invert=True))\n            scores[i] = (tp-fp*n-count)\/count\/(count-1)\n        else:\n            scores[i] = -np.inf\n    return scores\n\ndef score_event_fast(truth, submission):\n    truth = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n    df = truth.groupby(['track_id', 'particle_id']).hit_id.count().to_frame('count_both').reset_index()\n    truth = truth.merge(df, how='left', on=['track_id', 'particle_id'])\n    \n    df1 = df.groupby(['particle_id']).count_both.sum().to_frame('count_particle').reset_index()\n    truth = truth.merge(df1, how='left', on='particle_id')\n    df1 = df.groupby(['track_id']).count_both.sum().to_frame('count_track').reset_index()\n    truth = truth.merge(df1, how='left', on='track_id')\n    truth.count_both *= 2\n    score = truth[(truth.count_both > truth.count_particle) & (truth.count_both > truth.count_track)].weight.sum()\n    particles = truth[(truth.count_both > truth.count_particle) & (truth.count_both > truth.count_track)].particle_id.unique()\n\n    return score, truth[truth.particle_id.isin(particles)].weight.sum(), 1-truth[truth.track_id>0].weight.sum()\n\ndef evaluate_tracks(tracks, truth):\n    submission = pd.DataFrame({'hit_id': truth.hit_id, 'track_id': tracks})\n    score = score_event_fast(truth, submission)[0]\n    track_id = tracks.max()\n    print('%.4f %2.2f %4d %5d %.4f %.4f'%(score, np.sum(tracks>0)\/track_id, track_id, np.sum(tracks==0), 1-score-np.sum(truth.weight.values[tracks==0]), np.sum(truth.weight.values[tracks==0])))\n\ndef extend_path(path, mask, thr, last = False):\n    a = 0\n    for p in path[:-1]:\n        c = get_predict2(p)\n        if last == False:\n            mask = (c > thr)*mask\n        mask[p] = 0\n        cand = np.where(c>thr)[0]\n        mask[cand[np.isin(module_id[cand], module_id[path])]]=0\n        a = (c + a)*mask\n\n    while True:\n        c = get_predict2(path[-1])\n        if last == False:\n            mask = (c > thr)*mask\n        mask[path[-1]] = 0\n        cand = np.where(c>thr)[0]\n        mask[cand[np.isin(module_id[cand], module_id[path])]]=0\n        a = (c + a)*mask\n            \n        if a.max() < thr*len(path):\n            break\n\n        path.append(a.argmax())\n        if last: break\n    \n    return path","7f6d9081":"# calculate track's confidence (about 2 mins)\nscores = get_track_score(tracks_all, 8)","1cfbe4cf":"# merge tracks by confidence and get score\nidx = np.argsort(scores)[::-1]\ntracks = np.zeros(len(hits))\ntrack_id = 0\n\nfor hit in idx:\n\n    path = np.array(tracks_all[hit])\n    path = path[np.where(tracks[path]==0)[0]]\n\n    if len(path)>3:\n        track_id = track_id + 1  \n        tracks[path] = track_id\n\nevaluate_tracks(tracks, truth)","bd9f9891":"# multistage\nidx = np.argsort(scores)[::-1]\ntracks = np.zeros(len(hits))\ntrack_id = 0\n\nfor hit in idx:\n    path = np.array(tracks_all[hit])\n    path = path[np.where(tracks[path]==0)[0]]\n\n    if len(path)>6:\n        track_id = track_id + 1  \n        tracks[path] = track_id\n\nevaluate_tracks(tracks, truth)\n\nfor track_id in range(1, int(tracks.max())+1):\n    path = np.where(tracks == track_id)[0]\n    path = extend_path(path.tolist(), 1*(tracks==0), 0.6)\n    tracks[path] = track_id\n        \nevaluate_tracks(tracks, truth)\n        \nfor hit in idx:\n    path = np.array(tracks_all[hit])\n    path = path[np.where(tracks[path]==0)[0]]\n\n    if len(path)>3:\n        path = extend_path(path.tolist(), 1*(tracks==0), 0.6)\n        track_id = track_id + 1  \n        tracks[path] = track_id\n        \nevaluate_tracks(tracks, truth)\n\nfor track_id in range(1, int(tracks.max())+1):\n    path = np.where(tracks == track_id)[0]\n    path = extend_path(path.tolist(), 1*(tracks==0), 0.5)\n    tracks[path] = track_id\n        \nevaluate_tracks(tracks, truth)\n\nfor hit in idx:\n    path = np.array(tracks_all[hit])\n    path = path[np.where(tracks[path]==0)[0]]\n\n    if len(path)>1:\n        path = extend_path(path.tolist(), 1*(tracks==0), 0.5)\n    if len(path)>2:\n        track_id = track_id + 1\n        tracks[path] = track_id\n        \nevaluate_tracks(tracks, truth)\n\nfor track_id in range(1, int(tracks.max())+1):\n    path = np.where(tracks== track_id)[0]\n    if len(path)%2 == 0:\n        path = extend_path(path.tolist(), 1*(tracks==0), 0.5, True)\n        tracks[path] = track_id\n        \nevaluate_tracks(tracks, truth)","4e933a3b":"# Step 4 - Test event 1001","15a8b279":"# Step 5 - Predict and Score\n","f38d03b9":"# Step 1 - Prepare training data\n* use 10 events for training\n* input: hit pair\n* output: 1 if two hits are the same particle_id, 0 otherwise.\n* feature size: 10 (5 per hit)","c9a1ca92":"# Step 2 - Train model","5cbfe41a":"# Step 3 - Hard Negative Mining"}}