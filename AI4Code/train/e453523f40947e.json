{"cell_type":{"0f872c68":"code","c9722ad2":"code","1da8b87e":"code","5b545c6b":"code","9529f2fe":"code","c98d43d5":"code","46ad88e9":"code","aec96472":"code","bb217dec":"code","9f4ed922":"code","4634f493":"code","1fa53317":"code","fef3d4c0":"code","4dec83a2":"code","c2bf355a":"code","f33095e2":"code","82b02b07":"code","50e5d4e1":"code","4a07c593":"code","5a7465bb":"code","dd3c0f36":"code","3141cfc6":"code","e161daff":"code","aafef691":"code","78a42dcc":"code","3e2f50f1":"code","f422e2d4":"code","ec87815a":"code","0a477205":"code","0f8f1afa":"code","84f9782b":"code","626cab61":"code","698e282d":"code","f31b8889":"code","f8c406d6":"code","5dcc8314":"code","4b1b6868":"code","820d6f60":"code","d9ca880d":"code","d93f7e9b":"code","d9be7592":"code","b3b66553":"code","365757c5":"code","35f81994":"code","91b54cac":"code","ec076598":"code","53b25cbf":"code","ce65cdad":"code","a1deaff7":"code","60a5dd7c":"code","fa54806e":"code","51e65e47":"code","db46f6fd":"code","5163a8a2":"code","78bfdcb3":"markdown","e5d45eea":"markdown","6dbe3b8f":"markdown","559bf80c":"markdown","44f61807":"markdown","d7613e49":"markdown","220a664f":"markdown","0438ae7b":"markdown","8dfe5bc4":"markdown","c6eb0889":"markdown","1e1a8dd7":"markdown","bea98cfb":"markdown","8881c821":"markdown","375c55f1":"markdown","0cc478d2":"markdown","f82ccb04":"markdown","2e6cc70e":"markdown","23bdc5e6":"markdown","067cc6b3":"markdown","d0380171":"markdown","48d7e73a":"markdown","4ee8709a":"markdown","01215296":"markdown","e37db52c":"markdown","38f77809":"markdown","79b64101":"markdown","1f62e8c9":"markdown","cb349092":"markdown","dad716b5":"markdown","e3fdd833":"markdown","053c3165":"markdown","ae7223e2":"markdown","47199476":"markdown","cd514de5":"markdown","16916848":"markdown","0973b983":"markdown","e32e2723":"markdown","ed1afd61":"markdown","b62f3f81":"markdown","c6254a94":"markdown","1bd0968b":"markdown","4fc9b258":"markdown","c32db887":"markdown","e90c0b4e":"markdown","6de2b451":"markdown","ef72c647":"markdown","2b1841d4":"markdown"},"source":{"0f872c68":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n%matplotlib inline\nrstate = 42 #establish a fixed random state","c9722ad2":"directory = \"\/kaggle\/input\/airline-passenger-satisfaction\/\"\nfeature_tables = ['train.csv', 'test.csv']\ndf_train_str = directory + feature_tables[0]\ndf_test_str = directory + feature_tables[1]\ndf_train = pd.read_csv(df_train_str)\ndf_test = pd.read_csv(df_test_str)","1da8b87e":"df_train.sample(5, random_state=rstate)","5b545c6b":"df_train.shape #amount of records is rather high, sampling will probably be required to avoid overloadign processor","9529f2fe":"df_train.info()","c98d43d5":"df_train.isnull().sum()","46ad88e9":"df_test.info()","aec96472":"df_train.describe()","bb217dec":"plt.figure(figsize=(20,15))\nsns.heatmap(df_train.corr(),annot=True,cmap='YlGnBu')\nplt.tight_layout","9f4ed922":"sns.countplot(x='satisfaction',data=df_train)","4634f493":"d = {'satisfied': True, 'neutral or dissatisfied': False} #create a dictionary to use map on\ndf_train['sat_label'] = df_train['satisfaction'].map(d) #map the values according to the dictionary to a new column\ndf_train.drop('satisfaction',inplace=True,axis=1) #erase old column\ndf_train[\"sat_label\"] = df_train[\"sat_label\"].astype(int) #convert to int to use later in models","1fa53317":"#convert the same way for the testset\ndf_test['sat_label'] = df_test['satisfaction'].map(d)\ndf_test.drop('satisfaction',inplace=True,axis=1)\ndf_test[\"sat_label\"] = df_test[\"sat_label\"].astype(int)","fef3d4c0":"df_train['sat_label'].value_counts()","4dec83a2":"df_test['sat_label'].value_counts()","c2bf355a":"df_train.corr()['sat_label'].sort_values().drop('sat_label').plot(kind='bar')","f33095e2":"df_train['Online boarding'].sort_values().value_counts()","82b02b07":"sns.boxplot(x='sat_label',y = 'Online boarding',data=df_train)","50e5d4e1":"Gender_cat = pd.get_dummies(df_train['Gender'],drop_first=True)\nCustomer_cat = pd.get_dummies(df_train['Customer Type'],drop_first=True)\nTravel_cat = pd.get_dummies(df_train['Type of Travel'],drop_first=True)\nClass_cat = pd.get_dummies(df_train['Class'],drop_first=True)\ndf_train = pd.concat([df_train,Gender_cat,Customer_cat,Travel_cat,Class_cat],axis =1) # add all the newly created columns to the existing dataframe\ndf_train.drop(['Gender','Customer Type','Type of Travel','Class'],inplace =True,axis = 1) #erase the old categorical columns","4a07c593":"Gender_cat = pd.get_dummies(df_test['Gender'],drop_first=True)\nCustomer_cat = pd.get_dummies(df_test['Customer Type'],drop_first=True)\nTravel_cat = pd.get_dummies(df_test['Type of Travel'],drop_first=True)\nClass_cat = pd.get_dummies(df_test['Class'],drop_first=True)\ndf_test = pd.concat([df_test,Gender_cat,Customer_cat,Travel_cat,Class_cat],axis =1)\ndf_test.drop(['Gender','Customer Type','Type of Travel','Class'],inplace =True,axis = 1)","5a7465bb":"df_train.sample(random_state=rstate)","dd3c0f36":"df_test.sample(random_state=rstate)","3141cfc6":"plt.figure(figsize=(26,20))\nsns.heatmap(df_train.corr(),annot = True,cmap='YlGnBu')","e161daff":"df_train.corr()['sat_label'].sort_values().drop('sat_label').plot(kind='bar', color='maroon')","aafef691":"plt.hist(df_train['Flight Distance'])","78a42dcc":"sns.lmplot(x='Departure Delay in Minutes',y='Arrival Delay in Minutes',data=df_train)","3e2f50f1":"df_train.drop(['Unnamed: 0','id', 'Arrival Delay in Minutes'],axis=1,inplace=True)\ndf_train.dropna(axis=0,inplace=True)","f422e2d4":"df_test.drop(['Unnamed: 0','id', 'Arrival Delay in Minutes'],axis=1,inplace=True)\ndf_test.dropna(axis=0,inplace=True)","ec87815a":"df_train.isnull().sum()","0a477205":"df_train_grouped = df_train.copy() #copy to aviod deletion on original memory\ndf_train_grouped['Order'] = df_train[['Inflight wifi service','Departure\/Arrival time convenient','Ease of Online booking', 'Gate location']].mean(axis=1)\ndf_train_grouped['Comfort'] = df_train[['Food and drink','Online boarding','Seat comfort', 'Inflight entertainment']].mean(axis=1)\ndf_train_grouped['Service'] = df_train[['Inflight entertainment','On-board service','Leg room service', 'Baggage handling']].mean(axis=1)\ndf_train_grouped.drop(['Inflight wifi service','Departure\/Arrival time convenient','Ease of Online booking', 'Gate location', 'Food and drink','Online boarding','Seat comfort', 'Inflight entertainment', 'On-board service','Leg room service', 'Baggage handling', 'Age', 'Flight Distance', 'Departure Delay in Minutes'],axis=1,inplace=True)","0f8f1afa":"df_test_grouped = df_test.copy()\ndf_test_grouped['Order'] = df_test[['Inflight wifi service','Departure\/Arrival time convenient','Ease of Online booking', 'Gate location']].mean(axis=1)\ndf_test_grouped['Comfort'] = df_test[['Food and drink','Online boarding','Seat comfort', 'Inflight entertainment']].mean(axis=1)\ndf_test_grouped['Service'] = df_test[['Inflight entertainment','On-board service','Leg room service', 'Baggage handling']].mean(axis=1)\ndf_test_grouped.drop(['Inflight wifi service','Departure\/Arrival time convenient','Ease of Online booking', 'Gate location', 'Food and drink','Online boarding','Seat comfort', 'Inflight entertainment', 'On-board service','Leg room service', 'Baggage handling', 'Age', 'Flight Distance', 'Departure Delay in Minutes'],axis=1,inplace=True)","84f9782b":"df_train_grouped.sample(random_state=rstate)","626cab61":"df_test_grouped.sample(random_state=rstate)","698e282d":"#Import the relevant libraries\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import MeanShift","f31b8889":"sample_size = 10000\ndf_train_grouped_sample = df_train_grouped.sample(sample_size,random_state=rstate) #save the sample\ndf_train_grouped_sample_std = StandardScaler().fit_transform(df_train_grouped_sample) #scaled data\ndf_train_grouped_std = StandardScaler().fit_transform(df_train_grouped)#save a version of the entire scaled trainset\ndf_test_grouped_std = StandardScaler().fit_transform(df_test_grouped)#test as well","f8c406d6":"range_n_clusters = list (range(3,10)) \nprint (\"Number of clusters from 3 to 10: \\n\", range_n_clusters)\nfor n_clusters in range_n_clusters:\n    clusterer = KMeans (n_clusters=n_clusters)\n    preds = clusterer.fit_predict(df_train_grouped_sample_std)\n    centers = clusterer.cluster_centers_\n    score = silhouette_score (df_train_grouped_sample_std, preds) \n    print (\"For %d clusters, average Silhouette score is %.2f\" % (n_clusters, score))","5dcc8314":"kclusters = 5\nkmeans = KMeans(n_clusters=kclusters, random_state=rstate).fit(df_train_grouped)\ncluster_results_kmeans = kmeans.labels_\nnp.bincount(cluster_results_kmeans)","4b1b6868":"Summary_kmeans = df_train_grouped.copy()\nSummary_kmeans.insert(0, 'K Cluster Label', kmeans.labels_) #input the column that contains the labels of each record to the table\nSummary_kmeans_full = Summary_kmeans \nSummary_kmeans = Summary_kmeans.groupby(['K Cluster Label']).mean() #group by cluster id\nSummary_kmeans","820d6f60":"agglom = AgglomerativeClustering(n_clusters = 2, linkage = 'complete') #two clusters are set, aiming for a large difference in satisfaction labels\nAC_labels = agglom.fit_predict(df_train_grouped_sample)\ncluster_results_AC = agglom.labels_\nnp.bincount(cluster_results_AC)","d9ca880d":"Summary_AC = df_train_grouped_sample.copy()\nSummary_AC.insert(0, 'AC Cluster Labels', AC_labels)\nSummary_AC = Summary_AC.groupby(['AC Cluster Labels']).mean()\nSummary_AC","d93f7e9b":"from sklearn.cluster import SpectralClustering\nSpec = SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=rstate).fit(df_train_grouped_sample)\ncluster_results_SC = Spec.labels_\nnp.bincount(cluster_results_SC)","d9be7592":"#Same process of addition of cluster id to table\nSummary_SC = df_train_grouped_sample.copy()\nSummary_SC.insert(0, 'SC Cluster Labels', cluster_results_SC)\nSummary_SC = Summary_SC.groupby(['SC Cluster Labels']).mean()\nSummary_SC","b3b66553":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV","365757c5":"x_train = np.delete(df_train_grouped_std, 3, 1) #remove label from input vector\ny_train = df_train_grouped_std[:,3]\nx_test = np.delete(df_train_grouped_std, 3, 1) #remove label from input vector\ny_test = df_train_grouped_std[:,3]","35f81994":"#Needs to be set to int (binary) to allow classification algorithms to perform fit\ny_train = y_train.astype(int)\ny_test = y_test.astype(int)","91b54cac":"def run_model(model, x_train, y_train, x_test, y_test, verbose=True):\n    if verbose == False:\n        model.fit(x_train,y_train, verbose=0)\n    else:\n        model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    roc_auc = roc_auc_score(y_test, y_pred)\n    print(\"ROC_AUC = {}\".format(roc_auc))\n    print(classification_report(y_test,y_pred,digits=5))\n    plot_confusion_matrix(model, x_test, y_test,cmap=plt.cm.Blues, normalize = 'all')\n    \n    return model, roc_auc #function returns model object and ROC_AUC","ec076598":"#Since runtime is extremely long for a full grid search, we will use its best parameters for setup:\nparams_rf = {'max_depth': 25, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1200,'random_state': rstate}\n\n#hparam_rf = {'criterion': ['gini', 'entropy'], 'n_estimators': [50,100,200,500,1200],'max_depth':[5,10,25,35], 'min_samples_split':[1,2,3]}\n\n#model_rf = GridSearchCV(RandomForestClassifier(), param_grid=hparam_rf,scoring = 'roc_auc', n_jobs=-1)\nmodel_rf = RandomForestClassifier(**params_rf) #get an unspecified number of parameters to the function\nmodel_rf, roc_auc_rf = run_model(model_rf, x_train, y_train, x_test, y_test) #pass the model together with the vectors for training and prediction","53b25cbf":"model_rf.feature_importances_","ce65cdad":"#listing all parameters for grid search:\nhparam_lgb = {'n_estimators': [50, 100, 200],'max_depth':[5,10,15,20],'num_leaves': [25, 50, 100], 'random_state': [rstate]}","a1deaff7":"model_lgb = GridSearchCV(lgb.LGBMClassifier(), param_grid=hparam_lgb,scoring = 'roc_auc', n_jobs=-1)\nmodel_lgb, roc_auc_lgb = run_model(model_lgb, x_train, y_train, x_test, y_test)","60a5dd7c":"model_lgb.best_estimator_.feature_importances_","fa54806e":"#Same principal as in random forest - we'll use the optimal hparameters obtained using gridsearch as the finite input.\nparams_svc ={'C': 1, \n         'kernel': 'linear', \n         'degree': 3, \n         'gamma': 'scale',\n          'random_state':rstate}\n\n#hparam_svc = {'C': [1,2,3],'kernel':['rbf', 'linear'],'degree': [2, 3, 4], 'gamma': ['scale', 'auto']}","51e65e47":"model_svc = SVC(**params_svc)\n#model_svc = GridSearchCV(SVC(), param_grid=hparam_svc,scoring = 'roc_auc', n_jobs=-1)\nmodel_svc, roc_auc_svc = run_model(model_svc, x_train, y_train, x_test, y_test)","db46f6fd":"hparam_ada = {'learning_rate':[0.8, 1.0, 1.1],'n_estimators':[50,100,150,200,500,1000], 'random_state':[rstate]}\nmodel_ada = GridSearchCV(AdaBoostClassifier(), param_grid=hparam_ada,scoring = 'roc_auc', n_jobs=-1)\nmodel_ada, roc_auc_ada = run_model(model_ada, x_train, y_train, x_test, y_test)","5163a8a2":"model_ada.best_estimator_.feature_importances_","78bfdcb3":"Since random forest performed very well, we will continue with another  decision tree based algorithm - lightGBM:","e5d45eea":"Due to the heavy processing, sometimes a 1\/10 random sample we be used in the following questions. Otherwise, the available machine will not be able to process the task in a reasonable amount of time. Data is also normalized and converted to numpy for performance improvement.","6dbe3b8f":"Since we are about to use the data for supervised learning tasks eventually, it is important to see if the labels are balance. If not, an adjustment in the training process is required such that bias is minimized as a result of uneven labeling. As can be seen, the labeled train set is balanced so no need for further actions.","559bf80c":"We create a generalized function to deal with fitting, predicting satisfaction and measuring results. For each run we print out the ROC area under curve score, classification metrics and a confusion matrix:","44f61807":"In order for the labels to be used in the training, we need to convert them to binary. Satisfied will be assigned with 1 and all others will take zeros:","d7613e49":"And the results show the same trends again (for the third time) so I think the conclusions are strong.","220a664f":"First, all the categorical variables need to be transformed to indicator variables. Most of them have only 2 types but class has 3 and therefore 2 columns are created in order to encode it. This is essential for the correlation to be calculated at first and for the trainning to be done later (in case the algorithms that will be used cannot use categorical features).","0438ae7b":"We can see that the mean in all survey categories is between 2-3.5 and std is 1-1.5. Hence, the categories are rather balanced. The flight distance has a very high variance so it should be treated carefully. Age mean\\median are around 40 so no exceptions here as well. \"id\" & \"Unnamed...\" have little value as information.","8dfe5bc4":"Easy to see cluster 0,2 have significantly higher satisfaction ratios. If we look at the means of those clusters we can see that compared to the others:\n1. Cleanliness is higher\n2. Inflight service is higher\n3. Gender is meaningless\n4. Less disloyal customers\n5. Less eco\\eco-plus classes\n6. Higher scores for the 3 systetic subgroups (a very good sign!)","c6eb0889":"We see a very long thick tail indeed. Though this feature is correlated relatively strongly with the label, it might be difficult to process for some of the algorithms. Also, we noticed before that arrival and departure delay is very heavly correlated which means there's little sense in using them both as feature. It can be seen how strong is the relationship between them (not surprising - once there's a delay in departure, assuming flight time is approx the same, the delay in arrival will be super close).","1e1a8dd7":"Display the correlation differently, highlighting top correlated features with the satisfaction label. Easy to see that on-line boarding takes the lead with 50% while there's another ~10 features that are 20-40% correlated. This is not a bad start. If a single features has that much measured correlation, prediction will probably be very solid.","bea98cfb":"Random forest generates outstanding results and as can be seen from the feature importance analysis, our synthetic aggregated features do a great job helping out (50% accumulated feature importance)! the results are better than the top rated kaggle notebooks I've seen:","8881c821":"Same for testset:","375c55f1":"Now we can plot a more comprehensive matrix that will take into consideration additional variables. Neverthelss, the strongest patterns was already seen before - 3 subgroups of survey categories.","0cc478d2":"After recognizing the 3 subgroups as detailed above, what we will do is aggregate the relevant features into 3 groups (with a minor overlap) and take the value of the subgroup as the mean of its components. This should reduce the noise within each group of correlated features and allow a better fit using the different algorithms. Age, flight distance and arrival delay have also been removed to narrow dimensions since they are not expected to generate much value.","f82ccb04":"Import the relevant libraries:","2e6cc70e":"Further inspecting the strongest predictor we find out nothing out of the ordinary. 4 is the leading score which is quite high and this means that the average satisfied customer ranked \"online boarding\" close to 5.","23bdc5e6":"### Prediction of overall satisfaction label","067cc6b3":"Also, we see the same effect in feature importance once more:","d0380171":"First up is random forest which is a decision-tree based classic algorithm. Since some of the optimizations are taking an extremely long amount of time, what I will do is run it offline (many hours), display the code with a comment FYI, while the actual run will be done with determinstic parameters. Other algorithms that run quickly we be applied by initializing a Grid object:","48d7e73a":"Same for the testset:","4ee8709a":"Results are not that good as the previous, decision tree-based, algorithms. For the final run we'll use Adaboost, expecting a performance similar to light GBM & random forest. In practice, the results are worse. ","01215296":"Now we will try to apply a different algorithmic basis - support vectors:","e37db52c":"Same thing is done for the testset:","38f77809":"### Pre processing","79b64101":"Now we'll try agglomerative clustering since it is well built for our requirement because we can set the amount of cluster to 2 and the merge will continue all the way there and find the commonalities:","1f62e8c9":"Once we check correlation against the label once more we see more relationships (especially negative ones) with type of travel and class:","cb349092":"Another algorithm we'll try is spectral and we can see the same conclusions can be derived from it. Spectral is also showing the best seperation ratio (75% vs 18% satisfaction) and it fits our task well because it decompresses the features, reducing the information to 2 clusters - similar to agglomerative:","dad716b5":"Make sure the change was applied:","e3fdd833":"Once we plot the initial correlation matrix (before data is cleaned), we immediately notice a pattern of 3 correlated chunks with 4 categories each:\n1. Comfort related categories: seat comfort, food and drink etc\n2. Flight order related categories: wifi, ease of inline booking etc\n3. Service related categories: leg room, on board etc\n\nAlso, seems like departure \\ arrival delay are heavily correlated.","053c3165":"Sample the data to make sure it is OK:","ae7223e2":"### Feature importance analysis","47199476":"Add the cluster id as an additional column and display the mean values for every cluster once grouped:","cd514de5":"The training data is almost perfect in terms of readiness for analysis. The only nulls are in \"Arrival Delay in Minutes\" and since the amount is insignificant (300\/100K), these records can be easily removed. Also we can see that there are some dtypes that are objects due to the fact these are categorical variables. They will need to be converted later on to dummies \\ indicators.","16916848":"Drop irrelevant columns (as discussed above) and NAN records:","0973b983":"Import the relevant libraries and create a random state variable to be used all across the notebook:","e32e2723":"Same principal as in random forest - the 3 sub groups have a major impact on prediction (last 3 features in list):","ed1afd61":"Analyzing the results of agglomerative clustering, the picture is rather clear and the same trends can be seen as listed for Kmeans. Now the same groupby is performed again:","b62f3f81":"### Exploring the data","c6254a94":"The same applies for the test data:","1bd0968b":"Best score is obtained with 5 clusters which turn out rather balanced:","4fc9b258":"Load the data from csv to dataframes and check out some basic properties:","c32db887":"Since flight distance was marked before as a high variance feature, let's plot a histogram to visualize the exact distribution:","e90c0b4e":"Here it is seen very clearly in a box plot. The satisfied customers (sat_label = 1) score this parameter between 4-5 (q1-3) while the other are settled in the range of 2 to 3.","6de2b451":"We begin with the clustering task and Kmeans as its basic pioneer. Using the silhouette score to determine optimal amount of clusters:","ef72c647":"After trying out different aggregations (groupings) of age (I've erased them since the notebook is long enough without them), we see that correlation is not increased so we leave it as is. Now the data is ready for analysis and contains no nulls:","2b1841d4":"Preparing the train & test vectors for use:"}}