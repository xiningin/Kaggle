{"cell_type":{"93b8396f":"code","f7f17b3f":"code","64f52db9":"markdown"},"source":{"93b8396f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nnp.random.seed(1212)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f7f17b3f":"import tensorflow as tf\nimport keras\n\n# Reading train and test data\ntrain_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\n# Class names\nval_data = train_data.iloc[:5000,:]\ntrain_data = train_data.iloc[5000:,:]\n\n# Fetching the labels\ntrain_labels = train_data.label\nval_labels = val_data.label\n\n# Reshaping training data\ntrain_images = train_data.iloc[:,1:].values.reshape(37000, 28, 28)\n# Reshaping validation data\nval_images = val_data.iloc[:,1:].values.reshape(5000, 28, 28)\n\n# Scaling data in the range of 0-1\ntrain_images = train_images\/255.0\nval_images = val_images\/255.0\n\n\n\n# Defining multi-layer perceptron model with 3 hidden layer having 10 neurons each and with non-linearity\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)), # Perform conversion of higher dimensional data (here, 2-D) to 1-D data.\n    keras.layers.Dense(10, activation=tf.nn.leaky_relu), # Hidden layer with 10 neurons and ReLU activation function\n    keras.layers.Dense(10, activation=tf.nn.leaky_relu), # Hidden layer with 10 neurons and ReLU activation function\n    keras.layers.Dense(10, activation=tf.nn.leaky_relu), # Hidden layer with 10 neurons and ReLU activation function\n    keras.layers.Dense(10, activation=tf.nn.leaky_relu), # Hidden layer with 10 neurons and ReLU activation function\n    keras.layers.Dense(10, activation=tf.nn.softmax) # Output layer with softmax activation function \n])\n# Defining parameters like optimizer, loss function and evaluating metric\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])\nmodel4 = model.fit(train_images, train_labels, epochs=20, validation_data=(val_images, val_labels))\n\n\ntest_images = test_data.iloc[:,:].values.reshape(28000, 28, 28)\ntest_images = test_images\/255.0\n\noutput = model.predict(test_images)\nresults = np.argmax(output,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"mnist_submission.csv\",index=False)\n","64f52db9":"\n# > Introduction\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201cHello World\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\nIn this competition, we aim to correctly identify digits from a dataset of tens of thousands of handwritten images. Kaggle has curated a set of tutorial-style kernels which cover everything from regression to neural networks. They hope to encourage us to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n\n# > Approach\n\nFor this competition, we will be using Keras (with TensorFlow as our backend) as the main package to create a simple neural network to predict, as accurately as we can, digits from handwritten images. In particular, we will be calling the Functional Model API of Keras, and creating a 4-layered neural network.\n\n> **Result\n\nFollowing our simulations on the cross validation dataset, it appears that a 4-layered neural network, using 'Adam' as the optimizer. We proceed to introduce dropout in the model, and use the model to predict for the test set.\n\nThe test predictions (submitted to Kaggle) generated by our model predicts with an accuracy score of 93.840%, which places us at the top  percentile of the competition.\n"}}