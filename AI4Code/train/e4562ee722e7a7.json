{"cell_type":{"c3109797":"code","808800ad":"code","b304c416":"code","933e9506":"code","1924a9bd":"code","914d68a0":"code","eb98b5ed":"code","f4ba8ca1":"code","5ad804d6":"code","583d685c":"code","13a109a8":"code","f5300d02":"code","478b470b":"markdown","aee494e7":"markdown","a9be0ca6":"markdown","4885e416":"markdown","a5eab02d":"markdown","5693a200":"markdown","98044d33":"markdown","013b7269":"markdown","5ac3e184":"markdown","7ba80aed":"markdown"},"source":{"c3109797":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras import layers\nfrom keras import models\nfrom keras import backend as K\nfrom keras import regularizers\n\n# Clearing Keras backend\nK.clear_session()\n\n# setting matplotlib style to ggplot\nplt.style.use('ggplot')\n%matplotlib inline","808800ad":"def smooth_curve(points, factor=0.9):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\n\ndef build_model():\n    \"\"\"Function that builds a densely connect neural network\"\"\"\n    model = models.Sequential()\n    model.add(layers.Dense(128, kernel_regularizer=regularizers.l2(0.002), activation='relu', input_shape=(train_x.shape[1], )))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(32, kernel_regularizer=regularizers.l2(0.002), activation='relu'))\n    model.add(layers.Dropout(0.25))\n    model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.002), activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n    return model","b304c416":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","933e9506":"fraud = df.loc[df['Class'] == 1]\n\n# this returns a subset of all fraudulent cases, a total of 492 rows. Let's select 5000 samples and use the class_weight parameter\nclean = df.loc[df['Class'] == 0].iloc[:5000]\n\n# Concatenating the fraud and non-fraud datasets\ndata = pd.concat([clean, fraud])\n\n# Randomly shuffling the dataset and reseting the index\ndata = data.sample(frac=1).reset_index(drop=True)\ndata.head()","1924a9bd":"# Splitting the dataset into training and testing\n\nindex = int(data.shape[0] * 0.25)\n\n# Dropping the 'Time' column and splitting the labels into seperate dataframes\ntest_x = data[data.columns[1:-2]].iloc[:index]\ntest_y = data[data.columns[-1]].iloc[:index]\n\ntrain_x = data[data.columns[1:-2]].iloc[index:]\ntrain_y = data[data.columns[-1]].iloc[index:]\n","914d68a0":"mean = train_x.mean()\nstd = train_x.std()\ntrain_x = (train_x - mean)\/std\n\n# Plotting the distribution after standardization\ntrain_x.plot.hist(bins = 50, color='b', legend=None)\nplt.title(\"Training Data Standardized Distribution\")\nplt.show()","eb98b5ed":"test_x -= mean\ntest_x \/= std\n\n# Plotting the distribution after standardization\ntest_x.plot.hist(bins = 50, color='b', legend=None)\nplt.title(\"Test Data Standardized Distribution\")\nplt.show()","f4ba8ca1":"# Clearing memory from last evaluation to avoid overfitting on test data\nK.clear_session()\n\nnum_epochs = 40\n\n# Model instantiation\nmodel = models.Sequential()\nmodel.add(layers.Dense(128, activation='relu', input_shape=(train_x.shape[1], )))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nclass_weights = {0: 1.,\n                1: 15.}\n\nhistory = model.fit(train_x, train_y, epochs = num_epochs, class_weight = class_weights, batch_size = 64, validation_split=0.15)\n\n# Variables for plotting performance\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, num_epochs + 1)\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'bo', label='Validation Accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training vs Validation Accuracy\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.plot(epochs, val_loss, 'bo', label='Validation Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.show()","5ad804d6":"K.clear_session()\nmodel.fit(train_x, train_y, epochs = 80, class_weight = class_weights, batch_size = 64)\nresults = model.evaluate(test_x, test_y)\nprint(f\"--------------SIMPLE MODEL--------------\\nLOSS: {results[0]:.2f}\\nACCURACY: {results[1]*100:.2f}%\")","583d685c":"# Clearing memory from last evaluation to avoid overfitting on test data\nK.clear_session()\n\nk = 4\nnum_val_samples = len(train_x) \/\/ k\nnum_epochs = 50\nall_val_acc_history = []\nacc_history = []\n\nfor i in range(k):\n    print(f\"Processing fold #: {i}\")\n    print(f\"RANGE: [{i*num_val_samples}, {(i+1)*num_val_samples}]\")\n    val_data = train_x[i*num_val_samples : (i+1)*num_val_samples]\n    val_targets = train_y[i*num_val_samples : (i+1)*num_val_samples]\n\n    partial_train_data = np.concatenate([train_x[:i*num_val_samples], train_x[(i+1)*num_val_samples:]], axis=0)\n    partial_train_targets = np.concatenate([train_y[:i*num_val_samples], train_y[(i+1)*num_val_samples:]], axis=0)\n    model = build_model()\n    history = model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=32, verbose=0, validation_data=(val_data, val_targets))\n    all_val_acc_history.append(history.history['val_acc'])\n    acc_history.append(history.history['acc'])\n\navg_val_acc = [np.mean([x[i] for x in all_val_acc_history]) for i in range(num_epochs)]\navg_acc = [np.mean([x[i] for x in acc_history]) for i in range(num_epochs)]\n\nsmooth_val_acc = smooth_curve(avg_val_acc)\nsmooth_acc = smooth_curve(avg_acc)\nplt.plot(range(1, len(smooth_val_acc) +1), smooth_val_acc, label='Validation Accuracy')   \nplt.plot(range(1, len(smooth_acc) +1), smooth_acc, label='Training Accuracy')  \nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.show()","13a109a8":"K.clear_session()\n\nmodel = build_model()\nhistory = model.fit(train_x, train_y, epochs=num_epochs, batch_size=64)","f5300d02":"results = model.evaluate(test_x, test_y)\nprint(f\"--------------K-FOLD MODEL--------------\\nLOSS: {results[0]:.2f}\\nACCURACY: {results[1]*100:.2f}%\")","478b470b":"# Credit Card Fraud Analysis","aee494e7":"First, let's evalute the above model on our test set.","a9be0ca6":"# Data Cleaning and Preparation\n\nDue to the skewedness of the data, tactics must be used to ensure the model doesn't overfit to one specific class (in this case, class 0 - which indicates no fraud has occured). There are other ways of doing this, such as oversampling and using SMOTE or utilizing the Keras \n*class_weight* parameter. For this example, we'll use a combination of random undersampling and the class_weight parameter in the Keras Sequential fit() method.","4885e416":"Using K-Fold Validation is a way to test the generalization of the model. Iteratively training the model over different samples of the dataset, using different samples for both validation and training, yields a more accurate picture of the models generalization.\n\nThis performs as well as the simpler approach (~99% on the validation set). Let's train on the entire data set and investigate the results.","a5eab02d":"This model performs well with the current data ( ~98% accuracy on the validation data). There is some variance in the accuracy of the validation data. This could be caused by the relatively small dataset we created. Let's see if we can do any better using a strategy such as K-Fold Validation.\n","5693a200":"Best practice is to standardized the data before passing it to a neural network. To do so, we can subtract the dataframe by the mean and divide by the standard deviation - this is known as the [Z-score](https:\/\/en.wikipedia.org\/wiki\/Standard_score).\n\nThis is easy with Pandas and NumPy:","98044d33":"# Part 2: K-Fold Validation \n## Pushing for >99%\nBecause we are dealing with a small amount of data, we may be able to achieve higher accuracy using K-Fold Validation. This is a process of iteratively training the model on small samples of the dataset.This can be used as a method to optimize performance on models.","013b7269":"We can see that the data is now standardized around 0.\n\nLet's apply the same standardization to the test data. It's important to not use the test data for standardization because it may result in overfitting.","5ac3e184":"\n# Model training\/development\n\n## Part 1: Simple Dense Model","7ba80aed":"Performance is about the same, given that the model was trained on the same data as the previous model. However, using K-Fold Validation is also a nice way to verify the integrity and accuracy of the model. "}}