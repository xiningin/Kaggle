{"cell_type":{"9e7ac792":"code","a1aef272":"code","01d8d27b":"code","d26a9e24":"code","2c93450e":"code","5352084b":"code","8fc7249a":"code","5f1e6c8e":"code","0492d95a":"code","17d8b477":"code","641a0bc5":"code","54b569b5":"code","9d1e905b":"code","ad85c769":"code","49684860":"code","a2a75c91":"code","c25ab3ef":"code","ee5464b4":"code","b6e80c6f":"code","e3109958":"code","1b6a93e5":"code","bf7dd2d7":"code","20ef20fd":"code","aaafade1":"code","c3045083":"code","e2b883a3":"code","0796368b":"code","bb11782b":"code","f42f4cfa":"code","d03b29a3":"code","ba3f29db":"code","d56097aa":"code","257d5d0d":"code","c782b9ed":"code","e0dd3e83":"code","96d81b69":"code","bfa9c093":"code","28a4236c":"code","9a1a2419":"code","bb2e91fd":"code","5449b395":"code","e5355609":"code","1869ca8f":"code","6087d80f":"markdown","5fd34e34":"markdown","a53bc323":"markdown","edb482de":"markdown","d8879e71":"markdown","bbc0e4ca":"markdown","61f7a98f":"markdown","e6f57087":"markdown","9f60797d":"markdown","4009ffc7":"markdown","c66b3ac3":"markdown","94879481":"markdown","06fe6f8b":"markdown","4f175b19":"markdown","a1c070e0":"markdown","5ffd1c98":"markdown","b1e24f87":"markdown","ce708a59":"markdown"},"source":{"9e7ac792":"import os, glob, math, cv2, gc, logging, warnings, random\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom scipy import stats\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, QuantileTransformer, KBinsDiscretizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.cluster import KMeans\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nfrom umap import UMAP\n\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport tensorflow as tf\nimport shap\nwarnings.filterwarnings(\"ignore\")","a1aef272":"train = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv').set_index(\"id\")\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv').set_index(\"id\")\n\nsample_submission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\n\ntrain[\"song_duration_ms\"] = train[\"song_duration_ms\"] \/ (1000*60)\ntest[\"song_duration_ms\"] = test[\"song_duration_ms\"] \/ (1000*60)\n\ntrain.rename(columns={\"song_duration_ms\":\"song_duration_m\"}, inplace=True)\ntest.rename(columns={\"song_duration_ms\":\"song_duration_m\"}, inplace=True)","01d8d27b":"train = train.drop(train[train.song_popularity==0].sample(n=10848, random_state=42).index)\ntrain = train.reset_index(drop=True)","d26a9e24":"feature_cols = test.columns.tolist()\ncat_cols = [\"key\", \"time_signature\", \"audio_mode\"]\ncat_non_bin_cols = [\"key\", \"time_signature\"]\ncnt_cols = [col for col in feature_cols if col not in cat_cols]","2c93450e":"plt.figure(figsize=(25, 10))\nfor i, col in enumerate(cnt_cols):\n    plt.subplot(2, 5, i+1)\n    sns.violinplot(data=train, x='song_popularity', y=col)\n    plt.title(col)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\nplt.show()","5352084b":"plt.figure(figsize=(20, 10))\nsns.heatmap(train.corr(), annot=True)\nplt.show()","8fc7249a":"plt.figure(figsize=(25, 10))\nfor i in range(len(train.columns)):\n    plt.subplot(3, 5, i+1)\n    sns.boxplot(data=train, x=train.columns[i])\n\nplt.show()","5f1e6c8e":"z_score_threshold = 0.9999\n\ndef get_outlier_counts(df, threshold):\n    df = df.copy()\n    threshold_z_score = stats.norm.ppf(threshold)\n    z_score_df = pd.DataFrame(np.abs(stats.zscore(df)), columns=df.columns)\n    return (z_score_df > threshold_z_score).sum(axis=0)\n\ndef detect_outliers(df, threshold):\n    df = df.copy()\n    threshold_z_score = stats.norm.ppf(threshold)\n    z_score_df = pd.DataFrame(np.abs(stats.zscore(df)), columns=df.columns)\n    z_score_df = z_score_df > threshold_z_score\n    outliers = z_score_df.sum(axis=1)\n    df[\"is_outlier\"] = (outliers > 0).astype(\"uint8\")\n    return df\n\noutlier_df_train = detect_outliers(train, threshold=z_score_threshold)\noutlier_df_test = detect_outliers(test, threshold=z_score_threshold)\n\nprint(get_outlier_counts(train, threshold=z_score_threshold))\nprint(\"-------------------------------------\")\nprint(get_outlier_counts(test, threshold=z_score_threshold))","0492d95a":"plt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\naxs = sns.countplot(x=outlier_df_train.loc[outlier_df_train.is_outlier==1,\"song_popularity\"])\naxs.bar_label(axs.containers[0])\nplt.title(\"Outliers Counts Train Set\")\n\nplt.subplot(1,2,2)\naxs = sns.countplot(x=outlier_df_test.is_outlier)\naxs.bar_label(axs.containers[0])\nplt.title(\"Test Set is Outlier\")\n\nplt.show()","17d8b477":"%%time\n# sns pairplot is really slow, you can use of scatter_matrix from pandas as an alternative\nplt.figure(figsize=(20,15))\nsns.pairplot(data=train, hue=\"song_popularity\", vars=cnt_cols)\nplt.show()","641a0bc5":"si = SimpleImputer(strategy=\"most_frequent\")\nx = train.copy()\nt = test.copy()\nx[feature_cols] = si.fit_transform(x[feature_cols])\nt[feature_cols] = si.transform(t[feature_cols])","54b569b5":"sc = MinMaxScaler()\nx[cnt_cols] = sc.fit_transform(x[cnt_cols])\nt[cnt_cols] = sc.transform(t[cnt_cols])","9d1e905b":"%%time\n\ninertia = {}\nfor i in range(2,30):\n    kmeans = KMeans(n_clusters=i, random_state=42)\n    kmeans.fit_predict(x[cnt_cols])\n    inertia.update({i:kmeans.inertia_})\n\ninertia_df = pd.Series(inertia)\nplt.plot(inertia_df,marker=\"o\")\nplt.xticks(inertia_df.index)\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Inertia\")\nplt.show()","ad85c769":"%%time\nn_clusters = 8\ncd_feature = True # cluster distance instead of cluster number  \n\nkmeans = KMeans(n_clusters=n_clusters, max_iter=500, n_init=20, random_state=42)\n\nif cd_feature:\n    cluster_cols = [f\"cluster{i+1}\" for i in range(n_clusters)]\n    \n    X_cd = kmeans.fit_transform(x[cnt_cols])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=x.index)\n    x = x.join(X_cd)\n    \n    X_cd = kmeans.transform(t[cnt_cols])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=t.index)\n    t = t.join(X_cd)\n\nelse:\n    cluster_cols = [\"cluster\"]  \n    x[\"cluster\"] = kmeans.fit_predict(x[cnt_cols])\n    t[\"cluster\"] = kmeans.predict(t[cnt_cols])\n    \nx.head()","49684860":"feature_cols += cluster_cols\n\nif cd_feature:\n    cnt_cols += cluster_cols\nelse:\n    cat_cols += cluster_cols\n\n    \nfor col in cluster_cols:\n    train[col] = x[col]\n    test[col] = t[col]","a2a75c91":"if cd_feature:\n    c = n_clusters\/\/2\n    fig, ax = plt.subplots(2, c, figsize=(25, 8))\n    for i in [0,1]:\n        for j in range(c):\n            sns.violinplot(data=train, x=\"song_popularity\", y= f\"cluster{c*i+j+1}\", ax=ax[i][j])\n            ax[i][j].set_title(f\"cluster{c*i+j+1}\")\n            ax[i][j].set_xlabel(\"\")\n            ax[i][j].set_ylabel(\"\")\n    plt.show()\nelse:\n    plt.figure(figsize=(20,8))\n    ax = sns.countplot(x=\"cluster\", data=train, hue=\"song_popularity\")\n    plt.xlabel(\"Clusters\")\n    plt.show()","c25ab3ef":"pca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(x[cnt_cols])\nT_pca = pca.transform(t[cnt_cols])\n\npca_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n\nX_pca = pd.DataFrame(X_pca, columns=pca_cols, index=x.index)\nT_pca = pd.DataFrame(T_pca, columns=pca_cols, index=t.index)\n\nx = pd.concat([x, X_pca], axis=1)\nt = pd.concat([t, T_pca], axis=1)\n\nfor col in pca_cols:\n    train[col] = x[col]\n    test[col] = t[col]\n    \nt.head()","ee5464b4":"loadings = pd.DataFrame(pca.components_, index=pca_cols, columns=train[cnt_cols].columns)\nloadings.style.bar(align='mid', color=['#d65f5f', '#5fba7d'], vmin=-1.0, vmax=1.0)","b6e80c6f":"plt.figure(figsize=(20,8))\nsns.scatterplot(data=train[(train.PC1<20)&(train.PC2<20)], x=\"PC1\", y=\"PC2\", hue=\"song_popularity\", alpha=0.8, palette=\"deep\")\nplt.show()","e3109958":"feature_cols += pca_cols\ncnt_cols += pca_cols","1b6a93e5":"def add_feature(df):\n    df[\"new_f1\"] = df[\"acousticness\"] - df[\"energy\"] - df[\"audio_valence\"]\n    df[\"new_f2\"] = df[\"acousticness\"] + df[\"danceability\"] + df[\"audio_valence\"]\n#     df[\"new_f3\"] = df[\"cluster3\"] + df[\"cluster5\"] + df[\"cluster7\"] + df[\"cluster8\"] - df[\"cluster6\"]\n#     df[\"new_f4\"] = df[\"cluster4\"] + df[\"cluster5\"] - df[\"cluster2\"]\n    return df\n\nx = add_feature(x)\nt = add_feature(t)","bf7dd2d7":"new_features = [\"new_f1\", \"new_f2\"]#, \"new_f3\", \"new_f4\"]\nfeature_cols += new_features\ncnt_cols += new_features\n\nfor col in new_features:\n    train[col] = x[col]\n    test[col] = t[col]","20ef20fd":"%%time\nscores = []\n\nx[\"lr\"] = 0\nt[\"lr\"] = 0\n\nfolds = 5\nx[\"kfold\"] = -1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(x,x[\"song_popularity\"])):\n    x.loc[valid_indicies, \"kfold\"] = fold\n    \nfor fold in range(folds):\n    x_train = x[x.kfold != fold].copy()\n    x_valid = x[x.kfold == fold].copy()\n    x_test  = t[feature_cols].copy()\n    \n    y_train = x_train['song_popularity']\n    y_valid = x_valid['song_popularity']\n    \n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n\n    lr_model = LogisticRegression(solver='liblinear')\n    lr_model.fit(x_train, y_train)\n    \n    preds_train = lr_model.predict_proba(x_train)[:,1]\n    preds_valid = lr_model.predict_proba(x_valid)[:,1]\n    auc_train = roc_auc_score(y_train, preds_train)\n    auc = roc_auc_score(y_valid, preds_valid)\n    print(f\"| Fold {fold+1} | train: {auc_train:.5f} | valid: {auc:.5f} |\")\n    print(\"|--------|----------------|----------------|\")\n    scores.append(auc)\n    \n    preds_test = lr_model.predict_proba(x_test)[:,1]\n    x[\"lr\"].loc[x_valid.index] = preds_valid\n    t[\"lr\"] += preds_test\n    \nt[\"lr\"] \/= folds\nprint(\"\\nAVG AUC:\",np.mean(scores))","aaafade1":"train[\"lr\"] = x[\"lr\"]\ntest[\"lr\"] = t[\"lr\"]","c3045083":"feature_cols.append(\"lr\")\ncnt_cols.append(\"lr\")","e2b883a3":"%%time\nscores = []\n\ntrain[\"xgb\"] = 0\ntest[\"xgb\"] = 0\n\nfolds = 5\ntrain[\"kfold\"] = -1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(train,train[\"song_popularity\"])):\n    train.loc[valid_indicies, \"kfold\"] = fold\n    \nfor fold in range (5):\n    x_train = train[train.kfold != fold].copy()\n    x_valid = train[train.kfold == fold].copy()\n    x_test  = test[feature_cols].copy()\n    \n    y_train = x_train['song_popularity']\n    y_valid = x_valid['song_popularity']\n    \n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n    \n    xgb_params = {\n        'eval_metric': 'auc', \n        'objective': 'binary:logistic', \n        'tree_method': 'gpu_hist', \n        'gpu_id': 0, \n        'predictor': 'gpu_predictor', \n        'n_estimators': 10000, \n        'learning_rate': 0.01, \n        'gamma': 0.4, \n        'max_depth': 3, \n        'seed': 42,       \n        'min_child_weight': 300, \n        'subsample': 0.7, \n        'colsample_bytree': 0.8, \n        'colsample_bylevel': 0.9, \n        'use_label_encoder': False,\n        'lambda': 0, \n        'alpha': 10\n    }\n    \n    xgb_model = XGBClassifier(**xgb_params)\n    xgb_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n    \n    preds_train = xgb_model.predict_proba(x_train)[:,1]\n    preds_valid = xgb_model.predict_proba(x_valid)[:,1]\n    auc_train = roc_auc_score(y_train, preds_train)\n    auc = roc_auc_score(y_valid, preds_valid)\n    print(f\"| Fold {fold+1} | train: {auc_train:.5f} | valid: {auc:.5f} |\")\n    print(\"|--------|----------------|----------------|\")\n    scores.append(auc)\n    \n    preds_test = xgb_model.predict_proba(x_test)[:,1]\n    train[\"xgb\"].loc[x_valid.index] = preds_valid\n    test[\"xgb\"] += preds_test\n    \ntest[\"xgb\"] \/= folds\nprint(\"\\nAVG AUC:\",np.mean(scores))","0796368b":"x[\"xgb\"] = train[\"xgb\"]\nt[\"xgb\"] = test[\"xgb\"]","bb11782b":"feature_cols.append(\"xgb\")\ncnt_cols.append(\"xgb\")","f42f4cfa":"%%time\nscores = []\n\ntrain[\"lgb\"] = 0\ntest[\"lgb\"] = 0\n\nfolds = 5\ntrain[\"kfold\"] = -1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(train,train[\"song_popularity\"])):\n    train.loc[valid_indicies, \"kfold\"] = fold\n    \nfor fold in range (5):\n    x_train = train[train.kfold != fold].copy()\n    x_valid = train[train.kfold == fold].copy()\n    x_test  = test[feature_cols].copy()\n    \n    y_train = x_train['song_popularity']\n    y_valid = x_valid['song_popularity']\n    \n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n    \n    params_lgb = {\n                \"task\": \"train\",\n                \"boosting_type\": \"gbdt\",\n                \"objective\": \"binary\",\n                'learning_rate': 0.001635,\n                \"max_depth\": 3,\n                \"feature_fraction\": 0.2256038826485174,\n                \"bagging_fraction\": 0.7705303688019942,\n                \"min_child_samples\": 290,\n                \"reg_alpha\": 14.68267919457715,\n                \"reg_lambda\": 66.156,\n                \"max_bin\": 772,\n                \"min_data_per_group\": 177,\n                \"bagging_freq\": 1,\n                \"cat_smooth\": 96,\n                \"cat_l2\": 17,\n                \"verbose\": -1,\n                'random_state':42,\n                'n_estimators':5000,\n    }\n    \n    lgb_model = LGBMClassifier(**params_lgb)\n    lgb_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n    \n    preds_train = lgb_model.predict_proba(x_train)[:,1]\n    preds_valid = lgb_model.predict_proba(x_valid)[:,1]\n    auc_train = roc_auc_score(y_train, preds_train)\n    auc = roc_auc_score(y_valid, preds_valid)\n    print(f\"| Fold {fold+1} | train: {auc_train:.5f} | valid: {auc:.5f} |\")\n    print(\"|--------|----------------|----------------|\")\n    scores.append(auc)\n    \n    preds_test = lgb_model.predict_proba(x_test)[:,1]\n    train[\"lgb\"].loc[x_valid.index] = preds_valid\n    test[\"lgb\"] += preds_test\n    \ntest[\"lgb\"] \/= folds\nprint(\"\\nAVG AUC:\",np.mean(scores))","d03b29a3":"x[\"lgb\"] = train[\"lgb\"]\nt[\"lgb\"] = test[\"lgb\"]","ba3f29db":"feature_cols.append(\"lgb\")\ncnt_cols.append(\"lgb\")","d56097aa":"%%time\nscores = []\n\nx[\"nn\"] = 0\nt[\"nn\"] = 0\n\nfolds = 5\nx[\"kfold\"] = -1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(x,x[\"song_popularity\"])):\n    x.loc[valid_indicies, \"kfold\"] = fold\n    \nfor fold in range (5):\n    x_train = x[x.kfold != fold].copy()\n    x_valid = x[x.kfold == fold].copy()\n    x_test  = t[feature_cols].copy()\n    \n    y_train = x_train['song_popularity']\n    y_valid = x_valid['song_popularity']\n    \n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n    \n    nn_model = tf.keras.Sequential([\n                tf.keras.layers.InputLayer(input_shape=(x_train.shape[-1],)),\n                tf.keras.layers.Dense(64,  activation=\"swish\"),\n                tf.keras.layers.Dense(128, activation=\"swish\"),\n                tf.keras.layers.Dense(256, activation=\"swish\"),\n                tf.keras.layers.Dense(128, activation=\"swish\"),\n                tf.keras.layers.Dropout(0.2),\n                tf.keras.layers.Dense(1,   activation=\"sigmoid\")\n    ])\n    nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\"])\n    cb_es = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=5, mode=\"max\", restore_best_weights=True)\n    cb_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=3, min_lr=0.0001)\n\n    history = nn_model.fit(x_train, \n                            y_train, \n                            epochs=50, \n                            validation_data=(x_valid, y_valid), \n                            batch_size=64, \n                            validation_batch_size=64,\n                            callbacks=[cb_es, cb_lr],\n                            verbose=0)\n    \n    preds_train = nn_model.predict(x_train).squeeze()\n    preds_valid = nn_model.predict(x_valid).squeeze()\n    auc_train = roc_auc_score(y_train, preds_train)\n    auc = roc_auc_score(y_valid, preds_valid)\n    print(f\"| Fold {fold+1} | train: {auc_train:.5f} | valid: {auc:.5f} |\")\n    print(\"|--------|----------------|----------------|\")\n    scores.append(auc)\n    \n    preds_test = nn_model.predict(x_test).squeeze()\n    x[\"nn\"].loc[x_valid.index] = preds_valid\n    t[\"nn\"] += preds_test\n    \nt[\"nn\"] \/= folds\nprint(\"\\nAVG AUC:\",np.mean(scores))","257d5d0d":"train[\"nn\"] = x[\"nn\"]\ntest[\"nn\"] = t[\"nn\"]","c782b9ed":"feature_cols.append(\"nn\")\ncnt_cols.append(\"nn\")","e0dd3e83":"%%time\nmi_scores = mutual_info_regression(x.iloc[:10000,:][feature_cols], x.iloc[:10000,:]['song_popularity'])\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=feature_cols)\nmi_scores = mi_scores.sort_values(ascending=False)","96d81b69":"top = 15\nplt.figure(figsize=(20,5))\nsns.barplot(x=mi_scores.values[:top], y=mi_scores.index[:top], palette=\"summer\")\nplt.title(f\"Top {top} Strong Relationships Between Feature Columns and Target Column\")\nplt.xlabel(\"Relationship with Target\")\nplt.ylabel(\"Feature Columns\")\nplt.show()","bfa9c093":"%%time\n\nfolds = 5\ntrain[\"kfold\"] = -1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(train,train[\"song_popularity\"])):\n    train.loc[valid_indicies, \"kfold\"] = fold\n    \n\nfinal_test_predictions = []\nscores = []\n\nprint(\"--------------------------------------------\")\nfor fold in range(folds):\n    x_train = train[train.kfold != fold].copy()\n    x_valid = train[train.kfold == fold].copy()\n    x_test  = test[feature_cols].copy()\n\n    y_train = x_train['song_popularity']\n    y_valid = x_valid['song_popularity']\n\n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n    xgb_params = {\n        'eval_metric': 'auc', \n        'objective': 'binary:logistic', \n        'tree_method': 'gpu_hist', \n        'gpu_id': 0, \n        'predictor': 'gpu_predictor', \n        'n_estimators': 10000, \n        'learning_rate': 0.01, \n        'gamma': 0.4, \n        'max_depth': 3, \n        'seed': 42,       \n        'min_child_weight': 300, \n        'subsample': 0.7, \n        'colsample_bytree': 0.8, \n        'colsample_bylevel': 0.9, \n        'use_label_encoder': False,\n        'lambda': 0, \n        'alpha': 10\n    }\n\n    xgb_model = XGBClassifier(**xgb_params)\n    xgb_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n\n    preds_train = xgb_model.predict_proba(x_train)[:,1]\n    preds_valid = xgb_model.predict_proba(x_valid)[:,1]\n    auc_train = roc_auc_score(y_train, preds_train)\n    auc = roc_auc_score(y_valid, preds_valid)\n    print(f\"| Fold {fold+1} | train: {auc_train:.5f} | valid: {auc:.5f} |\")\n    if fold+1 != folds:\n        print(\"|--------|----------------|----------------|\")\n    scores.append(auc)\n\n    preds_test = xgb_model.predict_proba(x_test)[:,1]\n    final_test_predictions.append(preds_test)\n\nprint(\"--------------------------------------------\")\nprint(\"AVG AUC:\",np.mean(scores), \"\\n\")","28a4236c":"d = pd.DataFrame(np.array([feature_cols,list(xgb_model.feature_importances_)]).T, columns=['feature','importance'])\nd[\"importance\"] = pd.to_numeric(d[\"importance\"])\nd = d.sort_values('importance', ascending=False)","9a1a2419":"plt.figure(figsize=(20,8))\nsns.barplot(y=\"feature\", x=\"importance\", data=d.iloc[:15])\nplt.show()","bb2e91fd":"shap_values = shap.TreeExplainer(xgb_model).shap_values(x_valid)\nshap.summary_plot(shap_values, x_valid)","5449b395":"idx = 5\ndata_for_prediction = x_valid.iloc[idx]\ndata_for_prediction_array = pd.DataFrame(data_for_prediction.values.reshape(1,-1), columns=data_for_prediction.index.tolist())\n\nprint(xgb_model.predict_proba(data_for_prediction_array))\n\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(data_for_prediction_array)\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","e5355609":"sample_submission['song_popularity'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","1869ca8f":"plt.figure(figsize=(10,5))\nsns.histplot(sample_submission[\"song_popularity\"], kde=True, color=\"blue\")\nplt.title(\"Predictions\")\nplt.show()","6087d80f":"# Blending","5fd34e34":"# Submission","a53bc323":"# Explore and Preprocess Data","edb482de":"# PCA","d8879e71":"# Some Visualization","bbc0e4ca":"# Neural Networks","61f7a98f":"# SHAP Values Feature Importance","e6f57087":"## Logistic Regression","9f60797d":"# Add New Features","4009ffc7":"# Under Sampling\n\nAs I did some Error Anaysis in [this notebook](https:\/\/www.kaggle.com\/kavehshahhosseini\/spp-error-analysis), one problem is the skewed dataset. The easiest way to tackle this problem is to under sample the larger class.\n\n[![0-49-Lg-Gs-Y4l09s-Nwc-R.png](https:\/\/i.postimg.cc\/6qDLZjWD\/0-49-Lg-Gs-Y4l09s-Nwc-R.png)](https:\/\/postimg.cc\/30Fv5ZDB)","c66b3ac3":"# Train XGBoost","94879481":"## XGBoost","06fe6f8b":"# KMeans","4f175b19":"# Model Feature Importance","a1c070e0":"# Mutual Information","5ffd1c98":"## LGBM\n\nHyperparameters are borrowed from [this notebook](https:\/\/www.kaggle.com\/venkatkumar001\/spp2-lgbm).","b1e24f87":"# Load Data","ce708a59":"# Outlier Detection"}}