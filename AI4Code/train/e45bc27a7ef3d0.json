{"cell_type":{"6524e1b7":"code","94206822":"code","3fe1993b":"code","e872fdae":"code","f60c537b":"code","be1611de":"code","d1d8b6d3":"code","c8ff2911":"code","901f0c32":"code","cc20ec1a":"code","ed7bda4b":"code","dd9c5264":"code","cb6df7f0":"code","b951714e":"code","717509b6":"code","3124684e":"code","8b63b053":"code","6e8d3c2e":"code","38e4eb56":"code","14554348":"code","3f264c76":"code","18d3bdce":"code","d81ad6a1":"code","b9a0e3f0":"code","016d2233":"code","19a40e86":"code","920a6057":"code","369878be":"code","c4143ac4":"code","b32255c4":"code","52bc1536":"code","e6f63dbb":"code","33f91bc5":"code","c2777c20":"code","a93e20c9":"code","5d420211":"code","bec7765b":"code","23a41086":"code","ac80be24":"code","80ff796d":"code","04b74275":"code","9167aab1":"code","7b23b08e":"code","a11f434a":"code","b5c6a10f":"code","a03162ae":"code","8b729851":"code","950e940b":"code","cc80fef8":"code","6af67264":"code","af6ff1be":"code","a397a572":"code","61cbfbb8":"code","fe26c311":"code","ac24c73b":"code","46e98754":"code","3dd3d2b1":"code","40b7540f":"code","87acf850":"code","22e5b333":"code","ce8da10e":"code","0b44bd77":"code","1cc34320":"code","2824177d":"code","b45a570f":"code","6c8945a0":"code","2a2e0090":"markdown","4ad21607":"markdown","11d7562f":"markdown","0469595e":"markdown","8daeecbc":"markdown","08109a49":"markdown","01e592c6":"markdown","517ebf50":"markdown","694884d1":"markdown","920da6f0":"markdown","67f09c75":"markdown","8491f20a":"markdown","5aa7cf8e":"markdown","55e9d1bb":"markdown","a7acd869":"markdown","cc353dcd":"markdown","7ac3d548":"markdown","15b740a8":"markdown","aa332809":"markdown","3f854238":"markdown","7a7dbffe":"markdown","21b44fd5":"markdown","02e8d662":"markdown","da1fd279":"markdown","7f9027d9":"markdown","f067f374":"markdown","17c1e70e":"markdown","72de1e75":"markdown","b4a9ea62":"markdown","4cd4d187":"markdown","9fabd63a":"markdown","1cf0c4fe":"markdown","55e28a2e":"markdown","03e8929d":"markdown","08b791fa":"markdown","2e738c66":"markdown","94bbacfe":"markdown","e6cf0a5a":"markdown","59788d3c":"markdown","786cea72":"markdown","75375222":"markdown","0eb28a19":"markdown","68289b6f":"markdown","e3304cee":"markdown","3e07e19c":"markdown"},"source":{"6524e1b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94206822":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # for plotting facilities\nimport seaborn as sns; \nsns.set(color_codes=True)\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3fe1993b":"df = pd.read_csv(\"\/kaggle\/input\/wholesale-customers-data-set\/Wholesale customers data.csv\")","e872fdae":"df.shape","f60c537b":"df.head()","be1611de":"print(\"Channel unique values:\",df['Channel'].unique())\nprint(\"Region unique values\",df['Region'].unique())","d1d8b6d3":"df.info()","c8ff2911":"df.describe()","901f0c32":"df.isnull().sum()","cc20ec1a":"categorical_features = ['Channel', 'Region']\ncontinuous_features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']","ed7bda4b":"df['Channel'].value_counts()","dd9c5264":"def categorical_multi(i,j):\n    pd.crosstab(df[i],df[j]).plot(kind='bar')\n    plt.show()\n    print(pd.crosstab(df[i],df[j]))\n\ncategorical_multi(i='Channel',j='Region')    ","cb6df7f0":"df.corr()","b951714e":"# Correlation analysis\ncorrMatt = df.corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nsns.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)","717509b6":"scaler = preprocessing.MinMaxScaler()\nscaled = scaler.fit_transform(df)\nscaled_data = pd.DataFrame(scaled, columns = [name for name in list(df)])\nfig = plt.figure(figsize = (20,10))\nscaled_data.boxplot(column=[name for name in list(scaled_data)], grid=False)","3124684e":"# No obvious trend in data set as sequence\n%matplotlib inline\ndf.plot(figsize=(12,6), style='.');","8b63b053":"df1 = df.copy()","6e8d3c2e":"from sklearn.preprocessing import StandardScaler\n\nstd_scale = StandardScaler().fit_transform(df1)\n\nscaled_frame = pd.DataFrame(std_scale, columns=df1.columns)\n\nscaled_frame.head()","38e4eb56":"scaled_frame.describe()","14554348":"pd.DataFrame(scaled_frame).plot(kind='kde')","3f264c76":"pd.DataFrame(scaled_frame).plot(kind='hist', bins=10)","18d3bdce":"from sklearn.preprocessing import MinMaxScaler\n\nminmax_scale = MinMaxScaler().fit_transform(df)\n\nscaled_frame2 = pd.DataFrame(minmax_scale,columns=df.columns)\n\nscaled_frame2.head()","d81ad6a1":"scaled_frame2.describe()","b9a0e3f0":"pd.DataFrame(scaled_frame2).plot(kind='kde')","016d2233":"pd.DataFrame(scaled_frame2).plot(kind='hist', bins=30)","19a40e86":"df_Con=df.drop(['Channel','Region'], axis=1)  # drop Categorical features","920a6057":"df_Con.head()","369878be":"# Histogram\nplt.figure(figsize=(10,8))\ndf_Con.plot(kind='hist', alpha=0.8,bins=60, subplots=True,layout=(3,2),legend=True,figsize=(12,10))","c4143ac4":"plot1=sns.pairplot(df_Con, diag_kind='kde')","b32255c4":"mean_df=df.describe().loc['mean',:]\nmean_df","52bc1536":"X = scaled_frame.drop(['Channel'], axis=1)\ny = df['Channel'] # Channel has 2 values so we will use channel here from main table\n\n# convert channel into binary values\ny[y == 2] = 0\ny[y == 1] = 1\n\ny.head()","e6f63dbb":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import train_test_split\n\nlr, knn = LinearRegression(), KNeighborsClassifier()","33f91bc5":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=1)","c2777c20":"print(\"Fit raw features:\")\nprint(\" LR:\", lr.fit(X_train, y_train).score(X_test, y_test))\nprint(\"KNN:\", knn.fit(X_train, y_train).score(X_test, y_test))\nprint(\"GBC: \", GradientBoostingClassifier().fit(X_train, y_train).score(X_test, y_test))\nprint(\"RFC: \", RandomForestClassifier().fit(X_train, y_train).score(X_test, y_test))","a93e20c9":"rf = RandomForestClassifier()\nrfecv = RFECV(estimator=rf)\nrfecv.fit(X, y)\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,5))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.title(\"Optimal number of features : %d\" % rfecv.n_features_)\nplt.show()","5d420211":"model = GradientBoostingClassifier()\nrfecv = RFECV(estimator=GradientBoostingClassifier())","bec7765b":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import RFE\nimport numpy as np","23a41086":"pipeline = Pipeline([('Feature Selection', rfecv), ('Model', model)])\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=42)\nn_scores = cross_val_score(pipeline, X,y, scoring='accuracy', cv=cv, n_jobs = -1)\nnp.mean(n_scores)","ac80be24":"pipeline.fit(X,y)","80ff796d":"print(\"Optimal number of features : %d\" % rfecv.n_features_)","04b74275":"rfecv.support_","9167aab1":"rfecv_df = pd.DataFrame(rfecv.ranking_, index=X.columns, columns = ['Rank']).sort_values(by='Rank', ascending=True)\nrfecv_df.head(10)","7b23b08e":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score ( nb of correct classification)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","a11f434a":"from sklearn.cluster import KMeans\n# from sklearn.metrics import silhouette_score","b5c6a10f":"# First we need to convert our categorical features (region and channel) to dummy variable:\n# df2 = pd.get_dummies(df)\n# X1 = df2.iloc[:,:].values","a03162ae":"wcss = []\nfor i in range(2, 16):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(scaled_frame) # Standard scaler \n    wcss.append(kmeans.inertia_)\n    cluster_labels = kmeans.labels_\n    # silhouette score\n#     silhouette_avg = silhouette_score(df, cluster_labels)\n#     print(\"For n_clusters={0}, the silhouette score is {1}\".format(i, silhouette_avg))\nplt.plot(range(2, 16), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('n: Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","8b729851":"kmeans = KMeans(n_clusters = 6,random_state = 111)\ny_means = kmeans.fit(scaled_frame)","950e940b":"y_means","cc80fef8":"print('SSE: ', kmeans.inertia_)\nprint('\\nCentroids: \\n', kmeans.cluster_centers_)","6af67264":"#count number of records in every cluster\npd.Series(kmeans.labels_).value_counts()","af6ff1be":"kmeans = KMeans(n_clusters=6, init='k-means++', max_iter=400, n_init=100, random_state=0)\ny_means = kmeans.fit(scaled_frame)","a397a572":"pca2 = PCA(n_components=2).fit(scaled_frame)\npca2d = pca2.transform(scaled_frame)\n\nprint(\"Explained variance is:\",pca2.explained_variance_)\nprint(\"Explained variance ratio\",pca2.explained_variance_ratio_)\nprint(\"Variance for 1st component is 38.75% & 2nd component is 22.37%\")","61cbfbb8":"pca4 = PCA(n_components=4).fit(scaled_frame)\npca4d = pca4.transform(scaled_frame)\n\nprint(\"Explained variance is:\",pca4.explained_variance_)\nprint(\"Explained variance ratio is:\",pca4.explained_variance_ratio_)\n\nplt.figure(figsize = (8,8))\nsns.scatterplot(pca4d[:,0], pca4d[:,1], \n                hue=y_means.labels_, \n                palette='Set1',\n                s=100, alpha=0.2).set_title('KMeans Clusters (4) Derived from Elbow Method', fontsize=15)\nplt.legend()\nplt.ylabel('PC2')\nplt.xlabel('PC1')\nplt.show()","fe26c311":"X = df.drop('Channel', axis=1)\n\ny = df['Channel']","ac24c73b":"X.head()","46e98754":"y.head()","3dd3d2b1":"# convert labels into binary values\n\ny[y == 2] = 0\n\ny[y == 1] = 1","40b7540f":"y.head()","87acf850":"# import sys\n# !{sys.executable} -m pip install xgboost\n# to install xgboost","22e5b333":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold","ce8da10e":"model = XGBClassifier(eval_metric='mlogloss')\nkfold = KFold(n_splits=5)","0b44bd77":"results = cross_val_score(model, X, y, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\nAccuracy = results.mean()*100","1cc34320":"# max_depth = 5, alpha = 10, n_estimators = 10)\n\nmodel = XGBClassifier(eval_metric='mlogloss')\nkfold = KFold(n_splits=5)","2824177d":"PW = cross_val_score(model, X, y, cv=kfold, scoring='precision_weighted')\nRscore = cross_val_score(model, X, y, cv=kfold, scoring='recall_weighted')","b45a570f":"Recall = (round((Rscore.mean()*100),3))\nPrecision = (round((PW.mean()*100),3))\n\nf1score=round(2*((Recall*Precision)\/(Recall+Precision)),3)","6c8945a0":"print(\"Accuracy= %f, Recall=%f, Precision=%f, f1score=%f\" % (Accuracy,Recall,Precision,f1score))","2a2e0090":"## Outliers","4ad21607":" Implementing Scaling Using the \n ## MinMax Scaler Method","11d7562f":"We can see that no null value and all the data inputs are having numeric datatype","0469595e":"The histograms show an exponential decline in the number of orders for the respected products. hence this could be a cluster consisting of larger size companies with higher purchase quanitites for these particular items.","8daeecbc":"We have concluded that Milk, Grocery, Frozen, Detergents_Paper and Delicassen are optimal features","08109a49":"Here we will use RFECV using GradientBoostingClassifier with pipleline","01e592c6":"# EDA","517ebf50":"From the above output, we can observe that the principal component 1 holds 38.75% of the information while the principal component 2 holds only 22.37% , 3 holds 12% and 4 holds 9%of the information.","694884d1":"### Channel Count","920da6f0":"Ans: In Standardscaler, it assumes that data has normally distributed features and it has scaled them to zero mean and 1 standard deviation.\n\nAll the features are of the same scale after applying the scaler.\n\nWhile in Minmaxscaler, it shrinks the data within the range of -1 to 1(if there are negative values) and responds well if standard deviation is small and is used when distribution is not Gaussian.This scaler is sensitive to outliers.\n\n\nIn Standard scaler, centered curves are there with no outliers and in minmax, outliers are there.\n\nSo here, we will continue with standard scaler.","67f09c75":"Q. Implement XGBoost Classifier with 5 Fold CV and report the performance metrics","8491f20a":"y label contain values as 1 and 2\nWe will convert it into 0 and 1 for further analysis.","5aa7cf8e":"Q. Implement PCA with number of original features to answer how much variance is explained by first 2 components and by first 4 components and visualize the clusters in the data","55e9d1bb":"We will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains.","a7acd869":"From the above output, we can observe that the principal component 1 holds 38.75% of the information while the principal component 2 holds only 22.37% of the information.","cc353dcd":"We can see that Channel variable contains values as 1 and 2.\n\nThese two values classify the customers from two different channels as\n\n1 for Horeca (Hotel\/Retail\/Caf\u00e9) customers and\n2 for Retail channel (nominal) customers.\n\nRegion - 3 unique values\nLisnon, Oporto or Other (Nominal)","7ac3d548":"Let\u2019s fit the pipeline and then obtain the optimal number of features.","15b740a8":"Using the MinMaxScaler method, we have again scaled the data into a uniform unit over all the columns. As you can see in the preceding table, the values of all the features have been converted into a uniform range of the same scale[0,1].","aa332809":"Instead of manually configuring the number of features(RFE), it would be very nice if we could automatically select them. This can be achieved via recursive feature elimination and cross-validation. This is done via the RFECV.","3f854238":"Implementing Scaling Using the\n## Standard Scaler Method","7a7dbffe":"Using the StandardScaler method, we have scaled the data into a uniform unit over all the columns. As you can see in the preceding table, the values of all the features have been converted into a uniform range of the same scale. Because of this, it becomes easier for the model to make predictions.\nKind kde is seems to be better then hist because in hist, data seems to be overlapped and unable to see the value for each field, whereas for kde, data visualization is finer.","21b44fd5":"Once fitted, the following attributes can be obtained:\n  \n    ranking_ \u2014 the ranking of the features.\n    n_features_ \u2014 the number of features that have been selected.\n    support_ \u2014 an array that indicates whether or not a feature was selected.\n    grid_scores_ \u2014 the scores obtained from cross-validation.","02e8d662":"## Summary of dataset","da1fd279":"Feature selection is an important task and it is crucial when the data in question has many features. The optimal number of features also leads to improved model accuracy. Obtaining the most important features and the number of optimal features can be obtained via feature importance or feature ranking.","7f9027d9":"This shows that no outliers present in the dataset","f067f374":"This graph looks like elbow and we have to(can) determine that elbow point.\nHere the elbow point comes at around 6 and this our optimal number of clusters for the above data which we should choose.\nIf we look at the figure carefully after 6 when we go on increasing the number of cluster WCSS reduces slightly.","17c1e70e":" Pipeline \u2014 since we\u2019ll perform some cross-validation. It\u2019s best practice in order to avoid data leakage.\n    RepeatedStratifiedKFold \u2014 for repeated stratified cross-validation.\n    cross_val_score \u2014 for evaluating the score on cross-validation.\n    GradientBoostingClassifier \u2014 the estimator we\u2019ll use.\n    numpy \u2014 so that we can compute the mean of the scores.","72de1e75":" 6 continuous types of feature ('Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen')\n 2 categoricals features ('Channel', 'Region')","b4a9ea62":"Q. Find optimal number of features using RFECV and show the plot between Number of features \nselected vs Cross validation score (use channel as target variable)\n","4cd4d187":"## Recursive Feature Elimination","9fabd63a":"## Summary statistics of dataset ","1cf0c4fe":"440 instances and 8 attributes ","55e28a2e":"In this kernel, I have implemented XGBoostclassifier with Python and Scikit-Learn to classify the customers from two different channels as Horeca (Hotel\/Retail\/Caf\u00e9) customers or Retail channel (nominal) customers.\n\nThe y labels contain values as 1 and 2. We have converted them into 0 and 1 for further analysis.\n\nWe have performed k-fold cross-validation with XGBoost classifier.\nThis example summarizes the performance of the default model configuration on the dataset including both the mean and standard deviation classification accuracy.","03e8929d":"With the grid_scores_ we can plot a graph showing the cross-validated scores","08b791fa":"## Check missing value","2e738c66":"##  Preview dataset ","94bbacfe":"Lets put that into a dataframe and check the result","e6cf0a5a":"## Introduction\n\nThe data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units on diverse product categories The dataset is available at UCI machine learning repository.\n\nData Set Information:\n\nProvide all relevant information about your data set.\n\n\n### Attribute Information:\n\n1) FRESH: annual spending on fresh products (Continuous);\n2) MILK: annual spending on milk products (Continuous);\n3) GROCERY: annual spending on grocery products (Continuous);\n4) FROZEN: annual spending on frozen products (Continuous)\n5) DETERGENTS_PAPER: annual spending on detergents and paper products (Continuous)\n6) DELICATESSEN: annual spending on and delicatessen products (Continuous);\n7) CHANNEL: customers\u2122 Channel - Horeca (Hotel\/Restaurant\/Cafe) or Retail channel (Nominal)\n8) REGION: customers\u2122 Region\u201c Lisnon, Oporto or Other (Nominal)\n\n### Descriptive Statistics:\n\n(Minimum, Maximum, Mean, Std. Deviation)\nFRESH ( 3, 112151, 12000.30, 12647.329)\nMILK (55, 73498, 5796.27, 7380.377)\nGROCERY (3, 92780, 7951.28, 9503.163)\nFROZEN (25, 60869, 3071.93, 4854.673)\nDETERGENTS_PAPER (3, 40827, 2881.49, 4767.854)\nDELICATESSEN (3, 47943, 1524.87, 2820.106)\n\nREGION Frequency\nLisbon 77\nOporto 47\nOther Region 316\nTotal 440\n\nCHANNEL Frequency\nHoreca 298\nRetail 142\nTotal 440\n\n### Implementation:\n\n    EDA and any data cleaning\n    Implemented Feature Scaling to Normalize the data(compare the histogram and KDE for MinMaxScaler and StandardScaler)\n    Finding optimal number of features using RFECV and shown the plot between Number of features selected vs Cross validation score (used channel as target variable)\n    Implemented KMeans Clustering for K=2 to K=15 and based on elbow method identify what is the optimum number of clusters\n    Implemented PCA with number of original features to answer how much variance is explained by first 2 components and by first 4 components and visualize the clusters in the data\n    Implemented XGBoost Classifier with 5 Fold CV and report the performance metrics\n\n### Task:\nGoal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with.\n\n","59788d3c":"We can see that there are no missing values in the dataset. Thus, there is no need to use the dropna() function.","786cea72":"Looking at the plot above, there are a few pairs of features that exhibit some degree of correlation. They include:\n\n#### Grocery and Detergents_Paper are highly correlated  - 0.92\n#### Milk and Groceries - 0.73\n#### Milk and Detergents_Paper - 0.66","75375222":"## Shape of Data set","0eb28a19":"## Train the XGBoost Classifier ","68289b6f":"Q. Implement Feature Scaling to Normalize the data(compare the histogram\/KDE for MinMaxScaler \nand StandardScaler). Choose one of the Scaler to proceed ahead and provide reasoning as to \nwhy it was selected?\n\nSome features in our data might have high-magnitude values (for example, annual salary), while others might have relatively low values (for example, the number of years worked at a company). Just because some data has smaller values does not mean it is less significant. So, to make sure our prediction does not vary because of different magnitudes of features in our data, we can perform feature scaling, standardization, or normalization\n\n","e3304cee":"### k-fold Cross Validation using XGBoost","3e07e19c":"Q. Implement KMeans Clustering for K=2 to K=15 and based on elbow method identify what is the optimum number of clusters"}}