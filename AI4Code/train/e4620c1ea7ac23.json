{"cell_type":{"f0263458":"code","57598ea7":"code","07a06e93":"code","b70e0da0":"code","19d5a8be":"code","58a3ee8b":"code","a1ba0d14":"code","a41053fe":"code","dc952023":"code","d78d190e":"code","bb7b0be5":"code","c136538d":"code","61112263":"code","b6184128":"code","237f863f":"code","6e662fa1":"code","0b2d3779":"code","6c10991c":"code","da218d40":"code","b50d679b":"code","7b3667a4":"code","1f5e123f":"code","b7222850":"code","7230b381":"code","f964ec05":"code","3a4e67ff":"code","b7c670d1":"code","a800201c":"code","7f206439":"code","82b12436":"code","3ae3a199":"code","5f17aabe":"code","00b384b2":"code","1969fb79":"code","24b08692":"code","bf60506f":"code","8c603be3":"code","c20bd2b3":"code","014e142d":"code","214275a8":"code","470ca656":"code","e32acaa8":"code","6b0ceca6":"code","ce84ed6c":"code","70756880":"code","0d64cb52":"code","e4232a6e":"code","30494e2d":"code","22bf9b43":"markdown","5ed1eb8f":"markdown","2d8e73cd":"markdown","4add6690":"markdown","56b97016":"markdown","3e510830":"markdown","3efff404":"markdown","afd09964":"markdown","8647c05d":"markdown","af599cb9":"markdown","bc9d8f03":"markdown","98247db4":"markdown","a07cf72e":"markdown","fa7370da":"markdown","05624e5b":"markdown","904ee21f":"markdown","873ebff1":"markdown","b3d42bc6":"markdown","74d7ce09":"markdown","7e96ecff":"markdown","24384c53":"markdown","73cfce4a":"markdown","eb12c8b5":"markdown","89721062":"markdown","a3b37f20":"markdown","7470236e":"markdown"},"source":{"f0263458":"from torch.utils.data import DataLoader, ConcatDataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as T","57598ea7":"IMAGE_DIR = \"..\/input\/pokemon-images-dataset\"\nimage_size = 64\nbatch_size = 8\nnormalization_stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # Convert channels from [0, 1] to [-1, 1]","07a06e93":"normal_dataset = ImageFolder(IMAGE_DIR, transform=T.Compose([\n    T.Resize(image_size),\n    T.CenterCrop(image_size),\n    T.ToTensor(),\n    T.Normalize(*normalization_stats)]))\n\n# Augment the dataset with mirrored images\nmirror_dataset = ImageFolder(IMAGE_DIR, transform=T.Compose([\n    T.Resize(image_size),\n    T.CenterCrop(image_size),\n    T.RandomHorizontalFlip(p=1.0),\n    T.ToTensor(),\n    T.Normalize(*normalization_stats)]))\n\n# Augment the dataset with color changes\ncolor_jitter_dataset = ImageFolder(IMAGE_DIR, transform=T.Compose([\n    T.Resize(image_size),\n    T.CenterCrop(image_size),\n    T.ColorJitter(0.5, 0.5, 0.5),\n    T.ToTensor(),\n    T.Normalize(*normalization_stats)]))\n\n# Combine the datasets\ndataset_list = [normal_dataset, mirror_dataset, color_jitter_dataset]\ndataset = ConcatDataset(dataset_list)\n\ndataloader = DataLoader(dataset, batch_size, shuffle=True, num_workers=4, pin_memory=False)","b70e0da0":"def denorm(image):\n    return image * normalization_stats[1][0] + normalization_stats[0][0]","19d5a8be":"import torch\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n%matplotlib inline","58a3ee8b":"def show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n    \ndef show_batch(dataloader, nmax=64):\n    for images, _ in dataloader:\n        show_images(images, nmax)\n        break","a1ba0d14":"show_batch(dataloader)","a41053fe":"import torch.nn as nn","dc952023":"disc_1 = nn.Sequential(\n    # Input is 3 x 256 x 256\n    nn.Conv2d(3, 16, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(16),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 16 x 128 x 128\n    \n    nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(32),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 32 x 64 x 64\n    \n    nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 64 x 32 x 32\n    \n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 128 x 16 x 16\n    \n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 256 x 8 x 8\n    \n    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 512 x 4 x 4\n    \n    # With a 4x4, we can condense the channels into a 1 x 1 x 1 to produce output\n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.Flatten(),\n    nn.Sigmoid()\n)","d78d190e":"disc_2 = torch.hub.load('pytorch\/vision:v0.6.0', 'resnet50', pretrained=True)\ndisc_2.fc.out_features = 1\ndisc_2","bb7b0be5":"# Same as Discriminator 1, but with smaller kernel size\ndisc_3 = nn.Sequential(\n    # Input is 3 x 256 x 256\n    nn.Conv2d(3, 16, kernel_size=2, stride=2, padding=0, bias=False),\n    nn.BatchNorm2d(16),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 16 x 128 x 128\n    \n    nn.Conv2d(16, 32, kernel_size=2, stride=2, padding=0, bias=False),\n    nn.BatchNorm2d(32),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 32 x 64 x 64\n    \n    nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 64 x 32 x 32\n    \n    nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 128 x 16 x 16\n    \n    nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 256 x 8 x 8\n    \n    nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 512 x 4 x 4\n    \n    # Additional layer to make it 2x2\n    nn.Conv2d(512, 1024, kernel_size=2, stride=2, padding=0, bias=False),\n    nn.BatchNorm2d(1024),\n    nn.LeakyReLU(0.3, inplace=True),\n    # Layer Output: 512 x 2 x 2\n    \n    # With a 2x2, we can condense the channels into a 1 x 1 x 1 to produce output\n    nn.Conv2d(1024, 1, kernel_size=2, stride=1, padding=0, bias=False),\n    nn.Flatten(),\n    nn.Sigmoid()\n)","c136538d":"disc_5 = nn.Sequential(\n    # Input is 3 x 256 x 256\n    nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(32),\n    nn.LeakyReLU(0.15, inplace=True),\n    # Layer Output: 64 x 128 x 128\n    \n    nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.15, inplace=True),\n    # Layer Output: 128 x 64 x 64\n    \n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.15, inplace=True),\n    # Layer Output: 256 x 32 x 32\n    \n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.15, inplace=True),\n    # Layer Output: 256 x 16 x 16\n    \n    nn.Conv2d(256, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.15, inplace=True),\n    # Layer Output: 256 x 8 x 8\n    \n    nn.Conv2d(256, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.15, inplace=True),\n    # Layer Output: 256 x 4 x 4\n    \n    # With a 4x4, we can condense the channels into a 1 x 1 x 1 to produce output\n    nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.Flatten(),\n    nn.Sigmoid()\n)","61112263":"disc_64_1 = nn.Sequential(\n    # Input is 3 x 64 x 64\n    nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(32),\n    nn.LeakyReLU(0.2, inplace=True),\n    # Layer Output: 32 x 32 x 32\n    \n    nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    # Layer Output: 64 x 16 x 16\n    \n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # Layer Output: 128 x 8 x 8\n    \n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True),\n    # Layer Output: 256 x 4 x 4\n    \n    # With a 4x4, we can condense the channels into a 1 x 1 x 1 to produce output\n    nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.Flatten(),\n    nn.Sigmoid()\n)","b6184128":"disc_64_2 = nn.Sequential(\n    # Input is 3 x 64 x 64\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    # Layer Output: 64 x 32 x 32\n    \n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # Layer Output: 128 x 16 x 16\n    \n    nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # Layer Output: 128 x 8 x 8\n    \n    nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # Layer Output: 128 x 4 x 4\n    \n    # With a 4x4, we can condense the channels into a 1 x 1 x 1 to produce output\n    nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.Flatten(),\n    nn.Sigmoid()\n)","237f863f":"seed_size = 16","6e662fa1":"gen_1 = nn.Sequential(\n    # Input seed_size x 1 x 1\n    nn.ConvTranspose2d(seed_size, 512, kernel_size=4, padding=0, stride=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    # Layer output: 512 x 4 x 4\n    \n    nn.ConvTranspose2d(512, 256, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # Layer output: 256 x 8 x 8\n    \n    nn.ConvTranspose2d(256, 128, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # Layer output: 128 x 16 x 16\n    \n    nn.ConvTranspose2d(128, 64, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # Layer output: 64 x 32 x 32\n    \n    nn.ConvTranspose2d(64, 32, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(32),\n    nn.ReLU(True),\n    # Layer output: 32 x 64 x 64\n    \n    nn.ConvTranspose2d(32, 16, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(16),\n    nn.ReLU(True),\n    # Layer output: 16 x 128 x 128\n    \n    nn.ConvTranspose2d(16, 3, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.Tanh()\n    # Output: 3 x 256 x 256\n)","0b2d3779":"# Generator matching Discriminator 3\ngen_3 = nn.Sequential(\n    # Input seed_size x 1 x 1\n    nn.ConvTranspose2d(seed_size, 1024, kernel_size=2, padding=0, stride=1, bias=False),\n    nn.BatchNorm2d(1024),\n    nn.ReLU(True),\n    # Layer output: 1024 x 2 x 2\n    \n    nn.ConvTranspose2d(1024, 512, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    # Layer output: 512 x 4 x 4\n    \n    nn.ConvTranspose2d(512, 256, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # Layer output: 256 x 8 x 8\n    \n    nn.ConvTranspose2d(256, 128, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # Layer output: 128 x 16 x 16\n    \n    nn.ConvTranspose2d(128, 64, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # Layer output: 64 x 32 x 32\n    \n    nn.ConvTranspose2d(64, 32, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.BatchNorm2d(32),\n    nn.ReLU(True),\n    # Layer output: 32 x 64 x 64\n    \n    nn.ConvTranspose2d(32, 16, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.BatchNorm2d(16),\n    nn.ReLU(True),\n    # Layer output: 16 x 128 x 128\n    \n    nn.ConvTranspose2d(16, 3, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.Tanh()\n    # Output: 3 x 256 x 256\n)","6c10991c":"# Generator with lots of upsampling weirdness\ngen_4 = nn.Sequential(\n    # Input seed_size x 1 x 1\n    nn.ConvTranspose2d(seed_size, 1024, kernel_size=2, padding=0, stride=1, bias=False),\n    nn.BatchNorm2d(1024),\n    nn.ReLU(True),\n    # Layer output: 1024 x 2 x 2\n    \n    nn.ConvTranspose2d(1024, 512, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    # Layer output: 512 x 4 x 4\n    \n    nn.ConvTranspose2d(512, 256, kernel_size=2, padding=0, stride=2, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # Layer output: 256 x 8 x 8\n    \n    nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1, stride=9, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # Layer output: 128 x 64 x 64\n    \n    nn.ConvTranspose2d(128, 3, kernel_size=4, padding=0, stride=4, bias=False),\n    nn.Tanh()\n    # Output: 3 x 256 x 256\n)","da218d40":"gen_5 = nn.Sequential(\n    # Input seed_size x 1 x 1\n    nn.ConvTranspose2d(seed_size, 256, kernel_size=4, padding=0, stride=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # Layer output: 256 x 4 x 4\n    \n    nn.ConvTranspose2d(256, 256, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # Layer output: 256 x 8 x 8\n    \n    nn.ConvTranspose2d(256, 256, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # Layer output: 256 x 16 x 16\n    \n    nn.ConvTranspose2d(256, 128, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # Layer output: 128 x 32 x 32\n    \n    nn.ConvTranspose2d(128, 64, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # Layer output: 128 x 64 x 64\n    \n    nn.ConvTranspose2d(64, 32, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(32),\n    nn.ReLU(True),\n    # Layer output: 64 x 128 x 128\n    \n    nn.ConvTranspose2d(32, 3, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.Tanh()\n    # Output: 3 x 256 x 256\n)","b50d679b":"gen_64_1 = nn.Sequential(\n    # Input seed_size x 1 x 1\n    nn.ConvTranspose2d(seed_size, 256, kernel_size=4, padding=0, stride=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # Layer output: 256 x 4 x 4\n    \n    nn.ConvTranspose2d(256, 128, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # Layer output: 128 x 8 x 8\n    \n    nn.ConvTranspose2d(128, 64, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # Layer output: 64 x 16 x 16\n    \n    nn.ConvTranspose2d(64, 32, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(32),\n    nn.ReLU(True),\n    # Layer output: 32 x 32 x 32\n    \n    nn.ConvTranspose2d(32, 3, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.Tanh()\n    # Output: 3 x 64 x 64\n)","7b3667a4":"gen_64_2 = nn.Sequential(\n    # Input seed_size x 1 x 1\n    nn.ConvTranspose2d(seed_size, 128, kernel_size=4, padding=0, stride=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # Layer output: 256 x 4 x 4\n    \n    nn.ConvTranspose2d(128, 128, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # Layer output: 128 x 8 x 8\n    \n    nn.ConvTranspose2d(128, 128, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # Layer output: 64 x 16 x 16\n    \n    nn.ConvTranspose2d(128, 64, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # Layer output: 32 x 32 x 32\n    \n    nn.ConvTranspose2d(64, 3, kernel_size=4, padding=1, stride=2, bias=False),\n    nn.Tanh()\n    # Output: 3 x 64 x 64\n)","1f5e123f":"test_model_size = False\n\nif test_model_size:\n    # Make some latent tensors to seed the generator\n    seed_batch = torch.randn(batch_size, seed_size, 1, 1, device=device)\n\n    # Get some fake pokemon\n    generator=gen_64_1\n    to_device(generator, device)\n    fake_pokemon = generator(seed_batch)\n    print(fake_pokemon.size())","b7222850":"def get_training_device():\n    # Use the GPU if possible\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    # Otherwise use the CPU :-(\n    return torch.device('cpu')\n\ndef to_device(data, device):\n    # This moves the tensors to the device (GPU, CPU)\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dataloader, device):\n        self.dataloader = dataloader\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dataloader: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dataloader)","7230b381":"device = get_training_device()\ndevice","f964ec05":"# Using the dataloader from the top of the notebook, and the selected device\n# create a device data loader\ndev_dataloader = DeviceDataLoader(dataloader, device)","3a4e67ff":"def train_discriminator(real_pokemon, disc_optimizer):\n    # Reset the gradients for the optimizer\n    disc_optimizer.zero_grad()\n    \n    # Train on the real images\n    real_predictions = discriminator(real_pokemon)\n    # real_targets = torch.zeros(real_pokemon.size(0), 1, device=device) # All of these are real, so the target is 0.\n    real_targets = torch.rand(real_pokemon.size(0), 1, device=device) * (0.1 - 0) + 0 # Add some noisy labels to make the discriminator think harder.\n    real_loss = F.binary_cross_entropy(real_predictions, real_targets) # Can do binary loss function because it is a binary classifier\n    real_score = torch.mean(real_predictions).item() # How well does the discriminator classify the real pokemon? (Higher score is better for the discriminator)\n    \n    # Make some latent tensors to seed the generator\n    latent_batch = torch.randn(batch_size, seed_size, 1, 1, device=device)\n    \n    # Get some fake pokemon\n    fake_pokemon = generator(latent_batch)\n    \n    # Train on the generator's current efforts to trick the discriminator\n    gen_predictions = discriminator(fake_pokemon)\n    # gen_targets = torch.ones(fake_pokemon.size(0), 1, device=device)\n    gen_targets = torch.rand(fake_pokemon.size(0), 1, device=device) * (1 - 0.9) + 0.9 # Add some noisy labels to make the discriminator think harder.\n    gen_loss = F.binary_cross_entropy(gen_predictions, gen_targets)\n    gen_score = torch.mean(gen_predictions).item() # How well did the discriminator classify the fake pokemon? (Lower score is better for the discriminator)\n    \n    # Update the discriminator weights\n    total_loss = real_loss + gen_loss\n    total_loss.backward()\n    disc_optimizer.step()\n    return total_loss.item(), real_score, gen_score","b7c670d1":"def train_generator(gen_optimizer):\n    # Clear the generator gradients\n    gen_optimizer.zero_grad()\n    \n    # Generate some fake pokemon\n    latent_batch = torch.randn(batch_size, seed_size, 1, 1, device=device)\n    fake_pokemon = generator(latent_batch)\n    \n    # Test against the discriminator\n    disc_predictions = discriminator(fake_pokemon)\n    targets = torch.zeros(fake_pokemon.size(0), 1, device=device) # We want the discriminator to think these images are real.\n    loss = F.binary_cross_entropy(disc_predictions, targets) # How well did the generator do? (How much did the discriminator believe the generator?)\n    \n    # Update the generator based on how well it fooled the discriminator\n    loss.backward()\n    gen_optimizer.step()\n    \n    # Return generator loss\n    return loss.item()","a800201c":"import os\nfrom torchvision.utils import save_image\n\nRESULTS_DIR = 'results'\nos.makedirs(RESULTS_DIR, exist_ok=True)","7f206439":"def save_results(index, latent_batch, show=True):\n    # Generate fake pokemon\n    fake_pokemon = generator(latent_batch)\n    \n    # Make the filename for the output\n    fake_file = \"result-image-{0:0=4d}.png\".format(index)\n    \n    # Save the image\n    save_image(denorm(fake_pokemon), os.path.join(RESULTS_DIR, fake_file), nrow=8)\n    print(\"Result Saved!\")\n    \n    if show:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_pokemon.cpu().detach(), nrow=8).permute(1, 2, 0))","82b12436":"from tqdm.notebook import tqdm\nimport torch.nn.functional as F\n\n# Static generation seed batch\nfixed_latent_batch = torch.randn(64, seed_size, 1, 1, device=device)\n\ndef train(epochs, learning_rate, start_idx=1):\n    # Empty the GPU cache to save some memory\n    torch.cuda.empty_cache()\n    \n    # Track losses and scores\n    disc_losses = []\n    disc_scores = []\n    gen_losses = []\n    gen_scores = []\n    \n    # Create the optimizers\n    disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n    gen_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n    \n    # Run the loop\n    for epoch in range(epochs):\n        # Go through each image\n        for real_img, _ in tqdm(dev_dataloader):\n            # Train the discriminator\n            disc_loss, real_score, gen_score = train_discriminator(real_img, disc_optimizer)\n\n            # Train the generator\n            gen_loss = train_generator(gen_optimizer)\n        \n        # Collect results\n        disc_losses.append(disc_loss)\n        disc_scores.append(real_score)\n        gen_losses.append(gen_loss)\n        gen_scores.append(gen_score)\n        \n        # Print the losses and scores\n        print(\"Epoch [{}\/{}], gen_loss: {:.4f}, disc_loss: {:.4f}, real_score: {:.4f}, gen_score: {:.4f}\".format(\n            epoch+start_idx, epochs, gen_loss, disc_loss, real_score, gen_score))\n        \n        # Save the images and show the progress\n        save_results(epoch + start_idx, fixed_latent_batch, show=False)\n    \n    # Return stats\n    return disc_losses, disc_scores, gen_losses, gen_scores","3ae3a199":"device = get_training_device()\ndevice","5f17aabe":"def debug_memory():\n    import collections, gc, resource, torch\n    print('maxrss = {}'.format(\n        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n    tensors = collections.Counter((str(o.device), o.dtype, tuple(o.shape))\n                                  for o in gc.get_objects()\n                                  if torch.is_tensor(o))\n    for line in tensors.items():\n        print('{}\\t{}'.format(*line))","00b384b2":"mem_debug = False\nif mem_debug:\n    debug_memory()","1969fb79":"# Clean up everything\ncleanup = False\nif cleanup:\n    import gc\n    del dev_dataloader\n    del discriminator\n    del generator\n    dev_dataloader = None\n    discriminator = None\n    generator = None\n    gc.collect()\n    torch.cuda.empty_cache()","24b08692":"# Re-initialize the device dataloader\ndev_dataloader = DeviceDataLoader(dataloader, device)","bf60506f":"# Discriminators\n# discriminator = disc_1\n# discriminator = disc_2\n# discriminator = disc_3\n# discriminator = disc_5\n\n# 64 x 64 Discriminators\n# discriminator = disc_64_1\ndiscriminator = disc_64_2\n\n# Send to device\ndiscriminator = to_device(discriminator, device)","8c603be3":"# Generators\n# generator = gen_1\n# generator = gen_3\n# generator = gen_5\n\n# 64 x 64 Generators\n# generator = gen_64_1\ngenerator = gen_64_2\n\n# Send to device\ngenerator = to_device(generator, device)","c20bd2b3":"# learning_rate = 0.0025 # worked fairly well for disc\/gen_64_1\nlearning_rate = 0.00275\nepochs = 50","014e142d":"history = train(epochs, learning_rate)","214275a8":"from IPython.display import Image","470ca656":"Image('.\/results\/result-image-0010.png')","e32acaa8":"Image('.\/results\/result-image-0025.png')","6b0ceca6":"Image('.\/results\/result-image-0050.png')","ce84ed6c":"# Extract metrics\ndisc_losses, disc_scores, gen_losses, gen_scores = history","70756880":"# Plot generator and discriminator losses\nplt.plot(disc_losses, '-')\nplt.plot(gen_losses, '-')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Discriminator', 'Generator'])\nplt.title('Losses');","0d64cb52":"# Plots scores vs. epochs\nplt.plot(disc_scores, '-')\nplt.plot(gen_scores, '-')\nplt.xlabel('Epoch')\nplt.ylabel('Score')\nplt.legend(['Real', 'Fake'])\nplt.title('Scores');","e4232a6e":"# Save generator and discriminator weights\ntorch.save(discriminator.state_dict(), 'discriminator-model.pth')\ntorch.save(generator.state_dict(), 'generator-model.pth')","30494e2d":"# Commit to Jovian\n# !pip install jovian\nimport jovian\nproject_name = 'course-project-pokegan'\njovian.commit(project=project_name)","22bf9b43":"Now let's show a sample batch of real Pokemon images","5ed1eb8f":"## Results Viewer\nWe want to be able to see the results as we generate pokemon","2d8e73cd":"Since these images have been normalized to [-1, 1],  we need to denormalize them in order to view them. Below is a denormalization function to do just that.","4add6690":"If the above output doesn't say something about 'cuda', then make sure the notebook is set up to run on the GPU accelerator.  \n\nNow, we use the device dataloader class to send data to our chosen device:","56b97016":"## Discriminator","3e510830":"And by 50 epochs, more distinct features start appearing:","3efff404":"# Discriminator Models\n\nBelow I make a few possible discriminator models to try out","afd09964":"# Generator Models\n\nBelow is a list of some possible generator models to try. These models are inverse operations of the discriminators.","8647c05d":"This is my first effort at making a discriminator for this dataset","af599cb9":"# GPU Clean-up\nSometimes the Kaggle GPU runs out of memory. This block frees up the GPU and resends the device dataloader to the GPU","bc9d8f03":"There are some blobs appearing at 10 epochs","98247db4":"# Model Testing\n\nThis is to make sure everything is the correct size","a07cf72e":"Now that we know the visualization works, we can continue with making the GAN. First, we'll start with the discriminator","fa7370da":"## Generator","05624e5b":"# Acknowledgements\n\nThis wouldn't be possible without some good resources:\n\n- Jovian.ml DCGAN example from PyTorch Zero to GANs Course: https:\/\/jovian.ml\/aakashns\/06b-anime-dcgan\n- Tips and tricks about training GANs: https:\/\/medium.com\/@utk.is.here\/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9","904ee21f":"# Performance Analysis\n\nHere are some plots to help tune the generator\/discriminator. This shows losses and scores over time.","873ebff1":"At 25 epochs the fake pokemon have more defined shapes:","b3d42bc6":"# View Results\nHere, we will view the results for the PokeGAN using IPython's Image class","74d7ce09":"# GPU Setup\nWe should train this on a GPU, so I'll set that up right now","7e96ecff":"Here's the first generator:","24384c53":"# **Training Functions**","73cfce4a":"# **Full Training**","eb12c8b5":"# Analyzing the PokeDex\nLet's take a look at what Pokemon there are to work with","89721062":"# Training Time\nHere I'll edit hyperparameters and train the GAN","a3b37f20":"# Exporting Data\nIf the model works well, it definitely should be saved for later! Here I will export the weights for both the discriminator and generator, as well as send the notebook to Jovian.ml","7470236e":"# Selecting Models\nHere, I select the models I want to send to the GPU"}}