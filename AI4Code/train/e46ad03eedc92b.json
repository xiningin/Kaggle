{"cell_type":{"920ca180":"code","c73af71c":"code","b6fae6cf":"code","5ef06946":"code","329b35b5":"code","8870e623":"code","d506a19c":"code","0073ff00":"code","fc6e0ed5":"code","d491adfd":"code","e8df428d":"code","91cdf438":"code","e60e9669":"code","f6d184f9":"code","a66b899f":"code","8f9f2bff":"code","a1976392":"code","f5b3143a":"markdown","4e90e39e":"markdown"},"source":{"920ca180":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization\nimport matplotlib.pyplot as plt # matplotlib\nfrom datetime import date\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c73af71c":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\",index_col=\"PassengerId\")\ntrain_df.head()","b6fae6cf":"test_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\",index_col=\"PassengerId\")\ntest_df.head()","5ef06946":"fig, ax = plt.subplots(2,5,figsize=(25, 10))\n\nsns.countplot(x=train_df[\"Survived\"],ax=ax[0,0]) # count of survivors and deceased\nsns.countplot(x=train_df[\"Sex\"],ax=ax[0,1]) # Count of each sex\n\nsns.barplot(x=train_df[\"Survived\"],y=train_df[\"Pclass\"],ax=ax[0,2]) # Pclass cat vs survival\nsns.barplot(x=train_df[\"Survived\"],y=train_df[\"Sex\"],ax=ax[0,3]) # Sex vs survival\nsns.boxplot(x=train_df[\"Survived\"],y=train_df[\"Age\"],ax=ax[0,4]) # Age vs survival\nsns.boxplot(x=train_df[\"Survived\"],y=train_df[\"Fare\"],ax=ax[1,0]) # Fare cost vs survival\nsns.barplot(x=train_df[\"Survived\"],y=train_df[\"Embarked\"],ax=ax[1,1]) # Embark category vs survived\nsns.barplot(x=train_df[\"Survived\"],y=train_df[\"SibSp\"],ax=ax[1,2]) # SibSp category vs survived\nsns.barplot(x=train_df[\"Survived\"],y=train_df[\"Parch\"],ax=ax[1,3]) # Parch category vs survived\n\ntrain_df[\"Cabin\"] = train_df[\"Cabin\"].str[0]\nsns.barplot(x=train_df[\"Survived\"],y=train_df[\"Cabin\"],ax=ax[1,4]) # Cabin vs survived\n\nfig.show()","329b35b5":"# Train a random forest model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics","8870e623":"# encoding classes as one hot vectors\ntrain_df = train_df.replace(np.nan, 0)\n\n\nX_data = pd.get_dummies(data=train_df[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Cabin\",\"Embarked\"]])\nX_data.drop(labels=[\"Cabin_0\",\"Embarked_0\",\"Cabin_T\"],axis=1,inplace=True)\n\ny_data = train_df[\"Survived\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_data,y_data,train_size=0.7,shuffle=True)\n\nX_data.head()","d506a19c":"threshold = 0.5 # need to optimize the threshold\n# find an optimal tree count\nx_axis = []\ny_axis = []\nfor numTrees in range(5,150,5):\n    regressor = RandomForestRegressor(n_estimators=numTrees)\n    regressor.fit(X_train, y_train)\n    y_pred_prob = regressor.predict(X_test)\n    x_axis.append(numTrees)\n    y_axis.append(metrics.f1_score(y_test, y_pred_prob > threshold))","0073ff00":"sns.scatterplot(x=x_axis,y=y_axis)\nplt.xlabel(\"Number of Trees\")\nplt.ylabel(\"f1 score\")\n\nnumTrees = x_axis[np.argmax(y_axis)] # max tree value","fc6e0ed5":"regressor = RandomForestRegressor(n_estimators=numTrees)\nregressor.fit(X_train, y_train)\n\ny_pred_prob = regressor.predict(X_test)\ny_pred = y_pred_prob > threshold # assign a classification above a given threshold\n\nconfusion_matrix = metrics.confusion_matrix(y_test,y_pred > threshold)\naccuracy_score = metrics.accuracy_score(y_test, y_pred)\nprecision_score = metrics.precision_score(y_test, y_pred)\nrecall_score = metrics.recall_score(y_test, y_pred)\nf1_score = metrics.f1_score(y_test, y_pred)","d491adfd":"sns.heatmap(confusion_matrix,cbar=False,annot=True,square=True,fmt=\"d\")\nplt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = False, labeltop=True)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\n\nprint(\"accuracy:\",accuracy_score)\nprint(\"precision:\",precision_score)\nprint(\"recall:\",recall_score)\nprint(\"f1 score:\",f1_score)","e8df428d":"# assign predictions to unlabeled data set\ntest_df = test_df.replace(np.nan, 0)\ntest_df[\"Cabin\"] = test_df[\"Cabin\"].str[0]\n\nX_data_unk = pd.get_dummies(data=test_df[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Cabin\",\"Embarked\"]])\n\ny_pred_prob = regressor.predict(X_data_unk)\ny_pred = y_pred_prob > threshold # assign a classification above a given threshold\ntest_df[\"Survived\"] = y_pred.astype(int) # append to result to unlabeled data","91cdf438":"result_df = pd.DataFrame(test_df[\"Survived\"])\nresult_df.head() # submission preview","e60e9669":"result_df.to_csv(\"\/kaggle\/working\/submission_randomforest{}.csv\".format(date.today()))","f6d184f9":"clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\nmax_depth=1).fit(X_train, y_train)\nclf.score(X_test, y_test)","a66b899f":"y_pred = clf.predict(X_data_unk)","8f9f2bff":"test_df[\"Survived\"] = y_pred.astype(int)\nresult_df.head()","a1976392":"result_df.to_csv(\"\/kaggle\/working\/submission_xgboost{}.csv\".format(date.today()))","f5b3143a":"# XGBoost","4e90e39e":"# Random Forest"}}