{"cell_type":{"702d8167":"code","cd6790fb":"code","016f6b35":"code","4f76e29a":"code","af8d631a":"code","2308b507":"code","98ffa60f":"code","9fff10b1":"code","ba9fc153":"code","36d967c7":"code","3fbaef5c":"code","da36f7dc":"code","8164c4a7":"code","13780435":"code","d5b58ce5":"code","2aeeda0e":"code","3483f2db":"code","2f139e2c":"code","d6b2b081":"code","0a9d777f":"code","1047843e":"code","f73efc37":"code","92c7e13f":"code","0ee60a66":"code","fc9af4bb":"code","e4874e3a":"code","b51c5b68":"code","0d178186":"markdown","0b0f448a":"markdown","3194c8d4":"markdown","26af377f":"markdown","f243ac02":"markdown","46ec51bf":"markdown","c10a607c":"markdown"},"source":{"702d8167":"import os\nfrom os import path\nimport shutil\nimport numpy as np\nimport pandas as pd\n\n#Import PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision import datasets, transforms, models\n\n#Import Matplotlib\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom PIL import Image","cd6790fb":"PATH = '\/kaggle\/input\/numta\/'\nos.listdir(PATH)","016f6b35":"# Hyperparameters\n\nbatch_size = 220\nnum_iters = 5000\ninput_dim = 28*28 # num_features = 784\noutput_dim = 10\nlearning_rate = 0.001\n\n# Device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","4f76e29a":"def showRawTrainingSamples(csv_filename):\n  df = pd.read_csv(PATH + csv_filename)\n  print(csv_filename)\n  print(df.columns)\n  return df","af8d631a":"a_csv = showRawTrainingSamples('training-a.csv')","2308b507":"def dropColumns(csv_file):\n  csv_file = csv_file[['filename', 'digit']]\n  print(csv_file.iloc[:5, :])   #First 5 Rows of the CSV File\n  print(\"=============================\")\n  return csv_file","98ffa60f":"data_csv = dropColumns(a_csv)","9fff10b1":"TRAIN_PATH = '\/kaggle\/input\/numta\/training-a'","ba9fc153":"class Dataset(Dataset):\n    def __init__(self, df, root, transform=None):\n        self.data = df\n        self.root = root\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        item = self.data.iloc[index]\n        \n        path = self.root + \"\/\" + item[0]\n        image = Image.open(path).convert('L')\n        label = item[1]\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        return image, label","36d967c7":"mean = [0.5,]\nstd = [0.5, ]\n\ntrain_transform = transforms.Compose([\n    transforms.Resize(28),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntest_transform = transforms.Compose([\n        transforms.Resize(28),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n])\n\ntrain_dataset  = Dataset(data_csv, TRAIN_PATH, train_transform)\ntest_dataset = Dataset(data_csv, TRAIN_PATH, test_transform)\nprint(\"Trainig Samples: \",len(train_dataset))","3fbaef5c":"# split data 10% for testing\ntest_size = 0.1\n\n# obtain training indices that will be used for validation\nnum_train = len(train_dataset)\n\n# mix data\n# index of num of train\nindices = list(range(num_train))\n\n# random the index\nnp.random.shuffle(indices)\nsplit = int(np.floor(test_size * num_train))\n\n# divied into two part\ntrain_idx, test_idx = indices[split:], indices[:split]\n\n# define the sampler\ntrain_sampler = SubsetRandomSampler(train_idx)\ntest_sampler = SubsetRandomSampler(test_idx)\n\nprint(\"Train Samples: \",len(train_idx))\nprint(\"Test Samples: \",len(test_idx))","da36f7dc":"'''\nMAKING DATASET ITERABLE\n'''\nnum_epochs = num_iters \/ (len(train_dataset) \/ batch_size)\nnum_epochs = int(num_epochs)\n\nprint('Total Epochs: '+ str(num_epochs))\n\n# prepare loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,sampler=train_sampler)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,sampler=test_sampler)\n\nprint(\"Train dataloader: {}\".format(len(train_loader)))\nprint(\"Test dataloader: {}\".format(len(test_loader))) ","8164c4a7":"# Inspecting a single image (28 pixel x 28 pixel) -->  28x28 matrix of numbers\ntrain_dataset[0]","13780435":"# One Image Size\nprint(train_dataset[0][0].numpy().shape)\n\n# First Image Label\nprint(train_dataset[0][1])","d5b58ce5":"## Displaying a Bangla Handwritten Digit Image\nshow_img = train_dataset[0][0].numpy().reshape(28, 28)\nplt.imshow(show_img, cmap='gray')","2aeeda0e":"class LogisticRegressionModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, 512)    #Input Layer\n        self.fc2 = nn.Linear(512, 256)          #Hidden Layer\n        self.fc3 = nn.Linear(256, 128)          #Hidden Layer\n        self.fc4 = nn.Linear(128, output_dim)   #Output Layer\n\n    def forward(self, x):\n        x = x.view(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        \n        probas = F.log_softmax(self.fc4(x), dim=1)\n        logits = self.fc4(x)\n        \n        return logits, probas","3483f2db":"'''\nINSTANTIATE MODEL CLASS\n'''\nmodel = LogisticRegressionModel(input_size=input_dim,num_classes=output_dim)\n\n# To enable GPU\nmodel.to(device)","2f139e2c":"# INSTANTIATE OPTIMIZER CLASS\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","d6b2b081":"'''\nTRAIN THE MODEL\n'''\niteration_loss = []\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        images = images.view(-1, 28*28).to(device)\n        labels = labels.to(device)\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output\/logits\n        logits, probas = model(images) \n\n        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\n        loss = F.cross_entropy(logits, labels)\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 100 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n               \n                images = images.view(-1, 28*28).to(device)\n\n                # Forward pass only to get logits\/output\n                logits, probas = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(probas, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n\n                # Total correct predictions\n                if torch.cuda.is_available():\n                    correct += (predicted.cpu() == labels.cpu()).sum() \n                else:\n                    correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct.item() \/ total\n            \n\n            # Print Loss\n            iteration_loss.append(loss.item())\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","0a9d777f":"# Save model with accuracy\nn = accuracy-int(accuracy)\nif n>.5:     \n    last_accuracy = int(accuracy) +1 \nelse: \n    last_accuracy = int(accuracy)","1047843e":"print (iteration_loss)\nplt.plot(iteration_loss)\nplt.ylabel('Cross Entropy Loss')\nplt.xlabel('Iteration (in every 100)')\nplt.show()","f73efc37":"os.mkdir('model') #need for first run","92c7e13f":"root_path = '.\/model\/'","0ee60a66":"save_model = True\n\nif save_model is True:\n    # Saves only parameters\n    # wights & biases\n    torch.save(model.state_dict(), root_path + 'Bangla_Digits_Handwritten_Recognizer_Logistic_Model_'+str(last_accuracy)+'%_Accuracy.pkl') \n\nos.listdir(root_path)","fc9af4bb":"load_model = True\n\nselected_accuracy = last_accuracy\n\nif load_model is True:\n    model.load_state_dict(torch.load(root_path + 'Bangla_Digits_Handwritten_Recognizer_Logistic_Model_'+str(selected_accuracy)+'%_Accuracy.pkl'))\n    print('Loaded Model With Acuracy = '+str(selected_accuracy)+' %')","e4874e3a":"for images, labels in test_loader:\n    break\n    \nfig, ax = plt.subplots(1, 5)\nfor i in range(5):\n    ax[i].imshow(images[i].view(28, 28), cmap=matplotlib.cm.binary)\nprint(plt.show())\nplt.show()","b51c5b68":"_, predictions = model.forward(images[:5].view(-1, 28*28).to(device))\npredictions = torch.argmax(predictions, dim=1)\nprint('Predicted labels', predictions.cpu().numpy())","0d178186":"In this tutorial we shall go through a bengali digit recognizer model in details. Our model is going to be based on a Deep Neural Network (DNN). There are six steps in building this digit recognizer.\n\n* Step 1 : Process the data\n* Step 2 : Design the model\n* Step 3 : Train the model\n* Step 4 : Save Model With Accuracy\n* Step 5 : Load Model With Accuracy\n* Step 6 : Test the Model","0b0f448a":"# Step 6 : Test Model","3194c8d4":"# Step 3 : Train the model.","26af377f":"# Step 2 : Design the model. ","f243ac02":"# Step 4: Save Model With Accuracy","46ec51bf":"# Step 1 : Process the data.","c10a607c":"**Follow Socials: [Social Links](https:\/\/linktr.ee\/imbodrulalam)**\n**THANK YOU!**"}}