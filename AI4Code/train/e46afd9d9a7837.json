{"cell_type":{"26c042bc":"code","600a05f7":"code","4db9809c":"code","64741d2b":"code","bbd92f5e":"code","c4ff7a48":"code","9b527bdf":"code","b1d64780":"code","6c6dbdc8":"code","5cbbc1a9":"code","1f5b8a57":"code","28c736a7":"code","e20030d3":"code","245b36d5":"code","84e93553":"code","f710c3e3":"code","29c42cfc":"code","fc78b569":"code","a138f543":"code","bd49a3ab":"code","622ce26e":"code","66f71130":"code","29aaee0c":"code","775c275a":"markdown","fd155b29":"markdown","4424b3d6":"markdown","7f70a342":"markdown","2984b149":"markdown","ff0f8a7d":"markdown","10b9706f":"markdown","ceee0d83":"markdown","086bddb0":"markdown","485ba5b9":"markdown","b9a7ecbc":"markdown","b84bd737":"markdown","9a3efc24":"markdown","003f9ce2":"markdown","5228bee9":"markdown"},"source":{"26c042bc":"class_path = \"\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/train\/train\/healthy_wheat\"\n! ls {class_path}","600a05f7":"class_path = \"\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/train\/train\/healthy_wheat\"\n! ls {class_path}","4db9809c":"import matplotlib.pyplot as plt\n\nimage = plt.imread(class_path+\"\/I9JOL9.jpg\")\nplt.imshow(image)\nplt.show()","64741d2b":"# Images are matrix\nimage.shape","bbd92f5e":"#each pixel is a color RGB\nimage_R = image.copy()\nimage_R[:,:,1]=0\nimage_R[:,:,2]=0\nplt.imshow(image_R)\nplt.show()\n\nimage_G = image.copy()\nimage_G[:,:,0]=0\nimage_G[:,:,2]=0\nplt.imshow(image_G)\nplt.show()\n\nimage_B = image.copy()\nimage_B[:,:,1]=0\nimage_B[:,:,0]=0\nplt.imshow(image_B)\nplt.show()","c4ff7a48":"# each pixel contains the intensity encoded as a number from 0 to 255\n\nimage_gray = image.mean(axis=2)\n\nplt.imshow(image_gray, cmap=\"gray\")\nplt.show()\n\n# we can get a subimage by indexing the axis\nimage_small = image_gray[::50,::50]\n\nplt.imshow(image_small, cmap=\"gray\")\nplt.show()\n\n# for each cell, we can also check the intensity value\nimage_small[:4,:4]","9b527bdf":"!ls \/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/train\/train","b1d64780":"import torchvision\nimport torch\n\nbase_dataset = torchvision.datasets.ImageFolder(\n    root='\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/train\/train',\n    transform=None)\n\nprint(base_dataset.classes)\nprint(len(base_dataset))","6c6dbdc8":"base_dataset.imgs","5cbbc1a9":"image, label = base_dataset[0]\n\nplt.imshow(image)\nplt.show()","1f5b8a57":"import torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n    [transforms.Resize((224, 224)),\n     transforms.RandomRotation(20),\n     transforms.RandomVerticalFlip()])#,\n     #transforms.ToTensor(),\n     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])","28c736a7":"for i in range(5):\n    plt.imshow(transform(image))\n    plt.show()","e20030d3":"# transforms\ntransform = transforms.Compose(\n    [transforms.Resize((224, 224)),#(224, 224)),\n     transforms.RandomRotation(20),\n     transforms.RandomVerticalFlip(),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# dataset\nbase_dataset = torchvision.datasets.ImageFolder(\n    root='\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/train\/train',\n    transform=transform)\n\n# dataloader\nbase_loader = torch.utils.data.DataLoader(base_dataset, batch_size=16,shuffle=True, num_workers=4)","245b36d5":"# where does the model do all the operations?\nimport torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","84e93553":"# creating a model based on a preexisting architecture\nfrom torchvision import models\nimport torch.nn as nn\n\nmodel = models.resnet18(pretrained=False)\n\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(base_dataset.classes))\n\nmodel.to(device)\nmodel","f710c3e3":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","29c42cfc":"# train\nfor epoch in range(1):\n    for i, data in enumerate(base_loader, 0):\n        # get the inputs\n        inputs, labels = data\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        outputs = model(inputs.to(device))\n        loss = criterion(outputs.to(device), labels.to(device))\n        loss.backward()\n        optimizer.step()","fc78b569":"import time\n\n# create arrays to save metrics\nt_start = time.time()\nloss_arr=[]\nacc_arr=[]\nfor epoch in range(1):  # loop over the dataset multiple times\n    \n    #initialize epoch metrics\n    t_epoch = time.time()\n    running_loss = 0.0\n    running_acc = 0.0\n    running_n = 0\n    \n    for i, data in enumerate(base_loader, 0):\n        # get the inputs\n        inputs, labels = data\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs.to(device))\n        loss = criterion(outputs.to(device), labels.to(device))\n        loss.backward()\n        optimizer.step()\n        \n        # compute minibatch metrics\n        running_acc += (torch.argmax(outputs, dim=1).to(device) == labels.to(device)).float().sum()\n        running_n += len(labels)\n        running_loss += loss.item()\n        \n    # compute epoch metrics\n    running_acc = running_acc\/running_n\n    running_loss = running_loss\/running_n\n    loss_arr.append(running_loss)\n    acc_arr.append(running_acc)\n    \n    # print epoch metrics\n    print('epoch: {} loss: {} acc: {} time epoch: {} time total: {}'.format(epoch + 1, running_loss, running_acc, time.time()-t_epoch, time.time()-t_start))\n\nprint('Finished Training in {} seconds'.format(time.time()-t_start))","a138f543":"view_dataset = torchvision.datasets.ImageFolder(\n    root='\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/train\/train',\n    transform=None)\n\ntransform_view = transforms.Compose(\n    [transforms.Resize((224, 224)),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nimport numpy as np\nindexes = np.random.choice(len(view_dataset), 5)\nfor i in indexes:\n    x,y = view_dataset[i]\n    x_view = transform_view(x)\n    x_tr = transform(x)\n    \n    y_out = model(x_tr.to(device).unsqueeze(0))\n    y_prob = nn.functional.softmax(y_out, dim=1)\n    y_pred = torch.argmax(y_prob, dim=1)\n    \n    fig, axs = plt.subplots(1,2)\n    axs[0].imshow(x)\n    axs[0].set_title(\"[{}] label {}\".format(i,y))\n    \n    axs[1].imshow(transforms.ToPILImage()(x_tr))\n    axs[1].set_title(\"pred {}\".format(y_pred[0]))\n    plt.show()","bd49a3ab":"len(base_dataset)","622ce26e":"# split our dataset\ntrain_dataset, val_dataset = torch.utils.data.random_split(base_dataset, [450, 112])\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=4)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16,shuffle=True, num_workers=4)","66f71130":"# create arrays to save metrics\nt_start = time.time()\nloss_arr=[]\nacc_arr=[]\nval_acc_arr=[]\nfor epoch in range(50):  # loop over the dataset multiple times\n    \n    #initialize epoch metrics\n    t_epoch = time.time()\n    running_loss = 0.0\n    running_acc = 0.0\n    running_n = 0\n    running_val_acc = 0.0\n    running_val_n = 0\n    \n    # train\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs\n        inputs, labels = data\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs.to(device))\n        loss = criterion(outputs.to(device), labels.to(device))\n        loss.backward()\n        optimizer.step()\n        \n        # compute minibatch metrics\n        running_acc += (torch.argmax(outputs, dim=1).to(device) == labels.to(device)).float().sum()\n        running_n += len(labels)\n        running_loss += loss.item()\n    \n    # validate\/evaluate\n    for i, data in enumerate(val_loader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # forward\n        outputs = model(inputs.to(device))\n        loss = criterion(outputs.to(device), labels.to(device))\n        \n        # compute minibatch metrics\n        running_val_acc += (torch.argmax(outputs, dim=1).to(device) == labels.to(device)).float().sum()\n        running_val_n += len(labels)\n        \n    # compute epoch metrics\n    running_acc = running_acc\/running_n\n    running_val_acc = running_val_acc\/running_val_n\n    running_loss = running_loss\/running_n\n    loss_arr.append(running_loss)\n    acc_arr.append(running_acc)\n    val_acc_arr.append(running_val_acc)\n    \n    # print epoch metrics\n    print('epoch: {} loss: {} acc: {} valacc: {} time epoch: {} time total: {}'.format(epoch + 1, running_loss, running_acc, running_val_acc, time.time()-t_epoch, time.time()-t_start))\n\nprint('Finished Training in {} seconds'.format(time.time()-t_start))","29aaee0c":"plt.plot(val_acc_arr, label=\"val_acc\")\nplt.plot(acc_arr, label=\"acc\")\nplt.legend()\nplt.show()\nplt.plot(loss_arr, label=\"loss\")\nplt.legend()\nplt.show()","775c275a":"# Introduction\n\n## What is image classification?\n![Image classification](https:\/\/miro.medium.com\/max\/3840\/1*oB3S5yHHhvougJkPXuc8og.gif)\n\n## What are notebooks?\n![Notebooks](https:\/\/threathunterplaybook.com\/_images\/JUPYTER_ARCHITECTURE.png)\n\n## How are we going to tackle this example?\n![CRISP-DM](https:\/\/miro.medium.com\/max\/494\/1*VH9RlYPSjL6YOtBtHxunqQ.png)","fd155b29":"## How to read image files?\n\nIn python, we can find multiple libraries which will allow us to read and display images, some of them are:\n- PIL\n- opencv\n- matplotlib","4424b3d6":"## Transforms\n\n* Data augmentation is a commonly used technique in computer vision problems which helps to deal with small datasets and helps algorithms converge more smoothly.\n\n![data augmentation](https:\/\/www.frontiersin.org\/files\/Articles\/469305\/fncom-13-00083-HTML\/image_m\/fncom-13-00083-g003.jpg)","7f70a342":"# What to do now?\n\n![CRISP-DM](https:\/\/www.mdpi.com\/applsci\/applsci-09-02407\/article_deploy\/html\/images\/applsci-09-02407-g001.png)\n\n![CRISP-DM](https:\/\/miro.medium.com\/max\/494\/1*VH9RlYPSjL6YOtBtHxunqQ.png)\n","2984b149":"-------------------------\n-------------------------\n![CRISP-DM](https:\/\/www.mdpi.com\/applsci\/applsci-09-02407\/article_deploy\/html\/images\/applsci-09-02407-g001.png)\n# Modeling\n\n![modeling](https:\/\/drive.google.com\/uc?id=1jnHXWqQdBAaJYmIlCNyRINX8KJgUghnM)\n\n\n![resnet18](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcSVhix4X-U2-fuTuEdTIWpkhFqTiCAoGROmxw&usqp=CAU)","ff0f8a7d":"* RGB Images are basically a 3D matrix. or 3 2D matrix, one for each color intencity (Red Green and Blue).\n![RGB images](https:\/\/webstyleguide.com\/wsg2\/graphics\/graphics\/7.07.gif)","10b9706f":"### Evaluate","ceee0d83":"## Dataloaders\n\n\n![dataloaders](https:\/\/miro.medium.com\/max\/3480\/1*3vAYjhGh_EopD0cRdxrbOQ.png)","086bddb0":"## Datasets","485ba5b9":"-------------------------\n-------------------------\n![CRISP-DM](https:\/\/www.mdpi.com\/applsci\/applsci-09-02407\/article_deploy\/html\/images\/applsci-09-02407-g001.png)\n# Data Preparation\n\n![data pipeline](https:\/\/i2.wp.com\/mlexplained.com\/wp-content\/uploads\/2018\/02\/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-02-07-10.32.59.png?fit=1998%2C938&ssl=1)\n","b9a7ecbc":"### Train test split?\n\n* current size of the dataset: 562","b84bd737":"-------------------------\n-------------------------\n![CRISP-DM](https:\/\/www.mdpi.com\/applsci\/applsci-09-02407\/article_deploy\/html\/images\/applsci-09-02407-g001.png)\n# Data understanding\n\n## Where is our data?\n\n![Notebooks](https:\/\/threathunterplaybook.com\/_images\/JUPYTER_ARCHITECTURE.png)\n\n* Our data is currently in a server, managed by Kaggle and assigned to us.\n\n```\n\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\n|-- \/test\/test\n|-- \/train\/train\n    |-- \/healthy_wheat\n    |-- \/leaf_rust\n    |-- \/stem_rust\n\n```","9a3efc24":"## What is an image?\n\n* Images are 2D matrix composed of pixels.\n* Pixels can have one or three values defining their intensity\n\n![Images](https:\/\/mozanunal.com\/images\/pixel.png)","003f9ce2":"-------------------------\n-------------------------\n# Business Understanding\n\n## Objective\n\nThe imagery data comes from a variety of sources. The bulk of the data was collected in-field by CIMMYT and CIMMYT partners in Ethiopia and Tanzania. The remainder of the data is sourced from public images found on Google Images.\n\nThe aim of this challenge is to build a machine learning model to accurately classify the wheat in the images as: healthy, stem rust, or leaf rust.\n\nSome images may contain both stem and leaf rust, there is always one type of rust that is more dominant than the other, i.e. you will not find images where both appear equally. The goal is to classify the image according to the type of wheat rust that appears most prominently in the image. The values for each classification can be between 0 and 1, inclusive, and should represent the probability that the wheat in the image belongs in each category.\n\n\n\n## Strategy + problem simplification\n\n![image classification](https:\/\/miro.medium.com\/max\/1128\/1*ATIx1SmkEH0FaL_5fMvX2w.jpeg)\n\n* Collect images (DONE)\n* Label images (DONE)\n* Define as an Image classification problem (!)\n","5228bee9":"### Training\n\n![gradient descent](https:\/\/thumbs.gfycat.com\/AngryInconsequentialDiplodocus-size_restricted.gif)"}}