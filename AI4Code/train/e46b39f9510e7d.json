{"cell_type":{"b3616310":"code","562328cf":"code","13c8a6a7":"code","8203fcd9":"code","32e6d970":"code","3c4bb064":"code","90a95f40":"code","4ebf383d":"code","b770d4a8":"code","6c1d7739":"code","5b0bb95b":"code","82ccbeaf":"code","ca2b7f19":"code","a579789e":"code","9d0b9d6c":"code","284b37b1":"code","689fb8dc":"code","f9bcb132":"code","46c0aae1":"code","c2d97123":"code","7144e872":"code","81513310":"code","0ad2b964":"code","c67eb3d0":"code","96f1c95f":"markdown","5736a516":"markdown","263a140c":"markdown","f8c84598":"markdown","c08fa44c":"markdown","bd203e1e":"markdown","7f98ca96":"markdown","cdfde463":"markdown"},"source":{"b3616310":"import tensorflow as tf\nimport matplotlib.pyplot as plt\ntf.__version__","562328cf":"base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n#false- dense layers is not included-only Con and pooling layers are included\n#include imagenet weights also","13c8a6a7":"base_model.summary()","8203fcd9":"len(base_model.layers)","32e6d970":"#names = ['mixed3', 'mixed5', 'mixed8', 'mixed9'] #names of some mixed layers in the architecture\nnames = ['mixed3', 'mixed5']\nlayers = [base_model.get_layer(name).output for name in names]\nlayers","3c4bb064":"deep_dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)#two output layers mixed3 mixed5\n# get outputs from the hidden layers to achieve deep dream images","90a95f40":"image = tf.keras.preprocessing.image.load_img('..\/input\/deepdream\/StaryNight.jpg',\n                                              target_size=(225, 375)) #with respect to inception layer\nplt.figure(figsize=(15,15))\nplt.axis('off')\nplt.imshow(image);","4ebf383d":"image.mode, len(image.mode), image.size","b770d4a8":"image = tf.keras.preprocessing.image.img_to_array(image) #convert image to np array for compatibility\ntype(image), image.shape","6c1d7739":"image = tf.keras.applications.inception_v3.preprocess_input(image)   #all preprocessing takes place to be compatible with inception network","5b0bb95b":"image.shape","82ccbeaf":"image_batch = tf.expand_dims(image, axis=0)\nimage_batch.shape","ca2b7f19":"activations = deep_dream_model.predict(image_batch)\nlen(activations)","a579789e":"#activations[0]\nactivations[0].shape, activations[1].shape #mixed3, mixed5","9d0b9d6c":"def calculate_loss(image, network):\n    image_batch = tf.expand_dims(image, axis=0) #convert one image to batch format\n    #activations = network.predict(image_batch)\n    activations = network(image_batch)\n\n    losses = []\n    for act in activations:\n        loss = tf.math.reduce_mean(act) #get average\n        losses.append(loss)\n\n    return tf.reduce_sum(losses)","284b37b1":"loss = calculate_loss(image, deep_dream_model)\nloss","689fb8dc":"@tf.function #used to define as a global variable\n\ndef deep_dream(network, image, learning_rate):\n    with tf.GradientTape() as tape: #GradientTape - class in tf to access gradient\n        tape.watch(image) #make changes in image\n        loss = calculate_loss(image, network)\n    \n    gradients = tape.gradient(loss, image) #derivative\n    gradients \/= tf.math.reduce_std(gradients)  #normalizing\n    \n    image = image + gradients*learning_rate  #making changes to original image repeatedly till the loss is maximum(gradient ascent)\n    \n    image = tf.clip_by_value(image, -1, 1) #scale values in the range [-1, 1]\n    \n    return loss, image","f9bcb132":"#reverse the preprocessing steps from inception layers to get image after change from gradient ascent\n\ndef inverse_transform(image):\n    image = 255 * (image + 1.0) \/ 2.0 #scale from [-1, 1] to original formats\n    return tf.cast(image, tf.uint8) #convert pixels to integer values","46c0aae1":"def run_deep_dream(network, image, epochs, learning_rate):\n    for epoch in range(epochs):\n        loss, image = deep_dream(network, image, learning_rate)\n        \n        if (epoch % 200==0):\n            plt.figure(figsize=(15,15))\n            plt.imshow(inverse_transform(image))\n            plt.show()\n            print('Epoch {}, loss {}'.format(epoch, loss))","c2d97123":"image.shape, type(image)","7144e872":"run_deep_dream(network=deep_dream_model,\n               image=image,\n               epochs=8000,\n               learning_rate=0.0001\n              )","81513310":"image = tf.keras.preprocessing.image.load_img('..\/input\/deepdream\/sky.jpeg',\n                                              target_size = (225, 375))\nplt.figure(figsize=(15,15))\nplt.axis('off')\nplt.imshow(image);","0ad2b964":"image = tf.keras.preprocessing.image.img_to_array(image)\nimage = tf.keras.applications.inception_v3.preprocess_input(image)","c67eb3d0":"run_deep_dream(network = deep_dream_model, image = image, epochs = 8000, learning_rate = 0.0001)","96f1c95f":"# Gradient Ascent\n## Custom function (Opposite to gradient descent)","5736a516":"# Generating Images\n## Image 1","263a140c":"# Getting the Activations","f8c84598":"## Image 2","c08fa44c":"# Load Pre-Built CNN","bd203e1e":"# Import Libraries","7f98ca96":"# Load and Preprocess the Image","cdfde463":"# Calculating the Loss"}}