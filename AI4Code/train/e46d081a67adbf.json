{"cell_type":{"7d83759d":"code","8f05c1dd":"code","5b594538":"code","f5c1fe9d":"code","5bd13fba":"code","d433c0e0":"code","4cc859e7":"code","3cefb117":"code","1f9f84fa":"code","ae8414cb":"code","418f78c7":"code","2d1f5097":"code","e6eef815":"code","03e0e30f":"code","1a1b0b3d":"code","4e51cc06":"code","71de8ccc":"code","ccaf5cd3":"code","98594809":"code","1397b788":"code","cc562a19":"code","6fcab3a3":"markdown","c7e8c785":"markdown","7f0188dd":"markdown","493017a1":"markdown","8f218d2c":"markdown","f8d47e0e":"markdown"},"source":{"7d83759d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f05c1dd":"import tensorflow as tf\nimport sklearn\nfrom tqdm import tqdm\ndf=pd.read_csv(\"\/kaggle\/input\/immunization-data-in-india\/india_immunization_data2.csv\")\ndf.sample()","5b594538":"!pip install pywaffle","f5c1fe9d":"from pywaffle import Waffle\nimport random\n\nsentiment = df[\"Sentiment\"].value_counts()\n\n\nplt.figure(\n    FigureClass=Waffle,\n    rows=5,\n    columns=10,\n    values=sentiment,\n    title={'label': 'Sentiment Distribution', 'loc': 'left'},\n    labels=[\"{}({})\".format(a, b) for a, b in zip(sentiment.index, sentiment) ],\n    # Set the position of the legend\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)},\n    dpi=100\n)\nplt.show()","5bd13fba":"# Loading the BERT Classifier and Tokenizer along with Input module\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import InputExample, InputFeatures\n\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","d433c0e0":"# changing positive and negative into numeric values\n\ndef cat2num(value):\n    if value=='positive': \n        return 1\n    else: \n        return 0\n    \ndf['Sentiment']  =  df['Sentiment'].apply(cat2num)\ntrain = df[:45000]\ntest = df[45000:]","4cc859e7":"# But first see BERT tokenizer exmaples and other required stuff!\n\nexample='In this Kaggle notebook, I will do sentiment analysis using BERT with Huggingface'\ntokens=tokenizer.tokenize(example)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(tokens)\nprint(token_ids)","3cefb117":"df = df.rename(columns={'Tweet-text':'Tweet'})","1f9f84fa":"def convert_data_to_examples(train, test, Username, Sentiment): \n    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[Username], \n                                                          label = x[Sentiment]), axis = 1)\n\n    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[Username], \n                                                          label = x[Sentiment]), axis = 1,)\n  \n    return train_InputExamples, validation_InputExamples\n\ntrain_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'Username',  'Sentiment')","ae8414cb":"def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n    features = [] # -> will hold InputFeatures to be converted later\n\n    for e in tqdm(examples):\n        input_dict = tokenizer.encode_plus(\n            e.text_a,\n            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n            max_length=max_length,    # truncates if len(s) > max_length\n            return_token_type_ids=True,\n            return_attention_mask=True,\n            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n            truncation=True\n        )\n\n        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n\n    def gen():\n        for f in features:\n            yield (\n                {\n                    \"input_ids\": f.input_ids,\n                    \"attention_mask\": f.attention_mask,\n                    \"token_type_ids\": f.token_type_ids,\n                },\n                f.label,\n            )\n\n    return tf.data.Dataset.from_generator(\n        gen,\n        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n        (\n            {\n                \"input_ids\": tf.TensorShape([None]),\n                \"attention_mask\": tf.TensorShape([None]),\n                \"token_type_ids\": tf.TensorShape([None]),\n            },\n            tf.TensorShape([]),\n        ),\n    )\n\n\nDATA_COLUMN = 'Username'\nLABEL_COLUMN = 'Sentiment'","418f78c7":"df.isnull().sum()","2d1f5097":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","e6eef815":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","03e0e30f":"# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>0 and df[feature].dtypes=='O']\nprint(categorical_nan)","1a1b0b3d":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    df[feature] = df[feature].fillna('None')","4e51cc06":"df[categorical_nan].isna().sum()","71de8ccc":"train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\ntrain_data = train_data.shuffle(100).batch(32).repeat(2)","ccaf5cd3":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n\nmodel.fit(train_data, epochs=2, validation_data=validation_data)","98594809":"pred_sentences = ['Track immunization schedule of child using NHP', 'Let no child be left without #vaccines.']","1397b788":"#tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n#tf_outputs = model(tf_batch)                                  \n#tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\nlabels = ['Neutral']\n#label = tf.argmax(tf_predictions, axis=1)\n#label = label.numpy()\n#for i in range(len(pred_sentences)):\n    # print(pred_sentences[i], \": \", labels[label[i]])\nprint(pred_sentences, \": \", labels)    ","cc562a19":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n     \n    \ndhtml('From failure to failure without losing the enthusiasm. - Winston Churchill. Yeah, because it is not You that have failed.' )","6fcab3a3":"#Above I got an error, so another error below since `train_data` was not defined due to the previous error.","c7e8c785":"#BERT stands for Bidirectional Encoder Representations from Transformers and it is a state-of-the-art machine learning \n\n#model used for NLP tasks.","7f0188dd":"![](https:\/\/www.kdnuggets.com\/images\/sentiment-fig-1-689.jpg)KDnuggets.com","493017a1":"#Script by Satyam Prasad Tiwari https:\/\/www.kaggle.com\/satyampd\/imdb-sentiment-analysis-using-bert-w-huggingface\/notebook","8f218d2c":"#I got ValueError: Input 0.8679001331 is not valid. Should be a string, a list\/tuple of strings or a list\/tuple of \n\n#integers. In Input 30(?) so I changed Confidence for Likes. Reading that it doesn't sound so nice. Who changes Confidence \n\n#for Likes? Analyze that Sentiment: It's sarcasm.  Both didn't work. I tried to rename Tweet-text. Then I tried Username, \n\n#which returned \"Input nan is not valid.\" I removed Nan. Not working at all. Guess how is my sentiment now. ","f8d47e0e":"#Below, I changed 'Negative','Positive' to Neutral."}}