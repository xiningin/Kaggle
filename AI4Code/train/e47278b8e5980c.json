{"cell_type":{"319e73de":"code","49d6c603":"code","8ecfbaaf":"code","6e2fcaf3":"code","9776a0c9":"code","0c7ed661":"code","fc266c94":"code","d4d5eea5":"code","7b6e064c":"code","3f8cfe57":"code","f562b5ed":"markdown","4b074799":"markdown","5b30c159":"markdown","12f34a68":"markdown"},"source":{"319e73de":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom tqdm import tqdm_notebook\nimport datetime\nimport time\nimport random\nfrom joblib import Parallel, delayed\n\n\nimport lightgbm as lgb\nfrom tensorflow import keras\nfrom gplearn.genetic import SymbolicRegressor\nfrom catboost import Pool, CatBoostRegressor\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\nfrom sklearn.feature_selection import RFECV, SelectFromModel\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor","49d6c603":"train_X_0 = pd.read_csv(\"..\/input\/lanl-master-s-features-creating-0\/train_X_features_865.csv\")\ntrain_X_1 = pd.read_csv(\"..\/input\/lanl-master-s-features-creating-1\/train_X_features_865.csv\")\ny_0 = pd.read_csv(\"..\/input\/lanl-master-s-features-creating-0\/train_y.csv\", index_col=False,  header=None)\ny_1 = pd.read_csv(\"..\/input\/lanl-master-s-features-creating-1\/train_y.csv\", index_col=False,  header=None)","8ecfbaaf":"train_X = pd.concat([train_X_0, train_X_1], axis=0)\ntrain_X = train_X.reset_index(drop=True)\nprint(train_X.shape)\ntrain_X.head()","6e2fcaf3":"y = pd.concat([y_0, y_1], axis=0)\ny = y.reset_index(drop=True)\ny[0].shape","9776a0c9":"train_y = pd.Series(y[0].values)","0c7ed661":"test_X = pd.read_csv(\"..\/input\/lanl-master-s-features-creating-0\/test_X_features_10.csv\")\n# del X[\"seg_id\"], test_X[\"seg_id\"]","fc266c94":"scaler = StandardScaler()\ntrain_columns = train_X.columns\n\ntrain_X[train_columns] = scaler.fit_transform(train_X[train_columns])\ntest_X[train_columns] = scaler.transform(test_X[train_columns])","d4d5eea5":"train_columns = train_X.columns\nn_fold = 5","7b6e064c":"%%time\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train_X))\ntrain_score = []\nfold_idxs = []\n# if PREDICTION: \npredictions = np.zeros(len(test_X))\n\nfeature_importance_df = pd.DataFrame()\n#run model\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X,train_y.values)):\n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    fold_idxs.append(val_idx)\n\n    X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n    y_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n\n    model = CatBoostRegressor(n_estimators=30000, verbose=-1, objective=\"MAE\", loss_function=\"MAE\", boosting_type=\"Ordered\", task_type=\"GPU\")\n    model.fit(X_tr, \n              y_tr, \n              eval_set=[(X_val, y_val)], \n#               eval_metric='mae',\n              verbose=2500, \n              early_stopping_rounds=600)\n    oof[val_idx] = model.predict(X_val)\n\n    #feature importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    #predictions\n#     if PREDICTION:\n\n    predictions += model.predict(test_X[train_columns]) \/ folds.n_splits\n    train_score.append(model.best_score_['learn'][\"MAE\"])\n\ncv_score = mean_absolute_error(train_y, oof)\nprint(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")\n\ntoday = str(datetime.date.today())\nsubmission = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/sample_submission.csv')\n\nsubmission[\"time_to_failure\"] = predictions\nsubmission.to_csv(f'CatBoost_MAE_{today}_test_{cv_score:.3f}_train_{np.mean(train_score):.3f}.csv', index=False)\nsubmission.head()","3f8cfe57":"%%time\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train_X))\ntrain_score = []\nfold_idxs = []\n# if PREDICTION: \npredictions = np.zeros(len(test_X))\n\nfeature_importance_df = pd.DataFrame()\n#run model\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X,train_y.values)):\n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    fold_idxs.append(val_idx)\n\n    X_tr, X_val = train_X[train_columns].iloc[trn_idx], train_X[train_columns].iloc[val_idx]\n    y_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n\n    model = CatBoostRegressor(n_estimators=30000, verbose=-1, objective=\"RMSE\", loss_function=\"RMSE\", boosting_type=\"Ordered\", task_type=\"GPU\")\n    model.fit(X_tr, \n              y_tr, \n              eval_set=[(X_val, y_val)], \n#               eval_metric='mae',\n              verbose=2500, \n              early_stopping_rounds=600)\n    oof[val_idx] = model.predict(X_val)\n\n    #feature importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    #predictions\n#     if PREDICTION:\n\n    predictions += model.predict(test_X[train_columns]) \/ folds.n_splits\n    train_score.append(model.best_score_['learn'][\"RMSE\"])\n\ncv_score = mean_absolute_error(train_y, oof)\nprint(f\"After {n_fold} test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")\n\ntoday = str(datetime.date.today())\nsubmission = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/sample_submission.csv')\n\nsubmission[\"time_to_failure\"] = predictions\nsubmission.to_csv(f'CatBoost_RMSE_{today}_test_{cv_score:.3f}_train_{np.mean(train_score):.3f}.csv', index=False)\nsubmission.head()","f562b5ed":"MAE","4b074799":"# CatBoost","5b30c159":"* Below you may notice the effect of using MAE\/RMSE as objective functions of the resulting MAE for the competition\n* snippet from https:\/\/www.kaggle.com\/tocha4\/lanl-master-s-approach","12f34a68":"RMSE"}}