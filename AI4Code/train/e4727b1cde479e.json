{"cell_type":{"eadcd9b3":"code","d2cc6c4b":"code","086318df":"code","36084f4e":"code","bbd049f4":"code","d29834ac":"code","3b03e8ec":"code","8dc1142a":"code","74c1ac29":"code","51e95357":"code","94bdb658":"code","ef68723c":"code","1ae0d5bc":"code","542f4156":"code","48571304":"code","85acbc64":"code","f488d21c":"code","22c656f4":"code","dd275d61":"code","74754e2c":"code","51018b9c":"code","b75de211":"code","891cdf55":"code","5e64027e":"code","f8508840":"code","f8486535":"code","ba6270b7":"code","a2562e3e":"code","8022a535":"code","4652caec":"code","9716acde":"code","0f546d4f":"code","6d3ed016":"code","7676ec56":"code","39cf89fe":"code","53986515":"code","9c23aee7":"code","25d3817f":"code","3c8d7c9b":"code","c59be10e":"code","fc4135b3":"code","bfda9ba6":"code","fdd7d027":"code","47903f1d":"code","f0eb3451":"code","d8136512":"code","596aca78":"code","3f496c9e":"code","c5ae6115":"code","d0a7c098":"code","02d01b10":"code","5218306d":"code","d22c088d":"code","389a8a00":"code","1866c985":"code","18bd82d0":"code","3184e1ca":"code","f8ddd661":"code","84118ffb":"code","c9b3af13":"code","7b2cc9ac":"markdown","4597a102":"markdown","5dd8d196":"markdown","d6c29bec":"markdown","b5f7d030":"markdown","671b37d8":"markdown","2a75e0ef":"markdown","7b34e98f":"markdown","bcec07e5":"markdown","498257ee":"markdown","e0deb714":"markdown","07b9b9c1":"markdown","5cc53ab5":"markdown","489891cf":"markdown","ce1df643":"markdown","6fecfbb7":"markdown","8c408d63":"markdown","86ad040c":"markdown","0c76e86b":"markdown","7d934d6d":"markdown","5ce33024":"markdown","0bc8ca7b":"markdown","d5221bd5":"markdown","a71d4a81":"markdown","7197f7b8":"markdown","13698adb":"markdown","36decc69":"markdown","0d0d773e":"markdown","2d9c0484":"markdown","330aeaf5":"markdown","fd06ce47":"markdown","19d8d3c1":"markdown","1a7a9303":"markdown","64246163":"markdown","11fa89da":"markdown","107574bb":"markdown","b5402c08":"markdown","d3abebc5":"markdown","374fd214":"markdown","44b5add3":"markdown","57836455":"markdown","ebd61f11":"markdown","fb23da86":"markdown","9376202d":"markdown","1bfd4408":"markdown","1742a15c":"markdown","fb2e801a":"markdown","acd72792":"markdown","89effb98":"markdown","ef27b15d":"markdown","3e475d5a":"markdown","afc8e12c":"markdown","27b3c381":"markdown","f46be610":"markdown","922c9007":"markdown"},"source":{"eadcd9b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d2cc6c4b":"import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.naive_bayes import GaussianNB","086318df":"import seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score,confusion_matrix, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score","36084f4e":"%matplotlib inline","bbd049f4":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","d29834ac":"df.describe()","3b03e8ec":"print('The total number of transactions in dataset : ', len(df))\nprint('The total number of columns : ',len(list(df)))\nprint('The dimension of data : ', df.shape)\nprint('The target column is : ', list(df)[30])\nprint('Total number of unique values in target column is : ', len(df['Class'].unique()))\nprint('The unique values in Class column : ', df.Class.unique())","8dc1142a":"print('Total number of zeroes (non-fraud transactions) : ', df['Class'].value_counts()[0])\nprint('Total number of ones (fraud transactions) : ', df['Class'].value_counts()[1])\nprint('Percentage of non-fraud transactions : ', 100*(df['Class'].value_counts()[0])\/ len(df))\nprint('Percentage of fraud transactions : ', 100*(df['Class'].value_counts()[1])\/ len(df))\n","74c1ac29":"sns.countplot('Class', data=df, palette=None)\nplt.title(\"Target Column frequency distribution\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","51e95357":"df.plot(x='Time', y='Amount', style='-')\nplt.title(\"Transaction Amount vs Time\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Amount\")","94bdb658":"df = df.drop(['Time'],axis=1)","ef68723c":"df['Normalized_Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\ndf = df.drop(['Amount'],axis=1)\ndf.head()","1ae0d5bc":"Normalized_Amount = df['Normalized_Amount']\ndf=df.drop(['Normalized_Amount'],axis=1)\ndf.insert(0, 'Normalized_Amount', Normalized_Amount)\ndf.head()","542f4156":"flag = df.isnull().sum().any()\n\nif (flag == True):\n    df.isnull().sum()\n    print(\"There are null values in the dataframe\")\n    \nelse :\n    print(\"There are no null values and dataframe is clear for further analysis\")","48571304":"# ignore all future warnings\nfrom warnings import simplefilter\n\nsimplefilter(action='ignore', category=FutureWarning)","85acbc64":"X = df.drop(['Class'], axis = 1) \ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 2)\nclf = LogisticRegression().fit(X_train, y_train)\nprint('Accuracy of Logistic regression classifier on imbalanced training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on imbalanced test set: {:.2f}'.format(clf.score(X_test, y_test)))\n","f488d21c":"lr = LogisticRegression().fit(X_train, y_train)\nlr_predicted = lr.predict(X_test)\nconfusion = confusion_matrix(y_test, lr_predicted)\n\nprint('Logistic regression classifier (default settings)\\n', confusion)","22c656f4":"print(\"Logistic Regression Evaluation Parameters with imbalanced data\")\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, lr_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, lr_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, lr_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, lr_predicted)))","dd275d61":"scaler = MinMaxScaler()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nclf = MLPClassifier(hidden_layer_sizes = [100, 100], alpha = 5.0,\n                   random_state = 0, solver='lbfgs').fit(X_train_scaled, y_train)\n\nprint('Accuracy of NN classifier on training set: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\nprint('Accuracy of NN classifier on test set: {:.2f}'.format(clf.score(X_test_scaled, y_test)))","74754e2c":"nbclf = GaussianNB().fit(X_train, y_train)\nprint('Accuracy of GaussianNB classifier on training set: {:.2f}'.format(nbclf.score(X_train, y_train)))\nprint('Accuracy of GaussianNB classifier on test set: {:.2f}'.format(nbclf.score(X_test, y_test)))","51018b9c":"#Recalling the amount of unique values in 'Class' column\nprint('Total number of zeroes (non-fraud transactions) : ', df['Class'].value_counts()[0])\nprint('Total number of ones (fraud transactions) : ', df['Class'].value_counts()[1])","b75de211":"non_fraud_transactions_df = df[df['Class'] == 0]\nfraud_transactions_df = df[df['Class']==1]","891cdf55":"print('The dimension of fraud transactions dataframe is : ', fraud_transactions_df.shape)\nprint('The dimension of non-fraud transactions dataframe is : ', non_fraud_transactions_df.shape)","5e64027e":"sample_492_non_fraud_transactions_df = non_fraud_transactions_df.sample(n=492)\nprint('The dimension of sample non-fraud transactions df is : ', sample_492_non_fraud_transactions_df.shape)","f8508840":"method_1_df = pd.concat([sample_492_non_fraud_transactions_df, fraud_transactions_df])\nmethod_1_df = method_1_df.sample(frac=1).reset_index(drop=True)\n","f8486535":"method_1_df.head()","ba6270b7":"print('The dimension of dataframe for Method 1 is : ',method_1_df.shape )","a2562e3e":"sns.countplot('Class', data=method_1_df, palette=None)\nplt.title(\"Method 1 Frequency distribution plot\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","8022a535":"print('Percentage of non-fraud transactions in method_1_df : ',  100*(method_1_df['Class'].value_counts()[0])\/ len(method_1_df))\nprint('Percentage of fraud transactions in method_1_df : ',  100*(method_1_df['Class'].value_counts()[1])\/ len(method_1_df))","4652caec":"X = method_1_df.drop(['Class'], axis = 1) \ny = method_1_df['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)","9716acde":"print(\"Number transactions in training dataset for Method 1: \", len(X_train))\nprint(\"Number transactions in testing dataset  for Method 1: \", len(X_test))\nprint(\"Total number of transactions  for Method 1 : \", len(X_train)+len(X_test))","0f546d4f":"clf = LogisticRegression().fit(X_train, y_train)\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","6d3ed016":"lr_predicted = clf.predict(X_test)\nconfusion = confusion_matrix(y_test, lr_predicted)\nprint('Logistic regression classifier (default settings)\\n', confusion)","7676ec56":"print('Accuracy: {:.2f}'.format(accuracy_score(y_test, lr_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, lr_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, lr_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, lr_predicted)))","39cf89fe":"logistic_parameters = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid = GridSearchCV(LogisticRegression(), logistic_parameters)\ngrid.fit(X_train, y_train)\nbest_log_reg = grid.best_estimator_","53986515":"logistic_score = cross_val_score(best_log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', logistic_score.mean())","9c23aee7":"lr_predicted = grid.predict(X_test)\nconfusion = confusion_matrix(y_test, lr_predicted)\nprint('Logistic regression classifier with Cross-validation (default settings)\\n', confusion)","25d3817f":"print('Accuracy: {:.2f}'.format(accuracy_score(y_test, lr_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, lr_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, lr_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, lr_predicted)))","3c8d7c9b":"nbclf = GaussianNB().fit(X_train, y_train)\nprint('Accuracy of GaussianNB classifier on training set: {:.2f}'.format(nbclf.score(X_train, y_train)))\nprint('Accuracy of GaussianNB classifier on test set: {:.2f}'.format(nbclf.score(X_test, y_test)))","c59be10e":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nclf = MLPClassifier(hidden_layer_sizes = [100, 100], alpha = 5.0,\n                   random_state = 0, solver='lbfgs').fit(X_train_scaled, y_train)\n\nprint('Accuracy of NN classifier on training set: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\nprint('Accuracy of NN classifier on test set: {:.2f}'.format(clf.score(X_test_scaled, y_test)))","fc4135b3":"print('The dimension of fraud transactions dataframe is : ', fraud_transactions_df.shape)\nprint('The dimension of non-fraud transactions dataframe is : ', non_fraud_transactions_df.shape)","bfda9ba6":"len(non_fraud_transactions_df) \/ len(fraud_transactions_df)","fdd7d027":"upsampled_df = pd.concat([fraud_transactions_df] * 577, ignore_index=True)","47903f1d":"print('The dimension of upsampled fraud transactions dataframe is : ', upsampled_df.shape)\nprint('The dimension of non-fraud transactions dataframe is : ', non_fraud_transactions_df.shape)","f0eb3451":"print('Difference in number of rows between two dataframes after upsampling is : ', len(non_fraud_transactions_df) - len(upsampled_df))","d8136512":"upsampled_df.describe()","596aca78":"method_2_df = pd.concat([upsampled_df, non_fraud_transactions_df])\nmethod_2_df = method_2_df.sample(frac=1).reset_index(drop=True)\nmethod_2_df.describe()","3f496c9e":"print('The dimension of method_2_df is :', method_2_df.shape)","c5ae6115":"sns.countplot('Class', data=method_2_df, palette=None)\nplt.title(\"Method 2 Frequency distribution plot\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","d0a7c098":"print('Percentage of non-fraud transactions in method_2_df : ',  100*(method_2_df['Class'].value_counts()[0])\/ len(method_2_df))\nprint('Percentage of fraud transactions in method_2_df : ',  100*(method_2_df['Class'].value_counts()[1])\/ len(method_2_df))","02d01b10":"X = method_2_df.drop(['Class'], axis = 1) \ny = method_2_df['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)","5218306d":"print(\"Number transactions in training dataset for Method 2: \", len(X_train))\nprint(\"Number transactions in testing dataset  for Method 2: \", len(X_test))\nprint(\"Total number of transactions  for Method 2 : \", len(X_train)+len(X_test))","d22c088d":"clf = LogisticRegression().fit(X_train, y_train)\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","389a8a00":"lr_predicted = clf.predict(X_test)\nconfusion = confusion_matrix(y_test, lr_predicted)\nprint('Logistic regression classifier (default settings)\\n', confusion)","1866c985":"print('Accuracy: {:.2f}'.format(accuracy_score(y_test, lr_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, lr_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, lr_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, lr_predicted)))","18bd82d0":"logistic_parameters = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid = GridSearchCV(LogisticRegression(), logistic_parameters)\ngrid.fit(X_train, y_train)\nbest_log_reg = grid.best_estimator_\nlogistic_score = cross_val_score(best_log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', logistic_score.mean())","3184e1ca":"lr_predicted = grid.predict(X_test)\nconfusion = confusion_matrix(y_test, lr_predicted)\nprint('Logistic regression classifier with Cross-validation (default settings)\\n', confusion)","f8ddd661":"print('Accuracy: {:.2f}'.format(accuracy_score(y_test, lr_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, lr_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, lr_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, lr_predicted)))","84118ffb":"nbclf = GaussianNB().fit(X_train, y_train)\nprint('Accuracy of GaussianNB classifier on training set: {:.2f}'.format(nbclf.score(X_train, y_train)))\nprint('Accuracy of GaussianNB classifier on test set: {:.2f}'.format(nbclf.score(X_test, y_test)))","c9b3af13":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nclf = MLPClassifier(hidden_layer_sizes = [100, 100], alpha = 5.0,\n                   random_state = 0, solver='lbfgs').fit(X_train_scaled, y_train)\n\nprint('Accuracy of NN classifier on training set: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\nprint('Accuracy of NN classifier on test set: {:.2f}'.format(clf.score(X_test_scaled, y_test)))","7b2cc9ac":"## Neural Net in imbalanced data","4597a102":"**The FN and FP in Credit Card Fraud Detection**\n\n*     False negatives (FN) in this case would be those transactions which are actually fraud transacations, but classified as non-fraud type.\n*     False positives (FP) in this case would be those transactions which are actually non-fraud type, but classified as fraud transactions.\n*     Considering the above two, the first case is very sensitive where it may classify a fraud transaction as a legally valid one\n*     This needs to be sorted out\n\n**Revisiting Accuracy, Precision and Recall**\n* Accuracy = (TP+TN)\/Total Transactions\n*  Precision = TP\/(TP+FP)\n*    Recall = TP\/(TP+FN)\n\n\n\n","5dd8d196":"**Get a randomn sample of 492 transactions from non fraud transactions dataframe**","d6c29bec":"**Using GridSearch to find best parameters for Logistic Regression**","b5f7d030":"**Start by importing required packages**","671b37d8":"### For training and test data, two approaches are mentioned below\n#### Method 1\n\n*     Randomnly select equal number of non-fraud transactions from original data set\n*     So, in this case it is required to select 492 non-fraud transactions\n*     Create a new data frame with the new set of 492 non-fraud and 492 fraud transactions\n*     Build a model with the new data frame of 984 transactions (492 + 492)\n*     This method ensures a 50-50 split of both classes of targets (fraud & non-fraud)\n\n#### Method 2 - Random Over-Sampling\n\n*     Duplicate the 492 fraud transactions to make it equal to 284315 non-fraud transactions\n*     Create a new data frame with the new set of 284315 non-fraud and ~ 284315 fraud transactions\n*     Build a model with the new data frame of 568630 transactions (284315 + 284315)\n*     This method also ensures a 50-50 split of both classes of targets (fraud & non-fraud)\n\n**Both methods have its own pros and cons**\n\n*     Both method ensures there is a target label of 50-50 split, so that the class imbalance problem is avoided\n*     Model building will be fair as target label is equally split\n*     In Method 1, only 492 fair transactions out of total 284315 fair transactions is considered for model building. This amounts to only 0.17%. This doesnt guarantee an accurate model.\n*     In Method 2, all of the fair transactions are considered - and is a good thing. But the fraud transactions are duplicated 577 times (284315\/492 = 577). Still I believe this is a fair approach.\n\n","2a75e0ef":"**Precision & Recall Scores**","7b34e98f":"## Naive Bayes in Method 2","bcec07e5":"* The difference is quite negligible compared to total number of transacctions and so the new data frame can be used for analysis","498257ee":"Let's now verify this with a frequency distribution plot of method_1_df","e0deb714":"**Dataset Inferences**\n* The data is presented with Time, Amount, Class and a series of columns with naming that ranges from V1 to V28\n* Due to confidentiality issues, the actual names of V1-V28 is not provided by the source\n* V1-V28 are principal components obtained via PCA\n* This means V1 through V28 are important in determining whether a transaction is fraud or not and none of them can be neglected\n* 'Time' and 'Amount' columns are not transformed with PCA\n* Feature 'Class' is the target column, have value 1 for a fraud transaction and 0 otherwise.\n\n","07b9b9c1":"## Neural Net in Method 1","5cc53ab5":"**Confusion Matrix**","489891cf":"## Logistic Regression using Imbalanced data","ce1df643":"**Let us read and get some info from the data we read**","6fecfbb7":"# Credit Card Fraud Detection using Logistic Regression, Naive Bayes and Neural Nets","8c408d63":"**Frequency distribution of Class column values in method_2_df**","86ad040c":" ## Naive Bayes in Method 1","0c76e86b":"**Precision-Recall Trade Off\n**\n*     The False Negatives is a serious threat in this case as it could classify fraud transactions as non-fraud type\n*     From the equations in the above cell, it is pretty obvious that it is required to increase the Recall value. Means, a high recall accounts for minimal False Negatives\n*     This inturn means that the detection problem is a recall oriented problem\n*     The above value of Recall is much lower than Precision. It is required to find a mechanism to increase the value of Recall\n\n","7d934d6d":"**Combine the two dataframes and shuffle the rows **","5ce33024":"**Split the method_1_df into inputs and target labels for further analysis (X & y split)**","0bc8ca7b":"**Using GridSearch to find best parameters for Logistic Regression**","d5221bd5":"**Precision-Recall Tradeoff in under-sampled data**\n\n   * Clearly the Recall value have increased way better than that of unbalanced data\n\n","a71d4a81":"## Method 2 Analysis\n\n*     Here the 492 fraud transactions will be duplicated to reach and become equal to 284315 non-fraud transactions\n*     The analysis done in Method 1 will be repeated for this set of data\n","7197f7b8":"## Logistic Regression in Method 2","13698adb":"\n*     The above graph clearly illustrates there is absolutely no relationship between transaction amount over time\n*     This means the transaction time column can be eliminated from the original data frame before further analysis\n\n","36decc69":"**Confusion Matrix**","0d0d773e":"**Confusion Matrix after tuning in with best parameters**","2d9c0484":"Revisiting some codes","330aeaf5":"**Other required libraries are imported now**","fd06ce47":"**Plot Time vs Amount to identify if there is any relationship between transaction amount over time**","19d8d3c1":"**Inferences obtained by dealing directly with imbalanced data**\n\n* No algorithm could perform better, as the accuracies are very high\n* Clear case of overfitting due to imbalanced data\n* Some sort of data manipulation is very much required","1a7a9303":"## Analysis neglecting class imbalance problem\n\n\n*     Here the class imbalance is completely neglected\n*     Different algorithms are applied\n*     Accuracy is measured in all cases and the values will be obviously very high, indicating overfitting problems\n","64246163":"**Class column inference\n**\n*   The target column is heavily imbalanced\n*     Percentage of fraud transactions over total transactions is just 0.17%\n*     Building a model with this target column will definitely lead to overfitting issue\n*     Accuracy of such a model(irrespective of algorithm) will be > 99%\n\n**Feature Engineering requirements\n**\n*     'Class' column is heavily biased. So,it is not advised to proceed without doing something for the bias\n*     'Time' and 'Amount' columns are not transformed. So, it is required to transform them to match with the other values(V1 - V28)\n\n\n\n","11fa89da":"## Dealing with class imbalance\n\n\n","107574bb":"## Method 1 Analysis\n\n**Initial step**\n\n*     Randomnly select 492 non-fraud transactions\n*     Combine 492 non-fraud + 492 fraud to create a new dataframe\n\nSplit fraud & non-fraud transactions as two seperate dataframes\n","b5402c08":"## Naive Bayes in imbalanced data","d3abebc5":"**Precision & Recall Scores**","374fd214":"### Confusion Matrix with imbalanced data","44b5add3":"**Split the method_1_df into inputs and target labels for further analysis (X & y split)**","57836455":"Well before diving into processing, let us see if there is\/are any missing values in the dataframe.\n","ebd61f11":"## Neural Net in Method 2","fb23da86":"## Summary\n* This is a very basic Kernel which gives idea about how to deal with imbalanced data sets.\n* Of all the three classification algorithms used, Logistic Regression is found to be better when dealing with imbalanced data.\n* The Kernel can be improved by bringing in more classification algorithms, more hyper tuning parameters to existing algorithms etc.**\n","9376202d":"**Combine sample non-fraud transactions df with 492 fraud transactions df**\n* Also shuffle and reset the index","1bfd4408":"**Deleting 'Time' column from original dataframe**","1742a15c":"Plotting the number fraud transactions & non-fraud transactions","fb2e801a":"**Change the index of Normalized_Amount and insert the same in the beginning to have a better look of data frame**","acd72792":"**Let us now try two other algorithms also on this imbalanced data**","89effb98":"**More on Class column**","ef27b15d":"* This means, the 492 transactions of fraud_transactions_df needs to be duplicated 577 times to make the number of rows equal to that of non_fraud_transactions_df\n","3e475d5a":"**Scale the 'Amount' column before further analysis, name it as a new column and drop the 'Amount' column**","afc8e12c":"**method_1_df is the data frame with equally distributed target column values for further analysis**\n","27b3c381":"\n*     non_fraud_transactions_df can be kept intact as no change is required in Method 2 as well\n*     492 entries in fraud_transactions_df will be replicated to reach 284315\n* First step is to understand how many times bigger is non_fraud_transactions_df compared to fraud_transactions_df. This can be obtained by simply dividing total number of rows of non fraud transactions with that of fraud transactions.\n","f46be610":"### Applying Logistic Regression in Method 1 - (492 + 492)","922c9007":"**Start by reading data**"}}