{"cell_type":{"6e832299":"code","c0ef4336":"code","b0a7d65c":"code","b6f37d7c":"code","a16349b3":"code","268228ad":"code","d7105886":"code","ea4002fb":"code","7752952b":"code","0f28c57e":"code","b65864dd":"code","b9624926":"code","c5d40df9":"code","047784dc":"code","2bf7cc20":"code","050be68e":"code","da26a98e":"code","25675e54":"code","a4214827":"code","de045ecc":"code","7512bd03":"code","be010bf3":"code","720a86df":"code","2b3620a4":"markdown","994f7259":"markdown","d0731cdb":"markdown","726cf9fd":"markdown","9918deca":"markdown","b47e6b2e":"markdown","4f61e8c4":"markdown","9185e702":"markdown","a5cde942":"markdown","97b652f7":"markdown","5ed61ec3":"markdown","eabf4406":"markdown","1e7bdbac":"markdown","c6d27055":"markdown","c71020d4":"markdown","1dec3910":"markdown","992f4cab":"markdown","089b9b8a":"markdown"},"source":{"6e832299":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\n\n","c0ef4336":"nbeats_pred01_df = pd.read_csv('..\/input\/m5alignnbeatsv01\/nbeats_toplvl_forecasts1.csv')\nnbeats_pred02_df = pd.read_csv('..\/input\/m5alignnbeatsv02\/nbeats_toplvl_forecasts2.csv')\n\n#nbeats_pred_df.head()","b0a7d65c":"BUILD_ENSEMBLE = True","b6f37d7c":"if BUILD_ENSEMBLE:\n    \n    pred_01_df = pd.read_csv('..\/input\/m5-final-13\/submission_v1.csv')\n    pred_02_df = pd.read_csv('..\/input\/fork-of-m5-final-11\/submission_v1.csv')\n    pred_03_df = pd.read_csv('..\/input\/m5-final-12\/submission_v1.csv')\n    pred_04_df = pd.read_csv('..\/input\/m5-final-17\/submission_v1.csv')\n    pred_05_df = pd.read_csv('..\/input\/m5-final-16\/submission_v1.csv')\n    #pred_06_df = pd.read_csv('..')\n\n    avg_pred = ( np.array(pred_01_df.values[:,1:]) \n                + np.array(pred_02_df.values[:,1:]) \n                + np.array(pred_03_df.values[:,1:])\n                + np.array(pred_04_df.values[:,1:])  \n                + np.array(pred_05_df.values[:,1:])  \n               # + np.array(pred_06_df.values[:,1:])  \n               ) \/5.0\n    \n    ## Loading predictions\n    valid_pred_df = pd.DataFrame(avg_pred, columns=pred_01_df.columns[1:])\n    submission_pred_df = pd.concat([pred_01_df['id'],valid_pred_df],axis=1)\n    \nelse:\n    print('Should not submit single distibution')\n    #submission_pred_df = pd.read_csv('..\/input\/m5-final-13\/submission_v1.csv')","a16349b3":"validation_gt_data = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nvalidation_gt_data['id'] = validation_gt_data['id'].str.replace('_evaluation','_validation')\nvalidation_gt_data = validation_gt_data.drop(['item_id','dept_id','cat_id','store_id','state_id'],axis=1)\nvalidation_gt_data = pd.concat([validation_gt_data[['id']],validation_gt_data.iloc[:,-28:]],axis=1)\nvalidation_gt_data.columns=submission_pred_df.columns.values\n#validation_gt_data","268228ad":"submission_pred_df = pd.concat([validation_gt_data, submission_pred_df.iloc[30490:,:]],axis=0).reset_index(drop=True)\nsubmission_pred_df","d7105886":"bottom_lvl_pred_df = submission_pred_df.iloc[30490:,:].reset_index(drop=True)\nbottom_lvl_pred_df","ea4002fb":"name_cols = bottom_lvl_pred_df.id.str.split(pat='_',expand=True)\nname_cols['dept_id']=name_cols[0]+'_'+name_cols[1]\nname_cols['store_id']=name_cols[3]+'_'+name_cols[4]\nname_cols = name_cols.rename(columns={0: \"cat_id\", 3: \"state_id\"})\nname_cols = name_cols.drop([1,2,4,5],axis=1)\nbottom_lvl_pred_df = pd.concat([name_cols,bottom_lvl_pred_df],axis=1)","7752952b":"# Get column groups\ncat_cols = ['id', 'dept_id', 'cat_id',  'store_id', 'state_id']\nts_cols = [col for col in bottom_lvl_pred_df.columns if col not in cat_cols]\nts_dict = {t: int(t[1:]) for t in ts_cols}\n\n# Describe data\nprint('  unique forecasts: %i' % bottom_lvl_pred_df.shape[0])\nfor col in cat_cols:\n    print('   N_unique %s: %i' % (col, bottom_lvl_pred_df[col].nunique()))\n","0f28c57e":"# 1. All products, all stores, all states (1 series)\nall_sales = pd.DataFrame(bottom_lvl_pred_df[ts_cols].sum()).transpose()\nall_sales['id_str'] = 'all'\nall_sales = all_sales[ ['id_str'] +  [c for c in all_sales if c not in ['id_str']] ]","b65864dd":"# 2. All products by state (3 series)\nstate_sales = bottom_lvl_pred_df.groupby('state_id',as_index=False)[ts_cols].sum()\nstate_sales['id_str'] = state_sales['state_id'] \nstate_sales = state_sales[ ['id_str'] +  [c for c in state_sales if c not in ['id_str']] ]\nstate_sales = state_sales.drop(['state_id'],axis=1)","b9624926":"# 3. All products by store (10 series)\nstore_sales = bottom_lvl_pred_df.groupby('store_id',as_index=False)[ts_cols].sum()\nstore_sales['id_str'] = store_sales['store_id'] \nstore_sales = store_sales[ ['id_str'] +  [c for c in store_sales if c not in ['id_str']] ]\nstore_sales = store_sales.drop(['store_id'],axis=1)","c5d40df9":"# 4. All products by category (3 series)\ncat_sales = bottom_lvl_pred_df.groupby('cat_id',as_index=False)[ts_cols].sum()\ncat_sales['id_str'] = cat_sales['cat_id'] \ncat_sales = cat_sales[ ['id_str'] +  [c for c in cat_sales if c not in ['id_str']] ]\ncat_sales = cat_sales.drop(['cat_id'],axis=1)\n","047784dc":"# 5. All products by department (7 series)\ndept_sales = bottom_lvl_pred_df.groupby('dept_id',as_index=False)[ts_cols].sum()\ndept_sales['id_str'] = dept_sales['dept_id'] \ndept_sales = dept_sales[ ['id_str'] +  [c for c in dept_sales if c not in ['id_str']] ]\ndept_sales = dept_sales.drop(['dept_id'],axis=1)","2bf7cc20":"all_pred_agg = pd.concat([all_sales,state_sales,store_sales,cat_sales,dept_sales],ignore_index=True)\n","050be68e":"all_pred_agg.head()","da26a98e":"nbeats_pred01_df.head()","25675e54":"metrics_df = nbeats_pred01_df[['id_str']]\n\n## Calculate errors\n## CAUTION: nbeats_pred_df is \"truth\"\/actual values in this context\nerror = ( np.array(all_pred_agg.values[:,1:]) - np.array(nbeats_pred01_df.values[:,1:]) ) \n\n## Calc RMSSE\nsuccessive_diff = np.diff(nbeats_pred01_df.values[:,1:]) ** 2\ndenom = successive_diff.mean(1)\n\nnum = error.mean(1)**2\nrmsse = num \/ denom\n\nmetrics_df['rmsse'] = rmsse\n\n## Not so clean Pandas action :-) - supressing warnings for now...\nmetrics_df['mean_error'] = error.mean(1)\nmetrics_df['mean_abs_error'] = np.absolute(error).mean(1)\n\nsquared_error = error **2\nmean_squ_err = np.array(squared_error.mean(1), dtype=np.float64) \n\nmetrics_df['rmse'] = np.sqrt( mean_squ_err )\n\nmetrics_df","a4214827":"metrics_df = nbeats_pred02_df[['id_str']]\n\n## Calculate errors\n## CAUTION: nbeats_pred_df is \"truth\"\/actual values in this context\nerror = ( np.array(all_pred_agg.values[:,1:]) - np.array(nbeats_pred02_df.values[:,1:]) ) \n\n## Calc RMSSE\nsuccessive_diff = np.diff(nbeats_pred01_df.values[:,1:]) ** 2\ndenom = successive_diff.mean(1)\n\nnum = error.mean(1)**2\nrmsse = num \/ denom\n\nmetrics_df['rmsse'] = rmsse\n\n## Not so clean Pandas action :-) - supressing warnings for now...\nmetrics_df['mean_error'] = error.mean(1)\nmetrics_df['mean_abs_error'] = np.absolute(error).mean(1)\n\nsquared_error = error **2\nmean_squ_err = np.array(squared_error.mean(1), dtype=np.float64) \n\nmetrics_df['rmse'] = np.sqrt( mean_squ_err )\n\nmetrics_df","de045ecc":"for i in range(0,nbeats_pred01_df.shape[0]):\n    plot_df = pd.concat( [nbeats_pred01_df.iloc[i], all_pred_agg.iloc[i] ]  , axis=1, ignore_index=True)\n    plot_df = plot_df.iloc[1:,]\n    plot_df = plot_df.rename(columns={0:'NBeats',1:'Forecast'})\n    plot_df = plot_df.reset_index()\n    plot_df['index'] = plot_df['index'].str.replace('F','H')\n    #plot_df\n    \n    ax = plot_df.plot(x='index', y=['NBeats', 'Forecast'] ,figsize=(10,5), grid=True, title=nbeats_pred02_df.iloc[i,0], fontsize=16  )\n    ax.legend(loc=1,fontsize=12)\n    ax.set_title(ax.get_title(),pad=20, fontdict={'fontsize':14})\n    axlabel = ax.axes.get_xaxis().get_label()\n    axlabel.set_visible(False)","7512bd03":"for i in range(0,nbeats_pred02_df.shape[0]):\n    plot_df = pd.concat( [nbeats_pred02_df.iloc[i], all_pred_agg.iloc[i] ]  , axis=1, ignore_index=True)\n    plot_df = plot_df.iloc[1:,]\n    plot_df = plot_df.rename(columns={0:'NBeats',1:'Predictions'})\n    plot_df = plot_df.reset_index()\n    #plot_df\n    \n    plot_df.plot(x='index', y=['NBeats', 'Predictions'] ,figsize=(10,5), grid=True, title=nbeats_pred02_df.iloc[i,0]  )","be010bf3":"submission_pred_df","720a86df":"submission_pred_df.to_csv('m5-final-submission.csv', index=False)\n","2b3620a4":"# Visualizations","994f7259":"## Reconstruct level descriptions for aggregation","d0731cdb":"## NBeats 01","726cf9fd":"# Submit based on above analysis and manual selection\/clearance","9918deca":"### NBeats 01","b47e6b2e":"## Interpretation\n\nIf prediction is bigger than \"true\" values error will be positive -> prediction is overshooting (pos error)\n\nIf prediction is smaller than \"true\" values error will be negative -> prediction is undershooting (neg error) \n","4f61e8c4":"## NBeats 02","9185e702":"NBeats predictions trained and predicted on Colab with two different settings (only change in setting is num_epochs to get slightly different ensembles)","a5cde942":"## Load bottom level lgb predictions for alignment","97b652f7":"## Load NBEATS reference predictions for global alignment","5ed61ec3":"## NBeats 02","eabf4406":"# Calculating comparision metrics","1e7bdbac":"## Only work on evaluation forecasts","c6d27055":"# Overall approach\n\nWe have two different inputs: \n\n1) Bottom level forecasts on item level (30490 signal) that are derived from a lgbm model that models a probability of this item being bought based on datatime features, price features and a few other features that are not time dependent. (Credits: https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe)\n2) Top level forecasts for the levels 1-5 that are created with N-Beats. \n\nWe can now aggregate the bottom level \"probabilit draws\" up to the levels 1-5. By comparing\/aligning the possible results we can select the most suitable probability distribution for the forecast period. ( The multiplier in the custom loss of the bottom level lgbm models seems to help adjust for trend or other effects not fully understood yet)","c71020d4":"## Build aggregates of predictions","1dec3910":"Even though it would not make sense at all to score public validation data it might be safest to set the submission validation values to the ground truth....\n\nSpamming the LB a bit more ... ","992f4cab":"## Fill validation rows - we have no info about validation scoring\n","089b9b8a":"### Overall analysis result: \n\nThe multiplier 0.95 seems to represent the lowest available fit so we build an ensemble with the 2 upper and 2 lower distributions to generate a robust test loss.\n<br><br>\nFinal-11: 0.9 <br>\nFinal-12: 0.93 <br>\nFinal-17: 0.95 <br>\nFinal-13: 0.97 <br>\nFinal-16: 0.99"}}