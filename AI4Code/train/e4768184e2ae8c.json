{"cell_type":{"4ce5b2f7":"code","45a83de8":"code","edbfaa95":"code","8c1daddc":"code","55f27375":"code","d11f4a8e":"code","b8926e8b":"code","3c9d2480":"code","506ceb36":"code","97832115":"code","9fad312c":"code","9aea0fcb":"code","3a65f9d5":"code","0fbef94e":"code","38724a5e":"code","98660ab6":"code","3f5613dd":"code","426b4331":"code","0033fe27":"code","9f42e7ad":"code","2a1fb18d":"code","02c495a0":"code","28b3c90d":"code","eb15e222":"code","031b8922":"code","5df514ba":"code","41f8ac87":"code","7b22c693":"code","7d2f4fc2":"code","9cf66321":"code","6269fdff":"code","d61830e5":"code","370194c7":"code","23295335":"code","56de74f3":"code","b3c86cd2":"code","0a37aa68":"code","0407c0af":"code","fa4f6b33":"code","dabfb535":"code","f1d15753":"code","70f9a41c":"code","ec8d101a":"markdown","72733063":"markdown","49bc566d":"markdown","04fc1c3d":"markdown","52e9c8db":"markdown","9cfacf7e":"markdown","195954ca":"markdown","fa6c43b2":"markdown","604d1b4f":"markdown","1b1f6852":"markdown","a8706552":"markdown","d7fd6fbf":"markdown","21f59589":"markdown","ea5b513e":"markdown"},"source":{"4ce5b2f7":"# basics\nimport numpy as np \nimport pandas as pd\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\n\n\n# plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# modeling\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.callbacks import History \nfrom keras.callbacks import EarlyStopping\n","45a83de8":"test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","edbfaa95":"print(train.shape) # 784 pixels in each image\ntrain.head() ","8c1daddc":"print(test.shape) # test dataset do not has label column\ntest.head()","55f27375":"X_train = (train.iloc[:,1:].values).astype('float32') # make a matrix pixel values for each label(number)\ny_train = train.iloc[:,0].values.astype('int32') # number of each image - one number per row\nX_test = test.values.astype('float32')","d11f4a8e":"print(X_train.shape)\nX_train","b8926e8b":"print(y_train.shape)\ny_train","3c9d2480":"X_test","506ceb36":"# convert rows to matrixes for each number\ntrain.iloc[[1]] # each image with 785 pixels - images with 28x28 pixels\n\nX_train = X_train.reshape(X_train.shape[0], 28, 28) ","97832115":"plt.imshow(X_train[9], cmap=plt.get_cmap('gray')) # display one image\nplt.title(y_train[9]) ","9fad312c":"# display more imagens\nfor i in range(0, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(X_train[i],cmap=plt.get_cmap('gray'))\n    plt.title(y_train[i]);","9aea0fcb":"X_train = X_train.reshape(X_train.shape[0], 28*28,) # back to initial matrix\nX_test = X_test.reshape(X_test.shape[0], 28*28,)\nprint(X_train.shape)\nprint(X_test.shape)","3a65f9d5":"print(X_train.max(axis=1)) # max values in each row ","0fbef94e":"# normalizing\nscale = np.max(X_train)\nX_train \/= scale\nX_test \/= scale","38724a5e":"X_train.max(axis=1) # values normalized - max values = 1","98660ab6":"# take out the mean to decrease the correlation\nmean = np.std(X_train)\nX_train -= mean\nX_test -= mean","3f5613dd":"input_shape = X_train.shape[1]\nclasses = y_train","426b4331":"y_train =  to_categorical(y_train) # number of each set of pixels needs to be categorical","0033fe27":"model1 = Sequential()\nmodel1.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel1.add(Dense(10, activation='softmax'))\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","9f42e7ad":"model1.summary()","2a1fb18d":"history_model1 = model1.fit(X_train, y_train, nb_epoch=10, batch_size=16,validation_split=0.1)","02c495a0":"history_model1.history['loss'][-1]","28b3c90d":"results = pd.DataFrame(data = {'model': 'model1', \n                               'loss': [history_model1.history['loss'][-1]], \n                               'accuracy': [history_model1.history['accuracy'][-1]], \n                               'val_loss': [history_model1.history['val_loss'][-1]], \n                               'val_accuracy': [history_model1.history['val_accuracy'][-1]]})\nresults","eb15e222":"model2 = Sequential()\nmodel2.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel2.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel2.add(Dense(10, activation='softmax'))\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory_model2 = model2.fit(X_train, y_train, nb_epoch=10, batch_size=16,validation_split=0.1)","031b8922":"results = pd.DataFrame(data = {'model': ['model1', 'model2'], \n                               'loss': [history_model1.history['loss'][-1], history_model2.history['loss'][-1]], \n                               'accuracy': [history_model1.history['accuracy'][-1], history_model2.history['accuracy'][-1]], \n                               'val_loss': [history_model1.history['val_loss'][-1], history_model2.history['val_loss'][-1]], \n                               'val_accuracy': [history_model1.history['val_accuracy'][-1], history_model2.history['val_accuracy'][-1]]})\nresults\n","5df514ba":"model3 = Sequential()\nmodel3.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel3.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel3.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel3.add(Dense(10, activation='softmax'))\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory_model3 = model3.fit(X_train, y_train, nb_epoch=10, batch_size=16,validation_split=0.1)","41f8ac87":"results = pd.DataFrame(data = {'model': ['model1', 'model2', 'model3'], \n                               'loss': [history_model1.history['loss'][-1], history_model2.history['loss'][-1], history_model3.history['loss'][-1]], \n                               'accuracy': [history_model1.history['accuracy'][-1], history_model2.history['accuracy'][-1], history_model3.history['accuracy'][-1]], \n                               'val_loss': [history_model1.history['val_loss'][-1], history_model2.history['val_loss'][-1], history_model3.history['val_loss'][-1]], \n                               'val_accuracy': [history_model1.history['val_accuracy'][-1], history_model2.history['val_accuracy'][-1], history_model3.history['val_accuracy'][-1]]})\nresults","7b22c693":"model4 = Sequential()\nmodel4.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel4.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel4.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel4.add(Dense(100, activation='relu', input_shape=(input_shape,)))\nmodel4.add(Dense(10, activation='softmax'))\nmodel4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory_model4 = model4.fit(X_train, y_train, nb_epoch=10, batch_size=16,validation_split=0.1)","7d2f4fc2":"results = pd.DataFrame(data = {'model': ['model1', 'model2', 'model3', 'model4'], \n                               'loss': [history_model1.history['loss'][-1], history_model2.history['loss'][-1], history_model3.history['loss'][-1], history_model4.history['loss'][-1]], \n                               'accuracy': [history_model1.history['accuracy'][-1], history_model2.history['accuracy'][-1], history_model3.history['accuracy'][-1], history_model4.history['accuracy'][-1]], \n                               'val_loss': [history_model1.history['val_loss'][-1], history_model2.history['val_loss'][-1], history_model3.history['val_loss'][-1], history_model4.history['val_loss'][-1]], \n                               'val_accuracy': [history_model1.history['val_accuracy'][-1], history_model2.history['val_accuracy'][-1], history_model3.history['val_accuracy'][-1], history_model4.history['val_accuracy'][-1]]})\nresults","9cf66321":"model5 = Sequential()\nmodel5.add(Dense(50, activation='relu', input_shape=(input_shape,)))\nmodel5.add(Dense(50, activation='relu', input_shape=(input_shape,)))\nmodel5.add(Dense(50, activation='relu', input_shape=(input_shape,)))\nmodel5.add(Dense(50, activation='relu', input_shape=(input_shape,)))\nmodel5.add(Dense(10, activation='softmax'))\nmodel5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory_model5 = model5.fit(X_train, y_train, nb_epoch=10, batch_size=16,validation_split=0.1)","6269fdff":"results = pd.DataFrame(data = {'model': ['model1', 'model2', 'model3', 'model4', 'model5'], \n                               'loss': [history_model1.history['loss'][-1], history_model2.history['loss'][-1], history_model3.history['loss'][-1], history_model4.history['loss'][-1], history_model5.history['loss'][-1]], \n                               'accuracy': [history_model1.history['accuracy'][-1], history_model2.history['accuracy'][-1], history_model3.history['accuracy'][-1], history_model4.history['accuracy'][-1], history_model5.history['accuracy'][-1]], \n                               'val_loss': [history_model1.history['val_loss'][-1], history_model2.history['val_loss'][-1], history_model3.history['val_loss'][-1], history_model4.history['val_loss'][-1], history_model5.history['val_loss'][-1]], \n                               'val_accuracy': [history_model1.history['val_accuracy'][-1], history_model2.history['val_accuracy'][-1], history_model3.history['val_accuracy'][-1], history_model4.history['val_accuracy'][-1], history_model5.history['val_accuracy'][-1]]})\nresults","d61830e5":"model6 = Sequential()\nmodel6.add(Dense(50, activation='relu', input_shape=(input_shape,)))\nmodel6.add(Dense(50, activation='relu', input_shape=(input_shape,)))\nmodel6.add(Dense(50, activation='relu', input_shape=(input_shape,)))\nmodel6.add(Dense(10, activation='softmax'))\nmodel6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory_model6 = model6.fit(X_train, y_train, nb_epoch=10, batch_size=16,validation_split=0.1)","370194c7":"results = pd.DataFrame(data = {'model': ['model1', 'model2', 'model3', 'model4', 'model5', 'model6'], \n                               'loss': [history_model1.history['loss'][-1], history_model2.history['loss'][-1], history_model3.history['loss'][-1], history_model4.history['loss'][-1], history_model5.history['loss'][-1], history_model6.history['loss'][-1]], \n                               'accuracy': [history_model1.history['accuracy'][-1], history_model2.history['accuracy'][-1], history_model3.history['accuracy'][-1], history_model4.history['accuracy'][-1], history_model5.history['accuracy'][-1], history_model6.history['accuracy'][-1]], \n                               'val_loss': [history_model1.history['val_loss'][-1], history_model2.history['val_loss'][-1], history_model3.history['val_loss'][-1], history_model4.history['val_loss'][-1], history_model5.history['val_loss'][-1], history_model6.history['val_loss'][-1]], \n                               'val_accuracy': [history_model1.history['val_accuracy'][-1], history_model2.history['val_accuracy'][-1], history_model3.history['val_accuracy'][-1], history_model4.history['val_accuracy'][-1], history_model5.history['val_accuracy'][-1], history_model6.history['val_accuracy'][-1]]})\nresults","23295335":"plt.plot(history_model1.history['val_loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()","56de74f3":"preds_model1 = model1.predict_classes(X_test, verbose=0)","b3c86cd2":"y_pred = model1.predict(X_test)\ny_predict_classes = model1.predict_classes(X_test)","0a37aa68":"y_predict_classes","0407c0af":"y_test = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\ny_test = y_test['Label']\ny_test = y_test.astype('int32')","fa4f6b33":"Y_pred = np.argmax(y_pred, 1)\nprint(Y_pred)","dabfb535":"def write_preds(preds, fname):\n    pd.DataFrame({\"ImageId\": list(range(1,len(preds)+1)), \"Label\": preds}).to_csv(fname, index=False, header=True)\n","f1d15753":"write_preds(y_predict_classes, \"to_submit.csv\")","70f9a41c":"pd.read_csv('to_submit.csv')","ec8d101a":"## Test some models","72733063":"### model4 - 4 hidden layers with 100 neurons","49bc566d":"## Setting libraries","04fc1c3d":"### Some results about model1","52e9c8db":"### model1: 1 hidden layer with 100 neurons","9cfacf7e":"## Setting the data","195954ca":"### Show some images","fa6c43b2":"### model3 - 3 hidden layers with 100 neurons","604d1b4f":"### model5 - 4 hidden layers with 50 neurons","1b1f6852":"## Pre-processing the data","a8706552":"### model2 - 2 hidden layers with 100 neurons","d7fd6fbf":"### model6 - 3 hidden layers with 50 neurons","21f59589":"## Analyzing the data","ea5b513e":"**About the results**:\n* It is worse when add layers\n* To decrease neurons number do not make the results better\n* For this configuration and model, model1 provides the bests results"}}