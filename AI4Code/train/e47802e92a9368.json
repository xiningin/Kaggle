{"cell_type":{"3486aa72":"code","da1f7497":"code","287da661":"code","640c8908":"code","0c1f0cb0":"code","b05a7fd3":"code","c11bdc4c":"code","c71c77e4":"code","e84ce95a":"code","3eb187f8":"code","87d53ca4":"code","5a53f2c1":"code","0c91b486":"code","e141afd8":"markdown","e17fe163":"markdown","7d12abd1":"markdown","d51ce4b3":"markdown","13338721":"markdown","549196b8":"markdown","35de5b73":"markdown"},"source":{"3486aa72":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \nimport umap\nfrom IPython.display import display, HTML\nfrom pprint import pprint\nimport scipy.optimize as optimize\nfrom matplotlib import pyplot as plt\nimport warnings\nimport seaborn as sns\n\nwarnings.filterwarnings(\"ignore\")","da1f7497":"df_test = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv\")\ndf_test_l = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv\")\ndf_test_l = df_test_l[df_test_l[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']].sum(axis=1)>=0]\nprint(df_test_l.shape)\nprint(df_test.shape)\ndf_test = pd.merge(df_test, df_test_l, how=\"inner\", on = \"id\")\ndf_test.shape","287da661":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(df.shape)\n\ndf = pd.concat([df, df_test])\nprint(df.shape)\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(10))","640c8908":"labels=['toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfeature_wts_all = {}\nprint(df.shape)\n\nfor lbl in labels:\n    print(\"*\"*30 + lbl.upper() + \"*\"*30)\n    features_tmp = FeatureUnion([\n        (\"vect1\", TfidfVectorizer(min_df= 3, \n                                  max_df=0.5, \n                                  analyzer = 'word', \n                                 )),\n\n    ])\n    pipeline_tmp = Pipeline(\n        [\n            (\"features\", features_tmp),\n            (\"clf\", Ridge()),\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline_tmp.fit(df[(df[lbl]>0)|(df[labels].sum(axis=1)==1)]['comment_text'], df[(df[lbl]>0)|(df[labels].sum(axis=1)==1)][lbl])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline_tmp['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline_tmp['features'].get_feature_names(), \n                                  np.round(pipeline_tmp['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    print(\"High score features\")\n    #pprint(feature_wts[:50])\n    feature_wts_all[lbl] = [(x.replace('vect1__',''),y) for x,y in feature_wts if (y > 0.25) & (x.replace('vect1__','').isalpha()) & ( len(x.replace('vect1__','')) > 2)]\n\npprint(feature_wts_all)","0c1f0cb0":"# Put words in DF with toxicity score for each category","b05a7fd3":"df_wts = []\nfor k in feature_wts_all.keys():\n    df_wts.append(pd.DataFrame(feature_wts_all[k], columns = [\"word\",\"wt\"]).assign(label=k))\n\nimp_words_df = pd.concat(df_wts).pivot(index='word', columns='label', values='wt').fillna(0)#.reset_index()\nprint(imp_words_df.shape)\n\nimp_words_df.head()\n","c11bdc4c":"for lbl in labels:\n    if len(feature_wts_all[lbl]) > 0:\n        print(lbl.upper())\n        ax = imp_words_df\\\n                .sort_values(lbl,ascending=False)\\\n                .head(30)\\\n                .sort_values(lbl,ascending=True)\\\n                .plot\\\n                .barh(rot=0, width=1, figsize = (12,12))\n        plt.show()","c71c77e4":"def load_fasttext_model(path):\n    embeddings = {}\n    f = open(path, encoding='utf-8')\n    for line in f:\n        values = line.strip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings[word] = coefs\n    f.close()\n    return embeddings","e84ce95a":"ft_model = load_fasttext_model('..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec')","3eb187f8":"# Assign label to word \nimp_words_df['label_max'] = imp_words_df.idxmax(axis=1)\nimp_words_df['val_max'] = imp_words_df.max(axis=1)\n","87d53ca4":"imp_words_df_tmp = imp_words_df.sort_values('val_max',ascending=False).head(300).copy()\n\nvect = []\nids = []\nlbls = []\nfor idx in imp_words_df_tmp.index.tolist():\n    #print(idx)\n    if idx in ft_model:\n        vect.append(ft_model[idx])\n        ids.append(idx)\n        lbls.append(imp_words_df_tmp.loc[idx].label_max)\n","5a53f2c1":"vect_arr = np.array(vect)\nreducer = umap.UMAP()\nvect_arr_red = reducer.fit_transform(vect_arr)\nvect_arr_red.shape","0c91b486":"color_map = {\"identity_hate\":\"blue\",\n             \"insult\":\"green\",\n             \"obscene\":\"red\",\n             \"severe_toxic\":\"black\",\n             \"threat\":\"yellow\"}\nplt.figure(figsize=(15,15))\n\nplt.scatter(\n    vect_arr_red[:, 0],\n    vect_arr_red[:, 1],\n    c=[x for x in pd.Series(lbls).map(color_map)])\n\n#red_patch = mpatches.Patch(color='red', label='The red data')\nhandlelist = [plt.plot([], marker=\"o\", ls=\"\", color=color)[0] for x,color in color_map.items()]\nplt.legend(handlelist,[x for x,y in color_map.items()],loc='upper left')#plt.legend(handles={color_map})\n\nplt.title('UMAP projection of the important words', fontsize=24)\nfor i, txt in enumerate(ids):\n    plt.annotate(txt, \n                 (vect_arr_red[i, 0], vect_arr_red[i, 1]), \n                 textcoords=\"offset points\",  # how to position the text\n                 size=10,\n                 xytext=(0, 0.3),  # distance from text to points (x,y)\n                 ha='left')","e141afd8":"# Reduce dimensionality with UMAP","e17fe163":"# Extract sample of IMPORTANT words","7d12abd1":"# Bar chart showing words with highest toxicity within each group","d51ce4b3":"# read toxic comments data","13338721":"# Visualize\n\n## Can see some clear groups among the highly toxic words for each category","549196b8":"# Find IMPORTANT words for each type of toxicity","35de5b73":"# Load fasttext vectors"}}