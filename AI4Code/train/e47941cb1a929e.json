{"cell_type":{"521c5873":"code","5f37c671":"code","a9142db5":"code","431ea02e":"code","9122b7cd":"code","b4a1aa4c":"code","72878dc4":"code","34b5a8d2":"code","f9e40d61":"code","07970997":"code","98739929":"code","1e8e7181":"code","2720f6ac":"code","5e5be0bf":"code","5a7b4cef":"code","78974633":"code","2b0d26d5":"code","268a54a0":"code","334ef490":"code","eea9f943":"code","c2e0c212":"code","be3ee4a8":"code","ef4c69a6":"code","9e651047":"code","49de2a0c":"code","8576c0c3":"code","35e63387":"code","7359f41e":"code","bc3019b4":"code","52051bed":"code","09289960":"code","2709766e":"code","1af87c5c":"code","1f2064ba":"code","cb238ee7":"code","81bb7917":"code","6e031dad":"code","2606e0b5":"code","df6d0e25":"code","5006ca3b":"code","a6f96daf":"code","8b351af6":"code","a560d3db":"code","fa3e8b06":"code","f7012f2a":"code","338eeb63":"code","241ef5b9":"code","f7d85a21":"code","69d16792":"code","4952994a":"code","934b81ec":"code","2a599fee":"code","2ab7f6f0":"code","ee6fa94a":"code","79ecabb3":"code","366f4bce":"code","e1b9ddb9":"code","924f47a6":"code","64e88dc1":"code","78eba68b":"markdown","d4ffe49d":"markdown","fe9ffc18":"markdown","b638162a":"markdown","41a74ecc":"markdown","698fb0c8":"markdown","884fd8a0":"markdown","30d5a330":"markdown","65bff840":"markdown","35572ede":"markdown","80808226":"markdown","9ab42653":"markdown","e07bd627":"markdown","8915a6a1":"markdown"},"source":{"521c5873":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f37c671":"import numpy as np\n# Basic Visualization\nimport seaborn as sn\nimport matplotlib.pyplot as plt \n%matplotlib inline \nsn.set_style(style=\"whitegrid\")\nimport cufflinks as cf\ncf.go_offline()\nfrom sklearn import svm\nfrom mlxtend.plotting import plot_decision_regions","a9142db5":"df= pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")","431ea02e":"df.head(4)","9122b7cd":"df.shape","b4a1aa4c":"df.groupby('Species').size()","72878dc4":"sn.countplot(\"Species\",data=df)","34b5a8d2":"label = 'Species'\n\nf,axes = plt.subplots(2,2, figsize = (10,10) , dpi=100)\nsn.violinplot(x = label , y = 'SepalLengthCm', data = df , ax= axes[0,0])\nsn.violinplot(x = label , y = 'SepalWidthCm', data = df , ax= axes[0,1])\nsn.violinplot(x = label , y = 'PetalLengthCm', data = df , ax= axes[1,0])\nsn.violinplot(x = label  , y = 'PetalWidthCm', data = df , ax= axes[1,1])\nplt.show()","f9e40d61":"# Creating a pairplot to visualize the similarities and especially difference between the species\nsn.pairplot(data=df, hue='Species', palette='Set1')","07970997":"Df = df.drop(\"Id\",axis=1)\nDf.iloc[:,:4].iplot(kind= 'box' , boxpoints = 'outliers')\nplt.show()","98739929":"df.columns","1e8e7181":"# Separating the independent variables from dependent variables\nX=df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ny=df.Species","2720f6ac":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","5e5be0bf":"from sklearn.svm import SVC\nmodel=SVC()","5a7b4cef":"model.fit(X_train, y_train)","78974633":"pred=model.predict(X_test)","2b0d26d5":"# Importing the classification report and confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix","268a54a0":"print(confusion_matrix(y_test,pred))","334ef490":"print(classification_report(y_test, pred))","eea9f943":"from sklearn.svm import SVC\nmodel=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)","c2e0c212":"print(confusion_matrix(y_test,pred))","be3ee4a8":"print(classification_report(y_test, pred))","ef4c69a6":"from sklearn.semi_supervised import LabelSpreading\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\niris = datasets.load_iris()\n\nX = iris.data[:, :2]\ny = iris.target\n\nrng = np.random.RandomState(0)\n# step size in the mesh\nh = .02\n\ny_30 = np.copy(y)\ny_30[rng.rand(len(y)) < 0.3] = -1\ny_50 = np.copy(y)\ny_50[rng.rand(len(y)) < 0.5] = -1\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nls30 = (LabelSpreading().fit(X, y_30), y_30)\nls50 = (LabelSpreading().fit(X, y_50), y_50)\nls100 = (LabelSpreading().fit(X, y), y)\nrbf_svc = (svm.SVC(kernel='rbf', gamma=.5).fit(X, y), y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['Label Spreading 30% data',\n          'Label Spreading 50% data',\n          'Label Spreading 100% data',\n          'SVC with rbf kernel']\n\ncolor_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}\n\nfor i, (clf, y_train) in enumerate((ls30, ls50, ls100, rbf_svc)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.axis('off')\n\n    # Plot also the training points\n    colors = [color_map[y] for y in y_train]\n    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='black')\n\n    plt.title(titles[i])\n\nplt.suptitle(\"Unlabeled points are colored white\", y=0.1)\nplt.show()","9e651047":"svc = svm.SVC(kernel='rbf', C=1,gamma=\"auto\").fit(X, y)","49de2a0c":"# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nh = (x_max \/ x_min)\/100\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n np.arange(y_min, y_max, h))","8576c0c3":"plt.subplot(1, 1, 1)\nZ = svc.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)","35e63387":"svc = svm.SVC(kernel='rbf', C=10,gamma=\"auto\").fit(X, y)","7359f41e":"# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nh = (x_max \/ x_min)\/100\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n np.arange(y_min, y_max, h))","bc3019b4":"plt.subplot(1, 1, 1)\nZ = svc.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)","52051bed":"#Changing gamma to Scale\nsvc = svm.SVC(kernel='rbf', C=10,gamma=\"scale\").fit(X, y)","09289960":"# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nh = (x_max \/ x_min)\/100\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n np.arange(y_min, y_max, h))","2709766e":"plt.subplot(1, 1, 1)\nZ = svc.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)","1af87c5c":"C = 100  # SVM regularization parameter\nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)","1f2064ba":"# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))","cb238ee7":"# title for the plots\ntitles = ['SVC with linear kernel',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel',\n          'LinearSVC (linear kernel)']","81bb7917":"plt.set_cmap(plt.cm.Paired)\n\nfor i, clf in enumerate((svc, rbf_svc, poly_svc, lin_svc)):\n    # Plot the decision boundary. For that, we will asign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.set_cmap(plt.cm.Paired)\n    plt.contourf(xx, yy, Z)\n    plt.axis('off')\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n\n    plt.title(titles[i])\n\nplt.show()","6e031dad":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","2606e0b5":"# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :]\ny = iris.target\nprint (\"Number of data points ::\", X.shape[0])\nprint(\"Number of features ::\", X.shape[1])","df6d0e25":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","5006ca3b":"fig = plt.figure(1, figsize=(14, 12))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(X_scaled)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n           cmap=plt.cm.Set1, edgecolor='b', s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()\nprint(\"The number of features in the new subspace is \" ,X_reduced.shape[1])","a6f96daf":"X, y = datasets.load_iris(return_X_y=True)\nX.shape, y.shape","8b351af6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)","a560d3db":"X_train.shape, y_train.shape","fa3e8b06":"X_test.shape, y_test.shape","f7012f2a":"clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)","338eeb63":"clf.score(X_test, y_test)","241ef5b9":"from sklearn.model_selection import cross_val_score","f7d85a21":"clf = svm.SVC(kernel='linear', C=1)","69d16792":"scores = cross_val_score(clf, X, y, cv=5)","4952994a":"scores","934b81ec":"print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","2a599fee":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)\n","2ab7f6f0":"pred=clf.predict(X_test)","ee6fa94a":"print(confusion_matrix(y_test,pred))","79ecabb3":"print(classification_report(y_test, pred))","366f4bce":"from sklearn import svm, datasets\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import StratifiedKFold","e1b9ddb9":"# Data IO and generation\n\n# Import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX, y = X[y != 2], y[y != 2]\nn_samples, n_features = X.shape\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]","924f47a6":"# Classification and ROC analysis\n\n# Run classifier with cross-validation and plot ROC curves\ncv = StratifiedKFold(n_splits=6)\nclassifier = svm.SVC(kernel='linear', probability=True,\n                     random_state=random_state)","64e88dc1":"tprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\n\nfig, ax = plt.subplots()\nfor i, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = plot_roc_curve(classifier, X[test], y[test],\n                         name='ROC fold {}'.format(i),\n                         alpha=0.3, lw=1, ax=ax)\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\nax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n        label='Chance', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nax.plot(mean_fpr, mean_tpr, color='b',\n        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n        lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                label=r'$\\pm$ 1 std. dev.')\n\nax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n       title=\"Receiver operating characteristic example\")\nax.legend(loc=\"lower right\")\nplt.show()","78eba68b":"# Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality using cross-validation","d4ffe49d":"# Different SVM classifiers in the iris dataset","fe9ffc18":"# Will try one more approach","b638162a":"# Computing cross-validated metrics","41a74ecc":"# Let's try using Dimentionality Reduction by PCA","698fb0c8":"# In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. ****Wikipedia****\n","884fd8a0":"# ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the \u201cideal\u201d point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\n\n# The \u201csteepness\u201d of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\n\n# This shows the ROC response of different datasets, created from K-fold cross-validation. Taking all of these curves, it is possible to calculate the mean area under curve, and see the variance of the curve when the training set is split into different subsets. This roughly shows how the classifier output is affected by changes in the training data, and how different the splits generated by K-fold cross-validation are from one another.","30d5a330":"# Model after modifying test sample size","65bff840":"# **Important Parameters: -**\n**C - >  float, default=1.0. C is a regularization parameter.The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty**\n\n**Kernel -> {\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019\nSpecifies the kernel type to be used in the algorithm. It must be one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 or a callable. If none is given, \u2018rbf\u2019 will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices.\n**\n\n**Gamma - > {\u2018scale\u2019, \u2018auto\u2019} or float, default=\u2019scale\u2019\n\u2022\tIf gamma='scale' (default) is passed then it uses 1 \/ (n_features * X.var()) as value of gamma,\n\u2022\tif \u2018auto\u2019, uses 1 \/ n_features.\n**","35572ede":"# Receiver Operating Characteristic (ROC) with cross validation","80808226":"# Loading Iris DataSet & working in a different way","9ab42653":"# class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)  ","e07bd627":"# sklearn.svm.SVC","8915a6a1":"# Benefits of SVM\n* It works really well with a clear margin of separation\n* It is effective in high dimensional spaces.\n* It is effective in cases where the number of dimensions is greater than the number of samples.\n* It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n"}}