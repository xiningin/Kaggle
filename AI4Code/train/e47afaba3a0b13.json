{"cell_type":{"100f61fe":"code","49008497":"code","13cc87d2":"code","a3e3227d":"code","5cd2216f":"code","864558ad":"code","e5d186ae":"code","41698d21":"code","90164c7c":"code","c00a0f6c":"code","444664d4":"code","15eaff12":"code","5005f6ce":"code","cb3ad0ce":"code","87d48c94":"code","87d87d45":"code","187aea0f":"code","3520ab8c":"code","c4760804":"code","aaaabc73":"code","d12d5ee2":"code","ae46a946":"code","22234ca6":"code","31a829d2":"code","3247f6b5":"code","c45c7089":"code","8fb9d2f5":"code","648e7cde":"code","b81cbec7":"code","225fb8d1":"code","6b4e3d6e":"code","0f136322":"code","80c4e38c":"code","0fa1055e":"code","f25ea32a":"code","a3a9df54":"code","70a6b9c5":"code","5ac6b8a9":"code","97f676fb":"code","e3ef0388":"code","52a23290":"code","d798465b":"code","8d04ee21":"code","6556a296":"code","1ac88e95":"code","e5da16fc":"code","e170e11c":"code","11f7d95c":"code","ffd2f1a9":"code","3f49730a":"code","40e6b975":"code","d9bc518a":"markdown","f96d104c":"markdown","eec52150":"markdown","64a4f72f":"markdown","e9825793":"markdown","1df8c754":"markdown","701af5f7":"markdown","83460e36":"markdown","80df2920":"markdown","fb0d0663":"markdown","22459da2":"markdown","7c29595d":"markdown","9f08f846":"markdown","81bfc4a3":"markdown","797eb873":"markdown","52f60906":"markdown","cb0d86ca":"markdown","c21d6fe6":"markdown","b8e701b6":"markdown","8d05b56a":"markdown","b91f17be":"markdown","a1c8ad9c":"markdown"},"source":{"100f61fe":"#for data\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn import preprocessing\n\n#for plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#for feature\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#for model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n","49008497":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\n\ntrain_num = len(train_data)\ntest_num = len(test_data)\n\nall_data = train_data.append(test_data, ignore_index = True)\nall_data.head(3)","13cc87d2":"# get each columns mssing ratio\ndef na_check(df_data):\n    data_na = (df_data.isnull().sum() \/ len(df_data)) * 100\n    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\n    missing_data = pd.DataFrame({'Missing Ratio %' :data_na})\n    display(missing_data.head(10))\n    return missing_data","a3e3227d":"missing_data = na_check(all_data)","5cd2216f":"# first extract numeric features\nnum_features = []\ncat_features = []\nfor dtype, feature in zip(all_data.dtypes, all_data.columns):\n    if dtype == 'float64' or dtype == 'int64':\n        num_features.append(feature)\n    else:\n        cat_features.append(feature)\nprint(f'{len(num_features)} Numeric Features : {num_features}\\n')\nprint(f'{len(cat_features)} Category Features : {cat_features}\\n')","864558ad":"not_in_the_right_col_list = ['Survived', 'Pclass']\nfor col in not_in_the_right_col_list:\n    num_features.remove(col)\n    cat_features.append(col)\nnum_features.remove('PassengerId')\nprint(f'{len(num_features)} Numeric Features : {num_features}\\n')\nprint(f'{len(cat_features)} Category Features : {cat_features}\\n')","e5d186ae":"for col in num_features:\n    sns.distplot(all_data[col][:train_num].fillna(0))\n    plt.show()","41698d21":"plt.figure(figsize = (8, 6))\nsns.heatmap(all_data.corr(), cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap')","90164c7c":"na_cols = missing_data.index.values\nna_cols","c00a0f6c":"# Cabin:\u3000Cabin number\nall_data['Cabin'][~pd.isnull(all_data['Cabin'])].head(20)","444664d4":"cabin_count = pd.DataFrame([i[0] if not pd.isnull(i) else 'X' for i in all_data['Cabin']], columns=['Cabin'])\ncabin_count['count'] = [1] * len(cabin_count) \ncabin_count.groupby('Cabin').count()","15eaff12":"temp_data = all_data.dropna()\ntemp_data['Cabin_code'] = [i[0] if not pd.isnull(i) else 'X' for i in temp_data['Cabin']]","5005f6ce":"plt.figure(figsize = (12, 9))\nsns.heatmap(pd.get_dummies(temp_data[['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Cabin_code']]).corr(), cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap')","cb3ad0ce":"temp_data[(temp_data['Cabin_code'] == 'F') | (temp_data['Cabin_code'] == 'G')].sort_values('Cabin_code')","87d48c94":"cabin_code_g_masking = (all_data['Cabin'].isna()) & \\\n                       (all_data.Fare >= min(temp_data[(temp_data['Cabin_code'] == 'G')].Fare)) & \\\n                       (all_data.Fare <= max(temp_data[(temp_data['Cabin_code'] == 'G')].Fare)) & \\\n                       (all_data.Pclass == '3') \nall_data.loc[cabin_code_g_masking, 'Cabin'] = 'G'\n\ncabin_code_f_masking = (all_data['Cabin'].isna()) & \\\n                       ((all_data.Fare >= min(temp_data[(temp_data['Cabin_code'] == 'F') & (temp_data['Pclass'] == 2)].Fare)) & \\\n                       (all_data.Fare <= max(temp_data[(temp_data['Cabin_code'] == 'F') & (temp_data['Pclass'] == 2)].Fare))) | (\n                       (all_data.Fare <= max(temp_data[(temp_data['Cabin_code'] == 'F') & (temp_data['Pclass'] == 3)].Fare)))\nall_data.loc[cabin_code_f_masking, 'Cabin'] = 'F'\n\n# deal with remain\nall_data['Cabin_code'] = [i[0] if not pd.isnull(i) else 'X' for i in all_data['Cabin']]","87d87d45":"# remove the Cabin and check the cabin_code count after fill na.\nall_data = all_data.drop('Cabin', axis = 1)\nall_data.groupby('Cabin_code').count()['Name']","187aea0f":"temp_data = all_data[['Age', 'Pclass', 'SibSp']]\ntemp_data['SibSp_group'] = pd.cut(all_data.SibSp, 4)\nage_group_result = temp_data.groupby(['Pclass', 'SibSp']).median()","3520ab8c":"# Age\n# There are two features have some related in 'Age': Pclass' and 'SibSp'.\n# And if we cut SibSp into 4 groups, there are only 3 * 4 = 12 combination, we can fill missing value with median within each group.\n\n# calculate each group median\ntemp_data = all_data[['Age', 'Pclass', 'SibSp']]\ntemp_data['SibSp_group'] = pd.cut(all_data.SibSp, 4)\nage_group_result = temp_data.groupby(['Pclass', 'SibSp']).median().reset_index()\nage_median = all_data.Age.median()\n\nage_na_idx_list = list(all_data[\"Age\"][all_data[\"Age\"].isnull()].index)\n\nfor i in age_na_idx_list:\n    pclass = all_data['Pclass'].iloc[i]\n    sib = all_data['SibSp'].iloc[i]\n    if (pclass != 'Nan') & (sib != 'Nan'):\n        masking = (age_group_result.Pclass == pclass) & (age_group_result.SibSp == sib)\n        all_data['Age'].iloc[i] =  age_group_result.loc[masking, 'Age'].values[0]\n    else:\n        all_data['Age'].iloc[i] = age_median","c4760804":"# Fare\n# There is only about 0.07% missing value % and the contribution is skew, so we fillna with 0\nall_data[\"Fare\"] = all_data[\"Fare\"].fillna(all_data[\"Fare\"].median())","aaaabc73":"# Embarked\nall_data[all_data['Embarked'].isna()]","d12d5ee2":"type(7.25) == 'float64'","ae46a946":"# There are only two rows missing Embarked and note that their Ticker is the same, so if we find some rules \n# I guess it can apply for two of them.\nall_data[(all_data['Cabin_code'] == 'B') & (all_data['Fare'] >= 70) & (all_data['Fare'] <= 90)].sort_values('Ticket')","22234ca6":"# There is no significant pattern from above table, 'C' and 'S' these two Embarked are very close.\n# I choose 'S' due to their Ticket number is more close than 'C', maybe I'm wrong but sometimes you need to make a choise from uncertainty.\nall_data.loc[all_data['Embarked'].isna(), 'Embarked'] = 'S'","31a829d2":"na_check(all_data)","3247f6b5":"# We already know the distribution of Fare is skew in section 2-3, let's observe more info.\nprint(all_data['Fare'][:train_num].describe())\n\n# Though there is no any unreasonable value, but it need some transform to make later modeling better.\nall_data[\"Fare\"] = all_data[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","c45c7089":"# Now it's better.\nsns.distplot(all_data[\"Fare\"])","8fb9d2f5":"need_one_hot_cols = []\n\nfor dtype, feature in zip(all_data.dtypes, all_data.columns):\n    if (dtype != 'float64') & (dtype != 'int64'):\n        need_one_hot_cols.append(feature)\nprint(need_one_hot_cols)","648e7cde":"# extract title from name\nall_data['Title'] = [name[name.find(',') + 2 : name.find('.')] for name in all_data['Name']]\ngroup_title_result = all_data.groupby('Title').count()\ngroup_title_result['Name']","b81cbec7":"# There are some rare title in passengers.\nrare_titles = group_title_result[group_title_result['Name'] < 9].index.values\nrare_titles","225fb8d1":"rare_titles_making = all_data['Title'].isin(rare_titles)\nall_data.loc[rare_titles_making, 'Title'] = 'Rare'\nall_data.groupby('Title').count()['Name']","6b4e3d6e":"# Ticket\n# First, take a look.\nall_data.Ticket.unique()","0f136322":"# Many of them are pure digit, but other ticket numbers seem exist some infos.\n# Let's extract these specify alphabets.\nticket = []\nfor i in all_data.Ticket:\n    if not i.isdigit():\n        ticket.append(i.replace(\".\", \"\").replace(\"\/\", \"\").split(\" \")[0])\n    else:\n        ticket.append('X')\n\nall_data['ticket_code'] = ticket\ngroup_ticket_result = pd.DataFrame(ticket, columns=['ticket']).groupby('ticket').size().sort_values(ascending = False)\ngroup_ticket_result.head(10)","80c4e38c":"# There are some rare ticket alphabets in passengers.\nrare_ticket = group_ticket_result[group_ticket_result < 10].index.values\nrare_ticket","0fa1055e":"rare_ticket_masking = all_data.ticket_code.isin(rare_ticket)\nall_data.loc[rare_ticket_masking, 'ticket_code'] = 'Rare_Ticket'\nall_data.groupby('ticket_code').count()['Name']","f25ea32a":"# Before we use one-hot-encoding to transfer these category features, I want to deal with three variables first.\n# Age, Parch and SibSp\n# Age\n# Take a look first\nprint(all_data.Age.describe())\nsns.distplot(all_data.Age)","a3a9df54":"# I think there is no significant difference between age of 11 and 12, so I'd like to give them a range instead of true age.\nall_data['age_group'] = pd.cut(all_data.Age, range(0, 81, 10))\nall_data['age_group'] = ['age_' + str(ag).split(',')[0][1:] + '_' + str(ag).split(',')[1][1:3] for ag in all_data['age_group']]\nall_data.groupby('age_group').size()","70a6b9c5":"# Second, Parch and SibSp all represent family size actually.\n# So, Let's combine them together, and take a look. \n# By the way, don't forget plus 1 just like when you count how many people in the class be remember count yourself.\nall_data['family_size'] = all_data.Parch + all_data.SibSp + 1\ngroup_family_size_result = all_data.groupby('family_size').size().reset_index()\nsns.barplot(group_family_size_result['family_size'], group_family_size_result[0])","5ac6b8a9":"# I'll sum all of family size greater equal than 4 to 'Big_Family'\nstr_family_size = []\nfor i in all_data.family_size:\n    if i < 4:\n        str_family_size.append(str(i))\n    else:\n        str_family_size.append('Big_Family')\n\nall_data['family_size'] = str_family_size","97f676fb":"all_data.head()","e3ef0388":"# leave need columns\nleave_cols = ['Embarked', 'Fare', 'Sex', 'Cabin_code', 'Title', 'ticket_code', 'age_group', 'family_size']\nclean_data = pd.get_dummies(all_data[leave_cols])\nclean_data = clean_data.drop('Sex_male', axis = 1)\nclean_data.head()","52a23290":"X_train, y_train = clean_data[:train_num], all_data.Survived[:train_num]\nX_test, y_test = clean_data[train_num:], all_data.Survived[train_num:]","d798465b":"# Prepare classifiers\nkfold = StratifiedKFold(n_splits=10)\n\nrandom_state = 1234\nclassifiers = []\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(XGBClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())","8d04ee21":"# generate cv accuracy results\ncv_results = []\ncv_scores = []\nfor classifier in classifiers :\n    cv_result = cross_val_score(classifier, X_train, y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4)\n    cv_results.append(cv_result)\n    cv_scores.append(cv_result.mean())","6556a296":"cv_plot_data = pd.DataFrame({\"Algorithm\": [\"LogisticReg\", \"RandomForest\", \"SVC\", \"GradientBoosting\",\n                                           \"AdaBoost\", \"XGBoost\", \"MLP\", \"KNN\"],\n                             \"CV_scores\": cv_scores})\ncv_plot_data = cv_plot_data.sort_values('CV_scores')\ncv_score_bar_plot = sns.barplot(\"Algorithm\", \"CV_scores\", data = cv_plot_data)\nfor item in cv_score_bar_plot.get_xticklabels():\n    item.set_rotation(90)","1ac88e95":"# XGB Parameters tunning \nXGB = XGBClassifier()\n\n## Search grid for optimal parameters\nparam_XGB = { 'max_depth':range(3, 10, 2),\n                  'min_child_weight':range(1, 6, 2),\n                  'gamma':[i\/10.0 for i in range(0, 5)],\n                  'subsample':[i\/10.0 for i in range(6, 10, 2)],\n                  'colsample_bytree':[i\/10.0 for i in range(6, 10, 2)],\n                  'reg_alpha':[0, 0.001, 0.01],\n                  'learning_rate': [0.1, 0.01],\n                 }\n\nrandomCV = RandomizedSearchCV(XGB,\n                            param_XGB,\n                            cv=10,\n                            n_iter=20,\n                            n_jobs=1,\n                            verbose=2)\n\nrandomCV.fit(X_train,y_train)\n\nXGB_best = randomCV.best_estimator_\n\n# Best score\nrandomCV.best_score_","e5da16fc":"MLP = MLPClassifier(random_state=random_state, max_iter=300)\n\nparam_grid_MLP = {\n    'hidden_layer_sizes': [(50,50), (50,100,50), (100,)],\n    'activation': ['logistic', 'tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.0001, 0.001, 0.01],\n    'learning_rate': ['constant','adaptive'],\n    'learning_rate_init': [0.01, 0.001]\n}\n\nrandomCV_MLP = RandomizedSearchCV(MLP,\n                            param_grid_MLP,\n                            cv=10,\n                            n_iter=20,\n                            n_jobs=1,\n                            verbose=2)\n\nrandomCV_MLP.fit(X_train, y_train)\n\nMLP_best = randomCV_MLP.best_estimator_\n\nrandomCV_MLP.best_score_","e170e11c":"GB = GradientBoostingClassifier(random_state=random_state)\n\nparam_grid_GB = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.1, 0.05, 0.01],\n    'n_estimators': [50, 100, 200],\n    'subsample': [1, 0.9, 0.8],\n    'max_depth': range(2, 6)\n}\n\nrandomCV_GB = RandomizedSearchCV(GB,\n                            param_grid_GB,\n                            cv=10,\n                            n_iter=20,\n                            n_jobs=1,\n                            verbose=2)\n\nrandomCV_GB.fit(X_train, y_train)\n\nGB_best = randomCV_GB.best_estimator_\n\nrandomCV_GB.best_score_","11f7d95c":"LGR = LogisticRegression(random_state = random_state, solver = 'liblinear')\n\nparam_LGR = {\n    'penalty': ['l1', 'l2'],\n    'C': [0.1, 0.5, 1, 5, 10]\n}\n\nrandomCV_LGR = RandomizedSearchCV(LGR,\n                                 param_LGR,\n                                 cv=10,\n                                 n_iter=20,\n                                 n_jobs=1,\n                                 verbose=2)\n\nrandomCV_LGR.fit(X_train, y_train)\n\nLGR_best = randomCV_LGR.best_estimator_\n\nrandomCV_LGR.best_score_","ffd2f1a9":"svc = SVC(random_state=random_state, probability=True)\n\nparam_SVC = {\n    'C': [0.1, 0.5, 1, 5, 10],\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n    'gamma': ['scale', 'auto']\n}\n\nrandomCV_SVC = RandomizedSearchCV(svc,\n                                 param_SVC,\n                                 cv=10,\n                                 n_iter=20,\n                                 n_jobs=1,\n                                 verbose=2)\n\nrandomCV_SVC.fit(X_train, y_train)\n\nSVC_best = randomCV_SVC.best_estimator_\n\nrandomCV_SVC.best_score_","3f49730a":"voting_estmators = [('XGB', XGB_best), ('MLP', MLP_best), ('GB',GB_best), \n                    ('LGR',LGR_best), ('SVC', SVC_best)]\nvoting_cls = VotingClassifier(estimators = voting_estmators, voting = 'soft', n_jobs = 4)\n\nvoting_cls = voting_cls.fit(X_train, y_train)\n\ny_hat = voting_cls.predict(X_test)","40e6b975":"y_hat = [int(y) for y in y_hat]\n\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_hat}).to_csv(\"voting_cls_with_top_5_models.csv\",index=False)","d9bc518a":"Cabin_code_F and Cabin_code_G are significant in Pclass!\nLet's take a look.","f96d104c":"### 2-2. check columns type","eec52150":"## 2. Mining the data\n### 2-1. missing value%","64a4f72f":"## 3. Feature engineer\n### 3-1. fill na","e9825793":"We can make inferences from above table.\n+ 10.4625 <= Fare <= 16.7 and Pclass = 3 then Cabin_code = G\n+ (10.5 <= Fare <= 39 and Pclass = 2) or (Fare <= 7.65 and Pclass = 3)then then Cabin_code = F","1df8c754":"### 2-4. correlation of each feature","701af5f7":"# Analysis Flow\n1. Preprocessing\n   + 1-1. import required package \n   + 1-2. load data\n2. Mining the data\n   + 2-1. missing value%\n   + 2-2. check columns type\n   + 2-3. any outlier?\n   + 2-4. correlation of each feature\n3. Feature engineer\n   + 3-1. fill na\n   + 3-2. deal with outlier\n   + 3-3. ont-hot encoding\n4. Modeling\n   + 4-1. Choosing models for esembling\n    * Logistic regression\n    * Random Forest\n    * SVC\n    * Gradient Boosting\n    * AdaBoost\n    * XGBoost\n    * MLP\n    * KNN\n   + 4-2. Tuning hyper parameters \n   + 4-3. Ensembling models\n    * Voting","83460e36":"## 4. Modeling\n### 4-1. Choosing models for esembling\nDue to the goal is to predict each passenger is survived or not, I choose these models as below.\n* Logistic regression\n* Random Forest\n* SVC\n* Gradient Boosting\n* AdaBoost\n* XGBoost\n* MLP\n* KNN","80df2920":"There is -0.56 corr between Pclass and Fare, it's very intuitive. Maybe it's a good idea to keep one of them and remove the other.","fb0d0663":"### 3-3. ont-hot encoding","22459da2":"### 3-2. deal with outlier","7c29595d":"### 2-3. any outlier?\nIn this cell, I will replace all the na into 0 temporarily just for observe the distribution of each numeric feature.","9f08f846":"There are too many missing values in this feature, let's try observe the relationship between Cabin and others features in order to get some tips.\nNotice that I'll only observe rows that do not contain missing values.","81bfc4a3":"After getting this information, all na will be replaced by some value or just remove in section 3-1","797eb873":"## 1. Preprocessing\n### 1.1 import required package","52f60906":"### 1-2. load data","cb0d86ca":"Here, I choose the best 5 Algorithm to build esemble model.","c21d6fe6":"Though these features \"Survived\", \"Pclass\" seems like numeric feature, but it's category actually. So we remove it from num_features.","b8e701b6":"There are some outliers in these features except \"PassengerId\".","8d05b56a":"There are some unknown info like \"F G73\" or \"C23 C25 C27\", so this is a noisy feature unless we get additional information.","b91f17be":"### 4-2. Tuning hyper parameters","a1c8ad9c":"### 4-3. Ensembling models\n* Voting"}}