{"cell_type":{"f2d734ae":"code","f2789861":"code","cb286638":"code","b5871b9c":"code","66b7ddc8":"code","92213024":"code","242d8eea":"code","bac57958":"code","c80df980":"code","357dfafc":"code","2decc436":"code","d1eb1862":"code","b268b135":"code","79282180":"code","896be2c8":"code","ce098168":"code","c5c0bd81":"code","6a882bc1":"code","9546c4a3":"code","15211c15":"code","bf8280c1":"code","b2067e23":"code","93e3d05c":"code","a52b20c2":"code","f24791ae":"code","85de73a7":"code","a21ad158":"markdown","5ec3a5bb":"markdown","941a6b1f":"markdown","0e75db92":"markdown","36720584":"markdown","aab3ac11":"markdown","56f96a7b":"markdown","9cd32e4b":"markdown","cc26ec4c":"markdown","c940ecfe":"markdown"},"source":{"f2d734ae":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" # silence the warning","f2789861":"import transformers\nimport torch\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport textwrap as wrap\nfrom torch.utils import data\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nfrom collections import defaultdict\nimport re,random\nfrom transformers import XLMRobertaForSequenceClassification\nfrom transformers import XLMRobertaTokenizer","cb286638":"# constants - config params\nRANDOM_SEED = 42\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS=3\nLR=5e-6\nTEST_SIZE = 0.10\nTRAINING_FILE = \"..\/input\/contradictory-my-dear-watson\/train.csv\"\nSAMPLE_FILE = \"..\/input\/contradictory-my-dear-watson\/sample_submission.csv\"\nTEST_FILE = \"..\/input\/contradictory-my-dear-watson\/test.csv\"\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)\nMODEL_TYPE ='xlm-roberta-large'# \"bert-base-multilingual-cased\"\nMODEL_FILENAME = 'model.bin'\n\n# Seed it - it can be a function or part of config\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\n","b5871b9c":"if MODEL_TYPE == 'xlm-roberta-large':\n    TOKENIZER = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)\nelse:\n    TOKENIZER = transformers.BertTokenizer.from_pretrained(MODEL_TYPE)","66b7ddc8":"df_train = pd.read_csv(TRAINING_FILE)\ndf_test = pd.read_csv(TEST_FILE)\ndf_train['plen'] = df_train['premise'].apply(len)\ndf_train['hlen'] = df_train['hypothesis'].apply(len)\ndf_train['classes'] = np.where(df_train['label'] == 0, 'Entailment', np.where(df_train['label'] == 1,'Neutral','Contradiction'))\ndf=df_train;\ndf_train.head()","92213024":"sns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"]=(8,6)\nsns.countplot(df_train.classes);\nsns.set(style=\"dark\")\nclass_names = ['Entailment','Neutral','Contradiction'];","242d8eea":"plt.rcParams[\"figure.figsize\"]=(8,6)\nsns.distplot(df_train.plen,kde=True, rug=False, color='red');","bac57958":"sns.distplot(df_train.hlen, color='blue');","c80df980":"sns.distplot(df_train.hlen.values + df_train.plen.values, color='green');","357dfafc":"class ContradictoryDatasetXlmr(data.Dataset):\n    def __init__(self,premise,hypothesis,target,tokenizer,max_len):\n        self.premise = premise\n        self.hypothesis = hypothesis\n        self.target = target\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.premise)\n\n    def __getitem__(self, item):\n        ptext = str(self.premise[item])\n        htext = str(self.hypothesis[item])\n        ptext = \" \".join(ptext.split())\n        htext = \" \".join(htext.split())\n\n        # Encoding\n        encoding = TOKENIZER.encode_plus(\n            ptext, \n            htext, \n            max_length = self.max_len,\n            add_special_tokens = True,\n            truncation=True,\n            padding='max_length',\n            return_attention_mask=True,\n            pad_to_max_length=True,\n            return_tensors='pt',\n        )\n        return {\n            'ptext': ptext,\n            'htext': htext,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(self.target[item],dtype=torch.long)\n        }","2decc436":"class ContradictoryModelXlmr(nn.Module):\n    def __init__(self, n_classes):\n        super(ContradictoryModelXlmr,self).__init__()\n        self.bert = XLMRobertaForSequenceClassification.from_pretrained(MODEL_TYPE)\n        self.bert_drop_1 = nn.Dropout(0.2)\n        self.bert_relu = nn.PReLU()\n        self.fc = nn.Linear(self.bert.config.hidden_size,self.bert.config.hidden_size)\n        self.fc2 = nn.Linear(self.bert.config.hidden_size,self.bert.config.hidden_size)\n        self.bn = nn.BatchNorm1d(self.bert.config.hidden_size)\n        self.out = nn.Linear(self.bert.config.hidden_size,n_classes) # (768,3)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_ids, attention_mask):\n        pooled_output = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n        )\n        output = self.fc(pooled_output[0])\n        output = self.bn(output)\n        output = self.bert_relu(output)\n        output = self.fc2(output)\n        output = self.bn(output)\n        output = self.out(output)        \n        return self.softmax(output)\n","d1eb1862":"df_train, df_val = train_test_split(df_train, test_size=TEST_SIZE,random_state=RANDOM_SEED)\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_train.shape, df_val.shape","b268b135":"def create_data_loader_xlmr(df, tokenizer, max_len, bsz, shuffle):\n    ds = ContradictoryDatasetXlmr(\n        premise=df.premise.to_numpy(),\n        hypothesis=df.hypothesis.to_numpy(),\n        target=df.label.to_numpy(),\n        tokenizer=TOKENIZER,\n        max_len = MAX_LEN,\n    )\n    return data.DataLoader(\n        ds,\n        batch_size=bsz,\n        num_workers=4,\n        shuffle=shuffle\n    )\n\ntrain_dl = create_data_loader_xlmr(df_train, tokenizer=TOKENIZER, max_len=MAX_LEN, bsz=TRAIN_BATCH_SIZE,shuffle=True)\nval_dl = create_data_loader_xlmr(df_val, tokenizer=TOKENIZER, max_len=MAX_LEN, bsz=VALID_BATCH_SIZE,shuffle=False)","79282180":"#model = ContradictoryModel(len(class_names))\nmodel = ContradictoryModelXlmr(len(class_names))\nmodel = model.to(DEVICE)\n\n#optimizer parameters\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n                        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n\n#optimizer \noptimizer = AdamW(optimizer_parameters, lr=LR)\nsteps = len(train_dl) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = steps\n)\nloss_fn = nn.CrossEntropyLoss().to(DEVICE)","896be2c8":"def train_fn(model, dl, loss_fn, optimizer, device, scheduler, n_examples):\n    model.train()\n    losses = []\n    predictions = 0\n\n    #iterate each from dl\n    for d in tqdm(dl, total=len(dl), position=0, leave=True):\n        input_ids = d['input_ids'].to(DEVICE)\n        attention_mask = d['attention_mask'].to(DEVICE)\n#        token_type_ids = None #d['token_type_ids'].to(DEVICE, dtype=torch.long)\n        targets = d['targets'].to(DEVICE)\n\n        outputs = model(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n#            token_type_ids = token_type_ids,\n        )\n\n        preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        predictions += torch.sum(preds==targets)\n        losses.append(loss.item())\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return predictions.double() \/ n_examples, np.mean(losses)\n\ndef eval_fn(model, dl, loss_fn, device, n_examples):\n    model.eval()\n    losses = []\n    predictions = 0\n\n    with torch.no_grad():\n        for d in tqdm(dl, total=len(dl), position=0, leave=True):\n            input_ids = d['input_ids'].to(DEVICE)\n            attention_mask = d['attention_mask'].to(DEVICE)\n#            token_type_ids = None # d['token_type_ids'].to(DEVICE, dtype=torch.long)\n            targets = d['targets'].to(DEVICE)\n\n            outputs = model(\n                input_ids = input_ids,\n                attention_mask = attention_mask,\n#                token_type_ids = token_type_ids,\n            )\n\n            preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n\n            predictions += torch.sum(preds==targets)\n            losses.append(loss.item())\n    return predictions.double() \/ n_examples, np.mean(losses)","ce098168":"%%time\nhist = defaultdict(list)\nbest_acc = 0\n#print(model)\nfor epoch in range(EPOCHS):\n    print(f'\\nEpoch {epoch + 1} \/ {EPOCHS}')\n    train_acc, train_loss = train_fn(model,train_dl,loss_fn,optimizer,DEVICE,scheduler,len(df_train))\n    print(f'Train loss {train_loss} Accuracy {train_acc}')\n\n    val_acc, val_loss = eval_fn(model,val_dl,loss_fn,DEVICE,len(df_val))\n    print(f'Validation loss {val_loss} Accuracy {val_acc}')\n    print()\n\n    hist['train_acc'].append(train_acc)\n    hist['train_loss'].append(train_loss)\n    hist['val_acc'].append(val_acc)\n    hist['val_loss'].append(val_loss)\n\n    if val_acc > best_acc:\n        torch.save(model.state_dict(),MODEL_FILENAME)\n        best_acc = val_acc","c5c0bd81":"plt.figure(figsize=(8,6))\nplt.gca().title.set_text(f'Accuracy Chart')\nplt.plot(np.arange(EPOCHS),hist['train_acc'],label='Training')\nplt.plot(np.arange(EPOCHS),hist['val_acc'],label='Validation')\nplt.legend();","6a882bc1":"plt.figure(figsize=(8,6))\nplt.gca().title.set_text(f'Loss Chart')\nplt.plot(np.arange(EPOCHS),hist['train_loss'],label='Training')\nplt.plot(np.arange(EPOCHS),hist['val_loss'],label='Validation')\nplt.legend();","9546c4a3":"def get_preds(model, data_loader):\n    model.eval()\n    predictions = []\n    prediction_proba = []\n \n    with torch.no_grad():\n        for d in tqdm(data_loader, total=len(data_loader)):\n            input_ids = d['input_ids'].to(DEVICE)\n            attention_mask = d['attention_mask'].to(DEVICE)\n            token_type_ids = d['token_type_ids'].to(DEVICE, dtype=torch.long)\n            outputs = model(\n                input_ids = input_ids,\n                attention_mask = attention_mask,\n                token_type_ids = token_type_ids,\n            )\n\n            _,preds = torch.max(outputs, dim=1)\n            predictions.extend(preds)\n            prediction_proba.extend(outputs)\n    predictions = torch.stack(predictions).cpu()\n    prediction_proba = torch.stack(prediction_proba).cpu()\n\n    return predictions, prediction_proba","15211c15":"model = ContradictoryModelXlmr(len(class_names))\nmodel.load_state_dict(torch.load(MODEL_FILENAME))\nmodel = model.to(DEVICE)\n#df_test = pd.read_csv(TEST_FILE)\ndf_test['label']=-1\ntest_dl = create_data_loader_xlmr(df_test, tokenizer=TOKENIZER, max_len=MAX_LEN, bsz=TRAIN_BATCH_SIZE,shuffle=False)\n","bf8280c1":"preds, proba = get_preds(model,test_dl)\nlen(preds)","b2067e23":"df_sample = pd.read_csv(SAMPLE_FILE)","93e3d05c":"preds[:10], proba[:10]","a52b20c2":"df_sample['prediction']=preds;df_sample.head()","f24791ae":"df_sample.to_csv(\"submission.csv\",index=False)","85de73a7":"!ls .\/submission.csv","a21ad158":"### Using bert-base-multilingual-cased on GPU\n\nThis is an interesting competition and good one for NLI with multilingual. An amazing starter Notebook that was given in the competition is pretty good which gives the score of about .65 for just single epoch. I would suggest anyone who starts with this competition to read through that notebook first.\n\nI would like to share this very simple and basic starter notebook using BERT Model with the use of next sentence prediction classification. Most of the models available in the transformer library are mono-lingual models (English, Chinese and German etc.). However, a few multi-lingual models are available and have a different mechanisms than mono-lingual models. I will be using the smaller bert-base-multilingual-cased model in this notebook which supports Masked language modeling + Next sentence prediction for 104 languages.\n\nTried with both cased and uncased multi-lingual bert models but found the cased gave me better results. Hope this is helpful to those who are starting their experiments in this area.\n","5ec3a5bb":"#### Cleanse Data","941a6b1f":"### Training Engine","0e75db92":"### Optimization\n\n#### AdamW (PyTorch)\n* optimizer with weight decay fixed that can be used to fine-tuned models, and\n\n* several schedules in the form of schedule objects that inherit from _LRSchedule:\n\n* a gradient accumulation class to accumulate the gradients of multiple batches\n\n### transformers.get_linear_schedule_with_warmup\n![image.png](attachment:image.png)","36720584":"### Sentence Length","aab3ac11":"### Runner","56f96a7b":"### Data Loader\n\nBasic data loader","9cd32e4b":"### Prediction","cc26ec4c":"### Dataset\nDataset returns encoded input_ids","c940ecfe":"### Visualize the Run"}}