{"cell_type":{"67dfb15e":"code","8815d356":"code","1b1c95ae":"code","22dbb77d":"code","67974b3d":"code","3a35db6b":"code","79b8028b":"code","2eb82a6a":"code","d09af387":"code","9b66b52c":"code","81b6b7dd":"code","472fc55d":"markdown","34acfc99":"markdown","b387a770":"markdown","82a7c972":"markdown","e817ebbe":"markdown","e83173b8":"markdown","b1c48083":"markdown","a1c3d72b":"markdown","f63bc360":"markdown"},"source":{"67dfb15e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8815d356":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","1b1c95ae":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","22dbb77d":"# Calculating the rate of women who survived\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\nprint(\"% of women who survived:\", rate_women)\n\n# Calculating the rate of men who survived\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\nprint(\"% of men who survived:\", rate_men)","67974b3d":"from sklearn.ensemble import RandomForestClassifier\n\n# Getting the label column of the training data.\ny = train_data[\"Survived\"]\n\n# Initial features that might have a strong influence on our model.\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\n# n_estimators = number of trees.\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n\n# Fitting model (with our training data) (i.e. training the model)\nmodel.fit(X, y)\n\n# Making predictions with the recently-trained model using our test data.\npredictions = model.predict(X_test)\n\n# Building the predictions dataset (that is very similar, in structure, to the\n# gender_submission.csv file)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","3a35db6b":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Instantiating our linear regression model.\nmodel = LinearRegression()\n\n# Possible high-influence features\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\n# Getting random entries from our label and input matrix (in the training data)\nX_train = pd.get_dummies(train_data[features])\nprint(len(X_train))\ny_train = train_data[\"Survived\"]\nprint(len(y_train))\nX_test = pd.get_dummies(test_data[features])\n\n# Using seed 22 for the random_state to split data\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=22)\n\n# Training model.\nmodel.fit(X_train, y_train)\n\n# Getting predictions\ny_predictions = model.predict(X_test)\nprint(len(y_predictions))\n\n# Calculating model accuracy\n# print(mean_squared_error(y, y_predictions))\nprint(model.score(X_train, y_train))","79b8028b":"# Exploring the dataset with a bird's eye view.\n#\n# We can definitely see that the Age, Embarked, and Cabin have some null values.\ntrain_data.info()","2eb82a6a":"# Loading the training and testing datasets.\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# This time, I am going to drop some features from the training data set that I particularly\n# believe won't influence in any way the outcome of the trained model\n#\n# These include: name, fare, cabin, and ticket type\ntrain_data = train_data.drop(\"Name\", axis=1).drop(\"Fare\", axis=1).drop(\"Cabin\", axis=1).drop(\"Ticket\", axis=1).drop(\"PassengerId\", axis=1)\ntest_data = test_data.drop(\"Name\", axis=1).drop(\"Fare\", axis=1).drop(\"Cabin\", axis=1).drop(\"Ticket\", axis=1).drop(\"PassengerId\", axis=1)\n\n# I merged the siblings and parents\/children columns into just one. I added 1 the result\n# to account for that row's person.\ntrain_data[\"RelativesAboard\"] = 1 + train_data[\"SibSp\"] + train_data[\"Parch\"]\ntrain_data = train_data.drop(\"SibSp\", axis=1).drop(\"Parch\", axis=1)\n\n# Let's do the same with our testing set so that we have matching shapes.\ntest_data[\"RelativesAboard\"] = 1 + test_data[\"SibSp\"] + test_data[\"Parch\"]\ntest_data = test_data.drop(\"SibSp\", axis=1).drop(\"Parch\", axis=1)","d09af387":"# Loading the training and testing datasets again from scratch.\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# These are the feature I don't think will highly influence the model's outcome.\nfeatures_training_to_drop = [\"PassengerId\", \"Age\", \"SibSp\", \"Parch\", \"Name\", \"Ticket\", \"Cabin\"]\ntrain_data.drop(features_training_to_drop, axis=1)\n\nfeatures_test_to_drop = [\"PassengerId\", \"Age\", \"Name\", \"Ticket\", \"Cabin\", \"SibSp\", \"Parch\"]\ntest_data.drop(features_test_to_drop, axis=1)\n\n# As we saw above, the Fare column has some missing values. So, let's fill them with\n# the mean of all the column values for the Fare.\nmean_embarked_test = test_data[\"Fare\"].mean()\ntest_data[\"Fare\"] = test_data[\"Fare\"].fillna(mean_embarked_test)\nprint(test_data.info())\n\n# Also filling the Embarked column's missing values...\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(\"S\")","9b66b52c":"from sklearn.preprocessing import LabelEncoder\nprint(train_data.loc[:, \"Pclass\"])\nlabelEncoder = LabelEncoder()\n\n# We have to do the same procedure (conversion of non-categorical features to categorical ones)\n# on both the training and testing sets.\ntrain_data.loc[:, \"Pclass\"] = labelEncoder.fit_transform(train_data.loc[:, \"Pclass\"].values)\ntrain_data.loc[:, \"Sex\"] = labelEncoder.fit_transform(train_data.loc[:, \"Sex\"].values)\nprint(train_data.info())\n\ntest_data.loc[:, \"Pclass\"] = labelEncoder.fit_transform(test_data.loc[:, \"Pclass\"].values)\ntest_data.loc[:, \"Sex\"] = labelEncoder.fit_transform(test_data.loc[:, \"Sex\"].values)\nprint(test_data.info())","81b6b7dd":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Splitting the data into training and testing sets (again)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=4)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Fitting model\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\n# Final score: (last seen at around ~76.20%)\nprint(metrics.accuracy_score(y_test, y_pred))","472fc55d":"# Using Nearest Neighbors Model","34acfc99":"# Starting our KNN model","b387a770":"# Using Linear Regression Model","82a7c972":"# Loading training data (train.csv)\n### I'm also exploring the dataset by printing its first five rows.","e817ebbe":"# Building the first ML model (Random forest)","e83173b8":"# Exploring some feature combinations for our KNN model","b1c48083":"# Let's not forget to transform non-categorical (continuous) data to categorical data so that we can use them in our model","a1c3d72b":"# Generally exploring the data","f63bc360":"# Loading test data (test.csv)\n### I'm also exploring the dataset by printing its first five rows."}}