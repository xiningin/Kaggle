{"cell_type":{"f714d329":"code","f2c49054":"code","a97ba17a":"code","d52360f2":"code","02639b61":"code","b11e1c4b":"code","329c30bc":"code","fa507af9":"code","c00551be":"code","de0de475":"code","65d79674":"code","e9aab19a":"code","e528701d":"code","23cb1cb8":"code","2c2abd85":"code","0a25ee1d":"code","636bdb83":"code","69d1a47b":"code","349e7ccc":"code","dbf0828d":"markdown"},"source":{"f714d329":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n#Reading train and test data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/income-qualification\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/income-qualification\/test.csv\")","f2c49054":"#Analyzing the train data\nprint(train_data.head)\n\n#Target variable has 4 values corresponding to:\n#1 = extreme poverty\n#2 = moderate poverty\n#3 = vulnerable households\n#4 = non vulnerable households","a97ba17a":"print(train_data.describe())","d52360f2":"#check if the data is imbalanced\ntarget_count = train_data['Target'].value_counts()\ntarget_count.plot(kind=\"bar\", title=\"Target count\")","02639b61":"#Data preprocessing\n\n#check for null values\ntrain_data.isnull().any().any()","b11e1c4b":"#Drop the ID column\ntrain_data.drop(\"Id\", axis=1, inplace=True)","329c30bc":"#Converting string columns to integer columns\n#from sklearn import preprocessing\n\n#encoder = preprocessing.LabelEncoder()\n\nprint(train_data.dtypes)\nfor columns in train_data.columns:\n    if (train_data[columns].dtype == \"object\"):\n        (train_data[columns], uniques) = pd.factorize(train_data[columns])\n\nprint(train_data.dtypes)","fa507af9":"train_data = train_data.fillna(train_data.mean())\nprint(train_data.isnull().any().any())","c00551be":"print(train_data.dtypes)","de0de475":"import imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn import preprocessing\n\n#dividing X and Y in data\nX = np.array(train_data.iloc[:, train_data.columns != 'Target'])\nY = np.array(train_data.iloc[:, train_data.columns == 'Target'])\nprint(\"X_data size is \", X.shape, \" Y_data size is \", Y.shape)\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_normalized = min_max_scaler.fit_transform(X)\nX = pd.DataFrame(X_normalized)\n\nros = RandomOverSampler()\nX_oversampled, Y_oversampled = ros.fit_sample(X, Y)\nprint(\"X_data size is \", X_oversampled.shape, \" Y_data size is \", Y_oversampled.shape)","65d79674":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)\nprincipalComponents = pca.fit_transform(X_oversampled)\nX_oversampled = principalComponents\nprincipalDf = pd.DataFrame(principalComponents)\nprint(principalDf.head)","e9aab19a":"from sklearn.model_selection import train_test_split\n\n#Splitting training and testing data\n(X_train, X_Test, Y_train, Y_Test) = train_test_split(X_oversampled, Y_oversampled, test_size = 0.33, stratify = Y_oversampled, random_state=1)","e528701d":"#Using Random Forest as Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=10, random_state=0)\nclf.fit(X_train, Y_train)\n","23cb1cb8":"from sklearn.metrics import accuracy_score\n\ny_pred = clf.predict(X_Test)\naccuracy_score(Y_Test, y_pred)","2c2abd85":"#Randomforest using cross validation\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42)\n# Fit the random search model\nrf_random.fit(X_train, Y_train)","0a25ee1d":"#Best parameters are:\nrf_random.best_params_","636bdb83":"base_model = RandomForestClassifier(n_estimators = 500,min_samples_split = 2,min_samples_leaf = 1, max_features= 'sqrt',max_depth=35, bootstrap= False, random_state = 42)\nbase_model.fit(X_train, Y_train)","69d1a47b":"from sklearn.metrics import accuracy_score\n\ny_pred_regres = base_model.predict(X_Test)\naccuracy_score(Y_Test, y_pred_regres)","349e7ccc":"print(y_pred)\nprint(y_pred_regres)","dbf0828d":"**Since data is biased and consists of more data corresponding to non-vulnerable households, we need to balance the data.\nI'm using oversampling technique to balance the data.**"}}