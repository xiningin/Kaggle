{"cell_type":{"d85ccfce":"code","57fba877":"code","2db0d70e":"code","c7165d20":"code","371432d0":"code","ac8396bc":"code","61d41a41":"code","30dc4aaf":"code","6116c0ca":"code","b51356ad":"code","90231308":"code","8399c54e":"code","533b0f93":"code","5b50501b":"code","d87605f7":"markdown"},"source":{"d85ccfce":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport torch\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom glob import glob","57fba877":"!mkdir -p \/root\/.config\/Ultralytics\n!cp \/kaggle\/input\/yolov5\/starfish_yolov5\/Arial.ttf \/root\/.config\/Ultralytics\/","2db0d70e":"shutil.copytree('\/kaggle\/input\/yolov5\/starfish_yolov5', '\/kaggle\/working\/yolov5\/starfish_yolov5')\nos.chdir('\/kaggle\/working\/yolov5\/starfish_yolov5') # install dependencies","c7165d20":"file = open(\"\/kaggle\/input\/starfish-fold\/video\/video\/val_video1.txt\")\nlines = file.readlines()\n# ..\/input\/starfish-fold\/video\/video\/val_video1.txt","371432d0":"%%writefile fold0.yaml\n\nnames: [starfish]\nnc: 1\ntrain: '\/kaggle\/input\/starfish-fold\/video\/video\/val_video1.txt'\nval: '\/kaggle\/input\/starfish-fold\/video\/video\/val_video1.txt'","ac8396bc":"# ..\/input\/reef-baseline-fold12\/l6_3600_uflip_vm5_f12_up\/f1\/best.pt","61d41a41":"import argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom threading import Thread\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nROOT= '\/kaggle\/working\/yolov5\/starfish_yolov5\/'\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT)) \n\nfrom models.experimental import attempt_load\nfrom utils.datasets import create_dataloader\nfrom utils.general import coco80_to_coco91_class, check_dataset, check_img_size, check_requirements, \\\n    check_suffix, check_yaml, box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, \\\n    increment_path, colorstr, print_args\nfrom utils.metrics import ap_per_class, ConfusionMatrix\nfrom utils.plots import output_to_target, plot_images, plot_val_study\nfrom utils.torch_utils import select_device, time_sync\nfrom utils.callbacks import Callbacks\n\ndef save_one_txt(predn, save_conf, shape, file):\n    # Save one txt result\n    gn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh\n    for *xyxy, conf, cls in predn.tolist():\n        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) \/ gn).view(-1).tolist()  # normalized xywh\n        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n        with open(file, 'a') as f:\n            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n\n\ndef save_one_json(predn, jdict, path, class_map):\n    # Save one JSON result {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}\n    image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n    box = xyxy2xywh(predn[:, :4])  # xywh\n    box[:, :2] -= box[:, 2:] \/ 2  # xy center to top-left corner\n    for p, b in zip(predn.tolist(), box.tolist()):\n        jdict.append({'image_id': image_id,\n                      'category_id': class_map[int(p[5])],\n                      'bbox': [round(x, 3) for x in b],\n                      'score': round(p[4], 5)})\n\n\ndef process_batch(detections, labels, iouv):\n    \"\"\"\n    Return correct predictions matrix. Both sets of boxes are in (x1, y1, x2, y2) format.\n    Arguments:\n        detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n        labels (Array[M, 5]), class, x1, y1, x2, y2\n    Returns:\n        correct (Array[N, 10]), for 10 IoU levels\n    \"\"\"\n    correct = torch.zeros(detections.shape[0], iouv.shape[0], dtype=torch.bool, device=iouv.device)\n    iou = box_iou(labels[:, 1:], detections[:, :4])\n    x = torch.where((iou >= iouv[0]) & (labels[:, 0:1] == detections[:, 5]))  # IoU above threshold and classes match\n    if x[0].shape[0]:\n        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detection, iou]\n        if x[0].shape[0] > 1:\n            matches = matches[matches[:, 2].argsort()[::-1]]\n            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n            # matches = matches[matches[:, 2].argsort()[::-1]]\n            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n        matches = torch.Tensor(matches).to(iouv.device)\n        correct[matches[:, 1].long()] = matches[:, 2:3] >= iouv\n    return correct","30dc4aaf":"# ..\/input\/starfish-yolo\/best_fold1_f2_64017.pt\n@torch.no_grad()\ndef run(data = '\/kaggle\/working\/yolov5\/starfish_yolov5\/fold0.yaml',\n        weights='\/kaggle\/input\/reef-baseline-fold12\/l6_3600_uflip_vm5_f12_up\/f1\/best.pt',  # model.pt path(s)\n        batch_size=2,  # batch size\n        imgsz=9000,  # inference size (pixels)\n        conf_thres=0.001,  # confidence threshold\n        iou_thres=0.45,  # NMS IoU threshold\n        task='val',  # train, val, test, speed or study\n        device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        single_cls=False,  # treat as single-class dataset\n        augment=True,  # augmented inference\n        verbose=False,  # verbose output\n        save_txt=False,  # save results to *.txt\n        save_hybrid=False,  # save label+prediction hybrid results to *.txt\n        save_conf=False,  # save confidences in --save-txt labels\n        save_json=False,  # save a COCO-JSON results file\n        project=ROOT + 'runs\/val',  # save to project\/name\n        name='exp',  # save to project\/name\n        exist_ok=False,  # existing project\/name ok, do not increment\n        half=True,  # use FP16 half-precision inference\n        model=None,\n        dataloader=None,\n        save_dir=Path(''),\n        plots=True,\n        callbacks=Callbacks(),\n        compute_loss=None,\n        ):\n    # Initialize\/load model and set device\n    training = model is not None\n    if training:  # called by train.py\n        device = next(model.parameters()).device  # get model device\n\n    else:  # called directly\n        device = select_device(device, batch_size=batch_size)\n\n        # Directories\n        save_dir = increment_path(Path(project) \/ name, exist_ok=exist_ok)  # increment run\n        (save_dir \/ 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n        # Load model\n        check_suffix(weights, '.pt')\n        model = attempt_load(weights, map_location=device)  # load FP32 model\n        gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n        imgsz = check_img_size(imgsz, s=gs)  # check image size\n\n        # Multi-GPU disabled, incompatible with .half() https:\/\/github.com\/ultralytics\/yolov5\/issues\/99\n        # if device.type != 'cpu' and torch.cuda.device_count() > 1:\n        #     model = nn.DataParallel(model)\n\n        # Data\n        data = check_dataset(data)  # check\n\n    # Half\n    half &= device.type != 'cpu'  # half precision only supported on CUDA\n    model.half() if half else model.float()\n\n    # Configure\n    model.eval()\n    is_coco = isinstance(data.get('val'), str) and data['val'].endswith('coco\/val2017.txt')  # COCO dataset\n    nc = 1 if single_cls else int(data['nc'])  # number of classes\n    iouv = torch.linspace(0.3, 0.8, 11).to(device)  # iou vector for mAP@0.5:0.95\n    niou = iouv.numel()\n\n    # Dataloader\n    if not training:\n        if device.type != 'cpu':\n            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n        pad = 0.0 if task == 'speed' else 0.5\n        task = task if task in ('train', 'val', 'test') else 'val'  # path to train\/val\/test images\n        dataloader = create_dataloader(data[task], imgsz, batch_size, gs, single_cls, pad=pad, rect=True,\n                                       prefix=colorstr(f'{task}: '))[0]\n\n    seen = 0\n    confusion_matrix = ConfusionMatrix(nc=nc)\n    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n    class_map = coco80_to_coco91_class() if is_coco else list(range(1000))\n    s = ('%20s' + '%11s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n    dt, p, r, f1, mp, mr, map50, map = [0.0, 0.0, 0.0], 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n    loss = torch.zeros(3, device=device)\n    jdict, stats, ap, ap_class = [], [], [], []\n    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n        t1 = time_sync()\n        img = img.to(device, non_blocking=True)\n        img = img.half() if half else img.float()  # uint8 to fp16\/32\n        img \/= 255.0  # 0 - 255 to 0.0 - 1.0\n        targets = targets.to(device)\n        nb, _, height, width = img.shape  # batch size, channels, height, width\n        t2 = time_sync()\n        dt[0] += t2 - t1\n\n        # Run model\n        out, train_out = model(img, augment=augment)  # inference and training outputs\n        dt[1] += time_sync() - t2\n\n        # Compute loss\n        if compute_loss:\n            loss += compute_loss([x.float() for x in train_out], targets)[1]  # box, obj, cls\n\n        # Run NMS\n        targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n        lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n        t3 = time_sync()\n        out = non_max_suppression(out, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls)\n        dt[2] += time_sync() - t3\n\n        # Statistics per image\n        for si, pred in enumerate(out):\n            labels = targets[targets[:, 0] == si, 1:]\n            nl = len(labels)\n            tcls = labels[:, 0].tolist() if nl else []  # target class\n            path, shape = Path(paths[si]), shapes[si][0]\n            seen += 1\n\n            if len(pred) == 0:\n                if nl:\n                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n                continue\n\n            # Predictions\n            if single_cls:\n                pred[:, 5] = 0\n            predn = pred.clone()\n            scale_coords(img[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n\n            # Evaluate\n            if nl:\n                tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n                scale_coords(img[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n                labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n                correct = process_batch(predn, labelsn, iouv)\n                if plots:\n                    confusion_matrix.process_batch(predn, labelsn)\n            else:\n                correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool)\n            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))  # (correct, conf, pcls, tcls)\n\n            # Save\/log\n            if save_txt:\n                save_one_txt(predn, save_conf, shape, file=save_dir \/ 'labels' \/ (path.stem + '.txt'))\n            if save_json:\n                save_one_json(predn, jdict, path, class_map)  # append to COCO-JSON dictionary\n            callbacks.run('on_val_image_end', pred, predn, path, names, img[si])\n\n        # Plot images\n        if plots and batch_i < 3:\n            f = save_dir \/ f'val_batch{batch_i}_labels.jpg'  # labels\n            Thread(target=plot_images, args=(img, targets, paths, f, names), daemon=True).start()\n            f = save_dir \/ f'val_batch{batch_i}_pred.jpg'  # predictions\n            Thread(target=plot_images, args=(img, output_to_target(out), paths, f, names), daemon=True).start()\n\n    # Compute statistics\n    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n    if len(stats) and stats[0].any():\n        p, r, ap, f2, ap_class,conf_i = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n        ap50, ap = ap[:, 4], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n    else:\n        nt = torch.zeros(1)\n\n    # Print results\n    pf = '%20s' + '%11i' * 2 + '%11.3g' * 4  # print format\n    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n    print('conf:' + str(conf_i*0.001))\n    f2 = f2[0]\n    print('f2-score:'+str(f2))\n\n    # Print results per class\n    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n        for i, c in enumerate(ap_class):\n            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n\n    # Print speeds\n    t = tuple(x \/ seen * 1E3 for x in dt)  # speeds per image\n    if not training:\n        shape = (batch_size, 3, imgsz, imgsz)\n        print(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}' % t)\n\n    # Plots\n    if plots:\n        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n        callbacks.run('on_val_end')\n\n    # Save JSON\n    if save_json and len(jdict):\n        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n        anno_json = str(Path(data.get('path', '..\/coco')) \/ 'annotations\/instances_val2017.json')  # annotations json\n        pred_json = str(save_dir \/ f\"{w}_predictions.json\")  # predictions json\n        print(f'\\nEvaluating pycocotools mAP... saving {pred_json}...')\n        with open(pred_json, 'w') as f:\n            json.dump(jdict, f)\n\n        try:  # https:\/\/github.com\/cocodataset\/cocoapi\/blob\/master\/PythonAPI\/pycocoEvalDemo.ipynb\n            check_requirements(['pycocotools'])\n            from pycocotools.coco import COCO\n            from pycocotools.cocoeval import COCOeval\n\n            anno = COCO(anno_json)  # init annotations api\n            pred = anno.loadRes(pred_json)  # init predictions api\n            eval = COCOeval(anno, pred, 'bbox')\n            if is_coco:\n                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n            eval.evaluate()\n            eval.accumulate()\n            eval.summarize()\n            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n        except Exception as e:\n            print(f'pycocotools unable to run: {e}')\n\n    # Return results\n    model.float()  # for training\n    if not training:\n        s = f\"\\n{len(list(save_dir.glob('labels\/*.txt')))} labels saved to {save_dir \/ 'labels'}\" if save_txt else ''\n        print(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n    maps = np.zeros(nc) + map\n    for i, c in enumerate(ap_class):\n        maps[c] = ap[i]\n    return (mp, mr, map50, map, *(loss.cpu() \/ len(dataloader)).tolist()), maps, t,f2,conf_i","6116c0ca":"results, maps, _,f2,conf_i = run()","b51356ad":"'''mp, mr, map50, map0.3:0.8'''\nresults ","90231308":"best_conf = conf_i*0.001\nbest_conf ","8399c54e":"f2","533b0f93":"'''1.Change yolov5\/utils\/metrics.py  ,ap_per_class()'''","5b50501b":"# '''\n# def ap_per_class(tp, conf, pred_cls, target_cls, plot=False, save_dir='.', names=()):\n#     \"\"\" Compute the average precision, given the recall and precision curves.\n#     Source: https:\/\/github.com\/rafaelpadilla\/Object-Detection-Metrics.\n#     # Arguments\n#         tp:  True positives (nparray, nx1 or nx10).\n#         conf:  Objectness value from 0-1 (nparray).\n#         pred_cls:  Predicted object classes (nparray).\n#         target_cls:  True object classes (nparray).\n#         plot:  Plot precision-recall curve at mAP@0.5\n#         save_dir:  Plot save directory\n#     # Returns\n#         The average precision as computed in py-faster-rcnn.\n#     \"\"\"\n\n#     # Sort by objectness\n#     i = np.argsort(-conf)\n#     tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n#     # Find unique classes\n#     unique_classes = np.unique(target_cls)\n#     nc = unique_classes.shape[0]  # number of classes, number of detections\n\n#     # Create Precision-Recall curve and compute AP for each class\n#     px, py = np.linspace(0, 1, 1000), []  # for plotting\n#     ap, p, r = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n#     m_f2 = []\n#     for ci, c in enumerate(unique_classes):\n#         i = pred_cls == c\n#         n_l = (target_cls == c).sum()  # number of labels\n#         n_p = i.sum()  # number of predictions\n\n#         if n_p == 0 or n_l == 0:\n#             continue\n#         else:\n#             # Accumulate FPs and TPs\n#             fpc = (1 - tp[i]).cumsum(0)\n#             tpc = tp[i].cumsum(0)\n\n#             # Recall\n#             recall = tpc \/ (n_l + 1e-16)  # recall curve\n#             r[ci] = np.interp(-px, -conf[i], recall[:, 4], left=0)  # negative x, xp because xp decreases\n\n#             # Precision\n#             precision = tpc \/ (tpc + fpc)  # precision curve\n#             p[ci] = np.interp(-px, -conf[i], precision[:, 4], left=1)  # p at pr_score\n\n#             '''\u8ba1\u7b97mean-f2'''\n#             for j in range(tp.shape[1]):\n#                 '''\u904d\u5386iou 0.3~0.8\uff0c\u8ba1\u7b97\u5bf9\u5e94\u7684f2'''\n#                 # Recall\n#                 recall = tpc \/ (n_l + 1e-16)  # recall curve\n#                 current_p = np.interp(-px, -conf[i], precision[:, j], left=0)# negative x, xp because xp decreases\n\n#                 # Precision\n#                 precision = tpc \/ (tpc + fpc)  # precision curve\n#                 current_r = np.interp(-px, -conf[i], recall[:, j], left=1)\n\n#                 '''\u5bf9\u5e94\u5f53\u524diou\u4e0b\u7684f2'''\n#                 current_f2 = 5 * (current_p * current_r) \/ (4 * current_p + current_r + 1e-16)\n#                 m_f2.append(current_f2)\n\n#             '''\u6b64\u65f6m_f2\u5c3a\u5bf8\u4e3a[11,1000]'''\n#             m_f2 = np.array(m_f2)\n\n#             '''\u572811\u7684\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u53d6\u5e73\u5747\uff0c\u5f97\u5230F2-socre[0.3,0.8]'''\n#             m_f2=np.mean(m_f2, axis=0)\n#             '''\u53d8\u6362\u5f62\u72b6\uff0c\u65b9\u4fbf\u8ba1\u7b97\u6700\u5927\u503c'''\n#             m_f2=m_f2.reshape(1, 1000)\n\n#             # AP from recall-precision curve\n#             for j in range(tp.shape[1]):\n#                 ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n\n#                 if plot and j == 0:\n#                     py.append(np.interp(px, mrec, mpre))  # precision at mAP@0.5\n\n#     # Compute F1 (harmonic mean of precision and recall)\n#     f1 = 2 * p * r \/ (p + r + 1e-16)\n\n#     if plot:\n#         plot_pr_curve(px, py, ap, Path(save_dir) \/ 'PR_curve.png', names)\n#         plot_mc_curve(px, f1, Path(save_dir) \/ 'F1_curve.png', names, ylabel='F1')\n#         plot_mc_curve(px, m_f2, Path(save_dir) \/ 'meanF2_curve.png', names, ylabel='mF2')\n#         plot_mc_curve(px, p, Path(save_dir) \/ 'P_curve.png', names, ylabel='Precision')\n#         plot_mc_curve(px, r, Path(save_dir) \/ 'R_curve.png', names, ylabel='Recall')\n\n#     # i = f1.mean(0).argmax()  # max F1 index\n#     i = m_f2.mean(0).argmax()\n#     return p[:, i], r[:, i], ap, m_f2[:, i], unique_classes.astype('int32'),i\n# '''","d87605f7":"**copy from  yolov5\/val.py**"}}