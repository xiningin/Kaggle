{"cell_type":{"dffc0d2a":"code","a3c6dfe1":"code","b1ab5cc0":"code","f4cc6523":"code","e6a2d7bb":"code","e95efb96":"code","a4aea882":"code","a7bb5f34":"code","1da34014":"code","09a20aad":"code","b2d5aa57":"code","55c99583":"code","3b8ca922":"code","abc6f595":"code","5694b96a":"code","dcf11988":"code","427fa045":"code","458f22f4":"code","7fbfeb04":"code","82ba4584":"markdown","6585ac3a":"markdown","c22bd2a4":"markdown","7be61e66":"markdown","8a5dc192":"markdown","6d84cd85":"markdown","324b0134":"markdown","70eff1bb":"markdown"},"source":{"dffc0d2a":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport pylab\npylab.rcParams['figure.figsize'] = (8.0, 10.0)\nimport skimage.io as io\nimport os","a3c6dfe1":"!pip install -q pycocotools\nfrom pycocotools.coco import COCO","b1ab5cc0":"img_dir=('\/kaggle\/input\/coco2014\/train2014\/train2014\/')\nannotations_file='\/kaggle\/input\/coco2014\/captions\/annotations\/instances_train2014.json'","f4cc6523":"#Loading object detection annotations of 2017 validation set using COCO API\ncoco=COCO(annotations_file)","e6a2d7bb":"#validation set details\nimgIds = coco.getImgIds()\nprint(\"Total images: {}\".format(len(imgIds)))\nrand=np.random.randint(0,len(imgIds))\nimg = coco.loadImgs(imgIds[rand])[0]\nprint(\"Image example:\")\nprint(img)\nannIds=coco.getAnnIds()\nprint(\"\\nTotal annotations: {}\".format(len(annIds)))\nann=coco.loadAnns(coco.getAnnIds(imgIds=img['id']))\nprint(\"Annotation example:\")\nprint(ann[0])","e95efb96":"cats = coco.loadCats(coco.getCatIds())\nprint(\"Number of categories: {}\".format(len(cats)))\nnms=[cat['name'] for cat in cats]\nprint('\\nCOCO categories: \\n{}\\n'.format(' '.join(nms)))","a4aea882":"#Example\nI = io.imread(img['coco_url'])\nplt.imshow(I); plt.axis('off')\nannIds = coco.getAnnIds(imgIds=img['id'])\nanns = coco.loadAnns(annIds)\ncoco.showAnns(anns)","a7bb5f34":"#Create tfrecord output directories\n!mkdir '\/kaggle\/working\/COCO2014\/'\n!mkdir '\/kaggle\/working\/COCO2014\/train_tfrecords\/'\nrecords_path='\/kaggle\/working\/COCO2014\/train_tfrecords\/'","1da34014":"#Returns all the annotation data for a given image id\ndef get_annotations(imgId):\n    annIds=coco.getAnnIds(imgIds=imgId)\n    anns=coco.loadAnns(annIds)\n    segmentations=[]\n    segmentation_lengths=[]\n    bboxes=[]\n    catIds=[]\n    iscrowd_list=[]\n    area_list=[]\n    annotation_ids=[]\n    for ann in anns:\n        try:\n            catId=ann['category_id']\n            bbox=ann['bbox']\n            segmentation=ann['segmentation'][0]\n            iscrowd=ann['iscrowd']\n            area=ann['area']\n            annotation_id=ann['id']\n        except:\n            continue\n        if((not None in bbox) and (None!=catId)):\n            catIds.append(catId)\n            segmentations.append(segmentation)\n            segmentation_lengths.append(len(segmentation))\n            bboxes.append(bbox)\n            iscrowd_list.append(iscrowd)\n            area_list.append(area)\n            annotation_ids.append(annotation_id)\n    return len(anns),catIds,segmentation_lengths,sum(segmentations,[]),sum(bboxes,[]),iscrowd_list,area_list,annotation_ids","09a20aad":"#Size of each TFRecord file will be 100MB for improving performance\nn=len(imgIds)\nimgids=imgIds[0:n]\nsize=0\nfor i in imgids:\n    img=coco.loadImgs(i)\n    fn=img[0]['file_name']\n    size+=os.path.getsize(img_dir+fn)\navg_size=size\/n\nlimit=int(104857600\/\/avg_size)\ntotal_tfrecords=int(len(imgIds)\/\/limit)\nprint(\"{} TFRecord files will be created\".format(total_tfrecords))\n\nfor i in range(0,total_tfrecords):\n    examples=[]\n    start=i*limit\n    end=start+limit\n    imgids=imgIds[start:end]\n    \n    for img in coco.loadImgs(imgids):\n        with open(str(img_dir)+img['file_name'],'rb') as f:\n            image_string=f.read()\n\n        objects,catIds,segmentation_lengths,segmentations,bboxes,iscrowd,area,annotation_ids=get_annotations(img['id'])\n\n        # Create a Features message using tf.train.Example.\n        example = tf.train.Example(features=tf.train.Features(feature={\n            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_string])),\n            'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[img['height']])),\n            'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[img['width']])),\n            'id': tf.train.Feature(int64_list=tf.train.Int64List(value=[img['id']])),\n            'license': tf.train.Feature(int64_list=tf.train.Int64List(value=[img['license']])),\n            'file_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(img['file_name'])])),\n            'coco_url': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(img['coco_url'])])),\n            'flickr_url': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(img['flickr_url'])])),\n            'date_captured': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(img['date_captured'])])),\n            #objects-Number of objects in the image\n            'objects': tf.train.Feature(int64_list=tf.train.Int64List(value=[objects])),\n            #Follwing features hold all the annotations data given for the image\n            #category_ids-List of aannotation category ids\n            'category_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=catIds)),\n            #segmentation_lengths-List of segmentation lengths\n            'segmentation_lengths': tf.train.Feature(int64_list=tf.train.Int64List(value=segmentation_lengths)),\n            #segmention lists flattened into 1D list\n            'segmentations': tf.train.Feature(float_list=tf.train.FloatList(value=segmentations)),\n            #bboxes flattened into 1D list\n            'bboxes': tf.train.Feature(float_list=tf.train.FloatList(value=bboxes)),\n            #List of iscrowd values\n            'iscrowd': tf.train.Feature(int64_list=tf.train.Int64List(value=iscrowd)),\n            #List of area values\n            'area': tf.train.Feature(float_list=tf.train.FloatList(value=area)),\n            #List of annotation ids \n            'annotation_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=annotation_ids)),\n        }))\n        examples.append(example)\n    \n    with tf.io.TFRecordWriter(records_path+'coco'+str(i)+'.tfrecord') as writer:\n        for j in examples:\n            writer.write(j.SerializeToString())\n    examples.clear()\n    print(\"file {} created\".format(i))","b2d5aa57":"#zip the created tfrecord files. \n#!find records_path -type f -exec zip -g tfrecords.zip {} \\; -exec rm {} \\;","55c99583":"def parse(feature):\n    features = tf.io.parse_single_example(\n        feature,\n        features={\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'height': tf.io.FixedLenFeature([], tf.int64),\n        'width': tf.io.FixedLenFeature([], tf.int64),\n        'id': tf.io.FixedLenFeature([], tf.int64),\n        'license': tf.io.FixedLenFeature([], tf.int64),\n        'file_name': tf.io.FixedLenFeature([], tf.string),\n        'coco_url': tf.io.FixedLenFeature([], tf.string),\n        'flickr_url': tf.io.FixedLenFeature([], tf.string),\n        'date_captured': tf.io.FixedLenFeature([], tf.string),\n        'objects': tf.io.FixedLenFeature([], tf.int64),\n        'category_ids': tf.io.VarLenFeature(tf.int64),\n        'segmentation_lengths': tf.io.VarLenFeature(tf.int64),\n        'segmentations': tf.io.VarLenFeature(tf.float32),\n        'bboxes': tf.io.VarLenFeature(tf.float32),\n        'iscrowd': tf.io.VarLenFeature(tf.int64),\n        'area': tf.io.VarLenFeature(tf.float32),\n        'annotation_ids': tf.io.VarLenFeature(tf.int64),\n        \n    })\n\n    \n    print('Image id:')\n    print(features['id'])\n    print('\\nlicense:')\n    print(features['license'])\n    print('\\nfile_name:')\n    print(features['file_name'])\n    print('\\ncoco_url:')\n    print(features['coco_url'])\n    print('\\nflickr_url:')\n    print(features['flickr_url'])\n    print('\\ndate_captured:')\n    print(features['date_captured'])\n    print(\"\\nobjects:\")\n    print(features['objects'])\n    print(\"\\nheight:\")\n    print(features['height'])\n    print(\"\\nwidth:\")\n    print(features['width'])\n    print(\"\\ncategory ids:\")\n    print(features['category_ids'])\n    print(\"\\niscrowd:\")\n    print(features['iscrowd'])\n    print(\"\\narea:\")\n    print(features['area'])\n    print(\"\\nannotation_ids:\")\n    print(features['annotation_ids'])\n    \n    \n    \n    objects = features['objects']\n    bboxes = features['bboxes']\n    bboxes = tf.sparse.to_dense(bboxes)\n    bboxes = tf.reshape(bboxes, [objects, 4])\n    \n    print(\"\\nbboxes:\")\n    print(bboxes)\n    \n    print(\"\\nsegmentation lengths:\")\n    print(features['segmentation_lengths'])\n    \n    segmentations = features['segmentations']\n    segmentations = tf.sparse.to_dense(segmentations)\n    segmentation_lengths=tf.sparse.to_dense(features['segmentation_lengths'])\n    \n\n    segs=[]\n    start=0\n    for i in segmentation_lengths:\n        segs.append(tf.slice(segmentations,[start,],[i,]))\n        start+=i\n    print(\"\\nSegmentations:\")    \n    print(segs)\n    \n    image = tf.image.decode_jpeg(features['image'], channels=3)\n    plt.imshow(image); plt.axis('off')\n    \n    anns=[]\n    for i in range(0,len(segs)):\n        #plt.gca().add_patch(Rectangle((i[0],i[1]),i[2],i[3],linewidth=1,edgecolor='r',facecolor='none'))\n        ann={}\n        ann['segmentation']=[segs[i].numpy().tolist()]\n        ann['bbox']=bboxes[i].numpy().tolist()\n        anns.append(ann)\n    #print(anns)\n    coco.showAnns(anns,draw_bbox=True)\n    ","3b8ca922":"img_dir=('\/kaggle\/input\/coco2014\/val2014\/val2014\/')\nannotations_file='\/kaggle\/input\/coco2014\/captions\/annotations\/instances_val2014.json'","abc6f595":"coco=COCO(annotations_file)","5694b96a":"#validation set details\nimgIds = coco.getImgIds()\nprint(\"Total images: {}\".format(len(imgIds)))\nrand=np.random.randint(0,len(imgIds))\nimg = coco.loadImgs(imgIds[rand])[0]\nprint(\"Image example:\")\nprint(img)\nannIds=coco.getAnnIds()\nprint(\"\\nTotal annotations: {}\".format(len(annIds)))\nann=coco.loadAnns(coco.getAnnIds(imgIds=img['id']))\nprint(\"Annotation example:\")\nprint(ann[0])","dcf11988":"cats = coco.loadCats(coco.getCatIds())\nprint(\"Number of categories: {}\".format(len(cats)))\nnms=[cat['name'] for cat in cats]\nprint('\\nCOCO categories: \\n{}\\n'.format(' '.join(nms)))","427fa045":"#Create tfrecord output directories\n#!mkdir '\/kaggle\/working\/COCO2014\/'\n!mkdir '\/kaggle\/working\/COCO2014\/val_tfrecords\/'\nrecords_path='\/kaggle\/working\/COCO2014\/val_tfrecords\/'","458f22f4":"#Size of each TFRecord file will be 100MB for improving performance\nn=len(imgIds)\nimgids=imgIds[0:n]\nsize=0\nfor i in imgids:\n    img=coco.loadImgs(i)\n    fn=img[0]['file_name']\n    size+=os.path.getsize(img_dir+fn)\navg_size=size\/n\nlimit=int(104857600\/\/avg_size)\ntotal_tfrecords=int(len(imgIds)\/\/limit)\nprint(\"{} TFRecord files will be created\".format(total_tfrecords))\n\nfor i in range(0,total_tfrecords):\n    examples=[]\n    start=i*limit\n    end=start+limit\n    imgids=imgIds[start:end]\n    \n    for img in coco.loadImgs(imgids):\n        with open(str(img_dir)+img['file_name'],'rb') as f:\n            image_string=f.read()\n\n        objects,catIds,segmentation_lengths,segmentations,bboxes,iscrowd,area,annotation_ids=get_annotations(img['id'])\n\n        # Create a Features message using tf.train.Example.\n        example = tf.train.Example(features=tf.train.Features(feature={\n            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_string])),\n            'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[img['height']])),\n            'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[img['width']])),\n            'id': tf.train.Feature(int64_list=tf.train.Int64List(value=[img['id']])),\n            'license': tf.train.Feature(int64_list=tf.train.Int64List(value=[img['license']])),\n            'file_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(img['file_name'])])),\n            'coco_url': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(img['coco_url'])])),\n            'flickr_url': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(img['flickr_url'])])),\n            'date_captured': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(img['date_captured'])])),\n            #objects-Number of objects in the image\n            'objects': tf.train.Feature(int64_list=tf.train.Int64List(value=[objects])),\n            #Follwing features hold all the annotations data given for the image\n            #category_ids-List of aannotation category ids\n            'category_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=catIds)),\n            #segmentation_lengths-List of segmentation lengths\n            'segmentation_lengths': tf.train.Feature(int64_list=tf.train.Int64List(value=segmentation_lengths)),\n            #segmention lists flattened into 1D list\n            'segmentations': tf.train.Feature(float_list=tf.train.FloatList(value=segmentations)),\n            #bboxes flattened into 1D list\n            'bboxes': tf.train.Feature(float_list=tf.train.FloatList(value=bboxes)),\n            #List of iscrowd values\n            'iscrowd': tf.train.Feature(int64_list=tf.train.Int64List(value=iscrowd)),\n            #List of area values\n            'area': tf.train.Feature(float_list=tf.train.FloatList(value=area)),\n            #List of annotation ids \n            'annotation_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=annotation_ids)),\n        }))\n        examples.append(example)\n    \n    with tf.io.TFRecordWriter(records_path+'coco'+str(i)+'.tfrecord') as writer:\n        for j in examples:\n            writer.write(j.SerializeToString())\n    examples.clear()\n    print(\"file {} created\".format(i))","7fbfeb04":"! ls","82ba4584":"Each COCO object detection annotation have a category id.  The categories field of the annotation structure stores the mapping of category id to category and supercategory names. \n\n`categories[{\n\"id\": int,\n\"name\": str,\n\"supercategory\": str,\n}]`","6585ac3a":"The Tensor Processing Unit (TPU) hardware accelerators  are very fast. The challenge is often to feed them data fast enough to keep them busy. Google Cloud Storage (GCS) is capable of sustaining very high throughput but as with all cloud storage systems, initiating a connection costs some network back and forth. Therefore, having our data stored as thousands of individual files is not ideal. We are going to batch the **COCO dataset with object detection annotations** in a smaller number of files and use the power of tf.data.Dataset to read from multiple files in parallel. \n\n**The TFRecord file format**<br>\nTensorflow's preferred file format for storing data is the protobuf-based TFRecord format. Other serialization formats would work too but you can load a dataset from TFRecord files directly by writing:\n\n`\nfilenames = tf.io.gfile.glob(FILENAME_PATTERN)\ndataset = tf.data.TFRecordDataset(filenames)\ndataset = dataset.map(...)\n`\n\nFor more details https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-data\/","c22bd2a4":"tf.io.parse_single_example is used to parse a example in tfrecord dataset. Number of objects in the image is not same for all images. So annotation data should be retrieved using VarLenFeature. VarLenFeature returns a sparse tensor.\n\nAll the Segmentation and bbox data for a image are flattened into a 1D array when storing the data in tfrecord files. For example,<br>\ntwo bboxes of a image [100,70,70,80] and [120,88,200,300] are flattened into [100,70,70,80,120,88,200,300] . \ntwo segmentations of a image [23,45,67,89,45,43,23,12] and [78,123,156] are flattened into [23,45,67,89,45,43,23,12,78,123,156].\n\nobjects and segmentation_lengths data are used to retrieve the bbox and segmentation data.","7be61e66":"**COCO (Common Objects in COntext)** is a popular dataset in Computer Vision. It contains annotations for Computer Vision tasks - object detection, segmentation, keypoint detection, stuff segmentation, panoptic segmentation, densepose, and image captioning. For more details visit https:\/\/cocodataset.org\/#format-data","8a5dc192":"Annotations are stored using JSON. COCO API can be used to access and manipulate all anotations.","6d84cd85":"Details of each image in COCO dataset are stored in following format. \n\n`image{\n\"id\": int, \"width\": int, \"height\": int, \"file_name\": str, \"license\": int, \"flickr_url\": str, \"coco_url\": str, \"date_captured\": datetime,\n}`\n\nA image may have multiple annotaions.\n\nEach object instance annotation contains a series of fields, including the category id and segmentation mask of the object. The segmentation format depends on whether the instance represents a single object (iscrowd=0 in which case polygons are used) or a collection of objects (iscrowd=1 in which case RLE is used). Note that a single object (iscrowd=0) may require multiple polygons, for example if occluded. Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people). In addition, an enclosing bounding box is provided for each object (box coordinates are measured from the top left image corner and are 0-indexed).\n\nannotation is mapped to image by using image_id.\n\n`annotation{\n\"id\": int, \"image_id\": int, \"category_id\": int, \"segmentation\": RLE or [polygon], \"area\": float, \"bbox\": [x,y,width,height], \"iscrowd\": 0 or 1,\n}`\n\nVisit https:\/\/github.com\/cocodataset\/cocoapi\/blob\/master\/PythonAPI\/pycocoDemo.ipynb for COCO API demo.","324b0134":"This notebook use validation set due to the 5GB size limit of \/kaggle\/working folder. training set is around 20 GB in size. You can use training set by changing follwing paths to training set paths.","70eff1bb":"The tf.Example message (or protobuf) is a flexible message type that represents a {\"string\": value} mapping. In TFRecord files, all the available data for a image will be stored using tf.Example. For more details, visit https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord#tfexample\n\nget_annotations function returns all the annotation data for a given image id. \n\nOne tf.Example will be created with image data and annotation data for each image. These examples will be written into multiple tfrecord files. \n\nThe number of tfrecord files to be created is based on image size. The rule of thumb is to split your data across several (10s to 100s) larg-ish files (10s to 100s of MB). If you have too many files, thousands of files for example, the time to access each file might start getting in the way. If you have too few files, like one or two, then you are not getting the benefits of streaming from multiple files in parallel."}}