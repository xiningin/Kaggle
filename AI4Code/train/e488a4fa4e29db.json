{"cell_type":{"338da034":"code","96474cd1":"code","5061a056":"code","34565bc3":"code","9a0b4b33":"code","d93efb0c":"code","e1d9e7da":"code","e3044dd0":"code","cbe5ba35":"code","c770e671":"code","7cf01055":"code","962485c3":"code","71e0fbe5":"code","9c019773":"code","fa1bc172":"code","ceca2d6f":"code","51b42ca6":"code","5134e740":"code","ea2c74b1":"code","b0aa2ab0":"code","a5754754":"code","577b08f7":"code","94ba7eb4":"code","b3518b83":"code","1bc69786":"code","f84e14f0":"code","9cfbb24f":"code","7fa0fcae":"code","6a2e2c68":"markdown","b417fe95":"markdown","74b17cb1":"markdown","fb05f1eb":"markdown","5588923f":"markdown","8556f993":"markdown","4d31f592":"markdown","7240fe4a":"markdown","094a2253":"markdown","108cb060":"markdown"},"source":{"338da034":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as patches\nimport os\nfrom PIL import Image\nimport cv2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","96474cd1":"!pip install xmltodict #installing the library to read XMl files\nimport xmltodict","5061a056":"imagenames=[] #list of imagefile names\nxmlnames=[] #list of xmlfile names\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if os.path.join(dirname, filename)[-3:]!=\"xml\":\n            imagenames.append(filename)\n        else:\n            xmlnames.append(filename)","34565bc3":"path_im=\"\/kaggle\/input\/face-mask-detection\/images\/\"  #path for an image folder \npath_an=\"\/kaggle\/input\/face-mask-detection\/annotations\/\" #path for xmlfiles folder","9a0b4b33":"with open(path_an+xmlnames[0]) as f:\n    print(xmltodict.parse(f.read()))","d93efb0c":"## Code for finding the total no of labels in our dataset\n\nlisting=[]\nfor i in imagenames[:]:\n    with open(path_an+i[:-4]+\".xml\") as fd:\n        doc=xmltodict.parse(fd.read())\n    temp=doc[\"annotation\"][\"object\"]\n    if type(temp)==list:\n        for i in range(len(temp)):\n            listing.append(temp[i][\"name\"])\n    else:\n        listing.append(temp[\"name\"])\n\nfor i in  set(listing):\n    print(i)","e1d9e7da":"options={\"with_mask\":0,\"without_mask\":1,\"mask_weared_incorrect\":2} # mapping for predictions and analysis purpose","e3044dd0":"def draw_bounding_box(input_image): #function to visualize images\n    with open(path_an+input_image[:-4]+\".xml\") as fd:\n        doc=xmltodict.parse(fd.read())\n    image=plt.imread(os.path.join(path_im+input_image))\n    fig,ax=plt.subplots(1)\n    ax.axis(\"off\")\n    fig.set_size_inches(10,5)\n    temp=doc[\"annotation\"][\"object\"]\n    if type(temp)==list:\n        for i in range(len(temp)):\n            if temp[i][\"name\"]==\"with_mask\":\n                a,b,c,d=list(map(int,temp[i][\"bndbox\"].values()))\n                patch=patches.Rectangle((a,b),c-a,d-b,linewidth=1, edgecolor='g',facecolor=\"none\",)\n                ax.add_patch(patch)\n            if temp[i][\"name\"]==\"without_mask\":\n                a,b,c,d=list(map(int,temp[i][\"bndbox\"].values()))     \n                patch=patches.Rectangle((a,b),c-a,d-b,linewidth=1, edgecolor='r',facecolor=\"none\",)\n                ax.add_patch(patch)\n            if temp[i][\"name\"]==\"mask_weared_incorrect\":\n                a,b,c,d=list(map(int,temp[i][\"bndbox\"].values()))\n                patch=patches.Rectangle((a,b),c-a,d-b,linewidth=1, edgecolor='y',facecolor=\"none\",)\n                ax.add_patch(patch)\n    else:\n        a,b,c,d=list(map(int,temp[\"bndbox\"].values()))\n        edgecolor={\"with_mask\":\"g\",\"without_mask\":\"g\",\"mask_weared_incorrect\":\"y\"}\n        patch=patches.Rectangle((a,b),d-b,c-a,linewidth=1, edgecolor=edgecolor[temp[\"name\"]],facecolor=\"none\",)\n    ax.imshow(image)\n    ax.add_patch(patch)","cbe5ba35":"for i in range(0,5):\n    draw_bounding_box(imagenames[i])","c770e671":"def make_dataset(no_of_images): #function to make dataset\n    image_tensor=[]\n    label_tensor=[]\n    for i,j in enumerate(no_of_images):\n        with open(path_an+j[:-4]+\".xml\") as fd:\n            doc=xmltodict.parse(fd.read())\n        if type(doc[\"annotation\"][\"object\"])!=list:\n            temp=doc[\"annotation\"][\"object\"]\n            a,b,c,d=list(map(int,temp[\"bndbox\"].values()))\n            label=options[temp[\"name\"]]\n            image=transforms.functional.crop(Image.open(path_im+j).convert(\"RGB\"), b,a,d-b,c-a)\n            image_tensor.append(my_transform(image))\n            label_tensor.append(torch.tensor(label))\n        else:\n            temp=doc[\"annotation\"][\"object\"]\n            for k in range(len(temp)):\n                a,b,c,d=list(map(int,temp[k][\"bndbox\"].values()))\n                label=options[temp[k][\"name\"]]\n                image=transforms.functional.crop(Image.open(path_im+j).convert(\"RGB\"), b,a,d-b,c-a)\n                image_tensor.append(my_transform(image))\n                label_tensor.append(torch.tensor(label))\n                \n    final_dataset=[[k,l] for k,l in zip(image_tensor,label_tensor)]\n    return tuple(final_dataset)","7cf01055":"#importing neccessary libraries for deeplearning task..\nimport torch\nfrom torchvision import datasets,transforms,models\nfrom torch.utils.data import Dataset,DataLoader\n\nmy_transform=transforms.Compose([transforms.Resize((226,226)),\n                                 transforms.ToTensor()])\n\ndataset=make_dataset(imagenames) #making a datset\ntrain_size=int(len(dataset)*0.8)\ntest_size=len(dataset)-train_size\nbatch_size=32\ntrainset,testset=torch.utils.data.random_split(dataset,[train_size,test_size])\ntrain_loader =DataLoader(dataset=trainset,batch_size=batch_size,shuffle=True)\ntest_loader =DataLoader(dataset=testset,batch_size=batch_size,shuffle=True)","962485c3":"dataiter=iter(train_loader) \nimages,labels=dataiter.next()\nimages=images.numpy()\n\nfig=plt.figure(figsize=(25,4))\nfor idx in np.arange(20):\n    ax=fig.add_subplot(2,20\/2,idx+1,xticks=[],yticks=[])\n    plt.imshow(np.transpose(images[idx],(1,2,0)))","71e0fbe5":"# We Will use pretrained resnet34 layer model.\nresnet=models.resnet34(pretrained=True)","9c019773":"for param in resnet.parameters():\n    param.requires_grad=False","fa1bc172":"import torch.nn as nn\nn_inputs=resnet.fc.in_features\nlast_layer=nn.Linear(n_inputs,3)\nresnet.fc.out_features=last_layer\n\nif torch.cuda.is_available():\n    resnet.cuda()\n    \nprint(resnet.fc.out_features)","ceca2d6f":"if torch.cuda.is_available(): #checking for GPU availability\n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","51b42ca6":"for paramet in resnet.parameters():\n    paramet.requires_grad=True","5134e740":"import torch.optim as optim\n\ncriterion=nn.CrossEntropyLoss()\n\noptimizer=optim.SGD(resnet.parameters(),lr=0.001)","ea2c74b1":"n_epochs=3\n\nfor epoch in range(1,n_epochs+1):\n    train_loss = 0.0\n\n\n  ########################  TRAIN THE MODEL #################\n    for batch,(data,target) in enumerate(train_loader):\n    \n    \n        if torch.cuda.is_available():\n            data , target = data.cuda(), target.cuda()\n    \n        optimizer.zero_grad()\n        output=resnet(data)\n        loss=criterion(output,target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n        if batch%20==19:\n            print(\"Epoch {}, batch {}, training loss {}\".format(epoch, batch+1,train_loss\/20))\n        train_loss = 0.0","b0aa2ab0":"#########Testing##########\ntest_loss=0.0\nacc=0\nresnet.eval()\n\nfor data,target in test_loader:\n    if torch.cuda.is_available():\n        data,target=data.cuda(),target.cuda()\n    output=resnet(data)\n    loss=criterion(output,target)\n    test_loss+=loss.item()\n    _,pred=torch.max(output,1)\n    predicted=pred.numpy()[:,np.newaxis] if not torch.cuda.is_available() else pred.cpu().numpy()[:,np.newaxis]\n    actual=target.numpy()[:,np.newaxis] if not torch.cuda.is_available() else target.cpu().numpy()[:,np.newaxis]\n    acc+=np.sum(predicted==actual)\/len(target.cpu().numpy())\n\nAverage_loss=test_loss\/len(test_loader)\nAverage_acc=acc\/len(test_loader)\n\nprint(\"Avg total loss is {:.6f}\".format(Average_loss))\nprint(\"Avg accuracy is {:.6f}\".format(Average_acc))\n","a5754754":"torch.save(resnet,open(\"resnet_model_face_mask\",\"wb\")) # saving the trained model.","577b08f7":"device = torch.device(\"cuda\")\nmodel=torch.load(open(\"\/kaggle\/working\/resnet_model_face_mask\",\"rb\"),map_location=device) #loading the model","94ba7eb4":"!pip install mtcnn #installing library for predicting faces","b3518b83":"from mtcnn import MTCNN\ndetect=MTCNN()","1bc69786":"def trans(bndbox,newimage):\n    a,b,c,d=bndbox[\"box\"]\n    image_crop=transforms.functional.crop(newimage, b,a,d-b,c-a)\n    my_transform=transforms.Compose([transforms.Resize((226,226)),\n                                     transforms.RandomCrop((224,224)),\n                                     transforms.ToTensor()])(image_crop)\n    return my_transform","f84e14f0":"def tag_plot(bndbox,filepath,predicted):\n    configut=[\"with_mask\",\"without_mask\",\"mask_weared_incorrect\"]\n    x=plt.imread(filepath)\n    fig,ax=plt.subplots(1)\n    ax.axis(\"off\")\n    fig.set_size_inches(15,10)\n    for i,j in zip(bndbox,predicted):\n        a,b,c,d=i[\"box\"]\n        patch=patches.Rectangle((a,b),c,d,linewidth=1, edgecolor='r',facecolor=\"none\",)\n        ax.imshow(x)\n        ax.text(a, b, configut[predicted[0]], size=10,\n                style='italic',verticalalignment=\"bottom\", horizontalalignment=\"left\",color=\"blue\")\n        ax.add_patch(patch)","9cfbb24f":"model=model.eval()\ndef testing(filepath):\n    configut=[\"with_mask\",\"without_mask\",\"mask_weared_incorrect\"]\n    img = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n    newimage=Image.open(filepath).convert(\"RGB\")\n    bndbox=detect.detect_faces(img)\n    if len(bndbox)==1:\n        image_pred=trans(bndbox[0],newimage).unsqueeze(0)\n        _, pred=torch.max(model(image_pred.to(device)),1)\n        tag_plot(bndbox,filepath,predicted=pred)\n    else:\n        predicted=[]\n        for i in bndbox:\n            image_pred=trans(i,newimage).unsqueeze(0)\n            _, pred=torch.max(model(image_pred.to(device)),1)\n            predicted.append(pred)\n        tag_plot(bndbox,filepath,predicted)","7fa0fcae":"testing(path_im+imagenames[118]) # if you have images you can test them using this function..","6a2e2c68":"The dataset contains image files and XML files.\nWe will check what's actually there in the image and XML file has.","b417fe95":"### Thank you","74b17cb1":"here you can see Xml file has properties of an image file including face locations and class it belong to..","fb05f1eb":"## Testing","5588923f":"Now let's visualize some of the images.","8556f993":"We will start by importing necessary libraries","4d31f592":"Now lets see how the images in the dataset looks like..","7240fe4a":"## Training the model","094a2253":"Many Kagglers did very good job in this dataset,Since this is my first project in Kaggle so there are at many places codes need optimization.\nAny suugestions\/guidances in the code are most welcome.Feel free to comment.","108cb060":"## Now we will load the saved model and will see how it works."}}