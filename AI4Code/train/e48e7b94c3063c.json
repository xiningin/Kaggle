{"cell_type":{"74b1210e":"code","00326bd3":"code","692c3923":"code","b53acb8b":"code","3730544a":"code","d958b8f1":"code","7dcf50a8":"code","6f535048":"code","44abd0f8":"code","3e26c3db":"code","e4624d3a":"code","2b228e56":"code","af14900f":"code","7e88e1d8":"code","e0ba2d00":"code","e22b44c6":"code","742a9c47":"code","33ea33f9":"code","35ad0feb":"code","c28ec75c":"markdown","5f7b7376":"markdown","019fac2d":"markdown","661abe9f":"markdown","d708318b":"markdown","fd85ac55":"markdown","6e512441":"markdown","b1fac7d8":"markdown","afcb31fa":"markdown","2ed36ee6":"markdown"},"source":{"74b1210e":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","00326bd3":"dataset=pd.read_csv('\/kaggle\/input\/cricket-player-info\/cricket_bowler_information.csv')","692c3923":"#checking for missing values\ndataset.isnull().sum()","b53acb8b":"#bowlingStyle contains categorical values, so it is best to fill the missing values with mode\ndataset['bowlingStyle'].fillna(dataset['bowlingStyle'].mode()[0], inplace=True)\n\n\n#rest of the missing values are replaced by medians of the respective columns\ndef fmedian(df,col):\n    median_value=df[col].median()\n    df[col].fillna(median_value, inplace=True)\n    \nfmedian(dataset,'consistency')\nfmedian(dataset,'Average_Career')\nfmedian(dataset,'Strike_rate_Career')\nfmedian(dataset,'form')\nfmedian(dataset,'Average_Yearly')\nfmedian(dataset,'Strike_rate_Yearly')\nfmedian(dataset,'opposition')\nfmedian(dataset,'Average_opposition')\nfmedian(dataset,'Strike_rate_opposition')\nfmedian(dataset,'Strike_rate_venue')\nfmedian(dataset,'venue')\nfmedian(dataset,'Average_venue')","3730544a":"dataset.dtypes","d958b8f1":"from sklearn.preprocessing import LabelEncoder\n\ndef encode(df,col):\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col])\n\nencode(dataset,'Innings Player')\nencode(dataset,'Ground')\nencode(dataset,'Country')\nencode(dataset,'Opposition')\nencode(dataset,'Day')\nencode(dataset,'bowlingStyle')","7dcf50a8":"dataset.FF","6f535048":"from matplotlib import pyplot as plt\nimport seaborn as sns\ncorr = dataset.corr()\nfig, ax = plt.subplots(figsize=(30, 18))\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\ndropSelf = np.zeros_like(corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=colormap, linewidths=.5, annot=True, fmt=\".2f\", mask=dropSelf)\nplt.title('Wickets - Features Correlations')\nplt.show()","44abd0f8":"#Balls bowled and Overs Bowled are perfectly corelated, so it is necessary to remove one of them\ndataset.drop('Overs_Bowled',axis=1,inplace=True)","3e26c3db":"target=dataset['Wickets_Taken']\ntrain=dataset.drop('Wickets_Taken', axis=1)","e4624d3a":"target.value_counts()","2b228e56":"#applying SMOTE\nfrom imblearn.combine  import SMOTETomek\nsmk=SMOTETomek(random_state=42)\ntrain_new,target_new=smk.fit_sample(train,target)","af14900f":"#splitting dataset into 80% train and 20% test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_new, target_new, test_size = 0.20, random_state = 0)","7e88e1d8":"from sklearn.naive_bayes import GaussianNB\nnb_model=GaussianNB()\nnb_model.fit(X_train,y_train)\ny_pred=nb_model.predict(X_test)","e0ba2d00":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\naccuracy=accuracy_score(y_test,y_pred) \nprecision=precision_score(y_test,y_pred,average='weighted')\nrecall=recall_score(y_test,y_pred,average='weighted')\nf1=f1_score(y_test,y_pred,average='weighted')\n\nprint('Accuracy - {}'.format(accuracy))\nprint('Precision - {}'.format(precision))\nprint('Recall - {}'.format(recall))\nprint('F1 - {}'.format(f1))","e22b44c6":"from sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier(criterion='gini', splitter='best',\n                             max_depth=16, min_samples_split=2,\n                             min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n                             max_features=None, random_state=None,\n                             max_leaf_nodes=None, min_impurity_decrease=0.0, \n                             min_impurity_split=None, class_weight=None, \n                             presort='deprecated', ccp_alpha=0.0)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)","742a9c47":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\naccuracy=accuracy_score(y_test,y_pred) \nprecision=precision_score(y_test,y_pred,average='weighted')\nrecall=recall_score(y_test,y_pred,average='weighted')\nf1=f1_score(y_test,y_pred,average='weighted')\n\nprint('Accuracy - {}'.format(accuracy))\nprint('Precision - {}'.format(precision))\nprint('Recall - {}'.format(recall))\nprint('F1 - {}'.format(f1))","33ea33f9":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(n_estimators=50)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)","35ad0feb":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\naccuracy=accuracy_score(y_test,y_pred) \nprecision=precision_score(y_test,y_pred,average='weighted')\nrecall=recall_score(y_test,y_pred,average='weighted')\nf1=f1_score(y_test,y_pred,average='weighted')\n\nprint('Accuracy - {}'.format(accuracy))\nprint('Precision - {}'.format(precision))\nprint('Recall - {}'.format(recall))\nprint('F1 - {}'.format(f1))","c28ec75c":"# Conclusion\nWith an accuracy of 0.99, Random Forest has been performing the best for our dataset while Naive Bayes has been performing the worst with an accuracy of 0.68.","5f7b7376":"### Decision Trees","019fac2d":"## Checking Correlation\nWe will be checking correlations between the variables to better understand the data.","661abe9f":"## Class Imbalance\nWe observed that majority of the records fall within class 1 in bowling. This created a major imbalance in the distribution of values and affected the performance of the learning algorithms. To solve this problem, we applied an oversampling technique Supervised Minority Oversampling Technique (SMOTE) on minority classes to make all the classes equally distributed.","d708318b":"## Label Encoding\nHere, we are label encoding all the categorical variables using Label Encoder from sklearn library.","fd85ac55":"### Random Forest ","6e512441":"# Model Implementation\n1. Naive Bayes\n2. Decision Trees\n3. Random Forest","b1fac7d8":"# Introduction\nIn this notebook, we continue with ODI performance of international bowlers dataset. In this we will be completing the following tasks:\n\n### 1. Data Pre-processing\n* Removing unnecessary features\n* Feature Engineering\n* Encoding categorical features\n* Filling missing values\n\n### 2. Model implementations\n* Implementing supervised machine learning models\n* Calculating Accuracy score, Precision score, F1-score and Recall score of the implemented models","afcb31fa":"### Naive Baye's ","2ed36ee6":"# Data Pre-processing"}}