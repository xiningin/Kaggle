{"cell_type":{"3b1c4f60":"code","1daed2fc":"code","667cfb57":"code","1b4251c3":"code","f4e24b73":"code","7fceb6c7":"code","1856f00e":"code","c0fd25f7":"code","79a2a224":"code","c5395ff5":"code","f6741999":"code","c53d1568":"code","c2dae349":"code","23a1cea5":"code","a2ce077d":"code","7a63e5b0":"code","e5111188":"code","f804b418":"markdown","fe0d2ac8":"markdown","cee7e6dd":"markdown","f2ea031a":"markdown","353f515b":"markdown","f9b36490":"markdown","67fc7a43":"markdown"},"source":{"3b1c4f60":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ntrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nall_data = pd.concat([test_df, train_df])","1daed2fc":"def plot_nans(data):\n    ax = (data.isna().sum() \/ data.shape[0]).loc[data.isna().sum() \/ data.shape[0] > 0].sort_values().drop(\"SalePrice\")[-10:].plot.barh(title='Missing data')\n    ax.set_ylabel(\"Feature\")\n    ax.set_xlabel(\"NaN values proportion\")\n    plt.grid(False)\n    plt.show()\n\nplot_nans(all_data)","667cfb57":"# --- MEANS ----\nall_data['LotFrontage'] = all_data['LotFrontage'].fillna(all_data['LotFrontage'].median())\n\n# --- ZEROs ----\n# No garage -> 0\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\n\nall_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].fillna(0)\nall_data['BsmtFinSF2'] = all_data['BsmtFinSF2'].fillna(0)\nall_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(0)\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0)\nall_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(0)\nall_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].fillna(0)\n\nall_data['GarageCars'] = all_data['GarageCars'].fillna(0)\nall_data['GarageArea'] = all_data['GarageArea'].fillna(0)","1b4251c3":"# Change some int variables to categorical as they are that\nall_data['MoSold'] = all_data['MoSold'].astype('category')\n\n# One-hot encoding\nall_data = pd.get_dummies(all_data)\n\n# Split back to train and test dataframes\nprint(\"Data shape\", all_data.shape)\ntrain_df = all_data.loc[all_data['SalePrice'].isnull() == False]\ntest_df = all_data.loc[all_data['SalePrice'].isnull() == True].drop('SalePrice', axis=1)\nprint(test_df.isna().sum().loc[test_df.isna().sum() > 0])","f4e24b73":"train_df.drop('Id', axis=1).describe()","7fceb6c7":"# Highest Pearson correlations\ntop20 = pd.concat([train_df.corr()['SalePrice'].sort_values(ascending=False)[1:10], train_df.corr()['SalePrice'].sort_values(ascending=False)[-10:]])\nax = top20.plot.bar()\nax.set_ylim(-1.0, 1.0)\nplt.title(\"Correlations to SalePrice - Top 10 Positive & Negative\")\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Correlation Coefficient\")\nplt.grid(False)\nplt.show()","1856f00e":"train_df.plot.scatter(x='GrLivArea', y='SalePrice', title=\"SalePrice by GrLivArea\", marker='x')\ntrain_df.plot.scatter(x='TotalBsmtSF', y='SalePrice', title=\"SalePrice by TotalBsmtSF\", marker='x')\ntrain_df.plot.scatter(x='GarageArea', y='SalePrice', title=\"SalePrice by GarageArea\", marker='x')\nplt.show()","c0fd25f7":"train_df.boxplot(column='SalePrice', by='OverallQual', grid=False)\nplt.title(\"OverallQual effect on SalePrice\")\nplt.ylabel(\"SalePrice ($)\")\nplt.xlabel(\"OverallQual (1-10)\")\nplt.suptitle('')\nplt.show()\n\ntrain_df.boxplot(column='SalePrice', by='FullBath', grid=False)\nplt.title(\"FullBath effect on SalePrice\")\nplt.ylabel(\"SalePrice ($)\")\nplt.suptitle('')\nplt.show()","79a2a224":"# Train-test split\nfrom sklearn.model_selection import train_test_split\nX = train_df.drop(['Id', 'SalePrice'], axis=1).values\n\n# Square root of the output to make high values less dominant\ny = np.sqrt(train_df['SalePrice'].values)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","c5395ff5":"from sklearn.ensemble import IsolationForest\nfrom scipy import stats\ndef remove_outliers(X_tr, y_tr, mode):\n    if mode == \"isolation\":\n        iso = IsolationForest(contamination=0.1)\n        yhat = iso.fit_predict(X_tr)\n        mask = (yhat != -1)\n    elif mode == 'z-score':\n        z = np.nan_to_num(np.abs(stats.zscore(X_tr)))\n        mask = (z < 15).all(axis=1)\n\n    org_shape = X_tr.shape\n    X_tr, y_tr = X_tr[mask, :], y_tr[mask]\n    print(\"Removed\", (~mask).sum(), \"of\", org_shape[0], \"samples\")\n    \n    \n    return X_tr, y_tr\nX_train, y_train = remove_outliers(X_train, y_train, \"isolation\")","f6741999":"# ML Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import VarianceThreshold\n\ncoefs = []\nrmses = []\nalphas = []\nk_variables = 'all'\nalfa = 2\nwhile alfa >= 0:\n    pipe = Pipeline([('scaler', StandardScaler()), \n                     ('zero variance removal', VarianceThreshold()), \n                     ('feature_selection', SelectKBest(f_regression, k=k_variables)), \n                     ('regressor', Lasso(alpha=alfa, max_iter=100000))])\n    \n    pipe.fit(X_train, y_train)\n    coefs.append(pipe['regressor'].coef_)\n    rmses.append(mean_squared_error(np.log(y_test**2), np.log(pipe.predict(X_test)**2), squared=False))\n    alphas.append(alfa)\n    alfa -= 0.05\n    \ncoefs = np.array(coefs)\nbest_alpha = alphas[np.argmin(rmses)]","c53d1568":"used_features = train_df.drop(['Id', 'SalePrice'], axis=1).columns.values[pipe['zero variance removal'].get_support()][pipe['feature_selection'].get_support()]\ncoef_inds = (-np.abs(coefs)).mean(axis=0).argsort()\ncoef_inds = np.expand_dims(coef_inds, axis=0).repeat(coefs.shape[0], axis=0)\nsorted_coefs = np.take_along_axis(coefs, coef_inds, axis=1)\nsorted_used_features = used_features[(-np.abs(coefs)).mean(axis=0).argsort()]","c2dae349":"show_n_best = 12\nplt.figure(figsize=(8,5))\nplt.plot(alphas, sorted_coefs[:,:show_n_best], '-o')\nplt.title('Coefficients')\nplt.xlabel('Alpha')\nplt.ylabel(\"Coefficient\")\nplt.legend(sorted_used_features[:show_n_best], loc=(1.05,0))\nplt.grid(False)\nplt.show()","23a1cea5":"plt.plot(alphas, rmses, '-o')\nplt.title('Logarithmic Root Mean Squared Error')\nplt.xlabel('Alpha')\nplt.ylabel(\"RMSE (log)\")\nplt.grid(False)\nplt.show()","a2ce077d":"# Residuals for best alpha\npipe = Pipeline([('scaler', StandardScaler()), \n                 ('zero variance removal', VarianceThreshold()), \n                 ('feature_selection', SelectKBest(f_regression, k=k_variables)), \n                 ('regressor', Lasso(alpha=best_alpha, max_iter=100000))])\npipe.fit(X_train, y_train)\n\nresiduals = np.log(y_test**2) - np.log(pipe.predict(X_test)**2)\narr1inds = y_test.argsort()\ny_test_sorted = (y_test**2)[arr1inds[::-1]]\nresiduals_sorted = residuals[arr1inds[::-1]]\nplt.plot(y_test_sorted, residuals_sorted, '*')\nplt.axhline(y=0, color='black', linestyle='--')\nplt.xlabel(\"True value\")\nplt.ylabel(\"Residual\")\nplt.ylim(-1.1,1.1)\nplt.title(\"Residual Plot\")\nplt.grid(False)\nplt.show()","7a63e5b0":"\n# Outlier removal for all train data\nX, y = remove_outliers(X, y, 'isolation')\n\npipe = Pipeline([('scaler', StandardScaler()), \n                     ('zero variance removal', VarianceThreshold()), \n                     ('feature_selection', SelectKBest(f_regression, k=k_variables)), \n                     ('regressor', Lasso(alpha=best_alpha, max_iter=100000))])\npipe.fit(X, y)","e5111188":"# Validation for Kaggle competition\nX_valid = test_df.drop('Id', axis=1).values\n\ny_pred_valid = pipe.predict(X_valid)**2\nfinal_submission = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"SalePrice\": y_pred_valid})\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nfinal_submission.head()","f804b418":"## Outlier removal","fe0d2ac8":"## Exploratory Analysis","cee7e6dd":"# Missing Data Handling, Visualization & Lasso","f2ea031a":"## Descriptive Analysis","353f515b":"## Final Model Fitting","f9b36490":"## Train-test split","67fc7a43":"## NaN Handling\n\nLet's find out first which columns have the most NaN values."}}