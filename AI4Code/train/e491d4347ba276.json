{"cell_type":{"34f8559d":"code","79a53d87":"code","d80f0d4c":"code","d6bac401":"code","9734f119":"code","dd162853":"code","6302e5d0":"code","2976930d":"code","1a924370":"code","c55d3427":"code","04f56d19":"code","cf95b7cf":"code","97afd21a":"code","a9c426f0":"code","f749a0ff":"code","066935e9":"code","82c02a6a":"code","fcca5940":"code","1a0e4266":"code","a9d3477e":"code","3811c14b":"code","247b3e85":"code","ad508f8f":"code","faa22118":"code","032121e4":"code","4c73cff6":"code","e8053484":"code","86728f8c":"code","e2416ac4":"code","70473b72":"code","f43ac2ff":"code","fb0b080d":"code","fb16b416":"markdown","919b0e88":"markdown","b89262dc":"markdown","b800dba8":"markdown","aca109a7":"markdown","6a8aae21":"markdown","b8e8a38b":"markdown","0c748c14":"markdown","52f4c378":"markdown","a42a8974":"markdown","df951a61":"markdown","b38f8163":"markdown","8afa55c0":"markdown","b4b07130":"markdown","461fbbb2":"markdown","b7f0818d":"markdown","af020d8a":"markdown","69ec4761":"markdown","35a50422":"markdown","4a88edf3":"markdown"},"source":{"34f8559d":"from IPython.display import HTML\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n<\/script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on\/off the raw code.\"><\/form>''')","79a53d87":"import time\nt_Start = time.time() # start the clock","d80f0d4c":"# Import libraries\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\npd.options.display.max_columns = None # Displays all columns and when showing dataframes\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Hide warnings\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport time\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom collections import Counter\nfrom sklearn.linear_model import LogisticRegression","d6bac401":"# Import the data\n\nt0 = time.time()\n\ntrain = pd.read_csv('..\/input\/train.csv')\n#test = pd.read_csv('..\/input\/test.csv') Don't import test dataset as anticipate only performing EDA in this kernel and not making submission\n\nt1 = time.time()\n\nprint('Data imported - time taken %3.1f seconds' % (t1-t0))\n","9734f119":"# Redo the dataframe index\n\ntrain = train.set_index('id')","dd162853":"# Select three categories at random\nnp.random.seed(42)\ncat = np.random.randint(0,512,3)\nprint('I will analyse categories %i, %i and %i' % (cat[0],cat[1],cat[2]))","6302e5d0":"# Create data subsets\ntrain_1 = train[train['wheezy-copper-turtle-magic']==cat[0]].drop('wheezy-copper-turtle-magic',axis=1)\ntrain_2 = train[train['wheezy-copper-turtle-magic']==cat[1]].drop('wheezy-copper-turtle-magic',axis=1)\ntrain_3 = train[train['wheezy-copper-turtle-magic']==cat[2]].drop('wheezy-copper-turtle-magic',axis=1)\n","2976930d":"# plot the standard deviation\nfig, ax = plt.subplots(1, 3, figsize=(15, 3));\nax[0].hist(train_1.describe().loc['std'],bins=100);\nax[0].set_title('Distribution of sd - category A');\nax[0].set_xlabel('Standard Deviation');\nax[0].set_ylabel('Count');\nax[1].hist(train_2.describe().loc['std'],bins=100);\nax[1].set_title('Distribution of sd - category B');\nax[1].set_xlabel('Standard Deviation');\nax[1].set_ylabel('Count');\nax[2].hist(train_3.describe().loc['std'],bins=100);\nax[2].set_title('Distribution of sd - category C');\nax[2].set_xlabel('Standard Deviation');\nax[2].set_ylabel('Count');","1a924370":"count = 0\ncount += sum(train_1.describe().loc['std'].between(2,3))\ncount += sum(train_2.describe().loc['std'].between(2,3))\ncount += sum(train_3.describe().loc['std'].between(2,3))\n\nprint('There are %i columns with standard deviation between 2 and 3' % count)","c55d3427":"important_cols1 = train_1.columns[train_1.describe().loc['std']>2.5]\nimportant_cols2 = train_2.columns[train_2.describe().loc['std']>2.5]\nimportant_cols3 = train_3.columns[train_3.describe().loc['std']>2.5]","04f56d19":"print('There are %i, %i and %i \\'important\\' columns for each of the three categories' % (len(important_cols1),len(important_cols2),len(important_cols3)))","cf95b7cf":"all_names = {}\nfor col in train.columns:\n    col_split_list = col.split(\"-\")\n    for i in range(len(col_split_list)):\n        if col_split_list[i] in all_names:\n            all_names[col_split_list[i]] += 1\/(4*(len(train.columns)-1))\n        else:\n            all_names[col_split_list[i]] = 1\/(4*(len(train.columns)-1))            \n\nnames_1={}\nfor col in important_cols1:\n    col_split_list = col.split(\"-\")\n    for i in range(len(col_split_list)):\n        if col_split_list[i] in names_1:\n            names_1[col_split_list[i]] += 1\/(4*len(important_cols1))\n        else:\n            names_1[col_split_list[i]] = 1\/(4*len(important_cols1))\n\nnames_2={}\nfor col in important_cols2:\n    col_split_list = col.split(\"-\")\n    for i in range(len(col_split_list)):\n        if col_split_list[i] in names_2:\n            names_2[col_split_list[i]] += 1\/(4*len(important_cols1))\n        else:\n            names_2[col_split_list[i]] = 1\/(4*len(important_cols1))\n\nnames_3={}\nfor col in important_cols3:\n    col_split_list = col.split(\"-\")\n    for i in range(len(col_split_list)):\n        if col_split_list[i] in names_3:\n            names_3[col_split_list[i]] += 1\/(4*len(important_cols1))\n        else:\n            names_3[col_split_list[i]] = 1\/(4*len(important_cols1))\n            \nk = Counter(all_names)\nhigh = k.most_common(10)\nhigh_labels=[]\nhigh_values=[]\nfor i in range(10):\n    high_labels.append(high[i][0])\n    high_values.append(high[i][1])\n\nk1 = Counter(names_1)\nhigh1 = k1.most_common(10)\nhigh1_labels=[]\nhigh1_values=[]\nfor i in range(10):\n    high1_labels.append(high1[i][0])\n    high1_values.append(high1[i][1])\n\nk2 = Counter(names_2)\nhigh2 = k2.most_common(10)\nhigh2_labels=[]\nhigh2_values=[]\nfor i in range(10):\n    high2_labels.append(high2[i][0])\n    high2_values.append(high2[i][1])\n\nk3 = Counter(names_3)\nhigh3 = k3.most_common(10)\nhigh3_labels=[]\nhigh3_values=[]\nfor i in range(10):\n    high3_labels.append(high3[i][0])\n    high3_values.append(high3[i][1])\n    \n\nfig, ax = plt.subplots(2, 2, figsize=(15, 15));\nax[0,0].bar(range(10), high_values, align='center');\nax[0,1].bar(range(10), high1_values, align='center');\nax[0,0].set_title('Ten most frequent words in column names - all data')\nax[0,0].set_xticks(range(10));\nax[0,0].set_xticklabels(high_labels);\nfor tick in ax[0,0].get_xticklabels():\n    tick.set_rotation(45)\nax[0,1].set_title('Ten most frequent words in column names - category A')\nax[0,1].set_xticks(range(10));\nax[0,1].set_xticklabels(high1_labels);\nfor tick in ax[0,1].get_xticklabels():\n    tick.set_rotation(45)\nax[1,0].bar(range(10), high2_values, align='center');\nax[1,1].bar(range(10), high3_values, align='center');\nax[1,0].set_title('Ten most frequent words in column names - category B')\nax[1,0].set_xticks(range(10));\nax[1,0].set_xticklabels(high2_labels);\nfor tick in ax[1,0].get_xticklabels():\n    tick.set_rotation(45)\nax[1,1].set_title('Ten most frequent words in column names - category C')\nax[1,1].set_xticks(range(10));\nax[1,1].set_xticklabels(high3_labels);\nfor tick in ax[1,1].get_xticklabels():\n    tick.set_rotation(45)","97afd21a":"# For each category, create three training arrays and results array\n\nX_train1_all = train_1.drop('target',axis=1)\nX_train1_imp = X_train1_all[important_cols1]\nX_train1_unimp = X_train1_all.drop(important_cols1,axis=1)\ny_train1 = train_1['target']\n\nX_train2_all = train_2.drop('target',axis=1)\nX_train2_imp = X_train2_all[important_cols2]\nX_train2_unimp = X_train2_all.drop(important_cols2,axis=1)\ny_train2 = train_2['target']\n\nX_train3_all = train_3.drop('target',axis=1)\nX_train3_imp = X_train3_all[important_cols3]\nX_train3_unimp = X_train3_all.drop(important_cols3,axis=1)\ny_train3 = train_3['target']\n\n# For each category, train three models.\n# Use SVM as has been shown to perform fairly well\n\nsvm1_all = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train1_all,y_train1)\nsvm1_imp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train1_imp,y_train1)\nsvm1_unimp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train1_unimp,y_train1)\n\nsvm2_all = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train2_all,y_train2)\nsvm2_imp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train2_imp,y_train2)\nsvm2_unimp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train2_unimp,y_train2)\n\nsvm3_all = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train3_all,y_train3)\nsvm3_imp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train3_imp,y_train3)\nsvm3_unimp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train3_unimp,y_train3)\n\nprint('All models trained')","a9c426f0":"# Work out the AUC score for each model using 5-fold cross validation\nall_auc_1 = cross_val_score(svm1_all, X_train1_all, y_train1, scoring='roc_auc', cv=5).mean()\nimp_auc_1 = cross_val_score(svm1_imp, X_train1_imp, y_train1, scoring='roc_auc', cv=5).mean()\nunimp_auc_1 = cross_val_score(svm1_unimp, X_train1_unimp, y_train1, scoring='roc_auc', cv=5).mean()\n\nall_auc_2 = cross_val_score(svm2_all, X_train2_all, y_train2, scoring='roc_auc', cv=5).mean()\nimp_auc_2 = cross_val_score(svm2_imp, X_train2_imp, y_train2, scoring='roc_auc', cv=5).mean()\nunimp_auc_2 = cross_val_score(svm2_unimp, X_train2_unimp, y_train2, scoring='roc_auc', cv=5).mean()\n\nall_auc_3 = cross_val_score(svm3_all, X_train3_all, y_train3, scoring='roc_auc', cv=5).mean()\nimp_auc_3 = cross_val_score(svm3_imp, X_train3_imp, y_train3, scoring='roc_auc', cv=5).mean()\nunimp_auc_3 = cross_val_score(svm3_unimp, X_train3_unimp, y_train3, scoring='roc_auc', cv=5).mean()\n\ncat1 = [all_auc_1,imp_auc_1,unimp_auc_1]\ncat2 = [all_auc_2,imp_auc_2,unimp_auc_2]\ncat3 = [all_auc_3,imp_auc_3,unimp_auc_3]\n\nresults=[cat1,cat2,cat3]\n\nplt.figure(figsize=(20, 10));\nplt.plot(results);\nplt.legend(['Using all features', 'Using only important features', 'Using only unimportant features']);\nplt.title('AUC score for each model');\nplt.xlabel('Category');\nplt.xticks(range(3),cat);","f749a0ff":"t0 = time.time()\n\ncat_to_test = 100 # set the number of categories to test - 10 takes around one minute to run\ncat_temp = np.sort(np.random.randint(0,512,cat_to_test)) # pick the categories at random - re-order for graph below\n\nresults = np.zeros((cat_to_test,3))\ni = 0\n\nfor cat in cat_temp:\n    # Filter training data just for category i and identify 'important' columns\n    train_temp = train[train['wheezy-copper-turtle-magic']==cat].drop('wheezy-copper-turtle-magic',axis=1)\n    important_cols_temp = train_temp.columns[train_temp.describe().loc['std']>2.5]\n    \n    # Set up X and y datasets\n    X_train_temp_all = train_temp.drop('target',axis=1)\n    X_train_temp_imp = X_train_temp_all[important_cols_temp]\n    X_train_temp_unimp = X_train_temp_all.drop(important_cols_temp,axis=1)\n    y_train_temp = train_temp['target']\n    \n    # Train models\n    svm_temp_all = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train_temp_all,y_train_temp)\n    svm_temp_imp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train_temp_imp,y_train_temp)\n    svm_temp_unimp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train_temp_unimp,y_train_temp)\n    \n    # Work out the AUC score for each model using 5-fold cross validation\n    all_auc = cross_val_score(svm_temp_all, X_train_temp_all, y_train_temp, scoring='roc_auc', cv=5).mean()\n    imp_auc = cross_val_score(svm_temp_imp, X_train_temp_imp, y_train_temp, scoring='roc_auc', cv=5).mean()\n    unimp_auc = cross_val_score(svm_temp_unimp, X_train_temp_unimp, y_train_temp, scoring='roc_auc', cv=5).mean()\n    \n    # Add results to array\n    res_temp = [all_auc,imp_auc,unimp_auc]\n    results[i,:] = res_temp\n    i += 1\n    \n    if i%50 == 0:\n        print('Running category %i out of %i' % ((i+1),cat_to_test))\n\n# Plot the results\nplt.figure(figsize=(20, 10));\nplt.plot(results);\nplt.legend(['Using all features', 'Using only important features', 'Using only unimportant features']);\nplt.title('AUC score for each model');\nplt.xlabel('Category');\nplt.xticks(range(cat_to_test),cat_temp);\n\nt1 = time.time()\nprint('Total run time = %i minutes and %3.1f seconds' % ((t1-t0)\/\/60,(t1-t0)%60))","066935e9":"# Print the average AUC score across all categories\nprint('Models including all columns have an average AUC score of %3.3f' % np.mean(results[:,0]))\nprint('Models including just important columns have an average AUC score of %3.3f' % np.mean(results[:,1]))\nprint('Models including just unimportant columns have an average AUC score of %3.3f' % np.mean(results[:,2]))","82c02a6a":"t0 = time.time()\n\ncat_to_test = 100 # set the number of categories to test - 10 takes around one minute to run\ncat_temp = np.sort(np.random.randint(0,512,cat_to_test)) # pick the categories at random - re-order for graph below\n\nresults = np.zeros((cat_to_test,3))\ni = 0\n\nfor cat in cat_temp:\n    # Filter training data just for category i and identify 'important' columns\n    train_temp = train[train['wheezy-copper-turtle-magic']==cat].drop('wheezy-copper-turtle-magic',axis=1)\n    important_cols_temp = train_temp.columns[train_temp.describe().loc['std']>2.5]\n    \n    # Set up X and y datasets\n    X_train_temp_all = train_temp.drop('target',axis=1)\n    X_train_temp_imp = X_train_temp_all[important_cols_temp]\n    X_train_temp_unimp = X_train_temp_all.drop(important_cols_temp,axis=1)\n    \n    # Replace values in unimportant columns with random Normal samples\n    m = np.shape(X_train_temp_all)[0]\n    unimportant_cols = X_train_temp_unimp.columns\n    for col in unimportant_cols:\n        X_train_temp_all[col] = np.random.normal(size=m)\n   \n    y_train_temp = train_temp['target']\n    \n    # Train models\n    svm_temp_all = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train_temp_all,y_train_temp)\n    svm_temp_imp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train_temp_imp,y_train_temp)\n    svm_temp_unimp = SVC(probability=True,kernel='poly',degree=4,gamma='auto').fit(X_train_temp_unimp,y_train_temp)\n    \n    # Work out the AUC score for each model using 5-fold cross validation\n    all_auc = cross_val_score(svm_temp_all, X_train_temp_all, y_train_temp, scoring='roc_auc', cv=5).mean()\n    imp_auc = cross_val_score(svm_temp_imp, X_train_temp_imp, y_train_temp, scoring='roc_auc', cv=5).mean()\n    unimp_auc = cross_val_score(svm_temp_unimp, X_train_temp_unimp, y_train_temp, scoring='roc_auc', cv=5).mean()\n    \n    # Add results to array\n    res_temp = [all_auc,imp_auc,unimp_auc]\n    results[i,:] = res_temp\n    i += 1\n    \n    if i%50 == 0:\n        print('Running category %i out of %i' % ((i+1),cat_to_test))\n\n# Plot the results\nplt.figure(figsize=(20, 10));\nplt.plot(results);\nplt.legend(['Adding random features', 'Using only important features', 'Using only unimportant features']);\nplt.title('AUC score for each model');\nplt.xlabel('Category');\nplt.xticks(range(cat_to_test),cat_temp);\n\nt1 = time.time()\nprint('Total run time = %i minutes and %3.1f seconds' % ((t1-t0)\/\/60,(t1-t0)%60))","fcca5940":"# Print the average AUC score across all categories\nprint('Models including random features have an average AUC score of %3.3f' % np.mean(results[:,0]))\nprint('Models including just important columns have an average AUC score of %3.3f' % np.mean(results[:,1]))\nprint('Models including just unimportant columns have an average AUC score of %3.3f' % np.mean(results[:,2]))","1a0e4266":"# Create a dataframe of the counts of important and unimportant columns by the fourth word\nt0 = time.time()\n# For each category:\n# - identify the important columns\n# - count the unique column names\n# - add this to a dictionary\n# - create a dataframe with this information for each category\n\n# Create a dictionary of all fourth words (with count of zero):\nwords_dict={}\n\nfor col in train.columns.drop('target'):\n    words_dict[col.split(\"-\")[3]] = 0\n\n    \nresults = pd.DataFrame.from_dict(words_dict,orient='index')\n\nfor i in range(512):\n    # Filter training data just for category i and identify 'important' columns\n    train_temp = train[train['wheezy-copper-turtle-magic']==i].drop('wheezy-copper-turtle-magic',axis=1)\n    important_cols_temp = train_temp.columns[train_temp.describe().loc['std']>2.5]\n    \n    # Create a list of the fourth word for each important column\n    words_temp = []\n    \n    for col in important_cols_temp:\n        words_temp.append(col.split(\"-\")[3])\n    \n    # Create a dictionary of counts\n    for item in words_dict.keys():\n        if item in words_temp:\n            words_dict[item] = words_temp.count(item)\n        else:\n            words_dict[item] = 0\n    \n    results[i] = pd.DataFrame.from_dict(words_dict,orient='index')\n    \nt1 = time.time()\nprint('Total run time = %i minutes and %3.1f seconds' % ((t1-t0)\/\/60,(t1-t0)%60))    ","a9d3477e":"print('The number of important columns for each category ranges from %i to %i' % (results.sum(axis=0).min(),results.sum(axis=0).max()))","3811c14b":"# Show the number of categories with x important columns\nx = np.unique(results.sum(axis=0),return_counts=True)[0]\ny = np.unique(results.sum(axis=0),return_counts=True)[1]\nplt.figure(figsize=(20, 10));\nplt.bar(x,y);\nplt.title('Number of important features per category');\nplt.xlabel('Number of important features');\nplt.ylabel('Count');\n","247b3e85":"\nleast_common = results.sum(axis=1).sort_values().head(2).tail(1).index[0]\nleast_count = results.sum(axis=1).sort_values()[least_common]\nmost_common = results.sum(axis=1).sort_values().tail(1).index[0]\nmost_count = results.sum(axis=1).sort_values()[most_common]\n\nprint('The least common word ,other than magic, is %s (%i times) and the most common word is %s (%i times)' % (least_common,least_count,most_common,most_count))\n","ad508f8f":"plt.figure(figsize=(20, 10));\nplt.title('Frequency per category');\nplt.xlabel('Fourth column word');\nplt.xticks(range(len(results.index)),results.index,rotation=45);\nplt.plot(results.max(axis=1),marker='*',label='Maximum frequency');\nplt.plot(results.mean(axis=1),marker='*',label='Average frequency');\nplt.plot(results.min(axis=1),marker='*',label='Minimum frequency');\nplt.legend();","faa22118":"results_2 = results.drop(['distraction','noise','discard','dummy'])\nprint('The number of important columns ranges from %i to %i' % (results_2.sum(axis=0).min(),results_2.sum(axis=0).max()))","032121e4":"# Fit logistic regression models\n\nlr_imp_1 = LogisticRegression().fit(X_train1_imp,y_train1)\nlr_imp_2 = LogisticRegression().fit(X_train2_imp,y_train2)\nlr_imp_3 = LogisticRegression().fit(X_train3_imp,y_train3)\n\n#Test the AUC to check the models have some predictive ability\n\nauc_1 = cross_val_score(lr_imp_1, X_train1_imp, y_train1, scoring='roc_auc', cv=5).mean()\nauc_2 = cross_val_score(lr_imp_2, X_train2_imp, y_train2, scoring='roc_auc', cv=5).mean()\nauc_3 = cross_val_score(lr_imp_3, X_train3_imp, y_train3, scoring='roc_auc', cv=5).mean()\n\nprint('The AUC score for the three categories is %3.3f, %3.3f and %3.3f' % (auc_1,auc_2,auc_3))\n\n","4c73cff6":"# Feature importance for each category\nfeature_importance_1 = abs(lr_imp_1.coef_[0])\nfeature_importance_1 = 100.0 * (feature_importance_1 \/ feature_importance_1.max())\nsorted_idx_1 = np.argsort(feature_importance_1)\npos_1 = np.arange(sorted_idx_1.shape[0]) + .5\n\nfeature_importance_2 = abs(lr_imp_2.coef_[0])\nfeature_importance_2 = 100.0 * (feature_importance_2 \/ feature_importance_2.max())\nsorted_idx_2 = np.argsort(feature_importance_2)\npos_2 = np.arange(sorted_idx_2.shape[0]) + .5\n\nfeature_importance_3 = abs(lr_imp_3.coef_[0])\nfeature_importance_3 = 100.0 * (feature_importance_3 \/ feature_importance_3.max())\nsorted_idx_3 = np.argsort(feature_importance_3)\npos_3 = np.arange(sorted_idx_3.shape[0]) + .5\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 20));\nax[0].barh(pos_1, feature_importance_1[sorted_idx_1], align='center',alpha=0.3);\nytick_labels_1 = np.array(X_train1_imp.columns)[sorted_idx_1]\nax[0].set_xlabel('Relative Feature Importance');\nax[0].set_title('Feature importance for category A');\nax[0].set_yticklabels('')\nfor i, v in enumerate(ytick_labels_1):\n    ax[0].text(5, i + .25, str(v), color='black', fontweight='bold')\n\nax[1].barh(pos_2, feature_importance_2[sorted_idx_2], align='center',alpha=0.3);\nytick_labels_2 = np.array(X_train2_imp.columns)[sorted_idx_2]\nax[1].set_xlabel('Relative Feature Importance');\nax[1].set_title('Feature importance for category B');\nax[1].set_yticklabels('')\nfor i, v in enumerate(ytick_labels_2):\n    ax[1].text(5, i + .25, str(v), color='black', fontweight='bold')\n\nax[2].barh(pos_3, feature_importance_3[sorted_idx_3], align='center',alpha=0.3);\nytick_labels_3 = np.array(X_train3_imp.columns)[sorted_idx_3]\nax[2].set_xlabel('Relative Feature Importance');\nax[2].set_title('Feature importance for category C');\nax[2].set_yticklabels('')\nfor i, v in enumerate(ytick_labels_3):\n    ax[2].text(5, i + .25, str(v), color='black', fontweight='bold')","e8053484":"# Create a dataframe of the counts of important and unimportant columns by each word in the column names\nt0 = time.time()\n\n# Create a dictionary of all words (with count of zero):\nwords_dict={}\nunimp_dict={}\n\nfor j in range(4):\n    for col in train.columns.drop('target'):\n        words_dict[col.split(\"-\")[j]] = 0\n        unimp_dict[col.split(\"-\")[j]] = 0\n\nimp_results = pd.DataFrame.from_dict(words_dict,orient='index')\nunimp_results = pd.DataFrame.from_dict(words_dict,orient='index')\n\nfor i in range(512):\n    # Filter training data just for category i and identify 'important' columns\n    train_temp = train[train['wheezy-copper-turtle-magic']==i].drop(['wheezy-copper-turtle-magic','target'],axis=1)\n    important_cols_temp = train_temp.columns[train_temp.describe().loc['std']>2.5]\n    unimportant_cols_temp = train_temp.columns[train_temp.describe().loc['std']<2.5]   \n    # Create a list of the fourth word for each important column\n    words_temp = []\n    unimp_temp=[]\n    for j in range(4):\n        for col in important_cols_temp:\n            words_temp.append(col.split(\"-\")[j])\n        for col in unimportant_cols_temp:\n            unimp_temp.append(col.split(\"-\")[j])\n    \n    # Create a dictionary of counts\n    for item in words_dict.keys():\n        if item in words_temp:\n            words_dict[item] = words_temp.count(item)\n        else:\n            words_dict[item] = 0\n    imp_results[i] = pd.DataFrame.from_dict(words_dict,orient='index')\n    \n    # Create a dictionary of counts\n    for item in unimp_dict.keys():\n        if item in unimp_temp:\n            unimp_dict[item] = unimp_temp.count(item)\n        else:\n            unimp_dict[item] = 0\n    \n    unimp_results[i] = pd.DataFrame.from_dict(unimp_dict,orient='index')\n\nt1 = time.time()\nprint('Total run time = %i minutes and %3.1f seconds' % ((t1-t0)\/\/60,(t1-t0)%60))    ","86728f8c":"freq = (np.sum(imp_results,axis=1)\/(np.sum(imp_results,axis=1)+np.sum(unimp_results,axis=1))).rename('freq')\ncount = (imp_results[0]+unimp_results[0]).rename('count')\nsummary = pd.concat([freq,count],axis=1).sort_values(by='freq',ascending=True).drop('magic',axis=0)\n\nfig = plt.figure(figsize=(20,10))\n\ncx0 = fig.add_subplot(121)\ncx1 = cx0.twinx()\ncx2 = plt.subplot(122)\ncx3 = cx2.twinx()\n\nsummary.head(20)['freq'].plot(ax=cx0)\nsummary.head(20)['count'].plot(ax=cx1, kind='bar', secondary_y=True,alpha=0.3)\nsummary.tail(20)['freq'].plot(ax=cx2)\nsummary.tail(20)['count'].plot(ax=cx3, kind='bar', secondary_y=True,alpha=0.3)\n\ncx0.set_title('The twenty words which appear least often (by proportion) in important columns');\ncx2.set_title('The twenty words which appear most often (by proportion) in important columns');\ncx0.set_ylabel('Proportion word appears in important column');\ncx0.set_xticklabels(summary.head(20).index,rotation=45);\ncx1.set_ylabel('Total times word appears');\ncx2.set_ylabel('Proportion word appears in important column');\ncx3.set_ylabel('Total times word appears');\ncx2.set_xticklabels(summary.tail(20).index,rotation=45);","e2416ac4":"plt.figure(figsize=(20, 10));\nplt.title('Distributon of frequencies');\nplt.xlabel('Frequency as important column');\nplt.hist(summary['freq'],bins=75);","70473b72":"fourth_word = {}\n\nfor col in train.columns.drop('target'):\n    fourth_word[col.split(\"-\")[j]] = 0\n\nfourth_wrd = []\nfor key in fourth_word.keys():\n    fourth_wrd.append(key)\n    \nsummary_2 = summary.loc[fourth_wrd].sort_values(by='freq')","f43ac2ff":"fig = plt.figure(figsize=(25,10))\n\ncx0 = fig.add_subplot(121)\ncx1 = cx0.twinx()\n\nsummary_2['freq'].plot(ax=cx0)\nsummary_2['count'].plot(ax=cx1, kind='bar', secondary_y=True,alpha=0.3)\n\ncx0.set_title('The fourth words and how often they appear in important columns');\ncx0.set_ylabel('Proportion word appears in important column');\ncx0.set_xticklabels(summary_2.index,rotation=45);\ncx1.set_ylabel('Total times word appears');","fb0b080d":"t_End = time.time()\nprint('Total run time = %i minutes and %3.1f seconds' % ((t_End-t_Start)%60,(t_End-t_Start)\/\/60))","fb16b416":"There are no obvious trends or anomalies that I can see (although 'cat' has a slight gap to second place in terms of being important most frequently). However, many words are only present in one column. Let's look again just at the fourth word, which is more often repeated.","919b0e88":"There is no obvious distribution to the number of important features within a category. It is possible this could be a sample from a random uniform integer distribution between 33 and 47.","b89262dc":"### Identifying the 'important' columns for each category","b800dba8":"The AUC score is the same if the useless columns are re-populated with noise. Therefore, it seems likely these have no value for each category and should be removed to avoid over-fitting.","aca109a7":"Still no obvious conclusions to be drawn!! Dummy is the least often important but distraction is the most often important.","6a8aae21":"# Instant Gratification\n## EDA by category\nThere are many public kernels demonstrating that the dataset may be best treated as 512 different datasets. Furthermore, many of the numeric columns are of little value for most of the categories, but for each category a different c40 numeric variables contain most (possibly all) of the predictive value. These 'important' columns are identified by having a higher standard deviation than the other columns.\n\nIn this kernel, I will select three categories and perform EDA for the data that relates to these categories. I will also look to fit a model (K-Nearest Neighbours and SVMs seem to perform well for this dataset) to this data three times:\n- once using all numeric feature columns\n- once using just the c40 'important' feature columns'\n- once using the 'unimportant' feature columns.\n\nIf the 'unimportant' numeric columns have no predictive value, then the fitted models should perform no better than random chance.","b8e8a38b":"### Select three categories for analysis\nI will randomly select three categories to analyse and treat these as defining three separate training datasets.","0c748c14":"### Conclusions\n- All of the information for each category is contained within a small subset of the feature columns. The other columns have no value, unless they provide information in respect of interactions between categories.\n- The column names do not seem to have any use, other than identifying the 'magic' column","52f4c378":"I can see no pattern in the features which are important nor those which are unimportant. I can only conclude that the column names are meaningless (other than the 'magic' column).\nLet's try creating a database for all words and seeing if any are more common (or less common) in important features.","a42a8974":"### Set up libraries and import data","df951a61":"### Fit a model for each category to all the columns\nIf the important columns are the only ones that matter for a single category, then:\n- a model using just these columns should perform similarly (or better) than a model using all the numerical columns.\n- a model using the unimportant columns should perform no better than chance.\n\nNote: Even if the above hold true, the unimportant columns may have value if there are interactions between categories.","b38f8163":"There is one category with at least 10 important features with these four names. However, there is at least one column with none of these. This doesn't seem to help create a list of truly important columns. Let's fit a different model to the three categories to analyse and look at feature importance (feature importance only works for SVM if linear kernel is used).","8afa55c0":"The models with all numercial columns included score higher than those using just the important columns. However, models using just the unimportant columns do not score any better than chance. This suggests the unimportant features only have value when used in conjunction with the important columns. On their own, they have no value.\n\nTo see if this result is genuine (or if due to dimensionality or regularisation as suggested by Chris Deott in the comments), I will replace the values in the unimportant columns with samples from a standard Normal distribution and see if the results are replicated.","b4b07130":"There does seem to be some value in the unimportant columns, although the vast majority of the information is contained in the important columns. To check this, I will create the above results for more categories.","461fbbb2":"The columns with a standard deviation above 2.5 for the first datasets is:","b7f0818d":"### Important column names\nIn spite of the brief analysis above, I do wonder if there is something in the column names of the important columns. In particular, I wonder:\n- if there are a minimum number that are important and some are actually noise\n- if multiple columns could be re-combined to create a single feature which was predictive across categories.\n\nThis theory relies on there being some information in the names. The fourth word seems the most promising (primarily because there are relatively few unique words) so I will concentrate here.","af020d8a":"Whilst different column names occur most frequently in the total dataset and for the important columns, I can see no obvious pattern in the column names for the three categories analysed.","69ec4761":"As expected, there are a number of columns with a standard deviation around 3.75 and a greater number with standard deviation around 1.0. Let's look at which columns are in the first group.","35a50422":"For every word, there is at least one categoy where that word is totally absent. Also, for every word, there is at least one category where that word occurs between 4 and 8 times (other than magic).\n\nLet's try removing the following words:\n- distraction\n- noise\n- discard\n- dummy","4a88edf3":"For the three examples selected, none of the columns have a standard deviation between 2 and 3. Therefore, will use 2.5 as the dividing line."}}