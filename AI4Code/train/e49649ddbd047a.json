{"cell_type":{"939a7a51":"code","92913954":"code","adf3ef09":"code","5ad263bd":"code","a921435f":"code","04fae9d3":"code","5fcb3a3b":"code","7bebab4c":"code","58f2c4fa":"code","6275e95e":"code","69a1e2b5":"code","9fc1234f":"code","ab05b9b0":"code","0525c6b5":"code","f93c0738":"code","0e3f0e14":"code","b2f5bd20":"code","0dbaf1af":"code","8ff202e8":"code","b83a2803":"code","6bf02437":"code","3896ad42":"code","7db52810":"code","b25f4e34":"code","d850650c":"code","890d47fe":"code","870b19e9":"code","456c44c0":"code","f7e615eb":"code","3f066c29":"code","290a4453":"code","e49dd86e":"code","d285ba6f":"code","e241ab1b":"code","acfcb12f":"code","5598cc7e":"code","d2b2ecaf":"code","b0c81dc9":"code","9ef94a06":"code","578aefed":"code","8ead4f62":"code","d0438ea3":"code","253c7a84":"code","e322f6ad":"code","3460dab8":"code","19a87851":"code","12b640e4":"code","c6107c95":"code","78bd9f3f":"code","c08c00c2":"code","37d452d3":"code","7b9279a5":"code","dbd9c86d":"code","e8f93fb4":"code","db35bae3":"markdown","24a856a8":"markdown","6f981b9d":"markdown","099eadb1":"markdown","2dd66555":"markdown","cff91f8b":"markdown","5e192791":"markdown","05aa7d32":"markdown","f54ab1d2":"markdown","9245cff2":"markdown","7ffc7c00":"markdown","4846ef53":"markdown","759e1e16":"markdown","d82f5cbe":"markdown","a653e63a":"markdown","c53b39de":"markdown","ddcc6c68":"markdown","52681929":"markdown","492bfbc5":"markdown","f532d6b3":"markdown","882ebe66":"markdown","048ef4b9":"markdown","11b91884":"markdown","887849bc":"markdown","725b1b82":"markdown","1855f1fc":"markdown"},"source":{"939a7a51":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","92913954":"train_raw=pd.read_csv('..\/input\/train.csv')\ntest_raw=pd.read_csv('..\/input\/test.csv')\ntrain_raw['Title']=train_raw['Name'].apply(lambda x:x.split(',')[1].split()[0].strip('.'))\ntest_raw['Title']=test_raw['Name'].apply(lambda x:x.split(',')[1].split()[0].strip('.'))","adf3ef09":"train_raw.info()","5ad263bd":"test_raw.info()","a921435f":"train_raw.describe()","04fae9d3":"sns.heatmap(train_raw.corr(),cmap='viridis',cbar=False)","5fcb3a3b":"train=train_raw.copy()\ntest=test_raw.copy()","7bebab4c":"train.head()","58f2c4fa":"train.Title.unique()","6275e95e":"train.Title.replace(['Mlle','Ms'],'Miss',inplace=True)\ntrain.Title.replace('Mme','Mrs',inplace=True)\ntrain.Title.replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'the','Jonkheer', 'Dona'],'Higher',inplace=True)\n\ntest.Title.replace(['Mlle','Ms'],'Miss',inplace=True)\ntest.Title.replace('Mme','Mrs',inplace=True)\ntest.Title.replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'the','Jonkheer', 'Dona'],'Higher',inplace=True)","69a1e2b5":"train.Title.unique()","9fc1234f":"test.Title.unique()","ab05b9b0":"train.drop(columns=['Name','Cabin','Ticket','PassengerId'],inplace=True)\ntest.drop(columns=['Name','Cabin','Ticket'],inplace=True)","0525c6b5":"train=pd.get_dummies(data=train,columns=['Sex','Embarked'],drop_first=True)\ntest=pd.get_dummies(data=test,columns=['Sex','Embarked'],drop_first=True)","f93c0738":"train=pd.get_dummies(data=train,columns=['Title'],drop_first=True)\ntest=pd.get_dummies(data=test,columns=['Title'],drop_first=True)","0e3f0e14":"train.head()","b2f5bd20":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","0dbaf1af":"xage=train.dropna().drop(['Survived','Age','Embarked_Q','Embarked_S'],axis=1); yage=train.dropna()['Age']\nxage_train,xage_test,yage_train,yage_test=train_test_split(xage,yage,test_size=0.2,random_state=5)\nlmage=LinearRegression().fit(xage_train,yage_train)\nagepred=np.abs(lmage.predict(xage_test))","8ff202e8":"train_raw.columns","b83a2803":"def imputeage(cols):\n    pclass=cols[0]\n    sex=cols[1]\n    age=cols[2]\n    sibsp=cols[3]\n    title=cols[4]\n    return train_raw[(train_raw['Pclass']==pclass) & (train_raw['Sex']==sex) & (train_raw['SibSp']==sibsp)].drop('Cabin',axis=1).dropna()['Age'].mean()","6bf02437":"agedf2=train_raw[['Pclass', 'Sex', 'Age', 'SibSp','Title']].copy().dropna()\nagedf2['NewAge']=agedf2[['Pclass','Sex','Age','SibSp','Title']].apply(imputeage,axis=1)","3896ad42":"agedf2.info()","7db52810":"xage2_train,xage2_test,yage2_train,yage2_test=train_test_split(agedf2.dropna(subset=['Age'],axis=0)['Age'],agedf2.dropna(subset=['Age'],axis=0)['NewAge'],test_size=0.2,random_state=5)","b25f4e34":"fillmethodcompare=pd.DataFrame()\nfillmethodcompare['m1_age']=yage_test\nfillmethodcompare['m1_predage']=agepred\nfillmethodcompare['m2_age']=xage2_test\nfillmethodcompare['m2_newage']=yage2_test","d850650c":"fillmethodcompare.sample(10,random_state=101)","890d47fe":"from IPython.display import display, HTML\nagedf=pd.DataFrame()\nagedf['Real']=yage_test\nagedf['Predicted']=agepred\nagedf['Delta']=agedf['Real']-agedf['Predicted']\nagedf['Mean_Delta']=np.abs(agedf['Real']-agedf['Predicted'])\nagedf['Mean_Delta_Normalised']=(agedf['Mean_Delta']\/agedf['Real'])*100\n\n#display(HTML(agedf.to_html()))\n\nfrom sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(yage_test, agepred))\nprint('Mean Square Error:', metrics.mean_squared_error(yage_test, agepred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(yage_test, agepred)))\nprint('Mean Error by Percentage of value: ',agedf['Mean_Delta_Normalised'].mean(),'%')\nprint('Standard deviation of Error Percentage: ',np.std(agedf['Mean_Delta_Normalised']),'%')","870b19e9":"sns.distplot(agedf['Delta'],bins=15,kde=False); plt.tight_layout()","456c44c0":"age2_errorpercent=np.abs((yage2_test-xage2_test)*100\/xage2_test)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(xage2_test,yage2_test))\nprint('Mean Square Error:', metrics.mean_squared_error(xage2_test,yage2_test))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(xage2_test,yage2_test)))\nprint('Mean Error by Percentage of value: ',age2_errorpercent.mean(),'%')\nprint('Standard deviation of Error Percentage: ',np.std(age2_errorpercent),'%')","f7e615eb":"sns.distplot(yage2_test-xage2_test,bins=15,kde=False); plt.tight_layout()","3f066c29":"train['NewAge']=lmage.predict(train.drop(['Survived','Age','Embarked_Q','Embarked_S'],axis=1))\ndef imputeagefill(ages):\n    if pd.isnull(ages[0]):\n        return np.abs(ages[1])\n    else:\n        return np.abs(ages[0])\ntrain['Age']=train[['Age','NewAge']].apply(imputeagefill,axis=1)\ntrain.drop('NewAge',axis=1,inplace=True)\ntrain.sample(10)","290a4453":"test['Fare']=test['Fare'].fillna(test['Fare'].mean())\ntest['NewAge']=lmage.predict(test.drop(['Age','Embarked_Q','Embarked_S','PassengerId'],axis=1))\ntest['Age']=test[['Age','NewAge']].apply(imputeagefill,axis=1)\ntest.drop('NewAge',axis=1,inplace=True)\ntest.sample(10)","e49dd86e":"train.info()","d285ba6f":"test.info()","e241ab1b":"plt.figure(figsize=(12,8))\nsns.heatmap(train.corr(),cmap='coolwarm',annot=True,cbar=False); plt.show()","acfcb12f":"sns.countplot(x='Sex',data=train_raw,hue='Survived')","5598cc7e":"sns.countplot(x='SibSp',data=train_raw,hue='Survived')","d2b2ecaf":"sns.countplot(x='Pclass',data=train_raw,hue='Survived')","b0c81dc9":"sns.countplot(x='Embarked',data=train_raw,hue='Survived')","9ef94a06":"sns.kdeplot(train[train['Survived']==1]['Age'].dropna(),shade=True,label='Survived=1')\nsns.kdeplot(train[train['Survived']==0]['Age'].dropna(),shade=True,label='Survived=0')\nplt.legend(); plt.xlabel('Age'); plt.ylabel('KDE'); plt.show()","578aefed":"plt.figure(figsize=(8,5))\nsns.kdeplot(train[train['Survived']==0]['Fare'],shade=True,label='Survived=0')\nsns.kdeplot(train[train['Survived']==1]['Fare'],shade=True,label='Survived=1')\nplt.xlabel('Fare'); plt.ylabel('KDE'); plt.xlim(-40,300); plt.tight_layout()","8ead4f62":"train.info()","d0438ea3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix","253c7a84":"X=train.drop('Survived',axis=1); y=train['Survived']\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=5)","e322f6ad":"Algorithms = [LogisticRegression(), RandomForestClassifier(), SVC(), XGBClassifier(), KNeighborsClassifier()]\n\nalgoNames=[]; algoScores=[]\nfor algo in Algorithms:\n    algo.fit(X_train,y_train)\n    algoName=algo.__class__.__name__\n    algoScore=algo.score(X_train,y_train)\n    algoNames.append(algoName)\n    algoScores.append(algoScore)\nalgocompare=pd.DataFrame({'Algorithm Name':algoNames,'Score':algoScores}).sort_values(by='Score',ascending=False)","3460dab8":"sns.barplot(x='Score',y='Algorithm Name',data=algocompare); plt.show()","19a87851":"from sklearn.model_selection import GridSearchCV","12b640e4":"param_grid={'n_estimators':[10,50,100,200],'criterion':['gini','entropy'],'bootstrap':[True,False],\n           'random_state':[5],'max_features':['auto','log2',None]}\nRFC_grid_search=GridSearchCV(RandomForestClassifier(),param_grid)","c6107c95":"RFC_grid_search.fit(X_train,y_train)\nRFC_grid_search.best_params_","78bd9f3f":"RFC_grid_search.best_score_","c08c00c2":"from sklearn.metrics import accuracy_score\n\nrfc_pred=RFC_grid_search.predict(X_test)\nprint(accuracy_score(y_test,rfc_pred))","37d452d3":"print(classification_report(y_test,rfc_pred))","7b9279a5":"RFC=RandomForestClassifier(n_estimators=100).fit(X,y)","dbd9c86d":"RFC_test_predictions=RFC.predict(test.drop('PassengerId',axis=1))","e8f93fb4":"test['Survived']=RFC_test_predictions\nsubmit=test[['PassengerId','Survived']]\nsubmit.to_csv('submit.csv',index=False)\nsubmit.head()","db35bae3":"## Model Selection and verification","24a856a8":"**Comments:**\n\nThe plots above show several important relationships on survival of the passengers:\n+ Passangers with lower fare mostly didn't survive, whereas passengers with higher fares had higher chances of survival\n+ Passengers under the age of 20 as well as females had higher chances of survival as compared to passengers older than 20 years and males \n+ Couples had higher chances of survival compared to those that were single\n+ More than half the passengers travelling in class 1 survived, nearly half the passengers travelling in class 2 survived, more than 75% of the passengers travelling in class 3 didn't survive ","6f981b9d":"# Conclusion\n\nIn the current study, we undertook the following steps to predicted the survival of titanic passengers:\n\n+ Identified the missing data, some of age data and most of Cabin data was missing, we hence droped the cabin column\n+ Before filling the missing age data, we feature engineered some columns: extracted the titles from names, converted the categorical Embarked and sex column to dummy numerical columns\n+ For comparison purpose, we tested to fill methods to fill the missing age data:\n    + Linear Regression: applied linear regression to fit and predict age data based on other columns features of the same passenger, exluding the 'Survived' column which is the target column to avoid any bias in final results\n    + Mean of age grouped by other features: passangers with identical sex, class and number of siblings\/spouse were filtered and a mean of those passengers' age was used to fill the missing age records\n+ Based on accuracy of results, linear regression method was used to fill the missing age data in both training and test dataset\n+ A thorough data exploration was conducted to identify the most relevant\/correlated features and understand the data better\n+ Based on the training dataset, Random Forest Classifier, XGB Classifier, SVC classifier, Logistic Regression and KNeighborough Classifier were all used to compare and identify the highest scoring classifier for the current dataset\n+ Random Forest was determined to be the best based on the current analysis and was used for final predictions of the test dataset\n+ The model was further optimised using the GridSearchCV to find the best scoring parameters\n+ Using the best scroing parameters the results were predicted on the test dataset and output as submit.csv file","099eadb1":"Below are the unique titles extracted from the name column. Certain titles can be grouped together to reduce the dimensions of the dataset, as later these categorical columns will get converted to numberic columns. The same grouping is performed on the training and test datasets simultaneously, to ensure consistancy in categorical columns.","2dd66555":"### Comparison of Fill Methods","cff91f8b":"### Treat Missing Age Data: Method 1 (Linear Regression)\n\nIn this method linear regression is used to predict the missing age data. From the train dataset using the exisiting correlation to age with sex, class, title, number of sibings\/spouse, number of Parents\/children and Fare, age can be estimated. Irrelavent features that have very little correlation to age, and 'Survived' (the final target variable) are excluded in order to prevent bias in the final machine learning step while predicting survivals. ","5e192791":"## Final Predictions and Output","05aa7d32":"**General Distribution of the Data**\n\nTable below shows distribution of the data in various columns in the dataset. The describle method on the dataset shows the mean, standard deviation and distribution by quartiles of the total data available.","f54ab1d2":"For the final predictions, the entire train dataset available at the start, is used to maximise accuracy and minimise any bias, combined with the best parameters obtained from the grid search in the previous section. Only n_estimators requires updating, as the remaining of the best_params are the ones used by default in the current algorithm.","9245cff2":"**Conclusion on Model Selection:**\n\nRandom forest classifier shows the best results, with over 96% accuracy, followed by XGBClassifier and Support Vector Classifier both with nearly 88%. Note that these algorithms have not been tuned\/optimised yet, they are used with their default parameters, yet giving nearly accurate results. ","7ffc7c00":"# Learning and Predicting Survivals\n\nFor the current data, logistic regression, decision tree, support vector machine, KNN and random forest classifiers will be tested and compared to report on the best survival predictions","4846ef53":"## Feature Extraction and Engineering\n\nFirstly, the Title of the passanger is extracted from the name, and added as a separate column","759e1e16":"**Conclusion:**\n\nFrom the comparison, linear method shows more accurate predictions and have smaller absolute mean error. Also linear regression method ensures all missing age data is filled, method 2 could however result in a few data points unfilled if it doesn't find passengers with identical features. Hence linear regression method will be used to fill the missing age data in training and test datasets.","d82f5cbe":"For this method, the missing age data is filled using a custom function that finds a mean of age by matching data in other columns (Class, Sex and number of siblings\/spouse) that are most correlated to the age data. Previously split test data is used for evaluation of the method, and compared to the results from linear regression ","a653e63a":"**Semi-Clean Training Dataframe**\n\nThe dataset is almost ready. However it needs filling or dropping the missing data. As established earlier, the age data can be filled with approximation of age for of those passangers, that are unknown. In the following sections, two methods have been explored and compared to fill the missing age data. The semi-cleaned version of the dataset looks as under.","c53b39de":"Titanic Survival Predictions\n============================\n\nThis notebook contains a thorough machine learning application on predicting the survival of passengers based on the known features during the titanic disaster. Prior to identifying and filling the missing data, some of feature extraction and engineering is conducted to aid determine the best method of filling missing data. Once the data is cleaned and prepared, several classification algorithms are tested and the best performing algorithm is optimised to predict survival results on the test dataset.\n\nThe features \/ columns is the current dataset are as follows:\n\n+ **PassengerId**: Unique identifier of a passenger\n+ **Survived**: 1 indicates that the passenger survived, 0 indicates that the passanger didn't survive during the titanic incident\n+ **Pclass**: Passanger class 1, 2 or 3\n+ **Name**: Name of the passanger\n+ **Sex**: Sex of passanger Male or Female\n+ **Age**: Age of the passanger\n+ **SibSp**: Number of siblings or spouse\n+ **Parch**: Number of parents of children\n+ **Ticket**: Ticket number\n+ **Cabin**: Cabin of the passanger\n+ **Embarked**: Passanger embarked from","ddcc6c68":"With the RFC, the algorithm parameters can be further tuned to obtain the best results. Using the in-built GridSearchCV function from the sklearn library the best parameters can be automatically determined and utilised for the final predictions.","52681929":"The finalised training data is further split into train and test sets to finally select and verify the machine learning algorithm that will be used for prediction of survivals on the acualy test set","492bfbc5":"**Correlation of Features in Dataset**\n\nThe heatmap \/ colourmap below illustrates the correlation of various features in the dataset, so that it is easier to spot and drop irrelavant columns and give emphasis to the most meaningful columns when cleaning and preparing the dataset. ","f532d6b3":"The original test and train dataframes are kept unaltered after extracting the Title column, to later perform data exploraing and visualisation, and extract raw data if necessary. Copies of these dataframes are updated inplace to prepare the data for the machine learning algorithm.","882ebe66":"There are several records missing age data, most records are missing the cabin data, and just one record missing fare data. Based on this, it is ideal to drop the cabin column due to insufficient records to make unbiased or underfitted model, fill the missing age data and finally drop the single record missing the fare data as one record wouldn't add significant value to the predictions. Remaining of the categorical columns are first converted to dummy numeric columns so they can aid predicting and filling the missing age data, and also prepare the dataset for the machine learning algorithm.","048ef4b9":"# Optimising and Predicting with RFC","11b91884":"Certain columns with little correlation are dropped from the clean dataset, and the categorical columns are converted into dummy numeric columns using the get_dummies method in pandas, to establish a numeric\/quantitative relation to the target variable.","887849bc":"# Explore Data","725b1b82":"Shown below is a pairwise relationship between features of the dataset","1855f1fc":"### Treat Missing Age Data: Method 2 (Mean of Age by grouped columns)"}}