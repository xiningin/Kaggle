{"cell_type":{"38c38334":"code","c97c9078":"code","1a94ad0b":"code","93c324dc":"code","84fc9811":"code","e174326c":"code","55cafc0f":"code","1f48be15":"code","09d2dc64":"code","2e3bcc8a":"code","bad0c062":"code","4919fa75":"code","e4be0df5":"code","423bbe87":"code","b1114beb":"code","ed014f72":"code","189527f6":"code","6e099300":"code","a80bda4c":"code","3423153b":"code","e5f5e415":"code","87d8fa3a":"code","4d99140e":"code","e4cdd32d":"code","824b1ccd":"code","7ddc9ba8":"code","87d3706f":"code","ea1302ad":"code","1f312f3f":"code","bd6edef5":"code","e308392b":"code","3bcd31f0":"code","59972d0f":"code","9baf0313":"code","5e607e5a":"code","6e1fa4f3":"code","afb91872":"code","fdee519a":"code","12ae04a2":"code","614b129a":"markdown","b37500b4":"markdown","f501f4bd":"markdown","55dfef25":"markdown","8b2efb9e":"markdown","10dda643":"markdown","e80a8920":"markdown","fedffc5f":"markdown","b76986e9":"markdown","9a7a3ae9":"markdown","bfe64dae":"markdown","6d8e8e85":"markdown","15ff1e41":"markdown","62ed2659":"markdown","fa02016d":"markdown","4ab46a63":"markdown","4d4cb34c":"markdown","f018a23d":"markdown"},"source":{"38c38334":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\ndataset = pd.read_csv('turkiye-student-evaluation_generic.csv')\ndataset.head()","c97c9078":"dataset = pd.read_csv('turkiye-student-evaluation_generic.csv')","1a94ad0b":"dataset.head()","93c324dc":"dataset.describe()","84fc9811":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # visualize\nimport matplotlib.pyplot as plt\n\n","e174326c":"plt.figure(figsize=(18,18))\nsns.heatmap(dataset.corr(),annot = True,fmt = \".2f\",cbar = True)\nplt.xticks(rotation=90)\nplt.yticks(rotation = 0)","55cafc0f":"plt.figure(figsize=(10, 6))\nsns.countplot(x='class', data=dataset)","1f48be15":"plt.figure(figsize=(10,10))\nsns.boxplot(data=dataset.iloc[:,5:33 ])","09d2dc64":"# Calculate mean for each question response for all the classes.\nquestionmeans = []\nclasslist = []\nquestions = []\ntotalplotdata = pd.DataFrame(list(zip(classlist,questions,questionmeans))\n                      ,columns=['class','questions', 'mean'])\nfor class_num in range(1,14):\n    class_data = dataset[(dataset[\"class\"]==class_num)]\n    \n    questionmeans = []\n    classlist = []\n    questions = []\n    \n    for num in range(1,14):\n        questions.append(num)\n    #Class related questions are from Q1 to Q12\n    for col in range(5,17):\n        questionmeans.append(class_data.iloc[:,col].mean())\n    classlist += 12 * [class_num] \n    print(classlist)\n    plotdata = pd.DataFrame(list(zip(classlist,questions,questionmeans))\n                      ,columns=['class','questions', 'mean'])\n    totalplotdata = totalplotdata.append(plotdata, ignore_index=True)\n    ","2e3bcc8a":"plt.figure(figsize=(20, 10))\nsns.pointplot(x=\"questions\", y=\"mean\", data=totalplotdata, hue=\"class\")","bad0c062":"# Calculate mean for each question response for all the classes.\nquestionmeans = []\ninslist = []\nquestions = []\ntotalplotdata = pd.DataFrame(list(zip(inslist,questions,questionmeans))\n                      ,columns=['ins','questions', 'mean'])\nfor ins_num in range(1,4):\n    ins_data = dataset[(dataset[\"instr\"]==ins_num)]\n    questionmeans = []\n    inslist = []\n    questions = []\n    \n    for num in range(13,29):\n        questions.append(num)\n    \n    for col in range(17,33):\n        questionmeans.append(ins_data.iloc[:,col].mean())\n    inslist += 16 * [ins_num] \n    plotdata = pd.DataFrame(list(zip(inslist,questions,questionmeans))\n                      ,columns=['ins','questions', 'mean'])\n    totalplotdata = totalplotdata.append(plotdata, ignore_index=True)\n    plt.figure(figsize=(15, 5))\nsns.pointplot(x=\"questions\", y=\"mean\", data=totalplotdata, hue=\"ins\")","4919fa75":"dataset_questions = dataset.iloc[:,5:]","e4be0df5":"dataset_questions.head()","423bbe87":"import numpy as np\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nX=dataset_questions\npca = PCA().fit(scale(X))\nplt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","b1114beb":"X=dataset_questions\npca = PCA().fit(scale(X))\nplt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","ed014f72":"pca = PCA(n_components = 2)\ndataset_questions_pca = pca.fit_transform(dataset_questions)","189527f6":"print(np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100))\nprint(len(np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)))","6e099300":"np.cumsum(pca.explained_variance_ratio_)*100","a80bda4c":"pca.explained_variance_","3423153b":"from sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 7):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(dataset_questions_pca)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 7), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","e5f5e415":"kmeans = KMeans(n_clusters = 3, init = 'k-means++')\ny_kmeans = kmeans.fit_predict(dataset_questions_pca)\n# Visualising the clusters\nplt.scatter(dataset_questions_pca[y_kmeans == 0, 0], dataset_questions_pca[y_kmeans == 0, 1], s = 100, c = 'yellow', label = 'Cluster 1')\nplt.scatter(dataset_questions_pca[y_kmeans == 1, 0], dataset_questions_pca[y_kmeans == 1, 1], s = 100, c = 'green', label = 'Cluster 2')\nplt.scatter(dataset_questions_pca[y_kmeans == 2, 0], dataset_questions_pca[y_kmeans == 2, 1], s = 100, c = 'red', label = 'Cluster 3')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = 'blue', label = 'Centroids')\nplt.title('Clusters of students')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()\n","87d8fa3a":"import collections\ncollections.Counter(y_kmeans)","4d99140e":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(dataset_questions_pca, method = 'ward'))\n#plt.figure(figsize=(20, 10))\nplt.title('Dendrogram')\nplt.xlabel('questions')\nplt.ylabel('Euclidean distances')\nplt.show()","e4cdd32d":"import scipy.cluster.hierarchy as sch\n\ndendrogram = sch.dendrogram(sch.linkage(dataset_questions_pca, method = 'centroid'))\n#plt.figure(figsize=(20, 10))\nplt.title('Dendrogram')\nplt.xlabel('questions')\nplt.ylabel('Euclidean distances')\nplt.show()","824b1ccd":"# Fitting Hierarchical Clustering to the dataset\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(dataset_questions_pca)\nX = dataset_questions_pca\n# Visualising the clusters\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'yellow', label = 'Cluster 1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'red', label = 'Cluster 2')\nplt.title('Clusters of STUDENTS')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()","7ddc9ba8":"# Let me check the count of students in each cluster\nimport collections\ncollections.Counter(y_hc)","87d3706f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","ea1302ad":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nimport pandas as pd\nimport numpy as np\n#from sklearn.datasets import load_iris\nfrom sklearn.decomposition import FactorAnalysis\nfactor = FactorAnalysis(n_components=2, random_state=101).fit(X)\npd.DataFrame(factor.components_)","1f312f3f":"dataset.head(2)","bd6edef5":"from sklearn.preprocessing import scale\n","e308392b":"data=scale(dataset.iloc[:,3:])\ndata\n\n","3bcd31f0":"from pylab import * \nimport pandas as pd\nimport numpy as np   \nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB  \nimport sklearn.metrics as sm  \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom sklearn.naive_bayes import GaussianNB  \nfrom sklearn import tree, svm\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.under_sampling import ClusterCentroids\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import RandomOverSampler","59972d0f":"seed = 7\ntest_size = 0.15\nData=data\nTarget=dataset.iloc[:,2]\nClasses=Target","9baf0313":"Naive_Bayes_classifier = GaussianNB() \nDTC = tree.DecisionTreeClassifier()\nLR = LogisticRegression() \nSVM_Classifier = svm.SVC() \nKNN = KNeighborsClassifier(n_neighbors=5)  \nRF=RandomForestClassifier(n_estimators=25)\nX_train, X_test, y_train, y_test = train_test_split(Data, Target, test_size=test_size, random_state=seed)\nTrained_Classifier  =RF.fit(X_train, y_train)  \n\nTested_Classifier = Trained_Classifier.predict(X_test)  \n  \n# Compute the confusion matrix and calculation the performance creteria   \nP=sm.confusion_matrix(y_test,Tested_Classifier)  \nAccuracy = ((P[0,0] + P[1,1]+P[2,2])\/float(sum(P)))*100  \nsensitivity = (P[0,0] \/ float(sum(P[:,0])))*100  \nspecificity = (P[1,1] \/ float(sum(P[:,1])))*100  \nPrecision =(P[0,0] \/ float(sum(P[0,:])))*100 # TP\/(TP+FP)\nF_measure=(2*sensitivity*Precision )\/(sensitivity+Precision)\n\nprint(\" Accuracy = %s\" % Accuracy, \" Sensitivity = %s\" % sensitivity,\" Specificity = %s\" % specificity)  \nprint(\" Precision = %s\" % Precision ,\"F-measure= %s\"%F_measure)  \n\n             ","5e607e5a":"Naive_Bayes_classifier = GaussianNB() \nDTC = tree.DecisionTreeClassifier()\nLR = LogisticRegression() \nSVM_Classifier = svm.SVC() \nKNN = KNeighborsClassifier(n_neighbors=5)  \nRF=RandomForestClassifier(n_estimators=25)\nX_train, X_test, y_train, y_test = train_test_split(Data, Target, test_size=test_size, random_state=seed)\nTrained_Classifier = LR.fit(X_train, y_train)  \n\nTested_Classifier = Trained_Classifier.predict(X_test)  \n  \n# Compute the confusion matrix and calculation the performance creteria   \nP=sm.confusion_matrix(y_test,Tested_Classifier)  \nAccuracy = ((P[0,0] + P[1,1]+P[2,2])\/float(sum(P)))*100  \nsensitivity = (P[0,0] \/ float(sum(P[:,0])))*100  \nspecificity = (P[1,1] \/ float(sum(P[:,1])))*100  \nPrecision =(P[0,0] \/ float(sum(P[0,:])))*100 # TP\/(TP+FP)\nF_measure=(2*sensitivity*Precision )\/(sensitivity+Precision)\n\nprint(\" Accuracy = %s\" % Accuracy, \" Sensitivity = %s\" % sensitivity,\" Specificity = %s\" % specificity)  \nprint(\" Precision = %s\" % Precision ,\"F-measure= %s\"%F_measure)  \n","6e1fa4f3":"print('original data check balance: ',sorted(Counter(Target).items()))","afb91872":"plt.figure(figsize=(10, 6))\nsns.countplot(x=Target, data=dataset)\npercentage=print('Class1=',4909*100\/(5820),'Class2=' ,576*100\/(5820),'Class3=',335*100\/(5820))","fdee519a":"cc = ClusterCentroids(random_state=0)\nsmote_tomek = SMOTETomek(random_state=0)\nData1, Classes1 = smote_tomek.fit_sample(Data, Classes)\nprint(sorted(Counter(Classes1).items()))\nX_train, X_test, y_train, y_test = train_test_split(Data, Classes, test_size=test_size, random_state=seed)\nprint('modified data check balance using SMOTETomek: ',sorted(Counter(Classes1).items()))\nData2, Classes2 = cc.fit_sample(Data, Classes)\nprint('modified data check balance using undrsampling: ',sorted(Counter(Classes2).items()))\nros = RandomOverSampler(random_state=0)\nData3, Classes3 = ros.fit_sample(Data, Classes)\nprint('modified data check balance using oversampling: ',sorted(Counter(Classes3).items()))","12ae04a2":"seed = 7\ntest_size = 0.15\n\n\nNaive_Bayes_classifier = GaussianNB() \nDTC = tree.DecisionTreeClassifier()\nLR = LogisticRegression() \nSVM_Classifier = svm.SVC() \nKNN = KNeighborsClassifier(n_neighbors=5)  \nRF=RandomForestClassifier(n_estimators=25)\nX_train, X_test, y_train, y_test = train_test_split(Data1, Classes1, test_size=test_size, random_state=seed)\nTrained_Classifier  =RF.fit(X_train, y_train)  \nTested_Classifier = Trained_Classifier.predict(X_test)  \n  \n# Compute the confusion matrix and calculation the performance creteria   \nP=sm.confusion_matrix(y_test,Tested_Classifier)  \nAccuracy = ((P[0,0] + P[1,1]+P[2,2])\/float(sum(P)))*100  \nsensitivity = (P[0,0] \/ float(sum(P[:,0])))*100  \nspecificity = (P[1,1] \/ float(sum(P[:,1])))*100  \nPrecision =(P[0,0] \/ float(sum(P[0,:])))*100 # TP\/(TP+FP)\nF_measure=(2*sensitivity*Precision )\/(sensitivity+Precision)\n\nprint(\" Accuracy = %s\" % Accuracy, \" Sensitivity = %s\" % sensitivity,\" Specificity = %s\" % specificity)  \nprint(\" Precision = %s\" % Precision ,\"F-measure= %s\"%F_measure)  \n","614b129a":"# Try","b37500b4":"## Lets understand the students have responded for the questions against classes","f501f4bd":"### So we have 2358 students who have given negative ratings overall , 2222 students with positive ratings and 1240 students with nuetral response","55dfef25":"### Lets see how rating has been given against instructor wise.","8b2efb9e":"### Looking at the above graph , i see we have 3 clusters of students who have given like Negative, Neutral and Positive feedback","10dda643":"# PCA Analysis ","e80a8920":"# Clustring ","fedffc5f":"### Below Graph to see how the rating has been given by student for each questions","b76986e9":"# Factor analysis ","9a7a3ae9":"### By above graph, we can see that very less students have given completely disagree (Rating 1) for Question Q14, Q15, Q17, Q19 - Q22, Q25,Q28","bfe64dae":"#### Lets try to cluster all the students based on the Question responses data.","6d8e8e85":"## based on the Elbow graph , we can go for 3 clusters.","15ff1e41":"## To understand which course got most responde","62ed2659":"### Variance (% cumulative) explained by the principal components","fa02016d":"###  Eiegenvalues","4ab46a63":"##  Using the dendrogram to find the optimal number of clusters","4d4cb34c":"# we have biased data, so I am going to resolve it","f018a23d":"### Lets begin to cluster the students based on the questionaire data"}}