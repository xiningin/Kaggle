{"cell_type":{"27ce2ce9":"code","80ff4d42":"code","f4797519":"code","63539bff":"code","03b7185e":"code","6946df7b":"code","b6dddbbf":"code","112a0566":"code","bacb49f5":"code","09ea6dd5":"code","ad8bdc24":"code","b49f3eef":"code","1e14743f":"code","86280f33":"code","f78fdbb5":"code","7822aec7":"code","989f5f68":"code","a4030994":"code","331878b6":"code","ed159a50":"code","5fa6e85d":"code","d2716750":"code","fb0491e2":"code","e6f3711c":"code","2bf234ff":"code","c047689f":"code","fafc693d":"code","e94df51e":"code","229ff5d9":"code","390bbcb4":"code","2cb69da8":"code","5130d797":"code","05d9c314":"code","334fbd52":"code","f6b1863b":"code","b1ffa4e4":"code","fd31178e":"code","277f5d54":"code","5fbb3cab":"code","47af274a":"code","e0966b67":"code","f4a263d0":"code","8a2afd92":"code","ca65d905":"code","ff6b1f50":"code","c4a675db":"code","a50b9b76":"markdown","aa62a140":"markdown","e6f748c7":"markdown","7ae97c50":"markdown","c6119855":"markdown","794081d1":"markdown","b37869ea":"markdown","f0d138bb":"markdown","2098f199":"markdown","596d9e2e":"markdown","a20a8ea3":"markdown","edbbff5c":"markdown","66d5f696":"markdown","e50416f2":"markdown","6536c6d5":"markdown","8ae10194":"markdown","ea4c5443":"markdown","08f68d9d":"markdown","eafd2aa4":"markdown","fb88cc62":"markdown","c0419532":"markdown","53a84428":"markdown","bad90b31":"markdown","df5a17c9":"markdown","230f32b6":"markdown","6bef8fc9":"markdown","b9ede0bb":"markdown","ac25d820":"markdown","4bbad473":"markdown","56e5a01d":"markdown","a3291c23":"markdown","98b20ba9":"markdown","21a1ddd6":"markdown","3010d12c":"markdown","de749741":"markdown"},"source":{"27ce2ce9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80ff4d42":"import matplotlib.pyplot as plt\nimport seaborn as sns","f4797519":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/capture\/Capture1.PNG\"))","63539bff":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/capture2\/Capture.PNG\"))","03b7185e":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/capture3\/Capture2.PNG\"))","6946df7b":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/capture4\/Capture3.PNG\"))","b6dddbbf":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/capture5\/Capture5.PNG\"))","112a0566":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/capture6\/Capture6.PNG\"))","bacb49f5":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/capture9\/Capture9.PNG\"))","09ea6dd5":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/gradient\/gradient.jpg\"))","ad8bdc24":"df=pd.read_csv(\"..\/input\/bankdata\/Churn_Modelling.csv\")\ndf\n","b49f3eef":"df.isnull().sum()","1e14743f":"df.drop([\"RowNumber\",\"CustomerId\",\"Surname\"],axis=1,inplace=True)\ndf","86280f33":"from sklearn.preprocessing import LabelEncoder","f78fdbb5":"le=LabelEncoder()","7822aec7":"df[\"Gender\"]=le.fit_transform(df[\"Gender\"])\ndf","989f5f68":"df=pd.get_dummies(data=df,columns=[\"Geography\"],drop_first=True)\ndf","a4030994":"help(pd.get_dummies)","331878b6":"from sklearn.model_selection import train_test_split","ed159a50":"X=df.drop(\"Exited\",axis=1)\nX","5fa6e85d":"X=X.values\nX","d2716750":"y=df[\"Exited\"]\ny","fb0491e2":"y=y.values\ny","e6f3711c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","2bf234ff":"X_train.shape","c047689f":"y_train.shape","fafc693d":"from sklearn.preprocessing import StandardScaler","e94df51e":"sc=StandardScaler()","229ff5d9":"X_train=sc.fit_transform(X_train)\nX_train","390bbcb4":"X_test=sc.transform(X_test)\nX_test","2cb69da8":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","5130d797":"ann=Sequential()","05d9c314":"ann.add(Dense(6, activation=\"relu\")) # here we add a dense layer with 6 neurons among 11 features\n","334fbd52":"ann.add(Dense(units=6, activation=\"relu\")) # here we add a dense layer with 6 neurons among 11 features\n","f6b1863b":"\nann.add(Dense(1,activation=\"sigmoid\")) \n# here we add a dthe fina layer with 1 neurons because we have one output, that is the customer will exit or not","b1ffa4e4":"ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n#Here assign adam optimizer as our optimizer and mean squared error as our loss function for our deep learning model","fd31178e":"ann.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))\n#Here we fit our model into the training X and y set with batch_size 128 and 300 epaochs, and we use also test dataset as validation","277f5d54":"ann.history.history","5fbb3cab":"pd.DataFrame(ann.history.history)\n#Here we can see losses in both our loss function and validation loss in the test data ","47af274a":"pd.DataFrame(ann.history.history).plot(figsize=(15,10))","e0966b67":"ann.evaluate(X_train, y_train)","f4a263d0":"ann.evaluate(X_test, y_test)","8a2afd92":"predictions=ann.predict(X_test)\npredictions","ca65d905":"predictions_df=pd.DataFrame(predictions, columns=[\"Pred\"])\npredictions_df.head()","ff6b1f50":"y_test_df=pd.DataFrame(y_test, columns=[\"Exited\"])\ny_test_df.head()","c4a675db":"comparison_df=pd.concat([predictions_df, y_test_df], axis=1)\ncomparison_df.head()","a50b9b76":"Adding the Output Layer:","aa62a140":"# 3. Data Preprocessing:","e6f748c7":"<font color=\"blue\">\n    \ncompile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, **kwargs) method of tensorflow.python.keras.engine.sequential.Sequential instance Configures the model for training.\n\nArguments:\n    optimizer:this determines how we want to perform the gradient descent like adam optimizer\n    loss= represents cost function we want to use","7ae97c50":"# 2. How Neural Networks Learn?:","c6119855":"<font color=\"blue\">\nDeep Learning models can be used for a variety of complex tasks:\n    \n\n1.Artificial Neural Networks for Regression and Classification\n    \n2.Convolutional Neural Networks for Computer Vision:\n    Computer Vision, often abbreviated as CV, is defined as a field of study that seeks to develop techniques to help computers \u201csee\u201d and understand the content of digital images such as photographs and videos.\n    \n3.Recurrent Neural Networks for Time Series Analysis\n    Music generation\n    \n   Machine translation\n    \n4.Self Organizing Maps for Feature Extraction\n    \n5.Deep Boltzmann Machines for Recommendation Systems\n    \n6.Auto Encoders for Recommendation Systems","794081d1":"Adding Input Layer and First Layer:","b37869ea":"# 4. Building the ANN:","f0d138bb":"<font color=\"blue\">\n    \nWe want to eliminate all other weights except for the one right at the bottom of the U-shape, the one closest to 0.\n\nThe closer to 0, the lower the difference between output value and actual value.\n\nThat is where we find our optimal weight.\n\nYou could find your way to the best weight through a simple process of elimination. You could trial every weight through your network and one-by-one get closer and closer to your optimal weight. On a simple level, this would suffice, say if you only had a single weight to optimize. But the larger a network becomes, the number of weights that will emerge means this method is impracticable.\n\nYou could have an almost infinite amount of weights to adjust. Therefore a process of elimination would be like looking for the prettiest grain of sand on a beach. Good luck with that.\n\nTo put a mind-bending timescale on it, an advanced supercomputer would need a longer time than the universe has existed to find all the optimal weights in a mildly complex Neural Network, when using this process of elimination.","2098f199":"<font color=\"blue\">\n    \nInstead of going through every weight one at a time, and ticking every wrong weight off as you go, you instead look at the angle of the cost function line.\n\nIf the slope is negative, like it is from the highest red dot on the above line, that means you must go downhill from there.","596d9e2e":"<font color=\"blue\">\n    \nWe begin with the four variables on the left and the top neuron of the hidden layer in the middle.\n\nAll four variable all be connected to the neuron by synapses.\n\nHowever, not all of the synapses are weighted. They will either have a 0 value or non 0 value.\nThe former indicates importance while the latter means they will be discarded.\n\nFor instance, the Area and Distance variable may be valued as non 0. Which means they are weighted. This means they matter. The other two variables, Bedrooms and Age, aren\u2019t weighted and so are not considered by that first neuron. Got it? Good.\n\nYou may wonder why that first neuron is only considering two of the four variables.\nIn this case, it is common on the property market that larger homes become cheaper the further they are from the city.\n\nThat\u2019s a basic fact. So what this neuron may be doing is looking specifically for properties that are large but are not so far from the city.\n\nThis is where the power of the Neural Network comes from. There are many of these neurons, each making similar calculations with different combinations of these variables.\n\nThe next neuron down may have weighted synapses for Area, Bedroom and Age. It may have been trained up in a specific area or city where there is a high number of families but where many of the properties are new. New properties are often more expensive than old ones.\n\nThe way these neurons work and interact means the network itself is extremely flexible, allowing it to look for specific things and therefore make a comprehensive search for whatever it is they have been trained to identify.\n\n****","a20a8ea3":"<font color=\"blue\">\n    \nThe activation function is a mathematical \u201cgate\u201d in between the input feeding the current neuron and its output going to the next layer. It can be as simple as a step function that turns the neuron output on and off, depending on a rule or threshold. Or it can be a transformation that maps the input signals into output signals that are needed for the neural network to function.","edbbff5c":"3.Training ANN on the Training Set:","66d5f696":"<font color=\"blue\">\n    \nChoosing Right Loss Function\n\nLoss function is a mathematical way of measuring how wrong your predictions are.\n\nKeep in mind what kind of problem you are trying to solve:\n\nFor a multi-class classification problem\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\nFor a binary classification problem\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nFor a mean squared error regression problem\nmodel.compile(optimizer='rmsprop', loss='mse')","e50416f2":"Adding Second Layer:","6536c6d5":"# 1. How Neural Networks Work?:","8ae10194":"Type of Optimizers:\n\n1.Gradient Descent\n\n2.Stochastic Gradient Descent\n\n3.Mini-Batch Gradient Descent\n\n4.Adam:The intuition behind the Adam is that we don\u2019t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.","ea4c5443":"<font color=\"blue\">\n    \nTwo ways to adjust the weights:\n    \n1. The brute-force approach. This is far better suited to a single-layer feet-forward network. Here you take a number of possible weights. On a graph it looks like this:","08f68d9d":"<font color=\"blue\">\n    \nThe input variables do not change.\n    \nIt is only the weight of the synapses that alters after the cost function comes into play.\n\nIf you have thirty students the Y \/ \u0176 comparison will occur thirty times in each smaller network but the cost function will be applied to all of them together.","eafd2aa4":"<font color=\"blue\">\n In this simple example we have four variables:\n\nArea (feet sq)\nNumber of bedrooms\nDistance to city (miles)\nAge of property\n\nTheir values go through the weighted synapses straight over to the output layer.","fb88cc62":"<font color=\"blue\">\n    \nMaking Predictions:\nNow we will predict the y_test and compare them with the real values","c0419532":"<font color=\"blue\">\n\nWhen input variables go along the synapses and into the neuron, where an activation function is applied, the resulting data is the output value, \u0176.\n    \nWe are going to replace that with \u0176, which represents the output value.\n\nThe difference between Y and \u0176 is at the core of this entire process.\n\nIn order for our Network to learn we need to compare the output value with the actual value.\nThere will be a difference between the two.\n\nThen we apply what is called a cost function (Cost function - Wikipedia), which is one half of the squared difference between the output value and actual value.\n\nThe cost function tells us the error in our prediction.\n    \nOur aim is to minimize the cost function. The lower the cost function, the closer \u0176 is to Y, and hence, the closer our output value to our actual value.\n\nA lower cost function means higher accuracy for our Network. Once we have our cost function, a recycling process begins.\n\nAs long as there exists a disparity between Y and \u0176, we will need to adjust those weights. \n\nWe feed the resulting data back through the entire Neural Network. The weighted synapses connecting the input variables to the neuron are the only thing we have any control over.\n\nWe need to repeat this until we scrub the cost function down to as small a number as possible, as close to 0 as it will go.\n\nWhen the output value and actual value are almost touching we know we have optimal weights and can therefore proceed to the testing phase, or application phase.\n\n","53a84428":"# 2.1. Cost Function:","bad90b31":"#Feature Scaling:","df5a17c9":"# 2.2. Activation Function:","230f32b6":"#Splitting Data as Train and Test Set:","6bef8fc9":"<font color=\"blue\">\n    \nBackpropagation:\n    \nThis is when you feed the end data back through the Neural Network and then adjust the weighted synapses between the input value and the neuron.\n\nBy repeating this cycle and adjusting the weights accordingly, you reduce the cost function.","b9ede0bb":"<font color=\"blue\">\n    \nChoosing an optimizer: Keep in mind what kind of problem we are trying to solve:\n\nFor a multi-class classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\nFor a binary classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nFor a mean squared error regression problem:\n\nmodel.compile(optimizer='rmsprop', loss='mse')","ac25d820":"2. Compiling the ANN:","4bbad473":"<font color=\"blue\">\nPower Up Neural Networks with Hidden Layers:\n    \nYou can implement a hidden layer that sits between the input and output layers.\n","56e5a01d":"# 2.3. Gradient Descend:","a3291c23":"<font color=\"blue\">\n    \nWe feed the variables through the weighted synapses and the neuron to calculate our output value, \u0176.\n\nThen the cost function is applied and the data goes in reverse through the Neural Network.\n\nIf there is a disparity between Y and \u0176 then the weights will be adjusted and the process can begin all over again. Rinse and repeat until the cost function is minimized.","98b20ba9":"<font color=\"blue\">\n    \nTypes of Activation Function:\n\n1.Sigmoid \/ Logistic\n\n2.TanH \/ Hyperbolic Tangent\n\n3.ReLU (Rectified Linear Unit)\n\n4.Softmax","21a1ddd6":"<font color=\"blue\">\n\nFrom this new cobweb of arrows, representing the synapses, we begin to understand how these factors, in differing combinations, cast a wider net of possibilities. ","3010d12c":"4. Making Predictions and Evaluating the Performance of the ANN:","de749741":"*<font color=\"blue\">\n\n3.Backpropagation:\n\nBackpropagation is an advanced algorithm, driven by sophisticated mathematics, which allows us to adjust all the weights in our Neural Network.\n\nThis is important because it is through the manipulation of weights that we bring the output value (the value produced by our Neural Network) and the actual value closer together.\n\nThese two values need to be as close as possible."}}