{"cell_type":{"98a28b71":"code","710d9dfa":"code","d98f703a":"code","cbd0c4cd":"code","0057f008":"code","59d25a54":"code","1fa7f3d5":"code","8598a97f":"code","97220138":"code","d43a5bee":"code","ad517e42":"code","d9fd0740":"code","6ca201c6":"code","8db7c724":"code","0bdae63b":"code","053680e9":"code","aa8c1e64":"code","e7ad43cb":"code","927b51fa":"code","0d0b5877":"code","38425b96":"code","523f31de":"code","342a6c50":"code","d9cefaa1":"code","a67a2626":"code","a6386a11":"code","6d91d07c":"code","c7bf85e6":"code","84ef4863":"code","99f72b88":"code","a835d888":"code","672d68f7":"code","74e5a8f8":"markdown","a3031a7a":"markdown","2025cb1c":"markdown","96882611":"markdown","90f50ae5":"markdown","95c2a0a8":"markdown","d40f9400":"markdown","e04ca815":"markdown","ddc83d46":"markdown","8d97d033":"markdown","78240188":"markdown"},"source":{"98a28b71":"#!pip install split-folders --upgrade --quiet","710d9dfa":"import os\nimport torch\nimport torchvision\nimport tarfile\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.datasets.utils import download_url\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as T\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nfrom tqdm.notebook import tqdm\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'","d98f703a":"IMAGE_SIZE = 128\nBATCH_SIZE = 32\nImageDir = \"..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\"","cbd0c4cd":"#import splitfolders\n#splitfolders.ratio(ImageDir, output=\"Train-Val\", ratio = (0.85,0.15))","0057f008":"# Data transforms and normalization\nstats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\ntrain_tfms = T.Compose([\n    T.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    T.RandomRotation(18),\n    T.ToTensor(),\n    T.Normalize(*stats, inplace=True)\n])\n\ntest_tfms = T.Compose([\n    T.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n    T.ToTensor(),\n    T.Normalize(*stats, inplace=True)\n])","59d25a54":"train_ds = ImageFolder(\".\/Train-Val\/train\",train_tfms)\nval_ds = ImageFolder(\".\/Train-Val\/val\",test_tfms)","1fa7f3d5":"classes = train_ds.classes\nlen_classes = len(classes)\nlen_classes","8598a97f":"train_ds","97220138":"train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle = True, num_workers = 3, pin_memory = True)\nval_dl = DataLoader(val_ds, BATCH_SIZE, num_workers = 3, pin_memory = True)","d43a5bee":"def denormalize(images, means, stds):\n    means = torch.tensor(means).reshape(1, 3, 1, 1)\n    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n    return images * stds + means\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        denorm_images = denormalize(images, *stats)\n        ax.imshow(make_grid(denorm_images[:32], nrow=8).permute(1, 2, 0).clamp(0,1))\n        break","ad517e42":"show_batch(train_dl)","d9fd0740":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","6ca201c6":"device = get_default_device()\ndevice","8db7c724":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)","0bdae63b":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","053680e9":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 32)\n        self.conv2 = conv_block(32, 64, pool=True)\n        self.res1 = nn.Sequential(conv_block(64,64), conv_block(64,64))\n        \n        self.conv3 = conv_block(64, 128, pool=True)\n        self.conv4 = conv_block(128, 256, pool=True)\n        self.res2 = nn.Sequential(conv_block(256,256), conv_block(256,256))\n        \n        self.conv5 = conv_block(256,512, pool = True)\n        self.conv6 = conv_block(512,512, pool = True)\n        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n        \n        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n                                        nn.Flatten(), \n                                        nn.Dropout(0.2),\n                                        nn.Linear(512, num_classes))\n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.conv5(out)\n        out = self.conv6(out)\n        out = self.res3(out) + out\n        out = self.classifier(out)\n        return out","aa8c1e64":"model = to_device(ResNet9(3, 29), device)\nmodel","e7ad43cb":"torch.cuda.empty_cache()\nfor images,labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model.conv1(images)\n    out = model.conv2(out)\n    out = model.res1(out)\n    out = model.conv3(out)\n    out = model.conv4(out)\n    out = model.res2(out)\n    out = model.conv5(out)\n    out = model.conv6(out)\n    out = model.res3(out)\n    out = model.classifier(out)\n   \n    print('out.shape:', out.shape)\n    break","927b51fa":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","0d0b5877":"# without training\nhistory = [evaluate(model, val_dl)]\nhistory","38425b96":"epochs = 8\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","523f31de":"import time\nstart = time.time()\n\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)\n\nend = time.time()\n\nprint(f\"Finished training in {(end-start):.2f} seconds.\")","342a6c50":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","d9cefaa1":"plot_accuracies(history)","a67a2626":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","a6386a11":"plot_losses(history)","6d91d07c":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","c7bf85e6":"plot_lrs(history)","84ef4863":"test_ds = ImageFolder(\"..\/input\/asl-alphabet\/asl_alphabet_test\",test_tfms)","99f72b88":"def pred_image(img, model):\n    \n    img = to_device(img.unsqueeze(0), device)\n    img_pred = model(img)\n    \n    _, pred = torch.max(img_pred, dim = 1)\n    \n    return classes[pred[0].item()]\n\n\ndef denormalizeTest(images, means, stds):\n    means = torch.tensor(means).reshape(3, 1, 1)\n    stds = torch.tensor(stds).reshape(3, 1, 1)\n    return images * stds + means","a835d888":"plt.figure(figsize=[20,16])\nfor i in range(len(test_ds)):\n    img, _ = test_ds[i]\n    pred = pred_image(img, model)\n    \n    plt.subplot(7,4,i+1)\n    img = denormalizeTest(img, *stats).permute(1,2,0)\n    plt.imshow(img)\n    plt.title(f\"Prdicted : {pred}\")\n    plt.axis(\"off\")\nplt.show()","672d68f7":"torch.save(model, 'SLA-model.pth')","74e5a8f8":"## Model with Residual Blocks and Batch Normalization\n\nOne of the key changes to our CNN model is the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers.\n\n![](https:\/\/miro.medium.com\/max\/1140\/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n","a3031a7a":"## Visualization","2025cb1c":"## Training","96882611":"**Training Process**","90f50ae5":"## Libraries and Data Generation","95c2a0a8":"## Using a GPU","d40f9400":"## Plots","e04ca815":"### Save the model","ddc83d46":"## **Testing with Individual Images**","8d97d033":"### Preparing the Dataset","78240188":"##  American Sign Language\n<p>\nThe training data set contains 87,000 images which are 200x200 pixels. There are 29 classes, of which 26 are for the letters A-Z and 3 classes for SPACE, DELETE and NOTHING.\nThese 3 classes are very helpful in real-time applications, and classification.\nThe test data set contains a mere 29 images, to encourage the use of real-world test images.\n<\/p>"}}