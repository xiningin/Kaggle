{"cell_type":{"5e45327d":"code","6f996f3c":"code","39ecf2d8":"code","e371723b":"code","fab0419a":"code","eb549097":"code","e39b4b4f":"code","710750e6":"code","94564bf9":"code","d53b9efc":"code","023d83f5":"code","4853cc69":"code","1d29a16f":"code","62ba6b4f":"code","90f1f3b7":"code","78cf9239":"code","45cda8f1":"code","731fb271":"code","01ca8bfd":"code","b0d245f7":"code","fa5263c3":"code","b6e04c01":"code","5dd42f5a":"code","73953aba":"code","5080f03a":"code","0303218e":"code","4d0d7b0b":"code","cb9e0356":"code","9a6c79f6":"code","991358ce":"code","db110e29":"code","55020157":"code","9fc8764c":"code","ec3f5463":"code","4d21f4fe":"code","cdd7a4f4":"code","2ce6c006":"code","d3051989":"code","dfa77a6c":"code","19091722":"code","369307d9":"code","844c7acd":"code","a8f7b998":"code","135e195f":"code","44ccecc3":"code","95121d9b":"code","64d7a772":"code","f2017525":"markdown","2a6f83f0":"markdown","61c3d8f3":"markdown","8b6db736":"markdown","c6930a44":"markdown","f7576634":"markdown","aa07962a":"markdown","be73a778":"markdown","a276e521":"markdown"},"source":{"5e45327d":"!pip install -U cvxopt","6f996f3c":"from __future__ import print_function\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nfrom sklearn.svm import SVC\n\nfrom cvxopt import matrix, solvers","39ecf2d8":"np.random.seed(22)\n\nmeans = [[2, 2], [4, 2]]\ncov = [[.3, .2], [.2, .3]]\nN = 20\nX0 = np.random.multivariate_normal(means[0], cov, N) # class 1\nX1 = np.random.multivariate_normal(means[1], cov, N) # class -1 \nX = np.concatenate((X0.T, X1.T), axis = 1) # all data \ny = np.concatenate((np.ones((1, N)), -1*np.ones((1, N))), axis = 1) # labels ","e371723b":"y","fab0419a":"X","eb549097":"plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\nplt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\nplt.axis('equal')\nplt.plot()\nplt.show()","e39b4b4f":"# build K\nV = np.concatenate((X0.T, -X1.T), axis = 1)\nK = matrix(V.T.dot(V)) # see definition of V, K near eq (8)\n\np = matrix(-np.ones((2*N, 1))) # all-one vector \n# build A, b, G, h \nG = matrix(-np.eye(2*N)) # for all lambda_n >= 0\nh = matrix(np.zeros((2*N, 1)))\nA = matrix(y) # the equality constrain is actually y^T lambda = 0\nb = matrix(np.zeros((1, 1))) \nsolvers.options['show_progress'] = False\nsol = solvers.qp(K, p, G, h, A, b)\n\nl = np.array(sol['x'])\nprint('lambda = ')\nprint(l.T)","710750e6":"epsilon = 1e-6 # just a small number, greater than 1e-9\nS = np.where(l > epsilon)[0]\n\nVS = V[:, S]\nXS = X[:, S]\nyS = y[:, S]\nlS = l[S]\n# calculate w and b\nw = VS.dot(lS)\nb = np.mean(yS.T - w.T.dot(XS))\n\nprint('w = ', w.T)\nprint('b = ', b)","94564bf9":"# w1*x1 + w2*x2 + b = 0\nsepX1 = np.linspace(2.3, 3.3, 100)\nsepX2 = -b\/w[1] - w[0]*sepX1\/w[1]","d53b9efc":"plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\nplt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\nplt.plot(sepX1, sepX2, '-r', label=\"%f*x1 + %f*x2 + %f = 0\"%(w[0], w[1], b))\nplt.axis('equal')\nplt.plot()\nplt.show()","023d83f5":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n# list of points \nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nfrom matplotlib.backends.backend_pdf import PdfPages\nnp.random.seed(22)\n\nmeans = [[2, 2], [4, 2]]\ncov = [[.7, 0], [0, .7]]\nN = 20\nX0 = np.random.multivariate_normal(means[0], cov, N) # each row is a data point \nX1 = np.random.multivariate_normal(means[1], cov, N)","4853cc69":"#with PdfPages('data.pdf') as pdf:\nplt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = 1)\nplt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = 1)\nplt.axis('equal')\nplt.ylim(0, 4)\nplt.xlim(0, 5)\n\n# hide tikcs \ncur_axes = plt.gca()\ncur_axes.axes.get_xaxis().set_ticks([])\ncur_axes.axes.get_yaxis().set_ticks([])\n\nplt.xlabel('$x_1$', fontsize = 20)\nplt.ylabel('$x_2$', fontsize = 20)\n    #pdf.savefig()\n    # plt.savefig('logistic_2d.png', bbox_inches='tight', dpi = 300)\nplt.show()","1d29a16f":"X = np.vstack((X0, X1))\ny = np.vstack((np.ones((N,1 )), -np.ones((N,1 )))).reshape((2*N,))","62ba6b4f":"C = 100\nclf = SVC(kernel = 'linear', C = C)\nclf.fit(X, y) \n\nw_sklearn = clf.coef_.reshape(-1, 1)\nb_sklearn = clf.intercept_[0]","90f1f3b7":"print(w_sklearn.T, b_sklearn)","78cf9239":"from cvxopt import matrix, solvers\n# build K\nV = np.concatenate((X0.T, -X1.T), axis = 1)\nK = matrix(V.T.dot(V))\n\np = matrix(-np.ones((2*N, 1)))\n# build A, b, G, h \nG = matrix(np.vstack((-np.eye(2*N), np.eye(2*N))))\n\nh = matrix(np.vstack((np.zeros((2*N, 1)), C*np.ones((2*N, 1)))))\nA = matrix(y.reshape((-1, 2*N))) \nb = matrix(np.zeros((1, 1))) \nsolvers.options['show_progress'] = False\nsol = solvers.qp(K, p, G, h, A, b)\n\nl = np.array(sol['x'])\nprint('lambda = \\n', l.T)","45cda8f1":"l.T","731fb271":"S = np.where(l > 1e-5)[0]\nS2 = np.where(l < .99*C)[0]\n\nM = [val for val in S if val in S2] # intersection of two lists","01ca8bfd":"XT = X.T # we need each col is one data point in this alg\nVS = V[:, S]\n# XS = XT[:, S]\n# yS = y[ S]\nlS = l[S]\n# lM = l[M]\nyM = y[M]\nXM = XT[:, M]\nw_dual = VS.dot(lS).reshape(-1, 1)\nb_dual = np.mean(yM.T - w_dual.T.dot(XM))","b0d245f7":"print(w_dual.T, b_dual) ","fa5263c3":"def cost(w, lam):\n    u = w.T.dot(Z) # as in (23)\n    return (np.sum(np.maximum(0, 1 - u)) + \\\n            .5*lam*np.sum(w*w)) - .5*lam*w[-1]*w[-1]\n\ndef grad(w, lam):\n    u = w.T.dot(Z) # as in (23)\n    H = np.where(u < 1)[1]\n    ZS = Z[:, H]\n    g = (-np.sum(ZS, axis = 1, keepdims = True) + lam*w)\n    g[-1] -= lam*w[-1]\n    return g\n\neps = 1e-6\ndef num_grad(w):\n    g = np.zeros_like(w)\n    for i in range(len(w)):\n        wp = w.copy()\n        wm = w.copy()\n        wp[i] += eps \n        wm[i] -= eps \n        g[i] = (cost(wp, lam) - cost(wm, lam))\/(2*eps)\n    return g ","b6e04c01":"def grad_descent(w0, eta, lam):\n    w = w0\n    it = 0 \n    while it < 100000:\n        it = it + 1\n        g = grad(w, lam)\n        w -= eta*g\n        if (it % 10000) == 1:\n            print('iter %d' %it + ' cost: %f' %cost(w, lam))\n        if np.linalg.norm(g) < 1e-5:\n            break \n    return w ","5dd42f5a":"X0_bar = np.vstack((X0.T, np.ones((1, N)))) # extended data\nX1_bar = np.vstack((X1.T, np.ones((1, N)))) # extended data \n\nZ = np.hstack((X0_bar, - X1_bar)) # as in (22)\nlam = 1.\/C","73953aba":"w0 = np.random.randn(X0_bar.shape[0], 1) \ng1 = grad(w0, lam)\ng2 = num_grad(w0)\ndiff = np.linalg.norm(g1 - g2)\nprint('Gradient difference: %f' %diff)","5080f03a":"w0 = np.random.randn(X0_bar.shape[0], 1) \nw = grad_descent(w0, 0.001, lam)\nw_hinge = w[:-1].reshape(-1, 1)\nb_hinge = w[-1]\nprint(w_hinge.T, b_hinge)","0303218e":"def plotResult(X0, X1, w, b, title):\n    fig, ax = plt.subplots()\n\n    w0 = w[0]\n    w1 = w[1]\n    x1 = np.arange(-10, 10, 0.1)\n    y1 = -w0\/w1*x1 - b\/w1\n    y2 = -w0\/w1*x1 - (b-1)\/w1\n    y3 = -w0\/w1*x1 - (b+1)\/w1\n    plt.plot(x1, y1, 'k', linewidth = 3)\n    plt.plot(x1, y2, 'k')\n    plt.plot(x1, y3, 'k')\n\n    # equal axis and lim\n    plt.axis('equal')\n    plt.ylim(0, 3)\n    plt.xlim(2, 4)\n\n    # hide tikcs \n    cur_axes = plt.gca()\n    cur_axes.axes.get_xaxis().set_ticks([])\n    cur_axes.axes.get_yaxis().set_ticks([])\n\n    # fill two regions\n    y4 = 10*x1\n    plt.plot(x1, y1, 'k')\n    plt.fill_between(x1, y1, color='blue', alpha='0.1')\n    plt.fill_between(x1, y1, y4, color = 'red', alpha = '.1')\n\n    plt.xlabel('$x_1$', fontsize=12)\n    plt.ylabel('$x_2$', fontsize=12)\n    plt.title('Solution found by ' + title, fontsize=12)\n\n    plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = .8)\n    plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = .8)\n    plt.show()","4d0d7b0b":"plotResult(X0, X1, w_sklearn, b_sklearn, 'sklearn')\nplotResult(X0, X1, w_dual, b_dual, 'dual')\nplotResult(X0, X1, w_hinge, b_hinge, 'hinge')","cb9e0356":"# Change C\nlsC = [1e-2, 1, 10, 1000]","9a6c79f6":"X0_bar = np.vstack((X0.T, np.ones((1, N)))) # extended data\nX1_bar = np.vstack((X1.T, np.ones((1, N)))) # extended data \n\nZ = np.hstack((X0_bar, - X1_bar)) # as in (22)\nfor C in lsC:\n    lam = 1.\/C\n    w0 = np.random.randn(X0_bar.shape[0], 1) \n    w = grad_descent(w0, 0.001, lam)\n    w_hinge = w[:-1].reshape(-1, 1)\n    b_hinge = w[-1]\n    print(w_hinge.T, b_hinge)\n    plotResult(X0, X1, w_hinge, b_hinge, 'hinge')","991358ce":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# XOR dataset and targets\nX = np.c_[(0, 0),\n          (1, 1),\n          #---\n          (1, 0),\n          (0, 1)].T\nY = [0] * 2 + [1] * 2","db110e29":"# figure number\nfignum = 1\n\n# fit the model\nfor kernel in ('sigmoid', 'poly', 'rbf'):\n    clf = svm.SVC(kernel=kernel, gamma=4, coef0 = 0)\n    clf.fit(X, Y)\n    # plot the line, the points, and the nearest vectors to the plane\n    fig, ax = plt.subplots()\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors='None')\n    plt.plot(X[:2, 0], X[:2, 1], 'ro', markersize = 8)\n    plt.plot(X[2:, 0], X[2:, 1], 'bs', markersize = 8)\n\n    plt.axis('tight')\n    x_min, x_max = -2, 3\n    y_min, y_max = -2, 3\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    CS = plt.contourf(XX, YY, np.sign(Z), 200, cmap='jet', alpha = .2)\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n    plt.title(kernel, fontsize = 15)\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\nplt.show()","55020157":"import pandas as pd","9fc8764c":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn import datasets","ec3f5463":"## Iris flowers classification using svm\n!ls ..\/input\/iris-flower-dataset\nfilename = \"..\/input\/iris-flower-dataset\/IRIS.csv\"\npdfData = pd.read_csv(filename)\npdfData.head()","4d21f4fe":"pdfData.shape","cdd7a4f4":"pdfData.hist(bins=50, figsize=(20,15))\nplt.show()","2ce6c006":"trainSet, testSet = train_test_split(pdfData, test_size=0.2, random_state=42)","d3051989":"lsLabel = set(pdfData[\"species\"])\nprint(lsLabel)","dfa77a6c":"for l in lsLabel:\n    pdfData[\"label_%s\"%l[5:]] = (pdfData[\"species\"] == l)","19091722":"# Look for correlation\ncorrMatrix = pdfData.corr()","369307d9":"corrMatrix","844c7acd":"for l in lsLabel:\n    print(corrMatrix[\"label_%s\"%l[5:]].sort_values(ascending=False))\n    print(\"-\"*30)","a8f7b998":"lsFt = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nprint(trainSet[lsFt][trainSet[lsFt].isnull()])\ntraining = trainSet[lsFt].dropna().values\ntrainingLabel = trainSet[\"species\"].copy()","135e195f":"test = testSet[lsFt].dropna().values\ntestLabel = testSet[\"species\"].copy()","44ccecc3":"clf = {}\npredictions = {}\nlsKernel = ('linear', 'poly', 'rbf')\nfor kernel in lsKernel:\n    clf[kernel] = svm.SVC(kernel=kernel, gamma=4, coef0 = 0)","95121d9b":"lsLabel","64d7a772":"for kernel in lsKernel:\n    print(kernel)\n    labels = list(lsLabel)\n    clf[kernel].fit(training, trainingLabel)\n    \n    predictions[kernel] = clf[kernel].predict(test) \n\n    # model accuracy for X_test   \n    accuracy = clf[kernel].score(test, testLabel) \n    print(accuracy)\n\n    # creating a confusion matrix \n    cm = confusion_matrix(testLabel, predictions[kernel], labels) \n    print(cm)\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(cm)\n    # plt.title('Confusion matrix')\n    fig.colorbar(cax)\n    ax.set_xticklabels([''] + labels)\n    ax.set_yticklabels([''] + labels)\n    \n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    print(\"-\"*30)\n","f2017525":"### 4. SVM applications","2a6f83f0":"### 2. Solve SVM using sklearn-svm, cvxopt and gradient descent for SVM soft margin","61c3d8f3":"### 1. Solve SVM using optimization of dual function","8b6db736":"#### Prepare data","c6930a44":"#### Using gradient descent","f7576634":"### 3. Kernel function demo","aa07962a":"#### Using sklearn","be73a778":"### 0. Sample data","a276e521":"#### Using duality problem"}}