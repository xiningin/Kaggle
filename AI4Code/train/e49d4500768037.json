{"cell_type":{"42b59e89":"code","d6b890ad":"code","9299b699":"code","ab7c0c4b":"code","09f94162":"code","fb4c2cd7":"code","005064bd":"code","287a47d1":"code","aa67053e":"code","085a6fe7":"code","f6c1f619":"code","7eb0aa1a":"code","5a8a7d42":"code","8e7fabef":"code","25bfbfad":"code","2d1597c0":"code","48edb6b1":"code","82b491a9":"code","6ccdb369":"code","6ec94c15":"code","d81004d0":"code","811a04a8":"code","ece212d6":"code","bcf3c873":"code","3b41b1a7":"code","1950c578":"code","ff6dbe7e":"markdown","d3c04629":"markdown","ae7eba7c":"markdown","19046032":"markdown","3f77b731":"markdown","79184cec":"markdown","bf75e013":"markdown","9723224c":"markdown","99de17f1":"markdown","311363d2":"markdown","e43b40ce":"markdown","7d6a4486":"markdown","75aa8553":"markdown","d160804f":"markdown"},"source":{"42b59e89":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#First, import the training and test data.\ndf = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")","d6b890ad":"df.head()","9299b699":"l = []\nfor i in df[\"SibSp\"]:\n    if(i >=1):\n        l.append(0)\n    else:\n        l.append(1)\nl = pd.Series(l)\ndf.drop([\"SibSp\"],axis=1,inplace=True)\ndf[\"is_Alone\"] = l\ndf.head()","ab7c0c4b":"#Same process on the test_data\nl = []\nfor i in test_data[\"SibSp\"]:\n    if(i >=1):\n        l.append(0)\n    else:\n        l.append(1)\nl = pd.Series(l)\ntest_data.drop([\"SibSp\"],axis=1,inplace=True)\ntest_data[\"is_Alone\"] = l\ntest_data.head()","09f94162":"def statue(name):\n    l = name.split()\n    for i in l:\n        if(i == \"Mrs.\"):\n            return \"Mrs.\"\n        elif(i == \"Miss.\"):\n            return \"Miss.\"\n        elif(i == \"Mr.\"):\n            return \"Mr.\"\nstatues = []  \nfor i in df[\"Name\"]:\n    statues.append(statue(i))\nstatues = pd.Series(statues)\ndf[\"Statues\"] = statues\n\n#We have some None values in the \"Statues\" column. I'll fill it with most_frequent element in this column.\ndf[\"Statues\"].fillna(df[\"Statues\"].value_counts().index[0],inplace=True)\ndf.head()","fb4c2cd7":"#I'll apply same process on test data\ndef statue(name):\n    l = name.split()\n    for i in l:\n        if(i == \"Mrs.\"):\n            return \"Mrs.\"\n        elif(i == \"Miss.\"):\n            return \"Miss.\"\n        elif(i == \"Mr.\"):\n            return \"Mr.\"\nstatues = []  \nfor i in test_data[\"Name\"]:\n    statues.append(statue(i))\nstatues = pd.Series(statues)\ntest_data[\"Statues\"] = statues\n\n#We have some None values in the \"Statues\" column. I'll fill it with most_frequent element in this column.\ntest_data[\"Statues\"].fillna(test_data[\"Statues\"].value_counts().index[0],inplace=True)\ntest_data.head()","005064bd":"id = 0\nnew_class = []\nwhile id < 891:\n    if(df[\"Pclass\"][id] == 1):\n        new_class.append(\"High\")\n    elif(df[\"Pclass\"][id] == 2):\n        new_class.append(\"Medium\")\n    else:\n        new_class.append(\"Low\")\n    id += 1\nnew_class = pd.Series(new_class)\ndf[\"New_Class\"] = new_class\ndf.drop([\"Pclass\"],axis=1,inplace=True)\ndf.head()","287a47d1":"#Same process on the test_data.\nid = 0\nnew_class = []\nwhile id < 418:\n    if(test_data[\"Pclass\"][id] == 1):\n        new_class.append(\"High\")\n    elif(test_data[\"Pclass\"][id] == 2):\n        new_class.append(\"Medium\")\n    else:\n        new_class.append(\"Low\")\n    id += 1\nnew_class = pd.Series(new_class)\ntest_data[\"New_Class\"] = new_class\ntest_data.drop([\"Pclass\"],axis=1,inplace=True)\ntest_data.head()","aa67053e":"#Unsuccessfull : )\n\n# #Let's look at the \"Age\" feature closer.\n# #Maybe, we can create 4 category based on \"Age\"\n# #0  - 16 \n# #16 - 32\n# #32 - 48\n# #48 - 64\n# #64 - 80\n# #print (df[['Age', 'Survived']].groupby(['Age']).mean())\n\n# New_Age = []\n# id = 0\n# while id < 891:\n#     if(df[\"Age\"][id] < 16):\n#         New_Age.append(1)\n#     elif(df[\"Age\"][id] < 32):\n#         New_Age.append(2)\n#     elif(df[\"Age\"][id] < 48):\n#         New_Age.append(3)\n#     elif(df[\"Age\"][id] < 64):\n#         New_Age.append(4)\n#     elif(df[\"Age\"][id] <= 80):\n#         New_Age.append(5)\n#     id += 1\n\n# New_Age = pd.Series(New_Age)\n# df[\"New_Age\"] = New_Age\n# df.drop([\"Age\"],axis=1,inplace=True)\n# df[\"New_Age\"].fillna(df[\"New_Age\"].value_counts().index[0],inplace=True)\n# df.head()","085a6fe7":"#Unsuccessfull : )\n\n# #Same process on test_data\n\n# New_Age = []\n# id = 0\n# while id < 418:\n#     if(test_data[\"Age\"][id] < 16):\n#         New_Age.append(1)\n#     elif(test_data[\"Age\"][id] < 32):\n#         New_Age.append(2)\n#     elif(test_data[\"Age\"][id] < 48):\n#         New_Age.append(3)\n#     elif(test_data[\"Age\"][id] < 64):\n#         New_Age.append(4)\n#     elif(test_data[\"Age\"][id] <= 80):\n#         New_Age.append(5)\n#     id += 1\n\n# New_Age = pd.Series(New_Age)\n# test_data[\"New_Age\"] = New_Age\n# test_data.drop([\"Age\"],axis=1,inplace=True)\n# test_data[\"New_Age\"].fillna(test_data[\"New_Age\"].value_counts().index[0],inplace=True)\n# test_data.head()","f6c1f619":"#Unsuccessful : )\n#I'll not delete these blocks intetionally.So I can notice my mistakes.If you don't want to read blocks like that,\n#You can skip,if block has \"Unsuccessful\" header.\n\n# #Let's look at the \"Fare\" feature. I'll create new column based on \"Fare\" that will have four category.\n# #Min -> %25 --> 0\n# #%25 -> %50 --> 1\n# #%50 -> %75 --> 2\n# #%75 -> Max --> 3\n# df[\"Fare\"].describe()","7eb0aa1a":"#Unsuccessful : )\n\n# New_Fare = []\n# id = 0\n# while id < 891:\n#     if(df[\"Fare\"][id] < 8):\n#         New_Fare.append(0)\n#     elif(df[\"Fare\"][id] < 14.5):\n#         New_Fare.append(1)\n#     elif(df[\"Fare\"][id] < 31):\n#         New_Fare.append(2)\n#     elif(df[\"Fare\"][id] > 31):\n#         New_Fare.append(3)\n#     id += 1\n# New_Fare = pd.Series(New_Fare)\n# df[\"New_Fare\"] = New_Fare\n# df.drop([\"Fare\"],axis=1,inplace=True)\n\n# #Impute with most_frequent Nan datas.\n# df[\"New_Fare\"].fillna(df[\"New_Fare\"].value_counts().index[0],inplace=True)\n# df.head()","5a8a7d42":"#Unsuccessful : )\n\n# New_Fare = []\n# id = 0\n# while id < 418:\n#     if(test_data[\"Fare\"][id] < 8):\n#         New_Fare.append(0)\n#     elif(test_data[\"Fare\"][id] < 14.5):\n#         New_Fare.append(1)\n#     elif(test_data[\"Fare\"][id] < 31):\n#         New_Fare.append(2)\n#     elif(test_data[\"Fare\"][id] > 31):\n#         New_Fare.append(3)\n#     id += 1\n# New_Fare = pd.Series(New_Fare)\n# test_data[\"New_Fare\"] = New_Fare\n# test_data.drop([\"Fare\"],axis=1,inplace=True)\n# #Impute with most_frequent Nan datas.\n# test_data[\"New_Fare\"].fillna(test_data[\"New_Fare\"].value_counts().index[0],inplace=True)\n# test_data.head()","8e7fabef":"y = df[\"Survived\"]\ndf.drop([\"Survived\",\"Name\",\"Ticket\"],axis=1,inplace=True)\ntest_data.drop([\"Name\",\"Ticket\"],axis=1,inplace=True)","25bfbfad":"df.info()","2d1597c0":"df.head()","48edb6b1":"df.describe()","82b491a9":"nan_values = [\"Column Name: {} -> None Values: {} -> Type: {}\".format(column,df[column].isnull().sum(),df[column].dtype) for column in df.columns]\nnan_values","6ccdb369":"df.drop([\"Cabin\"],axis=1,inplace=True)\ntest_data.drop([\"Cabin\"],axis=1,inplace=True)\ndf.columns","6ec94c15":"df[\"Age\"].fillna((df[\"Age\"].mean()),inplace=True)\ndf[\"Age\"].isnull().sum() #None values -> 0. We impute them with its mean.\n\ntest_data[\"Age\"].fillna((df[\"Age\"].mean()),inplace=True)\ntest_data[\"Age\"].isnull().sum() #None values -> 0. We impute them with its mean.","d81004d0":"df[\"Embarked\"].fillna(df[\"Embarked\"].value_counts().index[0],inplace=True)\ndf[\"Embarked\"].isnull().sum()\n\ntest_data[\"Embarked\"].fillna(test_data[\"Embarked\"].value_counts().index[0],inplace=True)\ntest_data[\"Fare\"].fillna(test_data[\"Fare\"].value_counts().index[0],inplace=True)\ntest_data[\"Embarked\"].isnull().sum()\n","811a04a8":"df1 = pd.get_dummies(df)\ntest_data = pd.get_dummies(test_data)\ndf1.head()","ece212d6":"df1.drop([\"PassengerId\"],axis=1,inplace=True)\nfrom sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(df1,y,test_size=0.3,random_state=2)","bcf3c873":"from sklearn.metrics import accuracy_score\nimport xgboost as xgb\nxgboost = xgb.XGBClassifier(max_depth=15, n_estimators=400, learning_rate=0.02).fit(train_x, train_y)\nxgb_prediction = xgboost.predict(test_x)\nxgb_score=accuracy_score(test_y, xgb_prediction)\nprint(xgb_score)","3b41b1a7":"from sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100,random_state=0)\nclf.fit(train_x, train_y)\n\nprediction = clf.predict(test_x)\nscore=accuracy_score(test_y, prediction)\nprint(score)\n    ","1950c578":"test = test_data.drop([\"PassengerId\"],axis=1)\nprediction = clf.predict(test)\nsubmission = pd.DataFrame({\n       \"PassengerId\": test_data[\"PassengerId\"],\n       \"Survived\": prediction\n   })\nsubmission.to_csv('titanic.csv', index=False)","ff6dbe7e":"I'll create new column based on \"SibSp\" feature that is name is_Alone. If person have Sibling or Spouse,is_Alone will be equal 0 which means person is not alone.","d3c04629":"We see that our target value is \"Survived\" so we can take it for new variable. Also, I will drop Name and Ticket features because I don't think they're important to prediction. But firstly, I want to add a new column based on name.","ae7eba7c":"I'll create new column based on \"Pclass\" which is New_Class. If person is first class,his\/her New_Class will be High. Thus it is transformed into categorical data.","19046032":"I will impute age feature with mean.","3f77b731":"Create a model,predict and find the accuracy. (Xgboost)","79184cec":"Cabin feature has got a lot of None value so I want to drop it.","bf75e013":"# First Kernel\nThis is my very first kernel so if you want to give any advice about this kernel and my approaches, you can comment. My aim is about this kernel improving it by using what I've learned day by day.","9723224c":"Create a model,predict and find the accuracy. (Random Forest Classifier)","99de17f1":"Let's look at the some part of the data.","311363d2":"I'll split the data for model.","e43b40ce":"We have some none values that have to handle it.","7d6a4486":"I will do one-hot encoding for categorical features. (Sex,Statues and Embarked)","75aa8553":"I will impute Embarked feature with most_frequent value on that column.","d160804f":"We have 11 columns and types are int64,float64 and object so we have some categorical and numerical features."}}