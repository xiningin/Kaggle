{"cell_type":{"6d2140c8":"code","39f61573":"code","a85e865d":"code","b5dc19a7":"code","d61a56a7":"code","b9f7ae77":"code","6500a75b":"code","bfc38123":"code","c5a819f7":"code","8625dba5":"code","c3ac11ee":"code","d59b5f45":"code","ca424b18":"code","5d6ae379":"code","4fb23da1":"code","33c2a300":"code","5807cecf":"code","fd1bd46f":"code","325b7d7f":"code","c92be7c5":"code","8d5fa194":"code","1349a1f2":"code","0e2b73c6":"code","fdff078c":"code","068dc051":"code","9664854b":"code","cdeac709":"code","4f251e2f":"code","a7b4c3d7":"code","a18d217e":"code","3d842ce4":"code","24231a4d":"code","e211aa78":"code","038f19d1":"code","bf91ef6b":"code","0fdf9b5c":"markdown","57ec4525":"markdown","b0bbf834":"markdown","33e47622":"markdown","6855a147":"markdown","6e84e430":"markdown","59a0937d":"markdown","2f9f2002":"markdown","ccc3aed6":"markdown","46fe6c14":"markdown","65881763":"markdown","594b9c97":"markdown","b5c62af4":"markdown","ce21073e":"markdown","8e417982":"markdown","9e290a85":"markdown","378c5898":"markdown","f6677688":"markdown","d1b7f2ff":"markdown","15ef61a7":"markdown","3ceaea05":"markdown","af53c38b":"markdown","e48073d2":"markdown","4f29e0d3":"markdown","289e9f36":"markdown","1fa3824b":"markdown","39ff4105":"markdown","22fd980e":"markdown","a5e47c66":"markdown"},"source":{"6d2140c8":"#first, load packages:\r\nimport pandas as pd\r\nimport numpy as np\r\nimport scipy.stats as ss\r\nimport math as mt\r\nimport itertools","39f61573":"#next, let's have a look at our dataset:\ndata = pd.read_csv('..\/input\/ab-testing\/ab_data.csv')\ndf = data.copy()\ndf.head()","a85e865d":"df.info()\r\nprint(df.shape)","b5dc19a7":"#we want to analyse data of unique user's conversion rates given their unique landing page:\r\ndf['user_id'].nunique()\r\n#290584 unique users vs 294478 rows - why the discrepancy?","d61a56a7":"#Locate where treatment does not match with new_page or control does not match with old_page, and drop these rows\r\ni = df[((df['group']=='treatment') ==(df['landing_page']=='new_page')) == False].index\r\ndf2 = df.drop(i)","b9f7ae77":"# Number of unique users in df2 was return above, how many rows do we have now?\r\ndf2.shape[0]","6500a75b":"#If the number of unique rows is 1 greater than the number of unique users, then we have a duplicate user somewhere. We'll find the duplicate row first:\r\ndf2[df2.duplicated(['user_id'], keep=False)]","bfc38123":"#drop duplicate:\r\ndf2.drop_duplicates(subset ='user_id',keep ='first',inplace = True)","c5a819f7":"#total\/pooled probability of conversion:\r\nP_pool = (df2.query('converted == 1').converted.count())\/df2.shape[0]\r\nP_pool","8625dba5":"#probability of conversion given a user was in the control group:\r\ncontrol_df = df2.query('group ==\"control\"')\r\nP_old = control_df['converted'].mean()\r\nP_old","c3ac11ee":"#probability of conversion given a user was in the treatment group:\r\ntreatment_df = df2.query('group ==\"treatment\"')\r\nP_new = treatment_df['converted'].mean()\r\nP_new","d59b5f45":"#proportion of users seeing the new vs old page:\r\nN_new = df2.query('landing_page == \"new_page\"').landing_page.count()\r\nN_old = df2.query('landing_page == \"old_page\"').landing_page.count()\r\nproportion = (N_old\/df2.shape[0],N_new\/df2.shape[0])\r\nproportion","ca424b18":"#function for getting z-scores for alpha. For our experiemnt where alpha = 5%, keep in mind we want to input 1-alpha\/2 for Confidence Intervals.\r\ndef get_z_score(alpha):\r\n    return ss.norm.ppf(alpha)","5d6ae379":"#Guardrail Check on differences in proportions:\r\nsd = round(mt.sqrt((0.5*(1-0.5))\/df2.shape[0]),4)\r\nCI = (0.5 - sd*get_z_score(1-0.05\/2), 0.5 + sd*get_z_score(1-0.05\/2))\r\nprint('Does the control group proportion ' + str(N_old\/df2.shape[0]) + ' lie within ' + str(CI) + '?')","4fb23da1":"#1. Using statistical rule of thumb to calculate minimum sample size per variation:\r\n16*(0.12*(1-0.12))\/pow(0.0035,2)","33c2a300":"#calculating the minimum sample size for the ab test:\r\ndef get_sampSize(sds,alpha,beta,d):\r\n    n=pow((get_z_score(1-alpha\/2)*sds[0]+get_z_score(1-beta)*sds[1]),2)\/pow(d,2)\r\n    return n","5807cecf":"#baseline + expected change standard deviation calculations\r\ndef get_sds(p,d):\r\n    sd1=mt.sqrt(2*p*(1-p))\r\n    sd2=mt.sqrt(p*(1-p)+(p+d)*(1-(p+d)))\r\n    sds=[sd1,sd2]\r\n    return sds","fd1bd46f":"#2. Using Evan Miller's Calculator but deriving the values ourselves:\r\nround(get_sampSize(get_sds(0.12, 0.0035),0.05,0.2,0.0035))","325b7d7f":"CI_old = (P_old - get_z_score(1-0.025\/2)*mt.sqrt(P_old*(1-P_old)\/N_old),P_old + get_z_score(1-0.025\/2)*mt.sqrt(P_old*(1-P_old)\/N_old))\r\nCI_new = (P_new - get_z_score(1-0.025\/2)*mt.sqrt(P_new*(1-P_new)\/N_new),P_new + get_z_score(1-0.025\/2)*mt.sqrt(P_new*(1-P_new)\/N_new))\r\nprint('Do ' + str(CI_old) + ' and ' + str(CI_new) + ' overlap?') ","c92be7c5":"import statsmodels.api as sm\r\n#returning the total number of conversions for each group:\r\nconvert_old = df2.query(\"landing_page == 'old_page' and converted == 1\").shape[0]\r\nconvert_new = df2.query(\"landing_page == 'new_page' and converted == 1\").shape[0]","8d5fa194":"#calculating the z-score + p-value using the z-test (one-sided):\r\nz_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [N_old, N_new], alternative='smaller')\r\nz_score, p_value","1349a1f2":"SE_new = mt.sqrt(P_new*(1-P_new)\/N_new)\r\nSE_old = mt.sqrt(P_old*(1-P_old)\/N_old)","0e2b73c6":"p = 1 - ss.f.cdf(pow(SE_new,2)\/pow(SE_old,2), N_new - 1, N_old - 1)\r\np","fdff078c":"#Calculating pooled standard deviation and pooled CI:\r\nd_hat = P_new - P_old\r\nSE_pool = mt.sqrt(P_pool*(1-P_pool)*(1\/N_old+1\/N_new))\r\nCI_diff = (d_hat - get_z_score(1-0.05\/2)*SE_pool, d_hat + get_z_score(1-0.05\/2)*SE_pool)\r\nprint('The change due to the experiment is ' + str(round(d_hat,4)*100) + '%')\r\nprint('Is 0 inside the interval of (' + str(round(CI_diff[0],4)) + ', ' + str(round(CI_diff[1],4)) +')?')\r\nprint('Is D_min = 0.0035 inside the interval of (' + str(round(CI_diff[0],4)) + ', ' + str(round(CI_diff[1],4)) + ')?')  ","068dc051":"#building out another dataframe that will be useful in a sign test:\r\ndfsign = df2[['timestamp','group','converted']]","9664854b":"#timestamp is a concatenation of date + exact time of user landing on the page. We only want the date, or 1st 10 characters:\r\ndfsign['timestamp'] = df2['timestamp'].astype(str).str[:10]","cdeac709":"dfsign.head()","4f251e2f":"#grouping by timestamp and group will sum all the conversions for a group on a given day:\r\ndfsign2 = dfsign.groupby(['timestamp','group']).sum().reset_index()\r\ndfsign2.head(20)","a7b4c3d7":"#pivot the table, keeping the timestamps along the rows, the groups now in the columns and the sum of conversions as our values:\r\ndfsign3 = dfsign2.pivot(index=['timestamp'],columns=['group'],values=['converted']).reset_index()\r\ndfsign3.head(6)","a18d217e":"#building out a conditional column - we want it to be 1 when the treatment did better than the control, else 0:\r\ndfsign3['sign'] = np.where(dfsign3[('converted','treatment')] > dfsign3[('converted','control')], 1,0)\r\ndfsign3.head()","3d842ce4":"#find the total number of times that our daily treatment conversion was greater than our daily control conversion\r\nsuccesses = dfsign3[('sign','')].sum()\r\np_sign = successes\/dfsign3.shape[0]\r\nprint('We have ' + str(successes) + ' out of ' + str(dfsign3.shape[0]) + ' days where the total daily treatment conversions was greater than its control counterpart. This brings our probability of success to be ' + str(round(p_sign,4)) + '.')","24231a4d":"#binomial distribution:\r\ndef bin(x,n):\r\n    return mt.factorial(n)\/(mt.factorial(x)*mt.factorial(n-x))*0.5**x*0.5**(n-x)\r\n\r\n#sum all successes up until our desired success:   \r\ndef get_1side_pvalue(x,n):\r\n    return list(itertools.accumulate([bin(i,n) for i in range(0,x+1)]))[-1]\r\n","e211aa78":"\nprint('The two-sided P-value is ' + str(round(2*get_1side_pvalue(successes,dfsign3.shape[0]),4)))\r\nprint('The one-sided P-value is ' + str(round(get_1side_pvalue(successes,dfsign3.shape[0]),4)))","038f19d1":"#doing the 4 calculations as above:\ntreatment_converted = treatment_df.converted.sum()\ntreatment_not_converted = treatment_df.size - treatment_df.converted.sum()\ncontrol_converted = control_df.converted.sum()\ncontrol_not_converted = control_df.size - control_df.converted.sum()\n\n#create the array to do our chi-squared test: treatment\/control along the rows and converted\/not converted along the columns:\nChi = np.array([[treatment_converted,treatment_not_converted],[control_converted,control_not_converted]])\nChi","bf91ef6b":"#using scipt stats to perform our chi squared test:\nprint(ss.chi2_contingency(Chi,correction=False)[1])","0fdf9b5c":"Given our p-value = 0.2131 > 0.05, we do not reject the Null Hypothesis that the assigned treatment\/control group has no effect on conversion.\n    \n    This means that the new page is not better than the old page.","57ec4525":"<u><b>2. Z-test:<\/b><\/u>\r\n\r\nWe can use existing packages to calculate our test statistic and p-values and test for proportions based on the z-test. This is similar to the Binomial Proportion Confidence Interval Test, is quantitatively easier to draw conclusions out of due to it returning a p-value:","b0bbf834":"We've calculated 95% confidence intervals for our p\u0302 = 0.5 which lies within the CI. This means that we've passed our Guardrail Metric that the number of unique users is equal for each group.","33e47622":"<b><u>Probabilities:<\/u><\/b>","6855a147":"It's finally time to do our sign test - it is a relatively simple test (statistically speaking) which can be done via an online calculator:\r\n\r\nhttps:\/\/www.graphpad.com\/quickcalcs\/binomial1\/\r\n\r\nThe calculator also provides a sound explanation on the stats behind it after inputting your variables. However, we'll go the extra mile to quickly work out our p-values:","6e84e430":"Both CI intervals overlap plenty as $CI_{new}$ is completely contained within $CI_{old}$ , which means we do not reject the Null Hypothesis that $P_{new} = P_{old}$.\r\n\r\n    This means that the new page is not better than the old page.\r\n\r\nWhile our case is quite evident that the overlap is significantly clear, slight overlaps could tempt us to draw the same conclusion to reject the Null Hypothesis. However, this is a common misinterpretation of overlapping CIs when comparing groups. Failure to do so could result in incorrect or misleading conclusions being drawn (Tan & Tan, 2010, pp. 278).","59a0937d":"<u><b>5. Chi-Squared Test:<\/b><\/u>\n\nOne statistical test that came out from one of LinkedIn Learning's A\/B testing courses (Wahi, 2020) is the Chi-Squared Analysis (or $\\chi^{2}$ test). If we constructed a 2x2 contingency table for our observed frequencies in our dataset, and compared it to a 2x2 contingency table for the expected frequencies in our dataset, we can perform the $\\chi^{2}$ test under the Null Hypothesis that there is no relationship that exists on between our conversion vs their treatment\/control group in the population.\n\nFor reference, our 2x2 contingency table will have two groups: treatment\/control or converted\/not converted. We want to make 4 calculations that will be in our table:\n1. Treatment, converted\n2. Treatment, not converted\n3. Control, converted\n4. Control, not converted","2f9f2002":"    If using the statistical rule of thumb, the minimum sample size per variation = 137,926\/variation. Given we have 2 groups (treatment and control): the total minimum sample size = 275,852. Since we have a total sample size of 290,584, our a\/b test will have enough statistical power and significance.","ccc3aed6":"<b><u>3. Hypothesis testing on d\u0302 and Effect Size:<\/u><\/b>\r\n\r\nA couple of methdologies are found in Udacity's A\/B test course, where we also consider the pooled probability and standard deviations, under the assumption that the variances within each sample are equal (Shetty, 2016). The reason we do this is so we can do a z-test under the context of our Evaluation Metric $D_{min}$, and observe if our difference is practically significant to the business:\r\n\r\n\r\n$\\hat{p}_{pool} =\r\n \\frac{X_{new} + X_{old}}{N_{new} + N_{old}}$\r\n\r\n\r\n$SD_{pool} = \r\n\\sqrt{\\hat{p}_{pool}(1 - \\hat{p}_{pool})(\\frac{1}{N_{new}} + \\frac{1}{N_{old}})}$\r\n\r\n\r\nAll under the null hypothesis of $\\hat{d} = P_{new} - P_{old}$ where $\\hat{d} \\sim N(0,SD_{pool})$.\r\n\r\nWhat we will perform is the same CI calculations we've done above, but using $SD$ as our standard deviations:\r\n\r\n$CI_{diff} =\r\n \\hat{d} \\ \\pm SE_{pool}$\r\n\r\nAfter $CI_{diff}$ is calculated, the change is statistically significant if 0 lies outside the $CI_{diff}$ (equivalent to the above z-test in terms of Hypothesis Testing). \r\n\r\nHowever, another additional conclusion we can draw is if our Evaluation Metric $D_{min}$ is practically significant if it is outside $CI_{diff}$, especially if $D_{min}$ is below $CI_{diff}$. However, there is no statistical test that can truly tell you whether the effect is large enough to be important, so some level of subject area knowledge and expertise must be applied whether the effect is big enough to be meaningful (Frost, 2018).","46fe6c14":"Because 0 falls within our $CI_{diff}$ interval, the change due to the experiment is not statistically significant, therefore we do not reject the Null Hypothesis that $\\hat{d} = P_{new} - P_{old}$ where $\\hat{d} \\sim N(0,SD_{pool})$. Furthermore, given our one-sided test where $D_{min}$ is above our confidence intervals, it is also not practically significant.\n\n    This means that the new page is not better than the old page.\n","65881763":"Now, it's time to view various methods to conclude if $P_{new} = P_{old}$ or $P_{new} > P_{old}$:","594b9c97":"<b><u>1. Binomial Proportion Confidence Intervals:<\/u><\/b>","b5c62af4":"<b><u>Conclusion:<\/u><\/b>\r\n\r\nThe A\/B experiment was designed to determine if <b>FaceZonGoogAppFlix<\/b>'s new webpage would improve the conversion rate of their users compared to their existing one. \r\n\r\nAfter going through Metric checks, minimum sample size requirements and multiple statistical methods to determine a winner of the A\/B test, We've seen that <b>FaceZonGoogAppFlix<\/b>'s underlying goal had not been reached with their new webpage. \r\n\r\nTherefore, we recommend that to not continue with the new webpage change, but pursue other experiments.","ce21073e":"<u><b>Guardrail Metric Check:<\/b><\/u>\r\n\r\nAs stated above, we want to verify our Guardrail Metrics before diving deeper. We want to count the proportion of users in each group and see if there is a significant difference in the proportion amounts. If a statistically significant difference is present, it implies a biased experiment and our results should not be relied on.\r\n\r\nHow we test this is through the use of Confidence Intervals (CI). If one were to google the formula for CI with the t-distribution, we would get:\r\n\r\n$CI =\r\n \\bar{x} \\ \\pm \\ z_{1-\\alpha} \\ \\frac{s}{\\sqrt{n}}$\r\n\r\n- $CI$ = Confidence Interval\r\n- $\\bar{x}$ = Sample Mean\r\n- $z_{1-\\alpha}$ = Confidence Level Value (For simplicity, I'll write z to denote this)\r\n- $s$ = sample standard mean (notice how we don't use \u03c3, since \u03c3 represents a known standard deviation, and t-distribution is used for s, or unknown \u03c3)\r\n- $n$ = sample size.\r\n\r\nWe can adjust this formula using the Central Limit Theorem where a large enough summed Binomial Distribution will converge towards the Normal distribution, or $X \\sim N(np,\\sqrt{np(1-p)} \\ )$ given our sample size is large enough (typically \u2265 30). \r\n\r\nThis means that for our sampling proportion $\\hat{p} = \\frac{X}{n}$: our distribution will converge towards $N( \\hat{p},\\sqrt{ \\frac{ \\hat{p} (1 - \\hat{p})}{n}} )$.\r\n\r\nTherefore, our Binomial Confidence Intervals are calculated by:\r\n\r\n$CI =\r\n \\hat{p} \\ \\pm \\ z \\ \\sqrt{ \\frac{ \\hat{p} (1 - \\hat{p})}{n}}$\r\n\r\nWe will now use the above to calculate our Confidence Interval for p\u0302 and see if we've kept the approximate 50\/50 split:\r\n\r\n","8e417982":"<b><u>4. Sign Test:<\/u><\/b>\r\n\r\nThe last test to consider when picking a winner in an A\/B test is a sign test - checking the trend of change we observe by day. If we check how many days the conversion was higher in the Treatment vs the New, we can use this as our number of successes for our binomial distribution. The null hypothesis of the sign test is that the population median is equal to some value $M$ - implying the observations greater than $M$ (denoted by $r^{+}$) are equal to the observations less than $M$ (denoted by $r^{-}$).\r\n\r\nTherefore, the Null Hypothesis is: $r^{+}, r^{-} \\sim Bin(n,p)$ where $p = \\frac{1}{2}$ (Shier, 2004).\r\n\r\nNote that we are only concerning ourselves with the number of days (observations) that the conversion was higher for Treatment > number of days that the conversion was higher for Control. Therfore, this will be a one-sided test and we'll divide the p-value by 2.\r\n","9e290a85":"This method is quoted as the 'most common' method for A\/B testing, where we find Confidence Intervals (CI) for both $P_{new}$ and $P_{old}$. If we construct similar intervals for both and compare them, we will end up in either scenario: (jkndrkn, 2012)\r\n\r\n1. The Intervals do not overlap: This implies that we can say with some level of confidence that one is better than the other, therefore providing enough evidence to reject the Null Hypothesis. This level of confidence seems to be $\u2248 1-e \\alpha^{1.91}%$ (Lan, 2011). So if there is overlap and the 95% CI are the same size, the difference is significant at the 99.5% level.\r\n\r\n2. The Intervals do overlap: Then it is either a sign that our population does not have enough statistical power, or we do not have enough evidence to reject the Null Hypothesis that $P_{new} = P_{old}$.\r\n\r\nThere is a relationship between CI comparisons and hypothesis tests - given that the sample sizes are not too different and the two sets have similar standard deviations. This method wouldn't derive highly precise p-values from comparing two CIs, but there is a good write up that tries to quantify this (Lan, 2011).\r\n\r\nFinding the 'true' conversion rate of a particular group is usually impossible or difficult, but we can use our calculated $P_{new}$ and $P_{old}$ as point estimations to find the Confidence Intervals for the 'true' $P_{new}$ and $P_{old}$. A good explanation for beginners about using point intervals to calculate a CI for the 'true' conversion rate can be found in the Appendix of <i>\"A\/B Testing: The Most Powerful Way to Turn Clicks into Customers\"<\/i> (Siroker & Koomen, 2013)","378c5898":"    If using Evan Miller's calculator, the minimum sample size per group = 135,830 users\/variation. Given we have 2 groups (treatment and control): the total minimum sample size = 271,660 users. Since we have a total sample size of 290,584, our a\/b test will have enough statistical power and significance.","f6677688":"<b><u>Discussion:<\/u><\/b>\r\n\r\n\r\nWhile this was a satisfactory A\/B test, A\/B test data in a real-life setting will not be as clear-cut as what we've been provided.\r\n\r\nDesigning the A\/B test involves many steps to prevent factors that threaten your validity:\r\n\r\n1. Issues such as the seasonality effect need to be heavily considered. An individual user's behaviour will change a lot depending on time, what day of the week it is (weekday vs weekend) and seasonality. A solution would be the hold-back method: launch the change to everyone except for one small hold-back group of users, and continue comparing their behavior to the control group (Peng, 2017). In our case, our A\/B test was run over a duration of over 3 weeks right after the New Year, which should smooth out seasonality effects.\r\n\r\n2. Other noteworthy and more direct psychological effects are change aversion and the novelty effect which affect our treatment group. Cohort Analysis may be helpful (Peng, 2017).\r\n\r\n3. There can be statistical issues such as Interference, where the assumption of independence is violated between treatment and control groups. Interference will result in what is often known as the overestimation bias, because resources are being co-utilized by members in both control and treatment groups (Rosidi, 2021).\r\n\r\n4. Business Impact discussions can involve engineering costs\/maintenance, customer support, opportunity cost, etc. If we run a successful A\/B test and decide on changing our website that results in a worthwhile increase in our metric, but our ongoing costs off-set this, then that must be taken into consideration (Peng, 2017).\r\n\r\n5. There are even considerably minute effects that could poison our A\/B test data, such as the flicker effect: This is when the original variation flashes on the treatment user's screens before variation B gets loaded (Dube, 2019).","d1b7f2ff":"<u>F-test:<\/u>\r\n\r\nFirst, we must test if our variances between our treatment and control are significantly different. Therefore, we'll be using the F-test in Scipy Stats. Note that the F-Test is sensitive to non-normalities of groups, but our large enough sample size allows the Central Limit Theorem to take effect.\r\n","15ef61a7":"The biggest e-commerce company called <b>FaceZonGoogAppFlix<\/b> as approached us (a data science consulting firm) as a new client! \r\n\r\nThey have a potential new webpage designed with the intention to increase their current conversion rates of 12% by 0.35% or more. With such an ambiguous task, they have full trust in us to give them a recommendation whether to implement the new page or keep the old page. Unforunately they haven't built up a data science capability in their company, but they've used an external software called 'A\/B Tester' for 23 days and come back to us with a dataset. What do we do?\r\n\r\nThe goal of this notebook will serve as a primer into A\/B testing and go through as many of the foundations of A\/B testing I found online. In my opinion, A\/B testing isn't really explored properly or well enough, so hopefully this notebook will fill in this void. Through this notebook, I've placed some references and consolidated pieces of their methodologies here.\r\n\r\n    This notebook assumes a very surface and high-level level understanding of A\/B Testing - a statistical way to compare two or more versions (A or B?) to determine not only which one performs better but also to understand if the difference between two of them is statistically significant (Data Science Dojo, 2018).\r\n    \r\n    Furthermore, the current conversion rate of 12% is chosen from assumptions from the control data, and the intention to increase conversion rates by 0.35% is intentionally chosen as the lowest practical value to achieve the highest minimum Sample Size required. More on these later.","3ceaea05":"Given our one-sided p-value = 0.3388 > 0.05, we do not reject the Null Hypothesis that there is no difference between the groups.\r\n    \r\n    This means that the new page is not better than the old page.\r\n","af53c38b":"Using the F-test (under the Null Hypothesis that the variances are equal) to determine if the variances within each sample are equal, noting that the p-value = 1 - CDF (Zach, 2020):\r\n","e48073d2":"Given our p-value \u2248 0.9 > 0.05, we do not reject the Null Hypothesis.\r\n\r\n    This means that the new page is not better than the old page.","4f29e0d3":"<b><u>References:<\/u><\/b>\n\nData Science Dojo (2018) <i>What is A\/B Testing? | Data Science in Minutes<\/i>, Youtube.\n\nDube, S. (2019) <i>Top 6 A\/B Testing Questions Answered | #CRO <\/i>, Invesp.\n\nFrost, J. (2018) <i>Practical vs. Statistical Significance<\/i>,  Statistics by Jim.\n\njkndrkn (2012) <i>Safely determining sample size for A\/B testing<\/i>, Stats.stackexchange.\n\nKohavi, R., Tang, D. & Xu, Y. (2020) <i>Trustworthy Online Controlled Experiments - A practical guide to A\/B Testing<\/i>, Cambridge University Press.\n\nLan (2011) <i>SRelation between confidence interval and testing statistical hypothesis for t-test<\/i>, Stats.stackexchange.\n\nPeng, K. (2017) <i>A Summary of Udacity A\/B Testing Course<\/i>, Towards Data Science.\n\nRosidi, N. (2021) <i>A\/B Testing Data Science Interview Questions Guide<\/i>, Stratascratch.\n\nSaleh, K. (2018) <i>How To Calculate A\/B Testing Sample Size<\/i>, Invesp.\n\nShetty, N. (2016) <i>A\/B Testing - AWS<\/i>, Rstudio.\n\nShier, R. (2004) <i>Statistics: 2.1 The sign test<\/i>, Mathematics Learning Support Centre.\n\nTan, S. G. & Tan, S. B. (2010) <i>The Correct Interpretation of Confidence Intervals<\/i>, Proceedings of Singapore Healthcare, Volume 19, Number 3.\n\nvan Belle, G. (2008) <i>Statistical Rules of Thumb. 2nd edition<\/i>, WileyInterscience.\n\nWahi, M. (2020) <i>The Data Science of Experimental Design<\/i>, LinkedIn Learning.\n\nYu, P. (2020) <i>Understanding Power Analysis in AB Testing<\/i>, Towards Data Science.\n\nZach (2020) <i>How to Perform an F-Test in Python<\/i>, Statology.\n\nzthomas.nc (2019) <i>AB test sample size calculation by hand<\/i>, Stats.stackexchange. hand.<\/i>, Stats.stackexchange.","289e9f36":"<u><b>Sample Size<\/b><\/u>\r\n\r\nHaving a required Sample Size is one of the important cornerstones of a successful A\/B Test and is dependant on 3 factors (Saleh, 2018):\r\n\r\n1. Power of the test (usually $1 - \\beta = 0.8$): Probability of detecting that there is a difference between the conversion rates and rejecting the Null Hypothesis when the Null Hypothesis is True (Type II Error).    \r\n\r\n2. Significance level of the test ($\\alpha = 0.05$) - Probability that we reject the Null Hypothesis while it should NOT be rejected (Type I Error).\r\n\r\n3. Minimal Desired Effect (MDE, or $D_{min}$) - The desired relevant difference between the rates you would like to discover - what is the minimum improvement for a test to be worthwhile?\r\n\r\nAs a discussion point, A\/B Tests can begin to go bad when a Sample Size is not calculated before the A\/B test, or 'peeking' at your results before the total number of observations is less than your minimum Sample Size and concluding prematurely when you think you've achieved a positive result. Usually if an A\/B Test is running and there aren't enough users to match the required Sample Size, the 3 factors would have to be adjusted to lower the minimium Sample Size.\r\n\r\nThis online calculator does a great job of calculating Sample Size: https:\/\/www.evanmiller.org\/ab-testing\/sample-size.html\r\n\r\nHowever, we're going to manually calculate the required Sample Size using two different methods:\r\n\r\n1. The statistical rule of thumb:\r\n\r\n$n \u2248\r\n \\frac{16 \\sigma^{2}}{\\delta^{2}}$ (van Belle, 2008)\r\n\r\n2. Evan Miller's Calculator above, but we'll use a formula that is the foundation of Evan Miller's Calculator (zthomas.nc, 2019)\r\n","1fa3824b":"<u><b>Data Cleansing:<\/b><\/u>","39ff4105":"<u><b>Metrics<\/b><\/u>\r\n\r\nThis is probably something that I've seen where it's not exactly talked about in most A\/B tests found in Kaggle, but is a crucial talking point.\r\n\r\nThe purpose of Metrics (both Invariate and Evaluation Metrics) are to ensure during our experiment that we didn't screw up throughout its lifecycle. It helps verify that the experiment was conducted as expected and that other factors did not influence the data which we collected.\r\n\r\nImagine for this particular A\/B experiment, we propose to divide users exactly in half into the old and new page (50\/50) BEFORE we even begin. Now imagine we don't bother checking this Metric during or after the experiment, and we conclude that the new page is better, whoopee. But what if we checked the proportion AFTER the test was (33\/67)? Maybe the A\/B test software had a bug? Maybe the code to split the traffic had a bug? Either way...If this split drastically changes throughout our experiment, our data is inherently wrong.\r\n\r\nThere are two types of Metrics: \r\n\r\n1. Invariant (or Guardrail Metric, which seems to be the most updated term (Kohavi, Tang & Xu, 2020)) - similar to the example above, we expect Guardrail Metrics to not change.\r\n\r\n2. Evaluation Metric - these Metrics we DO expect to change due to the nature of A\/B testing. For each of these evaluation metrics, they should be related to the business objectives. \r\n\r\nThere are many Metrics in a real A\/B test we should always have our eyes on (the way we present change to a part of the population, the way we collect data, seasonality and timing of the experiment, daily active users, Click Through Probability, etc.)\r\n\r\nOne Guardrail Metric we can test given our dataset is the <b>number of unique users in each page group<\/b>. The experiment was designed to keep a 50\/50 split, so let's see if this holds true post-A\/B testing.\r\n\r\nOne Evaluation Metric we want to define will be the <b>increase in conversions<\/b>. For this metric, we want to define $D_{min}$: the minimum change which is practically significant to the business. In this case, the practical minimum difference would be the 0.35%, closely related to our business objectives of increasing our current conversion rates of 12% by 0.35%. Therefore, we'll set $D_{min}$ = 0.0035. The 0.0035 is also usually a sign that our clients are a big e-commerce company and such an increase in conversion would have a huge impact on their top-line (Yu, 2020).","22fd980e":"We can quickly observe that our new page isn't doing too hot on conversion improvement. In fact, it's doing slightly worse than the control group!\r\n\r\nHypothetically we could end the test here, but there are a few checks and measures we must do before and after an A\/B test to ensure our experiment as run properly.","a5e47c66":"Since the p-value is almost 1, we do not reject the Null Hypothesis that the variances are the same. Now that we've verified that our variances within each sample are equal, we'll continue calculating $CI_{diff}$ :"}}