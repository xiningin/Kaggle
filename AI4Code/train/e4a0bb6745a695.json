{"cell_type":{"295c02e1":"code","e91f8af1":"code","91fa60db":"code","46540acb":"code","f31e0626":"code","90665683":"code","6f9503f3":"code","d0ee2d5b":"code","47b47bf8":"code","0384457b":"code","5fc6e1e7":"code","c5427228":"code","6e61406b":"code","34065f8a":"code","1673a6c8":"code","2fe1c4ce":"code","87c96eaf":"code","624191af":"code","a7eb89ad":"code","e5a14f9a":"code","f42d620d":"code","a85d5c5c":"code","426424bd":"code","b9e03ec0":"code","9f4deee2":"code","fa42b4b8":"code","cf664e66":"code","f661009c":"code","2a0cbbca":"code","400826c2":"code","90e14cd4":"code","e402554c":"code","0e5709ba":"code","e3c314fe":"code","3aa21096":"code","336b78ce":"code","8b05c332":"markdown","1641ca1c":"markdown","79f243a4":"markdown","bb0db619":"markdown","292a5dee":"markdown","cd3a68cb":"markdown","8023d48e":"markdown","ab124651":"markdown","fb914072":"markdown","1b0eccf6":"markdown","37ae3e33":"markdown","5c86ff59":"markdown","e34609b7":"markdown","d4dac5db":"markdown","2aa5db2f":"markdown","1da26619":"markdown","89a93d4d":"markdown","082784be":"markdown","e19dcd98":"markdown","383aeafa":"markdown","c94a2652":"markdown","964cbb6b":"markdown","b7151a50":"markdown","63d0c482":"markdown","b7e1e676":"markdown","4c66e25c":"markdown","96a435ea":"markdown","0527003a":"markdown","197b1250":"markdown","50bea9e9":"markdown","7e7cf961":"markdown"},"source":{"295c02e1":"!pip install -U nb_black watermark","e91f8af1":"%load_ext lab_black\n%load_ext watermark\n\n%watermark -v -m -p numpy,matplotlib,tensorflow,imageio","91fa60db":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\n\n%matplotlib inline\n\n\nIMAGE_SIZE_NO_CROP = 256  # Size of image before cropping\nIMAGE_SIZE = 64  # Shapes of input image\nBATCH_SIZE = 64  # Batch size\nDATA_PATH = \"\/kaggle\/input\/celeba-dataset\/img_align_celeba\"\nRANDOM_SEED = 42\n\ntf.random.set_seed(RANDOM_SEED)","46540acb":"print(tf.config.experimental.list_physical_devices(\"GPU\"))\nprint(tf.test.gpu_device_name())","f31e0626":"num_images = len(os.listdir(os.path.join(DATA_PATH, \"img_align_celeba\")))\nprint(f\"Num images: {num_images}\")","90665683":"celeb_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    DATA_PATH,\n    label_mode=None,\n    color_mode=\"rgb\",\n    batch_size=BATCH_SIZE,\n    image_size=(IMAGE_SIZE_NO_CROP, IMAGE_SIZE_NO_CROP),\n    seed=RANDOM_SEED,\n)","6f9503f3":"CACHE_FILE = \"cache\"\n\n\ndef process(image):\n    #     images are centered on eyes, we will crop faces utilizing that fact\n    height, width = image.shape[1], image.shape[2]\n\n    offset_height = int(height * 0.35)\n    offset_width = int(height * 0.27)\n\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, int(width * 0.45), int(height * 0.45)\n    )\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE], preserve_aspect_ratio=True)\n\n    image = tf.cast((image - 127.5) \/ 127.5, tf.float32)\n    return image\n\n\nceleb_dataset = (\n    celeb_dataset.map(process)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    .cache(filename=CACHE_FILE)\n)","d0ee2d5b":"fig = plt.figure(figsize=(8, 4), constrained_layout=True)\n\n\nfor images in celeb_dataset.take(1):\n    for i in range(8):\n        ax = plt.subplot(2, 4, i + 1)\n        plt.imshow((images[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n        plt.axis(\"off\")","47b47bf8":"Z_DIM = 100  # Dimension of face's manifold\nGENERATOR_DENSE_SIZE = 512  # Length of first tensor in generator\nN_CHANNELS = 3  # Number channels of input image\nNUM_CONV_DISCRIMINATOR = 4  # amount of convolution layers in discriminator model","0384457b":"from tensorflow.keras import layers\nfrom tensorflow.keras import Sequential\n\n\ndef make_generator_model():\n    model = Sequential()\n    model.add(\n        layers.Dense(\n            4 * 4 * GENERATOR_DENSE_SIZE,\n            use_bias=False,\n            input_shape=(Z_DIM,),\n        )\n    )\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((4, 4, GENERATOR_DENSE_SIZE)))\n\n    assert model.output_shape == (\n        None,\n        4,\n        4,\n        GENERATOR_DENSE_SIZE,\n    )  # Note: None is the batch size\n\n    depth_mul = 1  # Depth decreases as spatial component increases.\n    size = 4  # Size increases as depth decreases.\n\n    while size < IMAGE_SIZE \/\/ 2:\n        filters = int(GENERATOR_DENSE_SIZE * depth_mul)\n        model.add(\n            layers.Conv2DTranspose(\n                filters,\n                (5, 5),\n                strides=(2, 2),\n                padding=\"same\",\n                use_bias=False,\n            )\n        )\n        assert model.output_shape == (\n            None,\n            size * 2,\n            size * 2,\n            filters,\n        )\n        model.add(layers.BatchNormalization())\n        model.add(layers.LeakyReLU())\n\n        size *= 2\n        depth_mul \/= 2\n\n    model.add(\n        layers.Conv2DTranspose(\n            3,\n            (5, 5),\n            strides=(2, 2),\n            padding=\"same\",\n            use_bias=False,\n            activation=\"tanh\",\n        )\n    )\n    assert model.output_shape == (None, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS)\n\n    return model","5fc6e1e7":"generator = make_generator_model()","c5427228":"def make_discriminator_model():\n    model = Sequential()\n\n    for i in range(NUM_CONV_DISCRIMINATOR):\n        model.add(\n            layers.Conv2D(\n                64 * 2 ** i,\n                (5, 5),\n                strides=(2, 2),\n                padding=\"same\",\n                input_shape=[IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS],\n            )\n        )\n        model.add(layers.LeakyReLU())\n        model.add(layers.Dropout(0.2))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","6e61406b":"discriminator = make_discriminator_model()","34065f8a":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","1673a6c8":"from tensorflow.keras import optimizers\n\n\ngenerator_optimizer = optimizers.Adam(1e-4)\ndiscriminator_optimizer = optimizers.Adam(1e-4)","2fe1c4ce":"EPOCHS = 30\nNUM_SAMPLES_TO_GENERATE = 8\nNUM_CHECKPOINT = 10\n\n\n# You will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.uniform([NUM_SAMPLES_TO_GENERATE, Z_DIM])","87c96eaf":"checkpoint_dir = \".\/training_checkpoints\"\n\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n\ncheckpoint = tf.train.Checkpoint(\n    generator_optimizer=generator_optimizer,\n    discriminator_optimizer=discriminator_optimizer,\n    generator=generator,\n    discriminator=discriminator,\n)","624191af":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.uniform([BATCH_SIZE, Z_DIM])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n        gradients_of_generator = gen_tape.gradient(\n            gen_loss, generator.trainable_variables\n        )\n        gradients_of_discriminator = disc_tape.gradient(\n            disc_loss, discriminator.trainable_variables\n        )\n\n        generator_optimizer.apply_gradients(\n            zip(gradients_of_generator, generator.trainable_variables)\n        )\n        discriminator_optimizer.apply_gradients(\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\n        )","a7eb89ad":"from IPython.display import clear_output\nimport time\n\n\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for image_batch in dataset:\n            train_step(image_batch)\n\n        # Produce images for the GIF as you go\n        clear_output(wait=True)\n        generate_and_save_images(generator, epoch + 1, seed)\n\n        # Save the model every 15 epochs\n        if (epoch + 1) % NUM_CHECKPOINT == 0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n\n        print(\"Time for epoch {} is {} sec\".format(epoch + 1, time.time() - start))\n\n    # Generate after the final epoch\n    clear_output(wait=True)\n    generate_and_save_images(generator, epochs, seed)","e5a14f9a":"def generate_and_save_images(model, epoch, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)\n\n    fig = plt.figure(figsize=(8, 4), constrained_layout=True)\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(2, 4, i + 1)\n        plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n        plt.axis(\"off\")\n\n    plt.savefig(\"image_at_epoch_{:04d}.png\".format(epoch))\n    plt.show()","f42d620d":"train(celeb_dataset, EPOCHS)","a85d5c5c":"# to load weights execute this\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","426424bd":"import imageio\nimport glob\nfrom IPython.display import Image\n\n\nanim_file = \"dcgan.gif\"\n\nfilenames = glob.glob(\"image_at_epoch_*.png\")\nimages = [imageio.imread(filename) for filename in sorted(filenames)]\nimageio.mimsave(anim_file, images, fps=8)\n\n# to show in notebook\n# Image(anim_file)","b9e03ec0":"vectors = tf.random.uniform([16, Z_DIM])\n\npredictions = generator(vectors, training=False)","9f4deee2":"fig = plt.figure(figsize=(8, 8), constrained_layout=True)\n\nfor i in range(predictions.shape[0]):\n    plt.subplot(4, 4, i + 1).set_title(i)\n    plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n    plt.axis(\"off\")","fa42b4b8":"NUM_ALPHAS = 30\n\nidx_first, idx_second = 6, 14\nalphas = np.linspace(0, 1, NUM_ALPHAS)\n\nto_interpolate = [vectors[idx_first]]\nfor alpha in alphas[::-1]:\n    to_interpolate.append(\n        alpha * vectors[idx_first] + (1 - alpha) * vectors[idx_second]\n    )\n\nto_interpolate.append(vectors[idx_second])\nto_interpolate = np.array(to_interpolate)\n\npredictions = generator(to_interpolate, training=False)","cf664e66":"for i in range(predictions.shape[0]):\n    fig = plt.figure(figsize=(4, 4), constrained_layout=True)\n    plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n    plt.axis(\"off\")\n    plt.savefig(f\"face_{i:02d}.png\")\n    plt.close()","f661009c":"anim_file = \"face.gif\"\n\nfilenames = glob.glob(\"face*.png\")\nimages = [imageio.imread(filename) for filename in sorted(filenames)]\nimageio.mimsave(anim_file, images, fps=5)\n\n# to show in notebook\n# Image(anim_file)","2a0cbbca":"to_test = tf.random.uniform([36, Z_DIM])\n\n\npredictions = generator(to_test, training=False)\n\nfig = plt.figure(figsize=(16, 16))\n\nfor i in range(predictions.shape[0]):\n    plt.subplot(6, 6, i + 1).set_title(i)\n    plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n    plt.axis(\"off\")","400826c2":"smiliing_indices = [4, 8, 14, 30, 31]\nnot_smiling_indices = [3, 6, 15, 25, 26]\n\nsmiliing_array = np.array([to_test[i] for i in smiliing_indices])\nnot_smiling_array = np.array([to_test[i] for i in not_smiling_indices])","90e14cd4":"def predict_and_plot(array, title=None):\n    predictions = generator(array, training=False)\n\n    fig = plt.figure(figsize=(16, 4))\n    if title:\n        fig.suptitle(title)\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(1, 5, i + 1)\n        plt.imshow((predictions[i].numpy() * 127.5 + 127.5).astype(\"uint8\"))\n        plt.axis(\"off\")","e402554c":"predict_and_plot(smiliing_array, \"Smiling faces\")","0e5709ba":"predict_and_plot(not_smiling_array, \"Not smiling faces\")","e3c314fe":"smile_arr_vec = smiliing_array.mean(axis=0)\nnot_smile_arr_vec = not_smiling_array.mean(axis=0)\n\nsmile_vec = smile_arr_vec - not_smile_arr_vec","3aa21096":"predict_and_plot(not_smiling_array, \"Original\")\npredict_and_plot(not_smiling_array + smile_vec, \"+ smile vector\")\npredict_and_plot(not_smiling_array - smile_vec, \"- smile vector\")","336b78ce":"predict_and_plot(smiliing_array, \"Original\")\npredict_and_plot(smiliing_array + smile_vec, \"+ smile vector\")\npredict_and_plot(smiliing_array - smile_vec, \"- smile vector\")","8b05c332":"### Building a smile vector","1641ca1c":"Run next cell to train and subsequent cell to load model","79f243a4":"Checking available GPUs","bb0db619":"That's what we have chosen to build a \"smile vector\"","292a5dee":"### Some network parameters","cd3a68cb":"#### Not smiling faces","8023d48e":"# Preparing the dataset\n\nHere we will check and prepare our data. \nWe need the faces only. Images in the dataset are centered on eyes, so we will crop faces utilizing that fact.\n\nI've found caching is extremely useful in this task. The whole dataset can be put into memory if you have >12GB RAM.\nPrefetching will also help us to utilize resources better.\n\nSometimes image_dataset_from_directory is slow as fuck. Also Kaggle won't let us to cache everything in memory and will kill the kernel during the training, that's frustrating.\n\nNevertheless training on the whole dataset will take some time (first epoch with BATCH_SIZE=64 takes ~1800 seconds to finish with GPU accelerator here, ~1300 seconds for BATCH_SIZE=512, after caching it's ~300 seconds per epoch).","ab124651":"## Common imports and variables","fb914072":"<img src=\"https:\/\/media.giphy.com\/media\/pvqX59RXU4GAKMUqm2\/giphy.gif\"><\/img>","1b0eccf6":"Let's take 6 and 14. The first is looking more feminine, so it's a woman, the second is a man.","37ae3e33":"And make animation!","5c86ff59":"Let's see what we have got","e34609b7":"# Training\n\nHere we will define some functions to determine the training process. We will train both models simultaneously.\nThe generator will be taking BATCH_SIZE random vectors at the every step and make images. The discriminator at first will check true images, then output and finally we will compute losses using both real and fake ones.\n\nThe model will be trained for 30 epochs. Every 15 epochs the weights of the model are being saved.\nWe will show progress on generating 8 images for the same random vectors after the end of every epoch.","d4dac5db":"### Generator\n\n\nGenerator has the folllowing architecture:\n\n<img src=\"http:\/\/bamos.github.io\/data\/2016-08-09\/discrim-architecture.png\">\n\nHere we have dense input and the rest of layers are transposed convolutions.\n\nA transposed convolution will reverse the spatial transformation of a regular convolution with the same parameters.\nIf you perform a regular convolution followed by a transposed convolution and both have the same settings (kernel size, padding, stride), then the input and output will have the same shape. This makes it super easy to build encoder-decoder networks with them. \n\nHere are some notes on the architecture of the generator:\n1. The deeper the convolution, the less filters it uses.\n1. Deconvolutions-relu layers are applied to achieve input image shape.\n1. Batch normalization is used before nonlinearity for speed and stability of learning.\n1. Tanh activation at the end of network allows to scale images to [-1, 1].\n1. To force generator not to collapse and produce different outputs bias is initialized with zero.","2aa5db2f":"# Making smiling faces\n\nHere we will check for some faces that are smiling and some that are not to extract \"smiling\" vector. Later we will apply that vector to inputs for getting smiliing images as outputs and vice versa.\n\nWe denote a \"smile vector\" as mean of vectors z with generated smile on it minus mean of vectors z without generated smile on it.","1da26619":"We will save images for every prediction and two original vectors","89a93d4d":"Gosh! The faces at the last row are looking so judgmental. \"Look what you've done\"","082784be":"### Applying smile vector\n\n\nTime to apply it to smiling faces, not smiling faces, it's also worth to try to apply anti-smiling vector that should make faces sad. Oof!","e19dcd98":"### Discriminator\n\n\nDiscriminator takes 3D tensor as input and outputs one number that is a probability of input being a face. Its architecture is quite similar to \"reverse\" generator.","383aeafa":"Nice!","c94a2652":"### Loss functions\n\n\nWe will use the following loss functions:\n$$ D\\_loss = \\frac{-1}{m} \\sum_{i=1}^{m}[\\log{D(x_i)} + \\log{(1 - D(G(z_i)))}]$$\n$$ G\\_loss = \\frac{1}{m} \\sum_{i=1}^{m} \\log{(1 - D(G(z_i)))}$$","964cbb6b":"### Optimizers\n\nThere are different optimizers for discriminator and generator","b7151a50":"# Defining a network\n\nWe will build two models: generator and discriminator.\nThe generator is producing images from the noise while the discriminator are trying to distinguish those images from the faces of celebrities.","63d0c482":"It's actually hard to find not smiling ones. I've executed previous cell for several times.\n\nSome faces are really strange. Who's 10th guy? Tiger man? The hell is 29th?\n\nLet's choose up to 5 images for the following groups:\n1. Big smiles: 4, 8, 14, 30, 31\n2. No smiles: 3 (poker face), 6 (reminds me of vampires), 15, 25, 26","b7e1e676":"## Prerequisites\n\nIn this section we will install some useful packages and extensions","4c66e25c":"<img src=\"https:\/\/media.giphy.com\/media\/AYvPzML6qfOPVGZ3I8\/giphy.gif\">","96a435ea":"# Face interpolation\n\nOur model performs quite well. Time for fun things. \n\nAt first, let's try to interpolate between faces: we will generate two vectors $z_1$ and $z_2$ and get a batch of vectors of the form $\\alpha\\cdot z_1 + (1- \\alpha)\\cdot  z_2, \\alpha \\in [0,1]$ for generating faces on them and looking at results.","0527003a":"# Visualizing the results\n\nLet's make an animation showing the progress of our model","197b1250":"Very cool! Isn't it?","50bea9e9":"#### Smiling faces","7e7cf961":"# Making DCGAN to generate faces and fun things using TensorFlow 2 & Keras\n\nWe are going to train GAN for generating faces and then we will make fun playing with it. Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the \u201cadversarial\u201d). One neural network, called the generator, generates new faces, while the other, the discriminator, decides whether each instance of face it reviews belongs to the actual training dataset or not.\n\nWe will use aligned faces of celebrities to train our GAN and make animations to visualize results!\n\nTF1 model source: http:\/\/bamos.github.io\/2016\/08\/09\/deep-completion\/"}}