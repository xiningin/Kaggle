{"cell_type":{"5b1745e1":"code","55e562e4":"code","a7600e50":"code","bfd677dc":"code","a30ce631":"code","cc4ac28f":"code","192b4079":"code","671f1a56":"code","faaf07b2":"code","f0672165":"code","378e9dfd":"code","6781fe47":"code","f6ce72f8":"code","33fb67f3":"code","d1255751":"code","e526b65c":"code","21a0a88d":"code","b0a743b5":"code","1cebe5a7":"code","0c9c5325":"code","217c44e5":"code","83af7e52":"code","4cd5de3b":"code","c7588a72":"code","f4628e0f":"code","3f2a2466":"code","c8566d71":"code","d346f74d":"code","6fb658dd":"code","ac4c0534":"code","500b4fc0":"code","2131e09c":"code","03c14a9d":"code","a0189b7d":"code","8a15c7df":"code","7dec8191":"code","5483b2c4":"code","7cc24274":"code","c6c79f72":"code","e5a40948":"code","763cf602":"code","964ecd4b":"code","962824ca":"code","5d33c93b":"code","685700f3":"code","21ea2129":"code","7b548bf9":"code","4f614fa6":"code","3d0916c9":"code","f714f461":"code","bca106d4":"code","664f58a2":"code","7adc5915":"code","1ace204c":"code","deeefa34":"code","fa9d2a61":"code","2e1df552":"code","77f9bb1f":"code","9b5ac8c2":"code","543a1c93":"code","c3478249":"code","8594184b":"code","b46c6780":"code","78336497":"code","26203600":"code","dbf71e36":"code","1f938eee":"code","01708f69":"code","c9fca659":"code","211df1b5":"code","08fafeab":"code","4c8c184e":"code","603122bc":"code","b0081d9f":"code","4214b828":"code","7c633be3":"code","8bfc9bb7":"code","25a6588b":"code","f98dbab7":"code","5f3f6f6c":"code","04534bdc":"markdown","9d3cbb85":"markdown","cd705c08":"markdown","ff554a49":"markdown","7766029f":"markdown","3437914a":"markdown","f9bd8ef4":"markdown","2b7c46ca":"markdown","4f2bef9c":"markdown","8cbe47ea":"markdown"},"source":{"5b1745e1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata=pd.read_csv(r'..\/input\/car-insurance-data\/Car_Insurance_Claim.csv')","55e562e4":"data.head()","a7600e50":"data.shape","bfd677dc":"data[\"AGE\"].replace({\"16-25\": \"Young\", \"26-39\": \"Middle Age\",\"40-64\":\"Old\",\"65+\":\"Very Old\"}, inplace=True)\ndata[\"DRIVING_EXPERIENCE\"].replace({\"0-9y\": \"Newbie\", \"10-19y\": \"Amateur\",\"20-29y\":\"Advanced\",\"30y+\":\"Expert\"}, inplace=True)\n","a30ce631":"data.isna().sum()","cc4ac28f":"def fillna(dataframe,feature_cols):\n    total_cols=0\n    for y in feature_cols:\n        total_cols+=1\n        if dataframe[y].isna().sum()>1:\n            try:\n                dataframe[y]=dataframe[y].fillna(int(np.mean(dataframe[y])))\n            except ValueError:\n                pass\n        else:\n            continue\n    print(f\"There are {total_cols} columns\")\n\nfeature_cols=[\"CREDIT_SCORE\",\"ANNUAL_MILEAGE\"]\nfillna(data,feature_cols)","192b4079":"data.rename(columns={'OUTCOME':\"LOAN\"}, inplace=True)","671f1a56":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['LOAN'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('LOAN')\nax[0].set_ylabel('')\nsns.countplot('LOAN',data=data,ax=ax[1])\nax[1].set_title('LOAN')\nplt.show()","faaf07b2":"def var_distribution2(dataframe):\n    import matplotlib.pyplot as plt\n    numbers = pd.Series(dataframe.columns)\n    dataframe[numbers].hist(figsize=(14,14))\n    plt.show();\n    print(\"\\nData Variance\")\n    return dataframe.var()\n\nvar_distribution2(data)","f0672165":"def Outliers(dataframe,cols):\n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n    numeric_col2=[]\n    for x in cols:\n        numeric_col2.append(x)\n\n    fig=make_subplots(rows=1, cols=len(cols))\n\n    for i,col in enumerate(numeric_col2):\n        fig.add_trace(go.Box(y=dataframe[col].values, name=dataframe[col].name), row=1, col=i+1)\n\n    return fig.show()\ncols=data.columns.values.tolist()\nOutliers(data,cols)","378e9dfd":"sns.catplot(x=\"RACE\", kind=\"count\", palette=\"muted\", data=data)","6781fe47":"sns.catplot(x=\"GENDER\", kind=\"count\", palette=\"muted\", data=data)","f6ce72f8":"sns.catplot(x=\"MARRIED\", kind=\"count\", palette=\"muted\", data=data)","33fb67f3":"sns.catplot(x=\"CHILDREN\", kind=\"count\", palette=\"muted\", data=data)","d1255751":"sns.catplot(x=\"AGE\", kind=\"count\", palette=\"muted\", data=data)","e526b65c":"sns.catplot(x=\"DRIVING_EXPERIENCE\", kind=\"count\", palette=\"muted\", data=data)","21a0a88d":"sns.catplot(x=\"INCOME\", kind=\"count\", palette=\"muted\", data=data)","b0a743b5":"sns.catplot(x=\"VEHICLE_OWNERSHIP\", kind=\"count\", palette=\"muted\", data=data)","1cebe5a7":"sns.catplot(x=\"EDUCATION\", kind=\"count\", palette=\"muted\", data=data)","0c9c5325":"sns.catplot(x=\"VEHICLE_YEAR\", kind=\"count\", palette=\"muted\", data=data)","217c44e5":"sns.catplot(x=\"VEHICLE_TYPE\", kind=\"count\", palette=\"muted\", data=data)","83af7e52":"\n%matplotlib inline\n\nsns.histplot(binwidth=0.5, x=\"RACE\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")","4cd5de3b":"sns.histplot(binwidth=0.5, x=\"INCOME\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")","c7588a72":"sns.histplot(binwidth=0.5, x=\"MARRIED\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")\n","f4628e0f":"sns.histplot(binwidth=0.5, x=\"CHILDREN\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")","3f2a2466":"sns.histplot(binwidth=0.5, x=\"VEHICLE_OWNERSHIP\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")","c8566d71":"sns.histplot(binwidth=0.5, x=\"EDUCATION\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")","d346f74d":"sns.histplot(binwidth=0.5, x=\"VEHICLE_YEAR\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")","6fb658dd":"sns.histplot(binwidth=0.5, x=\"AGE\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")","ac4c0534":"sns.histplot(binwidth=0.5, x=\"VEHICLE_TYPE\", hue=\"GENDER\", data=data, stat=\"count\",multiple=\"stack\")","500b4fc0":"sns.histplot(binwidth=0.5, x=\"DRIVING_EXPERIENCE\", hue=\"GENDER\", data=data, stat=\"count\", multiple=\"stack\")","2131e09c":"sns.catplot(y=\"GENDER\",x=\"CREDIT_SCORE\",\n            kind=\"violin\", split=True, data=data)","03c14a9d":"sns.catplot(y=\"GENDER\",x=\"ANNUAL_MILEAGE\",\n            kind=\"violin\", split=True, data=data)","a0189b7d":"sns.catplot(y=\"GENDER\",x=\"SPEEDING_VIOLATIONS\",\n            kind=\"violin\", split=True, data=data)","8a15c7df":"sns.catplot(y=\"GENDER\",x=\"DUIS\",\n            kind=\"violin\", split=True, data=data)","7dec8191":"sns.catplot(y=\"GENDER\",x=\"PAST_ACCIDENTS\",\n            kind=\"violin\", split=True, data=data)","5483b2c4":"data.groupby([\"LOAN\"]).mean()","7cc24274":"def corr(dataframe,target_variable):\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(15,15))\n    correlation_matrix = dataframe.corr().round(2)\n    sns.heatmap(data=correlation_matrix, annot=True)\n    \n    correlation = data.corr()[target_variable].abs().sort_values(ascending = False)\n    return correlation\ncorr(data,\"LOAN\")","c6c79f72":"sns.catplot(y=\"GENDER\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","e5a40948":"sns.catplot(y=\"RACE\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","763cf602":"sns.catplot(y=\"MARRIED\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","964ecd4b":"sns.catplot(y=\"CHILDREN\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","962824ca":"sns.catplot(y=\"VEHICLE_TYPE\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","5d33c93b":"sns.catplot(y=\"VEHICLE_YEAR\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","685700f3":"sns.catplot(y=\"EDUCATION\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","21ea2129":"sns.catplot(y=\"INCOME\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","7b548bf9":"sns.catplot(y=\"AGE\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","4f614fa6":"sns.catplot(y=\"VEHICLE_OWNERSHIP\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","3d0916c9":"sns.catplot(y=\"DRIVING_EXPERIENCE\", hue=\"LOAN\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=data)","f714f461":"sns.catplot(x=\"CREDIT_SCORE\" ,y=\"GENDER\", hue=\"LOAN\", kind=\"bar\", data=data)","bca106d4":"sns.catplot(x=\"ANNUAL_MILEAGE\" ,y=\"GENDER\", hue=\"LOAN\", kind=\"bar\", data=data)","664f58a2":"sns.catplot(x=\"DUIS\" ,y=\"GENDER\", hue=\"LOAN\", kind=\"bar\", data=data)","7adc5915":"sns.catplot(x=\"MARRIED\" ,y=\"GENDER\", hue=\"LOAN\", kind=\"bar\", data=data)","1ace204c":"sns.catplot(x=\"SPEEDING_VIOLATIONS\" ,y=\"GENDER\", hue=\"LOAN\", kind=\"bar\", data=data)","deeefa34":"sns.catplot(x=\"PAST_ACCIDENTS\" ,y=\"GENDER\", hue=\"LOAN\", kind=\"bar\", data=data)","fa9d2a61":"sns.catplot(x=\"CHILDREN\" ,y=\"GENDER\", hue=\"LOAN\", kind=\"bar\", data=data)","2e1df552":"sns.catplot(x=\"VEHICLE_OWNERSHIP\" ,y=\"GENDER\", hue=\"LOAN\", kind=\"bar\", data=data)","77f9bb1f":"data=data.drop([\"ID\"],axis=1)","9b5ac8c2":"data.duplicated().sum()","543a1c93":"data=data.drop_duplicates()","c3478249":"data=data[~(data[\"ANNUAL_MILEAGE\"]>=18000)]\ndata=data[~(data[\"ANNUAL_MILEAGE\"]<=5000)]\ndata=data[~(data[\"SPEEDING_VIOLATIONS\"]>=15)]\ndata=data[~(data[\"DUIS\"]>=5)]\ndata=data[~(data[\"PAST_ACCIDENTS\"]>=10)]","8594184b":"from sklearn import preprocessing\ndata2=data\nsubset=[\"CREDIT_SCORE\",\"VEHICLE_OWNERSHIP\",\"MARRIED\",\"CHILDREN\",\"PAST_ACCIDENTS\",\"SPEEDING_VIOLATIONS\",\"DUIS\",\"LOAN\",\"POSTAL_CODE\",\"ANNUAL_MILEAGE\"]\ndata2=data2.drop(subset,axis=1)\ndata2=data2.apply(preprocessing.LabelEncoder().fit_transform)\ndata=data.drop([\"AGE\",\"RACE\",\"GENDER\",\"EDUCATION\",\"DRIVING_EXPERIENCE\",\"INCOME\",\"VEHICLE_YEAR\",\"VEHICLE_TYPE\"],axis=1)\ndata=pd.concat([data,data2],axis=1)","b46c6780":"data.head()","78336497":"def feature_selector(dataframe,feature_number,target_variable):\n    from sklearn import datasets\n    from sklearn.feature_selection import RFE\n    from sklearn.linear_model import LogisticRegression\n    \n    dataframe2=dataframe[target_variable]\n    dataframe=dataframe.drop([target_variable],axis=1)\n     \n    n=feature_number\n    lr = LogisticRegression(solver=\"liblinear\")\n    rfe=RFE(lr,n)\n    rfe=rfe.fit(dataframe,dataframe2)\n    cols=[]\n    for x in dataframe.columns.values.tolist():\n        cols.append(x)\n    ranking=[]\n    for x in rfe.ranking_:\n        ranking.append(x)\n    n=0\n    for x in rfe.support_:\n        print(f\"{ranking[n]}: {x}----> {cols[n]}\")\n        n+=1\n    selected=[]\n    n=0\n    z=zip(dataframe.columns.values.tolist(),rfe.support_)\n    z=list(z)\n    for x in range(len(z)+1):\n    \n        try:\n            if str(z[n][1])==\"True\":\n                selected.append(z[n])\n            else:\n                pass\n        except IndexError:\n            pass\n        n+=1\n    cols_selected=[]   \n    \n    for x,y in selected:\n        cols_selected.append(x)\n    \n    if len(cols_selected)==feature_number:\n        return  cols_selected\n    else:\n        print(\"ERROR!. Cols_Selected does not meet feature_number requirements\")\n","26203600":"feature_selector(data,9,\"LOAN\")","dbf71e36":"def VIF(dataframe,chosen_cols):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n    from statsmodels.tools.tools import add_constant\n    X=dataframe[chosen_cols]\n    X=add_constant(X)\n    vif_data=pd.DataFrame()\n    vif_data[\"feature\"]=X.columns\n    vif_data[\"VIF\"]=[VIF(X.values, i) for i in range(len(X.columns))]\n    return vif_data","1f938eee":"chosen_cols=['CREDIT_SCORE',\n 'VEHICLE_OWNERSHIP',\n 'MARRIED',\n 'CHILDREN',\n 'DUIS',\n 'PAST_ACCIDENTS',\n 'GENDER',\n 'DRIVING_EXPERIENCE',\n 'VEHICLE_YEAR']\nVIF(data,chosen_cols)","01708f69":"def Logistic_Regression(dataframe,feature_number,target_variable,Tresh_Number,t_size):\n    from sklearn.feature_selection import RFE\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split\n    from sklearn import linear_model\n    import pandas as pd\n    from sklearn import metrics\n    import warnings\n    import numpy as np\n    warnings.filterwarnings('ignore')\n    dataframe2=dataframe[target_variable]\n    dataframe=dataframe.drop([target_variable],axis=1)\n    n=feature_number\n    lr = LogisticRegression(solver=\"liblinear\")\n    rfe=RFE(lr,n)\n    rfe=rfe.fit(dataframe,dataframe2)\n    selected=[]\n    z=zip(dataframe.columns.values.tolist(),rfe.support_)\n    z=list(z)\n    n2=0\n    for x in range(len(z)+1):\n        try:\n            if str(z[n2][1])==\"True\":\n                selected.append(z[n2])\n            else:\n                pass\n        except IndexError:\n            pass\n        n2+=1\n    cols_selected=[]   \n    for x,y in selected:\n        cols_selected.append(x)\n    X=dataframe[cols_selected]\n    Y=dataframe2\n    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=t_size,random_state=0)\n    lm=linear_model.LogisticRegression()\n    lm.fit(X_train,Y_train)\n    predict=lm.predict(X_test)\n    probs=lm.predict_proba(X_test)\n    prob=probs[:,1]\n    prob_df=pd.DataFrame(prob)\n    \n    if 0 < Tresh_Number < 1:\n        treshold = Tresh_Number\n    else:\n        print(\"ERROR! Treshold must be between 0 and 1\")\n        return\n    print(f\"Prediction Accuracy (Test): {metrics.accuracy_score(Y_test,predict)}\")\n    print()\n    prob_df[\"prediction\"]= np.where(prob_df[0]> treshold,1,0)\n    print(prob_df)\n    print(f\"\\nTreshold: {Tresh_Number}\")\n    print()\n    con_tab=pd.crosstab(prob_df[\"prediction\"],columns=\"Count\")\n    print(f\"Number of Positive Cases: {con_tab.values[1]\/len(prob_df)*100}%\")\n    return con_tab\n    ","c9fca659":"Logistic_Regression(data,9,\"LOAN\",0.6,0.25)","211df1b5":"from ggplot import *\ndef ggplot_ROC_AUC(dataframe,feature_number,target_variable,t_size):\n    from sklearn import datasets\n    from sklearn.feature_selection import RFE\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split\n    \n    dataframe2=dataframe[target_variable]\n    dataframe=dataframe.drop([target_variable],axis=1)\n    \n    n=feature_number\n    \n    lr = LogisticRegression(solver=\"liblinear\")\n    rfe=RFE(lr,n)\n    rfe=rfe.fit(dataframe,dataframe2)\n    \n    selected=[]\n    z=zip(dataframe.columns.values.tolist(),rfe.support_)\n    z=list(z)\n    \n    n2=0\n    for x in range(len(z)+1):\n        try:\n            if str(z[n2][1])==\"True\":\n                selected.append(z[n2])\n            else:\n                pass\n        except IndexError:\n            pass\n        n2+=1\n        \n    cols_selected=[]   \n    for x,y in selected:\n        cols_selected.append(x)\n        \n    X=dataframe[cols_selected]\n    Y=dataframe2\n    \n    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=t_size,random_state=0)\n    \n    from sklearn import linear_model\n    lm=linear_model.LogisticRegression()\n    lm.fit(X_train,Y_train)\n    \n    predict=lm.predict(X_test)\n    probs=lm.predict_proba(X_test)\n    prob=probs[:,1]\n    prob_df=pd.DataFrame(prob)\n    \n    from sklearn import metrics\n  \n    \n    especifities,sensibilities,_=metrics.roc_curve(Y_test,prob)\n    \n    df=pd.DataFrame({\n        \"x\":especifities,\n        \"y\":sensibilities\n    })\n    \n    auc=metrics.auc(especifities,sensibilities)\n    \n    print(f\"The AUC is: {auc}\")\n    \n    \n    \n    return ggplot(df,aes(x=\"x\",y=\"y\"))+geom_line()+geom_abline(linetype=\"dashed\")+xlim(-0.01,1.01)+ylim(-0.01,1.01)    ","08fafeab":"ggplot_ROC_AUC(data,9,\"LOAN\",0.25)","4c8c184e":"def sklearn_decision_tree_classifier(dataframe,chosen_cols,target_variable,max_depth):\n    from sklearn.tree import DecisionTreeClassifier\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn import metrics\n    X=dataframe[chosen_cols]\n    Y=dataframe[target_variable]\n    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\n    \n    tree=DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=int((len(dataframe)\/16)),max_depth=max_depth)\n    tree.fit(X_train,Y_train)\n    predict=tree.predict(X_test)\n    predict2=tree.predict(X_train)\n    print(f\"Prediction Accuracy (Train): {metrics.accuracy_score(Y_train,predict2)}\")\n    print(f\"Prediction Accuracy (Test): {metrics.accuracy_score(Y_test,predict)}\")","603122bc":"sklearn_decision_tree_classifier(data,chosen_cols,\"LOAN\",10)","b0081d9f":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom ggplot import *\nX=data[chosen_cols]\nY=data[\"LOAN\"]\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\ntree=DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=int((len(data)\/16)),max_depth=10)\ntree.fit(X_train,Y_train)\npredict=tree.predict(X_test)\n\nprobs=tree.predict_proba(X_test)\nprob=probs[:,1]\nprob_df=pd.DataFrame(prob)\n\nfrom sklearn import metrics\n  \n    \nespecifities,sensibilities,_=metrics.roc_curve(Y_test,prob)\n    \ndf=pd.DataFrame({\n    \"x\":especifities,\n    \"y\":sensibilities\n})\n    \nauc=metrics.auc(especifities,sensibilities)\n    \nprint(f\"The AUC is: {auc}\")\n    \nggplot(df,aes(x=\"x\",y=\"y\"))+geom_line()+geom_abline(linetype=\"dashed\")+xlim(-0.01,1.01)+ylim(-0.01,1.01)    \n    \n","4214b828":"def sklearn_Random_Forest_Classifier(dataframe,chosen_cols,target_variable):\n    from sklearn.ensemble import RandomForestClassifier\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn import metrics\n  \n    X=dataframe[chosen_cols]\n    Y=dataframe[target_variable]\n    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\n    forest=RandomForestClassifier(n_jobs=2,n_estimators=500)\n    forest.fit(X_train,Y_train)\n    predict=forest.predict(X_test)\n    predict2=forest.predict(X_train)\n    print(f\"Prediction Accuracy (Train): {metrics.accuracy_score(Y_train,predict2)}\")\n    print(f\"Prediction Accuracy (Test): {metrics.accuracy_score(Y_test,predict)}\")","7c633be3":"sklearn_Random_Forest_Classifier(data,chosen_cols,\"LOAN\")","8bfc9bb7":"from sklearn.ensemble import RandomForestClassifier\nfrom ggplot import *\nX=data[chosen_cols]\nY=data[\"LOAN\"]\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\nforest=RandomForestClassifier(n_jobs=2,n_estimators=500)\nforest.fit(X_train,Y_train)\npredict=forest.predict(X_test)\n\nprobs=forest.predict_proba(X_test)\nprob=probs[:,1]\nprob_df=pd.DataFrame(prob)\n\nfrom sklearn import metrics\n  \n    \nespecifities,sensibilities,_=metrics.roc_curve(Y_test,prob)\n    \ndf=pd.DataFrame({\n    \"x\":especifities,\n    \"y\":sensibilities\n})\n    \nauc=metrics.auc(especifities,sensibilities)\n    \nprint(f\"The AUC is: {auc}\")\n    \nggplot(df,aes(x=\"x\",y=\"y\"))+geom_line()+geom_abline(linetype=\"dashed\")+xlim(-0.01,1.01)+ylim(-0.01,1.01)    \n    \n","25a6588b":"def Sklearn_KNNC(dataframe,chosen_cols,target_variable):\n    from sklearn.neighbors import KNeighborsClassifier\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn import metrics\n  \n    X=dataframe[chosen_cols]\n    Y=dataframe[target_variable]\n    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\n    KNNC=KNeighborsClassifier()\n    KNNC.fit(X_train,Y_train)\n    predict=KNNC.predict(X_test)\n    predict2=KNNC.predict(X_train)\n    print(f\"Prediction Accuracy (Train): {metrics.accuracy_score(Y_train,predict2)}\")\n    print(f\"Prediction Accuracy (Test): {metrics.accuracy_score(Y_test,predict)}\")","f98dbab7":"Sklearn_KNNC(data,chosen_cols,\"LOAN\")","5f3f6f6c":"from sklearn.neighbors import KNeighborsClassifier\nfrom ggplot import *\nX=data[chosen_cols]\nY=data[\"LOAN\"]\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\nKNNC=KNeighborsClassifier()\nKNNC.fit(X_train,Y_train)\npredict=KNNC.predict(X_test)\nprobs=KNNC.predict_proba(X_test)\nprob=probs[:,1]\nprob_df=pd.DataFrame(prob)\n\nfrom sklearn import metrics\n  \n    \nespecifities,sensibilities,_=metrics.roc_curve(Y_test,prob)\n    \ndf=pd.DataFrame({\n    \"x\":especifities,\n    \"y\":sensibilities\n})\n    \nauc=metrics.auc(especifities,sensibilities)\n    \nprint(f\"The AUC is: {auc}\")\n    \nggplot(df,aes(x=\"x\",y=\"y\"))+geom_line()+geom_abline(linetype=\"dashed\")+xlim(-0.01,1.01)+ylim(-0.01,1.01)    \n    \n","04534bdc":"# Data Cleaning","9d3cbb85":"# Predictions","cd705c08":"Our Best Model Is The Decision Tree Clasiffier With 84% Acc","ff554a49":"# EDA","7766029f":"# Data Preprocessing","3437914a":"**RANDOM FORESST CLASSIFIER**","f9bd8ef4":"**K NEAREST NEIGHBORS CLASSIFIER**","2b7c46ca":"# Car Insurance ","4f2bef9c":"**LOGISTIC REGRESSION**","8cbe47ea":"**DECISION TREE CLASSIFIER**"}}