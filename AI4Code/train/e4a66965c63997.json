{"cell_type":{"168fe522":"code","09c8f2db":"code","87fc30b8":"code","927006e9":"code","8d8aae26":"code","de8ff3f9":"code","2a5f9f62":"code","7074600b":"code","0b8b988c":"code","96f2e002":"code","77519869":"code","3f48247b":"markdown","97f97d45":"markdown"},"source":{"168fe522":"!pip install -U torch wandb transformers","09c8f2db":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n!wandb login $secret_value","87fc30b8":"import gc\nimport os\nimport random\nimport wandb\nimport math\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AdamW\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn.preprocessing import RobustScaler\n\ndevice = torch.device(\"cuda\")","927006e9":"class config:\n    EXP_NAME = \"exp137_transformer_head\"\n    CLS_MODEL = \"..\/input\/ventilator-classification-model\/exp075_dropout0\"\n    \n    INPUT = \"\/kaggle\/input\/ventilator-pressure-prediction\"\n    OUTPUT = \"\/kaggle\/working\"\n    N_FOLD = 5\n    SEED = 0\n    \n    LR = 5e-3\n    N_EPOCHS = 50\n    EMBED_SIZE = 64\n    HIDDEN_SIZE = 256\n    BS = 512\n    WEIGHT_DECAY = 1e-3\n\n    USE_LAG = 4\n    CATE_FEATURES = ['R_cate', 'C_cate', 'RC_dot', 'RC_sum']\n    CONT_FEATURES = ['u_in', 'u_out', 'time_step'] + ['u_in_cumsum', 'u_in_cummean', 'area', 'cross', 'cross2']\n    LAG_FEATURES = ['breath_time']\n    LAG_FEATURES += [f'u_in_lag_{i}' for i in range(1, USE_LAG+1)]\n    LAG_FEATURES += [f'u_in_time{i}' for i in range(1, USE_LAG+1)]\n    LAG_FEATURES += [f'u_out_lag_{i}' for i in range(1, USE_LAG+1)]\n    ALL_FEATURES = CATE_FEATURES + CONT_FEATURES + LAG_FEATURES\n    NORM_FEATURES = CONT_FEATURES + LAG_FEATURES\n    \n    NOT_WATCH_PARAM = ['INPUT']","8d8aae26":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","de8ff3f9":"class VentilatorDataset(Dataset):\n    \n    def __init__(self, df, label_dic=None):\n        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n        self.label_dic = label_dic\n        \n    def __len__(self):\n        return len(self.dfs)\n    \n    def __getitem__(self, item):\n        df = self.dfs[item]\n        X = df[config.ALL_FEATURES].values\n        y = df['pressure'].values\n\n        d = {\n            \"X\": torch.tensor(X).float(),\n            \"y\" : torch.tensor(y).float(),\n        }\n        return d","2a5f9f62":"class VentilatorModel(nn.Module):\n    \n    def __init__(self):\n        super(VentilatorModel, self).__init__()\n        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n        self.rc_dot_emb = nn.Embedding(8, 4, padding_idx=0)\n        self.rc_sum_emb = nn.Embedding(8, 4, padding_idx=0)\n        self.seq_emb = nn.Sequential(\n            nn.Linear(12+len(config.NORM_FEATURES), config.EMBED_SIZE),\n            nn.LayerNorm(config.EMBED_SIZE),\n        )\n        \n        self.lstm = nn.LSTM(config.EMBED_SIZE, config.HIDDEN_SIZE, batch_first=True, bidirectional=True, num_layers=4, dropout=0.0)\n        self.head = nn.Sequential(\n            nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n            nn.ReLU(),\n            nn.Linear(config.HIDDEN_SIZE * 2, 950),\n        )\n\n    def forward(self, X):\n        # embed\n        bs = X.shape[0]\n        r_emb = self.r_emb(X[:,:,0].long()).view(bs, 80, -1)\n        c_emb = self.c_emb(X[:,:,1].long()).view(bs, 80, -1)\n        rc_dot_emb = self.rc_dot_emb(X[:,:,2].long()).view(bs, 80, -1)\n        rc_sum_emb = self.rc_sum_emb(X[:,:,3].long()).view(bs, 80, -1)\n        \n        seq_x = torch.cat((r_emb, c_emb, rc_dot_emb, rc_sum_emb, X[:, :, 4:]), 2)\n        emb_x = self.seq_emb(seq_x)\n        \n        out, _ = self.lstm(emb_x, None)\n\n        return out\n\n\nclass VentilatorModelRegr(nn.Module):\n    \n    def __init__(self, load_path):\n        super(VentilatorModelRegr, self).__init__()\n        self.cls_model = VentilatorModel()\n        self.cls_model.load_state_dict(torch.load(load_path))\n\n        encoder_layers = nn.TransformerEncoderLayer(d_model=config.HIDDEN_SIZE * 2, nhead=1, dim_feedforward=2048, dropout=0.0, batch_first=True)\n        self.regression = nn.Sequential(\n            #nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n            nn.TransformerEncoder(encoder_layers, num_layers=1),\n            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n            nn.ReLU(),\n            nn.Linear(config.HIDDEN_SIZE * 2, 1),\n        )\n\n    def forward(self, X, y=None):\n        out = self.cls_model(X)\n        regr = self.regression(out)\n\n        if y is None:\n            loss = None\n        else:\n            loss = self.loss_fn(regr.squeeze(2), y)\n            \n        return regr, loss\n    \n    def loss_fn(self, y_pred, y_true):\n        criterion = nn.SmoothL1Loss()\n        loss = criterion(y_pred, y_true)\n        return loss\n\n    def freeze_cls(self):\n        for param in self.cls_model.parameters():\n            param.requires_grad = False","7074600b":"def train_loop(model, optimizer, scheduler, loader):\n    losses, lrs = [], []\n    model.train()\n    optimizer.zero_grad()\n    for d in loader:\n        out, loss = model(d['X'].to(device), d['y'].to(device))\n        \n        losses.append(loss.item())\n        step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n        lrs.append(step_lr)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        scheduler.step()\n\n    return np.array(losses).mean(), np.array(lrs).mean()\n\ndef valid_loop(model, loader, target_dic_inv):\n    losses, predicts = [], []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, loss = model(d['X'].to(device), d['y'].to(device))\n        losses.append(loss.item())\n        predicts.append(out.cpu())\n    return np.array(losses).mean(), torch.vstack(predicts).squeeze(2).numpy().reshape(-1)\n\ndef test_loop(model, loader, target_dic_inv):\n    predicts = []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, _ = model(d['X'].to(device))\n        predicts.append(out.cpu())\n    return torch.vstack(predicts).squeeze(2).numpy().reshape(-1)","0b8b988c":"def add_feature(df):\n    df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['delta'] = df['time_delta'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] \/ df['count']\n    \n    df = df.drop(['count','one'], axis=1)\n    return df\n\ndef add_lag_feature(df):\n    # https:\/\/www.kaggle.com\/kensit\/improvement-base-on-tensor-bidirect-lstm-0-173\n    for lag in range(1, config.USE_LAG+1):\n        df[f'breath_id_lag{lag}']=df['breath_id'].shift(lag).fillna(0)\n        df[f'breath_id_lag{lag}same']=np.select([df[f'breath_id_lag{lag}']==df['breath_id']], [1], 0)\n\n        # u_in \n        df[f'u_in_lag_{lag}'] = df['u_in'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n        df[f'u_in_time{lag}'] = df['u_in'] - df[f'u_in_lag_{lag}']\n        df[f'u_out_lag_{lag}'] = df['u_out'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n\n    # breath_time\n    df['time_step_lag'] = df['time_step'].shift(1).fillna(0) * df[f'breath_id_lag{lag}same']\n    df['breath_time'] = df['time_step'] - df['time_step_lag']\n\n    drop_columns = ['time_step_lag']\n    drop_columns += [f'breath_id_lag{i}' for i in range(1, config.USE_LAG+1)]\n    drop_columns += [f'breath_id_lag{i}same' for i in range(1, config.USE_LAG+1)]\n    df = df.drop(drop_columns, axis=1)\n\n    # fill na by zero\n    df = df.fillna(0)\n    return df\n\nc_dic = {10: 0, 20: 1, 50:2}\nr_dic = {5: 0, 20: 1, 50:2}\nrc_sum_dic = {v: i for i, v in enumerate([15, 25, 30, 40, 55, 60, 70, 100])}\nrc_dot_dic = {v: i for i, v in enumerate([50, 100, 200, 250, 400, 500, 2500, 1000])}    \n\ndef add_category_features(df):\n    df['C_cate'] = df['C'].map(c_dic)\n    df['R_cate'] = df['R'].map(r_dic)\n    df['RC_sum'] = (df['R'] + df['C']).map(rc_sum_dic)\n    df['RC_dot'] = (df['R'] * df['C']).map(rc_dot_dic)\n    return df\n\nnorm_features = config.CONT_FEATURES + config.LAG_FEATURES\ndef norm_scale(train_df, test_df):\n    scaler = RobustScaler()\n    all_u_in = np.vstack([train_df[norm_features].values, test_df[norm_features].values])\n    scaler.fit(all_u_in)\n    train_df[norm_features] = scaler.transform(train_df[norm_features].values)\n    test_df[norm_features] = scaler.transform(test_df[norm_features].values)\n    return train_df, test_df","96f2e002":"def main():\n    \n    train_df = pd.read_csv(f\"{config.INPUT}\/train.csv\")\n    test_df = pd.read_csv(f\"{config.INPUT}\/test.csv\")\n    sub_df = pd.read_csv(f\"{config.INPUT}\/sample_submission.csv\")\n    oof = np.zeros(len(train_df))\n    test_preds_lst = []\n\n    target_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].unique().tolist()))}\n    target_dic_inv = {v: k for k, v in target_dic.items()}\n\n    gkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\n    for fold, (_, valid_idx) in enumerate(gkf):\n        train_df.loc[valid_idx, 'fold'] = fold\n\n    train_df = add_feature(train_df)\n    test_df = add_feature(test_df)\n    train_df = add_lag_feature(train_df)\n    test_df = add_lag_feature(test_df)\n    train_df = add_category_features(train_df)\n    test_df = add_category_features(test_df)\n    train_df, test_df = norm_scale(train_df, test_df)\n\n    test_df['pressure'] = -1\n    test_dset = VentilatorDataset(test_df)\n    test_loader = DataLoader(test_dset, batch_size=config.BS,\n                             pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n    \n    for fold in range(config.N_FOLD):\n        print(f'Fold-{fold}')\n        train_dset = VentilatorDataset(train_df.query(f\"fold!={fold}\"), target_dic)\n        valid_dset = VentilatorDataset(train_df.query(f\"fold=={fold}\"), target_dic)\n\n        set_seed()\n        train_loader = DataLoader(train_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),\n                                  worker_init_fn=lambda x: set_seed())\n        valid_loader = DataLoader(valid_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n\n        load_path = f\"{config.OUTPUT}\/{config.CLS_MODEL}\/ventilator_f{fold}_best_model.bin\"\n        model = VentilatorModelRegr(load_path)\n        model.to(device)\n        model.freeze_cls()\n\n        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n        num_train_steps = int(len(train_loader) * config.N_EPOCHS)\n        num_warmup_steps = int(num_train_steps \/ 10)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n\n        uniqe_exp_name = f\"{config.EXP_NAME}_f{fold}\"\n        wandb.init(project='Ventilator', entity='trtd56', name=uniqe_exp_name, group=config.EXP_NAME)\n        wandb_config = wandb.config\n        wandb_config.fold = fold\n        for k, v in dict(vars(config)).items():\n            if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n                continue\n            wandb_config[k] = v\n        wandb.watch(model)\n        \n        os.makedirs(f'{config.OUTPUT}\/{config.EXP_NAME}', exist_ok=True)\n        model_path = f\"{config.OUTPUT}\/{config.EXP_NAME}\/ventilator_f{fold}_best_model.bin\"\n        \n        valid_best_score = float('inf')\n        valid_best_score_mask = float('inf')\n        for epoch in tqdm(range(config.N_EPOCHS)):\n            train_loss, lrs = train_loop(model, optimizer, scheduler, train_loader)\n            valid_loss, valid_predict = valid_loop(model, valid_loader, target_dic_inv)\n            valid_score = np.abs(valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values).mean()\n\n            mask = (train_df.query(f\"fold=={fold}\")['u_out'] == -1).values  # u_out is normalized [-1, 0]\n            _score = valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values\n            valid_score_mask = np.abs(_score[mask]).mean()\n\n            if valid_score < valid_best_score:\n                valid_best_score = valid_score\n                torch.save(model.state_dict(), model_path)\n                oof[train_df.query(f\"fold=={fold}\").index.values] = valid_predict\n\n            if valid_score_mask < valid_best_score_mask:\n                valid_best_score_mask = valid_score_mask\n\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"valid_loss\": valid_loss,\n                \"valid_score\": valid_score,\n                \"valid_best_score\": valid_best_score,\n                \"valid_score_mask\": valid_score_mask,\n                \"valid_best_score_mask\": valid_best_score_mask,\n                \"learning_rate\": lrs,\n            })\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        model.load_state_dict(torch.load(model_path))\n        test_preds = test_loop(model, test_loader, target_dic_inv)\n        test_preds_lst.append(test_preds)\n        \n        sub_df['pressure'] = test_preds\n        sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/sub_f{fold}.csv\", index=None)\n\n        train_df['oof'] = oof\n        train_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/oof.csv\", index=None)\n        wandb.finish()\n\n        del model, optimizer, scheduler, train_loader, valid_loader, train_dset, valid_dset\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    sub_df['pressure'] = np.stack(test_preds_lst).mean(0)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_mean.csv\", index=None)\n\n    sub_df['pressure'] = np.median(np.stack(test_preds_lst), axis=0)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_median.csv\", index=None)\n    \n    # Post Processing: https:\/\/www.kaggle.com\/snnclsr\/a-dummy-approach-to-improve-your-score-postprocess\n    unique_pressures = train_df[\"pressure\"].unique()\n    sorted_pressures = np.sort(unique_pressures)\n    total_pressures_len = len(sorted_pressures)\n\n    def find_nearest(prediction):\n        insert_idx = np.searchsorted(sorted_pressures, prediction)\n        if insert_idx == total_pressures_len:\n            # If the predicted value is bigger than the highest pressure in the train dataset,\n            # return the max value.\n            return sorted_pressures[-1]\n        elif insert_idx == 0:\n            # Same control but for the lower bound.\n            return sorted_pressures[0]\n        lower_val = sorted_pressures[insert_idx - 1]\n        upper_val = sorted_pressures[insert_idx]\n        return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n    \n    sub_df = pd.read_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_mean.csv\")\n    sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_mean_pp.csv\", index=None)\n    \n    sub_df = pd.read_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_median.csv\")\n    sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_median_pp.csv\", index=None)\n    \n    cv_score = train_df.apply(lambda x: abs(x['oof'] - x['pressure']), axis=1).mean()\n    print(\"CV:\", cv_score)","77519869":"if __name__ == \"__main__\":\n    main()","3f48247b":"# Ventilator Fine-tune Regression","97f97d45":"I train as a classification task. (https:\/\/www.kaggle.com\/takamichitoda\/ventilator-train-classification?scriptVersionId=76597714)\n\nThe classification converges earlier than regression.\n\nIn my experience, to got  OOF score=0.17xx by using 4 layer bi-LSTM model\n- regression needs 150 epoch\n- classification needs 50 epoch\n\nHowever, the classification case easily makes overfit.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/trtd56\/RFCX\/main\/tmp\/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202021-10-14%2015.40.55.png\" width=\"400\">\n\nSo in this code, I fine-tuning the regression task by using [classification model weight](https:\/\/www.kaggle.com\/takamichitoda\/ventilator-classification-model).\n\nClassification model result is CV=0.1708 \/ LB=0.155, this model result is CV=0.1765 \/ LB=0.165.\n\nIf I use custom header based on 1d-CNN (the implementation like [here](https:\/\/www.kaggle.com\/takamichitoda\/ventilator-1dcnn-lstm)), I got CV=0.1622 \/ LB=0.150.\n\n## UPDATE\n\n|Date|Version|Detail|CV|LB|\n|--|--|--|--|--|\n|2021.10.27|4|Transformer Header|0.1665|0.1545|"}}