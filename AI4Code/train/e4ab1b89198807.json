{"cell_type":{"788abbe7":"code","310785b6":"code","874170f8":"code","9b6adddd":"code","673aadf8":"code","976dbb33":"code","90e50b21":"code","c493376a":"code","31769a6a":"code","49723645":"code","0751c35c":"code","e27f2c83":"code","1533557f":"code","0a273a39":"code","eff213b3":"code","7d5e2a53":"code","3cd962bb":"code","828199bd":"code","9d27a1a1":"code","23aa9872":"code","c3dbebb1":"code","40f5d3a6":"code","b81cdd06":"code","444f2f03":"code","6803d47e":"code","6266f348":"code","b3464c3e":"code","9061a6a5":"code","2d6f4e27":"code","3eb424ae":"code","8c6ac087":"markdown","5110884f":"markdown","b55fe50c":"markdown"},"source":{"788abbe7":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","310785b6":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntrain.head()","874170f8":"#train_original = train.copy(deep=True)\n#train = train_original.sample(100)","9b6adddd":"train[\"sentiment\"].value_counts()","673aadf8":"# Get all sentiments\nall_sentiments = train[\"sentiment\"].unique()","976dbb33":"# Get training data\ntrain_dict = dict()\ntrain_dict_index = dict()\nfor sentiment in all_sentiments:\n    train_dict[sentiment] = train[train[\"sentiment\"]==sentiment]\n    train_dict_index[sentiment] = list(train_dict[sentiment].index)\n    train_dict[sentiment] = train_dict[sentiment].reset_index(drop=True)","90e50b21":"def get_training_data(train):\n    \n    ct = train.shape[0]\n    input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n    attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n    token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n    start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n    end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\n    for k in range(train.shape[0]):\n\n        # FIND OVERLAP\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        text2 = \" \".join(train.loc[k,'selected_text'].split())\n        idx = text1.find(text2)\n        chars = np.zeros((len(text1)))\n        chars[idx:idx+len(text2)]=1\n        if text1[idx-1]==' ': chars[idx-1] = 1 \n        enc = tokenizer.encode(text1) \n\n        # ID_OFFSETS\n        offsets = []; idx=0\n        for t in enc.ids:\n            w = tokenizer.decode([t])\n            offsets.append((idx,idx+len(w)))\n            idx += len(w)\n\n        # START END TOKENS\n        toks = []\n        for i,(a,b) in enumerate(offsets):\n            sm = np.sum(chars[a:b])\n            if sm>0: toks.append(i) \n\n        s_tok = sentiment_id[train.loc[k,'sentiment']]\n        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n        attention_mask[k,:len(enc.ids)+5] = 1\n        if len(toks)>0:\n            start_tokens[k,toks[0]+1] = 1\n            end_tokens[k,toks[-1]+1] = 1\n    \n    return input_ids, attention_mask, token_type_ids, start_tokens, end_tokens","c493376a":"# Get preprocessed training data\ntrain_data_dict = dict()\nfor sentiment in all_sentiments:\n    train_data_dict[sentiment] = get_training_data(train_dict[sentiment])","31769a6a":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')","49723645":"# Get test data\ntest_dict = dict()\ntest_dict_index = dict()\nfor sentiment in all_sentiments:\n    test_dict[sentiment] = test[test[\"sentiment\"]==sentiment]\n    test_dict_index[sentiment] = list(test_dict[sentiment].index)\n    test_dict[sentiment] = test_dict[sentiment].reset_index(drop=True)","0751c35c":"def get_test_data(test):\n    ct = test.shape[0]\n    input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n    attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n    token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\n    for k in range(test.shape[0]):\n\n        # INPUT_IDS\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)                \n        s_tok = sentiment_id[test.loc[k,'sentiment']]\n        input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n        attention_mask_t[k,:len(enc.ids)+5] = 1\n    return input_ids_t, attention_mask_t, token_type_ids_t","e27f2c83":"# Get preprocessed test data\ntest_data_dict = dict()\nfor sentiment in all_sentiments:\n    test_data_dict[sentiment] = get_test_data(test_dict[sentiment])","1533557f":"# Build model\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","0a273a39":"# Define Jaccard similarity metric\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","eff213b3":"def find_a_b(start_array, end_array):\n    #print(start_array)\n    #print(end_array)\n    a = np.argmax(start_array)\n    b = np.argmax(end_array)\n    if a<b:\n        return a,b\n    else:\n        if a<len(start_array)-1:\n            end_array_copy = np.array(end_array)\n            end_array_copy[b]=0.0\n            return find_a_b(start_array, end_array_copy)\n        else: \n            if b<len(start_array)-1:\n                start_array_copy = np.array(start_array)\n                start_array_copy[a]=0.0\n                return find_a_b(start_array_copy, end_array)\n            else:\n                return 0, len(end_array)-1","7d5e2a53":"def train_model_for_sentiment(train_data_dict, test_data_dict, sentiment, train_dict):\n    \n    input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = train_data_dict[sentiment]\n    input_ids_t, attention_mask_t, token_type_ids_t = test_data_dict[sentiment]\n    train = train_dict[sentiment]\n    \n    jac = []; VER='v1'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n    oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n    oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n    preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n    preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\n    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n    print('#'*25)\n    print('### SENTIMENT %s'%(sentiment))\n    print('#'*25)\n    for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n        print('#'*25)\n        print('### FOLD %i'%(fold+1))\n        print('#'*25)\n\n        K.clear_session()\n        model = build_model()\n\n        sv = tf.keras.callbacks.ModelCheckpoint(\n            '%s-roberta-%i-%s.h5'%(VER,fold,sentiment), monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=True, mode='auto', save_freq='epoch')\n\n        model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n            epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n            validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n            [start_tokens[idxV,], end_tokens[idxV,]]))\n\n        print('Loading model...')\n        model.load_weights('%s-roberta-%i-%s.h5'%(VER,fold,sentiment))\n\n        print('Predicting OOF...')\n        oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n\n        print('Predicting Test...')\n        preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n        preds_start += preds[0]\/skf.n_splits\n        preds_end += preds[1]\/skf.n_splits\n\n        # DISPLAY FOLD JACCARD\n        all = []\n        for k in idxV:\n            #a = np.argmax(oof_start[k,])\n            #b = np.argmax(oof_end[k,])\n            #if a>b: \n                # st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n            #else:\n            a,b = find_a_b(oof_start[k,], oof_end[k,])\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n            all.append(jaccard(st,train.loc[k,'selected_text']))\n        jac.append(np.mean(all))\n        print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n        print()\n    print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))    \n    return preds_start, preds_end","3cd962bb":"def get_string_predictions(test_data_dict, sentiment, predictions, test_dict):\n    preds_start, preds_end = predictions[sentiment]\n    input_ids_t, attention_mask_t, token_type_ids_t = test_data_dict[sentiment]\n    test = test_dict[sentiment]\n    \n    all = []\n    for k in range(input_ids_t.shape[0]):\n        a = np.argmax(preds_start[k,])\n        b = np.argmax(preds_end[k,])\n        if a>b: \n            st = test.loc[k,'text']\n        else:\n            text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(st)\n    return all","828199bd":"def get_string_predictions_neutral(test_dict, sentiment):\n    test = test_dict[sentiment]\n    # Do nothing for neutral sentiment? Some sentences are slightly different, may be better to train a model\n    return list(test[\"text\"])","9d27a1a1":"# Train and transform output into  predictions\npredictions = dict()\nstring_predictions = dict()\nfor sentiment in all_sentiments:\n    if not sentiment == \"neutral\":\n        predictions[sentiment] = train_model_for_sentiment(train_data_dict, test_data_dict, sentiment, train_dict)\n        string_predictions[sentiment] = get_string_predictions(test_data_dict, sentiment, predictions, test_dict)\n    else:\n        string_predictions[sentiment] = get_string_predictions_neutral(test_dict, \"neutral\")","23aa9872":"# Reordering\nconcat_string_predictions = []\nconcat_index = []\nfor sentiment in all_sentiments:\n    concat_string_predictions += string_predictions[sentiment]\n    concat_index += test_dict_index[sentiment]\nsorted_concat_string_predictions = sorted(list(zip(concat_index, concat_string_predictions)), key=lambda x:x[0])\nsorted_concat_string_predictions_list = list(zip(*sorted_concat_string_predictions))[1]","c3dbebb1":"test['selected_text'] = sorted_concat_string_predictions_list\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","40f5d3a6":"def get_predictions_on_oof_for_sentiment(train_data_dict, test_data_dict, sentiment, train_dict):\n    \n    input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = train_data_dict[sentiment]\n    input_ids_t, attention_mask_t, token_type_ids_t = test_data_dict[sentiment]\n    train = train_dict[sentiment]\n    \n    # PATH to saved weights\n    #PATH_SAVED_WEIGHTS = '..\/input\/twitter-comp-roberta-version-9\/'\n    PATH_SAVED_WEIGHTS = ''\n    \n    # Initialise similarity and prediction oof\n    train[\"prediction_oof\"] = \"\"\n    train[\"jaccard_sim\"] = np.nan\n    \n    jac = []; VER='v1'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n    oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n    oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n    preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n    preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\n    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n    print('#'*25)\n    print('### SENTIMENT %s'%(sentiment))\n    print('#'*25)\n    for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n        print('#'*25)\n        print('### FOLD %i'%(fold+1))\n        print('#'*25)\n\n        K.clear_session()\n        model = build_model()\n\n        #sv = tf.keras.callbacks.ModelCheckpoint(\n        #    '%s-roberta-%i-%s.h5'%(VER,fold,sentiment), monitor='val_loss', verbose=1, save_best_only=True,\n        #    save_weights_only=True, mode='auto', save_freq='epoch')\n        \n        # NO NEED TO TRAIN THE MODEL, JUS LOAD THE WEIGHTS\n        #model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        #    epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n        #    validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        #    [start_tokens[idxV,], end_tokens[idxV,]]))\n\n        print('Loading model...')\n        model.load_weights(PATH_SAVED_WEIGHTS+'%s-roberta-%i-%s.h5'%(VER,fold,sentiment))\n\n        print('Predicting OOF...')\n        oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n\n        #print('Predicting Test...')\n        #preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n        #preds_start += preds[0]\/skf.n_splits\n        #preds_end += preds[1]\/skf.n_splits\n\n        # DISPLAY FOLD JACCARD\n        all = []\n        for k in idxV:\n            a = np.argmax(oof_start[k,])\n            b = np.argmax(oof_end[k,])\n            if a>b: \n                st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n            else:\n            #a,b = find_a_b(oof_start[k,], oof_end[k,])\n                text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n                enc = tokenizer.encode(text1)\n                st = tokenizer.decode(enc.ids[a-1:b])\n            jaccard_sim = jaccard(st,train.loc[k,'selected_text'])\n            train.at[k, \"jaccard_sim\"] = jaccard_sim\n            train.at[k, \"prediction_oof\"] = st\n            all.append(jaccard_sim)\n            \n        jac.append(np.mean(all))\n        print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n        print()\n        \n    print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))    \n    return train","b81cdd06":"oof_predictions = get_predictions_on_oof_for_sentiment(train_data_dict, test_data_dict, \"positive\", train_dict)","444f2f03":"pd.set_option('display.max_colwidth', -1)\noof_predictions.sample(20)","6803d47e":"oof_predictions[\"len_selected_text\"] = oof_predictions[\"selected_text\"].map(lambda x:str(x).strip()).map(len)\noof_predictions[\"len_prediction_oof\"] = oof_predictions[\"prediction_oof\"].map(lambda x:str(x).strip()).map(len)","6266f348":"oof_predictions[\"num_words_text\"] = oof_predictions[\"text\"].map(lambda x:len(x.split()))\noof_predictions[\"num_words_selected_text\"] = oof_predictions[\"selected_text\"].map(lambda x:len(x.split()))\noof_predictions[\"num_words_prediction_oof\"] = oof_predictions[\"prediction_oof\"].map(lambda x:len(x.split()))","b3464c3e":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20, 5))\n_ = plt.hist(oof_predictions[\"len_selected_text\"], alpha=0.5, bins=np.arange(0, 140, 1), label='len_selected_text')\n_ = plt.hist(oof_predictions[\"len_prediction_oof\"], alpha=0.5, bins=np.arange(0, 140, 1), label='len_prediction_oof')\nplt.legend()","9061a6a5":"#oof_predictions[oof_predictions[\"len_selected_text\"]==3][[\"text\", \"selected_text\", \"len_selected_text\",\"prediction_oof\", \"len_prediction_oof\",\"sentiment\", \"jaccard_sim\"]].sample(10)","2d6f4e27":"_ = plt.hist(oof_predictions[\"num_words_selected_text\"], alpha=0.5, bins=np.arange(0, 25, 1), label='num_words_selected_text')\n_ = plt.hist(oof_predictions[\"num_words_prediction_oof\"], alpha=0.5, bins=np.arange(0, 25, 1), label='num_words_prediction_oof')\nplt.legend()","3eb424ae":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 5))\n\noof_predictions[\"num_words_text_rand\"] = oof_predictions[\"text\"].map(lambda x:len(x.split()) + np.random.normal(0, 0.3, 1)[0])\noof_predictions[\"num_words_prediction_oof_rand\"] = oof_predictions[\"prediction_oof\"].map(lambda x:len(x.split())+ np.random.normal(0, 0.3, 1)[0])\nax1.scatter(oof_predictions[\"num_words_text_rand\"], oof_predictions[\"num_words_prediction_oof_rand\"], s=0.01)\n\noof_predictions[\"num_words_text_rand\"] = oof_predictions[\"text\"].map(lambda x:len(x.split()) + np.random.normal(0, 0.3, 1)[0])\noof_predictions[\"num_words_selected_text_rand\"] = oof_predictions[\"selected_text\"].map(lambda x:len(x.split())+ np.random.normal(0, 0.3, 1)[0])\nax2.scatter(oof_predictions[\"num_words_text_rand\"], oof_predictions[\"num_words_selected_text_rand\"], s=0.01)","8c6ac087":"# TensorFlow roBERTa Starter - LB 0.705\nThis notebook is a TensorFlow template for solving Kaggle's Tweet Sentiment Extraction competition as a question and answer roBERTa formulation. In this notebook, we show how to tokenize the data, create question answer targets, and how to build a custom question answer head for roBERTa in TensorFlow. Note that HuggingFace transformers don't have a `TFRobertaForQuestionAnswering` so we must make our own from `TFRobertaModel`. This notebook can achieve LB 0.715 with some modifications. Have fun experimenting!\n\nYou can also run this code offline and it will save the best model weights during each of the 5 folds of training. Upload those weights to a private Kaggle dataset and attach to this notebook. Then you can run this notebook with the line `model.fit()` commented out, and this notebook will instead load your offline models. It will use your offline models to predict oof and predict test. Hence this notebook can easily be converted to an inference notebook. An inference notebook is advantageous because it will only take 10 minutes to commit and submit instead of 2 hours. Better to train 2 hours offline separately.","5110884f":"# Load Libraries, Data, Tokenizer\nWe will use HuggingFace transformers [here][1]\n\n[1]: https:\/\/huggingface.co\/transformers\/","b55fe50c":"### GET PREDICTIONS ON OOF"}}