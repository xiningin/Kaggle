{"cell_type":{"dbd2c513":"code","2bb0f027":"code","725dc57d":"code","2a3c5046":"code","1bf7b033":"code","5dfcae15":"code","6a6d4112":"code","1ba399bc":"code","e21894ef":"code","4c3e0dbe":"code","60dd38c6":"code","2f7f9980":"code","321f289a":"code","6d475ddf":"code","26baba8a":"code","c1c2cb2e":"code","f56655bd":"code","2c705f43":"code","101c1dfa":"code","012da507":"code","cda20df3":"code","32a87665":"code","550bc318":"code","5e31e2af":"code","c3fec1fe":"code","e314da4c":"code","8450ebcb":"code","f21c2220":"code","3e64c7fe":"code","75001be3":"code","0241c691":"code","a521e50d":"code","fdfcfe33":"code","8aff95a5":"code","c3b513e3":"code","2d1ba76d":"code","addf5171":"code","622356ab":"code","c1c8e54a":"code","fc03b4be":"code","6386c837":"code","c0a686a3":"code","301fa23c":"code","88348879":"code","dae6dccb":"code","2a5ed287":"code","d5d3ea0e":"code","45f8e31a":"code","43db92a3":"code","877a92c0":"code","d20ec878":"code","dfc2ed18":"code","fb3552b0":"markdown","c81790e1":"markdown","8e467897":"markdown","a3febd90":"markdown","73708450":"markdown","7eeff1e1":"markdown","f71b1aa9":"markdown","12263572":"markdown","f66614c2":"markdown","8f1ae12e":"markdown","145e0117":"markdown","f6d11901":"markdown","9d38be3e":"markdown","dfdbfdd1":"markdown","08f0a33c":"markdown","06ff4e87":"markdown","89889646":"markdown","36048c98":"markdown","1dfe8418":"markdown","c3504c0f":"markdown","88f252ec":"markdown","d7825380":"markdown","720dc071":"markdown","7b1eb3a9":"markdown","56dcf1d2":"markdown","8f7a283f":"markdown","7f328b41":"markdown","66b9b148":"markdown","198d09d0":"markdown","56817da6":"markdown","6ee1dd4a":"markdown","d1c4ad02":"markdown","4143c40b":"markdown","79d07512":"markdown","944b8cc6":"markdown","c9965969":"markdown","3bb10ffa":"markdown","c9665fae":"markdown","cd6c7f52":"markdown","77441f78":"markdown","1554105d":"markdown","0a7c6518":"markdown","2050f4df":"markdown","8472e9c8":"markdown","1876b8ac":"markdown","ee0c8101":"markdown","53b1e1e2":"markdown","9f6fdf87":"markdown","606c7723":"markdown","f4ecfd47":"markdown","36025d57":"markdown","c55a58a6":"markdown","8754c374":"markdown","671a2f43":"markdown","4bd66810":"markdown","e3c6f901":"markdown","2f4471f9":"markdown"},"source":{"dbd2c513":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bb0f027":"#Import Libraries\n\n# Libraries Data Analyst and magnament numeric\nimport numpy as np\nimport pandas as pd\n\n#Visualization and plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff # Displot for view the distribution data\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\n\npd.options.plotting.backend = 'plotly'\nsns.set_style(\"whitegrid\")","725dc57d":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(\"#\" * 20 + \" TRAIN \" + \"#\" * 20)\nprint(train.info())\nprint(\"#\" * 20 + \" TEST \" + \"#\" * 20)\nprint(test.info())\ntrain.describe(include = \"all\")","2a3c5046":"# View Train Table in a sample:\ntrain.sample(10)","1bf7b033":"# Count Values is Null:\nprint(pd.isnull(train).sum())","5dfcae15":"print('Percentage of missing \"Age\" records is %.2f%%' %((train['Age'].isnull().sum()\/train.shape[0])*100))","6a6d4112":"# Visualization of distribution data\nhist_data = [train['Age'].drop(train[train['Age'].isnull()].index, axis = 0)]\nfig = ff.create_distplot(hist_data, group_labels = [\"Age\"],\n                        bin_size = 1, curve_type = \"kde\")\n\nfig.update(layout_title_text = \"Age Normal Distribution\")\nfig.show()","1ba399bc":"print(\"The median value is {} years.\".format(train['Age'].median(skipna = True)))","e21894ef":"print('Percentage of missing \"Embarked\" records is %.2f%%' %((train['Embarked'].isnull().sum()\/train.shape[0])*100))","4c3e0dbe":"locations = list(train['Embarked'].value_counts().reset_index()['index'])\nprint(locations)","60dd38c6":"# View in bar plot the number embarked for each location\nlocations = list(train['Embarked'].value_counts().reset_index()['index'])\nvalues = train['Embarked'].value_counts()\nfig = px.bar(y = values, x = locations, color = locations,\n            labels = {'y': 'Count', 'x': 'Locations'})\nfig.update(layout_title_text = 'Number of Embarked for Location')\nfig.show()","2f7f9980":"# Apply method fill from pandas library\ntrain_data = train.copy()\ntrain_data['Age'].fillna(train['Age'].median(skipna = True), inplace = True)\ntrain_data['Embarked'].fillna(train['Embarked'].value_counts().idxmax(), inplace = True)\n\n# Drop \"Cabin\" field\ntrain_data = train_data.drop('Cabin', axis = 1)\n\nprint(train_data.info())\nprint(\"-\" * 40)\nprint(pd.isnull(train_data).sum())","321f289a":"# Visualization of distribution data\nhist_data = [train['Age'].drop(train[train['Age'].isnull()].index, axis = 0), \n             train_data['Age']]\n\nfig = ff.create_distplot(hist_data, group_labels = [\"Age\", \"Age after adjustment\"],\n                        bin_size = 1, curve_type = \"kde\")\n\nfig.update(layout_title_text = \"Age Normal Distribution\")\nfig.update_layout(yaxis = dict(range = ([0, 0.06])))\nfig.show()","6d475ddf":"# Created the new field - \"Travel Alone\"\ntrain_data['Travel Alone'] = np.where((train_data['SibSp'] + train_data['Parch']) > 0, 0, 1)\ntrain_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\n\ntrain_data.sample(5)","26baba8a":"# new variable, the minor age.\ntrain_data['IsMinor'] = np.where(train_data['Age'] < 16, 1, 0)\ntrain_data['IsMinor'].value_counts()","c1c2cb2e":"# Generate dataframe finale, we get a new dataset with fields more descriptive in Sex, Embarked and Pclass fields\ntraining_final = pd.get_dummies(train_data, columns = ['Sex', 'Embarked', 'Pclass'])\ntraining_final.drop(['Name','Ticket', 'PassengerId'], axis = 1, inplace = True)\n\ntraining_final.sample(5)","f56655bd":"# Count Missing Values\nprint(pd.isnull(test).sum())\nprint(\"-\" * 80)\nprint(test.describe())","2c705f43":"# Fill values NaN and Null and remove Cabin\ntest_data = test.copy()\ntest_data['Age'].fillna(test_data['Age'].median(skipna = True), inplace = True)\ntest_data['Fare'].fillna(test_data['Age'].median(skipna = True), inplace = True)\ntest_data = test_data.drop('Cabin', axis =1)\n\ntest_data.sample(4)","101c1dfa":"# New Varible and remove old variable:\ntest_data['Travel Alone'] = np.where((test_data['SibSp'] + test_data['Parch']) > 0, 0, 1)\ntest_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\ntest_data.head()","012da507":"# new variable, the minor age.\ntest_data['IsMinor'] = np.where(test_data['Age'] < 16, 1, 0)\ntest_data['IsMinor'].value_counts()","cda20df3":"#With a new dataframe Construct variable categorical and transpose in dataseet\ntesting_final = pd.get_dummies(test_data, columns = ['Pclass', 'Sex', 'Embarked'])\\\n                  .drop(['Name', 'Ticket'], axis = 1)\n\ntesting_final.sample(4)","32a87665":"train_data['Fare'].describe()","550bc318":"# We build vector with range interquartile\nvector = []\nfor k in range(0, train_data['Fare'].describe().iloc[3:].shape[0]):\n    vector.append(train_data['Fare'].describe().iloc[3:][k])\nprint(vector)","5e31e2af":"# Cut data in fuction vector\nfare_cut = pd.cut(train_data['Fare'], vector)\n\nfare_pivot = train_data.pivot_table('Fare', index = [fare_cut],\n                                    columns = ['Survived', 'Travel Alone'], \n                                    aggfunc = 'count')\n\n# sum amount survived in the passengers that travel alone\nfare_pivot['Travel Alone Survived'] = fare_pivot.loc(axis = 1)[(1,1)]\nfare_pivot['Not Travel Alone Survived'] = fare_pivot.loc(axis = 1)[(1,0)]\n\nfare_pivot.head()","c3fec1fe":"# Plot distribution graph both in travel alone as travel not alone\nfig, axes = plt.subplots(1, 2, figsize = (10, 8))\nfig.suptitle(\"Fares Distribution With Passengers Travel Alone or Not\")\n\nsns.distplot(train_data['Fare'].drop(train_data[train_data['Travel Alone'] == 1].index, axis = 0),\n             ax = axes[0])\n\naxes[0].set_title(\"Travel Alone\")\n\nsns.distplot(train_data['Fare'].drop(train_data[train_data['Travel Alone'] == 0].index, axis = 0), \n             ax = axes[1])\n\naxes[1].set_title(\"Travel not Alone\")\n","e314da4c":"# Plot in three subplots with each passenger class.\nfig, axes = plt.subplots(1,2, figsize = (20,6), sharey = True)\nfig.suptitle(\"Fares vs Survived For Each Range Interquartile Fare in Travel Alone or Not\")\n\nsns.barplot(ax = axes[0], data = fare_pivot, x = fare_pivot.index.values, y = 'Travel Alone Survived')\\\n            .set_title(\"Travel Alone\")\nsns.barplot(ax = axes[1], data = fare_pivot, x = fare_pivot.index.values, y = 'Not Travel Alone Survived')\\\n            .set_title(\"Not Travel Alone\")","8450ebcb":"sns.barplot(data = train_data, x = 'Pclass', y = 'Survived', hue = 'Travel Alone').set_title(\"Passenger Class vs Survived\")","f21c2220":"sns.distplot(train_data['Age']).set_title(\"Age Normal Distribution\")","3e64c7fe":"# Vector Range\nvector = [0,10,18,30,45,80]\n\n# Index build\nage_cut = pd.cut(train_data['Age'], vector)\n\n# Pivot Table\nage_pivot = train_data.pivot_table('Age', index = [age_cut],\n                          columns = ['Survived', 'Pclass'],\n                          aggfunc = 'count')\nage_pivot.fillna(0, inplace = True)\n\n# We build Rate Survived field for each class passenger\nfor k in range(1, 4):\n    age_pivot[(\"Rate Survived\", k)] = age_pivot[(1,k)] \/ (age_pivot[(0,k)] + age_pivot[(1,k)])\n    age_pivot[(\"Rate Survived\", k)] = age_pivot[(\"Rate Survived\", k)].apply(lambda x: round(x, 3))\n    \nage_pivot","75001be3":"fig, ax = plt.subplots(1, 3, figsize = (20, 6), sharey = True)\nfig.suptitle(\"Age (for range) vs Survived per Passenger Class\")\n\nfor k in range(0, ax.shape[0]):\n    sns.barplot(data = age_pivot, x = age_pivot.index.values, y = (\"Rate Survived\", (k + 1)), ax = ax[k]).set_title(\"Class-\" + str(k + 1))","0241c691":"# Create an pivote table counting the Survived\nembarked_pivot = pd.pivot_table(data = train_data, values = 'Survived', \n                                index =['Embarked', 'Sex'], columns = 'Pclass', \n                                aggfunc = np.sum).fillna(0)\nembarked_pivot","a521e50d":"# Compared in barplot with 3 variables\nfig, ax = plt.subplots(1,3, figsize = (18,6), sharey = False)\n\nsize = list(train_data[\"Embarked\"].value_counts())\nlabels = ['Southampton', 'Cherbourg', 'Queenstown']\nexplode = (0.15,0,0)\n\n# With Pclass\nsns.pointplot(data = train_data, x = 'Embarked', y = 'Survived', hue = 'Pclass', ax = ax[0]).set_title(\"With Class Passenger\")\n# With Sex\nsns.barplot(data = train_data, x = 'Embarked', y = 'Survived', hue = 'Sex', ax = ax[1]).set_title(\"With Sex Passenger\")\n# Distribution Percentage\nax[2].pie(size, explode = explode, labels = labels, autopct = '%1.1f%%', shadow = True, startangle = 90)\nax[2].axis('equal')","fdfcfe33":"# Table Correlations with method of pandas.\ncorrelations = train_data.drop([\"PassengerId\", \"Survived\"], axis = 1).corr(method = 'spearman').apply(lambda x: x.round(2))\ncorrelations","8aff95a5":"# With seaborn we can build this map.\nfig = px.imshow(correlations)\nfig.show()","c3b513e3":"# MC as confusion matrix that is generate for Scikit-Learn\ndef general_index(MC, names = None):\n    global_press = np.sum(MC.diagonal()) \/ np.sum(MC)\n    global_error = 1 - global_press\n    cat_press = pd.DataFrame(MC.diagonal(), np.sum(MC, axis = 1)).T\n    if names != None:\n        cat_press.columns = names\n    return {'Confusi\u00f3n Matrix': MC,\n            'Global Precision': global_press,\n            'Global Error': global_error,\n            'Pressure by Categorical': cat_press}","2d1ba76d":"from sklearn.feature_selection import RFE\nfrom sklearn import linear_model\nfrom sklearn.metrics import confusion_matrix\n\ncols = training_final.drop(['Survived'], axis = 1).columns.to_list()","addf5171":"# Selection Features\ncols = ['Age', 'Fare', 'Travel Alone', 'IsMinor', 'Sex_male', \n       'Embarked_C', 'Embarked_Q', 'Pclass_1', 'Pclass_2']\n\nX = training_final[cols]\ny = training_final['Survived']\n\n# Build model Logistic Regression for compute feature importance\nlr = linear_model.LogisticRegression(solver = 'liblinear')\n# We use RFE model and select 8 better atributes\nrfe = RFE(lr, n_features_to_select = 8, step = 1)\nrfe.fit(X, y)\nprint('Selected Features of RFE Model: %s' % list(X.columns[rfe.support_]))","622356ab":"from sklearn.feature_selection import RFECV\n\nrfecv = RFECV(estimator = linear_model.LogisticRegression(solver = 'liblinear'), \n             cv = 10, step = 1, scoring = 'accuracy')\nrfecv.fit(X, y)\n\nprint('Optimal number of features: %s' % str(rfecv.n_features_))\nprint('Selected features: %s' % X.columns[rfecv.support_].to_list())\n\n# Plot number features vs cross-validation scores\nplt.figure(figsize = (12, 5))\nsns.lineplot(x = range(1 , len(rfecv.grid_scores_) + 1), y = rfecv.grid_scores_)\nplt.xlabel('N\u00ba Features')\nplt.ylabel('Cross-Validantion Scores')\nplt.title('N\u00ba Features vs Cross Validantion')\nplt.show()","c1c8e54a":"# Import Library for model\n\n# Model Regresion instance\nfrom sklearn import linear_model, model_selection","fc03b4be":"gender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ny_submission = np.array(gender_submission['Survived'])","6386c837":"# Convert data in a vector with numpy.\ncols = X.columns[rfecv.support_].to_list()\n\nX = np.array(training_final[cols])\ny = np.array(training_final['Survived'])\n\n# We Create the data trainning and test of model original training with test size of 20%: \nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)\n\n# Now, X array with values testing_finale\nX_testing_final = np.array(testing_final[cols])\n\n# We create an instance contained in linear_model class called Logistic Regresion\nlr_model = linear_model.LogisticRegression(solver = 'liblinear',  max_iter = 100)\nlr_model.fit(X_train,y_train)\n\ny_predict = lr_model.predict(X_test)\nMC = confusion_matrix(y_test, y_predict)\n\nindex = general_index(MC)\nfor k in index:\n    print(\"\\n%s:\\n%s\"%(k, index[k]))","c0a686a3":"# Evaluation finale in data testing predict:\ny_finale = lr_model.predict(X_testing_final)\n\n# General index\nMC = confusion_matrix(y_submission, y_finale)\n\nindex = general_index(MC)\nfor k in index:\n    print(\"\\n%s:\\n%s\"%(k, index[k]))","301fa23c":"# Import Library\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, train_test_split","88348879":"X_train, X_test, y_train, y_test = train_test_split(training_final[cols], training_final['Survived'], test_size = 0.25)","dae6dccb":"# Grid params\nparameters = [\n    {\n        'kernel': ['rbf'],\n        'gamma': [1e-3, 1e-2, 0.1, 0.2,],\n        'C': [1, 10, 20, 50, 100]\n    },\n    {\n        'kernel': ['linear'],\n        'C': [1, 10, 20, 50 ,100]\n    }\n]","2a5ed287":"# With GridSearchCV and five cross-validation\nclf = GridSearchCV(SVC(decision_function_shape = 'ovr'), param_grid = parameters, cv = 10)\nclf.fit(X_train, y_train)","d5d3ea0e":"print(clf.best_params_)","45f8e31a":"means = clf.cv_results_['mean_test_score']\nstd = clf.cv_results_['std_test_score']\nparams = clf.cv_results_['params']\n\nfor m, s, p in zip(means, std, params):\n    print(\"Mean: {:.3f} (+\/- {:.3f}) for --> {})\".format(m, s, p))","43db92a3":"y_pred =  clf.predict(X_test)\nprint(classification_report(y_test, y_pred))","877a92c0":"y = clf.predict(X_testing_final)\n\n# General index\nMC = confusion_matrix(y_submission, y)\n\nindex = general_index(MC)\nfor k in index:\n    print(\"\\n%s:\\n%s\"%(k, index[k]))","d20ec878":"testing_final['Survived'] = y_finale\nsubmission = testing_final[['PassengerId', 'Survived']]\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index = False)","dfc2ed18":"submission.tail()","fb3552b0":"# Titanic Survival Prediction With ML and Data Exploratory Analytics (Beginner)","c81790e1":"### 3.1.2 Passengers Class vs Survived\n\nWe confirm the relations between Passengers Class and Survived its what analysis previous was get show.","8e467897":"Drop Embarked_S and Pclass_3 because if the others variables have a values of zero, then this feature have value of one.","a3febd90":"The Feature '*Survived*' is target in the predictions method.","73708450":"The scored obtened is little minor that the results of Logistic Regressor. The best model is Logistic Regression","7eeff1e1":"## **2.2 We fill values in conditions previously observed**\n\nWe make the changes that were observed.\n\n* If \"Age\" value is NaN or Null, we fill this space with the median value (28 years)\n* If \"Embarked\" value is Nan or Null, we fill this space with the value max from the locations\n* The \"Cabin\" field isn't necessary for our model,  in addition, it has missing values \u200b\u200ba lot. We'll ignore the \"Cabin\" field","f71b1aa9":"We can see as the first class had better-survived opportunities in all vector range age. However, the second class the priority was to be focus in those passenger with minor ages. We know that this politics was present in the moment disaster so, in the third class had the same tendency in spite of having a minor rate.","12263572":"In addition at this, a new feature that will permisive identify which passengeris is minor age. This is an variable categorical.","f66614c2":"### 3.1.4 Embarked vs Survived\n\nThe passengers were embarked in three place different. \n* Cherbourg (C), France\n* Queenstown (Q), Republic of Irland\n* Southampton (S), United Kingdom\n\nThis analytics will be compared with other variables for search correlations that to we can interest.","8f1ae12e":"To avoid that the data is skewed (for right), we use the median from the 'age' field and fill with this value.","145e0117":"We obtain an good results, with the Global Precision of 78% in the model with data train. \n\nNow, we evaluate the model with data of test and to common with submission data. ","f6d11901":"# 2. Data Cleaning - Preprocessing\n\nWith end for having the data free of values atypic that could distortion the values that return from the algorithms for predictions categoricals, was initiated a preprocessing data that the cleaning it.\n\n","9d38be3e":"For the training data, the major amount passengers were embarked in **Southampton, United Kingdom** with the **72.5%**. We validate that the women in those years to had better possibilities for survived. ","dfdbfdd1":"Charge gender_submissi\u00f3n for measure the success in prediction finale.","08f0a33c":"### **2.2.1 Field \"Age\" Distribution and Its Missing Values**","06ff4e87":"A 0.22% of values are missing in training data, this value is very low. We see a bar graph to determine the distribution from this categorical variable which describes the number for embarked in Southampton, Cherbourg and Queenstown. ","89889646":"## 3.1 Analytics in Attribute Vs Survived in Training Data\n\nFor we understant the present proportions in variables with respect a the variable \"Survived\". We realize graph for each a variable.","36048c98":"It is a curiosity that RFE model not selected the **'Fare'** feature as a variable of importance.\n\nIt is important that we build RFE model with cross-validation, with it we can know how many variables are necessary for our model to obtain a better score. ","1dfe8418":"### 3.1.1 Fare vs Survived\n\nWe build a vector between interquartile range for next build graph normal.","c3504c0f":"At this moment, the fields \"Cabin\" isn't necessary for our model, in addition, the quantity of missing values is very large. However, we are focus on \"Age\" and \"Embarked\" Features that can is a field critical to measure our predictions model correctly.\n\nWe go to measure the fields that we interest.","88f252ec":"# 3. Analytics Exploratory of Data (EDA).\n\nAlthough some sections from process EDA was developed previously. Now, it is necessary to make an analytics exploratory of data with the some tools from EDA method for to can know the true dimension and correlation of data, its correspondence between variable and how are distribution. \n\n* Analytics in Attribute Vs Survived in training data.\n* Correlations intervariable map","d7825380":"### 4.1.1 Feature Selection with Cross Validantion\n\nWith the final of to help have a model representative, we go selection features that to can be critical for the prediction model.","720dc071":"## **2.2 Count Missing Values for Each Fields**\n\nCount missing values to determine that data is applied to its eliminate.","7b1eb3a9":"As we have all the data of training, this should be divided into two. For a side the predictor variables and the other the categorical output result. We used the variables that RFECV returned in the section before.\n\nWith the 20% per cent test is enough. So we used an 80% in training","56dcf1d2":"Return with the best hyperparameters:","8f7a283f":"### 3.1.3 Age vs Survived\n\n* The first place, we confirm the normal distribution for age. This offer data about the age density into the titanic ship.\n* The second place, we create a new vector age in fuction interquartile range form age. This method is same at behind. ","7f328b41":"## 4.2 Support Vector Machine Classifier\n\nNow, the other classifier model is Support Vector Machine (SVM). We can build a model with Scikit-Learn library in module \"svc\". We'll find the best params with GridsearchCV.","66b9b148":"## 3.2 Correlations Map For Variables All\n\nWith the final purpose in EDA for we know the behaviour of the variables, a correlations study can offer signs of relationship between variables. For this, an correlations tables and a calor map will show these interactions.\n\nWe used the correlation of Spearman method that offer Pandas library. ","198d09d0":"Graph pivot table in fuction Rate Survived for each passenger class","56817da6":"With a number of features equal to 8, the model reaches its highest score in the model. We will work with the features selected for RFECV model.","6ee1dd4a":"### 4.2.1 Find the Best Hyperparameters with GridSearchCV\n\nBuild grid params with range evaluation for each hyperparameter. The kernel used is _\"Linear, Radial Basic Function\"_. ","d1c4ad02":"## 4.2.2 Results of SVM Model","4143c40b":"In this graph, the passengers class influences to survive. The 3rd class had a minor probability for survived both first and second class. This is probability is incress when the passengers is not travel alone. ","79d07512":"## 4.1 Regresion Logistic Model\n\nThe Regression Model Logistic is a method more used for predictive analytics when the data is structured to give outputs categoricals. We import the library from scikit learn as well as its functions for measure the success.","944b8cc6":"We extract the values in list. This through a loop for.","c9965969":"# **4. Testing Prediction with Machine Learning**\n\nOnce we realize the data cleaning, create new variables and to know the correlations variables. We can build various predictive models around of Machine Learnings Methods. In this notebook, the algorithms used to will focus on logistic predictions because the categorical variables into the data training indicate its use, in addition, we will test models differently to find the best models that offer score promising better.\n\n**Define Funtions**\n\nDefine a the new fuctions for visaulize general index:\n* Confusion Matrix\n* Global Pressure\n* Global Error\n* Pressure by Categorical\n\nThis is utility special for comparisons between models and it will select the models to offer better satisfaction in its indexes. Its return a dictionary:\n","3bb10ffa":"All it seems to indicate is the correlation higher is present in Fare vs Class, an highest fare will get the Passenger had class better. The rest correlations are weak more, we build calor map of correlations for view best this relationships.","c9665fae":"As we can see, The travel alone conditions is a variable it's very critical for survival rate in spite of fare spending from passenger.","cd6c7f52":"## 2.3 New Variables Categorical\n\nWe created a new variable in combined with SiSb and Parch that, as the description dataset indicate, it points out the number the companions travel ware bring with him. With this, we'll simplify a bit through for variable categorical that it will most representative and to premise eliminated two variables.","77441f78":"This model was result be a good model for predict survival in the titanic event. With a Global Precision of 94% and Global Error of 0.54%, we can return a CVS that contain the results for submission ","1554105d":"## **2.1 Fields Description**\n\nThe Numerical Features: \n    * Age(Continuous Variable)\n    * Fare(Continuous Variable) \n    * SibSp(Discrete) \nThe Categorical Features: \n    * Survived, Class, Embarked, Pclass\nThe Alphanumeric Features:\n    * Ticked, Cabin","0a7c6518":"Create table pivot for Fares range and count the survived in travel alone function.","2050f4df":"### **3.2.1 Correaltions Table**","8472e9c8":"### **2.2.2 Count Field \"Embarked\" and Its Missing Value** ","1876b8ac":"The classification report on X_test is:","ee0c8101":"A ~20% of values are missing in training data, let's see how the 'Age' variable is distributed in a graphic.","53b1e1e2":"# 1. Charge of Data, Import Libraries and Define Functions Necessary\n\n**Import Libraries**\n\nImport libraries that is will use for cleaning data, visualization and created the predictions basics models With Machine Learning. The model basic predictions this is will focus in a **categorical regressions**, so much with **Decision Tree** in **Random Forest** and the **Boosting** as well as **Regression Categorical Model**. ","9f6fdf87":"**Charge of Data**\n\nCharge of data as much test.csv as train.cvs in two dataframe and view the descriptions in each one.\n\nValues exist Nulls in of data for training with the \"*Age*\" and \"*Cabin*\" variables in a percentage of  18.9% and 77.1% respectively on total data. ","606c7723":"I'm novice complete of Data Science, but I want to toward advance in this discipline so incredible. A so traditional dataset as the Titanic is good to begin for know tools in machine learning. I hope you like this notebook and considered a vote for it. This karnel was inspired for [Baligh Mnassri](http:\/\/www.kaggle.com\/mnassrib\/titanic-logistic-regression-with-python) and his work in data cleaning and preprocessing for model predictive.","f4ecfd47":"### **3.2.2 Calor Map of Correlations**\n\nIn this Heat Map, we corroborate the negative correlation between Fare and Pclass as well as the weak correlation of the other variables.","36025d57":"## 2.4 Training Data With Variable Categorical.\n\nIf we have data training with contain features categorical in the \"Sex, P-class and Embarked location\". To can submit the dataset to training and get a rate prediction return.","c55a58a6":"With this grid, we can build model","8754c374":"For range interquartile in Fare function, when there are more survived in the range is where fare highest and the passengers travel not alone. However, those passengers that was travel alone had a rate survived better when fare range is minor.","671a2f43":"The variable age have a normal distribution. Now we build a pivot table with a range age vector that will describe the rate survived. The Pclass variable will also to taken into account for this table.","4bd66810":"Again, we fill the missing values with the value Maximus in this case, Southampton","e3c6f901":"### 4.1.2 Results of Model Logistic","2f4471f9":"## 2.5 Apply the Same Changes to Seen Behind at Testing Data.\n\nThe test data should be structured in the same form that training data. For that reason, subject the test data to same changes, method and process. \n\n* In the first order, we get the count missing values that contain in test data.\n* Next, we replace NaN and Null values into \"Age\" and \"Fare\" variable per median its and remove the cabin field. \n* We create the new variable \"Travel Alone\" from \"SibSp, Parch\" and next remove the old variables.\n* In Addition at this, new variable \"IsMinor\" as categorical variable. \n* Finally, we construct the variable categorical for each value it's in \"Sex, Embarked and P Class\" field and transpose in dataset."}}