{"cell_type":{"d4a60a71":"code","d1f661d8":"code","512777a0":"code","f916c936":"code","4f9eb66b":"code","881d5bfc":"code","60da93be":"code","b8dacb2e":"code","7dfd1d58":"code","7a3e53c7":"code","dd7d8940":"code","35db902a":"code","0fcaab12":"code","4bf5b198":"code","83dc3d93":"code","5371b004":"code","3993dd00":"code","5e1bd11d":"code","288639fd":"code","4d6c7e19":"code","ca051c15":"code","caeab6d9":"code","4d61beb5":"code","f7021f3d":"code","b3bfe251":"code","80946bc8":"code","01ea61db":"code","5004ed3c":"code","e174e841":"code","32f0f353":"code","ed814b0b":"code","b760b025":"code","cbff8e19":"code","40125c19":"code","b3f5ec6f":"code","3cfbf3a6":"code","ba6d6832":"code","7605be79":"code","24591906":"code","812d602c":"code","f551bcb6":"code","8d3954e7":"code","c2f07ea9":"code","01c2df71":"code","80d62ff3":"code","fe66d040":"code","ec0a0ecc":"markdown","fd718e13":"markdown","90aaf606":"markdown","0390edf2":"markdown","07cf9b08":"markdown","f6ef2347":"markdown","f49a99bc":"markdown","d15ed8d6":"markdown","507d0a4d":"markdown","07a3898b":"markdown","2f5c8fb7":"markdown","e2dd075d":"markdown","c7b36421":"markdown","89e2319d":"markdown","5f2058f3":"markdown","106a3e6f":"markdown","03864d9d":"markdown","aeef6386":"markdown","71b5f3b3":"markdown","9fbbc5d0":"markdown","f7c0999d":"markdown","df184fc1":"markdown"},"source":{"d4a60a71":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import accuracy_score\nfrom pprint import pprint\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import mutual_info_regression","d1f661d8":"cancer_data = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/cancer_death_rate\/Training_set_label.csv\")\ntest_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/cancer_death_rate\/Testing_set_label.csv')\ntest_data.head()","512777a0":"cancer_data.info()","f916c936":"cancer_data.isnull().sum()","4f9eb66b":"data=cancer_data","881d5bfc":"data.duplicated().sum()","60da93be":"data=data.drop_duplicates()","b8dacb2e":"data=data[data['PctEmployed16_Over'].notna()]","7dfd1d58":"data=data.fillna(cancer_data.mean())","7a3e53c7":"data.isnull().sum()","dd7d8940":"data.corr()","35db902a":"data=data[['avgAnnCount','avgDeathsPerYear','incidenceRate','medIncome','popEst2015','povertyPercent','studyPerCap',\n'MedianAge','MedianAgeMale','MedianAgeFemale','AvgHouseholdSize','PercentMarried','PctNoHS18_24','PctHS18_24','PctSomeCol18_24',\n'PctBachDeg18_24','PctHS25_Over','PctBachDeg25_Over','PctEmployed16_Over','PctUnemployed16_Over','PctPrivateCoverage','PctPrivateCoverageAlone',\n'PctEmpPrivCoverage','PctPublicCoverage','PctPublicCoverageAlone','PctWhite','PctBlack','PctAsian','PctOtherRace','PctMarriedHouseholds','BirthRate','TARGET_deathRate']]","0fcaab12":"test_data=test_data[['avgAnnCount','avgDeathsPerYear','incidenceRate','medIncome','popEst2015','povertyPercent','studyPerCap',\n'MedianAge','MedianAgeMale','MedianAgeFemale','AvgHouseholdSize','PercentMarried','PctNoHS18_24','PctHS18_24','PctSomeCol18_24',\n'PctBachDeg18_24','PctHS25_Over','PctBachDeg25_Over','PctEmployed16_Over','PctUnemployed16_Over','PctPrivateCoverage','PctPrivateCoverageAlone','PctEmpPrivCoverage','PctPublicCoverage','PctPublicCoverageAlone','PctWhite','PctBlack','PctAsian','PctOtherRace','PctMarriedHouseholds','BirthRate']]","4bf5b198":"test_data.isnull().sum()","83dc3d93":"test_data=test_data.fillna(test_data.mean())","5371b004":"X=data.drop('TARGET_deathRate',axis=1)\ny=data['TARGET_deathRate']","3993dd00":"X_train,X_test,y_train, y_test=train_test_split(X,y,test_size=0.1,random_state=42)","5e1bd11d":"model=LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)","288639fd":"MSE=mean_squared_error(y_pred, y_test)\nMSE","4d6c7e19":"# predications=model.predict(test_data)\n# # To create Dataframe of predicted value with particular respective index\n# res = pd.DataFrame(predictions) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\n# res.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\n# res.columns = [\"prediction\"]\n\n# # To download the csv file locally\n# from google.colab import files\n# res.to_csv('prediction_results.csv')         \n# files.download('prediction_results.csv')","ca051c15":"rlf=RandomForestRegressor(random_state=42)\nrlf.fit(X_train, y_train)\ny_pred=rlf.predict(X_test)","caeab6d9":"MSE=mean_squared_error(y_pred,y_test)\nMSE","4d61beb5":"# predications=rlf.predict(test_data)\n# # To create Dataframe of predicted value with particular respective index\n# res = pd.DataFrame(predictions) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\n# res.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\n# res.columns = [\"prediction\"]\n\n# # To download the csv file locally\n# from google.colab import files\n# res.to_csv('prediction_results.csv')         \n# files.download('prediction_results.csv')","f7021f3d":"rf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","b3bfe251":"# Number of trees in random forest\nn_estimators = [522,622]\n# Number of features to consider at every split\nmax_features = ['auto']\n# Maximum number of levels in tree\nmax_depth = [20,10]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [5,4]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [2, 3]\n# Method of selecting samples for training each tree\nbootstrap = [True,False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","80946bc8":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","01ea61db":"# rf_random.best_params_","5004ed3c":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\n","e174e841":"base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)","32f0f353":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)","ed814b0b":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","b760b025":"y_pred=best_random.predict(X_test)\nMSE=mean_squared_error(y_test,y_pred)\nMSE","cbff8e19":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [19,20,21],\n    'max_features': ['auto'],\n    'min_samples_leaf': [12,13],\n    'min_samples_split': [5,3],\n    'n_estimators': [522]\n}\n# Create a based model\nrfr = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rfr, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)","40125c19":"grid_search.fit(X_train, y_train)\ngrid_search.best_params_","b3f5ec6f":"print(grid_search.best_estimator_)","3cfbf3a6":"best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)","ba6d6832":"print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) \/ base_accuracy))","7605be79":"y_pred=best_grid.predict(X_test)\nMSE=mean_squared_error(y_pred, y_test)\nMSE","24591906":"predictions = best_grid.predict(test_data) # you must do a pre-processing on your evaluation data\n# To create Dataframe of predicted value with particular respective index\nres = pd.DataFrame(predictions) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\nres.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\nres.columns = [\"prediction\"]\n\n# To download the csv file locally\nfrom google.colab import files\nres.to_csv('prediction_results.csv')         \nfiles.download('prediction_results.csv')","812d602c":"feat_importances = pd.Series(best_grid.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","f551bcb6":"# Create a selector object that will use the random forest classifier to identify\n# It will select the features based on the importance score\nrf_sfm = SelectFromModel(best_grid)\n#print(rf_sfm)\n#Alternatively if you want to set a specific threshold and select the features you may the following code that is commented\n# features that have an importance of more than 0.13 ( all top 4 features lie above this)\n# rf_sfm = SelectFromModel(rf_clf, threshold=0.13)\n\n# Train the selector\nrf_sfm = best_grid.fit(X_train, y_train)\n","8d3954e7":"# feature selection\nf_selector = SelectKBest(score_func=f_regression, k='all')\n# learn relationship from training data\nf_selector.fit(X_train, y_train)\n# transform train input data\nX_train_fs = f_selector.transform(X_train)\n# transform test input data\nX_test_fs = f_selector.transform(X_test)\n# Plot the scores for the features\nplt.bar([i for i in range(len(f_selector.scores_))], f_selector.scores_)\nplt.xlabel(\"feature index\")\nplt.ylabel(\"F-value (transformed from the correlation values)\")\nplt.show()","c2f07ea9":"X_train_fs","01c2df71":"grid_search.fit(X_train, y_train)\ny_pred=grid_search.predict(X_test)\nMSE=mean_squared_error(y_pred, y_test)\nMSE","80d62ff3":"grid_search_1=GridSearchCV(estimator = rfr, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\ngrid_search_1.fit(X_train_fs,y_train)\ny_pred=grid_search_1.predict(X_test_fs)\nMSE=mean_squared_error(y_pred, y_test)\nMSE","fe66d040":"predictions = grid_search_1.predict(test_data) # you must do a pre-processing on your evaluation data\n# To create Dataframe of predicted value with particular respective index\nres = pd.DataFrame(predictions) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\nres.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\nres.columns = [\"prediction\"]\n\n# To download the csv file locally\nfrom google.colab import files\nres.to_csv('prediction_results.csv')         \nfiles.download('prediction_results.csv')","ec0a0ecc":"**Grid Search**","fd718e13":"**Evaluate your model with MSE**","90aaf606":"**Evaluate MSE of model**","0390edf2":"**Select the Input and Target Features of the data**","07cf9b08":"**Load the data and display first 5 rows.**","f6ef2347":"# Task 4\n\n**Feature Selection**","f49a99bc":"# Task 3","d15ed8d6":"**Correlation table**","507d0a4d":"**Evaluate MSE of Grid Search**","07a3898b":"**Select the test data**","2f5c8fb7":"**Check data types**","e2dd075d":"**Evaluate your model with Mean Squared Error (MSE)**","c7b36421":"**Exploratory Data Analysis**","89e2319d":"**Import Libraries**\n\n","5f2058f3":"# Task 1","106a3e6f":"**Build Random Forest Model**","03864d9d":"**Select the train data**","aeef6386":"**Build a Linear Regression Model**","71b5f3b3":"**Hyperparameter tuning on Random Forest Model**","9fbbc5d0":"**Split the data into Train and Test (for linear regression)**\n","f7c0999d":"# Task 2","df184fc1":"**Checking and filling missing values of test data**"}}