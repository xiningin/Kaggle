{"cell_type":{"9d0cc281":"code","3aad088d":"code","ff065ee7":"code","5f09cff1":"code","896a1426":"code","70ab7ce7":"code","105a928b":"code","6f7182bc":"code","17dc48b7":"code","17a33277":"code","fdd2227c":"code","5a88411d":"code","71f2f176":"code","b12206fa":"code","bcbe5915":"code","f958b13c":"code","34c7d33d":"code","b3795680":"code","b4619519":"code","3b2d858d":"code","5307f3cc":"code","bd124b37":"code","b3375a95":"code","7b92a931":"code","17ad9097":"code","82db6253":"code","67e0d359":"code","c3d429f2":"code","8bb37993":"code","1634754a":"code","58136f64":"code","643df07f":"code","f5e1622e":"code","7281ed7f":"code","4913d428":"code","35353f18":"code","497c2dad":"code","8939832a":"code","89a76808":"code","07cef8b5":"code","947301db":"code","55013b14":"code","57b3dc3d":"code","f04a64d0":"code","5d37fe66":"code","ce91daa3":"code","77c32596":"code","bc03d864":"code","aa3d66a0":"code","783dfa52":"code","0c8d06ea":"code","0f973b99":"code","a58e4f0a":"code","fd8dd508":"code","5cc31fd2":"code","1cd72c16":"markdown","9972c571":"markdown","25f69a96":"markdown"},"source":{"9d0cc281":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3aad088d":"!pip install openpyxl\ndf = pd.read_excel(r\"\/kaggle\/input\/flight-fare-prediction-mh\/Data_Train.xlsx\")\ntest_data = pd.read_excel(\"\/kaggle\/input\/flight-fare-prediction-mh\/Test_set.xlsx\")","ff065ee7":"df.head()","5f09cff1":"print(df.shape,test.shape)","896a1426":"df.describe()","70ab7ce7":"df.info()","105a928b":"df.nunique()","6f7182bc":"df[df[\"Total_Stops\"].isnull()]","17dc48b7":"test.info()","17a33277":"df.dropna(inplace = True)","fdd2227c":"df.shape","5a88411d":"df[\"Journey_Month\"] = pd.to_datetime(df[\"Date_of_Journey\"],format = \"%d\/%m\/%Y\").dt.month\ndf[\"Journey_Day\"] = pd.to_datetime(df[\"Date_of_Journey\"],format=\"%d\/%m\/%Y\").dt.day","71f2f176":"df.head()","b12206fa":"df.drop([\"Date_of_Journey\"],axis=1,inplace=True)","bcbe5915":"df['Dep_hour'] = pd.to_datetime(df[\"Dep_Time\"]).dt.hour\ndf[\"Dep_min\"] = pd.to_datetime(df[\"Dep_Time\"]).dt.minute\ndf.drop(['Dep_Time'],axis=1,inplace=True)","f958b13c":"df.head()","34c7d33d":"df[\"Arrival_hour\"] = pd.to_datetime(df['Arrival_Time']).dt.hour\ndf[\"Arrival_min\"] = pd.to_datetime(df['Arrival_Time']).dt.minute\ndf.drop([\"Arrival_Time\"],axis=1,inplace=True)","b3795680":"df.head()","b4619519":"duration = list(df[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]\n            # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))","3b2d858d":"df[\"Duration_hour\"] = duration_hours\ndf[\"Duration_min\"] = duration_mins","5307f3cc":"df.head()","bd124b37":"df.drop([\"Duration\"],axis=1,inplace=True)","b3375a95":"df.head()","7b92a931":"Airline = pd.get_dummies(df[[\"Airline\"]],drop_first=True)\nSource = pd.get_dummies(df[[\"Source\"]],drop_first=True)\nDestination = pd.get_dummies(df[[\"Destination\"]],drop_first=True)","17ad9097":"Airline.head()","82db6253":"Source.head()","67e0d359":"Destination.head()","c3d429f2":"df.drop(['Route','Additional_Info'],axis=1,inplace=True)","8bb37993":"df.head()","1634754a":"df['Total_Stops'].unique()","58136f64":"df.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","643df07f":"df.head()","f5e1622e":"train_data = pd.concat([df,Airline,Source,Destination],axis=1)","7281ed7f":"train_data.head()","4913d428":"train_data.drop(['Airline',\"Source\",\"Destination\"],axis=1,inplace=True)","35353f18":"train_data.head()","497c2dad":"train_data.shape","8939832a":"train_data.shape","89a76808":"test_data.head()","07cef8b5":"print(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)","947301db":"import seaborn as sns \nimport matplotlib.pyplot as plt\nplt.figure(figsize = (18,18))\nsns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","55013b14":" \nX = train_data.drop([\"Price\"],axis=1)\ny = train_data[\"Price\"]","57b3dc3d":"from sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","f04a64d0":"print(selection.feature_importances_)","5d37fe66":"plt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","ce91daa3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nfrom sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)\ny_pred = reg_rf.predict(X_test)","77c32596":"reg_rf.score(X_test,y_test)","bc03d864":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]\n","aa3d66a0":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","783dfa52":"rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","0c8d06ea":"rf_random.fit(X_train,y_train)","0f973b99":"rf_random.best_params_","a58e4f0a":"preds = rf_random.predict(X_test)","fd8dd508":"preds","5cc31fd2":"from sklearn.metrics import accuracy_score\n\nclf = RandomForestRegressor(n_estimators = 700,\n min_samples_split = 15,\n min_samples_leaf =  1,\n max_features = 'auto',\n max_depth =  20)\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)","1cd72c16":"There are a few columns with incorrect data type, so we need to make them into a readable format \n","9972c571":"Test","25f69a96":"We see that routes, stop do the same thing"}}