{"cell_type":{"48d08d7a":"code","71bb1e79":"code","ccc60433":"code","6a20a5b4":"code","40e7e091":"code","836f5c46":"code","42b42ddf":"code","317efda8":"markdown","9d3e65cc":"markdown","0347d93a":"markdown","59a68e82":"markdown","68936afc":"markdown"},"source":{"48d08d7a":"import os\nimport cv2 as cv\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\n\n# Path to images\npath_images = \"..\/input\/the-oxfordiiit-pet-dataset\/images\/\"\n\n# List of all images' names\nfiles = os.listdir(path_images)\n\nprint('Amount of images:', len(files))","71bb1e79":"n_rows = 3\nn_cols = 3\n\nfig, ax = plt.subplots(n_rows, n_cols, figsize = (20, 20))\n\nfor i in range(n_rows):\n    for j in range(n_cols):\n        index = np.random.randint(len(files))\n        \n        ax[i][j].imshow(Image.open(path_images + files[index]))\n\nplt.tight_layout()\nplt.show()","ccc60433":"test = np.asarray(Image.open(path_images + files[np.random.randint(len(files))]))\n\nimage_resized = resize(test, (test.shape[0] \/\/ 7, test.shape[1] \/\/ 7), anti_aliasing = True)\nimage_resized = resize(image_resized, (test.shape[0], test.shape[1]))\n\nfig, ax = plt.subplots(1, 2, figsize = (15, 15))\n\nax[0].imshow(test)\nax[0].set_title('Original')\n\nax[1].imshow(image_resized)\nax[1].set_title('Resized')\n\nplt.tight_layout()\nplt.show()","6a20a5b4":"new_image = cv.convertScaleAbs(image_resized, alpha = 200)\n\nfig, ax = plt.subplots(1, 2, figsize = (15, 15))\n\nax[0].imshow(test)\nax[0].set_title('Original')\n\nax[1].imshow(new_image)\nax[1].set_title('Processed')\n\nplt.tight_layout()\nplt.show()","40e7e091":"fig, ax = plt.subplots(1, 3, figsize = (15, 15))\n\nax[0].imshow(test)\nax[0].set_title('Original Image')\n\nax[1].imshow(image_resized)\nax[1].set_title('Resized Image')\n\nax[2].imshow(new_image)\nax[2].set_title('Resized Image with Less Contrast')\n\nplt.tight_layout()\nplt.show()","836f5c46":"def rot(image, factor, alpha):\n    rot_image = resize(image, (image.shape[0] \/\/ factor, image.shape[1] \/\/ factor),\n                      anti_aliasing = True)\n    \n    rot_image = resize(rot_image, (image.shape[0], image.shape[1]))\n    \n    rot_image = cv.convertScaleAbs(rot_image, alpha = alpha)\n    \n    return rot_image","42b42ddf":"factor = 8\nalpha = 230\nn_rows = 4\n\nfig, ax = plt.subplots(n_rows, 2, figsize = (15, 15))\n\nfor i in range(n_rows):\n    index = np.random.randint(len(files))\n    \n    image = np.asarray(Image.open(path_images + files[index]))\n    rot_test = rot(image, factor, alpha)\n    \n    ax[i][0].imshow(image)\n    ax[i][1].imshow(rot_test)\n\nplt.tight_layout()\nplt.show()","317efda8":"# Testing the function","9d3e65cc":"# Putting it all together in a function","0347d93a":"# Starting the rottening  \nLet's start the rottenning, this is going to be fairly easy, I'm thinking about shrinking the image and then expanding it by a factor *n*. This is going to make the edges of the images really ugly and a lot of details are going to be lost. As a plus, I would play a little with the contrast of the images.\n","59a68e82":"# Rottenning Images  \n\nFor a project in my school I would like to create an app to be able to increase the resolution of an image, totally based on the Pix2Pix architecture. The first step is to get the dataset of images, fortunately, Oxford has this huge [dataset of pets images](https:\/\/www.kaggle.com\/amanagr\/the-oxfordiiit-pet-dataset) that is available, so that's what I'm going to use.  \nIf you haven't seen the Pix2Pix paper you should check it out [here ](https:\/\/www.tensorflow.org\/tutorials\/generative\/pix2pix).  \nNow, the model receives an image with a low resolution and it outputs the same image with better resolution, so, given that the Oxford dataset already has good resolution, our job here is to \"rot\" the images. This is a good oportunity to show some basics of working with images.\n","68936afc":"# Looking at some images on the dataset"}}