{"cell_type":{"47dae0c9":"code","34f0b1f9":"code","7b00b60b":"code","60d504c0":"code","0820d682":"code","0b76d714":"code","891fc7d4":"code","f010189e":"code","8efb3de0":"code","a2b5c86a":"code","0626f758":"code","f41d3d79":"code","e303205d":"code","d6c0eecb":"code","98bf897a":"code","5cb36d3e":"code","88d165f1":"code","3d0771ef":"code","1216cf1b":"code","5abbbb83":"code","9a41425c":"code","6a891363":"code","147e1ae6":"markdown","72fd72e8":"markdown","2eb38608":"markdown","431c24ba":"markdown","1d3e9aa3":"markdown"},"source":{"47dae0c9":"import numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.signal import *\nimport gc\nfrom sklearn.feature_selection import f_classif\nimport lightgbm as lgbm\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon, uniform, norm\nfrom scipy.stats import randint, poisson\nfrom sklearn.metrics import confusion_matrix, make_scorer\n\nsns.set(style=\"darkgrid\", context=\"notebook\")\nrand_seed = 135\nnp.random.seed(rand_seed)\nxsize = 12.0\nysize = 8.0\n\nimport os\nprint(os.listdir(\"..\/input\"))","34f0b1f9":"def reduce_mem_usage(df, verbose=True):\n    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print(\"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","7b00b60b":"%%time\n\ntrain_meta_df = pd.read_csv(\"..\/input\/metadata_train.csv\")\ntrain_df = pq.read_pandas(\"..\/input\/train.parquet\").to_pandas()","60d504c0":"%%time\n\ntrain_meta_df = reduce_mem_usage(train_meta_df)\ntrain_df = reduce_mem_usage(train_df)\ngc.collect()","0820d682":"train_meta_df.shape","0b76d714":"train_meta_df.head(6)","891fc7d4":"train_df.head()","f010189e":"fig, axes = plt.subplots(nrows=2)\nfig.set_size_inches(xsize, 2.0*ysize)\n\nsns.countplot(x=\"phase\", data=train_meta_df, ax=axes[0])\n\nsns.countplot(x=\"target\", data=train_meta_df, ax=axes[1])\n\nplt.show()","8efb3de0":"fig, ax = plt.subplots()\nfig.set_size_inches(xsize, ysize)\n\nsns.countplot(x=\"phase\", hue=\"target\", data=train_meta_df, ax=ax)\n\nplt.show()","a2b5c86a":"fig, axes = plt.subplots(nrows=3, ncols=2)\nfig.set_size_inches(2.0*xsize, 2.0*ysize)\naxes = axes.flatten()\n\naxes[0].plot(train_df[\"0\"].values, marker=\"o\", linestyle=\"none\")\naxes[0].set_title(\"Signal ID: 0\")\n\naxes[1].plot(train_df[\"2\"].values, marker=\"o\", linestyle=\"none\")\naxes[1].set_title(\"Signal ID: 1\")\n\naxes[2].plot(train_df[\"3\"].values, marker=\"o\", linestyle=\"none\")\naxes[2].set_title(\"Signal ID: 2\")\n\naxes[3].plot(train_df[\"4\"].values, marker=\"o\", linestyle=\"none\")\naxes[3].set_title(\"Signal ID: 3\")\n\naxes[4].plot(train_df[\"5\"].values, marker=\"o\", linestyle=\"none\")\naxes[4].set_title(\"Signal ID: 4\")\n\naxes[5].plot(train_df[\"6\"].values, marker=\"o\", linestyle=\"none\")\naxes[5].set_title(\"Signal ID: 5\")\n\nplt.show()","0626f758":"%%time\n\ntrain_meta_df[\"signal_mean\"] = train_df.agg(np.mean).values\ntrain_meta_df[\"signal_sum\"] = train_df.agg(np.sum).values\ntrain_meta_df[\"signal_std\"] = train_df.agg(np.std).values","f41d3d79":"train_meta_df.head()","e303205d":"fig, axes = plt.subplots(nrows=2, ncols=2)\nfig.set_size_inches(2.0*xsize, 2.0*ysize)\naxes = axes.flatten()\n\nf, Pxx = welch(train_df[\"0\"].values)\naxes[0].plot(f, Pxx, marker=\"o\", linestyle=\"none\")\naxes[0].set_title(\"Signal ID: 0\")\naxes[0].axhline(y=2.5, color=\"k\", linestyle=\"--\")\n\nf, Pxx = welch(train_df[\"1\"].values)\naxes[1].plot(f, Pxx, marker=\"o\", linestyle=\"none\")\naxes[1].set_title(\"Signal ID: 1\")\naxes[1].axhline(y=2.5, color=\"k\", linestyle=\"--\")\n\nf, Pxx = welch(train_df[\"2\"].values)\naxes[2].plot(f, Pxx, marker=\"o\", linestyle=\"none\")\naxes[2].set_title(\"Signal ID: 2\")\naxes[2].axhline(y=2.5, color=\"k\", linestyle=\"--\")\n\nf, Pxx = welch(train_df[\"3\"].values)\naxes[3].plot(f, Pxx, marker=\"o\", linestyle=\"none\")\naxes[3].set_title(\"Signal ID: 3\")\naxes[3].axhline(y=2.5, color=\"k\", linestyle=\"--\")\n\nplt.show()","d6c0eecb":"%%time\n\ndef welch_max_power_and_frequency(signal):\n    f, Pxx = welch(signal)\n    ix = np.argmax(Pxx)\n    strong_count = np.sum(Pxx>2.5)\n    avg_amp = np.mean(Pxx)\n    sum_amp = np.sum(Pxx)\n    std_amp = np.std(Pxx)\n    median_amp = np.median(Pxx)\n    return [Pxx[ix], f[ix], strong_count, avg_amp, sum_amp, std_amp, median_amp]\n\npower_spectrum_summary = train_df.apply(welch_max_power_and_frequency, result_type=\"expand\")","98bf897a":"power_spectrum_summary = power_spectrum_summary.T.rename(columns={0:\"max_amp\", 1:\"max_freq\", 2:\"strong_amp_count\", 3:\"avg_amp\", \n                                                                  4:\"sum_amp\", 5:\"std_amp\", 6:\"median_amp\"})\npower_spectrum_summary.head()","5cb36d3e":"power_spectrum_summary.index = power_spectrum_summary.index.astype(int)\ntrain_meta_df = train_meta_df.merge(power_spectrum_summary, left_on=\"signal_id\", right_index=True)\ntrain_meta_df.head()","88d165f1":"X_cols = [\"phase\"] + train_meta_df.columns[4:].tolist()\nX_cols","3d0771ef":"Fvals, pvals = f_classif(train_meta_df[X_cols], train_meta_df[\"target\"])\n\nprint(\"F-value | P-value | Feature Name\")\nprint(\"--------------------------------\")\n\nfor i, col in enumerate(X_cols):\n    print(\"%.4f\"%Fvals[i]+\" | \"+\"%.4f\"%pvals[i]+\" | \"+col)","1216cf1b":"def mcc(y_true, y_pred, labels=None, sample_weight=None):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=labels, sample_weight=sample_weight).ravel()\n    mcc = (tp*tn - fp*fn)\/np.sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))\n    return mcc\n\nmcc_scorer = make_scorer(mcc)\n\nlgbm_classifier = lgbm.LGBMClassifier(boosting_type='gbdt', max_depth=-1, subsample_for_bin=200000, objective=\"binary\", \n                                      class_weight=None, min_split_gain=0.0, min_child_weight=0.001, subsample=1.0, \n                                      subsample_freq=0, random_state=rand_seed, n_jobs=1, silent=True, importance_type='split')\n\nparam_distributions = {\n    \"num_leaves\": randint(16, 48),\n    \"learning_rate\": expon(),\n    \"reg_alpha\": expon(),\n    \"reg_lambda\": expon(),\n    \"colsample_bytree\": uniform(0.25, 1.0),\n    \"min_child_samples\": randint(10, 30),\n    \"n_estimators\": randint(50, 250)\n}\n\nclf = RandomizedSearchCV(lgbm_classifier, param_distributions, n_iter=100, scoring=mcc_scorer, fit_params=None, n_jobs=1, iid=True, \n                         refit=True, cv=5, verbose=1, random_state=rand_seed, error_score=-1.0, return_train_score=True)\nclf.fit(train_meta_df[X_cols], train_meta_df[\"target\"])","5abbbb83":"print(clf.best_score_)","9a41425c":"clf.best_estimator_","6a891363":"fig, ax = plt.subplots()\nfig.set_size_inches(xsize, ysize)\n\nlgbm.plot_importance(clf.best_estimator_, ax=ax)\n\nplt.show()","147e1ae6":"Note signals 0, 1, and 2 are not faulty and signals 3, 4, and 5 are faulty. They're messy, noisy, and not obviously periodic, oh boy. However, there are quite a few signal processing techniques that can be used anyways. Speaking of which, it's time for some feature engineering. Starting with some basic aggregations.","72fd72e8":"So the phase counts are all equal, so this will not be a useful variable on its own for detecting a fault. Furthermore, it's interesting to not that the target much more likely to be 0, or the line has no fault, by default. This might might make models difficult to calibrate later on, but that's a later issue.\n\nNow let's take a look at some of these signals.","2eb38608":"These results are interesting. The features signal_std, signal_mean, and avg_amp seem to be the most important. This makes sense intuitively because a faulty line will have more noise in its signal than a non faulty line, so for a faulty line we would expect a abnormally large signal_std, a signal_mean that is outside of the normal range due to outliers, and abnormally large avg_amp that results from lower frequencies becoming more present due to the noise from a faulty line. The next set of important features, max_amp, strong_amp_count, std_amp, and median_amp while not as important still support the current hypothesis of what the lgbm model is capturing. Finally signal_sum, sum_amp, max_freq are not important features because sum and median are robust to large outliers hence why they are not important and as determined earlier phase is not an important feature at all. \n\n\nSo the moral of this brief EDA is that we should look for features that quantify the abnormal \"noise of a signal,\" and we expect faulty lines to have large amounts of noise and not faulty lines to have low amounts of noise. Thanks for reading this kernel and good luck in detecting faulty power lines!\n\n\n*Correction*: Versions 1 and 2 of this kernel incorrectly calculated median_amp by instead calculating the std of power spectrum amplitudes (effectively creating two std_amp features). Fixing this goof doesn't change my hypothesis about what kinds of features will do well in this competition, but it does even out the distribution of feature importance a bit. If you see any other issues with the kernel let me know or if you have any questions or discussion bits let me know.","431c24ba":"So as expected phase is a useless feature on its own, but interestingly std_amp, median_amp, signal_std, max_amp may not be extremely useful variables because we cannot reject the null with a significance of 0.01 for these. However the features signal_mean, signal_sum, max_freq, strong_amp_count, avg_amp, and sum_amp all look like very useful features, even on their own.","1d3e9aa3":"Now to look into some power spectrums since this is a signal processing challenge after all."}}