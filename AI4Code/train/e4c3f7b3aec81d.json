{"cell_type":{"beb76970":"code","73b9c8ed":"code","3ecbcd85":"code","bd1ab8ef":"code","eec69249":"code","d17edee6":"code","6782cfda":"code","ab392a0d":"code","4a09b515":"code","fe51df36":"code","6521b23c":"code","b51576c5":"code","d08d5ffb":"code","9fb1159c":"code","6da311d9":"code","9d41bd7c":"code","694b9024":"code","eb52816d":"code","21927371":"markdown","dc77b510":"markdown","9c82e891":"markdown","5d6f557e":"markdown","33ec7176":"markdown","f5822118":"markdown"},"source":{"beb76970":"import os # operation system variables\nimport gc \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nfrom sklearn.model_selection import train_test_split # creat train and test datasets to modeling\nfrom sklearn.metrics import classification_report, f1_score, precision_score, recall_score # report and metrics modules \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler # data preprocessing\nfrom sklearn.pipeline import make_pipeline # additionals modules\nfrom sklearn.compose import make_column_transformer\n\n# ML models upload\nfrom sklearn.linear_model import LogisticRegression\n\n# Additional models\nimport xgboost as xgb, lightgbm as lgbm\n\nfrom tensorflow import keras # nn modeling\nfrom tensorflow.keras import layers\nimport tensorflow as tf","73b9c8ed":"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #suppressing GPU warnings","3ecbcd85":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif\n\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","bd1ab8ef":"%%time\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv', index_col='id')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv', index_col='id')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","eec69249":"%%time\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","d17edee6":"y = train.pop('target')\nsubmission_index = test.index\nfeatures = list(train.columns)","6782cfda":"numerical_transformer = make_pipeline(\n    StandardScaler(), #Standardization\n    MinMaxScaler(),    #Normalization\n)\n\npreprocessor = make_column_transformer(\n    (numerical_transformer, features), #since all features are numerical\/continous\n)\n\ntrain = preprocessor.fit_transform(train)\ntest = preprocessor.transform(test)","ab392a0d":"x_train, x_val, y_train, y_val = train_test_split(train, y, test_size=0.33)","4a09b515":"model_lr = LogisticRegression(solver='liblinear', verbose=1)\nmodel_lr.fit(train, y)","fe51df36":"input_shape = [x_train.shape[1]]\nPATIENCE = 10\nMIN_DELTA = 0.0005\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.4),\n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.4),\n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.4),\n    layers.Dense(units=1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])","6521b23c":"%%time\n\nBATCH_SIZE = 128\nEPOCHS = 20\n\nhistory = model.fit(\n    x_train, y_train,\n    validation_data=(x_val, y_val),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    #callbacks=[early_stopping], \n    verbose=1 # we need it to control ou NN training\n)","b51576c5":"model_lgbm = lgbm.LGBMClassifier(num_iterations = 400,\n    objective = \"binary\",\n    metric = \"auc\")\nmodel_lgbm.fit(train, y)","d08d5ffb":"%%time\ny_pred_f = model.predict(test).ravel()","9fb1159c":"%%time\ny_pred_f_logref = model_lr.predict_proba(test)","6da311d9":"y_pred_f_lgbm = model_lgbm.predict_proba(test)","9d41bd7c":"sub['target'] = (y_pred_f+y_pred_f_logref[:,1]+y_pred_f_lgbm[:,1])\/3","694b9024":"sub.head()","eb52816d":"sub.to_csv('submission.csv', index = 0)\nsub","21927371":"# TPS November: simple ensemble NN, LGBM and logreg\n\nThis notebook work with simple ensemble models\n\nNotebook plan:\n\n1. Modules import.\n2. Utils.\n3. Data load and prepare\n4. Models train\n5. Results.","dc77b510":"## Modules","9c82e891":"## 5. Results\n\nPredict our results and save them to `submission.csv`","5d6f557e":"## 3. Data load and prepare\n\nAt first we load data to our memory, then reduce memory usage.","33ec7176":"## 2. Utils\n\nWe use only one util to reduce memory usage.","f5822118":"## 4. Models train"}}