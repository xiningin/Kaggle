{"cell_type":{"48f5cafd":"code","1faa1b81":"code","838b31d7":"code","a4e01c78":"code","5a738fc4":"code","95fde278":"code","fc67a9ae":"code","ea887fb1":"code","5b2df547":"code","c68bfe1b":"code","a4569a9f":"code","f59ad095":"code","7e716121":"code","eee73f76":"code","cc79412f":"code","6e8888a9":"code","a7056b25":"code","54f3bafa":"code","d444758c":"code","dffa4a85":"code","3565aa5e":"code","3e5d05ff":"code","2e73de26":"code","b77030f8":"code","e9be749c":"code","3c1e6859":"code","e3787c8a":"code","09543ce4":"code","cff7266a":"code","a175d924":"code","50003649":"code","c3fc4582":"code","0abbd13a":"code","42747e98":"markdown","533b0e4d":"markdown","6d743cee":"markdown","16a87b11":"markdown","80c9ab4f":"markdown","0b50af44":"markdown","312546da":"markdown","873a56ac":"markdown","2d788731":"markdown","fc65f05e":"markdown","9008ab68":"markdown","3389f98c":"markdown","357517be":"markdown","119dd059":"markdown"},"source":{"48f5cafd":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Input, Model\nfrom keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, RepeatVector, Reshape, concatenate\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization, Activation, LeakyReLU, Flatten\nfrom keras.layers import Dense, Add, ZeroPadding2D\nimport keras.optimizers as KO\n\nfrom tqdm import tqdm_notebook\nimport datetime\n","1faa1b81":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","838b31d7":"train_df = pd.read_csv(\"..\/input\/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"..\/input\/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","a4e01c78":"train_df[\"images\"] = [np.array(load_img(\"..\/input\/train\/images\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","5a738fc4":"train_df[\"masks\"] = [np.array(load_img(\"..\/input\/train\/masks\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","95fde278":"train_df[\"coverage\"] = train_df.masks.map(np.sum) \/ pow(img_size_ori, 2)","fc67a9ae":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","ea887fb1":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","5b2df547":"sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","c68bfe1b":"max_images = 25\ngrid_width = 5\ngrid_height = int(max_images \/ grid_width)*2\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*4, grid_height*4))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax_image = axs[int(i \/ grid_width)*2, i % grid_width]\n    ax_image.imshow(img, cmap=\"Greys\")\n    ax_image.set_title(\"Image {0}\\nDepth: {1}\".format(i, train_df.loc[idx].z))\n    ax_image.set_yticklabels([])\n    ax_image.set_xticklabels([])\n    ax_mask = axs[int(i \/ grid_width)*2+1, i % grid_width]\n    ax_mask.imshow(img, cmap=\"Greys\")\n    ax_mask.imshow(mask, alpha=0.2, cmap=\"Greens\")\n    ax_mask.set_title(\"Mask {0}\\nCoverage: {1}\".format(i,  round(train_df.loc[idx].coverage, 2)))\n    ax_mask.set_yticklabels([])\n    ax_mask.set_xticklabels([])","a4569a9f":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","f59ad095":"def identity_block(input_tensor, kernel_size, filters):\n    filters1, filters2, filters3 = filters\n\n    x = Conv2D(filters1, (1, 1))(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters2, kernel_size, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters3, (1, 1))(x)\n    x = BatchNormalization()(x)\n\n    x = Add()([x, input_tensor])\n    x = Activation('relu')(x)\n    return x\n\ndef conv_block(input_tensor, kernel_size, filters, strides=(2, 2)):\n    filters1, filters2, filters3 = filters\n\n    x = Conv2D(filters1, (1, 1), strides=strides)(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters2, kernel_size, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters3, (1, 1))(x)\n    x = BatchNormalization()(x)\n\n    shortcut = Conv2D(filters3, (1, 1), strides=strides)(input_tensor)\n    shortcut = BatchNormalization()(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation('relu')(x)\n    return x\n\ndef ResNet50(img_input):\n    fmap = []\n\n    x = Conv2D(64, (7, 7), strides=(1, 1), padding='same')(img_input)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    fmap.append(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n    x = conv_block(x, 3, [64, 64, 256], strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256])\n    x = identity_block(x, 3, [64, 64, 256])\n    fmap.append(x)\n\n    x = conv_block(x, 3, [128, 128, 512])\n    x = identity_block(x, 3, [128, 128, 512])\n    x = identity_block(x, 3, [128, 128, 512])\n    x = identity_block(x, 3, [128, 128, 512])\n    fmap.append(x)\n\n    x = conv_block(x, 3, [256, 256, 1024])\n    x = identity_block(x, 3, [256, 256, 1024])\n    x = identity_block(x, 3, [256, 256, 1024])\n    x = identity_block(x, 3, [256, 256, 1024])\n    x = identity_block(x, 3, [256, 256, 1024])\n    x = identity_block(x, 3, [256, 256, 1024])\n    fmap.append(x)\n\n    x = conv_block(x, 3, [512, 512, 2048])\n    x = identity_block(x, 3, [512, 512, 2048])\n    x = identity_block(x, 3, [512, 512, 2048])\n    fmap.append(x)\n\n    return fmap\n\ndef up_block(x, _x, num_filters, dropout):\n    conv = Conv2DTranspose(num_filters, (3, 3), padding='same', activation='relu', strides=(2, 2)) (x)\n    conv = concatenate([conv, _x])\n    conv = Dropout(dropout)(conv) if dropout != 0.0 else conv\n    conv = Conv2D(num_filters, (3, 3), padding='same', activation='relu')(conv)\n    conv = Conv2D(num_filters, (3, 3), padding='same', activation='relu')(conv)\n    return conv","7e716121":"def ResnetUnet():\n    img_input = Input(shape=(128, 128, 1), name='input_image')\n    fmap = ResNet50(img_input)\n    x = Conv2D(2048, (3, 3), padding='same', activation='relu')(fmap[-1])\n    x = MaxPooling2D((2, 2))(x)\n    for i in range(1, len(fmap)+1):\n        f = fmap[-i]\n        x = up_block(x, f, f.shape[-1].value, 0.5)\n    x = Conv2D(1, (1, 1), padding='same', activation='sigmoid', name='mask') (x)\n    return Model(img_input, x)","eee73f76":"#model = Model(input_layer, output_layer)\nmodel = ResnetUnet()\nmodel ","cc79412f":"optimizer = KO.Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n#optimizer = KO.SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n#optimizer = KO.RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","6e8888a9":"model.summary()","a7056b25":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","54f3bafa":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)\/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)\/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","d444758c":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\".\/keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000005, verbose=1)\n\nepochs = 200\nbatch_size = 8\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])","dffa4a85":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\nax_acc.legend()","3565aa5e":"model = load_model(\".\/keras.model\")","3e5d05ff":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\nmask_valid = np.array([downsample(x) for x in y_valid])","2e73de26":"# src: https:\/\/www.kaggle.com\/aglotero\/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection \/ union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp \/ (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","b77030f8":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(mask_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","e9be749c":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","3c1e6859":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","e3787c8a":"max_images = 30\ngrid_width = 5\ngrid_height = int(max_images \/ grid_width)*3\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*4, grid_height*4))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    #print(idx)\n    img = downsample(np.squeeze(x_valid[i]))\n    mask = np.squeeze(mask_valid[i])\n    pred = np.squeeze(preds_valid[i]>threshold_best)\n    ax_image = axs[int(i \/ grid_width)*3, i % grid_width]\n    ax_image.imshow(img, cmap=\"Greys\")\n    ax_image.set_title(\"Image {0}\\nDepth: {1}\".format(idx, train_df.loc[idx].z))\n    ax_image.set_yticklabels([])\n    ax_image.set_xticklabels([])\n    ax_mask = axs[int(i \/ grid_width)*3+1, i % grid_width]\n    ax_mask.imshow(img, cmap=\"Greys\")\n    ax_mask.imshow(mask, alpha=0.9, cmap=\"Greens\")\n    ax_mask.set_title(\"Mask {0}\\nCoverage: {1}\".format(idx,  round(train_df.loc[idx].coverage, 2)))\n    ax_mask.set_yticklabels([])\n    ax_mask.set_xticklabels([])\n    ax_pred = axs[int(i \/ grid_width)*3+2, i % grid_width]\n    ax_pred.imshow(img, cmap=\"Greys\")\n    ax_pred.imshow(pred, alpha=0.9, cmap=\"Blues\")\n    coverage_pred = np.sum(pred) \/ pow(img_size_ori, 2)\n    ax_pred.set_title(\"Predict {0}\\nCoverage: {1}\".format(idx,  round(coverage_pred, 2)))\n    ax_pred.set_yticklabels([])\n    ax_pred.set_xticklabels([])","09543ce4":"# plot small charts\nmax_images = 24\ngrid_width = 12\ngrid_height = int(max_images \/ grid_width)*3\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    #print(idx)\n    img = downsample(np.squeeze(x_valid[i]))\n    mask = np.squeeze(mask_valid[i])\n    pred = np.squeeze(preds_valid[i]>threshold_best)\n    ax_image = axs[int(i \/ grid_width)*3, i % grid_width]\n    ax_image.imshow(img, cmap=\"Greys\")\n    ax_image.set_title(\"Image\")\n    ax_image.set_yticklabels([])\n    ax_image.set_xticklabels([])\n    ax_mask = axs[int(i \/ grid_width)*3+1, i % grid_width]\n    ax_mask.imshow(img, cmap=\"Greys\")\n    ax_mask.imshow(mask, alpha=0.9, cmap=\"Greens\")\n    ax_mask.set_title(\"Mask\")\n    ax_mask.set_yticklabels([])\n    ax_mask.set_xticklabels([])\n    ax_pred = axs[int(i \/ grid_width)*3+2, i % grid_width]\n    ax_pred.imshow(img, cmap=\"Greys\")\n    ax_pred.imshow(pred, alpha=0.9, cmap=\"Blues\")\n    coverage_pred = np.sum(pred) \/ pow(img_size_ori, 2)\n    ax_pred.set_title(\"Predict\")\n    ax_pred.set_yticklabels([])\n    ax_pred.set_xticklabels([])","cff7266a":"# Source https:\/\/www.kaggle.com\/bguberfain\/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","a175d924":"x_test = np.array([upsample(np.array(load_img(\"..\/input\/test\/images\/{}.png\".format(idx), grayscale=True))) \/ 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","50003649":"preds_test = model.predict(x_test, verbose=True)","c3fc4582":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","0abbd13a":"import base64\nimport pandas as pd\nfrom IPython.display import HTML\n\ndef create_download_link( df, title = \"Download CSV file\", filename = \"sub.csv\"):\n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\nsub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\nprint('Submission output to: sub-{}.csv'.format(timestamp))\nsub.to_csv(\"sub.csv\")\ncreate_download_link(sub)","42747e98":"# Plotting the depth distributions\nSeparatelty plotting the depth distributions for the training and the testing data.","533b0e4d":"# Training","6d743cee":"# Params and helpers","16a87b11":"# Data augmentation","80c9ab4f":"# Scoring\nScore the model and do a threshold optimization by the best IoU.","0b50af44":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage.","312546da":"# Submission\nLoad, predict and submit the test image predictions.","873a56ac":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions.","2d788731":"# Sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold.","fc65f05e":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255.","9008ab68":"# Create train\/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling.","3389f98c":"# Build model","357517be":"# Show some example images","119dd059":"# Loading of training\/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."}}