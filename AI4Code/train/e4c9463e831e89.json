{"cell_type":{"008ad32e":"code","3dc4b8d7":"code","88ee09c2":"code","2705ee0c":"code","cce8fd80":"code","d25e02fd":"code","dd261b25":"code","a7955ce0":"code","54d76338":"code","77d21df1":"code","d871b11a":"code","f37a996f":"code","116b74a9":"code","a9597cd7":"code","617bd6db":"code","35b1a116":"code","589dedac":"code","09eccc11":"code","c0487744":"code","65a49d68":"code","dff5e1c1":"code","0577d95c":"code","74a8b03a":"code","a88c7af2":"code","22aa8fb6":"code","225c6c2d":"code","29d7c7c0":"code","cf31c160":"code","bd20587f":"code","ae977189":"code","080ab89d":"code","9021b8be":"code","8dd3020b":"code","ae23f739":"code","f607f03d":"code","1a67ef1b":"code","d898647d":"code","5e9a0abf":"markdown","5d84f466":"markdown","e8d414d6":"markdown","f6d6dc4e":"markdown","bcb8d661":"markdown","96fdb83d":"markdown","2aab6665":"markdown","0c009554":"markdown","be0f3380":"markdown","16ea9727":"markdown","ca4523e1":"markdown","8a00d09d":"markdown","903602e6":"markdown","37b2010b":"markdown","c17df545":"markdown","7ceb16c8":"markdown","d78ead3c":"markdown","b0dba326":"markdown","b8acf545":"markdown","819cf99e":"markdown","eff2cdf6":"markdown","5cf58466":"markdown","37c2fc74":"markdown","aed165a8":"markdown","d3dbe626":"markdown","0b758b3b":"markdown","f022301e":"markdown","3bfe4f68":"markdown","510d505b":"markdown","eb9c9c5b":"markdown","6869f55f":"markdown","727f06a6":"markdown","8397ba56":"markdown"},"source":{"008ad32e":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nprint(os.listdir(\"..\/input\"))\n","3dc4b8d7":"# Training data\ndata = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")\ndata.head()","88ee09c2":"print(\"Number of rows: \", data.shape[0])\nprint(\"Number of columns: \", data.shape[1])","2705ee0c":"sns.distplot(data['Chance of Admit '] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data['Chance of Admit '])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Chance of Admit distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(data['Chance of Admit '], plot=plt)\nplt.show()","cce8fd80":"print(\"Total Characteristics with NaN values = \" + str(data.columns[data.isnull().sum() != 0].size))\nif (data.columns[data.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(data.columns[data.isnull().sum() != 0])))\n    train_df[data.columns[train.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","d25e02fd":"# Extract the EXT_SOURCE variables and show correlations\nExtractData= data\nExtractDataCorrs = ExtractData.corr()\nExtractDataCorrs","dd261b25":"plt.figure(figsize = (8, 6))\n# Heatmap of correlations\nsns.heatmap(ExtractDataCorrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","a7955ce0":"data.drop([\"Serial No.\"],axis=1,inplace = True)","54d76338":"data.head()","77d21df1":"data=data.rename(columns = {'Chance of Admit ':'Chance of Admit'})","d871b11a":"# we decided our data\ny = data[\"Chance of Admit\"].values\nx = data.drop([\"Chance of Admit\"],axis=1)","f37a996f":"x.head()","116b74a9":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx[['GRE Score', 'TOEFL Score', 'University Rating','SOP',\n       'LOR ', 'CGPA', 'Research']]=scaler.fit_transform(data[['GRE Score', 'TOEFL Score', 'University Rating','SOP',\n       'LOR ', 'CGPA', 'Research']])","a9597cd7":"x.head()","617bd6db":"# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)","35b1a116":"#Validation function\nn_folds = 10\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","589dedac":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'alpha':[10,15,25,30,50,100,150,200,500,600,700,750,785,790,800,900,1000,1001]}\n     \nRGD =  Ridge(solver='lsqr',fit_intercept=False)\ngrid_search = GridSearchCV(RGD, parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\ngrid_search.fit( X_train, y_train )  \ngrid_search.best_params_, grid_search.best_score_  ","09eccc11":"model_rigd =  Ridge(solver='lsqr',fit_intercept=False,alpha=790)\nscore = rmsle_cv(model_rigd)\nprint(\"Rigde score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c0487744":"from sklearn.linear_model import  Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'alpha':[0.0005,0.0006,0.06,0.5,0.0001,0.01,1,2,3,4,4.4,4]}\n\nlasso_grid = Lasso(random_state=1)\ngrid_search_lasso = GridSearchCV(lasso_grid, parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\ngrid_search_lasso.fit(X_train, y_train )  \ngrid_search_lasso.best_params_, grid_search_lasso.best_score_  ","65a49d68":"model_lasso =  Lasso(random_state=1,alpha=0.0006)\nscore = rmsle_cv(model_lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","dff5e1c1":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'alpha':[0.0005,0.0006,0.06,0.5,0.0001,0.01,1,2,3,4,4.4,4]}\n\nENet_grid = ElasticNet(random_state=3,l1_ratio=.9)\ngrid_search_Elastic = GridSearchCV(ENet_grid, parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\ngrid_search_Elastic.fit(X_train, y_train )  \ngrid_search_Elastic.best_params_, grid_search_Elastic.best_score_  ","0577d95c":"model_ENet =  ElasticNet(random_state=3,l1_ratio=.9,alpha= 0.0006)\nscore = rmsle_cv(model_ENet)\nprint(\"Elastic Net Regression score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","74a8b03a":"import xgboost as xgb\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","a88c7af2":"score = rmsle_cv(model_xgb)\nprint(\"Model xgb score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","22aa8fb6":"from sklearn.linear_model import LinearRegression\n\nmodel_lr = LinearRegression()","225c6c2d":"score = rmsle_cv(model_lr)\nprint(\"Linear Regression  models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","29d7c7c0":"from sklearn.ensemble import GradientBoostingRegressor\n\nmodel_gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","cf31c160":"score = rmsle_cv(model_gboost)\nprint(\"Models Gradient increase regression score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","bd20587f":"import lightgbm as lgb\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","ae977189":"score = rmsle_cv(model_lgb)\nprint(\"Models LightGBM score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","080ab89d":"model_lasso_fit = Lasso(random_state=1,alpha=0.0006).fit(X_train, y_train)","9021b8be":"coefficients = pd.Series(model_lasso_fit.coef_, index = X_train.columns)","8dd3020b":"print(\"Lasso picked \" + str(sum(coefficients != 0)) + \" variables and eliminated the other \" +  str(sum(coefficients == 0)) + \" variables\")","ae23f739":"imp_coef = pd.concat([coefficients.sort_values().head(7)])\n\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","f607f03d":"pred = model_lasso_fit.predict(X_test)","1a67ef1b":"predictions = pd.DataFrame({\"Predictions\":pred, \"Real value\": y_test})\n","d898647d":"predictions.round(2).head()","5e9a0abf":"# Models","5d84f466":"# Lost data","e8d414d6":"# Elimination of characteristics","f6d6dc4e":"# Data correlation","bcb8d661":"**RMSE:**","96fdb83d":"## Grid Rigde","2aab6665":"# Importance of characteristics","0c009554":"**RMSE:**","be0f3380":"## LASSO Regression","16ea9727":"## Gradient increase regression","ca4523e1":"## Variable Target","8a00d09d":"# Define a cross validation strategy\n\nwe will use the function ** cross_val_score ** of Sklearn. However, this function does not have a random attribute, so we added a line of code, to shuffle the data set before the cross-validation\n","903602e6":"**We will choose the Lasso model to make our predictions, we will also see the importance of our characteristics.\nFirst we will adjust it with our training data**","37b2010b":"## Linear Regression ","c17df545":"**RMSE:**","7ceb16c8":"## LightGBM","d78ead3c":"**We can see that the target variable is slightly to the right.**","b0dba326":"## XGBoost","b8acf545":"**RMSE: **","819cf99e":"# Read in data","eff2cdf6":"**RMSE: **","5cf58466":"# Standardization","37c2fc74":"# Other models: ","aed165a8":"**RMSE:**","d3dbe626":"# Small exploratory analysis of the data","0b758b3b":"Examine the number of rows and columns: ","f022301e":"**Note**\nEliminating the SOP feature, we will see a slight improvement in the model, but it is too little, so we will leave with the model we have","3bfe4f68":"**RMSE:**","510d505b":"Before adjusting our models we make a search of the best parameters for each one using GridSearchCV and then we will see what is your RMSE","eb9c9c5b":"# Data\n\n**Context**\nThis dataset is created for prediction of Graduate Admissions from an Indian perspective.\n\n**Content**\nThe dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are : \n1. GRE Scores ( out of 340 ) \n2. TOEFL Scores ( out of 120 ) \n3. University Rating ( out of 5 ) \n4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) \n5. Undergraduate GPA ( out of 10 ) \n6. Research Experience ( either 0 or 1 ) \n7. Chance of Admit ( ranging from 0 to 1 )","6869f55f":"# Predictions","727f06a6":"** Chance of Admit ** is the variable we need to predict. So, first let's make an analysis about this variable.","8397ba56":"## Elastic Net Regression"}}