{"cell_type":{"a5a79dc9":"code","ee78f9bf":"code","a1472d72":"code","ec6e9773":"code","50de8602":"code","e18aabb2":"code","b450df72":"code","fab15224":"code","73341d48":"code","1e7ad493":"code","819697c2":"code","067fa152":"code","b21db06e":"code","20a0c898":"code","72b2833b":"code","262bf0fa":"markdown","8ce62cec":"markdown","06820d02":"markdown","117e61eb":"markdown","8154ed42":"markdown","97f49fff":"markdown","058d671b":"markdown","55b44c64":"markdown","0b3378de":"markdown","0d2d1df2":"markdown"},"source":{"a5a79dc9":"!pip install --upgrade seaborn","ee78f9bf":"# system\nimport os, time, datetime\n# data structure\nimport pandas as pd\nimport numpy as np\n\n# math\nfrom scipy import stats\n\n# model\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.python.keras.utils.data_utils import Sequence\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import manifold, datasets\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\n# utilities\nfrom collections import OrderedDict\nfrom functools import partial\nfrom time import time\nimport warnings\nwarnings.simplefilter(\"ignore\")\n","a1472d72":"sns.__version__","ec6e9773":"root_dir = '..\/input\/lish-moa\/'\nos.listdir(root_dir)","50de8602":"train_features_dir = root_dir + 'train_features.csv'\ntrain_targets_dir = root_dir + 'train_targets_scored.csv'\ntest_features_dir = root_dir + 'test_features.csv'\ntrain_features = pd.read_csv(train_features_dir)\ntrain_targets = pd.read_csv(train_targets_dir).drop(columns = 'sig_id')\ntest_features = pd.read_csv(test_features_dir)\ntest_id = test_features['sig_id']","e18aabb2":"def preprocess(df):\n#     df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 1, 'ctl_vehicle': 0})\n#     df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n#     df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72:2})\n    del df['sig_id']\n    return df\ntrain_features = preprocess(train_features)\ntest_features = preprocess(test_features)","b450df72":"feature_names = list(train_features.columns)\ntarget_names = list(train_targets.columns)","fab15224":"MoA_sum = train_targets.sum().to_frame().reset_index(drop=False).rename(columns={\"index\": \"MoA\", 0: \"sum\"}).sort_values(ascending = False, by= 'sum')\n\nfig, ax = plt.subplots()\nplt.barh(MoA_sum.head(20)['MoA'], MoA_sum.head(20)['sum'])\nplt.gca().invert_yaxis()\nplt.title('The count of MoAs')\nplt.show()\nMoA_sum.head(20)","73341d48":"pie_data = train_features[['cp_type', 'cp_time', 'cp_dose']].astype(str)\npie_data.insert(3, 'count', 1)\npie_cp_type = pie_data.groupby(by = ['cp_type']).sum().reset_index().sort_values(by = ['cp_type'])\npie_cp_time = pie_data.groupby(by = ['cp_type', 'cp_time']).sum().reset_index().sort_values(by = ['cp_type', 'cp_time'])\npie_cp_dose = pie_data.groupby(by = ['cp_type', 'cp_time', 'cp_dose']).sum().reset_index().sort_values(by = ['cp_type', 'cp_time', 'cp_dose'])\npie_cp_dose","1e7ad493":"fig, ax = plt.subplots(figsize = (6,6))\nplt.pie(labels = pie_cp_type['cp_type'], x = pie_cp_type['count'], radius = 1.2, labeldistance=0.8, wedgeprops=dict(width=0.3, edgecolor='w'))\nplt.pie(labels = pie_cp_time['cp_time'], x = pie_cp_time['count'], radius = 0.9, labeldistance=0.8, wedgeprops=dict(width=0.3, edgecolor='w'))\nplt.pie(labels = pie_cp_dose['cp_dose'], x = pie_cp_dose['count'], radius = 0.6, labeldistance=0.8, wedgeprops=dict(width=0.3, edgecolor='w'))\nax.set(title='MoA datapoint counting grouped by `cp_type`, `cp_time`, `cp_dose`')\nplt.show()","819697c2":"tag = pd.DataFrame(train_features['cp_type'])\n\ncp_dose_time = train_features['cp_dose'].astype(str) + \" \" + train_features['cp_time'].astype(str) + \"hrs \" \ntag['cp_dose_time'] = cp_dose_time\ncp_dose_time = list(cp_dose_time.drop_duplicates().sort_values())\nfeature = train_features.drop(columns = ['cp_type','cp_time', 'cp_dose'])\n","067fa152":"MoA_count = train_targets.sum(axis = 1)\nsns.countplot(MoA_count)\nplt.title('datapoint counting of concurrent MoA targets')\nplt.show()","b21db06e":"MoA_concurrent = train_targets.multiply(MoA_count, axis = 0)\nMoA_concurrent = MoA_concurrent.reset_index().melt(id_vars=['index'], value_vars=list(train_targets.columns)).drop(columns = 'index')\nMoA_concurrent.columns = ['MoA', 'concurrent_level']\nMoA_concurrent = MoA_concurrent[MoA_concurrent['concurrent_level'] >=0]\nMoA_concurrent.insert(2, 'count', 1)\nMoA_concurrent = MoA_concurrent.groupby(['MoA', 'concurrent_level']).sum('count').reset_index(drop= False)\nMoA_concurrent = MoA_concurrent.pivot(index='concurrent_level', columns='MoA', values='count').fillna(0)\nMoA_concurrent.astype(bool).sum(axis=1).plot.bar(label = 'count', title = 'number of target variable by concurrent level')\nplt.show()","20a0c898":"kstest_result = np.ones((len(cp_dose_time)), dtype = float)\nkstest_result_m = np.ones((feature.shape[1]), dtype = float)\nt0 = time()\n\nfor i in np.arange(feature.shape[1]):\n    tmp = pd.concat([tag, feature.iloc[:,i]], axis = 1).sort_values('cp_dose_time')\n    for j in np.arange(len(cp_dose_time)):\n        tmp_dose_time = tmp[tmp['cp_dose_time'] == cp_dose_time[j]]\n        tmp_dose_time_trt_cp = np.array(tmp_dose_time[tmp_dose_time['cp_type'] == 'trt_cp'].iloc[:,2])\n        tmp_dose_time_ctl_vehicle = np.array(tmp_dose_time[tmp_dose_time['cp_type'] == 'ctl_vehicle'].iloc[:,2])\n        kstest_result[j] = stats.ks_2samp(tmp_dose_time_trt_cp, tmp_dose_time_ctl_vehicle).pvalue\n    kstest_result_m[i] = kstest_result.max()\n#     if i >= 100:\n#         break\npval_rank = list(pd.DataFrame(kstest_result_m, columns = ['ks_pval']).reset_index(drop = False).sort_values(by = 'ks_pval')['index'])\nprint('time spend:', time() - t0)","72b2833b":"def facetgrid_two_axes(*args, **kwargs):\n    data = kwargs.pop('data')\n    dual_axis = kwargs.pop('dual_axis')\n\n    ax = plt.gca()\n    ax.set_ylabel('Count of MoA')\n    \n    sns.scatterplot(tmp.iloc[:,2],tmp.iloc[:,3],\n                    alpha=0.01)\n\n    ax.set_ylabel('MoA Count')\n\n    ax2 = ax.twinx()\n    ax2.set_ylabel('freq')\n    sns.kdeplot(data.iloc[:,2], fill = True, hue = data.iloc[:,0], \n                common_norm=False,  legend=False,\n                palette=['dodgerblue','coral'],\n                alpha=0.5)   \n    \nfor i in pval_rank:\n    tmp = pd.concat([tag, feature.iloc[:,i], MoA_count], axis = 1).sort_values('cp_dose_time')\n    g = sns.FacetGrid(tmp, col='cp_dose_time') \n    g.map_dataframe(facetgrid_two_axes, dual_axis=True)\n    g.fig.suptitle('Distribution of Var: ['+ feature.iloc[:,i].name + \"] (blue: trt_cp, red: ctl_vehicle) with MoA targets counting\", y= 1.1)\n    plt.show()\n#     if i >= 100:\n#         break","262bf0fa":"#  Outliers are Target = 1!","8ce62cec":"## binary encoding","06820d02":"# Now, you see it.\n- The distributions of feature variables are plot in a increasing order goodness of fit between [cp_type] \n- Let's look at distribution of cp_type = ctl_vehicle. the distribution of all cp_dose_time combination are consistent for almost all feature variables, with a few exceptions. \n- Let's look at distribution of cp_type = trt_cp. the distribution of most of feature variable has higher shift with D2 than with D1. Especially, when the peak of trt_cp distribution is much lower than that of ctl_vehicle, there are more outliers exist.\n- **Importantly, the outlier datapoint have higher chances to have greater MoA concurrent.** for exmaple the feature [g-100] at the 6st row below.\n- I guess now you have some ideas for feature engineering this dataset... \n- Hope you will find some magic features. Good luck~","117e61eb":"# Ranking varibles by goodness of fit\n- By considering we might be able to encode the outlier in a distribution, I test the distribution similarity between cp_type in 6 subgroups of combination of cp_time and cp_dose using two samples Kolmogorov-Smirnov test for goodness of fit.\n- A rank of feature variable is created by the mean of p-value from the Kolmogorov-Smirnov test.\n- The rank will be used in the visualization in the next section.","8154ed42":"# The count of MoAs","97f49fff":"# The next step for feature engineering:  \n- starting from the 7-level concurrent target variables, \n- mark an MoA target as scatter points at the above distribution plot. \n- Then look for the feature variables that the outliers are the farthest from the trt_verticle mean with t-test.\n- encode feature variables by thresholds. the threshold is determined by a 95% confidence boundary trt_verticle distribution (assuming to be t-distribution).\n- welcome to folk this kernel for your own experiment. Hope you enjoy this one~","058d671b":"# read file","55b44c64":"# datapoint and MoA counting by concurrent level\n- many datapoint cover the single MoA Activation, but only ~1500 datapoint for 2ed level concurrent \n- might be luck, 50% of MoA has 2ed level concurrent.\n- So, if we can find out some relationship between the 2+ level concurrent and the trt_cp distribution, then we might be able to make some feature engineeing on this dataset. ","0b3378de":"# Preprocess","0d2d1df2":"# The experiment setup of this MoA dataset\n- From the following table and pie chart, we can see the numbers of datapoint in each 'cp_time'-'cp_dose' grouping is similar in each 'cp_type'."}}