{"cell_type":{"e3ee5f6b":"code","01b851be":"code","5d153cc6":"code","ff84ba75":"code","2f521724":"code","2071e35a":"code","9c20171c":"code","aff90b04":"code","914e514a":"code","cdfd9f1c":"code","b0ebd827":"code","36013c05":"code","255f32ef":"code","91e1ba19":"code","eb602a37":"code","0c00f6d5":"code","148f408c":"code","a7a04baa":"code","c46d9f23":"code","25deb8cf":"markdown","aae5a904":"markdown","9afc6de3":"markdown","27b63f75":"markdown"},"source":{"e3ee5f6b":"import os\nimport sys\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom math import ceil\n\nimport warnings\nwarnings.filterwarnings('ignore')","01b851be":"def get_graph_features(G, all_embds, nmax=15):\n\n    n = len(G.nodes())\n    \n    node2id = {node:i for i, node in enumerate(G.nodes())}\n    id2node = {i:node for node,i in node2id.items()}\n\n    adj = np.zeros((nmax,nmax))\n    embds = np.zeros((nmax, all_embds.shape[1]))\n\n    for i in G.nodes():\n        embds[node2id[i]] = all_embds[i]\n        for j in G.neighbors(i):\n            adj[node2id[j],node2id[i]] = 1\n    \n    return adj, embds","5d153cc6":"def simulate_graph(nb_samples=50, vocab_size=15, graph_length=10, \n                   word_embed_dim=3, nb_classes=5, probas = None, \n                   centers=None, random_state=None):\n    \n    np.random.seed(random_state)\n    \n    graphs = []\n    \n    y = np.zeros(nb_samples)\n    \n    for i in range(nb_samples):\n        \n        p = np.random.uniform()\n        \n        cat = int(p*nb_classes)\n        \n        y[i] = cat\n        \n        if probas is None:\n        \n            G = nx.binomial_graph(graph_length, p)\n        else:\n            \n            G = nx.binomial_graph(graph_length, probas[cat])\n        \n        new_nodes = np.random.randint(vocab_size*cat,vocab_size*(cat+1), graph_length).tolist()\n        \n        mapping  = dict(zip(G.nodes(), new_nodes))\n        \n        G = nx.relabel_nodes(G,mapping)\n        \n        graphs.append(G)\n\n    try:\n        embds = np.vstack((c + np.random.normal(size = (vocab_size, word_embed_dim))  for c in centers))\n    except:\n        embds = np.random.normal(size = (nb_classes *  vocab_size, word_embed_dim))\n    \n    return embds, graphs, y","ff84ba75":"def one_hot_encode(y):\n    mods = len(np.unique(y))\n    y_enc = np.zeros((y.shape[0], mods))\n    \n    for i in range(y.shape[0]):\n        y_enc[i, y[i]] = 1\n    return y_enc","2f521724":"import tensorflow as tf","2071e35a":"class GCN():\n    \n    def __init__(self, node_dim=2, graph_dim=2, nb_classes=2, nmax=15, alpha=0.025):\n        \"\"\"\n        Parameters of the model architecture\n        \n        \"\"\"\n        self.node_dim = node_dim\n        self.graph_dim = graph_dim\n        self.nb_classes = nb_classes\n        self.nmax = nmax\n        self.alpha = alpha\n        \n        self.build_model()\n        \n    def build_model(self):\n        self.adjs = tf.placeholder(tf.float32, shape=[None, self.nmax, self.nmax])\n        self.embeddings = tf.placeholder(tf.float32, shape=[None, self.nmax, self.node_dim])\n        self.targets = tf.placeholder(tf.float32, shape=[None, self.nb_classes])\n        \n        A1 = tf.Variable(tf.random_normal([self.graph_dim, self.node_dim], seed=None))\n        B1 = tf.Variable(tf.random_normal([self.graph_dim, self.node_dim]))\n        W  = tf.Variable(tf.random_normal([self.graph_dim, self.nb_classes]))\n        \n        M1 = tf.einsum('adc,adb->abc', self.embeddings, self.adjs)\n        H1 = tf.nn.relu(tf.tensordot(M1, A1, (2, 1)) + tf.tensordot(self.embeddings, B1, (2, 1)))\n        G1 = tf.reduce_mean(H1, 1)\n        \n        Y_OUT = tf.matmul(G1,W)\n        cost = tf.losses.softmax_cross_entropy(self.targets, Y_OUT)\n        \n        self.predictions = tf.argmax(Y_OUT, 1)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.alpha)\n        self.train = optimizer.minimize(cost)\n        \n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        \n    def fit(self, adj, embds, y, epochs=20, batch_size=10, shuffle=True):\n        self.scores = []\n        y_enc = one_hot_encode(y)\n        minibatches = ceil(len(adj) \/ batch_size)\n        \n        j = 0\n        for i in range(epochs):\n            INDS = np.array(range(len(adj)))\n            \n            if shuffle:\n                idx = np.random.permutation(y.shape[0]) \n                INDS = INDS[idx]\n                \n            mini = np.array_split(INDS, minibatches)\n            \n            for inds in mini:\n                j+=1\n                sys.stderr.write('\\rEpoch: %d\/%d' % (j, epochs*minibatches))\n                sys.stderr.flush()\n                self.sess.run(self.train, feed_dict={self.adjs:adj[inds], self.embeddings:embds[inds], \n                                                self.targets:y_enc[inds]})\n                #\n            self.scores.append(self.score(adj, embds, y))\n            #\n    def predict(self, adj, embds):\n        return self.sess.run(self.predictions, feed_dict={self.adjs:adj, self.embeddings:embds})\n    \n    def score(self, adj, embds,y):\n        y_ = self.predict(adj, embds)\n        return 100*(y==y_).mean()\n        \n        ","9c20171c":"WORD_EMBED_DIM = 5\nNB_SAMPLES = 10000\nVOCAB_SIZE = 15\nMAX_LENGTH = 10\nNB_CLASSES = 3\nPROBAS = [0.3, 0.4, 0.55]\nCENTERS  =[0.1, 0.15, 0.2]\nSHARE = .75\nGRAPH_DIM = 10","aff90b04":"embds, graphs, y = simulate_graph(NB_SAMPLES, VOCAB_SIZE, MAX_LENGTH,\n                                                    WORD_EMBED_DIM, NB_CLASSES, \n                                                    PROBAS, CENTERS, random_state=123)\ny = y.astype(int)\n\nAdjs, Ids = [], []\n\nfor graph in graphs:\n    adj, embds_g = get_graph_features(graph, embds, nmax=VOCAB_SIZE)\n    Adjs.append(adj)\n    Ids.append(embds_g)","914e514a":"ADJ = np.array(Adjs)\nID = np.array(Ids)\n\nCUT = int(NB_SAMPLES * SHARE)\nADJ_train, y_train, ADJ_test, y_test = ADJ[:CUT], y[:CUT], ADJ[CUT:], y[CUT:]\nID_train, ID_test = ID[:CUT], ID[CUT:]","cdfd9f1c":"gcn_tf = GCN(node_dim=WORD_EMBED_DIM, graph_dim=GRAPH_DIM, nb_classes=NB_CLASSES, \n             nmax=VOCAB_SIZE, alpha=0.025)","b0ebd827":"gcn_tf.fit(ADJ_train, ID_train, y_train, epochs=15, batch_size=32)","36013c05":"gcn_tf.score(ADJ_train, ID_train, y_train)","255f32ef":"gcn_tf.score(ADJ_test, ID_test, y_test)","91e1ba19":"class MLGCN():\n    \n    def __init__(self, node_dim=2, graph_dim=[3,3], nb_classes=2, nmax=15, alpha=0.025):\n        \"\"\"\n        Parameters of the model architecture\n        \n        \"\"\"\n        self.graph_dims = [node_dim] + graph_dim\n        self.n_layers = len(graph_dim)\n        self.nb_classes = nb_classes\n        self.nmax = nmax\n        self.alpha = alpha\n        \n        self.build_model()\n        \n    def build_model(self):\n        self.adjs = tf.placeholder(tf.float32, shape=[None, self.nmax, self.nmax])\n        self.targets = tf.placeholder(tf.float32, shape=[None, self.nb_classes])\n        \n        self.A = {i+1: tf.Variable(tf.random_normal([self.graph_dims[i+1], self.graph_dims[i]])) \\\n             for i in range(self.n_layers)}\n        self.B = {i+1: tf.Variable(tf.random_normal([self.graph_dims[i+1], self.graph_dims[i]])) \\\n             for i in range(self.n_layers)}\n        self.W  = tf.Variable(tf.random_normal([self.graph_dims[-1], self.nb_classes]))\n        \n        \n        self.M, self.H, self.G = {}, {}, {}\n        \n        self.H[0] = tf.placeholder(tf.float32, shape=[None, self.nmax, self.graph_dims[0]])\n        \n        for i in range(1, self.n_layers+1):\n        \n            self.M[i] = tf.einsum('adc,adb->abc', self.H[i-1], self.adjs)\n            self.H[i] = tf.nn.relu(tf.tensordot(self.M[i], self.A[i], (2, 1)) \n                                   + tf.tensordot(self.H[i-1], self.B[i], (2, 1)))\n            self.G[i] = tf.reduce_mean(self.H[i], 1)\n        \n        Y_OUT = tf.matmul(self.G[self.n_layers], self.W)\n        cost = tf.losses.softmax_cross_entropy(self.targets, Y_OUT)\n        \n        self.predictions = tf.argmax(Y_OUT, 1)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.alpha)\n        self.train = optimizer.minimize(cost)\n        \n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        \n    def fit(self, adj, embds, y, epochs=20, batch_size=10, shuffle=True):\n        self.scores = []\n        minibatches = ceil(len(adj) \/ batch_size)\n        \n        y_enc = one_hot_encode(y)\n        \n        j = 0\n        for i in range(epochs):\n            INDS = np.array(range(len(adj)))\n            \n            if shuffle:\n                idx = np.random.permutation(y.shape[0]) \n                INDS = INDS[idx]\n                \n            mini = np.array_split(INDS, minibatches)\n            \n            for inds in mini:\n                j+=1\n                sys.stderr.write('\\rEpoch: %d\/%d' % (j, epochs*minibatches))\n                sys.stderr.flush()\n                self.sess.run(self.train, feed_dict={self.adjs:adj[inds], self.H[0]:embds[inds], \n                                                self.targets:y_enc[inds]})\n                \n            self.scores.append(self.score(adj, embds, y))\n            \n        \n        \n    def predict(self, adj, embds):\n        return self.sess.run(self.predictions, feed_dict={self.adjs:adj, self.H[0]:embds})\n    \n    def score(self, adj, embds,y):\n        y_ = self.predict(adj, embds)\n        return 100*(y==y_).mean()\n        \n        ","eb602a37":"mlgcn_tf = MLGCN(node_dim=WORD_EMBED_DIM, graph_dim=[10,10], nb_classes=NB_CLASSES, \n             nmax=VOCAB_SIZE, alpha=0.025)","0c00f6d5":"mlgcn_tf.fit(ADJ_train, ID_train, y_train, epochs=15, batch_size=32)","148f408c":"mlgcn_tf.score(ADJ_train, ID_train, y_train)","a7a04baa":"mlgcn_tf.score(ADJ_test, ID_test, y_test)","c46d9f23":"plt.plot(gcn_tf.scores, label = 'one-layer')\nplt.plot(mlgcn_tf.scores, label = 'multi-layer')\nplt.title('accuracy')\nplt.xlabel('epochs')\nplt.legend(loc='best')\nplt.show()","25deb8cf":"## Helper functions","aae5a904":"## MLGCN","9afc6de3":"## TOY EXAMPLE WITH SIMULATED COMMUNITIES","27b63f75":"## One Layer GCN"}}