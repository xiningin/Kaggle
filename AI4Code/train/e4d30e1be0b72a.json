{"cell_type":{"6756639e":"code","91fbc2b5":"code","c449f1cf":"code","84c360a4":"code","5d98dfd8":"code","f36c2bbc":"code","97154963":"code","2720410d":"markdown","9f1020a3":"markdown","1e3deb5a":"markdown","55e4444f":"markdown","74744c71":"markdown","cb9bf1c9":"markdown","d37ffd4d":"markdown","d11a4ec3":"markdown","f501a3af":"markdown"},"source":{"6756639e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91fbc2b5":"import tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nTotalDataset = pd.read_csv(\"\/kaggle\/input\/salary\/Salary.csv\")\nTotalDataset.head()","c449f1cf":"from sklearn.model_selection import train_test_split\n\nX = TotalDataset['YearsExperience']\ny = TotalDataset['Salary']\n\nX_train, X_test, y_train,  y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#Verify the lengths of the training and testing datasets\nlen(X_train), len(y_train), len(X_test), len(y_test)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","84c360a4":"tf.random.set_seed(42)\n\nmodel = tf.keras.Sequential([\n      tf.keras.layers.Dense(32, activation='relu'),\n      tf.keras.layers.Dense(16, activation='relu'),\n      tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss=tf.keras.losses.mae,\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n              metrics=[\"mae\"])\n\n\nhistory = model.fit(tf.expand_dims(X_train,axis=-1), y_train, epochs=270, verbose=0, validation_data=(X_test, y_test))","5d98dfd8":"import pandas as pd\npd.DataFrame(history.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");","f36c2bbc":"pred = model.predict(X_test)\n\n#Lets compare pred and y_test\npred, y_test","97154963":"from sklearn.metrics import mean_squared_error\nimport math\n\nerror = abs(pred.squeeze() - (y_test).squeeze())\n\nresult = pd.DataFrame({'Prediction':np.round(pred.squeeze()), 'Actual Salary':y_test, 'Error':np.round(error)})\nprint(result,\"\\n\")\n\nerrors = math.sqrt(mean_squared_error(y_test, pred))\nprint(f'Mean squared error: {errors}')","2720410d":"## Let's predict the data from the test dataset","9f1020a3":"Now we can see that, out of 35 samples, 28 were being sent to training dataset and remaining 7 samples were sent to testing dataset","1e3deb5a":"## Now, we try to load the CSV file","55e4444f":"## Creating the Neural Network","74744c71":"## Split the total dataset into 80% training and 20% testing","cb9bf1c9":"## Let's Check the loss curve","d37ffd4d":"## Lets check the MSE and Error","d11a4ec3":"## Try out the problem in your stype and make sure this MSE gets reduced.","f501a3af":"## I see that both the loss curves were decreasing, i.e., Our model was learning. \n\n## You can tweek the Neural Network and see how the model was performing.\n\n## Note:\n### Make sure that the model do not get overfit to the training dataset."}}