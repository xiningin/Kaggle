{"cell_type":{"f30e77a0":"code","2c9ce900":"code","cec8748d":"code","9acea746":"code","474c5e25":"code","5d7deb6d":"code","a4d9fb78":"code","a35cf82e":"code","b3d10905":"code","3881679c":"code","8dbf45a0":"code","f063f4a4":"code","1c06dd5a":"code","f78fd4b6":"code","0b46c863":"code","399b749f":"code","04540676":"code","98428c0f":"markdown","07e6d42f":"markdown","1218fa6b":"markdown","cf322f24":"markdown","217a1915":"markdown","b456a8a4":"markdown","2ce30864":"markdown","2c63a4fd":"markdown","1c437dd8":"markdown","7c9b2031":"markdown"},"source":{"f30e77a0":"%%bash\ntree -d \/kaggle\/input\/flowers-recognition\/","2c9ce900":"import torch\nimport torch.nn as nn\nimport torchvision\n\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n)\n\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import (\n    RandomResizedCrop,\n    RandomHorizontalFlip,\n    ColorJitter,\n    RandomGrayscale,\n    RandomApply,\n    Compose,\n    GaussianBlur,\n    ToTensor,\n)\nimport torchvision.models as models\n\nimport os\nimport glob\nimport time\nfrom skimage import io\nimport matplotlib.pyplot as plt","cec8748d":"print(f'Torch-Version {torch.__version__}')\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'DEVICE: {DEVICE}')","9acea746":"def get_complete_transform(output_shape, kernel_size, s=1.0):\n    \"\"\"\n    The color distortion transform.\n    \n    Args:\n        s: Strength parameter.\n    \n    Returns:\n        A color distortion transform.\n    \"\"\"\n    rnd_crop = RandomResizedCrop(output_shape)\n    rnd_flip = RandomHorizontalFlip(p=0.5)\n    \n    color_jitter = ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n    rnd_color_jitter = RandomApply([color_jitter], p=0.8)\n    \n    rnd_gray = RandomGrayscale(p=0.2)\n    gaussian_blur = GaussianBlur(kernel_size=kernel_size)\n    rnd_gaussian_blur = RandomApply([gaussian_blur], p=0.5)\n    to_tensor = ToTensor()\n    image_transform = Compose([\n        to_tensor,\n        rnd_crop,\n        rnd_flip,\n        rnd_color_jitter,\n        rnd_gray,\n        rnd_gaussian_blur,\n    ])\n    return image_transform\n\n\nclass ContrastiveLearningViewGenerator(object):\n    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n\n    def __init__(self, base_transform, n_views=2):\n        self.base_transform = base_transform\n        self.n_views = n_views\n\n    def __call__(self, x):\n        views = [self.base_transform(x) for i in range(self.n_views)]\n        return views","474c5e25":"class CustomDataset(Dataset):\n    \"\"\"Flowers Dataset\"\"\"\n\n    def __init__(self, list_images, transform=None):\n        \"\"\"\n        Args:\n            list_images (list): List of all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.list_images = list_images\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.list_images)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        img_name = self.list_images[idx]\n        image = io.imread(img_name)\n        if self.transform:\n            image = self.transform(image)\n\n        return image","5d7deb6d":"# The size of the images\noutput_shape = [224,224]\nkernel_size = [21,21] # 10% of the output_shape\n\n# The custom transform\nbase_transforms = get_complete_transform(output_shape=output_shape, kernel_size=kernel_size, s=1.0)\ncustom_transform = ContrastiveLearningViewGenerator(base_transform=base_transforms)\n\nflowers_ds = CustomDataset(\n    list_images=glob.glob(\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/*\/*.jpg\"),\n    transform=custom_transform\n)","a4d9fb78":"plt.figure(figsize=(10,20))\ndef view_data(flowers, index):\n    for i in range(1,6):\n        images = flowers[index]\n        view1, view2 = images\n        plt.subplot(5,2,2*i-1)\n        plt.imshow(view1.permute(1,2,0))\n        plt.subplot(5,2,2*i)\n        plt.imshow(view2.permute(1,2,0))\n\nview_data(flowers_ds,2000)","a35cf82e":"BATCH_SIZE = 128\n\n# Building the data loader\ntrain_dl = torch.utils.data.DataLoader(\n    flowers_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=os.cpu_count(),\n    drop_last=True,\n    pin_memory=True,\n)","b3d10905":"class Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, x):\n        return x\n\nclass SimCLR(nn.Module):\n    def __init__(self, linear_eval=False):\n        super().__init__()\n        self.linear_eval = linear_eval\n        resnet18 = models.resnet18(pretrained=False)\n        resnet18.fc = Identity()\n        self.encoder = resnet18\n        self.projection = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256)\n        )\n    def forward(self, x):\n        if not self.linear_eval:\n            x = torch.cat(x, dim=0)\n        \n        encoding = self.encoder(x)\n        projection = self.projection(encoding) \n        return projection","3881679c":"LABELS = torch.cat([torch.arange(BATCH_SIZE) for i in range(2)], dim=0)\nLABELS = (LABELS.unsqueeze(0) == LABELS.unsqueeze(1)).float() # Creates a one-hot with broadcasting\nLABELS = LABELS.to(DEVICE) #128,128\n\ndef cont_loss(features, temp):\n    \"\"\"\n    The NTxent Loss.\n    \n    Args:\n        z1: The projection of the first branch\n        z2: The projeciton of the second branch\n    \n    Returns:\n        the NTxent loss\n    \"\"\"\n    similarity_matrix = torch.matmul(features, features.T) # 128, 128\n    # discard the main diagonal from both: labels and similarities matrix\n    mask = torch.eye(LABELS.shape[0], dtype=torch.bool).to(DEVICE)\n    # ~mask is the negative of the mask\n    # the view is required to bring the matrix back to shape\n    labels = LABELS[~mask].view(LABELS.shape[0], -1) # 128, 127\n    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1) # 128, 127\n\n    # select and combine multiple positives\n    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1) # 128, 1\n\n    # select only the negatives\n    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1) # 128, 126\n\n    logits = torch.cat([positives, negatives], dim=1) # 128, 127\n    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(DEVICE)\n\n    logits = logits \/ temp\n    return logits, labels","8dbf45a0":"simclr_model = SimCLR().to(DEVICE)\ncriterion = nn.CrossEntropyLoss().to(DEVICE)\noptimizer = torch.optim.Adam(simclr_model.parameters())","f063f4a4":"EPOCHS = 10\nfor epoch in range(EPOCHS):\n    t0 = time.time()\n    running_loss = 0.0\n    for i, views in enumerate(train_dl):\n        projections = simclr_model([view.to(DEVICE) for view in views])\n        logits, labels = cont_loss(projections, temp=2)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # print statistics\n        running_loss += loss.item()\n        if i % 10 == 9:    # print every 10 mini-batches\n            print(f'EPOCH: {epoch+1} BATCH: {i+1} LOSS: {(running_loss\/100):.4f} ')\n            running_loss = 0.0\n    print(f'Time taken: {((time.time()-t0)\/60):.3f} mins')","1c06dd5a":"from torchvision.transforms import (\n    CenterCrop,\n    Resize\n)","f78fd4b6":"resize = Resize(255)\nccrop = CenterCrop(224)\nttensor = ToTensor()\n\ncustom_transform = Compose([\n    resize,\n    ccrop,\n    ttensor,\n])\n\nflowers_ds = ImageFolder(\n    root=\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/\",\n    transform=custom_transform\n)\n\nnu_classes = len(flowers_ds.classes)\n\nBATCH_SIZE = 128\n\n# Building the data loader\ntrain_dl = torch.utils.data.DataLoader(\n    flowers_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=os.cpu_count(),\n    drop_last=True,\n    pin_memory=True,\n)","0b46c863":"class Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, x):\n        return x\n\nclass LinearEvaluation(nn.Module):\n    def __init__(self, model, nu_classes):\n        super().__init__()\n        simclr = model\n        simclr.linear_eval = True\n        simclr.projection = Identity()\n        self.simclr = simclr\n        for param in self.simclr.parameters():\n            param.requires_grad = False\n        self.linear = nn.Linear(512, nu_classes)\n    def forward(self, x):\n        encoding = self.simclr(x)\n        pred = self.linear(encoding) \n        return pred","399b749f":"eval_model = LinearEvaluation(simclr_model, nu_classes).to(DEVICE)\ncriterion = nn.CrossEntropyLoss().to(DEVICE)\noptimizer = torch.optim.Adam(eval_model.parameters())","04540676":"EPOCHS = 10\nfor epoch in range(EPOCHS):\n    t0 = time.time()\n    running_loss = 0.0\n    for i, element in enumerate(train_dl):\n        image, label = element\n        image = image.to(DEVICE)\n        label = label.to(DEVICE)\n        pred = eval_model(image)\n        loss = criterion(pred, label)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # print statistics\n        running_loss += loss.item()\n        if i % 10 == 9:    # print every 10 mini-batches\n            print(f'EPOCH: {epoch+1} BATCH: {i+1} LOSS: {(running_loss\/100):.4f} ')\n            running_loss = 0.0\n    print(f'Time taken: {((time.time()-t0)\/60):.3f} mins')","98428c0f":"# Stochastic Augmentation Module\n\nIn this section we will talk about the data pipeline. The authors suggest that a strong data augmentation is useful for unsupervised learning.\nThe following augmentation are suggested by the authors:\n- Random Crop with Resize\n- Random Horizontal Flip with 50% probability\n- Random Color Distortion\n    - Random Color Jitter with 80% probability\n    - Random Color Drop with 20% probability\n- Random Gaussian Blur with 50% probability\n\n<div style=\"text-align:center\">\n    <img src=\"https:\/\/amitness.com\/images\/simclr-random-transformation-function.gif\" width=\"400\" height=\"500\">\n<\/div>\n<p style=\"text-align:center\">Source: <a href=\"https:\/\/amitness.com\/2020\/03\/illustrated-simclr\/\">Amitness' take on SimCLR<\/a><\/p>\n\nThe data pipeline does not take an image and output a single augmented view, but on the contrary, outputs two randomly augmented views of the original image. The reason behind two distinct views will become simpler as we proceed to the next section.\n","07e6d42f":"# SimCLR Model\n\nAs we remember the data pipeline gives us two augmented views of an image. The views go into a neural network encoder $f(.)$ that gives us the corresponding representation of the augmented views. Our objective is to maximise the similarity quotient of the two distinct learned representations. The idea here is to force the model to learn a general representation of an object from two distinct augmented views of it. The intuition is quite similar to viewing an object from different perspectives and gaining a better understanding.\n\nThe authors do not put constraint on the encoder model, here I have used ResNet18 as the data is quite small to use a bigger architecture. The similarity quotient comes from the representations learned by the encoder model. It was seen that projecting the encoded vector into a different latent space by an MLP helped in learning better (richer) representations. Here comes the projector $g(.)$ layer. The only objective of the projector is to project the encoder output to a richer latent space.\n\n$$\nh = f(x)\\\\\nz = g(h)\n$$","1218fa6b":"# Down Stream Task\n\nAfter we have a trained SimCLR model we will discard the projection head and use the encoder only. We provide a `Centre Cropped` image to the encoder and get a learned representation of the images. This representation is then used in down stream tasks like classification.\n\nHere we train the representations to predict the classification task provided with the flowers dataset.","cf322f24":"### Visualise the data pipeline","217a1915":"# Ending note\n\nI have taken help from the following places, I am sure it will help the keen reader.\n- [SimCLR](https:\/\/arxiv.org\/abs\/2002.05709)\n- [Official SimCLR repository](https:\/\/github.com\/google-research\/simclr)\n- [Amit Chaudhary's blog on SimCLR](https:\/\/amitness.com\/2020\/03\/illustrated-simclr\/)\n- [Sayak Paul's blog on SimCLR](https:\/\/wandb.ai\/sayakpaul\/simclr\/reports\/Towards-Self-Supervised-Image-Understanding-with-SimCLR--VmlldzoxMDI5NDM)\n- [@sthalles' SimCLR implementation in PyTorch](https:\/\/github.com\/sthalles\/SimCLR)\n\nI have tried producing a minimal implementation of SimCLR with PyTorch here. This was my first time with PyTorch, feedbacks are always welcome.","b456a8a4":"## Prepaer the Data\nIn PyTorch we have two primitive classes to deal with data:\n- torch.utils.data.DataSet\n- torch.utils.data.DataLoader\n\nThe DataSet class helps in encapsulating the raw data. The DataLoader class helps in iterating over the DataSet that we make.","2ce30864":"# A `Sim`ple framework for `c`ontrastive `l`earning of visual `r`epresentation\n<div style=\"text-align:center\">\n    <img src=\"https:\/\/1.bp.blogspot.com\/--vH4PKpE9Yo\/Xo4a2BYervI\/AAAAAAAAFpM\/vaFDwPXOyAokAC8Xh852DzOgEs22NhbXwCLcBGAsYHQ\/s640\/image4.gif\" width=\"400\" height=\"500\">\n<\/div>\n<p style=\"text-align:center\">Source: <a href=\"https:\/\/ai.googleblog.com\/2020\/04\/advancing-self-supervised-and-semi.html\">Google AI Blog<\/a><\/p>\n\nIn this kernel we will code a minimal implementation of [A Simple Framework for Contrastive Learning of Visual Representations](https:\/\/arxiv.org\/abs\/2002.05709) by Chen et. al. With the analogy of the [Le Cake](https:\/\/youtu.be\/7I0Qt7GALVk?t=2773) Yann LeCun quite warmly directs us into the realms of the unsupervised learning paradigm. The goal of unsupervised learning is to utilise the vast amount of unlabelled data to formalise a representation that helps in learning in general. SimCLR provides a great platform (and an easy one too) to help achieve a good representation out of unlabelled images. The itutions and conjectures provided by the paper comes quite naturally to one's mind. The ease of the concepts will be portrayed in the kernel along with some comments on the same.\n\nSimCLR is based out of the following simplified modules:\n- A stochastic data augmentation module.\n- A neural network base encoder $f(.)$.\n- A neural network projection head $g(.)$\n- A contrastive loss function.\n\n> This will not be one of my reports where I investigate the ins and outs of the paper. I would likely try my hand at PyTorch and the paper both at the same time. Please feel free to fork the kernel or start a discussion in the comments about the ways I can boost the efficiency of the code.","2c63a4fd":"> Here we see a redundant folder `floweres\/flowers`. This is a probelm with the dataset. We will be using the nested flowers directory in the kernel.","1c437dd8":"# Imports\nThe following imports will be used in the kernel.","7c9b2031":"# Contrastive Loss\n\nThe loss function is basically built on top of two things:\n- softmax\n- cosine similarity\n\nWith the projected vectors we perform a cosine similarity function to check how similar they are. We perform the cosine similarity upon both the positive and negative pairs. After we have the similarity matrix we perform a softmax to have a probability distribution of the entire model. Our objective is to tune the knobs of the model so that this softmax distribution is peaked on the positive pair. The paper uses negative log of the softmax,  here we will approach a similar idea and use the cross entropy loss with the softmax similarity distribution. The authors have used the same approach in the [official code](https:\/\/github.com\/google-research\/simclr\/blob\/3ad6700c1b139ee18e43f73546b7263a710de699\/objective.py#L82-L86) too."}}