{"cell_type":{"4d6c5b52":"code","026bfd56":"code","01355269":"code","308e915d":"code","d0e7b692":"code","fd9bef24":"code","faabee4c":"code","b82569cd":"code","e02718be":"code","41410845":"code","c3ce7a4b":"code","342809d4":"code","1c5102d2":"code","e83b928c":"code","717ca53e":"code","b160b537":"code","746917c4":"code","b86512b8":"markdown","5534d128":"markdown","f7481dd5":"markdown","69a6bfdb":"markdown","060608f1":"markdown","69f62a65":"markdown","461b2010":"markdown","5d9cc259":"markdown","dc767902":"markdown","dbb336e5":"markdown"},"source":{"4d6c5b52":"!cat \/proc\/cpuinfo | grep name | cut -f2 -d: | uniq -c\n!nvidia-smi","026bfd56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01355269":"!pip install torchsummary","308e915d":"# Set your own project id here\nPROJECT_ID = 'fcn8_resnet50'\nfrom google.cloud import storage\nstorage_client = storage.Client(project=PROJECT_ID)\ndef upload_files(bucket_name, source_folder):\n bucket = storage_client.get_bucket(bucket_name)\n for filename in os.listdir(source_folder):\n \n  blob = bucket.blob(filename)\n  blob.upload_from_filename(source_folder + filename)","d0e7b692":"!mkdir -p '..\/content\/kgs_data'\n!cp '\/kaggle\/input\/tgs-salt-identification-challenge\/train.zip' -d '..\/content\/kgs_data_train.zip'\n!unzip '..\/content\/kgs_data_train.zip' -d '..\/content\/kgs_data'","fd9bef24":"import numpy as np\nimport os\nimport time \nimport math\nimport glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as T\nfrom torchsummary import summary\n\ndef timeSince(since):\n  now = time.time()\n  s = now - since\n  m = math.floor(s \/ 60)\n  s -= m * 60\n  return '%dm %ds' % (m, s)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"current device is : \", device)","faabee4c":"!ls","b82569cd":"image_path = \"..\/content\/kgs_data\/images\"\nmask_path = \"..\/content\/kgs_data\/masks\"","e02718be":"# \u4ece\u8bad\u7ec3\u96c6\u4e2d\u968f\u610f\u6311\u9009\u51e0\u5f20\u56fe\u7247\u548c\u8499\u7248\uff0c\u8f93\u51fa\u6765\u770b\u770b\nnames = ['000e218f21','41cfd4b320','3c2f5ba174']\nimages = [Image.open(os.path.join(image_path, name+'.png')) for name in names]\nmasks = [Image.open(os.path.join(mask_path, name+'.png')) for name in names]\n\n'''Transform \u7528\u6cd5\ntransform = transforms.Compose([\n    transforms.Grayscale(),  # \u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u7070\u5ea6\u56fe\n    transforms.RandomCrop(32, padding=4),  #\u5148\u56db\u5468\u586b\u51450\uff0c\u5728\u5427\u56fe\u50cf\u968f\u673a\u88c1\u526a\u621032*32\n    transforms.RandomHorizontalFlip(),  #\u56fe\u50cf\u4e00\u534a\u7684\u6982\u7387\u7ffb\u8f6c\uff0c\u4e00\u534a\u7684\u6982\u7387\u4e0d\u7ffb\u8f6c\n    transforms.RandomRotation((-45,45)), #\u968f\u673a\u65cb\u8f6c\n    transforms.ToTensor(),   # \u56fe\u50cf\u8f6ctensor\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.229, 0.224, 0.225)), #R,G,B\u6bcf\u5c42\u7684\u5f52\u4e00\u5316\u7528\u5230\u7684\u5747\u503c\u548c\u65b9\u5dee\n])\n'''\ntransforms1 = T.Resize((56,56),interpolation=Image.NEAREST)\ntransforms2 = T.Compose([T.Grayscale(), T.ToTensor()]) # \u8f6c\u6362\u6a21\u677f\nx = torch.stack([transforms2(transforms1(image)) for image in images])\ny = torch.stack([transforms2(transforms1(mask)) for mask in masks])\nprint(x.size())\nfig = plt.figure( figsize=(9, 9))\n\nax = fig.add_subplot(331)\nplt.imshow(images[0])\nax = fig.add_subplot(332)\nplt.imshow(masks[0])\nax = fig.add_subplot(333)\nax.imshow(x[0].squeeze(), cmap=\"Greys\")\nax.imshow(y[0].squeeze(), alpha=0.5, cmap=\"Greens_r\")\n\nax = fig.add_subplot(334)\nplt.imshow(images[1])\nax = fig.add_subplot(335)\nplt.imshow(masks[1])\nax = fig.add_subplot(336)\nax.imshow(x[1].squeeze(), cmap=\"Greys\")\nax.imshow(y[1].squeeze(), alpha=0.5, cmap=\"Greens_r\")\n\nax = fig.add_subplot(337)\nplt.imshow(images[2])\nax = fig.add_subplot(338)\nplt.imshow(masks[2])\nax = fig.add_subplot(339)\nax.imshow(x[2].squeeze(), cmap=\"Greys\")\nax.imshow(y[2].squeeze(), alpha=0.5, cmap=\"Greens_r\")\n\nplt.show()\n","41410845":"class segmentDataset(Dataset):\n    def __init__(self, image_path, mask_path):\n        self.image_path = image_path\n        self.mask_path = mask_path\n        \n        # \u6839\u636e\u6240\u89c4\u5b9a\u7684pattern\uff0c\u8fd4\u56de\u56fe\u7247\u76ee\u5f55\u7ec4\u6210\u7684list\n        image_list= glob.glob(image_path +'\/*.png')\n        # \u5229\u7528for\u5faa\u73af\u7684\u8bed\u53e5\u83b7\u53d6\u56fe\u50cf\u6587\u4ef6\u7684\u6587\u4ef6\u540d\n        sample_names = []\n        for file in image_list:\n            sample_names.append(file.split('\/')[-1].split('.')[0])\n        self.sample_names = sample_names\n        # \u56fe\u50cf\u7f29\u653e\uff0c\u83b7\u5f97(224, 224)\n        # \u83b7\u53d6\u7070\u5ea6\u56fe + \u5c06\u5176\u8f6c\u5316\u4e3atensor\n        self.transforms = T.Compose([T.Grayscale(), T.ToTensor(), T.Resize((112,112),interpolation=Image.NEAREST)])\n            \n    def __getitem__(self, idx):\n        image = Image.open(os.path.join(self.image_path, self.sample_names[idx]+'.png') )\n        mask = Image.open(os.path.join(self.mask_path, self.sample_names[idx]+'.png') )\n        return self.transforms(image), self.transforms(mask)\n\n    def __len__(self):\n        return len(self.sample_names)\n","c3ce7a4b":"train_dataset = segmentDataset(image_path = image_path, mask_path = mask_path)","342809d4":"import torchvision.models as models\n       \n\n \nclass FCNx8_ResNet(nn.Module):\n    debug_info = True\n    def __init__(self,num_classes = 1):\n        super(FCNx8_ResNet, self).__init__()\n        pretrained_net = models.resnet50(pretrained=True)\n        pretrained_net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        conv_sequential= list(pretrained_net.children())[:-1]\n        summary(pretrained_net.to(device), (1, 56, 56))\n        modules_list = []\n        for i in range(4):\n            modules_list.append(conv_sequential[i])\n        self.head = nn.Sequential(*modules_list)\n \n        modules_list = []\n        for i in range(4,6):\n            temp = list(conv_sequential[i])\n            for j in range(len(temp)):\n                modules_list.append(temp[j])\n        self.stage1 = nn.Sequential(*modules_list)\n \n        modules_list = []\n        temp = list(conv_sequential[6])\n        for j in range(len(temp)):\n            modules_list.append(temp[j])\n        self.stage2 = nn.Sequential(*modules_list)\n \n        modules_list = []\n        temp = list(conv_sequential[7])\n        for j in range(len(temp)):\n            modules_list.append(temp[j])\n        modules_list.append(conv_sequential[8])\n        modules_list.append(nn.Conv2d(in_channels=2048,out_channels=1024,kernel_size=1,stride=1,padding=0))\n        modules_list.append(nn.Conv2d(in_channels=1024,out_channels=512,kernel_size=1,stride=1,padding=0))\n        self.stage3 = nn.Sequential(*modules_list)\n \n        self.scores3 = nn.Conv2d(in_channels=512,out_channels=num_classes,kernel_size=1)\n        self.scores2 = nn.Conv2d(in_channels=1024,out_channels=num_classes,kernel_size=1)\n        self.scores1 = nn.Conv2d(in_channels=512,out_channels=num_classes,kernel_size=1)\n        #\n        # # N=(w-1)xs+k-2p\n        self.upsamplex8 = nn.ConvTranspose2d(in_channels=num_classes,out_channels=num_classes,kernel_size=16,stride=8,padding=4,bias= False)\n        self.upsamplex8.weight.data = self.bilinear_kernel(num_classes,num_classes,16)\n        self.upsamplex16 = nn.ConvTranspose2d(in_channels=num_classes,out_channels=num_classes,kernel_size=4,stride=2,padding=1,bias=False)\n        self.upsamplex16.weight.data = self.bilinear_kernel(num_classes,num_classes,4)\n        self.upsamplex32= nn.ConvTranspose2d(in_channels=num_classes,out_channels=num_classes,kernel_size=5,stride=3,padding=0,bias=False)\n        self.upsamplex32.weight.data = self.bilinear_kernel(num_classes,num_classes,7)\n \n    def forward(self, x):\n        x = self.head(x)\n        x = self.stage1(x)\n        s1 = x\n \n        x = self.stage2(x)\n        s2 = x\n \n        x = self.stage3(x)\n        s3 = x\n        if self.debug_info is True:\n            print(s1.size(),s2.size(),s3.size())\n \n        s3 = self.scores3(s3)\n        s3 = self.upsamplex32(s3)\n \n        s2 = self.scores2(s2)\n        if self.debug_info is True:\n            print(s1.size(),s2.size(),s3.size())\n        s2 = s2 + s3\n        s2 = self.upsamplex16(s2)\n \n        s1 = self.scores1(s1)\n        if self.debug_info is True:\n            print(s1.size(),s2.size(),s3.size())\n        s = s1 + s2\n        s = self.upsamplex8(s)\n         \n        self.debug_info = False\n        return s\n \n    def bilinear_kernel(self,in_channels, out_channels, kernel_size):\n        '''\n        return a bilinear filter tensor\n        '''\n        factor = (kernel_size + 1) \/\/ 2\n        if kernel_size % 2 == 1:\n            center = factor - 1\n        else:\n            center = factor - 0.5\n        og = np.ogrid[:kernel_size, :kernel_size]\n        filt = (1 - abs(og[0] - center) \/ factor) * (1 - abs(og[1] - center) \/ factor)\n        weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype='float32')\n        weight[range(in_channels), range(out_channels), :, :] = filt\n        return torch.from_numpy(weight)\n# Example\nfcn_model = FCNx8_ResNet(num_classes = 1).to(device)\n# print(fcn_model)\nsummary(fcn_model, (1, 112, 112))","1c5102d2":"def get_iou_score(outputs, labels):\n    A = labels.squeeze().bool()\n    pred = torch.where(outputs<0., torch.zeros_like(outputs), torch.ones_like(outputs))\n    B = pred.squeeze().bool()\n    intersection = (A & B).float().sum((1,2))\n    union = (A | B).float().sum((1, 2)) \n    iou = (intersection + 1e-6) \/ (union + 1e-6)  \n    return iou\n  \ndef train_one_batch(model, x, y):\n    # print(\"input x: \", x.size(),\", input y = \", y.size())\n    x, y = x.to(device), y.to(device)\n    outputs = model(x)\n    # print(\"outputs:\", outputs.size())\n    loss = loss_fn(outputs, y)\n    iou = get_iou_score(outputs, y).mean()\n    \n    optimizer.zero_grad() # \u5c06\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u8bbe\u7f6e\u4e3a0\n    loss.backward()\n    optimizer.step()\n    return loss.item(), iou.item()","e83b928c":"def save_model_args(epoch):\n    # \u6a21\u578b\u5730\u5740\n    model_path = 'fcn{}_resnet{}_model_{}_batch_{}.pth'.format(8,50,epoch,BATCH_SIZE)  \n    # \u4e09\u4e2a\u53c2\u6570\uff1a\u7f51\u7edc\u53c2\u6570\uff1b\u4f18\u5316\u5668\u53c2\u6570\uff1bepoch\n    state = {'net':fcn_model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}\n    torch.save(state, model_path)\n\n    # \u4fdd\u5b58\u8bad\u7ec3\u635f\u5931\u6570\u636e\u548cIoU\u5f97\u5206\u6570\u636e\n    train_losses_save = np.array(train_losses)\n    train_ious_save = np.array(train_ious)\n    plt.plot(train_losses_save, label = 'loss')\n    plt.plot(train_ious_save, label = 'IoU')\n    plt.xlabel('Epoch')\n    plt.ylabel('Metric')\n    plt.legend()\n    # \u4fdd\u5b58\u66f2\u7ebf\n    plt.savefig('fcn{}_resnet{}_loss_{}_batch_{}.png'.format(8,50,200,64), bbox_inches='tight')\n    plt.show()\n    np.save('fcn{}_resnet{}_loss_{}_batch_{}'.format(8,50,epoch,BATCH_SIZE),train_losses_save)\n    np.save('fcn{}_resnet{}_iou_{}_batch_{}'.format(8,50,epoch,BATCH_SIZE),train_ious_save)\n\n","717ca53e":"!mkdir -p '..\/content\/drive\/MyDrive\/\u5b9e\u9a8c\u6570\u636e&\u6a21\u578b\/fcn_resnet'","b160b537":"NUM_EPOCHS = 200\nBATCH_SIZE = 64\n\nfcn_model.train() # \u4e00\u5b9a\u8981\u8868\u660e\u662f\u8bad\u7ec3\u6a21\u5f0f!!!\noptimizer = torch.optim.Adam(fcn_model.parameters())\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n\nloss_fn = nn.BCEWithLogitsLoss()\n\ntrain_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\nsteps  = train_dataset.__len__() \/\/ BATCH_SIZE\nprint(steps,\"steps per epoch\")\n\nstart = time.time()\ntrain_losses = []\ntrain_ious = []\nfor epoch in range(1, NUM_EPOCHS + 1):\n    print('-' * 10)\n    print('Epoch {}\/{}'.format(epoch, NUM_EPOCHS))\n    running_iou = []\n    running_loss = []\n    for step, (x, y) in enumerate(train_dataloader):\n        loss, iou = train_one_batch(fcn_model, x, y)\n        running_iou.append(iou)\n        running_loss.append(loss)\n        print('\\r{:6.1f} %\\tloss {:8.4f}\\tIoU {:8.4f}'.format(100*(step+1)\/steps, loss,iou), end = \"\") \n        \n    print('\\r{:6.1f} %\\tloss {:8.4f}\\tIoU {:8.4f}\\t{}'.format(100*(step+1)\/steps,np.mean(running_loss),np.mean(running_iou), timeSince(start)))\n    scheduler.step(np.mean(running_iou))\n    \n    train_losses.append(loss)\n    train_ious.append(iou)\n    if epoch % 50 is 0:\n        save_model_args(epoch)\n\n\n","746917c4":"'''\nimport torchvision.models as models\n# \u52a0\u8f7d\u8bad\u7ec3\u53c2\u6570\uff08\u635f\u5931+iou\uff09\ntrain_losses_save = np.load(\"..\/content\/drive\/MyDrive\/\u5b9e\u9a8c\u6570\u636e&\u6a21\u578b\/fcn_resnet\/fcn{}_resnet{}_loss_{}_batch_{}.npy\".format(8,50,200,64))\ntrain_ious_save = np.load(\"..\/content\/drive\/MyDrive\/\u5b9e\u9a8c\u6570\u636e&\u6a21\u578b\/fcn_resnet\/fcn{}_resnet{}_iou_{}_batch_{}.npy\".format(8,50,200,64))\n\n\n# \u52a0\u8f7d\u6a21\u578b\u53c2\u6570\nparams = torch.load('..\/content\/drive\/MyDrive\/\u5b9e\u9a8c\u6570\u636e&\u6a21\u578b\/fcn_resnet\/fcn{}_resnet{}_model_{}_batch_{}.pth'.format(8,50,50,64))\nprint(params)\n\n# \u52a0\u8f7d\u6a21\u578b\nnet = fcn_model()\npthfile = r'..\/content\/drive\/MyDrive\/\u5b9e\u9a8c\u6570\u636e&\u6a21\u578b\/fcn_resnet\/fcn{}_resnet{}_model_{}_batch_{}.pth'.format(8,50,50,64)\nnet.load_state_dict(torch.load(pthfile)['net'])\nprint(net)\n'''","b86512b8":"## \u6570\u636e\u9884\u5904\u7406\n#### \u8c03\u7528\u5fc5\u8981\u5e93","5534d128":"## \u6a21\u578b\u8bad\u7ec3","f7481dd5":"#### \u6837\u4f8b\u8f93\u51fa","69a6bfdb":"# \u57fa\u4e8eVGG\u4e3b\u5e72\u7684FCN\u7f51\u7edc\u7684\u56fe\u50cf\u5206\u5272\n\n**TGS Salt Identification Challenge**\n\nSegment salt deposits beneath the Earth's surface\n\n![image](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/10151\/logos\/header.png)\n\n\u6570\u636e\u96c6\u7f51\u5740\uff1ahttps:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge","060608f1":"## FCN(fully Convolutional Networks)\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\n\u63d0\u51fa\u8bba\u6587\uff1a[Fully Convolutional Networks\nfor Semantic Segmentation](https:\/\/arxiv.org\/pdf\/1605.06211.pdf)\n\n### \u4e0eVGG-Net\u7684\u4e0d\u540c\u4e4b\u5904\n\n\u5bf9\u4e8e\u4e00\u822c\u7684\u5206\u7c7bCNN\u7f51\u7edc\uff0c\u5982VGG\u548cResnet\uff0c\u90fd\u4f1a\u5728\u7f51\u7edc\u7684\u6700\u540e\u52a0\u5165\u4e00\u4e9b\u5168\u8fde\u63a5\u5c42\uff0c\u7ecf\u8fc7softmax\u540e\u5c31\u53ef\u4ee5\u83b7\u5f97\u7c7b\u522b\u6982\u7387\u4fe1\u606f\u3002\n```python\n# \u4e09\u5c42\u7ebf\u6027\u5168\u8fde\u63a5\u5c42\nself.classifier = nn.Sequential(\n    # first layer\n    nn.Linear(512 * 7 * 7, 4096, bias = True),\n    nn.ReLU(True),\n    nn.Dropout(),\n    # second layer\n    nn.Linear(4096, 4096, bias = True),\n    nn.ReLU(True),\n    nn.Dropout(),\n    # third layer\n    nn.Linear(4096, num_classes, bias = True),\n)\n```\n\u4f46\u662f\u8fd9\u4e2a\u6982\u7387\u4fe1\u606f\u662f1\u7ef4\u7684\uff0c\u5373\u53ea\u80fd\u6807\u8bc6\u6574\u4e2a\u56fe\u7247\u7684\u7c7b\u522b\uff0c\u4e0d\u80fd\u6807\u8bc6\u6bcf\u4e2a\u50cf\u7d20\u70b9\u7684\u7c7b\u522b\uff0c\u6240\u4ee5\u8fd9\u79cd\u5168\u8fde\u63a5\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u56fe\u50cf\u5206\u5272\u3002\n\n\u5f3a\u884c\u8bad\u7ec3\u4f1a\u5728\u8bc4\u4f30\u65f6\u4e0e\u8bc4\u4f30\u51fd\u6570\u53d1\u751f\u51b2\u7a81\uff0c\u5982\u4e0b\uff1a\n```bash\nValueError: Target size (torch.Size([64, 1, 224, 224])) must be the same as input size (torch.Size([64, 1000]))\n```\n\n\u4e3a\u6b64\uff0cFCN\u63d0\u51fa\u53ef\u4ee5\u628a\u540e\u9762\u51e0\u4e2a\u5168\u8fde\u63a5\u90fd\u6362\u6210\u5377\u79ef\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u83b7\u5f97\u4e00\u5f202\u7ef4\u7684feature map\uff0c\u540e\u63a5softmax\u83b7\u5f97\u6bcf\u4e2a\u50cf\u7d20\u70b9\u7684\u5206\u7c7b\u4fe1\u606f\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u5206\u5272\u95ee\u9898\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a\n\n![iamge](https:\/\/pic1.zhimg.com\/80\/v2-7bfe6e1792c2fb8bcfab6eea632d5e2c_720w.jpg)\n\n![image](https:\/\/pic1.zhimg.com\/80\/v2-721ef7417b32a5aa4973f1e8dd16d90c_720w.jpg)\n\n1. \u5bf9\u4e8eFCN-32s\uff0c\u76f4\u63a5\u5bf9pool5 feature\u8fdb\u884c32\u500d\u4e0a\u91c7\u6837\u83b7\u5f9732x upsampled feature\uff0c\u518d\u5bf932x upsampled feature\u6bcf\u4e2a\u70b9\u505asoftmax prediction\u83b7\u5f9732x upsampled feature prediction\uff08\u5373\u5206\u5272\u56fe\uff09\u3002\n2. \u5bf9\u4e8eFCN-16s\uff0c\u9996\u5148\u5bf9pool5 feature\u8fdb\u884c2\u500d\u4e0a\u91c7\u6837\u83b7\u5f972x upsampled feature\uff0c\u518d\u628apool4 feature\u548c2x upsampled feature\u9010\u70b9\u76f8\u52a0\uff0c\u7136\u540e\u5bf9\u76f8\u52a0\u7684feature\u8fdb\u884c16\u500d\u4e0a\u91c7\u6837\uff0c\u5e76softmax prediction\uff0c\u83b7\u5f9716x upsampled feature prediction\u3002\n3. \u5bf9\u4e8eFCN-8s\uff0c\u9996\u5148\u8fdb\u884cpool4+2x upsampled feature\u9010\u70b9\u76f8\u52a0\uff0c\u7136\u540e\u53c8\u8fdb\u884cpool3+2x upsampled\u9010\u70b9\u76f8\u52a0\uff0c\u5373\u8fdb\u884c\u66f4\u591a\u6b21\u7279\u5f81\u878d\u5408\u3002\u5177\u4f53\u8fc7\u7a0b\u4e0e16s\u7c7b\u4f3c\uff0c\u4e0d\u518d\u8d58\u8ff0\u3002\n\n### \u603b\u7ed3\nFCN\u7f51\u7edc\u662f\u4ee5VGG\u4e3a\u4e3b\u5e72\uff0c\u5c06\u5206\u7c7b\u5668\u7528\u53cd\u5377\u79ef\u8fdb\u884c\u66ff\u6362\uff0c\u6709\u70b9\u7c7b\u4f3c\u4e8e\u8fd9Encode-Decode\u8fc7\u7a0b\u3002\n\n### \u53c2\u8003\n1. https:\/\/zhuanlan.zhihu.com\/p\/31428783\n2. https:\/\/zhuanlan.zhihu.com\/p\/32506912\n","69f62a65":"### \u6062\u590d\u6a21\u578b","461b2010":"### \u4fdd\u5b58\u6a21\u578b\u548c\u53c2\u6570","5d9cc259":"#### \u786e\u8ba4\u6570\u636e\u96c6\u4f4d\u7f6e\uff08\u67e5\u770btrain\uff09\n*@note*\uff1a\u4e00\u5b9a\u8981\u89e3\u538b\u5728\u975e\u4e91\u76d8\u4f4d\u7f6e\uff0c\u907f\u514d\u540e\u671f\u8bad\u7ec3\u96c6\u56e0\u4e3a\u56fe\u7247\u8bfb\u53d6\u901f\u7387\u800c\u5bfc\u81f4\u7684\u901f\u5ea6\u95ee\u9898","dc767902":"#### \u8bad\u7ec3\u96c6\u83b7\u53d6","dbb336e5":"epoch 200\/200 253m46s"}}