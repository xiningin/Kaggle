{"cell_type":{"62ec790f":"code","f5ff288e":"code","bac590be":"code","43bf9baf":"code","8e8df152":"code","d11d0601":"code","0c6cfcad":"code","313ce950":"code","5aa6d9ef":"code","4022aaa6":"code","c124709a":"code","5acf2779":"code","9f8cd8b8":"code","3baa092f":"code","469f5150":"markdown","8ddf2a8c":"markdown","7f8d6d58":"markdown","bb4e95bd":"markdown","df35644d":"markdown","a9d0f631":"markdown","d5c4b709":"markdown"},"source":{"62ec790f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('wordnet')\nstemmer = SnowballStemmer('english')\n\nfrom numpy import dot\nfrom numpy.linalg import norm\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5ff288e":"train_df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntest_df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\nDATA_PATH = '..\/input\/shopee-product-matching\/'","bac590be":"train_df['image'] = DATA_PATH + 'train_images\/' + train_df['image']\ntmp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain_df['target'] = train_df.label_group.map(tmp)\ntrain_df","43bf9baf":"train_df['title'][5]\n","8e8df152":"\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return result","d11d0601":"processed_docs = train_df['title'].map(preprocess)\nprocessed_docs =list(processed_docs)\n","0c6cfcad":"processed_docs[:10] # clean document","313ce950":"def word2vec_model():\n    w2v_model = Word2Vec(min_count=1,\n                     window=3,\n                     vector_size=50,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20)\n    \n    w2v_model.build_vocab(processed_docs)\n    w2v_model.train(processed_docs, total_examples=w2v_model.corpus_count, epochs=300, report_delay=1)\n    \n    return w2v_model","5aa6d9ef":"w2v_model = word2vec_model()\nw2v_model.save('word2vec_model')","4022aaa6":"emb_vec = w2v_model.wv","c124709a":"emb_vec['anak'] # It will return vector representation of the word anak","5acf2779":"\ndef find_similarity(sen1, sen2, model):\n    p_sen1 = preprocess(sen1)\n    p_sen2 = preprocess(sen2)\n    \n    sen_vec1 = np.zeros(50)\n    sen_vec2 = np.zeros(50)\n    for val in p_sen1:\n        sen_vec1 = np.add(sen_vec1, model[val])\n\n    for val in p_sen2:\n        sen_vec2 = np.add(sen_vec2, model[val])\n    \n    return dot(sen_vec1,sen_vec2)\/(norm(sen_vec1)*norm(sen_vec2))\n    ","9f8cd8b8":"find_similarity('Atasan Rajut Wanita LISDIA SWEATER', 'Atasan Rajut Wanita LISDIA',emb_vec )","3baa092f":"find_similarity('Atasan Rajut Wanita LISDIA SWEATER', 'CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa cod)',emb_vec )","469f5150":"# **This notebook is about finding similarity between two titles of the product using word2vec and cosine similarity. This feature can be used with image to find the similar product.**","8ddf2a8c":"# Getting embedding vector","7f8d6d58":"# Word2vec model\n\nI choose embedding dim of size 50. This means that each word will be represented by a vector of size 50","bb4e95bd":"# Fell free to use this notebook and please upvote if you like the work.\n# Thank You","df35644d":"# Add target column in the dataframe","a9d0f631":"# Data Cleaning","d5c4b709":"# Finding similarity between two vector using cosine similarity"}}