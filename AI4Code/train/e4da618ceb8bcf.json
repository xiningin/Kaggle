{"cell_type":{"2e40990a":"code","e2e8de1a":"code","68c8b852":"code","b2ac1c9c":"code","3cde6d80":"code","f834a9a4":"code","8ded9402":"code","eb0cffb6":"code","c99d1a14":"code","cd4e7dca":"code","0b53d4f7":"code","3f27d52d":"code","7e8f75a8":"code","814c9826":"code","9fd068e5":"code","9871ce85":"code","bffcee9e":"code","c97e9c74":"code","8abbfd19":"code","c6964158":"code","479bcb6e":"code","f9dc0033":"code","5bd59603":"code","30980c4e":"code","69aa6ffc":"code","eab57bb3":"code","d886aecb":"code","05c13927":"markdown","60ac9681":"markdown","39856512":"markdown","45776958":"markdown","f3a41fd4":"markdown","4a433408":"markdown","928e1e7f":"markdown","45cdae77":"markdown","7487d7b3":"markdown","9177a3dc":"markdown"},"source":{"2e40990a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e2e8de1a":"!pip install catboost\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import HTML","68c8b852":"data_raw = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata_val  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata_cleaner = [data_raw, data_val]\n\ndata_val_passenger_id = data_val.PassengerId\n\nprint (data_raw.info())\ndata_raw.sample(10)","b2ac1c9c":"print('Train columns with null values:\\n', data_raw.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","3cde6d80":"for dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n    #delete the cabin feature\/column and others previously stated to exclude in train dataset\n    drop_column = ['PassengerId','Cabin', 'Ticket']\n    dataset.drop(drop_column, axis=1, inplace = True)\n\nprint(data_raw.isnull().sum())\nprint(\"-\"*10)\nprint(data_val.isnull().sum())","f834a9a4":"for dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\n    stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics\n    title_names = (dataset['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\n    print(dataset['Title'].value_counts())\n    print(\"-\"*10)\n\n#preview data again\nprint(data_raw.info())\nprint(data_val.info())\ndata_raw.sample(10)","8ded9402":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Fare_Code'] = label.fit_transform(dataset['FareBin'])\n    dataset['Age_Code'] = label.fit_transform(dataset['AgeBin'])","eb0cffb6":"data_raw.head()","c99d1a14":"data_raw_dummy_columns = pd.get_dummies(data_raw[[\"Title\", \"Embarked\"]])\ndata_raw = pd.concat([data_raw, data_raw_dummy_columns], axis=1)\n\ndata_val_dummy_columns = pd.get_dummies(data_val[[\"Title\", \"Embarked\"]])\ndata_val = pd.concat([data_val, data_val_dummy_columns], axis=1)","cd4e7dca":"data_raw.columns.values","0b53d4f7":"data_raw.drop(columns=[\"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Title\", \"FareBin\", \"AgeBin\"], inplace=True)\ndata_val.drop(columns=[\"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Title\", \"FareBin\", \"AgeBin\"], inplace=True)","3f27d52d":"data_raw.head()","7e8f75a8":"data_val.head()","814c9826":"X = data_raw.drop(columns=[\"Survived\"])\ny = data_raw.Survived","9fd068e5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","9871ce85":"cols = X_train.columns.values\nfor col in cols:\n    X_train[col].astype(\"int64\")","bffcee9e":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    XGBClassifier(),\n    CatBoostClassifier()  \n    ]\n\nrow_index = 0\nMLA_compare = pd.DataFrame()\n\nclassifier_names = []\nclassifier_accuracies = []\nkfold_classifier_accuracies = []\n\nfor classifier in MLA:\n    classifier.fit(X_train, y_train)\n\n    y_pred = classifier.predict(X_test)\n    classifier_accuracy_score = accuracy_score(y_test, y_pred)\n\n    kfold_accuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n\n    MLA_name = classifier.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'Accuracy Score'] = classifier_accuracy_score*100\n    MLA_compare.loc[row_index, 'K-Fold Accuracy'] = kfold_accuracy.mean()*100\n    \n    print(classifier.__class__.__name__)\n\n    row_index+=1","c97e9c74":"MLA_compare","8abbfd19":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (20,5)\nsns.barplot(x='MLA Name',y='Accuracy Score',data=MLA_compare, estimator=np.median)\nplt.title(\"Compare Accuracy Scores of Machine Learning Models\")\nplt.xlabel(\"Machine Learning Model\")\nplt.ylabel(\"Accuracy Score\")\nplt.xticks(rotation=90)\n","c6964158":"plt.rcParams[\"figure.figsize\"] = (20,5)\nsns.barplot(x='MLA Name',y='K-Fold Accuracy',data=MLA_compare, estimator=np.median)\nplt.title(\"Compare Accuracy Scores of Machine Learning Models\")\nplt.xlabel(\"Machine Learning Model\")\nplt.ylabel(\"K-Fold Accuracy Score\")\nplt.xticks(rotation=90)","479bcb6e":"MLA_compare = MLA_compare.sort_values(by='K-Fold Accuracy', ascending=False)\nMLA_compare.reset_index(drop=True, inplace=True)","f9dc0033":"MLA_compare[MLA_compare[\"K-Fold Accuracy\"] >= 83]","5bd59603":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\ndata_val_scaled = sc.transform(data_val)","30980c4e":"from sklearn.model_selection import GridSearchCV\n\nclassifier = svm.SVC(random_state=0)\n\nparameters = [{'C': [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'kernel': ['linear']},\n              {'C': [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\n\ngrid_search.fit(X_train_scaled, y_train)\n\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","69aa6ffc":"from sklearn.metrics import confusion_matrix\n\nclassifier = svm.SVC(\n    random_state = 0,\n    C= 0.5,\n    gamma= 0.2,\n    kernel= 'rbf'\n)\nclassifier.fit(X_train_scaled, y_train)\n\naccuracies = cross_val_score(estimator = classifier, X = X_train_scaled, y = y_train, cv = 10)\nprint(f\"K-Fold Accuracy : {accuracies.mean()*100:.2f} %\")\n\ny_pred = classifier.predict(X_test_scaled)\nprint(\"Test Accuracy : \", accuracy_score(y_test, y_pred), \"\\n\")\n\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix \\n\", cm)","eab57bb3":"y_data_val = classifier.predict(data_val_scaled)\ny_data_val","d886aecb":"output = pd.DataFrame({'PassengerId': data_val_passenger_id, 'Survived': y_data_val})\noutput.to_csv('submission-SVC.csv', index=False)","05c13927":"### Encoding Categorical Variables","60ac9681":"### Saving Predictions to CSV file","39856512":"### COMPLETING: complete or delete missing values in train and test\/validation dataset","45776958":"### Training and Predictiong SVC Model with Tuned Parameter","f3a41fd4":"### Machine Learning Algorithm (MLA) Selection and Initialization","4a433408":"### Hyperparameter Tuning for SVC Model","928e1e7f":"### CREATE: Feature Engineering for train and test\/validation dataset","45cdae77":"### Splitting data_raw into training and testing data","7487d7b3":"### Importing Data","9177a3dc":"### Import Libraries"}}