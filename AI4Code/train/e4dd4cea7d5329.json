{"cell_type":{"d6f8ddb2":"code","5824fbb5":"code","6ab187a4":"code","59741357":"code","bfe9c303":"code","38c2b687":"code","faa50856":"code","3ae4bd65":"code","28a49eff":"code","7cd9082f":"code","fea5a990":"code","5a85688b":"code","d97a62bd":"code","b9a5d2cf":"code","ac2eb8de":"code","a8f1bb23":"code","b6501451":"code","3c9beb66":"code","b903a8cf":"code","4a693c3f":"code","64fc9aec":"code","36b93594":"code","b1460501":"code","3b7d1b64":"code","2a1b3d3a":"code","30e79d0f":"code","89fa4487":"code","a9d3a061":"code","d14d57a5":"code","aa6b4559":"code","fcebacd0":"code","21a0af16":"code","dc2d39b6":"code","f8a918cd":"code","c1deef78":"code","d2112f8f":"code","94ac7a4c":"code","61b2e4a1":"code","a16d8f54":"code","75130a83":"code","17519552":"code","e68a83b0":"code","90edde8a":"code","46c96440":"code","ff612663":"code","0652358b":"code","46684224":"code","1752205d":"code","9b1c31de":"code","019f4979":"code","4dfe58de":"code","0ac503fa":"code","75e4bb9e":"code","e36f1127":"code","57e99801":"code","293058fc":"code","138cdb38":"code","020c8828":"code","b7eb8b6f":"code","0d7ece0e":"code","89187bc2":"code","41007140":"code","d0dfd838":"code","8589ba47":"code","073b4203":"code","badb5365":"code","0a0d24fa":"code","8e3b6519":"code","21b02961":"code","d999a1a7":"code","99c2f6a6":"code","010f02a5":"code","004cd007":"code","cda1b1bd":"code","b9d1b662":"code","17ae223d":"code","ca1fdaf5":"code","7c26efe3":"code","8cd048f7":"code","6dd0edc6":"code","d8422890":"code","392777e9":"code","5685cb84":"code","ef44bfc1":"code","5711730f":"code","03958777":"code","b1bad89b":"code","8cd9bb13":"markdown","c41181d8":"markdown","99388391":"markdown","63f15ae2":"markdown","9dba55d7":"markdown","dd987de0":"markdown","81fbfb80":"markdown","34a33d7e":"markdown","0443dd45":"markdown","85cdd0c4":"markdown","57756a45":"markdown","5d6740fd":"markdown","c0f2d981":"markdown","18409357":"markdown","43a80be3":"markdown","313e52da":"markdown","412e651e":"markdown","d4d6a131":"markdown","a5d8009c":"markdown","29bc8f96":"markdown","c53b5fdf":"markdown","ad59b92e":"markdown","52685051":"markdown","50da4df9":"markdown","3fb03076":"markdown","b89e7a9c":"markdown","f30e55d7":"markdown","542a065d":"markdown","3e4a7197":"markdown","25aacb75":"markdown","e5593caf":"markdown","eba1185a":"markdown","11042817":"markdown","08d34ea5":"markdown","e6f499b5":"markdown","546c71a2":"markdown","2297cf41":"markdown","bcd457d2":"markdown","59356d7a":"markdown","2ece6612":"markdown","f6f48d7b":"markdown","c1e6ef03":"markdown","11d5fa73":"markdown","b6c3a780":"markdown","80450f49":"markdown","c89fc6ac":"markdown","443371a6":"markdown"},"source":{"d6f8ddb2":"import pandas as pd\n\nimport matplotlib\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\n\nimport numpy as np\n\nimport lightgbm as lgb\n\nimport plotly as py\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\n\nfrom hyperopt import hp\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\nimport math\nfrom scipy.stats import norm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom time import time\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as ctb\nfrom hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n","5824fbb5":"# Configurando o estilo dos plots\nsns.set_style(\"whitegrid\")","6ab187a4":"!pip install -U kaleido","59741357":"# Obtendo os dados de treinamento\ndf = pd.read_csv('..\/input\/VLabs-DC\/sales_20_21_train.csv')\n\ndf.head()","bfe9c303":"# Verificando o shape\nprint(\"O dataset cont\u00e9m {} linhas e {} colunas.\".format(*df.shape))\n\n# Verificando duplicatas\nprint(\"E cont\u00e9m {} duplicatas.\".format(df.duplicated().sum()))","38c2b687":"# Verificando a porcentagem de valores faltantes dentro do Dataframe\npercent_missing = df.isnull().sum() * 100 \/ len(df)\n\nmissing_value_df = pd.DataFrame({\n                                 'PORCENTAGEM_VALORES_FALTANTES': percent_missing})\n\ndisplay(missing_value_df)","faa50856":"# Visualizando algumas vari\u00e1veis estat\u00edsticas de cada feature cont\u00ednua\ndf.describe()","3ae4bd65":"# Visualizando os valores \u00fanicos de cada feature\nfor col in df:\n    print('A coluna {} possui {} valores \u00fanicos'.format(col, len(df[col].unique())))\n","28a49eff":"# Convertendo o tipo da coluna DT_VENDA de string para datetime.\ndf['DT_VENDA'] = pd.to_datetime(df['DT_VENDA'])\n\ndf.head()","7cd9082f":"# Visualizando os dados estat\u00edsticos das datas\ndf['DT_VENDA'].describe(datetime_is_numeric=True)","fea5a990":"# Dataset para visualizar quantas vendas\/receita foram geradas em cada loja durante todo o per\u00edodo do Dataset\nstore_sells = pd.DataFrame(df.groupby('LOJA').count()['ID_VENDA'])\n\nstore_sells.columns = ['QTD_TOTAL_VENDAS']\n\nstore_sells.head()","5a85688b":"fig, ax = pyplot.subplots(figsize=(50, 15))\n\nsns.barplot(x=store_sells.index, y='QTD_TOTAL_VENDAS', data=store_sells)\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"LOJA\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Quantidade de vendas por loja\", pad=25, fontsize=24)\n\nplt.show()","d97a62bd":"# Dataset para visualizar a receita total gerada em cada loja durante todo o per\u00edodo do Dataset\nstore_sells = pd.DataFrame(df.groupby(\"LOJA\").sum()['VALOR'])\n\nstore_sells.head()","b9a5d2cf":"fig, ax = pyplot.subplots(figsize=(50, 15))\n\nsns.barplot(x=store_sells.index, y='VALOR', data=store_sells)\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"LOJA\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Receita total gerada por loja\", pad=25, fontsize=24)\n\nplt.show()","ac2eb8de":"# Dataset para visualizar quantas vendas foram realizadas por cada tipo de canal durante todo o per\u00edodo do Dataset\nchannel_sells = df['CANAL'].value_counts()\n\nchannel_sells.head()","a8f1bb23":"fig, ax = pyplot.subplots(figsize=(50, 15))\n\nstore_sells_plot = sns.barplot(x=channel_sells.index, y=channel_sells.values)\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"CANAL\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Quantidade de compras por CANAL\", pad=25, fontsize=24)\n\nplt.show(store_sells_plot)","b6501451":"# Dataset para visualizar quantas compras foram realizadas por dia durante todo o per\u00edodo do Dataset\ndate_count_df = df.groupby('DT_VENDA').count()['ID_VENDA']\n\ndate_count_df.head()","3c9beb66":"fig, ax = pyplot.subplots(figsize=(50, 15))\n\nsns.lineplot(data=date_count_df)\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"Data\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Quantidade de compras por dia\", pad=25, fontsize=24)\n\nplt.show()","b903a8cf":"# Dataset para visualizar a receita gerada por dia durante todo o per\u00edodo do Dataset\ndate_revenue_sum_df = df.groupby('DT_VENDA').sum()['VALOR']\n\ndate_revenue_sum_df.head()","4a693c3f":"# Dataset para visualizar a m\u00e9dia de receita gerada por dia durante todo o per\u00edodo do Dataset\ndate_revenue_mean_df = df.groupby('DT_VENDA').mean()['VALOR']\n\ndate_revenue_mean_df.head()","64fc9aec":"fig, ax = pyplot.subplots(figsize=(50, 15))\n\nsns.lineplot(data=date_revenue_sum_df)\n\nsns.lineplot(data=date_revenue_mean_df)\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"Data\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Receita total por dia\", pad=25, fontsize=24)\n\nplt.show()","36b93594":"# Computa a matriz de correla\u00e7\u00e3o\ncorr = df.corr()\n\n\nfifg, ax = plt.subplots(figsize=(11, 9))\n\n# Gerando um colormap customizado\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","b1460501":"# Convers\u00e3o das vari\u00e1veis categ\u00f3ricas em num\u00e9ricas\ndf_labeled = pd.get_dummies(df)\n\ndf_labeled.head()","3b7d1b64":"# Computa a matriz de correla\u00e7\u00e3o\ncorr = df_labeled[['VALOR', 'CANAL_ECM', 'CANAL_FIS', 'CANAL_IFOOD', 'CANAL_TELEVENDAS', 'CANAL_WHATSAPP']].corr()\n\n\nfifg, ax = plt.subplots(figsize=(11, 9))\n\n# Gerando o mapeamento de cores\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","2a1b3d3a":"# Dataframe que cont\u00e9m os clientes \u00fanicos\nusers_df = pd.DataFrame(df[\"ID_CLIENTE\"].unique(), columns=[\"ID_CLIENTE\"])\n\n\nusers_df.head()","30e79d0f":"df_max_purchase = df.groupby('ID_CLIENTE')['DT_VENDA'].max().reset_index()\n\ndf_max_purchase.columns = ['ID_CLIENTE','DT_COMPRA_MAIS_RECENTE']\n\ndf_max_purchase['DT_COMPRA_MAIS_RECENTE'] = pd.to_datetime(df_max_purchase['DT_COMPRA_MAIS_RECENTE'])\n\ndf_max_purchase.head()","89fa4487":"# Compara a \u00faltima transa\u00e7\u00e3o do conjunto de dados com as datas da \u00faltima transa\u00e7\u00e3o dos IDs de clientes individuais.\ndf_max_purchase['RECENCIA'] = (df_max_purchase['DT_COMPRA_MAIS_RECENTE'].max() - df_max_purchase['DT_COMPRA_MAIS_RECENTE']).dt.days\n\ndf_max_purchase.head()","a9d3a061":"# mesclar este dataframe ao nosso dataframe de usu\u00e1rios \u00fanicos\nusers_df = pd.merge(users_df, df_max_purchase[['ID_CLIENTE','RECENCIA']], on='ID_CLIENTE')\n\nusers_df.head()","d14d57a5":"sse={} # Erro\n\nrecency_df = users_df[['RECENCIA']]\n\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(recency_df)\n\n    # Nome dos clusters relativos ao valor da rec\u00eancia\n    recency_df[\"CLUSTERS\"] = kmeans.labels_  \n\n    # Erro correspondente aos clusters\n    sse[k] = kmeans.inertia_ \n\n\n\nfig, ax = pyplot.subplots(figsize=(30, 15))\n\nsns.lineplot(x=list(sse.keys()), y=list(sse.values()))\n\nplt.xlabel(\"N\u00famero de Cluster - K\")\n\nplt.xticks(fontsize=16, rotation=0, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.show()\n\n","aa6b4559":"# Construindo 4 clusters para a rec\u00eancia e colocando no dataframe\nkmeans = KMeans(n_clusters=4)\n\nusers_df['CLUSTER_RECENCIA'] = kmeans.fit_predict(users_df[['RECENCIA']])\n\nusers_df.head()","fcebacd0":"# Visualizando dados estat\u00edsticos da rec\u00eancia para cada cluster criado.\nusers_df.groupby('CLUSTER_RECENCIA')['RECENCIA'].describe()","21a0af16":"# M\u00e9todo que ordena o cluster\ndef order_cluster(cluster_field_name, target_field_name, df, ascending):\n    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n\n    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n\n    df_new['index'] = df_new.index\n\n    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n\n    df_final = df_final.drop([cluster_field_name],axis=1)\n\n    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n    \n    return df_final","dc2d39b6":"# Ordenando os clusters da rec\u00eancia\nusers_df = order_cluster('CLUSTER_RECENCIA', 'RECENCIA', users_df, False)\n\nusers_df.head()\n","f8a918cd":"# Visualizando dados estat\u00edsticos da rec\u00eancia para cada cluster criado.\nusers_df.groupby('CLUSTER_RECENCIA')['RECENCIA'].describe()\n","c1deef78":"# Visualizando a quantidade de clientes pertencentes a cada cluster\ndisplay(users_df['CLUSTER_RECENCIA'].value_counts(ascending=True))","d2112f8f":"# Criando um label pra a melhor visualiza\u00e7\u00e3o dos dados relativos aos clusters da rec\u00eancia\nusers_df['LABEL_RECENCIA'] = 'Alta'\n\nusers_df.loc[users_df['CLUSTER_RECENCIA'] == 0, 'LABEL_RECENCIA'] = 'Muito Baixa' \n\nusers_df.loc[users_df['CLUSTER_RECENCIA'] == 1, 'LABEL_RECENCIA'] = 'Baixa' \n\nusers_df.loc[users_df['CLUSTER_RECENCIA'] == 2, 'LABEL_RECENCIA'] = 'M\u00e9dia' \n\nrecency_values_count = users_df['LABEL_RECENCIA'].value_counts(ascending=True)","94ac7a4c":"# Visualizando a quantidade de clientes pertencentes a cada cluster\ndisplay(users_df['LABEL_RECENCIA'].value_counts(ascending=True))","61b2e4a1":"fig, ax = pyplot.subplots(figsize=(50, 15))\n\nlabel_recency_plot = sns.barplot(x=recency_values_count.index, y=recency_values_count)\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"Cluster\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Agrupamento das categorias de Rec\u00eancia\", pad=50, fontsize=24)\n\nplt.show(label_recency_plot)","a16d8f54":"# Recupera a quantidade de compras por cliente \nfrequency_df = df.groupby('ID_CLIENTE')['DT_VENDA'].count().reset_index()\n\nfrequency_df.columns = ['ID_CLIENTE','FREQUENCIA']\n\nfrequency_df.head()","75130a83":"# Mescla este dataframe ao nosso dataframe de usu\u00e1rios \u00fanicos\nusers_df = pd.merge(users_df, frequency_df, on='ID_CLIENTE')\n\nusers_df.head()","17519552":"sse={} # Erro\n\nrecency_df = users_df[['FREQUENCIA']]\n\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(recency_df)\n\n    # Nome dos clusters relativos ao valor da rec\u00eancia\n    recency_df[\"CLUSTERS\"] = kmeans.labels_  \n\n    # Erro correspondente aos clusters\n    sse[k] = kmeans.inertia_ \n\n\n\nfig, ax = pyplot.subplots(figsize=(30, 15))\n\nsns.lineplot(x=list(sse.keys()), y=list(sse.values()))\n\nplt.xlabel(\"N\u00famero de Cluster - K\")\n\nplt.xticks(fontsize=16, rotation=0, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.show()","e68a83b0":"# Aplicando K-means na coluna de frequ\u00eancia\nkmeans = KMeans(n_clusters = 4)\n\nusers_df['CLUSTER_FREQUENCIA'] = kmeans.fit_predict(users_df[['FREQUENCIA']])\n\n# Visualizando os dados estat\u00edsticos de cada cluster em rela\u00e7\u00e3o a frequ\u00eancia.\nusers_df.groupby('CLUSTER_FREQUENCIA')['FREQUENCIA'].describe()\n","90edde8a":"users_df = order_cluster('CLUSTER_FREQUENCIA', 'FREQUENCIA', users_df, True)\n\n# Visualizando os dados estat\u00edsticos de cada cluster em rela\u00e7\u00e3o a frequ\u00eancia.\nusers_df.groupby('CLUSTER_FREQUENCIA')['FREQUENCIA'].describe()","46c96440":"# Visualizando a quantidade de clientes pertencentes a cada cluster\ndisplay(users_df['CLUSTER_FREQUENCIA'].value_counts(ascending=True))","ff612663":"# Definindo um label para cada cluster para visualiza\u00e7\u00e3o\nusers_df['LABEL_FREQUENCIA'] = 'Alta'\n\nusers_df.loc[users_df['CLUSTER_FREQUENCIA'] == 0, 'LABEL_FREQUENCIA'] = 'Muito Baixa' \n\nusers_df.loc[users_df['CLUSTER_FREQUENCIA'] == 1, 'LABEL_FREQUENCIA'] = 'Baixa' \n\nusers_df.loc[users_df['CLUSTER_FREQUENCIA'] == 2,'LABEL_FREQUENCIA'] = 'M\u00e9dia' ","0652358b":"# Visualizando a quantidade de clientes pertencentes a cada cluster\ndisplay(users_df['LABEL_FREQUENCIA'].value_counts(ascending=True))","46684224":"fig, ax = pyplot.subplots(figsize=(50, 15))\n\nlabel_frequency_plot = sns.barplot(x=users_df['LABEL_FREQUENCIA'].value_counts(ascending=True).index, y=users_df['LABEL_FREQUENCIA'].value_counts(ascending=True))\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"Cluster\", fontsize=24)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Agrupamento das categorias de frequ\u00eancia\", pad=50, fontsize=24)\n\nplt.show(label_frequency_plot)","1752205d":"# Recupera a receita total das compras por cliente \nclient_revenue = df[['ID_CLIENTE', 'VALOR']].groupby(\"ID_CLIENTE\").sum()\n\nclient_revenue.head()","9b1c31de":"# Cria um dataset com a receita total das compras por cliente \nclient_revenue_df = client_revenue.reset_index()[['ID_CLIENTE', 'VALOR']]\n\nclient_revenue_df = client_revenue_df.rename(columns={\"ID_CLIENTE\": \"ID_CLIENTE\", \"VALOR\": \"RECEITA\"}, errors=\"raise\")\n\nclient_revenue_df.head()","019f4979":"# Mescla este dataframe ao nosso dataframe de usu\u00e1rios \u00fanicos\nusers_df = pd.merge(users_df, client_revenue_df, on='ID_CLIENTE')\n\nusers_df.head()","4dfe58de":"# Analisando a distribui\u00e7\u00e3o de receita\n\nfig, ax = pyplot.subplots(figsize=(50, 10))\n\nsns.histplot(data=users_df, x=\"RECEITA\", bins=150, kde=True)\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"Receita\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Histograma da receita\", pad=25, fontsize=24)\n\nplt.show()","0ac503fa":"sse={} # Erro\n\nrecency_df = users_df[['RECEITA']]\n\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(recency_df)\n\n    # Nome dos clusters relativos ao valor da receita\n    recency_df[\"CLUSTERS\"] = kmeans.labels_  \n\n    # Erro correspondente aos clusters\n    sse[k] = kmeans.inertia_ \n\n\n\nfig, ax = pyplot.subplots(figsize=(30, 15))\n\nsns.lineplot(x=list(sse.keys()), y=list(sse.values()))\n\nplt.xlabel(\"N\u00famero de Cluster - K\")\n\nplt.xticks(fontsize=16, rotation=0, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.show()","75e4bb9e":"# Aplicando a clusteriza\u00e7\u00e3o\nkmeans = KMeans(n_clusters=4)\n\nusers_df['CLUSTER_RECEITA'] = kmeans.fit_predict(users_df[['RECEITA']])\n\n#Visualizando dados estat\u00edsticos dos clusters da receita\nusers_df.groupby('CLUSTER_RECEITA')['RECEITA'].describe()","e36f1127":"# Ordena os clusters de forma crescente, em que o \u00faltimo cluster ter\u00e1 o maior valor de receita\nusers_df = order_cluster('CLUSTER_RECEITA', 'RECEITA', users_df, True)\n\n\n#Visualizando dados estat\u00edsticos dos clusters da receita\nusers_df.groupby('CLUSTER_RECEITA')['RECEITA'].describe()","57e99801":"# Visualizando a quantidade de clientes pertencentes a cada cluster\nusers_df['CLUSTER_RECEITA'].value_counts(ascending=True)","293058fc":"# Definindo um label para cada cluster para visualiza\u00e7\u00e3o\nusers_df['LABEL_RECEITA'] = 'Alta'\n\nusers_df.loc[users_df['CLUSTER_RECEITA'] == 0, 'LABEL_RECEITA'] = 'Muito Baixa' \n\nusers_df.loc[users_df['CLUSTER_RECEITA'] == 1, 'LABEL_RECEITA'] = 'Baixa' \n\nusers_df.loc[users_df['CLUSTER_RECEITA'] == 2,'LABEL_RECEITA'] = 'M\u00e9dia' ","138cdb38":"# Visualizando a quantidade de clientes pertencentes a cada cluster\nusers_df['LABEL_RECEITA'].value_counts(ascending=True)","020c8828":"fig, ax = pyplot.subplots(figsize=(50, 15))\n\nsns.barplot(x=users_df['LABEL_RECEITA'].value_counts(ascending=True).index, y=users_df['LABEL_RECEITA'].value_counts(ascending=True))\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"Cluster\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=0, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Agrupamento das categorias de receita\", pad=25, fontsize=24)\n\nplt.show()","b7eb8b6f":"# Calcula a pontua\u00e7\u00e3o geral e use a m\u00e9dia para ver os detalhes\nusers_df['RFM'] = users_df['CLUSTER_RECENCIA'] + users_df['CLUSTER_FREQUENCIA'] + users_df['CLUSTER_RECEITA']\n\nusers_df.groupby('RFM')['RECENCIA','FREQUENCIA','RECEITA'].mean()","0d7ece0e":"# Visualizando a quantidade de clientes pertencentes a cada cluster\nusers_df['RFM'].value_counts(ascending=True)","89187bc2":"users_df['SEGMENTO'] = 'Baixo'\nusers_df.loc[users_df['RFM'] > 3,'SEGMENTO'] = 'M\u00e9dio' \nusers_df.loc[users_df['RFM'] > 6,'SEGMENTO'] = 'Alto' \n\nusers_df.head()","41007140":"# Visualizando a quantidade de clientes pertencentes a cada cluster\nusers_df['SEGMENTO'].value_counts(ascending=True)","d0dfd838":"# Visualizando os dados por Segmento\nfig, ax = pyplot.subplots(figsize=(50, 15))\n\nsns.barplot(x=users_df['SEGMENTO'].value_counts(ascending=True).index, y=users_df['SEGMENTO'].value_counts(ascending=True))\n\nplt.ylabel(\"\")\n\nplt.xlabel(\"Cluster\", fontsize=16)\n\nplt.xticks(fontsize=16, rotation=30, horizontalalignment='center')\n\nplt.yticks(fontsize=16, rotation=0)\n\nplt.title(\"Agrupamento das categorias do RFM\", pad=25, fontsize=24)\n\nplt.show()","8589ba47":"users_df.head()","073b4203":"recency_plot_data = [\n    go.Scatter(\n        x=users_df.query(\"SEGMENTO == 'Baixo'\")['RFM'],\n        y=users_df.query(\"SEGMENTO == 'Baixo'\")['CLUSTER_RECENCIA'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=users_df.query(\"SEGMENTO == 'M\u00e9dio'\")['RFM'],\n        y=users_df.query(\"SEGMENTO == 'M\u00e9dio'\")['CLUSTER_RECENCIA'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=users_df.query(\"SEGMENTO == 'Alto'\")['RFM'],\n        y=users_df.query(\"SEGMENTO == 'Alto'\")['CLUSTER_RECENCIA'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"Classifica\u00e7\u00e3o da Rec\u00eancia\"},\n        xaxis= {'title': \"Score do RFM\"},\n        title='LTV'\n    )\nfig = go.Figure(data=recency_plot_data, layout=plot_layout)\nfig.show(renderer='png')","badb5365":"frequency_plot_data = [\n    go.Scatter(\n        x=users_df.query(\"SEGMENTO == 'Baixo'\")['RFM'],\n        y=users_df.query(\"SEGMENTO == 'Baixo'\")['CLUSTER_FREQUENCIA'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=users_df.query(\"SEGMENTO == 'M\u00e9dio'\")['RFM'],\n        y=users_df.query(\"SEGMENTO == 'M\u00e9dio'\")['CLUSTER_FREQUENCIA'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=users_df.query(\"SEGMENTO == 'Alto'\")['RFM'],\n        y=users_df.query(\"SEGMENTO == 'Alto'\")['CLUSTER_FREQUENCIA'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"Classifica\u00e7\u00e3o da Frequ\u00eancia\"},\n        xaxis= {'title': \"Score do RFM\"},\n        title='LTV'\n    )\nfig = go.Figure(data=frequency_plot_data, layout=plot_layout)\nfig.show(renderer=\"png\")","0a0d24fa":"#revenue_plot_data = [\n#    go.Scatter(\n#        x=users_df.query(\"SEGMENTO == 'Baixo'\")['RFM'],\n#        y=users_df.query(\"SEGMENTO == 'Baixo'\")['CLUSTER_RECEITA'],\n#        mode='markers',\n#        name='Low',\n#        marker= dict(size= 7,\n#           line= dict(width=1),\n#            color= 'blue',\n#            opacity= 0.8\n#           )\n#    ),\n#        go.Scatter(\n#        x=users_df.query(\"SEGMENTO == 'M\u00e9dio'\")['RFM'],\n#        y=users_df.query(\"SEGMENTO == 'M\u00e9dio'\")['CLUSTER_RECEITA'],\n#        mode='markers',\n#        name='Mid',\n#        marker= dict(size= 9,\n#            line= dict(width=1),\n#            color= 'green',\n#            opacity= 0.5\n#           )\n#    ),\n#        go.Scatter(\n#        x=users_df.query(\"SEGMENTO == 'Alto'\")['RFM'],\n#        y=users_df.query(\"SEGMENTO == 'Alto'\")['CLUSTER_RECEITA'],\n#        mode='markers',\n#        name='High',\n#        marker= dict(size= 11,\n#            line= dict(width=1),\n#            color= 'red',\n#            opacity= 0.9\n#           )\n#    ),\n#]\n\n#plot_layout = go.Layout(\n#        yaxis= {'title': \"Classifica\u00e7\u00e3o da Receita\"},\n#        xaxis= {'title': \"Score do RFM\"},\n#        title='LTV'\n#    )\n#fig = go.Figure(data=revenue_plot_data, layout=plot_layout)\n#fig.show(renderer=\"png\")","8e3b6519":"# Sele\u00e7\u00e3o das features\nfeatured_df = users_df[['ID_CLIENTE', 'RECENCIA', 'CLUSTER_RECENCIA', 'FREQUENCIA', 'CLUSTER_FREQUENCIA', 'RECEITA', 'CLUSTER_RECEITA', 'RFM']]\n\nfeatured_df.head()","21b02961":"# Criando um dataframe com a coluna ANO_MES_VENDA para ser usado como s\u00e9rie temporal mensal\ndf_month = df\n\ndf_month['ANO_MES_VENDA'] = df_month['DT_VENDA'].map(lambda date: 100*date.year + date.month)\n\ndf_month.head()","d999a1a7":"# Fun\u00e7\u00e3o que gera as features relativas a tend\u00eancia no dataset\ndef get_trend_for_each_period(reference_df, months):\n    _df = reference_df[['ID_CLIENTE', 'VALOR']].groupby('ID_CLIENTE').sum().reset_index()\n    \n    aux_dict = {\n        3 : pd.to_timedelta(30,unit='d'),\n        6 : pd.to_timedelta(60,unit='d'),\n        9 : pd.to_timedelta(90,unit='d'),\n        12: pd.to_timedelta(360,unit='d')\n    }\n    \n    for month in months: \n        date_condition = reference_df['DT_VENDA'].max() - aux_dict[month]\n\n        _df[f'RECEITA_{month}m'] = reference_df.where(reference_df['DT_VENDA'] > date_condition) \\\n                                    .sort_values(['ID_CLIENTE','ANO_MES_VENDA'], ascending=True) \\\n                                    .groupby('ID_CLIENTE')['VALOR'] \\\n                                    .sum() \\\n                                    .fillna(0)\n\n        _df[f'MEDIA_{month}m'] = reference_df.where(reference_df['DT_VENDA'] > date_condition) \\\n                                    .sort_values(['ID_CLIENTE','ANO_MES_VENDA'], ascending=True) \\\n                                    .groupby('ID_CLIENTE')['VALOR'] \\\n                                    .mean() \\\n                                    .fillna(0)\n\n        _df[f'DESVIO_PRADRAO_{month}m'] = reference_df.where(reference_df['DT_VENDA'] > date_condition) \\\n                                            .sort_values(['ID_CLIENTE','ANO_MES_VENDA'], ascending=True) \\\n                                            .groupby('ID_CLIENTE')['VALOR'] \\\n                                            .std() \\\n                                            .fillna(0)\n\n        _df[f'VARIANCIA_{month}m'] = reference_df.where(reference_df['DT_VENDA'] > date_condition) \\\n                                        .sort_values(['ID_CLIENTE','ANO_MES_VENDA'], ascending=True) \\\n                                        .groupby('ID_CLIENTE')['VALOR'] \\\n                                        .var() \\\n                                        .fillna(0)\n    return _df.fillna(0)","99c2f6a6":"list_of_months = [3, 6, 9, 12]\n\n# client_df = get_trend_for_each_period(df_month, list_of_months)","010f02a5":"features = ['RECEITA_3m', 'RECENCIA', 'CLUSTER_RECENCIA',\n            'FREQUENCIA', 'CLUSTER_FREQUENCIA','RECEITA', 'CLUSTER_RECEITA','RFM',\n            'MEDIA_3m', 'DESVIO_PRADRAO_3m','VARIANCIA_3m',\n            'RECEITA_6m', 'MEDIA_6m', 'DESVIO_PRADRAO_6m',\n            'VARIANCIA_6m', 'RECEITA_9m', 'MEDIA_9m', 'DESVIO_PRADRAO_9m',\n            'VARIANCIA_9m', 'RECEITA_12m', 'MEDIA_12m', 'DESVIO_PRADRAO_12m',\n            'VARIANCIA_12m']\n\ntarget = ['VALOR']","004cd007":"# Criando as novas features no nosso dataset para 3, 6, 9 e 12 meses \n# Utilizando os 3 \u00faltimos meses como o modelo de predi\u00e7\u00e3o da sa\u00edda dos dias 24\/02\/2021 at\u00e9 25\/05\/2021\npredict_df = get_trend_for_each_period(df_month[df_month['ANO_MES_VENDA'] >= 202012], list_of_months)\n\n# Com os dados anteriores ao per\u00edodo de predi\u00e7\u00e3o iremos utilizar para realizar o treinamento\ntrain_test_df = get_trend_for_each_period(df_month[df_month['ANO_MES_VENDA'] < 202012], list_of_months)","cda1b1bd":"predict_df = predict_df.merge(featured_df, how=\"inner\", on=\"ID_CLIENTE\")\n\ntrain_test_df = train_test_df.merge(featured_df, how=\"inner\", on=\"ID_CLIENTE\")","b9d1b662":"# Visualizando os dados\ntrain_test_df.head()","17ae223d":"# Visualizando os dados\npredict_df.head()","ca1fdaf5":"from sklearn.ensemble import RandomForestRegressor\n\n# Inicializando o modelo com 100 estimators\nrf = RandomForestRegressor(n_estimators = 100, random_state = 42)\n\nX_train, X_test, y_train, y_test = train_test_split(train_test_df[features], train_test_df[target], test_size=0.33, random_state=42)\n\n\n# Treinando o modelo\nrf.fit(X_train, y_train)","7c26efe3":"predict_df['prediction'] = rf.predict(predict_df[features])","8cd048f7":"predict_df[['VALOR', 'prediction']]","6dd0edc6":"# Avaliando o modelo com RMSE\n\nrmse = mean_squared_error(predict_df['VALOR'], predict_df['prediction'], squared=False)\n\nprint('O RMSE dos dados de valida\u00e7\u00e3o \u00e9: ', rmse)\n","d8422890":"predict_df.groupby('ID_CLIENTE').sum().reset_index()","392777e9":"# Gerando a sa\u00edda \noutput_df = predict_df.groupby('ID_CLIENTE').sum().reset_index()[['ID_CLIENTE', 'prediction']]\n\noutput_df.columns = ['ID_CLIENTE', 'VALOR']\n\noutput_df[output_df['VALOR'] < 0] = 0\n\noutput_df = output_df.round(2)\n\noutput_df.head()","5685cb84":"sample_submission = pd.read_csv('..\/input\/VLabs-DC\/sample_submission.csv')\n\nsample_submission.head()","ef44bfc1":"result_submission = sample_submission.merge(output_df, on='ID_CLIENTE', how='left')\n\nresult_submission.head()","5711730f":"result_submission.shape","03958777":"result_submission = result_submission[['ID_CLIENTE', 'VALOR_y']]\n\nresult_submission.columns = ['ID_CLIENTE', 'VALOR']\n\nresult_submission.head()","b1bad89b":"result_submission.fillna(0).to_csv('.\/result.csv', index=False)","8cd9bb13":"Com isto, podemos determinar que K = 4 nos d\u00e1 uma clusteriza\u00e7\u00e3o mais otimizada.","c41181d8":"<a id=\"section-three-three\"><\/a>\n\n### 4.3. Analisando os canais de vendas mais recorrentes\n\n","99388391":"<a id=\"section-two\"><\/a>\n# 3. Pr\u00e9-processamento dos dados","63f15ae2":"# Prevendo o LTV de clientes do Varejo\n\nO LTV - \u201clifetime value\u201d ou \u201cvalor vital\u00edcio\u201d \u00e9 uma m\u00e9trica estat\u00edstica que estima o lucro l\u00edquido da vida de um cliente dentro da empresa. \u00c9 comum termos clientes mais valiosos que outros, estes se d\u00e3o pela frequ\u00eancia  que compram ou pela grande receita que geram, e \u00e9 importante investirmos mais naqueles que geram um maior valor para a empresa.\n\n\nEste projeto tem como objetivo identificar padr\u00f5es de comportamento dos clientes do Varejo a fim de prever o LTV do cliente para a empresa. Mais especificamente, foi dado um dataset refer\u00eante \u00e0 01\/2020 at\u00e9 02\/2021 e devemos prever o LTV de cada cliente do dia 25\/02\/2021 at\u00e9 24\/05\/2021. \n\n### O Dataset\nFoi disponibilizado um Dataset que cont\u00e9m as seguintes informa\u00e7\u00f5es:\n\n- ID_VENDA - Id \u00fanico para cada registro\n- DT_VENDA - Data da venda\n- LOJA - Id \u00fanico da loja em que a venda foi realizada\n- QTD_SKU - Quantidade de produtos vendidos\n- VALOR - Valor total da venda\n- ID_CLIENTE - Id \u00fanico do cliente\n- CANAL - Canal de venda (FIS - F\u00edsico, ECM - E-commerce, TELEVENDAS - Telefone, WHATSAPP - WhatsApp, IFOOD - iFood)\n\nO dataset \u00e9 nomeado por `sales_20_21_train.csv` e foi obtido em 01\/10\/2021 disponibilizado pela equipe do VLabs para o desafio de previs\u00e3o do LTV. ","9dba55d7":"Para a cria\u00e7\u00e3o do modelo iremos utilizar o modelo LGBM para realizar a regress\u00e3o. Como auxilio, iremos utilizar o m\u00e9todo TimeSeriesSplit do Scikit-learn para realizamos a Cross Validation dos nossos dados. ","dd987de0":"Primeiro iremos determina o n\u00famero K de clusters ideal utilizando o m\u00e9todo Elbow.","81fbfb80":"\u00c9 interessante tamb\u00e9m analisarmos a rela\u00e7\u00e3o que a vari\u00e1vel *CANAL* tem com o Valor de cada compra.\n\nPara que possamos utilizar esse algor\u00edtmo, precisamos criar labels para cada vari\u00e1vel categ\u00f3rica. Come\u00e7aremos por este passo.","34a33d7e":"<a id=\"section-four-two-two\"><\/a>\n\n### 5.2.2. Atribuindo um Score para a Frequ\u00eancia\nSemelhante ao caso da Rec\u00eancia, iremos clusterizar os clientes com base em sua frequ\u00eancia. Quanto maior a frequ\u00eancia, mais importante se torna este cliente em rela\u00e7\u00e3o ao RFM.","0443dd45":"Agora, iremos analisar a rela\u00e7\u00e3o entre o RFM e as categorias de Rec\u00eancia, Frequ\u00eancia e Receita. ","85cdd0c4":"Com isto, temos o resultado da predi\u00e7\u00e3o do LTV pr\u00f3ximos 90 dias de cada cliente. \n\nPr\u00f3ximos passos: \n\n- Evoluir a an\u00e1lise da s\u00e9rie temporal;\n- Complementar as features com valores de tend\u00eancia em cada per\u00edodo de compras;\n- Melhorar a predi\u00e7\u00e3o com v\u00e1rios algor\u00edtmos de Regress\u00e3o estudando os melhores par\u00e2metros;","57756a45":"<a id=\"section-four\"><\/a>\n\n### 5. Segmentando os dados\n\nPara o problema de LTV podemos criar v\u00e1rias segmenta\u00e7\u00f5es. Caso a Reten\u00e7\u00e3o dos clientes nas lojas seja interessante, o dado pode ser segmentado baseado na probabilidade de Churn, entre outros m\u00e9todos. Para este caso, iremos trabalhar com RFM que diz respeito \u00e0 Recency - Frequency - Monetary Value que em tradu\u00e7\u00e3o livre se diz respeito \u00e0 Rec\u00eancia - Frequ\u00eancia  e Valor monet\u00e1rio. \n\n - Baixo valor de RFM: Clientes que s\u00e3o menos ativos que outros e n\u00e3o realizam compras frequentes e que gastam pouco - normalmente s\u00e3o visitantes. \n\n - Valor m\u00e9dio de RFM: Representa o cliente m\u00e9dio. Ele compra com certa frequ\u00eancia e de forma recente e gera um valor consider\u00e1vel de receita, mas que n\u00e3o chega a ser muito alto. \n\n - Alto valor de RFM: Cliente que gera muita receita de forma frequente e de forma recente, esses s\u00e3o os clientes que voc\u00ea n\u00e3o quer perder.\n\nPara isso, iremos calcular a Rec\u00eancia, Frequ\u00eancia e Valor monet\u00e1rio de cada cliente e iremos aplicar um processo de aprendizado n\u00e3o-supervisionado para gerar diferentes cluster para cada valor individual.\n\nSer\u00e1 gerado um Score final que representa o RFM somando cada cluster das vari\u00e1veis Rec\u00eancia, Frequ\u00eancia e Valor monet\u00e1rio.","5d6740fd":"Para o cluster de Rec\u00eancia temos que clientes no perfil de Baixo RFM tendem a estar em todos os 4 n\u00edveis de Rec\u00eancia. J\u00e1 para os clientes com perfis de M\u00e9dio RFM tendem a estar nos n\u00edveis 1 a 3 de Rec\u00eanica. Por outro lado, os clientes com Alto perfil de RFM tendem a estar nos n\u00edveis 2 a 3 de Rec\u00eancia.","c0f2d981":"<a id=\"section-three\"><\/a>\n# 4. Analise dos dados","18409357":"Semelhante aos casos anteriores, K = 4 nos d\u00e1 um n\u00famero de clusters ideal. ","43a80be3":"1. [Configura\u00e7\u00f5es iniciais](#section-zero)\n\n2. [Obten\u00e7\u00e3o e visualiza\u00e7\u00e3o dos dados](#section-one)\n\n3. [Pr\u00e9-processamento dos dados](#section-two)\n\n4. [Analise dos dados](#section-three)\n    \n    4.1. [Quantidade de vendas realizadas por cada loja](#section-three-one)\n    \n    4.2. [Receita total gerada por cada loja](#section-three-two)\n    \n    4.3. [Canais de vendas mais recorrentes](#section-three-three)\n    \n    4.4. [Quantidade de vendas por dia](#section-three-four)\n    \n    4.5. [Receita total por dia](#section-three-five)\n    \n    4.6. [Correla\u00e7\u00e3o entre as vari\u00e1veis](#section-three-six)\n\n5. [Segmentando os dados](#section-four)\n\n    5.1.1. [C\u00e1lculo da Rec\u00eancia](#section-four-one-one)\n    \n    5.1.2. [Atribuindo um Score para a Rec\u00eancia](#section-four-one-two)\n    \n    5.2.1. [C\u00e1lculo da Frequ\u00eancia](#section-four-two-one)\n    \n    5.2.2. [Atribuindo um Score para a Frequ\u00eancia](#section-four-two-two)\n    \n    5.3.1. [C\u00e1lculo da Receita](#section-three-one)\n    \n    5.3.2. [Atribuindo um Score para a Receita](#section-four-three-two)\n\n6. [C\u00e1lculo do RFM](#section-five)\n\n7. [C\u00e1lculo do LTV](#section-six)\n\n8. [Determinando features](#section-seven)\n\n9. [Cria\u00e7\u00e3o do modelo](#section-eight)\n\n","313e52da":"Para o c\u00e1lculo do RFM \u00e9 interassante que os clusters em rela\u00e7\u00e3o \u00e0 rec\u00eancia estejam ordenados de forma decrescente em rela\u00e7\u00e3o a rec\u00eancia, em que o maior cluster corresponda ao cliente com uma boa rec\u00eancia (com um menor valor de rec\u00eancia). ","412e651e":"<a id=\"section-four-three-one\"><\/a>\n\n### 5.3.1 C\u00e1lculo da Receita\n\nIremos analisar como se comportam a receita (valor total) gasto nas lojas por cliente durante todo o per\u00edodo do Dataset.\n","d4d6a131":"Para o cluster de Frequ\u00eancia temos que clientes no perfil de Baixo RFM tendem a estar em todos os 3 primeiros n\u00edveis de Frequ\u00eancia. J\u00e1 para os clientes com perfis de M\u00e9dio RFM tendem a estar em todos os n\u00edveis de Frequ\u00eancia. Por outro lado, os clientes com Alto perfil de RFM tendem a estar nos n\u00edveis 2 a 3 de Frequ\u00eancia.","a5d8009c":"Neste momento iremos criar as features relativas a tend\u00eancia e sazonalidade nos dados de cada cliente al\u00e9m da segmenta\u00e7\u00e3o dos dados por janela deslizante.","29bc8f96":"### Sele\u00e7\u00e3o das features\n","c53b5fdf":"<a id=\"section-three-six\"><\/a>\n\n### 4.6. Analisando a correla\u00e7\u00e3o entre as vari\u00e1veis","ad59b92e":"Temos que o LTV \u00e9 dado pela Receita total - Custo total.\n\nComo no dataset n\u00e3o temos a informa\u00e7\u00e3o do Custo por cliente, iremos considerar que o Lifetime Value ser\u00e1 determinado apenas pela receita total do cliente.","52685051":"<a id=\"section-one\"><\/a>\n# 2. Obten\u00e7\u00e3o e visualiza\u00e7\u00e3o dos dados","50da4df9":"### 9.2. Avaliando o modelo","3fb03076":"<a id=\"section-four-three-two\"><\/a>\n\n### 5.3.2. Atribuindo um Score para a Receita\nAssim como nos casos anteriores, iremos clusterizar os clientes com base na receita gerada e determinar o n\u00famero K ideal de Clusters utilizando o m\u00e9todo Elbow.","b89e7a9c":"Para o cluster de Receita temos que clientes no perfil de Baixo RFM tendem a estar no n\u00edvel 0 a 2 de Receita. J\u00e1 para os clientes com perfis de M\u00e9dio RFM tendem a estar em todos n\u00edveis de Receita. Enquanto isso, os clientes com Alto perfil de RFM tendem a estar nos n\u00edveis 2 a 3 de Receita.","f30e55d7":"<a id=\"section-four-one-two\"><\/a>\n\n### 5.1.2 Atribuindo um Score para a Rec\u00eancia\n\nIremos aplicar a clusteriza\u00e7\u00e3o por K-means para atribuir um score para a rec\u00eancia. Para que isto ocorra de forma otimizada, iremos verificar quantos cluster s\u00e3o necess\u00e1rios para termos o melhor trade-off do algor\u00edtmo K-means utilizando o m\u00e9todo de Elbow. \n\n\"O m\u00e9todo Elbow se trata de uma t\u00e9cnica interessante para encontrar o valor ideal do par\u00e2metro k. Basicamente o que o m\u00e9todo faz \u00e9 testar a vari\u00e2ncia dos dados em rela\u00e7\u00e3o ao n\u00famero de clusters. \u00c9 considerado um valor ideal de k quando o aumento no n\u00famero de clusters n\u00e3o representa um valor significativo de ganho.\" - Mineirando dados","542a065d":"\n\nCom isso, podemos perceber que, quanto maior o Score RFM, mais confi\u00e1vel \u00e9 o consumidor. ","3e4a7197":"Com isto, podemos determinar que K = 4 nos d\u00e1 uma clusteriza\u00e7\u00e3o mais otimizada.","25aacb75":"<a id=\"section-eight\"><\/a>\n\n### 9. Cria\u00e7\u00e3o do modelo","e5593caf":"<a id=\"section-five\"><\/a>\n\n### 6. C\u00e1lculo do RFM\n\nN\u00f3s possu\u00edmos um Cluster Score para rec\u00eancia, frequ\u00eancia e receita, com isto, iremos criar um score geral que engloba esses tr\u00eas Cluster principais gerando o nosso RFM.\n\n O RFM \u00e9 dado pela soma de cada cluster indivualmente.","eba1185a":"<a id=\"section-three-five\"><\/a>\n\n### 4.5. Analisando a receita total por dia","11042817":"<a id=\"section-three-one\"><\/a>\n\n### 4.1. Visualizando a quantidade de vendas realizadas por cada loja","08d34ea5":"Para este caso, quanto maior o cluster, maior deve ser a receita total gerada. ","e6f499b5":"Iremos analisar a correla\u00e7\u00e3o entre as vari\u00e1veis cont\u00ednuas do Dataset. \n\nA **correla\u00e7\u00e3o** \u00e9 uma an\u00e1lise bivariada que mede a for\u00e7a da associa\u00e7\u00e3o entre duas vari\u00e1veis \u200b\u200be a dire\u00e7\u00e3o da rela\u00e7\u00e3o. Em termos da for\u00e7a da rela\u00e7\u00e3o, o valor do coeficiente de correla\u00e7\u00e3o varia entre +1 e -1. Um valor de \u00b1 1 indica um grau de associa\u00e7\u00e3o entre as duas vari\u00e1veis. \u00c0 medida que o valor do coeficiente de correla\u00e7\u00e3o vai para 0, a rela\u00e7\u00e3o entre as duas vari\u00e1veis \u200b\u200bser\u00e1 mais fraca. \n\nExistem v\u00e1rios m\u00e9todos de correla\u00e7\u00e3o, de Perason, Spearman e Kendall. \n\nPara o m\u00e9todo de correla\u00e7\u00e3o iremos utilizar a correla\u00e7\u00e3o de Pearson.","546c71a2":"<a id=\"section-three-two\"><\/a>\n\n### 4.2. Visualizando a receita total gerada por cada loja","2297cf41":"<a id=\"section-four-one-one\"><\/a>\n\n### 5.1.1 C\u00e1lculo da Rec\u00eancia\n\nIremos calcular a rec\u00eancia do cliente que nada mais \u00e9 que a diferen\u00e7a de dias em que o usu\u00e1rio realizou sua \u00faltima transa\u00e7\u00e3o em rela\u00e7\u00e3o ao dia mais recente do *Dataset* - 24\/02\/2021. Quanto maior a rec\u00eancia, menos recente \u00e9 as compras deste cliente. ","bcd457d2":"<a id=\"section-six\"><\/a>\n\n### 7. C\u00e1lculo do LTV\nAgora entramos na parte que interessa, iremos calcular o valor de LTV de cada cliente durante todo o per\u00edodo do Dataset.\n\n","59356d7a":"<a id=\"section-three-four\"><\/a>\n\n### 4.4. Analisando a quantidade de vendas por dia\n","2ece6612":"<a id=\"section-seven\"><\/a>\n\n### 8. Determinando features","f6f48d7b":"<a id=\"section-four-two-one\"><\/a>\n\n### 5.2.1. C\u00e1lculo da Frequ\u00eancia\n\nA frequ\u00eancia se d\u00e1 com base no total de compras que cada cliente realizou durante todo o per\u00edodo do *Dataset*.\n","c1e6ef03":"Agora iremos gerar as features relativas \u00e0 tend\u00eancia de consumo de cada cliente no determinado Ano\/M\u00eas analisando os pr\u00f3ximos 3, 6, 9 e 12 meses de consumo. ","11d5fa73":"Em primeiro momento iremos preservar todos os dados, por mais que tenhamos um usu\u00e1rio com um gasto exorbitante. ","b6c3a780":"<a id=\"section-zero\"><\/a>\n\n# 1. Configura\u00e7\u00f5es iniciais","80450f49":"Para o c\u00e1lculo do RFM geral \u00e9 interassante que os clusters em rela\u00e7\u00e3o \u00e0 frequ\u00eancia estejam ordenados de forma crescente, em que o maior cluster corresponda ao cliente com uma boa frequ\u00eancia (com um maior valor de compras dentro do per\u00edodo determinado). ","c89fc6ac":"Est\u00e1 na hora de gerar a sa\u00edda :)","443371a6":"## Estrat\u00e9gia\n\nPara que possamos prever com precis\u00e3o o LTV de cada cliente nos pr\u00f3ximos 3 meses do Dataset iremos calcular a tend\u00eancia de consumo de cada cliente nos per\u00edodos a serem determinados. Como estrat\u00e9gia de segmenta\u00e7\u00e3o dos dados iremos utilizar o m\u00e9todo de **janela deslizante (rolling window)**.\n\nUsaremos para o treinamento do modelo todo o Dataset menos as \u00faltimas compras realizadas nos \u00faltimos 90 dias - elas ser\u00e3o utilizadas como a valida\u00e7\u00e3o do modelo (e conseguequentemente o resultado final).\n\n## Tend\u00eancia\nA tend\u00eancia de compra de cada cliente se d\u00e1 pela quantidade de compras realizadas, o desvio padr\u00e3o das compras, a m\u00e9dia, a mediana e vari\u00e2ncia - tudo isso em um per\u00eddo de X dias.  Nesta etapa usaremos per\u00edodos de 30 dias para calcular a tend\u00eancia em cada per\u00eddo, sendo assim, teremos colunas com dados estat\u00edsticos de para 30, 60, 90, 120, 150, 180 dias. Al\u00e9m disso, como iremos utilizar o m\u00e9todo de janela deslizante, para cada cliente e para cada m\u00eas associado a este cliente haver\u00e1 essas features de tend\u00eancia, assim, teremos mais dados para poder trabalhar o modelo.\n"}}