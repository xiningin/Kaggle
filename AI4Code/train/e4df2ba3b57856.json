{"cell_type":{"488f6b34":"code","7cc12f37":"code","9fb81d09":"code","1a368a50":"code","9bd2509e":"code","0af526c1":"code","b87be158":"code","b9751e61":"code","8b7c3f8b":"code","8bab6fa9":"code","b49ef1b2":"code","d4d4110e":"code","d0a27900":"code","53703b85":"code","7fafa543":"code","1e56cd86":"code","cea4cfa0":"code","97acf34f":"code","2c58af9b":"code","10b394ea":"code","a499a62c":"code","2ba1d818":"code","220a63f0":"code","2861726b":"code","5da4bf0f":"code","68ce0ee3":"code","eeabde4f":"code","4c45f9c9":"code","2943aba9":"code","24c164e3":"code","574fc35d":"code","2352407b":"markdown","085428eb":"markdown","1101c897":"markdown","1762692a":"markdown","305ea428":"markdown","255ff596":"markdown","f5fcedd1":"markdown","1ca65121":"markdown","c72a74d0":"markdown","24655c5d":"markdown","75bb2837":"markdown","75be67b4":"markdown","f9ad7a65":"markdown","ddd56e30":"markdown","cc435c19":"markdown","f90eb47e":"markdown","4d0e14eb":"markdown","25fcb5d2":"markdown","661bba15":"markdown","7e4b12ba":"markdown","356fc3c7":"markdown","bea6da18":"markdown","837e874d":"markdown","51657d88":"markdown"},"source":{"488f6b34":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport eli5","7cc12f37":"train_val = pd.read_csv('..\/input\/amazontrainreviews\/train.csv', index_col=0)\ntrain_val.reset_index(drop=True, inplace=True)","9fb81d09":"print(train_val.info())\ndisplay(train_val.head())","1a368a50":"sns.countplot(train_val['labels']);\nplt.title('Labels distribution');","9bd2509e":"train_val['len'] = train_val['sentences'].apply(lambda x: len(x.split()))\nsns.distplot(train_val['len']);","0af526c1":"neg_mean_len = train_val.groupby('labels')['len'].mean().values[0]\npos_mean_len = train_val.groupby('labels')['len'].mean().values[1]\n\nprint(f\"Negative mean length: {neg_mean_len:.2f}\")\nprint(f\"Positive mean length: {pos_mean_len:.2f}\")\nprint(f\"Mean Difference: {neg_mean_len-pos_mean_len:.2f}\")\nax = sns.catplot(x='labels', y='len', data=train_val, kind='box')","b87be158":"neg_array = train_val[train_val['labels']==0]['len'].values\npos_array = train_val[train_val['labels']==1]['len'].values\nmean_diff = neg_mean_len - pos_mean_len","b9751e61":"def permutation_sample(data1, data2):\n    # Permute the concatenated array: permuted_data\n    data = np.concatenate((data1,data2))\n    permuted_data = np.random.permutation(data)\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2","8b7c3f8b":"def draw_perm_reps(data_1, data_2, size=1):\n\n    perm_replicates = np.empty(size)\n\n    for i in range(size):\n        # Generate permutation sample\n        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n\n        # Compute the test statistic\n        perm_replicates[i] = np.mean(perm_sample_1) - np.mean(perm_sample_2)\n\n    return perm_replicates","8bab6fa9":"perm_replicates = draw_perm_reps(neg_array, pos_array,\n                                 size=10000)\n\n# Compute p-value: p\np = np.sum(perm_replicates >= mean_diff) \/ len(perm_replicates)\n\nprint(f'p-value = {p}')","b49ef1b2":"def prediction(model, X_train, y_train, X_valid, y_valid):\n    model.fit(X_train, y_train)\n    pred = model.predict(X_valid)\n    acc = accuracy_score(y_valid, pred)\n    f1 = f1_score(y_valid, pred)\n    conf = confusion_matrix(y_valid, pred)\n    joblib.dump(model, f\"model_acc_{acc:.5f}.pkl\")\n    return model, acc, f1, conf","d4d4110e":"transformer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3), \n                              lowercase=True, max_features=100000)\nX = transformer.fit_transform(train_val['sentences'])\ny = train_val.labels","d0a27900":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, \n                                                      random_state=42, stratify=y)\nmodel = LogisticRegression(C=1, random_state=42, n_jobs=-1)\nfit_model, acc, f1, conf = prediction(model, X_train, y_train, X_valid, y_valid)","53703b85":"print(f\"Accuracy: {acc:.5f}\")\nprint(f\"F1_Score: {f1:.5f}\")\nprint(f\"Confusion Matrix: {conf}\")","7fafa543":"eli5.show_weights(estimator=fit_model, \n                  feature_names= list(transformer.get_feature_names()),\n                    top=(20,20))","1e56cd86":"!pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https:\/\/download.pytorch.org\/whl\/nightly\/cu101\/torch_nightly.html\nimport torch\ntorch.__version__","cea4cfa0":"import os\nos.environ['WANDB_SILENT'] = 'True'\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n\nfrom typing import Mapping, List\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom transformers import AutoConfig, AutoTokenizer, AutoModel\n\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback, OptimizerCallback, CheckpointCallback, WandbLogger\nfrom catalyst.utils import set_global_seed, prepare_cudnn\nfrom catalyst.contrib.nn import RAdam, Lookahead, OneCycleLRWithWarmup\nimport wandb","97acf34f":"MODEL_NAME = 'distilbert-base-uncased'\nLOG_DIR = \".\/amazon\" \nNUM_EPOCHS = 2 \nLEARNING_RATE = 5e-5\nMAX_SEQ_LENGTH = 512\nBATCH_SIZE = 32\nWEIGHT_DECAY = 1e-3\nACCUMULATION_STEPS = 3\nSEED = 42\nFP_16 = dict(opt_level=\"O1\")","2c58af9b":"set_global_seed(SEED)\nprepare_cudnn(deterministic=True, benchmark=True)","10b394ea":"class ReviewDataset(Dataset):\n\n    \n    def __init__(self,\n                 sentences: List[str],\n                 labels: List[str] = None,\n                 max_seq_length: int = MAX_SEQ_LENGTH,\n                 model_name: str = 'distilbert-base-uncased'):\n\n        self.sentences = sentences\n        self.labels = labels\n        self.max_seq_length = max_seq_length\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        \n    def __len__(self):\n\n        return len(self.sentences)\n\n    \n    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n\n        sentence = self.sentences[index]\n        encoded = self.tokenizer.encode_plus(sentence, add_special_tokens=True, \n                                        pad_to_max_length=True, max_length=self.max_seq_length, \n                                        return_tensors=\"pt\",)\n        \n        output = {\n            'input_ids': encoded['input_ids'],\n            'attention_mask': encoded['attention_mask']\n        }\n        \n        output['targets'] = torch.tensor(self.labels[index], dtype=torch.long)\n        \n        return output","a499a62c":"df_train, df_valid = train_test_split(\n            train_val,\n            test_size=0.2,\n            random_state=42,\n            stratify = train_val.labels.values\n        )\nprint(df_train.shape, df_valid.shape)","2ba1d818":"train_dataset = ReviewDataset(\n    sentences=df_train['sentences'].values.tolist(),\n    labels=df_train['labels'].values,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)\n\nvalid_dataset = ReviewDataset(\n    sentences=df_valid['sentences'].values.tolist(),\n    labels=df_valid['labels'].values,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)","220a63f0":"train_val_loaders = {\n    \"train\": DataLoader(dataset=train_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=True, num_workers=2, pin_memory=True),\n    \"valid\": DataLoader(dataset=valid_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=False, num_workers=2, pin_memory=True)    \n}","2861726b":"print(df_valid.sentences.values[50])\nvalid_dataset[50]","5da4bf0f":"class DistilBert(nn.Module):\n\n    def __init__(self, pretrained_model_name: str = MODEL_NAME, num_classes: int = 2):\n\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(\n             pretrained_model_name)\n\n        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n                                                    config=config)\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Linear(config.dim, num_classes)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n    def forward(self, input_ids, attention_mask=None, head_mask=None):\n\n        assert attention_mask is not None, \"attention mask is none\"\n        distilbert_output = self.distilbert(input_ids=input_ids,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        hidden_state = distilbert_output[0]  # [BATCH_SIZE=32, MAX_SEQ_LENGTH = 512, DIM = 768]\n        pooled_output = hidden_state[:, 0]  # [32, 768]\n        pooled_output = self.pre_classifier(pooled_output)  # [32, 768]\n        pooled_output = F.relu(pooled_output)  # [32, 768]\n        pooled_output = self.dropout(pooled_output)  # [32, 768]\n        logits = self.classifier(pooled_output)  # [32, 2]\n\n        return logits","68ce0ee3":"model = DistilBert()","eeabde4f":"param_optim = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']","4c45f9c9":"criterion = nn.CrossEntropyLoss()\n\nbase_optimizer = RAdam([\n    {'params': [p for n,p in param_optim if not any(nd in n for nd in no_decay)],\n     'weight_decay': WEIGHT_DECAY}, \n    {'params': [p for n,p in param_optim if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.0}\n])\noptimizer = Lookahead(base_optimizer)\nscheduler = OneCycleLRWithWarmup(\n    optimizer, \n    num_steps=NUM_EPOCHS, \n    lr_range=(LEARNING_RATE, 1e-8),\n    init_lr=LEARNING_RATE,\n    warmup_steps=0,\n)","2943aba9":"runner = SupervisedRunner(\n    input_key=(\n        \"input_ids\",\n        \"attention_mask\"\n    )\n)\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=train_val_loaders,\n    callbacks=[\n        AccuracyCallback(num_classes=2),\n        OptimizerCallback(accumulation_steps=ACCUMULATION_STEPS),\n        WandbLogger(name=\"Name\", project=\"sentiment-analysis\"),\n    ],\n    fp16=FP_16,\n    logdir=LOG_DIR,\n    num_epochs=NUM_EPOCHS,\n    verbose=True\n)","24c164e3":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ndef prediction(model, sentence: str, max_len: int = 512, device = 'cpu'):\n    x_encoded = tokenizer.encode_plus(sentence, add_special_tokens=True, pad_to_max_length=True, max_length=max_len, return_tensors=\"pt\",).to(device)\n    logits = model(x_encoded['input_ids'], x_encoded['attention_mask'])\n    probabilities = F.softmax(logits.detach(), dim=1)\n    output = probabilities.max(axis=1)\n    print(sentence)\n    print(f\"Class: {['Negative' if output.indices[0] == 0 else 'Positive'][0]}, Probability: {output.values[0]:.4f}\")","574fc35d":"prediction(plain_model, df_valid['sentences'].values[20])","2352407b":"We'll create dataset. Instantiate tokenizer. Then, we convert tokens to integers, add special tokens, use padding to max_length. Return `'input_ids', 'attention_mask', 'targets'`","085428eb":"# Baseline - LogReg (Tf-Idf)","1101c897":"Our baseline will be Logistic Regression with Tf-Idf. First, we define a function for prediction, which calculates accuracy, f1_score, confusion matrix and saves our model.","1762692a":"Here we'll use DistilBert from [transformers](https:\/\/huggingface.co\/transformers\/index.html). And [catalyst](https:\/\/github.com\/catalyst-team\/catalyst) for running experiment.\n\nFirst, we install torch nightly for Mixed-precision training.","305ea428":"We can see that negative sentences are longer on average. To say how significant this difference, we use permutation testing and calculate p-value.\n\nFirst, we define a function to generate a permutation sample from two arrays. Then, we generate permutation replicates, which are a single statistic computed from permutation sample. Last, we compute the probability of getting at least 5.91 difference in mean under the hypothesis that the distributions of words are identical.","255ff596":"After two epochs, we\u2019ll able to reach 96.22% accuracy, which is on 6% higher than logistic regression.\n\nTo improve our result even more, we can continue fine-tuning with frozen encoder.\n\n\n### Test","f5fcedd1":"![](https:\/\/www.topbots.com\/wp-content\/uploads\/2020\/01\/cover_sentiment_analysis_BERT_1600px_web-1280x640.jpg)\n\nHello Everyone!\n\nIn this notebook, I\u2019ll work with data from Amazon Review, which consists of 360000 reviews. There\u2019re only positive and negative sentences.\n\nSteps:\n* EDA\n* Baseline Logistic Regression(Tf-Idf)\n* DistilBert\n* [DistilBert Inference Optimization](https:\/\/www.kaggle.com\/alexalex02\/nlp-transformers-inference-optimization)","1ca65121":"Let\u2019s count number of words and see it distribution","c72a74d0":"# EDA","24655c5d":"Review and model input","75bb2837":"For reproducibility","75be67b4":"Interpreting model weights with ELI5.","f9ad7a65":"# DistilBert","ddd56e30":"Extracting unigrams, bigrams and trigrams, also removing stopwords.","cc435c19":"![](https:\/\/i.ibb.co\/9wxK0Zz\/Val-Metric.png)","f90eb47e":"We have 0 Null value and now let's look at target distribution","4d0e14eb":"Config setup","25fcb5d2":"Now we\u2019ll divide it by sentiment and calculate average values","661bba15":"Making train_test_split, defining datasets and loaders","7e4b12ba":"Initialize pre-trained model. From config we'll use dimensionality of the encoder layers and the pooler layer = 768. And dropout probabilities = 0.2. Then, we'll compute logits for the input sequence.","356fc3c7":"The p-value tells us that the null hypothesis is false.","bea6da18":"Training setup:\n\n1. We'll apply weight decay for all parameters except 'bias' and 'LayerNorm'\n1. Lookahead optimizer(improves the learning stability and lowers the variance of its inner optimizer)\n1. OneCycleLRWithWarmup with 0 warmup steps, cosine annealing from 5e-5 to 1e-8.\n1. Gradient accumulation for large batch training.","837e874d":"## Importing libraries and reading data","51657d88":"# Sentiment Analysis of Amazon Reviews"}}