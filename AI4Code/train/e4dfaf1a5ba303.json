{"cell_type":{"a06fd7cc":"code","7940870e":"code","a3e20686":"code","00fb5b01":"code","d07eeac2":"code","1b9bb9b7":"code","c3a6f8b9":"code","bc74d22f":"code","ddcc9d7e":"code","8a8f00b6":"code","b6e20a93":"code","d50dd22c":"code","f69f43c5":"code","cb633da4":"code","30c4be73":"code","80e82011":"code","cd62431f":"code","958fa167":"code","e3428c2f":"code","f9c33991":"code","a684b1e6":"code","eadee4f0":"code","951e6fbf":"code","e1904cff":"code","294ed551":"code","a3c9ee0b":"code","31d87446":"code","bc116825":"code","1bea0074":"code","50250e80":"code","ff503b9f":"code","05c07b29":"code","43b8bc8a":"code","bf5c120d":"code","c68464f7":"code","b2e5919c":"code","12332484":"code","06c664a8":"code","779b1e5d":"markdown","efb146ee":"markdown","10f0cfd5":"markdown","04357e71":"markdown"},"source":{"a06fd7cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7940870e":"pd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', None)  \npd.set_option('display.max_colwidth', -1) ","a3e20686":"train_data = pd.read_csv('..\/input\/competition\/Train.csv')\ntest_data  =pd.read_csv('..\/input\/competition\/Test.csv')","00fb5b01":"train_data.head(10)","d07eeac2":"train_data['score'].value_counts()","1b9bb9b7":"#creating a new column- length \n# this gives the length of the post\ntrain_data['length'] = np.NaN\nfor i in range(0,len(train_data['content'])):\n    train_data['length'][i]=(len(train_data['content'][i]))\ntrain_data.length = train_data.length.astype(int)","c3a6f8b9":"train_data.head()","bc74d22f":"#creating subplots to see distribution of length of reviews\nsns.set_style(\"darkgrid\");\nf, (ax1,ax3) = plt.subplots(figsize=(12,6),nrows=1, ncols=2,tight_layout=True);\nsns.distplot(train_data[train_data['score']==1][\"length\"],bins=30,ax=ax1);\nsns.distplot(train_data[train_data['score']==-1][\"length\"],bins=30,ax=ax3);\nax1.set_title('\\n Distribution of length of positives reviews \\n');\nax3.set_title('\\n Distribution of length of negatives reviews \\n');\nax1.set_ylabel('Frequency');","ddcc9d7e":"# word cloud for words related to positive review \ncontent=\" \".join(post for post in train_data[train_data['score']==1].content)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(content)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Frequntly occuring words related to positive review  \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","8a8f00b6":"# word cloud for words related to negative review \ncontent=\" \".join(post for post in train_data[train_data['score']==-1].content)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(content)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Frequntly occuring words related to negative review  \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","b6e20a93":"# word cloud for words related to neutre review \ncontent=\" \".join(post for post in train_data[train_data['score']==0].content)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(content)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Frequntly occuring words related to neutre review  \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","d50dd22c":"#Calculating basline accuracy\ntrain_data['score'].value_counts(normalize=True)","f69f43c5":"# Import Tokenizer\nfrom nltk.tokenize import RegexpTokenizer\n# Instantiate Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+') \n# Changing the contents of selftext to lowercase\ntrain_data.loc[:,'content'] = train_data.content.apply(lambda x : str.lower(x))\n# Removing hyper link, latin characters and digits\ntrain_data['content']=train_data['content'].str.replace('http.*.*', '',regex = True)\ntrain_data['content']=train_data['content'].str.replace('\u00fb.*.*', '',regex = True)\ntrain_data['content']=train_data['content'].str.replace(r'\\d+','',regex= True)\n# \"Run\" Tokenizer\ntrain_data['tokens'] = train_data['content'].map(tokenizer.tokenize)","cb633da4":"#displaying first 5 rows of dataframe\ntrain_data.head()","30c4be73":"# Printing french stopwords\nprint(stopwords.words(\"french\"))","80e82011":"stop = stopwords.words(\"english\") and  stopwords.words(\"arabic\") and stopwords.words(\"english\")","cd62431f":"# adding this stop word to list of stopwords as it appears on frequently occuring word\nitem=['amp'] #'https','co','http','\u00fb','\u00fb\u00f2','\u00fb\u00f3','\u00fb_'","958fa167":"stop.extend(item)","e3428c2f":"#removing stopwords from tokens\ntrain_data['tokens']=train_data['tokens'].apply(lambda x: [item for item in x if item not in stop])","f9c33991":"# When we \"lemmatize\" data, we take words and attempt to return their lemma, or the base\/dictionary form of a word.\n# Importing lemmatizer \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n# Instantiating lemmatizer \nlemmatizer = WordNetLemmatizer()\n\nlemmatize_words=[]\nfor i in range (len(train_data['tokens'])):\n    word=''\n    for j in range(len(train_data['tokens'][i])):\n        lemm_word=lemmatizer.lemmatize(train_data['tokens'][i][j])#lemmatize\n        \n        word=word + ' '+lemm_word # joining tokens into sentence    \n    lemmatize_words.append(word) # store in list\n\n#creating a new column to store the result\ntrain_data['lemmatized']=lemmatize_words\n#displaying first 5 rows of dataframe\ntrain_data.head()","a684b1e6":"#reading the test data\ntest=pd.read_csv('..\/input\/competition\/Test.csv')","eadee4f0":"test.head(10)","951e6fbf":"#creating a new column- length \n# this gives the length of the post\ntest['length'] = np.NaN\nfor i in range(0,len(test['content'])):\n    test['length'][i]=(len(test['content'][i]))\ntest.length = test.length.astype(int)","e1904cff":"# word cloud for Frequntly occuring words in test dataframe\ntext=\" \".join(post for post in test.content)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(content)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words in test dataframe \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","294ed551":"# Import Tokenizer\nfrom nltk.tokenize import RegexpTokenizer\n# Instantiate Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+') \n# Changing the contents of selftext to lowercase\ntest.loc[:,'content'] = test.content.apply(lambda x : str.lower(x))\n# Removing hyper link, latin characters and digits\ntest['content']=test['content'].str.replace('http.*.*', '',regex = True)\ntest['content']=test['content'].str.replace('\u00fb.*.*', '',regex = True)\ntest['content']=test['content'].str.replace(r'\\d+','',regex= True)\n# \"Run\" Tokenizer\ntest['tokens'] = test['content'].map(tokenizer.tokenize)","a3c9ee0b":"#displaying first 5 rows of dataframe\ntest.head()","31d87446":"#removing stopwords from tokens\ntest['tokens']=test['tokens'].apply(lambda x: [item for item in x if item not in stop])","bc116825":"# Importing lemmatizer \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n# Instantiating lemmatizer \nlemmatizer = WordNetLemmatizer()\n\nlemmatize_words=[]\nfor i in range (len(test['tokens'])):\n    word=''\n    for j in range(len(test['tokens'][i])):\n        lemm_word=lemmatizer.lemmatize(test['tokens'][i][j])#lemmatize\n        \n        word=word + ' '+lemm_word # joining tokens into sentence    \n    lemmatize_words.append(word) # store in list\n\n#creating a new column to store the result\ntest['lemmatized']=lemmatize_words\n#displaying first 5 rows of dataframe\ntest.head()","1bea0074":"# word cloud for Frequntly occuring words in test dataframe after lemmatizing\ntext=\" \".join(post for post in test.lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\n Frequntly occuring words in test dataframe after lemmatizing \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","50250e80":"#Text Vectorization using TfidfVectorizer \/\/Convert a collection of raw documents to a matrix of TF-IDF features.\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\ntrain_vectors = vectorizer.fit_transform(train_data[\"lemmatized\"])\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = vectorizer.transform(test[\"lemmatized\"])","ff503b9f":"#imports\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split","05c07b29":"#defining X and y for the model\nX = train_data['lemmatized']\ny = train_data['score']","43b8bc8a":"#Text Vectorization using TfidfVectorizer \/\/Convert a collection of raw documents to a matrix of TF-IDF features.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X)\nX_test = vectorizer.transform(test[\"lemmatized\"])","bf5c120d":"# Training the SVM model on the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', verbose=True,random_state = 0)\nclassifier.fit(X_train,y)","c68464f7":"from sklearn.metrics import classification_report\ny_pred = classifier.predict(X_test)","b2e5919c":"#Calcul of precision using training set\nfrom sklearn.metrics import accuracy_score\nre_y_pred = classifier.predict(X_train)\naccuracy_score(y,re_y_pred)","12332484":"# Creating an empty data frame\nsubmission_Zindi = pd.DataFrame()\n# Assigning values to the data frame-submission_kaggle\nsubmission_Zindi['ID'] = test.ID\nsubmission_Zindi['score'] = y_pred\n# Head of submission_kaggle\nsubmission_Zindi.head()","06c664a8":"# saving data as  SampleSubmission.csv\nsubmission_Zindi.loc[ :].to_csv('SampleSubmission.csv',index=False)","779b1e5d":"# Tokenization","efb146ee":"# Removing StopWords","10f0cfd5":"# TEST DATA","04357e71":"# Creating .csv file"}}