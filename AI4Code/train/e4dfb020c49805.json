{"cell_type":{"b3500f65":"code","c602cb3d":"code","330375d1":"code","cf83bfc8":"code","75fd943a":"code","72e0b378":"code","e6dc552f":"code","6402d8c8":"markdown","e2576001":"markdown","7428a683":"markdown","bedeca95":"markdown","a5a7e7d6":"markdown","e849ba6c":"markdown","d78ba6ed":"markdown","19ad41aa":"markdown"},"source":{"b3500f65":"from __future__ import absolute_import, division, print_function\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight')\nimport os\nimport sys\nimport random\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\n\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed\n\n# Set some parameters\nDATA_PATH = '..\/input'\nTRAIN_PATH = DATA_PATH + '\/stage1_train\/'\nTEST_PATH  = DATA_PATH + '\/stage1_test\/'\n\nIMAGE_W = 128\nIMAGE_H = 128\nIMAGE_C = 1","c602cb3d":"# Get train and test IDs\ntrain_ids = next(os.walk(TRAIN_PATH))[1]\ntest_ids  = next(os.walk(TEST_PATH))[1]\nlen(train_ids), len(test_ids)","330375d1":"# Get and resize train images and masks\ndef load_image(filename):\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.resize(img,(IMAGE_W,IMAGE_H))\n    img = img.astype(np.float32)\/255.0\n    img = np.expand_dims(img, axis=-1)\n    return img\n\ndef load_data(train_path, test_path, shuffle=False):\n    print('Getting and resizing train images and masks ... ')\n    sys.stdout.flush()\n\n    trainX = np.zeros((len(train_ids), IMAGE_H, IMAGE_W, IMAGE_C), dtype=np.float32)\n    trainY = np.zeros((len(train_ids), IMAGE_H, IMAGE_W, 1), dtype=np.bool)\n    for i, name in tqdm(enumerate(train_ids), total=len(train_ids)):\n        path = train_path + name\n        trainX[i] = load_image(path + '\/images\/' + name + '.png')\n\n        # \u5c06\u591a\u4e2a\u5206\u5f00\u7684\u63a9\u7801\u5408\u5728\u4e00\u8d77\uff0c\u8fb9\u754c\u5730\u65b9\u7684\u50cf\u7d20\u70b9\u503c\u4e3a True\n        mask = np.zeros((IMAGE_H, IMAGE_W, 1), dtype=np.bool)\n        for mask_file in next(os.walk(path + '\/masks\/'))[2]:\n            img = cv2.imread(path + '\/masks\/' + mask_file)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img = cv2.resize(img,(IMAGE_W,IMAGE_H))\n            img = np.expand_dims(img, axis=-1)\n            mask = np.maximum(mask, img)\n        trainY[i] = mask\n\n    # Get and resize test images\n    print('Getting and resizing test images ... ')\n    sys.stdout.flush()\n    testX = np.zeros((len(test_ids), IMAGE_H, IMAGE_W, IMAGE_C), dtype=np.float32)\n    sizes_test = []\n    for i, name in tqdm(enumerate(test_ids), total=len(test_ids)):\n        path = test_path + name\n        testX[i] = load_image(path + '\/images\/' + name + '.png')\n    \n    if shuffle:\n        trainX, trainY = shuffle_data(trainX, trainY)\n\n    return trainX, trainY, testX\n\ntrainX, trainY, testX = load_data(TRAIN_PATH, TEST_PATH)\ntrainX.shape, trainY.shape, testX.shape","cf83bfc8":"def image_augmentation(trainX, trainY):\n    n_imgaug = 1+2 # 1 origin + 2 augmentation\n    shape = trainX.shape\n    shape = (trainX.shape[0]*n_imgaug,) + trainX.shape[1:] \n    new_trainX = np.zeros(shape, dtype=np.float32)\n    new_trainY = np.zeros(shape, dtype=np.float32)\n    for i in tqdm(range(len(trainX))):\n        img = trainX[i]\n        new_trainX[i*n_imgaug+0] = img\n        new_trainX[i*n_imgaug+1] = np.fliplr(img)\n        new_trainX[i*n_imgaug+2] = np.flipud(img)\n        \n        img = trainY[i]\n        new_trainY[i*n_imgaug+0] = img\n        new_trainY[i*n_imgaug+1] = np.fliplr(img)\n        new_trainY[i*n_imgaug+2] = np.flipud(img)\n\n    return new_trainX, new_trainY\n\ntrainX, trainY = image_augmentation(trainX, trainY)\ntrainX.shape, trainY.shape","75fd943a":"# Check if training data looks all right\nix = random.randint(0, len(trainX)-1)\nimage = trainX[ix].reshape(IMAGE_H, IMAGE_W)\n\nplt.figure(figsize=(10, 10))\n\nplt.subplot(121)\nplt.grid(False)\nplt.imshow(image, plt.cm.gray)\nplt.title(\"Image\")\n\nplt.subplot(122)\nplt.grid(False)\nplt.imshow(np.squeeze(trainY[ix]), plt.cm.gray)\nplt.title(\"Mask\");\ndel image","72e0b378":"import time\nimport tflearn\nimport tensorflow as tf\nfrom tflearn import input_data, dropout, fully_connected\nfrom tflearn import conv_2d, max_pool_2d, conv_2d_transpose, upsample_2d\nfrom tflearn import merge\nfrom tflearn import regression\nfrom tflearn import ImagePreprocessing\nfrom tflearn import ImageAugmentation\nfrom tflearn import Momentum\n\ntf.reset_default_graph()\n\nd0 = input_data(shape=[None, IMAGE_H, IMAGE_W, IMAGE_C], name=\"input\")\n\nc1 = conv_2d(d0,  8, 3, weights_init='variance_scaling', activation='relu', name=\"conv1_1\")\nc1 = conv_2d(c1,  8, 3, weights_init='variance_scaling', activation='relu', name=\"conv1_2\")\np1 = max_pool_2d(c1, 2)\n\nc2 = conv_2d(p1, 16, 3, weights_init='variance_scaling', activation='relu', name=\"conv2_1\")\nc2 = conv_2d(c2, 16, 3, weights_init='variance_scaling', activation='relu', name=\"conv2_2\")\np2 = max_pool_2d(c2, 2)\n\nc3 = conv_2d(p2, 32, 3, weights_init='variance_scaling', activation='relu', name=\"conv3_1\")\nc3 = conv_2d(c3, 32, 3, weights_init='variance_scaling', activation='relu', name=\"conv3_2\")\np3 = max_pool_2d(c3, 2)\n\nc4 = conv_2d(p3, 64, 3, weights_init='variance_scaling', activation='relu', name=\"conv4_1\")\nc4 = conv_2d(c4, 64, 3, weights_init='variance_scaling', activation='relu', name=\"conv4_2\")\np4 = max_pool_2d(c4, 2)\n\nc5 = conv_2d(p4, 128, 3, weights_init='variance_scaling', activation='relu', name=\"conv5_1\")\nc5 = conv_2d(c5, 128, 3, weights_init='variance_scaling', activation='relu', name=\"conv5_2\")\n\nu6 = conv_2d_transpose(c5, 64, 2, [ 16, 16], strides=2)\nu6 = merge([u6, c4], mode='concat', axis=3, name='upsamle-5-merge-4')\nc6 = conv_2d(u6, 64, 3, weights_init='variance_scaling', activation='relu', name=\"conv6_1\")\nc6 = conv_2d(c6, 64, 3, weights_init='variance_scaling', activation='relu', name=\"conv6_2\")\n\nu7 = conv_2d_transpose(c6, 32, 2, [ 32, 32], strides=2)\nu7 = merge([u7, c3], mode='concat', axis=3, name='upsamle-6-merge-3')\nc7 = conv_2d(u7, 32, 3, weights_init='variance_scaling', activation='relu', name=\"conv7_1\")\nc7 = conv_2d(c7, 32, 3, weights_init='variance_scaling', activation='relu', name=\"conv7_2\")\n\nu8 = conv_2d_transpose(c7, 16, 2, [ 64, 64], strides=2)\nu8 = merge([u8, c2], mode='concat', axis=3, name='upsamle-7-merge-2')        \nc8 = conv_2d(u8, 16, 3, weights_init='variance_scaling', activation='relu', name=\"conv8_1\")\nc8 = conv_2d(c8, 16, 3, weights_init='variance_scaling', activation='relu', name=\"conv8_2\")\n\nu9 = conv_2d_transpose(c8,  8, 2, [128,128], strides=2)\nu9 = merge([u9, c1], mode='concat', axis=3, name='upsamle-8-merge-1')\nc9 = conv_2d(u9,  8, 3, weights_init='variance_scaling', activation='relu', name=\"conv9_1\")\nc9 = conv_2d(c9,  8, 3, weights_init='variance_scaling', activation='relu', name=\"conv9_2\")\n\nfc = conv_2d(c9,  1, 1, weights_init='variance_scaling', activation='linear', name=\"target\")\n\n# Define IoU metric\ndef mean_iou_accuracy_op(y_pred, y_true, x):\n    with tf.name_scope('Accuracy'):\n        prec = []\n        for t in np.arange(0.5, 1.0, 0.05):\n            y_pred_tmp = tf.to_int32(y_pred > 0.5)\n            score, update_op = tf.metrics.mean_iou(y_true, y_pred_tmp, 2)\n            with tf.Session() as sess:\n                sess.run(tf.local_variables_initializer())\n            with tf.control_dependencies([update_op]):\n                score = tf.identity(score)\n            prec.append(score)\n        acc = tf.reduce_mean(tf.stack(prec), axis=0, name='mean_iou')\n    return acc\n\nnet = regression(fc,\n                 optimizer='Adam',\n                 loss='binary_crossentropy',\n                 metric=mean_iou_accuracy_op,\n                 learning_rate=0.001\n                )\n\nmodel = tflearn.DNN(net, tensorboard_verbose=3)\n\nstart_time = time.time()\nmodel.fit(trainX, \n          trainY, \n          validation_set=0.1,\n          n_epoch=20,\n          batch_size=16,\n          shuffle=True,\n          show_metric=True,\n          run_id='bowl_unet')\n\nduration = time.time() - start_time\nprint('Training Duration %.3f sec' % (duration))","e6dc552f":"ix = random.randint(0, len(testX)-1)\nimage = testX[ix:ix+1]\ny_pred = model.predict(image)\ny_pred = (y_pred > 0.5).astype(np.uint8).reshape(IMAGE_H, IMAGE_W)\n\nplt.figure(figsize=(10, 10))\n\nplt.subplot(121)\nplt.grid(False)\nplt.imshow(image.reshape(IMAGE_H, IMAGE_W), plt.cm.gray)\nplt.title(\"Image\")\n\nplt.subplot(122)\nplt.grid(False)\nplt.imshow(np.squeeze(y_pred), plt.cm.gray)\nplt.title(\"Predicted Mask\");\ndel image,y_pred","6402d8c8":"# Predict","e2576001":"Quick and dirty kernel shows how to get started on segmenting nuclei using a neural network in TFLearn.\n\nKeras version: https:\/\/www.kaggle.com\/keegil\/keras-u-net-starter-lb-0-277\/notebook\n\n## Paper\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation https:\/\/arxiv.org\/abs\/1505.04597\n\nThe CNN will be built on the training data and applied to the test data.\n\nU-Net architecture flow:\n![u-net-architecture](https:\/\/lmb.informatik.uni-freiburg.de\/Publications\/2015\/RFB15a\/u-net-architecture.png)\n\n## Data\n\nusing data from\u00a0[2018 Data Science Bowl](https:\/\/www.kaggle.com\/c\/data-science-bowl-2018)","7428a683":"# Build and train U-Net network","bedeca95":"## Loss: What difference of binary_crossentropy between Keras and TFLearn\n\n> If the last conv_2d's activity function is 'sigmod'\n\nKeras binary_crossentropy: loss = sigmoid(x)\n```python\nx = sigmoid(x)  # the last conv_2d\ndef binary_crossentropy(x):\n    x = ~sigmoid(x) # undo sigmoid(x), transform back to logits\n    return tf.nn.sigmoid_cross_entropy_with_logits(x)\n```\n\nTFLearn binary_crossentropy: loss = sigmoid(sigmoid(x)) = always 0.693. it's wrong!!!\n```python\nx = sigmoid(x)  # the last conv_2d\ndef binary_crossentropy(x):\n    return tf.nn.sigmoid_cross_entropy_with_logits(x)\n```\n\nshould be\n```\nx = linear(x)  # the last conv_2d\ndef binary_crossentropy(x):\n    return tf.nn.sigmoid_cross_entropy_with_logits(x)\n```","a5a7e7d6":"## Image Augmentation","e849ba6c":"# TFLearn U-Net Starter","d78ba6ed":"# Preparing the Data","19ad41aa":"## Metric\n```python\n# Define IoU metric\ndef mean_iou_accuracy_op(y_pred, y_true, x):\n    with tf.name_scope('Accuracy'):\n        prec = []\n        for t in np.arange(0.5, 1.0, 0.05):\n            y_pred_tmp = tf.to_int32(y_pred > 0.5)\n            score, update_op = tf.metrics.mean_iou(y_true, y_pred_tmp, 2)\n            with tf.Session() as sess:\n                sess.run(tf.local_variables_initializer())\n            with tf.control_dependencies([update_op]):\n                score = tf.identity(score)\n            prec.append(score)\n        acc = tf.reduce_mean(tf.stack(prec), axis=0, name='mean_iou')\n    return acc\n```"}}