{"cell_type":{"81aa7a96":"code","63df3f5c":"code","cb8f04ac":"code","1c829841":"code","2c2a379a":"code","7bb2dbcd":"code","d40b246c":"code","2e914f12":"code","2d661dd6":"code","672ec24b":"code","b1c46ac8":"code","5e601fad":"code","ccf5be1e":"code","6cfac4be":"code","2ec4acfd":"code","6586348f":"code","fb21f942":"code","298a94d2":"code","55721e75":"code","7c1c528f":"code","6994908f":"code","f5585b94":"code","42b1f1eb":"code","95d11c8b":"code","37405e49":"code","5655bf80":"code","13d1fc07":"code","7bb8dd97":"code","0da95923":"code","d131220c":"code","5cbf8fb7":"code","bf91a138":"code","e7283ef1":"code","f1fc0498":"code","1f6c2fba":"code","e80ba685":"code","bcd0a6a4":"code","8cb052e4":"code","8a5e04c0":"code","3d98551e":"code","e45a5312":"code","f6ab364a":"code","4346e0d7":"code","b664954e":"code","c5de5503":"code","3f8327ea":"code","76746a73":"code","147f80d9":"code","afbf7ab6":"code","17712b07":"code","00aca100":"code","f61e0d15":"code","64f06a45":"code","c5ba2f64":"code","cf9f2d20":"code","85fb8315":"code","e23e1c28":"code","f865856d":"code","1e5f818f":"code","548f3ac2":"code","4aacb872":"markdown","4bcffd9e":"markdown","55bf77a7":"markdown","477a728a":"markdown","b56e7d31":"markdown","07f7172a":"markdown","adf12e8c":"markdown","d425adb3":"markdown","81517dc1":"markdown","8dc72ae5":"markdown","8b767e00":"markdown","405cade6":"markdown","8f6bafde":"markdown","a19bc4ea":"markdown","8769c202":"markdown","3e0a03e4":"markdown","8186ef26":"markdown","5252cb89":"markdown","ea497ce4":"markdown"},"source":{"81aa7a96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nfrom datetime import datetime\nimport dateutil\n\nfrom numpy import abs\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63df3f5c":"Train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv\",parse_dates=[\"date\"])","cb8f04ac":"Train","1c829841":"Train_plot = Train.copy()\nfor ind in Train_plot.index.values:\n    #print(ind)\n    if (Train.iloc[ind]['country'] != 'Finland' or\n        Train.iloc[ind]['store'] != 'KaggleMart' or\n        Train.iloc[ind]['product']  != 'Kaggle Mug'):\n        Train_plot = Train.drop(ind,0)\n\n#sns.lineplot('date','num_sold',data=Train_plot,)","2c2a379a":"sns.displot(Train['country'])","7bb2dbcd":"sns.displot(Train['product'])","d40b246c":"\nsns.displot(data=Train, x='store', col='country', height=3, aspect=1.6)\n","2e914f12":"per_month = Train.copy()\nper_month['month'] = Train['date'].dt.month\nper_month['countrystore'] = [Train.iloc[ind]['country'] + Train.iloc[ind]['store'] for ind in Train.index.values]\n\n\nsns.displot(data=per_month, x='month', y='num_sold', row='countrystore', col='product', color='red', height=3, aspect=1.6)\n","2d661dd6":"sns.displot(data=Train,x='num_sold')","672ec24b":"Train_copy = Train.copy()\nTrain_copy['dayofyear'] = Train_copy['date'].dt.dayofyear\nTrain_copy['dayofweek'] = Train_copy['date'].dt.day_of_week\n\nTrain_copy['year'] = Train_copy['date'].dt.year\ndef dtchristmas(date):\n    \n    christmas_curr = pd.Timestamp(date.year,12,25)\n    \n    curr_dt = (date - christmas_curr).days\n    \n    if curr_dt >= -70 and curr_dt <=8:\n        return curr_dt\n    else:\n        return 100\n    \ndef dteaster(date):\n    easter_curr = pd.Timestamp(dateutil.easter.easter(date.year))\n    dt = (date - easter_curr).days\n    if dt >= -1 and dt <=15:\n        return dt\n    else:\n        return 100 \n     \n\nTrain_copy['dtchristmas'] = [dtchristmas(Train_copy.iloc[ind]['date']) for ind in Train_copy.index.values]\nTrain_copy['dteaster'] = [dteaster(Train_copy.iloc[ind]['date']) for ind in Train_copy.index.values]\nTrain_copy['1jan'] = [int(Train_copy.iloc[ind]['date'] == pd.Timestamp(Train_copy.iloc[ind]['date'].year,1,1)) for ind in Train_copy.index.values]\nTrain_copy['wd4'] = Train_copy['date'].dt.day_of_week == 4\nTrain_copy['wd56'] = Train_copy['date'].dt.day_of_week >= 5\n# for k in range(1, 3): # 20\n#         Train_copy[f'sin{k}'] = np.sin(Train_copy['dayofyear'] \/ 365 * 2 * math.pi * k)\n#         Train_copy[f'cos{k}'] = np.cos(Train_copy['dayofyear'] \/ 365 * 2 * math.pi * k)\n#         Train_copy[f'mug_sin{k}'] = Train_copy[f'sin{k}'] * (Train_copy['product'] == 'Kaggle Mug')\n#         Train_copy[f'mug_cos{k}'] = Train_copy[f'cos{k}'] * (Train_copy['product'] == 'Kaggle Mug')\n#         Train_copy[f'sticker_sin{k}'] = Train_copy[f'sin{k}'] * (Train_copy['product'] == 'Kaggle Sticker')\n#         Train_copy[f'sticker_cos{k}'] = Train_copy[f'cos{k}'] * (Train_copy['product'] == 'Kaggle Sticker')\n#         Train_copy[f'sticker_sin{k}'] = Train_copy[f'sin{k}'] * (Train_copy['product'] == 'Kaggle Hat')\n#         Train_copy[f'sticker_cos{k}'] = Train_copy[f'cos{k}'] * (Train_copy['product'] == 'Kaggle Hat')\nTrain_copy","b1c46ac8":"def remove_december(data):\n    data_ = data.copy()\n    for ind in data_.index.values:\n        if data_.iloc[ind]['date'].month == 12:\n            data_.drop(ind,axis=0,inplace=True)\n    return data_\ndef remove_10_jan(data):\n    data_ = data.copy()\n    for ind in data_.index.values:\n        if data_.iloc[ind]['date'].month == 1 and data_.iloc[ind]['date'].day <= 10:\n            data_.drop(ind,axis=0,inplace=True)\n    return data_\ndef remove_easter(data):\n    data_ = data.copy()\n    for ind in data_.index.values:\n        date = data_.iloc[ind]['date']\n        easter_curr = pd.Timestamp(dateutil.easter.easter(date.year))\n        dt = (date - easter_curr).days\n        if dt >= -1 and dt <=15:\n            data_.drop(ind,axis=0,inplace=True)\n    return data_\n\nTrain_dec = Train_copy.query('date.dt.month == 12')\nTrain_clean = pd.concat([Train_copy,Train_dec]).drop_duplicates(keep=False)\nTrain_10jan = Train_copy.query('date.dt.month == 1 and date.dt.day <= 10')\nTrain_clean = pd.concat([Train_clean,Train_10jan]).drop_duplicates(keep=False)\n\nTrain_easter = Train_copy.query('dteaster != 100')\nTrain_clean = pd.concat([Train_clean,Train_easter]).drop_duplicates(keep=False)\n\n\n","5e601fad":"Train_easter","ccf5be1e":"from numpy import abs\nfrom pandas import Timestamp\n\n\nsns.lineplot(x=range(1,63),y=Train_copy.query('country == \"Finland\" and store == \"KaggleMart\" and product == \"Kaggle Mug\" and ((date.dt.year == 2015 and ( date.dt.month ==12)) or (date.dt.year == 2016 and (date.dt.month == 1)))')['num_sold'])","6cfac4be":"sns.heatmap(Train_copy.query('country == \"Finland\" and store == \"KaggleMart\" and product == \"Kaggle Mug\"').corr(),annot=True)\n\n","2ec4acfd":"cnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_copy.query('country == @cnt and store == @stor and product == @prod and date.dt.year == 2015')\nF_KM_KM_2016 = Train_copy.query('country == @cnt and store == @stor and product == @prod and date.dt.year == 2016')\nF_KM_KM_2017 = Train_copy.query('country == @cnt and store == @stor and product == @prod and date.dt.year == 2017')\nF_KM_KM_2018 = Train_copy.query('country == @cnt and store == @stor and product == @prod and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True) #, 'dteaster', 'dtchristmas', '1jan'\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_true_2015 = np.array(F_KM_KM_2015)\nsample_x_true_2016 = np.array(F_KM_KM_2016)\nsample_x_true_2017 = np.array(F_KM_KM_2017)\nsample_x_true_2018 = np.array(F_KM_KM_2018)\n\nsample_y_true_2015 = np.array(F_KM_KM_2015_y)\nsample_y_true_2016 = np.array(F_KM_KM_2016_y)\nsample_y_true_2017 = np.array(F_KM_KM_2017_y)\nsample_y_true_2018 = np.array(F_KM_KM_2018_y)\n\nmax_true_2015 = np.max(np.array(sample_y_true_2015))\nmax_true_2016 = np.max(np.array(sample_y_true_2016))\nmax_true_2017 = np.max(np.array(sample_y_true_2017))\nmax_true_2018 = np.max(np.array(sample_y_true_2018))\n\nsample_y_true_2015 = sample_y_true_2015 \/ max_true_2015\nsample_y_true_2016 = sample_y_true_2016 \/ max_true_2016\nsample_y_true_2017 = sample_y_true_2017 \/ max_true_2017\nsample_y_true_2018 = sample_y_true_2018 \/ max_true_2018","6586348f":"\ncnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_clean.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2015')\nF_KM_KM_2016 = Train_clean.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2016')\nF_KM_KM_2017 = Train_clean.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2017')\nF_KM_KM_2018 = Train_clean.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_2015 = np.array(F_KM_KM_2015)\nsample_x_2016 = np.array(F_KM_KM_2016)\nsample_x_2017 = np.array(F_KM_KM_2017)\nsample_x_2018 = np.array(F_KM_KM_2018)\n\nsample_y_2015 = np.array(F_KM_KM_2015_y)\nsample_y_2016 = np.array(F_KM_KM_2016_y)\nsample_y_2017 = np.array(F_KM_KM_2017_y)\nsample_y_2018 = np.array(F_KM_KM_2018_y)\n\nsample_y_2015 = sample_y_2015 \/ max_true_2015\nsample_y_2016 = sample_y_2016 \/ max_true_2016\nsample_y_2017 = sample_y_2017 \/ max_true_2017\nsample_y_2018 = sample_y_2018 \/ max_true_2018","fb21f942":"\ncnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_dec.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2015')\nF_KM_KM_2016 = Train_dec.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2016')\nF_KM_KM_2017 = Train_dec.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2017')\nF_KM_KM_2018 = Train_dec.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_dec_2015 = np.array(F_KM_KM_2015)\nsample_x_dec_2016 = np.array(F_KM_KM_2016)\nsample_x_dec_2017 = np.array(F_KM_KM_2017)\nsample_x_dec_2018 = np.array(F_KM_KM_2018)\n\nsample_y_dec_2015 = np.array(F_KM_KM_2015_y)\nsample_y_dec_2016 = np.array(F_KM_KM_2016_y)\nsample_y_dec_2017 = np.array(F_KM_KM_2017_y)\nsample_y_dec_2018 = np.array(F_KM_KM_2018_y)\n","298a94d2":"cnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_easter.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2015')\nF_KM_KM_2016 = Train_easter.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2016')\nF_KM_KM_2017 = Train_easter.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2017')\nF_KM_KM_2018 = Train_easter.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_easter_2015 = np.array(F_KM_KM_2015)\nsample_x_easter_2016 = np.array(F_KM_KM_2016)\nsample_x_easter_2017 = np.array(F_KM_KM_2017)\nsample_x_easter_2018 = np.array(F_KM_KM_2018)\n\nsample_y_easter_2015 = np.array(F_KM_KM_2015_y)\nsample_y_easter_2016 = np.array(F_KM_KM_2016_y)\nsample_y_easter_2017 = np.array(F_KM_KM_2017_y)\nsample_y_easter_2018 = np.array(F_KM_KM_2018_y)","55721e75":"cnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_10jan.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2015')\nF_KM_KM_2016 = Train_10jan.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2016')\nF_KM_KM_2017 = Train_10jan.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2017')\nF_KM_KM_2018 = Train_10jan.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_10jan_2015 = np.array(F_KM_KM_2015)\nsample_x_10jan_2016 = np.array(F_KM_KM_2016)\nsample_x_10jan_2017 = np.array(F_KM_KM_2017)\nsample_x_10jan_2018 = np.array(F_KM_KM_2018)\n","7c1c528f":"sample_x = np.concatenate((sample_x_2017,sample_x_2016,sample_x_2015),axis = 0)\nsample_y = np.concatenate((sample_y_2017,sample_y_2016,sample_y_2015),axis = 0)\n\nsample_x_true = np.concatenate((sample_x_true_2017,sample_x_true_2016,sample_x_true_2015),axis = 0)\nsample_y_true = np.concatenate((sample_y_true_2017,sample_y_true_2016,sample_y_true_2015),axis = 0)\n\nsample_x_dec = np.concatenate((sample_x_dec_2017,sample_x_dec_2016,sample_x_dec_2015),axis = 0)\nsample_y_dec = np.concatenate((sample_y_dec_2017,sample_y_dec_2016,sample_y_dec_2015),axis = 0)\n\nsample_x_easter = np.concatenate((sample_x_easter_2017,sample_x_easter_2016,sample_x_easter_2015),axis = 0)\nsample_x_10jan = np.concatenate((sample_x_10jan_2017,sample_x_10jan_2016,sample_x_10jan_2015),axis = 0)","6994908f":"from sklearn import svm\nfrom sklearn.linear_model import HuberRegressor, LinearRegression, Ridge\nfrom sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\ndef chosen_model():\n    #return Ridge()\n    return GradientBoostingRegressor(loss=\"huber\",max_depth=4,n_estimators=70,alpha=0.95,random_state=1)\n    #return svm.SVR(kernel=\"rbf\", C=10000, gamma=0.25, epsilon=1e-10, tol=1e-10)\n    #return AdaBoostRegressor(n_estimators = 1000, loss='square', random_state = 1)\nsample_regr = chosen_model()\nsample_regr.fit(sample_x, sample_y)\n","f5585b94":"deltay_2018 = sample_y_true_2018-sample_regr.predict(sample_x_true_2018)\ndeltay = sample_y_true-sample_regr.predict(sample_x_true)\ng = sns.lineplot(range(1,1097),deltay)\ng.axhline(0,color='red')","42b1f1eb":"len(deltay)","95d11c8b":"deltay_dec = np.zeros(len(sample_x_dec))\nfor i in range(len(sample_x_dec)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_dec[i]):\n            deltay_dec[i] = deltay[j]\n            break\n\n","37405e49":"sns.lineplot(range(1,94),deltay_dec)","5655bf80":"deltay_dec_2017 = np.zeros(31)\nj=0\nfor i in range(len(sample_x_dec)):\n    if sample_x_dec[i][2] == 2017:\n        deltay_dec_2017[j] = deltay_dec[i]\n        j += 1\n\n","13d1fc07":"sns.lineplot(range(1,32),deltay_dec_2017)","7bb8dd97":"dec_regr = chosen_model()\ndec_regr.fit(sample_x_dec, deltay_dec)","0da95923":"def predict_dec(sample,sample_dec):  \n    y = np.zeros(len(sample))\n    y = sample_regr.predict(sample)\n    dec_residual = dec_regr.predict(sample_dec)\n    \n    for i in range(len(sample)):\n        if sample[i][0] >= 335:\n            for j in range(len(sample_dec)):\n                if np.all(sample[i] == sample_dec[j]):\n                    y[i] +=  dec_residual[j]\n                    break\n    return y","d131220c":"dec_predict_2018 = predict_dec(sample_x_true_2018,sample_x_dec_2018)\n\nlen(dec_predict_2018)\nsns.lineplot(range(1,366),dec_predict_2018)\nsns.lineplot(range(1,366),sample_y_true_2018,style=2)","5cbf8fb7":"dec_predict = predict_dec(sample_x_true,sample_x_dec)\ndeltay = sample_y_true - dec_predict #Updating residuals\nsns.lineplot(range(1,1097),deltay)","bf91a138":"deltay_easter = np.zeros(len(sample_x_easter))\nfor i in range(len(sample_x_easter)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_easter[i]):\n            deltay_easter[i] = deltay[j]\n            break\n\n","e7283ef1":"deltay_easter_2017 = np.array([])\nj=0\nfor i in range(len(sample_x_easter)):\n    if sample_x_easter[i][2] == 2017:\n        deltay_easter_2017 = np.append(deltay_easter_2017,deltay_easter[i])\n        j += 1\n\nlen(deltay_easter_2017)","f1fc0498":"sns.lineplot(range(17),deltay_easter_2017)","1f6c2fba":"easter_regr = chosen_model()\neaster_regr.fit(sample_x_easter, deltay_easter)","e80ba685":"sample_x_easter_2018","bcd0a6a4":"def predict_easter(sample,sample_easter,y):  \n    y = y.copy()\n    #y = predict_dec(sample,sample_dec)\n    easter_residual = easter_regr.predict(sample_easter)\n    \n    for i in range(len(sample)):\n        if sample[i][0] >= 86 and sample[i][0] <= 121:\n            for j in range(len(sample_easter)):\n                if np.all(sample[i] == sample_easter[j]):\n                    y[i] +=  easter_residual[j]\n                    break\n    return y","8cb052e4":"easter_predict_2018 = predict_easter(sample_x_true_2018,sample_x_easter_2018,dec_predict_2018)\n\nlen(easter_predict_2018)\nsns.lineplot(range(1,366),easter_predict_2018)\nsns.lineplot(range(1,366),sample_y_true_2018,style=2)","8a5e04c0":"easter_predict = predict_easter(sample_x_true,sample_x_easter, dec_predict)\ndeltay = sample_y_true - easter_predict #Updating residuals","3d98551e":"sns.lineplot(range(1,1097), easter_predict)","e45a5312":"deltay_10jan = np.zeros(len(sample_x_10jan))\nfor i in range(len(sample_x_10jan)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_10jan[i]):\n            deltay_10jan[i] = deltay[j]\n            break\n\nlen(deltay_10jan)","f6ab364a":"sns.lineplot(range(30),deltay_10jan)","4346e0d7":"deltay_10jan_2017 = np.array([])\nj=0\nfor i in range(len(sample_x_10jan)):\n    if sample_x_10jan[i][2] == 2017:\n        deltay_10jan_2017 = np.append(deltay_10jan_2017,deltay_10jan[i])\n        j += 1\n\nlen(deltay_10jan_2017)","b664954e":"sns.lineplot(range(10),deltay_10jan_2017)","c5de5503":"jan10_regr = chosen_model()\njan10_regr.fit(sample_x_10jan, deltay_10jan)","3f8327ea":"def predict_10jan(sample,sample_jan10,y):  \n    y = y.copy()\n    jan10_residual = jan10_regr.predict(sample_jan10)\n    \n    for i in range(len(sample)):\n        if sample[i][0] <= 10:\n            for j in range(len(sample_jan10)):\n                if np.all(sample[i] == sample_jan10[j]):\n                    y[i] +=  jan10_residual[j]\n                    break\n    return y","76746a73":"jan10_predict_2018 = predict_10jan(sample_x_10jan_2018,sample_x_10jan_2018,easter_predict_2018)\nprint(np.all(jan10_predict_2018 == easter_predict_2018))\nlen(jan10_predict_2018)\nsns.lineplot(range(1,366),jan10_predict_2018)\n#sns.lineplot(range(1,366),easter_predict_2018)\n#sns.lineplot(range(1,366),sample_y_true_2018,style=2)","147f80d9":"from sklearn.metrics import mean_squared_error\n\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.round(np.mean(diff),5)\n\n#print(mean_squared_error(sample_y_2018,sample_mean))\nprint(SMAPE(sample_y_true_2018,(sample_regr.predict(sample_x_true_2018))))\nprint(SMAPE(sample_y_true_2018,dec_predict_2018))\nprint(SMAPE(sample_y_true_2018,easter_predict_2018))\nprint(SMAPE(sample_y_true_2018,jan10_predict_2018))\nprint(mean_squared_error(sample_y_2018,sample_regr.predict(sample_x_2018)))","afbf7ab6":"from sklearn.linear_model import LinearRegression\n\nmax_model = LinearRegression()\nmax_model.fit(np.array(range(2015,2018)).reshape(-1,1),[max_true_2015,max_true_2016,max_true_2017])\n\n(max_model.predict(np.array(2018).reshape(1,-1))[0])\n\n","17712b07":"country = 'Norway'\nstore = 'KaggleMart'\nproduct = 'Kaggle Sticker'\n\n\nF_KM_KM_2015 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\nF_KM_KM_2016 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\nF_KM_KM_2017 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\nF_KM_KM_2018 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True) #, 'dteaster', 'dtchristmas', '1jan'\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_true_2015 = np.array(F_KM_KM_2015)\nsample_x_true_2016 = np.array(F_KM_KM_2016)\nsample_x_true_2017 = np.array(F_KM_KM_2017)\nsample_x_true_2018 = np.array(F_KM_KM_2018)\n\nsample_y_true_2015 = np.array(F_KM_KM_2015_y)\nsample_y_true_2016 = np.array(F_KM_KM_2016_y)\nsample_y_true_2017 = np.array(F_KM_KM_2017_y)\nsample_y_true_2018 = np.array(F_KM_KM_2018_y)\n\nmax_true_2015 = np.max(np.array(sample_y_true_2015))\nmax_true_2016 = np.max(np.array(sample_y_true_2016))\nmax_true_2017 = np.max(np.array(sample_y_true_2017))\nmax_true_2018 = np.max(np.array(sample_y_true_2018))\n\nsample_y_true_2015 = sample_y_true_2015 \/ max_true_2015\nsample_y_true_2016 = sample_y_true_2016 \/ max_true_2016\nsample_y_true_2017 = sample_y_true_2017 \/ max_true_2017\nsample_y_true_2018 = sample_y_true_2018 \/ max_true_2018\n\nF_KM_KM_2015 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\nF_KM_KM_2016 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\nF_KM_KM_2017 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\nF_KM_KM_2018 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_2015 = np.array(F_KM_KM_2015)\nsample_x_2016 = np.array(F_KM_KM_2016)\nsample_x_2017 = np.array(F_KM_KM_2017)\nsample_x_2018 = np.array(F_KM_KM_2018)\n\nsample_y_2015 = np.array(F_KM_KM_2015_y)\nsample_y_2016 = np.array(F_KM_KM_2016_y)\nsample_y_2017 = np.array(F_KM_KM_2017_y)\nsample_y_2018 = np.array(F_KM_KM_2018_y)\n\nsample_y_2015 = sample_y_2015 \/ max_true_2015\nsample_y_2016 = sample_y_2016 \/ max_true_2016\nsample_y_2017 = sample_y_2017 \/ max_true_2017\nsample_y_2018 = sample_y_2018 \/ max_true_2018\n\nF_KM_KM_2015 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\nF_KM_KM_2016 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\nF_KM_KM_2017 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\nF_KM_KM_2018 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_dec_2015 = np.array(F_KM_KM_2015)\nsample_x_dec_2016 = np.array(F_KM_KM_2016)\nsample_x_dec_2017 = np.array(F_KM_KM_2017)\nsample_x_dec_2018 = np.array(F_KM_KM_2018)\n\nF_KM_KM_2015 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\nF_KM_KM_2016 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\nF_KM_KM_2017 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\nF_KM_KM_2018 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_10jan_2015 = np.array(F_KM_KM_2015)\nsample_x_10jan_2016 = np.array(F_KM_KM_2016)\nsample_x_10jan_2017 = np.array(F_KM_KM_2017)\nsample_x_10jan_2018 = np.array(F_KM_KM_2018)\n\nsample_x = np.concatenate((sample_x_2017,sample_x_2016,sample_x_2015),axis = 0)\nsample_y = np.concatenate((sample_y_2017,sample_y_2016,sample_y_2015),axis = 0)\n\nsample_x_true = np.concatenate((sample_x_true_2017,sample_x_true_2016,sample_x_true_2015),axis = 0)\nsample_y_true = np.concatenate((sample_y_true_2017,sample_y_true_2016,sample_y_true_2015),axis = 0)\n\nsample_x_dec = np.concatenate((sample_x_dec_2017,sample_x_dec_2016,sample_x_dec_2015),axis = 0)\nsample_y_dec = np.concatenate((sample_y_dec_2017,sample_y_dec_2016,sample_y_dec_2015),axis = 0)\n\nsample_x_easter = np.concatenate((sample_x_easter_2017,sample_x_easter_2016,sample_x_easter_2015),axis = 0)\nsample_x_10jan = np.concatenate((sample_x_10jan_2017,sample_x_10jan_2016,sample_x_10jan_2015),axis = 0)\n\n\nsample_regr = chosen_model()\nsample_regr.fit(sample_x, sample_y)\n\ndeltay = sample_y_true-sample_regr.predict(sample_x_true)\n\ndeltay_dec = np.zeros(len(sample_x_dec))\nfor i in range(len(sample_x_dec)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_dec[i]):\n            deltay_dec[i] = deltay[j]\n            break\n\ndec_regr = chosen_model()\ndec_regr.fit(sample_x_dec, deltay_dec)\n\n\n\ndec_predict = predict_dec(sample_x_true,sample_x_dec)\ndeltay = sample_y_true - dec_predict #Updating residuals\n\ndeltay_easter = np.zeros(len(sample_x_easter))\nfor i in range(len(sample_x_easter)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_easter[i]):\n            deltay_easter[i] = deltay[j]\n            break\n\neaster_regr = chosen_model()\neaster_regr.fit(sample_x_easter, deltay_easter)\n\n\neaster_predict = predict_easter(sample_x_true,sample_x_easter, dec_predict)\ndeltay = sample_y_true - easter_predict #Updating residuals\n\ndeltay_10jan = np.zeros(len(sample_x_10jan))\nfor i in range(len(sample_x_10jan)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_10jan[i]):\n            deltay_10jan[i] = deltay[j]\n            break\n\n\njan10_regr = chosen_model()\njan10_regr.fit(sample_x_10jan, deltay_10jan)\n\n\nmax_model = LinearRegression()\nmax_model.fit(np.array(range(2015,2019)).reshape(-1,1),[max_true_2015,max_true_2016,max_true_2017,max_true_2018])\n","00aca100":"# country = 'Finland'\n# store = 'KaggleRama'\n# product = 'Kaggle Mug'\n\n\n# sample_regr, dec_regr, easter_regr, jan10_regr, max_model = model[country][store][product]#learn_product(country,store,product)\n\n# F_KM_KM_2018 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n# F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n# F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n# sample_x_true_2018 = np.array(F_KM_KM_2018)\n# sample_y_true_2018 = np.array(F_KM_KM_2018_y)\n# max_true_2018 = np.max(np.array(sample_y_true_2018))\n# sample_y_true_2018 = sample_y_true_2018 \/ max_true_2018\n\n# row_arr = sample_x_true_2018\n# #y = np.zeros(len(row_arr))\n# #for i = range(len(row_arr))\n\n# y = sample_regr.predict(row_arr)\n# for i in range(len(row_arr)):\n#     if row_arr[i][0] >= 335:\n#         y[i] = y[i] + dec_regr.predict([row_arr[i]])\n#     elif row_arr[i][0] <= 10:\n#         y[i] += jan10_regr.predict([row_arr[i]])\n#     elif row_arr[0][4] != 100:\n#         y[i] += easter_regr.predict([row_arr[i]])\n# year_max = max_model.predict([[2018]])[0]\n\n# #dec_predict_2018 = predict_dec(sample_x_true_2018,sample_x_dec_2018)\n# #easter_predict_2018 = predict_easter(sample_x_true_2018,sample_x_easter_2018,dec_predict_2018)\n# #sns.lineplot(x=range(1,366),y=easter_predict_2018)\n# #y = predict_10jan(sample_x_true_2018,sample_x_10jan_2018,easter_predict_2018)\n\n# print(year_max)\n\n# #\n# sns.lineplot(x=range(1,366),y=y)\n# #sns.lineplot(x=range(1,366),y=dec_regr.predict(row_arr))\n# #sns.lineplot(x=range(1,366),y=dec_regr.predict(row_arr) + y)\n# sns.lineplot(x=range(1,366),y=sample_y_true_2018)\n# print(SMAPE(sample_y_true_2018*max_true_2018,y*year_max))","f61e0d15":"def learn_product(country,store,product):\n\n    F_KM_KM_2015 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\n    F_KM_KM_2016 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\n    F_KM_KM_2017 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\n    F_KM_KM_2018 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\n    F_KM_KM_2015_y = F_KM_KM_2015['num_sold']\n    F_KM_KM_2016_y = F_KM_KM_2016['num_sold']\n    F_KM_KM_2017_y = F_KM_KM_2017['num_sold']\n    F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\n    F_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True) #, 'dteaster', 'dtchristmas', '1jan'\n    F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\n    sample_x_true_2015 = np.array(F_KM_KM_2015)\n    sample_x_true_2016 = np.array(F_KM_KM_2016)\n    sample_x_true_2017 = np.array(F_KM_KM_2017)\n    sample_x_true_2018 = np.array(F_KM_KM_2018)\n\n    sample_y_true_2015 = np.array(F_KM_KM_2015_y)\n    sample_y_true_2016 = np.array(F_KM_KM_2016_y)\n    sample_y_true_2017 = np.array(F_KM_KM_2017_y)\n    sample_y_true_2018 = np.array(F_KM_KM_2018_y)\n\n    max_true_2015 = np.max(np.array(sample_y_true_2015))\n    max_true_2016 = np.max(np.array(sample_y_true_2016))\n    max_true_2017 = np.max(np.array(sample_y_true_2017))\n    max_true_2018 = np.max(np.array(sample_y_true_2018))\n\n    sample_y_true_2015 = sample_y_true_2015 \/ max_true_2015\n    sample_y_true_2016 = sample_y_true_2016 \/ max_true_2016\n    sample_y_true_2017 = sample_y_true_2017 \/ max_true_2017\n    sample_y_true_2018 = sample_y_true_2018 \/ max_true_2018\n\n    F_KM_KM_2015 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\n    F_KM_KM_2016 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\n    F_KM_KM_2017 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\n    F_KM_KM_2018 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\n    F_KM_KM_2015_y = F_KM_KM_2015['num_sold']\n    F_KM_KM_2016_y = F_KM_KM_2016['num_sold']\n    F_KM_KM_2017_y = F_KM_KM_2017['num_sold']\n    F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\n    F_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\n    sample_x_2015 = np.array(F_KM_KM_2015)\n    sample_x_2016 = np.array(F_KM_KM_2016)\n    sample_x_2017 = np.array(F_KM_KM_2017)\n    sample_x_2018 = np.array(F_KM_KM_2018)\n\n    sample_y_2015 = np.array(F_KM_KM_2015_y)\n    sample_y_2016 = np.array(F_KM_KM_2016_y)\n    sample_y_2017 = np.array(F_KM_KM_2017_y)\n    sample_y_2018 = np.array(F_KM_KM_2018_y)\n\n    sample_y_2015 = sample_y_2015 \/ max_true_2015\n    sample_y_2016 = sample_y_2016 \/ max_true_2016\n    sample_y_2017 = sample_y_2017 \/ max_true_2017\n    sample_y_2018 = sample_y_2018 \/ max_true_2018\n\n    F_KM_KM_2015 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\n    F_KM_KM_2016 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\n    F_KM_KM_2017 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\n    F_KM_KM_2018 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\n    F_KM_KM_2015_y = F_KM_KM_2015['num_sold']\n    F_KM_KM_2016_y = F_KM_KM_2016['num_sold']\n    F_KM_KM_2017_y = F_KM_KM_2017['num_sold']\n    F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\n    F_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\n    sample_x_dec_2015 = np.array(F_KM_KM_2015)\n    sample_x_dec_2016 = np.array(F_KM_KM_2016)\n    sample_x_dec_2017 = np.array(F_KM_KM_2017)\n    sample_x_dec_2018 = np.array(F_KM_KM_2018)\n\n    F_KM_KM_2015 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\n    F_KM_KM_2016 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\n    F_KM_KM_2017 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\n    F_KM_KM_2018 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\n    F_KM_KM_2015_y = F_KM_KM_2015['num_sold']\n    F_KM_KM_2016_y = F_KM_KM_2016['num_sold']\n    F_KM_KM_2017_y = F_KM_KM_2017['num_sold']\n    F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\n    F_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\n    sample_x_10jan_2015 = np.array(F_KM_KM_2015)\n    sample_x_10jan_2016 = np.array(F_KM_KM_2016)\n    sample_x_10jan_2017 = np.array(F_KM_KM_2017)\n    sample_x_10jan_2018 = np.array(F_KM_KM_2018)\n\n    sample_x = np.concatenate((sample_x_2017,sample_x_2016,sample_x_2015),axis = 0)\n    sample_y = np.concatenate((sample_y_2017,sample_y_2016,sample_y_2015),axis = 0)\n\n    sample_x_true = np.concatenate((sample_x_true_2017,sample_x_true_2016,sample_x_true_2015),axis = 0)\n    sample_y_true = np.concatenate((sample_y_true_2017,sample_y_true_2016,sample_y_true_2015),axis = 0)\n\n    sample_x_dec = np.concatenate((sample_x_dec_2017,sample_x_dec_2016,sample_x_dec_2015),axis = 0)\n    sample_y_dec = np.concatenate((sample_y_dec_2017,sample_y_dec_2016,sample_y_dec_2015),axis = 0)\n\n    sample_x_easter = np.concatenate((sample_x_easter_2017,sample_x_easter_2016,sample_x_easter_2015),axis = 0)\n    sample_x_10jan = np.concatenate((sample_x_10jan_2017,sample_x_10jan_2016,sample_x_10jan_2015),axis = 0)\n\n\n    sample_regr = chosen_model()\n    sample_regr.fit(sample_x, sample_y)\n\n    deltay = sample_y_true-sample_regr.predict(sample_x_true)\n\n    deltay_dec = np.zeros(len(sample_x_dec))\n    for i in range(len(sample_x_dec)):\n        for j in range(len(sample_x_true)):\n            if np.all(sample_x_true[j] == sample_x_dec[i]):\n                deltay_dec[i] = deltay[j]\n                break\n\n    dec_regr = chosen_model()\n    dec_regr.fit(sample_x_dec, deltay_dec)\n\n\n\n    dec_predict = predict_dec(sample_x_true,sample_x_dec)\n    deltay = sample_y_true - dec_predict #Updating residuals\n\n    deltay_easter = np.zeros(len(sample_x_easter))\n    for i in range(len(sample_x_easter)):\n        for j in range(len(sample_x_true)):\n            if np.all(sample_x_true[j] == sample_x_easter[i]):\n                deltay_easter[i] = deltay[j]\n                break\n\n    easter_regr = chosen_model()\n    easter_regr.fit(sample_x_easter, deltay_easter)\n\n\n    easter_predict = predict_easter(sample_x_true,sample_x_easter, dec_predict)\n    deltay = sample_y_true - easter_predict #Updating residuals\n\n    deltay_10jan = np.zeros(len(sample_x_10jan))\n    for i in range(len(sample_x_10jan)):\n        for j in range(len(sample_x_true)):\n            if np.all(sample_x_true[j] == sample_x_10jan[i]):\n                deltay_10jan[i] = deltay[j]\n                break\n\n\n    jan10_regr = chosen_model()\n    jan10_regr.fit(sample_x_10jan, deltay_10jan)\n\n\n    max_model = LinearRegression()\n    max_model.fit(np.array(range(2015,2019)).reshape(-1,1),[max_true_2015,max_true_2016,max_true_2017,max_true_2018])\n\n    return sample_regr, dec_regr, easter_regr, jan10_regr, max_model\n\ndef fit_model():\n    country_dict = {}\n    for country in ['Finland','Sweden','Norway']:\n        curr_country_dict = {}\n        for store in ['KaggleMart','KaggleRama']:\n            curr_store_dict = {}\n            for product in ['Kaggle Mug','Kaggle Sticker', 'Kaggle Hat']:\n                curr_store_dict[product] = learn_product(country,store,product)\n            curr_country_dict[store] = curr_store_dict\n        country_dict[country] = curr_country_dict\n    return country_dict","64f06a45":"model = fit_model()","c5ba2f64":"Test  = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv\",parse_dates=[\"date\"])\n\n\nTest_copy = Test.copy()\nTest_copy['dayofyear'] = Test_copy['date'].dt.dayofyear\nTest_copy['dayofweek'] = Test_copy['date'].dt.day_of_week\n\nTest_copy['year'] = Test_copy['date'].dt.year\n\nTest_copy['dtchristmas'] = [dtchristmas(Test_copy.iloc[ind]['date']) for ind in Test_copy.index.values]\nTest_copy['dteaster'] = [dteaster(Test_copy.iloc[ind]['date']) for ind in Test_copy.index.values]\nTest_copy['1jan'] = [int(Test_copy.iloc[ind]['date'] == pd.Timestamp(Test_copy.iloc[ind]['date'].year,1,1)) for ind in Test_copy.index.values]\nTest_copy['wd4'] = Test_copy['date'].dt.day_of_week == 4\nTest_copy['wd56'] = Test_copy['date'].dt.day_of_week >= 5\n# for k in range(1, 3): # 20\n#         Test_copy[f'sin{k}'] = np.sin(Test_copy['dayofyear'] \/ 365 * 2 * math.pi * k)\n#         Test_copy[f'cos{k}'] = np.cos(Test_copy['dayofyear'] \/ 365 * 2 * math.pi * k)\n#         Test_copy[f'mug_sin{k}'] = Test_copy[f'sin{k}'] * (Test_copy['product'] == 'Kaggle Mug')\n#         Test_copy[f'mug_cos{k}'] = Test_copy[f'cos{k}'] * (Test_copy['product'] == 'Kaggle Mug')\n#         Test_copy[f'sticker_sin{k}'] = Test_copy[f'sin{k}'] * (Test_copy['product'] == 'Kaggle Sticker')\n#         Test_copy[f'sticker_cos{k}'] = Test_copy[f'cos{k}'] * (Test_copy['product'] == 'Kaggle Sticker')\n#         Test_copy[f'sticker_sin{k}'] = Test_copy[f'sin{k}'] * (Test_copy['product'] == 'Kaggle Hat')\n#         Test_copy[f'sticker_cos{k}'] = Test_copy[f'cos{k}'] * (Test_copy['product'] == 'Kaggle Hat')\n        ","cf9f2d20":"def predict_data(model,row):\n    country = row['country']\n    store = row['store']\n    product = row['product']\n    #return country,store,product\n    sample_regr, dec_regr, easter_regr, jan10_regr, max_model = model[country][store][product] \n    \n    row_arr = row.copy()\n    row_arr.drop(['date', 'country', 'store', 'product', 'row_id'],inplace=True)\n    row_arr = np.array(row_arr)\n    row_arr = [row_arr]\n    \n    #product_model = sample_regr, dec_regr, easter_regr, jan10_regr, max_model\n    #return row_arr\n    y = sample_regr.predict(row_arr)\n    if row_arr[0][0] >= 335:\n        y += dec_regr.predict(row_arr)\n    elif row_arr[0][0] <= 10:\n        y += jan10_regr.predict(row_arr)\n    elif row_arr[0][4] != 100:\n        y += easter_regr.predict(row_arr)\n    year_max = max_model.predict([[2018]])[0]\n    \n    return y[0] * year_max\n    ","85fb8315":"#SMAPE(Train_copy['num_sold'],[predict_data(model,Train_copy.iloc[ind]) for ind in Train_copy.index.values])","e23e1c28":"Result_DF = pd.DataFrame()\nResult_DF['row_id'] = Test['row_id']\nResult_DF['num_sold'] = [predict_data(model,Test_copy.iloc[ind]) for ind in Test_copy.index.values]","f865856d":"Result_DF['num_sold'] = np.round(Result_DF['num_sold']).astype(int)","1e5f818f":"Result_DF","548f3ac2":"Result_DF.to_csv('submission.csv',index=False)","4aacb872":"## **Input Data**:","4bcffd9e":"### Fitting a SVR to the data:","55bf77a7":"## Adding Holiday and day-of-week features:","477a728a":"Distribution of Data:","b56e7d31":"Seems Good! Now applying to to all country\/store\/product combinations (3x2x3 = 18 different combinations)","07f7172a":"## Submission","adf12e8c":"## Prediction","d425adb3":"The error is decent.","81517dc1":" Holidays appear to affect the sales gravely.","8dc72ae5":"Sales distribution","8b767e00":"### Correcting for December","405cade6":"We can see that predictibly the largest residual is situated at the hollidays.","8f6bafde":"# **Fitting one part of the data**:","a19bc4ea":"# **Exploratory Data Analysis**","8769c202":"Segregating December and first 10 days of January","3e0a03e4":"### Correcting for Easter","8186ef26":"The plan is as follows: Each country\/store\/product combination is treated seperately. I get the sales data per day for each year and divide by the average sales for that year. Then a SVR is fitted to the data and the next sales average is extrapolated via linear regression.","5252cb89":"Fitting the Linear regressor for the year-by-year average","ea497ce4":"### Extracting Data:"}}