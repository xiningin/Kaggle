{"cell_type":{"b3b9304a":"code","2310e591":"code","55f52cb8":"code","82d7fecb":"code","e695785e":"code","9323d74a":"code","820dc498":"code","40f26bfd":"code","be4a8ecb":"code","3d48f49f":"code","f655599d":"code","734bcefb":"code","84a4b248":"code","fdb2c9e8":"code","2d69f60e":"code","26a7827d":"code","42f558f7":"code","80e7090e":"code","ed86cd9e":"code","ace3b2f5":"code","28110b74":"code","62fbb0e4":"code","773207be":"code","342d5de3":"code","59a56f21":"code","8503a27b":"code","218ffc97":"code","d322f3fb":"code","4a24ee3b":"code","250a79a8":"code","8914459e":"code","76b5efaf":"code","3d573e7f":"code","c4d4246a":"code","b81b9eb8":"code","9739ff84":"code","03726b03":"code","9a68532b":"code","5f736169":"code","c442d747":"code","49b60817":"code","723cc1c3":"code","af99b6b7":"code","1e8c331c":"code","45198f65":"code","6135b36b":"code","22b486c6":"code","16537213":"code","08ea1cdc":"code","ec662042":"code","9a9e9800":"code","e78a9af3":"code","d88374ad":"code","c75754c7":"code","be40c0ba":"code","37c9dc7e":"code","ae8e79de":"code","d710b74d":"code","5d8b6b32":"code","19ea53e6":"code","421860d2":"code","739d7415":"code","d9937b65":"code","26847d9b":"code","aced183d":"code","8e0c6321":"code","f0afbfd7":"code","4aa2996e":"code","eedd2380":"code","29813875":"code","8e88f2ed":"code","bab3a906":"code","55a21f8a":"code","327050b6":"code","e30c19b8":"code","1aa23aa8":"code","be49ce93":"code","3012f8d8":"code","f1fba7b2":"code","ce8490f0":"code","049b5bfe":"code","6e8ae399":"code","29e3ac45":"code","a4397912":"code","ecf2daaa":"code","a7bf68d6":"code","e407177d":"code","48317624":"code","1f059625":"code","b5c48ba8":"code","160b96ec":"code","8d48099b":"code","3e9cd388":"code","bb5d8185":"code","98910fc8":"code","06ec8811":"code","df5b58cd":"code","067ba9c1":"code","2532fedb":"code","39f40526":"code","194f920d":"code","f19db283":"code","7ae607c4":"code","a9a29dca":"code","c73f0449":"code","4ae48e6f":"code","70ab29f7":"code","5ac1cb61":"code","6354e0a8":"code","a9489ee9":"code","9d43d702":"code","fe738bcd":"code","83392952":"code","1737b785":"code","0a0aa93b":"code","4cf8c0b1":"code","02f14ce7":"code","1d16d7ff":"code","75a46a79":"code","3ff088e3":"code","e1351372":"code","f61acf07":"code","80bfbbb4":"code","19a777b2":"code","caf6635b":"code","b559a88e":"code","76602b5f":"code","92000877":"code","2ed2ef00":"code","fa6564e7":"code","af260c45":"code","97257a05":"code","6b1ffcc2":"code","406796ab":"code","c76a2d10":"code","e2c9b6b5":"code","20010a78":"code","0f4bf3af":"code","019597c4":"code","21403250":"code","33f7c018":"code","26604549":"code","b62895a0":"code","e9da6351":"code","9fdd8518":"code","c37d8ed7":"code","f6b4e3f1":"code","4feed0bb":"code","601bb189":"code","690c73de":"code","e19fcdec":"code","33cf8f1a":"code","f919ad1b":"markdown","51fa7bc5":"markdown","312b0584":"markdown","f5685b60":"markdown","0dc66b6a":"markdown","adb2af88":"markdown","8561f37c":"markdown","b166c2b0":"markdown","142a9644":"markdown","b54ef789":"markdown","539206d2":"markdown","6bf2cde4":"markdown","3e3ba3d3":"markdown","e92aa908":"markdown","8c38e5d0":"markdown","3cf5e124":"markdown","8bfdd930":"markdown","120f328b":"markdown","8a5dbb65":"markdown","8389ad63":"markdown","4082b87f":"markdown","5d50b829":"markdown","c1d9260d":"markdown","ca40af8d":"markdown","b65b7a8d":"markdown","c2da9404":"markdown","39243454":"markdown","16c9b61d":"markdown","7ecea959":"markdown","d4f616ab":"markdown","c2adce3e":"markdown","dbce71fa":"markdown","10ae4842":"markdown","e40698c5":"markdown","64b4db28":"markdown","59b23e34":"markdown","231d86ce":"markdown","94499aec":"markdown","1df270d6":"markdown","9729069c":"markdown","2ef5ade2":"markdown","93670eee":"markdown","4c2aede7":"markdown","286e8384":"markdown","891da5a4":"markdown","765f15f8":"markdown","42ea1aba":"markdown","d19b29ef":"markdown","37f537ba":"markdown","98f9a662":"markdown","0fb733bb":"markdown","f5f91ed1":"markdown","f85476d5":"markdown","5ac6b345":"markdown","f2cd8483":"markdown","0630ed38":"markdown","11bebf77":"markdown","6ac6a80b":"markdown","561fb253":"markdown","31f6c909":"markdown","65bef89a":"markdown","49396cd0":"markdown","8eb9e426":"markdown","24809113":"markdown","50ab6f27":"markdown","476a9ba6":"markdown","8bd97be1":"markdown","74273e54":"markdown","2ce174c9":"markdown","a0f43556":"markdown","a8ce557a":"markdown","2edcbe8c":"markdown","630d0f10":"markdown","14ea17f5":"markdown","bb62f6b7":"markdown","ed8c0214":"markdown"},"source":{"b3b9304a":"# Suppress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2310e591":"# Importing required libraries for now\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","55f52cb8":"# Display all 37 columns\npd.set_option('display.max_columns', 50)","82d7fecb":"# Importing Lead dataset\ndf = pd.read_csv(\"..\/input\/lead-scoring-case-study\/Leads.csv\")\ndf.head(4)","e695785e":"# Check shape of the dataframe 'df'\ndf.shape","9323d74a":"# Check the summary statistics of the dataframe 'df'\ndf.describe()","820dc498":"# Check column data types and nullability\ndf.info()","40f26bfd":"df.isnull().sum().sort_values(ascending=False)","be4a8ecb":"# Printing the list of columns that need to be removed\nfor col in df.columns:\n    if df[col].isnull().sum() > 3000:\n        print(col, df[col].isnull().sum())","3d48f49f":"for col in df.columns:\n    if df[col].isnull().sum() > 3000:\n        df.drop(col, axis=1, inplace=True)","f655599d":"# Checking if the shape has changed post column removal\ndf.shape","734bcefb":"df.drop(['Country','City'], axis=1, inplace=True)","84a4b248":"df.shape","fdb2c9e8":"df.drop(['Prospect ID','Lead Number'], axis=1, inplace=True)","2d69f60e":"df.shape","26a7827d":"# Checking distinct values for each column\nfor x in df.columns:\n    print(df[x].astype('category').value_counts())\n    print(\"\\n\\n--------------------------------------------------------------------------\\n\\n\")","42f558f7":"# Adding the to-be dropped columns to a list\ndrop_cols_var = ['Do Not Call','What matters most to you in choosing a course','Search','Magazine','Newspaper Article',\n                'X Education Forums','Newspaper','Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses',\n                'Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque']\ndrop_cols_select = ['How did you hear about X Education','Lead Profile']","80e7090e":"print(len(drop_cols_var))\nprint(len(drop_cols_select))","ed86cd9e":"# Dropping less variance columns\ndf.drop(drop_cols_var, axis=1, inplace=True)\n\n# Dropping columns with 'select' values >= 50%\ndf.drop(drop_cols_select, axis=1, inplace=True)","ace3b2f5":"# Checking the count of remaining columns post removal\ndf.shape","28110b74":"# Checking top 4 rows of \"df\"\ndf.head(4)","62fbb0e4":"df.isnull().sum().sort_values(ascending=False)","773207be":"df['What is your current occupation'].value_counts(ascending=False)","342d5de3":"df['What is your current occupation'].isna().sum()","59a56f21":"# Getting the NOT NULL values from the column 'What is your current occupation' and assigning it back to 'df'\ndf = df[~pd.isna(df['What is your current occupation'])]","8503a27b":"# Checking the rows left in \"df\"\ndf.shape","218ffc97":"df.isnull().sum().sort_values(ascending=False)","d322f3fb":"df['TotalVisits'].isna().sum()","4a24ee3b":"# Getting the NOT NULL values from the column 'TotalVisits' and assigning it back to 'df'\ndf = df[~pd.isna(df['TotalVisits'])]","250a79a8":"# Checking the rows left in \"df\"\ndf.shape","8914459e":"df.isnull().sum().sort_values(ascending=False)","76b5efaf":"df['Lead Source'].isna().sum()","3d573e7f":"# Getting the NOT NULL values from the column 'Lead Source' and assigning it back to 'df'\ndf = df[~pd.isna(df['Lead Source'])]","c4d4246a":"# Checking the rows left in \"df\"\ndf.shape","b81b9eb8":"df.isnull().sum().sort_values(ascending=False)","9739ff84":"df['Specialization'].isna().sum()","03726b03":"# Getting the NOT NULL values from the column 'Specialization' and assigning it back to 'df'\ndf = df[~pd.isna(df['Specialization'])]","9a68532b":"# Checking the rows left in \"df\"\ndf.shape","5f736169":"df.isnull().sum().sort_values(ascending=False)","c442d747":"# Checking top 5 rows in the dataframe \"df\"\ndf.head()","49b60817":"df.info()","723cc1c3":"df['Do Not Email'].value_counts()","af99b6b7":"df['A free copy of Mastering The Interview'].value_counts()","1e8c331c":"# List of binary variables\nvarlist = ['Do Not Email','A free copy of Mastering The Interview']\n\n# Map function for conversion\ndef binary_map(x):\n    return x.map({'Yes':1,'No':0})\n\n# Apply Map function to the binary variables\ndf[varlist] = df[varlist].apply(binary_map)","45198f65":"# Check if the binary variables are converted to 1's and 0's\ndf.head(3)","6135b36b":"df.info()","22b486c6":"# Get the list of categorical variables\ndf.select_dtypes(include='object').columns","16537213":"# Checking distinct values for each categorical column\nfor x in df.select_dtypes(include='object').columns:\n    print(df[x].value_counts())\n    print(\"\\n\\n--------------------------------------------------------------------------\\n\\n\")","08ea1cdc":"# Create dummy variables for all columns except 'Specialization' and drop the first one\ndummy1 = pd.get_dummies(df[['Lead Origin', 'Lead Source', 'Last Activity', \n                                 'What is your current occupation', 'Last Notable Activity']], drop_first=True)\n\n# Add the results to the dataframe\ndf = pd.concat([df,dummy1], axis=1)","ec662042":"# Checking if the dummy variables have been added to the DataFrame 'df'\ndf.head(2)","9a9e9800":"# Creating dummy variables for \"Specialization\" and dropping the level with 'select'.\n\n# Creating dummy variables for the variable 'Specialization'\nml = pd.get_dummies(df['Specialization'], prefix='Specialization')\n# Dropping \"Specialization_Select\" column\nml1 = ml.drop(['Specialization_Select'], 1)\n#Adding the results to the master dataframe 'df'\ndf = pd.concat([df,ml1], axis=1)","e78a9af3":"df.head()","d88374ad":"df.drop(['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization',\n       'What is your current occupation', 'Last Notable Activity'], axis=1, inplace=True)","c75754c7":"df.shape","be40c0ba":"df.head(3)","37c9dc7e":"# Add feature variables to X\nX = df.drop('Converted', axis=1)","ae8e79de":"# Add response variable to y\ny = df['Converted']","d710b74d":"from sklearn.model_selection import train_test_split","5d8b6b32":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","19ea53e6":"df.info()","421860d2":"df.head()","739d7415":"df.select_dtypes(exclude='uint8')","d9937b65":"# Getting the list of numerical features from df\nnum_feat = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\nprint(num_feat)\nprint(len(num_feat))","26847d9b":"# Import sklearn library for StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Instantiate an object of StandardScaler\nscaler = MinMaxScaler()\n\n# Perform fit and transform on the X_train dataset\nX_train[num_feat] = scaler.fit_transform(X_train[num_feat])\n\n# Perform only transform on the X_test dataset\nX_test[num_feat] = scaler.transform(X_test[num_feat])","aced183d":"# Checking Lead Conversion Rate\nconversion_rate = (sum(df['Converted']) \/ len(df['Converted'].index)) * 100\n\nconversion_rate","8e0c6321":"# This is to mask the contents of the uppper half of the heatmap\nmask = np.array(df.corr())\n\nmask","f0afbfd7":"corr_df1 = df.corr()\ncorr_df1.head()","4aa2996e":"# Looking at places where the correlation is above 60%\nplt.figure(figsize = (20,12),dpi=100)\nmask[np.tril_indices_from(mask)] = False\nsns.heatmap(data = corr_df1[(corr_df1 >= 0.6) & (corr_df1 <= 0.9999)], annot = True, \n            cmap = \"RdYlGn\", cbar = True, fmt='.2f', mask=mask)\nplt.show()","eedd2380":"corr_df1[(corr_df1 >= 0.6) & (corr_df1 <= 0.9999)].unstack().sort_values(ascending = False).head(18)","29813875":"# Import library from statsmodels\nimport statsmodels.api as sm","8e88f2ed":"logm1 = sm.GLM(y_train, (sm.add_constant(X_train)),  family=sm.families.Binomial())","bab3a906":"logm1.fit().summary()","55a21f8a":"# Importing LogisticRegression from sklearn\nfrom sklearn.linear_model import LogisticRegression","327050b6":"logreg = LogisticRegression(solver='liblinear')","e30c19b8":"# Importing RFE from sklearn\nfrom sklearn.feature_selection import RFE","1aa23aa8":"# Selection of 15 features by RFE\nrfe = RFE(logreg, 15)\nrfe = rfe.fit(X_train, y_train)","be49ce93":"# Looking at the feature names selected by RFE\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","3012f8d8":"# Get only the columns selected by RFE\ncol = X_train.columns[ rfe.support_ ]","f1fba7b2":"col","ce8490f0":"# Columns not selected by RFE\nX_train.columns[~rfe.support_]","049b5bfe":"X_train_sm = sm.add_constant(X_train[col])","6e8ae399":"# Second model\nlogm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())","29e3ac45":"# Fitting the model\nres = logm2.fit()","a4397912":"# Checking the summary of model statistics\nres.summary()","ecf2daaa":"# import variance_inflation_factor from statsmodels\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","a7bf68d6":"# Create a dataframe that contains the names of all the feature variables and their corresponding VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by = 'VIF', ascending=False)\nvif","e407177d":"col = col.drop('Lead Source_Reference', 1)\nlen(col)","48317624":"X_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","1f059625":"# import variance_inflation_factor from statsmodels\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","b5c48ba8":"# Create a dataframe that contains the names of all the feature variables and their corresponding VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by = 'VIF', ascending=False)\nvif","160b96ec":"col = col.drop('Last Notable Activity_Had a Phone Conversation', 1)\nlen(col)","8d48099b":"X_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","3e9cd388":"col = col.drop('What is your current occupation_Housewife', 1)\nlen(col)","bb5d8185":"X_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","98910fc8":"col = col.drop('What is your current occupation_Working Professional', 1)\nlen(col)","06ec8811":"X_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","df5b58cd":"# Create a dataframe that contains the names of all the feature variables and their corresponding VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by = 'VIF', ascending=False)\nvif","067ba9c1":"X_train_sm.shape","2532fedb":"# Predicting probabilities on the Train set\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","39f40526":"y_train_pred_final = pd.DataFrame({\"Converted\": y_train.values, \"Conversion_Prob\": y_train_pred})\ny_train_pred_final.head()","194f920d":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","f19db283":"# For evaluation metrics, import metrics from sklearn\nfrom sklearn import metrics","7ae607c4":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","a9a29dca":"# Confusion Matrix\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","c73f0449":"# Evaluation of other metrics than accuracy\n\nTP = confusion[1,1] # True Positive\nTN = confusion[0,0] # True Negative\nFP = confusion[0,1] # False Positive\nFN = confusion[1,0] # False Negative","4ae48e6f":"# Calculating Sensitivity of the model\nTP \/ (TP + FN)","70ab29f7":"# Calculating Specificity of the model\nTN \/ (TN + FP)","5ac1cb61":"# Calculating False Positive rate (i.e. Predicting Churn when the Customer has not Churned)\nprint(FP \/ float(TN + FP))","6354e0a8":"# Calculating Positive Predictive rate (i.e. Rate of identifying Actual Churn as Churn) (also called \"Precision\")\nprint(TP \/ float(TP + FP))","a9489ee9":"# Calculating Negative Predictive rate (i.e. Rate of identifying Actual Non-Churn as Non-Churn)\nprint(TN \/ float(TN + FN))","9d43d702":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","fe738bcd":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, drop_intermediate = False )","83392952":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","1737b785":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","0a0aa93b":"y_train_pred_final.head(20)","4cf8c0b1":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","02f14ce7":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","1d16d7ff":"y_train_pred_final['Final_Predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.43 else 0)\n\ny_train_pred_final.head()","75a46a79":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted)","3ff088e3":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted )\nconfusion2","e1351372":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","f61acf07":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","80bfbbb4":"# Let us calculate specificity\nTN \/ float(TN+FP)","19a777b2":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","caf6635b":"# Positive predictive value \nprint (TP \/ float(TP+FP))","b559a88e":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","76602b5f":"# Creating a confusion matrix\nconf = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted )\nconf","92000877":"# Precision\nconfusion[1,1]\/(confusion[0,1]+confusion[1,1])","2ed2ef00":"# Recall\nconfusion[1,1]\/(confusion[1,0]+confusion[1,1])","fa6564e7":"# Import precision_recall_curve from sklearn\nfrom sklearn.metrics import precision_recall_curve","af260c45":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","97257a05":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","6b1ffcc2":"X_test = X_test[col]\nX_test.head()","406796ab":"# Adding intercept to the test set\nX_test_sm = sm.add_constant(X_test)","c76a2d10":"y_test_pred = res.predict(X_test_sm)","e2c9b6b5":"y_test_pred[:10]","20010a78":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","0f4bf3af":"# Let's see the head\ny_pred_1.head()","019597c4":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","21403250":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","33f7c018":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","26604549":"y_pred_final.head()","b62895a0":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Conversion_Prob'})\n\ny_pred_final.head()","e9da6351":"# As optimal cut-off point is 0.43 (where \"Sensitivity\" and \"Specificity\" intersect)\ny_pred_final['Final_Predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.43 else 0)","9fdd8518":"y_pred_final.head()","c37d8ed7":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.Final_Predicted)","f6b4e3f1":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.Final_Predicted )\nconfusion2","4feed0bb":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","601bb189":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","690c73de":"# Let us calculate specificity\nTN \/ float(TN+FP)","e19fcdec":"y_pred_final['Lead Score'] = y_pred_final['Conversion_Prob'].apply(lambda x: round(x*100, 1))","33cf8f1a":"y_pred_final.head(20)","f919ad1b":"Total columns to be dropped are ***15***","51fa7bc5":"An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\n\nA typical lead conversion process can be represented using the following funnel:\n<img style=\"float: right;\" src=\"https:\/\/cdn.upgrad.com\/UpGrad\/temp\/189f213d-fade-4fe4-b506-865f1840a25a\/XNote_201901081613670.jpg\">","312b0584":"## Looking at Correlations","f5685b60":"As this seems to be an important predictor, we will remove the rows which are NULL from the dataframe `df`","0dc66b6a":"# Data Cleaning","adb2af88":"# Modeling","8561f37c":"# Model Evaluation","b166c2b0":"City and Country columns have been removed from \"df\"","142a9644":"### `Lead Source` - handling NULL values","b54ef789":"Column to be dropped with high p-value - `What is your current occupation_Housewife`","539206d2":"Only *6* columns have NULL values now","6bf2cde4":"# Lead Scoring Case Study","3e3ba3d3":"## Precision and Recall trade-off","e92aa908":"## Making Predictions on the test set","8c38e5d0":"We can see that ***6*** columns have been removed from `df`","3cf5e124":"An ROC (Receiver Operating Characteristic) curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","8bfdd930":"# Model Performance","120f328b":"### Dropping columns with NULL values above 3000 (~32%)","8a5dbb65":"## Model 4","8389ad63":"As this seems to be an important predictor, we will remove the rows which are NULL from the dataframe `df`","4082b87f":"# Model Building","5d50b829":"We are left with ***14*** columns in the dataframe `df`","c1d9260d":"We will also remove the columns `Prospect ID` and `Lead Number` as these are simply unique identifiers for the customers and lead. Due to this, they will not provide any value addition to our model","ca40af8d":"### Check VIFs","b65b7a8d":"## Evaluation","c2da9404":"We have almost ~48% conversion rate","39243454":"As the company sells online courses, it doesn't really matter which country or city the students are from. So, both `City` and `Country` columns are not necessary for this analysis and hence we will be removing them from the dataframe \"df\"","16c9b61d":"As this seems to be an important predictor, we will remove the rows which are NULL from the dataframe `df`","7ecea959":"### `TotalVisits` - handling NULL values","d4f616ab":"# Data Preparation","c2adce3e":"## Create new column \"Predicted\" with 1 if 'Conversion_Prob' > 0.5 else 0","dbce71fa":"## Precision and Recall","10ae4842":"## For categorical features with multiple levels, create dummy features (One-Hot Encoding)","e40698c5":"## Model 5","64b4db28":"Even with Precision and Recall, we get the optimal cut-off at `0.43`","59b23e34":"## Create a dataframe with y_train (target value) and y_train_pred (predicted probability)","231d86ce":"## `0.43` is the optimal cut-off point based on the intersection values in above curve","94499aec":"Handling dummy variable creation for 'Specialization' column","1df270d6":"## Finding the Optimal cut-off point","9729069c":"### Check VIFs","2ef5ade2":"Since 0.5 is an arbitrary cut-off chosen by us, we want to find the Optimal cut-off point where sensitivity and specificity are balanced","93670eee":"### `Specialization` - handling NULL values","4c2aede7":"All p-values are now < 0.05. We will again check VIF to make sure it's less than 5","286e8384":"We have the Area Under the Curve as 0.86 which is a good model. So, we will go ahead and find the Optimal Cut-off Point based on Sensivitivity V\/S Specificity trade-off","891da5a4":"## Model 6","765f15f8":"## Checking NULL values with remaining columns","42ea1aba":"We can see that all VIF values are less than 5. <br>\nSo, we will start dropping columns with high p-values: <br>\nColumn to be dropped - `Last Notable Activity_Had a Phone Conversation`","d19b29ef":"### `What is your current occupation` - handling NULL values","37f537ba":"There are **75** columns post dummy variable creation","98f9a662":"Column to be dropped with high p-value - `What is your current occupation_Working Professional`","0fb733bb":"| Actual\/Predicted | Not Converted | Converted |\n| --- | --- | --- |\n| Not Converted | 1929   | 383 |\n| Converted | 560     | 1589 |","f5f91ed1":"There are 3 variables with high VIF. It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex.\nWe will start by dropping the variable 'Lead Source_Reference' as it has high VIF and high p-value (> 0.05)","f85476d5":"## Model 1\nAssessing the model with statsmodels","5ac6b345":"We are not left with any NULL values in the columns","f2cd8483":"## Feature Selection using RFE","0630ed38":"# Importing and Understanding data","11bebf77":"## Model 2","6ac6a80b":"## Converting binary categorical variables (Yes,No) to (1,0)","561fb253":"As the dataset has **74** feature variables, we will get a subset of 15 important features using RFE. Before that, let's look at the model with all variables","31f6c909":"## Removing unnecessary columns","65bef89a":"### Check VIFs","49396cd0":"| Train\/Test | Accuracy | Sensitivity | Specificity |\n| --- | --- | --- | --- |\n| Train | 0.7895  | 0.7854 | 0.7932 | \n| Test  | 0.7850  | 0.7751 | 0.7941 |","8eb9e426":"# Calculating Lead Score","24809113":"## Train | Test Split and Scaling","50ab6f27":"## Model3","476a9ba6":"## Drop the categorical variables for which we have created dummies","8bd97be1":"## Plotting ROC curve","74273e54":"## Handling columns with less variance & columns with large values of 'select'","2ce174c9":"Getting the list of columns with 60% or above correlations","a0f43556":"As this seems to be an important predictor, we will remove the rows which are NULL from the dataframe `df`","a8ce557a":"After going through the above results manually, we will be dropping the following columns based on the two criterion mentioned below:\n\n**Below columns with very less variance should be dropped**\n- 'Do Not Call','What matters most to you in choosing a course','Search','Magazine','Newspaper Article','X Education Forums','Newspaper','Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses','Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque'\n                \n**Below columns have >= 50% \"select\" values and they should be dropped**\n- 'How did you hear about X Education','Lead Profile'","2edcbe8c":"- There are columns where the variance is very less (i.e. most of the values are either Yes\/No). In such cases, the model will not be able to provide any useful results because there is no learning for the model. We will remove these columns from `df`.\n- Additionally, there are columns where large portion of the data is having the value 'select'. This means that the user did not select any value for the field. We will check and remove such columns as well from `df`.","630d0f10":"## Feature Scaling (Min Max Scaler)","14ea17f5":"The above table contains the lead score which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted","bb62f6b7":"## Handling NULL data","ed8c0214":"We will create dummy variables for all columns except `Specialization` as it still contains \"select\" and must be handled separately"}}