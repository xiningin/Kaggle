{"cell_type":{"513d85b8":"code","372ff136":"code","3cd5eea2":"code","2f1f9311":"code","32c14369":"code","cf71bbf4":"code","4bee7ba9":"code","57516c10":"code","2ed3ecfe":"code","ea3fa1bb":"code","f2a5d574":"code","6166d996":"code","21b12c1d":"code","628d9493":"code","93408a49":"code","1264dd8a":"code","725058ab":"code","df246fde":"code","905bc631":"code","880de51d":"code","58ce8d7d":"code","e27d00b4":"code","34ebd1ba":"code","a6e29dfb":"code","a3cf1ee9":"code","bad05aaf":"code","25349037":"code","7eae6ad1":"code","3b52801d":"code","d61f6b42":"code","4ef86df2":"code","faebdeee":"code","3ae50efc":"code","a54bf881":"code","e2d97587":"code","f046c46f":"code","7e5e8549":"code","2af9c8bb":"code","b12ee572":"code","364e3d7e":"code","a2dc0a87":"code","27f485c9":"code","7231ec1b":"code","c36674de":"code","aa06fb2e":"code","b79f80db":"code","06dd596f":"code","bd24d7d8":"code","fe76de96":"code","59b2f501":"code","16bc0f0e":"code","d1106d5c":"code","e92109e1":"code","f1459bfc":"code","a3084c7e":"code","8e28d535":"code","85ad03f8":"code","716353d8":"code","8b014689":"code","ece65d5b":"code","da202400":"code","e9d8147b":"code","592e8a9c":"code","6c452679":"code","3196e3cb":"code","51b27daf":"code","140b1c1d":"code","28954bb3":"code","88acaa7b":"code","13ab0417":"code","7ff59544":"code","7d643de3":"code","a2219132":"code","df4b8db1":"code","1ae2c391":"markdown","313577f9":"markdown","cd890a4a":"markdown","fbefed10":"markdown","7449ac05":"markdown","738a55ba":"markdown","d89a7898":"markdown","7e73eff3":"markdown","b0c822fa":"markdown","67537f9a":"markdown","f31c5513":"markdown","d1d6ac9a":"markdown","354c261c":"markdown","2c7d8329":"markdown","9f76f37e":"markdown","b68f6245":"markdown"},"source":{"513d85b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","372ff136":"# Importing all tool we need \n\n# Regular EDA (exploratory data analysis) and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we want our plots to appear inside the notebook\n%matplotlib inline \n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# Model Evaluations\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","3cd5eea2":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.shape  # (row, column)","2f1f9311":"df.head()","32c14369":"df.tail()","cf71bbf4":"df[\"target\"].value_counts()","4bee7ba9":"df[\"target\"].value_counts().plot(kind = \"bar\", color = [\"salmon\", \"lightblue\"]);","57516c10":"df.info()","2ed3ecfe":"# check for missing value \ndf.isna().sum()","ea3fa1bb":"df.describe()","f2a5d574":"df.sex.value_counts()","6166d996":"# Compare target column with sex column\n\npd.crosstab(df.target, df.sex)","21b12c1d":"# Create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind = \"bar\",figsize=(10, 6), color = [\"salmon\", \"lightblue\"] )\nplt.title(\"Heart Disease Frequency for sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\" Amount\")\n\nplt.legend([\"Female\", \"Male\"])\n\nplt.xticks(rotation = 0);","628d9493":"df.head()","93408a49":"df[\"thalach\"].value_counts()","1264dd8a":"# Create another figure \nplt.figure(figsize=(10, 6))\n\n# Scatter with positive example \nplt.scatter(df.age[df.target == 1], df.thalach[df.target ==1],\n           c = \"salmon\")\n\n# Scatter with negative example \n\nplt.scatter(df.age[df.target == 0], df.thalach[df.target==0],\n           c = \"lightblue\")\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","725058ab":"# Check the distribution of the age column with a histgram\ndf.age.plot.hist();\n","df246fde":"pd.crosstab(df.cp, df.target)","905bc631":"# Make the crosstab more visual\n\npd.crosstab(df.cp, df.target).plot(kind = \"bar\",\n                                  figsize = (10, 6),\n                                  color = [\"salmon\", \"lightblue\"])\n\n# Add some communication\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation=0);","880de51d":"df.head(2)","58ce8d7d":"# Make a correlation matrix \ndf.corr()","e27d00b4":"# Let's make our correlation matrix a little prettier\ncorr_matrix = df.corr()\n\nfig, ax = plt.subplots(figsize = (15, 10))\n\nax = sns.heatmap(corr_matrix, \n                annot=True,\n                linewidths= 0.5,\n                fmt=\"0.2f\",\n                cmap=\"YlGnBu\");\n# if bottom an top have some cut off portion\n# bottom, top = ax.get_ylim()\n# ax.set_ylim(bottom + 0.5, top - 0.5)","34ebd1ba":"df.head(2)","a6e29dfb":"# split the data into x and y\nX = df.drop(\"target\", axis = 1)\ny = df[\"target\"]","a3cf1ee9":"X","bad05aaf":"y","25349037":"# Split data into train and test sets\nnp.random.seed(42)\n\n# Split into train & test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n","7eae6ad1":"X_train","3b52801d":"y_train","d61f6b42":"# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","4ef86df2":"algos = [LogisticRegression(solver=\"liblinear\"), SVC(),GaussianNB(),\n         KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier()]\n\nnames = [\"LogisticRegression\", \"SVC\",\"Naive Bayes\",\"KNN\",\"Decision Tree Classifier\",\"Random Forest\"]","faebdeee":"models = {\"Logistic Regression\": LogisticRegression(solver=\"liblinear\"),\n          \"SVC\": SVC(),\n          \"Naive Bayes\":GaussianNB(),\n          \"KNN\": KNeighborsClassifier(),\n          \"Decision Tree Classifier\":DecisionTreeClassifier(),\n          \"Random Forest\": RandomForestClassifier()}","3ae50efc":"model_scores = {}\n\nfor name, model in models.items():\n    \n    model.fit(X_train, y_train)\n    # Evaluate the model and append its score to model_scores\n    model_scores[name] = model.score(X_test, y_test)\n    ","a54bf881":"evaluation = pd.DataFrame(model_scores, index=[\"Accuracy\"])\nevaluation","e2d97587":"evaluation","f046c46f":"evaluation.T.plot.bar()\n# Put a legend to the right of the current axis\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))","7e5e8549":"# Let's tune KNN\n\ntrain_scores = []\ntest_scores = []\n\n# Create a list of a different values for n_neighbours\nneighbors = range(1, 21)\n\n# Setup KNN instance \nknn = KNeighborsClassifier()\n\n#Loop through different n_neighbors\n\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    \n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update the training score list\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update the test score list\n    test_scores.append(knn.score(X_test, y_test))","2af9c8bb":"train_scores","b12ee572":"test_scores","364e3d7e":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","a2dc0a87":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","27f485c9":"# Tune Logistic Regression\n# Setup random hyperparameter search for logisticRegression\nrs_log_reg = RandomizedSearchCV(estimator=LogisticRegression(),\n                               param_distributions=log_reg_grid,\n                               cv = 5,\n                               n_iter=20,\n                               verbose=True)\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","7231ec1b":"rs_log_reg.best_params_","c36674de":"rs_log_reg.best_estimator_","aa06fb2e":"rs_log_reg.score(X_test, y_test)","b79f80db":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","06dd596f":"# Find the best hyperparameters\nrs_rf.best_params_","bd24d7d8":"# Evaluate the randomized search RandomForestClassifier model\nrs_rf.score(X_test, y_test)","fe76de96":"# Different hyperparameters for our LogisticRegression model\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","59b2f501":"# Check the best hyperparmaters\ngs_log_reg.best_params_","16bc0f0e":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(X_test, y_test)","d1106d5c":"# Make predictions with tuned model \ny_preds = gs_log_reg.predict(X_test)","e92109e1":"y_preds","f1459bfc":"y_test","a3084c7e":"# Plot ROC curve and calculate and calculat\n\nplot_roc_curve(gs_log_reg, X_test, y_test);","8e28d535":"# Confusion matrix\n\nprint(confusion_matrix(y_test, y_preds))","85ad03f8":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True,\n                     cbar=False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \n    # bottom, top = ax.get_ylim()\n    # ax.set_ylim(bottom + 0.5, top - 0.5)\n    \nplot_conf_mat(y_test, y_preds)","716353d8":"print(classification_report(y_test, y_preds))","8b014689":"# Check best hyperparameters\ngs_log_reg.best_params_","ece65d5b":"# Create a new classifier with best parameter \n\nclf = LogisticRegression(C = 0.20433597178569418, \n                        solver= \"liblinear\")","da202400":"from sklearn.model_selection import cross_val_score","e9d8147b":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf, \n                        X,\n                        y,\n                        cv = 5,\n                        scoring=\"accuracy\")\n\ncv_acc","592e8a9c":"cv_acc=np.mean(cv_acc)\ncv_acc","6c452679":"# Cross-validated precision \ncv_precision = cross_val_score(clf, \n                              X,\n                              y,\n                              cv= 5,\n                              scoring=\"precision\")\n\ncv_precision = np.mean(cv_precision)\ncv_precision","3196e3cb":"# Cross-validated recoll\ncv_recall = cross_val_score(clf, \n                              X,\n                              y,\n                              cv= 5,\n                              scoring=\"recall\")\n\ncv_recall = np.mean(cv_recall)\ncv_recall","51b27daf":"# Cross-validated f1-score\ncv_f1 = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1","140b1c1d":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                           \"Precision\": cv_precision,\n                           \"Recall\": cv_recall,\n                           \"F1\": cv_f1},\n                          index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n                      legend=False);","28954bb3":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")\n\nclf.fit(X_train, y_train);","88acaa7b":"# Check coef_\nclf.coef_","13ab0417":"df.head()","7ff59544":"# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","7d643de3":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\", legend=False);","a2219132":"pd.crosstab(df[\"sex\"], df[\"target\"])","df4b8db1":"pd.crosstab(df[\"slope\"], df[\"target\"])","1ae2c391":"## Evaluting our tuned machine learning classifier, beyond accuracy\n\n* ROC curve and AUC score\n* Confusion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score\n\n... and it would be great if cross-validation was used where possible.\n\nTo make comparisons and evaluate our trained model, first we need to make predictions.","313577f9":"### Heart Disease Frequency per Chest Pain Type\n\n3. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease","cd890a4a":"## Hyperparameter tuning with RandomizedSearchCV\n\nWe're going to tune:\n* LogisticRegression()\n* RandomForestClassifier()\n\n... using RandomizedSearchCV","fbefed10":"### Heart Disease Frequency according to Sex","7449ac05":"Now we've got our data split into training and test sets, it's time to build a machine learning model.\n\nWe'll train it (find the patterns) on the training set.\n\nAnd we'll test it (use the patterns) on the test set.\n\nWe're going to try 6 different machine learning models:\n1. Logistic Regression \n2. SVC\n3. Naive Bayes\n4. K-Nearest Neighbours Classifier\n5. Decision Tree Classifier\n6. Random Forest Classifier","738a55ba":"Now we've got hyperparameter grids setup for each of our models, let's tune them using RandomizedSearchCV...","d89a7898":"### Model Comparision ","7e73eff3":"slope - the slope of the peak exercise ST segment\n* 0: Upsloping: better heart rate with excercise (uncommon)\n* 1: Flatsloping: minimal change (typical healthy heart)\n* 2: Downslopins: signs of unhealthy heart","b0c822fa":"### Modelling ","67537f9a":"### Age vs. Max Heart Rate for Heart Disease","f31c5513":"**Create data dictionary**\n\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)","d1d6ac9a":"Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n\nLet's look at the following:\n* Hypyterparameter tuning\n* Feature importance\n* Confusion matrix\n* Cross-validation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the curve (AUC)\n\n### Hyperparameter tuning (manually)","354c261c":"### Calculate evaluation metrics using cross-validation\n\nWe're going to calculate accuracy, precision, recall and f1-score of our model using cross-validation and to do so we'll be using `cross_val_score()`.","2c7d8329":"### Hyperparamter Tuning with GridSearchCV\nSince our LogisticRegression model provides the best scores so far, we'll try and improve them again using GridSearchCV...","9f76f37e":"Now we've tuned LogisticRegression(), let's do the same for RandomForestClassifier()...","b68f6245":"### Feature Importance\n\nFeature importance is another as asking, \"which features contributed most to the outcomes of the model and how did they contribute?\"\n\nFinding feature importance is different for each machine learning model. One way to find feature importance is to search for \"(MODEL NAME) feature importance\".\n\nLet's find the feature importance for our LogisticRegression model..."}}