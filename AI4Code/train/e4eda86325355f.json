{"cell_type":{"0913bb6b":"code","c19de530":"code","761a9a44":"code","e9a63422":"code","7928cb8b":"code","77c8ff60":"code","79b3cfbe":"code","9847a6d7":"code","7f04badd":"code","690e6de8":"code","6b88f754":"code","011e8015":"code","58d3cb6b":"code","e65a82da":"code","82d0c707":"code","8914e23b":"code","5e94a264":"code","7d0dec18":"code","ede2490d":"code","6318b7f8":"markdown","b91ee0bf":"markdown","a01b6129":"markdown","0c3a0bcf":"markdown","42c7398e":"markdown","b31c1721":"markdown","cb73a506":"markdown","dbec660d":"markdown","adc86eed":"markdown","5e686bb0":"markdown","da0c6dae":"markdown","5bdeb98f":"markdown","b5d854c6":"markdown","aeddf85f":"markdown","8600d1e4":"markdown","da99f227":"markdown","505f0b55":"markdown","7d30c71d":"markdown","5a0bf6ae":"markdown","3e493cdc":"markdown","25e51473":"markdown","ee1d2ac3":"markdown","204c2625":"markdown","841295b1":"markdown","4c9f590d":"markdown","69279fb0":"markdown","72e6b365":"markdown","129798be":"markdown","de08d63b":"markdown","c53a8316":"markdown"},"source":{"0913bb6b":"# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c19de530":"!pip install rank_bm25","761a9a44":"\nimport json\nimport re\nimport sys\nimport pandas as pd\nimport os\nimport pprint\nimport datetime as dt\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import wordnet\nimport numpy as np\n\nimport gensim\nfrom gensim.parsing.preprocessing import strip_tags, strip_punctuation,strip_non_alphanum, strip_multiple_whitespaces,stem_text, strip_numeric, remove_stopwords \nfrom gensim.summarization import summarize\nfrom gensim import similarities\nfrom gensim.summarization.textcleaner import split_sentences\nimport pyLDAvis.gensim\nimport collections\n\nimport joblib\nfrom rank_bm25 import BM25Okapi, BM25Plus\nimport timeit\nimport ipywidgets as widgets\nfrom IPython.display import display , HTML\nfrom jinja2 import Template\nimport json\nfrom json import JSONEncoder\nfrom scipy import spatial\n\nimport warnings\nwarnings.filterwarnings('ignore')","e9a63422":"current_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\ndata_dir =  current_dir + \"\/input\/data-covid\/structured\"\nversion = 1\nmodel_dir = current_dir + \"\/input\/model-covid\/model\/v\"+str(version)\nview_dir = current_dir + \"\/input\/view-covid\/view\"\nprint(view_dir)","7928cb8b":"start_time = timeit.default_timer()\nbase_df = pd.read_parquet(data_dir + \"\/all_articles.parquet\")\njournal_df = pd.read_csv(data_dir + \"\/journal_ranking_info.csv\")\ncomplete_index_list = base_df.index\nbm25Plus_complete = joblib.load(model_dir + \"\/bm25Plus_complete.mdl\")\nword2Vec = joblib.load(model_dir + \"\/word2Vec_full_corpus.mdl\")\nindex2word_set = set(word2Vec.wv.index2word)\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","77c8ff60":"# stop words related to corpus\ncorpus_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'introduction', 'section', 'abstract', 'summary',\n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.', 'al'\n    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si','also', 'too',\n    'que', 'qui', 'le', 'la', 'les', 'un', 'une', 'si','de', 'des', 'est', 'sont', 'plus', 'dans', 'par', 'ici',\n    'para', 'por', 'lo', 'sera', 'caso', 'entre', 'avec', 'sur', 'ont', 'pour', 'pa', 'ce', 'ca', 'ces', 'cehz', 'son', 'moi', 'toi',\n]\n\n# general stop words of english\ngeneric_stop_words = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \n               \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \n               \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\",\n               \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \n               \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \n               \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \n               \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \n               \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]\n\n# Complete set of stopwords\ncustomize_stop_words_set = set(corpus_stop_words + generic_stop_words)\n\n# Text columns of Articles\n# By combining all these columns, we get complete text present in a article \ntext_columns = ['title', 'abstract', 'text', 'introduction', 'method', 'result', 'discussion', 'conclusion']\n                      \n# initializing of Singleton objects \n\n# Lemmatizer Object\nlemmatizer = WordNetLemmatizer()\n                      \n#Stemmer Object to find the root of the word\nstemmer = PorterStemmer()","79b3cfbe":"class Article:\n    \n    def __init__(self,df_row):\n        self.paper_id = str(df_row['paper_id'])\n        self.title = str(df_row['title'])\n        self.link = str(df_row['url'])\n        self.abstract = str(df_row['abstract'])\n        self.text = str(df_row['text'])\n        self.publish_time = str(df_row['publish_time'])\n        self.search_score = str(df_row['score'])\n        self.authors = [ x for x in zip(list(df_row['authors']),list(df_row['author_institutions']))]\n        self.journal = str(df_row['journal'])\n        self.document_type = str(df_row['document_type'])\n        #self.document_type = \"COMMERCIAL USE SUBSET\"\n        self.introduction = str(df_row['introduction'])\n        self.method = str(df_row['method'])\n        self.result = str(df_row['result'])\n        self.discussion = str(df_row['discussion'])\n        self.conclusion = str(df_row['conclusion'])\n        #self.institution = str(df_row['institution'])\n        self.country_tags = list(df_row['country_tags'])\n        self.row_num = str(df_row['row_num'])\n        \n    \n    def __str__(self):\n        return f\"title = {self.title}, link ={self.link}, publish_year ={self.publish_year}\"\n    \n    def set_country_tags(self):\n        loc_list = GeoText(self.complete_text()).country_mentions\n        location_list = [reverse_country_dict[x].title() for x,y in collections.Counter(loc_list).most_common()]\n        self.country_tags = location_list\n    \n    def set_journal_info(self):\n        row_df = journal_df[journal_df['orig_title'] == self.journal.lower()]\n        if len(row_df) > 0:\n            self.journal_rank = str(row_df.iloc[0]['rank'])\n            self.journal_title = str(row_df.iloc[0]['title'])\n        else:\n            self.journal_rank = 100000\n            self.journal_title = self.journal\n    \n    def set_search_rank(self,rank):\n        self.search_rank =  int(rank)\n    \n    def set_cluster_id(self,id):\n        self.cluster_id =  int(id)\n    \n    def set_semantic_tags(self,tags):\n        self.semantic_tags =  tags\n    \n    def set_publish_year(self):\n        date_str = self.publish_time\n        if date_str == \"nan\" or date_str == None or date_str == \"None\":\n            self.publish_year = int(dt.datetime.today().year)\n        else:\n            self.publish_year = int(date_str[0:4])\n    \n    def complete_text(self):\n        text_list = [self.title,self.abstract,self.text,self.introduction,self.method,self.result,self.discussion,self.conclusion]\n        texts_present = [text for text in text_list if not check_pd_nan(text)]\n        return \" \".join(texts_present)\n    \n    def summary(self):\n        #return summarize(self.text,word_count=200)\n        try:\n            if not check_pd_nan(self.text):\n                if len(split_sentences(self.text)) > 1 and len(self.text) > 800:\n                    return summarize(self.text,word_count=200)\n                else:\n                    return self.text\n            else:\n                text_list = [self.introduction,self.discussion,self.method,self.result,self.conclusion]\n                texts_present = [text for text in text_list if not check_pd_nan(text)]\n                if len(split_sentences(texts_present)) > 1 and len(texts_present) > 800:\n                    return summarize(texts_present,word_count=200)\n        except:\n            print(\"row_number : \"+ self.row_num)\n            return \"Data is not sufficient in this Article for generating valid summary.\"\n        print(\"row_number 2 : \"+ self.row_num)\n        return  \"Data is not sufficient in this Article for generating valid summary.\"\n    \n    def set_search_score(self,score):\n        self.search_score = float(score)\n        \n    def set_query_sim_score(self,score):\n        self.query_sim_score = float(score)\n    \n    def marked_complete_html(self,query,query_avg_vector,word2Vec,index2word_set):\n        \n        section_list = [\"Introduction\" , \"Discussion\", \"Content\",\"Method\", \"Result\", \"Conclusion\"]\n        section_data = [self.introduction,self.discussion,self.text,\n                        self.method,self.result,self.conclusion]\n        \n        marked_section_data =[]\n        for index,data in enumerate(section_data):\n            if check_pd_nan(data):\n                marked_section_data.append(data)\n                continue\n                \n            section_type = section_list[index]\n            num_sent_per_para = 6\n            num_mark_per_para = 2\n            if section_type != \"Content\":\n                num_sent_per_para=2\n                num_mark_per_para = 1\n            custom_section = custom_paragraph_in_document(data,num_sent_per_para)\n            marked_data = mark_best_paragraph_in_document(custom_section,query_avg_vector,word2Vec,index2word_set,num_mark_per_para)\n            marked_section_data.append(marked_data)\n        \n        css_template_str = custom_article_css_template.render()\n        \n        # fetching the article list html and turn into jinja2 template to render\n        article_html = custom_article_template.render(article=self,\n                        check_pd_nan = check_pd_nan,\n                        section_list=section_list,\n                        section_data =marked_section_data,\n                        css = css_template_str)\n\n        self.html_str = article_html.replace(\"\\n\",\"\").replace(\"'\", r\"\\'\")\n\n        \n# Json Encoder class for generating Json object corresponding to Article class\nclass ArticleEncoder(JSONEncoder):\n    def default(self, o):\n        return o.__dict__","9847a6d7":"def check_pd_nan(value):\n    \"\"\" func check_pd_nan(value) ->\n\n        Usage : to check null value from panda dataframe\n        Input : value to check\n        Output : Boolean - true if ti is none or nan\n    \"\"\"\n    return value == \"nan\" or value == None or value == \"None\"   \n\n\ndef custom_paragraph_in_document(document,num_sentences=6):\n    \"\"\" func custom_paragraph_in_document(document,num_sentences=6) -->\n    \n        Usage : Creating paragraph based on number of sentences from document.\n                Used for the purpose of finding the similiar section in document\n                instead of paragraph(more abstract) or sentence (fine-grain)\n                \n        Input :  document : document or section of document \n                        num_sentences : number of sentences\n        Output : List of paragraph in document\n    \"\"\"\n    \n    section_list =[]\n    count=0;\n    current_section=\"\"\n    for x,sentence in enumerate(split_sentences(document)):\n        count=count+1\n        if count <= num_sentences:\n            current_section = current_section + \" \"+ sentence\n            if count == num_sentences:\n                section_list.append(current_section)\n                count=0\n                current_section=\"\"\n    if count>0 :\n        section_list.append(current_section)\n    return section_list\n\ndef avg_feature_vector(document, model, num_features, index2word_set):\n    \"\"\"func avg_feature_vector(document, model, num_features, index2word_set) -->\n    \n        Usage : finding the average of word vector(feature vecture) using word2vec model\n                Using this average vector to find cosine similiartiy between sentence or document\n                \n        Input: document :  document or section or sentence\n                model : word2Vec model\n                num_features : should be same as number of features in word2vec model\n                index2word_set : index mapping of words present in word2vec model\n        Output: Word Vector (numpy array)\n    \"\"\"\n    words = create_tokens(document)\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n        if word in index2word_set:\n            n_words += 1\n            feature_vec = np.add(feature_vec, model[word])\n    if (n_words > 0):\n        feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec\n\ndef mark_best_paragraph_in_document(document,query_avg_vector,word2Vec,index2word_set,num_marks=2):\n    \"\"\" func mark_best_paragraph_in_document(document, model, num_features, index2word_set) -->\n    \n        Usage : finding the cosine similarity between query and document(list of sentence)\n                return the most similiar sections in document to query with marking\n                \n        Input: document :  list of section\n                query_avg_vector : average word vector of query\n                model : word2Vec model\n                index2word_set : index mapping of words present in word2vec model\n                num_marks : number of marks need in the document\n                \n        Output: Document in str with marked content similiar to query\n    \"\"\"\n    sim_scores =[]\n    for index,paragraph in enumerate(document):\n        para_avg_vector = avg_feature_vector(paragraph, model=word2Vec, num_features=300, index2word_set=index2word_set)\n        sim = 1 - spatial.distance.cosine(query_avg_vector, para_avg_vector)\n        sim_scores.append((index,sim))\n    sorted_sim_scores = sorted(sim_scores,key=lambda x: x[1],reverse=True)\n    top_indexes = [para_index for para_index,score in sorted_sim_scores[0:num_marks]]\n    marked_corpus=\"\"\n    for index,para in enumerate(document):\n        if index in top_indexes:\n            marked_corpus +=  '<span class=\"mark-sentence\">'+para +'<\/span>';\n        else:\n            marked_corpus +=  para\n    return marked_corpus   \n","7f04badd":" \n\ndef create_tokens(text_str):\n    \"\"\" func create_tokens(text_str) ->\n        \n        Usage : create tokens using following processing\n                     strip_tags,\n                     strip_non_alphanum,\n                     strip_punctuation,\n                     strip_multiple_whitespaces,\n                     strip_numeric,\n                     remove_stopwords,\n                     stem_text\n        Input : text_str : document or paragraph or query\n        Output : List of tokens\n    \"\"\"\n    \n    tokens_list = gensim.parsing.preprocess_string(text_str, filters =\n                                            [strip_tags,\n                                             strip_non_alphanum,\n                                             strip_punctuation,\n                                             strip_multiple_whitespaces,\n                                             strip_numeric,\n                                             remove_stopwords,\n                                             stem_text])\n    \n    cleaned_tokens = []\n    for token in tokens_list:\n            if token == token[0] * len(token) :\n                continue\n            if token  in customize_stop_words_set:\n                continue\n            else:\n                cleaned_tokens.append(token)   \n    return cleaned_tokens\n\ndef create_tm_tokens(text_str, lemmatizer):\n    \"\"\" func create_tm_tokens(text_str,lemmatizer) ->\n        \n        Usage : create tokens for topic modelling.\n                For topic modelling we used following preprocessing.\n                Different from above tokenization in last step. Here we use lemmatizer\n                     strip_tags,\n                     strip_non_alphanum,\n                     strip_punctuation,\n                     strip_multiple_whitespaces,\n                     strip_numeric,\n                     remove_stopwords,\n                     Lemmatizer\n        Input : text_str : document or paragraph or query\n                Lemmatizer : Singleton Object \n        Output : List of tokens\n        \"\"\"\n    tokens_list = gensim.parsing.preprocess_string(text_str, filters =\n                                            [strip_tags,\n                                             strip_non_alphanum,\n                                             strip_punctuation,\n                                             strip_multiple_whitespaces,\n                                             strip_numeric,\n                                             remove_stopwords])\n    \n    cleaned_tokens = []\n    for token in tokens_list:\n        final_token = lemmatizer.lemmatize(token)\n        final_token = final_token.lower() if final_token.istitle() else final_token\n        token_lower = final_token.lower()\n        if token_lower == token_lower[0] * len(token_lower) :\n            continue\n        if token_lower  in customize_stop_words_set:\n            continue\n        else:\n            cleaned_tokens.append(final_token)   \n    return cleaned_tokens\n\ndef create_complete_text(row,text_columns):\n    \"\"\" func create_complete_text(row,text_columns) ->\n        \n        Usage : Create complete text of an article which is row of dataframe\n                It is user defined function for Pandas dataframe to apply row wise\n            \n        Input : row : row of dataframe or an article in pandas row format\n                text_columns : list of columns in row to be part of return text\n        Output : Complete text in str\n    \"\"\"\n    \n    texts_present = [row[col] for col in text_columns if not pd.isna(row[col])]\n    return \" \".join(texts_present)\n\ndef create_stem_dictionary(doc_tokens_list,stemmer):\n    \"\"\" func create_stem_dictionary(doc_tokens_list,stemmer) ->\n        \n        Usage : Create stem words dictionary\n                dictionary will be used to display purpose of LDA tokens\n                becuase stem tokens will be difficult to understand \n                so we find stem_word -> [all the words have same root word(stem_word)]\n                later we replace stem_word with word of minimum length from the list\n            \n        Input : doc_tokens : list of tokens\n                stemmer : Singelton object to do stemming\n        Output : Dictionary { stem_word -> human_understandable_word}\n    \"\"\"\n    \n    dictionary={}\n    for doc_tokens in doc_tokens_list:\n        for token in doc_tokens:\n            stem_token = stemmer.stem(token)\n            if stem_token in dictionary:\n                prev_val = dictionary[stem_token]\n                if len(prev_val) > len(token):\n                    dictionary[stem_token] = token\n            else:\n                dictionary[stem_token] = token\n    return dictionary\n\ndef create_stem_dictionary_tokens(tokens_list,stemmer,dictionary=None):\n    \"\"\" func create_stem_dictionary_tokens(doc_tokens_list,stemmer) ->\n        \n        Usage : Create stem words dictionary and stem tokens \n                dictionary will be used to display purpose of LDA tokens\n                becuase stem tokens will be difficult to understand \n                so we find stem_word -> [all the words have same root word(stem_word)]\n                later we replace stem_word with word of minimum length from the list\n            \n        Input : doc_tokens : list of tokens\n                stemmer : Singelton object to do stemming\n                dictionary : if stemming dictionary supplied, it will not create stem dictionary\n        Output : List of stem tokens\n    \"\"\"\n    # Creating the dictionary: \n    stem_dictionary = dictionary\n    if stem_dictionary == None:\n        stem_dictionary = create_stem_dictionary(tokens_list,stemmer)\n        \n    # Stemming the tokens:\n    stem_tokens_list =[]\n    for doc_tokens in tokens_list:\n        stem_tokens_list.append([stem_dictionary[stemmer.stem(token)] for token in doc_tokens if stemmer.stem(token) in stem_dictionary])\n    \n    return (stem_dictionary,stem_tokens_list)","690e6de8":"def create_topic_model(tokens_list, stem_dictionary,num_topics):\n    \"\"\"func create_topic_model(tokens_list, stem_dictionary,num_topics) ->\n        \n        Usage : Create Topic modelling using LDA to generate topics or clusters \n                Using Unigram, bigram, trigram and doc2bow\n                we are generating the clusters from the search results\n            \n        Input : tokens_list : list of tokens\n                stem_dictionary : stem dictionary\n                num_topics : number of clusters or topics to be generated \n        Output : lda_model  = LDA model\n                doc2bows = List of Bag of words \n                dictionary =  dictionary of final_tokens\n                final_tokens = list of unigram,bigram and trigram tokens\n    \"\"\"\n    \n    unigram_tokens = tokens_list\n\n    ''' Preparing Bigram and Trigram '''\n    bigram = gensim.models.Phrases(unigram_tokens,min_count= 5,threshold=10) # higher threshold fewer phrases.\n    trigram = gensim.models.Phrases(bigram[unigram_tokens], threshold=10)  \n    bigram_mod = gensim.models.phrases.Phraser(bigram)\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\n\n    #### Storing the bigram and trigram tokens:\n\n    bigram_tokens = [bigram_mod[doc] for doc in unigram_tokens]\n    trigram_tokens = [trigram_mod[bigram_mod[doc]] for doc in bigram_tokens]\n\n    #### Creating the dictionary and corpus for the LDA :\n    final_tokens = trigram_tokens\n    dictionary = gensim.corpora.Dictionary(final_tokens)\n    #dictionary.filter_extremes( no_above=0.5) \n    doc2bows = [dictionary.doc2bow(text) for text in final_tokens]\n\n    # Build LDA model\n    lda_model = gensim.models.ldamodel.LdaModel(corpus=doc2bows,\n                                               id2word=dictionary,\n                                               num_topics=num_topics,\n                                               passes = 50,\n                                               update_every=5,\n                                               alpha='symmetric',\n                                               iterations=50,\n                                               random_state=np.random.seed(42),\n                                               minimum_probability=0)\n    \n    return (lda_model,doc2bows,dictionary,final_tokens)\n\n\ndef find_doc_topic_cluster(lda_model,doc2bows):\n    \"\"\"func find_doc_topic_cluster(lda_model,doc2bows) ->\n        \n        Usage : finding the cluster for each document in the search results\n            \n        Input : lda_model : LDA Model \n                doc2bows : list of bag of words\n        Output : topics_df :\n                 columns : 'document number','dominant_topic' ,'percentage_contribution', 'topic_keywords' \n    \"\"\"\n    \n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(lda_model[doc2bows]):\n        row = row_list[0] if lda_model.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = lda_model.show_topic(topic_num)\n                topic_keywords = [word for word, prop in wp]\n                sent_topics_df = sent_topics_df.append(pd.Series([i,int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['doc_no','dominant_topic' ,'percentage_contribution', 'topic_keywords']\n\n    return sent_topics_df\n","6b88f754":"\ndef query_docs_similiarity(query,query_avg_vector,result_df,word2Vec,index2word_set):\n    \"\"\"func query_docs_similiarity(query,query_avg_vector,result_df,word2Vec,index2word_set) ->\n        \n        Usage : finding the query document similarity using word2vec and cosine similarity for every document in search result\n            \n        Input : query : search query \n                query_avg_vector : average word vector of query\n                result_df : search result as pandas dataframe\n                word2Vec : word2vec model\n                index2word_set : index mapping of words present in word2vec model\n        Output : List of document similarity score\n    \"\"\"\n    \n    doc_sim_scores=[]\n    for index, row in result_df.iterrows():\n        complete_text = create_complete_text(row,text_columns)\n        doc_avg_vector = avg_feature_vector(complete_text, model=word2Vec, num_features=300, index2word_set=index2word_set)\n        sim = 1 - spatial.distance.cosine(query_avg_vector, doc_avg_vector)\n        doc_sim_scores.append(sim)\n    return doc_sim_scores","011e8015":"def find_query_results(query,bm25plus) :\n    \"\"\"func find_query_results(query,bm25plus) ->\n        \n        Usage : finding the search results using BM25plus model\n            \n        Input : query : search query \n                bm25plus : bm25plus search engine model\n        Output : List of index  with highest search score\n    \"\"\"\n    \n    tokenized_query = create_tokens(query)\n    doc_scores = bm25plus.get_scores(tokenized_query)\n    doc_list = [(i,x) for i,x in enumerate(doc_scores)]\n    top_n = sorted(doc_list,key = lambda key : key[1],reverse=True)[0:5000] \n    #doc_index = model.get_top_n(tokenized_query, index_list, n=num_top_results)\n    return top_n\n\ndef find_query_df(doc_scores,is_covid_only,num_records):\n    \"\"\"func find_query_df(doc_scores,is_covid_only,num_records) ->\n        \n        Usage : finding the result df form the index list returned by find_query_results\n            \n        Input : doc_scores : List of index  with highest search score\n                is_covid_only : True : covid related articles or False : all articles in corpus\n                num_records : number of records to be fetched \n        Output : Pandas dataframe with top results\n    \"\"\"\n    index_list = [ index for index,score in doc_scores]\n    #score_df = pd.DataFrame(doc_scores, columns =['index_1', 'score'])\n    df = None\n    score_df = None\n    if not is_covid_only:\n        index_list = index_list[0:num_records]\n        score_df = pd.DataFrame(doc_scores[0:num_records], columns =['index_1', 'score'])\n        df = base_df[base_df.index.isin(index_list)]  \n    else:\n        score_df = pd.DataFrame(doc_scores, columns =['index_1', 'score'])\n        df = base_df[base_df.index.isin(index_list)]  \n        df = df[df['is_covid'] == True]  \n    \n    df['index_1'] = df['row_num']\n    final_df = pd.merge(df, score_df, on='index_1', how='left')\n    sort_final_df = final_df.sort_values(by=['score'], ascending=False).reset_index()\n    return sort_final_df[0:num_records]\n\ndef find_num_cluster(num_records):\n    \"\"\"func find_num_cluster(num_records) ->\n        \n        Usage : finding the number of clusters for topic modelling\n            \n        Input : num_records (int)\n        Output : number of clusters (int)\n    \"\"\"\n    \n    num_cluster = int(num_records\/5)\n    num_cluster = 10 if num_cluster >=10 else num_cluster\n    return num_cluster\n\ndef find_final_results(query,bm25plus ,is_covid,num_top_results):\n    \"\"\"func find_final_results(query,is_covid,num_results,num_cluster)->\n        \n        Usage : finding the search result with search score and cluster id \n                with the help of BM25Plus and Topic modelling using lda\n            \n        Input : query : search query \n                bm25plus : bm25plus search model\n                is_covid_only : True : covid related articles or False : all articles in corpus\n                num_top_results : number of top ranked document need to return\n        Output : lda_model  = LDA model\n                doc2bows = List of Bag of words \n                dictionary =  dictionary of final_tokens\n                final_tokens = list of unigram,bigram and trigram tokens\n                result_df = search result dataframe with search score\n                doc_topic_df = document cluster or topic dataframe\n    \"\"\"\n    \n    num_cluster = find_num_cluster(num_top_results)\n    doc_scores = find_query_results(query,bm25plus)\n    result_df = find_query_df(doc_scores,is_covid,num_top_results)\n    result_df['complete_text'] = result_df.apply (lambda row: create_complete_text(row,text_columns), axis=1)\n    result_df['tokens'] = result_df.apply (lambda row: create_tm_tokens(row['complete_text'],lemmatizer), axis=1)\n    stem_dictionary,stem_tokens_list = create_stem_dictionary_tokens(result_df['tokens'].tolist(),stemmer)\n    lda_model,doc2bows,dictionary,final_tokens = create_topic_model(stem_tokens_list, stem_dictionary,num_topics=num_cluster)\n    doc_topic_df = find_doc_topic_cluster(lda_model,doc2bows)\n    return (lda_model,doc2bows,dictionary,final_tokens,result_df,doc_topic_df)","58d3cb6b":"def find_pos_tags(word):\n    \"\"\"func find_pos_tags(word) ->\n        \n        Usage : finding the pos tagging of word\n                posibility of word to be noun,adjuctive and verb\n            \n        Input : word \n        Output : list of pos tags like 'n' for noun, 'v' for verb etc\n    \"\"\"\n    \n    syns = wordnet.synsets(word)\n    check_pos = set([p.pos() for p in syns])\n    return check_pos\n\ndef find_semantic_tags(doc_no,doc_topic_df,doc2bows,dictionary,lookup_dict,num_tags):\n    \"\"\"func find_semantic_tags(doc_no,doc_topic_df,doc2bows,dictionary,lookup_dict,num_tags) ->\n        \n        Usage : find the semantic tags with bag of words and pos tagging\n                Sorting with the help of Bag of words(BoW)\n                Return the noun only\n            \n        Input : doc_no : document number\n                doc_topic_df : cluster related infor\n                doc2bows : list of Bag of words \n                dictionary : dictionary of corpus ( index , word)\n                lookup_dict : dictionary of corpus (word , dict)\n                num_tags : number of tags to be retured \n        Output : list of pos tags like 'n' for noun, 'v' for verb etc\n    \"\"\"\n    sorted_bow = sorted(doc2bows[doc_no] , key = lambda x:x[1], reverse=True)  \n    sorted_bow2 = [(lookup_dict[x],y) for x,y in sorted_bow]\n    sorted_bow3 = [(x,y,find_pos_tags(x)) for x,y in sorted_bow2]\n    noun_bow = [x for x,y,z in sorted_bow3 if not ('v' in z or 's' in z or 'a' in z or 'r' in z)]\n    return noun_bow[0:num_tags]","e65a82da":"def get_articles_from_df(result_df,doc_topic_df,doc_sim_scores,doc2bows,dictionary,query,query_avg_vector,word2Vec,index2word_set):\n    \"\"\" function get_articles_from_df(result_df,doc_topic_df,doc_sim_scores,doc2bows,dictionary,query,query_avg_vector,word2Vec,index2word_set) ->\n        \n        Usage : Convert search result df to list of Article object \n                Output will be used for UI purpose\n        \n        Input : result_df = search result dataframe with search score\n                doc_topic_df = document cluster or topic dataframe\n                doc_sim_scores = list of query document similarity score\n                lda_model  = LDA model\n                doc2bows = List of Bag of words \n                dictionary =  dictionary of final_tokens\n                final_tokens = list of unigram,bigram and trigram tokens\n                query = search query\n                query_avg_vector : average word vector of query\n                word2Vec : word2vec model\n                index2word_set : index mapping of words present in word2vec model\n        Output : List of Article object        \n                \n        \n    \"\"\"\n    article_list=[]\n    lookup_dict = { item[0]:item[1] for item in dictionary.items()}\n    for index, row in result_df.iterrows() :\n        article = Article(row)\n        article.set_search_rank(index)\n        article.set_search_score(row['score'])\n        #article.set_country_tags()\n        article.set_journal_info()\n        article.marked_complete_html(query,query_avg_vector,word2Vec,index2word_set)\n        article.set_publish_year()\n        article.set_query_sim_score(get_query_sim_score(index, doc_sim_scores))\n        article.set_cluster_id(find_cluster_id(index,doc_topic_df))\n        article.set_semantic_tags(find_semantic_tags(index,doc_topic_df,doc2bows,dictionary,lookup_dict,50))\n        article_list.append(article)\n    return article_list\n\ndef get_query_sim_score(doc_no, doc_sim_scores):\n    \"\"\"func get_query_sim_score(doc_no, doc_sim_scores) ->\n        \n        Usage : finding query document similarity score for particular document by document number\n            \n        Input : doc_no : document number\n                sim_socres = List of document similarity score\n        Output : query document similarity score(float) \n    \"\"\"\n    return doc_sim_scores[doc_no]\n\ndef find_cluster_id(doc_no,doc_topic_df):\n    \"\"\"func find_cluster_id(doc_no,doc_topic_df) ->\n        \n        Usage : finding cluster id for particular document by document number\n            \n        Input : doc_no : document number\n                doc_topic_df = document topic dataframe\n        Output : Cluster id (int)\n    \"\"\"\n    \n    return int(doc_topic_df.iloc[doc_no].dominant_topic) + 1\n\ndef sort_articles_by_query_sim(result_articles):\n    \"\"\"func sort_articles_by_query_sim(result_articles) ->\n        \n        Usage : Sort the list of articles by query document similarity score\n            \n        Input : list of result_articles\n        Output : sorted list of result_articles\n    \"\"\"\n    return sorted(result_articles , key = lambda article : article.query_sim_score, reverse=True)","82d0c707":"# reading jinja2 templates for UI purpose\narticle_list_css_template =  Template(open(view_dir +\"\/css\/article_list_style.css\").read())\narticle_list_js_template = Template(open(view_dir +\"\/js\/search-engine-custom.js\").read())\ngraph_banner_template  = Template(open(view_dir +\"\/banner_view.html\").read())\narticle_list_template = Template(open(view_dir +\"\/article_list_view.html\").read())\n\n\ncustom_article_css_template =  Template(open(view_dir +\"\/css\/custom_publish_style.css\", encoding=\"utf8\").read())\n        \n\ncustom_article_content=\"\"\nwith open(view_dir +\"\/custom_publish.html\", \"rb\") as f:\n    custom_article_content = f.read().decode(\"UTF-8\")\ncustom_article_template = Template(custom_article_content)\n\n\ndef find_min_max_year(result_articles):\n    \"\"\"func find_min_max_year(result_articles) ->\n        \n        Usage : finding the min and max year from the list of result articles\n                Used for UI to provide search slider corresponding to publish year\n        Input : list of result_articles\n        Output : (min_year, max_year)\n    \"\"\"\n    min_year = 5000\n    max_year = -1\n    for article in result_articles: \n        if article.publish_year < min_year:\n            min_year = article.publish_year\n        if article.publish_year > max_year:\n            max_year = article.publish_year\n    return (min_year,max_year)\n\ndef collect_countries(result_articles):\n    \"\"\"func collect_countries(result_articles) ->\n        \n        Usage : finding all the countries from the list of result articles\n                Country present in the content of result articles ( not where it is published)\n                we are focusing on infected countries information in articles\n                Used for UI to search based on countries\n                first five country in the drop down will be in this order beacuse they are most affected\n                ['China','United States','Italy','Spain','Germany','Iran']\n                and other countries will be present after that based on number of times they mentioned in articles.\n                \n        Input : list of result_articles\n        Output : List of articles\n    \"\"\"\n    countries_list=[]\n    for article in result_articles:\n        for tag in article.country_tags:\n                countries_list.append(tag)\n    #print(countries_list)\n    most_common_countries = collections.Counter(countries_list).most_common()\n    final_country_list = [country for country,count in most_common_countries]\n    most_common_sets = set(final_country_list)\n\n    static_countries = ['China','United States','Italy','Spain','Germany','Iran']\n\n    filter_static_countries = [country for country in static_countries if country in most_common_sets]\n    filter_static_countries_set = set(filter_static_countries)\n    filter_country_list = [ country for country in final_country_list if country not in  filter_static_countries_set]\n    \n    return  filter_static_countries + filter_country_list\n\n\ndef fetch_cluster_graphical_data(lda_model,doc2bows,dictionary):\n    \"\"\"func fetch_cluster_graphical_data(lda_model,doc2bows,dictionary) ->\n        \n        Usage : preparing the pyLDAvis graphical data for interactive visualization\n                \n        Input : lad_model = LDA model \n                doc2bows = List of bag of words\n                dictionary = corpus dictionary used for topic modelling\n        Output : pyLDAvis formatted data for graph\n    \"\"\"\n    lda_display = pyLDAvis.gensim.prepare(lda_model, doc2bows, dictionary=dictionary)\n    return lda_display\n\ndef display_complete_result(query=\"corona virus\",num_records=25,is_covid=True):\n    \"\"\" display_complete_result(query=\"corona virus\",num_records=25,is_covid=True)->\n        \n        Usage : Fetching and preparing the result data for UI purpose\n                \n        Input : query = search query \n                num_records = number of records to fetch\n                is_covid = True - for covid related articles only ; False : all articles in corpus\n        Output : Display output in cell output section where it is called\n    \"\"\"\n    \n    # backend logic for retreiving all the result data\n    query_avg_vector =  avg_feature_vector(query, model=word2Vec, num_features=300, index2word_set=index2word_set)\n    num_cluster = find_num_cluster(num_records)\n    lda_model,doc2bows,dictionary,final_tokens,result_df,doc_topic_df = find_final_results(query, bm25Plus_complete,is_covid,num_records)\n    #doc_sim_scores = query_docs_similiarity(query,lda_model,doc2bows,dictionary,lemmatizer,stemmer)\n    doc_sim_scores = query_docs_similiarity(query,query_avg_vector,result_df,word2Vec,index2word_set)\n    \n    \n    \n    #  logic for preparing the above backend data for UI purpose \n    result_articles = get_articles_from_df(result_df,doc_topic_df,doc_sim_scores,doc2bows,dictionary,query,query_avg_vector,word2Vec,index2word_set)  \n    sorted_result_articles = sort_articles_by_query_sim(result_articles)\n    cluster_graph_data = fetch_cluster_graphical_data(lda_model,doc2bows,dictionary)\n    \n    cluster_filter_list = [str(i+1) for i in range(0,num_cluster)]\n    country_filter_list = collect_countries(sorted_result_articles)\n    min_year,max_year = find_min_max_year(sorted_result_articles)\n    \n    complete_result_json =json.dumps(sorted_result_articles,cls=ArticleEncoder)\n    \n    # fetching the article list css and turn into jinja2 template to render\n    css_template_str = article_list_css_template.render()\n    \n     # fetching the javascript and turn into jinja2 template to render\n    js_template_str = article_list_js_template.render(data=complete_result_json,\n                                        min_year=min_year,max_year=max_year)\n    # fetching the article list html and turn into jinja2 template to render\n    article_list_html = article_list_template.render(result_articles=sorted_result_articles,\n                    check_pd_nan = check_pd_nan,\n                    min_year=min_year,max_year=max_year,\n                    country_filter_list=country_filter_list,\n                    cluster_filter_list=cluster_filter_list,\n                    script = js_template_str,\n                    css = css_template_str)\n    \n    # fetching the banner html and turn into jinja2 template to render\n    graph_banner_title = \"Topic based Representation of Relevant Papers\"\n    graph_banner_html = graph_banner_template.render(title = graph_banner_title)\n    \n    # display command\n    display(HTML(graph_banner_html))\n    display(pyLDAvis.display(cluster_graph_data))\n    display(HTML(article_list_html))","8914e23b":"# Run this cell(Ctrl-Enter) to see the ranked seach articles with augmented information\nquery = \"\"\"\n            Implementation of diagnostics and products to improve clinical processes\n        \"\"\"\nnum_reocds = 25  #  we recommend between 25-100 ( this is enough articles to read to fetch required information)\nis_covid = True  # True - for covid related articles only ; False :  all type of articles in corpus\n\ndisplay_complete_result(query,num_reocds,is_covid)","5e94a264":"# Run this cell(Ctrl-Enter) to see the ranked seach articles with augmented information\nquery = \"\"\"\n            Natural history of the virus and shedding of it from an infected person\n        \"\"\"\nnum_reocds = 25  #  we recommend between 25-100 ( this is enough articles to read to fetch required information)\nis_covid = True  # True - for covid related articles only ; False :  all type of articles in corpus\n\ndisplay_complete_result(query,num_reocds,is_covid)","7d0dec18":"# Run this cell(Ctrl-Enter) to see the ranked seach articles with augmented information\nquery = \"\"\"\n            Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n        \"\"\"\nnum_reocds = 25  #  we recommend between 25-100 ( this is enough articles to read to fetch required information)\nis_covid = True  # True - for covid related articles only ; False :  all type of articles in corpus\n\ndisplay_complete_result(query,num_reocds,is_covid)","ede2490d":"# Run this cell(Ctrl-Enter) to see the ranked seach articles with augmented information\nquery = \"\"\"\n            Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n        \"\"\"\nnum_reocds = 25  #  we recommend between 25-100 ( this is enough articles to read to fetch required information)\nis_covid = True  # True - for covid related articles only ; False :  all type of articles in corpus\n\ndisplay_complete_result(query,num_reocds,is_covid)","6318b7f8":"![ProcessDiagramUpdated.PNG](attachment:ProcessDiagramUpdated.PNG)","b91ee0bf":"**Note the following when interpreting the visualization output of the cell below:**\n- Each bubble on the left side of the plot represents a topic. The larger the bubble, the more prevalent the topic.\n- An ideal topic model has large, non-overlapping bubbles scattered throughout the chart versus a clustering of bubbles in one quadrant.\n- A model with excessive topics typically has many overlaps, with smaller bubbles clustered in one region of the chart.\n- When positioning the cursor over one of the bubbles, the words and bars on the right-side of the visualization update. The words associated with the bars are the salient keywords that form the selected topic.\n","a01b6129":"---\n## 4.1 Import Packages \n\nMain Libraries :\n* nltk : stemming, lemmatizer , wordnet\n* gensim : LDA , Summarizer, Sentence Preprocessing \n* jinja2 : HTML, CSS Template\n* scipy : cosine similarity\n* pyLDAvis : Graphics library for LDA\n* rank_bm25 : search functionality","0c3a0bcf":"## 4.2 Initializing the Working and Model Directory","42c7398e":"## 4.8 Topic Modelling  and Finding Topic Cluster\n* create_topic_model  : Create Topic modelling using LDA to generate topics or clusters \n* find_doc_topic_cluster : finding the cluster for each document in the search results\n\n##### For complete information of those functions, check docstring as a first statement in each function","b31c1721":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\"1.-Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Goal\" data-toc-modified-id=\"1.1-Goal-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Goal<\/a><\/span><\/li><li><span><a href=\"#1.2-How-it-Works\" data-toc-modified-id=\"1.2-How-it-Works-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>How it Works<\/a><\/span><\/li><li><span><a href=\"#1.3-Results-Preview\" data-toc-modified-id=\"1.3-Results-Preview-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>Results Preview<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#2.-Methodology\" data-toc-modified-id=\"2.-Methodology-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Methodology<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Technical-Approach\" data-toc-modified-id=\"2.1-Technical-Approach-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Technical Approach<\/a><\/span><\/li><li><span><a href=\"#2.2-Pros-and-Cons\" data-toc-modified-id=\"2.2-Pros-and-Cons-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Pros and Cons<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.-Coding-Process\" data-toc-modified-id=\"3.-Coding-Process-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Coding Process<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Offline-Processing\" data-toc-modified-id=\"3.1-Offline-Processing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Offline Processing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.1.1-Downloading-CORD-19-from-Kaggle\" data-toc-modified-id=\"3.1.1-Downloading-CORD-19-from-Kaggle-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;<\/span>Downloading CORD-19 from Kaggle<\/a><\/span><\/li><li><span><a href=\"#3.1.2-Data-Preprocessing\" data-toc-modified-id=\"3.1.2-Data-Preprocessing-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><\/li><li><span><a href=\"#3.1.3-Training-the-BM25Plus-Search-Model-and-Word2Vec-Model\" data-toc-modified-id=\"3.1.3-Training-the-BM25Plus-Search-Model-and-Word2Vec-Model\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;<\/span>Training the BM25Plus Search Model and Word2Vec Model<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.2-Online-Processing\" data-toc-modified-id=\"3.2-Online-Processing-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Online Processing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.2.1-BM25-Search\" data-toc-modified-id=\"3.2.1-BM25-Search-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;<\/span>BM25 Search<\/a><\/span><\/li><li><span><a href=\"#3.2.2-Topic-Modeling\" data-toc-modified-id=\"3.2.2-Topic-Modeling-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;<\/span>Topic Modeling<\/a><\/span><\/li><li><span><a href=\"#3.2.3-Semantic-Tagging\" data-toc-modified-id=\"3.2.3-Semantic-Tagging-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;<\/span>Semantic Tagging<\/a><\/span><\/li><li><span><a href=\"#3.2.4-Applying-the-Word2Vec-Model\" data-toc-modified-id=\"3.2.4-Applying-the-Word2Vec-Model-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;<\/span>Applying the Word2Vec Model<\/a><\/span><\/li><li><span><a href=\"#3.2.5-Applying-the-Ranking-Logic\" data-toc-modified-id=\"3.2.5-Applying-the-Ranking-Logic-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;<\/span>Applying the Ranking Logic<\/a><\/span><\/li><li><span><a href=\"#3.2.6-Using-the-Text-Summarizer\" data-toc-modified-id=\"3.2.6-Using-the-Text-Summarizer-3.2.6\"><span class=\"toc-item-num\">3.2.6&nbsp;&nbsp;<\/span>Using the Text Summarizer<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.3-Process-Diagram\" data-toc-modified-id=\"3.3-Process-Diagram-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Process Diagram<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#4.-Coding-the-COVID-19-Kaggle-Search-Engine\" data-toc-modified-id=\"4.-Coding-the-COVID-19-Kaggle-Search-Engine-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Coding the COVID-19 Kaggle Search Engine<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Import-Packages\" data-toc-modified-id=\"4.1-Import-Packages-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Import Packages<\/a><\/span><\/li><li><span><a href=\"#4.2-Initializing-the-Working-and-Model-Directory\" data-toc-modified-id=\"4.2-Initializing-the-Working-and-Model-Directory-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Initializing the Working and Model Directory<\/a><\/span><\/li><li><span><a href=\"#4.3-Loading-the-Corpus-and-Pre-Trained-Model\" data-toc-modified-id=\"4.3-Loading-the-Corpus-and-Pre-Trained-Model-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Loading the Corpus and Pre-Trained Model<\/a><\/span><\/li><li><span><a href=\"#4.4-Initializing-Static-Variables-and-Singleton-Objects\" data-toc-modified-id=\"4.4-Initializing-Static-Variables-and-Singleton-Objects-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Initializing Static Variables and Singleton Objects<\/a><\/span><\/li><li><span><a href=\"#4.5-Class-:-Article-and-ArticleEncoder\" data-toc-modified-id=\"4.5-Class-:-Article-and-ArticleEncoder-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;<\/span>Class : Article and ArticleEncoder<\/a><\/span><\/li><li><span><a href=\"#4.6-Utility-Functions-related-to-creating-the-section-in-paragraph,-cosine-similarity-and-marking-sentence-in-paragraph-with-help-of-cosine-similarity\" data-toc-modified-id=\"4.6-Utility-Functions-related-to-creating-the-section-in-paragraph,-cosine-similarity-and-marking-sentence-in-paragraph-with-help-of-cosine-similarity-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;<\/span>Utility Functions related to creating the section in paragraph, cosine similarity and marking sentence in paragraph with help of cosine similarity<\/a><\/span><\/li><li><span><a href=\"#4.7-Text-Preprocessing-and-Token-Generation\" data-toc-modified-id=\"4.7-Text-Preprocessing-and-Token-Generation-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;<\/span>Text Preprocessing and Token Generation<\/a><\/span><\/li><li><span><a href=\"#4.8-Topic-Modelling--and-Finding-Topic-Cluster\" data-toc-modified-id=\"4.8-Topic-Modelling--and-Finding-Topic-Cluster-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;<\/span>Topic Modelling  and Finding Topic Cluster<\/a><\/span><\/li><li><span><a href=\"#4.-9-Query-Document-Similarity\" data-toc-modified-id=\"4.-9-Query-Document-Similarity-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;<\/span>Query Document Similarity<\/a><\/span><\/li><li><span><a href=\"#4.10-Query-Results\" data-toc-modified-id=\"4.10-Query-Results-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;<\/span>Query Results<\/a><\/span><\/li><li><span><a href=\"#4.-11-Semantic-Tagging\" data-toc-modified-id=\"4.-11-Semantic-Tagging-4.11\"><span class=\"toc-item-num\">4.11&nbsp;&nbsp;<\/span>Semantic Tagging<\/a><\/span><\/li><li><span><a href=\"#4.12-Converting-all-the-search-research-information-to-List-of-Articles-Object(-DTO)\" data-toc-modified-id=\"4.12-Converting-all-the-search-research-information-to-List-of-Articles-Object(-DTO)-4.12\"><span class=\"toc-item-num\">4.12&nbsp;&nbsp;<\/span>Converting all the search research information to List of Articles Object( DTO)<\/a><\/span><\/li><li><span><a href=\"#4.13-Fetch-and-Prepare-Data-for-UI\" data-toc-modified-id=\"4.13-Fetch-and-Prepare-Data-for-UI-4.13\"><span class=\"toc-item-num\">4.13&nbsp;&nbsp;<\/span>Fetch and Prepare Data for UI<\/a><\/span><\/li><li><span><a href=\"#4.14-Demo-Result\" data-toc-modified-id=\"4.14-Demo-Result-4.14\"><span class=\"toc-item-num\">4.14&nbsp;&nbsp;<\/span>Demo Result<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#4.14.1-Run-this-cell(Ctrl-Enter)-to-see-the-ranked-seach-articles-with-augmented-information\" data-toc-modified-id=\"4.14.1-Run-this-cell(Ctrl-Enter)-to-see-the-ranked-seach-articles-with-augmented-information-4.14.1\"><span class=\"toc-item-num\">4.14.1&nbsp;&nbsp;<\/span>Run this cell(Ctrl-Enter) to see the ranked seach articles with augmented information<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#5.-License-Agreements\" data-toc-modified-id=\"5.-License-Agreements-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>License Agreements<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Python-Foundation-License-Version-2\u00b6\" data-toc-modified-id=\"5.1-Python-Foundation-License-Version-2\u00b6-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Python Foundation License Version 2\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.2-BEOPEN.COM-LICENSE-AGREEMENT-FOR-PYTHON-2.0\u00b6\" data-toc-modified-id=\"5.2-BEOPEN.COM-LICENSE-AGREEMENT-FOR-PYTHON-2.0\u00b6-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>BEOPEN.COM LICENSE AGREEMENT FOR PYTHON 2.0\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.3-CNRI-LICENSE-AGREEMENT-FOR-PYTHON-1.6.1\u00b6\" data-toc-modified-id=\"5.3-CNRI-LICENSE-AGREEMENT-FOR-PYTHON-1.6.1\u00b6-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>CNRI LICENSE AGREEMENT FOR PYTHON 1.6.1\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.4-CWI-LICENSE-AGREEMENT-FOR-PYTHON-0.9.0-THROUGH-1.2\u00b6\" data-toc-modified-id=\"5.4-CWI-LICENSE-AGREEMENT-FOR-PYTHON-0.9.0-THROUGH-1.2\u00b6-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>CWI LICENSE AGREEMENT FOR PYTHON 0.9.0 THROUGH 1.2\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.5-PSF-LICENSE-AGREEMENT-FOR-PYTHON-3.8.2\u00b6\" data-toc-modified-id=\"5.5-PSF-LICENSE-AGREEMENT-FOR-PYTHON-3.8.2\u00b6-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;<\/span>PSF LICENSE AGREEMENT FOR PYTHON 3.8.2\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.6-The-3-Clause-BSD-License\u00b6\" data-toc-modified-id=\"5.6-The-3-Clause-BSD-License\u00b6-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;<\/span>The 3-Clause BSD License\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.7-NumPy-license\u00b6\" data-toc-modified-id=\"5.7-NumPy-license\u00b6-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;<\/span>NumPy license\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.8-GNU-LESSER-GENERAL-PUBLIC-LICENSE\u00b6\" data-toc-modified-id=\"5.8-GNU-LESSER-GENERAL-PUBLIC-LICENSE\u00b6-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;<\/span>GNU LESSER GENERAL PUBLIC LICENSE\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.9-The-MIT-License\u00b6\" data-toc-modified-id=\"5.9-The-MIT-License\u00b6-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;<\/span>The MIT License\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.10-License-agreement-for-matplotlib-versions-1.3.0-and-later\u00b6\" data-toc-modified-id=\"5.10-License-agreement-for-matplotlib-versions-1.3.0-and-later\u00b6-5.10\"><span class=\"toc-item-num\">5.10&nbsp;&nbsp;<\/span>License agreement for matplotlib versions 1.3.0 and later\u00b6<\/a><\/span><\/li><li><span><a href='#5.11-3-clause-license-(\"BSD-License-2.0\",-\"Revised-BSD-License\",-\"New-BSD-License\",-or-\"Modified-BSD-License\")\u00b6' data-toc-modified-id='5.11-3-clause-license-(\"BSD-License-2.0\",-\"Revised-BSD-License\",-\"New-BSD-License\",-or-\"Modified-BSD-License\")\u00b6-5.11'><span class=\"toc-item-num\">5.11&nbsp;&nbsp;<\/span>3-clause license (\"BSD License 2.0\", \"Revised BSD License\", \"New BSD License\", or \"Modified BSD License\")\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.12-Apache-License\" data-toc-modified-id=\"5.12-Apache-License-5.12\"><span class=\"toc-item-num\">5.12&nbsp;&nbsp;<\/span>Apache License<\/a><\/span><\/li><li><span><a href=\"#5.13-Geopy-License\u00b6\" data-toc-modified-id=\"5.13-Geopy-License\u00b6-5.13\"><span class=\"toc-item-num\">5.13&nbsp;&nbsp;<\/span>Geopy License\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.14-Zope-Public-License-2.1\u00b6\" data-toc-modified-id=\"5.14-Zope-Public-License-2.1\u00b6-5.14\"><span class=\"toc-item-num\">5.14&nbsp;&nbsp;<\/span>Zope Public License 2.1\u00b6<\/a><\/span><\/li><li><span><a href=\"#5.15-Zero-Clause-BSD-\/-Free-Public-License-1.0.0-(0BSD)\" data-toc-modified-id=\"5.15-Zero-Clause-BSD-\/-Free-Public-License-1.0.0-(0BSD)-5.15\"><span class=\"toc-item-num\">5.15&nbsp;&nbsp;<\/span>Zero-Clause BSD \/ Free Public License 1.0.0 (0BSD)<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","cb73a506":"## 4.6 Utility Functions related to creating the section in paragraph, cosine similarity and marking sentence in paragraph with help of cosine similarity\n* custom_paragraph_in_document : Creating paragraph based on number of sentences from document\n* avg_feature_vector : finding the average of word vector(feature vecture) using word2vec model\n* mark_best_paragraph_in_document : finding the cosine similarity between query and document(list of sentence) and mark them\n* check_pd_nan : check null value from panda dataframe\n\n##### For complete information of those functions, check docstring as a first statement in each function","dbec660d":"## 3.3 Process Diagram\n\nThe offline and online processes described in previous sections is illustrated below.","adc86eed":"## 4.3 Loading the Corpus and Pre-Trained Model \n\n* ##### Type of Corpus:\n    * base_df : Complete corpus of challenge\n    * journal_df : external datasource for providing the ranking of journal\n\n* ##### Type of Pre-trained Model:\n    * bm25Plus_complete : bm25Plus search model on base_df(complete corpus)\n    * w2v_full_corpus_model_joblib : word2Vec model on corpus model\n\nAll Pre-Trained Model are loaded by using joblib library.","5e686bb0":"![Task1_banner.jpg](attachment:Task1_banner.jpg)","da0c6dae":"## 4. 11 Semantic Tagging\n* find_pos_tags  : finding the pos tagging of word\n* find_semantic_tags : find the semantic tags with bag of words and pos tagging . Future Improvement : sorting of topic's tags \n\n##### For complete information of those functions, check docstring as a first statement in each function","5bdeb98f":"## 3.1 Offline Processing\n\nOffline processing has been completed on CORD-19 and provided as supporting data for this notebook. The process entailed the following major steps:\n1. Downloading CORD-19 from Kaggle and Formatting\n2. Data Preprocessing (Data Cleaning, Custom Stop Words, and Stemming)\n3. Training the BM25Plus Search Model and Word2Vec Model \n\n### 3.1.1 Downloading CORD-19 from Kaggle\n\nThe data_downloader.ipynb notebook implements this offline step. The code in it downloads CORD-19 from Kaggle. The first step after downloading involves creating a connection to Kaggle (using user credentials) to retrieve the data sources. The data sources retrieved consist of four folders: comm_use_subset, noncomm_use_subset, biorxiv_medrxiv, and pmc_custom_license. These folders contain research papers in JSON format. \n\n### 3.1.2 Data Preprocessing\n\nThe data_preparation.ipynb notebook implements this offline step. The code in it loads the data output from [Section 3.1](#3.1-Downloading-and-Formatting-CORD-19-from-Kaggle \"Downloading and Formatting CORD-19 from Kaggle\"). This step processes the data to the degree which is useful for training a search engine. The data frame is categorized into COVID-19 and Non-COVID-19 related research papers. Regular expressions are used to match the corona virus related words within the text. A set of data cleaning strategies is applied to both the COVID-19 and non-COVID-19 data sets.\n\nThe second step involves formatting the data into tabular form and extracting the Title, Abstract, and text information values from the JSON file. Bibliography and Paper ID information are also extracted. The content of each research paper is imported into the \"Text\" column of the table. This text column is broken down into five main parts: Introduction, Method, Results, Discussion, and Conclusion. We are also finding the countries of interest in the articles. The metadata provided is used to retrieve data such as  authors names, journal publish time, and other details necessary for analysis of the data. The output of this code is a data frame containing the following columns: title, paper_id, document_type, abstract, text, introduction, method, result, discussion, conclusion, authors, author_locations, author_countries, author_institutions, biblographies, is_covid, country_tags, publish_time, journal, Microsoft Academic Paper ID, WHO #Covidence, url and row_num.\n\n### 3.1.3 Training the BM25Plus Search Model and Word2Vec Model\n\nTokenization is performed which divides the text into a sequence of words or sentences. Next preprocessing step transform the tokens into lower case. This avoids having multiple copies of the same words. The next step removes punctuation, as to avoid adding any extra information while treating the text data. Other cleaning incorporated methods include removing hyperlinks, addressing post and pre white spaces, and removing numbers and special characters from the text data. Stop Words are also removed from the text data. Both a user-defined list of stop words and stop words defined in predefined libraries serve as inputs for this process. The Stemming function is used to generate the roots of words. The Clean Tokens are then saved in a corpus and will be used as input to training models.\n\nThe bm25_and_word2Vec_model.ipynb trains Best Match 25 (BM25) which is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. This technique is employed to ensure that the resultant articles are free from any noise as in the case of unsupervised techniques. The rank_bm25 module is then used in the online process to determine the research articles that are most applicable. The data structure for the word tokens is its input, and is automatically converted into a BM25 index by the rank_bm25 library. \n\nThis step also trains the word2Vec model to generate word embeddings for the corpus which will be used for query document similarity and ranking of research papers in online process.","b5d854c6":"# 2. Methodology\n\nOur approach intentionally began by conducting individual interviews with medical professionals to gain a deeper understanding of the specific usability and functional considerations that would be most beneficial to COVID-19 related research needs during this time. While rapid and accurate synthesis of the large volume of available research stands as the primary focus, the team has placed a strong emphasis on enabling customized filtering, controlling the input query, and presenting structured and easily accessible results that assist in validating a hypothesis or honing in on a significant find. For an example of the visualization and search UI produced by this notebook, see [Figure 1](#Figure_1 \"Example Search Results\").\n\n## 2.1 Technical Approach\n\n1.  Although LDA topic modeling is one of the most widely used techniques for NLP specific use cases, we encountered a few challenges with respect to managing the number of clusters, considering the subjectivity involved, especially when a medical subject matter expert is not readily available to validating the results from topic modeling. Also, it is challenging for a person to fully comprehend and validate the results against the large corpus of tens of thousands of papers. \n2.  Thus, the team decided to take the approach of using a BM25 + a search engine based approach to first generate a refined list of papers and then apply LDA topic modeling on this reduced subset of papers. This approach not only provides user control of the list of highly relevant papers, but it also affords a contrasting perspective of the topics as the quantity is now quite manageable. \n3.  Additionally, we have also attempted a Question & Answer NLP model approach for retrieving the top relevant highlighted sentences alongside the list of most- related papers. However, we feel because the input \n     is fundamentally a research query and not a straightforward binary problem, the team decided it would be much more valuable to return sufficient information related to the research papers and topics versus      \n     providing a simple specific answer in isolation.  We have included  an additional method of highlighting sentences by incorporating the semantic similarity method from the Gensim library\n4.  The text summarizer algorithm has been used to machine generate a summary for the papers that do not contain an abstract.\n\n\n\n## 2.2 Pros and Cons\n\n**Pros**\n1. This solution has been optimized to quickly return results that are highly related to the input query, enabling tens of thousands of research papers to be analyzed in seconds. This is accomplished by developing a scalable system of interworking Natural Language Processing and Automation techniques to provide the most tangible results while optimizing the computational aspect by utilizing a combination of offline and online processing.\n2. The User Interface (UI) of the solution has been designed to provide intuitive controls that produce orderly, informative, and granular results.  The input criteria enables filtering by Search Type (including journal preference ranking), Date Range, Country of Interest, and Clusters of Similar Papers.\n3. The modularized development approach combines an optimal set of open source automation\/AI functions enabling the seamless additions of future capabilities such as configurable Knowledge Graphs and Paper Impact Rank based on sections, citations, journal ranking, and other criteria.\n\n**Cons**\n1. While we are pleased to offer a machine generated summary in the absence of a journal-provided abstract, at present, we incorporate a generic text summarizer that may not produce the best possible results. A text summarizer that is specifically trained to generate summaries from scientific texts would lead to more powerful summarization results.\n2. While the solution produces relevant results on a general level, providing specific research insights and associations could have been further honed with capabilities such as Knowledge Graph which would allow for actionable results based on the weighted relationship between the primary concepts.   ","aeddf85f":"## 4.13 Fetch and Prepare Data for UI \n* find_min_max_year : finding the min and max year from the list of result articles. Used for UI to provide search slider corresponding to publish year\n* collect_countries : finding all the countries from the list of result articles. Country present in the content of result articles( not where it is published)\n* fetch_cluster_graphical_data : preparing the pyLDAvis graphical data for interactive visualization\n* display_complete_result : Fetching and preparing the result data for UI purpose\n\n##### For complete information of those functions, check docstring as a first statement in each function","8600d1e4":"## 4.4 Initializing Static Variables and Singleton Objects","da99f227":"# 1. Introduction\n\nThis notebook has been created to develop code that finds the most relevant research data with respect to what is known about the transmission, incubation, and environmental stability of the 2019 novel coronavirus (2019-nCoV) and the disease that results from it (COVID-19). The urgency in determining methods for reducing the spread, finding a vaccine, and significantly increasing the recovery rate from COVID-19 amplifies the need to provide ways of getting the latest research into the hands of those directly involved with treating those infected and eliminating the pandemic. \n\n## 1.1 Goal\n\nThe objective of this submission to the CORD-19 Challenge is to provide AI\/Automation-enabled solutions that offer a flexible and efficient means of isolating meaningful research papers and relevant research elements in support of the ongoing efforts of global medical professionals to address the many challenging aspects of the COVID-19 Pandemic. \n\n## 1.2 How it Works\n\nThe code associated with completing this task is provided under two categories: \n1. Code that has been processed offline with the associated notebooks uploaded to Kaggle as input data and pre-trained models for this notebook\n2. Code that is processed online and is documented in this notebook\n\nThe functions performed during the offline and online processes are shown in [Figure 2](#Figure_2 \"Process Flow\").\n\n## 1.3 Results Preview\n\nThe following is a static example of the results produced by this notebook. For the interactive UI that is produced and a detailed explanation of the UI, see [Section 4.14](#4.14-Demo-Result \"Demo Result\").\n","505f0b55":"**Note:** This approach was tested against the CORD-19 dataset available on 04\/16\/2020. As the corpus of papers grows, we may not be able to load the input data due to Kaggle's memory limitations. Should this occur, we recommend downloading this notebook and running it locally in an environment with more than 16 GB of RAM.","7d30c71d":"# 4. Coding the COVID-19 Kaggle Search Engine\n\nThis notebook is used to search articles related to COVID-19 or other articles in CORD-19. \n\nThis notebook provides the following high level functions:\n* Efficient search related to query\n* Clustering of the similiar documents using LDA so that user will not end up reading similiar articles\n* Ranking of the results based on query-doc similiarities, search score and journal reputation\n* Search Refinement using Year, Country and Cluster\n* Summary in case of missing abstract\n* Tagging of a search article to provide complete overview of Article content without reading and seeing complete article\n* Affected Countries mentioned in the Articles\n\nPre-requisites :\n\n* Initialize the working directory, data directory, model directory and view directory based on your environment\n* We are using two input files :\n            * all_articles.csv : complete dataset\n            * journal_ranking_info.csv : external dataset for journal ranking\n  If your directory structure and file name is different, then you have to change two sections: \n         For different directory :\n             * change the variables of this section \"Initialize Data and Model Directory\"\n         For different file name :\n             * change the variables of this section \"Load Corpus and Pre-Trained Model\"\n\n* View files should be present in the view-covid directory\n* Pre Trained models required.\n        BM25Plus model for searching\n        Word2Vec model for query document similarity\n\n        for different model directory structure and model name, change the variables as mentioned above","5a0bf6ae":"![NtbkCui5.jpg](attachment:NtbkCui5.jpg)","3e493cdc":"# 5. License Agreements\n\n\n\n## 5.1 Python Foundation License Version 2\u00b6\n--------------------------------------------\n\n1.\tThis LICENSE AGREEMENT is between the Python Software Foundation (\"PSF\"), and the Individual or Organization (\"Licensee\") accessing and otherwise using this software (\"Python\") in source or binary form and its associated documentation.\n2.\tSubject to the terms and conditions of this License Agreement, PSF hereby grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce, analyze, test, perform and\/or display publicly, prepare derivative works, distribute, and otherwise use Python alone or in any derivative version, provided, however, that PSF's License Agreement and PSF's notice of copyright, i.e., \"Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020 Python Software Foundation; All Rights Reserved\" are retained in Python alone or in any derivative version prepared by Licensee.\n3.\tIn the event Licensee prepares a derivative work that is based on or incorporates Python or any part thereof, and wants to make the derivative work available to others as provided herein, then Licensee hereby agrees to include in any such work a brief summary of the changes made to Python.\n4.\tPSF is making Python available to Licensee on an \"AS IS\" basis.  PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n5.\tPSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON, OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n6.\tThis License Agreement will automatically terminate upon a material breach of its terms and conditions.\n7.\tNothing in this License Agreement shall be deemed to create any relationship of agency, partnership, or joint venture between PSF and Licensee.  This License Agreement does not grant permission to use PSF trademarks or trade name in a trademark sense to endorse or promote products or services of Licensee, or any third party.\n8.\tBy copying, installing or otherwise using Python, Licensee agrees to be bound by the terms and conditions of this License Agreement.\n\n## 5.2 BEOPEN.COM LICENSE AGREEMENT FOR PYTHON 2.0\u00b6\n\n-------------------------------------------\n\nBEOPEN PYTHON OPEN SOURCE LICENSE AGREEMENT VERSION 1\n\n1.\tThis LICENSE AGREEMENT is between BeOpen.com (\"BeOpen\"), having an office at 160 Saratoga Avenue, Santa Clara, CA 95051, and the Individual or Organization (\"Licensee\") accessing and otherwise using this software in source or binary form and its associated documentation (\"the Software\").\n2.\tSubject to the terms and conditions of this BeOpen Python License Agreement, BeOpen hereby grants Licensee a non-exclusive, royalty-free, world-wide license to reproduce, analyze, test, perform and\/or display publicly, prepare derivative works, distribute, and otherwise use the Software alone or in any derivative version, provided, however, that the BeOpen Python License is retained in the Software, alone or in any derivative version prepared by Licensee.\n3.\tBeOpen is making the Software available to Licensee on an \"AS IS\" basis.  BEOPEN MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, BEOPEN MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE SOFTWARE WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n4.\tBEOPEN SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF THE SOFTWARE FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF USING, MODIFYING OR DISTRIBUTING THE SOFTWARE, OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n5.\tThis License Agreement will automatically terminate upon a material breach of its terms and conditions.\n6.\tThis License Agreement shall be governed by and interpreted in all respects by the law of the State of California, excluding conflict of law provisions.  Nothing in this License Agreement shall be deemed to create any relationship of agency, partnership, or joint venture between BeOpen and Licensee.  This License Agreement does not grant permission to use BeOpen trademarks or trade names in a trademark sense to endorse or promote products or services of Licensee, or any\n1.\tthird party.  As an exception, the \"BeOpen Python\" logos available at http:\/\/www.pythonlabs.com\/logos.html may be used according to the permissions granted on that web page.\n7.\tBy copying, installing or otherwise using the software, Licensee agrees to be bound by the terms and conditions of this License Agreement.\n\n## 5.3 CNRI LICENSE AGREEMENT FOR PYTHON 1.6.1\u00b6\n\n---------------------------------------\n\n1.\tThis LICENSE AGREEMENT is between the Corporation for National Research Initiatives, having an office at 1895 Preston White Drive, Reston, VA 20191 (\"CNRI\"), and the Individual or Organization (\"Licensee\") accessing and otherwise using Python 1.6.1 software in source or binary form and its associated documentation.\n2.\tSubject to the terms and conditions of this License Agreement, CNRI hereby grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce, analyze, test, perform and\/or display publicly, prepare derivative works, distribute, and otherwise use Python 1.6.1 alone or in any derivative version, provided, however, that CNRI's License Agreement and CNRI's notice of copyright, i.e., \"Copyright (c) 1995-2001 Corporation for National Research Initiatives; All Rights Reserved\" are retained in Python 1.6.1 alone or in any derivative version prepared by Licensee.  Alternately, in lieu of CNRI's License Agreement, Licensee may substitute the following text (omitting the quotes): \"Python 1.6.1 is made available subject to the terms and conditions in CNRI's License Agreement.  This Agreement together with Python 1.6.1 may be located on the Internet using the following unique, persistent identifier (known as a handle): 1895.22\/1013.  This Agreement may also be obtained from a proxy server on the Internet using the following URL: http:\/\/hdl.handle.net\/1895.22\/1013\".\n3.\tIn the event Licensee prepares a derivative work that is based on or incorporates Python 1.6.1 or any part thereof, and wants to make the derivative work available to others as provided herein, then Licensee hereby agrees to include in any such work a brief summary of the changes made to Python 1.6.1.\n4.\tCNRI is making Python 1.6.1 available to Licensee on an \"AS IS\" basis.  CNRI MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, CNRI MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON 1.6.1 WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n5.\tCNRI SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON 1.6.1 FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON 1.6.1, OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n6.\tThis License Agreement will automatically terminate upon a material breach of its terms and conditions.\n7.\tThis License Agreement shall be governed by the federal intellectual property law of the United States, including without limitation the federal copyright law, and, to the extent such U.S. federal law does not apply, by the law of the Commonwealth of Virginia, excluding Virginia's conflict of law provisions. Notwithstanding the foregoing, with regard to derivative works based on Python 1.6.1 that incorporate non-separable material that was previously distributed under the GNU General Public License (GPL), the law of the Commonwealth of Virginia shall govern this License Agreement only as to issues arising under or with respect to Paragraphs 4, 5, and 7 of this License Agreement.  Nothing in this License Agreement shall be deemed to create any relationship of agency, partnership, or joint venture between CNRI and Licensee.  This License Agreement does not grant permission to use CNRI trademarks or trade name in a trademark sense to endorse or promote products or services of Licensee, or any third party.\n8.\tBy clicking on the \"ACCEPT\" button where indicated, or by copying, installing or otherwise using Python 1.6.1, Licensee agrees to be bound by the terms and conditions of this License Agreement.\n\n\n        ACCEPT\n\n## 5.4 CWI LICENSE AGREEMENT FOR PYTHON 0.9.0 THROUGH 1.2\u00b6\n\n--------------------------------------------------\n\n\nCopyright (c) 1991 - 1995, Stichting Mathematisch Centrum Amsterdam,\nThe Netherlands.  All rights reserved.\n\nPermission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of Stichting Mathematisch Centrum or CWI not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission.\n\nSTICHTING MATHEMATISCH CENTRUM DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT SHALL STICHTING MATHEMATISCH CENTRUM BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n## 5.5 PSF LICENSE AGREEMENT FOR PYTHON 3.8.2\u00b6\n\n1.\tThis LICENSE AGREEMENT is between the Python Software Foundation (\"PSF\"), and the Individual or Organization (\"Licensee\") accessing and otherwise using Python 3.8.2 software in source or binary form and its associated documentation.\n2.\tSubject to the terms and conditions of this License Agreement, PSF hereby grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce, analyze, test, perform and\/or display publicly, prepare derivative works, distribute, and otherwise use Python 3.8.2 alone or in any derivative version, provided, however, that PSF's License Agreement and PSF's notice of copyright, i.e., \"Copyright \u00a9 2001-2020 Python Software Foundation; All Rights Reserved\" are retained in Python 3.8.2 alone or in any derivative version prepared by Licensee.\n3.\tIn the event Licensee prepares a derivative work that is based on or incorporates Python 3.8.2 or any part thereof, and wants to make the derivative work available to others as provided herein, then Licensee hereby agrees to include in any such work a brief summary of the changes made to Python 3.8.2.\n4.\tPSF is making Python 3.8.2 available to Licensee on an \"AS IS\" basis. PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON 3.8.2 WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n5.\tPSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON 3.8.2 FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON 3.8.2, OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n6.\tThis License Agreement will automatically terminate upon a material breach of its terms and conditions.\n7.\tNothing in this License Agreement shall be deemed to create any relationship of agency, partnership, or joint venture between PSF and Licensee. This License Agreement does not grant permission to use PSF trademarks or trade name in a trademark sense to endorse or promote products or services of Licensee, or any third party.\n8.\tBy copying, installing or otherwise using Python 3.8.2, Licensee agrees to be bound by the terms and conditions of this License Agreement.\n\n## 5.6 The 3-Clause BSD License\u00b6\n\nNote: This license has also been called the \"New BSD License\" or \"Modified BSD License\". See also the 2-clause BSD License.\nCopyright \nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n1.\tRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n2.\tRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n3.\tNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n## 5.7 NumPy license\u00b6\n\nCopyright \u00a9 2005-2020, NumPy Developers. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\nNeither the name of the NumPy Developers nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n## 5.8 GNU LESSER GENERAL PUBLIC LICENSE\u00b6\n\nVersion 2.1, February 1999\nCopyright (C) 1991, 1999 Free Software Foundation, Inc. 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n[This is the first released version of the Lesser GPL. It also counts as the successor of the GNU Library Public License, version 2, hence the version number 2.1.] Preamble The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public Licenses are intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users.\nThis license, the Lesser General Public License, applies to some specially designated software packages--typically libraries--of the Free Software Foundation and other authors who decide to use it. You can use it too, but we suggest you first think carefully about whether this license or the ordinary General Public License is the better strategy to use in any particular case, based on the explanations below.\nWhen we speak of free software, we are referring to freedom of use, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish); that you receive source code or can get it if you want it; that you can change the software and use pieces of it in new free programs; and that you are informed that you can do these things.\nTo protect your rights, we need to make restrictions that forbid distributors to deny you these rights or to ask you to surrender these rights. These restrictions translate to certain responsibilities for you if you distribute copies of the library or if you modify it.\nFor example, if you distribute copies of the library, whether gratis or for a fee, you must give the recipients all the rights that we gave you. You must make sure that they, too, receive or can get the source code. If you link other code with the library, you must provide complete object files to the recipients, so that they can relink them with the library after making changes to the library and recompiling it. And you must show them these terms so they know their rights.\nWe protect your rights with a two-step method: (1) we copyright the library, and (2) we offer you this license, which gives you legal permission to copy, distribute and\/or modify the library.\nTo protect each distributor, we want to make it very clear that there is no warranty for the free library. Also, if the library is modified by someone else and passed on, the recipients should know that what they have is not the original version, so that the original author's reputation will not be affected by problems that might be introduced by others.\nFinally, software patents pose a constant threat to the existence of any free program. We wish to make sure that a company cannot effectively restrict the users of a free program by obtaining a restrictive license from a patent holder. Therefore, we insist that any patent license obtained for a version of the library must be consistent with the full freedom of use specified in this license.\nMost GNU software, including some libraries, is covered by the ordinary GNU General Public License. This license, the GNU Lesser General Public License, applies to certain designated libraries, and is quite different from the ordinary General Public License. We use this license for certain libraries in order to permit linking those libraries into non-free programs.\nWhen a program is linked with a library, whether statically or using a shared library, the combination of the two is legally speaking a combined work, a derivative of the original library. The ordinary General Public License therefore permits such linking only if the entire combination fits its criteria of freedom. The Lesser General Public License permits more lax criteria for linking other code with the library.\nWe call this license the \"Lesser\" General Public License because it does Less to protect the user's freedom than the ordinary General Public License. It also provides other free software developers Less of an advantage over competing non-free programs. These disadvantages are the reason we use the ordinary General Public License for many libraries. However, the Lesser license provides advantages in certain special circumstances.\nFor example, on rare occasions, there may be a special need to encourage the widest possible use of a certain library, so that it becomes a de-facto standard. To achieve this, non-free programs must be allowed to use the library. A more frequent case is that a free library does the same job as widely used non-free libraries. In this case, there is little to gain by limiting the free library to free software only, so we use the Lesser General Public License.\nIn other cases, permission to use a particular library in non-free programs enables a greater number of people to use a large body of free software. For example, permission to use the GNU C Library in non-free programs enables many more people to use the whole GNU operating system, as well as its variant, the GNU\/Linux operating system.\nAlthough the Lesser General Public License is Less protective of the users' freedom, it does ensure that the user of a program that is linked with the Library has the freedom and the wherewithal to run that program using a modified version of the Library.\nThe precise terms and conditions for copying, distribution and modification follow. Pay close attention to the difference between a \"work based on the library\" and a \"work that uses the library\". The former contains code derived from the library, whereas the latter must be combined with the library in order to run.\nTERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n1.\tThis License Agreement applies to any software library or other program which contains a notice placed by the copyright holder or other authorized party saying it may be distributed under the terms of this Lesser General Public License (also called \"this License\"). Each licensee is addressed as \"you\".\nA \"library\" means a collection of software functions and\/or data prepared so as to be conveniently linked with application programs (which use some of those functions and data) to form executables.\nThe \"Library\", below, refers to any such software library or work which has been distributed under these terms. A \"work based on the Library\" means either the Library or any derivative work under copyright law: that is to say, a work containing the Library or a portion of it, either verbatim or with modifications and\/or translated straightforwardly into another language. (Hereinafter, translation is included without limitation in the term \"modification\".)\n\"Source code\" for a work means the preferred form of the work for making modifications to it. For a library, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the library.\nActivities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running a program using the Library is not restricted, and output from such a program is covered only if its contents constitute a work based on the Library (independent of the use of the Library in a tool for writing it). Whether that is true depends on what the Library does and what the program that uses the Library does.\n1.\tYou may copy and distribute verbatim copies of the Library's complete source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and distribute a copy of this License along with the Library.\nYou may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee.\n2.\tYou may modify your copy or copies of the Library or any portion of it, thus forming a work based on the Library, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions:\na) The modified work must itself be a software library. b) You must cause the files modified to carry prominent notices stating that you changed the files and the date of any change. c) You must cause the whole of the work to be licensed at no charge to all third parties under the terms of this License. d) If a facility in the modified Library refers to a function or a table of data to be supplied by an application program that uses the facility, other than as an argument passed when the facility is invoked, then you must make a good faith effort to ensure that, in the event an application does not supply such function or table, the facility still operates, and performs whatever part of its purpose remains meaningful. (For example, a function in a library to compute square roots has a purpose that is entirely well-defined independent of the application. Therefore, Subsection 2d requires that any application-supplied function or table used by this function must be optional: if the application does not supply it, the square root function must still compute square roots.)\nThese requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Library, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Library, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it.\nThus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Library.\nIn addition, mere aggregation of another work not based on the Library with the Library (or with a work based on the Library) on a volume of a storage or distribution medium does not bring the other work under the scope of this License.\n3.\tYou may opt to apply the terms of the ordinary GNU General Public License instead of this License to a given copy of the Library. To do this, you must alter all the notices that refer to this License, so that they refer to the ordinary GNU General Public License, version 2, instead of to this License. (If a newer version than version 2 of the ordinary GNU General Public License has appeared, then you can specify that version instead if you wish.) Do not make any other change in these notices.\nOnce this change is made in a given copy, it is irreversible for that copy, so the ordinary GNU General Public License applies to all subsequent copies and derivative works made from that copy.\nThis option is useful when you wish to copy part of the code of the Library into a program that is not a library.\n4.\tYou may copy and distribute the Library (or a portion or derivative of it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange.\nIf distribution of object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place satisfies the requirement to distribute the source code, even though third parties are not compelled to copy the source along with the object code.\n5.\tA program that contains no derivative of any portion of the Library, but is designed to work with the Library by being compiled or linked with it, is called a \"work that uses the Library\". Such a work, in isolation, is not a derivative work of the Library, and therefore falls outside the scope of this License.\nHowever, linking a \"work that uses the Library\" with the Library creates an executable that is a derivative of the Library (because it contains portions of the Library), rather than a \"work that uses the library\". The executable is therefore covered by this License. Section 6 states terms for distribution of such executables.\nWhen a \"work that uses the Library\" uses material from a header file that is part of the Library, the object code for the work may be a derivative work of the Library even though the source code is not. Whether this is true is especially significant if the work can be linked without the Library, or if the work is itself a library. The threshold for this to be true is not precisely defined by law.\nIf such an object file uses only numerical parameters, data structure layouts and accessors, and small macros and small inline functions (ten lines or less in length), then the use of the object file is unrestricted, regardless of whether it is legally a derivative work. (Executables containing this object code plus portions of the Library will still fall under Section 6.)\nOtherwise, if the work is a derivative of the Library, you may distribute the object code for the work under the terms of Section 6. Any executables containing that work also fall under Section 6, whether or not they are linked directly with the Library itself.\n6.\tAs an exception to the Sections above, you may also combine or link a \"work that uses the Library\" with the Library to produce a work containing portions of the Library, and distribute that work under terms of your choice, provided that the terms permit modification of the work for the customer's own use and reverse engineering for debugging such modifications.\nYou must give prominent notice with each copy of the work that the Library is used in it and that the Library and its use are covered by this License. You must supply a copy of this License. If the work during execution displays copyright notices, you must include the copyright notice for the Library among them, as well as a reference directing the user to the copy of this License. Also, you must do one of these things:\na) Accompany the work with the complete corresponding machine-readable source code for the Library including whatever changes were used in the work (which must be distributed under Sections 1 and 2 above); and, if the work is an executable linked with the Library, with the complete machine-readable \"work that uses the Library\", as object code and\/or source code, so that the user can modify the Library and then relink to produce a modified executable containing the modified Library. (It is understood that the user who changes the contents of definitions files in the Library will not necessarily be able to recompile the application to use the modified definitions.) b) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (1) uses at run time a copy of the library already present on the user's computer system, rather than copying library functions into the executable, and (2) will operate properly with a modified version of the library, if the user installs one, as long as the modified version is interface-compatible with the version that the work was made with. c) Accompany the work with a written offer, valid for at least three years, to give the same user the materials specified in Subsection 6a, above, for a charge no more than the cost of performing this distribution. d) If distribution of the work is made by offering access to copy from a designated place, offer equivalent access to copy the above specified materials from the same place. e) Verify that the user has already received a copy of these materials or that you have already sent this user a copy. For an executable, the required form of the \"work that uses the Library\" must include any data and utility programs needed for reproducing the executable from it. However, as a special exception, the materials to be distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable.\nIt may happen that this requirement contradicts the license restrictions of other proprietary libraries that do not normally accompany the operating system. Such a contradiction means you cannot use both them and the Library together in an executable that you distribute.\n7.\tYou may place library facilities that are a work based on the Library side-by-side in a single library together with other library facilities not covered by this License, and distribute such a combined library, provided that the separate distribution of the work based on the Library and of the other library facilities is otherwise permitted, and provided that you do these two things:\na) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities. This must be distributed under the terms of the Sections above. b) Give prominent notice with the combined library of the fact that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work.\n8.\tYou may not copy, modify, sublicense, link with, or distribute the Library except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, link with, or distribute the Library is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.\n9.\tYou are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Library or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Library (or any work based on the Library), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Library or works based on it.\n10.\tEach time you redistribute the Library (or any work based on the Library), the recipient automatically receives a license from the original licensor to copy, distribute, link with or modify the Library subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties with this License.\n11.\tIf, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Library at all. For example, if a patent license would not permit royalty-free redistribution of the Library by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Library.\nIf any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply, and the section as a whole is intended to apply in other circumstances.\nIt is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author\/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice.\nThis section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License.\n12.\tIf the distribution and\/or use of the Library is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Library under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License.\n13.\tThe Free Software Foundation may publish revised and\/or new versions of the Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Library specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Library does not specify a license version number, you may choose any version ever published by the Free Software Foundation.\n14.\tIf you wish to incorporate parts of the Library into other free programs whose distribution conditions are incompatible with these, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally.\nNO WARRANTY\n15.\tBECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND\/OR OTHER PARTIES PROVIDE THE LIBRARY \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE LIBRARY IS WITH YOU. SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n16.\tIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND\/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE LIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\nEND OF TERMS AND CONDITIONS How to Apply These Terms to Your New Libraries If you develop a new library, and you want it to be of the greatest possible use to the public, we recommend making it free software that everyone can redistribute and change. You can do so by permitting redistribution under these terms (or, alternatively, under the terms of the ordinary General Public License).\nTo apply these terms, attach the following notices to the library. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.\none line to give the library's name and an idea of what it does. Copyright (C) year name of author\nThis library is free software; you can redistribute it and\/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation; either version 2.1 of the License, or (at your option) any later version.\nThis library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.\nYou should have received a copy of the GNU Lesser General Public License along with this library; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n\n## 5.9 The MIT License\u00b6\n\nCopyright \nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n## 5.10 License agreement for matplotlib versions 1.3.0 and later\u00b6\n\n=========================================================\n1.\tThis LICENSE AGREEMENT is between the Matplotlib Development Team (\"MDT\"), and the Individual or Organization (\"Licensee\") accessing and otherwise using matplotlib software in source or binary form and its associated documentation.\n2.\tSubject to the terms and conditions of this License Agreement, MDT hereby grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce, analyze, test, perform and\/or display publicly, prepare derivative works, distribute, and otherwise use matplotlib alone or in any derivative version, provided, however, that MDT's License Agreement and MDT's notice of copyright, i.e., \"Copyright (c) 2012- Matplotlib Development Team; All Rights Reserved\" are retained in matplotlib alone or in any derivative version prepared by Licensee.\n3.\tIn the event Licensee prepares a derivative work that is based on or incorporates matplotlib or any part thereof, and wants to make the derivative work available to others as provided herein, then Licensee hereby agrees to include in any such work a brief summary of the changes made to matplotlib .\n4.\tMDT is making matplotlib available to Licensee on an \"AS IS\" basis. MDT MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION, MDT MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n5.\tMDT SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING MATPLOTLIB , OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n6.\tThis License Agreement will automatically terminate upon a material breach of its terms and conditions.\n7.\tNothing in this License Agreement shall be deemed to create any relationship of agency, partnership, or joint venture between MDT and Licensee. This License Agreement does not grant permission to use MDT trademarks or trade name in a trademark sense to endorse or promote products or services of Licensee, or any third party.\n8.\tBy copying, installing or otherwise using matplotlib , Licensee agrees to be bound by the terms and conditions of this License Agreement.\n\n## 5.11 3-clause license (\"BSD License 2.0\", \"Revised BSD License\", \"New BSD License\", or \"Modified BSD License\")\u00b6\n\nCopyright (c) , All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n* Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and\/or other materials provided with the distribution.\n* Neither the name of the <organization> nor the\n  names of its contributors may be used to endorse or promote products\n  derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n## 5.12 Apache License\n\nVersion 2.0, January 2004\nhttp:\/\/www.apache.org\/licenses\/\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n1.\tDefinitions.\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n2.\tGrant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n3.\tGrant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n4.\tRedistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and You must cause any modified files to carry prominent notices stating that You changed the files; and You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\n5.\tSubmission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n6.\tTrademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n7.\tDisclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n8.\tLimitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n9.\tAccepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and\/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\nEND OF TERMS AND CONDITIONS\n\n## 5.13 Geopy License\u00b6\n\nCopyright (c) 2006-2018 geopy authors (see AUTHORS)\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n## 5.14 Zope Public License 2.1\u00b6\n\nFull name\nZope Public License 2.1\nShort identifier\nZPL-2.1\nOther web pages for this license\nhttp:\/\/old.zope.org\/Resources\/ZPL\/\nNotes\nThis is a generic version of the ZPL 2.0 license\nText\nZope Public License (ZPL) Version 2.1\nA copyright notice accompanies this license document that identifies the copyright holders.\nThis license has been certified as open source. It has also been designated as GPL compatible by the Free Software Foundation (FSF).\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n1.\tRedistributions in source code must retain the accompanying copyright notice, this list of conditions, and the following disclaimer. \n2.\tRedistributions in binary form must reproduce the accompanying copyright notice, this list of conditions, and the following disclaimer in the documentation and\/or other materials provided with the distribution. \n3.\tNames of the copyright holders must not be used to endorse or promote products derived from this software without prior written permission from the copyright holders. \n4.\tThe right to distribute this software or to use it for any purpose does not give you the right to use Servicemarks (sm) or Trademarks (tm) of the copyright holders. Use of them is covered by separate agreement with the copyright holders. \n5.\tIf any files are modified, you must cause the modified files to carry prominent notices stating that you changed the files and the date of any change.\n    \n## 5.15 Zero-Clause BSD \/ Free Public License 1.0.0 (0BSD)\n\nSPDX short identifier: 0BSD \nNote: Despite its name, Zero-Clause BSD is an alteration of the ISC license, and is not textually derived from licenses in the BSD family. Zero-Clause BSD was originally approved under the name \"Free Public License 1.0.0\".\nZero-Clause BSD\nPermission to use, copy, modify, and\/or distribute this software for any purpose with or without fee is hereby granted.\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.# This Python 3 environment comes with many helpful analytics libraries installed\n","25e51473":"## 4.5 Class : Article and ArticleEncoder\n\n* Article class is used for the purpose of the transferring the data to View (Html). \n* Each Row from the result of search query need to be converted to Article object and send it to jinja2 for rendering.\n* Fields of this class are corresponding the row columns of result dataframe\n* ArticleEncoder : Json Encoder class for generating Json object corresponding to Article class","ee1d2ac3":"## 4. 9 Query Document Similarity\n* query_docs_similiarity  : finding the query document similarity using word2vec and cosine similarity for every document in search result \n\n##### For complete information of those functions, check docstring as a first statement in each function","204c2625":"## 4.12 Converting all the search research information to List of Articles Object( DTO)\n* get_articles_from_df  : Convert search result df to list of Article object. Output will be used for UI purpose\n* get_query_sim_score : finding query document similarity score for particular document by document number. Sorting is based on this value\n* find_cluster_id : finding cluster id for particular document by document number. Filtering Feature based on this value.\n* sort_articles_by_query_sim : Sort the list of articles by query document similarity score\n\n##### For complete information of those functions, check docstring as a first statement in each function","841295b1":"## 4.10 Query Results\n* find_query_results  : finding the search results using BM25plus model\n* find_query_df : finding the result dataframe form the index list returned by find_query_results\n* find_num_clusters : finding the number of clusters for topic modelling\n* find_final_results : finding the search result with search score and cluster id with the help of BM25Plus and Topic modelling using lda\n\n##### For complete information of those functions, check docstring as a first statement in each function","4c9f590d":"## 4.14 Demo Result\n\n### 4.14.1 Run this cell(Ctrl-Enter) to see the ranked seach articles with augmented information\n      \n* ###### Steps to run this cell :\n        * query : change the query variable  based on your interest\n        * num_records : number of record to be fetch. \n        * is_covid : True  # True - for covid related articles only ; False :  all type of articles in corpus\n        * run the cell by (ctrl-Enter)\n\nWhen interpreting the resulting visualization, note that larger topics indicate higher frequency and that topics in proximity to one another have a higher degree of similarity.","69279fb0":"## 3.2 Online Processing\n\nOnline processing is done to create the search UI and the functionality behind it. This functionality is used to search the dataset provided by the offline process. The code for the online process is developed in this notebook and entails the following key steps:\n1. BM25 Search\n2. Topic Modeling\n3. Semantic Tagging\n4. Applying the Word2Vec Model\n5. Applying the Ranking Logic\n6. Using the Text Summarizer\n\n### 3.2.1 BM25 Search\n\nThe search function takes the search query and the value of \"n,\" the number of top results to include, and preprocesses the query to create a BM25 score for every research article. After eacch research article is scored, the search function then returns a list of the top \"n\" papers based on the BM25 scores. This model is run on both on the COVID-19 and Non-COVID-19 research paper datasets.\n\n### 3.2.2 Topic Modeling\n\nTopic modelling is used to generate clusters. A cluster is assigned to each paper based on the results from BM25. Our working hypothesis is: Similar documents can be ranked higher for a query and the user will end up reading similar documents because of their rank by means of clustering. Using topic modelling, the user can hover over different clusters to read different types of articles. Latent Dirichlet Allocation (LDA) is used to associate text in a document to various topics. Using LDA, human interpretable topics are extracted from a document corpus, such that each topic is characterized by its most closely associated words.\n\nThe end-user provides an input query and the search function preprocesses the query and creates a BM25 score for every paper, then returns the top \"n\" BM25-scoring papers. The BM-25 scored articles are tokenized. Words are converted to lowercase and punctuation is removed. \n\nAll stop words are removed and words are lemmatized \u2014 words in third person are changed to first person and verbs in past and future tenses are changed to present tense. Words are also stemmed or as described above, reduced to their root form. In the final step, n-grams, which are the combinations of multiple words used together, are generated. n-grams with n=1 are called unigrams. Similarly, bigrams are defined as n=2 and trigrams as n=3. bigrams and trigrams usually contain much more valuable information than unigrams. The basic principle behind n-grams is the capturing of the language structure, such as which letter or word is likely to follow a given one. The longer the n-gram (the higher the value of n), the more context that is passed to the model. These n-grams are than loaded into the Topic Modelling function to generate a Cluster of Topics.\n\n### 3.2.3 Semantic Tagging\n\nThis block of the code creates semantic tags for the top articles shortlisted. These tags present a general overview of the content of the research paper. First, we use count vectorization which involves counting the number of occurrences of each word that appears in the article. Post this step, we perform Part of Speech (POS) tagging on the words generated. The part of speech explains how a word is used in a sentence. There are eight main parts of speech - nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections. In our case we have used NOUNS as tag display information. In Addition, we are extracting Country Tags from the article using geotext library.\n\n### 3.2.4 Applying the Word2Vec Model\n\nWord2Vec model is used to find similarity between the query and documents as well as the query and the sentences to highlight the most relevant text of the article corresponding to the query selected. We created word embeddings using word2vec model. Using Word2Vec embeddings, word will be represented as an array. Generated word embeddings need to be compared in order to get semantic similarity between two vectors. We have used Cosine similarity to calculate the similarity between the query vector and doc vector. \n\n### 3.2.5 Applying the Ranking Logic\n\nThe Ranking algorithm using Journal Reputation and document similarity between doc and the task query. Journals generated have been sorted according to their ranking in Google scholar publications page: https:\/\/www.scimagojr.com\/journalrank.php?order=h&ord=desc . Also, we are using text similarity to cluster similar documents together. Text similarity determines how \u2018close\u2019 two pieces of text are both in surface closeness: lexical similarity and meaning: semantic similarity. These both strategies along with search ranking is used to display the most relevant articles on the UI.\n\n### 3.2.6 Using the Text Summarizer\n\nIn cases where an abstract is not available for a research paper, the Text Summarizer is used to create and display a summary of the research paper. The Gensim summarization module automatically summarizes the given text, extracting one or more key sentences from the text. It does so by implementing Text Rank, an unsupervised algorithm based on weighted graphs. After preprocessing text, this algorithm builds a graph with sentences as nodes and selects the sentences with the highest page rank to include in the summary of the document.","72e6b365":"<a id='Figure_2'><\/a>\n**Figure 2**  Process Flow","129798be":"<a id='Figure_1'><\/a>\n**Figure 1**  Example Search Results","de08d63b":"# 3. Coding Process\n\nThe following subsections describe the processes used to provide our solution.","c53a8316":"## 4.7 Text Preprocessing and Token Generation\n* create_tokens  : create tokens using following processing like stemming and other basic processing\n* create_tm_tokens : create tokens for topic modelling ( with the help of lemmatizer also)\n* create_complete_text : Create complete text of an article which is row of dataframe\n* create_stem_dictionary : Create stem words dictionary . this dictionary will be used to display purpose of LDA tokens\n* create_stem_dictionary_tokens : Create stem words dictionary and stem tokens\n\n##### For complete information of those functions, check \"\"\" \"\"\" docstring in each function"}}