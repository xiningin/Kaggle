{"cell_type":{"c0339b74":"code","dfbd3b7e":"code","aa94ffed":"code","995fd20c":"code","d6d8a70b":"code","90efff6b":"code","860d70b7":"code","0eaec6b4":"code","ba66fa35":"code","68f887fa":"code","679fa9ad":"code","2759ff21":"code","70651c34":"code","38c60d77":"code","c581ecdb":"code","a3361265":"code","000b5c15":"code","c1e4e12b":"code","b6943671":"code","b9334534":"code","29de6550":"code","b2213044":"code","2244e48b":"code","402c7a89":"code","416eb5f8":"code","2a560878":"code","0589e7c2":"code","105e2519":"code","8aac2418":"code","32034fdf":"code","8279a4fc":"code","21257870":"code","4b92f6b6":"code","c199acc1":"code","0f983170":"code","2885029f":"markdown"},"source":{"c0339b74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dfbd3b7e":"#Lets load useful packages for start\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy","aa94ffed":"# Loading dataset (pulsar classification)\ndf = pd.read_csv(\"..\/input\/predicting-a-pulsar-star\/pulsar_stars.csv\")\ndf.head()","995fd20c":"# Let's see if we have some nan values in dataset\ndf.isnull().sum()","d6d8a70b":"# Ok no nan values -> that's nice!\n# I will treat features in the dataset as black box as i dont know what they mean. I will simply rename them as V1 to V8 and treat\n# them as some given values\ndf.rename({\"target_class\":\"Class\", df.columns[0]:\"V1\", df.columns[1]:\"V2\", df.columns[2]:\"V3\", df.columns[3]:\"V4\", df.columns[4]:\"V5\",\n                  df.columns[5]:\"V6\", df.columns[6]:\"V7\", df.columns[7]:\"V8\" }, axis=1, inplace=True) ","90efff6b":"df.head()","860d70b7":"# Let's look on balance between classes 0 and 1\nsns.countplot(\"Class\", data=df)","0eaec6b4":"# Let's look on percentage of classes\nprint(\"Percentage of 0 class: \", round(df[\"Class\"].value_counts()[0]\/len(df) * 100, 2), \"%\")\nprint(\"Percentage of 1 class: \", round(df[\"Class\"].value_counts()[1]\/len(df) * 100, 2), \"%\")","ba66fa35":"# As you can see, we have pretty unbalanced data so we need to take care of that later by undersampling\n# Let's see if we need to scale all of the data\ndf.describe()","68f887fa":"# Unfortunatly yes, we will choose RobustScaler for this task as its prone to outliers (there is a lot, we will see that soon)\nfrom sklearn.preprocessing import RobustScaler\n\ndf_class = df[\"Class\"]\ndf.drop(\"Class\", axis=1, inplace=True)\n\nscaler = RobustScaler()\n\nfor column in df.columns:\n    df[column] = scaler.fit_transform(df[column].values.reshape(-1, 1))\n\ndf = pd.concat([df, df_class], axis=1)\ndf.head()","679fa9ad":"# Ok we now have scaled all features\n# Now we need to make original train and test set, and then create undersampled set\n# original test set will be final evaluation\nfrom sklearn.model_selection import StratifiedKFold\n\nsss = StratifiedKFold()\nX = df.drop(\"Class\", axis=1)\ny = df[\"Class\"]\n\nfor train_index, test_index in sss.split(X, y):\n    original_X_train, original_X_test = X.iloc[train_index], X.iloc[test_index]\n    original_y_train, original_y_test = y.iloc[train_index], y.iloc[test_index]\n\noriginal_X_train = original_X_train.values\noriginal_X_test = original_X_test.values#original data\noriginal_y_train = original_y_train.values\noriginal_y_test = original_y_test.values","2759ff21":"#Now we need to create dataset with class ratio 50\/50 (undersampled)\n\ndf = df.sample(frac=1) #shuffling first\n\npulsar_yes_df = df.loc[df[\"Class\"] == 1]\npulsar_no_df = df.loc[df[\"Class\"] == 0][:len(pulsar_yes_df)]\n\ndf_new = pd.concat([pulsar_yes_df, pulsar_no_df])\n\ndf_new = df_new.sample(frac=1, random_state=42)\ndf_new.head()","70651c34":"sns.countplot(\"Class\", data=df_new) #perfectly balanced ..as all things should be (popculture joke haha)","38c60d77":"# Let's look on correlation matrix between features and classes\nf, ax1 = plt.subplots(1, 1, figsize=(8, 6))\n\ncorr = df_new.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot=True, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\nax1.set_ylim(9, 0)\nplt.show()","c581ecdb":"# The most negative are features V1 and V7 and the most positive are features V3 and V6 (we are looking on the last row)\n# Lets make boxplots\n\n# Negative Correlations\nf, axes = plt.subplots(ncols=2, figsize=(20, 6))\n\nsns.boxplot(x=\"Class\", y=\"V1\", data=df_new, ax=axes[0])\naxes[0].set_title(\"V1 vs Class Negative corr\")\n\nsns.boxplot(x=\"Class\", y=\"V7\", data=df_new, ax=axes[1])\naxes[1].set_title(\"V7 vs Class Negative corr\")","a3361265":"# Positive Correlations\nf, axes = plt.subplots(ncols=2, figsize=(20, 6))\nsns.boxplot(x=\"Class\", y=\"V3\", data=df_new, ax=axes[0])\naxes[0].set_title(\"V3 vs Class Positive corr\")\n\nsns.boxplot(x=\"Class\", y=\"V6\", data=df_new, ax=axes[1])\naxes[1].set_title(\"V6 vs Class Positive corr\")","000b5c15":"# we can see that we have a lot of outliers! Let's take care of that. Ouliers removing will make our model better in performance.\n# We just have to expand lower and upper quartile range\n\nv1_pulsar = df_new[\"V1\"].loc[df_new[\"Class\"] == 1].values\nq25, q75 = np.percentile(v1_pulsar, 25), np.percentile(v1_pulsar, 75)\n\nv1_iqr = q75 - q25\nv1_cutoff = v1_iqr * 2.75\nv1_lower, v1_upper = q25 - v1_cutoff, q75 + v1_cutoff\n\ndf_past = df_new.copy()\ndf_new = df_new.drop(df_new[(df_new[\"V1\"] > v1_upper) | (df_new[\"V1\"] < v1_lower)].index)\n\noutliers = len(df_past) - len(df_new)\nprint(\"Number of removed outliers: \", outliers)\n\n#V7 outlier delete\nv7_pulsar = df_new[\"V7\"].loc[df_new[\"Class\"] == 1].values \nq25, q75 = np.percentile(v7_pulsar, 25), np.percentile(v7_pulsar, 75)\n\nv7_iqr = q75 - q25\nv7_cutoff = v7_iqr * 2.75\nv7_lower, v7_upper = q25 - v7_cutoff, q75 + v7_cutoff\n\ndf_past = df_new.copy()\ndf_new = df_new.drop(df_new[(df_new[\"V7\"] > v7_upper) | (df_new[\"V7\"] < v7_lower)].index)\n\noutliers = len(df_past) - len(df_new)\nprint(\"Number of removed outliers: \", outliers)","c1e4e12b":"#Features after outliers removal\nf,  axes = plt.subplots(ncols=2, figsize=(20, 6))\n\nsns.boxplot(x=\"Class\", y=\"V1\", data=df_new, ax=axes[0])\naxes[0].set_title(\"V1 vs class negative corr after outliers removing\")\n\nsns.boxplot(x=\"Class\", y=\"V7\", data=df_new, ax=axes[1])\naxes[1].set_title(\"V7 vs class negative corr after outliers removing\")","b6943671":"#It's slightly better\nsns.countplot(\"Class\", data=df_new)","b9334534":"#Our undersampled dataset is still balanced\n#Let's try clustering and dimension reduction via PCA and tsne just for higher perspective\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\npca = PCA(n_components=2, random_state=42)\ntsne = TSNE(n_components=2, random_state=42)\n#df_new is from the random undersample data (fewer instances)\nX = df_new.drop(\"Class\", axis=1)\ny = df_new[\"Class\"]\n\nX_reduced = pca.fit_transform(X.values)\nX_reduced_tsne = tsne.fit_transform(X.values)","29de6550":"f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 6))\n\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Pulsars', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Pulsars', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\nax1.grid(True)\n\nax2.scatter(X_reduced[:,0], X_reduced[:,1], c=(y==0), cmap=\"coolwarm\", label=\"No Pulsars\", linewidth=2)\nax2.scatter(X_reduced[:,0], X_reduced[:,1], c=(y==1), cmap=\"coolwarm\", label=\"Pulsars\", linewidth=2)\nax2.grid(True)\nax2.set_title(\"PCA\", fontsize=14)\nplt.legend()\nplt.show()\n\n#Blue color is class 0","b2213044":"# We can see that pulsar and non-pulsar cases are kind of separated, so our models will perform well (and plots look cool)\n# Let's split out undersampled data\nX = df_new.drop(\"Class\", axis=1)\ny = df_new[\"Class\"]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","2244e48b":"# Turning to arrays so we can feed this data to our ML algorithms\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","402c7a89":"#importing four models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","416eb5f8":"#lets now test multiple classifiers\nclassifiers = {\n    \"LogisticRegression\":LogisticRegression(),\n    \"KNearest\":KNeighborsClassifier(),\n    \"Support Vector Machine\":SVC(),\n    \"Random Forest\":RandomForestClassifier()\n}","2a560878":"from sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifier: \",classifier.__class__.__name__, \"Has score: \", round(training_score.mean(), 4) * 100, \"%\")","0589e7c2":"#Ok we can see that Logistic Regression has achieved the best accuracy\n#Let's now plot ROC curves for all classifers and see which one is the best \nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg = LogisticRegression()\nsvm = SVC()\nknear = KNeighborsClassifier()\nforest = RandomForestClassifier()\n\ny_pred_log = cross_val_predict(log_reg, X_train, y_train, cv=5)\ny_pred_svm = cross_val_predict(svm, X_train, y_train, cv=5)\ny_pred_knear = cross_val_predict(knear, X_train, y_train, cv=5)\ny_pred_forest = cross_val_predict(forest, X_train, y_train, cv=5)\n\nlog_fpr, log_tpr, log_thresold = roc_curve(y_train, y_pred_log)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, y_pred_knear)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, y_pred_svm)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, y_pred_forest)\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, y_pred_log)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, y_pred_svm)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, y_pred_svm)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, y_pred_forest)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n\ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","105e2519":"# Okey, Logistic Regression is still the best! We will try to optimize this model\nfrom sklearn.model_selection import GridSearchCV\n\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 25, 50, 75, 100, 250, 500, 750, 1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n\nlog_reg = grid_log_reg.best_estimator_\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=10)\n\nprint(\"Accuracy of logistic regression with optimal parameters is: \", \n      round(log_reg_score.mean(), 4) * 100, \"%\")","8aac2418":"# We need to get sure if our model is overfitting or underfitting\n# Let's plot learning curve of logistic regression model\nfrom sklearn.model_selection import learning_curve, ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n","32034fdf":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, \"Logistic Regression with optimal parametets\", X_train, y_train, cv=cv, n_jobs=4)\nplt.show()","8279a4fc":"# Ok that's promising\n# I would like to look at precision\/recall tradeoff curve as well\nfrom sklearn.metrics import precision_recall_curve\n\ny_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_pred)\n\ndef plot_prec_recall_vs_thresh(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='precision')\n    plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='upper left')\n    plt.ylim([0,1])\n\nplot_prec_recall_vs_thresh(precisions, recalls, thresholds)\nplt.legend()\nplt.show()","21257870":"# Not great not terrible (joking it looks just fine)\n#-------------------------------------------ErrorEvaluation----------------------------------------\n# Let's make confusion matrix on undersampled data\nfrom sklearn.metrics import confusion_matrix\n\ny_test_under_pred = cross_val_predict(log_reg, X_test, y_test, cv=5)\n\nf, ax = plt.subplots(1, 1, figsize=(10, 6))\n\nconf = confusion_matrix(y_test, y_test_under_pred)\nsns.heatmap(conf, cmap=\"Greys\", fmt=\"d\", annot=True,  linewidths=5, annot_kws={\"size\": 15}, ax=ax)\nax.set_ylim(2, 0)\nplt.show()","4b92f6b6":"#Okey, that's great\n#Last thing we need to do is to classify original test data and look for accuracy\n\nf, ax = plt.subplots(1, 1, figsize=(10, 8))\n\ny_test_pred_original = cross_val_predict(log_reg, original_X_test, original_y_test, cv=5)\nconf = confusion_matrix(original_y_test, y_test_pred_original)\nsns.heatmap(conf, cmap=\"Greys\", annot=True, annot_kws={\"size\":15}, ax=ax, fmt=\"g\")\nax.set_ylim(2, 0)\nplt.show()","c199acc1":"#Dont panic, only one black square means that our original data are very unbalanced\n# Let's look on classification_report\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(original_y_test, y_test_pred_original))","0f983170":"#And last thing is our optimized model's accuracy:\nfrom sklearn.metrics import accuracy_score\n\nfinal_accuracy = accuracy_score(original_y_test, y_test_pred_original)\nprint(\"Our model's final accuracy is: {} %\".format(round(final_accuracy, 3)*100))","2885029f":"This score is satisfying!!\nThis was my first kernel so i will be really glad for any feedback from this wonderful community!\nSee you!"}}