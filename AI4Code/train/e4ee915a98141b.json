{"cell_type":{"bfe6c578":"code","88d2818d":"code","ebdc8336":"code","a2f3224a":"code","206859cd":"code","1e1cbb3c":"code","9d95965c":"code","82f8dcd5":"code","d599a192":"code","bef4a6fe":"code","197d533b":"code","800aa24c":"code","868edac5":"code","073bc9fd":"markdown","7f92e461":"markdown"},"source":{"bfe6c578":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88d2818d":"df = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ndf.head()","ebdc8336":"img = np.asarray(df.iloc[3,1:]) # Indexing the 4th row of the dataframe\nimg = img.reshape(28,28,1) # Reshaping to visualise\nplt.imshow(img)","a2f3224a":"X = df.iloc[:,1:].values\ny = df.iloc[:,0].values\n\nfrom sklearn.model_selection import train_test_split as tts\nX_train,X_test,y_train,y_test = tts(X,y,test_size=0.2,random_state=0)\n\nprint([x.shape for x in [X_train,X_test,y_train,y_test]])","206859cd":"X_train = X_train\/255\nX_test = X_test\/255\n\nX_train = X_train.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)","1e1cbb3c":"print([x.shape for x in [X_train,X_test,y_train,y_test]])","9d95965c":"def plot_history(history):\n    plt.figure(figsize=(25,10))\n    # summarize history for accuracy\n    plt.subplot(1,2,1)\n    plt.plot(history.history['accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.title('model loss')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","82f8dcd5":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten","d599a192":"model = Sequential()\nmodel.add(Conv2D(32,(2,2),activation=\"relu\",input_shape=(28,28,1)))\nmodel.add(MaxPooling2D((1,1)))\nmodel.add(Flatten())\nmodel.add(Dense(250,activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10,activation=\"softmax\"))\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])\nhistory = model.fit(X_train,y_train,epochs=10)","bef4a6fe":"model.evaluate(X_test,y_test)","197d533b":"plot_history(history)","800aa24c":"test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntest.head()","868edac5":"def get_results(test):\n    X_test = test.values\n    X_test = X_test\/255\n    X_test = X_test.reshape(-1,28,28,1)\n    sub = pd.DataFrame(columns=[\"ImageId\",\"Label\"])\n    sub[\"ImageId\"] = range(1,28001)\n    sub.index = sub[\"ImageId\"]\n    sub[\"Label\"] = np.argmax(model.predict(X_test), axis=-1)\n    sub.drop([\"ImageId\"],axis=1,inplace=True)\n    sub.to_csv(\"submission.csv\")\n    return sub\nget_results(test)","073bc9fd":"# Model","7f92e461":"# Let's visualise an image"}}