{"cell_type":{"c4f42c33":"code","9ec013f1":"code","94a52262":"code","7a8be617":"code","5cc16f57":"code","cb87166e":"code","20ce795e":"code","c1fee1ea":"code","0d3c017e":"code","ccaab6a5":"code","3adc9ed8":"code","5dbbbca0":"code","b72ccc01":"code","8919d73d":"code","5d18cd56":"code","6bdb01e4":"code","b09a6075":"code","388bfe97":"code","e336c057":"code","3c6bcf0f":"code","65e00a3c":"code","0a31eced":"code","f6becf5b":"code","fa38ca61":"code","dc4a8a0d":"code","e01d3c3a":"code","e273cf03":"code","39699a6c":"code","6d51bd4f":"code","2cb91cb2":"code","d8f6e3a4":"code","0965ff1d":"code","9527dc2e":"code","717b305e":"code","f5e24ce2":"code","f237727a":"code","8de93e84":"code","b63bf10f":"code","1e0e8b23":"code","4e0c9e32":"code","146dfd7f":"code","9308c8b0":"code","877215b9":"code","f6749afb":"code","f43b8709":"code","e920f505":"code","615a79a4":"code","c69ba171":"code","17451557":"code","17e88db6":"code","a41a9e92":"code","19e9b30a":"code","19f7a867":"markdown","90a8f028":"markdown","b8a8dd08":"markdown","b2b85ef4":"markdown","38a07177":"markdown","66cb8180":"markdown","797c302e":"markdown","b54861ad":"markdown","d9cead74":"markdown","f06f5e04":"markdown","bb558490":"markdown","394bfb4a":"markdown","1007def9":"markdown"},"source":{"c4f42c33":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt","9ec013f1":"import gc\nimport time\nfrom datetime import datetime\nimport warnings\nwarnings.simplefilter(action = 'ignore')","94a52262":"from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.cluster import KMeans","7a8be617":"import lightgbm as lgbm","5cc16f57":"def reduce_mem_usage(df_, max_reduce = True):\n    start_mem = df_.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n    \n    for c in df_.columns[df_.dtypes != 'object']:\n        col_type = df_[c].dtype\n        \n        c_min = df_[c].min()\n        c_max = df_[c].max()\n        if str(col_type)[:3] == 'int':\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                df_[c] = df_[c].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                df_[c] = df_[c].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                df_[c] = df_[c].astype(np.int32)\n            else:\n                df_[c] = df_[c].astype(np.int64)  \n        else:\n            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and max_reduce:\n                df_[c] = df_[c].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                df_[c] = df_[c].astype(np.float32)\n            else:\n                df_[c] = df_[c].astype(np.float64)\n\n    end_mem = df_.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df_","cb87166e":"def new_features_from_version(df_, feature_, target_):\n    \n    def version_to_int(value):\n        format_mask = ['{:0>2}', '{:0>3}', '{:0>5}', '{:0>5}']\n        new_value = [f.format(v) for f, v in zip(format_mask, str(value).split('.'))]\n        return int(new_value[0] + new_value[1] + new_value[2] + new_value[3])\n    \n    def split_column(df_, feature_, sep):\n        res = pd.DataFrame.from_records(list(df_[feature_].astype('object').apply(lambda x: tuple(str(x).split(sep))).values),\n                                        index = df_.index)\n        res.columns = [feature_ + '_' + str(c) for c in res.columns]\n        return res\n\n    new_features = pd.DataFrame(index = df_.index)\n    \n    # convert version into numerical representation\n    new_features[feature_ + '_int'] = df_[feature_].apply(version_to_int).astype(int)\n    new_features[feature_ + '_int'] -= new_features[feature_ + '_int'].min() # for reduce memory\n    \n    # frequencies encoding    \n    new_features[feature_ + '_frq'] = df_[feature_].map(df_[feature_].value_counts())\n    \n    # split version\n    split = split_column(df_, feature_, '.')\n    split['target'] = target_\n    \n    # features from major, minor, build, revision if it has sense\n    nun = split.nunique(dropna = False)\n    if sum(nun < 2) > 0:\n        to_drop = list(split.columns[nun < 2])\n        split.drop(to_drop, axis = 1, inplace = True)\n\n    # target encoding for risk zones\n    to_drop = []\n    for c in split.columns.drop('target'):\n        if split[c].value_counts(dropna = False).iloc[1] < 100000:\n            to_drop.append(c)\n        else:\n            new_features[c] = split[c].astype(int)\n    split.drop(to_drop, axis = 1, inplace = True)\n    \n    for c in split.columns.drop('target'):\n        te = split.groupby([c])['target'].transform(np.mean)\n        split[c + '_good'] = -(te <= .4).astype(int)\n        split[c + '_bad'] =  (te >= .6).astype(int)\n        \n    # risk zones feature \n    new_features[feature_ + '_risk'] = split.drop('target', axis = 1).sum(axis = 1)\n    \n    new_features = reduce_mem_usage(new_features)\n    \n    return new_features","20ce795e":"def lgbm_fast_check(df_, target_, params_):\n    \n    params = params_.copy()\n    params['metric'] = 'auc'\n\n    train_data = lgbm.Dataset(data = df_, label = target_)\n    clf = lgbm.train(params, \n                     train_set = train_data, valid_sets = [train_data], \n                     num_boost_round = 100, verbose_eval = 10, keep_training_booster = True)\n        \n    pred = clf.predict(df_)\n    \n    importances = pd.DataFrame(index = df_.columns)\n    importances['cnt'] = pd.Series(clf.feature_importance(), index = df_.columns)\n    importances['gain'] = pd.Series(clf.feature_importance(importance_type = 'gain'), index = df_.columns)\n    importances.fillna(0, inplace = True)\n    \n    return importances, \\\n           [roc_auc_score(target_, pred), log_loss(target_, pred), accuracy_score(target_, (pred >= .5) * 1)]","c1fee1ea":"def lgbm_cross_validation(df_, target_, params_,\n                          num_boost_round = 20000, early_stopping_rounds = 200,\n                          model_prefix = '',\n                          num_folds = 3, rs = 0, verbose = 100):\n    \n    print(params_)\n    \n    clfs = []\n    importances_cnt = pd.DataFrame(index = df_.columns)\n    importances_gain = pd.DataFrame(index = df_.columns)\n    folds = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = rs)\n    \n    valid_pred = np.zeros(df_.shape[0])\n    \n    # Cross-validation cycle\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(target_, target_)):\n        print('--- Fold {} started at {}'.format(n_fold, time.ctime()))\n        \n        train_x, train_y = df_.iloc[train_idx], target_.iloc[train_idx]\n        valid_x, valid_y = df_.iloc[valid_idx], target_.iloc[valid_idx]\n    \n        train_data = lgbm.Dataset(data = train_x, label = train_y)\n        valid_data = lgbm.Dataset(data = valid_x, label = valid_y, reference = train_data)\n        \n        clf = lgbm.train(params_, \n                         train_set = train_data, valid_sets = [train_data, valid_data], \n                         num_boost_round = num_boost_round, early_stopping_rounds = early_stopping_rounds, \n                         verbose_eval = verbose, keep_training_booster = True)\n        \n        clfs.append(clf)\n        if len(model_prefix) > 0:\n            clf.save_model(model_prefix + str(n_fold) + '.txt')\n\n        valid_pred[valid_idx] = clf.predict(valid_x)\n    \n        valid_score = valid_pred[valid_idx]\n        tn, fp, fn, tp = confusion_matrix(valid_y, (valid_score >= .5) * 1).ravel()\n        ras = roc_auc_score(valid_y, valid_score)\n        acc = accuracy_score(valid_y, (valid_score >= .5) * 1)\n        loss = log_loss(valid_y, valid_score)\n        print('--- Final valid score for this fold ---')\n        print('TN =', tn, 'FN =', fn, 'FP =', fp, 'TP =', tp)\n        print('AUC = ', ras, 'Loss =', loss, 'Acc =', acc)\n        print('-'*40)\n\n        importances_cnt[n_fold] = pd.Series(clf.feature_importance(), index = df_.columns)\n        importances_gain[n_fold] = pd.Series(clf.feature_importance(importance_type = 'gain'), \n                                             index = df_.columns)\n        \n        del train_x, train_y, valid_x, valid_y, train_data, valid_data, valid_score\n        gc.collect()\n\n    importances = pd.DataFrame(index = df_.columns)\n    importances['cnt'] = importances_cnt.mean(axis = 1)\n    importances['gain'] = importances_gain.mean(axis = 1)\n    importances.fillna(0, inplace = True)\n    \n    return clfs, valid_pred, importances","0d3c017e":"parameters = {}\nparameters['device'] = 'cpu'\nparameters['objective'] = 'binary'\nparameters['n_jobs'] = -1\nparameters['boosting'] = 'gbdt'\nparameters['two_round'] = True\nparameters['learning_rate'] = .05           # default = 0.1\nparameters['feature_fraction'] = .8         # default = 1.\nparameters['bagging_freq'] = 1              # default = 0\nparameters['bagging_fraction'] = .3         # default = 1.\nparameters['max_depth'] = -1                # default = -1 \nparameters['num_leaves'] = 20               # default = 31 \nparameters['max_bin'] = 1024                # default = 255, bigger is only for CPU!\nparameters['min_data_in_leaf'] = 100        # default = 20\nparameters['lambda_l1'] = 100.              # default = 0\nparameters['lambda_l2'] = 100.              # default = 0\nparameters['random_seed'] = 0","ccaab6a5":"protection_features = [\n    'HasTpm',\n    'ProductName',\n    'AVProductStatesIdentifier', \n    'AVProductsInstalled', \n    'AVProductsEnabled',\n    'IsProtected', \n    'SMode', \n    'SmartScreen', \n    'Firewall',\n    'UacLuaenable',\n    'Census_IsSecureBootEnabled'\n]","3adc9ed8":"version_num_features = ['AvSigVersion'] # only one for Kaggle kernel","5dbbbca0":"train = pd.read_csv('..\/input\/train.csv', \n                    usecols = ['MachineIdentifier', 'HasDetections'] + protection_features + version_num_features)\ntrain.drop(5244810, inplace = True) #bad AvSigVersion value\ntrain.set_index('MachineIdentifier', inplace = True)\ntrain.head()","b72ccc01":"target_train = train['HasDetections']\ntrain.drop('HasDetections', axis = 1, inplace = True)\ntarget_train.value_counts()","8919d73d":"test = pd.read_csv('..\/input\/test.csv', \n                   usecols = ['MachineIdentifier'] + protection_features + version_num_features)\ntest.set_index('MachineIdentifier', inplace = True)\ntest.head()","5d18cd56":"index_train = list(train.index)\nindex_test = list(test.index)\nprint(len(index_train), len(index_test))","6bdb01e4":"df_full = pd.concat([train, test], axis = 0)\ndf_full = reduce_mem_usage(df_full)","b09a6075":"del train, test\ngc.collect()","388bfe97":"for c in df_full.columns:\n    if df_full[c].dtypes == 'object':\n        df_full[c] = df_full[c].astype('category')\n        \ndf_full.info(null_counts = True)","e336c057":"scores = pd.DataFrame(index = ['AUC', 'LogLoss', 'Accuracy'])","3c6bcf0f":"imp, sc = lgbm_fast_check(df_full.loc[index_train], target_train, parameters)","65e00a3c":"scores['Source'] = sc\nscores","0a31eced":"imp.sort_values('gain', ascending = False)","f6becf5b":"gc.collect()","fa38ca61":"df_new = pd.DataFrame(index = df_full.index)","dc4a8a0d":"for c in version_num_features:\n    print(c, time.ctime())\n    df_new = pd.concat([df_new, new_features_from_version(df_full, c, target_train)], axis = 1)","e01d3c3a":"df_new.head()","e273cf03":"df_full = pd.concat([df_full, df_new], axis = 1)","39699a6c":"gc.collect()","6d51bd4f":"df_new.fillna(df_new.mean(axis = 0), inplace = True)","2cb91cb2":"clusters = KMeans(n_clusters = 3, random_state = 0, n_jobs = -1)\nclusters.fit(df_new.loc[index_train])\ncenters = clusters.cluster_centers_","d8f6e3a4":"gc.collect()","0965ff1d":"columns = df_new.columns\nclust_features = pd.DataFrame(index = df_new.index)\nfor i in range(len(centers)):\n    print(i, time.ctime())\n    # distance as manhattan metric\n    clust_features['clust_dist_' + str(i)] = (df_new[columns] - centers[i]).applymap(abs).apply(sum, axis = 1)\n    \nclust_features.head()","9527dc2e":"del df_new\ngc.collect()","717b305e":"clust_features = reduce_mem_usage(clust_features)","f5e24ce2":"df_full = pd.concat([df_full, clust_features], axis = 1)\ndf_full.head()","f237727a":"del clust_features\ngc.collect()","8de93e84":"df_full.info(null_counts = True)","b63bf10f":"imp, sc = lgbm_fast_check(df_full.loc[index_train], target_train, parameters)","1e0e8b23":"scores['New'] = sc\nscores","4e0c9e32":"imp.sort_values('gain', ascending = False)","146dfd7f":"gc.collect()","9308c8b0":"scores = pd.DataFrame(index = ['auc', 'acc', 'loss', 'tn', 'fn', 'fp', 'tp'])","877215b9":"clfs, valid_pred, importances = lgbm_cross_validation(df_full.loc[index_train], target_train, parameters)","f6749afb":"importances.sort_values('gain', ascending = False)","f43b8709":"train_pred = pd.DataFrame(index = index_train)\ntrain_pred['valid'] = valid_pred\ntrain_pred['target'] = target_train\n\ntrain_pred.head()","e920f505":"tn, fp, fn, tp = confusion_matrix(target_train, (train_pred['valid'] >= .5) * 1).ravel()\nscores['train valid'] = [roc_auc_score(target_train, train_pred['valid']), \n                         accuracy_score(target_train, (train_pred['valid'] >= .5) * 1), \n                         log_loss(target_train, train_pred['valid']),\n                         tn, fn, fp, tp]\n    \nscores = scores.T\nscores","615a79a4":"score_auc = scores.loc['train valid', 'auc']\nscore_acc = scores.loc['train valid', 'acc']\nprint(score_auc, score_acc)","c69ba171":"df_full = df_full.loc[index_test]\ndel train_pred, target_train\ngc.collect()","17451557":"test_pred = pd.DataFrame(index = index_test)\n\nfor i, clf in enumerate(clfs):\n    print(i, time.ctime())\n    test_pred[i] = clf.predict(df_full)\n    \ntest_pred['mean'] = test_pred.mean(axis = 1)\ntest_pred.head()","17e88db6":"col = 'mean'\nsubmit = test_pred[col].reset_index()\nsubmit.columns = ['MachineIdentifier', 'HasDetections']\nsubmit.head()","a41a9e92":"filename = 'subm_lgbm_{:.4f}_{:.4f}_{}fold_{}.csv'.format(score_auc, score_acc, len(clfs), \n                                                          datetime.now().strftime('%Y-%m-%d'))\nprint(filename)","19e9b30a":"submit.to_csv(filename, index = False)","19f7a867":"### Fast check for comparing scores","90a8f028":"### for memory saving","b8a8dd08":"## Load source train & test sets","b2b85ef4":"# Microsoft Malware Prediction - pipeline with some new features","38a07177":"### Fast check for comparing scores","66cb8180":"### cluster features","797c302e":"### for fast checking & cross-validation","b54861ad":"## New features ","d9cead74":"### for data converting","f06f5e04":"### from versions","bb558490":"## Test prediction & submit","394bfb4a":"## Functions","1007def9":"## Cross-validation"}}