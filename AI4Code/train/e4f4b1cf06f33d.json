{"cell_type":{"82e3abe3":"code","a8be54f4":"code","e33d3482":"code","e7c02b14":"code","72ec93e7":"code","b20951d2":"code","b81c7834":"code","2cee3c7b":"code","b30cc847":"code","820a2438":"code","9222e4c1":"code","10d1b3c9":"code","1ade41ee":"code","f566ec49":"code","af53dfd3":"code","947ff7f0":"code","fe0e7de1":"code","721fbb0e":"code","9a17be73":"code","ebc9bbbc":"code","d0250270":"code","7f20659c":"code","adadcf81":"code","d00574cc":"code","15e3433c":"code","50133656":"code","2fca1cc1":"code","05526107":"code","96d0f295":"code","d18af248":"code","9ff1e724":"code","28aa9008":"code","5880cc5c":"code","a90c2bd5":"code","dc926acc":"code","9b01c010":"code","37024f25":"code","4937a086":"code","e8a2ce8f":"markdown","fce7f95b":"markdown","b0189ed1":"markdown","1b95ca93":"markdown","8bd5c202":"markdown","e3874767":"markdown","afc8898b":"markdown","6ca9fcc1":"markdown","e7100cef":"markdown","0492d9c8":"markdown","198207f1":"markdown","2166d8ac":"markdown","2216d6f4":"markdown","65df69f6":"markdown","5355fe77":"markdown","7cf07465":"markdown","7ddf12d7":"markdown","74f76ad9":"markdown","5812847d":"markdown","c84283ab":"markdown","ca97cd44":"markdown","3600d264":"markdown","1f7a4a1b":"markdown","56c20fdd":"markdown","fdf87066":"markdown","e876434b":"markdown","54e6f99c":"markdown","9339de78":"markdown","232186cb":"markdown","40553939":"markdown","5c44ca78":"markdown","32139954":"markdown","0a538c89":"markdown","52c3b21a":"markdown"},"source":{"82e3abe3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport warnings\nfrom itertools import product\nwarnings.filterwarnings('ignore')\nimport os\nimport seaborn as sns\nimport lightgbm \nfrom xgboost import XGBRegressor, plot_importance\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.model_selection import KFold, cross_val_predict\nfrom sklearn.externals import joblib\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Any results you write to the current directory are saved as output.","a8be54f4":"# Reading the files\nroot_path = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\ndf_test = pd.read_csv(root_path + 'test.csv')\ndf_item_categories = pd.read_csv(root_path + 'item_categories.csv')\ndf_sales_train = pd.read_csv(root_path + 'sales_train.csv')\ndf_sample_submission = pd.read_csv(root_path + 'sample_submission.csv')\ndf_items = pd.read_csv(root_path + 'items.csv')\ndf_shops = pd.read_csv(root_path + 'shops.csv')\ndf_categories_translated = pd.read_csv('\/kaggle\/input\/translated\/item_categories-translated.csv')\ndf_shops_translated = pd.read_csv('\/kaggle\/input\/translated\/shops-translated.csv')\nprint('The files have been loaded!')\n\nprint('df_test')\nprint(df_test.shape)\ndisplay(df_test.head(3))\n\nprint('df_item_categories')\nprint(df_item_categories.shape)\ndisplay(df_item_categories.head(3))\n\nprint('df_sales_train')\nprint(df_sales_train.shape)\ndisplay(df_sales_train.head(3))\n\nprint('df_sample_submission')\nprint(df_sample_submission.shape)\ndisplay(df_sample_submission.head(3))\n\nprint('df_items')\nprint(df_items.shape)\ndisplay(df_items.head(3))\n\nprint('df_shops')\nprint(df_shops.shape)\ndisplay(df_shops.head(3))","e33d3482":"df_sales_timeseries = df_sales_train.groupby(['shop_id','item_id','date_block_num']).agg({'item_cnt_day':sum}).unstack(level=2)\ndf_sales_timeseries.columns = df_sales_train['date_block_num'].unique()\ndf_sales_timeseries = df_sales_timeseries.fillna(value=0)\nplt.plot(df_sales_timeseries.sum(axis=0))","e7c02b14":"plt.plot(df_sales_timeseries.sum(axis=0).values[:12])\nplt.plot(df_sales_timeseries.sum(axis=0).values[12:24])\nplt.plot(df_sales_timeseries.sum(axis=0).values[24:])\nplt.xticks(np.arange(12),np.arange(1,13))","72ec93e7":"print('The number of unique shop ids is', df_test['shop_id'].nunique())\nprint('The number of unique item ids is', df_test['item_id'].nunique())\nprint('The average number of item ids per shop is', df_test.groupby('shop_id').agg(count=('item_id', 'count')).mean()[0])","b20951d2":"train_combos = df_sales_train.drop_duplicates(['shop_id','item_id']).shape[0]\ntest_combos = df_test.drop_duplicates(['shop_id','item_id']).shape[0]\nseen_combos = pd.merge(df_sales_train.drop_duplicates(['shop_id','item_id']), df_test, on=['shop_id','item_id'], how='inner').shape[0]\nprint(\"The number of train combos is {} and the number of test combos is {}\".format(train_combos, test_combos))\nprint(\"The number of combos in test that are present in train is {}\".format(seen_combos))","b81c7834":"df_test = pd.merge(df_test, df_items[['item_id','item_category_id']], on=['item_id'], how='left')\ndf_sales_train = pd.merge(df_sales_train, df_items[['item_id','item_category_id']], on=['item_id'], how='left')\nunseen_item_categories = set(df_test['item_category_id']) - set(df_sales_train['item_category_id'])\nprint('The number of item categories that have never been sold before is', len(unseen_item_categories))","2cee3c7b":"unseen_items = set(df_test['item_id']) - set(df_sales_train['item_id'])\nprint('The number of items that have never been sold before is', len(unseen_items))","b30cc847":"df_item_price = df_sales_train.groupby('item_id').agg({'item_price':[np.mean, np.std,'count']})\n# Since the NaN values are for those items that have a count of 1\ndf_item_price.fillna(0)\ndf_item_price.columns = ['mean','std','count']\nprint(df_item_price['std'].describe(percentiles=np.linspace(0,1,11)[1:10]))\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.boxplot(df_item_price['std'])\n\nplt.subplot(122)\nsns.boxplot(df_item_price[df_item_price['std']>500]['mean'])","820a2438":"# Now if we were to look at variance of prices of the item category\ndf_item_category_price = df_sales_train.groupby('item_category_id').agg({'item_price':[np.mean, np.std]})\n# Since the NaN values are for those items that have a count of 1\ndf_item_category_price.fillna(0)\ndf_item_category_price.columns = ['mean','std']\nprint(df_item_category_price['std'].describe(percentiles=np.linspace(0,1,11)[1:10]))\nsns.boxplot(df_item_category_price['std'])","9222e4c1":"# To free up RAM\ndel df_sales_timeseries, unseen_items, unseen_item_categories, df_item_price, df_item_category_price\ngc.collect()","10d1b3c9":"# Deduplication. There are a few shop_ids that have the same names and similar shop names are mapped to the ids contained in the test set\ndf_sales_train.loc[df_sales_train.shop_id == 0, 'shop_id'] = 57\ndf_test.loc[df_test.shop_id == 0, 'shop_id'] = 57\ndf_sales_train.loc[df_sales_train.shop_id == 1, 'shop_id'] = 58\ndf_test.loc[df_test.shop_id == 1, 'shop_id'] = 58\ndf_sales_train.loc[df_sales_train.shop_id == 11, 'shop_id'] = 10\ndf_test.loc[df_test.shop_id == 11, 'shop_id'] = 10\ndf_sales_train.loc[df_sales_train.shop_id == 40, 'shop_id'] = 39\ndf_test.loc[df_test.shop_id == 40, 'shop_id'] = 39\n\n# Extracting city code\ndf_shops_translated['shop_name'] = df_shops_translated['shop_name_translated'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\ndf_shops_translated['shop_city'] = df_shops_translated['shop_name'].str.partition(' ')[0]\ndisplay(df_shops_translated[df_shops_translated['shop_id'].isin([0,57,1,58,11,10,40,39])])\ndf_shops_translated['city_code'] = LabelEncoder().fit_transform(df_shops_translated['shop_city'])\ndf_shops_translated = df_shops_translated[['shop_id','city_code']]\n\n# Extracting type and subtype of categories\ndf_categories_translated['split'] = df_categories_translated['item_category_name_translated'].str.lower().str.split('-')\ndf_categories_translated['type'] = df_categories_translated['split'].map(lambda x: x[0].strip())\ndf_categories_translated['subtype'] = df_categories_translated['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ndisplay(df_categories_translated.head())\ndf_categories_translated['type_code'] = LabelEncoder().fit_transform(df_categories_translated['type'])\ndf_categories_translated['subtype_code'] = LabelEncoder().fit_transform(df_categories_translated['subtype'])\ndf_categories_translated = df_categories_translated[['item_category_id','type_code','subtype_code']]","1ade41ee":"df_sales_grouped = df_sales_train.groupby(['shop_id','item_id','date_block_num']).agg(item_cnt_month=('item_cnt_day',sum)).reset_index()\ndf_prices_grouped = df_sales_train.groupby(['date_block_num','item_id']).agg(item_price=('item_price','mean')).reset_index() # aggregating price for each month using mean\ndf_prices_grouped['item_price_bin'], bins =  pd.qcut(df_prices_grouped['item_price'], 7, labels=np.arange(1,8), retbins=True) # binning prices\ndf_test['date_block_num'] = 34\ndf_test['item_cnt_month'] = np.nan\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \ngrid_cols = ['shop_id', 'item_id', 'date_block_num']\nfor date_block_num in df_sales_train['date_block_num'].unique():\n    cur_shops = df_sales_train.loc[df_sales_train['date_block_num'] == date_block_num, 'shop_id'].unique()\n    cur_items = df_sales_train.loc[df_sales_train['date_block_num'] == date_block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [date_block_num]])),dtype='int32'))\ndf_grid = pd.DataFrame(np.vstack(grid), columns = grid_cols, dtype=np.int32)\ndel grid\ndf_grid = pd.merge(df_grid, df_sales_grouped, on=['shop_id','item_id','date_block_num'], how='left') # to get other attributes from the sales data\ndf_grid['item_cnt_month'] = df_grid['item_cnt_month'].fillna(0).clip(0,20) # clipping values so that the train data is representative of the test data\ndf_grid = pd.concat([df_grid, df_test.drop(['item_category_id','ID'], axis=1)], axis=0, ignore_index=True) # concatenating to preprocess together\ndf_grid = pd.merge(df_grid, df_items[['item_id','item_category_id']], on='item_id', how='left') # to get item_category_id\ndf_grid = pd.merge(df_grid, df_shops_translated, on='shop_id', how='left') # to get city_code\ndf_grid = pd.merge(df_grid, df_categories_translated, on='item_category_id', how='left') # to get type_code and subtype_code\ndf_grid = pd.merge(df_grid, df_prices_grouped, on=['date_block_num','item_id'], how='left') # to get prices\nprint('The grid has been created!')\ndel df_sales_grouped, df_prices_grouped\ngc.collect()","f566ec49":"# Imputing prices for the test set (ugly code). The logic is defined in the EDA section\ndf_prices_shop_item = df_sales_train[['shop_id','item_id','item_price']].drop_duplicates(subset=['shop_id','item_id'], keep='last') \ndf_prices_item = df_sales_train[['item_id','item_price']].drop_duplicates(subset=['item_id'], keep='last')\ndf_prices_category = df_sales_train.groupby('item_category_id').agg(item_price=('item_price','mean')).reset_index()\n\ndf_test = df_grid[df_grid['date_block_num']==34]\ndf_test = pd.merge(df_test.drop('item_price',axis=1), df_prices_shop_item, on=['shop_id','item_id'], how='left')\nitem_price = pd.merge(df_test[pd.isnull(df_test['item_price'])], df_prices_item, on='item_id', \n                      how='left').set_index(df_test[pd.isnull(df_test['item_price'])].index)['item_price_y']\ndf_test['item_price'] = df_test['item_price'].fillna(item_price)\ncategory_price = pd.merge(df_test[pd.isnull(df_test['item_price'])], df_prices_category, on='item_category_id', \n                          how='left').set_index(df_test[pd.isnull(df_test['item_price'])].index)['item_price_y']\ndf_test['item_price'] = df_test['item_price'].fillna(category_price)\ndf_test['item_price_bin'] = pd.cut(df_test['item_price'], bins=bins, labels=np.arange(1,8), include_lowest=True)\n\n# Concatenating\ndf_grid = pd.concat([df_grid[df_grid['date_block_num']<34], df_test[df_grid.columns]], axis=0)\ndf_grid['item_price_bin'] = df_grid['item_price_bin'].astype(int)\ndel df_prices_shop_item, df_prices_item, df_prices_category, df_test, df_sales_train\ngc.collect()","af53dfd3":"# Feature engineering\n# The first few variables are group means of date_block_num + additional categorical variables\ndf_grid['date_item_mean'] = df_grid.groupby(['date_block_num','item_id'])['item_cnt_month'].transform('mean') \ndf_grid['date_category_mean'] = df_grid.groupby(['date_block_num','item_category_id'])['item_cnt_month'].transform('mean')\ndf_grid['date_item_shop_mean'] = df_grid.groupby(['date_block_num','item_id','shop_id'])['item_cnt_month'].transform('mean')\ndf_grid['date_item_category_mean'] = df_grid.groupby(['date_block_num','item_id','item_category_id'])['item_cnt_month'].transform('mean')\ndf_grid['date_shop_category_mean'] = df_grid.groupby(['date_block_num','shop_id','item_category_id'])['item_cnt_month'].transform('mean')\ndf_grid['month'] = df_grid['date_block_num'] % 12\nnumber_of_days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\ndf_grid['days'] = df_grid['month'].map(number_of_days)\n# Time dependent information\ndf_grid['months_since_item_sale'] = df_grid['date_block_num'] - df_grid.groupby('item_id')['date_block_num'].transform('min')\ndf_grid['months_since_item_shop_sale'] = df_grid['date_block_num'] - df_grid.groupby(['item_id','shop_id'])['date_block_num'].transform('min')","947ff7f0":"# Shop revenue\ndf_grid['shop_item_revenue'] = df_grid['item_cnt_month']*df_grid['item_price']\ndf_revenue = df_grid.groupby(['shop_id','date_block_num']).agg(shop_revenue=('shop_item_revenue',sum)).reset_index()\ndf_grid = pd.merge(df_grid, df_revenue, on=['shop_id','date_block_num'], how='left')\ndf_grid.drop('shop_item_revenue', axis=1, inplace=True) \ndel df_revenue\ngc.collect()","fe0e7de1":"# Downcasting data types to save memory\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int16)    \n    return df\nshop_revenue_cols = [col for col in df_grid.columns if 'shop_revenue' in col]\ncols_to_downcast = list(set(df_grid.columns) - set(shop_revenue_cols))\ndf_grid[shop_revenue_cols] = df_grid[shop_revenue_cols].astype(np.float32) \ndf_grid[cols_to_downcast] = downcast_dtypes(df_grid[cols_to_downcast])","721fbb0e":"# Lagging \ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    df.drop(col, axis=1, inplace=True)\n    del shifted\n    return(df)\n\nmean_encoded = [col for col in df_grid.columns if 'mean' in col]\ncols_to_lag = ['shop_revenue','item_price'] + mean_encoded\nfor col in cols_to_lag:\n    df_grid = lag_feature(df_grid, [1,2,3,6,12], col)\n    print('Lagged values for {} have been calculated'.format(col))\ndf_grid = df_grid[df_grid['date_block_num']>=12] # Since we are using a lag of 12, the feature set for the first 12 month is not useful\nlagged_cols = [col for col in df_grid.columns if 'lag' in col]\ndf_grid[lagged_cols] = df_grid[lagged_cols].fillna(0)\ndf_grid.reset_index(drop=True, inplace=True)\ngc.collect()","9a17be73":"# And this is how it looks \ndf_grid.head()","ebc9bbbc":"# Run this cell if the kernel gets killed in between. It contains the feature set for the models built below\ndf_grid = pd.read_feather('\/kaggle\/input\/data-prep\/df_sales.feather')\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float16)\n    df[int_cols]   = df[int_cols].astype(np.int16)    \n    return df\nshop_revenue_cols = [col for col in df_grid.columns if 'shop_revenue' in col]\ncols_to_downcast = list(set(df_grid.columns) - set(shop_revenue_cols))\ndf_grid[shop_revenue_cols] = df_grid[shop_revenue_cols].astype(np.float32) # To prevent numerical overflow\ndf_grid[cols_to_downcast] = downcast_dtypes(df_grid[cols_to_downcast])","d0250270":"# Dividing into train, validation and test. We use months upto 33 for training, the 33rd month for validation and the last (34th month) for testing\ntrain = df_grid[df_grid['date_block_num']<33]\nvalid = df_grid[df_grid['date_block_num']==33]\nX_test = df_grid[df_grid['date_block_num']==34].drop('item_cnt_month', axis=1)\n\n# Dividing into feature and label for regressor\nX_train = train[train['item_cnt_month']>0].drop('item_cnt_month', axis=1)\ny_train = train[train['item_cnt_month']>0]['item_cnt_month']\nX_valid = valid[valid['item_cnt_month']>0].drop('item_cnt_month', axis=1)\ny_valid = valid[valid['item_cnt_month']>0]['item_cnt_month']\n\n# For storing OOF predictions\ndf_classreg_valid  = pd.DataFrame()\ndf_classreg_test = pd.DataFrame()\ndf_reg_valid  = pd.DataFrame()\ndf_reg_test = pd.DataFrame()\n\ndel df_grid\ngc.collect()","7f20659c":"# LightGBM regressor\nregressor_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread': 1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 80,\n               'bagging_freq':1,\n               'verbose':0 \n              }\nlightgbm_train = lightgbm.Dataset(X_train, label=y_train)\nlightgbm_valid = lightgbm.Dataset(X_valid, label=y_valid)\nregressor = lightgbm.train(regressor_params, lightgbm_train, valid_sets=[lightgbm_train, lightgbm_valid], valid_names=['train','valid'], verbose_eval=10, num_boost_round=500, early_stopping_rounds=10)\nprint('LightGBM model for regression has been built!')\ndf_classreg_valid['lightgbm'] = regressor.predict(valid.drop('item_cnt_month', axis=1))\ndf_classreg_test['lightgbm'] = regressor.predict(X_test)\nlightgbm.plot_importance(regressor, figsize=(10,12))\nregressor.save_model('lightgbm_classreg_regressor.txt', num_iteration=regressor.best_iteration) # Load it back using, model = lightgbm.Booster(model_file='lightgbm_classreg_regressor.txt')\ndel lightgbm_train, lightgbm_valid, regressor\ngc.collect()","adadcf81":"# XGBoost regressor\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nregressor = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=100, \n    colsample_bytree=0.75, \n    subsample=0.75, \n    eta=0.1,    \n    seed=42)\n\nregressor.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_valid, y_valid)], \n    verbose=10, \n    early_stopping_rounds = 10)\nprint('XGBoost model for regression has been built!')\ndf_classreg_valid['xgboost'] = regressor.predict(valid.drop('item_cnt_month', axis=1))\ndf_classreg_test['xgboost'] = regressor.predict(X_test)\nplot_features(regressor, (10,12))\nregressor.save_model('xgboost_classreg_regressor.model') # Load it back using, model = xgboost.Booster(), model.load_model('xgboost_classreg_regressor.model')\ndel regressor\ngc.collect()","d00574cc":"# CatBoost regressor\ncat_features = ['shop_id','item_id','item_category_id','item_price_bin','month','date_block_num','city_code','type_code','subtype_code']\nregressor = CatBoostRegressor(\n    iterations=100,\n    random_seed=0,\n    learning_rate=0.1,\n    max_ctr_complexity=3, # To enable feature interactions\n    has_time=True, # To disable random permutations\n    boosting_type='Ordered', # To reduce overfitting\n    loss_function='RMSE',\n    od_type='Iter', \n    od_wait=10, # Early stopping\n)\nregressor.fit(\n    X_train, y_train,\n    cat_features=cat_features,\n    eval_set=(X_valid, y_valid),\n    verbose=10\n)\nprint('CatBoost model for regression has been built!')\ndf_classreg_valid['catboost'] = regressor.predict(valid.drop(['item_cnt_month'], axis=1))\ndf_classreg_test['catboost'] = regressor.predict(X_test)\nprint(regressor.get_feature_importance(prettified=True))\nregressor.save_model('catboost_classreg_regressor.bin') # Load it back using model = CatBoostRegressor(), model.load_model('catboost_classreg_regressor.bin')\ndel regressor\ngc.collect()","15e3433c":"# LightGBM classifier\nclassifier_params = {\n               'feature_fraction': 0.75,\n               'metric': 'auc', \n               'nthread': 1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.3, \n               'objective': 'binary', \n               'bagging_seed': 2**7, \n               'num_leaves': 80,\n               'bagging_freq':1,\n               'is_unbalance':True,\n               'verbose':0 \n              }\nlightgbm_train = lightgbm.Dataset(train.drop('item_cnt_month', axis=1), label=train['item_cnt_month'].apply(lambda x: 1 if x >= 1 else 0))\nlightgbm_valid = lightgbm.Dataset(valid.drop('item_cnt_month', axis=1), label=valid['item_cnt_month'].apply(lambda x: 1 if x >= 1 else 0))\nclassifier = lightgbm.train(classifier_params, lightgbm_train, valid_sets=[lightgbm_train, lightgbm_valid], valid_names=['train','valid'], verbose_eval=10, num_boost_round=300, early_stopping_rounds=10)\nprint('Model for classification has been built!')\ndf_classreg_valid['prob'] = classifier.predict(valid.drop('item_cnt_month', axis=1))\ndf_classreg_test['prob'] = classifier.predict(X_test)\nclassifier.save_model('lightgbm_classreg_classifier.txt', num_iteration=classifier.best_iteration) # Can load it back using model = lightgbm.Booster(model_file='lightgbm_classreg_classifier.txt')\ndel lightgbm_train, lightgbm_valid, classifier\ngc.collect()","50133656":"df_classreg_valid['average'] = (df_classreg_valid['lightgbm'] + df_classreg_valid['xgboost'] + df_classreg_valid['catboost'])\/3\ndf_classreg_valid['item_cnt_month'] = df_classreg_valid.apply(lambda row: row['average'] if row['prob']>=0.8 else 0, axis=1) \nprint('Validation RMSE is', np.sqrt(mean_squared_error(valid['item_cnt_month'], df_classreg_valid['item_cnt_month'])))\ndf_classreg_valid.to_csv('classreg_valid.csv', index=False)\n\ndf_classreg_test['average'] = (df_classreg_test['lightgbm'] + df_classreg_test['xgboost']+ df_classreg_test['catboost'])\/3\ndf_classreg_test['item_cnt_month'] = df_classreg_test.apply(lambda row: row['average'] if row['prob']>=0.8 else 0, axis=1) \ndf_classreg_test['ID'] = df_sample_submission['ID']\ndf_classreg_test = df_classreg_test[['ID','item_cnt_month']]\ndf_classreg_test.to_csv('classreg_test.csv', index=False) # Final submission file","2fca1cc1":"# Part of this code has been borrowed from this fantastic kernel https:\/\/www.kaggle.com\/mbrown89\/boost-your-score-guaranteed-leaderboard-probing\npath = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv'\ntrain_pairs = pd.read_csv(path)\ntrain_pairs = train_pairs[train_pairs['date_block_num']<33][['shop_id','item_id']].drop_duplicates(['shop_id','item_id'])\npairs={(a, b) for a, b in zip(train_pairs.shop_id, train_pairs.item_id)}\nitems={a for a in train_pairs.item_id}\ndf_reg_valid['group'] = [2 if (a,b) in pairs else (1 if b in items else 0) for a,b in zip(valid.shop_id, valid.item_id)]\ndf_reg_test['group'] = [2 if (a,b) in pairs else (1 if b in items else 0) for a,b in zip(X_test.shop_id, X_test.item_id)]\ndel train_pairs, pairs, items\ngc.collect()","05526107":"# LightGBM model fitting\nlightgbm_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 80,\n               'bagging_freq':1,\n               'verbose':0 \n              }\nlightgbm_train = lightgbm.Dataset(train.drop('item_cnt_month', axis=1), label=train['item_cnt_month'])\nlightgbm_valid = lightgbm.Dataset(valid.drop('item_cnt_month', axis=1), label=valid['item_cnt_month'])\nmodel = lightgbm.train(lightgbm_params, lightgbm_train, valid_sets=[lightgbm_train, lightgbm_valid], valid_names=['train','valid'], verbose_eval=10, num_boost_round=300, \n                       early_stopping_rounds=10)\ndf_reg_valid['lightgbm'] = model.predict(valid.drop('item_cnt_month', axis=1))\ndf_reg_test['lightgbm'] = model.predict(X_test)\nmodel.save_model('lightgbm_reg.txt', num_iteration=model.best_iteration) # Load it back using, model = lightgbm.Booster(model_file='lightgbm_reg.txt')\ndel lightgbm_train, lightgbm_valid, model\ngc.collect()","96d0f295":"# Dividing into train, validation and test for ridge regression\nX_train_iloc = train.index[-1] + 1\nX_valid_iloc = valid.index[-1] + 1\ny_train =  train['item_cnt_month']\ny_valid = valid['item_cnt_month']\ndel train, valid\ngc.collect()","d18af248":"# Scaling data\ndf_scaled = pd.read_feather('\/kaggle\/input\/scaling\/df_scaled.feather') # Kernel runs out of memory, hence the data has been preprocessed elsewhere\ndf_scaled = downcast_dtypes(df_scaled)\nX_train = df_scaled.iloc[:X_train_iloc]\nX_valid = df_scaled.iloc[X_train_iloc:X_valid_iloc]\nX_test = df_scaled.iloc[X_valid_iloc:]\ndel df_scaled\ngc.collect()\nprint('The scaled variables are', X_train.columns)","9ff1e724":"def plot_coef(coef, predictors):\n    plt.figure(figsize=(10,8))\n    coef = pd.Series(coef, predictors).sort_values()\n    coef.plot(kind='bar', title='Model Coefficients')\nridge = Ridge(alpha=10)\nridge.fit(X_train.values, y_train.values)\njoblib.dump(ridge, 'ridge_reg.pkl') # Load it back using, model = joblib.load('ridge_reg.pkl') \nprint('The RMSE for ridge regression is', np.sqrt(mean_squared_error(y_valid, ridge.predict(X_valid))))\nplot_coef(ridge.coef_, X_train.columns)\ndf_reg_valid['ridge'] = ridge.predict(X_valid)\ndf_reg_test['ridge'] = ridge.predict(X_test)","28aa9008":"lasso = Lasso(alpha=0.05)\nlasso.fit(X_train.values, y_train.values)\nprint('The RMSE for lasso regression is', np.sqrt(mean_squared_error(y_valid, lasso.predict(X_valid))))\nplot_coef(lasso.coef_, X_train.columns)","5880cc5c":"#Shallow depth\nensembler = XGBRegressor(\n    max_depth=2,\n    n_estimators=150,\n    min_child_weight=100, \n    colsample_bytree=0.75, \n    subsample=0.75, \n    eta=0.1,    \n    seed=42)\nkfold = KFold(n_splits=5, random_state=42)\ny_pred = cross_val_predict(ensembler, df_reg_valid, y_valid, cv=kfold)\nprint('The cross validation RMSE is', np.sqrt(mean_squared_error(y_valid, y_pred)))","a90c2bd5":"# To inspect predicted means and target means for the individual models and the ensemble\ndf_mean_analysis = pd.DataFrame()\ndf_mean_analysis['group'] = df_reg_valid['group']\ndf_mean_analysis['lightgbm'] = df_reg_valid['lightgbm']\ndf_mean_analysis['ridge'] = df_reg_valid['ridge']\ndf_mean_analysis['ensemble'] = y_pred\ndf_mean_analysis['target'] = y_valid.values\ndf_mean_analysis.groupby('group').agg(np.mean)","dc926acc":"ensembler.fit(df_reg_valid, y_valid)\nensembler.save_model('ensembler.model') # Load it back using, model = xgboost.Booster(), model.load_model('ensembler.model')\nplot_importance(ensembler)","9b01c010":"# Same analysis for the test set\ndf_mean_analysis = pd.DataFrame()\ndf_mean_analysis['group'] = df_reg_test['group']\ndf_mean_analysis['lightgbm'] = df_reg_test['lightgbm']\ndf_mean_analysis['ridge'] = df_reg_test['ridge']\ndf_mean_analysis['pred'] = ensembler.predict(df_reg_test)\ndf_mean_analysis.groupby('group').agg(np.mean)","37024f25":"del df_mean_analysis\ngc.collect()","4937a086":"df_reg_valid.to_csv('reg_valid.csv', index=False)\ndf_reg_test.to_csv('reg_preds_test.csv', index=False)\ndf_reg_test['item_cnt_month'] = ensembler.predict(df_reg_test)\ndf_reg_test['ID'] = df_sample_submission['ID']\ndf_reg_test = df_reg_test[['ID','item_cnt_month']]\ndf_reg_test.to_csv('reg_test.csv', index=False) # Final submission file","e8a2ce8f":"### Validation Scheme","fce7f95b":"#### LightGBM Regression","b0189ed1":"## Modelling","1b95ca93":"There isn't too much deviation in prices but it is there. Values above the 75th percentile could also be for those items that have a higher base price to begin with, which we can see in the second plot","8bd5c202":"The different subsections in this kernel are:\n1. [EDA](#EDA) \n2. [Data Preparation](#Data-Preparation) \n3. [Modelling](#Modelling)\n    1. [Classification + Regression](#Classification-+-Regression)\n    1. [Only Regression](#Only-Regression)","e3874767":"#### XGBoost Regression","afc8898b":"In order to provide additional context to tree based models, we need to lag the encoded categorical variables. Lags are basically values from 'k' months back which means a lag of 1 for date_item_mean for month 14 would correspond to date_item_mean for month 13. Using target information from the same month to predict the target is a bad idea as it leads to overfitting. We lag each of the encoded variables by a lag of 1,2,3,6 and 12 months.","6ca9fcc1":"In order to reduce cardinality of categorical variables, we can encode them with information from the target label (target or mean encoding) or use their cumulative frequency (frequency encoding). Grouping by combining several categorical variables and calculating their means results in powerful features that lead to superior models. Some of the variables and functions have been borrowed from Denis Larionov's [kernel](https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\/) ","e7100cef":"#### Q. How many items exist in the test set that are not present in the train set? ","0492d9c8":"#### Ridge Regression","198207f1":"### Classification + Regression","2166d8ac":"#### Q. How many shops and items exist in the test set? ","2216d6f4":"#### LightGBM Regression","65df69f6":"This part of the notebook will contain a LightGBM regression model and a Ridge regression model (L2 regularization) both trained on the augmented data and stacked using an XGBoost model. We feed the group that the shop_id, item_id combination belongs to as one of the meta features to the meta model. The groups are defined as follows:\n* Group 0 - unseen item_id\n* Group 1 - unseen shop_id, item_id combination\n* Group 2 - seen shop_id, item_id combination","5355fe77":"The idea is to build a classifier on the augmented data to predict whether an item is purchased or not. We build 3 regressors - LightGBM, XGBoost and CatBoost on the un-augmented data and average their predictions if the classifier predicts a purchase. All models built will stop training if the validation metric does not improve in 10 rounds","7cf07465":"A lot of the ideas for data preparation have been borrowed from the final project advice sections of the Coursera course and these two lovely kernels - [1st place solution - Part 1 - \"Hands on Data\"](https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data) and [Feature engineering, xgboost](https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\/). Courtesy - [deargle](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/discussion\/54949#latest-624337), we now have the translated text for shops, items and categories data sets","7ddf12d7":"We calculate shop revenue which is a product of item_cnt_month and item_price","74f76ad9":"#### LightGBM Classification","5812847d":"There is a downward linear trend that has spikes around the 11-12 mark and 23-24 mark. If we were to plot the timeseries for the 3 years seperately","c84283ab":"#### Q. How is sales distributed with time?","ca97cd44":"#### Q. How many item categories exist in the test set that are not present in the train set? ","3600d264":"This is interesting because we can see that there are 42 shops and 5100 items in total and the test set is basically the cartesian product of the two (42x5100 = 214200)","1f7a4a1b":"Since price is an important feature to consider in the model and it isn't available in the test set, there are a number of things one can do:\n\n* use the latest prices for the (shop_id, item_id) combos. This naturally won't cover all since there are unseen combos in the test set\n* use the average item price from the test set. The test set also contains new items that are yet to be sold at certain shops, thus if the variance of the items isn't high, this makes sense. Again, for those items that have never been sold and are a part of the test set, this will produce NaN values\n* use the average item category price for the combos that have NaN values. Since every item category that is present in train is also present in the test set, this will cover the reamaining NaN values","56c20fdd":"#### Q. How many shop_id, item_id combinations exist in the test set that are also present in the train set?","fdf87066":"There is a lot of multicollinearity between the mean encoded variables which is further validated by the coefficients of the lasso regression model (L1 regularization) below","e876434b":"These 363 items account for 15246 observations in the test set (363x42) that need to be predicted using shop_id and item_category_id","54e6f99c":"### Only Regression","9339de78":"## Data Preparation","232186cb":"This model scores 0.93131 on the public LB and 0.93987 on the private LB (Coursera)","40553939":"#### Q. How is price distributed vs item_id and item_category_id?","5c44ca78":"#### CatBoost Regression","32139954":"The sample submission file scores well particularly because the test set contains a lot of shop_id, item_id combinations that have zero sales against them. The idea is to create a training set similar to the test set by creating a grid of shop_id, item_id combinations for every month by taking a cartesian product and setting the sales of these artificially created combinations to zero","0a538c89":"## EDA","52c3b21a":"#### Ensembling"}}