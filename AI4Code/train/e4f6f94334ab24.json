{"cell_type":{"2b0ede22":"code","587092a1":"code","e81cbc4d":"code","67fd5373":"code","b3682597":"code","8610952f":"code","01e4984e":"code","1578881a":"code","feebbcc9":"code","0495b62f":"code","85a3657b":"code","d3b80557":"code","034a7e1d":"code","8dd60bfa":"code","b007a256":"code","ef2e84c8":"code","3e4b6891":"code","9f05ed5b":"code","515f25b9":"code","ddb5cdb8":"code","ce0621d3":"code","0510df51":"code","38ace40f":"code","5cf6cb61":"markdown","93e11e2c":"markdown","45db2e24":"markdown","21a00beb":"markdown","506c2470":"markdown","9d7bc612":"markdown","379e0bc1":"markdown","2dde55b3":"markdown","e8bf2a32":"markdown","3a71934c":"markdown","5e169e41":"markdown","0babd7b1":"markdown","03beb606":"markdown","5bbb76af":"markdown"},"source":{"2b0ede22":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport plotly\nfrom plotly.offline import iplot\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport cufflinks as cf\n\ncf.go_offline()\nplotly.offline.init_notebook_mode()\ncf.set_config_file(world_readable=True, theme='space', offline=True)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier\nfrom sklearn.svm import OneClassSVM\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom xgboost import XGBClassifier\n\nimport optuna","587092a1":"df = pd.read_csv(\"..\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\", index_col='Id')\ndf.shape","e81cbc4d":"feats = df.columns[:-1]\nis_cat = np.array([df[f].dtype == 'object' for f in feats])\ncat_feats, num_feats = feats[is_cat].tolist(), feats[~is_cat].tolist()\nprint(cat_feats, num_feats, sep='\\n')","67fd5373":"def show_hists(df: pd.DataFrame) -> None:\n    fig = plt.figure(figsize=(12, 8))\n    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n    for i, f in enumerate(df.columns):\n        axis = fig.add_subplot(3, 2, i + 1)\n        axis.hist(df[f])\n        if df[f].dtype == 'object' and df[f].nunique() > 3:\n            axis.set(xlabel=None, title=f)\n        else:\n            axis.set(title=f)\n    fig.show()","b3682597":"show_hists(df[num_feats])\nshow_hists(df[cat_feats[:3]])","8610952f":"def feat_likelihoods(\n    df: pd.DataFrame,\n    feat: str,\n    target: str,\n    is_num=False\n) -> pd.Series:\n    lh = (df[[feat, target]].groupby(feat).mean() - df[target].mean())[target].copy()\n    if lh.size > 10 and not is_num:\n        lh.sort_values(ascending=False, inplace=True)\n    return lh","01e4984e":"lh_by_cat_feats = pd.Series({f: feat_likelihoods(df, f, 'Risk_Flag') for f in cat_feats})\nlh_by_num_feats = pd.Series({f: feat_likelihoods(df, f, 'Risk_Flag', is_num=True) for f in num_feats[1:]})\nlikelihoods = pd.concat([lh_by_cat_feats, lh_by_num_feats])\n\nfor f, lh in likelihoods.items():\n    if lh.size > 10:\n        lh.iplot(orientation='h', title=f)\n    else:\n        lh.iplot(kind='bar', orientation='h', title=f)","1578881a":"prof_categories = {\n    'Mechanical_engineer'       : 'Tech',\n    'Software_Developer'        : 'Soft',\n    'Technical_writer'          : 'Tech',\n    'Civil_servant'             : 'Gov',\n    'Economist'                 : 'Math|Sc',\n    'Flight_attendant'          : 'Avia',\n    'Architect'                 : 'Design|Tech|Draw',\n    'Designer'                  : 'Design',\n    'Physician'                 : 'Sc|Tech',\n    'Financial_Analyst'         : 'Fin|Math|Sc',\n    'Air_traffic_controller'    : 'Avia',\n    'Politician'                : 'Gov',\n    'Police_officer'            : 'Law|Force',\n    'Artist'                    : 'Art|Draw',\n    'Surveyor'                  : 'Geo',\n    'Design_Engineer'           : 'Design|Tech',\n    'Chemical_engineer'         : 'Chem|Tech',\n    'Hotel_Manager'             : 'Fin|Manage',\n    'Dentist'                   : 'Med',\n    'Comedian'                  : 'Art',\n    'Biomedical_Engineer'       : 'Bio|Tech',\n    'Graphic_Designer'          : 'Design',\n    'Computer_hardware_engineer': 'Soft|Tech',\n    'Petroleum_Engineer'        : 'Tech|Chem|Geo',\n    'Computer_operator'         : 'Soft|Tech',\n    'Chartered_Accountant'      : 'Fin|Staff|Office',\n    'Microbiologist'            : 'Bio|Sc',\n    'Fashion_Designer'          : 'Design',\n    'Technician'                : 'Tech',\n    'Aviator'                   : 'Avia',\n    'Psychologist'              : 'Med|Sc',\n    'Magistrate'                : 'Law|Gov',\n    'Lawyer'                    : 'Law',\n    'Engineer'                  : 'Tech',\n    'Official'                  : 'Gov',\n    'Analyst'                   : 'Math|Sc|Office',\n    'Geologist'                 : 'Geo|Sc',\n    'Drafter'                   : 'Tech|Soft|Draw',\n    'Statistician'              : 'Math|Sc',\n    'Web_designer'              : 'Design',\n    'Army_officer'              : 'Gov|Force',\n    'Surgeon'                   : 'Med',\n    'Scientist'                 : 'Sc',\n    'Civil_engineer'            : 'Tech',\n    'Industrial_Engineer'       : 'Tech',\n    'Technology_specialist'     : 'Tech',\n    'Firefighter'               : 'Rescue',\n    'Consultant'                : 'Staff',\n    'Chef'                      : 'Food',\n    'Secretary'                 : 'Fin|Staff|Office',\n    'Librarian'                 : 'Staff'\n}\n\ndf['Profession'].replace(prof_categories, inplace=True)\ndf = pd.concat([df, df['Profession'].str.get_dummies('|')], axis=1)\ndf.drop(columns=['Profession'], inplace=True)","feebbcc9":"df['Own_House']  = (df['House_Ownership'] == 'owned')\ndf['Rent_House'] = (df['House_Ownership'] == 'rented')\ndf.drop(columns=['House_Ownership'], inplace=True)","0495b62f":"to_binary = ['Married\/Single', 'Car_Ownership']\nto_target_encoding = ['STATE', 'CITY']\n\ndef encode_categories(data: pd.DataFrame) -> None:\n    for bf in to_binary:\n        encoding = {name: code for code, name in enumerate(data[bf].unique())}\n        data[bf].replace(encoding, inplace=True)\n    for tef in to_target_encoding:\n        data[tef].replace(likelihoods[tef], inplace=True)\n    data.rename(columns={f: f+\"_lh\" for f in to_target_encoding}, inplace=True)\n    return data","85a3657b":"df = encode_categories(df)\ndf.head()","d3b80557":"def scale(train_X: pd.DataFrame, test_X: pd.DataFrame) -> None:\n    scaler = StandardScaler()\n    train_X = scaler.fit_transform(train_X)\n    test_X = scaler.transform(test_X)","034a7e1d":"def fit_predict(model, train_X, train_y, test_X):\n    model.fit(train_X, train_y)\n    return pd.Series(model.predict(test_X))","8dd60bfa":"def show_confusion_matrix(actual, predict):\n    cfm = confusion_matrix(actual, predict)\n    group_names = ['TN', 'FP', 'FN', 'TP']\n    group_percentages = [\n        '{0:.2%}'.format(value)\n        for value in cfm.flatten() \/ np.sum(cfm)\n    ]\n    labels = [\n        f\"{v2}\\n{v3}\"\n        for  v2, v3 in zip(group_names, group_percentages)\n    ]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cfm, annot=labels, fmt='', cmap='inferno')\n    plt.show()\n\ndef try_models(clfs, X_train, y_train, X_test, fit_predict=fit_predict):\n    for clf_name, clf_model in clfs.items():\n        real_y, pred_y = y_test, fit_predict(clf_model, X_train, y_train, X_test)\n        print(f\"{clf_name}:\")\n        print(f\"Accuracy: {round(accuracy_score(real_y, pred_y) * 100, 1)}%\")\n        print(f\"ROC-AUC: {round(roc_auc_score(real_y, pred_y), 3)}\\n\")\n        show_confusion_matrix(real_y, pred_y)","b007a256":"X, y = df.drop(columns=['Risk_Flag']), df['Risk_Flag']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nscale(X_train, X_test)","ef2e84c8":"def anomalies_fp(model, train_X, train_y, test_X):\n    return fit_predict(model, train_X, train_y, test_X).replace({-1: 1, 1: 0})\n\nclfs = {\n    'OneClassSVM': OneClassSVM(),\n    'IsolationForest': IsolationForest(random_state=0)\n}\ntry_models(clfs, X_train, y_train, X_test, anomalies_fp)","3e4b6891":"def smote_oversampling(train_X: pd.DataFrame, train_y: pd.Series) -> None:\n    smote = SMOTE(random_state=0)\n    train_X, train_y = smote.fit_resample(train_X, train_y)\n    \nX, y = df.drop(columns=['Risk_Flag']), df['Risk_Flag']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nsmote_oversampling(X_train, y_train)\nscale(X_train, X_test)","9f05ed5b":"clfs = {\n    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n    \"Logistic Regression\": LogisticRegression(solver='liblinear'),\n    \"Random Forest\": RandomForestClassifier(random_state=0),\n    \"XGBoost Classifier\": XGBClassifier(\n        n_estimators=5000, eval_metric='auc',\n        use_label_encoder=False)\n}\n\ntry_models(clfs, X_train, y_train, X_test)","515f25b9":"def rf_objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int('n_estimators', 10, 100),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\", [7, 8, 9, 10, 11, 12, None]),\n        \"criterion\": trial.suggest_categorical('criterion', [\"gini\", \"entropy\"]),\n        \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 5),\n        \"min_samples_leaf\": trial.suggest_categorical('min_samples_leaf', [1, 2]),\n        \"max_features\": trial.suggest_categorical('max_features', [\"auto\", \"sqrt\", \"log2\"]),\n        \"class_weight\": trial.suggest_categorical('class_weight', [\"balanced\"]),\n        \"random_state\": trial.suggest_categorical('random_state', [0]),\n        \"n_jobs\": trial.suggest_categorical('n_jobs', [-1]),\n    }\n    model = RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    return -roc_auc_score(y_test, model.predict(X_test))","ddb5cdb8":"def dt_objective(trial):\n    params = {\n        \"criterion\": trial.suggest_categorical('criterion', [\"gini\", \"entropy\"]),\n        \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 5),\n        \"min_samples_leaf\": trial.suggest_categorical('min_samples_leaf', [1, 2]),\n        \"max_features\": trial.suggest_categorical('max_features', [\"auto\", \"sqrt\", \"log2\"]),\n        \"class_weight\": trial.suggest_categorical('class_weight', [\"balanced\"]),\n        \"random_state\": trial.suggest_categorical('random_state', [0])\n    }\n    model = RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    return -roc_auc_score(y_test, model.predict(X_test))","ce0621d3":"# Best score is 0.8412\n# study = optuna.create_study()\n# study.optimize(rf_objective, n_trials=200, timeout=3600 * 2)\n# print(f\"Best RandomForest ROC-AUC: {-round(study.best_value, 4)} with parameters {study.best_params}\\n\\n\")\n\nrf_best_score = 0.8438\nrf_best_params = {\n    'n_estimators': 10, 'criterion': 'entropy',\n    'min_samples_split': 2, 'min_samples_leaf': 2,\n    'max_features': 'log2', 'class_weight': 'balanced',\n    'random_state': 0, 'n_jobs': -1}\nrf = RandomForestClassifier(**rf_best_params)\nrf.fit(X_train, y_train);","0510df51":"# Best score is 0.8409\n# study = optuna.create_study()\n# study.optimize(dt_objective, n_trials=200, timeout=3600 * 2)\n# print(f\"Best DecisionTree ROC-AUC: {-round(study.best_value, 4)} with parameters {study.best_params}\\n\\n\")\n\ndt_best_score = 0.8409\ndt_best_params = {\n    'criterion': 'gini', 'min_samples_split': 2,\n    'min_samples_leaf': 2, 'max_features': 'sqrt',\n    'class_weight': 'balanced', 'random_state': 0}\ndt = DecisionTreeClassifier(**dt_best_params)\ndt.fit(X_train, y_train);","38ace40f":"print(f\"Random Forest Best: {rf_best_score}\")\nshow_confusion_matrix(y_test, rf.predict(X_test))\n\nprint(f\"Decision Tree Best: {dt_best_score}\")\nshow_confusion_matrix(y_test, dt.predict(X_test))","5cf6cb61":"# Categorical feats encoding","93e11e2c":"# Results","45db2e24":"***Random Forest gives more accurate predictions, and its precision is greater. But Decision Tree can also be used, it have greater recall.***","21a00beb":"# Hyperparameters tuning","506c2470":"# Data","9d7bc612":"**Married\/Single** and **Car_Ownership** are easy to encode.    \nFor **CITY** and **STATE** we will try target encoding.  \nFor **Profession** we can use one-hot encoding after replacing each profession with its category (software, medicine, ...).","379e0bc1":"#### Conclusion.\n* Both models make too many type I errors, so they can't be used.","2dde55b3":"As you can see, dataset is imbalanced. Because of this we will try to solve the task as anomaly detection and as imbalanced classification using SMOTE oversampling.","e8bf2a32":"# Imports","3a71934c":"# Task - imbalanced classification.","5e169e41":"#### Conclusion.\n* Logistic Regression never detect loan defaults, so it can't be used\n* Precisions:\n    - Decision Tree ~ 0.52\n    - Random Forest ~ 0.61\n    - XGBoost       ~ 0.59\n* Random Forest is model with the best precision\n* Decision Tree is the model with the best roc-auc score","0babd7b1":"# Model selection","03beb606":"## Task - anomalies detection.","5bbb76af":"# Exploratory Data Analysis"}}