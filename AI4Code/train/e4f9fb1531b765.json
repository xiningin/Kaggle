{"cell_type":{"4cdb26d8":"code","82eeccdd":"code","9d39aeb9":"code","675bc554":"code","592e8392":"code","15a8b9c0":"code","837be0dd":"code","fd29084d":"code","e96e50b8":"code","d25536f6":"code","41c42ba9":"code","4fbdb65c":"markdown","4674fb62":"markdown"},"source":{"4cdb26d8":"import requests\nfrom scrapy.http import TextResponse\nfrom fake_useragent import UserAgent\n\ncat1 = 'bag'\ncat2 = 24\nurl = 'http:\/\/playnomore.co.kr\/category\/{}\/{}\/'.format(cat1, cat2)\nheaders = {'user-agent': UserAgent().Chrome}\nres = requests.get(url, headers=headers)\nresponse = TextResponse(res.url, body=res.text, encoding='utf-8')\nresponse","82eeccdd":"links = response.xpath('\/\/*[@id=\"contents\"]\/div[2]\/div\/ul\/li\/div\/a\/@href').extract()\nlinks = list(map(response.urljoin, links))\nlinks","9d39aeb9":"res = requests.get(links[0], headers=headers)\nresponse = TextResponse(res.url, body=res.text, encoding='utf-8')","675bc554":"title = ''.join(response.xpath('\/\/*[@id=\"contents\"]\/div[1]\/div[1]\/div[2]\/div[1]\/text()').extract())\nprice = response.xpath('\/\/*[@id=\"contents\"]\/div[1]\/div[1]\/div[2]\/div[2]\/text()')[0].extract()\nimg = 'https:' + response.xpath('\/\/*[@id=\"contents\"]\/div[1]\/div[1]\/div[1]\/div[1]\/img\/@src')[0].extract()\ntitle, price, img","592e8392":"!scrapy startproject pnm","15a8b9c0":"!tree pnm\/","837be0dd":"%%writefile pnm\/pnm\/items.py\n\nimport scrapy\n\n\nclass PnmItem(scrapy.Item):\n    title = scrapy.Field()\n    price = scrapy.Field()\n    img = scrapy.Field()\n    link = scrapy.Field()\n    \n    def to_dict(self):\n        ### For conveneince\n        data= {'title': self['title'],\n                'price': self['price'],\n                'img': self['img'],\n                'link': self['link']}\n        return data","fd29084d":"%%writefile pnm\/pnm\/spiders\/spider.py\nimport scrapy\nfrom pnm.items import PnmItem\n\n\nclass PnmSpider(scrapy.Spider):\n    name = 'PnmSpider'\n    custom_settings = {\n        'DOWNLOADER_MIDDLEWARES' : {\n            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware' : None,\n            'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware' : 300,\n        }\n    }\n    \n    def __init__(self, cat1='bag', cat2=24, **kwargs):\n        self.start_url = 'http:\/\/playnomore.co.kr\/category\/{}\/{}\/'.format(cat1, cat2)\n        super().__init__(**kwargs)\n        \n    def start_requests(self):\n        import requests\n        from fake_useragent import UserAgent\n        from scrapy.http import TextResponse\n        \n        # Iterate all pages\n        headers = {'user-agent': UserAgent().Chrome}\n        res = requests.get(self.start_url, headers=headers)\n        response = TextResponse(res.url, body=res.text, encoding='utf-8')\n        pages = len(response.xpath('\/\/*[@id=\"contents\"]\/div[3]\/ol\/li'))\n        for page in range(1, pages+1):\n            url = self.start_url + '?page={}'.format(page)\n            yield scrapy.Request(url, callback=self.parse)\n        \n    def parse(self, response):\n        links = response.xpath('\/\/*[@id=\"contents\"]\/div[2]\/div\/ul\/li\/div\/a\/@href').extract()\n        links = list(map(response.urljoin, links))\n        for link in links:\n            yield scrapy.Request(link, callback=self.parse_page)\n            \n    def parse_page(self, response):\n        item = PnmItem()\n        item['title'] = ''.join(response.xpath('\/\/*[@id=\"contents\"]\/div[1]\/div[1]\/div[2]\/div[1]\/text()').extract())\n        item['price'] = response.xpath('\/\/*[@id=\"contents\"]\/div[1]\/div[1]\/div[2]\/div[2]\/text()')[0].extract()\n        item['img'] = 'https:' + response.xpath('\/\/*[@id=\"contents\"]\/div[1]\/div[1]\/div[1]\/div[1]\/img\/@src')[0].extract()\n        item['link'] = response.url\n        yield item","e96e50b8":"%%writefile pnm\/pnm\/pipelines.py\n\n\nclass PnmPipeline:\n    def process_item(self, item, spider):\n        import pymongo\n        \n        # move to pymongo\n        client = pymongo.MongoClient('mongodb:\/\/<your_mongodb_ip_address>')\n        db = client.pnm\n        collection = db.beauty\n\n        collection.insert(item.to_dict())\n        \n        return item","d25536f6":"%%writefile run.sh\ncd pnm\nscrapy crawl PnmSpider -o Pnm.csv -a cat1=beauty -a cat2=28","41c42ba9":"!.\/run.sh","4fbdb65c":"### Make scrapy project","4674fb62":"### Xpath checking"}}