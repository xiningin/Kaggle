{"cell_type":{"a5eff258":"code","4d6e9d4e":"code","b2797049":"code","47cf357a":"code","74eb572e":"code","15462fe3":"code","666b2469":"code","9993ea0f":"code","ee2e0adb":"code","64790697":"code","b226e077":"code","7c7c2bcb":"code","001ac77a":"code","f9cd79e3":"code","57e1dc6d":"code","5314687f":"code","9b42648e":"code","82f05300":"code","a4bc5489":"code","8bd7b375":"code","f097faf9":"code","c8e07771":"code","4866e306":"code","5367fe56":"code","78be1207":"code","e3566cae":"code","b6b4090d":"code","9c8eb3d2":"code","f9d23a21":"markdown","b33b56a4":"markdown","49872f13":"markdown","867f2d2a":"markdown","cdacf41b":"markdown"},"source":{"a5eff258":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d6e9d4e":"df1 = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf1.head()\n","b2797049":"df1[\"quality\"].unique()","47cf357a":"df1.shape","74eb572e":"df1.describe()\n","15462fe3":"df1.info()","666b2469":"import seaborn as sns \nimport matplotlib.pyplot as plt ","9993ea0f":"sns.countplot(df1['quality'])","ee2e0adb":"fig,ax = plt.subplots(ncols = 6, nrows = 2, figsize = (20,10))\nax = ax.flatten()\nindex = 0\nfor col in df1.columns:\n    sns.boxplot(col,data = df1,ax = ax[index])\n    index+=1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)\n    \n","64790697":"sns.pairplot(df1)","b226e077":"plt.figure(figsize = (20,10))\nsns.heatmap(df1.corr(), annot = True)","7c7c2bcb":"fig,ax = plt.subplots(ncols = 6, nrows = 2,figsize = (20,10))\nax = ax.flatten()\nindex = 0\nfor col in df1.columns:\n    sns.distplot(df1[col], ax = ax[index])\n    index+=1","001ac77a":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nmodel_name = [DecisionTreeClassifier,RandomForestClassifier,ExtraTreesClassifier,XGBClassifier,LogisticRegression]\nfrom sklearn.metrics import accuracy_score\n","f9cd79e3":"df1.head()","57e1dc6d":"x = df1.drop([\"quality\"], axis =1)\ny = df1['quality']","5314687f":"x.head()","9b42648e":"x.shape","82f05300":"#scaler  =StandardScaler()\n#x = scaler.fit_transform(x)\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE(k_neighbors=4)\n# transform the dataset\nx, y = oversample.fit_resample(x, y)\nx.shape","a4bc5489":"from sklearn.model_selection import cross_val_score, train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)","8bd7b375":"model = XGBClassifier()\nmodel.fit(x_train,y_train)","f097faf9":"pred = model.predict(x_test)","c8e07771":"score = accuracy_score(y_test,pred)\nscore","4866e306":"from tensorflow import keras\nfrom tensorflow.keras import layers\n","5367fe56":"model = keras.Sequential([\n    layers.BatchNormalization(input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1),\n])","78be1207":"model.compile(\noptimizer =\"adam\",\nloss = \"mae\"\n    \n)","e3566cae":"history = model.fit(x_train,y_train, validation_data = (x_test,y_test), batch_size = 512, epochs = 100)","b6b4090d":"history_df = pd.DataFrame(history.history)\nhistory_df","9c8eb3d2":"history_df.loc[:100, \"loss\"].plot()\nhistory_df.loc[:100, \"val_loss\"].plot()","f9d23a21":"Lot of Outliers \n\nWe need to remove them ","b33b56a4":"Handling Imbalanced Dataset","49872f13":"# Exploratory Data Analysis","867f2d2a":"# Model using ANN","cdacf41b":"# Model Prediction XGBOOST\n\n"}}