{"cell_type":{"cb0700a7":"code","0092fa1d":"code","ab07d5a5":"code","4a6c2d0c":"code","914df1ef":"code","56714f6f":"code","817e2865":"code","afc1d2f5":"code","8bf8c22b":"code","51e28d80":"code","4ed1cd34":"code","74a77d2e":"code","0b26d0c8":"code","642d76e0":"code","a0371211":"code","7d0bfd82":"code","513787af":"code","ca6be6ba":"code","737dd699":"code","a4bff524":"code","ab63643b":"code","f86a9619":"code","b92f53f0":"code","4a3cd871":"code","cc9789b4":"code","efb4aacc":"code","67c54fa1":"code","1d77f802":"code","c2d3ca55":"code","a809d7ac":"code","a36a63fe":"code","80875120":"code","2921efd2":"code","bb64c415":"code","6cdf497c":"code","7f89dd45":"code","2e37e252":"code","2943002f":"code","9cc128f2":"markdown","0f1ceb38":"markdown","4a7b0a00":"markdown","98deec22":"markdown","c60630ca":"markdown","c65b616c":"markdown","d43a8e55":"markdown","b73a05cb":"markdown","fe8086b3":"markdown","8315581f":"markdown","0d198241":"markdown","fe6ae001":"markdown","8b11fbd3":"markdown"},"source":{"cb0700a7":"import os\nimport random\nimport pathlib\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\n\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n)","0092fa1d":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n)\n\nMAX_LEN = 256  # 248\n\nclass LitDataset(Dataset):\n    def __init__(self, df, model_name_or_path=\"roberta-base\", inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","ab07d5a5":"def get_dataloader(model_name):\n    test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\", usecols=[\"id\", \"excerpt\"])\n    dataset = LitDataset(test, model_name, inference_only=True)\n    dataloader = DataLoader(dataset, batch_size=32, drop_last=False, shuffle=False, num_workers=4)\n    return dataloader","4a6c2d0c":"def load_data():\n    data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\", usecols=[\"target\", \"standard_error\"])\n    data.drop(data[(data.target == 0) & (data.standard_error == 0)].index, inplace=True)\n    data.reset_index(drop=True, inplace=True)\n    return data\n\ntarget = load_data()[\"target\"].to_numpy()","914df1ef":"all_oof = []\nall_pred = []\nall_pred_for_stack = []","56714f6f":"def predict_by_roberta(\n    model: nn.Module,\n    model_name_or_path: str,\n    model_dir: str,\n    num_fold: int = 5,\n):\n    model_dir = pathlib.Path(model_dir)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dataloader = get_dataloader(model_name_or_path)\n    \n    pred = []\n    for i in range(num_fold):\n        model = model.to(device)\n        model.load_state_dict(torch.load(str(model_dir \/ f\"model_{i}.pth\")))\n        model.eval()  # Ignore dropout and bn layers.\n\n        pred_by_fold = []\n        with torch.no_grad():  # Skip gradient calculation\n            for batch in dataloader:\n                batch[0] = batch[0].to(device)\n                batch[1] = batch[1].to(device)\n\n                z = model(*batch)\n                pred_by_fold.append(z)\n\n        pred_by_fold = torch.cat(pred_by_fold, dim=0).detach().cpu().numpy().copy()\n        pred.append(pred_by_fold)\n\n    return np.mean(pred, axis=0)","817e2865":"SEEDS = [42, 422, 12, 123, 7]","afc1d2f5":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.attention = nn.Sequential(            \n            nn.Linear(hidden_size, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(    \n            nn.LayerNorm(hidden_size),                    \n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","8bf8c22b":"data_dir = \"commonlit-finetuned-roberta-base\"\nmodel_name_or_path = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\n\nfor seed in SEEDS:\n    oof = np.load(f\"..\/input\/{data_dir}\/seed{seed}\/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"..\/input\/{data_dir}\/seed{seed}\/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)\n    \nall_pred_for_stack.append(np.mean(all_pred[-5:], axis=0))\nprint(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-5:], axis=0), squared=False))","51e28d80":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","4ed1cd34":"data_dir = \"commonlit-finetuned-roberta-base-init-4layers\"\nmodel_name_or_path = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\n\nfor seed in SEEDS:\n    oof = np.load(f\"..\/input\/{data_dir}\/seed{seed}\/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path),\n        model_name_or_path=model_name_or_path,\n        model_dir=f\"..\/input\/{data_dir}\/seed{seed}\/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","74a77d2e":"all_pred_for_stack.append(np.mean(all_pred[-5:], axis=0))","0b26d0c8":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-5:], axis=0), squared=False))","642d76e0":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.attention = nn.Sequential(            \n            nn.Linear(hidden_size, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","a0371211":"data_dir = \"commonlit-finetuned-roberta-base-squad2\"\nmodel_name_or_path = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\n\nfor seed in SEEDS:\n    oof = np.load(f\"..\/input\/{data_dir}\/seed{seed}\/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"..\/input\/{data_dir}\/seed{seed}\/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","7d0bfd82":"all_pred_for_stack.append(np.mean(all_pred[-5:], axis=0))","513787af":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-5:], axis=0), squared=False))","ca6be6ba":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.attention = nn.Sequential(            \n            nn.Linear(hidden_size, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.LayerNorm(hidden_size),\n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","737dd699":"data_dir = \"commonlit-finetuned-roberta-large\"\nmodel_name_or_path = \"..\/input\/roberta-transformers-pytorch\/roberta-large\"\n\nfor seed in [42, 422, 12]:\n    oof = np.load(f\"..\/input\/{data_dir}\/seed{seed}\/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"..\/input\/{data_dir}\/seed{seed}\/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","a4bff524":"all_pred_for_stack.append(np.mean(all_pred[-3:], axis=0))","ab63643b":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-3:], axis=0), squared=False))","f86a9619":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.attention = nn.Sequential(            \n            nn.Linear(hidden_size, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.LayerNorm(hidden_size),\n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","b92f53f0":"data_dir = \"commonlit-finetuned-roberta-large-squad2\"\nmodel_name_or_path = \"..\/input\/roberta-transformers-pytorch\/roberta-large\"\n\nfor seed in [42, 422, 12]:\n    oof = np.load(f\"..\/input\/{data_dir}\/seed{seed}\/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"..\/input\/{data_dir}\/seed{seed}\/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","4a3cd871":"all_pred_for_stack.append(np.mean(all_pred[-3:], axis=0))","cc9789b4":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-3:], axis=0), squared=False))","efb4aacc":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.regressor = nn.Sequential(                        \n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n        \n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(mean_embeddings)","67c54fa1":"data_dir = \"commonlit-finetuned-roberta-large-meanpool\"\nmodel_name_or_path = \"..\/input\/roberta-transformers-pytorch\/roberta-large\"\n\nfor seed in [42, 422, 12]:\n    oof = np.load(f\"..\/input\/{data_dir}\/seed{seed}\/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"..\/input\/{data_dir}\/seed{seed}\/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","1d77f802":"all_pred_for_stack.append(np.mean(all_pred[-3:], axis=0))","c2d3ca55":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-3:], axis=0), squared=False))","a809d7ac":"all_oof = np.concatenate(all_oof, axis=1)\nprint(\"RMSE: \", mean_squared_error(target, np.mean(all_oof, axis=1), squared=False))","a36a63fe":"import sys\nsys.path.append(\"..\/input\/textfeatmodule\")\nsys.path.append(\"..\/input\/textfeatmodule\/readability-package\")\n\nfrom textfeat import create_text_feat","80875120":"test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntext_feat = create_text_feat(test)\n\nfeat_cols = [\n    'chars_per_word',\n    'syll_per_word',\n#     'words_per_sent',\n#     'kincaid',\n#     'ari',\n#     'coleman_liau',\n#     'flesch',\n#     'gunning_fog',\n#     'lix',\n#     'smog',\n#     'rix',\n#     'dale_chall',\n#     'tobeverb',\n#     'auxverb',\n#     'conjunction',\n#     'pronoun',\n#     'preposition',\n#     'nominalization',\n#     'pronoun_b',\n    'interrogative',\n#     'article',\n#     'subordination',\n#     'conjunction_b',\n#     'preposition_b',\n]\n\n# feat_cols += [f\"spacy_{i}\" for i in range(300)]\n\nfeat_cols += [\n#     'CC',\n#     'CD',\n#     'DT',\n#     'EX',\n#     'FW',\n#     'IN',\n#     'JJ',\n#     'JJR',\n#     'JJS',\n#     'LS',\n#     'MD',\n#     'NN',\n#     'NNS',\n#     'NNP',\n#     'NNPS',\n#     'PDT',\n#     'POS',\n#     'PRP',\n#     'RB',\n#     'RBR',\n#     'RBS',\n#     'RP',\n#     'TO',\n#     'UH',\n#     'VB',\n#     'VBD',\n#     'VBG',\n#     'VBZ',\n#     'WDT',\n#     'WP',\n#     'WRB',\n#     'periods',\n    'commas',\n#     'semis',\n#     'exclaims',\n#     'questions',\n#     'num_char',\n#     'num_words',\n#     'unique_words',\n#     'word_diversity',\n#     'longest_word',\n#     'avg_len_word',\n]","2921efd2":"X_stack = np.concatenate(all_pred_for_stack, axis=1)\nX_stack = np.concatenate([X_stack, text_feat[feat_cols].to_numpy()], axis=1)","bb64c415":"import pickle\n\n# Predict function for stacking.\ndef predict(\n    data: pd.DataFrame, \n    model_dir: str,\n    seed: int = 42,\n    n_splits: int = 5,\n    num_seed: int = 5,\n) -> np.ndarray:\n    all_pred = []\n    for i in range(num_seed):\n        _seed = seed + i\n        pred = np.zeros(data.shape[0])\n        for n_fold in range(n_splits):\n            with open(os.path.join(model_dir, f\"seed{_seed}\/{n_fold}-fold.pkl\"), mode=\"rb\") as file:\n                model = pickle.load(file)\n            pred += model.predict(data) \/ n_splits\n        all_pred.append(pred.reshape(-1, 1))\n    return np.mean(all_pred, axis=0)","6cdf497c":"model_names = (\"bayesian_ridge\", \"ridge\", \"mlp\", \"svr\", \"xgb\")\n\npred_stacked = []\nfor model_name in model_names:\n    pred = predict(\n        X_stack, \n        f\"..\/input\/k\/konumaru\/train-stack-models-roberta-base-rsa\/{model_name}\",\n        num_seed=7\n    )\n    pred_stacked.append(pred)","7f89dd45":"# submission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n# # Averaging for submission\n# submission[\"target\"] = (\n#     0.7 * np.mean(np.concatenate(all_pred, axis=1), axis=1).reshape(-1, 1)\n#     + 0.3 * np.mean(np.concatenate(pred_stacked, axis=1), axis=1).reshape(-1, 1)\n# )\n\n# submission.head()","2e37e252":"submission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n# Averaging for submission\nsubmission[\"target\"] = (\n    0.7 * np.mean(np.concatenate(all_pred, axis=1), axis=1).reshape(-1, 1)\n    + 0.3 * np.mean(np.concatenate(pred_stacked, axis=1), axis=1).reshape(-1, 1)\n) * 1.01\n\nsubmission.head()","2943002f":"submission.to_csv(\"submission.csv\", index=False)","9cc128f2":"## Experiments\n\n<!-- |  |  |  | -->\n    \n| Experiment Name | CV | LB |\n| :--- | ---: | ---: |\n| Baselien |  |  |\n","0f1ceb38":"## Averaging","4a7b0a00":"## Import Library","98deec22":"#### [commonlit-finetuned-roberta-large](https:\/\/www.kaggle.com\/konumaru\/commonlit-finetuned-roberta-large)","c60630ca":"#### [commonlit-finetuned-roberta-large-meanpool](https:\/\/www.kaggle.com\/konumaru\/commonlit-finetuned-roberta-large-meanpool)","c65b616c":"#### [commonlit-finetuned-roberta-base](https:\/\/www.kaggle.com\/konumaru\/commonlit-finetuned-roberta-base)","d43a8e55":"## Submission","b73a05cb":"#### [commonlit-finetuned-roberta-large-squad2](https:\/\/www.kaggle.com\/konumaru\/commonlit-finetuned-roberta-large-squad2)","fe8086b3":"### Feature Engineering for stacking model","8315581f":"#### [commonlit-finetuned-roberta-base-init-4layers](https:\/\/www.kaggle.com\/konumaru\/commonlit-finetuned-roberta-base-init-4layers)","0d198241":"#### [commonlit-finetuned-roberta-base-squad2](https:\/\/www.kaggle.com\/konumaru\/commonlit-finetuned-roberta-base-squad2)","fe6ae001":"## Dataset","8b11fbd3":"## Inference"}}