{"cell_type":{"84b80da8":"code","89d07962":"code","160fee8a":"code","c252a70d":"code","5fd7a32a":"code","5636c673":"code","54de45a8":"code","815c253c":"code","7928dd5a":"code","e6605c18":"code","c6eabd2e":"code","8d4ca793":"code","5a53d6bf":"code","3316021c":"code","dcf1f029":"code","512b9482":"code","8bd9628c":"code","ca6d41aa":"code","fbb3a83a":"code","594d7d08":"code","f089ab36":"code","7d179034":"code","9ec6e4df":"code","efb105ea":"code","2fc90eec":"code","7d82c806":"code","a1a54732":"code","1e8c6e49":"code","efc010a3":"code","dc6bb2f7":"code","49d35812":"code","f854060a":"code","ea831fbc":"code","be179621":"code","f2941418":"code","b9445e9f":"code","a9eb26bb":"code","0943196b":"code","d6078110":"code","e4608485":"code","a53ebe27":"code","c3c39c10":"code","aafa6fba":"code","e3462e64":"code","8a292b7b":"code","a9034ad2":"code","c54dd6ea":"code","b49f997a":"code","d1c505e2":"code","583997f9":"code","cf5900f4":"code","e38ba8e0":"code","779bdb0b":"code","46a09687":"code","33ba7a99":"code","a107d790":"code","8d25b193":"code","8f90d087":"code","b9d7fa8f":"code","110ba407":"code","0b9449b2":"code","58defc4c":"code","78ad17e6":"code","cb3d4277":"code","e82ac641":"code","70cba272":"markdown","e4311145":"markdown","93ab90e9":"markdown","63546ec7":"markdown","cbc50fc3":"markdown","8fd91228":"markdown","75b7ef37":"markdown","079d1658":"markdown","6c3d7d81":"markdown","e7d583b9":"markdown","ec7498f0":"markdown","e60e0b9b":"markdown","47d6270a":"markdown","25be0c90":"markdown","cca93f02":"markdown","dcef9bd6":"markdown","a81ebdf3":"markdown","f4e88573":"markdown","18f18aea":"markdown","ecc4bd85":"markdown","2310d0c0":"markdown","e3acdb1b":"markdown","b5ddb7aa":"markdown","a3741f4d":"markdown","6dda2819":"markdown","c592f8fc":"markdown","ca12329e":"markdown","5eb4fb32":"markdown","4baa1b1b":"markdown","d378e302":"markdown","62d11215":"markdown","2a6036d4":"markdown"},"source":{"84b80da8":"import pandas as pd \nimport numpy as np \n\n#Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Map\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\n#Worldcloud\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n#Regex\nimport re\n\n#String\nimport string\n\n#Sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n\n#Tensorflow\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\n#Spacy\nimport spacy\nfrom spacy import displacy\nnlp=spacy.load('en_core_web_sm')\n\nsns.set_style('whitegrid')\n%matplotlib inline\n\n\nfrom nltk.tokenize import word_tokenize\n","89d07962":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","160fee8a":"train.info()","c252a70d":"print(\"Train dataset: {} rows arranged in {} columns.\".format(train.shape[0],train.shape[1]))\nprint(\"Test dataset: {} rows arranged in {} columns.\".format(test.shape[0],test.shape[1]))","5fd7a32a":"train.head()","5636c673":"test.head()","54de45a8":"missing_columns = ['keyword', 'location']\n\n\nfig = plt.figure(figsize=(14,6))\n\nax1=fig.add_subplot(121)\nsns.barplot(x=train[missing_columns].isnull().sum().index, y=train[missing_columns].isnull().sum().values,palette='mako',ax=ax1)\nax1.set_title('Missing values in train set')\n\nax2=fig.add_subplot(122)\nsns.barplot(x=test[missing_columns].isnull().sum().index, y=test[missing_columns].isnull().sum().values,palette='mako',ax=ax2)\nax2.set_title('Missing values in test set')\n\nfig.suptitle('Missing values in dataset')\nplt.show()","815c253c":"#Extract number of target values.\nvalues=train.target.value_counts()\nplt.figure(figsize=(7,6))\nsns.barplot(x=values.index,y=values,palette=['blue','red'])\nplt.ylabel('Samples')\nplt.xlabel('0:Not disaster | 1:Disaster')\nplt.title('Distribution of target values',fontsize=16)\nplt.show()","7928dd5a":"disaster = train.target.value_counts()[1]\/len(train.target)\nnot_disaster = train.target.value_counts()[0]\/len(train.target)\npercentage = {'Disaster tweets %':[disaster], 'Non disaster tweets %':[not_disaster]}\npercentage_data = pd.DataFrame(percentage)\npercentage_data.head()","e6605c18":"data = train.groupby('target').size()\n\ndata.plot(kind='pie', subplots=True, figsize=(10, 8), autopct = \"%.2f%%\", colors=['blue','red'])\nplt.title(\"Pie chart of different types of disasters\",fontsize=16)\nplt.ylabel(\"\")\nplt.legend()\nplt.show()","c6eabd2e":"train.length = train.text.apply(len)","8d4ca793":"fig = plt.figure(figsize=(14,6))\nax1 = fig.add_subplot(121)\nsns.boxplot(x=train.target[train.target==0],y=train.length, ax=ax1,color='blue')\ndescribe = train.length[train.target==0].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(122)\nax2.axis('off')\nfont_size = 16\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for non disaster tweets.', fontsize=16)\n\nplt.show()","5a53d6bf":"fig = plt.figure(figsize=(14,6))\nax1 = fig.add_subplot(121)\nsns.boxplot(x=train.target[train.target==1],y=train.length, ax=ax1,color='red')\ndescribe = train.length[train.target==1].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(122)\nax2.axis('off')\nfont_size = 16\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for disaster tweets.', fontsize=16)\n\nplt.show()","3316021c":"print('Unique keywords in train data: {}'.format(len(train.keyword.unique())))\nprint('Unique keywords in test data: {}'.format(len(test.keyword.unique())))","dcf1f029":"fig = plt.figure(figsize=(14,6))\n\nax1 = fig.add_subplot(111)\ntrain_keywords=sns.barplot(x=train.keyword.value_counts()[:25].index,y=train.keyword.value_counts()[:25][:],palette='icefire',ax=ax1)\ntrain_keywords.set_xticklabels(train_keywords.get_xticklabels(),rotation=90)\ntrain_keywords.set_ylabel('Keyword frequency')\nplt.title('Top 25 common keywords in train data',fontsize=16)\nplt.show()","512b9482":"plt.figure(figsize=(14,6))\ncommon_keyword=sns.barplot(x=train.location.value_counts()[:25].index,y=train.location.value_counts()[:25][:],palette='icefire')\ncommon_keyword.set_xticklabels(common_keyword.get_xticklabels(),rotation=90)\ncommon_keyword.set_ylabel('Location frequency',fontsize=12)\nplt.title('Top 25 common location of tweets for train data',fontsize=16)\nplt.show()","8bd9628c":"fig = plt.figure(figsize=(14,6))\n\nax1 = fig.add_subplot(111)\ntest_keywords=sns.barplot(x=test.keyword.value_counts()[:25].index,y=test.keyword.value_counts()[:25][:],palette='icefire',ax=ax1)\ntest_keywords.set_xticklabels(test_keywords.get_xticklabels(),rotation=90)\ntest_keywords.set_ylabel('Keyword frequency')\nplt.title('Top 25 common keywords in test data',fontsize=16)\nplt.show()","ca6d41aa":"train['number_of_words'] = train.text.apply(lambda x: len((str(x).split())))\ntrain['number_of_unique_words'] = train.text.apply(lambda x: len(set(str(x).split())))","fbb3a83a":"fig,ax = plt.subplots(ncols=2,figsize=(14,7))\n\nword_count_1 = sns.distplot(train.number_of_words[train.target==1],color='red',ax=ax[0])\nword_count_0 = sns.distplot(train.number_of_words[train.target==0],color='blue',ax=ax[0])\nunique_count_1 = sns.distplot(train.number_of_unique_words[train.target==1],color='red',ax=ax[1])\nunique_count_0 = sns.distplot(train.number_of_unique_words[train.target==0],color='blue',ax=ax[1])\nword_count_1.set_title('Number of words used to disaster vs. non disaster tweets')\nunique_count_1.set_title('Number of unique words used to disaster vs. non disaster tweets')\nplt.suptitle('Analysis of number of words used in tweets',fontsize=16)\n\n\nplt.show()","594d7d08":"print('Unique keywords in train data: {}'.format(len(train.location.unique())))\nprint('Unique keywords in test data: {}'.format(len(test.location.unique())))","f089ab36":"plt.figure(figsize=(14,6))\ncommon_keyword=sns.barplot(x=test.location.value_counts()[:25].index,y=test.location.value_counts()[:25][:],palette='icefire')\ncommon_keyword.set_xticklabels(common_keyword.get_xticklabels(),rotation=90)\ncommon_keyword.set_ylabel('Location frequency',fontsize=12)\nplt.title('Top 25 common location of tweets for test data',fontsize=16)\nplt.show()","7d179034":"data = train.location.value_counts()[:25,]\ndata = pd.DataFrame(data)\ndata = data.reset_index()\ndata.columns = ['location', 'counts'] \ngeolocator = Nominatim(user_agent=\"Location Map\")\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n\ndict_latitude = {}\ndict_longitude = {}\nfor i in data.location.values:\n    print(i)\n    location = geocode(i)\n    dict_latitude[i] = location.latitude\n    dict_longitude[i] = location.longitude\ndata['latitude'] = data.location.map(dict_latitude)\ndata['longitude'] = data.location.map(dict_longitude)","9ec6e4df":"location_map = folium.Map(location=[7.0, 7.0], zoom_start=2)\nmarkers = []\nfor i, row in data.iterrows():\n    loss = row['counts']\n    if row['counts'] > 0:\n        count = row['counts']*0.4\n    folium.CircleMarker([float(row['latitude']), float(row['longitude'])], radius=float(count), color='red', fill=True).add_to(location_map)\nlocation_map","efb105ea":"example=\"AMAZING NOTEBOOK(shameless self promotion :D): https:\/\/www.kaggle.com\/michawilkosz\/simple-way-to-top-26-blended-regression-model\"","2fc90eec":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","7d82c806":"print(remove_URL(example))","a1a54732":"train['text'] = train['text'].apply(lambda x : remove_URL(x))\ntest['text'] = test['text'].apply(lambda x : remove_URL(x))","1e8c6e49":"example = \"\"\"<div>\n<h1>House Prices Notebook<\/h1>\n<p>Simple way to top 26 blended regression model<\/p>\n<a href=\"https:\/\/www.kaggle.com\/michawilkosz\/simple-way-to-top-26-blended-regression-model<\/a>\n<\/div>\"\"\"","efc010a3":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","dc6bb2f7":"print(remove_html(example))","49d35812":"train['text']=train['text'].apply(lambda x : remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_html(x))","f854060a":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","ea831fbc":"print(remove_emoji(\"Oh no! Another pandemic \ud83d\ude14\ud83d\ude14\"))","be179621":"train['text']=train['text'].apply(lambda x : remove_emoji(x))\ntest['text']=test['text'].apply(lambda x : remove_emoji(x))","f2941418":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"","b9445e9f":"print(remove_punct(example))","a9eb26bb":"test.dropna(how='any',inplace=True,axis=1)\ntrain.dropna(how='any',inplace=True,axis=1)","0943196b":"def exctract_text(data, target):\n    extracted=[]\n    \n    for x in data[data['target']==target]['text'].str.split():\n        for i in x:\n            extracted.append(i)\n    return extracted","d6078110":"extracted_text_1 = exctract_text(train,1)\nextracted_text_0 = exctract_text(train,0)","e4608485":"plt.figure(figsize=(14,6))\nword_cloud = WordCloud(background_color=\"white\",max_font_size=60).generate(\" \".join(extracted_text_1[:50]))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in disaster tweets.',fontsize=20)\nplt.show()","a53ebe27":"plt.figure(figsize=(14,6))\nword_cloud = WordCloud(background_color=\"white\",max_font_size=60).generate(\" \".join(extracted_text_0[:50]))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in non disaster tweets.',fontsize=20)\nplt.show()","c3c39c10":"import string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nmarks = string.punctuation\nmarks = list(marks)\nmarks.append(\"...\")\nmarks.append(\"....\")\n\n\n\nnlp = spacy.load('en')\nstop_words = list(STOP_WORDS)","aafa6fba":"def tokenizer(sentence):\n    doc = nlp(sentence)\n    clean_tokens = []\n    for token in doc:\n        if token.lemma_ != '-PRON-':\n            token = token.lemma_.lower().strip()\n        else:\n            token = token.lower_\n        if token not in stop_words and token not in marks:\n            clean_tokens.append(token)\n    return clean_tokens","e3462e64":"bow_vector = CountVectorizer(tokenizer = tokenizer, ngram_range=(1,1))\ntfidf_vector = TfidfVectorizer(tokenizer = tokenizer)","8a292b7b":"X = train['text']\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","a9034ad2":"#Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n\n#Pipeline\nlr_pipe = Pipeline([('vectorizer', bow_vector),\n                 ('classifier', classifier)])\n\n# model generation\nlr_pipe.fit(X_train,y_train)","c54dd6ea":"# Predicting with a test dataset\nlr_pred = lr_pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Accuracy:\",accuracy_score(y_test, lr_pred))\nprint(\"Logistic Regression Precision:\",precision_score(y_test, lr_pred))\nprint(\"Logistic Regression Recall:\",recall_score(y_test, lr_pred))","b49f997a":"svc = LinearSVC()\nsvc_pipe = Pipeline([('tfidf', tfidf_vector), ('clf', svc)])\nsvc_pipe.fit(X_train,y_train)","d1c505e2":"svc_pred = svc_pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"SVC Accuracy:\",accuracy_score(y_test, svc_pred))\nprint(\"SVC Precision:\",precision_score(y_test, svc_pred))\nprint(\"SVC Regression Recall:\",recall_score(y_test, svc_pred))","583997f9":"X = train['text']\ny = train['target']\nX = X.values\ny = y.values\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)  \nvocab_size = len(tokenizer.word_index) + 1 \n\nsequences = tokenizer.texts_to_sequences(X) \n\nmax_length = 0 \nfor i in X:\n    if len(i) > max_length:\n        max_length = len(i)\n        \npadded = pad_sequences(sequences, maxlen=max_length, padding='post')\n\nembeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt')\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n    \nglove_file.close()\n\nembeddings_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[index] = embedding_vector\n\nX_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2)\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","cf5900f4":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 50\n\nhistory = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)","e38ba8e0":"lstm_pred = model.predict_classes(X_test)","779bdb0b":"print(\"LSTM Accuracy:\",accuracy_score(y_test, lstm_pred))\nprint(\"LSTM Precision:\",precision_score(y_test, lstm_pred))\nprint(\"LSTM Recall:\",recall_score(y_test, lstm_pred))","46a09687":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","33ba7a99":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization\n\nmax_len=512","a107d790":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","8d25b193":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","8f90d087":"module_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","b9d7fa8f":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","110ba407":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","0b9449b2":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","58defc4c":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","78ad17e6":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","cb3d4277":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","e82ac641":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","70cba272":"# Train test split ","e4311145":"# Logistic Regression","93ab90e9":"### Visualize top 25 locations of disaster tweets on world map","63546ec7":"### Removing HTML tags","cbc50fc3":"# Cool looking and useful wordclouds\nThanks to: https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python","8fd91228":"# Exploratory Data Analysis","75b7ef37":"# Submission","079d1658":"Based on this plot we can assume that all disaster tweets(fake or real) contains some words related to disasters.","6c3d7d81":"# SVM","e7d583b9":"# Main objectives\n\n1. Performing exploratory data analysis of tweets contained in the dataset.\n2. Data cleaning of text data.\n3. Tokenizing interesting parts of tweets and prepare them for machine learning algorithm. \n4. Using different algorithms trying to achieve best results in public leaderboard. ","ec7498f0":"Columns description:\n\n* id - identifier for each tweet\n* keyword - a particular keyword from the tweet (contain NaN)\n* location - the location the tweet was sent from (contain NaN)\n* text - the text of the tweet\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","e60e0b9b":"### Sneek peek of datasets","47d6270a":"### Removing Emojis","25be0c90":"# Import libraries","cca93f02":"### Tweet keywords analysis\n\nUnique words in test and train data.","dcef9bd6":"And of course columns with NaN values:","a81ebdf3":"# Missing values\n\nThanks to: https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert","f4e88573":"#### Removing punctuations","18f18aea":"# Quick overview","ecc4bd85":"Based on sampled wordclouds for both disaster and non disaster tweets. It is clear that non disaster tweets contains more words used in colloquial speech(e.g. love, car, fruits, summer).","2310d0c0":"As we can see in both train and test sets there is a lot of missing values for two columns - 'keyword' and 'location'. For further building of machine learning models, missing values will be dropped.","e3acdb1b":"### Most frequent tweet locations","b5ddb7aa":"In considered population of tweets there is more non disaster tweets.","a3741f4d":"# LSTM","6dda2819":"### Removing URLS","c592f8fc":"### Preview of our target variable - \"target\".","ca12329e":"### Comparasion of text length in Real and Fake disaster Tweets\nAdd column to hold particular tweet length","5eb4fb32":"# Tokenizer from spacy","4baa1b1b":"# Missing values","d378e302":"From these distplots we can formulate a conclusion that the content of disaster tweets contains more words and at the same time more unique words.","62d11215":"### Analysis of the number of words used in tweets","2a6036d4":"# BERT\nSimple try of this powerful pre-trained model. Works really slowly.\n\nThanks for awesome tutorial: https:\/\/www.kaggle.com\/pavansanagapati\/knowledge-graph-nlp-tutorial-bert-spacy-nltk"}}