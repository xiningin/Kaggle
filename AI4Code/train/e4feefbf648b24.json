{"cell_type":{"dbb67e8c":"code","d51f2d3e":"code","9d1c5c16":"code","9fb4fa8e":"code","d60a859e":"code","30f0a5ae":"code","b53304cf":"code","a291e927":"code","495a5d28":"code","546875c4":"code","0b96bef4":"code","bca85f5a":"code","b4515637":"code","1df8fb39":"code","c5f66e70":"code","806ddb6f":"code","5df835b0":"markdown"},"source":{"dbb67e8c":"import torch\nimport torchvision\nimport numpy as np","d51f2d3e":"def load_dataset(data_path='..\/input\/anacondas_pythons\/train'):\n    train_dataset = torchvision.datasets.ImageFolder(\n        root=data_path,\n        transform=torchvision.transforms.ToTensor()\n    )\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=60,\n        num_workers=0,\n        shuffle=True\n    )\n    return train_loader","9d1c5c16":"trainloader = load_dataset()\ntestloader = load_dataset('..\/input\/anacondas_pythons\/valid')","9fb4fa8e":"dataiter = iter(trainloader) # To iterate the data set\nimages,labels = dataiter.next() # Gives the images in the order\n#labels = torch.zeros(1, 38).reshape(-1)\nprint(images.shape) # Printing the shape of the images.\n\nprint(images[1].shape) # Shape of each image\nprint(labels[1].item()) # Number of labels present.","d60a859e":"print(type(labels))\nprint(labels.shape)\nprint(labels)","30f0a5ae":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n\nimport torch.nn as nn\n\nclass LeNet(nn.Module):\n    def __init__(self): \n        super(LeNet, self).__init__()\n        self.cnn_model = nn.Sequential(\n            nn.Conv2d(3, 6, 5),         # (N, 3, 300, 400) -> (N,  6, 296, 396)\n            nn.Tanh(),\n            nn.AvgPool2d(2, stride=2),  # (N, 6, 28, 28) -> (N,  6, 14, 14)\n            nn.Conv2d(6, 16, 5),        # (N, 6, 14, 14) -> (N, 16, 10, 10)  \n            nn.Tanh(),\n            nn.AvgPool2d(2, stride=2)   # (N,16, 10, 10) -> (N, 16, 5, 5)\n        )\n        self.fc_model = nn.Sequential(\n            nn.Linear(111744,120),         # (N, 400) -> (N, 120)\n            nn.Tanh(),\n            nn.Linear(120,84),          # (N, 120) -> (N, 84)\n            nn.Tanh(),\n            nn.Linear(84,10)            # (N, 84)  -> (N, 10)\n        )\n        \n    def forward(self, x):\n        x = self.cnn_model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_model(x)\n        return x\ndef evaluation(dataloader, model):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (pred == labels).sum().item()\n    return 100 * correct \/ total\nimport torch.optim as optim\nnet = LeNet().to(device)\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.Adam(net.parameters())\n\nimport matplotlib.pyplot as plt\n","b53304cf":"%%time\nmax_epochs = 10\nloss_epoch_arr = []\n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(trainloader, 0):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        opt.zero_grad()\n\n        outputs = net(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n    loss_epoch_arr.append(loss.item()) # Collecting loss after each epoch.\n    #print('Epoch: %d\/%d' % (epoch, max_epochs))\n    print('Epoch: %d\/%d, Test acc: %0.2f, Train acc: %0.2f, loss: %0.2f' % (epoch, max_epochs, evaluation(testloader,net), evaluation(trainloader,net),loss.item()))\n\nplt.plot(loss_epoch_arr)\nplt.show()","a291e927":"from torchvision.models import resnet18\nimport copy","495a5d28":"resnet = resnet18(pretrained=True)","546875c4":"print(resnet)","0b96bef4":"for param in resnet.parameters():\n    param.requires_grad = False","bca85f5a":"num_classes = 2\nfinal_in_features = resnet.fc.in_features\nresnet.fc = nn.Linear(final_in_features, num_classes)","b4515637":"print(resnet)","1df8fb39":"resnet = resnet.to(device)\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.SGD(resnet.parameters(), lr=0.01)","c5f66e70":"loss_epoch_arr = []\nmax_epochs = 20\nbatch_size = 10\nmin_loss = 1000\n\nn_iters = np.ceil(60\/batch_size)\n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(trainloader, 0):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        opt.zero_grad()\n\n        outputs = resnet(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n        if min_loss > loss.item():\n            min_loss = loss.item()\n            best_model = copy.deepcopy(resnet.state_dict())\n            print('Min loss %0.2f' % min_loss)\n        \n        if i % 100 == 0:\n            print('Iteration: %d\/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n            \n        del inputs, labels, outputs\n        torch.cuda.empty_cache()\n        \n    loss_epoch_arr.append(loss.item())\n        \n    print('Epoch: %d\/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n        epoch, max_epochs, \n        evaluation(testloader, resnet), evaluation(trainloader, resnet)))\n    \n    \nplt.plot(loss_epoch_arr)\nplt.show()","806ddb6f":"resnet.load_state_dict(best_model)\nprint('The train accuracy is',evaluation(trainloader, resnet),'and the test accuracy is', evaluation(testloader, resnet))","5df835b0":"# In this notebook I have used LeNet and ResNet"}}