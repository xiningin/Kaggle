{"cell_type":{"20838549":"code","efea27fc":"code","b334725a":"code","ddb52ffe":"code","bf2e774a":"code","e41e2105":"code","6a6ee570":"code","a3127773":"code","068c7367":"code","48e4988b":"code","bed7f756":"code","424ec4e2":"code","76ff6862":"code","07b309f5":"code","6212274b":"code","dd42b619":"code","769a5615":"code","d9f0d0da":"code","7a0aa428":"code","f139ac28":"code","eca6ec27":"code","411ebaee":"code","b89fb0d7":"markdown","f83aee87":"markdown","35623a4e":"markdown","92ed2c6d":"markdown","5582c6c5":"markdown","1799ab9a":"markdown","05e96670":"markdown","2769f0fb":"markdown","810bd9af":"markdown","e377e14d":"markdown","0e92ba8f":"markdown","40b9c9da":"markdown","07337767":"markdown"},"source":{"20838549":"# import libraries\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom bokeh.layouts import gridplot\nfrom bokeh.plotting import figure\nfrom bokeh.io import show, output_notebook\nfrom bokeh.io import curdoc\ncurdoc().theme = 'light_minimal'\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","efea27fc":"# cargar la base de datos\nfilename = '..\/input\/weather-dataset-rattle-package\/weatherAUS.csv'\nweather_df = pd.read_csv (filename)","b334725a":"# We show the first 5 rows of our dataframe\nweather_df.head(5)","ddb52ffe":"print(\"Shape of the dataframe: \", weather_df.shape)\n# We can see the length of our dataframe\n# in this case we've got 145460 rows and 23 columns","bf2e774a":"# We can see the date type for each column by using dtypes\nweather_df.dtypes","e41e2105":"# To see an overall description of our dataframe we use the describe function\nweather_df.describe()","6a6ee570":"grouped = weather_df.groupby('Location')\ndisplay(grouped.filter(lambda x: x['Rainfall'].mean() > 4.).Location.unique().tolist())","a3127773":"# making some histogram and density graph\nfig, axes = plt.subplots(2, 1, figsize = (20, 12)) # syntax is plt.subplots(nrows, ncols, figsize=(width, height))\nax = axes.ravel()\nfig.suptitle(\"Histogram\/Density of Rainfall according to its location\", fontsize = 20)\n\nrain_cairns = weather_df.loc[weather_df.Location == 'Cairns']\nrain_coff = weather_df.loc[weather_df.Location == 'CoffsHarbour']\nrain_darwin = weather_df.loc[weather_df.Location == 'Darwin']\n\n# histogram\nrain_cairns[rain_cairns['RainToday'] == 'Yes'].Rainfall.plot(kind = 'hist', ax = ax[0], alpha = 0.5, bins=50, label = 'Cairns')\nrain_coff[rain_coff['RainToday'] == 'Yes'].Rainfall.plot(kind = 'hist', ax = ax[0], alpha = 0.3, bins=50, label = 'CoffsHarbour')\nrain_darwin[rain_darwin['RainToday'] == 'Yes'].Rainfall.plot(kind = 'hist', ax = ax[0], alpha = 0.2, bins=50, label = 'Darwin')\n\n# density\nrain_cairns[rain_cairns['RainToday'] == 'Yes'].Rainfall.plot(kind = 'density', ax = ax[1], alpha = 0.8, linewidth=4, label = 'Cairns')\nrain_coff[rain_coff['RainToday'] == 'Yes'].Rainfall.plot(kind = 'density', ax = ax[1], alpha = 0.8, linewidth=4, label = 'CoffsHarbour')\nrain_darwin[rain_darwin['RainToday'] == 'Yes'].Rainfall.plot(kind = 'density', ax = ax[1], alpha = 0.8, linewidth=4, label = 'Darwin')\n\nax[0].legend(fontsize=18)\nax[0].set_xlim(weather_df.Rainfall.min()-10, 125)\nax[0].set_title('Rainfall Histogram based on the top 3 rainy places on a rainy day', fontsize = 15)\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\n\nax[1].legend(fontsize=18)\nax[1].set_xlim(weather_df.Rainfall.min()-10, 125)\nax[1].set_xlabel('Rainfall', fontsize = 18)\nax[1].set_title('Rainfall distribution based on the top 3 rainy places on a rainy day', fontsize = 15)\nax[1].tick_params(axis='both', which='major', labelsize=14)\nax[1].tick_params(axis='both', which='minor', labelsize=14)","068c7367":"# this is a boxplot showing the % of humidity on each category of severity\nsns.boxplot(x=\"Location\", y=\"Sunshine\", data=weather_df)\nplt.xticks(rotation=80)\nplt.show()","48e4988b":"#correlation matrix\nsns.set(rc = {'figure.figsize':(15,8)})\ncorrplot = sns.heatmap(weather_df.corr(), annot = True)\n\nplt.show()","bed7f756":"#dispersion matrix\nfig = plt.figure(figsize=(8, 8))\ngroups = weather_df.groupby('Location')\n#Temperatura M\u00ednima vs. Temperatura M\u00e1xima\n# Plot\nfig, ax = plt.subplots()\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\nfor name, group in groups:\n    if(group['Rainfall'].mean() > 3):\n        ax.plot(group['MinTemp'], group['MaxTemp'], marker='o', linestyle='', ms=3, label=name)\n\nplt.title('Variaci\u00f3n en la temperatura')\nplt.xlabel('Temperatura M\u00ednima')\nplt.ylabel('Temperatura M\u00e1xima')\nplt.legend(loc='best')\nfig.tight_layout()\nplt.show()","424ec4e2":"weather_df = weather_df.dropna()\narray_df = weather_df.values\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nonly_num = weather_df.select_dtypes(include=numerics)\ny = array_df[:, 21:22]\nx = only_num\ny","76ff6862":"# scaling data using the min and max value for it\nfrom sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler()\nscaled_x = scaler.fit_transform(x)\nplt.scatter(scaled_x[:,2], scaled_x[:,3], color='red', label='standard-scaler', alpha=0.3)\n\nplt.title('Rainfall \/ Evaporation Relationship using MinMaxScaler and Stardardized Normalization')\nplt.xlabel('Rainfall')\nplt.ylabel('Evaporation')\nplt.legend(loc='upper left')\nplt.grid()\n\nplt.tight_layout()","07b309f5":"# scaling data using the min and max value for it\nfrom sklearn import preprocessing\n\nminmax = preprocessing.MinMaxScaler().fit_transform(x)\nplt.scatter(minmax[:,2], minmax[:,3],color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n\nnorm = preprocessing.Normalizer().fit_transform(x)\nplt.scatter(norm[:,2], norm[:,3],color='yellow', label='normalized', alpha=0.3)\n\nplt.title('Rainfall \/ Evaporation Relationship using MinMaxScaler and Stardardized Normalization')\nplt.xlabel('Rainfall')\nplt.ylabel('Evaporation')\nplt.legend(loc='upper left')\nplt.grid()\n\nplt.tight_layout()","6212274b":"from sklearn.model_selection import cross_val_score  # Metodo de validaci\u00f3n\nfrom sklearn.linear_model import LogisticRegression  # Modelo matem\u00e1tico\n\nmodel = LogisticRegression(solver=\"lbfgs\", max_iter=5000)\nresults = cross_val_score(model, x, y)\nmean_p = results.mean()*100.0\nstd_p = results.std()\nprint(f\"Accuracy: {mean_p:,.2f}% Standard Deviation: {std_p:,.2f}\")","dd42b619":"from sklearn.model_selection import cross_val_score  # Metodo de validaci\u00f3n\nfrom sklearn.model_selection import KFold  # Iteraciones\nfrom sklearn.linear_model import LogisticRegression  # Modelo matem\u00e1tico\n\nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\nmodel = LogisticRegression(solver=\"lbfgs\", max_iter=5000)\nresults = cross_val_score(model, x, y, cv=kfold)\nmean_p = results.mean()*100.0\nstd_p = results.std()\nprint(f\"Accuracy: {mean_p:,.2f}% Standard Deviation: {std_p:,.2f}\")","769a5615":"from sklearn.model_selection import train_test_split  # Divisi\u00f3n por porcentaje\nfrom sklearn.linear_model import LogisticRegression  # Modelo matem\u00e1tico\n\ntest_size = .33\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\nmodel = LogisticRegression(solver=\"lbfgs\", max_iter=4000)\nmodel.fit(x_train, y_train)\nresults = model.score(x_test, y_test)\nmean_p = results.mean()*100.0\nstd_p = results.std()\nprint(f\"Accuracy: {mean_p:,.2f}% {std_p:,.2f}\")","d9f0d0da":"from sklearn.model_selection import train_test_split  # Divisi\u00f3n por porcentaje\nfrom sklearn.linear_model import LogisticRegression  # Modelo matem\u00e1tico\n\ntest_size = .33\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, shuffle=True)\nmodel = LogisticRegression(solver=\"lbfgs\", max_iter=4000)\nmodel.fit(x_train, y_train)\nresults = model.score(x_test, y_test)\nmean_p = results.mean()*100.0\nstd_p = results.std()\nprint(f\"Accuracy: {mean_p:,.2f}% {std_p:,.2f}\")","7a0aa428":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\n# create X (features) and y (response)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=6)\n\n# check classification accuracy of KNN with K=5\nknn = KNeighborsClassifier(n_neighbors=5)\nresults = knn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nmatrix = confusion_matrix(y_test, y_pred)\ndisplay(matrix)","f139ac28":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import cohen_kappa_score\n\ntest_size = .33\nseed = 1\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=seed)\nmodel = LogisticRegression(max_iter=1900)\nmodel.fit(x_train, y_train)\npredicted = model.predict(x_test)\ncohen_score = cohen_kappa_score(y_test, predicted)\nprint(f\"Cohen Score: {cohen_score:.2f}\")","eca6ec27":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\nmodel = LogisticRegression(solver='lbfgs', max_iter=4000)\nscoring = 'roc_auc'\nresults = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\nprint(f\"AUC: {results.mean()*100:,.2f}% {results.std():,.2f}\")","411ebaee":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import label_binarize\n\nseed = 1\ntest_size = .33\ny = label_binarize(y, classes=['Yes','No'])\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=seed)\nmodel = LogisticRegression(solver='lbfgs', max_iter=2000)\nmodel.fit(x_train, y_train)\ny_score = model.decision_function(x_test)\nfpr, tpr, threshold = roc_curve(y_test, y_score)\n\nprint(f\"True Positive Ratio (TPR): {tpr.mean()*100:,.1f}% {tpr.std():,.1f}%\")\nprint(f\"False Positive Ratio (FPR): {fpr.mean()*100:,.1f}% {fpr.std():,.1f}%\")\nroc_auc = auc(fpr, tpr)\n\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","b89fb0d7":"## Funciones para la visualizaci\u00f3n de los datos.\n- hist().\n- Funci\u00f3n de densidad.\n- Boxplot.\n- Matriz de correlaci\u00f3n.\n- Matriz de dispersi\u00f3n por clase.","f83aee87":"# Cross Validation with repetitions","35623a4e":"## M\u00e9todos de Validaci\u00f3n (T\u00e9cnicas de Remuestreo) aplicando Regresi\u00f3n log\u00edstica.\n- Validaci\u00f3n cruzada.\n- Divisi\u00f3n por porcentaje.\n- Validaci\u00f3n cruzada con repeticiones.\n- Divisi\u00f3n por porcentaje repetidos aleatoriamente.","92ed2c6d":"# AUC Curve","5582c6c5":"# Actividad #2 de Miner\u00eda de Datos\n## Mar\u00eda Fernanda Gallo Cruz\n### Especialidad en Ciencia de Datos","1799ab9a":"# Kappa de Cohen","05e96670":"# ROC Curve","2769f0fb":"# Division by percentages","810bd9af":"# Cross Validation","e377e14d":"# Division by percentages randomly","0e92ba8f":"## Transformaci\u00f3n de datos.\n- Escalamiento.\n- Estandarizaci\u00f3n\n- Normalizaci\u00f3n.\n- Box-cox, Yeo-johnson (si se requiere).","40b9c9da":"# Confusion Matrix","07337767":"## M\u00e9tricas para evaluaci\u00f3n de Algoritmos.\n- Matriz de confusi\u00f3n.\n- Kappa de Cohen.\n- Curva ROC-AUC"}}