{"cell_type":{"8f376ef6":"code","846826a3":"code","0bf486b0":"code","968bd804":"code","77ccf995":"code","5922032d":"code","cb020e3c":"code","5bad69c4":"code","cbd65044":"code","7034a0b9":"code","6338d4d4":"code","da4841ff":"code","02ab66e2":"code","77b290b0":"code","d62ba8c8":"code","70848604":"code","2d2a9065":"code","97576c62":"code","cb25d70c":"code","9b23a77f":"code","922f6313":"markdown","de05c116":"markdown","d2fa366e":"markdown","650e2e87":"markdown","18e9b842":"markdown","4ca55bab":"markdown","4bac7aa3":"markdown","8a2c21b6":"markdown","daf4f3a3":"markdown","c1a7e3f6":"markdown","8d11b15b":"markdown","9fb73622":"markdown","78e06ab7":"markdown","2ca22b14":"markdown","b7d30c8f":"markdown","055b59d8":"markdown"},"source":{"8f376ef6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","846826a3":"train = pd.read_csv('\/kaggle\/input\/predict-who-is-more-influential-in-a-social-network\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/predict-who-is-more-influential-in-a-social-network\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/predict-who-is-more-influential-in-a-social-network\/sample_predictions.csv')\ntrain.head(5)","0bf486b0":"print(train.shape)\nprint(test.shape)\nprint((train.columns).difference(test.columns))","968bd804":"train.isnull().sum().sum()\ntest.isnull().sum().sum()","77ccf995":"train.Choice.value_counts()","5922032d":"y = train['Choice']\nx = train.drop('Choice',axis=1)","cb020e3c":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)","5bad69c4":"print(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)\n","cbd65044":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=200, criterion='gini', min_samples_split=5, min_samples_leaf=2, max_features='auto', bootstrap=True, n_jobs=-1, random_state=42)\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n","7034a0b9":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred))","6338d4d4":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in range(200,2000,200)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","da4841ff":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(x_train, y_train)","02ab66e2":"rf_random.best_params_","77b290b0":"from sklearn import metrics\n\ndef evaluate(model, test_features, test_labels):\n    y_pred = model.predict(test_features)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    print (accuracy)\n    print(confusion_matrix(y_test,y_pred))\n    \n\nbest_random = rf_random.best_estimator_\nevaluate(best_random, x_test, y_test)\n","d62ba8c8":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [10,15],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5,6],\n    'min_samples_split': [3,4,5,6],\n    'n_estimators': [1150, 1200, 1250, 1300,1350]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\n# Fit the grid search to the data\ngrid_search.fit(x_train,y_train)\n","70848604":"grid_search.best_params_","2d2a9065":"best_grid = grid_search.best_estimator_\nevaluate(best_grid,x_test,y_test)","97576c62":"sub = best_random.predict(test)\nsub","cb25d70c":"submission['Choice'] = sub","9b23a77f":"submission.to_csv('submission2.csv',index=False)","922f6313":"best parameter from gridseachCV","de05c116":"shape of train and test data","d2fa366e":"Training parameter grid by RandomizedSearchCV","650e2e87":"reading train and test dataset","18e9b842":"Scaling is not required\n\nScaling is done to Normalize data so that priority is not given to a particular feature. Role of Scaling is mostly important in algorithms that are distance based and require Euclidean Distance.\n\nRandom Forest is a tree-based model and hence does not require feature scaling.\n\nThis algorithm requires partitioning, even if you apply Normalization then also> the result would be the same.","4ca55bab":"Best parameters","4bac7aa3":"hyperparameter tuning by GridSearchCV","8a2c21b6":"RandomTreeClassifier hyperparameter tuning by RandomizedSearchCV","daf4f3a3":"model accuracy","c1a7e3f6":"splitting data in training and test data","8d11b15b":"Randomforestclassifier with no hyperparameter tuning","9fb73622":"best model prediction on test data","78e06ab7":"checking whether dataset is balanced or not","2ca22b14":"submission file","b7d30c8f":"Performance of best parameter","055b59d8":"Number of null values in train and test data"}}