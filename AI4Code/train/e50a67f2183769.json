{"cell_type":{"4e0493f0":"code","694a9320":"code","e9811edf":"code","39a69cbf":"code","131d0082":"code","53f61a90":"code","904c0c1a":"code","11427e84":"code","f98bf2df":"code","4efc40cc":"code","ce610ce3":"code","d4ece4c2":"code","68ebf8fa":"code","da1be101":"code","1583d53a":"code","25ff120b":"code","473782c2":"code","b3bf6439":"code","38073fb8":"code","414a5ba7":"code","09dfbd14":"code","366228cc":"code","17ab6e0e":"code","db8b2e0c":"code","c913b11b":"code","47f6b2ad":"code","c10a304a":"code","a9bf8e67":"code","d7904e8d":"code","71179bf1":"code","eb56735d":"code","b4ee59be":"code","f9651090":"code","6ba621fe":"code","75611960":"code","4b7f3c77":"code","96fce2b4":"code","a17e1afd":"code","690fee97":"code","d2689951":"code","98c9597b":"code","fe2a2d2e":"code","c0d430b9":"code","d088ae5b":"code","ce8e847c":"code","f7663de6":"code","0e8c141f":"code","f49ad093":"code","433bac54":"code","eb70ffbc":"code","f541fc86":"code","76c20fd4":"code","c92e4cb6":"code","7594c147":"code","dee7546b":"code","12f1353a":"code","cc8a5692":"code","4966b3bc":"code","2ab64f86":"code","09c5e5e9":"code","da9ff5e7":"code","9ca79212":"code","12b820a3":"code","3df81fd0":"code","2ce18142":"code","cb29a93d":"code","e84a3ff4":"code","85edaa49":"code","ec8e63e7":"code","6e248d55":"code","8a62571e":"code","6ba4f044":"code","330f48c1":"code","6a447cdf":"code","f5a9cdda":"code","15b561ce":"code","b1f9a3d3":"code","66ae6e53":"code","6f834c49":"code","5f71d4b6":"code","8276179e":"code","49eae204":"code","325201a4":"code","9aefbdae":"code","a53b8c3e":"code","149b51d0":"code","5e3801f7":"code","f5b3ffa4":"code","31259f01":"code","177ee791":"code","9a6e26f7":"code","2e7f1da2":"code","99cae74f":"code","b89b43ac":"code","b4b51629":"code","4a0a61b8":"markdown","463978cb":"markdown","834a4c20":"markdown","748e0144":"markdown","da97f838":"markdown","cc9e2051":"markdown","a93cb3f4":"markdown","e8863d45":"markdown","8fe9035e":"markdown","a31c8f74":"markdown","f252fa9e":"markdown","3c40395d":"markdown","69371204":"markdown","530988e0":"markdown","b418ca44":"markdown","875ac737":"markdown","03560aeb":"markdown","1c7c7b7d":"markdown","4894e59e":"markdown","1c79d5b7":"markdown","92d28f9c":"markdown","8eefc5f6":"markdown","3cfae5ee":"markdown","99b08a01":"markdown","3ce29cc6":"markdown","d2c22074":"markdown","5afbf041":"markdown","6387ff0a":"markdown","5cef7dcb":"markdown","9a716659":"markdown","82744235":"markdown","66df3ae4":"markdown","136ecf4d":"markdown","0a892bde":"markdown","3f82b2ab":"markdown","7646399f":"markdown","aec25ee9":"markdown","e913a2bd":"markdown","e3517680":"markdown","4bf5a429":"markdown","660b6731":"markdown","c0ea4d0d":"markdown","7999d18e":"markdown","c8e0f125":"markdown","38aad6a5":"markdown","63490a57":"markdown","585501dc":"markdown","55521ea9":"markdown","58b89c6c":"markdown","eebf7e9b":"markdown","b8be2e62":"markdown","bad82ff8":"markdown","e0022a04":"markdown","90d88273":"markdown","5a9e4b51":"markdown","1158050e":"markdown","8437df7e":"markdown","8dd9db1c":"markdown","8a83eab8":"markdown","3ca51707":"markdown","c153af50":"markdown","9acb03e4":"markdown","7f10cb09":"markdown","e68d17ba":"markdown","3ead14b6":"markdown","1c3e2e76":"markdown"},"source":{"4e0493f0":"from datetime import datetime \nstart_time = datetime.now()","694a9320":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# machine learning models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# preprocessing functions and evaluation models\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import accuracy_score\nfrom IPython.core.pylabtools import figsize\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# console ascii color code, not necessarry to use\nimport sys\nGREEN = \"\\033[0;32m\"\nBLUE = \"\\033[0;34m\"","e9811edf":"sys.stdout.write(BLUE)\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","39a69cbf":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\" )\n\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","131d0082":"display(train.info())\ndisplay(test.info())","53f61a90":"print(\"train shape:\",train.shape)\nprint(\"test shape :\",test.shape)","904c0c1a":"train.head()","11427e84":"test.head()","f98bf2df":"train.describe()","4efc40cc":"def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","ce610ce3":"plt.figure(figsize=(4,4))\nplt.title('SURVIVED',size=20)\ntrain.Survived.value_counts().plot.bar(color=['red','green'])","d4ece4c2":"plt.figure(figsize=(4,4))\nplt.title('SEX',size=20)\ntrain.Sex.value_counts().plot.bar(color=['skyblue','pink'])","68ebf8fa":"percent=round(np.mean(train['Survived']),3)*100\nprint(\"Percentage of Survivors:\",percent)","da1be101":"bar_chart('Sex')","1583d53a":"total=train['Survived'].sum()\ntotal\nmen=train[train['Sex']=='male']\nwomen=train[train['Sex']=='female']\nm=men['Sex'].count()\nw=women['Sex'].count()\nprint(\"male:\",m)\nprint(\"female:\",w)\nprint(\"percentage of women:\",round(w\/(m+w)*100))\nprint(\"percentage of men:\",round(m\/(m+w)*100))","25ff120b":"train.isnull().sum()","473782c2":"train['Cabin'] = train['Cabin'].fillna('X')\ntest['Cabin']=test['Cabin'].fillna('X')","b3bf6439":"train['Age'].hist(bins=40,color='salmon')\nplt.title(\"AGE\",size=20)\n","38073fb8":"plt.figure(figsize=(5,5))\nplt.title(\"CLASS DIVISION\",size=20)\ntrain.Pclass.value_counts().plot.bar(color=['olive','coral','gold'])","414a5ba7":"train['Fare'].hist(bins = 80, color = 'orange')\nplt.title(\"FARE\",size=20)","09dfbd14":"plt.figure(figsize=(5,5))\nplt.title(\"Embarked\",size=20)\ntrain.Embarked.value_counts().plot.bar(color=['olive','coral','gold'])","366228cc":"sns.heatmap(train.corr(), annot = True)","17ab6e0e":"plt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Sex', data = train)\nplt.title(\"SURVIVED AND SEX\",size=20)","db8b2e0c":"plt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Pclass', data = train)\nplt.title(\"SURVIVED AND PCLASS\",size=20)","c913b11b":"plt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Embarked', data = train)\nplt.title(\"SURVIVED AND EMBARKED\",size=20)","47f6b2ad":"age_group = train.groupby(\"Pclass\")[\"Age\"]\nprint(age_group.median())","c10a304a":"age_group = train.groupby(\"Embarked\")[\"Age\"]\nprint(age_group.median())","a9bf8e67":"train.loc[train.Age.isnull(),'Age']=train.groupby(\"Pclass\").Age.transform('median')\ntest.loc[test.Age.isnull(),'Age']=test.groupby(\"Pclass\").Age.transform('median')\nprint(train['Age'].isnull().sum())","d7904e8d":"test['Cabin'].unique().tolist()","71179bf1":"cab = test.groupby(\"Cabin\")[\"Age\"]\nprint(cab.median())","eb56735d":"\ntrain['Cabin'].unique().tolist()","b4ee59be":"\nimport re\n\ntest['Cabin'] = test['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ntest['Cabin'].unique().tolist()\n","f9651090":"category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'X':8}\ntest['Cabin'] = test['Cabin'].map(category)\ntest['Cabin'].unique().tolist()","6ba621fe":"cab = train.groupby(\"Cabin\")[\"Age\"]\nprint(cab.median())","75611960":"train['Cabin'] = train['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ntrain['Cabin'].unique().tolist()","4b7f3c77":"category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'X':8, 'T':9}\ntrain['Cabin'] = train['Cabin'].map(category)\ntrain['Cabin'].unique().tolist()","96fce2b4":"print(train.isnull().sum())","a17e1afd":"from statistics import mode\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(mode(train[\"Embarked\"]))","690fee97":"print(train.isnull().sum())","d2689951":"train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\ntrain[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n\ntest[\"Sex\"][test[\"Sex\"] == \"male\"] = 0\ntest[\"Sex\"][test[\"Sex\"] == \"female\"] = 1\n\ntrain[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\ntrain[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\ntrain[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2\n\ntest[\"Embarked\"][test[\"Embarked\"] == \"S\"] = 0\ntest[\"Embarked\"][test[\"Embarked\"] == \"C\"] = 1\ntest[\"Embarked\"][test[\"Embarked\"] == \"Q\"] = 2\n","98c9597b":"train['fam']=train['SibSp']+train['Parch']+1\ntest['fam']=test['SibSp']+test['Parch']+1","fe2a2d2e":"train['Age']=train['Age'].astype(str)\ntest['Age']=test['Age'].astype(str)","c0d430b9":"import re\ntrain['Age'] = train['Age'].map(lambda x: re.compile(\"[0-9]\").search(x).group())\ntrain['Age'].unique().tolist()","d088ae5b":"cat={'2':1, '3':2 , '5':3, '1':4 ,'4':5,'8':6,'6':7,'7':8,'0':9,'9':10}\ntrain['Age']=train['Age'].map(cat)\ntrain['Age'].unique().tolist()","ce8e847c":"test['Age'] = test['Age'].map(lambda x: re.compile(\"[0-9]\").search(x).group())\ntest['Age'].unique().tolist()","f7663de6":"cat={'2':1, '3':2 , '5':3, '1':4 ,'4':5,'8':6,'6':7,'7':8,'0':9,'9':10}\ntest['Age']=test['Age'].map(cat)\ntest['Age'].unique().tolist()","0e8c141f":"train['Title'] = train['Name'].map(lambda x: re.compile(\"([A-Za-z]+)\\.\").search(x).group())\ntest['Title'] = test['Name'].map(lambda x: re.compile(\"([A-Za-z]+)\\.\").search(x).group())\nprint(train['Title'].unique())\n    ","f49ad093":"print(test['Title'].unique())","433bac54":"\n    train['Title'] = train['Title'].replace(['Lady.', 'Capt.', 'Col.',\n    'Don.', 'Dr.', 'Major.', 'Rev.', 'Jonkheer.', 'Dona.'], 'Rare.')\n    \n    train['Title'] = train['Title'].replace(['Countess.', 'Lady', 'Sir'], 'Royal')\n    train['Title'] = train['Title'].replace('Mlle.', 'Miss.')\n    train['Title'] = train['Title'].replace('Ms.', 'Miss.')\n    train['Title'] = train['Title'].replace('Mme.', 'Mrs.')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","eb70ffbc":"test['Title'] = test['Title'].replace(['Lady.', 'Capt.', 'Col.',\n'Don.', 'Dr.', 'Major.', 'Rev.', 'Jonkheer.', 'Dona.'], 'Rare.')\n\ntest['Title'] = test['Title'].replace(['Countess.', 'Lady.', 'Sir.'], 'Royal.')\ntest['Title'] = test['Title'].replace('Mlle.', 'Miss')\ntest['Title'] = test['Title'].replace('Ms.', 'Miss.')\ntest['Title'] = test['Title'].replace('Mme.', 'Mrs.')","f541fc86":"title_mapping = {\"Mr.\": 1, \"Miss.\": 2, \"Mrs.\": 3, \"Master.\": 4, \"Royal.\": 5, \"Rare.\": 6}\n\ntrain['Title'] = train['Title'].map(title_mapping)\ntrain['Title'] = train['Title'].fillna(0)\n\ntest['Title'] = test['Title'].map(title_mapping)\ntest['Title'] = test['Title'].fillna(0)\n\ntrain.head()","76c20fd4":"bar_chart('Title')","c92e4cb6":"bar_chart('Age')","7594c147":"bar_chart('Cabin')","dee7546b":"bar_chart('Sex')","12f1353a":"bar_chart('Embarked')","cc8a5692":"bar_chart('fam')","4966b3bc":"\ntest = test.drop(['Ticket'], axis = 1)\ntest = test.drop(['Name'], axis = 1)\ntest = test.drop(['Parch'], axis = 1)\ntest = test.drop(['Fare','SibSp'], axis = 1)\n\ntrain.drop(['Name', 'Ticket'], axis = 1, inplace = True)\ntrain = train.drop(['Parch'], axis = 1)\ntrain = train.drop(['Fare','SibSp'], axis = 1)\n","2ab64f86":"train.head()","09c5e5e9":"'''\ncombine = [train]\nfor dataset in combine:\n    dataset['Cabin*fam'] = dataset.Cabin * dataset.fam\n    \ndel train['Cabin']\ndel train['fam']\n    \ncombine2 = [test]\nfor dataset in combine2:\n    dataset['Cabin*fam'] = dataset.Cabin * dataset.fam\n\n#train.loc[:, ['Cabin*fam', 'Cabin', 'fam']].head(10)\n\ndel test['Cabin']\ndel test['fam']\n'''","da9ff5e7":"train.head()","9ca79212":"train.info()","12b820a3":"train['Sex'] = train['Sex'].astype(int)\ntrain['Embarked'] = train['Embarked'].astype(int)\ntrain['Title'] = train['Title'].astype(int)\n\ntest['Sex'] = test['Sex'].astype(int)\ntest['Embarked'] = test['Embarked'].astype(int)\ntest['Title'] = test['Title'].astype(int)","3df81fd0":"def outlier_function(df, col_name):\n    ''' this function detects first and third quartile and interquartile range for a given column of a dataframe\n    then calculates upper and lower limits to determine outliers conservatively\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(3*IQR)\n    lower_limit = first_quartile-(3*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","2ce18142":"# loop through all columns to see if there are any outliers\nfor column in train.columns:\n    if outlier_function(train, column)[2] > 0:\n        print(\"There are {} outliers in {}\".format(outlier_function(train, column)[2], column))","cb29a93d":"'''\ntrain = train[(train['fam'] > outlier_function(train, 'fam')[0]) &\n              (train['fam'] < outlier_function(train, 'fam')[1])]\n\n'''\ntrain.shape","e84a3ff4":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived','PassengerId'], axis=1), \n                                                    train['Survived'], test_size = 0.2, \n                                                    random_state = 0)","85edaa49":"# Create dummy classifer\ndummy = DummyClassifier(strategy='stratified', random_state=1)\n\n# train the model\ndummy.fit(X_train, y_train)\n\n# Get accuracy score\nbaseline_accuracy = dummy.score(X_test, y_test)\nprint(\"Our dummy algorithm classified {:0.2f} of the of the train correctly\".format(baseline_accuracy))","ec8e63e7":"# create scaler\nscaler = StandardScaler()\n\n# apply normalization to training set and transform training set\nX_train_scaled = scaler.fit_transform(X_train, y_train)\n\n# transform validation set\nX_valid_scaled = scaler.transform(X_test)","6e248d55":"# function to train a given model, generate predictions, and return accuracy score\ndef fit_evaluate_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    y_predicted = model.predict(X_test)\n    return accuracy_score(y_test, y_predicted)","8a62571e":"# create model apply fit_evaluate_model\nknn_classifier = KNeighborsClassifier()\nknn_accuracy = fit_evaluate_model(knn_classifier, X_train_scaled, y_train, X_valid_scaled, y_test)\nprint(\"Number of correct predictions made out of all predictions are:\", knn_accuracy)","6ba4f044":"# create model apply fit_evaluate_model\nlgbm_classifier = LGBMClassifier()\nlgbm_accuracy = fit_evaluate_model(lgbm_classifier, X_train_scaled, y_train, X_valid_scaled, y_test)\nprint(\"Number of correct predictions made out of all predictions are:\", lgbm_accuracy)","330f48c1":"# create model apply fit_evaluate_model\n\nrf_classifier = RandomForestClassifier()\nrf_accuracy = fit_evaluate_model(rf_classifier, X_train, y_train, X_test, y_test)\nprint(\"Number of correct predictions made out of all predictions are:\", rf_accuracy)","6a447cdf":"# create model apply fit_evaluate_model\nxrf_classifier = ExtraTreesClassifier()\nxrf_accuracy = fit_evaluate_model(xrf_classifier, X_train, y_train, X_test, y_test)\nprint(\"Number of correct predictions made out of all predictions are:\", xrf_accuracy)","f5a9cdda":"# create model apply fit_evaluate_model\nxgb_classifier = XGBClassifier()\nxgb_accuracy = fit_evaluate_model(xgb_classifier, X_train, y_train, X_test, y_test)\nprint(\"Number of correct predictions made out of all predictions are:\", xgb_accuracy)","15b561ce":"# create model apply fit_evaluate_model\ncatboost_classifier = CatBoostClassifier()\ncatboost_accuracy = fit_evaluate_model(catboost_classifier, X_train, y_train, X_test, y_test)\nprint(\"Number of correct predictions made out of all predictions are:\", catboost_accuracy)","b1f9a3d3":"# create dataframe of accuracy and model and sort values\nperformance_comparison = pd.DataFrame({\"Model\": [\"K-Nearest Neighbor\", \"LightGBM\", \"Random Forests\", \n                                                    \"Extra Trees\", \"XGBoost\", \"CatBoost\"],\n                                       \"Accuracy\": [knn_accuracy, lgbm_accuracy, rf_accuracy, xrf_accuracy, \n                                                    xgb_accuracy, catboost_accuracy]})\n\nprint(\"---Reuglar Accuracy Scores---\")\nperformance_comparison.sort_values(by='Accuracy', ascending=False)","66ae6e53":"plt.figure(figsize=(14,6))\nfig = plt.bar(performance_comparison['Model'], performance_comparison['Accuracy'],color='aqua')\nplt.grid()\nplt.show()","6f834c49":"# The number of trees in the forest algorithm, default value is 10.\nn_estimators = [50, 100, 300, 500, 1000]\n\n# The minimum number of samples required to split an internal node, default value is 2.\nmin_samples_split = [2, 3, 5, 7, 9]\n\n# The minimum number of samples required to be at a leaf node, default value is 1.\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# The number of features to consider when looking for the best split, default value is auto.\nmax_features = ['auto', 'sqrt', 'log2', None] \n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'n_estimators': n_estimators,\n                       'min_samples_leaf': min_samples_leaf,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}","5f71d4b6":"# create model\nbest_model = ExtraTreesClassifier(random_state=42)\n\n# create Randomized search object\nrandom_cv = RandomizedSearchCV(estimator=best_model,\n                               param_distributions=hyperparameter_grid,\n                               cv=5, n_iter=20, \n                               scoring = 'accuracy',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True, \n                               random_state=42)","8276179e":"# Fit on the all training data using random search object\nrandom_cv.fit(X_train, y_train)","49eae204":"random_cv.best_estimator_","325201a4":"xrf_classifier_w_random_search = ExtraTreesClassifier(n_estimators=300,\n                                                max_features= 'log2', \n                                                min_samples_leaf=4, \n                                                min_samples_split=7,\n                                                random_state=42)\n\nxrf_accuracy_opt_w_rand_search = fit_evaluate_model(xrf_classifier_w_random_search, X_train, \n                                                    y_train, X_test, y_test)","9aefbdae":"print(\"Accuracy score in the previous extra random forests model:\", xrf_accuracy)\nprint(\"Accuracy score after hyperparameter tuning:\", xrf_accuracy_opt_w_rand_search)","a53b8c3e":"# Create a range of trees to evaluate\n# 100, 200, 300, 400, 500, 600, 700\n# 50, 100, 150, 200, 250, 300, 350\ntrees_grid = {'n_estimators': [300, 500, 700, 900, 1200, 1500]}\n\n# define all parameters except n_estimators\nxrf_classifier_w_grid_search = ExtraTreesClassifier(n_estimators=300,\n                                                    max_features= 'log2', \n                                                min_samples_leaf=4, \n                                                min_samples_split=7,\n                                                random_state=42)\n\n# Grid Search Object using the trees range, the model and 5-fold cross validation\ngrid_search = GridSearchCV(estimator = xrf_classifier_w_grid_search, param_grid=trees_grid, \n                           cv = 5, scoring = 'accuracy', verbose = 1,\n                           n_jobs = -1, return_train_score = True)","149b51d0":"# fit the dataset to grid search object\ngrid_search.fit(X_train, y_train)","5e3801f7":"# Get the results into a dataframe\nxrf_results = pd.DataFrame(grid_search.cv_results_)\n\n# Plot the training and testing error vs number of trees\nplt.figure(figsize=(5,5))\nplt.plot(xrf_results['param_n_estimators'], xrf_results['mean_test_score'], label = 'Testing Accuracy')\nplt.plot(xrf_results['param_n_estimators'], xrf_results['mean_train_score'], label = 'Training Accuracy')\n\n# set title, labels and legend\nplt.xlabel('Number of Estimators(Trees)'); plt.ylabel('Accuracy'); plt.legend();\nplt.title('Performance vs Number of Trees', size=10);","f5b3ffa4":"xrf_results[[\"param_n_estimators\", \"params\", \"mean_test_score\"]].sort_values(\n                            by=\"mean_test_score\", ascending=False)","31259f01":"xrf_optimal_model = ExtraTreesClassifier(n_estimators=700,\n                                    max_features= 'log2', \n                                                min_samples_leaf=4, \n                                                min_samples_split=7,\n                                                random_state=42)\n\nxrf_optimal_model_accuracy = fit_evaluate_model(xrf_optimal_model, X_train, y_train, X_test, y_test)","177ee791":"print(\"Accuracy score with random forests model when n_estimators=300:\", xrf_accuracy_opt_w_rand_search)\nprint(\"Accuracy score with random forests model when n_estimators=100:\", xrf_optimal_model_accuracy)","9a6e26f7":"test.head()","2e7f1da2":"# Feature Importance\ndef feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp","99cae74f":"# Plot the feature importance scores\nfeature_importance(xrf_optimal_model, X_train)","b89b43ac":"ids = test['PassengerId']\npredictions = xrf_optimal_model.predict(test.drop('PassengerId', axis=1))\n\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)\noutput.head()","b4b51629":"print('Time elapsed (hh:mm:ss.ms) {}'.format(datetime.now() - start_time))","4a0a61b8":"* **Sex and Embarked should be changed into int, float or boolean. Otherwise we can't execute our data into the boosting algorithm.","463978cb":"**Checking out the distribution of Fares**","834a4c20":"**AGE and CABIN have the higest number of Null Values,so they will not be of major help since most of the values are missing,especially CABIN.\nBut lets see the Maximum age groups present.**","748e0144":"**train_test_split :Split arrays or matrices into random train and test subsets**","da97f838":"# Outlier Function","cc9e2051":"# 7. Draw conclusions and document work","a93cb3f4":"**Lets check out the missing values again**","e8863d45":"**Now, I expect that following ML models will beat the accuracy score of 0.14! Otherwise, the model is underfitted.**","8fe9035e":"## Extra Tree","a31c8f74":"**Lets examine the types of classes that were present**","f252fa9e":"**Visualizing the data in our dataframe into a correlation heatmap **","3c40395d":"**Calculating median values of \"Age\" by using \"Pclass\" and \"Embarked\" to fill up the missing values.**","69371204":"**First we start by checking the counts of survived(1) and dead(0).\nthus from the below graph it is clear that there were more deaths than the ratio of survivors.\nWe also plot of graph for the division of genders, to see the ratio between men and women.\nwhen the graph i spotted we see that the range of women seem more prone to the range of survivors and the range of deaths seem more closely related to the range of men.\nSo thus that mean that there were more women who survived?\nWe shall se that further.**","530988e0":"* **Lets assign X value to all the NAN values, we're gonna categorized null value as 'X' and insert the median value of the 'age' column, as we can see from the graph that it's left skewed. **\n* **Median can normalize the column in that case**","b418ca44":"## LGBM","875ac737":"**Voila!!So no more missing values in our dataset**","03560aeb":"# Random Search","1c7c7b7d":"* **Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values. **\n* **Evidently we can't see the data about name, sex, abin, and embarked as they contain string value. **\n* **However, we can see several values of age is missing.**","4894e59e":"# 2. DATA CLEANING & FORMATTING","1c79d5b7":"Every data is important. As 204 outliers is not more than 60% and our data is not that much, I'm gonna retain them.","92d28f9c":"**Mapping values**","8eefc5f6":"**Lets create a new column of fam using SibSp which means number of Siblings or Spouse and Parch which means number of Parents or Children,later we will be dropping SibSp and Parch from our data set since these values are alreday being used in Fam**","3cfae5ee":"**Now only \"Embarked\" has two missing values in it.**","99b08a01":"**So now we have filled the NAN values of embarked too.Lets check the null values again!**","3ce29cc6":"**The Data that we are dropping from the dataset**","d2c22074":"**Lets check whether the conversion has worked or not**","5afbf041":"** I'm creating a function so that I can easily tweak the parameters.**","6387ff0a":"**Mean survival rate according to the Titles assigned**","5cef7dcb":"## Random Forest","9a716659":"# Steps\n\n<center>The notebook will follow the workflow suggested by Will Koehrsen in this [article](https:\/\/towardsdatascience.com\/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420), which is given below with few changes.<\/center>\n\n\n<center>1. Exploratory data analysis<br>\n2. Data cleaning and formatting<br>\n3. Feature engineering and selection<br>\n4. Compare several machine learning models on a performance metric<br>\n5. Perform hyperparameter tuning on the best model<br>\n6. Interpret the model results<br>\n7. Draw conclusions and document work<\/center>","82744235":"# 6. Interpret the model results","66df3ae4":"**Now we are assigning values to the initials that we had found in the above step and replace them with integers by mapping them.\nSame step will be repeated for train and test data**","136ecf4d":"# 5. Perform hyperparameter tuning on the best model","0a892bde":"* **I've juxtaposed train's title with test's title to show them in ascending order. The differences are subtle. Hence the logic behind categorizing them remains the same.**","3f82b2ab":"# 4. COMPARE SEVERAL MACHINE LEARNING MODELS ON A PERFORMANCE METRIC","7646399f":"* **Get the execution time of the whole notebook**","aec25ee9":"**Since we have explored all the features in our dataset,now we shall draw close comparisons with \"SURVIVED\" feature,to help us draw some inference.**","e913a2bd":"**Now we will do the same thing that we did with cabin so that we are left with the initials and can assign them numeric values accordingly.**","e3517680":"# Grid Search","4bf5a429":"**Searching for the titles and extracting them from the names in the given data**","660b6731":"* **We should know the size of the data we are working with.**","c0ea4d0d":"* **The following function just compares train columns with 'Survived' column**","7999d18e":"# 3. FEATURE ENGINEERING & SELECTION","c8e0f125":"# CatBoost","38aad6a5":"## XGB","63490a57":"**Lets find out the Survival Rate**","585501dc":"# BaseLine Matrix","55521ea9":"**First we are converting float to string for both the datasets namely test and train**","58b89c6c":"**Lets check for the number of Null Values in our DATA SET**","eebf7e9b":"** Normalization might be benificial for our model","b8be2e62":"**We will be searching for the initials of the cabin numbers like A,B,C,etc**","bad82ff8":"# 1. EXPLORATORY DATA ANALYSIS","e0022a04":"* **Lets see the 'sex' and 'survived' data combinedly**","90d88273":"# <center><font color='orange'>Please check & upvote if you find this notebook useful.<\/font><\/center>","5a9e4b51":"**Lets convert our categorical data to numeric form**","1158050e":"**Checking out Embarked Attribute.\n  It has 3 discrete Divisions,namely S , C ,Q.**","8437df7e":"# Import Packages","8dd9db1c":"* ** I've seen that extra trees classifier gives me the best performance after using hyperparameter which was got from using both random and grid search.**","8a83eab8":"## KNN","3ca51707":"**Lets play around with Name as well!**","c153af50":"# Train Test Split","9acb03e4":"**Lets work out with the Cabin numbers**","7f10cb09":"**Now we have no missing values for AGE**","e68d17ba":"**Lets play a little with Age as well**","3ead14b6":"![](https:\/\/skalle.dk\/wordpress\/wp-content\/uploads\/2016\/03\/titanic_iceberg_ahead.jpg)\n","1c3e2e76":"* ** From the visualisation given below, we can say \"SEX\" column influenced the most to have the prediction."}}