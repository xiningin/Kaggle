{"cell_type":{"95f3929f":"code","50d72a8c":"code","7c123292":"code","cbb399b6":"code","9101c8eb":"code","d296730e":"code","11281c4c":"code","7e68be6d":"code","84297be6":"code","aed9fbaa":"code","edbeaec1":"code","2fd83185":"code","d7cf858f":"markdown","8d25a352":"markdown","f765e894":"markdown","4f5f1b3f":"markdown","012cab3c":"markdown","687bc32b":"markdown","5471b6f0":"markdown","88e5faa7":"markdown","2c7f186a":"markdown","2898545f":"markdown","40058cf2":"markdown","7203aa6f":"markdown","f0e62798":"markdown","3fdd0ea4":"markdown","f6da1cf3":"markdown","9ed26f38":"markdown"},"source":{"95f3929f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","50d72a8c":"train = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')","7c123292":"def data_info(data):        \n    info = pd.DataFrame()\n    info['var'] = data.columns\n    info['# missing'] = list(data.isnull().sum())\n    info['% missing'] = info['# missing'] \/ data.shape[0]\n    info['types'] = list(data.dtypes)\n    info['unique values'] = list(len(data[var].unique()) for var in data.columns)\n    \n    return info","cbb399b6":"data_info(train)","9101c8eb":"x_train = train.drop(['Id', 'Cover_Type'], axis=1)\ny_train = train['Cover_Type']","d296730e":"from sklearn.model_selection import KFold\ncv_kfold = KFold(5, shuffle = False, random_state=12) ","11281c4c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\n\ndef grid_search(clf, X_train, y_train, params, score, cv):    \n    grid = GridSearchCV(clf, params, scoring = score, cv = cv, return_train_score=True)\n    grid_fitted = grid.fit(X_train, np.ravel(y_train))\n    print (\"Best score: %.4f\" % grid_fitted.best_score_)\n    print (\"Best parameters: %s\" % grid_fitted.best_params_)\n    return grid_fitted, grid_fitted.best_estimator_, grid_fitted.cv_results_","7e68be6d":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nparams = {\n    'n_estimators':[128, 200, 256],\n    'criterion' : ['gini'],\n    'max_depth': [3, 5, 7, None],\n    'max_features': ['sqrt']\n}\nclf = RandomForestClassifier()\ngrid, model, results = grid_search(clf, x_train, y_train, params, 'accuracy', cv_kfold)\nmodel","84297be6":"%%time\nmodel.fit(x_train, y_train)","aed9fbaa":"%%time\nx_test = test.drop('Id', axis=1)\ny_pred = model.predict(x_test)","edbeaec1":"test['Cover_Type'] = y_pred\ntest[['Id', 'Cover_Type']].to_csv('submission.csv', index=False)","2fd83185":"test.Cover_Type.value_counts()","d7cf858f":"Just curious to know how many of each class our model predicted :)","8d25a352":"All of our features are numeric and there are no null values, in which case our challenge is much simpler and we can go straight to the model (this is just for this tutorial, in the real world we still have to do a lot of work before we get to the models).\n\nFirst we separate our variables from our target:","f765e894":"... and we make an amazing submission!","4f5f1b3f":"First we import some libraries and we have some idea where the files are going to be used (thanks Kaggle!). Remembering that Numpy and Pandas are two important tools for working with data and the more familiar we are with it the simpler it will be to do what we came here to do.","012cab3c":"That's all folks, I hope this quick introduction can be helpful to you. If you have any questions please comment and we will all try to learn together!","687bc32b":"To make the selection of our model we will use cross validation, for this we first divide our data into k parts, here called folds, of the same size. After that, we trained a model in k-1 parts and validated on the data piece that was not used in training.","5471b6f0":"After the libraries we bring the data to our environment.","88e5faa7":"... we make our prediction...","2c7f186a":"We define which parameters we want to try for our Random Forest. Remember that this is just an example and there are many other parameters that can be optimized and a multitude of different values that can be passed as an option for our search.","2898545f":"Hello everyone. The idea of this notebook is to show how to do a gridsearch to find the best hyperparameters for our model. Making it clear that it can be any model, in this tutorial we use Random Forest because it is a simple but powerful model and because it makes sense to use a Forest to try to predict a forest. ;)","40058cf2":"After that we train our model with all the dataset we have available ...","7203aa6f":"# Grid Seach: Quick Intro","f0e62798":"## Our data","3fdd0ea4":"Here begins the magic. We define a function that will find the best hyperparameters for our model using grid search. To use this function we have six important parameters:\n* clf: the model we want to optimize, in this case Random Forest\n* X_train: our features\n* y_train: our target\n* params: (things get interesting here) params is a dictionary where keys are Random Forest parameters and values are various options for this parameter.\n* score: our evaluation metric, in this case accuracy\n* cv: cross validation, strategy to validate the model\n","f6da1cf3":"data_info is a very simple function defined to help us get an overview of our dataset. We have information about the data type, missings and amount of unique values. Only from this analysis can we get some very useful information that can help us select some features, avoid making noise in our model and even have some idea of new features to create ...\n","9ed26f38":"## Our model"}}