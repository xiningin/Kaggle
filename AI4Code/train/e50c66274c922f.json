{"cell_type":{"8d066f59":"code","7ed460dd":"code","13d75897":"code","4d54b82c":"code","210987e9":"code","40e6e478":"code","8c29f64d":"code","4c4d2946":"code","ae2aed40":"code","8cf70c16":"code","3a38fea3":"code","f9c4e827":"code","c903b7dc":"code","8edadd6a":"code","bfbc26b0":"code","444492e2":"code","216ee0d1":"code","0c5129bf":"code","f3d50805":"code","62e529b5":"code","e3acba37":"code","c0a29876":"code","4bc4e689":"code","9d7e6da3":"code","e878ec3d":"code","a4e37210":"code","4439fbf7":"code","72f65637":"code","a7352d22":"code","f648e7a3":"code","ec334b05":"markdown","068d8ccc":"markdown","2aa3d3f8":"markdown","2935a946":"markdown","48d97b2c":"markdown","2c6264d8":"markdown","9063596d":"markdown","c2495a6d":"markdown","198a0f71":"markdown","2e17eac3":"markdown","0fb1e226":"markdown","ddf84346":"markdown","0a3e45cd":"markdown","564123be":"markdown","4f9a86eb":"markdown","55fbfe7f":"markdown","6db0af2a":"markdown","77690c0d":"markdown","d12249df":"markdown","40d2a87a":"markdown","8faa2db0":"markdown","5f7a4f30":"markdown","5a0bb423":"markdown","84504345":"markdown","6b4f108e":"markdown","bdffe9da":"markdown"},"source":{"8d066f59":"#importing libraries\nimport pandas as pd\nimport seaborn as sns\nimport string \nimport re\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nnltk.download('stopwords')\nnltk.download('wordnet')","7ed460dd":"#reading the dataset \n#dataset: https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-data\nmsg=pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin-1')\nmsg.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\nmsg.rename(columns={'v1':'label','v2':'text'},inplace=True)\nmsg.head()","13d75897":"#mapping ham=0 and spam=1\nfor i in msg.index:\n  if msg['label'][i]=='ham':\n    msg['label'][i]=0\n  else:\n    msg['label'][i]=1\nmsg.head()","4d54b82c":"#category count plot (count of spam and ham)\nsns.countplot(msg.label)","210987e9":"#data description grouped by labels \nmsg.groupby('label').describe()","40e6e478":"#dropping duplicate rows\nmsg=msg.drop_duplicates()\nmsg.groupby('label').describe()","8c29f64d":"#adding length column to the dataset \nmsg['length']=msg['text'].apply(len)\nmsg.head()","4c4d2946":"msg[msg.label==0].describe()","ae2aed40":"sns.distplot(a=msg[msg['label']==0].length,kde=False)","8cf70c16":"msg[msg.label==1].describe()","3a38fea3":"sns.distplot(a=msg[msg['label']==1].length,kde=False)","f9c4e827":"#examining spam texts\nfor i in range(50):\n  if msg['label'][i]==1:\n    print(msg['text'][i])","c903b7dc":"msg['contain']=msg['text'].str.contains('\u00a3').map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains('%').map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains('\u20ac').map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains('\\$').map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"T&C\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"www|WWW\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"http|HTTP\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"https|HTTPS\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"@\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"email|Email|EMAIL\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"SMS|sms|FREEPHONE\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{11}\",regex=True).map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{10}\",regex=True).map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{5}\",regex=True).map({False:0,True:1})\n\nmsg.head()","8edadd6a":"sns.distplot(a=msg[msg['label']==0].contain,kde=False)","bfbc26b0":"sns.distplot(a=msg[msg['label']==1].contain,kde=False)","444492e2":"#data cleaning\/preprocessing - removing punctuation and digits \nmsg['cleaned_text']=\"\"\n\nfor i in msg.index:\n  updated_list=[]\n  for j in range(len(msg['text'][i])):\n    if msg['text'][i][j] not in string.punctuation:\n      if msg['text'][i][j].isdigit()==False:\n        updated_list.append(msg['text'][i][j])\n  updated_string=\"\".join(updated_list)\n  msg['cleaned_text'][i]=updated_string\n\nmsg.drop(['text'],axis=1,inplace=True)\nmsg.head() ","216ee0d1":"#data cleaning\/preprocessing - tokenization and convert to lower case \nmsg['token']=\"\"\n\nfor i in msg.index:\n  msg['token'][i]=re.split(\"\\W+\",msg['cleaned_text'][i].lower())\n\nmsg.head()","0c5129bf":"#data cleaning\/preprocessing - stopwords\nmsg['updated_token']=\"\"\nstopwords=nltk.corpus.stopwords.words('english')\n\nfor i in msg.index:\n  updated_list=[]\n  for j in range(len(msg['token'][i])):\n    if msg['token'][i][j] not in stopwords:\n      updated_list.append(msg['token'][i][j])\n  msg['updated_token'][i]=updated_list\n\nmsg.drop(['token'],axis=1,inplace=True)\nmsg.head()","f3d50805":"#data cleaning\/preprocessing - lemmatization \nmsg['lem_text']=\"\"\nwordlem=nltk.WordNetLemmatizer()\n\nfor i in msg.index:\n  updated_list=[]\n  for j in range(len(msg['updated_token'][i])):\n    updated_list.append(wordlem.lemmatize(msg['updated_token'][i][j]))\n  msg['lem_text'][i]=updated_list \n\nmsg.drop(['updated_token'],axis=1,inplace=True)\nmsg.head()","62e529b5":"#data cleaning\/preprocessing - merging token\nmsg['final_text']=\"\"\n\nfor i in msg.index:\n  updated_string=\" \".join(msg['lem_text'][i])\n  msg['final_text'][i]=updated_string\n\nmsg.drop(['cleaned_text','lem_text'],axis=1,inplace=True)\nmsg.head()","e3acba37":"#separating target and features\ny=pd.DataFrame(msg.label)\nx=msg.drop(['label'],axis=1)","c0a29876":"#splitting the data (80:20 ratio)\nx_train,x_val,y_train,y_val=train_test_split(x,y,train_size=0.8,test_size=0.2,random_state=0)","4bc4e689":"#count vectorization \ncv=CountVectorizer(max_features=5000)\ntemp_train=cv.fit_transform(x_train['final_text']).toarray()\ntemp_val=cv.transform(x_val['final_text']).toarray()","9d7e6da3":"#tfidf\ntf=TfidfTransformer()\ntemp_train=tf.fit_transform(temp_train)\ntemp_val=tf.transform(temp_val)","e878ec3d":"#merging temp datafram with original dataframe\ntemp_train=pd.DataFrame(temp_train.toarray(),index=x_train.index)\ntemp_val=pd.DataFrame(temp_val.toarray(),index=x_val.index)\nx_train=pd.concat([x_train,temp_train],axis=1,sort=False)\nx_val=pd.concat([x_val,temp_val],axis=1,sort=False)\n\nx_train.head()","a4e37210":"#dropping the final_text column\nx_train.drop(['final_text'],axis=1,inplace=True)\nx_val.drop(['final_text'],axis=1,inplace=True)\n\nx_train.head()","4439fbf7":"#converting the labels to int datatype (for model training)\ny_train=y_train.astype(int)\ny_val=y_val.astype(int)","72f65637":"#Multinomial Naive Bayes\nmodel=MultinomialNB()\nmodel.fit(x_train,y_train)\ny_preds=model.predict(x_val)\nprint(\"Multinomial Naive Bayes:\",accuracy_score(y_val,y_preds))","a7352d22":"#Decision Tree\nmodel=DecisionTreeClassifier(random_state=0)\nmodel.fit(x_train,y_train)\ny_preds=model.predict(x_val)\nprint(\"Decision Tree:\",accuracy_score(y_val,y_preds))","f648e7a3":"#Random Forest\nmodel=RandomForestClassifier(n_estimators=100,random_state=0)\nmodel.fit(x_train,y_train)\ny_preds=model.predict(x_val)\nprint(\"Random Forest:\",accuracy_score(y_val,y_preds))","ec334b05":"### Mapping Labels\nMapping ham to 0 and spam to 1","068d8ccc":"# 4. Data Preprocessing\n\nThe goal of Text Preprocessing is to convert the text in a form that is easy to process and analyze. \n\nIt helps us get rid of unwanted data & noise by removing punctuations\/digits\/stopwords, converting to lower case, etc.","2aa3d3f8":"# 3. Exploratory Data Analysis (EDA)","2935a946":"### Count Vectorization\nIt involves counting the number of occurrences of each word\/token in a given text.\n\nMore on Count Vectorization: https:\/\/www.educative.io\/edpresso\/countvectorizer-in-python","48d97b2c":"### Tokenizing & converting to lower case \nUsing re.split() to split text into words(tokens) and using .lower() to convert them into lower case.","2c6264d8":"### Lemmatization \nLemmatization is the process in which different forms of a word are converted to its root word.\nFor example,\neating->eat, \nran->run, \nruns->run, \nbooks->book\n\nMore on Lemmatization: https:\/\/www.geeksforgeeks.org\/python-lemmatization-with-nltk\/","9063596d":"We have 4852 ham messages (4516 unique) and 747 spam messages (653 unique)","c2495a6d":"# Spam Detection with NLTK (98.1% accuracy)","198a0f71":"### Removing punctuation & digits\nUsing inbuilt functions string.punctuation and.isdigit() to check for punctuations and digits and remove them.","2e17eac3":"### Merging Tokens\nMerging tokens to form the final text string.","0fb1e226":"### TFIDF\nIt tells us how important a word is to a text in a group of text. It is calculated by multiplying the frequency of a word, and the inverse document frequency (how common a word is, calculated by log(number of text\/number of text which contains the word)) of the word across a group of text.\n\nMore on TFIDF: https:\/\/monkeylearn.com\/blog\/what-is-tf-idf\/","ddf84346":"# Overview\n1. Importing Libraries\n2. Reading the Dataset\n3. Exploratory Data Analysis (EDA)\n     - Mapping Labels\n     - Dropping Duplicates\n     - Adding \"length\" column\n     - Adding \"contain\" column\n4. Data Preprocessing\n     - Removing Punctuations & Digits\n     - Tokenization & Lower Case\n     - Removing Stopwords\n     - Lemmatization \n     - Merging Tokens\n     - Count Vectorization\n     - TFIDF\n5. Model Training\n    - Multinomial Naive Bayes \n    - Decision Trees\n    - Random Forest","0a3e45cd":"The above graphs confirm our observation that spam texts have a high occurrence of numbers, emails, links, and symbols as compared to ham texts.","564123be":"### Dropping Duplicates","4f9a86eb":"# 1. Importing Libraries","55fbfe7f":"### Decision Tree","6db0af2a":"### Random Forest","77690c0d":"From the above outputs and graphs, we notice that\n* Most of the ham messages have length<100 (mean around 70)\n* Most of the spam messages have a length of 150 (mean around 132)\n\nSo, we have discovered that spam messages generally have more characters than ham messages.","d12249df":"### Adding \"length\" column","40d2a87a":"### Multinomial Naive Bayes","8faa2db0":"We observe that spam texts are more likely to contain numbers (charges, phone numbers), emails, links, and symbols!\n\nLet us add a column named \"contain\" denoting whether a text contains numbers, emails, links, or symbols!","5f7a4f30":"### Removing Stopwords\nStopwords refer to the most commonly used words in a language. For English, some of stopwords are \"on\",\"in\",\"a\",\"the\".\n\nMore on stopwords: https:\/\/www.tutorialspoint.com\/python_text_processing\/python_remove_stopwords.htm","5a0bb423":"# 5. Model Training","84504345":"# 2. Reading the Dataset\nDataset has 3 empty columns (Unnamed: 2, Unnamed 3, Unnamed: 4}. Dropping those columns.\n\nRenaming v1 and v2 columns as 'label' and 'text' respectively.","6b4f108e":"Let's separate the targets & features, and then let's split them into training and validation set.","bdffe9da":"### Adding \"contain\" column\nLet us examine the spam messages and see if we can find any trends."}}