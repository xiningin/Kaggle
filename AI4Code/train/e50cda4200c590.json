{"cell_type":{"b52dbcf3":"code","230f4086":"code","bde5222a":"code","32091156":"code","52020047":"code","079c84a4":"code","0c04bbf0":"code","82013880":"code","99840223":"code","e84609dc":"code","e45359d3":"code","ed21c840":"code","26003841":"code","b82a36ed":"code","ac90ce69":"code","e1399f18":"markdown","5138fcad":"markdown","6d501d35":"markdown","b4b042de":"markdown","16294bf2":"markdown","24badebe":"markdown","dcabbcb4":"markdown","2046346e":"markdown","18e208a2":"markdown","0fc39941":"markdown"},"source":{"b52dbcf3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ntrain_df = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")","230f4086":"train_df = train_df[train_df[\"language\"] == \"English\"]\ntest_df = test_df[test_df[\"language\"] == \"English\"]","bde5222a":"import tensorflow as tf\nimport transformers","32091156":"max_length = 128  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2\n\n# \"bert-base-uncased\", do_lower_case=True\n#\u00a0\"bert-base-multilingual-uncased\", do_lower_case=True\n#\u00a0\"bert-base-multilingual-cased\"\n# \"roberta-base\"\n# \"jplu\/tf-xlm-roberta-base\"\n# \"albert-base-v2\"\n#\u00a0\"roberta-large\"\n# \"roberta-large-mnli\"\n\n# \"microsoft\/deberta-base\" (ISSUE: only pytorch)\n\n# Labels in our dataset.\nlabels = [\"contradiction\", \"entailment\", \"neutral\"]","52020047":"class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n    \"\"\"Generates batches of data.\n\n    Args:\n        sentence_pairs: Array of premise and hypothesis input sentences.\n        labels: Array of labels.\n        batch_size: Integer batch size.\n        shuffle: boolean, whether to shuffle the data.\n        include_targets: boolean, whether to incude the labels.\n\n    Returns:\n        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n        (or just `[input_ids, attention_mask, `token_type_ids]`\n         if `include_targets=False`)\n    \"\"\"\n\n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            padding='max_length',\n            truncation='longest_first',\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n        # Convert batch of encoded features to numpy array.\n        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        # Set to true if data generator is used for training\/validation.\n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)\n","079c84a4":"#\u00a0Use GPU (doesn't work on TPU)\n# Create the model under a distribution strategy scope.\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    bert_model = transformers.TFAutoModel.from_pretrained(\"bert-base-uncased\")\n    # Freeze the BERT model to reuse the pretrained features without modifying them.\n    bert_model.trainable = False\n    \n    bert_output = bert_model(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n    #sequence_output = bert_output.last_hidden_state\n    #pooled_output = bert_output.pooler_output\n    \n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    #bi_lstm = tf.keras.layers.Bidirectional(\n    #    tf.keras.layers.LSTM(64, return_sequences=True)\n    #)(sequence_output)\n    # Applying hybrid pooling approach to bi_lstm sequence output.\n    #avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    #max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    #concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    #dropout = tf.keras.layers.Dropout(0.3)(concat)\n    #batch_norm = tf.keras.layers.BatchNormalization()(dropout)\n    \n    #features = tf.keras.layers.GlobalAveragePooling1D()(bert_output[0])\n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(bert_output[0][:, 0, :])\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )\n\n\nprint(f\"Strategy: {strategy}\")\nmodel.summary()\n\n# WARNING: This is expected, and tells you that you won't have good performance with your \n#\u00a0BertForSequenceClassification model before you fine-tune it.","0c04bbf0":"y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\ntrain_data = BertSemanticDataGenerator(\n    train_df[[\"premise\", \"hypothesis\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)","82013880":"history = model.fit(\n    train_data,\n    epochs=epochs,\n    verbose=1, \n    use_multiprocessing=True,\n    workers=-1,\n) ","99840223":"# Unfreeze the bert_model.\nbert_model.trainable = True\n# Recompile the model to make the change effective.\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()","e84609dc":"history = model.fit(\n    train_data,\n    epochs=epochs,\n    verbose=1, \n    use_multiprocessing=True,\n    workers=-1,\n) ","e45359d3":"\ndef check_similarity(sentence1, sentence2):\n    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n    test_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\n    proba = model.predict(test_data)[0]\n    idx = np.argmax(proba)\n    proba = f\"{proba[idx]: .2f}%\"\n    pred = labels[idx]\n    return pred, proba\n","ed21c840":"sentence1 = \"Two women are observing something together.\"\nsentence2 = \"Two women are standing with their eyes closed.\"\ncheck_similarity(sentence1, sentence2)","26003841":"sentence1 = \"A smiling costumed woman is holding an umbrella\"\nsentence2 = \"A happy woman in a fairy costume holds an umbrella\"\ncheck_similarity(sentence1, sentence2)","b82a36ed":"sentence1 = \"A soccer game with multiple males playing\"\nsentence2 = \"Some men are playing a sport\"\ncheck_similarity(sentence1, sentence2)","ac90ce69":"test_data = BertSemanticDataGenerator(\n    test_df[[\"premise\", \"hypothesis\"]].values.astype(\"str\"), \n    labels=None, \n    batch_size=1, \n    shuffle=False, \n    include_targets=False,\n)\npredictions = [np.argmax(i) for i in model.predict(test_data)]\nsubmission = test_df.id.copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.to_csv(\"submission.csv\", index = False)","e1399f18":"## Train the Model\n\nTraining is done only for the top layers to perform \"feature extraction\",\nwhich will allow the model to use the representations of the pretrained model.","5138fcad":"## Inference on custom sentences","6d501d35":"## Setup\n\nNote: install HuggingFace `transformers` via `pip install transformers` (version >= 2.11.0).","b4b042de":"Check results on some example sentence pairs.","16294bf2":"## Fine-tuning\n\nThis step must only be performed after the feature extraction model has\nbeen trained to convergence on the new data.\n\nThis is an optional last step where `bert_model` is unfreezed and retrained\nwith a very low learning rate. This can deliver meaningful improvement by\nincrementally adapting the pretrained features to the new data.","24badebe":"## Build the model.","dcabbcb4":"# Train the entire model end-to-end.","2046346e":"## Configuration","18e208a2":"## Introduction\n\nSemantic Similarity is the task of determining how similar\ntwo sentences are, in terms of what they mean.\nThis example demonstrates the use of SNLI (Stanford Natural Language Inference) Corpus\nto predict sentence semantic similarity with Transformers.\nWe will fine-tune a BERT model that takes two sentences as inputs\nand that outputs a similarity score for these two sentences.\n\n### References\n\n* [BERT](https:\/\/arxiv.org\/pdf\/1810.04805.pdf)\n* [SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/)","0fc39941":"# Results"}}