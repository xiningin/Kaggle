{"cell_type":{"3b187f09":"code","86239ecf":"code","99b90ffb":"code","5c1ee3e2":"code","3a22b0ca":"code","b98ed4f7":"code","c4c8765e":"code","5ebb8829":"code","5a96b69b":"code","b7b82260":"code","5b8ea131":"code","063b33cf":"code","d02805cc":"code","4904b6e9":"code","942e4237":"code","d773058c":"code","0e6022b5":"code","c8ffb026":"code","9141e62c":"code","52b1ee86":"code","478eab41":"code","a93b70a1":"code","e5c126c3":"code","e7f3a3e0":"code","656862eb":"code","a4741cf3":"code","b48377b5":"code","fceaebd1":"code","471c19fa":"code","6b49876b":"code","56a211f5":"code","ac51513e":"code","39c0e12b":"code","29436338":"code","6edc6500":"code","cca0632e":"code","5ccb0a2a":"code","ca300006":"code","f2aed4f6":"code","055fde71":"code","d50ed021":"code","7eb05d6b":"code","118cf0de":"code","6f1ef50f":"code","173b5461":"code","4a997dac":"code","b152f233":"code","294e24a7":"code","4b406098":"code","c1111be4":"code","922be874":"code","f912c048":"code","7283bbab":"code","4b0ae79a":"code","675f10b9":"code","cc49db14":"code","a87838e5":"code","abd9cafa":"code","e02e7776":"code","a6c2cf2f":"code","93322502":"code","3950ca03":"code","90d363ab":"markdown","7ac57692":"markdown","e75f0dbe":"markdown","498aaeb5":"markdown","033d8f31":"markdown","a08aa8a2":"markdown","ea32abdf":"markdown","8d87dc06":"markdown","c8fd0c9b":"markdown","87e7c539":"markdown","14df38dd":"markdown","58ac65cc":"markdown","aead2437":"markdown","45465797":"markdown","d7fe26b1":"markdown","188e8bf2":"markdown","3b3d1067":"markdown","3f4c9860":"markdown","fc9c433f":"markdown","2c155d26":"markdown","80558bc7":"markdown","cb14b129":"markdown","e3c9788e":"markdown","09b2e4c2":"markdown","fa037d39":"markdown","496c37e9":"markdown","73b5e1c3":"markdown","26cab43f":"markdown","26336921":"markdown","9b2a6b12":"markdown","6d8dfddb":"markdown","438434ef":"markdown","6cbaf45b":"markdown","7a16b712":"markdown","63056cdc":"markdown","0fde873e":"markdown","43766847":"markdown","fca22861":"markdown","ca597ce5":"markdown","7ca66ea7":"markdown","8de59d2e":"markdown","2f407457":"markdown","b37fabd0":"markdown","43dc9f03":"markdown","eea93db4":"markdown","e158d1e8":"markdown","fe62955c":"markdown","e89865fc":"markdown","359b63e8":"markdown","314c6991":"markdown","dbea9083":"markdown","09329463":"markdown","9fb3f106":"markdown","3a62d9b3":"markdown"},"source":{"3b187f09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","86239ecf":"data = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ncolumn_3C_weka = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv\")","99b90ffb":"data.head()","5c1ee3e2":"data.info()","3a22b0ca":"data.describe()","b98ed4f7":"data.loc[:,'class']  # data['class'] ayni sonucu verir","c4c8765e":"data.loc[:,data.columns !=  'class' ].head()","5ebb8829":"color_list = ['red' if i == 'Abnormal' else 'green' for i in data['class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                           c = color_list,\n                           figsize = [15,15],\n                           diagonal = 'hist',\n                           alpha=0.5,s = 200,\n                           marker = '@',\n                           edgecolor= \"orange\")\nplt.show()","5a96b69b":"data['class'].value_counts()","b7b82260":"sns.countplot('class', data=data)\nplt.show()","5b8ea131":"data.columns","063b33cf":"data[['pelvic_incidence','sacral_slope']]","d02805cc":"data1 = data[data['class'] == 'Abnormal']\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\nplt.figure(figsize=[6,6])\nplt.scatter(x=x,y=y)\nplt.xlabel('elvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","4904b6e9":"# import sklearn libary\nfrom sklearn.linear_model import LinearRegression\n\n# Linear Reggression model\nreg = LinearRegression()\n\n# Predict space   \npredict_space = np.linspace(min(x),max(x)).reshape(-1,1)    # X values\n\n# Fit Line\nreg.fit(x,y)\n\npredicted = reg.predict(predict_space)\n\n# R^2\nprint('R^2 score: ',reg.score(x, y))\n\n# Plot regression line and scatter\nplt.plot(predict_space,predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","942e4237":"reg.predict([[ 1126.14792141]])","d773058c":"avocado = pd.read_csv(\"..\/input\/avocado-prices\/avocado.csv\")","0e6022b5":"avocado.head()","c8ffb026":"avocado.info()","9141e62c":"avocado.corr(method ='kendall')","52b1ee86":"avocado.drop(\"Unnamed: 0\", axis = 1, inplace=True)","478eab41":"# Visualize with seaborn library\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(avocado.corr(method ='kendall'), annot=True, linewidths=.5, fmt= '.1f',cmap=\"YlGnBu\", ax=ax, cbar_kws={\"orientation\": \"vertical\"})\nplt.show()","a93b70a1":"# Years vs Average Price\nbdata = avocado[[\"year\",\"AveragePrice\"]].groupby([\"year\"],as_index =False).mean().sort_values(by=\"year\", ascending = True)\nplt.figure(figsize=(9,3))\nplt.bar(bdata[\"year\"].values, bdata[\"AveragePrice\"].values)\nplt.xticks(bdata[\"year\"].values)\nplt.title(\"AveragePrice per Years\")\nplt.show()","e5c126c3":"# %% linear regression\nx = avocado.loc[:,\"Total Volume\"].values.reshape(-1,1)\ny = avocado.loc[:,\"AveragePrice\"].values.reshape(-1,1)\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x,y)\n\n#predict\npredict_space = np.linspace(min(x),max(x)).reshape(-1,1)\npredicted = lr.predict(predict_space)\ny_head = lr.predict(x)\n\n\nplt.scatter(x=x,y=y_head)\nplt.plot(predict_space ,predicted ,color=\"red\",label =\"linear\")\nplt.xlabel('Total Volume')\nplt.ylabel('AveragePrice')\nplt.show()","e7f3a3e0":"# Predicts\nprint(lr.predict([[1000]]))","656862eb":"# x = Total Volume, y = AveragePrice\n\n## Polynomial Linear Regression with 2nd degree\n# y =b0 + b1.x + b2.x^2\nx = avocado.loc[:,\"Total Volume\"].values.reshape(-1,1)\ny = avocado.loc[:,\"AveragePrice\"].values.reshape(-1,1)\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 2)\nx_polynomial = polynomial_regression.fit_transform(x)\n\n# Line fit\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_polynomial,y)\n\n# Visualization\ny_head = linear_regression2.predict(x_polynomial)\n\nplt.plot(x,y_head,color= \"green\",label = \"Polynomial\")\nplt.scatter(x=x,y=y)\nplt.xlabel('Total Volume')\nplt.ylabel('AveragePrice') \nplt.legend()\nplt.show()","a4741cf3":"multidata = avocado[['Total Volume','4046','4225','4770','AveragePrice']]","b48377b5":"multidata.head()","fceaebd1":"y = multidata[\"AveragePrice\"].values.reshape(-1,1)\nx = multidata.iloc[:,[0,1,2,3]]    # x are \"Total Volume\",\"4046\", \"4225\", \"4770\"\n\nmultiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\ny_head = multiple_linear_regression.predict(x)\nplt.plot(x,y_head)\nplt.legend()\nplt.xlabel('Total Volume, 4046, 4225, 4770')\nplt.ylabel('AveragePrice')\nplt.show()\n\nprint(\"b0: \", multiple_linear_regression.intercept_)\nprint(\"b1,b2, b2, b3 : \",multiple_linear_regression.coef_)","471c19fa":"multiple_linear_regression.predict([[6.423662, 1.036740, 5.445485, 4.81600]])","6b49876b":"x = multidata.iloc[:,[0]].values.reshape(-1,1)\ny = multidata.iloc[:,[4]].values.ravel()\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Decision Tree Regression\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\n# Predict\ny_ = tree_reg.predict(x)\n\n# Visualization\nplt.scatter(x,y,color=\"red\")\nplt.plot(x,y_,color=\"black\")\nplt.xlabel(\"Total Avocado Volume\")\nplt.ylabel(\"Avarage Price\")\nplt.show()\n\n","56a211f5":"tree_reg.predict([[14.3]])","ac51513e":"from sklearn.metrics import r2_score\nprint(\"r_score: \", r2_score(y,y_))","39c0e12b":"x = multidata.iloc[:,0].values.reshape(-1,1)\ny = multidata.iloc[:,4].values.ravel()\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 100, random_state = 42) \n\n# n_estimators = number of tree, how many tree we we are going to use.\nrf.fit(x,y)\ny_head = rf.predict(x)\n\n#Predict\nrf.predict([[1154876.98]])\n\n# Visualize\nplt.scatter(x,y,color = \"red\")\nplt.plot(x,y_head,color = \"blue\")\nplt.xlabel(\"Total Avocado Volume\")\nplt.ylabel(\"Avarage Price\")\nplt.show()\n","29436338":"\n# Random Forest Algoristmasi r-score hesaplama\n\nfrom sklearn.metrics import r2_score\nprint(\"r_score: \", r2_score(y,y_head))","6edc6500":"x_l = np.load('\/kaggle\/input\/sign-language-digits-dataset\/X.npy')\nY_l = np.load('\/kaggle\/input\/sign-language-digits-dataset\/Y.npy')\n# 0 - 204 are the numbers equal to 3\n# 1236 - 1442 are the numbers equal to 4\nplt.imshow(x_l[1237])\nplt.show()\n","cca0632e":"three = []\nfor i in range(205):\n    three.append(3)\nthree = np.array(three)\nfour = []\nfor i in range(205):\n    four.append(4)\nfour = np.array(four)","5ccb0a2a":"X = np.concatenate((x_l[0:205], x_l[1236:1441] ), axis=0) # from 0 to 204 is three sign and from 205 to 410 is four sign\nY = np.concatenate((three,four), axis=0).reshape(-1,1)\nprint(\" X shape {}, Y shape {}\".format(X.shape,Y.shape))","ca300006":"# Then lets create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)","f2aed4f6":"print(\"number_of_train : {} and number_of_test : {}\".format(X_train.shape[0],X_test.shape[0]))\nprint(\"X_train.shape : {}\".format(X_train.shape))\nprint(\"X_test.shape  : {}\".format(X_test.shape))\nprint(\"Y_train.shape : {}\".format(Y_train.shape))\nprint(\"Y_test.shape  : {}\".format(Y_test.shape))","055fde71":"X_train_flatten = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","d50ed021":"x_train = X_train_flatten\nx_test = X_test_flatten\ny_train = Y_train\ny_test = Y_test\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","7eb05d6b":"import warnings\nwarnings.filterwarnings(\"ignore\")\ntest_accuracy = []\ntrain_accuracy = []\ntest_accuracy_decent = []\nindex_decent =[]\nindex = []\nfrom sklearn import linear_model\nfor i in range(150):\n    logreg = linear_model.LogisticRegression(random_state = 42,max_iter= i )\n    test_accuracy.append(logreg.fit(x_train, y_train).score(x_test, y_test))\n    train_accuracy.append(logreg.fit(x_train, y_train).score(x_train, y_train))\n    index.append(i)\n    if i % 10 == 0:\n        test_accuracy_decent.append(logreg.fit(x_train, y_train).score(x_test, y_test))\n        index_decent.append(i)\n\n\n\nplt.plot(index_decent,test_accuracy_decent)\nplt.xticks(index_decent)\nplt.xlabel(\"Number of Iterarion\")\nplt.ylabel(\"Test Accuracy\")\nplt.show()","118cf0de":"print(\"Test Accurcy is : {}\".format(max(test_accuracy_decent)))","6f1ef50f":"data = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","173b5461":"data.head()","4a997dac":"# We have 210 Abnormal and 100 Normal samples in the dataset.\ndata[\"class\"].value_counts()","b152f233":"A = data[data[\"class\"] == \"Abnormal\"]\nN = data[data[\"class\"] == \"Normal\"]\n# scatter plot\nplt.scatter(A.pelvic_incidence,A[\"degree_spondylolisthesis\"],color=\"red\",label=\"Abnormal\",alpha= 0.3)\nplt.scatter(N.pelvic_incidence,N[\"degree_spondylolisthesis\"],color=\"green\",label=\"Normal\",alpha= 0.3)\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"degree_spondylolisthesis\")\nplt.legend()\nplt.show()","294e24a7":"data[\"class\"] = [1 if each == \"Abnormal\" else 0 for each in data[\"class\"]]","4b406098":"data.head()","c1111be4":"y = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)","922be874":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","f912c048":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)","7283bbab":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} knn score: {} \".format(3,knn.score(x_test,y_test)))","4b0ae79a":"score_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","675f10b9":"data.head()","cc49db14":"# SVM Classification\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))","a87838e5":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))","abd9cafa":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"score: \", dt.score(x_test,y_test))","e02e7776":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))","a6c2cf2f":"score_rf = []\nfor each in range(1,100):\n    rf2 = RandomForestClassifier(n_estimators = each,random_state = 1)\n    rf2.fit(x_train,y_train)\n    score_rf.append(rf2.score(x_test,y_test))\n    \nplt.plot(range(1,100),score_rf)\nplt.xlabel(\"estimators\")\nplt.ylabel(\"accuracy\")\nplt.show()","93322502":"print('Random Forest max Accucancy is {}'.format(max(score_rf)))","3950ca03":"for i in range(len(score_rf)):\n    if pd.DataFrame(score_rf).values[i] == max(pd.DataFrame(score_rf).values):\n        print('Max Accuracy Estimater', i)","90d363ab":"* How KNN Algoritm works is very simple. Lets try to find the class of element C in the picture.\n* If you select K=3, you will have 2 O and 1 A which are the three nearest neighbour of element C.\n* So we decide the class of C is O.","7ac57692":"<a is = \"8\"><\/a><br>\n# Naive Bayes Classification","e75f0dbe":"## Loading Dataset\n* We are going to use sign language dataset.\n* In this dataset lines between 0 - 204 shows 3 and lines between 1236 to 1442 shows number 4.","498aaeb5":"<a is = \"7\"><\/a><br>\n# Support Vector Machine (SVM) Classification\nSupervised learning: It uses data that has labels. Example, there are orthopedic patients data that have labels normal and abnormal.","033d8f31":"### Evoluation of Random Forest Regression","a08aa8a2":"## Polynomial Linear Regression\n* y =b0 + b1.x + b2.x^2 + ...+bn.x^n  # single x feature","ea32abdf":"# Contents\n1. [Biomechanical features of orthopedic patients](#1)\n2. [Avocado Prices Datasets](#2)\n3. [Evoluation Regression Models](#3)\n4. [Cheat-Sheet](#4)\n5. [Logistic Regression](#5)\n6. [K-Nearest Neighbour (KNN) Classification](#6)\n7. [Support Vector Mechine (SVM) Classification](#7)\n8. [Naive Bayes Classification](#8)\n9. [Decision Tree Classification](#9)\n10. [Random Forest Classification](#10)\n\n","8d87dc06":"<a is = \"4\"><\/a><br>\n## Cheat-Sheat\n#### Below three commands given as example and returns the same output and can be used to perepare your data.\n* data1.loc[:,'pelvic_incidence'].values\n* np.array(data1.loc[:,'pelvic_incidence'])\n* data1[\"pelvic_incidence\"].values\n\n#### You can select which columns to include in your data. Below will shows the columns 0,2 and 3.\n* x = multidata.iloc[:,[0,2,3]]","c8fd0c9b":"* In the dataset, sign three is between indexes 0 and 204. Number of three signs are 205.\n* Also sign four is between indexes 1236 and 1442. Number of four signs are 206. Therefore, we will use 205 samples from each classes(labels).\n* In Y dataset there will be 3 for the three signs and 4 for the four signs.","87e7c539":"### Evoluation of Decision Tree","14df38dd":"<a is = \"6\"><\/a><br>\n# K-Nearest Neighbour (KNN) Classification","58ac65cc":"<a is = \"9\"><\/a><br>\n# Decision Tree Classification","aead2437":"### Biomechanical features of orthopedic patients datase\n\n* We are going to use Biomechanical features of orthopedic patients dataset to predict with KNN Algorithm.\n* Class is the target variable and this target variable contains two types of strings which are \"Normal\" and \"Abnormal\".","45465797":"### Linear Regression\n* y = b0 + b1.x\n* b0 is the y value when x = 0 and b1 is the angle of the line.\n* Mean Square Error should be minumum (MSE)\n* residual = y-y_head\n* MSE = sum(residual^2)\/n","d7fe26b1":"## Loss (Error) Function\n\n![image.png](attachment:image.png)\n\n* Forward propagation steps:\n* find z = w.T*x+b\n* y_head = sigmoid(z)\n* loss(error) = loss(y,y_head)\n* cost = sum(loss)\n","188e8bf2":"There are features predictor and target variable. Features are like pelvic_incidence, pelvic_tilt numeric, lumbar_lordosis_angle etc are predictor and class feature is target variable.","3b3d1067":"## Linear Regression\n* linear regression =  y = b0 + b1*x\n* multiple linear regression   y = b0 + b1*x1 + b2*x2","3f4c9860":"### EXPLORATORY DATA ANALYSIS (EDA)","fc9c433f":"* Biomechanical features of orthopedic patients datase\n* We are going to use Biomechanical features of orthopedic patients dataset to predict with SVM.\n* Class is the target variable and this target variable contains two types of strings which are \"Normal\" and \"Abnormal\".\n* We already have the dataset from KNN study and normalization and Train-Test Split is already done.","2c155d26":"# Random Forest Regression","80558bc7":"#### Step 3: If we do a normalization on x_data, we will not have any rule out x feature. It is wise to do a normalization.","cb14b129":"# Decision Tree Regression\n* CART : Clasification and Regression Tree","e3c9788e":"# Logistic Regression with Sklearn","09b2e4c2":"<a is = \"1\"><\/a><br>\n# Biomechanical features of orthopedic patients ","fa037d39":"<a is = \"2\"><\/a><br>\n# Avocado Prices Dataset","496c37e9":"* z = b + px1w1 + px2w2 + ... + px4096*w4096\n* y_head = sigmoid(z)","73b5e1c3":"* Logistic regression is actually a very simple neural network.\n* logistic regression comes to mind first when we have binary output like 0 or 1.","26cab43f":"## Multiple Linear Regression\n* y = b0 + b1*x1 + b2*x2  # multiple x feature","26336921":"* \"Unnamed:0\" column is not going to be used. So we are going to drop this column as below.\n* inplace= True parameter is used for writing this change into Dataset directly.","9b2a6b12":"* Now we have changed our 3D X train and test sets to 2D and now there are 4096 pixels in the column and 348 image in row.","6d8dfddb":"* We have 410 images with three and four signs.\n* 64 means picture size is 64 x 64 pixels\n* Y 410 means we have 410 three and four\n* Lets split X and Y into train and test sets. test = 15% and train = 75%\n","438434ef":"There are relations between each feature. How many normal(green) and abnormal(red) classes are there?\n\nSearborn library has countplot() that counts number of classes. #Also you can print it with value_counts() method\n\ndata.class.value_counts()","6cbaf45b":"#### EXPLORATORY DATA ANALYSIS (EDA)\n* We will analys Avocado Prioces datasets","7a16b712":"#### Step 5: KNN Model","63056cdc":"<a is = \"10\"><\/a><br>\n# Random Forest Classification","0fde873e":"#### Step 6: Find K value","43766847":"## Forward Propagation\n\n![image.png](attachment:image.png)","fca22861":"## Regression","ca597ce5":"## Computition Graph of a Logistic Regression\n\n\n![image.png](attachment:image.png)","7ca66ea7":"#### Step 4: Train-Test Split","8de59d2e":"## Visualization","2f407457":"In python there are some ML libraries like sklearn, keras or tensorflow. We will use sklearn.","b37fabd0":"### scatter_matrix\n> We will create a scatterplot matrix of several different variables so that we can determine whether there are any relationships among the variables in the dataset.\n\n> * green : normal and red : abnormal\n> * c: color\n> * figsize: figure size\n> * diagonal : histogram of each features\n> * alpha : opacity of the image\n> * s : size of our marker\n> * marker : marker type","43dc9f03":"<a is = \"5\"><\/a><br>\n# Logistic Regression\n","eea93db4":"![image.png](attachment:image.png)","e158d1e8":"#### Step 2: After we have a numerical target feature we create our x and y sets.","fe62955c":"As seen in the graph test accurcy is stable after approximatelly 100 iterations and peak accurcy is around after 15 iterations apx.","e89865fc":"### Visualization of the dataset \n\n* Below graphics shows that when \"degree_spondylolisthesis\" feature is high than Abnormality is seen more often. \n* Almost all Normal sets has a very small \"degree_spondylolisthesis\" value.","359b63e8":"<a is = \"3\"><\/a><br>\n# Evoluation of Regression Models\n\n### Evoluation od Decision Tree\nr_score:  0.9987758391982751\n\n### Evoluation of Random Forest Regression\nr_score:  0.8720635845642947\n\nFor our dataset Decison Tree give more accurate accurancy score.\n\n","314c6991":"* Now we have 3 dimensional (3D) input array (X) so we need to make it flatten (2D) in order to use as input for our first deep learning model.\n* Our label array (Y) is already flatten(2D) so we leave it like that.\n* Lets flatten X array(images array).","dbea9083":"#### Step 1: We have a target value which is class in this dataset. This target feature should be an integer value or categorical. So we do replace Abnormal status with 1 and Normal state with 0.","09329463":"* Supervised learning\n* We will learn linear and logistic regressions\n* This orthopedic patients data is not proper for regression so I only use two features that are sacral_slope and pelvic_incidence of abnormal\n* I consider feature is pelvic_incidence and target is sacral_slope\n* Lets look at scatter plot to understand it better\n* reshape(-1,1): If you do not use it shape of x or y becaomes (210,) and we cannot use it in sklearn, so we use shape(-1,1) and shape of x or y be (210, 1).","9fb3f106":"Target variable is object. This means it is string.","3a62d9b3":"![image.png](attachment:image.png)"}}