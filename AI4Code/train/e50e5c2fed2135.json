{"cell_type":{"e56e3e02":"code","089dca8d":"code","a8ec46b9":"code","9fb515fb":"code","664137f8":"code","cc1c7fb4":"code","31152fc7":"code","2835aa72":"code","938b5c1d":"code","a089627c":"code","1e8872f9":"code","27f50fe3":"code","22a01f31":"code","18257de2":"code","ec307cad":"code","4ecb3f04":"code","35a44f67":"code","b0108fb9":"code","917e8c82":"code","05d92182":"code","d5ad16d4":"code","561fd261":"code","ab964e34":"code","fc7185a4":"code","f0f8c182":"code","3a2faf37":"code","3cbc9f55":"code","70b1a4c7":"code","f2267ca7":"code","c541c1f1":"code","ca50ec26":"code","c6b6d941":"code","dbffb8e5":"code","8ed8de04":"code","b51c3459":"code","aae27ed8":"code","eb10d303":"code","47059e57":"code","72c7ed1f":"markdown","6ecaabd7":"markdown","e523478b":"markdown","20626ca7":"markdown","b818ca63":"markdown","cc1bd4c4":"markdown","8ef85a1f":"markdown","41ad5cd1":"markdown"},"source":{"e56e3e02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","089dca8d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport nltk.classify.util\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.classify import NaiveBayesClassifier\nimport numpy as np\nimport re\nimport string\nimport nltk\n%matplotlib inline","a8ec46b9":"#Creating a data frame out of the csv file\ndf = pd.read_csv(r\"\/kaggle\/input\/consumer-reviews-of-amazon-products\/1429_1.csv\")\ndf.head()","9fb515fb":"#Some information about the dataset\ndf.info()","664137f8":"df[\"reviews.rating\"].value_counts().sort_values().plot.bar()","cc1c7fb4":"#df['reviews.text'] = df['reviews.text'].astype('str')\n#df['reviews.text'] = df['reviews.text'].str.replace('\/',' ')","31152fc7":"#df['reviews.text'] = df['reviews.text'].apply(lambda x: str(x))","2835aa72":"#Converting Emojis to their Respective Emotions\ndf[\"reviews.text\"] = df[\"reviews.text\"].replace([\"\\:\\)\", \"\\:\\-\\)\", \"\\:\\-\\}\", \"\\;\\-\\}\", \"\\:\\-\\>\", \"\\;\\-\\)\"], [\"Happy\",\"Happy\",\"Happy\",\"Happy\",\"Happy\",\"Happy\"], regex=True)\ndf[\"reviews.text\"] = df[\"reviews.text\"].replace([\"\\:\\-\\(\", \"\\:\\(\", \"\\:\\-\\|\", \"\\;\\-\\(\", \"\\;\\-\\<\", \"\\|\\-\\{\"], [\"Sad\", \"Sad\", \"Sad\", \"Sad\", \"Sad\", \"Sad\",], regex=True)\ndf[\"reviews.text\"] = df[\"reviews.text\"].replace([\"\\:\\D\", \"\\:\\'\\-\\)\", \"\\:\\`\\-\\(\"], [\"laugh\", \"tear of joy\", \"tear of sadness\"], regex=True)","938b5c1d":"#Removing all punctuations in the Reviews\nimport string\nexclude = set(string.punctuation)\nfinal = exclude\n\nfor i in final:\n  df['reviews.text'] = df['reviews.text'].str.replace(i, ' ', regex=True)","a089627c":"permanent = df[['reviews.rating', 'reviews.text', 'reviews.title', 'reviews.username']]\n#Find and print the number of null values\nprint(permanent.isnull().sum())\npermanent.head()","1e8872f9":"#Filtering the null values\ncheck = permanent[permanent['reviews.rating'].isnull()]\nprint(check[['reviews.text', 'reviews.rating']])","27f50fe3":"senti= permanent[permanent[\"reviews.rating\"].notnull()]\nprint(senti[['reviews.text', 'reviews.rating']])","22a01f31":"senti.count()","18257de2":"senti.info()","ec307cad":"from stop_words import get_stop_words\nfrom nltk.corpus import stopwords\n\nstop_words = list(get_stop_words('en'))         #About 900 stopwords\nnltk_words = list(stopwords.words('english')) #About 179 stopwords\nstop_words.extend(nltk_words)\n\nlen(stop_words)","4ecb3f04":"#Data Cleaning  \nfrom nltk.corpus import stopwords \nstop = set(stopwords.words('english')) \nfrom nltk.corpus import stopwords \ndef remove_stopword(word): \n    return word not in words \n \n#StopWords Removed  \nsenti['reviewsStopWords'] = senti['reviews.text'].str.lower().str.split()\n","35a44f67":"import nltk\nnltk.download('punkt')\n","b0108fb9":"senti.dtypes","917e8c82":"senti['reviews.text'] = df['reviews.text'].astype(str).str.replace('\/',' ')","05d92182":"senti[\"reviews.text\"] = senti[\"reviews.text\"].apply(nltk.word_tokenize)","d5ad16d4":"senti.head()","561fd261":"import pandas as pd\nfrom nltk.stem.snowball import SnowballStemmer\n\n# Use English stemmer.\nstemmer = SnowballStemmer(\"english\")\n","ab964e34":"senti['reviews.text'] = senti['reviews.text'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.","fc7185a4":"senti[\"senti\"] = senti[\"reviews.rating\"]>=4\nsenti[\"senti\"] = senti[\"senti\"].replace([True , False] , [1 , -1])","f0f8c182":"data_set = senti[[\"reviews.text\", \"senti\"]]\ndata_set.columns = [\"reviews\", \"score\"]\ndata_set.head()","3a2faf37":"data_set[\"reviews\"] = data_set[\"reviews\"].apply(' '.join)","3cbc9f55":"train = data_set\ntrain.columns = [\"reviews\", \"score\"]\ntrain.head()","70b1a4c7":"train.count()","f2267ca7":"!ls","c541c1f1":"!rm train.csv\n","ca50ec26":"!rm test.csv","c6b6d941":"train.to_csv('train.csv', index=False)","dbffb8e5":"# A nice python class that lets you count how many times items occur in a list\nfrom collections import Counter\nimport csv\nimport re\n\n# Read in the training data.\nwith open(\"train.csv\", 'r') as file:\n  reviews = list(csv.reader(file))\n\ndef get_text(reviews, score):\n  # Join together the text in the reviews for a particular tone.\n  # We lowercase to avoid \"Not\" and \"not\" being seen as different words, for example.\n  return \" \".join([r[0].lower() for r in reviews if r[1] == str(score)])\n\ndef count_text(text):\n  # Split text into words based on whitespace.  Simple but effective.\n  words = re.split(\"\\s+\", text)\n  # Count up the occurence of each word.\n  return Counter(words)\n\nnegative_text = get_text(reviews, -1)\npositive_text = get_text(reviews, 1)\n# Generate word counts for negative tone.\nnegative_counts = count_text(negative_text)\n# Generate word counts for positive tone.\npositive_counts = count_text(positive_text)\n\nprint(\"Negative text sample: {0}\".format(negative_text[:100]))\nprint(\"Positive text sample: {0}\".format(positive_text[:100]))\n\n","8ed8de04":"import re\nfrom collections import Counter\n\ndef get_y_count(score):\n  # Compute the count of each classification occuring in the data.\n  return len([r for r in reviews if r[1] == str(score)])\n\n# We need these counts to use for smoothing when computing the prediction.\npositive_review_count = get_y_count(1)\nnegative_review_count = get_y_count(-1)\n\n# These are the class probabilities (we saw them in the formula as P(y)).\nprob_positive = positive_review_count \/ len(reviews)\nprob_negative = negative_review_count \/ len(reviews)\n\ndef make_class_prediction(text, counts, class_prob, class_count):\n  prediction = 1\n  text_counts = Counter(re.split(\"\\s+\", text))\n  for word in text_counts:\n      # For every word in the text, we get the number of times that word occured in the reviews for a given class, add 1 to smooth the value, and divide by the total number of words in the class (plus the class_count to also smooth the denominator).\n      # Smoothing ensures that we don't multiply the prediction by 0 if the word didn't exist in the training data.\n      # We also smooth the denominator counts to keep things even.\n      prediction *=  text_counts.get(word) * ((counts.get(word, 0) + 1) \/ (sum(counts.values()) + class_count))\n  # Now we multiply by the probability of the class existing in the documents.\n  return prediction * class_prob\n\n# As you can see, we can now generate probabilities for which class a given review is part of.\n# The probabilities themselves aren't very useful -- we make our classification decision based on which value is greater.\nprint(\"Review: {0}\".format(reviews[3][0]))\nprint(\"Negative prediction: {0}\".format(make_class_prediction(reviews[3][0], negative_counts, prob_negative, negative_review_count)))\nprint(\"Positive prediction: {0}\".format(make_class_prediction(reviews[3][0], positive_counts, prob_positive, positive_review_count)))","b51c3459":"test = data_set.sample(3000)\ntest.to_csv('test.csv', index=False)","aae27ed8":"import csv\n\ndef make_decision(text, make_class_prediction, sentiment):\n    # Compute the negative and positive probabilities.\n    negative_prediction = make_class_prediction(text, negative_counts, prob_negative, negative_review_count)\n    positive_prediction = make_class_prediction(text, positive_counts, prob_positive, positive_review_count)\n    \n    print(\"==================\")\n    print(\"Review: \", text)\n    print(\"Sentiment: \", sentiment)\n    # We assign a classification based on which probability is greater.\n    if negative_prediction > positive_prediction:\n        print(\"Predicted: -1\")\n        return -1\n    else:\n        print(\"Predicted: 1\")\n        return 1\n\nwith open(\"test.csv\", 'r') as file:\n    test = list(csv.reader(file))\n\n\npredictions = [make_decision(r[0], make_class_prediction, r[1]) for r in test]","eb10d303":"actual = []\nfor r in test:\n    if r[1] == \"1\":\n        actual.append(1)\n    else:\n        actual.append(-1)\n\nfrom sklearn import metrics\n\n# Generate the roc curve using scikit-learn.\nfpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n\n# Measure the area under the curve.  The closer to 1, the \"better\" the predictions.\nprint(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))\n","47059e57":"review_text1 = input(\"Enter the review: \")\n\nimport nltk \nfrom nltk.stem.snowball import SnowballStemmer \n  \n#the stemmer requires a language parameter \nsnow_stemmer = SnowballStemmer(language='english') \n  \n#list of tokenized words \nwords = review_text1.split() \n  \n#stem's of each word \nstem_words = [] \nfor w in words: \n    x = snow_stemmer.stem(w) \n    stem_words.append(x)\n    \nreview_text = \"\"\nreview_text.join(stem_words)\n\nnegative_prediction = make_class_prediction(review_text, negative_counts, prob_negative, negative_review_count)\npositive_prediction = make_class_prediction(review_text, positive_counts, prob_positive, positive_review_count)\nif negative_prediction > positive_prediction:\n      print(\"\\n\\nNegetive Review\")\nelse:\n    print(\"\\n\\nPositive Review\")\n","72c7ed1f":"**Extrating requried columns from the data frame**","6ecaabd7":"**Plotting the graph of the rating distribution**","e523478b":"**Removing the punctuation**","20626ca7":"**Importing the required libraries**","b818ca63":"**Cleaning the data**","cc1bd4c4":"**Filtering**","8ef85a1f":"**Reading the csv file**","41ad5cd1":"**Converting the emojis to text**"}}