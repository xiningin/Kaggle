{"cell_type":{"4a42370f":"code","1c1df0d0":"code","ada92236":"code","bacfe0d5":"code","c00f1ecf":"code","a6704972":"code","6e314d29":"code","dde29b40":"code","1c166259":"code","88cb6690":"code","af57edb7":"code","7bdf5759":"code","77626cc1":"code","4ab4e0cb":"code","c1275c93":"code","14e5353b":"code","f3a151fe":"markdown","9bb16b8d":"markdown","f5f97657":"markdown","868a80fd":"markdown","47f59a29":"markdown","c7c8a444":"markdown","94d4e06d":"markdown","fce06802":"markdown"},"source":{"4a42370f":"!pip install larq\n!pip install larq-compute-engine","1c1df0d0":"!mkdir test train valid\n!cp -r ..\/input\/landuse-scene-classification\/images_train_test_val\/test\/airplane .\/test\/airplane\n!cp -r ..\/input\/landuse-scene-classification\/images_train_test_val\/test\/buildings .\/test\/buildings\n\n!cp -r ..\/input\/landuse-scene-classification\/images_train_test_val\/train\/airplane .\/train\/airplane\n!cp -r ..\/input\/landuse-scene-classification\/images_train_test_val\/train\/buildings .\/train\/buildings\n\n!cp -r ..\/input\/landuse-scene-classification\/images_train_test_val\/validation\/airplane .\/valid\/airplane\n!cp -r ..\/input\/landuse-scene-classification\/images_train_test_val\/validation\/buildings .\/valid\/buildings","ada92236":"from tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras.metrics import Recall, Precision\nfrom tqdm import tqdm\n\nimport tensorflow.keras.layers as layers\nimport larq_compute_engine as lce\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport larq as lq","bacfe0d5":"batch_size = 32\ntrain = image_dataset_from_directory('.\/train', batch_size=batch_size)\nvalid = image_dataset_from_directory('.\/valid', batch_size=batch_size)\ntest = image_dataset_from_directory('.\/test', batch_size=batch_size)","c00f1ecf":"iterations_count = 5","a6704972":"def create_model():\n    kwargs = dict(\n                strides=1,\n                padding='same',\n                activation='relu'\n                )\n    \n    model = tf.keras.Sequential([\n        layers.Normalization(axis=None,\n                            mean=0,\n                            variance=1,\n                            input_shape=(256,256,3)), \n\n        layers.Conv2D(\n            filters=8,\n            kernel_size=3,\n            **kwargs),\n        layers.MaxPooling2D(pool_size=2),\n        layers.BatchNormalization(momentum=0.9),\n\n        layers.Conv2D(\n            filters=16,\n            kernel_size=5,\n            **kwargs),\n        layers.MaxPooling2D(pool_size=2),\n        layers.BatchNormalization(momentum=0.9),\n\n        layers.Conv2D(\n            filters=32,\n            kernel_size=7,\n            **kwargs),\n        layers.MaxPooling2D(pool_size=2),\n        layers.BatchNormalization(momentum=0.9),\n\n        layers.Conv2D(\n            filters=64,\n            kernel_size=9,\n            **kwargs),\n        layers.MaxPooling2D(pool_size=2),\n        layers.BatchNormalization(momentum=0.9),\n\n        layers.Conv2D(\n            filters=128,\n            kernel_size=11,\n            **kwargs),\n        layers.MaxPooling2D(pool_size=2),\n        layers.BatchNormalization(momentum=0.9),\n\n\n        layers.GlobalAveragePooling2D(),    \n        layers.Dense(1024, activation='relu'),\n        layers.Dense(64, activation='relu'),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['binary_accuracy', Precision(), Recall()]    \n    )\n\n    return model","6e314d29":"def scheduler(epoch, lr):\n    if epoch < 80:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n    \nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)","dde29b40":"for i in tqdm(range(iterations_count)):\n    model = create_model()\n    history = model.fit(\n        train,\n        validation_data=valid,\n        batch_size=batch_size,\n        epochs=100,\n        shuffle=True,\n        callbacks=[lr_scheduler],\n        # verbose=0\n    )\n    df_history = pd.DataFrame(history.history)\n    df_history.loc[:, ['loss', 'val_loss']].plot(xlabel='Epochs', ylabel='Loss')\n    df_history.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(xlabel='Epochs', ylabel='Accuracy')\n    print(model.evaluate(test, batch_size=batch_size))\n    with open(f\"full_model{i}.tflite\", \"wb\") as flatbuffer_file:\n        flatbuffer_bytes = lce.convert_keras_model(model)\n        flatbuffer_file.write(flatbuffer_bytes)","1c166259":"model.summary()","88cb6690":"lq.models.summary(model)","af57edb7":"def conv_with_shortcut(x, filters, kernel_size, kwargs):\n    \"\"\"Convolutional block with shortcut connection.\n\n    Args:\n      x: input tensor with high-precision activations\n\n    Returns:\n      Tensor with high-precision activations\n    \"\"\"\n    x = lq.layers.QuantConv2D(filters, 1)(x)\n    # get number of filters\n    filters = x.get_shape().as_list()[-1]\n\n    # create shortcut that retains the high-precision information\n    shortcut = x\n\n    # efficient binarized convolutions (note inputs are also binarized)\n    x = lq.layers.QuantConv2D(\n        filters=filters,\n        kernel_size=kernel_size,\n        **kwargs,\n    )(x)\n\n    # normalize the (integer) output of the binary conv and merge\n    # with shortcut    \n    x = tf.keras.layers.add([x, shortcut])\n    x = layers.MaxPooling2D(pool_size=2)(x)\n    out = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n    return out","7bdf5759":"def create_model():\n    kwargs = dict(\n                input_quantizer='approx_sign',\n                kernel_quantizer='approx_sign',\n                kernel_constraint='weight_clip',\n                activation='leaky_tanh',\n                padding='same',\n                use_bias=False,\n                strides=1,\n                # pad_values=1          \n                )\n    \n    \n    input = tf.keras.Input((256,256,3))\n    x = layers.Normalization(axis=None,\n                            mean=0,\n                            variance=1)(input) \n        \n    #first\n    x = lq.layers.QuantConv2D(\n            filters=8,\n            kernel_size=3,\n            strides=1,\n            padding='same')(x) \n    x = layers.Activation('relu')(x) \n    x = layers.MaxPooling2D(pool_size=2)(x) \n    x = layers.BatchNormalization(momentum=0.9)(x) \n\n    x = conv_with_shortcut(x, 16, 5, kwargs)\n    x = conv_with_shortcut(x, 32, 7, kwargs)\n    x = conv_with_shortcut(x, 64, 9, kwargs)\n    x = conv_with_shortcut(x, 128, 11, kwargs)\n\n    #last\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Reshape((1,1,-1))(x)\n    x = lq.layers.QuantConv2D(\n        filters=1,\n        kernel_size=1,\n        **kwargs\n    )(x)\n    x = lq.layers.QuantConv2D(\n        filters=1024,\n        kernel_size=1,\n        use_bias=False\n    )(x)\n    x = layers.Activation('relu')(x)\n    x = lq.layers.QuantConv2D(\n        filters=64,\n        kernel_size=1,\n        use_bias=False\n    )(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Reshape((-1,))(x)\n    x = layers.Dropout(0.2)(x)\n    output = lq.layers.QuantDense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.models.Model(input, output)\n\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['binary_accuracy', Precision(), Recall()]    \n    )\n    \n    return model","77626cc1":"def scheduler(epoch, lr):\n    if epoch < 180:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n    \nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)","4ab4e0cb":"for i in tqdm(range(iterations_count)):\n    model = create_model()\n    history = model.fit(\n        train,\n        validation_data=valid,\n        batch_size=batch_size,\n        epochs=200,\n        shuffle=True,\n        callbacks=[lr_scheduler],\n        # verbose=0\n    )\n    df_history = pd.DataFrame(history.history)\n    df_history.loc[:, ['loss', 'val_loss']].plot(xlabel='Epochs', ylabel='Loss')\n    df_history.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(xlabel='Epochs', ylabel='Accuracy')\n    print(model.evaluate(test, batch_size=batch_size))\n    with open(f\"binary_model{i}.tflite\", \"wb\") as flatbuffer_file:\n        flatbuffer_bytes = lce.convert_keras_model(model)\n        flatbuffer_file.write(flatbuffer_bytes)","c1275c93":"model.summary()","14e5353b":"lq.models.summary(model)","f3a151fe":"# Set imports","9bb16b8d":"## Create","f5f97657":"# Full-precision model","868a80fd":"## Train and evaluate","47f59a29":"## Train and evaluate","c7c8a444":"# Load datasets","94d4e06d":"## Create","fce06802":"# Binary model"}}