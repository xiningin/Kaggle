{"cell_type":{"ea0d76e7":"code","38b4710d":"code","5a2563c3":"code","159bc77f":"code","d65c2110":"code","dbd0c20a":"code","a39c58f5":"code","571b69cf":"code","66a8c54d":"code","1d146296":"code","9f1560b6":"code","03061e87":"code","ea9d13d0":"code","c284fbac":"code","90fae03e":"code","5ce26175":"code","15d8918b":"code","b85fc2af":"code","2a5a2fd2":"code","d0e89054":"code","195ca4a6":"code","fbde96b9":"code","d5c89118":"markdown","c227a527":"markdown","d494fd26":"markdown","a1bcca66":"markdown","ffbaa9ab":"markdown","d3a4965b":"markdown"},"source":{"ea0d76e7":"%matplotlib inline\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport os\nimport json\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14\nimport seaborn as sns\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\nstart = dt.datetime.now()","38b4710d":"DP_DIR = '..\/input\/shuffle-csvs\/'\nINPUT_DIR = '..\/input\/quickdraw-doodle-recognition\/'\n\nBASE_SIZE = 256\nNCSVS = 100\nNCATS = 340\nnp.random.seed(seed=1987)\ntf.set_random_seed(seed=1987)\n\ndef f2cat(filename: str) -> str:\n    return filename.split('.')[0]\n\ndef list_all_categories():\n    files = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\n    return files","5a2563c3":"def apk(actual, predicted, k=3):\n    \"\"\"\n    Source: https:\/\/github.com\/benhamner\/Metrics\/blob\/master\/Python\/ml_metrics\/average_precision.py\n    \"\"\"\n    if len(predicted) > k:\n        predicted = predicted[:k]\n    score = 0.0\n    num_hits = 0.0\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits \/ (i + 1.0)\n    if not actual:\n        return 0.0\n    return score \/ min(len(actual), k)\n\ndef mapk(actual, predicted, k=3):\n    \"\"\"\n    Source: https:\/\/github.com\/benhamner\/Metrics\/blob\/master\/Python\/ml_metrics\/average_precision.py\n    \"\"\"\n    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n\ndef preds2catids(predictions):\n    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)","159bc77f":"STEPS = 800\nEPOCHS = 9\nsize = 64\nbatchsize = 680","d65c2110":"model = MobileNet(input_shape=(size, size, 3), alpha=1., weights=None, classes=NCATS)\nmodel.compile(optimizer=Adam(lr=0.002), loss='categorical_crossentropy',\n              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])\nprint(model.summary())","dbd0c20a":"def draw_cv2(raw_strokes, size=256, lw=6, time_color=True):\n    img = np.zeros((BASE_SIZE, BASE_SIZE,3), np.uint8)\n    \n    colors = [ (255,0,0),\n               (255,127,0),\n               (255,255,0),\n               (0,255,0),\n               (0,0,255),\n               (75,0,130),\n               (139,0,255)]\n    \n    for t, stroke in enumerate(raw_strokes):\n        color = ( 255 - colors[t % 7][0], 255 - colors[t % 7][1], 255 - colors[t % 7][2])\n        for i in range(len(stroke[0]) - 1):\n            _ = cv2.line(img, (stroke[0][i], stroke[1][i]),\n                         (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n#     print(np.unique(img))\n    if size != BASE_SIZE:\n        return cv2.resize(img, (size, size))\n    else:\n        return img\n\ndef image_generator_xd(size, batchsize, ks, lw=6, time_color=True):\n    while True:\n        for k in np.random.permutation(ks):\n            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n            for df in pd.read_csv(filename, chunksize=batchsize):\n                df['drawing'] = df['drawing'].apply(json.loads)\n                x = np.zeros((len(df), size, size, 3))\n                for i, raw_strokes in enumerate(df.drawing.values):\n                    x[i, :, :, :] = draw_cv2(raw_strokes, size=size, lw=lw,\n                                             time_color=time_color)\n                x = preprocess_input(x).astype(np.float32)\n                y = keras.utils.to_categorical(df.y, num_classes=NCATS)\n                yield x, y\n\ndef df_to_image_array_xd(df, size, lw=6, time_color=True):\n    df['drawing'] = df['drawing'].apply(json.loads)\n    x = np.zeros((len(df), size, size, 3))\n    for i, raw_strokes in enumerate(df.drawing.values):\n#         print(raw_strokes)\n        x[i, :, :, :] = draw_cv2(raw_strokes, size=size, lw=lw, time_color=time_color)\n    x = preprocess_input(x).astype(np.float32)\n    return x","a39c58f5":"valid_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(NCSVS - 1)), nrows=34000)\n# valid_df = pd.read_csv(os.path.join(INPUT_DIR, 'train_simplified'))\nx_valid = df_to_image_array_xd(valid_df, size)\ny_valid = keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\nprint(x_valid.shape, y_valid.shape)\nprint('Validation array memory {:.2f} GB'.format(x_valid.nbytes \/ 1024.**3 ))","571b69cf":"train_datagen = image_generator_xd(size=size, batchsize=batchsize, ks=range(NCSVS - 1))","66a8c54d":"x, y = next(train_datagen)\nn = 8\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(12, 12))\nfor i in range(n**2):\n    ax = axs[i \/\/ n, i % n]\n    ax.imshow( (255*(-x[i, :, :, :] + 1)\/2).astype(np.uint8) )\n    ax.axis('off')\nplt.tight_layout()\nfig.savefig('gs.png', dpi=300)\nplt.show();","1d146296":"%%timeit\nx, y = next(train_datagen)","9f1560b6":"callbacks = [\n    ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.75, patience=3, min_delta=0.001,\n                          mode='max', min_lr=1e-5, verbose=1),\n    ModelCheckpoint('model.h5', monitor='val_top_3_accuracy', mode='max', save_best_only=True,\n                    save_weights_only=True),\n]\nhists = []\nhist = model.fit_generator(\n    train_datagen, steps_per_epoch=STEPS, epochs=1, verbose=1,\n    validation_data=(x_valid, y_valid),\n    callbacks = callbacks\n)\nhists.append(hist)","03061e87":"valid_predictions = model.predict(x_valid, batch_size=128, verbose=1)\nmap3 = mapk(valid_df[['y']].values, preds2catids(valid_predictions).values)\nprint('Map3: {:.3f}'.format(map3))","ea9d13d0":"from tqdm import tqdm\nfrom dask import bag","c284fbac":"from PIL import Image, ImageDraw \nfrom pprint import pprint\nimport ast\nimheight, imwidth = 64, 64 \n# # faster conversion function\n\n# for i, raw_strokes in enumerate(df.drawing.values):\n#     x[i, :, :, :] = draw_cv2(raw_strokes, size=size, lw=lw, time_color=time_color)\n    \ndef df_to_image_array_chunk(chunk_values, size=64, lw=6, time_color=True):\n#     df['drawing'] = df['drawing'].apply(json.loads)\n    x = np.zeros((len(chunk_values),size, size, 3), np.uint8)\n    for i, raw_strokes in enumerate(chunk_values):\n        x[i, :, :, :] = draw_cv2(raw_strokes, size=size, lw=lw, time_color=time_color)\n    x = preprocess_input(x).astype(np.float32)\n    return cv2.resize(x, (size, size))\n\ndef draw_it(raw_strokes, size=64, lw=6, time_color=True):\n    img = np.zeros((BASE_SIZE, BASE_SIZE,3), np.uint8)\n    colors = [ (255,0,0),\n               (255,127,0),\n               (255,255,0),\n               (0,255,0),\n               (0,0,255),\n               (75,0,130),\n               (139,0,255)]\n    print(len(raw_strokes), len(raw_strokes[0]))\n    for t, stroke in enumerate(raw_strokes):\n#         if t == 1 or t == 2:\n#             print(stroke)\n        color = ( 255 - colors[t % 7][0], 255 - colors[t % 7][1], 255 - colors[t % 7][2])\n        for i in range(len(stroke[0]) - 1):\n            _ = cv2.line(img, (stroke[0][i], stroke[1][i]),\n                         (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n#     pprint(np.unique(img))\n    img = preprocess_input(img).astype(np.float32)\n    return cv2.resize(img, (size, size))","90fae03e":"ttvlist = []\nreader = pd.read_csv(os.path.join(INPUT_DIR, 'test_simplified.csv'), index_col=['key_id'],\n    chunksize=2048)\nfor df in tqdm(reader, total=55):\n    df['drawing'] = df['drawing'].apply(json.loads)\n    \n    x = np.zeros((len(df), size, size, 3))\n    for i, raw_strokes in enumerate(df.drawing.values):\n        x[i, :, :, :] = draw_cv2(raw_strokes, size=size)\n    x = preprocess_input(x).astype(np.float32)\n    \n    testpreds = model.predict(x, verbose=0)\n    ttvs = np.argsort(-testpreds)[:, 0:3]  # top 3\n    ttvlist.append(ttvs)\n    \nttvarray = np.concatenate(ttvlist)","5ce26175":"# #%% set label dictionary and params\n# classfiles = list_all_categories()\n# numstonames = {i: v[:-4].replace(\" \", \"_\") for i, v in enumerate(classfiles)}","15d8918b":"classfiles = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\nnumstonames = {i: v[:-4].replace(\" \", \"_\") for i, v in enumerate(classfiles)} #adds underscores","b85fc2af":"print(numstonames)","2a5a2fd2":"preds_df = pd.DataFrame({'first': ttvarray[:,0], 'second': ttvarray[:,1], 'third': ttvarray[:,2]})\npreds_df = preds_df.replace(numstonames)\npreds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n\nsub = pd.read_csv('..\/input\/quickdraw-doodle-recognition\/sample_submission.csv', index_col=['key_id'])\nsub['word'] = preds_df.words.values\n# sub.to_csv('..\/input\/quickdraw-doodle-recognition\/submission_mobilenet.csv')\nsub.to_csv('submission_mobilenet.csv')\nsub.head()","d0e89054":"preds_df.head()","195ca4a6":"# test = sub\n# submission = test[['key_id', 'word']]\n# submission.to_csv('gs_mn_submission_{}.csv'.format(int(map3 * 10**4)), index=False)\n# submission.head()\n# submission.shape","fbde96b9":"end = dt.datetime.now()\nprint('Latest run {}.\\nTotal time {}s'.format(end, (end - start).seconds))","d5c89118":"## Setup\nImport the necessary libraries and a few helper functions.","c227a527":"## Create Submission","d494fd26":"# \ud504\ub85c\uadf8\ub798\uba38\uc2a4 DS \uacfc\uc81c [\uc774\ubcf4\ud604]\n\n## \ucc38\uc870 \uc624\ud508\uc18c\uc2a4\n1. https:\/\/www.kaggle.com\/gaborfodor\/greyscale-mobilenet-lb-0-892\n2. https:\/\/www.kaggle.com\/jpmiller\/image-based-cnn\/data\n\n## \uac1c\uc694\n\n\uc878\uc5c5\uc791\ud488 \ub54c CNN\uc744 \uc0ac\uc6a9\ud574\ubcf8 \uacbd\ud5d8\uacfc, \uc774\ubbf8\uc9c0 \ucc98\ub9ac \uadf8\ub9ac\uace0 \uc81c\ud55c\ub41c \ub514\ubc14\uc774\uc2a4 \uc548\uc5d0\uc11c \uad6c\ud604\ud574\uc57c \ud588\uae30 \ub54c\ubb38\uc5d0 MobileNet\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc624\ud508\uc18c\uc2a4\ub85c \uc81c\ucd9c\ub41c 20\uc704\uad8c \uc548\uc5d0\uc11c MobileNet\uc744 Greyscale\ub85c\ub9cc \uc81c\ucd9c\uc774 \ub418\uc5b4\uc788\uc5b4\uc11c RGB\ucc44\ub110\uc744 \uc774\uc6a9\ud558\uc5ec \uc804\ucc98\ub9ac\ub97c \uad6c\ud604\ud574 \ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n\n\uc624\ud508\uc18c\uc2a4 1\uc744 \uae30\ubc18\uc73c\ub85c MobileNet \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\uc600\uace0, RGB\ucc44\ub110\uc744 \ubaa8\ub450 \uc0ac\uc6a9\ud558\uac8c \uc804\ucc98\ub9ac \ud55c \uacbd\uc6b0 test \uc2e4\ud589 \uc2dc \uba54\ubaa8\ub9ac \ucd08\uacfc\uac00 \uc788\uc5b4\uc11c \uc624\ud508\uc18c\uc2a4 2\ub97c \ucc38\uace0\ud558\uc5ec chunk\ub85c \ubd84\ud560\ud558\uc5ec \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uc600\uc2b5\ub2c8\ub2e4.","a1bcca66":"This kernel has three main components:\n\n* MobileNet\n* Fast and memory efficient Image Generator with temporal colored strokes\n* Full training & submission with Kaggle Kernel","ffbaa9ab":"## MobileNet\n\nMobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks.\n\n[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https:\/\/arxiv.org\/pdf\/1704.04861.pdf)","d3a4965b":"## Training with Image Generator"}}