{"cell_type":{"177c4119":"code","9b49b7fc":"code","ad9724f8":"code","1242094a":"code","7581d7cc":"code","b13948ff":"code","291464de":"code","54e70523":"code","d4f2fc3b":"code","297210ec":"code","4e6bc4bb":"code","c488a46c":"code","82fb9843":"code","5fe1274a":"code","2c02b278":"code","2ab595b6":"code","cb397891":"code","0d045bdc":"code","0c9f7948":"code","4cd6c89b":"code","739857a3":"code","f3d5d4e7":"code","0df2cafb":"code","d308eb4d":"code","f8965313":"code","d880cb0f":"markdown","e066bf82":"markdown","b2c5d2db":"markdown","4767be58":"markdown","8ac4c9f0":"markdown","b440dfdf":"markdown","3a153a8a":"markdown","0b73488d":"markdown","98426058":"markdown","25e6e3a6":"markdown","45f34d07":"markdown","f9948e35":"markdown","47882b66":"markdown","c01939a0":"markdown"},"source":{"177c4119":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings \nwarnings.filterwarnings('ignore')","9b49b7fc":"train_df = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/test.csv')\n","ad9724f8":"train_df","1242094a":"train_df.info()","7581d7cc":"train_df.describe()","b13948ff":"train_df.drop(['row_id'], axis=1, inplace=True)\ntest_df.drop(['row_id'], axis=1, inplace=True)\n\nTARGET = 'target'\nFEATURES = [col for col in train_df.columns if col not in [TARGET]]","291464de":"train_df.iloc[:, :-1].describe().T.sort_values(by='std' , ascending = False)","54e70523":"train_df.isna().sum(), test_df.isna().sum()","d4f2fc3b":"X = train_df[FEATURES]\ny = train_df[TARGET]","297210ec":"# Extra Tree Forest to analyse the best features \nfrom sklearn.ensemble import ExtraTreesClassifier\n\nextra_tree_forest = ExtraTreesClassifier()\nextra_tree_forest.fit(X, y)\nranked_features = pd.DataFrame(extra_tree_forest.feature_importances_, index=X.columns)\nranked = ranked_features.sort_values(by=0, ascending=False)\nranked = ranked.iloc[0:285]\nnew_cols = ranked.index","4e6bc4bb":"ranked_series = pd.Series(extra_tree_forest.feature_importances_, index=X.columns)\nax = ranked_series.nlargest(285).plot(kind= 'barh', figsize=(30,80))\nax.set_xlabel(\"Score\")\nax.set_ylabel(\"Features\")\nplt.show()","c488a46c":"# Feature Selection using Mutual Information \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\nsel_features = SelectKBest(mutual_info_classif, k=200)\nsel_features.fit(X, y)\nkeep_columns = X.columns[sel_features.get_support()]\nprint(keep_columns)","82fb9843":"train_df1 = train_df.copy()\n\ntrain_df1 = train_df1[keep_columns]\ntest_df = test_df[keep_columns]\ntrain_df1['target'] = -1\ntrain_df1['target'] = train_df[TARGET]\ntrain_df = train_df1","5fe1274a":"# New Features(after applying KSelectBest)\nFEATURES = [col for col in train_df.columns if col not in [TARGET]]","2c02b278":"from sklearn.model_selection import StratifiedKFold\ntrain_df['kfold'] = -1\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(skf.split(X=train_df[FEATURES], y=train_df[TARGET])):\n    train_df.loc[valid_indicies, 'kfold'] = fold","2ab595b6":"train_df.kfold.value_counts()","cb397891":"for i in range(0,5):\n    train_df[train_df.kfold == i].target.hist(figsize=(25,5))\n    plt.show()","0d045bdc":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntrain_df[TARGET] = encoder.fit_transform(train_df[TARGET])","0c9f7948":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import RobustScaler\nprediction = []\nscore = []\n\nfor fold in range (10):\n    X_train = train_df[train_df.kfold != fold].reset_index(drop=True)\n    X_val = train_df[train_df.kfold == fold].reset_index(drop=True)\n    X_test = test_df.copy()\n\n    # dependent variables \n    y_train = X_train[TARGET]\n    y_val = X_val[TARGET]\n\n    # independent variables\n    X_train = X_train[FEATURES]\n    X_val = X_val[FEATURES]\n\n    scaler = RobustScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n\n\n    # XGBRegressor moddelling \n    model = XGBClassifier(tree_method='gpu_hist', gpu_id=0, predictor='gpu_predictor')\n    model.fit(X_train,y_train,early_stopping_rounds=100,eval_set=[(X_val,y_val)],verbose=False)\n\n\n    preds_valid = model.predict(X_val)\n\n    #Training model apply the test data and predict the output\n    test_predict = model.predict(X_test)\n    prediction.append(test_predict)\n    accuracy= accuracy_score(y_val,preds_valid)\n\n    #Score \n    score.append(accuracy)\n    print(f\"fold:{fold},accuracy:{accuracy}\")\n    \nprint(np.mean(score),np.std(score))\n\n\n\n","4cd6c89b":"import optuna \n\ndef hyp_optimizer(trial):\n    fold = 0\n    # hyperparameters for XGBoost\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1,7)\n\n    X_train = train_df[train_df.kfold != fold].reset_index(drop=True)\n    X_val = train_df[train_df.kfold == fold].reset_index(drop=True)\n    # X_test = test_df.copy()\n\n    # dependent variables \n    y_train = X_train[TARGET]\n    y_val = X_val[TARGET]\n\n    # independent variables\n    X_train = X_train[FEATURES]\n    X_val = X_val[FEATURES]\n\n    scaler = RobustScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    # X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n\n\n    # XGBClassifier moddelling \n    model = XGBClassifier(\n      tree_method='gpu_hist', \n      gpu_id=0, predictor='gpu_predictor',\n      n_estimators=1000,\n      learning_rate=learning_rate, \n      reg_lambda=reg_lambda,\n      reg_alpha=reg_alpha,\n      subsample=subsample,\n      colsample_bytree=colsample_bytree,\n      max_depth=max_depth,\n      )\n\n    model.fit(X_train,y_train,early_stopping_rounds=100,eval_set=[(X_val,y_val)],verbose=False)\n\n    preds_valid = model.predict(X_val)\n\n    #Training model apply the test data and predict the output\n    # test_predict = model.predict(X_test)\n    # prediction.append(test_predict)\n    accuracy= accuracy_score(y_val,preds_valid)\n\n    #Score \n    # score.append(accuracy)\n    # print(f\"fold:{fold},accuracy:{accuracy}\")\n\n    return accuracy","739857a3":"study = optuna.create_study(direction='maximize')\nstudy.optimize(hyp_optimizer, n_trials=100)","f3d5d4e7":"print(study.best_params)","0df2cafb":"prediction = []\nscore = []\n\nfor fold in range (10):\n    X_train = train_df[train_df.kfold != fold].reset_index(drop=True)\n    X_val = train_df[train_df.kfold == fold].reset_index(drop=True)\n    X_test = test_df.copy()\n\n    # dependent variables \n    y_train = X_train[TARGET]\n    y_val = X_val[TARGET]\n\n    # independent variables\n    X_train = X_train[FEATURES]\n    X_val = X_val[FEATURES]\n\n    scaler = RobustScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n\n\n    # XGBRegressor moddelling \n    model = XGBClassifier(tree_method='gpu_hist', \n                          gpu_id=0, \n                          predictor='gpu_predictor',\n                          n_estimators=1000,\n                          learning_rate=0.18827343854947293,\n                          reg_lambda=5.5808003258392374e-05,\n                          reg_alpha=0.01179548036436762,\n                          subsample=0.5157261441197126,\n                          colsample_bytree=0.5274634372446382,\n                          max_depth=6\n                         )\n    model.fit(X_train,y_train,early_stopping_rounds=100,eval_set=[(X_val,y_val)],verbose=False)\n\n\n    preds_valid = model.predict(X_val)\n\n    #Training model apply the test data and predict the output\n    test_predict = model.predict(X_test)\n    prediction.append(test_predict)\n    accuracy= accuracy_score(y_val,preds_valid)\n\n    #Score \n    score.append(accuracy)\n    print(f\"fold:{fold},accuracy:{accuracy}\")\n    \nprint(np.mean(score),np.std(score))","d308eb4d":"submission_df = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/sample_submission.csv')\nsubmission_df","f8965313":"from scipy.stats import mode\nxgb_submission = submission_df.copy()\nxgb_submission[\"target\"] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(prediction),axis = 1)[0]).astype('int'))\nxgb_submission.to_csv(\"xgb-subs_v3.csv\",index=False)\nxgb_submission.head()","d880cb0f":"## Introduction","e066bf82":"### Modelling - XGBoost Classifier","b2c5d2db":"Training the model again with the best parameters","4767be58":"### Observation:\n* Classes are balanced ","8ac4c9f0":"### Understanding the Feature Importance ","b440dfdf":"## Null Values","3a153a8a":"### Hyperparameters Tunning using Optuna ","0b73488d":"### Feature Emgineering \n* Selecting the best 200 features ","98426058":"The task of this compeition is to classify 10 different bacteria species using data from a genomic \nanalysis technique that has some data compression and data loss. \nThe dataset used for this compeition is derived from this [paper](https:\/\/www.frontiersin.org\/articles\/10.3389\/fmicb.2020.00257\/full).\n\nSubmissions are evaluated based on their categorization accuracy.","25e6e3a6":"## Imports","45f34d07":"## Basic EDA","f9948e35":"## Data Preparation ","47882b66":"### Observations:\n* No NULL VALUES ","c01939a0":"## Reading the dataset"}}