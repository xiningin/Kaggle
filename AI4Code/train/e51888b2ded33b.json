{"cell_type":{"ec1d7fc8":"code","7d6e0e6c":"code","4af61040":"code","9f7cdd63":"code","1552b44e":"code","221cc928":"code","1390f193":"code","edc15850":"code","781e0606":"code","a7dcd482":"code","5a3a6e5c":"code","ddc6c31f":"code","565b026c":"code","9445c9f5":"code","10f13c27":"code","2f1e4a5d":"code","8ff96361":"code","794d8f5d":"code","0a9b49a1":"code","060f6242":"code","186985ad":"code","188bb64a":"code","ed7b49c2":"code","12841a73":"code","35e60e49":"markdown"},"source":{"ec1d7fc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\ndf=pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')","7d6e0e6c":"df.columns","4af61040":"df=df.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'])\ndf.describe","9f7cdd63":"df=df.replace('Unknown',np.nan)\n","1552b44e":"df.isnull().any()","221cc928":"y=df[['Attrition_Flag']]\nx=df[df.columns[2:]]","1390f193":"import seaborn as sns\nimport matplotlib.pyplot as plt\nax=sns.countplot(x='Attrition_Flag',data=y)\nprint(y.value_counts())","edc15850":"from sklearn.model_selection import train_test_split as tts\nxtr,xte,ytr,yte=tts(x,y,test_size=0.2)","781e0606":"yte=yte[xte.notna().all(axis=1)]\nxte=xte.dropna('index')","a7dcd482":"#custom distribution imputation\nms=dict(xtr['Marital_Status'].value_counts())\nsintodiv=round(ms['Single']\/(ms['Divorced']+ms['Single']),1)\nunms=np.random.choice(['Single','Divorced'],xtr.loc[:,'Marital_Status'].isna().sum(),p=[sintodiv,1-sintodiv])\nunc=0\nfor index,row in xtr.iterrows():\n    if pd.isnull(row['Marital_Status']):\n        if row['Dependent_count']==0:\n            xtr.loc[index,'Marital_Status']=unms[unc]\n            unc+=1\n        else:\n            xtr.loc[index,'Marital_Status']='Married'\n","5a3a6e5c":"row['Gender']","ddc6c31f":"#Max imputation\nxtr.loc[:,'Income_Category'].fillna('Less than $40K',inplace=True)\nxtr.loc[:,'Education_Level'].fillna('Graduate',inplace=True)","565b026c":"xtr.dtypes","9445c9f5":"#encoding\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(['Less than $40K','$40K - $60K','$60K - $80K','$80K - $120K','$120K +'])\nxtr.loc[:,'Income_Category']=le.transform(xtr.loc[:,'Income_Category'])\nxte.loc[:,'Income_Category']=le.transform(xte.loc[:,'Income_Category'])\nle.fit(['Blue','Silver','Gold','Platinum'])\nxtr.loc[:,'Card_Category']=le.transform(xtr.loc[:,'Card_Category'])\nxte.loc[:,'Card_Category']=le.transform(xte.loc[:,'Card_Category'])\nle.fit(['Uneducated','High School','College','Graduate','Post-Graduate','Doctorate'])\nxtr.loc[:,'Education_Level']=le.transform(xtr.loc[:,'Education_Level'])\nxte.loc[:,'Education_Level']=le.transform(xte.loc[:,'Education_Level'])\nxtr.loc[:,'Marital_Status']=le.fit_transform(xtr.loc[:,'Marital_Status'])\nxte.loc[:,'Marital_Status']=le.fit_transform(xte.loc[:,'Marital_Status'])\nxte.loc[:,'Gender']=le.fit_transform(xte.loc[:,'Gender'])\nxtr.loc[:,'Gender']=le.fit_transform(xtr.loc[:,'Gender'])","10f13c27":"print(max(df['Customer_Age']),min(df['Customer_Age']))\nhist=df['Customer_Age'].hist(bins=8)","2f1e4a5d":"def age(cage):\n    if cage<=40:\n        return 0\n    elif cage>40 and cage<=50:\n        return 1\n    elif cage>50 and cage<=60:\n        return 2\n    else:\n        return 3","8ff96361":"for ind,row in xtr.iterrows():\n    xtr.loc[ind,'Age']=age(row['Customer_Age'])\nfor ind,row in xte.iterrows():\n    xte.loc[ind,'Age']=age(row['Customer_Age'])\nxtr=xtr.drop('Customer_Age',axis=1)\nxte=xte.drop('Customer_Age',axis=1)","794d8f5d":"xtr_st=(xtr-xtr.mean())\/xtr.std()\nxtrn=pd.concat([ytr,xtr_st.iloc[:,0:10]],axis=1)\nxtrn=pd.melt(xtrn,id_vars='Attrition_Flag',var_name='features',value_name='value')\nplt.figure(figsize=(12,12))\n#tic=time.time()\nsns.violinplot(x='features',y='value',hue='Attrition_Flag',data=xtrn,split=True,inner='quart')\nplt.xticks(rotation=90)","0a9b49a1":"xtrn=pd.concat([ytr,xtr_st.iloc[:,10:]],axis=1)\nxtrn=pd.melt(xtrn,id_vars='Attrition_Flag',var_name='features',value_name='value')\nplt.figure(figsize=(12,12))\n#tic=time.time()\nsns.violinplot(x='features',y='value',hue='Attrition_Flag',data=xtrn,split=True,inner='quart')\nplt.xticks(rotation=90)","060f6242":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(xtr.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","186985ad":"#feature selection\nfeatures=['Total_Revolving_Bal','Total_Amt_Chng_Q4_Q1','Total_Trans_Amt','Total_Ct_Chng_Q4_Q1','Avg_Utilization_Ratio']\nxtefs=xte[features]\nxtrfs=xtr[features]","188bb64a":"le = preprocessing.LabelEncoder()\nle.fit(['Attrited Customer','Existing Customer'])\nytr=le.transform(ytr['Attrition_Flag'].values.reshape((len(ytr),)))\nyte=le.transform(yte['Attrition_Flag'].values.reshape((len(yte),)))","ed7b49c2":"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score,f1_score\n\nmodel = RandomForestClassifier(random_state=1)\nmodel.fit(xtrfs, ytr)\npreds = model.predict(xtefs)\n\nfor i in range(len(preds)):\n    if preds[i]<0.5:\n        preds[i]=0\n    else:\n        preds[i]=1\n\nprint('MAE :',mean_absolute_error(yte, preds))\nprint('Accuracy :',accuracy_score(yte,preds))","12841a73":"from sklearn.metrics import confusion_matrix as cm\ncm1=cm(yte,preds)\nsns.heatmap(cm1,annot=True,fmt='d')\nprint('F1 score :',f1_score(yte,preds))","35e60e49":"Imbalance in dataset;\nEvaluation for prediction should include recall and precision"}}