{"cell_type":{"7d318711":"code","9b9bec64":"code","4ca30f45":"code","c876594c":"code","6b0e5c06":"code","8015c172":"code","fb3a93b7":"code","bd2e0b07":"code","9f50752a":"code","5a351143":"code","a7d679fb":"code","2fd5e6f6":"code","981cde57":"code","b061d582":"code","f5bead6b":"code","a1a31439":"code","ab829884":"code","722dab38":"code","7befab06":"code","323b3880":"code","ffcadf1e":"code","c325fc93":"code","05781fd6":"code","bc5656a8":"code","153cb565":"code","e5ab172f":"code","22af5f06":"code","e79a151b":"code","438f0f5b":"code","6b5a1281":"code","f385f65f":"code","a88ddc73":"code","8146c46f":"markdown","e0afd114":"markdown","57b5f86c":"markdown","b5998995":"markdown","ae9ec80d":"markdown","242a0a67":"markdown","a37cbb41":"markdown","335b117b":"markdown","d9365750":"markdown","af92c08a":"markdown","0b57db9c":"markdown","451616b3":"markdown","1c506b17":"markdown","4f792c9c":"markdown","bd599a4d":"markdown","9ee2a238":"markdown","522b40c0":"markdown","765753dc":"markdown","3d5ee59d":"markdown","f04f037c":"markdown","e5312911":"markdown","fb803284":"markdown","f4ec4114":"markdown","188b6206":"markdown","510d3bee":"markdown","17cbc237":"markdown","59a3408c":"markdown","13af02bb":"markdown","95f68b85":"markdown","b94dc424":"markdown","b7b4453f":"markdown","7e37eef1":"markdown","b5940ed4":"markdown","24e75220":"markdown","a6b71df6":"markdown","f95a6d33":"markdown","3323b9bd":"markdown","09ce9790":"markdown","c8b108d8":"markdown","6f37b552":"markdown","34f128ce":"markdown"},"source":{"7d318711":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('seaborn')\nsns.set(style='white', context='notebook', palette='deep')\n\n\n\ncrimes1 = pd.read_csv('..\/input\/Chicago_Crimes_2005_to_2007.csv',error_bad_lines=False)\ncrimes2 = pd.read_csv('..\/input\/Chicago_Crimes_2008_to_2011.csv',error_bad_lines=False)\ncrimes3 = pd.read_csv('..\/input\/Chicago_Crimes_2012_to_2017.csv',error_bad_lines=False)\ncrimes = pd.concat([crimes1, crimes2, crimes3], ignore_index=False, axis=0)\n\ncrimes.head()","9b9bec64":"crime_count = pd.DataFrame(crimes.groupby('Primary Type').size().sort_values(ascending=False).rename('Count').reset_index())\ncrime_count","4ca30f45":"crime_count[:20].plot(x='Primary Type',y='Count',kind='bar')","c876594c":"crime_count = pd.DataFrame(crimes.groupby('Location Description').size().rename('Count').sort_values(ascending=False).reset_index())\ncrime_count[:20]\n","6b0e5c06":"crime_count = pd.DataFrame(crimes.groupby('District').size().rename('Count').sort_values(ascending=False).reset_index())\ncrime_count\n","8015c172":"crimes[['X Coordinate', 'Y Coordinate']] = crimes[['X Coordinate', 'Y Coordinate']].replace(0, np.nan)\ncrimes.dropna()\ncrimes.plot(kind='scatter',x='X Coordinate', y='Y Coordinate', c='District', cmap=plt.get_cmap('jet'))","fb3a93b7":"plt.figure(figsize=(15,15))\nsns.jointplot(x=crimes['X Coordinate'].values, y=crimes['Y Coordinate'].values, size=10, kind='hex')\nplt.ylabel('Longitude', fontsize=12)\nplt.xlabel('Latitude', fontsize=12)\nplt.show()","bd2e0b07":"plt.figure(figsize=(12,12))\nsns.lmplot(x='X Coordinate', y='Y Coordinate', size=10, hue='Primary Type', data=crimes, fit_reg=False)\nplt.ylabel('Longitude', fontsize=15)\nplt.xlabel('Latitude', fontsize=15)\nplt.show()","9f50752a":"topk = crimes.groupby(['District', 'Primary Type']).size().reset_index(name='counts').groupby('District').apply(lambda x: x.sort_values('counts',ascending=False).head(3))\ntopk[:51]","5a351143":"g =sns.factorplot(\"Primary Type\", y='counts', col=\"District\", col_wrap=4,\n                   data=topk, kind='bar')\nfor ax in g.axes:\n    plt.setp(ax.get_xticklabels(), visible=True, rotation=30, ha='right')\n\nplt.subplots_adjust(hspace=0.4)","a7d679fb":"g =sns.factorplot(\"Arrest\", col=\"District\", col_wrap=4, legend_out=True,\n                   data=crimes, orient='h',\n                    kind=\"count\")\nfor ax in g.axes:\n    plt.setp(ax.get_xticklabels(), visible=False)","2fd5e6f6":"df_theft = crimes[crimes['Primary Type'] == 'NARCOTICS']\nplt.figure(figsize = (15, 7))\nsns.countplot(y = df_theft['Description'],order=df_theft['Description'].value_counts().index[:20])","981cde57":"Beat = pd.DataFrame(crimes.groupby('Beat').size().rename('Count').sort_values(ascending=False).reset_index())\nBeat[:20]","b061d582":"crimes.Date = pd.to_datetime(crimes.Date, format='%m\/%d\/%Y %I:%M:%S %p')\ncrimes.index = pd.DatetimeIndex(crimes.Date)\n\ncrimes.resample('M').size().plot(legend=True)\nplt.title('Number of crimes per month (2001 - 2018)')\nplt.xlabel('Months')\nplt.ylabel('Number of crimes')\nplt.show()","f5bead6b":"crimes['day_of_week']=crimes['Date'].dt.weekday_name\ncrimes['month']=crimes['Date'].dt.month\ncrimes['day']=crimes['Date'].dt.day\ncrimes['hour']=crimes['Date'].dt.hour\ncrimes['minute']=crimes['Date'].dt.minute\n\n\ncrimes.head()\n","a1a31439":"crime_ = pd.DataFrame(crimes.groupby('day').size().rename('Count').reset_index())\ncrime_.plot(x='day',y='Count',kind='bar')\ncrime_ = pd.DataFrame(crimes.groupby('month').size().rename('Count').reset_index())\ncrime_.plot(x='month',y='Count',kind='bar')\ncrime_ = pd.DataFrame(crimes.groupby('day_of_week').size().rename('Count').reset_index())\ncrime_.plot(x='day_of_week',y='Count',kind='bar')\ncrime_ = pd.DataFrame(crimes.groupby('hour').size().rename('Count').reset_index())\ncrime_.plot(x='hour',y='Count',kind='bar')\ncrime_ = pd.DataFrame(crimes.groupby('minute').size().rename('Count').reset_index())\ncrime_.plot(x='minute',y='Count',kind='bar')","ab829884":"crimes_count_date = crimes.pivot_table('ID', aggfunc=np.size, columns='Primary Type', index=crimes.index.date, fill_value=0)\ncrimes_count_date.index = pd.DatetimeIndex(crimes_count_date.index)\nplo = crimes_count_date.rolling(365).sum().plot(figsize=(12, 30), subplots=True, layout=(-1, 2), sharex=False, sharey=False)","722dab38":"crimes['day_of_week']=crimes['Date'].dt.weekday\ncrimes.head()\n\n","7befab06":"crimes=crimes.dropna()\ncrimes.isnull().sum(axis = 0)","323b3880":"crimes=crimes.drop('Case Number', axis=1)\ncrimes=crimes.drop('ID', axis=1)\ncrimes=crimes.drop('FBI Code', axis=1)\ncrimes=crimes.drop('Date', axis=1)\ncrimes=crimes.drop('Block', axis=1)\ncrimes=crimes.drop('Updated On', axis=1)\ncrimes=crimes.drop('Location', axis=1)\ncrimes=crimes.drop('Longitude', axis=1)\ncrimes=crimes.drop('Latitude', axis=1)\ncrimes=crimes.drop('IUCR', axis=1)\n","ffcadf1e":"\nx=crimes['X Coordinate'].mean()\ny=crimes['Y Coordinate'].mean()\nprint(x,y)\n","c325fc93":"categories_type = {c:i for i,c in enumerate(crimes['Primary Type'].unique())}\ncategories_description = {c:i for i,c in enumerate(crimes['Description'].unique())}\ncategories_location_des = {c:i for i,c in enumerate(crimes['Location Description'].unique())}\ncategories_Arrest = {c:i for i,c in enumerate(crimes['Arrest'].unique())}\ncategories_Domestic = {c:i for i,c in enumerate(crimes['Domestic'].unique())}\n#categories_IUCR = {c:i for i,c in enumerate(crimes['IUCR'].unique())}\n\ncrimes['Primary_Type_Num'] = [float(categories_type[t]) for t in crimes['Primary Type']]\ncrimes['Description_Num'] = [float(categories_description[t]) for t in crimes['Description']]\ncrimes['Location_Des_Num'] = [float(categories_location_des[t]) for t in crimes['Location Description']]\ncrimes['Arrest_Num'] = [float(categories_Arrest[t]) for t in crimes['Arrest']]\ncrimes['Domestic_Num'] = [float(categories_Domestic[t]) for t in crimes['Domestic']]\n#crimes['IUCR_Num'] = [float(categories_IUCR[t]) for t in crimes['IUCR']]\n\ncrimes=crimes.drop('Primary Type', axis=1)\ncrimes=crimes.drop('Description', axis=1)\ncrimes=crimes.drop('Location Description', axis=1)\ncrimes=crimes.drop('Arrest', axis=1)\ncrimes=crimes.drop('Domestic', axis=1)\n#crimes=crimes.drop('IUCR', axis=1)\n\ncrimes.head()","05781fd6":"from sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder as le\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ncrimes['X Coordinate'] = preprocessing.scale(list(map(lambda x: x-1164537.87395, crimes['X Coordinate'])))\ncrimes['Y Coordinate'] = preprocessing.scale(list(map(lambda x: x-1885607.09892, crimes['X Coordinate'])))\ncrimes.head()","bc5656a8":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nprint(crimes.shape)\nX_train, X_test, y_train, y_test = train_test_split(crimes.loc[:, crimes.columns != 'Primary_Type_Num'], crimes['Primary_Type_Num'], test_size = 0.2, random_state = 0)\nclf = RandomForestClassifier(max_features=\"log2\", max_depth=16, n_estimators=25,\n                             min_samples_split=600, oob_score=False,n_jobs=4).fit(X_train,y_train)\ny = clf.predict(X_test)\nprint(X_train.shape,X_test.shape)\n\nprint(recall_score(y,y_test, average='micro'))\nprint(recall_score(y,y_test, average='macro'))\nprint(recall_score(y,y_test, average='weighted'))\n\nprint(precision_score(y,y_test, average='micro'))\nprint(precision_score(y,y_test, average='macro'))\nprint(precision_score(y,y_test, average='weighted'))\n\nprint(f1_score(y,y_test, average='micro'))\nprint(f1_score(y,y_test, average='macro'))\nprint(f1_score(y,y_test, average='weighted'))\n","153cb565":"crime_count = pd.DataFrame(crimes.groupby('Primary_Type_Num').size().sort_values(ascending=False).rename('Count').reset_index())\ncrime_count","e5ab172f":"from imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(crimes.loc[:, crimes.columns != 'Primary_Type_Num'], crimes['Primary_Type_Num'], test_size = 0.2, random_state = 0)\n\nprint(X_train.shape,y_train.shape,'before')\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\nprint(X_train_res.shape,y_train_res.shape,'after')\n\nclf = RandomForestClassifier(max_features=\"log2\", max_depth=20, n_estimators=25,\n                             min_samples_split=600, oob_score=False,n_jobs=4).fit(X_train_res,y_train_res.ravel())\ny = clf.predict(X_test)\n\n\n\n\nprint(recall_score(y,y_test, average='micro'))\nprint(recall_score(y,y_test, average='macro'))\nprint(recall_score(y,y_test, average='weighted'))\n\nprint(precision_score(y,y_test, average='micro'))\nprint(precision_score(y,y_test, average='macro'))\nprint(precision_score(y,y_test, average='weighted'))\n\nprint(f1_score(y,y_test, average='micro'))\nprint(f1_score(y,y_test, average='macro'))\nprint(f1_score(y,y_test, average='weighted'))\n","22af5f06":"from sklearn.linear_model import LogisticRegression\nlgr = LogisticRegression()\nlgr.fit(X_train,y_train)\ny = lgr.predict(X_test)\n\n\nprint(recall_score(y,y_test, average='micro'))\nprint(recall_score(y,y_test, average='macro'))\nprint(recall_score(y,y_test, average='weighted'))\n\nprint(precision_score(y,y_test, average='micro'))\nprint(precision_score(y,y_test, average='macro'))\nprint(precision_score(y,y_test, average='weighted'))\n\nprint(f1_score(y,y_test, average='micro'))\nprint(f1_score(y,y_test, average='macro'))\nprint(f1_score(y,y_test, average='weighted'))","e79a151b":"Image(filename='xai.jpg')","438f0f5b":"Image(filename='house.png')","6b5a1281":"Image(filename='final.png')","f385f65f":"Image(filename='anchor.png')","a88ddc73":"import sklearn\nimport sklearn.datasets\nimport sklearn.ensemble\nimport numpy as np\nimport lime\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport lime.lime_tabular\n\n\npredict_fn_logreg = lambda x: lgr.predict_proba(x).astype(float)\npredict_fn_rf = lambda x: clf.predict_proba(x).astype(float)\n\n\n\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train.values ,feature_names = list(crimes.loc[:, crimes.columns != 'Primary_Type_Num'].columns)\n,class_names=list(crimes.Primary_Type_Num.unique()))\n\n\n\nobservation_1 = 4\n# Get the explanation for Logistic Regression\nexp = explainer.explain_instance(X_test.values[observation_1], predict_fn_logreg, num_features=6)\nexp.show_in_notebook(show_all=False)\n\n# Get the explanation for RandomForest\nexp = explainer.explain_instance(X_test.values[observation_1], predict_fn_rf, num_features=6)\nexp.show_in_notebook(show_all=False)\n\n\nprint('Actual class of our observation --->  ',y_test[observation_1])\n\n\n\n\nobservation_2 = 124\n# Get the explanation for Logistic Regression\nexp = explainer.explain_instance(X_test.values[observation_2], predict_fn_logreg, num_features=6)\nexp.show_in_notebook(show_all=False)\n\n# Get the explanation for RandomForest\nexp = explainer.explain_instance(X_test.values[observation_2], predict_fn_rf, num_features=6)\nexp.show_in_notebook(show_all=False)\n\n\nprint('Actual class of our observation --->  ',y_test[observation_2])","8146c46f":"Okay, interesting! So we have a high score for micro in every case but not for macro. So basically when the difference is that much it means that our classifier is suffering in predicting classes that have a lower number of instances. Let's see how many samples each class does again.","e0afd114":"Now let's drop some columns that have nothing to do with primary type prediction or we have another version of it in our features. We will also drop IUCR since base on their explanation it is sort of encoding of the primary type itself as the image below. It would cause data leakage problem in our prediction and gives us an unrealisticly good prediction.","57b5f86c":"So precision and time are pretty obvious but there is another factor which is coverage. It is something that tries to explain how this model could explain its behavior based on our feature set on all over data. Unclear coverage can lead to low human precision and they might think that insight from an explanation can be applied to unseen samples even when it does not. \n\nBase on different parameters such as Time, precision and coverage we can see that the anchor model is performing better than LIME. Does that mean LIME simply doesn't work? Well, the short answer is it depends. So, for example, the household income dataset has a lot of features and when it comes to using Anchor method it can give us a lot of boundaries that it's thinking is responsible for its action at that point. The result of that would be low coverage and interpretability of unseen data. On the other hand, LIME is trying to give us lower predictions and simpler ones, that would be something more appealing to users. In the below image we can see that Anchor is giving a lot of factors, but LIME is trying to give less!","b5998995":"Let's train a Random Forest model. Some of pros and cons of this model are:\n\nPros:\n+ Works well on large datasets\n+ One of the most accurate decision models\n+ Can be used to extract variable importance\n+ Low chance of overfitting \n\nCons:\n- Unlike decision trees, results are difficult to interpret\n- Hyperparameters need good tuning for high accuracy\n\nCompare to other ensembling algorithms like XGBOOST, it's easier to tune, also it can be trained in parallel due to its nature, but since XGBOOST at each iteration needs the previous step to calculate gradients it cannot be done in parallel, But XGBOOST due to some benchmarks can have higher accuracy.","ae9ec80d":"Another evidence that we need XAI platforms as they discussed in (1) is Christianity\/Atheism text classification. Base on different algorithms such SVM they were getting high accuracy in terms of classification, but after taking closer look into the model, they realized that words like \u201cPosting\u201d, \u201cHost\u201d and \u201cRe\u201d are things that classifier are looking for them to base on presence  or lack of that decide what class this text belongs to. Things that are not a good metric( not any relation between them to either Christianity or Atheism) for classifying.  \n\nSo after testing on different datasets and considering both how people think that's interpretable and accuracy of the model they have summarized the results of LIME and Anchor into the table below.","242a0a67":"2) Are they happening mostly on streets, apartments or somewhere else?","a37cbb41":"As we can see most occured crime is possession of 30gr or less  canabis. Also we can see lower rate of crime for more than 30 grams or people who were narcotics. ","335b117b":"By looking at those hitmaps and this link(https:\/\/data.cityofchicago.org\/Public-Safety\/Boundaries-Police-Districts-current-\/fthy-xz3r) we can see that district 7, 8 and 11 as we expected to have the most number of crimes. But district 1 and 18 also have very heated on the map but they don't have that much of occurrence on our table, so why is like that? Because it's close to downtown! We can say the density of crimes are high there(number of them\/square feet). ","d9365750":"Okay as we expected we can see that for some classes we have a lot of data but not for all of them(Imbalanced dataset). We can do over\/under sampling. So basically try to artificially make some instances for those classes that don't have that much of data. In my experience and as I have been reading, over-sampling usually works better since for undersampling we are losing a portion of real data which is valuable and nothing interesting is in losing data. For oversampling, there are several methods such as ROSE or SMOTH. I've found SMOTE more accurate and robust. So let's see what will happen after oversampling our data.\n\nNOTE: Running code below might take several hours, in case you need to run it.","af92c08a":"For our metric, we can use log loss for multiclass classification or Micro\/Macro score. In here I'm going to use the second one.","0b57db9c":"Now it would be interesting to take a look into the crimes from time perspective.","451616b3":"In this notebook we are going to explore different aspects of this dataset and do some feature engineering, selection and finally create a multi-class classifier with some well-known algorithms. After that, I'm going to explain a little bit about my understanding about XAI and we'll  see more of explanatory analysis using the LIME framework.","1c506b17":"We have some fields that are related to the location such as Latitude, Longitude, X Coordinate, Block, District, and some others. So we are going to remove some of such as Longitude and Latitude since we have X and Y coordinate also we will make them zero-mean center. ","4f792c9c":"Well seems we cannot get so much out of it since the map is very condensed, so let's see on some tables.\nNow I'm going to find what are the top 3 crimes in each district.","bd599a4d":"Now let's see a break down of crimes during the time we gathered data.","9ee2a238":"What we get out of this are:\n\n1) We don't have any data for district 13 on this dataset so that's why something doesn't show up. Also for district 31, we have only 181 crimes so that's why it's not that obvious in bars.\n\n2) Theft and Battery are happening in all districts.\n\n3) Deceptive Practise is most prevalent in Districts 1 and 18( What is Deceptive Practice? Based on this website:\nhttps:\/\/www.consumerjusticecenter.com\/blog\/2017\/05\/examples-of-deceptive-trade-practices.shtml\nIt's mostly when:\n- Saying that services are needed when they are not\n- Saying that products have sponsorship that they don't actually have\nWhere are they happening mostly? Downtown! What else we expected? It's downtown and it's all about sales!\n\n4) Narcotics are mostly at Districts 7, 10, 11, 15\n\n----------------------------------------------------------------------------------------","522b40c0":"Ok, now let's jump into making a predictive model. ","765753dc":"Now let's train a simpler model like Logistic regression. Some of pros and cons for this model are:\n\nPros:\n+ It's easy and convenient to interpret due to probability scores for observations.\n+ can be regularized to avoid overfitting\n\nCons:\n- Doesn\u2019t perform well when feature space is too large\n- Doesn\u2019t handle a large number of categorical features\/variables well\n- Not doing well on non-linear patterns","3d5ee59d":"Let's jump into the (1) reference. So they offered an algorithm which LIME and the way it works is that basically, it tries to find boundaries by sampling instances. So like the image below, it tries to find out why this model decided to set bold red + as a red class base on its training. We can see that this model is pretty complex and those boundaries cannot be well explained with a linear model. Now, what LIME is doing is that it tries to samples points around the specific data we are looking for and then base on distribution and distances between that point and they try to explain why we classified this as to be red. So, for example, some of those points are closer so they are bolder and so on. Another factor that this model is considering is that how much it is interpretable for a human. \n\nAfter making different experiments on different dataset they found interesting results. For example, they tried to see what's the reason that an image classification would say about wolves and huskies. Without XAI system, we are getting high accuracy but when they try to look at the decision boundaries it was pretty disappointing. Sometimes the model tries to classify images as wolves just because there is a snow background in it. Definitely, it has some relation to that but we know that is not a reliable factor. So after using LIME they were showing that for example to classify a Labrador with a guitar which is pretty hard for computers to recognize different things in it, it can pretty well explain what is going on in this image. We might get lower accuracy for our classification or regression but we would know why the model is doing this and it would be more reliable in terms of human interaction.\n\nAnother interesting paper from those is (3), so the method they explained is anchors and how they can be a very high-precision method to explain machine learning algorithms. So the idea behind anchor is basically what causality is about. We try to see what are things that when we know they are happening can affect our decision boundaries to decide which class this record needs to belong to. We would try to define a threshold for that as hyper-parameter. Later they did interesting experiments on POS tagging and household dataset. For example in the image below, Anchor is trying to explain why it has been classifying those instances into different classes. It tries to show us what are important things that affected the model at that specific point.","f04f037c":"We are going to answer some questions about this dataset:\n\nHow many crimes do we have base on different Primary Type?","e5312911":"District 11 is having a high number of both arrest True and False, also they are close to each other in comparison to other districts' ratio. Let's back to see what is mostly happening on District 11. Interesting, it's mostly about narcotics. Seems that there are a lot of rats over there reporting them to the cups! HAHA. However, we don't know if Narcotics means organized crime or regular people smoking weed. So let's take look into NARCOTICS crimes description and see if we can find out something!","fb803284":"3) What districts are most likely to have crimes in it?","f4ec4114":"Interesting! This is the breakdown for months, as we can see we have seasonality in our data. We have an up trending from the beginning of the year to the middle and then downward from middle to the end pretty much each year! Now let's see what's happening base on days, months and weekdays. First, we need to add columns for those features in our dataset.","188b6206":"__Explainable AI (XAI)__","510d3bee":"These were pretty much a general understanding of myself about these methods. Now let's try to make a simple XAI platform for this data set that we have been working with.","17cbc237":"So as we can see for the first example our actual class was 4. Our models have a different prediction for first observation, logistic regression classified as group 6 while random forest with high confidence of 89% classified that correctly.  On the right-hand side, we are seeing features that were contributing to our prediction. For example, it says that something like being domestic or not is something important to classify this specific instance as class 1 or not class 1 and so forth. The length of the bar is showing how much that predictor is helping in our prediction base on what side is has been placed. It pretty explains how each feature and its related value domain can affect our model's prediction. For observation 2 our models both predicted correctly but as we can see and expected random forest has a higher probability about its prediction.\n\nThank you for giving the time to reading my notebook.","59a3408c":"Now let's encode some of our field into numbers so that would be easier for our algorithms to compute. ","13af02bb":"As we expected there is an upward and downward trend in during the year on average. In terms of days of week and month, it's pretty much same and evenly distributed. There are only two anomalies on 31th and the first day of each month. 31st day we only have 6 of them but if we multiply that number by 2, it will be mostly around the first day of each month. It seems that at the end of each month and the first day of the month we are seeing a peak in our crime rate, but that might be something due to our data sampling and the timeline we have been gathering data. We are seeing a pattern in the hourly rate of crimes, mostly happening at midnights! seems 5 AM is the safest time during the day and that sort of make sense.\n\nIn terms of minutes, we can see a huge peak on 30th and first minutes of the hours. It might be because people are most likely to round things to 30th minutes or the closest hour in case they cannot remember the exact timing of the incident.","95f68b85":"It doesn't help us that much so let's cisualize this on map so we can have a better sense of what's going on!","b94dc424":"So what is beat? base on dataset description:\n\"A beat is the smallest police geographic area\"\nSo take a look into it to see what beats most have the most dangerous area to work in.","b7b4453f":"We already have done some feature engineering by adding the day, minute, hour and so on to our dataset. Let's do this for other features that we have too.","7e37eef1":"Now zero-mean centering our X and Y coordinates","b5940ed4":"Interesting! Most of these crimes had a jump in 2016, it might because of political and presidential issue or something else.  Some of them bounce back some of them not. One of them which is pretty interesting is deceptive practices which are now we are an all-time high in the last 16 years! It would be something interesting to dig into it if I had more time.","24e75220":"Okay, that's pretty much what we expected from our logistic regression. When our feature domain become large. Our model cannot perform very well in this case but it's good to have them and see its performance on LIME too.","a6b71df6":"Great! So the first point is that before oversampling we had around 5 million rows of data, but after that around 35 Million! After training our model we can see our Micro score has been decreased by about 7% but on the other hand our Macro score has been improved significantly. Based on precision, recall or F-1 score our Macro score has been improved from 15% to 30% which is pretty good. It means that even for classes that have a very low number of samples we can predict with a reasonable percentage. So based on what we need from our data, lower false positives or false negatives we can use either of those. Due to the need for high computational power for training this model I wasn't able to do that much hyper-parameter tuning. I'm sure after doing some tuning our micro score would be higher too.","f95a6d33":"Now let's see what would be the break down of crimes on the map.","3323b9bd":"__Loading Data and libraries__","09ce9790":"We have different features in here, let's take a closer look into them. ","c8b108d8":"Now let's see what arrest column can give us!","6f37b552":"So let's talk about the interesting topic of explainable AI (XAI). In the following, I'm going to refer these references with numbers attached to them.\n\n+ Why Should I Trust You?\u201d Explaining the Predictions of Any Classifier --> 1\n+ A Unified Approach to Interpreting Model Predictions --> 2\n+ Anchors: High-Precision Model-Agnostic Explanations --> 3\n+ https:\/\/github.com\/datascienceinc\/Skater --> 4\n\nBase on (1) and (3) and some other web sources these are my understanding about XAI. \n\nEach day we are facing new algorithms, some of them are easy to interpret some of them like neural networks or random forest are pretty impossible to interpret. We are training them all over the places and make our decisions based on those models. When it comes to sensitive data and decisions people need to know why this model is saying what is trying to say. They need to know what are factors that have been affected this model to tell us, for example, this person is eligible for getting a loan or not. \n\nThat was when this concern raised in different communities and they have decided to pass new roles for using machine learning algorithms. Something like GDPR which is mostly the right to be informed about the decision and they were supposed to start the needed structure for doing so in Europe in 2018. \n\nSo these algorithms are becoming more and more like black boxes that we have no or little idea what's going on in them. I found this image as something that we are here today and something that we want to have.\n","34f128ce":"We are seeing a full break down of crimes and their types above. Let's visualize top 20 of them now."}}