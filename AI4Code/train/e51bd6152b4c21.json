{"cell_type":{"b4ef879f":"code","479fcd3f":"code","575679bf":"code","92804349":"code","d417784e":"code","8c6f67c7":"code","4fc6c721":"code","ce3d101c":"code","fd296269":"code","74c463e8":"code","a444256c":"code","6ce6a73e":"code","ac97bcda":"code","4b33263e":"code","c3b25635":"code","dd530203":"code","6bafe9af":"code","d8de86ac":"code","f5a90034":"code","8169cf2f":"code","548da5c7":"code","64b2a86d":"code","e8715f74":"code","6d398153":"code","177c9494":"code","72cbeca6":"code","e92b3d0d":"code","cd05aaf1":"code","9ad923c4":"code","200bc4bc":"code","b73a37a5":"code","43471dac":"code","8588b8aa":"code","6e9081e7":"code","63121dd5":"code","602d9256":"code","87890586":"code","54efc797":"code","001774cf":"code","4ce080e6":"code","a212f61e":"code","4d3555df":"code","368e999b":"code","05830091":"code","bbb560f9":"code","ae5286bb":"code","7e580f11":"code","2fd5a3c2":"code","6f4a46d1":"code","5f8571a4":"code","35e7bb08":"code","a68d51b8":"code","e2896081":"code","0a834aff":"code","6d8143bf":"code","7184051f":"code","658a0876":"code","44c71858":"code","3e7ab38a":"code","77097c8f":"code","02cbc5da":"code","3f30e1fb":"code","2c8eb0ef":"markdown","a66fe27e":"markdown","859e759c":"markdown","dab2a160":"markdown","c72cefb8":"markdown","6af036b6":"markdown","b2b8a110":"markdown","4c92b8cb":"markdown","ea6b99b3":"markdown","849b443f":"markdown","13f811f8":"markdown","edd4db12":"markdown","95c501f1":"markdown","1251828a":"markdown","e6cc73b6":"markdown","aed3431f":"markdown","e9c24527":"markdown","64df3e4c":"markdown","956d40f9":"markdown","6dd5f4f6":"markdown","204588de":"markdown","be70c920":"markdown","7d5055c0":"markdown","e8263f09":"markdown","74cf0bdd":"markdown","13ca50e0":"markdown","5c602a62":"markdown","36b49e7a":"markdown","fcf5ac61":"markdown","236d1f35":"markdown","3fe2cf15":"markdown","8b1a8334":"markdown","9852b49e":"markdown","062406e8":"markdown","36e80647":"markdown","9adce533":"markdown","f9aca6c8":"markdown","830c7ba3":"markdown","500b9d56":"markdown","1d9b87f6":"markdown","781fa557":"markdown","44dc8012":"markdown","e2f6d102":"markdown","d53c3906":"markdown","02f6b38f":"markdown","91458d19":"markdown","95948c78":"markdown","71a2ea2e":"markdown","460e69c1":"markdown","f06dd333":"markdown","ed7ea39b":"markdown","1248f099":"markdown","33f60fd9":"markdown","d5066c90":"markdown","30d0cfe9":"markdown","5e10ade8":"markdown","a2f3eea7":"markdown","9279544d":"markdown","2fd89f04":"markdown","693be919":"markdown","844cbc9c":"markdown","5676f387":"markdown","0f513319":"markdown","732c1cea":"markdown","ea7918c7":"markdown","13c43a8e":"markdown","8517257a":"markdown","e6446ddf":"markdown","28f01fb8":"markdown","d131f3ee":"markdown","821b4d69":"markdown","dd9b56cc":"markdown","f62c4173":"markdown","7344f959":"markdown","3e5c6a14":"markdown","5251a887":"markdown","2dc30cd8":"markdown","9ba1bb0e":"markdown","6e695337":"markdown","485a33e3":"markdown","01c340b3":"markdown","586dfb48":"markdown","1880a213":"markdown","fed1228f":"markdown","1bcf6549":"markdown","4546d1a7":"markdown","5f2936e0":"markdown","b846bda0":"markdown","b8671416":"markdown","27258186":"markdown","f47a7536":"markdown","66cb2c0d":"markdown","54920a1d":"markdown","9667c6fb":"markdown","92527a1b":"markdown","6f5855da":"markdown","f523ed09":"markdown","33768e93":"markdown","c25c1669":"markdown","90e78244":"markdown","293ab2c9":"markdown","f916322b":"markdown","c7c78b2b":"markdown"},"source":{"b4ef879f":"import pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\n%matplotlib inline\nsns.set_style('darkgrid')\n%matplotlib inline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn import model_selection\nimport warnings\nwarnings.filterwarnings(\"ignore\")","479fcd3f":"df1=pd.read_csv('..\/input\/patients-condition\/Part1 - Normal.csv')","575679bf":"df1.head()","92804349":"df1.shape","d417784e":"df2=pd.read_csv('..\/input\/patients-condition\/Part1 - Type_H.csv')","8c6f67c7":"df2.shape","4fc6c721":"df2.head()","ce3d101c":"df2.shape","fd296269":"df3=pd.read_csv('..\/input\/patients-condition\/Part1 - Type_S.csv')","74c463e8":"df3.head()","a444256c":"df3.shape","6ce6a73e":"df=df1.append([df2,df3])","ac97bcda":"df.shape","4b33263e":"df.info()","c3b25635":"df.dtypes","dd530203":"df.isnull().sum()","6bafe9af":"df['Class'].value_counts()","d8de86ac":"df.loc[df['Class']=='tp_s','Class']='Type_S'\ndf.loc[df['Class']=='Nrmal','Class']='Normal'\ndf.loc[df['Class']=='type_h','Class']='Type_H'","f5a90034":"df['Class'].value_counts()","8169cf2f":"df['Class']=df['Class'].astype('category') #changing to category datatype","548da5c7":"df['Class'].nunique()","64b2a86d":"df.describe()","e8715f74":"f, axes = plt.subplots(1, 2, figsize=(17,7))\nsns.boxplot(x = 'P_incidence', data=df,  orient='h' , ax=axes[1],color='Green')\nsns.distplot(df['P_incidence'],  ax=axes[0],color='Green')\naxes[0].set_title('Distribution plot')\naxes[1].set_title('Box plot')\nplt.show()\n#checking count of outliers.\nq25,q75=np.percentile(df['P_incidence'],25),np.percentile(df['P_incidence'],75)\nIQR=q75-q25\nThreshold=IQR*1.5\nlower,upper=q25-Threshold,q75+Threshold\nOutliers=[i for i in df['P_incidence'] if i < lower or i > upper]\nprint('{} Total Number of outliers in P_incidence: {}'.format('\\033[1m',len(Outliers)))","6d398153":"f, axes = plt.subplots(1, 2, figsize=(17,7))\nsns.boxplot(x = 'P_tilt', data=df,  orient='h' , ax=axes[1],color='Green')\nsns.distplot(df['P_tilt'],  ax=axes[0],color='Green')\naxes[0].set_title('Distribution plot')\naxes[1].set_title('Box plot')\nplt.show()\n#checking count of outliers.\nq25,q75=np.percentile(df['P_tilt'],25),np.percentile(df['P_tilt'],75)\nIQR=q75-q25\nThreshold=IQR*1.5\nlower,upper=q25-Threshold,q75+Threshold\nOutliers=[i for i in df['P_tilt'] if i < lower or i > upper]\nprint('{} Total Number of outliers in P_tilt: {}'.format('\\033[1m',len(Outliers)))","177c9494":"f, axes = plt.subplots(1, 2, figsize=(17,7))\nsns.boxplot(x = 'L_angle', data=df,  orient='h' , ax=axes[1],color='Green')\nsns.distplot(df['L_angle'],  ax=axes[0],color='Green')\naxes[0].set_title('Distribution plot')\naxes[1].set_title('Box plot')\nplt.show()\n#checking count of outliers.\nq25,q75=np.percentile(df['L_angle'],25),np.percentile(df['L_angle'],75)\nIQR=q75-q25\nThreshold=IQR*1.5\nlower,upper=q25-Threshold,q75+Threshold\nOutliers=[i for i in df['L_angle'] if i < lower or i > upper]\nprint('{} Total Number of outliers in L_angle: {}'.format('\\033[1m',len(Outliers)))","72cbeca6":"f, axes = plt.subplots(1, 2, figsize=(17,7))\nsns.boxplot(x = 'S_slope', data=df,  orient='h' , ax=axes[1],color='Green')\nsns.distplot(df['S_slope'],  ax=axes[0],color='Green')\naxes[0].set_title('Distribution plot')\naxes[1].set_title('Box plot')\nplt.show()\n#checking count of outliers.\nq25,q75=np.percentile(df['S_slope'],25),np.percentile(df['S_slope'],75)\nIQR=q75-q25\nThreshold=IQR*1.5\nlower,upper=q25-Threshold,q75+Threshold\nOutliers=[i for i in df['S_slope'] if i < lower or i > upper]\nprint('{} Total Number of outliers in S_slope: {}'.format('\\033[1m',len(Outliers)))","e92b3d0d":"f, axes = plt.subplots(1, 2, figsize=(17,7))\nsns.boxplot(x = 'P_radius', data=df,  orient='h' , ax=axes[1],color='Green')\nsns.distplot(df['P_radius'],  ax=axes[0],color='Green')\naxes[0].set_title('Distribution plot')\naxes[1].set_title('Box plot')\nplt.show()\n#checking count of outliers.\nq25,q75=np.percentile(df['P_radius'],25),np.percentile(df['P_radius'],75)\nIQR=q75-q25\nThreshold=IQR*1.5\nlower,upper=q25-Threshold,q75+Threshold\nOutliers=[i for i in df['P_radius'] if i < lower or i > upper]\nprint('{} Total Number of outliers in P_radius: {}'.format('\\033[1m',len(Outliers)))","cd05aaf1":"f, axes = plt.subplots(1, 2, figsize=(17,7))\nsns.boxplot(x = 'S_Degree', data=df,  orient='h' , ax=axes[1],color='Green')\nsns.distplot(df['S_Degree'],  ax=axes[0],color='Green')\naxes[0].set_title('Distribution plot')\naxes[1].set_title('Box plot')\nplt.show()\n#checking count of outliers.\nq25,q75=np.percentile(df['S_Degree'],25),np.percentile(df['S_Degree'],75)\nIQR=q75-q25\nThreshold=IQR*1.5\nlower,upper=q25-Threshold,q75+Threshold\nOutliers=[i for i in df['S_Degree'] if i < lower or i > upper]\nprint('{} Total Number of outliers in S_Degree: {}'.format('\\033[1m',len(Outliers)))","9ad923c4":"f,axes=plt.subplots(1,2,figsize=(17,7))\ndf['Class'].value_counts().plot.pie(autopct='%1.1f%%',ax=axes[0])\nsns.countplot('Class',data=df,ax=axes[1])\naxes[0].set_title('Response Variable Pie Chart')\naxes[1].set_title('Response Variable Bar Graph')\nplt.show()","200bc4bc":"plt.figure(figsize=(15,7))\nsns.boxplot(x='Class', y='P_incidence', data= df)\nplt.show()","b73a37a5":"plt.figure(figsize=(15,7))\nsns.boxplot(x='Class', y='P_tilt', data= df)\nplt.show()","43471dac":"plt.figure(figsize=(15,7))\nsns.boxplot(x='Class', y='L_angle', data= df)\nplt.show()","8588b8aa":"plt.figure(figsize=(15,7))\nsns.boxplot(x='Class', y='S_slope', data= df)\nplt.show()","6e9081e7":"plt.figure(figsize=(15,7))\nsns.boxplot(x='Class', y='P_radius', data= df)\nplt.show()","63121dd5":"plt.figure(figsize=(15,7))\nsns.boxplot(x='Class', y='S_Degree', data= df)\nplt.show()","602d9256":"df.columns","87890586":"sns.pairplot(df)\nplt.show()","54efc797":"sns.pairplot(df,hue='Class')","001774cf":"class_summary=df.groupby('Class') #getting mean values of each class for all independent variables\nclass_summary.mean().reset_index()","4ce080e6":"col=['P_incidence','P_tilt','L_angle','S_slope','P_radius','S_Degree']\nfor i in col:\n    print('{} Ho: Class types does not affect the {}'.format('\\033[1m',i))\n    print('\\n')\n    print('{} H1: Class types affect the {}'.format('\\033[1m',i))\n    print('\\n')\n    df_normal=df[df.Class=='Normal'][i]\n    df_typeH=df[df.Class=='Type_H'][i]\n    df_typeS=df[df.Class=='Type_S'][i]\n    f_stats,p_value=stats.f_oneway(df_normal,df_typeH,df_typeS)\n    print('{} F_stats: {}'.format('\\033[1m',f_stats))\n    print('{} p_value: {}'.format('\\033[1m',p_value))\n    if p_value < 0.05:  # Setting our significance level at 5%\n        print('{} Rejecting Null Hypothesis.Class types has efect on {}'.format('\\033[1m',i))\n    else:\n        print('{} Fail to Reject Null Hypothesis.Class types has no effect on {}'.format('\\033[1m',i))\n    print('\\n')","a212f61e":"plt.figure(dpi = 120,figsize= (5,4))\nmask = np.triu(np.ones_like(df.corr()))\nsns.heatmap(df.corr(),mask = mask, fmt = \".2f\",annot=True,lw=1,cmap = 'plasma')\nplt.yticks(rotation = 0)\nplt.xticks(rotation = 90)\nplt.title('Correlation Heatmap')\nplt.show()","4d3555df":"for c in col:\n    #getting upper lower quartile values\n    q25,q75=np.percentile(df[c],25),np.percentile(df[c],75)\n    IQR=q75-q25\n    Threshold=IQR*1.5\n    lower,upper=q25-Threshold,q75+Threshold\n    Outliers=[i for i in df[c] if i < lower or i > upper]\n    print('{} Total Number of outliers in {} Before Imputing : {}'.format('\\033[1m',c,len(Outliers)))\n    print('\\n')\n    #taking mean of a column without considering outliers\n    df_include = df.loc[(df[c] >= lower) & (df[c] <= upper)]\n    mean=int(df_include[c].mean())\n    print('{} Mean of {} is {}'.format('\\033[1m',c,mean))\n    print('\\n')\n    #imputing outliers with mean\n    df[c]=np.where(df[c]>upper,mean,df[c])\n    df[c]=np.where(df[c]<lower,mean,df[c])\n    Outliers=[i for i in df[c] if i < lower or i > upper]\n    print('{} Total Number of outliers in {} After Imputing : {}'.format('\\033[1m',c,len(Outliers)))  \n    print('\\n')","368e999b":"le=LabelEncoder()\ndf['Class']=le.fit_transform(df['Class'])\ndf['Class'].value_counts()","05830091":"df['Class']=df['Class'].astype('category') #changing datatype to category.","bbb560f9":"f,axes=plt.subplots(1,2,figsize=(17,7))\ndf['Class'].value_counts().plot.pie(autopct='%1.1f%%',ax=axes[0])\nsns.countplot('Class',data=df,ax=axes[1],order=[2,0,1])\naxes[0].set_title('Response Variable Pie Chart')\naxes[1].set_title('Response Variable Bar Graph')\nplt.show()","ae5286bb":"# Arrange data into independent variables and dependent variables\nX=df.drop(columns='Class')\ny=df['Class'] #target","7e580f11":"X.describe()","2fd5a3c2":"X_Scaled=X.apply(zscore)","6f4a46d1":"X_Scaled.describe().T","5f8571a4":"# Split X and y into training and test set in 70:30 ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)","35e7bb08":"KNN = KNeighborsClassifier(n_neighbors= 5 , metric = 'euclidean' ) #Building knn with 5 neighbors","a68d51b8":"KNN.fit(X_train, y_train)\npredicted_labels = KNN.predict(X_test)","e2896081":"print('Accuracy on Training data:',KNN.score(X_train, y_train) )\nprint('Accuracy on Test data:',KNN.score(X_test, y_test) )","0a834aff":"cm = confusion_matrix(y_test, predicted_labels, labels=[0, 1,2])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"Normal\",\"Type_H\",\"Type_S\"]],\n                  columns = [i for i in [\"Normal\",\"Type_H\",\"Type_S\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True ,fmt='g')\nplt.show()","6d8143bf":"print(\"classification  Matrix:\\n\",classification_report(y_test,predicted_labels))","7184051f":"train_score=[]\ntest_score=[]\nfor k in range(1,51):\n    KNN = KNeighborsClassifier(n_neighbors= k , metric = 'euclidean' ) \n    KNN.fit(X_train, y_train)\n    train_score.append(KNN.score(X_train, y_train))\n    test_score.append(KNN.score(X_test, y_test))","658a0876":"plt.plot(range(1,51),train_score)\nplt.show()","44c71858":"plt.plot(range(1,51),test_score)\nplt.show()","3e7ab38a":"k=[1,3,5,7,9,11,13,15,17,19]\nfor i in k:\n    KNN = KNeighborsClassifier(n_neighbors=i, metric = 'euclidean' ) #Building knn with 5 neighbors\n    KNN.fit(X_train, y_train)\n    predicted_labels = KNN.predict(X_test)\n    print('Accuracy on Training data for k {} is {}:'.format(i,KNN.score(X_train, y_train)))\n    print('Accuracy on Test data for k {} is {}:'.format(i,KNN.score(X_test, y_test)))\n    print(\"classification  Matrix:\\n\",classification_report(y_test,predicted_labels))","77097c8f":"LR_model=LogisticRegression()\nKNN_model=KNeighborsClassifier(n_neighbors=13)\nGN_model=GaussianNB()\nsvc_model_linear = SVC(kernel='linear',C=1,gamma=.6)\nsvc_model_rbf = SVC(kernel='rbf',degree=2,C=.009)\nsvc_model_poly  = SVC(kernel='poly',degree=2,gamma=0.1,C=.01)","02cbc5da":"seed = 7\n# prepare models\nmodels = []\nmodels.append(('LR', LR_model))\nmodels.append(('KNN', KNN_model))\nmodels.append(('NB', GN_model))\nmodels.append(('SVM-linear', svc_model_linear))\nmodels.append(('SVM-poly', svc_model_poly))\nmodels.append(('SVM-rbf', svc_model_rbf))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed,shuffle=True)\n\tcv_results = model_selection.cross_val_score(model,  X,y, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","3f30e1fb":"seed = 7\n# prepare models\nmodels = []\nmodels.append(('LR', LR_model))\nmodels.append(('KNN', KNN_model))\nmodels.append(('NB', GN_model))\nmodels.append(('SVM-linear', svc_model_linear))\nmodels.append(('SVM-poly', svc_model_poly))\nmodels.append(('SVM-rbf', svc_model_rbf))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed,shuffle=True)\n\tcv_results = model_selection.cross_val_score(model,X_Scaled,y, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","2c8eb0ef":"**P_incidence:**\n\n>  **Mean and Median are nearly equal .**\n\n> **Distribution might be normal. we have 75 % of values are less than 72 but maxiumum value is 129**\n\n**P_tilt:**\n\n> **Mean and median are nearly equal.**\n\n> **Distribution might be normal.**\n\n> **It contains negative values**\n\n> **75 % of values are less than 22 but maximum value is 49 so there might be little right skewness**\n\n**L_angle:**\n\n> **Mean and Median are nearly equal. There is no  deviation.**\n\n> **Distribution might be normal**\n\n> **There might be few outliers because of the maximum value**\n\n**S_slope:**\n\n> **Mean and Median are nearly equal.**\n\n> **Towards the end there is little devation. 75% of values are lesser than 52 but maximum value is 121.**\n\n**P_radius:**\n\n> **Distribution might be normal.**\n\n> **There is no much Deviation.**\n\n**S_Degree:**\n\n> **Mean is greater than Median so there might be right skewness in the data .**\n\n> **We can see 75% of values are less than 41 but maximum value is 418 so there is obvious outliers in the data.**\n\n","a66fe27e":"**When the scaled values are used instead of normal values Logistic regression is performing well.**\n\n**Logistic Regression gives 81% accuracy with little standard deviation.**","859e759c":"# Classification Accuracy","dab2a160":"> **S_slope has huge values for Type_S class**\n\n>**Normal class has high s_slope compared to Type_H**","c72cefb8":"# 3.Data Analysis & Visulaization","6af036b6":"**Distribution and outlier analysis of numerical variables**","b2b8a110":"**We have 7 columns and 100 rows**","4c92b8cb":"**Shape of the dataset**","ea6b99b3":"> **We can see P_radius value is more for Normal Class**\n\n> **There is some extreme values for Type_s class**\n\n> **All classes has higher and lower Value**","849b443f":"> **It is Normally distributed**\n\n> **Little right skewness because of one outlier**","13f811f8":"**We have imbalanced target variable**\n\n**Every class is not equally distributed.**\n\n**48% of data is occupied by Type_S**\n\n**When you have imbalance dataset model does not learn about less distributed classes. This gives\npoor performance in unseen data**","edd4db12":"**S_Degree**","95c501f1":"> **Along the diagonal we can see the distribution of individual variable**\n\n> **P_incidence has  postive realtionship with all variables except P_radius. Relationship is higher for S_slope and L_angle**\n\n> **P_tilt has Higher Relationship with P_incidence and L_angle.There is no Relationship with s_slope and p_radius**\n\n> **L_angle has postive Relationship with p_tilt,s_slope and s_degree. It has no Relationship with P_radius**\n\n> **s_slope has positive Relationship with L_angle and s_degree**\n\n> **p_radius has no Relationship with s_degree,p_tilt,l_angle.**\n\n> **S_degree has no strong positive Relationship with any of the variables.**","1251828a":"> **Normality is maintained with very less extreme values**\n\n> **We can see three outliers exists in the column**","e6cc73b6":"1. **pelvic_incidence-P_incidence**\n2. **pelvic_tilt numeric-P_tilt**\n3. **lumbar_lordosis_angle-L_angle**\n4. **sacral_slope-S_slope**\n5. **pelvic_radius-P_radius**\n6. **degree_spondylolisthesis-S_degree**\n7. **class**","aed3431f":"**P_incidence**","e9c24527":"**Class vs S_slope**","64df3e4c":"> **P_Incidence Value is larger for Type_S Class. We can see some extreme values as well**\n\n> **Normal Value is slightly higher than Type_H**","956d40f9":"**Shape of the dataset**","6dd5f4f6":"**Shape of the dataset**","204588de":"# **Outlier Analysis**","be70c920":"**S_slope**","7d5055c0":"# **\u2022 CONTEXT: Medical research university X is undergoing a deep research on patients with certain conditions.University has an internal AI team. Due to confidentiality the patient\u2019s details and the conditions are masked by the client by providing different datasets to the AI team for developing a AIML model which can predict the condition of the patient depending on the received test results.**","e8263f09":"**Dataset-3**","74cf0bdd":"# Train - Test Split","13ca50e0":"# K-Fold CV for finding best model","5c602a62":"**P_tilt**","36b49e7a":"**Checking Datatypes**","fcf5ac61":"> **Mean of Type_S is slightly higher than rest two**\n\n> **Few cases Normal and Type_H also has huge values**","236d1f35":"> **There is right skewness due to one outlier**","3fe2cf15":"**Here we have three different class in our dataset**","8b1a8334":"# **KNN Classifier**","9852b49e":"**Missing Value Check**","062406e8":"**We will check with scaled values to see whether there is improvement in model**","36e80647":"# **6.Conclusion and improvisation:**","9adce533":"# Classification Report","f9aca6c8":"**Type_S variable has 48.4% of total values followed by Normal and Type_H**","830c7ba3":"**Shape of the dataset**","500b9d56":"# 5. Model training, testing and tuning","1d9b87f6":"**\u2022 DATA DESCRIPTION: The data consists of biomechanics features of the patients according to their current\nconditions. Each patient is represented in the data set by six biomechanics attributes derived from the shape and\norientation of the condition to their body part.**\n> **1. P_incidence**\n\n> **2. P_tilt**\n\n> **3. L_angle**\n\n> **4. S_slope**\n\n> **5. P_radius**\n\n> **6. S_degree**\n\n> **7. Class**","781fa557":"# **Scaling Independent Variables**","44dc8012":"# **Univariate Analysis**","e2f6d102":"> **We have scaled independent variables to corresponding z-score.**\n\n> **We can see Mean becomes close to zero and Standard Deviation becomes 1**","d53c3906":"# **Multivariate Analysis**","02f6b38f":"# Is the distribution of independent variables across normal,type_H and type_s, the same?","91458d19":"**Correlation between s_degree and p_incidence have high correlation.**\n\n**S_degree and p_radius has negative correlation**","95948c78":"**P_radius**","71a2ea2e":"**Dataset-2**","460e69c1":"**Dataset 1**","f06dd333":"**Target Variable:**","ed7ea39b":"> **Here training accuracy decreases when increase k value**","1248f099":"**It is clear that s_Degree of Type_S contains larger values.**\n","33f60fd9":"> **S_Degree has extreme values for type_S Class**\n\n>**Few Normal class also has huge values for S_Degree**","d5066c90":"> **Data is normally distributed**\n\n> **We can see outliers at both the ends.**","30d0cfe9":"**Distribution of Target Variable**","5e10ade8":"**Here we are using one-way anova to do statistical test.**","a2f3eea7":"**We have 7 columns and 60 rows**","9279544d":"**Checking First 5 Rows**","2fd89f04":"# 1. Import and warehouse data:","693be919":"**Checking First 5 Rows**","844cbc9c":"> **Data is Normally distributed and we can see one peakness in the center**\n\n> **It is has little skewness towards right side**\n\n> **We can see one outlier in negative end and few outliers in positive end.**","5676f387":"# **Encoding Target Variable**","0f513319":"> **For K=13 we have balanced train and test error**\n\n> **we can use k value as 13 because when we increase this value the precision becomes100% for class 2**","732c1cea":"# **Finding best K value**","ea7918c7":"# **Checking on Target Imbalance**","13c43a8e":"**Normal: 0**\n\n**Type_H: 1**\n\n**Type_S: 2**","8517257a":"**Class vs P_tilt**","e6446ddf":" # **Bi Variate Analysis**","28f01fb8":"**Class vs P_radius**","d131f3ee":"> **All the variables has significant effect on target class**\n\n> **class belongs to type_s has higher mean value for alomst all variables**\n\n> **Class belongs to normal has lower values for all variables**\n\n> **For almost all variables the distribution is normal**\n\n> **For Knn, k=13 we are getting balanced train and test error**\n\n> **We can use KNN as a final model because of balanced train and test error also the recall and precision values are good**\n\n> **Clear description on each variables may help to understand problem statement better because of medical domain**","821b4d69":"> **L_Angle has higher value for Type_S Class**\n\n>**We can see Normal class has higher values compared to type_H class**\n\n> **Each class contains one outlier**","dd9b56cc":"**Class vs L_angle**","f62c4173":"**Final Dataframe**","7344f959":"> **Along the diagonal we can see distribution of variable for three claases are not same.We can prove that statistically as well**\n\n> **It is evident that type_s class is more compared to other two**\n\n> **Normal class has higher values compared to Type_H**","3e5c6a14":"**There is no junk values in the dataset**\n\n**Class is object we need to change the datatype of this column**","5251a887":"> **Accuracy is more for KNN,LR and svm-linear. However the standard deviation is less for svm-linear model.**\n\n> **We can tell  svm-linear be a better algorithm for this dataset because of high accuracy and less Standard deviation**","2dc30cd8":"**As we have seen in our EDA we have very less outliers which needs to be handled**","9ba1bb0e":"**Class vs S_Degree**","6e695337":"**Checking First 5 Rows**","485a33e3":"> **Our model predicts Type_S correctly most of the time. Only two misclassification on this class**\n\n> **Misclassification of labels are more when predicting normal class**","01c340b3":"**5 Point Summary**","586dfb48":"**Shape of the dataset**","1880a213":"> **There is Positive Skewness in the data**\n\n> **Hugely affected by Outliers**","fed1228f":"> **Training Acuracy is 0.89 and Testing Accuracy is 0.77. Performance is less in test data.**\n\n> **This is due to overfitting of data**","1bcf6549":"**Class vs P_incidence**","4546d1a7":"**We are imputing outiers with mean**","5f2936e0":"**Pair Plot of independent Variables**","b846bda0":"**Final Dataset have 7 columns and 310 rows**","b8671416":"**L_angle**","27258186":"# **Hypotesis Testing**","f47a7536":"**Here tp_s and Type_S, Normal and Nrmal,Type_H and type_ h represents same class.**","66cb2c0d":"**We can see class type affects each and every independent variables**","54920a1d":"> **We have imputed all outliers with mean value**","9667c6fb":"# **\u2022 DOMAIN: Healthcare**","92527a1b":"# 2. Data cleansing:","6f5855da":"**Information about the data**","f523ed09":"# Importing Necessary Packages","33768e93":"**We have 7 columns and 150 rows**","c25c1669":"> **The maximum accuracy occures when k is less than 20.**\n\n> **We will fix k value as less than 20.**","90e78244":"# 4. Data Pre-processing","293ab2c9":"> **Precision for Normal class: It tells,out of all predicted normal class what fraction are predicted correctly**\n\n> **Recall(sensitivity or TPR) for Normal class: Out of all actual Normal class how much fraction we identified correctly**\n\n> **class 0 predicted correctly for 68% of time. similary for class 1 48% and class 2 98%**\n\n> **By F1 score we can say that precison and recall is balanced for class 0 by 60% and for class 1 by 56 %**\n\n> **We have maximum F1 score for class 2.**","f916322b":"**There is no missing value in the dataset**","c7c78b2b":"**Basic Model**"}}