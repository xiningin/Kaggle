{"cell_type":{"e5584ca1":"code","acc1d5a9":"code","05afa185":"code","470c4845":"code","fed7fdae":"code","19f0fd2d":"code","153d7ccc":"code","7e58bccf":"code","1ef314a7":"code","b78ccbe8":"code","6f909288":"code","2bacf081":"code","18ce11a6":"code","363a1e09":"markdown","776d4f45":"markdown","3121a1aa":"markdown","dd722b1e":"markdown","a5ccb0f1":"markdown","3670d67f":"markdown"},"source":{"e5584ca1":"# module imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\nfrom sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\nfrom scipy import stats\n\n# read data into dataframe\ndf = pd.read_csv(\"\/kaggle\/input\/heart-disease-cleveland-uci\/heart_cleveland_upload.csv\")\n\nprint('count(*): ' + str(len(df.index)) + '\\n')\n\nsns.countplot(x = 'condition', data = df)\nplt.title('Counts by Condition')\nplt.show()\n\nfor col in df.columns:\n    print(str(col) + ' null count: ' + str(df[col].isnull().sum()))","acc1d5a9":"df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']].corr()","05afa185":"cont_flds = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n\nfor i in cont_flds:\n    a = stats.pointbiserialr(df['condition'].values, df[i].values)\n    print(i + \": \" + str(a[0]))","470c4845":"sns.boxplot(x = 'condition', y = 'thalach', data = df)","fed7fdae":"fig, axes = plt.subplots(1,2, figsize = (12,6))\n\n\nsns.countplot(x = 'condition', data = df.loc[df['thalach'] <= 150], ax = axes[0])\nsns.countplot(x = 'condition', data = df.loc[df['thalach'] > 150], ax = axes[1])","19f0fd2d":"fig, axes = plt.subplots(1,3, figsize = (18,6))\n\n# chest pain types by sex\nsns.countplot(x = 'cp', hue = 'slope', data = df.loc[df['slope'] != 2], ax=axes[0])\nsns.countplot(x = 'thal', hue='slope', data = df.loc[df['slope'] != 2], ax = axes[1])\nsns.countplot(x = 'condition', hue='slope', data = df.loc[df['slope'] != 2], ax = axes[-1])","153d7ccc":"fig, axes = plt.subplots(1,4, figsize = (12,4))\n\nsns.distplot(a = df['age'].values, ax = axes[0])\n\n# resting blood pressure\nsns.distplot(a = df['trestbps'].values, ax = axes[1])\nsns.distplot(a = df['chol'].values, ax = axes[2])\nsns.distplot(a = df['thalach'].values, ax = axes[3])\n","7e58bccf":"# convert categorical variables into dummy indicators\ncat_flds = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\ntrain = pd.get_dummies(df, columns = cat_flds, dummy_na=False)\n\n# restructure dataframe so labels are at the end\ntrain = train[['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'sex_0',\n       'sex_1', 'cp_0', 'cp_1', 'cp_2', 'cp_3', 'fbs_0', 'fbs_1', 'restecg_0',\n       'restecg_1', 'restecg_2', 'exang_0', 'exang_1', 'slope_0', 'slope_1',\n       'slope_2', 'ca_0', 'ca_1', 'ca_2', 'ca_3', 'thal_0', 'thal_1',\n       'thal_2', 'condition']]\n\n# create label array\ny = train['condition'].values\n# create x array\nx = train.drop(labels = 'condition', axis = 1)\n\n# \nseed = 7\nnp.random.seed(seed)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=seed)","1ef314a7":"# Random Forest\nrf_clf = RandomForestClassifier(random_state=42)\nrf_y_train_pred = cross_val_predict(rf_clf, x_train, y_train, cv = 3)\n\n# Stochastic Gradient Descent\nsgd_clf = SGDClassifier(random_state = 42)\nsgd_y_train_pred = cross_val_predict(sgd_clf, x_train, y_train, cv = 3)","b78ccbe8":"print('------- Random Forest -------')\nprint('Confusion Matrix: \\n', confusion_matrix(y_train, rf_y_train_pred))\nprint('Precision:', precision_score(y_train, rf_y_train_pred))\nprint('Recall:', recall_score(y_train, rf_y_train_pred))\nprint('\\n')\nprint('------- SGD -------')\nprint('Confusion Matrix: \\n', confusion_matrix(y_train, sgd_y_train_pred))\nprint('Precision:', precision_score(y_train, sgd_y_train_pred))\nprint('Recall:', recall_score(y_train, sgd_y_train_pred))","6f909288":"rf_clf = RandomForestClassifier(random_state=42)\n\nparam_grid = [{'n_estimators': [3, 10, 15], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 5, 7]}]\n\n# refit by default is true but specifying because we want the estimator to be retrained on the whole train set\ngrid_search = GridSearchCV(rf_clf, param_grid, cv = 3, scoring = 'f1', return_train_score=True, refit=True)\n\ngrid_search.fit(x_train, y_train)\n\nbest_params = grid_search.best_params_\nprint('Best parameters for RF: ', best_params)","2bacf081":"# get best estimator and save it to a new object\nnew_rf_model = grid_search.best_estimator_\n\n# obtain predictions on test set and check f1 score\ny_test_pred = new_rf_model.predict(x_test)\nprint('f1_score:', f1_score(y_test, y_test_pred))","18ce11a6":"# probabilities example\ny_test_probabilites = new_rf_model.predict_proba(x_test)\ny_test_probabilites\n\n# probabilities of 1 class\nprobs_of_heart_disease = y_test_probabilites[:, 1]\nprobs_of_heart_disease","363a1e09":"The labels are relatively balanced and none of our fields contain null values. \n\nNext, we can try to see if there any relationships between the attributes. First, we'll look at the Pearson correlation coefficents between the continuous variables to see if any of them are positively\/negatively correlated. ","776d4f45":"#### Summary:\n\nA random forest model with max_features set to 2 and n_estimators equal to 15, was able to achieve 87% accuracy. If one desired, they could also obtain the probabilities for a patient having heart disease using the predict_proba method. That information may be more valuable for someone having a ~40% probability of having heart disease that may want to make changes to prevent them from potentially increasing their chances in the future. ","3121a1aa":"### Binary Classification: Stochastic Gradient Descent and Random Forest","dd722b1e":"### About:\n\nThis notebook uses the UCI Heart Disease dataset. It is a collection of 14 attributes including the labels. I'm including a copy of the short descriptions for each attribute below.\n\nKaggle Dataset:\nhttps:\/\/www.kaggle.com\/cherngs\/heart-disease-cleveland-uci\n\nSource Page:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\n\n### Objective:\n\nWe want to predict whether an individual has heart disease or not. First, we'll compare two models, Random Forest and Stochastic Gradient Descent, using cross validation. We'll take the most accurate model and perform a grid search to find the best parameters then refit the tuned model to the whole train set. Last, we'll use the F1 score to see how accurate our model is on the test set.  \n\n### Creators:\n\n1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D.\n\n### Attribute Info:\n\n* age: age in years\n* sex: sex \n    * 1 = male\n    * 0 = female\n* cp: chest pain type\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic\n* trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n* chol: serum cholestoral in mg\/dl\n* fbs: (fasting blood sugar > 120 mg\/dl)\n    * 1 = true\n    * 0 = false\n* restecg: resting electrocardiographic results\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* thalach: maximum heart rate achieved\n* exang: exercise induced angina\n    * 1 = yes\n    * 0 = no\n* oldpeak = ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n    * Value 0: upsloping\n    * Value 1: flat\n    * Value 2: downsloping\n* ca: number of major vessels (0-3) colored by flourosopy\n* thal: \n    * 0 = normal\n    * 1 = fixed defect\n    * 2 = reversable defect\n* condition: 0 = no disease, 1 = disease","a5ccb0f1":"None of the continuous variable seem to be strongly correlated (>= 0.5\/<= -0.5). Thalach, the maximum heart rate achieved is somewhat negatively correlated with age and oldpeak.\n\nWe can also take a look at the Point Biserial Correlation Coefficent between our target, condition (a binary variable), and the continuous variables in the data. ","3670d67f":"It doesn't seem like anything is strongly correlated. We can try plotting the attributes in different ways just to see if there are any interesting characteristics. "}}