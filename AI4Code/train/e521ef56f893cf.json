{"cell_type":{"60fe2e5a":"code","18c34596":"code","d3174a45":"code","4ee8f1e2":"code","a9a763bf":"code","f782fe1c":"code","4a38b584":"code","1454c7a1":"code","791f434d":"code","5d9689a6":"code","f2b3a834":"code","7017cea8":"code","8de6438f":"code","01ac5ae6":"code","24990d2b":"code","8aaef9de":"code","b004502e":"code","da713b94":"code","e43b807f":"code","229cbc13":"code","b44f4102":"code","3c13600d":"code","9d0e2ba7":"code","f2c24cf8":"code","3a33f801":"code","707ab16f":"code","0a219c38":"code","05ed7392":"code","173d7e3c":"code","075c7e1a":"code","4db74398":"code","2820b4e1":"code","630283e7":"code","8db9e08b":"code","eeb35707":"code","ee712685":"code","24d4523b":"code","2eca719d":"code","4208e3b6":"code","4a428ca9":"code","87f9dce2":"code","19cd0d42":"code","a68d0599":"markdown","40bc9cd3":"markdown","97b1491e":"markdown","15b7cf24":"markdown","691c2ea8":"markdown","5c1aea6d":"markdown","4ffac8e8":"markdown","038dc7ee":"markdown","eb694dd4":"markdown","0117cb86":"markdown","92241815":"markdown","06e385df":"markdown","4a54fdd6":"markdown","332c9638":"markdown","943ccc20":"markdown","f5af9ffc":"markdown","3dd68747":"markdown","fe6e0cab":"markdown","83ea6571":"markdown","a518ab1a":"markdown","becb9433":"markdown","6c5b635b":"markdown"},"source":{"60fe2e5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","18c34596":"import numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")","d3174a45":"N=10000","4ee8f1e2":"cov = [[3,3],[3,4]] # Kovarianzmatrix\nmu = [1,2] #Mittelpunkt der Punktwolke\nvar1,var2 = np.linalg.eig(cov)[0]  # Sog. Hauptkomponenten: Eigenwerte der Kovarianzmatrix\nsig1,sig2 = np.sqrt([var1,var2])   # Standardabweichungen: Wurzeln der Eigenwerte der Kovarianzmatrix","a9a763bf":"X = np.random.multivariate_normal(mu,cov,N)\nplt.scatter(X[:,0],X[:,1],s=1);","f782fe1c":"eigval,eigvec = np.linalg.eig(np.dot(X.T,X))\nnp.sqrt(eigval),var1,var2","4a38b584":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)","1454c7a1":"#Den Mittelwert k\u00f6nnen wir aus den Daten sch\u00e4tzen\nmu = np.mean(X,axis=0)\nplt.scatter(X[:,0],X[:,1],s=1)\nax = plt.gca()\n\nax.arrow(mu[0], mu[1], sig2*pca.components_[0,0], sig2*pca.components_[0,1],color='r', head_width=0.5, head_length=0.4)\nax.arrow(mu[0], mu[1], sig1*pca.components_[1,0], sig1*pca.components_[1,1],color='r', head_width=0.5, head_length=0.4)\nplt.axis('equal');","791f434d":"#Die Hauptkomponenten werden auf die Einheitsvektoren transformiert:\ntransformed_principal_compontents = pca.transform(pca.components_+mu)\ntransformed_principal_compontents","5d9689a6":"#Der selbe Plot nach der Transformation:\nXhat = pca.transform(X)\n\nplt.scatter(Xhat[:,0],Xhat[:,1],s=1)\nax = plt.gca()\n\n#plt.scatter(transformed_principal_compontents[:,0],transformed_principal_compontents[:,1],color='green',marker='x',s=250)\nax.arrow(0, 0, 0, sig1,color='r', head_width=0.5, head_length=0.4)\nax.arrow(0, 0, sig2, 0,color='r', head_width=0.5, head_length=0.4)\nplt.axis('equal');\nplt.xticks(np.arange(-7,7));\nplt.xlim(-7,7);","f2b3a834":"proj0=np.var(np.dot(pca.components_[0,:].reshape(-1,2),(X-mu).T))\nproj1=np.var(np.dot(pca.components_[1,:].reshape(-1,2),(X-mu).T))\nprint(f'Varianz entlang der 1.Hauptkomponente: {proj0:1.2f}')\nprint(f'Varianz entlang der 2.Hauptkomponente: {proj1:1.2f}')\n#Varianzen der den Daten zu Grunde liegenden Verteilung:\nprint(f'{var1:1.2f},{var2:1.2f}')","7017cea8":"pca.singular_values_\/np.sqrt(N),sig1,sig2","8de6438f":"!ls ..\/input\/spiraldatensatz\/","01ac5ae6":"import pandas as pd\ndf = pd.read_csv('..\/input\/spiraldatensatz\/spiraldatensatz.csv')\ndf.head()","24990d2b":"df.plot(kind='scatter',x='x1',y='x2');","8aaef9de":"df.plot(kind='scatter',x='x2',y='x3');","b004502e":"df.plot(kind='scatter',x='x1',y='x3');","da713b94":"from sklearn.decomposition import PCA\ntrf = PCA(n_components=3)","e43b807f":"trf.fit(df.values)","229cbc13":"trf.singular_values_","b44f4102":"import matplotlib.pyplot as plt\nplt.figure(1,figsize=(15,5))\nplt.subplot(1,2,1)\nplt.plot(trf.explained_variance_ratio_);\nplt.subplot(1,2,2)\nplt.bar([1,2,3],trf.explained_variance_);","3c13600d":"trf = PCA(n_components=2)\nXhat = trf.fit_transform(df.values)","9d0e2ba7":"plt.scatter(Xhat[:,0],Xhat[:,1],s=0.1);","f2c24cf8":"#trf = PCA(n_components=??)\n#Xhat = trf.fit_transform(df.values)\n#plt.scatter(Xhat[??],Xhat[??],s=0.1)","3a33f801":"trf = PCA(n_components=3)\nXhat = trf.fit_transform(df.values)\ndf_pca = pd.DataFrame({'pc1':Xhat[:,0], 'pc2':Xhat[:,1], 'pc3':Xhat[:,2]})","707ab16f":"df['s']=df.x1**2\ndf['s'].iloc[0]=1000 #n\u00f6tig, ansonsten zeichnet plotly alle Scatterpunkte zu gross?\ndf.head()","0a219c38":"import plotly.express as px\npx.scatter_3d(df, x='x1', y='x2', z='x3',size='s')","05ed7392":"df = pd.read_csv('\/kaggle\/input\/csvversion-der-kreuz-kreis-und-plusdaten\/KreuzKreisPlus_train.csv')\ndf.head()","173d7e3c":"y=df.target\nX=df.iloc[:,:-2].values\nX.shape","075c7e1a":"import matplotlib.pyplot as plt\nimport random\nX2x2 = X.reshape(-1,15,15)\ni=random.randint(0,X2x2.shape[0]-1)\nplt.imshow(X2x2[i].reshape(15,15))\ntargetlabel={0:'Kreuz', 1: 'Kreis', 2:'Plus'}\nplt.title(f'Label: {targetlabel[y[i]]}');","4db74398":"from sklearn.decomposition import PCA\nn_components=50 #Wieviele Hauptkomponenten wollen wir?\ntrf = PCA(n_components=n_components)\ntrf.fit_transform(X,None)","2820b4e1":"plt.figure(1,figsize=(15,20)),plt.subplot(1,3,1),plt.imshow(trf.components_[0].reshape(-1,15));\nplt.subplot(1,3,2),plt.imshow(trf.components_[1].reshape(-1,15));\nplt.subplot(1,3,3),plt.imshow(trf.components_[2].reshape(-1,15));","630283e7":"plt.bar(np.arange(trf.singular_values_.size),trf.singular_values_);\nplt.title('Singul\u00e4rwerte: Eigenwerte der Kovarianzmatrix');","8db9e08b":"trf.explained_variance_ratio_","eeb35707":"import matplotlib.pyplot as plt\nplt.figure(1,figsize=(15,5))\nplt.plot(100*np.cumsum(trf.explained_variance_ratio_));\nplt.title('Anteil der durch die ersten $m$ Kompontenten erkl\u00e4reten Varianz [in Prozent]')\nplt.ylim(0,100)\nplt.xlabel('$m$');","ee712685":"Xhat = trf.fit_transform(X,None)\nX.shape,Xhat.shape","24d4523b":"X_reconstructed = trf.inverse_transform(Xhat)","2eca719d":"import matplotlib.pyplot as plt\nimport random\nX2x2 = X.reshape(-1,15,15)\ni=random.randint(0,X2x2.shape[0]-1)\nplt.subplot(1,2,1)\nim = X2x2[i].reshape(15,15)\nim_mean = np.mean(X2x2,axis=0)  \nplt.imshow(im-im_mean) #Mittelwert muss abgezogen werden\nplt.title('Original')\nplt.subplot(1,2,2)\nplt.imshow(X_reconstructed[i].reshape(15,15))\nplt.title(f'PCA-Rekonstruktion mit n={n_components}')\ntargetlabel={0:'Kreuz', 1: 'Kreis', 2:'Plus'}\nplt.suptitle(f'Label: {targetlabel[y[i]]}');","4208e3b6":"from sklearn.tree import DecisionTreeClassifier\nclf=DecisionTreeClassifier(max_depth=5)","4a428ca9":"from sklearn.pipeline import Pipeline\npip1 = Pipeline([('dt',clf)])\npip2 = Pipeline([('pca',trf),('dt',clf)])","87f9dce2":"pip1.fit(X,y)\npip1.score(X,y)","19cd0d42":"pip2.fit(X,y)\npip2.score(X,y)","a68d0599":"Wirklich gegeben sind nur die Daten X:","40bc9cd3":"# Strukturerkennung (unsupervised machine learning) mit PCA","97b1491e":"Offenbar ist die dritte Komponente deutlich weniger relevant als die ersten zwei. betrachten wir also nur die ersten zwei:","15b7cf24":"Fast alle Datens\u00e4tze haben eine Struktur, bei der man irrelevante Hauptkomponenten abschneiden kann. Bitte beachten Sie, dass die PCA f\u00fcr ordinale oder kathegorische Daten eher ungeeignet ist.","691c2ea8":"Beim Trainieren von Klassifikatoren kann das einen erheblichen Zeitgewinn erzeugen. Auch kann u.U. der Klassifikator sehr einfach gew\u00e4hlt werden, weil wir ein gutes Feature Engineering gemacht haben.","5c1aea6d":"# Spiraldatensatz\nWir suchen eine Struktur in diesem Datensatz:","4ffac8e8":"# Hauptkomponentenanalyse und Eigenfaces","038dc7ee":"...und berechnen die Hauptkomponenten:","eb694dd4":"## 2d-Beispiel zur Intuitionsgewinnung\nWir basteln uns einen einfachen Datensatz mit $N$ Zeilen:","0117cb86":"Die erste Hauptkomponente k\u00fcmmert sich um Formen, die wie Kreise aussehen. Die zweite unterscheidet Plusse und Kreuze. Ab der Dritten werden bereits variationen im Bild ber\u00fccksichtigt, welche schwieriger zu interpretieren sind, aber wohl haupts\u00e4chlich unterschiedliche, nicht ganz geschlossene Kreise betrifft?","92241815":"Die `singular_values_` sind die Varianzen des Datensatzes in Richtung der entsprechenden Hauptkomponenten. Sie sagen uns etwas dar\u00fcber aus, welchen Anteil der Gesamt-Varianz des Datensatzes auf Variationen in der Richtung der entsprechenden Hauptkomponente zur\u00fcckgeht.\nSch\u00e4tzungen der Eigenvektoren gibt's unter dem Attribut `components_`:","06e385df":"Indem wir die Daten auf die ersten paar Hauptkomponenten projizieren: ","4a54fdd6":"erhalten wir massiv weniger Daten, aber nur eine geringe Einbusse an Datenqualit\u00e4t.","332c9638":"# 3D-Plots\nNat\u00fcrlich k\u00f6nnen wir diesen einfachen 3d-Datensatz auch visualisieren. Aber bis man etwas sieht, ist das gar nicht so einfach:","943ccc20":"Wir zeichnen nun die sog. Hauptkomponenten dieses Datensatzes:","f5af9ffc":"Mit der Hauptkomponentenanalyse finden wir die wichtigen Richtungen in diesem Datensatz:","3dd68747":"Die Information in der n\u00e4chsten Zellen kennen wir eigentlich nicht!","fe6e0cab":"Dieser Datensatz hat eine sehr spezielle Struktur! Er besteht aus der obigen Spirale, welche schr\u00e4g im 3D-Raum liegt.   \n**Aufgabe:** Erzeugen Sie einen Plot, welcher von der Seite auf diese Spirale schaut. ","83ea6571":"Dies sind zwar Vektoren der L\u00e4nge 225, weil wir die Bilder als Vektoren betrachten. Sobald man sie aber als 15x15-Bilder anschaut, erkennt man mehr:","a518ab1a":"**Aufgabe**: Erstellen Sie einen Parameterplot f\u00fcr die Gr\u00f6sse `n_components`!","becb9433":"# Eigenfaces\nEigenfaces ist nur ein anderer Name f\u00fcr Eigenvektoren im Bildraum. Die obigen Beispiele zeigten 2-dimensionale und 3-dimensionale Daten. 10x10-Bilder entsprechen 100-dimensionalen Daten, aber auch in diesem Bildraum gibt es Richtungen, in welchen ein Bilddatensatz haupts\u00e4chlich variiert. Probieren wir's aus!\nWir laden unseren xo+-Datensatz...","6c5b635b":"Was diese Hauptkomponenten sind, ist wohl intuitiv klar (und die wichtigste Einsicht hier!). Es handelt sich um die senkrecht aufeinanderstehenden Richtungen, in denen die Daten am meisten variieren. \n\nWeniger klar (aber trotzdem wahr) ist, dass es sich bei diesen zwei Richtungen um die Eigenvektoren der Kovarianzmatrix handelt.\nZur Erinnerung: F\u00fcr eine Matrix `A` sind $v$ und $\\lambda$ ein Eigenvektor-Eigenwert-Paar, wenn gilt:\n$$A\\cdot v=\\lambda v$$\nBei Rotationen sind die Drehachsen z.B. Eigenvektoren zum Eigenwert 1.\nIn `sklearn` sind die Eigenwerte unter `singular_values_` abgelegt. Dabei handelt es sich um Sch\u00e4tzungen der wahren Werte (`sig1` und `sig2`):"}}