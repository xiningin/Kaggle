{"cell_type":{"62e05fff":"code","676121dd":"code","673b1178":"code","d8cd54e8":"code","4a724fc6":"code","91bd7bd2":"code","9fd80b31":"code","9043644e":"code","08c10e4a":"code","7a9927dc":"code","d2bd0d8b":"code","61cb1225":"code","22c4c385":"code","7a8fda59":"code","1858f0aa":"code","f134c43b":"code","af47d2d0":"code","a824b802":"code","261d33d5":"code","8cade02c":"code","5843275a":"code","640273a4":"code","b7005503":"code","559f6f63":"code","a56b5e08":"code","353ca6f8":"code","e119520c":"code","fbca2859":"code","78e1145b":"code","36a8cb85":"code","37ea2912":"code","7c9a13f0":"code","95af29db":"code","393476f7":"code","446d723c":"code","792ca97a":"code","15029c2a":"code","56b8eb45":"code","2377a0b1":"code","a6d2c93e":"code","d393d997":"code","33d4803a":"code","4f81cb36":"code","539dba48":"code","bd2a1cc5":"code","66a64bd1":"code","6a40d6ee":"code","25ebd41d":"code","4a930bee":"code","84e8adbb":"code","79caa1c4":"code","cbe027ca":"code","82e4369f":"code","e508f493":"code","b20d4ef3":"code","27741284":"code","3f3f9c39":"code","982013d0":"code","c0b24782":"code","17c99dc5":"code","fc118142":"code","8059f55f":"code","b79477f5":"code","8c51c814":"code","cef7efd1":"code","fe26036c":"code","600d8d7e":"code","ebcea227":"code","cd113208":"code","881e5f1a":"code","40f7140d":"code","00eee1ab":"markdown","2ffcfcc5":"markdown","6c9b7ec1":"markdown","0ad96e71":"markdown","7473ce30":"markdown","cede5891":"markdown","e5fa1d87":"markdown","029c9d67":"markdown","77919fff":"markdown","7d6149a4":"markdown","d9abc563":"markdown","a7d089ca":"markdown","534874c3":"markdown","1a77fc31":"markdown","6673c499":"markdown","3ad365d7":"markdown","66711749":"markdown","1c1dad94":"markdown","8922896a":"markdown","35ded7f7":"markdown","7e780874":"markdown","e9b0be15":"markdown","c4bcb752":"markdown","de137925":"markdown","cbcd7743":"markdown","4a990828":"markdown","64212e73":"markdown","193198d4":"markdown","9c41eff9":"markdown","446d75dd":"markdown","a5468274":"markdown","61afb01f":"markdown","f87f15dd":"markdown","0373a3ad":"markdown","9fec8bb5":"markdown","659c43ce":"markdown","54a73ed2":"markdown","47304220":"markdown","d1c71198":"markdown","bdb37305":"markdown","8e103647":"markdown","9e5829b7":"markdown","3f671764":"markdown"},"source":{"62e05fff":"import warnings\nwarnings.filterwarnings('ignore')","676121dd":"# installing libraries\n\n!cp -rp \/kaggle\/input\/pyspellchecker-and-ekpharasis-libs-for-python \/kaggle\/working\/\n!cp -rp \/kaggle\/input\/transformers280 \/kaggle\/working\/\n\n!pip install \/kaggle\/working\/pyspellchecker-and-ekpharasis-libs-for-python\/pyspellchecker-0.5.4-py2.py3-none-any.whl >> \/dev\/null\n\n!pip install \/kaggle\/working\/pyspellchecker-and-ekpharasis-libs-for-python\/ekphrasis-0.5.1\/ekphrasis-0.5.1\/ >> \/dev\/null\n!python \/kaggle\/working\/pyspellchecker-and-ekpharasis-libs-for-python\/ekphrasis-0.5.1\/ekphrasis-0.5.1\/setup.py install >> \/dev\/null\n!mkdir -p \/root\/.ekphrasis\/stats \n!cp -r \/kaggle\/working\/pyspellchecker-and-ekpharasis-libs-for-python\/stats\/* \/root\/.ekphrasis\/stats >> \/dev\/null\n\n!pip install \/kaggle\/working\/transformers280\/transformers-2.8.0 >> \/dev\/null\n\n!rm -rf \/kaggle\/working\/*","673b1178":"in_testing_mode = True","d8cd54e8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport copy\nimport os\nimport re # regular expression\n\nfrom nltk.tokenize import SpaceTokenizer\n\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\n\nfrom spellchecker import SpellChecker\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nfrom transformers import RobertaConfig, RobertaModel, BertPreTrainedModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n\nimport datetime, time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","4a724fc6":"input_dir = '\/kaggle\/input'\nworking_dir = '\/kaggle\/working'\ndata_dir = os.path.join(input_dir, \"tweet-sentiment-extraction\/\")\nmodel_dir = os.path.join(input_dir, \"robertabase\/\")\n\ntrain_file = os.path.join(data_dir, \"train.csv\")\ntest_file = os.path.join(data_dir, \"test.csv\")\n\nsample_submission_file = os.path.join(data_dir, \"sample_submission.csv\")\nsubmission_file = os.path.join(working_dir, \"submission.csv\")","91bd7bd2":"dfTrain = pd.read_csv(train_file,encoding=\"utf-8\")\ndfTest = pd.read_csv(test_file,encoding=\"utf-8\")\nprint(\"dfTrain\")\nprint(dfTrain.describe())\nprint(\"\\n====================================\\n\")\nprint(\"dfTest\")\nprint(dfTest.describe())","9fd80b31":"print(\"dfTrain\")\nprint(dfTrain.info())\nprint(\"\\n====================================\\n\")\nprint(\"dfTest\")\nprint(dfTest.info())","9043644e":"dfTrain.dropna(inplace=True,axis=0)\ndfTrain = dfTrain[~dfTrain.text.str.contains(\"\u00ef|\u00bf|\u00bd\")] # drop columns that include some unknown chars(\"\u00ef|\u00bf|\u00bd\")\ndfTrain.reset_index(inplace=True,drop=True)\n\nprint(\"dfTrain describe\")\nprint(dfTrain.describe())\nprint(\"\\n====================================\\n\")\nprint(\"dfTrain info\")\nprint(dfTrain.info())","08c10e4a":"dfTrain.head(20)","7a9927dc":"sns.countplot(x= 'sentiment',data = dfTrain)","d2bd0d8b":"def jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","61cb1225":"dfJaccards = copy.deepcopy(dfTrain)\ndfJaccards[\"jaccard\"] = dfJaccards.apply(lambda df: jaccard(df.text, df.selected_text), axis=1)\nprint(\"Ratios of selected text in text\")\ndfJaccards = dfJaccards[['sentiment','jaccard']].groupby(['sentiment']).mean()\ndfJaccards","22c4c385":"dfJaccards.plot.bar()\ndel(dfJaccards)","7a8fda59":"def stringtowords(text, selected_text):\n    text_words = str(text).lower().split()\n    selected_text_words = str(selected_text).lower().split()\n    selected_length = len(selected_text_words)\n    start_char = \"\".join(text_words).find(\"\".join(selected_text_words))\n    if(start_char >= 0 and selected_length > 0):\n        remaining_char = start_char\n        for i,word in enumerate(text_words):\n            remaining_char-=len(word)\n            if remaining_char < 0:\n                selected_text_words = text_words[i:i+selected_length]\n                break\n            \n    text_word_numbers = [i for i in range(len(text_words))]\n    return text_words, text_word_numbers ,selected_text_words","1858f0aa":"test_string = \"is back home now gonna miss every one\" #dfTrain[dfTrain[\"textID\"] == \"af3fed7fc3\"]\ntest_selected_string = \"onna mis\"\nprint(\"Sample text:\\t\", test_string)\nprint(\"Sample selected text:\\t\", test_selected_string)\n\noutput_tweet_list, output_tweet_word_numbers, output_selected_list = stringtowords(test_string,test_selected_string)\nprint(\"\\nstringtowords outputs\")\nprint(\"output tweet list: \",output_tweet_list)\nprint(\"output tweet word numbers:\",output_tweet_word_numbers)\nprint(\"output selected text list: \",output_selected_list)","f134c43b":"dfTrain['text_w'], dfTrain['text_w_n'], dfTrain['selected_w'] = zip(*dfTrain.apply(lambda df: stringtowords(df.text,df.selected_text), axis=1))\ndfTrain.head()","af47d2d0":"print(\"text\")\nprint(dfTrain[\"text_w\"].iloc[0])\nprint(\"selected_text\")\nprint(dfTrain[\"selected_w\"].iloc[0])","a824b802":"def fix_difference(old_word, new_word, oldtext_w_n, newtext_w, newtext_w_n, word_ind, expand_number):\n    new_word = [word for word in new_word if word != \"\"] # simple empty word delete function\n    if len(new_word) == 0:\n        newtext_w.pop(word_ind+expand_number)\n        if newtext_w_n is not None:\n            newtext_w_n.pop(word_ind+expand_number)\n        expand_number -= 1\n    elif (len(new_word) > 1) | (new_word[0] != old_word):\n        for i, word in enumerate(new_word):\n            if i == 0:\n                newtext_w[word_ind+expand_number] = word\n            else:\n                expand_number += 1\n                newtext_w.insert(word_ind+expand_number,word)\n                if newtext_w_n is not None:\n                    newtext_w_n.insert(word_ind+expand_number,oldtext_w_n[word_ind])\n    return newtext_w, newtext_w_n, expand_number","261d33d5":"def cleanstring(oldtext_w, oldtext_w_n = None):\n    dictionary = {'w\/':'with',\n                  'dont':'do not',\n                  'doesnt':'does not',\n                  'didnt':'did not',\n                  'cant':'can not',\n                  'couldnt':'could not',\n                  'wouldnt':'would not',\n                  'shouldnt':'should not'\n                 }\n    expand = 0\n    newtext_w = copy.deepcopy(oldtext_w)\n    newtext_w_n = copy.deepcopy(oldtext_w_n)\n    for word_ind ,current_word in enumerate(oldtext_w):\n        new_word = current_word\n        if current_word in dictionary:\n            new_word = dictionary[current_word]\n        \n        new_word = new_word.split()\n        newtext_w, newtext_w_n, expand = fix_difference(current_word, new_word, oldtext_w_n, newtext_w, newtext_w_n, word_ind, expand)\n    return newtext_w, newtext_w_n","8cade02c":"test_string = [\"u\",\"cant\",\"seperate\",\"it\",\"w\/\",\"this\"]\nprint(\"Before cleanstring\")\nprint(test_string)\nprint(\"After cleanstring\")\nprint(cleanstring(test_string)[0])","5843275a":"dfTrain['text_w'], dfTrain['text_w_n'] = zip(*dfTrain.apply(lambda df: cleanstring(df.text_w,df.text_w_n), axis=1))\ndfTrain['selected_w'], _ = zip(*dfTrain.apply(lambda df: cleanstring(df.selected_w), axis=1))\nprint(\"===== E> Replaced unpunctuated texts with punctuated ones <3 =====\")","640273a4":"text_processor = TextPreProcessor(\n    # terms that will be normalized\n    normalize=['url'],\n    # terms that will be annotated\n    annotate={},\n    fix_html=True,  # fix HTML tokens\n    \n    # corpus from which the word statistics are going to be used \n    # for word segmentation \n    segmenter=\"twitter\", \n    \n    # corpus from which the word statistics are going to be used \n    # for spell correction\n    corrector=\"twitter\", \n    \n    unpack_hashtags=False, #True, # perform word segmentation on hashtags\n    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n    spell_correct_elong=True,  # spell correction for elongated words\n    \n    # select a tokenizer. We selected SpaceTokenizer, you can understand it in \n    # add spaces before word section\n    tokenizer=SpaceTokenizer().tokenize,\n    \n    # list of dictionaries, for replacing tokens extracted from the text,\n    dicts=[emoticons]\n)\n\ndef tweetCleaner(oldtext_w, oldtext_w_n = None):\n    expand = 0\n    newtext_w = copy.deepcopy(oldtext_w)\n    newtext_w_n = copy.deepcopy(oldtext_w_n)\n    for word_ind ,current_word in enumerate(oldtext_w):\n        new_word = current_word\n        new_word = text_processor.pre_process_doc(current_word)\n        for i,word in enumerate(new_word):\n            if \"<\" in word and \">\" in word:  # unpack the tags that tweetcleaner created\n                new_word[i] = re.sub(\"[<>]\",\"\",word)\n        newtext_w, newtext_w_n, expand = fix_difference(current_word, new_word, oldtext_w_n, newtext_w, newtext_w_n, word_ind, expand)\n    return newtext_w, newtext_w_n","b7005503":"test_string = [\"@rootofarch\",\"https:\/\/kaggle.com\",\"is\",\"working!!!!\",\":D\",\"#tbt\"]\nprint(\"Before tweetCleaner\")\nprint(test_string)\nprint(\"After tweetCleaner\")\nprint(tweetCleaner(test_string)[0])","559f6f63":"dfTrain['text_w'], dfTrain['text_w_n'] = zip(*dfTrain.apply(lambda df: tweetCleaner(df.text_w,df.text_w_n), axis=1))\ndfTrain['selected_w'], _ = zip(*dfTrain.apply(lambda df: tweetCleaner(df.selected_w), axis=1))\nprint(\"===== E> Applied ekphrasis text preprocessor <3 =====\")","a56b5e08":"def cleanstring2(oldtext_w, oldtext_w_n = None):\n    dictionary = {'ahah':'laugh',\n                  'haha':'laugh',\n                  'ahhah':'laugh',\n                  'haaha':'laugh',\n                  'hahha':'laugh',\n                  'ahaah':'laugh',\n                  'oxox':'kiss',\n                  'xoxo':'kiss',\n                  'xxox':'kiss',\n                  'ooxo':'kiss'\n                 }\n    expand = 0\n    newtext_w = copy.deepcopy(oldtext_w)\n    newtext_w_n = copy.deepcopy(oldtext_w_n)\n    for word_ind ,current_word in enumerate(oldtext_w):\n        new_word = current_word\n        for repeated in dictionary.keys():\n            if repeated in current_word:\n                new_word = dictionary[repeated]\n\n        new_word = new_word.split()\n        newtext_w, newtext_w_n, expand = fix_difference(current_word, new_word, oldtext_w_n, newtext_w, newtext_w_n, word_ind, expand)\n    return newtext_w, newtext_w_n","353ca6f8":"test_string = [\"ahhahahahaha\",\"it\",\"is\",\"a\",\"dummy\",\"string\",\"xoxoxooxoxoxo\"]\nprint(\"Before cleanstring2\")\nprint(test_string)\nprint(\"After cleanstring2\")\nprint(cleanstring2(test_string)[0])","e119520c":"dfTrain['text_w'], dfTrain['text_w_n'] = zip(*dfTrain.apply(lambda df: cleanstring2(df.text_w,df.text_w_n), axis=1))\ndfTrain['selected_w'], _ = zip(*dfTrain.apply(lambda df: cleanstring2(df.selected_w), axis=1))\nprint(\"===== E> Some repeated structures replaced with their meanings <3 =====\")","fbca2859":"spell = SpellChecker(distance=1) # Set max how many chars can be misstyped in a word\n\ndef correctSpelling(oldtext_w, oldtext_w_n = None):\n    expand = 0\n    newtext_w = copy.deepcopy(oldtext_w)\n    newtext_w_n = copy.deepcopy(oldtext_w_n)\n    for word_ind ,current_word in enumerate(oldtext_w):\n        new_word = spell.correction(current_word)\n        new_word = new_word.split()\n        newtext_w, newtext_w_n, expand = fix_difference(current_word, new_word, oldtext_w_n, newtext_w, newtext_w_n, word_ind, expand)\n    return newtext_w, newtext_w_n","78e1145b":"test_string = [\"it\",\"is\",\"not\",\"an\",\"englidh\",\"wotd\"]\nprint(\"Before correctSpelling\")\nprint(test_string)\nprint(\"After correctSpelling\")\nprint(correctSpelling(test_string)[0])","36a8cb85":"dfTrain['text_w'], dfTrain['text_w_n'] = zip(*dfTrain.apply(lambda df: correctSpelling(df.text_w,df.text_w_n), axis=1))\ndfTrain['selected_w'], _ = zip(*dfTrain.apply(lambda df: correctSpelling(df.selected_w), axis=1))\nprint(\"===== E> Fixed 1 char typos <3 =====\")","37ea2912":"def addSpace(oldtext_w):\n    newtext_w = copy.deepcopy(oldtext_w)\n    for word_ind ,current_word in enumerate(oldtext_w):\n            newtext_w[word_ind] = \" \"+current_word\n    return newtext_w","7c9a13f0":"test_string = [\"hello\",\"world\"]\nprint(\"Before addSpace\")\nprint(test_string)\nprint(\"After addSpace\")\nprint(addSpace(test_string))","95af29db":"dfTrain['text_w'] = dfTrain.apply(lambda df: addSpace(df.text_w), axis=1)\ndfTrain['selected_w'] = dfTrain.apply(lambda df: addSpace(df.selected_w), axis=1)\nprint(\"===== E> Added spaces before every word <3 =====\")","393476f7":"dfTest['text_w'], dfTest['text_w_n'], _ = zip(*dfTest.apply(lambda df: stringtowords(df.text,\"\")             , axis=1))\ndfTest['text_w'], dfTest['text_w_n']    = zip(*dfTest.apply(lambda df: cleanstring(df.text_w,df.text_w_n)    , axis=1))\ndfTest['text_w'], dfTest['text_w_n']    = zip(*dfTest.apply(lambda df: tweetCleaner(df.text_w,df.text_w_n)   , axis=1))\ndfTest['text_w'], dfTest['text_w_n']    = zip(*dfTest.apply(lambda df: cleanstring2(df.text_w,df.text_w_n)   , axis=1))\ndfTest['text_w'], dfTest['text_w_n']    = zip(*dfTest.apply(lambda df: correctSpelling(df.text_w,df.text_w_n), axis=1))\ndfTest['text_w']                        = dfTest.apply(lambda df: addSpace(df.text_w), axis=1)\ndfTest.sample(n=5, random_state=50)","446d723c":"print('Loading RoBERTa tokenizer...')\ntokenizer = RobertaTokenizer.from_pretrained(model_dir)","792ca97a":"def robertaTokenize(oldtext_w, oldtext_w_n = None):\n    expand = 0\n    newtext_w = copy.deepcopy(oldtext_w)\n    newtext_w_n = copy.deepcopy(oldtext_w_n)\n    for word_ind ,current_word in enumerate(oldtext_w):\n        new_word = tokenizer.tokenize(current_word)\n        newtext_w, newtext_w_n, expand = fix_difference(current_word, new_word, oldtext_w_n, newtext_w, newtext_w_n, word_ind, expand)\n    return newtext_w, newtext_w_n","15029c2a":"test_string = [\" hello\",\" worldasd\"]\nprint(\"Before robertaTokenize\")\nprint(test_string)\nprint(\"After robertaTokenize\")\nprint(robertaTokenize(test_string)[0])","56b8eb45":"dfTrain['text_w'], dfTrain['text_w_n'] = zip(*dfTrain.apply(lambda df: robertaTokenize(df.text_w,df.text_w_n), axis=1))\ndfTrain['selected_w'], _ = zip(*dfTrain.apply(lambda df: robertaTokenize(df.selected_w), axis=1))\nprint(\"===== E> Tokenized every word using RoBERTa tokenization <3 =====\")","2377a0b1":"dfTest['text_w'], dfTest['text_w_n'] = zip(*dfTest.apply(lambda df: robertaTokenize(df.text_w,df.text_w_n), axis=1))\ndfTest.sample(n=5, random_state=50)","a6d2c93e":"def find_sub_list(sub_list,super_list):\n    sub_list_length=len(sub_list)\n    matching_list = []\n    found_start = 0\n    for start in range(0,sub_list_length): # find first matching word\n        for ind in (j for j,e in enumerate(super_list) if e==sub_list[start]): # for every matched first word\n            found_start = ind-start\n            if sub_list_length < 2:\n                return [ind-start,ind-start+sub_list_length-1]\n            else:\n                for end in range(sub_list_length,start+1,-1): # from last to first every word as last word\n                    if super_list[ind:ind+end-start]==sub_list[start:end]: # if first to last matches\n                        return [ind-start,ind-start+sub_list_length-1] # return first(possible) and last(possible)+1 index\n    return [found_start, found_start+sub_list_length-1] # if you couldn't find the part you just return by any first match of super list","d393d997":"test_string = [\"this\",\"is\",\"a\",\"super\",\"list\"]\ntest_selected_string = [\"r\",\"list\"]\nprint(\"super list\")\nprint(test_string)\nprint(\"sub list\")\nprint(test_selected_string)\nprint(\"location that find_sub_list found\")\nprint(find_sub_list(test_selected_string,test_string))","33d4803a":"dfTrain['selected_start_end'] = dfTrain.apply(lambda df: find_sub_list(df.selected_w,df.text_w), axis=1)\ndfTrain.head()","4f81cb36":"if in_testing_mode:\n    dfTrain, dfLasttest = train_test_split(dfTrain, test_size=0.01, random_state=50)\n    dfTrain.reset_index(drop=True, inplace=True)\n    dfLasttest.reset_index(drop=True, inplace=True)","539dba48":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"the device we will run on is: \",device)\nif device.type == \"cuda\":\n    print(\"the GPU is: \", torch.cuda.get_device_name(0))","bd2a1cc5":"test_question = \"positive\"\ntest_string = [\" hello\",\" world\"]\ntest_context  = robertaTokenize(test_string)[0]\nprint(\"question: \", test_question)\nprint(\"context: \", test_context)\ntest = tokenizer.encode_plus(\n                      [test_question],                # sentiment to encode\n                      test_context,                   # Sentence to encode.\n                      add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n                      max_length = 10,            # Pad & truncate all sentences.\n                      pad_to_max_length = True,\n                      return_token_type_ids = True,\n                      return_attention_mask = True,   # Construct attn. masks.\n                      device=device,\n                      return_tensors = 'pt',      # Return pytorch tensors.\n                      )\nprint(\"input_ids: \",test[\"input_ids\"])\nprint(\"attention_mask: \",test[\"attention_mask\"])\nprint(\"token_type_ids: \",test[\"token_type_ids\"])\nprint(\"decoded: \",tokenizer.decode(test[\"input_ids\"][0]))","66a64bd1":"max_word_count = max(np.max(dfTrain[\"text_w\"].apply(lambda x: len(x))), np.max(dfTest[\"text_w\"].apply(lambda x: len(x))))\nmax_word_count+=5\nprint(\"Max clean word count is {}\".format(max_word_count))","6a40d6ee":"def convert_text2ids(sentiment, word_list, output, max_word_count, input_ids, attention_masks, token_type_ids, start_positions, end_positions):\n    encoded_dict = tokenizer.encode_plus(\n                                          [sentiment],                  # sentiment to encode\n                                          word_list,                    # Sentence to encode.\n                                          add_special_tokens = True,    # Add '[CLS]' and '[SEP]'\n                                          max_length = max_word_count,  # Pad & truncate all sentences.\n                                          pad_to_max_length = True,     # Create with padding.\n                                          return_token_type_ids = True, # Construct token type ids.\n                                          return_attention_mask = True, # Construct attn. masks.\n                                          device=device,\n                                          return_tensors = 'pt',        # Return pytorch tensors.\n                                        )\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n    token_type_ids.append(encoded_dict['token_type_ids'])\n    if output is not None:\n        start_positions.append(output[0])\n        end_positions.append(output[1])","25ebd41d":"train_input_ids, train_attention_masks, train_token_type_ids, train_start_positions, train_end_positions  = [], [], [], [], []\n\ndfTrain.apply(lambda df: convert_text2ids(df.sentiment, df.text_w, df.selected_start_end, max_word_count, train_input_ids, train_attention_masks, train_token_type_ids, train_start_positions, train_end_positions), axis=1)\n\n# convert lists to torch tensors\ntrain_input_ids = torch.cat(train_input_ids, dim=0) \ntrain_attention_masks = torch.cat(train_attention_masks, dim=0)\ntrain_token_type_ids = torch.cat(train_token_type_ids, dim=0)\ntrain_start_positions = torch.tensor(train_start_positions, dtype = torch.long)\ntrain_end_positions = torch.tensor(train_end_positions, dtype = torch.long)","4a930bee":"print(\"Shape of train input ids: \", train_input_ids.size())\nprint(\"Shape of train input masks: \", train_attention_masks.size())\nprint(\"Shape of train token type ids: \", train_token_type_ids.size())\nprint(\"Shape of train output start positions: \", train_start_positions.size())\nprint(\"Shape of train output end positions: \", train_end_positions.size())","84e8adbb":"batch_size = 64\n\n# Number of training epochs.\nepochs = 4\n\nlearning_rate = 5e-5\nadam_epsilon = 1e-8\n\nvalidation_size = 0.01\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","79caa1c4":"data = TensorDataset(train_input_ids,\n                     train_attention_masks,\n                     train_token_type_ids,\n                     train_start_positions,\n                     train_end_positions)\n\ntrain_data_size = int((1-validation_size) * len(data))\nval_data_size = len(data) - train_data_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(data, [train_data_size, val_data_size])\n\ntrain_dataloader = DataLoader(train_dataset, # The train samples.\n                              sampler=RandomSampler(train_dataset), # Pull out batches randomly.\n                              batch_size = batch_size)# Evaluate with this batch size.\n\nval_dataloader   = DataLoader(val_dataset, # The validation samples.\n                              sampler=SequentialSampler(val_dataset), # Pull out batches sequentially.\n                              batch_size = batch_size)# Evaluate with this batch size.\n\nprint(\"train data length :\", len(train_dataset))\nprint(\"validation data length :\", len(val_dataset))","cbe027ca":"random_step = random.randint(0,len(train_dataloader))\nfor step, batch in enumerate(train_dataloader):\n    if step == random_step:\n        random_data = random.randint(0,len(batch[0]))\n        sample_input_id       = batch[0][random_data]\n        sample_attention_mask = batch[1][random_data]\n        sample_token_type_id  = batch[2][random_data]\n        sample_start_position = batch[3][random_data]\n        sample_end_position   = batch[4][random_data]\n        break\n\nprint(\"sample_input_id\")\nprint(\"\\t\",sample_input_id)\nprint(\"sample_attention_mask\")\nprint(\"\\t\",sample_attention_mask)\nprint(\"sample_token_type_id\")\nprint(\"\\t\",sample_token_type_id)\nprint(\"sample_start_position\")\nprint(\"\\t\",sample_start_position)\nprint(\"sample_end_position\")\nprint(\"\\t\",sample_end_position)","82e4369f":"class tweetModel(BertPreTrainedModel):\n    def __init__(self, conf):\n        super(tweetModel, self).__init__(conf)\n        self.roberta = RobertaModel.from_pretrained(model_dir, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, dictionary):\n        outputs = self.roberta(\n            dictionary[\"input_ids\"],\n            attention_mask=dictionary[\"attention_mask\"],\n            token_type_ids=dictionary[\"token_type_ids\"]\n        )\n\n        out = torch.cat((outputs[2][-1], outputs[2][-2]), dim=-1)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits)\n        if \"start_positions\" in dictionary and \"end_positions\" in dictionary:  # if the output positions given also calculate and return the loss\n            if dictionary[\"start_positions\"] is not None and dictionary[\"end_positions\"] is not None:\n                if len(dictionary[\"start_positions\"].size()) > 1:\n                    dictionary[\"start_positions\"] = dictionary[\"start_positions\"].squeeze(-1)\n                if len(dictionary[\"end_positions\"].size()) > 1:\n                    dictionary[\"end_positions\"] = dictionary[\"end_positions\"].squeeze(-1)\n                # sometimes the start\/end positions are outside our model inputs, we ignore these terms\n                ignored_index = start_logits.size(1)\n                dictionary[\"start_positions\"].clamp_(0, ignored_index)\n                dictionary[\"end_positions\"].clamp_(0, ignored_index)\n\n                loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n                start_loss = loss_fct(start_logits, dictionary[\"start_positions\"])\n                end_loss = loss_fct(end_logits, dictionary[\"end_positions\"])\n                total_loss = (start_loss + end_loss) \/ 2 # basicly average of start positon and end positon loss\n                outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits","e508f493":"rb_config = RobertaConfig.from_pretrained(model_dir , output_hidden_states = True)\n\n# Initializing a model\nmodel = tweetModel(conf=rb_config)\nmodel.to(device)","b20d4ef3":"# prepare optimizer\nparam_optimizer = list(model.named_parameters())\n# hack to remove \"pooler\"  which is not used\nparam_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n\nno_decay =['bias','LayerNorm.weight']\noptimizer_grouped_parameters =[\n    {'params':[ p for n , p in param_optimizer \n               if not any(nd in n for nd in no_decay)],'weight_decay':0.01},\n    {'params':[p for n, p in param_optimizer\n               if any(nd in n for nd in no_decay)],'weigth_decay':0.0}\n    ]\n\noptimizer = AdamW(optimizer_grouped_parameters,\n                  lr = learning_rate,\n                  eps = adam_epsilon\n                 )","27741284":"# Total number of training steps is [number of batches] x [number of epochs]. \ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_squad.py\n                                            num_training_steps = total_steps)","3f3f9c39":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","982013d0":"# This training code is based on the `run_squad.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/question-answering\/run_squad.py\ndef train(model, optimizer, scheduler, epochs, train_dataloader, validation_dataloader, training_stats): \n    # Measure the total training time for the whole run.\n    total_t0 = time.time()\n\n    # For each epoch...\n    for epoch_i in range(0, epochs):\n        # ========================================\n        #               Training\n        # ========================================\n\n        # Perform one full pass over the training set.\n        print('\\n======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n\n        # Measure how long this training epoch takes.\n        t0 = time.time()\n\n        # Reset the total loss for this epoch.\n        total_train_loss = 0\n\n        # Put the model into training mode. Don't be mislead--the call to \n        # `train` just changes the *mode*, it doesn't *perform* the training.\n        # `dropout` and `batchnorm` layers behave differently during training\n        # vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n        model.train()\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n          # we'll copy each tensor to the GPU using the `to` method.\n            batch_device = tuple(t.to(device) for t in batch)\n            # Progress update every 20 batches.\n            if step % 20 == 0 and not step == 0:\n                # Calculate elapsed time in minutes.\n                elapsed = format_time(time.time() - t0)\n                # Report progress.\n                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n            # a `batch` contains five pytorch tensors:\n            #   [0]: input ids \n            #   [1]: attention masks\n            #   [2]: token type ids\n            #   [3]: selected text start position in text\n            #   [4]: selected text end position in text\n            inputs = {\n                      \"input_ids\": batch_device[0],\n                      \"attention_mask\": batch_device[1],\n                      \"token_type_ids\": batch_device[2],\n                      \"start_positions\": batch_device[3],\n                      \"end_positions\": batch_device[4],\n                      }\n\n            # Always clear any previously calculated gradients before performing a\n            # backward pass. PyTorch doesn't do this automatically because \n            # accumulating the gradients is \"convenient while training RNNs\". \n            # (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n            model.zero_grad()        \n\n            # It returns different numbers of parameters depending on what arguments\n            # are given and what flags are set. For our useage here, it returns\n            # the loss (because we provided start and end positions) and the \"logits\"\n            # --the model outputs prior to activation.\n            outputs = model(inputs)\n            loss = outputs[0]\n\n            # Accumulate the training loss over all of the batches so that we can\n            # calculate the average loss at the end. `loss` is a Tensor containing a\n            # single value; the `.item()` function just returns the Python value \n            # from the tensor.\n            total_train_loss += loss.item()\n\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0.\n            # This is to help prevent the \"exploding gradients\" problem.\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and take a step using the computed gradient.\n            # The optimizer dictates the \"update rule\"--how the parameters are\n            # modified based on their gradients, the learning rate, etc.\n            optimizer.step()\n\n            # Update the learning rate.\n            scheduler.step()\n\n        # Calculate the average loss over all of the batches.\n        avg_train_loss = total_train_loss \/ len(train_dataloader)            \n\n        # Measure how long this epoch took.\n        training_time = format_time(time.time() - t0)\n\n        print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n\n        # ========================================\n        #               Validation\n        # ========================================\n        # After the completion of each training epoch, measure our performance on\n        # our validation set.\n        print(\"\\nRunning Validation...\")\n\n        t0 = time.time()\n\n        # Put the model in evaluation mode--the dropout layers behave differently\n        # during evaluation.\n        model.eval()\n\n        # Tracking variables\n        total_eval_accuracy = 0\n        total_eval_loss = 0\n        nb_eval_steps = 0\n\n        # Evaluate data for one epoch and unpack the batches from the dataloader.\n        for batch in validation_dataloader:\n            batch_device = tuple(t.to(device) for t in batch)\n            inputs = {\n                      \"input_ids\": batch_device[0],\n                      \"attention_mask\": batch_device[1],\n                      \"token_type_ids\": batch_device[2],\n                      \"start_positions\": batch_device[3],\n                      \"end_positions\": batch_device[4],\n                      }\n\n            # Tell pytorch not to bother with constructing the compute graph during\n            # the forward pass, since this is only needed for backprop (training).\n            with torch.no_grad():        \n                outputs = model(inputs)\n                loss = outputs[0]\n\n            # Accumulate the validation loss.\n            total_eval_loss += loss.item()\n\n        # Calculate the average loss over all of the batches.\n        avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n\n        # Measure how long the validation run took.\n        validation_time = format_time(time.time() - t0)\n\n        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n        print(\"  Validation took: {:}\".format(validation_time))\n\n        # Record all statistics from this epoch.\n        training_stats.append({\n                                'epoch': epoch_i + 1,\n                                'Training Loss': avg_train_loss,\n                                'Valid. Loss': avg_val_loss,\n                                'Training Time': training_time,\n                                'Validation Time': validation_time\n                              })\n\n    print(\"\")\n    print(\"Training complete!\")\n\n    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","c0b24782":"training_stats = []\ntrain(model, optimizer, scheduler, epochs, train_dataloader, val_dataloader, training_stats)","17c99dc5":"# Display floats with two decimal places.\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the index values.\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table.\ndf_stats","fc118142":"# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks(list(range(1,epochs+1)))\n\nplt.show()","8059f55f":"random_step = random.randint(0,len(val_dataloader))\nfor step, batch in enumerate(val_dataloader):\n    if step == random_step:\n        random_data = random.randint(0,len(batch[0]))\n        sample_input_id       = batch[0][random_data].to(device)\n        sample_attention_mask = batch[1][random_data].to(device)\n        sample_token_type_id  = batch[2][random_data].to(device)\n        sample_start_position = batch[3][random_data].to(device)\n        sample_end_position   = batch[4][random_data].to(device)\n        break\n\nprint(\"sample_input_id\")\nprint(\"\\t\",sample_input_id)\nprint(\"sample_attention_mask\")\nprint(\"\\t\",sample_attention_mask)\nprint(\"sample_token_type_id\")\nprint(\"\\t\",sample_token_type_id)\n\n# Prepare a test input for sample prediction\ntest_inputs = {\n               \"input_ids\": sample_input_id.view(1,-1).to(device),\n               \"attention_mask\": sample_attention_mask.view(1,-1).to(device),\n               \"token_type_ids\": sample_token_type_id.view(1,-1).to(device)\n              }\n# Give the tokens representing our input text to our model and take the predictions.\nsample_answer_start_scores, sample_answer_end_scores = model(test_inputs)\n\n# Find the tokens with the highest `start` and `end` scores.\nanswer_start = torch.argmax(sample_answer_start_scores)\nanswer_end = torch.argmax(sample_answer_end_scores)\n\nprint(\"\\n\\nReal start: \", sample_start_position, 'Predicted start: ', answer_start)\nprint(\"Real end: \", sample_end_position, 'Predicted end: ', answer_end)","b79477f5":"def getSelected(text, text_w_n, answer_start_scores, answer_end_scores):\n    # Find the tokens with the highest start and end scores.\n    start_sorted_indexes = np.argsort(-answer_start_scores, kind='quicksort', order=None)[0]\n    end_sorted_indexes   = np.argsort(-answer_end_scores, kind='quicksort', order=None)[0]\n    found_start_end = False\n    for i in range(0,len(start_sorted_indexes)):\n        current_start = start_sorted_indexes[i]\n        if current_start >= len(text_w_n):\n            continue\n        for j in range(0,i+2 if i+2<len(end_sorted_indexes) else len(end_sorted_indexes)):\n            current_end = end_sorted_indexes[j]\n            if current_end >= len(text_w_n):\n                continue\n            if current_start <= current_end:\n                found_start_end = True\n                real_start = text_w_n[current_start]\n                real_end   = text_w_n[current_end]\n                break\n        if found_start_end:\n            break\n\n    if not found_start_end:\n        return text\n\n    splitted_text = text.split()\n    selected_text = splitted_text[real_start:real_end+1]\n    selected_text = \" \".join(selected_text)\n\n    return selected_text\n\ndef predict_test(text, sentiment, text_w, text_w_n):\n    encoded_dict = tokenizer.encode_plus(\n                                        [sentiment],                  # sentiment to encode\n                                        text_w,                       # Sentence to encode.\n                                        add_special_tokens = True,    # Add '[CLS]' and '[SEP]'\n                                        max_length = max_word_count,  # Pad & truncate all sentences.\n                                        pad_to_max_length = True,\n                                        return_token_type_ids = True,\n                                        return_attention_mask = True, # Construct attn. masks.\n                                        device=device,\n                                        return_tensors = 'pt',        # Return pytorch tensors.\n                                        )\n    encoded_dict[\"input_ids\"]       = encoded_dict[\"input_ids\"].to(device)\n    encoded_dict[\"token_type_ids\"]  = encoded_dict[\"token_type_ids\"].to(device)\n    encoded_dict[\"attention_mask\"]  = encoded_dict[\"attention_mask\"].to(device)\n\n\n    answer_start_scores, answer_end_scores = model(encoded_dict)\n    answer_start_scores = answer_start_scores.cpu().detach().numpy()\n    answer_end_scores   = answer_end_scores.cpu().detach().numpy()\n    selected_text = getSelected(text, text_w_n, answer_start_scores, answer_end_scores)\n    return selected_text","8c51c814":"x = random.randint(0,len(dfTest))\nwhile dfTest.iloc[x].sentiment == \"neutral\":\n    x = random.randint(0,len(dfTest))\nprint(dfTest.iloc[x].text)\nprint(dfTest.iloc[x].sentiment)\npredict_test(dfTest.iloc[x].text, dfTest.iloc[x].sentiment, dfTest.iloc[x].text_w, dfTest.iloc[x].text_w_n)","cef7efd1":"dfTest[\"selected_text\"] = dfTest.apply(lambda df: predict_test(df.text, df.sentiment, df.text_w, df.text_w_n), axis=1)\ndfTest['selected_text'] = dfTest['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\ndfTest['selected_text'] = dfTest['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\ndfTest['selected_text'] = dfTest['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\ndfTest[[\"textID\", \"text\", \"sentiment\", \"selected_text\"]].head(20)","fe26036c":"dfContrib = pd.read_csv(sample_submission_file)\ndfContrib.head()","600d8d7e":"dfContrib = pd.merge(dfContrib[[\"textID\"]], dfTest[[\"textID\",\"selected_text\"]], on='textID')\ndfContrib.to_csv(submission_file,index=False)\npd.read_csv(submission_file).head()","ebcea227":"def jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","cd113208":"dfNeutral = dfTrain[dfTrain['sentiment'] == 'neutral']\ndfNeutral['jaccard'] = dfNeutral.apply(lambda df: jaccard(df.selected_text, df.text), axis=1)\nprint(\"Average jaccard score of neutral data when text used as selected text: \",dfNeutral[\"jaccard\"].mean())\ndfNeutral[\"predicted_selected_text\"] = dfNeutral.apply(lambda df: predict_test(df.text, df.sentiment, df.text_w, df.text_w_n), axis=1)\ndfNeutral['predicted_jaccard'] = dfNeutral.apply(lambda df: jaccard(df.selected_text, df.predicted_selected_text), axis=1)\nprint(\"Average jaccard score of neutral data when models prediction used as selected text: \",dfNeutral[\"predicted_jaccard\"].mean())\ndel(dfNeutral)","881e5f1a":"if in_testing_mode:\n    dfLasttest[\"predicted_selected_text\"] = dfLasttest.apply(lambda df: predict_test(df.text, df.sentiment, df.text_w, df.text_w_n), axis=1)\n    dfLasttest[\"jaccard\"] = dfLasttest.apply(lambda df: jaccard(df.selected_text, df.predicted_selected_text), axis=1)\n    print(\"average jaccard score of test data is: \",dfLasttest[\"jaccard\"].mean())\n    print(\"average jaccard score of positive test data is: \",dfLasttest[dfLasttest['sentiment'] == 'positive'][\"jaccard\"].mean())\n    print(\"average jaccard score of negative test data is: \",dfLasttest[dfLasttest['sentiment'] == 'negative'][\"jaccard\"].mean())\n    print(\"average jaccard score of neutral test data is: \",dfLasttest[dfLasttest['sentiment'] == 'neutral'][\"jaccard\"].mean())","40f7140d":"if in_testing_mode:\n    dfTrain[\"predicted_selected_text\"] = dfTrain.apply(lambda df: predict_test(df.text, df.sentiment, df.text_w, df.text_w_n), axis=1)\n    dfTrain[\"jaccard\"] = dfTrain.apply(lambda df: jaccard(df.selected_text, df.predicted_selected_text), axis=1)\n    print(\"average jaccard score of train data is: \",dfTrain[\"jaccard\"].mean())\n    print(\"average jaccard score of positive train data is: \",dfTrain[dfTrain['sentiment'] == 'positive'][\"jaccard\"].mean())\n    print(\"average jaccard score of negative train data is: \",dfTrain[dfTrain['sentiment'] == 'negative'][\"jaccard\"].mean())\n    print(\"average jaccard score of neutral train data is: \",dfTrain[dfTrain['sentiment'] == 'neutral'][\"jaccard\"].mean())","00eee1ab":"### Train function","2ffcfcc5":"### Load dataframes","6c9b7ec1":"With the help of the following function, the numbers of the words are arranged according to the old word and the processed word, so as an example\n\n**Before Processing**\n\n| text_w         | text_w_n |\n|----------------|----------|\n|[\"dont\",\"split\"]|  [0,1]   |\n\n**After Processing**\n\n| text_w             | text_w_n |\n|--------------------|----------|\n|[\"do\",\"not\",\"split\"]| [0,0,1]  |","0ad96e71":"### Prediction sampling","7473ce30":"### Split texts","cede5891":"# Hugging Face","e5fa1d87":"## Add spaces before words","029c9d67":"### Train process visualization","77919fff":"As we can see above we are using 5 additional tokens in roberta tokenizer. These are:\n\n*   1 * **0**\n*   1 * **sentiment's id**\n*   3 * **2** \n\nSo we need to calculate the longest contexts length of tweet text and add 5 to it. Thus, we will have the max input size.","7d6149a4":"We should also apply this tokenization to the test dataframe","d9abc563":"After this point, we need to prepare the output we will expect from the model. Since we apply all operations to both tweet text and selected text, we can find the position of the selected text in the tweet text. The following function is defined for this job. This function returns the start and end word order of the selected text in the tweet text even if there are some errors and missings.","a7d089ca":"Detect the device that notebook runs on","534874c3":"#### Preparing tensor dataset\n","1a77fc31":"Roberta's tokenizer tokenizes the words \" world\" and \"world\" differently. Since we separated the words for cleaning, we lost the spaces between them. In other words, the input that we should normally give as \"Hello world\" is now as \"Helloworld\". Therefore, we should make our input as \" Hello world\" to make the tokenizer do its job more accurately.","6673c499":"# TRAINING","3ad365d7":"# Data Cleaning","66711749":"We will define a function that creates and collects the encoded values in lists","1c1dad94":"# Submission","8922896a":"### Correct misspellings","35ded7f7":"### Convert some repeated text to a tag","7e780874":"### Clean the missing quotes or some undefined word","e9b0be15":"As we can see below there is 3 sentiment type. Positive and negative sentiments distributed in close proportions.","c4bcb752":"# Prepare Train Output","de137925":"#### Preparing optimizer","cbcd7743":"### Use ekphrasis to convert text like urls to tags","4a990828":"Now let's find out how much the selected text keeps part of the text in different sentiments.","64212e73":"#### Preparing scheduler","193198d4":"# Split Train data to Train and Last Test","9c41eff9":"### visualize the sentiments","446d75dd":"### Train process","a5468274":"### Predicting","61afb01f":"if in_testing_mode allocate some of the data for testing purpose","f87f15dd":"# Roberta Tokenizer","0373a3ad":"### Define Mode","9fec8bb5":"One of the most important points is here. We will divide the tweet text and selected text into words and put them into a list. Then we will assign a number to every word in the tweet text for accesing to orginal form after cleaning. However, since we will work word-based rather than character-based, we will extract the selected text list as it is in the tweet text list. In this way, even if the selected text is cropped in letters, it will appear in list as cropped in words.","659c43ce":"### Defining difference fixer function","54a73ed2":"### import some useful libs","47304220":"## Apply all the cleanings to test dataframe","d1c71198":"As we can see above the **selected text contains almost all text in Neutral Sentiment**","bdb37305":"#### Preparing model","8e103647":"# Start","9e5829b7":"# Predicting Jaccards\n(if in testing mode)","3f671764":"## RoBerta MODEL"}}