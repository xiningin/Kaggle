{"cell_type":{"3020319f":"code","94565cc0":"code","c2e9fc82":"code","5cf9bee1":"code","b9081ee7":"code","521d1f31":"code","f508d778":"code","23fba193":"code","315b580a":"code","39e0bc5c":"code","4ed1f1e9":"code","0870c332":"code","5b0e54e0":"code","6dfb8d55":"code","8f7047f5":"code","1ae03fef":"code","5bc9cab7":"code","6b462d71":"code","a149a348":"code","94141e59":"code","b269719f":"code","b4485b56":"code","38d49117":"code","d455acb6":"code","6f9fb726":"code","f6390538":"code","fff7d61c":"code","90d5180c":"code","2b926efb":"code","475b72f4":"code","e6b60fda":"code","01704e10":"markdown","33393289":"markdown","ff86b772":"markdown","3f03132f":"markdown","645c0220":"markdown","f4494a38":"markdown","87f8bb8c":"markdown","f1d7eb37":"markdown","aa8bdbbe":"markdown","441e56e4":"markdown","c49c2f15":"markdown","05e0bbcb":"markdown","eb3f7c52":"markdown","a0f3f5c3":"markdown"},"source":{"3020319f":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport random\nimport json\nimport os\nimport functools\nimport re\nimport eli5\n\nfrom matplotlib import pyplot as plt\nfrom geopy.distance import geodesic #Distances\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split, cross_validate, KFold, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder, normalize, scale\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score","94565cc0":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c2e9fc82":"PATH_DATA = '\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv'","5cf9bee1":"data = pd.read_csv(PATH_DATA)","b9081ee7":"#Fix NaN in reviews per month\ndata.loc[data['reviews_per_month'].isna(), 'reviews_per_month'] = 0\n#Remove \"empty records\"\ndata = data[data['room_type'] != 0]\n#Remove empty prices\ndata = data[data['price']>0]\n#Remove empty strings\ndata = data[~data['name'].isna()]\n#Remove not availables \ndata = data[data['availability_365'] > 0]\n\n#Remove minimum_nights outliers (>95 percentile)\ndata = data[data['minimum_nights'] <= 30]\n\ndata['d_central_park'] = data.apply(lambda r: geodesic((40.782865, -73.964885), (r['latitude'], r['longitude'])).meters, axis=1)\ndata['d_lga'] = data.apply(lambda r: geodesic((40.772897, -73.873043), (r['latitude'], r['longitude'])).meters, axis=1)\ndata['d_jfk'] = data.apply(lambda r: geodesic((40.641739, -73.781193), (r['latitude'], r['longitude'])).meters, axis=1)\n\n#Median price in the neighbourhood\nmedian_price_neigh = pd.DataFrame(data.groupby('neighbourhood')['price'].median()) #or mean\n#Median price in the neighbourhood group\nmedian_price_neigh_gp = pd.DataFrame(data.groupby('neighbourhood_group')['price'].median()) #or mean\n#Avg popularity of rooms in the neighbourhood\nmedian_pop_neigh = pd.DataFrame(data.groupby('neighbourhood')['reviews_per_month'].median()) #or mean\n#Number of postings in the neigbourhood\ncnt_postings_neigh = pd.DataFrame(data.groupby('neighbourhood')['reviews_per_month'].count()).rename(columns={'reviews_per_month':'postings_in_neighbourhood'})\n\n# Number of postings in the neighbourhood of the same type\nd = pd.DataFrame(data.groupby(['neighbourhood', 'room_type'])['room_type'].count())\nd.index = d.index.set_names(['neighbourhood', 'r_type'])\nd.reset_index(inplace=True)  \nd = d.rename(columns={'room_type':'r_type_cnt'})\n\n#Join the new features\ndata = data.join(median_price_neigh, on='neighbourhood', rsuffix='_median')\ndata = data.join(median_price_neigh_gp, on='neighbourhood_group', rsuffix='_gp_median')\ndata = data.join(median_pop_neigh, on='neighbourhood', rsuffix='_median')\ndata = data.join(cnt_postings_neigh, on='neighbourhood')\ndata = pd.merge(data, d, how='inner', left_on=['neighbourhood', 'room_type'], right_on=['neighbourhood', 'r_type']).drop(['r_type'], axis=1)\n\n\n# Check whether name contains \"luxury\" words\nLUXURY_WORDS = ['loft', 'studio', 'townhouse', 'views', 'midtown', 'luxury', 'central', 'park', 'soho', 'village', 'suite', 'west', 'new']\nHOT_WORDS = ['iron', 'internet', 'wifi', 'laptop', 'wi-fi', 'family', 'kid', 'private', 'air']\ndef contains(title, words):\n    for w in words:\n        if w.lower() in title.lower():\n            return True\n    return False\n\ndata['luxury'] = data.apply(lambda r: contains(r['name'], LUXURY_WORDS), axis=1)\ndata['hotwords'] = data.apply(lambda r: contains(r['name'], HOT_WORDS), axis=1)\n\n\ndata['price_residual'] = data['price'] - data['price_median']\ndata['price_residual_percent'] = data['price_residual'] \/ data['price_median']\ndata['price_residual_percent'] = data['price_residual_percent'].fillna(0)\n\n#\u00a0We can also convert some features to log scale in order to make them closer to normal distributed.\n#LOG nb postings\n#data['calculated_host_listings_count'] = np.log1p(data['calculated_host_listings_count'])\n#LOG price\n#data['price'] = np.log1p(data['price'])\n#data['price_median'] = np.log1p(data['price_median'])\n#LOG popularity\n#data['reviews_per_month'] = np.log1p(data['reviews_per_month'])\n#data['reviews_per_month_median'] = np.log1p(data['reviews_per_month_median'])\n\ndata['len_title'] = data.apply(lambda r: len(r['name']), axis=1)\n\n\n#Get the number of days since a room was reviewed\nzero_time_revies = '2019-07-08' #Last day in the dataset will be our t0\ndata['days_since_last_review'] = (pd.to_datetime(zero_time_revies) - pd.to_datetime(data['last_review'])).apply(lambda i: -1 if pd.isnull(i) else int(i.days))\n\n\n# This line is optional. We can choose to remove the entries with -1 (= NaN, I suppose that correspond to rooms with no reviews) if we don't want to contaminate \n# the model with \"no reviews\" targets. However, the forecasting of bookings is also interesting to know for new users\/rooms with no postings.\n#data = data[(data['days_since_last_review'] > -1)]","521d1f31":"corr = data[['latitude', 'longitude', 'price', 'price_median', 'price_residual', 'price_residual_percent', 'price_gp_median','minimum_nights', 'calculated_host_listings_count', 'availability_365', 'reviews_per_month', 'd_central_park', 'reviews_per_month_median', \n                                   'postings_in_neighbourhood', 'r_type_cnt']].corr(method='pearson')\nfig, ax = plt.subplots(figsize=(14,14))\nsns.heatmap(corr, \n            cmap=sns.diverging_palette(220, 10, as_cmap=True), \n            center=0, vmin=-1, vmax=1, \n            square=True,\n            annot=True,\n            linewidths=.5,\n            ax = ax)\n_ = ax.set_title('Correlation matrix')\n","f508d778":"data_linear = data.copy()","23fba193":"oh_rooom_type = pd.get_dummies(data_linear['room_type'])\n#oh_neighbourhood = pd.get_dummies(data_linear['neighbourhood'])\noh_neighbourhood_group = pd.get_dummies(data_linear['neighbourhood_group'])\n\noh_features = pd.concat([oh_rooom_type], axis=1)\n\ndata_linear = pd.concat([data_linear,oh_features], axis=1)","315b580a":"predictors, target = data_linear[[\n 'minimum_nights',\n 'availability_365',\n 'price_median',\n 'reviews_per_month_median',\n 'luxury',\n 'hotwords',\n 'len_title']],\\\n                     data_linear['reviews_per_month']\n\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.33, random_state=42)","39e0bc5c":"reg = LinearRegression().fit(X_train, y_train)","4ed1f1e9":"eli5.explain_weights(reg, feature_names=predictors.columns.tolist())","0870c332":"res = reg.predict(X_test)\ny_test2 = y_test.values","5b0e54e0":"print(f\"MSE: {mean_squared_error(y_test2, res)}\")\nprint(f\"MAE: {mean_absolute_error(y_test2, res)}\")\nprint(f\"R2: {r2_score(y_test2, res)}\")\n\nfig, ax= plt.subplots(1, figsize=(28,8))\nax.scatter(res, y_test2, alpha=0.4, marker='+')\nax.set_xlabel('Predicted values')\nax.set_ylabel('Actual values')\nax.plot(list(range(-2,10)), list(range(-2,10)), color='black', linestyle='--')\n\nfig, ax= plt.subplots(1, figsize=(28,8))\nsns.distplot(res, ax=ax, label='Predicted distr', bins=100)\nsns.distplot(y_test2, ax=ax, label='Real distr', bins=100)\nax.set_xlabel(\"Popularity\")\nax.legend()\nax.grid(True)","6dfb8d55":"enc_neigh = LabelEncoder() \nenc_neigh_gp = LabelEncoder()\nenc_rtype = LabelEncoder()\n\ndata['enc_neighbourhood'] = enc_neigh.fit_transform(data['neighbourhood'])\ndata['enc_room_type'] = enc_rtype.fit_transform(data['room_type'])\ndata['enc_neighbourhood_gp'] = enc_neigh_gp.fit_transform(data['neighbourhood_group'])","8f7047f5":"data.columns.tolist()","1ae03fef":"predictors, target = data[[\n 'latitude',\n 'longitude',\n 'price',\n 'minimum_nights',\n 'calculated_host_listings_count',\n 'availability_365',\n 'd_central_park',\n 'd_lga',\n 'd_jfk',\n 'price_median',\n 'price_gp_median',\n 'reviews_per_month_median',\n 'postings_in_neighbourhood',\n 'r_type_cnt',\n 'luxury',\n 'hotwords',\n 'price_residual',\n 'price_residual_percent',\n 'len_title',\n 'days_since_last_review',\n 'enc_neighbourhood',\n 'enc_room_type',\n 'enc_neighbourhood_gp']],\\\n                     data['reviews_per_month']\n\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.33, random_state=42)","5bc9cab7":"param = {\n    \"objective\": \"reg:squarederror\",\n    \"nthread\":4,\n    'n_estimators': 2000,\n}\n\nxgb_model = xgb.XGBRegressor(**param)","6b462d71":"xgb_model.fit(X_train, y_train)\n\nres = xgb_model.predict(X_test)\n\nprint(f\"MSE: {mean_squared_error(y_test, res)}\")\nprint(f\"MAE: {mean_absolute_error(y_test, res)}\")\nprint(f\"R2: {r2_score(y_test, res)}\")\n\nfig, ax= plt.subplots(1, figsize=(28,8))\nax.scatter(res, y_test.values, alpha=0.3, marker='+')\nax.set_xlabel('Predicted values')\nax.set_ylabel('Actual values')\nax.plot(list(range(-1,10)), list(range(-1,10)), color='black', linestyle='--')\n\nfig, ax= plt.subplots(1, figsize=(28,8))\nsns.distplot(res, ax=ax, label='Predicted distr')\nsns.distplot(y_test.values, ax=ax, label='Real distr')\nax.legend()\nax.grid(True)\n","a149a348":"eli5.explain_weights(xgb_model)","94141e59":"most_interesting_features = eli5.explain_weights_df(xgb_model).iloc[:7, 0].tolist()\nmost_interesting_features","b269719f":"predictors, target = data[most_interesting_features],\\\n                     data['reviews_per_month']\n\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.33, random_state=42)","b4485b56":"grid = {\n        'max_depth': [5, 7, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 7, 10, 12],\n        'n_estimators': [1200, 1700, 2000, 2500]\n        }","38d49117":"xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")","d455acb6":"# Lets sample only 6 locations in the search space. We don't want to wait.\nrandom_search = RandomizedSearchCV(xgb_model, param_distributions=grid, n_iter=6, scoring='r2', n_jobs=4, cv=5,verbose=3, random_state=1001) ","6f9fb726":"random_search.fit(X_train, y_train)","f6390538":"results = pd.DataFrame(random_search.cv_results_).sort_values('rank_test_score')\nresults","fff7d61c":"results.iloc[0]['params']","90d5180c":"print(f'R2 (Train):{results.iloc[0][\"mean_test_score\"]}')","2b926efb":"bst = random_search.best_estimator_\nres = bst.predict(X_test)","475b72f4":"print(f\"MSE: {mean_squared_error(y_test, res)}\")\nprint(f\"MAE: {mean_absolute_error(y_test, res)}\")\nprint(f\"R2: {r2_score(y_test, res)}\")\n\nfig, ax= plt.subplots(1, figsize=(28,8))\nax.scatter(res, y_test.values, alpha=0.3, marker='+')\nax.set_xlabel('Predicted values')\nax.set_ylabel('Actual values')\nax.plot(list(range(0,10)), list(range(0,10)), color='black', linestyle='--')\n\nfig, ax= plt.subplots(1, figsize=(28,8))\nsns.distplot(res, ax=ax, label='Predicted distr')\nsns.distplot(y_test.values, ax=ax, label='Real distr')\nax.legend()\nax.grid(True)","e6b60fda":"fig = plt.figure(figsize=(16,8))\nplt.plot(y_test.values, label='real', alpha=0.7)\nplt.plot(res, label='predicted', alpha=0.4, color='red')\nplt.ylim((0,20))\nplt.legend()\nfig.show","01704e10":"> **NOTE**: *After training several Linear models, I got to use this particular features (were the more important in most of the configurations).*","33393289":"##\u00a00. Preparing the data","ff86b772":"## 1. Baseline: Simple Linear Regression\n\nHere, a linear regression model is fitted to the data . We can't expect very much of the model, as the popularity of a posting doesn't seem to be high linearly correlated to the other features.","3f03132f":"#\u00a0Predicting posting popularity\n\n![](https:\/\/www.wraltechwire.com\/wp-content\/uploads\/2018\/07\/airbnb-3399753__340.jpg)\n\nIn this notebook, we are going to predict the popularity that a given posting will have. This could be useful to owners and business as they will be able to optimize their postings to maximize bookings or to predict the income a particular posting is going to generate in the future.\n\nTo measure 'popularity' of a posting, `reviews_per_month` variable is used as a proxy. It's worth to say that this could not be the most appropritate solution because we are assuming that all the guests leave a review when they stay at a room, which it isn't necessarily true. However, this is a fun project, we can take it as valid :)\n\nFurther down, two models are tested and evaluated. The first one, a simple Linear Regression model which will serve as a baseline for future models and later, a tree bossting based model (XGB). \n\n<div class='alert alert-warning'>\n    <b>NOTE:<\/b> What you will see in this notebook is the result after trying several types of models (LR, RF, NN, KNN and GB), but I've only kept the GB one as a 'serious' option as it was the one that I got the most promising results with.\n<\/div>\n\nAlso, the things tried in this notebook are derived from the insights extracted in my [data analysis kernel](https:\/\/www.kaggle.com\/alvaroibrain\/airbnb-data-analysis).","645c0220":"We can see that the top 2 have most of the weight in the decisions, however, 7 will be a reasonably number of variables to work with. Besides of that, although the price doesn't seem to be very related with the popularity, in the analysis kernel, we saw that the most expensive postings tend to have less visitors, so this intuition is the main reason I'm woing to keep up to the 7th feature.","f4494a38":"We use a random grid search with cross validation (5 folds) to find a good configuration of the hyperparameters.","87f8bb8c":"Here, we clean\/preprocess the data and generate new features (inspired from the [data analysis insights](https:\/\/www.kaggle.com\/alvaroibrain\/airbnb-data-analysis)). \n\nIn short, the following steps are executed:\n\n1. Assign 0 to the empty `reviews_per_month` entries (NaN).\n2. Remove potentially wrong postings (wrong room types, negative or null prices, empty names or excesivelly high `minimum_nights` required)\n3. Calculate the distance to 3 hot points in NYC (Central Park, JFK airport and LGA airport).\n4. Calculate the median price of postings in a neighbourhood and in the neighbourhood groups.\n5. Calculate the median number of posts in a neighbourhood.\n6. Calculate, for each posting, the total number of rooms of the same type in the neighbourhood.\n7. Check whether a posting name contains words associated with luxury postings.\n8. Check whether a posting name contains words associated with popular postings.\n9. Calculate the difference of price between a posting and the neighbourhood. (In absolute terms and in percentage).\n10. Calculate te length in characters of the title.\n11. Get the number of days since the last review.","f1d7eb37":"## 3. Summary \/ Future work\n\nThe Boosting method shows promising results in this task, however, the model doesn't achieve to explain a very high percentage of the data variance. This could be an indicator that more feature engineering is needed or that the popularity of a posting can't be accurately determined with the available data. For example, important factors (not present in this dataset) that make someone to book a particular posting on Airbnb would be the photos of the rooms, the comments of the posting, the descriptions, ratings, or even information about the profile of the room owner.\n\n**More things to try in the future**\n* Try to predict a derived feature as the *difference between the popularity and the median popularity in the neighbourhood*. Maybe the distribution of this is easier to learn.\n* Try to make new useful features combining data from external sources. (Maybe possible to find the postings on airbnb?)\n* Use richer features derived from the titles (embeddings\/TF-IDF), to capture better information than only with \"hotwords\" and combine it with NN\n\n\nHope you liked this notebook and if so, drop a \ud83d\udc4d\ud83c\udffb, it'll be welcome :). ","aa8bdbbe":"##\u00a02. More advanced: Tree Boosting\n\nFor this model, we don't need to one-hot encode our features, so we encode with labels (ids) and use all of them. Later we will get only the most important ones and fit the models using a random grid search + cv with 5 folds.","441e56e4":"###\u00a02.1 Model tuning","c49c2f15":"We can see that the model does not achieve a good fit to the original data distribution. \n\nA more sophisticated model will be needed, so I'm going to bet for ensembles (using boosted trees).","05e0bbcb":"This are much promising results. We then take a look at the most important features that the model chose.","eb3f7c52":"We achieved a little bit more of performance.\n\nThe model can fit better to the skewed data distribution although the predicted has a peak arround 3 reviews\/month, maybe this is due to a over-representation in the training dataset. We will have to study it to reduce this bias and rise more the R2 metric.","a0f3f5c3":"### 2.2 Model evaluation (test set)"}}