{"cell_type":{"5cc31a4e":"code","cc7c4a0a":"code","16798e95":"code","2a142630":"code","77a56a19":"code","6450953f":"code","3df364a1":"code","5ab44b51":"code","c0cad3f5":"code","4a542f4f":"code","d56d5e07":"code","7d18f7d7":"code","20f11c5f":"code","4913a5fd":"code","9f7cc43f":"code","1cca8d15":"code","935a86ce":"code","3bda4594":"code","2e4c2fa5":"code","10f48dbe":"code","4c437a50":"code","e7c75e17":"code","1e66c7c5":"code","79001ac8":"code","d6c27d7b":"code","7144b2ea":"code","930e2da3":"code","1bc9e2b2":"code","a53909f8":"code","e5e9e53a":"code","7566f5fb":"code","6732498e":"code","eaf232ce":"code","a1e758b7":"code","a72f965d":"code","d653ddb3":"code","35de4b4a":"code","3b77efa0":"code","6b2c6ab6":"code","508d136f":"code","50f3716b":"code","57129925":"code","8a9f3207":"code","5f09fa11":"code","f07e0fe5":"code","de5dc8c9":"code","dd3ad7f6":"code","27d25cf3":"code","b1f5d475":"code","1fdc8ced":"code","f5bb4524":"code","70e38301":"code","73e9304d":"code","a012ddc0":"code","a07b3c52":"code","222334c1":"code","478bc9e4":"code","ddaba033":"code","197f3c3c":"code","656bc645":"code","bc64761b":"code","644085f2":"code","df6f78ef":"markdown","a90c6589":"markdown","b00c8f56":"markdown","e0fccade":"markdown","21a12926":"markdown","28476dbc":"markdown","5cd96bcd":"markdown","e1298902":"markdown","bd08f3d2":"markdown","ac5fc1fc":"markdown","e9a9f627":"markdown","aaabaaf9":"markdown","355e1fd6":"markdown","97ed5aee":"markdown","73a60ce0":"markdown","d6705217":"markdown","817553d1":"markdown","334e9db0":"markdown","8afee874":"markdown","c375ce73":"markdown","b3362749":"markdown","0ed7e0b8":"markdown","e5a90bd1":"markdown","264dc6ca":"markdown","e22e69c1":"markdown","bceac5a5":"markdown","54e1ea67":"markdown"},"source":{"5cc31a4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc7c4a0a":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","16798e95":"train.head()","2a142630":"test.head()","77a56a19":"train.describe()","6450953f":"test.describe()","3df364a1":"import matplotlib.pyplot as plt\n%matplotlib inline\nnum_hist=5\nfig,ax=plt.subplots(num_hist,2,figsize=(10,20))\nfeature_hist=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\"]\nfor i in range(5):\n    ax[i][0].hist(train[feature_hist[i]])\n    ax[i][0].set_title(\"train_\"+feature_hist[i])\n    ax[i][1].hist(test[feature_hist[i]])\n    ax[i][1].set_title(\"test_\"+feature_hist[i])","5ab44b51":"train.info()","c0cad3f5":"test.info()","4a542f4f":"import seaborn as sns\n#function to get the missing value count in dataframe and plot their percent missing value as well\ndef findMissing(data):\n    total=data.isnull().sum().sort_values(ascending=False)\n    percent=(100*data.isnull().sum()\/(data.isnull().count())).sort_values(ascending=False)\n    ms=pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n    ms=ms[ms['Percent']>0]\n    fig,ax=plt.subplots(figsize=(6,4))\n    plt.xticks(rotation='90')\n    fig=sns.barplot(ms.index,ms[\"Percent\"],color=\"red\")\n    return ms","d56d5e07":"findMissing(train)","7d18f7d7":"findMissing(test)","20f11c5f":"train.head(20)","4913a5fd":"(train.isnull()[\"Cabin\"]==(train[\"Pclass\"]==1)).value_counts()","9f7cc43f":"(test.isnull()[\"Cabin\"]==(test[\"Pclass\"]==1)).value_counts()","1cca8d15":"ClassCount=[0,0,0]\n\nfor i in range(train.shape[0]):\n    if(train.isnull().loc[i,\"Cabin\"]==False):\n        temp=train.loc[i,\"Pclass\"]-1\n        ClassCount[temp]+=1\n\n    \nsns.barplot(['C1','C2','C3'],ClassCount,color=\"green\")","935a86ce":"ClassCount=[0,0,0]\n\nfor i in range(test.shape[0]):\n    if(test.isnull().loc[i,\"Cabin\"]==False):\n        temp=test.loc[i,\"Pclass\"]-1\n        ClassCount[temp]+=1\n\n    \nsns.barplot(['C1','C2','C3'],ClassCount,color=\"green\")","3bda4594":"train[\"Cabin\"].fillna(\"NC\",inplace=True)\ntest[\"Cabin\"].fillna(\"NC\",inplace=True)","2e4c2fa5":"train[\"Cabin\"].isnull().value_counts()","10f48dbe":"test[\"Cabin\"].isnull().value_counts()","4c437a50":"train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0],inplace=True)\ntest[\"Fare\"].fillna(test[\"Fare\"].median(),inplace=True)","e7c75e17":"findMissing(train)","1e66c7c5":"findMissing(test)","79001ac8":"import re\n\ndef get_title(name):\n    title_search=re.search(' ([A-Za-z]+)\\.',name)\n    if(title_search):\n        return title_search.group(1)\n    return \"\"\n\ntrain[\"Title\"]=train[\"Name\"].apply(get_title)\ntest[\"Title\"]=test[\"Name\"].apply(get_title)\ntrain.head()","d6c27d7b":"train[\"Title\"].value_counts()","7144b2ea":"train[\"Title\"]=train[\"Title\"].replace(['Dr','Rev','Major','Col','Capt','Sir','Don','Lady','Countess','Jonkheer'],'Super')\ntrain[\"Title\"]=train[\"Title\"].replace(['Mlle','Ms'],'Miss')\ntrain[\"Title\"]=train[\"Title\"].replace('Mme','Mrs')\ntrain[\"Title\"].value_counts()","930e2da3":"test[\"Title\"].value_counts()","1bc9e2b2":"test[\"Title\"]=test[\"Title\"].replace('Ms','Miss')\ntest[\"Title\"]=test[\"Title\"].replace([\"Col\",\"Rev\",\"Dr\",\"Dona\"],\"Super\")\ntest[\"Title\"].value_counts()","a53909f8":"train.drop(\"Name\",axis=1,inplace=True)\ntrain.head()","e5e9e53a":"test.drop(\"Name\",axis=1,inplace=True)\ntest.head()","7566f5fb":"train=pd.get_dummies(train,columns=[\"Sex\",\"Title\",\"Embarked\"])\ntrain.head()","6732498e":"test=pd.get_dummies(test,columns=[\"Sex\",\"Title\",\"Embarked\"])\ntest.head()","eaf232ce":"from sklearn.ensemble import RandomForestRegressor\nrfr1=RandomForestRegressor()\ncol_age=[\"Title_Master\",\"Title_Miss\",\"Title_Mr\",\"Title_Mrs\",\"Title_Super\",\"Fare\",\"Parch\",\"SibSp\"]\nx_list1=[]\nx_list2=[]\nfor i in range(len(train)):\n    if train[\"Age\"].isnull().loc[i]==False:\n        x_list1.append(i)\n    else:\n        x_list2.append(i)\n\nX_agetrain=train.loc[x_list1,col_age]\ny_agetrain=train.loc[x_list1,\"Age\"]\n\nrfr1.fit(X_agetrain,y_agetrain)\n\nfor i in range(len(train)):\n    if(train[\"Age\"].isnull().loc[i]==True):\n        train.loc[x_list2,\"Age\"]=rfr1.predict(train.loc[x_list2,col_age])","a1e758b7":"train[\"Age\"].isnull().value_counts()","a72f965d":"rfr2=RandomForestRegressor()\ncol_age=[\"Title_Master\",\"Title_Miss\",\"Title_Mr\",\"Title_Mrs\",\"Title_Super\",\"Fare\",\"Parch\",\"SibSp\"]\nx_list_1=[]\nx_list_2=[]\nfor i in range(len(test)):\n    if test[\"Age\"].isnull().loc[i]==False:\n        x_list_1.append(i)\n    else:\n        x_list_2.append(i)\n\nX_agetest=test.loc[x_list_1,col_age]\ny_agetest=test.loc[x_list_1,\"Age\"]\n\nrfr2.fit(X_agetest,y_agetest)\n\nfor i in range(len(test)):\n    if(test[\"Age\"].isnull().loc[i]==True):\n        test.loc[x_list_2,\"Age\"]=rfr2.predict(test.loc[x_list_2,col_age])","d653ddb3":"test[\"Age\"].isnull().value_counts()","35de4b4a":"test.head()","3b77efa0":"train[\"Fare_bin\"]=pd.cut(train[\"Fare\"],bins=[0,8,15,30,120,1000],labels=['Low_fare','median_fare','average_fare','high_fare','very_high_fare'])\ntrain[\"Age_bin\"]=pd.cut(train[\"Age\"],bins=[0,12,25,40,60,120],labels=['children','youth','young','m-aged','elderly'])\ntest[\"Fare_bin\"]=pd.cut(test[\"Fare\"],bins=[0,8,15,30,120,1000],labels=['Low_fare','median_fare','average_fare','high_fare','very_high_fare'])\ntest[\"Age_bin\"]=pd.cut(test[\"Age\"],bins=[0,10,20,40,60,120],labels=['children','youth','young','m-aged','elderly'])","6b2c6ab6":"train.head()","508d136f":"train=pd.get_dummies(train,columns=[\"Fare_bin\",\"Age_bin\"])\ntest=pd.get_dummies(test,columns=[\"Fare_bin\",\"Age_bin\"])\ntrain.head()","50f3716b":"train.drop([\"Age\",\"Fare\",\"Ticket\"],axis=1,inplace=True)\ntest.drop([\"Age\",\"Fare\",\"Ticket\"],axis=1,inplace=True)\ntrain.head()","57129925":"test.head()","8a9f3207":"print(train[\"Cabin\"].values.tolist())","5f09fa11":"train[\"Cabin_encoding\"]=train[\"Pclass\"]\nfor i in range(len(train)):\n    if(train[\"Cabin\"].loc[i]==\"NC\"):\n        train[\"Cabin_encoding\"].loc[i]=1000\n    else:\n        if(len(train[\"Cabin\"].loc[i])>4):\n            train[\"Cabin_encoding\"].loc[i]=500\n        elif(len(train[\"Cabin\"].loc[i])>1):\n            temp=train[\"Cabin\"].loc[i]\n            train[\"Cabin_encoding\"].loc[i]=(ord(temp[0])-ord('A'))*125+int(temp[1:])\n        else:\n            temp=train[\"Cabin\"].loc[i]\n            train[\"Cabin_encoding\"].loc[i]=min((ord(temp[0])-ord('A'))*125,1000)\ntrain[[\"Cabin\",\"Cabin_encoding\"]].head()","f07e0fe5":"test[\"Cabin_encoding\"]=test[\"Pclass\"]\nfor i in range(len(test)):\n    if(test[\"Cabin\"].loc[i]==\"NC\"):\n        test[\"Cabin_encoding\"].loc[i]=1000\n    else:\n        if(len(test[\"Cabin\"].loc[i])>4):\n            test[\"Cabin_encoding\"].loc[i]=500\n        elif(len(test[\"Cabin\"].loc[i])>1):\n            temp=test[\"Cabin\"].loc[i]\n            test[\"Cabin_encoding\"].loc[i]=(ord(temp[0])-ord('A'))*125+int(temp[1:])\n        else:\n            temp=test[\"Cabin\"].loc[i]\n            test[\"Cabin_encoding\"].loc[i]=min((ord(temp[0])-ord('A'))*125,1000)","de5dc8c9":"list1=[0,0,0,0,0,0,0,0,0,0,0]\nlist2=[0,0,0,0,0,0,0,0,0,0,0]\n\nfor i in range(len(train)):\n    x=train.loc[i,\"Cabin_encoding\"]\n    t=x\/\/100\n    if(train.loc[i,\"Survived\"]==1):\n        list1[t]+=1\n    list2[t]+=1\n    \nlisty=[0,0,0,0,0,0,0,0,0,0,0]\nfor i in range(11):\n    if(list2[i]>0):\n        listy[i]=list1[i]\/list2[i]\n        \nplt.scatter([0,100,200,300,400,500,600,700,800,900,1000],listy)","dd3ad7f6":"train.drop(\"Cabin\",axis=1,inplace=True)\ntrain.head()","27d25cf3":"test.drop(\"Cabin\",axis=1,inplace=True)\ntest.head()","b1f5d475":"train[\"Cabin_encoding\"]=train[\"Cabin_encoding\"]\/1000\ntrain.info()","1fdc8ced":"train.head()\ntest[\"Cabin_encoding\"]=test[\"Cabin_encoding\"]\/1000","f5bb4524":"sns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2)\nfig=plt.gcf()\nfig.set_size_inches(20,20)\nplt.show()","70e38301":"train.info()","73e9304d":"from sklearn.model_selection import train_test_split\nX=train.drop([\"Survived\",\"PassengerId\"],axis=1)\ny=train[\"Survived\"]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=100)","a012ddc0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\nlr=LogisticRegression(max_iter=1000,random_state=100)\n\nlr.fit(X_train,y_train)\nprediction=lr.predict(X_test)\n\nacc=np.mean(prediction==y_test)\n\nprint(\"Logistic Regression accuracy is \",acc)\n\nsns.heatmap(confusion_matrix(prediction,y_test),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1, size=15)\n\np1=np.array([[91\/109,18\/70],[18\/109,57\/70]])","a07b3c52":"from sklearn.ensemble import RandomForestClassifier\nranF=RandomForestClassifier(random_state=100)\nranF.fit(X_train,y_train)\nprediction=ranF.predict(X_test)\n\nacc=np.mean(y_test==prediction)\nprint(\"RandomForest accuracy is \",acc)\n\nsns.heatmap(confusion_matrix(prediction,y_test),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1, size=15)\n\np2=np.array([[93\/116,11\/63],[23\/116,52\/63]])","222334c1":"from sklearn.ensemble import AdaBoostClassifier\nabc= AdaBoostClassifier(random_state=10)\nabc.fit(X_train,y_train)\nprediction=abc.predict(X_test)\n\nacc=np.mean(prediction==y_test)\nprint(\"AdaBoost Classifier accuracy is \",acc)\n\nsns.heatmap(confusion_matrix(prediction,y_test),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1, size=15)\n\np3=np.array([[91\/116,13\/73],[15\/116,60\/73]])","478bc9e4":"from sklearn.ensemble import GradientBoostingClassifier\ngbc= GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\nprediction=gbc.predict(X_test)\n\nacc=np.mean(prediction==y_test)\nprint(\"GradientBoost Classifier accuracy is \",acc)\n\nsns.heatmap(confusion_matrix(prediction,y_test),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1, size=15)\n\np4=np.array([[92\/110,12\/69],[18\/110,57\/69]])","ddaba033":"from catboost import CatBoostClassifier\ncate_features_index = np.where(X.dtypes != float)[0]\nclf =CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=100)\nclf.fit(X_train,y_train,cat_features=cate_features_index,eval_set=(X_test,y_test),early_stopping_rounds=50)\nprediction=clf.predict(X_test)\nacc=np.mean(prediction==y_test)\n\nprint(\"CatBoost Classifier accuracy is \",acc)\n\nsns.heatmap(confusion_matrix(prediction,y_test),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1, size=15)\np5=np.array([[95\/119,9\/60],[24\/119,51\/60]])","197f3c3c":"ids=test[\"PassengerId\"]\nXtest=test.drop([\"PassengerId\"],axis=1)\npred_1=lr.predict(Xtest)\npred_2=ranF.predict(Xtest)\npred_3=abc.predict(Xtest)\npred_4=gbc.predict(Xtest)\npred_5=clf.predict(Xtest)\npred=[]\n\nfor i in range(len(pred_1)):\n    l_0=1\n    l_1=1\n    x=pred_1[i]+pred_3[i]+pred_4[i]+pred_5[i]\n    if(pred_1[i]==0):\n        l_0*=p1[0,0]\n        l_1*=p1[1,0]\n    else:\n        l_0*=p1[0,1]\n        l_1*=p1[1,1]\n        \n    if(pred_2[i]==0):\n        l_0*=p2[0,0]\n        l_1*=p2[1,0]\n    else:\n        l_0*=p2[0,1]\n        l_1*=p2[1,1]\n        \n    if(pred_3[i]==0):\n        l_0*=p3[0,0]\n        l_1*=p3[1,0]\n    else:\n        l_0*=p3[0,1]\n        l_1*=p3[1,1]\n        \n    if(pred_4[i]==0):\n        l_0*=p4[0,0]\n        l_1*=p4[1,0]\n    else:\n        l_0*=p4[0,1]\n        l_1*=p4[1,1]\n    if(pred_5[i]==0):\n        l_0*=p4[0,0]\n        l_1*=p4[1,0]\n    else:\n        l_0*=p4[0,1]\n        l_1*=p4[1,1]\n        \n    if(x>1):\n        pred.append(1)\n    else:\n        pred.append(0)","656bc645":"print(pred_5)","bc64761b":"Survived=pd.Series(pred)\noutput=pd.concat([ids,Survived],axis=1)\noutput.columns=[\"PassengerId\",\"Survived\"]","644085f2":"output.to_csv(\"submission.csv\",index=False)\noutput.head()","df6f78ef":"**Both the sets have a similar distribution but seems like Pclass,Fare,Sibsp have some relative bias**","a90c6589":"#### Lets check that graphically using histograms","b00c8f56":"#### Lets analyse the missing values","e0fccade":"## Logistic Regression","21a12926":"**Now we need to fill Age which has about 20% missing data in both sets**","28476dbc":"#### Lets make a note of useless columns","5cd96bcd":"## Step1-Loading and rudimentary analysis","e1298902":"**let's check the relation between Cabin and Pclass,it seems only first class have a cabin**","bd08f3d2":"**The features used to interpolate age Name,Parch,SibSp,Fare**\n\nWe need 2 regular expression match the name initials and create another column & use that","ac5fc1fc":"### Time to fill the missing age values","e9a9f627":"*Check the distribution among those having a Cabin*","aaabaaf9":"**Now lets fill Embarked and Fare**","355e1fd6":"## AdaBoost Classifier","97ed5aee":"## Random Forest","73a60ce0":"# Plz Upvote if u found it useful!!!","d6705217":"## Ensemble the 5 models    ->     p(1|0)=p[1,0]","817553d1":"### This shows NAN denotes mostly not having a cabin and they arent missing values\n### So lets replace NAN with a value NC denoting no cabin","334e9db0":"## Steps involved\n1)Load and analyse train and test data \n\n2)Missing value interpolation\n\n3)EDA to find relationships between features\n\n4)Feature engineering\n\n5)Modelling \n\n6)Prediction\n\n7)Hyperparameter tuning using Grid search\n","8afee874":"**Observations**\n\n1)Age definitely has bias there are more infants and less elderly in the train set compared to the test ","c375ce73":"### Lets check the distribution of features","b3362749":"### Step5:Modelling","0ed7e0b8":"### Use 2 random forest regressor to interpolate the age in train,test sets\n\n### Use 2 as age distribution isnt same in both the sets","e5a90bd1":"### Get the training,validation splits","264dc6ca":"## Cat Boosting","e22e69c1":"## Step2&3-Fill the missing values and feature engg","bceac5a5":"**Lets try something with the cabin**","54e1ea67":"## GB Classifier"}}