{"cell_type":{"62fd925d":"code","1654f715":"code","b0ee99b2":"code","78845d78":"code","8aa36ea9":"code","d881c9b0":"code","da6b4bbe":"code","5d5abb4e":"code","770c08b2":"code","c68c2a81":"code","9d7d0df2":"code","c2d59707":"code","24a3b78b":"code","a84df0cf":"code","71f98b47":"code","94cbdd93":"code","99d60e1a":"code","ff95fca3":"code","2aaf878d":"code","65bac0d4":"code","e857d3de":"code","349181d8":"code","31c45634":"code","cdfce881":"code","83fcb2b2":"code","e37dca30":"code","c2206123":"code","9aab8329":"code","c50bd682":"code","6a0e9d59":"code","0163fcd0":"code","564fdce9":"code","115778d2":"code","43ee6471":"code","9d0752eb":"code","ff4e23b4":"code","1a5675d4":"code","3aaf4633":"code","976f57e6":"code","8f4201ae":"code","10a91b5a":"code","035d0433":"code","d0b48737":"code","143ded11":"code","f56899f7":"code","b9cc92d3":"code","2fe81a66":"code","e3ca5a5e":"code","a5584c8b":"code","01d578f5":"code","c662d3a6":"code","f40345de":"code","10957bdf":"code","adc48a89":"code","678219d7":"code","1dc31772":"code","f968ae57":"code","f793454f":"code","c9da3aaf":"code","e6dac95b":"code","2af6984f":"code","ab6ea186":"code","9e756b53":"code","e1518c0f":"code","a53db8a8":"code","2a1de72e":"code","b39e6682":"code","3dbf1ee6":"code","c84d0c05":"code","bf142631":"code","58736c51":"code","9c4739db":"code","51997ed0":"code","db474bce":"code","29497896":"code","b08a1303":"code","01eb47ac":"code","c25a876c":"code","75fb13eb":"code","0ec4767a":"code","d901c39b":"code","b92328be":"code","34374e9d":"markdown","9e1fdef3":"markdown","37a386fd":"markdown","47514493":"markdown","88d7fd6b":"markdown","83c451ef":"markdown","062b3a88":"markdown","20c4cf87":"markdown","1cc6d132":"markdown","3feede5b":"markdown","baa8c18a":"markdown","d1f91dbf":"markdown","08f769f3":"markdown","5eed5da3":"markdown","50bb41f9":"markdown","ecf81fb0":"markdown","acac5acb":"markdown","b80d73ae":"markdown","5e316d38":"markdown","de09bdcf":"markdown","5b9ce701":"markdown","feda7c72":"markdown","182fa7f6":"markdown","b3dde5fa":"markdown","b66383b9":"markdown","0d06b46f":"markdown","208120d7":"markdown","73ca4f31":"markdown","f4d54125":"markdown","5941b405":"markdown","1554f61b":"markdown","ce4c411d":"markdown","eb68a88f":"markdown","f29c0884":"markdown","8650b2c3":"markdown"},"source":{"62fd925d":"import re\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn')\n\n%matplotlib inline","1654f715":"from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport statsmodels.api as sm\n\nimport itertools\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n","b0ee99b2":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","78845d78":"item_categories_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nprint(\"item_categories_df Shape : {}\".format(item_categories_df.shape))\nitem_categories_df.head()","8aa36ea9":"shops_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nshops_df.head()","d881c9b0":"items_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nprint(\"items_df Shape : {}\".format(items_df.shape))\nitems_df.head()","da6b4bbe":"test_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nprint(\"train dataset Shape : {}\".format(train_df.shape))\ntrain_df.head()","5d5abb4e":"print(\"train dataset Shape : {}\".format(train_df.shape))\nprint(\"test dataset Shape : {}\".format(test_df.shape))","770c08b2":"train_df.info()","c68c2a81":"train_df.describe()","9d7d0df2":"sns.color_palette(\"YlOrRd\",  as_cmap=True)","c2d59707":"Reds_palette = sns.color_palette(\"Reds\", 10)\nYlOrBr_palette = sns.color_palette(\"YlOrRd\", 10)\n\nsns.palplot(Reds_palette)\nsns.palplot(YlOrBr_palette)","24a3b78b":"train_df.head()","a84df0cf":"pd.DataFrame(train_df.iloc[:, 4].sort_values(ascending=False)).head(10).style.background_gradient(cmap='Reds')","71f98b47":"pd.DataFrame(train_df.iloc[:, 5].sort_values(ascending=False)).head(10).style.background_gradient(cmap='YlOrBr')","94cbdd93":"fig, axes = plt.subplots(2,1, figsize=(10,5), constrained_layout=True)\n\nsns.boxplot(train_df['item_price'], ax=axes[0])\naxes[0].set_title(\"Distribution [item_price] Boxplots\", fontweight=\"bold\", fontfamily='serif', fontsize=14)\naxes[0].patch.set_alpha(0)\n\nsns.boxplot(train_df['item_cnt_day'], ax=axes[1])\naxes[1].set_title(\"Distribution [item_cnt_day] Boxplots\", fontweight=\"bold\", fontfamily='serif', fontsize=14)\naxes[1].patch.set_alpha(0)\n\nplt.show()","99d60e1a":"def preporcess_data(df):\n    print(\"Before cleansing shape : {}\".format(df.shape))\n    print(\"----- CLEANSING START -----\")\n    \n    # drop outlier value\n    df = df.drop(df[df['item_price']>=45000].index)\n    df = df.drop(df[df['item_price']<0].index)\n    df = df.drop(df[df['item_cnt_day']>=600].index)\n    df = df.drop(df[df['item_cnt_day']<0].index)\n    print(df.shape)\n    \n    \n    print(\"----- CLEANSING END -----\")\n    print(\"After cleansing shape : {}\".format(df.shape))\n    return df","ff95fca3":"train_df = preporcess_data(train_df)","2aaf878d":"Reds_palette_59 = sns.color_palette(\"Reds\", 59)","65bac0d4":"fig, ax = plt.subplots(1, 1, figsize=(15,6))\n\nsns.countplot(data=train_df, x='shop_id', ax=ax, palette=Reds_palette_59)\nax.set_title('Destribute \"Shop ID\" count of train data', fontweight=\"bold\", fontfamily='serif', fontsize=18)\nax.set(xlabel=\"Shop ID\", ylabel = \"\")\nax.patch.set_alpha(0)\nplt.show()","e857d3de":"YlOrBr_palette_89 = sns.color_palette(\"YlOrRd\", 89)","349181d8":"def input_items_key_output_items_value(key):\n    return item_dict[key]","31c45634":"item_dict = {key:value for key, value in zip(items_df['item_id'], items_df['item_category_id'])}\nprint(\"item_dict size : {}\".format(len(item_dict)))","cdfce881":"train_df['item_category_id'] = train_df.apply(lambda x : input_items_key_output_items_value(x['item_id']), axis=1)\ntrain_df.head()","83fcb2b2":"fig, ax = plt.subplots(1, 1, figsize=(20,7))\n\nsns.countplot(data=train_df, x='item_category_id', ax=ax, palette=YlOrBr_palette_89)\nax.set_title('Destribute \"Item Categori\" count of train data', fontweight=\"bold\", fontfamily='serif', fontsize=19)\nax.set(xlabel=\"Item Category ID\", ylabel = \"\")\nax.patch.set_alpha(0)\nplt.show()","e37dca30":"train_df['sales'] = train_df['item_price'] * train_df['item_cnt_day']\ntrain_df.head()","c2206123":"def input_shopid_output_sales(tim_data, shop_id):\n    shop_sales = []\n    for i in range(len(tim_data)):\n        a = train_df[(train_df['date_block_num']==i) & (train_df['shop_id']==shop_id)]['sales'].sum()\n        shop_sales.append(a)\n    \n    return shop_sales","9aab8329":"time_data = train_df['date_block_num'].unique()\n\nfig, axes = plt.subplots(2,4, figsize=(15,7), constrained_layout=True)\nx_idx = 0\ny_idx = 0\n\nfor i in range(2*4):\n    if x_idx == 4:\n        x_idx = 0\n        y_idx += 1\n        \n    random_index = np.random.randint(0,61)\n    sales_data = input_shopid_output_sales(time_data, random_index)\n    sales_data_mean = np.mean(sales_data)\n\n    \n    axes[y_idx][x_idx].plot(sales_data, linewidth=4.0, color=Reds_palette[-1*(i+1)], label='Shop id {}'.format(random_index))\n    axes[y_idx][x_idx].axhline(y=sales_data_mean, color='k',linestyle='--', label='Sales mean')\n    axes[y_idx][x_idx].set_ylim(0, train_df['sales'].max()*6)\n    axes[y_idx][x_idx].set_title(\"Shop id : {} | mean : {:.2f}\".format(random_index, sales_data_mean), fontweight=\"bold\", fontfamily='serif', fontsize=11)\n    axes[y_idx][x_idx].legend()\n    \n    axes[y_idx][x_idx].set_xticks([])\n    if x_idx!=0:\n        axes[y_idx][x_idx].set_yticks([])\n        \n    axes[y_idx][x_idx].patch.set_alpha(0)\n    x_idx += 1\nfig.text(0,1.08, \"Destribute sales figures for each store\", fontweight=\"bold\", fontfamily='serif', fontsize=18)\nplt.show()","c50bd682":"train_df.head()","6a0e9d59":"group = train_df.groupby(['date_block_num', 'shop_id', 'item_id']).agg(\n    {'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\ntrain_df = pd.merge(train_df, group, on=['shop_id', 'item_id', 'date_block_num'], how='left')\ntrain_df['item_cnt_month'] = (train_df['item_cnt_month']\n                        .fillna(0)\n                        .clip(0, 100)\n                        .astype(np.float16))","0163fcd0":"train_df[(train_df['shop_id']==31) & (train_df['date_block_num']==0) & (train_df['item_id']==4906)].head()","564fdce9":"train_df = train_df.drop_duplicates(['date_block_num','shop_id','item_id','item_cnt_month'])\nprint(\"train data Shape : {}\".format(train_df.shape))\ntrain_df.head()","115778d2":"Reds_palette_89 = sns.color_palette(\"Reds\", 89)\nReds_palette_100 = sns.color_palette(\"Reds\", 100)","43ee6471":"fig, ax = plt.subplots(1,1, figsize=(12,5))\n\n\nsns.kdeplot(x='item_price', data=train_df, \n                        fill=True, cut=0, bw_method=0.20, color=Reds_palette[2],\n                        lw=1.4 , ax=ax, alpha=0.3)\nax.set_ylabel('')\nax.set_xlabel('')\nax.set_yticks([])\nax.spines[[\"top\",\"right\",\"left\",\"bottom\"]].set_visible(False)\nax.patch.set_alpha(0)\nax.legend([],[], frameon=False) #remove legend\n\nfig.text(0.15,0.91,\"Count distribution by Price in Data\", fontweight=\"bold\", fontfamily='serif', fontsize=17)\nplt.show()","9d0752eb":"price_dict = {key:value for key, value in zip(train_df['item_id'], train_df['item_price'])}\nprint(\"price_dict size : {}\".format(len(price_dict)))","ff4e23b4":"price_dict_df = pd.DataFrame.from_dict(price_dict, orient='index', columns=['price'])\nprice_df = price_dict_df.reset_index()\nprice_df.columns = ['item_id', 'price']\n\nprice_df.head()","1a5675d4":"price_list = []\nfor val in price_df['price'].values:\n    price_list.append([val])\n    \nprint(price_list[:10])","3aaf4633":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nstd_scaler = StandardScaler()\n\nstd_scaler.fit(price_list)\nstd_price = std_scaler.transform(price_list)\nstd_price_df = pd.DataFrame(std_price, columns=['std_price'])","976f57e6":"price_df = pd.concat([price_df, std_price_df], axis=1)\nprice_df.head()","8f4201ae":"std_price_dict = {key: val for key, val in zip(price_df['item_id'], price_df['std_price'])}\nprint(\"std_price_dict size : {}\".format(len(std_price_dict)))","10a91b5a":"def input_items_key_output_items_price(key):\n    return std_price_dict[key]\n\ntrain_df['item_price_std'] = train_df.apply(lambda x : input_items_key_output_items_price(x['item_id']), axis=1)","035d0433":"train_df['item_price_log'] = np.log1p(train_df[\"item_price\"])\ntrain_df.head()","d0b48737":"fig, ax = plt.subplots(1,1, figsize=(12,5))\n\n\nsns.kdeplot(x='item_price_std', data=train_df, \n                        fill=True, cut=0, bw_method=0.20, color=Reds_palette[3],\n                        lw=1.4 , ax=ax, alpha=0.3, label='StandardScaler')\nsns.kdeplot(x='item_price_log', data=train_df, \n                        fill=True, cut=0, bw_method=0.20, color=Reds_palette[7],\n                        lw=1.4 , ax=ax, alpha=0.3, label='LogTransform')\nax.set_ylabel('')\nax.set_xlabel('')\nax.set_yticks([])\nax.spines[[\"top\",\"right\",\"left\",\"bottom\"]].set_visible(False)\nax.patch.set_alpha(0)\nax.legend()\n\nfig.text(0.15,0.91,\"Distribution by StandardScaler vs LogTransform\", fontweight=\"bold\", fontfamily='serif', fontsize=17)\nplt.show()","143ded11":"train_df.head()","f56899f7":"train_df = train_df.drop(['date', 'item_price', 'item_cnt_day', 'sales', 'item_price_std', ], axis=1)\nprint(\"train data Shape : {}\".format(train_df.shape))\ntrain_df.head()","b9cc92d3":"test_df = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest_df.head()","2fe81a66":"test_df['date_block_num'] = 34","e3ca5a5e":"items_df.head()","a5584c8b":"item_dict = {key:value for key, value in zip(items_df['item_id'], items_df['item_category_id'])}\nprint(\"item_dict size : {}\".format(len(item_dict)))\n\ntest_df['item_category_id'] = test_df.apply(lambda x : input_items_key_output_items_value(x['item_id']), axis=1)","01d578f5":"price_df.head()","c662d3a6":"id_price_dict = {key: val for key, val in zip(price_df['item_id'], price_df['price'])}\nprint(\"id_price_dict size : {}\".format(len(std_price_dict)))","f40345de":"def input_items_key_output_items_price_test(key):\n    try:\n        return_val = id_price_dict[key]\n    except:\n        return_val = None\n    return return_val\n\ntest_df['item_price'] = test_df.apply(lambda x : input_items_key_output_items_price_test(x['item_id']), axis=1)","10957bdf":"# Get test data's item price based on train data\n# If test data that had null values, then I'll use median values\n\ntest_df['item_price'].fillna(test_df['item_price'].median(), inplace=True)\nprint(\"Value of item median preice : {}\".format(test_df['item_price'].median()))","adc48a89":"test_df['item_price_log'] = np.log(test_df['item_price'])","678219d7":"test_df.drop(['ID', 'item_price'],axis=1, inplace=True)\ntest_df.head()","1dc31772":"from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport statsmodels.api as sm\n\nimport itertools","f968ae57":"sample_series = pd.DataFrame(train_df[(train_df['shop_id']==29) & (train_df['item_id']==5822)]['item_cnt_month']).reset_index().drop('index',axis=1)\nsample_series.head()","f793454f":"train_sample, test_sample = train_test_split(sample_series, test_size=0.15, shuffle=False)\n\nfig, ax = plt.subplots(1,1, figsize=(11,5))\nplt.plot(sample_series, label=\"Train data\", color=YlOrBr_palette[5])\nplt.plot(test_sample, label=\"Test data\",color=YlOrBr_palette[3])\nplt.axvline(x=len(train_sample), color='r',linestyle='--', linewidth=2)\n\nax.set_yticks(np.arange(0,31,10))\nax.set_xticks([])\n\nax.legend()\nax.patch.set_alpha(0)\n\nfig.text(0.15,0.91,\"Sample Train data & Sample Test data Plot\", fontweight=\"bold\", fontfamily='serif', fontsize=16)\nplt.show()","c9da3aaf":"print(\"Sample Train data size : {}\".format(train_sample.shape))\nprint(\"Sample Test data size : {}\".format(test_sample.shape))","e6dac95b":"res = sm.tsa.seasonal_decompose(train_sample.values, freq=12, model='addidtive')\nfig = res.plot()\nfig.set_size_inches(10,6)\nfig.text(0.28,1,\" Trend \/ Seasonal \/ Residual Distribute of train data\", fontweight=\"bold\", fontfamily='serif', fontsize=14)\nplt.show()","2af6984f":"fig, axes = plt.subplots(1,2, figsize=(12,5))\n\nplot_acf(sample_series,lags=13, ax=axes[0])\nplot_pacf(sample_series,lags=13, ax=axes[1])\naxes[1].set_yticks([])\n\nfig.text(0.38,1,\"Distribute ACF \/ PACF\", fontweight=\"bold\", fontfamily='serif', fontsize=16)\nplt.show()","ab6ea186":"diff_train_sample = train_sample.copy()\ndiff_train_sample = diff_train_sample.diff()\ndiff_train_sample = diff_train_sample.dropna()\nprint(\"=== Basic Data ===\")\nprint(diff_train_sample[:5],\"\\n\\n\")\n\nprint(\"=== Diff Data ===\")\nprint(diff_train_sample[:5])","9e756b53":"fig, ax = plt.subplots(1,1, figsize=(11,5))\nax.plot(train_sample, label=\"Train data\", color=YlOrBr_palette[5])\nax.plot(diff_train_sample, label=\"Diff train data\",color=YlOrBr_palette[3])\n\nax.set_yticks(np.arange(0,31,10))\nax.set_xticks([])\n\nax.legend()\nax.patch.set_alpha(0)\n\nfig.text(0.15,0.91,\"Train data & Diff Train data Plot\", fontweight=\"bold\", fontfamily='serif', fontsize=16)\nplt.show()\n","e1518c0f":"fig, axes = plt.subplots(1,2, figsize=(12,5))\n\nplot_acf(diff_train_sample,lags=10, ax=axes[0])\nplot_pacf(diff_train_sample,lags=10, ax=axes[1])\naxes[1].set_yticks([])\n\nfig.text(0.315,1,\"Distribute ACF \/ PACF Diff of Train data\", fontweight=\"bold\", fontfamily='serif', fontsize=16)\nplt.show()","a53db8a8":"p = range(0,5)\nd = range(1,2)\nq = range(0,5)\npdq = list(itertools.product(p,d,q))\npdq = pdq[1:] # Drop (0,1,0)\n\nprint(\"Using parameters : {}\".format(pdq))","2a1de72e":"# Parameter Search\naic = []\nfor i in pdq:\n    model = ARIMA(train_sample.values, order=(i))\n    model_fit = model.fit()\n    print(\"ARIMA : {} | AIC : {:.3f}\".format(i,model_fit.aic))\n    aic.append(model_fit.aic)","b39e6682":"best_pdq = [pdq[idx] for idx, num in enumerate(aic) if num==min(aic)][0]\nprint(\"Best Parameters : {}\".format(best_pdq))","3dbf1ee6":"arima_model = ARIMA(train_sample.values, order=best_pdq).fit()\narima_model.summary()","c84d0c05":"pred = arima_model.forecast(len(test_sample))\nprint(\"Prdiction values : {}\".format(pred))","bf142631":"fig, ax = plt.subplots(1,1, figsize=(11,5))\n\nax.plot(sample_series, label=\"Train data\", color=YlOrBr_palette[4])\nax.plot(np.arange(0,len(pred)) + len(train_sample),pred, label=\"Prediction data\",color='r', linewidth=2)\nax.plot(test_sample, label=\"Label data\",color='k', linewidth=1)\n        \nax.axvline(x=len(train_sample), color='r',linestyle='--', linewidth=2)\n\nax.set_yticks(np.arange(0,31,10))\nax.set_xticks([])\n\nax.legend()\nax.patch.set_alpha(0)\n\nfig.text(0.15,0.91,\"Train data & Diff Train data Plot\", fontweight=\"bold\", fontfamily='serif', fontsize=16)\nplt.show()","58736c51":"x = train_df.drop('item_cnt_month', axis=1)\ny = train_df['item_cnt_month']","9c4739db":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\nprint(\"X train data shape : {}\".format(x_train.shape))\nprint(\"Y train data shape : {}\".format(y_train.shape))\n\nprint(\"X test data shape : {}\".format(x_test.shape))\nprint(\"Y test data shape : {}\".format(y_test.shape))","51997ed0":"x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.6)\nprint(\"X validation data shape : {}\".format(x_val.shape))\nprint(\"Y validation data shape : {}\".format(y_val.shape))\n\nprint(\"X test data shape : {}\".format(x_test.shape))\nprint(\"Y test data shape : {}\".format(y_test.shape))","db474bce":"import lightgbm as lgb\nfrom lightgbm import plot_importance","29497896":"train_data = lgb.Dataset(x_train, label=y_train)\nval_data = lgb.Dataset(x_val, label=y_val)","b08a1303":"params = {\n    'device' : 'gpu',\n    'n_estimators': 5000,\n    'num_leaves': 255,\n    'max_depth': 20,\n    'min_data_in_leaf': 1000,\n    'learning_rate': 0.02,\n    'boosting': 'gbdt',\n    'num_boost_round': 3000,\n    'feature_fraction': 0.8, \n    'bagging_fraction': 0.7,\n    'objective': 'mse',\n    'metric': 'rmse',\n    'n_jobs': -1\n}","01eb47ac":"model = lgb.train(params,\n                  train_data,\n                  valid_sets=(train_data, val_data), \n                  verbose_eval=20,\n                  early_stopping_rounds=100)","c25a876c":"YlOrBr_palette_5 = sns.color_palette(\"YlOrRd\", 5)\nsns.palplot(YlOrBr_palette_5)","75fb13eb":"fig, ax = plt.subplots(1,1, figsize=(8,6))\nplot_importance(model, max_num_features=5,color=YlOrBr_palette_5, ax=ax)\nax.set_title(\"Distribution of Column Specific Importance \", fontweight=\"bold\", fontsize=15)\nax.patch.set_alpha(0) \nplt.show()","0ec4767a":"submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\ntest_df.head()","d901c39b":"pred = model.predict(test_df)\npred = list(x if x>0 else 0 for x in pred)\n\nsubmission['item_cnt_month'] = pred\nsubmission.head()","b92328be":"submission.to_csv('lgbm_submission1.csv', index=False)","34374e9d":"##### 5-2-1) Training","9e1fdef3":"# 1. Import & Install libray\n* Import Basic libray\n* Import Enginnering libray","37a386fd":"# 4. Feature Engineering\n* Get \"item_cnt_month\" column\n* Drop rows with columns \"date_block_num\", \"shop_id\", and \"item_id\"\n* Collect information about prices for items_id and Normalize prices\n* Drop columns what the model does not help to learn.\n* Test data Feature Engineering","47514493":"##### reference \n* https:\/\/www.kaggle.com\/tylerchenchen\/predict-sales-problem-step-by-step-part1\n* https:\/\/byeongkijeong.github.io\/ARIMA-with-Python\/\n* https:\/\/www.youtube.com\/watch?v=rdR2fNDq6v0\n\n###  If this notebook is useful for your kaggling, \"UPVOTE\" for it \ud83d\udc40\n#### THX to Reading My Notebook\ud83c\udf08","88d7fd6b":"##### 5-1-3) Check ACF \/ PACF\n* ACF \/ PACF are the function that confirms the normality of time series data.","83c451ef":"#### \u2714\ufe0f This notebook will use this palettes.","062b3a88":"### 5-2) LightGBM Modeling\n* Divide the data into training, testing and evaluatio\n* Training","20c4cf87":"### 4-3) Collect information about prices for items_id and Normalize prices","1cc6d132":"### 5-1) Arima Modeling\n##### 5-1-1) Get sample data","3feede5b":"#### => Get Log Transform value in price DataFrame","baa8c18a":"### 3-3) Plot Destribute \"Item Categori\" count of train data","d1f91dbf":"##### 5-2-1) Divide the data into training, testing and evaluatio","08f769f3":"* You can check the same \"item_cnt_month\" column","5eed5da3":"# 6. Submission\n* Submit the predictions","50bb41f9":"##### 5-1-6) ARIMA Model's Prediction","ecf81fb0":"![pred_sales.jpg](attachment:2afd5708-ddd3-4417-8b6a-6f172a592afc.jpg)","acac5acb":"# Predict Future Sales\n\n## Overview\nTrain data is given a variety of information, but Test data is not given much information. <br\/>\nSo we have to learn Train data in a time series and then predict sales volume for the next month based on Test data. <br\/>\nWe believe that various datasets should be flexibly used to create data that the model can learn. <br\/>\n\n#### My Opinion\n* 1) Let's visualize the time-series elements of the data to understand the patterns.\n* 2) The input dimension of the model learned with Train data should be well thought out and predicted with Test data.","b80d73ae":"* Get item_dictionary => ( item_id : item_category_id )","5e316d38":"##### 5-1-2) Check the \"Trend \/ Seasonal \/ Residual\" in train data","de09bdcf":"### 3-1) Check \"item_price\", \"item_cnt_day\" columns outlier","5b9ce701":"# 2. Check out my data\n* Check Shape \/ Info","feda7c72":"##### 5-1-5) ARIMA Modeling\n\n##### 5-1-6) ARIMA Model's Prediction","182fa7f6":"### 4-4) Drop columns what the model does not help to learn.","b3dde5fa":"### 4-5) Test data Feature Engineering","b66383b9":"# 5.Modeling\n##### 1) Arima Modeling\n* Get sample data\n* Check the \"Trend \/ Seasonal \/ Residual\" in train data\n* Check ACF \/ PACF\n* Differencing train data\n* ARIMA Modeling\n* ARIMA Model's Prediction\n\n\n##### 2) LightGBM Modeling\n* Divide the data into training, testing and evaluatio\n* Training","0d06b46f":"### 4-1) Get \"item_cnt_month\" column","208120d7":"* We use LogTransform Values","73ca4f31":"* Verify that the graph is skewed to one side. We should Standard the data","f4d54125":"***\n\n## My workflow\n#### 1. Import & Install libray\n* Import Basic libray\n* Import Enginnering libray\n\n#### 2. Check out my data\n* Check Shape \/ Info\n\n#### 3. Exploratory Data Analysis(EDA) with Visualization [with Preprocessing]\n* Check \"item_price\", \"item_cnt_day\" columns outlier\n* Plot the Destribute \"Shop ID\" count of train data\n* Plot Destribute \"Item Categori\" count of train data\n* Plot the sales figures for each store.\n\n#### 4. Feature Engineering\n* Get \"item_cnt_month\" column\n* Drop rows with columns \"date_block_num\", \"shop_id\", and \"item_id\"\n* Collect information about prices for items_id and Normalize prices\n* Drop columns what the model does not help to learn.\n\n#### 5.Modeling\n##### 1) Arima Modeling\n* Get sample data\n* Check the \"Trend \/ Seasonal \/ Residual\" in train data\n* Check ACF \/ PACF\n* Differencing train data\n* ARIMA Modeling\n* ARIMA Model's Prediction\n\n##### 2) LightGBM Modeling\n* Divide the data into training, testing and evaluatio\n* Training\n\n#### 6. Submission\n* Submit the predictions","5941b405":"# 3. Exploratory Data Analysis(EDA) with Visualization [with Preprocessing]\n* Check \"item_price\", \"item_cnt_day\" columns outlier\n* Plot the Destribute \"Shop ID\" count of train data\n* Plot Destribute \"Item Categori\" count of train data\n* Plot the sales figures for each store.","1554f61b":"##### 5-1-4) Differencing train data\n* Non Stationary data is converted into Stationary data through differencing.","ce4c411d":"#### => Get Srandard value in price DataFrame","eb68a88f":"### 4-2) Drop rows with columns \"date_block_num\", \"shop_id\", and \"item_id\"","f29c0884":"### 3-2) Plot the Destribute \"Shop ID\" count of train data","8650b2c3":"### 3-4) Plot the sales figures for each store.\n* get \"Sales\" column => ( \"item_price\" * \"item_cnt_day\" )"}}