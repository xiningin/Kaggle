{"cell_type":{"8a1c5a1d":"code","88549d5a":"code","dc18a4f9":"code","27e6b095":"code","020cee08":"code","9229c3ef":"code","64dafe05":"code","6b768f3a":"code","8911a478":"code","01c7e87d":"code","3681c51a":"code","f1d031d4":"code","d498352e":"code","79f8c43d":"code","0a6e72e3":"code","dce68981":"code","8ebd20f1":"code","e9bf9372":"code","1c89f419":"code","13107944":"code","aca82df5":"code","c07f167c":"code","18961d2f":"code","dd4e6329":"code","04c0ebc1":"code","1c0ef51c":"code","57de8d44":"code","ab783bd2":"code","4bb9131b":"code","d174d774":"code","6f83f377":"code","9afed5a1":"code","b26daf2d":"code","e8e64f52":"code","a1927b37":"code","2c9b2825":"code","ed4864ba":"code","b44cc4bb":"code","d790cf13":"markdown","b885af70":"markdown","9d04cc2e":"markdown","0094db63":"markdown","34e0f68f":"markdown","c8cbc910":"markdown","9c61d00e":"markdown","401f764a":"markdown","9dbfdc40":"markdown","fbe8b512":"markdown","b51c951c":"markdown","4867ab52":"markdown","070b893d":"markdown","4f86102f":"markdown","6c1c2551":"markdown","293305f2":"markdown","19af0c67":"markdown","435bb001":"markdown","3f268e9d":"markdown","7a74bcb9":"markdown","8cac36d0":"markdown","0f45a9dd":"markdown","cab2e54a":"markdown","5a7afd35":"markdown","42778595":"markdown"},"source":{"8a1c5a1d":"import  numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset","88549d5a":"train_csv = pd.read_csv('..\/input\/digit-recognizer\/train.csv', dtype = np.float32)\ntest_csv = pd.read_csv('..\/input\/digit-recognizer\/test.csv', dtype = np.float32)\ntrain_csv.head(5)","dc18a4f9":"labels = train_csv.pop('label')\nlabels = labels.to_numpy() # convert to numpy array\n\ndata = train_csv.to_numpy() \/ 255.0 # normalization\nprint('Data shape:', data.shape, 'Labels shape:', labels.shape)","27e6b095":"sns.countplot(labels)\nplt.title('Class Distribution')","020cee08":"plt.figure(figsize=(12, 10))\nfor i in range(20):\n    plt.subplot(5, 4, i+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(data[i].reshape(28,28)) ","9229c3ef":"x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2)\nprint('Train shape:', x_train.shape, 'Val shape:', x_val.shape)\n\nx_train_tensor = torch.from_numpy(x_train)\ny_train_tensor = torch.from_numpy(y_train).type(torch.LongTensor)\n\nx_val_tensor = torch.from_numpy(x_val)\ny_val_tensor = torch.from_numpy(y_val).type(torch.LongTensor)","64dafe05":"train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","6b768f3a":"device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')","8911a478":"input_size = 28 * 28\noutput_size = 10\nhidden_size = 100\nlearning_rate = 0.001\nnum_epochs = 10\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LogisticRegression, self).__init__()\n        # Linear model\n        self.linear = nn.Linear(input_size, output_size)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\nmodel = LogisticRegression(input_size, output_size).to(device)","01c7e87d":"# Cross Entropy Loss  \ncriterion = nn.CrossEntropyLoss()\n\n# SGD Optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","3681c51a":"loss_list = []\niteration_list = []\naccuracy_list = []\niter_num = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # input and label\n        train = images.view(-1, 28*28)\n        labels = labels\n      \n        # Forward pass\n        outputs = model(train)\n        \n        # Loss calculate\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        iter_num += 1\n        \n        if (i+1) % 50 == 0:\n            correct = 0\n            total = 0\n            for images, labels in val_loader:\n                \n                # Forward pass\n                images = images.view(-1, 28*28)\n                outputs = model(images)\n                \n                # Predictions\n                predicted = torch.max(outputs, 1)[1]\n                \n                # Total number of samples\n                total += labels.size(0)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * (correct\/total)\n            loss_list.append(loss.data)\n            iteration_list.append(iter_num)\n            accuracy_list.append(accuracy)\n        if (iter_num+1) % 1000 == 0:\n            # Print Loss\n            print('Iteration: {} Loss: {:0.4f} Val Accuracy: {:0.4f}%'.format(iter_num+1, loss.data, accuracy))","f1d031d4":"# Visualize loss\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Num. of Iters.\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Num. of Iters.\")\nplt.show()\n\n# Visualize loss\nplt.plot(iteration_list,accuracy_list)\nplt.xlabel(\"Num. of Iters.\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Logistic Regression: Accuracy vs Num. of Iters.\")\nplt.show()","d498352e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision","79f8c43d":"data = pd.read_csv('..\/input\/digit-recognizer\/train.csv', dtype = np.float32)\nlabels = data.pop('label').astype('int32')\n\ndata.head() # let's see first five rows","0a6e72e3":"labels.head() # after converting labels to `int32`","dce68981":"data = data.to_numpy() \/ 255.0 # converting to numpy and normalizing between 0 and 1\nlabels = labels.to_numpy()","8ebd20f1":"x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2)\nprint(f'x_train.shape: {x_train.shape}, x_val.shape: {x_val.shape}')","e9bf9372":"plt.figure(figsize=(12, 10))\nfor i in range(16):\n    plt.subplot(4, 4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_train[i].reshape(28,28), cmap=plt.cm.binary)\n    plt.xlabel(y_train[i])\nplt.show()","1c89f419":"x_train_tensor = torch.from_numpy(x_train)\ny_train_tensor = torch.from_numpy(y_train).type(torch.LongTensor)\n\nx_val_tensor = torch.from_numpy(x_val)\ny_val_tensor = torch.from_numpy(y_val).type(torch.LongTensor)\n\nprint(f'x_train_tensor.dtype: {x_train_tensor.dtype}, y_train_tensor.dtype: {y_train_tensor.dtype}')","13107944":"train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\nval_data = torch.utils.data.TensorDataset(x_val_tensor, y_val_tensor)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=1, shuffle=False)","aca82df5":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","c07f167c":"learning_rate = 0.001\nnum_epochs = 10\nnum_classes = 10\n\n# Convolutional neural network (two convolutional layers)\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\n\nmodel = ConvNet(num_classes).to(device)","18961d2f":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","dd4e6329":"iter_num = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.view(32, 1, 28, 28).to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        iter_num += 1\n        \n        if iter_num % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through val dataset\n            for images, labels in val_loader:\n                test = images.view(1, 1, 28, 28).to(device)\n                labels = labels.to(device)\n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n\n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(iter_num)\n            accuracy_list.append(accuracy)\n        if iter_num % 500 == 0:\n            # Print Loss\n            print('Iteration: {},  Loss: {:.4f},  Accuracy: {:.4f} %'.format(iter_num, loss.data, accuracy))","04c0ebc1":"# Evaluation\ncorrect = 0\ntotal = 0\n# Iterate through val dataset\nfor images, labels in val_loader:\n    test = images.view(1, 1, 28, 28).to(device)\n    labels = labels.to(device)\n    # Forward propagation\n    outputs = model(test)\n\n    # Get predictions from the maximum value\n    predicted = torch.max(outputs.data, 1)[1]\n\n    # Total number of labels\n    total += len(labels)\n\n    correct += (predicted == labels).sum()\n\naccuracy = 100 * correct \/ float(total)\nprint(accuracy)","1c0ef51c":"# Visualize loss\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Num. of Iters.\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss vs Num. of Iters.\")\nplt.show()\n\n# Visualize Accuracy\nplt.plot(iteration_list,accuracy_list)\nplt.xlabel(\"Num. of Iters.\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracyvs Num. of Iters.\")\nplt.show()","57de8d44":"import  numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms","ab783bd2":"data = pd.read_csv('..\/input\/digit-recognizer\/train.csv', dtype = np.float32)\nlabels = data.pop('label').astype('int64')\n\ndata = data.to_numpy() \/ 255.0 # converting to numpy and normalizing between 0 and 1\nlabels = labels.to_numpy()\n\ndata = data.reshape(-1, 28, 28, 1)\nlabels = labels.reshape(-1,1)\nprint(labels.shape)","4bb9131b":"x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2)\nprint(f'x_train.shape: {x_train.shape}, x_val.shape: {x_val.shape}')","d174d774":"class MNISTDataset(Dataset):\n        \n    def __init__(self, images, labels, transform = None):\n        \"\"\"Method to initilaize variables.\"\"\" \n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __getitem__(self, index):\n        label = self.labels[index]\n        image = self.images[index]\n        \n        if self.transform is not None:\n            image = self.transform(image)\n        image = image.repeat(3, 1, 1)\n        return image, label\n\n    def __len__(self):\n        return len(self.images)","6f83f377":"# Transform data into Tensor that has a range from 0 to 1\ntrain_set = MNISTDataset(x_train, y_train, transform=transforms.Compose([transforms.ToTensor()]))\nval_set = MNISTDataset(x_val, y_val, transform=transforms.Compose([transforms.ToTensor()]))\nall_data = MNISTDataset(data, labels, transform=transforms.Compose([transforms.ToTensor()]))\n\n\ntrain_loader = DataLoader(train_set, batch_size=32)\nval_loader = DataLoader(val_set, batch_size=32)\nall_data_loader = DataLoader(all_data, batch_size=32)","9afed5a1":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","b26daf2d":"learning_rate = 0.001\nnum_classes = 10\nnum_epochs = 10\nmodel = torchvision.models.resnet18(pretrained=True)\nnum_ftrs = model.fc.in_features\n# Here the size of each output sample is set to 10.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nmodel.fc = nn.Linear(num_ftrs, num_classes)\n\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","e8e64f52":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n\n# For updating learning rate\ndef update_lr(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\n# Train the model\ntotal_step = len(train_loader)\ncurr_lr = learning_rate\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels.flatten())\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i + 1) % 300 == 0:\n            print(f'Epoch: {epoch + 1}\/{num_epochs}, Loss: {loss.item()}')\n\n# Test the model\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in val_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.flatten()).sum()\n\n    print(f'Test acc: {100 * correct \/ total}')","a1927b37":"learning_rate = 0.001\nnum_classes = 10\nnum_epochs = 10\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n\n# For updating learning rate\ndef update_lr(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n# Train the model\ncurr_lr = learning_rate\niter_num = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(all_data_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels.flatten())\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i + 1) % 300 == 0:\n            print(f'Epoch: {epoch + 1}\/{num_epochs}, Loss: {loss.item()}')","2c9b2825":"test_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv', dtype=np.float32)\ntest_data = test_data.to_numpy() \/ 255.0\ntest_data = test_data.reshape(-1, 28, 28, 1)\n\n\ntest_tensor = torch.from_numpy(test_data).permute(0, 3, 1, 2)\ntest_tensor = test_tensor.repeat(1, 3, 1, 1)  # fine-tuned model has 3 channels, so we can get 3 channels by repeating one channels itself","ed4864ba":"images= test_tensor.to(device)\noutputs = model(images)\n_, predictions = torch.max(outputs, 1)","b44cc4bb":"predictions = predictions.cpu()\nsubmission = pd.DataFrame({'ImageId': np.arange(1, (predictions.size(0) + 1)), 'Label': predictions})\nsubmission.to_csv(\"submission.csv\", index = False)\nprint(\"Done!\")","d790cf13":"## Training and Evaluation on CPU","b885af70":"## Load, reshape, convert the test data","9d04cc2e":"## Training and Evaluation","0094db63":"## Import Libraries","34e0f68f":"## Visualization","c8cbc910":"## PyTorch dataset and DataLoader","9c61d00e":"## Trade off between Num. Iter. and Validation Accuracy, Loss","401f764a":"## Writing custom dataset","9dbfdc40":"# PyTorch Tutorials\n### Content:\n* Digit Recognizer using Logistic Regression\n* Implementation of Convolutional Neural Networks (CNNs)\n* Transfer-Learninig and Fine-Tuning\n\n### You will learn:\n* How to load .csv data\n* How to convert to numpy array\n* Visualization\n* How to use Custom TensorDataset\n* How to write Custom DataLoaders\n* How to write training code in PyTorch\n* How to train on CPU and GPU\n\n! If you find this notebook useful, I would be glad...","fbe8b512":"## Reading the .csv. Preparing labels and convert to numpy","b51c951c":"# Digit Recognizer using Logistic Regression\n\n1. Import Libraries\n2. Reading the .csv\n3. Preparing **labels** and convert to **numpy**\n4. Visualization\n5. PyTorch dataset and DataLoader\n6. Logistic Regression Model\n7. Training and Evaluation on CPU","4867ab52":"## PyTorch dataset and DataLoader\n","070b893d":"## Logistic Regression Model\n","4f86102f":"## Submission\nFor submission, we will use all data for training","6c1c2551":"## Visualization","293305f2":"## Reading the .csv","19af0c67":"## Loading the model from torchvision","435bb001":"## Reading the .csv. Preparing **labels** and convert to **numpy**","3f268e9d":"## Preparing **labels** and convert to **numpy**","7a74bcb9":"# Implementation of Convolutional Neural Networks (CNNs)\n\n1. Import Libraries\n2. Reading the .csv. Preparing **labels** and convert to **numpy**\n4. Visualization\n5. PyTorch dataset and DataLoader\n6. CNN Model\n7. Training and Evaluation on GPU","8cac36d0":"## Import Libraries","0f45a9dd":"# Fine-Tuning\n1. Import Libraries\n2. Reading the .csv. Preparing labels and convert to numpy\n3. Writing custom dataset\n4. Loading the model from torchvision\n5. Training and Evaluation\n6. Submission","cab2e54a":"## Training and Evaluation","5a7afd35":"## CNN Model","42778595":"## Import Libraries"}}