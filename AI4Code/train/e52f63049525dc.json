{"cell_type":{"47ef8f5b":"code","8750ffee":"code","721a883b":"code","20be2431":"code","7465d48d":"code","47230d24":"code","ec6b9042":"code","fa696ff7":"code","3af5dba2":"code","84b7118a":"code","9220e918":"code","5e9d46b6":"code","f886f58a":"code","b8ca381e":"code","2f3e4262":"code","6ef8bc1d":"code","0236089d":"code","96375cbc":"code","742c13fb":"code","d7aa00c5":"code","74153a5e":"code","1a8360e2":"code","466d80ee":"code","1d389489":"markdown","db8817b7":"markdown","6ffa3ff3":"markdown","e3cd155a":"markdown","81b879e9":"markdown","a8c7632e":"markdown","78578abb":"markdown","1e9593c2":"markdown","a675a375":"markdown","a3cec032":"markdown","2626b650":"markdown","57bf0d76":"markdown","57ffcfcf":"markdown","f8dc840f":"markdown","bcfac9a4":"markdown"},"source":{"47ef8f5b":"#import EDA tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#import modeling tools and metrics\nfrom sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture \nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import davies_bouldin_score\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","8750ffee":"#load dataset\ndf=pd.read_csv(\"..\/input\/ccdata\/CC GENERAL.csv\")\ndf","721a883b":"#find missing values\ndf.isna().sum()","20be2431":"df1 = df.drop('CUST_ID', axis = 1) #dropping Customer ID \ndf1.fillna(method ='bfill', inplace = True) #filling the missing values with BFILL method\n\n#df1[\"MINIMUM_PAYMENTS\"].fillna(df1[\"MINIMUM_PAYMENTS\"].mean(), inplace=True)\n#df1[\"CREDIT_LIMIT\"].fillna(df1[\"CREDIT_LIMIT\"].mean(), inplace=True)","7465d48d":"#Detecting outliers\nfor i in df1.select_dtypes(include=['float64','int64']).columns:\n  max_threshold=df1[i].quantile(0.95)\n  min_threshold=df1[i].quantile(0.05)\n  df1_no_outlier=df1[(df1[i] < max_threshold) & (df1[i] > min_threshold)].shape\n  print(\" outlier in \",i,\"is\" ,int(((df1.shape[0]-df1_no_outlier[0])\/df1.shape[0])*100),\"%\")","47230d24":"#remove outliers from columns having nearly 10% outlier\nmax_threshold_BALANCE=df1[\"BALANCE\"].quantile(0.95)\nmin_threshold_BALANCE=df1[\"BALANCE\"].quantile(0.05)\nmax_threshold_CREDIT_LIMIT=df1[\"CREDIT_LIMIT\"].quantile(0.95)\nmin_threshold_CREDIT_LIMIT=df1[\"CREDIT_LIMIT\"].quantile(0.05)\nmax_threshold_PAYMENTS=df1[\"PAYMENTS\"].quantile(0.95)\nmin_threshold_PAYMENTS=df1[\"PAYMENTS\"].quantile(0.05)\ndf1_no_outlier=df1[(df1[\"CREDIT_LIMIT\"] < max_threshold_CREDIT_LIMIT) & (df1[\"CREDIT_LIMIT\"] > min_threshold_CREDIT_LIMIT) & (df1[\"BALANCE\"] < max_threshold_BALANCE) & (df1[\"BALANCE\"] > min_threshold_BALANCE) &  (df1[\"PAYMENTS\"] < max_threshold_PAYMENTS) & (df1[\"PAYMENTS\"] > min_threshold_PAYMENTS)]","ec6b9042":"#Normalizing the Data \nnormalized_df = pd.DataFrame(normalize(df1_no_outlier))\nnormalized_df.head()","fa696ff7":"#Correlation matrix\n\nfig = plt.subplots(figsize=(15,8))\n\nsns.heatmap(normalized_df.corr(),\n            annot=True,\n            fmt=\"0.2f\",\n            cmap=\"inferno\")","3af5dba2":"#appling PCA\npca = PCA() \npcadf = pca.fit_transform(normalized_df) \npcadf = pd.DataFrame(pcadf) ","84b7118a":"plt.subplots(figsize=(12,6))\nplt.plot(pca.explained_variance_ratio_.cumsum())\nplt.xticks(np.arange(0,16,1));","9220e918":"#Setting 2 as n_components\npca = PCA(n_components = 2) \npcadf = pca.fit_transform(normalized_df) \npcadf = pd.DataFrame(pcadf) \npcadf.columns = ['PC1', 'PC2']   \n\npcadf.head()","5e9d46b6":"pca.explained_variance_ratio_","f886f58a":"pca.explained_variance_ratio_.cumsum()","b8ca381e":"#using elbow rule on pcadf (PCA applied)\ninertia = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(pcadf)\n    inertia.append(kmeanModel.inertia_)\nplt.figure(figsize=(15,6))\nplt.plot(K, inertia, 'bx-')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method \\n(PCA applied)')\nplt.show()","2f3e4262":"#Creating evaluate function to evaluate our models performance\ndef evaluate(model, data, silhouette_visualizer=False):\n  \"\"\"\n  Plotting Silhouette Diagram or Silhouette Visualizer, print Silhouette Score and Davies Bouldin Score to evaluate your model performance.\n\n  Parameters:\n  model : pass your estimator with model argument to this function.\n  data : data is your dataframe which is going to be trained.\n  silhouette_visualizer : defualt is False, you can plot silhouette diagram by passing True. \n  \"\"\"\n  if silhouette_visualizer:\n    from yellowbrick.cluster import SilhouetteVisualizer\n\n    visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n    visualizer.fit(data)\n\n  y = model.fit_predict(data)\n\n\n  SC = \"Silhouette Score\"+\" : \"+str(\"{:.2f}\".format(silhouette_score(data, y)))\n  DBC = \"Davies Bouldin Score\"+\" : \"+str(\"{:.2f}\".format(davies_bouldin_score(data, y)))\n\n\n  print(SC)\n  print(DBC)","6ef8bc1d":"#training model and evaluation\nfor i in range(2,11):\n  kmeans = KMeans(n_clusters=i, random_state=42)\n  print(\"Number of Clusters : \" + str(i))\n  evaluate(kmeans, pcadf)\n  print(\"====================\")","0236089d":"#plotting silhouette diagram for n_clusters = 3\nkmeans = KMeans(n_clusters=3, random_state=42)\nevaluate(kmeans, pcadf, silhouette_visualizer=True)","96375cbc":"#training model and evaluation\nfor i in range(2,11):\n  gmm = GaussianMixture(n_components=i, random_state=42)\n  print(\"Number of Components : \" + str(i))\n  evaluate(gmm, pcadf)\n  print(\"====================\")","742c13fb":"#training model and evaluation\nfor i in range(2,11):\n  spc = SpectralClustering(n_clusters=i, affinity='rbf') \n  print(\"Number of Clusters : \" + str(i))\n  evaluate(spc, pcadf)\n  print(\"====================\")","d7aa00c5":"#train model and evaluate the performance\nlinkages = [\"ward\", \"single\", \"average\"]\nfor i in range(2,11):\n  for lk in linkages:\n    hcluster = AgglomerativeClustering(n_clusters=i, affinity='euclidean', linkage=lk)\n    print(\"Number of Clusters : \" + str(i) + \"  linkage : \"+lk)\n    evaluate(hcluster,pcadf)\n    print(\"====================\")","74153a5e":"#create plot model clustering function\ndef plot_model_clustering(model, data):\n  plt.figure(figsize=(15,7))\n  model.fit(data)\n  labels= model.labels_\n  df_label=data.copy()\n  df_label['labels']= labels\n  ax = sns.scatterplot(x='PC1', y='PC2', hue='labels', data=df_label, palette='bright')\n  for index in range(len(str(model))):\n    if str(model)[index] == '(':\n      ind = index\n  ax.set_title(str(model)[:ind])\n\n  evaluate(model, data)","1a8360e2":"plot_model_clustering(kmeans, pcadf)","466d80ee":"hcluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage=\"ward\")\nplot_model_clustering(hcluster, pcadf)","1d389489":"The best case is:\n\nNumber of Clusters = 3 and linkage = ward","db8817b7":"* Conclusion : We should choose 2 as n_components","6ffa3ff3":"### Agglomerative Clustering","e3cd155a":"### KMeans","81b879e9":"# Data Cleaning","a8c7632e":"# Clustering Visualization","78578abb":"# Data Description\n* CUSTID : Identification of Credit Card holder (Categorical)\n* BALANCE : Balance amount left in their account to make purchases\n* BALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n* PURCHASES : Amount of purchases made from account\n* ONEOFFPURCHASES : Maximum purchase amount done in one-go\n* INSTALLMENTSPURCHASES : Amount of purchase done in installment\n* CASHADVANCE : Cash in advance given by the user\n* PURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n* ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n* PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n* CASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n* CASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n* PURCHASESTRX : Numbe of purchase transactions made\n* CREDITLIMIT : Limit of Credit Card for user\n* PAYMENTS : Amount of Payment done by user\n* MINIMUM_PAYMENTS : Minimum amount of payments made by user\n* PRCFULLPAYMENT : Percent of full payment paid by user\n* TENURE : Tenure of credit card service for user\n","1e9593c2":"##### Dimension Reduction (PCA)","a675a375":"### Spectral Clustering","a3cec032":"* Conclusion : We can accept the KMeans with 3 as n_clusters because the size of clusters are almost similar.\n\n  Let's try another estimators and choose the best one!","2626b650":"# Import Libraries","57bf0d76":"# Load Dataset","57ffcfcf":"### Last Note : Both KMeans and Agglomerative with 3 as number of clusters are good to use however, based on Silhouette Score KMeans is the best one!","f8dc840f":"# Train model\n### Choosing k as number of clusters\n\n`Elbow method`\n* This method helps us to choose the right k for number of clusters\n\n`Silhouette Score`\n* This metric can help us to evaluate our performance and find out the best k for number of clusters","bcfac9a4":"### GaussianMixture"}}