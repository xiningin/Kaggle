{"cell_type":{"1e183a9e":"code","62ebc76f":"code","8eed0c92":"code","7d426646":"code","cf07599b":"code","cbfbb545":"code","1fd181fb":"code","c33fd2cd":"code","65fb8e7f":"code","3118572f":"code","df7ab243":"code","8d408768":"code","64cff0bb":"code","7cbc28ce":"code","0537bd6a":"code","801565ba":"code","8d09cf36":"code","72b256f2":"code","8c332fb6":"code","ba15505b":"code","f2567a29":"code","cdf1ae55":"code","90741ec5":"code","6711b16c":"code","55d30916":"code","cc0d0aef":"code","1f9af103":"code","4b0125d7":"code","720a86d6":"code","952ab09b":"code","c0f28b6c":"code","15dcd94b":"code","579879b3":"code","92256dbf":"code","18050f62":"code","e9a0db4c":"code","1dcad5c3":"code","3b1e7f21":"code","c6c80ef2":"code","fa63b91e":"code","9973e822":"code","dbbea0a5":"code","0fbd7d6f":"code","9ebd5c56":"code","c879a263":"code","2828e83e":"code","d310324b":"code","903d85b4":"code","468e2992":"markdown","69de17c4":"markdown","c5ec83b4":"markdown","b0ebea4d":"markdown","e3207f75":"markdown","8642a7d5":"markdown","a1b8753a":"markdown"},"source":{"1e183a9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62ebc76f":"import networkx as nx\nimport community\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.cluster import normalized_mutual_info_score","8eed0c92":"## Main Code\n\ndef utility(node_i,S, G):\n    total_utility = 0\n    k_i = G.degree[node_i]\n    for node_j in S:\n        if node_i != node_j:\n            curr_edges = A[node_i][node_j]\n\n            k_j = G.degree[node_j]\n            m = G.number_of_edges()\n            degree_prod = (k_i*k_j)\/(2.0*m)\n\n            indiv_utility = curr_edges - degree_prod\n            total_utility += indiv_utility\n    return total_utility\n\n\ndef internalCommunityEdges(nodes_list, community_label):\n    internaledges = 0\n    for node in nodes_list:\n        p = len(np.where(S[[n for n in G.neighbors(node)]] == community_label)[0])\n        #print(node)\n        internaledges += p\n    \n    inter = internaledges\/(2.0)\n    return inter\n\n\ndef community_ext(nodes_list):\n    unique_external_nodes = []\n    total_sum = 0\n    community_label = S[nodes_list[0]]\n    total_connections = 0\n    totalExternalNodeDegree = 0\n    for nod in nodes_list:\n        #neighbors of node in that community \n        unique_external_nodes = [neig for neig in G[nod] if S[neig] != S[nod] and neig not in unique_external_nodes]\n        for neig in G[nod]:\n            # external node\n            if S[neig] != S[nod]:\n#                 if neig not in unique_external_nodes:\n#                     unique_external_nodes.append(neig)\n                total_connections += 1\n                \n    communityDegree = len(unique_external_nodes)\n    for externalNode in unique_external_nodes:\n        totalExternalNodeDegree += G.degree(externalNode)\n        \n    \n    totalCommunityEdges = totalEdges - internalCommunityEdges(nodes_list, community_label)\n    modularityComm = total_connections - ((totalExternalNodeDegree*communityDegree)\/(2.0 * totalCommunityEdges))\n    modularityComm = modularityComm\/(2.0 * totalCommunityEdges)\n    return modularityComm\n\n\ndef join(node_i,S, G, lamda, probabilities):\n    max_utility = []\n    community_toJoin = [S[neighbor] for neighbor in G[node_i]]\n    currentCommunity = S[node_i]\n    currentCommunity_nodes_list = np.where(S == currentCommunity)[0]\n    \n    loss = utility(node_i, currentCommunity_nodes_list, G)\n    #print(loss)\n    for neighbor in G[node_i]:\n        community_label = S[neighbor]\n        \n        nodes_list = np.where(S == community_label)[0]\n        gain = utility(node_i, nodes_list, G)\n        \n        community_label = S[neighbor]\n#         print(community_label)\n        #community_toJoin.append(community_label)\n        same_community_nodes_list = np.where(S == community_label)[0]\n        other = community_ext(same_community_nodes_list)\n        val = lamda*(gain-loss) + (1-lamda)*(other)\n        max_utility.append(val)\n        #print(nodes_list)\n    \n    maximum_utility_val = max(max_utility)\n    if(maximum_utility_val) >0:\n        probabilities[node_i] *= (1-lamda)\n    maxUtilityIndex = max_utility.index(maximum_utility_val)\n    communityIndex = community_toJoin[maxUtilityIndex]\n    \n    if S[node_i]==communityIndex:\n        return S\n#     print('Node: ',node_i)\n#     print('Old Community: ', S[node_i])\n    #update the community for node_i\n    S[node_i] = communityIndex\n#     print('New Community: ', S[node_i])\n#     print()\n\n    return S\n\n\nfrom sklearn.metrics.cluster import normalized_mutual_info_score\n\ndef partitionModularity(S,G):\n#     totalModularity = 0\n#     for node_i in range(G.number_of_nodes()):\n#         totalModularity += utility(node_i,np.where(mod_list==mod_list[node_i])[0],G)\n#     return (totalModularity\/(G.number_of_edges()*2.0))\n    mapp = dict()\n    j=0\n    s_dict = dict()\n\n    for i in range(G.number_of_nodes()):\n        if S[i] not in mapp.keys():\n            mapp[S[i]] = j\n            j += 1\n\n    for i in range(G.number_of_nodes()):\n        s_dict[i] = mapp[S[i]]\n        \n    return community.modularity(s_dict, G)\n        \n    \n\nfrom tqdm.auto import tqdm\n\ndef communityDetect(S,G, nIter, LAMBDA, return_dict ):\n    total_nodes = G.number_of_nodes()\n    i = 0\n    prev_S = initial_S\n    val = []\n    random.seed(0)\n#   while(i<nIter):\n    for _ in tqdm(range(nIter)):\n#         print(\"Iteration No. : \", _+1)\n        probab = np.array(probabilities)\n        probab \/= probab.sum()\n        random_node = choice(nodesList, 1, p=probab)\n        prev_S = S \n        S = join(random_node[0], S, G, LAMBDA, probab)\n#         val.append(partitionModularity(S,G))\n            \n    return_dict[LAMBDA] = {'PM':partitionModularity(S,G), 'Val':val, 'S':S}\n        ","7d426646":"import numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\ndef community_layout(g, partition):\n    pos_communities = _position_communities(g, partition, scale=3.)\n    pos_nodes = _position_nodes(g, partition, scale=1.)\n    # combine positions\n    pos = dict()\n    for node in g.nodes():\n        pos[node] = pos_communities[node] + pos_nodes[node]\n\n    return pos\n\ndef _position_communities(g, partition, **kwargs):\n\n    # create a weighted graph, in which each node corresponds to a community,\n    # and each edge weight to the number of edges between communities\n    between_community_edges = _find_between_community_edges(g, partition)\n\n    communities = set(partition.values())\n    hypergraph = nx.DiGraph()\n    hypergraph.add_nodes_from(communities)\n    for (ci, cj), edges in between_community_edges.items():\n        hypergraph.add_edge(ci, cj, weight=len(edges))\n\n    # find layout for communities\n    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n\n    # set node positions to position of community\n    pos = dict()\n    for node, community in partition.items():\n        pos[node] = pos_communities[community]\n\n    return pos\n\ndef _find_between_community_edges(g, partition):\n\n    edges = dict()\n\n    for (ni, nj) in g.edges():\n        ci = partition[ni]\n        cj = partition[nj]\n\n        if ci != cj:\n            try:\n                edges[(ci, cj)] += [(ni, nj)]\n            except KeyError:\n                edges[(ci, cj)] = [(ni, nj)]\n\n    return edges\n\ndef _position_nodes(g, partition, **kwargs):\n    \"\"\"\n    Positions nodes within communities.\n    \"\"\"\n\n    communities = dict()\n    for node, community in partition.items():\n        try:\n            communities[community] += [node]\n        except KeyError:\n            communities[community] = [node]\n\n    pos = dict()\n    for ci, nodes in communities.items():\n        subgraph = g.subgraph(nodes)\n        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n        pos.update(pos_subgraph)\n\n    return pos","cf07599b":"\nfrom random import choice\nimport random\n\nG = nx.karate_club_graph()\ntotalEdges = G.number_of_edges()\ntotalNodes= G.number_of_nodes()\nA = nx.to_numpy_array(G)\nS = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\n\ninitial_S = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\nnodesList = [node_k for node_k in range(G.number_of_nodes())]\n\nfrom numpy.random import choice\nprobabilities = [float(1\/(G.number_of_nodes()*1.0)) for i in range(G.number_of_nodes())]","cbfbb545":"nx.draw(G, with_labels = True)","1fd181fb":"import multiprocessing\nimport time\nmanager = multiprocessing.Manager()\nreturn_dict = manager.dict()\n\nprocess1 = multiprocessing.Process(target= communityDetect, args = (S,G,70,0.1,return_dict))\nprocess1.start()\nprocess1.join()\n\nprint('Partition Modularity (lambda = 0.1) = '+ str(return_dict[0.1]['PM']))","c33fd2cd":"part1 = dict()\n\nfor i in range(len(return_dict[0.1]['S'])):\n    part1[i] = return_dict[0.1]['S'][i]\n\n# pos = community_layout(G, part1)\nnx.draw(G, pos, node_color=list(part1.values()), with_labels = True)\nplt.savefig('Iteration1.png')","65fb8e7f":"part1 = dict()\n\nfor i in range(len(return_dict[0.1]['S'])):\n    part1[i] = return_dict[0.1]['S'][i]\n\n# pos = community_layout(G, part1)\nnx.draw(G, pos, node_color=list(part1.values()), with_labels = True)\nplt.savefig('Iteration2.png')","3118572f":"import networkx as nx\nimport pandas as pd\n\nl1 = []\nl2 = []\nfile = open('..\/input\/amazon\/com-amazon_agm.txt')\n\nfor line in file:\n    l1.append(int(line.split()[0]))\n    l2.append(int(line.split()[1]))\n\ndf = pd.DataFrame()\ndf[1] = l1\ndf[2] = l2\n\nG = nx.Graph()\nG = nx.from_pandas_edgelist(df, 1, 2)\n\nmapping  = {} \ni = 0\nfor node in G.nodes:\n    mapping[node] = i\n    i += 1\n    \n    \nG=nx.relabel_nodes(G,mapping)\n\nprint(nx.info(G))","df7ab243":"\nfrom random import choice\nimport random\n\ntotalEdges = G.number_of_edges()\ntotalNodes= G.number_of_nodes()\nA = nx.to_numpy_array(G)\nS = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\n\ninitial_S = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\nnodesList = [node_k for node_k in range(G.number_of_nodes())]\n\nfrom numpy.random import choice\nprobabilities = [float(1\/(G.number_of_nodes()*1.0)) for i in range(G.number_of_nodes())]","8d408768":"dic= dict()\ncommunityDetect(S,G, 15000, 0.8, dic)\n\nprint(dic[0.8]['PM'])","64cff0bb":"len(np.unique(S))","7cbc28ce":"import networkx as nx\nimport pandas as pd\n\nl1 = []\nl2 = []\nfile = open('..\/input\/enronemail\/email-enron.txt')\n\nfor line in file:\n    l1.append(int(line.split()[0]))\n    l2.append(int(line.split()[1]))\n\ndf = pd.DataFrame()\ndf[1] = l1\ndf[2] = l2\n\nG = nx.Graph()\nG = nx.from_pandas_edgelist(df, 1, 2)\n\nmapping  = {} \ni = 0\nfor node in G.nodes:\n    mapping[node] = i\n    i += 1\n    \n    \nG=nx.relabel_nodes(G,mapping)\n\nprint(nx.info(G))","0537bd6a":"part = community.best_partition(G)\ncommunity.modularity(part, G)","801565ba":"\nfrom random import choice\nimport random\n\ntotalEdges = G.number_of_edges()\ntotalNodes= G.number_of_nodes()\nA = nx.to_numpy_array(G)\nS = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\n\ninitial_S = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\nnodesList = [node_k for node_k in range(G.number_of_nodes())]\n\nfrom numpy.random import choice\nprobabilities = [float(1\/(G.number_of_nodes()*1.0)) for i in range(G.number_of_nodes())]","8d09cf36":"dic= dict()\ncommunityDetect(S,G, 2000, 0.8, dic)\n\nprint(dic[0.8]['PM'])","72b256f2":"len(np.unique(S))","8c332fb6":"%%time\n\nimport multiprocessing\nimport time\nmanager = multiprocessing.Manager()\nreturn_dict = manager.dict()\n\nprocess1 = multiprocessing.Process(target= communityDetect, args = (S,G,2000,0.2,return_dict))\nprocess2 = multiprocessing.Process(target= communityDetect, args = (S,G,2000,0.8,return_dict))\n# process3 = multiprocessing.Process(target= communityDetect, args = (S,G,200,1,return_dict))\n\nprocess1.start()\nprocess2.start()\n# process3.start()\nprocess1.join()\nprocess2.join()\n# process3.join()\n\nprint('Partition Modularity (lambda = 0.2) = '+ str(return_dict[0.2]['PM']))\nprint('Partition Modularity (lambda = 0.8) = '+ str(return_dict[0.8]['PM']))\n# print('Partition Modularity (lambda = 1.0) = '+ str(return_dict[1]['PM']))\n","ba15505b":"import seaborn as sns \n\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.set(style = 'whitegrid')\nplt.title(\"Enron Dataset\")\nsns.lineplot([i for i in range(len(return_dict[0.2]['Val']))] , return_dict[0.2]['Val'],  label=\"lambda 0.2\")\nsns.lineplot([i for i in range(len(return_dict[0.8]['Val']))] , return_dict[0.8]['Val'],  label=\"lambda 0.8\")\n \nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Modularity Value\")\nplt.legend() \nplt.show()\nplt.savefig(\"Enron_1.png\")","f2567a29":"import seaborn as sns \n\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.set(style = 'whitegrid')\nplt.title(\"Bio-DM-LC Dataset\")\nsns.lineplot([i for i in range(len(return_dict[1]['Val']))] , return_dict[1]['Val'],  label=\"lambda 1\")\nsns.lineplot([i for i in range(len(return_dict[0.8]['Val']))] , return_dict[0.8]['Val'],  label=\"lambda 0.8\")\n \nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Modularity Value\")\nplt.legend() \n# plt.show()\n# plt.savefig(\"Bio-DM-LC_2.svg\")","cdf1ae55":"import networkx as nx\nimport pandas as pd\n\nl1 = []\nl2 = []\nfile = open('..\/input\/erdos992\/Erdos992.mtx')\n\nfor line in file:\n    l1.append(int(line.split()[0]))\n    l2.append(int(line.split()[1]))\n\ndf = pd.DataFrame()\ndf[1] = l1\ndf[2] = l2\n\nG = nx.Graph()\nG = nx.from_pandas_edgelist(df, 1, 2)\n\nmapping  = {} \ni = 0\nfor node in G.nodes:\n    mapping[node] = i\n    i += 1\n    \n    \nG=nx.relabel_nodes(G,mapping)\n\nprint(nx.info(G))","90741ec5":"part = community.best_partition(G)\ncommunity.modularity(part, G)","6711b16c":"from random import choice\nimport random\nimport numpy as np\n\ntotalEdges = G.number_of_edges()\ntotalNodes= G.number_of_nodes()\nA = nx.to_numpy_array(G)\nS = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\n\ninitial_S = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\nnodesList = [node_k for node_k in range(G.number_of_nodes())]\n\nfrom numpy.random import choice\nprobabilities = [float(1\/(G.number_of_nodes()*1.0)) for i in range(G.number_of_nodes())]","55d30916":"dic = dict()\n\ncommunityDetect(S,G,5000,0.8,dic)\n\ndic[0.8]['PM']","cc0d0aef":"len(np.unique(S))","1f9af103":"%%time\n\nimport multiprocessing\nimport time\nmanager = multiprocessing.Manager()\nreturn_dict = manager.dict()\n\nprocess1 = multiprocessing.Process(target= communityDetect, args = (S,G,1000,0.2,return_dict))\nprocess2 = multiprocessing.Process(target= communityDetect, args = (S,G,1000,0.8,return_dict))\nprocess3 = multiprocessing.Process(target= communityDetect, args = (S,G,1000,1,return_dict))\n\nprocess1.start()\nprocess2.start()\nprocess3.start()\nprocess1.join()\nprocess2.join()\nprocess3.join()\n\nprint('Partition Modularity (lambda = 0.2) = '+ str(return_dict[0.2]['PM']))\nprint('Partition Modularity (lambda = 0.8) = '+ str(return_dict[0.8]['PM']))\nprint('Partition Modularity (lambda = 1.0) = '+ str(return_dict[1]['PM']))\n","4b0125d7":"import seaborn as sns \n\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.set(style = 'whitegrid')\nplt.title(\"Enron Dataset\")\nsns.lineplot([i for i in range(len(return_dict[0.2]['Val']))] , return_dict[0.2]['Val'],  label=\"lambda 0.2\")\nsns.lineplot([i for i in range(len(return_dict[0.8]['Val']))] , return_dict[0.8]['Val'],  label=\"lambda 0.8\")\n \nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Modularity Value\")\nplt.legend() \n# plt.show()\nplt.savefig(\"Enron_1.svg\")","720a86d6":"import seaborn as sns \n\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.set(style = 'whitegrid')\nplt.title(\"Enron Dataset\")\nsns.lineplot([i for i in range(len(return_dict[1]['Val']))] , return_dict[1]['Val'],  label=\"lambda 1\")\nsns.lineplot([i for i in range(len(return_dict[0.8]['Val']))] , return_dict[0.8]['Val'],  label=\"lambda 0.8\")\n \nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Modularity Value\")\nplt.legend() \n# plt.show()\nplt.savefig(\"Enron_2.svg\")","952ab09b":"import networkx as nx\nimport pandas as pd\n\nl1 = []\nl2 = []\nfile = open('..\/input\/retweet\/rt-retweet.txt')\n\nfor line in file:\n    l1.append(int(line.split()[0]))\n    l2.append(int(line.split()[1]))\n\ndf = pd.DataFrame()\ndf[1] = l1\ndf[2] = l2\n\nG = nx.Graph()\nG = nx.from_pandas_edgelist(df, 1, 2)\n\nmapping  = {} \ni = 0\nfor node in G.nodes:\n    mapping[node] = i\n    i += 1\n    \n    \nG=nx.relabel_nodes(G,mapping)\n\nprint(nx.info(G))","c0f28b6c":"part = community.best_partition(G)\ncommunity.modularity(part, G)","15dcd94b":"from random import choice\nimport random\nimport numpy as np\n\ntotalEdges = G.number_of_edges()\ntotalNodes= G.number_of_nodes()\nA = nx.to_numpy_array(G)\nS = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\n\ninitial_S = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\nnodesList = [node_k for node_k in range(G.number_of_nodes())]\n\nfrom numpy.random import choice\nprobabilities = [float(1\/(G.number_of_nodes()*1.0)) for i in range(G.number_of_nodes())]","579879b3":"return_dict = dict()\n\ncommunityDetect(S,G,600,0.8,return_dict)\n\nreturn_dict[0.8]['PM']","92256dbf":"communityDetect(S,G,600,1,return_dict)\n\nreturn_dict[1]['PM']","18050f62":"%%time\n\nimport multiprocessing\nimport time\n# manager = multiprocessing.Manager()\n# return_dict = manager.dict()\n\n# process1 = multiprocessing.Process(target= communityDetect, args = (S,G,600,0.2,return_dict))\n# process2 = multiprocessing.Process(target= communityDetect, args = (S,G,600,0.8,return_dict))\n# process3 = multiprocessing.Process(target= communityDetect, args = (S,G,600,1,return_dict))\n\n# process1.start()\n# process2.start()\n# process3.start()\n# process1.join()\n# process2.join()\n# process3.join()\n\n# print('Partition Modularity (lambda = 0.2) = '+ str(return_dict[0.2]['PM']))\n# print('Partition Modularity (lambda = 0.8) = '+ str(return_dict[0.8]['PM']))\n# print('Partition Modularity (lambda = 1.0) = '+ str(return_dict[1]['PM']))\n","e9a0db4c":"import seaborn as sns \n\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.set(style = 'whitegrid')\nplt.title(\"Retweet Dataset\")\nsns.lineplot([i for i in range(len(return_dict[0.2]['Val']))] , return_dict[0.2]['Val'],  label=\"lambda 0.2\")\nsns.lineplot([i for i in range(len(return_dict[0.8]['Val']))] , return_dict[0.8]['Val'],  label=\"lambda 0.8\")\n \nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Modularity Value\")\nplt.legend() \n# plt.show()\nplt.savefig(\"retweet1.png\")","1dcad5c3":"import seaborn as sns \n\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.set(style = 'whitegrid')\nplt.title(\"Retweet Dataset\")\nsns.lineplot([i for i in range(len(return_dict[1]['Val']))] , return_dict[1]['Val'],  label=\"lambda 1\")\nsns.lineplot([i for i in range(len(return_dict[0.8]['Val']))] , return_dict[0.8]['Val'],  label=\"lambda 0.8\")\n \nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Modularity Value\")\nplt.legend() \n# plt.show()\nplt.savefig(\"retweet2.png\")","3b1e7f21":"import networkx as nx\nimport pandas as pd\n\nl1 = []\nl2 = []\nfile = open('..\/input\/fbpagesfood\/fb-pages-food.txt')\n\nfor line in file:\n    l1.append(int(line.split(',')[0]))\n    l2.append(int(line.split(',')[1]))\n\ndf = pd.DataFrame()\ndf[1] = l1\ndf[2] = l2\n\nG = nx.Graph()\nG = nx.from_pandas_edgelist(df, 1, 2)\n\nmapping  = {} \ni = 0\nfor node in G.nodes:\n    mapping[node] = i\n    i += 1\n    \n    \nG=nx.relabel_nodes(G,mapping)\n\nprint(nx.info(G))","c6c80ef2":"part = community.best_partition(G)\ncommunity.modularity(part, G)","fa63b91e":"from random import choice\nimport random\nimport numpy as np\n\ntotalEdges = G.number_of_edges()\ntotalNodes= G.number_of_nodes()\nA = nx.to_numpy_array(G)\nS = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\n\ninitial_S = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\nnodesList = [node_k for node_k in range(G.number_of_nodes())]\n\nfrom numpy.random import choice\nprobabilities = [float(1\/(G.number_of_nodes()*1.0)) for i in range(G.number_of_nodes())]\n","9973e822":"dic = dict()\ncommunityDetect(S,G,100,0.8,dic)\nprint(dic[0.8]['PM'])\nlen(np.unique(S))","dbbea0a5":"%%time\n\nimport multiprocessing\nimport time\nmanager = multiprocessing.Manager()\nreturn_dict = manager.dict()\n\nprocess1 = multiprocessing.Process(target= communityDetect, args = (S,G,5000,0.2,return_dict))\nprocess2 = multiprocessing.Process(target= communityDetect, args = (S,G,5000,0.8,return_dict))\n# process3 = multiprocessing.Process(target= communityDetect, args = (S,G,5000,1,return_dict))\n\nprocess1.start()\nprocess2.start()\n# process3.start()\nprocess1.join()\nprocess2.join()\n# process3.join()\n\nprint('Partition Modularity (lambda = 0.2) = '+ str(return_dict[0.2]['PM']))\nprint('Partition Modularity (lambda = 0.8) = '+ str(return_dict[0.8]['PM']))\n# print('Partition Modularity (lambda = 1.0) = '+ str(return_dict[1]['PM']))\n","0fbd7d6f":"import seaborn as sns \n\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.set(style = 'whitegrid')\nplt.title(\"Fb-food Dataset\")\nsns.lineplot([i for i in range(len(return_dict[0.2]['Val']))] , return_dict[0.2]['Val'],  label=\"lambda 0.2\")\nsns.lineplot([i for i in range(len(return_dict[0.8]['Val']))] , return_dict[0.8]['Val'],  label=\"lambda 0.8\")\n \nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Modularity Value\")\nplt.legend() \n# plt.show()\nplt.savefig(\"Fb-food_1.svg\")","9ebd5c56":"import seaborn as sns \n\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.set(style = 'whitegrid')\nplt.title(\"Fb-food Dataset\")\nsns.lineplot([i for i in range(len(return_dict[1]['Val']))] , return_dict[1]['Val'],  label=\"lambda 1\")\nsns.lineplot([i for i in range(len(return_dict[0.8]['Val']))] , return_dict[0.8]['Val'],  label=\"lambda 0.8\")\n \nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Modularity Value\")\nplt.legend() \n# plt.show()\nplt.savefig(\"Fb-food_2.svg\")","c879a263":"import networkx as nx\nimport pandas as pd\n\nl1 = []\nl2 = []\nfile = open('..\/input\/fbpagespolitician\/fb-pages-politician.edges')\nnode_set = set()\n\nfor line in file:\n    node_set.add(int(line.split(',')[0]))\n    node_set.add(int(line.split(',')[1]))\n    if len(node_set)>0.5*G.number_of_nodes():\n        break\n        \nfile = open('..\/input\/fbpagespolitician\/fb-pages-politician.edges')\n\nfor line in file:\n    if int(line.split(',')[0]) in node_set and int(line.split(',')[0]) in node_set:\n        l1.append(int(line.split(',')[0]))\n        l2.append(int(line.split(',')[1]))\n    \n    \ndf = pd.DataFrame()\ndf[1] = l1\ndf[2] = l2\n\nG = nx.Graph()\nG = nx.from_pandas_edgelist(df, 1, 2)\n\nmapping  = {} \ni = 0\nfor node in G.nodes:\n    mapping[node] = i\n    i += 1\n    \n    \nG=nx.relabel_nodes(G,mapping)\n\nprint(nx.info(G))","2828e83e":"part = community.best_partition(G)\ncommunity.modularity(part, G)","d310324b":"from random import choice\nimport random\nimport numpy as np\n\ntotalEdges = G.number_of_edges()\ntotalNodes= G.number_of_nodes()\nA = nx.to_numpy_array(G)\nS = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\n\ninitial_S = np.array([(\"C\" + str(i)) for i in range(G.number_of_nodes())])\nnodesList = [node_k for node_k in range(G.number_of_nodes())]\n\nfrom numpy.random import choice\nprobabilities = [float(1\/(G.number_of_nodes()*1.0)) for i in range(G.number_of_nodes())]\n","903d85b4":"dic = dict()\ncommunityDetect(S,G,3000,0.8,dic)\nprint(dic[0.8]['PM'])\nlen(np.unique(S))","468e2992":"# Amazon","69de17c4":"# Politician\n","c5ec83b4":"# Karate Dataset","b0ebea4d":"# Fb Food Dataset","e3207f75":"# Retweet dataset","8642a7d5":"# Enron Email Dataset","a1b8753a":"# Erdos Dataset"}}