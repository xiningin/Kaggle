{"cell_type":{"1eb56b43":"code","bd8da360":"code","b17c179c":"code","d79dde82":"code","b55fc037":"code","1af202b3":"code","f0a92b4f":"code","1592d7d9":"code","5c63c24b":"code","b1b31e52":"code","ea56ad21":"code","00c60e91":"code","aa413a66":"code","0ce9a9d8":"code","5274d7ac":"code","86e6e577":"code","b6f57a5d":"code","b5ea580b":"code","887b4813":"code","9bb4e403":"code","1a0bbfeb":"code","c6e96997":"code","b6ae04a8":"code","d0f1b9cd":"code","7e6954c9":"code","c2526716":"code","4e7ffa99":"code","12fc4fdb":"code","39d12a13":"code","11a2f8a9":"code","5f32739e":"code","3665bb26":"code","b1828db8":"code","8fbd4473":"code","2643b8d7":"code","db4fcb57":"code","d6475355":"markdown","15b127df":"markdown","41c38ab9":"markdown","e0621d0a":"markdown","a4d62850":"markdown","868ef02b":"markdown","ef99ef3e":"markdown","5740feb2":"markdown","6fa131bc":"markdown","605fe9e9":"markdown","5f421d27":"markdown","9da99813":"markdown","2377b0ea":"markdown","da4536c0":"markdown","8729bf53":"markdown","e89b39e0":"markdown","275cf4a0":"markdown","f2eeff4e":"markdown","b29a761f":"markdown","819db971":"markdown","508891f7":"markdown","f5ba13ee":"markdown","3fff96a7":"markdown","4ec7a869":"markdown","0772aaaf":"markdown","bb25ed07":"markdown","65a9609d":"markdown"},"source":{"1eb56b43":"# Importing all Required Libraries\nimport nltk, string, re, os, pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)","bd8da360":"from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n\ndf = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\ndf.columns = ['text', 'target']\n\ntargets = pd.DataFrame( newsgroups_train.target_names)\ntargets.columns=['title']\n\ndataframe = pd.merge(df, targets, left_on='target', right_index=True)\ndataframe.head()","b17c179c":"for text, topic in zip(dataframe['text'][125:130], dataframe['title'][125:130]):\n    print(\"#\"*125)\n    print(\"Topic: \"+ topic) \n    print(\"Text: \"+ text)","d79dde82":"x = dataframe['title'].value_counts().index.values.astype('str')\ny = dataframe['title'].value_counts().values\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(dataframe))]\n\ntrace1 = go.Bar(x=x, y=y, text=pct,\n                marker=dict(\n                color = y,colorscale='Portland',showscale=True,\n                reversescale = False\n                ))\nlayout = dict(title= 'Topic Level split in the dataset',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Titles'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)","b55fc037":"stop = set(stopwords.words('english'))\ndef tokenize(text):\n    \"\"\"\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    \"\"\"\n    try: \n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        text = regex.sub(\" \", text) # remove punctuation\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens\n            \n    except TypeError as e: print(text,e)","1af202b3":"# create a dictionary of words for each category\ncat_desc = dict()\nfor cat in newsgroups_train.target_names: \n    text = \" \".join(dataframe.loc[dataframe['title']==cat, 'text'].values)\n    cat_desc[cat] = tokenize(text)\n\n# flat list of all words combined\nflat_lst = [item for sublist in list(cat_desc.values()) for item in sublist]\nallWordsCount = Counter(flat_lst)\nall_top10 = allWordsCount.most_common(20)\nx = [w[0] for w in all_top10]\ny = [w[1] for w in all_top10]","f0a92b4f":"y = dataframe['title'].value_counts().values\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(dataframe))]\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Overall Word Frequency in the dataset',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Word'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)","1592d7d9":"%time\n# apply the tokenizer into the \"text\" column\ndataframe['tokens'] = dataframe['text'].map(tokenize)\ndataframe['tokens'] = dataframe['text'].map(tokenize)\n\ndataframe.reset_index(drop=True, inplace=True)\n\nprint(\"Let's eyeball how the sentences have been tokenized:\")\n\nfor description, tokens in zip(dataframe['text'].head(), dataframe['tokens'].head()):\n    print('description:', description)\n    print('tokens:', tokens)\n    print()","5c63c24b":"# build dictionary with key=title and values as all the descriptions related.\ncat_desc = dict()\nfor cat in newsgroups_train.target_names: \n    text = \" \".join(dataframe.loc[dataframe['title']==cat, 'text'].values)\n    cat_desc[cat] = tokenize(text)\n\n\n# find the most common words for the top 4 categories\nautos100 = Counter(cat_desc['rec.autos']).most_common(100)\nspace100 = Counter(cat_desc['sci.space']).most_common(100)\nchristian100 = Counter(cat_desc['soc.religion.christian']).most_common(100)\nmideast100 = Counter(cat_desc['talk.politics.mideast']).most_common(100)","b1b31e52":"def generate_wordcloud(tup):\n    wordcloud = WordCloud(background_color='white',\n                          max_words=50, max_font_size=40,\n                          random_state=42\n                         ).generate(str(tup))\n    return wordcloud","ea56ad21":"fig,axes = plt.subplots(2, 2, figsize=(30, 15))\n\nax = axes[0, 0]\nax.imshow(generate_wordcloud(autos100), interpolation=\"bilinear\")\nax.axis('off')\nax.set_title(\"Title: Automobile; Top: 100\", fontsize=30)\n\nax = axes[0, 1]\nax.imshow(generate_wordcloud(space100))\nax.axis('off')\nax.set_title(\"Title: Space; Top: 100\", fontsize=30)\n\nax = axes[1, 0]\nax.imshow(generate_wordcloud(christian100))\nax.axis('off')\nax.set_title(\"Title: Christian; Top: 100\", fontsize=30)\n\nax = axes[1, 1]\nax.imshow(generate_wordcloud(mideast100))\nax.axis('off')\nax.set_title(\"Title: Mid-East; Top: 100\", fontsize=30)","00c60e91":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10, max_features=180000, tokenizer=tokenize, ngram_range=(1, 2))","aa413a66":"all_desc = dataframe['text'].values\nvz = vectorizer.fit_transform(list(all_desc))","0ce9a9d8":"#  create a dictionary mapping the tokens to their tfidf values\ntfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\ntfidf = pd.DataFrame(columns=['idf']).from_dict( dict(tfidf), orient='index')\ntfidf.columns = ['idf']","5274d7ac":"tfidf.sort_values(by=['idf'], ascending=True).head(10)","86e6e577":"tfidf.sort_values(by=['idf'], ascending=False).head(10)","b6f57a5d":"sample_sz = 10000\n\ndataframe_sample = dataframe.sample(n=sample_sz)\nvz_sample = vectorizer.fit_transform(list(dataframe_sample['text']))","b5ea580b":"from sklearn.decomposition import TruncatedSVD\n\nn_comp= 30\nsvd = TruncatedSVD(n_components=n_comp, random_state=42)\nsvd_tfidf = svd.fit_transform(vz_sample)","887b4813":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)","9bb4e403":"tsne_tfidf = tsne_model.fit_transform(svd_tfidf)","1a0bbfeb":"output_notebook()\nplot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"tf-idf clustering of 'text'\", tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\", x_axis_type=None, y_axis_type=None, min_border=1)","c6e96997":"dataframe_sample.reset_index(inplace=True, drop=True)\n\ntfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\ntfidf_df['text'] = dataframe_sample['text']\ntfidf_df['tokens'] = dataframe_sample['tokens']\ntfidf_df['title'] = dataframe_sample['title']","b6ae04a8":"plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"text\": \"@text\", \"tokens\": \"@tokens\", \"title\":\"@title\"}\nshow(plot_tfidf)","d0f1b9cd":"from IPython.display import Video\nVideo(\"..\/input\/resources-latest\/Topic_model_scheme.webm\")\n# Video(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/70\/Topic_model_scheme.webm\")","7e6954c9":"from sklearn.cluster import MiniBatchKMeans\n\nnum_clusters = 20 # need to be selected wisely\nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=0, max_iter=1000)\n\nkmeans = kmeans_model.fit(vz)\nkmeans_clusters = kmeans.predict(vz)\nkmeans_distances = kmeans.transform(vz)\n\nsorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    print(\"Cluster %d:\" % i)\n    aux = ''\n    for j in sorted_centroids[i, :10]:\n        try:\n            aux += terms[j] + ' | '\n        except:\n            pass\n    print(aux)\n    print() ","c2526716":"# repeat the same steps for the sample\nkmeans = kmeans_model.fit(vz_sample)\nkmeans_clusters = kmeans.predict(vz_sample)\nkmeans_distances = kmeans.transform(vz_sample)\n# reduce dimension to 2 using tsne\ntsne_kmeans = tsne_model.fit_transform(kmeans_distances)","4e7ffa99":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])\n\n#combined_sample.reset_index(drop=True, inplace=True)\nkmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\nkmeans_df['cluster'] = kmeans_clusters\nkmeans_df['text'] = dataframe_sample['text']\nkmeans_df['title'] = dataframe_sample['title']\n\nplot_kmeans = bp.figure(plot_width=700, plot_height=600, title=\"KMeans clustering of 'text'\", tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\", x_axis_type=None, y_axis_type=None, min_border=1)\nsource = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'], color=colormap[kmeans_clusters], text=kmeans_df['text'], title=kmeans_df['title'], cluster=kmeans_df['cluster']))\n\nplot_kmeans.scatter(x='x', y='y', color='color', source=source)\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover.tooltips={\"text\": \"@text\", \"title\": \"@title\", \"cluster\":\"@cluster\" }\nshow(plot_kmeans)","12fc4fdb":"?LatentDirichletAllocation\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.LatentDirichletAllocation.html","39d12a13":"cvectorizer = CountVectorizer(min_df=4, max_features=180000, tokenizer=tokenize, ngram_range=(1,2))\ncvz = cvectorizer.fit_transform(dataframe_sample['text'])\nlda_model = LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20, random_state=42)\nX_topics = lda_model.fit_transform(cvz)\n\nn_top_words = 20\ntopic_summaries = []\n\ntopic_word = lda_model.components_  # get the topic words\nvocab = cvectorizer.get_feature_names()","11a2f8a9":"for i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))","5f32739e":"# reduce dimension to 2 using tsne\ntsne_lda = tsne_model.fit_transform(X_topics)","3665bb26":"unnormalized = np.matrix(X_topics)\ndoc_topic = unnormalized\/unnormalized.sum(axis=1)\n\nlda_keys = []\nfor i, tweet in enumerate(dataframe_sample['text']):\n    lda_keys += [doc_topic[i].argmax()]\n\nlda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\nlda_df['description'] = dataframe_sample['text']\nlda_df['category'] = dataframe_sample['title']\nlda_df['topic'] = lda_keys\nlda_df['topic'] = lda_df['topic'].map(int)\n\nplot_lda = bp.figure(plot_width=700,\n                     plot_height=600,\n                     title=\"LDA topic visualization\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","b1828db8":"source = ColumnDataSource(data=dict(x=lda_df['x'], y=lda_df['y'],\n                                    color=colormap[lda_keys],\n                                    description=lda_df['description'],\n                                    topic=lda_df['topic'],\n                                    category=lda_df['category']))\n\nplot_lda.scatter(source=source, x='x', y='y', color='color')\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover = plot_lda.select(dict(type=HoverTool))\nhover.tooltips={\"description\":\"@description\",\n                \"topic\":\"@topic\", \"category\":\"@category\"}\nshow(plot_lda)","8fbd4473":"def prepareLDAData():\n    data = {\n        'vocab': vocab,\n        'doc_topic_dists': doc_topic,\n        'doc_lengths': list(lda_df['len_docs']),\n        'term_frequency':cvectorizer.vocabulary_,\n        'topic_term_dists': lda_model.components_\n    } \n    return data","2643b8d7":"import pyLDAvis\n\nlda_df['len_docs'] = dataframe_sample['tokens'].map(len)\nldadata = prepareLDAData()\npyLDAvis.enable_notebook()\nprepared_data = pyLDAvis.prepare(**ldadata)\npyLDAvis.display(prepared_data)","db4fcb57":"from IPython.display import HTML\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\nif (code_show){\n$('div.input').hide();\n} else {\n$('div.input').show();\n}\ncode_show = !code_show\n} \n$( document ).ready(code_toggle);\n<\/script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on\/off the raw code.\"><\/form>''')","d6475355":"#### 2.2. K-means Clustering\n\nK-means clustering obejctive is to minimize the average squared Euclidean distance of the document \/ text from their cluster centroids. ","15b127df":"##### Some sample texts from the dataset:","41c38ab9":"#### LDA Results\n\nLDA considers each documents consists of multiple topics.\n\n<img src=\"..\/input\/resources-latest\/LDA_Results.png\">","e0621d0a":"vz is a tfidf matrix where:\n* the number of rows is the total number of descriptions\n* the number of columns is the total number of unique tokens across the descriptions","a4d62850":"### To summarize:\n\nGeneral Rule of Thumb while doing Topic Modeling using LDA.\n\n<img src=\"..\/input\/resources-latest\/LDA_Steps.png\">","868ef02b":"#### 2.3 **Latent Dirichlet Allocation**\n\nLatent Dirichlet Allocation (LDA) is an algorithms used to discover the topics that are present in a corpus.\n\n>  LDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents.\n> \n> Reference: https:\/\/medium.com\/intuitionmachine\/the-two-paths-from-natural-language-processing-to-artificial-intelligence-d5384ddbfc18\n\nIts input is a **bag of words**, i.e. each document represented as a row, with each columns containing the count of words in the corpus. We are going to use a powerful tool called pyLDAvis that gives us an interactive visualization for LDA. \n\n<img src=\"..\/input\/resources-latest\/LDA.PNG\">","ef99ef3e":"In order to plot these clusters, first we will need to reduce the dimension of the distances to 2 using tsne: ","5740feb2":"## **References**\n\n1. [Topic Modeling Wikipedia Page](https:\/\/en.wikipedia.org\/wiki\/Topic_model)\n2. [Topic Modeling in Python: Latent Dirichlet Allocation (LDA); Author- Shashank Kapadia](https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)\n3. [Beginners Guide to Topic Modeling in Python; Author - Shivam Bhansal](https:\/\/www.analyticsvidhya.com\/blog\/2016\/08\/beginners-guide-to-topic-modeling-in-python\/)\n4. [Topic Modeling with Gensim (Python); Author - Selva Prabhakaran](https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/)\n5. [Python for NLP: Topic Modeling; Author - Usman Malik](https:\/\/stackabuse.com\/python-for-nlp-topic-modeling\/)\n6. [Mercari Interactive EDA + Topic Modelling; Author -ThyKhuely](https:\/\/www.kaggle.com\/thykhuely\/mercari-interactive-eda-topic-modelling?scriptVersionId=1923301)\n7. [Topic Modeling for The New York Times News Dataset; Author - Moorissa Tjokro](https:\/\/towardsdatascience.com\/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac)\n8. [LDA Topic Models (YouTube Video); Author - Andrius Knispelis](https:\/\/www.youtube.com\/watch?v=3mHy4OSyRf0)","6fa131bc":"#### How to interpret the results\n\n<img src=\"..\/input\/resources-latest\/LDA_Space.png\">","605fe9e9":"It's now possible to visualize our data points. Note that the deviation as well as the size of the clusters imply little information  in t-SNE.","5f421d27":"Inference:","9da99813":"# **Introduction**\n\nThis is the course material for 'Topic Modeling'. The notebook is prepared in Python 3.7+.\n\nAuthor: Avinash OK ( okavinashok@gmail.com )\n\nIn this notebook we try to give you answers to the following questions:\n\n* What is Topic Modeling?\n* What are the different steps involved in the process?\n* What all approaches can be used for Topic Modeling?\n* When should you use Topic Modeling?\n* How can this Notebook help you to build a Topic Model from scratch?","2377b0ea":"### Definition","da4536c0":"We could aso use the package `WordCloud` to easily visualize which words has the highest frequencies within each title:","8729bf53":"##### 2.1.a Tokenizing\n\nMost of the time, the first steps of an NLP project is to **\"tokenize\"** your documents, whose main purpose is to normalize our texts. The three fundamental stages will usually include: \n* break the descriptions into sentences and then break the sentences into tokens\n* remove punctuation and stop words\n* lowercase the tokens\n* herein, We can also only consider words that have length equal to or greater than 3 characters","e89b39e0":"### Some general Use-cases:\n\n1. Text Categorization problem where the labels given from the business are not very reliable.\n2. Recommender Engines for suggesting similar articles or books to potential customers.\n3. Text description for a fixed number of Products are given and asked to be clustered without label information.","275cf4a0":"#### 1. Explanatory Data Analysis \n\nHere, we choose a very popular NLP dataset [`20 News Groups`](http:\/\/qwone.com\/~jason\/20Newsgroups\/). It can be downloaded from this [link](https:\/\/www.kaggle.com\/crawford\/20-newsgroups\/download) or imported directly from Scikit learn, a popular Data Science library in Python.\n\n#####  What is 20 News Groups?\n- The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It was originally collected by Ken Lang,  for his Newsweeder. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n\n##### **Here is a list of the 20 newsgroups, partitioned according to subject matter:**\n<img src=\"..\/input\/resources-latest\/20NewsGroup.PNG\">","f2eeff4e":"#### 2. Text pre-processing\nIn this stage, we perform a basic preprocessing over our focussed column `text`.\n\nIt will be slightly challenging to parse through this column since it's unstructured data. As a part of text preprocessing, we will strip out all punctuations, remove some english stop words (i.e. redundant words such as \"a\", \"the\", etc.) and any other words with a length less than 3.\n\n<img src=\"..\/input\/resources-latest\/TopicModelingPreprocessing.png\">","b29a761f":"### How to choose hyperparameters?\n\nThere are primarily three Hyperparameters which directly influence the output.\n\n###### *doc_topic_prior : float, optional (default=None)*\nPrior of document topic distribution theta. If the value is None, defaults to 1 \/ n_components. This is called `alpha`.\n\n###### *topic_word_prior : float, optional (default=None)*\nPrior of topic word distribution `beta`. If the value is None, defaults to 1 \/ n_components.\n\n<img src=\"..\/input\/resources-latest\/LDA_Hyperparameters.PNG\">","819db971":"#### Topic level split in the dataset","508891f7":"A topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents.\n\n\nContents covered in this notebook:\n1. Explanatory Data Analysis \n2. Text Processing  \n    2.1. Tokenizing and  tf-idf algorithm  \n    2.2. K-means Clustering  \n    2.3. Latent Dirichlet Allocation (LDA)  \/ Topic Modelling\n","f5ba13ee":"Below is the 10 tokens with the **lowest IDF score**, which is unsurprisingly, very generic words that we could not use to distinguish one description from another.","3fff96a7":"Given the high dimension of our tfidf matrix, we need to reduce their dimension using the Singular Value Decomposition (SVD) technique. And to visualize our vocabulary, we could next use t-SNE to reduce the dimension from 50 to 2. t-SNE is more suitable for dimensionality reduction to 2 or 3. \n\n#### **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n\nt-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. It is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. But since t-SNE complexity is significantly high, usually we'd use other high-dimension reduction techniques before applying t-SNE.\n\nFirst, let's take a sample from the our focused column `text` since t-SNE can take a very long time to execute. We can then reduce the dimension of each vector from to n_components (30) using SVD.","4ec7a869":"Below is the 10 tokens with the **highest IDF score**, which includes words that are a lot specific that by looking at them, we could guess the categories that they belong to: ","0772aaaf":"These are some of the approaches to do Topic Modeling:\n- **Latent Dirichlet Allocation**\n- Hierarchical Dirichlet process (HDP)\n- Gibbs Sampling Dirichlet Mixture Model (GSDMM)\n- Nonnegative Matrix Factorization (NMF)\n- Latent Semantic Analysis (LSA\/LSI)\n- Probabilistic Latent Semantic Analysis (pLSA)\n- Explicit semantic analysis (ESA)","bb25ed07":"##### 2.1.b tf-idf algorithm\n\ntf-idf is the acronym for **Term Frequency\u2013inverse Document Frequency**. It quantifies the importance of a particular word in relative to the vocabulary of a collection of documents or corpus. The metric depends on two factors: \n- **Term Frequency**: the occurences of a word in a given document (i.e. bag of words)\n- **Inverse Document Frequency**: the reciprocal number of times a word occurs in a corpus of documents\n\nThink about of it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide us much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as \"a\", \"the\", \"and\", etc. tf-idf can therefore, be seen as a weighting scheme for words relevancy in a specific document. For more info refer this [link](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf)","65a9609d":"#### Our Game-Plan\n\nNow that we have an intial understanding of the dataset, let's quickly move on to the meat of the problem. For now, let's assume that we don't have the column `title` and try to see if we can cluster various documents in the column `text` based on similar words present. This is our \"focused column\" through out this process."}}