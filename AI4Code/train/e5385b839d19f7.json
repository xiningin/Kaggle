{"cell_type":{"603f97fa":"code","d5cfe1b8":"code","52224f71":"code","f8d1bf98":"code","01d5ae31":"code","17bd82a4":"code","9fde2f8b":"code","e126e20d":"code","1f0a526c":"code","e44eaac4":"code","e3ab318f":"code","358cb378":"code","7819606c":"code","c18768d1":"code","dcc38a14":"code","386f3669":"code","4ad5c73b":"code","ca390763":"code","8947f423":"code","7560182a":"code","d4d83205":"code","c658b689":"code","d7449f1d":"code","3fdb26eb":"code","f4795d22":"code","75789b70":"code","73ebd03f":"markdown","e4b90631":"markdown","5d0a8442":"markdown","bdf68dca":"markdown","3aa040d7":"markdown","57d37fc8":"markdown","d84ab074":"markdown","84212d5c":"markdown","ea811e3f":"markdown","c21395e3":"markdown","976a481f":"markdown","0865eabf":"markdown","994443ae":"markdown","3001cc7f":"markdown","ff0d7bb9":"markdown","d85a610d":"markdown","5b20bf21":"markdown","ea2fe0e2":"markdown","5e97819d":"markdown","5bfcde36":"markdown","b61a82a3":"markdown","43edabc0":"markdown","d797e829":"markdown","a0ac3325":"markdown","07f1910a":"markdown","7fdbabf5":"markdown","3c090474":"markdown","d5cd4c33":"markdown","21593d28":"markdown","2f993734":"markdown","99c31d2e":"markdown"},"source":{"603f97fa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.offline as py\npio.templates.default = \"plotly_dark\"\nfrom plotly.subplots import make_subplots\nimport re\nfrom collections import defaultdict,OrderedDict\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom plotly import tools\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport string\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom nltk import word_tokenize,pos_tag\nfrom nltk import RegexpParser\nimport json\nimport cufflinks\nfrom plotly.offline import iplot\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nnlp = spacy.load('en_core_web_sm')\n!mkdir -p data\n\nPATH='..\/input\/tweet-sentiment-extraction\/'\n\ntrain=pd.read_csv(PATH+'train.csv')\ntest=pd.read_csv(PATH+'test.csv')\nsubmission=pd.read_csv(PATH+'sample_submission.csv')\n\nMODEL_PATH = '\/kaggle\/input\/distilbertbaseuncased\/'","d5cfe1b8":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n\n    return summary","52224f71":"resumetable(train)","f8d1bf98":"resumetable(test)","01d5ae31":"train=train.dropna()","17bd82a4":"## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" and token not in STOP_WORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from postive tweets ##\nfreq_dict = defaultdict(int)\nfor sent in train[train['sentiment']=='positive'][\"selected_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), '#C5197D')\n\n## Get the bar chart from negative tweets ##\nfreq_dict = defaultdict(int)\nfor sent in train[train['sentiment']=='negative'][\"selected_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), '#C5197D')\n\n## Get the bar chart from neutral questions ##\nfreq_dict = defaultdict(int)\nfor sent in train[train['sentiment']=='neutral'][\"selected_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), '#C5197D')\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=3, vertical_spacing=0.04,\n                          subplot_titles=[\"Positive\", \n                                          \"Negative\",\n                                         \"Neutral\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 1, 3)\npy.iplot(fig, filename='word-plots')\n\n","9fde2f8b":"## Get the bar chart from postive tweets ##\nfreq_dict = defaultdict(int)\nfor sent in train[train['sentiment']=='positive'][\"selected_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'rgb(51,255,255)')\n\n## Get the bar chart from negative tweets ##\nfreq_dict = defaultdict(int)\nfor sent in train[train['sentiment']=='negative'][\"selected_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'rgb(51,255,255)')\n\n## Get the bar chart from neutral questions ##\nfreq_dict = defaultdict(int)\nfor sent in train[train['sentiment']=='neutral'][\"selected_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'rgb(51,255,255)')\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=3, vertical_spacing=0.04,\n                          subplot_titles=[\"Positive-bigram\", \n                                          \"Negative-bigram\",\n                                         \"Neutral-bigram\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 1, 3)\n\npy.iplot(fig, filename='word-plots')\n","e126e20d":"def clean(reg_exp, text):\n    text = re.sub(reg_exp, \" \", text)\n\n    # replace multiple spaces with one.\n    text = re.sub('\\s{2,}', ' ', text)\n\n    return text\n\n\ndef remove_urls(text):\n    text = clean(r\"http\\S+\", text)\n    text = clean(r\"www\\S+\", text)\n    text = clean(r\"pic.twitter.com\\S+\", text)\n\n    return text\n\ndef basic_clean(text):\n    text=remove_urls(text)\n    text = clean(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', text) #replace double punctuation with single\n    text = clean(r\"[^A-Za-z0-9\\.\\'!\\?,\\$]\", text) #removes unicode characters\n    return text\n","1f0a526c":"train['text']=train['text'].apply(lambda x: basic_clean(x))","e44eaac4":"def plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOP_WORDS)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(text)\n    \n    plt.figure(figsize=figure_size)\n    \n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nd = '..\/input\/masks\/masks-wordclouds\/'","e3ab318f":"comments_text = str(train.text)\ncomments_mask = np.array(Image.open(d + 'upvote.png'))\nplot_wordcloud(comments_text, comments_mask, max_words=2000, max_font_size=300, \n               title = 'Most common words in all of the tweets', title_size=30)","358cb378":"train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOP_WORDS]))\ntest[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOP_WORDS]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of upper case words in the text ##\ntrain[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","7819606c":"#Reference:https:\/\/www.kaggle.com\/parulpandey\/basic-preprocessing-and-eda\n\ngrid = gridspec.GridSpec(3, 4)\nplt.figure(figsize=(16,6*4))\n\nplt.suptitle('Meta features', size=20)\ncount=0\ntop_cats=train['sentiment'].value_counts().index\nfor n, col in enumerate(top_cats):\n    colr=['green','black','pink']\n    for i, q_t in enumerate(['num_words', 'num_unique_words', 'num_chars','num_stopwords']):\n        filter_df=train[train['sentiment']==col]\n       \n        filter_df[q_t].iplot(\n            kind='hist',\n            bins=100,\n            xTitle='text length',\n            linecolor='black',\n            color=colr[n],\n            yTitle='count',\n            title=f'{col} {q_t} Distribution')","c18768d1":"\ndef extract(x):\n    if len((x.split(' ')))<=6:\n        return x\n    else:\n        result=[]\n        pattern = r\"\"\"S1: {<PR.*>+<VB.*>+<VB>},\n              S2: {<JJ>?<NN.*>?<PR.*>+<VB.*>},\n              S3: {<JJ>?<NN.*>}\"\"\"\n\n        sentence = word_tokenize(x)\n        PChunker = RegexpParser(pattern)\n        output= PChunker.parse(pos_tag(sentence))\n        \n        for subtree in (output.subtrees(filter=lambda t: t.label() == 'S1' or t.label() == 'S2' or t.label() == 'S3')):\n          result.append(' '.join([x[0] for x in subtree]))\n        \n        return ' '.join(result)","dcc38a14":"train['ex_text']=train['text'].apply(lambda x: extract(x))\ntest['ex_text']=test['text'].apply(lambda x: extract(x))","386f3669":"def jaccard(strs): \n    str1=strs['selected_text']\n    str2=strs['ex_text']\n    \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ntrain['jaccard']=train[['selected_text','ex_text']].apply(lambda x: jaccard(x),axis=1)\n\nprint(f\"Average jaccard index in training data {train['jaccard'].mean()}\")","4ad5c73b":"#Reference: https:\/\/towardsdatascience.com\/textrank-for-keyword-extraction-by-python-c0bae21bcec0\n\nclass TextRank4Keyword():\n    \"\"\"Extract keywords from text\"\"\"\n    \n    def __init__(self):\n        self.d = 0.85 # damping coefficient, usually is .85\n        self.min_diff = 1e-5 # convergence threshold\n        self.steps = 10 # iteration steps\n        self.node_weight = None # save keywords and its weight\n\n    \n    def set_stopwords(self, stopwords):  \n        \"\"\"Set stop words\"\"\"\n        for word in STOP_WORDS.union(set(stopwords)):\n            lexeme = nlp.vocab[word]\n            lexeme.is_stop = True\n    \n    def sentence_segment(self, doc, candidate_pos, lower):\n        \"\"\"Store those words only in cadidate_pos\"\"\"\n        sentences = []\n        for sent in doc.sents:\n            selected_words = []\n            for token in sent:\n                # Store words only with cadidate POS tag\n                if token.pos_ in candidate_pos and token.is_stop is False:\n                    if lower is True:\n                        selected_words.append(token.text.lower())\n                    else:\n                        selected_words.append(token.text)\n            sentences.append(selected_words)\n        return sentences\n        \n    def get_vocab(self, sentences):\n        \"\"\"Get all tokens\"\"\"\n        vocab = OrderedDict()\n        i = 0\n        for sentence in sentences:\n            for word in sentence:\n                if word not in vocab:\n                    vocab[word] = i\n                    i += 1\n        return vocab\n    \n    def get_token_pairs(self, window_size, sentences):\n        \"\"\"Build token_pairs from windows in sentences\"\"\"\n        token_pairs = list()\n        for sentence in sentences:\n            for i, word in enumerate(sentence):\n                for j in range(i+1, i+window_size):\n                    if j >= len(sentence):\n                        break\n                    pair = (word, sentence[j])\n                    if pair not in token_pairs:\n                        token_pairs.append(pair)\n        return token_pairs\n        \n    def symmetrize(self, a):\n        return a + a.T - np.diag(a.diagonal())\n    \n    def get_matrix(self, vocab, token_pairs):\n        \"\"\"Get normalized matrix\"\"\"\n        # Build matrix\n        vocab_size = len(vocab)\n        g = np.zeros((vocab_size, vocab_size), dtype='float')\n        for word1, word2 in token_pairs:\n            i, j = vocab[word1], vocab[word2]\n            g[i][j] = 1\n            \n        # Get Symmeric matrix\n        g = self.symmetrize(g)\n        \n        # Normalize matrix by column\n        norm = np.sum(g, axis=0)\n        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n        \n        return g_norm\n\n    \n    def get_keywords(self, number=10):\n        \"\"\"Print top number keywords\"\"\"\n        text_list=[]\n        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n        for i, (key, value) in enumerate(node_weight.items()):\n            #print(key + ' - ' + str(value))\n            text_list.append(key)\n            \n            #if i > number:\n        return ' '.join(text_list)\n        \n        \n    def analyze(self, text, \n                candidate_pos=['NOUN', 'PROPN'], \n                window_size=4, lower=False, stopwords=list()):\n        \"\"\"Main function to analyze text\"\"\"\n        \n        # Set stop words\n        self.set_stopwords(stopwords)\n        \n        # Pare text by spaCy\n        doc = nlp(text)\n        \n        # Filter sentences\n        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n        \n        # Build vocabulary\n        vocab = self.get_vocab(sentences)\n        \n        # Get token_pairs from windows\n        token_pairs = self.get_token_pairs(window_size, sentences)\n        \n        # Get normalized matrix\n        g = self.get_matrix(vocab, token_pairs)\n        \n        # Initionlization for weight(pagerank value)\n        pr = np.array([1] * len(vocab))\n        \n        # Iteration\n        previous_pr = 0\n        for epoch in range(self.steps):\n            pr = (1-self.d) + self.d * np.dot(g, pr)\n            if abs(previous_pr - sum(pr))  < self.min_diff:\n                break\n            else:\n                previous_pr = sum(pr)\n\n        # Get weight for each node\n        node_weight = dict()\n        for word, index in vocab.items():\n            node_weight[word] = pr[index]\n        \n        self.node_weight = node_weight","ca390763":"tr4w = TextRank4Keyword()\n\ndef keywordextract(x):\n    tr4w.analyze(x, candidate_pos = ['NOUN', 'PROPN','VERB','ADJ'], window_size=4, lower=False)\n    return tr4w.get_keywords(10)\n\ntrain['ex_text']=train['text'].apply(lambda x: keywordextract(x))\ntest['ex_text']=test['text'].apply(lambda x: keywordextract(x))","8947f423":"def jaccard(strs): \n    str1=strs['selected_text']\n    str2=strs['ex_text']\n    \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ntrain['jaccard']=train[['selected_text','ex_text']].apply(lambda x: jaccard(x),axis=1)\n\nprint(f\"Average jaccard index in training data using textrank {train['jaccard'].mean()}\")","7560182a":"train=pd.read_csv(PATH+'train.csv')\ntest=pd.read_csv(PATH+'test.csv')\nsubmission=pd.read_csv(PATH+'sample_submission.csv')\n\ntrain=train.dropna()\n\ntrain_np = np.array(train)\ntest_np = np.array(test)","d4d83205":"def find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    paragraphs = []\n    for line in train:\n        context = line[1]\n\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n        \n    return paragraphs\n\nqa_train = do_qa_train(train_np)\n\nwith open('data\/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)","c658b689":"\"\"\"\nPrepare testing data in QA-compatible format\n\"\"\"\n\noutput = {}\noutput['version'] = 'v1.0'\noutput['data'] = []\n\ndef do_qa_test(test):\n    paragraphs = []\n    for line in test:\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return paragraphs\n\nqa_test = do_qa_test(test_np)\n\nwith open('data\/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)","d7449f1d":"!pip install '\/kaggle\/input\/simple-transformers-pypi\/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '\/kaggle\/input\/simple-transformers-pypi\/simpletransformers-0.22.1-py3-none-any.whl' -q","3fdb26eb":"%%time\n\nfrom simpletransformers.question_answering import QuestionAnsweringModel\n\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', \n                               MODEL_PATH, \n                               args={'reprocess_input_data': True,\n                                     'overwrite_output_dir': True,\n                                     'learning_rate': 5e-5,\n                                     'num_train_epochs': 3,\n                                     'max_seq_length': 192,\n                                     'doc_stride': 64,\n                                     'fp16': False,\n                                    },\n                              use_cuda=True)\n\nmodel.train_model('data\/train.json')","f4795d22":"%%time\n\npredictions = model.predict(qa_test)\npredictions_df = pd.DataFrame.from_dict(predictions)\n\npredictions_df = pd.DataFrame.from_dict(predictions)\nsubmission['selected_text'] = predictions_df['answer']\n\nfor i in range(len(submission)):\n    id_ = submission['textID'][i]\n    if test['sentiment'][i] == 'neutral' or len(test['text'][i].split())<4: # neutral postprocessing\n        submission.loc[i, 'selected_text'] = test['text'][i]\n\nsubmission.to_csv('submission.csv',index=None)\nprint(\"File submitted successfully.\")","75789b70":"submission.head()","73ebd03f":"### Unigram charts","e4b90631":"<font size=4 color='blue'>Preparing train data in SQUAD format<\/font>","5d0a8442":"<font size=4 color='blue'>Preparing test data in SQUAD format<\/font>","bdf68dca":"* There are 3535 unique records in the test data to be predicted","3aa040d7":"The following code is taken from the simplified kernel  [here](https:\/\/www.kaggle.com\/jonathanbesomi\/question-answering-starter-pack)","57d37fc8":"<img align=\"left\" src=\"https:\/\/media.sproutsocial.com\/uploads\/2019\/08\/twitter-stats.svg\"><\/img>","d84ab074":"I could see urls,single letters and lots of special characters in the raw text.\n\n<p>Let's do some basic cleaning for further processing as it needs to maintain the punctuations<\/p>","84212d5c":"### Bigram charts","ea811e3f":"<font size=4 color='red'>Keyword phrase extraction using textrank<\/font>","c21395e3":"<img align=\"center\" src=\"https:\/\/miro.medium.com\/max\/515\/1*DkZjlRNEjPSc8RNL7yWggA.jpeg\"><\/img>","976a481f":"<font size=4 color='red'>Peeping into data:<\/font>","0865eabf":"<font size=5 color='orange'>Kindly upvote the kernel if you like my work!!<\/font>","994443ae":"<font size=4 color='blue'>Inference<\/font>","3001cc7f":"The formula to find the Index is:\nJaccard Index = (the number in both sets) \/ (the number in either set) * 100\n\nThe same formula in notation is:\nJ(X,Y) = |X\u2229Y| \/ |X\u222aY|\n\nIn Steps, that\u2019s:\n\n   Count the number of members which are shared between both sets.\n   Count the total number of members in both sets (shared and un-shared).\n   Divide the number of shared members (1) by the total number of members (2).\n   Multiply the number you found in (3) by 100.\n\nThis percentage tells you how similar the two sets are.\n\n*    Two sets that share all members would be **100**% similar. **the closer to 100%, the more similarity **(e.g. 90% is more similar than 89%).\n*    If they share **no members**, they are **0% similar**.\n*    The **midway point \u2014 50%** \u2014 means that the two sets share half of the members.\n","ff0d7bb9":"<font size=4 color='red'>Baseline - Chunking<\/font>","d85a610d":"<font size=4 color='red'>Cleaning the text - minimal:<\/font>","5b20bf21":"<font size=4 color='green'>Text Rank<\/font>\n<n><\/n>         \n<p>TextRank is an algorithm based on PageRank, which often used in keyword extraction and text summarization.<\/p>\n<n><\/n>\n\nPageRank (PR) is an algorithm used to calculate the weight for web pages. We can take all web pages as a big directed graph. In this graph, a node is a webpage. If webpage A has the link to web page B, it can be represented as a directed edge from A to B.\n\nIn text rank we consider our nodes as text,whereas in pagerank it is webpage\n\nAfter we construct the whole graph, we can assign weights for web pages by the following formula.\n\n<img align=\"left\" src=\"https:\/\/miro.medium.com\/max\/1526\/1*hheHfLOTjPW3uSsSxWKylQ.png\"><\/img>","ea2fe0e2":"<font size=4 color='green'>Updates:<\/font>\n* Text rank implementation    ","5e97819d":"<font size=4 color='red'>Insights from selected texts:<\/font>","5bfcde36":"### Let's get started","b61a82a3":"<font size=4 color='green'>Multi-word Phrase Extraction<\/font>\n<n><\/n>   \n\nA brief outline of the keyword extraction process using TextRank:\n\n* Words are tokenized and annotated with parts-of-speech tags\n* Words are added to the graph as vertices (but first filtered based on whether they are a noun or adjective)\n* An edge is added between words that co-occur within N words of each other\n* The TextRank vertices ranking algorithm is run until convergence\n* The vertices are ranked by their score and the top T are selected as keywords\n* If vertices in the top T keywords appear as adjacent vertices in the graph, they are grouped together to form a multi-word expression\/phrase.","43edabc0":"Few meanings are explicit and few are implicit as well and checking the plots, neutral tweets seems to be tougher to get right.","d797e829":"<font size=4 color='blue'>Simple transformers<\/font>","a0ac3325":"Again this is a standard code from the prev competitions. To look into the distribution of ngrams","07f1910a":"<font size=4 color='blue'>Reference<\/font>\n<n><\/n>\nYou can see the detailed explanation from this blog\n\n<p>https:\/\/towardsdatascience.com\/textrank-for-keyword-extraction-by-python-c0bae21bcec0<\/p>\n<p>https:\/\/www.kaggle.com\/cheongwoongkang\/roberta-baseline-starter-simple-postprocessing<\/p>","7fdbabf5":"<font size=4 color='red'>About the metrics<\/font>","3c090474":"* There is one missing data in the text field. \n* There are 27486 unique records here and 3 sentiments.","d5cd4c33":"<font size=4 color='red'>Meta features<\/font>","21593d28":"<font size=4 color='blue'>One line training<\/font>","2f993734":"<img align=\"left\" src=\"https:\/\/thatware.co\/wp-content\/uploads\/2019\/02\/Jaccard-Index-ThatWare.png\"><\/img>","99c31d2e":"<font size=4 color='green'>Objective:<\/font>\n     <p>In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.We need to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.<\/p>\n     \n     \n<font size=4 color='green'>Interesting aspect:<\/font>\n      <p> What's so special in this competition? Apart from regular sentiment classification, Here the actual task is to extract phrases that results in the sentiment <\/p>\n      "}}