{"cell_type":{"f4b3556a":"code","9aa181ae":"code","864f1323":"code","1f8a81b5":"code","9c34b1e3":"code","fd7f9a57":"code","8b28ca39":"code","b6c4858e":"code","00481335":"code","c775aaaa":"code","23d0b82a":"code","5e046c0f":"code","a5eae9c8":"code","0f7922b5":"code","a79c64fd":"code","b47cc194":"code","a32e4016":"code","f40f8dfa":"code","b1129de9":"code","2cfcc50b":"code","0aa09262":"code","5d187147":"code","c0cba290":"code","322c50f5":"code","2687053f":"code","cc4c8d5e":"code","090d86d1":"code","ffc86692":"code","0a73492f":"code","c02e8084":"code","37e21746":"code","4b3a6524":"code","575a2624":"code","026d7745":"code","8a5f0876":"code","989eb065":"code","f4b0cdb9":"code","f8be24a1":"code","83f94520":"code","e82005dd":"code","8bf7b0ec":"code","aa646dc3":"code","5c3b7cb9":"markdown","1aa1732f":"markdown","914756ef":"markdown","31b07810":"markdown","33e985d7":"markdown","6197e08a":"markdown","ae727491":"markdown","c6018626":"markdown","78ddeb33":"markdown","15dbd862":"markdown","6df8d415":"markdown","940dc6fd":"markdown","2df0072d":"markdown","97e4898f":"markdown","a9f8d866":"markdown","1b97bc95":"markdown","6d835680":"markdown","c605f1d4":"markdown","189c76f9":"markdown","2e4bf3d8":"markdown","1da319e5":"markdown","359b2ffc":"markdown","a60b6243":"markdown","4a0d19a2":"markdown","62bfd47f":"markdown","bcdb071e":"markdown","b8067137":"markdown","c3c02cd3":"markdown","ed53aeef":"markdown","594f203b":"markdown","7ff24102":"markdown","826adf64":"markdown","80d6e905":"markdown","c4a1454d":"markdown","515406f3":"markdown","d46ff986":"markdown","881844f9":"markdown","97e8e8a1":"markdown","0ee1a656":"markdown","3725164d":"markdown","931bbd66":"markdown","2a5efb33":"markdown","a3d1ce7f":"markdown","2a6f6b67":"markdown","044d3715":"markdown","48f3c7f6":"markdown","2f6ee93f":"markdown","ba865b0c":"markdown","d5e67eda":"markdown","c2bcc78b":"markdown","4e37eabe":"markdown","3f4e62e2":"markdown","65e3f9e9":"markdown","2ebfe157":"markdown","c269b089":"markdown","b1058b8f":"markdown","1eb44b66":"markdown","3a32ec60":"markdown","888d2211":"markdown","38ef3b6c":"markdown","11776b2e":"markdown","38ce5d67":"markdown","60616f18":"markdown","861c1d6b":"markdown","5f030c55":"markdown","15e59733":"markdown","83923ae2":"markdown","677b1abb":"markdown","ab59cc16":"markdown","fbd96049":"markdown","a2c19cc8":"markdown","12f4093a":"markdown","ace3084f":"markdown","715460c5":"markdown","7f703018":"markdown","89216a9d":"markdown","95315c1a":"markdown","43f9b0dd":"markdown","174238ea":"markdown","0c444233":"markdown","f17d7985":"markdown","5f04ddd4":"markdown","9a96eb26":"markdown","cb20c9d3":"markdown","4a1364b9":"markdown","88a73b47":"markdown","988a4704":"markdown","78ea173c":"markdown","cf4e361c":"markdown","f98d6d51":"markdown"},"source":{"f4b3556a":"#import os\n#path=os.path.normpath(os.getcwd() + os.sep + os.pardir)","9aa181ae":"import pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn import metrics\n#train = pd.read_csv(path+'\\\\data\\\\train.csv')\ntrain = pd.read_csv('..\/input\/customer-churn-prediction-2020\/train.csv')\ntrain.head(5)","864f1323":"train.columns","1f8a81b5":"print(\"Null values per Column in Training Dataset:\\n\",train.isnull().sum(axis = 0))","9c34b1e3":"train[train.duplicated(keep=False)]","fd7f9a57":"train.dtypes","8b28ca39":"box_df = train[['number_vmail_messages', 'total_day_minutes',\n       'total_day_calls', 'total_day_charge', 'total_eve_minutes',\n       'total_eve_calls', 'total_eve_charge', 'total_night_minutes',\n       'total_night_calls', 'total_night_charge', 'total_intl_minutes',\n       'total_intl_calls', 'total_intl_charge',\n       'number_customer_service_calls']]\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nrcParams['figure.figsize'] = 7, 5\nfor column in box_df:\n    plt.figure()\n    box_df.boxplot([column])","b6c4858e":"plt.hist(train['international_plan'])\nplt.title(\"Distribution of International Plans\")\nplt.ylabel(\"Value Counts\")\nplt.xlabel(\"Factor\")\nplt.show()","00481335":"plt.hist(train['voice_mail_plan'])\nplt.title(\"Distribution of voice_mail_plan\")\nplt.ylabel(\"Value Counts\")\nplt.xlabel(\"Factor\")\nplt.show()","c775aaaa":"plt.hist(train['churn'])\nplt.title(\"Distribution of churn\")\nplt.ylabel(\"Value Counts\")\nplt.xlabel(\"Factor\")\nplt.show()","23d0b82a":"plt.hist(train['total_night_minutes'], 5)\nplt.title(\"Distribution of total_night_minutes\")\nplt.ylabel(\"Value Counts\")\nplt.xlabel(\"Group\")\nplt.show()","5e046c0f":"import seaborn as sns\nsns.pairplot(train[['international_plan',\n       'voice_mail_plan','number_vmail_messages', 'total_day_minutes',\n       'total_day_calls', 'total_day_charge', 'total_eve_minutes',\n       'total_eve_calls', 'total_eve_charge', 'total_night_minutes',\n       'total_night_calls', 'total_night_charge', 'total_intl_minutes',\n       'total_intl_calls', 'total_intl_charge',\n       'number_customer_service_calls', 'churn']], diag_kind='kde')","a5eae9c8":"\n\ncorr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","0f7922b5":"train.head()","a79c64fd":"sns.boxplot(x=train['account_length'])","b47cc194":"sns.boxplot(x=train['number_vmail_messages'])","a32e4016":"sns.boxplot(x=train['total_day_minutes'])","f40f8dfa":"sns.boxplot(x=train['total_day_calls'])","b1129de9":"numeric = train[['number_vmail_messages', 'total_day_minutes',\n       'total_day_calls', 'total_day_charge', 'total_eve_minutes',\n       'total_eve_calls', 'total_eve_charge', 'total_night_minutes',\n       'total_night_calls', 'total_night_charge', 'total_intl_minutes',\n       'total_intl_calls', 'total_intl_charge',\n       'number_customer_service_calls']]\nfrom sklearn.preprocessing import MinMaxScaler\n# fit scaler on training data\nnorm = MinMaxScaler().fit(numeric)\n# transform training data\nnormal_numeric_data = norm.transform(numeric)\ncols = numeric.columns.values\n# normalized data to dataframe\nnormal_numeric_data = pd.DataFrame(data = normal_numeric_data, columns = cols)\nnormal_numeric_data.head(5)","2cfcc50b":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nCategorical = train[['international_plan','voice_mail_plan','churn']]\nCategorical = Categorical.apply(LabelEncoder().fit_transform)\nCategorical.head(5)","0aa09262":"import warnings\nwarnings.filterwarnings('ignore')\ntotal_data = numeric\ntotal_data['international_plan'] = Categorical.international_plan\ntotal_data['voice_mail_plan'] = Categorical.voice_mail_plan\ntotal_data['churn'] = Categorical.churn\ntotal_data.head(5)","5d187147":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom matplotlib import pyplot\n# feature selection\ndef select_features(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=f_classif, k='all')\n    # learn relationship from training data\n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n# load the dataset\nl = len(total_data.columns.values)\nl = l - 1\nX = total_data.iloc[:,0:l]\ny = total_data.churn\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n# what are scores for the features\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()","c0cba290":"score = []\nscore = fs.scores_\nselected_feature = []\ni = 0\nwhile i<len(score):\n    if score[i] > 15:\n        selected_feature.append(i)\n    i = i+1\nprint(\"These Feature Has been Selected: \",selected_feature)\nprint()\nprint(\"Getting indexes the Actual Data Columns:\")\nprint()\ni = 0\nwhile i<len(total_data.columns):\n    print(\"Index: \",i,\" \",total_data.columns[i])\n    i = i+1\nfinal_selected_df = pd.DataFrame()\nfinal_selected_df['number_vmail_messages'] = total_data['number_vmail_messages']\nfinal_selected_df['total_day_minutes'] = total_data['total_day_minutes']\nfinal_selected_df['total_day_charge'] = total_data['total_day_charge']\nfinal_selected_df['total_eve_minutes'] = total_data['total_eve_minutes']\nfinal_selected_df['total_eve_charge'] = total_data['total_eve_charge']\nfinal_selected_df['number_customer_service_calls'] = total_data['number_customer_service_calls']\nfinal_selected_df['international_plan'] = total_data['international_plan']\nfinal_selected_df['voice_mail_plan'] = total_data['voice_mail_plan']\nfinal_selected_df['churn'] = total_data['churn']\nfinal_selected_df.head(5)","322c50f5":"final_selected_df.to_csv(\"final_selected_df.csv\",index = False)","2687053f":"import pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nprint(\"Reading the Dataset\")\nchurn_data = pd.read_csv('final_selected_df.csv')\nprint()\nprint(\"Shape of Dataset is: \",churn_data.shape)","cc4c8d5e":"churn_data.columns","090d86d1":"print(\"Null values per column are:\")\nprint(churn_data.isnull().sum(axis = 0))\nprint(\"Filling out the Null values\")\nprint(\"Spliting the Data into Input and output for the Model\")","ffc86692":"X=churn_data.loc[:, churn_data.columns != 'churn']","0a73492f":"y=churn_data['churn']","c02e8084":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\nprint(\"A support vector machine model to predict whether a customer churn or not\")\nclf = SVC()\nclf.fit(X_train, y_train.values.ravel())\ny_true, y_pred = y_test, clf.predict(X_test)\nprint(\"\\tModel\\t\\t\\tTrain Acc\\tTest Acc\")\nprint(\"Support Vector Machine\\t\\t  \",int(clf.score(X_train,y_train)*100),\"% \\t  \", int(100*clf.score(X_test,y_test)),\"% \\t  \")\nTrueResultslat=y_test\nPredictedResultslat=clf.predict(X_test)\ndata = {'y_Actual':    y_test,\n    'y_Predicted': PredictedResultslat\n    }\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrixf = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrixf, annot=True)\nplt.show()\nprint((\"Confusion Matrix: SVM \"))\nprint(confusion_matrix(y_test,PredictedResultslat))\nprint((\"Classification Report:SVM \"))\nprint(classification_report(y_test,PredictedResultslat))\nLR_accuracy = metrics.accuracy_score(y_test, PredictedResultslat)\nprint((\"Support Vector Machine Classifier Accuracy:\" ,LR_accuracy*100))\nAccuracy_SVM=LR_accuracy*100","37e21746":"print(\"A Logistic Regression model to predict whether a customer churn or not\")\nclf = LogisticRegression()\nclf.fit(X_train, y_train.values.ravel())\ny_true, y_pred = y_test, clf.predict(X_test)\nprint(\"\\tModel\\t\\t\\tTrain Acc\\tTest Acc\")\nprint(\"Logistic Regression\\t\\t  \",int(clf.score(X_train,y_train)*100),\"% \\t  \", int(100*clf.score(X_test,y_test)),\"% \\t  \")\nTrueResultslat=y_test\nPredictedResultslat=clf.predict(X_test)\ndata = {'y_Actual':    y_test,\n    'y_Predicted': PredictedResultslat\n    }\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrixf = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrixf, annot=True)\nplt.show()\nprint((\"Confusion Matrix: Logistic Regression \"))\nprint(confusion_matrix(y_test,PredictedResultslat))\nprint((\"Classification Report:Logistic Regression \"))\nprint(classification_report(y_test,PredictedResultslat))\nLR_accuracy = metrics.accuracy_score(y_test, PredictedResultslat)\nprint((\"Logistic Regression Classifier Accuracy:\" ,LR_accuracy*100))\nAccuracy_Logistic=LR_accuracy*100","4b3a6524":"print(\"A Decision Tree Classifier model to predict whether a customer churn or not\")\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train.values.ravel())\ny_true, y_pred = y_test, clf.predict(X_test)\nprint(\"\\tModel\\t\\t\\tTrain Acc\\tTest Acc\")\nprint(\"Decision Tree Classifier\\t\\t  \",int(clf.score(X_train,y_train)*100),\"% \\t  \", int(100*clf.score(X_test,y_test)),\"% \\t  \")\nTrueResultslat=y_test\nPredictedResultslat=clf.predict(X_test)\ndata = {'y_Actual':    y_test,\n    'y_Predicted': PredictedResultslat\n    }\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrixf = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrixf, annot=True)\nplt.show()\nprint((\"Confusion Matrix: Decision Tree Classifier \"))\nprint(confusion_matrix(y_test,PredictedResultslat))\nprint((\"Classification Report:Decision Tree Classifier \"))\nprint(classification_report(y_test,PredictedResultslat))\nLR_accuracy = metrics.accuracy_score(y_test, PredictedResultslat)\nprint((\"Decision Tree Classifier Accuracy:\" ,LR_accuracy*100))\nAccuracy_Decision=LR_accuracy*100","575a2624":"print(\"A Random Forest Classifier model to predict whether a customer churn or not\")\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train.values.ravel())\ny_true, y_pred = y_test, clf.predict(X_test)\nprint(\"\\tModel\\t\\t\\tTrain Acc\\tTest Acc\")\nprint(\"Random Forest Classifier \\t\\t  \",int(clf.score(X_train,y_train)*100),\"% \\t  \", int(100*clf.score(X_test,y_test)),\"% \\t  \")\nTrueResultslat=y_test\nPredictedResultslat=clf.predict(X_test)\ndata = {'y_Actual':    y_test,\n    'y_Predicted': PredictedResultslat\n    }\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrixf = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrixf, annot=True)\nplt.show()\nprint((\"Confusion Matrix: Random Forest Classifier \"))\nprint(confusion_matrix(y_test,PredictedResultslat))\nprint((\"Classification Report:Random Forest Classifier \"))\nprint(classification_report(y_test,PredictedResultslat))\nLR_accuracy = metrics.accuracy_score(y_test, PredictedResultslat)\nprint((\"Random Forest Classifier Accuracy:\" ,LR_accuracy*100))\nAccuracy_Random=LR_accuracy*100","026d7745":"from sklearn.linear_model import RidgeClassifier\nprint(\"A RidgeClassifier model to predict whether a customer churn or not\")\nclf = RidgeClassifier()\nclf.fit(X_train, y_train.values.ravel())\ny_true, y_pred = y_test, clf.predict(X_test)\nprint(\"\\tModel\\t\\t\\tTrain Acc\\tTest Acc\")\nprint(\"RidgeClassifier\\t\\t  \",int(clf.score(X_train,y_train)*100),\"% \\t  \", int(100*clf.score(X_test,y_test)),\"% \\t  \")\nTrueResultslat=y_test\nPredictedResultslat=clf.predict(X_test)\ndata = {'y_Actual':    y_test,\n    'y_Predicted': PredictedResultslat\n    }\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrixf = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrixf, annot=True)\nplt.show()\nprint((\"Confusion Matrix: RidgeClassifier \"))\nprint(confusion_matrix(y_test,PredictedResultslat))\nprint((\"Classification Report:RidgeClassifier\"))\nprint(classification_report(y_test,PredictedResultslat))\nLR_accuracy = metrics.accuracy_score(y_test, PredictedResultslat)\nprint((\"RidgeClassifier Accuracy:\" ,LR_accuracy*100))\nAccuracy_RidgeClassifier=LR_accuracy*100","8a5f0876":"from sklearn.linear_model import SGDClassifier\nprint(\"A SGDClassifier model to predict whether a customer churn or not\")\nclf = RidgeClassifier()\nclf.fit(X_train, y_train.values.ravel())\ny_true, y_pred = y_test, clf.predict(X_test)\nprint(\"\\tModel\\t\\t\\tTrain Acc\\tTest Acc\")\nprint(\"SGDClassifier\\t\\t  \",int(clf.score(X_train,y_train)*100),\"% \\t  \", int(100*clf.score(X_test,y_test)),\"% \\t  \")\nTrueResultslat=y_test\nPredictedResultslat=clf.predict(X_test)\ndata = {'y_Actual':    y_test,\n    'y_Predicted': PredictedResultslat\n    }\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrixf = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrixf, annot=True)\nplt.show()\nprint((\"Confusion Matrix: SGDClassifier \"))\nprint(confusion_matrix(y_test,PredictedResultslat))\nprint((\"Classification Report:SGDClassifier\"))\nprint(classification_report(y_test,PredictedResultslat))\nLR_accuracy = metrics.accuracy_score(y_test, PredictedResultslat)\nprint((\"SGDClassifier Accuracy:\" ,LR_accuracy*100))\nAccuracy_SGDClassifier=LR_accuracy*100\n","989eb065":"from sklearn.neighbors import KNeighborsClassifier\nprint(\"A KNeighborsClassifier model to predict whether a customer churn or not\")\nclf = RidgeClassifier()\nclf.fit(X_train, y_train.values.ravel())\ny_true, y_pred = y_test, clf.predict(X_test)\nprint(\"\\tModel\\t\\t\\tTrain Acc\\tTest Acc\")\nprint(\"KNeighborsClassifier\\t\\t  \",int(clf.score(X_train,y_train)*100),\"% \\t  \", int(100*clf.score(X_test,y_test)),\"% \\t  \")\nTrueResultslat=y_test\nPredictedResultslat=clf.predict(X_test)\ndata = {'y_Actual':    y_test,\n    'y_Predicted': PredictedResultslat\n    }\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrixf = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrixf, annot=True)\nplt.show()\nprint((\"Confusion Matrix: KNeighborsClassifier \"))\nprint(confusion_matrix(y_test,PredictedResultslat))\nprint((\"Classification Report:KNeighborsClassifier\"))\nprint(classification_report(y_test,PredictedResultslat))\nLR_accuracy = metrics.accuracy_score(y_test, PredictedResultslat)\nprint((\"KNeighborsClassifier Accuracy:\" ,LR_accuracy*100))\nAccuracy_KNeighborsClassifier=LR_accuracy*100\n","f4b0cdb9":"AA=pd.DataFrame()\nAA['Algorithm']=[ 'SVM','Random Forest Classifier','Logistic Regression','Decision Tree Classifier','Ridge Classifier','SGD Classifier'\n                ,'KNeighborsClassifier ']","f8be24a1":"AA['Accuracy (%)']=[Accuracy_SVM , Accuracy_Random,Accuracy_Logistic, Accuracy_Decision,Accuracy_RidgeClassifier\n                   ,Accuracy_SGDClassifier,Accuracy_KNeighborsClassifier]","83f94520":"AA=AA.sort_values(['Accuracy (%)'])","e82005dd":"AA","8bf7b0ec":"import plotly.express as px","aa646dc3":"fig = px.bar(AA, x='Algorithm', y='Accuracy (%)',title=\"Accuracy of each Classifier for Churn Predictions\",color='Accuracy (%)')\nfig.show()","5c3b7cb9":"## * Numerical:","1aa1732f":"## The above two plots are showing the correaltion between all parameters of the dataset","914756ef":"# The Business Pain:\n### Most telecom companies suffer from voluntary churn. Churn rate has strong impact on the life time value of the customer because it affects the length of service and the lifetime customer value. For example, if a company has a 25% churn rate, the average customer lifetime is 4 years; similarly, a company with a churn rate of 50% has an average customer lifetime of 2 years. It is estimated that 75 percent of the 17 to 20 million subscribers signing up with a new wireless carrier every year are coming from another wireless provider, which means they are churners. Telecom companies spend on average, hundreds of dollars to acquire a new customer and when that customer leaves, the company loses not only the future revenue from that customer but also the resources spent to acquire that customer. Churn erodes profitability.","31b07810":"## *Numerical:","33e985d7":"## ANOVA TEST IS DONE IN NEXT STEP","6197e08a":"## Steps that have been adopted by telecom companies so far:\n### Telecom companies have used two approaches to address churn - (a) Untargeted approach and (b) Targeted approach. The untargeted approach relies on superior product and mass advertising to increase brand loyalty and thus retain customers. The targeted approach relies on identifying customers who are likely to churn, and provide suitable intervention to encourage them to stay.","ae727491":"# Since it's a Classification Problem\n# The following  Classification algorithms are used\n- Support Vector Machine\n- Decision Tree Classification \n- Ridge Classification \n- SGD Classification\n- KNEIGHBORSCLASSIFIER \n- Rabdom Forest Classification \n- Logistic Regression\n\n## The classification reports, training, and testing accuracy for each algorithm is drawn\n## Testing accuracy is used to compare each algorithm","c6018626":"# Code for Parameter Tuning SVM Classifier ","78ddeb33":"### * No","15dbd862":"## Ridge Regression tends to a portion of the issues of Ordinary Least Squares by forcing a punishment on the size of the coefficients. The edge coefficients limit a punished remaining amount of squares \n## The multifaceted nature boundary controls the measure of shrinkage: the bigger the estimation of, the more noteworthy the measure of shrinkage and hence the coefficients become stronger to collinearity. Likewise, with other direct models, Ridge will take in its fit strategy exhibits X, y and will store the coefficients of the straight model in its coef_ part. This strategy has similar request of unpredictability as Ordinary Least Squares. \n## It will be used as classification here with binary lables\n","6df8d415":"### * No","940dc6fd":"## Categorical: Bar plots","2df0072d":"## Are dates in the date data type?","97e4898f":"# SGD Classification","a9f8d866":"## Assessing Missing Values (but don't fill\/impute them yet!)","1b97bc95":"## Accuracy Measures techniques used\n- Mainly 4 techqniques are used\n#### Precision \n##### Precision talks about how precise\/accurate our model is out of those predicted positive, how many of them are actual positive. Precision is a good measure to determine, when the costs of False Positive is high.\n#### Recall\n##### Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.\n#### F1- Score\n##### F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).\n#### Accuracy \n#####  Accuracy shows the combined effect of the precision and recall when both factors are important \n\n###### Since it is important to know which factors will affect customer churn and what factors do not, we focused on combined accuracy for the final purpose","6d835680":"# Conclusion\n### For a company to be profitable we believe that they should focus on 80 20 rules. From our prediction results and feature selections we can conclude that only eight variables 'number_vmail_messages', 'total_day_minutes', 'total_day_charge', 'total_eve_minutes', 'total_eve_charge','number_customer_service_calls', 'international_plan', 'voice_mail_plan' are enough to predict whether the customer will churn or not. If they use our algorithm random forest we can say that it is 93% accurate that the customer will churn will depend upon these five variables. So, companies need to focus on these eight features to determine what they can improve so that customers may not leave.  Since our model is 93% accurate, it can be said that it is imperitive that these five variables should never be neglected.","c605f1d4":"<br>\n<br> <center> <font size=\"7\" color=Orange>Milestone 4: Final Project Submission<\/font> <\/center> <br\/>","189c76f9":"# Combined Accuracy for each Algorithm","2e4bf3d8":"### How many Customers will Churn","1da319e5":"<br>\n<br> <font size=\"6\" color=Blue>Data Cleaning<\/font> <br\/>\n<br>","359b2ffc":"## The Problem is based data compiled using records from a telecom company where the company wants to predict the Churn of a customer depending upon past customer data. Churn refers to wether or not a customer will sever their business relationship in the next quarter depending upon its previous credit history.","a60b6243":"# K Neighbour Classification ","4a0d19a2":"## Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the computational burden, achieving faster iterations in trade for a lower convergence rate","62bfd47f":"# Decision Tree Classifier ","bcdb071e":"<br>\n<br> <font size=\"6\" color=Blue>Next Steps<\/font> <br\/>\n<br>","b8067137":"### There is no Null value in the Dataset","c3c02cd3":"### There are no completely identical rows","ed53aeef":"# Train Test Splitting the dataset","594f203b":"## Box Plot to see Each Numeric Column Data Distribution","7ff24102":"# Libraries and Dataset Reading","826adf64":"## Are there lists or dictionaries packed into one feature?","80d6e905":"### * Yes there is a column named \"churn\" that should be numeric when we will use in the machine learning Model","c4a1454d":"We have already encoded the categorical Columns, so now we are just going to do the Numeric Feature Selection using ANOVA","515406f3":"## Feature Engineering (what columns\/features can you make to add value & information to your data?) and Feature Selection like:","d46ff986":"<br>\n<br> <center> <font size=\"7\" color=Orange>Milestone 3: Machine Learning<\/font> <\/center> <br\/>","881844f9":"## Are there categorical columns that should be numerical?","97e8e8a1":"<br>\n<br> <font size=\"6\" color=Blue>Visualizations<\/font> <br\/>\n<br>","0ee1a656":"## Is the data in the first few rows consistent with the name of the feature?","3725164d":"## Copyrights:\n###    Nicholas Russo","931bbd66":"## The need for doing any relevant statistical tests (e.g., T-tests)","2a5efb33":"### Decision tree algorithm falls under the category of supervised learning. They can be used to solve both regression and classification problems. Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label and attributes are represented on the internal node of the tree. We can represent any boolean function on discrete attributes using the decision tree.","a3d1ce7f":"## Columns in the Dataset","2a6f6b67":"## Research Questions\n## With how much accuracy can we predict wether a customer will churn or not?\n## What are the main features that affect churn predictions the most?","044d3715":"<br>\n<br> <center> <font size=\"4\" color=Orange>Milestone 2: Exploratory Data Analysis (EDA)<\/font> <\/center> <br\/>","48f3c7f6":"### A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.","2f6ee93f":"## Checking for Duplicates","ba865b0c":"### **  Correlation ANOVA","d5e67eda":"## From the above box plots it can be clearly seen that there are outliers in the dataset and the dataset is not evenly distributed ","c2bcc78b":"## Reading the Data","4e37eabe":"### Selecting Final Features that will be used in the Machine Learning","3f4e62e2":"<br>\n<br> <font size=\"6\" color=Blue>Surprises<\/font> <br\/>\n<br>","65e3f9e9":"## Visualizing and Understanding the Data, including understanding:\n## How your data is distributed (numerical & categorical)","2ebfe157":"## Checking if there is any Null value in any Column","c269b089":"### * I have taken the classification dataset from Kaggle.\n### * The dataset can be found at the following link: <a href=\"url\">https:\/\/www.kaggle.com\/c\/customer-churn-prediction-2020\/data<\/a>\n","b1058b8f":"### How many Customers are using voice mail plan","1eb44b66":"## Further work\n#### Further we can tune the parameters of each algorithm to achieve the greatest accuracy\n#### A disadvantage to this is the time it takes to select the best parameters","3a32ec60":"### Writing Final Data for Milestone 3","888d2211":"## *Categorical:","38ef3b6c":"### ** Label encoder (ordinal)","11776b2e":"### *I have chosen the dataset of a Telecommunication company and I am going to predict whether the customer will churn the network or not.\n### * We have different attributes like daily call time, night call time, daily recharge etc.\n### * On the base of these attributes we are going to predict whether they will churn or not.","38ce5d67":"### The ML Pipeline that I have followed is :\n#### Importing the necessary libraries and the dataset\n### Performing Data Preprocessing (Exploratory Data Analysis and Data Manipulation)\n### Modelling using Logistic Regression, KNN and Random Forest\n### Performing Prediction\n### Visualization of combined errors","60616f18":"## If there are any outliers... (note them but don't remove them yet!)","861c1d6b":"## Logistic Regression:\n### Logistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature. Dichotomous means there are only two possible classes. For example, it can be used for cancer detection problems. It computes the probability of an event occurrence.\n### It is a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function.\n    ","5f030c55":"### K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. In Credit ratings, financial institutes will predict the credit rating of customers. In loan disbursement, banking institutes will predict whether the loan is safe or risky. In political science, classifying potential voters in two classes will vote or won\u2019t vote. KNN algorithm used for both classification and regression problems. KNN algorithm based on feature similarity approach.","15e59733":"# Random Forest Classification","83923ae2":"<br>\n<br> <center> <font size=\"7\" color=Orange>Milestone 5: Churn Prediction<\/font> <\/center> <br\/>","677b1abb":"# Checking for Null Values","ab59cc16":"### There is no missing data in our dataset","fbd96049":"### How many Customers are using an international plan \n\n*   List item\n*   List item\n\n","a2c19cc8":"### Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.","12f4093a":"print(\"Improving Model using Grid Search\")\nprint()\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n\n    print()\n    print(\"Working with\",tuned_parameters)\n    clf = GridSearchCV(SVC(), tuned_parameters, scoring='%s_macro' % score)\n    clf.fit(X_train, y_train.values.ravel())\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n\n    print(\"Detailed classification report:\")\n    print()\n    print(\"The model is trained on the full development set.\")\n    print(\"The scores are computed on the full evaluation set.\")\n    print()\n    y_true, y_pred = y_test, clf.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    print()\n","ace3084f":"### There is no missing value in the Dataset","715460c5":"## Datatype per Column","7f703018":"## Basic approach?","89216a9d":"### Churn (loss of customers to competition) is a major problem for telecom companies because it is well known that it is more expensive to acquire a new customer than to keep an existing customer. This assignment is about enabling churn reduction using analytics.","95315c1a":"# Support Vector Machine","43f9b0dd":"<br>\n<br> <font size=\"6\" color=Blue>Abstract<\/font> <br\/>\n<br>","174238ea":"## How the columns are related? (correlations or other relationships)","0c444233":"# Ridge Classification ","f17d7985":"## Dealing with Missing Data; e.g., things like:","5f04ddd4":"## To answer our question...\n## From our approach it can be seen that companies can predict, with up-to 93% accuracy, wether customers will churn or not\n## The main features that are to focused are 'number_vmail_messages', 'total_day_minutes', 'total_day_charge', 'total_eve_minutes', 'total_eve_charge','number_customer_service_calls', 'international_plan', 'voice_mail_plan'","9a96eb26":"## Transforming Data","cb20c9d3":"# Logistic Regression","4a1364b9":"<br>\n<br> <font size=\"6\" color=Blue>Problem Statement and Background<\/font> <br\/>\n<br>","88a73b47":"<br>\n<br> <font size=\"6\" color=Blue>Dataset<\/font> <br\/>\n<br>","988a4704":"### ** Normalize","78ea173c":"# Random Forest Classification is providing the best accuracy with a score 93%","cf4e361c":"### * Yes","f98d6d51":"## Role of predictive modeling:\n### In the targeted approach the company tries to identify in advance customers who are likely to churn. The company then targets those customers with special programs or incentives. This approach can bring in huge loss for a company, if churn predictions are inaccurate, because then firms are wasting incentive money on customers who would have stayed anyway."}}