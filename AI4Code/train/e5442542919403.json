{"cell_type":{"8bea4aa2":"code","7527bcf4":"code","70cbfb61":"code","9c8c9b77":"code","f65cefb4":"code","2d9d619f":"markdown","2b99ddee":"markdown","05b41cf7":"markdown","dbc376f4":"markdown","3735d537":"markdown","c0cf9a8c":"markdown","fa3253d4":"markdown"},"source":{"8bea4aa2":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler,MinMaxScaler\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,matthews_corrcoef,precision_score,recall_score\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.utils import class_weight\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7527bcf4":"df = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ndf = df.drop(df.columns[21:23],axis=1)\ndf=df.drop('CLIENTNUM',axis=1)  \n\ndf['Gender'].replace('M',1,inplace = True)\ndf['Gender'].replace('F',0,inplace = True)\n \ndf['Education_Level'].replace('Unknown',0,inplace = True)\ndf['Education_Level'].replace('Uneducated',1,inplace = True)\ndf['Education_Level'].replace('High School',2,inplace = True)\ndf['Education_Level'].replace('College',3,inplace = True)\ndf['Education_Level'].replace('Graduate',4,inplace = True)\ndf['Education_Level'].replace('Post-Graduate',5,inplace = True)\ndf['Education_Level'].replace('Doctorate',6,inplace = True)\n\ndf['Marital_Status'].replace('Unknown',0,inplace = True)\ndf['Marital_Status'].replace('Single',1,inplace = True)\ndf['Marital_Status'].replace('Married',2,inplace = True)\ndf['Marital_Status'].replace('Divorced',3,inplace = True)\n\ndf['Card_Category'].replace('Blue',0,inplace = True)\ndf['Card_Category'].replace('Gold',1,inplace = True)\ndf['Card_Category'].replace('Silver',2,inplace = True)\ndf['Card_Category'].replace('Platinum',3,inplace = True)\n\n\ndf['Income_Category'].replace('Unknown',0,inplace = True)\ndf['Income_Category'].replace('Less than $40K',1,inplace = True)\ndf['Income_Category'].replace('$40K - $60K',2,inplace = True)\ndf['Income_Category'].replace('$60K - $80K',3,inplace = True)\ndf['Income_Category'].replace('$80K - $120K',4,inplace = True)\ndf['Income_Category'].replace('$120K +',5,inplace = True)\n\ndf['Attrition_Flag'].replace('Existing Customer',0,inplace = True)\ndf['Attrition_Flag'].replace('Attrited Customer',1,inplace = True)\n\ndf.head()","70cbfb61":"#%%\nx = df[df.columns[1:20]]\ny = df[df.columns[0]]\n\n#%%\nrs = RobustScaler()\nx =rs.fit_transform(x)\n\n","9c8c9b77":"#%%\nX_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3, random_state=103)\n\n#%%\ncw = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\na = y.value_counts()\nratio = a[1]\/(a[1]+a[0])\n\n#%%\nweights = [ratio, 1-ratio]\nmodel = Sequential()\nmodel.add(Dense(19,activation=\"sigmoid\"))\nmodel.add(Dense(10,activation=\"sigmoid\"))\nmodel.add(Dense(1))\nmodel.compile(optimizer='rmsprop',loss = \"binary_crossentropy\",metrics=[\"BinaryAccuracy\"],loss_weights=weights)\n\nhistory = model.fit(x=X_train,y=Y_train,epochs=100, class_weight = {0:cw[0], 1:cw[1]})\n\npredictions = model.predict_classes(X_test) ","f65cefb4":"cm = confusion_matrix(Y_test, predictions)\nmcc = matthews_corrcoef(Y_test,predictions)      \nprint('\\n')\nprint('Neural Network Accuracy: ', accuracy_score(Y_test,predictions))\nprint('Neural Network Recall score: ', recall_score(Y_test,predictions))\n#%%\n\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt='g')\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Neural Network Confusion Matrix')","2d9d619f":"# Preprocessing the Dataset","2b99ddee":" # Prediticting Churn with ANN (Acc 90%,Recall 89%)","05b41cf7":"In this project we will build an Artificial Neural Network Model which predicts credit cards churn. \n\nThe dataset consists of 10,000 clients (entries) who mention their age, salary, marital status, credit card limit, credit card category, etc. Thus, the 19 attributes(features) will be our input to the neural network. As the dataset is have features is multiple formats; mainly string and intger it will require preparation.\n\nWe only have 16.07% of customers who have abandoned credit card services. Therefore, we have an unbalanced data set. In order to deal with this imbalance we will assigning weights to the two classes of target variable to balance it out.\n\nWe prepapre the dataset for ANN by replacing string variables to integers in the feature columns, We also drop the 'CLIENTNUM' column as it isnt a feature which will affect out target variable.","dbc376f4":"# Introduction Preparing the Dataset","3735d537":"As seen below, we get highly accurate model which is successfully able to predict the right labels for the churned customers.","c0cf9a8c":"At this stage we split our dataset into feature matrix and target matrix. We scale the feature matrix to get a normally distributed data. \n","fa3253d4":"Now we split out preprocessed feature matrix and target matrix into Training Data and Testing Data.\n\nAs the data set is unbalanced we need to assign class weights to it. This is done by taking the ratio of Churned Customers to the total number of customers.  \n\nNext, we build a 3-layer neural network. The input layer contains the same number of neurons as the number of columns in the feauture matrix. The output layer consits of a single layer which preditcs the output i.e 1 for churned customer and 0 for existing customer.\n\nThe neurons for hidden layer are usually a value between the number of neurons in the input layer and output layer. It is considered safe to take numebr of neurons in the hidden layer to be the mean of neurons in the input and output layer."}}