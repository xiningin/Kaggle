{"cell_type":{"9b50ac87":"code","34f28966":"code","f04b4ac6":"code","511ab7d0":"code","2b79df4c":"code","0919c0ed":"code","9e82499a":"code","454ec3ac":"code","2a053ef5":"code","fa8e5420":"code","41f142c0":"code","a8c67696":"code","9e916b55":"code","1ecc0927":"code","3f6e07e3":"code","2b3df6d5":"code","b1cceda8":"code","12aca0e2":"code","416abe4b":"code","44bde7ef":"code","a338ef0b":"code","dc286acb":"code","ceebc447":"code","e3ed3130":"code","32769f3d":"code","7c994cdb":"code","b3a5c3c6":"code","eb7c81b2":"code","47e1a5bc":"code","9f4e4788":"code","a0544acf":"code","093d8d25":"code","abbc115a":"code","824565c0":"code","c70839b3":"code","9b65817d":"code","dbcd5c60":"code","0e0dd060":"code","88c28ed9":"code","caad1289":"code","604667a1":"code","ca90a17f":"code","716ea9f4":"code","db895fd0":"code","5a219ac2":"code","beade351":"code","29931110":"code","24e01c3d":"code","16266364":"code","a68f737f":"code","1696885c":"code","b58cb36b":"code","51321483":"code","1aa30a5a":"code","f0442a3e":"code","f4eb4d23":"code","11b2148d":"code","882f523d":"code","a1ea0bb5":"code","dc5c8168":"code","4132c411":"code","af5f6aa1":"code","8ce3e515":"code","73c58a8e":"code","508627f0":"code","e6f462c2":"code","20351f8e":"code","8d60c683":"code","16bc606e":"code","d7d47048":"code","56aa73d0":"code","d1c58fa0":"code","1d388791":"code","292f47e4":"code","33eb4ece":"code","6df00505":"code","7930d4ae":"code","5b2240cc":"code","2837c7fb":"code","c60f65c0":"code","c01077e0":"code","d75005b1":"code","5417f50f":"code","081d8755":"code","4758be09":"code","4abfad8a":"code","e9e94066":"code","92122737":"code","a2f653ae":"code","64653796":"code","c1c27b37":"code","882d3a3d":"code","813427eb":"code","2b56e986":"code","f23c58e1":"code","d9768283":"code","c3cdc890":"code","65cb4bc9":"code","fb48e3b0":"code","65dda5af":"code","b436c1a2":"code","09e3c6cd":"code","181b05d6":"code","50fc09bb":"markdown","b36e5ff5":"markdown","9f9131fa":"markdown","669d0784":"markdown","ccd39e4e":"markdown","140e8c1a":"markdown","d5918be9":"markdown","dd1a10fd":"markdown","c95da1d1":"markdown","c10dc895":"markdown"},"source":{"9b50ac87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34f28966":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict, Counter\nimport scipy.stats as stats\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom time import sleep\nfrom tqdm.notebook import tqdm\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import one_hot\nfrom keras.layers import LSTM,Embedding,Dense,Bidirectional,GlobalMaxPool2D,BatchNormalization,Dropout,TimeDistributed,GlobalMaxPool1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential,Model\nfrom keras.layers import SpatialDropout1D,GRU\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom lightgbm import LGBMClassifier","f04b4ac6":"from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import plot_confusion_matrix, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedShuffleSplit\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, LabelEncoder, Binarizer, OneHotEncoder\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost\nfrom sklearn.linear_model import ElasticNet\nimport re\nimport sys\nfrom time import sleep\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nimport joblib\nimport json\nfrom sklearn.model_selection import ShuffleSplit\nfrom keras.callbacks import Callback,ModelCheckpoint\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport keras.backend as K\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nimport tensorflow as tf\nimport xgboost\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import ExtraTreesClassifier\n%matplotlib inline","511ab7d0":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","2b79df4c":"train.head()","0919c0ed":"test.head()","9e82499a":"train.dtypes","454ec3ac":"test.dtypes","2a053ef5":"print(\"Training Shape: {}\".format(train.shape))\nprint(\"Test Shape: {}\".format(test.shape))","fa8e5420":"train.isnull().sum(axis=0)","41f142c0":"train.isnull().sum(axis=1)[20:40]","a8c67696":"test.isnull().sum()","9e916b55":"train[train['location'].isnull() == False]['location'].value_counts()","1ecc0927":"test[test['location'].isnull() == False]['location'].value_counts()","3f6e07e3":"real_not_real = train['target'].value_counts()\nsns.barplot(real_not_real.index, real_not_real)","2b3df6d5":"train['text'].describe()","b1cceda8":"plt.figure(figsize=(20,6))\nplt.hist(train[train.target == 1]['text'].str.len(), bins = 150, label = \"Real\")\nplt.hist(train[train.target == 0]['text'].str.len(),bins = 150, label=\"Fake\")\nplt.legend()\nplt.show()","12aca0e2":"fig,(plt1,plt2) = plt.subplots(1,2, figsize = (10,5))\nreal = train[train['target'] == 1]['text'].str.len()\nfake = train[train['target'] == 0]['text'].str.len()\nplt1.hist(real, color=\"Red\")\nplt1.set_title(\"Real Tweets\")\nplt2.hist(fake,color=\"Green\")\nplt2.set_title(\"Fake Tweets\")\nplt.suptitle(\"Characters vs Target\")","416abe4b":"fig,(plt1,plt2) = plt.subplots(1,2, figsize = (10,5))\nreal = train[train['target'] == 1]['text'].str.split().map(lambda x: len(x))\nfake = train[train['target'] == 0]['text'].str.split().map(lambda x: len(x))\nplt1.hist(real, color=\"Red\")\nplt1.set_title(\"Real Tweets\")\nplt2.hist(fake,color=\"Green\")\nplt2.set_title(\"Fake Tweets\")\nplt.suptitle(\"Words vs Target\")","44bde7ef":"fig,(plt1,plt2) = plt.subplots(1,2, figsize = (20,6))\nreal = train[train['target'] == 1]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(real.map(lambda x: np.mean(x)), ax=plt1, color=\"Red\", label=\"Real\")\nfake = train[train['target'] == 0]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(real.map(lambda x: np.mean(x)), ax=plt2, color=\"Green\")\nplt1.set_title(\"Real Tweets\")\nplt2.set_title(\"Fake Tweets\")\nfig.suptitle(\"Average word length vs Title\")","a338ef0b":"plt.figure(figsize=(20,4))\nsns.heatmap(train.isnull())","dc286acb":"plt.figure(figsize=(20,4))\nsns.heatmap(test.isnull())","ceebc447":"train['keyword'].fillna('no_keyword')\ntrain['keyword_mean'] = train.groupby('keyword')['target'].transform('mean')\nfig = plt.figure(figsize = (20,50))\nsns.countplot(y = train.sort_values(by='keyword_mean', ascending = False)['keyword'], hue = train.sort_values(by = 'keyword_mean', ascending=False)['target'],palette=[\"green\",'red'],saturation = 1)\nplt.legend(loc = 1)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.title('Keywords vs Target')\nplt.tight_layout()\nplt.show()","e3ed3130":"train['word_count'] = train['text'].apply(lambda x: len(str(x).split()))\ntest['word_count'] = test['text'].apply(lambda x: len(str(x).split()))\n\ntrain['char_count'] = train['text'].apply(lambda x: len(str(x)))\ntest['char_count'] = test['text'].apply(lambda x: len(str(x)))\n\ntrain['stop_word_count'] = train['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords.words('english')]))\ntest['stop_word_count'] = test['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stopwords.words('english')]))\n\ntrain['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))\ntest['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n\ntrain['hashtag_count'] = train['text'].apply(lambda x: len([char for char in str(x) if char == '#']))\ntest['hashtag_count'] = test['text'].apply(lambda x: len([char for char in str(x) if char == '#']))\n\ntrain['mention_count'] = train['text'].apply(lambda x: len([char for char in str(x) if char == '@']))\ntest['mention_count'] = test['text'].apply(lambda x: len([char for char in str(x) if char == '@']))\n\ntrain['average_word_length'] = train['text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\ntest['average_word_length'] = test['text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))","32769f3d":"train.head()","7c994cdb":"test.head()","b3a5c3c6":"train.describe()","eb7c81b2":"test.describe()","47e1a5bc":"def diagnostic_plots(df, variable):\n    # function takes a dataframe (df) and\n    # the variable of interest as arguments\n \n    # define figure size\n    plt.figure(figsize=(16, 4))\n \n\n \n    # Q-Q plot\n    plt.subplot(1, 2, 1)\n    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n    plt.ylabel('Variable quantiles')\n \n    # boxplot\n    plt.subplot(1, 2, 2)\n    sns.boxplot(y=df[variable])\n    plt.title('Boxplot')\n \n    plt.show()","9f4e4788":"\nnum_var = [\"word_count\",\"char_count\",\"stop_word_count\",\"unique_word_count\",\"average_word_length\",\"hashtag_count\",\"mention_count\"]\nfor var in num_var:\n    print(\"******* {} *******\".format(var))\n    diagnostic_plots(train, var)","a0544acf":"num_var = [\"word_count\",\"char_count\",\"stop_word_count\",\"unique_word_count\",\"average_word_length\",\"hashtag_count\",\"mention_count\"]\nreal = train['target'] == 1\nfig, axes = plt.subplots(nrows=len(num_var), ncols = 2, figsize = (20,50))\nfor index, feature in enumerate(num_var):\n    sns.distplot(train.loc[real][feature], label = 'Real', ax = axes[index][0], color = 'red',kde_kws={'bw': .5})\n    sns.distplot(train.loc[~real][feature], label = 'Fake', ax = axes[index][0], color = 'green',kde_kws={'bw': .5})\n    sns.distplot(train[feature], label = 'Training', ax = axes[index][1], color = 'blue',kde_kws={'bw': .5})\n    sns.distplot(test[feature], label = 'Test', ax = axes[index][1], color = 'yellow',kde_kws={'bw': .5})\n    for j in range(2):\n        axes[index][j].set_label(\" \")\n        axes[index][j].legend()\n    axes[index][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[index][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n    \nplt.show()","093d8d25":"#coorelation heatmap\nplt.figure(figsize=(20,6))\nnum_vars = [\"word_count\",\"char_count\",\"stop_word_count\",\"unique_word_count\",\"average_word_length\",\"hashtag_count\",\"mention_count\",\"target\"]\nsns.heatmap(train[num_vars].corr(),cmap=\"coolwarm\", annot=True)","abbc115a":"stoplist = set(stopwords.words('english'))\nprint(len(stoplist))","824565c0":"stop_fake = defaultdict(int)\nfor words in train[train['target'] == 0]['text'].str.split():\n    for word in words:\n        if word in stoplist:\n            stop_fake[word] += 1\n","c70839b3":"stop_fake_15 = sorted(stop_fake.items(), key = lambda x: x[1], reverse = True)[:15]","9b65817d":"stop_real = defaultdict(int)\nfor words in train[train['target'] == 1]['text'].str.split():\n    for word in words:\n        if word in stoplist:\n            stop_real[word] += 1\nstop_real_15 = sorted(stop_real.items(), key = lambda x: x[1], reverse = True)[:15]","dbcd5c60":"fig,(plt1,plt2) = plt.subplots(1,2, figsize = (20,6))\nx_fake, y_fake = zip(*stop_fake_15)\nplt1.bar(x_fake,y_fake, color = \"green\")\nx_real, y_real = zip(*stop_real_15)\nplt2.bar(x_real,y_real, color = \"red\")\nplt1.set_title(\"Fake Tweets\")\nplt2.set_title(\"Real Tweets\")\nfig.suptitle(\"Stop Word Dist.\")","0e0dd060":"import string","88c28ed9":"punc_real = defaultdict(int)\nfor words in train[train['target'] == 1]['text'].str.split():\n    for word in words:\n        if word in string.punctuation:\n            punc_real[word] += 1","caad1289":"punc_real_15 = sorted(punc_real.items(), key = lambda x: x[1], reverse = True)","604667a1":"punc_fake = defaultdict(int)\nfor words in train[train['target'] == 0]['text'].str.split():\n    for word in words:\n        if word in string.punctuation:\n            punc_fake[word] += 1\npunc_fake_15 = sorted(punc_fake.items(), key = lambda x: x[1], reverse = True)","ca90a17f":"fig,(plt1,plt2) = plt.subplots(1,2, figsize = (20,6))\nx_fake, y_fake = zip(*punc_fake_15)\nplt1.bar(x_fake,y_fake, color = \"green\")\nx_real, y_real = zip(*punc_real_15)\nplt2.bar(x_real,y_real, color = \"red\")\nplt1.set_title(\"Fake Tweets\")\nplt2.set_title(\"Real Tweets\")\nfig.suptitle(\"Stop Word Dist.\")","716ea9f4":"df = pd.concat([train,test])\nprint(df.shape)","db895fd0":"\nfrom bs4 import BeautifulSoup\nimport re","5a219ac2":"cList = {\n  \"ain't\": \"am not\",\n  \"aren't\": \"are not\",\n  \"can't\": \"cannot\",\n  \"can't've\": \"cannot have\",\n  \"'cause\": \"because\",\n  \"could've\": \"could have\",\n  \"couldn't\": \"could not\",\n  \"couldn't've\": \"could not have\",\n  \"didn't\": \"did not\",\n  \"doesn't\": \"does not\",\n  \"don't\": \"do not\",\n  \"hadn't\": \"had not\",\n  \"hadn't've\": \"had not have\",\n  \"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\n  \"he'd\": \"he would\",\n  \"he'd've\": \"he would have\",\n  \"he'll\": \"he will\",\n  \"he'll've\": \"he will have\",\n  \"he's\": \"he is\",\n  \"how'd\": \"how did\",\n  \"how'd'y\": \"how do you\",\n  \"how'll\": \"how will\",\n  \"how's\": \"how is\",\n  \"I'd\": \"I would\",\n  \"I'd've\": \"I would have\",\n  \"I'll\": \"I will\",\n  \"I'll've\": \"I will have\",\n  \"I'm\": \"I am\",\n  \"I've\": \"I have\",\n  \"isn't\": \"is not\",\n  \"it'd\": \"it had\",\n  \"it'd've\": \"it would have\",\n  \"it'll\": \"it will\",\n  \"it'll've\": \"it will have\",\n  \"it's\": \"it is\",\n  \"let's\": \"let us\",\n  \"ma'am\": \"madam\",\n  \"mayn't\": \"may not\",\n  \"might've\": \"might have\",\n  \"mightn't\": \"might not\",\n  \"mightn't've\": \"might not have\",\n  \"must've\": \"must have\",\n  \"mustn't\": \"must not\",\n  \"mustn't've\": \"must not have\",\n  \"needn't\": \"need not\",\n  \"needn't've\": \"need not have\",\n  \"o'clock\": \"of the clock\",\n  \"oughtn't\": \"ought not\",\n  \"oughtn't've\": \"ought not have\",\n  \"shan't\": \"shall not\",\n  \"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\n  \"she'd\": \"she would\",\n  \"she'd've\": \"she would have\",\n  \"she'll\": \"she will\",\n  \"she'll've\": \"she will have\",\n  \"she's\": \"she is\",\n  \"should've\": \"should have\",\n  \"shouldn't\": \"should not\",\n  \"shouldn't've\": \"should not have\",\n  \"so've\": \"so have\",\n  \"so's\": \"so is\",\n  \"that'd\": \"that would\",\n  \"that'd've\": \"that would have\",\n  \"that's\": \"that is\",\n  \"there'd\": \"there had\",\n  \"there'd've\": \"there would have\",\n  \"there's\": \"there is\",\n  \"they'd\": \"they would\",\n  \"they'd've\": \"they would have\",\n  \"they'll\": \"they will\",\n  \"they'll've\": \"they will have\",\n  \"they're\": \"they are\",\n  \"they've\": \"they have\",\n  \"to've\": \"to have\",\n  \"wasn't\": \"was not\",\n  \"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\n  \"we'll\": \"we will\",\n  \"we'll've\": \"we will have\",\n  \"we're\": \"we are\",\n  \"we've\": \"we have\",\n  \"weren't\": \"were not\",\n  \"what'll\": \"what will\",\n  \"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\n  \"what's\": \"what is\",\n  \"what've\": \"what have\",\n  \"when's\": \"when is\",\n  \"when've\": \"when have\",\n  \"where'd\": \"where did\",\n  \"where's\": \"where is\",\n  \"where've\": \"where have\",\n  \"who'll\": \"who will\",\n  \"who'll've\": \"who will have\",\n  \"who's\": \"who is\",\n  \"who've\": \"who have\",\n  \"why's\": \"why is\",\n  \"why've\": \"why have\",\n  \"will've\": \"will have\",\n  \"won't\": \"will not\",\n  \"won't've\": \"will not have\",\n  \"would've\": \"would have\",\n  \"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\n  \"y'all\": \"you all\",\n  \"y'alls\": \"you alls\",\n  \"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\n  \"y'all're\": \"you all are\",\n  \"y'all've\": \"you all have\",\n  \"you'd\": \"you had\",\n  \"you'd've\": \"you would have\",\n  \"you'll\": \"you you will\",\n  \"you'll've\": \"you you will have\",\n  \"you're\": \"you are\",\n  \"you've\": \"you have\"\n}\n","beade351":"c_re = re.compile(r'\\b(?:%s)\\b' % '|'.join(cList.keys()))\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text.lower())\n","29931110":"print(expandContractions('Don\\'t you get it?'))\nprint(expandContractions('You\\'ve got serious cojones coming in here like that.'))\n","24e01c3d":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\nc_re = re.compile(r'\\b(?:%s)\\b' % '|'.join(cList.keys()))\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text.lower())\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef denoise_text(text):\n    text = expandContractions(text)\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_punct(text)\n    text = remove_emoji(text)\n    return text","16266364":"print(denoise_text('You\\'ve got [serious] cojones coming in here like that.<html> ##?@[]'))","a68f737f":"df = pd.concat([train,test])\nprint(\"Data Frame shape: {}\".format(df.shape))","1696885c":"df.isnull().sum()","b58cb36b":"df.drop(['keyword','location','keyword_mean'],axis = 1, inplace = True)\nprint(\"Data Frame shape: {}\".format(df.shape))","51321483":"df.drop('id',axis = 1, inplace = True)","1aa30a5a":"df.head()","f0442a3e":"lemmatizer = WordNetLemmatizer()\ndf['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word,'v')for word in x.split()]))\ndf['text'] = df['text'].apply(lambda x: denoise_text(x))","f4eb4d23":"df.head()","11b2148d":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","882f523d":"print('Loaded %s word vectors.' % len(embedding_dict))","a1ea0bb5":"corpus = []\nfor tweet in tqdm(df['text']):\n    words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stoplist))]\n    corpus.append(words)","dc5c8168":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","4132c411":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","af5f6aa1":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,200))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","8ce3e515":"train.shape, test.shape,df.shape","73c58a8e":"train_1=tweet_pad[:train.shape[0]]\ntest_1=tweet_pad[train.shape[0]:]","508627f0":"train.shape,test.shape","e6f462c2":"train_1_df = pd.DataFrame(train_1)\ntest_1_df = pd.DataFrame(test_1)","20351f8e":"train_1_df['word_count'] = train['word_count']\ntest_1_df['word_count'] = test['word_count']\n\ntrain_1_df['char_count'] = train['char_count']\ntest_1_df['char_count'] = test['char_count']\n\ntrain_1_df['stop_word_count'] = train['stop_word_count']\ntest_1_df['stop_word_count'] = test['stop_word_count']\n\ntrain_1_df['unique_word_count'] = train['unique_word_count']\ntest_1_df['unique_word_count'] = test['unique_word_count']\n\ntrain_1_df['hashtag_count'] = train['hashtag_count']\ntest_1_df['hashtag_count'] = test['hashtag_count']\n\ntrain_1_df['mention_count'] = train['mention_count']\ntest_1_df['mention_count'] = test['mention_count']\n\ntrain_1_df['average_word_length'] = train['average_word_length']\ntest_1_df['average_word_length'] = test['average_word_length']","8d60c683":"X_train,X_val,y_train,y_val=train_test_split(train_1_df,train['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_val.shape)","16bc606e":"def get_f1(y_true, y_pred): \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","d7d47048":"model_bdlstm=Sequential()\ne=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\nmodel_bdlstm.add(e)\nmodel_bdlstm.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\nmodel_bdlstm.add(BatchNormalization())\nmodel_bdlstm.add(Dense(1, activation = \"sigmoid\"))\nmodel_bdlstm.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=[get_f1])","56aa73d0":"model_bdlstm.summary()","d1c58fa0":"hist=model_bdlstm.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=20,batch_size=64,verbose=1)","1d388791":"plt.plot(hist.history['get_f1'], label='train f1_score')\nplt.plot(hist.history['val_get_f1'], label='validation f1_score')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","292f47e4":"y_pred_val = model_bdlstm.predict_classes(X_val)","33eb4ece":"accuracy_bilstm= accuracy_score(y_pred_val,y_val)","6df00505":"print(\"The accuracy for the Bidirectional Lstm model is {} %\".format(accuracy_bilstm*100))","7930d4ae":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ny_pred_bdlstm=model_bdlstm.predict_classes(test_1_df)\ny_pred_bdlstm=np.round(y_pred_bdlstm).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pred_bdlstm})\nsub.to_csv('submission_bdlstm.csv',index=False)","5b2240cc":"model_gru=Sequential()\ne=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\nmodel_gru.add(e)\nmodel_gru.add(SpatialDropout1D(0.3))\nmodel_gru.add(GRU(200))\nmodel_gru.add(Dense(1, activation = \"sigmoid\"))\nmodel_gru.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=[get_f1])","2837c7fb":"hist=model_gru.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=20,batch_size=64,verbose=1)","c60f65c0":"plt.plot(hist.history['get_f1'], label='train f1_score')\nplt.plot(hist.history['val_get_f1'], label='validation f1_score')\nplt.title('Training history')\nplt.ylabel('F1-score')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","c01077e0":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ny_pred_gru=model_gru.predict_classes(test_1_df)\ny_pred_gru=np.round(y_pred_gru).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pred_gru})\nsub.to_csv('submission_gru.csv',index=False)","d75005b1":"y_pred_val = model_gru.predict_classes(X_val)\naccuracy_gru= accuracy_score(y_pred_val,y_val)\nprint(\"The accuracy for the GRU model is {} %\".format(accuracy_gru*100))","5417f50f":"from keras.layers import Layer\nimport keras.backend as K\nclass attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","081d8755":"import keras\nmax_length = 57\ninputs=keras.Input(shape=(max_length,))\nx=(e)(inputs)\natt_in=LSTM(100,return_sequences=True,dropout=0.3,recurrent_dropout=0.2)(x)\natt_out=attention()(att_in)\noutputs=Dense(1,activation='sigmoid',trainable=True)(att_out)\nmodelA=Model(inputs,outputs)\nmodelA.summary()","4758be09":"modelA.compile(loss='binary_crossentropy', optimizer='adam', metrics=[get_f1])","4abfad8a":"hist=modelA.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=20,batch_size=64,verbose=1)","e9e94066":"y_pred_val = modelA.predict(X_val)","92122737":"for i in range(len(y_pred_val)):\n    if (y_pred_val[i]>=0.5):\n        y_pred_val[i]=1\n    else:\n        y_pred_val[i]=0","a2f653ae":"accuracy_attention = accuracy_score(y_pred_val,y_val)\nprint(\"The accuracy for the attention model is {} %\".format(accuracy_attention*100))","64653796":"plt.plot(hist.history['get_f1'], label='train f1_score')\nplt.plot(hist.history['val_get_f1'], label='validation f1_score')\nplt.title('Training history')\nplt.ylabel('F1-score')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","c1c27b37":"def find_best_model(X, Y, tqdm=tqdm):\n    \n    algos = {\n            'Logistic_Reg': {\n                'model': LogisticRegression(random_state=42),\n                'params':{\n                    'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100],\n                    'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n                }\n            },\n#             'XGB': {\n#                 'model': XGBClassifier(random_state=42),\n#                 'params': {\n#                    'n_estimators' :[25,50,100],\n#                    'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n#                    'gamma':[0.5, 0.1, 1, 10],\n#                    'max_depth':[5, 10, 15]\n#                 }\n#             },\n            'LGBM': {\n                'model': LGBMClassifier(random_state=42),\n                'params':{\n                    'max_depth': [7,8,9,10,11],\n                    'min_child_weight': [5,6,7,8],\n                    'learning_rate': [0.1,0.05,0.001],\n                    'n_estimators': [1500]\n                }\n            },\n            \n            'SVC': {\n                'model': SVC(random_state=42),\n                'params': {\n                    'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100]\n                }\n            },\n            'KNN' :{\n                'model': KNeighborsClassifier(),\n                'params': {\n                    'weights': ['uniform','distance']\n                }\n            },\n            \n          'Extra_Randomized_tree': {\n              'model': ExtraTreesClassifier(),\n              'params': {\n                  'n_estimators':[10,20, 50, 100],\n                  'max_depth':[4, 6, 10, 15, 20, 50]\n              }\n          }\n#           'RandomForest':{\n#               'model': RandomForestClassifier(),\n#                 'params': {\n#                   'criterion' : ['gini', 'entropy'],\n#                    'n_estimators': [100,300,500],\n#               }\n#           },\n#         'XGBoost':{\n#             'model': XGBClassifier(),\n#             'params': {\n#                 'max_depth': [7,8,9,10,11],\n#                 'min_child_weight': [5,6,7,8],\n#                 'learning_rate': [0.1,0.05,0.001],\n#                 'n_estimators': [1500]\n#             }\n#         }\n  \n        \n    }\n    values = (algos.items())\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n    print(f'Grid Search CV Initiated..' )    \n    with tqdm(total=len(values), file=sys.stdout) as pbar:\n        for algo_name, config in algos.items():\n            pbar.set_description('Processed')\n            gs =  GridSearchCV(config['model'], config['params'], cv=cv, scoring='f1',return_train_score=False)\n            gs.fit(X,Y)\n            scores.append({\n                'model': algo_name,\n                'best_score': gs.best_score_,\n                'best_params': gs.best_params_\n            })\n            pbar.update(1)\n            print(f'Grid search CV for {algo_name} done')\n        print(\"Grid Search CV completed!\")\n    return pd.DataFrame(scores,columns=['model','best_score','best_params'])","882d3a3d":"result = find_best_model(train_1_df,train['target'])","813427eb":"result","2b56e986":"a = np.array(result[result.model == 'LGBM']['best_params'])\na","f23c58e1":"model_log_reg = LogisticRegression(C = 0.0001, solver = 'liblinear')\nmodel_log_reg.fit(X_train,y_train)\ny_pred_log = model_log_reg.predict(X_val)\nprint(accuracy_score(y_pred_log,y_val)*100)","d9768283":"model_lgbm = LGBMClassifier(learning_rate=0.001, max_depth=7,min_child_weight=7,\n                 n_estimators=1500)\nmodel_lgbm.fit(X_train,y_train)\ny_pred_lgbm = model_lgbm.predict(X_val)\nprint(accuracy_score(y_pred_lgbm,y_val)*100)","c3cdc890":"model_ERT = ExtraTreesClassifier(max_depth=25,\n                                n_estimators = 1500)\nmodel_ERT.fit(X_train,y_train)\ny_pred_ERT = model_ERT.predict(X_val)\nprint(accuracy_score(y_pred_ERT,y_val)*100)","65cb4bc9":"model_xgb = XGBClassifier(max_depth=9,min_child_weight=9,\n                         learning_rate = 0.001,n_estimators=1500)\n\nmodel_xgb.fit(X_train,y_train)\ny_pred_xgb = model_xgb.predict(X_val)\nprint(accuracy_score(y_pred_xgb,y_val)*100)","fb48e3b0":"y_pred_attention = modelA.predict(test_1_df)\nfor i in range(len(y_pred_attention)):\n    if (y_pred_attention[i]>=0.5):\n        y_pred_attention[i]=1\n    else:\n        y_pred_attention[i]=0\ny_pred_bdlstm = model_bdlstm.predict_classes(test_1_df)\ny_pred_lgbm = model_lgbm.predict(test_1_df)\ny_pred_ert = model_ERT.predict(test_1_df)\ny_pred_xgb = model_xgb.predict(test_1_df)","65dda5af":"y_pred_model = np.array((y_pred_bdlstm)*0.5 + (0.5*(y_pred_attention)))\ny_pred_model","b436c1a2":"y_pred_final_df['num'] = [1 if num >= 0.5 else 0 for num in y_pred_final_df['num']]","09e3c6cd":"y_pred_final = np.array(y_pred_final_df['num'])","181b05d6":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n\ny_pred_model=np.round(y_pred_model).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pred_model})\nsub.to_csv('submission.csv',index=False)","50fc09bb":"Analyzing punctuation dist. in real and fake tweets","b36e5ff5":"Length of tweets vs Target","9f9131fa":"**Contractions**","669d0784":"**Feature Engineering**","ccd39e4e":"EDA","140e8c1a":"1. Similiar Dist.: mention_count, hashtag_count (for both [train,test] and [real,fake])\n2. Similiar Dist: all features in train and test sets\n3. Different Dist: word_count,char_count,stop_word_count,average_word_length, unique_word_count","d5918be9":"**Data Cleaning**","dd1a10fd":"* It seems that the location feature has been entered manually and not automatically generated ","c95da1d1":"Number of charatcers vs Target","c10dc895":"**Analysing keywords Dist.**\n\n***Mean Encodings***"}}