{"cell_type":{"8ae4923e":"code","96fea00b":"code","a21c209e":"code","db79e402":"code","ad48c417":"code","1435d801":"code","0a8156e3":"code","e4a7b64b":"code","2673539e":"code","e6de4aab":"code","992c5453":"code","743cfe99":"code","fcd00b56":"code","d7b71fcc":"code","ccaf6537":"code","a5419842":"code","952cee43":"code","eed1b879":"code","e732b127":"code","6a5a4684":"code","63ca3146":"code","dbb6791c":"code","344a85f3":"code","7cc2226b":"code","318074df":"code","5f22bccd":"code","314b61a8":"code","529f4319":"code","d4e4bfb9":"code","97d250dc":"code","815daa4d":"code","f60a0f90":"code","3f376398":"code","b63a2f28":"code","13153d7e":"code","c8c370f9":"code","c6158596":"code","e802a226":"code","4d663d75":"code","a984d757":"code","50c1e0ec":"code","a4767589":"code","688436c0":"code","7e9e906c":"code","66a32db5":"code","2f7875dc":"code","1374fe7e":"code","750ec7f3":"code","91e609a6":"code","0d49a604":"code","4fb0b3b2":"code","7c9baea1":"code","26918cc9":"code","3fbfb788":"code","547ef1d8":"code","67d7d651":"code","7582081d":"code","d9cf5e61":"code","6647d645":"code","9ec8bac8":"code","78b4c3bd":"code","8fdeb912":"markdown","4968452a":"markdown","bac79688":"markdown","96bef5d7":"markdown","b1824855":"markdown","0acc6538":"markdown","9e7142be":"markdown","ce069634":"markdown","88adb8ca":"markdown","017552af":"markdown","000a9334":"markdown","628e6a7f":"markdown","9ff43bc4":"markdown","438c141d":"markdown","a79bafa2":"markdown","29fb3056":"markdown","d4f8f9d4":"markdown","65be16c6":"markdown","7577b159":"markdown","4a00fb94":"markdown","054de0b5":"markdown","e8537774":"markdown","71a219e6":"markdown","917636ae":"markdown","a3485849":"markdown","ee7d7cc1":"markdown","af2b805d":"markdown","b6bdf0dd":"markdown","bde72b39":"markdown","af599b22":"markdown","44576cd3":"markdown","fcb5d9f9":"markdown","bcfb2507":"markdown","2045dfe3":"markdown","49a0afdc":"markdown","7e0ff07d":"markdown","b4703d73":"markdown","80810d95":"markdown","f8ac565f":"markdown","551a3bff":"markdown","1c840b8d":"markdown","a5c185d0":"markdown","9e9c430a":"markdown","6b1baadf":"markdown","d3f8979a":"markdown","ccaa527a":"markdown","5a8eacee":"markdown","7c8f3299":"markdown","bcff1322":"markdown","0867de5c":"markdown","f86a055b":"markdown","0c0623a7":"markdown","99a9db53":"markdown","405442a2":"markdown","95f6dd7a":"markdown","69e868f7":"markdown","eb7ea202":"markdown","e5bfa988":"markdown","dac5c5e5":"markdown","622a387b":"markdown","9c1ef642":"markdown","6d268874":"markdown","a779aa7f":"markdown","b1c9981c":"markdown","9624f344":"markdown","655ab153":"markdown","4eead0f6":"markdown"},"source":{"8ae4923e":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nfrom statistics import mean\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, SimpleRNN, TimeDistributed, ConvLSTM2D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.utils import plot_model","96fea00b":"train = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\n\nprint(train.shape)\nprint(test.shape)","a21c209e":"train.head()","db79e402":"test.head()","ad48c417":"train['comment_text'][0]","1435d801":"train['comment_text'][1689]","0a8156e3":"test['comment_text'][4]","e4a7b64b":"test['comment_text'][0]","2673539e":"train.isnull().any().sum()\n","e6de4aab":"test.isnull().any().sum()","992c5453":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_train = train[\"comment_text\"]\nlist_sentences_test = test[\"comment_text\"]","743cfe99":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","fcd00b56":"index = tokenizer.word_index\nprint(index['the'])\nprint(index['cat'])\nprint(index['cars'])\n","d7b71fcc":"len(list_tokenized_train[1])","ccaf6537":"len(list_tokenized_train[250])","a5419842":"totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\nplt.hist(totalNumWords,bins = np.arange(0,410,10))\nplt.axvline(x=200, color='gray')\nplt.show()","952cee43":"maxlen = 200\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","eed1b879":"print(X_t[0])","e732b127":"print(len(X_t[0]))","6a5a4684":"# For the Input \ninp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\nembed_size = 128\nx = Embedding(max_features, embed_size)(inp)\n\n# != layers \n\nx = Dense(6, activation=\"tanh\")(x)\nx = Dense(6, activation=\"tanh\")(x)\n\n\n# For the Output\nx = GlobalMaxPool1D()(x)","63ca3146":"model = Model(inputs=inp, outputs=x)\nmodel.compile(loss='mean_squared_error',\n                  optimizer='adam',\n                  metrics=['accuracy'])","dbb6791c":"plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, dpi=96)","344a85f3":"batch_size = 64\nepochs = 5\nhistory = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","7cc2226b":"history_dict = history.history\nhistory_dict.keys()\n\nacc_basic = history.history['accuracy']\nval_loss_basic = history.history['val_loss']\nloss_basic = history.history['loss']\nval_acc_basic = history.history['val_accuracy']\n\nepochs = range(1, len(acc_basic) + 1)\n\nplt.subplot(221)\nplt.plot(epochs, loss_basic, 'b', label='Training loss')\nplt.plot(epochs, val_loss_basic, 'g', label='Validation loss')\n\nplt.title('Training vs validation (loss)')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n\nplt.subplot(224)\n\nplt.plot(epochs, acc_basic, 'b', label='Training accuracy')\nplt.plot(epochs, val_acc_basic, 'red', label='Validation accuracy')\n\nplt.title('Training vs Validation (accuracy)')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","318074df":"score, test_acc_basic = model.evaluate(X_t,y,verbose =1)","5f22bccd":"print(test_acc_basic)","314b61a8":"y_pred = model.predict(X_te, verbose = 1)","529f4319":"submission = pd.DataFrame(columns=['id'] + list_classes)\nsubmission['id'] = test['id'].values \nsubmission[list_classes] = y_pred\nsubmission.to_csv(\".\/submission_basicmodel.csv\", index=False)","d4e4bfb9":"# For the Input \ninp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\nembed_size = 128\nx = Embedding(max_features, embed_size)(inp)\n\n# != layers \n\n\nx = SimpleRNN(6,return_sequences=True)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\n\n\n# For the Output\nx = GlobalMaxPool1D()(x)","97d250dc":"model_rnn = Model(inputs=inp, outputs=x)\nmodel_rnn.compile(loss='mean_squared_error',\n                  optimizer='adam',\n                  metrics=['accuracy'])","815daa4d":"plot_model(model_rnn, to_file='model_gru.png', show_shapes=True, show_layer_names=True, dpi=96)","f60a0f90":"batch_size = 64\nepochs = 5\nhistory = model_rnn.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","3f376398":"history_dict = history.history\nhistory_dict.keys()\n\nacc_rnn = history.history['accuracy']\nloss_rnn = history.history['loss']\nval_acc_rnn = history.history['val_accuracy']\nval_loss_rnn = history.history['val_loss']\n\nepochs = range(1, len(acc_rnn) + 1)\n\nplt.subplot(221)\nplt.plot(epochs, loss_rnn, 'b', label='Training loss')\nplt.plot(epochs, val_loss_rnn, 'g', label='Validation loss')\n\nplt.title('Training vs validation (loss)')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n\nplt.subplot(224)\n\nplt.plot(epochs, acc_rnn, 'b', label='Training accuracy')\nplt.plot(epochs, val_acc_rnn, 'red', label='Validation accuracy')\n\nplt.title('Training vs Validation (accuracy)')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","b63a2f28":"score, test_acc_rnn = model_rnn.evaluate(X_t,y,verbose =1)","13153d7e":"y_pred = model_rnn.predict(X_te, verbose = 1)","c8c370f9":"submission = pd.DataFrame(columns=['id'] + list_classes)\nsubmission['id'] = test['id'].values \nsubmission[list_classes] = y_pred\nsubmission.to_csv(\"\/kaggle\/working\/submission_model_RNN.csv\", index=False)","c6158596":"# For the Input \ninp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\nembed_size = 128\nx = Embedding(max_features, embed_size)(inp)\n\n# != layers \n\nx = LSTM(6, return_sequences=True)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\n\n\n# For the Output\nx = GlobalMaxPool1D()(x)","e802a226":"model_lstm = Model(inputs=inp, outputs=x)\nmodel_lstm.compile(loss='mean_squared_error',\n                  optimizer='adam',\n                  metrics=['accuracy'])","4d663d75":"plot_model(model_lstm, to_file='model_lstm.png', show_shapes=True, show_layer_names=True, dpi=96)","a984d757":"batch_size = 64\nepochs = 5\nhistory = model_lstm.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","50c1e0ec":"history_dict = history.history\nhistory_dict.keys()\n\nacc_lstm = history.history['accuracy']\nval_acc_lstm = history.history['val_accuracy']\nval_loss_lstm = history.history['val_loss']\nloss_lstm = history.history['loss']\n\nepochs = range(1, len(acc_lstm) + 1)\n\nplt.subplot(221)\nplt.plot(epochs, loss_lstm, 'b', label='Training loss')\nplt.plot(epochs, val_loss_lstm, 'g', label='Validation loss')\n\nplt.title('Training vs validation (loss)')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n\nplt.subplot(224)\n\nplt.plot(epochs, acc_lstm, 'b', label='Training accuracy')\nplt.plot(epochs, val_acc_lstm, 'red', label='Validation accuracy')\n\nplt.title('Training vs Validation (accuracy)')\nplt.xlabel('Accuracy')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","a4767589":"score, test_acc_lstm = model_lstm.evaluate(X_t,y,verbose =1)","688436c0":"test_acc_lstm","7e9e906c":"y_pred = model_lstm.predict(X_te, verbose = 1)","66a32db5":"submission = pd.DataFrame(columns=['id'] + list_classes)\nsubmission['id'] = test['id'].values \nsubmission[list_classes] = y_pred\nsubmission.to_csv(\".\/submission_model_LSTM.csv\", index=False)","2f7875dc":"# For the Input \ninp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\nembed_size = 128\nx = Embedding(max_features, embed_size)(inp)\n\n# != layers \n\nx = GRU(10,return_sequences=True, activation=\"sigmoid\")(x)\nx = Dense(6, activation=\"sigmoid\")(x)\n\n\n# For the Output\nx = GlobalMaxPool1D()(x)","1374fe7e":"model_gru = Model(inputs=inp, outputs=x)\nmodel_gru.compile(loss='mean_squared_error',\n                  optimizer='adam',\n                  metrics=['accuracy'])","750ec7f3":"plot_model(model_gru, to_file='model_gru.png', show_shapes=True, show_layer_names=True, dpi=96)","91e609a6":"batch_size = 64\nepochs = 5\nhistory = model_gru.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","0d49a604":"history_dict = history.history\nhistory_dict.keys()\n\nacc_gru = history.history['accuracy']\nloss_gru = history.history['loss']\nval_acc_gru = history.history['val_accuracy']\nval_loss_gru = history.history['val_loss']\n\nepochs = range(1, len(acc_gru) + 1)\n\nplt.subplot(221)\nplt.plot(epochs, loss_gru, 'b', label='Training loss')\nplt.plot(epochs, val_loss_gru, 'g', label='Validation loss')\n\nplt.title('Training vs validation (loss)')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n\nplt.subplot(224)\n\nplt.plot(epochs, acc_gru, 'b', label='Training accuracy')\nplt.plot(epochs, val_acc_gru, 'red', label='Validation accuracy')\n\nplt.title('Training vs Validation (accuracy)')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","4fb0b3b2":"score, test_acc_gru = model_gru.evaluate(X_t,y,verbose =1)","7c9baea1":"test_acc_gru","26918cc9":"y_pred = model_gru.predict(X_te, verbose = 1)","3fbfb788":"submission = pd.DataFrame(columns=['id'] + list_classes)\nsubmission['id'] = test['id'].values \nsubmission[list_classes] = y_pred\nsubmission.to_csv(\".\/submission_model_GRU.csv\", index=False)","547ef1d8":"comp = pd.DataFrame(columns = ['Models'] + ['Training Accuracy (avg)'] + ['Validation Accuracy (avg)'] + ['Test Accuracy'] + ['Time \/epochs (avg)']+ ['Kaggle Score'])\ncomp['Models'] = ['Baseline',\"LSTM\",'GRU','SimpleRNN']\ncomp['Training Accuracy (avg)'] = [mean(acc_basic),mean(acc_lstm),mean(acc_gru),mean(acc_rnn)]\ncomp['Validation Accuracy (avg)'] = [mean(val_acc_basic), mean(val_acc_lstm), mean(val_acc_gru), mean(val_acc_rnn)]\ncomp['Test Accuracy'] = [test_acc_basic, test_acc_lstm, test_acc_gru, test_acc_rnn]\ncomp['Time \/epochs (avg)'] = [\"54s\", \"208s\", \"246.2s\", \"151.4s\"]\ncomp['Kaggle Score'] = [\"0.89238\",\"0.92040\",\"0.95050\",\"0.91707\"]\ncomp.to_csv(\".\/Models_comp.csv\",index = False)","67d7d651":"displaycomp = pd.read_csv('\/kaggle\/working\/Models_comp.csv')\nprint(displaycomp)","7582081d":"predict = pd.read_csv('\/kaggle\/working\/submission_model_GRU.csv')\npredict_val = predict[list_classes].values","d9cf5e61":"test['comment_text'][4]","6647d645":"predict_val[4]","9ec8bac8":"test['comment_text'][0]","78b4c3bd":"predict_val[0]","8fdeb912":"[Here](https:\/\/en.wikipedia.org\/wiki\/Recurrent_neural_network#Long_short-term_memory) to have more explaination about the LSTM cell","4968452a":"[Here](https:\/\/en.wikipedia.org\/wiki\/Recurrent_neural_network#Gated_recurrent_unit) to have more explaination about the GRU cell\n","bac79688":"To choose between the two, we look at the Kaggle score. We notice that this one is 0.95050 for the GRU against 0.92040 for the LSTM.The GRU also got better performance to all the metrics, comparing to the LSTM. It tends to converge to the optimal solution.","96bef5d7":"*we check what we said about the difficulties to predict some comments:","b1824855":"For the prediction :","0acc6538":"# *Deep Learning Models :*","9e7142be":"We are lucky to work with a 'clean' database. We don't find null values. We won't check all the cases by hand, it would take too much time. So we prefer to use already available functions. ","ce069634":"When we've got the prediction, we need to put this value into a csv file for the submission in Kaggle. We use Pandas to creathe such a file \n","88adb8ca":"You can find more information on how to use this network layer at the following link: [SimpleRNN](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/SimpleRNN)","017552af":"We need to use the predic function proposed by keras. We make the prediction with the test value.","000a9334":"**dataset explanation :**\n\n\n> We retrieved some comments from the internet. We are going to use these comments in order to know their nature.\n> For this we will classify them in different categories: toxic, severe toxic, obscene, threat, insult, indentity hate.","628e6a7f":"*But some of them are also not so easy. *\n","9ff43bc4":"First we will use the simplest RNN of the Keras API: The simple RNN.\nThis is a fully connected RNN where the output of the previous time step must be sent to the next time step.\n","438c141d":"Here is a way to illustrate it (only the Enbedding part, the rest does not concern us) ","a79bafa2":"One LSTM cell :","29fb3056":"Next, we need to convert our text into number. A way to do this is to use the Tokenizer function in keras.\nThe *fit_on_texts* method will update the internal vocabulary based on the sentences, it will also create the index based on the frequency.\nThen, the *texts_to_sequences* method will transform each text into the corresponding numbers in the dictionary.","d4f8f9d4":"You can find more information on how to use this network layer at the following link: [LSTM](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/LSTM)","65be16c6":"![](https:\/\/www.researchgate.net\/profile\/Xingsheng_Yuan\/publication\/332810604\/figure\/fig2\/AS:754128875683841@1556809743129\/Simple-word-embedding-based-model-with-modified-hierarchical-pooling-strategy.png)","7577b159":"\n# *Preprocessing Tasks:*","4a00fb94":"![](https:\/\/miro.medium.com\/max\/552\/1*GSZ0ZQZPvcWmTVatAeOiIw.png)","054de0b5":"The first problem is the coronavirus. Indeed, the arrival of this virus has already changed the way the project has been carried out because of the introduction of online courses. As we are all erasmus students (two from Luxembourg and one from France), we all went home. So setting up slots to work on this project was a bit more difficult than it would have been without this situation with COVID.","e8537774":"In order to get a better idea of the performance of each model, we mainly look at the test accuracy, but also to the other metrics (Training accuracy, validation accuracy). We notice that the LSTM and the GRU have the two best results for all categories. ","71a219e6":"Here we use 3 states for each cell: The memory of the previous LSTM layer (on the top line), the result of the previous block (Ht-1) and finally the input layer (Xt). To have the result of all the states, mathematical calculations are made according to the diagram below.  ","917636ae":"# *Table Comparison of Models:*","a3485849":"Then, we separate our training list, on one side we have the comments(list_sentences_train) and on the other side we have the categories (y). We also retrieve the comments from the test list (list_sentences_test). ","ee7d7cc1":"X(t) is input, h(t) is output and A is the neural network which gains information from the previous step in a loop. The output of one neurons goes into the next one and forward the information.","af2b805d":"The second layer that we will use is the Long Short Time Memory (LSTM).","b6bdf0dd":"*** Baseline model performance (simple model) 0.91761 **","bde72b39":"You can find more information on how to use this network layer at the following link: [GRU](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/GRU)","af599b22":"*At first, we can think that this comment is easy to classify. Indeed, we find insults, which puts it in the class toxic or even insult. But one wonders if he can't get into identity_hate because of the last sentence. This comment is not easy to classify.*","44576cd3":"It allows to facilitate the learning by the network. ","fcb5d9f9":"# *Dataset Information :*","bcfb2507":"Why is this project usefull for the society ?\n>  There are some concrete application of this project in our society:\n\n> This could initially limit cases of cyber harassment. Indeed, we are in a world that is more and more virtualized. The virtual world plays an important role in our lives. \n> For example, access to social networks is easier, so many young people have the opportunity to create an account. Some children are victims of cyber harassment. In France, 22% of teenagers admit to having been a victim of cyber harassment on social networks in 2019.\n\n> An application of this project could make a selection of the messages a person can send. And thus limit any form of harm such as insults, for example.\n\n\nWhy is team members interested in this project ?\n> Lo\u00efc : I did tutoring at a high school a few years ago. I realized that a lot of people feel bad about cyber stalking, and they don't talk about it. This project can provide a solution so that there are less cases by being able to detect and delete messages of this type \n\n> Dusan : I chose this project because I think that a lot of racism begins at the Internet and there are a lot of toxic and racist comments.\n\n> Dejan : I\u2018m interested in this project becaus I hate the toxic comments like for example the racist comments and there are also a lot of fatal cases that I heard about.\n\n\n","2045dfe3":"Here are some examples of comments :","49a0afdc":"The last is called the Gated Recurrent Unit (GRU).This is a variant of the LSTM.","7e0ff07d":"We can see here wich number is linked with which text :","b4703d73":"# *Conclusion:*","80810d95":"*** Complex Models**","f8ac565f":"For our first model, we decided to put 2 layers : \nThe first with 20 neurons and the second with 6. \nOur last layer need to have 6 neurons because we've got 6 class to predict.","551a3bff":"We decided to do the test on 5 epochs. Doing it on more might be more interesting in terms of values. But it would take too much time. Indeed, for the longest model (GRU) which has an average of 246s \/ epochs. This would make on 50 epochs or 205 min total and for 100 epochs 410 min which is too much waiting time.  ","1c840b8d":"One GRU cell : ","a5c185d0":"\n**Let's have a look on some examples**\n","9e9c430a":"# *Introduction :*\n","6b1baadf":"First, we made a model with base layers. We will now use a more powerful type of neuron to deal with languages: Recurent Neural Networks ","d3f8979a":"We had already noticed that the average time by epochs and logic. Indeed, the time for the simplest model (baseline) and the one that takes the shortest time, on average 54s.The more complex the model, the longer the time increases: simpleRNN got an average time of 154.4s per epochs, 208s for the LSTM and finally 246.2s for the GRU.","ccaa527a":"Two people hadn't really practiced before the Python. So it was difficult to start working directly on the project. Luckily, attending Python classes in parallel to this course allowed a better understanding of this scripting language.   ","5a8eacee":"The first preprocessing step is to check if there are any NULL value. If there are, we need to fill this value with something.\n","7c8f3299":"We would say that, to answer the problem, we prefer to use our GRU model, in this case. ","bcff1322":"# ***Team Members :***\n* Dusan MAKSIMOVIC\n* Dejan LUTOVAC\n* Lo\u00efc CHAUDY","0867de5c":"As on all projects, we had to face several problems to complete this one :","f86a055b":"We use an embedding layer. Indeed, it allows to transform a word into a vector, which makes it more easily processed by the network. It also allows azussi to reduce the representation of words (compared to a vector model for example). \n","0c0623a7":"![](https:\/\/miro.medium.com\/max\/1400\/1*xTKE0g6XNMLM8IQ4aFdP0w.png)","99a9db53":"It also took a long time to understand the existing notebook before starting to work on this project. Indeed, we did not start this project from scratch, we started from an existing notebook. All the members of the group had never done neural networks before, so it would have been complicated to start from scratch without any basis. ","405442a2":"To create Kaggle submission :","95f6dd7a":"Therefore, we need to add padding. We decided to have a maximum length of 200. So we can see from the graph that we won't lose a lot of data. ","69e868f7":"It's a multiplication of the current input (Xt) and the previous output (Ht-1). The get the current output, the activation function is applied to the result of the multiplication of the other two values. ","eb7ea202":"But it's not finised, we still got one problem. Neural Network need to have the same type of input to work but the comments may not have the same amount of word. \nThis two comments didn't have the same length","e5bfa988":"![](https:\/\/miro.medium.com\/max\/415\/1*28XR1ajfW1WuTOkjpOc9xA.png)","dac5c5e5":"*Loading the data*","622a387b":"We now have our two list ready for the model : **X_t = train** & **X_te = test**","9c1ef642":"*Some of them are very short, you can't misinterpret them, there's nothing toxic. *","6d268874":"*Thanks to the keras API, there are already layers that are implemented in it. So we are going to use the 3 following layers: LSTM, GRU and SimpleRNN*","a779aa7f":"A GRU cell depends on only two parameters: the input value and the value of previous output blocks. \nThe interest of GRU compared to LSTM is the execution time which is faster since fewer parameters have to be calculated.","b1c9981c":"# *Difficulties encountered:*","9624f344":"One simpleRNN cell : ","655ab153":"![](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)","4eead0f6":"\n# ***Toxic Comment Classification :***"}}