{"cell_type":{"7eceaa42":"code","69866b56":"code","72dd1cc9":"code","9aa564c1":"code","13f502d8":"code","1ebafed6":"code","44c10723":"code","4d5c875c":"code","b511de84":"code","8b4b4a10":"code","79257dbd":"code","e64085bb":"code","19ac402a":"code","c46da466":"code","cfa8e6a1":"code","8d34c71f":"code","36ecaedd":"code","beeba681":"code","a6aac213":"code","84e22fcf":"code","529b4118":"markdown","ffca799f":"markdown","51c28627":"markdown","ac121d86":"markdown","5a548b98":"markdown","b54a5d6f":"markdown","8dbc82c1":"markdown","61be0d74":"markdown","01a94633":"markdown","6b4d3b76":"markdown","c3e2ae52":"markdown","a996cb51":"markdown","eb8dcd87":"markdown","84f1b028":"markdown","fe02ed8e":"markdown","2c980f30":"markdown","3423aa72":"markdown","f63ecadc":"markdown","c6f2537c":"markdown","84d5f771":"markdown","71c535be":"markdown","227e3fef":"markdown","50b30e72":"markdown","be74e27f":"markdown","324fae93":"markdown","96c9463b":"markdown","20efc360":"markdown"},"source":{"7eceaa42":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler # ver 19\n\nimport janestreet\nimport warnings\nwarnings.filterwarnings(action='ignore')","69866b56":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.patches as patches\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls\nfrom plotly.subplots import make_subplots\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom IPython.display import HTML, Image\npd.set_option('max_columns', 150)\n\nimport os,random, math, psutil, pickle    \nfrom sklearn.preprocessing import RobustScaler","72dd1cc9":"print(os.listdir(\"..\/input\/jane-street-market-prediction\/\"))\n","9aa564c1":"%%time\nroot = '..\/input\/jane-street-market-prediction\/'\n\ntrain = pd.read_csv(root+'train.csv')\nfeatures = pd.read_csv(root+'features.csv')\nexample_test = pd.read_csv(root+'example_test.csv')\nsample_prediction_df = pd.read_csv(root+'example_sample_submission.csv')","13f502d8":"print('Size of train data', train.shape)\nprint('Size of features data', features.shape)\nprint('Size of example_test data', example_test.shape)","1ebafed6":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","44c10723":"train = reduce_mem_usage(train)\nfeatures = reduce_mem_usage(features)\nexample_test = reduce_mem_usage(example_test)","4d5c875c":"train['resp'] = (((train['resp'].values)*train['weight']) > 0).astype(int)\ntrain['resp_1'] = (((train['resp_1'].values)*train['weight']) > 0).astype(int)\ntrain['resp_2'] = (((train['resp_2'].values)*train['weight']) > 0).astype(int)\ntrain['resp_3'] = (((train['resp_3'].values)*train['weight']) > 0).astype(int)\ntrain['resp_4'] = (((train['resp_4'].values)*train['weight']) > 0).astype(int)\n","b511de84":"train.head()","8b4b4a10":"features.head()","79257dbd":"example_test.head()","e64085bb":"total = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending = False)\nmissing__train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing__train_data.head(10)","19ac402a":"total = features.isnull().sum().sort_values(ascending = False)\npercent = (features.isnull().sum()\/features.isnull().count()*100).sort_values(ascending = False)\nmissing__train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing__train_data.head(10)","c46da466":"# Number of each type of column\ntrain.dtypes.value_counts()","cfa8e6a1":"# Number of each type of column\nfeatures.dtypes.value_counts()","8d34c71f":"traces = [\n    go.Histogram(\n        x = train[train.columns[1:7][i]].value_counts().index, \n        name=train.columns[1:7][i]\n    ) for i in range(len(train.columns[1:7]))\n]\n\nfig = make_subplots(rows=2, cols=3,subplot_titles=(train.columns))\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 3) + 1, \n        (i % 3) + 1\n    )\n\nfig.update_layout(\n    template=\"plotly_white\"\n)\n\nfig.show()","36ecaedd":"features = ['feature_{}'.format(i) for i in range(0,130)]\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp']\ntrain_df = train[train['date']>85]\ntrain_data = train_df[features]\ntrain_target = np.stack([(train_df[c] > 0).astype('int') for c in resp_cols]).T\n\nrb = RobustScaler().fit(train_data)\ntrain_data = pd.DataFrame(rb.transform(train_data), columns=train_data.columns)","beeba681":"lgb_models = []\nlgb_params = {\n    'n_jobs':-1,\n    'num_leaves':400,\n    'learning_rate':0.15,\n    'n_estimators':1000,\n    'objective':'binary',\n    'subsample':0.75,\n    'colsample_bytree':0.5,\n    'metric':'auc',\n    'max_bin':500\n}\n\nfor i in range(train_target.shape[1]):\n    x_tr,x_val,y_tr,y_val = train_test_split(train_data ,train_target[:,i],test_size=0.2, stratify=train_target[:,i], random_state=i)\n    lgb_clf = LGBMClassifier(**lgb_params)\n    lgb_clf.fit(x_tr, y_tr, eval_set=[(x_tr, y_tr),(x_val,y_val)], eval_metric='auc', early_stopping_rounds=100, verbose=50)\n    lgb_models.append(lgb_clf)\nprint('Average CV score:',np.mean([model.best_score_['valid_1']['auc'] for model in lgb_models]))    ","a6aac213":"th = 0.5\nenv = janestreet.make_env()","84e22fcf":"for (test_df, pred_df) in env.iter_test():\n    if test_df['weight'].item() > 0:\n        x_tt = test_df[features]        \n        pred = np.median([model.predict_proba(x_tt)[:,1] for model in lgb_models]).T\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","529b4118":"Files\n- train.csv - the training set, contains historical data and returns\n- example_test.csv - a mock test set which represents the structure of the unseen test set. You will not be directly using the test set or sample submission in this competition, as the time-series API will get\/set the test set and predictions.\n- example_sample_submission.csv - a mock sample submission file in the correct format\n- features.csv - metadata pertaining to the anonymized features","ffca799f":"\n## Market Prediction -Starter Data Exploration \ud83d\udcc8\n\n- <a href='#1'>1. Market Prediction -Starter Data Exploration \ud83d\udcc8<\/a>\n    - <a href='#1-1'>1.1. Data Description<\/a>\n    - <a href='#1-2'>1.2. Evaluation Metric<\/a>\n- <a href='#2'>2. Imports<\/a>\n- <a href='#3'>3. Read in Data<\/a>\n- <a href='#4'>4. Glimpse of Data<\/a>\n- <a href='#5'>5. Reducing Memory Size<\/a>\n- <a href='#6'>6. Exploratory Data Analysis<\/a>\n    - <a href='#6-1'>6.1. Examine the Distribution of the Target Column<\/a>\n    - <a href='#6-2'>6.2. Examine Missing Values<\/a>\n    - <a href='#6-3'>6.3. Column Types<\/a>\n- <a href='#7'>7. Ploting<\/a>\n\n\n\n\n    ","51c28627":"\n## <a id='1-1'>1.2 Evaluation Metric<\/a>","ac121d86":"\n\n<html>\n<body>\n<p><font size=\"5\" color=\"Red\">\ud83d\udd13MEMORY USAGE AFTER COMPLETION:<\/font><\/p>\n<p>Mem. usage decreased to  : <b> 631.49 Mb (74.9% reduction)<\/b><\/p>\n<p>Mem. usage decreased to  : <b>  0.00 Mb (0.0% reduction)<\/b><\/p>\n<p>Mem. usage decreased to  : <b> 3.83 Mb (75.2% reduction)<\/b><\/p>\n\n<\/body>\n<\/html>\n\n\n\n","5a548b98":"## COMPREHENSIVE DATA EXPLORATION WITH PYTHON\n[Crisl\u00e2nio Mac\u00eado](https:\/\/medium.com\/sapere-aude-tech) -  Feb, 21th, 2021\n\n----------","b54a5d6f":"features","8dbc82c1":"# <a id='3'>3. Read in Data <\/a>\n<a href='#1'>Top<\/a>","61be0d74":"\n## <a id='1-1'>1.1 Data<\/a>\n","01a94633":"# End Notebook","6b4d3b76":"\n\n> About the Host\n\nJane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world. Admittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there\u2019s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge.","c3e2ae52":"example_test","a996cb51":"### checking missing data for feature","eb8dcd87":"train data","84f1b028":"### Submission","fe02ed8e":"# <a id='8'>8. Modeling<\/a>\n<a href='#1'>Top<\/a>","2c980f30":"### checking missing data for train","3423aa72":"# <a id='2'>2. Imports <\/a>\n<a href='#1'>Top<\/a>\n\n> We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. ","f63ecadc":"# <a id='5'>5. Reducing Memory Size<\/a>\n<a href='#1'>Top<\/a>","c6f2537c":"# <a id='4'>4. Glimpse of Data<\/a>\n<a href='#1'>Top<\/a>","84d5f771":"## <a id='6-3'>6.3 Column Types<\/a>\n\n\nLet's look at the number of columns of each data type. `int64` and `float64` are numeric variables ([which can be either discrete or continuous](https:\/\/stats.stackexchange.com\/questions\/206\/what-is-the-difference-between-discrete-data-and-continuous-data)). `object` columns contain strings and are  [categorical features.](http:\/\/support.minitab.com\/en-us\/minitab-express\/1\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/basics\/what-are-categorical-discrete-and-continuous-variables\/) . ","71c535be":"# <a id='7'>7. Ploting<\/a>\n<a href='#1'>Top<\/a>","227e3fef":"\n<p><font size=\"3\" color=\"blue\" style=\"Comic Sans MS;\">\nReducing memory\n<\/font><\/p>","50b30e72":"# <a id='6'>6. Exploratory Data Analysis<\/a>\n<a href='#1'>Top<\/a>\n\n\n<p><font size=\"3\" color=\"blue\" style=\"Comic Sans MS;\">\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. \n<\/font><\/p>","be74e27f":"## <a id='6-2'>6.3 Examine Missing Values<\/a>\n\n\n<p><font size=\"3\" color=\"blue\" style=\"Comic Sans MS;\">\nNext we can look at the number and percentage of missing values in each column. \n\n<\/font><\/p>\n\n","324fae93":"\n<p><font size=\"3\" color=\"blue\" style=\"Comic Sans MS;\">\nIt is necessary that after using this code, carefully check the output results for each column.\n<\/font><\/p>","96c9463b":"# <a id='1'>1. Introduction: Jane Street Market Prediction<\/a>","20efc360":"This competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it. Each trade j has an associated weight and resp, which represents a return.\n\nFor each date i, we define:\n$pi=\u2211j(weightij\u2217respij\u2217actionij)$,\n\n$t=\u2211pi\u2211p2i\u2212\u2212\u2212\u2212\u221a\u2217250|i|$, \n\nwhere |i| is the number of unique dates in the test set. The utility is then defined as:\n\n$u=min(max(t,0),6)\u2211pi$."}}