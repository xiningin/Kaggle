{"cell_type":{"934d29f5":"code","d7f5d4f3":"code","48dd213e":"code","7077dd59":"code","60be78e2":"code","c72a6d96":"code","f0ebe26d":"code","3702d594":"code","6635b1f6":"code","cd7127c8":"code","2b6f1acb":"code","624d1eef":"code","d883374d":"code","f8b94f39":"code","aca9b54a":"code","0a6e5562":"code","c505b91f":"code","641a2fe1":"code","34cd9493":"code","d56a3d60":"code","06f69dab":"code","2dce056c":"code","62a0f3e1":"code","3706c888":"code","45d77ab3":"code","c6868d3f":"code","51df23c8":"code","ce19cc51":"code","6a70af17":"code","80abd553":"code","df459edb":"code","178f070c":"code","12cc6852":"code","e5ea8b99":"code","8303143a":"code","e2c95ca1":"code","a1704e3e":"code","81347868":"code","01fcb5e0":"code","b11da05e":"code","d395053c":"code","c6e0ca59":"code","b084d990":"code","41a54ef4":"code","fa864385":"code","5e5574f4":"code","c5dd6ad4":"code","f7a80b96":"code","11af2030":"code","0fa85938":"code","25545463":"code","68755bf7":"code","4fc974e0":"code","512c5489":"code","8c8bd3a2":"markdown","457f1226":"markdown","4eb22d44":"markdown","19bf7931":"markdown","90c17094":"markdown","1ed7b369":"markdown","23bfcb5c":"markdown","00e0d71a":"markdown","7210151f":"markdown","aa941a7b":"markdown","194c527c":"markdown","b023c20c":"markdown","cbe9155d":"markdown","41abb77f":"markdown","e9191daa":"markdown","19252189":"markdown","f1145335":"markdown","443ab075":"markdown","2ed92591":"markdown","6579a924":"markdown"},"source":{"934d29f5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d7f5d4f3":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"]=(20,10)","48dd213e":"df1=pd.read_csv(\"\/kaggle\/input\/bengaluru-house-price-data\/Bengaluru_House_Data.csv\")\ndf1.head()","7077dd59":"#LOOKING AT NUMBER OF ROWS AND COLUMNS\ndf1.shape\n# Thus 13320=rows   9=columns\n#This is an important step since we may have to remove certain data(rows or columns) due to many nan(absent)values thus to have an overlook at the size of data in the beginning is a smart step to take.","60be78e2":"df1.columns\n","c72a6d96":"# Dont get confused this line shows how the column \"area_type\" has different categorical entries and there number of counts.\ndf1.groupby(\"area_type\")[\"area_type\"].agg(\"count\")","f0ebe26d":"# Do not confuse area_type with the space of house area_type is just the description of space of the house which can be infered by total_sqft\n# Society does not matter since location is given it just makes the data set more complicated to deal with\n\ndf2=df1.drop([\"area_type\",\"society\",\"balcony\",\"availability\"],axis=\"columns\")\ndf2.head()\n# We also create a new data frame in case we get a abrupt accuracy.Also this is a good habit incase you want to compare accuracies in the end.","3702d594":"#This code here tells us the number of missing enteris in each column\ndf2.isnull().sum()\n","6635b1f6":"#DROPPING ALL NA VALUES\n#Again a new data frame after some major changes in previous data frame.\ndf3=df2.dropna()\ndf3.isnull().sum()","cd7127c8":"df3[\"size\"].unique()","2b6f1acb":"df3[\"bhk\"]=df3[\"size\"].apply(lambda x:int(x.split(\" \")[0]))\ndf3.head()\n#The output here is trying to tell us to use .loc instrad of our code but it is fine since the end results are same.\n# We use lambda function(google it highly important)this functions are one liner function that have one specific task.","624d1eef":"#NEXT WE SEARCH FOR ANY OUTLIERS.\ndf3[\"bhk\"].unique()\n#we see 43 and 24 bhk which may be wrong","d883374d":"#PRINTING DATA WHICH HAS BHK MORE THAN 20\ndf3[df3.bhk>20]\n#WE CAN SEE IT IS AN ERROR SINCE SQFT IS VERY LESS AND BHK IS VERY HIGH","f8b94f39":"df3[\"total_sqft\"].unique()","aca9b54a":"#WE WOULD DROP ROWS WITH SQ METER AND PERCH\n#FOR DATA IN RANGE WE WOULD REPLACE IT WITH AVERAGE.\n#USING THIS FUNCTION WE ARE CREATING A CLEANER VERSION OF total_sqft\n\ndef convert_sqft_to_num(x):\n  tokens=x.split(\"-\")\n  if len(tokens)==2:\n    return (float(tokens[0])+float(tokens[1]))\/2\n  try:\n    return float(x)\n  except:\n    return None ","0a6e5562":"#CREATING COPY AND CONTINUING:\n\ndf4=df3.copy()\ndf4[\"total_sqft\"]=df4[\"total_sqft\"].apply(convert_sqft_to_num)","c505b91f":"df5=df4.copy()\ndf5[\"price_per_sqft\"]=df5[\"price\"]*1000000\/df5[\"total_sqft\"]\ndf5.head()","641a2fe1":"len(df5.location.unique())\n#These are the unique(caterogircally different values) in location before our cleaning","34cd9493":"#Stripping any whitespaces\ndf5.location.apply(lambda x:x.strip())","d56a3d60":"#Basic data observation\nlocation_stats=df5.groupby(\"location\")[\"location\"].agg(\"count\").sort_values(ascending=False)\nprint(location_stats)","06f69dab":"len(location_stats[location_stats<=10])\n# THEREFORE WE SEE THAT OUT OF 1304 ROWS ALMOST 1063 ROWS HAVE LESS TAHN 10 REPETATION SO RATHER TAHN LOSING THIS DATA WE PLACE THEM UNDER \"OTHER\"","2dce056c":"#A new data frame\nlocation_stats_less_than_10=location_stats[location_stats<=10]\nlocation_stats_less_than_10","62a0f3e1":"\"\"\"using lambda func we place these 1063 in a location=\"other\"\"\"\ndf5.location=df5.location.apply(lambda x:\"other\" if x in location_stats_less_than_10 else x)\nlen(df5.location.unique())","3706c888":"df5.head(10)","45d77ab3":"\"\"\" IN THIS DATA SET WE HAVE (BHK AND TOTAL_SQFT) THEREFORE WE CAN SEE ANY OUTLIERS SINCE BOTH THESE VALUES ARE CONNECTED\"\"\"\n\"\"\" WE DIVIDE TOTAL_SQFT BY BHK IF THE ANSWER IS UNUSUALLY SMALL(<300) THEN ITS AN OUTLIER\"\"\"\n# FOR EG------TOTAL_SQFT = 1000 AND BHK = 6  (1000\/6<300) THEREFORE AN OUTLIER SINCE A SINGLE BEDROOM TAKES ABOUT 300 SQFT MINIMUM\ndf5[df5.total_sqft\/df5.bhk<300].head() #this would give us first five outliers","c6868d3f":"df5.shape","51df23c8":"#REMOVING OUTLIERS\ndf6=df5[~(df5.total_sqft\/df5.bhk<300)]\ndf6.shape","ce19cc51":"#FINDING MORE OUTLIERS\ndf6.price_per_sqft.describe()\n#MIN is very less (e+03) ","6a70af17":"#FUNCTION TO REMOVE OUTLIERS(ANY VALUEOF DATA IN RANGE OF MEAN-STANDARD DEVIATION AND MEAN+STANDARD DEVIATION IS OBMITTED)\n#(M-ST<VALUE<=M+ST) NICE LOGIC\ndef remove_pps_outliers(df):\n  df_out=pd.DataFrame()\n  for key,subdf in df.groupby(\"location\"):\n    m=np.mean(subdf.price_per_sqft)\n    st=np.std(subdf.price_per_sqft)\n    reduced_df=subdf[(subdf.price_per_sqft>(m-st))&(subdf.price_per_sqft<=(m+st))]\n    df_out=pd.concat([df_out,reduced_df],ignore_index=True)\n  return df_out\n","80abd553":"df7=remove_pps_outliers(df6)\ndf7.shape\n#therefor we removed approx 2000 outliers\n#For any data scientist it is very important to remove outliers and clean the data.\n#Getting a good accuracy becomes relatively easier whe your data is clean and easy to read.","df459edb":"#NOW CHECKING IF 2BHK AND 3BHK HAVE SAME PRICE IN SIMILAR LOCATION OF SAME TOTAL_SQFT\n# WE PLOT A SCATTER PLOT\n#THE PLOT WOULD TAKE A SPECIFIC LOCATION AND DATAFRAME AS INPUT\n#IT WOULD COMPARE 2BHK AND 3BHK PRICE WITH SQFT AS A PARAMETER\ndef plot_scatter_chart(df,location):\n  bhk2=df[(df.location==location)&(df.bhk==2)]\n  bhk3=df[(df.location==location)&(df.bhk==3)]\n\n  #matplotlib.reParams[\"figure.figsize\"]=(15,10)\n  plt.scatter(bhk2.total_sqft,bhk2.price,color=\"blue\",label=\"2 BHK\",s=50)\n  plt.scatter(bhk3.total_sqft,bhk3.price,marker=\"+\",color=\"green\",label=\"3 BHK\",s=50)\n  plt.xlabel(\"TOTAL_SQFT_AREA\")\n  plt.ylabel(\"PRICE\")\n  plt.title(\"LOCATION\")","178f070c":"plot_scatter_chart(df7,\"Hebbal\")\n# WE CAN SEE WHEN SAME SQFT 2BHK IS OF HIGHER PRICE THAN 3BHK THUS THIS COULD CREATE PRBEL IN PREDICTION SO REMOVE ALL 3BHK WITH SAME SQFT AS OF 2BHK AND LESS PRICE","12cc6852":"plot_scatter_chart(df7,\"Rajaji Nagar\")","e5ea8b99":"def remove_bhk_outliers(df):\n  exclude_indices=np.array([])\n  for location,location_df in df.groupby(\"location\"):\n    bhk_stats={}\n    for bhk,bhk_df in location_df.groupby(\"bhk\"):\n      bhk_stats[bhk]={\n          \"mean\":np.mean(bhk_df.price_per_sqft),\n          \"std\":np.std(bhk_df.price_per_sqft),\n          \"count\":bhk_df.shape[0]\n      }\n    for bhk,bhk_df in location_df.groupby(\"bhk\"):\n      stats=bhk_stats.get(bhk-1)\n      if stats and stats[\"count\"]>5:\n        exclude_indices=np.append(exclude_indices,bhk_df[bhk_df.price_per_sqft<(stats[\"mean\"])].index.values)\n  return df.drop(exclude_indices,axis=\"index\")\n# You may find this a little difficult but give it a 5 min read.","8303143a":"df8=remove_bhk_outliers(df7)\ndf8.shape","e2c95ca1":"plot_scatter_chart(df8,\"Hebbal\")\n# NOW ALMOST ALL COINCIDING POINTS ARE REMOVED","a1704e3e":"import matplotlib\nplt.hist(df8.price_per_sqft,rwidth=0.8)\nplt.xlabel(\"PRICE_PER_SQFT\")\nplt.ylabel(\"COUNT\")","81347868":"#WE SEE MAX DATA POINTS LIE FROM 0 TO 10,000 SQFT\n#EXPLAINING BATHROOM FEATURES\ndf8.bath.unique()","01fcb5e0":"df8[df8.bath>10]","b11da05e":"#WE NOTICE SOME DATA POINTS HAVE: NUMBER OF BATHROOMS> NUMBER OF BHK\n#HISTOGRAM:\nplt.hist(df8.bath,rwidth=0.8)\nplt.xlabel(\"NO.OF BATHROOMS\")\nplt.ylabel(\"COUNT\")","d395053c":"#MAXIMUM VALUE ARE BTW (2 TO 4)\ndf8[df8.bath>df8.bhk+2]\n#ALL ARE OUTLIERS","c6e0ca59":"df9=df8[df8.bath<df8.bhk+2]\ndf9.shape","b084d990":"# DROPPING SOME FEATURES WHICH ARE UNNECESSARY\ndf10=df9.drop([\"size\",\"price_per_sqft\"],axis=\"columns\")\ndf10.head()","41a54ef4":"# NOW ALL OUR COLUMNS ARE NUMERICAL EXCEPT FOR \"LOCATION\". THEREFORE WE WOULD USE ONE HOT ENCODING OR DUMMIES TO CONVERT CATEGORICAL TO NUMERICAL\ndummies=pd.get_dummies(df10.location)\ndummies.head()\n# Always do this for categorical data\n# Here since the columns is not a level catergorical column(high low medium...) we use one hot encoding","fa864385":"# Appending int dataframe:\ndf11=pd.concat([df10,dummies.drop(\"other\",axis=\"columns\")],axis=\"columns\")\ndf11.head()","5e5574f4":"#WHILE CONCATING WE USUALLY DROP A COLUMN HENCE WE DROPPED \"OTHER\" FROM DUMMIES DATAFRAME\n#DROPPING OCATION COLUMN\ndf12=df11.drop(\"location\",axis=\"columns\")\ndf12.head()","c5dd6ad4":"#Observe how the number of rows remained same just the columns increased.\ndf12.shape","f7a80b96":"x=df12.drop(\"price\",axis=\"columns\")\nx.head()","11af2030":"y=df12.price\ny.head()","0fa85938":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=10)","25545463":"from sklearn.linear_model import LinearRegression\nlr_clf=LinearRegression()\nlr_clf.fit(x_train,y_train)\nlr_clf.score(x_test,y_test)","68755bf7":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\ncv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\ncross_val_score(LinearRegression(),x,y,cv=cv)","4fc974e0":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Lasso\n\ndef find_best_model_using_gridsearchcv(x,y):\n  algos={\n      \"linear_regressor\":{\n          \"model\":LinearRegression(),\n          \"parms\":{\n              \"normalize\":[True,False]\n           }\n       },\n      \"lasso\":{\n          \"model\":Lasso(),\n          \"parms\":{\n              \"alpha\":[1,2],\n              \"selection\":[\"random\",\"cyclic\"]\n           }\n       },          \n      \"decision_tree\":{\n          \"model\":DecisionTreeRegressor(),\n          \"parms\":{\n              \"criterion\":[\"mse\",\"freidman_mse\"],\n              \"splitter\":[\"best\",\"random\"]\n           }\n       }\n   }\n  scores=[]\n  cv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n  for algo_name,config in algos.items():\n    gs=GridSearchCV(config[\"model\"],config[\"parms\"],cv=cv,return_train_score=False)\n    gs.fit(x,y)\n    scores.append({\n        \"model\":algo_name,\n        \"best_score\":gs.best_score_,\n        \"best_parms\":gs.best_params_\n    })\n  return pd.DataFrame(scores,columns=[\"model\",\"best_score\",\"best_parms\"])\n\nfind_best_model_using_gridsearchcv(x,y)","512c5489":"#WE SEE LINEAR REGRESSION IS THE BEST WITH NORMALIZE=FALSE\ndef predict_price(location,sqft,bath,bhk):\n  loc_index=np.where(x.columns==location)[0][0]\n  z=np.zeros(len(x.columns))\n  z[0]=sqft\n  z[1]=bath\n  z[2]=bhk\n  if loc_index>=0:\n    z[loc_index]=1\n  return lr_clf.predict([z])[0]\npredict_price(\"1st Phase JP Nagar\",1000,2,2)","8c8bd3a2":"**Creating price_per_sqft**\n> Later this would help remove the outliers (we saw two in outliers in bhk(24,42)).","457f1226":"> BY BASIC OBSERVATION WE CAN CONCLUDE THAT AREA_TYPE DOES NOT AFFECT THE PRICE SINCE THE TOTAL SQFT IS GIVEN.\n\n > ALSO SOCIETY,BALCONY,AVAILABILITY DOES NOT AFFECT","4eb22d44":"*WE WILL REMOVE ALL 3BHK WITH SAME LOCATION AS 2BHK and at a lesser price.*\n\n** We will do the same with any 2bhk flat with less price than 1bhk if in sam elocation and same sqft.**","19bf7931":"**Hence a high accuracy model was build with basic data cleaning steps and machine learning algorithms.**","90c17094":"**WE SEE DATA IS ENTERED IN BHK OR BEDROOM FORM, BOTH OF THEM MEAN THE SAME SO WE WOULD PICK JUST THE NUMBER FROM THIS COLUMN AND PUT IT IN NEW COLUMN FOR EASY PREDICTION.**","1ed7b369":"**OUTLIERS**","23bfcb5c":"GRID SEARCH CV(ALL IN ONE FUNCTION)","00e0d71a":"**LIBRARIES**","7210151f":"**Size column**","aa941a7b":"*** TEST TRAIN AND SPLIT***","194c527c":"K FOLD CROSS VALIDATION","b023c20c":"PARAMETER SELECTION FROM ABOVE","cbe9155d":"**Reading and importing the file**","41abb77f":"HERE WE CAN SEE TOTAL MISSING VALUES ARE 1+16+73(considering the worst case in which every missing value is in different row) WHICH IS STILL A NEGLIGIBLE LOSS OF DATA THEREFORE WE TOTALLY DROP THESE ROWS INSTEAD OF FILLING THEM.\n> We wouldnt have taken this step in case the loss of data was too much since it would have affected our accuracy.","e9191daa":"**THERE MAY BE OUTLIERS IN SOME LOCATIONS, LIKE AN AREA WITH LOWPRICE RATE MAY HAVE A HOUSE WITH LOW SQFT AND HIGH PRICE.**\n\nTHEREFORE WE WILL CALCULATE STANDARD DEVIATION OF PRICE_PER_SQFT W.R.T LOCATION","19252189":"LINEAR REGRESSION","f1145335":"**MAJOR PROBLEM IN TOTAL_SQFT BECAUSE IT HAS DATA IN DIFFERENT UNITS AND RANGE:\nEG- 8000-7500, 1331-1350, 35.56 perch......**","443ab075":"***MODEL BUILDING***","2ed92591":"> **Observing and cleaning each column**","6579a924":"**Location**\n\n*In this column we have high unique values(1304)and it is impossible to know every value and use it for prediction.*\nThus we check which area(LOCATION) has maximum repetation in our data.\nAny location with repetation less than 10 can be replaced as \"others\".\n\nAFTER DOING THIS WE WOULD HAVE ONLY 242 UNIQUE VALUES IN LOCATION."}}