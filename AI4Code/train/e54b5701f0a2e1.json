{"cell_type":{"0299d168":"code","2fdfcb8f":"code","63599d2b":"code","e6fda4a9":"code","6165a2f1":"code","87147cb4":"code","967fc5de":"code","752b34b2":"code","667c6013":"code","f96cf0c8":"code","9a211b86":"code","8237a2cf":"code","421fd2b7":"code","878e3e75":"code","82cccac1":"code","fc5dcae1":"code","9fb94fb2":"code","c250d96e":"code","bc8af661":"code","5894ec6e":"code","3f2bcd69":"code","4075c79b":"code","ae45ebd7":"code","62f4bf0e":"code","9c814253":"code","7e435c4c":"code","1d65db3e":"code","c02948fa":"code","01b83cd8":"code","f1ce8162":"markdown","a6e159a6":"markdown","2d928b9f":"markdown","b668330f":"markdown","699725c5":"markdown","8b2b123c":"markdown","29eb8825":"markdown","a9195e57":"markdown","53aefa7f":"markdown","6b75785e":"markdown","2a7ac913":"markdown","5129ffc9":"markdown","5e594e95":"markdown","d8eb9b2c":"markdown"},"source":{"0299d168":"# Package imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix,make_scorer\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2fdfcb8f":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","63599d2b":"# Reads the .csv files into the train and test pandas data frames\n\ntrain = pd.read_csv(\"..\/input\/learn-together\/train.csv\") # training data set\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\") # testing data set","e6fda4a9":"# Confirming the size of 'train' data frame - 56 columns with 15120 rows,\n# Confirming the size of 'test' data frame - 55 columns with 565892 rows\ntrain.shape,test.shape","6165a2f1":"# First few rows of the 'train' dataset\ntrain.head()","87147cb4":"# Confirming data types of the data frame\ntrain.info()","967fc5de":"train.describe().T","752b34b2":"print(train['Soil_Type40'].value_counts())","667c6013":"# Let's look into if any column has missing values to handle in the process\ntrain.isna().sum()","f96cf0c8":"# Remove the Id column as it has no use in coorelation matrix\ntrain = train.drop([\"Id\"], axis = 1)","9a211b86":"# Let's check the correlation among non-binary type data and draw a heat map. So the approach is\n# 1. Find all the binary type columns without typing names\n# 2. Then find all non-binary type columns\n\nall_columns = train.columns # All column names\nbinary_cols = [col_ for col_ in all_columns if (set(train[col_].unique()).issubset([0,1]))] # all binary columns\nnon_binary_cols = set(all_columns) - set(binary_cols) - set(['Cover_Type']) # all non binary columns excluding 'Cover Type'\n\n# Let's get the correlation matrix for non_binary_cols\nmatrix = train[non_binary_cols].corr()\n\n# Let's draw the heatmap for correlation matrix for non_binary_cols\nplt.figure(figsize=(10, 7)) \nplt.title(\"Correlation Plot For Non-binary Fields\")\nmask = np.zeros_like(matrix)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"): \n    ax = sns.heatmap(matrix, annot=True, fmt=\".2f\", linewidths=.5, mask=mask, cmap=\"viridis\")\n","8237a2cf":"# Let's add wilderness area description for the dataset for clarity.\ntrain_c = train.copy() # Create a copy to keep original dataframe unchanged\n\ndef wilderness(row):\n    if(row['Wilderness_Area1'] == 1):\n        return 'Rawah'\n    elif(row['Wilderness_Area2'] == 1):\n        return 'Neota'\n    elif(row['Wilderness_Area3'] == 1):\n        return 'Comanche Peak'\n    elif(row['Wilderness_Area4'] == 1):\n        return 'Cache la Poudre'\n    else:\n        return 'No-Wilderness'\n    \n# Let's add a new column for 'Wilderness' description\ntrain_c['Wilderness'] = [wilderness(row_[1]) for row_ in train_c.iterrows()]\n\n# Let's add a new column for 'Cover_Type' description\ncover_dict ={1:'Spruce\/Fir',2:'Lodgepole Pine',3:'Ponderosa Pine',4:'Cottonwood\/Willow',5:'Aspen',6:'Douglas-fir',7:'Krummholz'}\ntrain_c['Cover'] = train_c['Cover_Type'].map(cover_dict)","421fd2b7":"# The count distribution according to 'Wilderness' in the training dataset\np = sns.countplot(data=train_c,y = 'Wilderness')","878e3e75":"# Let's draw kdeplots for few selected attributes (attribs) classified on 'Cover' and 'Wilderness'\nattribs = ['Elevation','Aspect','Horizontal_Distance_To_Hydrology','Hillshade_Noon'] # Selected Attributes\nfor attr_ in attribs:\n    g = sns.FacetGrid(train_c, col=\"Wilderness\",hue=\"Cover\",height=5)\n    g.map(sns.kdeplot, attr_,shade=True)\n    g.add_legend()","82cccac1":"test_ids = test[\"Id\"]\ntest = test.drop([\"Id\"], axis = 1)\n\n# Remove the columns Soil_Type7 and Soil_Type15 from the test set as they all 0\n# cols_to_drop = ['Soil_Type7', 'Soil_Type15']\n# test = test[test.columns.drop(cols_to_drop)]\n\ntrain.head()","fc5dcae1":"# train test split 20% for validation\nX = train.drop(['Cover_Type'], axis=1)\ny = train['Cover_Type']\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2,random_state=0)","9fb94fb2":"# Import StandardScaler\n# from sklearn.preprocessing import StandardScaler\n\n# # Instantiate StandardScaler and use it to rescale X_train and X_val\n# scaler = StandardScaler()\n# rescaledX_train = scaler.fit_transform(X_train)\n# rescaledX_val = scaler.fit_transform(X_val)","c250d96e":"# Building the model \nmodel = ExtraTreesClassifier(n_estimators=1000,random_state=42)\n\nmodel.fit(X_train,y_train)\npredictions = model.predict(X_val)\naccuracy_score(y_val, predictions)","bc8af661":"# from sklearn.model_selection import GridSearchCV\n# # Create the parameter grid based on the results of random search \n# param_grid = {\n#     'bootstrap': [True],\n#     'max_depth': [80, 90, 100, 110],\n#     'max_features': [2, 3],\n#     'min_samples_leaf': [3, 4, 5],\n#     'min_samples_split': [8, 10, 12],\n#     'n_estimators': [500, 700]\n# }\n\n# model = RandomForestClassifier()\n\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = model, param_grid = param_grid,cv = 3, n_jobs = -1, verbose = 2)\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train,y_train)\n# grid_search.best_params_\n\n# best_grid = grid_search.best_estimator_\n# grid_accuracy = evaluate(best_grid, X_val, y_val)\n","5894ec6e":"# def evaluate(model, test_features, test_labels):\n#     predictions = model.predict(test_features)\n#     errors = abs(predictions - test_labels)\n#     mape = 100 * np.mean(errors \/ test_labels)\n#     accuracy = 100 - mape\n#     print('Model Performance')\n#     print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n#     print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n#     return accuracy","3f2bcd69":"# Drawing confusion matrix\ncm = confusion_matrix(y_val, predictions)\nplt.figure(figsize=(10,7))\nsns.heatmap(cm, annot=True,fmt=\"2d\",linewidths=.5,cmap=\"viridis\")\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","4075c79b":"# # Using GridSearchCV for finding right hyperparameters\n# # Building the model \n# model = RandomForestClassifier()\n\n# parameters = {'n_estimators': [100, 200, 300]}\n\n# # Type of scoring used to compare parameter combinations\n# acc_scorer = make_scorer(accuracy_score)\n\n# # Run the grid search\n# grid_obj = GridSearchCV(model, parameters, scoring=acc_scorer)\n# # grid_obj = grid_obj.fit(X_train, y_train)\n# grid_obj = grid_obj.fit(rescaledX_train, y_train)\n\n# # Set the clf to the best combination of parameters\n# model = grid_obj.best_estimator_\n\n# # Fit the best algorithm to the data. \n# model.fit(rescaledX_train, y_train)\n\n# # Prediction\n# predictions = model.predict(rescaledX_val)\n# accuracy_score(y_val, predictions)\n\n","ae45ebd7":"# Use K-fold Cross Validation\n# from sklearn.model_selection import cross_val_score,cross_val_predict\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.svm import SVC\n\n# score = cross_val_score(model,X_train,y=y_train,cv=5).mean()\n# print(score)","62f4bf0e":"# Using XGBoost\n# import xgboost as xgb\n\n# xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", random_state=42)\n# xgb_model.fit(rescaledX_train,y_train)\n\n# predictions = xgb_model.predict(rescaledX_val)\n# accuracy_score(y_val, predictions)","9c814253":"# Evaluating the feature importance\n# import eli5\n# from eli5.sklearn import PermutationImportance\n\n# perm = PermutationImportance(model, random_state=0).fit(X_val, y_val)\n# eli5.show_weights(perm, feature_names = X_val.columns.tolist())","7e435c4c":"# Using GridSearchCV for finding right hyperparameters\n","1d65db3e":"test.head()","c02948fa":"# scaler = StandardScaler()\n# rescaled_test = scaler.fit_transform(test)\n\n\n# Applying the model on the test set\n# test_pred = model.predict(rescaled_test)\ntest_pred = model.predict(test)\n# test_pred = cross_val_predict(model,X_train,y=y_train,cv=5) # model.predict(test)\n","01b83cd8":"# Save test predictions to file\noutput = pd.DataFrame({'Id': test_ids,\n                       'Cover_Type': test_pred})\noutput.to_csv('submission.csv', index=False)","f1ce8162":"Let's use 80% of the Data for training, and 20% for validation. We'll then train a simple Random Forest Classifier with 100 trees","a6e159a6":"# Future work\n\n* Explorative Data Analysis to extract the most relevant features\n* Feature engineering\n* Cross-validation so we can use the entire training data\n* Grid-Search to find the optimal parameters for our classifier so we can fight overfitting\n* Try a different classifer. XgBoost for example (I suspect the winning solution will use an xgboost. highly recommended\n* Deep-learning ? hummm probably not. Overkill","2d928b9f":"**Data Description**\n\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. Need to predict an integer classification for the forest cover type. The seven types are:\n\n1 - Spruce\/Fir  \n2 - Lodgepole Pine  \n3 - Ponderosa Pine  \n4 - Cottonwood\/Willow  \n5 - Aspen  \n6 - Douglas-fir  \n7 - Krummholz  \n\n\n**Data Fields**\n\n**Elevation** - Elevation in meters\n**Aspect** - Aspect in degrees azimuth\n**Slope** - Slope in degrees\n**Horizontal_Distance_To_Hydrology** - Horz Dist to nearest surface water features\n**Vertical_Distance_To_Hydrology** - Vert Dist to nearest surface water features\n**Horizontal_Distance_To_Roadways** - Horz Dist to nearest roadway\n**Hillshade_9am (0 to 255 index)** - Hillshade index at 9am, summer solstice\n**Hillshade_Noon (0 to 255 index)** - Hillshade index at noon, summer solstice\n**Hillshade_3pm (0 to 255 index)** - Hillshade index at 3pm, summer solstice\n**Horizontal_Distance_To_Fire_Points** - Horz Dist to nearest wildfire ignition points\n**Wilderness_Area (4 binary columns, 0 = absence or 1 = presence)** - Wilderness area designation\n**Soil_Type (40 binary columns, 0 = absence or 1 = presence)** - Soil Type designation\n**Cover_Type (7 types, integers 1 to 7)** - Forest Cover Type designation","b668330f":"### Wilderness Area Analysis\nThe target is to analyze different attributes according to the Wilderness Area type","699725c5":"Our Model has a 84% accuracy on the test set.","8b2b123c":"**Takeaway:** There are few columns with strong positive and negative correlations  \n  \n**Positive Correlation (0.5 < Corr <= 1)**  \n*Horizontal_Distance_To_Hydrology and Verticle_Distance_To_Hydrology  \nAspect and Hillshade_3pm  \nHillshade_Noon and Hillshade_3pm  \nElevation and Horizontal_Distance_To_Roadways* \n\n**Negative Correlation (-0.5 > Corr >= -1)**  \n*Hillshade_9am and Hillshade_3pm  \nHillshade_noon and Slope  \nAspect and Hillshade_9am*","29eb8825":"# Model Training","a9195e57":"\n\nLet's delete the Id column in the training set but store it for the test set before deleting","53aefa7f":"**Import Data**","6b75785e":"### Correlation Analysis\nLet's look into the data type of each column in the dataset. The idea is to categorize columns into batches according to type\nassuming it will help easy handling of data in the process. But still we don't know if it helps but, let's try first","2a7ac913":"**Take Away**\nAll columns are not null and they are all integers","5129ffc9":"**Takeaway:** No column has missing values to be handled in the process","5e594e95":"## EDA - Exploratory Data Analysis  \nUse graphical and numerical techniques to uncover the structure of the data  \n\nThe credit should go to https:\/\/www.kaggle.com\/thedatabeast\/eda-a-tool-to-play-with-your-data for some parts EDA","d8eb9b2c":"# Classify Forest Types - Roosevelt National Forest\n\nThe study conducted in four wilderness areas within the [Roosevelt National Forest](https:\/\/en.wikipedia.org\/wiki\/Roosevelt_National_Forest) of northern Colorado. These areas represent forests with very little human disturbances \u2013 the existing forest cover types there are more a result of ecological processes rather than forest management practices.\n\nThe objective of this study is to predict what types of trees there are in an area based on various geographic features.\n\n**Acknowledgements:**\nThis dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:\n\nBache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/c\/ce\/Rawah_Wilderness.jpg\/568px-Rawah_Wilderness.jpg)"}}