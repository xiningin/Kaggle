{"cell_type":{"902cf80a":"code","b01eca42":"code","40d62043":"code","b0fceac5":"code","a1141931":"code","767eaee5":"code","1c282d40":"code","a275d75d":"code","7164df9c":"code","fff539b2":"code","6ce1f966":"code","7b8d5a0b":"code","7167fda9":"code","8a149b9a":"code","2c154db3":"code","ede27516":"code","e42d7bcc":"code","7291a630":"code","f2e73722":"code","c6905bf2":"code","733e35cd":"code","9fd1424e":"markdown","fc63d993":"markdown","e029ff3a":"markdown","0484cbee":"markdown","b64da945":"markdown","5f17e66e":"markdown","4970c187":"markdown","067f0fe3":"markdown","b1b41898":"markdown","964fc558":"markdown"},"source":{"902cf80a":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px","b01eca42":"path_traindata = '..\/input\/birds-songs-numeric-dataset\/train.csv'\npath_testdata = '..\/input\/birds-songs-numeric-dataset\/test.csv'\n\ntrain_data = pd.read_csv(path_traindata)\ntest_data = pd.read_csv(path_testdata)\n\nprint('Data imported')","40d62043":"#Total rows and columns\ntrain_data_shape = train_data.shape\ntest_data_shape = test_data.shape\n#Total of nulls per column\ntrain_total_nulls = train_data.isnull().sum().sort_values(ascending=False)\ntest_total_nulls = test_data.isnull().sum().sort_values(ascending=False)\n\nprint('Train Rows: ', train_data_shape[0],', Train Columns: ', train_data_shape[1])\nprint('Test Rows: ', test_data_shape[0],', Test Columns: ', test_data_shape[1])\n\nprint('\\nTotal nulls per column Train Data:\\n', train_total_nulls)\nprint('\\nTotal nulls per column Test Data:\\n', test_total_nulls)","b0fceac5":"train_data.head()","a1141931":"# Check object type columns\n\n#Get object type columns\nobjecttype_columns = train_data.select_dtypes(include=[object]).columns   #Object type columns\ndata_objecttype = train_data[objecttype_columns]   #Copy data object type\n#print(objecttype_columns)\ntrain_data[objecttype_columns].head()","767eaee5":"# Check object type columns\n\n#Get object type columns\nobjecttype_columns_test = test_data.select_dtypes(include=[object]).columns   #Object type columns\ndata_objecttype_test = test_data[objecttype_columns_test]   #Copy data object type\n#print(objecttype_columns)\ntest_data[objecttype_columns_test].head()","1c282d40":"#Count distinct species\nprint(\"Distinct species: \", train_data.species.nunique())\n#Count distinct genus\nprint(\"Distinct genus: \", train_data.genus.nunique())","a275d75d":"#Encode categorical data\ntrain_data[objecttype_columns] = train_data[objecttype_columns].astype('category')\ntest_data[objecttype_columns_test] = test_data[objecttype_columns_test].astype('category')","7164df9c":"#Encoder train set\nencoder = OrdinalEncoder()\nencoder.fit(train_data[objecttype_columns])\ntrain_data[objecttype_columns] = encoder.transform(train_data[objecttype_columns])\n\n#Encoder test set\ntest_data[objecttype_columns_test] = encoder.transform(test_data[objecttype_columns_test])","fff539b2":"train_data[objecttype_columns].head()","6ce1f966":"#The data is sorted, therefore accuracy for test split will always be 0\n#To fix this we must shuffle the data\n\ntrain_data = train_data.sample(frac=1)","7b8d5a0b":"#Remove target from X data and column id\ny_train = train_data[\"species\"].copy()\nx_train = train_data.drop(\"species\", axis=1)\nx_train = x_train.drop(\"id\", axis=1)\n\ny_test = test_data[\"species\"].copy()\nx_test = test_data.drop(\"species\", axis=1)\nx_test = x_test.drop(\"id\", axis=1)\n\nprint(x_train.shape, y_train.shape)","7167fda9":"#Standard Scaler\n#Scaler improves the accuracy\n\ncolumns = list(x_train.columns)\n\nstandard_scaler = StandardScaler()\nx_train[columns] = standard_scaler.fit_transform(x_train[columns])\nx_test[columns] = standard_scaler.fit_transform(x_test[columns])","8a149b9a":"#Model Definition\ninput_shape = (32,170)\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Dense(510, activation=\"relu\"),\n        layers.Dense(340, activation=\"relu\"),\n        layers.Dense(170, activation=\"relu\"),\n        layers.Dense(510, activation=\"relu\"),\n        layers.Dense(340, activation=\"relu\"),\n        layers.Dense(170, activation=\"relu\"),\n        layers.Dense(340, activation=\"relu\"),\n        layers.Dense(510, activation=\"relu\"),\n        layers.Dense(340, activation=\"relu\"),\n        layers.Dense(170, activation=\"relu\"),\n        layers.Dense(340, activation=\"relu\"),\n        layers.Dense(510, activation=\"relu\"),\n        layers.Dense(340, activation=\"relu\"),\n        layers.Dense(85, activation=\"softmax\"),\n    ]\n)\n\n\n# Build Model\nmodel.build(input_shape)\n\n#Check the architecture\nmodel.summary()","2c154db3":"lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.001,\n    decay_steps=10000,\n    decay_rate=0.01)\n\nopt=keras.optimizers.Adamax(learning_rate=lr_schedule)","ede27516":"model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'],  )","e42d7bcc":"history = model.fit(x_train, y_train, epochs=20, verbose=1, validation_split=0.4)","7291a630":"xepochs = [n for n in range(len(history.history['accuracy']))]\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Accuracy over time\", \"Loss over time\"))\n\nfor metric in ['accuracy', 'val_accuracy']:\n    fig.add_trace(go.Scatter(x=xepochs, y=history.history[metric], mode='lines+markers', name=metric), row=1, col=1)\n\nfor metric in ['loss', 'val_loss']:\n    fig.add_trace(go.Scatter(x=xepochs, y=history.history[metric], mode='lines+markers', name=metric), row=1, col=2)\n\n\nfig.update_layout(title_text=\"Original Model\")\n\nfig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\nfig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n\nfig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\nfig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n\nfig.show()","f2e73722":"predictions = model.predict(x_test)\n\ny_pred = []\n\nfor i in predictions:\n    y_pred.append(np.argmax(i))\n    \n#print(y_pred)","c6905bf2":"cm = confusion_matrix(y_test, y_pred)\n\nfig = px.imshow(\n    cm,\n)\n\nfig.show()","733e35cd":"#Classification Report\nprint(classification_report(\n    y_test, \n    y_pred,\n))","9fd1424e":"# Import Data","fc63d993":"# History Plot","e029ff3a":"# Encode Categorical","0484cbee":"# Imports","b64da945":"# Scaler","5f17e66e":"# Predictions and Reports","4970c187":"# Birds' Songs Classification - DNN\n\nC\u00e9sar Jim\u00e9nez Mena","067f0fe3":"# Data Analysis","b1b41898":"# Remove Unnecessary Columns","964fc558":"# Deep Neural Network"}}