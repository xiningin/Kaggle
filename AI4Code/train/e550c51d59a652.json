{"cell_type":{"42342fc3":"code","39fe9c21":"code","6e591570":"code","c91f8580":"code","82b105f9":"code","7086d53f":"code","fb2ab441":"code","81b57867":"code","faa4eede":"code","327ac8ba":"code","80f90fe9":"code","cc0c8c07":"code","5a44b3d8":"code","951b9db3":"code","96bd6086":"code","1615338d":"code","8276324e":"code","463013cf":"code","8293bbca":"code","13718ce9":"code","da02d08f":"code","38caff4c":"code","e7fd9dbf":"code","c527eebc":"code","7cb3e5ed":"code","f911543a":"code","2690c107":"code","1903cba6":"code","df502221":"code","d720fdef":"code","c1eb4dda":"code","f1b102b7":"code","eed3cf20":"code","af0f5c34":"code","014a9e56":"code","c63c6282":"code","4acbd9b4":"code","a8bd73d7":"code","057f8971":"code","3d1538c5":"code","0de16bb6":"code","140f310b":"code","716dd951":"code","f3e3b36f":"code","458d4b30":"code","d5ce7428":"code","4f6edf41":"code","113af638":"code","76e76e5f":"code","68935181":"code","ed660e64":"code","3a9f6855":"code","7b680814":"code","de64dd40":"code","8970faa9":"code","b98d941e":"code","ff0fdd84":"code","2b38a744":"code","af4e037e":"code","2832c964":"code","690f7d31":"code","6fd6425a":"markdown","313f22d5":"markdown","58844e64":"markdown","9b3cba76":"markdown","bd894396":"markdown","3f5b520f":"markdown","9ff65d2b":"markdown","be2166d5":"markdown","0e19e495":"markdown","2a34bc0e":"markdown","fe96fcad":"markdown","930f9ee9":"markdown","ae158ed2":"markdown","8e2696f2":"markdown","300f6c88":"markdown","b8ea9010":"markdown","abf39726":"markdown","984f63a2":"markdown","4c07b826":"markdown","d190ce9b":"markdown","728d5a31":"markdown","88ed4293":"markdown","a90c8b97":"markdown","cfef0dc3":"markdown","7caf230f":"markdown","2f9f9ab5":"markdown","12261f86":"markdown","3b3844ad":"markdown","d9fab5f6":"markdown","f2b36806":"markdown","ae5d3b80":"markdown","a3b0c0c1":"markdown","3f0f6082":"markdown","47bc8799":"markdown","f42e4cb8":"markdown","8d55af8c":"markdown","3719315a":"markdown","55228af8":"markdown","23f40685":"markdown","af9b9114":"markdown","93893201":"markdown","7c8d10eb":"markdown","7cf4316e":"markdown","1e7d7aff":"markdown","cc04063e":"markdown","54e7a45e":"markdown","2dd3b5e6":"markdown","7aa1dcc7":"markdown","6543cba0":"markdown","59d67183":"markdown"},"source":{"42342fc3":"import warnings\nwarnings.filterwarnings('ignore')\nimport os, sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime \nimport seaborn as sns\nimport scipy\nimport random\nimport math\nfrom scipy.stats.mstats import winsorize\nfrom tqdm import tqdm\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn import svm\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","39fe9c21":"data = pd.read_csv('\/kaggle\/input\/customer-personality-analysis\/marketing_campaign.csv',sep='\\t')\ndata.head()","6e591570":"data.info()","c91f8580":"# Check NUll data\ndata.isna().sum()","82b105f9":"data['Income'].fillna(np.mean(data['Income']), inplace=True)\n# Change unit to 1K $ \ndata['Income'] = data['Income'] \/ 1000 ","7086d53f":"pd.DataFrame(data.nunique()).sort_values(0).rename( {0: 'Unique Values'}, axis=1)","fb2ab441":"# Drop constant columns - Z_CostContact & Z_Revenue\ndata.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True) \n# User ID & Regist Date is useless for classification & Segmentation task\ndata.drop(['ID', 'Dt_Customer'], axis=1, inplace=True) ","81b57867":"# Change Year_Birth to Age (Age is more informative)\ndata['Age'] = 2021 - data.Year_Birth.to_numpy()\ndata.drop('Year_Birth', axis=1, inplace=True)","faa4eede":"data['Marital_Status'].value_counts()","327ac8ba":"data['Marital_Status'] = data['Marital_Status'].replace(['Alone','YOLO','Absurd'],'Single')","80f90fe9":"data['Education'].value_counts()","cc0c8c07":"data['Education'].replace(['2n Cycle', 'Graduation'], ['Master', 'Bachelor'], inplace=True)","5a44b3d8":"# plot all numerical columns (14)\nnum_coln = data.select_dtypes(include=np.number).columns.tolist()\nbins=10\nj=1\nfig = plt.figure(figsize = (20, 30))\nfor i in num_coln:\n    plt.subplot(7,4,j)\n    plt.boxplot(data[i])\n    j=j+1\n    plt.xlabel(i)\n    # plt.legend(i)\nplt.show()","951b9db3":"data.drop(data[(data['Income']>200)|(data['Age']>100)].index,inplace=True)","96bd6086":"# Maritial Status & Education levels\ncust_count=data.groupby(\"Marital_Status\").count()['Age']\nlabel=data.groupby('Marital_Status').count()['Age'].index\nfig, ax = plt.subplots(1, 2, figsize = (10, 12))\nax[0].pie(cust_count, labels=label, shadow=True, autopct='%1.2f%%',radius=2,explode=[0.3,0.3,0.2,0.1,0.1])\nax[0].set_title('Maritial Status', y=-0.6)\n\ncust_count = data.groupby(\"Education\").count()['Age']\nlabel = data.groupby('Education').count()['Age'].index\nax[1].pie(cust_count, labels=label, shadow=True, autopct='%1.2f%%',radius=2,explode=[0.3,0.3,0.3,0.1])\nax[1].set_title('Education Level', y=-0.6)\nplt.subplots_adjust(wspace = 1.5, hspace =0)\nplt.show()","1615338d":"# Age Range\nplt.figure(figsize=(20, 6))\nplt.title('Age distribution')\nax = sns.histplot(data['Age'].sort_values(), bins=56)\nsns.rugplot(data=data['Age'], height=.05)\nplt.xticks(np.linspace(data['Age'].min(), data['Age'].max(), 56, dtype=int, endpoint = True))\nplt.show()","8276324e":"# Kid Home & Teen Home\nplt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.histplot(data=data, x='Kidhome', stat=\"percent\", discrete=True)\nplt.xticks([0, 1, 2])\n\nplt.subplot(122)\nsns.histplot(data=data, x='Teenhome', stat=\"percent\",discrete=True)\nplt.xticks([0, 1, 2])\nplt.show()","463013cf":"# Response dist. & Imcome Range\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nsns.histplot(data=data, x='Response', stat=\"percent\", discrete=True)\nplt.xticks([0, 1])\n\nplt.subplot(122)\n# Income Range\nsns.kdeplot(data=data, x=\"Income\", shade=True, log_scale=True)\nplt.show()","8293bbca":"# Education & Response\nplt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.histplot(data=data, x=\"Education\", hue=\"Response\", multiple=\"stack\", stat=\"percent\")\n\n# Marital_Status & Response\nplt.subplot(122)\nsns.histplot(data=data, x=\"Marital_Status\", hue=\"Response\",stat=\"percent\", multiple=\"stack\")\nplt.show()","13718ce9":"# Kid Home & Response\nplt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.histplot(data=data, x=\"Kidhome\", hue=\"Response\", multiple=\"stack\", stat=\"percent\", discrete=True)\nplt.xticks([0, 1, 2])\n# Teen Home & Response\nplt.subplot(122)\nsns.histplot(data=data, x=\"Teenhome\", hue=\"Response\", multiple=\"stack\", stat=\"percent\", discrete=True)\nplt.xticks([0, 1, 2])\nplt.show()","da02d08f":"# Income (by Response\/Marital_Status\/Education\/Kidhome)\nplt.figure(figsize=(15,10))\nplt.subplot(221)\nsns.kdeplot(\n   data=data, x=\"Income\", hue=\"Response\", log_scale= True,\n   fill=True, common_norm=False,\n   alpha=.5, linewidth=0,\n)\nplt.gca().axes.get_yaxis().set_visible(False) # Set y invisible\nplt.xlabel('Income')\n\n# segment by Marital_Status\nplt.subplot(222)\nsns.kdeplot(\n   data=data, x=\"Income\", hue=\"Marital_Status\", log_scale= True,\n   fill=True, common_norm=False, palette=\"crest\",\n   alpha=.5, linewidth=0,\n)\nplt.gca().axes.get_yaxis().set_visible(False) \n\n# segment by Education\nplt.subplot(223)\nsns.kdeplot(\n   data=data, x=\"Income\", hue=\"Education\", log_scale= True,\n   fill=True, common_norm=False, palette=\"crest\",\n   alpha=.5, linewidth=0,\n)\nplt.gca().axes.get_yaxis().set_visible(False) \n\n# segment by Kidhome\nplt.subplot(224)\nsns.kdeplot(\n   data=data, x=\"Income\", hue=\"Kidhome\", log_scale= True,\n   fill=True, common_norm=False, palette=\"crest\",\n   alpha=.5, linewidth=0,\n)\nplt.gca().axes.get_yaxis().set_visible(False)","38caff4c":"# Check all Vriable Type\npd.DataFrame(data.nunique()).sort_values(0).rename( {0: 'Unique Values'}, axis=1)","e7fd9dbf":"# Choose features have over 50 different values\nNUMERICAL_FEATURES = ['Age', 'Recency', 'MntFruits', \n                      'MntSweetProducts', 'MntFishProducts', 'MntGoldProds', \n                      'MntMeatProducts', 'MntWines', 'Income']\n\nsns.pairplot(data=data[NUMERICAL_FEATURES], \n             kind='scatter', plot_kws={'alpha':0.4})\nplt.show()","c527eebc":"# Heat map among all numerical variables\ncor = data.corr()\nplt.figure(figsize = (27,26))\nsns.heatmap(cor, annot = True, cmap = 'coolwarm')\nplt.show()","7cb3e5ed":"# Education have orders, so we change Basic-Phd to scale 0-3\ndf_cluster = data.copy() # leave for clustering \n\ndata['Education'] = data['Education'].replace(['Basic'], 0)\ndata['Education'] = data['Education'].replace(['Bachelor'], 1)\ndata['Education'] = data['Education'].replace(['Master'], 2)\ndata['Education'] = data['Education'].replace(['PhD'], 3)\n\n# Change Marital_Status to dummies\ndata = pd.get_dummies(data)\ndata.info()","f911543a":"# Using Random Forest to gain an insight on Feature Importance\nclf = RandomForestClassifier()\nclf.fit(data.drop('Response', axis=1), data['Response'])\n\nplt.style.use('seaborn-whitegrid')\nimportance = clf.feature_importances_\nimportance = pd.DataFrame(importance, index=data.drop('Response', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2));","2690c107":"# Choose Features (Keep 90% Importance ration)\nfeature_nums = 18\nascend_import = importance.sort_values(by='Importance', ascending=False)\nall_info = ascend_import['Importance'].iloc[:feature_nums].sum()\nall_choose_features = list(ascend_import.iloc[:feature_nums].index)\n\nprint('Names: ', all_choose_features)\nprint('Importance Raio: ', all_info)","1903cba6":"from sklearn.decomposition import PCA\n# Calculating PCA for both datasets, and graphing the Variance for each feature, per dataset\nstd_scale = preprocessing.StandardScaler().fit(data.drop('Response', axis=1))\nX = std_scale.transform(data.drop('Response', axis=1))\n\npca1 = PCA(0.90, whiten=True) # Keep 90% information\nfit1 = pca1.fit(X)\n\n# Graphing the variance per feature\nplt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(25,7)) \nplt.xlabel('PCA Feature')\nplt.ylabel('Variance')\nplt.title('PCA for Whole Dataset')\nplt.bar(range(0, fit1.explained_variance_ratio_.size), fit1.explained_variance_ratio_);\n\n# Get pca transformed data\npca_data = pca1.transform(X)\npca_data = np.array(pca_data)\nprint('PCA data shape: ', pca_data.shape)","df502221":"# This list is used to store model performance of different datasets(raw, feature selection, pca)\n'''\n!!! Do NOT rerun this blk, when choosing different dataset, \n\nbecause this list is used to store the performances of different models.\n'''\nperf_df_lst = [None, None, None] ","d720fdef":"# Choose dataset\ndataset_num = 1\n\n# 0: Raw Data; 1: Feature Selection Data; 2: PCA Data\nall_datasets = [data.drop('Response', axis=1).values, data[all_choose_features].values, pca_data]\n\n# Choose data\nfinal_data = all_datasets[dataset_num]","c1eb4dda":"# Split the dataset\nfrom imblearn.over_sampling import SMOTE\nfrom collections import  Counter\n\nx_train = final_data[:2000]\ny_train = data['Response'].values[:2000]\nx_test = final_data[2000:]\ny_test = data['Response'].values[2000:]\n\nprint('Train: ', len(x_train))\nprint('Test: ',  len(x_test))\nprint('N\/P Sample: ', Counter(y_train))\n\n# SMOTE Samples\nsm = SMOTE(random_state=2)\nx_train, y_train = sm.fit_resample(x_train, y_train.ravel())\n\nprint('Train: ', len(x_train))\nprint('Test: ',  len(x_test))\nprint('N\/P Sample: ', Counter(y_train))\n\n# MCC scorer function\nmcc_scorer = make_scorer(matthews_corrcoef)","f1b102b7":"LR = LogisticRegression()\n\n# K-Fold Validation\nkfold = 8\n\n# ACC Score\nLR_cv_results_acc = cross_val_score(LR, x_train, y_train, cv=kfold, scoring='accuracy')   \nmsg = \"%s k-fold ACC: %f (%f)\" % ('LR', LR_cv_results_acc.mean(), LR_cv_results_acc.std())\nprint(msg)\n# MCC Score\nLR_cv_results_mcc = cross_val_score(LR, x_train, y_train, cv=kfold, scoring=mcc_scorer)   \nmsg = \"%s k-fold MCC: %f (%f)\" % ('LR', LR_cv_results_mcc.mean(), LR_cv_results_mcc.std())\nprint(msg)","eed3cf20":"# Validation for Boosting Tree\nclf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=160, random_state=1)  \n# ACC Score\nBT_cv_results_acc = cross_val_score(clf, x_train, y_train, cv=kfold, scoring='accuracy')   \nmsg = \"k-fold ACC: %f (%f)\" % (BT_cv_results_acc.mean(), BT_cv_results_acc.std())\nprint(msg)\n\n# MCC Score\nBT_cv_results_mcc = cross_val_score(clf, x_train, y_train, cv=kfold, scoring=mcc_scorer)   \nmsg = \"k-fold MCC: %f (%f)\" % (BT_cv_results_mcc.mean(), BT_cv_results_mcc.std())\nprint(msg)","af0f5c34":"SVM=svm.SVC(kernel = 'rbf', C = 10, gamma = 0.01)\n\n# ACC Score\nsvm_cv_results_acc = cross_val_score(SVM, x_train, y_train, cv=kfold, scoring='accuracy')   \nmsg = \"k-fold ACC: %f (%f)\" % (svm_cv_results_acc.mean(), svm_cv_results_acc.std())\nprint(msg)\n\n# MCC Score\nsvm_cv_results_mcc = cross_val_score(SVM, x_train, y_train, cv=kfold, scoring=mcc_scorer)   \nmsg = \"k-fold MCC: %f (%f)\" % (svm_cv_results_mcc.mean(), svm_cv_results_mcc.std())\nprint(msg)","014a9e56":"# import modules\nfrom keras import models\nfrom keras import layers\n\nx_nn = x_train\ny_nn = y_train","c63c6282":"# Define the K-fold Cross Validator\nkfold_obj = KFold(n_splits=kfold, shuffle=True)\nepochs = 150\nbatch_size = 64\nacc_per_fold = []\nmcc_per_fold = []\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\nfor train, test in kfold_obj.split(x_nn, y_nn):\n    \n    model = models.Sequential()\n    # Only use shallow Neural Network\n    model.add(layers.Dense(32, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n    \n    # Fit data to model\n    history = model.fit(x_nn[train], y_nn[train].reshape(-1, 1),\n              batch_size=64,\n              epochs=100)\n\n    # Generate generalization metrics\n    predictions = model.predict(x_nn[test])\n    \n    for i in range(len(predictions)):\n        if predictions[i] >=0.5:\n            predictions[i] = 1\n        else:\n            predictions[i] = 0\n            \n    acc_per_fold.append(accuracy_score(predictions, y_nn[test]))\n    mcc_per_fold.append(matthews_corrcoef(predictions, y_nn[test]))\n\n    # Increase fold number\n    fold_no = fold_no + 1\n","4acbd9b4":"# NN Performance  \nacc_per_fold, mcc_per_fold = np.array(acc_per_fold), np.array(mcc_per_fold)\nmsg = \"k-fold ACC: %f (%f)\" % (acc_per_fold.mean(), acc_per_fold.std())\nprint(msg)\nmsg = \"k-fold MCC: %f (%f)\" % (mcc_per_fold.mean(), mcc_per_fold.std())\nprint(msg)","a8bd73d7":"perf_df_lst[dataset_num] = pd.DataFrame({\n    'LR': LR_cv_results_mcc,\n    'SVM': svm_cv_results_mcc,\n    'NN': mcc_per_fold,\n    'BT': BT_cv_results_mcc\n})\n\nplt.figure(figsize=(15,10))\nname_lst = ['LR', 'SVM', 'NN', 'Boosting Tree']\n\ntry:\n    ax = plt.subplot(2, 2, 1)\n    ax.boxplot(perf_df_lst[0].values, vert = 0)\n    ax.set_yticklabels(name_lst)\n    ax.set_title('Raw DataSet(MCC Score)')\nexcept:\n    pass\n    \ntry:\n    ax2 = plt.subplot(2, 2, 2)\n    ax2.boxplot(perf_df_lst[1].values, vert = 0)\n    ax2.set_yticklabels(name_lst)\n    ax2.set_title('Feature Select DataSet(MCC Score)')\nexcept:\n    pass\n\ntry:\n    ax2 = plt.subplot(2, 2, 3)\n    ax2.boxplot(perf_df_lst[2].values, vert = 0)\n    ax2.set_yticklabels(name_lst)\n    ax2.set_title('PCA DataSet(MCC Score)')\nexcept:\n    pass\n\nplt.show()","057f8971":"# Raw dataset + Boosting Tree\ndataset_num = 1 # feature selection\ndataset = all_datasets[dataset_num]\n\n# Split train & test\nx_train = dataset[:2000]\ny_train = data['Response'].values[:2000]\nx_test = dataset[2000:]\ny_test = data['Response'].values[2000:]\n\n# SMOTE Samples\nsm = SMOTE(random_state=2)\nx_train, y_train = sm.fit_resample(x_train, y_train.ravel())\n\n'''\nTrain & Test Model\n'''\nBT = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=200)  \nBT.fit(x_train, y_train)\npredictions = BT.predict(x_test)\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, predictions)\nprint(cm)\nplt.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Ground Truth\")\nplt.colorbar()\nplt.show()\n\n# Classification Report\nBT_MCC = matthews_corrcoef(y_test, predictions)\n\nprint('Train MCC: '+ str(matthews_corrcoef(BT.predict(x_train), y_train)))\nprint('Test MCC: ', str(BT_MCC))\nprint('Test ACC: ', accuracy_score(predictions, y_test))\nprint(classification_report(y_test, predictions))\n","3d1538c5":"# Inspect df_cluster\npd.DataFrame(df_cluster.nunique()).sort_values(0).rename( {0: 'Unique Values'}, axis=1)","0de16bb6":"# We don't need detailed marital status, intead we only care about if the customer is single or in relationship\ndf_cluster['Marital_Status'] = df_cluster['Marital_Status'].replace(['Widow','Divorced'],'Single')\ndf_cluster['Marital_Status'] = df_cluster['Marital_Status'].replace(['Married','Together'],'Relationship')\ndf_cluster.Marital_Status.value_counts()","140f310b":"# Combine 'Kidhome' and 'Teenhome' to 'Children', indicating the total num of children in home.\ndf_cluster['Children'] = df_cluster['Kidhome'] + df_cluster['Teenhome']","716dd951":"# Add 'MntTotal' - the total purchasing amount of all products\ndf_cluster['MntTotal'] = np.sum(df_cluster.filter(regex='Mnt'), axis=1)","f3e3b36f":"# Add 'NumTotal' - the total purchasing number of different purchasing types\ndf_cluster['NumTotal'] = np.sum(df_cluster.filter(regex='Purchases'), axis=1)","458d4b30":"# We don't care the which compaign the customer participate in; Instead, we care about the total participation times\ndf_cluster['TotalAccepted'] = np.sum(df_cluster.filter(regex='Accepted'),axis=1)\ndf_cluster.drop(columns=['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2'],inplace=True)","d5ce7428":"# Cacluate Average of purchases per visit to the website, which can reflect the personality of the customer.\ndf_cluster['AvgWeb'] = round(df_cluster['NumWebPurchases'] \/ df_cluster['NumWebVisitsMonth'], 2)\ndf_cluster.fillna({'AvgWeb' : 0},inplace=True) # Handling for cases where division by 0\ndf_cluster.replace(np.inf, 0, inplace=True)","4f6edf41":"# Encoding non-numerical columns: 'Education' & 'Marital_Status'\nfrom sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\nfor i in ['Education', 'Marital_Status']:\n    df_cluster[i]=df_cluster[[i]].apply(encode.fit_transform)","113af638":"df_cluster","76e76e5f":"from yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\n\n# Get cluster data\ncluster_data = df_cluster.values\n\n# Standardize data\nstd_scale = preprocessing.StandardScaler().fit(cluster_data)\ncluster_data = std_scale.transform(cluster_data)","68935181":"# Determine K using Inertia_\ninertia = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 10)\n    kmeans.fit(cluster_data)\n    inertia.append(kmeans.inertia_)\n    \nelbow = 2\nplt.plot(range(1, 10), inertia, marker = '*', alpha=0.5)\nplt.scatter(elbow, inertia[elbow-1], s=100, c='r', marker='*')\nplt.xlabel('Clusters')\nplt.ylabel('Inertia')\nplt.annotate('Elbow Point' ,(elbow, inertia[elbow-1]), xytext=(elbow,inertia[elbow-1] + 3000))\nplt.show()","ed660e64":"# Instantiate the clustering model and visualizer\nmodel = KMeans(init = 'k-means++')\nk_lst = []\n\n# perform K-means 4 times(different intial clusters)\nplt.figure(figsize=(15,10))\nplt.subplot(221)\nvisualizer = KElbowVisualizer(model, k=(2,15), metric='distortion')\nvisualizer.fit(cluster_data)        # Fit the data to the visualizer\nvisualizer.finalize()\nk_lst.append(visualizer.elbow_value_)\n\nplt.subplot(222)\nvisualizer = KElbowVisualizer(model, k=(2,15), metric='distortion')\nvisualizer.fit(cluster_data)        # Fit the data to the visualizer\nvisualizer.finalize()\nk_lst.append(visualizer.elbow_value_)\n\nplt.subplot(223)\nvisualizer = KElbowVisualizer(model, k=(2,15), metric='distortion')\nvisualizer.fit(cluster_data)        # Fit the data to the visualizer\nvisualizer.finalize()\nk_lst.append(visualizer.elbow_value_)\n\nplt.subplot(224)\nvisualizer = KElbowVisualizer(model, k=(2,15), metric='distortion')\nvisualizer.fit(cluster_data)        # Fit the data to the visualizer\nvisualizer.finalize()\nk_lst.append(visualizer.elbow_value_)\n\nprint('Mean K: ', np.mean(k_lst))","3a9f6855":"# Calinski_harabasz Scoring Matrix\nplt.figure(figsize=(18,5))\n\nplt.subplot(121)\nvisualizer = KElbowVisualizer(model, k=(2,15), metric='calinski_harabasz')\nvisualizer.fit(cluster_data)        # Fit the data to the visualizer\nvisualizer.finalize()\n\n# Silhouette Scoring Matrix\nplt.subplot(122)\nvisualizer = KElbowVisualizer(model, k=(2,15), metric='silhouette')\nvisualizer.fit(cluster_data)        # Fit the data to the visualizer\nvisualizer.finalize()\nplt.show()","7b680814":"# Building & Fitting K-Means Models\nkmeans = KMeans(n_clusters=2, init = 'k-means++').fit(cluster_data)\npred = kmeans.predict(cluster_data)\ndf_cluster['Cluster'] = pred + 1","de64dd40":"# Inspect the cluter nums\ndf_cluster[\"Cluster\"].value_counts()","8970faa9":"# We could see the the clear difference between the 2 cluster (A.T Income and MntTotal)\nsns.scatterplot(x='Income',y='MntTotal',hue='Cluster',data=df_cluster)\nplt.show()","b98d941e":"# For every columns in dataset\nfor i in df_cluster:\n    g = sns.FacetGrid(df_cluster, col = \"Cluster\", hue = \"Cluster\", palette = \"coolwarm\", sharey=False, sharex=False)\n    g.map(sns.histplot,i) \n    \n    g.set_xticklabels(rotation=30)\n    g.set_yticklabels()\n    g.fig.set_figheight(5)\n    g.fig.set_figwidth(20)","ff0fdd84":"from sklearn.mixture import GaussianMixture\n\nlog_like_lst = []\nall_cluster = 15\n\nfor k in range(2, all_cluster):\n    gmm = GaussianMixture(n_components = k, random_state = 100).fit(cluster_data)\n    log_like = gmm.bic(cluster_data)\n    log_like_lst.append(log_like)\n\nelbow = 8\nplt.plot(range(2, all_cluster), log_like_lst, alpha=0.5)\nplt.scatter(elbow, log_like_lst[elbow-2], s=100, c='r', marker='*')\nplt.ylabel('BIC')\nplt.xlabel('K')\nplt.annotate('Optimal Point' ,(elbow, log_like_lst[elbow-1]), xytext=(elbow - 0.5,log_like_lst[elbow-2] + 3000))\nplt.show()","2b38a744":"# Building & Fitting GMM Models\ngmm = GaussianMixture(n_components = 8, random_state = 100).fit(cluster_data)\nlabels = gmm.predict(cluster_data)\n\ndf_cluster['Cluster_GMM'] = labels + 1","af4e037e":"# Inspect the cluter nums\ndf_cluster[\"Cluster_GMM\"].value_counts()","2832c964":"# Inspect the cluster difference (By Income & Age)\nsns.scatterplot(x='Income',y='Age',hue='Cluster_GMM',data=df_cluster)\nplt.show()","690f7d31":"# For every columns in dataset\nfor i in df_cluster:\n    if i == 'Cluster':\n        continue\n    g = sns.FacetGrid(df_cluster, col = \"Cluster_GMM\", hue = \"Cluster_GMM\", palette = \"coolwarm\", sharey=False, sharex=False)\n    g.map(sns.histplot,i) \n    g.set_xticklabels(rotation=30)\n    g.set_yticklabels()\n    g.fig.set_figheight(5)\n    g.fig.set_figwidth(20)\n    ","6fd6425a":"## Observations\n* There are 8 different clusters, which is difficult to describe, but we could see clear difference in their basic information, family condition and consumption power ...\n","313f22d5":"## Final Model Performance<a name='FMT'><\/a>","58844e64":"* The age of the customers are mainly clustering in **40s or 60s**, the young people(under 30) are very few. \n\n* These people are at their middle ages or old ages, so their family condition should be further taken care of.","9b3cba76":"### Boosting Tree<a name='BT'><\/a>","bd894396":"## Unsupervised Learning Task - Customer Personality Segmentation","3f5b520f":"### Nerual Networks<a name='NN'><\/a>","9ff65d2b":"## Gaussian Mixture Model<a name=GMM><\/a>","be2166d5":"### Multivariate Analysis<a name='MA'><\/a>","0e19e495":"### Different Scoring Metrics for K-means\n\n* The above scoring parameter metric is set to **distortion**, which computes the sum of squared distances from each point to its assigned center. \n\n* The **silhouette** score calculates the mean Silhouette Coefficient of all samples, while the **calinski_harabasz** score computes the ratio of dispersion between and within clusters. I also use these 2 metrics to get clusters.","2a34bc0e":"## Appendix\n\n* DataSet URL in Kaggle: https:\/\/www.kaggle.com\/imakash3011\/customer-personality-analysis","fe96fcad":"#### The plots about Income VS 4  different Discrete Variables give us some interesting information.\n     1) The high income groups have larger possibility to accept offer in the compaign, as we can see the income distributions of people who say 'yes' and 'no' have a slight difference.\n     2) There are no clear income difference between people with different maritial status.\n     3) Customers only with basic education have significantly lower income, while bachelors, masters, and PhDs do not have clear difference in income level. \n     4) It seems that customers who don't have any kids at home have higher income levels.","930f9ee9":"### Observations:\n* Z_Revenue & Z_CostContact have **Constant** value, which don't provide any information so we should drop them.\n* Response - AcceptedCmp5 are all **Binary Variables**.\n* Marital_Status & Education can be seen as **Categorical Variable**.\n\n* Kidhome & Teenhome are **Discrete Ordinal Variables**, for which have clear orders but not many different values.\n* The rest 16 variables can be seen as **Continuous Ordinal Variables**.","ae158ed2":"#### First Observation:\n* There are 3 'object type' variables - ['Education', 'Marital_Status', 'Dt_Customer'], and the rest are all numerical Vs**\n","8e2696f2":"### Why we need feature engineering for clustering ?\n1) For customer segmentation, it is better to have a more generalistic approach than diving into the finer details, because we want the number of clusters to be relatively less, and the algorithm will automatically group based on what the data is feeded. \n\n2) The clustering algorithm won't be able to distinguish or categorize related records into groups, like we have done - segment people into Single or in a Relationship, or sum up all the amount of purchasing goods.\n\n3) Some interesting ratios, like 'AvgWeb', is not easy to extract for algorithms. So it is better to predefine some ratios that might reflect the personalities of customers.","300f6c88":"## Data Preprocess<a name='DP'><\/a>","b8ea9010":"* According to Response, this is an **unbalanced dataset**, over 80% customers say no to the last compaign. So we should take care and use more comprehensive and accurate indicators(like F-1 or MCC) to evaluate the classification models.\n\n* Most customers income levels are in the range [10K, 100K] per year.","abf39726":"## Supervised Learning Task - Predict Response","984f63a2":"* These 2 figures give a quick look of the customer distribution, we could see the most our customer(64%) are in relationships(Married or Together) and most(97%) are at least bachelor degrees.","4c07b826":"#### Why SMOTE (Dataset Extended Trick) & MCC Scorer (Matthews correlation coefficient) ?\n\n* As we find in the data exploration phase, this is an unbalanced dataset(over 80% say no to the compaign). So the models are easy to learn some traits about negative samples, but it might be hard to get from positive samples.\n\n\n* While **SMOTE** alleviate the problem by offering us more positive training samples.\n\n\n* At the same time, **MCC scorer** takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. In this learning task, MCC is a more efficient measure than accuracy in test period, because there are only a few positive samples in the test set.","d190ce9b":"### Conclusion\n* The overall test accuracy of the model is **0.877**. But dive deep into the score report, the model performs quite good in recognizing negative samples(0), but not good in positive samples (precision: 0.55, recall: 0.55).\n\n\n* The test MCC is **0.469**, which indicates that the model may not good at finding positive samples in test set. While we find the Train MCC is **0.98**, this result shows there might exist **overfitting problem** in the model. But the fact is even I tried a lot to simplify the model and the train MCC decreases a lot, the test mcc still can not show much improvements. This result might indicate the predictors we use in this dataset might not predict 'Response' very well.","728d5a31":"## Feature Selection by Random Forest <a name='FS'><\/a>","88ed4293":"### Univariate Analysis<a name='UA'><\/a>","a90c8b97":"## Exploratory Data Analysis","cfef0dc3":"\n* We can see some clear outliers in **Income** and **Age**. We will remove the rows where the Income is greater than 200K and birth year is less than 1920.\n\n\n* For other columns, we cannot blindly remove these outliers as there could be cases where the requirement for these products is high by the user. Maybe the consumer is hosting a party or an event or is more comfortable getting his products from a particular channel.\n","7caf230f":"**There are 24 NA rows in 'Income' columns, so we fill these NA with the average income of all people**","2f9f9ab5":"* It seems that customers with no kids and no teens at home are more likely to accept the offer in this compaign.","12261f86":"## Notebook Content \n\n### 1. [Data Preprocessing](#DP)\n\n### 2. Exploratory Data Analysis\n* **2.1 [Univariate Analysis](#UA)**\n* **2.2 [Bivriate Analysis](#BA)**\n* **2.3 [Multivriate Analysis](#MA)**\n\n### 3. Feature Selection & Dimension Reduction\n* **3.1 [Feature Selection by Random Forest](#FS)**\n* **3.2 [PCA](#PCA)**\n\n### 4. Supervised Task - Predict Response\n* **4.1 [Logistic Regression](#LR)**\n* **4.2 [Boosting Tree\u00b6](#BT)**\n* **4.3 [SVM](#SVM)**\n* **4.4 [Nerual Network](#NN)**\n* **4.5 [Final Model Performance](#FMT)**\n\n### 5. Unsupervised Task - Customer Personality Segmentation\n* **5.1 [Feature Engineering for Clustering Algorithm](#FECA)**\n* **5.2 [K-Means](#Kmeans)**\n* **5.3 [Guassian Mixture Model](#GMM)**","3b3844ad":"## PCA Transformation<a name='PCA'><\/a>","d9fab5f6":"### Logistic Regression<a name='LR'><\/a>","f2b36806":"## Preparations for Prediction Models","ae5d3b80":"### K-Means <a name=Kmeans><\/a>","a3b0c0c1":"* GMM is a soft version of K-means, calculating the sample probability to different clusters. It is also a good clustering algorithm.\n\n\n* Here, we use **BIC** to evluate the effectiveness of clustering. When **K=8**, the BIC score comes to the balanced point (will not show much improvement when increasing K), so we choose 8 as the final clustering result.","3f0f6082":"* Most customers have 1 kid or 0 kid at home, very few have 2 kids, and no one have kids over 2.\n\n* The number of teens at home is very similar to kid num. ","47bc8799":"### Feature Engineering for Clustering Algorithm <a name=FECA><\/a>","f42e4cb8":"### Model Choosing\n\n* **Boosting Tree** performs the best among all the models in all 3 datasets; The performances of SVM and NN are almost comparable in 3 different dataset; LR is the worst model because it is too simple for this classfication task.\n\n\n* Boosting Tree have some outliers in raw datasets and feature selection datasets, which indicates this algorithm might not be stable in these datasets.\n\n* In conclusion, in this classfication task, we could use **Feature Selection Dataset + Boosting Tree**, because \n    - 1) This combination achieve the best MCC performance. \n    - 2) Although BT might be unstable, even the lower outliers are comparable to NN and SVM.","8d55af8c":"* Univariate Analysis\n* Bivriate Analysis\n    * 2 categorical variables\n    * 1 categorical V + 1 numerical V\n* Multivriate Analysis (Relationships between 2 numerical Vs)","3719315a":"### Performance Comparison Among 4 Models","55228af8":"## Choose Data Source\n\n* 0: Raw data\n* 1: Feature Selection\n* 2: PCA","23f40685":"# Customer Response Predict & Personality Analysis\n","af9b9114":"## Tasks Description\n\nThis dataset gives **2240** different customers basic information, their product purchasing preferences as well as their reactions to some marketing compaigns. I want to perform 2 tasks on this dataset:\n\n1) **Supervised Learning Task - Predict Response:** As the data description says, the column 'Response' stands for if certian customer accepted the offer in the last campaign. So the question is **whether we can use some customers' responses to this compaign to predict someone else's reactions** ? If we can achieve this, a business could promote the compaign to customers that are more likely to accept the offer, which could help it make more efficient marketing plan.   \n\n2) **Unsupervised Learning Task - Customer Personality Segmentation:** This dataset also gives us some information about customers(Including their basic information and purchasing preference). So we could perform **Customer Personality Analysis** to help find a company\u2019s ideal customers. It helps a business to better understand its customers and makes it easier for them to make proper market plans, like modifying and promoting products for different customers according to the specific needs, behaviors and concerns.\n\n\nBut before doing the tasks above, some **data preprocessing** work need to be done. Also, we need to do some **exporatory data analysis(EDA)** to help people better understand the dataset. ","93893201":"## Observations\n* The **income level** of this 2 groups shows clear difference. Cluster 1 have obvious higher income than Cluster 2.\n\n\n* Cluster 1 have less **kids**(as well as less children) at home. Most of them have no kids and only a few(about 5%) have 1 kids; While most customers in cluster 2 have 1 kid, and some have 2 kids.\n\n\n* Cluster 1 customers buy much more amount products than cluster 2 customers. Almost every products shows the same trend. This result indicates that people in cluster 1 have more **consumption power**, and they are more likely to purchase goods from the company. The **total number of accepting offers** in compaigns is also consistent with my conclusion. The group with more consumption power(cluster 1) accept more offers than the other.\n\n\n* Also, people in cluster 1 have much more **purchasing numbers** in different place. Among all these places, they may prefer to buy products in **real store**. It is not surprising since most of our customers are in their middle age or old age.\n\n\n* The last small obervation is that cluster 2 have some **extreme situations in product purchasing amount**. Some customer in cluster 2 purchasing unusual amount of products. One plausible assumption is that they might buy a lot of goods for special festivals or parties. Checking their purchasing date may verify this assumption.","7c8d10eb":"### Choosing K value\n\n* When we use inertia, Calinski_harabasz scorer and Silhouette scorer, we all get elbow point at K=2; \n\n* While Distortion scorer says that K=7, we could see distortion scores come at eblow point at about 3. \n\n* Considering all the clutering results, I finally choose **K=2** to cluster the customers. ","7cf4316e":"### Data Preparation for Classification Models","1e7d7aff":"* There 5 different values of Education, but according to 'Three Cycle System' from the European Higher Education Area, **2n Cycle** is equal to **Master degree**.\n* **Graduation** degree is the same as **Bachelor** in Europe. Thus, we adjust the education level of all customers.","cc04063e":"* From the left figure, we can find that the **compaign acceptance rate** in high education groups(Master and PhD) are higher than that in low education groups.\n\n* From the right plot, we find that the **single people** tend to say yes to this compaign. ","54e7a45e":"### Bivriate Analysis<a name='BA'><\/a>","2dd3b5e6":"* From the plot above, we could see tha these numerical variables do not have clear linear trend between each other.\n* While **Income** might have **log relationships** with these product purchasing amount (say, Mntfruit, MntSweetProducts and etc.).","7aa1dcc7":"### SVM<a name='SVM'><\/a>","6543cba0":"## Dataset Description\n\nFirst give a brief introduction to all attributes in the dataset we use:\n\n* People \n    - ID: Customer's unique identifier\n    - Year_Birth: Customer's birth year\n    - Education: Customer's education level\n    - Marital_Status: Customer's marital status\n    - Income: Customer's yearly household income\n    - Kidhome: Number of children in customer's household\n    - Teenhome: Number of teenagers in customer's household\n    - Dt_Customer: Date of customer's enrollment with the company\n    - Recency: Number of days since customer's last purchase\n    - Complain: 1 if customer complained in the last 2 years, 0 otherwise\n\n* Products\n    - MntWines: Amount spent on wine in last 2 years\n    - MntFruits: Amount spent on fruits in last 2 years\n    - MntMeatProducts: Amount spent on meat in last 2 years\n    - MntFishProducts: Amount spent on fish in last 2 years\n    - MntSweetProducts: Amount spent on sweets in last 2 years\n    - MntGoldProds: Amount spent on gold in last 2 years\n\n* Promotion\n    - NumDealsPurchases: Number of purchases made with a discount\n    - AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n    - AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n    - AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n    - AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n    - AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n    - **Response**: 1 if customer accepted the offer in the last campaign, 0 otherwise\n\n* Place\n    - NumWebPurchases: Number of purchases made through the company\u2019s web site\n    - NumCatalogPurchases: Number of purchases made using a catalogue\n    - NumStorePurchases: Number of purchases made directly in stores\n    - NumWebVisitsMonth: Number of visits to company\u2019s web site in the last month\n","59d67183":"* There are too many maritial status, which might affects the efficiency of classification algorithms.\n* Alone, Adsurd and YOLO are all specific situations of 'Single', so replace all of them with 'Single'"}}