{"cell_type":{"5fcaffc0":"code","a09266c9":"code","32145c56":"code","f3e06d15":"code","59f6fe2d":"code","f99ddbef":"code","8d641211":"code","b6ff0126":"code","19240849":"code","53984c32":"code","4556ca8c":"code","d96165f9":"code","218f930b":"code","5b14233c":"code","7e677b85":"code","4e328953":"code","873c3e02":"code","983c7e07":"code","25270d3a":"code","a818db60":"code","119c714c":"code","a0d304e0":"code","eebb9449":"code","53c19659":"code","f4c5ed4f":"code","03b9f357":"code","c2a6f28c":"code","202dcccf":"code","cd26cd81":"code","d9e27284":"code","24df6509":"code","69c28f45":"code","81ff31fe":"code","75b9e34b":"code","92495bce":"code","71a9589d":"code","e3cbcd90":"code","2ca75de2":"code","992d13f6":"code","44d4e5d9":"code","11505440":"code","be032863":"code","00dd192d":"code","9e8361de":"code","554313af":"code","e5f8e45d":"code","4bd2f44a":"code","e3eadd48":"code","1e3456ac":"code","9dc8f381":"code","0fb8aaad":"code","b58fe68a":"code","1364ddb6":"code","06f711ac":"code","392bd158":"code","0fa474f9":"code","1870be08":"code","1fef1640":"markdown","036cb68a":"markdown","a7be7c4e":"markdown","c55a613a":"markdown","c12a58c2":"markdown","84d2cc3d":"markdown","28390f13":"markdown","feff468f":"markdown","8d9a228d":"markdown"},"source":{"5fcaffc0":"# Dataset taken from https:\/\/www.kaggle.com\/iarunava\/cell-images-for-detecting-malaria","a09266c9":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","32145c56":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.metrics import error_rate\nimport numpy as np","f3e06d15":"torch.cuda.set_device(0)","59f6fe2d":"torch.cuda.is_available(), torch.backends.cudnn.enabled","f99ddbef":"import fastai.utils.collect_env; fastai.utils.collect_env.show_install(1)","8d641211":"path = Path('..\/input\/cell_images\/cell_images'); path","b6ff0126":"path.ls()","19240849":"import os","53984c32":"np.random.seed(42)","4556ca8c":"path_image_parasitized = Path('..\/input\/cell_images\/cell_images\/Parasitized\/')\npath_image_uninfected = Path('..\/input\/cell_images\/cell_images\/Uninfected\/')","d96165f9":"parasitized_images = get_image_files(path_image_parasitized)[:10]\nparasitized_images","218f930b":"path_para, dirs, files = next(os.walk(path_image_parasitized))\nfile_count = len(files)\nprint(\"Number of images for parasitized:\", file_count)","5b14233c":"images_para = open_image(parasitized_images[np.random.randint(0, 9)])\nimages_para.show(figsize=(5,5))\nimages_para.size","7e677b85":"uninfected_images = get_image_files(path_image_uninfected)[:10]\nuninfected_images","4e328953":"path_unin, dirs, files = next(os.walk(path_image_uninfected))\nfile_count = len(files)\nprint(\"Number of images for uninfected:\", file_count)","873c3e02":"images_unin = open_image(uninfected_images[np.random.randint(0, 9)])\nimages_unin.show(figsize=(5,5))\nimages_unin.size","983c7e07":"path","25270d3a":"data = ImageDataBunch.from_folder(path, train=\".\",\n                                  valid_pct=0.2, # Splits the dataset into 80\/20% training\/validation\n                                  ds_tfms=get_transforms(do_flip = True, flip_vert = True, max_warp=0), # AFAIK these types of images can be flipped any direction vertically, horizontally, 90 degrees in actual cell images\n                                  size=224,bs=64 # Trying out a larger 256 size at first\n                                 ).normalize(imagenet_stats)","a818db60":"data.show_batch(rows=3, figsize=(15,14))","119c714c":"print(data.classes)\nlen(data.classes),data.c","a0d304e0":"# Trying out mixed precision learning\n# https:\/\/docs.fast.ai\/callbacks.fp16.html\n# https:\/\/docs.nvidia.com\/deeplearning\/sdk\/mixed-precision-training\/index.html\n# Update: Did not work. Error: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same\n\nlearn = cnn_learner(data, models.resnet34, metrics=error_rate, model_dir=\"\/kaggle\/model\")","eebb9449":"learn.model","53c19659":"learn.fit_one_cycle(4)","f4c5ed4f":"learn.save('\/kaggle\/working\/malaria_resnet34_initial_training')","03b9f357":"interp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\nlen(data.valid_ds)==len(losses)==len(idxs)","c2a6f28c":"# Note: these are images that are in the top_losses section. Model was not sure what these were \n# TODO: Ask someone with biology background to predict these\ninterp.plot_top_losses(9, figsize=(15,14),  heatmap = False)","202dcccf":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","cd26cd81":"learn.unfreeze()","d9e27284":"learn.fit_one_cycle(1)","24df6509":"learn.lr_find()\nlearn.recorder.plot()","69c28f45":"learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-3))","81ff31fe":"interp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\nlen(data.valid_ds)==len(losses)==len(idxs)","75b9e34b":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","92495bce":"learn.save('\/kaggle\/working\/malaria_resnet34_unfrozen_and_tuned')","71a9589d":"# learn.load('malaria_unfrozen_and_tuned')","e3cbcd90":"data = ImageDataBunch.from_folder(path, train=\".\",\n                                  valid_pct=0.2, # Splits the dataset into 80\/20% training\/validation\n                                  ds_tfms=get_transforms(flip_vert = True), # AFAIK images can be flipped any direction vertically, horizontally, 90 degrees in actual cell images\n                                  size=299,bs=32\n                                 ).normalize(imagenet_stats)","2ca75de2":"learn.data = data\ndata.train_ds[0][0].shape","992d13f6":"learn.freeze()","44d4e5d9":"learn.lr_find()\nlearn.recorder.plot()","11505440":"learn.fit_one_cycle(10, slice(1e-3\/2))","be032863":"learn.save('\/kaggle\/working\/malaria_initial_training_resnet50')","00dd192d":"learn.unfreeze()","9e8361de":"learn.lr_find()\nlearn.recorder.plot()","554313af":"learn.fit_one_cycle(5, slice(1e-4, (1e-3)\/5))\n\n# learn.fit_one_cycle(8, 3e-3)\n# https:\/\/youtu.be\/ccMHJeQU4Qw?t=3552 \u00af\\_(\u30c4)_\/\u00af","e5f8e45d":"learn.recorder.plot_losses()","4bd2f44a":"learn.save('\/kaggle\/working\/malaria_resnet50_unfrozen_and_tuned')","e3eadd48":"# Wrongly implemented transfer learning","1e3456ac":"#learn = cnn_learner(data, models.resnet50, metrics=error_rate, model_dir=\"\/kaggle\/model\")","9dc8f381":"# learn.lr_find()\n# learn.recorder.plot()","0fb8aaad":"# learn.fit_one_cycle(8, 3e-3)\n# https:\/\/youtu.be\/ccMHJeQU4Qw?t=3552 \u00af\\_(\u30c4)_\/\u00af","b58fe68a":"# learn.save('\/kaggle\/working\/malaria_initial_training_resnet50')","1364ddb6":"# learn.lr_find()\n# learn.recorder.plot()","06f711ac":"#TODO: Figure out how many epochs before this starts doing badly. Just starting to get good! Could push a couple more.\n\n# learn.unfreeze()\n# learn.fit_one_cycle(15, max_lr=slice(1e-6,1e-4))","392bd158":"interp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\nlen(data.valid_ds)==len(losses)==len(idxs)","0fa474f9":"interp.plot_top_losses(9, figsize=(15,14), heatmap = False)\n\n# Observation - some of these uninfected have the blob in the middle that make it look like an infected blood cell\n# and vice versa. Some are clean that are labeled as infected.\n# I wonder if these are mislabeled.","1870be08":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","1fef1640":"# Model Training","036cb68a":"# But 97% accuracy is probably too low for medical diagnosis. I'm guessing they want 99.9% or higher\n> <input type=\"checkbox\"> ResNet50","a7be7c4e":"## Observations: \n- Data is evenly balanced in number of images for uninfected and infected\n- Images aren't evenly sized.\n- I assume purple cells are white blood cells and peach are red blood cells\n- Not positive on what ThinF means. I assume it means thin slices of cells\n- Unsure what the various versions of ReThinF, NThinF, ThinF, thinF, etc. denotes in the images\n","c55a613a":"# Examining results of initial training","c12a58c2":"# Data Bunching\n<input type=\"checkbox\"> Combine uninfected & infected for training set\n<input type=\"checkbox\"> Resize images to uniform size\n","84d2cc3d":"# Reexamining results\nImprovement! \ud83d\ude4c","28390f13":"# Data Exploration","feff468f":"# Conclusions (for now)\n- Time could have been saved if I had questioned the results more after my ResNet34 fine tuning. The ResNet50 took a day and a half after and I didn't really look at them too carefully. Sort of just ignored it because I wanted ResNet50 tuning practice. While there was slight improvement, my results could be summed up as the follow: training a 97% accurate model took 3-4 hours while adding an extra .5% accuracy took a day and a half.\n- It seems that some of these images are indeed mislabeled. The most blatantly obvious ones would be the ones labeled parasitized when they are uninfected.\n- The malaria infected\/parasitized images in the plot_top_losses function are inconclusive. After asking several subject matter experts on Reddit there was a general hesitance to qualify an answer in either direction. Higher resolution images would be needed to draw a conclusion.\n- Other more diagnostic and higher quality images were linked in the Reddit thread below. A future to-do could be to train addendums featuring said images with a slightly higher learning rate\n\nSources (Thanks Reddit!)\n- https:\/\/www.reddit.com\/r\/pathology\/comments\/bt1dxn\/im_a_data_science_student_and_i_made_a_classifier\/\n- https:\/\/www.reddit.com\/r\/medlabprofessionals\/comments\/bt30l2\/im_a_data_science_student_and_i_made_a_classifier\/","8d9a228d":"# Model improvement\n<input type=\"checkbox\"> Unfreeze layer 1. All the information gleamed off resnet34 to form basic shapes as I understand it.\n<input type=\"checkbox\"> Fine tune learning rate"}}