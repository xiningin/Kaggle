{"cell_type":{"dfb1db98":"code","86ab4315":"code","4a012af7":"code","e6441bf5":"code","b3ec53fc":"code","8e3b1cba":"code","7e51d27a":"code","491cb14a":"code","e2bbd13b":"code","e5c9976e":"markdown","6d456758":"markdown","f3b0ee6a":"markdown","c3795b05":"markdown"},"source":{"dfb1db98":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nimport pandas_profiling as pp\nfrom sklearn import preprocessing\nfrom sklearn import ensemble\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor","86ab4315":"df = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')","4a012af7":"# Take a quick look at the dataset\nprint(f\"Shape of the BMW dataset: {df.shape}\\n\")\n\ndf.info()\n\ndf.head()","e6441bf5":"eda = pp.ProfileReport(df, title=\"Exploratory Data Analysis of the BMW dataset\", explorative=True)\neda.to_notebook_iframe()","b3ec53fc":"df_train, df_test = model_selection.train_test_split(df, test_size=0.2, random_state=42)","8e3b1cba":"df_train['kfold'] = -1\ndf_train = df_train.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)","7e51d27a":"# Seperate numerical features and categorical features\nfeats = [col for col in df_train.columns if col not in ('price', 'kfold')]\ncat_feats = [col for col in feats if df_train[col].dtype == 'object']\nnum_feats = [col for col in feats if df_train[col].dtype != 'object']\nprint(f\"Numerical features: {num_feats}\\nCategorical features: {cat_feats}\\n\")","491cb14a":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_indices, test_indices) in enumerate(kf.split(X=df_train)):\n    df_train.loc[test_indices, 'kfold'] = fold","e2bbd13b":"for fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    X_test = df_test.copy()\n    \n    y_train = X_train.price\n    y_valid = X_valid.price\n    y_test = X_test.price\n    \n    X_train = X_train[feats]\n    X_valid = X_valid[feats]\n    X_test = X_test[feats]\n    \n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore')\n    X_train_ohe = pd.DataFrame(ohe.fit_transform(X_train[cat_feats]))\n    X_valid_ohe = pd.DataFrame(ohe.transform(X_valid[cat_feats]))\n    X_test_ohe = pd.DataFrame(ohe.transform(X_test[cat_feats]))\n    \n    X_train = pd.concat([X_train_ohe, X_train[num_feats]], axis=1)\n    X_valid = pd.concat([X_valid_ohe, X_valid[num_feats]], axis=1)\n    X_test = pd.concat([X_test_ohe, X_test[num_feats]], axis=1)\n    # Baseline\n    #model = ensemble.RandomForestRegressor(n_estimators=1200, random_state=42)\n    model = XGBRegressor(n_estimators=1000, learning_rate=0.05, random_state=fold, \n                         tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    model.fit(X_train, y_train,\n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\n    mse = mean_squared_error(model.predict(X_test), y_test, squared=False)\n    print(f'Mean squared error of fold {fold}: {mse}')","e5c9976e":"# Import the BMW dataset","6d456758":"# Auto EDA","f3b0ee6a":"The dataset has 9 columns and 10,781 rows without any missing value","c3795b05":"# Improvement\nThis is just baseline. There are something that need to be improved:\n* Apply GridSearch to evaluate many models\n* Feature engineering\n* Create pipelines"}}