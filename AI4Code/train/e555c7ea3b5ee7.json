{"cell_type":{"88cbde98":"code","06920d5e":"code","135be981":"code","fd663518":"code","54365285":"code","ef2db996":"code","d2b506ec":"code","4ff85a28":"code","e10cce06":"code","ede7ab86":"code","e5d85176":"code","2849c53b":"code","3b6a9454":"code","95b3ffbd":"code","5f845a96":"code","45c0be94":"code","a9c32b16":"code","99ecef8c":"code","719b96b1":"code","b67d9ccd":"code","83345c48":"code","3b1da5f4":"code","9f49e738":"code","259965f5":"code","c5f1bd46":"code","5b038260":"code","44a01582":"code","4c7ad6d3":"code","5b10ddec":"code","cf044f66":"code","8ceef5b9":"code","778f6edc":"code","9794336e":"code","3fca6c2f":"code","1dc7c8da":"code","6afdb963":"code","5557388a":"code","b9d5e014":"code","a8b771c1":"code","13a10d02":"code","106914c7":"code","4ad6b57f":"code","3d9b8d95":"code","d8ebda7e":"code","11690bc6":"code","f9bef09b":"code","8df939ee":"code","8bb2f693":"code","c086fb25":"code","e2503a8f":"code","2b150ceb":"code","b6660665":"code","a64d6bd1":"markdown","1ef24a89":"markdown","15f84346":"markdown","60d88d56":"markdown","a9f8ab9a":"markdown","48bb302d":"markdown","a31c7249":"markdown","4b72478f":"markdown","c90dbf50":"markdown","9fb20bed":"markdown","bd579b97":"markdown","832ba79f":"markdown","18c4b9f8":"markdown","5c89266a":"markdown","7c952b28":"markdown","c67b3106":"markdown"},"source":{"88cbde98":"# installation required for Spark\n#!pip install sparkmagic\n#!pip install pyspark\n\n# libraries\nimport warnings\n# import findspark\nimport pandas as pd\nimport seaborn as sns\nfrom pyspark.ml.classification import GBTClassifier, LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, StandardScaler\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import Bucketizer\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.2f' % x)","06920d5e":"#!pip install sparkmagic\n#!pip install pyspark","135be981":"spark = SparkSession.builder.master(\"local[*]\").getOrCreate()","fd663518":"spark_df = spark.read.csv('..\/input\/bank-churn-prediction\/churn2.csv', inferSchema=True, header=True)\nspark_df.show(10)","54365285":"print(\"Shape: \", (spark_df.count(), len(spark_df.columns)))","ef2db996":"# De\u011fi\u015fken tipleri\nspark_df.printSchema()","d2b506ec":"spark_df.show(5)","4ff85a28":"# De\u011fi\u015fken isimlerinin k\u00fc\u00e7\u00fclt\u00fclmesi\nspark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns])\nspark_df.show(5)","e10cce06":"# \u00f6zet istatistikler\nspark_df.describe().show()","ede7ab86":"# sadece belirli de\u011fi\u015fkenler i\u00e7in \u00f6zet istatistikler\nspark_df.describe([\"age\", \"exited\"]).show()","e5d85176":"# Kategorik de\u011fi\u015fken s\u0131n\u0131f istatistikleri\nspark_df.groupby(\"exited\").count().show()","2849c53b":"# e\u015fsiz s\u0131n\u0131flar\nspark_df.select(\"exited\").distinct().show()","3b6a9454":"# groupby transactions\nspark_df.groupby(\"exited\").count().show()","95b3ffbd":"spark_df.groupby(\"exited\").agg({\"tenure\": \"mean\"}).show()","5f845a96":"#  T\u00fcm numerik de\u011fi\u015fkenlerin se\u00e7imi ve \u00f6zet istatistikleri\nnum_cols = [col[0] for col in spark_df.dtypes if col[1] != 'string']\nspark_df.select(num_cols).describe().show()","45c0be94":"# T\u00fcm kategorik de\u011fi\u015fkenlerin se\u00e7imi ve \u00f6zeti\ncat_cols = [col[0] for col in spark_df.dtypes if col[1] == 'string']","a9c32b16":"# hedef de\u011fi\u015fkene g\u00f6re say\u0131sal de\u011fi\u015fkenlerin ortalamas\u0131\nfor col in [col.lower() for col in num_cols]:\n    spark_df.groupby(\"exited\").agg({col: \"mean\"}).show()","99ecef8c":"from pyspark.sql.functions import when, count, col\nspark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns]).toPandas().T","719b96b1":"# gerekli olmayan kolonlar\u0131n silinmesi\nspark_df = spark_df.drop('rownumber', \"customerid\", \"surname\")","b67d9ccd":"spark_df = spark_df.withColumn('creditscore_salary', spark_df.creditscore \/ spark_df.estimatedsalary)\nspark_df = spark_df.withColumn('creditscore_tenure', spark_df.creditscore * spark_df.tenure)\nspark_df = spark_df.withColumn('balance_salary', spark_df.balance \/ spark_df.estimatedsalary)\nspark_df.show(5)","83345c48":"# age de\u011fi\u015fkeni\nspark_df.select('age').describe().toPandas().transpose()\nspark_df.select(\"age\").summary(\"count\", \"min\", \"25%\", \"50%\",\"75%\", \"max\").show()\nbucketizer = Bucketizer(splits=[0, 35, 55, 75, 95], inputCol=\"age\", outputCol=\"age_cat\")\nspark_df = bucketizer.setHandleInvalid(\"keep\").transform(spark_df)\nspark_df = spark_df.withColumn('age_cat', spark_df.age_cat + 1)","3b1da5f4":"spark_df.groupby(\"age_cat\").count().show()","9f49e738":"spark_df.groupby(\"age_cat\").agg({'exited': \"mean\"}).show()","259965f5":"# float de\u011ferleri intere \u00e7evirmek\nspark_df = spark_df.withColumn(\"age_cat\", spark_df[\"age_cat\"].cast(\"integer\"))","c5f1bd46":"# WHEN KULLANIMI\nspark_df.withColumn('creditscore_2',\n                    when(spark_df['creditscore'] < 301, \"deep\").\n                    when((301 < spark_df['creditscore']) & (spark_df['creditscore'] < 601), \"very poor\").\n                    when((500 < spark_df['creditscore']) & (spark_df['creditscore'] < 601), \"poor\").\n                    when((601 < spark_df['creditscore']) & (spark_df['creditscore'] < 661), \"fair\").\n                    when((661 < spark_df['creditscore']) & (spark_df['creditscore'] < 781), \"good\").\n                    when((781 < spark_df['creditscore']) & (spark_df['creditscore'] < 851), \"excellent\").\n                    otherwise(\"top\")).show()\n","5b038260":"from pyspark.sql.types import IntegerType, StringType, FloatType\nfrom pyspark.sql.functions import udf\n\n# udf ile fonksiyon yazma\ndef segment(tenure):\n    if tenure < 5:\n        return \"segment_b\"\n    else:\n        return \"segment_a\"\n\nfunc_udf = udf(segment, StringType())\nspark_df = spark_df.withColumn('segment', func_udf(spark_df['tenure']))\nspark_df.show(5)","44a01582":"spark_df.groupby(\"segment\").count().show()","4c7ad6d3":"indexer = StringIndexer(inputCol=\"segment\", outputCol=\"segment_label\")\nindexer.fit(spark_df).transform(spark_df).show(5)\ntemp_sdf = indexer.fit(spark_df).transform(spark_df)\nspark_df = temp_sdf.withColumn(\"segment_label\", temp_sdf[\"segment_label\"].cast(\"integer\"))\nspark_df = spark_df.drop('segment')","5b10ddec":"indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_label\")\nindexer.fit(spark_df).transform(spark_df).show(5)\ntemp_sdf = indexer.fit(spark_df).transform(spark_df)\nspark_df = temp_sdf.withColumn(\"gender_label\", temp_sdf[\"gender_label\"].cast(\"integer\"))\nspark_df = spark_df.drop('gender')","cf044f66":"spark_df.show(5)","8ceef5b9":"indexer = StringIndexer(inputCol=\"geography\", outputCol=\"geography_label\")\nindexer.fit(spark_df).transform(spark_df).show(5)\ntemp_sdf = indexer.fit(spark_df).transform(spark_df)\nspark_df = temp_sdf.withColumn(\"geography_label\", temp_sdf[\"geography_label\"].cast(\"integer\"))\nspark_df = spark_df.drop('geography')","778f6edc":"spark_df.show(5)","9794336e":"encoder = OneHotEncoder(inputCols=[\"age_cat\", \"geography_label\"], outputCols=[\"age_cat_ohe\", \"geography_label_ohe\"])\nspark_df = encoder.fit(spark_df).transform(spark_df)","3fca6c2f":"stringIndexer = StringIndexer(inputCol='exited', outputCol='label')\n\ntemp_sdf = stringIndexer.fit(spark_df).transform(spark_df)\ntemp_sdf.show()","1dc7c8da":"spark_df = temp_sdf.withColumn(\"label\", temp_sdf[\"label\"].cast(\"integer\"))\nspark_df.show(5)","6afdb963":"cols = ['creditscore', 'age', 'tenure', 'balance','numofproducts', 'hascrcard',\n        'isactivemember', 'estimatedsalary', 'creditscore_salary', 'creditscore_tenure',\n        'balance_salary', 'segment_label', 'gender_label',\n        'age_cat_ohe', 'geography_label_ohe']","5557388a":"# Vectorize independent variables.\nva = VectorAssembler(inputCols=cols, outputCol=\"features\")\nva_df = va.transform(spark_df)\nva_df.show()","b9d5e014":"# Final sdf\nfinal_df = va_df.select(\"features\", \"label\")\nfinal_df.show(5)","a8b771c1":"# StandardScaler\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\nfinal_df = scaler.fit(final_df).transform(final_df)","13a10d02":"# Split the dataset into test and train sets.\ntrain_df, test_df = final_df.randomSplit([0.7, 0.3], seed=17)\ntrain_df.show(5)","106914c7":"test_df.show(5)","4ad6b57f":"print(\"Training Dataset Count: \" + str(train_df.count()))\nprint(\"Test Dataset Count: \" + str(test_df.count()))","3d9b8d95":"log_model = LogisticRegression(featuresCol='features', labelCol='label').fit(train_df)\ny_pred = log_model.transform(test_df)\ny_pred.show()","d8ebda7e":"y_pred.select(\"label\", \"prediction\").show()","11690bc6":"# accuracy\ny_pred.filter(y_pred.label == y_pred.prediction).count() \/ y_pred.count()","f9bef09b":"evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\nevaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")","8df939ee":"evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\nevaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n\nacc = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"accuracy\"})\nprecision = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"precisionByLabel\"})\nrecall = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"recallByLabel\"})\nf1 = evaluatorMulti.evaluate(y_pred, {evaluatorMulti.metricName: \"f1\"})\nroc_auc = evaluator.evaluate(y_pred)\n\nprint(\"accuracy: %f, precision: %f, recall: %f, f1: %f, roc_auc: %f\" % (acc, precision, recall, f1, roc_auc))","8bb2f693":"gbm = GBTClassifier(maxIter=100, featuresCol=\"features\", labelCol=\"label\")\ngbm_model = gbm.fit(train_df)\ny_pred = gbm_model.transform(test_df)\ny_pred.show(5)","c086fb25":"y_pred.filter(y_pred.label == y_pred.prediction).count() \/ y_pred.count()","e2503a8f":"evaluator = BinaryClassificationEvaluator()\n\ngbm_params = (ParamGridBuilder()\n              .addGrid(gbm.maxDepth, [2, 4, 6])\n              .addGrid(gbm.maxBins, [20, 30])\n              .addGrid(gbm.maxIter, [10, 20])\n              .build())","2b150ceb":"cv = CrossValidator(estimator=gbm,\n                    estimatorParamMaps=gbm_params,\n                    evaluator=evaluator,\n                    numFolds=5)","b6660665":"cv_model = cv.fit(train_df)\ny_pred = cv_model.transform(test_df)\nac = y_pred.select(\"label\", \"prediction\")\nac.filter(ac.label == ac.prediction).count() \/ ac.count()","a64d6bd1":"<a id = \"4\"><\/a><br>\n# 4. Data Preprocessing & Feature Engineering\n\n<a id = \"5\"><\/a><br>\n## 4.1  Missing Values","1ef24a89":"<a id = \"13\"><\/a><br>\n# 5. Modeling\n\n<a id = \"14\"><\/a><br>\n## 5.1 Logistic Regression","15f84346":"<a id = \"3\"><\/a><br>\n# 3. Exploratory Data Analysis","60d88d56":"<a id = \"11\"><\/a><br>\n## 4.7 Defining TARGET","a9f8ab9a":"# 2. Creating A Spark Session","48bb302d":"<a id = \"8\"><\/a><br>\n## 4.4 User Defined Functions (UDFs)","a31c7249":"<a id = \"7\"><\/a><br>\n## 4.3 Bucketization \/ Bining \/ Num to Cat","4b72478f":"<a id = \"1\"><\/a><br>\n# 1. Libraries","c90dbf50":"<a id = \"9\"><\/a><br>\n# 4.5 Label Encoding","9fb20bed":"<a id = \"6\"><\/a><br>\n## 4.2 Feature Interaction","bd579b97":"<a id = \"17\"><\/a><br>\n# 6. References\n* https:\/\/github.com\/mvahit\n* https:\/\/github.com\/mathchi\n* https:\/\/github.com\/rabia-koc\n* https:\/\/www.kaggle.com\/tylerx\/machine-learning-with-spark","832ba79f":"<a id = \"15\"><\/a><br>\n## 5.2 Gradient Boosted Tree Classifier","18c4b9f8":"<a id = \"16\"><\/a><br>\n# 5.3 Model Tuning","5c89266a":"# Introduction\nAs we know, it is much more expensive to sign in a new client than keeping an existing one.\n\nIt is advantageous for banks to know what leads a client towards the decision to leave the company.\n\nChurn prevention allows companies to develop loyalty programs and retention campaigns to keep as many customers as possible.\n\n* **Surname:** corresponds to the record (row) number and has no effect on the output.\n* **CreditScore:** contains random values and has no effect on customer leaving the bank.\n* **Geography:** a customer\u2019s location can affect their decision to leave the bank.\n* **Gender:** it\u2019s interesting to explore whether gender plays a role in a customer leaving the bank.\n* **Age:** this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n* **Tenure:** refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n* **NumOfProducts:** refers to the number of products that a customer has purchased through the bank.\n* **HasCrCard:** denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n* **IsActiveMember:** active customers are less likely to leave the bank.\n* **EstimatedSalary:** as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n* **Exited:** (Dependent Variable): whether or not the customer left the bank.\n* **Balance:** also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n\n\n<font color = 'blue'>\nContent: \n\n1. [Libraries](#1)\n1. [Creating Spark Session](#2)\n1. [Exploratory Data Analysis](#3)\n1. [Data Preprocessing & Feature Engineering](#4)\n   * 4.1 [Missing Values](#5)\n   * 4.2 [Feature Intraction](#6)\n   * 4.3 [Bucketization \/ Bining \/ Num to Cat](#7)\n   * 4.4 [User Defined Functions (UDFs)](#8)\n   * 4.5 [Label Encoding](#9)\n   * 4.6 [One Hot Encoding](#10)\n   * 4.7 [Defining Target](#11)\n   * 4.8 [Defining Features](#12)\n1. [Modeling](#13)\n   * 5.1 [Logistic Regression](#14)\n   * 5.2 [Gradient Boosted Tree Classifier](#15)\n   * 5.3 [Model Tuning](#16)\n1. [References](#17)\n","7c952b28":"<a id = \"10\"><\/a><br>\n## 4.6 One Hot Encoding","c67b3106":"<a id = \"12\"><\/a><br>\n## 4.8 Defining Features"}}