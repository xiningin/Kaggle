{"cell_type":{"19ca008f":"code","b6fc1d17":"code","3089f39c":"code","ba53b544":"code","0e89326b":"code","e97c7cfb":"code","b81ba2dc":"code","dabece41":"code","713693e4":"code","a9fc870b":"code","16f83312":"code","8d5fa7e7":"code","3ebd772b":"code","0e437963":"code","4f6adacc":"code","d1c35280":"code","4ee616ca":"code","bddc350d":"code","f91e2419":"code","761e106b":"code","c3945c1e":"code","e7a14332":"code","62d06fbc":"code","91a8f44a":"code","881dd760":"code","b7ea9601":"code","7381b314":"code","7fa7fbec":"code","c95c96b7":"code","0dc156ae":"markdown","ec9e0e63":"markdown","af13b75a":"markdown","a20397ca":"markdown","c10d439e":"markdown","eb85a7da":"markdown","91c215a3":"markdown","680e0b52":"markdown","1ca22eae":"markdown","af448b3e":"markdown","74736054":"markdown","a2873920":"markdown","e5144904":"markdown","e93af277":"markdown","82512949":"markdown","952b0d63":"markdown","7930c856":"markdown","9933923a":"markdown","5c8d7e37":"markdown","7447a128":"markdown","d8601639":"markdown","ed5f3247":"markdown","e7d5c2ad":"markdown","1f7907cc":"markdown","e48ab0f1":"markdown","ea08459e":"markdown","ea609c40":"markdown","df1cf3e1":"markdown","e44bbfb5":"markdown","31ca3fad":"markdown","d3df8a52":"markdown","bbd1419a":"markdown","2a1d3a71":"markdown","07a8f5f3":"markdown"},"source":{"19ca008f":"import numpy as np \nimport pandas as pd \nfrom collections import OrderedDict\n\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.io as pio\nimport cufflinks as cf\n!pip install circlify\nimport circlify\nimport matplotlib.pyplot as plt\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\npy.init_notebook_mode(connected=True)\n%config InlineBackend.figure_format = 'retina' \npio.templates.default = \"plotly_white\"\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv('..\/input\/kaggle-data-science-survey-20172021\/kaggle_survey_2017_2021.csv',low_memory=False)\ndata.drop('Time from Start to Finish (seconds)',axis=1,inplace=True)\ndata.replace('United Kingdom of Great Britain and Northern Ireland','United Kingdom',inplace = True)\ndata.replace('Viet Nam','Vietnam',inplace=True)\ndata.replace('Iran, Islamic Republic of...','Iran',inplace=True)\ndata.rename(columns={'-': 'Year'}, inplace=True)\n\ndf_2021 = data[data['Year'] == '2021']\ndf_2020 = data[data['Year'] == '2020']\ndf_2019 = data[data['Year'] == '2019']\ndf_2018 = data[data['Year'] == '2018']\ndf_2017 = data[data['Year'] == '2017']\n\n\ndf2021 = pd.read_csv('..\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv',low_memory=False)\ndf2021.drop(0,inplace=True)\n\ncolor = '#00c0c7'\ncolors = [\"#e60049\", \"#0bb4ff\", \"#50e991\", \"#e6d800\", \"#9b19f5\", \"#ffa300\", \"#dc0ab4\", \"#b3d4ff\", \"#00bfa0\",\"#0230c7\",\"#00c707\",\"#008b05\"]\n\n'''\nAdapted from https:\/\/www.kaggle.com\/paultimothymooney\/2020-kaggle-data-science-machine-learning-survey\n'''\n\ndef count_then_return_percent(dataframe,column_name):\n    '''\n    A helper function to return value counts as percentages.\n    '''\n    \n    counts = dataframe[column_name].value_counts(dropna=False)\n    percentages = round(counts*100\/(dataframe[column_name].count()),1)\n    return percentages\n\ndef count_then_return_percent_for_multiple_column_questions(dataframe,list_of_columns_for_a_single_question,dictionary_of_counts_for_a_single_question):\n    '''\n    A helper function to convert counts to percentages.\n    '''\n    \n    df = dataframe\n    subset = list_of_columns_for_a_single_question\n    df = df[subset]\n    df = df.dropna(how='all')\n    total_count = len(df) \n    dictionary = dictionary_of_counts_for_a_single_question\n    for i in dictionary:\n        dictionary[i] = round(float(dictionary[i]*100\/total_count),1)\n    return dictionary\n\ndef create_dataframe_of_counts(dataframe,column,rename_index,rename_column,return_percentages=False):\n    '''\n    A helper function to create a dataframe of either counts \n    or percentages, for a single multiple choice question. \n    '''\n    df = dataframe[column].value_counts().reset_index() \n    if return_percentages==True:\n        df[column] = (df[column]*100)\/(df[column].sum())\n    df = pd.DataFrame(df) \n    df = df.rename({'index':rename_index, 'Q3':rename_column}, axis='columns')\n    return df\n\ndef sort_dictionary_by_percent(dataframe,list_of_columns_for_a_single_question,dictionary_of_counts_for_a_single_question): \n    ''' \n    A helper function that can be used to sort a dictionary.\n    \n    It is an adaptation of a similar function\n    from https:\/\/www.kaggle.com\/sonmou\/what-topics-from-where-to-learn-data-science.\n    '''\n    dictionary = count_then_return_percent_for_multiple_column_questions(dataframe,\n                                                                list_of_columns_for_a_single_question,\n                                                                dictionary_of_counts_for_a_single_question)\n    dictionary = {v:k    for(k,v) in dictionary.items()}\n    list_tuples = sorted(dictionary.items(), reverse=False) \n    dictionary = {v:k for (k,v) in list_tuples}   \n    return dictionary\n\n\ndef binarize_data(df):\n    '''\n    A utility function to binarize the data.\n    '''\n    df_cols = df.columns\n    for col in df.columns:\n        if \"_Part_\" in col or 'OTHER' in col:\n            k = df[col].value_counts()\n            df[col] = df[col].replace(k.keys()[0],True)\n            df[col] = df[col].fillna(False)\n            df[col] = df[col].astype('bool')\n    return df\n\ndef generateColumnNames(qNo,parts):\n    '''\n    A utility function to generate columns names in the formart 'Q12_Part_1'\n    '''\n    colNames = []\n    for i in range(1,parts+1):\n        colNames.append('Q'+str(qNo)+'_Part_'+str(i))\n    return colNames\n\ndef mergeDict (keys, values):\n    '''\n    A helper function to merge values of different dictionaries\n    '''\n    set_keys=[set(list(i)).pop() for i in zip(*keys)]\n    set_values=map(list,zip(*values))\n    return dict(zip(set_keys,set_values))\n\ndef getQ17Count(df):\n    q17_count = {\n        'Linear or Logistic Regression' : (df['Q17_Part_1'].count()),\n        'Decision Trees or Random Forests': (df['Q17_Part_2'].count()),\n        'Gradient Boosting Machines' : (df['Q17_Part_3'].count()),\n        'Bayesian Approaches' : (df['Q17_Part_4'].count()),\n        'Evolutionary Approaches' : (df['Q17_Part_5'].count()),\n        'Dense Neural Networks' : (df['Q17_Part_6'].count()),\n        'Convolutional Neural Networks' : (df['Q17_Part_7'].count()),\n        'Generative Adversarial Networks' : (df['Q17_Part_8'].count()),\n        'Recurrent Neural Networks' : (df['Q17_Part_9'].count()),\n        'Transformer Networks' : (df['Q17_Part_10'].count()),\n        'None' : (df['Q17_Part_11'].count())\n    }\n    return q17_count\n\n\nq18_count = {\n    'General purpose image\/video tools' : (df_2021['Q18_Part_1'].count()),\n    'Image segmentation methods': (df_2021['Q18_Part_2'].count()),\n    'Object detection methods' : (df_2021['Q18_Part_3'].count()),\n    'Image classification and other general purpose networks' : (df_2021['Q18_Part_4'].count()),\n    'Generative Networks' : (df_2021['Q18_Part_5'].count()),\n    'None' : (df_2021['Q18_Part_6'].count()),\n    'Other' : (df_2021['Q18_OTHER'].count())\n}\n\nq19_count = {\n    'Word embeddings\/vectors' : (df_2021['Q19_Part_1'].count()),\n    'Encoder-decoder models': (df_2021['Q19_Part_2'].count()),\n    'Contextualized embeddings' : (df_2021['Q19_Part_3'].count()),\n    'Transformer language models' : (df_2021['Q19_Part_4'].count()),\n    'None' : (df_2021['Q19_Part_5'].count()),\n    'Other' : (df_2021['Q19_OTHER'].count())\n}\n\n","b6fc1d17":"val = (abs(len(df_2021) - len(df_2017)) \/ len(df_2017)) * 100.0\nnoOfRespondents = [len(df_2017), len(df_2018), len(df_2019), len(df_2020), len(df_2021)]\nfig1 = go.Bar(x=['2017', '2018', '2019', '2020', '2021'], y=noOfRespondents, text=noOfRespondents,\n              width=0.4, textposition='auto',\n              marker_color=color)\n\nfig2 = go.Indicator(\n    domain={'row': 1, 'column': 3},\n    value=val,\n    mode='number+delta',\n    delta={'reference': 0},\n    gauge={'axis': {'range': [0, 100]}})\n\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    specs=[[{'type': 'bar'}, {'type': 'indicator'}]],\n    subplot_titles=('Kaggle Survery Respondents Trend', '55% Surge from 2017 -> 2021'))\nfig.append_trace(fig1, row=1, col=1)\nfig.append_trace(fig2, row=1, col=2)\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","3089f39c":"df_2021.replace('I do not use machine learning methods','Do not use machine learning methods',inplace = True)\ndf_exp = df_2021.dropna(subset=['Q15'])\nk = df_exp['Q15'].value_counts().sort_values(ascending=False)\nk = (k \/ len(df_exp)) * (100)\nk = k.round(0).astype(int)\nexperience = pd.DataFrame(k.items(), columns=['Experience', '% of respondents'])\ntrace = go.Funnel(\n    y=experience[\"Experience\"],\n    x=experience['% of respondents'], marker=dict(color=color))\n\nlayout = go.Layout(barmode=\"group\", width=1000, height=500, showlegend=False)\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(title='Fig 1.1 Machine Learning Methods Usage Experience', width=800,height=500)\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","ba53b544":"algos_2021 = sort_dictionary_by_percent(df_2021, generateColumnNames(17, 11), getQ17Count(df_2021))\nml_methods = pd.DataFrame(algos_2021.items(), columns=['Algorithms', '% of respondents'])\ntrace = go.Bar(y=ml_methods[\"Algorithms\"], x=ml_methods['% of respondents'],\n               orientation='h', marker=dict(color=color))\nlayout = go.Layout(barmode=\"group\", width=1000, height=500, showlegend=False)\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(title=' Fig 1.2 Machine Learning Algorithms Used',\n    width=800,\n    height=500,\n                 annotations=[\n       go.layout.Annotation(\n            showarrow=False,\n            text='Source: World Bank',\n            xanchor='right',\n            x=35,\n            xshift=275,\n            yanchor='top',\n            y=0.05,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=10,\n                color=\"black\"\n            )\n        )])\nfig.update_traces(texttemplate='%{x:.2s}', textposition='outside')\nfig.update_xaxes(showgrid=False, zeroline=False, title='Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","0e89326b":"group_df = df_2021.groupby(['Q15'])['Q17_Part_1','Q17_Part_2','Q17_Part_3','Q17_Part_4','Q17_Part_5','Q17_Part_6',\n 'Q17_Part_7','Q17_Part_8','Q17_Part_9','Q17_Part_10','Q17_Part_11'].count()\npercent_df = ((group_df.T\/group_df.T.sum().values))*100\n\npercent_df = percent_df[['Under 1 year','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-20 years','20 or more years']]\npercent_df.index = ['Linear\/ <br> Logistic Regression', 'Decision Trees or <br> Random Forests', 'Gradient Boosting Machines', 'Bayesian Approaches', 'Evolutionary Approaches', 'Dense Neural Networks', 'Convolutional Neural Networks', 'Generative Adversarial Networks', 'Recurrent Neural Networks', 'Transformer Networks', 'None']\npercent_df.T.round(1).iplot(kind='barh',gridcolor='white',theme='white',\n                              barmode = 'stack',\n                              title=' Fig 1.3 Years of using ML Methods and their ML Algorithms Preference',\n                              xTitle='Respondents (in Percentage)',color = colors)","e97c7cfb":"researchers = df2021[df2021['Q24_Part_5'] == 'Experimentation and iteration to improve existing ML models']\ndeepLearning = [researchers['Q17_Part_6'].count(),researchers['Q17_Part_7'].count(),researchers['Q17_Part_8'].count(),researchers['Q17_Part_9'].count(),researchers['Q17_Part_10'].count()]\ndeepLearning_methods = ['Dense Neural Network','Convolutional Neural Network','Generative Adversarial Networks','Recurrent Neural Networks','Transformer Networks','None']\ndeepLearning_percent = list(np.divide(deepLearning,len(researchers)))\ndeepLearning_percent  = [(element * 100).round(0).astype(int) for element in deepLearning_percent]\ndeepLearning_dict = dict(zip(deepLearning_methods,deepLearning_percent))\nsorted_dict={}\n\nsortedList=sorted(deepLearning_dict.values())\n\nfor sortedKey in sortedList:\n    for key, value in deepLearning_dict.items():\n        if value==sortedKey:\n            sorted_dict[key]=value\n            \ndeepLearning= pd.DataFrame(sorted_dict.items(),columns=['deepLearning', '% of respondents'])    \n\nxx = deepLearning['deepLearning']\nxx = [_ + \"<br>(\" +str(sorted_dict[_])+ \"%)\" for _ in xx]\nyy = [\"\"]*len(sorted_dict)\nzz = deepLearning['% of respondents']\ncc = [\"#e60049\", \"#0bb4ff\", \"#50e991\", \"#e6d800\", \"#9b19f5\", \"#ffa300\", \"#dc0ab4\", \"#b3d4ff\", \"#00bfa0\",\"#0230c7\",\"#00c707\"]\n\ntrace1 = go.Scatter(x = xx, y = [\"\"]*len(xx), mode='markers', name=\"\", marker=dict(color=colors, opacity=0.6, size = zz*2.5))\nlayout = go.Layout(barmode='stack', height=300, margin=dict(l=100), title=' Fig 1.4 Deep Learning Methods Used by People Whose Job Involves Experimentation and <br> Iteration to Improve Existing ML models',\n                   legend = dict(orientation=\"h\", x=0.1, y=1.15), plot_bgcolor='#fff', paper_bgcolor='#fff', \n                   showlegend=False)\n\nfig = go.Figure(data=[trace1], layout=layout)\nfig.update_xaxes(showgrid=False, zeroline=False,title = 'Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","b81ba2dc":"q19_columns = generateColumnNames(19,5)\nq19_columns.append('Q19_OTHER')\nalgos_2021 = sort_dictionary_by_percent(df_2021,q19_columns,q19_count)\nnlp_methods = pd.DataFrame(algos_2021.items(),columns=['Algorithms', '% of respondents'])    \ntrace = go.Bar(y = nlp_methods[\"Algorithms\"],x = nlp_methods['% of respondents'] ,\norientation='h',marker=dict(color=color))\nlayout = go.Layout(barmode = \"group\",width=1000, height=500, showlegend=False)  \nfig = go.Figure(data = [trace], layout = layout)\nfig.update_traces(texttemplate='%{x:.2s}', textposition='outside')\nfig.update_layout(title='Fig 1.5 Natural Language Processing Usage', width=800,height=500)\nfig.update_xaxes(showgrid=False, zeroline=False,title = 'Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()  ","dabece41":"group_df = df_2021.groupby(['Q15'])['Q19_Part_1', 'Q19_Part_2', 'Q19_Part_3', 'Q19_Part_4', 'Q19_Part_5','Q19_OTHER'].count()\npercent_df = ((group_df.T\/group_df.T.sum().values))*100\npercent_df = percent_df[['Under 1 year','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-20 years','20 or more years']]\npercent_df.index = ['Word embeddings\/vectors', 'Encoder-decoder models', 'Contextualized embeddings', 'Transformer language models', 'None', 'Other']\npercent_df.T.round(1).iplot(kind='barh',gridcolor='white',theme='white',\n                              barmode = 'stack',\n                              title='Fig 1.6 Years of using ML Methods and their NLP Methods Preference',\n                              xTitle='Respondents (in Percentage)',color=colors)","713693e4":"q18_columns = generateColumnNames(18,6)\nq18_columns.append('Q18_OTHER')\nalgos_2021 = sort_dictionary_by_percent(df_2021,q18_columns,q18_count)\ncv_methods = pd.DataFrame(algos_2021.items(),columns=['Algorithms', '% of respondents'])    \ntrace = go.Bar(y = cv_methods[\"Algorithms\"],x = cv_methods['% of respondents'] ,\norientation='h',marker=dict(color=color, opacity=0.6))\nlayout = go.Layout(barmode = \"group\",width=1000, height=500, showlegend=False)  \nfig = go.Figure(data = [trace], layout = layout)\nfig.update_traces(texttemplate='%{x:.2s}', textposition='outside')\nfig.update_layout(width=800,height=500,title = 'Fig 1.7 Computer Vision Methods Usage')\nfig.update_xaxes(showgrid=False, zeroline=False,title = 'Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()  ","a9fc870b":"group_df = df_2021.groupby(['Q15'])['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5',\n 'Q18_Part_6','Q18_OTHER'].count()\npercent_df = ((group_df.T\/group_df.T.sum().values))*100\npercent_df = percent_df[['Under 1 year','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-20 years','20 or more years']]\npercent_df.index = ['General purpose image\/video tools', 'Image segmentation methods', 'Object detection methods', 'Image classification', 'Generative Networks', 'None', 'Other']\nfig = percent_df.T.round(1).iplot(kind='barh',gridcolor='white',theme='white',\n                              barmode = 'stack',\n                              title='Fig 1.8 Years of using ML Methods and their Computer Vision Methods Preference',\n                              xTitle='Respondents (in Percentage)',color = colors)","16f83312":"k = df_2021['Q13'].value_counts().sort_values()\nk = (k\/len(df_2021))*(100)\nk = k.round(0).astype(int)\ntpus= pd.DataFrame(k.items(),columns=['TPU', '% of respondents'])    \n\nxx = tpus['TPU']\nxx = [_ + \"<br>(\" +str(k[_])+ \"%)\" for _ in xx]\nyy = [\"\"]*len(k)\nzz = tpus['% of respondents']\n\ntrace1 = go.Scatter(x = xx, y = [\"\"]*len(xx), mode='markers', name=\"\", marker=dict(color=colors, opacity=0.6, size = zz*2.5))\nlayout = go.Layout(barmode='stack', height=300, margin=dict(l=100), title='Fig 2.1 TPU Usage',\n                   legend = dict(orientation=\"h\", x=0.1, y=1.15), plot_bgcolor='#fff', paper_bgcolor='#fff', \n                   showlegend=False)\n\nfig = go.Figure(data=[trace1], layout=layout)\nfig.update_xaxes(showgrid=False, zeroline=False,title = 'Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","8d5fa7e7":"df12_nvidia = df2021[df2021['Q13'] == 'Never']\ndf12_nvidia = df12_nvidia[df12_nvidia['Q15'].notna()]\nk = df12_nvidia['Q15'].value_counts().sort_values()\nk = (k\/len(df12_nvidia))*(100)\nk = k.round(0)\ndf = pd.DataFrame(k)\ndf['experience'] = df.index\nfig = px.treemap(df,path=[\"experience\"],values = df['Q15'],labels = df['experience'],\n                  color='experience', hover_data=['Q15'],\n                  color_continuous_scale='RdBu',title = 'Fig 2.2 ML Experience of Respondents who doesnt use TPU')\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","3ebd772b":"deep_learners = df2021[['Q11','Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_Part_4','Q12_Part_5','Q12_OTHER','Q13','Q17_Part_6','Q17_Part_7','Q17_Part_8','Q17_Part_9','Q17_Part_10']]\ndeep_learners.dropna(how = 'all',subset=['Q17_Part_6','Q17_Part_7','Q17_Part_8','Q17_Part_9','Q17_Part_10'],inplace=True)\ndeep_learners.replace('A deep learning workstation (NVIDIA GTX, LambdaLabs, etc)','A deep learning workstation',inplace=True)\ndeep_learners.replace('A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc)','A cloud computing platform ',inplace=True)\n\nk = deep_learners['Q13'].value_counts().sort_values()\nk = (k\/len(deep_learners))*(100)\nk = k.round(0).astype(int)\ndf =  pd.DataFrame(k.items(),columns=['TPU', '% of respondents'])   \ncircles = circlify.circlify(\n    list(df['% of respondents']), \n    show_enclosure=False, \n    target_enclosure=circlify.Circle(x=0, y=0, r=4)\n)\n\nfig, ax = plt.subplots(figsize=(6,6))\n\nax.axis('off')\n\nlim = max(\n    max(\n        abs(circle.x) + circle.r,\n        abs(circle.y) + circle.r,\n    )\n    for circle in circles\n)\nplt.xlim(-lim, lim)\nplt.ylim(-lim, lim)\n\nlabels = list(df['TPU'])\n\nfor circle, label in zip(circles, labels):\n    x, y, r = circle\n    if label in ['Never']:\n        ax.add_patch(plt.Circle((x, y), r*0.95, alpha=0.6, facecolor=color, edgecolor=color))\n        plt.annotate(label, (x,y ) ,va='center', ha='center',  fontsize=30)\n    else:\n        ax.add_patch(plt.Circle((x, y), r*0.95, alpha=0.5, facecolor='#E6E6E6', edgecolor='#E6E6E6'))\n        plt.annotate(label, (x,y ) ,va='center', ha='center',  fontsize=10)\n\nplt.title(\"Fig 2.3 Respondents who use Deep Learning Methods and their TPU Usage\")\nplt.show()","0e437963":"df_2021.replace('A deep learning workstation (NVIDIA GTX, LambdaLabs, etc)','A deep learning workstation',inplace=True)\ndf_2021.replace('A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc)','A cloud computing platform ',inplace=True)\n\nk = df_2021['Q11'].value_counts().sort_values()\nk = (k\/len(df_2021))*(100)\nk = k.round(0).astype(int)\ntpus= pd.DataFrame(k.items(),columns=['TPU', '% of respondents'])    \n\nxx = tpus['TPU']\nxx = [_ + \"<br>(\" +str(k[_])+ \"%)\" for _ in xx]\nyy = [\"\"]*len(k)\nzz = tpus['% of respondents']\n\ntrace1 = go.Scatter(x = xx, y = [\"\"]*len(xx), mode='markers', name=\"\", marker=dict(color=colors, opacity=0.4, size = zz*2))\nlayout = go.Layout(barmode='stack',   title='Fig 2.4 Computing Platform Usage',\n                   legend = dict(orientation=\"h\", x=0.1, y=1.15), plot_bgcolor='#fff', paper_bgcolor='#fff', \n                   showlegend=False)\n\nfig = go.Figure(data=[trace1], layout=layout)\nfig.update_xaxes(showgrid=False, zeroline=False,title = 'Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","4f6adacc":"df12_nvidia = df2021[df2021['Q11'] == 'None']\ndf12_nvidia = df12_nvidia[df12_nvidia['Q15'].notna()]\nk = df12_nvidia['Q15'].value_counts().sort_values()\nk = (k\/len(df12_nvidia))*(100)\nk = k.round(0)\ndf = pd.DataFrame(k)\ndf['experience'] = df.index\nfig = px.treemap(df,path=[\"experience\"],values = df['Q15'],labels = df['experience'],\n                  color='experience', hover_data=['Q15'],\n                  color_continuous_scale='RdBu',title = 'Fig 2.5 ML Experience of Respondents who doesnt use Computing Platforms')\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","d1c35280":"k = deep_learners['Q11'].value_counts().sort_values()\nk = (k\/len(deep_learners))*(100)\nk = k.round(0).astype(int)\nk = k.iloc[1:]\ndf =  pd.DataFrame(k.items(),columns=['TPU', '% of respondents'])   \ncircles = circlify.circlify(\n    list(df['% of respondents']), \n    show_enclosure=False, \n    target_enclosure=circlify.Circle(x=0, y=0, r=4)\n)\n\nfig, ax = plt.subplots(figsize=(6,6))\n\nax.axis('off')\n\nlim = max(max(\n        abs(circle.x) + circle.r,\n        abs(circle.y) + circle.r,)\n    for circle in circles\n)\nplt.xlim(-lim, lim)\nplt.ylim(-lim, lim)\n\nlabels = list(df['TPU'])\n\nfor circle, label in zip(circles, labels):\n    x, y, r = circle\n    if label in ['A laptop']:\n        ax.add_patch(plt.Circle((x, y), r*0.95, alpha=0.6, facecolor=color, edgecolor=color))\n        plt.annotate(label, (x,y ) ,va='center', ha='center',  fontsize=30)\n    else:\n        ax.add_patch(plt.Circle((x, y), r*0.95, alpha=0.5, facecolor='#E6E6E6', edgecolor='#E6E6E6'))\n        plt.annotate(label, (x,y ) ,va='center', ha='center',  fontsize=10)\n\nplt.title(\"Fig 2.6 Respondents who use Deep Learning Methods and their Computing Platforms Usage\")\nplt.show()","4ee616ca":"q12_count = {\n     'NVIDIA GPUs' : (df2021['Q12_Part_1'].count()),\n    'Google Cloud TPUs': (df2021['Q12_Part_2'].count()),\n    'AWS Trainium Chips' : (df2021['Q12_Part_3'].count()),\n    'AWS Inferentia Chips' : (df2021['Q12_Part_4'].count()),\n    'None' : (df2021['Q12_Part_5'].count()),\n    'Other' : (df2021['Q12_OTHER'].count())\n}\nq12_columns = generateColumnNames(12,5)\nq12_columns.append('Q12_OTHER')\nhardwares = sort_dictionary_by_percent(df2021,q12_columns,q12_count)\nhardware = pd.DataFrame(hardwares.items(),columns=['Hardware', '% of respondents'])    \nxx = hardware['Hardware']\nxx = [_ + \"<br>(\" +str(hardwares[_])+ \"%)\" for _ in xx]\nyy = [\"\"]*len(hardwares)\nzz = hardware['% of respondents']\n\ntrace1 = go.Scatter(x = xx, y = [\"\"]*len(xx), mode='markers', name=\"\", marker=dict(color=colors, opacity=0.4, size = zz*2))\nlayout = go.Layout(barmode='stack',   title='Fig 2.7 Specialized Hardware Usage',\n                   legend = dict(orientation=\"h\", x=0.1, y=1.15), plot_bgcolor='#fff', paper_bgcolor='#fff', \n                   showlegend=False)\nfig = go.Figure(data=[trace1], layout=layout)\nfig.update_xaxes(showgrid=False, zeroline=False,title = 'Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","bddc350d":"df12_none = df2021[df2021['Q12_Part_5'] == 'None']\ndf12_none = df12_none[df12_none['Q15'].notna()]\nk = df12_none['Q15'].value_counts().sort_values()\nk = (k\/len(df12_none))*(100)\nk = k.round(0)\ndf = pd.DataFrame(k)\ndf['experience'] = df.index\nfig = px.treemap(df,path=[\"experience\"],values = df['Q15'],labels = df['experience'],\n                  color='experience', hover_data=['Q15'],\n                  color_continuous_scale='RdBu',title = 'Fig 2.8 ML Experience of Respondents who doesnt use Specialized Hardwares')\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","f91e2419":"xx = [deep_learners['Q12_Part_1'].count(),deep_learners['Q12_Part_2'].count(),deep_learners['Q12_Part_5'].count(),deep_learners['Q12_OTHER'].count(),deep_learners['Q12_Part_3'].count(),deep_learners['Q12_Part_4'].count()]\nyy = ['AWS Inferentia Chips','AWS Traininum Chips','Other','None','Google Cloud TPU','NVIDIA GPU']\nxx  = list(np.divide(xx,len(deep_learners)))\nxx  = [(element * 100).round(0).astype(int) for element in xx]\ncircles = circlify.circlify(\n   list(xx), \n    show_enclosure=False, \n    target_enclosure=circlify.Circle(x=0, y=0, r=4)\n)\n\nfig, ax = plt.subplots(figsize=(6,6))\nax.axis('off')\nlim = max(\n    max(\n        abs(circle.x) + circle.r,\n        abs(circle.y) + circle.r,\n    )\n    for circle in circles\n)\nplt.xlim(-lim, lim)\nplt.ylim(-lim, lim)\n\nlabels = yy\n\nfor circle, label in zip(circles, labels):\n    x, y, r = circle\n    if label in ['NVIDIA GPU']:\n        ax.add_patch(plt.Circle((x, y), r*0.95, alpha=0.6, facecolor=color, edgecolor=color))\n        plt.annotate(label, (x,y ) ,va='center', ha='center',  fontsize=25)\n    else:\n        ax.add_patch(plt.Circle((x, y), r*0.95, alpha=0.5, facecolor='#E6E6E6', edgecolor='#E6E6E6'))\n        plt.annotate(label, (x,y ) ,va='center', ha='center',  fontsize=10)\n\nplt.title(\"Fig 2.9 Respondents who use Deep Learning Methods and their Specialized Hardware Usage\")\nplt.show()","761e106b":"q27a_columns = ['Q27_A_Part_1',\n                        'Q27_A_Part_2',\n                        'Q27_A_Part_3',\n                        'Q27_A_Part_4',\n                        'Q27_A_Part_5',\n                        'Q27_A_Part_6',\n                        'Q27_A_Part_7',\n                        'Q27_A_Part_8',\n                        'Q27_A_Part_9',\n                        'Q27_A_Part_10',\n                        'Q27_A_Part_11',\n                        'Q27_A_OTHER']\n\nresponses_df_2021 = df_2021\nq27a_count = {\n    'Amazon Web Services (AWS)' : (responses_df_2021['Q27_A_Part_1'].count()),\n    'Microsoft Azure': (responses_df_2021['Q27_A_Part_2'].count()),\n    'Google Cloud Platform (GCP)' : (responses_df_2021['Q27_A_Part_3'].count()),\n    'IBM Cloud \/ Red Hat' : (responses_df_2021['Q27_A_Part_4'].count()),\n    'Oracle Cloud' : (responses_df_2021['Q27_A_Part_5'].count()),\n    'SAP Cloud' : (responses_df_2021['Q27_A_Part_6'].count()),\n    'Salesforce Cloud' : (responses_df_2021['Q27_A_Part_7'].count()),\n    'VMware Cloud' : (responses_df_2021['Q27_A_Part_8'].count()),\n    'Alibaba Cloud' : (responses_df_2021['Q27_A_Part_9'].count()),\n    'Tencent Cloud' : (responses_df_2021['Q27_A_Part_10'].count()),\n    'None' : (responses_df_2021['Q27_A_Part_11'].count()),\n    'Other' : (responses_df_2021['Q27_A_OTHER'].count())\n}\n\ncloud_professional = sort_dictionary_by_percent(responses_df_2021,q27a_columns,q27a_count)\ncloud_professional  = OrderedDict(reversed(list(cloud_professional.items())))\ncloud_professional = pd.DataFrame(cloud_professional.items(), columns=['Tools', '% of respondents'])\ntrace = go.Funnel(\n    y=cloud_professional[\"Tools\"],\n    x=cloud_professional['% of respondents'], marker=dict(color=color))\n\nlayout = go.Layout(barmode=\"group\", width=1000, height=500, showlegend=False)\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(title='Fig 3.1 Cloud Computing PLatforms Usage', width=800,height=500)\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","c3945c1e":"cloud_cost = df_2021[['Q15','Q26']]\ncloud_cost.dropna(inplace=True)\ngroup_df = pd.crosstab(cloud_cost['Q15'], cloud_cost['Q26'])\ngroup_df = group_df[['$0 ($USD)','$1-$99','$100-$999','$1000-$9,999','$10,000-$99,999','$100,000 or more ($USD)']]\npercent_df = ((group_df.T\/group_df.T.sum().values))*100\npercent_df.index = [' 0 USD','1-99 USD','100-999 USD','1000-9,999 USD','10,000-99,999 USD','>100,000 USD']\nfig = percent_df.T.round(1).iplot(kind='barh',gridcolor='white',theme='white',\n                              barmode = 'stack',\n                              title='Fig 3.2 Years of using ML Methods and their Amount Spent on Cloud Computing ',\n                              xTitle='Respondents (in Percentage)',color = colors)","e7a14332":"q36a_count = {\n    'Automated data augmentation' : (df_2021['Q36_A_Part_1'].count()),\n    'Automated feature engineering\/selection': (df_2021['Q36_A_Part_2'].count()),\n    'Automated model selection ' : (df_2021['Q36_A_Part_3'].count()),\n    'Automated model architecture searches' : (df_2021['Q36_A_Part_4'].count()),\n    'Automated hyperparameter tuning' : (df_2021['Q36_A_Part_5'].count()),\n    'Automation of full ML pipelines' : (df_2021['Q36_A_Part_6'].count()),\n    'None' : (df_2021['Q36_A_Part_7'].count()),\n    'Other' : (df_2021['Q36_A_OTHER'].count())\n}\nq36a_columns = ['Q36_A_Part_1','Q36_A_Part_2','Q36_A_Part_3','Q36_A_Part_4','Q36_A_Part_5','Q36_A_Part_6','Q36_A_Part_7','Q36_A_OTHER']\nautoml_2021 = sort_dictionary_by_percent(df_2021, q36a_columns, q36a_count)\nautoml_2021  = OrderedDict(reversed(list(automl_2021.items())))\nautoml_methods = pd.DataFrame(automl_2021.items(), columns=['Tools', '% of respondents'])\ntrace = go.Funnel(\n    y=automl_methods[\"Tools\"],\n    x=automl_methods['% of respondents'], marker=dict(color=color))\n\nlayout = go.Layout(barmode=\"group\", width=1000, height=500, showlegend=False)\nfig = go.Figure(data=[trace], layout=layout)\nfig.update_layout(title='Fig 4.1 Auto ML Tools Usage', width=800,height=500)\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","62d06fbc":"researchers = df2021[df2021['Q24_Part_5'] == 'Experimentation and iteration to improve existing ML models']\nautoml = [researchers['Q36_A_Part_1'].count(),researchers['Q36_A_Part_2'].count(),researchers['Q36_A_Part_3'].count(),researchers['Q36_A_Part_4'].count(),researchers['Q36_A_Part_5'].count(),researchers['Q36_A_Part_6'].count(),researchers['Q36_A_Part_7'].count(),researchers['Q36_A_OTHER'].count()]\nautoml_methods = ['Automated data augmentation','Automated feature engineering\/selection','Automated model selection','Automated model architecture searches','Automated hyperparameter tuning','Automation of full ML pipelines','No \/ None','Other']\nautoml_percent = list(np.divide(automl,len(researchers)))\nautoml_percent  = [(element * 100).round(0).astype(int) for element in automl_percent]\nautoml_dict = dict(zip(automl_methods,automl_percent))\n\nsorted_dict={}\n\nsortedList=sorted(automl_dict.values())\n\nfor sortedKey in sortedList:\n    for key, value in automl_dict.items():\n        if value==sortedKey:\n            sorted_dict[key]=value\n            \nautoml= pd.DataFrame(sorted_dict.items(),columns=['Automl', '% of respondents'])    \n\nxx = automl['Automl']\nxx = [_ + \"<br>(\" +str(sorted_dict[_])+ \"%)\" for _ in xx]\nyy = [\"\"]*len(sorted_dict)\nzz = automl['% of respondents']\n\ntrace1 = go.Scatter(x = xx, y = [\"\"]*len(xx), mode='markers', name=\"\", marker=dict(color=colors, opacity=0.6, size = zz*2.5))\nlayout = go.Layout(barmode='stack', height=300, margin=dict(l=100), title='Fig 4.2 Auto ML Methods Used by People Whose Job Involves Experimentation and <br> Iteration to Improve Existing ML models',\n                   legend = dict(orientation=\"h\", x=0.1, y=1.15), plot_bgcolor='#fff', paper_bgcolor='#fff', \n                   showlegend=False)\n\nfig = go.Figure(data=[trace1], layout=layout)\nfig.update_xaxes(showgrid=False, zeroline=False,title = 'Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()","91a8f44a":"analysis = df2021[df2021['Q24_Part_1'] == 'Analyze and understand data to influence product or business decisions']\nk = analysis['Q41'].value_counts().sort_values()\nk = (k\/len(analysis))*(100)\nk= k.round(0).astype(int)\ntool= pd.DataFrame(k.items(),columns=['Tool', '% of respondents'])  \ntrace = go.Bar(y = tool[\"Tool\"],x = tool['% of respondents'] ,\norientation='h',marker=dict(color=color, opacity=0.6))\nlayout = go.Layout(barmode = \"group\",width=1000, height=500, showlegend=False)  \nfig = go.Figure(data = [trace], layout = layout)\nfig.update_traces(texttemplate='%{x:.2s}', textposition='outside')\nfig.update_layout(plot_bgcolor='white',width=800,height=500,title='Fig 5.1 Primary Tools Used by People Whose Job Involves Analyze and Understand Data')\nfig.update_xaxes(showgrid=False, zeroline=False, title='Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()  ","881dd760":"analysis = df2021[df2021['Q24_Part_1'] == 'Analyze and understand data to influence product or business decisions']\n\ngroup_df = analysis.groupby(['Q24_Part_1'])['Q34_A_Part_1',\n                        'Q34_A_Part_2',\n                        'Q34_A_Part_3',\n                        'Q34_A_Part_4',\n                        'Q34_A_Part_5',\n                        'Q34_A_Part_6',\n                        'Q34_A_Part_7',\n                        'Q34_A_Part_8',\n                        'Q34_A_Part_9',\n                        'Q34_A_Part_10',\n                        'Q34_A_Part_11',\n                        'Q34_A_Part_12',\n                        'Q34_A_Part_13',\n                        'Q34_A_Part_14',\n                        'Q34_A_Part_15',\n                        'Q34_A_Part_16',\n                        'Q31_A_OTHER'].count()\npercent_df = ((group_df.T\/group_df.T.sum().values))*100\npercent_df.index = ['Amazon Quicksight','Power BI','Google Data Studio','Looker','Tableau','Salesforce','Einstein Analytics','Qlik','Domo','TIBCO Spotfire','Alteryx','Sisense','SAP Analytics Cloud','Microsoft Azure Synapse','Thoughtspot','None','Other']\npercent_df = percent_df.sort_values('Analyze and understand data to influence product or business decisions')\ntrace = go.Bar(y = list(percent_df.index),x = percent_df['Analyze and understand data to influence product or business decisions'] ,\norientation='h',marker=dict(color=color, opacity=0.6))\nlayout = go.Layout(barmode = \"group\",width=1000, height=500, showlegend=False)  \nfig = go.Figure(data = [trace], layout = layout)\nfig.update_traces(texttemplate='%{x:.2s}', textposition='outside')\nfig.update_layout(width=800,height=500,title='Fig 5.2 Business Intelligence Tools Used by People Whose Job Involves Analyze and <br> Understand Data to Influence Product or Business Decisions')\nfig.update_xaxes(showgrid=False, zeroline=False, title='Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()  ","b7ea9601":"researchers = df2021[df2021['Q24_Part_5'] == 'Experimentation and iteration to improve existing ML models']\nml_tools = [researchers['Q38_A_Part_1'].count(),researchers['Q38_A_Part_2'].count(),researchers['Q38_A_Part_3'].count(),researchers['Q38_A_Part_4'].count(),researchers['Q38_A_Part_5'].count(),researchers['Q38_A_Part_6'].count(),researchers['Q38_A_Part_7'].count(),researchers['Q38_A_Part_8'].count(),researchers['Q38_A_Part_9'].count(),researchers['Q38_A_Part_10'].count(),researchers['Q38_A_Part_11'].count(),researchers['Q38_A_OTHER'].count()]\nml_tools_methods = ['Neptune.ai','Weights and Biases','Comet.ml','Sacred + Omniboard','TensorBoard','Guild.ai','Polyaxon','Trains','Domino Model Monitor','MLflow','No \/ None','Other']\nml_tools_percent = list(np.divide(ml_tools,len(researchers)))\nml_tools_percent  = [(element * 100).round(0).astype(int) for element in ml_tools_percent]\nml_tools_dict = dict(zip(ml_tools_methods,ml_tools_percent))\n\nsorted_dict={}\n\nsortedList=sorted(ml_tools_dict.values())\n\nfor sortedKey in sortedList:\n    for key, value in ml_tools_dict.items():\n        if value==sortedKey:\n            sorted_dict[key]=value\n            \nml_tools= pd.DataFrame(sorted_dict.items(),columns=['Tools', '% of respondents'])    \n\ntrace = go.Bar(y = ml_tools[\"Tools\"],x = ml_tools['% of respondents'] ,\norientation='h',marker=dict(color=color))\nlayout = go.Layout(barmode = \"group\",width=1000, height=500, showlegend=False)  \nfig = go.Figure(data = [trace], layout = layout)\nfig.update_traces(texttemplate='%{x:.2s}', textposition='outside')\nfig.update_layout(width=800,height=500,title='Fig 5.3 ML Tools Used by People Whose Job Involves Experimentation and <br> Iteration to Improve Existing ML Models')\nfig.update_xaxes(showgrid=False, zeroline=False, title='Respondents (in Percentage)')\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.show()  ","7381b314":"researchers = df2021[df2021['Q24_Part_6'] == 'Do research that advances the state of the art of machine learning']\nval = (researchers['Q42_Part_9'].count()\/len(researchers))*100\nval = val.round(0).astype(int)\nfig_09_trace_02 =go.Figure( go.Indicator(\n    domain = {'row' : 1, 'column' : 3},\n    value = val,\n    mode = 'gauge+number',\n    title = {'text' : 'Fig 6.1 Journal Publications Preference by People whose Job Involves <br> Research that Advances the State of the Art of ML'},\n    delta = {'reference' : 0},\n    gauge = {'axis' : {'range' : [0, 100]},'bar': {'color': color},}))\nfig_09_trace_02.show()","7fa7fbec":"beginners = ['Do not use machine learning methods','Under 1 year']\ndf_beginners = df_2021[df_2021.Q15.isin(beginners)]\nq40_list_of_columns_2021 = ['Q40_Part_1',\n                       'Q40_Part_2',\n                       'Q40_Part_3',\n                       'Q40_Part_4',\n                       'Q40_Part_5',\n                       'Q40_Part_6',\n                       'Q40_Part_7',\n                       'Q40_Part_8',\n                       'Q40_Part_9',\n                       'Q40_Part_10',\n                       'Q40_Part_11',\n                       'Q40_OTHER']\n\nq42_list_of_columns_2021 = ['Q42_Part_1',\n                       'Q42_Part_2',\n                       'Q42_Part_3',\n                       'Q42_Part_4',\n                       'Q42_Part_5',\n                       'Q42_Part_6',\n                       'Q42_Part_7',\n                       'Q42_Part_8',\n                       'Q42_Part_9',\n                       'Q42_Part_10',\n                       'Q42_Part_11',\n                       'Q39_OTHER']\n\nq40_dictionary_of_counts_2021 = {\n    'Coursera' : (responses_df_2021['Q40_Part_1'].count()),\n    'EdX': (responses_df_2021['Q40_Part_2'].count()),\n    'Kaggle Learn Courses' : (responses_df_2021['Q40_Part_3'].count()),\n    'DataCamp' : (responses_df_2021['Q40_Part_4'].count()),\n    'Fast.ai' : (responses_df_2021['Q40_Part_5'].count()),\n    'Udacity' : (responses_df_2021['Q40_Part_6'].count()),\n    'Udemy' : (responses_df_2021['Q40_Part_7'].count()),\n    'LinkedIn Learning' : (responses_df_2021['Q40_Part_8'].count()),\n    'Cloud-certification programs' : (responses_df_2021['Q40_Part_9'].count()),\n    'University Courses' : (responses_df_2021['Q40_Part_10'].count()),\n    'None' : (responses_df_2021['Q40_Part_11'].count()),\n    'Other' : (responses_df_2021['Q40_OTHER'].count())\n}\nq42_dictionary_of_counts_2021 = {\n    'Twitter (data science influencers)' : (responses_df_2021['Q42_Part_1'].count()),\n    'Email newsletters (Data Elixir, OReilly Data & AI, etc)': (responses_df_2021['Q42_Part_2'].count()),\n    'Reddit (r\/machinelearning, etc)' : (responses_df_2021['Q42_Part_3'].count()),\n    'Kaggle (notebooks, forums, etc)' : (responses_df_2021['Q42_Part_4'].count()),\n    'Course Forums (forums.fast.ai, Coursera forums, etc)' : (responses_df_2021['Q42_Part_5'].count()),\n    'YouTube (Kaggle YouTube, Cloud AI Adventures, etc)' : (responses_df_2021['Q42_Part_6'].count()),\n    'Podcasts (Chai Time Data Science, OReilly Data Show, etc)' : (responses_df_2021['Q42_Part_7'].count()),\n    'Blogs (Towards Data Science, Analytics Vidhya, etc)' : (responses_df_2021['Q42_Part_8'].count()),\n    'Journal Publications (peer-reviewed journals, conference proceedings, etc)' : (responses_df_2021['Q42_Part_9'].count()),\n    'Slack Communities (ods.ai, kagglenoobs, etc)' : (responses_df_2021['Q42_Part_10'].count()),\n    'None' : (responses_df_2021['Q42_Part_11'].count()),\n    'Other' : (responses_df_2021['Q42_OTHER'].count())\n}\n\ngroup_df = df_beginners.groupby(['Q15'])['Q40_Part_1',\n                       'Q40_Part_2',\n                       'Q40_Part_3',\n                       'Q40_Part_4',\n                       'Q40_Part_5',\n                       'Q40_Part_6',\n                       'Q40_Part_7',\n                       'Q40_Part_8',\n                       'Q40_Part_9',\n                       'Q40_Part_10',\n                       'Q40_Part_11',\n                       'Q40_OTHER'].count()\npercent_df = ((group_df.T\/group_df.T.sum().values))*100\npercent_df.index = ['Coursera', 'EdX', 'Kaggle Learn Courses', 'DataCamp', 'Fast.ai', 'Udacity', 'Udemy', 'LinkedIn Learning', 'Cloud-certification programs', 'University Courses', 'None', 'Other']\nfig = percent_df.T.round(1).iplot(kind='barh',gridcolor='white',theme='white',\n                              barmode = 'stack',\n                              title='Fig 6.2 ML Beginners and their Learning Platforms Preference',\n                              xTitle='Respondents (in Percentage)',color = colors)","c95c96b7":"q42_list_of_columns_2021 = ['Q42_Part_1',\n                       'Q42_Part_2',\n                       'Q42_Part_3',\n                       'Q42_Part_4',\n                       'Q42_Part_5',\n                       'Q42_Part_6',\n                       'Q42_Part_7',\n                       'Q42_Part_8',\n                       'Q42_Part_9',\n                       'Q42_Part_10',\n                       'Q42_Part_11',\n                       'Q39_OTHER']\ngroup_df = df_beginners.groupby(['Q15'])['Q42_Part_1',\n                       'Q42_Part_2',\n                       'Q42_Part_3',\n                       'Q42_Part_4',\n                       'Q42_Part_5',\n                       'Q42_Part_6',\n                       'Q42_Part_7',\n                       'Q42_Part_8',\n                       'Q42_Part_9',\n                       'Q42_Part_10',\n                       'Q42_Part_11',\n                       'Q42_OTHER'].count()\npercent_df = ((group_df.T\/group_df.T.sum().values))*100\npercent_df.index = ['Twitter ', 'Email newsletters', 'Reddit', 'Kaggle', 'Course Forums', 'YouTube', 'Podcasts', 'Blogs', 'Journal Publications', 'Slack Communities', 'None', 'Other']\nfig = percent_df.T.round(1).iplot(kind='barh',gridcolor='white',theme='white',\n                              barmode = 'stack',\n                              title='Fig 6.3 ML Beginners and their Media Sources Preference',\n                              xTitle='Respondents (in Percentage)',color = colors)","0dc156ae":"\n<p style=\"text-align:justify;font-size:16px\"> Figure 1.1 shows that around 59% (39% less than 1 year and 20% with 1-2 years) of the respondents are beginners. Also 16% of them aren't using any ML Methods. This could make the further analysis biased as most of them are beginners. Some of the questions we wish to answer in this section are \n<ol style=\"font-size:16px\">\n  <li>Does the respondents prefer deep learning algorithms?<\/li>\n  <li>Are they using Computer Vision and Natural Language Processing Methods?<\/li>\n  <li>Are the beginners interested in\/exposed to the new technologies?<\/li>\n<\/ol><\/p>\n","ec9e0e63":"<p style=\"text-align:justify;font-size:16px\"> \nFor Figure 1.4 respondents whose prime job involves Experimentation and Interation to improve existing ML models are considered. This set of people tries various algorithms, tunes the hyper parameters to see if they could improve the existing solutions.\n    \n<\/p>\n <ol style=\"font-size:16px\">\n  <li>Convolutional Neural Networks are highly prefered <\/li>\n\n  <\/ol>\n\n<br>\n<p style=\"text-align:justify;font-size:16px\"> \nTough we keep telling deep learning is gaining popularity in the recent years, solves any complex problems, more accuracy etc it is very evident that basic machine learning algorithms are more prefered or they are familiar with.\n<\/p>\n\n****\n<p style=\"text-align:justify;font-size:16px\"> \nComputer Vision and NLP are the technologies that enable computer\/machines to perceive things. Over the years both computer vision and NLP has seen tremendous growth. <br><\/p>\n    <center><img src=\"https:\/\/i.imgur.com\/JxdPigO.png[\/img]\" width = 500><\/center>   \n\n\n <p style=\"text-align:justify;font-size:16px\">   <i>Natural language processing<\/i> is the branch of artificial intelligence that deals with training a computer to understand, process, and generate language. Search engines, machine translation services, and voice assistants are all powered by the technology. In the AI hype cycle, Gartner has placed NLP in the 'Trough of Disillusionment' reaching the 'Plateau of Productivity' in 5 to 10 years. <\/p>\n","af13b75a":"<p style=\"text-align:justify;font-size:16px\">\nFrom figure 6.2 and 6.3 it is good to see the none percentage is very less and the respondents have some platform to keep themselves updated. Figure 6.1 is an example to check if the respondents whose job involves research that advances the state of art of ML are reading journal publications as only by reading various publications literature survey can be done. But only 33% of them are using journal publications.\n<\/p>","a20397ca":"<p style=\"text-align:justify;font-size:16px\">Kaggle has conducted their survey successfully for 5 years since 2017 which covers a wide range of questions related to data science community. It has seen a <b> increase of 55% <\/b> of respondents from 2017 to 2021. <i> More the sample, better the insights!<\/i>   <\/p>","c10d439e":"<p style=\"text-align:justify;font-size:16px\">\nRespondents whose job involves analyze and understand data to influence product or business decisions might require tools to gather, process, analyze, and visualize large volumes of past, current, and future data in order to generate actionable business insights, create interactive reports, and simplify the decision-making processes.\n    <ol style=\"font-size:16px\">\n       <li> From figure most of them are using some sort of tool predominantly local developement environments and basic statistical softwares\n           <li> Advanced tools  and newer business intelligence tools aren't prefered\n    <\/ol>    \n <\/p>","eb85a7da":"<p style=\"text-align:justify;font-size:16px\"> \n    <ol style=\"font-size:16px\">\n<li> Majority of beginners haven't spent on cloud computing platforms\n    <li> On average, the mid experienced people have spent 1000 - 9999 USD\n        <\/ol>\n<\/p>","91c215a3":"<div>\n    <h2><center style=\"background-color:#00878b; color:white;\">  CONTENTS <\/center><\/h2>\n<\/div>\n\n* [Machine Learning Technologies](#1)\n* [Specialized Hardware](#2)\n* [Cloud Computing](#3)\n* [AutoML](#4)\n* [Machine Learning Tools](#5)\n* [The Quest to Learn](#6)\n* [Key Insights](#7)\n* [Conclusion](#8)\n\n<div>\n    <h2><center style=\"background-color:#00878b; color:white;\">  1. MACHINE LEARNING TECHNOLOGIES <\/center><a id=\"1\"><\/a><\/h2>\n<\/div>\n\n<p style=\"text-align:justify;font-size:16px\">This section focuses on currently trending technologies from the Gartner Hyper Cycle for AI, 2021.   <\/p>","680e0b52":"<p style=\"text-align:justify;font-size:16px\"> The above tree map repesents the respondents who haven't used TPU. It is evident that beginners aren't using such TPUs. Its is good to see only a few percent of the experienced respondents aren't using. This makes it clear that beginners require more knowledge on existence of such specialized hardware.  <\/p>","1ca22eae":"  <p style=\"text-align:justify;font-size:16px\">   \n  <ol style=\"font-size:16px\">\n      <li> Red Red everywhere! all of them majorly use word embeddings \n      <li> Again transfomer language models are used by all category respondent\n      <li> 26% of respondents with experience under 1 year aren't using NLP methods which takes it to the point beginners aren't using the newer technology.\n      <li> Something dubious is among all category of respondents the none percentage is more\n     <\/ol>\n<\/p>\n\n****\n\n<p style=\"text-align:justify;font-size:16px\">   <i>Computer vision<\/i> is a form of artificial intelligence that trains computers to interpret and understand the visual world. By combining digital images from camera feeds and videos with deep learning, machines can quickly learn to accurately identify and classify disparate objects. In the AI hype cycle, Gartner has placed computer vision in the 'Trough of Disillusionment' reaching the 'Plateau of Productivity' in 2 to 5 years. Which implies powerful computer vision solutions are arriving much sooner to production.  \n<\/p>","af448b3e":"<p style=\"text-align:justify;font-size:16px\"> \nFor Figure 2.3 respondents use deep learning algorithms are considered as it is only for them TPUs are more useful. It is sad to see most of them have never used TPUs. \n<\/p>\n\n****","74736054":"<p style=\"text-align:justify;font-size:16px\">\nRespondents whose prime job involves experimentation and iteration to improve exisiting ML models are considered as the autoML methods mentioned would be highly useful for them fro tasks such as hyper tuning, running full experiment, feature engineering etc.  \n<ol style=\"font-size:16px\">\n  <li>39% of them aren't using of the autoML methods.<\/li>\n  <li>Remaining 61% of them are using atleast one of the autoML methods<\/li>\n<\/ol>  <\/p>","a2873920":"<p style=\"text-align:justify;font-size:16px\"> From figure 1.2 it is evident that machine learning algorithms such as Linear Regression, Decision trees are more popular or preferred compared to Deep Learning Algorithms. This could be because of the following reasons\n    <ol style=\"font-size:16px\">\n  <li>Any machine learning course would start with supervised learning\/ unsupervised learning which takes it to regression and classification in which linear regresssion and decision tress are beginner friendly algorithms. Since majority of the respondents are beginners they would be more exposed to them.<\/li>\n  <li>The above point brings it to the fact that machine learning algorithms are less complex than deep learning algorithms<\/li>\n  <li>Machine Learning requires less computing power or specialized hardwares than deep learning<\/li>\n<\/ol>\n    \n<\/p>","e5144904":"<p style=\"text-align:justify;font-size:16px\">\nTough autoML is evolving so rapidly figure shows autoML tools usage by 68% of the respondents is none which is contradicting! This could be because most of the respondents are beginners. Hence, a subset of the respondents can be chose for further analysis on autoML. <\/p>","e93af277":"<p style=\"text-align:justify;font-size:16px\"> \nAgain, For Figure 2.6 respondents use deep learning algorithms are considered as specialized hardwares are closely associated with them. Ignoring the obvious one laptops and desktops a good amount of them are using deep learning workstations and cloud computing platforms.\n<\/p>\n\n****\n<p style=\"text-align:justify;font-size:16px\"> \nThis section focuses on specilized hardwares in general which comprises NVIDIA GPUs, Google Cloud TPUs, AWS Trainium Chips, AWS Inferentia Chips <\/p>","82512949":"<p>\n    <ol style=\"font-size:16px\">\n  <li>All respondents prefer linear and logistic regression irrespective of their ML experience <\/li>\n  <li>Machine Learning algorithms as in whole are highly prefered<\/li>\n  <li>Among deep learning algorithms Convolutional Neural Networks are highly used<\/li>\n        <li> Around 6% of respondents whose ML experience is less than 1 year doesnt use any of them <\/li>\n<\/ol><\/p>\n","952b0d63":"<p style=\"text-align:justify;font-size:16px\"> \nAgain percentage of none is more than 50% when compared to usage of specilized hardwares. Let's have a look at who are these none.<\/p>","7930c856":"<p style=\"text-align:justify;font-size:16px\">\n    \n <\/p>","9933923a":"<p style=\"text-align:justify;font-size:16px\"> It is hard to see 63% of them aren't using TPUs. Also 2% of them have used it more than 25 times which takes it to the fact that people aren't using TPUs even though it's access is freely provided by Google through third  its Cloud TPU service as part of the Google Cloud Platform and through its notebook-based services Kaggle and Colaboratory. Let's zoom into the respondents who chose None. <\/p>","5c8d7e37":"<center><img src=\"https:\/\/i.imgur.com\/QqfcZ36.png[\/img]\" width = 600><\/center>  \n<br>\n<p style=\"text-align:justify;font-size:16px\"> With the increase in use cases, interest in the field of AI has gained its attention from the research community. Many existing algorithms have went through and a lot of new ML and DL algorithms have emerged. We say particularly deep learning is gaining much popularity due to it\u2019s supremacy in terms of accuracy when trained with huge amount of data.  The idea of this section is to understand if traditional machine learning algorithms or recent deep learning algorithms are prefered by the respondents<\/p>\n\n","7447a128":"<p style=\"text-align:justify;font-size:16px\"> \nAmong computing platforms laptop followed by personal computer is the majority. This can't be considered, as everybody would have mostly chosen their personal devices. But there are 1% respondents who doesn't use any such computing platforms even their personal device. Let's check the further plot to understand who are these 1%.\n<\/p>","d8601639":"<div>\n    <h2><center style=\"background-color:#00878b; color:white;\">  4. AUTO ML <\/center><a id=\"4\"><\/a><\/h2>\n<\/div>\n\n<p style=\"text-align:justify;font-size:16px\">\nAutomated machine learning, also referred to as AutoML, is the process of automating the time-consuming, iterative tasks of machine learning model development. It allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality. According to a report by Latentview, AutoML revenue is expected to reach 14,512 million dollars by 2030. <\/p>","ed5f3247":"<p style=\"text-align:justify;font-size:16px\">    <\/p>\n\u200b","e7d5c2ad":"<div>\n    <h2><center style=\"background-color:#00878b; color:white;\"> (NOT) AI HYPE CYCLE 2021<\/center><\/h2>\n<\/div>   \n<p style=\"text-align:justify;font-size:16px\"> Every year the American research, advisory and information technology firm Gartner publishes a report in graphical representation called Gartner Hype Cycle which represents the maturity, adoption, and social application of specific technologies. <\/p>\n    \n  <center><img src=\"https:\/\/i.imgur.com\/Xnpug2R.png[\/img]\" width = 700><\/center> \n  \n  <p style=\"text-align:justify;font-size:16px\"> According to Gartner analysis 2021, the following are key takeaway <br>\n  <b>Current<\/b> <br>\n <ol style=\"font-size:16px\">\n  <li>Machine Learning <\/li>\n  <li>Deep Learning<\/li>\n  <li>Chatbots<\/li>\n  <li>Natural Language Processing <\/li>\n  <li> AI Cloud Services <\/li>\n  <li> Semantic Search <\/li>\n  <li> Computer Vision <\/li>\n<\/ol>  \n <b>Near-future focus<\/b> <br>\n <ol style=\"font-size:16px\">\n  <li>Generative AI <\/li>\n  <li>AI Governance, AI Orchestration and Automation Platforms, MLOps, ModelOps<\/li>\n  <li>Responsible AI<\/li>\n<\/ol>  \n <b>Later in future<\/b> <br>\n <ol style=\"font-size:16px\">\n  <li>Small and Wide Data <\/li>\n  <li>Transformers<\/li>\n  <li>Knowledge Graphs<\/li>\n<\/ol>  \n<\/p>\n<p style=\"text-align:justify;font-size:16px\"> From the Gartner hype cycle for Artificial Intelligence (AI) we can see AI comprises a lot of technologies and the AI world is running at a fast pace. Each year so many new technologies and tools are popping up also, the existing ones are growing rapidly. The objective of this analysis is to understand are we running in equal pace with AI. As mentioned above, machine learning, deep learning, computer vision, natural language processing, cloud services are the current trend and the Gartner team have already predicted what could be the future but, are the commoners or machine learning enthusiasts using these technologies for their work? If not what could be done? Any analysis we would focus on the YES i.e what is being used or prefered but this analysis focuses more on the NO\/NONE. Which subset of people doesn't use any particular tool\/technology? What could be the reason?  <\/p>\n\n<p style=\"text-align:justify;font-size:14px\"> The subset among the total respondents for analysis are selected by either their ML experince (Q15 in the survey) or what task does their job involve (Q24 in the survey).<\/p>","1f7907cc":"<div>\n    <h2><center style=\"background-color:#00878b; color:white;\">  5. MACHINE LEARNING TOOLS <\/center><a id=\"5\"><\/a><\/h2>\n    <\/div>\n    <p style=\"text-align:justify;font-size:16px\">\nMachine Learning tools and resources are available so that anyone can use technology to solve problems. It makes the process easier. Machine Learning has a vast array of tools, software, and platforms to train and deploy ML models, draw insights from data etc. Also, the technology keeps on advancing forward.  <\/p>\n","e48ab0f1":" <p style=\"text-align:justify;font-size:16px\">   \n  <ol style=\"font-size:16px\">\n      <li> Yellove! all of them  majorly use image classfication methods \n      <li> Every other methods also has a good amount of respondents using them\n      <li> 15% of beginners doesn't use any CV methods\n      <li> When compared to NLP, the None numbers are better for CV\n     <\/ol>\n<\/p>\n\n<div>\n    <h2><center style=\"background-color:#00878b; color:white;\">  2. SPECIALIZED HARDWARES <\/center><a id=\"2\"><\/a><\/h2>\n<\/div>\n<p style=\"text-align:justify;font-size:16px\">   With the advent of Neural networks over the recent years, it has triggered a huge demand for specialised hardware in the field of Machine learning. The need for reducing Training time and inference time for Neural networks came as key challenge for Hardware designers. This challenge and growth of usage over years led to high competition in the field of Hardware manufacturing to cater the fast growing requirement from the Industry. This focuses on specialized hardwares to see are they really used by the commoners. <\/p>\n\n<p style=\"text-align:justify;font-size:16px\">Google introduced Tensor Processing Units  in the year 2016. TPUs, unlike GPUs, is custom-designed to deal with operations such as matrix multiplications in neural network training. <\/p>\n","ea08459e":"<p style=\"text-align:justify;font-size:16px\"> \n    <ol style=\"font-size:16px\">\n<li>Figure 2.5 is quite obvious as majority of respondents who doesnt use any ML methods doesn't require computing platforms \n <li> Something which is odd is the beginners, 29% of respondents with less than 1 year experience and 6% of respondents with 1-2 years experience are also not using any such computing platforms                                                                                   <\/ol>\n<\/p>","ea609c40":"<p style=\"text-align:justify;font-size:16px\"> \nIt is evident that beginners aren't using specialized hardware as they hold the major space in the tree map. <\/p>","df1cf3e1":"<p style=\"text-align:justify;font-size:16px\"> \n    <ol style=\"font-size:16px\">\n<li> AWS and GCP tops the list \n    <li> 28% of respondents doesn't use any cloud computing tools\n        <\/ol>\n<\/p>","e44bbfb5":"<p style=\"text-align:justify;font-size:16px\">\nFigure 5.3 shows the ML tools that are used by respondents whose job involves experimentation and iteration to improve existing ML models. \n<\/p>\n  <ol style=\"font-size:16px\">\n       <li> 37% of them aren't using any such special ML tools for their job\n           <li> TensorBoard tops among the tools as it is popular could be used for deep learning experiments\n    <\/ol>\n    <br>\n  <div>\n    <h2><center style=\"background-color:#00878b; color:white;\"> 6. THE QUEST TO LEARN <\/center><a id=\"6\"><\/a><\/h2>\n<\/div>   \n<p style=\"text-align:justify;font-size:16px\">\nFor most of the tools and technologies discussed above the majority is either none or any any one item which is popular among that group. Respondents aren't familiar with most of the new tools and technologies. Especially, the beginners! It is important for them to understand and make themselves familiar with various tools and technology. This section compares the respondents and their learning platform preference for improving. \n<\/p>","31ca3fad":"  <p style=\"text-align:justify;font-size:16px\">   \n  <ol style=\"font-size:16px\">\n      <li> Word Embedding techniques such as GLoVe, word2Vec are highly used this is dues to the fact that before any computation the text has to be represented in some form.<\/li>\n         <li> 45% of them are using transformer language models such as GPT-3, BERT which is great!\n             <li> 26% of them aren't using any of the NLP methods\n     <\/ol>\n<\/p>","d3df8a52":"<p style=\"text-align:justify;font-size:16px\"> \n    <ol style=\"font-size:16px\">\n<li>Respondents who use deep learning algorithms are using NVIDIA GPU majorly followed by Google Cloud TPU which is a good sign.\n    <li> AWS Traininum and AWS Inferentia Chips are the least prefered\n        <\/ol>\n<\/p>\n\n<div>\n    <h2><center style=\"background-color:#00878b; color:white;\">  3. CLOUD COMPUTING <\/center><a id=\"3\"><\/a><\/h2>\n<\/div>\n\n<p style=\"text-align:justify;font-size:16px\"> The field of data science is varied, and today there are many different roles and responsibilities involved in the process. Data science work typically involves working with unstructured data, implementing machine learning (ML) concepts and techniques, generating insights. This process typically ends in a visual presentation of data-driven insights. Machine learning is a critical element of the process, but training ML models is often a time-consuming process that requires a lot of resources. In the past, gaining access to ML resources was difficult and expensive. Today, many cloud computing vendors offer resources for data science in the cloud. <\/p>\n","bbd1419a":" <div>\n    <h2><center style=\"background-color:#00878b; color:white;\">  KEY INSIGHTS <\/center> <a id=\"7\"><\/a><\/h2>\n<\/div>\n\n  <ol style=\"font-size:16px\">\n   <li> For most of the categories discussed above the percentage of people who doesn't use them are high\n\n   <li> When the None category is zoomed it is predominantly filled by beginners (less than 1 year or 1-2 years of ML experience)\n   <li> This makes it clear beginners aren't using any new tools and technology\n   <li> Respondents aren't using the sophisticated tools and technologies available for them that makes their job easier\n   <li> They are aware of various learning platforms available and are using them\n\n<\/ol>   \n\n <div>\n    <h2><center style=\"background-color:#00878b; color:white;\">  CONCLUSION <\/center><a id=\"8\"><\/a><\/h2>\n<\/div>\n<p style=\"text-align:justify;font-size:16px\">\nAs we look through the adoption of AI, advancements in core machine learning, computer vision, NLP are dominating usage and applications. But according to the survey results, the machine learning enthusiasts aren't much aware of all the recent developements or doesn't prefer them. Now we understand why this report is named as (Not) AI Hype Cycle 2021. A new technology always gets discovered, one doesn't has to expertise in all the tools and technologies but one can atleast experiment them through various resources available. This would help them build their profile if they are looking for a career in data science or even if they are already a part of data science community. \n<\/p>\n\n****\n### References\n1. [Gartner Hype Cycle For AI 2021](https:\/\/www.gartner.com\/en\/articles\/the-4-trends-that-prevail-on-the-gartner-hype-cycle-for-ai-2021)\n2. [Gartner Hype Cycle For AI 2021 Glance by Kamal Mishra ](https:\/\/medium.com\/@mishra.kamal\/gartner-hype-cycle-for-ai-2021-3c11c1d80358)\n3. [A survey of Deep Learning and its Applications](https:\/\/link.springer.com\/article\/10.1007\/s11831-019-09344-w)\n4. [AutoML is the future of Machine learning, LatentView](https:\/\/www.latentview.com\/blog\/is-automl-the-future-of-machine-learning\/)\n5. [Artificial Intelligence and Machine Learning for Beginners](https:\/\/digitate.com\/blog\/artificial-intelligence-and-machine-learning-for-beginners-part-1-of-3\/)\n6. [Data Science in the Cloud](https:\/\/www.kaggle.com\/nitishabharathi\/data-science-in-the-cloud-who-wins-the-cloud-war)\n7. [2021 Kaggle Data Science & Machine Learning Survey, Paul Mooney](https:\/\/www.kaggle.com\/paultimothymooney\/2021-kaggle-data-science-machine-learning-survey)\n\n****\n\n<p style=\"text-align:justify;font-size:11px\"> <i> All the insights discussed are with respect to only 2021 Kaggle Survey Data<\/i> <\/p>","2a1d3a71":"  <p style=\"text-align:justify;font-size:16px\">   \n  <ol style=\"font-size:16px\">\n      <li> Image classification methods are highly used. This could be because image classification is a primary problem in computer vision which has it's use in many use cases. <\/li>\n         <li> 20% of them are using GANs which is the current trend in CV\n             <li> 19% of them aren't using any of the CV methods which is a good sign when compared to NLP\n     <\/ol>\n<\/p>","07a8f5f3":"  <ol style=\"font-size:16px\">\n       <li> 24% aren't using any of the business intelligence tools but for people who does analysis to understand it would be highly useful\n           <li> Tableau and Power BI are the popular among other tools\n    <\/ol>"}}