{"cell_type":{"21693dda":"code","8d1898e1":"code","6c9a8423":"code","5efd9d12":"code","a3dca43f":"code","ab355c38":"code","e1c6b208":"code","72e78772":"code","250a4910":"code","2a2a65a5":"code","fe12d02e":"code","6b185bff":"code","5b612249":"code","3c67f007":"code","90f793fa":"code","f8a6c8c3":"code","2bb36a2d":"code","5136fac2":"markdown","0501eccf":"markdown","36100e67":"markdown","77e01095":"markdown","e310bb16":"markdown","1e5372e0":"markdown","cf96dad6":"markdown","48983057":"markdown","3d714f79":"markdown","3005e667":"markdown","853d3a48":"markdown","0b254141":"markdown"},"source":{"21693dda":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8d1898e1":"from sklearn.preprocessing import LabelBinarizer\nlb= LabelBinarizer()","6c9a8423":"df= pd.read_csv(\"..\/input\/traindata.csv\")\ndf.dropna(axis= 0)\ndf\n","5efd9d12":"#Regular expressions help us deal with characters and numerics and modify them according to our requirement \nimport re\n#import the nltk library\nfrom nltk.corpus import stopwords\nsw = set(stopwords.words('english'))","a3dca43f":"from nltk.stem.porter import PorterStemmer \nps=PorterStemmer()","ab355c38":"#creating a function to encapsulate our entire preprocessing and feature engineering steps\ndef processing(df):\n    #get the no. of punctuations\n    df['specialchars'] = df['text'].apply(lambda x : len(x) - len(re.findall('[\\w]', x)))\n    #lowering and removing punctuation\n    df['processed'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n    #removing the numerical values and working only with text values\n    df['processed'] = df['processed'].apply(lambda x : re.sub('[^a-zA-Z]', \" \", x ))\n    #removing the stopwords\n    df['processed'] = df['processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in sw ]))\n    #stemming the words\n    df['processed'] = df['processed'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n    #count the no. of words in the text\n    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n    df[\"Intent\"]= lb.fit_transform(df[\"Intent\"])\n    return df\n    \ndf= processing(df)   \ndf.head()\n\n","e1c6b208":"from sklearn.model_selection import train_test_split\n\ndef Xysplit(df):\n    features= [x for x in df.columns.values if x not in ['text', 'Intent']]\n    target = 'Intent'\n    X= df[features]\n    y= df[target]\n    return X, y","72e78772":"X, y =Xysplit(df)\n","250a4910":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass TextSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations on\n    Use on text columns in the data\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.key]\n    \nclass NumberSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations on\n    Use on numeric columns in the data\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[[self.key]]\n","2a2a65a5":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n\n\ntext = Pipeline([\n                ('selector', TextSelector(key='processed')),\n                ('vec', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                 ])\n\n","fe12d02e":"from sklearn.preprocessing import StandardScaler\n\n\nwords =  Pipeline([\n                ('selector', NumberSelector(key='words')),\n                ('standard', StandardScaler())\n            ])\nspecialchars =  Pipeline([\n                ('selector', NumberSelector(key='specialchars')),\n                ('standard', StandardScaler()),\n            ])","6b185bff":"from sklearn.pipeline import FeatureUnion\nfeats = FeatureUnion([('text', text),              \n                      ('words', words),\n                      ('specialchars', specialchars),\n                      ])\n","5b612249":"from sklearn.ensemble import RandomForestClassifier\npipeline = Pipeline([\n    ('features', feats),\n    ('classifier', RandomForestClassifier())\n])","3c67f007":"from sklearn.model_selection import RandomizedSearchCV\nparameters = { \n    'features__text__vec__ngram_range': [(1,1), (1,2)],\n    'classifier__min_samples_leaf' : [1, 2, 3],\n    'classifier__min_samples_split' : [2, 3, 4],\n    'classifier__max_depth' : [30, 40, 50, 55],\n    'classifier__n_estimators' : [1200, 1300, 1350, 1375]\n\n              }\n\nclf= RandomizedSearchCV(pipeline, parameters, cv= 5, n_jobs= -1)","90f793fa":"clf.fit(X, y)","f8a6c8c3":"#predicting on test data\nsubmission = pd.read_csv('..\/input\/testdata.csv')\n\n#preprocessing\nsubmission = processing(submission)\n\n#splitting\nX_test, y_test= Xysplit(submission)\npredictions = clf.predict(X_test)\n#printing the accuracy score\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, predictions)\n\n","2bb36a2d":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","5136fac2":"train_test_split splits our dtaset into train and test set and we have set the size of test set as 20%","0501eccf":"Using the preprocessing step, we''ll add and modify some features. \"Processed\" column is free from punctuations, capitals, and special characters. We'll perform the text transformation on this column. We have also included columns such as  length of words, special characters, etc","36100e67":"The nltk library has a corpus of words called stopwords. This contains words like a, an, the, etc. ","77e01095":"The purpose of stemming is to understand that the words love, loved and loving have basically the same meaning.","e310bb16":"FeatureUnion is used to join all our pipelines together into a single pipeline.","1e5372e0":"Our precision score for \"NO intent\" is fair with >80%. But the Intent classificatin has a low precision because the no. of data available for the model to train on intent was comparitively less. Precision\/Recall score show how likely we are to classify an Intent message as irrelevant or vice versa. Good values for Recall\/Precision score\nPrecision = TP\/TP+FP     (TP True positive, FP: False positive, TN: True Negative, FN: False Negative)\nRecall= TP\/TP+FN\n\nour model predicted with an accuracy of 73%. I tried different model such as LogisticRegression, NaiveBayes, XGBoost, \nand also trained the data using neural network but the accuracy was nearly 70% everywhere.\nMore feature engineering would do some imporvement in our set, or using a word2vec embedding would lead to \nimproved accuracy. Also, the dataset contains nearly 3000 values, so  if there was more Intent relevant  data available to us,\n our classifier could predict with greater accuracy.\n","cf96dad6":"Count vectorizer is a bag-of-words model. It returns a sparse matrix with 0\/1 values for all the words present in the datset. \nTF-IDF( Term Frequency- Inverse Document Frequency)\n TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n TfidfTransformer is used to transform COuntvectorizer to Tfidfvectorizer model","48983057":"We will be using pipelines to make our model. Pipelines helps us modularize our code  and reduce the complexities and steps involved. But pipelines only take transformer functions, i.e. functions which have fit_transform methods. \nOur dataset contains both text and numbers, sofor feature engineering using pipelines, we'll seperately create pipelines for text (using TextSelector) and for numbers( using Number Selector)","3d714f79":"Before using the data in our model, it was converted from txt format to csv file. The target column has been labeled as \"Intent\" & the email data column as \"text\". Now, pandas library is used to read the csv file in Python using pd.read_csv.  ","3005e667":"Label Binarizer converts out target variable to binary output ","853d3a48":"GridSearchCV is usedfor parametric tuning of our model and fit the model with best estimators. A grid\/dictionary is used to feed the classifier with varying parameters.","0b254141":"Standard scaler satndardizes the data depending on the distribution of data present. Here StandardScaler is called on all the numeric columns."}}