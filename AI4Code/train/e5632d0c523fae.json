{"cell_type":{"9d873a90":"code","a4f17a18":"code","025c84fd":"code","bc00cb06":"code","ccc39d46":"code","a308de1b":"code","f05032f7":"code","1926686b":"code","ab4b6657":"code","5ca086e9":"code","db3829d5":"code","73739957":"code","120aefb8":"code","ca38cf1e":"code","244cb28b":"code","5e386ca5":"code","4485118c":"code","aa184a1a":"code","d61ebc3e":"code","22cf2fe1":"code","bede8d84":"code","a470a95b":"code","4bc7b321":"code","f9d0239f":"code","0c0160da":"code","0eefbbe8":"code","516f4498":"code","43171307":"code","cfa3d9c1":"markdown","5947e5a8":"markdown","4ff83d71":"markdown","c5cdbfe3":"markdown","e2e1cee2":"markdown","bbbe3bb3":"markdown","31887889":"markdown","c2648f19":"markdown","6d06d177":"markdown","c5faca6e":"markdown","9f403d59":"markdown","227be812":"markdown","e8e2c9e8":"markdown","88d04c10":"markdown","ceada75c":"markdown"},"source":{"9d873a90":"import tensorflow as tf\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n% matplotlib inline","a4f17a18":"EXAMINE = 21\nSEED = 22\nnp.random.seed(SEED)","025c84fd":"def get_gender_as_num(gender):\n    if gender == \"male\":\n        return 0\n    else:\n        return 1","bc00cb06":"def get_age_group(age): # HIGH NOTE: changing each of the scalars to a vector. This is probably not a good idea\n    if age < 18:\n        # 13 - 17\n        return [1, 0, 0]\n    elif age < 28:\n        # 23 - 27\n        return [0, 1, 0]\n    elif age < 49:\n        # 33 - 48\n        return [0, 0, 1]\n    else:\n        return [0, 0, 0]","ccc39d46":"blog_posts_data_dir = \"..\/input\/blog-posts-labeled-with-age-and-gender\/\"\ntrain_file_name = \"train.json\"\ntest_file_name = \"test.json\"\n\n# Load data\nwith open(blog_posts_data_dir + train_file_name) as r:\n    training_set = json.load(r)\nraw_posts = [instance[\"post\"] for instance in training_set]","a308de1b":"print(raw_posts[EXAMINE])","f05032f7":"median_words_per_sample = np.median([len(instance[\"post\"]) for instance in training_set])\n\n# Map each word to a unique int value\nMAX_WORD_COUNT = 20000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = MAX_WORD_COUNT)\nposts = [instance[\"post\"] for instance in training_set]\ntokenizer.fit_on_texts(posts)\nword_index = dict(list(tokenizer.word_index.items())[:20000])\nsequences = tokenizer.texts_to_sequences(posts)\nmedian_words_per_tokenized_sample = np.median([len(post) for post in sequences])\ndata = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen = int(median_words_per_tokenized_sample),\n                                                     padding = \"post\")\nfor i, instance in enumerate(training_set):\n    instance[\"post\"] = data[i]\n    instance[\"gender\"] = get_gender_as_num(instance[\"gender\"])\n    instance[\"age\"] = get_age_group(int(instance[\"age\"]))\n","1926686b":"print(training_set[EXAMINE][\"post\"])\nprint(training_set[EXAMINE][\"age\"])","ab4b6657":"print(list(word_index.items())[ : 100])","5ca086e9":"samples_count = len(training_set)\n\ncategories_count = len(training_set[0][\"age\"])\n\nsamples_per_class = {0 : 0, 1 : 0, 2 : 0}\nfor instance in training_set:\n    for i, a in enumerate(instance[\"age\"]):\n        if a == 1:\n            samples_per_class[i] += 1\n            break\n ","db3829d5":"print(\"Number of Samples:\", samples_count)\nprint(\"Number of Categories:\", categories_count)\nprint(\"Samples per Class:\", samples_per_class)\nprint(\"Median Words per Sample:\", median_words_per_sample)\nprint(\"Median Words per Tokenized Sample:\", median_words_per_tokenized_sample)\nprint(\"Samples to Words Per Sample Ratio:\", samples_count \/ median_words_per_tokenized_sample)","73739957":"# plt.hist(list(length_distribution.keys()))\n# plt.xlabel(\"Length of a Sample\")\n# plt.ylabel(\"Number of samples\")\n# plt.show()","120aefb8":"EMBEDDING_DIM = 50\n\nglove_path = \"..\/input\/glove6b\/\"\nglove_dict = {}\nwith open(glove_path + \"glove.6B.50d.txt\") as f:\n    for line in f:\n        line_values = line.split(\" \")\n        word = line_values[0]\n        embedding_coefficients = np.asarray(line_values[1 : ], dtype = \"float32\")\n        glove_dict[word] = embedding_coefficients\n\nglove_weights = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    glove_vector = glove_dict.get(word)\n    if glove_vector is not None:\n        glove_weights[i] = glove_vector","ca38cf1e":"print(len(glove_weights))","244cb28b":"# Define the model\n# Embedding, Conv, Pool, Conv, Pool, Flatten, Dense, Dense\nmodel_1 = tf.keras.Sequential()\nmodel_1.add(tf.keras.layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights = [glove_weights],\n                                    input_length = median_words_per_tokenized_sample, trainable = True))\nmodel_1.add(tf.keras.layers.SeparableConv1D(50, 5, activation = \"relu\"))\nmodel_1.add(tf.keras.layers.MaxPooling1D())\nmodel_1.add(tf.keras.layers.SeparableConv1D(100, 3, activation = \"relu\"))\nmodel_1.add(tf.keras.layers.MaxPooling1D())\nmodel_1.add(tf.keras.layers.Flatten())\nmodel_1.add(tf.keras.layers.Dense(24, activation = \"sigmoid\"))\nmodel_1.add(tf.keras.layers.Dense(3, activation = \"softmax\"))\n","5e386ca5":"posts_train = np.array([instance[\"post\"] for instance in training_set])\nages_train = np.array([instance[\"age\"] for instance in training_set])","4485118c":"model_1.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\nmodel_1.summary()\nhistory_1 = model_1.fit(posts_train, ages_train, epochs = 10, batch_size = 500, validation_split = 0.2)","aa184a1a":"# Load data\nwith open(blog_posts_data_dir + test_file_name) as r:\n    test_set = json.load(r)","d61ebc3e":"test_posts = [instance[\"post\"] for instance in test_set]\ntest_sequences = tokenizer.texts_to_sequences(test_posts)\ntest_post_data = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen = int(median_words_per_tokenized_sample),\n                                                     padding = \"post\")\nfor i, instance in enumerate(test_set):\n    instance[\"post\"] = test_post_data[i]\n    instance[\"gender\"] = get_gender_as_num(instance[\"gender\"])\n    instance[\"age\"] = get_age_group(int(instance[\"age\"]))","22cf2fe1":"posts_test = np.array([instance[\"post\"] for instance in test_set])\nages_test = np.array([instance[\"age\"] for instance in test_set])","bede8d84":"model_1.evaluate(posts_test, ages_test)","a470a95b":"# Define the model\n# Embedding, Conv, Pool, Conv, Pool, Flatten, Dense, Dense, Dense\nmodel_2 = tf.keras.Sequential()\nmodel_2.add(tf.keras.layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights = [glove_weights],\n                                    input_length = median_words_per_tokenized_sample, trainable = True))\nmodel_2.add(tf.keras.layers.SeparableConv1D(100, 5, activation = \"relu\"))\nmodel_2.add(tf.keras.layers.MaxPooling1D())\nmodel_2.add(tf.keras.layers.SeparableConv1D(200, 3, activation = \"relu\"))\nmodel_2.add(tf.keras.layers.MaxPooling1D())\nmodel_2.add(tf.keras.layers.Flatten())\nmodel_2.add(tf.keras.layers.Dense(48, activation = \"sigmoid\"))\nmodel_2.add(tf.keras.layers.Dense(24, activation = \"sigmoid\"))\nmodel_2.add(tf.keras.layers.Dense(3, activation = \"softmax\"))","4bc7b321":"model_2.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\nmodel_2.summary()\nhistory_2 = model_2.fit(posts_train, ages_train, epochs = 7, batch_size = 500, validation_split = 0.2)","f9d0239f":"model_2.evaluate(posts_test, ages_test)","0c0160da":"model_2.evaluate(posts_test, ages_test)","0eefbbe8":"# Define the model\n# Embedding, Conv, Pool, Conv, Pool, Flatten, Dense, Dense\nmodel_3 = tf.keras.Sequential()\nmodel_3.add(tf.keras.layers.Embedding(len(word_index) + 1, EMBEDDING_DIM, weights = [glove_weights],\n                                    input_length = median_words_per_tokenized_sample, trainable = True))\nmodel_3.add(tf.keras.layers.SeparableConv1D(100, 5, activation = \"relu\"))\nmodel_3.add(tf.keras.layers.MaxPooling1D())\nmodel_3.add(tf.keras.layers.SeparableConv1D(200, 3, activation = \"relu\"))\nmodel_3.add(tf.keras.layers.MaxPooling1D())\nmodel_3.add(tf.keras.layers.SeparableConv1D(100, 3, activation = \"relu\"))\nmodel_3.add(tf.keras.layers.MaxPooling1D())\nmodel_3.add(tf.keras.layers.Flatten())\nmodel_3.add(tf.keras.layers.Dense(24, activation = \"sigmoid\"))\nmodel_3.add(tf.keras.layers.Dense(3, activation = \"softmax\"))\n","516f4498":"model_3.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\nmodel_3.summary()","43171307":"history_3 = model_3.fit(posts_train, ages_train, epochs = 10, batch_size = 500, validation_split = 0.2)","cfa3d9c1":"## Define","5947e5a8":"# Model 1","4ff83d71":"# Model 2","c5cdbfe3":"[An Introduction to Different Types of Convolutions](https:\/\/towardsdatascience.com\/types-of-convolutions-in-deep-learning-717013397f4d)","e2e1cee2":"## Test","bbbe3bb3":"## Train","31887889":"## Imports","c2648f19":"## Define","6d06d177":"## Find Key Metrics","c5faca6e":"# Model 3","9f403d59":"## Load and Preprocess Training Data","227be812":"## Import Pretrained Embeddings","e8e2c9e8":"## Define","88d04c10":"## Test","ceada75c":"## Train"}}