{"cell_type":{"af2ba45f":"code","d6579888":"code","cb16ed0c":"code","7dd2ef0d":"code","b3e027a3":"code","9359995a":"code","188422d1":"code","4cfb8670":"code","0ade956c":"code","5ee296c9":"code","f68497f3":"code","0345cbaa":"code","bd6589fb":"code","3add98a7":"code","3f0a521d":"code","3ea8d4f9":"code","ce9e53ab":"code","851e4e85":"code","b7778cc4":"code","3dda0788":"code","b4b19095":"code","a1363f06":"code","07bc3134":"code","6426eca8":"code","413498e9":"code","d3204ecf":"code","7869f8b3":"markdown","97d12310":"markdown","b8b0f3bd":"markdown"},"source":{"af2ba45f":"# Additional Dependencies\n!pip install barbar torchsummary","d6579888":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport time\nimport copy\nimport pickle\nfrom barbar import Bar\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport cv2\n%matplotlib inline\n\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\nfrom torchsummary import summary\n\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport gc\nRANDOMSTATE = 0\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","cb16ed0c":"class Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels,\n                               kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=stride, bias=False, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.relu(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out","7dd2ef0d":"class DeconvBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels, expansion=2, stride=1, upsample=None):\n        super(DeconvBottleneck, self).__init__()\n        self.expansion = expansion\n        self.conv1 = nn.Conv2d(in_channels, out_channels,\n                               kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        if stride == 1:\n            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                                   stride=stride, bias=False, padding=1)\n        else:\n            self.conv2 = nn.ConvTranspose2d(out_channels, out_channels,\n                                            kernel_size=3,\n                                            stride=stride, bias=False,\n                                            padding=1,\n                                            output_padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.upsample = upsample\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.relu(out)\n\n        if self.upsample is not None:\n            shortcut = self.upsample(x)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out","b3e027a3":"class ResNet_autoencoder(nn.Module):\n    def __init__(self, downblock, upblock, num_layers, n_classes=3):\n        super(ResNet_autoencoder, self).__init__()\n\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_downlayer(downblock, 64, num_layers[0])\n        self.layer2 = self._make_downlayer(downblock, 128, num_layers[1],\n                                           stride=2)\n        self.layer3 = self._make_downlayer(downblock, 256, num_layers[2],\n                                           stride=2)\n        self.layer4 = self._make_downlayer(downblock, 512, num_layers[3],\n                                           stride=2)\n\n        self.uplayer1 = self._make_up_block(\n            upblock, 512,  num_layers[3], stride=2)\n        self.uplayer2 = self._make_up_block(\n            upblock, 256, num_layers[2], stride=2)\n        self.uplayer3 = self._make_up_block(\n            upblock, 128, num_layers[1], stride=2)\n        self.uplayer4 = self._make_up_block(\n            upblock, 64,  num_layers[0], stride=2)\n\n        upsample = nn.Sequential(\n            nn.ConvTranspose2d(self.in_channels,  # 256\n                               64,\n                               kernel_size=1, stride=2,\n                               bias=False, output_padding=1),\n            nn.BatchNorm2d(64),\n        )\n        self.uplayer_top = DeconvBottleneck(\n            self.in_channels, 64, 1, 2, upsample)\n\n        self.conv1_1 = nn.ConvTranspose2d(64, n_classes, kernel_size=1, stride=1,\n                                          bias=False)\n\n    def _make_downlayer(self, block, init_channels, num_layer, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != init_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, init_channels * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(init_channels * block.expansion),\n            )\n        layers = []\n        layers.append(\n            block(self.in_channels, init_channels, stride, downsample))\n        self.in_channels = init_channels * block.expansion\n        for i in range(1, num_layer):\n            layers.append(block(self.in_channels, init_channels))\n\n        return nn.Sequential(*layers)\n\n    def _make_up_block(self, block, init_channels, num_layer, stride=1):\n        upsample = None\n        # expansion = block.expansion\n        if stride != 1 or self.in_channels != init_channels * 2:\n            upsample = nn.Sequential(\n                nn.ConvTranspose2d(self.in_channels, init_channels * 2,\n                                   kernel_size=1, stride=stride,\n                                   bias=False, output_padding=1),\n                nn.BatchNorm2d(init_channels * 2),\n            )\n        layers = []\n        for i in range(1, num_layer):\n            layers.append(block(self.in_channels, init_channels, 4))\n        layers.append(\n            block(self.in_channels, init_channels, 2, stride, upsample))\n        self.in_channels = init_channels * 2\n        return nn.Sequential(*layers)\n\n    def encoder(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        return x\n\n    def decoder(self, x, image_size):\n        x = self.uplayer4(x)\n        x = self.uplayer_top(x)\n\n        x = self.conv1_1(x, output_size=image_size)\n        return x\n\n    def forward(self, x):\n        img = x\n        tmp1 = self.encoder(x)\n        tmp2 = self.decoder(tmp1, img.size())\n        tmp3 = self.encoder(tmp2)\n\n        return tmp1, tmp2, tmp3\n\n\ndef ResNet50(**kwargs):\n    return ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3, **kwargs)\n\n\ndef ResNet101(**kwargs):\n    return ResNet_autoencoder(Bottleneck, [3, 4, 23, 2], 3, **kwargs)","9359995a":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","188422d1":"summary(ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3).to(device),(3,512,512))","4cfb8670":"# preparing intermediate DataFrame\ndatasetPath = Path('\/kaggle\/input\/cbir-dataset\/dataset\/')\ndf = pd.DataFrame()\n\ndf['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\ndf['image'] = '\/kaggle\/input\/cbir-dataset\/dataset\/' + df['image'].astype(str)\n\ndf.head()","0ade956c":"class CBIRDataset(Dataset):\n    def __init__(self, dataFrame):\n        self.dataFrame = dataFrame\n        \n        self.transformations = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n    \n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            raise NotImplementedError('slicing is not supported')\n        \n        row = self.dataFrame.iloc[key]\n        image = self.transformations(Image.open(row['image']))\n        return image\n    \n    def __len__(self):\n        return len(self.dataFrame.index)","5ee296c9":"def save_checkpoint(state, filename):\n    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n    print (\"=> Saving a new best\")\n    torch.save(state, filename)  # save checkpoint\n    \ndef load_ckpt(checkpoint_fpath, model, optimizer):\n    \n    # load check point\n    checkpoint = torch.load(checkpoint_fpath)\n\n    # initialize state_dict from checkpoint to model\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    # initialize optimizer from checkpoint to optimizer\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    # initialize valid_loss_min from checkpoint to valid_loss_min\n    #valid_loss_min = checkpoint['valid_loss_min']\n\n    # return model, optimizer, epoch value, min validation loss \n    return model, optimizer, checkpoint['epoch']\n    \ndef train_model(model,  \n                criterion, \n                optimizer, \n                #scheduler, \n                num_epochs):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n\n            # Iterate over data.\n            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n                inputs = inputs.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    tmp1, tmp2, tmp3 = model(inputs)\n\n                    loss1 = criterion(tmp2,inputs.detach())\n                    loss2 = criterion(tmp3,tmp1.detach())\n                    loss = loss1 + loss2\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n            #if phase == 'train':\n            #    scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f}'.format(\n                phase, epoch_loss))\n\n            # deep copy the model\n            if phase == 'val' and epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                save_checkpoint(state={   \n                                    'epoch': epoch,\n                                    'state_dict': model.state_dict(),\n                                    'best_loss': best_loss,\n                                    'optimizer_state_dict':optimizer.state_dict()\n                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_loss))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, optimizer, epoch_loss","f68497f3":"# Intermediate Function to process data from the data retrival class\ndef prepare_data(DF):\n    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n    train_set = CBIRDataset(trainDF)\n    validate_set = CBIRDataset(validateDF)\n    \n    return train_set, validate_set","0345cbaa":"EPOCHS = 100\nNUM_BATCHES = 8\nRETRAIN = False\n\ntrain_set, validate_set = prepare_data(DF=df)\n\ndataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n                }\n\ndataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n\nmodel = ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3).to(device)\n\ncriterion = nn.MSELoss()\n# Observe that all parameters are being optimized\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)","bd6589fb":"# If re-training is required:\n# Load the old model\nif RETRAIN == True:\n    # load the saved checkpoint\n    model, optimizer, start_epoch = load_ckpt('..\/input\/cbirpretrained\/conv_autoencoder.pt', model, optimizer)\n    print('Checkpoint Loaded')","3add98a7":"model, optimizer, loss = train_model(model=model, \n                    criterion=criterion, \n                    optimizer=optimizer, \n                    #scheduler=exp_lr_scheduler,\n                    num_epochs=EPOCHS)","3f0a521d":"# Save the Trained Model\ntorch.save({\n            'epoch': EPOCHS,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            }, 'ResNet_autoencoder_100ep.pt')","3ea8d4f9":"# cropped_df = df.iloc[:400]\n# test = CBIRDataset(cropped_df)\n# testloader = DataLoader(test, batch_size=1, num_workers=1)\n\ntransformations = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])","ce9e53ab":"# Load Model in Evaluation phase\nmodel = ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3).to(device)\nmodel.load_state_dict(torch.load('..\/input\/resnetcbir-pretrained\/ResNet_autoencoder_100ep.pt', map_location=device)['model_state_dict'], strict=False)\n#model.half()\nmodel.eval()","851e4e85":"# tensor = transformations(Image.open('..\/input\/cbir-dataset\/dataset\/0.jpg'))\n# tensor = tensor.to(device)\n# tensor = tensor.half()","b7778cc4":"#latent_features = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()","3dda0788":"def get_latent_features(images, transformations):\n    \n    latent_features = np.zeros((400,256,128,128))\n    \n    for i,image in enumerate(tqdm(images)):\n        tensor = transformations(Image.open(image))\n        tensor = tensor.to(device)\n        #tensor = tensor.half()\n        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n\n#     for i,image in enumerate(tqdm(testloader)):\n#         tensor = image.to(device)\n#         latent_features[i] = model.encoder(tensor).detach().numpy()\n        \n    del tensor\n    gc.collect()\n    return latent_features","b4b19095":"images = df.image.values[:350]\nlatent_features = get_latent_features(images, transformations)","a1363f06":"indexes = list(range(0, 350))\nfeature_dict = dict(zip(indexes,latent_features))\nindex_dict = {'indexes':indexes,'features':latent_features}","07bc3134":"def euclidean(a, b):\n    # compute and return the euclidean distance between two vectors\n    return np.linalg.norm(a - b)","6426eca8":"def perform_search(queryFeatures, index, maxResults=64):\n\n    results = []\n\n    for i in range(0, len(index[\"features\"])):\n        # compute the euclidean distance between our query features\n        # and the features for the current image in our index, then\n        # update our results list with a 2-tuple consisting of the\n        # computed distance and the index of the image\n        d = euclidean(queryFeatures, index[\"features\"][i])\n        results.append((d, i))\n    \n    # sort the results and grab the top ones\n    results = sorted(results)[:maxResults]\n    # return the list of results\n    return results","413498e9":"def build_montages(image_list, image_shape, montage_shape):\n\n    if len(image_shape) != 2:\n        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n    if len(montage_shape) != 2:\n        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n    image_montages = []\n    # start with black canvas to draw images onto\n    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n                          dtype=np.uint8)\n    cursor_pos = [0, 0]\n    start_new_img = False\n    for img in image_list:\n        if type(img).__module__ != np.__name__:\n            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n        start_new_img = False\n        img = cv2.resize(img, image_shape)\n        # draw image to black canvas\n        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n        cursor_pos[0] += image_shape[0]  # increment cursor x position\n        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n            cursor_pos[1] += image_shape[1]  # increment cursor y position\n            cursor_pos[0] = 0\n            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n                cursor_pos = [0, 0]\n                image_montages.append(montage_image)\n                # reset black canvas\n                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n                                      dtype=np.uint8)\n                start_new_img = True\n    if start_new_img is False:\n        image_montages.append(montage_image)  # add unfinished montage\n    return image_montages","d3204ecf":"# take the features for the current image, find all similar\n# images in our dataset, and then initialize our list of result\n# images\nfig, ax = plt.subplots(nrows=2,figsize=(15,15))\nqueryIdx = 23 # Input Index for which images \nMAX_RESULTS = 10\n\n\nqueryFeatures = latent_features[queryIdx]\nresults = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\nimgs = []\n\n# loop over the results\nfor (d, j) in results:\n    img = np.array(Image.open(images[j]))\n    imgs.append(img)\n\n# display the query image\nax[0].imshow(np.array(Image.open(images[queryIdx])))\n\n# build a montage from the results and display it\nmontage = build_montages(imgs, (512, 512), (5, 2))[0]\nax[1].imshow(montage)","7869f8b3":"## Search","97d12310":"# Inference","b8b0f3bd":"## Indexing"}}