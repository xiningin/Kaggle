{"cell_type":{"52b404f2":"code","53161863":"code","1b171603":"code","6e76e9fd":"code","32001fb3":"code","a4635ab9":"code","7f1bf827":"code","71f92cf6":"code","66e2b746":"code","7d8b15e2":"code","c513cac6":"code","f0fcabcf":"code","44fd2007":"code","fcbdc305":"code","60f134b4":"code","7c9beeca":"code","e1e14b4e":"code","e5c889fe":"code","390ac299":"code","fef8cf6c":"code","f1850740":"code","f52fd811":"code","68fec1f0":"code","48627a84":"code","5af83e58":"code","e3b32bf5":"code","d1438065":"code","44b06f22":"code","747a0bca":"code","e14e3e2a":"code","37a7f25c":"code","dbff24be":"code","3fcec86e":"code","f6bb6b17":"code","edc23d7b":"code","336aecfe":"code","4e195278":"code","f45db309":"code","d0163861":"code","14ab90d0":"code","8fd03734":"code","e028a11e":"code","56cb885f":"code","287e4054":"code","eedc044e":"code","f97662c1":"code","47638009":"code","c55fc76d":"code","e86b05e7":"code","1259a760":"code","36da6311":"code","bc5eab75":"code","0937056b":"code","5564eeac":"code","11f629a5":"code","40a84670":"code","6231d27f":"code","a0a858d4":"code","7ff80ab3":"code","4b0ae659":"code","a04ad087":"code","d3320a1b":"code","79acd2e8":"code","363c42b9":"code","2677d400":"code","dc514719":"code","621301da":"code","5328fa40":"code","46366b31":"code","f91d29c1":"code","6e43e1cb":"code","c85b9213":"code","8a1e5d7e":"code","68c6186c":"code","0ff5e1b9":"code","1dc82892":"code","abacda73":"code","c80d6154":"code","14965b6a":"code","9adf6553":"code","65dfe3ff":"code","2efc70c0":"code","41ba596d":"code","2abea48b":"markdown","81ec0a09":"markdown","c6a6f3cc":"markdown","dcc24ffc":"markdown","f084fd3d":"markdown","ac47fbd8":"markdown","7765bb30":"markdown","acc4ffde":"markdown","2121400b":"markdown","fdb2b60c":"markdown","207f5bf0":"markdown","458ec72b":"markdown","cd8e14b9":"markdown","f32e7073":"markdown","43b8d014":"markdown","724b857e":"markdown","26a7d860":"markdown","2f9c51a8":"markdown","bb115d5f":"markdown","356fccd6":"markdown","603999d2":"markdown","443e6acc":"markdown","84229d02":"markdown"},"source":{"52b404f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","53161863":"import seaborn as sb\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nimport spacy\nimport string\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# models\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, GRU\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport tensorflow as tf\n","1b171603":"pd.options.display.max_colwidth = 200  # set a value as required for better visualization","6e76e9fd":"!ls -lh \/kaggle\/input\/glovetwitter27b200dtxt","32001fb3":"# tweet train data\ntwdata=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n# test data\ntestdata=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","a4635ab9":"twdata.head()","7f1bf827":"testdata.head()","71f92cf6":"# check some random sample to get some more understanding on the keyword & location. \n# I am just showing one random sample, but to understand the data better I have checked many samples.\ntwdata.sample(n=5)","66e2b746":"# check the NaN count in various features.\ntwdata.isnull().sum()","7d8b15e2":"testdata.isnull().sum()","c513cac6":"print('training data shape {}, test data shape {}'.format(twdata.shape,testdata.shape))","f0fcabcf":"def plot_feature_col(data, col, comment):\n    keyword_dist=data[col].value_counts()\n\n    fig = plt.figure(figsize=(16, 3), dpi=100) # figsize-width,height\n\n    keyword_dist[:80].plot.bar() # take the top 80\n\n    plt.title(comment)\n    plt.legend([col+' count'])\n    plt.show()\n    \n    return keyword_dist","44fd2007":"# check the keyword distribution\nkeyword_dist_training = plot_feature_col(twdata, 'keyword', 'keyword Distribution in training data')","fcbdc305":"# check the keyword distribution using plot\nkeyword_dist_test = plot_feature_col(testdata, 'keyword', 'keyword Distribution in test data')","60f134b4":"# for keyword fill the NaN values as NAN, will remove the same later\ntwdata['keyword'].fillna('NAN', inplace=True)\ntestdata['keyword'].fillna('NAN', inplace=True)","7c9beeca":"# combine keyword & text\ntwdata['text']=twdata['text'] + \" \" + twdata['keyword']\ntestdata['text']=testdata['text'] + \" \" + testdata['keyword']","e1e14b4e":"# Now I will drop location & keyword.\ntwdata.drop(columns=['location','keyword'],axis=1, inplace=True)\ntestdata.drop(columns=['location','keyword'],axis=1, inplace=True)","e5c889fe":"twdata.head(2)","390ac299":"testdata.head(2)","fef8cf6c":"twdata.isnull().sum()","f1850740":"testdata.isnull().sum()","f52fd811":"target_dist_training = plot_feature_col(twdata, 'target', 'target distribution in training data')","68fec1f0":"# load spacy for data cleaning\nnlp = spacy.load('en_core_web_sm')","48627a84":"# tokenize \ntwdata['clean_text']=twdata['text'].apply(lambda x: list(nlp(x)))","5af83e58":"# function to clean the text data using spaCy\ndef spacy_clean_text(text):\n    # remove punctuations \n    text = [t for t in text if (t.is_punct == False)]\n\n    # remove stopwords\n    text = [t for t in text if (t.is_stop == False)]\n    \n    # remove digits\n    text = [t for t in text if (t.is_digit == False)]\n    \n    # lemmatize (should be done at the end)\n    text = [t.lemma_ for t in text]\n    \n    # join it to get back the original sentence\n    text = \" \".join(text)\n\n    # convert to lower case\n    text = text.lower()\n\n    return text","e3b32bf5":"twdata['clean_text'] = twdata['clean_text'].apply(lambda x: spacy_clean_text(x))","d1438065":"# check the cleaned data\ntwdata.sample(5)","44b06f22":"# clean text data still has http:\/\/, @..., so need to clean further\ndef clean_regex(text):\n    # remove http:\/\/\n    text = re.sub(r'http\\S+', '', text)\n    # remove '!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~'\n    text = ''.join([x for x in text if x not in string.punctuation])\n    # remove non ascii\n    text = ''.join([x for x in text if ord(x) < 128])\n\n    return text","747a0bca":"twdata['clean_text'] = twdata['clean_text'].apply(lambda x: clean_regex(x))","e14e3e2a":"twdata.sample(5)","37a7f25c":"# check the clean data length distribution\nclean_text_length=[len(x) for x in twdata['clean_text']] \nsb.distplot(clean_text_length, axlabel='clean_text length', color=\"g\")","dbff24be":"twdata.shape","3fcec86e":"# rt & nan is still there\ndef clean_regex_next(text):\n    # remove rt\n    text = re.sub(r'rt', '', text)\n    # remove nan\n    text = re.sub(r'nan', '', text)\n    # remove digits\n    text = \"\".join(t for t in text if not t.isdigit())\n   \n    return text","f6bb6b17":"twdata['clean_text'] = twdata['clean_text'].apply(lambda x: clean_regex_next(x))","edc23d7b":"twdata.head()","336aecfe":"# lets plot to understand the most common word's\nall_data= \" \".join(twdata['clean_text'])\nwords=all_data.split()\n\n\nsb.set(rc={'figure.figsize':(20,5)})\n\nnltk_plot=nltk.FreqDist(words)\nnltk_plot.plot(100)","4e195278":"def drop_single_char(text):\n    text =  text.split()\n    text = \" \".join(t for t in text if len(t)>1)\n\n    return text","f45db309":"twdata['clean_text'] = twdata['clean_text'].apply(lambda x: drop_single_char(x))","d0163861":"# there should be no single charecters now\nall_data=\" \".join(twdata['clean_text'])\nwords=all_data.split()\n\nsb.set(rc={'figure.figsize':(20,5)})\n\nnltk_plot=nltk.FreqDist(words)\nnltk_plot.plot(100)","14ab90d0":"testdata['clean_text']=testdata['text'].apply(lambda x: list(nlp(x)))\ntestdata['clean_text'] = testdata['clean_text'].apply(lambda x: spacy_clean_text(x))","8fd03734":"testdata['clean_text'] = testdata['clean_text'].apply(lambda x: clean_regex(x))\ntestdata['clean_text'] = testdata['clean_text'].apply(lambda x: clean_regex_next(x))\ntestdata['clean_text'] = testdata['clean_text'].apply(lambda x: drop_single_char(x))","e028a11e":"# verify the test data\ntestdata.head()","56cb885f":"# check the clean text length distribution for test data\nclean_text_length=[len(x) for x in testdata['clean_text']] \nsb.distplot(clean_text_length, axlabel='test clean_text length', color=\"g\")","287e4054":"testdata.shape","eedc044e":"# plot the word frequency to understand the test data\nall_data_test=\" \".join(testdata['clean_text'])\nwords_test=all_data_test.split()\n\nsb.set(rc={'figure.figsize':(20,5)})\n\nnltk_plot=nltk.FreqDist(words_test)\nnltk_plot.plot(100)","f97662c1":"testdata.sample(5)","47638009":"# find single word in training data sentences\nsingle_word_train=[]\nfor sent in twdata.clean_text:\n  if len(sent.split())==1:\n    single_word_train.append(sent)\n\nprint(single_word_train)","c55fc76d":"for idx in twdata.index:\n    row_series=twdata.loc[idx]\n    if len(row_series['clean_text'].split())==1:\n        twdata.drop(idx, inplace=True) # drop th erecord","e86b05e7":"twdata.shape","1259a760":"# find single word in test data sentences\nsingle_word_test=[]\nfor sent in testdata.clean_text:\n  if len(sent.split())==1:\n    single_word_test.append(sent)\n\nprint(single_word_test)","36da6311":"df2 = pd.DataFrame(columns=['id', 'text', 'target', 'clean_text'],\n                  data=[[7607, 'hey',0, 'hey'],\n                        [7608, 'fuck',0, 'fuck'],\n                        [7609, 'nooooooooo',0, 'nooooooooo'],\n                        [7610, 'tell',0, 'tell'],\n                        [7611, 'awesome',0, 'awesome']])\ndf2.head()","bc5eab75":"twdata = twdata.append(df2)\ntwdata.shape","0937056b":"twdata.tail()","5564eeac":"# save the cleaned data for future processing.\ntwdata.to_pickle('train_clean.pickle')\ntestdata.to_pickle('test_clean.pickle')","11f629a5":"# create the word embedding index.\nembeddings_index = dict()\nf = open('..\/input\/glovetwitter27b200dtxt\/glove.twitter.27B.200d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","40a84670":"# join the train and test data to tokenize.\ntrain_rows=twdata['clean_text'].shape[0]\ntest_rows=testdata['clean_text'].shape[0]\n\ntotal_data=pd.concat([twdata, testdata], sort=False)\nprint('total date shape {}'.format(total_data.shape))","6231d27f":"# tokenize the total data\nvocabulary_size = 20000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(total_data['clean_text'])\n\nsequences = tokenizer.texts_to_sequences(total_data['clean_text'])\ndata = pad_sequences(sequences, maxlen=20)","a0a858d4":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocabulary_size, 200))\nfor word, index in tokenizer.word_index.items():\n    if index > vocabulary_size - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","7ff80ab3":"model_glove = Sequential()\n# word embedding, trainable should be False.\nmodel_glove.add(Embedding(vocabulary_size, 200, input_length=20, weights=[embedding_matrix], trainable=False))\n\nmodel_glove.add(Dropout(0.2)) # to avoid overfitting.\nmodel_glove.add(Conv1D(128, 5, activation='relu'))\nmodel_glove.add(MaxPooling1D(pool_size=4))\n\n# using GRU we can do training faster than LSTM.\nmodel_glove.add(GRU(128,dropout=0.1, recurrent_dropout=0.2))\n\n# final dense layers\nmodel_glove.add(Dense(32, activation='relu'))\nmodel_glove.add(Dense(16, activation='relu'))\nmodel_glove.add(Dense(8, activation='relu'))\n\n#outout layer.\nmodel_glove.add(Dense(1, activation='sigmoid'))\n","4b0ae659":"model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","a04ad087":"model_glove.summary()","d3320a1b":"# create early stop criteria and save the best model.\nes = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=2)  \nmc = ModelCheckpoint('best_model.h5', monitor='loss', mode='min', save_best_only=True,verbose=1)","79acd2e8":"# lets train the model.\nbatch_size = 128\nnum_epochs = 20\n\nhistory = model_glove.fit(data[:train_rows, :], twdata['target'], batch_size = batch_size, epochs = num_epochs, callbacks=[es,mc])","363c42b9":"# plot the training loss and accuracy to understand how the model has performed.\nhistory_dict = history.history\n\nacc = history_dict['accuracy']\nloss=history_dict['loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss, 'b', label='loss')\nplt.plot(epochs, acc, 'r', label='accuracy')\n\nplt.title('Training loss & accuracy')\nplt.xlabel('loss')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\n","2677d400":"from tensorflow.keras.models import load_model\n\n# load best model\nmodel = load_model('best_model.h5')\ny_pred = model.predict(data[train_rows:, :])","dc514719":"sample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\", nrows=test_rows)\nsample['target'] = np.round(y_pred).astype('int')\nsample.to_csv('model_submission.csv', index=False)","621301da":"# lets manually check the prediction value and compare with test data.\nsample.head(20)","5328fa40":"testdata.head(20)","46366b31":"!kaggle competitions submit -c nlp-getting-started -f model_submission.csv -m \"Message\"","f91d29c1":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","6e43e1cb":"import tensorflow_hub as hub\n\nimport tokenization\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n","c85b9213":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","8a1e5d7e":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","68c6186c":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","0ff5e1b9":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","1dc82892":"train_input = bert_encode(twdata.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(testdata.text.values, tokenizer, max_len=160)\ntrain_labels = twdata.target.values","abacda73":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","c80d6154":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","14965b6a":"from tensorflow.keras.models import load_model\n\n# load best model\nmodel.load_weights('model.h5')\ntest_pred = model.predict(test_input)","9adf6553":"test_pred[:10]","65dfe3ff":"sample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample['target'] = test_pred.round().astype(int)\nsample[:20]","2efc70c0":"testdata[:20]","41ba596d":"sample.to_csv('model_submission.csv', index=False)","2abea48b":"# Remove noise from data\nIn the train & test data both consists of lot of noise (stop words, delimeters, special symbols, lower case etc.), that need to be removed before we process it further.","81ec0a09":"**Check the test data keyword distribution**","c6a6f3cc":"Some sentence in training and test data consists of only one word. If they are not useful it has to be dropped.","dcc24ffc":"**Analysis:**\n\nFrom the above its observed that many places keyword's are allready present in the text and some time not.\nAlso many keyword's are common in training & test data.\n**Will combine the keyword and text features for further analysis**","f084fd3d":"We need to remove the NaN from the datsets, this will be done in the next section.","ac47fbd8":"# create model with Google BERT","7765bb30":"All the above records need to be dropped as they are not adding any value in the training data.","acc4ffde":"# Check the training and test data source","2121400b":"# Create Model with Tensorflow ","fdb2b60c":"Check the training & test data","207f5bf0":"Now training data is ready for creating a model.","458ec72b":"**Decision:**\n\nthere is no big gap between two different kind of taget types(i.e. 1 & 0), will go with this as it is","cd8e14b9":"# Introduction\n\n* Here I will analyze training and test data, will understand its features. \n* As normally twitter texts are unprocessed, so I will clean the data using various tools like regex, spaCy etc.\n* Will use GloVe for pretrained vectors.\n* Finally, I will train a model using deep learning (Tensorflow).\n* Will do the prediction using the trained mode.\n\nI will try to keep the implementation simple with explanations in each step.\n\nI have learned from various sources including https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub, please upvote it also.\nFor anything unclear or any suggestion for improvement of the kernel please do let me know.\n\nThanks\n\nArijit","f32e7073":"**location\" does not contain valid data and in many places data is missing. So will drop this feature.**","43b8d014":"# Data Analysis","724b857e":"**Lets use the GloVe vectors and prepare the word embedding**\n\nwill use the http:\/\/nlp.stanford.edu\/data\/glove.twitter.27B.zip data for the pretrained vectors.","26a7d860":"**Use the model for doing the prediction**","2f9c51a8":"** similar way clean the test data also **","bb115d5f":"Check the training and test data features ","356fccd6":"**In the above its visible that there are few single charecters, need to drop them.**","603999d2":"this are also not usefull for test data, but record cant be dropped so the submission length will go wrong.\nInsted we can add this data as record in training data for creating model, then model will take care of the same while doing prediction.","443e6acc":"*Analyze the **\"keyword\"** feature","84229d02":"**lets check the 'target' to understand its distribution.**"}}