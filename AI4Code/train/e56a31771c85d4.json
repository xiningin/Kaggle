{"cell_type":{"d42402eb":"code","1ac6fa92":"code","4c2f89a1":"code","74bfea20":"code","6dbc4003":"code","0aa81f4d":"code","a2dd60d0":"code","5f7cf9d8":"code","c03c84d1":"code","04ad6e44":"code","bf38bd87":"code","71c1d38a":"code","07609f1f":"code","72abb7ea":"code","067eb9b8":"code","c36dce36":"code","ad4cd3e5":"code","f8c78490":"code","373bc682":"code","90bcb9f1":"code","129b53f1":"code","4fb7e96c":"code","87fe9aa5":"code","aa41dae0":"code","0eb3d143":"code","afebb916":"code","6b555acd":"code","5e9df776":"code","8d0eea87":"code","7fcf7257":"code","539d5518":"code","8ecdd469":"code","d0b9a402":"code","016ab2b2":"code","c60d3a6d":"code","6540495e":"code","baea1e45":"code","4590ff1d":"code","70489209":"code","9ce52ef8":"code","450eced6":"code","4bbe58e7":"code","c2aa79ff":"code","e22dd9a7":"code","ac2bc018":"code","43738b45":"code","7fde2c49":"code","fbd079f7":"code","945be194":"code","30b00f7f":"code","4e0434d2":"code","274c09c2":"code","4fbd54c1":"code","9ecd1f91":"code","aa5c4b26":"code","954456b7":"markdown","4fbb6f79":"markdown","1c04bf41":"markdown","aa03f791":"markdown","51696208":"markdown","6615b17d":"markdown","db003dd3":"markdown","1ba5e64d":"markdown","636a3666":"markdown","8b226f08":"markdown","e08e684b":"markdown","de772394":"markdown"},"source":{"d42402eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Importing the libraries for ploting\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1ac6fa92":"#Importing the datasets\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_sub = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","4c2f89a1":"#Lets check the dataset\ndf_train.shape, df_test.shape, df_sub.shape","74bfea20":"df_train.info()","6dbc4003":"df_test.info()","0aa81f4d":"#At first we check our target variable\nsns.set_style(\"whitegrid\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\nsns.distplot(df_train['SalePrice'],color=\"b\")\nsns.despine(trim=True, left=True)\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.title('Distribution of Saleprice')\nprint('Skewness: %f', df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())\nplt.show()","a2dd60d0":"#Log transformation  \ndf_train['SalePrice_Log'] = np.log(df_train['SalePrice'])\nprint('Skewness: %f', df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())\nsns.set_style(\"whitegrid\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\nsns.distplot(df_train['SalePrice_Log'], color ='blue')\nplt.xlabel('SalePrice_Log')\nplt.ylabel('Frequency')\nplt.title('Distribution of Saleprice_Log')\nplt.show()","5f7cf9d8":"#Lets drop the  SalePrice column\ndf_train.drop('SalePrice', axis =1, inplace = True)","c03c84d1":"fig = plt.figure(figsize=(20, 20))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=df_train['GrLivArea'], y=df_train['SalePrice_Log'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=df_train['TotalBsmtSF'], y=df_train['SalePrice_Log'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )","04ad6e44":"fig = plt.figure(figsize=(20,20))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=df_train['1stFlrSF'], y=df_train['SalePrice_Log'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=df_train['MasVnrArea'], y=df_train['SalePrice_Log'], color=('gold'),alpha=0.9)\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )","bf38bd87":"fig = plt.figure(figsize=(20,20))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=df_train['GarageArea'], y=df_train['SalePrice_Log'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1230, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=df_train['TotRmsAbvGrd'], y=df_train['SalePrice_Log'], color=('orange'),alpha=0.9)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('TotRmsAbvGrd - Price scatter plot', fontsize=15, weight='bold' )","71c1d38a":"# Now remove outliers\ndf_train.drop(df_train[df_train['GrLivArea'] > 4600].index, inplace=True)\ndf_train.drop(df_train[df_train['MasVnrArea'] > 1500].index, inplace=True)\ndf_train.shape ","07609f1f":"#Lets check the correlation and heat map\ncorr = df_train.corr()\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\nplt.figure(figsize = (20,16))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values,\n            annot=True,fmt='.2f',linewidths=0.30,\n            cmap = colormap, linecolor='white')\nplt.title('Correlation of df Features', y = 1.05, size=15)","72abb7ea":"#Lets look the correlation score\nprint (corr['SalePrice_Log'].sort_values(ascending=False), '\\n')","067eb9b8":"## Scatter plotting for and Putting a regression line.\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nsns.scatterplot(x=df_train['OverallQual'], y=df_train['SalePrice_Log'], color=('red'),alpha=0.5)\nsns.regplot(x=df_train['OverallQual'], y=df_train['SalePrice_Log'], color='red')\nplt.title('correlation of SalePrice_Log and OverallQual',weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nsns.scatterplot( x = df_train['GrLivArea'], y = df_train['SalePrice_Log'], color = 'red')\nsns.regplot(x=df_train['GrLivArea'], y=df_train['SalePrice_Log'], color = 'deeppink')\nplt.title('correlation of SalePrice_Log and GrLivArea',weight='bold' )","c36dce36":"# Scatter plotting and putting regression line\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nsns.scatterplot(x = df_train.GarageArea,y = df_train.SalePrice_Log , color= 'green')\nsns.regplot(x=df_train.GarageArea, y=df_train.SalePrice_Log, color = 'green')\nplt.title('corelation of SalePrice_Log and GarageArea',weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nsns.scatterplot(x = df_train. TotalBsmtSF,y = df_train.SalePrice_Log , color= 'orange')\nsns.regplot(x=df_train.TotalBsmtSF, y=df_train.SalePrice_Log, color = 'orange')\nplt.title('corelation of SalePrice_Log and TotalBsmtSF',weight='bold' )","ad4cd3e5":"#We can see almost same columns of both train and test set have missing values. So we join the datasets\nall_data = df_train.append(df_test)\nall_data.shape","f8c78490":"#Lets clean the dataset. Note that we don't need 'ID' to predict the value of the house. Lets drop the columns\nall_data.drop('Id', axis =1, inplace = True)","373bc682":"#Lets ckeck again the missing value\nall_data.isnull().sum()","90bcb9f1":"#We note the LotFrontage has almost 20% missing value, we replace the nan value with mean\nall_data['LotFrontage'] = all_data['LotFrontage'].fillna(all_data['LotFrontage'].mean())","129b53f1":"all_data['PoolQC'] = all_data['PoolQC'].fillna('None')\nall_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')\nall_data['Alley'] = all_data['Alley'].fillna('None')\nall_data['Fence'] = all_data['Fence'].fillna('None')\nall_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('None')","4fb7e96c":"for col in ('GarageType', 'GarageFinish', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\nfor col in ('GarageArea', 'GarageCars', 'GarageYrBlt', 'GarageQual'):\n    all_data[col] = all_data[col].fillna(0)","87fe9aa5":"for feat in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[feat] = all_data[feat].fillna('None')\nfor feat in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath', 'TotalBsmtSF'):\n    all_data[feat] = all_data[feat].fillna(0)","aa41dae0":"all_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\nall_data['Functional'] = all_data['Functional'].fillna('Typical')\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna('None') ","0eb3d143":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities']).mode()[0]   ","afebb916":"## Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)","6b555acd":"#get numeric features\nnumeric_features = [f for f in all_data.columns if all_data[f].dtype != object]","5e9df776":"#transform the numeric features using log(x + 1)\nfrom scipy.stats import skew\nskewed = all_data[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))\nskewed = skewed[skewed > 0.75]\nskewed = skewed.index\nall_data[skewed] = np.log1p(all_data[skewed])","8d0eea87":"#add TotalSF\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","7fcf7257":"#Log features\ndef logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\nall_data = logs(all_data, log_features)","539d5518":"#Square features\ndef squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\nall_data = squares(all_data, squared_features)","8ecdd469":"all_data = pd.get_dummies(all_data).reset_index(drop=True)\nall_data.shape","d0b9a402":"# Remove any duplicated column names\nall_data = all_data.loc[:,~all_data.columns.duplicated()]","016ab2b2":"train_new = all_data[all_data['SalePrice_Log'].notnull()]\ntest_new = all_data[all_data['SalePrice_Log'].isnull()]\ntest_new = test_new.drop(['SalePrice_Log'], axis = 1)","c60d3a6d":"train_new.shape, test_new.shape","6540495e":"y_train = train_new['SalePrice_Log'].values","baea1e45":"x = train_new.drop(['SalePrice_Log'], axis = 1)\nx_train = x.iloc[:,:].values\nx_test = test_new.iloc[:,:].values","4590ff1d":"from sklearn.preprocessing import RobustScaler\nRS = RobustScaler()\nx_train = RS.fit_transform(x_train)\nx_test = RS.transform(x_test)","70489209":"#Lets use the Lasso regression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\nlasso = Lasso()","9ce52ef8":"# list of alphas to tune\nparams = {'alpha': [0.001, 0.002, 0.003, 0.005, 0.006, 0.007, 0.008, 0.009]}","450eced6":"# cross validation\ngrid_search = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = 10, \n                        return_train_score=True,\n                        verbose = 1)            \ngrid_search.fit(x_train, y_train)","4bbe58e7":"#checking the value of optimum number of parameters\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=1000]\ncv_results","c2aa79ff":"#Fitting Lasso Regression to the tranning set\nlasso = Lasso(alpha = 0.001, max_iter = 1000)\nlasso.fit(x_train, y_train)","e22dd9a7":"#Lets check the accuracy \naccuracy_train = lasso.score(x_train, y_train)\nprint(accuracy_train)","ac2bc018":"#Xgboot regression\nfrom xgboost import XGBRegressor\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3000,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\nxgboost.fit(x_train,y_train)","43738b45":"accuracy_train = xgboost.score(x_train, y_train)\nprint(accuracy_train)","7fde2c49":"from lightgbm import LGBMRegressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\nlightgbm.fit(x_train, y_train)","fbd079f7":"accuracy_train = lightgbm.score(x_train, y_train)\nprint(accuracy_train)","945be194":"#Gradient boost regression\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\ngbr.fit(x_train, y_train)","30b00f7f":"accuracy_train = gbr.score(x_train, y_train)\nprint(accuracy_train)","4e0434d2":"##Now predicting the test set result\ny_pred = 0.25*lasso.predict(x_test)+0.25*gbr.predict(x_test)+0.25*lightgbm.predict(x_test)+0.25*xgboost.predict(x_test)","274c09c2":"#convert back from logarithmic values to SalePrice\ny_pred =np.expm1(y_pred) ","4fbd54c1":"#Output\nsub = pd.DataFrame()\nsub['Id'] = df_test['Id']\nsub['SalePrice'] = y_pred","9ecd1f91":"# Fix outleir predictions\nq1 = sub['SalePrice'].quantile(0.0045)\nq2 = sub['SalePrice'].quantile(0.99)\nsub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n# Scale predictions\nsub['SalePrice'] *= 1.001619","aa5c4b26":"sub.to_csv('submission.csv', index=False)","954456b7":"Let's plot how SalePrice_Log relates to some of the features in the dataset","4fbb6f79":"lets create some features","1c04bf41":"Lets Deal with catagorical variables","aa03f791":"Outliers visualization:","51696208":"Lets see the correlation with target variable","6615b17d":"**PoolQC, MiscFeature, Alley, Fence, FireplaceQu:**\n1. PoolQC: data description file says NA means no pool\n2. MiscFeature: data description file says NA means no misc feature\n3. Alley: data description file says NA means no alley access\n4. Fence: data description file says NA means no fence\n5. FireplaceQu: data description file says NA means no fireplace","db003dd3":"**GarageType, GarageFinish, GarageQual, GarageCond, GarageYrBuilt, GarageArea, GarageCars**\n* Garage Attributes: data description file says NA means no garage","1ba5e64d":"From above graph we see our target varibale is right skewed. so we need to log transform it.","636a3666":"* MSZoning, Utilities, KitchenQual, SaleType, Exterior1st, Exterior2nd, Electrical\ncan all be filled with the most common string in their respective columns:   ","8b226f08":"**BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtFinSF1,\nBsmtFinSF2, BsmtUnfSF, BsmtFullBath, BsmtHalfbath, TotalBsmtSF**\n*  Basement Attributes: data description file says NA means no basement   ","e08e684b":"* MasVnrArea, MasVnrType, Functional, MsSubClass,MasVnrArea and MasVnrType: data description file says NA means no masonry veneerFunctional: data description file says NA means typical  ","de772394":"Mechine Learning"}}