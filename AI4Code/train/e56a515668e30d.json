{"cell_type":{"883dce2b":"code","fa95e9c7":"code","2aacaba5":"code","2fbbd0db":"code","ca5352ac":"code","0c8e00d6":"code","112096d9":"code","f8486025":"code","e44db822":"code","5eed3f48":"code","6f24704e":"code","ff0c11e2":"code","80b64638":"code","b373cea1":"code","6d101523":"code","7c67894e":"code","bd811c1b":"code","ea5bd086":"code","20fc59cc":"code","e3fe3855":"code","aaa8a4c5":"code","1cdc45d3":"code","15a35d21":"code","49af6d50":"code","54e9be98":"code","49560639":"code","fcb6b2a5":"code","b513031a":"code","add00883":"markdown","28668745":"markdown","908df0d5":"markdown","bce52d16":"markdown","c7bdc176":"markdown","1e82dfdb":"markdown","91390d42":"markdown","cc5ee3eb":"markdown","b698b644":"markdown","e29edb85":"markdown","26af5d6a":"markdown","c73c2d07":"markdown","307e00fc":"markdown","c803bdca":"markdown","fade77cf":"markdown","9aabb191":"markdown","0abeac58":"markdown","3b6ea415":"markdown","410dc6ee":"markdown","f4894097":"markdown","7b8ba3cb":"markdown"},"source":{"883dce2b":"import os \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler ,RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(\"Packages Imported \")","fa95e9c7":"train_data= pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_data= pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsample_sub = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv') \nprint(\"Data imported\")","2aacaba5":"train_data.head()","2fbbd0db":"print(f'Number of rows: {train_data.shape[0]};  Number of columns: {train_data.shape[1]}; No of missing values: {sum(train_data.isna().sum())}')","ca5352ac":"train_data.describe().style.background_gradient(cmap='coolwarm')","0c8e00d6":"# variables variaition   \ndf_var=train_data.var().reset_index()\ndf_var.columns =['feature', 'variation']\ndf_var.sort_values(\"variation\",ascending = True)","112096d9":"# Correlationmatrix\ncorrMatrix =train_data.corr(method='pearson', min_periods=1)\ncorrMatrix","f8486025":"cor_targ = train_data.corrwith(train_data[\"target\"]).reset_index()\ncor_targ.columns =['feature', 'CorrelatioWithTarget']\ncor_targ.sort_values('CorrelatioWithTarget',ascending = False)","e44db822":"print('Count of target values:')\ncountplt, ax = plt.subplots(figsize = (8,5))\nax =sns.countplot(train_data['target'],palette=\"GnBu_r\")","5eed3f48":"test_data.head()","6f24704e":"print(f'Number of rows: {test_data.shape[0]};  Number of columns: {test_data.shape[1]}; No of missing values: {sum(test_data.isna().sum())}')","ff0c11e2":"test_data.describe().style.background_gradient(cmap='YlOrRd')","80b64638":"plt.figure(figsize=(15,8))\nfeatures = train_data.columns.values[1:101]\nsns.distplot(train_data[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test_data[features].mean(axis=1),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of mean values per row in the train and test data\")\nplt.legend()\nplt.show()","b373cea1":"plt.figure(figsize=(15,5))\nsns.distplot(train_data[features].mean(axis=0),color=\"orange\",kde=True,bins=120, label='train')\nsns.distplot(test_data[features].mean(axis=0),color=\"blue\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of mean values per column in the train and test set\")\nplt.legend()\nplt.show()","6d101523":"\nplt.figure(figsize=(15,5))\nsns.distplot(train_data[features].std(axis=1),color=\"#2F4F4F\", kde=True,bins=120, label='train')\nsns.distplot(test_data[features].std(axis=1),color=\"#FF6347\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of std per row in the train and test data \")\nplt.legend()\nplt.show()","7c67894e":"plt.figure(figsize=(15,5))\nsns.distplot(train_data[features].std(axis=0),color=\"#778899\",kde=True,bins=120, label='train')\nsns.distplot(test_data[features].std(axis=0),color=\"#800080\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of std per column in the train and test data\")\nplt.legend()\nplt.show()","bd811c1b":"i = 1\nplt.figure()\nfig, ax = plt.subplots(figsize=(30, 30))\nfor feature in features:\n    plt.subplot(20, 5,i)\n    sns.distplot(train_data[feature],color=\"blue\", kde=True,bins=120, label='train')\n    sns.distplot(test_data[feature],color=\"orange\", kde=True,bins=120, label='test')\n    i += 1\nplt.title(\"Feature Distribution in train and test data\")  \nplt.legend()\nplt.show()","ea5bd086":"### Install packages \n!pip install tensorflow-gpu==2.6.0 deeptables","20fc59cc":"# verify installation \nfrom deeptables.utils.quicktest import test; test()","e3fe3855":"y = train_data[\"target\"] # target\nused_feat = [ col for col in train_data.columns if col not in ['id','target']]\ntrain= train_data[[*used_feat]]  # train\ntest = test_data[[*used_feat]] # test \n","aaa8a4c5":"# feature enginnering \n#train_data \ntrain[\"r_sum\"] = train.sum(axis=1)\ntrain[\"r_skew\"] = train.skew(axis=1)\n#test_data\ntest[\"r_sum\"] = test.sum(axis=1)\ntest[\"r_skew\"] = test.skew(axis=1)","1cdc45d3":"# data standarization \nscaler = RobustScaler()\ntrain_scal=  scaler.fit_transform(train)\ntest_scal = scaler.transform(test)","15a35d21":"## import modules \nfrom deeptables.models.deeptable import DeepTable, ModelConfig\nfrom deeptables.models.deepnets import AutoInt , xDeepFM ,FGCNN ,DeepFM\nfrom deeptables.utils.shap import DeepTablesExplainer\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model","49af6d50":"# define model \nconf = ModelConfig(\n nets=xDeepFM, \n    auto_discrete=False,\n    earlystopping_patience =5,\n    metrics=['AUC'],\n    optimizer= 'Adam') \ndt = DeepTable(config=conf)\n","54e9be98":"# fit the model \noof_proba, eval_proba, test_proba = dt.fit_cross_validation(\n    train_scal, \n    y,\n    X_test=test_scal, \n    num_folds=5,\n    batch_size =1024,\n    verbose =0,\n    stratified= True, \n    epochs=50)","49560639":"plot_model(dt.get_model().model,rankdir='TB')","fcb6b2a5":"#check folds leaderboard \ndt.leaderboard.sort_values(by=['val_auc'] ,ascending = False)\n","b513031a":"predictions = pd.DataFrame(test_proba) # convert to data frame \nsample_sub[\"target\"]= predictions\nsample_sub.to_csv(\"deeptablesSub.csv\",index=False)\nsample_sub.head()","add00883":"#### Correlation matrix ","28668745":"Some features have very low variation maybe we will consider removing them","908df0d5":"#### Plot Model ","bce52d16":"Looking at this we can say we have similar ( train , test ) data distribution.","c7bdc176":"#### Test data ","1e82dfdb":"### Submission ","91390d42":"<a id=\"1\"><\/a>\n<h2 style='background:#00bfff; border:0; color:white'><center> Simple EDA <\/center><\/h2>\nBefore establishing a baseline , We may first take a look at our dataframes ( train and test ).\n<h3> Train data <h3> ","cc5ee3eb":"#### Correlation with the target ","b698b644":"<h2 style='background:#00bfff; border:0; color:white'><center>Notebook Set up<\/center><\/h2>","e29edb85":"<h2 style='background:#00bfff; border:0; color:white'><center>About the data<\/center><\/h2>\n\n  The data consists of three files :\n* train.csv - the training data with the target column\n* test.csv - the test set; you will be predicting the target for each row in this file (the probability of the binary target)\n* sample_submission.csv - a sample submission file in the correct format","26af5d6a":"<div class=\"alert alert-success\">\n  <p><strong>NOTE!<\/strong> This is just for the demonstration of deeptables for classification problems ,  which i hope you find it usefull , There is always a room for improvement (adjusting the ModelConfig parameter  , feature engineering , feature selection ). i will update this kernal as soon as possible. Happy learning.<\/p>\n<\/div> \n","c73c2d07":"<a id=\"1\"><\/a>\n<h2 style='background:#00bfff; border:0; color:white'><center> Establishing Baseline <\/center><\/h2>\n\nNow after we did some exploration about the data we start creating our model and then we can iterate . for this problem we are going to use  [Deeptables](https:\/\/github.com\/DataCanvasIO\/DeepTables) which is an is a easy-to-use toolkit that enables deep learning to unleash great power on tabular data.","307e00fc":"#### Target variable\nagain the target for this competion is also a binary value.","c803bdca":"<h1> <center> Tabular Playground Series - Nov 2021<\/center><\/h1>\n\n![Capture.JPG](attachment:ebf947cc-b576-4276-95f8-7c01183f3085.JPG)","fade77cf":"#### Data preparation ","9aabb191":"<a id=\"1\"><\/a>\n<h2 style='background:#00bfff; border:0; color:white'><center>Overview<\/center><\/h2>\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, kaggle team have launched many Playground competitions that are more approachable than Featured competitions and thus, more beginner-friendly.\n\nFor this competition, we will be predicting a binary target based on a number of feature columns given in the data. ll columns are continuous features .","0abeac58":"We have a balanced class problem with a little bit ones then zeros xD.","3b6ea415":"#### Feature Distribution ","410dc6ee":"#### Distribution of Mean and Standard deviation\nwe want to see the distribution of the mean and standard deviation values per row and column in both train and test data.","f4894097":"#### Summarry statistic ","7b8ba3cb":"<h2 style='background:#00bfff; border:0; color:white'><center>Load data<\/center><\/h2>"}}