{"cell_type":{"1d14ccf0":"code","fd1eb65c":"code","68ce791f":"code","01e3d3f4":"code","11b1b84b":"code","7205fa7f":"code","8da5508b":"code","b8810a3d":"code","0b499fbd":"code","bc6c6feb":"code","16092326":"code","8240ecc1":"code","8a7af3e1":"code","e107a4cb":"code","bf26f0c3":"code","6442deb2":"code","f7ff8a68":"code","4d88cbb7":"code","961c099d":"code","7733d273":"code","4e426c9d":"code","d5659eaa":"code","356c69e3":"code","9e881242":"code","9ada9f75":"code","162f7bad":"code","0e235d01":"code","38a94b10":"code","e6f3c2a7":"code","4eceb8c8":"code","e13e45d8":"code","76ebf6f2":"code","a618406f":"code","7caa7e2e":"code","b1fe8167":"code","f82638b4":"code","d67bdc53":"code","8c783ee6":"code","528eb771":"code","6079a9cf":"code","f95673e6":"code","00a6b326":"code","a7a4f8f9":"code","b15752af":"code","5a48c7c1":"code","a7e766fd":"code","b5b6e0c7":"code","086a5277":"code","621c5fdc":"code","db75b559":"code","371fe32f":"code","aedcdb00":"code","fb9d7c54":"code","78a122de":"code","69b1a837":"code","8d28e19f":"code","82f92d55":"code","ac31a84c":"code","8ecece57":"code","26db6842":"code","6b2956c1":"code","edfe8aa2":"code","76320309":"code","d19801aa":"code","9965e7c6":"code","b331e451":"code","6290896c":"code","1d334f53":"code","7db02be6":"code","d683cb5e":"code","f7a3479a":"code","9b1c4054":"code","cc5fe0ee":"code","a21aa368":"code","bfd5021a":"code","51e14b9c":"code","a73f466f":"code","74c2b462":"code","5818bdfe":"code","f7cfbb7b":"markdown","d363c4cc":"markdown","431b6fad":"markdown","0c46d42c":"markdown","bc3784bd":"markdown","2879af66":"markdown","4ae3119b":"markdown","344beab9":"markdown","0ac9000f":"markdown","06fbe2ba":"markdown","3fcbefb6":"markdown","f7d321fd":"markdown","780615cf":"markdown","2752228a":"markdown","f515adea":"markdown","f4b84176":"markdown","f4743659":"markdown","3cfc9a52":"markdown","cc41c200":"markdown","a5efe2cf":"markdown","604c12cc":"markdown","4eb6494b":"markdown","6e80d694":"markdown","804bd934":"markdown","8333b3c3":"markdown","542b1757":"markdown","16116e82":"markdown","cf7f6773":"markdown","f8b6c2ba":"markdown","ca9dfff5":"markdown","10667df0":"markdown","583363d5":"markdown","86042e85":"markdown","79617a26":"markdown"},"source":{"1d14ccf0":"# Generales\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nfrom datetime import *\nimport time\nfrom scipy import stats\nfrom statistics import mode\n\n#Graficos\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\n#Models y preprocesamiento\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold, ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold,GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb\n\n# Metricas\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import precision_score,recall_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, roc_curve\n\n#series temporales\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom sklearn.model_selection import TimeSeriesSplit \nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\n\n# Otros\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.pipeline import make_pipeline\n\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd1eb65c":"# !pip install xgboost","68ce791f":"## para leer en kaggle\ndf_index = pd.read_csv('\/kaggle\/input\/evol-metricas-prestadores\/index_50.csv')\ndf_metricas = pd.read_csv('\/kaggle\/input\/evol-metricas-prestadores\/metricas_prestador_plan.csv')\ndf_procedimientos = pd.read_csv('\/kaggle\/input\/evol-metricas-prestadores\/procedimientos.csv')\n\n## para leer en notebook local\n# df_index = pd.read_csv('data\/index_50.csv')\n# df_metricas = pd.read_csv('data\/metricas_prestador_plan.csv')\n# df_procedimientos  = pd.read_csv('data\/procedimientos.csv')\n\n\n\nprint(df_metricas.shape)\n\nprint(len(df_metricas.Prestador_ID.unique()))\n\ndf_metricas.sample(4)\n\n# (Prestador_ID\tPeriodo \tAgrupador_Plan) unico!","01e3d3f4":"print(sum(df_metricas.Periodo == '2020-12-01'))\ndf_metricas.columns","11b1b84b":"sns.histplot(df_metricas[df_metricas.Periodo=='2018-11-01'].afiliado_count, log_scale=True);\n# sns.boxplot(df_metricas[df_metricas.Periodo=='2018-11-01'].afiliado_count);\n","7205fa7f":"fig, ax = plt.subplots(figsize=(15,5))\nsns.set_theme(style=\"whitegrid\")\nsns.barplot(y = 'afiliado_nunique', x = 'Periodo', data = df_metricas, palette=\"Blues_d\", ax = ax).\\\n    set(title='Evoluci\u00f3n mensual - Todos los prestadores')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()\nfig, ax = plt.subplots(figsize=(15,5))\n\nsns.barplot(y = 'afiliado_prestaciones_afil_unic', x = 'Periodo', data = df_metricas, palette=\"Blues_d\", ax = ax).\\\n    set(title='Evoluci\u00f3n mensual - Todos los prestadores')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","8da5508b":"len(df_metricas.Periodo.unique())","b8810a3d":"aa = df_metricas.groupby(['Prestador_ID', 'Agrupador_Plan']).Periodo.count()\nsns.histplot(aa.values);","0b499fbd":"print(len(df_metricas.Agrupador_Plan.unique()))\naa = df_metricas.groupby(['Prestador_ID']).Periodo.count()\nsns.histplot(aa.values);","bc6c6feb":"df_metricas.groupby('Prestador_ID').Periodo.nunique().reset_index().head(20)","16092326":"13*30 ## cantidad de prestadores x cantidad de meses","8240ecc1":"df = df_metricas.copy()\ndf.drop(columns = ['Prestador_ID'], inplace = True)\ncol_numericas = [i for i in df.columns if df.loc[:,i].apply(np.isreal).all()]\ncol_numericas","8a7af3e1":"# ### Creamos la matriz de correlacion y ordenos de mayor a menor\nccc = df[col_numericas].corr()\n# ccc\n\nplt.figure(figsize=(20, 8))\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(ccc, dtype=np.bool))\nheatmap = sns.heatmap(ccc, mask=mask, vmin=-0.5, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);","e107a4cb":"ll = ['afiliado_nunique',\n 'afiliado_count',\n 'sexo_afiliado_prop_mujeres',\n 'monto_prestacion_q0.50',\n#  'monto_prestacion_q0.95',\n#  'edad_afil_median',\n 'edad_afil_q0.50',\n#  'edad_afil_q0.95',\n#  'permanencia_median',\n 'permanencia_q0.50',\n# 'Agrupador_Plan', \n#  'permanencia_q0.95'\n     ]\ndf = df_metricas[ll].sample(100)\nplt.figure(figsize=(20, 10))\npplot = sns.pairplot(df, kind=\"hist\", corner=True);\n# pplot = sns.pairplot(df);","bf26f0c3":"plt.figure(figsize=(10,5))\ng = sns.lineplot(\n    data=df_index,\n    x='Periodo',\n    y= 'Index_50',\n    palette='Set1'\n)\ng.set_xticklabels(labels=df_index.Periodo, rotation=90)\nplt.show()","6442deb2":"## Cual es la edad de cada plan\naa = df_metricas.groupby('Agrupador_Plan').agg({'edad_afil_q0.50' : 'mean'}).reset_index(drop = False)\naa['anios'] = round(aa['edad_afil_q0.50']\/12,1)\naa.sort_values(by = 'anios', ascending = False)\n# df_metricas['edad_afil_q0.50']","f7ff8a68":"## Cuales son los prestadores q atienden a gente mayor\n\naa = df_metricas.groupby('Prestador_ID').agg({'edad_afil_q0.50' : 'mean'}).reset_index(drop = False)\naa['anios'] = round(aa['edad_afil_q0.50']\/12,1)\naa.sort_values(by = 'anios', ascending = False)\n\n","4d88cbb7":"## de los montos factoramos nos quedamos solo con la MEDIANA que usaremos con target\n\ndf_metricas.drop(['monto_prestacion_median', 'monto_prestacion_q0.25', 'monto_prestacion_q0.75', 'monto_prestacion_q0.95'], axis = 1,inplace = True)\n","961c099d":"df_metricas.info()","7733d273":"df_metricas['costo_afil_uniq'] = df_metricas['monto_prestacion_q0.50'] \/ df_metricas['afiliado_nunique']\n# sns.histplot(df_metricas['costo_afil_unico']);","4e426c9d":"df_last = df_metricas.merge(df_index[['Periodo', 'Index_50']], on = 'Periodo', how = 'left')\ndf_last['costo_afil_uniq_A'] = round(df_last['costo_afil_uniq'] * df_last['Index_50'],2)\ndf_last.sample(3)","d5659eaa":"df_last['log_costo_afil_uniq'] = df_last['costo_afil_uniq'].apply(lambda x : x if x == 0 else np.log10(x))\ndf_last['log_costo_afil_uniq_A'] = df_last['costo_afil_uniq_A'].apply(lambda x : x if x == 0 else np.log10(x))\nsns.histplot(df_last['log_costo_afil_uniq'], bins = 30);\nsns.histplot(df_last['log_costo_afil_uniq_A'], bins = 30, color = 'red', kde = True);","356c69e3":"df_last_dummies = pd.get_dummies(df_last, columns = ['Prestador_ID', 'Agrupador_Plan'], prefix = ['prestador', 'plan']).drop(['monto_prestacion_q0.50', 'Periodo'], axis = 1)","9e881242":"# for periodo in df_metricas.Periodo.unique():\n#     mask_mes = df_metricas.Periodo == periodo\n#     df_metricas[mask_mes]\ny = df_last['monto_prestacion_q0.50']\nX = df_last_dummies\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","9ada9f75":"## REGRESION LINEAL como modelo base","162f7bad":"model_lr = LinearRegression()\nmodel_lr.fit(X_train, y_train)\ny_pred_train = model_lr.predict(X_train)\ny_pred_test = model_lr.predict(X_test)","0e235d01":"xx = np.linspace(0, max(y_train), 5)\nsns.lineplot(x = xx, y = xx, color = 'black')\n\n\nsns.scatterplot(x  = y_train, y = y_pred_train, color = 'blue');\nsns.scatterplot(x  = y_test, y = y_pred_test, color = 'red');\n\n","38a94b10":"print(df_procedimientos.shape)\ndf_procedimientos.groupby(['Prestador_ID', 'grupo_procedimiento']).count().head(15)","e6f3c2a7":"df_metricas.shape","4eceb8c8":"df_metr_proc = pd.merge(df_metricas,df_procedimientos, how = 'inner', on = ['Prestador_ID','Periodo','Agrupador_Plan'])\ndf_metr_proc.sample(4)\ndf_m_p_group = df_metr_proc.groupby(['Prestador_ID','Periodo','grupo_procedimiento']).agg({'cant_gp' : ['sum','mean']}).reset_index()\ndf_m_p_group.columns = ['Prestador_ID','Periodo','grupo_procedimiento','suma_proc','media_proc']\ndf_m_p_group","e13e45d8":"df_metr_proc = pd.merge(df_metricas,df_procedimientos, how = 'inner', on = ['Prestador_ID','Periodo','Agrupador_Plan'])\ndf_metr_proc.sample(4)\ndf_m_p_group = df_metr_proc.groupby(['Prestador_ID','Periodo','grupo_procedimiento']).mean()\n\n#df_m_p_group.columns = ['Prestador_ID','Periodo','grupo_procedimiento','suma_proc','media_proc']\ndf_m_p_group = df_m_p_group.reset_index()\n\ndf_m_p_group","76ebf6f2":"df_metr_proc.shape","a618406f":"df_metr_proc.notna().count()","7caa7e2e":"df_m_p = df_metr_proc.copy()\ndf_m_p.drop(columns = ['Prestador_ID','Periodo'], inplace = True)\ncol_numericas_m_p = [i for i in df_m_p.columns if df_m_p.loc[:,i].apply(np.isreal).all()]\ncol_numericas_m_p","b1fe8167":"# ### Creamos la matriz de correlacion y ordenos de mayor a menor\nccc_m_p = df_m_p[col_numericas_m_p].corr()\n# ccc\n\nplt.figure(figsize=(20, 8))\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(ccc_m_p, dtype=np.bool))\nheatmap = sns.heatmap(ccc_m_p, mask=mask, vmin=-0.5, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Matriz correlaci\u00f3n M\u00e9tricas y Procedimientos', fontdict={'fontsize':18}, pad=16);","f82638b4":"df_reg=df_last[df_last['log_costo_afil_uniq_A']>1]\n\ndf_reg_dummies = pd.get_dummies(df_reg, columns = ['Prestador_ID', 'Agrupador_Plan'], prefix = ['prestador', 'plan']).drop(['monto_prestacion_q0.50', 'Periodo'], axis = 1)","d67bdc53":"y = df_reg['log_costo_afil_uniq_A']\nX = df_reg_dummies.drop(columns=['costo_afil_uniq','costo_afil_uniq_A','log_costo_afil_uniq','log_costo_afil_uniq_A'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","8c783ee6":"print(\"X_train Shape: {}\".format(X_train.shape))\nprint(\"X_test Shape: {}\".format(X_test.shape))","528eb771":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","6079a9cf":"regresores= {}","f95673e6":"regresores.update({'lasso' : make_pipeline(StandardScaler(), Lasso(alpha =0.5, random_state=1))})","00a6b326":"regresores.update({'e_net' : make_pipeline(StandardScaler(), ElasticNet(alpha=0.5, l1_ratio=.9, random_state=3))})","a7a4f8f9":"regresores.update({'ridge' : make_pipeline(StandardScaler(), linear_model.Ridge(alpha=0.5))})","b15752af":"regresores.update({'svr' : make_pipeline(StandardScaler(), SVR(kernel='poly'))})","5a48c7c1":"regresores.update({'xgb_r' : xgb.XGBRegressor(objective ='reg:squarederror',n_estimators = 10, seed = 123)})","a7e766fd":"regresores.update({'gbr' : GradientBoostingRegressor()})","b5b6e0c7":"regresores.update({'r_f' : RandomForestRegressor()})","086a5277":"perf=pd.DataFrame()","621c5fdc":"a=0\nfor i in regresores.keys():\n    score = rmsle_cv(regresores[i])\n    perf.loc[a,'regresor'] = i\n    print(str(i)+\"\\n score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n    perf.loc[a,'media']= score.mean()\n    perf.loc[a,'desvio']= score.std()\n    a+=1","db75b559":"perf.media.min()","371fe32f":"mejor_reg=perf[perf.desvio==perf.desvio.min()].reset_index().loc[0,'regresor']\nprint(regresores[mejor_reg])\nregresores[mejor_reg].fit(X_test,y_test)\n\nxx = np.linspace(0, max(y_train), 5)\nsns.lineplot(x = xx, y = xx, color = 'black')\nsns.scatterplot(x  = y_test, y = regresores[mejor_reg].predict(X_test), color = 'blue');","aedcdb00":"desvio_rel=((-regresores['gbr'].predict(X_test)+y_test)\/y_test)*100","fb9d7c54":"sns.histplot(desvio_rel,  color = 'purple');","78a122de":"sns.histplot(desvio_rel,  color = 'purple', cumulative = True, stat = 'density');","69b1a837":"# Funci\u00f3n que plotea la serie:\ndef plot_ts(df, x, y, title=\"\", xlabel='Fecha', ylabel='Valor', dpi=100):\n    plt.figure(figsize=(16,5), dpi=dpi)\n    plt.plot(x, y, color='tab:red')\n    plt.xticks(rotation=45 )\n    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n    plt.show()\n\n# df_last.head(3)","8d28e19f":"## Listado de 10 prestadores m\u00e1s 'presentes' (meses x planes)\ndf = df_last.copy()\ndf['time'] = pd.to_datetime(df['Periodo'])\n# df.set_index(keys = 'time', inplace = True)\ndf.Prestador_ID.value_counts().head(10)","82f92d55":"mask_prestador = df.Prestador_ID == 158\n# mask_prestador = df.Prestador_ID == df.Prestador_ID  ## TODOS LOS PRESTADORES\ndf=df[mask_prestador]\n# mask_prestador.sum()\n\nts = df[['time','log_costo_afil_uniq_A']].groupby('time').mean().round(2)\nts.columns = ['valor']\nts = ts.iloc[:-1,:]\nplot_ts(ts, x=ts.index, y=ts.valor,\\\n        title='media del log(costo por afiliado) ')","ac31a84c":"ts['year'] = ts.index.year\n# ts['month'] = ts.index.month\nts['month'] = [d.strftime('%b') for d in ts.index]\nts[\"timeIndex\"] = pd.Series(np.arange(len(ts['valor'])), index=ts.index)\nts[\"timeIndex_sq\"] = ts[\"timeIndex\"]**2\n# ts.head(3)\n# Creamos variables dummy para los meses:\ndummies_mes = pd.get_dummies(ts['month'], drop_first=True)\n\n# Hacemos el join entre el DataFrame con la serie de tiempo y las dummies:\nts = ts.iloc[:, :5]\nts = ts.join(dummies_mes)\nts.tail(4)","8ecece57":"ts_train, ts_test = train_test_split(ts, test_size=3, random_state=1, shuffle=False)\n# ts_train.tail()","26db6842":"model_linear = smf.ols('valor ~ timeIndex', data = ts_train).fit()\n# model_linear.summary()\n\n## para train\nts_train[\"LinearTrend\"] = model_linear.predict(ts_train.timeIndex)\nts_train.plot(kind = \"line\", y = [\"valor\",\"LinearTrend\"]);\n# para test\nts_test[\"LinearTrend\"] = model_linear.predict(ts_test.timeIndex)\nts_test.plot(kind = \"line\", y = [\"valor\",\"LinearTrend\"]);","6b2956c1":"def RMSE(predicted, actual):\n    mse = (predicted - actual) ** 2\n    rmse = np.sqrt(mse.sum() \/ mse.count())\n    return rmse\n\n# Guardamos el resultado en un DataFrame:\ndf_Results = pd.DataFrame(columns = [\"Model\", \"RMSE\"])\ndf_Results.loc[0, \"Model\"] = \"LinearTrend\"\ndf_Results.loc[0, \"RMSE\"] = RMSE(ts_test.LinearTrend, ts_test.valor)\ndf_Results","edfe8aa2":"model_quadratic = smf.ols('valor ~ timeIndex + timeIndex_sq', data = ts_train).fit()\n# model_quadratic.summary()\n\n## R2 es similar al modelo lineal\n## la componente cuadratica NO RESULTA SIGNIFICATIVA (ver p-valor)\n## para train\nts_train[\"QuadraticTrend\"] = model_quadratic.predict(ts_train[['timeIndex','timeIndex_sq']])\nts_train.plot(kind = \"line\", y = [\"valor\",\"QuadraticTrend\"]);\n## para test\nts_test[\"QuadraticTrend\"] = model_quadratic.predict(ts_test[['timeIndex','timeIndex_sq']])\nts_test.plot(kind = \"line\", y = [\"valor\",\"QuadraticTrend\"]);","76320309":"# Guardamos el resultado en un DataFrame:\ndf_Results.loc[1, \"Model\"] = \"QuadraticTrend\"\ndf_Results.loc[1, \"RMSE\"] = RMSE(ts_test.QuadraticTrend, ts_test.valor)\ndf_Results","d19801aa":"model_lin_est = smf.ols('valor ~ timeIndex + Aug + Dec + Feb + Jan + Jul + Jun + Mar + May + Nov + Oct + Sep',\\\n                          data = ts_train).fit()\nmodel_lin_est.summary()","9965e7c6":"ts_train['model_lin_est'] = model_lin_est.predict(ts_train[[\"timeIndex\", \\\n                                              \"Aug\", \"Dec\", \"Feb\", \"Jan\",\\\n                                               \"Jul\", \"Jun\", \"Mar\", \"May\",\\\n                                                \"Nov\", \"Oct\", \"Sep\"]])\n\n\nts_test['model_lin_est'] = model_lin_est.predict(ts_test[[\"timeIndex\", \\\n                                              \"Aug\", \"Dec\", \"Feb\", \"Jan\",\\\n                                               \"Jul\", \"Jun\", \"Mar\", \"May\",\\\n                                                \"Nov\", \"Oct\", \"Sep\"]])\nts_train.plot(kind = \"line\", y = ['valor', 'model_lin_est']);\nts_test.plot(kind = \"line\", y = ['valor', 'model_lin_est']);","b331e451":"# Guardamos el resultado en un DataFrame:\ndf_Results.loc[2, \"Model\"] = \"model_lin_est\"\ndf_Results.loc[2, \"RMSE\"] = RMSE(ts_test.model_lin_est, ts_test.valor)\ndf_Results","6290896c":"model_exp_smoothing = SimpleExpSmoothing(ts_train.valor).fit(smoothing_level=1., optimized=False)\nts_train.plot(kind = \"line\", y = \"valor\")\nmodel_exp_smoothing.fittedvalues.plot();","1d334f53":"# tscv = TimeSeriesSplit(n_splits=5) ## instaciamos un objeto TimeSeriesSplit\n\ndef timeseriesCVscore_exp_smoot(alpha, series):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # creamos un array de errores:\n    errors = []\n    \n    values = series.valor\n    \n    # instanciamos el objeto que realiza el tscv:\n    tscv = TimeSeriesSplit(n_splits=5) \n    \n    # Aplicamos cross validation:\n\n    for train, test in tscv.split(values):\n    \n        model = SimpleExpSmoothing(values[train]).fit(smoothing_level=alpha,\\\n                                                             optimized=False)\n        \n        predictions = model.forecast(len(test))\n        actual = values[test]\n    \n        error = mean_squared_error(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","7db02be6":"alphas = [0.3, 0.4, 0.5, 0.6, 0.9,  1.03, 1.1, 1.125, 1.15, 1,7, 1.175, 1.8, 1.2]\nerrors = []\n\nfor alpha in alphas:\n    error = timeseriesCVscore_exp_smoot(alpha, ts_train)\n    errors.append(error)\n\nprint('Alpha \u00f3ptimo:', alphas[np.argmin(errors)]) \nprint('\\nEste valor depende de el o los prestador seleccionados para hacer el forecast')","d683cb5e":"# Entrenamos el modelo optimizado:\n# alphas[np.argmin(errors)]\nmodel_exp_smoothing = SimpleExpSmoothing(ts_train.valor, initialization_method = 'heuristic').fit(smoothing_level= 0, optimized=False)\nts_test[\"Simple_Smoothing\"] = model_exp_smoothing.forecast(4)\n# ts_test.tail(3)\nres_model = ts_train['valor'] - ts_train['QuadraticTrend']\nplt.plot(ts_train.timeIndex, res_model, '-');","f7a3479a":"# Testeamos la estacionariedad de los residuos:\nresult = adfuller(res_model)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nfor key, value in  result[4].items():\n    print('Valor cr\u00edtico %s: %.2f' % (key,value))\n## En general para ninguno de los modelos puedo rechazar la hipotesis nulo \n## (no podemos garantizar que la serie de los residuos sea estacionaria)","9b1c4054":"## calculamos la autocorrelacion del residuo (lag correlation)\nlag_acf = acf(res_model, nlags = 5)\nACF = pd.Series(lag_acf)\nACF.plot(kind = \"bar\");","cc5fe0ee":"lag_pacf = pacf(res_model, nlags=6);\nPACF = pd.Series(lag_pacf)\nPACF.plot(kind = \"bar\");","a21aa368":"# Corremos la funci\u00f3n con nuestra serie res_log:\ndef tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n    \"\"\" \n        Plotea la serie de tiempo, el ACF y PACF y el test de Dickey\u2013Fuller\n        \n        y - serie de tiempo\n        lags - cu\u00e1ntos lags incluir para el c\u00e1lculo de la ACF y PACF\n        \n    \"\"\"\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        \n        # definimos ejes\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        \n        # obtengo el p-value con h0: raiz unitaria presente\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        \n        ts_ax.set_title('An\u00e1lisis de la Serie de Tiempo\\n Dickey-Fuller: p={0:.5f}'\\\n                        .format(p_value))\n        \n        # plot de autocorrelacion\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        # plot de autocorrelacion parcial\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()\ntsplot(res_model, lags=7)","bfd5021a":"# Instanciamos el modelo ARIMA ( en este caso d = 1 ya que no pudimos validar que el residuo sea estacionario)\nmodel_ARIMA = ARIMA(res_model, order=(3,1,2)) ##The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use.\n\n# Estimo el modelo:\nresults_ARIMA = model_ARIMA.fit()\n# print(results_ARIMA.summary())\n# results_ARIMA.fittedvalues.head()\nres_model.plot()\nresults_ARIMA.fittedvalues.plot();\nresults_ARIMA.plot_predict(end=len(ts['valor']));","51e14b9c":"## analizo los residuos del modelo ARIMA\nres_ARIMA =  results_ARIMA.fittedvalues - res_model\nres_ARIMA.plot();","a73f466f":"predictions_ARIMA, se, conf = results_ARIMA.forecast(len(ts_test['valor']), alpha=0.05)\nts_train.head()","74c2b462":"ts_train['lin_model_ARIMA'] = ts_train['LinearTrend'] + results_ARIMA.fittedvalues\nts_test['lin_model_ARIMA'] = ts_test['LinearTrend'] + predictions_ARIMA\nts_train.plot(kind = \"line\", y = ['valor', 'lin_model_ARIMA']);\nts_test.plot(kind = \"line\", y = ['valor', 'lin_model_ARIMA']);","5818bdfe":"# Guardamos el resultado en un DataFrame:\ndf_Results.loc[3, \"Model\"] = \"con ARIMA\"\ndf_Results.loc[3, \"RMSE\"] = RMSE(ts_test.lin_model_ARIMA, ts_test.valor)\ndf_Results","f7cfbb7b":"El gr\u00e1fico anterior permite visualizar claramente el impacto de la pandemia y la cuarentena implementada a mediados de marzo 2020","d363c4cc":"# Feature Engeneering","431b6fad":"## Inflaci\u00f3n efectiva de la OS","0c46d42c":"### smoothing con cross validation","bc3784bd":"- **Ridge Regression** :","2879af66":"- **SVR Regression** :","4ae3119b":"No todos los prestadores atienden a todos los planes. \nAbajo una lista con la cantidad de prestadores por cada grupo de plan:","344beab9":"# Librerias requeridas","0ac9000f":"- **Elastic Net Regression** :","06fbe2ba":"- Elimino el ruido de la muestra donde solo me interesan los valores mayores a 1\n- Esto lo hacemos quitando los registros con log de costo por afiliado < 1, es decir menos de $10 por afiliado (son prestadores de bajo costo, osea no nos interesan)\n","3fcbefb6":"- **Gradient Boosting Regression** :","f7d321fd":"-  **LASSO  Regression**  : ","780615cf":"## Modelos basados en el uso adicional del detalle de los Procedimientos ","2752228a":"### lineal","f515adea":"# Leactura de los set de datos","f4b84176":"### cuadratico","f4743659":"### Nuevas variables:\nContruimos nuevas variables relacionadas con el costo: \n+ Costo por afiliado\n+ Costo por afiliado actualizado por indice de imflaci\u00f3n\n+ Log (base 10) del costo","3cfc9a52":"Definimos que el mejor regresor ser\u00e1 aquel que obtenga el score y desv\u00edo menor","cc41c200":"Defininos una funci\u00f3n de validacion donde divido la muestra en 5 con shuffle y a la media del error cuadrado como variable scrore","a5efe2cf":"Guardamos todas las metricas en un data frame para comparar las performances","604c12cc":"# MODELOS\n\nEn los modelos que se presentan se busca predecir la cantidad facturada por un prestador a partir del uso de la informaci\u00f3n disponible del resto de los prestadores. \n\nEl objetivo de estos modelos es, en todos los casos, detectar anomal\u00edas en la liquidaci\u00f3n de un prestador (desviaciones respecto a la predcicci\u00f3n superiores a un umbral).\n\nSe trata de un an\u00e1isis retrospectivo","4eb6494b":"## Oferta de servicios de los prestadores seg\u00fan PLAN\nNO TODOS LOS PRESTADORES OFRECEN SERVICIOS LOS 30 MESES NI A TODOS LOS PLANES","6e80d694":"- **Xgboost Regression** :","804bd934":"Mediante un histogramos buscaremos donde se forma el \"codo\" que se aleja de la media de los valores de la predicci\u00f3n","8333b3c3":"- **Random Forest Regression** :","542b1757":"# FORECAST DEL CONSUMO POR AFILIADO: Series de tiempo","16116e82":"## Influencia de la EDAD en el CONSUMO","cf7f6773":"### modelo lineal con estacionalidad","f8b6c2ba":"## Comparaci\u00f3n de multiples regresores","ca9dfff5":"Se hace un join entre el data set de metricas y el de procedimientos","10667df0":"Definimos los modelos que utilizaremos","583363d5":"Como variable target elegimos el log del costo por afiliado y de las variables dependientes eliminamos el resto de las m\u00e9tricas relacionas explicitamente con el consumo","86042e85":"# EDA","79617a26":"## Valor por prestador y\/o global y\/u otro\n\nEn la siguiente celda definimos el dataframe 'ts' seg\u00fan la variable que querramos predecir"}}