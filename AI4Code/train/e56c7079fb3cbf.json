{"cell_type":{"e74b3e75":"code","5157598a":"code","6656f9b8":"code","e379c62f":"code","a20e5a5e":"code","84afd3a3":"code","4be19d28":"code","510fdbd8":"code","bf9c70c1":"code","11a97f2e":"code","0c66720c":"code","06cc2c6a":"code","4ab7f018":"code","6a1c5419":"code","6c059e70":"code","61ec2b08":"code","27aa2689":"code","099794fb":"code","fd1a1da2":"code","201d22ce":"code","535ee583":"code","8503272e":"code","3aaff243":"code","f1656ad1":"code","76926025":"code","140a025d":"code","e723e256":"code","c80ed7c7":"code","bd9272b6":"code","2fad4c32":"markdown","bc95eadf":"markdown","4faf580e":"markdown","9c1e7a93":"markdown","d649c30f":"markdown","bf3e54a1":"markdown","6ce56f70":"markdown","f7bf5436":"markdown","56355d43":"markdown","ae884e7c":"markdown","d15a5f83":"markdown","ec336aaf":"markdown","de63c57a":"markdown","60018bbc":"markdown","737c7db5":"markdown","0d409384":"markdown","dd307ba6":"markdown","c8bd6f98":"markdown","d8a49f8d":"markdown","65355877":"markdown","324addeb":"markdown","df367c12":"markdown","285113d5":"markdown"},"source":{"e74b3e75":"# Import everything we need\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt, image as mpimg\nfrom tqdm import tqdm\nfrom time import time\nfrom collections import Counter\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, optimizers, losses, metrics, utils, callbacks\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications import ResNet152\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG\nfrom skimage.transform import resize\n\nIMG_SIZE = 512\nBATCH_SIZE = 32\n\n\n\n# Set up all the paths and load the data\n\ntrain_dir = '..\/input\/ocular-disease-recognition-odir5k\/ODIR-5K\/Training Images'\ntest_dir = '..\/input\/ocular-disease-recognition-odir5k\/ODIR-5K\/Testing Images'\nmain_dir = '..\/input\/ocular-disease-recognition-odir5k\/ODIR-5K'\nhistory_dir = '\/kaggle\/working\/history'\nmodels_dir = '\/kaggle\/working\/models'\ntry:\n    os.mkdir(history_dir)\n    os.mkdir(models_dir)\nexcept Exception as e:\n    pass\n\n\ndata = pd.read_excel(os.path.join(main_dir,'data.xlsx'), sheet_name=None)\ndata = pd.DataFrame(data[list(data.keys())[0]])\n\nconditions = list(data.columns[-8:])\n\n\n# copy-pasted from previous notebook's output:\n\ndiagnostic_keyphrases = {'N': ['normal fundus'], \n 'D': ['nonproliferative retinopathy',\n  'non proliferative retinopathy',\n  'proliferative retinopathy'],\n 'G': ['glaucoma'],\n 'C': ['cataract'],\n 'A': ['age-related macular degeneration'],\n 'H': ['hypertensive'],\n 'M': ['myopi'],\n 'O': ['macular epiretinal membrane',\n  'epiretinal membrane',\n  'drusen',\n  'lens dust',\n  'myelinated nerve fibers',\n  'laser spot',\n  'vitreous degeneration',\n  'refractive media opacity',\n  'spotted membranous change',\n  'tessellated fundus',\n  'maculopathy',\n  'chorioretinal atrophy',\n  'branch retinal vein occlusion',\n  'retinal pigmentation',\n  'white vessel',\n  'post retinal laser surgery',\n  'epiretinal membrane over the macula',\n  'retinitis pigmentosa',\n  'central retinal vein occlusion',\n  'optic disc edema',\n  'post laser photocoagulation',\n  'retinochoroidal coloboma',\n  'atrophic change',\n  'optic nerve atrophy',\n  'old branch retinal vein occlusion',\n  'depigmentation of the retinal pigment epithelium',\n  'chorioretinal atrophy with pigmentation proliferation',\n  'central retinal artery occlusion',\n  'old chorioretinopathy',\n  'pigment epithelium proliferation',\n  'retina fold',\n  'abnormal pigment ',\n  'idiopathic choroidal neovascularization',\n  'branch retinal artery occlusion',\n  'vessel tortuosity',\n  'pigmentation disorder',\n  'rhegmatogenous retinal detachment',\n  'macular hole',\n  'morning glory syndrome',\n  'atrophy',\n  'low image quality',\n  'arteriosclerosis',\n  'asteroid hyalosis',\n  'congenital choroidal coloboma',\n  'macular coloboma',\n  'optic discitis',\n  'oval yellow-white atrophy',\n  'wedge-shaped change',\n  'wedge white line change',\n  'retinal artery macroaneurysm',\n  'retinal vascular sheathing',\n  'suspected abnormal color of  optic disc',\n  'suspected retinal vascular sheathing',\n  'suspected retinitis pigmentosa',\n  'silicone oil eye']}\n","5157598a":"con2img = {condition:[] for condition in conditions}\nfor i, row in data.iterrows():\n    image_L = row['Left-Fundus']\n    image_R = row['Right-Fundus']\n    if row['N']==1:\n        con2img['N'] += [image_L, image_R]\n        continue\n        \n    keyphrases_L = row['Left-Diagnostic Keywords']\n    keyphrases_R = row['Right-Diagnostic Keywords']\n    diagnosed_conditions = []\n    for condition in conditions[1:]:\n        if row[condition]==1:\n            diagnosed_conditions.append(condition)\n            \n    if 'normal fundus' in keyphrases_L:\n        con2img['N'].append(image_L)\n        for condition in diagnosed_conditions:\n            con2img[condition].append(image_R)\n        continue\n    if 'normal fundus' in keyphrases_R:\n        con2img['N'].append(image_R)\n        for condition in diagnosed_conditions:\n            con2img[condition].append(image_L)\n        continue\n    \n    for condition in diagnosed_conditions:\n        if any(keyphrase in keyphrases_L for keyphrase in diagnostic_keyphrases[condition]):\n            con2img[condition].append(image_L)\n        if any(keyphrase in keyphrases_R for keyphrase in diagnostic_keyphrases[condition]):\n            con2img[condition].append(image_R)\n\n            \nimg2con = {}\nfor condition in conditions:\n    for im in con2img[condition]:\n        if im not in img2con:\n            img2con[im] = [condition]\n        else:\n            img2con[im] = sorted(img2con[im]+[condition])\n\n# Sanity check: are the numbers the same as in the previous notebook?     \nfor condition in conditions:\n    print(condition, len(con2img[condition]))\nprint(len(img2con))","6656f9b8":"imgdata_columns = ['Image', 'Patient Age', 'Patient Sex', *conditions]\nimgdata = []\n\nfor i, row in data.iterrows():\n    image_L = row['Left-Fundus']\n    image_R = row['Right-Fundus']\n    if image_L in img2con:\n        image_conditions = [int(condition in img2con[image_L]) for condition in conditions]\n        imgdata.append([image_L, row['Patient Age'], row['Patient Sex'], *image_conditions])\n    if image_R in img2con:\n        image_conditions = [int(condition in img2con[image_R]) for condition in conditions]\n        imgdata.append([image_R, row['Patient Age'], row['Patient Sex'], *image_conditions])\n\nimgdata = pd.DataFrame(imgdata, columns=imgdata_columns)\n#imgdata['Patient Sex'] = imgdata['Patient Sex'].apply(lambda x:0  if x=='Female' else 1) # encode sex: 'Female'=>0, 'Male'=>1\n\nimgdata_labels = []\nfor i, row in imgdata.iterrows():\n    row_labels = []\n    for condition in conditions:\n        if row[condition]==1:\n            row_labels.append(condition)\n    imgdata_labels.append(row_labels)\nimgdata['Labels'] = imgdata_labels\nimgdata","e379c62f":"con2ind = {condition: [] for condition in conditions}\nfor i, row in imgdata.iterrows():\n    for condition in conditions:\n        if row[condition]==1:\n            con2ind[condition].append(i)\nfor condition in conditions:\n    con2ind[condition] = np.array(con2ind[condition])\n            \nprint('con2img sizes:')\nfor condition in conditions:\n    print(condition, len(con2img[condition]))\n\nprint('\\ncon2ind sizes:')\nfor condition in conditions:\n    print(condition, len(con2img[condition]))","a20e5a5e":"def image_prep(image, target_shape=(IMG_SIZE,IMG_SIZE,3)):\n    non_0_rows = np.array([row_idx for row_idx, row in enumerate(image) if np.count_nonzero(row)!=0])\n    non_0_cols = np.array([col_idx for col_idx, col in enumerate(image.transpose(1,0,2)) if np.count_nonzero(col)!=0])\n    image = image[non_0_rows.min():non_0_rows.max()+1, non_0_cols.min():non_0_cols.max()+1, :] # clip\n    image = resize(image, target_shape) # resize\n    return image\n\n# TEST:\nimage = mpimg.imread(os.path.join(train_dir, os.listdir(train_dir)[0]))\nprint('original:\\t',image.shape)\nplt.imshow(image)\nplt.show()\n\nimage_prepped = image_prep(image)\nprint('preprocessed:\\t', image_prepped.shape)\nplt.imshow(image_prepped)\nplt.show()","84afd3a3":"N_indices = con2ind['N'].copy() # get indices of 'N' images in the imgdata DataFrame\nnp.random.shuffle(N_indices) # randomly shuffle these indices\nN_indices_train, N_indices_val, N_indices_test = N_indices[:2480], N_indices[2480:2790], N_indices[2790:] # split these indices into training, validation, and test set\nN_indices_train.shape, N_indices_val.shape, N_indices_test.shape # sanity check","4be19d28":"# idg template for training and validation data\ntrain_idg = IDG(\n    horizontal_flip=True, vertical_flip=True, rotation_range=180, # a modest data augmentation\n    rescale=1.\/255.,\n    preprocessing_function=image_prep # preprocessing function defined earlier\n)\n# idg template for testing data \ntest_idg = IDG(\n    rescale=1.\/255.,\n    preprocessing_function=image_prep\n)\n\n# training data generator for age prediction\nage_train_generator = train_idg.flow_from_dataframe(\n    dataframe = imgdata.iloc[N_indices_train, :],\n    directory = train_dir,\n    x_col='Image',\n    y_col='Patient Age',\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode='raw',\n    target_size=(IMG_SIZE, IMG_SIZE)\n)\n# validation data generator for age prediction\nage_val_generator = train_idg.flow_from_dataframe(\n    dataframe = imgdata.iloc[N_indices_val, :],\n    directory = train_dir,\n    x_col='Image',\n    y_col='Patient Age',\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode='raw',\n    target_size=(IMG_SIZE, IMG_SIZE)\n)\n# testing data generator for age prediction\nage_test_generator = test_idg.flow_from_dataframe(\n    dataframe = imgdata.iloc[N_indices_test, :],\n    directory = train_dir,\n    x_col='Image',\n    y_col='Patient Age',\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode='raw',\n    target_size=(IMG_SIZE, IMG_SIZE)\n)","510fdbd8":"resnet152 = ResNet152(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\nresnet152.trainable = False # We freeze the weights of the convolutional base\n\nage_model = models.Sequential(name='age_model')\nage_model.add(resnet152)\nage_model.add(layers.Flatten(name='flatten'))\nage_model.add(layers.Dense(1, activation=None, name='dense_output'))\n\nage_model.compile(\n    optimizer=optimizers.Adam(lr=1e-5),\n    loss='huber',\n    metrics=['mae']\n)\n\nage_model.summary()\n","bf9c70c1":"age_history = age_model.fit(\n    age_train_generator, \n    validation_data=age_val_generator, \n    epochs=8, steps_per_epoch=len(N_indices_train)\/\/BATCH_SIZE, validation_steps=len(N_indices_val)\/\/BATCH_SIZE,\n    verbose=1,\n    shuffle=True\n)\npd.DataFrame(age_history.history).to_csv('history\/age_history_0.csv') # Save the training history\nage_model.save('models\/age_model_0.h5') # Save the model with its current weights","11a97f2e":"train_loss = age_history.history['loss']\nval_loss = age_history.history['val_loss']\ntrain_mae = age_history.history['mae']\nval_mae = age_history.history['val_mae']\n\nplt.plot(range(1,9), train_loss, 'r-', label='training loss')\nplt.plot(range(1,9), val_loss, 'b--', label='validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Dense classifier training: Loss')\nplt.legend()\nplt.show()\n\nplt.plot(range(1,9), train_mae, 'r-', label='training MAE')\nplt.plot(range(1,9), train_mae, 'b--', label='validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Absolute Error')\nplt.title('Dense classifier training: MAE')\nplt.legend()\nplt.show()","0c66720c":"for layer in age_model.layers[0].layers:\n    if 'conv5_block3' in layer.name:\n        layer.trainable = True\n        \nage_model.compile(\n    optimizer=optimizers.Adam(1e-7),\n    loss='huber',\n    metrics=['mae']\n)","06cc2c6a":"age_history = age_model.fit(\n        age_train_generator, \n        validation_data=age_val_generator, \n        epochs=16, steps_per_epoch=len(N_indices_train)\/\/BATCH_SIZE, validation_steps=len(N_indices_val)\/\/BATCH_SIZE,\n        verbose=1,\n        shuffle=True\n)\npd.DataFrame(age_history.history).to_csv('history\/age_history_1.csv')\nage_model.save('models\/age_model_1.h5')","4ab7f018":"train_loss = age_history.history['loss']\nval_loss =  age_history.history['val_loss']\ntrain_mae = age_history.history['mae']\nval_mae = age_history.history['val_mae']\n\n\nplt.plot(range(1,17), train_loss, 'r-', label='training loss')\nplt.plot(range(1,17), val_loss, 'b--', label='validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Fine-tuning: Loss')\nplt.legend()\nplt.show()\n\nplt.plot(range(1,17), train_mae, 'r-', label='training MAE')\nplt.plot(range(1,17), train_mae, 'b--', label='validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Absolute Error')\nplt.title('Fine-tuning: MAE')\nplt.legend()\nplt.show()","6a1c5419":"age_model_before_ft = models.load_model('models\/age_model_0.h5')\nage_model_after_ft = models.load_model('models\/age_model_1.h5')\n\nscore_before_ft = age_model_before_ft.evaluate(age_test_generator)\nscore_after_ft = age_model_after_ft.evaluate(age_test_generator)","6c059e70":"print('Evaluation:')\nprint('\\tBefore fine-tuning:\\tLoss: %.3f\\tMAE: %.3f' % (score_before_ft[0], score_before_ft[1]))\nprint('\\tAfter fine-tuning: \\tLoss: %.3f\\tMAE: %.3f' % (score_after_ft[0], score_after_ft[1]))\n","61ec2b08":"age_model = models.load_model('models\/age_model_0.h5')\n\nage_test_generators = {'N': age_test_generator}\nfor condition in conditions[1:]:\n    age_test_generators[condition] = test_idg.flow_from_dataframe(\n        dataframe = imgdata.iloc[con2ind[condition], :],\n        directory = train_dir,\n        x_col='Image',\n        y_col='Patient Age',\n        batch_size=32,\n        seed=42,\n        shuffle=True,\n        class_mode='raw',\n        target_size=(IMG_SIZE, IMG_SIZE)\n    )\n\nage_model_scores = {}\nfor condition in conditions:\n    age_model_scores[condition] = age_model.evaluate(age_test_generators[condition], verbose=1)","27aa2689":"for condition in conditions:\n    print('%s:\\tLoss:\\t%.3f\\t\\tMAE:\\t%.3f' % (condition, age_model_scores[condition][0], age_model_scores[condition][1]) )","099794fb":"age_groups_indices = [\n    np.asarray(imgdata.query('`Patient Age` < 30').index),\n    np.asarray(imgdata.query('30 <= `Patient Age` < 60').index),\n    np.asarray(imgdata.query('60 <= `Patient Age`').index)\n]\n\nprint('%i images from people below 30'%len(age_groups_indices[0]))\nprint('%i images from people between 30 and 60'%len(age_groups_indices[1]))\nprint('%i images from people older than 60'%len(age_groups_indices[2]))","fd1a1da2":"age_groups_test_generators = []\nfor age_group_indices in age_groups_indices:\n    age_groups_test_generators.append(\n        test_idg.flow_from_dataframe(\n            dataframe = imgdata.iloc[age_group_indices, :],\n            directory = train_dir,\n            x_col='Image',\n            y_col='Patient Age',\n            batch_size=32,\n            seed=42,\n            shuffle=True,\n            class_mode='raw',\n            target_size=(IMG_SIZE, IMG_SIZE)\n        )\n    )","201d22ce":"age_groups_test_scores = []\nfor age_group_test_generator in age_groups_test_generators:\n    age_groups_test_scores.append(age_model.evaluate(age_group_test_generator, verbose=1))","535ee583":"print('1-29:\\tLOSS:\\t%.3f\\tMAE:\\t%.3f'%(age_groups_test_scores[0][0],age_groups_test_scores[0][1]))\nprint('30-59:\\tLOSS:\\t%.3f\\tMAE:\\t%.3f'%(age_groups_test_scores[1][0],age_groups_test_scores[1][1]))\nprint('60+:\\tLOSS:\\t%.3f\\tMAE:\\t%.3f'%(age_groups_test_scores[2][0],age_groups_test_scores[2][1]))","8503272e":"N_age_groups_indices = [\n    np.asarray(imgdata.query('`Patient Age` < 30 & N==1').index),\n    np.asarray(imgdata.query('30 <= `Patient Age` < 60 & N==1 ').index),\n    np.asarray(imgdata.query('60 <= `Patient Age` & N==1').index)\n]\n\nprint('%i images from healthy people below 30'%len(N_age_groups_indices[0]))\nprint('%i images from healthy people between 30 and 60'%len(N_age_groups_indices[1]))\nprint('%i images from healthy people older than 60'%len(N_age_groups_indices[2]))\n\nN_age_groups_test_generators = []\nfor N_age_group_indices in N_age_groups_indices:\n    N_age_groups_test_generators.append(\n        test_idg.flow_from_dataframe(\n            dataframe = imgdata.iloc[N_age_group_indices, :],\n            directory = train_dir,\n            x_col='Image',\n            y_col='Patient Age',\n            batch_size=32,\n            seed=42,\n            shuffle=True,\n            class_mode='raw',\n            target_size=(IMG_SIZE, IMG_SIZE)\n        )\n    )\n    \nN_age_groups_test_scores = []\nfor N_age_group_test_generator in N_age_groups_test_generators:\n    N_age_groups_test_scores.append(age_model.evaluate(N_age_group_test_generator, verbose=1))","3aaff243":"print('\\tHealthy patients only:\\n')\nprint('1-29:\\tLOSS:\\t%.3f\\tMAE:\\t%.3f'%(N_age_groups_test_scores[0][0],N_age_groups_test_scores[0][1]))\nprint('30-59:\\tLOSS:\\t%.3f\\tMAE:\\t%.3f'%(N_age_groups_test_scores[1][0],N_age_groups_test_scores[1][1]))\nprint('60+:\\tLOSS:\\t%.3f\\tMAE:\\t%.3f'%(N_age_groups_test_scores[2][0],N_age_groups_test_scores[2][1]))","f1656ad1":"female_N_indices = []\nmale_N_indices = []\nfor i, row in imgdata.query('N == 1').iterrows():\n    if row['Patient Sex']=='Female':\n        female_N_indices.append(i)\n    elif row['Patient Sex']=='Male':\n        male_N_indices.append(i)\n\nN_per_sex = len(female_N_indices)\nnp.random.shuffle(male_N_indices)\nfemale_N_indices = np.array(female_N_indices)\nmale_N_indices = np.array(male_N_indices[:N_per_sex]) # cutting out a randomly sampled male overrepresantation\n\nfemale_N_indices_train, female_N_indices_val, female_N_indices_test = female_N_indices[:int(.8*N_per_sex)], female_N_indices[int(.8*N_per_sex):int(.9*N_per_sex)], female_N_indices[int(.9*N_per_sex):]\nmale_N_indices_train, male_N_indices_val, male_N_indices_test = male_N_indices[:int(.8*N_per_sex)], male_N_indices[int(.8*N_per_sex):int(.9*N_per_sex)], male_N_indices[int(.9*N_per_sex):]\n\nsex_N_indices_train = np.concatenate([female_N_indices_train, male_N_indices_train])\nsex_N_indices_val = np.concatenate([female_N_indices_val, male_N_indices_val])\nsex_N_indices_test = np.concatenate([female_N_indices_test, male_N_indices_test])","76926025":"print(np.bincount(imgdata['Patient Sex'].apply(lambda x:0 if x=='Female' else 1)[sex_N_indices_train]))\nprint(np.bincount(imgdata['Patient Sex'].apply(lambda x:0 if x=='Female' else 1)[sex_N_indices_val]))\nprint(np.bincount(imgdata['Patient Sex'].apply(lambda x:0 if x=='Female' else 1)[sex_N_indices_test]))","140a025d":"sex_train_generator = train_idg.flow_from_dataframe(\n    dataframe = imgdata.iloc[sex_N_indices_train, :],\n    directory = train_dir,\n    x_col='Image',\n    y_col='Patient Sex',\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode='categorical',\n    target_size=(IMG_SIZE, IMG_SIZE)\n)\nsex_val_generator = train_idg.flow_from_dataframe(\n    dataframe = imgdata.iloc[sex_N_indices_val, :],\n    directory = train_dir,\n    x_col='Image',\n    y_col='Patient Sex',\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode='categorical',\n    target_size=(IMG_SIZE, IMG_SIZE)\n)\nsex_test_generator = test_idg.flow_from_dataframe(\n    dataframe = imgdata.iloc[sex_N_indices_test, :],\n    directory = train_dir,\n    x_col='Image',\n    y_col='Patient Sex',\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode='categorical',\n    target_size=(IMG_SIZE, IMG_SIZE)\n)","e723e256":"resnet152.trainable = False\n\nsex_model = models.Sequential(name='sex_model')\nsex_model.add(resnet152)\nsex_model.add(layers.Flatten(name='flatten'))\nsex_model.add(layers.Dense(1, activation='sigmoid', name='dense_output'))\n\nsex_model.compile(\n    optimizer=optimizers.Adam(lr=1e-5),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nsex_model.summary()","c80ed7c7":"sex_history = sex_model.fit(\n    sex_train_generator,\n    validation_data=sex_val_generator,\n    epochs=4, steps_per_epoch=len(sex_N_indices_train)\/\/BATCH_SIZE, validation_steps=len(sex_N_indices_val)\/\/BATCH_SIZE,\n    shuffle=True\n)\npd.DataFrame(sex_history.history).to_csv('history\/sex_history_0.csv')\nsex_model.save('models\/sex_model_0.h5')","bd9272b6":"train_loss = sex_history.history['loss']\nval_loss = sex_history.history['val_loss']\ntrain_acc = sex_history.history['accuracy']\nval_acc = sex_history.history['val_accuracy']\n\nplt.plot(range(1,5), train_loss, 'r-', label='training loss')\nplt.plot(range(1,5), val_loss, 'b--', label='validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Sex prediction: Loss')\nplt.legend()\nplt.show()\n\nplt.plot(range(1,5), train_acc, 'r-', label='training accuracy')\nplt.plot(range(1,5), val_acc, 'b--', label='validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Sex prediction: Accuracy')\nplt.legend()\nplt.show()","2fad4c32":"# Prepare the data\n\nThis notebook is the continuation of my work on the ODIR5K dataset.\n\nIn the previous [notebook](https:\/\/www.kaggle.com\/mateuszbagiski\/odir5k-eda-keyphrases-image-sorting) I performed an Exploratory Data Analysis with an emphasis on diagnostic keyphrases, useful for predicting the clinical condition of left and right eye separately in a given patient. Using this set of keyphrases, I created a dataset of 6943 images of retinal fundi (out of original 7000), in which each image is provided with information about age, sex and clinical condition.\n\nIn this notebook, I'm going to build models for predicting age, sex and diseases from the images of retinal fundi. I will attempr to replicate the results of [Kim et al. (2020)](https:\/\/www.nature.com\/articles\/s41598-020-61519-9). They built convolutional models based on ResNet-152 architecture and trained them on almost 220,000 images of healthy fundi, to predict age and sex. Afterwards, they tested these models on separate test sets from healthy patients, as well as those from patients with diabetes mellitus, hypertension as well as those who were smokers. In general, they obtained very good results training their models on a pretty small learning rate (barely 1e-5) for just 10 epochs: MAE (mean absolute error) of around 3 years for age prediction and AUC (area under the curve) of about 0.97 for sex prediction.\n\nHere we have access to only 3100 images of healthy retinal fundi, so we may not get as good results as Kim et al. did. We also have much less images displaying symptoms of diseases for the task of predicting diseases from images, which was not undertaken by Kim et al. \n\nNevertheless, let's see how it goes.","bc95eadf":"Validation MAE seems to have plateaued at a slightly higher value than before fine-tuning. Let's compare the performance of both models (before and after fine-tuning) on the N-class test set.","4faf580e":"Although the loss somewhat declined, the accuracy has remained exactly on the chance level. I don't know why it didn't work.","9c1e7a93":"# Conclusions:\nThis probably is not enough data to reliably predict age and\/or sex of the patient.","d649c30f":"This is it for age and sex prediction. I'm going to continue working on this dataset to developa a model for predicting diseases in a separate notebook.","bf3e54a1":"Apparently fine-tuning didn't help at all, although it didn't make things worse either. Maybe Kim et al. knew it wouldn't help and therefore didn't bother with implementing it.\n\nThe most plausible reason, why we got worse results than Kim et al. (MAE 7.876 instead of close to 3) is a much smaller dataset (3100 images instead of 219,032).\n\nLet's see how this model, trained on healthy fundi, performs when tested on images of fundi displaying symptoms of particular diseases. Models developed by Kim et al., performed similarly on images of retinal fundi from patients with diabetes, hypertension, and history of smoking, as they did on images of \"normal\", healthy fundi.\n\nFor each of the clinical conditions in our dataset, we will use a separate generator.","6ce56f70":"imgdata - a dataframe (built from the original 'data' dataframe), where each of 6934 rows contains information about an individual eye\n\nPreviously we encoded the patient's sex numerically, but ImageDataGenerators, which we're going to use here, require categorical outputs, such as binary sex, to be encoded as strings. This is also true for the clinical conditions, so instead of a set of 0s and 1s, we need to encode the entire clinical state of the eye in one column (at least this is the most efficient way I know about). We will name this column 'Labels'.","f7bf5436":"It seems that all the learning took place during the first epoch. We could try to decrease the learning rate and train the classifier a little longer, but I found out that this does not result here in any improvement. \n\nPreviously, when I used smaller image sizes (256x256), MAE plateaued at similar values, but not already during the first epoch - the learning was substantially slower.\n\nWe will try to fine-tune the model, i.e. unfreeze some the top-most layers of the pre-trained base, so that they can learn to extract more useful features (more relevant for the age prediction) from the image. (Please note that Kim et al. do not report using this method in their original paper).\n\nResNet152 consists of 5 stacks of convolutions, extracting progressively more complex features from the image. We will unfreeze only the last (the third) block of the last (the fifth) stack. We will also use a much smaller learning rate, as it is recommended for fine-tuning.","56355d43":"We import the pre-trained ResNet152 model as the convolutional base and add a densely connected classifier on its top to create a model for age prediction.\n\nLike Kim et al., we will use Adam optimizer with learning rate of 1e-5 (as well as beta1=0.9 and beta2=0.999, set as default values), Huber loss function and MAE as a performance metric.","ae884e7c":"con2img - a dictionary mapping clinical conditions to the names of images displaying a given condition\n\nimg2con - a dictionary mapping images to the set of their clinical conditions","d15a5f83":"We will try to fine-tune this model for 16 epochs","ec336aaf":"# Sex prediction model\nLet's now move on to the sex prediction model. Note, however, that males are slightly overrepresented in our dataset, which may be a source of bias. We will therefore equate the number of males and females in our training by creating a list of indices with a balanced representation of both sexes.","de63c57a":"We will split the set of 3100 'N' images into training, validation, and test sets with proportions of 8:1:1 (or 2480:310:310), respectively","60018bbc":"Indeed, the MAE is a little greater for the oldest group, when compared to the middle age group. However, MAE is astonishingly high for the youngest group, completely unlike what Kim et al. reported.\n\nLet's maybe test this model only the images taken from healthy patients.","737c7db5":"The accuracy of predictions for different conditions somewhat varies. \n\nKim et al. report higher MAEs, when they tested their age prediction model on images taken from older (60yo+) patients. Let's see whether we can replicate their findings.","0d409384":"The model architecture will be essentially the same, the only difference being the activation function of the final layer. In age_model it was 'None' (or linear), so it would output just a number, i.e. the predicted age of the patient. For sex prediction, we're going to use the sigmoid function, which can take any real number and output a value between 0 and 1, which will be interpreted as the probability of belonging to one of the two categories.\n\nSince this is a classification problem (is this patient a male or a female?) we are going to use a different loss function and metrics than we used for age prediction, which was a regression problem.","dd307ba6":"We can re-use IDGs blueprints from before to make new generators","c8bd6f98":"Because we are dealing with huge (in terms of memory space taken) amounts of data, it's going to be much more efficient to use data generators, which will feed our models with a constant stream of data directly from the directories, instead of loading all these images and storing them locally as variables.","d8a49f8d":"con2ind - a dictionary mapping clinical conditions to the indices of images of fundi (as represented in 'imdata') displaying a given condition","65355877":"We need a preprocessing function, which will remove the black background from the images and resize them to the desired size: 256px height, 256px width, 3 color channels (RGB)","324addeb":"When we restrict testing to healthy people only, the MAE for the youngest group is slightly lower. However, it's hard in this case to discern the effects of stochastic fluctutations (due to small dataset) from those related to having more diseases. \n\nThe small difference between the middle and the oldest age groups reversed when we restricted the test group to healthy people. This also suggests that we cannot conclude too much from results of this experiment.","df367c12":"Since we have frozen the pre-trained convolutional base (taken from ResNet152), the only part of this model doing learning will be the (currently naive\/randomly initialized) densely connected classifier put on its top.\n\nWe will train this classifier for 8 epochs.","285113d5":"Let's make sure, that the distribution of sexes is even in train, validation, and test sets:"}}