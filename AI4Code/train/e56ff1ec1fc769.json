{"cell_type":{"8cf1a620":"code","9a18e1c4":"code","99a800ea":"code","5b968876":"code","9676b3c2":"code","84def901":"code","27a6fb55":"code","580448f8":"code","f3640383":"code","82ee057b":"code","0c711e44":"code","0e52c532":"code","37813b81":"code","3285917f":"code","33696877":"code","2eb997ff":"code","288716e6":"code","28bfde7a":"code","bff52113":"code","1e2c659a":"code","de33817c":"markdown","b5235771":"markdown","9dd185ab":"markdown","bc706341":"markdown","a0e7a849":"markdown","e22add7a":"markdown","acae72fc":"markdown","f71d97bf":"markdown","6301b90b":"markdown","f6f427df":"markdown","f685f8e1":"markdown","4d7e2102":"markdown"},"source":{"8cf1a620":"import spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\n\nfrom nltk.chunk import conlltags2tree, tree2conlltags\nfrom pprint import pprint\nfrom nltk.chunk import ne_chunk\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9a18e1c4":"ex = \"European authorities fined Google a record $5.1 billion on Wednesday\"\\\n\" for abusing its power in the mobile phone market and ordered the company to alter its practices\"","99a800ea":"def preprocess(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent\n\nsent = preprocess(ex)\nsent","5b968876":"pattern = 'NP: {<DT>?<JJ>*<NN>}'\ncp      = nltk.RegexpParser(pattern)\ncs       = cp.parse(sent)\n\nprint(cs)","9676b3c2":"iob_tagged = tree2conlltags(cs)\npprint(iob_tagged)","84def901":"ne_tree = ne_chunk(pos_tag(word_tokenize(ex)))\nprint(ne_tree)","27a6fb55":"nlp = en_core_web_sm.load()","580448f8":"doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\npprint([(X.text, X.label_) for X in doc.ents])","f3640383":"pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])","82ee057b":"def url_to_string(url):\n    res = requests.get(url)\n    html = res.text\n    soup = BeautifulSoup(html, 'html5lib')\n    for script in soup([\"script\", \"style\", 'aside']):\n        script.extract()\n    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))","0c711e44":"url = \"https:\/\/www.nytimes.com\/2018\/08\/13\/us\/politics\/peter-strzok-fired-fbi.html\"\\\n\"?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=\"\\\n\"first-column-region&region=top-news&WT.nav=top-news\"\n\nny_bb   = url_to_string(url)","0e52c532":"f = open(\"..\/input\/kernel.txt\",\"r\",encoding='UTF-8',errors='ignore')\nny_bb = f.read()","37813b81":"article = nlp(ny_bb)\nlen(article.ents)","3285917f":"labels = [x.label_ for x in article.ents]\nCounter(labels)","33696877":"items = [x.text for x in article.ents]\nCounter(items).most_common(3)","2eb997ff":"sentences = [x for x in article.sents]\nprint(sentences[20])","288716e6":"displacy.render(nlp(str(sentences[20])), jupyter=True, style='ent')","28bfde7a":"displacy.render(nlp(str(sentences[20])), style='dep', jupyter = True, options = {'distance': 120})","bff52113":"[(x.orth_,x.pos_, x.lemma_) for x in [y for y in nlp(str(sentences[20])) \n                                      if not y.is_stop and y.pos_ != 'PUNCT']]","1e2c659a":"dict([(str(x), x.label_) for x in nlp(str(sentences[20])).ents])","de33817c":"<ul>\n    <li>B ==> the token begins an entity<\/li>\n    <li>I ==> the token is inside an entity<\/li>\n    <li>O ==> the token is outside an entity<\/li>\n    <li>' ' ==> no entity tag is set<\/li>\n<\/ul>","b5235771":"There are 188 entities in the article","9dd185ab":"# Extracting named entity from an article","bc706341":"#### verbatim, extract part-of-speech and lemmatize this sentence.","a0e7a849":"in the code below : <br>the server refuses my connection (sending too many requests from same ip address in short period of time).\nso am going to use an external file wich contains the text data**","e22add7a":"### Entity","acae72fc":"### applying word tokenization and part-of-speech tagging to the sentence.","f71d97bf":"### Token","6301b90b":"#### the above sentence and its dependencies :","f6f427df":"### chunking","f685f8e1":"they are represented as 10 unique labels:","4d7e2102":"#### the three most frequent tokens : "}}