{"cell_type":{"a78e59a7":"code","a14aea9b":"code","50798fb3":"code","9128c568":"code","ce9141fc":"code","929667ee":"code","b28f8163":"code","3b1bc456":"code","895a4fec":"code","62f08bea":"code","4b82d764":"code","87d8ae91":"code","ac60ecbd":"code","3c5cb905":"code","f164b5f8":"code","400120dc":"code","5cdf4f29":"code","a00effbd":"code","f6bce9a1":"code","d88c4d61":"code","5f556e14":"code","53f235e4":"code","967a3ff3":"code","50335f8a":"code","e8dbfea2":"code","5f19e085":"code","441210f9":"code","02c49278":"code","b0644832":"code","6792d15c":"code","97ae0d7d":"code","185b2148":"code","1a689bae":"code","8a540e1e":"code","3696435c":"code","f5760ba3":"code","fd79e5b4":"code","1e03d769":"code","72c13084":"code","78eeb1df":"code","da037539":"code","0182eaf3":"code","7fccf4cb":"code","149cb567":"code","4bca4760":"code","9123b50d":"code","5d7b33e5":"code","db6101b0":"code","ea75fb10":"code","7bf663cd":"code","c09a0cb5":"code","29a31bc7":"code","2319ad6c":"code","0a1803c8":"code","eeab706e":"code","af9fcbff":"code","0c6519bd":"code","b2036a2b":"code","c7955624":"code","7e7f9096":"code","38063c7d":"code","ecea3679":"code","566c0b06":"code","3339605b":"code","43c4a628":"code","cc09017f":"code","34b7f6e9":"code","67aaaa97":"code","ea708493":"code","3a9368b5":"code","29190af4":"code","e65ae8fe":"code","7ba8f403":"code","891c3f6b":"code","f61935da":"code","590f5204":"code","46a65d75":"code","ba6cf032":"code","915d44b9":"code","ce8cd877":"code","b2db2ce4":"code","cf4b4164":"code","fc2e442a":"code","520f5fa7":"code","4d65d78c":"code","5744f82c":"code","dfff108c":"code","06cb063b":"code","41a7da18":"code","4e49410b":"code","21f2ac4e":"code","2e5e7665":"code","a58c5908":"code","b684b9ff":"code","9caed681":"code","6e670820":"code","0146a9aa":"code","5c0b5fbe":"code","0885b47a":"code","7a5cbda5":"code","b89190c6":"code","34a0c548":"code","fd2b5bfb":"code","024d8dc3":"code","8295ce25":"code","62149fc5":"code","81dee58e":"code","a422aa3a":"code","88eaf47a":"code","f24b58a4":"code","715c59c2":"code","043a0720":"code","3577acdb":"code","4136e13b":"code","e6d8cd9f":"code","8246e144":"code","35e7a330":"code","baf2ac85":"code","64181740":"code","b5fe4aea":"code","5015fcfe":"code","612815ab":"code","2ef2c43c":"code","62a22a3a":"code","40becd90":"code","51a271da":"code","65079aaf":"code","1ebd3c84":"markdown","1414f732":"markdown","2522dad2":"markdown","6c22a8bb":"markdown","22be22b7":"markdown","8b1f6700":"markdown","7f986230":"markdown","e29607e9":"markdown","ee26e395":"markdown","e13e8717":"markdown","8df2609e":"markdown","8b3d147d":"markdown","6c0dfcef":"markdown","401cb42e":"markdown","7106714d":"markdown","da7a1903":"markdown","42b00987":"markdown","2160233c":"markdown","0b49e9b0":"markdown","443df872":"markdown","4b2a57ca":"markdown","4d806659":"markdown","436b5273":"markdown","8bf409cf":"markdown","34c6251c":"markdown","bb4e90c0":"markdown","9c2c6f04":"markdown","a96355d8":"markdown","c8b3af6b":"markdown","676ab869":"markdown","86df368b":"markdown","d2f20809":"markdown","0528d066":"markdown","858bb1ee":"markdown","f432271e":"markdown"},"source":{"a78e59a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a14aea9b":"# importing required \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing as pre\nimport sklearn.neighbors as NN\nimport sklearn.linear_model as lm\nfrom sklearn import model_selection as ms\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","50798fb3":"#loading the training data\n# since the first row of the original file was unordered, it was made coherent at first\ndata = pd.read_csv(\"..\/input\/training-data\/train_SJC.csv\")\ndata.head()","9128c568":"data.info() ","ce9141fc":"data.describe()","929667ee":"data.describe(include='object') ","b28f8163":"data.shape   # to check the shape of the dataset","3b1bc456":"data['Gender'].value_counts()","895a4fec":"# removing the gender other than male or female, since there is only one such value\ndata = data[data['Gender']!='U']\ndata['Gender'].value_counts()","62f08bea":"data['MaritalStatus'].value_counts()","4b82d764":"(3542\/36175)*100\n# leaving marital status as such since other category constitute about 10 % of the total data","87d8ae91":"data['PartTimeFullTime'].value_counts()","ac60ecbd":"data.isnull().sum()    # checking for missing values","3c5cb905":"# copying the dataset so that original dataset remains in tact and we can do the transformations on the new dataset\n\ndata_new = data","f164b5f8":"# comparing the data shape before and after dropping the missing values\n\ndata_new.shape , data_new.dropna().shape\n","400120dc":"# so, if we drop missing values, 127 rows are dropped. Since the dataset is large, we can proceed with this step\n\ndf= data_new.dropna()\ndf.head()","5cdf4f29":"df.columns","a00effbd":"df['Age'].min() , df['Age'].max()","f6bce9a1":"# creating a new columns in which age is grouped\n\ndf['Age_group']=pd.cut(df['Age'],bins=[13,18,41,59,80], labels=['Too young','Young','Middle aged','Old'])","d88c4d61":"df['Age_group'].value_counts()","5f556e14":"df['WeeklyWages'].min() , df['WeeklyWages'].max()","53f235e4":"# creating a new columns in which Weekly wage is grouped\n\ndf['Wage_category'] = pd.cut(df['WeeklyWages'],bins=[0,50,250,900,2500,8000], labels=['very low','low','moderate','high','very high'])","967a3ff3":"df['Wage_category'].value_counts()","50335f8a":"# copying the dataset for data encoding and scaling\ndf_non_miss = df","e8dbfea2":"# finding out the range of the column 'UltimateIncurredClaimCost'\ndf['UltimateIncurredClaimCost'].min() , df['UltimateIncurredClaimCost'].max()","5f19e085":"# binning the result column for ease of EDA\ndf['UIC_bin'] = pd.cut(df['UltimateIncurredClaimCost'],bins=[100,500,1000,5000,10000,5000000], labels=['too low','level2','level3','level4','too high'])","441210f9":"df['UIC_bin'].value_counts()","02c49278":"# checking the shape of the dataset\ndf.shape","b0644832":"# outlier analysis\n\ndf.plot.box(figsize=(25,6))  # only continuous variables are plotted","6792d15c":"sns.distplot(df['UltimateIncurredClaimCost']) # plot to find the distribution of a continuous variable.","97ae0d7d":"sns.catplot(data=df,x='UltimateIncurredClaimCost',kind ='box', col = 'Gender')","185b2148":"sns.catplot(data=df,x='UltimateIncurredClaimCost',kind ='box', col = 'Age_group')","1a689bae":"sns.catplot(data=df,x='UltimateIncurredClaimCost',kind ='box', col = 'Wage_category')","8a540e1e":"# removing outlier\n\ndf[df['UltimateIncurredClaimCost']>1000000]  # this is the outlier value","3696435c":"# data without outlier\n\ndf = df[df['UltimateIncurredClaimCost']<1000000] #outlier removed","f5760ba3":"df.plot.box(figsize=(25,6))  #since there are no outliers, the plot is more clear now.","fd79e5b4":"df.columns","1e03d769":"cat_df = df.select_dtypes(exclude=[float,int])\ncat_df.head()","72c13084":"num_df = df.select_dtypes(include=[float,int])\nnum_df.head()","78eeb1df":"list_df = [  'Gender','MaritalStatus', 'DependentChildren', 'DependentsOther', \n       'PartTimeFullTime', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n        'InitialIncurredCalimsCost', 'Age_group', 'Wage_category', 'UIC_bin']","da037539":"sns.pairplot(df[list_df],hue='Gender',diag_kind=\"kde\")","0182eaf3":"sns.countplot(data=df , x= 'Gender' , hue = 'UIC_bin')","7fccf4cb":"sns.countplot(data=df , x= 'Age_group' , hue = 'UIC_bin')","149cb567":"sns.countplot(data=df , x= 'Wage_category' , hue = 'UIC_bin')","4bca4760":"sns.countplot(data=df , x= 'PartTimeFullTime' , hue = 'UIC_bin')","9123b50d":"sns.countplot(data=df , x= 'DaysWorkedPerWeek', hue = 'UIC_bin')","5d7b33e5":"sns.countplot(data=df , x= 'DependentChildren' , hue = 'UIC_bin')","db6101b0":"sns.countplot(data=df , x='DependentsOther' , hue = 'UIC_bin')","ea75fb10":"le = pre.LabelEncoder()","7bf663cd":"df.columns","c09a0cb5":"# for data transformation purpose, all the variables should be either float or string.\n# taking the required columns (with float value)\nlist_df_new=['Age', 'Gender','MaritalStatus', 'DependentChildren', 'DependentsOther', 'WeeklyWages',\n       'PartTimeFullTime', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n        'InitialIncurredCalimsCost','UltimateIncurredClaimCost']  ","29a31bc7":"#label encoding all the values\nfor x in list_df_new:\n    df[x] = le.fit_transform(df[x])","2319ad6c":"# old dataset modified with selected columns to create a new dataset\n# all the columns are encoded\ndf_ml = df[list_df_new] \n","0a1803c8":"df_ml.head()","eeab706e":"df_ml.info()","af9fcbff":"# for data normalisation\n# each value in the data is scaled between 0 and 1\n\nmin_max=pre.minmax_scale","0c6519bd":"df_test=min_max(df_ml.values)","b2036a2b":"# normalised data to a dataframe\ndf_norm = pd.DataFrame(data=df_test,columns = df_ml.columns.to_list())\ndf_norm.head()","c7955624":"# the dataset is divided into two.\n# first part consists of all other values other than the output variable\n\nfeatures=df_norm.drop('UltimateIncurredClaimCost',axis=1)\nY=df_norm['UltimateIncurredClaimCost']","7e7f9096":"# splitting the data into train and test data.\n\nx_train,x_test,y_train,y_test=ms.train_test_split(features,Y,test_size=0.3,random_state=1234566)","38063c7d":"#checking the shape of the data\n\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","ecea3679":"#using KNN and specifying the parameters.\n\nKNN=NN.KNeighborsRegressor(n_neighbors=25)","566c0b06":"# fitting the data to the model\n\nKNN.fit(x_train,y_train)","3339605b":"KNN.predict(x_test)","43c4a628":"KNN.score(x_test,y_test)","cc09017f":"KNN.score(x_train,y_train)","34b7f6e9":"# splitting the dataset into other variables and outcome variable \n# similar to what we did earlier.\n\nfeatures=df_norm.drop('UltimateIncurredClaimCost',axis=1)\ny=df_norm['UltimateIncurredClaimCost']","67aaaa97":"features.head()","ea708493":"y.head()","3a9368b5":"# splitting the data into train and test set\n\nx_train,x_test,y_train,y_test=ms.train_test_split(features,y,test_size=0.3,random_state=1234566)","29190af4":"x_train.shape ,x_test.shape ,y_train.shape ,y_test.shape","e65ae8fe":"# using glm for linear reagression\nglm = lm.LinearRegression()","7ba8f403":"glm.fit(x_train,y_train)","891c3f6b":"glm.predict(x_test)","f61935da":"glm.score(x_test,y_test)","590f5204":"glm.score(x_train,y_train)","46a65d75":"\nmodel = DecisionTreeRegressor()\n# fit the model\nmodel.fit(x_train,y_train)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nprint('Feature \\t Score: ')\nfor i,v in enumerate(importance):\n    \n    print(i,\"\\t\",v)\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","ba6cf032":"# function to calculate glm test and train score\n\ndef f_train_test_score(data,drop_col):\n    #normalising the data\n    df_test=min_max(df_ml.values)  \n    \n    #converting normalised data to dataframe\n    df_norm = pd.DataFrame(data=df_test,columns = df_ml.columns.to_list()) \n    \n    \n    # dividing features and outcomes into two files and then splitting it into train ans test sets\n    features= df_norm.drop(drop_col,axis=1) \n    y= df_norm['UltimateIncurredClaimCost']\n    x_train,x_test,y_train,y_test=ms.train_test_split(features,y,test_size=0.3,random_state=1234566)\n    \n    #using glm, fitting the data into glm and printing the train and test scores.\n    glm = lm.LinearRegression()\n    glm.fit(x_train,y_train)\n    print(\" The test score is \",glm.score(x_test,y_test))\n    print(\" The train score is \",glm.score(x_train,y_train))\n   ","915d44b9":"f_train_test_score(df_ml,['UltimateIncurredClaimCost'])","ce8cd877":"f_train_test_score(df_ml,['DependentsOther','DependentChildren','UltimateIncurredClaimCost'])","b2db2ce4":"f_train_test_score(df_ml,['DependentsOther','PartTimeFullTime','UltimateIncurredClaimCost'])","cf4b4164":"f_train_test_score(df_ml,['DependentsOther','PartTimeFullTime','Gender','UltimateIncurredClaimCost'])","fc2e442a":"f_train_test_score(df_ml,['DependentsOther','PartTimeFullTime','MaritalStatus','UltimateIncurredClaimCost'])","520f5fa7":"f_train_test_score(df_ml,['DependentsOther','HoursWorkedPerWeek','PartTimeFullTime','UltimateIncurredClaimCost'])","4d65d78c":"f_train_test_score(df_ml,['DependentsOther','PartTimeFullTime','Age','UltimateIncurredClaimCost'])","5744f82c":"f_train_test_score(df_ml,['DependentsOther','PartTimeFullTime','WeeklyWages','UltimateIncurredClaimCost'])","dfff108c":"f_train_test_score(df_ml,['DependentsOther','PartTimeFullTime','InitialIncurredCalimsCost','UltimateIncurredClaimCost'])","06cb063b":"list_df_new1=['Age', 'Gender','MaritalStatus', 'DependentChildren', 'WeeklyWages',\n             'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n        'InitialIncurredCalimsCost','UltimateIncurredClaimCost']  ","41a7da18":"#label encoding all the values\nfor x in list_df_new1:\n    df[x] = le.fit_transform(df[x])","4e49410b":"df_ml1 = df[list_df_new1]","21f2ac4e":"df_test=min_max(df_ml1.values)","2e5e7665":"# normalised data to a dataframe\ndf_norm = pd.DataFrame(data=df_test,columns = df_ml1.columns.to_list())\ndf_norm.head()","a58c5908":"# splitting the dataset into other variables and outcome variable \n# similar to what we did earlier.\n\nfeatures=df_norm.drop('UltimateIncurredClaimCost',axis=1)\ny=df_norm['UltimateIncurredClaimCost']","b684b9ff":"# splitting the data into train and test data.\n\nx_train,x_test,y_train,y_test=ms.train_test_split(features,Y,test_size=0.3,random_state=1234566)","9caed681":"KNN=NN.KNeighborsRegressor(n_neighbors=20)","6e670820":"KNN.fit(x_train,y_train)","0146a9aa":"KNN.predict(x_test)","5c0b5fbe":"KNN.score(x_test,y_test)","0885b47a":"KNN.score(x_train,y_train)","7a5cbda5":"# final test and train score is:\nf_train_test_score(df_ml,['DependentsOther','PartTimeFullTime','UltimateIncurredClaimCost'])","b89190c6":"# importing the test data\n\nnew_test = pd.read_csv(\"..\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv\")\nnew_test.head()","34a0c548":"#checking for null values\n\nnew_test.isnull().sum()","fd2b5bfb":"# replacing null values from test data\n\nnew_test['MaritalStatus']=new_test['MaritalStatus'].fillna(new_test['MaritalStatus'].mode()[0])\nnew_test.isnull().sum()","024d8dc3":"new_test","8295ce25":"new_list = ['Age','Gender', 'MaritalStatus', 'DependentChildren',\n        'WeeklyWages', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek', 'InitialIncurredCalimsCost']","62149fc5":"test =new_test[new_list]\ntest.shape","81dee58e":"\nfor x in new_list:\n    test[x]=le.fit_transform(test[x])","a422aa3a":"# normalising test data\nfinal_test=min_max(test.values)","88eaf47a":"# importing the sample submission csv\nsample = pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv')\nsample.shape","f24b58a4":"sample1 = sample","715c59c2":"#predicted value of knn\npredicted_knn = KNN.predict(test)","043a0720":"# adding new column to sample csv\nsample['UltimateIncurredClaimCost']= predicted_knn","3577acdb":"sample.head()  # dataframe of prediction using KNN","4136e13b":"sample.to_csv(\"Sample Submission new.csv\", index = False)  #saving the modified dataframe as a csv file","e6d8cd9f":"#new_sample = pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv')","8246e144":"# prediction using glm\n\n#predicted_glm = glm.predict(final_test)\n#predicted_glm.shape","35e7a330":"# adding new column to the sample csv file\n#new_sample['UltimateIncurredClaimCost']= predict_glm","baf2ac85":"#new_sample['UltimateIncurredClaimCost']= predict_glm","64181740":"#new_sample.to_csv(\"NewSampleSubmission.csv\", index = False)","b5fe4aea":"#new_sample.head()  # dataframe of prediction using GLM","5015fcfe":"# Instantiate model with 3000 decision trees\nRF = RandomForestRegressor(n_estimators = 1000, random_state = 50)\n\n\n# Train the model on training data\nRF.fit(x_train, y_train);","612815ab":"# Use the forest's predict method on the test data\n\npredictions = RF.predict(x_test)\n","2ef2c43c":"# Calculate the absolute errors\nerrors = abs(predictions - y_test)\nerrors","62a22a3a":"predictions = RF.predict(final_test)","40becd90":"# adding new column to sample csv\nsample1['UltimateIncurredClaimCost']= predictions","51a271da":"sample1.head()","65079aaf":"sample1.to_csv(\"SampleSubmission.csv\", index = False)  #saving the modified dataframe as a csv file","1ebd3c84":"## 6. Feature importance","1414f732":"**K  ....................  train score  ..............    test score** <br>\n <br>\n 5 ..............0.8463563754247978 .........  0.7609426674886294 <br>\n 10 .......... 0.824918474501286 .........  0.7785861677346249 <br>\n 15 .......... 0.8165400362017157 ...........  0.7831364931774443 <br>\n 25 .......... 0.8070922295398893..........  0.7835218825541196 <br>\n 30...........0.8036581846568207..........0.7828346411572328","2522dad2":"Here, by dropping the column 'PartTimeFullTime', the model performance is slightly increasing. So we can drop that column.","6c22a8bb":"As the n-neighbour value(k value) increases, the test score is achieving better value. But, after a certain point, test score is declining. So, we can choose k=25 as the preferred value","22be22b7":"MaritalStatus, WeeklyWages, HoursWorkedPerWeek are the three columns with missig values. <br>\n","8b1f6700":"## 10. Conclusion<br><br>\nOut of all the models used, the rsme score for KNN was the least.","7f986230":"['DependentsOther','PartTimeFullTime','UltimateIncurredClaimCost'] are the columns to be dropped","e29607e9":"## 4. Data Encoding and Scaling","ee26e395":"## 4. EDA","e13e8717":"KNN regressor: for different values of n_neighbours, the train and test score are calculated.","8df2609e":"**From the above graphs, we can infer that there is one person who got a claimed of a large sum. <br>\n<br>\nIt is a middle aged women with a salary ranging between 250 and 900 units.**","8b3d147d":"****\n\n#### EDA CONCLUSION <br> \n\n1. Most of the claimers were men.\n2. Most of them claimed an amount between 1000 and 5000 units.\n3. Most of the claimers were young or middle aged.\n4. Their weekly wages ranged between 50 and 900 units.\n5. Most of the claimers worked for 5 days a week.\n6. Mostly there were no dependent people.\n\n\n\n****","6c0dfcef":"## 7. Prediction","401cb42e":"Since the datapoint is an outlier, we can remove the point from the dataset for ease of analysis. <br>\nOtherwise, it can affect our prediction.","7106714d":"## 3. Data transformation","da7a1903":"## 9. Reason for selecting the model\n\n* KNN was a primitive model and it required lesser time.\n* The other two models were tried out to find out if there was any change in the final rsme score.","42b00987":"The data is highly positively skewed","2160233c":"#### All the values are scaled between the range 0 and 1 scale","0b49e9b0":"## KNN after feature selection","443df872":"### checking whether the variables of object datatype have \"unknown\" category ","4b2a57ca":"## 1. Data loading","4d806659":"## 2. Data preprocessing","436b5273":"### data summary <br>\n#### Data fields\n* ClaimNumber: Unique policy identifier\n* DateTimeOfAccident: Date and time of accident\n* DateReported: Date that accident was reported\n* Age: Age of worker\n* Gender: Gender of worker\n* MaritalStatus: Martial status of worker. (M)arried, (S)ingle, (U)unknown.\n* DependentChildren: The number of dependent children\n* DependentsOther: The number of dependants excluding children\n* WeeklyWages: Total weekly wage\n* PartTimeFullTime: Binary (P) or (F)\n* HoursWorkedPerWeek: Total hours worked per week\n* DaysWorkedPerWeek: Number of days worked per week\n* ClaimDescription: Free text description of the claim\n* InitialIncurredClaimCost: Initial estimate by the insurer of the claim cost\n* UltimateIncurredClaimCost: Total claims payments by the insurance company. This is the field you are asked to predict in the test set.","8bf409cf":"\n#### The dataset, contain information on 36,000 insurance policies . We have to predict the ultimate insurance amount claimed by a person. In this, notebook, I am trying to compare the working of KNN, GLM and Random Forest for linear regression. Both KNN and GLM were carried with and without feature selection\n<br>","34c6251c":"## 5. Machine learning model\n### KNN and GLM (before feature selction)\n* Here, I have to build a regression model. At first, i am going to create a KNN regressor model, then a linear regressor model","bb4e90c0":"If we drop the column 'InitialIncurredCalimsCost', the train and test scores are decreasing rapidly. So, don't drop the column and it is the most important feature in prediction of ultimate claim incurred.","9c2c6f04":" \n\nNow, let us consider the Generalised linear model","a96355d8":"## 8. Model Used: Random Forest","c8b3af6b":"### Feature importance selection","676ab869":"the test score is increasing, therefore the variable we dropped is of less importance","86df368b":"Thus the training dataset has 36176 examples with 15 features.","d2f20809":"trying to delete columns which are less important on the basis of test score.","0528d066":"test score decreased, so don't drop the variable. similarly, continue till we get maximum test score.","858bb1ee":"Now, since,'UltimateIncurredClaimCost' is the final outcome, we are trying to plot it on the basis of several categorical variables(either provided in the dataset or created by us).","f432271e":"#### The missing values accounted to about 120 datapoint, so they were removed."}}