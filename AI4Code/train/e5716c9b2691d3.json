{"cell_type":{"67bb9264":"code","6cb9fbbc":"code","739ed3c1":"code","07a708d2":"code","346ae97b":"code","99b92a6f":"markdown","4a585702":"markdown","4f360320":"markdown","735a1e08":"markdown","72ce4f98":"markdown","5734bdf3":"markdown"},"source":{"67bb9264":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom transformers import AutoTokenizer, AutoModel #for embeddings\nfrom sklearn.metrics.pairwise import cosine_similarity #for similarity\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6cb9fbbc":"#download pretrained model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",)\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\",output_hidden_states=True)","739ed3c1":"#create embeddings\ndef get_embeddings(text,token_length):\n    tokens=tokenizer(text,max_length=token_length,padding='max_length',truncation=True)\n    output=model(torch.tensor(tokens.input_ids).unsqueeze(0),\n                 attention_mask=torch.tensor(tokens.attention_mask).unsqueeze(0)).hidden_states[-1]\n    return torch.mean(output,axis=1).detach().numpy()","07a708d2":"#calculate similarity\ndef calculate_similarity(text1,text2,text3,token_length=20):\n    out1=get_embeddings(text1,token_length=token_length) #create embeddings of text\n    out2=get_embeddings(text2,token_length=token_length) #create embeddings of text\n    out3=get_embeddings(text3,token_length=token_length) #create embeddings of text\n    sim1= cosine_similarity(out1,out3)[0][0]\n    sim2= cosine_similarity(out2,out3)[0][0]\n    print(sim1,sim2)\n    if sim1>sim2:\n        print('sentence 1 is more similar to input sentence')\n    else:\n        print('sentence 2 is more similar to input sentence')","346ae97b":"text1 = 'It is therefore to a description of these systems that we now turn.'\ntext2 = 'There is another way of thinking about this process that ties in with our ideas about memory.'\ntext3 = 'There are therefore two complementary ways of looking at what is happening.'\n\ncalculate_similarity(text1,text2,text3)","99b92a6f":"# Measure Sentence Similarity using the pre-trained BERT Model","4a585702":"## Loading Pretrained Model","4f360320":"## What is BERT?\n\n**BERT (Bidirectional Encoder Representations from Transformers)** is a transformer-based NLP pretraining model developed by Google in 2018. It is nothing but a Transformer language model with multiple encoder layers and self-attention heads. With the release of BERT, anyone in the world can now train their own question answering model, sentiment analysis, or any other language model quickly and efficiently.","735a1e08":"## Creating text embeddings","72ce4f98":"## Import Libraries and module","5734bdf3":"## Function to calculate similarity"}}