{"cell_type":{"08d00d8d":"code","e85383db":"code","2bdc8ced":"code","0a497828":"code","17685ff2":"code","92e692d0":"code","68f2569d":"code","43cb3c9a":"code","dd1bc59e":"code","0b5c4c62":"code","8f012577":"code","8764401d":"code","f962b48b":"code","fca6d289":"code","1a31efed":"code","de6052fb":"code","90b36b98":"code","c4c97109":"code","81b55be1":"code","85fc78bc":"code","5423a2df":"code","f426f02b":"code","866d9e9d":"code","8ecf9f22":"code","c4bc003f":"code","06ac1f25":"code","7d0f6678":"code","c38ea834":"code","14792113":"code","e9de7866":"code","ddb210ee":"code","e5625376":"code","8dec7f8f":"code","b4443a40":"code","ccf59887":"code","6aa07fdf":"code","ab1eb22a":"code","218c0c80":"code","923a2f1b":"code","dc6915f2":"code","a59aa0b6":"code","01bede55":"code","d167c799":"code","bdfd7b0c":"code","57797305":"code","026b6eba":"code","f1a0798d":"code","414356ab":"code","61592010":"code","703eb4ca":"code","0610f57f":"code","2597d3c0":"markdown","e63814ab":"markdown"},"source":{"08d00d8d":"import re\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.util import ngrams\nstop = set(stopwords.words('english'))","e85383db":"# from google.colab import drive\n# drive.mount('\/gdrive')","2bdc8ced":"# df_train = pd.read_csv('\/gdrive\/My Drive\/nlp_getting_statred\/train.csv')\n# df_test = pd.read_csv('\/gdrive\/My Drive\/nlp_getting_statred\/test.csv')","0a497828":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","17685ff2":"def dsply_all(df):\n    with pd.option_context('display.max_rows', 5000, 'display.max_columns', 300):\n        display(df)","92e692d0":"dsply_all(df_train.head(3000))","68f2569d":"df_train[df_train['target'] == 1].head()","43cb3c9a":"df_train[df_train['target'] == 0].head()","dd1bc59e":"# converting all the data into lowercase\ndf_train['text'] = df_train['text'].str.lower()\ndf_test['text'] = df_test['text'].str.lower()","0b5c4c62":"df_train.tail()","8f012577":"df_test.tail()","8764401d":"# Removing noise before doing any processing to the data\n\ndef rmove_noise(df):\n    #remove html markup\n    df = re.sub(\"(<.*?>)\", \"\", df)\n    #remove non-ascii and digits\n    df = re.sub(\"(\\\\W|\\\\d)\", \" \", df)\n    #remove whitespace\n    df = df.strip()\n    return df","f962b48b":"df_train['text'] = df_train['text'].apply(rmove_noise)\ndf_test['text'] = df_test['text'].apply(rmove_noise)","fca6d289":"df_train","1a31efed":"real_disater = df_train[df_train['target'] == 1].shape[0]\nnot_real = df_train[df_train['target'] == 0].shape[0]","de6052fb":"# getting an estimate of the real disasters and unreal ones using a bar graph\n\nplt.rcParams['figure.figsize'] = (7,5)\nplt.bar(10, real_disater,3 ,  label = 'real_disaster', color = 'green')\nplt.bar(15, not_real, 3,  label = 'not disaster', color = 'blue')\nplt.legend()\nplt.ylabel('Number of Examples')\nplt.title('proportion of tweets')   \nplt.show()","90b36b98":"def mking_toekns(df):\n     txt = \"\".join([c for c in df if c not in string.punctuation])\n     tokens = re.split('\\W+', df)\n     return tokens","c4c97109":"df_train['text'] = df_train['text'].apply(mking_toekns)\ndf_test['text'] = df_test['text'].apply(mking_toekns)","81b55be1":"# Now replacing bbreviations, slangs and misspelled words for real meanings -- Normalization.\n# following abbreviations are being used along with the nltk lemmatizer.\n# I'm using this before stopwords because it fullforms comtain many stopwords.\n# Normalization-- This tweets are not written in a gramatical fashion, So in order to understand that the shortcuts and abbreviatons have the\n# same meaning.\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",   \n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"}\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower()  in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokens = [convert_abbrev(word) for word in text]\n    # text = ' '.join(tokens)\n    return text\n","85fc78bc":"df_train['text'] = df_train['text'].apply(convert_abbrev_in_text)\ndf_test['text'] = df_test['text'].apply(convert_abbrev_in_text)","5423a2df":"# Here we will remove the stopwords, so that we it so that the data gets clean and lite.\ndef remove_stopwords(df):\n    # txt = \"\".join([c for c in df if c not in string.punctuation])\n    # tokens = re.split('\\W+', df)\n    txt = [w for w in df if w not in stop]\n    return txt\n# remove_stopwords = lambda x:[[w for w in word_tokenize(sent) if w not in stop] for sent in sent_tokenize(x)]\n","f426f02b":"df_train['text'] = df_train['text'].apply(remove_stopwords)\ndf_test['text'] = df_test['text'].apply(remove_stopwords)","866d9e9d":"dsply_all(df_train['text'].head(3000))","8ecf9f22":"# Using WordNetLemmatizer to lemmatize the words which have same meaning but\n# different spellings, later in the section we will lemmatize some abbrs.\n\n# words_lematize = lambda k:[w for w in lemmatizer.lemmatize(k)]\ndef words_lematize(df):\n    wn = WordNetLemmatizer()\n    text = [wn.lemmatize(w) for w in df]\n    text = ' '.join(text)\n    return text\n","c4bc003f":"df_train['text'] = df_train['text'].apply(words_lematize)\ndf_test['text'] = df_test['text'].apply(words_lematize)","06ac1f25":"dsply_all(df_train.head(2000))","7d0f6678":"import tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom keras.initializers import Constant","c38ea834":"import zipfile \nzip_ref = zipfile.ZipFile(\"\/gdrive\/My Drive\/glove.6B.zip\", 'r')\nzip_ref.extractall(\"\/tmp\")\nzip_ref.close()","14792113":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","e9de7866":"corpus=create_corpus(df_train)","ddb210ee":"df_train","e5625376":"embedding_dict={}\nwith open('\/tmp\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()\n","8dec7f8f":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","b4443a40":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","ccf59887":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","6aa07fdf":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","ab1eb22a":"model.summary()","218c0c80":"model.summary()","923a2f1b":"train=tweet_pad[:df_train.shape[0]]\ntest=tweet_pad[:df_test.shape[0]]","dc6915f2":"X_train,X_test,y_train,y_test=train_test_split(train,df_train['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","a59aa0b6":"history=model.fit(X_train,y_train,batch_size=4,epochs=50,validation_data=(X_test,y_test),verbose=2)","01bede55":"# history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","d167c799":"model.save('realornot2.hdf5')","bdfd7b0c":"train_pred = model.predict(X_train)\ntrain_bool = np.argmax(train_pred, axis=1)\ntest_pred = model.predict(X_test)\ntest_bool = np.argmax(test_pred, axis=1)","57797305":"from sklearn.metrics import classification_report\nX_train_score = classification_report(y_train, train_bool)\nX_test_score = classification_report(y_test,test_bool)\n# print('F1 Score for train and test data resp.: {} & {}'.format(X_train_score,X_test_score))","026b6eba":"print(X_train_score)","f1a0798d":"print(X_test_score)","414356ab":"y_pre=model.predict(test_trial)","61592010":"y_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':df_test['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","703eb4ca":"y_pre.shape","0610f57f":"df_test","2597d3c0":"I built this notebook on google colab,So there might be some inconsistensies while running on colab.","e63814ab":"# Model building for the data.."}}