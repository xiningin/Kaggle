{"cell_type":{"f7bab333":"code","84f62829":"code","f1a7d8aa":"code","9f93460a":"code","99912a8d":"code","8c2d71d6":"code","49336942":"markdown","3fdac862":"markdown","8a5e40a5":"markdown","3109928f":"markdown","9563392e":"markdown","ba87ad63":"markdown"},"source":{"f7bab333":"# Numpy & matplotlib for notebooks \n%pylab inline\n\n# Pandas\nimport pandas as pd # Data analysis and manipulation \n\n# Sklearn \nfrom sklearn.preprocessing import StandardScaler # to standardize features by removing the mean and scaling to unit variance (z=(x-u)\/s)\nfrom sklearn.neural_network import MLPClassifier # Multi-layer Perceptron classifier which optimizes the log-loss function using LBFGS or sdg.\nfrom sklearn.model_selection import train_test_split # to split arrays or matrices into random train and test subsets\nfrom sklearn.model_selection import KFold # K-Folds cross-validator providing train\/test indices to split data in train\/test sets.\nfrom sklearn.decomposition import PCA, TruncatedSVD # Principal component analysis (PCA); dimensionality reduction using truncated SVD.\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.naive_bayes import MultinomialNB # Naive Bayes classifier for multinomial models\nfrom sklearn.feature_extraction.text import CountVectorizer # Convert a collection of text documents to a matrix of token counts\nfrom sklearn.metrics import roc_auc_score as roc # Compute Area Under the Receiver Operating Characteristic Curve from prediction scores\nfrom sklearn.metrics import roc_curve, auc # Compute ROC; Compute Area Under the Curve (AUC) using the trapezoidal rule\n\n# Matplotlib\nimport matplotlib # Data visualization\nimport matplotlib.pyplot as plt \nimport matplotlib.patches as mpatches  \n\n# Seaborn\nimport seaborn as sns # Statistical data visualization (based on matplotlib)","84f62829":"# Import the test dataset and create a list of authors\ntest_data = pd.read_csv(\"..\/input\/final-dataset\/test_data.csv\", encoding=\"utf8\")\n\na_test = []\nfor author, group in test_data.groupby(\"author\"):\n    a_test.append(author)\n\n# Load predictions on validation \n\n# MLP on doc2vec\nx1 = np.load(\"..\/input\/final-dataset\/y_scoremlpClf.npy\") #y_D2V-mlpClf.npy\n\n# XGB on countvectorized texts\nx2 = np.load(\"..\/input\/final-dataset\/y_predict_XGB.npy\")\n\n# MLP on binary countvectorized subreddits\nx3 = np.load(\"..\/input\/final-dataset\/y_score_MLPs.npy\")\n\n# Load predictions of all models\ny = np.load(\"..\/input\/final-dataset\/y_valid.npy\") # common validation y of previous steps\n\n# Load predicted test doc2vec\nt1 = np.load(\"..\/input\/final-dataset\/y_testD2V.npy\")\n\n# Load predicted countvectorized test texts\nt2 = np.load(\"..\/input\/final-dataset\/y_predict_testXGBnS.npy\") #  #y_testXGBnS.npy\n\n# Load predicted countvectorized test subreddits\nt3 = np.load(\"..\/input\/final-dataset\/y_testMLPs.npy\")","f1a7d8aa":"a = np.vstack((x3,x2,x1))\n\nt = np.vstack((t3,t2,t1))\n\nX = a.T # transpose\nT = t.T # transpose","9f93460a":"# Plot the test data along the 2 dimensions of largest variance\ndef plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='M')\n            blue_patch = mpatches.Patch(color='blue', label='F')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 20})\n\nfig = plt.figure(figsize=(8, 8))          \nplot_LSA(X, y)\nplt.show()","99912a8d":"# Logistic regression \nlrClf = LogisticRegression(class_weight = \"balanced\",solver = \"saga\",C = 0.00005)  #modello\n\n# Model fit\nlrClf.fit(X, y)","8c2d71d6":"# Final prediction \ny_scorel = lrClf.predict_proba(T)[:,1]\n\n# Create test dictionary \ntest = {'author': a_test,\n        'gender': y_scorel\n        }\n\n# Create DataFrame\ndf = pd.DataFrame(test, columns = ['author', 'gender'])\n\n\n# Create submission csv file\ndf.to_csv(r'..\/working\/Submission.csv', index = False)","49336942":"## Validation Data Visualization ","3fdac862":"## Data Collection ","8a5e40a5":"## Final Prediction & Submission","3109928f":"## Model Definition & Training ","9563392e":"## Validation Data Manipulation","ba87ad63":"# Data Mining Challange\n\nThe full description of the challange and its solution can be found in this [Github page](https:\/\/inphyt.github.io\/DataMiningChallange\/), while all the relevant notebooks are publicly available in the associated [Github repository](https:\/\/github.com\/InPhyT\/DataMiningChallange).\n\n## Modules"}}