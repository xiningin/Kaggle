{"cell_type":{"7f43fa1d":"code","4a4cb2e2":"code","dc1df015":"code","0b35cb5c":"code","8d9b3727":"code","6780cf18":"code","2252cb98":"code","00796ea9":"code","830b4319":"code","49d03609":"code","8ed02068":"code","939ac223":"code","ec8ae425":"code","aa18912e":"code","2854265e":"code","8c5e8348":"code","e36feb46":"code","12fa72bf":"code","a05c3ea1":"code","9079a126":"code","71624777":"code","17abd5cc":"code","addcbab5":"code","5ab31b5f":"code","d9f913e1":"code","8b2b7862":"code","f643fc79":"code","1c6884d8":"code","f03ed90a":"code","bf47ff18":"code","58389562":"code","7c1e5e80":"code","4b5724bd":"code","c07483be":"code","cf90c035":"code","4becc215":"code","a393a9b0":"code","9faa9e1c":"code","db557fd7":"code","b687de66":"code","b5716bea":"code","bbccd379":"code","9ae47983":"code","ff73c5a5":"code","94cf9909":"code","6e5ae65e":"code","c2143f21":"code","5f437e50":"code","42bb7ecf":"code","ebd818b2":"markdown","03cac91a":"markdown","7d81ec24":"markdown","16b4fbb8":"markdown","e797e56c":"markdown","6aeb77fa":"markdown","eee6115f":"markdown","b9e9b5e3":"markdown","0d477ace":"markdown","9839fc9d":"markdown","ee5dfd33":"markdown","ea249f3b":"markdown","83c5bdd6":"markdown","e443c12a":"markdown","e8a34cb4":"markdown","a556f623":"markdown"},"source":{"7f43fa1d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4a4cb2e2":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","dc1df015":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","0b35cb5c":"df.head()","8d9b3727":"df.info()","6780cf18":"df.describe().transpose()","2252cb98":"sns.countplot(x='quality',data=df)","00796ea9":"df['quality'].value_counts()","830b4319":"sns.heatmap(df.corr(),vmin=-1,cmap='viridis')","49d03609":"df.corr()[abs(df.corr()['quality']) > 0.25]['quality'] ","8ed02068":"df['binary quality'] = df['quality'].apply(lambda x: 1 if x>=6 else 0)\ndf.head()","939ac223":"sns.histplot(x='fixed acidity',data=df)","ec8ae425":"sns.lmplot(x='density',y='residual sugar',data=df,hue='binary quality')","aa18912e":"sns.histplot(x='alcohol',data=df,hue='binary quality')","2854265e":"sns.jointplot(x='free sulfur dioxide',y='total sulfur dioxide',data=df,hue='binary quality')","8c5e8348":"by_quality = df.groupby('quality')\nby_quality.mean()","e36feb46":"fig, axes = plt.subplots(2, 2, figsize=(18, 10))\nfig.suptitle('Groupby quality mean')\n\naxes[0,0].plot(by_quality.mean().index.values,\n        by_quality.mean()['total sulfur dioxide'].values,\n         color='r',linestyle='dashed', marker='o',\n         markersize=8)\naxes[0,0].title.set_text('total sulfur dioxide VS quality')\n\naxes[0,1].plot(by_quality.mean().index.values,\n        by_quality.mean()['alcohol'].values,\n         color='k',linestyle='dashed', marker='o',\n         markersize=8)\naxes[0,1].title.set_text('alcohol VS quality')\n\naxes[1,0].plot(by_quality.mean().index.values,\n        by_quality.mean()['volatile acidity'].values,\n         color='y',linestyle='dashed', marker='o',\n         markersize=8)\naxes[1,0].title.set_text('volatile acidity VS quality')\n\naxes[1,1].plot(by_quality.mean().index.values,\n        by_quality.mean()['sulphates'].values,\n         color='b',linestyle='dashed', marker='o',\n         markersize=8)\naxes[1,1].title.set_text('sulphates VS quality')","12fa72bf":"from sklearn.model_selection import train_test_split","a05c3ea1":"train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)","9079a126":"df.columns","71624777":"input_cols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\ntarget_col = 'binary quality'","17abd5cc":"\ntrain_inputs = train_df[input_cols].copy()\ntrain_targets = train_df[target_col].copy()\n\nval_inputs = val_df[input_cols].copy()\nval_targets = val_df[target_col].copy()\n\ntest_inputs = test_df[input_cols].copy()\ntest_targets = test_df[target_col].copy()\n","addcbab5":"from sklearn.preprocessing import MinMaxScaler","5ab31b5f":"scaler = MinMaxScaler().fit(df[input_cols])","d9f913e1":"train_inputs[input_cols] = scaler.transform(train_inputs[input_cols])\nval_inputs[input_cols] = scaler.transform(val_inputs[input_cols])\ntest_inputs[input_cols] = scaler.transform(test_inputs[input_cols])","8b2b7862":"from sklearn.linear_model import LogisticRegression","f643fc79":"logmodel = LogisticRegression(penalty='l1',solver='liblinear',max_iter=180 )","1c6884d8":"logmodel.fit(train_inputs,train_targets)","f03ed90a":"logmodel.score(train_inputs,train_targets)","bf47ff18":"logmodel.score(val_inputs,val_targets)","58389562":"list_of_reg_optm = [{'penalty':'l2', 'train_score': 0.7382690302398331, 'solver': 'lbfgs' , 'val_score':0.753125},\n{'penalty':'l1', 'train_score': 0.7476538060479666, 'solver': 'liblinear' , 'val_score':0.775},\n{'penalty':'elasticnet', 'train_score': 0.7382690302398331, 'solver': 'saga' , 'val_score':0.759375, 'l1_ratio':0.5},\n{'penalty':'elasticnet', 'train_score': 0.7393117831074035, 'solver': 'saga' , 'val_score':0.771875, 'l1_ratio':0.7},\n{'penalty':'elasticnet', 'train_score': 0.7413972888425443, 'solver': 'saga' , 'val_score':0.771875, 'l1_ratio':0.8},\n{'penalty':'elasticnet', 'train_score': 0.7434827945776851, 'solver': 'saga' , 'val_score':0.775, 'l1_ratio':0.9},\n{'penalty':'l1', 'train_score': 0.7434827945776851, 'solver': 'saga' , 'val_score':0.775},\n{'penalty':'l2', 'train_score': 0.7372262773722628, 'solver': 'newton-cg' , 'val_score':0.753125}  \n]\n","7c1e5e80":"var_optm = pd.DataFrame(list_of_reg_optm)\nvar_optm","4b5724bd":"preds = logmodel.predict(test_inputs)","c07483be":"from sklearn.metrics import classification_report","cf90c035":"print(classification_report(test_targets,preds))","4becc215":"from sklearn.tree import DecisionTreeClassifier","a393a9b0":"tree = DecisionTreeClassifier(max_leaf_nodes=32,max_depth=7,max_features=None,random_state=42)","9faa9e1c":"tree.fit(train_inputs, train_targets)","db557fd7":"tree.score(train_inputs, train_targets)","b687de66":"tree.score(val_inputs, val_targets)","b5716bea":"from sklearn.tree import plot_tree, export_text","bbccd379":"plt.figure(figsize=(80,20))\nplot_tree(tree, feature_names=train_inputs.columns, max_depth=2, filled=True);","9ae47983":"%%time\nlst =[]\n\nfor x in range(1,21):\n    model = DecisionTreeClassifier(max_depth=x, random_state=42)\n    model.fit(train_inputs, train_targets)\n    train_acc = 1 - model.score(train_inputs, train_targets)\n    val_acc = 1 - model.score(val_inputs, val_targets)\n    lst.append({'Max Depth': x,'Training Error': train_acc, 'Validation Error': val_acc})","ff73c5a5":"error_df = pd.DataFrame(lst)\nerror_df.head()","94cf9909":"plt.figure()\nplt.plot(error_df['Max Depth'], error_df['Training Error'])\nplt.plot(error_df['Max Depth'], error_df['Validation Error'])\nplt.title('Training vs. Validation Error')\nplt.xticks(range(0,21, 2))\nplt.xlabel('Max. Depth')\nplt.ylabel('Prediction Error (1 - Accuracy)')\nplt.legend(['Training', 'Validation'])","6e5ae65e":"%%time\nlst =[]\nl3 = [50,75,100,125,150,175,200]\nfor y in l3:\n    model = DecisionTreeClassifier(max_depth=7,max_leaf_nodes=y, random_state=42)\n    model.fit(train_inputs, train_targets)\n    train_acc = 1 - model.score(train_inputs, train_targets)\n    val_acc = 1 - model.score(val_inputs, val_targets)\n    lst.append({'max leaf nodes': y,'Training Error': train_acc, 'Validation Error': val_acc})","c2143f21":"error_df = pd.DataFrame(lst)\nerror_df.head()","5f437e50":"plt.figure()\nplt.plot(error_df['max leaf nodes'], error_df['Training Error'])\nplt.plot(error_df['max leaf nodes'], error_df['Validation Error'])\nplt.title('Training vs. Validation Error')\nplt.xticks(l3)\nplt.xlabel('max leaf nodes')\nplt.ylabel('Prediction Error (1 - Accuracy)')\nplt.legend(['Training', 'Validation'])","42bb7ecf":"predictions = tree.predict(test_inputs)\n\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nprint(classification_report(test_targets,predictions))\nprint(confusion_matrix(test_targets,predictions))","ebd818b2":"* majority has the fixed acidity of 7-8 pH","03cac91a":"## Future Work\n* try random forest\n* tune hyperparamerters for rfc\n\n","7d81ec24":"# RED WINE QUALITY","16b4fbb8":"solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n\n    Algorithm to use in the optimization problem.\n\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n      'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n      schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n","e797e56c":"# Logistic Regression","6aeb77fa":"### adding a new column 'binary quality'","eee6115f":"# Decision Tree Model","b9e9b5e3":"* higher the alcohol concenteration better the quality","0d477ace":"# Data Preprocessing","9839fc9d":"## Model's Predictions on test data","ee5dfd33":"## tuning optimiser, solver and penalty for Logistic Regression","ea249f3b":"### grouping by 'Quality' column","83c5bdd6":"# Exploratory Data Analysis","e443c12a":"### Model's Predictions on Test Data","e8a34cb4":"* lesser the total sulfur dioxide better the quality","a556f623":"penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n    Used to specify the norm used in the penalization. The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n"}}