{"cell_type":{"e6dbbfae":"code","19e3f37c":"code","b5eac4e9":"code","112a13ed":"code","4cff2aaa":"code","316f9274":"code","d2260cff":"code","77de683c":"code","f72f5dff":"code","002b180c":"code","996755b2":"code","df64cd40":"code","afaa5796":"code","049d6b58":"code","3b94fcc0":"code","82cead79":"code","d5aa7b74":"code","beb3215c":"code","e0eb6860":"code","17173b30":"code","19115216":"code","1457679c":"code","4831f02f":"code","b4306cf9":"code","3a51f878":"code","b630c07e":"code","64008ff7":"code","837394a3":"code","f2a5a937":"code","e9aa375c":"code","528f5712":"code","69fe52de":"code","3580d581":"code","c3f3e8d4":"code","29201ae8":"code","902ad982":"code","8ec70b44":"code","c6290446":"code","5f83bd62":"code","30b47641":"code","7656ba6c":"code","2820b239":"code","7e546555":"code","3e837d7b":"code","1b600145":"code","11fd9658":"code","4fe240ee":"code","b7b9893e":"code","c68140ba":"code","548c984f":"code","d2e541fb":"code","b1bf38cf":"code","ddf6b316":"code","e66bbd70":"code","bf891b9c":"code","fcc3b8b1":"code","0ded71ae":"code","6710050e":"markdown","4ceb002d":"markdown","5c400f86":"markdown","2bda7c3c":"markdown","0e152407":"markdown","548b1b97":"markdown","7b2fb9a4":"markdown","843c4f48":"markdown","6912242d":"markdown","4a990dd3":"markdown","7cfd2e5d":"markdown","9b779018":"markdown","f161638a":"markdown","085fb400":"markdown","31a4e90f":"markdown","9f9c9288":"markdown","37f446d2":"markdown","62f9a6b4":"markdown","c3d562f1":"markdown","33b0e435":"markdown","f9e4b56a":"markdown"},"source":{"e6dbbfae":" # linear algebra\nimport numpy as np\n# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\n#data loading\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","19e3f37c":"#train and test set\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","b5eac4e9":"#overview\ntrain.head()","112a13ed":"#shape\nprint(\"Training set shape: {}\".format(train.shape))\nprint(\"Test set shape: {}\".format(test.shape))","4cff2aaa":"#distribution target\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(8, 6))\nsns.countplot(x=train.target)","316f9274":"# Nan-values\ntrain.info()\nprint(\"-\" * 40)\ntest.info()","d2260cff":"# operation tool\ncombine = [train, test]","77de683c":"#fill Nan-values with its own category\nfor dataset in combine:\n    dataset[\"keyword\"].fillna(\"Unknown\", inplace=True)\n    dataset[\"location\"].fillna(\"Unknown\", inplace=True)","f72f5dff":"#check\ntrain.info()\nprint(\"-\" * 40)\ntest.info()","002b180c":"# tools for text processing\nimport re\nimport string\nfrom nltk.corpus import stopwords","996755b2":"#merge for operational reasons\ndf = pd.concat([train,test])\nntrain = train.shape[0]\nntest = test.shape[0]","df64cd40":"# function to Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers\n# Special thanks to https:\/\/www.kaggle.com\/andreshg for this function. Check his kernel out - it's great!\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","afaa5796":"#define stopword collection\nstop_words = stopwords.words('english') + [\"u\", \"im\", \"ur\", \"dont\", \"doin\", \"ure\"]","049d6b58":"#final cleaning function\ndef preprocess_data(text):\n    # Clean puntuation, urls, and so on\n    text = clean_text(text)\n    # Remove stopwords\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text","3b94fcc0":"#create processed \"clean_txt\" feature\ndf[\"clean_txt\"] = df[\"text\"].apply(preprocess_data)","82cead79":"#comparison of original and processed text\ndf[[\"text\",\"clean_txt\"]].head().values","d5aa7b74":"#Length of each message\ndf[\"Message_Length\"] = df.text.apply(lambda x: len(x))\n\nplt.figure(figsize=(8, 6))\ntrain.text.loc[train.target==1].apply(lambda x: len(x)).plot(bins=35, kind='hist', color='red', label='disaster', alpha=0.4)\ntrain.text.loc[train.target==0].apply(lambda x: len(x)).plot(bins=35, kind='hist', color='blue', label='no disaster', alpha=0.3)\nplt.legend()\nplt.xlabel(\"Message Length\")","beb3215c":"# number of words in each message\ndf[\"Word_Count\"] = df.text.str.split().apply(lambda x: len(x))\n\nplt.figure(figsize=(8, 6))\ntrain.text.loc[train.target==1].str.split().apply(lambda x: len(x)).plot(bins=25, kind='hist', color='red', label='disaster', alpha=0.4)\ntrain.text.loc[train.target==0].str.split().apply(lambda x: len(x)).plot(bins=25, kind='hist', color='blue', label='no disaster', alpha=0.2)\nplt.legend()\nplt.xlabel(\"Word Count\")","e0eb6860":"# average word length\ndf[\"Avg_Word\"] = df.text.str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x))\n\nplt.figure(figsize=(8, 6))\ntrain.text.loc[train.target==1].str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x)).plot(bins=25, kind='hist', color='red', label='disaster', alpha=0.4)\ntrain.text.loc[train.target==0].str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x)).plot(bins=25, kind='hist', color='blue', label='no disaster', alpha=0.2)\nplt.legend()\nplt.xlabel(\"Average Word Length\")","17173b30":"# unique words in each message\ndf[\"Unique_Words\"] = df.text.str.split().apply(lambda x: len(set(str(x).split())))\nplt.figure(figsize=(8, 6))\ntrain.text.loc[train.target==1].str.split().apply(lambda x: len(set(str(x).split()))).plot(bins=25, kind='hist', color='red', label='disaster', alpha=0.4)\ntrain.text.loc[train.target==0].str.split().apply(lambda x: len(set(str(x).split()))).plot(bins=25, kind='hist', color='blue', label='no disaster', alpha=0.2)\nplt.legend()\nplt.xlabel(\"Unique Words\")","19115216":"#number of stopwords in each message\nfrom wordcloud import STOPWORDS\ndf[\"Stopwords\"] = df.text.apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\nplt.figure(figsize=(8, 6))\ntrain.text.loc[train.target==1].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS])).plot(bins=20, kind='hist', color='red', label='disaster', alpha=0.4)\ntrain.text.loc[train.target==0].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS])).plot(bins=20, kind='hist', color='blue', label='no disaster', alpha=0.2)\nplt.legend()\nplt.xlabel(\"Count Stopwords\")","1457679c":"# number \"#\"'s\ndf[\"#_Count\"] = df.text.apply(lambda x: len([h for h in str(x) if h == \"#\"]))\n\nplt.figure(figsize=(8, 6))\ntrain.text.loc[train.target==1].apply(lambda x: len([h for h in str(x) if h == \"#\"])).plot(bins=20, kind='hist', color='red', label='disaster', alpha=0.4)\ntrain.text.loc[train.target==0].apply(lambda x: len([h for h in str(x) if h == \"#\"])).plot(bins=20, kind='hist', color='blue', label='no disaster', alpha=0.2)\nplt.legend()\nplt.xlabel(\"#-Count\")","4831f02f":"# number \"@\"'s\ndf[\"@_Count\"] = df.text.apply(lambda x: len([h for h in str(x) if h == \"@\"]))\n\nplt.figure(figsize=(8, 6))\ntrain.text.loc[train.target==1].apply(lambda x: len([h for h in str(x) if h == \"@\"])).plot(bins=35, kind='hist', color='red', label='disaster', alpha=0.4)\ntrain.text.loc[train.target==0].apply(lambda x: len([h for h in str(x) if h == \"@\"])).plot(bins=35, kind='hist', color='blue', label='no disaster', alpha=0.2)\nplt.legend()\nplt.xlabel(\"@-Count\")","b4306cf9":"#seperate to train and test set again\ntrain = df[:ntrain]\ntest = df[ntrain:]","3a51f878":"#function to process each text seperatly \ndef create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['clean_txt'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","b630c07e":"# make a list of each word and how often it occurs for non-disaster tweets\nfrom collections import defaultdict\n\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","64008ff7":"#plot\nplt.figure(figsize=(8, 6))\nx,y=zip(*top)\nplt.bar(x,y, color=\"blue\")\nplt.title(label=\"Most common words non-disaster Tweets\")","837394a3":"# make a list of each word and how often it occurs for disaster tweets\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","f2a5a937":"#plot\nplt.figure(figsize=(8, 6))\nx,y=zip(*top)\nplt.bar(x,y, color=\"red\", )\nplt.title(label=\"Most common words disaster Tweets\")","e9aa375c":"# assign target\ntarget = train[\"target\"]","528f5712":"# convert text into token counts matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(stop_words='english',ngram_range=(1, 2),max_df=0.5,min_df=2)\nX_train_vec = vect.fit_transform(train[\"clean_txt\"])\nX_test_vec = vect.transform(test[\"clean_txt\"])","69fe52de":"# normalize token counts matrix\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit_transform(X_train_vec)\ntfidf_transformer.fit_transform(X_test_vec)","3580d581":"# 10-times cross validation\nfrom sklearn.model_selection import StratifiedKFold\nrandom_state = 2021\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)","c3f3e8d4":"# arrays for base model predictions\nnb_train_preds = np.zeros(len(train.index), )\nnb_test_preds = np.zeros(len(test.index), )\n\nsvc_train_preds = np.zeros(len(train.index), )\nsvc_test_preds = np.zeros(len(test.index), )\n\nlr_train_preds = np.zeros(len(train.index), )\nlr_test_preds = np.zeros(len(test.index), )","29201ae8":"# bag-of-words base model\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[\"clean_txt\"], target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n    \n    x_train = X_train_vec[train_index]\n    x_valid = X_train_vec[test_index]\n    \n    nb_model = MultinomialNB()\n    nb_model.fit(x_train, y_train)\n    \n    train_oof_preds = nb_model.predict(x_valid)\n    nb_test_preds = nb_model.predict(X_test_vec)\n    nb_train_preds[test_index] = train_oof_preds\n    print(\": Bag-of-Words NB - Score = {}\".format(f1_score(y_valid, train_oof_preds)))\n      \n    svc_model = LinearSVC(random_state=1, dual=False, max_iter=10000)\n    svc_model.fit(x_train, y_train)\n    \n    train_oof_preds = svc_model.predict(x_valid)\n    svc_test_preds = svc_model.predict(X_test_vec)\n    svc_train_preds[test_index] = train_oof_preds\n    print(\": Bag-of-Words SCV - Score = {}\".format(f1_score(y_valid, train_oof_preds))) \n    \n    lr_model = LogisticRegression()\n    lr_model.fit(x_train, y_train)\n    \n    train_oof_preds = lr_model.predict(x_valid)\n    lr_test_preds = lr_model.predict(X_test_vec)\n    lr_train_preds[test_index] = train_oof_preds\n    print(\": Bag-of-Words LR - Score = {}\".format(f1_score(y_valid, train_oof_preds))) \n    \nprint(\"--> Overall metrics\")\nprint(\": Bag-of-Words NB = {}\".format(f1_score(target, nb_train_preds)))\nprint(\": Bag-of-Words SVC = {}\".format(f1_score(target, svc_train_preds)))\nprint(\": Bag-of-Words LR = {}\".format(f1_score(target, lr_train_preds)))","902ad982":"# arrays for stacked model predictions\nbag_train_preds = np.zeros(len(train.index), )\nbag_test_preds = np.zeros(len(test.index), )","8ec70b44":"# bag-of-words stacked model \nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\n\nl1_train = pd.DataFrame(data={\n    \"nb\": nb_train_preds.tolist(),\n    \"svc\": svc_train_preds.tolist(),\n    \"lr\": lr_train_preds.tolist(),\n    \"target\": target.tolist()\n})\nl1_test = pd.DataFrame(data={\n    \"nb\": nb_test_preds.tolist(),\n    \"svc\": svc_test_preds.tolist(),\n    \"lr\": lr_test_preds.tolist(),\n})\n\ntrain_preds = np.zeros(len(l1_train.index), )\ntest_preds = np.zeros(len(l1_test.index), )\nfeatures = [\"nb\", \"svc\", \"lr\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = pd.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pd.DataFrame(l1_train[features].iloc[test_index])\n    \n    bag_model = XGBClassifier(nthread = -1, eval_metric=\"logloss\", use_label_encoder=False)\n    \n    bag_model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_valid, y_valid)], \n    verbose=0,\n    early_stopping_rounds=200\n    )\n\n    train_oof_preds = bag_model.predict(x_valid)\n    test_oof_preds = bag_model.predict(l1_test[features])\n    bag_train_preds[test_index] = train_oof_preds\n    bag_test_preds = test_oof_preds\n    print(\": Bag-of-Words Stacked Model Score = {}\".format(f1_score(y_valid, train_oof_preds)))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": Bag-of-Words Stacked Model Score = {}\".format(f1_score(target, bag_train_preds)))\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(target, bag_train_preds))","c6290446":"# base model importances of stacked model\nlist(zip(features, bag_model.feature_importances_))","5f83bd62":"#import embedding tool\nimport spacy\nnlp = spacy.load('en_core_web_lg')","30b47641":"#embedding process\nwith nlp.disable_pipes():\n    X_train_embedding = np.array([nlp(text).vector for text in train[\"clean_txt\"]])\n    X_test_embedding = np.array([nlp(text).vector for text in test[\"clean_txt\"]])","7656ba6c":"# arrays for base model predictions\nnb_train_preds = np.zeros(len(train.index), )\nnb_test_preds = np.zeros(len(test.index), )\n\nsvc_train_preds = np.zeros(len(train.index), )\nsvc_test_preds = np.zeros(len(test.index), )\n\nlr_train_preds = np.zeros(len(train.index), )\nlr_test_preds = np.zeros(len(test.index), )\n","2820b239":"# embedding base model \nfrom sklearn.ensemble import RandomForestClassifier\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[\"clean_txt\"], target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n    \n    x_train = X_train_embedding[train_index]\n    x_valid = X_train_embedding[test_index]\n    \n    nb_model = RandomForestClassifier()\n    nb_model.fit(x_train, y_train)\n    \n    train_oof_preds = nb_model.predict(x_valid)\n    nb_test_preds = nb_model.predict(X_test_embedding)\n    nb_train_preds[test_index] = train_oof_preds\n    print(\": Embedding NB - Score = {}\".format(f1_score(y_valid, train_oof_preds)))\n    \n    svc_model = LinearSVC(random_state=1, dual=False, max_iter=10000)\n    svc_model.fit(x_train, y_train)\n    \n    train_oof_preds = svc_model.predict(x_valid)\n    svc_test_preds = svc_model.predict(X_test_embedding)\n    svc_train_preds[test_index] = train_oof_preds\n    print(\": Embedding SVC - Score = {}\".format(f1_score(y_valid, train_oof_preds)))\n    \n    lr_model = LogisticRegression(max_iter=400)\n    lr_model.fit(x_train, y_train)\n    \n    train_oof_preds = lr_model.predict(x_valid)\n    lr_test_preds = lr_model.predict(X_test_embedding)\n    lr_train_preds[test_index] = train_oof_preds\n    print(\": Embedding LR - Score = {}\".format(f1_score(y_valid, train_oof_preds)))\n    \nprint(\"--> Overall metrics\")\nprint(\": Embedding NB Score = {}\".format(f1_score(target, nb_train_preds)))\nprint(\": Enbedding SVC Score = {}\".format(f1_score(target, svc_train_preds)))\nprint(\": Embedding LR Score = {}\".format(f1_score(target, lr_train_preds)))","7e546555":"# arrays for stacked model\nemb_train_preds = np.zeros(len(train.index), )\nemb_test_preds = np.zeros(len(test.index), )","3e837d7b":"# embedding stacked model\nl1_train = pd.DataFrame(data={\n    \"nb\": nb_train_preds.tolist(),\n    \"svc\": svc_train_preds.tolist(),\n    \"lr\": lr_train_preds.tolist(),\n    \"target\": target.tolist()\n})\nl1_test = pd.DataFrame(data={\n    \"nb\": nb_test_preds.tolist(),\n    \"svc\": svc_test_preds.tolist(),\n    \"lr\": lr_test_preds.tolist(),\n})\n\ntrain_preds = np.zeros(len(l1_train.index), )\ntest_preds = np.zeros(len(l1_test.index), )\nfeatures = [\"nb\", \"svc\", \"lr\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = pd.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pd.DataFrame(l1_train[features].iloc[test_index])\n    \n    emb_model = XGBClassifier(nthread = -1, eval_metric=\"logloss\", use_label_encoder=False)\n    \n    emb_model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_valid, y_valid)], \n    verbose=0,\n    early_stopping_rounds=200\n    )\n\n    train_oof_preds = bag_model.predict(x_valid)\n    test_oof_preds = bag_model.predict(l1_test[features])\n    emb_train_preds[test_index] = train_oof_preds\n    emb_test_preds = test_oof_preds\n    print(\": Embedding Stacked Model Score = {}\".format(f1_score(y_valid, train_oof_preds)))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\":  Embedding Stacked Model Score = {}\".format(f1_score(target, emb_train_preds)))\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(target, emb_train_preds))","1b600145":"# base model importances of stacked model\nlist(zip(features, emb_model.feature_importances_))","11fd9658":"#unique values of \"keyword\" and \"location\" feature\nprint(\"Unique 'keyword' entries: {}\".format(len(df.keyword.unique())))\nprint(\"-\"*30)\nprint(\"Unique 'location' entries: {}\".format(len(df.location.unique())))","4fe240ee":"#trim \"location\" and \"keyword\" to first 3 letters\ndf[\"keyword\"] = df.keyword.apply(lambda x: str(x)[:3])\ndf[\"location\"] = df.keyword.apply(lambda x: str(x)[:3])","b7b9893e":"#check unique values, seems trimmed now\nprint(\"Unique 'keyword' entries: {}\".format(len(df.keyword.unique())))\nprint(\"-\"*30)\nprint(\"Unique 'location' entries: {}\".format(len(df.location.unique())))","c68140ba":"#seperate again\ntrain = df[:ntrain]\ntest = df[ntrain:]","548c984f":"#choose meta features\ntrain = train[[\"keyword\",\"location\",\"Message_Length\", \"Word_Count\",\"Avg_Word\", \"Unique_Words\", \"Stopwords\", \"#_Count\", \"@_Count\"]]\ntest = test[[\"keyword\",\"location\",\"Message_Length\", \"Word_Count\",\"Avg_Word\", \"Unique_Words\", \"Stopwords\", \"#_Count\", \"@_Count\"]]","d2e541fb":"#dummies\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)","b1bf38cf":"# arrays for base model prediction\nrf_train_preds = np.zeros(len(train.index), )\nrf_test_preds = np.zeros(len(test.index), )","ddf6b316":"# base meta features model\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n    \n    x_train = pd.DataFrame(train.iloc[train_index])\n    x_valid = pd.DataFrame(train.iloc[test_index])\n    \n    rf_model = RandomForestClassifier(n_estimators=500, n_jobs=-1, min_samples_leaf=3, min_samples_split=2)\n    \n    rf_model.fit(x_train,y_train,)\n    \n    train_oof_preds = rf_model.predict(x_valid)\n    rf_test_preds = rf_model.predict(test)\n    rf_train_preds[test_index] = train_oof_preds\n    print(\": META - Score = {}\".format(f1_score(y_valid, train_oof_preds))) \n    \nprint(\"--> Overall metrics\")\nprint(\": META Score = {}\".format(f1_score(target, rf_train_preds)))","e66bbd70":"# arrays for final stacked model prediction\ntrain_preds = np.zeros(len(train.index), )\ntest_preds = np.zeros(len(test.index), )","bf891b9c":"# final stacked model\nfrom xgboost import XGBClassifier\n\nl1_train = pd.DataFrame(data={\n    \"meta\": rf_train_preds.tolist(),\n    \"embedding\": emb_train_preds.tolist(),\n    \"bag\": bag_train_preds.tolist(),\n    \"target\": target.tolist()\n})\nl1_test = pd.DataFrame(data={\n    \"meta\": rf_test_preds.tolist(),\n    \"embedding\": emb_test_preds.tolist(),\n    \"bag\": bag_test_preds.tolist(),\n})\n\ntrain_preds = np.zeros(len(l1_train.index), )\ntest_preds = np.zeros(len(l1_test.index), )\nfeatures = [\"meta\", \"embedding\", \"bag\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = pd.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pd.DataFrame(l1_train[features].iloc[test_index])\n    \n    model = XGBClassifier(nthread = -1, eval_metric=\"logloss\", use_label_encoder=False, max_depth = 7)\n\n    model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_valid, y_valid)], \n    verbose=0,\n    early_stopping_rounds=400\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    test_oof_preds = model.predict(l1_test[features])\n    train_preds[test_index] = train_oof_preds\n    test_preds = test_oof_preds\n    print(\": Final Stacked Model Score = {}\".format(f1_score(y_valid, train_oof_preds)))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": Final Stacked Model Score = {}\".format(f1_score(target, train_preds)))\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(target, train_preds))","fcc3b8b1":"# base model importances of stacked model\nlist(zip(features, model.feature_importances_))","0ded71ae":"#submission\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission[\"target\"] = test_preds.tolist()\nsubmission.to_csv(\"submission.csv\", index=False)","6710050e":"Spacy is transforming each word into a token. Afterwards we embed all tokens into vectors via \"Word2Vec\". ","4ceb002d":"We have a slight majority of \"non-disaster\"-Tweets. We should keep that in mind for our EDA.","5c400f86":"# 4.1 Bag-of-Words Model\ud83d\udc5c","2bda7c3c":"At our data exploration we have already built the majority of our meta features. Now we transform the \"location\" and \"keyword\" features and create \"dummie\" variables.","0e152407":"# Natural Language Processing with Disaster Tweets\n\n![image.png](attachment:3957ecf9-103b-498d-bfae-45f8a2d51e23.png)\n\nIn this notebook we are going to predict whether a Twitter tweet is about a real occuring disaster or it isn't. For further explanation check out the Kaggle Competition Description. https:\/\/www.kaggle.com\/c\/nlp-getting-started \n\n**Best results: 80.05% (f1 score)**\n\n# Agenda\ud83d\udcc3:\n1. First Look\n2. Data Cleaning and Text Preprocessing\n3. Data Exploration\n4. Base Model\n* 4.1 Bag of Words Model\n* 4.2 Embedding Model\n* 4.3 Meta Features Model\n5. Stacked Model\n6. Submission\n","548b1b97":"For our \"Bag-of-Words\" model we use Countvectorizer to encode each text into a vector. An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document. For further information check out: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html","7b2fb9a4":"When it comes to the EDA we got really interesting insights. Especially the word appreance section seems the most interesting. When it comes to our newly built meta features \"Average Word Lenght\", \"@-count\" and \"#-count\" seems the most valuable as disaster Tweets have higher average word length, more \"#\"'s and fewer \"@\"'s. My guess is that we have a lot of news Tweets with a more formal lingo.","843c4f48":"In this kernel I tried to improve my stacking skills. That being said we are going to build a stacked \"Bag-of-Words\"-model, a stacked \"Embedding-model\" and a simple RandomForest model based on meta features. We stack those again into one final Level-2 stacked model.","6912242d":"When it comes to word appearance we use our \"cleaned\" Tweets.","4a990dd3":"Finally we have made it to our final level-2 stacked model. XGBClassifier we do the job for this one. Just a quick reminder, In this model we are stacking:\n* Bag-of-Words model\n* Embedding model\n* Meta Features model","7cfd2e5d":"# 4.3 Meta Features Model\ud83d\udd27","9b779018":"# 6. Submission\ud83d\udd11","f161638a":"Our Data doesn't seem too complicated. With just a hand full of features we mainly will focus on the text processing and evaluation part.","085fb400":"# 1. First Look\ud83d\udd0e","31a4e90f":"# 4.2 Word Embedding Model\ud83e\udd32\ud83c\udffb","9f9c9288":"First we want to clear all \"nan\"-values. Afterwards we try to simplify our Tweets (\"text\"-feature) the best way possible.","37f446d2":"# 3. Data Exploration\ud83e\udded","62f9a6b4":"# 5. Final Stacked Model\u2699\ufe0f","c3d562f1":"Arriving at the EDA part of our kernel we try to find diffrences in the text structure of our tweets. We also will use these insights as new meta features for one of our models.\nWe will investigate:\n* length of each message\n* number of words in each message\n* average word length\n* unique words in each message\n* number of stopwords in each message\n* number of \"#\"'s\n* number of \"@\"'s","33b0e435":"# 2. Data Cleaning and Text Preprocessing\ud83e\uddf9","f9e4b56a":"# 4. Base Model\ud83d\udd29"}}