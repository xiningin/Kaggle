{"cell_type":{"fdd65b58":"code","051fe549":"code","cf908da9":"code","293ad293":"code","923b23cf":"code","cc904a2e":"code","f8068201":"code","4c9b5a8b":"code","77d7dc23":"code","292acbcd":"code","04b4c26b":"code","8162f0a6":"code","a09a4225":"code","6f4f826e":"code","ca607bb2":"code","30b86a92":"code","f3ef2d48":"code","9fbba0b3":"code","b0f1ce5f":"code","26471341":"code","8849a4d4":"code","84a6e9c9":"code","fd461082":"code","d6788bea":"code","054e015a":"code","df28cb72":"code","a8e06815":"code","3510ed60":"markdown","a399f2dc":"markdown","d46e2ec7":"markdown","4c692dee":"markdown","480e5cb2":"markdown","0243fc08":"markdown","4ff7f02d":"markdown"},"source":{"fdd65b58":"%matplotlib inline\nimport pandas as pd\nfrom fastai.tabular import *\nimport fastai \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","051fe549":"# check version\nfastai.__version__","cf908da9":"# Feture importance extraction from NN weights\ndef feature_importance(learner): \n  # based on: https:\/\/medium.com\/@mp.music93\/neural-networks-feature-importance-with-fastai-5c393cf65815\n    data = learner.data.train_ds.x\n    cat_names = data.cat_names\n    cont_names = data.cont_names\n    loss0=np.array([learner.loss_func(learner.pred_batch(batch=(x,y.to(\"cpu\"))), y.to(\"cpu\")) for x,y in iter(learner.data.valid_dl)]).mean()\n    fi=dict()\n    types=[cat_names, cont_names]\n    for j, t in enumerate(types):\n      for i, c in enumerate(t):\n        loss=[]\n        for x,y in iter(learner.data.valid_dl):\n          col=x[j][:,i] \n          idx = torch.randperm(col.nelement())\n          x[j][:,i] = col.view(-1)[idx].view(col.size())\n          y=y.to('cpu')\n          loss.append(learner.loss_func(learner.pred_batch(batch=(x,y)), y))\n        fi[c]=np.array(loss).mean()-loss0\n    d = sorted(fi.items(), key=lambda kv: kv[1], reverse=True)\n    return pd.DataFrame({'cols': [l for l, v in d], 'imp': np.log1p([v for l, v in d])})","293ad293":"# load all datasets\ncol = [\"age\",\"class of worker\",\"detailed industry recode\",\"detailed occupation recode\",\"education\",\n       \"wage per hour\",\"enroll in edu inst last wk\",\"marital status\",\"major industry code\",\n       \"major occupation code\",\"race\",\"hispanic origin\",\"sex\",\"member of a labor union\",\n       \"reason for unemployment\",\"full or part time employment stat\",\"capital gains\",\"capital losses\",\n       \"dividends from stocks\",\"tax filer stat\",\"region of previous residence\",\"state of previous residence\",\n       \"detailed household and family stat\",\"detailed household summary in household\",\"instance weight\",\n       \"migration code-change in msa\",\"migration code-change in reg\",\"migration code-move within reg\",\n       \"live in this house 1 year ago\",\"migration prev res in sunbelt\",\"num persons worked for employer\",\n       \"family members under 18\",\"country of birth father\",\"country of birth mother\",\"country of birth self\",\n       \"citizenship\",\"own business or self employed\",\"fill inc questionnaire for veteran\\'s admin\",\n       \"veterans benefits\",\"weeks worked in year\",\"year\",\"income class\"]\n\ndf = pd.read_csv(\"..\/input\/ml-challenge-week6\/census-income.data\", names=col, header=None)\nprint(\"Shape of Train dataframe is: {}\".format(df.shape))\nprint('NaN in Train:',df.isnull().values.any())\ntest = pd.read_csv(\"..\/input\/ml-challenge-week6\/census-income.test\", names=col[0:-1], header=None)\nprint(\"Shape of Test dataframe is: {}\".format(test.shape))\nprint('NaN in Test:',test.isnull().values.any())\nsub = pd.read_csv(\"..\/input\/ml-challenge-week6\/sampleSubmission.csv\")\nzub = sub['index']","923b23cf":"# build inventory of categorical & continuous features\ncat = ['class of worker', 'education', 'marital status', 'major industry code',\n       'major occupation code', 'race','sex', 'reason for unemployment',\n       'detailed household and family stat', 'detailed household summary in household']\n\ncont = ['age', 'detailed occupation recode', 'capital gains', 'capital losses',\n        'dividends from stocks', 'num persons worked for employer', \n        'veterans benefits', 'weeks worked in year'] # removed \"instance weight\"\n\n# align features with test dataset...\nfor i in cat:\n    df[i] = df[i].str[1:]    \n\n","cc904a2e":"# align test target\ntest['income class'] = ' - 50000.'\n# NaN corrections in test dataset\ntest[\"hispanic origin\"] = test[\"hispanic origin\"].fillna('NA')\ntest[\"state of previous residence\"] = test[\"state of previous residence\"].fillna('?')\ntest['migration code-change in msa'] = test['migration code-change in msa'].fillna('?')\ntest['migration code-change in reg'] = test['migration code-change in reg'].fillna('?')\ntest['migration code-move within reg'] = test['migration code-move within reg'].fillna('?')\ntest['migration prev res in sunbelt'] = test['migration prev res in sunbelt'].fillna('?')\ntest['country of birth father'] = test['country of birth father'].fillna('?')\ntest['country of birth mother'] = test['country of birth mother'].fillna('?')\ntest['country of birth self'] = test['country of birth self'].fillna('?')","f8068201":"# merge train & test dataset\ndf = df.append(test, ignore_index = True)","4c9b5a8b":"# transform target in category type\ndf = df.join(pd.get_dummies(df['income class']))","77d7dc23":"# parameters for NN model\ndep_var =  ' 50000+.'\nprocs = [FillMissing, Categorify, Normalize]","292acbcd":"# check % positive values in train set\ndf[dep_var].value_counts()[1]\/199523","04b4c26b":"# split by index\nidx_test = df.iloc[199523:].index # last N rows\nidx_val  = df.iloc[159619:199522].index # last 20% of train rows\nidx_val, idx_test","8162f0a6":"# check % positive values in validation set\ndf.loc[idx_val, dep_var].value_counts()[1]\/(199522-159619)","a09a4225":"# prepare databunch ingestion of test set\ntest = TabularList.from_df(df.loc[idx_test].copy(), path='', cat_names=cat, cont_names=cont)","6f4f826e":"# databunch ingestion of data sets\n\nBS = 64\n\ndata = (TabularList.from_df(df, path='', cat_names=cat, cont_names=cont, procs=procs)\n                           .split_by_idx(idx_val)\n                           .label_from_df(cols=dep_var)\n                           .add_test(test)\n                           .databunch())\n\ndata.batch_size = BS\n","ca607bb2":"# build NN learner and look at learning rate curve\nlearn = tabular_learner(data, layers=[200,100], metrics=[accuracy, AUROC()],callback_fns=ShowGraph)\n\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","30b86a92":"# Initial learning phase using suggested learning rate during 3 cycles\nlr = 1e-03\nlearn.fit_one_cycle(5, lr)","f3ef2d48":"# look again at learning rate curve\nlearn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","9fbba0b3":"# 2nd learning phase using suggested learning rate during 3 cycles\nlearn.fit_one_cycle(3,max_lr=1e-8)","b0f1ce5f":"# predict test classes...\nprobas_test, _ = learn.get_preds(ds_type=DatasetType.Test) # run inference on test\nprobas_test = probas_test[:, 1] # only get probability tensor","26471341":"# create submission dataframe\nsubmission_df = pd.DataFrame({'index':sub.index.values,dep_var:probas_test})\nsubmission_df.columns = ['index','income class']","8849a4d4":"# get tp driver\ntp = range(0,40000,100) \ny = []\nfor x in tp:\n    y.append(0.6432042 + 0.00002635951*x - 9.49824e-10*x**2 + 9.629459e-15*x**3) # from my own analysis...\n    \nplt.plot(tp,y);","84a6e9c9":"print('Best amount of true classes should be', np.argmax(y)*100,'with expected AUC arround',y[np.argmax(y)])","fd461082":"# should have more positive classes, let's align...\npivot = .2\nwhile len(submission_df[submission_df['income class']>=pivot])< 19100: #(np.argmax(y)*100):\n    pivot = pivot-.000001\ncorrection = .5 - pivot\nprint('Pivot is',pivot,'- tensor correction is', correction)","d6788bea":"# apply correction, classify 0\/1 and make it int\nsubmission_df['income class'] = submission_df['income class'] + correction # tensor correction\nsubmission_df['income class'] = submission_df['income class'].apply(np.round)\nsubmission_df['income class'] = submission_df['income class'].astype(int)","054e015a":"# check results\nsubmission_df.describe()","df28cb72":"\nsubmission_df.to_csv('FastAI_v6_corrected.csv', index = False) #","a8e06815":"# Here are our NN feature importance \nfi = feature_importance(learn)\nfi[:20].plot.barh(x=\"cols\", y=\"imp\", figsize=(10, 10))","3510ed60":"\n# Part 1 : Data Ingestion & Basic Cleaning\n\nWe simply load data files, treat NaN and merge everything in a single dataframe. ","a399f2dc":"![image.png](attachment:image.png)\n\n# Part 3 : Train NN Model","d46e2ec7":"\n\n# Annexes  Feature Importance & Lesson's Learned","4c692dee":"\n# Part 4 : Inception & build submission file","480e5cb2":"\n\n\nAfter some experiments with more traditional DS techniques, fast.AI was selected for ease of use, flexibility and performances.\nDeep learning models are particularly useful for datasets with high cardinality categorical variables because they provide embeddings that can be used to train efficient models.\n\nMore info : https:\/\/www.mdpi.com\/2078-2489\/11\/2\/108\n\nKernel used is inspired by the tabular model described in lesson 4 of \"Practical Deep Learning for Coders\" course from Jeremy Howard.\nMore info : https:\/\/course.fast.ai\/videos\/?lesson=4","0243fc08":"# These improvements have been tested but don't give significant results:\n\n### Under\/Over sampling sampling to realign unbalanced class ratio.\n### Incestuous learning (re-use some predicted classes to re balance train set).\n### [5,8,10] Kfolds.\n### Optimizers : Flatten and Anneal - Ranger - Mish.\n### Custom metric (fbeta).\n\n\n","4ff7f02d":"# Part 2 : Split Train-Valid-Test using index\n\nSince we have a single dataframe, we split it like this...\n\n![image.png](attachment:image.png)"}}