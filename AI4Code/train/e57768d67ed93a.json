{"cell_type":{"128ba10c":"code","78892ca8":"code","1ca9bdfb":"code","0ed7fc97":"code","a4563af2":"code","284498f3":"code","553a97ca":"code","3ca38da5":"code","9eb14f3b":"code","2f0ab175":"code","bd4e73db":"code","c9b8bcf1":"markdown","18197707":"markdown","22d32bdc":"markdown","768687c6":"markdown","49653b0d":"markdown","c782608d":"markdown","f843fbb2":"markdown","4be6298f":"markdown","877456a6":"markdown","7900a522":"markdown","bb1da9d2":"markdown"},"source":{"128ba10c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nimport cv2\nimport skimage.io\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom PIL import Image\nimport keras.backend as K\nK.set_image_data_format('channels_last')\nK.set_learning_phase(1)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","78892ca8":"train = pd.read_csv(\"..\/input\/landmark-retrieval-2020\/train.csv\")\ndef get_paths(index_location):\n    index = os.listdir('..\/input\/landmark-retrieval-2020\/train\/')\n    paths = []\n    a=index_location\n    for b in index:\n        for c in index:\n            try:\n                paths.extend([f\"..\/input\/landmark-retrieval-2020\/train\/{a}\/{b}\/{c}\/\" + x for x in os.listdir(f\"..\/input\/landmark-retrieval-2020\/train\/{a}\/{b}\/{c}\")])\n            except:\n                pass\n\n    return paths\n\ndef show_sample(pathes):\n    plt.rcParams[\"axes.grid\"] = False\n    f, axarr = plt.subplots(3, 3, figsize=(20, 20))\n    axarr[0, 0].imshow(cv2.imread(pathes[0]))\n    axarr[0, 1].imshow(cv2.imread(pathes[1]))\n    axarr[0, 2].imshow(cv2.imread(pathes[2]))\n    axarr[1, 0].imshow(cv2.imread(pathes[3]))\n    axarr[1, 1].imshow(cv2.imread(pathes[4]))\n    axarr[1, 2].imshow(cv2.imread(pathes[5]))\n    axarr[2, 0].imshow(cv2.imread(pathes[6]))\n    axarr[2, 1].imshow(cv2.imread(pathes[7]))\n    axarr[2, 2].imshow(cv2.imread(pathes[8]))\n\nshow_sample(get_paths(2))\n#train.describe()","1ca9bdfb":"# Landmark ID distribution\n# from https:\/\/www.kaggle.com\/machinesandi\/google-landmark-retrieval-2020-extensive-eda\/data\nfig, axs = plt.subplots(ncols=3,figsize = (20, 5))\nplt.title('Category Distribuition')\nsns.kdeplot(train['landmark_id'], color=\"tomato\", shade=True, ax=axs[0])\n\n# Occurance of landmark_id in decreasing order(Top categories)\ntemp = pd.DataFrame(train.landmark_id.value_counts().head(10))\ntemp.reset_index(inplace=True)\ntemp.columns = ['landmark_id','count']\n\nplt.title('Most frequent landmarks')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"landmark_id\", y=\"count\", data=temp,label=\"Count\",ax=axs[1])\n\n# Occurance of landmark_id in increasing order(Top categories)\ntemp = pd.DataFrame(train.landmark_id.value_counts().tail(8))\ntemp.reset_index(inplace=True)\ntemp.columns = ['landmark_id','count']\n\nplt.title('Least frequent landmarks')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"landmark_id\", y=\"count\", data=temp, label=\"Count\",ax=axs[2])\nplt.show()","0ed7fc97":"!git clone https:\/\/github.com\/tensorflow\/models.git\n!bash models\/research\/delf\/delf\/python\/training\/install_delf.sh","a4563af2":"#In case the installation was not successful:\n!cp -r models\/research\/delf\/* .\/\n!protoc delf\/protos\/*.proto --python_out=.","284498f3":"!mkdir \/tmp\/data\n#!rm cleadn_data\/*\n#!ls -lh cleadn_data\n!python3 delf\/python\/training\/build_image_dataset.py \\\n  --train_csv_path=..\/input\/landmark-retrieval-2020\/train.csv \\\n  --train_clean_csv_path=..\/input\/train-clean-sample\/train_clean_sample.csv \\#..\/input\/cleaned-subsets-of-google-landmarks-v2\/GLDv2_train_cleaned.csv \\\n  --train_directory=..\/input\/landmark-retrieval-2020\/train\/*\/*\/*\/ \\\n  --output_directory=\/tmp\/data \\\n  --num_shards=12 \\\n  --generate_train_validation_splits \\\n  --validation_split_size=0.2","553a97ca":"!curl -Os http:\/\/storage.googleapis.com\/delf\/resnet50_imagenet_weights.tar.gz\n!tar -xzvf resnet50_imagenet_weights.tar.gz","3ca38da5":"# add the delf to the pythonpath\nos.environ['PYTHONPATH']+=':models\/research\/delf\/:delf:protoc'\n!cp -r delf\/protos\/*  models\/research\/delf\/delf\/protos\/\n","9eb14f3b":"# installing the object detection api, required by the delf model\n!pip install tensorflow-object-detection-api","2f0ab175":"#-- dont forget to increase the number of iterations, default is max_iters=500.000\n!cp ..\/input\/cleancode\/train2.py models\/research\/delf\/delf\/python\/training\/train2.py\n!python3 models\/research\/delf\/delf\/python\/training\/train2.py \\\n  --train_file_pattern=\/tmp\/data\/train* \\\n  --seed=1 \\\n  --max_iters=20000 \\\n  --validation_file_pattern=\/tmp\/data\/validation* \\\n  --imagenet_checkpoint=resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n  --dataset_version=gld_v2_clean \\\n  --logdir=gldv2_training\/","bd4e73db":"!python3 models\/research\/delf\/delf\/python\/training\/model\/export_global_model.py \\\n  --ckpt_path=gldv2_training\/delf_weights \\\n  --export_path=gldv2_model_global \\\n  --input_scales_list=0.70710677,1.0,1.4142135 \\\n  --multi_scale_pool_type=sum \\\n  --normalize_global_descriptor","c9b8bcf1":"# Running the Training\nhttps:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/delf <br>\nFor the training to converge faster, it is possible to initialize the ResNet backbone with the weights of a pretrained ImageNet model. The ImageNet checkpoint is available at the following location: http:\/\/storage.googleapis.com\/delf\/resnet50_imagenet_weights.tar.gz. To download and unpack it run the following commands on a Linux box:","18197707":"# Deep Local and Global Image Features\nIn this kernel I will use DELF for building a first baseline kerenl. DELF project presents code for deep local and global image feature methods, which are particularly useful for the computer vision tasks of instance-level recognition and retrieval. These were introduced in the DELF, Detect-to-Retrieve, DELG.  <br>\n**Acknowledgment:** In the following link, you can find the project source code, installation guidlines and pretrained models by **@andre faraujo**: <br>\nhttps:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/delf\n\n**Please upvote if you find this kernel useful**\n\n![https:\/\/www.google.com\/search?q=Deep+Local+and+Global+Image+Features&rlz=1C1GCEB_enDE900DE900&sxsrf=ALeKk01IRNDaUmMd87q0q7jM4VDHYj86Sg:1593688159103&source=lnms&tbm=isch&sa=X&ved=2ahUKEwieqeyEt67qAhUlxIUKHY6jCfwQ_AUoAXoECAwQAw&biw=1707&bih=838&dpr=1.13#imgrc=ZdwiuxwezpqU4M](attachment:image.png)","22d32bdc":"*********************************************************************************************************\n# Data Exploration\n*********************************************************************************************************\n\nThe label distribution shows clear label imbalance","768687c6":"What we need to do further is using the full clean subsubset\/ optimize it by augmenting the examples by excluding irrelevant positives in the training set, hyperparameterze optimizing, and use other feature extraction techniques\n*********************************************************************************************************\n# ****Please upvote if you find this kernel useful****\n*********************************************************************************************************","49653b0d":"Assuming the TFRecord files were generated in the gldv2_dataset\/tfrecord\/ directory, running the following command should start training a model and output the results in the gldv2_training directory:","c782608d":"# Additional resources\n* Baseline Format **@Cam Askew**: <br\/>\nhttps:\/\/www.kaggle.com\/camaskew\/baseline-submission\/execution\n\n* Previous winning solutions from **@Sanyam Bhutani**:<br\/>\n https:\/\/www.kaggle.com\/c\/landmark-retrieval-2020\/discussion\/163287\n \n* Submission Guidleines by **Andre Araujo** <br\/>\n https:\/\/www.kaggle.com\/c\/landmark-retrieval-2020\/discussion\/163350","f843fbb2":"# Testing the Trained Model\nyou can follow this link to test the generated model\nhttps:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/delf\/delf\/python\/training","4be6298f":"# Exporting the Trained Model in Kaggle Format for Submission","877456a6":"# Show one Example of landmark images\n\n\nTaken from the public kernel...thanks for sharing this. It was quite helpful\nhttps:\/\/www.kaggle.com\/derinformatiker\/landmark-retrieval-all-paths","7900a522":"*********************************************************************************************************\n# Now lets prepare the data and train the DELF model\n*********************************************************************************************************\n1. Install Prerequisites\nClone the TensorFlow Model Garden repository and move into the models\/research\/delf\/delf\/python\/trainingfolder:\nInstall TensorFlow 2.2 and TensorFlow 2.2 for GPU.\nInstall the TF-Slim library from source.\nDownload protoc and compile the DELF Protocol Buffers.\nInstall the matplotlib, numpy, scikit-image, scipy and python3-tk Python libraries.\nInstall the TensorFlow Object Detection API from the cloned TensorFlow Model Garden repository.\nInstall the DELF package.","bb1da9d2":"2. **Prepare the Data for Training**\nPreparing the data for training consists of creating TFRecord files from the raw GLDv2 images grouped into TRAIN and VALIDATION splits. The training set produced contains only the clean subset of the GLDv2 dataset. The CVPR'20 paper introducing the GLDv2 dataset contains a detailed description of the clean subset.\n\n***For testing purpose, I am using a subset of the cleaned data. In order to use the full dataset, use:\n--train_clean_csv_path=..\/input\/cleaned-subsets-of-google-landmarks-v2\/GLDv2_train_cleaned.csv** \\*\n  "}}