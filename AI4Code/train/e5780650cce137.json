{"cell_type":{"b5284b31":"code","d87fe6f4":"code","e3a8f42d":"code","00083902":"code","8ca81c1b":"code","ceb13d59":"code","ac569e0b":"code","74441451":"code","e709957b":"code","2d22e817":"code","c4d1c3be":"code","5bba4a88":"code","27ee41c7":"code","51fd81b1":"code","28a6e67b":"code","3c3d321a":"code","bdee7e9c":"code","3730ea8d":"code","e4c10609":"code","0a86e65c":"code","2409773a":"code","c8b80751":"code","0f375b7e":"code","dc05a073":"code","6c4eac8b":"code","ddc7175f":"code","271cc1aa":"code","0c2bd8ee":"markdown","1a35e60b":"markdown","68dbe3fd":"markdown","544d4cfa":"markdown","381cb68e":"markdown","a66b08ea":"markdown","b2ffc9bb":"markdown","c28073de":"markdown","0be4718c":"markdown","018fa223":"markdown","2e113234":"markdown","277a5f17":"markdown","6e43dc03":"markdown","e6e55f26":"markdown","77779900":"markdown","271cf1ad":"markdown","2c59cdc4":"markdown","943b5981":"markdown","bc3837b7":"markdown","803c4e2e":"markdown","c9ae6ff1":"markdown","bf1a1354":"markdown","5ad34aaf":"markdown","0d3193bc":"markdown","1de7c2a5":"markdown"},"source":{"b5284b31":"#Importing the basic librarires\n\nimport os\nimport math\nimport scipy\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import tree\nfrom scipy.stats import randint\nfrom scipy.stats import loguniform\nfrom IPython.display import display\n\nfrom scipy.sparse import csr_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.neighbors import NearestNeighbors\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, \\\nf1_score, roc_auc_score, roc_curve, precision_score, recall_score\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10,6]\n\nimport warnings \nwarnings.filterwarnings('ignore')","d87fe6f4":"#Importing the dataset\n\ndf1 = pd.read_csv('..\/input\/instacart-online-grocery-basket-analysis-dataset\/products.csv')\n#display(df1.head())\nprint(df1.shape)\ndf2 = pd.read_csv('..\/input\/instacart-online-grocery-basket-analysis-dataset\/aisles.csv')\n#display(df2.head())\nprint(df2.shape)\ndf3 = pd.read_csv('..\/input\/instacart-online-grocery-basket-analysis-dataset\/departments.csv')\n#display(df3.head())\nprint(df3.shape)\ndf4 = pd.read_csv('..\/input\/instacart-online-grocery-basket-analysis-dataset\/order_products__train.csv')\n#display(df4.head())\nprint(df4.shape)\ndf5 = pd.read_csv('..\/input\/instacart-online-grocery-basket-analysis-dataset\/orders.csv')\n#display(df5.head())\nprint(df5.shape)\n\ndf12 = pd.merge(df2, df1, how='inner', on='aisle_id')\ndf123 = pd.merge(df12, df3, how='inner', on='department_id')\ndf1234 = pd.merge(df4, df123, how='inner', on='product_id')\ndf12345 = pd.merge(df5, df1234, how='inner', on='order_id')\n\ndf = df12345.drop(['aisle_id','department_id','eval_set','order_number'],axis=1)\ndisplay(df.head())\nprint('\\n\\033[1mInference:\\033[0m The Datset consists of {} features & {} samples.'.format(df.shape[1], df.shape[0]))","e3a8f42d":"#Checking the dtypes of all the columns\n\nprint(df.info())","00083902":"#Checking number of unique rows in each feature\n\nprint(df.nunique().sort_values())","8ca81c1b":"#Checking the stats of all the columns\n\ndisplay(df.describe())","ceb13d59":"#Let us first analyze the department distribution\n\nsns.countplot(df.department, order=df.department.value_counts().index)\nplt.title('Online Shopping Department-Frequency')\nplt.xticks(rotation=90)\nplt.show()","ac569e0b":"#Let us now analyze the aisle distribution\n\nsns.countplot(df.aisle, order=df.aisle.value_counts().index[:20])\nplt.title('Online Shopping Aisle-Frequency')\nplt.xticks(rotation=90)\nplt.show()","74441451":"#Let us now analyze the products distribution\n\nsns.countplot(df.product_name, order=df.product_name.value_counts().index[:20])\nplt.title('Online Shopping Products-Frequency')\nplt.xticks(rotation=90)\nplt.show()","e709957b":"#Let us now analyze the hourly distribution\n\nsns.countplot(df.order_hour_of_day)#, order=df.aisle.value_counts().index[:20])\nplt.title('Online Shopping Horly-Frequency')\nplt.xticks(rotation=90)\nplt.show()","2d22e817":"#Check for empty elements\n\nprint(df.isnull().sum())\nprint('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any null elements')","c4d1c3be":"# Removal of any Duplicate rows (if any)\n    \ncounter = 0\nr,c = df.shape\n\ndf1 = df.drop_duplicates()\ndf1.reset_index(drop=True,inplace=True)\n\nif df1.shape==(r,c):\n    print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\nelse:\n    print(f'\\n\\033[1mInference:\\033[0m Number of duplicates dropped ---> {r-df.shape[0]}')\n    ","5bba4a88":"dummies_df = pd.get_dummies(data=df, prefix=['Day','Hour'], columns=['order_dow','order_hour_of_day'], drop_first=True)\ndummies_df.head()","27ee41c7":"# #Final Dataset size after performing Preprocessing\n\n# plt.title('Final Dataset Samples')\n# plt.pie([df1.shape[0], df1.shape[0]-df.shape[0]], radius = 1, shadow=True,\n#         labels=['Retained','Dropped'], counterclock=False, autopct='%1.1f%%', pctdistance=0.9, explode=[0,0])\n# plt.pie([df.shape[0]], labels=['100%'], labeldistance=-0, radius=0.78, shadow=True, colors=['powderblue'])\n# plt.show()\n\n# print('\\n\\033[1mInference:\\033[0mThe final dataset after cleanup has {} samples & {} rows.'.format(df.shape[0], df.shape[1]))","51fd81b1":"user_prod_df = dummies_df.groupby(['user_id','product_id']).agg({'order_id':'nunique',\n                                                                 'days_since_prior_order':'mean',\n                                                                 'reordered':'max',\n                                                                 'Day_1':'sum',\n                                                                 'Day_2':'sum',\n                                                                 'Day_3':'sum',\n                                                                 'Day_4':'sum',\n                                                                 'Day_5':'sum',\n                                                                 'Day_6':'sum',\n                                                                 'Hour_1':'sum',\n                                                                 'Hour_2':'sum',\n                                                                 'Hour_3':'sum',\n                                                                 'Hour_4':'sum',\n                                                                 'Hour_5':'sum',\n                                                                 'Hour_6':'sum',\n                                                                 'Hour_7':'sum',\n                                                                 'Hour_8':'sum',\n                                                                 'Hour_9':'sum',\n                                                                 'Hour_10':'sum',\n                                                                 'Hour_11':'sum',\n                                                                 'Hour_12':'sum',\n                                                                 'Hour_13':'sum',\n                                                                 'Hour_14':'sum',\n                                                                 'Hour_15':'sum',\n                                                                 'Hour_16':'sum',\n                                                                 'Hour_17':'sum',\n                                                                 'Hour_18':'sum',\n                                                                 'Hour_19':'sum',\n                                                                 'Hour_20':'sum',\n                                                                 'Hour_21':'sum',\n                                                                 'Hour_22':'sum',\n                                                                 'Hour_23':'sum'\n                                                                }).reset_index()\nuser_prod_df.head()","28a6e67b":"user_purchase_df = dummies_df.groupby(['user_id']).agg({         'order_id':'nunique',\n                                                                 'product_id': 'nunique',\n                                                                 'days_since_prior_order':'mean',\n                                                                 'reordered':'sum',\n                                                                 'Day_1':'sum',\n                                                                 'Day_2':'sum',\n                                                                 'Day_3':'sum',\n                                                                 'Day_4':'sum',\n                                                                 'Day_5':'sum',\n                                                                 'Day_6':'sum',\n                                                                 'Hour_1':'sum',\n                                                                 'Hour_2':'sum',\n                                                                 'Hour_3':'sum',\n                                                                 'Hour_4':'sum',\n                                                                 'Hour_5':'sum',\n                                                                 'Hour_6':'sum',\n                                                                 'Hour_7':'sum',\n                                                                 'Hour_8':'sum',\n                                                                 'Hour_9':'sum',\n                                                                 'Hour_10':'sum',\n                                                                 'Hour_11':'sum',\n                                                                 'Hour_12':'sum',\n                                                                 'Hour_13':'sum',\n                                                                 'Hour_14':'sum',\n                                                                 'Hour_15':'sum',\n                                                                 'Hour_16':'sum',\n                                                                 'Hour_17':'sum',\n                                                                 'Hour_18':'sum',\n                                                                 'Hour_19':'sum',\n                                                                 'Hour_20':'sum',\n                                                                 'Hour_21':'sum',\n                                                                 'Hour_22':'sum',\n                                                                 'Hour_23':'sum'\n                                                                }).reset_index()\nuser_purchase_df.head()","3c3d321a":"product_purchase_df = dummies_df.groupby(['product_id']).agg({   'order_id':'nunique',\n                                                                 'user_id': 'nunique',\n                                                                 'days_since_prior_order':'mean',\n                                                                 'reordered':'sum',\n                                                                 'Day_1':'sum',\n                                                                 'Day_2':'sum',\n                                                                 'Day_3':'sum',\n                                                                 'Day_4':'sum',\n                                                                 'Day_5':'sum',\n                                                                 'Day_6':'sum',\n                                                                 'Hour_1':'sum',\n                                                                 'Hour_2':'sum',\n                                                                 'Hour_3':'sum',\n                                                                 'Hour_4':'sum',\n                                                                 'Hour_5':'sum',\n                                                                 'Hour_6':'sum',\n                                                                 'Hour_7':'sum',\n                                                                 'Hour_8':'sum',\n                                                                 'Hour_9':'sum',\n                                                                 'Hour_10':'sum',\n                                                                 'Hour_11':'sum',\n                                                                 'Hour_12':'sum',\n                                                                 'Hour_13':'sum',\n                                                                 'Hour_14':'sum',\n                                                                 'Hour_15':'sum',\n                                                                 'Hour_16':'sum',\n                                                                 'Hour_17':'sum',\n                                                                 'Hour_18':'sum',\n                                                                 'Hour_19':'sum',\n                                                                 'Hour_20':'sum',\n                                                                 'Hour_21':'sum',\n                                                                 'Hour_22':'sum',\n                                                                 'Hour_23':'sum'\n                                                                }).reset_index()\nproduct_purchase_df.head()","bdee7e9c":"temp = pd.merge(left=user_prod_df,  right=user_purchase_df, on='user_id', suffixes=('','_user'))\ntemp.head(10)","3730ea8d":"features_df = pd.merge(left=temp,  right=product_purchase_df, on='product_id', suffixes=('','_prod'))\nfeatures_df.head(10)","e4c10609":"def my_reset(varnames):\n    \"\"\"\n    varnames are what you want to keep\n    \"\"\"\n    globals_ = globals()\n    to_save = {v: globals_[v] for v in varnames}\n    to_save['my_reset'] = my_reset  # lets keep this function by default\n    del globals_\n    get_ipython().magic(\"reset\")\n    globals().update(to_save)\n    \nvariables = ['features_df']\nmy_reset(variables)","0a86e65c":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        col_type2 = df[col].dtype.name\n        \n        if ((col_type != object) and (col_type2 != 'category')):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","2409773a":"import numpy as np\nreduce_features_df = reduce_mem_usage(features_df)","c8b80751":"reduce_features_df.isnull().sum().sort_values()","0f375b7e":"reduced_feature= reduce_features_df[:1000]\nreduced_feature","dc05a073":"#Splitting Training & Testing Data\n\nX_train, X_test = train_test_split(reduced_feature, test_size=0.3, random_state=100)\nprint(X_train.shape)\nprint(X_test.shape)","6c4eac8b":"# Building Neareset Neighbours Classifier with Cosine distance measure\n\nfrom sklearn.neighbors import NearestNeighbors\n\nmodel_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\nmodel_knn.fit(X_train)","ddc7175f":"import numpy as np\nquery_index = np.random.choice(X_train.shape[0])\ndistances, indices = model_knn.kneighbors(X_train.iloc[query_index, :].values.reshape((1, -1)), n_neighbors = 6)\n\nj=1\nfor i in range(0, len(distances.flatten())):\n    if i == 0:\n        print('Recommendations for {0}:\\n'.format(X_train.index[indices.flatten()[i]]))\n    else:\n        try:\n            print('{0}: {1}'.format(j, df1[df1['product_id']==X_train.index[indices.flatten()[i]]].product_name.values[0]))\n            j+=1\n        except:\n            pass","271cc1aa":"#<<<--------------------------------------------THE END-------------------------------------------->>>","0c2bd8ee":"## K-Nearest Neighbours Classfier:","1a35e60b":"# <center> \u2605 AI \/ ML Project - Online Grocery Recommendation System \u2605\n#### <center> ***Domain: Reatail & E-Commerce***","68dbe3fd":"## <center> 3. Data Preprocessing","544d4cfa":"---","381cb68e":"## <center> 4. Data Manipulation","a66b08ea":"---","b2ffc9bb":"**Inference:** The median of the majority votes seem to be at 3.5, while small number of votes are offered for 0.5 & 1...","c28073de":"### Description:\n\nWhether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering and delivery app, aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you.\n\nInstacart\u2019s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced.\n\nIn this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user\u2019s next order. They\u2019re not only looking for the best model, Instacart\u2019s also looking for machine learning engineers to grow their team.\n\n\n### Acknowledgements:\nThis dataset is taken from Kaggle, \\\nhttps:\/\/www.kaggle.com\/c\/instacart-market-basket-analysis\/data\n\n### Objective:\n- Understand the Dataset & cleanup (if required).\n- Build classification model to recommend groceries based on users past purchases.","0be4718c":"**Inference:** The stats seem to be fine, let us gain more undestanding by visualising the dataset.","018fa223":"### Here are some of the key outcomes of the project:\n- The Dataset was quiet large with combined data totally around 1.3M. \n- There were also few outliers & no duplicates present in the datset, which had to be dropped.\n- Visualising the distribution of data & their relationships, helped us to get some insights on the relationship between the featureset.\n- Further filtering was done with threshold for the number of user id's & product id's.\n- Finally Nearest Neighbours Algorithm was employed to get the similar Groceries Recommendations based on the Cosine Similarity.","2e113234":"---","277a5f17":"---","6e43dc03":"<center> <img src=\"https:\/\/raw.githubusercontent.com\/Masterx-AI\/Project_InstaCart_Market_Basket_Analysis\/main\/ogr.jpg\" style=\"width: 500px;\"\/>","e6e55f26":"---","77779900":"---","271cf1ad":"---","2c59cdc4":"## <center> 5. Predictive Modeling","943b5981":"**We aim to solve the problem statement by creating a plan of action, Here are some of the necessary steps:**\n1. Data Exploration\n2. Data Pre-processing\n3. Exploratory Data Analysis (EDA)\n4. Data Manipulation\n5. Predictive Modelling\n6. Project Outcomes & Conclusion","bc3837b7":"## <center> Stractegic Plan of Action:","803c4e2e":"## <center> 2. Exploratory Data Analysis (EDA)","c9ae6ff1":"---","bf1a1354":"---","5ad34aaf":"## <center>1. Data Exploration","0d3193bc":"## <center> 6. Project Outcomes & Conclusions","1de7c2a5":"---"}}