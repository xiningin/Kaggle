{"cell_type":{"2e9959e3":"code","e99c8856":"code","fda5f91f":"code","a1a32917":"code","658f4b79":"code","b1289520":"code","42111675":"code","17339176":"code","0b10b18c":"code","07981006":"code","08491ed5":"code","951ae55d":"code","e64a0f61":"code","79d5ec67":"code","823df90e":"code","c6470fe5":"code","be7c271d":"code","58f4220d":"code","bca683f9":"code","ac5c1b44":"markdown","b5831471":"markdown","4814d811":"markdown","7487de9c":"markdown","32a14470":"markdown","2e664138":"markdown","57fe90a5":"markdown","2ae2ed6e":"markdown"},"source":{"2e9959e3":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport nltk\nimport string\nimport tqdm\n\nnltk.download('punkt')\nnltk.download('stopwords')\nprint(tf.__version__)","e99c8856":"# for better look with colab's dark theme\nplt.rcParams['figure.facecolor'] = 'white'","fda5f91f":"train_df = pd.read_csv('..\/input\/genre-classification-dataset-imdb\/Genre Classification Dataset\/train_data.txt', sep=\":::\", header=None, engine='python')\ntrain_df.columns = ['id', 'title', 'genre', 'description']\ntrain_df.head()","a1a32917":"test_df = pd.read_csv('..\/input\/genre-classification-dataset-imdb\/Genre Classification Dataset\/test_data_solution.txt', sep=\":::\", header=None, engine='python')\ntest_df.columns = ['id', 'title', 'genre', 'description']\ntest_df.head()","658f4b79":"data_type = {0: 'train_data', 1: 'test_data'}\n\nfor i, df in enumerate([train_df, test_df]):\n    print(f'Dataset \"{data_type[i]}\" info:')\n    print(df.info())\n    print('Number of unique genres: ', df['genre'].nunique())\n    print('=====================================\\n')\n\nplt.figure(figsize=(15,5))\nfor i, df in enumerate([train_df, test_df]):\n    data = df.groupby('genre').count()\n    plt.subplot(1, 2, i + 1)\n    plt.bar(data.index, data['id'])\n    plt.xlabel('Genre')\n    plt.ylabel('Number of entries')\n    plt.title(f'Genres distribution {data_type[i]}')\n    plt.xticks(rotation=90)\nplt.show()\ndel data_type\nNUMBER_OF_GENRES = train_df['genre'].nunique()","b1289520":"stop_words = set(nltk.corpus.stopwords.words('english'))\n\ndef tokenize_text(raw_text: str):\n    tokenized_str = nltk.word_tokenize(raw_text)\n    tokens = [i.lower() for i in tokenized_str if (i not in string.punctuation) and (i not in stop_words)]\n    # filtered_tokens = [i for i in tokens if ( i not in stop_words )]\n    # lemmatized_tokens = [morph.parse(i)[0].normal_form for i in tokens]\n    return tokens\n\ntrain_df['tokenized'] = train_df.description.apply(tokenize_text)\ntest_df['tokenized'] = test_df.description.apply(tokenize_text)\nprint('Max description length in tokens: ', train_df.tokenized.apply(len).max())","42111675":"plt.hist(train_df.tokenized.apply(len), bins=30, log=True)\nplt.title('Distribution of the number of tokens in sequences')\nplt.xlabel('Length of sequence')\nplt.show()","17339176":"temp = train_df.tokenized.apply(len)\nprint('Number of entries with description length > 512: ', temp[temp > 512].count())","0b10b18c":"# create a dictionary of words to convert the description to a sequence of numbers\nwords_dict = {'<PAD>': 0}\nwords_dict['<UNK>'] = 1\n\nindex = 2\nfor seq in tqdm.tqdm(train_df['tokenized']):\n    for token in seq:\n        if token not in words_dict:\n            words_dict[token] = index\n            index += 1\nprint('\\nVocabulary length: ', index)","07981006":"inverse_words_dict = {index: token for token, index in words_dict.items()}","08491ed5":"# sequence of indices into a string of word tokens\ndef decode_text(text):\n    return ' '.join([inverse_words_dict.get(i, '?') for i in text])\n\n# text to sequence of indices\ndef encode_text(text):\n    words = tokenize_text(text)\n    idxs = [words_dict.get(word, words_dict['<UNK>']) for word in words]\n    return idxs","951ae55d":"# test the functions of translating plain text into a list of word indices and vice versa\nsample_text = train_df['description'][4]\nprint(sample_text, '\\n')\nprint(encode_text(sample_text), '\\n')\nprint(decode_text(encode_text(sample_text)))\ndel sample_text","e64a0f61":"# create arrays for training\ngenres_dict = {}\nindex = 0\nfor gen in train_df.genre.unique():\n    genres_dict[gen] = index\n    index += 1\nreversed_genres_dict = {index: gen for gen, index in genres_dict.items()} # for later use during model evaluation\n\ny_train =  train_df['genre'].map(genres_dict).values\ny_test = test_df['genre'].map(genres_dict).values","79d5ec67":"x_train = train_df['tokenized'].apply(lambda x: [words_dict.get(i, words_dict['<UNK>']) for i in x]).values\nx_test = test_df['tokenized'].apply(lambda x: [words_dict.get(i, words_dict['<UNK>']) for i in x]).values\n\nMAX_SEQ_LEN = 256 # we limit the maximum length of the sequence\nVOCAB_SIZE = len(words_dict)\n\nx_train = tf.keras.preprocessing.sequence.pad_sequences(\n    x_train,\n    value=words_dict[\"<PAD>\"],\n    padding='post',\n    maxlen=MAX_SEQ_LEN)\n\nx_test = tf.keras.preprocessing.sequence.pad_sequences(\n    x_test,\n    value=words_dict[\"<PAD>\"],\n    padding='post',\n    maxlen=MAX_SEQ_LEN)","823df90e":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# allocate 20% test_data for the validation dataset keeping proportions by genre\n# bite off test_data because retraining happens very quickly for the first model, possibly due to lack of data\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_indices, val_indices = next(sss.split(x_test, y_test))\nx_val, y_val = x_test[val_indices], y_test[val_indices]\nx_test, y_test = x_test[train_indices], y_test[train_indices]\n\nprint('TRAIN DATA SHAPE: ', x_train.shape, y_train.shape)\nprint('VALIDATION DATA SHAPE: ', x_val.shape, y_val.shape)\nprint('TEST DATA SHAPE: ', x_test.shape, y_test.shape)","c6470fe5":"# double check that all three datasets have the same genre distributions\ndata_type = {0: 'train_data', 1: 'test_data', 2: 'val_data'}\nplt.figure(figsize=(18,5))\nfor i, data in enumerate([y_train, y_test, y_val]):\n    plt.subplot(1, 3, i + 1)\n    plt.hist(data)\n    plt.xlabel('Genre code')\n    plt.ylabel('Number of entries')\n    plt.title(f'Genres distribution {data_type[i]}')\n    plt.xticks(rotation=90)\nplt.show()","be7c271d":"EMB_SIZE = 16\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN, mask_zero=True),\n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(EMB_SIZE, return_sequences=True, dropout=0.5, recurrent_dropout=0.5), \n        # merge_mode='sum'\n    ),\n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(EMB_SIZE, return_sequences=False, dropout=0.5, recurrent_dropout=0.5)\n    ),\n    # tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(EMB_SIZE*2, activation='relu'),\n    tf.keras.layers.Dense(NUMBER_OF_GENRES, activation=tf.nn.sigmoid),\n])\n\nmodel.summary()\n\nloss = tf.losses.SparseCategoricalCrossentropy()\noptimizer = tf.optimizers.Adam(learning_rate=0.001)\n# metric = tf.keras.metrics.SparseCategoricalAccuracy()\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=1)\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['sparse_categorical_accuracy'])","58f4220d":"# run this cell if you are starting new training to delete old logs\n# otherwise restart the next cell to continue training\n\nepochs_counter = 0\n\n!rm -r logs","bca683f9":"EPOCHS = 20\nBATCH_SIZE = 512\n\nmodel.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS + epochs_counter, \n          callbacks=[tensorboard_callback], validation_data=(x_val, y_val), \n          initial_epoch=epochs_counter, verbose=1)\nepochs_counter += EPOCHS","ac5c1b44":"## Option 1. Model with Embedding layer","b5831471":"> training","4814d811":"The data is ready, go to the next step.\n### Create and train the model","7487de9c":"In total, we have:\n- 54214 records and movies \/ TV shows in train_data (no gaps)\n- in test_data 54200 records and movies \/ TV shows (no gaps)\n- 27 different genres in total\n- recordings are unevenly distributed by genre (in some genres 13k + films, and in others a little more than 100) - therefore, one can expect that some genres will not be correctly identified\n- even distribution of films by genre between train_data and test_data","32a14470":"## Loading data, EDA, data preparation.","2e664138":"### Creating training, validation and test datasets","57fe90a5":"### Brief conclusion from the hyperparameter matching process:\nAll attempts to train a model with 256 or 512 sequence lengths did not raise the accuracy higher than 47% for different EMB_SIZE values \u200b\u200bfrom 16 to 128.\nWe tried different architectures: 1-2 layer (LSTM layers) with one or two Dense layers after LSTM, dropout as a separate layer or LSTM layer parameter. The recurrent_dropout = 0.5 option for LSTM layers did not give any result.\n\n** At 8-15 epochs, retraining begins (depending on the learning rate) ... ** this model is not prone to generalization of features and the trained Embedding layer is too flexible to adjust to the data without proper generalization.","2ae2ed6e":"despite the fact that the maximum length of a film description is 1845 words (tokens), the vast majority of filters have a description of <512 words (only 216 films out of 54 thousand do not meet this condition)"}}