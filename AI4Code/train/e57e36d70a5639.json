{"cell_type":{"02894fde":"code","8e298958":"code","4a500cc3":"code","7402d278":"code","cfdf4673":"code","b93c62ff":"code","21c912e2":"code","412363ed":"code","6cf603dc":"code","7a94a576":"code","09b66a68":"code","7567e3a8":"code","83532a4b":"code","675629d5":"code","9497a562":"code","b2341317":"code","2fa4af66":"code","2f9b2435":"code","1d430612":"code","e4bb375a":"code","099f95a1":"code","2dc1341a":"code","a9868d46":"code","b6df2388":"code","1de8c399":"code","8879bdee":"code","a484bd02":"code","47778f91":"code","c21225d5":"code","d855eb54":"code","c3402b6a":"code","dbb173fc":"code","43b93cd0":"code","41f7e7f3":"code","165419bd":"code","46919620":"code","dbb270ff":"code","3fb81f1c":"code","b1c03b06":"markdown","88f50226":"markdown","22e09509":"markdown","d768dc7e":"markdown","1114604e":"markdown","08702ea2":"markdown","205e5350":"markdown","0dacfa39":"markdown","87e3d45a":"markdown","d10caec5":"markdown","39730cd8":"markdown","f316bd5e":"markdown","1008c885":"markdown","542debc4":"markdown","df209afc":"markdown","0741f07f":"markdown","001e41ef":"markdown","b7e51a3c":"markdown","5dc4a63c":"markdown","8eb71458":"markdown","ff43a8fd":"markdown","3edee07b":"markdown"},"source":{"02894fde":"# --- CSS STYLE ---\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/2020-cost-of-living\/alerts.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\ncss_styling()","8e298958":"!pip install sentence-transformers\n!pip install pandarallel","4a500cc3":"# Libraries\nimport os\nimport re\nimport wandb\nimport tqdm\nimport ast\nimport pickle\nimport string\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom sentence_transformers import SentenceTransformer\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\n\n# GPU Libraries\nimport cudf\nimport cupy\nimport cuml\n\n# Environment check\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"WANDB_SILENT\"] = \"true\"\nCONFIG = {'competition': 'common-lit', '_wandb_kernel': 'aot'}\n\n# Secrets \ud83e\udd2b\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\n# Custom colors\nmy_colors = [\"#E4916C\", \"#E36A67\", \"#FFB2C8\", \"#BCE6EF\", \"#1E5656\"]\nsns.palplot(sns.color_palette(my_colors))\n\n# Set Style\nsns.set_style(\"white\")\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.spines.left'] = False\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.spines.top'] = False\nplt.rcParams.update({'font.size': 22})\n\nclass color:\n    BOLD = '\\033[1m' + '\\033[93m'\n    END = '\\033[0m'","7402d278":"! wandb login $secret_value_0","cfdf4673":"def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    '''Plots the value at the end of the a seaborn barplot.\n    axs: the ax of the plot\n    h_v: weather or not the barplot is vertical\/ horizontal'''\n    \n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(round(_x, 5), round(_y, 5), format(round(value, 5), ','), ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                ax.text(_x, _y, format(value, ','), ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n        \n\ndef create_wandb_plot(x_data=None, y_data=None, x_name=None, y_name=None, title=None, log=None, plot=\"line\"):\n    '''Create and save lineplot\/barplot in W&B Environment.\n    x_data & y_data: Pandas Series containing x & y data\n    x_name & y_name: strings containing axis names\n    title: title of the graph\n    log: string containing name of log'''\n    \n    data = [[label, val] for (label, val) in zip(x_data, y_data)]\n    table = wandb.Table(data=data, columns = [x_name, y_name])\n    \n    if plot == \"line\":\n        wandb.log({log : wandb.plot.line(table, x_name, y_name, title=title)})\n    elif plot == \"bar\":\n        wandb.log({log : wandb.plot.bar(table, x_name, y_name, title=title)})\n    elif plot == \"scatter\":\n        wandb.log({log : wandb.plot.scatter(table, x_name, y_name, title=title)})\n        \n        \ndef create_wandb_hist(x_data=None, x_name=None, title=None, log=None):\n    '''Create and save histogram in W&B Environment.\n    x_data: Pandas Series containing x values\n    x_name: strings containing axis name\n    title: title of the graph\n    log: string containing name of log'''\n    \n    data = [[x] for x in x_data]\n    table = wandb.Table(data=data, columns=[x_name])\n    wandb.log({log : wandb.plot.histogram(table, x_name, title=title)})\n    \n    \n# Cosine Similarity\ndef cosine_similarity(u, v):\n    # Get similarity between 2 vectors.\n    # To test how effective is our Embedding method\n    return np.dot(u, v) \/ (np.linalg.norm(u) * np.linalg.norm(v))\n\n\ndef save_dataset_artifact(run_name, artifact_name, path):\n    '''Saves dataset to W&B Artifactory.\n    run_name: name of the experiment\n    artifact_name: under what name should the dataset be stored\n    path: path to the dataset'''\n    \n    run = wandb.init(project='commonlit', \n                     name=run_name, \n                     config=CONFIG,\n                     anonymous=\"allow\")\n    artifact = wandb.Artifact(name=artifact_name, \n                              type='dataset')\n    artifact.add_file(path)\n\n    wandb.log_artifact(artifact)\n    wandb.finish()\n    print(\"Artifact has been saved successfully.\")","b93c62ff":"# Read in data\ntrain = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n\nprint(color.BOLD + \"Train Shape:\" + color.END, train.shape, \"\\n\" +\n      color.BOLD + \"Test Shape:\" + color.END, test.shape)\n\n# Plot\nplt.figure(figsize = (25, 11))\nsns.heatmap(train.isna(), cmap = [my_colors[3],\n                                                    my_colors[4]], xticklabels=train.columns)\nplt.title(\"Missing values in Train Data\", size=20);","21c912e2":"# Save data to W&B Dashboard\nsave_dataset_artifact(run_name='save-original-data',\n                      artifact_name='original-data', \n                      path=\"..\/input\/commonlitreadabilityprize\/train.csv\")","412363ed":"run = wandb.init(project='commonlit', name='target-exploration', config=CONFIG, anonymous=\"allow\")","6cf603dc":"print(color.BOLD + \"Min Target:\" + color.END ,train[\"target\"].min(), \"\\n\" +\n      color.BOLD + \"Text:\" + color.END, train[train[\"target\"] == train[\"target\"].min()][\"excerpt\"][1705], \"\\n\" +\n      \"\\n\" +\n      color.BOLD + \"Max Target:\" + color.END ,train[\"target\"].max(), \"\\n\" +\n      color.BOLD + \"Text:\" + color.END, train[train[\"target\"] == train[\"target\"].max()][\"excerpt\"][2829])\n\n# Plot\nplt.figure(figsize = (25, 11))\nplt.hist(train[\"target\"], bins=50, histtype='step', lw=1, facecolor=my_colors[3], \n         hatch='\/', edgecolor=my_colors[4], fill=True)\nplt.title(\"Target Distribution\", size=25)\nplt.xlabel(\"Value\", size=20)\nplt.ylabel(\"Frequency\", size=20);","7a94a576":"# Log plot into W&B\ncreate_wandb_hist(x_data=train[\"target\"], x_name=\"Target\", title=\"Target Distribution\", log=\"target-distribution\")","09b66a68":"print(color.BOLD + \"Target Value\" + color.END ,train.iloc[2718][\"target\"], \"\\n\" +\n      color.BOLD + \"Text:\" + color.END, train[train[\"target\"] == train.iloc[2718][\"target\"]][\"excerpt\"][2718], \"\\n\" +\n      \"\\n\" +\n      color.BOLD + \"Target Value:\" + color.END ,train.iloc[47][\"target\"], \"\\n\" +\n      color.BOLD + \"Text:\" + color.END, train[train[\"target\"] == train.iloc[47][\"target\"]][\"excerpt\"][47], \"\\n\" +\n      \"\\n\" +\n      color.BOLD + \"Target Value:\" + color.END ,train.iloc[106][\"target\"], \"\\n\" +\n      color.BOLD + \"Text:\" + color.END, train[train[\"target\"] == train.iloc[106][\"target\"]][\"excerpt\"][106], \"\\n\" +\n     \"\\n\" +\n      color.BOLD + \"Target Value:\" + color.END ,train.iloc[2216][\"target\"], \"\\n\" +\n      color.BOLD + \"Text:\" + color.END, train[train[\"target\"] == train.iloc[2216][\"target\"]][\"excerpt\"][2216])","7567e3a8":"print(color.BOLD + \"Min error:\" + color.END ,train[\"standard_error\"].min(), \"\\n\" +\n      color.BOLD + \"Target Value:\" + color.END, train[train[\"standard_error\"] == train[\"standard_error\"].min()][\"target\"][106], \"\\n\" +\n      color.BOLD + \"Text:\" + color.END, train[train[\"standard_error\"] == train[\"standard_error\"].min()][\"excerpt\"][106], \"\\n\" +\n      \"\\n\" +\n      color.BOLD + \"Max error:\" + color.END ,train[\"standard_error\"].max(), \"\\n\" +\n      color.BOLD + \"Target Value:\" + color.END, train[train[\"standard_error\"] == train[\"standard_error\"].max()][\"target\"][2235], \"\\n\" +\n      color.BOLD + \"Text:\" + color.END, train[train[\"standard_error\"] == train[\"standard_error\"].max()][\"excerpt\"][2235])\n\n# Plot\nplt.figure(figsize = (25, 11))\n# sns.kdeplot(train[\"standard_error\"], fill=my_colors[0], color=my_colors[0], lw=0.1, alpha=0.55)\nplt.hist(train[\"standard_error\"], bins=50, histtype='step', lw=1, facecolor=my_colors[2], \n         hatch='\/', edgecolor=my_colors[1],fill=True)\nplt.title(\"Standard Error Distribution\", size=25)\nplt.xlabel(\"Value\", size=20)\nplt.ylabel(\"Frequency\", size=20);","83532a4b":"# Log plot into W&B\ncreate_wandb_hist(x_data=train[\"standard_error\"], x_name=\"Std Error\", title=\"Standard Error Distribution\", \n                  log=\"error-distribution\")","675629d5":"corr = round(pearsonr(train[\"target\"], train[\"standard_error\"])[0], 4)\n\n# Plot\nplt.figure(figsize = (25, 11))\nsns.scatterplot(x=train[\"target\"], y=train[\"standard_error\"], color=my_colors[4], \n                size=train[\"standard_error\"], sizes=(50, 400))\nplt.title(f\"Target vs Error | Pearson: {corr}\", size=25)\nplt.xlabel(\"Target\", size=20)\nplt.ylabel(\"Standard Error\", size=20)\nplt.legend(fontsize=15, loc=\"lower right\");\n\n# Arrow\nstyle = \"Simple, tail_width=3, head_width=16, head_length=16\"\nkw = dict(arrowstyle=style, color=my_colors[0])\narrow = patches.FancyArrowPatch((0.3, 0.1), (0.05, 0.01),\n                             connectionstyle=\"arc3,rad=-.15\", **kw)\nplt.gca().add_patch(arrow);","9497a562":"wandb.log({\"pearsonr\" : corr})\n\n# Log plot into W&B\ncreate_wandb_plot(x_data=train[\"target\"], y_data=train[\"standard_error\"], \n                  x_name=\"Target\", y_name=\"Error\", title=\"Target vs Error\", log=\"scatterplot\", plot=\"scatter\")","b2341317":"# Create segments\nsegm1 = int(len(train)\/3)\nsegm2 = segm1 * 2\n\ntrain = train.sort_values(\"target\", ascending=True).reset_index(drop=True)\ntrain[\"target_segment\"] = 0\ntrain.loc[0:segm1, \"target_segment\"] = \"high\"\ntrain.loc[segm1:segm2, \"target_segment\"] = \"medium\"\ntrain.loc[segm2:, \"target_segment\"] = \"low\"","2fa4af66":"# Plot\nplt.figure(figsize = (25, 11))\nsns.kdeplot(train[\"target\"], hue=train[\"target_segment\"], fill=my_colors, color=my_colors, palette=my_colors[1:4], lw=0.1, alpha=0.65)\nplt.title(\"Target Distribution (segmented)\", size=25)\nplt.xlabel(\"Value\", size=20)\nplt.ylabel(\"Frequency\", size=20);","2f9b2435":"data = train.groupby(\"target_segment\")[[\"target\", \"standard_error\"]].mean().reset_index()\n\n# Plot\nplt.figure(figsize = (25, 11))\nax = sns.barplot(data=data, x=\"target_segment\", y=\"standard_error\", palette=my_colors[1:4], order=[\"high\", \"medium\", \"low\"])\n# show_values_on_bars(ax, h_v=\"v\", space=1)\nplt.title(f\"Standard Error (segmented)\", size=25)\nplt.xlabel(\"\", size=20)\nplt.ylabel(\"Standard Error\", size=20);","1d430612":"# Log into W&B\ncreate_wandb_plot(x_data=data[\"target_segment\"], y_data=data[\"standard_error\"], \n                  x_name=\"Std Error\", y_name=\"Segment\", title=\"Standard Error (segmented)\", \n                  log=\"error_segment\", plot=\"bar\")","e4bb375a":"wandb.finish()","099f95a1":"run = wandb.init(project='commonlit', name='embeddings_exploration', config=CONFIG, anonymous=\"allow\")","2dc1341a":"paragraphs_len = train[\"excerpt\"].apply(lambda x: len(x.split(\" \")))\n\n# Plot\nplt.figure(figsize = (25, 11))\nsns.kdeplot(paragraphs_len, fill=my_colors[0], color=my_colors[0], lw=0.1, alpha=0.65)\nplt.title(\"Count of Words in a Paragraph\", size=25)\nplt.xlabel(\"Value\", size=20)\nplt.ylabel(\"Frequency\", size=20);","a9868d46":"# Log plot into W&B\ncreate_wandb_hist(x_data=paragraphs_len, x_name=\"Count of Words\", title=\"Frequency of Words in Paragraphs\", \n                  log=\"word_count\")","b6df2388":"paragraphs = train[\"excerpt\"]\n\n# Tokenize each paragraph\ntokenized_paragraphs = [word_tokenize(p.lower()) for p in paragraphs]","1de8c399":"# Represents a document along with a tag\ntagged_paragraphs = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_paragraphs)]\n\n# Train model\ndoc2vec_model = Doc2Vec(tagged_paragraphs,    ### the tagged documents\n                        vector_size = 50,     ### how big are the feature vectors\n                        window = 2,           ### max distance between current & predicted word\n                        min_count = 1,        ### ignores words that appear once\n                        epochs = 50)          ### no. epochs to train\n\n# Example of new paragraph\ntest_paragraph = word_tokenize(\"Do you think that dinosaurs actually had furr instead of lizard skin?\".lower())\ntest_paragraph = doc2vec_model.infer_vector(test_paragraph)\n\nprint(color.BOLD + \"Word2Vec Embedding\" + color.END, \"\\n\", test_paragraph)","8879bdee":"# Encode the paragraphs\nbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\nparagraph_embeddings = bert_model.encode(paragraphs)\n\n# Test with an example\ntest_paragraph = \"Do you think that dinosaurs actually had furr instead of lizard skin? Do you think they were like giant furry toys?\"\nexample_BERT = bert_model.encode([test_paragraph])[0]\n\n# Print the most similar sentence to our example\nprint(color.BOLD + \"SentenceBERT Embedding\" + color.END)\nfor paragr in train.loc[2832:2833][\"excerpt\"].values:\n    similarity = cosine_similarity(paragraph_embeddings, bert_model.encode([paragr])[0])\n    print(color.BOLD + \"Sentence = \" + color.END, paragr, color.BOLD + \"| similarity = \" + color.END, np.sum(similarity), \"\\n\")","a484bd02":"import gc\ndel bert_model, paragraph_embeddings\ngc.collect()","47778f91":"def clean_paragraph(paragraph, verbose=False):\n    '''Cleans paragraph before tokenization'''\n    \n    # Tokenize & convert to lower case\n    tokens = word_tokenize(paragraph)\n    tokens = [t.lower() for t in tokens]\n\n    # Remove punctuation & non alphabetic characters from each word\n    table = str.maketrans('', '', string.punctuation)\n    tokens = [t.translate(table) for t in tokens]\n    tokens = [t for t in tokens if t.isalpha()]\n\n    # Filter out stopwords\n    stop_words = stopwords.words('english')\n    tokens = [t for t in tokens if not t in stop_words]\n\n    # Lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    tokens_lemm = [lemmatizer.lemmatize(t) for t in tokens]\n\n    if verbose:\n        print(color.BOLD + \"Show difference between original and lemmatized token:\" + color.END)\n        for a, b, in zip(tokens, tokens_lemm):\n            if a != b: print(a, \" | \", b)\n                \n    return \" \".join(tokens_lemm)","c21225d5":"# Example\ncleaned_paragraph = clean_paragraph(paragraph=train[\"excerpt\"][1], verbose=True)\n\nprint(\"\\n\" + \n      color.BOLD + \"Original Text:\" + color.END, \"\\n\" +\n      train[\"excerpt\"][1], \"\\n\" +\n      color.BOLD + \"After Cleaning:\" + color.END, \"\\n\" +\n      cleaned_paragraph)\n\n# Apply to the entire text\ntrain[\"text\"] = train[\"excerpt\"].apply(lambda x: clean_paragraph(x))\nwandb.finish()","d855eb54":"#! This cell was taking me 3 hours to run (on how I wrote the code - very poorly)\n#! But @adityaecdrid came to the resque with this amazing script\n#! Now it runs in less than a second \u2764\n\n# English Word Frequencies Dataset\nword_freq = pd.read_csv(\"..\/input\/english-word-frequency\/unigram_freq.csv\")\n# Convert it into a dict (i.e. hashmap)\nword_freq = dict(zip(word_freq[\"word\"], word_freq[\"count\"])) #### change - 1\navailable_words = set(word_freq.keys()) #### change - 2\n\n# Tokenize full text\ntrain[\"split_text\"] = train[\"text\"].apply(lambda x: [word for word in x.split(\" \")])\n\n# Get word count for each word\ntrain[\"freq_text\"] = train[\"split_text\"].parallel_apply(lambda x: [word_freq.get(word, 0) for word in x \n                                                                   if word in available_words]) #### change - 3\n\n\n# Save data to W&B Dashboard\nsave_dataset_artifact(run_name='save-original-data',\n                      artifact_name='original-data', \n                      path=\"..\/input\/commonlitreadabilityprize\/train.csv\")","c3402b6a":"# Get sum, mean, std etc. from the text frequencies\ntrain[\"freq_sum\"] = train[\"freq_text\"].apply(lambda x: np.sum(x))\ntrain[\"freq_mean\"] = train[\"freq_text\"].apply(lambda x: np.mean(x))\ntrain[\"freq_std\"] = train[\"freq_text\"].apply(lambda x: np.std(x))\ntrain[\"freq_min\"] = train[\"freq_text\"].apply(lambda x: np.min(x))\ntrain[\"freq_max\"] = train[\"freq_text\"].apply(lambda x: np.max(x))\n\n# Get more info from text itself\ntrain[\"no_words\"] = train[\"text\"].apply(lambda x: len(x.split(\" \")))\ntrain[\"no_words_paragraph\"] = train[\"excerpt\"].apply(lambda x: len(x.split(\" \")))\n\n# Scale these features (as they are HUGE)\nX = train[['freq_sum', 'freq_mean', 'freq_std', 'freq_min', \n           'freq_max', 'no_words', 'no_words_paragraph']]\ny = cudf.Series(train[\"target\"])\n\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X))\nX_scaled.columns = X.columns\n\n\n# === TFIDF Vectorizer ===\n### parameters from here: https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle\ntfv = TfidfVectorizer(min_df=3,  max_features=None,\n                      strip_accents='unicode', analyzer='word',\n                      token_pattern=r'\\w{1,}', ngram_range=(1, 3), \n                      use_idf=1,smooth_idf=1,sublinear_tf=1,\n                      stop_words = 'english')\ntfv.fit(train[\"text\"])\ntrain_tf_matrix = pd.DataFrame.sparse.from_spmatrix(tfv.transform(train[\"text\"]))\npickle.dump(tfv.vocabulary_, open(\"tfidfvectorizer.pkl\",\"wb\"))\n\n\n# Create final X variable, containing all info\nX_cpu = pd.concat([X_scaled, train_tf_matrix], axis=1)\nX_gpu = cudf.DataFrame(X_cpu)","dbb173fc":"# This is how our data looks now :)\nX_cpu.head(2)","43b93cd0":"# Libraries for models ;)\nfrom wandb.xgboost import wandb_callback\nfrom cuml.metrics import mean_squared_error\nfrom cuml.preprocessing.model_selection import train_test_split\nimport xgboost\n\n# Basic Data Validation\nX_train, X_test, y_train, y_test = train_test_split(X_gpu, y, \n                                                    test_size=0.3, shuffle=False)","41f7e7f3":"def train_xgb_model(X_train, X_test, y_train, y_test, params, \n                    details=\"default\", prints=True, step=1):\n    \n    run = wandb.init(project='commonlit', name=f'xgboost_{step}', \n                     config=CONFIG, anonymous=\"allow\")\n    wandb.log(params)\n    \n    # Create DMatrix - is optimized for both memory efficiency and training speed.\n    train_matrix = xgboost.DMatrix(data = X_train, label = y_train)\n    \n    # Create & Train the model\n    model = xgboost.train(params, dtrain = train_matrix, callbacks=[wandb_callback()])\n\n    # Make prediction\n    predicts = model.predict(xgboost.DMatrix(X_test))\n    rmse = mean_squared_error(y_test.astype('float32'), predicts)\n    wandb.log({'rmse':rmse}, step=step)\n\n    if prints:\n        print(color.BOLD + details + color.END + \" | RMSE: {}\".format(rmse))\n    \n    wandb.finish()\n    return model, rmse","165419bd":"# === TRAIN ===\nparams1 = {'max_depth' : 4,\n           'max_leaves' : 2**4,\n           'tree_method' : 'gpu_hist',\n           'objective' : 'reg:squarederror',\n           'grow_policy' : 'lossguide',\n           'colsample_bynode': 0.8,}\n\nmodel1, roc1 = train_xgb_model(X_train, X_test, y_train, y_test, \n                               params1, details=\"Baseline Model\", step=2)\n\n# Save the model\n### you can access the saved models in the my commonlit-dataset\n### or in the output of this notebook\npickle.dump(model1, open(\"xgb_word_freq.sav\", \"wb\"))","46919620":"# Libraries for models ;)\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, RepeatedKFold\nfrom xgboost import XGBRegressor, XGBRFRegressor\n\n# Convert data to CPU\ny_cpu = pd.Series(cupy.asnumpy(y.values))\n\n# Create Folds\nfolds = RepeatedKFold(n_splits=7, n_repeats=2, random_state=33)","dbb270ff":"# === TRAIN ===\nstep = 1\nrun = wandb.init(project='commonlit', name=f'xgbrf_model_{step}', \n                 config=CONFIG, anonymous=\"allow\")\n\nmodel2 = XGBRFRegressor(n_estimators=120)\n\nrmses = []\nfor k, (train_index, val_index) in enumerate(folds.split(X_cpu, y_cpu)):\n    X_train, y_train = X_cpu.iloc[train_index], y_cpu[train_index]\n    X_valid, y_valid = X_cpu.iloc[val_index], y_cpu[val_index]\n    \n    model2.fit(X_train, y_train, callbacks=[wandb_callback()])\n    preds = model2.predict(X_valid)\n    \n    rmse = mean_squared_error(y_valid, preds)\n    rmses.append(rmse)\n    wandb.log({'rmse':np.float(rmse)}, step=k+1)\n    \n    print(color.BOLD + f\"{k}. RMSE :\" + color.END, round(rmse, 5))\n    \nprint(\"\\n\", color.BOLD + \"Mean Fold RMSE:\" + color.END, round(np.mean(rmses), 5))\nwandb.log({'mean_rmse' : np.float(np.mean(rmses))})\nwandb.finish()\n\n\n# Save the model\n### you can access the saved models in the my commonlit-dataset\n### or in the output of this notebook\npickle.dump(model2, open(\"xgbrf_word_freq.sav\", \"wb\"))","3fb81f1c":"fe_dict = model2.get_booster().get_score(importance_type='weight')\nfe_dict = pd.DataFrame({\"feature\":fe_dict.keys(), \"weight\":fe_dict.values()}).\\\n                        sort_values(\"weight\", ascending=False).head(10)\n\n# Plot\nplt.figure(figsize = (25, 11))\nax = sns.barplot(data=fe_dict, x=\"feature\", y=\"weight\", palette=\"ocean\")\nshow_values_on_bars(ax, h_v=\"v\", space=1)\nplt.title(f\"XGBRF: Feature Importance\", size=25)\nplt.xlabel(\"\", size=20)\nplt.ylabel(\"Weight\", size=20)\nplt.yticks([]);","b1c03b06":"# 7. English Word Frequency Model & Submission\n\nI was super curious to see if I would create a SUPER simple model, using only some basic features from text & the word frequencies [dataset here](https:\/\/www.kaggle.com\/rtatman\/english-word-frequency), what would be the RMSE score?\n\nUsing embeddings and more advanced techniques will MOST DEFINITELY render better results. However, I wanted to see how a very simple baseline would perform. Can't be that bad ... right?\n\n## I. Data Preprocessing\n\nWe'll use **parallelization** to append to each word the frequency from the `english_word_frequency` dataset.\n\n<div class=\"alert success-alert\">\n\ud83d\udccc <b>Note:<\/b> I also chose to use a `TfIdfVectorizer` for the `text` feature, to add more information to the models.\n<\/div>","88f50226":"**We can also look at feature importance**, to see which features (out of the ones we've already created) are the most important. This way, we can choose afterwards which one to choose when creating the more complex models (more details are coming in my second notebook).\n\n<div class=\"alert success-alert\">\n\ud83d\udccc <b>Note:<\/b>We can see that freq_sum, freq_min and freq_mean are the most important features, although we have more than 11,700 columns for the words in our texts. It means that <b>the word_frequencies dataset is actually helpful!<\/b>\n<\/div>","22e09509":"### What does tokenization mean?\n\n> **Tokenization** is a fancy word for saying \"splitting sequences into words\". \n\n<center><img src=\"https:\/\/i.imgur.com\/JwLBUnN.png\" width=700><\/center>","d768dc7e":"<div class=\"alert simple-alert\">\n\ud83d\udccc <b>Yay!<\/b> This is a big improvement! The RMSE dropped from a value of 2.31 to around 0.82! I would call this a win, especially because we didn't really do much to our dataset.\n<\/div>\n\n<center><img src=\"https:\/\/i.imgur.com\/k6xGQHx.gif\" width=850><\/center>\n\n<center><img src=\"https:\/\/i.imgur.com\/cUQXtS7.png\"><\/center>\n\n# \u2328\ufe0f\ud83c\udfa8 My Specs\n\n* **Z8 G4** Workstation \ud83d\udda5\n* 2 CPUs & 96GB Memory \ud83d\udcbe\n* NVIDIA **Quadro RTX 8000** \ud83c\udfae\n* **RAPIDS** version 0.17 \ud83c\udfc3\ud83c\udffe\u200d\u2640\ufe0f\n\n\n> \ud83d\udccc **Leaderboard**: And the leaderboard score for the XGBRF Model using Repeated Folds is **0.93** (if you have any questions on how to submit, don't hesitate to ask - don't forget to name your submission `submission.csv` so you won't get an error!)\n<center><img src=\"https:\/\/i.imgur.com\/V9faI1T.png\"><\/center>","1114604e":"So now, our distribution would look like this:","08702ea2":"# 2. The Data\n\n> Let's observe the structure of the data first:\n\n<center><img src=\"https:\/\/i.imgur.com\/WpEGQkd.png\" width=800><\/center>","205e5350":"## II. Create More Features\n\nNow we can create some features from the new created dataset.","0dacfa39":"<img src=\"https:\/\/i.imgur.com\/GvOB7o2.png\">\n<center><h1>\ud83d\udcd6 CommonLit: Target Understanding and Text FE<\/h1><\/center>\n<center><h2>The problem is more complex than you think<\/h2><\/center>\n\n# 1. Introduction\n\nYet another amazing competition brought by Kaggle! \ud83d\ude0e My mother is a teacher, and I know the struggle of keeping kids involved and interested in reading, so I can say this competition is a bit closer to my heart.\n\nThis one looks *simple* in terms of understanding the problem, goal and competition metric. However, *don't be rush in throwing it into a model*. On a second look, there's more to it than it allows to show on the surface.\n\nThis competition differenciates itself from others because there is **only 1 feature to be used**: the text, which can be highly subjective. The target might be **missleading** as well, as it might behave differently in the `test` data than what we see in the training data.\n\nWe don't want to **turn into a smoothie after the shakeup**, as [Laura Fink](https:\/\/www.kaggle.com\/allunia) says. \ud83d\udc40\n\n<div class=\"alert simple-alert\">\n\ud83d\udccc <b>Competition Goal<\/b>: Rating the complexity of literary passages from grades 3 to 12.\n<\/div>\n\n### \u2b07\ufe0f Libraries:\n* RAPIDS info here: https:\/\/rapids.ai\/\n* Link to my W&B Dashboard here: https:\/\/wandb.ai\/andrada\/commonlit?workspace=user-andrada\n* Learn more on why and how to use W&B here: [Experiment Tracking using W&B](https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases).","87e3d45a":"## Target vs Error Comparison\n\n> \ud83d\udccc **Note**: How do we interpret this plot? When the target is ~ -1 (so the complexity is quite neutral), the error decreases a little. However, at the ends of the distribution, the standard error increases slightly, meaning that there is more dissagreement between coders in these cases.\n\n<div class=\"alert success-alert\">\nWhat does this mean? It means that if in the <code>test<\/code> set we'll have more examples that have the target value more towards the end of the histogram, then our Machine Learning might have even more troubles classifying them. And who can blame the machine, if even the humans are in such dissagreement?\n<\/div>\n\nWe also have a little guy, completely off charts. It is an outlier, with the complexity set to 0.0 and the error as well.\n\n*There is **no correlation** between the `target` and `standard_error`.*","d10caec5":"# 4. The Error\n\n### \ud83d\udcd6 Story Time\nLet's forget for a second about the classification problem and the fact that we want to train an AI to distinguish between a more complex text and a rather easy one.\n\nLet's assume that WE (as people, not as Data Scientists), need to rate these texts by hand. One by one. Try to give a *score* to one of the texts. Now try for another one. Is the second one simpler, or more complex? And how big is the difference between the 2?\n\nFor picking up emotion in images (expressions like anger, fear, sadness), **coders** are trained to score the images. **Coders** are people that are qualified to assess images and follow a strict set of rules to classify if, for example, in an image the person is happy or sad.\n\nBut problems are NEVER that easy. Usually a face can express many more feelings, like happiness, love, disgust and a little bit of surprise in the same time. In front of such picture, even the best rules and the most skilled coders can fail ... **as the answer becomes subjective**.\n\n<div class=\"alert success-alert\">\n<b>There is subjectivity in our text too.<\/b> Some could find a paragraph being a bit more easy than others. In this case, we'll have <b>an error<\/b>.\n<\/div>\n\n## ! Understanding the Standard Error\n\nOur error is **very skewed to the left**.\n\nMeaning that we have MANY texts with more than 0.5 **standard error**. So, the coders dissagreed. If humans dissagreed *half a point* on so many paragraphs, how is our AI going to perform?","39730cd8":"Let's look at a few more examples of text and the **target difficutly** that it was given:","f316bd5e":"# 6. Text Preprocessing\n\nAnother thing to be done (besides words embeddings) is preprocessing our text in a manner that it would be much cleaner and easier for the models to \"digest\".\n\nLuckly, we don't have to do much in this case. The texts are very clean, as they're paragraphs from academia. However, we can still make some adjustments:\n* **lower casing** all words\n* removing **punctuation**\n* filtering **stopwords**\n* **lemmatization** of the tokens - bringing the word to the root (*e.g.: beautifly\/ beautiful\/ beautify\/ beautification = beautiful*)","1008c885":"# 3. The Target\n\nOur target variable starts at **-3.67**, the highest possible difficulty (text I can't understand myself \ud83d\ude05) and stops at **1.71**, which is the lowest difficulty (dinosaurs and pretty things \u2764\ufe0f).\n\n> \ud83d\udccc **Note**: The target in our case has a distribution very close to normal. However ... is the test set following **the same pattern**? The CV technique in this competition will prove very valuable.","542debc4":"> \ud83d\udccc **Note**: Well ... **not good**. The **RMSE is huge**, considering the fact that we have numbers between -3 and 2. I will try to improve this one a bit, but I was just curious to see if the dataset would help ... so you don't have to :)\n\n## IV. XGBRF + Cross Validation\n\nLet's try a different approach. I'll try to improve the model by choosing a different method + making a simple `RepeatedKFold` on the data.\n\nLet's see how this fares!","df209afc":"Now let's take *2 Word Embeddings* one by one as our possible methods for the models.\n> \ud83d\udccc **Note**: My Inspo from [this article](https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/top-4-sentence-embedding-techniques-using-python\/) and [this one](https:\/\/medium.com\/analytics-vidhya\/text-classification-using-word-embeddings-and-deep-learning-in-python-classifying-tweets-from-6fe644fcfc81).\n\n## I. Doc2Vec","0741f07f":"### \u2b07\ufe0f\ud83d\ude09 Custom Functions Below ","001e41ef":"Ok, now let's look at the `standard_error` in terms of segmentation.\n\nYou can observe that indeed the error decreases a little for medium complexity. However, we can state that usually **we'll encounter a `standard_error` of 0.5 on a normal rating**.","b7e51a3c":"## II. SentenceBERT","5dc4a63c":"# 5. The Word Embeddings\n\n**What is a Word Embedding**?\nEmbedding words is the process of vectorizing text; meaning that we're changing the characters *we understand* to numbers that the *computer can understand*.\n<center><img src=\"https:\/\/i.imgur.com\/zTIGT0u.png\" width=550><\/center>\n\nLet's first see how many words we usually have in a paragraph. This will later help us understand how long we'll need to make the embeddings. If we'll choose to create embeddings of 100 numbers a vector and we have 200 words in a paragraph, and we have 2,834 paragraphs, then we'll end up with and object of size 100 x 200 x 2,834 = 56,680,000.\n\nWhich is A LOT.\n\n> \ud83d\udccc **Note**: It seems that our paragraphs have between ~140 and ~210 words. Quite long I would say.","8eb71458":"Oooook, so we looked at the `target` and its `error` to observe what we're dealing with in terms of text. How hard it is and how much dissagreement is between the coders (or raters).\n\n> After our first experiment, the W&B Dashboard looks like this:\n\n<center><img src=\"https:\/\/i.imgur.com\/BFz866C.gif\" width=800><\/center>\n\n## Target Segmentation\nNow, let's look a bit at the `standard_error` in terms of segmentation.\n\n### We'll segment the target into 3 groups:\n* high (complexity)\n* medium (complexity)\n* low (complexity)\n\nWe'll follow the natural distribution of the histogram and we'll segment the data into 3 thirds.","ff43a8fd":"> \ud83d\udccc **Bonus**: So! we can't retrain the `TfIdfVectorizer` when we submit the data. So, I saved the trained information into a `pickle`. To use it for inference, follow the code below:\n\n`transformer = TfidfTransformer()\nsaved = CountVectorizer(decode_error = \"replace\", vocabulary = pickle.load(open(\"tfidfvectorizer.pkl\", \"rb\")))\nX_test = transformer.fit_transform(saved.fit_transform(X_test))`\n\n## III. RAPIDS XGBoost\n\nI will try just a basic XGBoost, as it is usually the best performing one.","3edee07b":"> \ud83d\udccc **Note**: If this line throws an error, try using `wandb.login()` instead. It will ask for the API key to login, which you can get from your W&B profile (click on Profile -> Settings -> scroll to API keys)."}}