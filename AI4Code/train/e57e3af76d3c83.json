{"cell_type":{"a76d8015":"code","b1743db5":"code","97e03527":"code","632aa4ad":"code","9a15b0b6":"code","c0474acc":"code","a9ffa81f":"code","e2a4b6aa":"code","8d3235be":"code","182ac880":"code","26752eb5":"code","3c5cbe2a":"code","ebfc64dd":"code","d844ccf4":"code","ab40af7d":"code","da8380d2":"code","503f21a6":"code","bcac67c1":"code","f8a2a9bb":"code","6d35db58":"code","bbe978da":"code","adfcfb7b":"code","64d7a382":"code","f676de0f":"code","bd9f8078":"code","62ec1626":"code","739ac830":"code","ba788559":"code","c6853eb6":"code","ec1acdbf":"code","4ce1d7f4":"code","b00cca79":"code","bd3e84a9":"markdown","92a2293a":"markdown","c4a4207c":"markdown","7c467892":"markdown","5e80be19":"markdown","8f0a64ae":"markdown","1411649e":"markdown","545b15b1":"markdown","fa2ff5e7":"markdown","890e3e8e":"markdown","cc1d619d":"markdown","3523720e":"markdown","b4556d86":"markdown","3d675d3b":"markdown","e7c1bef2":"markdown","01fc7826":"markdown"},"source":{"a76d8015":"from IPython.display import Image ","b1743db5":"Image(filename=\"..\/input\/sf-picture\/sf1.jpg\")","97e03527":"#\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom matplotlib import rcParams\n\n%config InlineBackend.figure_format = 'retina'\nsns.set_style(\"white\")\nrcParams['figure.figsize'] = (8,4)\nimport matplotlib.ticker as ticker\nfrom IPython.display import Image \n\nfrom sklearn.preprocessing import RobustScaler # hay outliers\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import log_loss","632aa4ad":"\n# change astype to np.float32 to reduce memory usage\ndf = pd.read_csv(\"..\/input\/sf-crime\/train.csv.zip\",dtype={\"X\":np.float32,\"Y\":np.float32})","9a15b0b6":"df.head(3)","c0474acc":"# remove duplicates\nprint(df.duplicated(keep=False).value_counts())\ndf = df.drop_duplicates()","a9ffa81f":"df[[\"X\",\"Y\"]].describe()","e2a4b6aa":"df.describe(include=\"object\")","8d3235be":"def convert_dataframe(df):\n    \"\"\"\n    remove outliers and create time and block columns. Convert to np.int32 \n    due to memory usage\n    \"\"\"\n    \n    # time columns\n    df[\"Dates\"] = pd.to_datetime(df[\"Dates\"],infer_datetime_format=True)\n    df['Date'] = df['Dates'].dt.date\n    df[\"Year\"] = df[\"Dates\"].dt.year.astype(np.int32)\n    df[\"Month\"] = df[\"Dates\"].dt.month.astype(np.int32)\n    df[\"Day\"] = df[\"Dates\"].dt.day.astype(np.int32)\n    df[\"Hour\"] = df[\"Dates\"].dt.hour.astype(np.int32)\n    df[\"Minute\"] = df[\"Dates\"].dt.minute.astype(np.int32)\n    df[\"Day_week_numeric\"] = df[\"Dates\"].dt.dayofweek.astype(np.int32)\n    df[\"Weekend\"]= np.where((df[\"Day_week_numeric\"] >= 4) & (df[\"Day_week_numeric\"] <=6),1,0)\n    df[\"count_days\"] = (df['Date'] - df['Date'].min()).apply(lambda x: x.days)\n    # create block column from Adress column\n    df[\"Block\"] = df.Address.str.contains(\"Block\").astype(np.int32)\n    # drop\n    df = df.drop([\"Date\",\"Address\"],axis=1)\n    return df","182ac880":"df_date = convert_dataframe(df)","26752eb5":"# label encoder Category and PdDistrict\n\nlabel_cat = LabelEncoder()\ndf_date[\"Category_encode\"] = label_cat.fit_transform(df_date.Category)\nlabel_dist = LabelEncoder()\ndf_date[\"PdDistric_encode\"] = label_dist.fit_transform(df_date.PdDistrict)\n","3c5cbe2a":"# remove outliers\ndf_outliers = df_date.loc[df_date.Y < 90.].copy()","ebfc64dd":"df_outliers.head(3)","d844ccf4":"# count values (we can use value_counts() as well)\nmonth_count = df_outliers.groupby([\"Month\"])[\"Dates\"].count().reset_index()\n# lineplot\nax = sns.lineplot(x=\"Month\",y=\"Dates\",data=month_count,color=\"#6549DA\")\n# add horizontal line\nax.axhline(month_count['Dates'].mean(),color=\"#9CDEF6\")\nsns.despine()\n# adding text\nax.text(0.5,83000,\"Case count per Month\",\n        fontsize=13,        \n         fontweight='bold') \nax.text(0.5,81500,\"What happens during the Vacations?\",\n        fontsize=11)\nax.text(10,72000,\"Month,s Cases mean\",\n        fontsize=8,        \n         fontweight='bold')\nplt.xlabel(\"Mounth\")\nplt.ylabel(\"Frecuency\")\n# so only the graphic appears without any text referring to the object type.\nplt.show(block=False)","ab40af7d":"day_count = df_outliers.groupby([\"Day_week_numeric\"])[\"Dates\"].count().reset_index()\n#lineplot\nax = sns.lineplot(x=\"Day_week_numeric\",y=\"Dates\",data=day_count,color=\"#6549DA\")\n# add horizontal line\nax.axhline(day_count['Dates'].mean(),color=\"#9CDEF6\")\nsns.despine()\n# add text\nax.text(-0.1,136000,\"Case count per Day of Week\",\n        fontsize=13,        \n         fontweight='bold') \nax.text(-0.1,134500,\"Are Sundays quieter?\",\n        fontsize=11)\nax.text(5,124000,\"Day,s Cases mean\",\n        fontsize=11,        \n         fontweight='bold')\n# axis title\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Frecuency\")\n# so only the graphic appears without any text referring to the object type.\nplt.show(block=False)","da8380d2":"hour_count = df_outliers.groupby([\"Hour\"])[\"Dates\"].count().reset_index()\n#barplot\nax = sns.barplot(y=\"Dates\",x=\"Hour\",data=hour_count,color=\"#B8EBE9\")\n# with axvline we can draw a vertical line\nax.axhline(hour_count[\"Dates\"].mean(),color=\"#6549DA\")\n\nplt.ylabel(\"Frecuency\")\nplt.xlabel(\"Hour\")\nplt.grid(False)\nsns.despine()\n\n# add text anotation\nax.text(18,34000, \"Hour,s Cases Mean\", horizontalalignment='left', size='medium', color='black', weight='semibold')\nax.text(-0.1,59000,\"Cases Count per Hour\",\n        fontsize=13,        \n         fontweight='bold') \nax.text(-0.1,55000,\"Nights are for sleeping?\",\n        fontsize=11) \nplt.show(block=False)\n","503f21a6":"# dataframe\ncategory_counts = df_outliers.Category.value_counts(normalize=True).reset_index().head(10)\n#barplot\nax=sns.barplot(y=\"index\",x=\"Category\",data=category_counts,color=\"#04A4B5\")\n\nplt.ylabel(\"\")\nplt.xlabel(\"Cases,s Percentage\")\nplt.grid(False)\nsns.despine()\n#add text\nax.text(0,-2,\"Cases Percentage per Category\",\n        fontsize=13,        \n         fontweight='bold') \nax.text(0,-1.3,\"Protect your pockets well...\",\n        fontsize=11)\n# with a loop I add the values to the graphic\nfor num,text in zip(range(10),round(category_counts[\"Category\"],2)):\n    ax.text(text,num,text)\nplt.show(block=False)","bcac67c1":"#Let's use cumsum to see a cumulative\ndistric_counts_cumsum = df_outliers.PdDistrict.value_counts(normalize=True).cumsum().reset_index()\n#barplot\nax=sns.barplot(y=\"PdDistrict\",x=\"index\",data=distric_counts_cumsum,color=\"#30BFBF\")\n\nplt.ylabel( \"Cases,s Percentage\")\nplt.xlabel(\"\")\nplt.xticks(rotation=45)\nplt.grid(False)\nsns.despine()\n#add text\nax.text(-0.2,1.10,\"Cumulative Cases Percentage per PdDistrict\",\n        fontsize=13,        \n         fontweight='bold') \nax.text(-0.2,1.0,\"Where should I buy my house?\",\n        fontsize=11)\n# with a loop I add the values to the graphic\nfor num,text in zip(range(10),round(distric_counts_cumsum[\"PdDistrict\"],2)):\n    ax.text(num,text,text)\nplt.show(block=False)","f8a2a9bb":"distric_category = pd.crosstab(columns=df_outliers[\"PdDistrict\"],index=df[\"Category\"],normalize=\"index\")\ncategory_district_max = pd.concat([distric_category.idxmax(axis=1),distric_category.max(axis=1)],axis=1).sort_values(by=1,ascending=False).reset_index()\ncategory_district_max.columns = [\"Category\",\"District\",\"Percentage\"]\ncategory_district_max.head(10)","6d35db58":"pd.crosstab(index=df_outliers.PdDistrict,columns=df_outliers.Weekend,normalize=\"index\")","bbe978da":"plt.figure(figsize=(10,8))\nsns.scatterplot(x=\"X\",y=\"Y\",data=df_outliers.loc[df_outliers.Year==2012],alpha=0.5,color=\"#B3BDB2\")\nsns.scatterplot(x=\"X\",y=\"Y\",data=df_outliers.loc[(df_outliers.Year==2012)&(df_outliers.Category==\"PROSTITUTION\")],alpha=0.8,color=\"r\")\n\n\n#add text\nplt.text(-122.514741,37.829977,\"San Francisco Prostitution Cases by District in 2012\",\n        fontsize=15,        \n         fontweight='bold') \nplt.text(-122.416145,37.761631,\"MISSION\",\n        fontsize=12,        \n         fontweight='bold') \nplt.text(-122.420296,37.788879,\"NORTHERN\",\n        fontsize=12,        \n         fontweight='bold') \n\n\nplt.show(block=False)\n","adfcfb7b":"X = df_outliers.drop([\"Dates\",\"Category\",\"Descript\",\"DayOfWeek\",\"PdDistrict\",\"Resolution\",\"Category_encode\"],axis=1).copy()\ny = df_outliers[\"Category_encode\"]\n\n# train and validation split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state = 21)\n\n# Feature Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\n\n#kmeans\nkmeans = KMeans(n_clusters=6,random_state=0).fit(X_train)\n  ","64d7a382":"#I have to convert the array X_train into a dataframe to add a new column\nX_train_df = pd.DataFrame(X_train)\nX_val_df = pd.DataFrame(X_val)\n## add new columns\nX_train_df[\"Kmean\"] = kmeans.labels_\nX_val_df[\"Kmean\"] = kmeans.predict(X_val)","f676de0f":"# Fitting Random Forest Classification to the Training set\nclassifier = RandomForestClassifier( n_jobs = -1,random_state =50,max_depth=10,max_features=\"auto\",min_samples_split=4)\nclassifier.fit(X_train_df, y_train)","bd9f8078":"# predict\npredict_proba = classifier.predict_proba(X_val_df)\nlog_loss(y_val,predict_proba)","62ec1626":"test_data = pd.read_csv(\"..\/input\/sf-crime\/test.csv.zip\")","739ac830":"test_data_transformed = convert_dataframe(test_data)\ntest_data_transformed[\"PdDistric_encode\"] = label_dist.fit_transform(test_data_transformed.PdDistrict)\ntest_data_final = test_data_transformed.drop([\"DayOfWeek\",\"PdDistrict\",\"Dates\",\n                \"Id\"],axis=1).copy()\n\n","ba788559":"test_data_scaler = scaler.transform(test_data_final)","c6853eb6":"#scaler\ntest_data_final = pd.DataFrame(test_data_scaler)\n#add kmean column\ntest_data_final[\"Kmean\"] = kmeans.predict(test_data_final)\n","ec1acdbf":"test_data_pred_proba = classifier.predict_proba(test_data_final)\n#from label encoder we use classes to have the original values of Category, \n#and we will use them as columns in the submission dataframe.\nkeys = label_cat.classes_\n","4ce1d7f4":"result = pd.DataFrame(data=test_data_pred_proba,columns=keys)\nresult.head(3)","b00cca79":"result.to_csv(path_or_buf=\"classifier_sf.csv\",index=True, index_label = 'Id')","bd3e84a9":"## San Francisco Crime Classification\n### Predict the category of crimes that occurred in the city by the bay","92a2293a":"From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred.","c4a4207c":"At first glance, it seems that during the summer and Christmas months the number of cases is below the monthly average.\n\nLet's do the same but with the days of the week to see if we can see any difference.\nWe can see that cases go up a bit during Wednesdays and Fridays; Sundays are quieter. \n\nThe graph may be misleading and show that there is a big difference between the days, but the range is only between 132500 and 117500 (Day of Week) and 80000-65000 (Month).\n\nMany will think it is common sense that during the vacation months and Sundays there are fewer cases, but it never hurts to show it in a graph.","7c467892":"Image: https:\/\/unsplash.com\/@mvdheuvel\n  ","5e80be19":"We are going to extract information from the columns Date and Adress; for example we can have columns like Year, Month, DayofWeek,Weekend,Minute; and from Adress the cases happen either in a street (ST) or block (Block), so we can create a column called \"Block\".\t\n\nWe will also use LabelEncoder to transform the \"Category\" and \"PdDistrict\" columns.","8f0a64ae":"It is not surprising that, if we graph the cases by time of day, we see that during the night there are fewer cases.","1411649e":"### EDA\nWe have already prepared the dataframe, so now we can do an exploratory analysis to see what information we can obtain.","545b15b1":"We can also see by districts and weekends (Friday-Sunday); it strikes me that Tinderloin, where 32 percent of the drug cases occur there, the weekends are quieter.","fa2ff5e7":"### Submision","890e3e8e":"Using pd.crosstab we can see by categories which are the districts where there are more incidences (using normalize=\"index\" shows us the percentages per row).\n\nWe can see, for example, that 48 percent of the prostitution cases take place in Mission, 32 percent of the Drug cases take places in Tenderloin..\n\n","cc1d619d":"Let's make a graph of cases by districts; using cumsum we can see the cumulative. We can see that 54 percent of the cases occur in only four districts.","3523720e":"\nFor this, the first thing I am going to do, using Kmeans, is to create a new feature that can help to improve the predictions.","b4556d86":"### Model\nI could go on and on analyzing the cases for example by year, by resolution, focusing on a category (see scatter below) etc... but let's go directly to the model.\n\n","3d675d3b":"In columns X and Y there seem to be outliers (Y = 90.0000); it seems that this event belongs to another location. So we will remove it !","e7c1bef2":"Let us now see which are the most common categories, expressed as a percentage of the total,and the districts where there are more incidences.","01fc7826":"Now let's see some information about the non-numeric columns.\nThe most repeated category is LARCENY\/THEFT; Fridays seem to be quite entertaining and in SOUTHERN I don't think they get bored. \t"}}