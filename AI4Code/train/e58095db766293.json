{"cell_type":{"ea5de231":"code","ae95ac02":"code","bf55331c":"code","0480f8d9":"code","5f693385":"code","6d70d6a4":"code","a50620bb":"code","9acfce5b":"code","467720e3":"code","aea6a8da":"code","0893457e":"code","eb47c917":"code","381c7ad4":"code","bf653883":"code","a7d17ea9":"code","eb5598d2":"code","43e856b8":"code","e53f8cf2":"code","3e01a99c":"code","6adabbef":"code","4fda67f4":"code","141622e8":"code","1acc252a":"code","7c127f16":"code","9de3d77f":"code","dc515ed1":"code","15adbec4":"code","0528a17b":"code","40f8ff86":"code","edb20cc5":"code","ac4ce81b":"code","a93788e7":"code","27132971":"code","42c59745":"code","c940cb63":"code","f3874100":"code","3ef7ace8":"code","b43fa407":"code","e8145992":"code","88458676":"code","5b50f172":"markdown","95f94d26":"markdown","ee217817":"markdown","f25c196f":"markdown","d2d4b4e6":"markdown","70b9cfe1":"markdown","5f7755d0":"markdown","3040d9ae":"markdown","f4daa4ae":"markdown","e458b28d":"markdown","207dd85c":"markdown","d6cc8c5c":"markdown","165b4815":"markdown","fece93ac":"markdown","bcc111ea":"markdown","9680eb27":"markdown","d41f3901":"markdown","ffed0c3a":"markdown","ccb8bbea":"markdown","a111feb7":"markdown","a7edf840":"markdown","158d5d07":"markdown","96d5a3d1":"markdown","6d308fcd":"markdown","a162b9a0":"markdown","06ef8276":"markdown","a61b7731":"markdown","7051da94":"markdown","6634e486":"markdown","df8aa896":"markdown","d3657036":"markdown","1a67f246":"markdown","f86d45f1":"markdown","ffb21248":"markdown","c3b165ca":"markdown","15c08c25":"markdown","b59f7fdc":"markdown","94725071":"markdown","942975ea":"markdown","30effb38":"markdown","13764a98":"markdown","6b20361e":"markdown","6499e998":"markdown","7f353df2":"markdown","077832fa":"markdown"},"source":{"ea5de231":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport statsmodels.api as sm","ae95ac02":"df_import = pd.read_csv('..\/input\/airplane-movements-passengers-schiphol-airportcsv\/CSV Vliegtuigbewegingen  passagiers Schipholt.csv', sep=';', header=1)\ndf_import.head(30)","bf55331c":"df_import.info()","0480f8d9":"df_import['Date'] = pd.to_datetime(df_import['Year'].astype(str) + '\/' + df_import['Month'].astype(str)) #+ '\/01')\ndf_import['Date'].head()","5f693385":"# Select only the Date column, and the second Total row from the table above\ndf = df_import[['Date', 'Total*']]\ndf = df.rename({'Total*':'Total passengers'}, axis=1)\n\n# Drop the rows from 2020 onwards, keeping 1994-2019\ndf = df.loc[df['Date'] <= '2019-12-01']\ndf = df.set_index('Date')\ndf","6d70d6a4":"ax = df.plot(figsize=(20,10))\n#plt.grid(True, color = \"grey\", linewidth = \"0.5\", linestyle = 'dashed', axis='y')\nplt.legend(loc='upper left', fontsize=25)\nplt.title('Passengers Schiphol', fontsize=25)\nax.set_ylabel('Passengers in millions', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nxcoords = ['1992-01-01','1993-01-01','1994-01-01','1995-01-01','1996-01-01','1997-01-01','1998-01-01','1999-01-01','2000-01-01','2001-01-01','2002-01-01','2003-01-01','2004-01-01','2005-01-01','2006-01-01','2007-01-01','2008-01-01','2009-01-01','2010-01-01','2011-01-01','2012-01-01','2013-01-01','2014-01-01', '2015-01-01', '2016-01-01','2017-01-01', '2018-01-01', '2019-01-01']\nfor xc in xcoords:\n    plt.axvline(x=xc, color='black', linestyle='-.', alpha=0.3)\n    \nplt.show()","a50620bb":"decompose_result_mult = seasonal_decompose(df, model=\"multiplicative\")\nobserved = decompose_result_mult.observed\ntrend = decompose_result_mult.trend\nseasonal = decompose_result_mult.seasonal\nresidual = decompose_result_mult.resid\n\n#Graph decomposition\nfig, axes = plt.subplots(2, 2, figsize=(20,10))\n\naxe = axes.ravel()\n\nfig = df.plot(ax=axe[0], title='Actual data') \nfig = trend.plot(ax=axe[1], title='Trend') \nfig = seasonal.plot(ax=axe[2], title='Seasonality') \nfig = residual.plot(ax=axe[3], title='Residuals') \n\nplt.show()","9acfce5b":"result = adfuller(df)\nprint(\"P value of the dicky-fuller test {:.2f}\".format(result[1]))","467720e3":"df_log = np.log(df) # log\ndf_log_diff_1 = df_log.diff(1) # difference\ndf_log_diff_1 = df_log_diff_1.dropna() \nresult2 = adfuller(df_log_diff_1)\nprint(\"P value of the dicky-fuller test {:.2f}\".format(result2[1]))\ndf_log_diff_1.plot(title = 'df_log_diff_1')\ndf_log_diff_1","aea6a8da":"df_log = df_log.resample('M').sum() # M = Monthly\ndf_log.head()\n# Date has changed from the first day of the month to the last","0893457e":"from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(df_log,  order =  (1,1,1), trend = 'n', freq = 'M' ) \nmodel_fit = model.fit() \nprint(model_fit.summary()) ","eb47c917":"p_lags = [0,1,2,3,4,5] # AR lags\nd_list = [0,1] # Differencing \nq_lags = [0,1,2] # MA lags\n\nAIC_list = list() # List with AIC scores\norder_list = list() \n\ntotal_models = len(p_lags) * len(q_lags)\nprint('Total number of models: {:.0f}'.format(total_models))\n\nfor p in p_lags:\n     for d in d_list:\n        for q in q_lags:\n            order_x = (p,d,q) # specify ARIMA order\n            model = ARIMA(df_log,  order =  order_x, trend = 'n',freq = 'M') # Use df_log !\n            model_fit = model.fit()\n            AIC_list.append(model_fit.aic) # Add each AIC score to list\n            order_list.append(order_x)\n\nresult_df_log = pd.DataFrame({'ARMINA_order': order_list , 'AIC': AIC_list }) # Create results DataFrame from AIC scores and model orders\nresult_df_log.sort_values(by = 'AIC').head(20)","381c7ad4":"model = ARIMA(df_log,  order =  (3,1,2), trend = 'n', freq = 'M' ) \nmodel_fit = model.fit() \nprint(model_fit.summary()) ","bf653883":"# Predict\npredictions = model_fit.predict()\nprediction_df = pd.concat( [predictions, df_log], axis =1  ) \nprediction_df = prediction_df.rename({'predicted_mean': 'Prediction' ,'Total passengers': 'Total passengers' },axis = 1)\n\n# Revert the logged data back to normal before plotting the predictions\nprediction_df = np.exp(prediction_df) # take the exponent of logs to go to normal data\nprediction_df = prediction_df.iloc[1:,:]\nprediction_df = round(prediction_df,2)\n\n# Show DataFrame\nprediction_df","a7d17ea9":"prediction_df.plot(figsize=(15,5))\nplt.grid(True, color = \"grey\", linewidth = \"0.5\", linestyle = 'dashed', axis='y')\nplt.legend(['Predicted passengers', 'Actual passengers'], loc='best', fontsize=16)\nplt.title('Actual passengers vs. Predicted passengers', fontsize=18)\nplt.ylabel('Passengers in millions', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","eb5598d2":"prediction_df['prediction_error'] = np.abs(prediction_df['Total passengers'] - prediction_df['Prediction'])\nMAE = prediction_df['prediction_error'].mean()\ntotal_passengers = prediction_df['Total passengers'].mean()\nprint('MAE of the model is: {:.0f}'.format(MAE))\nprint('The average total passengers is {:.0f}'.format(total_passengers))","43e856b8":"#Graph data\nfig, axes = plt.subplots(1, 3, figsize=(15,4))\nfig = model_fit.resid[1:].plot( ax=axes[0]) # residuals\nfig = sm.graphics.tsa.plot_acf(model_fit.resid[1:], lags=30, ax=axes[1]) # autocorrelation\nfig = sm.graphics.tsa.plot_pacf(model_fit.resid[1:], lags=30, ax=axes[2]) # partial autocorrelation","e53f8cf2":"#let's test different options of ar and ma lag structures\n\n# Just year\nar1 = np.zeros(12)\nar1[0] = 1 # L1\nar1[11] = 1 # L12 \n\n# Year and quartile\nar2 = np.zeros(12)\nar2[0] = 1 #L1\nar2[3] = 1 #L4\nar2[7] = 1\nar2[11] = 1 #L12\n\n# Just year, no L1\nar3 = np.zeros(12)\nar3[11] = 1 # L12\n\n# Just year\nma1 = np.zeros(12)\nma1[0] = 1 # L1\nma1[11] = 1 # L12\n\n# Year and quartile\nma2 = np.zeros(12)\nma2[0] = 1 #L1\nma2[3] = 1 #L4\nma2[7] = 1\nma2[11] = 1 #L12\n\nar_options = [1,ar1,2,3,ar2,ar3] # AR lags\nma_options = [1,ma1,2,ma2] # MA lags\n\nar_order_list = list()\nma_order_list = list()\naic_list = list()\n\ntotal_models = len(ar_options) * len(ma_options)\nprint('Total number of models: {:.0f}'.format(total_models))\n\nfor ar in ar_options:\n    for ma in ma_options:\n        try:\n            mod = ARIMA(df_log, trend = 'n', order= (ar,1,ma), freq = 'M') # Use df_log !\n            res = mod.fit(disp=False , method = 'bfgs') # This method is more robust when trying lots of models\n\n            ar_order_list.append(ar)\n            ma_order_list.append(ma)\n            aic_list.append(res.aic)\n        except:\n            print(\"model failed\")\nresult_df = pd.DataFrame({'AR_order': ar_order_list , 'MA_order': ma_order_list, 'AIC': aic_list})\nresult_df.sort_values(by='AIC').head(30)","3e01a99c":"model = ARIMA(df_log,  order = ([1,12],1,[1,12]), trend = 'n', freq = 'M' )  \n\nmodel_fit = model.fit()\nprint(model_fit.summary()) \n\n# Using square brackets is the same as using np.zero()","6adabbef":"# Predict\npredictions = model_fit.predict()\nprediction_df = pd.concat( [predictions, df_log], axis =1  ) \nprediction_df = prediction_df.rename({'predicted_mean': 'Prediction' ,'Total passengers': 'Total passengers' },axis = 1)\n\n# Revert the logged data back to normal before plotting the predictions\nprediction_df = np.exp(prediction_df) # take the exponent of logs to go to normal data\nprediction_df = prediction_df.iloc[1:,:]\nprediction_df = round(prediction_df,2)\n\n# Show DataFrame\nprediction_df","4fda67f4":"prediction_df.plot(figsize=(15,5))\nplt.grid(True, color = \"grey\", linewidth = \"0.5\", linestyle = 'dashed', axis='y')\nplt.legend(['Predicted passengers', 'Actual passengers'], loc='best', fontsize=16)\nplt.title('Actual passengers vs. Predicted passengers', fontsize=18)\nplt.ylabel('Passengers in millions', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","141622e8":"prediction_df['prediction_error'] = np.abs(prediction_df['Total passengers'] - prediction_df['Prediction'])\nMAE = prediction_df['prediction_error'].mean()\ntotal_passengers = prediction_df['Total passengers'].mean()\nprint('MAE of the model is: {:.0f}'.format(MAE))\nprint('The average total passengers is {:.0f}'.format(total_passengers))","1acc252a":"#Graph data\nfig, axes = plt.subplots(1, 3, figsize=(15,4))\nfig = model_fit.resid[1:].plot( ax=axes[0]) # residuals\nfig = sm.graphics.tsa.plot_acf(model_fit.resid[1:], lags=30, ax=axes[1]) # autocorrelation\nfig = sm.graphics.tsa.plot_pacf(model_fit.resid[1:], lags=30, ax=axes[2]) # partial autocorrelation","7c127f16":"p_lags = [0,1]\nd_differences = [0,1]\nq_lags = [0,1,2]\nP_lags = [0,1,2]\nD_differences = [0,1]\nQ_lags = [1,2,3]\n\n\ntotal_models = len(p_lags) * len(d_differences) * len(q_lags) * len(P_lags) * len(D_differences) * len(Q_lags)\nprint('Total number of models: {:.0f}'.format(total_models))\n\nAIC_list = list()\norder_list = list()\nseasonal_order_list = list()\n\nfor p in p_lags:\n    for d in d_differences:\n        for q in q_lags:\n            for P in P_lags:\n                for D in D_differences:\n                    for Q in Q_lags:\n                        ARIMA_order = (p,d,q)\n                        seasonal_order = (P,D,Q,12) # S = 12 \n                        mod = sm.tsa.statespace.SARIMAX(df_log, order = ARIMA_order , seasonal_order = seasonal_order, freq = \"M\", trend = 'n') # Use np.log !\n                        try:\n                            sesonal_res = mod.fit(disp = False, method = 'bfgs') # select bfgs for more robustness\n                            AIC_list.append(sesonal_res.aic)\n                            order_list.append(ARIMA_order)\n                            seasonal_order_list.append(seasonal_order)\n                        except:\n                            print('convergence failed')\n                        \n\n                        \nresult_df = pd.DataFrame({'ARIMA_order': order_list ,'seasonal_order:': seasonal_order_list, 'AIC': AIC_list })\nresult_df.sort_values(by='AIC').head(30)","9de3d77f":"model = sm.tsa.statespace.SARIMAX(df_log, order= (0,1,1) , seasonal_order = (1,0,1,12), trend= 'n', freq = 'M')\n\nmodel_fit = model.fit() \nprint(model_fit.summary()) ","dc515ed1":"# Predict\npredictions = model_fit.predict()\nprediction_df = pd.concat( [predictions, df_log], axis =1  ) \nprediction_df = prediction_df.rename({'predicted_mean': 'Prediction' ,'Total passengers': 'Total passengers' },axis = 1)\n\n# Revert the logged data back to normal before plotting the predictions\nprediction_df = np.exp(prediction_df) # take the exponent of logs to go to normal data\nprediction_df = prediction_df.iloc[1:,:]\nprediction_df = round(prediction_df,2)\n\n# Show DataFrame\nprediction_df","15adbec4":"prediction_df.plot(figsize=(15,5))\nplt.grid(True, color = \"grey\", linewidth = \"0.5\", linestyle = 'dashed', axis='y')\nplt.legend(['Predicted passengers', 'Actual passengers'], loc='best', fontsize=16)\nplt.title('Actual passengers vs. Predicted passengers', fontsize=18)\nplt.ylabel('Passengers in millions', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","0528a17b":"prediction_df['prediction_error'] = np.abs(prediction_df['Total passengers'] - prediction_df['Prediction'])\nMAE = prediction_df['prediction_error'].mean()\ntotal_passengers = prediction_df['Total passengers'].mean()\nprint('MAE of the model is: {:.0f}'.format(MAE))\nprint('The average total passengers is {:.0f}'.format(total_passengers))","40f8ff86":"#Graph data\nfig, axes = plt.subplots(1, 3, figsize=(15,4))\nfig = model_fit.resid[1:].plot( ax=axes[0]) # residuals\nfig = sm.graphics.tsa.plot_acf(model_fit.resid[1:], lags=30, ax=axes[1]) # autocorrelation\nfig = sm.graphics.tsa.plot_pacf(model_fit.resid[1:], lags=30, ax=axes[2]) # partial autocorrelation","edb20cc5":"model = sm.tsa.statespace.SARIMAX(df_log, order= ([6],1,[1,6,7]) , seasonal_order = (1,0,1,12), trend='n', freq = 'M')\n\nmodel_fit = model.fit(method='bfgs') \nprint(model_fit.summary()) ","ac4ce81b":"# Predict\npredictions = model_fit.predict()\nprediction_df = pd.concat( [predictions, df_log], axis =1  ) \nprediction_df = prediction_df.rename({'predicted_mean': 'Prediction' ,'Total passengers': 'Total passengers' },axis = 1)\n\n# Revert the logged data back to normal before plotting the predictions\nprediction_df = np.exp(prediction_df) # take the exponent of logs to go to normal data\nprediction_df = prediction_df.iloc[1:,:]\nprediction_df = round(prediction_df,2)\n\n# Show DataFrame\nprediction_df","a93788e7":"prediction_df.plot(figsize=(15,5))\nplt.grid(True, color = \"grey\", linewidth = \"0.5\", linestyle = 'dashed', axis='y')\nplt.legend(['Predicted passengers', 'Actual passengers'], loc='best', fontsize=16)\nplt.title('Actual passengers vs. Predicted passengers', fontsize=18)\nplt.ylabel('Passengers in millions', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","27132971":"prediction_df['prediction_error'] = np.abs(prediction_df['Total passengers'] - prediction_df['Prediction'])\nMAE = prediction_df['prediction_error'].mean()\ntotal_passengers = prediction_df['Total passengers'].mean()\nprint('MAE of the model is: {:.0f}'.format(MAE))\nprint('The average total passengers is {:.0f}'.format(total_passengers))","42c59745":"#Graph data\nfig, axes = plt.subplots(1, 3, figsize=(15,4))\nfig = model_fit.resid[1:].plot( ax=axes[0]) # residuals\nfig = sm.graphics.tsa.plot_acf(model_fit.resid[1:], lags=30, ax=axes[1]) # autocorrelation\nfig = sm.graphics.tsa.plot_pacf(model_fit.resid[1:], lags=30, ax=axes[2]) # partial autocorrelation","c940cb63":"# Define the model again so the MAE column doesn't interfere\nmodel = sm.tsa.statespace.SARIMAX(df_log, order= ([6],1,[1,6,7]) , seasonal_order = (1,0,1,12), trend='n', freq = 'M')\nmodel_fit = model.fit(method='bfgs') \n\n# Predict\npredictions = model_fit.predict()\nprediction_df = pd.concat( [predictions, df_log], axis =1  ) \nprediction_df = prediction_df.rename({'predicted_mean': 'Prediction' ,'Total passengers': 'Total passengers' },axis = 1)\n\n# Revert the logged data back to normal before plotting the predictions\nprediction_df = np.exp(prediction_df) # take the exponent of logs to go to normal data\nprediction_df = prediction_df.iloc[1:,:]\nprediction_df = round(prediction_df,2)\n\n# Plot the time series\nprediction_df['2017-01-31':'2019-12-31'].plot(figsize=(15,5))\nplt.grid(True, color = \"grey\", linewidth = \"0.5\", linestyle = 'dashed', axis='y')\nplt.legend(['Predicted passengers', 'Actual passengers'], loc='best', fontsize=16)\nplt.title('Actual passengers vs. Predicted passengers', fontsize=18)\nplt.ylabel('Passengers in millions', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","f3874100":"get_fcast = model_fit.get_forecast(12) # 12 months in the future\n\nfcast = get_fcast.predicted_mean\nfcast = np.exp(fcast)\nfcast = round(fcast,2)\nprint('Forecast voor 2020 is:')\nprint(fcast)","3ef7ace8":"# Save the confidence interval, so we can use np.exp() to revert the logged forecast back to it's normal values\n\nfcast_ci = get_fcast.conf_int(alpha=0.05) # Alpha 0.05 = 95% confidence interval\nfcast_ci = np.exp(fcast_ci)\nfcast_ci = round(fcast_ci,2)\nprint('\\n', 'The confidence interval lies between:')\nfcast_ci","b43fa407":"ax = prediction_df['Total passengers'].plot(label='Total Passengers', figsize=(15,5))\nfcast.plot(ax=ax, label='Predictions', alpha=1)\n\nax.fill_between(fcast_ci['lower Total passengers'].index,\n                fcast_ci['lower Total passengers'],\n                fcast_ci['upper Total passengers'], color='k', alpha=.15)\n\nplt.grid(True, color = \"grey\", linewidth = \"0.5\", linestyle = 'dashed', axis='y')\nplt.legend(['Total passengers per year', 'Predicted passengers in 2020'], loc='upper left', fontsize=14)\nplt.title('Prediction for passengers in 2020', fontsize=18)\nplt.ylabel('Passengers in millions', fontsize=16)\nplt.xlabel('Date', fontsize=16)\n\nplt.show()","e8145992":"ax = prediction_df['Total passengers']['2010-01-31':].plot(label='Total Passengers', figsize=(15,5)) # From 2010 onwards\nfcast.plot(ax=ax, label='Predictions', alpha=1)\n\nax.fill_between(fcast_ci.index,\n                fcast_ci.iloc[:, 0],\n                fcast_ci.iloc[:, 1], color='k', alpha=.15)\n\nplt.grid(True, color = \"grey\", linewidth = \"0.5\", linestyle = 'dashed', axis='y')\nplt.legend(['Total passengers per year', 'Predicted passengers in 2020'], loc='upper left', fontsize=14)\nplt.title('Prediction for passengers in 2020', fontsize=18)\nplt.ylabel('Passengers in millions', fontsize=16)\nplt.xlabel('Date', fontsize=16)\n\nplt.show()","88458676":"fcast = pd.DataFrame(fcast)\nfcast = fcast.rename({'predicted_mean':'Predicted passengers'}, axis=1)\n#fcast.columns = ['Predicted passengers']\n#print(fcast)\nfcast['Monthly capacity per team'] = 1000 * 20.3 \nfcast['Teams needed'] = fcast['Predicted passengers'] \/ fcast['Monthly capacity per team']\nfcast['Total FTE needed'] = fcast['Teams needed'] * 3\nfcast['Total FTE needed'] = fcast['Total FTE needed'].round(1)\nfcast['Teams needed'] = fcast['Teams needed'].round(1)\nfcast['Monthly capacity per team'] = fcast['Monthly capacity per team'].astype(int)\nfcast","5b50f172":"Time series is not stationary yet\n\nUse np.log(), then difference","95f94d26":"Plot the time series","ee217817":"Calculate the Mean Absolute Error","f25c196f":"Some small lags at 2, 4, 5, 6 and 7 appear to be significant to the model","d2d4b4e6":"Time series is stationary after using np.log() and differencing once\n\nResample time series to Monthly ","70b9cfe1":"We now know how many FTE's we need every month","5f7755d0":"Plot the time series","3040d9ae":"Solid improvement on the multiplicate model\n\nCheck the residuals, Autocorrelation and Partial Autocorrelation functions to see at which lags the model fails to predict properly","f4daa4ae":"Gridsearch for the best multiplicate model, trying to incoperate different seasonal effects by defining the S (for seasonal) in SARIMAX ","e458b28d":"# **Forecasting 2020**\n\nNow the best model has been found the can use it to forecast the year 2020","207dd85c":"Gridsearch for the best additive model, trying to incoperate different seasonal effects by using np.zero() to skip insignificant lags (where p-value > 0)","d6cc8c5c":"Calculate the Mean Absolute Error","165b4815":"Calculate confidence interval of 95%","fece93ac":"Model works, now gridsearch for the best ARIMA model","bcc111ea":"Create time series DataFrame","9680eb27":"Plot the forecast with the confidence intervals","d41f3901":"Small improvement on the additive model\n\nCheck the residuals, Autocorrelation and Partial Autocorrelation functions to see at which lags the model fails to predict properly","ffed0c3a":"The best model is a SARIMAX(0,1,1)(1,0,1,12) model","ccb8bbea":"Use this model to predict for 1994-2019","a111feb7":"# **Multiplicative SARIMAX model**","a7edf840":"Use this model to predict 1994-2019","158d5d07":"Use this model to predict 1994-2019","96d5a3d1":"# **Additive ARIMA model**","6d308fcd":"Lags 12, 24, 36 et cetera indicate a yearly seasonal pattern the model is not considering \n\nThis can be incoperated using an additive model or a multiplicate model","a162b9a0":"Plot the time series","06ef8276":"Clear seasonal pattern and upward trend\n\nCheck for stationarity","a61b7731":"Get the forecast for 2020","7051da94":"Calculate the Mean Absolute Error","6634e486":"# **Additive en multiplicative SARIMAX model**","df8aa896":"# **Forecasting the total passengers in 2020 for Schiphol Airport**\n\nIn this assigment the goal was to forecast a time series, in this case with data from Schiphol Airport, ranging from 1994-2019, to predict the year 2020. \nWe only look at the Total passengers for this assigment.","d3657036":"Model appears to catch all significant lags and information inside the model\n\nThis is the model we\"ll use to predict the passengers of Schiphol Airport in 2020","1a67f246":"Check the residuals, Autocorrelation and Partial Autocorrelation functions to see at which lags the model fails to predict properly","f86d45f1":"Use this model to predict for 1994-2019","ffb21248":"Best model from this gridsearch is ARIMA(3,1,2)","c3b165ca":"Solid improvement of the MAE by incoperating the yearly seasonal effect\n\nCheck the residuals, Autocorrelation and Partial Autocorrelation functions to see at which lags the model fails to predict properly","15c08c25":"We now have the forecast for 2020\n\nSay Schiphol Airport wants to know how many FTE (Full Time Employees) they need to enroll based on this forecast.\n\n- A team can handle a 1000 passengers per shift\n- Three shifts per day (different teams)\n- An employee works 20.3 days a month","b59f7fdc":"Plot the time series","94725071":"For this part I contuined with the SARIMAX(0,1,1)(1,0,1,12), manually adding lags and checking the p-value for their added value to the model. \n\nIn the end I added an AR lag at L6 and MA lags at L1, L6 and L7 (see order) and left the seasonal_order intact. After adding these lags, L2 as seen above no longer seemed significant. The AIC score is higher than the model above, but the MAE has greatly improved.","942975ea":"Try a standard ARIMA(1,1,1) model to predict\n\n> Use df_log (instead of df_log_diff_1) to difference INSIDE the model. No constant (trend = 'c') is needed when differencing in the model, so trend can be 'n'","30effb38":"Use decomposition model to see seasonality and trend of the time series","13764a98":"Create a Date column from the Year and Month columns","6b20361e":"Calculate the Mean Absolute Error","6499e998":"Best additive model is a ARIMA([1,12],1,[1,12]) model","7f353df2":"Plot the time series","077832fa":"# **ARIMA model**"}}