{"cell_type":{"1a1c9827":"code","e7abacab":"code","8b405d5d":"code","2d9bb5cc":"code","7ffb77a1":"code","8879e79f":"code","5d3a1b6b":"code","396ed554":"code","a3054490":"code","170a612f":"code","b8cf8641":"code","7fc1c896":"code","28b12796":"code","663310e6":"code","f97220a9":"code","fbcb9e84":"code","2670647d":"code","19bba0cb":"code","16a9edc0":"code","472cc693":"code","401edbfe":"code","fb5a9f52":"code","a417a81e":"code","b91933d7":"code","db0939e0":"code","46401003":"code","aed4f5db":"code","b7201f69":"code","fb03237a":"code","2322d0a0":"code","8ddc06c6":"code","c2faf2a4":"code","65c19f9d":"code","91af9b03":"code","c656e2ec":"code","5b57f91c":"code","dfcfa982":"code","dfd3e1c3":"code","8f968d9b":"code","d725eaca":"code","a5c4488d":"code","709e1639":"markdown","8239326d":"markdown","8eff1194":"markdown","ea14116a":"markdown","ebbdb6d9":"markdown","a8d917b4":"markdown","d7c238de":"markdown","edbf5481":"markdown","f0b8958f":"markdown","37f0bc8d":"markdown","d2d86c68":"markdown","ad87cf30":"markdown","6e3f204f":"markdown","9aee951d":"markdown","0133a702":"markdown","b7925e5b":"markdown","ab53497a":"markdown","5b518506":"markdown","a074d2c7":"markdown","5a85a187":"markdown","9540c25e":"markdown","1aeb8823":"markdown","5d8877a8":"markdown","e243dad5":"markdown","5c6b430b":"markdown","359b4931":"markdown","51ddf71e":"markdown","320cdf9a":"markdown"},"source":{"1a1c9827":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e7abacab":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize","8b405d5d":"fake=pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue=pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")","2d9bb5cc":"fake.head()","7ffb77a1":"true.head()","8879e79f":"true['target']=1\nfake['target']=0","5d3a1b6b":"df=pd.concat([true,fake])","396ed554":"df.shape","a3054490":"df.head(2)","170a612f":"temp = df.groupby('target').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","b8cf8641":"sns.countplot(x='target',data=df,palette=['orange','purple'])","7fc1c896":"from plotly import graph_objs as go\nfig = go.Figure(go.Funnelarea(\n    text =temp.target,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Target Distribution\"}\n    ))\nfig.show()","28b12796":"df.isnull().sum()","663310e6":"df.nunique()","f97220a9":"df.dtypes","fbcb9e84":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(15,5))\nsns.countplot(x='subject',data=df,hue='target')","2670647d":"df['text']=df['text']+\" \"+df['title']","19bba0cb":"df.drop(['title','subject','date'],axis=1,inplace=True)\ndf.head()","16a9edc0":"no_words=df[df['target']==1].text.str.split().map(lambda x:len(x))\nno_words.plot(kind='hist',edgecolor='black',color='lightgreen',title='no of words in Real')\nplt.show()\nno_words=df[df['target']==0].text.str.split().map(lambda x:len(x))\nno_words.plot(kind='hist',edgecolor='black',color='lightblue',title='no of words in fake')\nplt.show()","472cc693":"avg_len_word=df[df['target']==1].text.str.split().map(lambda x:np.mean([len(word) for word in x]))\navg_len_word.plot(kind='hist',edgecolor='black',color='lightcoral',title='avg length of words in true')\nplt.show()\navg_len_word=df[df['target']==0].text.str.split().map(lambda x:np.mean([len(word) for word in x]))\navg_len_word.plot(kind='hist',edgecolor='black',color='lightyellow',title='avg length of words in false')\nplt.show()","401edbfe":"# creating sample words\ndef create_words(target):\n    words = []\n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            words.append(i)\n    return words","fb5a9f52":"from collections import defaultdict\ndef analyze_stopwords(data,fun,target):\n    values_list=[]\n    dic=defaultdict(int)\n    for i in range(0,len(target)):\n        corpus=fun(target[i])\n        for word in corpus:\n            dic[word]+=1\n        top=sorted(dic.items(),key=lambda x:x[1],reverse=True)[:10]\n        x_items,y_items=zip(*top)\n        values_list.append(x_items)\n        values_list.append(y_items)\n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.bar(values_list[0],values_list[1],color=\"lightblue\",edgecolor='black', linewidth=1.2)\n    ax1.set_title(\"Real\")\n    \n    ax2.bar(values_list[2],values_list[3],color=\"lightgreen\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Fake\")\n            \n    plt.suptitle(\"Top Stop words in text\")\n    plt.show()","a417a81e":"analyze_stopwords(df,create_words,[1,0])","b91933d7":"from collections import Counter\ndf['temp_list']=df['text'].apply(lambda x: str(x).split())\ntop=Counter([word for li in df['temp_list'] for word in li])\ntemp_1=pd.DataFrame(top.most_common(20))\ntemp_1.columns=[\"most_common_words\",\"frequency\"]\ntemp_1.style.background_gradient(cmap='Blues')","db0939e0":"import plotly.express as pe\nimport plotly.figure_factory as ff\nfig = pe.bar(temp_1, x=\"frequency\", y=\"most_common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='most_common_words')\nfig.show()","46401003":"import string\npunctuation_list=list(string.punctuation)\nvalue_list=[]\ndef most_occuring(dataset,fun,target):\n    d=defaultdict(int)\n    for j in range(0,len(target)):\n        words=fun(target[j])\n        for i in words:\n            if i in punctuation_list:\n                d[i]+=1\n        top=sorted(d.items(),key=lambda x: x[1],reverse=True)[:10]\n        x_items,y_counts=zip(*top)\n        value_list.append(x_items)\n        value_list.append(y_counts)\n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.bar(value_list[0],value_list[1],color=\"lightcoral\",edgecolor='black', linewidth=1.2)\n    ax1.set_title(\"Real\")\n    \n    ax2.bar(value_list[2],value_list[3],color=\"purple\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Fake\")\n            \n    plt.suptitle(\"Punctuations in text\")\n    plt.show()\n\nmost_occuring(df,create_words,[1,0])","aed4f5db":"import re\nimport string\nfrom nltk.corpus import stopwords\n\ndef clean_text(text):\n    \"\"\"Process text function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    stopwords_english = stopwords.words('english')\n    text= re.sub('\\[[^]]*\\]', '', text)\n    # remove stock market tickers like $GE\n    text = re.sub(r'\\$\\w*', '', text)\n    #removal of html tags\n    review =re.sub(r'<.*?>',' ',text) \n    # remove old style retweet text \"RT\"\n    text = re.sub(r'^RT[\\s]+', '', text)\n    # remove hyperlinks\n    text = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', text)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    text = re.sub(r'#', '', text)\n    text = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',text)\n    text = re.sub('[^a-zA-Z]',' ',text) \n    text = text.lower()\n    text_tokens =word_tokenize(text)\n\n    text_clean = []\n    for word in  text_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            lem_word =lemmatizer.lemmatize(word)  # lemmitiging word\n            text_clean.append(lem_word)\n    text_mod=[i for i in text_clean if len(i)>2]\n    text_clean=' '.join(text_mod)\n    return  text_clean","b7201f69":"df['clean_text']=df['text'].apply(lambda x: clean_text(x))","fb03237a":"df['clean_text'][:2]","2322d0a0":"# wordcloud\nfrom PIL import Image\nbook_mask = np.array(Image.open('..\/input\/masksforwordclouds\/book-logo-1.jpg'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=book_mask,\n)\nwc.generate(' '.join(text for text in df.loc[:, 'clean_text']))\nplt.figure(figsize=(18,10))\nplt.title('Top words', \n          fontdict={'size': 28,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","8ddc06c6":"df['clean_temp']=df['clean_text'].apply(lambda x: str(x).split())\ntop=Counter([word for li in df['clean_temp'] for word in li])\ntemp_2=pd.DataFrame(top.most_common(20))\ntemp_2.columns=[\"common_words\",'frequency']\ntemp_2.style.background_gradient(cmap='Blues')","c2faf2a4":"top.most_common(20)","65c19f9d":"fig = pe.treemap(temp_2, path=['common_words'], values='frequency',title='Tree of Most Common Words')\nfig.show()","91af9b03":"# analyze top 50 words in Real and false texts\ndata_1=df[df['target']==1]\npd.Series(' '.join([i for i in data_1.clean_text]).split()).value_counts()[:50].plot(kind='bar',figsize=(20,8),color='yellow'\n                                                                                       ,edgecolor='black',title='Real')\nplt.show()\n\ndata_0=df[df['target']==0]\npd.Series(' '.join([i for i in data_0.clean_text]).split()).value_counts()[:50].plot(kind='bar',figsize=(20,8),color='green'\n                                                                                       ,edgecolor='black',title='Fake')\nplt.show()","c656e2ec":"data=' '.join([sentance for sentance in df['clean_text']])","5b57f91c":"import nltk\nfrom nltk.util import ngrams\n \n# Function to generate n-grams from sentences.\ndef extract_ngrams(data, num):\n    n_grams = ngrams(nltk.word_tokenize(data), num)\n    return [ ' '.join(grams) for grams in n_grams]\n \nunigrams=extract_ngrams(data, 1)\nbigrams= extract_ngrams(data, 2)\ntrigrams= extract_ngrams(data, 3)\nfourgrams=extract_ngrams(data, 4)","dfcfa982":"freq_uni = nltk.FreqDist(unigrams)\nfreq_bi = nltk.FreqDist(bigrams)\nfreq_tri = nltk.FreqDist(trigrams)\nfreq_four = nltk.FreqDist(fourgrams)","dfd3e1c3":"# top 20 uigrams\ntop_20_uni=freq_uni.most_common(20)\ntop_20_uni_words,top_20_uni_freq=list(zip(*top_20_uni))\nplt.figure(figsize=(20,7))\nplt.bar(top_20_uni_words, top_20_uni_freq, color ='maroon',\n        width = 0.4)","8f968d9b":"top_20_bi=freq_bi.most_common(20)\ntop_20_bi_words,top_20_bi_freq=list(zip(*top_20_bi))\nplt.figure(figsize=(20,7))\nplt.bar(top_20_bi_words, top_20_bi_freq, color ='lightcoral',\n        width = 0.4)\nplt.xticks(rotation=90) ","d725eaca":"top_20_tri=freq_tri.most_common(20)\ntop_20_tri_words,top_20_tri_freq=list(zip(*top_20_tri))\nplt.figure(figsize=(20,7))\nplt.bar(top_20_tri_words, top_20_tri_freq, color ='lightgreen',\n        width = 0.4)\nplt.xticks(rotation=90) ","a5c4488d":"top_20_four=freq_four.most_common(20)\ntop_20_four_words,top_20_four_freq=list(zip(*top_20_four))\nplt.figure(figsize=(20,7))\nplt.bar(top_20_four_words, top_20_four_freq, color ='blue',\n        width = 0.4)\nplt.xticks(rotation=90) ","709e1639":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>No of words in real & fake news<\/strong><\/p>","8239326d":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightcoral; border-radius: 5px 5px;\"><strong>Top 20 Bigrams<\/strong><\/p>","8eff1194":"<p style = \"font-size : 35px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Importing Libraries<\/strong><\/p>","ea14116a":"<p style=\"font-size:150%; font-family:cursive;\">More of analysis and model part will be updated soon ! <\/p>","ebbdb6d9":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Analyzing the top punctuation marks in  real & fake news<\/strong><\/p>","a8d917b4":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Generating N-Grams<\/strong><\/p>","d7c238de":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightcoral; border-radius: 5px 5px;\"><strong>Top 20 Trigrams<\/strong><\/p>","edbf5481":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Read The Datasets<\/strong><\/p>","f0b8958f":"<p style=\"font-size:150%; font-family:cursive;\">In the final dataset the count of fake news is greater than real news <\/p>","37f0bc8d":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Look at the top words of the dataset<\/strong><\/p>","d2d86c68":"<p style = \"font-size : 35px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>N-Grams<\/strong><\/p>","ad87cf30":"<p style=\"font-size:150%; font-family:cursive;\">No of words in real news are lying in the range of 0 to 1500\nwhereas in the case of fake news it lies in the range of 0 to 2000.This shows that number of words in the fake news are higher than that of real news<\/p>","6e3f204f":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightcoral; border-radius: 5px 5px;\"><strong>Top 20 Unigrams<\/strong><\/p>","9aee951d":"# Second way","0133a702":"<p style=\"font-size:150%; font-family:cursive;\">Now we introduce a new column named as target. Here target 1 shows the real news and target 0 shows the fake news.And then we just concatenate both the dataframes to give a single dataframe and then we can further proceed for exploratory data analysis <\/p>","b7925e5b":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>target feature view<\/strong><\/p>","ab53497a":"# First way","5b518506":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Average length of words in real & fake news<\/strong><\/p>","a074d2c7":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Analyzing the top stop words in the real and fake news<\/strong><\/p>","5a85a187":"<p style=\"font-size:150%; font-family:cursive;\">This shows that all the real news have the subject politicsNews and worldnews. Apart from these subjects other six subjects lies in the fake news category <\/p>","9540c25e":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightcoral; border-radius: 5px 5px;\"><strong>Top 20 Fourgrams<\/strong><\/p>","1aeb8823":"<p style = \"font-size : 50px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : #bedcfa; border-radius: 5px 5px;\"><strong>EDA on fake-and-real news<\/strong><\/p>","5d8877a8":"<p style=\"font-size:150%; font-family:cursive;\">The average length of word in real news ranging from 4.5 to 6.0\nwhereas in the case of fake news it lies in the range of 0 to 20.This shows that in the fake news most of the words have length greater than the words of real news.<\/p>","e243dad5":"<p style=\"font-size:150%; font-family:cursive;\">No null values are present in the dataset <\/p>","5c6b430b":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>counting the frequency of n- grams<\/strong><\/p>","359b4931":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Top 50 words in real & fake news<\/strong><\/p>","51ddf71e":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Text preprocessing<\/strong><\/p>","320cdf9a":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>subject vs target<\/strong><\/p>"}}