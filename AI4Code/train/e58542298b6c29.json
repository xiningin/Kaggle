{"cell_type":{"9aefae49":"code","19e43411":"code","a853a4b6":"code","928bb3d0":"code","e8abccbc":"code","2e8481cb":"code","7efa52f4":"code","ac170df5":"code","f5639d93":"code","141c4458":"code","81ecc577":"code","b9701952":"code","ac57ba29":"code","e1d4109a":"code","296059bd":"code","44e050f7":"code","72a45b2c":"code","0dd2a017":"code","5bf71c77":"code","98e6c82d":"code","69fa452d":"code","62b6f8bf":"code","39be88dc":"code","c7c897d8":"code","854f2b34":"code","39ad690b":"code","1dd4d70f":"code","089ab9a3":"code","f79fe4ee":"markdown","24bfd033":"markdown","203db41f":"markdown","db552cbd":"markdown","2434796e":"markdown","2a7d9e46":"markdown","5013193b":"markdown","bba41dd5":"markdown","10924da5":"markdown","b3d43389":"markdown","9db987b1":"markdown","d7d5d5fd":"markdown"},"source":{"9aefae49":"import numpy as np \nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam,RMSprop\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","19e43411":"# uploading data\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","a853a4b6":"train.head()","928bb3d0":"test.head()","e8abccbc":"y_train = train['label'].astype('int32')\nX_train = (train.drop(['label'], axis = 1)).values.astype('float32')\nX_test = test.values.astype('float32')\n\nbatch_size, img_rows, img_cols = 64, 28, 28\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n\nX_train.shape, X_test.shape","2e8481cb":"# ploting countplot for train data\nplt.figure(figsize=(8,6))\nplt.title('Countplot for train data')\nsns.countplot(y_train)","7efa52f4":"# images \nplt.figure(figsize=(10,10))\nfor i in range(20):\n    plt.subplot(4, 5, i+1)\n    plt.imshow(X_train[i].reshape((28, 28)))\nplt.show()","ac170df5":"# normalize the data\nX_train \/= 255\nX_test \/= 255","f5639d93":"# one-hot encoding for y_train\ny_train = np_utils.to_categorical(y_train, 10)","141c4458":"# split train data\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n                                                      test_size = 0.1, random_state = 12345)","81ecc577":"input_shape = (img_rows, img_cols, 1)\n# using early stopping\ncallback_es = EarlyStopping(monitor = 'val_accuracy', patience = 3)\ndef first_cnn_model_keras(optimizer):\n    model = Sequential()\n    # CNN layers\n    model.add(Convolution2D(64, 5, 5, padding = 'same', kernel_initializer = 'he_uniform', \n                            input_shape = input_shape))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same'))\n    model.add(Convolution2D(128, 5, 5, padding = 'same', kernel_initializer = 'he_uniform'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same'))\n    # Fully connected layers\n    model.add(Flatten())\n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    # compile model\n    model.compile(optimizer, loss='categorical_crossentropy', metrics = ['accuracy'])\n    return model","b9701952":"# compile and fit first model (Adam optimizer)\nmodel1 = first_cnn_model_keras(Adam(learning_rate = 0.001, amsgrad = True))\nh1 = model1.fit(X_train, y_train, batch_size = batch_size, epochs = 20, verbose = 1,\n          validation_data = (X_valid, y_valid), callbacks = [callback_es])\nfinal_loss_first_adam, final_acc_first_adam = model1.evaluate(X_valid, y_valid, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss_first_adam, final_acc_first_adam))","ac57ba29":"# compile and fit first model (RMSprop optimizer)\nmodel2 = first_cnn_model_keras(RMSprop(lr=0.001))\nh2 = model2.fit(X_train, y_train, batch_size = batch_size, epochs = 20, verbose = 1,\n          validation_data = (X_valid, y_valid),callbacks = [callback_es])\nfinal_loss_first_rmsprop, final_acc_first_rmsprop = model2.evaluate(X_valid, y_valid, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss_first_rmsprop, final_acc_first_rmsprop))","e1d4109a":"# data augmentation\ndatagen = ImageDataGenerator(rotation_range = 10, \n                             zoom_range = 0.1, \n                             width_shift_range = 0.1,\n                             height_shift_range = 0.1)\ndatagen.fit(X_train)\ntrain_batches = datagen.flow(X_train, y_train, batch_size = batch_size)","296059bd":"# compile and fit first model (Adam optimizer and data augmentation)\nmodel3 = first_cnn_model_keras(Adam(learning_rate = 0.001, amsgrad = True))\nh3 = model3.fit_generator(train_batches, epochs = 40, verbose = 1,\n          validation_data = (X_valid, y_valid), callbacks = [callback_es])\nfinal_loss_first_adam_aug, final_acc_first_adam_aug = model3.evaluate(X_valid, y_valid, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss_first_adam_aug, final_acc_first_adam_aug))","44e050f7":"# compile and fit first model (RMSprop optimizer and data augmentation)\nmodel4 = first_cnn_model_keras(RMSprop(lr=0.001))\nh4 = model4.fit_generator(train_batches, epochs = 40, verbose = 1,\n          validation_data = (X_valid, y_valid), callbacks = [callback_es])\nfinal_loss_first_rmsprop_aug, final_acc_first_rmsprop_aug = model4.evaluate(X_valid, y_valid, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss_first_rmsprop_aug, final_acc_first_rmsprop_aug))","72a45b2c":"# second model\ndef second_cnn_model_keras(optimizer):\n    model = Sequential()\n    # CNN layers\n    model.add(Convolution2D(64, kernel_size = (5, 5), input_shape = input_shape, kernel_initializer = 'he_uniform'))\n    model.add(Activation('relu'))\n    model.add(Convolution2D(64, kernel_size = (5, 5), kernel_initializer = 'he_uniform'))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same'))\n    model.add(Dropout(0.25))\n    model.add(Convolution2D(128, kernel_size = (3, 3), kernel_initializer = 'he_uniform'))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Convolution2D(128, kernel_size = (3, 3), kernel_initializer = 'he_uniform'))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same'))\n    model.add(Dropout(0.25))\n    # Fully connected layers\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    # compile model\n    model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n    return model","0dd2a017":"# compile and fit second model (Adam optimizer)\nmodel5 = second_cnn_model_keras(Adam(learning_rate = 0.001, amsgrad = True))\nh5 = model5.fit(X_train, y_train, batch_size = batch_size, epochs = 20, verbose = 1,\n          validation_data = (X_valid, y_valid), callbacks = [callback_es])\nfinal_loss_second_adam, final_acc_second_adam = model5.evaluate(X_valid, y_valid, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss_second_adam, final_acc_second_adam))","5bf71c77":"# compile and fit second model (RMSprop optimizer)\nmodel6 = second_cnn_model_keras(RMSprop(lr=0.001))\nh6 = model6.fit(X_train, y_train, batch_size = batch_size, epochs = 20, verbose = 1,\n          validation_data = (X_valid, y_valid), callbacks = [callback_es])\nfinal_loss_second_rmsprop, final_acc_second_rmsprop = model6.evaluate(X_valid, y_valid, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss_second_rmsprop, final_acc_second_rmsprop))","98e6c82d":"# compile and fit second model (Adam optimizer and data augmentation)\nmodel7 = second_cnn_model_keras(Adam(learning_rate = 0.001, amsgrad = True))\nh7 = model7.fit_generator(train_batches, epochs = 20, verbose = 1,\n          validation_data = (X_valid, y_valid),callbacks = [callback_es])\nfinal_loss_second_adam_aug, final_acc_second_adam_aug = model7.evaluate(X_valid, y_valid, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss_second_adam_aug, final_acc_second_adam_aug))","69fa452d":"# compile and fit second model (RMSprop optimizer and data augmentation)\nmodel8 = second_cnn_model_keras(RMSprop(lr=0.001))\nh8 = model8.fit_generator(train_batches, epochs = 20, verbose = 1,\n          validation_data = (X_valid, y_valid), callbacks = [callback_es])\nfinal_loss_second_rmsprop_aug, final_acc_second_rmsprop_aug = model8.evaluate(X_valid, y_valid, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss_second_rmsprop_aug, final_acc_second_rmsprop_aug))","62b6f8bf":"models = ['first_cnn_adam', 'first_cnn_rmsprop', 'first_cnn_adam_aug', 'first_cnn_rmsprop_aug', \n          'second_cnn_adam', 'second_cnn_rmsprop', 'second_cnn_adam_aug', 'second_cnn_rmsprop_aug']\ndict_values = {'loss': [final_loss_first_adam, final_loss_first_rmsprop, final_loss_first_adam_aug, \n                   final_loss_first_rmsprop_aug, final_loss_second_adam, final_loss_second_rmsprop,\n                   final_loss_second_adam_aug, final_loss_second_rmsprop_aug],\n           'accuracy': [final_acc_first_adam, final_acc_first_rmsprop, final_acc_first_adam_aug, \n                   final_acc_first_rmsprop_aug, final_acc_second_adam, final_acc_second_rmsprop,\n                   final_acc_second_adam_aug, final_acc_second_rmsprop_aug]}\n\ndf = pd.DataFrame(dict_values, index = models, columns = ['loss', 'accuracy'])\ndf","39be88dc":"accuracy = h5.history['accuracy']\nval_accuracy = h5.history['val_accuracy']\nloss = h5.history['loss']\nval_loss = h5.history['val_loss']\nepochs = range(len(accuracy))\n\nf, ax = plt.subplots(1, 2, figsize=(18, 8))\nax[0].plot(epochs, accuracy, 'r--', label='Training accuracy')\nax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nax[0].set_title('Train and validation accuracy')\nax[0].set_xlabel('epoch')\nax[0].set_ylabel('accuracy')\nax[0].grid()\nax[1].plot(epochs, loss, 'r--', label='Training loss')\nax[1].plot(epochs, val_loss, 'b', label='Validation loss')\nax[1].set_title('Train and validation loss')\nax[1].set_xlabel('epoch')\nax[1].set_ylabel('loss')\nax[1].grid()\nplt.show()","c7c897d8":"# predict the values from validation data\ny_predict = model5.predict(X_valid)\n# convert predict and validation data to one-hot vectors\ny_predict_class = np.argmax(y_predict, axis = 1)\ny_true = np.argmax(y_valid, axis = 1)\n\nplt.subplots(figsize = (12, 10))\nsns.heatmap(confusion_matrix(y_true, y_predict_class), annot=True, \n            linewidths = 0.5, fmt = '.0f', cmap = 'Reds', linecolor = 'black')\nplt.xlabel('Predicted Label')\nplt.ylabel(\"True Label\")\nplt.title('Confusion Matrix')\nplt.show()","854f2b34":"# create ensemble models\nmodel = [0]*10\nfor i in range(10):  \n    model[i] = Sequential()\n    # CNN layers\n    model[i].add(Convolution2D(64, kernel_size = (3, 3), input_shape = input_shape, kernel_initializer = 'he_uniform'))\n    model[i].add(Activation('relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(Convolution2D(64, kernel_size = (3, 3), kernel_initializer = 'he_uniform'))\n    model[i].add(Activation('relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(Convolution2D(64, kernel_size = (5, 5), kernel_initializer = 'he_uniform'))\n    model[i].add(Activation('relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n    model[i].add(Dropout(0.45))\n    model[i].add(Convolution2D(128, kernel_size = (3, 3), kernel_initializer = 'he_uniform'))\n    model[i].add(Activation('relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(Convolution2D(128, kernel_size = (3, 3), kernel_initializer = 'he_uniform'))\n    model[i].add(Activation('relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(Convolution2D(128, kernel_size = (5, 5), kernel_initializer = 'he_uniform'))\n    model[i].add(Activation('relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n    model[i].add(Dropout(0.45))\n    # Fully connected layers\n    model[i].add(Flatten())\n    model[i].add(Dense(512))\n    model[i].add(Activation('relu'))\n    model[i].add(Dropout(0.45))\n    model[i].add(Dense(1024))\n    model[i].add(Activation('relu'))\n    model[i].add(Dropout(0.45))\n    model[i].add(Dense(10))\n    model[i].add(Activation('softmax'))\n    # compile model\n    model[i].compile(loss = 'categorical_crossentropy', optimizer = Adam(lr = 0.0005, amsgrad = True), metrics=['accuracy'])","39ad690b":"# edit early stopping and learning models\ncallback_lrs = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\nepochs = 40\nhistory = [0]*10\nfor j in range(10):\n    X_train1, X_valid1, y_train1, y_valid1 = train_test_split(X_train, y_train, test_size = 0.1)\n    history[j] = model[j].fit_generator(datagen.flow(X_train1, y_train1, batch_size = batch_size), epochs = epochs, verbose = 0,\n          validation_data = (X_valid1, y_valid1), callbacks = [callback_lrs])\n    print('CNN:', j+1, 'Epochs =', epochs, 'Train accuracy:', max(history[j].history['accuracy']), 'Validation accuracy:', max(history[j].history['val_accuracy']))","1dd4d70f":"# predict the values from validation data\nresults_valid = np.zeros((X_valid.shape[0],10)) \n# convert predict and validation data to one-hot vectors\nfor j in range(10):\n    results_valid = results_valid + model[j].predict(X_valid)\ny_valid_class = np.argmax(results_valid, axis = 1)\ny_true = np.argmax(y_valid, axis = 1)\n\nplt.subplots(figsize = (12, 10))\nsns.heatmap(confusion_matrix(y_true, y_valid_class), annot=True, \n            linewidths = 0.5, fmt = '.0f', cmap = 'Reds', linecolor = 'black')\nplt.xlabel('Predicted Label')\nplt.ylabel(\"True Label\")\nplt.title('Confusion Matrix')\nplt.show()","089ab9a3":"# create zero matrix\nresults = np.zeros((X_test.shape[0],10)) \n# predict the values for test dataset\nfor j in range(10):\n    results = results + model[j].predict(X_test)\n# convert predict to one-hot vectors\ny_test_class = np.argmax(results, axis = 1)\n# create and save predict dataframe\nsubmission = pd.DataFrame({'ImageId': list(range(1, len(y_test_class)+1)), 'Label': np.array(y_test_class)})\nsubmission.to_csv('submission.csv', index=False)\nprint(submission)","f79fe4ee":"# 1. Preprocessing and analysis data","24bfd033":"## 2.3 Evaluate table of single models","203db41f":"## 2.4 Visualization of learning process for single model","db552cbd":"## 2.2 Second CNN model","2434796e":"## 2.5 Confusion matrix for single model","2a7d9e46":"# 2. Create and train models","5013193b":"## 2.6 Ensemble models","bba41dd5":"## 2.1 First CNN model","10924da5":"## 2.8 Submit task","b3d43389":"# Convolutional neural network (CNN).\n###  The MNIST database of handwritten digits, available from this page, has  a set of 70,000 examples. It is a subset of a larger set available from NIST.  It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.","9db987b1":"### The optimal single model is second CNN model with Adam optimizer (without augmentation).","d7d5d5fd":"## 2.7 Confusion Matrix for ensemble models"}}