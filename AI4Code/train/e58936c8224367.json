{"cell_type":{"3024cd6d":"code","4e69a99a":"code","4a236c30":"code","c271d09b":"code","2e70bc16":"code","8270b8fd":"code","61a36367":"code","fdc79d93":"code","24eaa3a6":"code","16a8f8ec":"code","ca9b450f":"code","e8a04579":"code","6eaa1d30":"code","7ba7ec3d":"code","7c4ea18a":"code","5f3e2347":"code","4b942780":"code","e3dada1d":"code","dab9b846":"code","7677bb3e":"code","3288e8c9":"code","4de756e1":"code","736b93e5":"code","6a7e3db5":"code","67c7b54a":"code","0c80a15d":"code","f9927bdf":"code","83f4d8a1":"code","dff22b3f":"code","1c37c1ab":"code","c23b6279":"code","3ec7e9f0":"code","9bb88cd7":"code","55c1da89":"code","c7252e93":"code","9631a729":"code","62511115":"code","4dfe4acd":"code","7da65c9d":"code","30b1ab6d":"code","c6bb11bb":"code","5ce75abb":"code","a02b6e4d":"code","c4653f0a":"code","12c6738e":"code","e5057be8":"code","f6db6626":"code","5d94cc85":"code","91865ce8":"code","4faf5c18":"code","0269fa7a":"markdown","c2eadb63":"markdown","1f9c2ba5":"markdown"},"source":{"3024cd6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e69a99a":"import pandas as pd\n#Read in the dataset of july uber pickups\nuber_data = pd.read_csv(\"..\/input\/uber-pickups-in-new-york-city\/uber-raw-data-jul14.csv\")\nprint(uber_data.head(10))","4a236c30":"#Convert the type of the column to datetime\nuber_data[\"Date\/Time\"] = pd.to_datetime(uber_data[\"Date\/Time\"])","c271d09b":"#Using a floor can round off the date-time into discrete increments\nprint(uber_data[\"Date\/Time\"].dt.floor('1H').head(10))","2e70bc16":"#Let's get value counts to see the number of trips at given times\nprint(uber_data[\"Date\/Time\"].dt.floor('1H').value_counts().head(10))","8270b8fd":"#And sort the data so it is chronological\nhourly_data = uber_data[\"Date\/Time\"].dt.floor('1H').value_counts()\nhourly_data = hourly_data.sort_index()\nprint(hourly_data.head(10))","61a36367":"import matplotlib.pyplot as plt\n#Plotting the data shows some trend components\nhourly_data.plot(kind=\"line\")\nplt.show()","fdc79d93":"#We are going to see the average number of trips for each hour\/week day combination\n#First split the date into the week day, hour and the actual date\nhours = uber_data[\"Date\/Time\"].dt.hour\nweek_day = uber_data[\"Date\/Time\"].dt.weekday\ndate = uber_data[\"Date\/Time\"].dt.date\nweekly_data = pd.concat([week_day, hours, date], axis=1)\nweekly_data.columns = [\"Week Day\", \"Hour\", \"Date\"]\n\nprint(weekly_data.head(10))","24eaa3a6":"import calendar\n#The calendar library can map the integer versions of calendar weekdays to the actual name\n#0 -> Monday, 1 -> Tuesday, etc.\nprint(calendar.day_name[0])","16a8f8ec":"#Map the name\nweekly_data[\"Week Day\"] = weekly_data[\"Week Day\"].apply(lambda x: calendar.day_name[x])\nprint(weekly_data[\"Week Day\"].head(10))","ca9b450f":"#By grouping by the date, week day, and hour we can aggregate the size (# of entries) on each date\nweekly_data = weekly_data.groupby([\"Date\",\"Week Day\", \"Hour\"]).size()\nprint(weekly_data.head(10))","e8a04579":"#Reset the index\nweekly_data = weekly_data.reset_index()\nprint(weekly_data.head(10))","6eaa1d30":"#Rename 0, the default column name to be size\nweekly_data = weekly_data.rename(columns={0: \"Size\"})\nprint(weekly_data.head(10))","7ba7ec3d":"#Now we can group by the week day and average to get the mean for each week day\/hour\nweekly_data = weekly_data.groupby([\"Week Day\", \"Hour\"]).mean()[\"Size\"]\nprint(weekly_data.head(10))","7c4ea18a":"#Unstack takes a level of the index and translates it to be a column\n#We pick level=0 because we want the week day name to be the column\nweekly_data = weekly_data.unstack(level=0)\nprint(weekly_data)","5f3e2347":"#Reindex allows you to re-arrange the columns however you would like\nweekly_data = weekly_data.reindex(columns=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\nprint(weekly_data)","4b942780":"import seaborn as sns\n#Plot a heatmap of the data\n#Change the color map to blue (default is red)\nsns.heatmap(weekly_data, cmap='Blues')\nplt.show()","e3dada1d":"#geopy is a library which finds distance between latitude and longitude\nimport geopy.distance","dab9b846":"#Check to make sure latitude and longitude are in the right order\nmetro_art_coordinates = (40.7794, -73.9632)\nempire_state_building_coordinates = (40.7484, -73.9857)\ndistance = geopy.distance.distance(metro_art_coordinates, empire_state_building_coordinates)\nprint(distance)  # gives distance in km\nprint(distance.mi)  # in miles","7677bb3e":"#Easy way to convert our latitude and longitude columns to tuples\nprint(uber_data[[\"Lat\", \"Lon\"]].apply(lambda x: tuple(x),axis=1))","3288e8c9":"#Using the geopy version may take too long, so we will use the haversine formula instead\nfrom math import radians, cos, sin, asin, sqrt\n\ndef haversine(coordinates1, coordinates2):\n\n    lon1 = coordinates1[1]\n    lat1 = coordinates1[0]\n    lon2 = coordinates2[1]\n    lat2 = coordinates2[0]\n    #Change to radians\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    \n    \n    # Apply the harversine formula\n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat\/2)**2 + cos(lat1) * cos(lat2) * sin(dlon\/2)**2\n    c = 2 * asin(sqrt(a)) \n    r = 3956\n    return c * r\nprint(haversine(metro_art_coordinates, empire_state_building_coordinates))","4de756e1":"#Now, we can find the distances to both attractions\nuber_data[\"Distance MM\"] = uber_data[[\"Lat\", \"Lon\"]].apply(lambda x: haversine(metro_art_coordinates,tuple(x)),axis=1)\nuber_data[\"Distance ESB\"] = uber_data[[\"Lat\", \"Lon\"]].apply(lambda x: haversine(empire_state_building_coordinates,tuple(x)),axis=1)\nprint(uber_data[\"Distance MM\"].head(5))\nprint(uber_data[\"Distance ESB\"].head(5))","736b93e5":"#Summarize the data\nprint(uber_data[[\"Distance MM\", \"Distance ESB\"]].describe())","6a7e3db5":"#Using boolean indexing, we can sum to find the count within a specified range\nprint((uber_data[[\"Distance MM\", \"Distance ESB\"]] < .25).sum())","67c7b54a":"import numpy as np\n#Distance range takes a start, end (non-inclusive) and step amount\ndistance_range = np.arange(.1,5.1,.1)\nprint(distance_range)","0c80a15d":"#Run our analysis for each distance\ndistance_data = [(uber_data[[\"Distance MM\", \"Distance ESB\"]] < dist).sum() for dist in distance_range]\nprint(distance_data)","f9927bdf":"#Concat\ndistance_data = pd.concat(distance_data, axis=1)\nprint(distance_data)","83f4d8a1":"#Transpose and add in the index\ndistance_data = distance_data.transpose()\ndistance_data.index = distance_range\nprint(distance_data)","dff22b3f":"#And plot\ndistance_data.plot(kind=\"line\")\nplt.show()","1c37c1ab":"#Folium can let us map geographical data, first get a base map with latitude and longitude\nimport folium as folium\nuber_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\nuber_map","c23b6279":"#Pick the first five latitude\/longitude combinations\nlat = uber_data[\"Lat\"].values[:5]\nlon = uber_data[\"Lon\"].values[:5]\n\nuber_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\n#Marker let's you drop markers on the map\n#You can also add text to the markers with the popup argument\nfor i in range(len(lat)):\n    folium.Marker((lat[i], lon[i]), popup=\"Rider {}\".format(i+1)).add_to(uber_map)\nuber_map","3ec7e9f0":"from folium.plugins import HeatMap\n\nlat_lon = uber_data[[\"Lat\", \"Lon\"]].values[:10000]\nuber_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\n#A heatmap can be plotted like so... the radius argument controls the radius of each point within the map\n#You can zoom in on this map to see more specific areas, or out to see more general\nHeatMap(lat_lon, radius=13).add_to(uber_map)\nuber_map","9bb88cd7":"lat_lon = uber_data[[\"Lat\", \"Lon\"]].values[:10000]\nuber_map = folium.Map(location=[40.7128, -74.0060], zoom_start=10)\n#A bigger radius (and more zoom) can let us observe drop offs outside of the city that happen often\n#Such as the airport\nHeatMap(lat_lon, radius=30).add_to(uber_map)\nuber_map","55c1da89":"#We can also give a weight to either give different values to points, or to make the graphs less dense looking\nuber_data[\"Weight\"] = .5\nlat_lon = uber_data[[\"Lat\", \"Lon\", \"Weight\"]].values[:10000]\nuber_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\n#Now let's increase radius since the weights are less\nHeatMap(lat_lon, radius=15).add_to(uber_map)\nuber_map","c7252e93":"#Let's get the points that are within distance of either point of interest\n#There won't be overlap if we use only points that are .25 mile away\ni = uber_data[[\"Distance MM\", \"Distance ESB\"]] < .25\nprint(i)","9631a729":"#Take data where either one is true\ni = i.any(axis=1)\nprint(i)","62511115":"#This is our map data\nmap_data = uber_data[i].copy()\nprint(map_data)","4dfe4acd":"#Let's draw on a heatmap with the locations within the radius\n#Notice that one heatmap is a semi-circle because drop offs can't happen to the left of it\nmap_data[\"Weight\"] = .1\nlat_lon = map_data[[\"Lat\", \"Lon\", \"Weight\"]].values\nuber_map = folium.Map(location=[40.7728, -74.0060], zoom_start=13)\nHeatMap(lat_lon, radius=10).add_to(uber_map)\nuber_map","7da65c9d":"#Let's grab only the date and hour by replacing the other parts with 0\nuber_data[\"Date_Hour\"] = uber_data[\"Date\/Time\"].apply(lambda x: x.replace(microsecond=0,second=0,minute=0))\nprint(uber_data[\"Date_Hour\"])","30b1ab6d":"from datetime import datetime\n#Take only the first week of data\nmap_data = uber_data[uber_data[\"Date\/Time\"] < datetime(2014,7,8)].copy()\nmap_data[\"Weight\"] = .5\n#Randomly sample 1\/3 the values in each group\nmap_data = map_data.groupby('Date_Hour').apply(lambda x: x[[\"Lat\", \"Lon\", \"Weight\"]].sample(int(len(x)\/3)).values.tolist())\n#Get the index\ndate_hour_index = [x.strftime(\"%m\/%d\/%Y, %H:%M:%S\") for x in map_data.index]\n#Get the data in list form (each element of this bigger list will be a list of lists with lat\/lon\/weight)\n#Each element of the bigger list is a for a date\/hour combo\ndate_hour_data = map_data.tolist()","c6bb11bb":"from folium.plugins import HeatMapWithTime\nuber_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\n#A heatmap with time can now be out together\nhm = HeatMapWithTime(date_hour_data, index=date_hour_index)\nhm.add_to(uber_map)\nuber_map","5ce75abb":"#Recall the seasonality we saw before\nhourly_data.plot(kind='line')\nplt.show()","a02b6e4d":"#What about the hourly trends?\nh = hourly_data.groupby(hourly_data.index.hour).mean()\nh.plot(kind=\"line\")\nplt.show()","c4653f0a":"#Something else of interest is the difference in hourly trends for weekdays and weekends\n#We will index with i for weekdays\ni = hourly_data.index.weekday <= 4\n\nh_week = hourly_data.loc[i].groupby(hourly_data.loc[i].index.hour).mean()\nh_weekend = hourly_data.loc[~i].groupby(hourly_data.loc[~i].index.hour).mean()\nh = pd.concat([h_week, h_weekend], axis=1)\nh.columns = [\"Weekday\", \"Weekend\"]\nprint(h)","12c6738e":"#And plot to see the difference\nh.plot(kind='line')\nplt.show()","e5057be8":"#We can also divide by the total number of trips for each to normalize and have each be a percent of total trips in a day\n(h \/ h.sum()).plot(kind='line')\nplt.show()","f6db6626":"#We can finish our assessment of whether or not we see hourly effects by using a t-test to see if each hour\n#has a statistically different proportion of rides for weekends vs. weekdays\nfrom scipy.stats import ttest_ind\n\n#The functions takes two samples and returns the t-stat and the p-value denoting the null hypothesis that they are the same\nprint(ttest_ind([100,105,110], [200,230,210]))","5d94cc85":"#Now for each day, let's normalize by the total number of rides in the day\nhourly_data_pct = hourly_data.groupby(hourly_data.index.date).apply(lambda x: x \/ x.sum())\nprint(hourly_data_pct)","91865ce8":"#For each group of hours, we will apply a function to test the null hypothesis where the first sample is\n#weekdays and the second is weekends found by taking the days of the index\nt_stats = hourly_data_pct.groupby(hourly_data_pct.index.hour).apply(lambda x: ttest_ind(x[x.index.weekday<=4], x[x.index.weekday>4])[0])\nprint(t_stats)","4faf5c18":"ax = t_stats.plot(kind='bar', color='blue')\nax.axhline(1.96, linestyle='--', color='grey', linewidth=2)\nax.axhline(0, color='black', linewidth=2)\nax.axhline(-1.96, linestyle='--', color='grey', linewidth=2)\nplt.xlabel(\"Hour\")\nplt.ylabel(\"T-Statistic\")\nplt.title(\"Hourly Differences for Weekend vs. Weekday Uber Rides\")\nplt.show()","0269fa7a":"# Mapping Data with Folium","c2eadb63":"### In this notebook I have performed EDA on the Uber dataset for july14 data (uber-raw-data-jul14.csv). My objective is to use geopy and Folium in order to visualize and draw out important points of interest such as Seasonlity etc. Similar idea can be extended to the complete dataset.","1f9c2ba5":"# Testing Seasonality"}}