{"cell_type":{"b91897d2":"code","b4b847e0":"code","c2076a37":"code","378e5e9b":"code","873ceca8":"code","3463df47":"code","3a6904e6":"code","9e3cf508":"code","e897234f":"code","9c861543":"code","d2dc7f0d":"code","bba3cb6d":"code","24720a6c":"code","966ef894":"code","4a53e84c":"code","ea2fa7d2":"code","2d22930a":"code","83411c8a":"code","14a81d23":"code","92ad99d5":"code","7280dbd9":"code","d45f8cd8":"code","a20f0110":"code","71ce9218":"code","acd8add4":"code","5eae225f":"code","0b197d58":"code","dde8bda6":"code","9d84e4d4":"code","342e6431":"code","5c50eda1":"code","43410f04":"code","1b8947d2":"code","20106ed9":"code","5dbbdd5a":"code","70fc0dc7":"code","af47f5dc":"code","724b3ebe":"code","63e50913":"code","7f258cb9":"code","fe2b7bd1":"code","fe75bc62":"code","e721ee98":"code","6eb6ad9f":"code","07d21002":"code","0c155fcd":"code","a59ab338":"code","66ce06ac":"code","9765146a":"code","2c4e0b29":"code","302801ea":"code","6e145d75":"code","89896058":"code","4d49170a":"code","b17b6523":"code","2f3adc5d":"code","3a772742":"code","9dfbdc01":"code","c324e35a":"code","3d523f41":"code","b2889099":"code","e12845a6":"code","3f49bbb8":"code","cb03140c":"code","eccef524":"code","d8deea31":"code","37c6c3a8":"code","b831e0c4":"code","c81cf04f":"code","11105382":"code","00f3f2cd":"code","8514acd0":"code","27f6491a":"code","098ee2fd":"code","56ba6a67":"code","ce5682eb":"code","831d04d5":"code","091a0d7c":"code","81013b86":"code","7b1d0350":"code","315c93af":"code","085c61ad":"code","b911857e":"code","5fdaa2fb":"code","7de00ded":"code","ae0c9e80":"code","b476ac6c":"code","bdad92df":"code","94e636a0":"code","4a7975d0":"code","07823944":"code","84889bf1":"code","237c8bce":"code","46392f5b":"code","2f059b16":"code","6fd3a245":"code","6e80fd33":"code","ddc12d36":"code","cad01481":"code","f722ddd9":"code","d17a21ea":"code","1dded78d":"code","90015bb1":"code","46f800fc":"code","6649b2ea":"code","6ce44eff":"code","bf459828":"code","66708343":"code","31a355d3":"code","d1be824c":"code","2444aec7":"code","ce130fec":"code","d8140520":"code","3b14e318":"code","e87b00e6":"code","b29bb347":"code","a5033488":"code","b32b926f":"code","3d089a1c":"code","0344b401":"code","d31fe440":"code","57f5bae2":"code","82f3f6c9":"code","280702ad":"code","0cf2547f":"code","48b4842c":"code","640debd6":"code","8c86823c":"code","21c69f1e":"code","49a4ccf7":"code","0532f570":"code","e6603841":"code","7f3565be":"code","82c81466":"code","d23e7206":"code","232e2aca":"code","37f65581":"code","186377bd":"code","7dea382f":"code","50d63253":"code","763f2f4a":"code","126cd069":"code","0b4b5337":"code","cec8fcf6":"code","5ac629f0":"code","5b2463d0":"code","205a4020":"code","44d8be47":"code","89bbf238":"code","bca45aaa":"code","e5e97310":"code","2d42d3be":"code","a7ae8686":"code","a97c1675":"code","a81a7ca8":"code","1745a669":"code","3489e067":"code","de65e275":"code","88ae4e2a":"code","f21bd7fe":"code","5b0be75c":"code","dad8ed1b":"code","229e5edf":"code","fb3a882c":"code","4ced9e9c":"code","2108d61f":"code","9aaf082b":"code","68750bfd":"code","1b4754ee":"code","b860fb14":"code","00b56639":"code","177c47dd":"code","f09e5e82":"code","85e59d5f":"code","c5c2ce66":"code","01ad53d9":"code","832e27e7":"code","397b4111":"code","f3e5a07f":"code","2c64d7b4":"code","ac3bec21":"code","11a02808":"code","68cf88fc":"code","fb4717c6":"code","25ce5f16":"code","69a7a515":"code","4d1acd87":"code","1b69d673":"code","d510e9a0":"code","4d0afaa3":"code","6094d4e0":"code","f906f8e3":"code","1b126159":"code","1a1a98ea":"code","f3a2ad81":"code","0b2a06a7":"code","891d6b33":"code","2df95031":"code","2b176472":"code","cb896838":"code","dbad4d76":"code","dfc18015":"code","d25d900c":"code","7b71b821":"code","4f990d33":"code","66216337":"code","39615f97":"code","8d0f667f":"code","4a19bf77":"code","6611f24e":"code","f2028553":"code","298cc090":"code","0a0a535b":"code","a60809f0":"code","62d4b5e7":"code","6b3b2447":"code","0034a4b2":"code","0ffa6b76":"code","f3dfe73f":"code","7bedb478":"code","a128ad04":"code","3a6219f1":"code","906e7605":"code","c5325556":"code","86f71200":"code","0b36d09e":"code","55e82a76":"code","dd35aa7e":"markdown","9bcba2f9":"markdown","70a99830":"markdown","b17c231b":"markdown","4cb6daaa":"markdown","6c02755a":"markdown","2bb5f7bb":"markdown","5d3b14be":"markdown","fd1ddc6b":"markdown","8bffeb7b":"markdown","133740b1":"markdown","e2a7e856":"markdown","61f17dc1":"markdown","8126bd6e":"markdown","4ef33ba8":"markdown","53ecd755":"markdown","d046686f":"markdown","8ad5a590":"markdown","7504cc05":"markdown","be87bd66":"markdown","87970c10":"markdown","b79a00c0":"markdown","a51c2e1e":"markdown","5fa3dc91":"markdown","0f001349":"markdown","3f3b04d8":"markdown","9992672f":"markdown","574f9592":"markdown","db981efd":"markdown","0efca0a9":"markdown","372675c9":"markdown","69dc66dd":"markdown","eff90abd":"markdown","b8dd354e":"markdown","80467197":"markdown","1b236b2a":"markdown","77ce17fd":"markdown","cc83e760":"markdown","2109f52d":"markdown","569b8579":"markdown","6a820937":"markdown","97d686ef":"markdown","4a90c810":"markdown","7d9eb59a":"markdown","7c238338":"markdown","aa72e3bf":"markdown","4259de42":"markdown","5caea73c":"markdown","fa7db373":"markdown","01a0106e":"markdown","efa1b01f":"markdown","16f28a26":"markdown","3ab8f913":"markdown","38c5fa1b":"markdown","28c93736":"markdown","c420014a":"markdown","4a8a19a8":"markdown","c2b230d9":"markdown","6c66aa0c":"markdown","cceb8825":"markdown","b28f939c":"markdown","422d215b":"markdown","9aad8ad7":"markdown","5755688b":"markdown","a6351447":"markdown","a15629d9":"markdown","7083f41e":"markdown","2e4a262f":"markdown","59e51983":"markdown","b00a02f0":"markdown","c129aae7":"markdown","5b73a859":"markdown","9319f0d5":"markdown","013dc76c":"markdown","d2faa322":"markdown","9ddca755":"markdown","60af632b":"markdown","c1decb04":"markdown","a58036a4":"markdown","c4e80f6f":"markdown","3b453ffe":"markdown","f7ebd896":"markdown","e239e789":"markdown","94307c05":"markdown","9399786e":"markdown","7f9c9ece":"markdown","84334b7a":"markdown","e15ff80d":"markdown","044725fa":"markdown","c9b4719f":"markdown","fad48e48":"markdown","dfc3b66f":"markdown","7619e043":"markdown","433aa0c7":"markdown","83faedaa":"markdown","e1b03ed8":"markdown","2af8b03f":"markdown","efdbb512":"markdown","6822f876":"markdown","9db36ee3":"markdown","3d62d13b":"markdown"},"source":{"b91897d2":"!pip install nb_black watermark","b4b847e0":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# from sklearnex import patch_sklearn\n# patch_sklearn()\n\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n%load_ext lab_black\n%load_ext watermark\n\nsns.set()\n\n%watermark -v -m -p numpy,scipy,pandas,matplotlib,statsmodels,sklearn,catboost,xgboost,lightgbm,tensorflow -g","c2076a37":"from tqdm.notebook import tqdm\nimport os\n\n\nRANDOM_STATE = 42\nNUM_SPLITS = 5\ntarget_feature = \"item_cnt_month\"\nDATA_DIR = os.path.join(\"..\", \"input\", \"competitive-data-science-predict-future-sales\")","378e5e9b":"sales = pd.read_csv(os.path.join(DATA_DIR, \"sales_train.csv\"))\nprint(sales.shape)\nsales.head()","873ceca8":"items = pd.read_csv(os.path.join(DATA_DIR, \"items.csv\"))\nprint(items.shape)\nitems.head()","3463df47":"item_categories = pd.read_csv(os.path.join(DATA_DIR, \"item_categories.csv\"))\nprint(item_categories.shape)\nitem_categories.head()","3a6904e6":"shops = pd.read_csv(os.path.join(DATA_DIR, \"shops.csv\"))\nprint(shops.shape)\nshops.head()","9e3cf508":"print(\"Unique shops:\", sales[\"shop_id\"].nunique())\nprint(\"Unique items:\", sales[\"item_id\"].nunique())\nprint(\"Unique categories in items:\", items[\"item_category_id\"].nunique())","e897234f":"test = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\nprint(test.shape)\ntest.head()","9c861543":"set(test.shop_id.value_counts().index).difference(set(shops.shop_id))","d2dc7f0d":"set(test.item_id.value_counts().index).difference(set(items.item_id))","bba3cb6d":"len(set(test.item_id.value_counts().index).difference(set(sales.item_id)))","24720a6c":"sales.date = pd.to_datetime(sales.date, format=\"%d.%m.%Y\").dt.date\n\nprint(\"Total days:\", len(sales.date.value_counts()))\nprint(\"First:\", sales.date.min())\nprint(\"Last:\", sales.date.max())\n\nsales.date.max() - sales.date.min()","966ef894":"g_indexed = (\n    sales.groupby([\"shop_id\", \"item_id\", \"date_block_num\", \"item_price\"])\n    .agg([\"count\"])\n    .reset_index()\n    .set_index([\"shop_id\", \"item_id\", \"date_block_num\"])\n)\ng_indexed[g_indexed.index.duplicated(keep=False)]","4a53e84c":"sales[\"item_price\"].plot(kind=\"box\")","ea2fa7d2":"sales[\"item_price\"].value_counts().sort_index()","2d22930a":"sales.loc[sales[\"item_price\"].idxmax()]","83411c8a":"items.loc[sales.loc[sales[\"item_price\"].idxmax()].item_id]","14a81d23":"series = sales[\"item_price\"]\n\nQ1 = series.quantile(0.25)\nQ3 = series.quantile(0.75)\nIQR = Q3 - Q1\n\nsales[((series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR)))].shape","92ad99d5":"low = series.quantile(0.01)\nhigh = series.quantile(0.99)\n\nsales[(series < low) | (series > high)].shape","7280dbd9":"sales[\"item_price\"].plot(kind=\"box\", showfliers=False)","d45f8cd8":"with pd.option_context(\"display.float_format\", lambda x: \"%.2f\" % x):\n    print(sales[\"item_price\"].describe())","a20f0110":"sales[\"item_cnt_day\"].plot(kind=\"box\")","71ce9218":"sales[\"item_cnt_day\"].plot(kind=\"box\", showfliers=False)","acd8add4":"with pd.option_context(\"display.float_format\", lambda x: \"%.2f\" % x):\n    print(sales[\"item_cnt_day\"].describe())","5eae225f":"sales[\"item_cnt_day\"].value_counts().sort_index()","0b197d58":"sales.loc[sales[\"item_cnt_day\"].idxmax()]","dde8bda6":"items.loc[sales.loc[sales[\"item_cnt_day\"].idxmax()].item_id]","9d84e4d4":"shops.loc[sales.loc[sales[\"item_cnt_day\"].idxmax()].shop_id]","342e6431":"sales[sales[\"item_id\"] == sales.loc[sales[\"item_cnt_day\"].idxmax()].item_id][\n    \"item_cnt_day\"\n].mean()","5c50eda1":"sales[sales[\"item_cnt_day\"] == 1000]","43410f04":"items.loc[20949]","1b8947d2":"print(\"Before:\", sales.shape)\nsales = sales[\n    (sales[\"item_cnt_day\"] < 1000)\n    & (sales[\"item_price\"] > 0)\n    & (sales[\"item_price\"] < 60000)\n].copy()\nprint(\"After:\", sales.shape)","20106ed9":"def max_grad(series):\n    return series.diff().max()\n\n\ndef min_grad(series):\n    return series.diff().min()\n\n\ndef negative_sum(series):\n    return series[series < 0].sum()\n\n\ndef positive_sum(series):\n    return series[series > 0].sum()\n\n\ndef monthly_trend(series):\n    half = len(series) \/\/ 2\n    return series.iloc[half:].mean() - series.iloc[:half].mean()\n\n\ncommon_aggregates = [\n    \"sum\",\n    \"min\",\n    \"max\",\n    \"mean\",\n    \"median\",\n    \"std\",\n    max_grad,\n    min_grad,\n    monthly_trend,\n]\n\n# sort frame before group by to correctly calculate monthly trend\nsales_ex = (\n    sales.sort_values(by=[\"shop_id\", \"item_id\", \"date_block_num\", \"date\"])\n    .groupby([\"shop_id\", \"item_id\", \"date_block_num\"])\n    .agg(\n        {\n            \"item_cnt_day\": common_aggregates + [negative_sum, positive_sum],\n            \"item_price\": common_aggregates + [\"count\"],\n        }\n    )\n    .reset_index()\n)\nprint(sales_ex.shape)\nsales_ex.head()","5dbbdd5a":"sales_ex.columns = [\n    \"_\".join(col).strip().rstrip(\"_\") for col in sales_ex.columns.values\n]\nsales_ex.head()","70fc0dc7":"sales_ex.columns.tolist()","af47f5dc":"sales_ex.rename(\n    columns={\n        \"item_cnt_day_sum\": \"item_cnt_month\",\n        \"item_price_count\": \"num_transactions\",\n    },\n    inplace=True,\n)","724b3ebe":"# clip to match conditions\nsales_ex[\"item_cnt_month\"] = sales_ex[\"item_cnt_month\"].clip(0, 20)","63e50913":"sales_ex.columns.tolist()","7f258cb9":"numeric_features = {\n    \"date_block_num\",\n    \"item_cnt_day_min\",\n    \"item_cnt_day_max\",\n    \"item_cnt_day_mean\",\n    \"item_cnt_day_median\",\n    \"item_cnt_day_std\",\n    \"item_cnt_day_max_grad\",\n    \"item_cnt_day_min_grad\",\n    \"item_cnt_day_negative_sum\",\n    \"item_cnt_day_positive_sum\",\n    \"item_price_sum\",\n    \"item_price_min\",\n    \"item_price_max\",\n    \"item_price_mean\",\n    \"item_price_median\",\n    \"item_price_std\",\n    \"item_price_max_grad\",\n    \"item_price_min_grad\",\n    \"num_transactions\",\n    \"item_cnt_day_monthly_trend\",\n    \"item_price_monthly_trend\",\n}\n\ncat_features = set()","fe2b7bd1":"sales_ex.info()","fe75bc62":"def downcast_numeric(df):\n    for d_type in [\"float\", \"integer\"]:\n        cols = df.select_dtypes(d_type).columns\n        df[cols] = df[cols].apply(pd.to_numeric, downcast=d_type)","e721ee98":"downcast_numeric(sales_ex)","6eb6ad9f":"sales_ex[\"item_income_month\"] = sales_ex[\"item_cnt_month\"] * sales_ex[\"item_price_mean\"]","07d21002":"sales_ex.info()","0c155fcd":"sales_ex.to_pickle(\"sales_ex.pkl\")","a59ab338":"sales_ex = pd.read_pickle(\"sales_ex.pkl\")","66ce06ac":"last_month_agg = (\n    sales[sales.date_block_num == sales.date_block_num.max()]\n    .groupby([\"shop_id\", \"item_id\"])\n    .agg(\n        {\n            \"item_cnt_day\": [\"sum\"],\n        }\n    )\n    .reset_index()\n)\nprint(last_month_agg.shape)\nlast_month_agg.head()","9765146a":"last_month_agg.columns = [\n    \"_\".join(col).strip().rstrip(\"_\") for col in last_month_agg.columns.values\n]","2c4e0b29":"test_enriched = pd.merge(\n    test,\n    last_month_agg,\n    left_on=[\"shop_id\", \"item_id\"],\n    right_on=[\"shop_id\", \"item_id\"],\n    how=\"left\",\n)\nprint(test_enriched.shape)\ntest_enriched.head()\ntest_enriched.rename(columns={\"item_cnt_day_sum\": \"item_cnt_month\"}, inplace=True)","302801ea":"test_enriched.item_cnt_month.value_counts(dropna=False)","6e145d75":"test_enriched.item_cnt_month.fillna(0, inplace=True)","89896058":"test_enriched.item_cnt_month.clip(0, 20, inplace=True)","4d49170a":"test_enriched.to_csv(\n    \"prev_month_clip.csv\", index_label=\"ID\", columns=[\"item_cnt_month\"]\n)","b17b6523":"clip = 15\n\ntest_enriched.item_cnt_month.clip(0, clip, inplace=True)\ntest_enriched.to_csv(\n    f\"prev_month_clip_{clip}.csv\", index_label=\"ID\", columns=[\"item_cnt_month\"]\n)","2f3adc5d":"last_year_agg = (\n    sales[sales.date_block_num == sales.date_block_num.max() - 11]\n    .groupby([\"shop_id\", \"item_id\"])\n    .agg(\n        {\n            \"item_cnt_day\": [\"sum\"],\n        }\n    )\n    .reset_index()\n)\nprint(last_year_agg.shape)\nlast_year_agg.head()","3a772742":"last_year_agg.columns = [\n    \"_\".join(col).strip().rstrip(\"_\") for col in last_year_agg.columns.values\n]","9dfbdc01":"test_enriched = pd.merge(\n    test,\n    last_year_agg,\n    left_on=[\"shop_id\", \"item_id\"],\n    right_on=[\"shop_id\", \"item_id\"],\n    how=\"left\",\n)\nprint(test_enriched.shape)\ntest_enriched.head()\ntest_enriched.rename(columns={\"item_cnt_day_sum\": \"item_cnt_month\"}, inplace=True)","c324e35a":"test_enriched.item_cnt_month.value_counts(dropna=False)","3d523f41":"test_enriched.item_cnt_month.fillna(0, inplace=True)","b2889099":"clip = 10\n\ntest_enriched.item_cnt_month.clip(0, clip, inplace=True)\ntest_enriched.to_csv(\n    f\"last_year_clip_{clip}.csv\", index_label=\"ID\", columns=[\"item_cnt_month\"]\n)","e12845a6":"from itertools import product\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = []\nfor block_num in sales_ex[\"date_block_num\"].unique():\n    cur_shops = sales_ex[sales_ex[\"date_block_num\"] == block_num][\"shop_id\"].unique()\n    cur_items = sales_ex[sales_ex[\"date_block_num\"] == block_num][\"item_id\"].unique()\n    grid.append(list(product(cur_shops, cur_items, [block_num])))\n\nindex_cols = [\"shop_id\", \"item_id\", \"date_block_num\"]\ngrid = pd.DataFrame(np.vstack(grid), columns=index_cols)\n\nsales_enriched = pd.merge(grid, sales_ex, how=\"left\", on=index_cols)\nsales_enriched.item_cnt_month = sales_enriched.item_cnt_month.fillna(0)\n\nprint(sales_enriched.shape)\nsales_enriched.head()","3f49bbb8":"# replace NaNs with zeros\nsales_enriched[list(numeric_features)] = sales_enriched[list(numeric_features)].fillna(\n    0\n)","cb03140c":"test_block = sales_enriched.date_block_num.max() + 1","eccef524":"test[\"date_block_num\"] = test_block\n\ncombined = pd.concat([sales_enriched, test], axis=0)","d8deea31":"combined.reset_index(inplace=True)","37c6c3a8":"combined.drop(columns=[\"index\"], inplace=True)","b831e0c4":"combined[\"num_transactions\"] = pd.to_numeric(\n    combined[\"num_transactions\"], downcast=\"integer\"\n)","c81cf04f":"downcast_numeric(combined)","11105382":"combined.to_pickle(\"combined.pkl\")","00f3f2cd":"combined = pd.read_pickle(\"combined.pkl\")","8514acd0":"# speed vs memory\n# for col in numeric_features.difference({\"date_block_num\"}):\n#     combined[col] = combined[col].astype(pd.SparseDtype(\"float32\", 0))","27f6491a":"total_shop_sales = (\n    sales_ex.groupby([\"shop_id\", \"date_block_num\"])\n    .agg(\n        {\n            \"item_cnt_month\": [\"sum\"],\n        }\n    )\n    .reset_index()\n)\ntotal_shop_sales.columns = [\n    \"_\".join(col).strip().rstrip(\"_\") for col in total_shop_sales.columns.values\n]\ntotal_shop_sales.rename(\n    columns={\"item_cnt_month_sum\": \"total_monthly_shop_sales\"}, inplace=True\n)\nprint(total_shop_sales.shape)\ntotal_shop_sales.head()","098ee2fd":"fig, ax = plt.subplots(figsize=(10, 10))\n\nfor key, group in total_shop_sales.groupby([\"shop_id\"]):\n    ax.plot(group[\"date_block_num\"], group[\"total_monthly_shop_sales\"], label=key)\n\nplt.show()","56ba6a67":"total_shop_sales.loc[total_shop_sales.total_monthly_shop_sales.idxmax()]","ce5682eb":"numeric_features.add(\"total_monthly_shop_sales\")","831d04d5":"total_item_sales = (\n    sales_ex.groupby([\"item_id\", \"date_block_num\"])\n    .agg(\n        {\n            \"item_cnt_month\": [\"sum\"],\n        }\n    )\n    .reset_index()\n)\ntotal_item_sales.columns = [\n    \"_\".join(col).strip().rstrip(\"_\") for col in total_item_sales.columns.values\n]\ntotal_item_sales.rename(\n    columns={\"item_cnt_month_sum\": \"total_monthly_item_sales\"}, inplace=True\n)\nprint(total_item_sales.shape)\ntotal_item_sales.head()","091a0d7c":"total_item_sales.total_monthly_item_sales.value_counts()","81013b86":"fig, ax = plt.subplots(figsize=(10, 10))\n\nfor key, group in total_item_sales.groupby([\"item_id\"]):\n    ax.plot(group[\"date_block_num\"], group[\"total_monthly_item_sales\"], label=key)\n\nplt.show()","7b1d0350":"numeric_features.add(\"total_monthly_item_sales\")","315c93af":"sales[sales.date_block_num == 11].date","085c61ad":"combined = pd.merge(\n    combined, total_item_sales, on=[\"item_id\", \"date_block_num\"], how=\"left\"\n)\ncombined = pd.merge(\n    combined, total_shop_sales, on=[\"shop_id\", \"date_block_num\"], how=\"left\"\n)","b911857e":"combined.tail()","5fdaa2fb":"combined[\"total_monthly_shop_sales\"].value_counts(dropna=False).sort_index()","7de00ded":"combined.info()","ae0c9e80":"items.head()","b476ac6c":"item_categories.head()","bdad92df":"combined = pd.merge(combined, items, on=\"item_id\", how=\"left\")","94e636a0":"combined = pd.merge(combined, item_categories, on=\"item_category_id\", how=\"left\")","4a7975d0":"combined = pd.merge(combined, shops, on=\"shop_id\", how=\"left\")","07823944":"combined.columns","84889bf1":"cat_info = (\n    combined.groupby(\"item_category_name\")\n    .agg({\"item_cnt_month\": [\"sum\"]})\n    .sort_values(\n        (\"item_cnt_month\", \"sum\"),\n    )\n)\n\nwith pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    print(cat_info)","237c8bce":"combined.groupby(\"item_category_name\").agg(\n    {\"item_cnt_month\": [\"sum\"]}\n).unstack().plot.bar()","46392f5b":"def make_cat_name(value):\n    split = value.split(\" - \")\n    if len(split) > 1:\n        return split[0]\n\n    return value\n\n\ncombined[\"global_item_category_name\"] = combined[\"item_category_name\"].apply(\n    make_cat_name\n)","2f059b16":"cat_info = (\n    combined.groupby(\"global_item_category_name\")\n    .agg({\"item_cnt_month\": [\"sum\"]})\n    .sort_values(\n        (\"item_cnt_month\", \"sum\"),\n    )\n)\n\nwith pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    print(cat_info)","6fd3a245":"def make_cat_name(value):\n    force_category = {\n        \"PC - \u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438\": \"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\",\n        \"\u0418\u0433\u0440\u044b MAC - \u0426\u0438\u0444\u0440\u0430\": \"\u0418\u0433\u0440\u044b\",\n        \"\u0418\u0433\u0440\u044b Android - \u0426\u0438\u0444\u0440\u0430\": \"\u0418\u0433\u0440\u044b\",\n        \"\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)\": \"\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438\",\n        \"\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)\": \"\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438\",\n    }\n\n    if value in force_category:\n        return force_category[value]\n\n    split = value.split(\" - \")\n    if len(split) > 1:\n        return split[0]\n\n    return value\n\n\ncombined[\"global_item_category_name\"] = combined[\"item_category_name\"].apply(\n    make_cat_name\n)","6e80fd33":"cat_info = (\n    combined.groupby(\"global_item_category_name\")\n    .agg({\"item_cnt_month\": [\"sum\"]})\n    .sort_values(\n        (\"item_cnt_month\", \"sum\"),\n    )\n)\n\nwith pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    print(cat_info)","ddc12d36":"cat_info.unstack().plot.bar()","cad01481":"cat_features.add(\"global_item_category_name\")","f722ddd9":"combined.drop(columns=[\"item_category_name\"], inplace=True)","d17a21ea":"shop_info = (\n    combined.groupby(\"shop_name\")\n    .agg({\"item_cnt_month\": [\"sum\"]})\n    .sort_values(\n        (\"item_cnt_month\", \"sum\"),\n    )\n)\n\nwith pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    print(shop_info)","1dded78d":"def get_shop_type(value):\n    online_category = {\n        \"\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d \u0427\u0421\",\n        \"\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0441\u043a\u043b\u0430\u0434 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d\",\n    }\n    if value in online_category:\n        return \"Online\"\n\n    other_category = {\n        \"\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f\",\n        '\u041c\u043e\u0441\u043a\u0432\u0430 \"\u0420\u0430\u0441\u043f\u0440\u043e\u0434\u0430\u0436\u0430\"',\n    }\n    if value in other_category:\n        return \"Other\"\n\n    shop_category = {\n        \"\u0422\u0420\u0426\": \"Shopping and entertaiment center\",\n        \"\u0422\u0426\": \"Shopping center\",\n        \"\u0422\u0420\u041a\": \"Retail and entertaiment complex\",\n        \"\u0422\u041a\": \"Retail complex\",\n    }\n\n    for k, v in shop_category.items():\n        if k in value:\n            return v\n\n    return \"Shop\"\n\n\ncombined[\"shop_type\"] = combined[\"shop_name\"].apply(get_shop_type)","90015bb1":"shop_info = (\n    combined.groupby([\"shop_type\"])\n    .agg({\"item_cnt_month\": [\"sum\"]})\n    .sort_values(\n        (\"item_cnt_month\", \"sum\"),\n    )\n)\n\nwith pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    print(shop_info)","46f800fc":"shop_info.unstack().plot.bar()","6649b2ea":"cat_features.add(\"shop_type\")","6ce44eff":"def get_city(value):\n    unknown_city = {\n        \"\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f\",\n        \"\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d \u0427\u0421\",\n        \"\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0441\u043a\u043b\u0430\u0434 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d\",\n    }\n\n    if value in unknown_city:\n        return \"Unknown\"\n\n    return value.split()[0]\n\n\ncombined[\"shop_city\"] = combined[\"shop_name\"].apply(get_city)","bf459828":"shop_info = (\n    combined.groupby([\"shop_city\"])\n    .agg({\"item_cnt_month\": [\"sum\"]})\n    .sort_values(\n        (\"item_cnt_month\", \"sum\"),\n    )\n)\n\nwith pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    print(shop_info)","66708343":"shop_info.unstack().plot.bar()","31a355d3":"cat_features.add(\"shop_city\")","d1be824c":"combined.drop(columns=[\"shop_name\"], inplace=True)","2444aec7":"for f in cat_features:\n    combined[f] = combined[f].astype(\"category\")","ce130fec":"for f in [\"item_category_id\", \"shop_id\", \"item_id\"]:\n    combined[f] = combined[f].astype(\"category\")","d8140520":"combined.drop(columns=\"item_category_id\", inplace=True)","3b14e318":"combined.info()","e87b00e6":"combined.item_name.value_counts()","b29bb347":"MAX_TF_IDF = 20","a5033488":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntf_idf = TfidfVectorizer(ngram_range=(1, 3), max_features=MAX_TF_IDF)\ntf_idf_mat = tf_idf.fit_transform(items[\"item_name\"]).toarray()\nitems_text_features = pd.DataFrame(tf_idf_mat)\n\nfor i in range(MAX_TF_IDF):\n    feature_name = f\"tfidf_item_name_{i}\"\n    items[feature_name] = items_text_features[items_text_features.columns[i]]","b32b926f":"tf_idf.get_feature_names()","3d089a1c":"tfidf_features = [f\"tfidf_item_name_{i}\" for i in range(MAX_TF_IDF)]","0344b401":"items.head()","d31fe440":"combined = pd.merge(\n    combined, items[[\"item_id\"] + tfidf_features], on=\"item_id\", how=\"left\"\n)","57f5bae2":"combined.drop(columns=[\"item_name\"], inplace=True)","82f3f6c9":"combined.info()","280702ad":"downcast_numeric(combined)","0cf2547f":"combined.info()","48b4842c":"import gc\n\ngc.collect()","640debd6":"combined[\"item_id\"] = combined[\"item_id\"].astype(\"category\")","8c86823c":"cat_features","21c69f1e":"from sklearn.model_selection import KFold\n\n\nmeans_features = set()\ntrain_alias = combined[combined[\"date_block_num\"] != test_block]\nglobal_mean = train_alias[target_feature].mean()\ny_tr = train_alias[target_feature].values\n\nfor col in tqdm(cat_features):\n    col_tr = train_alias[[col] + [target_feature]].copy()\n\n    feature_name = col + \"_cnt_month_mean_expanding\"\n    means_features.add(feature_name)\n\n    # Expanding mean scheme\n    cumsum = col_tr.groupby(col)[target_feature].cumsum() - col_tr[target_feature]\n    cumcount = col_tr.groupby(col).cumcount()\n\n    col_tr[feature_name] = cumsum \/ cumcount\n    col_tr[feature_name].fillna(global_mean, inplace=True)\n\n    combined.loc[combined[\"date_block_num\"] != test_block, feature_name] = col_tr[\n        feature_name\n    ]\n\n    # cumulative means is for training, test is transforming just by means\n    #     https:\/\/stackoverflow.com\/questions\/60266373\/how-to-use-target-encoding-expanding-mean-on-the-test-set\n    mapper = col_tr.groupby(col)[target_feature].mean()\n    combined.loc[combined[\"date_block_num\"] == test_block, feature_name] = (\n        combined[combined[\"date_block_num\"] == test_block][col]\n        .map(mapper)\n        .astype(\"float\")\n    )\n\n    combined[feature_name].fillna(global_mean, inplace=True)","49a4ccf7":"means_features = [col + \"_cnt_month_mean_expanding\" for col in cat_features]","0532f570":"combined.info()","e6603841":"downcast_numeric(combined)","7f3565be":"combined.to_pickle(\"mean_encoded_clip.pkl\")","82c81466":"combined = pd.read_pickle(\"mean_encoded_clip.pkl\")","d23e7206":"numeric_features","232e2aca":"end_month = combined.date_block_num.max()","37f65581":"import gc\n\n\nlookback_range = [1, 2, 3, 4, 6, 12]\nto_look = list(numeric_features.difference({\"date_block_num\"}))\n\nlookback_features = {}\n\nfor diff in tqdm(lookback_range):\n    to_future = combined[combined[\"date_block_num\"] + diff <= end_month][\n        [\"shop_id\", \"item_id\", \"date_block_num\"] + to_look\n    ].copy()\n    to_future.date_block_num += diff\n\n    name_map = {f: f\"prev_{diff}_{f}\" for f in to_look}\n\n    to_future.rename(columns=name_map, inplace=True)\n    lookback_features[diff] = list(name_map.values())\n\n    combined = pd.merge(\n        combined, to_future, on=[\"shop_id\", \"item_id\", \"date_block_num\"], how=\"left\"\n    )\n    gc.collect()","186377bd":"from itertools import chain\n\n\nlookback_features_list = list(chain(*lookback_features.values()))","7dea382f":"combined[lookback_features_list] = combined[lookback_features_list].fillna(0)","50d63253":"downcast_numeric(combined)","763f2f4a":"combined[combined[\"date_block_num\"] < end_month].columns.duplicated().sum()","126cd069":"combined.info()","0b4b5337":"num_algo_features = (\n    [\"date_block_num\"] + lookback_features_list + tfidf_features + means_features\n)\n\nnum_algo_cat_features = list(cat_features)","cec8fcf6":"set(combined.columns.tolist()).difference(\n    set(num_algo_features + num_algo_cat_features)\n)","5ac629f0":"combined.to_pickle(\"encoded_with_lags_clip.pkl\")","5b2463d0":"import json\n\nwith open(\"all-features.json\", \"w\") as f:\n    json.dump({\"numeric\": num_algo_features, \"categorical\": num_algo_cat_features}, f)","205a4020":"combined = pd.read_pickle(\"encoded_with_lags_clip.pkl\")","44d8be47":"end_month = combined[\"date_block_num\"].max()","89bbf238":"print(\"train:\", len(combined[combined[\"date_block_num\"] < end_month - 1]))\nprint(\"validation:\", len(combined[combined[\"date_block_num\"] == end_month - 1]))\nprint(\"test:\", len(combined[combined[\"date_block_num\"] == end_month]))","bca45aaa":"combined[combined[\"date_block_num\"] < end_month - 1].to_pickle(\"train_clip.pkl\")\ncombined[combined[\"date_block_num\"] == end_month - 1].to_pickle(\"validation_clip.pkl\")\ncombined[combined[\"date_block_num\"] == end_month].to_pickle(\"test_clip.pkl\")","e5e97310":"train_shifted = combined[\n    (combined[\"date_block_num\"] > combined[\"date_block_num\"].min() + 12)\n    & (combined[\"date_block_num\"] < end_month - 1)\n]\n\nprint(\"train shifted:\", train_shifted.shape)\ntrain_shifted.to_pickle(\"train_shifted.pkl\")","2d42d3be":"del combined, train_shifted\n\ngc.collect()","a7ae8686":"train = pd.read_pickle(\"train_clip.pkl\")\ntrain_x, train_y = (\n    train[num_algo_features + num_algo_cat_features],\n    train[target_feature],\n)\n\nvalidation = pd.read_pickle(\"validation_clip.pkl\")\nvalidation_x, validation_y = (\n    train[num_algo_features + num_algo_cat_features],\n    train[target_feature],\n)","a97c1675":"num_cols = train_x.columns.get_indexer(num_algo_features)\ncat_cols = train_x.columns.get_indexer(num_algo_cat_features)","a81a7ca8":"test = pd.read_pickle(\"test_clip.pkl\")\ntest.set_index(\"ID\", inplace=True)","1745a669":"from catboost import CatBoostRegressor\n\n\nmodel = CatBoostRegressor(\n    random_seed=RANDOM_STATE,\n    depth=11,\n    iterations=1000,\n    learning_rate=0.01,\n    boosting_type=\"Plain\",\n    max_ctr_complexity=1,\n    task_type=\"GPU\",\n    devices=\"0:1\",\n    verbose=3,\n)","3489e067":"model.fit(\n    train_x,\n    train_y,\n    cat_features=num_algo_cat_features,\n    eval_set=(validation_x, validation_y),\n    # logging_level=\"Silent\",\n    # plot=True, not working in 0.25.1\n)","de65e275":"preds = model.predict(test[num_algo_features + num_algo_cat_features]).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_catboost.csv\")","88ae4e2a":"train = pd.read_pickle(\"train_shifted.pkl\")\ntrain_x, train_y = (\n    train[num_algo_features + num_algo_cat_features],\n    train[target_feature],\n)\n\nvalidation = pd.read_pickle(\"validation_clip.pkl\")\nvalidation_x, validation_y = (\n    train[num_algo_features + num_algo_cat_features],\n    train[target_feature],\n)\n\ntest = pd.read_pickle(\"test_clip.pkl\")\ntest.set_index(\"ID\", inplace=True)","f21bd7fe":"from catboost import CatBoostRegressor\n\n\nmodel = CatBoostRegressor(\n    random_seed=RANDOM_STATE,\n    depth=11,\n    iterations=1000,\n    learning_rate=0.01,\n    boosting_type=\"Plain\",\n    max_ctr_complexity=1,\n    task_type=\"GPU\",\n    devices=\"0:1\",\n    verbose=3,\n)","5b0be75c":"model.fit(\n    train_x,\n    train_y,\n    cat_features=num_algo_cat_features,\n    eval_set=(validation_x, validation_y),\n    # logging_level=\"Silent\",\n    # plot=True, not working in 0.25.1\n)","dad8ed1b":"preds = model.predict(test[num_algo_features + num_algo_cat_features]).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_catboost_shifted.csv\")","229e5edf":"import gc\n\n\ndel train, train_x, train_y, validation, validation_x, validation_y, test\ngc.collect()","fb3a882c":"from sklearn.preprocessing import OneHotEncoder\n\nall_features = num_algo_features + num_algo_cat_features\n\ntrain = pd.read_pickle(\"train_shifted.pkl\")\ntrain_x, train_y = train[all_features], train[target_feature]\nvalidation = pd.read_pickle(\"validation_clip.pkl\")\nvalidation_x, validation_y = validation[all_features], validation[target_feature]\n\ntest = pd.read_pickle(\"test_clip.pkl\")\ntest.set_index(\"ID\", inplace=True)\n\n\nencoder = OneHotEncoder().fit(train[num_algo_cat_features])\n\n\ndef convert_frame(df):\n    return pd.concat(\n        [\n            df[all_features],\n            pd.DataFrame.sparse.from_spmatrix(\n                encoder.transform(df[num_algo_cat_features]),\n                index=df.index,\n                columns=encoder.get_feature_names(),\n            ),\n        ],\n        axis=1,\n    ).drop(columns=num_algo_cat_features)\n\n\ntrain_x = convert_frame(train_x)\nvalidation_x = convert_frame(validation_x)\ntest = convert_frame(test)","4ced9e9c":"train_x.to_pickle(\"train_encoded_x.pkl\")\ntrain_y.to_pickle(\"train_y.pkl\")\n\nvalidation_x.to_pickle(\"validation_encoded_x.pkl\")\nvalidation_y.to_pickle(\"validation_encoded_y.pkl\")\n\ntest.to_pickle(\"test_encoded.pkl\")","2108d61f":"train_x = pd.read_pickle(\"train_encoded_x.pkl\")\ntrain_y = pd.read_pickle(\"train_y.pkl\")\n\nvalidation_x = pd.read_pickle(\"validation_encoded_x.pkl\")\nvalidation_y = pd.read_pickle(\"validation_encoded_y.pkl\")\n\ntest = pd.read_pickle(\"test_encoded.pkl\")","9aaf082b":"from xgboost import XGBRegressor\n\n\nmodel = XGBRegressor(max_depth=7, eta=0.2, num_round=100, seed=RANDOM_STATE,)\nmodel.fit(\n    train_x,\n    train_y,\n    eval_set=[(validation_x, validation_y)],\n    eval_metric=\"rmse\",\n    verbose=True,\n)","68750bfd":"preds = model.predict(test).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_xgb.csv\")","1b4754ee":"from catboost import CatBoostRegressor\n\n\nmodel = CatBoostRegressor(\n    random_seed=RANDOM_STATE,\n    depth=7,\n    iterations=1000,\n    learning_rate=0.01,\n    boosting_type=\"Plain\",\n    max_ctr_complexity=1,\n    task_type=\"GPU\",\n    devices=\"0:1\",\n    verbose=3,\n)\n\nmodel.fit(\n    train_x,\n    train_y,\n    eval_set=(validation_x, validation_y),\n    # logging_level=\"Silent\",\n    # plot=True, not working in 0.25.1\n)","b860fb14":"preds = model.predict(test).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_catboost_onehot.csv\")","00b56639":"import re\nfrom transliterate import translit\n\n\ntrain_x.columns = [\n    re.sub(\"[^A-Za-z0-9_]+\", \"\", translit(x, \"ru\", reversed=True))\n    for x in train_x.columns.tolist()\n]\nvalidation_x.columns = [\n    re.sub(\"[^A-Za-z0-9_]+\", \"\", translit(x, \"ru\", reversed=True))\n    for x in validation_x.columns.tolist()\n]\ntest.columns = [\n    re.sub(\"[^A-Za-z0-9_]+\", \"\", translit(x, \"ru\", reversed=True))\n    for x in test.columns.tolist()\n]","177c47dd":"train_x.to_pickle(\"train_encoded_x.pkl\")\ntrain_y.to_pickle(\"train_y.pkl\")\n\nvalidation_x.to_pickle(\"validation_encoded_x.pkl\")\nvalidation_y.to_pickle(\"validation_encoded_y.pkl\")\n\ntest.to_pickle(\"test_encoded.pkl\")","f09e5e82":"algo_cat_cols = [x for x in train_x.columns.tolist() if x.startswith(\"x\")]\nalgo_num_cols = [x for x in train_x.columns.tolist() if not x.startswith(\"x\")]","85e59d5f":"import lightgbm as lgb\n\n\nparams = {\n    \"feature_fraction\": 0.75,\n    \"metric\": \"rmse\",\n    \"min_data_in_leaf\": 2 ** 7,\n    \"bagging_fraction\": 0.75,\n    \"learning_rate\": 0.03,\n    \"objective\": \"mse\",\n    \"bagging_seed\": 2 ** 7,\n    \"num_leaves\": 2 ** 7,\n    \"bagging_freq\": 1,\n    \"verbose\": 1,\n}\n\nlgb_train = lgb.Dataset(train_x, train_y)\nlgb_eval = lgb.Dataset(validation_x, validation_y, reference=lgb_train)\n\nmodel = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=100,\n    valid_sets=lgb_eval,\n    early_stopping_rounds=10,\n)","c5c2ce66":"preds = model.predict(test, num_iteration=model.best_iteration).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_lgbm.csv\")","01ad53d9":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", MinMaxScaler(), algo_num_cols),\n        (\"cat\", \"passthrough\", algo_cat_cols),\n    ]\n)\n\npipe = make_pipeline(preprocessor, SGDRegressor(verbose=1))\n\npipe.fit(train_x, train_y)\npreds = pipe.predict(validation_x)\nprint(mean_squared_error(validation_y, preds, squared=False))","832e27e7":"preds = pipe.predict(test).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_sgd.csv\")","397b4111":"import tensorflow as tf\n\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))","f3e5a07f":"from tensorflow.keras import layers, models\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\n\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\n\ndef make_model():\n    model = models.Sequential()\n    model.add(\n        layers.Dense(\n            128, input_dim=len(algo_num_cols) + len(algo_cat_cols), activation=\"relu\"\n        )\n    )\n    model.add(layers.Dense(64, activation=\"relu\"))\n    model.add(layers.Dense(1))\n    model.compile(loss=root_mean_squared_error, optimizer=\"adam\")\n    return model\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", MinMaxScaler(), algo_num_cols),\n        (\"cat\", \"passthrough\", algo_cat_cols),\n    ]\n)\n\nvalidation_scaled = preprocessor.fit(train_x).transform(validation_x)\n\npipe = make_pipeline(\n    preprocessor,\n    KerasRegressor(\n        build_fn=make_model,\n        epochs=100,\n        batch_size=102400,\n        validation_data=(validation_scaled, validation_y),\n        verbose=1,\n    ),\n)\n\npipe.fit(train_x, train_y)","2c64d7b4":"preds = pipe.predict(test).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_nn.csv\")","ac3bec21":"train_x = pd.read_pickle(\"train_encoded_x.pkl\")\ntrain_y = pd.read_pickle(\"train_y.pkl\")\n\nvalidation_x = pd.read_pickle(\"validation_encoded_x.pkl\")\nvalidation_y = pd.read_pickle(\"validation_encoded_y.pkl\")\n\ntest = pd.read_pickle(\"test_encoded.pkl\")\n\nalgo_cat_cols = [x for x in train_x.columns.tolist() if x.startswith(\"x\")]\nalgo_num_cols = [x for x in train_x.columns.tolist() if not x.startswith(\"x\")]","11a02808":"from sklearn.model_selection import KFold, RandomizedSearchCV\nimport lightgbm as lgb\n\n\nNUM_SPLITS = 5\nNUM_ITER = 20\n\nparam_grid = {\n    \"max_depth\": [6, 7, 11],\n    \"learning_rate\": [0.1, 0.01, 0.03],\n    \"metric\": [\"rmse\"],\n    \"random_state\": [RANDOM_STATE],\n    \"bagging_freq\": [1],\n    \"feature_fraction\": [0.75, 1],\n    \"bagging_fraction\": [0.75, 1],\n    \"min_data_in_leaf\": [100],\n    \"num_leaves\": [100, 150],\n    \"num_iterations\": [20],\n    \"verbose\": [-1],\n}\n\ncv = KFold(n_splits=NUM_SPLITS, shuffle=False)\nclf = lgb.LGBMRegressor(verbose=-1)\n\ngrid = RandomizedSearchCV(\n    clf,\n    param_distributions=param_grid,\n    cv=cv,\n    verbose=3,\n    scoring=\"neg_root_mean_squared_error\",\n    n_iter=NUM_ITER,\n    random_state=RANDOM_STATE,\n)\ngrid.fit(train_x, train_y)\n\nprint(grid.best_params_, grid.best_score_)","68cf88fc":"best_lgb_params = {\n    \"verbose\": -1,\n    \"random_state\": 42,\n    \"num_leaves\": 150,\n    \"num_iterations\": 20,\n    \"min_data_in_leaf\": 100,\n    \"metric\": \"rmse\",\n    \"max_depth\": 11,\n    \"learning_rate\": 0.1,\n    \"feature_fraction\": 0.75,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.75,\n}","fb4717c6":"lgb_train = lgb.Dataset(train_x, train_y)\nlgb_eval = lgb.Dataset(validation_x, validation_y, reference=lgb_train)\n\nmodel = lgb.train(best_lgb_params, lgb_train, valid_sets=lgb_eval,)\n\npreds = model.predict(test, num_iteration=model.best_iteration).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_lgbm_tuned.csv\")","25ce5f16":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import KFold, RandomizedSearchCV\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", MinMaxScaler(), algo_num_cols),\n        (\"cat\", \"passthrough\", algo_cat_cols),\n    ]\n)\n\npipe = make_pipeline(preprocessor, SGDRegressor(verbose=1))\nNUM_SPLITS = 5\nNUM_ITER = 20\n\nparam_grid = {\n    \"sgdregressor__alpha\": np.linspace(0.0001, 2, 30),\n    \"sgdregressor__random_state\": [RANDOM_STATE],\n    \"sgdregressor__verbose\": [-1],\n}\n\ncv = KFold(n_splits=NUM_SPLITS, shuffle=False)\n\ngrid = RandomizedSearchCV(\n    pipe,\n    param_distributions=param_grid,\n    cv=cv,\n    verbose=3,\n    scoring=\"neg_root_mean_squared_error\",\n    n_iter=NUM_ITER,\n    random_state=RANDOM_STATE,\n    n_jobs=3,\n)\ngrid.fit(train_x, train_y)\n\nprint(grid.best_params_, grid.best_score_)","69a7a515":"best_sgd_params = {\n    \"verbose\": -1,\n    \"random_state\": 42,\n    \"alpha\": 0.0001,\n}","4d1acd87":"pipe = make_pipeline(preprocessor, SGDRegressor(**best_sgd_params))\npipe.fit(train_x, train_y)\n\npreds = pipe.predict(test).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_sgd_tuned.csv\")","1b69d673":"from tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\n\ndef make_model_1():\n    model = models.Sequential()\n    model.add(\n        layers.Dense(\n            128, input_dim=len(algo_num_cols) + len(algo_cat_cols), activation=\"relu\"\n        )\n    )\n    model.add(layers.Dense(1))\n    model.compile(loss=root_mean_squared_error, optimizer=\"adam\")\n    return model\n\n\ndef make_model_2():\n    model = models.Sequential()\n    model.add(\n        layers.Dense(\n            64, input_dim=len(algo_num_cols) + len(algo_cat_cols), activation=\"relu\"\n        )\n    )\n    model.add(layers.Dense(32))\n    model.add(layers.Dense(1))\n    model.compile(loss=root_mean_squared_error, optimizer=\"adam\")\n    return model\n\n\ndef make_model_3():\n    model = models.Sequential()\n    model.add(\n        layers.Dense(\n            128, input_dim=len(algo_num_cols) + len(algo_cat_cols), activation=\"relu\"\n        )\n    )\n    model.add(layers.Dropout(0.1))\n    model.add(layers.Dense(64))\n    model.add(layers.Dense(1))\n    model.compile(loss=root_mean_squared_error, optimizer=optimizers.SGD(momentum=0.1))\n    return model\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", MinMaxScaler(), algo_num_cols),\n        (\"cat\", \"passthrough\", algo_cat_cols),\n    ]\n)\n\nvalidation_scaled = preprocessor.fit(train_x).transform(validation_x)\n\nnetworks = []\n\n\nfor fn in [make_model_1, make_model_2, make_model_3]:\n    networks.append(\n        make_pipeline(\n            preprocessor,\n            KerasRegressor(\n                build_fn=fn,\n                epochs=100,\n                batch_size=102400,\n                validation_data=(validation_scaled, validation_y),\n                verbose=1,\n            ),\n        )\n    )","d510e9a0":"import gc\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n\ncv = KFold(n_splits=NUM_SPLITS, shuffle=False)\n\n\nscores = [[] for m in networks]\nnum_cv = 0\nfor train_index, test_index in cv.split(train_x, train_y):\n    gc.collect()\n    print(\"split:\", num_cv)\n    num_cv += 1\n    for idx, model in tqdm(list(enumerate(networks))):\n        model.fit(train_x.iloc[train_index], train_y.iloc[train_index])\n\n        train_preds = model.predict(train_x.iloc[train_index]).clip(0, 20)\n        test_preds = model.predict(train_x.iloc[test_index]).clip(0, 20)\n        scores[idx].append(\n            (\n                mean_squared_error(\n                    train_preds, train_y.iloc[train_index], squared=False\n                ),\n                mean_squared_error(test_preds, train_y.iloc[test_index], squared=False),\n            )\n        )","4d0afaa3":"def to_rows(label, scores):\n    train_scores, val_scores = list(zip(*scores))\n    return [\n        {\"label\": label, \"score\": np.array(train_scores).mean(), \"sample\": \"train\"},\n        {\"label\": label, \"score\": np.array(val_scores).mean(), \"sample\": \"validation\"},\n    ]\n\n\nrows = []\nrows.extend(to_rows(\"model1\", scores[::3]))\nrows.extend(to_rows(\"model2\", scores[1::3]))\nrows.extend(to_rows(\"model3\", scores[2::3]))","6094d4e0":"scores_df = pd.DataFrame.from_records(rows)\nsns.catplot(x=\"label\", y=\"score\", hue=\"sample\", data=scores_df, kind=\"bar\", height=6)","f906f8e3":"scores_df","1b126159":"nn_pipe = make_pipeline(\n    preprocessor,\n    KerasRegressor(\n        build_fn=make_model_1,\n        epochs=100,\n        batch_size=102400,\n        validation_data=(validation_scaled, validation_y),\n        verbose=1,\n    ),\n)\n\nnn_pipe.fit(train_x, train_y)\n\npreds = nn_pipe.predict(test).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_nn_tuned.csv\")","1a1a98ea":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\nrf = RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE, max_depth=7)\nrf.fit(train_x, train_y)\n\nval_preds = rf.predict(validation_x).clip(0, 20)\nprint(\"score:\", mean_squared_error(val_preds, validation_y))","f3a2ad81":"preds = rf.predict(test).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_rf_tuned.csv\")","0b2a06a7":"max_train_block = train_x[\"date_block_num\"].max()","891d6b33":"part_a, part_b = (\n    train_x[train_x.date_block_num < max_train_block],\n    train_x[train_x.date_block_num == max_train_block],\n)","2df95031":"part_a_y, part_b_y = (\n    train_y[train_x.date_block_num < max_train_block],\n    train_y[train_x.date_block_num == max_train_block],\n)","2b176472":"meta_df = pd.DataFrame(index=part_b.index.tolist() + validation_x.index.tolist())\ntest_meta_df = pd.DataFrame(index=test.index)","cb896838":"meta_df.shape[0] == part_b.shape[0] + validation_x.shape[0]","dbad4d76":"meta_df[\"date_block_num\"] = np.hstack(\n    [part_b.date_block_num, validation_x.date_block_num]\n)","dfc18015":"best_lgb_params = {\n    \"feature_fraction\": 0.75,\n    \"metric\": \"rmse\",\n    \"min_data_in_leaf\": 2 ** 7,\n    \"bagging_fraction\": 0.75,\n    \"learning_rate\": 0.03,\n    \"objective\": \"mse\",\n    \"bagging_seed\": 2 ** 7,\n    \"num_leaves\": 2 ** 7,\n    \"bagging_freq\": 1,\n    \"verbose\": 1,\n}","d25d900c":"import lightgbm as lgb\n\n\nlgb_part_a = lgb.Dataset(part_a, part_a_y)\nlgb_part_b = lgb.Dataset(part_b, part_b_y)\nlgb_part_c = lgb.Dataset(validation_x, validation_y)\n\n\nlgb_model = lgb.train(best_lgb_params, lgb_part_a)","7b71b821":"preds_b = lgb_model.predict(part_b, num_iteration=lgb_model.best_iteration).clip(0, 20)\npreds_c = lgb_model.predict(validation_x, num_iteration=lgb_model.best_iteration).clip(\n    0, 20\n)\npreds_test = lgb_model.predict(test, num_iteration=lgb_model.best_iteration).clip(0, 20)","4f990d33":"meta_df[\"lgb\"] = np.hstack([preds_b, preds_c])\ntest_meta_df[\"lgb\"] = preds_test","66216337":"lgb_model.save_model(\"lgb_classifier.txt\", num_iteration=lgb_model.best_iteration)","39615f97":"# lgb_model = lgb.Booster(model_file=\"lgb_classifier.txt\")","8d0f667f":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", MinMaxScaler(), algo_num_cols),\n        (\"cat\", \"passthrough\", algo_cat_cols),\n    ]\n)\n\nsgd_model = make_pipeline(preprocessor, SGDRegressor(**best_sgd_params))\nsgd_model.fit(part_a, part_a_y)","4a19bf77":"preds_b = sgd_model.predict(part_b).clip(0, 20)\npreds_c = sgd_model.predict(validation_x).clip(0, 20)\npreds_test = sgd_model.predict(test).clip(0, 20)","6611f24e":"meta_df[\"sgd\"] = np.hstack([preds_b, preds_c])\ntest_meta_df[\"sgd\"] = preds_test","f2028553":"import joblib\n\njoblib.dump(sgd_model, \"sgd_model.joblib\")","298cc090":"import joblib\n\n\nsgd_model = joblib.load(\"sgd_model.joblib\")","0a0a535b":"from tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\n\ndef make_model_1():\n    model = models.Sequential()\n    model.add(\n        layers.Dense(\n            128, input_dim=len(algo_num_cols) + len(algo_cat_cols), activation=\"relu\"\n        )\n    )\n    model.add(layers.Dense(1))\n    model.compile(loss=root_mean_squared_error, optimizer=\"adam\")\n    return model\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", MinMaxScaler(), algo_num_cols),\n        (\"cat\", \"passthrough\", algo_cat_cols),\n    ]\n)\n\nvalidation_scaled = preprocessor.fit(train_x).transform(validation_x)\n\nnn_model = make_pipeline(\n    preprocessor,\n    KerasRegressor(\n        build_fn=make_model_1,\n        epochs=100,\n        batch_size=102400,\n        validation_data=(validation_scaled, validation_y),\n        verbose=1,\n    ),\n)\n\nnn_model.fit(part_a, part_a_y)","a60809f0":"preds_b = nn_model.predict(part_b).clip(0, 20)\npreds_c = nn_model.predict(validation_x).clip(0, 20)\npreds_test = nn_model.predict(test).clip(0, 20)","62d4b5e7":"meta_df[\"nn\"] = np.hstack([preds_b, preds_c])\ntest_meta_df[\"nn\"] = preds_test","6b3b2447":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\nrf_model = RandomForestRegressor(\n    n_estimators=200, random_state=RANDOM_STATE, max_depth=7\n)\nrf_model.fit(part_a, part_a_y)","0034a4b2":"preds_b = rf_model.predict(part_b).clip(0, 20)\npreds_c = rf_model.predict(validation_x).clip(0, 20)\npreds_test = rf_model.predict(test).clip(0, 20)","0ffa6b76":"meta_df[\"rf\"] = np.hstack([preds_b, preds_c])\ntest_meta_df[\"rf\"] = preds_test","f3dfe73f":"import joblib\n\njoblib.dump(rf_model, \"rf_model.joblib\")","7bedb478":"import joblib\n\n\nrf_model = joblib.load(\"rf_model.joblib\")","a128ad04":"meta_df.head()","3a6219f1":"test_meta_df.head()","906e7605":"meta_df.to_pickle(\"meta_df.pkl\")","c5325556":"test_meta_df.to_pickle(\"test_meta_df.pkl\")","86f71200":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n\nmeta_features = [\"lgb\", \"sgd\", \"nn\", \"rf\"]\npart_b_meta = meta_df[meta_df.date_block_num == 32][meta_features]\npart_c_meta = meta_df[meta_df.date_block_num == 33][meta_features]\n\n\nalphas = np.linspace(0.01, 2, 30)\nbest_score = 0\nbest_alpha = 0\n\nfor alpha in tqdm(alphas):\n    clf = Ridge(alpha=alpha).fit(part_b_meta, part_b_y)\n    preds = clf.predict(part_c_meta).clip(0, 20)\n    score = mean_squared_error(preds, validation_y, squared=False)\n    if score > best_score:\n        best_score = score\n        best_alpha = alpha\n\nprint(\"Score:\", best_score)\nprint(\"Alpha:\", best_alpha)","0b36d09e":"meta_clf = Ridge(alpha=best_alpha).fit(\n    meta_df[meta_features], np.hstack([part_b_y, validation_y])\n)","55e82a76":"preds = meta_clf.predict(test_meta_df[meta_features]).clip(0, 20)\npreds_df = pd.DataFrame(preds, columns=[\"item_cnt_month\"])\npreds_df[\"ID\"] = test.index.astype(int)\npreds_df.set_index(\"ID\", inplace=True)\npreds_df.to_csv(\"test_preds_stacking.csv\")","dd35aa7e":"## Check result feature space","9bcba2f9":"The size is halved if we remove blocks from the first year","70a99830":"## SGDRegressor","b17c231b":"There are some items that do not exist in the training sample","4cb6daaa":"## Catboost onehot","6c02755a":"# Step 2. Feature engineering","2bb5f7bb":"We have two values to aggregate: price and count.\n\nNow we will check if the price can change in one month or it's constant for every item","5d3b14be":"## Making big dataset","fd1ddc6b":"## SGDRegressor","8bffeb7b":"### SGDRegressor","133740b1":"At this step we will tune 3 models: LightGBM, SGDRegressor and Neural Network using one-hot-encoded train dataset","e2a7e856":"## Diverse models","61f17dc1":"That's a lot","8126bd6e":"# Step 4. Tuning","4ef33ba8":"1.022547 and 1.021522","53ecd755":"## Previous value benchmark","d046686f":"Not good: 0.969629 and 0.970380","8ad5a590":"Better, there are still some small categories that can be merged","7504cc05":"## Save predictions","be87bd66":"### Train LGB","87970c10":"## Lag features","b79a00c0":"## Random Forest","a51c2e1e":"0.942457 and 0.938335","5fa3dc91":"## Meta model","0f001349":"1.000256 and 1.003907","3f3b04d8":"as the lag features we will use some statistics from previous sales for given periods\n\ndate_block_num needs to be excluded from past features","9992672f":"Next feature is item_name:\n\n```\nFeature extraction from text and images\n\nFeatures from text are extracted\nSpecial preprocessings for text are utilized (TF-IDF, stemming, levenshtening...)\n```","574f9592":"### Load data","db981efd":"## LightGBM","0efca0a9":"0.996570 and 0.994184","372675c9":"0.933830 and 0.932091","69dc66dd":"Simple holdout scheme\n\n1. Split train data into three parts: partA and partB and partC.\n2. Fit N diverse models on partA, predict for partB, partC, test_data getting meta-features partB_meta, partC_meta and test_meta respectively.\n3. Fit a metamodel to a partB_meta while validating its hyperparameters on partC_meta.\n4. When the metamodel is validated, fit it to [partB_meta, partC_meta] and predict for test_meta.","eff90abd":"## Converting data","b8dd354e":"Could be enought to just drop negative price and one very big sale","80467197":"Same for item sales, NaNs are in the test set only","1b236b2a":"Now it's way more better","77ce17fd":"We will use hold out approach. Train set should have date block less than 32, validation set ~ date block = 33 and test set = 34","cc83e760":"those features won't be used in the test set","2109f52d":"### Fix column names to avoid errors","569b8579":"How many outliers are there?","6a820937":"# Step 3. Building Models","97d686ef":"Clipped 20 values with zeroed NaNs = 1.16777 kaggle\n\nZeroed NaNs & No clip = around 8.5 kaggle\n\nZeroed NaNs & clip 25 = 1.202418 coursera\n\nZeroed NaNs & clip 19 = 1.161152 coursera\n\nZeroed NaNs & clip 18 = 1.155025 coursera\n\nZeroed NaNs & clip 17 = 1.149288 coursera\n\nZeroed NaNs & clip 16 = 1.143893 coursera\n\nZeroed NaNs & clip 15 = 1.138738 coursera\n\nZeroed NaNs & clip 10 = 1.123685 coursera\n\nZeroed NaNs & clip 5 = 1.135693 coursera\n\nOnes & Clip 20 give 1.39120 kaggle","4a90c810":"### Retrain & predict","7d9eb59a":"Someone wanted to make \u043f\u0430\u043a\u0435\u0442 \u0441 \u043f\u0430\u043a\u0435\u0442\u0430\u043c\u0438","7c238338":"that's clearly an outlier, can drop it","aa72e3bf":"# Step 5. Stacking","4259de42":"### Full","5caea73c":"### item_cnt_day","fa7db373":"Model 1 performs better on validation set during CV","01a0106e":"# Step 1. First look at the data","efa1b01f":"## Catboost","16f28a26":"## Tune LightGBM model","3ab8f913":"## XGBoost","38c5fa1b":"Public and private LB scores are: 0.951926 and 0.949549","28c93736":"The price can change, it might be related to sales or whatever, that is affecting target value too.\n\nWe need to add some feature there - something like price gradient that will let us know how was the price changed.","c420014a":"### Shifted","4a8a19a8":"It's messy, can see some trends","c2b230d9":"Public and private LB scores are: 1.022322 and 1.021003","6c66aa0c":"Previous results were better. Later I'll take params from simple lgb","cceb8825":"Some item categories are quite unpopular\n\nWe will merge them to make less categories and remove small categories. 1-3 rows make no sense","b28f939c":"During the plot analysis we will be looking for monthly trends in items, categories. Also need to check shop popularity, category popularity.\nAssuming there are no dates in the test set, trends info might not be useful at all.","422d215b":"Here we will encode categorical features","9aad8ad7":"### item_price","5755688b":"### Dropping outliers","a6351447":"Shop name consists of city and address\n\nSame thing, some shops are unpopular compared to others. That's related to their region\n\nMoreover, if we don't correctly predict for unpopular shops it almost won't affect score\n\nWhat features can be extracted from this data?\n1. City\n2. Type of shop: shopping center, shop, mall, etc\n\nAnd based on city and address we can calculate even more features including human activity, shop popularity","a15629d9":"## Processing categorical features\n\nThere are two categorical features:\n1. shop name, we can extract some features from it\n2. item category","7083f41e":"## Check for outliers","2e4a262f":"We will test 3 models","59e51983":"A lot of sales happens in December.\n\nData is seasonal.","b00a02f0":"This model achieves 0.954772 public and 0.952248 private score","c129aae7":"## Processing text features","5b73a859":"Converting numeric columns into sparse series gives tremendous change in file size, but also increases computation time\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/sparse.html","9319f0d5":"^ This gives 1.411213 public score on coursera","013dc76c":"There are 2935849 rows in the sales, 22170 items, 84 categories and 60 shops.","d2faa322":"The data looks quite unbalanced, there are a lot of sales in Moscow compared to other cities\n\nIn the case of not so good algorithms we can go further and extract features like mall traffic. ","9ddca755":"## Neural Network Tensorflow","60af632b":"scores are 0.936040 and 0.929004","c1decb04":"## Shared variables","a58036a4":"Feature names are mostly music\/games CDs, russian versions, etc","c4e80f6f":"### Retrain and predict","3b453ffe":"For the next steps i'll use shifted data and transform categorical data","f7ebd896":"Average parameters","e239e789":"### Neural Network","94307c05":"## Mean encodings","9399786e":"Current month columns won't be used to build algorithms","7f9c9ece":"# Goal\n\nThe Goal is to predict the total amount of products sold in the next month for every given pair (shop, product). That's regression problem\n\nWe also have no features in the test set except shop&item ids\n\nItem counts should be clipped into range [0, 20]\n\n## Feature space\n1. ID - an Id that represents a (Shop, Item) tuple within the test set\n1. shop_id - unique identifier of a shop\n1. item_id - unique identifier of a product\n1. item_category_id - unique identifier of item category\n1. item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n1. item_price - current price of an item\n1. date - date in format dd\/mm\/yyyy\n1. date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n1. item_name - name of item\n1. shop_name - name of shop\n1. item_category_name - name of item category","84334b7a":"## Neural Network","e15ff80d":"## Load Data","044725fa":"There we will plot some data and try to make features from the existing ones","c9b4719f":"### Random Forest","fad48e48":"0.935379 and 0.937202","dfc3b66f":"## Features construction\n\nThe following features are statistics based on historical data and some characteristics from time-series analysis","7619e043":"Soon it will take a lot more space in the memory","433aa0c7":"This model achieves 0.946026 public and 0.934927 private score","83faedaa":"That's weird, definitely an outlier. What about 1000?","e1b03ed8":"Someone has bought 2169 item deliveries","2af8b03f":"In the test test we have pairs shop_id & item_id. Need to reformat dataset to handle that representation","efdbb512":"It contains sales per day, need to aggregate it to monthly sales","6822f876":"### Retrain and predict","9db36ee3":"## Adding total sales features","3d62d13b":"### Light gbm"}}