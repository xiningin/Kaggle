{"cell_type":{"eae16561":"code","0c0a9648":"code","93f23f5b":"code","d1b7964b":"code","96b7e2ca":"code","6d090f0f":"code","ce52722d":"code","8e3601d7":"code","62427d3a":"code","927dc386":"code","6b4987d2":"code","15f0f73a":"code","a6913edb":"code","06aa88f7":"code","570aa696":"code","bcc7e218":"code","5ecbd0a9":"code","27d5ec6f":"code","fec5c382":"code","0284dc00":"code","90af389a":"code","82af9ad8":"code","c997b6d2":"code","a4202b06":"code","f9328786":"code","307cec9d":"code","e050d2bb":"code","d7cd1b64":"code","080c9c78":"code","e60505ae":"code","19bc4563":"code","c7c65757":"code","c5d65220":"code","ca140703":"code","ec5bb70e":"code","cdc06b60":"code","9fd71c8b":"code","c6bda42e":"code","bd7e7f42":"code","a2d2fdf3":"code","dac4006b":"code","f7da98d7":"code","34ca40fb":"code","3bf54795":"code","5e38fef4":"code","90ec395e":"code","5d4a1f9a":"code","f707747b":"code","81b4c529":"code","3f8aa5d8":"code","cb678a63":"code","b31de51c":"code","7dd3a3d9":"code","266b13d4":"code","c8490390":"code","486bd1c4":"code","a1a282f8":"code","868137f3":"code","37c603dc":"code","b3f57c98":"code","c1621a0c":"code","c440fcf4":"code","803c3adf":"code","a846bb8b":"code","8afaf843":"code","65351296":"code","979f0510":"code","3e8e6589":"code","52f62726":"code","1d29b57d":"code","87e8d503":"code","a90ef091":"code","154303c4":"code","b3216b1b":"code","b58dcc28":"code","efb1a0d9":"code","784f7def":"code","35c6a804":"code","019418d9":"code","b907a8d6":"code","f7260d04":"code","8cd41bda":"code","0dda6f99":"code","9d0e2ca2":"code","e7d1dff7":"code","a3fb24ed":"code","03f67078":"code","92766d68":"code","3802e286":"code","c4be4e7c":"markdown","1fc38345":"markdown","8ba9d9fb":"markdown","5883a267":"markdown","330a451e":"markdown","3e6afbfc":"markdown","0acfb1c4":"markdown","3e338d90":"markdown","5891795c":"markdown","22eb1705":"markdown","d5e5fe19":"markdown","15f47025":"markdown","37c2dabd":"markdown","b359597a":"markdown","0adc4e94":"markdown","51c1ce38":"markdown","cfcf9925":"markdown","a363c015":"markdown","bbf482f8":"markdown","be7db495":"markdown","0e3fe9c6":"markdown","21e76951":"markdown","51bc1667":"markdown","f89f0481":"markdown","ce5657f1":"markdown","18cf8dca":"markdown","4ed1d43b":"markdown","b85d4bf6":"markdown","5e147208":"markdown","87a5e166":"markdown","40b2c509":"markdown","75b35707":"markdown","bc0fe49c":"markdown","a14436e5":"markdown","6b63c777":"markdown","d09de757":"markdown","e00b2dcd":"markdown","de730b20":"markdown","d1bc5ffb":"markdown","8e8be7b1":"markdown","1e38b398":"markdown","66a00c54":"markdown","e7c0d9a4":"markdown","37d5763b":"markdown","8deb84e9":"markdown","84442464":"markdown","e5de5ce3":"markdown","5a6f1d70":"markdown"},"source":{"eae16561":"import re\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\n%matplotlib inline","0c0a9648":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve\nfrom sklearn.feature_selection import SelectKBest,chi2","93f23f5b":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","d1b7964b":"df_train['train'] = 1\ndf_test['train']  = 0\ndata = df_train.append(df_test, ignore_index=True)\ndata.info()","96b7e2ca":"sns.distplot(data['Age'].dropna())\nprint('Min = {}, Max= {}'.format(data['Age'].min(), data['Age'].max()))","6d090f0f":"sns.distplot(data['Fare'].dropna())\nprint('Min = {}, Max= {}'.format(data['Fare'].min(), data['Fare'].max()))","ce52722d":"data['Embarked'].value_counts()","8e3601d7":"data['Age'].fillna(data['Age'].mean(), inplace=True)\ndata['Fare'].fillna(data['Fare'].median(), inplace=True)\ndata['Embarked'].fillna('S', inplace=True)","62427d3a":"data.isnull().sum()","927dc386":"# Getting Training data from Full dataset\ntrain = data[data['train']==1]","6b4987d2":"train['Survived'].value_counts()","15f0f73a":"# chi-square test to test independance of two categorical varaible.\ndef chi_test_categorical_feature(alpha, feature, target='Survived'):\n    contigency_pclass = pd.crosstab(train[feature], train[target])\n    stat, p, dof, expected = chi2_contingency(contigency_pclass)\n    if p < alpha:\n        print('There is relationship btw {} and target variable with p_value = {} and Chi-squared = {}'.format(feature, p, stat) )\n    else:\n        print('not good predictor with p_value = {} and Chi-squared = {}'.format( p, stat))","a6913edb":"train['Sex'].value_counts()","06aa88f7":"sns.countplot(train['Sex'], hue=train['Survived'])","570aa696":"# for making contegancy table \npd.crosstab(train['Sex'], train['Survived'], normalize='all')*100","bcc7e218":"chi_test_categorical_feature(0.01, 'Sex')","5ecbd0a9":"train['Pclass'].value_counts()","27d5ec6f":"sns.countplot(train['Pclass'], hue=train['Survived'])","fec5c382":"pd.crosstab(train['Pclass'], train['Survived'], normalize='all')*100","0284dc00":"chi_test_categorical_feature(0.01, 'Pclass')","90af389a":"train['SibSp'].value_counts()","82af9ad8":"sns.countplot(train['SibSp'], hue=train['Survived'])","c997b6d2":"pd.crosstab(train['SibSp'], train['Survived'], normalize='all')*100","a4202b06":"chi_test_categorical_feature(0.01, 'SibSp')","f9328786":"train['Parch'].value_counts()","307cec9d":"sns.countplot(train['Parch'], hue=train['Survived'])","e050d2bb":"pd.crosstab(train['Parch'], train['Survived'], normalize='all')*100","d7cd1b64":"chi_test_categorical_feature(0.01, 'Parch')","080c9c78":"train['Embarked'].value_counts()","e60505ae":"sns.countplot(train['Embarked'], hue=train['Survived'])","19bc4563":"pd.crosstab(train['Embarked'], train['Survived'], normalize='all')*100","c7c65757":"chi_test_categorical_feature(0.01, 'Embarked')","c5d65220":"sns.boxplot(y=train['Age'], x=train['Survived'])","ca140703":"sns.violinplot(y=train['Age'], hue=train['Survived'], x=[\"\"]*len(train), palette=\"Set2\")","ec5bb70e":"train[['Age', 'Survived']].corr()","cdc06b60":"sns.boxplot(y=train['Fare'], x=train['Survived'])","9fd71c8b":"train[['Fare', 'Survived']].corr()","c6bda42e":"sns.violinplot(y=train['Fare'], hue=train['Survived'], x=[\"\"]*len(train), palette=\"Set2\")","bd7e7f42":"train[train['Cabin'].isnull()]['Survived'].value_counts()","a2d2fdf3":"train[train['Cabin'].isnull()==False]['Survived'].value_counts()","dac4006b":"pd.crosstab([train['Cabin'].isnull()], train['Survived'], normalize='all')*100","f7da98d7":"# creating featur fromcabin column if cabin exist then 1 else 0\ndata['Has_Cabin'] = ~data['Cabin'].isnull()\ndata['Has_Cabin'] = data['Has_Cabin'].astype(int)","34ca40fb":"train = data[['Has_Cabin', 'Survived']]\nsns.countplot(train['Has_Cabin'], hue=train['Survived'])","3bf54795":"# Creating feature FamilySize from SibSp and Parch\ndata['Family_Size'] = data['SibSp'] + data['Parch'] + 1","5e38fef4":"train = data[['Family_Size', 'Survived']]\nsns.countplot(train['Family_Size'], hue=train['Survived'])","90ec395e":"data['Is_Alone'] = data['Family_Size'].apply(lambda x: 1 if x==1 else 0)\n","5d4a1f9a":"train = data[['Is_Alone', 'Survived']]\nsns.countplot(train['Is_Alone'], hue=train['Survived'])","f707747b":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","81b4c529":"data['Title'] = data['Name'].apply(get_title)","3f8aa5d8":"data['Title'].value_counts()","cb678a63":"data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'OTHER')","b31de51c":"data['Title'] = data['Title'].replace('Mlle', 'Miss')\ndata['Title'] = data['Title'].replace('Ms', 'Miss')\ndata['Title'] = data['Title'].replace('Mme', 'Mrs')","7dd3a3d9":"train = data[['Title', 'Survived']]\nsns.countplot(train['Title'], hue=train['Survived'])","266b13d4":"data.info()","c8490390":"data['Age'].max()","486bd1c4":"bins = np.linspace(0, 80, 6)\ndata['Age_binned']= pd.cut(data['Age'], bins, labels=[1,2,3,4,5], include_lowest=True)\ndata['Age_binned'] = data['Age_binned'].astype(int)","a1a282f8":"train = data[['Age_binned', 'Survived']]\nsns.countplot(train['Age_binned'], hue=train['Survived'])","868137f3":"bins = [-1,50,100,390, 520]\ndata['Fare_binned'] = pd.cut(data['Fare'], bins ,labels=[1,2,3,4], include_lowest=True)\ndata['Fare_binned'] = data['Fare_binned'].astype(int)","37c603dc":"train = data[['Fare_binned', 'Survived']]\nsns.countplot(train['Fare_binned'], hue=train['Survived'])","b3f57c98":"data['Sex'].replace({'male': 1, 'female': 0}, inplace=True)\ndata['Embarked'].replace({'S': 1, 'C': 2, 'Q': 3}, inplace=True)\ndata['Title'].replace({'Mr': 1, 'Mrs': 2, 'Miss': 3, 'Master': 4, 'OTHER': 5}, inplace=True)","c1621a0c":"feature = ['Embarked','Pclass', 'Sex', 'SibSp','Parch', 'Has_Cabin', 'Family_Size', 'Is_Alone', 'Title', 'Age_binned', 'Fare_binned']","c440fcf4":"# converting feature to category so that we perform encoding on them.\ndata[feature] = data[feature].astype('category')\ndummy_data = pd.get_dummies(data[feature])\n# join with orginal dataset\ndata = pd.concat([data, dummy_data], axis=1)","803c3adf":"# Separating training and testing data. \ntraining_data = data[data['train']==1]\ntesting_data = data[data['train']==0]","a846bb8b":"# All features\nfeature_1 = ['Embarked_1', 'Embarked_2', 'Embarked_3', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_0', 'Sex_1',\n             'SibSp_0', 'SibSp_1', 'SibSp_2', 'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8', 'Parch_0', 'Parch_1',\n             'Parch_2','Parch_3', 'Parch_4', 'Parch_5', 'Parch_6', 'Parch_9','Has_Cabin_0', 'Has_Cabin_1', 'Family_Size_1',\n             'Family_Size_2', 'Family_Size_3', 'Family_Size_4', 'Family_Size_5','Family_Size_6', 'Family_Size_7',\n             'Family_Size_8', 'Family_Size_11','Is_Alone_0', 'Is_Alone_1', 'Title_1','Title_2', 'Title_3', 'Title_4',\n             'Title_5','Age_binned_1', 'Age_binned_2', 'Age_binned_3', 'Age_binned_4','Age_binned_5', 'Fare_binned_1',\n             'Fare_binned_2', 'Fare_binned_3','Fare_binned_4'] ","8afaf843":"feature_set = []\nchi2_selector = SelectKBest(chi2, k=30)\nchi2_selector.fit_transform(training_data[feature_1], y=training_data['Survived'].astype(int))\nfor feature, chi_result in zip(feature_1, chi2_selector.get_support()):\n    if chi_result==True:\n        feature_set.append(feature)","65351296":"train_accuracy = pd.DataFrame(columns=['Name of Model', 'Accuracy'])","979f0510":"seed = 101","3e8e6589":"# 1.Decision Tree Classifier\ndt = DecisionTreeClassifier(random_state = seed)\n\n# 2.Support Vector Machines\nsvc = SVC(gamma = 'auto')\n\n# 3.Random Forest Classifier\n\nrf = RandomForestClassifier(random_state = seed, n_estimators = 100)\n\n#4.Gaussian Naive Bayes\ngnb = GaussianNB()\n\n#5.Gradient Boosting Classifier\ngbc = GradientBoostingClassifier(random_state = seed)\n\n#6.Adaboost Classifier\nabc = AdaBoostClassifier(random_state = seed)\n\n#7.ExtraTrees Classifier\netc = ExtraTreesClassifier(random_state = seed)\n\n#10.Extreme Gradient Boosting\nxgbc = XGBClassifier(random_state = seed)\n\n","52f62726":"clf_list = [dt, svc, rf, gnb, gbc, abc, etc, xgbc]\nclf_list_name = ['dt', 'svc', 'rf', 'gnb', 'gbc', 'abc', 'etc', 'xgbc']","1d29b57d":"def train_accuracy_model(model):\n    model.fit(X_train, y_train)\n    accuracy = (model.score(X_train, y_train))*100\n    return accuracy","87e8d503":"# For Feature set 1 which countain all Feature.\nX_train = training_data[feature_set]\ny_train = training_data['Survived'].astype(int)\nfor clf, name in zip(clf_list, clf_list_name):\n    accuracy = train_accuracy_model(clf)\n    r = train_accuracy.shape[0]\n    train_accuracy.loc[r] = [name, accuracy]","a90ef091":"train_accuracy.sort_values(by='Accuracy', ascending=False)","154303c4":"cross_val_df = pd.DataFrame(columns=['Name of Model', 'Accuracy'])","b3216b1b":"def cross_val_accuracy(model):\n    score = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1).mean()\n    score = np.round(score*100, 2)\n    return score\n    ","b58dcc28":"# For Feature set 1 which countain all Feature.\nX_train = training_data[feature_set]\ny_train = training_data['Survived'].astype(int)\nfor clf, name in zip(clf_list, clf_list_name):\n    accuracy = cross_val_accuracy(clf)\n    r = cross_val_df.shape[0]\n    cross_val_df.loc[r] = [name, accuracy]","efb1a0d9":"cross_val_df.sort_values(by='Accuracy', ascending=False)","784f7def":"#Define dataframe for Parameter tuning.\n# Accuracy here is mean value of cross validation score of model with best paramters\nparam_df = pd.DataFrame(columns=['Name of Model', 'Accuracy', 'Parameter'])","35c6a804":"# For GBC, the following hyperparameters are usually tunned.\ngbc_params = {'learning_rate': [0.01, 0.02, 0.05, 0.01],\n              'max_depth': [4, 6, 8],\n              'max_features': [1.0, 0.3, 0.1], \n              'min_samples_split': [ 2, 3, 4],\n              'random_state':[seed]}\n\n# For SVC, the following hyperparameters are usually tunned.\nsvc_params = {'C': [6, 7, 8, 9, 10], \n              'kernel': ['linear','rbf'],\n              'gamma': [0.5, 0.2, 0.1, 0.001, 0.0001]}\n\n# For DT, the following hyperparameters are usually tunned.\ndt_params = {'max_features': ['auto', 'sqrt', 'log2'],\n             'min_samples_split': [2, 3, 4, 5, 6, 7, 8], \n             'min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8],\n             'random_state':[seed]}\n\n# For RF, the following hyperparameters are usually tunned.\nrf_params = {'criterion':['gini','entropy'],\n             'n_estimators':[10, 15, 20, 25, 30],\n             'min_samples_leaf':[1, 2, 3],\n             'min_samples_split':[3, 4, 5, 6, 7], \n             'max_features':['sqrt', 'auto', 'log2'],\n             'random_state':[44]}\n\n\n# For ABC, the following hyperparameters are usually tunned.'''\nabc_params = {'n_estimators':[1, 5, 10, 50, 100, 200],\n              'learning_rate':[0.001, 0.01, 0.1, 0.2, 0.3, 1.5],\n              'random_state':[seed]}\n\n# For ETC, the following hyperparameters are usually tunned.\netc_params = {'max_depth':[None],\n              'max_features':[1, 3, 10],\n              'min_samples_split':[2, 3, 10],\n              'min_samples_leaf':[1, 3, 10],\n              'bootstrap':[False],\n              'n_estimators':[100, 300],\n              'criterion':[\"gini\"], \n              'random_state':[seed]}\n\n# For XGBC, the following hyperparameters are usually tunned.\nxgbc_params = {'n_estimators': (150, 250, 350,450,550,650, 700, 800, 850, 1000),\n              'learning_rate': (0.01, 0.6),\n              'subsample': (0.3, 0.9),\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'colsample_bytree': (0.5, 0.9),\n              'min_child_weight': [1, 2, 3, 4],\n              'random_state':[seed]}\n","019418d9":"clf_list = [dt, svc, rf, gbc, abc, etc, xgbc]\nclf_list_name = ['dt', 'svc', 'rf', 'gbc', 'abc', 'etc', 'xgbc']\nclf_param_list = [dt_params, svc_params, rf_params, gbc_params, abc_params, etc_params, xgbc_params]","b907a8d6":"# Create a function to tune hyperparameters of the selected models.'''\ndef tune_hyperparameters(model, params):\n    from sklearn.model_selection import GridSearchCV\n    # Construct grid search object with 10 fold cross validation.\n    grid = GridSearchCV(model, params, verbose = 0, cv = 10, scoring = 'accuracy', n_jobs = -1)\n    # Fit using grid search.\n    grid.fit(X_train, y_train)\n    best_params, best_score = grid.best_params_, np.round(grid.best_score_*100, 2)\n    return best_params, best_score","f7260d04":"# Tuning Parameters of all Model\nX_train = training_data[feature_set]\ny_train = training_data['Survived'].astype(int)\nfor clf, name, params in zip(clf_list, clf_list_name, clf_param_list):\n    best_params, best_score = tune_hyperparameters(clf, params)\n    r = param_df.shape[0]\n    param_df.loc[r] = [name, best_score, best_params]","8cd41bda":"param_df.sort_values(by='Accuracy', ascending=False)\n","0dda6f99":"param_df.to_pickle(\".\/dummy.pkl\")","9d0e2ca2":"def ploting_learning_curve(model):\n    # Create CV training and test scores for various training set sizes\n    train_sizes, train_scores, test_scores = learning_curve(model, \n                                                        X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n\n    # Create means and standard deviations of training set scores\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n\n    # Create means and standard deviations of test set scores\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Draw lines\n    plt.plot(train_sizes, train_mean, '--',  label=\"Training score\")\n    plt.plot(train_sizes, test_mean, label=\"Cross-validation score\")\n\n    # Draw bands\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n    # Create plot\n    plt.title(\"Learning Curve\")\n    plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.show()\n","e7d1dff7":"#  RandomFrostClassifier Learning curve\nmodel = RandomForestClassifier(criterion='entropy', max_features='log2', min_samples_leaf=1, min_samples_split=7,\n                      n_estimators=20, random_state = 44)\nploting_learning_curve(model)","a3fb24ed":"model = XGBClassifier(colsample_bytree= 0.9, learning_rate= 0.01, max_depth= 6, min_child_weight= 2,\n                      n_estimators= 1000, random_state= 101, subsample= 0.3)\nploting_learning_curve(model)","03f67078":"model = GradientBoostingClassifier(learning_rate= 0.02, max_depth= 4, max_features= 0.3, min_samples_split= 4,\n                                   random_state= 101)\nploting_learning_curve(model)","92766d68":"X_train = training_data[feature_set]\ny_train = training_data['Survived'].astype(int)\nmodel = XGBClassifier(colsample_bytree= 0.9, learning_rate= 0.01, max_depth= 6, min_child_weight= 2,\n                      n_estimators= 1000, random_state= 101, subsample= 0.3)\n\n\nmodel.fit(X_train, y_train)","3802e286":"X_test=testing_data[feature_set]\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': testing_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)","c4be4e7c":"## Eploratory Data Analysis of Traning Data","1fc38345":"## Machine Learning","8ba9d9fb":"All Features that we get after feature engineering.","5883a267":"## Import Libraries","330a451e":"### Fare","3e6afbfc":"As we can see very weak correlation btw Age and Survived feature.we can create bins of age feature to make it more useful.\n","0acfb1c4":"### Ploting Learning Curve","3e338d90":"Here we also seen that person with one children or parent are more likely to survived.","5891795c":"Now Gradient-Boost Calssifier and Xgbc is doing best among all other model. So let try hyperparameter Tuning to select model.","22eb1705":"The dataset is pretty clean with Only Three columns **Age, Ebmarked, Fare'** have Some missing values and **Cabin** has lots of missing values.","d5e5fe19":"### Cabin","15f47025":"## Feature Engineering","37c2dabd":"Looks like all the tree based models have highest train accuracy followed by SVC and GBN.But train accuracy of a model is not enough to tell if a model can be able to generalize the unseen data or not. so We perform Cross Validation. ","b359597a":"As age is float varibale to so we perform different analysis on this.","0adc4e94":"As we can see the Mrs, Miss, Master are more likely to survived so it is good feature for making prediction.","51c1ce38":"The outcome is not equally distributed instead it skewed class.In Our training set out of 819 only 342 peoples managed to survived.","cfcf9925":"As Standard deviation of XGBC is less and accuracy is also high enough so we used it for Final submission.","a363c015":"There is  stong relationship btw **Sex** and **Survived**","bbf482f8":"Distribution is almost Normal Unimodel with right skewed. we can fill Age with mean.  ","be7db495":"Now we Explore training data so we can find feature that have more impact on survival of passenger.","0e3fe9c6":"### Final submission","21e76951":"Here we seen that Person that has one sibling or spouses are more likly to survived. All other category have less likely to survived.","51bc1667":"## DataTypes  &  Binning","f89f0481":"### Embarked","ce5657f1":"### Sex ","18cf8dca":"### Hyperparameter Tunning","4ed1d43b":"## Data Transformation","b85d4bf6":"### Pclass","5e147208":"## Filling Missing value","87a5e166":"We can Create Feature Like family_size which can be helpin predicting outcome.","40b2c509":"### SibSp","75b35707":"### Age","bc0fe49c":"Import training and testing dataset and combine them for filling missing value and for feature engineering.","a14436e5":"### Parch","6b63c777":"As fare increase mean higher Pclass so more chance of survival.","d09de757":"Let first explore them a little bit.","e00b2dcd":"#### Selecting features By Chi-Squared Method.","de730b20":"### Cross validation.","d1bc5ffb":"# Titanic Machine Learning compitetion","8e8be7b1":"Fill missing value in Embarked with most common value **(S)**.","1e38b398":"Person who get in titanic from **C** is more likely to survived then other station.","66a00c54":"## Import DataSet","e7c0d9a4":"### Training Model","37d5763b":"The distribution is Unimodel with right skewed and has some outlier on right side. We must fill missing value with medium because mean is skewed because of outlier.","8deb84e9":"From above boxplot we can not seen any clear difference between distribution of survived and died.","84442464":"As **Cabin** as lots of missing value so we are going to leave as it now and think about it in feature engineering process. Fill missing values of Other three features.","e5de5ce3":"The Person That have more then Four sibling or Spouses are very less likely to survived.","5a6f1d70":"Person that have cabin feature is more likely to survive then person that have missing cabin."}}