{"cell_type":{"827c49aa":"code","f078f58d":"code","4a88b220":"code","a592bfbf":"code","b5356277":"code","963ed2f1":"code","54e0989a":"code","3b6d5e2b":"code","fc5b84f8":"code","ffc00251":"code","1af5b806":"code","e3378f18":"code","3a927d9f":"code","48928223":"code","c2cd1b0e":"code","878a2f06":"code","c3acaf41":"code","e34a0d07":"code","1edd23d3":"code","bec64f25":"code","83ae1ef6":"code","3cd299c7":"code","e57604eb":"code","80e69814":"code","85e6a320":"code","12ee31df":"code","9e8e1fe2":"code","f7bb94a9":"code","842cf37f":"code","2b4c08be":"code","563e9f7a":"code","8d0ce42f":"code","7237fd7f":"code","42be18cd":"code","d2f48dc5":"code","e8e06f1b":"code","3d122850":"code","4418b96e":"code","87a83e43":"code","4811e2b1":"code","689cfd9d":"code","03652440":"code","059b8c68":"markdown","d4d06d0c":"markdown","a903442c":"markdown","e7d8e49a":"markdown","cba640fe":"markdown","7ccf95d5":"markdown","796ac48e":"markdown","d9714475":"markdown","13f74f08":"markdown"},"source":{"827c49aa":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore warning (from sklearn and seaborn)\n\nimport os\nprint(os.listdir(\"..\/input\"))","f078f58d":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/dnsprechallenge\/train.csv')\ntest = pd.read_csv('..\/input\/dnsprechallenge\/test.csv')\n","4a88b220":"##display the first five rows of the train dataset.\ntrain.head(5)","a592bfbf":"##display the first five rows of the train dataset.\ntest.head(5)","b5356277":"#Lets check the number of features in our train and test set\nprint('Train size is {}'.format(train.shape))\nprint('Test size is {}'.format(test.shape))","963ed2f1":"#we'll drop some columns as they are unneccessary in the prediction process\nid_cols = ['Product_Supermarket_Identifier']\n\ntrain.drop(id_cols, axis=1, inplace=True)\ntest.drop(id_cols, axis=1, inplace=True)\n\nprint('Train size after dropping three columns is {}'.format(train.shape))\nprint('Test size after dropping three columns  is {}'.format(test.shape))","54e0989a":"# #Check the distribution\n# sns.distplot(train['Product_Supermarket_Sales'], fit=norm)\n# fig = plt.figure()\n# res = stats.probplot(train['Product_Supermarket_Sales'], plot=plt)","3b6d5e2b":"# #applying log transformation\n# train['Product_Supermarket_Sales'] = np.log10(train['Product_Supermarket_Sales'])","fc5b84f8":"# #Check the distribution\n# sns.distplot(train['Product_Supermarket_Sales'], fit=norm)\n# fig = plt.figure()\n# res = stats.probplot(train['Product_Supermarket_Sales'], plot=plt)","ffc00251":"# #Lets plot some heatmap to find correlation among the features\n# corrmat = train.corr()\n# f, ax = plt.subplots(figsize=(5,4))\n# sns.heatmap(corrmat, square=True)","1af5b806":"#Get percentage of missing data\ntrain_missing = (train.isnull().sum() \/ len(train)) * 100\ntrain_missing = train_missing.drop(train_missing[train_missing == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Percentage' : train_missing})\nmissing_data","e3378f18":"#scatterplot of all features\ncat_col = ['Product_Fat_Content','Product_Type','Supermarket_Location_Type','Supermarket_Type']\nfor col in cat_col: \n    sns.set()\n    cols = ['Product_Identifier', 'Supermarket_Identifier',\n           'Product_Fat_Content', 'Product_Shelf_Visibility', 'Product_Type',\n           'Product_Price', 'Supermarket_Opening_Year',\n           'Supermarket_Location_Type', 'Supermarket_Type',\n           'Product_Supermarket_Sales']\n    plt.figure()\n    sns.pairplot(train[cols], size = 3.0, hue=col)\n    plt.show()\n","3a927d9f":"#concatenate train and test sets\nntrain = train.shape[0]\nntest = test.shape[0]\n\n#get target variable\ny_train = train.Product_Supermarket_Sales.values\n\nall_data = pd.concat((train,test)).reset_index(drop=True)\n#drop target variable\nall_data.drop(['Product_Supermarket_Sales'], axis=1, inplace=True)\nprint(\"Total data size is : {}\".format(all_data.shape))","48928223":"#Get percentage of missing data\nall_data_nan = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_nan = all_data_nan.drop(all_data_nan[all_data_nan == 0].index).sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'Missing Percentage' : all_data_nan})\nmissing_data","c2cd1b0e":"def fill_nan_supermarket_size(mkt_type, mk_location, val):\n    temp_df = all_data['Supermarket _Size'].loc[(all_data['Supermarket_Type']==  mkt_type ) & (all_data['Supermarket_Location_Type'] == mk_location)]\n    temp_df.fillna(value=val, axis=0, inplace=True)\n    all_data['Supermarket _Size'].loc[(all_data['Supermarket_Type']== mkt_type) & (all_data['Supermarket_Location_Type'] == mk_location)] = temp_df\n    return 'Done'\n\n    ","878a2f06":"# Fill all nan in Supermarket_size according to categories\nfill_nan_supermarket_size('Grocery Store','Cluster 3', 'Medium')\nfill_nan_supermarket_size('Supermarket Type3','Cluster 3', 'Medium')\nfill_nan_supermarket_size('Supermarket Type2','Cluster 3', 'Medium')\nfill_nan_supermarket_size('Supermarket Type1','Cluster 3', 'High')\nfill_nan_supermarket_size('Supermarket Type1','Cluster 2', 'Small')\nfill_nan_supermarket_size('Supermarket Type1','Cluster 1', 'Medium')\n","c3acaf41":"#Lets check the missing data percentage again\nall_data_nan = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_nan = all_data_nan.drop(all_data_nan[all_data_nan == 0].index).sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'Missing Percentage' : all_data_nan})\nmissing_data","e34a0d07":"all_data.head()","1edd23d3":"#Create the log version of product price\nall_data['Product_Price_log'] = np.log1p(all_data['Product_Price'])\nall_data['Product_Price_sqrt'] = np.sqrt(all_data['Product_Price'])\nall_data['Product_Price_square'] = np.square(all_data['Product_Price'])\n\n\n#Create some cross features\nall_data['cross_Price_weight'] = all_data['Product_Price'] * all_data['Product_Weight']\nall_data['cross_Price_visibility'] = all_data['Product_Price'] * all_data['Product_Shelf_Visibility']\nall_data['cross_Price_visibility_weight'] = all_data['Product_Price'] * all_data['Product_Shelf_Visibility'] * all_data['Product_Weight']\n","bec64f25":"all_data.head()","83ae1ef6":"#change opening year to categories to remove \ntrain['Supermarket_Opening_Year'].unique()","3cd299c7":"#Supermarket size is a categorical feature.\ndict_mkt_size = {'Small':1,'Medium':2,'High': 3}\ndict_fat_content = {'Ultra Low fat': 1,'Low Fat': 2,'Normal Fat':3}\ndict_year = {2005:'A', 1994:'B', 2014:'C', 2016:'D', 2011:'E', 2009:'F', 1992:'G', 2006:'H', 2004:'I'}\n\nall_data['Supermarket _Size'] = all_data['Supermarket _Size'].map(dict_mkt_size)\nall_data['Product_Fat_Content'] = all_data['Product_Fat_Content'].map(dict_fat_content)\nall_data['Supermarket_Opening_Year'] = all_data['Supermarket_Opening_Year'].map(dict_year)\n","e57604eb":"all_data.head()","80e69814":"X = pd.get_dummies(all_data)\nprint('All data size: ' + str(X.shape))\n","85e6a320":"import random\n#Lets get the new train and test set\ntrain = X[:ntrain]\ntest = X[ntrain:]\n\n#Let's shuffle our train set and labels\n# random_indx = np.arange(ntrain)\n# np.random.shuffle(random_indx)\n\n# train = np.array(train)[random_indx]\n# y_train = y_train[random_indx]\n\n#Get the columns for importance plot\ncols_4_imp = train.columns\n\nprint('Train size: ' + str(train.shape))\nprint('Test size: ' + str(test.shape))","12ee31df":"train.to_csv(\"DSN_Supermarket_data_ensemble_train_4.csv\", index=False)\ntest.to_csv(\"DSN_Supermarket_data_ensemble_test_4.csv\", index=False)","9e8e1fe2":"from sklearn.preprocessing import StandardScaler,Imputer\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport xgboost as xgb\nimport lightgbm as lgb\n","f7bb94a9":"imp = Imputer()\nimp.fit(train)\ntrain = imp.transform(train)\ntest = imp.transform(test)","842cf37f":"#Scale features\nscaler = RobustScaler()\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)","2b4c08be":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train)\n    rmse= np.sqrt(-cross_val_score(model, train, np.expm1(y_train), scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","563e9f7a":"xgb1 = xgb.XGBRegressor(\n learning_rate =0.01,\n n_estimators=20000,\n max_depth=4,\n min_child_weight=8,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0.1,\n nthread=4,\n scale_pos_weight=1,\n seed=27)","8d0ce42f":"# print(rmsle_cv(xgb1))","7237fd7f":"# xgb1.fit(train,y_train)","42be18cd":"# imp_feats = pd.DataFrame({'features':cols_4_imp,\"importance\": xgb1.feature_importances_})\n# print(imp_feats)","d2f48dc5":"# model_lgb = lgb.LGBMRegressor(learning_rate=0.01,n_estimators=5000)","e8e06f1b":"# print(rmsle_cv(model_lgb))","3d122850":"# model_lgb.fit(train,y_train)","4418b96e":"# avg_pred = ( 0.5 * xgb1.predict(test)) + (0.5 * model_lgb.predict(test))\n# print(\"Mean Absolute Error : \" + str(mean_absolute_error(avg_pred, y_test)))","87a83e43":"# final = np.expm1(xgb1.predict(test))","4811e2b1":"# df = pd.read_csv('..\/input\/dsn2018intercampus\/SampleSubmission.csv')\n# sub = df.drop('Product_Supermarket_Sales', axis=1)\n# sub['Product_Supermarket_Sales'] = final","689cfd9d":"# sub.head()","03652440":"# sub.to_csv('fe2_submission.csv', index=False)","059b8c68":"**LET'S DO SOME FEATURE ENGINEERING**","d4d06d0c":"\n\nWe'll use the sklearn input function to take care of Product Weight","a903442c":"> **Now let's MODEL**","e7d8e49a":"From the plot above, we can confirm that an increase in price of product really makes Total sales increase. Also there seems to be a very little trend in the Supermarket  opening year and the total sales, otherwise no other feature really correlates with Total sales","cba640fe":"**Let's take care of MISSING DATA**","7ccf95d5":"**LET'S DO SOME EDA**\n\nProduct_Supermarket_Sales is the target variable we are trying to predict, so lets explore it.","796ac48e":"At a glance we can see that the highest correlated feature to Product_Supermaket_Sales is Product_Price followed by Supermaket_Opening_Year. And that makes sense because the more a shop sells  expensive goods the higher their total sales get.\nAnother observation is that it seems the year of opening also has some correlation with product sales. Lets plot some one to one plot to see if this is a negative or positive trend .\n","d9714475":"**Feature Engineering**","13f74f08":"So far we only have two columns with missing values.\n\n>Since our features are small, I studied the mising columns and came up with the following conclusion for the supermarket_size column.\n\n*  Grocery store in cluster 1 has a store size of small\n* Grocery store in cluster 3 has a store size of medium\n\n* Supermarket_Type_2 store in cluster 3 has a store size of medium\n\n* Supermarket_Type_3 store in cluster 3 has a store size of medium\n\n* Supermarket_Type_1 store in cluster 1 has a store size of medium\n* Supermarket_Type_1 store in cluster 2 has a store size of small\n* Supermarket_Type_1 store in cluster 3 has a store size of high\n\nThis info will be used to fill the empty cells in Supermarket_size column."}}