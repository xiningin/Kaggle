{"cell_type":{"364eb5ee":"code","8a20e446":"code","ae98902c":"code","1e2a6482":"code","b8db1200":"code","12c58dfe":"code","89bba5a4":"code","d9559828":"code","3a7fe684":"code","20b26527":"code","abc8dfe5":"code","45aa90b5":"code","9bfff185":"code","c33348d6":"code","68c7db4c":"code","9721a1c5":"code","01cfc980":"code","a3429129":"code","eba7ee18":"code","a29abe40":"code","e359540e":"code","2ec457f3":"code","8b3c082e":"code","ecf3084d":"code","79ed132c":"code","56a88d26":"code","cfd44910":"code","de9c6f64":"code","deb67238":"code","1fd85905":"code","62b5f67e":"code","f5017f59":"code","e906fa6d":"code","ba16c3c3":"code","9c8e357d":"code","9bef93ac":"code","9512247d":"code","710ded76":"code","4d1992b6":"code","facf7109":"code","a2cfee47":"code","d15c51f8":"code","33be2b55":"code","cfc2b97a":"code","1a343a9e":"code","f6f1a059":"code","79438693":"code","860ed46a":"markdown","f5a98517":"markdown","58dc1dbf":"markdown","3dfd1996":"markdown","f789438f":"markdown","49be67c0":"markdown","cdb3ac44":"markdown","f111dc0f":"markdown","a2526d51":"markdown","432ca65d":"markdown","b78a2430":"markdown","651ecf22":"markdown","963a2e4e":"markdown","b49429e8":"markdown"},"source":{"364eb5ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nprint(f\"The above prediction is correct as it has identified the person name {artist_name} with the given image\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8a20e446":"import numpy as np\nimport os\n\nclass IdentityMetadata():\n    def __init__(self, base, name, file):\n        #print(base, name, file)\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path):\n    metadata = []\n    for i in os.listdir(path):\n        for f in os.listdir(os.path.join(path, i)):\n            # Check file extension. Allow only jpg\/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\n# metadata = load_metadata('images')\nmetadata = load_metadata('\/kaggle\/input\/aligned-face-dataset-from-pinterest\/Aligned Face Dataset from Pinterest\/PINS')","ae98902c":"metadata","1e2a6482":"pathimg = str(metadata[210])\npathimg","b8db1200":"import cv2 # opencv\nimg = cv2.imread(pathimg,1)","12c58dfe":"from matplotlib import pyplot as plt\nplt.imshow(img, cmap='gray', interpolation='bicubic')\nplt.xticks([])\nplt.yticks([])\nplt.show()","89bba5a4":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n\ndef vgg_face():\t\n    model = Sequential()\n    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(2622, (1, 1)))\n    model.add(Flatten())\n    model.add(Activation('softmax'))\n    return model","d9559828":"# Loading Model weights\nmodel = vgg_face()\n# Loading Model weights\nWEIGHTS_FILE = \"\/kaggle\/input\/aligned-face-dataset-from-pinterest\/vgg_face_weights.h5\"\nmodel.load_weights(WEIGHTS_FILE)","3a7fe684":"from tensorflow.keras.models import Model\nvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)","20b26527":"import cv2\ndef load_image(path):\n    #print(path)\n    img = cv2.imread(path, 1)\n    return img[...,::-1]","abc8dfe5":"# Get embedding vector for first image in the metadata using the pre-trained model\nimg_path = metadata[0].image_path()\nprint(img_path)\nimg = load_image(img_path)\n# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\nimg = (img \/ 255.).astype(np.float32)\nimg = cv2.resize(img, dsize = (224,224))\nprint(img.shape)\n\n# Obtain embedding vector for an image\n# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \n#print(vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0])\nembedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\nprint(type(embedding_vector))\nprint(embedding_vector.shape)","45aa90b5":"embedding_vector","9bfff185":"embeddings = np.zeros((metadata.shape[0], 2622))\nimport time\nstart_time = time.time()\nfor i, m in enumerate(metadata):\n  img_path = m.image_path()\n  img = load_image(img_path)\n  # Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n  img = (img \/ 255.).astype(np.float32)\n  img = cv2.resize(img, dsize = (224, 224))\n  embeddings[i] = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n\ncomputational_time = time.time() - start_time\nprint('Done in %0.3fs' %(computational_time))\n\nembeddings[0]","c33348d6":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))","68c7db4c":"import matplotlib.pyplot as plt\n\ndef show_pair(idx1, idx2):\n    plt.figure(figsize=(8,3))\n    plt.suptitle(f'Distance = {distance(embeddings[idx1], embeddings[idx2])}')\n    plt.subplot(121)\n    plt.imshow(load_image(metadata[idx1].image_path()))\n    plt.subplot(122)\n    plt.imshow(load_image(metadata[idx2].image_path()));    \n\nshow_pair(2, 4)\nshow_pair(2, 411)","9721a1c5":"train_idx = np.arange(metadata.shape[0]) % 9 != 0\ntest_idx = np.arange(metadata.shape[0]) % 9 == 0","01cfc980":"np.sum(train_idx)","a3429129":"np.sum(test_idx)","eba7ee18":"X_train = embeddings[train_idx]\nX_test = embeddings[test_idx]","a29abe40":"X_train.shape","e359540e":"X_test.shape","2ec457f3":"from sklearn import preprocessing \nfrom sklearn.preprocessing import LabelEncoder","8b3c082e":"targets = np.array([m.name for m in metadata])\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(targets) ","ecf3084d":"y_train = y[train_idx]\ny_test  = y[test_idx]","79ed132c":"np.unique(targets)","56a88d26":"np.unique(y_test)","cfd44910":"np.unique(y_train)","de9c6f64":"# Standarize features\nfrom sklearn.preprocessing import StandardScaler\n\n#### Add your code here ####\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\n\nprint(X_train)\nprint(X_train_scaled)","deb67238":"from sklearn.decomposition import PCA","1fd85905":"covMatrix = np.cov(X_train_scaled,rowvar=False)\nprint(covMatrix)","62b5f67e":"eig_vals, eig_vecs = np.linalg.eig(covMatrix)\ntot = sum(eig_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n","f5017f59":"features = 2622","e906fa6d":"# Taking the attribute count 2622 except the target column.\npca = PCA(n_components=features)\npca.fit(X_train)","ba16c3c3":"print(\"Eigen Values :\")\nprint(\"====================\")\nprint(pca.explained_variance_)","9c8e357d":"print(\"Eigen Vectors :\")\nprint(\"====================\")\nprint(pca.components_)","9bef93ac":"print(\"The percentage of variation explained by each eigen Vector : \")\nprint(\"============================================================\")\nprint(pca.explained_variance_ratio_)","9512247d":"fig1 = plt.figure(figsize=(15,6))\nplt.bar(list(range(1,(features+1))),pca.explained_variance_ratio_,alpha=0.7)\nplt.ylabel('Variation explained')\nplt.xlabel('Eigen Value')\nplt.show()","710ded76":"fig1 = plt.figure(figsize=(15,6))\nplt.step(list(range(1,(features+1))),np.cumsum(pca.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('Eigen Value')\nplt.show()","4d1992b6":"# Set variable for the decided dimension\nfinal_n_component = 100\n# Taking the attribute count as per the decision.\npca_component = PCA(n_components=final_n_component, svd_solver='full')\npca_component.fit(X_train_scaled)\nprint(f\"Eigen Values (with {final_n_component} PCA components):\")\nprint(\"===========================================\")\nprint(pca_component.explained_variance_)\nprint(f\"Eigen Vectors (with {final_n_component} PCA components):\")\nprint(\"=================================================\")\nprint(pca_component.components_)\nprint(f\"The percentage of variation explained by each eigen Vector (with {final_n_component} PCA components):\")\nprint(\"==========================================================================================\")\nprint(pca_component.explained_variance_ratio_)\n# Transforming the dataset\npca_X_train = pca_component.transform(X_train_scaled)\npca_X_test = pca_component.transform(X_test_scaled)\npca_X_train","facf7109":"from sklearn.svm import SVC\n","a2cfee47":"pca_svm = SVC(C = 1, kernel = 'linear', degree=3, gamma= \"scale\")","d15c51f8":"pca_svm.fit(pca_X_train, y_train)","33be2b55":"pca_svm.score(pca_X_test, y_test)","cfc2b97a":"import warnings\n# Suppress LabelEncoder warning\nwarnings.filterwarnings('ignore')\n\nexample_idx = 222\n\nexample_image = load_image(metadata[test_idx][example_idx].image_path())\nexample_prediction = pca_svm.predict([pca_X_test[example_idx]])\n#### Add your code here ####\nexample_identity = label_encoder.inverse_transform(example_prediction)[0]\n\nplt.imshow(example_image)\nname = example_identity.split('_')\nperson_name = name[1].split(' ')\nperson_name = person_name[0].capitalize() + ' ' + person_name[1].capitalize()\nplt.title(f'Identified as {person_name}');","1a343a9e":"print(f\"The above prediction is correct as it has identified the person name {person_name} with the given image\")","f6f1a059":"import warnings\n# Suppress LabelEncoder warning\nwarnings.filterwarnings('ignore')\n\nexample_idx = 500\n\nexample_image = load_image(metadata[test_idx][example_idx].image_path())\nexample_prediction = pca_svm.predict([pca_X_test[example_idx]])\n#### Add your code here ####\nexample_identity = label_encoder.inverse_transform(example_prediction)[0]\n\nplt.imshow(example_image)\nname = example_identity.split('_')\nperson_name = name[1].split(' ')\nperson_name = person_name[0].capitalize() + ' ' + person_name[1].capitalize()\nplt.title(f'Identified as {person_name}');","79438693":"print(f\"The above prediction is correct as it has identified the person name {person_name} with the given image\")","860ed46a":"Standardizing the feature values","f5a98517":"Using SVM Classifier to predict the person in the given image","58dc1dbf":"**Test results**\n\nIndentifying which person(folder name in dataset) the image belongs to","3dfd1996":"train and test sets","f789438f":"Encoding the Labels","49be67c0":"**Summary ::**\n\nAbove I have tried with two examples and in both the cases model predicted absolutely correct. Infact PCA has done a great job here because of which I am able to do it with only 100 components olny.","cdb3ac44":"Here I am going to build a face identification model to recognize faces.\n\nDataset : Aligned Face Dataset from Pinterest. This dataset contains 10.770 images for 100 people. All images are taken from 'Pinterest' and aligned using dlib library.\n","f111dc0f":"**VGG Face model**\n\nHere I am using the predefined model for VGG face","a2526d51":"**Observation** : From above Elbow plot and Bar plot we can see that steep drops in variance explained with increase in number of PC's.","432ca65d":"Reducing feature dimensions using Principal Component Analysis","b78a2430":"Ploting images and get distance between the pairs given below\u00b6","651ecf22":"**PCA Obeservation and Components selection : -**\nThis plot tells us that selecting 100 components we can preserve something around 94% or 95% of the total variance of the data. It makes sense, we will not use 100% of our variance, because it denotes all components, and we want only the principal ones. Now 100 dimensions seems very reasonable. With 100 dimensions we can explain over 95% of the variation in the original data.\n\nWith this information in our hands, we can implement the PCA for 100 best components.","963a2e4e":"**Generating embeddings for all images**","b49429e8":"**Comment:** Here we can see that the accuracy is 95% which looks good with 100 dimensions."}}