{"cell_type":{"01f38f60":"code","f58dc21c":"code","a3a94cc2":"code","74f7556e":"code","297730fb":"code","7ecfb8f1":"code","f20e1dcd":"code","c7001a1d":"code","04e1e202":"code","1f7fdd8b":"code","a9074570":"code","7d3bdf71":"code","c4d1ddae":"code","eb9faccc":"code","d331fc19":"code","e20ac202":"code","93b3d78a":"code","3244fa05":"code","bae9acbd":"code","08a1722e":"code","3f3fe4e0":"code","1fd2fd35":"code","f4caa6b5":"code","355e58c0":"code","7de1875c":"code","e2919d33":"code","a927ed3f":"code","482807c9":"code","79aec248":"code","8fb92801":"code","e80bc47b":"code","e5599922":"code","65ad5889":"code","65e39ce8":"code","18173aef":"code","1ec7f4db":"code","d2561c5a":"code","05d4b654":"code","1e0e1f1e":"code","a367ee15":"code","8f941bd0":"code","c6e3104b":"code","b8237cde":"code","c4a6e867":"code","9e39fed9":"code","7d2f9d97":"code","44e9a86e":"code","7fee284c":"code","f193abd8":"markdown","8cb5cc88":"markdown","df5ca14c":"markdown","e79c0c30":"markdown","30382f28":"markdown","1db73b42":"markdown","3b9880e0":"markdown","aa5fb1bd":"markdown","f7a62f37":"markdown","97648f5f":"markdown","80abd2fc":"markdown","e98a6c99":"markdown","c72b84ab":"markdown","4aacda67":"markdown","0ca2d4f4":"markdown","bdc23719":"markdown","b8be9cd6":"markdown","b348df6f":"markdown","7f87e5e0":"markdown"},"source":{"01f38f60":"%config Completer.use_jedi = False\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter","f58dc21c":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndata.head()","a3a94cc2":"data.isnull().sum()","74f7556e":"for feature in data.columns:\n    print(feature, ':', len(data[feature].unique()))","297730fb":"discrete_features, continuous_features = [], []\nfor feature in data.columns:\n    if feature == 'DEATH_EVENT':\n        label = ['DEATH_EVENT']\n    elif len(data[feature].unique()) >= 10:\n        continuous_features.append(feature)\n    else:\n        discrete_features.append(feature)\n\nprint('Discrete: ', discrete_features, '\\n', 'Continuous', continuous_features)","7ecfb8f1":"correlation = data.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(correlation, annot=True)\nplt.show()","f20e1dcd":"fig, ax = plt.subplots(len(discrete_features), 2, figsize=(14,20))\n\nfor i in range(len(discrete_features)):\n    sns.countplot(ax=ax[i, 0], x=discrete_features[i], data=data)\n    sns.countplot(ax=ax[i, 1], x=discrete_features[i], hue='DEATH_EVENT', data=data)\nfig.tight_layout(pad=1)\nplt.show()","c7001a1d":"sns.countplot(x='DEATH_EVENT', data=data)","04e1e202":"fig, ax = plt.subplots(len(continuous_features), 2, figsize=(14,22))\n\nfor i in range(len(continuous_features)):\n    sns.kdeplot(ax=ax[i, 0], x=continuous_features[i], hue='DEATH_EVENT', data=data, fill = True)\n    sns.boxplot(ax=ax[i, 1], x=continuous_features[i], data=data)\nfig.tight_layout(pad=1)\nplt.show()","1f7fdd8b":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.feature_selection import chi2, SelectFromModel, SelectKBest\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nimport xgboost","a9074570":"best_features = SelectKBest(chi2, k=10)\nfeatures_ranking = best_features.fit(data.drop(['DEATH_EVENT'], axis=1), data['DEATH_EVENT'])\nranking_dictionary = {}\nfor i in range(len(features_ranking.scores_)):\n    ranking_dictionary[data.columns[i]] = round(features_ranking.scores_[i], 3)\n\nasc_sort = sorted(ranking_dictionary.items(), key = lambda kv:(kv[1], kv[0]))\n\nfor i, j in asc_sort:\n    print(i, ':', j)","7d3bdf71":"feature_model = SelectFromModel(Lasso(alpha=0.05, random_state=0))\nfeature_model.fit(data.drop(['DEATH_EVENT'], axis=1), data['DEATH_EVENT'])","c4d1ddae":"mask = feature_model.get_support() \nfor i in range(len(mask)):\n    if not mask[i]:\n        print(data.drop(['DEATH_EVENT'], axis=1).columns[i])","eb9faccc":"X = data.drop(['DEATH_EVENT'], axis=1)\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape, X_test.shape)","d331fc19":"features_with_outliers = ['creatinine_phosphokinase', 'platelets', 'serum_creatinine', 'serum_sodium']","e20ac202":"lof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]","93b3d78a":"fig, ax = plt.subplots(len(continuous_features), 2, figsize=(14,22))\n\nfor i in range(len(continuous_features)):\n    sns.boxplot(ax=ax[i, 0], x=continuous_features[i], data=X_train)\n    sns.boxplot(ax=ax[i, 1], x=continuous_features[i], data=data)\nfig.tight_layout(pad=1)\nplt.show()","3244fa05":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","bae9acbd":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","08a1722e":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","3f3fe4e0":"X = data[['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'time']]\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\nprint(X_train.shape, X_test.shape)\n\nlof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","1fd2fd35":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","f4caa6b5":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","355e58c0":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Gradient Boost is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","7de1875c":"X = data[['ejection_fraction', 'serum_creatinine', 'time']]\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\nprint(X_train.shape, X_test.shape)\n\nlof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","e2919d33":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","a927ed3f":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","482807c9":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Gradient Boost is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","79aec248":"X = data.drop(['DEATH_EVENT'], axis=1)\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape, X_test.shape)\n\nlof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]\n\noversample = RandomOverSampler(sampling_strategy='minority')\nX_sampled, y_train = oversample.fit_resample(X_train, y_train)\nprint(Counter(y_train))\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_sampled)\nX_test = scaler.transform(X_test)","8fb92801":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","e80bc47b":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","e5599922":"X = data[['ejection_fraction', 'serum_creatinine', 'time']]\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nlof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]\n\noversample = RandomOverSampler(sampling_strategy='minority')\nX_train, y_train = oversample.fit_resample(X_train, y_train)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","65ad5889":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","65e39ce8":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","18173aef":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Gradient Boost is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","1ec7f4db":"X = data.drop(['DEATH_EVENT'], axis=1)\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nlof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]\n\noversample = SMOTE(sampling_strategy='minority')\nX_train, y_train = oversample.fit_resample(X_train, y_train)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","d2561c5a":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","05d4b654":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","1e0e1f1e":"X = data[['ejection_fraction', 'serum_creatinine', 'time']]\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nlof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]\n\noversample = SMOTE(sampling_strategy='minority')\nX_train, y_train = oversample.fit_resample(X_train, y_train)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","a367ee15":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","8f941bd0":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","c6e3104b":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\nprint (\"The accuracy of Gradient Boost is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","b8237cde":"X = data[['ejection_fraction', 'serum_creatinine', 'time']]\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nlof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]\n\noversample = RandomOverSampler(sampling_strategy='minority')\nX_train, y_train = oversample.fit_resample(X_train, y_train)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","c4a6e867":"model = RandomForestClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\nscore = cross_val_score(model, X_train, y_train, cv=cv)\nprint(score)\nprint(score.mean())","9e39fed9":"model = GradientBoostingClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\nscore = cross_val_score(model, X_train, y_train, cv=cv)\nprint(score)\nprint(score.mean())","7d2f9d97":"X = data[['ejection_fraction', 'serum_creatinine', 'time']]\ny = data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nlof = LocalOutlierFactor()\noutlier_rows = lof.fit_predict(X_train)\n\nmask = outlier_rows != -1\nX_train, y_train = X_train[mask], y_train[mask]\n\noversample = SMOTE(sampling_strategy='minority')\nX_train, y_train = oversample.fit_resample(X_train, y_train)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","44e9a86e":"model = RandomForestClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\nscore = cross_val_score(model, X_train, y_train, cv=cv)\nprint(score)\nprint(score.mean())","7fee284c":"model = GradientBoostingClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\nscore = cross_val_score(model, X_train, y_train, cv=cv)\nprint(score)\nprint(score.mean())","f193abd8":"# Model Building with SMOTE","8cb5cc88":"#### Trying with different sets of features based on the observations.","df5ca14c":"#### Trying with different sets of features based on the observations.","e79c0c30":"Considering all the features, the accuracy with **Logistic Regression is 78% and Random Forest is 85%**","30382f28":"If I consider the ones which are highly correlated to the output label,\n**features = {ejection_fraction, serum_creatinine, time}**\n\n* Logistic Regression --> 90%\n* Random Forest --> 90%\n* Gradient Boosting --> 95%","1db73b42":"### Observations\n- There is an imbalance with the target variable, so we can apply cross validation technique with over sampling method compared to under sampling as the data size is small.","3b9880e0":"# Feature Selection","aa5fb1bd":"**features = {ejection_fraction, serum_creatinine, time}**\n\nThe accuracies over here are slightly corrected as it is tested on sampled data.\n\n* Logistic Regression --> 90%\n* Random Forest --> 90%\n* Gradient Boosting --> 95%","f7a62f37":"# EDA","97648f5f":"# Model Building without Sampling","80abd2fc":"Considering all the features, the accuracy with **Logistic Regression is 78% and Random Forest is 85%**","e98a6c99":"**features = {ejection_fraction, serum_creatinine, time}**\n\nThe accuracies over here are slightly corrected as it is tested on sampled data.\n\n* Logistic Regression --> 90%\n* Random Forest --> 90%\n* Gradient Boosting --> 95%","c72b84ab":"# Model Building using Sampling","4aacda67":"#### Trying with different sets of features based on the observations.","0ca2d4f4":"### Conclusion:\nIf I eliminate a few features and considering **creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, time,**\n\n\nThe accuracies are:\n* Logistic Regression --> 87%\n* Random Forest -->91%\n* Gradient Boosting --> 91%","bdc23719":"### Romove Outliers","b8be9cd6":"- There is nothing to conclude from discrete features correlation matrix.\n- From the correlation matrix for continuous features, time is inversely correlated to death. Thus patients with less follow up time are prone to heart failure.","b348df6f":"### Observations\n- Based on EDA and Feature Selection, features such as **anaemia, diabetes, age, sex, smoking** are less contributing.\n- Features to be considered are, **platelets, time, creatinine_phosphokinase, ejection_fraction.**","7f87e5e0":"# Using Repeated Stratified KFold Method "}}