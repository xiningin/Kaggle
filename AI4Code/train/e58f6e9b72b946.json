{"cell_type":{"f6414a3e":"code","edd5031a":"code","c69cdc7f":"code","29848f01":"code","3139c793":"code","b51f32e2":"code","746a71b2":"code","956a46e9":"code","bb3c5d1d":"code","9222aae1":"code","deabea6b":"code","2bb3dc3a":"code","e254d640":"code","86e942a7":"code","bdf2c98a":"code","d1e887ff":"code","58f3115a":"code","a8eb7996":"code","70fbb4c1":"code","d77a04f1":"code","c9063080":"code","f43815fa":"code","175853ef":"code","01e7da9f":"code","fdbeeaed":"code","1bf1faf6":"code","93f35309":"code","2f891cbc":"code","1a67acc7":"code","82f68a99":"code","a77fae1c":"code","b175d3e6":"code","a53e73ee":"code","fa9b192f":"code","79022d76":"code","b9d5c74b":"code","cf95a7c4":"code","5f83e1e0":"code","586a4f05":"code","f21526e6":"code","37b18001":"code","4903e1cf":"code","d8aa429b":"code","6651afc9":"code","9cf62b08":"code","3814f3d7":"code","ca13d6dc":"code","d4d434ee":"code","dfc406ce":"code","ab845103":"code","b3900b09":"code","ed33c63e":"code","ec1e1979":"code","3409f582":"code","468554ee":"code","c74bcad8":"code","2d579ba1":"code","714f7a79":"code","9ec62d5a":"code","3682615f":"code","677a90d6":"code","fffb0d7e":"code","b29b9fcc":"code","6e794e20":"code","6891624f":"code","3f75cdbd":"code","eac4a171":"code","ad8e272a":"code","586cf9e5":"code","53663477":"code","3a1ce154":"code","66c4b68d":"code","f68e886b":"code","2bef8ff5":"code","e560baf1":"code","a872dfe8":"code","6bd5aa82":"code","73eafa47":"code","b1ef3ca3":"code","108aaa3e":"code","14e2b6ee":"code","df59d0e1":"code","803e7569":"code","67bdd7ed":"code","191bb3ce":"code","81e931c5":"code","2281b2a7":"code","4fa17d3b":"code","e7dc5df1":"code","97635ee3":"code","e9b4f30f":"code","bfc7cfc3":"code","93dfa892":"code","95685857":"code","95cbcbb9":"code","4eb0c862":"code","8858f18c":"code","5fd76a83":"code","896f6b53":"code","b772088c":"code","0aff40f0":"code","3089103a":"code","d1c207ed":"code","68cbdd00":"code","d2b39fbe":"code","d41659d1":"code","7b8a9176":"code","1fa9b696":"code","dc4340a5":"code","cdbdb91e":"code","6276148d":"code","0f82f30c":"code","8666e712":"code","18086664":"code","81865806":"code","929a7d58":"code","d5d1c12c":"code","108022b3":"code","0f72344f":"code","75830ec7":"markdown","4232ea6f":"markdown","b4084bb3":"markdown","effc181b":"markdown","3c6e0335":"markdown","70359361":"markdown","34c4c722":"markdown","db357e7c":"markdown","228d999a":"markdown","1acda7fb":"markdown","bafc0046":"markdown","9b1e4c46":"markdown","8950e50d":"markdown","b45a77e1":"markdown","e2cee4c3":"markdown","526c2789":"markdown","b8678bbc":"markdown","5f395e99":"markdown","cd01fd25":"markdown","858983aa":"markdown","03e59e15":"markdown","897c31a3":"markdown"},"source":{"f6414a3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly as py \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score,cross_val_predict\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,log_loss\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","edd5031a":"df=pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","c69cdc7f":"df.head()","29848f01":"df.info()","3139c793":"df.isnull().sum()","b51f32e2":"df.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target'] # renaming columns","746a71b2":"df.head()","956a46e9":"sns.countplot(x=\"chest_pain_type\",hue=\"target\",data=df) # chest pain type-Target","bb3c5d1d":"sns.scatterplot(x=df.cholesterol,y=df.resting_blood_pressure,hue=df.target) #cholesterol-resting_blood_pressure-Target","9222aae1":"sns.scatterplot(x=df.cholesterol,y=df.max_heart_rate_achieved,hue=df.target,) #cholesterol-max_heart_rate_achieved-Target","deabea6b":"sns.scatterplot(x=df.resting_blood_pressure,y=df.max_heart_rate_achieved,hue=df.target,) #resting_blood_pressure-max_heart_rate_achieved-target","2bb3dc3a":"sns.countplot(x=\"thalassemia\",hue=\"target\",data=df) # thalassemia-Target","e254d640":"sns.countplot(x=\"num_major_vessels\",hue=\"target\",data=df) #num_major_vessels-Target","86e942a7":"sns.countplot(x=\"fasting_blood_sugar\",hue=\"target\",data=df) #fasting_blood_sugar-Target","bdf2c98a":"sns.countplot(x=\"rest_ecg\",hue=\"target\",data=df) # rest_ecg-Target","d1e887ff":"sns.countplot(x=\"st_slope\",hue=\"target\",data=df) #st_slope-Target","58f3115a":"sns.countplot(x=\"sex\",hue=\"target\",data=df)# Sex-Target","a8eb7996":"def barplot(x_features,y_features,target):\n    for i in x_features:\n        for j in y_features:\n            f,ax = plt.subplots(figsize = (5,5))\n            sns.barplot(x=i,y=j,hue=target,data=df,errwidth=False)\n            ","70fbb4c1":"x_f=[\"chest_pain_type\",\"st_slope\",\"sex\",\"rest_ecg\",\"fasting_blood_sugar\",\"num_major_vessels\",\"thalassemia\"]\ny_f=[\"st_depression\",\"cholesterol\",\"max_heart_rate_achieved\",\"max_heart_rate_achieved\"]\na=\"target\"\n\nbarplot(x_f,y_f,a)","d77a04f1":"sns.distplot(df.age,color=\"blue\",kde=True)","c9063080":"def turn_categorical(features):\n    for i in features:\n        df[i]=df[i].astype(\"object\") # I am going to turn int data to object data to use \"one hot encoding\" because some columns are categorical like \"sex,\"thalasemia\" \n","f43815fa":"turn_categorical([\"sex\",\"chest_pain_type\",\"fasting_blood_sugar\",\"rest_ecg\",\"exercise_induced_angina\",\"st_slope\",\"thalassemia\"])","175853ef":"df.info()","01e7da9f":"df = pd.get_dummies(df, drop_first=True) # make categorical variables to dummy variables\n","fdbeeaed":"df.head()","1bf1faf6":"df=df.rename(columns={\"sex_1\":\"sex_male\"})","93f35309":"df.head()","2f891cbc":"y=df.target\nX=df.drop(\"target\",axis=1)","1a67acc7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","82f68a99":"log_reg=LogisticRegression(solver=\"liblinear\")\nlog_reg.fit(X_train,y_train)\n","a77fae1c":"y_pred=log_reg.predict(X_test)","b175d3e6":"from sklearn.metrics import accuracy_score,precision_score\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))\n","a53e73ee":"log_reg.score(X_test,y_test) # other way to see accuracy score","fa9b192f":"cm=confusion_matrix(y_test,y_pred)\nsns.heatmap(cm,annot=True)","79022d76":"y_pred_proba = log_reg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc =roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","b9d5c74b":"# the highest value of auc is 1 ","cf95a7c4":"knn_model=KNeighborsClassifier(n_neighbors=5)","5f83e1e0":"knn_model.fit(X_test,y_test)","586a4f05":"y_pred=knn_model.predict(X_test)","f21526e6":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred)) #for k=3,Our accuracy is lower than logistic regression ","37b18001":"k_list=[]\nacc_list=[]\n\nfor i in range(1,26):\n    k_list.append(i)\n    knn_model=KNeighborsClassifier(n_neighbors=i)\n    knn_model.fit(X_test,y_test)\n    acc_list.append(knn_model.score(X_test,y_test))\n    \nplt.plot(k_list,acc_list) \nplt.xlabel(\"K = ?\")\nplt.ylabel(\"Accuracy Score\")\nplt.grid()\n    \n#we can train our model with best K value according to graph\n    ","4903e1cf":"rf_model=RandomForestClassifier(random_state=42)\nrf_model.fit(X_train,y_train)","d8aa429b":"y_pred=rf_model.predict(X_test)","6651afc9":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))","9cf62b08":"rf_params={\"max_depth\":[2,5,8,10],\n           \"max_features\":[2,5,8],\n           \"n_estimators\":[10,500,1000],\n          \"min_samples_split\":[2,5,10]}","3814f3d7":"rf=RandomForestClassifier(random_state=42)","ca13d6dc":"rf_cv=GridSearchCV(rf,rf_params,cv=10,n_jobs=-1,verbose=2)","d4d434ee":"rf_cv_model=rf_cv.fit(X_train,y_train)","dfc406ce":"rf_cv_model.best_params_","ab845103":"rf_tuned_model=RandomForestClassifier(random_state=42,max_depth=2,max_features= 5,min_samples_split= 2,n_estimators= 1000)","b3900b09":"rf_tuned_model.fit(X_train,y_train)","ed33c63e":"y_pred=rf_tuned_model.predict(X_test)","ec1e1979":"cm=confusion_matrix(y_test,y_pred)","3409f582":"sns.heatmap(cm,annot=True) # we have 9 false predictions","468554ee":"Importance=pd.DataFrame({\"Importance\":rf_tuned_model.feature_importances_*100},index=X_train.columns)","c74bcad8":"Importance.sort_values(by=\"Importance\",axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\n\nplt.xlabel(\"Feature \u0130mportances\") ","2d579ba1":"rf_tuned_model.score(X_test,y_test) # Logistic regression gives us a highest accuracy score so far","714f7a79":"from sklearn.naive_bayes import BernoulliNB","9ec62d5a":"nb=GaussianNB()\nnb2=BernoulliNB()","3682615f":"nb_model=nb.fit(X_train,y_train)\nnb2_model=nb2.fit(X_train,y_train)","677a90d6":"y_pred=nb_model.predict(X_test)\ny_pred2=nb2_model.predict(X_test)","fffb0d7e":"print(\"Accuracy_NB1:\",accuracy_score(y_test, y_pred))\nprint(\"Precision_NB1:\",precision_score(y_test, y_pred))\nprint(\"**************************************************************\")\nprint(\"Accuracy_NB2:\",accuracy_score(y_test, y_pred2))\nprint(\"Precision_NB2:\",precision_score(y_test, y_pred2))\n","b29b9fcc":"cm=confusion_matrix(y_test,y_pred)\ncm2=confusion_matrix(y_test,y_pred2)","6e794e20":"sns.heatmap(cm,annot=True) # heatmap for GaussianNB 8 false prediction","6891624f":"sns.heatmap(cm2,annot=True) # Heatmap for Bernoulli 10 false prediction","3f75cdbd":"svm=SVC(kernel=\"linear\") #linear model\nsvm2=SVC(kernel=\"rbf\")   #Non-Linear model","eac4a171":"svm_model1=svm.fit(X_train,y_train)\nsvm_model2=svm2.fit(X_train,y_train)","ad8e272a":"y_pred1=svm_model1.predict(X_test)\ny_pred2=svm_model2.predict(X_test)","586cf9e5":"print(\"Accuracy_svm1:\",accuracy_score(y_test, y_pred1))\nprint(\"Precision_svm1:\",precision_score(y_test, y_pred1))\nprint(\"**************************************************************\")\nprint(\"Accuracy_svm2:\",accuracy_score(y_test, y_pred2))\nprint(\"Precision_svm2:\",precision_score(y_test, y_pred2))\n\n# I am going to continue with Linear SVM so I am going to tune 'C' parameter","53663477":"cm=confusion_matrix(y_test,y_pred1)\nsns.heatmap(cm,annot=True) # 9 false prediction","3a1ce154":"# for linear svm,We can just tune 'C' parameter. C is penalty for each misclassification ","66c4b68d":"svc_params={\"C\":(0.001,0.1,0.01,1,5,10,50,100,500,1000,2000)} # these are random number\n\nsvc=SVC(kernel=\"linear\")\n\nsvm_cv_model=GridSearchCV(svc,svc_params,cv=10,n_jobs=-1,verbose=2)\n\nsvv_tuned=svm_cv_model.fit(X_train,y_train)","f68e886b":"svv_tuned.best_params_ # Best parameter for C is 500","2bef8ff5":"svm_tuned_model=SVC(C=500,kernel=\"linear\")","e560baf1":"svm_tuned_model.fit(X_train,y_train)","a872dfe8":"y_pred=svm_tuned_model.predict(X_test)\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred)) ","6bd5aa82":"#Accuracy of my model decreased. :\/ ","73eafa47":"gbm_model=GradientBoostingClassifier()\ngbm_model.fit(X_train,y_train)","b1ef3ca3":"y_pred=gbm_model.predict(X_test)","108aaa3e":"cm=confusion_matrix(y_test,y_pred)","14e2b6ee":"sns.heatmap(cm,annot=True)","df59d0e1":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred)) ","803e7569":"gbm_params={\"learning_rate\":[0.001,0.01,0.1,0.05],\n           \"n_estimators\":[100,500,1000],\n           \"max_depth\":[3,5,10],\n           \"min_samples_split\":[2,5,10]}","67bdd7ed":"gbm=GradientBoostingClassifier()\ngbm_cv=GridSearchCV(gbm,gbm_params,cv=10,n_jobs=-1,verbose=2)\ngbm_cv_model=gbm_cv.fit(X_train,y_train)","191bb3ce":"gbm_cv_model.best_params_","81e931c5":"gbm=GradientBoostingClassifier(learning_rate= 0.1,max_depth= 3,min_samples_split= 5,n_estimators= 100)\ngbm_tuned_model=gbm.fit(X_train,y_train)","2281b2a7":"y_pred=gbm_tuned_model.predict(X_test)","4fa17d3b":"cm=confusion_matrix(y_test,y_pred)","e7dc5df1":"sns.heatmap(cm,annot=True)","97635ee3":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))  # my accuracy score didnt change ","e9b4f30f":"Importance=pd.DataFrame({\"Importance\":gbm_tuned_model.feature_importances_*100},index=X_train.columns)\n\nImportance.sort_values(by=\"Importance\",axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\n\nplt.xlabel(\"Feature \u0130mportances\") ","bfc7cfc3":"y_pred_proba = gbm_tuned_model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc =roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show() # auc score is 0.89","93dfa892":"xgb=XGBClassifier()","95685857":"xgb_model=xgb.fit(X_train,y_train)","95cbcbb9":"y_pred=xgb_model.predict(X_test)","4eb0c862":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))","8858f18c":"xgbm_params={\"n_estimators\":[100,500,1000,2000],\n           \"subsample\":[0.6,0.8,1],\n           \"max_depth\":[3,4,5,6],\n           \"learning_rate\":[0.1,0.01,0.02,0.05],\n           \"min_samples_split\":[2,5,10]} # n_estimators= number of iterations","5fd76a83":"xgbm=XGBClassifier()\nxgbm_cv=GridSearchCV(xgbm,xgbm_params,cv=10,n_jobs=-1,verbose=2)\nxgbm_cv_model=xgbm_cv.fit(X_train,y_train)","896f6b53":"xgbm_cv_model.best_params_","b772088c":"xgbm=XGBClassifier(learning_rate= 0.05,max_depth= 3,min_samples_split= 2,n_estimators= 100,subsample= 0.6)\nxgbm_tuned_model=xgbm.fit(X_train,y_train)","0aff40f0":"y_pred=xgbm_tuned_model.predict(X_test)\n","3089103a":"cm=confusion_matrix(y_test,y_pred)","d1c207ed":"sns.heatmap(cm,annot=True)","68cbdd00":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))","d2b39fbe":"y_pred_proba = xgbm_tuned_model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc =roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show() # Auc=0.92","d41659d1":"lgbm_model=LGBMClassifier().fit(X_train,y_train)","7b8a9176":"y_pred=lgbm_model.predict(X_test)","1fa9b696":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))","dc4340a5":"lgbm_params={\"n_estimators\":[100,500,100,2000],\n            \"subsample\":[0.6,0.8,1.0],\n            \"learning_rate\":[0.1,0.01,0.02,0.05],\n            \"min_child_samples\":[5,20,10],\n            \"max_depth\":[3,4,5,6]}","cdbdb91e":"lgbm=LGBMClassifier()\nlgbm_cv=GridSearchCV(lgbm,lgbm_params,cv=10,n_jobs=-1,verbose=2)\nlgbm_cv_model=lgbm_cv.fit(X_train,y_train)","6276148d":"lgbm_cv_model.best_params_","0f82f30c":"lgbm=LGBMClassifier(learning_rate= 0.05,max_depth= 6,min_child_samples= 10,n_estimators= 100,subsample= 0.6)\nlgbm_tuned_model=lgbm.fit(X_train,y_train)","8666e712":"y_pred=lgbm_tuned_model.predict(X_test)","18086664":"cm=confusion_matrix(y_test,y_pred)","81865806":"sns.heatmap(cm,annot=True)","929a7d58":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred))","d5d1c12c":"y_pred_proba = lgbm_tuned_model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc =roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show() # AUC=0.9","108022b3":"models=[lgbm_tuned_model,xgbm_tuned_model,gbm_tuned_model,svm_tuned_model,nb_model,knn_model,log_reg]\nstr_models=[\"lgbm_tuned_model\",\"xgbm_tuned_model\",\"gbm_tuned_model\",\"svm_tuned_model\",\"nb_model\",\"knn_model\",\"log_reg\"]\nacc_score=[]\n\nfor i in models:\n    acc_score.append(i.score(X_test,y_test))\n\ndata=pd.DataFrame({\"Models\": str_models,\"Accuracy_Score\":acc_score})\ndata.sort_values(by=\"Accuracy_Score\",ascending=False)\ndata.reset_index(drop=True)\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"Accuracy_Score\",y=\"Models\",data=data,orient=\"h\")\nplt.xticks(rotation=90)\nplt.grid()\n\n        \n        ","0f72344f":"?plt","75830ec7":"## For this data I am going to compare accuracy score of Classification Algorithms to see which Model is better for this data","4232ea6f":"### KNN Classifier","b4084bb3":"For This Data,Logistic Regression gives best accuracy score ","effc181b":"cholesterol and others features that below the cholesterol  on graph are not that important than others","3c6e0335":"1: Male,0:Female","70359361":"0: typical angina,\n1: atypical angina, \n2: non-anginal pain,\n3: asymptomatic,","34c4c722":"Support Vector Machine","db357e7c":"## SVM Model Tunning","228d999a":"## Gradient Boosting Classifier","1acda7fb":"### Model Tunning with GridSearchCV","bafc0046":"## LightGBM Classifier","9b1e4c46":"### Train Test Split","8950e50d":"Now I am going to make bar plots","b45a77e1":"## LightGBM Model Tunning","e2cee4c3":"## Visualization","526c2789":"Naive Bayes Classification","b8678bbc":"## Logistic Regression","5f395e99":"## Xgbm Classifier","cd01fd25":"### RandomForest Classifier","858983aa":"GB Classifier Model Tunning","03e59e15":"## XGBM Classifier Model Tunning","897c31a3":"1: greater than 120\n0:lower than 120"}}