{"cell_type":{"706d0b10":"code","0a2418e5":"code","b5281c7f":"code","9dfc513c":"code","886d13d8":"code","a0e4c657":"code","5c30fad5":"code","727cf452":"code","6f0ad71c":"code","dba0d60f":"code","916a6440":"code","147fc831":"code","8fec0e7d":"code","dd5495fc":"code","3b180ddc":"code","451f061d":"code","74210266":"code","6a398a3a":"code","135923e6":"code","1f124047":"code","c01e9c28":"code","a9dfd955":"code","88227c23":"code","9a25b148":"code","6c1a6290":"code","498a0dc3":"code","f673446b":"code","99ced435":"code","c8c232a3":"code","45c08aef":"code","3dec2be1":"code","6c3afd9e":"code","31d651a5":"code","95ddee9e":"code","18dbe138":"code","8e52c2d5":"code","8e83f7c2":"code","ffb745f8":"code","bbef4d07":"code","627afc2b":"code","5a15f285":"code","546ac983":"code","b9c9a6a4":"code","7f4aab1b":"code","3b4f5073":"code","c91de90d":"code","41f2cb67":"code","ca39508b":"code","a395a760":"code","8a6fe4b7":"code","76394b7e":"code","bcd409c7":"code","9afecf9a":"code","7de65f9c":"code","e1323ec5":"code","9101f85c":"code","168ef4de":"code","08edcdbc":"code","10a8f42a":"code","a913aa3e":"code","a0da9012":"code","e3c1931f":"code","0da3c43a":"code","0dae5294":"code","71523d1c":"code","3e73489e":"code","e5695e51":"code","dd2e682a":"code","1c6d3d34":"code","1fe4bae1":"code","ed1e5b1e":"code","d478460d":"code","895b6fa6":"code","3135cdc2":"code","37e15c46":"code","97b8f5f8":"code","9fbd42af":"code","2e166758":"code","6be5aaf1":"code","051a9500":"code","01bc41b8":"code","a47102ec":"code","26d502fb":"code","f4328774":"code","fb5d35dd":"code","330cc2f7":"code","ec0d1798":"markdown","6a465e5e":"markdown","1c3ad021":"markdown","e9b443b8":"markdown","10f18ad3":"markdown","0136d610":"markdown","6189e98b":"markdown","4991c6e8":"markdown","682c72db":"markdown","a9a14c40":"markdown","b52112b0":"markdown","56db4441":"markdown","a921b464":"markdown","1c0cedc2":"markdown"},"source":{"706d0b10":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom scipy.ndimage.filters import convolve\nfrom skimage import data, io, filters\nimport skimage\nfrom skimage.morphology import convex_hull_image, erosion\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D,\\\nZeroPadding2D, Convolution2D, ZeroPadding2D, Conv2DTranspose,ReLU\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.models import load_model\nfrom keras import backend\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","0a2418e5":"Class_Dict_Path = \"..\/input\/deepglobe-road-extraction-dataset\/class_dict.csv\"\nMetadata_Path = \"..\/input\/deepglobe-road-extraction-dataset\/metadata.csv\"\nMain_Direction = \"..\/input\/deepglobe-road-extraction-dataset\"","b5281c7f":"Reading_Class_Dict = pd.read_csv(Class_Dict_Path)\nReading_Metadata = pd.read_csv(Metadata_Path)","9dfc513c":"print(Reading_Class_Dict.head())","886d13d8":"print(Reading_Metadata.head(-1))","a0e4c657":"Metadata_Train = Reading_Metadata[Reading_Metadata[\"split\"] == \"train\"]\nMetadata_Test = Reading_Metadata[Reading_Metadata[\"split\"] == \"test\"]","5c30fad5":"print(Metadata_Train.head(-1))","727cf452":"Metadata_Train.drop(\"split\",inplace=True,axis=1)","6f0ad71c":"Metadata_Test.drop(\"split\",inplace=True,axis=1)","dba0d60f":"Metadata_Test.drop(\"mask_path\",inplace=True,axis=1)","916a6440":"print(Metadata_Test.head(-1))","147fc831":"print(Metadata_Train.head(-1))","8fec0e7d":"Metadata_Train = Metadata_Train.reset_index()\nMetadata_Test = Metadata_Test.reset_index()","dd5495fc":"print(Metadata_Test.head(-1))","3b180ddc":"print(Metadata_Train.head(-1))","451f061d":"Metadata_Train[\"sat_image_path\"] = Metadata_Train[\"sat_image_path\"].apply(lambda sat_path: os.path.join(Main_Direction,sat_path))","74210266":"Metadata_Train[\"mask_path\"] = Metadata_Train[\"mask_path\"].apply(lambda mask_path: os.path.join(Main_Direction,mask_path))","6a398a3a":"print(Metadata_Train.head(-1))","135923e6":"Metadata_Test[\"sat_image_path\"] = Metadata_Test[\"sat_image_path\"].apply(lambda sat_path: os.path.join(Main_Direction,sat_path))","1f124047":"print(Metadata_Test.head(-1))","c01e9c28":"Metadata_Train = Metadata_Train.sample(frac=1).reset_index(drop=True)","a9dfd955":"print(Metadata_Train.head(-1))","88227c23":"Validation_Data = Metadata_Train.sample(frac=0.1,random_state=123)","9a25b148":"print(len(Validation_Data))","6c1a6290":"print(Validation_Data.head(-1))","498a0dc3":"Train_Data = Metadata_Train.drop(Validation_Data.index)","f673446b":"print(len(Train_Data))","99ced435":"print(Train_Data.head(-1))","c8c232a3":"Class_Names = Reading_Class_Dict[\"name\"].tolist()","45c08aef":"print(Class_Names)","3dec2be1":"RGB_Values = Reading_Class_Dict[[\"r\",\"g\",\"b\"]].values.tolist()","6c3afd9e":"print(RGB_Values)","31d651a5":"Class_Type = ['background', 'road']","95ddee9e":"Class_Indices = [Class_Names.index(cls.lower()) for cls in Class_Type]","18dbe138":"print(Class_Indices)","8e52c2d5":"Class_RGB_Values = np.array(RGB_Values)[Class_Indices]","8e83f7c2":"print(Class_RGB_Values)","ffb745f8":"Example_Sat_Image = cv2.cvtColor(cv2.imread(Train_Data[\"sat_image_path\"][3]),cv2.COLOR_BGR2RGB)\nExample_Mask_Image = cv2.cvtColor(cv2.imread(Train_Data[\"mask_path\"][3]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Example_Sat_Image)\naxis[1].imshow(Example_Mask_Image)","bbef4d07":"Example_Sat_Image = cv2.cvtColor(cv2.imread(Train_Data[\"sat_image_path\"][30]),cv2.COLOR_BGR2RGB)\nExample_Mask_Image = cv2.cvtColor(cv2.imread(Train_Data[\"mask_path\"][30]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Example_Sat_Image)\naxis[1].imshow(Example_Mask_Image)","627afc2b":"Example_Sat_Image = cv2.cvtColor(cv2.imread(Train_Data[\"sat_image_path\"][587]),cv2.COLOR_BGR2RGB)\nExample_Mask_Image = cv2.cvtColor(cv2.imread(Train_Data[\"mask_path\"][587]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Example_Sat_Image)\naxis[1].imshow(Example_Mask_Image)","5a15f285":"Example_Sat_Image = cv2.cvtColor(cv2.imread(Train_Data[\"sat_image_path\"][7]),cv2.COLOR_BGR2RGB)\nExample_Mask_Image = cv2.cvtColor(cv2.imread(Train_Data[\"mask_path\"][7]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Example_Sat_Image)\naxis[1].imshow(Example_Mask_Image)","546ac983":"def one_hot_encode(label, label_values):\n    semantic_map = []\n    for colour in label_values:\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis = -1)\n        semantic_map.append(class_map)\n    semantic_map = np.stack(semantic_map, axis=-1)\n\n    return semantic_map","b9c9a6a4":"Example_Sat_Image = cv2.cvtColor(cv2.imread(Train_Data[\"sat_image_path\"][7]),cv2.COLOR_BGR2RGB)\nExample_Mask_Image = cv2.cvtColor(cv2.imread(Train_Data[\"mask_path\"][7]),cv2.COLOR_BGR2RGB)\n\nMask_One_Hot_Example = []\n\nfor color in RGB_Values:\n    Equality_Value = np.equal(Example_Mask_Image,color)\n    Class_Value = np.all(Equality_Value,axis = -1)\n    Mask_One_Hot_Example.append(Class_Value)\n    \nMask_One_Hot_Example = np.stack(Mask_One_Hot_Example,axis = -1).astype(\"float\")","7f4aab1b":"print(Mask_One_Hot_Example[:,:,0])","3b4f5073":"print(Example_Mask_Image[:,:,0])","c91de90d":"print(Example_Mask_Image[:,:,0].shape)","41f2cb67":"print(Example_Mask_Image.shape)\nprint(Mask_One_Hot_Example.shape)","ca39508b":"Array_Mask_Img = np.array(Example_Mask_Image)\nArray_One_Hot_Img = np.array(Mask_One_Hot_Example)","a395a760":"print(Array_Mask_Img.shape)\nprint(Array_One_Hot_Img.shape)","8a6fe4b7":"print(np.argmax(Mask_One_Hot_Example, axis = -1))","76394b7e":"print(np.argmax(Mask_One_Hot_Example, axis = -1).shape)","bcd409c7":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Example_Mask_Image)\naxis[1].imshow(np.argmax(Mask_One_Hot_Example, axis = -1))","9afecf9a":"colour_codes_example = np.array(Class_RGB_Values)\ntruth_mask = colour_codes_example[np.argmax(Mask_One_Hot_Example, axis = -1).astype(int)]","7de65f9c":"figure,axis = plt.subplots(1,3,figsize=(10,10))\n\naxis[0].imshow(Example_Mask_Image)\naxis[1].imshow(truth_mask)\naxis[1].set_xlabel(truth_mask.shape)\naxis[2].imshow(Example_Sat_Image)","e1323ec5":"Splitting_Data = Metadata_Train[0:3000]","9101f85c":"print(Splitting_Data.head(-1))","168ef4de":"Sat_Image = []\nMask_Image = []\n\nfor sat_img,mask_img in zip(Splitting_Data.sat_image_path,Splitting_Data.mask_path):\n    Reading_Sat = cv2.cvtColor(cv2.imread(sat_img),cv2.COLOR_BGR2RGB)\n    Reading_Sat = cv2.resize(Reading_Sat,(180,180))\n    Reading_Sat = Reading_Sat\/255.\n    \n    Reading_Mask = cv2.cvtColor(cv2.imread(mask_img),cv2.COLOR_BGR2RGB)\n    Reading_Mask = cv2.resize(Reading_Mask,(180,180))\n    Reading_Mask = Reading_Mask\/255.\n    \n    Sat_Image.append(Reading_Sat)\n    Mask_Image.append(Reading_Mask[:,:,0])","08edcdbc":"print(Sat_Image[0].shape)\nprint(Mask_Image[0].shape)","10a8f42a":"print(Sat_Image[0].dtype)\nprint(Mask_Image[0].dtype)","a913aa3e":"Sat_Array = np.array(Sat_Image)\nMask_Array = np.array(Mask_Image)","a0da9012":"print(Sat_Array.shape)\nprint(Mask_Array.shape)","e3c1931f":"compile_loss = \"binary_crossentropy\"\ncompile_optimizer = \"adam\"\ncompile_metrics = [\"accuracy\"]\ninput_dim = (Sat_Array.shape[1],Sat_Array.shape[2],Sat_Array.shape[3])\noutput_class = 1","0da3c43a":"Early_Stopper = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")\nCheckpoint_Model = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_accuracy\",\n                                                      save_best_only=True,\n                                                      save_weights_only=True,\n                                                      filepath=\".\/modelcheck\")","0dae5294":"Encoder_G = Sequential()\nEncoder_G.add(Conv2D(32, (5,5),kernel_initializer = 'he_normal'))\nEncoder_G.add(ReLU())\nEncoder_G.add(Conv2D(64, (5,5),kernel_initializer = 'he_normal'))\nEncoder_G.add(ReLU())\nEncoder_G.add(Conv2D(128, (5,5),kernel_initializer = 'he_normal'))\nEncoder_G.add(ReLU())","71523d1c":"Decoder_G = Sequential()\nDecoder_G.add(Conv2DTranspose(64,(5,5)))\nDecoder_G.add(ReLU())\nDecoder_G.add(Conv2DTranspose(32,(5,5)))\nDecoder_G.add(ReLU())\nDecoder_G.add(Conv2DTranspose(1,(5,5)))\nDecoder_G.add(ReLU())","3e73489e":"Auto_Encoder = Sequential([Encoder_G,Decoder_G])","e5695e51":"Auto_Encoder.compile(loss=compile_loss,optimizer=compile_optimizer,metrics=compile_metrics)","dd2e682a":"Auto_Encoder_Model = Auto_Encoder.fit(Sat_Array,Mask_Array,epochs=5,callbacks=[Early_Stopper,Checkpoint_Model])","1c6d3d34":"Prediction_IMG = Auto_Encoder.predict(Sat_Array[:5])","1fe4bae1":"print(Prediction_IMG[0].shape)","ed1e5b1e":"prediction_img_number = 1\nprint(\"NORMAL\")\nplt.imshow(Sat_Array[prediction_img_number])\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG[prediction_img_number])","d478460d":"prediction_img_number = 2\nprint(\"NORMAL\")\nplt.imshow(Sat_Array[prediction_img_number])\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG[prediction_img_number])","895b6fa6":"prediction_img_number = 3\nprint(\"NORMAL\")\nplt.imshow(Sat_Array[prediction_img_number])\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG[prediction_img_number])","3135cdc2":"backend.set_image_data_format('channels_last')\n\nNon_S_IMG = cv2.cvtColor(cv2.imread(\"..\/input\/satellitegooglemapsmasks\/content\/drive\/MyDrive\/Google maps\/train\/images\/1013.jpg\"),\n                        cv2.COLOR_BGR2RGB)\n\nResize_IMG = cv2.resize(Non_S_IMG,(180,180))\nResize_IMG = Resize_IMG\/255.","37e15c46":"print(Resize_IMG.shape)","97b8f5f8":"Resize_IMG_Prediction = Resize_IMG.reshape(-1,Resize_IMG.shape[0],Resize_IMG.shape[1],Resize_IMG.shape[2])","9fbd42af":"print(Resize_IMG_Prediction.shape)","2e166758":"Prediction_IMG_Another = Auto_Encoder.predict(Resize_IMG_Prediction)","6be5aaf1":"print(Prediction_IMG_Another.shape)","051a9500":"Prediction_IMG_Another = Prediction_IMG_Another.reshape(Prediction_IMG_Another.shape[1],\n                                                        Prediction_IMG_Another.shape[2],\n                                                        Prediction_IMG_Another.shape[3])","01bc41b8":"print(Prediction_IMG_Another.shape)","a47102ec":"print(\"NORMAL\")\nplt.imshow(Resize_IMG)\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG_Another)","26d502fb":"backend.set_image_data_format('channels_last')\n\nNon_S_IMG = cv2.cvtColor(cv2.imread(\"..\/input\/satellitegooglemapsmasks\/content\/drive\/MyDrive\/Google maps\/train\/images\/1022.jpg\"),\n                        cv2.COLOR_BGR2RGB)\n\nResize_IMG = cv2.resize(Non_S_IMG,(180,180))\nResize_IMG = Resize_IMG\/255.\n\nResize_IMG_Prediction = Resize_IMG.reshape(-1,Resize_IMG.shape[0],Resize_IMG.shape[1],Resize_IMG.shape[2])\n\nPrediction_IMG_Another = Auto_Encoder.predict(Resize_IMG_Prediction)\n\nPrediction_IMG_Another = Prediction_IMG_Another.reshape(Prediction_IMG_Another.shape[1],\n                                                        Prediction_IMG_Another.shape[2],\n                                                        Prediction_IMG_Another.shape[3])\n\n\nprint(\"NORMAL\")\nplt.imshow(Resize_IMG)\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG_Another)","f4328774":"backend.set_image_data_format('channels_last')\n\nNon_S_IMG = cv2.cvtColor(cv2.imread(\"..\/input\/satellitegooglemapsmasks\/content\/drive\/MyDrive\/Google maps\/train\/images\/1024.jpg\"),\n                        cv2.COLOR_BGR2RGB)\n\nResize_IMG = cv2.resize(Non_S_IMG,(180,180))\nResize_IMG = Resize_IMG\/255.\n\nResize_IMG_Prediction = Resize_IMG.reshape(-1,Resize_IMG.shape[0],Resize_IMG.shape[1],Resize_IMG.shape[2])\n\nPrediction_IMG_Another = Auto_Encoder.predict(Resize_IMG_Prediction)\n\nPrediction_IMG_Another = Prediction_IMG_Another.reshape(Prediction_IMG_Another.shape[1],\n                                                        Prediction_IMG_Another.shape[2],\n                                                        Prediction_IMG_Another.shape[3])\n\n\nprint(\"NORMAL\")\nplt.imshow(Resize_IMG)\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG_Another)","fb5d35dd":"plt.plot(Auto_Encoder_Model.history['loss'], label = 'training_loss')\nplt.plot(Auto_Encoder_Model.history['accuracy'], label = 'training_accuracy')\nplt.legend()\nplt.grid(True)","330cc2f7":"plot_model(Auto_Encoder, to_file='AEModel.png', show_shapes=True, show_layer_names=True)","ec0d1798":"#### DIRECTION PROCESS","6a465e5e":"# PACKAGES AND LIBRARIES","1c3ad021":"# PATH,LABEL,TRANSFORMATION PROCESS","e9b443b8":"# MODEL","10f18ad3":"#### CHECKING","0136d610":"# DATA PROCESS","6189e98b":"#### PREDICTION","4991c6e8":"#### SPLITTING TRAIN AND VALIDATION","682c72db":"# IMAGE PROCESS","a9a14c40":"#### MAIN PATH","b52112b0":"#### CLASS PROCESS","56db4441":"#### SHUFFLING","a921b464":"#### SPLITTING PATH AND PROCESS","1c0cedc2":"#### SPECIAL PREDICTION \/ THE MODEL HAS NEVER SEEN BEFORE"}}