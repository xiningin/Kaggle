{"cell_type":{"34da995f":"code","f02e660a":"code","d8c9c44e":"code","31a89a3f":"code","e61bb992":"code","8d3243fe":"code","4a2eceee":"code","56e0239f":"code","bfeb7f7a":"code","f731cff2":"code","0dabcf85":"code","5c07d7d8":"code","dc4ec003":"code","6631fd99":"code","e111eb32":"code","e26be3c9":"code","367bd3aa":"code","da655dee":"code","63f1410d":"code","cbf4c971":"code","427aea69":"code","25dbae08":"code","d3b51ec7":"code","b2351dc4":"code","20b39c20":"code","dbb49b83":"code","2b924800":"code","2020b383":"code","2e971c2a":"code","58fc6506":"markdown","80fde0f3":"markdown","d1bebda8":"markdown","29c088ea":"markdown","e7859d6f":"markdown","60efcf0a":"markdown","e4bd175f":"markdown","ea04babb":"markdown","8816079b":"markdown","bdcaa657":"markdown","d1c9f088":"markdown","bebbda57":"markdown","341a037f":"markdown","9a4cb96d":"markdown","50d86bbb":"markdown","e8adeef7":"markdown","8218d04d":"markdown","2cffd039":"markdown"},"source":{"34da995f":"import numpy as np \nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nimport string as s\nimport re\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f02e660a":"cb_data= pd.read_csv('\/kaggle\/input\/clickbait-dataset\/clickbait_data.csv')\ncb_data.head()","d8c9c44e":"x=cb_data.headline\ny=cb_data.clickbait\ntrain_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.25,random_state=2)","31a89a3f":"print(\"No. of elements in training set\")\nprint(train_x.size)\nprint(\"No. of elements in testing set\")\nprint(test_x.size)","e61bb992":"train_x.head(10)","8d3243fe":"train_y.head(10)","4a2eceee":"test_x.head()","56e0239f":"test_y.head()","bfeb7f7a":"def tokenization(text):\n    lst=text.split()\n    return lst\ntrain_x=train_x.apply(tokenization)\ntest_x=test_x.apply(tokenization)","f731cff2":"def lowercasing(lst):\n    new_lst=[]\n    for i in lst:\n        i=i.lower()\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lowercasing)\ntest_x=test_x.apply(lowercasing)  ","0dabcf85":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for j in s.punctuation:\n            i=i.replace(j,'')\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_punctuations)\ntest_x=test_x.apply(remove_punctuations)  ","5c07d7d8":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n    for i in lst:\n        for j in s.digits:    \n            i=i.replace(j,'')\n        nodig_lst.append(i)\n    for i in nodig_lst:\n        if i!='':\n            new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_numbers)\ntest_x=test_x.apply(remove_numbers)","dc4ec003":"print(\"All stopwords of English language \")\n\", \".join(stopwords.words('english'))","6631fd99":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_stopwords)\ntest_x=test_x.apply(remove_stopwords)  ","e111eb32":"def remove_spaces(lst):\n    new_lst=[]\n    for i in lst:\n        i=i.strip()\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_spaces)\ntest_x=test_x.apply(remove_spaces)","e26be3c9":"train_x.head()","367bd3aa":"test_x.head()","da655dee":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatzation(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lemmatzation)\ntest_x=test_x.apply(lemmatzation)","63f1410d":"train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=test_x.apply(lambda x: ''.join(i+' ' for i in x))","cbf4c971":"freq_dist={}\nfor i in train_x.head(20):\n    x=i.split()\n    for j in x:\n        if j not in freq_dist.keys():\n            freq_dist[j]=1\n        else:\n            freq_dist[j]+=1\nfreq_dist","427aea69":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer()\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)","25dbae08":"print(\"Number of features extracted\")\nprint(len(tfidf.get_feature_names()))\nprint()\nprint(\"The 100 features extracted from TF-IDF \")\nprint(tfidf.get_feature_names()[:100])\n","d3b51ec7":"print(\"Shape of train set\",train_1.shape)\nprint(\"Shape of test set\",test_1.shape)","b2351dc4":"train_arr=train_1.toarray()\ntest_arr=test_1.toarray()","20b39c20":"NB_MN=MultinomialNB()\n","dbb49b83":"NB_MN.fit(train_arr,train_y)\npred=NB_MN.predict(test_arr)\nprint('first 20 actual labels: ',test_y.tolist()[:20])\nprint('first 20 predicted labels: ',pred.tolist()[:20])","2b924800":"from sklearn.metrics import f1_score,accuracy_score\nprint(\"F1 score of the model\")\nprint(f1_score(test_y,pred))\nprint(\"Accuracy of the model\")\nprint(accuracy_score(test_y,pred))\nprint(\"Accuracy of the model in percentage\")\nprint(accuracy_score(test_y,pred)*100,\"%\")","2020b383":"from sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(test_y,pred))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report\")\nprint(classification_report(test_y,pred))\n","2e971c2a":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(ngram_range=(1,3),max_features=6500)\ntrain_2=tfidf.fit_transform(train_x)\ntest_2=tfidf.transform(test_x)\n\nNB_MN.fit(train_2.toarray(),train_y)\npred2=NB_MN.predict(test_2.toarray())\n\nprint(\"Accuracy of the model in percentage\")\nprint(accuracy_score(test_y,pred2)*100,\"%\")\n\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(test_y,pred2))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report\")\nprint(classification_report(test_y,pred2))","58fc6506":"# TF-IDF (Term frequency-Inverse Data Frequency)\n\nThis method is used to convert the text into features.","80fde0f3":"# Removing Stopwords","d1bebda8":"# Lemmatization\n\nLemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. It involves the morphological analysis of words.\n\nIn lemmatization we find the root word or base form of the word rather than just clipping some characters from the end e.g. *is, are, am* are all converted to its base form *be* in Lemmatization\n\nHere lemmatization is done using NLTK library.","29c088ea":"# Tokenization of Data\n\nThe data is tokenized i.e. split into tokens which are the smallest or minimal meaningful units. The data is split into words.","e7859d6f":"# Loading the Dataset","60efcf0a":"# Clickbait detector using Naive Bayes Classifier\n\nThis kernel focuses on classifying News headlines into clickbaits and non-clickbaits.\n\nThe clickbaits are labelled as **1** and non-clickbaits as **0**.\nThe headlines are collected from different news sites.\n\nThe dataset consists of 32000 headlines of which 50% are clickbaits and the other 50% are non-clickbait.\n\nI have used a *Multinomial Naive Bayes* classification algorithm for text classification of the given dataset. ","e4bd175f":"# Converting to lowercase\n\nThe data is converted into lowercase to avoid ambiguity between same words in different cases like 'NLP', 'nlp' or 'Nlp'. ","ea04babb":"# Evaluation of Result\n\nThe Accuracy and F1 score of the model are printed to evaluate the model for text classification.","8816079b":"# Removing punctuation\n\nThe punctuations are removed to increase the efficiency of the model. They are irrelevant because they provide no added information.","bdcaa657":"# Analyzing Train and Test Data","d1c9f088":"# Importing different tools and libraries\n\nThe main libraries used are *Numpy*, *Pandas*, *NLTK*(Natural language toolkit) and *Scikit-learn*.","bebbda57":"# Splitting into Train and Test sets\n\nThe dataset is splitted into training and testing sets. The percentage of training data is 75% and testing data is 25%.","341a037f":"# Removing Numbers","9a4cb96d":"# Removing extra spaces","50d86bbb":"# Analyzing data after preprocessing\n\nAfter preprocessing the data i.e. after removing punctuation, stopwords, spaces and numbers.","e8adeef7":"## A second model for TF-IDF with different n-grams and fixed feature size","8218d04d":"# Training the model","2cffd039":"# Define Naive Bayes Classifier"}}