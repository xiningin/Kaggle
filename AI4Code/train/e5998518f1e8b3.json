{"cell_type":{"47b584dd":"code","81f433dd":"code","dad35470":"code","5cda8361":"code","e6dba60e":"code","a4d14f8c":"code","c193f597":"code","7bf5de9a":"code","178e3de9":"code","a7dc4c20":"code","442bef56":"code","52bd3ed5":"code","a9a8230a":"code","fdb4f5b3":"code","d9f34edd":"code","82580dbb":"code","50eebfa1":"code","62cc6109":"code","cc81c5c4":"code","f761002f":"code","e1b8b6fc":"code","e3e04d01":"code","21729eee":"code","6aa8df55":"code","fc018b04":"code","5d922cd9":"code","6b703943":"code","19e9baa5":"code","f9d80efe":"code","57f76119":"code","25a8497f":"code","8eecae15":"code","390cd8ff":"code","9f6632c5":"code","ac4efa9e":"code","7be01015":"code","627668d2":"code","2004bb5d":"code","bebce06f":"code","913cfa40":"code","5e0c419a":"code","6aade34a":"code","290c37d4":"code","5e048acb":"code","cf912888":"code","2a60f10f":"code","589c9e77":"code","2c12a607":"code","79505a0b":"code","a524819c":"code","aabd554a":"code","91312e5f":"code","44e9b049":"code","60b9eb68":"code","6b31095b":"code","f60a4cad":"code","816b1ece":"code","2896013f":"code","1234d601":"code","25da4e17":"code","20d5bf37":"code","fb20ace9":"code","ddaf59e8":"code","aee07068":"code","0ffa77e0":"code","cc27c439":"code","8cd0e655":"code","554cc0c9":"code","1fd1729f":"code","5cae2b98":"code","fd86649e":"code","a1c59a51":"code","019a47a1":"code","8dd013e6":"code","2af411a5":"code","8a9a86bd":"code","4315ee90":"code","73f38752":"code","764a4f5d":"code","f1a32853":"code","511951b1":"code","c3f449d4":"code","91e8897f":"code","4999c947":"code","24291f33":"code","510a3e47":"code","6f13fa12":"code","7a03bd22":"code","450a0196":"code","b6d08205":"code","c6e22fdc":"code","21f2c727":"code","45444ba1":"code","17d81a26":"code","80859dd7":"code","3171fa4c":"code","8b29bfd5":"code","669113fa":"code","f66051a6":"code","a6a310f6":"code","34f007ef":"code","745aa777":"code","9d0087d2":"code","475976c3":"code","b07084df":"code","33b27263":"code","869127df":"code","a0d75c71":"code","de6f1371":"code","94efb35b":"code","1658fbf3":"code","7ecc585f":"markdown","6cdf8246":"markdown","9b4bd66e":"markdown","e96c03f0":"markdown","718ae243":"markdown","726bca0e":"markdown","8b7c238b":"markdown","286023e6":"markdown","8dc45e5a":"markdown","2f4e582e":"markdown","e098cd80":"markdown","404d0cfa":"markdown","5c4aade1":"markdown","2de60754":"markdown","2e2bf33c":"markdown"},"source":{"47b584dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.tabular import *\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nimport gc\ngc.collect()\n\nimport re\nimport os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMRegressor\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom functools import lru_cache\nfrom tqdm import tqdm as tqdm\nfrom fastai.text import *\nfrom fastai.metrics import *","81f433dd":"def seed_everything(seed):\n    '''\n    Seeds all sources of randomness in machine\n    '''\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 42\nseed_everything(SEED)","dad35470":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\nsub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","5cda8361":"train.shape, test.shape, sub.shape","e6dba60e":"# List of punctuations to handle\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\n', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\n# Dictionary to handle misspellings and to standardized how phrases are written\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    '''\n    1. Handles punctuations by adding whitespace to both sides of the punctuation.\n    2. This allows the NLP model to account for the \"meaning\" of punctuations separately from the words themselves.\n    3. This also increases the frequency of the tokens (less sparse), which makes them more effective features.\n    '''\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    \"\"\"\n    Replace numbers with as many `#` as there are digits in the number (min 2, max 5).\n    This can give numbers more meaning in the context of NLP modelling.\n    This is because it now turns into a higher frequency token that indicates, whether it's a big number (i.e. #####) or a small number (i.e. ##)\n    \"\"\"\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    \"\"\"\n    Returns the mispelling dictionary and the regex that's supposed to apply all the mappings to the text\n    \"\"\"\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    \"\"\"\n    Replace misspellings with the \"correct\" spellings\n    \"\"\"\n    \n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    \"\"\"\n    Apply all the text processing functions above to the text\n    \"\"\"\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","a4d14f8c":"# Distinguish between the target columns and the input columns\n\ntarget_cols_questions = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']\n\ntarget_cols_answers = ['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ntargets = target_cols_questions + target_cols_answers\n\ninput_columns = ['question_title', 'question_body', 'answer']","c193f597":"# Clean data using the preprocessing script\ntrain = clean_data(train, ['answer', 'question_body', 'question_title'])\ntest = clean_data(test, ['answer', 'question_body', 'question_title'])","7bf5de9a":"# Parse the urls from the question and answer text and extract the domains as features\n\nfind = re.compile(r\"^[^.]*\")\n\ntrain['netloc_1'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_1'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_2'] = train['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_2'] = test['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_3'] = train['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_3'] = test['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])","178e3de9":"train['netloc_1'].head(), train['netloc_2'].head(), train['netloc_3'].head()","a7dc4c20":"# Filter to input and target columns for the training set\ntrain = train[input_columns + targets]\n\n# Filter to only input columns for the test set\ntest = test[input_columns]","442bef56":"# Split dataset to a train and validation set with a 20% random sample for the test set\ntrain, val = train_test_split(train, test_size=0.2, shuffle=True, random_state=42)","52bd3ed5":"train.shape, val.shape","a9a8230a":"# Installing packages from local\n!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","fdb4f5b3":"!ls ..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased","d9f34edd":"from collections import defaultdict\nfrom dataclasses import dataclass\nimport functools\nimport gc\nimport itertools\nimport json\nfrom multiprocessing import Pool\nimport os\nfrom pathlib import Path\nimport random\nimport re\nimport shutil\nimport subprocess\nimport time\nfrom typing import Callable, Dict, List, Generator, Tuple\nfrom os.path import join as path_join\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json._json import JsonReader\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, Subset, DataLoader\n\nfrom transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, BertConfig\nfrom transformers.optimization import get_linear_schedule_with_warmup","82580dbb":"# Creating a config object to store task specific information\nclass Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n        \nconfig = Config(\n    testing=False,\n    seed = 42,\n    roberta_model_name='bert-base-uncased', # can also be exchnaged with roberta-large \n    use_fp16=False,\n    bs=16, \n    max_seq_len=128, \n    hidden_dropout_prob=.25,\n    hidden_size=768, # 1024 for roberta-large\n    start_tok = \"[CLS]\",\n    end_tok = \"[SEP]\",\n)","50eebfa1":"# forward tokenizer\n\nclass FastAiRobertaTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]","62cc6109":"# backward tokenizer\n\nclass FastAiRobertaTokenizerBackward(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.end_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.start_tok]","cc81c5c4":"# create fastai tokenizer for roberta\nbert_tok = BertTokenizer.from_pretrained('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt')\n\n# Create fastai tokenizer from bert tokenizer\nfastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(bert_tok, max_seq_len=config.max_seq_len), \n                             pre_rules=[], post_rules=[])\n\n# Create fastai backward tokenizer from bert tokenizer\nfastai_tokenizer_bwd = Tokenizer(tok_func=FastAiRobertaTokenizerBackward(bert_tok, max_seq_len=config.max_seq_len), \n                             pre_rules=[], post_rules=[])","f761002f":"# create fastai vocabulary for roberta\npath = Path()\nbert_tok.save_vocabulary(path)\n   \nfastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","e1b8b6fc":"# Create fastai databunch\n\ndatabunch = TextDataBunch.from_df(\".\", train, val, test,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=input_columns,\n                  label_cols=targets,\n                  bs=16,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\n# Save databunch file\ndatabunch.save('databunch.pkl')","e3e04d01":"# Load the databunch from a saved file\n\ndatabunch = load_data(path, 'databunch.pkl', bs=16)","21729eee":"# Show one batch of data\n\ndatabunch.show_batch()","6aa8df55":"# Note that the question and answer are concatenated into the same string without separating them with a [SEP] in between\n# A proper approach should treat this as a two sequence input (1. Question, 2. Answer). This is the formatting used for SQuAD\n\ndatabunch.single_ds[0]","fc018b04":"start_time = time.time()\n\nseed = 42\n\nnum_labels = len(targets)\nn_epochs = 3\nlr = 2e-5\nwarmup = 0.05\nbatch_size = 16\naccumulation_steps = 4\n\nbert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\n\n# Uncased version of BERT is used - this means capitalizations aren't accounted for by the model\nbert_model = 'bert-base-uncased'\ndo_lower_case = 'uncased' in bert_model\ndevice = torch.device('cuda')\n\n# Output files from model training\noutput_model_file = 'bert_pytorch.bin'\noutput_optimizer_file = 'bert_pytorch_optimizer.bin'\noutput_amp_file = 'bert_pytorch_amp.bin'\n\n# Setting seeds to make \"randomness\" reproducible\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","5d922cd9":"class BertForSequenceClassification(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification\/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n    \"\"\"\n    def __init__(self, config):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask,\n                            inputs_embeds=inputs_embeds)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        return logits","6b703943":"loss_func = nn.BCEWithLogitsLoss()","19e9baa5":"bert_config = BertConfig.from_json_file(bert_model_config)\nbert_config.num_labels = len(targets)\n\nmodel_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/')\n\nmodel = BertForSequenceClassification.from_pretrained(model_path, config=bert_config)\nlearn_bert = Learner(databunch, model, loss_func=loss_func, model_dir='\/temp\/model')","f9d80efe":"def bert_clas_split(self) -> List[nn.Module]:\n    \n    bert = model.bert\n    embedder = bert.embeddings\n    pooler = bert.pooler\n    encoder = bert.encoder\n    classifier = [model.dropout, model.classifier]\n    n = len(encoder.layer)\/\/3\n    print(n)\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","57f76119":"x = bert_clas_split(model)","25a8497f":"learn_bert.layer_groups","8eecae15":"learn_bert","390cd8ff":"learn_bert.split([x[2],  x[4],  x[5]])","9f6632c5":"learn_bert.freeze()","ac4efa9e":"learn_bert.lr_find()","7be01015":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib.style as style\nstyle.use('seaborn-poster')\nstyle.use('ggplot')","627668d2":"learn_bert.recorder.plot(suggestion=True)","2004bb5d":"learn_bert.fit_one_cycle(7, max_lr=slice(1e-3, 1e-2), moms=(0.8,0.7), pct_start=0.2, wd =0.1)","bebce06f":"learn_bert.save('head-1')","913cfa40":"learn_bert.freeze_to(-2)\nlearn_bert.fit_one_cycle(7, max_lr=slice(1e-4, 1e-3), moms=(0.8,0.7), pct_start=0.4, wd =0.1)","5e0c419a":"learn_bert.save('head-2')","6aade34a":"learn_bert.freeze_to(-3)\nlearn_bert.fit_one_cycle(7, max_lr=slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","290c37d4":"learn_bert.unfreeze()\nlearn_bert.lr_find()\nlearn_bert.recorder.plot(suggestion=True)","5e048acb":"learn_bert.fit_one_cycle(12, slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.4, wd =0.1)","cf912888":"bs, bptt = 32, 80\n\ndata_lm = TextLMDataBunch.from_df('.', train, val, test,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=['question_title', 'question_body', 'answer'],\n                  label_cols=targets,\n                  bs=bs,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndata_lm.save('data_lm.pkl')","2a60f10f":"path = \".\"\ndata_lm = load_data(path, 'data_lm.pkl', bs=bs, bptt=bptt)","589c9e77":"path = \".\"\ndata_bwd = load_data(path, 'data_lm.pkl', bs=bs, bptt = bptt, backwards=True)","2c12a607":"data_lm.show_batch()","79505a0b":"data_bwd.show_batch()","a524819c":"awd_lstm_lm_config = dict( emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.1,\n                          hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2, tie_weights=True, out_bias=True)","aabd554a":"awd_lstm_clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.4,\n                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)","91312e5f":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5,\n                               config=awd_lstm_lm_config, pretrained = False)\nlearn = learn.to_fp16(clip=0.1)","44e9b049":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","60b9eb68":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","6b31095b":"learn.fit_one_cycle(2, max_lr=slice(5e-3, 5e-2), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","f60a4cad":"learn.save('fit_head')","816b1ece":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","2896013f":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","1234d601":"learn.recorder.plot_losses()","25da4e17":"learn.save('fine-tuned')\nlearn.load('fine-tuned')\nlearn.save_encoder('fine-tuned-fwd')","20d5bf37":"learn = language_model_learner(data_bwd, AWD_LSTM, drop_mult=0.5,\n                               config=awd_lstm_lm_config, pretrained = False)\nlearn = learn.to_fp16(clip=0.1)","fb20ace9":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","ddaf59e8":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","aee07068":"learn.fit_one_cycle(2, max_lr=slice(5e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","0ffa77e0":"learn.save('fit_head-bwd')","cc27c439":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","8cd0e655":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","554cc0c9":"learn.recorder.plot_losses()","1fd1729f":"learn.save('fine-tuned-bwd')\nlearn.load('fine-tuned-bwd')\nlearn.save_encoder('fine-tuned-bwd')","5cae2b98":"text_cols = ['question_title', \"question_body\", 'answer']","fd86649e":"data_cls = TextClasDataBunch.from_df('.', train, val, test, vocab = data_lm.vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=text_cols,\n                  label_cols=targets,\n                  bs=bs,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndata_cls.save('data_cls.pkl')","a1c59a51":"data_cls = load_data(path, 'data_cls.pkl', bs=bs)","019a47a1":"data_cls.show_batch()","8dd013e6":"data_cls_bwd = load_data(path, 'data_cls.pkl', bs=bs, backwards=True)","2af411a5":"data_cls_bwd.show_batch()","8a9a86bd":"learn = text_classifier_learner(data_cls, AWD_LSTM, drop_mult=0.5,config=awd_lstm_clas_config, pretrained = False, loss_func=loss_func)\nlearn.load_encoder('fine-tuned-fwd')\nlearn = learn.to_fp16(clip=0.1)\n#learn.loss_func = L1LossFlat()\nfnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","4315ee90":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","73f38752":"learn.fit_one_cycle(2, max_lr=slice(1e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","764a4f5d":"learn.save('first-head')\nlearn.load('first-head')","f1a32853":"learn.freeze_to(-2)\nlearn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","511951b1":"learn.save('second')\nlearn.load('second')","c3f449d4":"learn.freeze_to(-3)\nlearn.fit_one_cycle(2, slice(1e-4\/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","91e8897f":"learn.save('third')\nlearn.load('third')","4999c947":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","24291f33":"learn.fit_one_cycle(7, slice(1e-4\/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","510a3e47":"learn.recorder.plot_losses()","6f13fa12":"learn.save('fwd-cls')","7a03bd22":"learn_bwd = text_classifier_learner(data_cls_bwd, AWD_LSTM, drop_mult=0.5, config=awd_lstm_clas_config, loss_func=loss_func,\n                                    pretrained = False)\nlearn_bwd.load_encoder('fine-tuned-bwd')\nlearn_bwd = learn_bwd.to_fp16(clip=0.1)","450a0196":"fnames = ['..\/input\/awd-lstm\/lstm_wt103.pth','..\/input\/awd-lstm\/itos_wt103.pkl']\nlearn_bwd.load_pretrained(*fnames, strict=False)\nlearn_bwd.freeze()","b6d08205":"learn_bwd.lr_find()\nlearn_bwd.recorder.plot(suggestion=True)","c6e22fdc":"learn_bwd.fit_one_cycle(2, max_lr=slice(5e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =0.1)","21f2c727":"learn_bwd.save('first-head-bwd')\nlearn_bwd.load('first-head-bwd')","45444ba1":"learn_bwd.freeze_to(-2)\nlearn_bwd.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","17d81a26":"learn_bwd.save('second-bwd')\nlearn_bwd.load('second-bwd')","80859dd7":"learn_bwd.freeze_to(-3)\nlearn_bwd.fit_one_cycle(2, slice(1e-5\/(2.6**4),1e-5), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","3171fa4c":"learn_bwd.save('third-bwd')\nlearn_bwd.load('third-bwd')","8b29bfd5":"learn_bwd.unfreeze()\nlearn_bwd.lr_find()\nlearn_bwd.recorder.plot(suggestion=True)","669113fa":"learn_bwd.fit_one_cycle(7, slice(1e-5\/(2.6**4),1e-5), moms=(0.8,0.7), pct_start=0.3, wd =0.1)","f66051a6":"learn_bwd.recorder.plot_losses()","a6a310f6":"learn_bwd.save('bwd-cls')","34f007ef":"def get_ordered_preds(learn_bert, ds_type, preds):\n  np.random.seed(42)\n  sampler = [i for i in learn_bert.data.dl(ds_type).sampler]\n  reverse_sampler = np.argsort(sampler)\n  preds = [p[reverse_sampler] for p in preds]\n  return preds","745aa777":"test_raw_preds = learn_bert.get_preds(ds_type=DatasetType.Test)\ntest_preds_bert = get_ordered_preds(learn_bert, DatasetType.Test, test_raw_preds)","9d0087d2":"pred_fwd_test, lbl_fwd_test = learn.get_preds(ds_type=DatasetType.Test,ordered=True)\npred_bwd_test, lbl_bwd_test = learn_bwd.get_preds(ds_type=DatasetType.Test,ordered=True)","475976c3":"type(pred_fwd_test)","b07084df":"test_preds_bert = torch.FloatTensor(test_preds_bert[0])","33b27263":"final_preds_test = (0.3*pred_fwd_test + 0.3*pred_bwd_test + 0.4*test_preds_bert)","869127df":"sub.iloc[:, 1:] = final_preds_test.numpy()\nsub.to_csv('submission.csv', index=False)\nsub.head()","a0d75c71":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n    sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\nplt.tight_layout()\nplt.show()\nplt.close()","de6f1371":"# y_train = train[targets].values\n\n# for column_ind in range(30):\n#     curr_column = y_train[:, column_ind]\n#     values = np.unique(curr_column)\n#     map_quantiles = []\n#     for val in values:\n#         occurrence = np.mean(curr_column == val)\n#         cummulative = sum(el['occurrence'] for el in map_quantiles)\n#         map_quantiles.append({'value': val, 'occurrence': occurrence, 'cummulative': cummulative})\n            \n#     for quant in map_quantiles:\n#         pred_col = test_preds_bert[0][:, column_ind]\n#         q1, q2 = np.quantile(pred_col, quant['cummulative']), np.quantile(pred_col, min(quant['cummulative'] + quant['occurrence'], 1))\n#         pred_col[(pred_col >= q1) & (pred_col <= q2)] = quant['value']\n#         test_preds_bert[0][:, column_ind] = pred_col","94efb35b":"# sub.iloc[:, 1:] = test_preds_bert[0].numpy()\n# sub.to_csv('submission.csv', index=False)\n# sub.head()","1658fbf3":"# fig, axes = plt.subplots(6, 5, figsize=(18, 15))\n# axes = axes.ravel()\n# bins = np.linspace(0, 1, 20)\n\n# for i, col in enumerate(targets):\n#     ax = axes[i]\n#     sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n#     sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n#     # ax.set_title(col)\n#     ax.set_xlim([0, 1])\n# plt.tight_layout()\n# plt.show()\n# plt.close()","7ecc585f":"# train-val-test split","6cdf8246":"# Prediction","9b4bd66e":"# Forward Training","e96c03f0":"## Cleaning the data","718ae243":"## Setup model","726bca0e":"# Transformers","8b7c238b":"# Fastai - ULMFiT","286023e6":"# Splitting the model","8dc45e5a":"# Import data and Libraries","2f4e582e":"## Forward Training","e098cd80":"### Forward Training","404d0cfa":"# Modelling","5c4aade1":"## Build BERT Model","2de60754":"## Backward Training\n","2e2bf33c":"# Backward Training"}}