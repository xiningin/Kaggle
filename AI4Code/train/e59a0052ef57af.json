{"cell_type":{"7e97ed78":"code","57949134":"code","0bd2dbbb":"code","61fd1526":"code","123dc600":"code","3ac6e333":"code","e4eedbf8":"code","3eb27919":"code","591535fa":"code","d1a52894":"code","5018d9c8":"code","cb6ee03c":"code","181f8407":"code","3d0280ff":"code","2d1d0492":"code","f2ab513b":"code","d0243e40":"code","46c400e2":"code","147e6625":"code","6ce3c15f":"code","66ccf42a":"code","0295938a":"code","b49b7932":"code","47423d06":"code","be66ae29":"markdown","d6ec9113":"markdown","cbb4172e":"markdown","47c179ae":"markdown","920ea827":"markdown","508577c7":"markdown","aefa8ae2":"markdown","e318dda5":"markdown","8082197f":"markdown","32b36ba1":"markdown","2d213764":"markdown","5ad401db":"markdown","86ef419b":"markdown","4bbc0c1f":"markdown","185e241d":"markdown","5c9e5a14":"markdown","647223f7":"markdown","eedf7053":"markdown","12889652":"markdown","72b3e7a1":"markdown","4da89a1b":"markdown","deb2ee4f":"markdown","cfa00022":"markdown","a47e7fe2":"markdown","677249da":"markdown","8bc2ccb1":"markdown","0f19345d":"markdown","9ef59c72":"markdown","5a52c524":"markdown","5c9254b7":"markdown","01efd51c":"markdown","3499ce24":"markdown","196e915f":"markdown","1a5c9fbb":"markdown","bbb05504":"markdown"},"source":{"7e97ed78":"#Manipula\u00e7\u00e3o dos dados e visualiza\u00e7\u00e3o\nimport numpy as np \nimport pandas as pd \nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\n\n#Normaliza\u00e7\u00e3o dos dados\nfrom sklearn.preprocessing import MinMaxScaler\n\n#Sele\u00e7\u00e3o de vari\u00e1veis\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n#Divis\u00e3o de dados\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\n#Modelos de Machine Learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n#M\u00e9tricas\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\n\n#Otimiza\u00e7\u00e3o Performance com M\u00e9todos Ensemble\nfrom sklearn.ensemble import BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","57949134":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","0bd2dbbb":"df.columns","61fd1526":"#Tamanho do df\ndf.shape","123dc600":"#Verificar se existe valores NaN\ndf.isnull().sum()","3ac6e333":"#Tipos de dados que iremos trabalhar\ndf.dtypes","e4eedbf8":"#Verificar a distribui\u00e7\u00e3o entre casos confirmados e n\u00e3o confirmados, o ideal seria uma distribui\u00e7\u00e3o de 50%.\n\n(df['target'].value_counts()\/df.shape[0])*100","3eb27919":"#Skew se refere a distribui\u00e7\u00e3o dos dados que \u00e9 assumida ser normal ou gaussiana. \n#Muitos algoritmos de Machine Learning consideram que os dados possuem uma distribui\u00e7\u00e3o normal.\n\ndf.skew()","591535fa":"correlacao = df.corr()\nmask = np.zeros_like(correlacao)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    \n    f, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(correlacao,center=0,cmap=\"coolwarm\",mask=mask,annot=True, square=True, fmt='.2f')","d1a52894":"#Normaliza\u00e7\u00e3o sobre as escalas de valores\n\ndf_X = df.drop(columns=[\"target\"])\ndf_Y = df[\"target\"]\n\n\nX = df_X.values\nY = df_Y.values\n\nscaler = MinMaxScaler(feature_range=(0,1))\nrescaledx = scaler.fit_transform(X)","5018d9c8":"#Extra\u00e7ao de vari\u00e1veis\ntestagem = SelectKBest(score_func=chi2,k=7)\nfit = testagem.fit(rescaledx,Y)\n\n#Sumariza\u00e7\u00e3o do score\nscore = fit.scores_\ncolunas = df_X.columns\n\nnew_df = pd.DataFrame()\nnew_df[\"Colunas\"] = colunas\nnew_df[\"Score chi2\"] = score\n","cb6ee03c":"#Criando o modelo\nmodelo = LogisticRegression(max_iter=1000)\n\n#Recursive Feature Elimination (RFE)\nrfe = RFE(modelo, n_features_to_select=1)\nfit = rfe.fit(rescaledx,Y)\n\n#Resultados\nnew_df[\"ranking RFE\"] = fit.ranking_","181f8407":"#Criando o modelo\nmodelo = ExtraTreesClassifier()\nmodelo.fit(rescaledx,Y)\n\nnew_df[\"Enseable\"] = modelo.feature_importances_","3d0280ff":"#Melhores vari\u00e1veis \nprint(new_df)\n\n\n#Vari\u00e1veis qui-quadrado(Chi2)\ncolunas_chi2 = [\"exang\",\"cp\",\"ca\",\"oldpeak\",\"sex\",\"slope\",\"thal\"]\n\n#Vari\u00e1veis RFE\ncolunas_rfe = [\"oldpeak\",\"thalach\",\"ca\",\"thal\",\"cp\",\"slope\",\"sex\"]\n\n#Vari\u00e1veis Enseable\ncolunas_enseable = [\"ca\",\"cp\",\"exang\",\"thal\",\"thalach\",\"oldpeak\",\"age\"]","2d1d0492":"#DataFrame para verifica\u00e7ao de todos os modelos\n\ndf_modelos = pd.DataFrame(columns=[\"Modelo\",\"Categoria\",\"Tempo (s)\",\"Train AUC ROC\",\"Train ACUR\u00c1CIA\",\"Train PRECIS\u00c3O\",\"Train RECALL\",\n                                             \"Test AUC ROC\",\"Test ACUR\u00c1CIA\",\"Test PRECIS\u00c3O\",\"Test RECALL\"])","f2ab513b":"#Selece\u00e7ao de modelos\n\ndef selecao_modelos(df,colunas,previsao,df_modelo,variavel):\n    \n    df_X = df[colunas]\n    df_Y = df[previsao]\n    \n    #Realizando o split sobre os dados\n    variaveis = variavel\n    seed = 7   \n    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.25,random_state=seed)  \n    \n   \n    #Separando o array em componentes X e Y\n    X = X_train\n    Y = y_train\n    \n    #Normaliza\u00e7\u00e3o\n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx = scaler.fit_transform(X)\n    \n    #Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    num_instances = len(rescaledx)\n    \n    \n    #Preparando o modelo\n    modelos = []\n    modelos.append((\"LR\",LogisticRegression(max_iter=400)))\n    modelos.append((\"LDA\",LinearDiscriminantAnalysis()))\n    modelos.append((\"NB\",GaussianNB()))\n    modelos.append((\"KNN\",KNeighborsClassifier()))\n    modelos.append((\"CART\",DecisionTreeClassifier()))\n    modelos.append((\"SVM\",SVC()))\n       \n    for nome,modelo in modelos:\n        \n        kfold = StratifiedKFold(n_splits = num_folds,random_state=seed,shuffle = True)\n        \n        inicio =time.time()\n        cv_roc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"roc_auc\")\n        cv_acc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"accuracy\")\n        cv_prec = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"precision\")\n        cv_recall = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"recall\")\n        \n        \n        modelo_test = modelo.fit(rescaledx,Y)\n        \n        #Normaliza\u00e7\u00e3o\n        \n        scaler = MinMaxScaler(feature_range=(0,1))\n        rescaledx_test = scaler.fit_transform(X_test)\n        \n        y_pred = modelo_test.predict(rescaledx_test)\n\n        cv_acc_test = accuracy_score(y_test,y_pred)\n        cv_roc_test = roc_auc_score(y_test,y_pred)\n        cv_prec_test = precision_score(y_test,y_pred)\n        cv_recall_test = recall_score(y_test,y_pred)\n        \n        fim = time.time()\n\n        \n                \n        msg = \"Modelo: %s, AUC ROC: %.2f%% (%.2f%%) Acur\u00e1cia: %.2f%% (%.2f%%) Precis\u00e3o: %.2f%% (%.2f%%) Recall: %.2f%% (%.2f%%) \" %(nome,cv_roc.mean()*100,cv_roc.std()*100,\n                                                                          cv_acc.mean()*100,cv_acc.std()*100,\n                                                                          cv_prec.mean()*100,cv_prec.std()*100,\n                                                                          cv_recall.mean()*100,cv_recall.std()*100)\n        tempo = fim - inicio\n        print(msg)\n        df_modelo.loc[len(df_modelo)+1] = [nome,variaveis,tempo,cv_roc.mean()*100,cv_acc.mean()*100,cv_prec.mean()*100,cv_recall.mean()*100,\n                                                cv_acc_test.mean()*100,cv_roc_test.mean()*100,cv_prec_test.mean()*100,cv_recall_test.mean()*100]\n       \n              \n        \nprint(\"Modelo CHI2\")\nselecao_modelos(df,colunas_chi2,\"target\",df_modelo = df_modelos,variavel=\"CHI2\")\n\nprint(\"\\n\")\nprint(\"Modelo RFE\")\nselecao_modelos(df,colunas_rfe,\"target\",df_modelo = df_modelos,variavel=\"RFE\")\n\nprint(\"\\n\")\nprint(\"Modelo ENSEABLE\")\nselecao_modelos(df,colunas_enseable,\"target\",df_modelo = df_modelos,variavel=\"ENSEABLE\")","d0243e40":"# Bagged Decision Trees\n\n\ndef selecao_modelos(df,colunas,previsao,df_modelo,variavel):\n    df_X = df[colunas]\n    df_Y = df[previsao]\n    \n    #Realizando o split sobre os dados\n    variaveis = variavel\n    seed = 7   \n    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.25,random_state=seed)  \n    \n   \n    #Separando o array em componentes X e Y\n    X = X_train\n    Y = y_train\n    \n    #Normaliza\u00e7\u00e3o\n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx = scaler.fit_transform(X)\n    \n    #Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    num_instances = len(rescaledx)\n    \n    #Preparando o modelo\n    modelos = []\n    modelos.append((\"BDT LR\",LogisticRegression(max_iter=400)))\n    modelos.append((\"BDT LDA\",LinearDiscriminantAnalysis()))\n    modelos.append((\"BDT NB\",GaussianNB()))\n    modelos.append((\"BDT KNN\",KNeighborsClassifier()))\n    modelos.append((\"BDT CART\",DecisionTreeClassifier()))\n    modelos.append((\"BDT SVM\",SVC()))\n    \n    \n    for nome,modelo in modelos:\n        kfold = StratifiedKFold(n_splits = num_folds,random_state=seed,shuffle = True)\n        \n        inicio =time.time()\n        modelo_bagging = BaggingClassifier(base_estimator=modelo,\n                                          n_estimators = 100,random_state = seed)\n\n        cv_roc = cross_val_score(modelo_bagging,rescaledx,Y,cv = kfold,scoring=\"roc_auc\")\n        cv_acc = cross_val_score(modelo_bagging,rescaledx,Y,cv = kfold,scoring=\"accuracy\")\n        cv_prec = cross_val_score(modelo_bagging,rescaledx,Y,cv = kfold,scoring=\"precision\")\n        cv_recall = cross_val_score(modelo_bagging,rescaledx,Y,cv = kfold,scoring=\"recall\")\n\n        \n        \n        modelo_test = modelo_bagging.fit(rescaledx,Y)\n        \n        #Normaliza\u00e7\u00e3o\n        \n        scaler = MinMaxScaler(feature_range=(0,1))\n        rescaledx_test = scaler.fit_transform(X_test)\n        \n        y_pred = modelo_test.predict(rescaledx_test)\n\n        cv_acc_test = accuracy_score(y_test,y_pred)\n        cv_roc_test = roc_auc_score(y_test,y_pred)\n        cv_prec_test = precision_score(y_test,y_pred)\n        cv_recall_test = recall_score(y_test,y_pred)\n\n        \n        fim = time.time()\n        \n                \n        msg = \"Modelo: %s, AUC ROC: %.2f%% (%.2f%%) Acur\u00e1cia: %.2f%% (%.2f%%) Precis\u00e3o: %.2f%% (%.2f%%) Recall: %.2f%% (%.2f%%) \" %(nome,cv_roc.mean()*100,cv_roc.std()*100,\n                                                                          cv_acc.mean()*100,cv_acc.std()*100,\n                                                                          cv_prec.mean()*100,cv_prec.std()*100,\n                                                                          cv_recall.mean()*100,cv_recall.std()*100)\n        tempo = fim - inicio\n        print(msg)\n        df_modelo.loc[len(df_modelo)+1] = [nome,variaveis,tempo,cv_roc.mean()*100,cv_acc.mean()*100,cv_prec.mean()*100,cv_recall.mean()*100,\n                                                cv_acc_test.mean()*100,cv_roc_test.mean()*100,cv_prec_test.mean()*100,cv_recall_test.mean()*100]\n\nprint(\"Modelo CHI2\")\nselecao_modelos(df,colunas_chi2,\"target\",df_modelo = df_modelos,variavel=\"CHI2\")\nprint(\"\\n\")\nprint(\"Modelo RFE\")\nselecao_modelos(df,colunas_rfe,\"target\",df_modelo = df_modelos,variavel=\"RFE\")\nprint(\"\\n\")\nprint(\"Modelo ENSEABLE\")\nselecao_modelos(df,colunas_enseable,\"target\",df_modelo = df_modelos,variavel=\"ENSEABLE\")","46c400e2":"# Random Forest\n\n\ndef selecao_modelos(df,colunas,previsao,df_modelo,variavel):\n    df_X = df[colunas]\n    df_Y = df[previsao]\n    \n    #Realizando o split sobre os dados\n    variaveis = variavel\n    seed = 7   \n    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.25,random_state=seed)  \n    \n   \n    #Separando o array em componentes X e Y\n    X = X_train\n    Y = y_train\n    \n    #Normaliza\u00e7\u00e3o\n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx = scaler.fit_transform(X)\n    \n    #Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    num_instances = len(rescaledx)\n    \n    nome = \"Random Forest\"\n    \n    kfold = StratifiedKFold(n_splits = num_folds,random_state=seed,shuffle = True)\n    inicio =time.time()\n        \n    modelo= RandomForestClassifier(n_estimators = 100,max_features = 3)\n        \n    cv_roc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"roc_auc\")\n    cv_acc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"accuracy\")\n    cv_prec = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"precision\")\n    cv_recall = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"recall\")\n\n        \n    \n    modelo_test = modelo.fit(rescaledx,Y)\n    \n    #Normaliza\u00e7\u00e3o\n        \n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx_test = scaler.fit_transform(X_test)\n        \n    y_pred = modelo_test.predict(rescaledx_test)\n\n    cv_acc_test = accuracy_score(y_test,y_pred)\n    cv_roc_test = roc_auc_score(y_test,y_pred)\n    cv_prec_test = precision_score(y_test,y_pred)\n    cv_recall_test = recall_score(y_test,y_pred)\n\n    fim = time.time()    \n                \n    msg = \"Modelo: %s, AUC ROC: %.2f%% (%.2f%%) Acur\u00e1cia: %.2f%% (%.2f%%) Precis\u00e3o: %.2f%% (%.2f%%) Recall: %.2f%% (%.2f%%) \" %(nome,cv_roc.mean()*100,cv_roc.std()*100,\n                                                                          cv_acc.mean()*100,cv_acc.std()*100,\n                                                                          cv_prec.mean()*100,cv_prec.std()*100,\n                                                                          cv_recall.mean()*100,cv_recall.std()*100)\n    tempo = fim - inicio\n    print(msg)\n    df_modelo.loc[len(df_modelo)+1] = [nome,variaveis,tempo,cv_roc.mean()*100,cv_acc.mean()*100,cv_prec.mean()*100,cv_recall.mean()*100,\n                                                cv_acc_test.mean()*100,cv_roc_test.mean()*100,cv_prec_test.mean()*100,cv_recall_test.mean()*100]\n\nprint(\"Modelo CHI2\")\nselecao_modelos(df,colunas_chi2,\"target\",df_modelo = df_modelos,variavel=\"CHI2\")\nprint(\"\\n\")\nprint(\"Modelo RFE\")\nselecao_modelos(df,colunas_rfe,\"target\",df_modelo = df_modelos,variavel=\"RFE\")\nprint(\"\\n\")\nprint(\"Modelo ENSEABLE\")\nselecao_modelos(df,colunas_enseable,\"target\",df_modelo = df_modelos,variavel=\"ENSEABLE\")","147e6625":"# AdaBoost\n\ndef selecao_modelos(df,colunas,previsao,df_modelo,variavel):\n    df_X = df[colunas]\n    df_Y = df[previsao]\n    \n    #Realizando o split sobre os dados\n    variaveis = variavel\n    seed = 7   \n    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.25,random_state=seed)  \n    \n   \n    #Separando o array em componentes X e Y\n    X = X_train\n    Y = y_train\n    \n    #Normaliza\u00e7\u00e3o\n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx = scaler.fit_transform(X)\n    \n    #Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    num_instances = len(rescaledx)\n    \n    nome = \"AdaBoost\"\n\n    kfold = StratifiedKFold(n_splits = num_folds,random_state=seed,shuffle = True)\n    inicio =time.time()\n        \n    modelo= AdaBoostClassifier(n_estimators = 30,random_state=seed)\n    \n    cv_roc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"roc_auc\")\n    cv_acc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"accuracy\")\n    cv_prec = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"precision\")\n    cv_recall = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"recall\")\n\n        \n    \n    modelo_test = modelo.fit(rescaledx,Y)\n    #Normaliza\u00e7\u00e3o\n        \n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx_test = scaler.fit_transform(X_test)\n        \n    y_pred = modelo_test.predict(rescaledx_test)\n\n    cv_acc_test = accuracy_score(y_test,y_pred)\n    cv_roc_test = roc_auc_score(y_test,y_pred)\n    cv_prec_test = precision_score(y_test,y_pred)\n    cv_recall_test = recall_score(y_test,y_pred)\n\n        \n    fim = time.time()\n    \n    msg = \"Modelo: %s, AUC ROC: %.2f%% (%.2f%%) Acur\u00e1cia: %.2f%% (%.2f%%) Precis\u00e3o: %.2f%% (%.2f%%) Recall: %.2f%% (%.2f%%) \" %(nome,cv_roc.mean()*100,cv_roc.std()*100,\n                                                                          cv_acc.mean()*100,cv_acc.std()*100,\n                                                                          cv_prec.mean()*100,cv_prec.std()*100,\n                                                                          cv_recall.mean()*100,cv_recall.std()*100)\n    tempo = fim - inicio\n    print(msg)\n    df_modelo.loc[len(df_modelo)+1] = [nome,variaveis,tempo,cv_roc.mean()*100,cv_acc.mean()*100,cv_prec.mean()*100,cv_recall.mean()*100,\n                                                cv_acc_test.mean()*100,cv_roc_test.mean()*100,cv_prec_test.mean()*100,cv_recall_test.mean()*100]\n\nprint(\"Modelo CHI2\")\nselecao_modelos(df,colunas_chi2,\"target\",df_modelo = df_modelos,variavel=\"CHI2\")\nprint(\"\\n\")\nprint(\"Modelo RFE\")\nselecao_modelos(df,colunas_rfe,\"target\",df_modelo = df_modelos,variavel=\"RFE\")\nprint(\"\\n\")\nprint(\"Modelo ENSEABLE\")\nselecao_modelos(df,colunas_enseable,\"target\",df_modelo = df_modelos,variavel=\"ENSEABLE\")","6ce3c15f":"# Gradiente Boosting\n\n\ndef selecao_modelos(df,colunas,previsao,df_modelo,variavel):\n    df_X = df[colunas]\n    df_Y = df[previsao]\n    \n    #Realizando o split sobre os dados\n\n    variaveis = variavel\n    seed = 7   \n    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.25,random_state=seed)  \n    \n   \n    #Separando o array em componentes X e Y\n    X = X_train\n    Y = y_train\n    \n    #Normaliza\u00e7\u00e3o\n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx = scaler.fit_transform(X)\n    \n    #Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    num_instances = len(rescaledx)\n    \n    nome = \"Gradiente Boosting\"\n\n    kfold = StratifiedKFold(n_splits = num_folds,random_state=seed,shuffle = True)\n    inicio =time.time()\n        \n    modelo= GradientBoostingClassifier(n_estimators = 30,random_state=seed)\n\n    cv_roc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"roc_auc\")\n    cv_acc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"accuracy\")\n    cv_prec = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"precision\")\n    cv_recall = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"recall\")\n\n        \n    \n    modelo_test = modelo.fit(rescaledx,Y)\n    \n    #Normaliza\u00e7\u00e3o\n        \n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx_test = scaler.fit_transform(X_test)\n        \n    y_pred = modelo_test.predict(rescaledx_test)\n\n    cv_acc_test = accuracy_score(y_test,y_pred)\n    cv_roc_test = roc_auc_score(y_test,y_pred)\n    cv_prec_test = precision_score(y_test,y_pred)\n    cv_recall_test = recall_score(y_test,y_pred)\n\n    fim = time.time()\n                \n    msg = \"Modelo: %s, AUC ROC: %.2f%% (%.2f%%) Acur\u00e1cia: %.2f%% (%.2f%%) Precis\u00e3o: %.2f%% (%.2f%%) Recall: %.2f%% (%.2f%%) \" %(nome,cv_roc.mean()*100,cv_roc.std()*100,\n                                                                          cv_acc.mean()*100,cv_acc.std()*100,\n                                                                          cv_prec.mean()*100,cv_prec.std()*100,\n                                                                          cv_recall.mean()*100,cv_recall.std()*100)\n    tempo = fim - inicio\n    print(msg)\n    df_modelo.loc[len(df_modelo)+1] = [nome,variaveis,tempo,cv_roc.mean()*100,cv_acc.mean()*100,cv_prec.mean()*100,cv_recall.mean()*100,\n                                                cv_acc_test.mean()*100,cv_roc_test.mean()*100,cv_prec_test.mean()*100,cv_recall_test.mean()*100]\n\nprint(\"Modelo CHI2\")\nselecao_modelos(df,colunas_chi2,\"target\",df_modelo = df_modelos,variavel=\"CHI2\")\nprint(\"\\n\")\nprint(\"Modelo RFE\")\nselecao_modelos(df,colunas_rfe,\"target\",df_modelo = df_modelos,variavel=\"RFE\")\nprint(\"\\n\")\nprint(\"Modelo ENSEABLE\")\nselecao_modelos(df,colunas_enseable,\"target\",df_modelo = df_modelos,variavel=\"ENSEABLE\")","66ccf42a":"#Voting Ensenble\n\n\ndef selecao_modelos(df,colunas,previsao,df_modelo,variavel):\n    df_X = df[colunas]\n    df_Y = df[previsao]\n    \n    #Realizando o split sobre os dados\n    variaveis = variavel\n    seed = 7   \n    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.25,random_state=seed)  \n    \n   \n    #Separando o array em componentes X e Y\n    X = X_train\n    Y = y_train\n    \n    #Normaliza\u00e7\u00e3o\n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx = scaler.fit_transform(X)\n    \n    #Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    num_instances = len(rescaledx)\n    \n    #Preparando o modelo\n    modelos = []\n    modelos.append((\"LR\",LogisticRegression(max_iter=400)))\n    modelos.append((\"LDA\",LinearDiscriminantAnalysis()))\n    modelos.append((\"NB\",GaussianNB()))\n    modelos.append((\"KNN\",KNeighborsClassifier()))\n    modelos.append((\"CART\",DecisionTreeClassifier()))\n    modelos.append((\"SVN\",SVC()))\n    \n    #Criando o modelo ensemble\n    inicio =time.time()\n    ensemble = VotingClassifier(modelos)\n    kfold = StratifiedKFold(n_splits = num_folds,random_state=seed,shuffle = True)\n    \n    cv_acc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"accuracy\")\n    cv_prec = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"precision\")\n    cv_recall = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"recall\")\n\n        \n    \n    modelo_test = modelo.fit(rescaledx,Y)\n    #Normaliza\u00e7\u00e3o\n        \n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx_test = scaler.fit_transform(X_test)\n        \n    y_pred = modelo_test.predict(rescaledx_test)\n\n    cv_acc_test = accuracy_score(y_test,y_pred)\n    cv_prec_test = precision_score(y_test,y_pred)\n    cv_recall_test = recall_score(y_test,y_pred)\n\n    \n    nome = \"Voting Ensenble\"\n    fim = time.time()\n                \n    msg = \"Modelo: %s, Acur\u00e1cia: %.2f%% (%.2f%%) Precis\u00e3o: %.2f%% (%.2f%%) Recall: %.2f%% (%.2f%%) \" %(nome,\n                                                                          cv_acc.mean()*100,cv_acc.std()*100,\n                                                                          cv_prec.mean()*100,cv_prec.std()*100,\n                                                                          cv_recall.mean()*100,cv_recall.std()*100)\n    tempo = fim - inicio\n    print(msg)\n    df_modelo.loc[len(df_modelo)+1] = [nome,variaveis,tempo,None,cv_acc.mean()*100,cv_prec.mean()*100,cv_recall.mean()*100,\n                                                None,cv_acc_test.mean()*100,cv_prec_test.mean()*100,cv_recall_test.mean()*100]\n\nprint(\"Modelo CHI2\")\nselecao_modelos(df,colunas_chi2,\"target\",df_modelo = df_modelos,variavel=\"CHI2\")\nprint(\"\\n\")\nprint(\"Modelo RFE\")\nselecao_modelos(df,colunas_rfe,\"target\",df_modelo = df_modelos,variavel=\"RFE\")\nprint(\"\\n\")\nprint(\"Modelo ENSEABLE\")\nselecao_modelos(df,colunas_enseable,\"target\",df_modelo = df_modelos,variavel=\"ENSEABLE\")","0295938a":"#XGBoost - Extreme Gradient Boosting\n\ndef selecao_modelos(df,colunas,previsao,df_modelo,variavel):\n    df_X = df[colunas]\n    df_Y = df[previsao]\n    \n    #Realizando o split sobre os dados\n    variaveis = variavel\n    seed = 7   \n    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.25,random_state=seed)  \n    \n   \n    #Separando o array em componentes X e Y\n    X = X_train\n    Y = y_train\n    \n    #Normaliza\u00e7\u00e3o\n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx = scaler.fit_transform(X)\n    \n    #Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    num_instances = len(rescaledx)\n    \n    \n    #Criando o modelo\n    nome = \"XGBoost\"\n    inicio =time.time()\n    modelo = XGBClassifier()\n    kfold = StratifiedKFold(n_splits = num_folds,random_state=seed,shuffle = True)\n\n    cv_roc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"roc_auc\")\n    cv_acc = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"accuracy\")\n    cv_prec = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"precision\")\n    cv_recall = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"recall\")\n\n        \n    \n    modelo_test = modelo.fit(rescaledx,Y)\n    #Normaliza\u00e7\u00e3o\n        \n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx_test = scaler.fit_transform(X_test)\n        \n    y_pred = modelo_test.predict(rescaledx_test)\n\n    cv_acc_test = accuracy_score(y_test,y_pred)\n    cv_roc_test = roc_auc_score(y_test,y_pred)\n    cv_prec_test = precision_score(y_test,y_pred)\n    cv_recall_test = recall_score(y_test,y_pred)\n\n    fim = time.time()\n                \n    msg = \"Modelo: %s, AUC ROC: %.2f%% (%.2f%%) Acur\u00e1cia: %.2f%% (%.2f%%) Precis\u00e3o: %.2f%% (%.2f%%) Recall: %.2f%% (%.2f%%) \" %(nome,cv_roc.mean()*100,cv_roc.std()*100,\n                                                                          cv_acc.mean()*100,cv_acc.std()*100,\n                                                                          cv_prec.mean()*100,cv_prec.std()*100,\n                                                                          cv_recall.mean()*100,cv_recall.std()*100)\n    tempo = fim - inicio\n    print(msg)\n    df_modelo.loc[len(df_modelo)+1] = [nome,variaveis,tempo,cv_roc.mean()*100,cv_acc.mean()*100,cv_prec.mean()*100,cv_recall.mean()*100,\n                                                cv_acc_test.mean()*100,cv_roc_test.mean()*100,cv_prec_test.mean()*100,cv_recall_test.mean()*100]\n\nprint(\"Modelo CHI2\")\nselecao_modelos(df,colunas_chi2,\"target\",df_modelo = df_modelos,variavel=\"CHI2\")\nprint(\"\\n\")\nprint(\"Modelo RFE\")\nselecao_modelos(df,colunas_rfe,\"target\",df_modelo = df_modelos,variavel=\"RFE\")\nprint(\"\\n\")\nprint(\"Modelo ENSEABLE\")\nselecao_modelos(df,colunas_enseable,\"target\",df_modelo = df_modelos,variavel=\"ENSEABLE\")","b49b7932":"df_modelos.sort_values([\"Test RECALL\"],ascending=False,ignore_index=True)","47423d06":"#KNN (CHI2)\n\ndef selecao_modelos(df,colunas,previsao,variavel):\n    df_X = df[colunas]\n    df_Y = df[previsao]\n    \n    variaveis = variavel\n    seed = 7   \n    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.25,random_state=seed) \n    \n    \n    #Separando o array em componentes X e Y\n    X = df_X.values\n    Y = df_Y.values\n\n    \n    #Normaliza\u00e7\u00e3o\n    scaler = MinMaxScaler(feature_range=(0,1))\n    rescaledx = scaler.fit_transform(X)\n    \n    #Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    num_instances = len(rescaledx)   \n    \n    \n    kfold = StratifiedKFold(n_splits = num_folds,random_state=seed,shuffle = True)\n    inicio =time.time()\n    modelo = KNeighborsClassifier()\n    cv_results = cross_val_score(modelo,rescaledx,Y,cv = kfold,scoring=\"accuracy\")\n    fim = time.time()\n    tempo = fim - inicio\n        \n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n\n    fig, ax = plt.subplots(figsize=(8,6))\n    for i, (train, test) in enumerate(kfold.split(rescaledx, Y)):\n        modelo.fit(rescaledx[train], Y[train])\n        viz = plot_roc_curve(modelo, rescaledx[test], Y[test],\n                             name='ROC fold {}'.format(i),\n                             alpha=0.3, lw=1, ax=ax)\n        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(viz.roc_auc)\n\n    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n                    label='Chance', alpha=.8)\n\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    ax.plot(mean_fpr, mean_tpr, color='b',\n            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n            lw=2, alpha=.8)\n\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                    label=r'$\\pm$ 1 std. dev.')\n\n    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n                 title=\"k-nearest neighbors (CHI2)\")\n    ax.legend(loc=\"lower right\")\n    plt.show()\n       \nprint(\"Caracter\u00edstica de Opera\u00e7\u00e3o do Receptor (ROC) com cross validation para o melhor modelo.\")\nselecao_modelos(df,colunas_chi2,\"target\",variavel=\"RFE\")","be66ae29":"<a id=\"8\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">8 - Conclus\u00e3o<\/div>\n\n**\u00c9 poss\u00edvel observar o dataframe sobre todos os modelos e suas m\u00e9tricas avaliadas com os dados de treino e com os dados de teste. Onde, diversos modelos distintos apresentaram \u00f3timos valores em m\u00e9tricas distintas.**\n\n**Por\u00e9m, neste trabalho o principal fator para prever doen\u00e7as card\u00edacas sera aquele que obteve a melhor avalia\u00e7\u00e3o sobre a m\u00e9trica de Recall nos dados de teste.\nEssa m\u00e9trica se mostrou com maior signific\u00e2ncia pois um paciente ser di\u00e1gn\u00f3sticado como falso negativo \u00e9 extremamente grave. E essa m\u00e9trica \u00e9 utilizada em uma situa\u00e7\u00e3o em que os Falsos Negativos s\u00e3o considerados mais prejudiciais que os Falsos Positivos.**\n\n**Os modelos com os maiores valores para o Recall, foram:**\n- KNN (CHI2)\n- BDT SVM (CHI2)\n- SVM (CHI2)\n- BDT KNN (CHI2)\n- SVM (RFE)\n\n**Por\u00e9m, entre esses modelos, o modelo KNN (CHI2) apresentou a maior curva AUC ROC, acur\u00e1cia, precis\u00e3o para os dados de teste.**\n\n**Portanto o modelo KNN utilizando a sele\u00e7\u00e3o de vari\u00e1veis qui-quadrado (CHI2), obteve:**\n\n**Dados de TESTE**\n\n**AUC ROC: 81.57%**\n\n**\u00c1cur\u00e1cia: 81.84%**\n\n**Precis\u00e3o: 75.55%**\n\n**Recall: 91.89%**\n\n\n**Tempo de treinamento do modelo: 0.099 segundos**\n\n\n\n\n","d6ec9113":"<h3 style=\"text-align:center;font-size:200%;;\">Conclus\u00e3o do projeto<\/h3>\n<div class=\"progress\">\n  <div class=\"progress-bar\" role=\"progressbar\" style=\"width: 75%;\" aria-valuenow=\"25\" aria-valuemin=\"0\" aria-valuemax=\"100\">75% Conclu\u00eddo<\/div>\n<\/div>","cbb4172e":"<a id=\"2\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">2 - Ajustando dados para os modelos de Machine Learning<\/div>\n\n**Muitos algoritmos esperam receber os dados em um formato espec\u00edfico. Portanto, \u00e9 necess\u00e1rio entregar os dados\ncom uma estrutura que seja adequada ao algoritmo que ser\u00e1 utilizado**","47c179ae":"<a id=\"6.5\"><\/a> <br>\n# <div class=\"alert alert-warning\">6.5 - Decision Tree Classifier<\/div>\n\n\n**Uma \u00e1rvore de decis\u00e3o\ntoma como entrada um objeto ou situa\u00e7\u00e3o descrito por um\nconjunto de atributos e retorna uma decis\u00e3o - o valor de\nsa\u00edda previsto, de acordo com a entrada**\n\n**A \u00e1rvore de classifica\u00e7\u00e3o \u00e9 o resultado de se fazer uma\nsequ\u00eancia ordenada de perguntas, e as perguntas feitas a cada\npasso na sequ\u00eancia dependem das respostas \u00e0s perguntas\nanteriores. A sequ\u00eancia termina em uma previs\u00e3o da classe.**\n\n<h1 style=\"text-align:center\">\n<img src=\"https:\/\/i.ytimg.com\/vi\/CcQMS_eaBWE\/maxresdefault.jpg\" style=\"width:600px;height:250px;\">","920ea827":"<a id=\"4\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">4 - Divis\u00e3o dos dados<\/div>\n\n**Foi utilizado a fun\u00e7\u00e3o train_test_split() para dividir os dados de treino e de teste.\nFoi realizado um split de 75% para dados de treino e 25% para dados de teste.**\n\n**Por fim utilizamos o Cross Validation sobre os dados de treino em cada modelo.**\n\n**Cross validation \u00e9 uma t\u00e9cnica que pode ser utilizada para avaliar a performance de um modelo\ncom menos vaari\u00e2ncia que a t\u00e9cnica de dividir os dados de treino\/teste.**\n\n\n**Com est\u00e1 t\u00e9cnica dividmos os dados em partes normalmente chamadas em k-folds.\nEle executa cada fold por vez at\u00e9 k-1 fold, por fim, podemos sumarizar a performance\nde cada fold usando a m\u00e9dia e o desvio padr\u00e3o**\n\n<h1 style=\"text-align:center\">\n<img src=\"https:\/\/miro.medium.com\/max\/500\/1*0_jdEVi6l1Nj-DKK8wJTTQ.png\">","508577c7":"<a id=\"7.5\"><\/a> <br>\n# <div class=\"alert alert-warning\">7.5 - Voting Ensenble<\/div>\n\n**Este \u00e9 um dos m\u00e9todos Ensemble mais simples. Este m\u00e9todo cria dois ou mais modelos separados a partir do dataset de treino. O classificados Voting ent\u00e3o utiliza a m\u00e9dia das previs\u00f5es em novos conjuntos de dados.**\n\n**As previs\u00f5es de cada sub-modelo podem receber pesos, atr\u00e1ves de heur\u00edstica.**","aefa8ae2":"<a id=\"5.1\"><\/a> <br>\n# <div class=\"alert alert-warning\">5.1 - Curva ROC<\/div>\n\n**A curva ROC permite analisar a m\u00e9trica AUC (Area Unde the Curve).**\n\n**A curva ROC mostra o qu\u00e3o bom o modelo criado pode distinguir entre duas coisas (j\u00e1 que \u00e9 utilizado para classifica\u00e7\u00e3o). \nEssas duas coisas podem ser 0 ou 1, ou positivo e negativo. Os melhores modelos conseguem distinguir com precis\u00e3o o bin\u00f4mio.**\n\n**O valor do AUC varia de 0,0 at\u00e9 1,0 e o limiar entre a classe \u00e9 0,5. Ou seja, acima desse limite, o algoritmo classifica em uma classe e abaixo na outra classe.\nQuanto maior o AUC, melhor.**\n\n<h1 style=\"text-align:center\">\n<img src=\"https:\/\/miro.medium.com\/max\/2400\/1*RqK5DjVxcj4qZsCdN4FOSQ.png\">","e318dda5":"<a id=\"7.1\"><\/a> <br>\n# <div class=\"alert alert-warning\">7.1 - Bagged Decision Trees<\/div>\n\n**Este m\u00e9todo funciona bem quando existe alta vari\u00e2ncia nos dados**","8082197f":"<a id=\"7.6\"><\/a> <br>\n# <div class=\"alert alert-warning\">7.6 - XGBoost <\/div>\n\n**O algoritmo XGBoost \u00e9 uma extens\u00e3o do GBM (Gradient Boosting Method) que permite trabalhar com multi threading em uma \u00fanica m\u00e1quina e precessamento paralelo em um cluster de v\u00e1rios servidores.**\n\n**A principal vantagem do XGBoost sobre GBM \u00e9 sua capacidade de gerenciar dados esparsos**","32b36ba1":"<a id=\"7\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">7 - Otimiza\u00e7\u00e3o de Performance com M\u00e9todos Ensemble<\/div>\n\n**M\u00e9todos Ensemble permitem aumentar consideralvemente o n\u00edvel de precis\u00e3o nas suas previs\u00f5es. Foi criado m\u00e9todos Ensemble mais poderosos:**\n\n**Bagging: Para consutru\u00e7\u00e3o de m\u00faltiplos modelos a partir de diferentes subset no dataset de treino**\n\n**Boosting: Para constru\u00e7\u00e3o de m\u00faltiplos modelos, onde cada modelo aprende a corrigir os erros gerados pelo modelo anterior.**\n\n**Voting: Para constru\u00e7\u00e3o de m\u00faltiplos modelos (normalmente de tipos diferentes) e estat\u00edstica simples s\u00e3o usadas para combinar as previs\u00f5es.**","2d213764":"<a id='top'>Top reference<\/a>\n<h1 style=\"text-align:center\">\n<img src=\"https:\/\/files.aredacao.com.br\/upload\/content\/doencas-cardiacas-merecem-atencao-em-meio-a-pandemia.jpg\" style=\"width:900px;height:400px;\">\n\n### Notas de lan\u00e7amento\n\n- Vers\u00e3o 1: Verifica\u00e7\u00e3o dos dados e distribui\u00e7\u00e3o dos dados.\n- Vers\u00e3o 2: Sele\u00e7\u00e3o sobre as melhores vari\u00e1veis.\n- Vers\u00e3o 3: Ajuste sobre as escalas.\n- Vers\u00e3o 4: Divis\u00e3o dados de treino\/teste e cross validation.\n- Vers\u00e3o 5: Treinamento de diversos modelos de machine learning.\n- Vers\u00e3o 6: Calculo das m\u00e9tricas de treino e de teste.\n\n\n## Conte\u00fado\n\n1. [Introdu\u00e7\u00e3o](#1)\n 1. [M\u00f3dulos](#1.1)\n2. [Ajustando dados para os modelos de Machine Learning](#2)\n 1. [Escala](#2.1)\n3. [Featute Selection](#3)\n 1. [Sele\u00e7\u00e3o Univariada](#3.1)\n 2. [Elimina\u00e7\u00e3o Recursiva de Atributos](#3.2)\n 3. [M\u00e9todo Ensemble](#3.3)\n4. [Divis\u00e3o dos dados](#4)\n5. [Avaliando a Performance](#5)\n 1. [Curva ROC](#5.1)\n 2. [Acur\u00e1cia](#5.2)\n 3. [Precis\u00e3o](#5.3)\n 4. [Recall](#5.3)\n6. [Modelos de Machine Learning](#6)\n 1. [Logistic Regression](#6.1)\n 2. [Decision Tree Classifier](#6.2)\n 3. [Kneighbors Classifier](#6.3)\n 4. [Linear Discriminant Analysis](#6.4)\n 5. [Gaussian NB](#6.5)\n 6. [Suport Vector Machine](#6.6)\n7. [Otimiza\u00e7\u00e3o de Performance com M\u00e9todos Ensemble](#7)\n 1. [Bagged Decision Trees](#7.1)\n 2. [Random Forest](#7.2)\n 3. [AdaBoost](#7.3)\n 4. [Gradient Boosting](#7.4) \n 4. [Voting Ensemble](#7.5)\n 5. [XGboost](#7.6)\n8. [Conclus\u00e3o](#8)","5ad401db":"<a id=\"1.1\"><\/a> <br>\n# <div class=\"alert alert-warning\">1.1 - M\u00f3dulos<\/div>","86ef419b":"<a id=\"6\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">6 - Modelos de Machine Learning<\/div>","4bbc0c1f":"<a id=\"5.4\"><\/a> <br>\n# <div class=\"alert alert-warning\">5.4 - Recall<\/div>\n\n**O recall pode ser usada em uma situa\u00e7\u00e3o em que os Falsos Negativos s\u00e3o considerados mais prejudiciais que os Falsos Positivos.**\n\n**Por exemplo, o modelo deve de qualquer maneira encontrar todos os pacientes doentes, mesmo que classifique alguns saud\u00e1veis como doentes (situa\u00e7\u00e3o de Falso Positivo) no processo. Ou seja, o modelo deve ter alto recall, pois classificar pacientes doentes como saud\u00e1veis pode ser uma trag\u00e9dia.**","185e241d":"<a id=\"5\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">5 - Avaliando a Performance <\/div>\n\n\n**As m\u00e9tricas que s\u00e3o escolhidas para avaliar a performance do modelo v\u00e3o influenciar\na forma como a performance \u00e9 medida e comparada com modelos criados com outros algoritmos.**","5c9e5a14":"<a href=\"#top\" class=\"btn btn-success btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Voltar para o topo<\/a>","647223f7":"<a id=\"3.3\"><\/a> <br>\n# <div class=\"alert alert-warning\">3.3 - M\u00e9todo Enseable para sele\u00e7\u00e3o de vari\u00e1veis<\/div>\n\n**Bagged Decision Trees, como o algoritmo Random Forest, podem ser usados para estimar\na import\u00e2ncia de cada atributo.\nEsse m\u00e9todo retorna um score para cada atributo, quanto maior, maior a import\u00e2ncia de\ncada atributo**","eedf7053":"<a id=\"3.2\"><\/a> <br>\n# <div class=\"alert alert-warning\">3.2 - Elimina\u00e7\u00e3o Recursiva de Atributos<\/div>\n\n**Esta \u00e9 outra t\u00e9cnica para sele\u00e7\u00e3o de atributos, que recursivamente remove os atributos\ne constro\u00ed o modelo com os atributos remanescentes.\nEsta t\u00e9cnica utiliza a acur\u00e1cia do modelo para identificar os atributos que mais\ncontribuem para prever a vari\u00e1vel alvo.**","12889652":"1. **age:** Idade\n1. **sex:** Sexo\n1. **cp:** Tipo de dor no peito (4 valores)\n1. **trestbps:** Press\u00e3o sangu\u00ednea em repouso\n1. **chol:** Colesterol s\u00e9rico em mg \/ dl\n1. **fbs:** A\u00e7\u00facar no sangue em jejum> 120 mg \/ dl\n1. **restecg:** Resultados eletrocardiogr\u00e1ficos de repouso (valores 0,1,2)\n1. **thalach:** Frequ\u00eancia card\u00edaca m\u00e1xima alcan\u00e7ada\n1. **exang:** Angina induzida por exerc\u00edcio\n1. **oldpeak:** Depress\u00e3o de ST induzida por exerc\u00edcio em rela\u00e7\u00e3o ao repouso\n1. **slope:** A inclina\u00e7\u00e3o do segmento ST de pico do exerc\u00edcio\n1. **ca:** N\u00famero de vasos principais (0-3) coloridos por fluorosopia\n1. **thal:** 3 = normal; 6 = defeito corrigido; 7 = defeito revers\u00edvel\n1. **target:** Indica a presen\u00e7a ou aus\u00eancia de doen\u00e7a card\u00edaca. (= o atributo previsto)","72b3e7a1":"<a id=\"5.3\"><\/a> <br>\n# <div class=\"alert alert-warning\">5.3 - Precis\u00e3o<\/div>\n\n**A precis\u00e3o pode ser usada em uma situa\u00e7\u00e3o em que os Falsos Positivos s\u00e3o considerados mais prejudiciais que os Falsos Negativos. Por exemplo, ao classificar uma a\u00e7\u00e3o como um bom investimento, \u00e9 necess\u00e1rio que o modelo esteja correto, mesmo que acabe classificando bons investimentos como maus investimentos (situa\u00e7\u00e3o de Falso Negativo) no processo.**\n\n**Ou seja, o modelo deve ser preciso em suas classifica\u00e7\u00f5es, pois a partir do momento que consideramos um investimento bom quando na verdade ele n\u00e3o \u00e9, uma grande perda de dinheiro pode acontecer.**","4da89a1b":"<a id=\"1\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">1 - Introduc\u00e3o<\/div>\n\n**As Doen\u00e7as Cardiovasculares (DCV) s\u00e3o, atualmente, a maior causa de mortes no mundo. Elas foram respons\u00e1veis\npor mais de 17 milh\u00f5es de \u00f3bitos em 2008, dos quais tr\u00eas milh\u00f5es ocorreram antes dos 60 anos de idade, e grande\nparte poderia ter sido evitada. A Organiza\u00e7\u00e3o Mundial de Sa\u00fade estima que em 2030 quase 23,6 milh\u00f5es de\npessoas morrer\u00e3o de doen\u00e7as cardiovasculares.**\n\n**O \u00f4nus econ\u00f4mico das doen\u00e7as cardiovasculares cresceu exponencialmente nas \u00faltimas d\u00e9cadas. Em 2000, as doen\u00e7as\ncardiovasculares foram respons\u00e1veis pela principal aloca\u00e7\u00e3o de recursos p\u00fablicos em hospitaliza\u00e7\u00f5es no Brasil e\nforam a terceira causa de perman\u00eancia hospitalar prolongada. Entre 1991 e 2000, os custos hospitalares atribu\u00eddos \n\u00e0s doen\u00e7as cardiovasculares aumentaram cerca de 176%**","deb2ee4f":"<a id=\"7.3\"><\/a> <br>\n# <div class=\"alert alert-warning\">7.3 - AdaBoost<\/div>\n\n**Algoritmo baseado em Boosting Ensemble criam uma sequ\u00eancia de modelos que tentam corrigir os erros dos modelos anteriores dentro da sequ\u00eancia. Uma vez criados, os modelos fazem previs\u00f5es que podem receber um peso de acordo com sua acur\u00e1cia e os resultados s\u00e3o combinados para criar uma previs\u00e3o \u00fanica final.**\n\n**O AdaBoost atribui pesos \u00e0s inst\u00e2ncias no dataset, definindo qu\u00e3o f\u00e1cil ou dif\u00edcil elas s\u00e3o para o precesso de classifica\u00e7\u00e3o, permitindo que o algoritmo tenham mais ou menos aten\u00e7\u00e3o \u00e0s inst\u00e2ncias durante o processo de constru\u00e7\u00e3o dos modelos.**","cfa00022":"<a id=\"2.1\"><\/a> <br>\n# <div class=\"alert alert-warning\">2.1 - Escala<\/div>\n\n**A escala \u00e9 uma das primeiras tarefas dentro do pr\u00e9-processamento, que consiste em deixar os dados na mesma escala.\nMuitos algoritmos de machine learning v\u00e3o se beneficiar disso e produzir resultados melhores.**\n\n**Na etapa abaixo \u00e9 chamado de normaliza\u00e7\u00e3o e significa colocar os dados em uma escala com range entre 0 e 1.\nIsso \u00e9 \u00fatil para otimiza\u00e7\u00e3o, sendo usado nos algoritmos de Machine Learning, como gradient descent.**","a47e7fe2":"<a id=\"7.4\"><\/a> <br>\n# <div class=\"alert alert-warning\">7.4 - Gradiente Boosting<\/div>\n\n**Tamb\u00e9m chamado de Stochastic Gradient Boosting, \u00e9 um dos m\u00e9todos Ensemble mais sofisticados**","677249da":"<a id=\"6.1\"><\/a> <br>\n# <div class=\"alert alert-warning\">6.1 - Logistic Regression<\/div>\n\n**A Regress\u00e3o Log\u00edstica \u00e9 uma an\u00e1lise que nos permite estimar a probabilidade associada \u00e0 ocorr\u00eancia de determinado evento em face de um conjunto de vari\u00e1veis explanat\u00f3rias.**\n\n**As vantagens desse tipo de regress\u00e3o incluem:**\n- Facilidade para lidar com vari\u00e1veis independentes categ\u00f3ricas;\n- Fornece resultados em termos de probabilidade;\n- Facilidade de classifica\u00e7\u00e3o de indiv\u00edduos em categorias;\n- Requer pequeno n\u00famero de suposi\u00e7\u00f5es;\n- Possui alto grau de confiabilidade.\n\n<h1 style=\"text-align:center\">\n<img src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/logistic-regression-in-machine-learning.png\" style=\"width:900px;height:400px;\">","8bc2ccb1":"<a id=\"6.6\"><\/a> <br>\n# <div class=\"alert alert-warning\">6.6 - Suport Vector Machine<\/div>\n\n\n**As M\u00e1quinas de Vetor Suporte destacam-se pela forte fundamenta\u00e7\u00e3o te\u00f3rica\nexistente, possuindo como base a teoria da aprendizagem estat\u00edstica, sendo esta caracter\u00edstica\num diferencial sobre outras t\u00e9cnicas como redes neurais, que n\u00e3o possui um modelo te\u00f3rico.**\n\n**A capacidade em trabalhar com padr\u00f5es de alta dimensionalidade \u00e9 outra\ncaracter\u00e9stica interessante desta t\u00e9cnica, sendo ideal para aplica\u00e7\u00e3o em problemas de vis\u00e3o\ncomputacional, como reconhecimento de padr\u00f5es e filtragem**\n","0f19345d":"<a id=\"3.1\"><\/a> <br>\n# <div class=\"alert alert-warning\">3.1 - Sele\u00e7\u00e3o Univariada<\/div>\n\n**Testes estat\u00edsticos podem ser usados para selecionar os atributos que possuem forte\nrelacionamento com a vari\u00e1vel que queremos prever.\nFoi utilizado a fun\u00e7\u00e3o SelectKBest() que pode ser utilizada em diversos testes estat\u00edsticos,\npara selecionar os atributos.**\n\n**Foi utilizado o tese do Qui-Quadrado**\n\n**O teste de independ\u00eancia Qui-Quadrado \u00e9 usado para descobrir se existe uma associa\u00e7\u00e3o entre \na vari\u00e1vel de linha e coluna vari\u00e1vel em uma tabela de conting\u00eancia constru\u00eddo \u00e0 partir dos dados.**","9ef59c72":"<h3 style=\"text-align:center;font-size:200%;;\">Conclus\u00e3o do projeto<\/h3>\n<div class=\"progress\">\n  <div class=\"progress-bar\" role=\"progressbar\" style=\"width: 100%;\" aria-valuenow=\"25\" aria-valuemin=\"0\" aria-valuemax=\"100\">100% Conclu\u00eddo<\/div>\n<\/div>","5a52c524":"<a id=\"6.3\"><\/a> <br>\n# <div class=\"alert alert-warning\">6.3 - Gaussian NB<\/div>\n\n**O algoritmo Na\u00efve Bayes \u00e9 um algoritmo de aprendizagem supervisionada, baseado no teorema de Bayes e utilizado para resolver problemas de classifica\u00e7\u00e3o.**\n\n**O Na\u00efve Bayes Classifier \u00e9 um dos algoritmos de classifica\u00e7\u00e3o mais simples e eficazes que ajuda na constru\u00e7\u00e3o de modelos de aprendizado de m\u00e1quina r\u00e1pidos que podem fazer previs\u00f5es r\u00e1pidas.**\n\n**\u00c9 um classificador probabil\u00edstico, o que significa que prev\u00ea com base na probabilidade de um objeto .**","5c9254b7":"<a id=\"3\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">3 - Featute Selection<\/div>\n\n**Os atributos presentes no dataset, ter\u00e3o grande influ\u00eancia na precis\u00e3o e no modelo predito.**\n- Atributos irrelevantes ter\u00e3o impacto negativo na performance.\n- Atributos colineares podem afetar na acur\u00e1cia do modelo.\n\n**Nesta etapa \u00e9 onde selecionamos os atributos (vari\u00e1veis) que ser\u00e3o melhores candidatas a vari\u00e1veis preditoras.\nAjudando a reduzir o overfitting, aumentando a acur\u00e1cia do modelo e reduzindo o tempo de treinamento.**","01efd51c":"<a id=\"7.2\"><\/a> <br>\n# <div class=\"alert alert-warning\">7.2 - Random Forest<\/div>\n\n**Random Forest \u00e9 uma extens\u00e3o do Bagging Decision Tree. Amostras do dataset de treino s\u00e3o usadas com reposi\u00e7\u00e3o, mas as \u00e1rvoes s\u00e3o criadas de uma forma que reduz a correla\u00e7\u00e3o entre classificadores individuais.**","3499ce24":"<a id=\"6.2\"><\/a> <br>\n# <div class=\"alert alert-warning\">6.2 - Linear Discriminant Analysis<\/div>\n\n**O objetivo da an\u00e1lise discriminante \u00e9 desenvolver fun\u00e7\u00f5es discriminantes que nada mais s\u00e3o do que a combina\u00e7\u00e3o linear de vari\u00e1veis \u200b\u200bindependentes que discriminar\u00e3o entre as categorias da vari\u00e1vel dependente de maneira perfeita.**\n\n**Deseja-se que as amostras tenham a maior dist\u00e2ncia entre classes e a menor dist\u00e2ncia dentro da classe, ou seja, maximiza\u00e7\u00e3o da separa\u00e7\u00e3o entre duas ou mais classes**","196e915f":"<a id=\"6.4\"><\/a> <br>\n# <div class=\"alert alert-warning\">6.4 - KNeighbors Classifier<\/div>\n\n**Em estat\u00edstica , o algoritmo k-nearest neighbors ( k-NN ) \u00e9 um m\u00e9todo n\u00e3o param\u00e9trico proposto por Thomas Cover usado para classifica\u00e7\u00e3o e regress\u00e3o .**\n\n**k-NN \u00e9 um tipo de aprendizagem baseada em inst\u00e2ncia , ou aprendizagem pregui\u00e7osa , onde a fun\u00e7\u00e3o \u00e9 aproximada apenas localmente e todos os c\u00e1lculos s\u00e3o adiados at\u00e9 a avalia\u00e7\u00e3o da fun\u00e7\u00e3o. Como esse algoritmo depende da dist\u00e2ncia para classifica\u00e7\u00e3o, normalizar os dados de treinamento pode melhorar drasticamente sua precis\u00e3o.**\n\n**Tanto para classifica\u00e7\u00e3o quanto para regress\u00e3o, uma t\u00e9cnica \u00fatil pode ser atribuir pesos \u00e0s contribui\u00e7\u00f5es dos vizinhos, de modo que os vizinhos mais pr\u00f3ximos contribuam mais para a m\u00e9dia do que os mais distantes.**\n\n<h1 style=\"text-align:center\">\n<img src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/k-nearest-neighbor-algorithm-for-machine-learning2.png\" style=\"width:900px;height:400px;\">","1a5c9fbb":"<h3 style=\"text-align:center;font-size:200%;;\">Conclus\u00e3o do projeto<\/h3>\n<div class=\"progress\">\n  <div class=\"progress-bar\" role=\"progressbar\" style=\"width: 35%;\" aria-valuenow=\"25\" aria-valuemin=\"0\" aria-valuemax=\"100\">35% Conclu\u00eddo<\/div>\n<\/div>","bbb05504":"<a id=\"5.2\"><\/a> <br>\n# <div class=\"alert alert-warning\">5.2 - Acur\u00e1cia<\/div>\n\n\n**Esta \u00e9 a m\u00e9trica mais simples. \u00c9 basicamente o n\u00famero de acertos (positivos) divido pelo n\u00famero total de exemplos. Ela deve ser usada em datasets com a mesma propor\u00e7\u00e3o de exemplos para cada classe, e quando as penalidades de acerto e erro para cada classe forem as mesmas.**\n\n**Em problemas com classes desproporcionais, ela causa uma falsa impress\u00e3o de bom desempenho. Por exemplo, num dataset em que 80% dos exemplos perten\u00e7am a uma classe, s\u00f3 de classificar todos os exemplos naquela classe j\u00e1 se atinge uma precis\u00e3o de 80%, mesmo que todos os exemplos da outra classe estejam classificados incorretamente.**"}}