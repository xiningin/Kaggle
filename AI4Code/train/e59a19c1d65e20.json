{"cell_type":{"9f6f1425":"code","3e12fe1f":"code","468c407f":"code","9811c807":"code","e8a7bef6":"code","d2f592f6":"code","d9def08d":"code","206479c6":"code","ed2636da":"code","99e6a37f":"code","498a01c7":"code","eaee8978":"code","7575df79":"code","331c2eb4":"code","9f936554":"code","e709c4a5":"code","f55020f7":"code","31d0fd95":"code","831a5e88":"code","b811734f":"code","123bee7d":"code","07230f53":"code","faedabc0":"code","c05f0536":"code","f22177d5":"code","0f73aee7":"code","9a3140f3":"code","070e9fc8":"code","4aa41ffa":"code","f19fbf7c":"code","7c5858e4":"code","f0d0251c":"code","5d212cf0":"code","9cd743a5":"code","405f4872":"code","2e0262e0":"code","15e0c2c7":"code","c01d42f0":"code","7333fadb":"code","2b5e84cd":"code","f5176039":"code","45ccc7ff":"code","7077730a":"code","daf1ae20":"code","89f87914":"code","c1713241":"code","02bd47b0":"code","2f459a97":"code","a850e277":"code","0812f57f":"code","87a151b0":"code","518a7aaa":"code","a4cbcf8e":"code","e35b7e65":"code","bd668111":"code","0eae007a":"code","67639aed":"code","d263c0e2":"code","3a47093b":"code","8bf9cfd0":"code","874aa5b6":"code","2f6db575":"code","7e8e374f":"code","08740805":"code","f01fa677":"code","a435ba5a":"code","a48eca90":"code","4931e04d":"code","dc445d4d":"code","cbbc0c1d":"code","2484b6c0":"code","92fa1e72":"code","aca04719":"code","bc531074":"code","96b643be":"code","fb23b5e2":"code","d782c401":"code","3c2fb78c":"code","d301199e":"code","5fa0825f":"code","b14c00a5":"code","d9e1dd6b":"code","ad5e0a19":"code","73ae293e":"code","8c1921ed":"code","95180417":"code","7a7d43b7":"code","e3a5ad3a":"code","5808ea49":"code","8778dc09":"code","2b1c13f5":"code","27503b37":"code","3209dfd8":"code","fe9418c5":"code","d1a07df9":"code","4c757bd9":"code","87c8c921":"code","262c7299":"code","0e50e623":"code","576a524d":"code","5f405ed3":"code","5505da69":"code","3c95212b":"code","f0a990c1":"code","4919ecf0":"code","5dddfec9":"code","b8b44e74":"code","2b96f904":"markdown","bf41d8f5":"markdown","b175a4fa":"markdown","e74221e1":"markdown","62915115":"markdown","fd1556b8":"markdown","eb15a062":"markdown","fb261a71":"markdown","56ec53d7":"markdown","4b0be0d2":"markdown","51764dfa":"markdown","235d10e7":"markdown","4775095e":"markdown","4c4000f2":"markdown","42f939a9":"markdown","44c92391":"markdown","c80bd8fd":"markdown","d93a979a":"markdown","13d8ddc6":"markdown","f0221e8f":"markdown","3f953484":"markdown","6b23ed0f":"markdown","38a1d0cf":"markdown","ae3b4d37":"markdown","f8318daf":"markdown","4abbe32a":"markdown","80fc52c0":"markdown","cc7100c3":"markdown","da5ddd20":"markdown","6084ba57":"markdown","06de7465":"markdown","3878c48c":"markdown","1abcd4d0":"markdown","a966042d":"markdown","83afe313":"markdown","3bae0fcf":"markdown","853c966c":"markdown","93c10313":"markdown","cdf15398":"markdown","e4a78da7":"markdown","d3ced8b8":"markdown","77dda88e":"markdown","294361c9":"markdown","ec7bde57":"markdown","a501e202":"markdown","caf2dfbc":"markdown","89b64a91":"markdown","ba546577":"markdown","785a4480":"markdown","1491ced8":"markdown","17598e65":"markdown","06274f76":"markdown","d2e6efb7":"markdown","62baf62e":"markdown","fe8da990":"markdown","25b9c998":"markdown","f1f7a87d":"markdown","46fde088":"markdown","7b747e1a":"markdown","c01b0e2e":"markdown","ceecf9e7":"markdown","3569126b":"markdown","a9f0ee3e":"markdown","8db412d0":"markdown","4eed7d0c":"markdown","97da9d41":"markdown","484638e6":"markdown","fcc9d0ed":"markdown","7c20e0fa":"markdown","19a026a1":"markdown","1f840841":"markdown","c0558553":"markdown","47d6ad53":"markdown","3bacfe61":"markdown"},"source":{"9f6f1425":"# Importing all the necessary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","3e12fe1f":"data = pd.read_csv(\"..\/input\/ML_Lab_I_C28\/train.csv\")\nunseen = pd.read_csv(\"..\/input\/ML_Lab_I_C28\/test.csv\")\nsample = pd.read_csv(\"..\/input\/ML_Lab_I_C28\/sample.csv\")\ndata_dict = pd.read_csv(\"..\/input\/ML_Lab_I_C28\/data_dictionary.csv\")\npd.set_option('display.max_columns',100)\npd.set_option(\"display.max_rows\",100)\nprint(data.shape)\nprint(unseen.shape)\nprint(sample.shape)\nprint(data_dict.shape)","468c407f":"# check the data properly\ndata_dict","9811c807":"# Let's see the head of our master dataset and set the index to id\ndata = data.set_index(\"id\")\ndata.head()","e8a7bef6":"# Checking the percentage of missing values\nround(100*(data.isnull().sum()\/len(data.index)), 2)","d2f592f6":"# To check each variable datatype\ndata.info(verbose=1)","d9def08d":"# Sumarising the data\ndata.describe(include=\"all\")","206479c6":"#inspect the columns\ndata.columns","ed2636da":"# Drop all the columns in which greater than 3000 missing values are present\n\nfor col in data.columns:\n    if data[col].isnull().sum() > 3000:\n        data.drop(col, 1, inplace=True)","99e6a37f":"# Checking the percentage of missing values\nround(100*(data.isnull().sum()\/len(data.index)), 2)","498a01c7":"# To check each variable datatype\ndata.info(verbose=1,null_counts=True)","eaee8978":"#checking the number of rows and columns after removing null values\ndata.shape","7575df79":"# Checking if there are columns with one unique value since it won't affect our analysis\ndata.nunique(dropna=True)","331c2eb4":"# Get the value counts of all the columns\n\nfor column in data:\n    print(data[column].astype('category').value_counts())\n    print('___________________________________________________')","9f936554":"# Dropping unique valued columns\ndata1= data.drop([\"circle_id\",\"loc_og_t2o_mou\",\"std_og_t2o_mou\",\"loc_ic_t2o_mou\",\n                  \"last_date_of_month_6\",\"last_date_of_month_7\",\"last_date_of_month_8\",\n                  \"std_og_t2c_mou_6\",\"std_og_t2c_mou_7\",\"std_ic_t2o_mou_6\",\"std_ic_t2o_mou_7\",],axis=1)\n","e709c4a5":"# Checking the percentage of missing values again\nround(100*(data1.isnull().sum()\/len(data1.index)), 2)","f55020f7":"# Get the value counts of all the columns\n\nfor column in data1:\n    print(data1[column].astype('category').value_counts())\n    print('___________________________________________________')","31d0fd95":"#checking the number of rows and columns\ndata1.shape","831a5e88":"# Let's see the head of our master dataset\ndata1.head(100)","b811734f":"# checking the null values again\nround(100*(data1.isnull().sum()\/len(data1.index)), 2)\n#data1.isnull().sum().head(100).sort_values(ascending = False)","123bee7d":"# Drop all the columns which are greater than equal to 2687 missing values are present\n\nfor col in data1.columns:\n    if data1[col].isnull().sum() >= 2687:\n        data1.drop(col, 1, inplace=True)","07230f53":"# checking the null values again\ndata1.isnull().sum().head(100).sort_values(ascending = False)","faedabc0":"# Drop the null values rows in the column 'date_of_last_rech_8'\ndata1 = data1[~pd.isnull(data1[\"date_of_last_rech_8\"])]","c05f0536":"# Drop the null values rows in the column 'date_of_last_rech_7'\ndata1 = data1[~pd.isnull(data1[\"date_of_last_rech_7\"])]","f22177d5":"# Drop the null values rows in the column 'date_of_last_rech_6'\ndata1 = data1[~pd.isnull(data1[\"date_of_last_rech_6\"])]","0f73aee7":"# checking the null values again\ndata1.isnull().sum().head(100).sort_values(ascending = False)","9a3140f3":"# checking the shape again\ndata1.shape","070e9fc8":"# Checking the information of each columns again\ndata1.info()","4aa41ffa":"# Convert date columns to date format\n\ndata1[\"date_of_last_rech_6\"]= pd.to_datetime(data[\"date_of_last_rech_6\"],format = \"%m\/%d\/%Y\")\ndata1[\"date_of_last_rech_7\"]= pd.to_datetime(data[\"date_of_last_rech_7\"],format = \"%m\/%d\/%Y\")\ndata1[\"date_of_last_rech_8\"]= pd.to_datetime(data[\"date_of_last_rech_8\"],format = \"%m\/%d\/%Y\")","f19fbf7c":"# checking the data types again whether it's changed or not\ndata1.info()","7c5858e4":"# check for the null values again\ndata1.isnull().sum()","f0d0251c":"sns.countplot(x=\"churn_probability\",data = data1)","5d212cf0":"fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20, 6))\n\n# distribution plot for aon\nsns.distplot(data1['aon'], ax=ax1)\n\n# bin the aon column with yearwise segments and plot the counts for each segments\nbins = [0, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nlabels = [0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n#sns.countplot(pd.cut(round(((data['aon']\/30)\/12),1), bins = bins, labels = labels ), ax =ax1)\npd.crosstab(pd.cut(round(((data1['aon']\/30)\/12),1), bins = bins, labels = labels ), data1['churn_probability']).plot(kind='bar', stacked=True, ax = ax2)\nplt.show()","9cd743a5":"cols =[[\"total_ic_mou_6\"],\n      [\"total_ic_mou_7\"],[\"total_ic_mou_8\"]]\n#column description stats\nfor i in range(0,3):\n    display(data1[cols[i]].describe())\n\n# plot for the incoming calls usage\nplt.figure(figsize=(18, 5))\nfor i in range(0,3):\n    plt.subplot(1,3,i+1)\n    X = pd.concat([data1[cols[i]],data1['churn_probability']], axis=1)\n    X = pd.melt(X,id_vars=\"churn_probability\",var_name=\"features\",value_name='value')\n    sns.boxplot(x=\"features\", y=\"value\", hue=\"churn_probability\",data = X)\n    plt.xticks(rotation=90)    \n    plt.suptitle('Incoming Calls Usage')","405f4872":"cols = [['total_og_mou_6'],\n        ['total_og_mou_7'],\n        ['total_og_mou_8']]\n# column description stats\nfor i in range(0,3):\n    display(data1[cols[i]].describe())\n\n# plot for the outgoing calls usage\nplt.figure(figsize=(18, 5))\n#ic call usage\nfor i in range(0,3):\n    plt.subplot(1,3,i+1)\n    X = pd.concat([data1[cols[i]], data1['churn_probability']], axis=1)\n    X = pd.melt(X,id_vars=\"churn_probability\",var_name=\"features\",value_name='value')\n    sns.boxplot(x=\"features\", y=\"value\", hue=\"churn_probability\", data=X)\n    plt.xticks(rotation=90)    \n    plt.suptitle('Outgoing Calls Usage')","2e0262e0":"cols = ['total_rech_num_6','total_rech_num_7','total_rech_num_8']\n# column description stats\nfor i in range(0,3):\n    display(data1[cols[i]].describe())\n\n# plot for the recharge count columns\nplt.figure(figsize=(18, 10))\nplt.subplots_adjust(hspace=0.5)\nfor i in range(0,3):\n    plt.subplot(2,3,i+1)\n    X = pd.concat([data1[cols[i]], data1['churn_probability']], axis=1)\n    X = pd.melt(X,id_vars=\"churn_probability\",var_name=\"features\",value_name='value')\n    sns.boxplot(x=\"features\", y=\"value\", hue=\"churn_probability\", data=X)\n    plt.xticks(rotation=90)    \n    plt.suptitle('Recharge Count')","15e0c2c7":"cols = ['arpu_6','arpu_7','arpu_8']\n        \n\n# column description stats\nfor i in range(0,3):\n    display(data1[cols[i]].describe())\n\n# plot for the arpu\nplt.figure(figsize=(18, 5))\nfor i in range(0,3):\n    plt.subplot(1,3,i+1)\n    X = pd.concat([data1[cols[i]], data1['churn_probability']], axis=1)\n    X = pd.melt(X,id_vars=\"churn_probability\",var_name=\"features\",value_name='value')\n    sns.boxplot(x=\"features\", y=\"value\", hue=\"churn_probability\", data=X)\n    plt.xticks(rotation=90)    \n    plt.suptitle('Arpu')","c01d42f0":"cols = [\n        ['vol_2g_mb_6','vol_2g_mb_7','vol_2g_mb_8'],\n        ['vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8'],\n        ['monthly_2g_6','monthly_2g_7','monthly_2g_8'],\n        ['monthly_3g_6','monthly_3g_7','monthly_3g_8'],\n        ['sachet_2g_6','sachet_2g_7','sachet_2g_8'],\n        ['sachet_3g_6','sachet_3g_7','sachet_3g_8'],\n        ['jun_vbc_3g','jul_vbc_3g','aug_vbc_3g']\n       ]\n\n# column description stats\nfor i in range(0,7):\n    display(data[cols[i]].describe())\n\n# plot for the 2g-3g volume\nplt.figure(figsize=(18, 15))\nplt.subplots_adjust(hspace=0.5)\nfor i in range(0,7):\n    plt.subplot(3,3,i+1)\n    X = pd.concat([data1[cols[i]], data1['churn_probability']], axis=1)\n    X = pd.melt(X,id_vars=\"churn_probability\",var_name=\"features\",value_name='value')\n    sns.boxplot(x=\"features\", y=\"value\", hue=\"churn_probability\", data=X)\n    plt.xticks(rotation=90)    \n    plt.suptitle('2G-3G Volume')","7333fadb":"for col in data1.columns:\n    percentiles = data1[col].quantile([0.01, 0.99]).values\n    data1[col][data1[col] <= percentiles[0]] = percentiles[0]\n    data1[col][data1[col] >= percentiles[1]] = percentiles[1]","2b5e84cd":"data1.columns","f5176039":"# Looking at the correlation table\nplt.figure(figsize = (25,15))\nsns.heatmap(data1.corr(),cmap=\"YlGnBu\", annot=True)\nplt.show()","45ccc7ff":"# storing the columns so that we can use those in unseen data \n# choosing the variables with less correlation among themselves\nnew_vars=['total_rech_num_6', 'total_rech_num_7', 'total_rech_num_8',\n       'total_rech_amt_6', 'total_rech_amt_7', 'total_rech_amt_8',\n       'max_rech_amt_6', 'max_rech_amt_7', 'max_rech_amt_8',\n       'vol_2g_mb_6', 'vol_2g_mb_7', 'vol_2g_mb_8', 'vol_3g_mb_6',\n       'vol_3g_mb_7', 'vol_3g_mb_8', 'monthly_2g_6', 'monthly_2g_7',\n       'monthly_2g_8', 'sachet_2g_6', 'sachet_2g_7', 'sachet_2g_8',\n       'monthly_3g_6', 'monthly_3g_7', 'monthly_3g_8', 'sachet_3g_6',\n       'sachet_3g_7', 'sachet_3g_8','aug_vbc_3g', 'jul_vbc_3g',\n       'jun_vbc_3g']\n\n\n","7077730a":"cols = ['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8']\n# get the recent date of recharge in the last 3 months\ndata1['last_rech_date'] = data1[cols].max(axis=1)\n# get the number of days from the recent recharge date till the last date of august month\ndata1['days_since_last_rech'] = np.floor(( pd.to_datetime('2014-08-31', format='%Y-%m-%d') - data1['last_rech_date'] ).astype('timedelta64[D]'))\n","daf1ae20":"# Checking the dataframe again as two new columns is being created\ndata1.info()","89f87914":"# total columns\ncols = ['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8']\ndata1['mean_total_ic_mou'] = round(data1[cols].mean(axis=1),2)\n# Weightage for ic for the last 3 months\ndf = data1[cols].astype(bool)\ndata1['total_ic_weightage'] = ( df['total_ic_mou_6'] * 1 ) + ( df['total_ic_mou_7'] * 10 ) + ( df['total_ic_mou_8'] * 100 )\n\ncols = ['total_og_mou_6','total_og_mou_7','total_og_mou_8']\ndata1['mean_total_og_mou'] = round(data1[cols].mean(axis=1),2)\n# Weightage for og for the last 3 months\ndf = data1[cols].astype(bool)\ndata1['total_og_weightage'] = ( df['total_og_mou_6'] * 1 ) + ( df['total_og_mou_7'] * 10 ) + ( df['total_og_mou_8'] * 100 )\n\ndata1['mean_total_mou'] = data1['mean_total_ic_mou'] + data1['mean_total_og_mou']\n\ndata1['mean_total_mou_6'] = round(data1[['total_ic_mou_6','total_og_mou_6']].mean(axis=1),2)\ndata1['mean_total_mou_7'] = round(data1[['total_ic_mou_7','total_og_mou_7']].mean(axis=1),2)\ndata1['mean_total_mou_8'] = round(data1[['total_ic_mou_8','total_og_mou_8']].mean(axis=1),2)","c1713241":"data1.info()","02bd47b0":"# total_rech_num columns\ncols = ['total_rech_num_6','total_rech_num_7','total_rech_num_8']\n# mean of total recharge number\ndata1['mean_total_rech_num'] = round(data1[cols].mean(axis=1),2)\n# Minimum of total recharge number\ndata1['min_total_rech_num'] = data1[cols].min(axis=1)\n# Maximum of total recharge number\ndata1['max_total_rech_num'] = data1[cols].max(axis=1)","2f459a97":"# total_rech_amt columns\ncols = ['total_rech_amt_6','total_rech_amt_7','total_rech_amt_8']\ndata1['mean_total_rech_amt'] = round(data1[cols].mean(axis=1),2)\ndata1['min_total_rech_amt'] = data1[cols].min(axis=1)\ndata1['max_total_rech_amt'] = data1[cols].max(axis=1)","a850e277":"# max_rech_amt columns\ncols = ['max_rech_amt_6','max_rech_amt_7','max_rech_amt_8']\ndata1['mean_max_rech_amt'] = round(data1[cols].mean(axis=1),2)\n\n# last_day_rch_amt columns\ncols = ['last_day_rch_amt_6','last_day_rch_amt_7','last_day_rch_amt_8']\ndata1['mean_last_day_rch_amt'] = round(data1[cols].mean(axis=1),2)","0812f57f":"#get recharge num weightage for the last three months\ncols = ['total_rech_num_6','total_rech_num_7','total_rech_num_8']\ndf = data1[cols].astype(bool)\ndata1['rech_num_weightage'] = ( df['total_rech_num_6'] * 1 ) + ( df['total_rech_num_7'] * 10 ) + ( df['total_rech_num_8'] * 100 )","87a151b0":"#get recharge amount weightage for the last three months\ncols = ['total_rech_amt_6','total_rech_amt_7','total_rech_amt_8']\ndf = data1[cols].astype(bool)\ndata1['rech_amt_weightage'] = ( df['total_rech_amt_6'] * 1 ) + ( df['total_rech_amt_7'] * 10 ) + ( df['total_rech_amt_8'] * 100 )","518a7aaa":"# arpu columns\n# ARPU = Total Revenue \/ Average Subscribers\ncols = ['arpu_6','arpu_7','arpu_8']\ndata1['mean_arpu'] = round(data1[cols].mean(axis=1),2)\n\ncols = ['vol_2g_mb_6','vol_2g_mb_7','vol_2g_mb_8']\ndata1['mean_vol_2g_mb_data'] = round(data1[cols].mean(axis=1),2)\n\ncols = ['vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8']\ndata1['mean_vol_3g_mb_data'] = round(data1[cols].mean(axis=1),2)","a4cbcf8e":"#get vbc mean for the last three months\ncols = ['jun_vbc_3g','jul_vbc_3g','aug_vbc_3g']\ndata1['mean_vbc_3g'] = round(data1[cols].mean(axis=1),2)","e35b7e65":"#get monthly pack weightage for the last three months\ndata1['monthly_2g_weightage'] = ( data1['monthly_2g_6'] * 1 ) + ( data1['monthly_2g_7'] * 10 ) + ( data1['monthly_2g_8'] * 100 )\ndata1['monthly_3g_weightage'] = ( data1['monthly_3g_6'] * 1 ) + ( data1['monthly_3g_7'] * 10 ) + ( data1['monthly_3g_8'] * 100 )\n\n#get sachet pack weightage for the last three months\ndata1['sachet_2g_weightage'] = ( data1['sachet_2g_6'] * 1 ) + ( data1['sachet_2g_7'] * 10 ) + ( data1['sachet_2g_8'] * 100 )\ndata1['sachet_3g_weightage'] = ( data1['sachet_3g_6'] * 1 ) + ( data1['sachet_3g_7'] * 10 ) + ( data1['sachet_3g_8'] * 100 )","bd668111":"# checking the dataframe\ndata1.info()","0eae007a":"# Checking the number of rows and columns\ndata1.shape","67639aed":"data1.info()","d263c0e2":"#Taking a backup of dataset for later use\nm_df = data1.copy()","3a47093b":"# prepare the dataset\nchurn = data1['churn_probability']\ndata1 = data1.drop('churn_probability', axis=1)\n# we have to drop the date columns otherwise scaling and log transform is not possible\ndata1 = data1.drop(\"date_of_last_rech_6\",axis = 1)\ndata1 = data1.drop(\"date_of_last_rech_7\",axis = 1)\ndata1 = data1.drop(\"date_of_last_rech_8\",axis = 1)\ndata1 = data1.drop(\"last_rech_date\",axis = 1)\n\n#split the columns into category and numerical\ncat_cols = ['monthly_2g_6','sachet_2g_6','monthly_3g_6','sachet_3g_6',\n            'monthly_2g_7','sachet_2g_7','monthly_3g_7','sachet_3g_7',\n            'monthly_2g_8','sachet_2g_8','monthly_3g_8','sachet_3g_8'] \n\nnum_cols = list(set(data1.columns).difference(set(cat_cols)))","8bf9cfd0":"data1.info()","874aa5b6":"# dummy encode the categorical columns\ndata1 = pd.concat([data1,pd.get_dummies(data1[cat_cols], drop_first=True)], axis=1)\n\n# drop the original columns\ndata1.drop(cat_cols, axis=1, inplace=True)","2f6db575":"# log transform with constant 10000 for real numbers\ndata1[num_cols] = np.log((10000 + data1[num_cols]))\ndata1 = np.log((10000 + data1))\n","7e8e374f":"#Standardize the numeric values\ndata1[num_cols] = (( data1[num_cols] - data1[num_cols].mean() ) \/ data1[num_cols].std())\n\n","08740805":"# check the data again\ndata1.head()","f01fa677":"X = data1\nY = churn\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3, random_state=42)\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)","a435ba5a":"#initialize the pca with randomized\npca = PCA(svd_solver='randomized', random_state=42)\n# fit the training dataset\npca.fit(X_train)\n","a48eca90":"pca.components_","4931e04d":"pca.explained_variance_ratio_","dc445d4d":"#Screeplot for the PCA components\nfig = plt.figure(figsize=[12,8])\nplt.vlines(x=30, ymax=1.2, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.99, xmax=50, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('Cumulative variance explained')\nplt.show()","cbbc0c1d":"pca_final = IncrementalPCA(n_components=30)","2484b6c0":"df_train_pca = pca_final.fit_transform(X_train)","92fa1e72":"df_train_pca.shape","aca04719":"#correlation matrix\ncorrmat = np.corrcoef(df_train_pca.transpose())","bc531074":"#correlation matrix shape\ncorrmat.shape","96b643be":"plt.figure(figsize=[15,15])\nsns.heatmap(corrmat, annot=True)\nplt.show()","fb23b5e2":"df_test_pca = pca_final.transform(X_test)\ndf_test_pca.shape","d782c401":"# fit and transform the whole dataset\nX_pca = pca.fit_transform(X)","3c2fb78c":"# List to store the model scores\nmodel_score_list = []","d301199e":"# Will create a function and call it again and again for different models\ndef evaluate_model(actual, pred):\n    # Accuracy Score\n    acc_score = round(accuracy_score(actual, pred)*100,2)\n    print('Accuracy Score : ',acc_score)\n    # ROC AUC score\n    roc_score = round(roc_auc_score(actual, pred)*100,2)\n    print('ROC AUC score : ',roc_score)\n    # Precision score\n    prec_score = round(precision_score(actual, pred)*100,2)\n    print('Precision score : ', prec_score)\n    # Recall score\n    rec_score = round(recall_score(actual, pred)*100,2)\n    print('Recall score : ', rec_score)\n\n    return acc_score, roc_score, prec_score, rec_score","5fa0825f":"# Initialize the Logistic regression\nmodel1 = LogisticRegression(class_weight='balanced', random_state=42)\n# fit the pca training data\nmodel1.fit(df_train_pca, Y_train)\n# predict the testing pca data\nY_pred = model1.predict(df_test_pca)\n# Model evaluation\nacc_score, roc_score, prec_score, rec_score = evaluate_model(Y_test, Y_pred)\n# add the model scores to score list \nmodel_score_list.append({'model_name':'LogisticRegression', 'acc_score':acc_score, 'roc_score':roc_score, 'precision_score':prec_score, 'recall_score':rec_score})","b14c00a5":"# initialize the randomforest\nmodel2 = RandomForestClassifier(class_weight='balanced', random_state=42)\n# fit the pca training data\nmodel2.fit(df_train_pca, Y_train)\n# predict the pca testing data\nY_pred = model2.predict(df_test_pca)\n\n# Model evaluation\nacc_score, roc_score, prec_score, rec_score = evaluate_model(Y_test, Y_pred)\n# add the model scores to score list\nmodel_score_list.append({'model_name':'RandomForestClassifier', 'acc_score':acc_score, 'roc_score':roc_score, 'precision_score':prec_score, 'recall_score':rec_score})","d9e1dd6b":"# convert the model scores to dataframe\nmodel_score_df = pd.DataFrame(model_score_list,columns=['model_name', 'acc_score', 'roc_score','precision_score','recall_score'])\n\n# Order by highest recall score and roc_auc_score\nmodel_score_df.sort_values(['acc_score','roc_score'], ascending=False)","ad5e0a19":"#Cross val score for Logistic regression\ncross_val_score(model1, X_train, Y_train, cv=5, n_jobs=-1)\ncross_val_score(model1, X_train, Y_train, cv=5, n_jobs=-1).mean()","73ae293e":"#Cross val score for Random Forest Classifier\ncross_val_score(model2, X_train, Y_train, cv=5, n_jobs=-1)\ncross_val_score(model2, X_train, Y_train, cv=5, n_jobs=-1).mean()","8c1921ed":"rf = RandomForestClassifier(random_state=42, n_jobs=-1)","95180417":"hyper_params = {'max_depth': [3, 5, 10, 15, 20],\n                'max_features': [3, 5, 7, 11, 15],\n                'min_samples_leaf': [20, 50, 100, 200, 400],\n                'n_estimators': [10, 25, 50, 80, 100]\n               }","7a7d43b7":"model_cv = GridSearchCV(estimator=rf, \n             param_grid=hyper_params,\n             verbose=1,\n             cv=5,\n             n_jobs=-1,\n             return_train_score=True)","e3a5ad3a":"model_cv.fit(X_train, Y_train)","5808ea49":"# print the grid results\nprint('\\n Best estimator:')\nprint(model_cv.best_estimator_)\nprint('\\n Best score:')\nprint(model_cv.best_score_ * 2 - 1)\nprint('\\n Best parameters:')\nprint(model_cv.best_params_)","8778dc09":"# initialize the model with the best estimor\nmodel = model_cv.best_estimator_\n\n# fit the pca training dataset\nmodel.fit(df_train_pca, Y_train)\n\n# Predict the model with the pca testing dataset\nY_pred = model.predict(df_test_pca)\n\n# get the predict probablities of pca testing dataset\nY_pred_proba = model.predict_proba(df_test_pca)","2b1c13f5":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = roc_curve( actual, probs, drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()","27503b37":"# Classification Report\nprint('\\nClassification Report : \\n\\n', classification_report(Y_test, Y_pred))\n\n# Confusion matrix\ntn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\nprint('\\nTN = {0}, FP = {1}, FN = {2}, TP = {3}\\n\\n'.format(tn, fp, fn, tp))\n\n# Model evaluation\nacc_score, roc_score, prec_score, rec_score = evaluate_model(Y_test, Y_pred)","3209dfd8":"# ROC-AUC curve\ndraw_roc(Y_test, Y_pred)","fe9418c5":"# Frame the dataset with the predicted probabilities\nY_pred_final = pd.DataFrame({'actual':Y_test,'pred_nonchurn_prob':Y_pred_proba[:,0],'pred_churn_prob':Y_pred_proba[:,1],'predicted':Y_pred})\nY_pred_final.head(5)","d1a07df9":"# create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    Y_pred_final[i]= Y_pred_final['pred_churn_prob'].map( lambda x: 1 if x > i else 0)\nY_pred_final.head()","4c757bd9":"# calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = confusion_matrix( Y_pred_final['actual'], Y_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    sensi = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    speci = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","87c8c921":"# plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])","262c7299":"Y_pred_final['final_predicted'] = Y_pred_final['pred_churn_prob'].map( lambda x: 1 if x > 0.1 else 0)","0e50e623":"# Classification Report\nprint('\\nClassification Report : \\n\\n', classification_report(Y_test, Y_pred_final['final_predicted']))\n\n# Confusion matrix\ntn, fp, fn, tp = confusion_matrix(Y_test, Y_pred_final['final_predicted']).ravel()\nprint('\\nTN = {0}, FP = {1}, FN = {2}, TP = {3}\\n\\n'.format(tn, fp, fn, tp))\n\n# Model evaluation\nacc_score, roc_score, prec_score, rec_score = evaluate_model(Y_test, Y_pred_final['final_predicted'])","576a524d":"# ROC-AUC curve\ndraw_roc(Y_test, Y_pred_final['final_predicted'])","5f405ed3":"# Doing an observation of the data bfre feature engineering and which are uncorrelated\ncmap = plt.get_cmap('Spectral')\ncolors = [cmap(i) for i in np.linspace(0, 1, 9)]\n\nplt.figure(figsize=(20,15))\nplt.pie(model2.feature_importances_, labels=new_vars, autopct='%2.2f%%', shadow=False, colors=colors)\nplt.axis('equal')\nplt.show()","5505da69":"#Unseen Data\nunseen.head()\n","3c95212b":"unseen.info(verbose=1)\n","f0a990c1":"new_vars","4919ecf0":"submission_data = unseen.set_index('id')[new_vars]\nsubmission_data.shape","5dddfec9":"unseen['churn_probability'] = model2.predict(submission_data)\noutput = unseen[['id','churn_probability']]\noutput.head()","b8b44e74":"output.to_csv('submission_pca_lr_25_Aug.csv',index=False)","2b96f904":"### So, incoming call usage have outliers at the top percentile","bf41d8f5":"### Since it is more important to identify churners than the non-churners accurately, we go with the RandomForestClassifier which has high accuracy score and CV score","b175a4fa":"### Age on Network (aon)","e74221e1":"## Hyperparameter tuning for RANDOMFOREST CLASSIFIER","62915115":"### Now all the columns are fine and no null values so now we are ready to go for EDA Analysis","fd1556b8":"## for kaggle upload creating submission.csv file","eb15a062":"## Since we can see here we have so many columns having the same null values having the value 2768 , we will drop those columns\n","fb261a71":"So, here we can see RANDOM FOREST CLASSIFIER is giving more Accuracy Score with 93.95% ","56ec53d7":"# Now from 172 columns we have removed the columns with missing values greater than 3000 and now the columns have dropped to 112","4b0be0d2":"### As we can see the dates column- 'date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8' are object data type ..Will change it to date data type\n","51764dfa":"## Recharge Count","235d10e7":"### Fitting the Best Estimator","4775095e":"# 3. Data Cleaning and Preparation","4c4000f2":"Adding columns for each category using dummy encoding and drop the original columns","42f939a9":"### Components from PCA","44c92391":"# 1 . Importing the Libraries","c80bd8fd":"#### Now we will extract the weightage for recharge count and amount columns for the last three months (June, July and August) in increasing order","d93a979a":"## We can see a lot of null values out here so need to inspect it one by one","13d8ddc6":"#### - We can see more number of obervations between 1 and 3 years\n#### - The churn rate is also higher in this time than the others\n","f0221e8f":"# 6. Feature Selection and Dimentionality Reduction using PCA ","3f953484":"## Numeric columns - Recharge ","6b23ed0f":"### Churn(Target Variable) ","38a1d0cf":"### 2G and 3G ","ae3b4d37":"- We will extract the mean, min,max values of recharge related columns for the last three months (June,July,August)","f8318daf":"#### 2G and 3G volume columns also seem to have outliers at the top percentile","4abbe32a":"### Model Evaluation","80fc52c0":"### Let's take a backup so that we can use it for future for model 2","cc7100c3":"Now let's try with the cutoff of 0.1 and see if the False negatives can be reduced.","da5ddd20":"#### Will perform with 30 components","6084ba57":"### Dummy encode the categorical features","06de7465":"### Observation","3878c48c":"## Date columns","1abcd4d0":"Let's view the models score list to select the top models","a966042d":"All the features are almost positively skewed, we'll log transform the numerical features to neutralize the positive skewness in the dataset.","83afe313":"Next, lets create a new column in the unseen dataset called churn_probability and use the model pipeline to predict the probabilities for this data","3bae0fcf":"# Model Building","853c966c":"## Random Forest","93c10313":"## Split into train test ","cdf15398":"### Recharge count columns also seems to have outliers at the top percentile","e4a78da7":"## Outgoing calls usage","d3ced8b8":"Looking at the explained variance ratio","77dda88e":"Plotting the heatmap of the corr matrix","294361c9":"### Average Revenue Per user(arpu)","ec7bde57":"# 5. Feature Engineering","a501e202":"#### Optimal probability cutoff to minimize False Negatives","caf2dfbc":"### As we can see churn rate is low in the overall dataset. So we would need to handle the class imbalance.","89b64a91":"# BASE LINE MODELLING","ba546577":"### We can see here no correlation is there in the given data frame","785a4480":"# 4. EDA Analysis","1491ced8":"#### Now that we have all the necessary features, we can start select the important features, build the baseline models and evaluate them.","17598e65":"### Outgoing calls also seem to have outliers at the top percentile ","06274f76":"Applying the transformation on the test set","d2e6efb7":"### As per the graph it is clear that total recharge amount in the month of June and August were maximum. So it will be better if we focus on the customers who made the last recharge on the month of June and August.It will help us in getting maximum churn ","62baf62e":"### Earlier we have converted the date columns to datetime format , now we will extract the columns from last recharge date and days since last recharge","fe8da990":"### Arpu also seems to have outliers at the top percentile","25b9c998":"\n- Now we will extract the mean values for incoming and outgoing columns for the last three months (June, July and August) \n- And also extract the weightage for incoming and outgoing columns for the last three months (June, July , August) in the increasing order","f1f7a87d":"### Log Transform the numerical features","46fde088":"The best score we got using GridSearchCV is 88.87% and best parameters are - {'max_depth': 15, 'max_features': 15, 'min_samples_leaf': 20, 'n_estimators': 25}\n\nNow let's try fitting the best estimator and predict the results","7b747e1a":"### Dropping the null value rows of  the columns \"date_of_last_rech_8\",\"date_of_last_rech_7\",\"date_of_last_rech_6\" as these columns might be useful\n","c01b0e2e":"### Splitting the columns into categorical and numerical type","ceecf9e7":"It is observed from the final observation that the accuracy score is around 88% and recall score with 76 % which is a good one","3569126b":" Will draw a SCREE PLOT","a9f0ee3e":"## CROSS VALIDATION ","8db412d0":"### PCA","4eed7d0c":"# Outliers Treatment ","97da9d41":"## Logistic Regression","484638e6":"# Final Model Evaluation","fcc9d0ed":"Finally, lets create a csv file out of this dataset, ensuring to set index=False to avoid an addition column in the csv.","7c20e0fa":"We have got the accuracy score of 94.18% with the best estimator which is slightly better than the Cross Validation mean score. Also the above Roc Curve seems to be in a good shape.\n\nNows let's see if we can optimize the cutoff further to minimize the False Negatives","19a026a1":"### Incoming calls usage","1f840841":"# 2. Read and Understand the data\n","c0558553":"## Model Score List","47d6ad53":"### We need to cap the outliers ","3bacfe61":"Now we will apply PCA on the training dataset for dimentionality reduction and feature selection "}}