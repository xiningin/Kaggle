{"cell_type":{"dc3b8ce7":"code","dae5086f":"code","3566016b":"code","a0a4ff6d":"code","f3855444":"code","c4ec9a04":"code","4c137937":"code","6317f90b":"code","1a45b633":"code","0cefd909":"code","2a690289":"code","a50ca7df":"code","681a3aba":"code","1afd8d17":"code","11f1ee66":"code","215eb698":"code","da077380":"code","4cd2944c":"markdown","43b1cd01":"markdown","3e3fee3a":"markdown","2f377911":"markdown"},"source":{"dc3b8ce7":"import numpy as np\nimport pandas as pd\nimport unicodedata\nimport string\nimport re\nimport queue\nimport math\nimport os\nimport gc\nimport itertools\n\nimport tensorflow as tf\nfrom tensorflow_hub import KerasLayer\nimport torch\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","dae5086f":"# Plots display settings\nplt.rcParams['figure.figsize'] = 12, 8\nplt.rcParams.update({'font.size': 14})","3566016b":"# TensorFlow settings\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nIMG_SIZE = 299\nBATCH_SIZE = 32","a0a4ff6d":"# Number of features (words in dict) for TF-IDF\ntf_idf_features = 2000","f3855444":"# Train data paths\ntrain_csv_path = '\/kaggle\/input\/shopee-product-matching\/train.csv'\ntrain_img_path = '\/kaggle\/input\/shopee-product-matching\/train_images'\n\n# Test data paths\ntest_csv_path = '\/kaggle\/input\/shopee-product-matching\/test.csv'\ntest_img_path = '\/kaggle\/input\/shopee-product-matching\/test_images'","c4ec9a04":"# Pretrained image classification model EfficientNetB7\n# from tf.keras.applications with global average pooling as a final layer\n# In this notebook the model is loaded from a public dataset on Kaggle\n# at https:\/\/www.kaggle.com\/ekaterinadranitsyna\/keras-applications-models\nimg_model_path = '..\/input\/keras-applications-models\/EfficientNetB7.h5'","4c137937":"# Universal sentence encoder model\n# Original model by Google could be loaded from: https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\n# In this notebook the model is loaded from a public dataset on Kaggle\n# at https:\/\/www.kaggle.com\/dimitreoliveira\/universalsentenceencodermodels\ntxt_model_path = '..\/input\/universalsentenceencodermodels\/universal-sentence-encoder-models\/use'","6317f90b":"# Variable defines if the cross-validation will be performed for the final classifier\nvalidation = True","1a45b633":"# Target values and features to thain the classifier\ntarget = ['true_match']\n\nfeatures = ['text_score', 'image_score', 'txt_img_score', 'words_ratio',\n            'txt_img_words', 'phash_match', 'nums_match']","0cefd909":"# Regex expression to remove traces of emoji from text\nRE_SYMBOLS = re.compile(\"x\\w\\d\\S+\")","2a690289":"def get_csv_data(csv_path: str, img_dir: str) -> pd.DataFrame:\n    \"\"\"Function reads data from a csv file, performs text cleaning\n    in titles column and transforms image file names into file paths.\n    :param csv_path: Path to a scv file\n    :param img_dir: Path to directory with images\n    :return Processed pd.DataFrame\n    \"\"\"\n    data = pd.read_csv(csv_path)\n    data['title'] = data['title'].apply(preprocess_titles)\n    data['image'] = data['image'].apply(abs_path, args=(img_dir,))\n    return data\n\n\ndef preprocess_titles(s: str) -> str:\n    \"\"\"Function converts text to lowercase, removes punctuation,\n    replaces multiple spaces, normalizes and removes traces of emoji symbols.\n    :param s: original text string\n    :return: cleaned text string\n    \"\"\"\n    s = RE_SYMBOLS.sub(r'', s)\n    s = s.translate(str.maketrans('', '', string.punctuation))\n    s = re.sub('\\s+', ' ', s)\n    s = s.lower()\n    return unicodedata.normalize('NFKC', s)\n\n\ndef get_tfidf_features(n_features) -> np.array:\n    \"\"\"Function creates an array of TF-IDF features\n    for titles referencing DataFrame objects from outer scope.\n    :param n_features: Maximum number of words in TF-IDF dictionary\n    :return: Array of TF-IDF features with shape n_items x n_features\n    \"\"\"\n    # Transform all titles from the original DataFrame into TF-IDF matrix\n    vectorizer = TfidfVectorizer(decode_error='ignore',\n                                 stop_words='english',\n                                 max_features=n_features)\n\n    vectors = vectorizer.fit_transform(data['title']).toarray().astype(np.float16, copy=False)\n    print('TF-IDF features extracted. Shape:', vectors.shape)\n\n    return vectors\n\n\ndef get_text_features() -> np.array:\n    \"\"\"Function loads Universal sentence encoder model model from Kaggle dataset,\n    transforms titles from the original DataFrame into feature matrix\n    using embeddings from the pretrained language model.\n    :returns: Features array with the shape n_samples x 512 features\n    \"\"\"\n    # Universal sentence encoder model\n    # Original model by Google could be loaded from: https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\n    # In this notebook the model is loaded from a public dataset on Kaggle\n    # at https:\/\/www.kaggle.com\/dimitreoliveira\/universalsentenceencodermodels\n    text_model = tf.keras.Sequential(\n        [KerasLayer(txt_model_path, input_shape=[], dtype=tf.string,  # Pretrained model\n                    output_shape=[512], trainable=False),\n         tf.keras.layers.Layer(512, dtype='float16')]  # This layer reduces precision of float numbers\n    )\n\n    # Convert all texts to vectors\n    features = text_model.predict(data['title'],\n                                  batch_size=BATCH_SIZE,\n                                  use_multiprocessing=True,\n                                  workers=-1)\n    print('Text features extracted. Shape:', features.shape)\n\n    return features\n\n\ndef get_image_features(paths: pd.Series) -> np.array:\n    \"\"\"Function loads pretrained image classification model from file,\n    transforms images into feature matrix with the shape n_samples x n_features.\n    :param paths: Series object containing paths to image files\n    :returns: Features array with the shape n_samples x 2560 features\n    \"\"\"\n    # Pretrained image classification model to convert images into embeddings\n    image_model = tf.keras.Sequential(\n        [tf.keras.models.load_model(img_model_path),  # Pretrained model\n         tf.keras.layers.Layer(2560, dtype='float16')]  # This layer reduces precision of float numbers\n    )\n\n    # Transform paths to files into tf.data.Dataset\n    input_data = tf.data.Dataset.from_tensor_slices(paths)\n    # Preprocess images\n    input_data = input_data.map(process_path, num_parallel_calls=AUTOTUNE)\n    input_data = configure_for_performance(input_data)\n\n    # Convert all images into embeddings and average colors\n    features = image_model.predict(input_data,\n                                   batch_size=BATCH_SIZE,\n                                   use_multiprocessing=True,\n                                   workers=-1)\n    print('Image features extracted. Shape:', features.shape)\n\n    return features\n\n\n@tf.function\ndef process_path(file_path: str):\n    \"\"\"Function reads image from the file and returns\n    preprocessed image.\n    :param file_path: Path to the image file\n    :return Tensor with preprocessed image from the file\n    \"\"\"\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n    return tf.keras.applications.efficientnet.preprocess_input(img)  # Shape: IMG_SIZE x IMG_SIZE x 3\n\n\ndef configure_for_performance(ds):\n    \"\"\"Function applies batches and prefetches dataset\n    to optimize data processing.\n    :param ds: TensorFlow Dataset object\n    :return Batched TensorFlow Dataset object with prefetch() applied\n    \"\"\"\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\n\ndef word_intersection() -> list:\n    \"\"\"Function processes DataFrame with candidate item pairs\n    and returns a list containing a ratio of intersecting words\n    in title pairs to the union of unique words. References objects\n    from outer scope.\n    :return: List of float values ranging between 0 and 1.\n    Length of list equals to the number of given item pairs.\n    \"\"\"\n\n    def count_words(title_pair: np.array) -> float:\n        \"\"\"Function calculates number of unique words present\n        in both titles and ratio of that number to the union of unique words.\n        :param title_pair: Array containing two titles\n        :return: Ratio of intersecting words to union of unique words\n        \"\"\"\n        title_1, title_2 = title_pair\n        # Transform into sets of words\n        title_1 = set(title_1.split())\n        title_2 = set(title_2.split())\n        # Divide length of intersection by length of union\n        ratio = len(title_1.intersection(title_2)) \/ len(title_1.union(title_2))\n        return ratio\n\n    # Find titles for each pair in the current chunk by their indexes\n    tmp_df = pd.DataFrame()\n    tmp_df['title_1'] = data.loc[pairs.loc[:, 'idx_1'].values, 'title'].values\n    tmp_df['title_2'] = data.loc[pairs.loc[:, 'idx_2'].values, 'title'].values\n\n    # Process title pairs in current chunk and add results to the list\n    scores = [result for result in map(count_words, tmp_df[['title_1', 'title_2']].values)]\n\n    return scores\n\n\ndef combinations(n) -> float:\n    \"\"\"Function calculates number of unique combinations\n    of two titles among n samples with the same label group.\"\"\"\n    c = math.factorial(n) \/ (math.factorial(2) * math.factorial(n - 2))\n    return c\n\n\ndef feature_mining(features: np.array,\n                   query_chunk_size: int = 5000,\n                   corpus_chunk_size: int = 100000,\n                   max_pairs: int = 500000,\n                   top_k: int = 100) -> list:\n    \"\"\"Given an array of features, this function performs data mining.\n    It compares all items against all other items and returns a list\n    with the pairs that have the highest cosine similarity score.\n\n    :param features: np.array of shape n_samples * n_features for all items\n    :param query_chunk_size: Search for most similar pairs for query_chunk_size at the same time.\n           Decrease, to lower memory footprint (increases run-time).\n    :param corpus_chunk_size: Compare an image simultaneously against corpus_chunk_size other items.\n           Decrease, to lower memory footprint (increases run-time).\n    :param max_pairs: Maximal number of item pairs returned.\n    :param top_k: For each item, we retrieve up to top_k other items.\n    :return: Returns a list of triplets with the format [score, id1, id2]\n    \"\"\"\n\n    top_k += 1  # An image has the highest similarity to itself. Increase +1 as we are interest in distinct pairs\n\n    # Mine for duplicates\n    pairs = queue.PriorityQueue()\n    min_score = -1\n    num_added = 0\n\n    for corpus_start_idx in range(0, len(features), corpus_chunk_size):\n        corpus_end_idx = min(corpus_start_idx + corpus_chunk_size, len(features))\n        for query_start_idx in range(0, len(features), query_chunk_size):\n            query_end_idx = min(query_start_idx + query_chunk_size, len(features))\n\n            cos_scores = torch.Tensor(\n                cosine_similarity(features[query_start_idx:query_end_idx],\n                                  features[corpus_start_idx:corpus_end_idx])\n            )\n\n            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(top_k, len(cos_scores[0])),\n                                                                       dim=1, largest=True, sorted=False)\n            cos_scores_top_k_values = cos_scores_top_k_values.tolist()\n            cos_scores_top_k_idx = cos_scores_top_k_idx.tolist()\n\n            for query_itr in range(len(cos_scores)):\n                for top_k_idx, corpus_itr in enumerate(cos_scores_top_k_idx[query_itr]):\n                    i = query_start_idx + query_itr\n                    j = corpus_start_idx + corpus_itr\n\n                    if i != j and cos_scores_top_k_values[query_itr][top_k_idx] > min_score:\n                        pairs.put((cos_scores_top_k_values[query_itr][top_k_idx], i, j))\n                        num_added += 1\n\n                        if num_added >= max_pairs:\n                            entry = pairs.get()\n                            min_score = entry[0]\n\n    # Get the pairs\n    added_pairs = set()  # Used for duplicate detection\n    pairs_list = []\n    while not pairs.empty():\n        score, i, j = pairs.get()\n        sorted_i, sorted_j = sorted([i, j])\n\n        if sorted_i != sorted_j and (sorted_i, sorted_j) not in added_pairs:\n            added_pairs.add((sorted_i, sorted_j))\n            pairs_list.append([score, i, j])\n\n    return pairs_list\n\n\ndef check_identity(par: str) -> np.array:\n    \"\"\"Function finds values in 'par' column of the original DataFrame\n    using 2 row indexes from DataFrame of candidate pairs and returns\n    a binary column for match between two items (1 - matching, 0 - non-matching).\n    :param par: parameter to check\n    :return Series with binary values - result of the comparison\n    \"\"\"\n    # Temporary dataframe to compare 'par' values for item pairs\n    identity = pd.DataFrame()\n    # Look up values by respective row indexes\n    identity['item_1'] = data.iloc[pairs['idx_1'], :][par].values\n    identity['item_2'] = data.iloc[pairs['idx_2'], :][par].values\n    # Binary column signifying match or mismatch\n    identity['match'] = (identity['item_1'] == identity['item_2']).astype('int')\n    return identity['match'].values\n\n\ndef numbers_identity() -> np.array:\n    \"\"\"Function extracts numbers from title pairs and compares them,\n    returns a binary column for match between numbers in two titles.\n    :return Series with binary values - result of the comparison\n    \"\"\"\n    # Temporary dataframe to compare numbers for item pairs\n    identity = pd.DataFrame()\n    # Look up values by respective row indexes\n    identity['item_1'] = data.iloc[pairs['idx_1'], :]['title'].values\n    identity['item_2'] = data.iloc[pairs['idx_2'], :]['title'].values\n    # Extract numbers and convert them to space-delimited string\n    identity['nums_1'] = identity['item_1'].apply(lambda x: ' '.join(re.findall(r'\\d+', x)))\n    identity['nums_2'] = identity['item_2'].apply(lambda x: ' '.join(re.findall(r'\\d+', x)))\n    # Binary column signifying match or mismatch\n    identity['match'] = (identity['nums_1'] == identity['nums_2']).astype('int')\n    return identity['match'].values\n\n\ndef check_pairs(reference_df: pd.DataFrame, check_labels=False) -> list:\n    \"\"\"Function finds indexes of all phash pairs and label group pairs\n    (for train set only) in the original DataFrame.\n    :param reference_df: original DataFrame with items\n    :param check_labels: boolean flag, if True - add indexes for all known title pairs\n           from the train set regardless of similarity scores and phash values\n    :return: Returns a list of index pairs\n    \"\"\"\n    print('Number of candidate pairs:', len(pairs))\n    # Indexes of add pairs with identical phash\n    all_pairs = set()\n    groups = reference_df.groupby(by='image_phash')\n    # Numeric index of rows in train set as a temporary column\n    reference_df['num_idx'] = [idx for idx in range(len(reference_df))]\n    for group in groups.indices:\n        num_index = groups.get_group(group)['num_idx']\n        # All combinations of index pairs with the same phash\n        cur_pairs = set(itertools.combinations(num_index, 2))\n        all_pairs = all_pairs.union(cur_pairs)\n    print(f'Total number of phash pairs: {len(all_pairs)}')\n\n    # When dealing with train set, add indexes of all label group pairs\n    if check_labels:\n        groups = reference_df.groupby(by='label_group')\n        for group in groups.indices:\n            num_index = groups.get_group(group)['num_idx']\n            # All combinations of index pairs with the same label group\n            cur_pairs = set(itertools.combinations(num_index, 2))\n            all_pairs = all_pairs.union(cur_pairs)\n        print(f'With labels added, total number of pairs: {len(all_pairs)}')\n\n    return list(all_pairs)\n\n\ndef abs_path(file_name: str, directory: str) -> str:\n    \"\"\"Function returns a Series of absolute paths to images\n    given file names and directory name.\n    :param file_name: Name of the image file\n    :param directory: Name of directory containing the file\n    :return Path to the image file\n    \"\"\"\n    return os.path.join(directory, file_name)\n\n\ndef analyze_similarities():\n    \"\"\"Function compares number of actual item pairs\n    for various cosine similarity thresholds and binary features.\n    \"\"\"\n    print('Total number of candidate pairs:', len(pairs))\n    print(f'\\nNumber of actual item pairs in the train set: {pairs[\"true_match\"].sum()}\\n')\n\n    for feature in ['text_score', 'image_score', 'txt_img_score', 'words_ratio', 'txt_img_words']:\n\n        # Check distribution of True and False predictions for various similarity scores\n        print('-' * 50)\n        print(f'\\nDistribution of True\/False predictions for {feature}')\n        for thr in (0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95):\n            print('-' * 50)\n            print(f'Similarity score over {thr}')\n            pairs_sample = pairs[pairs[feature] >= thr]\n            print(f'Number of similar item pairs: {len(pairs_sample)}')\n            print(pairs_sample['true_match'].value_counts(normalize=True))\n\n    # Check if identical phash can be used to improve the accuracy\n    same_phash = pairs[pairs['phash_match'] == 1]\n    different_phash = pairs[pairs['phash_match'] == 0]\n\n    print('\\nFor item pairs with the same phash:')\n    print(same_phash['true_match'].value_counts(normalize=True))\n    print('Number of item pairs in this subset:', len(same_phash))\n\n    print('\\nFor item pairs with different phash:')\n    print(different_phash['true_match'].value_counts(normalize=True))\n    print('Number of item pairs in this subset:', len(different_phash))\n\n    # Check if numbers in titles can be used to improve the accuracy\n    same_numbers = pairs[pairs['nums_match'] == 1]\n    different_numbers = pairs[pairs['nums_match'] == 0]\n\n    print('\\nFor item pairs with the same numbers:')\n    print(same_numbers['true_match'].value_counts(normalize=True))\n    print('Number of item pairs in this subset:', len(same_numbers))\n\n    print('\\nFor item pairs with different numbers:')\n    print(different_numbers['true_match'].value_counts(normalize=True))\n    print('Number of item pairs in this subset:', len(different_numbers))\n\n\ndef correlation_and_distribution(pairs: pd.DataFrame, features: list, target: list):\n    \"\"\"Function plots a heatmap of correlation for features\n    in 'pairs' and distribution of feature values for two classes.\n    :param pairs: DataFrame with candidate item pairs\n    :param features: List of input features names\n    :param target: List with target name\n    \"\"\"\n    # Correlation between all features and predicted binary target\n    ax = sns.heatmap(pairs[features + target].corr(),\n                     center=0, annot=True, cmap='RdBu_r')\n    l, r = ax.get_ylim()\n    ax.set_ylim(l + 0.5, r - 0.5)\n    plt.yticks(rotation=0)\n    plt.title('Correlation matrix')\n    plt.show()\n\n    # Distribution of feature values for two classes\n    similar = pairs[pairs['true_match'] == 1]\n    different = pairs[pairs['true_match'] == 0]\n\n    for feature in features:\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.hist(similar[feature], bins=30)\n        plt.title(f'{feature} in similar samples')\n        plt.subplot(1, 2, 2)\n        plt.hist(different[feature], bins=30)\n        plt.title(f'{feature} in different samples')\n        plt.show()\n\n\ndef find_similar_items(df: pd.DataFrame, pairs: pd.DataFrame) -> pd.Series:\n    \"\"\"Function returns pd.Series with space-delimited IDs\n    for products with similar titles, including self-match.\n    :param df: DataFrame with original data\n    :param pairs: DataFrame with candidate pairs\n    :return Series object with space-delimited IDs\n    \"\"\"\n\n    # Add columns with posting IDs from the original DataFrame\n    pairs['id_1'] = pairs['idx_1'].apply(lambda x: df.loc[x, 'posting_id'])\n    pairs['id_2'] = pairs['idx_2'].apply(lambda x: df.loc[x, 'posting_id'])\n\n    # Group posting IDs by id_1\n    id_1 = pd.DataFrame(pairs.groupby('id_1')['id_2'].unique())\n\n    # Convert lists to space-delimited strings\n    id_1['id_2'] = id_1['id_2'].apply(lambda x: ' '.join(x))\n    # Convert to dictionary\n    id_1 = id_1.to_dict()['id_2']\n    # Create a column for self-match and other matching IDs (space-delimited)\n    df['title_match'] = df['posting_id'] + ' '\n    df['title_match'] = df['title_match'] + df['posting_id'].apply(lambda x: id_1[x] if x in id_1 else '')\n\n    # Group posting IDs by id_2\n    id_2 = pd.DataFrame(pairs.groupby('id_2')['id_1'].unique())\n    id_2['id_1'] = id_2['id_1'].apply(lambda x: ' '.join(x))\n    id_2 = id_2.to_dict()['id_1']\n    df['title_match'] = df['title_match'] + ' ' + df['posting_id'].apply(lambda x: id_2[x] if x in id_2 else '')\n\n    return df['title_match']","a50ca7df":"# Load train data\ndata = get_csv_data(csv_path=train_csv_path, img_dir=train_img_path)\n\n# Get TF-IDF vectors\nfeatures_arr = get_tfidf_features(n_features=tf_idf_features)\ngc.collect()  # Launch garbage collector to avoid memory errors\n\n# Get text embeddings\nfeatures_arr = np.hstack((features_arr, get_text_features()))\ngc.collect()\n\n# Get a list of similar item pairs based on text\npairs = feature_mining(features_arr, query_chunk_size=1000, corpus_chunk_size=30000, top_k=100)\npairs = pd.DataFrame(pairs, columns=['text_score', 'idx_1', 'idx_2'])\ngc.collect()\n\n# Order index pairs so that first index is lower than the second one\n# to avoid duplicates when adding phash and label pairs\nordered = pairs[['idx_1', 'idx_2']].agg(['min', 'max'], axis='columns')\npairs[['idx_1', 'idx_2']] = ordered.values\n\n# Get image embeddings\nimg_features_arr = get_image_features(data['image'])\ngc.collect()\n\n# Get a list of similar item pairs based on images\nimg_pairs = feature_mining(img_features_arr, query_chunk_size=1000, corpus_chunk_size=30000, top_k=100)\nimg_pairs = pd.DataFrame(img_pairs, columns=['image_score', 'idx_1', 'idx_2'])\ngc.collect()\n\n# Order index pairs so that first index is lower than the second one\n# to avoid duplicates when adding phash and label pairs\nordered = img_pairs[['idx_1', 'idx_2']].agg(['min', 'max'], axis='columns')\nimg_pairs[['idx_1', 'idx_2']] = ordered.values\n\n# Combine all candidate pairs\npairs = pd.merge(pairs, img_pairs, how='outer')\ndel img_pairs\ngc.collect()\n\n# Get indexes for all pairs with identical phash and label\nall_pairs = pd.DataFrame(check_pairs(data, check_labels=True), columns=['idx_1', 'idx_2'])\npairs = pairs.append(all_pairs, ignore_index=True).drop_duplicates(subset=['idx_1', 'idx_2'])\ngc.collect()\n\n# Fill in missing values in similarity scores in chunks of size 5,000\nchunk = 5_000\nwhile True:\n    if pairs['text_score'].isna().sum() == 0:\n        break\n    else:\n        replace_idx = pairs[pairs['text_score'].isna()].head(chunk).index\n        # Indexes for image pairs with missing image similarity scores\n        idx_1 = pairs[pairs['text_score'].isna()].head(chunk)['idx_1'].values\n        idx_2 = pairs[pairs['text_score'].isna()].head(chunk)['idx_2'].values\n        # Pass both feature arrays through Dot layer to get cosine similarity scores\n        pairs.loc[replace_idx, 'text_score'] = np.diagonal(\n            cosine_similarity(features_arr[idx_1], features_arr[idx_2])\n        )\ndel features_arr\ngc.collect()\nwhile True:\n    if pairs['image_score'].isna().sum() == 0:\n        break\n    else:\n        replace_idx = pairs[pairs['image_score'].isna()].head(chunk).index\n        # Indexes for image pairs with missing image similarity scores\n        idx_1 = pairs[pairs['image_score'].isna()].head(chunk)['idx_1'].values\n        idx_2 = pairs[pairs['image_score'].isna()].head(chunk)['idx_2'].values\n        # Pass both feature arrays through Dot layer to get cosine similarity scores\n        pairs.loc[replace_idx, 'image_score'] = np.diagonal(\n            cosine_similarity(img_features_arr[idx_1], img_features_arr[idx_2])\n        )    \ndel img_features_arr\ngc.collect()\n\n# Multuplicative scores\npairs['txt_img_score'] = pairs['text_score'] * pairs['image_score']","681a3aba":"# Ratio of intersecting words in title pairs to union of unique words\npairs['words_ratio'] = word_intersection()\ngc.collect()\n\n# Multiplicative feature\npairs['txt_img_words'] = pairs['txt_img_score'] * pairs['words_ratio']\n\n# Add binary features (0 or 1):\n# Ground truth labels for training\npairs['true_match'] = check_identity('label_group')\n# Same phash\npairs['phash_match'] = check_identity('image_phash')\n# Identical numbers in titles\npairs['nums_match'] = numbers_identity()\ngc.collect()\n\nprint(pairs.head())","1afd8d17":"# See if binary features can improve predictions\nanalyze_similarities()","11f1ee66":"# Visualize correlation between features and target\n# and distribution of values in two classes\ncorrelation_and_distribution(pairs, features, target)","215eb698":"# Final classifier\nclf = RandomForestClassifier(min_samples_leaf=10, n_jobs=-1)\n\n# If we run this notebook for cross-validation\nif validation:\n    skf = StratifiedKFold(3, shuffle=True, random_state=1)\n    scores = cross_val_score(clf, pairs[features], pairs[target].values.ravel(), cv=skf, scoring='f1')\n    print('VotingClassifier\\nCross-validation scores:', scores)\n    print('Average train CV score:', np.mean(scores))\n\n# Train the ensemble using all available data\nclf.fit(\n    pairs[features],\n    pairs[target].values.ravel()\n)","da077380":"# Load test data\ndata = get_csv_data(csv_path=test_csv_path, img_dir=test_img_path)\n\nif len(data) == 3:\n    data['matches'] = data['posting_id']\n    data[['posting_id', 'matches']].to_csv('\/kaggle\/working\/submission.csv', index=False)\n\nelse:\n    # Get TF-IDF vectors\n    features_arr = get_tfidf_features(n_features=tf_idf_features)\n    gc.collect()  # Launch garbage collector to avoid memory errors\n\n    # Get text embeddings\n    features_arr = np.hstack((features_arr, get_text_features()))\n    gc.collect()\n\n    # Get a list of similar item pairs based on text\n    pairs = feature_mining(features_arr, query_chunk_size=1000, corpus_chunk_size=20000, top_k=100)\n    pairs = pd.DataFrame(pairs, columns=['text_score', 'idx_1', 'idx_2'])\n    gc.collect()\n\n    # Order index pairs so that first index is lower than the second one\n    # to avoid duplicates when adding phash and label pairs\n    ordered = pairs[['idx_1', 'idx_2']].agg(['min', 'max'], axis='columns')\n    pairs[['idx_1', 'idx_2']] = ordered.values\n\n    # Get image embeddings\n    img_features_arr = get_image_features(data['image'])\n    gc.collect()\n\n    # Get a list of similar item pairs based on images\n    img_pairs = feature_mining(img_features_arr, query_chunk_size=1000, corpus_chunk_size=20000, top_k=100)\n    img_pairs = pd.DataFrame(img_pairs, columns=['image_score', 'idx_1', 'idx_2'])\n    gc.collect()\n\n    # Order index pairs so that first index is lower than the second one\n    # to avoid duplicates when adding phash and label pairs\n    ordered = img_pairs[['idx_1', 'idx_2']].agg(['min', 'max'], axis='columns')\n    img_pairs[['idx_1', 'idx_2']] = ordered.values\n\n    # Combine all candidate pairs\n    pairs = pd.merge(pairs, img_pairs, how='outer')\n    del img_pairs\n    gc.collect()\n\n    # Get indexes for all pairs with identical phash and label\n    all_pairs = pd.DataFrame(check_pairs(data, check_labels=True), columns=['idx_1', 'idx_2'])\n    pairs = pairs.append(all_pairs, ignore_index=True).drop_duplicates(subset=['idx_1', 'idx_2'])\n    gc.collect()\n\n    # Fill in missing values in similarity scores in chunks of size 5,000\n    chunk = 5_000\n    while True:\n        if pairs['text_score'].isna().sum() == 0:\n            break\n        else:\n            replace_idx = pairs[pairs['text_score'].isna()].head(chunk).index\n            # Indexes for image pairs with missing image similarity scores\n            idx_1 = pairs[pairs['text_score'].isna()].head(chunk)['idx_1'].values\n            idx_2 = pairs[pairs['text_score'].isna()].head(chunk)['idx_2'].values\n            # Pass both feature arrays through Dot layer to get cosine similarity scores\n            pairs.loc[replace_idx, 'text_score'] = np.diagonal(\n                cosine_similarity(features_arr[idx_1], features_arr[idx_2])\n            )\n    del features_arr\n    gc.collect()\n    while True:\n        if pairs['image_score'].isna().sum() == 0:\n            break\n        else:\n            replace_idx = pairs[pairs['image_score'].isna()].head(chunk).index\n            # Indexes for image pairs with missing image similarity scores\n            idx_1 = pairs[pairs['image_score'].isna()].head(chunk)['idx_1'].values\n            idx_2 = pairs[pairs['image_score'].isna()].head(chunk)['idx_2'].values\n            # Pass both feature arrays through Dot layer to get cosine similarity scores\n            pairs.loc[replace_idx, 'image_score'] = np.diagonal(\n                cosine_similarity(img_features_arr[idx_1], img_features_arr[idx_2])\n            ) \n    del img_features_arr\n    gc.collect()\n\n    # Multuplicative scores\n    pairs['txt_img_score'] = pairs['text_score'] * pairs['image_score']\n\n    # Ratio of intersecting words in title pairs to union of unique words\n    pairs['words_ratio'] = word_intersection()\n    gc.collect()\n\n    # Multiplicative feature\n    pairs['txt_img_words'] = pairs['txt_img_score'] * pairs['words_ratio']\n    \n    # Add binary features\n    pairs['phash_match'] = check_identity('image_phash')\n    pairs['nums_match'] = numbers_identity()\n    gc.collect()\n    print(pairs.head())\n\n    # Predict matching pairs with classification model\n    pairs['pred_match'] = clf.predict(pairs[features])\n\n    # Drop all pairs that did not pass the final classifier\n    pairs = pairs[pairs['pred_match'] == 1]\n\n    # Add a column with space-delimited IDs for matching items to the test data set\n    data['matches'] = find_similar_items(data, pairs)\n\n    # Save two required columns to csv file\n    data[['posting_id', 'matches']].to_csv('\/kaggle\/working\/submission.csv', index=False)","4cd2944c":"# Shoppee: Matching products posted on e-commerse platform\n### Version for usage with Internet access disabled\n\nThe goal of this competition is to produce a machine learning algorithm to identify similar items posted for sale on an e-commerse platform and for each item in the data set create a list of matching products.\n\nThe data consists of images and text descriptions (titles) for each item. In addition to that, each item has a \"phash\" - non-unique perceptual hash, which sometimes indicates that the products are matching (come from the same label group) but in other cases could be misleading.\n\nComparing images and texts also leads to some degree of uncertainty. Similar images or titles do not always signify that the products are matching. Minor differences in size, color or other characteristics of the products could be crucial for correctly grouping them.\n\nChecking some examples of highly similar items in the train we can conclude that the data contains some mislabeled samples. At least, there is no reason why two products with exact same titles and images indistinguishable to the human eye come from different label groups. Thus, resulting accuracy metrics do not wholly depend on the quality of the algorithm but also reflect quality of the original data.\n\nWe will treat the current task as a binary classification problem: we will identify probable item pairs in the data set and try to predict if they come from the same label group or not based on their features.\n\nThe algorithm to search for probable item pairs and classify them includes the following steps:\n- TF-IDF is used to convert titles into feature vector.\n- Pretrained model (Universal sentence encoder by Goodle) is used to tranform titles into text embeddings.\n- TF-IDF vectors and text embeddings are joined into texts feature matrix.\n- Pretrained EfficientNetB7 model from tensorflow library is used to transform imager into images feature matrix.\n- Both feature matrices are used to search for most similar item pairs based on cosine similarity scores. Results are combined.\n- The list of candidate item pairs is expanded to include all pairs that have the same phash. For training set label pairs are also added.\n- For each item pair in the combined data set a binary feature is created to indicate whether ot not both titles contain the same numbers.\n- For each pair of titles ratio of intersecting words to union of unique words is calculated.\n- For every item pair a binary feature is created to indicate whether ot not the products have the same phash.\n- Additional features are generated by multiplying similarity scores and other features.\n- In the final step Random Forest classifier is used to identify matching items among all probable pairs based on similarity scores and other generated features. The classifier is being trained on the train set and used for prediction on the test set.","43b1cd01":"# Processing train set\n### Feature extraction and feature engineering, evaluation of the classification model","3e3fee3a":"### Functions","2f377911":"# Applying models to test set"}}