{"cell_type":{"1dfe2061":"code","f7d551e7":"code","80f663ba":"code","4b763506":"code","f6922f5f":"code","26fab58d":"code","8091eef3":"code","ef05acd4":"code","bd504867":"code","2fbb1f62":"code","cce8309c":"code","1f9657cd":"code","af63a8b5":"code","10e97d18":"code","04857e48":"code","1f9de3e1":"code","0487f747":"code","925bbe56":"code","f36f10dd":"code","bba612d8":"code","30f50409":"code","c216f3e3":"code","45967cbd":"code","2bbdaa3d":"code","1c8ee0bf":"code","ce4f6746":"code","1aaad0e2":"code","91298189":"code","fc7f9407":"code","82be7656":"code","1118cc56":"code","9b67c27f":"code","adbc4504":"code","6e0bca89":"code","a02f12f7":"code","99218f00":"code","6e243748":"code","7876cfbc":"code","78a0c657":"code","d4b480a4":"code","fa720de1":"code","1fb81729":"markdown","285000fe":"markdown","dcc5f240":"markdown","a4a29cfe":"markdown","f476ce1b":"markdown","a681673a":"markdown","a44f6269":"markdown","33038f51":"markdown","0ae18de9":"markdown","198c4dea":"markdown","39cc02b5":"markdown","6bc80ad8":"markdown","7576a47b":"markdown","1958b307":"markdown","50b9424e":"markdown","0ce5b64a":"markdown"},"source":{"1dfe2061":"import io\nimport re\nimport json\nimport string\nimport unicodedata\nfrom random import randint\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nfrom wordcloud import STOPWORDS, WordCloud\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding","f7d551e7":"!pip install contractions\n!pip install tensorflow-addons","80f663ba":"from tensorflow_addons.metrics import F1Score","4b763506":"from contractions import contractions_dict\n\nfor key, value in list(contractions_dict.items())[:10]:\n    print(f'{key} == {value}')","f6922f5f":"def load_data(size_to_read=100_000):\n    filename = '\/kaggle\/input\/shakespeare-plays\/alllines.txt'\n    with open(filename, 'r') as f:\n        data = f.read(size_to_read)\n    return data\n\n\ndata = load_data(size_to_read=500_000)\ndata[:100]","26fab58d":"corpus = data.lower().split('\\n')\nprint(corpus[:10])","8091eef3":"def expand_contractions(text, contraction_map=contractions_dict):\n    # Using regex for getting all contracted words\n    contractions_keys = '|'.join(contraction_map.keys())\n    contractions_pattern = re.compile(f'({contractions_keys})', flags=re.DOTALL)\n\n    def expand_match(contraction):\n        # Getting entire matched sub-string\n        match = contraction.group(0)\n        expanded_contraction = contraction_map.get(match)\n        if not expand_contractions:\n            print(match)\n            return match\n        return expanded_contraction\n\n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n\n\nexpand_contractions(\"y'all can't expand contractions i'd think\")","ef05acd4":"for idx, sentence in enumerate(corpus):\n    corpus[idx] = expand_contractions(sentence)\ncorpus[:5]","bd504867":"# Remove puncuation from word\ndef rm_punc_from_word(word):\n    clean_alphabet_list = [\n        alphabet for alphabet in word if alphabet not in string.punctuation\n    ]\n    return ''.join(clean_alphabet_list)\n\n\nprint(rm_punc_from_word('#cool!'))","2fbb1f62":"# Remove puncuation from text\ndef rm_punc_from_text(text):\n    clean_word_list = [rm_punc_from_word(word) for word in text]\n    return ''.join(clean_word_list)\n\n\nprint(rm_punc_from_text(\"Frankly, my dear, I don't give a damn\"))","cce8309c":"# Remove numbers from text\ndef rm_number_from_text(text):\n    text = re.sub('[0-9]+', '', text)\n    return ' '.join(text.split())  # to rm `extra` white space\n\n\nprint(rm_number_from_text('You are 100times more sexier than me'))\nprint(rm_number_from_text('If you taught yes then you are 10 times more delusional than me'))","1f9657cd":"# Remove stopwords from text\ndef rm_stopwords_from_text(text):\n    _stopwords = stopwords.words('english')\n    text = text.split()\n    word_list = [word for word in text if word not in _stopwords]\n    return ' '.join(word_list)\n\n\nrm_stopwords_from_text(\"Love means never having to say you're sorry\")","af63a8b5":"# Cleaning text\ndef clean_text(text):\n    text = text.lower()\n    text = rm_punc_from_text(text)\n    text = rm_number_from_text(text)\n    text = rm_stopwords_from_text(text)\n\n    # there are hyphen(\u2013) in many titles, so replacing it with empty str\n    # this hyphen(\u2013) is different from normal hyphen(-)\n    text = re.sub('\u2013', '', text)\n    text = ' '.join(text.split())  # removing `extra` white spaces\n\n    # Removing unnecessary characters from text\n    text = re.sub(\"(\\\\t)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\r)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\n)\", ' ', str(text)).lower()\n\n    # remove accented chars ('S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt' => 'Some Accented text')\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\n        'utf-8', 'ignore'\n    )\n\n    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n\n    text = re.sub(r\"[<>()|&\u00a9\u00f8\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n\n    text = re.sub(\"(mailto:)\", ' ', str(text)).lower()\n    text = re.sub(r\"(\\\\x9\\d)\", ' ', str(text)).lower()\n    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)).lower()\n    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM',\n                  str(text)).lower()\n\n    text = re.sub(\"(\\.\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\-\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\:\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    try:\n        url = re.search(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', str(text))\n        repl_url = url.group(3)\n        text = re.sub(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', repl_url, str(text))\n    except:\n        pass\n\n    text = re.sub(\"(\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    return text\n\nclean_text(\"Mrs. Robinson, you're trying to seduce me, aren't you?\")","10e97d18":"for idx, sentence in enumerate(corpus):\n    corpus[idx] = clean_text(sentence)\ncorpus[:10]","04857e48":"print(f'Corpus size before: {len(corpus)}')\ncorpus = [sentence for sentence in corpus if len(sentence.split(' ')) > 1]\nprint(f'Corpus size now: {len(corpus)}')\ncorpus[:10]","1f9de3e1":"# To customize colours of wordcloud texts\ndef wc_blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(214, 67%%, %d%%)\" % randint(60, 100)","0487f747":"# stopwords for wordcloud\ndef get_wc_stopwords():\n    wc_stopwords = set(STOPWORDS)\n\n    # Adding words to stopwords\n    # these words showed up while plotting wordcloud for text\n    wc_stopwords.add('s')\n    wc_stopwords.add('one')\n    wc_stopwords.add('using')\n    wc_stopwords.add('example')\n    wc_stopwords.add('work')\n    wc_stopwords.add('use')\n    wc_stopwords.add('make')\n\n    return wc_stopwords","925bbe56":"# plot wordcloud\ndef plot_wordcloud(text, color_func):\n    wc_stopwords = get_wc_stopwords()\n    wc = WordCloud(\n        stopwords=wc_stopwords, width=1200, height=400, random_state=0\n    ).generate(text)\n\n    f, axs = plt.subplots(figsize=(20, 10))\n    with sns.axes_style(\"ticks\"):\n        sns.despine(offset=10, trim=True)\n        plt.imshow(\n            wc.recolor(color_func=color_func, random_state=0),\n            interpolation=\"bilinear\"\n        )\n        plt.xlabel('Title WordCloud')\n\n\nplot_wordcloud(' '.join(corpus), wc_blue_color_func)","f36f10dd":"print(f\"Vocab size: {len(set(' '.join(corpus).split(' ')))}\")\noov_token = '<UNK>'","bba612d8":"tokenizer = Tokenizer(oov_token=oov_token)\ntokenizer.fit_on_texts(corpus)\n\nword_index = tokenizer.word_index\n\ntotal_words = len(word_index) + 1\n\n'''\n Adding 1 to total_words to avoid below error\n \n IndexError                                Traceback (most recent call last)\n    <ipython-input-136-16f89b53d516> in <module>\n    ----> 1 y = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n          2 print(y[1])\n\n    \/opt\/conda\/lib\/python3.7\/site-packages\/tensorflow\/python\/keras\/utils\/np_utils.py in to_categorical(y, num_classes, dtype)\n         76   n = y.shape[0]\n         77   categorical = np.zeros((n, num_classes), dtype=dtype)\n    ---> 78   categorical[np.arange(n), y] = 1\n         79   output_shape = input_shape + (num_classes,)\n         80   categorical = np.reshape(categorical, output_shape)\n\n    IndexError: index 3049 is out of bounds for axis 1 with size 3049\n    \n This is because of reserving padding (i.e. index zero).\n \n Stackoverflow post for more into: https:\/\/stackoverflow.com\/questions\/53525994\/how-to-find-num-words-or-vocabulary-size-of-keras-tokenizer-when-one-is-not-as\n'''\n\nprint(total_words)","30f50409":"# Converting the text to sequence using the tokenizer\ndef get_input_sequences(corpus, tokenizer):\n    input_sequences = []\n\n    for line in corpus:\n        tokens_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(tokens_list)):\n            n_gram_sequence = tokens_list[:i + 1]\n            input_sequences.append(n_gram_sequence)\n\n    return input_sequences\n\n\ninput_sequences = get_input_sequences(corpus, tokenizer)\nprint(input_sequences[:5])","c216f3e3":"# getting the max len of among all sequences\nmax_sequence_len = max([len(x) for x in input_sequences])\nprint(max_sequence_len)","45967cbd":"# padding the input sequence\npadded_input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\nprint(padded_input_sequences[1])","2bbdaa3d":"# shuffling the data\nnp.random.shuffle(padded_input_sequences)","1c8ee0bf":"def map_sequence_to_text(x, y, index_word):\n    text = ''\n    for index in x:\n        text += index_word[index] + ' '\n    text += f'[{index_word[y]}]'\n    return text","ce4f6746":"# Removing the input and output texts\nx = padded_input_sequences[:, :-1]\nlabels = padded_input_sequences[:, -1]\n\nprint(x[17])\nprint(labels[17])\n\ndef map_sequence_to_text(x, y, index_word):\n    text = ''\n    for index in x:\n        if index == 0:\n            # index 0 == padded char\n            continue\n        text += index_word[index] + ' '\n    text += f'[{index_word[y]}]'\n    return text\nmap_sequence_to_text(x[17], labels[17], tokenizer.index_word)","1aaad0e2":"y = tf.keras.utils.to_categorical(labels, num_classes=total_words)\nprint(y[1])","91298189":"lstm_units = 512\nembedding_dim = 512\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nloss = 'categorical_crossentropy'\nnum_epochs = 30\n\nmetrics = [\n    'accuracy',\n    AUC(curve='ROC', multi_label=True, name='auc_roc'),\n    F1Score(num_classes=total_words, average='weighted')\n]","fc7f9407":"class CustomCallback(Callback):\n    def on_epoch_start(self, epoch, logs=None):\n        print()\n        \n    def on_epoch_end(self, epoch, logs=None):\n        loss = logs['loss']\n        accuracy = logs['accuracy']\n        f1_score = logs['f1_score']\n        auc_roc = logs['auc_roc']\n\n        info = {\n            'loss': round(loss, 5),\n            'accuracy': round(accuracy, 4),\n            'auc_roc': round(auc_roc, 4),\n            'f1_score': round(f1_score, 4),\n        }\n\n        print(f'\\n{json.dumps(info, indent=2)}')\n\n\ncallbacks = [\n    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.000001, verbose=1),\n    CustomCallback()\n]","82be7656":"def build_model(\n    total_words,\n    max_sequence_len,\n    lstm_units=lstm_units,\n    embedding_dim=embedding_dim,\n    loss=loss,\n    optimizer=optimizer,\n    metrics=metrics\n):\n    model = Sequential([\n        Embedding(total_words, embedding_dim, input_length=max_sequence_len - 1, trainable=True),\n        Bidirectional(LSTM(lstm_units)),\n        Dense(total_words, activation='softmax')\n    ])\n\n    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n    return model\n\n\nmodel = build_model(total_words, max_sequence_len, lstm_units, embedding_dim)\nmodel.summary()","1118cc56":"history = model.fit(x, y, epochs=num_epochs, callbacks=callbacks, verbose=1)","9b67c27f":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","adbc4504":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","6e0bca89":"# Saving embedding vectors and words to visualize embeddings using `Tensorflow Projector`.\n# Getting weights of our embedding layer\nembedding_layer = model.layers[0]\nembedding_layer_weigths = embedding_layer.get_weights()[0]\nprint(embedding_layer_weigths.shape)\n\n# Reversing the `word_index`\nword_index = tokenizer.word_index\nreverse_word_index = {value: key for key, value in word_index.items()}\n\n# Writing vectors and their meta data which when entered to Tensorflow Project,\n# it will display our Word Embedding\nout_vectors = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_metadata = io.open('meta.tsv', 'w', encoding='utf-8')\n# Skipping over the first word in vocabulary which is '<OOV>' (if set oov_token parameter set then)\nfor word_num in range(1, total_words):\n    words = reverse_word_index[word_num]\n    embeddings = embedding_layer_weigths[word_num]\n    out_metadata.write(words + '\\n')\n    out_vectors.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\nout_vectors.close()\nout_metadata.close()","a02f12f7":"def predict_next(model, text, tokenizer, max_sequence_len, num_of_words=10):\n    # predict next num_of_words for text\n    for _ in range(num_of_words):\n        input_sequences = tokenizer.texts_to_sequences([text])[0]\n        padded_input_sequences = pad_sequences(\n            [input_sequences], maxlen=max_sequence_len - 1, padding='pre'\n        )\n        predicted = model.predict_classes(padded_input_sequences, verbose=0)\n        output_word = ''\n\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n\n        text += ' ' + output_word\n\n    return text","99218f00":"seed_text = 'The sky is'\nprint(predict_next(model, seed_text, tokenizer, max_sequence_len, num_of_words=4))","6e243748":"seed_text = 'Everything is fair in love and'\nprint(predict_next(model, seed_text, tokenizer, max_sequence_len, num_of_words=10))","7876cfbc":"seed_text = 'My life'\nprint(predict_next(model, seed_text, tokenizer, max_sequence_len, num_of_words=15))","78a0c657":"seed_text = 'You are a type of guy that'\nprint(predict_next(model, seed_text, tokenizer, max_sequence_len, num_of_words=20))","d4b480a4":"seed_text = 'FROM off a hill whose concave womb reworded A plaintful'\nprint(predict_next(model, seed_text, tokenizer, max_sequence_len, num_of_words=5))","fa720de1":"model.save('model')","1fb81729":"The first value of the above output is the `vocab_size(total_words)` and second value is the `embedding_dim`.","285000fe":"**Plotting model's performance**","dcc5f240":"## \ud83d\udd2e Predictions","a4a29cfe":"**Preparing data for neural network**","f476ce1b":"Padding from starting since we are going to `predict the last word`.","a681673a":"![](https:\/\/media.giphy.com\/media\/3o6gbbt9uYNcOtJ29y\/giphy.gif)","a44f6269":"## \ud83d\udc69\u200d\ud83d\udd2c Modelling\n\n![](https:\/\/media.giphy.com\/media\/YS57N6teaevJASvcMA\/giphy.gif)","33038f51":"# Text Generation\n\nHere I'm using [Shakespeare plays](https:\/\/www.kaggle.com\/kingburrito666\/shakespeare-plays) by [LiamLarsen](https:\/\/www.kaggle.com\/kingburrito666) to create a deep learning model which will be able to `generate text` using some input text.\n\n**While doing all of this we will go through:**\n- `Preprocessing` text data\n- Building multilayer `Bidirectional RNN` model\n- Saving `wording embeddings` learned by the learning algorithm","0ae18de9":"## \ud83c\udf81 Saving the model","198c4dea":"## \u2708\ufe0f Getting data\n\nActual  word of text file is `4583798`","39cc02b5":"## \ud83c\udf6d Data preparation","6bc80ad8":"**\ud83d\udd76 Data visulaization**\n\n![](https:\/\/media.giphy.com\/media\/TEcW5rhdWYZsdQLHKW\/giphy.gif)","7576a47b":"---","1958b307":"Splitting our text data into `corpus` which in turn will be our `training samples`.","50b9424e":"Removing sentences whose length is `1`.","0ce5b64a":"If this kernel helped you then don't forget to \ud83d\udc4d `like` and give your \ud83c\udf99 `feedback`.\n\n![](https:\/\/media.giphy.com\/media\/oveqQA2LxpwYg\/giphy.gif)\n\n---"}}