{"cell_type":{"2ba0b053":"code","49efc02a":"code","d5faffed":"code","3369b9a5":"code","684d2c4f":"code","0ed28e58":"code","ec7ad7e7":"code","315b8b19":"code","06648bb5":"code","01e3ecfa":"code","bee3285b":"code","4ae04e01":"code","3f2f2332":"code","5652da8b":"code","93a7506d":"code","27f629f1":"code","0cc82011":"code","b0c09aa5":"code","b221bb1c":"code","b351a89b":"code","1f62a128":"code","368aee54":"code","c2214db3":"code","08bb2da2":"code","710d7f6f":"code","4e20b195":"code","d5e69288":"code","c8107da0":"code","6c5404d3":"code","6257e1ae":"code","f65421d3":"code","d3332915":"code","3081a574":"code","455cef4e":"code","7496506e":"code","216e3e15":"markdown","d607d109":"markdown","09c12b34":"markdown","6b081a18":"markdown","73dd2f70":"markdown","950a46a4":"markdown","ab4548f9":"markdown","f2700e0f":"markdown","f458626d":"markdown","13e21ad1":"markdown","d48924f4":"markdown"},"source":{"2ba0b053":"\n%matplotlib inline\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\nnp.random.seed(19)","49efc02a":"data_folder = \"..\/input\"\n#data_folder = \"data\"\ndata = pd.read_csv(os.path.join(data_folder, \"mushrooms.csv\"), header=None)\n","d5faffed":"data.head()","3369b9a5":"data[0] = data.apply(lambda row: 0 if row[0]=='e' else 1, axis=1) ","684d2c4f":"# \u6bcf\u4e00\u5217\u5982\u679c\u6709null\uff0c\u7528\"missing\"\u4ee3\u66ff\ncols = np.arange(1,23)\nfor col in cols:\n    if np.any(data[col].isnull()):\n        data.loc[data[col].isnull(), col] = 'missing'","0ed28e58":"labelEncoders = dict()\n\n# for each column, label encoding \nfor col in cols:\n    encoder = LabelEncoder()\n    values = data[col].tolist()\n    values.append('missing')  #\u52a0\u5165missing\u8fd9\u79cd\u503c\n    encoder.fit(values)\n    labelEncoders[col] = encoder\n    \n# \u8ba1\u7b97label encoding\u4e4b\u540e\u7684\u5217\u6570\ndimensionality = 0\nfor col, encoder in labelEncoders.items():\n    dimensionality += len(encoder.classes_) \nprint(\"dimensionality:  %d\" % (dimensionality))","ec7ad7e7":"# \u7528\u4e8e\u6d4b\u8bd5\u6570\u636e\u7684\u53d8\u6362, one-hot-encoding\ndef transform(df):\n    N, _ = df.shape\n    X = np.zeros((N, dimensionality))\n    i = 0\n    for col ,encoder in labelEncoders.items():\n        k = len(encoder.classes_)\n        X[np.arange(N), encoder.transform(df[col]) + i] = 1\n        i += k\n        \n    return X","315b8b19":"X = transform(data)\nY = data[0].as_matrix()  # label","06648bb5":"logistic_model = LogisticRegression() \nprint(\"logistic Regression performance: %f\" % (cross_val_score(logistic_model, X, Y, cv=8).mean()))","01e3ecfa":"tree_model = DecisionTreeClassifier()\nprint(\"Decision Tree performance: %f\" % (cross_val_score(tree_model, X, Y, cv=8).mean()))","bee3285b":"forest = RandomForestClassifier(n_estimators=20)\nprint(\"Random Forest performance: %f\" % (cross_val_score(tree_model, X, Y, cv=8).mean()))","4ae04e01":"from sklearn.base import BaseEstimator\nclass FakeRandomForest(BaseEstimator):\n    \n    def __init__(self, M):\n        self.M = M\n        \n    def fit(self, X, Y, n_features=None):\n        N,D = X.shape\n        if n_features is None:\n            #  # of features\n            n_features = int(np.sqrt(D))\n        \n        self.models = []\n\n        self.features = []\n        \n        for m in range(self.M):\n            tree = DecisionTreeClassifier()\n            \n            # \u6709\u653e\u56de\u7684\u968f\u673a\u62bd\u53d6 N\u4e2a\u6570\u636e\n            idx = np.random.choice(N, size=N, replace=True)\n            X_current = X[idx]\n            Y_current = Y[idx]\n            \n            # randomly get n_features, \u4e0d\u9700\u8981\u653e\u56de replace=False  <-- diff from bagging\n            features = np.random.choice(D, size=n_features, replace=False)\n            \n            #\u8bad\u7ec3\u5f53\u524d\u7684\u51b3\u7b56\u6811\u6a21\u578b\n            tree.fit(X_current[:, features], Y_current)\n            self.features.append(features)\n            self.models.append(tree)\n            \n    \n    def predict(self, X):\n        N = len(X)\n        results = np.zeros(N) \n        for features, tree in zip(self.features, self.models):\n            results += tree.predict(X[:, features])\n        return np.round(results\/ self.M)\n    \n    def score(self, X, Y):\n        prediction = self.predict(X)\n        return np.mean(prediction == Y)    \n            \n            ","3f2f2332":"class BaggedTreeClassifier(BaseEstimator):\n    def __init__(self, M):\n        self.M = M\n\n    def fit(self, X, Y):\n        N = len(X)\n        self.models = []\n        for m in range(self.M):\n            idx = np.random.choice(N, size=N, replace=True)\n            Xb = X[idx]\n            Yb = Y[idx]\n\n            model = DecisionTreeClassifier(max_depth=2)\n            model.fit(Xb, Yb)\n            self.models.append(model)\n\n    def predict(self, X):\n        # no need to keep a dictionary since we are doing binary classification\n        predictions = np.zeros(len(X))\n        for model in self.models:\n            predictions += model.predict(X)\n        return np.round(predictions \/ self.M)\n\n    def score(self, X, Y):\n        P = self.predict(X)\n        return np.mean(Y == P)","5652da8b":"baggedtc = BaggedTreeClassifier(20)","93a7506d":"cross_val_score(baggedtc, X, Y, cv=8).mean()","27f629f1":"fakerf = FakeRandomForest(20)","0cc82011":"cross_val_score(fakerf, X, Y, cv=8).mean()","b0c09aa5":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression","b221bb1c":"house_data = pd.read_csv(os.path.join(data_folder, \"kc_house_data.csv\"))","b351a89b":"house_data.head()","1f62a128":"house_data.columns","368aee54":"# price is the target\nNUMERICAL_COLS = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above','sqft_basement',\n                 'sqft_living15', 'sqft_lot15']","c2214db3":"# fit \u6bcf\u4e00\u5217\u6570\u636e\u7684 scaler\n# n\u4e2a\u6570reshape(-1, 1)\u5c31\u662fn\u884c1\u5217\u7684\u5217\u5411\u91cf\uff0creshape(1,-1)\u5c31\u662f1\u884cn\u5217\u7684\u884c\u5411\u91cf\nscalers = dict()\nfor col in NUMERICAL_COLS:\n    scaler = StandardScaler()\n    scaler.fit(house_data[col].as_matrix().astype(np.float64).reshape(-1,1))\n    scalers[col] = scaler\n    ","08bb2da2":"def transform_numerical(df):\n    N, _ = df.shape\n    D = len(NUMERICAL_COLS)\n    result = np.zeros((N,D))\n    i = 0\n    for col, scaler in scalers.items():\n        result[:, i] = scaler.transform(df[col].as_matrix().astype(np.float64).reshape(1,-1))\n        i += 1\n    return result    \n","710d7f6f":"from sklearn.model_selection import train_test_split","4e20b195":"hdata = transform_numerical(house_data)  # transform your data","d5e69288":"train_data, test_data = train_test_split(hdata, test_size=0.2)  # train test split","c8107da0":"trainX, trainY = train_data[:, 1:], train_data[:, 0]  # train data and train target\ntestX, testY = test_data[:, 1:], test_data[:, 0] # test data and test target\n","6c5404d3":"rfregressor = RandomForestRegressor(n_estimators=100)\nrfregressor.fit(trainX, trainY)\npredictions = rfregressor.predict(testX)","6257e1ae":"plt.scatter(testY, predictions)\n\nplt.xlabel(\"target\")\nplt.ylabel(\"prediction\")\nymin = np.round(min(min(testY), min(predictions)))\nymax = np.ceil(max(max(testY), max(predictions)))\nr = range(int(ymin), int(ymax) + 1)\nplt.plot(r,r)\nplt.show()","f65421d3":"# another way to plot for comparison, the more overlapping, the better.\nplt.plot(testY, label='targets')\nplt.plot(predictions, label='predictions')\nplt.legend()\nplt.show()","d3332915":"lr = LinearRegression() \nprint(\"linear regression performance: %f\" % (cross_val_score(lr, trainX, trainY).mean()))","3081a574":"print(\"random forest regressor performance: %f\" % (cross_val_score(rfregressor, trainX, trainY).mean())) ","455cef4e":"lr.fit(trainX, trainY)\nprint(\"linear regression test score: %f\" % (lr.score(testX, testY)))","7496506e":"rfregressor.fit(trainX, trainY)\nprint(\"random forest regressor test score: %f\" % (rfregressor.score(testX, testY)))","216e3e15":"### Labeling","d607d109":"### Prediction ","09c12b34":"### Use decision tree to implement regression  \uff08Optional\uff09\n\n#### Predict Housing Price\n- Dataset\uff1a https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction\/data  \n- Model\uff1a Random Forest, LR","6b081a18":"## Classify poison mushroom using Random Forest \n\n- Dataset\uff1a https:\/\/www.kaggle.com\/uciml\/mushroom-classification\/data  \n- Model comparison\uff1a Random Forest\uff0cDecision Tree, Logistic Regression","73dd2f70":"### Import dataset","950a46a4":"### Random Forest","ab4548f9":"### Bagging decision tree","f2700e0f":"### Fake Random Forest","f458626d":"### Decision Tree","13e21ad1":"### Logistic regression","d48924f4":"### Dataset analysis"}}