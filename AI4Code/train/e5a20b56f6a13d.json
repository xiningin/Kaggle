{"cell_type":{"32d0b1e9":"code","c0076eae":"code","2f0805d2":"code","aba2cae1":"code","dac9d678":"code","5b65a3b0":"code","de22d9af":"code","1105a857":"code","f970bd04":"code","7537e778":"code","511f2517":"code","f29114cf":"code","cc8067bf":"code","b7f381a8":"code","0ea3edcc":"code","e5703ba4":"code","6a1e75e7":"code","7322b738":"code","ae9e06a0":"markdown","875c6520":"markdown","9ffd0ad5":"markdown","85c93938":"markdown","fe4bfb07":"markdown","17478a6e":"markdown","7028ae14":"markdown","f0251f12":"markdown","0aee895e":"markdown","15e5c086":"markdown","93dea90e":"markdown","8b67457c":"markdown","92636c23":"markdown","1b74babb":"markdown","5a8194f7":"markdown"},"source":{"32d0b1e9":"import numpy as np\nimport pandas as pd\nimport gensim\nimport nltk\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\ndata = pd.read_csv('..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv')\ndata.head()","c0076eae":"# We'll write a function which will clean the text and prepare it.\ndef cleanText(text):\n    cleaned = re.sub(\"[^a-zA-Z0-9']\",\" \",text)\n    lowered = cleaned.lower()\n    return lowered.strip()\n\ncleanText(\"Let's test our function, by writing this string!\")","2f0805d2":"x,y = np.asarray(data[\"Message\"]),np.asarray(data[\"Category\"])\n\nx_cleaned = [cleanText(t) for t in x]\nx_cleaned[:4]","aba2cae1":"# Also we should convert our categories to the integer labels\nlabel_map = {cat:index for index,cat in enumerate(np.unique(y))}\ny_prep = np.asarray([label_map[l] for l in y])\n\nlabel_map","dac9d678":"x_tokenized = [[w for w in sentence.split(\" \") if w != \"\"] for sentence in x_cleaned]\nx_tokenized[0]","5b65a3b0":"# Now we'll create our model \nimport time\n\nstart = time.time()\n\nmodel = gensim.models.Word2Vec(x_tokenized,\n                 vector_size=100\n                 # Size is the length of our vector.\n                )\n\nend = round(time.time()-start,2)\nprint(\"This process took\",end,\"seconds.\")\n","de22d9af":"model.wv.most_similar(\"free\")","1105a857":"class Sequencer():\n    \n    def __init__(self,\n                 all_words,\n                 max_words,\n                 seq_len,\n                 embedding_matrix\n                ):\n        \n        self.seq_len = seq_len\n        self.embed_matrix = embedding_matrix\n        \"\"\"\n        temp_vocab = Vocab which has all the unique words\n        self.vocab = Our last vocab which has only most used N words.\n    \n        \"\"\"\n        temp_vocab = list(set(all_words))\n        self.vocab = []\n        self.word_cnts = {}\n        \"\"\"\n        Now we'll create a hash map (dict) which includes words and their occurencies\n        \"\"\"\n        for word in temp_vocab:\n            # 0 does not have a meaning, you can add the word to the list\n            # or something different.\n            count = len([0 for w in all_words if w == word])\n            self.word_cnts[word] = count\n            counts = list(self.word_cnts.values())\n            indexes = list(range(len(counts)))\n        \n        # Now we'll sort counts and while sorting them also will sort indexes.\n        # We'll use those indexes to find most used N word.\n        cnt = 0\n        while cnt + 1 != len(counts):\n            cnt = 0\n            for i in range(len(counts)-1):\n                if counts[i] < counts[i+1]:\n                    counts[i+1],counts[i] = counts[i],counts[i+1]\n                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]\n                else:\n                    cnt += 1\n        \n        for ind in indexes[:max_words]:\n            self.vocab.append(temp_vocab[ind])\n                    \n    def textToVector(self,text):\n        # First we need to split the text into its tokens and learn the length\n        # If length is shorter than the max len we'll add some spaces (100D vectors which has only zero values)\n        # If it's longer than the max len we'll trim from the end.\n        tokens = text.split()\n        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1\n        vec = []\n        for tok in tokens[:len_v]:\n            try:\n                vec.append(self.embed_matrix[tok])\n            except Exception as E:\n                pass\n        \n        last_pieces = self.seq_len - len(vec)\n        for i in range(last_pieces):\n            vec.append(np.zeros(100,))\n        \n        return np.asarray(vec).flatten()\n                \n                \n            \n        ","f970bd04":"sequencer = Sequencer(all_words = [token for seq in x_tokenized for token in seq],\n              max_words = 1200,\n              seq_len = 15,\n              embedding_matrix = model.wv\n             )","7537e778":"test_vec = sequencer.textToVector(\"i am in love with you\")\ntest_vec","511f2517":"test_vec.shape","f29114cf":"# But before creating a PCA model using scikit-learn let's create\n# vectors for our each vector\nx_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_tokenized])\nprint(x_vecs.shape)\n","cc8067bf":"from sklearn.decomposition import PCA\npca_model = PCA(n_components=50)\npca_model.fit(x_vecs)\nprint(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))","b7f381a8":"x_comps = pca_model.transform(x_vecs)\nx_comps.shape","0ea3edcc":"x_train,x_test,y_train,y_test = train_test_split(x_comps,y_prep,test_size=0.2,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n","e5703ba4":"start = time.time() \n\nsvm_classifier = SVC()\nsvm_classifier.fit(x_train,y_train)\n\nend = time.time()\nprocess = round(end-start,2)\nprint(\"Support Vector Machine Classifier has fitted, this process took {} seconds\".format(process))\n","6a1e75e7":"svm_classifier.score(x_test,y_test)","7322b738":"# More algorithms!!!!\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB\n\nrfc = RandomForestClassifier()\nrfc.fit(x_train,y_train)\nprint(\"Score of RFC\",rfc.score(x_test,y_test))\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\nprint(\"Score of LogReg\",logreg.score(x_test,y_test))\n\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)\nprint(\"Score of GaussianNB\",gnb.score(x_test,y_test))\n\nbnb = BernoulliNB()\nbnb.fit(x_train,y_train)\nprint(\"Score of BernoulliNB\",bnb.score(x_test,y_test))","ae9e06a0":"* We've got the best results using Random Forest Classifier, also Logistic Regression and Bernoulli Naive Bayes bringed nice results.\n","875c6520":"* Our accuracy is %94. Looks nice, but before finishing this kernel, let's check more machine learning algorithm.","9ffd0ad5":"* We've said we want 50 components and with those 50 component we can protect our data's %99. \n* Stats look nice, so let's use transform function and reduce dimension.\n","85c93938":"# PCA (Principal Component Analysis)","fe4bfb07":"* Our class is ready, let's take a last look at that.\n    1. In constructor function our class takes 4 parameters: all_words,max_words,seq_length,embedding_matrix\n        * All Words = This means give your all dataset in a list format which contains all tokens (not list of lists (sentences) concatenate all the sentences).\n        * Max Words = If your dataset has a lot of unique words you might want to limit the number of words. This parameter will be used in finding most used N (max_words) word.\n        * Sequence Length = In machine learning our dataset's number of variable has to be specified. But in real life each sentence might has a different length. In order to prevent this problem we'll determine a length and adapt our sentences to that length.","17478a6e":"# Support Vector Machine Classifier\nIn this section we're going to create and train our Support Vector Machine classifier. We'll use scikit-learn library and it'll ease our jobs a lot.\n \n","7028ae14":"# Writing A Class To Create Sequences\nOur model is ready, but we need a class to convert texts to create word embedding sequences","f0251f12":"# Data Preprocessing","0aee895e":"Everything looks fine, but as you see each vector for a sentence has 1500 elements and it'll consume a lot of time to train a Support Vector Machine Classifier on this.\n\nIn order to prevent this problem, we'll use the power of Statistics, Principal Component Analysis.\nPrincipal Component Analysis is a way to reduce dimension of vectors. It maximizes the variance and creates N components.\n","15e5c086":"* Everything is nice about the data, let's split our data to train and test set.","93dea90e":"# Training Word Embeddings\nUntil here I did not explain code so much with markdowns but the thing starts here so let's explain what will we do in this section.\n\nIn this section we're going to train word embeddings using our data and in order to train those word vectors we'll tokenize our data.\n\nTokenizing basically means splitting sentences into words (e.g. You are nice => [\"You\",\"are\",\"nice\"])\nThen we'll train our model using gensim. I won't get into details of word2vec but you should learn them because if you don't know how they work it's meaningless to use it\n","8b67457c":"# Preparing Environment","92636c23":"* Hey, it looks like our principal component analysis solution worked, let's check the accuracy.","1b74babb":"# Introduction: Word Embeddings with SVM \nHello people, welcome to this kernel. In this kernel I'm going to show you how to use word embeddings with traditional machine learning models like Support Vector Machine. We'll use word2vec to train word vectors, scikit-learn to train SVM.\n\nBefore starting let's take a look at some topics\n\n### Why Word Vectors\/Embeddings\nIn natural language processing, we have to make words understandable for computer. There are several ways to do this and in order to understand why word embeddings, let's examine a more primitive way, one hot encoding.\n\nIn one hot encoding technique, we have to create a one hot vector (a vector which has only one 1 value, also others have to be 0) which has a length number of the words we have in our vocab.\n\nSuch as let's make an example. We have a vocab like this => {\"My\",\"Mine\",\"Turkey\",\"Nice\"} So our each vector for a word will 4D.\n\nLet's check the vectors of the words:\nMy => [1,0,0,0]\nMine => [0,1,0,0]\nTurkey => [0,0,1,0]\nNice => [0,0,0,1]\n\nBut if we compute the distance between my&mine and Turkey&mine we can see the distances are same and this means **we could not protect the real relationships between the words**\n\nSo let's briefly explain the disadvantages of one hot encoding\n\n**Disadvantages of One Hot Encoding**:\n* In big vocabs vectors for the words are insanely big and this causes memory problems\n* Most of the elements of the vector don't have a meaning.\n* It's impossible to protect the relation between the words because each vector's distance is same.\n\nSo we need a better way to do this and we have this: **word embeddings!**\n\n### Word Embeddings\nBefore implementing it with a Support Vector Machine let's understand what are they. In word embeddings each element of vector is a different number. I said in **one hot encoding** most of the elements of vector don't have a meaning but word in word embeddings they have a meaning.\n\nLet's make an example.\n\nWe have a vocab like = {\"My,\"Mine\",\"Your\",\"Turkey\"}\n\nTheir vectors might like this:\n\nMy = [1.12,1.42.1,45.1,52]\nMine = [1.14,1.40,1.47,1.53]\nTurkey = [3.132,4.12312,6,123,7.123]\n\n\nAs you see if we compute the distance between *my* and *mine* it's closer than *mine* and *turkey*.\n\nI can hear your questions: \"Of course, but how can we create this vectors?\".\nWe'll create this vectors by computing the probabilities of being words side by side.\n\nThis information is enough, let's start.","5a8194f7":"# Conclusion \nHey, we finished one more kernel and I feel really excited. If you liked this kernel, please upvote. It motivates me a lot :)\n\nAlso if you have any question please ask me in the comment section by mentioning me. I'll return ASAP.\n\nHave a great day\/night.\n"}}