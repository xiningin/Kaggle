{"cell_type":{"ac2eca2f":"code","442349af":"code","aac09fb4":"code","8e525499":"code","d1e4340d":"code","35f110e6":"code","75884df1":"code","15f57ff5":"code","a09119c2":"code","6ecc5293":"code","dfbadf44":"code","d1236089":"code","d1b3dfe8":"code","c2f6e828":"code","54a46169":"code","8523ebe2":"code","4a450b50":"code","b5d7619b":"code","14ed2788":"code","9600a4d2":"markdown","44440545":"markdown","e34fbe79":"markdown","c1df333f":"markdown","41afaed3":"markdown","2aade11a":"markdown","ee582c61":"markdown","9febd0fe":"markdown"},"source":{"ac2eca2f":"# Installing bottleneck transformer library\n!pip install -q bottleneck-transformer-pytorch","442349af":"# Software library written for data manipulation and analysis.\nimport pandas as pd\n\n# Python library to interact with the file system.\nimport os\n\n# Python library for image augmentation\nimport albumentations as A\n\n# fastai library for computer vision tasks\nfrom fastai.vision.all import *\n\n# Developing and training neural network based deep learning models.\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet101\n\n# BotNet\nfrom bottleneck_transformer_pytorch import BottleStack","aac09fb4":"# Define path to dataset, whose benefit is that this sample is more balanced than original train data.\npath = Path('..\/input\/hpa-cell-tiles-sample-balanced-dataset')","8e525499":"df = pd.read_csv(path\/'cell_df.csv')\ndf.head()","d1e4340d":"# extract the the total number of target labels\nlabels = [str(i) for i in range(19)]\nfor x in labels: df[x] = df['image_labels'].apply(lambda r: int(x in r.split('|')))","35f110e6":"# Here a sample of the dataset has been taken, change frac to 1 to train the entire dataset!\ndfs = df.sample(frac=0.1, random_state=42)\ndfs = dfs.reset_index(drop=True)\nlen(dfs)","75884df1":"# obtain the input images.\ndef get_x(r): \n    return path\/'cells'\/(r['image_id']+'_'+str(r['cell_id'])+'.jpg')\n\n# obtain the targets.\ndef get_y(r): \n    return r['image_labels'].split('|')","15f57ff5":"'''AlbumentationsTransform will perform different transforms over both\n   the training and validation datasets ''' \nclass AlbumentationsTransform(RandTransform):\n    \n    '''split_idx is None, which allows for us to say when we're setting our split_idx.\n       We set an order to 2 which means any resize operations are done first before our new transform. '''\n    split_idx, order = None, 2\n    \n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    # Inherit from RandTransform, allows for us to set that split_idx in our before_call.\n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    # If split_idx is 0, run the trainining augmentation, otherwise run the validation augmentation. \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","a09119c2":"def get_train_aug(size): \n    \n    return A.Compose([\n            # allows to combine RandomCrop and RandomScale\n            A.RandomResizedCrop(size,size),\n            \n            # Transpose the input by swapping rows and columns.\n            A.Transpose(p=0.5),\n        \n            # Flip the input horizontally around the y-axis.\n            A.HorizontalFlip(p=0.5),\n        \n            # Flip the input horizontally around the x-axis.\n            A.VerticalFlip(p=0.5),\n        \n            # Randomly apply affine transforms: translate, scale and rotate the input.\n            A.ShiftScaleRotate(p=0.5),\n        \n            # Randomly change hue, saturation and value of the input image.\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        \n            # Randomly change brightness and contrast of the input image.\n            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n        \n            # CoarseDropout of the rectangular regions in the image.\n            A.CoarseDropout(p=0.5),\n        \n            # CoarseDropout of the square regions in the image.\n            A.Cutout(p=0.5) ])\n\ndef get_valid_aug(size): \n    \n    return A.Compose([\n    # Crop the central part of the input.   \n    A.CenterCrop(size, size, p=1.),\n    \n    # Resize the input to the given height and width.    \n    A.Resize(size,size)], p=1.)","6ecc5293":"'''The first step item_tfms resizes all the images to the same size (this happens on the CPU) \n   and then batch_tfms happens on the GPU for the entire batch of images. '''\n# Transforms we need to do for each image in the dataset\nitem_tfms = [Resize(224), AlbumentationsTransform(get_train_aug(224), get_valid_aug(224))]\n\n# Transforms that can take place on a batch of images\nbatch_tfms = [Normalize.from_stats(*imagenet_stats)]\n\nbs=6","dfbadf44":"dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock(vocab=labels)), # multi-label target\n                splitter=RandomSplitter(seed=42), # split data into training and validation subsets.\n                get_x=get_x, # obtain the input images.\n                get_y=get_y,  # obtain the targets.\n                item_tfms=item_tfms,\n                batch_tfms=batch_tfms\n                )\n\ndls = dblock.dataloaders(dfs, bs=bs)","d1236089":"# We can call show_batch() to see what a sample of a batch looks like.\ndls.show_batch()","d1b3dfe8":"layer = BottleStack(\n    # channels in\n    dim = 256, \n    \n    # feature map size\n    fmap_size = 56,  \n    \n    # channels out\n    dim_out = 2048, \n    \n    # projection factor\n    proj_factor = 4,\n    \n    # downsample on first layer or not\n    downsample = True, \n    \n    # number of heads\n    heads = 4, \n    \n    # dimension per head, defaults to 128\n    dim_head = 128,    \n    \n    # use relative positional embedding - uses absolute if False\n    rel_pos_emb = False, \n    \n    # activation throughout the network\n    activation = nn.ReLU()  \n)","c2f6e828":"# define the backbone architecture\nresnet = resnet101()\n\n# extract the backbone layers\nbackbone = list(resnet.children())","54a46169":"# define the model architecture for BotNet\nmodel = nn.Sequential(*backbone[:5],\n                      layer,\n                      nn.AdaptiveAvgPool2d((1, 1)),\n                      nn.Flatten(1),\n                      nn.Linear(2048, 19))","8523ebe2":"# Group together some dls, a model, and metrics to handle training\nlearn = Learner(dls, model, metrics= accuracy_multi)","4a450b50":"# Choosing a good learning rate\nlearn.lr_find()","b5d7619b":"# We can use the fine_tune function to train a model with this given learning rate\nlearn.fine_tune(4,0.0008317637839354575)","14ed2788":"# Plot training and validation losses.\nlearn.recorder.plot_loss()","9600a4d2":"# *Bottleneck Transformer: SOTA Visual Recognition model with Convolution + Attention that outperforms EfficientNet and DeiT in terms of performance-computes trade-off!*","44440545":"# Data Loading\n\n## Major credits to [Darek K\u0142eczek](https:\/\/www.kaggle.com\/thedrcat) for providing this dataset!","e34fbe79":"# Fetch the required libraries","c1df333f":"# Data Preprocessing","41afaed3":"![](https:\/\/i0.wp.com\/syncedreview.com\/wp-content\/uploads\/2021\/02\/image-15-1.png?resize=950%2C402&ssl=1)\n\n### This image represents a taxonomy of deep learning architectures using self-attention for visual recognition. \n### The proposed architecture: BoTNet is a hybrid model that uses both convolutions and self-attention. The specific implementation of self-attention could either resemble a Transformer block or a Non-Local block. \n\n### BoTNet is different from architectures such as DETR, VideoBERT, VILBERT, CCNet , etc by employing self-attention within the backbone architecture, in contrast to using them outside the backbone architecture. Being a hybrid model, BoTNet differs from pure attention models such as SASA , LRNet, SANet, Axial-SASA and ViT.","2aade11a":"## *Upvote the notebook if you find it insightful!* \ud83d\ude01","ee582c61":"![](https:\/\/paperswithcode.com\/media\/social-images\/GLkLywUAukspnNsa.png)\n\n### Here is Bottleneck Transformer or just simply BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. \n\n### By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, This approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. \n\n### Through the design of BoTNet, ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks.\n\n### To find out more :- https:\/\/arxiv.org\/abs\/2101.11605","9febd0fe":"# Model Definition"}}