{"cell_type":{"3de73c2c":"code","a65efba0":"code","2e73105a":"code","9ab7c0af":"code","44175651":"code","9660098d":"code","f47cb653":"code","9e8fba4c":"code","e282b9d6":"code","1d3af56e":"code","6c5f7d1e":"code","a3c414e1":"code","f2d85fca":"code","33a5443f":"markdown","0a25c662":"markdown"},"source":{"3de73c2c":"from fastai.vision.all import *\nfrom tqdm.notebook import  tqdm\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn import model_selection\nfrom tensorflow import keras\nfrom keras import backend as K\nimport tensorflow as tf\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport memory_profiler\nm1 = memory_profiler.memory_usage()\nPATH = Path('..\/input\/optiver-realized-volatility-prediction')\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/book_train.parquet'","a65efba0":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","2e73105a":"def fix_offsets(data_df):\n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    return data_df","9ab7c0af":"def ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()","44175651":"means = np.array([-0.0003,0.0003,0.077,0.0766,\n                  -0.00052,0.0005,0.0959,0.0928,\n                    6.85e-06,5.54e-06,6.44e-06,-6e-08,\n                  0.000663,0.000197,\n                    -0.0002\n                 ]).astype('float16')\n\nstds = np.array([0.00317,0.00317, 5.3541e-01, 4.9549e-01,\n                 0.00317,0.00317, 6.6838e-01, 5.7353e-01,\n                0.003,0.003,0.003,0.0003,\n                 0.0007,0.00024,\n                 0.00024\n                ]).astype('float16')","9660098d":"def load_data(fname):\n    data = pd.read_parquet(fname)\n    stock_id = str(fname).split('=')[1]\n    time_ids = data.time_id.unique()\n    row_ids = list(map(lambda x:f'{stock_id}-{x}', time_ids))\n    data = fix_offsets(data)\n    data = ffill(data)\n    \n    # Take every 4th second\n    data = data.iloc[::4]\n    \n    data['wap1'] = (data['bid_price1'] * data['ask_size1'] +\n                                            data['ask_price1'] * data['bid_size1']) \/ (data['bid_size1'] + data['ask_size1'])\n    data['wap2'] = (data['bid_price2'] * data['ask_size2'] +\n                                    data['ask_price2'] * data['bid_size2']) \/ (data['bid_size2'] + data['ask_size2'])\n    data['wap'] = data['wap1']*0.7+data['wap2']*0.3\n\n    \n    data['log_return'] = data.groupby(by = ['time_id'])['wap'].apply(log_return).fillna(0)\n    \n    data['price_spread'] = (data['ask_price1'] - data['bid_price1']) \/ ((data['ask_price1'] + data['bid_price1']) \/ 2)\n    data['bid_spread'] = data['bid_price1'] - data['bid_price2']\n    data['ask_spread'] = data['ask_price1'] - data['ask_price2']\n\n    # manipulating data to fit more accurate data into float16 type\n    for i in ['bid_price1', 'ask_price1','bid_price2', 'ask_price2',\n              'wap1','wap2','wap'\n             ]:\n        data[i] = data[i]-1\n    \n    # manipulating data to fit more accurate data into float16 type\n    for i in ['bid_size1', 'ask_size1','bid_size2', 'ask_size2'\n             ]:\n        data[i] = data[i]\/10000\n        \n    \n    data = data[['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1',\n                 'bid_price2', 'ask_price2', 'bid_size2', 'ask_size2',\n                'wap1','wap2','wap','log_return',\n                 'price_spread','bid_spread','ask_spread'\n                ]]\n    \n    cols = len(data.columns)\n    data = data.to_numpy()\n    data = (data - means) \/ stds\n    data = data.reshape(len(time_ids),150,cols)\n   \n    return data, row_ids","f47cb653":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')","9e8fba4c":"# Creating our training datasets\n\nX = []\ny = []\nstocks = []\nfor i in os.listdir(data_dir)[:]:\n    print(i)\n    stockid = int(i.split('=')[-1])\n    d = load_data(os.path.join(data_dir,i))\n    \n    # convert to float16 to reduce memory usage\n    x_train = d[0].astype('float16')\n    y_train = list(train['target'][train['stock_id']==stockid])\n    stocks = stocks + [stockid]*len(x_train)\n    \n    if X==[]:\n        X = x_train \n        y = y_train\n    else:\n        X = np.concatenate((X, x_train))\n        y = np.concatenate((y, y_train))\n    print(X.shape)\n    del x_train\n    gc.collect()\n    \n    # keeping track of memory usage\n    m2 = memory_profiler.memory_usage()\n    print(m2[0] - m1[0])","e282b9d6":"scores_folds = {}\nhidden_units = [256,128,64,32,16,8]\nstock_embedding_size = 24\nnb_features = X.shape[-1]\n\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n\ndef base_model(window_size):\n    \n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(window_size,nb_features), name='num_data')\n    \n    stock_embedded = keras.layers.Embedding(127, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    \n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    \n    conv = keras.layers.Conv1D(32,  kernel_size = (5), padding='same', activation='relu', input_shape=(window_size, nb_features))(num_input)\n    for i in range(15):\n        conv = keras.layers.Conv1D(32,  kernel_size = (5), padding='same', activation='relu', input_shape=(window_size, nb_features))(conv)\n \n    conv = keras.layers.Flatten()(conv)\n    \n    out = keras.layers.Concatenate()([stock_flattened, conv])\n    \n    out = keras.layers.GaussianNoise(0.1)(out)\n    \n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, kernel_initializer='normal', activation='swish')(out)\n\n    out = keras.layers.Dense(1, activation='swish', name='prediction')(out)#bias_initializer=tf.keras.initializers.Constant(value = mean_target)\n\n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model\ngc.collect()\n#tf.keras.utils.plot_model(base_model(150), to_file='model.png', show_shapes=True)","1d3af56e":"model_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=1e-05, patience=6, verbose=1,\n    mode='min')\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=4, verbose=1, min_lr=5e-7,\n    mode='min')\n\n# nothing fancy for the k-fold split, there was little improvement in grouping the split by time_ids \n# we stop after single fold here, but i would encourage you to experiment with more \/ different splits\nfor dev_index, val_index in kf.split(range(len(X))):\n    \n    if counter>1:\n        break\n    \n    model = base_model(window_size=150)\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.0004), #0.0004\n        loss='mean_squared_error',\n        metrics=['MSE',root_mean_squared_per_error],\n    )\n    \n    num_data = X[dev_index]\n    cat_data = np.array(stocks)[dev_index]\n    \n    num_data_test = X[val_index]\n    cat_data_test = np.array(stocks)[val_index]\n    \n    model.fit([cat_data, num_data], \n              y[dev_index], \n              sample_weight = 1\/np.square(y[dev_index]),\n              batch_size=256,\n              epochs=100,\n              validation_data=([cat_data_test, num_data_test], y[val_index], 1\/np.square(y[val_index])),\n              callbacks=[es, plateau],\n              shuffle=True,\n             verbose = 1)\n    \n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    score = round(rmspe(y_true = y[val_index], y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    print(len(preds))\n    \n    counter += 1\ngc.collect()\n","6c5f7d1e":"def get_preds(data, model, stock_id):\n    preds = model.predict([np.array([stock_id]*len(data)),data]).reshape(1,-1)[0]\n    \n    return preds","a3c414e1":"test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\ndata_dir = PATH\/'book_test.parquet'\nif len(test)<4:\n    df_pred = pd.DataFrame()\n    df_pred['row_id'] = list(test['row_id'])\n    df_pred['target'] = [0,0,0]\n    df_pred\nelse:\n    all_preds = []\n    for fname in tqdm(data_dir.ls()[:]):\n\n        stock = int(str(fname).split('=')[-1])\n        print(stock)\n        \n        data, row_ids = load_data(fname)\n        preds = get_preds(data, model, stock)\n        df_pred = pd.DataFrame(zip(row_ids, preds.tolist()),columns=['row_id', 'target'])\n        all_preds.append(df_pred)\n        \n    df_pred = pd.concat(all_preds)","f2d85fca":"df_pred.to_csv('submission.csv', index=False)","33a5443f":"![model.png](attachment:7fc93b68-d1ac-4fef-ab15-da37c6208bb0.png)","0a25c662":"# Keras CNN Baseline with stock embedding\n\n#### A 1D CNN baseline with stock id embedding implemented in Keras.\n\n> Embedding idea from: https:\/\/www.kaggle.com\/colinmorris\/embedding-layers \/ \n https:\/\/www.kaggle.com\/lucasmorin\/tf-keras-nn-with-stock-embedding\n\n> Very useful preprocessing functions for CNN, and great CNN implementation:\nhttps:\/\/www.kaggle.com\/slawekbiel\/deep-learning-approach-with-a-cnn-inference\n\n\n#### The main differences here to previous CNN attempts are \n- stock embedding\n- extra features (wap, log return, bid \/ ask spread)\n- simple cnn architecture (dropout \/ pooling didnt seem to improve results)\n- no trade data, just book data\n- reduced image size, taking every 4th second\n\nI have also made an effort to manage memory usage so that the model can be trained without a generator.\n\n#### Further improvements to this notebook score can likely be made by \n- experimenting with more features \/ combinations of features (weaker features can weaken score so pick carefully)\n- experimenting with cnn architecture\n- use full image size (full 600 seconds per image)\n- including time_id averages as features\n\n> ### Currently the highest scoring public CNN.\n> *This is my first competition & first public notebook so upvote if you find it useful!*"}}