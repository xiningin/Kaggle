{"cell_type":{"b81499ea":"code","82ad7296":"code","2c94ce4e":"code","9370ab70":"code","9111bead":"code","383f10d3":"code","456fc04a":"code","1931d638":"code","eb6231e1":"code","48541460":"code","a52f0ac2":"code","d55c35c7":"code","a66d94fc":"code","fca73892":"code","5d7917c7":"code","61260e32":"code","5099edf0":"code","3454dd78":"code","65b0a2dc":"code","12cd0739":"code","f27e5128":"code","6ec3c5b8":"code","2cbdf423":"code","1e160263":"code","5a2d9dd3":"code","eb7050e9":"code","b89b86d5":"code","3b6c6cc7":"code","a4b4ab6f":"code","c94853c7":"code","ef144c3a":"code","16e00be6":"code","8372f97b":"code","61bfb64a":"markdown","80c016f5":"markdown","1e32d45a":"markdown","c21a2e01":"markdown","5f300ea1":"markdown"},"source":{"b81499ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82ad7296":"# ! pip install pyphen\n# !pip download ..\/input\/ pyphen \n!pip install \/kaggle\/input\/textstat\/Pyphen-0.10.0-py3-none-any.whl","2c94ce4e":"# ! pip install textstat\n# !pip download textstat\n!pip install \/kaggle\/input\/textstat\/textstat-0.7.0-py3-none-any.whl","9370ab70":"import pandas as pd\nimport numpy as np\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport json\nfrom collections import defaultdict\nimport pyphen\nimport textstat\nimport gc\n\ngc.enable()\nfrom IPython.display import clear_output\nfrom tqdm import tqdm","9111bead":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error \nfrom xgboost import XGBRegressor","383f10d3":"train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')","456fc04a":"# Calculate numberof syllables per word\ndic = pyphen.Pyphen(lang='en')\ndef getSylabblesperWord(word):\n    sylables = dic.inserted(train.excerpt[0]).count(\"-\") - word.count(\"-\")\n    return sylables \/ len(word.split(\" \"))","1931d638":"train_pre = train.copy()\ntrain_pre[\"Syllable_Ratio\"] = train_pre[\"excerpt\"].apply(getSylabblesperWord)","eb6231e1":"train_pre","48541460":"def get_readability_scores(df):\n    \n    flesch_score = []\n    smog_index = []\n    flesch_kincaid = []\n    coleman_liau = []\n    automated_readability = []\n    dale_chall_readability = []\n    difficult_w_ratio = []\n    linsear = []\n    gunning_fog = []\n    text_standard = []\n    fernandez_huerta = []\n    szigriszt_pazos = []\n    gutierrez_polini = []\n    crawford = []\n       \n    \n    \n    for sent in tqdm(df[\"excerpt\"]):\n        sent_len = len(sent.split(\" \"))\n        \n        \n        flesch_score.append(textstat.flesch_reading_ease(sent))\n        smog_index.append(textstat.smog_index(sent))\n        flesch_kincaid.append(textstat.flesch_kincaid_grade(sent))\n        coleman_liau.append(textstat.coleman_liau_index(sent))\n        automated_readability.append(textstat.automated_readability_index(sent))\n        dale_chall_readability.append(textstat.dale_chall_readability_score(sent))\n        difficult_w_ratio.append(textstat.difficult_words(sent)\/sent_len)\n        linsear.append(textstat.linsear_write_formula(sent))\n        gunning_fog.append(textstat.gunning_fog(sent))\n        text_standard.append(textstat.text_standard(sent))\n        fernandez_huerta.append(textstat.fernandez_huerta(sent))\n        szigriszt_pazos.append(textstat.szigriszt_pazos(sent))\n        gutierrez_polini.append(textstat.gutierrez_polini(sent))\n        crawford.append(textstat.crawford(sent))\n        \n    \n    flesch_score = np.array(flesch_score)\n    smog_index = np.array(smog_index)\n    flesch_kincaid = np.array(flesch_kincaid)\n    coleman_liau = np.array(coleman_liau)\n    automated_readability = np.array(automated_readability)\n    dale_chall_readability = np.array(dale_chall_readability)\n    difficult_w_ratio = np.array(difficult_w_ratio)\n    linsear = np.array(linsear)\n    gunning_fog = np.array(gunning_fog)\n    text_standard = np.array(text_standard)\n    fernandez_huerta = np.array(fernandez_huerta)\n    szigriszt_pazos = np.array(szigriszt_pazos)\n    gutierrez_polini = np.array(gutierrez_polini)\n    crawford = np.array(crawford)\n       \n    \n    return (flesch_score, smog_index, flesch_kincaid, coleman_liau, automated_readability, dale_chall_readability, \n            difficult_w_ratio, linsear, gunning_fog, text_standard, fernandez_huerta, szigriszt_pazos, gutierrez_polini,\n            crawford)","a52f0ac2":"(flesch_score, smog_index, flesch_kincaid, coleman_liau, automated_readability, dale_chall_readability, \n            difficult_w_ratio, linsear, gunning_fog, text_standard, fernandez_huerta, szigriszt_pazos, gutierrez_polini,\n            crawford) = get_readability_scores(train_pre)","d55c35c7":"train_pre[\"flesch_score\"] = flesch_score\ntrain_pre[\"smog_index\"] = smog_index\ntrain_pre[\"flesch_kincaid\"] = flesch_kincaid\ntrain_pre[\"coleman_liau\"] = coleman_liau\ntrain_pre[\"automated_readability\"] = automated_readability\ntrain_pre[\"dale_chall_readability\"] = dale_chall_readability\ntrain_pre[\"difficult_w_ratio\"] = difficult_w_ratio\ntrain_pre[\"linsear\"] = linsear\ntrain_pre[\"gunning_fog\"] = gunning_fog\ntrain_pre[\"text_standard\"] = text_standard\ntrain_pre[\"fernandez_huerta\"] = fernandez_huerta\ntrain_pre[\"szigriszt_pazos\"] = szigriszt_pazos\ntrain_pre[\"gutierrez_polini\"] = gutierrez_polini\ntrain_pre[\"crawford\"] = crawford","a66d94fc":"train_pre[\"num_words\"] = train_pre.excerpt.apply(lambda x : len(x.split( )))","fca73892":"sns.heatmap(train_pre.corr());","5d7917c7":"train_pre.columns","61260e32":"def model_input_pipeline(df):\n#     Feature Engineering\n    df[\"Syllable_Ratio\"] = df[\"excerpt\"].apply(getSylabblesperWord)\n    \n    (flesch_score, smog_index, flesch_kincaid, coleman_liau, automated_readability, dale_chall_readability, \n            difficult_w_ratio, linsear, gunning_fog, text_standard, fernandez_huerta, szigriszt_pazos, gutierrez_polini,\n            crawford) = get_readability_scores(df)\n    \n    df[\"flesch_score\"] = flesch_score\n    df[\"smog_index\"] = smog_index\n    df[\"flesch_kincaid\"] = flesch_kincaid\n    df[\"coleman_liau\"] = coleman_liau\n    df[\"automated_readability\"] = automated_readability\n    df[\"dale_chall_readability\"] = dale_chall_readability\n    df[\"difficult_w_ratio\"] = difficult_w_ratio\n    df[\"linsear\"] = linsear\n    df[\"gunning_fog\"] = gunning_fog\n    df[\"text_standard\"] = text_standard\n    df[\"fernandez_huerta\"] = fernandez_huerta\n    df[\"szigriszt_pazos\"] = szigriszt_pazos\n    df[\"gutierrez_polini\"] = gutierrez_polini\n    df[\"crawford\"] = crawford\n    \n    df[\"num_words\"] = df.excerpt.apply(lambda x : len(x.split( )))\n    \n    \n    df = pd.get_dummies(df, columns=[\"text_standard\"])\n#     [\"id\", \"target\", \"standard_error\", \"url_legal\", \"excerpt\", \"license\"]\n    if \"standard_error\" in df.columns:\n        del df[\"standard_error\"]\n    if \"licence\" in df.columns:\n        del df[\"license\"]\n        \n    return df\n    ","5099edf0":"data = model_input_pipeline(train)","3454dd78":"X = data.drop(columns=[\"id\", \"target\", \"url_legal\", \"excerpt\", \"license\"])\ny = data.target","65b0a2dc":"train_cols = X.columns","12cd0739":"poly = PolynomialFeatures(2)\nX = poly.fit_transform(X)","f27e5128":"train_X, test_X, train_y, test_y = train_test_split(X, y,\n                      test_size = 0.15, random_state = 123)","6ec3c5b8":"xgb_r = XGBRegressor( n_estimators = 15)\nxgb_r.fit(train_X, train_y)","2cbdf423":"pred = xgb_r.predict(test_X)","1e160263":"rmse = np.sqrt(mean_squared_error(test_y, pred))\nprint(\"RMSE : % f\" %(rmse))","5a2d9dd3":"test_pre=test.copy()","eb7050e9":"test_pre = model_input_pipeline(test_pre)","b89b86d5":"test_X = test_pre.drop(columns=[\"id\", \"url_legal\", \"excerpt\"])","3b6c6cc7":"# To ensure all dummy columns are vailable\ntest_X = test_X.reindex(columns = train_cols)","a4b4ab6f":"test_X = test_X.fillna(0)","c94853c7":"test_X = poly.fit_transform(test_X)","ef144c3a":"pred = xgb_r.predict(test_X)","16e00be6":"submission_df = pd.DataFrame()\n\nsubmission_df[\"id\"] = test_pre[\"id\"] \nsubmission_df[\"target\"] = pred","8372f97b":"submission_df.to_csv('\/kaggle\/working\/submission.csv', index=False)","61bfb64a":"## Feature Engineering","80c016f5":"## Importing Libraries","1e32d45a":"## Loading Data","c21a2e01":"### The Main Idea behind this notebook is the specific feature engineering techniques used.\n### This is an experimentation as to how much we can infer from the text without making custom embeddings.And Fitting an XGB Regressor with those extracted features.\n### I hope you canfindsomething useful :). Happy Coding!","5f300ea1":"## XGB Regressor"}}