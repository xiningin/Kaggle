{"cell_type":{"8ad9e6c9":"code","f8e7f2c9":"code","ea4a0a2f":"code","e5a702f7":"code","59066e74":"code","7c7df27e":"code","eda909e0":"code","365ff51e":"code","176e0a39":"code","2c01202c":"code","506105b5":"code","1c78c844":"code","cc280242":"code","aedbc6e8":"code","5b9c2ac8":"code","0104091f":"code","b1d342a6":"code","1f5efe47":"code","02c8a413":"code","3a9f0eae":"code","0f27412f":"code","6849c211":"code","ef37d8ac":"code","21aaefc4":"code","dd79fc35":"code","82aeb94f":"code","fe16d26b":"code","69a1fd86":"code","d41affe5":"code","04177cf3":"code","209f9e2a":"code","6411907a":"code","4fb36257":"code","31ec84b7":"code","1f26dd8b":"code","93578667":"code","c3924dd7":"code","c22b3dc3":"code","cee00578":"code","adbd0433":"code","a76bdffd":"code","a72126c7":"code","afbc4a87":"code","7c60a88a":"code","9ace8e2b":"code","6b7500dc":"markdown","77b9df0f":"markdown","0b502c20":"markdown","12989d67":"markdown","d919659a":"markdown","75aa6ad0":"markdown","9d85c060":"markdown","2a6c566f":"markdown","1f3902c8":"markdown","ba25f34b":"markdown","fc9b48fa":"markdown","ff6b3336":"markdown","4c572391":"markdown","7c2d11ff":"markdown","6b5bb033":"markdown"},"source":{"8ad9e6c9":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sn\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nimport re","f8e7f2c9":"File = pd.read_csv(\"..\/input\/leukemia-gene-expression-cumida\/Leukemia_GSE9476.csv\")\ndt = pd.DataFrame(File)","ea4a0a2f":"dt","e5a702f7":"from sklearn.utils import shuffle\ndt = shuffle(dt)\ndt","59066e74":"h = dt.iloc[:64, :20]\nfig = plt.figure(figsize = (15,15))\nax = fig.gca()\nh.hist(ax = ax)","7c7df27e":"fig = plt.figure(figsize = (12,6))\nax = fig.gca()\nh.plot.kde(ax = ax, bw_method=2)","eda909e0":"plt.figure(figsize = (30,10))\nsn.pairplot(dt, x_vars=['samples','1007_s_at', '1053_at','117_at','121_at'], y_vars=['type'], size=7, aspect=0.7)","365ff51e":"dt.type.value_counts()","176e0a39":"dt['type'] = dt['type'].astype('category').cat.codes\ndt.type.value_counts()","2c01202c":"dt.type=dt.type.astype(int)\ndict(dt.dtypes)","506105b5":"#check the number of rows and column\ndt.shape","1c78c844":"#drop colum with NUll entries\ndt = dt.dropna(axis=1)\n#drop rows with Null entries\ndt = dt.dropna()","cc280242":"#check the number of rows and column after drop th missing values\n\ndt.shape","aedbc6e8":"target = dt['type']\ntarget.shape","5b9c2ac8":"dt","0104091f":"#dataset visualization\nplt.figure(figsize = (15,15))\nax = dt.plot.bar(x='type', y='samples', rot=1)\n","b1d342a6":"c = dt.columns\nc = c.tolist()\nc.remove('type')","1f5efe47":"c.remove('samples')","02c8a413":"features = dt[c]","3a9f0eae":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20,stratify=target,random_state=42)","0f27412f":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)","6849c211":"print (X_train_smote.shape, y_train_smote.shape)\nprint (X_test.shape, y_test.shape)","ef37d8ac":"def fun(y_pred):\n    l = []\n    for i in y_pred:\n        res = list(map(int, re.findall('\\d', str(i))))\n        l.append(res[0])\n    return l\n","21aaefc4":"def graph(names, values):\n\n    plt.figure(figsize=(15, 6))\n\n    plt.subplot(131)\n    plt.bar(names, values)\n    plt.subplot(132)\n    plt.scatter(names, values)\n    plt.subplot(133)\n    plt.plot(names, values)\n    plt.suptitle('Categorical Plotting')\n    plt.show()","dd79fc35":"reg = LinearRegression().fit(X_train_smote, y_train_smote)\ny_pred = reg.predict(X_test)\ny_pred = y_pred\n\n\nplt.scatter(y_test, y_pred.astype(int))\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")","82aeb94f":"print(\"Accuracy:\",metrics.accuracy_score(y_test, fun(y_pred)))","fe16d26b":"graph(y_test, fun(y_pred))","69a1fd86":"labels = ['0','1','2','3','4']","d41affe5":"print(classification_report(y_test.tolist(), fun(y_pred), ))","04177cf3":"confusion = confusion_matrix(y_test, fun(y_pred), labels=target)","209f9e2a":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train_smote, y_train_smote)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\nplt.scatter(y_test, y_pred. astype(int))\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\n\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","6411907a":"# Results\nprint(classification_report(y_test.tolist(), fun(y_pred), ))","4fb36257":"graph(y_test, fun(y_pred))","31ec84b7":"plt.figure(figsize = (10,10))\nax = plt.subplot()\nsn.heatmap(confusion, annot=True, ax = ax); #annot=True to annotate cells\n","1f26dd8b":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\n\nclf = clf.fit(X_train_smote, y_train_smote)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n\nplt.scatter(y_test, y_pred. astype(int))\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","93578667":"# Results\nprint(classification_report(y_test.tolist(), fun(y_pred), ))\n\ngraph(y_test, fun(y_pred))","c3924dd7":"from sklearn.ensemble import RandomForestClassifier\nclf = GaussianNB()\n# Train RandomForestClassifier\nclf = RandomForestClassifier(max_depth=10, random_state=0).fit(X_train_smote, y_train_smote)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n\nplt.scatter(y_test, y_pred. astype(int))\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")","c22b3dc3":"# Results\nprint(classification_report(y_test.tolist(), fun(y_pred), ))\n\ngraph(y_test, fun(y_pred))","cee00578":"from sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\n\nregr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n\nclf = regr.fit(X_train_smote, y_train_smote)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\nplt.scatter(y_test, y_pred. astype(int))\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")","adbd0433":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, fun(y_pred)))","a76bdffd":"# Results\nprint(classification_report(y_test.tolist(), fun(y_pred), ))\n\ngraph(y_test, fun(y_pred))","a72126c7":"from sklearn import ensemble\nparams = {'n_estimators': 500,\n          'max_depth': 4,\n          'min_samples_split': 5,\n          'learning_rate': 0.01,\n          'loss': 'ls'}\nreg = ensemble.GradientBoostingRegressor(**params)\nclf = reg.fit(X_train_smote, y_train_smote)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\nplt.scatter(y_test, y_pred. astype(int))\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")","afbc4a87":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, fun(y_pred)))","7c60a88a":"# Results\nprint(classification_report(y_test.tolist(), fun(y_pred), ))\n\ngraph(y_test, fun(y_pred))","9ace8e2b":"# importing libraries \nfrom sklearn.ensemble import VotingClassifier \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.svm import SVC \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.datasets import load_iris \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import train_test_split \n \nestimator = [] \nestimator.append(('LR',  \n                  LogisticRegression(multi_class='multinomial', solver='newton-cg', \n                                     max_iter = 200))) \nestimator.append(('SVC', SVC(gamma ='auto', probability = True))) \nestimator.append(('DTC', DecisionTreeClassifier())) \n  \n# Voting Classifier with hard voting \nvot_hard = VotingClassifier(estimators = estimator, voting ='hard') \nvot_hard.fit(X_train_smote, y_train_smote) \ny_pred = vot_hard.predict(X_test) \n  \n# using accuracy_score metric to predict accuracy \nscore = accuracy_score(y_test, y_pred) \n\nprint(\"++++++++++++++++++++++++++++++++++++++++++++++Hard Voting++++++++++++++++++++++++++++++++++++++++++++++\") \nprint(\"Hard Voting Accuracy  = \",score) \n\n# Results\nprint(classification_report(y_test.tolist(), fun(y_pred), ))    \ngraph(y_test, fun(y_pred))\n    \n    \n# Voting Classifier with soft voting \nvot_soft = VotingClassifier(estimators = estimator, voting ='soft') \nvot_soft.fit(X_train_smote, y_train_smote) \ny_pred = vot_soft.predict(X_test) \n\n\n\nprint(\"+++++++++++++++++++++++++++++++++++++++++++++++Soft Voting++++++++++++++++++++++++++++++++++++++++++++++\") \n  \n# using accuracy_score \nscore = accuracy_score(y_test, y_pred) \nprint(\"Soft Voting Accuracy  = \", score)\n\n\n# Results\nprint(classification_report(y_test.tolist(), fun(y_pred), ))    \ngraph(y_test, fun(y_pred)) ","6b7500dc":"# Decision Tree","77b9df0f":"### Visualization of Dataset in terms of Labels and Samples ","0b502c20":"# SVM","12989d67":"# Linear regression","d919659a":"## Uploading Leukemia_GSE9476 dataset that is in the CSV file format\n\n* gene expression levels of 22284 genes (columns) \n* 64 samples (rows)\n* 5 different types of leukemia\n\n","75aa6ad0":"### Dataset\n* Label = Type = subcategory of cancer Leukemia_GSE9476","9d85c060":"### Classification Report Shows Results in terms of :\n* Precision\n* Recall\n* F1-Score\n* Support","2a6c566f":"### Perform random shuffling ","1f3902c8":"### Train Test Split\n* 20% Test Data\n* 80% Training Data","ba25f34b":"# Random Forest","fc9b48fa":"# naive bayes","ff6b3336":"# Hybrid approach","4c572391":"### Visualization of Cancer genes according to classes  ","7c2d11ff":"### Drop NULL Values from the dataset","6b5bb033":"# Gradient Boosting"}}