{"cell_type":{"fbcf3d3d":"code","092640de":"code","4868ad38":"code","d6c7969b":"code","2a8b5324":"code","846a740f":"code","9be7ab98":"code","f5b31103":"code","9bcd9b2d":"code","c424048b":"code","45add0da":"code","e4278e15":"code","635c6b0a":"code","25040cbb":"code","3ad29843":"code","33629d00":"code","26cbd930":"code","3f8e514c":"code","b78b6d53":"code","6ecfc8ef":"code","eedda444":"code","5c44851a":"code","1b57aa6c":"markdown","c48879d4":"markdown","b11ba2c7":"markdown","d7cc7920":"markdown","689cc04c":"markdown","5689d565":"markdown","2d121a06":"markdown","3478e625":"markdown","6735692a":"markdown","8fccfe8e":"markdown","b2d61154":"markdown","af48842c":"markdown","40372147":"markdown","dee1427d":"markdown","8068ed28":"markdown","554e4aac":"markdown","daf8c9af":"markdown","cff38a3b":"markdown"},"source":{"fbcf3d3d":"# import the packages\/libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n","092640de":"# Load the data\ndf = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf.head(7)","4868ad38":"# Count the number of rows and columns in the datasets\ndf.shape","d6c7969b":"# Count the empty (NaN, NAH, na) values in each column\ndf.isna().sum()","2a8b5324":"# Drop the column with all missing values (na, NAN, NaN)\n# Note: The drops the columns Unnamed\ndf = df.dropna(axis=1)","846a740f":"#Get the new count of the number of rows and cols\ndf.shape","9be7ab98":"# Get a count of the number of 'M' & 'B' cells\ndf['diagnosis'].value_counts()","f5b31103":"# Visualize this count\nsns.countplot(df['diagnosis'], label = 'Count')","9bcd9b2d":"# Look at the data types\ndf.dtypes","c424048b":"# Encoding categorical data values\nlabelencoder_y = LabelEncoder()\ndf.iloc[:,1] = labelencoder_y.fit_transform(df.iloc[:,1].values)\nprint(labelencoder_y.fit_transform(df.iloc[:,1].values))","45add0da":"df['diagnosis']","e4278e15":"df.head()","635c6b0a":"# Get ehe correlation of the columns\ndf.corr().T","25040cbb":"plt.figure(figsize=(20,20))  \nsns.heatmap(df.corr(), annot=True, fmt='.0%')","3ad29843":"X = df.iloc[:, 2:31].values \nY = df.iloc[:, 1].values ","33629d00":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)\n","26cbd930":"#Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3f8e514c":"def models(X_train,Y_train):\n  \n  #Using Logistic Regression \n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train, Y_train)\n  \n  #Using KNeighborsClassifier \n  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n  knn.fit(X_train, Y_train)\n\n  #Using SVC linear\n  svc_lin = SVC(kernel = 'linear', random_state = 0)\n  svc_lin.fit(X_train, Y_train)\n\n  #Using SVC rbf\n  svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n  svc_rbf.fit(X_train, Y_train)\n\n  #Using GaussianNB \n  gauss = GaussianNB()\n  gauss.fit(X_train, Y_train)\n\n  #Using DecisionTreeClassifier \n  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n  tree.fit(X_train, Y_train)\n\n  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n  forest.fit(X_train, Y_train)\n  \n  #print model accuracy on the training data.\n  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train))\n  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train))\n  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train))\n  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train))\n  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train))\n  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train))\n  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train))\n  \n  return log, knn, svc_lin, svc_rbf, gauss, tree, forest","b78b6d53":"model = models(X_train,Y_train)\n","6ecfc8ef":"for i in range(len(model)):\n  cm = confusion_matrix(Y_test, model[i].predict(X_test))\n  \n  TN = cm[0][0]\n  TP = cm[1][1]\n  FN = cm[1][0]\n  FP = cm[0][1]\n  \n  print(cm)\n  print('Model[{}] Testing Accuracy = \"{}!\"'.format(i,  (TP + TN) \/ (TP + TN + FN + FP)))\n  print()# Print a new line","eedda444":"#Show other ways to get the classification accuracy & other metrics \nfor i in range(len(model)):\n  print('Model ',i)\n  #Check precision, recall, f1-score\n  print( classification_report(Y_test, model[i].predict(X_test)) )\n  #Another way to get the models accuracy on the test data\n  print( accuracy_score(Y_test, model[i].predict(X_test)))\n  print()#Print a new line","5c44851a":"#Print Prediction of Random Forest Classifier model\npred = model[6].predict(X_test)\nprint(pred)\n\n#Print a space\nprint()\n\n#Print the actual values\nprint(Y_test)\n","1b57aa6c":"- Create the model that contains all of the models, and look at the accuracy score on the training data for each model to classify if a patient has cancer or not.\nmodel","c48879d4":"- Get a count of the number of patients with Malignant (M) cancerous and Benign (B) non-cancerous cells.","b11ba2c7":"- Print the new data set which now has only 32 columns. Print only the first 5 rows.","d7cc7920":"- Encode the categorical data. Change the values in the column \u2018diagnosis\u2019 from M and B to 1 and 0 respectively, then print the results.","689cc04c":"- Create a function to hold many different models (e.g. Logistic Regression, Decision Tree Classifier, Random Forest Classifier) to make the classification. These are the models that will detect if a patient has cancer or not. Within this function I will also print the accuracy of each model on the training data.","5689d565":"- Get the new count of the number of rows and columns.\n","2d121a06":"- Split the data again, but this time into 75% training and 25% testing data sets.","3478e625":"- Explore the data and count the number of rows and columns in the data set. Their are 569 rows of data which means their are 569 patients in this data set, and 33 columns which mean their are 33 features or data points for each patient.","6735692a":"From the accuracy and metrics above, the model that performed the best on the test data was the Random Forest Classifier with an accuracy score of about 96.5%. So I will choose that model to detect cancer cells in patients. Make the prediction\/classification on the test data and show both the Random Forest Classifier model classification\/prediction and the actual values of the patient that shows rather or not they have cancer.\nI notice the model, misdiagnosed a few patients as having cancer when they didn\u2019t and it misdiagnosed patients that did have cancer as not having cancer. Although this model is good, when dealing with the lives of others I want this model to be better and get it\u2019s accuracy as close to 100% as possible or at least as good as if not better than doctors. So a little more tuning of each of the models is necessary.","8fccfe8e":"- Look at the data types to see which columns need to be transformed \/ encoded. I can see from the data types that all of the columns\/features are numbers except for the column \u2018diagnosis\u2019, which is categorical data represented as an object in python.","b2d61154":"- Other ways to get metrics on the model to see how well each one performed.","af48842c":"- Scale the data to bring all features to the same level of magnitude, which means the feature \/ independent data will be within a specific range for example 0\u2013100 or 0\u20131.","40372147":"- Remove the column \u2018Unnamed: 32\u2019 from the original data set since it adds no value.","dee1427d":"- Continue exploring the data and get a count of all of the columns that contain empty (NaN, NAN, na) values. Notice none of the columns contain any empty values except the column named \u2018Unnamed: 32\u2019 , which contains 569 empty values (the same number of rows in the data set, this tells me this column is completely useless)","8068ed28":"- Visualize the counts, by creating a count plot.\n","554e4aac":"Show the confusion matrix and the accuracy of the models on the test data. The confusion matrix tells us how many patients each model misdiagnosed (number of patients with cancer that were misdiagnosed as not having cancer a.k.a false negative, and the number of patients who did not have cancer that were misdiagnosed with having cancer a.k.a false positive) and the number of correct diagnosis, the true positives and true negatives.\n\n1. False Positive (FP) = A test result which incorrectly indicates that a particular condition or attribute is present.\n2. True Positive (TP) = Sensitivity (also called the true positive rate, or probability of detection in some fields) measures the proportion of actual positives that are correctly identified as such.\n3. True Negative (TN) = Specificity (also called the true negative rate) measures the proportion of actual negatives that are correctly identified as such.\n4. False Negative (FN) = A test result that indicates that a condition does not hold, while in fact it does. For example a test result that indicates a person does not have cancer when the person actually does have it\n","daf8c9af":"# This program detects breast caner, based off of data.","cff38a3b":"- Now I am done exploring and cleaning the data. I will set up my data for the model by first splitting the data set into a feature data set also known as the independent data set (X), and a target data set also known as the dependent data set (Y)."}}