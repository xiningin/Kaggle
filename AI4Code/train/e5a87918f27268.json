{"cell_type":{"bb99e26e":"code","50bb6a3c":"code","d553994c":"code","ad597bdf":"code","e64da3cd":"code","8f059387":"code","1c1838a0":"code","d0083f4e":"code","a176df07":"code","e6a92c46":"code","d8774cc3":"code","7dbe7c56":"code","f7998e40":"code","cd9214b7":"code","f33501f7":"code","c18eb1f3":"code","03282780":"code","186e4542":"code","34b9e390":"code","c2060f6e":"code","b0613d52":"code","4dd95427":"code","836b59ac":"code","1269d0aa":"code","c9452d11":"code","91fca214":"code","af4dccf0":"code","ff77f030":"markdown","c2c847d7":"markdown","5252ab60":"markdown","a877ec23":"markdown","9d1a0131":"markdown","4f76cd22":"markdown","103029ec":"markdown","0049d0e7":"markdown","ea423191":"markdown","054ef7be":"markdown","2189fc63":"markdown","d86b2099":"markdown","be8b438c":"markdown","84fea4d8":"markdown","59f28d9e":"markdown","5a88b2e3":"markdown","9f1240a3":"markdown"},"source":{"bb99e26e":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.resnet import ResNet101\nfrom tensorflow.keras.applications.resnet_v2 import ResNet101V2\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization,Convolution2D,MaxPooling2D,GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization \nfrom keras import optimizers\nimport tensorflow.keras\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping\n","50bb6a3c":"train=pd.read_csv('\/kaggle\/input\/classification-of-images\/dataset\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/classification-of-images\/dataset\/test.csv')\ntrain.head()","d553994c":"\nClass_map={'Food':0,'Attire':1,'Decorationandsignage':2,'misc':3}\ninverse_map={0:'Food',1:'Attire',2:'Decorationandsignage',3:'misc'}\ntrain['Class']=train['Class'].map(Class_map)","ad597bdf":"train['Class']","e64da3cd":"train_img=[]\ntrain_label=[]\nj=0\npath='\/kaggle\/input\/classification-of-images\/dataset\/Train Images'\nfor i in tqdm(train['Image']):\n    final_path=os.path.join(path,i)\n    img=cv2.imread(final_path)\n    img=cv2.resize(img,(150,150))\n    img=img.astype('float32')\n    train_img.append(img)\n    train_label.append(train['Class'][j])\n    j=j+1","8f059387":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.3, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(train_img)\n\n","1c1838a0":"test_img=[]\npath='\/kaggle\/input\/classification-of-images\/dataset\/Test Images'\nfor i in tqdm(test['Image']):\n    final_path=os.path.join(path,i)\n    img=cv2.imread(final_path)\n    img=cv2.resize(img,(150,150))\n    img=img.astype('float32')\n    test_img.append(img)","d0083f4e":"train_img=np.array(train_img)\ntest_img=np.array(test_img)\ntrain_label=np.array(train_label)\nprint(train_img.shape)\nprint(test_img.shape)\nprint(train_label.shape)","a176df07":"from keras import backend as K\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","e6a92c46":"from tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nsgd = SGD(lr=0.0001,momentum=0.9)","d8774cc3":"base_model=VGG16(include_top=False, weights='imagenet',input_shape=(150,150,3), pooling='avg')","7dbe7c56":"i=0\nfor layer in base_model.layers:\n    layer.trainable = False\n    i = i+1","f7998e40":"model=Sequential()\nmodel.add(base_model)\nmodel.add(Dense(128, activation='sigmoid'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(4,activation='softmax'))\n\n\n\n\nreduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n                                         factor=0.1,\n                                         patience=2,\n                                         cooldown=2,\n                                         min_lr=0.00001,\n                                         verbose=1)\n\ncallbacks = [reduce_learning_rate]\n   \n\n\nmodel.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=64),\n                    epochs=30)","cd9214b7":"model.save('vgg16_model.h5')","f33501f7":"model_3.save_weights('vgg16_weights_model3.h5')","c18eb1f3":"base_model=VGG16(include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')\nmodel_4 = Sequential()\nmodel_4.add(base_model)\nmodel_4.add(Dense(128,activation='relu'))\nmodel_4.add(Dropout(0.3))\nmodel_4.add(Dense(4,activation='softmax'))\n\nmodel_4.load_weights('vgg16_weights_model2.h5')\nmodel_4.compile( optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel_4.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=32),\n                    epochs=20)","03282780":"base_model=Xception(include_top=False, weights='imagenet',input_shape=(150,150,3), pooling='avg')\nfor layer in base_model.layers:\n    layer.trainable = False","186e4542":"model=Sequential()\nmodel.add(base_model)\nmodel.add(Dense(1024, activation='sigmoid'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(4,activation='softmax'))\n\n\nmodel.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=256),\n                    epochs=50)","34b9e390":"model.save_weights('Xception_weights_model.h5')","c2060f6e":"base_model=Xception(include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')\nmodel_4 = Sequential()\nmodel_4.add(base_model)\nmodel_4.add(Dense(1024,activation='sigmoid'))\nmodel_4.add(Dropout(0.4))\nmodel_4.add(Dense(4,activation='softmax'))\n\nmodel_4.load_weights('Xception_weights_model.h5')\nmodel_4.compile( optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel_4.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=32),\n                    epochs=20)","b0613d52":"base_model=VGG19(include_top=False, weights='imagenet',input_shape=(150,150,3), pooling='avg')\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dense(1024, activation='sigmoid'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(4,activation='softmax'))\n\n\nmodel.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=256),\n                    epochs=50)\n\nmodel.save_weights('vgg19_weights_model.h5')","4dd95427":"base_model=VGG19(include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')\nmodel_4 = Sequential()\nmodel_4.add(base_model)\nmodel_4.add(Dense(1024,activation='sigmoid'))\nmodel_4.add(Dropout(0.4))\nmodel_4.add(Dense(4,activation='softmax'))\n\nmodel_4.load_weights('vgg19_weights_model_afterft.h5')\nmodel_4.compile( optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel_4.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=32),\n                    epochs=20)","836b59ac":"base_model=ResNet101V2(include_top=False, weights='imagenet',input_shape=(150,150,3), pooling='avg')\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(4,activation='softmax'))\n\n\nmodel.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=256),\n                    epochs=50)\n\nmodel.save('resnet101_v2_model.h5')","1269d0aa":"# base_model=ResNet101(include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')\n# model_4 = Sequential()\n# model_4.add(base_model)\n# model_4.add(Dense(1024,activation='sigmoid'))\n# model_4.add(Dropout(0.4))\n# model_4.add(Dense(4,activation='softmax'))\ndep = {'f1_m':f1_m}\n\nmodel_4 = load_model('resnet101_model.h5',custom_objects=dep)\nmodel_4.compile( optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel_4.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=32),\n                    epochs=20)","c9452d11":"base_model=MobileNet(include_top=False, weights='imagenet',input_shape=(150,150,3), pooling='avg')\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dense(1024, activation='sigmoid'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(4,activation='softmax'))\nreduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n                                         factor=0.1,\n                                         patience=2,\n                                         cooldown=2,\n                                         min_lr=0.00001,\n                                         verbose=1)\n\ncallbacks = [reduce_learning_rate]\n\nmodel.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=32),callbacks=callbacks,\n                    epochs=20)\n\nmodel.save_weights('mobilenet_weights_model.h5')","91fca214":"base_model=VGG19(include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')\nmodel_4 = Sequential()\nmodel_4.add(base_model)\nmodel_4.add(Dense(1024,activation='sigmoid'))\nmodel_4.add(Dropout(0.4))\nmodel_4.add(Dense(4,activation='softmax'))\n\nmodel_4.load_weights('mobilenet_weights_model.h5')\nmodel_4.compile( optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',f1_m])\nmodel_4.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=32),\n                    epochs=20)","af4dccf0":"labels = model.predict(test_img)\nprint(labels[:4])\nlabel = [np.argmax(i) for i in labels]\nclass_label = [inverse_map[x] for x in label]\nprint(class_label[:3])\nsubmission = pd.DataFrame({ 'Image': test.Image, 'Class': class_label })\nsubmission.head(10)\nsubmission.to_csv('submission_mobilenet.csv', index=False)","ff77f030":"# Vgg16 model -- 81.5","c2c847d7":"### Define the F1 accuracy function","5252ab60":"**Now fine Tune with the saved weights**","a877ec23":"**Load Xception model and freeze all layers**","9d1a0131":"# MobileNet\/Inceptionv3 gives around 22-24...not very useful.","4f76cd22":"# Xception model -- 26.5","103029ec":"# convert categorical to numerical for training model","0049d0e7":"# Import required library","ea423191":"# VGG19 model -- 82.4","054ef7be":"**Freeze all pretrained layers**","2189fc63":"1. # Resnet101 (74.1) & Resnet101v2 (74.4)","d86b2099":"**Fine Tune the model using the previusly saved weights**","be8b438c":"**Now fine Tune with the previously saved weights**","84fea4d8":"# predict for test dataset","59f28d9e":"# train and test data set","5a88b2e3":"# Image agumentation","9f1240a3":"**Load Model**"}}