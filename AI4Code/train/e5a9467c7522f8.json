{"cell_type":{"b7c2dadb":"code","dc496a28":"code","fe1f4b0e":"code","4d49e081":"code","85f6c3aa":"code","8de0be88":"code","24805dff":"code","a25cbaa8":"code","e6dbbbc7":"code","53db894e":"code","24bbda74":"code","53551c51":"code","5c4091a0":"code","1254aa55":"code","68f7ca94":"code","1af231f7":"code","a8f02eac":"code","ab79aabf":"code","355b05f3":"code","7cf3e301":"code","a154793c":"code","ce10aaff":"markdown","75663438":"markdown","47d27d22":"markdown","ce64db9d":"markdown","ca686321":"markdown","20dc3c42":"markdown","3d3cd717":"markdown","79263f1e":"markdown","24e78fb7":"markdown","e22a6852":"markdown","99d4c538":"markdown"},"source":{"b7c2dadb":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nimport warnings\nimport time\nimport sys\nimport datetime\nfrom datetime import timedelta\nimport gc\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats.mstats import mode\nfrom functools import reduce\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option('display.max_columns', 500)\nimport os\nprint(os.listdir(\"..\/input\"))\n","dc496a28":"train = pd.read_csv(\"..\/input\/elo-merchant-category-recommendation\/train.csv\")\ntest = pd.read_csv(\"..\/input\/elo-merchant-category-recommendation\/test.csv\")\nhistory =pd.read_csv(\"..\/input\/elo-merchant-category-recommendation\/historical_transactions.csv\",parse_dates=['purchase_date'])\nnew =pd.read_csv(\"..\/input\/elo-merchant-category-recommendation\/new_merchant_transactions.csv\",parse_dates=['purchase_date'])\ncardreference = pd.read_csv(\"..\/input\/feature-engineering-on-multiple-reference-dates\/Cardreferencedate.csv\",parse_dates=['reference_date'])\n","fe1f4b0e":"history=history.loc[history.authorized_flag==\"Y\",]\nhistory.purchase_amount += 0.75\nnew.purchase_amount += 0.75","4d49e081":"cardrfm = history.groupby('card_id').agg({'card_id': 'count','purchase_date': 'max','purchase_amount': 'sum'})\ncardrfm.rename(columns={'card_id' : 'frequency','purchase_date': 'date_recency','purchase_amount': 'value'},inplace=True)\ncardrfm = pd.merge(cardrfm,cardreference.iloc[:,0:2],on='card_id',how='left')\ncardrfm['recency'] = cardrfm['reference_date'] - cardrfm['date_recency']\ncardrfm.recency= cardrfm.recency\/(24*np.timedelta64(1, 'h')) # to convert to day fractions\ncardrfm.drop(columns=['date_recency','reference_date'],inplace=True)\ncardrfm.head()","85f6c3aa":"print('Target value minimum',train.target.min())\nprint('Target value maximum',train.target.max())\nprint('Target value median',train.target.median())","8de0be88":"train.target.quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989])","24805dff":"quantiles = cardrfm.quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989])\nquantiles = quantiles.to_dict()\nquantiles","a25cbaa8":"def RScore(x,p,d):\n    if x <= d[p][0.011]:\n        return 1\n    elif x <= d[p][0.050]:\n        return 2\n    elif x <= d[p][0.25]: \n        return 3\n    elif x <= d[p][0.5]:\n        return 4\n    elif x <= d[p][0.75]:\n        return 5\n    elif x <= d[p][0.95]:\n        return 6\n    elif x <= d[p][0.989]:\n        return 7\n    else:\n        return 8\n    \ndef FMScore(x,p,d):\n    if x <= d[p][0.011]:\n        return 8\n    elif x <= d[p][0.050]:\n        return 7\n    elif x <= d[p][0.25]: \n        return 6\n    elif x <= d[p][0.5]:\n        return 5\n    elif x <= d[p][0.75]:\n        return 4\n    elif x <= d[p][0.95]:\n        return 3\n    elif x <= d[p][0.989]:\n        return 2\n    else:\n        return 1","e6dbbbc7":"cardrfm['r_quantile'] = cardrfm['recency'].apply(RScore, args=('recency',quantiles))\ncardrfm['f_quantile'] = cardrfm['frequency'].apply(FMScore, args=('frequency',quantiles))\ncardrfm['v_quantile'] = cardrfm['value'].apply(FMScore, args=('value',quantiles))\ncardrfm['RFMindex'] = cardrfm.r_quantile.map(str)+cardrfm.f_quantile.map(str)+cardrfm.v_quantile.map(str)                       \ncardrfm['RFMScore'] = cardrfm.r_quantile+cardrfm.f_quantile+cardrfm.v_quantile \ncardrfm.head()","53db894e":"cardrfm.RFMindex= cardrfm.RFMindex.astype(int)\nRFMindex=pd.DataFrame(np.unique(np.sort(cardrfm.RFMindex)),columns=['RFMindex'])\nRFMindex.index=RFMindex.index.set_names(['RFMIndex'])\nRFMindex.reset_index(inplace=True)\ncardrfm =pd.merge(cardrfm,RFMindex,on='RFMindex',how='left')\ncardrfm.drop(columns=\"RFMindex\",inplace=True) \ncardrfm.head()","24bbda74":"cardrfm_new = new.groupby('card_id').agg({'card_id': 'count','purchase_date': 'max','purchase_amount': 'sum'})\ncardrfm_new.rename(columns={'card_id' : 'frequency_new','purchase_date': 'date_recency','purchase_amount': 'value_new'},inplace=True)\ncardrfm_new = pd.merge(cardrfm_new,cardreference.iloc[:,0:2],on='card_id',how='left')\ncardrfm_new['recency_new'] = cardrfm_new['reference_date'] - cardrfm_new['date_recency'] + datetime.timedelta(days=61)\ncardrfm_new.recency_new= cardrfm_new.recency_new\/(24*np.timedelta64(1, 'h')) # to convert to day fractions\ncardrfm_new.drop(columns=['date_recency','reference_date'],inplace=True)\nnewquantiles = cardrfm_new.quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989])\nnewquantiles = newquantiles.to_dict()\nnewquantiles","53551c51":"cardrfm_new['rnew_quantile'] = cardrfm_new['recency_new'].apply(RScore, args=('recency_new',newquantiles))\ncardrfm_new['fnew_quantile'] = cardrfm_new['frequency_new'].apply(FMScore, args=('frequency_new',newquantiles))\ncardrfm_new['vnew_quantile'] = cardrfm_new['value_new'].apply(FMScore, args=('value_new',newquantiles))\ncardrfm_new['RFMnewindex'] = cardrfm_new.rnew_quantile.map(str)+cardrfm_new.fnew_quantile.map(str)+cardrfm_new.vnew_quantile.map(str)                       \ncardrfm_new['RFMnewScore'] = cardrfm_new.rnew_quantile+cardrfm_new.fnew_quantile+cardrfm_new.vnew_quantile \ncardrfm_new.head()","5c4091a0":"cardrfm_new.RFMnewindex= cardrfm_new.RFMnewindex.astype(int)\nRFMnewindex=pd.DataFrame(np.unique(np.sort(cardrfm_new.RFMnewindex)),columns=['RFMnewindex'])\nRFMnewindex.index=RFMnewindex.index.set_names(['RFMnewIndex'])\nRFMnewindex.reset_index(inplace=True)\ncardrfm_new =pd.merge(cardrfm_new,RFMnewindex,on='RFMnewindex',how='left')\ncardrfm_new.drop(columns=\"RFMnewindex\",inplace=True) \ncardrfm_new.head()","1254aa55":"for df in [train,test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['year'] = df['first_active_month'].dt.year\n    df['month'] = df['first_active_month'].dt.month\n    df['weekday'] = df['first_active_month'].dt.weekday\n    df['feature_comb'] = df.feature_1.map(str) + df.feature_2.map(str) + df.feature_3.map(str)\n    df['feature_comb']= df['feature_comb'].astype(int)","68f7ca94":"featureindex=pd.DataFrame(np.unique(np.sort(train['feature_comb'])),columns=['feature_comb'])\nfeatureindex.index=featureindex.index.set_names(['feature_comb_index'])\nfeatureindex.reset_index(inplace=True)\ntrain =pd.merge(train,featureindex,on='feature_comb',how='left')\ntrain.drop(columns=\"feature_comb\",inplace=True) \ntest =pd.merge(test,featureindex,on='feature_comb',how='left')\ntest.drop(columns=\"feature_comb\",inplace=True)\ntest.head()","1af231f7":"train_df= pd.merge(train,cardrfm,on='card_id',how='left')\ntrain_df= pd.merge(train_df,cardrfm_new,on='card_id',how='left')\ntrain_df= pd.merge(train_df,cardreference,on='card_id',how='left')\ntrain_df['frequency_new_hist'] =train_df.frequency_new\/train_df.frequency\ntrain_df['value_new_hist'] =train_df.value_new\/train_df.value\ntrain_df['recency_new_hist'] =train_df.recency_new\/train_df.recency\ntrain_df['elapsedtime']= (train_df['reference_date'] - train_df['first_active_month']).dt.days\ntest_df= pd.merge(test,cardrfm,on='card_id',how='left')\ntest_df= pd.merge(test_df,cardrfm_new,on='card_id',how='left')\ntest_df= pd.merge(test_df,cardreference,on='card_id',how='left')\ntest_df['frequency_new_hist'] =test_df.frequency_new\/test_df.frequency\ntest_df['value_new_hist'] =test_df.value_new\/test_df.value\ntest_df['recency_new_hist'] =test_df.recency_new\/test_df.recency\ntest_df['elapsedtime']= (test_df['reference_date'] - test_df['first_active_month']).dt.days\ntrain_df['first_active_month'] = pd.DatetimeIndex(train_df['first_active_month']).\\\n                                      astype(np.int64) * 1e-9\ntrain_df['reference_date'] = pd.DatetimeIndex(train_df['reference_date']).\\\n                                      astype(np.int64) * 1e-9\ntest_df['first_active_month'] = pd.DatetimeIndex(test_df['first_active_month']).\\\n                                      astype(np.int64) * 1e-9\ntest_df['reference_date'] = pd.DatetimeIndex(test_df['reference_date']).\\\n                                    astype(np.int64) * 1e-9\ntest_df.head()","a8f02eac":"train_df.to_csv(\"trainrfm.csv\",index=False)\ntest_df.to_csv(\"testrfm.csv\",index=False)","ab79aabf":"target = train_df.target\ntrain_df= train_df.drop(['card_id','target'],axis=1)\ncard_id = test_df['card_id']\ntest_df= test_df.drop(['card_id'],axis=1)","355b05f3":"features = [c for c in train_df.columns if c not in ['card_id','target']]\ncategorical_feats = ['feature_1','feature_2','feature_3','feature_comb_index','year', 'month','weekday','category_month_lag','r_quantile','f_quantile','v_quantile','RFMScore','RFMIndex','RFMIndex','rnew_quantile','fnew_quantile','vnew_quantile','RFMnewScore','RFMnewIndex','RFMIndex']","7cf3e301":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\noof1 = np.zeros(len(train_df))\npredictions1 = np.zeros(len(test_df))\nstart = time.time()\nfeature_importance_df1 = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values,train_df['RFMScore'].values)):\n     print(\"fold n\u00b0{}\".format(fold_))\n     trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n     val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n     num_round = 10000\n#     params= param\n     params ={\n                 'task': 'train',\n                 'boosting': 'goss',\n                 'objective': 'regression',\n                 'metric': 'rmse',\n                 'learning_rate': 0.01,\n                 'subsample': 0.9855232997390695,\n                 'max_depth': 7,\n                 'top_rate': 0.9064148448434349,\n                 'num_leaves': 63,\n                 'min_child_weight': 41.9612869171337,\n                 'other_rate': 0.0721768246018207,\n                 'reg_alpha': 9.677537745007898,\n                 'colsample_bytree': 0.5665320670155495,\n                 'min_split_gain': 9.820197773625843,\n                 'reg_lambda': 8.2532317400459,\n                 'min_data_in_leaf': 21,\n                 'verbose': -1,\n                 'seed':int(2**fold_),\n                 'bagging_seed':int(2**fold_),\n                 'drop_seed':int(2**fold_)}\n     \n    \n     clf1 = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 200)\n     oof1[val_idx] = clf1.predict(train_df.iloc[val_idx][features], num_iteration=clf1.best_iteration)\n    \n     fold_importance_df1 = pd.DataFrame()\n     fold_importance_df1[\"feature\"] = features\n     fold_importance_df1[\"importance\"] = clf1.feature_importance()\n     fold_importance_df1[\"fold\"] = fold_ + 1\n     feature_importance_df1 = pd.concat([feature_importance_df1, fold_importance_df1], axis=0)\n    \n     predictions1 += clf1.predict(test_df[features], num_iteration=clf1.best_iteration) \/ folds.n_splits\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof1, target)**0.5))","a154793c":"cols = (feature_importance_df1[[\"feature\", \"importance\"]]\n         .groupby(\"feature\")\n         .mean()\n         .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features1 = feature_importance_df1.loc[feature_importance_df1.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n             y=\"feature\",\n             data=best_features1.sort_values(by=\"importance\",\n                                            ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances1.png')","ce10aaff":"RFMindex is obtained by combining the recency ,frequency and value quantiles whereas RFMScore is by adding all three together . For RFM score equal weights are given to all 3 and will help in having a continous scores in a range. RFM index will be sparse. RFMindex will be convertred into contionous value s from 0 as below.","75663438":"As mentioned in my kernel https:\/\/www.kaggle.com\/rajeshcv\/feature-engineering-on-multiple-reference-dates there are multiple reference dates . Recency of transactions for  a credit card  will be based on this refernce date of the credit card","47d27d22":"Susan Li in her article divides Recency , Frequency and Monetary value in four quantiles for the puropse of customer segmentation.\nBut in our case the target customer Loyalty scores have  values from -33.219 to 17.965 with a  median value close to 0. \nHence we need to consider more quantiles ","ce64db9d":"**RFM features are on top in  importance as per the model. **","ca686321":"An interaction varaible feature_comb is created from feature_1,feature_2 ,feature_3 and is made continous as below","20dc3c42":"**Lets build a model based on this few features to test if RFM is of any importance.**","3d3cd717":"We will apply the RFM to new merchant transactions also ","79263f1e":"To avoid negative values for purchase amount 0.75 is added to purchase_amount . This is rounded value of the minimum purchase_amount which is 0.7486","24e78fb7":"Functions Rscore is for assigning the recency quantile and RMScore is for assigning  frequency and monetary value uantiles.\nNote the assigning of quantile of frequency and value is reverse to recency as lower the recency the better but higher the frequency and value the better from the perspective of customer loyalty.","e22a6852":"These quantiles seems to be reasonable as it sepeartes most of the target value groups.. Assigning this quantiles to cardrm variables as below.","99d4c538":"RFM is an old technique of customer segmentation to evaluate customer loyalty.R stands for Recency , F for Frequency and M for Monetary value.\nThis kernel is based on the article by Susan Li at https:\/\/towardsdatascience.com\/find-your-best-customers-with-customer-segmentation-in-python-61d602f9eee6\n\n**RFM Score Calculations**\n\n**RECENCY (R):** Days since last purchase\n\n**FREQUENCY (F):** Total number of purchases\n\n**MONETARY VALUE (M):** Total money this customer spent"}}