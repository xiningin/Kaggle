{"cell_type":{"92dd40d0":"code","c748b91c":"code","8fede0f9":"code","b529a398":"code","177f8d34":"code","fb154ed0":"code","6e6b5e9a":"code","6073bc37":"code","e2315d00":"code","de8aabcf":"code","7baecb43":"code","7f65ca17":"code","e00c9ec8":"code","11e6aa3d":"code","a834e4c9":"code","d95e27c2":"code","67f5694a":"code","6653d470":"code","d2e236bf":"code","b80fb023":"code","521d0bb6":"code","463227a4":"code","c7f94ccd":"code","74a8795a":"code","4d99a015":"code","5f1b2f12":"code","f011ee8c":"code","d61299fe":"code","5f4e7d51":"code","07cee48e":"code","5242b3fb":"code","ba2f3299":"code","8267112d":"code","51587c84":"code","6fc75e74":"code","5113d4a1":"code","d9b67c06":"markdown","61fe40e1":"markdown","51375045":"markdown","0b5392ef":"markdown","b2cc2aee":"markdown","ea4739a9":"markdown","cb9cd233":"markdown","71eee600":"markdown","19381819":"markdown","e6c23895":"markdown","4a39f927":"markdown","75ce4f64":"markdown","2c10f593":"markdown","68ee9736":"markdown","a52a6475":"markdown","045691c5":"markdown","6cc1157a":"markdown","2bd3d9e2":"markdown","b635cdd8":"markdown","26b4dc1b":"markdown","7066f06a":"markdown","bbe26acf":"markdown"},"source":{"92dd40d0":"# we will be using various libraries like os for taking the input,etc\n# I have used some libraries like seaborn, wordcloud, matplotlib for data visualization so\n# you can skip them if you don't understand \n\nimport os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport operator\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\n\n# re is used for cleaning the dataset \n\nimport re\n\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow import keras\n\n# callbacks are important here as sometimes you get the best accuracy earlies and then it \n# goes down so as to stop the training there you need to use them\n\n\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding,Conv1D,LSTM,GRU,BatchNormalization,Flatten,Dense\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c748b91c":"df= pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head()","8fede0f9":"df.info()","b529a398":"df.isnull().sum()","177f8d34":"sns.countplot(x=df['sentiment'])\nplt.grid()","fb154ed0":"sentences=df['review']\nle=LabelEncoder()\ndf['sentiment']= le.fit_transform(df['sentiment'])","6e6b5e9a":"stopwords = set(STOPWORDS) \n\npos=' '.join(map(str,sentences[df['sentiment']==1]))\nneg=' '.join(map(str,sentences[df['sentiment']==0]))\n  \nwordcloud1 = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(pos) \n\nplt.figure(figsize=(8,8))\nplt.imshow(wordcloud1)\nplt.title('Positive Sentiment')\nplt.axis('off')","6073bc37":"plt.figure(figsize=(8,8))\nwordcloud2 = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(neg) \n\nplt.imshow(wordcloud2)\nplt.title('Negative Sentiment')\nplt.axis('off')\n\nplt.show() ","e2315d00":"labels=to_categorical(df['sentiment'],num_classes=2)\nX_train,X_test,Y_train,Y_test = train_test_split(df['review'],labels,test_size=0.1,random_state=10)","de8aabcf":"glove_embeddings= np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl',\n                          allow_pickle=True)","7baecb43":"def vocab_build(review):\n    \n    comments = review.apply(lambda s: s.split()).values\n    vocab={}\n    \n    for comment in comments:\n        for word in comment:\n            try:\n                vocab[word]+=1\n                \n            except KeyError:\n                vocab[word]=1\n    return vocab","7f65ca17":"def embedding_coverage(review,embeddings):\n    \n    vocab=vocab_build(review)\n    \n    covered={}\n    word_count={}\n    oov={}\n    covered_num=0\n    oov_num=0\n    \n    for word in vocab:\n        try:\n            covered[word]=embeddings[word]\n            covered_num+=vocab[word]\n            word_count[word]=vocab[word]\n        except:\n            oov[word]=vocab[word]\n            oov_num+=oov[word]\n    \n    vocab_coverage=len(covered)\/len(vocab)*100\n    text_coverage = covered_num\/(covered_num+oov_num)*100\n    \n    sorted_oov=sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    sorted_word_count=sorted(word_count.items(), key=operator.itemgetter(1))[::-1]\n    \n    return sorted_word_count,sorted_oov,vocab_coverage,text_coverage\n        ","e00c9ec8":"train_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,glove_embeddings)\ntest_covered,test_oov, test_vocab_coverage, test_text_coverage = embedding_coverage(X_test,glove_embeddings)\n\nprint(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\nprint(f\"Glove embeddings cover {round(test_vocab_coverage,2)}% of vocabulary and {round(test_text_coverage,2)}% text in testing set\")","11e6aa3d":"train_oov[:10]","a834e4c9":"def clean_sentences(line):\n    \n    line=re.sub('<.*?>','',line) # removing html tags\n    \n    #removing contractions\n    line=re.sub(\"isn't\",'is not',line)\n    line=re.sub(\"he's\",'he is',line)\n    line=re.sub(\"wasn't\",'was not',line)\n    line=re.sub(\"there's\",'there is',line)\n    line=re.sub(\"couldn't\",'could not',line)\n    line=re.sub(\"won't\",'will not',line)\n    line=re.sub(\"they're\",'they are',line)\n    line=re.sub(\"she's\",'she is',line)\n    line=re.sub(\"There's\",'there is',line)\n    line=re.sub(\"wouldn't\",'would not',line)\n    line=re.sub(\"haven't\",'have not',line)\n    line=re.sub(\"That's\",'That is',line)\n    line=re.sub(\"you've\",'you have',line)\n    line=re.sub(\"He's\",'He is',line)\n    line=re.sub(\"what's\",'what is',line)\n    line=re.sub(\"weren't\",'were not',line)\n    line=re.sub(\"we're\",'we are',line)\n    line=re.sub(\"hasn't\",'has not',line)\n    line=re.sub(\"you'd\",'you would',line)\n    line=re.sub(\"shouldn't\",'should not',line)\n    line=re.sub(\"let's\",'let us',line)\n    line=re.sub(\"they've\",'they have',line)\n    line=re.sub(\"You'll\",'You will',line)\n    line=re.sub(\"i'm\",'i am',line)\n    line=re.sub(\"we've\",'we have',line)\n    line=re.sub(\"it's\",'it is',line)\n    line=re.sub(\"don't\",'do not',line)\n    line=re.sub(\"that\u00b4s\",'that is',line)\n    line=re.sub(\"I\u00b4m\",'I am',line)\n    line=re.sub(\"it\u2019s\",'it is',line)\n    line=re.sub(\"she\u00b4s\",'she is',line)\n    line=re.sub(\"he\u2019s'\",'he is',line)\n    line=re.sub('I\u2019m','I am',line)\n    line=re.sub('I\u2019d','I did',line)\n    line=re.sub(\"he\u2019s'\",'he is',line)\n    line=re.sub('there\u2019s','there is',line)\n    \n    #special characters and emojis\n    line=re.sub('\\x91The','The',line)\n    line=re.sub('\\x97','',line)\n    line=re.sub('\\x84The','The',line)\n    line=re.sub('\\uf0b7','',line)\n    line=re.sub('\u00a1\u00a8','',line)\n    line=re.sub('\\x95','',line)\n    line=re.sub('\\x8ei\\x9eek','',line)\n    line=re.sub('\\xad','',line)\n    line=re.sub('\\x84bubble','bubble',line)\n    \n    # remove concated words\n    line=re.sub('trivialBoring','trivial Boring',line)\n    line=re.sub('Justforkix','Just for kix',line)\n    line=re.sub('Nightbeast','Night beast',line)\n    line=re.sub('DEATHTRAP','Death Trap',line)\n    line=re.sub('CitizenX','Citizen X',line)\n    line=re.sub('10Rated','10 Rated',line)\n    line=re.sub('_The','_ The',line)\n    line=re.sub('1Sound','1 Sound',line)\n    line=re.sub('blahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblah','blah blah',line)\n    line=re.sub('ResidentHazard','Resident Hazard',line)\n    line=re.sub('iameracing','i am racing',line)\n    line=re.sub('BLACKSNAKE','Black Snake',line)\n    line=re.sub('DEATHSTALKER','Death Stalker',line)\n    line=re.sub('_is_','is',line)\n    line=re.sub('10Fans','10 Fans',line)\n    line=re.sub('Yellowcoat','Yellow coat',line)\n    line=re.sub('Spiderbabe','Spider babe',line)\n    line=re.sub('Frightworld','Fright world',line)\n    \n    #removing punctuations\n    \n    punctuations = '@#!~?+&*[]-%._-:\/\u00a3();$=><|{}^' + '''\"\u201c\u00b4\u201d'`'''\n    for p in punctuations:\n        line = line.replace(p, f' {p} ')\n        \n    line=re.sub(',',' , ',line)\n        \n    # ... and ..\n    line = line.replace('...', ' ... ')\n    \n    if '...' not in line:\n        line = line.replace('..', ' ... ')\n        \n    return line\n    ","d95e27c2":"X_train=X_train.apply(lambda s: clean_sentences(s))\nX_test=X_test.apply(lambda s: clean_sentences(s))\n\ntrain_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,glove_embeddings)\nprint(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\n\ntest_covered,test_oov,test_vocab_coverage,test_text_coverage=embedding_coverage(X_test,glove_embeddings)\nprint(f\"Glove embeddings cover {round(test_vocab_coverage,2)}% of vocabulary and {round(test_text_coverage,2)}% text in training set\")\n","67f5694a":"punctuations = '@#!~?+&*[]-%._-:\/\u00a3();$=><|{},^' + '''\"\u201c\u00b4\u201d'`'''\ntrain_word=[]\ntrain_count=[]\n\ni=1\nfor word,count in train_covered: \n    if word not in punctuations:\n        train_word.append(word)\n        train_count.append(count)\n        i+=1\n    if(i==15):\n        break","6653d470":"test_word=[]\ntest_count=[]\n\ni=1\nfor word,count in test_covered: \n    if word not in punctuations:\n        test_word.append(word)\n        test_count.append(count)\n        i+=1\n    if(i==15):\n        break","d2e236bf":"plt.figure(figsize=(12,8))\nsns.barplot(x=train_count,y=train_word).set_title('Count of 15 most used word in training set')\nplt.grid()","b80fb023":"plt.figure(figsize=(12,8))\nsns.barplot(x=test_count,y=test_word).set_title('Count of 15 most used word in testing set')\nplt.grid()","521d0bb6":"del glove_embeddings,train_oov,test_oov\ngc.collect()","463227a4":"num_words=80000\nembeddings=256","c7f94ccd":"tokenizer=Tokenizer(num_words=num_words,oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\nword_index=tokenizer.word_index\ntotal_vocab=len(word_index)","74a8795a":"print(\"Vocabulary of the dataset is : \",total_vocab)","4d99a015":"sequences_train=tokenizer.texts_to_sequences(X_train)\nsequences_test=tokenizer.texts_to_sequences(X_test)\n\nmax_len=max(max([len(x) for x in sequences_train]),max([len(x) for x in sequences_test]))\n\ntrain_padded=pad_sequences(sequences_train,maxlen=max_len)\ntest_padded=pad_sequences(sequences_test,maxlen=max_len)\n","5f1b2f12":"X_train,X_val,Y_train,Y_val=train_test_split(train_padded,Y_train,\n                                             test_size=0.05,random_state=10)","f011ee8c":"model= keras.Sequential()\nmodel.add(Embedding(num_words,embeddings,input_length=max_len))\nmodel.add(Conv1D(256,10,activation='relu'))\nmodel.add(keras.layers.Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(LSTM(64))\nmodel.add(keras.layers.Dropout(0.4))\nmodel.add(Dense(2,activation='softmax'))","d61299fe":"model.summary()","5f4e7d51":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy']\n             )","07cee48e":"es= EarlyStopping(monitor='val_accuracy',\n                  patience=2\n                 )\n\ncheckpoints=ModelCheckpoint(filepath='.\/',\n                            monitor=\"val_accuracy\",\n                            verbose=0,\n                            save_best_only=True\n                           )\n\ncallbacks=[es,checkpoints]","5242b3fb":"history=model.fit(X_train,Y_train,validation_data=(X_val,Y_val),epochs=5,callbacks=callbacks)","ba2f3299":"def plot_graph(history,string):\n    \n    plt.plot(history.history[string],label='training '+string)\n    plt.plot(history.history['val_'+string],label='validation '+string)\n    plt.legend()\n    plt.xlabel('epochs')\n    plt.ylabel(string)\n    plt.title(string+' vs epochs')\n    plt.show()","8267112d":"plot_graph(history,'loss')","51587c84":"plot_graph(history,'accuracy')","6fc75e74":"model.save('imdb_model.h5')","5113d4a1":"print(\"Model Performance on test set\")\nresult = model.evaluate(test_padded,Y_test)\nprint(dict(zip(model.metrics_names, result)))","d9b67c06":"**We will delete the embeddings as it takes too much memory**","61fe40e1":"# Model Building","51375045":"# Data Preprocessing","0b5392ef":"**Callbacks are really helpful as they stop our model when the validation accuracy of our model starts decreasing for consecutive 2 epochs as well save the best possible weights which gives highest validation accuracy**","b2cc2aee":"**We will 2 LSTM layers and Conv1D layer for training the model.<br>\nUsing Dropout reduces the overfitting by decreasing the bias and is a must since there is lot of variance seen.**\n","ea4739a9":"# <h1 style='font-family:cursive;color:red'><center>Sentiment Classification on Movie Reviews<\/center><\/h1>","cb9cd233":"<b>Data visualization using word cloud for finding the most used words for each type of sentiment<\/b>","71eee600":"<h3 style='color:red'>I hope you Liked my kernel. An upvote is a gesture of appreciation that will help me to create more kernels and keep me motivated ,be kind to show one ;-)<\/h3>","19381819":"**using seaborn's barplot let's find out the count of 10 most used words in training and testing set**","e6c23895":"<h4 style='color:blue'><span style='color:red'>Note: <\/span>In this model I will be using glove embeddings.It has a large vocabulary and we can find the words from our data which are not present in the glove( these words are contractions, misspelled words, concated words or emojis which can decrease our model's performance. We will then use re library to remove these words from the dataset.<\/h4>","4a39f927":"<h3 style='color:green'>What is Sentiment Analysis?<\/h3>\n<b style='color:blue'>Sentiment analysis is a natural language processing technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs.<\/b>","75ce4f64":"# Model Evaluation","2c10f593":"# Importing Libraries","68ee9736":"<b>We will find the count of each type of sentiment in the dataset using seaborn library<\/b>","a52a6475":"<b>train_oov shows the words which we need to preprocess<\/b>","045691c5":"<span style='color:green'>After cleaning the dataset we can see that now our vocabulary covers almost 87% on training set and 95.5% on testing set which initially was far less.<\/span>","6cc1157a":"<b>We will build vocabulary and count of each vocabulary using the below function<\/b>","2bd3d9e2":"<b>Embedding Coverage tells how much percentage of the words in our data are covered by the vocabulary.<br>\n<i>sorted_oov<\/i> is the list of words which we need to do text cleaning on. <\/b>","b635cdd8":"<b>Let's find if the data contains any missing value<\/b>","26b4dc1b":"# Data Cleaning","7066f06a":"<h3 style='color:orange'>How we will do sentiment analysis?<\/h3>\n<b>While tackling with Text Data it is very important that the data is in correct format. For ex. we will remove emojis,contraction, mixed word,wrongly spelled words,punctuation and other unwanted stuff as they can decrease the model accuracy.<\/b><br>\n<b>We will use a technique called tokenization to transform the data into numerical form so as to use it to train the deep learning model.<\/b><br>\n<b>We would be using Glove embeddings to make the data in correct format.You can find the dataset for the glove embedding at <a href='https:\/\/www.kaggle.com\/authman\/pickled-glove840b300d-for-10sec-loading'>Glove Embeddings<\/a>","bbe26acf":"![](https:\/\/static.amazon.jobs\/teams\/53\/images\/IMDb_Header_Page.jpg?1501027252)"}}