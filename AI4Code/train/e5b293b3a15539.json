{"cell_type":{"43f7f4b0":"code","25fe4bc0":"code","cf385c10":"code","6d8853f5":"code","5241d466":"code","b8ac83d3":"code","c726a862":"code","1e7401f8":"code","9c45a787":"code","7c6a3c1e":"code","28d8d41f":"code","c34f4390":"code","695f0f72":"code","05e46d23":"code","2f0b5031":"code","099afaef":"code","b23d5d11":"code","3c908b16":"code","74931c9b":"code","d91a5e26":"code","eaa25f0b":"code","5327b5dc":"code","09a744cb":"code","fd0c1e60":"code","bbc7487b":"code","207f4b9d":"code","6082c684":"code","d60f1a42":"code","8f27b6a1":"code","067f32cf":"code","b983222d":"code","89c6eb0c":"code","2858e25e":"code","70e218f4":"code","a1617b70":"code","c352eae5":"code","9805cbaf":"code","0806723d":"code","3c1a2c2e":"code","d3cc3871":"code","40e3b901":"code","bccbef68":"code","8fe2fc96":"code","d1f1b0d9":"code","9eb0bfde":"code","c86b6282":"code","4867647c":"code","a2be87cb":"code","842be13c":"markdown","2bdf4bab":"markdown","78e42d30":"markdown","657ade59":"markdown","4d4a808d":"markdown","ca409ed1":"markdown","26593c63":"markdown","06291a4b":"markdown","16b2d979":"markdown","587e28a4":"markdown","a68f6290":"markdown","701a2609":"markdown","6823d399":"markdown","cf675fc6":"markdown","4bcec9ba":"markdown","011827ee":"markdown","257cfc85":"markdown","f21a433a":"markdown","8229615d":"markdown","7c5061d7":"markdown"},"source":{"43f7f4b0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport scipy.stats as stats","25fe4bc0":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","cf385c10":"# 2.1 Check number of observations and columns\nprint(train.shape)\nprint(test.shape)\n# Train has one more column the predictor variable SalePrice than the test data\nprint(set(train.columns)-set(test.columns))","6d8853f5":"# 2.2 Check the number of observations, data types and NAN for each variable\nprint(train.info())\nprint('-----------------------'*3)\nprint(test.info())","5241d466":"# 2.3 Check the descriptive info for 37 numeric variables, exclude Id\ntrain.drop(['Id'], axis=1).describe()","b8ac83d3":"# Check how many numeric columns and how many object columns, 38 numeric columns including ID\n# 43 character columns\nprint(train.select_dtypes(exclude=['object']).shape)\nprint(train.select_dtypes(include=['object']).shape)","c726a862":"# 2.4 Check the first 5 observations and last five observations of train and head\n#print(train.head(5))\nprint(train[:5])\nprint('\/n')\nprint(train.tail(5))\n#print(train[-5:])","1e7401f8":"plt.figure(figsize=(13,10))\nsns.heatmap(train.corr(), vmax=0.8)","9c45a787":"train.drop(['GarageYrBlt','TotRmsAbvGrd','TotalBsmtSF'], axis=1, inplace=True)\ntest.drop(['GarageYrBlt','TotRmsAbvGrd','TotalBsmtSF'], axis=1, inplace=True)","7c6a3c1e":"# Top 10 numeric features positively correlated with SalePrice\ntrain.corr()['SalePrice'].sort_values(ascending=False).head(11)","28d8d41f":"train.corr()['SalePrice'].abs().sort_values(ascending=False).head(11)\ntop_corr_features=train.corr()['SalePrice'].abs().sort_values(ascending=False).head(11).index\ntop_corr_features","c34f4390":"# Box-plot to check relationship between SalePrice and OverallQual\nplt.figure(figsize=(10,7))\nsns.boxplot(x='OverallQual', y='SalePrice', data=train)","695f0f72":"# Scatterplot to check the relationship between SalePrice and GrLivArea\nplt.scatter(x='GrLivArea', y='SalePrice', data=train, color='r', marker='*')\ntrain['GrLivArea'].sort_values(ascending=False).head(2)","05e46d23":"train.index[[523, 1298]]","2f0b5031":"print(train.shape)\ntrain.drop(train.index[[523, 1298]], inplace=True)\nprint(train.shape)","099afaef":"print(top_corr_features)\nbox_feature=['SalePrice','OverallQual','GarageCars','FullBath', 'YearBuilt','YearRemodAdd','Fireplaces']\nscatter_feature=['SalePrice', 'GrLivArea','1stFlrSF','GarageArea']\n# Use sns.pairplot to check the relationship between the SalePrice and top 10 correlated features\nsns.pairplot(train[scatter_feature])","b23d5d11":"sns.pairplot(train[box_feature], kind='scatter', diag_kind='hist')","3c908b16":"train.isnull().sum().sort_values(ascending=False)\n# Check the NAN values as percentage\ntrain_nan_pct=(train.isnull().sum())\/(train.isnull().count())\ntrain_nan_pct=train_nan_pct[train_nan_pct>0]\ntrain_nan_pct.sort_values(ascending=False)","74931c9b":"train.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1, inplace=True)\ntest.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1, inplace=True)","d91a5e26":"train['GarageQual'].value_counts()","eaa25f0b":"train_impute_index=train_nan_pct[train_nan_pct<0.3].index\ntrain_impute_index\ntrain_impute_mode=['MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ntrain_impute_median=['LotFrontage', 'MasVnrArea']","5327b5dc":"# Impute character or qualitative feature with mode\nfor feature in train_impute_mode:\n    train[feature].fillna(train[feature].mode()[0], inplace=True)\n    test[feature].fillna(test[feature].mode()[0], inplace=True)","09a744cb":"# Impute numeric feature with median\nfor feature in train_impute_median:\n    train[feature].fillna(train[feature].median(), inplace=True)\n    test[feature].fillna(test[feature].median(), inplace=True)","fd0c1e60":"# There are no nan values in train\ntrain.isnull().sum().sort_values(ascending=False).head(5)","bbc7487b":"test_only_nan=test.isnull().sum().sort_values(ascending=False)\ntest_only_nan=test_only_nan[test_only_nan>0]\nprint(test_only_nan.index)\ntest_impute_mode=['MSZoning', 'BsmtFullBath', 'Utilities','BsmtHalfBath', 'Functional', 'SaleType', 'Exterior2nd', 'Exterior1st', 'GarageCars', 'KitchenQual']\ntest_impute_median=['BsmtFinSF2','GarageArea', 'BsmtFinSF1','BsmtUnfSF' ]","207f4b9d":"# Impute test character feature with mode\nfor feature in test_impute_mode:\n    test[feature].fillna(test[feature].mode()[0], inplace=True)\nfor feature in test_impute_median:\n    test[feature].fillna(test[feature].median(), inplace=True)\n#Impute test numeric feature with median","6082c684":"# Now there are no NAN values in both train and test data\ntest.isnull().sum().sort_values(ascending=False).head(5)","d60f1a42":"# Store the test data ID for competition purpose\nTestId=test['Id']","8f27b6a1":"total_features=pd.concat((train.drop(['Id','SalePrice'], axis=1), test.drop(['Id'], axis=1)))","067f32cf":"total_features=pd.get_dummies(total_features, drop_first=True)\ntrain_features=total_features[0:train.shape[0]]\ntest_features=total_features[train.shape[0]:]","b983222d":"sns.distplot(train['SalePrice'])\n# The response variable is right-skewed, we will log1p() transform y","89c6eb0c":"train['Log SalePrice']=np.log1p(train['SalePrice'])\nsns.distplot(train['Log SalePrice'])\n# natural log one plus the array log(y+1) is more symmetric ","2858e25e":"plt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nsns.kdeplot(train['SalePrice'], legend=True)\nplt.subplot(1,2,2)\nsns.kdeplot(train['Log SalePrice'], legend=True)","70e218f4":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val=train_test_split(train_features, train[['SalePrice']], test_size=0.3, random_state=100)","a1617b70":"# Import Ridge regression from sklearn\nfrom sklearn.linear_model import Ridge\n# Evaluate model performance using root mean square error\nfrom sklearn.metrics import mean_squared_error\nrmse=[]\n# check the below alpha values for Ridge Regression\nalpha=[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n\nfor alph in alpha:\n    ridge=Ridge(alpha=alph, copy_X=True, fit_intercept=True)\n    ridge.fit(X_train, y_train)\n    predict=ridge.predict(X_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)","c352eae5":"rmse=pd.Series(rmse, index=alpha)\nrmse.argmin()","9805cbaf":"# Adjust alpha based on previous result\nalpha=np.arange(8,14, 0.5)\nrmse=[]\n\nfor alph in alpha:\n    ridge=Ridge(alpha=alph, copy_X=True, fit_intercept=True)\n    ridge.fit(X_train, y_train)\n    predict=ridge.predict(X_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse=pd.Series(rmse, index=alpha)\nprint(rmse.argmin())","0806723d":"# Adjust alpha based on previous result\nalpha=np.arange(10.5, 11.6, 0.1)\nrmse=[]\n\nfor alph in alpha:\n    ridge=Ridge(alpha=alph, copy_X=True, fit_intercept=True)\n    ridge.fit(X_train, y_train)\n    predict=ridge.predict(X_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse=pd.Series(rmse, index=alpha)\nprint(rmse.argmin())","3c1a2c2e":"# Use alpha=11.1 to predict the test data\nridge=Ridge(alpha=11.1)\n# Use all training data to fit the model\nridge.fit(train_features, train[['SalePrice']])\npredicted=ridge.predict(test_features)","d3cc3871":"submission=pd.DataFrame()\nsubmission['Id']=TestId\nsubmission['SalePrice']=predicted\nsubmission.to_csv('submission.csv', index=False)","40e3b901":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val=train_test_split(train_features, train[['Log SalePrice']], test_size=0.3, random_state=100)","bccbef68":"# Import Ridge regression from sklearn\nfrom sklearn.linear_model import Ridge\n# Evaluate model performance using root mean square error\nfrom sklearn.metrics import mean_squared_error\nrmse=[]\n# check the below alpha values for Ridge Regression\nalpha=[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n\nfor alph in alpha:\n    ridge=Ridge(alpha=alph, copy_X=True, fit_intercept=True)\n    ridge.fit(X_train, y_train)\n    predict=ridge.predict(X_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse=pd.Series(rmse, index=alpha)\nprint(rmse.argmin())\nprint(rmse.min())","8fe2fc96":"# Adjust alpha based on previous result\nalpha=np.arange(8,14, 0.5)\nrmse=[]\n\nfor alph in alpha:\n    ridge=Ridge(alpha=alph, copy_X=True, fit_intercept=True)\n    ridge.fit(X_train, y_train)\n    predict=ridge.predict(X_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse=pd.Series(rmse, index=alpha)\nprint(rmse.argmin())\nprint(rmse.min())","d1f1b0d9":"# Adjust alpha based on previous result\nalpha=np.arange(10.5, 11.5, 0.1)\nrmse=[]\n\nfor alph in alpha:\n    ridge=Ridge(alpha=alph, copy_X=True, fit_intercept=True)\n    ridge.fit(X_train, y_train)\n    predict=ridge.predict(X_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse=pd.Series(rmse, index=alpha)\nprint('Minimum RMSE at alpah: ', rmse.argmin())\nprint('Minimum RMSE is: ', rmse.min())","9eb0bfde":"# Use alpha=11 to predict the test data\nridge=Ridge(alpha=11)\n# Use all training data to fit the model\nridge.fit(train_features, train[['Log SalePrice']])\npredicted_log_price=ridge.predict(test_features)","c86b6282":"# Transform back the log(SalePrice+1) to SalePrice\nTest_price=np.exp(list(predicted_log_price))-1\nTest_price","4867647c":"submission=pd.DataFrame()\nsubmission['Id']=TestId\nsubmission['SalePrice']=Test_price","a2be87cb":"submission.to_csv('submission.csv', index=False)","842be13c":"* We will compare the performance of buiding the model using y and log(y+1)","2bdf4bab":"## Part 4. Check missing values in features and impute","78e42d30":"### Goal\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n### Metric\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)","657ade59":"From the above plot, there are several features highly correlated. These will cause multicollinearity. We need to drop one of them.\n* YearBuilt and GarageYrBlt, this is reasonable since many times YearBuilt and GarageYrBlt will be the same. Drop GarageYrBlt\n* GrLivArea and TotRmsAbvGrd, drop TotRmsAbvGrd\n* 1stFlrSF and TotalBsmtSF, drop TotalBsmtSF\n","4d4a808d":"* Check multicollinearity\n* Check outliers","ca409ed1":"For this competition, I will use Ridge Regression to predict the house price. The reason I chose Ridge regression is because  Ridge regression will overcome the overfitting problem since we have  many features.\n\nThe below helps improve the validaiton root mean square error and final prediction score.\n1.  Log transform the y variable since the evaluation is based on the RMSE of the logarithm of the predicted house and observed sales price (Improved prediction performance a lot, competition score from 0.14705 to 0.11959)\n1.  Exclude outliers (Improved prediction performance). \n1. Get rid of multicollinearity features (Improved prediction performance).  \n\nBelows are the things I have tried but did not help with the prediction performance\n\n1.  Remove the 10 features with smallest absolute Pearson Correlation coefficient (Prediction performance gets worse)\n1.  Use the RidgeCV instead of the simple train\/validaiton split to choose regularization parameter (Prediction performance stayed the same)\n1.  Use Lasso and LassoCV (Prediction performance gets a little bit worse)\n1. Use the StandardScaler to the numeric features makes the performance worse, score dropped from 0.11959 to 0.12037. For redige regression, set normalize=True also has worse performance.\n1. Use principal component regression. The reason is that the variance could not be explained by the first few principal components. \n\nNext I will try random forest regression, support vector machine to build regression model, and use GridSearchCV to tune hyperparameters.","26593c63":"## Part 5. Combine train features and test features and create dummy variables for character features before runnning machine learning models","06291a4b":"## Part 6: Train Validation split on the training data and Build Ridge Regression","16b2d979":"## Part I: Import the libraries","587e28a4":"## Part 7 Use log(1+SalePrice)\n* By building a Ridge Regression with SalePrice on features, we achieve a score of 0.14705\n* Next we will build the Ridge model with the log(SalePrice+1) to check whether the prediction performance improves","a68f6290":"* By using some simple techniques, we have get rank of 16%. I am still working on improving this kernel. I will keep updating my tries and whether they work or not. \n* If you think my kernel is helpful, please give me a voteup. This is very important for new people like me. Thank you in advance.\n* If you have any question, please feel free to leave me a message, I will check every day. Thank you so much.","701a2609":"* Drop features PoolQC, MiscFeature, Alley, Fence, FireplaceQu which has more than 40% of NAN and seems not very correlated with SalePrice\n* For other missing features, impute quantitative feature with median (since data are skewed from scatter plot). Impute category variable or qualitative variable with mode","6823d399":"As we can see the better the overall quality of the house, the higher the salePrice which is very reasonable.","cf675fc6":"* 3.2 Check the relationship between SalePrice and predictor variables","4bcec9ba":"## Part II: Import the raw data and check data","011827ee":"## Part 6: Check the response variables","257cfc85":"## Part 3. Exploratory check relationship between the dependent variable and independent variable and outliers","f21a433a":"* I am thinking of better ways to visualize the relationship between the SalPrice and categorical features, any suggestion will be greatly appreciated.","8229615d":"* From the scatter plot, there are two outliers with gross living area 5642, 4676 but the SalePrice is low. We will drop these two outliers by index","7c5061d7":"* 3.1 Use heatmap to check the correlation between all numeric variables"}}