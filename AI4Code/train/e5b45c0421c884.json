{"cell_type":{"0e0d847f":"code","fe45519f":"code","1f40c116":"code","87bab421":"code","3b1c5567":"code","43704130":"code","7dcc7505":"code","ae96fa4e":"code","f94e2543":"code","1bb91c68":"code","5d6732c8":"code","da8bd090":"code","72a7418d":"code","d428e345":"code","84a11943":"code","dbe2e5b8":"code","24504842":"code","0cbc616d":"code","d66ca52e":"code","2984e412":"code","487e9125":"code","875cb9a1":"code","eff782bb":"code","0deec54d":"code","29ac9f04":"code","ea5b7133":"code","3ca3152f":"code","f00c4283":"code","bc1d91f7":"code","012fd230":"code","1104a894":"code","61dbb359":"code","9f6ddaef":"code","d14631c9":"code","373c27a6":"code","51728e25":"code","50308eba":"code","7755a8b0":"code","5456762b":"code","66ea335f":"code","b5d044f2":"code","b4fe9ecc":"code","73f1cac0":"markdown","ee60fb11":"markdown","a7736f2c":"markdown","cb146fcf":"markdown","26e708ce":"markdown","1bad54a5":"markdown","b322b1a7":"markdown","7e28c315":"markdown","86940aaf":"markdown","b0b0f1cc":"markdown","e1d875ec":"markdown","7f2cb268":"markdown"},"source":{"0e0d847f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns',None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fe45519f":"#Loading the dataframe\ndf = pd.read_csv('\/kaggle\/input\/adult-census-income\/adult.csv')","1f40c116":"#Viewing the dataset\ndf.head()","87bab421":"#Information about dataset\ndf.info()","3b1c5567":"#Seperating continuous and categorical columns\ncont_col = []\ncat_col  = []\n\nfor i in df.columns :\n    if df[i].dtypes == 'O':\n        cat_col.append(i)\n    else :\n        cont_col.append(i)","43704130":"print('The categorical columns are :\\n',cat_col)\nprint()\nprint('The continuous columns are  :\\n',cont_col)","7dcc7505":"plt.figure(figsize=(10,10))\nsns.heatmap(df.isnull(),cbar=False)\nplt.xticks(rotation=60, fontsize=10)\nplt.show()","ae96fa4e":"df.isnull().sum()","f94e2543":"#Gives Frequency of elements in all categorical columns \n\nfor i in cat_col :\n    print('\\t\\t\\t\\t',i)\n    print(df[i].value_counts())\n    plt.title('Countplot')\n    plt.ylabel('counts')\n    plt.xlabel(i)\n    sns.barplot(df[i].value_counts().index[:5] , df[i].value_counts().values[:5])\n    plt.tight_layout()\n    plt.show()\n    print('\\n\\n')","1bb91c68":"#Cleaning Marital Status column\ndf['marital.status'].replace({'Married-civ-spouse' : 'Married' ,\n                              'Divorced' : 'Separated' , \n                              'Married-AF-spouse' : 'Married' , \n                              'Married-spouse-absent':'Separated'},inplace = True)\n\nsns.countplot(df['marital.status'])","5d6732c8":"#Cleaning education column\n\ndf['education'].replace({'HS-grad':'HighSchool' , \n                         'Some-college':'College' , \n                         'Bachelors' : 'Bachelor degree' , \n                         'Masters' : 'Masters Degree' ,\n                         'Assoc-voc' : 'College' , \n                         'Assoc-acdm':'College' , \n                         'Prof-school' : 'Masters Degree' , \n                         'Doctorate' : 'PhD' , \n                         '11th' : 'Dropout' , \n                         '10th' : 'Dropout' ,\n                         '7th-8th' : 'Dropout' ,\n                         '9th' : 'Dropout' , \n                         '12th' : 'Dropout' ,\n                         '5th-6th': 'Dropout' ,\n                         '1st-4th': 'Dropout' ,\n                         'Preschool':'Dropout'} , inplace = True)\n\nplt.figure(figsize=(10,4))\nsns.countplot(df['education'])","da8bd090":"#The number of columns with both occupation and workclass unknown\n\ndf[(df['occupation'] == \"?\") & (df['workclass'] == \"?\")]['age'].count()\n\n#So a new cateogry called unknown is created for both occupation and workclass columns","72a7418d":"#Imputing WorkClass\nsns.countplot(df[df['workclass'] == '?']['workclass'] , hue = df['income'])","d428e345":"#Since there are around 1800 unknown workclass we will create a new seperate unknown category\ndf['workclass' ].replace({'?' : 'Unknown'} , inplace = True)\ndf['occupation' ].replace({'?' : 'Unknown'} , inplace = True)","84a11943":"cont_col","dbe2e5b8":"df.describe()","24504842":"#Box plot and distplot for all continuous variables\nfor i in ['age','fnlwgt','education.num','hours.per.week']:\n    print('\\t\\t\\t\\t',i)\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plt.title('Boxplot')\n    df[i].plot(kind= 'box')\n    plt.subplot(1,2,2)\n    plt.title('Distribution plot')\n    sns.distplot(df[i],hist = False )\n    plt.show()","0cbc616d":"plt.figure(figsize=(10,6))\nsns.heatmap(df.corr() , annot =True , cmap = 'summer' )","d66ca52e":"#Creating copy of the dataframe df_n\ndf_n = df.copy()","2984e412":"#The following code does the one hot encoding only when the frequency of each category is greater than 50. \n#After encoding then the first column is dropped inorder to avoid the curse of dimensionality.\n#And it deletes the original column after encoding\n\nprint('The Encoding is applied for: ')\nfor col in cat_col[:-1]:\n    freqs=df_n[col].value_counts()\n    k=freqs.index[freqs>50][:-1]\n    for cat in k:\n        name=col+'_'+cat\n        df_n[name]=(df_n[col]==cat).astype(int)\n    del df_n[col]\n    print(col)\n","487e9125":"#EDA is done for our datasets\ndf_n.head()","875cb9a1":"#Checking our Target column\ndf_n.groupby(by = 'income').count()","eff782bb":"#Mapping income greater than 50k as 1\ndf_n['income'] = df_n['income'].map({'<=50K' : 0 , '>50K' : 1})","0deec54d":"    #Splitting the target variable\n\n    X = df_n.drop(columns='income')\n    y = df_n['income']","29ac9f04":"#Scaling the dataset\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX[X.columns] = ss.fit_transform(X[X.columns])","ea5b7133":"X.head()","3ca3152f":"from sklearn.decomposition import PCA\npca = PCA()\ndf_pca = pca.fit_transform(X)","f00c4283":"#Checking for cumulative explained varience ratio to choose the best n components for PCA\n\npd.DataFrame(np.cumsum(pca.explained_variance_ratio_)*100 , index = range(1, X.shape[1]+1) , columns=['Cum. Var']).T","bc1d91f7":"#We will choose 54 PC dimensions since 99% varience is explained in 54 pc dimensions\npca = PCA(n_components=54)\ndf_pca1 = pca.fit_transform(X)\ndf_pca1 = pd.DataFrame(df_pca1 , columns=['PC '+str(i) for i in range(1,55)])","012fd230":"display(df_pca1.head())\ndisplay(df_pca1.shape)","1104a894":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_pca1, y, test_size=0.3, random_state=3 )\nprint('X_train shape',X_train.shape)\nprint('y_train shape',y_train.shape)\nprint('X_test  shape',X_test.shape)\nprint('y_test  shape',y_test.shape)","61dbb359":"X1 = pd.concat([df_pca1, y],axis=1)\nX1 = X1.sample(frac=0.1, replace=True, random_state=3) \nXt = X1.drop(columns = 'income')\nyt = X1['income']\n\n#we are taking 10% of data with replacement as a sample \n#This is done here to reduce the run time , Since it takes more than 3 hours for a single randomized search to be done.\n# This increases the risk of overfitting (the hyperparameters) on that specific test set result. So randomized search needs to be run on full dataset\nprint(Xt.shape)\nprint(yt.shape)","9f6ddaef":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n\nrfc = RandomForestClassifier(random_state=3)\nparams = { 'n_estimators' : sp_randint(50 , 200) , \n           'max_features' : sp_randint(1 , 54) ,\n           'max_depth' : sp_randint(2,15) , \n           'min_samples_split' : sp_randint(2,30) ,\n           'min_samples_leaf' : sp_randint(1,30) ,\n           'criterion' : ['gini' , 'entropy']\n    \n}\n\nrsearch_rfc = RandomizedSearchCV(rfc , param_distributions= params , n_iter= 200 , cv = 3 , scoring='roc_auc' , random_state= 3 , return_train_score=True , n_jobs=-1)\n\nrsearch_rfc.fit(Xt,yt)","d14631c9":"#The best parameters are,\nrsearch_rfc.best_params_","373c27a6":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV , GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\nknn = KNeighborsClassifier()\n\nparams = {\n    'n_neighbors' : sp_randint(1 , 20) ,\n    'p' : sp_randint(1 , 5) ,\n}\n\nrsearch_knn = RandomizedSearchCV(knn , param_distributions = params , cv = 3 , random_state= 3  , n_jobs = -1 , return_train_score=True)\n\nrsearch_knn.fit(Xt , yt)","51728e25":"#The best parameters are,\nrsearch_knn.best_params_","50308eba":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(random_state=3)\nparams = { 'max_features' : sp_randint(1 , 54) ,\n           'max_depth' : sp_randint(2,15) , \n           'min_samples_leaf' : sp_randint(1,30) ,\n           'criterion' : ['gini' , 'entropy']\n    \n}\n\nrsearch_dtc = RandomizedSearchCV(rfc , param_distributions= params , n_iter= 200 , cv = 3 , scoring='roc_auc' , random_state= 3 , return_train_score=True , n_jobs=-1)\n\nrsearch_dtc.fit(Xt,yt)","7755a8b0":"#The best parameters are,\nrsearch_dtc.best_params_","5456762b":"#Calling remaining classification algorithms\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(fit_intercept=True , solver='liblinear' , random_state=3)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt= DecisionTreeClassifier(**rsearch_dtc.best_params_ , random_state=3)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(**rsearch_rfc.best_params_ , random_state=3)\n\nfrom sklearn.naive_bayes import GaussianNB\ngb = GaussianNB()\n\nfrom sklearn.neighbors import KNeighborsClassifier\n#knn = KNeighborsClassifier(**rsearch_knn.best_params_ )\nknn = KNeighborsClassifier()\n\nfrom sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(random_state=3)\n\nimport lightgbm as lgb\nlgbm = lgb.LGBMClassifier(random_state=3)","66ea335f":"#Executing various classification algorithms , Test train overall accuracy score , Test train AUC score and ROC AUC curve\n\nfrom sklearn.metrics import confusion_matrix, classification_report , accuracy_score , roc_auc_score ,roc_curve\n\nalgo_arr = [lr ,dt, rfc , gb ,knn , ada ,lgbm]\n\nl={0:'LogisticRegression' ,1:'Decision Tree', 2:'Random Forest' , 3:'Gaussian NB' ,4:'KNN' , 5:'AdaBoost' , 6:'Lightbgm'}\nmetric=[]\n    \nfor i in range(len(algo_arr)):\n    algo_arr[i].fit(X_train , y_train)\n\n    y_train_pred=algo_arr[i].predict(X_train)\n    y_train_prob=algo_arr[i].predict_proba(X_train)[:,1]\n\n    print('\\t','\\t','\\t','\\t','\\t','\\t','\\t',l[i])\n    print('Confusion Matrix - Train' , '\\n' , confusion_matrix(y_train,y_train_pred))\n\n    print('Overall Accuracy - Train ', accuracy_score(y_train,y_train_pred))\n    metric.append(accuracy_score(y_train,y_train_pred))\n\n    print('AUC - Train: ', roc_auc_score(y_train,y_train_prob))\n    metric.append(roc_auc_score(y_train,y_train_prob))\n\n    print('\\n')\n\n    y_test_pred = algo_arr[i].predict(X_test)\n    y_test_prob=algo_arr[i].predict_proba(X_test)[:,1]\n\n    print('Confusion Matrix - Test ', '\\n' , confusion_matrix(y_test,y_test_pred))\n\n    print('Overall Accuracy - Test ', accuracy_score(y_test,y_test_pred))\n    metric.append(accuracy_score(y_test,y_test_pred))\n\n    print('AUC - Test: ', roc_auc_score(y_test,y_test_prob))\n    metric.append(roc_auc_score(y_test,y_test_prob))\n    \n    #ROC AUC Curve\n    fpr , tpr , threshold = roc_curve(y_test , y_test_prob)\n    plt.plot(fpr , tpr)\n    plt.plot(fpr , fpr , 'r-')\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.show()","b5d044f2":"#Creating accuracies as dataframe for easy interpretation\nal=['LogisticRegression','Decision Tree','Random Forest', 'Gaussian NB','KNN','AdaBoost','Lightbgm']\n\ncol=['Train_Accuracy_score', 'Test_Accuracy_score','Train_AUC_score','Test_AUC_score']\n\nme=np.array(metric).reshape(7,4)\n\nc_t=pd.DataFrame(np.round(me,2),columns= col , index=al)","b4fe9ecc":"#Comparing various algorithms scores\nc_t","73f1cac0":"**FOR CONTINUOUS VARIABLES**","ee60fb11":"There are no missing NaN values in the dataset","a7736f2c":"There is no correlation with any of the columns","cb146fcf":"**HYPERPARAMETER TUNING (Finding the best paramters using RandomizedSearchCV for Random Forest) :**","26e708ce":"Observation :\n   -  ? in all columns need to be imputed\n   - Education column can be combined\n   - No changes needed for Relationship , Race and Sex columns\n   - Native country columns needs to be binned together since there are very less values for some countries","1bad54a5":"**CHECKING FOR MISSING VALUES**","b322b1a7":"**HYPERPARAMETER TUNING (Finding the best paramters using RandomizedSearchCV for Decision Tree Classifier) :**","7e28c315":"# Dataset Description\n\n### Categorical Attributes :\n\n**1. workclass**: (categorical) Private, Self-emp-not-inc(Unincorporated self employment), Self-emp-inc(Incorporated self employment:), Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n    - Individual work category\n**2. education**: (categorical) Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n    - Individual's highest education degree\n**3. marital-status**: (categorical) Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n    - Individual marital status\n**4. occupation**: (categorical) Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n     - Individual's occupation\n**5. relationship**: (categorical) Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n    - Individual's relation in a family\n**6. race**: (categorical) White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n    - Race of Individual\n**7. sex**: (categorical) Female, Male.\n\n**8. native-country**: (categorical) United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n    - Individual's native country\n\n### Continuous Attributes :\n\n**1. age**: continuous.\n    - Age of an individual\n**2. education-num**: number of education year, continuous.\n    - Individual's year of receiving education\n**3. fnlwgt**: final weight, continuous.\n    - The weights on the CPS files are controlled to independent estimates of the civilian noninstitutional population of the US. These are prepared monthly for us by Population Division here at the Census Bureau.\n    \n    - The term estimate refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. \n    \n**4. capital-gain**: continuous.\n\n**5. capital-loss**: continuous.\n\n**6. hours-per-week**: continuous.\n    - Individual's working hour per week","86940aaf":"**TEST TRAIN SPLIT**","b0b0f1cc":"**FEATURE ENGINEERING**","e1d875ec":"**APPLYING PCA**","7f2cb268":"**HYPERPARAMETER TUNING (Finding the best paramters using RandomizedSearchCV for KNN) :**"}}