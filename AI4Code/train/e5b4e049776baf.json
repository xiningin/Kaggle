{"cell_type":{"63bf1609":"code","4033a131":"code","ef0d4b60":"code","8a8ec18c":"code","28b62de0":"code","8648148a":"code","c8484cac":"code","d93c2cc4":"code","ce2d08f0":"code","66782a69":"code","cb40b229":"code","90bb31a8":"code","236de529":"code","0df4a911":"code","ce748456":"code","41f18d9b":"code","d852f9f5":"code","1da51510":"code","85a4a9d3":"code","741e38e6":"code","395c7bf6":"code","1b22971e":"code","998450e4":"code","e43963dd":"code","63f454ef":"code","5fa5b164":"code","af6f55bb":"code","e736590c":"code","32dc4d2c":"code","fbf6cc9b":"code","70508ce8":"code","137ea8bc":"code","b2951d80":"code","7966871d":"code","e380b8a4":"code","6d10caf6":"code","e1af8c4f":"code","4e479871":"code","ac9ca35a":"code","08f78b15":"code","94551a1c":"code","d81fff1d":"markdown","8f71ff88":"markdown","7a72f454":"markdown","452cb2f2":"markdown","45e809d6":"markdown","93c9c6e5":"markdown","f44c7978":"markdown","8b56e7f8":"markdown","e396d735":"markdown","b45210f6":"markdown","f43bd09d":"markdown","15c12020":"markdown","fa5c131a":"markdown","f4b72866":"markdown","87f02c9b":"markdown","1691a62d":"markdown","432b891a":"markdown","3d57c903":"markdown","d1dcd986":"markdown","02857cfe":"markdown","00c2b598":"markdown","59314e31":"markdown","12605ec0":"markdown","bd64e9f1":"markdown","64f9bc88":"markdown","cab171de":"markdown","e4e952b9":"markdown","824b69c6":"markdown","e6bc50eb":"markdown","2885474d":"markdown","b035823e":"markdown","ac840ef9":"markdown","0d5dcff8":"markdown","19b4e29b":"markdown","0361f211":"markdown","bec3611d":"markdown","e45d5c8e":"markdown","3dfa3834":"markdown","ec391b15":"markdown","11445754":"markdown","6c3b97ed":"markdown","f4daf7e5":"markdown","4d9306aa":"markdown","4b3c4793":"markdown"},"source":{"63bf1609":"# required import  \n\nimport keras  #this package must be installed first using your package manager\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nimport sklearn\nfrom sklearn.datasets import make_classification, make_circles\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\n","4033a131":"# a function that plots decision boundary and encircle support vectors of SVM algorithm\n\ndef bndry(x,y,mdl):\n    \n    plt.scatter(x[:, 0], x[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# plot the decision function\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n# create grid to evaluate model\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = mdl.decision_function(xy).reshape(XX.shape)\n\n# plot decision boundary and margins\n    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n           linestyles=['--', '-', '--'])\n# plot support vectors\n    ax.scatter(mdl.support_vectors_[:, 0], mdl.support_vectors_[:, 1], s=100,linewidth=1, facecolors='none', edgecolors='k')\n    plt.show()\n    \n","ef0d4b60":"X,Y= make_classification(n_samples=100, n_features=2,n_informative=2, n_redundant=0, n_clusters_per_class=1,weights=np.array([0.5]),\\\n                        flip_y=0.001, class_sep=2,random_state=69)\n# show the database's head\n# df=pd.DataFrame(np.append(x, y.reshape(100,1), axis=1),columns=[\"v\"+str(i) for i in range(1,4)])\ndf = pd.DataFrame(dict(v1=X[:,0], v2=X[:,1], target=Y))\ndf.head()   # show how our data looks","8a8ec18c":"sb.lmplot(\"v1\",\"v2\",df,\"target\",fit_reg=False) #scatter plot of our data","28b62de0":"X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.3,random_state=42) ","8648148a":"mdl=svm.SVC(kernel='linear')  # choose the linear kernel, C is set on default value\nmdl.fit(X_train,Y_train)          # fit the model on the training set\npredictions =mdl.predict(X_test)    # predict the labels of x_test\n\nbndry(X_train,Y_train,mdl)          # plot the decision function ","c8484cac":"print(confusion_matrix(Y_test,predictions))\nprint(classification_report(Y_test,predictions))\nprint(\"the outsample accuracy = \", accuracy_score(Y_test,predictions))\nprint(\"the insample accuracy = \", accuracy_score(Y_train,mdl.predict(X_train)))","d93c2cc4":"X,Y= make_classification(n_samples=100, n_features=2,n_informative=2, n_redundant=0, n_clusters_per_class=1,weights=np.array([0.5]),\\\n                        flip_y=0.4, class_sep=1.5,random_state=69)\ndf = pd.DataFrame(dict(v1=X[:,0], v2=X[:,1], target=Y))\nsb.lmplot(\"v1\",\"v2\",df,\"target\",fit_reg=False)","ce2d08f0":"X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.3,random_state=42) \nmdl=svm.SVC(C=1,kernel='linear')\nmdl.fit(X_train,Y_train)\npredictions =mdl.predict(X_test)   ","66782a69":"bndry(X_train,Y_train,mdl) ","cb40b229":"def classify(o,sep,w):\n    X,Y= make_classification(n_samples=1000, n_features=2,n_informative=2, n_redundant=0, n_clusters_per_class=1,weights=w,\\\n                        flip_y=o, class_sep=sep,random_state=69)\n    X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.3,random_state=42) \n    mdl=svm.SVC(C=1,kernel='linear')\n    mdl.fit(X_train,Y_train)\n    predictions =mdl.predict(X_test)\n    numberSV=mdl.n_support_.sum()\n    out_acc=accuracy_score(Y_test,predictions)\n    in_acc=accuracy_score(Y_train,mdl.predict(X_train))\n    return numberSV,out_acc,in_acc\n\ntrn=[]\ntst=[]\nnSV=[]\nl=[0.01, 0.1, 0.4, 0.7, 1]\nfor o in l:\n    clf=classify(o,1,np.array([0.5]))\n    nSV.append(clf[0])\n    tst.append(clf[1])\n    trn.append(clf[2])\n\n\nfig, ax = plt.subplots(figsize=(10,7))\nax2 = ax.twiny()\nax2.set_xlabel(r\"proprtion of mislabeled data\")\nax.plot(nSV,trn , label='train')\nax.plot(nSV,tst, label='test')\nax.set_xlabel(r\"number of SV\")\nax.set_ylabel(r\"accuracy\")\nax.legend(loc=\"upper right\", frameon=False)\nplt.show()","90bb31a8":"print(accuracy_score(Y_test,predictions))\nprint(recall_score(Y_test,predictions))\nprint(precision_score(Y_test,predictions))","236de529":"for c in [0.1, 10, 100 , 10000]:\n    mdl=svm.SVC(C=c,kernel='linear')\n    mdl.fit(X_train,Y_train)\n    predictions =mdl.predict(X_test) \n    print(\"C= \",c,\", accuracy= \", accuracy_score(Y_test,predictions) )","0df4a911":"X,Y= make_classification(n_samples=700, n_features=2,n_informative=2, n_redundant=0, n_clusters_per_class=1,weights=np.array([0.9]),flip_y=0.001, class_sep=1.5,random_state=67) \ndf = pd.DataFrame(dict(v1=X[:,0], v2=X[:,1], target=Y)) \nsb.lmplot(\"v1\",\"v2\",df,\"target\",fit_reg=False)","ce748456":"X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.3,random_state=48) \nmdl=svm.SVC(C=1,kernel='linear')\nmdl.fit(X_train,Y_train)\npredictions =mdl.predict(X_test)  \nbndry(X_train,Y_train,mdl) ","41f18d9b":"print(confusion_matrix(Y_test,predictions))\nprint(accuracy_score(Y_test,predictions))\nprint(recall_score(Y_test,predictions))\nprint(precision_score(Y_test,predictions))\n","d852f9f5":"for s in [3000, 5000, 15000 , 30000, 45000]:\n    X,Y= make_classification(n_samples=s, n_features=2,n_informative=2, n_redundant=0, n_clusters_per_class=1,weights=np.array([0.9]),flip_y=0.001, class_sep=1.5,random_state=67) \n    X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.3,random_state=23) \n    mdl=svm.SVC(C=1,kernel='linear')\n    mdl.fit(X_train,Y_train)\n    predictions =mdl.predict(X_test) \n    print(\"size= \",s,\", accuracy= \", accuracy_score(Y_test,predictions),\" ,precision= \", precision_score(Y_test,predictions),\" ,recall= \",recall_score(Y_test,predictions)  )","1da51510":"X,Y= make_circles(n_samples=500,noise=0.2, random_state=0,factor=0.4)\ndf = pd.DataFrame(dict(v1=X[:,0], v2=X[:,1], target=Y))\nsb.lmplot(\"v1\",\"v2\",df,\"target\",fit_reg=False)","85a4a9d3":"X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.2,random_state=23) \n\nfor k in [\"rbf\",\"poly\", \"sigmoid\"]:\n    mdl=svm.SVC(C=10,kernel=k)\n    mdl.fit(X_train,Y_train)\n    predictions =mdl.predict(X_test)\n    \n    print(\"{} kernel,   accuracy = {}\".format(k,accuracy_score(Y_test,predictions)))\n    bndry(X_train,Y_train,mdl)\n","741e38e6":"x,y= make_classification(n_samples=5000, n_features=8,n_informative=8, n_redundant=0, n_clusters_per_class=1,weights=np.array([0.9]),flip_y=0.2, class_sep=1,random_state=34)\n","395c7bf6":"#prepare the train and test data\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n","1b22971e":"\n# instanciate  sequantial model\nmodel = Sequential()  \n# build up the layers one by one\nmodel.add(Dense(8, activation='relu', input_shape=(8,)))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","998450e4":"model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\nhistory=model.fit(x_train,y_train,epochs=20,batch_size=32, validation_data=(x_test, y_test))","e43963dd":"# predictions on test data.\ny_pred = model.predict_classes(x_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"accuracy = \",accuracy_score(y_test,y_pred))\nprint(\"precision = \",precision_score(y_test,y_pred))\nprint(\"recall = \",recall_score(y_test,y_pred))","63f454ef":"def gen(o):\n    x,y= make_classification(n_samples=5000, n_features=8,n_informative=8, n_redundant=0, n_clusters_per_class=1,weights=np.array([o]),flip_y=0.2, class_sep=1,random_state=34)\n    x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n    model = Sequential()  \n    model.add(Dense(8, activation='relu', input_shape=(8,)))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n    model.fit(x_train,y_train,epochs=20,batch_size=32,verbose=0)\n    y_pred = model.predict_classes(x_test)\n    return recall_score(y_test,y_pred),recall_score(y_train, model.predict_classes(x_train))\ntrn=[]\ntst=[]\nl=[0.99, 0.9, 0.8, 0.7, 0.6,0.5]\nfor o in l:\n    clf=gen(o)\n    tst.append(clf[0])\n    trn.append(clf[1])\n\n\nfig, ax = plt.subplots(figsize=(10,7))\nax.set_xlabel(r\"proprtion of mislabeled data\")\nax.plot(l,trn , label='train')\nax.plot(l,tst, label='test')\nax.set_xlabel(r\"proprtion of negative class in the dataset\")\nax.set_ylabel(r\"Recall\")\nax.legend(loc=\"upper right\", frameon=False)\nplt.show()","5fa5b164":"\nx,y= make_classification(n_samples=5000, n_features=8,n_informative=8, n_redundant=0, n_clusters_per_class=1,weights=np.array([0.5]),flip_y=0.8, class_sep=0.5,random_state=20)\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3,random_state=89)\n\nmodel = Sequential()  \nmodel.add(Dense(8, activation='relu', input_shape=(8,)))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(x_train,y_train,epochs=100,batch_size=32,verbose=0)\ny_pred = model.predict_classes(x_test)\n","af6f55bb":"\nprint(confusion_matrix(y_test,y_pred))\nprint(\"outsample accuracy = \" ,accuracy_score(y_test,y_pred))\nprint(\"insample accuracy = \" ,accuracy_score(y_train, model.predict_classes(x_train)))\nprint(\"outsample recall = \" ,recall_score(y_test,y_pred))\nprint(\"insample recall = \" ,recall_score(y_train,model.predict_classes(x_train)))","e736590c":"url = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00471\/Data_for_UCI_named.csv\"\n# import dataset directly from the repository to pandas dataframe\ndata = pd.read_csv(url)\n\n# drop out the stab variable\ndata.drop(\"stab\",axis=1,inplace=True) \n\n#encode the target to dummy variable\ndata['stabf'] = data['stabf'].replace({'unstable': 0, 'stable': 1}) \ndata.head()","32dc4d2c":"      \n#to verify the correlation between variables we display the correlation matrix\nplt.figure(figsize=(11,5))\nsb.heatmap(data.iloc[:,:-1].corr(), annot=True)\nplt.show()","fbf6cc9b":"data.drop(\"p1\",axis=1,inplace=True) ","70508ce8":"print(sb.countplot(x=\"stabf\", data=data))\nprint(\"proportion of each class\")\nprint(data['stabf'].value_counts(normalize=True))","137ea8bc":"# data split\nx = data.iloc[:, :10]\ny = data.iloc[:, 11]\nx_train, x_test, y_train, y_test=train_test_split(x,y, stratify=y,test_size=0.2,random_state=42) \n\nprint(y_train.value_counts(normalize=True))\nprint(y_test.value_counts(normalize=True))","b2951d80":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","7966871d":"# instantiate an SVM model\nmodel = svm.SVC()\n\n#candidate values of hyperparameters\nparameters = [{'C' : [1, 10, 100, 1000],'gamma': [1, 0.1, 0.01, 0.001],'kernel': ['rbf']},\n             {'C': [1, 10, 100, 1000], 'kernel': ['linear']}]\n             \ngrid = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\ngrid.fit(x_train, y_train)","e380b8a4":"print('Best C:',grid.best_estimator_.C) \nprint('Best Kernel:',grid.best_estimator_.kernel)\nprint('Best Gamma:',grid.best_estimator_.gamma)","6d10caf6":"model=svm.SVC(C=1,kernel='rbf',gamma=0.1,probability=True,class_weight='balanced')\nmodel.fit(x_train,y_train)\ny_pred =model.predict(x_test)\ny_probas=model.predict_proba(x_test)[::,1]\n\nprint(confusion_matrix(y_test,y_pred))\nprint(\"outsample accuracy = \" ,accuracy_score(y_test,y_pred))\nprint(\"insample accuracy = \" ,accuracy_score(y_train, model.predict(x_train)))\nprint(\"outsample recall = \" ,recall_score(y_test,y_pred))\nprint(\"insample recall = \" ,recall_score(y_train,model.predict(x_train)))\n\n","e1af8c4f":"# Instantiate the model\nclassifier = Sequential()\n\n# Input layer and first hidden layer\nclassifier.add(Dense(units = 20, kernel_initializer = 'uniform', activation = 'relu', input_dim = 10))\n\n# Second hidden layer\nclassifier.add(Dense(units = 10,kernel_initializer = 'uniform', activation = 'relu'))\n# Single-node output layer\nclassifier.add(Dense(units = 1, activation = 'sigmoid'))\n\n# FNN compilation\nopt = keras.optimizers.SGD(learning_rate=0.1)\nclassifier.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n","4e479871":"history=classifier.fit(x_train, y_train, epochs=35, verbose=0,validation_data=(x_test,y_test))\ny_predfnn = classifier.predict_classes(x_test)\ny_probas_nn=classifier.predict(x_test)\n\nprint(\"outsample accuracy = \",accuracy_score(y_test,y_predfnn))\nprint(\"insample accuracy = \",accuracy_score(y_train,classifier.predict_classes(x_train)))","ac9ca35a":"print(confusion_matrix(y_test,y_predfnn))\n","08f78b15":"# evaluate the model\ntrain_acc = classifier.evaluate(x_train, y_train, verbose=0)\ntest_acc = classifier.evaluate(x_test, y_test, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))\n# plot loss during training\nplt.subplot(211)\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy during training\nplt.subplot(212)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","94551a1c":"from sklearn import metrics\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_probas)\n\nfpr_nn, tpr_nn, _ = metrics.roc_curve(y_test,  y_probas_nn)\n\nauc = metrics.roc_auc_score(y_test, y_probas)\n\nauc_nn = metrics.roc_auc_score(y_test, y_probas_nn)\n\nplt.plot(fpr,tpr,label=\"SVM, auc=\"+str(auc))\nplt.plot([0.0, 1.0],'b--',label=\"random guess model\")\nplt.plot(fpr_nn,tpr_nn,label=\"FNN, auc=\"+str(auc_nn))\n\nplt.legend(loc=4)\nplt.xlabel(r\"False Positive Rate\")\nplt.ylabel(r\"True Positive Rate\")\nplt.title('ROC curves')\nplt.show()\n","d81fff1d":"The C parameter is used as a penalty during the fit of the model, i.e the finding of the decision boundary. By default, each class has the same weighting, and the model try to learn from the two data classes in a balanced way.  \nNow given that there are significantly more examples in the majority class than the minority class, this means that there are no enough data belonging to the minority class to learn from, and the classifier will favor the majority class. as we see in the figure below, the decision boundary is skewed to the left (towards the minority class); \nAnother point is that some metrics can give misleading results in term of outsample performances, for example if the classifier in our example classify all new samples in class 0 (the majority class) the overall accuracy will be 90%, that's why in such cases of high imbalance level it's recomended to examine other metrics like precision due to its sensitivity to class imbalance because it considers the number of negative samples incorrectly labeled as positive. But precision alone is insufficient because it provides no insight into the number of samples from the positive group that were mislabeled as negative, On the other hand, Recall should be taken into account because it is only dependent on the positive group.\n","8f71ff88":"<a id=\"1\"><\/a> <br>\n# Introduction\n","7a72f454":"With a clean data (no mislabels or outliers),the number of support vectors is very small, in this case the model shows high performances on trainset as well as on testset, but with an increasing proportion of mislabeled data , the number of support vectors increases as well and the performance decreases in both sets.\nReturning to our example, the model performs very badly on the test set (only 76% are correctly classified) even with different values of C","452cb2f2":"the overall accuracy reached is 100% and the outsample error $\\hat E_{out}  = 0$, the algorithm can perfectly generalize ","45e809d6":" ##### 1.2 Data is linearly nonseparable (soft margin)\n \nin real life application we can not expect a clear-cut separation between the two classes, instead because of noise, we can have overlapping classes, so some of the constraints related to the previous model will be violated, to have more flexible formulation of the problem we introduce nonnegative slack variables ${\\xi}_{i}$ , one for each observation $({x}_{i},{y}_{i})$\n\nthe optimization problem become  \n\n\n\n<center> $ minimize$  $\\frac{{||\\beta||}^2}2 +C\\sum_{i} {\\xi}_{i}  $<\/center>\n       $ subject$  $to $    \n<center> $ {y}_{i}({\\beta}_{0} + {x}_{i}^{T}\\beta) \\ge 1-{\\xi}_{i}$ ,  $ i=1,...,n$ <\/center>  \n<center> $ {\\xi}_{i} \\ge 0$ <\/center>  \n\nthe idea here is to find an optimal hyperplane by maximizing the margin while softly penalizing misclassified points , thus for data points obeying the hard margin constraints ${\\xi}_{i}=0$, the hyper-parameter $C$ is tunned by the user, for small values the margin becomes wide and tolerate more misclassifications. Generally $C$ is determined using validation set if datasize is huge enough, otherwise, using cross-validation technique.  \n\nIn this case the set of support vectors are defined by {$x^* :  {y}_{i}({\\beta}_{0} + {x^*}^{T}\\beta) - (1-{\\xi}_{i})=0 $}   \n\n ","93c9c6e5":"# Comparaison using ROC curve","f44c7978":"among the kernel functions, the rbf (gaussian) kernel gives a clear separation cut, with an outsample accuracy of 94%.  \n  ","8b56e7f8":"like all artificial neural networks, FNN needs reasonably large data to learn and be able to generalize, by means the make_classification function we generate binary classification task of size 5000 and 8 features, with only 10% of positive class","e396d735":"The difference between the outsample scores and insample scores is not really considerable, they are almostly equal, we can say that the model gets to learn correctly and is not overfit or underfit.\n   \n> NB : if you run the model, your results may vary given the stochastic nature of the training procedure","b45210f6":"# To Be Continued...","f43bd09d":"the two first layers contains 8 nodes each and use ReLu activation function, since features are numerical real numbers within ranges, the output is binary hence the last layer is a sigmoid output layer with one node , the input_shape indicate the number of features.  \n  \nNow, after creating model we need to call its compile() method to specify the loss function and optimizer to use, and it's ready to train !  \n  \n* Loss : indicates the loss function to be minimized while learning the weights\n* optimizer : the algorithm used to minimize the loss function\n* metrics : the metric that measures the model insample performance\n* epoch : number of iteration (passes through the training dataset.)\n* batch_size : when specified, the data is divided into equal-sized batchs and processed in parallel, so 1 epoch= #of batches","15c12020":"To construct the FNN model we use keras's sequential model data structure, this tool helps in building up a stack of linear layers one bye one.","fa5c131a":"## 3.4 Feedforward neural network : impact of class imbalance on FNN performance","f4b72866":"# FNN model","87f02c9b":"# ","1691a62d":"<img src= \"https:\/\/lh3.googleusercontent.com\/proxy\/oQzNWc06hWUHGJFnv_TDAmr2dAdKjDp-xYzHlHqhw-Jp-tPN5jT3662f1-ocUzh1jXpG-oTKyZeiFGX-bewEnpDnmZFXMwAYtonepds_jy-rD2keO6dbb0hC7vmNrf3JQeGfbPVjAUs\" alt =\"class\" style='width: 900px;'>\n","432b891a":"FNN have a lot of hyperparameters to tune (from the external architecture to the hidden parameters of each function (ex:optimizer)), so it's very hard (may be impossible) to cross-validate every single value candidate. that's why i have tried several combinations (not all possible ones) and pick out the one that led to the best prediction performance on the test set.","3d57c903":"In this section  we will use \"Electrical Grid Stability Simulated Dataset\" hosted in University of California (UCI) Machine Learning [Repository](https:\/\/archive.ics.uci.edu\/ml\/index.php).  this dataset of dimendion 10000 contains 12 features and two target variables, \"stab\" variable for regression tasks which is unuseful for our case and \"stabf\" for classification task.\nthe purpose is to construct a model able to classify a DSGC as stable or unstable basing on 3 metrics measured for 4 participants to the DSGC ( one )  \n* tau[x] : reaction time of participant x, x=1,2,3,4\n* p[x] : nominal power produced or consumed by participant x, x=1,2,3,4\n* g[x] : price elasticity coefficient for each network participant x, x=1,2,3,4\n\nDecentral Smart Grid Control (DSGC) is an intelligent electricity network able to match electricity demand and supply decentrally and on an entirely self-organised basis, to prevent the hacker attacks in one side and ensure a constant supply independant of any disturbing factors in the other side.  \nto evaluate the efficiency of such grid in real life, the physicists developed a mathematical model in which they simulated a 4-node star architecture, comprising one power source (a centralized generation node) supplying energy to three consumption nodes (the participants), the model takes into consideration inputs (features) described above.   \n\n\nThe dataset has a synthetic nature so there is no mislabeled samples; at first step, we begin by an exploratory analysis, afterwards, we'll try to construct two predictive models using SVM and FNN, and compare their performance. ","d1dcd986":"## 3.1 Impact of mislabeled training examples on the SVM performances\n\nas we have seen in theory section, the creation of the decision boundary relies on support vectors, which means that the labels of these vectors are very important and are responsible for the performane of the algorithm, \nin case of the existence of mislabeled data points, there is a big chance that these data points will be nominated as support vectors since they are borderline examples.  \nto see how this really impact the performance, we modify some examples label  in the training set \n","02857cfe":"\n# 4. Real world dataset ","00c2b598":"## 3.3 Class overlap","59314e31":"## SVM model\n\na great way to find out the hyperparameters and kernel that fit the best our training set, is to conduct a grid search using scikit-learn\u2019s GridSearchCV.\n  \n> !!! the run time takes about 4 min !!!","12605ec0":"## 3.2 Impact of class imbalance\n\nto simulate the case of imbalanced data classes, we generate a sample where class 0 represent 90% of the data and class 1 represent 10%.","bd64e9f1":"class 1 represents only 36% of the total dataset, to handle this imbalance, first we use \"stratify\" argument in \"train_test_split\" sickitlearn's method to preserve the data distribution after split, second, in the training step, we'll use class weight in loss function to penalize mistakes on the minority class;\n","64f9bc88":"the recall score is dramatically low (0.45) even if precision score is high, the number of false negative is relatively considerable compared to the number of false positive.  \nin the next plot, we see how the recall metric (outsample and insample evaluation) decreases with high level of class imbalance.","cab171de":"in order to speed up the calculations and avoid the domination of features with large spread of values, we'll use standardization method of sickitlearn to rescale the feature space to be on the same scale.","e4e952b9":"we can clearly notice the non linearity of the separating hyperplane , so by means the kernel SVM method we'll try to determine which kernel function can construct the best classifier in terms of outsample accuracy","824b69c6":"The difference between the outsample and insample scores is inevitable given hyperparameter tuning, making the training scores optimistic, but in general the scores are close which prooves that the model is probably not overfit or underfit","e6bc50eb":"in the theory section we've talked about the situation when data classes are overlapped and don't accept linear separation, let's see how this data looks and how the kernel trick can solve the problem","2885474d":"class imbalance is a serious problem that impacts negatively any classification model performance if it's combined with other problems like existence of outliers and class overlap, it usually needs to be cured before the training step, there are many technique to do that, but not always work in case of severe level of imbalance.\nin the next section when we'll use realworld dataset, we'll try to figure out some of these techniques","b035823e":"## 3.5 Impact of noisy data  on FNN performance","ac840ef9":"In order to visualize the performance of the classifiers and compare them we use ROC curve. It is a representation of true positive rate (TPR) as a function of the false-positive rate (FPR), And it\u2019s useful only when compared with other curves  \nThis representation puts forward a new indicator called the area under the curve (AUC). The closer it gets to 1, the better is the classifier, in other words, the larger the air under the ROC curve of a model, the better this model is. \na random classifier will have an equivalent ROC curve to the blue dashed line.   \nas we see, both classifiers show aproximately the same performance (same auc), with a slight difference at the inflection point. ","0d5dcff8":"##### training\nTo train the model we split the data set to 2 parts : 70% for training set, and 30% for testing, \nthe validation techniques will be employed in the next section when we use real world dataset with size\n","19b4e29b":"we start by generating a noisy sample with many mislabeled data and class overlap and then instantiate an FNN model.","0361f211":"Noise in dataset leads to poor accuracy of the classifier , adding more layers and nodes to the model or even a high number of epochs (100 epochs in the example) would probably improve the insample performance, but this leads to overfit and incapacity of generalization on unseen data, this explains the difference between the outsample metrics and insample metrics.","bec3611d":"\n# 3.Application on synthetic data \n\n in this section we will focus on the impact of some data problems on the algorithm performance, to do so we generate small data sizes with 2 features to be able to make clear visualisations.\n\n### data generation\n\nwe will use make_classification function from sklearn.datasets library to generate a random 2-class classification problem. This function allows controlling the level of noisiness and a lot of bias generators like outliers and class imbalance by tuning its parameters, for example\n\n* weights: control the proportions of samples assigned to each class\n* flip_y: the degree of noisiness, the higher the value the harder is the classification task, wich means for large values the data can not be linearly separable and need nonlinear methods\n* class_sep: control the distance between classes, for small values classes are overlaped\n \n### simple case  \n> In first place we, we treat the simplest case, we tune the parameters to have a linearly separable data classes","e45d5c8e":"This notebook is adressed to beginners who have just started their journey in machine learning, and having confused ideas about classification vocabulary in particular, in which i have tried to present typical data anomalies that you may face during your journey and what impact this may have on your model performance.\n\nThe first part is reserved to a synthetic dataset, so we can modify it as needed and generate different types of anomalies in order to understand basics of binary classification, afterwards we'll apply what we've learned to a real world dataset, for these task, i have chosen two widely popular models;\n\n* Support vector machines\n* Feed forward neural networks\n\nthese algorithms are already implemented in python ML libraries, and are ready to use, but before doing this, we first explain how each model get to learn from data, and what are the parameters of each one.  \n  \n\n\n\n\ud83d\udccd\ud83d\udccd\ud83d\udccd Please don't hesitate to let me know if you have any question, or need further clarification.","3dfa3834":" ##### 1.3 Nonlinear SVM\n \nin some cases (actually the more realisitic ones) data classes are entirly overlapped and there is no way to separate them by an hyperplan despite allowing some classification errors, to handle this problem, we use a nonlinear mapping $ \\Phi : {R}^{p} \\to \\mathcal{H}  $ to map each observation ${x}_{i} \\in {R}^{p}$ into a higher dimensional feature space $\\mathcal{H}$ ;  \n\n$\\mathcal{H}$ will be assumed to have the structure of a Hilbert space of real valued functions on ${R}^{p}$ with scalar product and norm.  \n**the fantastic idea of nonlinear SVM is that the separation of the classes by means of an optimal separating hyperplan will be easier in that new feature space**.  \nThe transformed sample is {$\\Phi({x}_{i}),{y}_{i}$}, and can be used to find the separating hyperplan (same previous procedure), but while resolving the optimization problem, the term  $ \\Phi({x}_{i})^{T}\\Phi({x}_{j})$ may be very hard to compute and sometimes impossible in the case of $\\mathcal{H}$ infinite-dim\nthis difficulty can be avoided by the kernel trick, it consists in computing the scalar product by means a nonlinear kernel function $K({x}_{i},{x}_{j}) = \\Phi({x}_{i})^{T}\\Phi({x}_{j})$ defined in input space without even knowing the analytical form of $\\Phi$.  \n\nexamples of popular kernels  \n* linear kernel : $K({x}_{i},{x}_{j}) = 1+{x}_{i}^{T}{x}_{j}$  \n  \n  \n* polynomial kernel : $K({x}_{i},{x}_{j}) = ({1+{x}_{i}^{T}{x}_{j} })^{d}$  \n  \n  \n* gaussian kernel : $K({x}_{i},{x}_{j}) = \\exp(-{||{x}_{i}-{x}_{j}||}^2\/2{\\sigma}^2)$\n\nthe hyperparamers of the kernel functions like $d$ and $\\sigma$ are tunned using validation set or cross validation technique\n\n","ec391b15":"the impact of classes imbalance on support vectors is not really remarkable, if the datasize is reasonably large which means having enough information about the minority class , there will be a very little e\ufb00ect on the classi\ufb01er performance, and this's true since classes don't overlap, otherwise the classification task will become harder.  \nIn the following we examine the performance metrics values while increasing the data size","11445754":"# 2. Feedforward neural network\n\n\nfeedforward neural network (FNN) is an artificial neural network having a layered architecture, the first and latest are called the input and output layers respectively, the interior layers are called hidden layers.  \neach layer is composed of one or more nodes, and each node is connected with all nodes existed in the previous and following layer by real valued weights, but not to nodes in the same layer. FNN are generally implemented with an additional node in each layer, called the bias unit, except the output layer. This node plays the role of the intercept term ${\\beta}_{0}$ in linear models.  \nthe interest of this architecture is the capacity of handling non-linearly separable data and huge datasize\n\nlet\n\n* $p$ the number of inputs or features,\n* $L$ the number of layers,\n* ${H}^{(l)}$ the number of nodes of the lth layer $(l = 1,...,L)$ of the FNN, \n* ${w}^{(l)}_{kv}$ denote the weight of the link connecting the $k$th node in the $l\u22121$ layer and the $v$th node in the $l$ layer, \n* ${z}^{(l)}_{v}$ , $v = 1,...,{H}^{(l)}$ the output of the $v$th hidden node of the lth layer, \n* ${z}^{(l)}_{0}$ denote the bias for the $l, l = 1,...,L$ layer. \n* Let ${H}^{(0)}=p$ and ${z}^{(0)}_{v}={x}_{v}, v = 0,...,p.$  \n\n\n the output of the $v$th node of the $l$th layer, is  gived by \n \n \n \n <center> ${z}^{(l)}_{v} = {g}^{(l)}(\\sum\\limits_{k=0}^{{H}^{(l-1)}}  {w}^{(l)}_{kv} {z}^{(l-1)}_{k} ) $,     $v = 1,...,{H}^{(l)}$<\/center>    \n \n \n${g}^{(l)}(.)$ is a non linear transformation called the activation function, here we cite the most commonly used for binary classification \n\n  \n\n\\begin{align*}\ng(a) = \\frac{1}{1+e^{-a}} && \\text{Logistic sigmoid} \\\\\ng(a) = \\tanh(a) && \\text{Hyperbolic tangent} \\\\\ng(a) = \n\\begin{cases}\n0 & a < 0 \\\\\na & a \\ge 0 \\\\\n\\end{cases} && \\text{ReLU (Rectified Linear Unit)} \\\\\n\\end{align*}\n\n\nThe set of weights is constructed while the training procedure using the backpropagation algorithm, and the number of hidden nodes $H$ and hidden layers reflects the complexity of the architecture, the greater the number the more complex the architecture. usually they are fixed using cross-validation.\n        \n    \n### Backpropagation  \n\n\nonce the number of hidden nodes is given, the weights are calculted in an iterative way basing on the training set ${D}_{n}$, It is a gradient-based algorithm that aims to minimize the loss function  \nwe notice that the loss function may change according to the learning task, for example, for a binary classification problem, Cross-entropy is the most popular \n\n<center> $ C = L_{cross-entropy}(\\hat y, y) = - \\sum\\limits_{i=1}^n {y}_{i}\\log  ({\\hat y}_{i})$   <\/center>\n\nweights take random initial (small) values , and with each observation $({x}_{i},{y}_{j})$ these values are updated  \n\n<center> $W(t)= W(t-1) - \\eta \\frac{\\partial C}{\\partial W(t-1) }$ <\/center>\n\n    \n> $W(t)$ refers to the weight vector  {${w}^{(l)}_{kv}$}  in iteration t  \n$\\eta$ is an hyperparameter called the learning rate  \n\n\nlike hyperparameters of SVM algorithm, $\\eta$ can be tunned using validation set or cross validation\n       \n\n","6c3b97ed":"support vectors are those points encircled, as we can see all of noisy examples are captured as support vectors, this means that the existence of mislabeled data increases the number of support vectors.   \na high number of support vectors makes the model more complex and computationally expensive; that because, in order to make prediction on a new data point, the model need to compute the dot product of each support vector with the test point.  \n  \nthe following figure gives a quantification of the impact of outliers on SVM performances, we plot the number of support vectors vs the model outsample accuracy and insample accuracy;","f4daf7e5":"\nthe test set allows the calculation of an unbiased estimator of the outsample error $\\hat E_{out} = \\frac{FP+FN}{TP+TN+FP+FN}$   \nbesides,several metrics based on the confusion matrix can be calculated in order to measure the model performance  \n\n* accuracy = $\\frac {TP+TN}{TP+TN+FP+FN}$    \n  \n\n* precision = $\\frac {TP}{TP+FP}$    \n    \n    \n* recall = $\\frac {TP}{TP+FN}$      \n  \n  \n* f1score = $\\frac {2*precision*recall}{precision + recall}$","4d9306aa":"# 1. Support vector machines\n\n### The theory behind\n\nlet the training dataset   ${D}_{n} = \\{({x}_{i},{y}_{j}) ,{i}=1,...,{n}\\}$ with  ${x}_{i} \\in R^{p}  , {y}_{i} \\in \\{-1, 1\\} $\n\nthe objectif of the svm algorithm is to use ${D}_{n}$ to construct a function $ f:R^{p} \\to R $ such that $sign(f)$ is a classifier, each new point $x$ will be classified in one of the two classes depending on the $sign$ of $f(x)$ .  \n\nthe parametric identification of $f$ relies on the minimization of the classification error, we have 3 cases here;\n\n\n ##### 1.1 Data is linearly separable (hard margin)\n \nwe assume that the positive (${y}_{i}=1$) and the negative (${y}_{i}= -1$) examples can be perfectly separated by a hyperplane ${H}_{0} $: {$x:f(x) = {\\beta}_{0}+ {x}^{T}\\beta $ } without error, where $\\beta \\in R^{p} $  is the weight vector and ${\\beta}_{0} \\in R $ is the intercept, in this case there is an infinity of such hyperplans.\n\n to choose the best one the idea here is to separate the two classes with a sufficient margin free of examples wich means find two parallel hyperplans ${H}_{-} and {H}_{+}$, each cross one or more data point (called support vectors) and the separating hyperplan $ H_0 $ is that one in the middle.  \nAnalitically, if the two classes are linearly separable there exist ${\\beta}_{0}$ and  $\\beta$ such that \n*  ${\\beta}_{0} + {x}_{i}^{T}\\beta \\ge 1$  if ${y}_{i} = +1 $  \n*  ${\\beta}_{0} + {x}_{i}^{T}\\beta \\le -1$    if ${y}_{i} = -1 $      \n    > $\\beta $ and ${\\beta}_{0}$ can always be rescaled by multiplying both by a positive number  \n\nthe support vectors ${x}_{*}$ are the data points satisfying $({\\beta}_{0} - 1) + {x}_{*}^{T}\\beta = 0$ and  $({\\beta}_{0} + 1) + {x}_{*}^{T}\\beta = 0$  \n\n![Image](https:\/\/i.stack.imgur.com\/CqGQC.png)\n\nto find out the parameters we want to maximize the margin or the distance $d$ between the two hyperplans defined by their equations  \n                    $ {H}_{-} : ({\\beta}_{0} + 1) + {x}^{T}\\beta = 0$  \n                    $ {H}_{+} : ({\\beta}_{0} - 1) + {x}^{T}\\beta = 0$  \n\nunder the condition  $ {y}_{i}({\\beta}_{0} + {x}_{i}^{T}\\beta) \\ge 1$ ,  $ i=1,...,n$  \n\nwe have $d=\\frac2{||\\beta||}$ and maximize $d$ return to minimize $\\frac{{||\\beta||}^2}2$  \nso the problem can be formulated as following :  \n\n\n<center> $ minimize$ $\\frac{{||\\beta||}^2}2$     <\/center>\n   $\\space    $    $ subject$  $to $    \n <center> $ {y}_{i}({\\beta}_{0} + {x}_{i}^{T}\\beta) \\ge 1$ ,  $ i=1,...,n$ <\/center>\n\nThe optimization problem (OP) is convex (quadratic programm) ,then a global minimum exists and can be calculated using the QP solver implemented in [libsvm](https:\/\/www.csie.ntu.edu.tw\/~cjlin\/libsvm\/) .  \n\nlet $\\beta^*$ and $\\beta_0^*$ be the solution of the OP, then the optimal hyperplane is given by $f^*(x)=0$ with $f^*(x)= \\beta_0^* + x^T \\beta^*$\n\n","4b3c4793":"the matrix reveals the existence of correlation between 'p1' and 'p2', 'p3', 'p4', as expected, because the total power consumed equals the total power generated, p1 (supplier node) = - (p2 + p3 + p4);  so p1 doesn't add any information and will be dropped out;\n"}}