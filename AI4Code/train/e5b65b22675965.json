{"cell_type":{"e557649f":"code","d602fb45":"code","8da6b2e5":"code","ef8889ff":"code","bcf3a95b":"code","e964f672":"code","9c4e7863":"code","1b7d74b1":"code","dda67f38":"code","bbd3d101":"code","da3d8153":"code","ddeb3c1f":"code","ee3f47d0":"code","33a60ad3":"code","34017927":"code","3784b018":"code","f27f8112":"code","4f15af90":"code","f09ea950":"code","3c0cf251":"code","afb3978f":"code","545082d6":"code","51629fbc":"code","e96f49ef":"code","1c4f0fe0":"code","83b39f02":"code","15734b30":"code","fce7112f":"code","74921d1c":"code","0a8faeb9":"code","89d7bfc1":"code","1000e641":"code","c7a73772":"code","1f789b36":"code","cb3112fb":"code","6dbdac42":"code","9bb595df":"code","a9dc7ead":"code","9b4eaab8":"code","3d293f1b":"code","8ec53892":"code","4c3510cc":"code","99a419b2":"code","bdd59824":"code","adf55128":"code","ec459179":"code","45d33763":"code","298a3096":"code","97e63d34":"code","45e1460d":"code","6ce31652":"code","55c7cf6b":"code","6f825273":"code","eb27391f":"code","647dd769":"code","a929de75":"code","e477d185":"code","04576edd":"code","8ed0e6cf":"code","ad039fb2":"code","5d3aeef4":"code","f739e65f":"code","6234ce9e":"code","357b8ff0":"code","72983bcb":"code","ffe17db3":"code","7ae58a07":"code","0452f361":"code","4413fd74":"code","d045f379":"code","16298c0a":"code","915e633a":"code","ccb71102":"code","1da81ea9":"code","9943c0bd":"code","5d42ffa6":"code","e48e60e6":"code","2b7e1922":"code","5e330929":"code","7448f95b":"code","cafb6a61":"code","fceec132":"code","4320d3e3":"code","2e41e191":"code","a15475b4":"code","0ed5eeb1":"code","abe9479b":"code","36801b0f":"code","3d64cf06":"code","bbbd3e54":"code","cf6129e5":"code","4df02db5":"code","f46bb691":"code","4526a567":"code","97259c61":"code","74ae6e0f":"code","6fc3111b":"code","0f69eab6":"code","d8503422":"code","5266b657":"code","e65bc68f":"markdown","35905b3e":"markdown","c38e468d":"markdown","69b98d0c":"markdown","24e4f182":"markdown","a9f31765":"markdown","07b9173a":"markdown","81becc50":"markdown","6c8dcb9d":"markdown","02992a3f":"markdown","80be9253":"markdown","7af22543":"markdown","667503de":"markdown","db11945c":"markdown","c2e5733a":"markdown","bf40ff74":"markdown","dfd763ad":"markdown","b1385890":"markdown","9ed4124b":"markdown","4bc9ae3e":"markdown","a33d0dcc":"markdown","1f618fe9":"markdown","1d9ed346":"markdown","d3d60a44":"markdown","be3204e5":"markdown","09c5720f":"markdown","fb96f48b":"markdown","739343d0":"markdown","e5daedfc":"markdown","fb7c74d6":"markdown","a16b70e1":"markdown","3d7366eb":"markdown","5e8270cb":"markdown","4a7838ac":"markdown","b72b7b4e":"markdown","a98b5795":"markdown","adb327f9":"markdown","aee55dd4":"markdown","19d2736a":"markdown","a7de59ce":"markdown","bcbb7b25":"markdown","ef1aad23":"markdown","33f2595c":"markdown","8a398e09":"markdown","ff9cf0e3":"markdown","d5756213":"markdown","b54b97f6":"markdown","1d2cbe8d":"markdown","14bf4095":"markdown","fa2d2bfc":"markdown","498477f5":"markdown","dea64791":"markdown","10384df3":"markdown"},"source":{"e557649f":"import os\nprint(os.listdir(\"..\/Kaggle_HomeCredit\/infiles\"))","d602fb45":"# import libraries and Load data  \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom time import time\nimport datetime\nimport missingno as msno\n%matplotlib inline\nsns.set(style='whitegrid', palette='muted', font_scale=1.5)\n\ntstart = time()\n\n#from sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict, train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\n\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nimport gc\nimport sys\n#print(sys.base_prefix)\n\nfrom sklearn.metrics import auc, roc_curve, roc_auc_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom imblearn.metrics import classification_report_imbalanced\n\n#from xgboost.sklearn import XGBRegressor\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\n#from scipy.stats import randint\n#import scipy.stats as st\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nLABELS = [\"Normal\", \"Loan Default\"]","8da6b2e5":"file_path = '..\/input\/'\ntrain_file_path = file_path + 'application_train.csv'\ntest_file_path = file_path + 'application_test.csv'\nbureau_file_path = file_path + 'bureau.csv'\nbureau_balance_file_path = file_path + 'bureau_balance.csv'\ncredit_card_file_path = file_path + 'credit_card_balance.csv'\ninstallments_payments_file_path = file_path + 'installments_payments.csv'\nprevious_application_file_path = file_path + 'previous_application.csv'\nPOS_CASH_balance_file_path = file_path + 'POS_CASH_balance.csv'\n","ef8889ff":"def plot_bar_graph(df, feature, feature_label) :\n    val_count = df[feature].value_counts()\n    fig, ax = plt.subplots(figsize=(9,6))\n    sns.set(style=\"darkgrid\")\n    sns.barplot(val_count.index, val_count.values, alpha=0.9, ax=ax)\n    plt.title('Frequency Distribution ')\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xlabel(feature_label, fontsize=12)\n    plt.show()\n    \ndef plot_pie_graph(df, feature) :\n    labels = df[feature].astype('category').cat.categories.tolist()\n    counts = df[feature].value_counts()\n    sizes = [counts[var_cat] for var_cat in labels]\n    fig1, ax1 = plt.subplots(figsize=(10,10))\n    ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\n    ax1.axis('equal')\n    plt.title('Frequency Distribution ')\n    plt.show()\n    \n","bcf3a95b":"\ndef cat_data_distribution(df):\n    tot_cred_len = len(df)\n    for col in df.columns:\n        #print(col,  bureau_data[col].unique())\n        unique_cnt = len(df[col].unique())\n        if unique_cnt <= 25:\n            #if unique_cnt <= 10:\n            print(df[col].value_counts(), '\\n\\n Total : ', df[col].value_counts().sum(), ' out of : ', tot_cred_len )\n            plot_bar_graph(df, col, col)\n\n### Funtion to identify columns with 'Nan' or missing values\n\ndef missing_data_col(df):\n    null_col_list = df.columns[df.isna().any()].tolist()\n    return null_col_list\n\n### Function to Encode or convert Object type column to  numeric \n\ndef convert_category(df):\n    for col in df:\n        le=LabelEncoder()\n        if df[col].dtype == 'object':\n            col_name = df[col].name\n            \n            # replace or mask the 'Nan' values if any before applying the laberEncoder\n            df[col_name] = df[col_name].factorize()[0]\n            # Apply LabelEncoder : fit and transform categorical object data types to numeric \n            le.fit(df[col_name])\n            df[col_name] = le.transform(df[col_name])\n        \n    # print(df.head())\n    return df\n\ndef fill_missing_col(df, col , not_null_list):\n    \n    #Feature set\n    # Split sets into train and test\n    train  = df.loc[ (df[col].notnull()==True) ]# known COLUMN values\n    test = df.loc[ (df[col].isnull()==True) ]# null or unknown COLUMN values\n    \n    # missing values columns are stored in a target array\n    y = train[col] #.values\n    # print('len y ...', y)\n    # All the other values are stored in the feature array\n    X = train[not_null_list]   #.values[:, 1::]\n    # print('len X ...', X)\n    if len(df[col].value_counts()) < 25:\n        # Create and fit a model\n        #model = RandomForestClassifier(n_estimators=20, n_jobs=-1)\n        model = XGBClassifier(n_estimators=10, n_jobs=-1, random_state =101)\n    else:\n        #model = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n        model =  XGBRegressor(n_estimators=10, n_jobs=-1, random_state =101)\n    \n    model.fit(X, y)\n    \n    # Use the fitted model to predict the missing values\n    pred_y = model.predict(test[not_null_list])\n    \n    # Assign those predictions to the full data set\n    df.loc[ (df[col].isnull()), col ] = pred_y \n    \n    del train, test, X,y\n    gc.collect()\n    \n    return df\n\ndef data_preprocessing (df):\n    df = convert_category(df)\n    # Get columns with missing values\n    null_col_list = missing_data_col(df) \n    \n    # Get total column list \n    total_col_list = list(df.columns.values)\n    # Get columns without missing values\n    not_null_list = list(set(total_col_list) - set(null_col_list)) \n    #not_null_list\n    \n    #predicting missing values in age using Random Forest\n    for col in null_col_list:\n        print('Replacing missing value for :', col)\n        df = fill_missing_col(df, col, not_null_list)\n    \n    return df","e964f672":"\n# load data\ntrain_df = pd.read_csv(train_file_path)\nprint (\"Shape of Application Train data : \", train_df.shape)\ntest_df = pd.read_csv(test_file_path)\nprint (\"Shape of Application Test data : \",test_df.shape)\n","9c4e7863":"# cat_data_distribution(train_df)","1b7d74b1":"train_df = train_df[ train_df['CODE_GENDER'] != 'XNA']\ntrain_df = train_df[ train_df['NAME_FAMILY_STATUS'] != 'Unknown']\ntrain_df = train_df[ train_df['FLAG_MOBIL'] == 1]","dda67f38":"sns.set(style='whitegrid', palette='muted', font_scale=1.5)\ncount_classes = pd.value_counts(train_df['TARGET'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0, figsize = (10,6))\nplt.title(\"Transaction Target distribution\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Target\")\nplt.ylabel(\"Frequency\");","bbd3d101":"# tcred = time()\n# create 'TARGET' in test data and assgn value  '-1'\ntest_df['TARGET'] = -1\n\n# Concatenate train data and test data\ncredit_df = pd.concat([train_df, test_df], axis = 0, ignore_index=True)\ncredit_df.shape","da3d8153":"# credit_df['TARGET'].value_counts() # '=1' belongs to test data\ndel train_df, test_df\ngc.collect()\n\n# print( \"Concat train + test :  {} secs\" .format(time() - tcred))","ddeb3c1f":"msno.bar(credit_df);","ee3f47d0":"cred_col = ['AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_INCOME_TOTAL',\n       #'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_WEEK', \n       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',  'AMT_REQ_CREDIT_BUREAU_YEAR',\n       'APARTMENTS_AVG', 'APARTMENTS_MEDI', 'APARTMENTS_MODE',\n       #'BASEMENTAREA_AVG', 'BASEMENTAREA_MEDI', 'BASEMENTAREA_MODE',\n       'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'CODE_GENDER',  'DAYS_BIRTH',\n       'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE',\n       'DAYS_REGISTRATION', 'DEF_30_CNT_SOCIAL_CIRCLE',\n       'DEF_60_CNT_SOCIAL_CIRCLE', 'EMERGENCYSTATE_MODE',\n       # 'ELEVATORS_AVG', 'ELEVATORS_MEDI','ELEVATORS_MODE',  \n       #'ENTRANCES_MEDI', 'ENTRANCES_MODE', 'ENTRANCES_AVG',\n       'EXT_SOURCE_1', 'EXT_SOURCE_2',   'EXT_SOURCE_3', \n       'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_EMP_PHONE', 'FLAG_MOBIL',\n       'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'FLAG_PHONE', 'FLAG_WORK_PHONE',\n       #'FLOORSMAX_AVG', 'FLOORSMAX_MEDI', 'FLOORSMAX_MODE',\n       #'FLOORSMIN_AVG', 'FLOORSMIN_MEDI', 'FLOORSMIN_MODE',\n       'FONDKAPREMONT_MODE', 'HOUR_APPR_PROCESS_START', \n       'LIVE_CITY_NOT_WORK_CITY', 'LIVE_REGION_NOT_WORK_REGION',\n       'LIVINGAREA_AVG', 'LIVINGAREA_MEDI',\n       'LIVINGAREA_MODE', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE',\n       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE',\n       'NAME_TYPE_SUITE', \n       'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n       'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'OWN_CAR_AGE',\n       'REGION_POPULATION_RELATIVE', 'REGION_RATING_CLIENT',\n       'REGION_RATING_CLIENT_W_CITY', 'REG_CITY_NOT_LIVE_CITY',\n       'REG_CITY_NOT_WORK_CITY', 'REG_REGION_NOT_LIVE_REGION',\n       'REG_REGION_NOT_WORK_REGION', 'SK_ID_CURR', 'TARGET',\n       ]","33a60ad3":"credit_df = credit_df[cred_col]","34017927":"tt= time()\ncredit_df = data_preprocessing (credit_df)\nprint (\" Total time for Data Pre Processing :  ({0:.3f} s)\\n\".format(time() - tt) )","3784b018":"default_loan = credit_df[credit_df.TARGET == 1]\nnormal_loan = credit_df[credit_df.TARGET == 0]\ndefault_loan.shape","f27f8112":"normal_loan.shape","4f15af90":"# credit_df = data_preprocessing (credit_df)\n# cat_data_distribution(credit_df) ","f09ea950":"# Load Bureau Data\nbureau_data = pd.read_csv(bureau_file_path)\n# bureau_data.head()","3c0cf251":"# msno.bar(bureau_data);","afb3978f":"# Call \"cat_data_distribution\" funtion to plot all categorical features with 15 or less categories.\n# cat_data_distribution(bureau_data)        \n\nplot_pie_graph(bureau_data, 'CREDIT_TYPE')","545082d6":"print (\"Shape of Bureau data : \", bureau_data.shape)","51629fbc":"# Identify categorical important features and \n# Remove records with less number of nomber of transactions compared to total records. \n\nbureau_data = bureau_data.loc[(bureau_data['CREDIT_ACTIVE'] != 'Sold') & (bureau_data['CREDIT_ACTIVE'] != 'Bad debt') ]\nbureau_data = bureau_data.loc[(bureau_data['CREDIT_TYPE'] == 'Consumer credit') | (bureau_data['CREDIT_TYPE'] == 'Credit card') \n                             | (bureau_data['CREDIT_TYPE'] == 'Car loan') | (bureau_data['CREDIT_TYPE'] == 'Mortgage') ]","e96f49ef":"msno.bar(bureau_data);","1c4f0fe0":"bureau_data = bureau_data[ bureau_data['CREDIT_DAY_OVERDUE'] <=180]\n# bureau_data = data_preprocessing (bureau_data)\n\n# Remove features that has less impact on the transations. \n# Here you need your intutions keeping the classification modeling in mind.\nbureau_data = bureau_data.drop(['DAYS_CREDIT','CREDIT_DAY_OVERDUE','AMT_CREDIT_MAX_OVERDUE', 'AMT_ANNUITY',\n                                'CREDIT_CURRENCY','CNT_CREDIT_PROLONG', 'CREDIT_ACTIVE', 'DAYS_CREDIT_ENDDATE',\n                                'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE','CREDIT_TYPE'], axis = 1) \n\n# List columns with null data\n# bureau_data.columns[bureau_data.isna().any()].tolist()\n\nbureau_data = bureau_data.reset_index()\nbureau_data = bureau_data.drop(['index'], axis = 1) \n# check the shape of bureau data\n# bureau_data.info()","83b39f02":"bureau_balance_data = pd.read_csv(bureau_balance_file_path)\nprint (\"Shape of Bureau Balance data : \",bureau_balance_data.shape)\n# bureau_balance_data.head()","15734b30":"plot_pie_graph(bureau_balance_data, 'STATUS')","fce7112f":"# Discard 'Status' with negligible transaction\nbureau_balance_data = bureau_balance_data.loc[(bureau_balance_data['STATUS'] == 'C' ) \n                                              | (bureau_balance_data['STATUS'] == '0')\n                                             | (bureau_balance_data['STATUS'] == 'X')\n                                              | (bureau_balance_data['STATUS'] == 1)\n                                             ]\n\n# Lets create a column \"LATEST_MONTH\" to get transaction of the latest month in the record or dataset based on \"SK_ID_BUREAU\". \n# Based on 'LATEST_MONTH' we will select or extract the corresponding status in a new field 'BUREAU_BALANCE_STATUS' \n# and store \"Null\" value in records other than latest status. \n# Finally we will drop all thoese records with null values. This will gives us records with latest transaction status.\n\nbureau_balance_data['LATEST_MONTH'] = bureau_balance_data.groupby(['SK_ID_BUREAU'])['MONTHS_BALANCE'].transform(max)\nbureau_balance_data['BUREAU_BALANCE_STATUS'] = np.where((bureau_balance_data['MONTHS_BALANCE'] == bureau_balance_data['LATEST_MONTH']) \n                     , bureau_balance_data['STATUS'], np.nan)\n\n# Drop all the rows with 'NaN' values. This will leave only single row for each 'SK_ID_BUREAU'.\nbureau_balance_data = bureau_balance_data.dropna()\n\nbureau_balance_data = bureau_balance_data.drop(['LATEST_MONTH', 'BUREAU_BALANCE_STATUS', 'MONTHS_BALANCE'], axis = 1) \n# bureau_balance_data.info()\n","74921d1c":"bureau_result = pd.merge(bureau_data,\n                 bureau_balance_data[[ 'SK_ID_BUREAU', 'STATUS']],   #, 'STATUS'\n                 on='SK_ID_BUREAU', \n                 how='left')","0a8faeb9":"msno.bar(bureau_result);","89d7bfc1":"# cat_data_distribution(bureau_result) ","1000e641":"# bureau_result.columns[bureau_result.isna().any()].tolist()\n#bcol = bureau_result.columns.values\nnew_col =['SK_ID_CURR']\nselect_col = [   'AMT_CREDIT_SUM',   'AMT_CREDIT_SUM_DEBT' , 'AMT_CREDIT_SUM_LIMIT'  ]\n\nfor col in select_col:\n    #bureau_result['BUREAU_MEAN_'+ col] = bureau_result.groupby(['SK_ID_CURR','CREDIT_TYPE'])[col].transform('sum')\n    bureau_result['BUREAU_MEAN_'+ col] = bureau_result.groupby(['SK_ID_CURR'])[col].transform('sum')\n    new_col.append ('BUREAU_MEAN_'+ col)\n\n# new_col\nbureau_result = bureau_result[new_col]\n\nbureau_result.drop_duplicates(keep = 'first',inplace = True)\n\n# bureau_result.loc[(bureau_result['SK_ID_CURR'] == 120860)]","c7a73772":"credit_df = pd.merge(credit_df, bureau_result, on='SK_ID_CURR', how='left')\n# credit_df.head(10)\n\ndel bureau_result, bureau_data, bureau_balance_data\ngc.collect()\n\ncredit_df.shape","1f789b36":"previous_application_data = pd.read_csv(previous_application_file_path)\nprint (\"Shape of Previous Application data : \",previous_application_data.shape)\n# ","cb3112fb":"msno.bar(previous_application_data);","6dbdac42":"previous_application_data[['SK_ID_PREV','SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'AMT_ANNUITY',\n       'AMT_APPLICATION', 'AMT_CREDIT',        'AMT_GOODS_PRICE', \n        'NAME_CONTRACT_STATUS', 'DAYS_DECISION',\n       'NAME_PAYMENT_TYPE', \n       'NAME_CLIENT_TYPE', 'NAME_GOODS_CATEGORY', 'NAME_PORTFOLIO',\n       'NAME_PRODUCT_TYPE', 'CHANNEL_TYPE', 'SELLERPLACE_AREA',\n       'NAME_SELLER_INDUSTRY']].head()","9bb595df":"# cat_data_distribution(previous_application_data)","a9dc7ead":"\nprev_col = ['SK_ID_PREV', 'SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'AMT_ANNUITY',\n       'AMT_APPLICATION', 'AMT_CREDIT',        'AMT_GOODS_PRICE', \n        'NAME_CONTRACT_STATUS', 'DAYS_DECISION',\n       'NAME_PAYMENT_TYPE', \n       'NAME_CLIENT_TYPE', 'NAME_GOODS_CATEGORY', 'NAME_PORTFOLIO',\n       'NAME_PRODUCT_TYPE', 'CHANNEL_TYPE', 'SELLERPLACE_AREA',\n       'NAME_SELLER_INDUSTRY']\n\nprevious_application_data = previous_application_data[prev_col] ","9b4eaab8":"pos_cash_balance_data = pd.read_csv(POS_CASH_balance_file_path)\n# pos_cash_balance_data.head()","3d293f1b":"msno.bar(pos_cash_balance_data);","8ec53892":"# cat_data_distribution(pos_cash_balance_data)","4c3510cc":"# plot_pie_graph(pos_cash_balance_data, 'NAME_CONTRACT_STATUS')\n# Select records with 'Active', 'Completed' and 'Signed' status which are more than 10% of total transactions\npos_cash_balance_data = pos_cash_balance_data.loc[(pos_cash_balance_data['NAME_CONTRACT_STATUS'] == 'Active') \n                                              | (pos_cash_balance_data['NAME_CONTRACT_STATUS'] == 'Completed')\n                                             | (pos_cash_balance_data['NAME_CONTRACT_STATUS'] == 'Signed')\n                                             ]\n\n# select records where Days past Due is less than 365 days i.e 365.243\npos_cash_balance_data = pos_cash_balance_data[pos_cash_balance_data['SK_DPD'] <= 365243]","99a419b2":"# msno.bar(pos_cash_balance_data);","bdd59824":"# Drop columns \npos_cash_balance_data = pos_cash_balance_data.drop([ 'SK_DPD',  'SK_DPD_DEF', 'CNT_INSTALMENT_FUTURE',\n                                                     'NAME_CONTRACT_STATUS','MONTHS_BALANCE' ], axis = 1) \n\n# Drop Duplicate records\npos_cash_balance_data.drop_duplicates(keep = 'first',inplace = True)\n\n## Merge or Join the Previous Application data with POS Cash data \n\nprevious_result = pd.merge(previous_application_data,  pos_cash_balance_data, on= ['SK_ID_PREV','SK_ID_CURR'],  how='left')\n# previous_result.head()\n\ndel previous_application_data, pos_cash_balance_data\ngc.collect()","adf55128":"# previous_result.head()","ec459179":"credit_card_balance_data = pd.read_csv(credit_card_file_path)\n# credit_card_balance_data.head()","45d33763":"msno.bar(credit_card_balance_data);","298a3096":"# credit_card_balance_data.shape\n\n# cat_data_distribution(credit_card_balance_data)","97e63d34":"credit_card_balance_data = credit_card_balance_data.loc[(credit_card_balance_data['NAME_CONTRACT_STATUS'] == 'Active') \n                                              | (credit_card_balance_data['NAME_CONTRACT_STATUS'] == 'Completed')\n                                             | (credit_card_balance_data['NAME_CONTRACT_STATUS'] == 'Signed') ]\n\n#** select records where Days past Due is less than 365 days **\ncredit_card_balance_data = credit_card_balance_data[credit_card_balance_data['SK_DPD'] <= 365243]","45e1460d":"# credit_card_balance_data.head()","6ce31652":"# \nnew_col =['SK_ID_PREV','SK_ID_CURR']\nselect_col = [ 'AMT_BALANCE', 'AMT_PAYMENT_TOTAL_CURRENT',  'AMT_RECIVABLE' , 'AMT_TOTAL_RECEIVABLE'   ]\n\nfor col in select_col:\n    credit_card_balance_data['CC_'+ col] = credit_card_balance_data.groupby(['SK_ID_PREV','SK_ID_CURR'])[col].transform('mean')\n    new_col.append ('CC_'+ col)\n\n\ncredit_card_balance_data = credit_card_balance_data[new_col]","55c7cf6b":"# Drop Duplicate records\ncredit_card_balance_data.drop_duplicates(keep = 'first',inplace = True)","6f825273":"# msno.bar(credit_card_balance_data);","eb27391f":"previous_result = pd.merge(previous_result, credit_card_balance_data, on= ['SK_ID_PREV','SK_ID_CURR'],  how='left')  \n\n# previous_result.shape\ndel credit_card_balance_data\ngc.collect()","647dd769":"installments_payments_data = pd.read_csv(installments_payments_file_path)\nprint (\"Shape of Installment Payment data : \",installments_payments_data.shape)","a929de75":"msno.bar(installments_payments_data);","e477d185":"#cat_data_distribution(installments_payments_data)\nplot_pie_graph(installments_payments_data, 'NUM_INSTALMENT_VERSION')\n#plot_bar_graph(installments_payments_data, 'NUM_INSTALMENT_VERSION', 'NUM_INSTALMENT_VERSION') ","04576edd":"# Discard records with rare or less count of 'NUM_INSTALMENT_VERSION' compared to total transactions \ninstallments_payments_data = installments_payments_data.loc[  (installments_payments_data['NUM_INSTALMENT_VERSION'] == 0.0) \n                                                            | (installments_payments_data['NUM_INSTALMENT_VERSION'] == 1.0)\n                                                            | (installments_payments_data['NUM_INSTALMENT_VERSION'] == 2.0)\n                                                            | (installments_payments_data['NUM_INSTALMENT_VERSION'] == 3.0)\n                                                         #   | (installments_payments_data['NUM_INSTALMENT_VERSION'] == 4.0)\n                                                         #   | (installments_payments_data['NUM_INSTALMENT_VERSION'] == 5.0)\n                                              ]\n# installments_payments_data.loc[(installments_payments_data['SK_ID_PREV'] == 1496271)]\ninstallments_payments_data['DELAY_INSTALMENT_PAYMENT'] = installments_payments_data['DAYS_INSTALMENT'] - installments_payments_data['DAYS_ENTRY_PAYMENT']\ninstallments_payments_data = installments_payments_data[installments_payments_data['DELAY_INSTALMENT_PAYMENT'] > -181]","8ed0e6cf":"# msno.bar(installments_payments_data);","ad039fb2":"installments_col  = ['SK_ID_PREV', 'SK_ID_CURR','AMT_INSTALMENT', 'AMT_PAYMENT','DELAY_INSTALMENT_PAYMENT']\ninstallments_payments_data = installments_payments_data[installments_col]\n\n# installments_payments_data.loc[(installments_payments_data['SK_ID_PREV'] == 1496271)]\ninstallments_payments_data.drop_duplicates(keep = 'first',inplace = True)\n\n# examine duplicated rows\ninstallments_payments_data.loc[installments_payments_data.duplicated(), :]\n\n# installments_payments_data.shape\n# installments_payments_data.loc[(installments_payments_data['SK_ID_PREV'] == 1496271)]","5d3aeef4":"previous_result = pd.merge(previous_result,  installments_payments_data,  on= ['SK_ID_PREV','SK_ID_CURR'],  how='left')\n# previous_result.shape\n\ndel installments_payments_data\ngc.collect()","f739e65f":"msno.bar(previous_result);","6234ce9e":"previous_result.columns.values","357b8ff0":"select_col = ['SK_ID_PREV', 'SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'AMT_ANNUITY',\n       'AMT_APPLICATION', 'AMT_CREDIT', 'AMT_GOODS_PRICE',\n       'NAME_CONTRACT_STATUS', 'DAYS_DECISION', 'NAME_PAYMENT_TYPE',\n       'NAME_CLIENT_TYPE', 'NAME_GOODS_CATEGORY', \n         'AMT_INSTALMENT', 'AMT_PAYMENT'\n      ]\n\nprevious_result = previous_result[select_col]\n\nprevious_result.head()","72983bcb":"cat_data_distribution(previous_result)","ffe17db3":"previous_result = previous_result.loc[(previous_result['NAME_CONTRACT_TYPE'] != 'XNA')  ]\nprevious_result = previous_result.loc[(previous_result['NAME_CLIENT_TYPE'] != 'XNA')  ]","7ae58a07":"previous_result.drop_duplicates(keep = 'first',inplace = True)","0452f361":"previous_result.head()","4413fd74":"prev_col = ['SK_ID_CURR','NAME_CONTRACT_STATUS'] \nselect_col_sum = [ 'AMT_ANNUITY', 'AMT_APPLICATION', 'AMT_CREDIT', 'AMT_GOODS_PRICE','AMT_INSTALMENT', 'AMT_PAYMENT']\nfor col in select_col_sum:\n    previous_result['PREV_'+ col] = previous_result.groupby(['SK_ID_CURR'])[col].transform('sum')\n    prev_col.append ('PREV_'+ col)\n    \n\nprevious_result = previous_result[prev_col]\n# previous_result.head()\n\nprevious_result.drop_duplicates(keep = 'first',inplace = True)","d045f379":"credit_df = pd.merge(credit_df, previous_result, on='SK_ID_CURR', how='left') \n# credit_df.shape\n\ndel previous_result\ngc.collect()","16298c0a":"msno.bar(credit_df);","915e633a":"cred_col = credit_df.columns.values","ccb71102":"credit_df =  credit_df[cred_col]\ncredit_df.shape","1da81ea9":"credit_df.drop_duplicates(keep = 'first',inplace = True)\ncredit_df.shape","9943c0bd":"tt= time()\ncredit_df = data_preprocessing (credit_df)\nprint (\" Total time for Data Pre Processing :  ({0:.3f} s)\\n\".format(time() - tt) )","5d42ffa6":"# Separate the original Train data and Test data\ntrain_df = credit_df.loc[(credit_df['TARGET'] != -1)]\ntest_df = credit_df.loc[(credit_df['TARGET'] == -1)]\n\n# Drop the temporary target column from Test data\ntest_df = test_df.drop([\"TARGET\"], axis = 1) \n# train_df.head()\n\ndel credit_df\ngc.collect()","e48e60e6":"# train_df.hist(bins=10,figsize=(25,30),grid=False);","2b7e1922":"tt= time()\n\n# Get the training features and response variables\ntrain_Y = train_df['TARGET']                   # Responce variable column for training\ntrain_X = train_df.drop([\"TARGET\",\"SK_ID_CURR\"], axis = 1)  # Feature variable columns for training\n\nprint('Original target dataset shape {}'.format(Counter(train_Y)))\n\n# Scale the training data \nscaler = StandardScaler()  \n\n#print('Standard scaling of train and test data in progress ...')\ntrain_X_scale = scaler.fit(train_X).transform(train_X)\n\n#\n# Using XGBoost's \"plot_importance\" method to identify the importand features \nxgr = XGBRegressor(n_estimators=15, learning_rate=1.0, objective='binary:logistic', \n                    booster='gbtree',n_jobs= -1, #gamma=0.5 , \n                    min_child_weight=30, subsample=0.5, \n                    colsample_bytree=0.9, reg_alpha=0.01, reg_lambda=0.05,\n                    random_state=101)\n\n# train model  train_X_scale\nxgr.fit(train_X, train_Y)\n\n# Feature Importance\nplt.figure(figsize=(20,15))\nxgb.plot_importance(xgr, max_num_features=50, height=0.8, ax=plt.gca());\nprint (\" Total Feature impt time :  ({0:.3f} s)\\n\".format(time() - tt) )","5e330929":"#\nimportances_df = pd.DataFrame({'feature':train_X.columns,'importance':np.round(xgr.feature_importances_,3)})\nimportances_df = importances_df.sort_values('importance',ascending=False) #.set_index('feature')\nimportances_df[:30]","7448f95b":"feat_df = pd.DataFrame(importances_df[importances_df['importance']>= 0.019])\nfeat_col = list(feat_df['feature'])\nprint (\" Total Importance Feature selected :  \", len(feat_col) )\n# feat_col, len(feat_col)\n\ndel train_X, train_Y, importances_df, train_X_scale\ngc.collect()","cafb6a61":"def evaluate_result(clf, X_train, y_train, X_test, y_test, param_flg ) :\n    #print( 'Fitting the training set' ) \n    clf.fit(X_train, y_train)\n    if  param_flg == 'Y':     print (clf.best_params_ )\n \n    # Predict on training set\n    #print( 'Predicting the training set' )\n    pred_train_y = clf.predict(X_train)\n      \n    # Predict on testing set\n    pred_test_y = clf.predict(X_test)\n    #pred_prob_y = clf.predict_proba(X_test)\n    \n    # Is our model still predicting just one class? it shouldhave all classes say [0,1]\n    print( \"\\n PREDICTING TARGET CLASS [train data]  : \" , np.unique( pred_train_y ))\n    print( \"\\n PREDICTING TARGET CLASS [test data ]  : \" , np.unique( pred_test_y ) )\n    print(\"\\n\" )\n    \n    # How's our accuracy?\n    print(\" 1. ACCURACY SCORE [train data]           : \", accuracy_score(y_train, pred_train_y) )\n    print(\" 2. ACCURACY SCORE [test data ]           : \", accuracy_score(y_test, pred_test_y) )\n    \n    # How's our ROC score?\n    print(\" 3. ROC-AUC SCORE  [test data ]           :  \", roc_auc_score(y_test, pred_test_y) )\n\n    # How's actual vs prediction classification?\n    conf_mat = confusion_matrix(y_test, pred_test_y)\n    class_rpt = classification_report(y_test, pred_test_y)\n    \n    del pred_train_y, X_train, y_train, X_test, y_test\n    gc.collect()\n    \n    return  pred_test_y, clf, conf_mat, class_rpt\n\n### ROC Curve\ndef roc_plot(pred_y, test_y):\n    ##Computing false and true positive rates\n    fpr, tpr, thresholds= roc_curve(pred_y,test_y,drop_intermediate=False)\n    \n    print( \"\\n AUC and ROC curve : \" )\n    plt.figure()\n    \n    ##Adding the ROC\n    plt.plot(fpr, tpr, color='red',    lw=2, label='ROC curve')\n    \n    ##Random FPR and TPR\n    plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n    \n    ##Title and label\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title('ROC curve')\n    plt.show()\n    return\n\n### Precision-Recall Curve for imblance class\ndef precision_recall_plot(pred_y, test_y):\n    \n    precision, recall, thresholds = precision_recall_curve( test_y, pred_y)\n    \n    print(\" precision   : \", precision )\n    print(\" recall      : \", recall )\n    print(\" thresholds  : \", thresholds )\n    \n    plt.step(recall, precision, color='b', alpha=0.2,  where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.2,  color='b')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format( average_precision_score(test_y, pred_y)))\n    return\n\n# PLOT HEATMAP OF CONFUSION MATRIX\ndef conf_matrix():\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_mat, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n    plt.title(\"Confusion matrix\")\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.show()","fceec132":"def submission_file(subfilename):\n    \n    # Get 'SK_ID_CURR' for submission id column\n    submission_id = pd.DataFrame({ 'SK_ID_CURR' : test_df['SK_ID_CURR']}, dtype=np.int32)\n    \n    # Predict the target using the trained classifier and convert into dataframe\n    sub_test_data = test_df[feat_col]\n    pred_test_y = trained_clf.predict(sub_test_data)\n    #     \n    submission_tar = pd.DataFrame({ 'TARGET': pred_test_y},  dtype=np.int32)\n    print (\"Shape of Application Test data : \",submission_tar['TARGET'].value_counts())\n    \n    # Reset indexes\n    submission_id.reset_index(drop=True, inplace=True)\n    submission_tar.reset_index(drop=True, inplace=True)\n    \n    # Concat the the id and the predicted result\n    submission_df =  pd.concat([submission_id,submission_tar],axis=1)    \n    \n    now = datetime.datetime.now()\n    \n    subname = subfilename + str (now.strftime(\"%Y-%m-%d_%H-%M\")) + \".csv\"\n    # save into .csv submission file\n    \n    submission_df.to_csv(\"..\/Kaggle_HomeCredit\/Output\/\" + subname, index=False)\n    return submission_df\n","4320d3e3":"X = train_df[feat_col]\ny = train_df.TARGET","2e41e191":"t = time() \nhyperparameters = { 'xgbclassifier__max_depth': [15],                 'xgbclassifier__learning_rate': [0.25],\n                    'xgbclassifier__n_estimators': [10],              'xgbclassifier__nthread': [-1],\n                    'xgbclassifier__reg_alpha': [ 0.7],               'xgbclassifier__reg_lambda': [ 1.0],  \n                    'xgbclassifier__max_delta_step': [0],             'xgbclassifier__min_child_weight': [10.0],\n                    #'xgbclassifier__subsample': [1.0],              #'xgbclassifier__colsample_bytree': [0.9],\n                    'xgbclassifier__objective': ['binary:logistic' ], 'xgbclassifier__scale_pos_weight': [1],\n                    'xgbclassifier__gamma': [0.05],                   'xgbclassifier__seed': [101]\n                  }\n\n## Split the data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n\nprint( \"Test target shape: \\n\", y_train.value_counts() )\nprint( \"Test target shape: \\n\", y_test.value_counts() )\n\n# Train model\npipeline = make_pipeline (MinMaxScaler(), XGBClassifier())\n\n#print (pipeline.get_params())\nprint( \"\\n Model Training: \" )\nGSC = GridSearchCV(pipeline, hyperparameters, n_jobs = -1, scoring = 'roc_auc', cv=4)\n\n# Call train_evaluate to model and evaluate \npred_test_y, trained_clf, conf_mat, class_rpt =  evaluate_result(GSC, X_train, y_train, X_test, y_test, param_flg = 'Y')\n\nprint (\" Total time for Training and Evaluating the Model :  ({0:.3f} s)\\n\".format(time() - t) )\n\n","a15475b4":"# PLOT HEATMAP OF CONFUSION MATRIX\nconf_matrix()\n## Classification Report\nprint(class_rpt)\n## ROC and Precision-Recall Plot\nroc_plot(pred_test_y, y_test)\nprecision_recall_plot(pred_test_y, y_test)","0ed5eeb1":"t = time() \n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=7, shuffle=True, random_state=101)\n\n# Get the training features and response variables\n# train_Y = train_df['TARGET']      # Responce variable column for training\n# train_X = train_df[feat_col]      # Feature variable columns for training\nX = train_df[feat_col]\ny = train_df.TARGET\n# X is the feature set and y is the target\nfor train_index, test_index in skf.split(X, y): \n    print(\"Train:\", train_index) \n    print( \"Validation:\", test_index) \n    X_train, X_test = X.iloc[train_index], X.iloc[test_index] \n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n\n# Train model\nETC = ExtraTreesClassifier(n_estimators=15, max_depth=15, \n                           min_samples_split=5, min_samples_leaf=5, min_weight_fraction_leaf=0.0,\n                           max_leaf_nodes=15, class_weight ='balanced_subsample',n_jobs=1, random_state=101)  \n# (n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n# min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=1, random_state=None, verbose=0, \n# warm_start=False, class_weight=None)\n\n# Train and Evaluate Model\npred_test_y, trained_clf, conf_mat, class_rpt = evaluate_result(ETC, X_train, y_train, X_test, y_test, param_flg = 'N')","abe9479b":"# PLOT HEATMAP OF CONFUSION MATRIX\nconf_matrix()\n## Classification Report\nprint(class_rpt)\n## ROC and Precision-Recall Plot\nroc_plot(pred_test_y, y_test)\nprecision_recall_plot(pred_test_y, y_test)","36801b0f":"print (\" Total training time :  ({0:.3f} s)\\n\".format(time() - t) )","3d64cf06":"# RESAMPLE THE IMBALANCED CLASS BY OVERSAMPLING THE MINORITY CLASS \nprint( train_df['TARGET'].value_counts())\n\n# Separate majority and minority classes\ndf_majority = train_df[train_df.TARGET==0]\ndf_minority = train_df[train_df.TARGET==1]\n\n# df_majority['TARGET'].value_counts()\n# df_minority['TARGET'].value_counts()[1]\n\n# Upsample minority class\ndf_sampled = resample(df_minority, replace=True,                            # sample with replacement\n                      n_samples=df_majority['TARGET'].value_counts()[0],    # to match majority class\n                      random_state=123)                                     # reproducible results\n \n# Combine majority class with upsampled minority class\n# resampled_df = pd.concat([df_minority, df_sampled], axis = 0)\nresampled_df = pd.concat([df_majority, df_sampled]) \n\n#df_upsampled = df_upsampled.reset_index()\n#df_upsampled = df_upsampled.drop(['index'], axis = 1) \n\n# Display new class counts\ny = resampled_df['TARGET']\nX = resampled_df.drop('TARGET', axis = 1)\n\nprint( X.shape, y.shape )\nprint( y.value_counts())\n# Resampled data with selected importand Features\nX = X[feat_col]\nprint( X.shape)\n","bbbd3e54":"# TRAIN MODEL\nt = time()  \n# SPLIT TRAINING DATA \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\nprint( \"Train target shape : \\n\", y_train.value_counts() )\nprint( \"Test target shape  : \\n\", y_test.value_counts() )\n\n\nprint( \"\\n TRAINING THE MODEL \" )\nhyperparameters = { 'xgbclassifier__max_depth': [15],                 'xgbclassifier__learning_rate': [0.25],\n                    'xgbclassifier__n_estimators': [10],              'xgbclassifier__nthread': [-1],\n                    'xgbclassifier__reg_alpha': [ 0.7],               'xgbclassifier__reg_lambda': [ 1.0],  \n                    'xgbclassifier__max_delta_step': [0],             'xgbclassifier__min_child_weight': [10.0],\n                    #'xgbclassifier__subsample': [1.0],              #'xgbclassifier__colsample_bytree': [0.9],\n                    'xgbclassifier__objective': ['binary:logistic' ], 'xgbclassifier__scale_pos_weight': [1],\n                    'xgbclassifier__gamma': [0.05],                   'xgbclassifier__seed': [101]\n                  }\n\n#pipeline = make_pipeline (preprocessing.StandardScaler(), XGBClassifier())\npipeline = make_pipeline (MinMaxScaler(), XGBClassifier())\n#print (pipeline.get_params())\n\nGSC = GridSearchCV(pipeline, hyperparameters, n_jobs = -1, scoring = 'roc_auc', cv=4)\n\n# Call train_evaluate to model and evaluate \npred_test_y, trained_clf, conf_mat, class_rpt  =  evaluate_result(GSC, X_train, y_train, X_test, y_test, param_flg = 'Y')\n\nprint (\" Total time for Training and Evaluating the Model :  ({0:.3f} s)\\n\".format(time() - t) )","cf6129e5":"# PLOT HEATMAP OF CONFUSION MATRIX\nconf_matrix()\n## Classification Report\nprint(class_rpt)\n## ROC and Precision-Recall Plot\nroc_plot(pred_test_y, y_test)\nprecision_recall_plot(pred_test_y, y_test)","4df02db5":"from imblearn.over_sampling import     ADASYN \n\nX = train_df[feat_col]\ny = train_df.TARGET\nprint('Original dataset shape {}'.format(Counter(y)))\n\nada = ADASYN(random_state=42)\n# Train using original preprocessed training data\nX_res, y_res = ada.fit_sample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n\n# Split Training data\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.30, random_state=101)","f46bb691":"t = time() \n# Define Classifier\nETC = ExtraTreesClassifier( n_estimators=15, max_depth= 7, min_samples_split=5,min_samples_leaf=5)\n# Train and Evaluate Model\npred_test_y, trained_clf, conf_mat, class_rpt = evaluate_result(ETC, X_train, y_train, X_test, y_test, param_flg = 'N')\n\nprint (\" Total training time :  ({0:.3f} s)\\n\".format(time() - t) )","4526a567":"# PLOT HEATMAP OF CONFUSION MATRIX\nconf_matrix()\n## Classification Report\nprint(class_rpt)\n## ROC and Precision-Recall Plot\nroc_plot(pred_test_y, y_test)\nprecision_recall_plot(pred_test_y, y_test)","97259c61":"print (\" Total training time :  ({0:.3f} s)\\n\".format(time() - t) )","74ae6e0f":"from imblearn.over_sampling import SMOTE\nt = time()\nX = train_df[feat_col]\ny = train_df.TARGET\n\nprint('Original dataset shape {}'.format(Counter(y)))\n\nsmt = SMOTE(random_state=42)\nX_res, y_res = smt.fit_sample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n\n# Split Training data\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.30, random_state=101)\n\n# Call train_evaluate to model and evaluate \npred_test_y, trained_clf, conf_mat, class_rpt  =  evaluate_result(GSC, X_train, y_train, X_test, y_test, param_flg = 'N')\n\nprint (\" Total time for Training and Evaluating the Model :  ({0:.3f} s)\\n\".format(time() - t) )","6fc3111b":"# PLOT HEATMAP OF CONFUSION MATRIX\nconf_matrix()\n## Classification Report\nprint(class_rpt)\n## ROC and Precision-Recall Plot\nroc_plot(pred_test_y, y_test)\nprecision_recall_plot(pred_test_y, y_test)","0f69eab6":"from imblearn.under_sampling import AllKNN\n\nt = time()\nX = train_df[feat_col]\ny = train_df.TARGET\n\nprint('Original dataset shape {}'.format(Counter(y)))\n\nakn = AllKNN(random_state=42)\nX_res, y_res = akn.fit_sample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n\n\n# Split Training data\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.30, random_state=101)\n\n# Call train_evaluate to model and evaluate \n\npred_test_y, trained_clf, conf_mat, class_rpt  =  evaluate_result(GSC, X_train, y_train, X_test, y_test, param_flg = 'N')\n\nprint (\" Total time for Training and Evaluating the Model :  ({0:.3f} s)\\n\".format(time() - t) )","d8503422":"# PLOT HEATMAP OF CONFUSION MATRIX\nconf_matrix()\n## Classification Report\nprint(class_rpt)\n## ROC and Precision-Recall Plot\nroc_plot(pred_test_y, y_test)\nprecision_recall_plot(pred_test_y, y_test)","5266b657":"print( \"Time taken to train the training set in {} secs\" .format(time() - tstart))","e65bc68f":"print (\" Total time taken :  ({0:.3f} s)\\n\".format(time() - tstart) )","35905b3e":"## Merge or Join the Training Data with Previous Result Data ","c38e468d":"### GRAPHS and PLOTS","69b98d0c":"## Analysis of Previous Installment and Credit card data : ","24e4f182":"## Concatenate or merge the training and test data ","a9f31765":"** Split the Credit_df into original Train and Test data **","07b9173a":"sub_file = \"ETC_ADASYN_sub_\"\nsub_df = submission_file(sub_file)\nsub_df.head()","81becc50":"cred_col = ['AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_INCOME_TOTAL',\n       'AMT_REQ_CREDIT_BUREAU_YEAR', 'CODE_GENDER', 'DAYS_BIRTH',\n       'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE',\n       'DAYS_REGISTRATION', 'EMERGENCYSTATE_MODE', #'EXT_SOURCE_1',\n       'EXT_SOURCE_2', 'EXT_SOURCE_3', 'LIVE_CITY_NOT_WORK_CITY',\n       'LIVE_REGION_NOT_WORK_REGION', 'NAME_CONTRACT_TYPE',\n       'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',\n       'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE',\n       'ORGANIZATION_TYPE', 'SK_ID_CURR', 'TARGET', 'TOTALAREA_MODE']","6c8dcb9d":"sub_file = \"XGC_GridCV_sub_\"\nsub_df = submission_file(sub_file)\nsub_df.head()","02992a3f":"**Merge eligible valid \"bureau_balance_valid\" with \"bureau_data\"**","80be9253":"### 3. Oversampling minor class : ADASYN (Adaptive Synthetic) Sampling Approach ","7af22543":"## Merge or Join the Training data with Bureau data ","667503de":"## Process credit_card_balance.csv ","db11945c":"## Processed Training Data","c2e5733a":"## Import Libraries","bf40ff74":"previous_application_data.head()","dfd763ad":"msno.bar(bureau_balance_data);","b1385890":"previous_application_data.columns.values","9ed4124b":"** Application train data and test data **","4bc9ae3e":"### Functions to Train and plot graphs","a33d0dcc":"bureau_result.shape","1f618fe9":"sub_file = \"XGC_SMOTE_sub_\"\nsub_df = submission_file(sub_file)\nsub_df.head()","1d9ed346":"## Re-Sampled the Imbalanced Data : Oversampling","d3d60a44":"** Split the training data into feature and target ** ","be3204e5":"# Preparing the data","09c5720f":"** bureau.csv **\n\n    For one client 'SK_ID_CURR' there can be one or more rows of credits 'SK_ID_BUREAU' transactions. \n","fb96f48b":"**installments_payments.csv **","739343d0":"## Load the data","e5daedfc":"####      3a. ExtraTreesClassifier + ADASYN ","fb7c74d6":"## Analysis of Bureau Informations : 'Bureau' and 'Bureau_Balance'","a16b70e1":"sub_file = \"XGC_AllKNN_sub_\"\nsub_df = submission_file(sub_file)\nsub_df.head()","3d7366eb":"### Select Important Features in the data set","5e8270cb":"sub_file = \"ETC_StrafKFold_sub_\"\nsub_df = submission_file(sub_file)\nsub_df.head()","4a7838ac":"## Merge or Join the Previous Application Data with Credit Card Data ","b72b7b4e":"## Process POS_CASH_balance.csv ","a98b5795":"# Prediction and Submission on Test Data ","adb327f9":"### Data Preprocessing funtions","aee55dd4":"##  AllKNN\n","19d2736a":"# Feature Engineering : Feature Selection","a7de59ce":"# Building and Evaluating the Model","bcbb7b25":"** bureau_balance.csv **\n**--------------------**","ef1aad23":"## Some Generic and Useful Methods or Functions","33f2595c":"### Process previous_application.csv ","8a398e09":"sub_file = \"Resample_XGC_GridCV_sub_\"\nsub_df = submission_file(sub_file)\nsub_df.head()","ff9cf0e3":"### 2. Train using StratifiedKFold  Cross validation with ExtraTreesClassifier","d5756213":"## SMOTE (Synthetic Minority Over-sampling Technique) + XGBoost Classifier","b54b97f6":"## Merge or Join the Previous Application Data with Installment Payment Data ","1d2cbe8d":"##### TARGET variable takes values : \n\n     1 - client with payment difficulties: he\/she had late payment more than X days on at least one of\n                                            the first Y installments of the loan in our sample, \n     0 - all other cases\n","14bf4095":"cred_col = ['AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_INCOME_TOTAL',\n       'AMT_REQ_CREDIT_BUREAU_YEAR', 'CODE_GENDER', 'DAYS_BIRTH',\n       'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE',\n       'DAYS_REGISTRATION', 'EMERGENCYSTATE_MODE', 'EXT_SOURCE_2',\n       'EXT_SOURCE_3', 'LIVE_CITY_NOT_WORK_CITY',\n       'LIVE_REGION_NOT_WORK_REGION', 'NAME_CONTRACT_TYPE',\n       'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', # 'NAME_HOUSING_TYPE',\n       'NAME_INCOME_TYPE', #'NAME_TYPE_SUITE', 'OCCUPATION_TYPE',\n       'ORGANIZATION_TYPE', 'SK_ID_CURR', 'TARGET', #'TOTALAREA_MODE',\n       'BUREAU_MEAN_AMT_CREDIT_SUM', 'BUREAU_MEAN_AMT_CREDIT_SUM_DEBT',\n       'BUREAU_MEAN_AMT_CREDIT_SUM_LIMIT', # 'NAME_CONTRACT_TYPE_y',\n       #'NAME_CONTRACT_STATUS', 'NAME_PAYMENT_TYPE', #'NAME_CLIENT_TYPE',\n       #'PREV_AMT_ANNUITY', 'PREV_AMT_APPLICATION', \n            'PREV_AMT_CREDIT',\n       #'PREV_AMT_GOODS_PRICE', \n            'PREV_AMT_INSTALMENT', 'PREV_AMT_PAYMENT']\n\n","fa2d2bfc":"###  1. Train using GridSearch cross-validation with XGBClassifier","498477f5":"corr=train_df[feat_col].corr()#\nplt.figure(figsize=(20, 15))\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features');","dea64791":"def transactions_by_target( feat, htitle ):\n    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize = (14,6))\n    f.suptitle(htitle)\n\n    bins = 100\n\n    ax1.hist(default_loan[feat], bins = bins)\n    ax1.set_title('Default Loan')\n\n    ax2.hist(normal_loan[feat], bins = bins)\n    ax2.set_title('Normal Loan')\n\n    plt.xlabel('Amount ($)')\n    plt.ylabel('Number of Transactions')\n    plt.yscale('log')\n    plt.show();\n\ntransactions_by_target( 'AMT_ANNUITY', 'Amount Annuity per transaction by Target' )\ntransactions_by_target( 'AMT_CREDIT', 'Amount Credit per transaction by Target' )\ntransactions_by_target( 'AMT_GOODS_PRICE', 'Amount Goods Price per transaction by Target' )\ntransactions_by_target( 'EXT_SOURCE_2', 'Ext Source 2 per transaction by Target' )\ntransactions_by_target( 'EXT_SOURCE_3', 'Ext Source 3 per transaction by Target' )","10384df3":"##  Data Preprocessing of Training data"}}