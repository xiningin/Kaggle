{"cell_type":{"0680983c":"code","a7ae8708":"code","52712e02":"code","567724b7":"code","a5c0f452":"code","92d2289e":"code","9a0d270e":"code","a52e0aa4":"code","059e1f34":"code","babcdaa0":"code","de4b01eb":"code","dc1b5914":"code","ccee3697":"code","bae1d3e5":"code","5b8d112b":"code","d2994181":"code","c6db3e17":"code","f18619b5":"code","cfa3f051":"code","96ed9932":"code","3a103ae1":"code","ec8a88f8":"code","149b2033":"code","76d62332":"code","5f0d36ff":"code","ecfc2635":"code","e543d7f1":"code","f9ceb37f":"code","83cec8b7":"code","926903bd":"code","2f113d12":"code","5e64417a":"code","0f3936d1":"code","fe0bc163":"code","b4b73873":"code","c89216a9":"code","aed1e5b9":"code","64979e3f":"code","04aaa4d6":"code","29bb03c0":"code","a8acb97b":"code","f106a679":"code","1f66da53":"code","79b915f9":"code","568167cb":"code","cf64ba00":"code","c7a72c6f":"code","d8c4a4ac":"code","89e4b1f2":"code","d16b79aa":"code","1bb61608":"code","2ce146f7":"code","86b6a2ec":"code","007ae839":"code","85200fba":"code","0455bf28":"code","cb67f716":"code","55c00b76":"code","486e9802":"code","a1527041":"code","899542b1":"code","bf09ecb5":"code","6a49dfa3":"code","24ae7cf3":"code","6816d554":"code","c8ad56c5":"code","ac7a7406":"code","e7d2e2a3":"code","bf36cfad":"code","e02262ab":"code","f0c96dcd":"code","8e4cf311":"code","ac2b3a2b":"code","3ede5fbb":"code","e125d19a":"code","6a13ef33":"code","9f752d43":"code","c2c0a1bf":"code","5e85843f":"code","1f852535":"code","3bfdbb38":"code","43f02be7":"code","83442113":"code","e71bda1f":"code","22404eb3":"code","94017c08":"code","bb9b8b48":"code","6f225382":"code","c0cecce9":"code","56074063":"code","be6929e3":"code","13637698":"code","c73c8c2a":"code","46552dca":"code","e7516eed":"code","160b7684":"code","cc996ae7":"code","fc8af437":"code","7e746ea4":"code","5bd2c546":"code","6f957abe":"code","c833eacf":"code","0cd2c035":"code","f68c578b":"code","f7261715":"code","8f456cf3":"code","32c4ab78":"code","50be23c9":"code","d31c7555":"code","105ea2fe":"code","47524252":"markdown","9445d82a":"markdown","8fa3ee3c":"markdown","bd6b277e":"markdown","5bcd3b99":"markdown","e54db020":"markdown","c6c67cdd":"markdown","4fc91213":"markdown","b9759c2a":"markdown","9d348599":"markdown","54796e5d":"markdown","ece58ac1":"markdown"},"source":{"0680983c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7ae8708":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","52712e02":"import warnings\nwarnings.filterwarnings('ignore')","567724b7":"dat = pd.read_csv('..\/input\/ecommerce-data\/data.csv',encoding=\"ISO-8859-1\")\n# ,encoding='cp874'","a5c0f452":"dat.head(5)\ndff = dat.copy()\ndat.head(10)\n","92d2289e":"dat.head(5)\ndff = dat.copy()\ndat.head(10)\n","9a0d270e":"duplicated = dff.duplicated().sum()\nprint(duplicated)\ndff.drop_duplicates(inplace= True)","a52e0aa4":"dff.shape","059e1f34":"#transfore the data type\ndff['InvoiceDate']= pd.to_datetime(dff['InvoiceDate'])","babcdaa0":"dff.describe()","de4b01eb":"#deleting negative values from unit price and quantity\n#we have negtive value for quantity and price\ndf = dff[(dff['Quantity']>0) & (dff['UnitPrice']>0)]","dc1b5914":"df.shape\ndf.describe()","ccee3697":"fig, ax = plt.subplots(figsize=(10,8))\nax.scatter(df['Quantity'], df['UnitPrice'])\nax.set_xlabel('Quantity')\nax.set_ylabel('UnitPrice')\nplt.show()","bae1d3e5":"#Taking out outliers to get better plot\nfrom scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(df[['Quantity','UnitPrice']]))\ndf = df[(z < 3).all(axis=1)]","5b8d112b":"df = df[(df['Quantity']>=0) | (df['UnitPrice']>=0)]","d2994181":"sns.boxplot(df['Quantity'])","c6db3e17":"fig, ax = plt.subplots(figsize=(10,8))\nax.scatter(df['Quantity'], df['UnitPrice'])\nax.set_xlabel('Quantity')\nax.set_ylabel('UnitPrice')\nplt.show()","f18619b5":"#Checking the missings\ndf.isna().sum()","cfa3f051":"#Creating total amount column\ndf['TotalAmount'] = df['Quantity'] * df['UnitPrice']\n","96ed9932":"df['Hour']=df['InvoiceDate'].dt.hour\ndf['Month']=df['InvoiceDate'].dt.month\ndf['Weekdays']= df['InvoiceDate'].dt.weekday","3a103ae1":"rfm = df.copy()","ec8a88f8":"#because we are looking at the customer and not the product we drop InvoiceNo and Stcokcode\nab = df.groupby('CustomerID').agg({'InvoiceDate': 'min','TotalAmount': 'sum'})\nab.rename(columns={'InvoiceDate': \"Recency\",('InvoiceDate', 'nunique'): \"Frequency\",\"TotalAmount\": 'Monetization'},  inplace = True)\nfrequency = df.groupby('CustomerID').agg({'InvoiceDate':'nunique'})\nfrequency.rename(columns={'InvoiceDate': \"Frequency\"}, inplace = True)\n\nrfm = pd.merge(ab,frequency, on='CustomerID')\n\n#rfm.rename(columns={('InvoiceDate',     'min'): \"Recency\",('InvoiceDate', 'nunique'): \"Frequency\",\"TotalAmount\": 'Monetization'})","149b2033":"a = rfm.reset_index()","76d62332":"rfm.rename(columns={'InvoiceDate': \"Recency\",'InvoiceDate': \"Frequency\",\"TotalAmount\": 'Monetization'})","5f0d36ff":"rfm.describe()","ecfc2635":"#We save the most recent date to, then , calulate the recency\ne = df['InvoiceDate'].min()\nprint('minimun :'+ str(e))","e543d7f1":"rfm.info()","f9ceb37f":"###create receny, cad calculate the last time he bought something compare to e\nrfm['Recency'] = rfm['Recency'].apply(lambda x : (x - e).days)","83cec8b7":"rfm.describe()","926903bd":"rfm1 = rfm.copy()\n# df.groupby('CustomerID')['InvoiceDate'].max() - df.groupby('CustomerID')['InvoiceDate'].min()\ndf","2f113d12":"#why do we rank it and how?\nrfm['Rank_Recency'] = pd.qcut( rfm['Recency'],q=5, labels = range(6, 1, -1))\nrfm['Rank_Recency'] = pd.to_numeric(rfm['Rank_Recency'])","5e64417a":"def freq(x):\n    if x ==1:\n        return 1\n    elif x == 2:\n        return 2\n    elif x == 3:\n        return 3\n    elif x == 4:\n        return 4\n    else: \n        return 5\n\nrfm['Rank_Frequency'] =rfm['Frequency'].apply(freq)\n\n#rfm['Rank_Frequency'] = pd.qcut( rfm['Frequency'],q=5, labels = range(1, 6, 1))\n#rfm['Rank_Frequency'] = pd.to_numeric(rfm['Rank_Frequency'])\n","0f3936d1":"rfm['Rank_Monetization'] = pd.qcut( rfm['Monetization'],q=5, labels = range(1, 6, 1))\nrfm['Rank_Monetization'] = pd.to_numeric(rfm['Rank_Monetization'])\n\nrfm['RFM_Score'] = rfm['Rank_Recency'].astype(str)+ rfm['Rank_Frequency'].astype(str) + rfm['Rank_Monetization'].astype(str)\nrfm['Score'] = rfm['Rank_Recency']+ rfm['Rank_Frequency']+ rfm['Rank_Monetization']","fe0bc163":"rfm.describe()","b4b73873":"def client_segment(x):\n    if x == 15:\n        return 'Champions'\n    elif  x >= 14:\n        return 'Loyal Customers'\n    elif  x >= 11:\n        return 'Can\u2019t Lose Them'\n    elif  x >= 9:\n        return 'Potential Loyalist'\n    elif  x >= 7:\n        return 'Promising'\n    elif  x >= 6:\n        return 'Needs Attention'\n    elif  x >= 5:\n        return 'At Risk'\n    else:\n        return 'Lost'","c89216a9":"rfm['Clients'] = rfm['Score'].apply(client_segment)","aed1e5b9":"clients = rfm[['Clients', 'Frequency', 'Monetization', 'Recency']].groupby('Clients').median()\nclients.reset_index(inplace = True)","64979e3f":"rfm.head(5)","04aaa4d6":"ax = sns.barplot(x=\"Frequency\", y=\"Clients\", data=clients)\nax.set_ylabel('Clients')\nax.set_title('Median Visits')","29bb03c0":"ax = sns.barplot(x=\"Monetization\", y=\"Clients\", data=clients)\nax.set_ylabel('Clients')\nax.set_title('Median Expenditure')","a8acb97b":"ax = sns.barplot(x=\"Recency\", y=\"Clients\", data=clients)\nax.set_ylabel('Clients')\nax.set_title('Median time from last shop ')\n","f106a679":"import squarify\n\nsquarity =rfm['Clients'] .value_counts()\ncolor=['grey','orange','pink','purple', 'brown', 'blue', 'green', 'red']\n\n\nfig = plt.gcf()\nax = fig.add_subplot()\nfig.set_size_inches(12, 8)\nsquarify.plot(sizes= squarity , \n              label=['Promising',\n                     'Can\u2019t Lose Them',\n                     'Potential Loyalist',\n                     'Loyal Customer',\n                     'Promising', \n                     'Needs Attention',\n                     'At Risk',\n                     'Champions',\n                     'Lost',] ,color = color, alpha=0.5,)\nplt.title(\"RFM Segments\",fontsize=18,fontweight=\"bold\")\nplt.axis('off')\nplt.show()","1f66da53":"squarity","79b915f9":"sns.heatmap(clients.corr(),annot=True)","568167cb":"import pandas as pd\nimport numpy as np\n\nrs = np.random.RandomState(0)\ndf = pd.DataFrame(rs.rand(10, 10))\ncorr = clients.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)\n# 'RdBu_r' & 'BrBG' are other good diverging colormaps","cf64ba00":"# Compute the correlation matrix\ncorr = clients.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(240, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c7a72c6f":"# sns.heatmap(clients.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')\nsns.heatmap(clients.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', linewidths=3, linecolor='black')\n","d8c4a4ac":"clients","89e4b1f2":"rfm['Clients'].value_counts()","d16b79aa":"cluster = rfm.drop(['Rank_Recency','Rank_Frequency','Rank_Monetization','RFM_Score','Score','Clients'], axis = 1)","1bb61608":"from sklearn.preprocessing import StandardScaler,  MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom sklearn.cluster import AgglomerativeClustering","2ce146f7":"X= MinMaxScaler().fit_transform(cluster)\n#x = StandardScaler().fit_transform(X)","86b6a2ec":"sum_of_squared_distances = []\nK = range(1,15)\nfor k in K:\n    k_means = KMeans(n_clusters=k)\n    model = k_means.fit(X)\n    sum_of_squared_distances.append(k_means.inertia_)","007ae839":"plt.plot(K, sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('sum_of_squared_distances')\nplt.title('Elbow Method')\nplt.show()","85200fba":"# Elbow Method for K means\n# Import ElbowVisualizer\nfrom yellowbrick.cluster import KElbowVisualizer\nmodel = KMeans()\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(model, k=(2,30), timings= True)\nvisualizer.fit(X)        # Fit data to visualizer\nvisualizer.show()        # Finalize and render figure","0455bf28":"# Silhouette Score for K means\n# Import ElbowVisualizer\nfrom yellowbrick.cluster import KElbowVisualizer\nmodel = KMeans()\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(model, k=(2,30),metric='silhouette', timings= True)\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","cb67f716":"# Calinski Harabasz Score for K means\n# Import ElbowVisualizer\nfrom yellowbrick.cluster import KElbowVisualizer\nmodel = KMeans()\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(model, k=(2,30),metric='calinski_harabasz', timings= True)\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","55c00b76":"k_means_5 = KMeans(n_clusters=7)\nmodel = k_means_5.fit(X)\ny_hat_5 = k_means_5.predict(X)\nlabels_5 = k_means_5.labels_\nprint(metrics.silhouette_score(X, labels_5, metric = 'euclidean'))\nmetrics.calinski_harabasz_score(X, labels_5)","486e9802":"cluster['Cluster'] = labels_5","a1527041":"temp = cluster.reset_index()\nprint(temp.shape)\n# temp.join(dff,on='CustomerID',how='right',lsuffix='left',rsuffix='right')\n# pd.concat([dff,temp],join='inner',axis=1)\ndff.columns\ndff1 = dff.drop(columns=['InvoiceNo', 'StockCode','InvoiceDate'])\nprint(dff1.shape)\n# dff1.join(temp,on='CustomerID',how='left',lsuffix='left',rsuffix='right')","899542b1":"dff1['CustomerID'] = dff1['CustomerID'].astype(str)\ntemp['CustomerID'] = temp['CustomerID'].astype(str)\nprint(dff1.dtypes)\nprint(temp.dtypes)","bf09ecb5":"# temp.join(dff1,on='CustomerID',how='right',lsuffix='left',rsuffix='right')\ntemp1 = temp.merge(dff1,how='inner',left_on='CustomerID',right_on='CustomerID')\nprint(temp1.columns)\ntemp2 = temp1.drop(columns=['Recency', 'Monetization', 'Frequency'])","6a49dfa3":"temp2\ntrain = temp2.loc[:,temp2.columns != 'Cluster']\ntarget = temp2.loc[:,temp2.columns == 'Cluster']\nprint(train.shape)\nprint(target.shape)","24ae7cf3":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['DescriptionCode'] = le.fit_transform(train['Description'])\ntrain['CountryCode'] = le.fit_transform(train['Country'])\ntrain\n# train1 = train[['Description','Country']].apply(le.fit_transform)","6816d554":"temptrain = train.drop(columns=['Description','CustomerID','Country'])","c8ad56c5":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# train[['Quantity1','UnitPrice1']] = scaler.fit_transform(train[['Quantity','UnitPrice']])\nscaler.fit(train[['UnitPrice','Quantity']])\nscaled_data = pd.DataFrame(scaler.transform(train[['UnitPrice','Quantity']]),columns=['QuantScaled','UnitPriceScaled'])\ntrain1 = pd.concat([train,scaled_data],axis=1)\ntrain2 = train1.drop(columns=['Description','Quantity','UnitPrice','Country'])\ntrain3 = train2.drop(columns=['CustomerID'])\ntrain3","ac7a7406":"train3\n# train4.drop( )","e7d2e2a3":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(temptrain,target,test_size=0.2)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","bf36cfad":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit(X_train,y_train)","e02262ab":"prediction = clf.predict(X_train)","f0c96dcd":"from sklearn.metrics import classification_report\nprint(classification_report(y_true=y_train,y_pred=prediction))","8e4cf311":"import nltk, warnings\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer","ac2b3a2b":"import matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.cm as cm\nfrom sklearn import metrics\nimport plotly.graph_objects as go\nimport plotly.express as px\npd.set_option('display.max_rows', 100)\n%config InlineBackend.figure_format = 'svg'\nwarnings.filterwarnings(\"ignore\")","3ede5fbb":"df_cleaned = dff.copy(deep = True)\ndf_cleaned['QuantityCanceled'] = 0","e125d19a":"entry_to_remove = [] ; doubtfull_entry = []","6a13ef33":"remaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\nprint(\"nb of entries to delete: {}\".format(remaining_entries.shape[0]))\nremaining_entries[:5]","9f752d43":"df_cleaned.drop(remaining_entries.index, axis = 0, inplace = True)","c2c0a1bf":"list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\nlist_special_codes","5e85843f":"list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\nlist_special_codes","1f852535":"df_cleaned[(df_cleaned['UnitPrice'] == 0)].head(5)\ndf_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])","3bfdbb38":"uk_ecom = df_cleaned[df_cleaned['Country']=='United Kingdom']\nuk_ecom['Description'] = uk_ecom['Description'].astype(str)\nfreq = pd.Series(' '.join(uk_ecom['Description']).split()).value_counts()[:20]\nfreq","43f02be7":"#Uncommon items\nfreq1 =  pd.Series(' '.join(uk_ecom['Description']).split()).value_counts()[-20:]\nfreq1","83442113":"stop_words = set(stopwords.words(\"english\"))\nnew_words = ['RED','PINK', 'BLUE', 'OF', 'BROWN',\"BLACK\"]\nstop_words = stop_words.union(new_words)","e71bda1f":"for i in new_words:\n  if i in stop_words:\n    print(i)","22404eb3":"import re\nimport nltk.corpus\nfrom nltk import corpus","94017c08":"corpus = []","bb9b8b48":"for i in range(0, 8789):\n    text = re.sub('[^a-zA-Z]', ' ', uk_ecom['Description'].iloc[i])\n    text = text.lower()\n    text=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text)\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    text = text.split()\n    ps=PorterStemmer()\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if word not in stop_words]\n    text = \" \".join(text)\n    i\n    corpus.append(text)","6f225382":"corpus","c0cecce9":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nwordcloud = WordCloud(    background_color='white',\n                          stopwords=stop_words,\n                          max_words=200,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(str(corpus))","56074063":"plt.figure(figsize=(10,5))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Word Cloud for Customer\\'s Products')","be6929e3":"import datetime as dt","13637698":"def extract_days(x):\n\treturn dt.datetime(x.year, x.month, x.day)","c73c8c2a":"uk_ecom['date'] = pd.DatetimeIndex(uk_ecom['InvoiceDate']).date\nuk_ecom['InvoiceDay'] = uk_ecom['date'].apply(extract_days)\ngrouping = uk_ecom.groupby('CustomerID')['InvoiceDay']\nuk_ecom['CohortDay'] = grouping.transform('min')\nprint(uk_ecom.head())","46552dca":"def extract_month_int(x):\n    return dt.datetime(x.year, x.month, 1)\n","e7516eed":"#create invoice month column\nuk_ecom['InvoiceMonth'] = uk_ecom['date'].apply(extract_month_int)\ngrouping = uk_ecom.groupby('CustomerID')['InvoiceMonth']\nuk_ecom['CohortMonth'] = grouping.transform('min')\nuk_ecom.head()","160b7684":"def extract_dates_int(df, column):\n    year = df[column].dt.year\n    month = df[column].dt.month\n    day = df[column].dt.day\n    return year, month, day","cc996ae7":"invoice_year, invoice_month, _ = extract_dates_int(uk_ecom, 'InvoiceMonth')\ncohort_year, cohort_month, _ = extract_dates_int(uk_ecom, 'CohortMonth')\nyears_difference = invoice_year - cohort_year\nmonths_difference = invoice_month - cohort_month\n","fc8af437":"uk_ecom['CohortIndex'] = years_difference * 12 + months_difference + 1\ngrouping = uk_ecom.groupby(['CohortMonth', 'CohortIndex'])\ncohort_data = grouping['CustomerID'].apply(pd.Series.nunique).reset_index()","7e746ea4":"cohort_counts = cohort_data.pivot(index = 'CohortMonth', columns = 'CohortIndex', values = 'CustomerID')\ncohort_sizes = cohort_counts.iloc[:, 0]\nretention = cohort_counts.divide(cohort_sizes, axis = 0)","5bd2c546":"#review the retention table\nretention.round(3) * 100\ngrouping_avg_quantity = uk_ecom.groupby(['CohortMonth', 'CohortIndex'])\ncohort_data_avg_quantity = grouping_avg_quantity['Quantity'].mean().reset_index()\naverage_quantity = cohort_data_avg_quantity.pivot(index = 'CohortMonth', columns = 'CohortIndex', values = 'Quantity')\naverage_quantity.round(1).fillna('')","6f957abe":"plt.figure(figsize = (10, 5))\nplt.title('Retention Rate for Customers in United Kingdom')\nsns.heatmap(data = retention, annot = True, fmt = '.0%', vmin = 0.01, vmax = 0.5, cmap = 'BuGn')\nplt.show()","c833eacf":"data = dat.copy()","0cd2c035":"from mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\ndata_apriori = data[data['Country']=='United Kingdom']\ndata_apriori.head()","f68c578b":"# Which Product and Their Count \ndata_apr = data_apriori.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\ndata_apr.head()","f7261715":"def num(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n\nbasket_new = data_apr.applymap(num)\nbasket_new.head()","8f456cf3":"from mlxtend.frequent_patterns import fpgrowth\nrule_fp = fpgrowth(basket_new, min_support=0.02, use_colnames=True)\nrule_fp","32c4ab78":"items = apriori(basket_new, min_support=0.02, use_colnames=True)\nitems","50be23c9":"# pd.set_option('display.height', 100)\n# pd.set_option('display.max_rows', 100)\npd.options.display.max_rows","d31c7555":"rule = association_rules(items, metric=\"lift\", min_threshold=1)\nrule","105ea2fe":"association_rules(items, metric=\"lift\", min_threshold=1).head(10)","47524252":"## Assosiation Rules(Market Basket Analysis)","9445d82a":"### Building classification model to predict customer group","8fa3ee3c":"K-means clustering algorithm\nK-means is one of the most straightforward clustering analysis to implement. It attempts to divide the dataset into a fixed number k of a group. The k refers to the number of centroids you want in your model. A centroid is a point at the center of each cluster. The algorithm then allocates every data point to the nearest cluster while keeping it as small as possible.\n\nThe appropriate number of clusters is essential for an optimal score. In marketing, a wrong evaluation of it can result in a poor marketing campaign. For example, If you have five kinds of customers, but you only allow them to be segmented into two groups. Your campaign will not target the right audience, which is translated into a low return on investment (ROI)","bd6b277e":"from sklearn.metrics import silhouette_score\n\nsilhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(X, KMeans(n_clusters = n_cluster).fit_predict(X))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() ","5bcd3b99":"### Cohort Analysis","e54db020":"### Relation of unit price with quantity","c6c67cdd":"CLUSTER\n\nUnsupervised is a type of machine learning that concludes from a data set without a target variable. We usually implement this method to find a hidden pattern or understand the structure of the data. It discovers information that may not be visible to the human eye. The most popular method is clustering.\n\nClustering split your data set into several groups. The group are formed of similar data point compared to the other group. We can use this approach for marketing segmentation, medical imaging, and anomaly detection. They are different types of clustering methods, including:\nPartitioning methods\nHierarchical clustering\nFuzzy clustering\nDensity-based clustering\nModel-based clustering\nAnalogy:\n\nYou had a box of apples of different colors; red, green, and mixed colors. Individually picking each color would not only consume time but is likely to entail manifestations of human error. However, with cluster analysis, we can segment these different apples according to color. With such information, one can harvest information such as how many apples belong to which color, which color is of the best quality, or which color has the least quality.","4fc91213":"Interpretation\n\nWe have a deeper understanding of our segmentations from the graphs above, from the median expenditure to each group's number. Among the visualization, we observe three horizontal histograms (median frequency, median monetization and median recency for every group of clients) and one squarity.\n\nAs we can see in the Median Expenditure graph, our two best categories of clients are champions (in green) and loyal customer(in purple). As we can see, both clients provide almost 80% of our revenue and only represent 22% of our total clients! This follows the 80\/20 rule, also known as the Pareto Principle, attributed to the Italian economist Vilfredo Pareto. In one of his papers, Pareto noted that about 80% of Italy's land belonged to approximately 20% of its total population.\n\nThis is a presentation of the three different users:\n\nChampions gather 300 customer who has a median expenditure of $2446 and comes almost every month. For marketers, they should be the most critical clients because they make your market sustainable. In this case, you must regularly reward with the promotion or provide to them privilege access.\n\nPotential Loyalist represents 15% of our customers. They play an essential role in the growth of the market. They already show interest in the brand. The next step is to acquire them by building a relationship.\n\nCustomers that need attention constitutes 10% of our market. They spend and come below the average. We translated this with a potential loss, increased churn. It is vital to reactive interest with discounts and recommendations. Moreover, you may also receive feedback to improve your process.","b9759c2a":"rfm['BasketSize'] = (rfm['Monetization']\/rfm['Frequency']).round(2)","9d348599":"### RFM SEGMENTATION","54796e5d":"### Finding Optimal number of clusters","ece58ac1":"rfm['BasketSize'] = (rfm['Monetization']\/rfm['Frequency']).round(2)"}}