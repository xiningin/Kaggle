{"cell_type":{"650ad9df":"code","d81e25f9":"code","57a44e50":"code","516aa0b6":"code","99cec938":"code","9527f9fb":"code","d34e7e04":"code","18f0aa65":"code","895f3d45":"code","15071450":"code","25ac4226":"code","60b4bbee":"code","71d00f28":"code","35eea68e":"code","23d71fd5":"code","ea3d85c7":"code","30c29c65":"code","20c668ea":"code","af264bd1":"code","1f58ee84":"code","4f09fd98":"code","64fea432":"code","fe0646ae":"code","d1fb94f5":"code","a91bfcbb":"code","560f9d83":"code","fd94b8a8":"code","37aa986f":"code","a2e3a1d1":"code","8e8dd2aa":"code","5323c743":"code","33d07e63":"code","95b83ea5":"code","29724a6b":"code","656ed063":"code","e92325fc":"code","45af1659":"code","d6e43692":"code","5a8bd307":"code","f8ac10d1":"code","af4fcb70":"code","765e4b75":"code","c10e952d":"code","03a09952":"code","42bfdaa9":"code","2d8a88a2":"code","38ef4cb3":"code","974cdb14":"code","85aed4f4":"code","edba9459":"code","c79e38a2":"code","465177d8":"code","45987dd3":"code","2e2fd9e8":"code","e2a8ef25":"code","8a2bd479":"code","d1624a79":"code","49b2cc2b":"code","51afb9ed":"code","b150b8f5":"code","27b929bb":"code","503597c5":"code","25bd744b":"code","7fb66f43":"code","e4f134d0":"code","32072633":"code","b36abbab":"code","37742ff6":"code","080fdf78":"code","48d6ba86":"code","749e0ccc":"code","33d661c3":"code","f4ed1f98":"code","814e8cb3":"code","ff83cc2e":"code","a1a6e768":"code","3a3b0566":"code","8993a698":"code","31b9880b":"code","77e64048":"code","bd04a104":"markdown","5002f8c8":"markdown","12bba6a9":"markdown","a7350883":"markdown","90c7cc7f":"markdown","a8271443":"markdown","3ebc4882":"markdown","67bd1d8a":"markdown","ca23e72f":"markdown","4e8f4f8b":"markdown","4b55ca76":"markdown","957f50da":"markdown","f9e7be2c":"markdown","8cf0f4a3":"markdown","0ae31ed8":"markdown","5bec09ee":"markdown","a6ce560b":"markdown","7144cc54":"markdown","858aca48":"markdown"},"source":{"650ad9df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d81e25f9":"import time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nfrom sklearn.svm import SVC,SVR\nfrom sklearn import datasets\nimport scipy.stats as stats\nimport datetime\nimport optuna\nimport lightgbm as lgb\nimport sklearn.datasets\nimport sklearn.metrics\n\n\nfrom matplotlib.font_manager import FontProperties\n# \u663e\u793a\u4e2d\u6587\u6216\u8005\u65e5\u6587\u7684\u65b9\u6cd5\n# https:\/\/blog.csdn.net\/hezuijiudexiaobai\/article\/details\/104533154\nfont_set = FontProperties(fname=\"\/usr\/share\/fonts\/truetype\/wqy\/wqy-zenhei.ttc\", size=20) ","57a44e50":"# \u5b89\u88c5\u4e2d\u6587\u5b57\u4f53\uff0c\u5e76\u67e5\u770bwqy\u76ee\u5f55\u4e0b\u9762\u662f\u5426\u5df2\u7ecf\u6709wqy-zenhei.ttc\u6587\u4ef6\u4e86\n!apt-get install ttf-wqy-zenhei -y\n!ls \/usr\/share\/fonts\/truetype\/wqy\n!fc-list :lang=zh","516aa0b6":"X, y = datasets.load_boston(return_X_y=True)\nX.shape,y.shape","99cec938":"data_temp=datasets.load_boston()\ndata=pd.DataFrame(data_temp.data,columns=data_temp.feature_names)\ndata","9527f9fb":"starttime=datetime.datetime.now()\nbase_rf=RandomForestRegressor()\nscore=cross_val_score(base_rf,X,y,cv=3,scoring='neg_mean_squared_error')\nbase_rf_score=-score.mean()\nendtime=datetime.datetime.now()\nprocess_time_rf=endtime-starttime\nprint(\"RandomForestRegressor score is {}\".format(-score.mean()))\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_rf))","d34e7e04":"starttime=datetime.datetime.now()\nbase_svr=SVR()\nscore=cross_val_score(base_svr,X,y,cv=3,scoring='neg_mean_squared_error')\nbase_svr_score=-score.mean()\nendtime=datetime.datetime.now()\nprocess_time_svr=endtime-starttime\nprint(\"SVR score is {}\".format(-score.mean()))\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_svr))","18f0aa65":"starttime=datetime.datetime.now()\nbase_knn=KNeighborsRegressor()\nscore=cross_val_score(base_knn,X,y,cv=3,scoring='neg_mean_squared_error')\nbase_knn_score=-score.mean()\nendtime=datetime.datetime.now()\nprocess_time_knn=endtime-starttime\nprint(\"KNN score is {}\".format(-score.mean()))\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_knn))","895f3d45":"# \u67e5\u770b\u4e00\u4e0bX\u6bcf\u6b21feed\u5165\u6570\u636e\u7684\u5c3a\u5bf8\nX[0].shape","15071450":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Input,Dropout\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import EarlyStopping\n\ndef ANN(optimizer = 'adam',neurons=32,batch_size=32,epochs=50,activation='relu',patience=5,loss='mse'):\n    model = Sequential()\n    model.add(Dense(neurons, input_shape=(X.shape[1],), activation=activation))\n    model.add(Dense(neurons, activation=activation))\n    model.add(Dense(neurons, activation=activation))\n    model.add(Dense(1))\n    model.compile(optimizer = optimizer, loss=loss)\n    early_stopping = EarlyStopping(monitor=\"loss\", patience = patience)# early stop patience\n    history = model.fit(X, y,\n              batch_size=batch_size,\n              epochs=epochs,\n              callbacks = [early_stopping],\n              verbose=0) #verbose set to 1 will show the training process\n    return model\n\n# model_ann,history=ANN()","25ac4226":"# plt.plot(history.history['loss'],label='loss')\n# plt.title(\"show ANN model loss result\")\n# plt.legend()\n# plt.show()","60b4bbee":"starttime=datetime.datetime.now()\nbase_ann = KerasRegressor(build_fn=ANN, verbose=0)\nscore = cross_val_score(base_ann, X, y, cv=3,scoring='neg_mean_squared_error')\nbase_ann_score=-score.mean()\nendtime=datetime.datetime.now()\nprocess_time_ann=endtime-starttime\nprint(\"ANN score is {}\".format(str(-score.mean())))\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_ann))","71d00f28":"score=['baseline_score',base_rf_score,base_svr_score,base_knn_score,base_ann_score]\ntime=['baseline_process_time',process_time_rf,process_time_svr,process_time_knn,process_time_ann]\nbase_score=[score,time]\nbase_score_df=pd.DataFrame(data=base_score,columns=['description','RandomForestRegressor','SVR','KNeighborsRegressor','ANN'])\nbase_score_df.set_index('description',inplace=True)\nbase_score_df","35eea68e":"%%time\n#Random Forest\n\nfrom sklearn.model_selection import GridSearchCV\n# Define the hyperparameter configuration space\nrf_params = {\n    'n_estimators': [10, 20, 30],\n    'max_features': ['sqrt',0.5],\n    'max_depth': [15,20,30,50],\n    'min_samples_leaf': [1,2,4,8],\n    \"bootstrap\":[True,False],\n    \"criterion\":['mse','mae']\n}\nstarttime=datetime.datetime.now()\nclf = RandomForestRegressor(random_state=0)\ngrid_rf = GridSearchCV(clf, rf_params, cv=3, scoring='neg_mean_squared_error')\ngrid_rf.fit(X, y)\nprint(grid_rf.best_params_)\nprint(\"MSE:\"+ str(-grid_rf.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_rf=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_rf))","23d71fd5":"%%time\n#SVR\n\nfrom sklearn.model_selection import GridSearchCV\n# Define the hyperparameter configuration space\nsvr_params = {\n     'C': [1,10, 100],\n    \"kernel\":['poly','rbf','sigmoid'],\n    \"degree\":np.arange(1,10,1),\n    \"epsilon\":[0.01,0.1,1]\n}\nstarttime=datetime.datetime.now()\nclf = SVR()\ngrid_svr = GridSearchCV(clf, svr_params, cv=3, scoring='neg_mean_squared_error')\ngrid_svr.fit(X, y)\nprint(grid_svr.best_params_)\nprint(\"MSE:\"+ str(-grid_svr.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_svr=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_svr))","ea3d85c7":"%%time\n#KNN\nfrom sklearn.model_selection import GridSearchCV\nknn_params = {\n    'n_neighbors': [2, 3, 5, 7, 10]\n}\nstarttime=datetime.datetime.now()\nclf = KNeighborsRegressor()\ngrid_knn = GridSearchCV(clf, knn_params, cv=3, scoring='neg_mean_squared_error')\ngrid_knn.fit(X, y)\nprint(grid_knn.best_params_)\nprint(\"MSE:\"+ str(-grid_knn.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_knn=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_knn))","30c29c65":"%%time\n# ANN\nfrom sklearn.model_selection import GridSearchCV\nann_params={\n    \"optimizer\": ['adam'],\n    \"neurons\":[32,64,128],\n#     \"batch_size\":[16],\n#     \"epochs\":[20,40,60],\n#     \"activation\":['sigmoid','relu','tanh'],\n#     \"patience\":[2,5],\n    \"loss\":['mse','mae']\n}\nstarttime=datetime.datetime.now()\nclf = KerasRegressor(build_fn=ANN, verbose=0)\ngrid_ann = GridSearchCV(clf, ann_params, cv=3,scoring='neg_mean_squared_error')\ngrid_ann.fit(X,y)\nprint(grid_ann.best_params_)\nprint(\"MSE:\"+ str(-grid_ann.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_ann=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_ann))","20c668ea":"score=['gridsearch_score',-grid_rf.best_score_,-grid_svr.best_score_,-grid_knn.best_score_,-grid_ann.best_score_]\ntime=['gridsearch_process_time',process_time_rf,process_time_svr,process_time_knn,process_time_ann]\ngridsearch_score=[score,time]\ngridsearch_score_df=pd.DataFrame(data=gridsearch_score,columns=['description','RandomForestRegressor','SVR','KNeighborsRegressor','ANN'])\ngridsearch_score_df.set_index('description',inplace=True)\ngridsearch_score_df","af264bd1":"%%time\n#Random Forest\n\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV\n# Define the hyperparameter configuration space\nrf_params = {\n    'n_estimators': sp_randint(10,100),\n    \"max_features\":sp_randint(1,13),\n    'max_depth': sp_randint(5,50),\n    \"min_samples_split\":sp_randint(2,11),\n    \"min_samples_leaf\":sp_randint(1,11),\n    \"criterion\":['mse','mae']\n}\nn_iter_search=20 #number of iterations is set to 20, you can increase this number if time permits\nstarttime=datetime.datetime.now()\nclf = RandomForestRegressor(random_state=0)\nRandom_rf = RandomizedSearchCV(clf, param_distributions=rf_params,n_iter=n_iter_search,cv=3,scoring='neg_mean_squared_error')\nRandom_rf.fit(X, y)\nprint(Random_rf.best_params_)\nprint(\"MSE:\"+ str(-Random_rf.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_rf=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_rf))","1f58ee84":"%%time\n\n#SVM\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV\nrf_params = {\n    'C': stats.uniform(0,50),\n    \"kernel\":['poly','rbf','sigmoid'],\n    \"epsilon\":stats.uniform(0,1)\n}\nn_iter_search=20\nstarttime=datetime.datetime.now()\nclf = SVR(gamma='scale')\nRandom_svr = RandomizedSearchCV(clf, param_distributions=rf_params,n_iter=n_iter_search,cv=3,scoring='neg_mean_squared_error')\nRandom_svr.fit(X, y)\nprint(Random_svr.best_params_)\nprint(\"MSE:\"+ str(-Random_svr.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_svr=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_svr))","4f09fd98":"%%time\n\n#KNN\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV\nrf_params = {\n    'n_neighbors': sp_randint(1,20),\n}\nn_iter_search=10\nstarttime=datetime.datetime.now()\nclf = KNeighborsRegressor()\nRandom_knn = RandomizedSearchCV(clf, param_distributions=rf_params,n_iter=n_iter_search,cv=3,scoring='neg_mean_squared_error')\nRandom_knn.fit(X, y)\nprint(Random_knn.best_params_)\nprint(\"MSE:\"+ str(-Random_knn.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_knn=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_knn))","64fea432":"%%time\n\n#ANN\nfrom scipy.stats import randint as sp_randint\nfrom random import randrange as sp_randrange\nfrom sklearn.model_selection import RandomizedSearchCV\nrf_params = {\n    'optimizer': ['adam','rmsprop'],\n    'activation': ['relu','tanh'],\n    'loss': ['mse','mae'],\n    'batch_size': [16,32,64],\n    'neurons':sp_randint(10,100),\n    'epochs':[20,50],\n    #'epochs':[20,50,100,200],\n    'patience':sp_randint(3,20)\n}\nn_iter_search=10\nstarttime=datetime.datetime.now()\nclf = KerasRegressor(build_fn=ANN, verbose=0)\nRandom_ann = RandomizedSearchCV(clf, param_distributions=rf_params,n_iter=n_iter_search,cv=3,scoring='neg_mean_squared_error')\nRandom_ann.fit(X, y)\nprint(Random_ann.best_params_)\nprint(\"MSE:\"+ str(-Random_ann.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_ann=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_ann))","fe0646ae":"score=['randomsearch_score',-Random_rf.best_score_,-Random_svr.best_score_,-Random_knn.best_score_,-Random_ann.best_score_]\ntime=['randomsearch_process_time',process_time_rf,process_time_svr,process_time_knn,process_time_ann]\nrandomsearch_score=[score,time]\nrandomsearch_score_df=pd.DataFrame(data=randomsearch_score,columns=['description','RandomForestRegressor','SVR','KNeighborsRegressor','ANN'])\nrandomsearch_score_df.set_index('description',inplace=True)\nrandomsearch_score_df","d1fb94f5":"%%time\n#Random Forest\n\nfrom skopt import Optimizer\nfrom skopt import BayesSearchCV \nfrom skopt.space import Real, Categorical, Integer\n# Define the hyperparameter configuration space\nrf_params = {\n    'n_estimators': Integer(10,100),\n    \"max_features\":Integer(1,13),\n    'max_depth': Integer(5,50),\n    \"min_samples_split\":Integer(2,11),\n    \"min_samples_leaf\":Integer(1,11),\n    \"criterion\":['mse','mae']\n}\nstarttime=datetime.datetime.now()\nclf = RandomForestRegressor(random_state=0)\nBayes_rf = BayesSearchCV(clf, rf_params,cv=3,n_iter=20, scoring='neg_mean_squared_error') \n#number of iterations is set to 20, you can increase this number if time permits\nBayes_rf.fit(X, y)\nprint(Bayes_rf.best_params_)\nbclf = Bayes_rf.best_estimator_\nprint(\"MSE:\"+ str(-Bayes_rf.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_rf=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_rf))","a91bfcbb":"%%time\n\n#SVM\nfrom skopt import Optimizer\nfrom skopt import BayesSearchCV \nfrom skopt.space import Real, Categorical, Integer\nrf_params = {\n    'C': Real(1,50),\n    \"kernel\":['poly','rbf','sigmoid'],\n    'epsilon': Real(0,1)\n}\nstarttime=datetime.datetime.now()\nclf = SVR(gamma='scale')\nBayes_svr = BayesSearchCV(clf, rf_params,cv=3,n_iter=20, scoring='neg_mean_squared_error')\nBayes_svr.fit(X, y)\nprint(Bayes_svr.best_params_)\nprint(\"MSE:\"+ str(-Bayes_svr.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_svr=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_svr))","560f9d83":"%%time\n\n#KNN\nfrom skopt import Optimizer\nfrom skopt import BayesSearchCV \nfrom skopt.space import Real, Categorical, Integer\nrf_params = {\n    'n_neighbors': Integer(1,20),\n}\nstarttime=datetime.datetime.now()\nclf = KNeighborsRegressor()\nBayes_knn = BayesSearchCV(clf, rf_params,cv=3,n_iter=10, scoring='neg_mean_squared_error')\nBayes_knn.fit(X, y)\nprint(Bayes_knn.best_params_)\nprint(\"MSE:\"+ str(-Bayes_knn.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_knn=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_knn))","fd94b8a8":"%%time\n\n#ANN\nfrom skopt import Optimizer\nfrom skopt import BayesSearchCV \nfrom skopt.space import Real, Categorical, Integer\nrf_params = {\n    'optimizer': ['adam','rmsprop'],\n    'activation': ['relu','tanh'],\n    'loss': ['mse','mae'],\n    'batch_size': [16,32,64],\n    'neurons':Integer(10,100),\n    'epochs':[20,50],\n    #'epochs':[20,50,100,200],\n    'patience':Integer(3,20)\n}\nstarttime=datetime.datetime.now()\nclf = KerasRegressor(build_fn=ANN, verbose=0)\nBayes_ann = BayesSearchCV(clf, rf_params,cv=3,n_iter=10, scoring='neg_mean_squared_error')\nBayes_ann.fit(X, y)\nprint(Bayes_ann.best_params_)\nprint(\"MSE:\"+ str(-Bayes_ann.best_score_))\nendtime=datetime.datetime.now()\nprocess_time_ann=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_ann))","37aa986f":"score=['bayesiansearch_score',-Bayes_rf.best_score_,-Bayes_svr.best_score_,-Bayes_knn.best_score_,-Bayes_ann.best_score_]\ntime=['bayesiansearch_process_time',process_time_rf,process_time_svr,process_time_knn,process_time_ann]\nbayessearch_score=[score,time]\nbayessearch_score_df=pd.DataFrame(data=bayessearch_score,columns=['description','RandomForestRegressor','SVR','KNeighborsRegressor','ANN'])\nbayessearch_score_df.set_index('description',inplace=True)\nbayessearch_score_df","a2e3a1d1":"%%time\n\nstarttime=datetime.datetime.now()\n#Random Forest\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\n\nreg = RandomForestRegressor()\n# Define the hyperparameter configuration space\nspace  = [Integer(10, 100, name='n_estimators'),\n            Integer(5, 50, name='max_depth'),\n          Integer(1, 13, name='max_features'),\n          Integer(2, 11, name='min_samples_split'),\n          Integer(1, 11, name='min_samples_leaf'),\n         Categorical(['mse', 'mae'], name='criterion')\n         ]\n# Define the objective function\n@use_named_args(space)\ndef objective(**params):\n    reg.set_params(**params)\n\n    return -np.mean(cross_val_score(reg, X, y, cv=3, n_jobs=-1,\n                                    scoring=\"neg_mean_squared_error\"))\nfrom skopt import gp_minimize\nres_gp_rf = gp_minimize(objective, space, n_calls=20, random_state=0)\n#number of iterations is set to 20, you can increase this number if time permits\nprint(\"MSE:%.4f\" % res_gp_rf.fun)\nprint(res_gp_rf.x)\nendtime=datetime.datetime.now()\nprocess_time_rf=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_rf))","8e8dd2aa":"%%time\nreg_0216 = RandomForestRegressor(n_estimators=22,max_depth=50,max_features=6,min_samples_split=2,min_samples_leaf=1,criterion='mse')\nresult_0216=-np.mean(cross_val_score(reg_0216, X, y, cv=3, n_jobs=-1,scoring=\"neg_mean_squared_error\"))\n# \u548c\u4e0a\u9762\u7684\u2018MSE:27.0873\u2019\u57fa\u672c\u4e00\u6837\uff0c\u2018result_0216\u2019\u8fd9\u4e2a\u503c\u662fmse\u503c\nresult_0216","5323c743":"%%time\n\nstarttime=datetime.datetime.now()\n#SVM\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\n\nreg = SVR(gamma='scale')\nspace  = [Real(1, 50, name='C'),\n          Categorical(['poly','rbf','sigmoid'], name='kernel'),\n          Real(0, 1, name='epsilon'),\n         ]\n\n@use_named_args(space)\ndef objective(**params):\n    reg.set_params(**params)\n\n    return -np.mean(cross_val_score(reg, X, y, cv=3, n_jobs=-1,\n                                    scoring=\"neg_mean_squared_error\"))\nfrom skopt import gp_minimize\nres_gp_svr = gp_minimize(objective, space, n_calls=20, random_state=0)\nprint(\"MSE:%.4f\" % res_gp_svr.fun)\nprint(res_gp_svr.x)\nendtime=datetime.datetime.now()\nprocess_time_svr=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_svr))","33d07e63":"%%time\n\nstarttime=datetime.datetime.now()\n#KNN\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\n\nreg = KNeighborsRegressor()\nspace  = [Integer(1, 20, name='n_neighbors')]\n\n@use_named_args(space)\ndef objective(**params):\n    reg.set_params(**params)\n\n    return -np.mean(cross_val_score(reg, X, y, cv=3, n_jobs=-1,\n                                    scoring=\"neg_mean_squared_error\"))\nfrom skopt import gp_minimize\nres_gp_knn = gp_minimize(objective, space, n_calls=10, random_state=0)\nprint(\"MSE:%.4f\" % res_gp_knn.fun)\nprint(res_gp_knn.x)\nendtime=datetime.datetime.now()\nprocess_time_knn=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_knn))","95b83ea5":"score=['bayesian_gp_minimize_score',res_gp_rf.fun,res_gp_svr.fun,res_gp_knn.fun,'']\ntime=['bayesian_gp_minimize_process_time',process_time_rf,process_time_svr,process_time_knn,'']\nbayesian_gp_minimize_score=[score,time]\nbayesian_gp_minimize_score_df=pd.DataFrame(data=bayesian_gp_minimize_score,columns=['description','RandomForestRegressor','SVR','KNeighborsRegressor','ANN'])\nbayesian_gp_minimize_score_df.set_index('description',inplace=True)\nbayesian_gp_minimize_score_df","29724a6b":"%%time\n\n#Random Forest\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nstarttime=datetime.datetime.now()\n# Define the objective function\ndef objective(params):\n    params = {\n        'n_estimators': int(params['n_estimators']), \n        'max_depth': int(params['max_depth']),\n        'max_features': int(params['max_features']),\n        \"min_samples_split\":int(params['min_samples_split']),\n        \"min_samples_leaf\":int(params['min_samples_leaf']),\n        \"criterion\":str(params['criterion'])\n    }\n    clf = RandomForestRegressor( **params)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, n_jobs=-1,\n                                    scoring=\"neg_mean_squared_error\"))\n\n    return {'loss':score, 'status': STATUS_OK }\n# Define the hyperparameter configuration space\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 10, 100, 1),\n    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n    \"max_features\":hp.quniform('max_features', 1, 13, 1),\n    \"min_samples_split\":hp.quniform('min_samples_split',2,11,1),\n    \"min_samples_leaf\":hp.quniform('min_samples_leaf',1,11,1),\n    \"criterion\":hp.choice('criterion',['mse','mae'])\n}\n\ntrials_rf = Trials()\nbest_rf = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trials_rf)\nprint(\"Random Forest: Hyperopt estimated optimum {}\".format(best_rf))\nendtime=datetime.datetime.now()\nprocess_time_rf=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_rf))","656ed063":"BO_TPE_score_rf=min(trials_rf.losses())\nBO_TPE_score_rf","e92325fc":"trials_rf.trials","45af1659":"trials_rf.results","d6e43692":"%%time\n\n#SVM\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nstarttime=datetime.datetime.now()\ndef objective(params):\n    params = {\n        'C': abs(float(params['C'])), \n        \"kernel\":str(params['kernel']),\n        'epsilon': abs(float(params['epsilon'])),\n    }\n    clf = SVR(gamma='scale', **params)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, n_jobs=-1,\n                                    scoring=\"neg_mean_squared_error\"))\n    \n    return {'loss':score, 'status': STATUS_OK }\n\nspace = {\n    'C': hp.normal('C', 0, 50),\n    \"kernel\":hp.choice('kernel',['poly','rbf','sigmoid']),\n    'epsilon': hp.normal('epsilon', 0, 1),\n}\n\ntrials_svr = Trials()\nbest_svr = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trials_svr)\nprint(\"SVM: Hyperopt estimated optimum {}\".format(best_svr))\nendtime=datetime.datetime.now()\nprocess_time_svr=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_svr))","5a8bd307":"BO_TPE_score_svr=min(trials_svr.losses())\nBO_TPE_score_svr","f8ac10d1":"%%time\n\n#KNN\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nstarttime=datetime.datetime.now()\ndef objective(params):\n    params = {\n        'n_neighbors': abs(int(params['n_neighbors']))\n    }\n    clf = KNeighborsRegressor( **params)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, n_jobs=-1,\n                                    scoring=\"neg_mean_squared_error\"))\n\n    return {'loss':score, 'status': STATUS_OK }\n\nspace = {\n    'n_neighbors': hp.quniform('n_neighbors', 1, 20, 1),\n}\n\ntrials_knn=Trials()\nbest_knn = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10,\n            trials=trials_knn)\nprint(\"KNN: Hyperopt estimated optimum {}\".format(best_knn))\nendtime=datetime.datetime.now()\nprocess_time_knn=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_knn))","af4fcb70":"BO_TPE_score_knn=min(trials_knn.losses())\nBO_TPE_score_knn","765e4b75":"%%time\n\n#ANN\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nstarttime=datetime.datetime.now()\ndef objective(params):\n    params = {\n        \"optimizer\":str(params['optimizer']),\n        \"activation\":str(params['activation']),\n        \"loss\":str(params['loss']),\n        'batch_size': abs(int(params['batch_size'])),\n        'neurons': abs(int(params['neurons'])),\n        'epochs': abs(int(params['epochs'])),\n        'patience': abs(int(params['patience']))\n    }\n    clf = KerasRegressor(build_fn=ANN,**params, verbose=0)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, \n                                    scoring=\"neg_mean_squared_error\"))\n\n    return {'loss':score, 'status': STATUS_OK }\n\nspace = {\n    \"optimizer\":hp.choice('optimizer',['adam','rmsprop']),\n    \"activation\":hp.choice('activation',['relu','tanh']),\n    \"loss\":hp.choice('loss',['mse','mae']),\n    'batch_size': hp.quniform('batch_size', 16, 64, 16),\n    'neurons': hp.quniform('neurons', 10, 100, 10),\n    'epochs': hp.quniform('epochs', 20, 50, 10),\n    'patience': hp.quniform('patience', 3, 20, 3),\n}\n\ntrials_ann=Trials()\nbest_ann = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10,\n            trials=trials_ann)\nprint(\"ANN: Hyperopt estimated optimum {}\".format(best_ann))\nendtime=datetime.datetime.now()\nprocess_time_ann=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_ann))","c10e952d":"BO_TPE_score_ann=min(trials_ann.losses())\nBO_TPE_score_ann","03a09952":"!pip install ngboost","42bfdaa9":"%%time\n\n#NGBoost\nfrom ngboost.ngboost import NGBoost\nfrom ngboost.learners import default_tree_learner\nfrom ngboost.distns import Normal\nfrom ngboost.scores import MLE\nfrom ngboost import NGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import median_absolute_error, mean_absolute_error, mean_squared_error\nfrom hyperopt import hp, tpe, space_eval\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.fmin import fmin\nfrom hyperopt import STATUS_OK, Trials\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nboston = load_boston()\ndata = boston.data\ntarget = boston.target\n\n# 2\u6b21\u6570\u636e\u5212\u5206\uff0c\u8fd9\u6837\u53ef\u4ee5\u5206\u62103\u4efd\u6570\u636e  test  train  validation\nX_intermediate, X_test, y_intermediate, y_test = train_test_split(\n    data, target, shuffle=True, test_size=0.2, random_state=1)\n\n# train\/validation split (gives us train and validation sets)\nX_train, X_validation, y_train, y_validation = train_test_split(X_intermediate,\n                                                                y_intermediate,\n                                                                shuffle=False,\n                                                                test_size=0.25,\n                                                                random_state=1)\n\n# delete intermediate variables\ndel X_intermediate, y_intermediate\n\n# \u663e\u793a\u6570\u636e\u96c6\u7684\u5206\u914d\u6bd4\u4f8b\nprint('train: {}% | validation: {}% | test {}%'.format(\n    round(len(y_train) \/ len(target), 2),\n    round(len(y_validation) \/ len(target), 2),\n    round(len(y_test) \/ len(target), 2)))\n\nstarttime=datetime.datetime.now()\n\n# \u641c\u7d22\u7a7a\u95f4\u8bbe\u5b9a\nb1 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=2)\nb2 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3)\nb3 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=4)\n\nspace = {\n    'learning_rate': hp.uniform('learning_rate', 0.001, 0.5),\n    'minibatch_frac': hp.choice('minibatch_frac', [1.0, 0.5]),\n    'Base': hp.choice('Base', [b1, b2, b3])\n}\n\n# n_estimators\u8868\u793a\u4e00\u5957\u53c2\u6570\u4e0b\uff0c\u6709\u591a\u5c11\u4e2a\u8bc4\u4f30\u5668\uff0c\u7b80\u5355\u8bf4\u5c31\u662f\u8fed\u4ee3\u591a\u5c11\u6b21\ndefault_params = {\"n_estimators\": 20, \"verbose_eval\": 1, \"random_state\": 1}\n\n\ndef objective(params):\n\n    params.update(default_params)\n\n    print(\"current params:\", params)\n    ngb = NGBRegressor(**params).fit(\n        X_train,\n        y_train,\n        X_val=X_validation,\n        Y_val=y_validation,\n        #  \u5047\u5b9an_estimators\u8fed\u4ee3\u5668\u6709100\u4e2a\u8bbe\u5b9a\u4e86\u65e9\u671f\u505c\u6b62\u540e\u4e5f\u8bb8\u4e0d\u5230100\u6b21\u8fed\u4ee3\u5c31\u5b8c\u6210\u4e86\u8bad\u7ec3\u505c\u6b62\u4e86\n        early_stopping_rounds=2)\n    loss = ngb.evals_result['val']['LOGSCORE'][ngb.best_val_loss_itr]\n    results = {'loss': loss, 'status': STATUS_OK}\n\n    return results\n\ntrials_ngb = Trials()\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    best = fmin(\n        fn=objective,\n        space=space,\n        algo=tpe.suggest,\n        # max_evals\u662f\u8bbe\u5b9a\u591a\u5c11\u5957\u53c2\u6570\u7ec4\u5408\uff0c\u7ec4\u5408\u6570\u8d8a\u5927\u51c6\u786e\u5ea6\u53ef\u80fd\u66f4\u9ad8\u4f46\u662f\u8bad\u7ec3\u7684\u65f6\u95f4\u8d8a\u957f\n        max_evals=50,\n        trials=trials_ngb)\n\nbest_params = space_eval(space, best)\n    \nngb_new = NGBRegressor(**best_params).fit(\n    X_train,\n    y_train,\n    X_val=X_validation,\n    Y_val=y_validation,\n    #  \u5047\u5b9an_estimators\u8fed\u4ee3\u5668\u6709100\u4e2a\u8bbe\u5b9a\u4e86\u65e9\u671f\u505c\u6b62\u540e\u4e5f\u8bb8\u4e0d\u5230100\u6b21\u8fed\u4ee3\u5c31\u5b8c\u6210\u4e86\u8bad\u7ec3\u505c\u6b62\u4e86\n    early_stopping_rounds=2)\n\ny_pred = ngb_new.predict(X_test)\ntest_MSE_ngb = mean_squared_error(y_pred, y_test)\nprint('Test MSE_ngb', test_MSE_ngb)\n\nprint(\"NGBoost: Hyperopt estimated optimum {}\".format(best_params))\nendtime=datetime.datetime.now()\nprocess_time_ngb=endtime-starttime\nprint(\"\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\uff08\u79d2\uff09={}\".format(process_time_ngb))","2d8a88a2":"BO_TPE_score_ngb=test_MSE_ngb\nBO_TPE_score_ngb","38ef4cb3":"score=['BO-TPE_score',BO_TPE_score_rf,BO_TPE_score_svr,BO_TPE_score_knn,BO_TPE_score_ann,BO_TPE_score_ngb]\ntime=['BO-TPE_process_time',process_time_rf,process_time_svr,process_time_knn,process_time_ann,process_time_ngb]\nBO_TPE_score=[score,time]\nBO_TPE_score_df=pd.DataFrame(data=BO_TPE_score,columns=['description','RandomForestRegressor','SVR','KNeighborsRegressor','ANN','NGBoost'])\nBO_TPE_score_df.set_index('description',inplace=True)\nBO_TPE_score_df","974cdb14":"X, y = datasets.load_boston(return_X_y=True)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)","85aed4f4":"%%time\n# RandomForestRegressor\n\ndef objective(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 100)\n    max_depth=trial.suggest_int(\"max_depth\", 1, 20)\n    clf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=0)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, \n                                    scoring=\"neg_mean_squared_error\"))\n    return score\n\n# \u56e0\u4e3a\u6211\u4eec\u8981\u83b7\u5f97\u6700\u597d\u7684MSE\uff0c\u6240\u4ee5\u65b9\u5411\u662fmin\u3002direction=\"minimize\"\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=5)\noptuna_rf_mse_score=study.best_value\noptuna_rf_time=(study.best_trial.datetime_complete-study.best_trial.datetime_start).total_seconds()\n# \u79d2\u6570\u8f6c\u5316\u4e3a\u65f6\u95f4\u683c\u5f0f\nm, s = divmod(optuna_rf_time, 60)\nh, m = divmod(m, 60)\noptuna_rf_time=\"%d:%02d:%09f\" % (h, m, s)","edba9459":"# score\u5206\u6570\u662fMSE\uff0c\u8d8a\u5c0f\u8d8a\u597d\noptuna_rf_mse_score,optuna_rf_time","c79e38a2":"%%time\n# GradientBoostingRegressor\n\ndef objective(trial):\n#     \u8bbe\u5b9a\u4e864\u4e2a\u641c\u7d22\u8303\u56f4subsample\uff0cn_estimators\uff0cmax_depth\uff0clr\n    subsample = trial.suggest_discrete_uniform(\"subsample\", 0.1, 1.0, 0.1)\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-1)\n    clf = GradientBoostingRegressor(n_estimators=n_estimators,subsample=subsample,learning_rate=lr,max_depth=max_depth,random_state=0)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, \n                                    scoring=\"neg_mean_squared_error\"))\n    return score\n\n\nstudy_name_gbr = 'optuna-gbr'  # Unique identifier of the study.\nstudy_gbr = optuna.create_study(direction=\"minimize\",study_name=study_name_gbr)\n# \u53ef\u4ee5\u52a0\u8f7dsqlite3\u7684db\u6570\u636e\u5e93\u91cc\u9762\u7684\u4fe1\u606f\n# study = optuna.create_study(study_name='example-study', storage='sqlite:\/\/\/example.db', load_if_exists=True)\n# \u52a0\u8f7d\u540e\u76f4\u63a5\u4f18\u5316\u6a21\u578b\nstudy_gbr.optimize(objective, n_trials=5)\noptuna_gbr_mse_score=study_gbr.best_value\noptuna_gbr_time=(study_gbr.best_trial.datetime_complete-study_gbr.best_trial.datetime_start).total_seconds()\n# \u79d2\u6570\u8f6c\u5316\u4e3a\u65f6\u95f4\u683c\u5f0f\nm, s = divmod(optuna_gbr_time, 60)\nh, m = divmod(m, 60)\noptuna_gbr_time=\"%d:%02d:%09f\" % (h, m, s)","465177d8":"# score\u5206\u6570\u662fMSE\uff0c\u8d8a\u5c0f\u8d8a\u597d\noptuna_gbr_mse_score,optuna_gbr_time","45987dd3":"%%time\n# ANN\n# \u5b98\u7f51optuna\u90fd\u662f\u4f7f\u7528sklearn\u91cc\u9762\u5b9a\u4e49\u597d\u7684\u6a21\u578b\uff0c\u81ea\u5b9a\u4e49\u6a21\u578b\u8981\u60f3\u4f7f\u7528optuna\u6bd4\u8f83\u590d\u6742\u3002\n# \u4e00\u4e9b\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4\u5c31\u53ef\u4ee5\uff0c\u4e0d\u9700\u8981\u8c03\u6574\uff0c\u9ed8\u8ba4\u503c\u57fa\u672c\u90fd\u662fmes\u5206\u6570\u6700\u4f4e\u7684\n\ndef objective(trial):\n    params={\n        \"optimizer\":trial.suggest_categorical(\"optimizer\",['Adam','rmsprop']),\n        \"batch_size\":trial.suggest_categorical(\"batch_size\",[16,32,64]),\n#         \"loss\":trial.suggest_categorical(\"loss\",['mse','mae']),\n#         'neurons': trial.suggest_int(\"neurons\", 10,100,step=10),\n#         'epochs': trial.suggest_int(\"epochs\", 20,50,step=10),\n    }\n\n    clf = KerasRegressor(build_fn=ANN, **params, verbose=0)\n    score = cross_val_score(clf, X, y, cv=2,scoring='neg_mean_squared_error')\n    obtuna_ann_score=-score.mean()\n# \u5b98\u7f51optuna\u90fd\u662f\u4f7f\u7528sklearn\u91cc\u9762\u5b9a\u4e49\u597d\u7684\u6a21\u578b\uff0c\u81ea\u5b9a\u4e49\u6a21\u578b\u8981\u60f3\u4f7f\u7528optuna\u6bd4\u8f83\u590d\u6742\u3002\n    return obtuna_ann_score\n\nstudy_name_ann = 'optuna-ann'  # Unique identifier of the study.\nstudy_ann = optuna.create_study(direction=\"minimize\",study_name=study_name_ann)\nstudy_ann.optimize(objective, n_trials=5)\noptuna_ann_mse_score=study_ann.best_value\noptuna_ann_time=(study_ann.best_trial.datetime_complete-study_ann.best_trial.datetime_start).total_seconds()\n# \u79d2\u6570\u8f6c\u5316\u4e3a\u65f6\u95f4\u683c\u5f0f\nm, s = divmod(optuna_ann_time, 60)\nh, m = divmod(m, 60)\noptuna_ann_time=\"%d:%02d:%09f\" % (h, m, s)","2e2fd9e8":"# mse\u5206\u6570\u662f\u8d8a\u5c0f\u8d8a\u597d\noptuna_ann_mse_score,optuna_ann_time","e2a8ef25":"%%time\n# SVR\n\ndef objective(trial):\n    params={\n        \"kernel\":trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\"]),\n        \"C\":trial.suggest_loguniform(\"C\", 1e-5, 1e2),\n#         'degree': trial.suggest_int(\"degree\", 1,10,step=1),\n#         'epsilon': trial.suggest_float(\"epsilon\", 0.1,0.5,step=0.1),\n    }\n    \n    clf = SVR(**params, gamma=\"scale\")\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, \n                                    scoring=\"neg_mean_squared_error\"))\n    return score\n\nstudy_svr = optuna.create_study(direction=\"minimize\")\nstudy_svr.optimize(objective, n_trials=5)\noptuna_svr_mse_score=study_svr.best_value\noptuna_svr_time=(study_svr.best_trial.datetime_complete-study_svr.best_trial.datetime_start).total_seconds()\n# \u79d2\u6570\u8f6c\u5316\u4e3a\u65f6\u95f4\u683c\u5f0f\nm, s = divmod(optuna_svr_time, 60)\nh, m = divmod(m, 60)\noptuna_svr_time=\"%d:%02d:%09f\" % (h, m, s)","8a2bd479":"# score\u5206\u6570\u662fMSE\uff0c\u8d8a\u5c0f\u8d8a\u597d\noptuna_svr_mse_score,optuna_svr_time","d1624a79":"%%time\n#KNN\n\ndef objective(trial):\n    params = {\n        'n_neighbors': trial.suggest_int(\"n_neighbors\", 1,20,step=1),\n    }\n    clf = KNeighborsRegressor(**params)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, \n                                    scoring=\"neg_mean_squared_error\"))\n    return score\n\n\nstudy_knn = optuna.create_study(direction=\"minimize\")\nstudy_knn.optimize(objective, n_trials=5)\noptuna_knn_mse_score=study_knn.best_value\noptuna_knn_time=(study_knn.best_trial.datetime_complete-study_knn.best_trial.datetime_start).total_seconds()\n# \u79d2\u6570\u8f6c\u5316\u4e3a\u65f6\u95f4\u683c\u5f0f\nm, s = divmod(optuna_knn_time, 60)\nh, m = divmod(m, 60)\noptuna_knn_time=\"%d:%02d:%09f\" % (h, m, s)","49b2cc2b":"# score\u5206\u6570\u662fMSE\uff0c\u8d8a\u5c0f\u8d8a\u597d\noptuna_knn_mse_score,optuna_knn_time","51afb9ed":"%%time\n#XGB\n# \u53c2\u8003 https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/rf.html\n# https:\/\/data-analysis-stats.jp\/%e6%a9%9f%e6%a2%b0%e5%ad%a6%e7%bf%92\/python%e3%81%a7xgboost\/\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\n\ndef objective(trial):\n    params = {\n        'learning_rate': trial.suggest_float(\"learning_rate\",1e-4, 1, log=True),\n        'max_depth': trial.suggest_int(\"max_depth\", 1,46,step=5),\n        'n_estimators': trial.suggest_int(\"n_neighbors\", 100,220,step=30),\n        'objective':'reg:squarederror',\n    }\n    clf = xgb.XGBRegressor(**params)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, \n                                    scoring=\"neg_mean_squared_error\"))\n    return score\n\n\nstudy_xgb = optuna.create_study(direction=\"minimize\")\nstudy_xgb.optimize(objective, n_trials=5)\noptuna_xgb_mse_score=study_xgb.best_value\noptuna_xgb_time=(study_xgb.best_trial.datetime_complete-study_xgb.best_trial.datetime_start).total_seconds()\n# \u79d2\u6570\u8f6c\u5316\u4e3a\u65f6\u95f4\u683c\u5f0f\nm, s = divmod(optuna_xgb_time, 60)\nh, m = divmod(m, 60)\noptuna_xgb_time=\"%d:%02d:%09f\" % (h, m, s)","b150b8f5":"# score\u5206\u6570\u662fMSE\uff0c\u8d8a\u5c0f\u8d8a\u597d\noptuna_xgb_mse_score,optuna_xgb_time","27b929bb":"%%time\n#lightGBM\n# \u53c2\u8003 \n# https:\/\/github.com\/optuna\/optuna\/blob\/master\/examples\/lightgbm_simple.py\n# https:\/\/qiita.com\/TomokIshii\/items\/3729c1b9c658cc48b5cb\n\nimport lightgbm as lgb\n\ndef objective(trial):\n#     \u4e0b\u9762\u8fd9\u4e2a\u662f\u5206\u7c7bclassification\u4f7f\u7528\u7684\u6a21\u578b\uff0c\u4e0d\u80fd\u7528\u5728regressor\n#     dtrain = lgb.Dataset(X_train, label=y_train)\n\n    params = {\n        \"objective\": \"regression\",\n#         \"metric\": \"mse\",\n#         \"verbose\": 0,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\",1e-4, 1, log=True),\n        \"num_leaves\": 31,\n        'learning_rate' : 0.1,\n        'feature_fraction' : 0.9,\n        'bagging_fraction' : 0.8,\n        'bagging_freq': 5,\n        'verbose' : 0,\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n#         \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n#         \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \n    }\n\n    clf = lgb.LGBMRegressor(**params)\n    score = -np.mean(cross_val_score(clf, X, y, cv=3, \n                                    scoring=\"neg_mean_squared_error\"))\n    return score\n\nstudy_lgb = optuna.create_study(direction=\"minimize\")\nstudy_lgb.optimize(objective, n_trials=5)\noptuna_lgb_mse_score=study_lgb.best_value\noptuna_lgb_time=(study_lgb.best_trial.datetime_complete-study_lgb.best_trial.datetime_start).total_seconds()\n# \u79d2\u6570\u8f6c\u5316\u4e3a\u65f6\u95f4\u683c\u5f0f\nm, s = divmod(optuna_lgb_time, 60)\nh, m = divmod(m, 60)\noptuna_lgb_time=\"%d:%02d:%09f\" % (h, m, s)","503597c5":"# score\u5206\u6570\u662fMSE\uff0c\u8d8a\u5c0f\u8d8a\u597d\noptuna_lgb_mse_score,optuna_lgb_time","25bd744b":"score=['Optuna_score',optuna_rf_mse_score,optuna_gbr_mse_score,optuna_ann_mse_score,optuna_svr_mse_score,optuna_knn_mse_score,optuna_xgb_mse_score,optuna_lgb_mse_score]\ntime=['Optuna_process_time',optuna_rf_time,optuna_gbr_time,optuna_ann_time,optuna_svr_time,optuna_knn_time,optuna_xgb_time,optuna_lgb_time]\nOptuna_score=[score,time]\nOptuna_score_df=pd.DataFrame(data=Optuna_score,columns=['description','RandomForestRegressor','GradientBoostingRegressor','ANN','SVR','KNeighborsRegressor','XGboost','LightGBM'])\nOptuna_score_df.set_index('description',inplace=True)\nOptuna_score_df","7fb66f43":"all_result=pd.concat([base_score_df,gridsearch_score_df,randomsearch_score_df,bayessearch_score_df,bayesian_gp_minimize_score_df,BO_TPE_score_df,Optuna_score_df])\nall_result","e4f134d0":"# import pickle\n# all_result.to_csv('all_result.csv')\n# all_result.to_pickle('all_result.pkl')","32072633":"# csvfile=pd.read_csv('\/kaggle\/input\/processed-data-result\/all_result.csv')","b36abbab":"# import pickle\n# with open(\"\/kaggle\/input\/d\/qiulongquan\/processed-data-result\/all_result.pkl\", \"rb\") as f:\n#     all_result = pickle.load(f) #\u8aad\u307f\u51fa\u3057\n# all_result","37742ff6":"all_result_score=all_result.iloc[[0,2,4,6,8,10,12]]\nall_result_score\nall_result_score.reset_index(drop=False, inplace=True)\nall_result_score","080fdf78":"type_df=pd.DataFrame([['RandomForestRegressor']*7,['SVR']*7,['KNeighborsRegressor']*7,['ANN']*7,['NGBoost']*7,['GradientBoostingRegressor']*7,['XGboost']*7,['LightGBM']*7])\ntype_df=type_df.values.reshape(-1,1)\ntype_df=pd.DataFrame(type_df)\ntype_df","48d6ba86":"data_temp_svr=all_result_score[['description','SVR']]\ndata_temp_svr=data_temp_svr.rename(columns={'SVR':'RandomForestRegressor'})\ndata_temp_KNeighborsRegressor=all_result_score[['description','KNeighborsRegressor']]\ndata_temp_KNeighborsRegressor=data_temp_KNeighborsRegressor.rename(columns={'KNeighborsRegressor':'RandomForestRegressor'})\ndata_temp_ANN=all_result_score[['description','ANN']]\ndata_temp_ANN=data_temp_ANN.rename(columns={'ANN':'RandomForestRegressor'})\ndata_temp_NGBoost=all_result_score[['description','NGBoost']]\ndata_temp_NGBoost=data_temp_NGBoost.rename(columns={'NGBoost':'RandomForestRegressor'})\ndata_temp_GradientBoostingRegressor=all_result_score[['description','GradientBoostingRegressor']]\ndata_temp_GradientBoostingRegressor=data_temp_GradientBoostingRegressor.rename(columns={'GradientBoostingRegressor':'RandomForestRegressor'})\ndata_temp_XGboost=all_result_score[['description','XGboost']]\ndata_temp_XGboost=data_temp_XGboost.rename(columns={'XGboost':'RandomForestRegressor'})\ndata_temp_LightGBM=all_result_score[['description','LightGBM']]\ndata_temp_LightGBM=data_temp_LightGBM.rename(columns={'LightGBM':'RandomForestRegressor'})\ntransform_df=pd.concat([all_result_score,data_temp_svr,data_temp_KNeighborsRegressor,data_temp_ANN,data_temp_NGBoost,data_temp_GradientBoostingRegressor,data_temp_XGboost,data_temp_LightGBM],axis=0)\ntransform_df","749e0ccc":"transform_df['model_category']=type_df.values\ntransform_df=transform_df.drop(columns=['SVR','KNeighborsRegressor','ANN','NGBoost','GradientBoostingRegressor','XGboost','LightGBM'])\ntransform_df=transform_df.rename(columns={'RandomForestRegressor':'score'})\ntransform_df","33d661c3":"# \u6709\u4e00\u4e9b\u4f4d\u7f6e\u662fnp.nan\u6240\u4ee5\u9700\u8981\u586b\u51450\nclean_z=transform_df['score'].fillna(0)\n# \u6709\u4e00\u4e9b\u4f4d\u7f6e\u56e0\u4e3a\u6ca1\u6709\u503c\u6240\u4ee5\u9700\u8981\u8865\u4e0a0\nclean_z[clean_z==''] = 0\ntransform_df['score'] = clean_z\ntransform_df","f4ed1f98":"sns.set(style=\"whitegrid\", color_codes=True)\nplt.figure(figsize=(20,10))\nsns.barplot(x=\"description\",y=\"score\",hue=\"model_category\",data=transform_df)\nplt.grid(True)\nplt.title(\"\u4e0d\u540c\u6a21\u578bmse\u6d4b\u8bd5\u5206\u6570\uff08\u5206\u6570\u8d8a\u4f4e\u8d8a\u597d\uff0c\u53ef\u4ee5\u770b\u51faRandomForest,LightGBM,NGBoost\u6a21\u578b\u90fd\u662f\u4e0d\u9519\u7684\u9009\u62e9\uff09\",FontProperties=font_set)\nplt.legend(loc='upper left')\nplt.show()","814e8cb3":"all_result_time=all_result.iloc[[1,3,5,7,9,11,13]]\nall_result_time.reset_index(drop=False, inplace=True)\nall_result_time","ff83cc2e":"type_df=pd.DataFrame([['RandomForestRegressor']*7,['SVR']*7,['KNeighborsRegressor']*7,['ANN']*7,['NGBoost']*7,['GradientBoostingRegressor']*7,['XGboost']*7,['LightGBM']*7])\ntype_df=type_df.values.reshape(-1,1)\ntype_df=pd.DataFrame(type_df)\ntype_df","a1a6e768":"data_temp_svr=all_result_time[['description','SVR']]\ndata_temp_svr=data_temp_svr.rename(columns={'SVR':'RandomForestRegressor'})\ndata_temp_KNeighborsRegressor=all_result_time[['description','KNeighborsRegressor']]\ndata_temp_KNeighborsRegressor=data_temp_KNeighborsRegressor.rename(columns={'KNeighborsRegressor':'RandomForestRegressor'})\ndata_temp_ANN=all_result_time[['description','ANN']]\ndata_temp_ANN=data_temp_ANN.rename(columns={'ANN':'RandomForestRegressor'})\ndata_temp_NGBoost=all_result_time[['description','NGBoost']]\ndata_temp_NGBoost=data_temp_NGBoost.rename(columns={'NGBoost':'RandomForestRegressor'})\ndata_temp_GradientBoostingRegressor=all_result_time[['description','GradientBoostingRegressor']]\ndata_temp_GradientBoostingRegressor=data_temp_GradientBoostingRegressor.rename(columns={'GradientBoostingRegressor':'RandomForestRegressor'})\ndata_temp_XGboost=all_result_time[['description','XGboost']]\ndata_temp_XGboost=data_temp_XGboost.rename(columns={'XGboost':'RandomForestRegressor'})\ndata_temp_LightGBM=all_result_time[['description','LightGBM']]\ndata_temp_LightGBM=data_temp_LightGBM.rename(columns={'LightGBM':'RandomForestRegressor'})\ntransform_df=pd.concat([all_result_time,data_temp_svr,data_temp_KNeighborsRegressor,data_temp_ANN,data_temp_NGBoost,data_temp_GradientBoostingRegressor,data_temp_XGboost,data_temp_LightGBM],axis=0)\ntransform_df","3a3b0566":"transform_df['model_category']=type_df.values\ntransform_df=transform_df.drop(columns=['SVR','KNeighborsRegressor','ANN','NGBoost','GradientBoostingRegressor','XGboost','LightGBM'])\ntransform_df=transform_df.rename(columns={'RandomForestRegressor':'time'})\ntransform_df","8993a698":"# \u6709\u4e00\u4e9b\u4f4d\u7f6e\u662fnp.nan\u6240\u4ee5\u9700\u8981\u586b\u51450:00:00.000000\nclean_z=transform_df['time'].fillna('0:00:00.000000')\n# \u6709\u4e00\u4e9b\u4f4d\u7f6e\u56e0\u4e3a\u6ca1\u6709\u503c\u6240\u4ee5\u9700\u8981\u8865\u4e0a0:00:00.000000\nclean_z[clean_z==''] = '0:00:00.000000'\ntransform_df['time'] = clean_z\ntransform_df","31b9880b":"import datetime\n# \u65f6\u95f4\u8f6c\u6362\uff0c\u628a\u20180:00:00.462353\u2019\u8fd9\u6837\u7684\u65f6\u95f4\u8f6c\u6362\u6210\u6beb\u79d2milliseconds\nfor i in range(len(transform_df.index)):\n    if type(transform_df.iloc[i]['time']) is datetime.timedelta:\n        transform_df.iloc[i]['time']=transform_df.iloc[i]['time'].total_seconds()*1000\n    elif transform_df.iloc[i]['time'] is not None:\n        time_temp=datetime.datetime.strptime(str(transform_df.iloc[i]['time']), \"%H:%M:%S.%f\")\n        millisecond=time_temp.microsecond\/1000.0\n        second=time_temp.second\n        minute=time_temp.minute\n        time=(minute*60+second)*1000+millisecond\n        transform_df.iloc[i]['time']=time\ntransform_df","77e64048":"sns.set(style=\"whitegrid\", color_codes=True)\nplt.figure(figsize=(20,6))\nsns.barplot(x=\"description\",y=\"time\",hue=\"model_category\",data=transform_df)\nplt.grid(True)\nplt.title(\"\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u8d85\u53c2\u6570\u8c03\u6574\u65b9\u6cd5\u7684\u65f6\u95f4\u82b1\u8d39\u7edf\u8ba1\uff08\u65f6\u95f4\u8d8a\u5c11\u8d8a\u597d\uff09\",FontProperties=font_set)\nplt.xticks(rotation=30)\nplt.ylabel(\"time(microseconds)\")\nplt.xlabel(\"description \u5404\u79cd\u8d85\u53c2\u6570\u8c03\u6574\u65b9\u6cd5\",FontProperties=font_set)\nplt.show()","bd04a104":"### Load Boston Housing dataset   \nWe will take the Housing dataset which contains information about different houses in Boston. There are 506 samples and 13 feature variables in this Boston dataset. The main goal is to predict the value of prices of the house using the given features.","5002f8c8":"### HPOptimize using Optuna   \nUse categorical of model\n* RandomforestRegressor  \n* SVM  \n* GradientBoostingRegressor   \n* ANN  \n* KNN  \n* XGB  \n* NGBoost   \n* LightGBM     ","12bba6a9":"#### Sample code for regression problems\n* Dataset used:   \nBoson Housing dataset from sklearn   \n* Machine learning algorithms used:   \nRandom forest (RF), support vector machine (SVM), k-nearest neighbor (KNN)   \n* HPO algorithms used:    \nGrid search, random search, Bayesian Optimization with Tree-structured Parzen Estimator (BO-TPE),\u53e6\u5916\u6211\u4f7f\u7528\u4e86Optuna\u548cHyperOpt.   \n* Performance metric:   \nMean square error (MSE)\n","a7350883":"### load save and csv file","90c7cc7f":"### \u7ed3\u8bba\uff1a\n\n#### \u5728MSE\u6d4b\u8bd5\u65b9\u9762RandomForestRegression,LightGBM,NGBoost\u6bd4\u8f83\u4e0d\u9519\uff08MSE\u5206\u6570\u6bd4\u8f83\u4f4e\uff0cNGBoost\u662f\u6700\u597d\u7684\u6a21\u578b\uff09\uff0c\u5176\u6b21\u662fANN\u795e\u7ecf\u7f51\u7edc\u3002KNN\u548cSVR\u76f8\u5bf9\u6bd4\u8f83\u5dee\uff08MSE\u5206\u6570\u504f\u9ad8\uff09\u3002\n#### \u5728\u65f6\u95f4\u82b1\u8d39\u6d4b\u8bd5\u65b9\u9762LightGBM\uff0cNGBoost\u548cRandomForestRegression\u4e5f\u6bd4\u8f83\u4f4e\u3002\n```\nLightGBM + optuna \u8d85\u53c2\u6570\u8c03\u6574 \u53ef\u4ee5\u83b7\u5f97\u8f83\u597d\u7684\u7cbe\u5ea6\u548c\u8f83\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4\nNGBoost + hyperOPT(\u6216\u8005BO-TPE) \u8d85\u53c2\u6570\u8c03\u6574 \u53ef\u4ee5\u83b7\u5f97\u8f83\u597d\u7684\u7cbe\u5ea6\u548c\u8f83\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4\nRandomForest + randomsearch\u8d85\u53c2\u6570\u8c03\u6574 \u53ef\u4ee5\u83b7\u5f97\u8f83\u597d\u7684\u7cbe\u5ea6\u548c\u8f83\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4\nRandomForest + BO-TPE\u8d85\u53c2\u6570\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u5c1d\u8bd5\n```","a8271443":"#### Create ML model using KNN","3ebc4882":"### show chart using seaborn   \n\u5206\u62102\u5f20\u663e\u793a\uff0c\u4e00\u5f20\u662f\u65f6\u95f4\u7684\u6c47\u603b\uff0c\u4e00\u5f20\u662f\u5206\u6570\u6c47\u603b\uff082 pictures show, one picture is time summarization, one other is score summarization.\uff09","67bd1d8a":"### 1.\u663e\u793a\u5206\u6570\u7edf\u8ba1\u56fe  \n\u9700\u8981\u8fdb\u884c\u6570\u636e\u683c\u5f0f\u8f6c\u6362","ca23e72f":"### Using skopt.gp_minimize","4e8f4f8b":"#### Create ML model using RandomForestRegressor","4b55ca76":"### import package\nInclude Randomforest ,SVM and KNN model in this ipynb. ","957f50da":"#### Create ML model using SVM","f9e7be2c":"### Baseline Machine Learning models: Regressors with Default Hyperparameters","8cf0f4a3":"### HPO Algorithm 1: Grid Search   \n#### grid search\u641c\u7d22\u7cbe\u5ea6\u63d0\u9ad8\u662f\u6700\u9ad8\u7684\uff0c\u4f46\u662f\u65f6\u95f4\u82b1\u8d39\u662f\u6700\u591a\u7684\uff08Search all the given hyper-parameter configurations\uff09  \n```\n\u4f18\u70b9  \n* \u7b80\u5355\u6267\u884c\n\u7f3a\u70b9  \n* \u65f6\u95f4\u82b1\u8d39\u6700\u5927  \n* \u53ea\u5bf9\u5206\u7c7bHP\u6709\u6548\u679c  \n```","0ae31ed8":"### HPO Algorithm 4: BO-GP\nBayesian Optimization with Gaussian Process (BO-GP)\n* Advantages:\n    1. Fast convergence speed for continuous HPs.\n* Disadvantages:\n    1. Poor capacity for parallelization.\n    2. Not efficient with conditional HPs.","5bec09ee":"### save data to pickle and csv file","a6ce560b":"### HPO Algorithm 5: BO-TPE\nBayesian Optimization with Tree-structured Parzen Estimator (TPE)\n\n* Advantages:\n    1. Efficient with all types of HPs.\n    2. Keep conditional dependencies.\n\n* Disadvantages:\n    1. Poor capacity for parallelization.\n    \n* Model Category:\n    1. RandomForest\n    2. ANN\n    3. KNN\n    4. SVM\n    5. NGBoost\n","7144cc54":"### HPO Algorithm 2: Random Search   \n#### Random search\u662f\u968f\u673a\u7684\u5728\u641c\u7d22\u7a7a\u95f4\u91cc\u9762\u641c\u7d22\u8d85\u53c2\u6570\u7ec4\u5408\u503c\uff08Randomly search hyper-parameter combinations in the search space\uff09  \n```\n\u4f18\u70b9  \n* \u6bd4Grid Search\u66f4\u52a0\u9ad8\u6548\n* \u53ef\u4ee5\u5e76\u884c\u5904\u7406\n\u7f3a\u70b9  \n* \u4e0d\u8003\u8651\u524d\u9762\u7684\u8fd0\u884c\u7ed3\u679c\n* \u5bf9\u4e8e\u5e26\u6761\u4ef6\u7684HPs\u6548\u7387\u4e0d\u9ad8  \n```","858aca48":"### 2.\u663e\u793a\u65f6\u95f4\u82b1\u8d39\u7edf\u8ba1\u56fe  \n\u9700\u8981\u6570\u636e\u683c\u5f0f\u8f6c\u6362"}}