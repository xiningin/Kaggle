{"cell_type":{"439ae963":"code","c0b0f75e":"code","54f8bb4d":"code","6fb8a3de":"code","81fcdaa3":"code","b6bbf7b4":"code","6c5c49c5":"code","3e99016a":"code","b82bb164":"code","674bac09":"code","66e4eff5":"code","c5fe035b":"code","263da1d0":"code","1764ee97":"code","9ad9149c":"code","dd7cf535":"code","19bdad57":"code","a37af7fd":"code","d13ef31e":"code","8d9b28de":"code","50f05fb8":"code","c72137e4":"code","7bfe2ba7":"code","252640a8":"code","c2148772":"code","e8672d3e":"code","fd3cc07b":"code","4dc5a9d3":"code","7a08e36b":"code","664a3826":"code","af75ccef":"code","01f9fdd5":"code","15db68ae":"code","d159f480":"code","59313904":"code","b41b1ecb":"code","6bbc43c9":"markdown","e6db15d0":"markdown","3780cd9e":"markdown","920047e3":"markdown","5ac69d1d":"markdown","7cd520cc":"markdown","e589b981":"markdown","ff6cca0e":"markdown","e11701e6":"markdown","59f858db":"markdown","d6ae769a":"markdown","dc762831":"markdown","5de67b37":"markdown","f202bfda":"markdown","707b3ab9":"markdown","57f1d932":"markdown","bda8beb6":"markdown","e22a8654":"markdown","8be8fdbf":"markdown","6c968043":"markdown","1bd63d31":"markdown","936229ea":"markdown","7745c9bd":"markdown","c7987b4c":"markdown","43959cb9":"markdown","accd9515":"markdown","914aaa9a":"markdown"},"source":{"439ae963":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c0b0f75e":"img = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\nlabel = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')","54f8bb4d":"plt.imshow(img[2000])","6fb8a3de":"img[0].shape","81fcdaa3":"\nx = tf.placeholder(tf.float32, shape=[None, 64,64], name='X')\n\nx_image = tf.reshape(x, [-1, 64, 64, 1])\n\n# Placeholder variable for the true labels associated with the images\ny_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\ny_true_cls = tf.argmax(y_true, axis=1)","b6bbf7b4":"def new_conv_layer(input, num_input_channels, filter_size, num_filters, name,stride=[1, 1, 1, 1]):\n    \n    with tf.variable_scope(name) as scope:\n        # Shape of the filter-weights for the convolution\n        shape = [filter_size, filter_size, num_input_channels, num_filters]\n\n        # Create new weights (filters) with the given shape\n        weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n\n        # Create new biases, one for each filter\n        biases = tf.Variable(tf.constant(0.05, shape=[num_filters]))\n\n        # TensorFlow operation for convolution\n        layer = tf.nn.conv2d(input=input, filter=weights, strides=stride, padding='SAME')\n\n        # Add the biases to the results of the convolution.\n        layer += biases\n        \n        return layer, weights","6c5c49c5":"def new_pool_layer(input,name,stride=[1,2,2,1],ksize=[1,2,2,1]):\n  with tf.variable_scope(name) as scope:\n    layer = tf.nn.max_pool(input,strides=[1,2,2,1],ksize=[1,2,2,1],padding='SAME')\n    return layer\n  ","3e99016a":"def new_relu_layer(input,name):\n  with tf.variable_scope(name) as scope:\n        layer = tf.nn.relu(input)\n        \n        return layer","b82bb164":"\ndef new_fc_layer(input, num_inputs, num_outputs, name):\n\n  with tf.variable_scope(name) as scope:\n\n      # Create new weights and biases.\n      weights = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.05))\n      biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n\n      # Multiply the input and weights, and then add the bias-values.\n      layer = tf.matmul(input, weights) + biases\n\n      return layer\n\n\n  \n  ","674bac09":"layer_conv1, weights_conv1 = new_conv_layer(input=x_image, num_input_channels=1, filter_size=3, num_filters=6, name =\"conv1\")\n\nlayer_pool1 = new_pool_layer(layer_conv1,'pool1')\n\nlayer_relu1  =new_relu_layer(layer_pool1,'relu1')\n","66e4eff5":"layer_conv2, weights_conv2 = new_conv_layer(input=layer_relu1, num_input_channels=6, filter_size=3, num_filters=32, name =\"conv2\")\n\nlayer_pool2 = new_pool_layer(layer_conv2,'pool2')\n\nlayer_relu2  =new_relu_layer(layer_pool2,'relu2')\n","c5fe035b":"layer_conv3, weights_conv3 = new_conv_layer(input=layer_relu2, num_input_channels=32, filter_size=3, num_filters=64, name =\"conv3\")\n\nlayer_pool3 = new_pool_layer(layer_conv3,'pool3')\n\n\nlayer_relu3  =new_relu_layer(layer_pool3,'relu3')\n","263da1d0":"num_features = layer_relu3.get_shape()[1:4].num_elements()\nlayer_flat = tf.reshape(layer_relu3, [-1, num_features])\n\nlayer_fc1 = new_fc_layer(layer_flat, num_inputs=num_features, num_outputs=128,name='fc1')\n\n### Batch Normalisation\n\nlayer_bn = tf.contrib.layers.batch_norm(layer_fc1 ,center=True, scale=True)\n\nlayer_relu4 = new_relu_layer(layer_bn, name=\"relu3\")\n\nlayer_fc2 = new_fc_layer(input=layer_relu4, num_inputs=128, num_outputs=10, name=\"fc2\")","1764ee97":"with tf.variable_scope('softmax'):\n  y_pred = tf.nn.softmax(layer_fc2)\n  y_pred_cls = tf.argmax(y_pred, axis=1)","9ad9149c":"with tf.name_scope(\"cross_ent\"):\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc2, labels=y_true)\n    cost = tf.reduce_mean(cross_entropy)","dd7cf535":"with tf.name_scope('opt'):\n  optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nwith tf.name_scope(\"accuracy\"):\n    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","19bdad57":"# Initialize the FileWriter\nwriter = tf.summary.FileWriter(\"Training_FileWriter\/\")\nwriter1 = tf.summary.FileWriter(\"Validation_FileWriter\/\")\n# Add the cost and accuracy to summary\ntf.summary.scalar('loss', cost)\ntf.summary.scalar('accuracy', accuracy)\n\n# Merge all summaries together\nmerged_summary = tf.summary.merge_all()","a37af7fd":"num_epochs = 20\nbatchSize = 100\nimport time","d13ef31e":"def printResult(epoch, numberOfEpoch, trainLoss, validationLoss, validationAccuracy):\n    print(\"Epoch: {}\/{}\".format(epoch+1, numberOfEpoch),\n         '\\tTraining Loss: {:.3f}'.format(trainLoss),\n         '\\tValidation Loss: {:.3f}'.format(validationLoss),\n         '\\tAccuracy: {:.2f}%'.format(validationAccuracy*100))","8d9b28de":"from sklearn.model_selection import train_test_split\nwith tf.Session () as sess:\n    sess.run(tf.global_variables_initializer())    \n    for epoch in range(num_epochs):\n        # training data & validation data\n        train_x, val_x, train_y, val_y = train_test_split(img, label,\\\n                                                      test_size = 0.2)   \n        # training loss\n        for i in range(0, len(train_x), 100):\n            trainLoss, _= sess.run([cost, optimizer], feed_dict = {\n                x: train_x[i: i+batchSize],\n                y_true: train_y[i: i+batchSize]\n            })\n            \n        # validation loss\n        valAcc, valLoss = sess.run([accuracy, cost], feed_dict ={\n            x: val_x,\n            y_true: val_y,})\n        \n        \n        # print out\n        printResult(epoch, num_epochs, trainLoss, valLoss, valAcc)","50f05fb8":"\nx = tf.placeholder(tf.float32, shape=[None, 64,64], name='X')\n\nx_image = tf.reshape(x, [-1, 64, 64, 1])\n\n# Placeholder variable for the true labels associated with the images\ny_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\ny_true_cls = tf.argmax(y_true, axis=1)","c72137e4":"layer, weights = new_conv_layer(input=x_image, num_input_channels=1, filter_size=3, num_filters=96, name =\"conv1\",stride=[1,4,4,1])\nlayer  =new_relu_layer(layer,'relu1')\n\nlayer, weights = new_conv_layer(input=layer, num_input_channels=96, filter_size=3, num_filters=96, name =\"conv1\",stride=[1,2,2,1])\nlayer  =new_relu_layer(layer,'relu1')\n\nlayer = new_pool_layer(layer,'pool1',)\n\n#batch norm\nlayer, weights = new_conv_layer(input=layer, num_input_channels=96, filter_size=5, num_filters=256, name =\"conv1\")\nlayer =new_relu_layer(layer,'relu1')\nlayer = new_pool_layer(layer,'pool1',ksize=[1,3,3,1])\n#BAtch norm\nlayer = tf.contrib.layers.batch_norm(layer ,center=True, scale=True)\n\n\n\nlayer, weights = new_conv_layer(input=layer, num_input_channels=256, filter_size=5, num_filters=384, name =\"conv1\",)\nlayer =new_relu_layer(layer,'relu1')\n\nlayer ,weights= new_conv_layer(input=layer, num_input_channels=384, filter_size=5, num_filters=384, name =\"conv1\",)\nlayer  =new_relu_layer(layer,'relu1')\n\nlayer, weights = new_conv_layer(input=layer, num_input_channels=384, filter_size=5, num_filters=256, name =\"conv1\")\nlayer =new_relu_layer(layer,'relu1')\nlayer = new_pool_layer(layer,'pool1',ksize=[1,3,3,1])\n#batch norm\nlayer = tf.contrib.layers.batch_norm(layer ,center=True, scale=True)\n\n\nnum_features = layer.get_shape()[1:4].num_elements()\nlayer= tf.reshape(layer, [-1, num_features])\n\nlayer= new_fc_layer(layer, num_inputs=num_features, num_outputs=4096,name='fc1')\n\nlayer = tf.layers.dropout(layer,rate=0.5)\n\nlayer= new_relu_layer(layer, name=\"relu3\")\n\nlayer = new_fc_layer(input=layer, num_inputs=4096, num_outputs=4096, name=\"fc2\")\nlayer = tf.layers.dropout(layer,rate=0.5)\n\nlayer = new_fc_layer(input=layer, num_inputs=4096, num_outputs=10, name=\"fc2\")\n#layer = tf.layers.dropout(layer,rate=0.5)\n\n#layer = new_fc_layer(input=layer, num_inputs=128, num_outputs=10, name=\"fc2\")\n","7bfe2ba7":"with tf.variable_scope('softmax'):\n  y_pred = tf.nn.softmax(layer)\n  y_pred_cls = tf.argmax(y_pred, axis=1)","252640a8":"with tf.name_scope(\"cross_ent\"):\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer, labels=y_true)\n    cost = tf.reduce_mean(cross_entropy)\n\nwith tf.name_scope('opt'):\n  optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nwith tf.name_scope(\"accuracy\"):\n    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","c2148772":"# Initialize the FileWriter\nwriter = tf.summary.FileWriter(\"Training_FileWriter\/\")\nwriter1 = tf.summary.FileWriter(\"Validation_FileWriter\/\")\n# Add the cost and accuracy to summary\ntf.summary.scalar('loss', cost)\ntf.summary.scalar('accuracy', accuracy)\n\n# Merge all summaries together\nmerged_summary = tf.summary.merge_all()","e8672d3e":"num_epochs = 20\nbatchSize = 100\nimport time","fd3cc07b":"def printResult(epoch, numberOfEpoch, trainLoss, validationLoss, validationAccuracy):\n    print(\"Epoch: {}\/{}\".format(epoch+1, numberOfEpoch),\n         '\\tTraining Loss: {:.3f}'.format(trainLoss),\n         '\\tValidation Loss: {:.3f}'.format(validationLoss),\n         '\\tAccuracy: {:.2f}%'.format(validationAccuracy*100))","4dc5a9d3":"from sklearn.model_selection import train_test_split\nwith tf.Session () as sess:\n    sess.run(tf.global_variables_initializer())    \n    for epoch in range(num_epochs):\n        # training data & validation data\n        train_x, val_x, train_y, val_y = train_test_split(img, label,\\\n                                                      test_size = 0.2)   \n        # training loss\n        for i in range(0, len(train_x), 100):\n            trainLoss, _= sess.run([cost, optimizer], feed_dict = {\n                x: train_x[i: i+batchSize],\n                y_true: train_y[i: i+batchSize]\n            })\n            \n        # validation loss\n        valAcc, valLoss = sess.run([accuracy, cost], feed_dict ={\n            x: val_x,\n            y_true: val_y,})\n        \n        \n        # print out\n        printResult(epoch, num_epochs, trainLoss, valLoss, valAcc)","7a08e36b":"def fire(inputs,squeezeTo,expandTo):\n    h = squeeze(inputs,squeezeTo)\n    h = expand(h,expandTo)\n    h = tf.clip_by_norm(h,NORM) # Very important\n    activations.append(h)\n\ndef squeeze(inputs,squeezeTo):\n    with tf.name_scope('squeeze'):\n        inputSize = inputs.get_shape().as_list()[3]\n        w = tf.Variable(tf.truncated_normal([1,1,inputSize,squeezeTo]))\n        h = tf.nn.relu(tf.nn.conv2d(inputs,w,[1,1,1,1],'SAME'))        \n    return h\n\ndef expand(inputs,expandTo):\n    with tf.name_scope('expand'):\n        squeezeTo = inputs.get_shape().as_list()[3]\n        w = tf.Variable(tf.truncated_normal([1,1,squeezeTo,expandTo]))\n        h1x1 = tf.nn.relu(tf.nn.conv2d(inputs,w,[1,1,1,1],'SAME'))\n        w = tf.Variable(tf.truncated_normal([3,3,squeezeTo,expandTo]))\n        h3x3 = tf.nn.relu(tf.nn.conv2d(inputs,w,[1,1,1,1],'SAME'))\n        h = tf.concat([h1x1,h3x3],3)\n    return h","664a3826":"x = tf.placeholder(tf.float32, shape=[None, 64,64], name='X')\n\nx_image = tf.reshape(x, [-1, 64, 64, 1])\n\n# Placeholder variable for the true labels associated with the images\ny_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\ny_true_cls = tf.argmax(y_true, axis=1)","af75ccef":"layer, weights = new_conv_layer(input=x_image, num_input_channels=1, filter_size=3, num_filters=96, name =\"conv1\",stride=[1,4,4,1])\nlayer  =new_relu_layer(layer,'relu1')\n\nlayer, weights = new_conv_layer(input=layer, num_input_channels=96, filter_size=3, num_filters=96, name =\"conv1\",stride=[1,2,2,1])\nlayer  =new_relu_layer(layer,'relu1')\n\nlayer = new_pool_layer(layer,'pool1',)\n\n\n# Squeeze layer\nh = squeeze(layer,16)\nlayer = expand(h,64)\n#*****************************************************************************************************************************************\n# fire_squeeze, weights = new_conv_layer(input=layer, num_input_channels=96, filter_size=1, num_filters=16, name =\"conv1\",stride=[1,1,1,1])\n# fire_squeeze  = new_relu_layer(fire_squeeze,'relu1')\n\n# fire_expand1, weights = new_conv_layer(input=fire_squeeze, num_input_channels=16, filter_size=3, num_filters=64, name =\"conv1\",stride=[1,1,1,1])\n# fire_expand1  = new_relu_layer(fire_expand1,'relu3')\n\n# fire_expand2, weights = new_conv_layer(input=fire_squeeze, num_input_channels=16, filter_size=1, num_filters=64, name =\"conv1\",stride=[1,1,1,1])\n# fire_expand2  = new_relu_layer(fire_expand2,'relu4')\n\n# layer, weights = tf.concat([fire_expand1,fire_expand2],axis=3)\n#*****************************************************************************************************************************************\n\n\n\n#batch norm\nlayer, weights = new_conv_layer(input=layer, num_input_channels=128, filter_size=5, num_filters=256, name =\"conv1\")\nlayer =new_relu_layer(layer,'relu1')\nlayer = new_pool_layer(layer,'pool1',ksize=[1,3,3,1])\n#BAtch norm\nlayer = tf.contrib.layers.batch_norm(layer ,center=True, scale=True)\n\n\n\nlayer, weights = new_conv_layer(input=layer, num_input_channels=256, filter_size=5, num_filters=384, name =\"conv1\",)\nlayer =new_relu_layer(layer,'relu1')\n\nlayer ,weights= new_conv_layer(input=layer, num_input_channels=384, filter_size=5, num_filters=384, name =\"conv1\",)\nlayer  =new_relu_layer(layer,'relu1')\n\nlayer, weights = new_conv_layer(input=layer, num_input_channels=384, filter_size=5, num_filters=256, name =\"conv1\")\nlayer =new_relu_layer(layer,'relu1')\nlayer = new_pool_layer(layer,'pool1',ksize=[1,3,3,1])\n#batch norm\nlayer = tf.contrib.layers.batch_norm(layer ,center=True, scale=True)\n\n\nnum_features = layer.get_shape()[1:4].num_elements()\nlayer= tf.reshape(layer, [-1, num_features])\n\nlayer= new_fc_layer(layer, num_inputs=num_features, num_outputs=4096,name='fc1')\n\nlayer = tf.layers.dropout(layer,rate=0.5)\n\nlayer= new_relu_layer(layer, name=\"relu3\")\n\nlayer = new_fc_layer(input=layer, num_inputs=4096, num_outputs=4096, name=\"fc2\")\nlayer = tf.layers.dropout(layer,rate=0.5)\n\nlayer = new_fc_layer(input=layer, num_inputs=4096, num_outputs=10, name=\"fc2\")\n#layer = tf.layers.dropout(layer,rate=0.5)\n\n#layer = new_fc_layer(input=layer, num_inputs=128, num_outputs=10, name=\"fc2\")\n","01f9fdd5":"with tf.variable_scope('softmax'):\n  y_pred = tf.nn.softmax(layer)\n  y_pred_cls = tf.argmax(y_pred, axis=1)","15db68ae":"with tf.name_scope(\"cross_ent\"):\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer, labels=y_true)\n    cost = tf.reduce_mean(cross_entropy)\n\nwith tf.name_scope('opt'):\n  optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nwith tf.name_scope(\"accuracy\"):\n    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","d159f480":"# Initialize the FileWriter\nwriter = tf.summary.FileWriter(\"Training_FileWriter\/\")\nwriter1 = tf.summary.FileWriter(\"Validation_FileWriter\/\")\n# Add the cost and accuracy to summary\ntf.summary.scalar('loss', cost)\ntf.summary.scalar('accuracy', accuracy)\n\n# Merge all summaries together\nmerged_summary = tf.summary.merge_all()","59313904":"num_epochs = 20\nbatchSize = 100\nimport time","b41b1ecb":"from sklearn.model_selection import train_test_split\nwith tf.Session () as sess:\n    sess.run(tf.global_variables_initializer())    \n    for epoch in range(num_epochs):\n        # training data & validation data\n        train_x, val_x, train_y, val_y = train_test_split(img, label,\\\n                                                      test_size = 0.2)   \n        # training loss\n        for i in range(0, len(train_x), 100):\n            trainLoss, _= sess.run([cost, optimizer], feed_dict = {\n                x: train_x[i: i+batchSize],\n                y_true: train_y[i: i+batchSize]\n            })\n            \n        # validation loss\n        valAcc, valLoss = sess.run([accuracy, cost], feed_dict ={\n            x: val_x,\n            y_true: val_y,})\n        \n        \n        # print out\n        printResult(epoch, num_epochs, trainLoss, valLoss, valAcc)","6bbc43c9":"![](https:\/\/cdn-images-1.medium.com\/max\/1536\/1*qyc21qM0oxWEuRaj-XJKcw.png)","e6db15d0":"Implemetation in tensorflow-","3780cd9e":"### Does it really work?\nMost simplistic explanation would be that 1x1 convolution leads to dimension reductionality. For example, an image of 200 x 200 with 50 features on convolution with 20 filters of 1x1 would result in size of 200 x 200 x 20. But then again, is this is the best way to do dimensionality reduction in the convoluational neural network? What about the efficacy vs efficiency?\nrefer to this blog-http:\/\/iamaaditya.github.io\/2016\/03\/one-by-one-convolution\/","920047e3":"## Context\nSign languages (also known as signed languages) are languages that use manual communication to convey meaning. This can include simultaneously employing hand gestures, movement, orientation of the fingers, arms or body, and facial expressions to convey a speaker's ideas. Source: https:\/\/en.wikipedia.org\/wiki\/Sign_language","5ac69d1d":"## Writing Alexnet in tensorflow:\n","7cd520cc":"# Creating Placeholders\nA placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed data into the graph through these placeholders.<br>\nFirst we create a PlaceHolder like x in this case and then we will feed it during the session later on.<br>\n### Shape of Placeholders:\nI simply follow the rule that give [None,img_width,img_hieght] .\nand then convert this into the shape according to your requirement.\n","e589b981":"## Details of datasets:\nImage size: 64x64<br>\nColor space: Grayscale<br>\nFile format: npy<br>\nNumber of classes: 10 (Digits: 0-9)<br>\nNumber of participant students: 218<br>\nNumber of samples per student: 10","ff6cca0e":"# Tensorflow tutorial with different types of architecture.","e11701e6":"### Technique 2nd\nReduce the number of inputs for the remaining 3x3 filters<br>\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*C4Y78hoaN0hPxyWJnkG5vQ.png)","59f858db":"### Content:\n1)Basic tensorflow CNN's <br>\n2)Writing Alexnet from basic in tensorflow. <br>\n3)Introduction to squeezenets.","d6ae769a":"# Creating a ConvLayer\nMake  a function which will make convolutional layer according to your inputs.\n![](https:\/\/harishnarayanan.org\/images\/writing\/artistic-style-transfer\/conv-layer.gif)","dc762831":"## Defining activation function.\nSee in this image,it is very well defined that how activation function are used in NN.<br>\nType:<br>\n1)RELU<br>\n2)Tanh<br>\n3)Sigmoid<br>\n4)Leaky Relu<br>\n![](https:\/\/harishnarayanan.org\/images\/writing\/artistic-style-transfer\/neuron.gif)","5de67b37":"# Sessions in Tensorflow graphs:","f202bfda":"# Fully connected layer.\nAfter several convolution layers we use fully connected layer at the end as shown in picture below:\n![](https:\/\/www.kernix.com\/doc\/data\/cnn.png)","707b3ab9":"# Plotting the 2000th Sign image","57f1d932":"## These squeeze nets are so much memory efficient that they can easily be run on mobile phone also. \nThanks or reading<br>\nCOmment your valuable suggestions.<br>\nDo upvote, if you like!.<br>\nThanks<br>\n","bda8beb6":"# The above alexnet takes too much memory ,in order to reduce this i will try squeeze nets.\nStay tuned.","e22a8654":"### As you can see in above case the \nTotal channels=3<br>\nFilter_Size=3X3","8be8fdbf":"# Introduction to squeeze nets.\nThe make a network smaller by starting with a smarter design versus using a clever compression scheme.They reduce the parameters to approz 50 folds and giving accuracy almost same.<br>\nThey use 3 techniques to do this .<br>\n### Technique one.\n1) Make the network smaller by replacing 3x3 filters with 1x1 filters.<br>\nIn simple words, In my network where i was using filter size=5X5 or 3X3 replace it with 1X1. Sounds weird?\n\n![](https:\/\/raw.githubusercontent.com\/iamaaditya\/iamaaditya.github.io\/master\/images\/conv_arithmetic\/full_padding_no_strides_transposed_small.gif)\n","6c968043":"## Just go through each and every output layer carefully for dimensions","1bd63d31":"Here Strides =[1,2,2,1] means take a stride of 2 in x and y as well.<br>\nKernel_size=[1,2,2,1] mean take a block of this dimension and apply max function for that block and jump according to strides.","936229ea":"As you can see above, \u201csqueeze\u201d layers are convolution layers that are made up of only 1x1 filters and \u201cexpand\u201d layers are convolution layers with a mix of 1x1 and 3x3 filters. <br>\nBy reducing the number of filters in the \u201csqueeze\u201d layer feeding into the \u201cexpand\u201d layer, they are reducing the number of connections entering these 3x3 filters thus reducing the total number of parameters. The authors of this paper call this specific architecture the \u201cfire module\u201d and it serves as the basic building block for the SqueezeNet architecture.","7745c9bd":"# Defining a Pooling Layer\n![](http:\/\/cs231n.github.io\/assets\/cnn\/maxpool.jpeg)","c7987b4c":"# Model Architecture:\nConv layer - > Pool Layer - > Relu Layer - >Conv layer - > Pool Layer - > Relu Layer - >Conv layer - > Pool Layer - > Relu Layer - > Flatten ->Fully connected1 - > Batch Norm - > Fully connected2(10 )","43959cb9":"### Technique 3rd\nDownsample late in the network so that convolution layers have large activation maps.","accd9515":"**Why do we use batch normalization?**<br>\nWe normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and get 10 times or more improvement in the training speed.","914aaa9a":"## In short remember\nWe replace a conv layer with large variable by a cell which has structure like-<br>\n->Fire squeeze -> (fire expand1+fire expand2) -> merge_layer\n\nwhere bracket() means we merged them into one layer\n"}}