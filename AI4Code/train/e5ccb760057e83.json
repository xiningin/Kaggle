{"cell_type":{"1ce750bf":"code","039f3494":"code","72fd0b65":"code","8e40f89b":"code","868ff362":"code","8cefb3f5":"code","63bde0d4":"code","84d93974":"code","8e3346f1":"code","e3409a6a":"code","d313944b":"code","20c965ba":"code","4eb848a4":"code","635505c3":"code","2cc30edf":"code","8a958546":"code","018ab517":"code","dc49ee98":"code","6f01b96c":"code","3a98b28b":"code","10c0a8df":"code","8f8f2a77":"code","f699f638":"code","1f615169":"code","9f9caea5":"code","9d553807":"code","22123eb0":"code","86d48827":"code","c0e1a8bc":"code","925f7d05":"code","ff9ab6bf":"code","0e44dfa0":"code","761c48ce":"code","472304bd":"code","3b3ec0f3":"code","251ae22c":"code","16cff0bd":"code","f77edbd7":"code","1fffac5b":"code","49589b45":"code","20e8d5e3":"code","9617fa41":"code","7de46001":"code","9fbf2455":"code","215fd9ae":"code","5e55694e":"code","f07156a8":"code","546385b7":"code","51c77905":"code","d75548b1":"code","91bf21ec":"code","2ae9ece5":"code","5b0ece6e":"code","70e7862b":"code","025e0158":"code","1bd8f5d5":"code","c7b3cbf1":"code","5fe49d55":"code","9ae06797":"code","3b040500":"code","5376900b":"code","5af9eafc":"code","029df994":"code","df5fe6f2":"code","9d4bc448":"code","ceffdcb0":"code","7c573ab6":"code","84806695":"code","50568e53":"code","3c0223ce":"code","d6a53df6":"code","faaf2c9b":"code","e34d0153":"code","2219fc7f":"code","f81dd583":"code","99b2e1f0":"code","56e0bdf1":"code","250fc479":"code","b5057168":"code","5d495840":"code","0e8dd3ee":"code","ac1d997a":"code","3b5d83f0":"code","00a6fef5":"code","f705760f":"code","d0f2f4b9":"code","e3c5d39f":"code","9f497192":"code","58189ed9":"code","409ec831":"code","4d0df6b6":"code","5d1b6514":"code","33cedc26":"code","7f6411c4":"code","c09e89cf":"code","341dd410":"code","2a840e34":"code","edc63ca7":"code","ba1371d6":"code","0214060a":"code","b2518027":"code","ae6b13a2":"code","87cadd84":"code","1d99bf66":"code","4fcc243b":"code","7c87adbb":"code","66f431df":"code","bb74006d":"code","a087ec83":"code","f49ad1ea":"code","b6ed82ee":"code","075c85f5":"markdown","b9c1414c":"markdown","1931cb43":"markdown","050bfe88":"markdown","254adfa9":"markdown","83ba30f6":"markdown","b135e6c2":"markdown","5086d8ef":"markdown","943dc38f":"markdown","0d2caab7":"markdown","bfb0b63c":"markdown","550d5a1f":"markdown","b09bc1b2":"markdown","05cb595e":"markdown","f86a2aca":"markdown","3ef9034c":"markdown","c4afda5c":"markdown","13995ec9":"markdown","a32b50a3":"markdown","c1c20638":"markdown","4d6fac7b":"markdown","dfb0e092":"markdown","bd1f5129":"markdown","56007ebb":"markdown","69f0e114":"markdown","4137309d":"markdown","7c2ec84f":"markdown","749746dc":"markdown","b1c78bd6":"markdown","1f88b7ac":"markdown","3331f47d":"markdown","c3025b3a":"markdown","384f808f":"markdown","221a7840":"markdown","8bb9fa64":"markdown","a17d14c7":"markdown","aeb97d17":"markdown","75e0c619":"markdown","0d189454":"markdown","4473489d":"markdown","2486b7ed":"markdown","9e604dc6":"markdown","6f0ad0aa":"markdown","7dc207e0":"markdown","e1c64565":"markdown","ab1cc633":"markdown","f588876c":"markdown","6eaf0f87":"markdown","a50761cb":"markdown","089c6227":"markdown","8dfb46b6":"markdown","52f1a1f9":"markdown","873fff22":"markdown","a245a2ba":"markdown","f8c0b9ce":"markdown","fea7cac3":"markdown","23de0902":"markdown","ae431b47":"markdown","94db4b58":"markdown","4f1a26e9":"markdown","2f132a99":"markdown","e34bcddf":"markdown","7c918504":"markdown","c2fab794":"markdown","575c0dbd":"markdown","c274973b":"markdown","910852b4":"markdown","d2799283":"markdown","16d107f3":"markdown","29458162":"markdown","707c77c5":"markdown","a0015ee2":"markdown","1ed9ca17":"markdown","a83dc5b1":"markdown","6099e4d5":"markdown","935ebb7c":"markdown","f957f45c":"markdown","1de19c6f":"markdown","33a2890a":"markdown","81fffa43":"markdown","b3630955":"markdown","ad5a2a4c":"markdown","85993e62":"markdown","cf3aecb2":"markdown","9655de3d":"markdown","3bc052e3":"markdown","26dee847":"markdown","8aee842e":"markdown","188b933b":"markdown","47aed89d":"markdown","41cc5bd4":"markdown","36dad107":"markdown","8b167fcc":"markdown","48ffd723":"markdown","f3f7df6e":"markdown","c715bc5e":"markdown","2feff171":"markdown","3f34ef57":"markdown","f71ef706":"markdown","11611ae2":"markdown","ebef6cba":"markdown","5b12e668":"markdown","120fd2de":"markdown","fb0dad0c":"markdown","448b3b0f":"markdown","80009507":"markdown","3ed755c3":"markdown","81a9a9ec":"markdown","576d0f15":"markdown","7393ff53":"markdown"},"source":{"1ce750bf":"# numpy and pandas for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system management\nimport os\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","039f3494":"# List of files available.\nprint(os.listdir('..\/input\/home-credit-default-risk\/'))","72fd0b65":"# Training data\napp_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('Training data shape : ', app_train.shape)\napp_train.head()","8e40f89b":"# Testing data features\napp_test = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('Testing data shape : ', app_test.shape)\napp_test.head()","868ff362":"app_train['TARGET'].value_counts()","8cefb3f5":"app_train['TARGET'].astype(int).plot.hist()","63bde0d4":"# Total missing values\nmis_val = app_train.isnull().sum()\n\n# Percentage of missing values\nmis_val_percent = 100 * mis_val \/ len(app_train)","84d93974":"len(app_train)","8e3346f1":"mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)","e3409a6a":"mis_val_table","d313944b":"# Rename the columns\nmis_val_table_ren_columns = mis_val_table.rename(\n    columns = {\n        0:'Missing Values',\n        1:'% of Total Values'\n    }\n)","20c965ba":"mis_val_table_ren_columns","4eb848a4":"# Sort the table by percentage of missing descending\nmis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n'% of Total Values', ascending=False).round(1)\n\n# Print some summary information\nprint('Your selected dataframe has ' + str(app_train.shape[1]) + ' columns.\\n''There are '+str(mis_val_table_ren_columns.shape[0]) + ' columns that have missing values'\n     )","635505c3":"mis_val_table_ren_columns","2cc30edf":"def missing_values_table(df):\n    \"\"\"\n    param : \n        df : pandas DataFrame data, processing target dataframe.\n    Return :\n        mis_val_table_ren_columns : pandas DataFrame , \n        column data extracted from df that has null data\n    \"\"\"\n    # Total missing values\n    mis_val = df.isnull().sum()\n    \n    # Percentage of missing values\n    mis_val_percent = 100 * mis_val \/ len(df)\n    \n    # Make a table with the results \/ concat\uc740 \uc9c0\uc815 \ub370\uc774\ud130\ub97c \ud569\uccd0\uc900\ub2e4.\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    \n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(columns={\n        0:'Missing Values',\n        1:'% of Total Values'\n    })\n    \n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns=mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n    \n    # Print some summary information\n    print('You selected dataframe has '+str(df.shape[1])+' columns.\\n'\n    'There are '+str(mis_val_table_ren_columns.shape[0])+' columns that have missing values.')\n    \n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns","8a958546":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","018ab517":"# Number of each type of column\napp_train.dtypes.value_counts()","dc49ee98":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)\n# pd.Series.nunique \uba54\uc18c\ub4dc\ub294 \uac01 \uce74\ud14c\uace0\ub9ac \uceec\ub7fc\uc758 \ub370\uc774\ud130\ub97c \ubd84\ub958\ud558\ub294 \ubd84\ub958\uc790 \uc218\ub97c \ubc18\ud658\ud568 \/\/ \uc544\ub798 \ucc38\uc870","6f01b96c":"# \uc2e4\uc81c \uc774\ub807\uac8c \ub370\uc774\ud130 \ud0c0\uc785\uc744 \uc120\ud0dd\ud574\uc11c \ucd9c\ub825\ud574\ubcf4\uba74, \uac01 \uceec\ub7fc\ubcc4\ub85c \uc5b4\ub5a4 \uc885\ub958\uc758 \ubd84\ub958\uc790\ub85c \ubd84\ub958 \ub418\ub294\uc9c0 \ubcfc \uc218 \uc788\uace0, \n# nunique\ub294 \uae30\ubcf8\uc801\uc73c\ub85c null\ub370\uc774\ud130\ub294 \uc81c\uc678\ud558\uace0 \ubd84\ub958\uc790 \uac2f\uc218\ub97c \ubc18\ud658\ud574\uc900\ub2e4.\napp_train.select_dtypes('object')","3a98b28b":"# Create a label encoder object.\nle = LabelEncoder()\nle_count = 0","10c0a8df":"# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            \n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both Training and Testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            print('%s is processed !' % col)\n            # Keep track of how many columns label encoded\n            le_count +=1            \n            \nprint('%d columns were label encoded' % le_count)","8f8f2a77":"app_train[['FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE']].head(10)","f699f638":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Feature shape', app_train.shape)\nprint('Testing Feature shape', app_test.shape)","1f615169":"train_labels=app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join='inner', axis=1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels","9f9caea5":"print('Training Feature shape', app_train.shape)\nprint('Testing Feature shape', app_test.shape)","9d553807":"# original DAYS_BIRTH \ub370\uc774\ud130 \uac12\napp_train[['DAYS_BIRTH']].head(10)","22123eb0":"(app_train['DAYS_BIRTH'] \/ (-365)).describe()","86d48827":"(app_train['DAYS_EMPLOYED']).describe()","c0e1a8bc":"(app_train['DAYS_EMPLOYED']).plot.hist(title='Days Employment Histogram');\nplt.xlabel('Days Employment')","925f7d05":"# DAYS_EMPLOYED \uac12\uc774 \ucd5c\ub300\uac12\uc744 \uac16\ub294 \uace0\uac1d \ub370\uc774\ud130\ub97c \ubaa8\ub450 \uac00\uc838\uc640\uc11c anom\uc5d0 \uc800\uc7a5\nanom = app_train[app_train['DAYS_EMPLOYED']==365243]\n# DAYS_EMPLOYED \uac12\uc774 \ucd5c\ub300\uac12\uc774 \uc544\ub2cc \uace0\uac1d \ub370\uc774\ud130\ub97c \ubaa8\ub450 \uac00\uc838\uc634!\nnon_anom = app_train[app_train['DAYS_EMPLOYED']!=365243]","ff9ab6bf":"print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anamalous days of employment' % len(anom))","0e44dfa0":"# Create anomalous flag column\n# \ud574\ub2f9 \ub370\uc774\ud130\uac00 \uc788\ub294 \uacf3\uc5d4 True\ub97c \ubc18\ud658\ud574\uc90c\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED']==365243\n\n# Replace the anomalous values with nan\n# numpy\uc758 nan\uc744 \uc0ac\uc6a9, \uc0ac\uc804\ud615 \ud615\ud0dc\ub85c \ub370\uc774\ud130\uac00 365243\uc778 \uacf3\uc5d0 np.nan\uc744 \ubc18\ud658\ud574\uc90c!\napp_train['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)\n\n# \uadf8\ub7fc \ub2e4\uc2dc, \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \uadf8\ub824\uc11c, \ub370\uc774\ud130\uc758 \uac00\uacf5 \uc0c1\ud0dc\ub97c \ud655\uc778\napp_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram after processing ANOM data');\nplt.xlabel('Days Employment');\n\n","761c48ce":"(app_train[['DAYS_EMPLOYED']] \/ (-365)).describe()","472304bd":"app_test['DAYS_EMPLOYED'].describe()","3b3ec0f3":"app_test['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED'] == 365243\napp_test['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)","251ae22c":"print('There are %d anomalies in the test data out of %d entries' % \n      (app_test['DAYS_EMPLOYED_ANOM'].sum(), len(app_test)))","16cff0bd":"app_test[app_test['DAYS_EMPLOYED']==365243]","f77edbd7":"# \uba3c\uc800 \uc804\uccb4\uc801\uc778 \ucf54\ub9b4\ub808\uc774\uc158 \uacc4\uc218\ub97c \uc774\ub807\uac8c \ucd9c\ub825\ud560 \uc218 \uc788\ub2e4. \ub514\ud3f4\ud2b8\ub294 \ud53c\uc5b4\uc2a8 \ubc29\ubc95\uc73c\ub85c \ucf54\ub9b4\ub808\uc774\uc158 \ud568\ncorrmat = app_train.corr()\ncorrmat","1fffac5b":"f, ax = plt.subplots(figsize=(20,15))\nsns.heatmap(corrmat, vmax=1.0, square=True)","49589b45":"# Find correlations with the target and sort\ncorrmat = corrmat['TARGET'].sort_values()\n","20e8d5e3":"# Display correlations\nprint('Most Positive Correlations : \\n', corrmat.tail(15))\nprint('\\n')\nprint('Most Negative Correlations : \\n', corrmat.head(15))","9617fa41":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","7de46001":"app_train['DAYS_BIRTH'].describe()","9fbf2455":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] \/ 365, edgecolor = 'k', bins=25) # bins \uc635\uc158\uc740 \uba87\uac1c\uc758 \ub9c9\ub300\ub85c \ud45c\ud604\ud560\uc9c0 \uc815\ud574\uc90c\nplt.title('Age of Client')\nplt.xlabel('Age (years)')\nplt.ylabel('Count')           \n           ","215fd9ae":"plt.figure(figsize=(10,8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET']==0, 'DAYS_BIRTH'] \/ 365, label='target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET']==1, 'DAYS_BIRTH'] \/ 365, label='target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)')\nplt.ylabel('Density')\nplt.title('Distribution of Ages')","5e55694e":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] \/ 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=np.linspace(20,70, num=11))\nage_data.head(10)","f07156a8":"np.linspace(20,70, num=11) # \ucc98\uc74c \uc218, \ub05d \uc218, \ucd1d \uc22b\uc790 \uc218=11","546385b7":"# Group by the bin and calculate averages\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","51c77905":"age_groups.index.astype(str)","d75548b1":"plt.figure(figsize=(8,8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'] )\n\n# Plot labeling\nplt.xticks(rotation = 75)\nplt.xlabel('Age Group (years)')\nplt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","91bf21ec":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","2ae9ece5":"plt.figure(figsize=(8,6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)\nplt.title('Correlation Heatmap');","5b0ece6e":"plt.figure(figsize=(10,12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3,1,i+1) # 3 \ud589 1\uc5f4 \ubaa8\uc591\uc73c\ub85c \uc7a1\uace0, \uac01 \uc21c\uc11c\ub300\ub85c \ud50c\ub78f \ud568\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET']==0,source], label='target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET']==1,source], label='target == 1')\n    \n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source)\n    plt.ylabel('Density');\n\nplt.tight_layout(h_pad = 2.5)\n    ","70e7862b":"# Copy the ext_data for plotting\next_data.head()","025e0158":"plot_data = ext_data.drop(columns=['DAYS_BIRTH']).copy()","1bd8f5d5":"# \ub370\uc774\ud130 \uceec\ub7fc \ud655\uc778!\nplot_data.head()","c7b3cbf1":"# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]","5fe49d55":"plot_data.shape\n# \uc804\uccb4 \ub370\uc774\ud130 \uc218 \uac00 3\ubd84\uc758 1\ub85c \uc904\uc5c8\uc74c.","9ae06797":"# Function to calculate correlation coeffiecient between two columns\ndef corr_func(x,y, **kwargs):\n    r = np.corrcoef(x,y)[0][1]\n    ax = plt.gca()\n    ax.annotate('r = {:.2f}'.format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20\n               )","3b040500":"# Create the pairgrid object\n# \ud50c\ub78f\ud560 \ub300\uc0c1\uc744 \uc774 \uc624\ube0c\uc81d\ud2b8\uc5d0\uc11c \uc815\uc758\ud558\uace0, \ud398\uc5b4 \ud50c\ub78f\uc5d0\uc11c \ub300\uac01\uc120 , \ub300\uac01\uc120 \uc704\/\uc544\ub798 \ubc29\ud5a5\uc73c\ub85c \uc6d0\ud558\ub294 \ud50c\ub78f\uc744 \ub123\uc5b4\uc90c\ngrid = sns.PairGrid(\n    data = plot_data,\n    size = 3,\n    diag_sharey=False,\n    hue='TARGET',\n    vars = [x for x in list(plot_data.columns) if x != 'TARGET']\n)\n# \uc5ec\uae30 \uae4c\uc9c0 \ud558\uba74 \uadf8\ub798\ud504\ub97c \uadf8\ub9b4 \uc218 \uc788\ub294 \uc790\ub9ac\ub9cc \ub098\uc634!\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05)","5376900b":"# Make a new dataframe for polynomial features\npoly_features = app_train[\n    ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','TARGET']]\npoly_features_test = app_test[\n    ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy='median')\n\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns=['TARGET'])\n\npoly_features.head()","5af9eafc":"# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)","029df994":"poly_features","df5fe6f2":"# Create the polynomial object with specified degree\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_transformer = PolynomialFeatures(degree=3)","9d4bc448":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape : ', poly_features.shape)","ceffdcb0":"# 35\uac1c \ubcc0\uc218 \ubaa8\ub450 \ucd9c\ub825\ud568\npoly_transformer.get_feature_names(input_features=[\n    'EXT_SOURCE_1',\n    'EXT_SOURCE_2',\n    'EXT_SOURCE_3',\n    'DAYS_BIRTH'\n])","7c573ab6":"# Create a dataframe of the features\npoly_features = pd.DataFrame(poly_features, \n                             columns=poly_transformer.get_feature_names(input_features=[\n                                 'EXT_SOURCE_1',\n                                 'EXT_SOURCE_2',\n                                 'EXT_SOURCE_3',\n                                 'DAYS_BIRTH',\n                             ]))\n\n# Add in the target\n# \ud0c0\uac9f\uac12\uc740 \uc6d0\ub798 \uc774 \ub370\uc774\ud130\ud504\ub808\uc784\uc5d0 \uc788\uc5c8\ub294\ub370, polynomial feature \ub9cc\ub4dc\ub290\ub77c\uace0, \ub4dc\ub78d\uc2dc\ud0a4\uace0, \ub530\ub85c \uc0c8\ub85c\uc6b4 \ubcc0\uc218\uc5d0 \uc800\uc7a5\uc911\uc774\uc600\ub2e4.\n# \uadf8 \uac12\uc744 \ub2e4\uc2dc \uadf8\ub300\ub85c \uac00\uc838\uc628\ub2e4.\npoly_features['TARGET'] = poly_target","84806695":"poly_features.head()","50568e53":"# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","3c0223ce":"# Put test features into dataframe\nprint('Polynomial Features shape for test data : ', poly_features_test.shape)","d6a53df6":"poly_features_test = pd.DataFrame(poly_features_test, \n                                  columns=poly_transformer.get_feature_names(input_features=[\n                                      'EXT_SOURCE_1',\n                                      'EXT_SOURCE_2',\n                                      'EXT_SOURCE_3',\n                                      'DAYS_BIRTH']))","faaf2c9b":"poly_features_test.head()","e34d0153":"# Merge polynomial featueres into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on='SK_ID_CURR', how='left')\n\n# Merge polynomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on='SK_ID_CURR', how='left')","2219fc7f":"# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(\n    app_test_poly,\n    join='inner',\n    axis=1\n)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape : ', app_train_poly.shape)\nprint('Testing data with polynomial features shape : ', app_test_poly.shape)","f81dd583":"for item in app_train_poly.columns:\n    print(item)","99b2e1f0":"# \uc0c8\ub85c\uc6b4 \uceec\ub7fc\uc744 \ub9cc\ub4dc\ub294 \uacfc\uc815, \uae30\uc874\uc5d0 \uc788\uc5c8\ub358 \uceec\ub7fc\ub4e4\uc744 \uc870\ud569\ud574\uc11c \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uac00\uc9c0\ub294 \ud140\ub4e4\uc744 \uc0dd\uc131\n# annuity : \uc5f0\uae08\napp_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']","56e0bdf1":"app_train_domain.head()","250fc479":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","b5057168":"app_test_domain.head()","5d495840":"plt.figure(figsize=(12,20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT','ANNUITY_INCOME_PERCENT','CREDIT_TERM','DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4,1,i+1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label='target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label='target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n\nplt.tight_layout(h_pad = 2.5)\n    ","0e8dd3ee":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns=['TARGET'])\nelse:\n    train = app_train.copy()\n\n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()","ac1d997a":"# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range=(0,1))\n\n# Fit on the training data\n# fit \uacfc\uc815\uc740 Imputer\uc758 strategy\ub97c `median`\uc73c\ub85c \ud588\uae30 \ub54c\ubb38\uc5d0, \n# \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\ub97c \uae30\uc900\uc73c\ub85c \uc911\uac04\uac12? \uc744 \ucc3e\uc544\uc11c \uac1d\uccb4\uc5d0 \uac00\uc9c0\uace0 \uc788\uc744 \uac83\uc73c\ub85c \uc720\ucd94\ud560\uc218? \uc788\ub2e4.\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n# \uc5ec\uae30\uc11c \uc660\uc9c0\ubaa8\ub974\uaca0\uc9c0\ub9cc \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub294 \uc6d0\ub798 \ub370\uc774\ud130\ub97c \uadf8\ub300\ub85c \uc0ac\uc6a9? \n# \uc704\uc5d0 \uba3c\uc800 \ub9cc\ub4e0 test \ub77c\ub294 \uac12\ub3c4 \uc5b4\ucc28\ud53c \uce74\ud53c \ud55c \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uac12\uc740 \uac19\uaca0\uc9c0\ub9cc, \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \ud558\ub294\ub300\ub85c \uc77c\ub2e8 \ud55c\ub2e4.\n# \uc774\ub807\uac8c missing value\ub97c \ucc44\uc6cc\uc900\ub2e4.\n\n# Repeat with the scaler(0~1)\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape : ', train.shape)\nprint('Testing data shape : ', test.shape)\n\n","3b5d83f0":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C=0.0001)\n\n# Train on the training data\n# train_labels\uac12\uc740 app_train['TARGET'] \uc744 \ud560\ub2f9\ud55c \ubcc0\uc218\uc784\nlog_reg.fit(train, train_labels)\n# (X, y) \ud615\ud0dc\ub85c \ub370\uc774\ud130\ub97c \ud54f\ud305\ud558\uace0, \ud574\ub2f9 \ubaa8\ub378\uc774 \uc774\uc9c4 \ubd84\ub958 \ubb38\uc81c\ub85c \uc801\uc6a9\ub418\uae30 \ub54c\ubb38\uc5d0, \uc608\uce21\ub418\ub294 \uacb0\uacfc\ub3c4 \ub450\uac00\uc9c0 \uc885\ub958\uc5d0 \ub300\ud574\uc11c \ud655\ub960 \uac12\uc73c\ub85c \ubc18\ud658\ub41c\ub2e4.","00a6fef5":"# Make predictions\n# Make sure to select the second column only\n# input \uc740 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130, \ubcf4\uace0 \uc2f6\uc740 \ub370\uc774\ud130\ub294 \ubaa8\ub4e0 \ud589\uc5d0 \ub300\ud558\uc5ec \ub450\ubc88\uc9f8 \uceec\ub7fc\uc774\ub2e4.\nlog_reg_pred = log_reg.predict_proba(test)[:,1]","f705760f":"log_reg_pred","d0f2f4b9":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET']=log_reg_pred\n\nsubmit.head()","e3c5d39f":"# Save the submission to a csv file.\nsubmit.to_csv('log_reg_baseline.csv', index=False)","9f497192":"# \ub300\ucd9c\uc0c1\ud658 \uac00\ub2a5\/\ubd88\uac00\ub2a5\uc5d0 \ub300\ud55c \ubd84\ub958 \ubb38\uc81c \uc774\ubbc0\ub85c, classifier\ub97c \uc120\ud0dd!\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Make random forest classifier\nrandom_forest = RandomForestClassifier(\n    n_estimators=100,\n    random_state=50,\n    verbose=1,\n    n_jobs=-1\n)\n# \uc77c\ub2e8 \uc5d0\uc2a4\ud2f0\uba54\uc774\ud130? \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8 \uc54c\uace0\ub9ac\uc998\uc744 \uac00\uc9c0\ub294 \uac1d\uccb4\ub97c \uc0dd\uc131\ud55c\uac70\uc784","58189ed9":"# Train on the training data\nrandom_forest.fit(train,train_labels)","409ec831":"# Extract feature importances\nfeature_importance_value = random_forest.feature_importances_ # np.ndarray \ubc18\ud658\ud568\nfeature_importances = pd.DataFrame({'feature':features,'importance':feature_importance_value})","4d0df6b6":"# Make predictions on test data\npredictions = random_forest.predict_proba(test)[:,1]","5d1b6514":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\nsubmit.head()","33cedc26":"# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index=False)","7f6411c4":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = Imputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range=(0,1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)","c09e89cf":"# Train on the training data\n# \ud54f\ud305\ud560\ub54c \ud6c8\ub828\ub370\uc774\ud130\uc758 \ud0c0\uac9f\uac12, \uc989 \uc0c1\ud658 \uc5ec\ubd80\uc5d0 \ub300\ud55c \ub808\uc774\ube14 \uac12\uc740 \uc5b4\ub5a4 \ud53c\uccd0\ub97c \uc0ac\uc6a9\ud558\ub4e0, \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud558\ub4e0 \uac19\uc73c\ubbc0\ub85c,\n# \uc5ec\uae30\uc11c\ub3c4 train_labels\uc744 \uc0ac\uc6a9\ud568.\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:,1]","341dd410":"predictions","2a840e34":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission datafrmae\nsubmit.to_csv('random_forest_baseline_engineered.csv', index=False)\n","edc63ca7":"app_train_domain.head()","ba1371d6":"# \uc785\ub825\ub370\uc774\ud130\uc5d0\uc11c \ud0c0\uac9f \uceec\ub7fc\uc744 \ubd84\ub9ac\ud55c\ub2e4.\napp_train_domain = app_train_domain.drop(columns='TARGET')\n\n# \ub3c4\uba54\uc778 \ud53c\uccd0 \uc774\ub984 \uc0dd\uc131, \ub9ac\uc2a4\ud2b8\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domain nomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range=(0,1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\n# Create Random Forest object for domain features data\nrandom_forest_domain = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances, \ud6c4 \uc791\uc5c5\uc744 \uc704\ud55c \uc0ac\uc804 \uc791\uc5c5\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importance_domain = pd.DataFrame({'feature':domain_features_names,'importance':feature_importance_values_domain})\n\n# Make prediction on the test data\npredictons = random_forest_domain.predict_proba(domain_features_test)[:,1]\n\n\n\n\n","0214060a":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index=False)","b2518027":"#\uc601\ud5a5\uacc4\uc218? \ub97c \ud50c\ub78f\ud558\ub294 \ud568\uc218\ub97c \uc791\uc131!\ndef plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of \n    feature importance provided that higher importance is better.\n    \n    Args:\n        df(dataframe): feature importances. Must have the features in a column\n        called 'features' and the importances in a column called 'importance'\n        \n    Reutrns:\n        shows a plot of the 15 most importance features\n        \n        df(dataframe): feature importances sorted by importance(hightest to lowest)\n        with a column for normalized importance\n    \"\"\"\n    \n    # Sort features according to importance\n    # reset_index()\ub97c \ud558\uba74, sorting \ud6c4 \ub0b4\ub9bc\ucc28\uc21c\uc774 \ub41c \ub370\uc774\ud130\uc758 row\uc5d0 \ub300\ud574\uc11c \ub2e4\uc2dc 0\ubd80\ud130 \uc778\ub371\uc2a4\ub97c \ub9e4\uaca8\uc900\ub2e4!\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    # \uc804\uccb4 \uc601\ud5a5\uacc4\uc218? \ub97c \ub354\ud558\uba74 1\uc774 \ub418\ubbc0\ub85c, \uadf8 \uac12\uc73c\ub85c \uac01\uac01\uc758 \uc601\ud5a5 \uacc4\uc218\ub97c \ub098\ub220\uc11c \uc77c\ubc18\ud654 \ud574\uc90c.(0,1)\uc0ac\uc774\uc758 \uac12\uc744 \uac16\ub294\ub2e4.\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    \n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize=(10,6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    # \uc5ec\uae30\uc11c \uc778\ub371\uc2a4\ub97c \ub9ac\ubc84\uc2a4 \ud558\uba74\uc11c \uc55e\uc5d0 list\ub97c \uacb9\ucc98\uc11c \ubd99\uc5ec\uc8fc\ub294 \uc774\uc720\ub294, \n    # \uc0c1\uc18d\ub418\ub294 \ub300\uc0c1\uc774 \uac1d\uccb4(object) \ud0c0\uc785\uc758? \ub370\uc774\ud130\ub77c\uc11c, \ub9ac\uc2a4\ud2b8 \ud654 \uc2dc\ucf1c\uc8fc\uae30 \uc704\ud568\uc774\ub2e4.\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align='center', \n            edgecolor='k')\n    \n    # Set the yticks and labels, y\ucd95 \uac12 \ubc94\uc704 \uc815\ud574\uc8fc\uace0, y\ucd95 \ub370\uc774\ud130 \ub77c\ubca8 \uc774\ub984 \ud560\ub2f9\ud574\uc8fc\uae30\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance')\n    plt.title('Feature Importances')\n    plt.show()\n    \n    return df","ae6b13a2":"# Show the feature importances for the dafault features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","87cadd84":"feature_importances_domain_sorted = plot_feature_importances(feature_importance_domain)","1d99bf66":"# \ud544\uc694\ud55c \ubaa8\ub4c8 \ubd88\ub7ec\uc624\uae30\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc","4fcc243b":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \"\"\"\n    Train and Test a light gradient boosting model using cross validation.\n    \n    Parameters:\n    -----------\n        features(pd.DataFrame):\n            dataframe of training feature to use\n            for training a model. Must include Target column.\n        test_features(pd.DataFrame):\n            dataframe of testing feature to use\n            for making predictions with the model.\n        encoding(str, default = 'ohe'):\n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for interger label encoding\n        n_folds(int, default = 5): number of folds to use for cross validation\n        \n    Return:\n    -------\n        submission(pd.DataFrame):\n            dataframe with 'SK_ID_CURR' and 'TARGET' probabilities\n            predicted by the model.\n        feature_importances(pd.DataFrame):\n            dataframe with the feature importances from the model.\n        valid_metrics(pd.DataFrame):\n            dataframe with training and validation metrics(ROC AUC) for each fold and overall.\n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics\n    ","7c87adbb":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","66f431df":"fi_sorted = plot_feature_importances(fi)","bb74006d":"submission.to_csv('baseline_lgb.csv', index=False)","a087ec83":"app_train_domain['TARGET'] = train_labels\n\n# Test the domain knowledge features\nsumission_domain, fi_domain, metrics_domain = model(\n    app_train_domain,\n    app_test_domain,\n)\n\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)","f49ad1ea":"fi_sorted = plot_feature_importances(fi_domain)","b6ed82ee":"sumission_domain.to_csv('baseline_lgb_domain_features.csv', index=False)","075c85f5":"\uadf8\ub798\ub3c4 \uc815\uc0c1 \uc0c1\ud658 \ud558\ub294 \uacbd\uc6b0(0\uc774 \ub098\uc624\ub294 \uc22b\uc790)\uac00 \ud6e8\uc52c \ub9ce\ub124.","b9c1414c":">Kaggle competitions are won by feature engineering: those win are those who can create the most useful features out of the data. (This is true for the most part as the winning models, at least for structured data, all tend to be variants on gradient boosting). This represents one of the patterns in machine learning: feature engineering has a greater return on investment than model building and hyperparameter tuning. This is a great article on the subject). As Andrew Ng is fond of saying: \"applied machine learning is basically feature engineering.\"\n\n\ubaa8\ub378 \ube4c\ub529\ud558\uace0, \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\ud558\ub294 \uac83\ubcf4\ub2e4 \uacb0\uad6d\uc740 feature engineering\uc744 \uc5b4\ub5bb\uac8c \ud558\ub294\uc9c0\uac00 \ub354 \uc911\uc694\ud558\uace0, \ucef4\ud398\ud2f0\uc158 \uacb0\uacfc\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce5c\ub2e4!\n\n>While choosing the right model and optimal settings are important, the model can only learn from the data it is given. Making sure this data is as relevant to the task as possible is the job of the data scientist (and maybe some automated tools to help us out).\n\n\ubc18\uba74\uc5d0 \uc88b\uc740\ubaa8\ub378\uc744 \uc798 \uc120\ud0dd\ud558\uace0 \uc14b\ud305\uc744 \ucd5c\uc801\ud654 \ud558\ub294\uac83\ub3c4 \uc911\uc694\ud558\ub2e4. \ubaa8\ub378\uc740 \uc8fc\uc5b4\uc9c0\ub294 \ub370\uc774\ud130\ub9cc \uac00\uc9c0\uace0 \ud559\uc2b5\uc744 \ud560 \uc218 \uc788\uae30 \ub54c\ubb38.\n\n>Feature engineering refers to a geneal process and can involve both feature construction: adding new features from the existing data, and feature selection: choosing only the most important features or other methods of dimensionality reduction. There are many techniques we can use to both create features and select features.\n\nfeature engineering\uc740 \uc77c\ubc18\uc801\uc778 \uacfc\uc815\uc774\uba70, `feature construction`, `feature selection` \ub450\uac00\uc9c0 \uacfc\uc815\uc744 \ubaa8\ub450 \ud3ec\ud568\ud55c\ub2e4.\n\n* feature construction: adding new features from the existing data\n* feature selection: choosing only the most important features or other methods of dimensionality reduction.\n\n\n\ub9ce\uc740 feature engineering \ubc29\ubc95\uc774 \uc788\uc9c0\ub9cc \uc774 \ucee4\ub110\uc5d0\uc11c\ub294 \ub2e4\uc74c\uc758 \uac04\ub2e8\ud55c feature construction \uacfc\uc815\ub9cc \ud574\ubcf8\ub2e4.\n* Polynomial features\n* Domain knowledge features\n\n\n\n\n\n\n\n","1931cb43":"### Examine Missing value\n\n\uc774\uc81c \uac01 \uceec\ub7fc\uc758 missing value\uc758 \uc218\uc640 \uadf8 \ud37c\uc13c\ud2f0\uc9c0\ub97c \ud655\uc778\ud574\ubcf4\uc790.","050bfe88":"`DAYS_EMPLOYED_ANOM`\uc758 TRUE\uac12\uc744 \ub2e4 \ub354\ud558\uac8c \ub418\uba74 9274\uac1c\uac00 \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","254adfa9":"### Model Interpretation: Feature Importances","83ba30f6":"\ub370\uc774\ud130 \uc815\uaddc\ud654\ub97c \ud558\uac8c \ub418\uba74 \ubaa8\ub378 \uc608\uce21\uc2dc \uc624\ubc84\ud53c\ud305\ub418\ub294 \uac83\uc744 \uac10\uc18c\uc2dc\ud0ac\uc218 \uc788\uace0, \ud604\uc7ac \uc774 \uc791\uc5c5\uc5d0\uc11c \ub370\uc774\ud130 \uc14b\uc758 \ucc28\uc774\uc810\uc740 \uc815\uaddc\ud654 \ubcc0\uc218 C\uac12\uc774 \uc791\uc544\uc9c4 \uac83\uc774\ub77c\uace0 \ud558\ub294\ub370, \uc798 \ubaa8\ub974\uaca0\uc74c.\n\n\uc77c\ub2e8 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\uc11c fit\ud574\ubcf4\uace0, \uc608\uce21 \ud574\ubcf8\ub2e4.","b135e6c2":">Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n\n* .00-.19 \u201cvery weak\u201d\n* .20-.39 \u201cweak\u201d\n* .40-.59 \u201cmoderate\u201d\n* .60-.79 \u201cstrong\u201d\n* .80-1.0 \u201cvery strong\u201d","5086d8ef":"2. One-hot encoding : \n\n\ud55c id \ubcc4\ub85c \ubd84\ub958\uc7a5\uc758 \uceec\ub7fc\uc744 \ubaa8\ub450 \uc0dd\uc131\ud574\uc11c, \ud574\ub2f9\ub418\ub294 \uceec\ub7fc\uc5d0 1\uc744 \ud560\ub2f9\ud558\uace0,\n\ub098\uba38\uc9c0 \uceec\ub7fc\uc5d0\ub294 0\uc744 \ud560\ub2f9\ud558\ub294 \ubc29\uc2dd, \uc774\ub807\uac8c \ud558\uba74 \uceec\ub7fc\uc740 \ub9ce\uc544 \uc9c0\uae30 \ub54c\ubb38\uc5d0 \uc5c4\uccad \ub370\uc774\ud130\uc591\uc774 \ucee4\uc9c0\ub294 \uac83 \uac19\uc74c.\n\uadfc\ub370 \uad6c\ubd84\ud558\uae30\uc5d0\ub294 \uc88b\uc740\uac70 \uac19\uc74c\n![image.png](attachment:image.png)","943dc38f":"\uc608\uc0c1\ud55c \ub300\ub85c, `EXT_SOURCE`\uc640 `DAYS_BIRTH`\uac00 \uac00\uc7a5 \uc911\uc694\ud55c feature \uc784\uc744 \uadf8\ub798\ud504\ub97c \ud1b5\ud574\uc11c \uc54c\uc218\uc788\ub2e4.\n\n\ubaa8\ub378\uc744 \ud45c\ud604\ud558\uac70\ub098 \ucc28\uc6d0\uc744 \uc904\uc774\ub294 \ubc29\ubc95\uc73c\ub85c feature importance\uac00 \uac00\uc7a5 \uc815\uad50\ud55c \ubc29\ubc95\uc740 \uc544\ub2c8\uc9c0\ub9cc, \uc608\uce21\ud560 \ub54c \uc5b4\ub5a4 factor\ub4e4\uc774 \uc911\uc694\ud560\uc9c0\ub97c \uc54c\uac8c \ud574\uc8fc\ub294 \uc911\uc694\ud55c \uc694\uc18c\uc774\ub2e4.","0d2caab7":">Here I will focus on implementing the model rather than explaining the details, but for those who want to learn more about the theory of machine learning algorithms, I recommend both [An Introduction to Statistical Learning and Hands-On Machine Learning with Scikit-Learn and TensorFlow. Both of these books present the theory and also the code needed to make the models (in R and Python respectively). They both teach with the mindset that the best way to learn is by doing, and they are very effective!\nTo get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps.\n\n\uc774\uc81c \ubaa8\ub378 \uad6c\uc131\uc5d0 \ub300\ud574\uc11c \uad6c\uccb4\uc801\uc73c\ub85c \ubc30\uc6cc \ubcfc \uac70\uace0,\n\n\ub370\uc774\ud130\ub97c \uc804\ucc98\ub9ac\ud558\ub294 \uacfc\uc815\uc5d0 \ub300\ud574\uc11c\ub3c4 \ud574\ubcfc\uac70\ub2e4. \uc989, missing value\ub97c \ucc44\uc6b0\uac70\ub098, \ub370\uc774\ud130\ub97c \uc815\uaddc\ud654 \ud558\ub294 \uacfc\uc815\uc774 \uadf8\uc5d0 \ud3ec\ud568\ub41c\ub2e4.","bfb0b63c":">There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.\nThis is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time.\n\n","550d5a1f":"\uc77c\ub2e8 \uc774\ub807\uac8c \ucee4\ubc0b\ud558\uba74, \uc81c\ucd9c\uc774 \ub41c\ub2e4.\n\n\ucc38\uc870\ud55c \ucee4\ub110\uc5d0\uc11c\ub294 0.671 \uc810\uc218\ub97c \ubc1b\uc558\ub2e4\uace0 \ud55c\ub2e4. \uc774\uc81c \uc9c1\uc811 commit \ud574\uc11c \uba87\uc810\uc774 \ub098\uc624\ub294\uc9c0 \ud655\uc778!\n\n* \uc2e4\uc81c\ub85c public \uc810\uc218\ub85c 0.67041\uc774 \ub098\uc628\ub2e4.\n\n\ubaa8\ub378\uc744 \uac1c\uc120\ud574\ubcf4\uc790.\n","b09bc1b2":"## Baseline","05cb595e":"EXT_SOURCE_3\uc740 \ud0c0\uac9f\uc5d0 \ub530\ub978 \ubd84\ud3ec\uac00 \ud604\uc800\ud558\uac8c \ub2e4\ub978 \uac83\uc744 \ubcfc \uc218 \uc788\uc73c\uba70, \uc774\ub7f0 \ud56d\ubaa9\ub4e4\uc740 \ub300\ucd9c\uc0c1\ud658 \uac00\ub2a5\uc131\uacfc\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\uace0 \ud655\uc2e4\ud788 \ud310\ub2e8\ud560 \uc218 \uc788\ub2e4.\n\n\uadfc\ub370, \uc0ac\uc2e4 \uc774\uc815\ub3c4 \uc218\uce58\ub294 \uc0c1\uad00\uad00\uacc4 \uc218\uce58\uac00 \uc57d\ud55c \uac70\ub77c\uace0 \ud55c\ub2e4.\n\uadf8\ub798\ub3c4 \uba38\uc2e0\ub7ec\ub2dd\uc5d0 \uc0ac\uc6a9\ud558\ub294\ub370 \uc788\uc5b4\uc11c\ub294 \uc720\uc6a9\ud55c \uc815\ubcf4\uc774\uae30 \ub54c\ubb38\uc5d0 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc744 \uac83 \uac19\ub2e4.","f86a2aca":">As a final exploratory plot, we can make a pairs plot of the EXT_SOURCE variables and the DAYS_BIRTH variable. The Pairs Plot is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\nIf you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)!","3ef9034c":"### Encoding Categorical Variables","c4afda5c":"\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\ub3c4 \ub611\uac19\uc740 \uc791\uc5c5\uc744 \ud574\uc918\uc57c\ud55c\ub2e4. \uc989, \ucd5c\ub300\uac12\uc744 \uba3c\uc800 \uac19\uc740\uc9c0 \ud655\uc778\ud574\ubcf4\uace0, \uc774\ub840\uc801\uc778 \ub370\uc774\ud130\ub294 \ubd84\ub958\ud574\uc11c nan\uac12\uc73c\ub85c \ucc44\uc6cc\uc900\ub2e4.","13995ec9":"### Label Encoding and One-Hot Encoding","a32b50a3":"### Polynomial Features","c1c20638":"\ud0c0\uac9f\uacfc \uc74c\uc758 \uad00\uacc4\ub97c \uac16\ub294 \ud56d\ubaa9\uc911\uc5d0 \uac00\uc7a5 \ub192\uc740 \uc218\uce58\ub97c \ubcf4\uc774\ub294 \ud56d\ubaa9\uc5d0 \ub300\ud574\uc11c \uc0b4\ud3b4\ubcf8\ub2e4.\n\nEXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 \uc694\ub807\uac8c \uc138\uac20\ub370 \ub3c4\ud050\uba3c\ud2b8 \uc124\uba85\uc5d0 \uc758\ud558\uba74,\n\n\uc774 \ud56d\ubaa9\ub4e4\uc740 \uc678\ubd80\uc694\uc778\uc5d0 \uc758\ud574 \uc77c\ubc18\ud654\ub41c \ub370\uc774\ud130 \uc18c\uc2a4\ub77c\uace0 \ud568. \ubb54\uc9c0\ub294 \uc798 \ubaa8\ub974\uaca0\uc9c0\ub9cc \ub204\uc801\ub41c \uc2e0\uc6a9\uc815\ubcf4? \uc815\ub3c4\ub85c \uc774\ud574\ud558\uba74 \ub420 \uac83 \uac19\ub2e4.\n\n\uba3c\uc800, \uc774\ub7f0 \ud56d\ubaa9\ub4e4\uacfc \ud0c0\uac9f\uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ucc3e\uc544\ubcf4\uc790!","4d6fac7b":"\ucd5c\uace0 3\uc2b9\uae4c\uc9c0 \uc81c\uacf1\ud558\uc5ec \ubcc0\uc218\ub97c \ub9cc\ub4e4\uace0, \uc11c\ub85c \uacf1\ud558\ub294 \ud56d\uae4c\uc9c0 \ubaa8\ub450 \ub9cc\ub4e4\uc5b4 \ub0c8\ub2e4.\n\n\uc774\uc81c, \uc774\ub7f0 \ubcc0\uc218\ub4e4\uc774 \ud0c0\uac9f \ubcc0\uc218\uc640 \uc5b4\ub5a4 \uad00\uacc4\uac00 \uc788\ub294\uc9c0 \ud655\uc778\ud55c\ub2e4.","dfb0e092":"align \uacfc\uc815\uc5d0\uc11c join\uc744 `inner`\ub85c \ud574\uc11c, \ub370\uc774\ud130 \ucc28\uc6d0\uc774 \uc791\uc740 \ubc29\ud5a5\uc73c\ub85c \uc5bc\ub77c\uc778 \ub418\uace0,  app_train_poly\uc758 TARGET \uceec\ub7fc\uc774 \uc0ac\ub77c\uc9c0\uc9c0 \uc54a\uc558\uc744\uae4c \ud574\uc11c \uc544\ub798\uc640 \uac19\uc774 \ud655\uc778! , \uba38\uc9c0 \ud558\uba74\uc11c \uc774\ub984\uc774 \ubc14\ub010\uac74\uc9c0 \uc548\ubcf4\uc778\ub2e4. polynomial \ubcc0\uc218\ub4e4\ub3c4 1\ucc28\uc6d0 \ubcc0\uc218\ub4e4\uc758 \uc774\ub984\uc774 \ubc14\ub010\uac74\uc9c0 \uc548\ubcf4\uc784","bd1f5129":"\ud544\uc790\ub294, \uc9c4\uc9dc \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc774\ub77c\uace0 \ud55c\ub2e4.\n\nLightGBM \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 the gradient boosting machine\uc744 \uc774\uc6a9\ud55c\ub2e4.\n\nGBM\ubaa8\ub378\uc740 \uad6c\uc870\ud654 \ub41c \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc798\ub098\uac00\ub294 \ubaa8\ub378\uc774\ub780\uace0 \ud55c\ub2e4. \ud2b9\ud788 \uce90\uae00\uc5d0\uc11c. \ub610 \uc774 \ubaa8\ub378\uc740 \uc801\uc6a9\ud558\uae30 \uc704\ud55c \ud2b9\ubcc4\ud55c \ud3ec\ub9f7\uc774 \ud544\uc694\ud558\ub2e4.\n\n\uc77c\ub2e8, \uc804\uccb4\uc801\uc778 \ubaa8\ub378\uc744 \uad6c\ucd95\ud558\uae30 \uc704\ud55c \ucf54\ub4dc\ub97c \uc801\uc5b4\ub193\ub294 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uac81\uba39\uc744 \ud544\uc694\ub294 \uc5c6\ub2e8\ub2e4. \uc2dc\uc791\ud574\ubcf4\uc790.","56007ebb":"0.733 \uae4c\uc9c0 \ub098\uc654\ub2e4. \ubb54\uc9c0 \ubaa8\ub974\uaca0\uc9c0\ub9cc \uc774\uac8c \ud6a8\uacfc\uac00 \uc788\ub2e4. \ub098\uc911\uc5d0 \uc790\uc138\ud55c \ubd80\ubd84\uc740 \ucc28\ucc28 \uacf5\ubd80 \ud558\ub3c4\ub85d..","69f0e114":"### Visualize New Variables\n\n\uc2dc\uac01\ud654 \ud574\uc11c \uc704\uc5d0\uc11c \ub9cc\ub4e0 domain knowledge\ubcc0\uc218\ub4e4\uc744 \ud50c\ub78f\ud574\ubcf8\ub2e4.\n\n\ubaa8\ub4e0 \ubcc0\uc218\uc5d0 \ub300\ud574\uc11c, TARGET\uac12\uc744 [KDE(kernel density estimation plot) plot](https:\/\/en.wikipedia.org\/wiki\/Kernel_density_estimation)\uc73c\ub85c \ud45c\ud604\ud560 \uac70\ub2e4.","4137309d":"\ub370\uc774\ud130\ub97c \uac00\uacf5\ud558\ub2c8 \ubd84\ud3ec\uac00 \ub9e4\ub044\ub7fd\uac8c \uc790\uc5f0\uc2a4\ub7ec\uc6cc \uc84c\ub2e4.","7c2ec84f":"> One problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method. The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:\n\n\n\ub370\uc774\ud130 \uac12\ub4e4 \uc911\uc5d0 \uc774\ub840\uc801\uc73c\ub85c, \ubb38\uc81c\uac00 \uc788\ub294 \uac12\ub4e4\uc744 \ucc3e\uc544\ub0b4\uc57c \ud55c\ub2e4. \ud0c0\uc785\uc774 \uc798\ubabb\ub418\uac70\ub098, \uce21\uc815\uc774 \uc798\ubabb\ub420\uc218\ub3c4 \uc788\uace0 \uc5ec\ub7ec\uac00\uc9c0 \uc798\ubabb\ub41c \uacbd\uc6b0\uac00 \uc788\uc744 \uc218 \uc788\ub2e4.\n\n\uadf8\ub798\uc11c, \uac01 \uceec\ub7fc\uc5d0 \ub300\ud574\uc11c \uc815\ub7c9\uc801\uc73c\ub85c \ud655\uc778 \ud574\ubcf4\ub294 \uac83 \uc774 \uc88b\uc740\ub370, `describe` \uba54\uc18c\ub4dc\ub97c \uc774\uc6a9\ud574\uc11c \uc815\ub7c9\uc801\uc73c\ub85c \ub370\uc774\ud130\uc758 \ud1b5\uacc4 \ub370\uc774\ud130\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub2e4.\n\n`DAYS_BIRTH` \uceec\ub7fc\uc740 \ud604\uc7ac \ub860 \ub300\uc0c1\uc790\uc758 \ub098\uc774\ub97c \uc0c1\ub300\uc801\uc73c\ub85c \ub098\ud0c0\ub0b4\uae30 \ub54c\ubb38\uc5d0 \uc74c\uc218\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\ub2e4. \uadf8\ub7ec\ubbc0\ub85c -1\uc744 \uacf1\ud574\uc11c \uc591\uc218\ud654 \uc2dc\ud0a4\uace0 \ub098\uc774 \uac12\uc73c\ub85c \ubcc0\ud658\ud574\ubcf4\uc790.\n\n","749746dc":"### Make Predictions using Engineered Features\n\n>The only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features! We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering.\n\nPolynomial feature, Domain Knowledge feature\ub97c \uc9c1\uc811 \ubaa8\ub378\uc5d0 \ud53c\ub529\ud574\uc11c \uadf8 \ud6a8\uacfc\uac00 \uc788\ub294\uc9c0 \uc2dc\ud5d8\ud574\ubcf4\uc790.\n\n\uacb0\uacfc\ub294, submission \uc810\uc218\ub97c \ubcf4\uace0 \uacfc\uc5f0 \uc774 feature\ub4e4\uc774 \ud6a8\uacfc\uac00 \uc788\ub294\uc9c0 \ud310\ub2e8\ud560 \uc218 \uc788\uc744 \uac83\uc774\ub2e4.","b1c78bd6":"\uce74\ud14c\uace0\ub9ac \ubcc0\uc218\ub4e4\uc740 \ubb38\uc790\uc5f4 \ub4f1\uc73c\ub85c \uc774\ub8e8\uc5b4 \uc838 \uc788\uae30 \ub54c\ubb38\uc5d0, \ubaa8\ub378\uc5d0 \ubc14\ub85c \uc801\uc6a9 \ud560 \uc218 \uc5c6\ub2e4.\n\n\uadf8\ub798\uc11c, \uc774\ub7f0 \ubcc0\uc218\ub4e4\uc744 \uc218\uce58\uc801\uc73c\ub85c encoding \ud574\uc918\uc57c \ud558\ub294\ub370 \ub2e4\uc74c \ub450\uac00\uc9c0 \ubc29\ubc95\uc744 \ud65c\uc6a9 \ud560 \uc218 \uc788\ub2e4.\n","1f88b7ac":"\uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294\ub370 \uc788\uc5b4\uc11c, \uc774\ub7f0 \ub20c \ub370\uc774\ud130\ub97c \ucc44\uc6cc\uc57c \ud55c\ub2e4.(imputation?)\n\ub098\uc911\uc5d0 \uc6b0\ub9ac\ub294 XGBoost \uac19\uc740 \uac83\uc744 \uc0ac\uc6a9\ud574\uc11c \ub20c \ub370\uc774\ud130\uc5d0 \ub300\ud55c imputation \ud560 \ud544\uc694\uac00 \uc5c6\ub2e4?\n\ub610 \ub2e4\ub978 \ud558\ub098\uc758 \uc635\uc158\uc73c\ub85c\ub294 \ub20c \ub370\uc774\ud130\uac00 \ub9ce\uc740 \uceec\ub7fc\uc744 \uc544\uc608 \uc0ad\uc81c(\ub4dc\ub78d) \ud574\ubc84\ub9ac\ub294 \ubc29\ubc95\uc778\ub370, \uc774\ub7f0 \uceec\ub7fc\uc774 \uc911\uc694\ud558\uc9c0 \uc54a\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud558\uae30 \uc5b4\ub835\uae30 \ub54c\ubb38\uc5d0, \uc77c\ub2e8 \uc5ec\uae30\uc11c\ub294 \ub2e4 \ub0a8\uaca8 \ub193\uace0 \uc9c4\ud589\ud55c\ub2e4.","3331f47d":"\ub2e4\uc2dc, \ub3c4\uba54\uc778 feature\ub4e4\uc744 \ud3ec\ud568\uc2dc\ucf1c \uc911\uc694\ub3c4\ub97c \ud50c\ub78f\ud574\ubd24\ub2e4.\n\nCREDIT_TERM \uc774\ub77c\ub294 \uac83\uc774 \uc6d4\ub4f1\ud558\uac8c \uc601\ud5a5\ub3c4\uac00 \ud070\uac83\uc744 \ubcfc \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc, \uae08\uc735 \uc801\uc778 \ub0b4\uc6a9\uc744 \uc798 \ubaa8\ub974\uae30 \ub54c\ubb38\uc5d0 \uc5ec\uae30\uae4c\uc9c0\ub9cc \ud55c\ub2e4.","c3025b3a":"\uceec\ub7fc \uc218\uac00 \ub108\ubb34 \ub9ce\uc544\uc11c \uc774\uac74 \ubabb\ubcf8\ub2e4. \ud328\uc2a4!","384f808f":"> Well that is extremely interesting! It turns out that the anomalies have a lower rate of default.\nHandling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous.\n\n\n\ub514\ud3f4\ud2b8 \uac12, \uc989 \ud0c0\uac9f\uac12\uc758 \ud3c9\uade0\uac12\uc774 \uc774\ub840\uc801\uc778 \uacbd\uc6b0\uc758 \ub370\uc774\ud130\uac00 \ub354 \uc791\ub2e4. \uc989 \uc9c0\uc815 \uc2dc\uae30\uc5d0 \ub3c8\uc744 \ubaa8\ub450 \uac1a\uc744 \ud655\ub960\uc774 \ub192\ub2e4\ub294 \uac74\ub370 \uc74c...\uc5ec\uae30\uc11c\ub294 \uac00\uc7a5 \uc548\uc804\ud654 \ub370\uc774\ud130 \uac00\uacf5 \ubc29\ubc95\uc73c\ub85c\ub294, \uc774\ub7f0 \uc774\ub840\uc801\uc778 \ub370\uc774\ud130 \ub4e4\uc5d0 \ub300\ud574\uc11c\n\nmissing value \ucde8\uae09\uc744 \ud55c\ub2e4\uace0 \ud55c\ub2e4. \ud558\uc9c0\ub9cc `np.nan`\uac12\uc73c\ub85c \ubaa8\ub450 \ucc44\uc6cc\uc900\ub2e4\uace0 \ud55c\ub2e4. \uadf8\ub9ac\uace0, boolean\uceec\ub7fc\uc744 \uc0c8\ub85c \ub9cc\ub4e4\uc5b4\uc11c \ud574\ub2f9 \uace0\uac1d \uc815\ubcf4\uac00 \uc774\ub840\uc801\uc778 \ub370\uc774\ud130\uc778\uc9c0 \uc544\ub2cc\uc9c0\ub97c \uc54c\ub824\uc8fc\ub294 \uc6a9\ub3c4\ub85c \uc0ac\uc6a9\ud55c\ub2e4.","221a7840":"\uc6d0\ub798 NaN \uc774\uc600\ub358 \ub370\uc774\ud130 \ub4e4\uc774 \uae30\uc874\uc5d0 \uc788\ub294 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud574\uc11c \ucc44\uc6cc \uc9c4 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\ub2e4.\n\n\ud6c8\ub828\ub370\uc774\ud130\uc640 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ub458\ub2e4 \ub370\uc774\ud130 \ud504\ub808\uc784\uc5d0\uc11c, ndarray \ud615\ud0dc\ub85c \ub370\uc774\ud130 \ud0c0\uc785\uc774 \ubcc0\ud588\ub2e4.","8bb9fa64":">Maybe it's not entirely correct to call this \"domain knowledge\" because I'm not a credit expert, but perhaps we could call this \"attempts at applying limited financial knowledge\". In this frame of mind, we can make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan. Here I'm going to use five features that were inspired by [this script](https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features) by Aguiar:\n>* CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n* ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n* CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n* DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age\n\n>Again, thanks to Aguiar and [his great script](https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features) for exploring these features.\n\n","a17d14c7":">The submission has now been saved to the virtual environment in which our notebook is running. To access the submission, at the end of the notebook, we will hit the blue Commit & Run button at the upper right of the kernel. This runs the entire notebook and then lets us download any files that are created during the run.\nOnce we run the notebook, the files created are available in the Versions tab under the Output sub-tab. From here, the submission files can be submitted to the competition or downloaded. Since there are several models in this notebook, there will be multiple output files.\n\nThe logistic regression baseline should score around 0.671 when submitted.","aeb97d17":"## \ub370\uc774\ud130 \uc77d\uc5b4\ub4e4\uc774\uae30\n\n\uba3c\uc800, \uc0ac\uc6a9\uac00\ub2a5\ud55c \ubaa8\ub4e0 \ud30c\uc77c\uc744 \ub9ac\uc2a4\ud2b8 \uc5c5 \ud574\ubcf8\ub2e4. 9\uac1c\uc758 \ud30c\uc77c\uc774 \uc788\uc73c\uba70,\uac01\uac01 1\uac1c\uc529 \ud6c8\ub828, \ud14c\uc2a4\ud2b8 \ud30c\uc77c\uc774 \uc788\ub2e4. \ub610\ud55c, \uc81c\ucd9c \uc608\uc2dc \ud30c\uc77c\uacfc 6\uac1c\uc758 \ud53c\uccd0 \ub0b4\uc6a9\uc774 \ub4e4\uc5b4\uc788\ub294 \ud30c\uc77c\uc774 \uc788\uc74c.","75e0c619":"\uc5b4\ub9b0 \uace0\uac1d\uc774 \uba85\ubc31\ud558\uac8c \uc0c1\ud658\ud558\uc9c0 \ubabb\ud560 \ud655\ub960\uc774 \ud06c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\uace0 \uc788\ub2e4.\n\n\uadf8\ub798\uc11c \uc740\ud589\uc5d0\uc11c\ub294 \uc5b4\ub9b0 \uace0\uac1d\uc5d0\uac8c \ub860\uc5d0 \ub300\ud55c \uc2e0\uccad \ub4f1\uc774 \uc788\uc73c\uba74, \uc8fc\uc758\uc0ac\ud56d\uc774\ub098 \uc548\ub0b4\ub97c \ub354 \uc798 \ud574\uc57c \ub420 \uac83\uc774\ub77c\uace0 \ub9d0\ud558\uace0 \uc788\ub124. \ub9de\ub294 \ub4ef.","0d189454":"* \ucc38\uace0\uc0ac\ud56d : \ub77c\ubca8 \uc778\ucf54\ub529 vs. \uc6d0-\ud56b \uc778\ucf54\ub529\uc5d0 \ub300\ud55c \uc124\uba85 \n\n\n>The problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. In the example above, programmer recieves a 4 and data scientist a 1, but if we did the same process again, the labels could be reversed or completely different. The actual assignment of the integers is arbitrary. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example programmer = 4 and data scientist = 1) to assign weights which is not what we want. If we only have two unique values for a categorical variable (such as Male\/Female), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the safe option.\n\n>There is some debate about the relative merits of these approaches, and some models can deal with label encoded categorical variables with no issues. Here is a good Stack Overflow discussion. I think (and this is just a personal opinion) for categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions (while still trying to preserve information).\n\n>In this notebook, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us. (We will also not use any dimensionality reduction in this notebook but will explore in future iterations).\n\n\n\uc989, \uce74\ud14c\uace0\ub9ac \ubd84\ub958\uc5d0\uc11c \ubd84\ub958\uc790\uac00 2\uac1c \uc774\uc0c1\uc774\uba74, \uc6d0-\ud56b \uc778\ucf54\ub529\uc744 \uc4f0\uace0, 2\uac1c\uc77c\uacbd\uc6b0\ub294 \ub77c\ubca8\uc778\ucf54\ub529\uc73c\ub85c \uc9c4\ud589\ud55c\ub2e4.\n\n2\uac1c\uc77c\uacbd\uc6b0\uc5d0\ub9cc \ub77c\ubca8\uc778\ucf54\ub529\uc744 \uc4f0\ub294 \uc774\uc720\ub294, 2\uac1c \uc774\uc0c1\uc77c \ub54c \ub77c\ubca8 \uc778\ucf54\ub529\uc744 \uc4f0\uba74 \uc784\uc758\uc801\uc778 \uc21c\uc11c\ub97c \ubd80\uc5ec\ud558\ub294 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uc5b4\ub5a4 \ubd84\ub958\uc790\uc5d0 \ub300\ud55c \ud2b9\uc815 \uac00\uc911\uce58\uac00 \uac00\ud574\uc9c0\ub294 \ud6a8\uacfc? \uac00 \uc0dd\uae30\ub294 \uac83\uc73c\ub85c \uc0dd\uac01\ub418\ubbc0\ub85c, 2\uac1c \uc774\uc0c1\uc758 \ubd84\ub958\uc790\ub97c \uac00\uc9c8 \ub54c\ub294 \uc6d0-\ud56b \uc778\ucf54\ub529\uc744 \uc4f0\ub294\uac8c \uc88b\ub2e4.\n\n\ud558\uc9c0\ub9cc \uc548\uc88b\uc740 \uc810\uc740 \ubd84\ub958\uc790 \uc218\ub9cc\ud07c \uceec\ub7fc\uc774 \uc99d\uac00\ud558\uae30 \ub54c\ubb38\uc5d0 \uc5c4\uccad\ub09c \uc591\uc758 \ub370\uc774\ud130\uac00 \uc0dd\uc131\ub41c\ub2e4\ub294 \uc810\ub9cc \ube7c\uba74 \uc6d0-\ud56b \uc778\ucf54\ub529\uc774 \ud6e8\uc52c \uc88b\ub2e4\uace0 \ud568.","4473489d":"> One simple feature construction method is called polynomial features. In this method, we make features that are powers of existing features as well as interaction terms between existing features. For example, we can create variables EXT_SOURCE_1^2 and EXT_SOURCE_2^2 and also variables such as EXT_SOURCE_1 x EXT_SOURCE_2, EXT_SOURCE_1 x EXT_SOURCE_2^2, EXT_SOURCE_1^2 x EXT_SOURCE_2^2, and so on. These features that are a combination of multiple individual variables are called interaction terms because they capture the interactions between variables. In other words, while two variables by themselves may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target. Interaction terms are commonly used in statistical models to capture the effects of multiple variables, but I do not see them used as often in machine learning. Nonetheless, we can try out a few to see if they might help our model to predict whether or not a client will repay a loan.\nJake VanderPlas writes about polynomial features in his excellent book Python for Data Science for those who want more information.\nIn the following code, we create polynomial features using the EXT_SOURCE variables and the DAYS_BIRTH variable. Scikit-Learn has a useful class called PolynomialFeatures that creates the polynomials and the interaction terms up to a specified degree. We can use a degree of 3 to see the results (when we are creating polynomial features, we want to avoid using too high of a degree, both because the number of features scales exponentially with the degree, and because we can run into problems with overfitting).\n\n\uac04\ub2e8\ud558\uac8c \ub9d0\ud574\uc11c, \uc774\uac74 \uae30\uc874\uc758 \uc874\uc7ac\ud55c \ubcc0\uc218\uc5d0 \ub300\ud574\uc11c, \uc11c\ub85c \uacf1\ud558\uac70\ub098 \uc81c\uacf1\uc744 \ud558\uac70\ub098, \uc81c\uacf1\ud55c\uac83\uc5d0 \ub300\ud574\uc11c \uc11c\ub85c \uacf1\ud558\uac70\ub098 \ud574\uc11c \uc0c8\ub85c\uc6b4 \ubcc0\uc218\ub97c \uc0dd\uc131\ud558\ub294 \ubc29\uc2dd\uc774\ub2e4.\n\n\uac1c\uac1c\uc758 \ubcc0\uc218\ub4e4\uc774 \ud0c0\uac9f\uacfc\uc758 \uad00\ub828\uc774 \uc5c6\uc5c8\ub354 \ud558\ub354\ub77c\uace0 \uc774\ub7f0\uacfc\uc815\uc744 \ud1b5\ud574\uc11c \uacf1\ud574\uc9c4 \uc0c8\ub85c\uc6b4 \ubcc0\uc218\uac00 \ud0c0\uac9f\uacfc\uc758 \uad00\uacc4\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uacbd\uc6b0\uac00 \uc788\uc73c\ubbc0\ub85c, \uc774\ub97c \uc774\uc6a9\ud558\ub824\uace0 \ud55c\ub2e4. Scikit-Learn\uc758 \ud074\ub798\uc2a4 PolynomialFeatures\ub97c \uc774\uc6a9\ud568!\n\n\uc5ec\uae30\uc11c \uc6b0\ub9ac\ub294 3\ucc28\uc6d0 \uae4c\uc9c0\ub9cc degree \uc815\ub3c4? \ub97c \ubd80\uc5ec\ud558\uae30\ub85c \ud55c\ub2e4. \ucc28\uc6d0\uc218\uac00 \ub108\ubb34 \ub192\uc544\uc9c0\uba74 \ub108\ubb34 \ubcf5\uc7a1\ud574\uc9c8\uc218\ub3c4 \uc788\uace0, [\uc624\ubc84\ud53c\ud305](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)\uc774 \ub418\ub294 \ubb38\uc81c\ub85c \ube60\uc9c8\uc218\ub3c4 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4.\n\n\uc0ac\uc6a9\ud560 \ubcc0\uc218\ub294, EXT_SOURCE \ubcc0\uc218 3\uac1c\uc640 DAYS_BIRTH \ubcc0\uc218 \uc774\ub807\uac8c 4\uac1c\uac00 \ub418\uaca0\ub2e4.","2486b7ed":"\uadf8\ub798\uc11c, \uce74\ud14c\uace0\ub9ac \ub370\uc774\ud130\uc758 dtype==object \uc774\uba74\uc11c \ubd84\ub958\uc790\uac00 2\uac1c\uc774\uba74 label encoding\uc744 \uc0ac\uc6a9!\n\n\ubd84\ub958\uc790\uac00 2\uac1c\uc774\uc0c1\uc758 \uacbd\uc6b0\uc5d0\ub294 one-hot encoding\uc744 \ucc44\uc6a9\ud574\uc11c \uc0ac\uc6a9\ud568.\n\n\ub77c\ubca8 \uc778\ucf54\ub529\uc5d0\uc11c\ub294 Scikit-Learn `LabelEncoder`\uc744 \uc0ac\uc6a9\ud558\uace0,\none-hot encoding \uc5d0\uc11c\ub294 pandas \uc758  `get_dummies(df)` \ud568\uc218\ub97c \uc0ac\uc6a9\ud55c\ub2e4.","9e604dc6":"> From this information, we see this is an imbalanced class problem. There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance.","6f0ad0aa":"`DAYS_EMPLOYED == 365243` \uc774 \ucc38\uc778 \uacf3\uc740 \uc704\uc640 \uac19\uc774 \ub370\uc774\ud130\uac00 \uc5c6\uc74c\uc744 \ud655\uc778\uac00\ub2a5!","7dc207e0":"### Testing Domain Feature\n\n\uc774\uc81c \uc5ec\ub7ec \ubcc0\uc218\ub97c \uc870\ud569\ud574\uc11c \uc0c8\ub85c\uc6b4 \ud615\ud0dc\uc758 feature\ub97c \ub9cc\ub4e4\uc5c8\ub358, domain feature\uc5d0 \ub300\ud574\uc11c \ud14c\uc2a4\ud2b8 \ud574\ubcf4\uc790.","e1c64565":"\ud14c\uc2a4\ud2b8 \ud30c\uc77c\uc740 \ud6c8\ub828 \ud30c\uc77c\uc5d0 \ube44\ud574 \uc0c1\ub2f9\ud788 \ub370\uc774\ud130 \uc218\ub294 \uc791\ub2e4. \n`Target`\uceec\ub7fc\uc774 \ube60\uc838\uc788\ub294 \uac83\uc774 \ud2b9\uc9d5. \uceec\ub7fc\uc218\uac00 121\uc778 \uac83\uc5d0 \uc8fc\ubaa9!","ab1cc633":"feature\ubcc4\ub85c \uc911\uc694\ub3c4\uac00 \uc5b4\ub290 \uc815\ub3c4 \ub418\ub294\uc9c0\ub97c, \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\ub97c \ud54f\ud305\ud558\uba74\uc11c \uc54c\uc544\ub0b4\uc90c","f588876c":"\uc774\uc81c, \ud558\ub098\ub354 \ud574\ubcf4\ub294\ub370, feature\ud56d\ubaa9\uc5d0 \ubcc0\uc218\ub4e4\uc744 \uc870\ud569\ud574\uc11c \uc0c8\ub85c\uc6b4 \ubcc0\uc218\ub97c \ucd94\uac00\ud55c, domain knowledge\ubcc0\uc218\ub97c \ud6c8\ub828\ub370\uc774\ud130\ub85c \uc801\uc6a9\ud574\uc11c,\n\n\uc608\uce21\uc744 \ud574\ubcf8\ub2e4.","6eaf0f87":"> The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try dimensionality reduction (removing features that are not relevant) to reduce the size of the datasets.\n\n\uc6d0\ub798 app_train \ub370\uc774\ud130\uc758 \uceec\ub7fc\uc218\uac00 122\uac1c \uc600\ub294\ub370, \uc5c4\uccad \ub298\uc5c8\ub124. one-hot encoding\uc73c\ub85c 240\uac1c \uae4c\uc9c0 \ub298\uc5c8\ub2e4.\n\n\uc601\ud5a5\uc774 \uc801\uc740 \ub0b4\uc6a9\ub4e4\uc5d0 \ub300\ud574\uc11c\ub294 \ucc28\uc6d0\uc744 \uc904\uc774\uae30 \uc704\ud574\uc11c \uace8\ub77c\ub0bc \ud544\uc694\uac00 \uc788\ub2e4.","a50761cb":"* \ubcf8 \ucee4\ub110\uc740 \ub2e4\uc74c \ucee4\ub110\uc744 \ucc38\uc870\ud558\uc5ec, \uacf5\ubd80\ub97c \uc704\ud55c \ubaa9\uc801\uc73c\ub85c \ub530\ub77c \uc4f4 \ucee4\ub110\uc784.\n* It is referenced by the kernel down below for study.\n\nhttps:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction","089c6227":"\ub370\uc774\ud130 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ud53c\uc5b4\uc2a8\uacc4\uc218\ub97c \ud1b5\ud574\uc11c \ud655\uc778 \ud560 \uc218 \uc788\uc73c\uba70, `corr` \uba54\uc18c\ub4dc\ub97c \uc774\uc6a9\ud568.\n\n1\uc5d0 \uac00\uae4c\uc6b4 \uc218\uce58\ub294 Positive \uc0c1\uad00\uad00\uacc4 \/ -1\uc5d0 \uac00\uae4c\uc6b4 \uc218\uce58\ub294 Negative \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9d0.\n\n0\uc774 \ub418\uba74 \uc0c1\uad00\uad00\uacc4\uac00 \uc5c6\ub2e4\uace0 \ud310\ub2e8\ud558\uba74 \ub428","8dfb46b6":"> Let's take a look at some of more significant correlations: the DAYS_BIRTH is the most positive correlation. (except for TARGET because the correlation of a variable with itself is always 1!) Looking at the documentation, DAYS_BIRTH is the age in days of the client at the time of the loan in negative days (for whatever reason!). The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative.\n\n\ub0b4\uac00 \uc774\ud574\ud558\uae30\ub860, \ub2e4\uc74c\uacfc \uac19\ub2e4. \uc77c\ub2e8, \ubb38\uc11c\uc0c1 DAYS_BIRTH \ub294 \ub300\ucd9c \ub2f9\uc2dc \uace0\uac1d\uc758 \uc5f0\ub839\uc744 \uc758\ubbf8\ud55c\ub2e4.\n\nPositive \uc218\uce58\uac00 \ub192\ub2e4\ub294 \uc758\ubbf8\ub294 \uc989, \ud574\ub2f9 feature \uac12\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud0c0\uac9f \uac12\ub3c4 \uc99d\uac00\ud55c\ub2e4. \uc989, feature\uac12\uc758 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub300\ucd9c\uc744 \uc0c1\ud658 \ud558\uc9c0 \ubabb\ud560 \uac83\uc774\ub2e4 \ub77c\ub294 \ud655\ub960\uc774 \ub192\uc544\uc9c0\ub294 \uacbd\uc6b0\ub2e4.\n\n\uadfc\ub370, \uc5ec\uae30\uc11c Correlation\ud55c \uacb0\uacfc DAYS_BIRTH \ud56d\ubaa9\uc774 Positive \uc218\uce58\uac00 \uac00\uc7a5 \ub192\uc740\ub370, \uc0ac\uc2e4 'DAYS_BIRTH'\ub294 \uc74c\uc218\ub97c \uac00\uc9c0\ub294 \ud56d\ubaa9\uc774\uba70, \uc74c\uc218\uac00 \ucee4\uc9c8\uc218\ub85d \ub300\ucd9c \ub2f9\uc2dc \uace0\uac1d \uc5f0\ub839\uc774 \ub9ce\uc740 \uc0ac\ub78c \uc774\ub77c\ub294 \uc758\ubbf8\ub97c \ub098\ud0c0\ub0b8\ub2e4.\n\n\ud558\uc9c0\ub9cc \uc74c\uc218, \uc591\uc218\ub97c \ubaa8\ub450 \uace0\ub824\ud558\uc5ec \uc99d\uac00\ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \uacc4\uc0b0\ud588\ub2e4\uba74 0\uc5d0 \uac00\uae4c\uc6b4 \uc218, \uc989 \uc808\ub300\uac12\uc774 \uc791\uc740 \uc218\ub85c \uac08\uc218\ub85d TARGET \uac12\uc774 1\uc5d0 \uac00\uae4c\uc6cc\uc9c4\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\ubbc0\ub85c, \ub300\ucd9c \ub2f9\uc2dc \uace0\uac1d\uc758 \uc5f0\ub839\uc774 \uc5b4\ub9b0 \uace0\uac1d\uc77c\uc218\ub85d \uc0c1\ud658\ud558\uc9c0 \ubabb\ud558\ub294 \ud655\ub960\uc774 \ub192\uc544\uc9c4\ub2e4\ub294 \ud310\ub2e8\uc744 \ud560 \uc218\uc788\ub2e4. \n\n\uc774 \ub0b4\uc6a9\uc744 \uc808\ub300\uce58? \ub97c \ud1b5\ud574\uc11c \ub2e4\uc2dc \ud655\uc778\ud574 \ubcf4\uba74, \uc74c\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub098\uc62c \uc218 \uc788\uc744 \uac83\uc774\ub2e4~ \uc989, \ub098\uc774\uac00 \ub9ce\uc740 \uace0\uac1d\uc77c \uc218\ub85d \uc0c1\ud658 \uac00\ub2a5\ud55c \ud655\ub960\uc774 \ub192\uc544\uc9c8 \uac83\uc774\ub2e4 \ub77c\ub294 \uc0c1\uad00\uad00\uacc4\ub97c \uc9c0\uae08\ubd80\ud130 \ud655\uc778\ud558\ub294\ub4ef!","52f1a1f9":"\ud6c8\ub828\ub370\uc774\ud130\uc5d0\ub294 307511\uac1c\uc758 \uac01\uac01 \ub860\uc744 \ud55c \uc0ac\ub78c\uc5d0\ub300\ud574 \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \uc788\uc73c\uba70, 122\uac1c\uc758 \ud53c\uccd0 \uceec\ub7fc\uc744 \ubcf4\uc5ec\uc900\ub2e4. \ud53c\uccd0\uac00 \uc644\uc804 \ub9ce\ub124.","873fff22":"\uc804\uccb4 \ub370\uc774\ud130 \uc911\uc5d0\uc11c \uc774\ub840\uc801\uc778 \ub370\uc774\ud130? 1000\ub144\uc774 \ub118\ub294 \uace0\uc6a9\uae30\uac04\uc73c\ub85c \ub098\uc624\ub294 \ub370\uc774\ud130?\uac00 \uc77c\ubc18 \ub370\uc774\ud130\ub4e4\uc758 \ud1b5\uacc4 \uc218\uce58\ubcf4\ub2e4 \ub192\uc740\uc9c0 \ub0ae\uc740\uc9c0 \ubd80\ubd84\uc73c\ub85c \ub5bc\uc5b4\ub0b4\uc11c \uc0b4\ud3b4\ubcf4\uc790.\n\uc989, \uc815\uc0c1\uc778 \ub370\uc774\ud130, \uc815\uc0c1\uc774 \uc544\ub2cc \ub370\uc774\ud130!","a245a2ba":"EDA\ub97c \ud1b5\ud574\uc11c \ub370\uc774\ud130\ub4e4\uc758 \ud1b5\uacc4\uc801\uc778 \ucd94\uc774, \ub370\uc774\ud130 \uac04\uc758 \uad00\uacc4 \ubc0f \uc5b4\ub5a4 \ud328\ud134\ub4e4\uc744 \ud30c\uc545\ud560 \uc218 \uc788\ub2e4.\n\ubaa9\ud45c\ub294, \ub370\uc774\ud130\ub4e4\uc774 \uc5b4\ub5a4 \uac83\ub4e4\uc744 \uc598\uae30\ud558\uace0 \uc788\ub294\uc9c0 \ud30c\uc545\ud558\ub294 \uac83\uc774\uba70, \uc804\uccb4\uc801\uc778 \ucd94\uc774\ub97c \uba3c\uc800 \ud655\uc778\ud558\uace0, \uc810\uc810 \uc881\ud600 \ub098\uac00\ub294 \uc21c\uc73c\ub85c \ud558\ub294\uac83\uc774 \uc88b\ub2e4.\n\ub370\uc774\ud130\ub4e4\uc758 \uc5b4\ub5a4 \ud328\ud134\uc744 \ud30c\uc545\ud558\uba74, \ubaa8\ub378\uc744 \uc120\ud0dd\ud558\ub294\ub370 \ub3c4\uc6c0\uc744 \uc904 \uc218 \uc788\uace0, \uc5b4\ub5a4 \ud53c\uccd0\ub4e4\uc744 \uc0ac\uc6a9\ud560 \uc9c0 \uacb0\uc815\ud560 \uc218 \uc788\ub3c4\ub85d \ud574\uc900\ub2e4.","f8c0b9ce":"\ud788\uc2a4\ud1a0\uadf8\ub7a8\uc73c\ub85c \ub450 \ub370\uc774\ud130\ub97c \uc2dc\uac01\ud654 \ud574\ubcf4\uc790!","fea7cac3":"\uc774\uc81c, \uac1d\uccb4 \ub0b4\uc6a9\uc744 \uac00\uc9c0\ub294 \ud2b9\ubcc4\ud55c \uceec\ub7fc\uc758 \ub370\uc774\ud130\ub97c \uc0b4\ud45c\ubcf8\ub2e4.","23de0902":"> When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can handle missing values with no need for imputation. Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now.","ae431b47":"## Feature Engineering","94db4b58":"\uac00\uc7a5 \uc601\ud5a5\ub825\uc774 \uc788\ub294 \ubcc0\uc218\uac00 \uc5b4\ub5a4 \uac83\uc778\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574, feature importance \uac12\uc744 \uc0b4\ud3b4 \ubcf4\uba74\ub41c\ub2e4.\n\nEDA\ub97c \ud1b5\ud574\uc11c \uac00\uc7a5 \uadf8 \uc601\ud5a5\uc774 \ud074 \uac83\uc73c\ub85c \uc608\uc0c1\ub418\ub294 \uac83\uc740, EXT_SOURCE\ubcc0\uc218\uacfc DAYS_BIRTH\uc774\ub2e4.\n\n\ub098\uc911\uc5d0 \uc774\ub7f0 \ubcc0\uc218\ub4e4\uc740 \uc608\uce21\uc5d0 \uc0ac\uc6a9\ud560 \ubcc0\uc218\uc758 \ucc28\uc6d0\uc744 \uc904\uc774\uae30 \uc704\ud55c \uc218\ub2e8? \uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc744 \uac83 \uac19\ub2e4.","4f1a26e9":"> There need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!\n\n\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\ub294 \uc5c6\ub294 \uceec\ub7fc\ub370\uc774\ud130(\uce74\ud14c\uace0\ub9ac \ub370\uc774\ud130)\uac00 \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\uc5d0 \uc788\uae30 \ub54c\ubb38\uc5d0 \ucc28\uc774\uac00 \ub098\ub294\ub370, \uadf8 \uceec\ub7fc\ub4e4\uc744 \uc81c\uac70 \ud574\uc8fc\ub294 \uac8c \uc88b\ub2e4.\n\n\uc989, \ud2b8\ub808\uc774\ub2dd\uc5d0\ub9cc \uc788\ub294 \uceec\ub7fc\ub4e4\uc744 \uc0ad\uc81c\ud574\uc11c, \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc640 align \ud574\uc918\uc57c \ud55c\ub2e4\ub294 \ub9d0!\n\n\uba3c\uc800, \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\ub97c \uac00\uacf5\ud574\uc57c\ub418\ub294\ub370, \ud0c0\uac9f\ub370\uc774\ud130\ub97c \uc0b4\ub824\ub450\uae30 \uc704\ud574\uc11c \ub530\ub85c \uc800\uc7a5\ud574\ub193\uc790!\n\n\uc8fc\uc758\ud560 \uc810\uc740 axis=1\uc758 \uceec\ub7fc\ub370\uc774\ud130\ub97c \uac00\uacf5\ud574\uc57c\ud558\ub294 \uc810!, axis=0\uc758 \uacbd\uc6b0\uc5d0\ub294 row \ub370\uc774\ud130\ub97c \ub098\ud0c0\ub0c4!","2f132a99":">The predictions represent a probability between 0 and 1 that the loan will not be repaid. If we were using these predictions to classify applicants, we could set a probability threshold for determining that a loan is risky.\n\n\uc5ec\uae30\uc11c \ud655\ub960\uc740 \ub300\ucd9c\uc744 \uc0c1\ud658\ud558\uc9c0\ubabb\ud560 \ud655\ub960\uc744 \ub098\ud0c0\ub0b4\uace0, \ub9cc\uc57d \ub300\ucd9c\ub300\uc0c1\uc5d0 \ub300\ud574\uc11c \uc774 \uc608\uce21\uacb0\uacfc\ub97c \uc0ac\uc6a9\ud55c\ub2e4\uace0 \ud588\ub2e4\uba74, \ub300\ucd9c\uc0c1\ud658 \uc5ec\ubd80\uc5d0 \ub300\ud55c \uc704\ud5d8\uc815\ub3c4\ub97c \ud2b9\uc815 \uc784\uacc4\uce58\ub97c \uc124\uc815\ud574\uc11c \ud310\ub2e8\ud560\uc218 \uc788\uc5c8\uc744 \uac83\uc774\ub2e4.","e34bcddf":">To try and beat the poor performance of our baseline, we can update the algorithm. Let's try using a Random Forest on the same training data to see how that affects performance. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest.\n\n\uae30\ubcf8\uc14b\ud305(LogisticRegression)\uc758 \uc800\uc870\ud55c \uc131\ub2a5\uc744 \uac1c\uc120\ud574\ubcf4\uc790, \uc54c\uace0\ub9ac\uc998\uc744 \ubcc0\uacbd\ud560 \uac74\ub370, Random Forest \uc54c\uace0\ub9ac\uc998? \uc744 \uc0ac\uc6a9\ud574\uc11c \uc131\ub2a5\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc774 \uc788\ub294\uc9c0 \ub3d9\uc77c\ud55c \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \ud3c9\uac00\ud574\ubcf4\uc790.\n\nRandom Forest\ub294 \uc635\uc158\uc911\uc5d0 hundreds of trees\ub97c \uc0ac\uc6a9\ud560\ub54c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4\ub294\ub370, \uc774\uac74 \ubaa8\ub378 \ud54f\ud305\ud560\ub54c \uc635\uc158\uc911\uc5d0 \ud558\ub098 \uc77c \uac83 \uac19\ub2e4.\n\n[RandomForestClassifier \uc124\uba85](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","7c918504":"\ud0c0\ucf13\uc740 \ub860\uc5d0 \ub300\ud574\uc11c \ub300\uc0c1\uc790\uac00 \ub860\uc744 \ubaa8\ub450 \uc0c1\ud658 \ud560 \uac83\uc778\uc9c0 \uc544\ub2cc\uc9c0\ub97c \ubc14\uc774\ub108\ub9ac \ubd84\ub958\ub85c 0 \ub610\ub294 1\ub85c \ub098\ud0c0\ub0b4\uace0 \uc788\ub2e4.\n\n0\uc774 \ubaa8\ub450 \uc815\ud574\uc9c4 \uae30\uac04\ub0b4\uc5d0 \uc815\uc0c1 \uc0c1\ud658\ud558\ub294 \uacbd\uc6b0\n\n1\uc758 \uacbd\uc6b0\ub294 \uc0c1\ud658\ud558\uc9c0 \ubabb\ud558\ub294 \uacbd\uc6b0\ub97c \ub098\ud0d0\ub0c4!\n\n","c2fab794":"### Domain Knowledge Features\n","575c0dbd":"\uc774 \ub178\ud2b8\ubd81\uc744 \ud1b5\ud574\uc11c, \uba38\uc2e0\ub7ec\ub2dd\uc744 \uc5b4\ub5bb\uac8c \uc2dc\uc791\ud574\uc57c \ub418\ub294\uc9c0\uc5d0 \ub300\ud574\uc11c \uacf5\ubd80\ud588\ub2e4.\n\n\ub370\uc774\ud130, \ucef4\ud398\ud2f0\uc158\uc758 \ubaa9\ud45c, \uc5b4\ub5a4 \ubaa8\ub378\uc744 \uc4f8\uc9c0\uc5d0 \ub300\ud574\uc11c \uba3c\uc800 \uc774\ud574\ud55c\ub2e4.\\\n\n\ubaa8\ub378\ub9ac\uc5d0 \ub3c4\uc6c0\uc744 \uc904, \uac04\ub2e8\ud55c EDA\ub97c \ud1b5\ud574\uc11c \ub370\uc774\ud130\uc758 \uad00\uacc4, \ud2b8\ub80c\ub4dc, \ub370\uc774\ud130 \uc774\uc0c1\uc810 \ub4e4\uc744 \ud30c\uc545\ud55c\ub2e4. \/\/ \uacfc\uc815\uc73c\ub85c, \ub370\uc774\ud130\ub97c \ucc44\uc6b0\uace0, \uce74\ud14c\uace0\ub9ac\uceec \ub370\uc774\ud130\ub97c \uc778\ucf54\ub529\ud558\uace0, \ub370\uc774\ud130\ub97c \uc2a4\ucf00\uc77c\ub9c1\ud558\ub294 \ub4f1\uc758 \uc791\uc5c5\uc744 \uc9c4\ud589\ud568\n\n\uadf8 \ub2e4\uc74c\uc5d0, baseline\ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\uc11c, \uc608\uce21\uc744 \ud574\ubcf4\uace0, \ub2e4\uc74c\uc73c\ub85c \ubaa8\ub378\uc744 \uc870\uae08 \ub354 \ubcf5\uc7a1\ud558\uac8c \uac1c\uc120\uc2dc\ud0a4\uac70\ub098, feature\uc548\uc5d0 \ub610 \ub2e4\ub978 \ud2b9\uc131\ub4e4\uc744 \ucd94\uac00 \ud574 \uac00\uba74\uc11c \uc608\uce21 \uc810\uc218\ub97c \uc62c\ub824\uac04\ub2e4.\n\n\n\n>We followed the general outline of a machine learning project:\n1. Understand the problem and the data\n2. Data cleaning and formatting (this was mostly done for us)\n3. Exploratory Data Analysis\n4. Baseline model\n5. Improved model\n6. Model interpretation (just a little)\n\n\n\n","c274973b":"\uae30\uad00 \ubd84\ub958\uc778 `ORGANIZATION_TYPE` \uc774 \uac00\uc7a5 \ub9ce\uc740 \ubd84\ub958\uc790\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4.","910852b4":"\ud3c9\uade0\uce58\ub791 \uc911\uac04\uac12(50%\ubd84\uc704\uc218)\uc774 \uac70\uc758 \uadfc\uc0ac\ud55c \uc218\uce58\ub97c \uac00\uc9c0\uba70, \ucd5c\ub300\/\ucd5c\uc18c\uac12\ub3c4 \uc544\uc6c3\ub77c\uc774\uc5b4\ub77c\uace0 \ud310\ub2e8\ub418\uc9c0\ub294 \uc54a\uc73c\uba70 \ud06c\uac8c \ubb38\uc81c\ub294 \uc5c6\ub294 \uac83\uc73c\ub85c \ud310\ub2e8\ub41c\ub2e4!\n\n\ub2e4\uc74c\uc73c\ub85c\ub294, `DAYS_EMPLOYED` \uc218\uce58\ub97c \ud655\uc778\ud574\ubcf4\uc790!","d2799283":"\ucc38\uace0 \ucee4\ub110\uc0c1\uc758 \uc608\uce21 \uc810\uc218\ub294 0.678\uc774\ub2e4.\n\n\ub0b4 \uc810\uc218\ub3c4 0.67877 \uc774\ub807\uac8c \ub098\uc654\ub2e4. \ub2e8\uc9c0 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub85c \uc608\uce21 \uc54c\uace0\ub9ac\uc998? \uc744 \ubc14\uafb8\uc11c \ubaa8\ub378\uc131\ub2a5\uc744 \uac1c\uc120\ud558\ub294\ub370\ub294 \ud070 \ud6a8\uacfc\ub294 \uc5c6\ub294 \uac83 \uac19\ub2e4.","16d107f3":"### Back to Exploratory Data Analysis\n\nAnomalies(\ubcc0\uce59\uc801\uc778, \uc774\ub840\uc801\uc778 \ub0b4\uc6a9\ub4e4)","29458162":"\ub9c8\uc9c0\ub9c9 \ud50c\ub78f \uac80\ud1a0\ub85c, pairs plot \uc774\ub780 \uac83\uc744 \uc801\uc6a9\ud574\ubcfc\uac70\ub2e4. \uc989, EXT_SOURCE - DAYS_BIRTH \ubcc0\uc218\uac04\uc758 \uc5b4\ub5a4 \uad00\uacc4\ub97c \uc2dc\uac01\ud654 \ud574\uc11c \ubcf4\uac8c \ub420 \uac83\uc774\ub2e4. \n\nPairs Plot\uc740 \ud55c\uac00\uc9c0 \ubcc0\uc218\uc5d0 \ub300\ud55c \ubd84\ud3ec \ubfd0\ub9cc \uc544\ub2c8\ub77c, \uc5ec\ub7ec\uac00\uc9c0 \uba40\ud2f0 variables \uac04\uc758 \uad00\uacc4\ub97c \ud655\uc778\ud558\ub294 \uac83\ub3c4 \uac00\ub2a5\ud558\uac8c \ud574\uc900\ub2e4.\n\nseaborn\uc758 \uc2dc\uac01\ud654 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc640 `PairGrid` \ud568\uc218\ub97c \uc774\uc6a9\ud574\uc11c \ud50c\ub78f \uba87\uac1c\ub97c \uadf8\ub824\ubcfc \uac83\uc774\ub2e4.\n\n\uc790\uc138\ud55c \uc124\uba85\uc740 \uc704\uc758 \uc6d0\ubb38 \ucc38\uc870!\n","707c77c5":"### Exterior Sources\n","a0015ee2":"## Just for Fun : Light Gradient Boosting Machine","1ed9ca17":"### Logistic Regression Implementation","a83dc5b1":">Now that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model predict.proba method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan is not repaid, so we will select the second column.\n\n>The following code makes the predictions and selects the correct column.\n\n\ubaa8\ub378\uc774 \ud6c8\ub828\uc774 \ub418\uc5c8\uace0, \uc608\uce21\uc744 \ud560\uac70\ub2e4.\n\n\uc5ec\uae30\uc11c \uc6d0\ud558\ub294 \uac83\uc740 \ub300\ucd9c\uc744 \uc0c1\ud658\ud558\uc9c0 \ubabb\ud558\ub294 \ud655\ub960\uc774\ub2e4. \n\n\uc608\uce21\uc744 \uc9c4\ud589\ud558\uba74, m x 2 \ud615\ud0dc\ub85c \uc5b4\ub808\uc774\ub97c \ubc18\ud658\ud558\uac8c \ub418\ub294\ub370, m\uc740 \uc0d8\ud50c \uad00\ucc30\uc218\uc774\uba70,\n\n\uccab\ubc88\uc9f8 \uceec\ub7fc\uc740 \uc608\uce21\uc774 0\uc774 \ub420 \ud655\ub960, \ub450\ubc88\uc9f8 \uceec\ub7fc\uc740 \uc608\uce21\uc774 1 \uc774 \ub420 \ud65c\ub960.\n\n\uc6b0\ub9ac\ub294, \uc0c1\ud658\ud558\uc9c0 \ubabb\ud560 \ud655\ub960\uc744 \uc54c\uace0 \uc2f6\uae30 \ub54c\ubb38\uc5d0 \ub450\ubc88\uc9f8 \uceec\ub7fc\uc744 \uc120\ud0dd\ud55c\ub2e4.","6099e4d5":"\uc5ed\uc2dc \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\uc640 \uac19\uac8c \uc774\uc0c1\uce58\uac00 \ubcf4\uc778\ub2e4.","935ebb7c":"## EDA(Exploratory Data Analysis)","f957f45c":"\ud655\uc2e4\ud788, \ub098\uc774\uac00 \ub9ce\uc740 \uadf8\ub8f9\uc77c\uc218\ub85d \ud0c0\uac9f \ud3c9\uade0\uac12\uc774 \uc810\uc810 \uc791\uc544\uc9c0\ub294 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\ub2e4.\n","1de19c6f":"### Correlations","33a2890a":"\ub77c\ubca8 \uc778\ucf54\ub529 \ub41c\uac78 \ud655\uc778\ud560 \uc218 \uc788\ub2e4","81fffa43":"\uc774\uc81c \uac00\uacf5\ud55c \ub370\uc774\ud130 \uc14b\uc744 \ub300\uc0c1\uc73c\ub85c \ubaa8\ub378\uc744 \uad6c\uc131\ud574\uc11c, \ub300\ucd9c\uc0c1\ud658 \uac00\ub2a5\uc5ec\ubd80\ub97c \uc608\uce21\ud574\ubcf8\ub2e4.","b3630955":"\uac00\uacf5\ud55c feature\ub4e4\uc5d0 \ub300\ud574\uc11c \ud0d1 15\uac1c \uc548\uc5d0 \ub2e4 \ub4e4\uc5b4\uac00 \uc788\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","ad5a2a4c":"### Examine the Distribution of the Target Column","85993e62":"\uc608\uc0c1\ud55c \ubc14\uc640 \uac19\uc774, \uc74c\uc218\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub098\uc654\ub2e4.\n\n\uc989, \uace0\uac1d \ub098\uc774\uac00 \ub9ce\uc744 \uc218\ub85d, \ud0c0\uac9f\uc740 0\uc5d0 \uac00\uae4c\uc6cc \uc9c0\ub294 \uc120\ud615\uad00\uacc4\ub97c \uac00\uc9d0\uc744 \uc54c \uc218 \uc788\ub2e4. \uadf8\ub9ac\uace0, \uadf8 \ub9d0\uc740 \ub098\uc774\uac00 \ub9ce\uc744 \uc218\ub85d \ub300\ucd9c \uc0c1\ud658\uc744 \uc81c\ub54c \ud560 \ud655\ub960\uc774 \ub192\uc544\uc9c4\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4.\n\n\uc774\uc81c, \uace0\uac1d\uc758 \ub098\uc774 \ubd84\ud3ec\uc5d0 \ub300\ud574 \uc2dc\uac01\ud654 \ud558\uc5ec \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc73c\ub85c \ud50c\ub78f\ud574\ubcf4\uc790.","cf3aecb2":"\ub05d!","9655de3d":"app_test \uceec\ub7fc\uc5d0 \ub9de\ucdb0\uc11c app_train \ub370\uc774\ud130\uc758 \uceec\ub7fc\uc218\ub97c \ub9de\ucdc4\ub2e4!","3bc052e3":"## \uacb0\ub860","26dee847":"\ud655\uc2e4\ud788 \uc774 \ub370\uc774\ud130\uac00 \uc720\uc6a9\ud55c\uc9c0\ub294 \ub9d0\ud558\uae30 \uc5b4\ub835\ub2e4\uace0 \ud568. \uc9c1\uc811 \ubaa8\ub378\uc5d0 \uc0ac\uc6a9\ud574 \ubd10\uc57c \uc54c\uc218 \uc788\ub294 \ub4ef.","8aee842e":"poly_features \uceec\ub7fc \uc218\ub294 \uc6d0\ub798 EXT \ubcc0\uc218 3\uac1c\uc640 DAYS_BIRTH \uc774\ub807\uac8c 4\uac1c \uc600\ub294\ub370 35\uac1c \uae4c\uc9c0 \ub298\uc5b4\ub0ac\ub2e4.\n\n\uc0c8\ub85c \uc0dd\uae34 polynomial \ubcc0\uc218\uc758 \uc774\ub984\uc744 \uac00\uc838\uc624\uae30 \uc704\ud574 `get_feature_names`\uba54\uc18c\ub4dc\ub97c \uc774\uc6a9\ud55c\ub2e4.","188b933b":"\ucd5c\uc18c\uac12\uc774 \uc77c\ub2e8, \uc74c\uc218\uc774\uace0, \ucd5c\ub300\uac12\uc740 1000\uc774\uba70, 75% \ubd84\uc704\uc218\uac00 \uc5ec\uc804\uc774 \uc74c\uc218\uc774\ub2e4. \uc774\uc0c1\ud55c \uac70 \uac19\ub2e4. \n\n\uadf8\ub9ac\uace0, \uc5f0(year) \uc218\ub85c \ud658\uc0b0\ud588\ub294\ub370, 1000\ub144\uc774 \ub098\uc654\uc73c\ubbc0\ub85c, \uc62c\ubc14\ub978 \ub370\uc774\ud130\ub294 \uc544\ub2cc\ub4ef.","47aed89d":"\uba87\uba87 \uc0c8\ub85c\uc6b4 \ubcc0\uc218\ub4e4\uc740 \ub3c5\ub9bd\ub41c \ud558\ub098\uc758 \ubcc0\uc218\ub85c\uc368 \uc0c1\uad00\uad00\uacc4\ubcf4\ub2e4 \ub354 \ub192\uc740 \uc218\uce58\ub97c \ubcf4\uc5ec\uc8fc\uace0 \uc788\ub2e4.\n\n\uc6b0\ub9ac\ub294, \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc744 \ube4c\ub4dc\ud560\ub54c \uc720\uc6a9\ud55c\uc9c0 \uc5ec\ubd80\uc5d0 \ub530\ub77c \uc774\ub7f0 \ubcc0\uc218\ub4e4\uc744 \ud3ec\ud568 \ub610\ub294 \ud3ec\ud568\uc2dc\ud0a4\uc9c0 \uc54a\uc744 \uc218\ub3c4 \uc788\ub2e4.\n\n\uc774\uc81c, \ubaa8\ub378\uc744 \ud3c9\uac00\ud560\ub54c \uc774\ub7f0 feature\ub97c \ud3ec\ud568 \uc2dc\ud0a8 \ub370\uc774\ud130\uc640 \uadf8\ub807\uc9c0 \uc54a\uc740 \ub370\uc774\ud130\ub97c \ubaa8\ub378\uc5d0 \ud53c\ub4dc\ud574\uc11c \uacb0\uacfc\uac00 \uc5b4\ub5a4\uc9c0 \ud655\uc778\ud560 \uac70\ub2e4.\n\n\uba38\uc2e0\ub7ec\ub2dd\uc740 \uc5ec\ub7ec\ubc88 \ud574\ubd10\uc57c \ud568. Try!","41cc5bd4":"\uc704\uc758 \uacfc\uc815\uc744 \ud568\uc218\ud654 \ud558\uc5ec \ub2e4\uc2dc \ud45c\ud604\ud55c\ub2e4!","36dad107":">In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the EXT_SOURCE_1 and the DAYS_BIRTH (or equivalently YEARS_BIRTH), indicating that this feature may take into account the age of the client.\n\n\n\ube68\uac04\uc0c9\uc740 \ub300\ucd9c\uc0c1\ud658 x, \ud30c\ub780\uc0c9\uc740 \ub300\ucd9c\uc0c1\ud669 o\uc744 \uc758\ubbf8\ud55c\ub2e4.\n\nEXT_SOURCE_1 , YEARS_BIRTH\ub294 \uc120\ud615\uc801\uc778 \uad00\uacc4\ub97c \uac00\uc9c4\ub2e4\uace0 \ubcfc \uc218 \uc788\uc73c\uba70, \uc5ed\uc2dc\ub098 \ub098\uc774\uac00 \ub9ce\uc544\uc9d0\uc5d0 \ub530\ub77c \ud30c\ub780\uc0c9 \uc810\uc774 \ub9ce\uc544\uc9c0\ub294 \uac83\uc744 \uc54c\uc218 \uc788\ub2e4.","8b167fcc":"### Effect of Age on Repayment","48ffd723":"> By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a kernel density estimation plot (KDE) colored by the value of the target. A kernel density estimate plot shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph.\n\n\uc774 \uc790\uccb4\ub85c\ub294, \uc544\uc6c3\ub77c\uc774\uc5b4\uac00 \uc5c6\ub2e4\ub294\uac83 \uc774\uc0c1\uc73c\ub85c \ubb34\uc5b8\uac00 \ub9d0\ud574\uc904\uc218\ub294 \uc5c6\uc73c\uba70, \uadf8\ub798\ud504 \ubd84\ud3ec\ub294 \uc774\uc0c1\uc801\uc778\uac70 \uac19\ub2e4.\n\n\uc774\uc81c \uace0\uac1d \ub098\uc774\uc5d0 \ub530\ub978 \ud0c0\uac9f\uc758 \uc601\ud5a5\uc815\ub3c4\ub97c \ud655\uc778\ud558\uae30 \uc704\ud574, KDE\ub97c \uc0ac\uc6a9\ud574\uc11c \ud0c0\uac9f\uc758 \uc601\ud5a5\ub3c4\ub97c \uc2dc\uac01\ud654 \ud574\ubcf8\ub2e4.\n\n\uc774 \uacfc\uc815\uc5d0\uc11c seaborn\uc758 `kdeplot` \uba54\uc18c\ub4dc\ub97c \ud65c\uc6a9\ud55c\ub2e4.","f3f7df6e":"## \uc0ac\uc6a9\ud560 \ubaa8\ub4c8 \ubd88\ub7ec\uc624\uae30.","c715bc5e":"\ucc38\ub2f4\ud55c \uacb0\uacfc\ub2e4, public \uc810\uc218\ub294 0.60 \ub300 \uae4c\uc9c0 \ub5a8\uc5b4\uc84c\ub2e4. \ucc38\uace0\ud55c \ucee4\ub110\uc5d0\uc11c\ub2940.678\ub85c \ub3d9\ub4f1\ud55c \uc218\uc900\uc774 \ub098\uc654\ub2e4.\n\n\ub0b4\uac00 \uc5b4\ub518\uac00 \ud2c0\ub9b0 \uc870\uc791\uc744 \ud588\uac70\ub098, \uc544\ubb34\ud2bc polynomial feature\ub294 \ud070 \ud6a8\uacfc\uac00 \uc5c6\ub2e4. \uc624\ud788\ub824 \ud655\ub960 \uc815\ud655\ub3c4\uac00 \ub5a8\uc5b4\uc9c4\ub2e4.","2feff171":"DAYS_BIRTH_x,DAYS_BIRTH_y \ub85c \ubcc0\ud568.\n\nEXT_SOURCE_1_x\n\nEXT_SOURCE_2_x\n\nEXT_SOURCE_3_x\n\n\ub098\uba38\uc9c0 \uac83\ub3c4 y\ud56d\ub3c4 \uc0dd\uacbc\uc74c\n\n","3f34ef57":"### Improved Model : Random Forest","f71ef706":"### Column Types\n\n\uac01 \ub370\uc774\ud130 \ud0c0\uc785\uc5d0 \ub530\ub978 \uceec\ub7fc\uc758 \uc22b\uc790\ub97c \ud655\uc778\ud55c\ub2e4. `int64`, `float64`\ub294 \uc218\uce58\uc801\uc778 \uac12\uc744 \ub098\ud0c0\ub0b8\ub2e4(\uc5f0\uc18d\uc801 \ub610\ub294 \ubd88\uc5f0\uc18d\uc801\uc77c \uc218 \uc788\uc74c). \uac1d\uccb4(object) \uceec\ub7fc\uc740 \ubb38\uc790\uc5f4 \uadf8\ub9ac\uace0 \uce74\ud14c\uace0\ub9ac\uceec \ud53c\uccd0 \ub0b4\uc6a9\uc744 \uac00\uc9c0\uace0 \uc788\uc74c.","11611ae2":"### Pairs Plot","ebef6cba":"### Aligning Training and Testing Data\n\n\uc77c\ub2e8, \ud2b8\ub808\uc774\ub2dd\/\ud14c\uc2a4\ud305 \uceec\ub7fc \uc218 \ucc28\uc774\uac00 \ud0c0\uac9f \uceec\ub7fc \ud558\ub098\ub9cc \ub098\ub294\uac8c \uc815\uc0c1\uc778\ub370, \ud604\uc7ac \ub370\uc774\ud130\ub97c \ubcf4\uba74 \ucc28\uc774\uac00 \uc788\ub2e4. ","5b12e668":"> The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket.\nTo make this graph, first we cut the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category.\n\ntarget == 1 \uadf8\ub798\ud504\ub294 \ud655\uc2e4\ud788 \ub098\uc774\uac00 \uc5b4\ub9b0 \ucabd\uc73c\ub85c \ub354 \ubfb0\uc871\ud55c \ubd80\ubd84\uc774 \uc3e0\ub824\uc788\ub2e4.\n\n\ud558\uc9c0\ub9cc \uadf8\ub807\uac8c \ud070 \uc0c1\uad00\uacc4\uc218(-0.07)\ub294 \uc544\ub2c8\ub2e4. \uadf8\ub798\ub3c4 \uba38\uc2e0\ub7ec\ub2dd\uc5d0 \uc0ac\uc6a9\ud558\uae30\uc5d0\ub294 \uc720\uc6a9\ud558\ub2e4\uace0 \uc0dd\uac01 \ub41c\ub2e4.\n\n\uc870\uae08 \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c \uad00\uacc4\ub97c \uc0b4\ud3b4\ubcfc\ub824\uace0 \ud55c\ub2e4. average failure to repay loans by age bracket. \uc989 \ub098\uc774\ub97c \ubc84\ucf13\uc5d0 \ub2f4\uc544\uc11c \uc870\uac01\uc870\uac01 \ub098\ub220\uc11c \ubcf4\uac9f\ub2e4\ub294 \ub73b.\n\n\uc989, 5\uc0b4 \ub2e8\uc704\ub85c \ub098\uc774 \ubd84\ud3ec\ub97c \ub098\ub220\uc11c \uac01 \ubc84\ucf13\ubcc4\ub85c \ud0c0\uac9f\uc758 \ud3c9\uade0\uac12\uc744 \uc0b4\ud3b4\ubcf8\ub2e4. ","120fd2de":"\ud575\uc2ec\uc740 Feature Engineering!","fb0dad0c":"\uc704 \uc218\uce58\ub4e4\uc744 heatmap\uc744 \ud1b5\ud574\uc11c \uc2dc\uac01\ud654 \ub41c \ub370\uc774\ud130\ub97c \ud655\uc778\ud55c\ub2e4.","448b3b0f":"1. Label encoding :\n\n\uac01 \ubd84\ub958\uc790 \ubcc4\ub85c \uc815\uc218\uac12\uc744 \ud560\ub2f9\ud574\uc11c \ubd84\ub958\ud55c\ub2e4. \uc989, \uc0c8\ub85c\uc6b4 \uceec\ub7fc\uc744 \ub9cc\ub4e4 \ud544\uc694\uc5c6\uc774 \ud574\ub2f9 \ubd84\ub958\uc5d0 \ub9de\ub294 \uc815\uc218\uac12\uc744 \ud560\ub2f9\ud574\uc11c \ubc14\uafd4\uc8fc\uba74 \ub428\n![image.png](attachment:image.png)","80009507":"EXT \ubcc0\uc218\ub4e4\uc740 \ud0c0\uac9f\uacfc \uc74c\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uac16\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, EXT \ubcc0\uc218\uac12\uc774 \uc99d\uac00\ud558\uba74, \uace0\uac1d\ub4e4\uc774 \ub300\ucd9c\uc0c1\ud658 \ud560 \ud655\ub960\uc774 \ub192\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4.\n\n\ub610\ud55c, EXT 1 \ubcc0\uc218\uc640 DAYS_BIRTH\uac00 \uc591\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.\n\uc774\uac83\uc740, EXT 1 \ubcc0\uc218\uac00 \ub098\uc774\ub97c \ub098\ud0c0\ub0b4\ub294 \uc5b4\ub5a4 \uc2a4\ucf54\uc5b4 \uc77c \uac83\uc73c\ub85c \ucd94\uc815\ub41c\ub2e4.\n\n\uc774\uc81c \uc774 \ubcc0\uc218\ub4e4\uacfc \ud0c0\uac9f\uac04\uc758 \ubd84\ud3ec\ub97c \uc2dc\uac01\ud654 \ud574\uc11c \uc0b4\ud3b4 \ubcf8\ub2e4.\n","3ed755c3":">For a naive baseline, we could guess the same value for all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition (random guessing on a classification task will score a 0.5).\nSince we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression.","81a9a9ec":"domain features\ub97c \uc0ac\uc6a9\ud588\uc744\ub54c\ub294 \ucc38\uace0\ud55c \ucee4\ub110\uc5d0\uc11c\ub294 0.679\uac00 \ub098\uc654\ub2e4.\n\n\uc2e4\uc81c \ucee4\ubc0b\ud587\uc744\ub54c \uc131\ub2a5\uc758 \ubcc0\ud654\ub294 \uac70\uc758 \uc5c6\ub2e4.(\uadfc\ub370, \ucee4\ub110 \ub05d\uc5d0 \ub098\uc624\ub294, the Gradient Boosting\ubaa8\ub378\uc5d0\ub294 \ud6a8\uacfc\uac00 \uc788\ub2e4\uace0 \ud568)\n\n\ub098\uc911\uc5d0, feature engineering\uc744 \ub2e4\ub978 \ub370\uc774\ud130 \uc18c\uc2a4(\ud30c\uc77c)\uc5d0 \ub300\ud574\uc11c\ub3c4 \ud574\ubcfc \uc608\uc815\uc774\ub2e4. \uadf8\ub7ec\uba74 \uc5b4\ub290\uc815\ub3c4 \uc810\uc218\uac00 \uc62c\ub77c\uac08 \uac83\uc73c\ub85c \uc608\uc0c1","576d0f15":"\uc608\uce21\ud55c \ub0b4\uc6a9\uc774, \uc81c\ucd9c\ud3ec\ub9f7\uc5d0 \ub9de\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0, \uc81c\ucd9c\ud3ec\ub9f7\uc5d0 \ub9de\uac8c \uc124\uc815\ud55c\ub2e4. \uadf8\ub798\uc11c, \ub370\uc774\ud130\ud504\ub808\uc784\uc744 \uc0c8\ub85c \uc0dd\uc131!\n\n`sample_submission.csv`\ucc38\uace0","7393ff53":">We will use LogisticRegressionfrom Scikit-Learn for our first model. The only change we will make from the default model settings is to lower the regularization parameter, C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default LogisticRegression, but it still will set a low bar for any future models.\nHere we use the familiar Scikit-Learn modeling syntax: we first create the model, then we train the model using .fit and then we make predictions on the testing data using .predict_proba (remember that we want probabilities and not a 0 or 1)."}}