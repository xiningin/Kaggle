{"cell_type":{"872ad10b":"code","4d9b2689":"code","8c10ab42":"code","21e5883e":"code","8ac945c5":"code","f019c4ec":"code","1bfb1a0b":"code","8a160682":"code","32114762":"code","a33fd768":"code","ecb34eea":"code","398e1852":"code","755b75ad":"markdown","51784b49":"markdown","8920338a":"markdown"},"source":{"872ad10b":"# My upgrade of model parameters \nn_est = 20\nrandom_state = 42\nmax_samples_model_1 = 0.1\nmax_samples_model_3 = 0.3\nmax_samples_model_5 = 0.5","4d9b2689":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom itertools import product\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom itertools import combinations,permutations\nfrom sklearn.tree import *\nfrom sklearn import tree\nfrom sklearn.ensemble import BaggingClassifier\nimport random\nfrom math import floor","8c10ab42":"data_path = Path(\"\/kaggle\/input\/abstraction-and-reasoning-challenge\")\ntrain_path = data_path\/'training'\ntest_path = data_path\/'test'","21e5883e":"def plot_result(inp,eoup,oup,size=0):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 3, figsize=(15,15))\n    \n    axs[0].imshow(inp, cmap=cmap, norm=norm)\n    axs[0].axis('off')\n    axs[0].set_title('Input')\n\n    axs[1].imshow(eoup, cmap=cmap, norm=norm)\n    axs[1].axis('off')\n    axs[1].set_title('Output')\n    \n    axs[2].imshow(oup, cmap=cmap, norm=norm)\n    axs[2].axis('off')\n    number_model = '' if size == 0 else '_'+str(size)   # My upgrade - number of model visualization\n    axs[2].set_title('Model'+number_model+' prediction')\n    \n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n\ndef plot_mats(mats):\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, len(mats), figsize=(15,15))\n    \n    for i in range(len(mats)):\n        axs[i].imshow(mats[i], cmap=cmap, norm=norm)\n        axs[i].axis('off')\n        axs[i].set_title('Fig: '+str(i))\n    \n    plt.rc('grid', linestyle=\"-\", color='white')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()","8ac945c5":"def getiorc(pair):\n    inp = pair[\"input\"]\n    return pair[\"input\"],pair[\"output\"],len(inp),len(inp[0])\n    \ndef getAround(i,j,inp,size=1):\n    #v = [-1,-1,-1,-1,-1,-1,-1,-1,-1]\n    r,c = len(inp),len(inp[0])\n    v = []\n    sc = [0]\n    for q in range(size):\n        sc.append(q+1)\n        sc.append(-(q+1))\n    for idx,(x,y) in enumerate(product(sc,sc)):\n        ii = (i+x)\n        jj = (j+y)\n        v.append(-1)\n        if((0<= ii < r) and (0<= jj < c)):\n            v[idx] = (inp[ii][jj])\n    return v\n\ndef getDiagonal(i,j,r,c):\n    return\n        \n    \ndef getX(inp,i,j,size):\n    z = []\n    n_inp = np.array(inp)\n    z.append(i)\n    z.append(j)\n    r,c = len(inp),len(inp[0])\n    for m in range(5):\n        z.append(i%(m+1))\n        z.append(j%(m+1))\n    z.append(i+j)\n    z.append(i*j)\n    z.append((i+1)\/(j+1))\n    z.append((j+1)\/(i+1))\n    z.append(r)\n    z.append(c)\n    z.append(len(np.unique(n_inp[i,:])))\n    z.append(len(np.unique(n_inp[:,j])))\n    arnd = getAround(i,j,inp,size)\n    z.append(len(np.unique(arnd)))\n    z.extend(arnd)\n    return z\n\ndef getXy(inp,oup,size):\n    x = []\n    y = []\n    r,c = len(inp),len(inp[0])\n    for i in range(r):\n        for j in range(c):\n            x.append(getX(inp,i,j,size))\n            y.append(oup[i][j])\n    return x,y\n    \ndef getBkgColor(task_json):\n    color_dict = defaultdict(int)\n    \n    for pair in task_json['train']:\n        inp,oup,r,c = getiorc(pair)\n        for i in range(r):\n            for j in range(c):\n                color_dict[inp[i][j]]+=1\n    color = -1\n    max_count = 0\n    for col,cnt in color_dict.items():\n        if(cnt > max_count):\n            color = col\n            max_count = cnt\n    return color","f019c4ec":"def get_num_colors(inp,oup,bl_cols):\n    r,c = len(inp),len(inp[0])\n    return \n\ndef replace(inp,uni,perm):\n    r_map = { int(c):int(s) for c,s in zip(uni,perm)}\n    r,c = len(inp),len(inp[0])\n    rp = np.array(inp).tolist()\n    for i in range(r):\n        for j in range(c):\n            if(rp[i][j] in r_map):\n                rp[i][j] = r_map[rp[i][j]]\n    return rp\n            \n    \ndef augment(inp,oup,bl_cols):\n    cols = \"0123456789\"\n    npr_map = [1,9,72,3024,15120,60480,181440,362880,362880]\n    uni = \"\".join([str(x) for x in np.unique(inp).tolist()])\n    for c in bl_cols:\n        cols=cols.replace(str(c),\"\")\n        uni=uni.replace(str(c),\"\")\n\n    exp_size = len(inp)*len(inp[0])*npr_map[len(uni)]\n    \n    mod = floor(exp_size\/120000)\n    mod = 1 if mod==0 else mod\n    \n    result = []\n    count = 0\n    for comb in combinations(cols,len(uni)):\n        for perm in permutations(comb):\n            count+=1\n            if(count % mod == 0):\n                result.append((replace(inp,uni,perm),replace(oup,uni,perm)))\n    return result\n            \ndef get_flips(inp,oup):\n    result = []\n    n_inp = np.array(inp)\n    n_oup = np.array(oup)\n    result.append((np.fliplr(inp).tolist(),np.fliplr(oup).tolist()))\n    result.append((np.rot90(np.fliplr(inp),1).tolist(),np.rot90(np.fliplr(oup),1).tolist()))\n    result.append((np.rot90(np.fliplr(inp),2).tolist(),np.rot90(np.fliplr(oup),2).tolist()))\n    result.append((np.rot90(np.fliplr(inp),3).tolist(),np.rot90(np.fliplr(oup),3).tolist()))\n    result.append((np.flipud(inp).tolist(),np.flipud(oup).tolist()))\n    result.append((np.rot90(np.flipud(inp),1).tolist(),np.rot90(np.flipud(oup),1).tolist()))\n    result.append((np.rot90(np.flipud(inp),2).tolist(),np.rot90(np.flipud(oup),2).tolist()))\n    result.append((np.rot90(np.flipud(inp),3).tolist(),np.rot90(np.flipud(oup),3).tolist()))\n    result.append((np.fliplr(np.flipud(inp)).tolist(),np.fliplr(np.flipud(oup)).tolist()))\n    result.append((np.flipud(np.fliplr(inp)).tolist(),np.flipud(np.fliplr(oup)).tolist()))\n    return result\n    \ndef gettaskxy(task_json,aug,around_size,bl_cols,flip=True):    \n    X = []\n    Y = []\n    for pair in task_json['train']:\n        inp,oup=pair[\"input\"],pair[\"output\"]\n        tx,ty = getXy(inp,oup,around_size)\n        X.extend(tx)\n        Y.extend(ty)\n        if(flip):\n            for ainp,aoup in get_flips(inp,oup):\n                tx,ty = getXy(ainp,aoup,around_size)\n                X.extend(tx)\n                Y.extend(ty)\n                if(aug):\n                    augs = augment(ainp,aoup,bl_cols)\n                    for ainp,aoup in augs:\n                        tx,ty = getXy(ainp,aoup,around_size)\n                        X.extend(tx)\n                        Y.extend(ty)\n        if(aug):\n            augs = augment(inp,oup,bl_cols)\n            for ainp,aoup in augs:\n                tx,ty = getXy(ainp,aoup,around_size)\n                X.extend(tx)\n                Y.extend(ty)\n    return X,Y\n\ndef test_predict(task_json,model,size):\n    inp = task_json['test'][0]['input']\n    eoup = task_json['test'][0]['output']\n    r,c = len(inp),len(inp[0])\n    oup = predict(inp,model,size)\n    return inp,eoup,oup\n\ndef predict(inp,model,size):\n    r,c = len(inp),len(inp[0])\n    oup = np.zeros([r,c],dtype=int)\n    for i in range(r):\n        for j in range(c):\n            x = getX(inp,i,j,size)\n            o = int(model.predict([x]))\n            o = 0 if o<0 else o\n            oup[i][j]=o\n    return oup\n\ndef submit_predict(task_json,model,size):\n    pred_map = {}\n    idx=0\n    for pair in task_json['test']:\n        inp = pair[\"input\"]\n        oup = predict(inp,model,size)\n        pred_map[idx] = oup.tolist()\n        idx+=1\n        #plot_result(inp,oup,oup)\n    return pred_map\n\ndef dumb_predict(task_json):\n    pred_map = {}\n    idx=0\n    for pair in task_json['test']:\n        inp = pair[\"input\"]\n        pred_map[idx] = [[0,0],[0,0]]\n        idx+=1\n    return pred_map","1bfb1a0b":"def get_loss(model,task_json,size):\n    total = 0\n    for pair in task_json['train']:\n        inp,oup=pair[\"input\"],pair[\"output\"]\n        eoup = predict(inp,model,size)\n        total+= np.sum((np.array(oup) != np.array(eoup)))\n    return total\n\ndef get_test_loss(model,task_json,size):\n    total = 0\n    for pair in task_json['test']:\n        inp,oup=pair[\"input\"],pair[\"output\"]\n        eoup = predict(inp,model,size)\n        total+= np.sum((np.array(oup) != np.array(eoup)))\n    return total\n\ndef get_a_size(task_json):\n    return 4;\n\ndef get_bl_cols(task_json):\n    result = []\n    bkg_col = getBkgColor(task_json);\n    result.append(bkg_col)\n    # num_input,input_cnt,num_output,output_cnt\n    met_map = {}\n    for i in range(10):\n        met_map[i] = [0,0,0,0]\n        \n    total_ex = 0\n    for pair in task_json['train']:\n        inp,oup=pair[\"input\"],pair[\"output\"]\n        u,uc = np.unique(inp, return_counts=True)\n        inp_cnt_map = dict(zip(u,uc))\n        u,uc = np.unique(oup, return_counts=True)\n        oup_cnt_map = dict(zip(u,uc))\n        \n        for col,cnt in inp_cnt_map.items():\n            met_map[col][0] = met_map[col][0] + 1\n            met_map[col][1] = met_map[col][1] + cnt\n        for col,cnt in oup_cnt_map.items():\n            met_map[col][2] = met_map[col][2] + 1\n            met_map[col][3] = met_map[col][3] + cnt\n        total_ex+=1\n    \n    for col,met in met_map.items():\n        num_input,input_cnt,num_output,output_cnt = met\n        if(num_input == total_ex or num_output == total_ex):\n            result.append(col)\n        elif(num_input == 0 and num_output > 0):\n            result.append(col)\n    \n    result = np.unique(result).tolist()\n    if(len(result) == 10):\n        result.append(bkg_col)\n    return np.unique(result).tolist()","8a160682":"def flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred\n\ndef combine_preds(tid,pm1,pm3,pm5):\n    result = []\n    for i in range(len(pm1)):\n        tk_s = tid+\"_\"+str(i)\n        str_pred = flattener(pm1[i])+\" \"+flattener(pm3[i])+\" \"+flattener(pm5[i])\n        #print(tk_s,str_pred)\n        result.append([tk_s,str_pred])\n    return result        ","32114762":"def plots_train(task_json, model, size):\n    # My upgrade - outliers visualization\n    \n    for pair in task_json['train']:\n        inp, oup = pair[\"input\"], pair[\"output\"]\n        train_pred = predict(inp, model, size)\n        if np.sum((np.array(oup) != np.array(train_pred))) > 0:            \n        #if not np.all(inp == train_pred):\n            plot_result(inp, oup, train_pred, size)","a33fd768":"def inp_oup_dim_same(task_json):\n    return all([ len(pair[\"input\"]) == len(pair[\"output\"]) and len(pair[\"input\"][0]) == len(pair[\"output\"][0])\n                for pair in task_json['train']])\n\nsolved_task = 0\ntotal_task = 0\ntask_ids = []\ntask_preds = []\nfor task_path in test_path.glob(\"*.json\"):\n    task_json = json.load(open(task_path))\n    tk_id = str(task_path).split(\"\/\")[-1].split(\".\")[0]\n    #print(tk_id)\n    if(inp_oup_dim_same(task_json)):\n        a_size = get_a_size(task_json)\n        bl_cols = get_bl_cols(task_json)\n        \n        isflip = False\n        X1,Y1 = gettaskxy(task_json,True,1,bl_cols,isflip)\n        X3,Y3 = gettaskxy(task_json,True,3,bl_cols,isflip)\n        X5,Y5 = gettaskxy(task_json,True,5,bl_cols,isflip)\n        \n        # Tuning models\n        model_1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=random_state),\n                                    random_state=random_state, n_estimators=n_est, max_samples = max_samples_model_1).fit(X1, Y1)\n        model_3 = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=random_state),\n                                    random_state=random_state, n_estimators=n_est, max_samples = max_samples_model_3).fit(X3, Y3)\n        model_5 = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=random_state),\n                                    random_state=random_state, n_estimators=n_est, max_samples = max_samples_model_5).fit(X5, Y5)\n        \n        # Outliers visualizing \n        plots_train(task_json,model_1,1)\n        plots_train(task_json,model_3,3)\n        plots_train(task_json,model_5,5)  \n        \n        # Prediction\n        pred_map_1 = submit_predict(task_json,model_1,1)\n        pred_map_3 = submit_predict(task_json,model_3,3)\n        pred_map_5 = submit_predict(task_json,model_5,5)\n        \n        for tks,str_pred in combine_preds(tk_id,pred_map_1,pred_map_3,pred_map_5):\n            task_ids.append(tks)\n            task_preds.append(str_pred)\n        solved_task+=1\n    else:\n        pred_map_1 = dumb_predict(task_json)\n        pred_map_3 = dumb_predict(task_json)\n        pred_map_5 = dumb_predict(task_json)\n        \n        for tks,str_pred in combine_preds(tk_id,pred_map_1,pred_map_3,pred_map_5):\n            task_ids.append(tks)\n            task_preds.append(str_pred)\n        \n    total_task+=1","ecb34eea":"sub_df = pd.DataFrame({\"output_id\":task_ids,'output':task_preds})","398e1852":"sub_df.to_csv(\"submission.csv\", index=None)","755b75ad":"Analysis of scoring of participants shows that most solutions come down to only 18 classes (1.0, 0.99, 0.98, 0.97, 0.96, 0.95, \u2026). In such conditions, it is important to know what the difference between them is.\n\nIn this notebook:\n\n* **tuning parameters** when it gets the same accuracy LB=0.98 (improving solution from [@adityaork](https:\/\/www.kaggle.com\/adityaork)), but it is faster due to generalization of the depth of the tree \n\n* **visualization of outliers** - different (not matching) input-output pairs\n\nA comparative analysis shows that this solution has insufficient generalization and badly predicts large objects. This problem is not easy to solve, because in the way of its solution it is possible to overfit the model and worsen its accuracy for test data.","51784b49":"# [Abstraction and Reasoning Challenge](https:\/\/www.kaggle.com\/c\/abstraction-and-reasoning-challenge)","8920338a":"Many thanks to [@adityaork](https:\/\/www.kaggle.com\/adityaork):   [Decision tree + Smart data augmentation](https:\/\/www.kaggle.com\/adityaork\/decision-tree-smart-data-augmentation)"}}