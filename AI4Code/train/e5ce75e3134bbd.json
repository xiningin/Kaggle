{"cell_type":{"e07e5dbb":"code","445fd230":"code","2ee22384":"code","8aa00c0b":"code","ddb22582":"code","f410f97e":"code","44f22a1a":"code","87e007f1":"code","fc07b067":"code","eb4a9363":"code","091267e3":"code","44817593":"code","21401ec5":"code","8d4f23cb":"code","c53a7cf5":"code","69cd99aa":"code","4b1b3c73":"markdown","fffb4c4b":"markdown","462c3596":"markdown","d2863fc1":"markdown","7d26ac27":"markdown","91a4a85a":"markdown","6bf5f5ef":"markdown","08ddd826":"markdown","3ca46bf2":"markdown"},"source":{"e07e5dbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport time\nimport random\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchtext import data\nfrom torchtext.vocab import Vectors\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","445fd230":"def generate_bigrams(x):\n    n_grams = set(zip(*[x[i:] for i in range(2)]))\n    for n_gram in n_grams:\n        x.append(' '.join(n_gram)) # [\"this\", 'movie'] -> \"this movie\"\n    return x","2ee22384":"generate_bigrams(['this', 'moive', 'is' ,'terrible'])","8aa00c0b":"def load_data(filepath, device):\n    tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split()\n    TEXT = data.Field(sequential=True, lower=True, tokenize=tokenizer, preprocessing=generate_bigrams, fix_length=200)\n#     TEXT = data.Field(sequential=True, lower=True, tokenize=tokenizer, fix_length=200)\n    LABEL = data.Field(sequential=False, use_vocab=False)\n    \n    field = [('text', TEXT), ('label', LABEL)]\n    train, valid, test = data.TabularDataset.splits(filepath, train='Train.csv', validation='Valid.csv', test='Test.csv',\n                                                   format='csv', skip_header=True, fields=field)\n    cache = '\/kaggle\/working\/vector_cache'\n    if not os.path.exists(cache):\n        os.mkdir(cache)\n    vector = Vectors(name='\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt', cache=cache)\n    TEXT.build_vocab(train, vectors=vector, max_size=25000, unk_init=torch.Tensor.normal_)\n    \n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), device=device, batch_size=64, \n                                                             sort_key=lambda x:len(x.text), sort_within_batch=True)\n    return TEXT, LABEL, train_iter, valid_iter, test_iter\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nTEXT, LABEL, train_iter, valid_iter, test_iter = load_data('\/kaggle\/input\/imdb-dataset-sentiment-analysis-in-csv-format', \n                                                           device=device)","ddb22582":"class FastText(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx= pad_idx)\n        self.fc = nn.Linear(embedding_dim, output_dim)\n        \n    def forward(self, text):\n        # text : [sen_len, batch_size]\n        embedded = self.embedding(text)\n        # embedded : [sen_len, batch_size, emb_size]\n        embedded = embedded.permute(1, 0, 2)\n        # embedded : [batch_size, sen_len, emb_size]\n        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n        # pooled : batch_size,, emb_size\n        return self.fc(pooled)\nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nOUTPUT_DIM = 1\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n\nmodel = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)","f410f97e":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","44f22a1a":"pretrained_embeddings = TEXT.vocab.vectors\n\nmodel.embedding.weight.data.copy_(pretrained_embeddings)","87e007f1":"optimizer = optim.Adam(model.parameters())\n\ncriterion = nn.BCEWithLogitsLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)\n\ndef binary_accuracy(preds, y):\n    '''\n    Returns accuracy per batch...\n    '''\n    rounded_preds = torch.round(torch.sigmoid(preds)).long()\n    correct = (rounded_preds == y).float()\n    acc = correct.sum() \/ len(correct)\n    return acc\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time  \/ 60)\n    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n    return  elapsed_mins, elapsed_secs","fc07b067":"def train(model, iterator, optimizer, criterion):\n    epoch_loss, epoch_acc = 0, 0\n    model.train()\n    for i, batch in enumerate(iterator):\n        \n        predictions = model(batch.text).squeeze()\n        \n        loss = criterion(predictions, batch.label.float())\n        acc = binary_accuracy(predictions, batch.label)\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator) ","eb4a9363":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss, epoch_acc = 0, 0\n    \n    model.eval()\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n        \n            predictions = model(batch.text).squeeze(1)\n        \n            loss = criterion(predictions, batch.label.float())\n            \n            acc = binary_accuracy(predictions, batch.label)\n            \n            epoch_acc += acc.item()\n            epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","091267e3":"N_EPOCHS = 5\n\nbest_valid_loss = float('inf')\ntrain_loss_list = []\nvalid_loss_list = []\nfor epoch in range(N_EPOCHS):\n    start_time = time.time()\n    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n    train_loss_list.append(train_loss)\n    valid_loss_list.append(valid_loss)\n    end_time = time.time()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'SentimentModel3.pt')\n    print(f'Epoch: {epoch+1:02} | Epoch Time {epoch_mins}m {epoch_secs}s')\n    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')","44817593":"%pylab inline\n\nplt.figure(figsize=(10, 10))\nplt.plot(np.arange(1, N_EPOCHS+1, 1), train_loss_list, 'r', label=\"Train loss\")\nplt.plot(np.arange(1, N_EPOCHS+1, 1), valid_loss_list, 'b', label=\"Valid loss\")\nplt.xlabel('Epoches')\nplt.ylabel('Loss')\nplt.grid()","21401ec5":"def testModel():\n    bestModel = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX).to(device)\n    bestModel.load_state_dict(torch.load('SentimentModel3.pt'))\n    test_loss, test_acc = evaluate(bestModel, test_iter, criterion)\n    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n    \ntestModel()","8d4f23cb":"def predict_sentiment(sentence):\n    model.eval()\n    tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split()\n    tokenized = generate_bigrams(tokenizer(sentence))\n    print(tokenized)\n    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n    print(indexed)\n    tensor = torch.LongTensor(indexed).to(device)\n    tensor = tensor.unsqueeze(1)\n    bestModel = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX).to(device)\n    bestModel.load_state_dict(torch.load('SentimentModel3.pt'))\n    prediction = torch.sigmoid(bestModel(tensor))\n    return prediction.item()","c53a7cf5":"predict_sentiment(\"this movie is good, but make me tried\")","69cd99aa":"predict_sentiment(\"this movie is good\")","4b1b3c73":"## Summarize\n\nIn fact the bi-gram method is that we create some bi-phrase into our vocab. \n\n(for example we regrad 'is good' as a single word which has its own index 1806 in above example-\"this movie is good\")\n\nthough glove won't recognize it, But in fact it is efficient for our sentiment analysis job!","fffb4c4b":"## Import libs","462c3596":"TorchText Fields have a preprocessing argumnet. A function passed here will be applied to a sentence after it has been tokenized.(transformed from a string to a list of tokens), but before is has been numericalized(transformed from a list of tokens to a list of indexs). This is where we will pass our generate_bigrams.","d2863fc1":"## Train the Model","7d26ac27":"## visualize the results","91a4a85a":"## Preparing Data\n\nOne of the key concepts in the FastText paper is that they calculate the n-grams of an input sentence and append them to the end of a sentence. Here we are going to use bi-grams.\n\nThe generate_bigrams function takes a sentence that has already been tokenized, calculates the bi-grams and appends them to the end of the tokenized list.","6bf5f5ef":"# Faster Sentiment Analysis Tutorial\n\nThis notebook is origin from https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\/ tutorial using our datasets. And specific pretrained embedding. \n\nThis notebook is just use for learning pytorch pipeline and torchtext.","08ddd826":"## Build the Model\n\nThis model has far fewer parameters than the previous model as it only has 2 layers that have any parameters, the embedding layer and the linear layer. There is no RNN component in sight!\n\nInstead, it first calculates the word embedding for each word using the glove 100d (blue), then calculates the average of all of the word embeddings and feeds this through the Linear layer (sliver), and that's it.\n\n![](https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\/raw\/9210842371c3bbde7b2007051dafa4c74d9768cd\/assets\/sentiment8.png)\n\nWe implement the averaging with the avg_pool2d (average pool 2-dimensions) function. We can think of the word embeddings as a 2-dimension grid, where the words are along one axis and the dimensions of the word embeddings are along the other. The image below is an example sentence after being convert into 5-dim word embeddings, with the words along th vertical axis and the embeddings along the horizontal axis. Each element in this $4\\times 5$ tensor is represented by a green block.\n![](https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\/raw\/9210842371c3bbde7b2007051dafa4c74d9768cd\/assets\/sentiment9.png)\n\nThe avg_pool2d uses a filter of size \\[sentence_len, 1\\]. This is shown in pink in the image below.\n![](https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\/raw\/9210842371c3bbde7b2007051dafa4c74d9768cd\/assets\/sentiment10.png)\n\nWe calculate the average value of all elements covered by the filter, then the filter slides to the right, calculating the average over the next column of embedding values for each word in the sentence.\n\n![](https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\/raw\/9210842371c3bbde7b2007051dafa4c74d9768cd\/assets\/sentiment11.png)\n\nEach filter postion gives us a single value, the average of all covered elements. After the filter has covered all embedding dimensions, we get a $[1\\times5]$ tensor. This tensor is then passed through the linear layer to produce our prediction.\n","3ca46bf2":"## User Input\n\nAnd as before, we can test on any input the user provides making sure to generate bigrams from our tokenizerd sentence."}}