{"cell_type":{"6fdcd9d7":"code","66b0144b":"code","18a57d7d":"code","7fdd9900":"code","5693d7e0":"code","978b0003":"code","0c80d2dc":"code","743f3664":"code","0b7e8f69":"code","c520bbcd":"code","4e0f67b7":"code","be6890fc":"code","f04ecb0b":"code","4f95ef02":"code","294fe74c":"code","1b5e65cc":"code","e85fbecf":"code","6853b01b":"markdown","c9489f63":"markdown","d0e648d3":"markdown","d260f3bd":"markdown","6c8efc91":"markdown","40c595a2":"markdown","20e62a71":"markdown","f04f04c4":"markdown","56d8f3bf":"markdown","57c76dd5":"markdown","706630f5":"markdown","69658cf2":"markdown","bef71443":"markdown","1bb26fd4":"markdown","93c70cce":"markdown","ee4de50a":"markdown","38017816":"markdown"},"source":{"6fdcd9d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66b0144b":"!pip install word2number\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [20,8]\nimport seaborn as sns","18a57d7d":"##\ndf=pd.read_csv('..\/input\/auto85\/auto_clean.csv')","7fdd9900":"\n# select numerical columns\ndf_numeric = df.select_dtypes(include=[np.number])\nnumeric_cols = df_numeric.columns.values\n# select non-numeric columns\ndf_non_numeric = df.select_dtypes(exclude=[np.number])\nnon_numeric_cols = df_non_numeric.columns.values","5693d7e0":"# % of values missing in each column\nvalues_list = list()\ncols_list = list()\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())*100\n    cols_list.append(col)\n    values_list.append(pct_missing)\npct_missing_df = pd.DataFrame()\npct_missing_df['features'] = cols_list\npct_missing_df['percent_missing'] = values_list\n# display(pct_missing_df)\npct_missing_df.loc[pct_missing_df.percent_missing > 0].plot(kind='bar', figsize=(12,8))\nplt.show()","978b0003":"less_missing_values_cols_list = list(pct_missing_df.loc[(pct_missing_df.percent_missing < 0.5) & (pct_missing_df.percent_missing > 0), 'features'].values)\nprint('missing_values_cols_list',less_missing_values_cols_list)\ndf.dropna(subset=less_missing_values_cols_list, inplace=True)","0c80d2dc":"# dropping columns with more than 40% null values\n_40_pct_missing_cols_list = list(pct_missing_df.loc[pct_missing_df.percent_missing > 40, 'features'].values)\ndf.drop(columns=_40_pct_missing_cols_list, inplace=True)","743f3664":"# missing values in each numerical column with the median value of that column.\ndf_numeric = df.select_dtypes(include=[np.number])\nnumeric_cols = df_numeric.columns.values\nfor col in numeric_cols:\n    missing = df[col].isnull()\n    num_missing = np.sum(missing)\n    if num_missing > 0:  # impute values only for columns that have missing values\n        med = df[col].median() #impute with the median\n        df[col] = df[col].fillna(med)\n\n# categorical columns, we will replace missing values with the mode values of that column.\ndf_non_numeric = df.select_dtypes(exclude=[np.number])\nnon_numeric_cols = df_non_numeric.columns.values\nfor col in non_numeric_cols:\n    missing = df[col].isnull()\n    num_missing = np.sum(missing)\n    if num_missing > 0:  # impute values only for columns that have missing values\n        mod = df[col].describe()['top'] # impute with the most frequently occuring value\n        df[col] = df[col].fillna(mod)\n##\nprint('missing values left in our dataset = ',df.isnull().sum().sum()) #If the output is zero, it means that there are no missing values left in our dataset now.","0b7e8f69":"df.describe()","c520bbcd":"print('Before: ',df.shape)\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n##\ndf = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint('\\nAfter1: ',df.shape)\n##\nnumerical_cols=df.select_dtypes(['int64','float64'])\nfor col in numerical_cols:\n    feature_value_less_than_3sigma=df[col].mean()-3*(df[col].std())\n    feature_value_greater_than_3sigma=df[col].mean()+3*(df[col].std())\n    df = df[~((df[col]<feature_value_less_than_3sigma)|(df[col]>feature_value_greater_than_3sigma))]\nprint('\\nAfter2: ',df.shape)\n\n# cols = ['col_1', 'col_2'] # one or more\n# Q1 = df[cols].quantile(0.25)\n# Q3 = df[cols].quantile(0.75)\n# IQR = Q3 - Q1\n# df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]","4e0f67b7":"# dropping duplicates by considering all columns other than ID\ncols_other_than_id = list(df.columns)\n# print(cols_other_than_id)\ndf.drop_duplicates(subset=cols_other_than_id, inplace=True)\nprint('\\nSuccess\\n')","be6890fc":"categorical_cols=df.select_dtypes(['object'])\n# display(categorical_cols.sample(2))\nnumerical_cols=df.select_dtypes(['int64','float64'])\n# display(numerical_cols.sample(2))\n\nfrom word2number import w2n\ndf['num-of-cylinders'] = df['num-of-cylinders'].apply(w2n.word_to_num)\ndf['num-of-doors'] = df['num-of-doors'].apply(w2n.word_to_num)\n\n##\nordinal_encoding_dict ={'Low':0,'Medium':1,'High':2}\ndf['horsepower-binned']=df['horsepower-binned'].map(ordinal_encoding_dict)\n\n# Encode Categorical Columns\nfrom sklearn.preprocessing import LabelEncoder\ncat_col=categorical_cols.columns\nle = LabelEncoder()\ndf[cat_col] = df[cat_col].apply(le.fit_transform)\n# display(df.sample(2),df.dtypes)","f04ecb0b":"df.sample(5)","4f95ef02":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\ncv_estimator = RandomForestClassifier(random_state =42)\n\nX=df.drop('price',axis=1)\ny=df['price']\n##\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=0.3, random_state=42)\ncv_estimator.fit(X_train, Y_train)\n\n\n##\ncv_selector = RFECV(cv_estimator,cv= 2, step=1,scoring='accuracy')\ncv_selector = cv_selector.fit(X_train, Y_train)\nrfecv_mask = cv_selector.get_support() #list of booleans\n\n##view\ndata = {'Feature':X_train.columns,'Importance(selector)':rfecv_mask,'feature_importances_(estimator)':cv_estimator.feature_importances_}\nfeature_df=pd.DataFrame(data)\n# display(feature_df)\nimportant_feature_name=list(feature_df[feature_df['Importance(selector)']==True]['Feature'])\nprint(important_feature_name)\n##\nn_features = X_train.shape[1]\nplt.figure(figsize=(8,8))\nplt.barh(range(n_features), cv_estimator.feature_importances_, align='center') \nplt.yticks(np.arange(n_features), X_train.columns.values) \nplt.xlabel('Feature importance')\nplt.ylabel('Feature')\nplt.show()","294fe74c":"df1=df[['length','width','curb-weight','horsepower','price']]\n\n# df1.hist()\nplt.figure(figsize=(20,8))\nsns.heatmap(df1.corr(),linewidths=8,linecolor='black',annot=True,fmt='f',cmap='YlGnBu')","1b5e65cc":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import RobustScaler\n\nX=df1.drop('price',axis=1)\ny=df1['price']\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Building pipelins of standard scaler and model for varios regressors.\n\npipeline_lr=Pipeline([(\"scalar1\",StandardScaler()),\n                     (\"lr_classifier\",LinearRegression())])\n\npipeline_dt=Pipeline([(\"scalar2\",RobustScaler()),\n                     (\"dt_classifier\",DecisionTreeRegressor(max_depth=6))])\n\npipeline_rf=Pipeline([(\"scalar3\",StandardScaler()),\n                     (\"rf_classifier\",RandomForestRegressor(n_estimators = 10, random_state = 42))])\n\n\npipeline_kn=Pipeline([(\"scalar4\",RobustScaler()),\n                     (\"rf_classifier\",KNeighborsRegressor(n_neighbors=3))])\n\n\npipeline_xgb=Pipeline([(\"scalar5\",StandardScaler()),\n                     (\"rf_classifier\",XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8))])\n\n# List of all the pipelines\npipelines = [pipeline_lr, pipeline_dt, pipeline_rf, pipeline_kn, pipeline_xgb]\n\n# Dictionary of pipelines and model types for ease of reference\npipe_dict = {0: \"LinearRegression\", 1: \"DecisionTree\", 2: \"RandomForest\",3: \"KNeighbors\", 4: \"XGBRegressor\"}\n\n# Fit the pipelines\nfor pipe in pipelines:\n    pipe.fit(X_train, y_train)\n\ncv_results_rms = [];cv_mean=[];cv_min=[];cv_max=[]\ncv=10\nfor index, model in enumerate(pipelines):\n    cv_score = cross_val_score(model, X_train,y_train,scoring='r2', cv=cv)\n    cv_results_rms.append(cv_score)\n    cv_mean.append(cv_score.mean())\n    cv_min.append(cv_score.min())\n    cv_max.append(cv_score.max())\n\n##\ndata1={'model':list(pipe_dict.values()),'cross_val_score(min)':cv_min,'cross_val_score(mean)':cv_mean,'cross_val_score(max)':cv_max}\nmodel_df=pd.DataFrame(data1)\nscore_df=pd.DataFrame(cv_results_rms)\nresult = pd.concat([model_df,score_df], axis=1)\ndisplay(result.sort_values(by=['cross_val_score(mean)'], ascending=False))\n# print(cv_results_rms)","e85fbecf":"from sklearn import metrics\n\ndef model_evaluation(model_name,score,y_test, pred):\n    # Model Evaluation\n    r2=metrics.r2_score(y_test, pred) #The Higher the R-squared, the better the model.\n    adj_r2=1 - (1-metrics.r2_score(y_test, pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1) #The lower the RMSE, the better the model.\n    mae=metrics.mean_absolute_error(y_test, pred) # difference between observed and predicted outcomes\n    mse=metrics.mean_squared_error(y_test, pred)\n    rsme=np.sqrt(metrics.mean_squared_error(y_test, pred))\n    \n    data={'model_name':model_name,'score':score,'R^2':r2,'Adjusted R^2':adj_r2,'mean_absolute_error':mae,'mean_squared_error':mse,'Root mean_squared_error':rsme}\n    return data\n# Model prediction on test data\nresult3=[]\npipelines = [pipeline_lr, pipeline_dt, pipeline_rf, pipeline_kn, pipeline_xgb]\npipe_dict = {0: \"LinearRegression\", 1: \"DecisionTree\", 2: \"RandomForest\",3: \"KNeighbors\", 4: \"XGBRegressor\"}\nfor i,model in enumerate(pipelines):\n    pred = model.predict(X_test)\n    score=model.score(X_test, y_test)\n    result3.append(model_evaluation(pipe_dict.get(i),score,y_test, pred))\nresult_df=pd.DataFrame(result3)\ndisplay(result_df.sort_values(by=['score'], ascending=False))","6853b01b":"# machine learning algorithm","c9489f63":"**Drop observations**\n\ndrop observations that contain null in those columns that have less than 0.5% nulls.","d0e648d3":"**Outliers**","d260f3bd":"https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html","6c8efc91":"**separate the numeric columns from the categorical columns.**","40c595a2":"# 2. Feature Selection","20e62a71":"**Fixing data type**","f04f04c4":"**Missing values**","56d8f3bf":"**Duplicate records**","57c76dd5":"![](https:\/\/www.freecodecamp.org\/news\/content\/images\/2020\/08\/how-random-forest-classifier-work.PNG)","706630f5":"# 1. clean data","69658cf2":"# Reference\n\nhttps:\/\/predictivehacks.com\/feature-importance-in-python\/\n\nhttps:\/\/www.kaggle.com\/fazilbtopal\/exploratory-data-analysis-with-python\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/how-to-clean-data-in-python-for-machine-learning\/\n\nhttps:\/\/medium.com\/analytics-vidhya\/feature-selection-using-scikit-learn-5b4362e0c19b\n\nhttps:\/\/towardsdatascience.com\/do-you-know-how-to-choose-the-right-machine-learning-algorithm-among-7-different-types-295d0b0c7f60","bef71443":"**Normalization** is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n\n\n**Standardization**, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n\nStandardScaler, \n\nx_scaled = (x \u2014 u) \/ s\n\nMinMaxScaler,\n\nx_scaled = (x-min(x)) \/ (max(x)\u2013min(x))\n\n\nMaxAbsScaler \n\nx_scaled = x \/ max(abs(x))\n\nRobustScaler.\n","1bb26fd4":"![](https:\/\/miro.medium.com\/max\/1228\/1*A9d4SEX0t_bAAPzZeVqwAQ.png)","93c70cce":"**Impute missing values**","ee4de50a":"# Load","38017816":"**Remove columns (features)**\n"}}