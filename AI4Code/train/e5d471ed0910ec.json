{"cell_type":{"969b3986":"code","fa1cc0e6":"code","3c8fb8a2":"code","f86308d2":"code","4390f067":"code","2eb52c27":"code","a2a1a475":"code","c6f8a869":"code","1d3ab84a":"code","deeaa641":"code","bec2f3b3":"code","d4b64deb":"code","589f00df":"code","11d1ec1d":"code","11d59fc8":"code","a6ac4cdc":"code","38bbee7f":"code","179584d3":"code","24280503":"code","4338bd00":"code","359beb11":"code","02b1e714":"code","f99a55e1":"code","a742abe7":"code","aca0bffa":"code","ea0ccf56":"markdown","bdd0defa":"markdown","c04d5fdb":"markdown","d5263140":"markdown","c1ee9723":"markdown","89267f6c":"markdown","ba24056d":"markdown"},"source":{"969b3986":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport random as rd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom glob import glob\nfrom pandas import read_csv\nfrom PIL import Image as pil_image\n%matplotlib inline","fa1cc0e6":"# \u5c55\u793a\u4ea4\u901a\u4fe1\u53f7\u724c\ndef show_sign(imgs, tags, per_row=2):\n    n    = len(imgs)\n    rows = (n + per_row - 1)\/\/per_row\n    cols = min(per_row, n)\n    fig, axes = plt.subplots(rows,cols, figsize=(24\/\/per_row*cols,24\/\/per_row*rows))\n    for ax in axes.flatten(): ax.axis('off')\n    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): \n        ax.imshow(img.convert('RGB'))\n        ax.set_title(tags[i])","3c8fb8a2":"# glob\u51fd\u6570\u5339\u914d\u6587\u4ef6 \u8fd9\u91cc\u662f\u83b7\u53d6\u6240\u6709\u7684\u6570\u636e\u96c6\ntrainlist = glob('..\/input\/traffic-lights-identify\/data\/train\/*\/*')\n\n### \u968f\u673a\u5c55\u793a100\u5f20\u8bad\u7ec3\u96c6\u56fe\u7247\nexample = [rd.randint(0, len(trainlist)-1) for _ in range(100)]\nimgs = [pil_image.open(trainlist[i]) for i in example]\n\ndef tag(i):\n    m = trainlist[i].split('\/')[-2]\n    return str(int(m))\ntags = [tag(i) for i in example]\n\nshow_sign(imgs, tags, per_row=10)\n\n","f86308d2":"from prettytable import PrettyTable\n\n#\u5c06signName\u548c\u7d22\u5f15\u6253\u6210\u4e00\u4e2a\u5b57\u5178\nsign_names = dict([(p,w) for _,p,w in read_csv('..\/input\/traffic-lights-identify\/data\/signnames.csv').to_records()])\n#\u521b\u5efa\u4e00\u4e2aPrettyTable\u5bf9\u8c61\nsign_name_table = PrettyTable()\n#\u6dfb\u52a0PrettyTable\u5b57\u6bb5\u540d\u79f0\nsign_name_table.field_names = ['class value', 'Name of Traffic sign']\n#\u6dfb\u52a0\u5b57\u5178\u8fdb\u5165PrettyTable\u4e2d\nfor index,sign in sign_names.items():\n    sign_name_table.add_row([index,sign])\nsign_name_table","4390f067":"SIZE = (32,32)\nn_class = len(sign_names)\n\n### \u5bfc\u5165\u8bad\u7ec3\u96c6\ntrain = {}\ntrain['features'] = []\ntrain['label'] = []\n\nprint('Loading the training set:')\nfor sign in tqdm(range(n_class)):\n    path = '..\/input\/traffic-lights-identify\/\/data\/train\/' + str(sign).zfill(5) + '\/*'\n    for i in glob(path):\n        img = pil_image.open(i)\n        img = img.convert('L')  # \u5c06\u56fe\u50cf\u8f6c\u53d8\u4e3a\u7070\u5ea6\u56fe\n        train['features'].append(np.array(img.resize(SIZE)))\n        train['label'].append(sign)\ntrain['label'] = np.array(train['label'])\n\n### \u5bfc\u5165\u6d4b\u8bd5\u96c6\ntest = {}\ntest['features'] = []\ntest['label'] = []\n\nprint('Loading the testing set:')\npath = '..\/input\/traffic-lights-identify\/data\/test\/*'\nfor i in tqdm(glob(path)):\n    img = pil_image.open(i)\n    img = img.convert('L')\n    test['features'].append(np.array(img.resize(SIZE)))\n\nn_train, n_test = len(train['features']), len(test['features'])\n\n### \u5bf9\u6570\u636e\u7279\u5f81\u8fdb\u884c\u7ef4\u5ea6\u5339\u914d\u4ee5\u4fbf\u5bfc\u5165\u795e\u7ecf\u7f51\u7edc\ntrain['features'] = np.expand_dims(train['features'], axis=-1)\ntest['features'] = np.expand_dims(test['features'], axis=-1)\n\nprint('Total number of classes:{}'.format(n_class))\nprint('Number of training examples =',n_train)\nprint('Number of testing examples =',n_test)\nprint('Image data shape=',train['features'].shape[1:])","2eb52c27":"def get_count_imgs_per_class(y, verbose=False):\n    num_classes = len(np.unique(y))\n    count_imgs_per_class = np.zeros(num_classes)\n\n    for this_class in range(num_classes):\n        if verbose: \n            print('class {} | count {}'.format(this_class, np.sum(y==this_class)))\n        count_imgs_per_class[this_class] = np.sum(y==this_class)\n    return count_imgs_per_class\n\nclass_freq = get_count_imgs_per_class(train['label'])\nprint('------- ')\nprint('Highest count: {} (class {})'.format(int(np.max(class_freq)), np.argmax(class_freq)))\nprint('Lowest count: {} (class {})'.format(int(np.min(class_freq)), np.argmin(class_freq)))\nprint('------- ')\nplt.bar(np.arange(n_class), class_freq , align='center')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.xlim([-1, n_class])\nplt.title(\"class frequency in Training set\")\nplt.show()","a2a1a475":"from skimage import transform as transf\n\n### \u4eff\u5c04\u53d8\u6362\u51fd\u6570\ndef random_transform(img,angle_range=[-10,10],\n                    scale_range=[0.8,1.2],\n                    translation_range=[-3,3]):\n    '''\n    The function takes an image and performs a set of random affine transformation.\n    img:original images\n    ang_range:angular range of the rotation [-15,+15] deg for example\n    scale_range: [0.8,1.2]\n    shear_range:[10,-10]\n    translation_range:[-2,2]\n    '''\n    img_height,img_width,img_depth = img.shape\n    # Generate random parameter values\n    angle_value = np.random.uniform(low=angle_range[0],high=angle_range[1],size=None)\n    scaleX = np.random.uniform(low=scale_range[0],high=scale_range[1],size=None)\n    scaleY = np.random.uniform(low=scale_range[0],high=scale_range[1],size=None)\n    translationX = np.random.randint(low=translation_range[0],high=translation_range[1]+1,size=None)\n    translationY = np.random.randint(low=translation_range[0],high=translation_range[1]+1,size=None)\n\n    center_shift = np.array([img_height,img_width])\/2. - 0.5\n    transform_center = transf.SimilarityTransform(translation=-center_shift)\n    transform_uncenter = transf.SimilarityTransform(translation=center_shift)\n\n    transform_aug = transf.AffineTransform(rotation=np.deg2rad(angle_value),\n                                          scale=(1\/scaleY,1\/scaleX),\n                                          translation = (translationY,translationX))\n    #Image transformation : includes rotation ,shear,translation,zoom\n    full_tranform = transform_center + transform_aug + transform_uncenter\n    new_img = transf.warp(img,full_tranform,preserve_range=True)\n\n    return new_img.astype('uint8')\n\n### \u6570\u636e\u589e\u5f3a\u51fd\u6570\ndef data_augmentation(X_dataset,y_dataset,augm_nbr,keep_dist=True):\n    '''\n    X_dataset:image dataset to augment\n    y_dataset:label dataset\n    keep_dist - True:keep class distribution of original dataset,\n                False:balance dataset\n    augm_param - is the augmentation parameter\n                if keep_dist is True,increase the dataset by the factor 'augm_nbr' (2x,5x or 10x...)\n                if keep_dist is False,make all classes have same number of images:'augm_nbr'(2500,3000 or 4000 imgs)\n    '''\n    X_train_dtype = X_dataset\n    n_classes = len(np.unique(y_dataset))\n    _,img_height,img_width,img_depth = X_dataset.shape\n    class_freq = get_count_imgs_per_class(y_train)\n\n    if keep_dist:\n        extra_imgs_per_class = np.array([augm_nbr*x for x in get_count_imgs_per_class(y_dataset)])\n    else:\n        assert (augm_nbr>np.argmax(class_freq)),'augm_nbr must be larger than the height class count'\n        extra_imgs_per_class = augm_nbr - get_count_imgs_per_class(y_dataset)\n\n    total_extra_imgs = np.sum(extra_imgs_per_class)\n\n    #if extra data is needed->run the dataaumentation op\n    if total_extra_imgs > 0:\n        X_extra = np.zeros((int(total_extra_imgs),img_height,img_width,img_depth),dtype=X_dataset.dtype)\n        y_extra = np.zeros(int(total_extra_imgs))\n        start_idx = 0\n        #print('start data augmentation.....')\n        for this_class in range(n_classes):\n            #print('\\t Class {}|Number of extra imgs{}'.format(this_class,int(extra_imgs_per_class[this_class])))\n            n_extra_imgs = extra_imgs_per_class[this_class]\n            end_idx = start_idx + n_extra_imgs\n\n            if n_extra_imgs > 0:\n                #get ids of all images belonging to this_class\n                all_imgs_id = np.argwhere(y_dataset==this_class)\n                new_imgs_x = np.zeros((int(n_extra_imgs),img_height,img_width,img_depth))\n\n                for k in range(int(n_extra_imgs)):\n                    #randomly pick an original image belonging to this class\n                    rand_id = np.random.choice(all_imgs_id[0],size=None,replace=True)\n                    rand_img = X_dataset[rand_id]\n                    #Transform image\n                    new_img = random_transform(rand_img)\n                    new_imgs_x[k,:,:,:] = new_img\n                #update tensors with new images and associated labels\n                X_extra[int(start_idx):int(end_idx)] = new_imgs_x\n                y_extra[int(start_idx):int(end_idx)] = np.ones((int(n_extra_imgs),))*this_class\n                start_idx = end_idx\n        return [X_extra,y_extra]\n    else:\n        return [None,None]","c6f8a869":"# TODO: import train_test_split from sklearn\nfrom sklearn.model_selection import train_test_split\n\n# TODO: split the data into training and validation subsets\nX_train, X_valid, y_train, y_valid= train_test_split(train['features'], train['label'], test_size=0.2, random_state=42)\n\nprint('*** Before data augmentation:')\nprint('Train set size:{}|Validation set size:{}\\n'.format(X_train.shape[0],X_valid.shape[0]))\n\nX_extra,y_extra = data_augmentation(X_train,y_train,augm_nbr=2000,keep_dist=False)\n\nif X_extra is not None:\n    X_train = np.concatenate((X_train,X_extra.astype('uint8')),axis=0)\n    y_train = np.concatenate((y_train,y_extra),axis=0)\n    del X_extra,y_extra\n\nprint('*** After data augmentation:')\nprint('Train set size:{}|Validation set size:{}\\n'.format(X_train.shape[0],X_valid.shape[0]))\n\nwith mpl.rc_context(rc={'font.family': 'serif', 'font.size': 11}):\n    fig = plt.figure(figsize=(9.5,3.5))\n    ax1 = fig.add_subplot(121)\n    plt.bar(np.arange(n_class),get_count_imgs_per_class(y_train),align='center')\n    ax1.set_xlabel('Class')\n    ax1.set_ylabel('Frequency')\n    ax1.set_title('Training Set')\n    plt.xlim([-1,43])\n    ax2 = fig.add_subplot(122)\n    plt.bar(np.arange(n_class),get_count_imgs_per_class(y_valid),align='center')\n    ax2.set_xlabel('Class')\n    ax2.set_ylabel('Frequency')\n    ax2.set_title('Validation Set')\n    plt.xlim([-1,43])","1d3ab84a":"#\u5728\u5c06\u6570\u636e\u5bfc\u5165\u795e\u7ecf\u7f51\u7edc\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u56fe\u50cf\u6570\u7ec4\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\u3002\u540c\u65f6\u6211\u4eec\u9700\u8981\u5c06\u6807\u7b7e\u8fdb\u884c\u72ec\u70ed\u7f16\u7801\u3002\ndef preprocessed(dataset):\n    n_imgs,img_height,img_width,_ = dataset.shape\n    processed_dataset = np.zeros((n_imgs,img_height,img_width,1))\n    for i in range(len(dataset)):\n        img = dataset[i]\n        processed_dataset[i,:,:,:] = img\/255.-0.5\n    return processed_dataset\n\nX_train, X_valid = preprocessed(X_train), preprocessed(X_valid)","deeaa641":"X_train.shape","bec2f3b3":"X_train[1].shape","d4b64deb":"import torch\n\n#Dataset\u662f\u4e00\u4e2a\u7c7b DataLoader\u662f\u4e00\u4e2a\u5c01\u88c5\nfrom torch.utils.data import Dataset, DataLoader","589f00df":"#\u5c01\u88c5\u4ea4\u901a\u6570\u636e\u96c6\nclass TrafficDataset(Dataset): # from torch.utils.data import Dataset\n    def __init__(self, data, label):\n        self.data = data\n        self.label = label\n        \n    def __getitem__(self, index):\n        img = self.data[index]\n        img = torch.Tensor(img)\n        #\u9700\u8981\u628a\u6700\u540e\u4e00\u4f4d\uff08\u8868\u793a\u901a\u9053\u6570\uff09\u632a\u5230\u6570\u7ec4\u7684\u7b2c\u4e00\u4f4d\u53bb\n        #permute\u4f5c\u7528\u662f\u6539\u53d8Tensor\u7684\u7ef4\u5ea6\n        img = img.permute(2,0,1).type(torch.FloatTensor)\n        \n        if self.label is not None:\n            target =  int(self.label[index])\n        else:\n            target =  100\n        \n        return img,target\n\n    def __len__(self):\n        return self.data.shape[0]","11d1ec1d":"bs = 256\n\n#shuffle\u8868\u793a\u662f\u5426\u6253\u4e71\u6570\u636e\ntrain_loader = DataLoader(TrafficDataset(X_train, y_train), batch_size=bs, shuffle=True, num_workers=0)\nvalid_loader = DataLoader(TrafficDataset(X_valid, y_valid), batch_size=bs, shuffle=True, num_workers=0)\ntest_loader  = DataLoader(TrafficDataset(test['features'], None), batch_size=bs, shuffle=False, num_workers=0)","11d59fc8":"for x,y in train_loader:\n    break","a6ac4cdc":"y","38bbee7f":"import torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F","179584d3":"# CNN\u6a21\u578b\u4e3b\u9898 \u4e0a\u9762\u5b9a\u4e49\u65b9\u6cd5 \u4e0b\u9762\u76f4\u63a5\u8fdb\u884c\u8c03\u7528\nclass TrafficNet(nn.Module):\n    def __init__(self, num_classes):\n        #\u8fd9\u91cc\u6ce8\u610f\u4e00\u5b9a\u4e00\u5b9a\u8981\u6dfb\u52a0\u8fd9\u4e2a\uff01\uff01\u6ce8\u610f\u7ee7\u627f\u89c4\u5219\n        super().__init__()\n        self.num_classes = num_classes\n        \n        self.conv0 = nn.Conv2d(1,6,5)\n        self.pool0 = nn.MaxPool2d(2)\n        \n        self.conv1 = nn.Conv2d(6,16,5)\n        self.pool1 = nn.MaxPool2d(2)\n        \n        self.conv2 = nn.Conv2d(16,400,5)\n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(800,num_classes)\n        \n        #\u4e0a\u9762\u8fd9\u4e24\u5c42\u53ef\u4ee5\u5199\u6210\u8fd9\u6837   Sequential\u8868\u793a\u4e00\u4e2a\u5e8f\u5217\n        #self.fc = nn.Sequential(nn.Dropout(0.2),nn.Linear(800,num_classes))\n        \n        #\u6ce8\u610f\uff1aPytorch\u4e2d\u6700\u540e\u4e00\u5c42\u7684\u6fc0\u6d3b\u51fd\u6570\u5185\u542b\u5728\u635f\u5931\u51fd\u6570\u4e2d \u4e0d\u80fd\u518d\u5728\u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a\u6fc0\u6d3b\u51fd\u6570\n        \n        \n    #\u4e0a\u9762\u5b9a\u4e49\u5377\u79ef \u4e0b\u9762\u6765\u8c03\u7528\n    def forward(self, x):\n        x = self.conv0(x)\n        x = torch.relu(x)\n        x = self.pool0(x)\n        x = self.conv1(x)\n        x = torch.relu(x)\n        x1 = self.pool1(x)\n        x2 = self.conv2(x1)\n        x3 = torch.relu(x2)\n        \n        x4 = torch.flatten(x1,start_dim = 1)\n        x5 = torch.flatten(x3,start_dim = 1)\n        \n        x6 = torch.cat((x4,x5),dim = 1)\n        \n        x = self.fc(x6)\n        return x","24280503":"model = TrafficNet(num_classes=n_class)","4338bd00":"if not os.path.exists('models'):\n    os.makedirs('models') # \u7528\u4e8e\u50a8\u5b58\u6a21\u578b\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","359beb11":"criterion = nn.CrossEntropyLoss()\nmodel = model.to(device)\n\nlr = 0.001\noptimizer = optim.Adam(model.parameters(), lr=lr)","02b1e714":"#Pytorch \u6807\u51c6\u8bad\u7ec3\u4ee3\u7801\ndef train_model(epoch, history=None):\n    model.train() \n    t = tqdm(train_loader)\n    \n    for batch_idx, (img_batch, label_batch) in enumerate(t):\n        img_batch = img_batch.to(device)\n        label_batch = label_batch.to(device)\n        \n        optimizer.zero_grad()\n        output = model(img_batch)\n        loss = criterion(output, label_batch)\n        t.set_description(f'train_loss (l={loss:.4f})')\n        \n        if history is not None:\n            history.loc[epoch + batch_idx \/ len(train_loader), 'train_loss'] = loss.data.cpu().numpy()\n        \n        loss.backward()    \n        optimizer.step()\n    \n    torch.save(model.state_dict(), 'models\/epoch{}.pth'.format(epoch))","f99a55e1":"from sklearn.metrics import accuracy_score\n\ndef evaluate(epoch, history=None): # \u9a8c\u8bc1\u51fd\u6570\n    model.eval() # \u5f00\u542f\u9a8c\u8bc1\u6a21\u5f0f\uff0c\u6b64\u65f6\u6a21\u578b\u7684\u53c2\u6570\u4e0d\u53ef\u4fee\u6539\n    valid_loss = 0.\n    all_predictions, all_targets = [], []\n    \n    with torch.no_grad():\n        for batch_idx, (img_batch, label_batch) in enumerate(valid_loader):\n            all_targets.append(label_batch.numpy().copy())\n            img_batch = img_batch.to(device)\n            label_batch = label_batch.to(device)\n\n            output = model(img_batch)\n            loss = criterion(output, label_batch)\n            valid_loss += loss.data\n            predictions = torch.argmax(torch.softmax(output, axis=-1), axis=-1)\n            all_predictions.append(predictions.cpu().numpy())\n            \n    all_predictions = np.concatenate(all_predictions)\n    all_targets = np.concatenate(all_targets)\n    \n    valid_loss \/= (batch_idx+1)\n    valid_acc = accuracy_score(all_targets, all_predictions)\n    \n    if history is not None:\n        history.loc[epoch, 'valid_loss'] = valid_loss.cpu().numpy()\n    \n    print('Epoch: {}\\tLR: {:.6f}\\tValid Loss: {:.4f}\\tValid Acc: {:.4f}'.format(\n        epoch, optimizer.state_dict()['param_groups'][0]['lr'], valid_loss, valid_acc))\n    \n    return valid_loss, valid_acc","a742abe7":"history_train = pd.DataFrame()\nhistory_valid = pd.DataFrame()\n\nn_epochs = 100\ninit_epoch = 0\nmax_lr_changes = 2\nvalid_losses = []\nvalid_accs = []\nlrs = []\nlr_reset_epoch = init_epoch\npatience = 3\nlr_changes = 0\nbest_valid_loss = 1000.\n\nfor epoch in range(init_epoch, n_epochs):\n    torch.cuda.empty_cache()\n    gc.collect()\n    train_model(epoch, history_train)\n    valid_loss, valid_acc = evaluate(epoch, history_valid)\n    valid_losses.append(valid_loss)\n    valid_accs.append(valid_acc)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n    elif (patience and epoch - lr_reset_epoch > patience and\n          min(valid_losses[-patience:]) > best_valid_loss):\n        # \"patience\" epochs without improvement\n        lr_changes +=1\n        if lr_changes > max_lr_changes: # \u65e9\u671f\u505c\u6b62\n            break\n        lr \/= 5 # \u5b66\u4e60\u7387\u8870\u51cf\n        print(f'lr updated to {lr}')\n        lr_reset_epoch = epoch\n        optimizer.param_groups[0]['lr'] = lr\n    \n    lrs.append(optimizer.param_groups[0]['lr'])","aca0bffa":"# \u7ed8\u5236\u635f\u5931\u51fd\u6570\u3001\u7cbe\u786e\u7387\u4e0e\u5b66\u4e60\u901f\u7387\u7684\u53d8\u5316\u66f2\u7ebf\nwith mpl.rc_context(rc={'font.family': 'serif', 'font.size': 11}):\n    fig = plt.figure(figsize=(20,5))\n    ax1 = fig.add_subplot(131)\n    ax1.set_xlabel('Epoch')\n    ax1.set_title('Loss')\n    plt.plot(history_train.index, history_train['train_loss'])\n    plt.plot(history_valid.index, history_valid['valid_loss'])\n    plt.legend(['train', 'valid'], loc='upper right')\n    ax2 = fig.add_subplot(132)\n    ax2.set_xlabel('Epoch')\n    ax2.set_title('Accuracy')\n    plt.plot(valid_accs)\n    plt.legend(['valid'], loc='lower right')\n    ax3 = fig.add_subplot(133)\n    ax3.set_xlabel('Epoch')\n    ax3.set_title('Learning Rate')\n    plt.plot(lrs)","ea0ccf56":"## \u4ea4\u901a\u4fe1\u53f7\u724c\u7684\u8bc6\u522b\n","bdd0defa":"## \u5bfc\u5165\u6570\u636e\u96c6","c04d5fdb":"#### \u8fd9\u4e00\u90e8\u5206\u4e00\u5b9a\u8981\u81ea\u5df1\u5199","d5263140":"### \u6a21\u578b\u7684\u8bad\u7ec3","c1ee9723":"#### \u6570\u636e\u589e\u5f3a\n\u8fd9\u91cc\u7684\u6570\u636e\u589e\u5f3a\u4e3b\u8981\u662f\uff1a\n- \u589e\u52a0\u8bad\u7ec3\u96c6\u7684\u5927\u5c0f \n- \u8c03\u6574\u4e86\u7c7b\u522b\u5206\u5e03\uff08\u7531\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\u7c7b\u522b\u5206\u5e03\u662f\u4e0d\u5747\u8861\u7684\uff09 ","89267f6c":"\u56fe\u50cf\u6570\u636e\u683c\u5f0f\u4e3a ppm\uff0c\u65e0\u6cd5\u76f4\u63a5\u9884\u89c8\uff0c\u6211\u4eec\u4f7f\u7528 PIL \u5e93\u4e2d\u7684 Image \u51fd\u6570\u5bf9 ppm \u683c\u5f0f\u56fe\u50cf\u8fdb\u884c\u5904\u7406\u3002","ba24056d":"## CNN\u6a21\u578b\u6784\u5efa\n#### \u6570\u636e\u7684\u9884\u5904\u7406"}}