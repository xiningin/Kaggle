{"cell_type":{"d0faca06":"code","5e2babb4":"code","a7f634a5":"code","86fd911d":"code","8f93494a":"code","ce9947ae":"code","7d3afa5c":"code","1a9ca8bb":"code","92435200":"code","9a92f660":"code","eec2b83d":"code","cf01b351":"code","53b2165f":"code","a4a4c5e7":"code","082f4caa":"code","442b7ad2":"code","d0caeb5b":"code","d836f1c1":"code","f212b498":"code","f06a6bae":"code","be2bff34":"code","58c5d27a":"code","9c358096":"code","018b42f0":"code","ea9924df":"code","6ba319cb":"code","e1a7f7ea":"code","24f3ace4":"code","34fe83fe":"code","a3cdd4a4":"code","84922269":"code","3e5709d3":"code","40ecb30f":"code","83df09b4":"code","859fca95":"code","810be25c":"code","e0d5204a":"code","d1183d4c":"markdown","d29d2958":"markdown","0c685a90":"markdown","9c59fbc9":"markdown","23fca6fa":"markdown","49ef5337":"markdown","91d4312e":"markdown","3bc64657":"markdown","aeab7aaa":"markdown","aeadc1d8":"markdown","e885fb63":"markdown","d9a0011e":"markdown","8162924d":"markdown","870465ff":"markdown","363167d3":"markdown","eb79174a":"markdown","bb680491":"markdown","9124fe1d":"markdown","4e8aaf21":"markdown","e3dc85d1":"markdown","f322d656":"markdown","180bd9ea":"markdown"},"source":{"d0faca06":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5e2babb4":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n    '''\n    The function does not return, but transforms the input pd.DataFrame\n    \n    Encodes the Costa Rican Household Poverty Level data \n    following studies in https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-in-the-data\n    and the insight from https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403#359631\n    \n    The following columns get transformed: edjefe, edjefa, dependency, idhogar\n    The user most likely will simply drop idhogar completely (after calculating houshold-level aggregates)\n    '''\n    \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])","a7f634a5":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('bed_density', 'bedrooms', 'rooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 ('male_over_female', 'r4h3', 'r4m3'),\n                 ('man12plus_over_women12plus', 'r4h2', 'r4m2'),\n                 ('pesioner_over_working', 'hogar_mayor', 'hogar_adul'),\n                 ('children_over_working', 'hogar_nin', 'hogar_adul'),\n                 ('education_fraction', 'escolari', 'age')\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('non_bedrooms', 'rooms', 'bedrooms'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean', 'count'],\n                'escolari': ['min', 'max', 'mean', 'std'],\n                'fe_education_fraction': ['min', 'max', 'mean', 'std']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df","86fd911d":"def convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df","8f93494a":"train = pd.read_csv('..\/input\/train.csv', nrows=None)\ntest = pd.read_csv('..\/input\/test.csv')","ce9947ae":"train.info()","7d3afa5c":"def process_df(df_):\n    # fix categorical features\n    encode_data(df_)\n    #fill in missing values based on https:\/\/www.kaggle.com\/mlisovyi\/missing-values-in-the-data\n    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n        df_[f_] = df_[f_].fillna(0)\n    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n    # do feature engineering and drop useless columns\n    return do_features(df_)\n\ntrain = process_df(train)\ntest = process_df(test)","1a9ca8bb":"train.info()","92435200":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","9a92f660":"train, test = train_test_apply_func(train, test, convert_OHE2LE)","eec2b83d":"train.info()","cf01b351":"cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\ncols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\ndef convert_geo2aggs(df_):\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e + '_MEAN' for e in geo_agg.columns.tolist()])\n    \n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\ntrain, test = train_test_apply_func(train, test, convert_geo2aggs)","53b2165f":"train.info()","a4a4c5e7":"X = train.query('parentesco1==1')\n#X = train\n\n# pull out the target variable\ny = X['Target'] - 1\nX = X.drop(['Target'], axis=1)","082f4caa":"cols_2_drop = ['abastagua_LE', 'agg18_estadocivil1_MEAN', 'agg18_instlevel6_MEAN', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_MEAN', 'agg18_parentesco4_MEAN', 'agg18_parentesco5_MEAN', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_MEAN', 'agg18_parentesco9_MEAN', 'fe_people_not_living', 'fe_people_weird_stat', 'geo_elimbasu_LE_3_MEAN', 'geo_elimbasu_LE_4_MEAN', 'geo_energcocinar_LE_0_MEAN', 'geo_energcocinar_LE_1_MEAN', 'geo_energcocinar_LE_2_MEAN', 'geo_epared_LE_0_MEAN', 'geo_epared_LE_2_MEAN', 'geo_etecho_LE_2_MEAN', 'geo_eviv_LE_0_MEAN', 'geo_hogar_mayor_MEAN', 'geo_hogar_nin_MEAN', 'geo_manual_elec_LE_1_MEAN', 'geo_manual_elec_LE_2_MEAN', 'geo_manual_elec_LE_3_MEAN', 'geo_pared_LE_0_MEAN', 'geo_pared_LE_1_MEAN', 'geo_pared_LE_3_MEAN', 'geo_pared_LE_4_MEAN', 'geo_pared_LE_5_MEAN', 'geo_pared_LE_6_MEAN', 'geo_pared_LE_7_MEAN', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'techo_LE', 'v14a', 'v18q']\n#cols_2_drop = ['agg18_estadocivil1_MEAN', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_MEAN', 'agg18_parentesco4_MEAN', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_MEAN', 'fe_people_weird_stat', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'v14a']\n#cols_2_drop=[]\n\nX.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)\ntest.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)","442b7ad2":"XY = pd.concat([X,y], axis=1)\nmax_corr = XY.corr()['Target'].loc[lambda x: abs(x)>0.2].index\n#min_corr = XY.corr()['Target'].loc[lambda x: abs(x)<0.05].index","d0caeb5b":"_ = plt.figure(figsize=(10,7))\n_ = sns.heatmap(XY[max_corr].corr(), vmin=-0.5, vmax=0.5, cmap='coolwarm')","d836f1c1":"from sklearn.metrics import f1_score\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return max(lr, min_learning_rate)\n\nimport lightgbm as lgb\nfit_params={\"early_stopping_rounds\":300, \n            \"eval_metric\" : 'multiclass',\n            \"eval_metric\" : evaluate_macroF1_lgb, \n            #\"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n            'eval_names': ['train', 'early_stop'],\n            'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_power_0997)],\n            'verbose': False,\n            'categorical_feature': 'auto'}\n\n#fit_params['verbose'] = 200","f212b498":"#v8\n#opt_parameters = {'colsample_bytree': 0.93, 'min_child_samples': 56, 'num_leaves': 19, 'subsample': 0.84}\n#v9\n#opt_parameters = {'colsample_bytree': 0.89, 'min_child_samples': 70, 'num_leaves': 17, 'subsample': 0.96}\n#v14\n#opt_parameters = {'colsample_bytree': 0.88, 'min_child_samples': 90, 'num_leaves': 16, 'subsample': 0.94}\n#v17\nopt_parameters = {'colsample_bytree': 0.89, 'min_child_samples': 90, 'num_leaves': 14, 'subsample': 0.96}","f06a6bae":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.validation import has_fit_parameter, check_is_fitted\n\nclass VotingPrefitClassifier(VotingClassifier):\n    '''\n    This implements the VotingClassifier with prefitted classifiers\n    '''\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        self.estimators_ = [x[1] for x in self.estimators]\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        \n        return self    ","be2bff34":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\ndef train_single_model(clf_, X_, y_, random_state_=314, opt_parameters_={}, fit_params_={}):\n    c = clone(clf_)\n    c.set_params(**opt_parameters_)\n    c.set_params(random_state=random_state_)\n    return c.fit(X_, y_, **fit_params_)\n\ndef train_model_in_nestedCV(model, X, y, metric, metric_args={},\n                            model_name='xmodel',\n                            inner_seed=31416, inner_n=10, outer_seed=314, outer_n=10,\n                            opt_parameters_={}, fit_params_={},\n                            verbose=True):\n    # the list of classifiers for voting ensable\n    clfs = []\n    # performance \n    perf_eval = {'score_i_oof': [],\n                 'score_i_ave': [],\n                 'score_i_std': [],\n                 'score_i_early_stop_ave': [],\n                 'score_o_early_stop': [],\n                 'score_o_early_stop_vc_w0_soft': [],\n                 'score_o_early_stop_vc_w0_hard': []\n                }\n    # full-sample oof prediction\n    y_full_oof = pd.Series(np.zeros(shape=(y.shape[0],)), \n                          index=y.index)\n    \n    if 'sample_weight' in metric_args:\n        sample_weight=metric_args['sample_weight']\n\n    outer_cv = StratifiedKFold(outer_n, shuffle=True, random_state=outer_seed)\n    for n_outer_fold, (outer_trn_idx, outer_val_idx) in enumerate(outer_cv.split(X,y)):\n        print('--- Outer loop iteration: {} ---'.format(n_outer_fold))\n        X_out, y_out = X.iloc[outer_trn_idx], y.iloc[outer_trn_idx]\n        X_stp, y_stp = X.iloc[outer_val_idx], y.iloc[outer_val_idx]\n\n        inner_cv = StratifiedKFold(inner_n, shuffle=True, random_state=inner_seed+n_outer_fold)\n        # The out-of-fold (oof) prediction for the k-1 sample in the outer CV loop\n        y_outer_oof = pd.Series(np.zeros(shape=(X_out.shape[0],)), \n                          index=X_out.index)\n        scores_inner = []\n        clfs_inner = []\n\n        for n_inner_fold, (inner_trn_idx, inner_val_idx) in enumerate(inner_cv.split(X_out,y_out)):\n            X_trn, y_trn = X_out.iloc[inner_trn_idx], y_out.iloc[inner_trn_idx]\n            X_val, y_val = X_out.iloc[inner_val_idx], y_out.iloc[inner_val_idx]\n\n            if fit_params_:\n                # use _stp data for early stopping\n                fit_params_[\"eval_set\"] = [(X_trn,y_trn), (X_stp,y_stp)]\n                fit_params_['verbose'] = False\n\n            clf = train_single_model(model, X_trn, y_trn, 314+n_inner_fold, opt_parameters_, fit_params_)\n\n            clfs_inner.append(('{}{}_inner'.format(model_name,n_inner_fold), clf))\n            # evaluate performance\n            y_outer_oof.iloc[inner_val_idx] = clf.predict(X_val)\n            if 'sample_weight' in metric_args:\n                metric_args['sample_weight'] = y_val.map(sample_weight)\n            scores_inner.append(metric(y_val, y_outer_oof.iloc[inner_val_idx], **metric_args))\n            #cleanup\n            del X_trn, y_trn, X_val, y_val\n\n        # Store performance info for this outer fold\n        if 'sample_weight' in metric_args:\n            metric_args['sample_weight'] = y_outer_oof.map(sample_weight)\n        perf_eval['score_i_oof'].append(metric(y_out, y_outer_oof, **metric_args))\n        perf_eval['score_i_ave'].append(np.mean(scores_inner))\n        perf_eval['score_i_std'].append(np.std(scores_inner))\n        \n        # Do the predictions for early-stop sub-sample for comparison with VotingPrefitClassifier\n        if 'sample_weight' in metric_args:\n            metric_args['sample_weight'] = y_stp.map(sample_weight)\n        score_inner_early_stop = [metric(y_stp, clf_.predict(X_stp), **metric_args)\n                                   for _,clf_ in clfs_inner]\n        perf_eval['score_i_early_stop_ave'].append(np.mean(score_inner_early_stop))\n        \n        # Record performance of Voting classifiers\n        w = np.array(scores_inner)\n        for w_, w_name_ in [(None, '_w0')#,\n                            #(w\/w.sum(), '_w1'),\n                            #((w**2)\/np.sum(w**2), '_w2')\n                           ]:\n            vc = VotingPrefitClassifier(clfs_inner, weights=w_).fit(X_stp, y_stp)\n            for vote_type in ['soft', 'hard']:\n                vc.voting = vote_type\n                if 'sample_weight' in metric_args:\n                    metric_args['sample_weight'] = y_stp.map(sample_weight)\n                perf_eval['score_o_early_stop_vc{}_{}'.format(w_name_, vote_type)].append(metric(y_stp, vc.predict(X_stp), **metric_args))\n\n        if fit_params_:\n            # Train main model for the voting average\n            fit_params_[\"eval_set\"] = [(X_out,y_out), (X_stp,y_stp)]\n            if verbose:\n                fit_params_['verbose'] = 200\n        #print('Fit the final model on the outer loop iteration: ')\n        clf = train_single_model(model, X_out, y_out, 314+n_outer_fold, opt_parameters_, fit_params_)\n        if 'sample_weight' in metric_args:\n            metric_args['sample_weight'] = y_stp.map(sample_weight)\n        perf_eval['score_o_early_stop'].append(metric(y_stp, clf.predict(X_stp), **metric_args))\n        clfs.append(('{}{}'.format(model_name,n_outer_fold), clf))\n        y_full_oof.iloc[outer_val_idx] = clf.predict(X_stp)\n        # cleanup\n        del inner_cv, X_out, y_out, X_stp, y_stp, clfs_inner\n\n    return clfs, perf_eval, y_full_oof\n\ndef print_nested_perf_clf(name, perf_eval):\n    print('Performance of the inner-loop model (the two should agree):')\n    print('  Mean(mean(Val)) score inner {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                      np.mean(perf_eval['score_i_ave']),\n                                                                      np.std(perf_eval['score_i_ave'])\n                                                                     ))\n    print('  Mean(mean(EarlyStop)) score inner {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                      np.mean(perf_eval['score_i_early_stop_ave']),\n                                                                      np.std(perf_eval['score_i_early_stop_ave'])\n                                                                     ))\n    print('Mean(inner OOF) score inner {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                       np.mean(perf_eval['score_i_oof']), \n                                                                       np.std(perf_eval['score_i_oof'])\n                                                                      ))\n    print('Mean(EarlyStop) score outer {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                      np.mean(perf_eval['score_o_early_stop']),\n                                                                      np.std(perf_eval['score_o_early_stop'])\n                                                                     ))\n    print('Mean(EarlyStop) outer VotingPrefit SOFT: {:.4f}+-{:.4f}'.format(np.mean(perf_eval['score_o_early_stop_vc_w0_soft']),\n                                                                           np.std(perf_eval['score_o_early_stop_vc_w0_soft'])                                                                    \n                                                                    ))\n    print('Mean(EarlyStop) outer VotingPrefit HARD: {:.4f}+-{:.4f}'.format(np.mean(perf_eval['score_o_early_stop_vc_w0_hard']),\n                                                                           np.std(perf_eval['score_o_early_stop_vc_w0_hard'])\n                                                                    ))","58c5d27a":"clf  = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                         random_state=1, silent=True, metric='None', \n                         n_jobs=4, n_estimators=5000, class_weight='balanced')\n\nclfs_, perf_eval, y_full_oof = train_model_in_nestedCV(clf, X, y, f1_score, \n                                                      metric_args={'average':'macro'},\n                                                      model_name='lgbm', \n                                                      opt_parameters_=opt_parameters,\n                                                      fit_params_=fit_params, \n                                                      inner_n=10, outer_n=10,\n                                                      verbose=False)","9c358096":"w = np.array(perf_eval['score_o_early_stop'])\nws = [(None, '_w0'),\n  (w\/w.sum(), '_w1'),\n  ((w**2)\/np.sum(w**2), '_w2')\n ]\nvc = {}\nfor w_, w_name_ in ws:\n    vc['vc{}'.format(w_name_)] = VotingPrefitClassifier(clfs_, weights=w_).fit(X, y)\nclf_final = clfs_[0][1]","018b42f0":"global_score = np.mean(perf_eval['score_i_oof'])\nglobal_score_std = np.std(perf_eval['score_i_oof'])\n\nprint_nested_perf_clf('lgbm', perf_eval)\nprint('Outer OOF score {} Classifier: {:.4f}'.format('lgbm', f1_score(y, y_full_oof, average='macro')))","ea9924df":"perf_eval_df = pd.DataFrame(perf_eval)\nperf_eval_df","6ba319cb":"from sklearn.metrics import precision_score, recall_score, classification_report","e1a7f7ea":"#print(classification_report(y_test, clf_final.predict(X_test)))","24f3ace4":"#vc.voting = 'hard'\n#print(classification_report(y_test, vc.predict(X_test)))","34fe83fe":"#vc.voting = 'soft'\n#print(classification_report(y_test, vc.predict(X_test)))","a3cdd4a4":"def display_importances(feature_importance_df_, doWorst=False, n_feat=50):\n    # Plot feature importances\n    if not doWorst:\n        cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:n_feat].index        \n    else:\n        cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[-n_feat:].index\n    \n    mean_imp = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean()\n    df_2_neglect = mean_imp[mean_imp['importance'] < 1e-3]\n    print('The list of features with 0 importance: ')\n    print(df_2_neglect.index.values.tolist())\n    del mean_imp, df_2_neglect\n    \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n    plt.figure(figsize=(8,10))\n    sns.barplot(x=\"importance\", y=\"feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features')\n    plt.tight_layout()\n    #plt.savefig('lgbm_importances.png')\n    \nimportance_df = pd.DataFrame()\nimportance_df[\"feature\"] = X.columns.tolist()      \nimportance_df[\"importance\"] = clf_final.booster_.feature_importance('gain')\ndisplay_importances(feature_importance_df_=importance_df, n_feat=20)","84922269":"#display_importances(feature_importance_df_=importance_df, doWorst=True, n_feat=20)","3e5709d3":"import shap\nshap_values = shap.TreeExplainer(clf_final.booster_).shap_values(X)\n\n#shap_df = pd.DataFrame()\n#shap_df[\"feature\"] = X_train.columns.tolist()    \n#shap_df[\"importance\"] = np.sum(np.abs(shap_values), 0)[:-1]","40ecb30f":"#display_importances(feature_importance_df_=shap_df, n_feat=20)","83df09b4":"shap.summary_plot(shap_values, X, plot_type='bar')","859fca95":"y_subm = pd.read_csv('..\/input\/sample_submission.csv')","810be25c":"from datetime import datetime\nnow = datetime.now()\n\nsub_file = 'submission_LGB_{:.4f}_{}.csv'.format(global_score, str(now.strftime('%Y-%m-%d-%H-%M')))\ny_subm['Target'] = clf_final.predict(test) + 1\ny_subm.to_csv(sub_file, index=False)\n\n# Store predictions with voting classifiers\nfor vc_name_,vc_ in vc.items():\n    for vc_type_ in ['soft', 'hard']:\n        vc_.voting = vc_type_\n        name = '{}_{}'.format(vc_name_, vc_type_)\n        y_subm_vc = y_subm.copy(deep=True)\n        y_subm_vc.loc[:,'Target'] = vc_.predict(test) + 1\n        sub_file = 'submission_{}_LGB_{:.4f}_{}.csv'.format(name, \n                                                            global_score, \n                                                            str(now.strftime('%Y-%m-%d-%H-%M'))\n                                                           )\n        y_subm_vc.to_csv(sub_file, index=False)","e0d5204a":"!ls","d1183d4c":"# VERY IMPORTANT\n> Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.","d29d2958":"Look at the performance on invidivual folds:","0c685a90":"The following categorical mapping originates from [this kernel](https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-encoding-function).","9c59fbc9":"# Do feature engineering to improve LightGBM prediction\nThis kernel closely follows https:\/\/www.kaggle.com\/mlisovyi\/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster. \n\nSeveral key points:\n- **This kernel runs training on the heads of housholds only** (after extracting aggregates over households). This follows the announced scoring startegy: *Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.* (from the data description). \n- **It seems to be very important to balance class frequencies.** Without balancing a trained model gives ~0.39 PLB \/ ~0.43 local test, while adding balancing leads to ~0.42 PLB \/ 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set `class_weight='balanced'` in the LightGBM model constructor in sklearn API, which will assign different weights to different classes proportional to their representation. *Note that a better procedure would be to tune those weights in a CV loop instead of blindly assigning 1\/n weights*\n- **This kernel uses macro F1 score to early stopping in training**. This is done to align with the scoring strategy.\n- Categoricals are turned into numbers with proper mapping instead of blind label encoding. \n- **OHE is reversed into label encoding, as it is easier to digest for a tree model.** This trick would be harmful for non-tree models, so be careful.\n- **idhogar is NOT used in training**. The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :)\n- **Squared features (`SQBXXX` and `agesq`) are NOT used in training**. These would be useful for a linear model, but are useless for a tree-based model and only confused it (when bagging and resampling is done)\n- **There are aggregations done within households and new features are hand-crafted**. Note, that there are not so many features that can be aggregated, as most are already quoted on household level.\n- **NEW: There are geographical aggregates calculated from households**\n- **NEW: Models are build and evaluated in a nested CV loop**. This is done to reduce fluctuations in early-stopping criterion as well as to average over several performance estimates.\n- **A voting classifier is used to average over several LightGBM models**. This allows to get better predictions by bagging\/averaging.\n\nThe main goal is to do feature engineering","23fca6fa":"# Prepare submission","49ef5337":"Define helper functions","91d4312e":"# Plot feature importances (using SHAP)\nSee if added features show among most significant ones","3bc64657":"# F1 score across different classes\nLet's see if all classes show similar performance","aeab7aaa":"# LightGBM optimal parameters\n\nThe parameters are optimised with a random search in this kernel: https:\/\/www.kaggle.com\/mlisovyi\/lighgbm-hyperoptimisation-with-f1-macro\n","aeadc1d8":"# Model fitting\n\nWe will use LightGBM classifier - LightGBM allows to build very sophysticated models with a very short training time.","e885fb63":"Fit individual models as well as the voting classifier in a nested CV loop","d9a0011e":"Note the change in the number of features of different type. What we did was:\n- encoded categorical variables appropreately into numerical values;\n- dropped a few irrelevant columns;\n- added several columns with household aggregates and cand-crafted ratio and subtraction features\n\nNow, let's define `train_test_apply_func` helper function to apply a custom function to a concatenated test+train dataset","8162924d":"Set up the final voting classifier","870465ff":"Compare the number of features with `int64` type to the previous info summary. The difference comes from convertion of OHE into LE (`convert_OHE2LE` function)","363167d3":"Print evaluated performance of various models","eb79174a":"## Use test subset for early stopping criterion\n\nThis allows us to avoid overtraining and we do not need to optimise the number of trees. We also use F1 macro-averaged score to decide when to stop\n","bb680491":"**There is also feature engineering magic happening here:**","9124fe1d":"# Geo aggregates","4e8aaf21":"# Plot feature importances (using gain)\nSee if added features show among most significant ones","e3dc85d1":"## Let's look on the most correlated with `Target` features","f322d656":"# Fit a voting classifier\nDefine a derived VotingClassifier class that uses pre-","180bd9ea":"# Read in the data and clean it up"}}