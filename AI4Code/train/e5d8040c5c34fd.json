{"cell_type":{"3db7e563":"code","4a23e35b":"code","83d49aad":"code","73f4083f":"code","deabc83e":"code","dc3e8942":"code","1ec1622d":"code","beb24287":"code","aa6ff3db":"code","d4314e28":"code","9eca3db1":"code","1cf3e362":"code","2fab1531":"code","ecb7fac3":"code","92240cd2":"code","2a937302":"code","3e42893f":"code","ab369484":"code","2b3c8cde":"code","13354237":"code","2f93e6e7":"code","4c332ce0":"code","016790de":"code","1faf633e":"code","7b4dd5e4":"code","bc0f5486":"code","5a75ba02":"code","49ab16aa":"code","89ac54ae":"code","12410232":"code","e018843c":"code","94e4a8e0":"code","4782ceb0":"code","6db40ab2":"code","cc1d2f93":"code","1e3eaddc":"code","df4ede61":"code","554cf5c8":"code","3dd5ded5":"code","2719895d":"code","57c03b88":"code","2016d1cc":"code","5ec229fb":"code","ce3ba5fd":"code","02bd1c36":"code","5f9b43c8":"code","c5d3c442":"code","423ba0cd":"code","399955da":"code","8d08a5c0":"code","e95d2522":"code","848afc03":"code","aa009756":"code","b45e32bf":"code","5c2287b6":"code","aca6a0fb":"code","b8bc131e":"code","ff999fe9":"code","9bd5f4d4":"code","811df3e6":"code","ee82b23f":"code","d294b66d":"markdown","18dc00b4":"markdown","5cfcb15d":"markdown","3edbf43e":"markdown","092e693c":"markdown","35f49064":"markdown","5e58e408":"markdown","3a65984f":"markdown","36dd400f":"markdown","1f3003cb":"markdown","ce215774":"markdown","4404d30a":"markdown","5ee44d94":"markdown","f223d5ec":"markdown","791be371":"markdown","d5e93e22":"markdown","5c97b4a3":"markdown","5338009b":"markdown","50322b98":"markdown","62a9a038":"markdown","1be58b14":"markdown","e3e7521b":"markdown","f4c3c8cc":"markdown","2690c62d":"markdown","95d10d7d":"markdown","14895edd":"markdown","57f25f3a":"markdown","ebcbaa09":"markdown","06a3856b":"markdown","ee79aa1d":"markdown","7dba5fc3":"markdown","460f5783":"markdown","42e505a9":"markdown","7f0d227d":"markdown","67210409":"markdown","d5ac051d":"markdown","98e9911e":"markdown","2f99ddf8":"markdown","313f7afb":"markdown","5c3ba9f2":"markdown"},"source":{"3db7e563":"from IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/riiid-answer-correctness-prediction-rapids\/custom.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\ncss_styling()\n\n# <div class=\"gradient-box\">\n# <p>I. Intro: Rapids AI \ud83c\udfc3\u200d\u2642\ufe0f <\/p>\n# <\/div>","4a23e35b":"def notebook_styling():\n    styles = open(\"..\/input\/riiid-answer-correctness-prediction-rapids\/custom_rapids.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\nnotebook_styling()","83d49aad":"%%time\n\n# Import the Rapids suite here - takes abot 1.5 mins\n\nimport sys\n!cp ..\/input\/rapids\/rapids.0.18.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","73f4083f":"# Regular Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.image as mpimg\nfrom IPython.display import display_html\nfrom PIL import Image\nimport gc\nfrom scipy.stats import pearsonr\nimport tqdm\nimport copy\nimport re\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Color Palette\ncustom_colors = ['#7400ff', '#a788e4', '#d216d2', '#ffb500', '#36c9dd']\nsns.palplot(sns.color_palette(custom_colors))\n\n# Set Style\nsns.set_style(\"whitegrid\")\nsns.despine(left=True, bottom=True)\n\n# Set tick size\nplt.rc('xtick',labelsize=12)\nplt.rc('ytick',labelsize=12)","deabc83e":"# Rapids Imports\nimport cudf\nimport cupy # CuPy is an open-source array library accelerated with NVIDIA CUDA.\n\n\nfrom dask.distributed import Client, wait\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster()\nclient = Client(cluster)\nclient","dc3e8942":"!nvidia-smi","1ec1622d":"# Imports\nimport cudf\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom cuml.ensemble import RandomForestClassifier as curfc\nfrom cuml.metrics import accuracy_score\n\nfrom sklearn.ensemble import RandomForestClassifier as skrfc\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nimport time\n\n\n# Parameters for creating some fake data\nn_samples = 2**15\nn_features = 399\nn_info = 300\ndata_type = np.float32\n\n\n# Make Data\nX,y = make_classification(n_samples=n_samples,\n                          n_features=n_features,\n                          n_informative=n_info,\n                          random_state=123, n_classes=2)\n\nX = pd.DataFrame(X.astype(data_type))\n# cuML Random Forest Classifier requires the labels to be integers\ny = pd.Series(y.astype(np.int32))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size = 0.2,\n                                                    random_state=0)\n\n# From CPU to GPU\nX_cudf_train = cudf.DataFrame.from_pandas(X_train)\nX_cudf_test = cudf.DataFrame.from_pandas(X_test)\n\ny_cudf_train = cudf.Series(y_train.values)\n\nprint(\"X shape: \", X.shape, \"\\n\" +\n      \"y shape: \", y.shape)","beb24287":"print(\"==== CPU ====\", \"\\n\")\n# ==== Fit ====\n\nstart = time.time()\nsk_model = skrfc(n_estimators=40,\n                 max_depth=16,\n                 max_features=1.0,\n                 random_state=10)\n\nsk_model.fit(X_train, y_train)\nend = time.time()\n\nprint(\"Training time: {} mins\".format(round((end-start)\/60, 1)))\n\n# ==== Evaluate ====\nstart = time.time()\nsk_predict = sk_model.predict(X_test)\nsk_acc = accuracy_score(y_test, sk_predict)\nend = time.time()\n\nprint(\"Evaluation time: {} mins\".format(round((end-start)\/60, 1)))","aa6ff3db":"print(\"==== GPU ====\", \"\\n\")\n# ==== Fit ====\n\nstart = time.time()\ncuml_model = curfc(n_estimators=40,\n                   max_depth=16,\n                   max_features=1.0,\n                   random_state=10,\n                   n_streams=1) # for reproducibility\n\ncuml_model.fit(X_cudf_train, y_cudf_train)\nend = time.time()\n\nprint(\"Training time: {} mins\".format(round((end-start)\/60, 1)))\n\n# ==== Evaluate ====\nstart = time.time()\nfil_preds_orig = cuml_model.predict(X_cudf_test)\n\nfil_acc_orig = accuracy_score(y_test.to_numpy(), fil_preds_orig)\nend = time.time()\n\nprint(\"Evaluation time: {} mins\".format(round((end-start)\/60, 1)))","d4314e28":"%%time\n\n# Read in data\ndtypes = {\n    \"row_id\": \"int64\",\n    \"timestamp\": \"int64\",\n    \"user_id\": \"int32\",\n    \"content_id\": \"int16\",\n    \"content_type_id\": \"boolean\",\n    \"task_container_id\": \"int16\",\n    \"user_answer\": \"int8\",\n    \"answered_correctly\": \"int8\",\n    \"prior_question_elapsed_time\": \"float32\", \n    \"prior_question_had_explanation\": \"int8\"\n}\n\ntrain = cudf.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv', dtype=dtypes)\n\n# # Drop \"row_id\" column as it doesn't give any information\n# train = train.drop(columns = [\"row_id\"], axis=1, inplace=True)","9eca3db1":"# Data Information\nprint(\"Rows: {:,}\".format(len(train)), \"\\n\" +\n      \"Columns: {}\".format(len(train.columns)))\n\n# Find Missing Data if any\ntotal = len(train)\n\nfor column in train.columns:\n    if train[column].isna().sum() != 0:\n        print(\"{} has: {:,} ({:.2}%) missing values.\".format(column, train[column].isna().sum(), \n                                                             (train[column].isna().sum()\/total)*100))\n        \n        \n# Fill in missing values with \"-1\"\ntrain[\"prior_question_elapsed_time\"] = train[\"prior_question_elapsed_time\"].fillna(-1)\ntrain[\"prior_question_had_explanation\"] = train[\"prior_question_had_explanation\"].fillna(-1)\n\ntrain.head()","1cf3e362":"def distplot_features(df, feature, title, color = custom_colors[4], categorical=True):\n    '''Takes a column from the GPU dataframe and plots the distribution (after count).'''\n    \n    if categorical:\n        values = cupy.asnumpy(df[feature].value_counts().values)\n    else:\n        values = cupy.asnumpy(df[feature].values)\n        \n    print('Mean: {:,}'.format(np.mean(values)), \"\\n\"\n          'Median: {:,}'.format(np.median(values)), \"\\n\"\n          'Max: {:,}'.format(np.max(values)))\n\n    \n    plt.figure(figsize = (18, 3))\n    \n    if categorical:\n        sns.distplot(values, hist=False, color = color, kde_kws = {'lw':3})\n    else:\n        # To speed up the process\n        sns.distplot(values[::250000], hist=False, color = color, kde_kws = {'lw':3})\n    \n    plt.title(title, fontsize=15)\n    plt.show();\n    \n    del values\n    gc.collect()","2fab1531":"def barplot_features(df, feature, title, palette = custom_colors[2:]):\n    '''Takes the numerical columns (with less than 10 categories) and plots the barplot.'''\n    \n    # We need to extract both the name of the category and the no. of appearences\n    index = cupy.asnumpy(df[feature].value_counts().reset_index()[\"index\"].values)\n    values = cupy.asnumpy(df[feature].value_counts().reset_index()[feature].values) \n\n    plt.figure(figsize = (18, 3))\n    sns.barplot(x = index, y = values, palette = palette)\n    plt.title(title, fontsize=15)\n    plt.show();\n    \n    del index, values\n    gc.collect()","ecb7fac3":"numerical_features = ['timestamp', 'prior_question_elapsed_time']\n\nfor feature in numerical_features:\n    distplot_features(train, feature=feature, title = feature + \" distribution\", color = custom_colors[2], categorical=False)","92240cd2":"categorical_features = ['user_id', 'content_id', 'task_container_id']\n\nfor feature in categorical_features:\n    distplot_features(train, feature=feature, title = feature + \" countplot distribution\", color = custom_colors[3], categorical=True)","2a937302":"categorical_for_bar = ['content_type_id', 'user_answer', \n                       'answered_correctly', 'prior_question_had_explanation']\n\nfor feature in categorical_for_bar:\n    barplot_features(train, feature=feature, title = feature + \" barplot\", palette = custom_colors)","3e42893f":"# Total rows we started with\ntotal = len(train)\nfeature = \"timestamp\"\n\n# Compute Outliers\nQ1 = cupy.percentile(train[feature].values, q = 25).item()\nQ3 = cupy.percentile(train[feature].values, q = 75).item()\nIQR = Q3 - Q1\n\n# We'll look only at the upper interval outliers\noutlier_boundry = Q3 + 1.5*IQR\n\nprint('Timestamp: around {:.2}% of the data would be erased.'.format(len(train[train[feature] >= outlier_boundry])\/total * 100), \n      \"\\n\"+\n      'The outlier boundry is {:,}, which means {:,.5} hrs, which means {:,.5} days.'.format(outlier_boundry, (outlier_boundry \/ 3.6e+6),\n                                                                                       (outlier_boundry \/ 3.6e+6)\/24))\n\ngc.collect()","ab369484":"# Select ids to erase\nids_to_erase = train[\"user_id\"].value_counts().reset_index()[train[\"user_id\"].value_counts().reset_index()[\"user_id\"] < 5]\\\n                                                                                                                [\"index\"].values\n\n# Erase the ids\nnew_train = train[~train['user_id'].isin(ids_to_erase)]\n\nprint(\"We erased {} rows meaning {:.3}% of all data.\".format(len(train)-len(new_train), (1 - len(new_train)\/len(train))*100))\ndel ids_to_erase","2b3c8cde":"# Count how many times the user answered correctly out of all available times\nuser_performance = train.groupby(\"user_id\").agg({ 'row_id': ['count'], 'answered_correctly': ['sum'] }).reset_index()\nuser_performance.columns = [\"user_id\", \"total_count\", \"correct_count\"]\nuser_performance[\"performance\"] = user_performance[\"correct_count\"] \/ user_performance[\"total_count\"]\n\n# Create intervals for number of appearences\n# between 0 and 1000, 1000 and 2500 and 2500+\ndef condition(x):\n    if x <= 1000:\n        return 0\n    elif (x > 1000) & (x <= 2500):\n        return 1\n    else:\n        return 2\n    \nuser_performance[\"total_interval\"] = user_performance[\"total_count\"].applymap(condition)","13354237":"# Convert to numpy arrays (so we can plot)\nx = cupy.asnumpy(user_performance[\"total_interval\"].values)\ny = cupy.asnumpy(user_performance[\"performance\"].values)\n\n# Plot\nplt.figure(figsize = (18, 4))\nsns.barplot(x = x, y = y, palette = custom_colors[2:])\nplt.title(\"Performance over number of appearences\", fontsize = 15)\nplt.xticks([0, 1, 2], ['<1000', '1000-2500', '2500+']);\n\ndel user_performance","2f93e6e7":"# First cleanup\ndel train\ngc.collect()\n\n\n# Checkpoint: save to .parquet\nnew_train.to_parquet('new_train.parquet')","4c332ce0":"# Clean the environment\ndel new_train\ngc.collect()","016790de":"questions = cudf.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\n\n# Data Information\nprint(\"Rows: {:,}\".format(len(questions)), \"\\n\" +\n      \"Columns: {}\".format(len(questions.columns)))\n\n# Find Missing Data if any\ntotal = len(questions)\n\nfor column in questions.columns:\n    if questions[column].isna().sum() != 0:\n        print(\"{} has: {:,} ({:.2}%) missing values.\".format(column, questions[column].isna().sum(), \n                                                             (questions[column].isna().sum()\/total)*100))\n        \n        \n# Fill in missing values with \"-1\"\nquestions[\"tags\"] = questions[\"tags\"].fillna(-1)\n\nquestions.head()","1faf633e":"# ----- question_id -----\n\n# The table contains an equal number of IDs for each question\nprint('There is a total of {:,} IDs.'.format(len(questions['question_id'].value_counts())), \"\\n\")\n\n# ----- bundle_id -----\nprint('There are {:,} unique bundle IDs.'.format(questions['bundle_id'].nunique()))","7b4dd5e4":"# ----- part & correct_answer -----\n\nfor feature in ['part', 'correct_answer']:\n    barplot_features(questions, feature=feature, title=feature + \" - barplot distribution\")","bc0f5486":"# ----- tags -----\ndistplot_features(questions, 'tags', title = \"Tags - Count Distribution\", color = custom_colors[0], categorical=True)","5a75ba02":"# Checkpoint: save to parquet\nquestions.to_parquet('questions.parquet')","49ab16aa":"del questions\ngc.collect()","89ac54ae":"lectures = cudf.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')\n\n# Encode 'type_of' column\nlectures.type_of,codes = lectures['type_of'].factorize()\n\n# Data Information\nprint(\"Rows: {:,}\".format(len(lectures)), \"\\n\" +\n      \"Columns: {}\".format(len(lectures.columns)))\nlectures.head()","12410232":"# ----- lecture_id -----\n# The table contains an equal number of IDs for each question\nprint('There is a total of {:,} IDs.'.format(len(lectures['lecture_id'].value_counts())), \"\\n\")\n\n# There are 151 unique tags\nprint('There are a total of {:,} unique tags IDs.'.format(len(lectures['tag'].value_counts())))","e018843c":"lectures.to_parquet(\"lectures.parquet\")\n\ndel lectures\ngc.collect()","94e4a8e0":"cudf.set_allocator(\"managed\")","4782ceb0":"%%time\n# Import the data\ntrain = cudf.read_parquet(\"..\/input\/riiid-answer-correctness-prediction-rapids\/new_train.parquet\")\nquestions = cudf.read_parquet(\"..\/input\/riiid-answer-correctness-prediction-rapids\/questions.parquet\")\n\n# Lectures we won't load, as we are not supposed to predict for these rows","6db40ab2":"%%time\n# Let's exclude all observations where (content_type_id = 1) & (answered_correctly = -1)\ntrain = train[train['content_type_id'] != 1]\ntrain = train[train['answered_correctly'] != -1].reset_index(drop=True)","cc1d2f93":"# Parameters\ntrain_percent = 0.1\ntotal_len = len(train)","1e3eaddc":"# Split data into train data & feature engineering data (to use for past performance)\n# Timestamp is in descending order - meaning that the last 10% observations have\n# the biggest chance of having had some performance recorded before\n# so looking at the performance in the past we'll try to predict the performance now\n\nfeatures_df = train.iloc[ : int(total_len*(1-train_percent))]\ntrain_df = train.iloc[int(total_len*(1-train_percent)) : ]","df4ede61":"%%time\n# --- STUDENT ANSWERS ---\n# Group by student\nuser_answers = features_df[features_df['answered_correctly']!=-1].\\\n                            groupby('user_id').\\\n                            agg({'answered_correctly': ['sum', 'mean', 'min', \n                                                        'max', 'count', 'median', \n                                                        'std', 'var']}).\\\n                            reset_index()\n\nuser_answers.columns = ['user_id', 'user_sum', 'user_mean', 'user_min', 'user_max', \n                        'user_count', 'user_median', 'user_std', 'user_var']\n\n\n# --- CONTENT ID ANSWERS ---\n# Group by content\ncontent_answers = features_df[features_df['answered_correctly']!=-1].\\\n                            groupby('content_id').\\\n                            agg({'answered_correctly': ['sum', 'mean', 'min', \n                                                        'max', 'count', 'median', \n                                                        'std', 'var']}).\\\n                            reset_index()\n\ncontent_answers.columns = ['content_id', 'content_sum', 'content_mean', 'content_min', \n                           'content_max', 'content_count', 'content_median', 'content_std', \n                           'content_var']","554cf5c8":"user_answers.to_parquet('user_answers.parquet')\ncontent_answers.to_parquet('content_answers.parquet')","3dd5ded5":"del train, questions\ngc.collect()","2719895d":"from sklearn.preprocessing import StandardScaler","57c03b88":"# Features for ML\nfeatures_to_keep = ['user_sum', 'user_mean', 'user_min', 'user_max', \n                        'user_count', 'user_median', 'user_std', 'user_var',\n                   'content_sum', 'content_mean', 'content_min', \n                           'content_max', 'content_count', 'content_median', 'content_std', \n                           'content_var']\ntarget = 'answered_correctly'\n\nall_features = features_to_keep.copy()\nall_features.append(target)\n\n\n# We need to convert True-False variables to integers\ndef to_bool(x):\n    '''For the string variables.'''\n    if x == False:\n        return 0\n    else:\n        return 1\n\n    \ndef combine_features(data = None):\n    '''Combine the features with the Train\/Test data.'''\n    \n    # Add \"past\" information\n    features_data = data.merge(user_answers, how = 'left', on = 'user_id')\n    features_data = features_data.merge(content_answers, how = 'left', on = 'content_id')\n\n    # Apply\n    features_data['content_type_id'] = features_data['content_type_id'].applymap(to_bool)\n    features_data['prior_question_had_explanation'] = features_data['prior_question_had_explanation'].applymap(to_bool)\n\n    # Fill in missing spots\n    features_data.fillna(value = -1, inplace = True)\n    \n    return features_data\n\n\n# Scaling the data did not perform as I expected to - so for now we will exclude it\ndef scale_data(features_data=None, train=True, features_to_keep=None, target=None):\n    '''Scales the provided data - if the data is for training, excludes the target column.\n    It also chooses the features used in the prediction.'''\n    \n    data_for_standardization = features_data[features_to_keep]\n    matrix = data_for_standardization.as_matrix()\n    scaled_matrix = StandardScaler().fit_transform(matrix)\n    \n    scaled_data = cudf.DataFrame(scaled_matrix)\n    scaled_data.columns = data_for_standardization.columns\n    \n    # We don't want to scale the target also\n    if train:\n        scaled_data[target] = features_data[target]\n        \n    return scaled_data","2016d1cc":"%%time\n\ntrain_df = combine_features(data=train_df)\n# train_df = scale_data(features_data=train_df, train=True, features_to_keep=features_to_keep, target=target)\n\n# Comment this if you're scaling\ntrain_df = train_df[all_features]\n\nprint(\"Observations in train: {:,}\".format(len(train_df)))\ntrain_df.head()","5ec229fb":"# RAPIDS roc_auc_score is 16x faster than sklearn. - cdeotte\nfrom cuml.metrics import roc_auc_score\nfrom cuml.preprocessing.model_selection import train_test_split\nimport xgboost\nimport pickle","ce3ba5fd":"# Features, target and train\/test split\nX = train_df[features_to_keep]\ny = train_df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    shuffle=False, stratify=y)","02bd1c36":"def train_xgb_model(X_train, X_test, y_train, y_test, params, details=\"default\", prints=True):\n    '''Trains an XGB and returns the trained model + ROC value.'''\n    # Create DMatrix - is optimized for both memory efficiency and training speed.\n    train_matrix = xgboost.DMatrix(data = X_train, label = y_train)\n    \n    # Create & Train the model\n    model = xgboost.train(params, dtrain = train_matrix)\n\n    # Make prediction\n    predicts = model.predict(xgboost.DMatrix(X_test))\n    roc = roc_auc_score(y_test.astype('int32'), predicts)\n\n    if prints:\n        print(details + \" - ROC: {:.5}\".format(roc))\n    \n    return model, roc\n\n\ndef param_tuning_graph(param_values, roc_values):\n    '''Represents visually the ROC results for the speciffic parameter tune.'''\n    \n    plt.figure(figsize=(18, 3))\n    ax = sns.barplot(x=param_values, y=roc_values, palette=custom_colors)\n\n    for p in ax.patches:\n        width = p.get_width()\n        height = p.get_height()\n        x, y = p.get_xy() \n        ax.annotate(f'{height:.5%}', (x + width\/2, y + height*1.02), ha='center')","5f9b43c8":"%%time\n\nparams1 = {\n    'max_depth' : 4,\n    'max_leaves' : 2**4,\n    'tree_method' : 'gpu_hist',\n    'objective' : 'reg:logistic',\n    'grow_policy' : 'lossguide'\n}\n\nmodel1, roc1 = train_xgb_model(X_train, X_test, y_train, y_test, \n                               params1, details=\"baseline model\")","c5d3c442":"# save model to file\npickle.dump(model1, open(\"xgb_baseline.pickle.dat\", \"wb\"))","423ba0cd":"%%time\n\n# --- ETA ---\n# aka learning rate\n\nrocs2 = []\netas2 = [0.001, 0.005, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n\nfor eta in etas2:\n    params2 = {\n        'max_depth' : 4,\n        'max_leaves' : 2**4,\n        'tree_method' : 'gpu_hist',\n        'objective' : 'reg:logistic',\n        'grow_policy' : 'lossguide',\n        'eta' : eta\n    }\n    \n    _, roc = train_xgb_model(X_train, X_test, y_train, y_test, \n                             params2, details = f\"ETA: {eta}\")\n    rocs2.append(roc)\n    \n    \n# Optional: Further tunning\nmax_depths = [4, 5, 6, 7, 8]","399955da":"# # --- SAVE CHECKPOINT ---\n# params_final = {\n#     'max_depth' : 8,\n#     'max_leaves' : 2**4,\n#     'tree_method' : 'gpu_hist',\n#     'objective' : 'reg:logistic',\n#     'grow_policy' : 'lossguide',\n#     'eta' : 0.5\n# }\n\n# model2, roc2 = train_xgb_model(X_train, X_test, y_train, y_test, \n#                                params_final, prints=None)\n\n# # save model to file\n# pickle.dump(model2, open(\"baseline_model_tuned.pickle.dat\", \"wb\"))","8d08a5c0":"import shap\n\nexplainer = shap.TreeExplainer(model1)\nshap_values = explainer.shap_values(X_test.to_pandas())","e95d2522":"shap.summary_plot(shap_values, X_test.to_pandas(), plot_type=\"bar\", color=custom_colors[3],\n                  title = \"\")","848afc03":"from sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn import metrics\nimport lightgbm as lgbm\nfrom sklearn import metrics\nimport gc\nimport pickle","aa009756":"# We'll do a train | validation | test situation\ntrain, test = train_test_split(train_df, test_size=0.3, shuffle=False)\n\ntrain = train.to_pandas()\ntest = test.to_pandas()","b45e32bf":"# -----------\nn_splits = 2\n# -----------\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\n\n# Covertion to CPU data\nskf_split = skf.split(X=train[features_to_keep], y=cupy.asnumpy(train[target].values))","5c2287b6":"param = {\n        'num_leaves': 80,\n        'max_bin': 250,\n        'min_data_in_leaf': 11,\n        'learning_rate': 0.01,\n        'min_sum_hessian_in_leaf': 0.00245,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': 0.05,\n        'lambda_l1': 4.972,\n        'lambda_l2': 2.276,\n        'min_gain_to_split': 0.65,\n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n    }","aca6a0fb":"%%time\n\n# Training Loop\ncounter = 1\n\nfor train_index, valid_index in skf_split:\n    print(\"==== Fold {} ====\".format(counter))\n    \n    lgbm_train = lgbm.Dataset(data = train.iloc[train_index, :][features_to_keep].values,\n                              label = train.iloc[train_index, :][target].values,\n                              feature_name = features_to_keep,\n                              free_raw_data = False)\n    \n    lgbm_valid = lgbm.Dataset(data = train.iloc[valid_index, :][features_to_keep].values,\n                              label = train.iloc[valid_index, :][target].values,\n                              feature_name = features_to_keep,\n                              free_raw_data = False)\n    \n    lgbm_2 = lgbm.train(params = param, train_set = lgbm_train, valid_sets = [lgbm_valid],\n                        early_stopping_rounds = 12, num_boost_round=100, verbose_eval=25)\n    \n    \n    # X_valid to predict\n    oof[valid_index] = lgbm_2.predict(train.iloc[valid_index][features_to_keep].values, \n                                      num_iteration = lgbm_2.best_iteration)\n    predictions += lgbm_2.predict(test[features_to_keep], \n                                  num_iteration = lgbm_2.best_iteration) \/ n_splits\n    \n    counter += 1","b8bc131e":"print(\"CV ROC: {:<0.2f}\".format(metrics.roc_auc_score(test[target], predictions)))","ff999fe9":"# # save model to file\n# # the model was initially trained on 10,000 num_boost_round\n# # but to keep the notebook running quiqly I modified it in the example to only 100\n# pickle.dump(lgbm_2, open(\"lgbm_10000_12.pickle.dat\", \"wb\"))","9bd5f4d4":"lgbm_2 = pickle.load(open('..\/input\/riiid-answer-correctness-prediction-rapids\/lgbm_10000_12.pickle.dat', 'rb'))","811df3e6":"# Import library and create environment\nimport riiideducation\nenv = riiideducation.make_env()","ee82b23f":"# Here you would also add your pretrained model\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = cudf.from_pandas(test_df)\n    \n    # --- PREPROCESSING ---\n    # Here is time to apply the preprocessing to the test_df\n    test_df = combine_features(data = test_df)\n    #X = scale_data(features_data=test_df, train=False, features_to_keep=features_to_keep).to_pandas()\n    X = test_df[features_to_keep].to_pandas()\n    \n    # --- MODEL ---\n    test_df['answered_correctly'] = lgbm_2.predict(X, num_iteration = lgbm_2.best_iteration)\n    test_df = test_df.to_pandas()\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","d294b66d":"# 3. LightGBM Model\n\n<div class=\"alert alert-block alert-warning\">\n<p><b>If you know any resource that shows how you can use a GPU dataframe to work with Stratified Folding and LGBM please comment :) I've been struggling to find some good tutorials and this is the best I could do so far.<\/b><\/p>\n<\/div>","18dc00b4":"*\ud83d\udcccNote: Can't use `Dask-cuDF` because we oly have 1 worker and Memory: 13.96 in the Kaggle GPU Accelerator. If we would have had more than 1 worker, `Dask` would have performed even better :)*","5cfcb15d":"## 1.2 Handle Outliers\n\n> \ud83d\udcccNote: The **outliers** might strongly influence the future models. Hence, we need to carefully handle them. However, by trying to erase the outliers we can erase up to 10% of the data, which is valuable information for training our models.\n\nEg.: Below we find the upper boundry for the outliers for the feature `timestamp`. This value is equal to 26,138,936,852 milliseconds, which means 7,700 hours, which means about 320 days.","3edbf43e":"<div class=\"alert alert-block alert-info\">\n<p><b>We have a ROC score of 0.71623 in ~14 seconds.<\/b><\/p>\n<p>Incredible.<\/p>\n<\/div>","092e693c":"<img src=\"https:\/\/i.imgur.com\/QVotmCj.png\">\n\n<img src=\"https:\/\/developer.nvidia.com\/sites\/default\/files\/pictures\/2018\/rapids\/rapids-logo.png\" width=400>\n\nThe usually used libraries like `numpy`, `pandas` or `scikit-learn` are designed to work on **CPU**. However, this competition is based on a large amount of data (100+ million rows), so these libraries will perform slowly or, even worse, they won't be able to perform at all. Hence, if you have some **GPUs** at hand (especially here on Kaggle), you can put them to work to help you in this competition :)\n\nThankfully, the researchers at Nvidia worked hard and in our favor, developing the `RAPIDS` open-source package.\n\n> RAPIDS is an open-source suite of data processing and machine learning libraries developed by NVIDIA that enables GPU-acceleration for data science workflows.\n\n<div class=\"alert alert-block alert-info\">\n<b>References I used:<\/b>\n<ul>\n    <li><a href = \"https:\/\/rapids.ai\/index.html\">RAPIDS - Open GPU Data Science - Official Site<\/a><\/li>\n    <li><a href = \"https:\/\/docs.rapids.ai\/api\/cudf\/stable\/10min.html#Dask-Performance-Tips\">10 mins to CuDF<\/a><\/li>\n<\/ul>\n<\/div>\n\n### Libraries \ud83d\udcda","35f49064":"## 2.2 Save and delete","5e58e408":"# 3. lectures.csv\n\n* `lecture_id`: foreign key for the train\/test `content_id` column, when the content type is lecture (1).\n* `part`: top level category code for the lecture.\n* `tag`: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n* `type_of`: brief description of the core purpose of the lecture (`string` - so this data needs to be treated a bit different)\n\n*no missing values*","3a65984f":"<img src=\"https:\/\/i.imgur.com\/NvHmO3L.png\">\n\n<div class=\"alert alert-block alert-info\">\nIn this section we'll use the <code>cudf<\/code> and <code>cupy<\/code> libraries provided by RAPIDS, combined with <code>numpy<\/code> for the plotting part. The notebook runs at the moment in 3 minutes.\n<\/div>\n\n# 1. train.csv\n\n* `row_id`: (int64) ID code for the row.\n* `timestamp`: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n* `user_id`: (int32) ID code for the user.\n* `content_id`: (int16) ID code for the user interaction\n* `content_type_id`: (bool) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n* `task_container_id`: (int16) ID code for the *batch of questions or lectures*. (eg. a user might see three questions in a row before seeing the explanations for any of them - those three would all share a task_container_id)\n* `user_answer`: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n* `answered_correctly`: (int8) if the user responded correctly. Read -1 as null, for lectures.\n* `prior_question_elapsed_time`: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between (is null for a user's first question bundle or lecture)\n* `prior_question_had_explanation`: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.","36dd400f":"# 2. XGBoost Model\n\n> The [Stratified K Fold](https:\/\/nbviewer.jupyter.org\/github\/daxiongshu\/notebooks-extended\/blob\/kdd_plasticc\/competition_notebooks\/kaggle\/landmark\/cudf_stratifiedKfold_1000x_speedup.ipynb) function using `cudf` is from Grandmaster [Jiwei Liu](https:\/\/www.kaggle.com\/jiweiliu)","1f3003cb":"### Sklearn (CPU)","ce215774":"### Inspect numerical features","4404d30a":"<center><img src=\"https:\/\/i.imgur.com\/HrWLO8e.png\"><\/center>\n\n### Versions:\n* Version 7: first baseline model\n* Version 8: tuned baseline model (lower score in LB)\n* Version 10: added new features - tunned model (lower in LB)\n* Version 11: no tunning trial\n* Version 13: added standardization, train_test_split and Feature Importance (kept only the important ones)\n* Version 15: removed standardization (lower performance) and added stratified=y in `train_test_split` (shuffle=False)\n* Version 16: adjusted the `train_data` and `features_data`, as there was a leakage","5ee44d94":"> [Tesla P-100](https:\/\/www.nvidia.com\/en-us\/data-center\/tesla-p100\/)","f223d5ec":"> \ud83d\udcccNote: The only 2 columns with missing data (explained in documentation - `NULL` values are present for the first question bundle)","791be371":"> \ud83d\udcccNote: majority of the questions are from part 5 - if this distribution doesn't match the `test` set, there might be some issues :)","d5e93e22":"## 1.3 Apply Functions - getting data ready","5c97b4a3":"<img src=\"https:\/\/i.imgur.com\/3cBHzEF.png\">\n\n> Let's look again at the structure of our data:\n<img src=\"https:\/\/i.imgur.com\/gjuzFkl.png\" width=550>\n\n<div class=\"alert alert-block alert-success\">\n<p><b>This section uses the <code>cuML<\/code> package and XGBoost to compute the predictions.<\/b><\/p>\n<\/div>","5338009b":"<img src=\"https:\/\/i.imgur.com\/cUQXtS7.png\">\n\n# Specs on how I prepped & trained \u2328\ufe0f\ud83c\udfa8\n### (*locally*)\n* Z8 G4 Workstation \ud83d\udda5\n* 2 CPUs & 96GB Memory \ud83d\udcbe\n* NVIDIA Quadro RTX 8000 \ud83c\udfae\n* RAPIDS version 0.17 \ud83c\udfc3\ud83c\udffe\u200d\u2640\ufe0f","50322b98":"# 1. Feature Engineering","62a9a038":"### Inspect Categorical Features: many values","1be58b14":"<div class=\"alert alert-block alert-success\">\n<p>In this experiment, CuML is net superior (MUCH faster) than Sklearn - almost 6 minutes vs 30 seconds.<\/p>\n<\/div>","e3e7521b":"## 1.3 Curiosities\n\nFirst I would like to see if the number of appearences of 1 ID (1 pupil) corresponds to their overall performance (other words, if you assess 1 student multiple times, will they perform better?)","f4c3c8cc":"## CPU vs GPU comparison\n\n> code source is from RAPIDS demo notebooks: Random Forest Demo","2690c62d":"## 1.1 Columns individual analysis\n\n* numerical features (distplot): `timestamp`, `prior_question_elapsed_time`\n* categorical features (distplot): `user_id` count, `content_id` count, `task_container_id` count\n* categorical features (barplot): `user_answer` count, `answered_correctly` count, `prior_question_had_explanation` count\n\n### Predefined functions\ud83d\udcc2\n\nBecause there is no possibility (yet) to use Rapids for visualization we need to preprocess and convert the data to numpy arrays and plot it afterwards.","95d10d7d":"## 2.1 Baseline Model\n\n### Helper Function that runs multiple models","14895edd":"## 3.2 Save and delete","57f25f3a":"## 2.2 Fine Tuning\n\n> Let's see if we can improve the ROC by only tuning the hyperparameters.","ebcbaa09":"> \ud83d\udcccNote: However, I would erase all pupils (`user_id`) that have less than 5 appearences in the data (no prediction can be made on these students)  ","06a3856b":"<img src=\"https:\/\/i.imgur.com\/lrSAmpi.png\">\n\n> This competition has the inference a bit different - for reference please check [this notebook.](https:\/\/www.kaggle.com\/sohier\/competition-api-detailed-introduction\/comments) Hence, in the `for` loop we create the actual submission (can be seen in the output).\n\n> Training Part: We don't have access to test set labels because we are predicting before getting labels. That's how it is in real world too: You predict today and get the labels tomorrow. You can use tomorrow's labels to predict the day after. And so on. Per day or per batch. - Vopani\n\nThings we'll have to apply to `test` set:\n* we assume the `test` set DOESN'T have `content_type_id` = 1 NOR `answered_correctly` = -1\n* add information from `user_answers`\n* add information from `content_answers`\n* change True\/False columns to 1\/0\n* fill missing values with -1\n* normalize the features\n\nBonus: select only the features we want :)","ee79aa1d":"# 2. questions.csv\n\n* `question_id`: foreign key for the train\/test `content_id` column, when the content type is question (0).\n* `bundle_id`: code for which questions are served together.\n* `correct_answer`: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n* `part`: the relevant section of the TOEIC test.\n* `tags`: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.\n\n> The Test of English for International Communication (TOEIC) is an international standardized test of English language proficiency for non-native speakers.","7dba5fc3":"## 2.1 Inspect the columns\n\n* categorical features (distplot): `question_id` count, `bundle_id` count, `tags` count\n* categorical features (barplot): `correct_answer`, `part`","460f5783":"> Save FE data; we will use it for the `test` set too :)","42e505a9":"## 3.1 Inspect the columns","7f0d227d":"> \ud83d\udcccNote: So yes, the *average* performance increases along with the number of times one student appears in the data.","67210409":"## 1.2 Predefined Functions for Preprocesing\u00b6\n\n> Combine new features with the `train_df`","d5ac051d":"## 2.4 Feature Importance","98e9911e":"### Inspect Categorical Features: fiew values\n\n> There are only a fiew cases where content_type_id is = 1 (meaning lectures) - which is good, we're not supposed to predict those anyways.","2f99ddf8":"## 1.1 Feature Engineering - Create Data","313f7afb":"### CuML (GPU)","5c3ba9f2":"## 1.4 Save and delete\n\n> To keep the notebook as light as possible and to not overload the memory, we save the `train` data in .feather format (lighter, takes about 7 seconds to upload using `cudf`) and delete the dataframes.\n\n*UPDATE: even better - as Chris Deotte recommends: \"Instead of using .feather use .parquet. RAPIDS cuDF can read parquet 6x faster than feather (and writes parquet 1.1x faster)\"*"}}