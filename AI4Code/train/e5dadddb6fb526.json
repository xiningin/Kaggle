{"cell_type":{"4211af16":"code","8d4e1878":"code","57773670":"code","417d2109":"code","a962e05e":"code","5c2e054e":"code","9b015cc9":"code","31782e42":"code","5d89b03d":"code","0928d4d1":"code","146331d8":"code","3e30c08f":"code","bb76bf76":"code","4cda498c":"code","a95daa0c":"code","060a688b":"code","3ffd5847":"code","b8b972fd":"code","d4ab11ba":"code","03c1e2be":"code","e045c0ac":"code","348c03bb":"code","30913659":"code","1256468f":"code","5e817ec4":"code","c75aa3f0":"code","214b50b5":"code","88d0b1f2":"code","265f239b":"code","8430f736":"code","ac605953":"code","89043c64":"code","67e7a765":"code","5e5a6d6a":"code","9bba1984":"code","c65044a6":"code","addd5b3c":"code","bba45ffd":"code","80f3dd83":"code","18d0c15f":"code","c0c875d0":"code","873037f7":"code","01ddfc0d":"code","c94b18bd":"code","44abc7ff":"code","83a060eb":"code","1287821f":"code","bcd53f3c":"code","235c3cff":"code","b841dd0a":"code","11101160":"code","2f6a5bc0":"code","94acf88e":"code","03fbf7f5":"code","c01d13c6":"code","8e6d6306":"code","fc77db20":"code","dea40364":"code","1af94ee2":"code","8850081e":"code","ae310631":"code","de2840d2":"code","a707fa19":"code","55222079":"code","8dfc7dc9":"code","e5c87512":"code","cd4c1939":"code","2be1eaf7":"code","00b78a39":"code","230433eb":"code","289878e8":"code","b4f26a79":"code","275c970f":"code","694ef334":"code","09898a74":"code","48f5e885":"code","b74faf81":"code","da672dad":"code","285b16d3":"code","767f9662":"code","8fc31322":"code","3cffb34f":"code","aa1ada53":"code","a4dd7f70":"code","4b4faf52":"code","3c9bc8c4":"code","22e75354":"code","3f78abbc":"code","bbcdd662":"code","56068dc6":"code","8c0b8825":"code","9ae031d6":"code","fbdb7351":"code","ef0d14bb":"code","3491fd3f":"code","d8b887ad":"code","a475b748":"code","fbb10d35":"code","79a5a660":"code","7c76fd26":"markdown","0a98e64d":"markdown","f29f6c9a":"markdown","6db91023":"markdown","95f5aa1b":"markdown","0853a44c":"markdown"},"source":{"4211af16":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.preprocessing import StandardScaler\nfrom collections import Counter\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.ensemble import RandomForestClassifier\nimport re, string\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords ","8d4e1878":"import tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping","57773670":"import nltk; nltk.download('wordnet') ; nltk.download('stopwords')","417d2109":"train_df = pd.read_csv('..\/input\/iba-ml1-final-project\/train.csv')\ntest_df = pd.read_csv('..\/input\/iba-ml1-final-project\/test.csv')","a962e05e":"train_df['Rating'].hist()","5c2e054e":"train_df['Recommended'].hist()","9b015cc9":"# train_df = train_df.drop(\"Id\",axis=1)\ndf = pd.concat([train_df, test_df], 0,ignore_index=True)\nnrow_train = train_df.shape[0]\ny_train_1 = train_df[\"Rating\"]\ny_train_2 = train_df[\"Recommended\"]","31782e42":"df[\"Product_Category\"] = df[\"Product_Category\"].fillna(\"Other\").astype(\"category\")\ndf[\"Department\"] = df[\"Department\"].fillna(\"unknown\")\ndf[\"Division\"] = df[\"Division\"].fillna(\"unknown\")","5d89b03d":"df[\"Review\"] = df[\"Review\"].fillna(\"None\")\ndf[\"Review_Title\"] = df[\"Review_Title\"].fillna(\"None\")","0928d4d1":"def remove_punctuations(words):\n  table = str.maketrans('', '', string.punctuation)\n  stripped = [w.translate(table) for w in words]\n  return stripped","146331d8":"def filter_stop_words(txt):\n    stop_words = set(stopwords.words(\"english\"))\n    filtered_sentence = [w for w in txt if not w in stop_words] \n    return filtered_sentence","3e30c08f":"df['Review'] = df['Review'].apply(lambda x: re.split(r'\\W+', x))  # saving only words","bb76bf76":"df['Review'] = df['Review'].apply(remove_punctuations)  # removing punctuations","4cda498c":"df['Review'] = df['Review'].apply(lambda x: [y.lower() for y in x]) # converting to lower case","a95daa0c":"df['processed'] = df['Review'].apply(lambda x: [WordNetLemmatizer().lemmatize(y) for y in x]) # Lemmatize every word.","060a688b":"df['processed_review'] = df['processed'].apply(lambda x: [PorterStemmer().stem(y) for y in x]) # Stem every word.","3ffd5847":"df['processed_review'] = df['processed_review'].apply(filter_stop_words).apply(lambda x: ' '.join(x)) # filtering stop words and concatinating","b8b972fd":"df['processed_review'][3000:10000]","d4ab11ba":"rnn_df = df[['processed_review','Recommended','Rating']]","03c1e2be":"rnn_df","e045c0ac":"X = rnn_df['processed_review']\ny = rnn_df['Recommended']","348c03bb":"X_train,X_test,y_train,y_test = train_test_split(X[:nrow_train],y[:nrow_train])","30913659":"max_words = 5000\nmax_len = 500\ntok = Tokenizer(\n      num_words=max_words,\n      document_count=X_train.shape[0]\n)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","1256468f":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","5e817ec4":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","c75aa3f0":"model.fit(sequences_matrix,y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","214b50b5":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=500)","88d0b1f2":"model.save('Recommended_model')","265f239b":"accr = model.evaluate(test_sequences_matrix,y_test)","8430f736":"txts = tok.texts_to_sequences(X_test)\ntxts = sequence.pad_sequences(txts, maxlen=500)\npreds = model.predict(txts)","ac605953":"preds","89043c64":"preds[preds > 0.5] = 1\npreds[preds <= 0.5] = 0","67e7a765":"preds = preds.flatten()","5e5a6d6a":"print(recommended_preds)","9bba1984":"pd.Series(preds).hist()","c65044a6":"y_rating = rnn_df['Rating']","addd5b3c":"y_rating[:nrow_train] = y_rating[:nrow_train].apply(lambda x: x-1)","bba45ffd":"y_rating.unique()","80f3dd83":"# y_rating = np.asarray(y_rating).astype('float32').reshape((-1,1))","18d0c15f":"Xr_train,Xr_test,yr_train,yr_test = train_test_split(X[:nrow_train],y_rating[:nrow_train])","c0c875d0":"Xr_train.map(len).max()","873037f7":"max_words = 20000\nmax_len = 500\ntok = Tokenizer( num_words=max_words)\ntok.fit_on_texts(Xr_train)\nsequences = tok.texts_to_sequences(Xr_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","01ddfc0d":"BiLSTM = tf.keras.Sequential([\n    tf.keras.layers.Embedding(10000, 64), # Embedding Layer using the vocab-size from encoder\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)), # Create the first Bidirectional layer with 64 LSTM units\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # Second Bidirectional layer witth 32 LSTM units\n    tf.keras.layers.Dense(64, activation='relu'), # A Dense Layer with 64 units\n    tf.keras.layers.Dropout(0.1), # 50% Dropout\n    tf.keras.layers.Dense(5,activation=\"softmax\") # Final Dense layer with a single unit\n])","c94b18bd":"BiLSTM.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics= ['acc']) # Compiling the Model","44abc7ff":"BiLSTM_history = BiLSTM.fit(sequences_matrix,yr_train, batch_size=256,epochs = 10,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.00001)])","83a060eb":"test_sequences = tok.texts_to_sequences(Xr_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\nBiLSTM.evaluate(test_sequences_matrix,yr_test)","1287821f":"BiLSTM.evaluate(test_sequences_matrix,yr_test)","bcd53f3c":"rating_preds = BiLSTM.predict(test_sequences_matrix)","235c3cff":"preds_classes = np.argmax(rating_preds, axis=-1)","b841dd0a":"preds_classes +=1","11101160":"pd.Series(preds_classes).hist()","2f6a5bc0":"def RNN2():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(128,return_sequences=True)(layer)\n    layer = LSTM(64,return_sequences=True)(layer)\n    layer = LSTM(32)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    # layer = Dense(32,name='FC2')(layer)\n    # layer = Activation('relu')(layer)\n    layer = Dense(5,name='out_layer')(layer)\n    layer = Activation('softmax')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n\n    return model","94acf88e":"modelr = RNN2()\nmodelr.summary()\nmodelr.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","03fbf7f5":"modelr.fit(sequences_matrix,yr_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","c01d13c6":"test_sequences = tok.texts_to_sequences(Xr_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\naccr = modelr.evaluate(test_sequences_matrix,yr_test)","8e6d6306":"# txts = tok.texts_to_sequences(Xr_test)\n# txts = sequence.pad_sequences(txts, maxlen=max_len)\npredsr = modelr.predict(test_sequences_matrix)","fc77db20":"preds_classes = np.argmax(predsr, axis=-1)","dea40364":"preds_classes +=1","1af94ee2":"X_train = X[:nrow_train]\ny_train = y[:nrow_train]","8850081e":"X_test = X[nrow_train:]","ae310631":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=500)","de2840d2":"recommended_test = model.predict(test_sequences_matrix)","a707fa19":"recommended_test","55222079":"recommended_test[recommended_test > 0.5] = 1\nrecommended_test[recommended_test <= 0.5] = 0","8dfc7dc9":"recommended_test = recommended_test.flatten()","e5c87512":"pd.Series(recommended_test).hist()","cd4c1939":"rating_test = BiLSTM.predict(test_sequences_matrix)","2be1eaf7":"rating_test","00b78a39":"preds_classes = np.argmax(rating_test, axis=-1)","230433eb":"preds_classes+=1","289878e8":"preds_classes","b4f26a79":"len(preds_classes)","275c970f":"len(recommended_classes)","694ef334":"len(test_df['Id'])","09898a74":"prev_sub = pd.read_csv(\"submission2.csv\")","48f5e885":"submission = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"Rating\": prev_sub[\"Rating\"],\n        \"Recommended\" : recommended_test,\n    })","b74faf81":"submission","da672dad":"submission.to_csv('.\/submission9.csv', index=False)","285b16d3":"X_train = X[:nrow_train]\ny_train = y[:nrow_train]","767f9662":"yr_train = y_rating[:nrow_train]","8fc31322":"X_test = X[nrow_train:]","3cffb34f":"max_words = 5000\nmax_len = 500\ntok = Tokenizer(\n      num_words=max_words,\n      document_count=X_train.shape[0]\n)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","aa1ada53":"model_rec = RNN()\nmodel_rec.summary()\nmodel_rec.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","a4dd7f70":"model_rec.fit(sequences_matrix,y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","4b4faf52":"txts = tok.texts_to_sequences(X_test)\ntxts = sequence.pad_sequences(txts, maxlen=max_len)\npredsr = model_rec.predict(txts)","3c9bc8c4":"predsr = predsr.flatten()","22e75354":"predsr[predsr>0.5] = 1\npredsr[predsr<=0.5] = 0","3f78abbc":"pd.Series(predsr).hist()","bbcdd662":"tok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","56068dc6":"BiLSTM2 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(5000, 64), # Embedding Layer using the vocab-size from encoder\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)), # Create the first Bidirectional layer with 64 LSTM units\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # Second Bidirectional layer witth 32 LSTM units\n    tf.keras.layers.Dense(64, activation='relu'), # A Dense Layer with 64 units\n    tf.keras.layers.Dropout(0.5), # 50% Dropout\n    tf.keras.layers.Dense(5,activation=\"softmax\") # Final Dense layer with a single unit\n])","8c0b8825":"BiLSTM2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics= ['acc']) # Compiling the Model","9ae031d6":"BiLSTM2_history = BiLSTM2.fit(sequences_matrix,yr_train, epochs = 10,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","fbdb7351":"rating_preds = BiLSTM2.predict(txts)","ef0d14bb":"rating_preds","3491fd3f":"preds_classes = np.argmax(rating_preds, axis=-1)","d8b887ad":"preds_classes+=1","a475b748":"pd.Series(preds_classes).hist()","fbb10d35":"submission = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"Rating\": preds_classes,\n        \"Recommended\" : predsr,\n    })","79a5a660":"submission.to_csv('.\/submission.csv', index=False)","7c76fd26":"# Actual test predictions (training with whole train dataset)","0a98e64d":"# Actual test predictions ( using previous models)","f29f6c9a":"## Predicting recommended with RNN","6db91023":"# Bidirectional LSTM","95f5aa1b":"## Predicting Rating with RNN","0853a44c":"# Simple LSTM\n\n\n\n"}}