{"cell_type":{"2a2c708b":"code","f0022236":"code","03db2be0":"code","a51af56b":"code","28552c5f":"code","eb813140":"code","51e73344":"code","f7b2a223":"code","72fd4544":"code","479a4918":"code","4bd3922b":"code","e5855579":"code","37396362":"code","3ac46e0f":"code","e16c3c90":"code","3368b271":"code","3d0bd43b":"code","3269818a":"code","f755dff4":"code","e91e2a6e":"code","727f4cbb":"code","34e75666":"code","08dc1cca":"code","2030599e":"code","a2c7872a":"code","b04783fc":"code","d33b39ed":"code","edee2a10":"code","f02402d5":"code","7331e556":"code","a5c414e4":"code","1bcd558a":"code","9bc9c21c":"code","74f4f893":"code","69e6cd37":"code","f5d968c5":"markdown","5305102c":"markdown","bc66d119":"markdown","c8c72d3e":"markdown","2cdf2605":"markdown","a00ca26d":"markdown","02b68605":"markdown","613e382e":"markdown","a2d8eb4d":"markdown","d7cb106e":"markdown","12133548":"markdown","56711926":"markdown","2e8669fc":"markdown","574991f7":"markdown","300b9a55":"markdown","7862afc4":"markdown"},"source":{"2a2c708b":"#Importing required packages.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n%matplotlib inline","f0022236":"#Loading dataset\nwine = pd.read_csv('..\/input\/winequality-red.csv')","03db2be0":"#Let's check how the data is distributed\nwine.head()\nprint(np.unique(wine['quality']))","a51af56b":"#Information about the data columns\nwine.info()","28552c5f":"#Here we see that fixed acidity does not give any specification to classify the quality.\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = wine)","eb813140":"#Here we see that its quite a downing trend in the volatile acidity as we go higher the quality \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = wine)","51e73344":"#Composition of citric acid go higher as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = wine)","f7b2a223":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = wine)","72fd4544":"#Composition of chloride also go down as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = wine)","479a4918":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = wine)","4bd3922b":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = wine)","e5855579":"#Sulphates level goes higher with the quality of wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'sulphates', data = wine)","37396362":"#Alcohol level also goes higher as te quality of wine increases\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = wine)","3ac46e0f":"#Making binary classificaion for the response variable.\n#Dividing wine as good and bad by giving the limit for the quality\nbins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\nwine['quality'] = pd.cut(wine['quality'], bins = bins, labels = group_names)","e16c3c90":"#Now lets assign a labels to our quality variable\nlabel_quality = LabelEncoder()","3368b271":"#Bad becomes 0 and good becomes 1 \nwine['quality'] = label_quality.fit_transform(wine['quality'])","3d0bd43b":"import numpy as np\nwine['quality'].value_counts()\nprint(np.unique(wine['quality']))","3269818a":"sns.countplot(wine['quality'])","f755dff4":"#Now seperate the dataset as response variable and feature variabes\nX = wine.drop('quality', axis = 1)\ny = wine['quality']\n","e91e2a6e":"#Train and Test splitting of data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","727f4cbb":"#Applying Standard scaling to get optimized result\nsc = StandardScaler()","34e75666":"X_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","08dc1cca":"rfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)","2030599e":"#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))","a2c7872a":"#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","b04783fc":"sgd = SGDClassifier(penalty=None)\nsgd.fit(X_train, y_train)\npred_sgd = sgd.predict(X_test)","d33b39ed":"print(classification_report(y_test, pred_sgd))","edee2a10":"print(confusion_matrix(y_test, pred_sgd))","f02402d5":"svc = SVC()\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_test)","7331e556":"print(classification_report(y_test, pred_svc))","a5c414e4":"#Finding best parameters for our SVC model\nparam = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\ngrid_svc = GridSearchCV(svc, param_grid=param, scoring='accuracy', cv=10)","1bcd558a":"grid_svc.fit(X_train, y_train)","9bc9c21c":"#Best parameters for our svc model\ngrid_svc.best_params_","74f4f893":"#Let's run our SVC again with the best parameters.\nsvc2 = SVC(C = 1.2, gamma =  0.9, kernel= 'rbf')\nsvc2.fit(X_train, y_train)\npred_svc2 = svc2.predict(X_test)\nprint(classification_report(y_test, pred_svc2))","69e6cd37":"#Now lets try to do some evaluation for random forest model using cross validation.\nrfc_eval = cross_val_score(estimator = rfc, X = X_train, y = y_train, cv = 10)\nrfc_eval.mean()","f5d968c5":"## Our training and testing data is ready now to perform machine learning algorithm","5305102c":"## **Let's do some plotting to know how the data columns are distributed in the dataset**","bc66d119":"### Thank for going through this notebook","c8c72d3e":"## If you find this notebook useful then please upvote. So the beginners can find easily","2cdf2605":"## Cross Validation Score for random forest and SGD","a00ca26d":"# Hello there! prasad kevin here . hope you enjoy this keral \n\n*In this notebook, First I have done some exploration on the data using matplotlib and seaborn.\nThen, I use different classifier models to predict the quality of the wine.*\n\n**1. Random Forest Classifier**\n\n**2. Stochastic Gradient Descent Classifier**\n\n**3. Support Vector Classifier(SVC) **\n\n*Then I use cross validation evaluation technique to optimize the model performance.*\n\n**1. Grid Search CV**\n\n**2. Cross Validation Score**\n\n## **If you find this notebook useful then please upvote.**","02b68605":"\n\n## Stochastic Gradient Decent Classifier","613e382e":"## Let's try to increase our accuracy of models\n## Grid Search CV","a2d8eb4d":"## Support Vector Classifier","d7cb106e":"### Random forest accuracy increases from 87% to 91 % using cross validation score","12133548":"### Random Forest Classifier","56711926":"#### 84% accuracy using stochastic gradient descent classifier","2e8669fc":"### SVC improves from 86% to 90% using Grid Search CV","574991f7":"#### Random forest gives the accuracy of 87%","300b9a55":"## Preprocessing Data for performing Machine learning algorithms","7862afc4":"#### Support vector classifier gets 86%"}}