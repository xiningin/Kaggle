{"cell_type":{"8cf2ab22":"code","1c3e6706":"code","ea1dd26f":"code","e28b87ac":"code","e87fb44d":"code","fa381e4c":"code","3d25b3d0":"code","e919e110":"code","61502519":"code","a31e80e9":"code","f164f333":"code","c07445f8":"code","6390fa66":"code","af003912":"code","8c392570":"code","da81247a":"code","8c4f9c14":"code","2c1b63fa":"code","d21e8397":"code","b7c6dc51":"code","4b196288":"code","291a5a47":"code","c61ed07b":"code","bfd39c5d":"code","df058f57":"code","943f7435":"code","4767532f":"code","838eae05":"code","cd73e3f1":"code","322ecc16":"code","365015bc":"code","68b89d95":"code","2a8c9458":"code","9772a42a":"code","19948f43":"code","74f7fbbb":"code","684f2f1f":"code","f6affb2b":"code","567a9c59":"code","22b00b31":"code","7bcbf0eb":"code","c1cebd96":"code","5a74719f":"code","27d65662":"code","7023427b":"code","63f8faf0":"code","60bbb7ff":"code","584729f6":"code","bbe513de":"code","67aa6e66":"code","57958584":"code","00b734d0":"code","a3814943":"code","41601dd4":"code","22aeee59":"code","af176f4f":"markdown","96f7dae2":"markdown","7f5ad83d":"markdown","bd869a4e":"markdown","abe806df":"markdown","4767ab79":"markdown","89f692a9":"markdown","3558e1c7":"markdown","7040d035":"markdown","af0cffe5":"markdown","a6fdfc4e":"markdown","8310839d":"markdown","f7dd6f4e":"markdown","e746b0b6":"markdown","9c8e23fd":"markdown","14861f9d":"markdown","4eb6bb81":"markdown","4675cee9":"markdown","4423d303":"markdown","a2c61f0e":"markdown"},"source":{"8cf2ab22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom datetime import datetime\nfrom pandas import Series\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 20,10\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c3e6706":"#importing the data sets\n\ndf_train = pd.read_csv(r\"..\/input\/into-the-future\/train.csv\")\ndf_test = pd.read_csv(r'..\/input\/into-the-future\/test.csv')\n# Copy of original data\ntrain_original = df_train.copy()\ntest_original = df_test.copy()","ea1dd26f":"df_test.head()","e28b87ac":"df_train.head()","e87fb44d":"df_train.shape","fa381e4c":"# Converting to datetime format\ndf_train['Datetime'] = pd.to_datetime(df_train.time,format='%Y-%m-%d %H:%M:%S') \ndf_test['Datetime'] = pd.to_datetime(df_test.time,format='%Y-%m-%d %H:%M:%S') \ntest_original['Datetime'] = pd.to_datetime(test_original.time,format='%Y-%m-%d %H:%M:%S')","3d25b3d0":"# Start and end date for train\ndf_train['Datetime'].min(),df_train['Datetime'].max()","e919e110":"# Start and end date for test\ndf_test['Datetime'].min(),df_test['Datetime'].max()","61502519":"# Creating new features\nfor i in (df_train, df_test, test_original):\n    i['year'] = i.Datetime.dt.year \n    i['month'] = i.Datetime.dt.month \n    i['day'] = i.Datetime.dt.day\n    i['Hour'] = i.Datetime.dt.hour\n    i['minute'] = i.Datetime.dt.minute\n    i['seconds'] = i.Datetime.dt.second","a31e80e9":"# Droping id,time column from train\ndf_train = df_train.drop(['id', 'time'],axis = 1)","f164f333":"df_train.Timestamp = pd.to_datetime(df_train.Datetime,format='%Y-%m-%d %H:%M:%S') \ndf_train.index =df_train.Timestamp\n\ndf_test.Timestamp = pd.to_datetime(df_test.Datetime,format='%Y-%m-%d %H:%M:%S') \ndf_test.index = df_test.Timestamp ","c07445f8":"# Hourly time series\nhourly = df_train.resample('H').mean()\n\n# Minutly time series\nMinutly = df_train.resample('min').mean()\n\n# Secondly time series\nsecondly = df_train.resample('S').mean()","6390fa66":"fig, axs = plt.subplots(2,1)\n\nhourly.feature_2.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0])\nMinutly.feature_2.plot(figsize=(15,8), title= 'Minutly', fontsize=14, ax=axs[1])\n\nplt.show()","af003912":"df_train.tail()","8c392570":"# Training and Validation data split\n\nTrain=df_train.loc['2019-03-19 00:00:00':'2019-03-19 01:00:00']\nvalid=df_train.loc['2019-03-19 01:00:00':'2019-03-19 01:33:00']","da81247a":"Train.feature_2.plot(figsize=(15,8), title= 'Minutes TSA', fontsize=14, label='train') \nvalid.feature_2.plot(figsize=(15,8), title= 'Minutes TSA', fontsize=14, label='valid') \nplt.xlabel(\"Datetime\") \nplt.ylabel(\"Feature_2\") \nplt.legend(loc='best') \nplt.show()","8c4f9c14":"ts = df_train['feature_2']\nplt.figure(figsize=(16,8)) \n\nplt.plot(ts,label='Feature_2')\n# plt.plot(df['feature_1'],label='Other_feture')\nplt.title('Time Series') \nplt.xlabel(\"Time\") \nplt.ylabel(\"number_count\") \nplt.legend()","2c1b63fa":"import seaborn as sns\nsns.jointplot(x='feature_1',y='feature_2',data=df_train)\n","d21e8397":"df_train.groupby('year')['feature_2'].mean().plot.bar()","b7c6dc51":"df_train.groupby('Hour')['feature_2'].mean().plot.bar()","4b196288":"df_train.groupby(['month','minute'])['feature_2'].mean().plot.bar()","291a5a47":"# Moving Average \ny_hat_avg = valid.copy()\ny_hat_avg['moving_avg_forecast'] = Train['feature_2'].rolling(10).mean().iloc[-1] # average of last 10 observations.\nplt.figure(figsize=(15,5)) \nplt.plot(Train['feature_2'], label='Train')\nplt.plot(valid['feature_2'], label='Valid')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 10 observations')\nplt.legend(loc='best')\nplt.show()\n\ny_hat_avg = valid.copy()\ny_hat_avg['moving_avg_forecast'] = Train['feature_2'].rolling(20).mean().iloc[-1] # average of last 20 observations.\nplt.figure(figsize=(15,5))\nplt.plot(Train['feature_2'], label='Train')\nplt.plot(valid['feature_2'], label='Valid')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 20 observations')\nplt.legend(loc='best')\nplt.show()\n\ny_hat_avg = valid.copy()\ny_hat_avg['moving_avg_forecast'] = Train['feature_2'].rolling(50).mean().iloc[-1] # average of last 50 observations.\nplt.figure(figsize=(15,5))\nplt.plot(Train['feature_2'], label='Train')\nplt.plot(valid['feature_2'], label='Valid')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 50 observations')\nplt.legend(loc='best')\nplt.show()","c61ed07b":"# Calculating rmse value\nrmse = sqrt(mean_squared_error(valid.feature_2, y_hat_avg.moving_avg_forecast))\nprint('RMSE value is :',rmse)","bfd39c5d":"# Simple exponential smoothing\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\ny_hat_avg = valid.copy()\nfit2 = SimpleExpSmoothing(np.asarray(Train['feature_2'])).fit(smoothing_level=0.6,optimized=False)\ny_hat_avg['SES'] = fit2.forecast(len(valid))\nplt.figure(figsize=(16,8))\nplt.plot(Train['feature_2'], label='Train')\nplt.plot(valid['feature_2'], label='Valid')\nplt.plot(y_hat_avg['SES'], label='SES')\nplt.legend(loc='best')\nplt.show()","df058f57":"# calculating rmse value\nrmse = sqrt(mean_squared_error(valid.feature_2, y_hat_avg.SES))\nprint('RMSE value is :',rmse)","943f7435":"# Holt's Linear trend model\ny_hat_avg = valid.copy()\n\nfit1 = Holt(np.asarray(Train['feature_2'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_avg['Holt_linear'] = fit1.forecast(len(valid))\n\nplt.figure(figsize=(16,8))\nplt.plot(Train['feature_2'], label='Train')\nplt.plot(valid['feature_2'], label='Valid')\nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","4767532f":"# calculating rmse value\nrmse = sqrt(mean_squared_error(valid.feature_2, y_hat_avg.Holt_linear))\nprint('RMSE value is :',rmse)","838eae05":"# Function for checking stationarity of data (Dickey Fuller test for stationarity)\nfrom statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = pd.Series.rolling(timeseries, 24).mean()\n    rolstd = pd.Series.rolling(timeseries, 24).std()\n    \n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='green',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","cd73e3f1":"test_stationarity(train_original['feature_2'])","322ecc16":"\n#Estimating & Eliminating Trend\n\nTrain_log = np.log(Train['feature_2'])\nvalid_log = np.log(valid['feature_2'])","365015bc":"# Taking rolling mean\nmoving_avg = pd.Series.rolling(Train_log, 20).mean()\nplt.plot(Train_log)\nplt.plot(moving_avg, color = 'red')\nplt.show()","68b89d95":"# Removing trend\ntrain_log_moving_avg_diff = Train_log - moving_avg\n\ntrain_log_moving_avg_diff.dropna(inplace = True)\ntest_stationarity(train_log_moving_avg_diff)","2a8c9458":"train_log_diff = Train_log - Train_log.shift(1)\ntest_stationarity(train_log_diff.dropna())\n","9772a42a":"# Decomposing the time series into trend, seasonality and residual\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(pd.DataFrame(Train_log).feature_2.values, freq = 24)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(Train_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","19948f43":"train_log_decompose = pd.DataFrame(residual)\ntrain_log_decompose['date'] = Train_log.index\ntrain_log_decompose.set_index('date', inplace = True)\ntrain_log_decompose.dropna(inplace=True)\ntest_stationarity(train_log_decompose[0])","74f7fbbb":"#ACF & PACF plots\nfrom statsmodels.tsa.stattools import acf, pacf\nlag_acf = acf(train_log_diff.dropna(), nlags=20)\nlag_pacf = pacf(train_log_diff.dropna(), nlags=20, method='ols')\nrcParams['figure.figsize']  =  10, 5\n#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y = 0, linestyle = '--', color = 'gray')\nplt.axhline(y = -1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = '--', color = 'gray')\nplt.axhline(y = 1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = '--', color = 'gray')\nplt.xticks(np.arange(0,22,2))\nplt.title('Autocorrelation Function')            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y = 0, linestyle = '--', color = 'gray')\nplt.axhline(y = -1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = '--', color = 'gray')\nplt.axhline(y = 1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = '--', color = 'gray')\nplt.xticks(np.arange(0,22,2))\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()            ","684f2f1f":"from statsmodels.tsa.arima_model import ARIMA\n\n# AR model\nmodel = ARIMA(Train_log, order=(2, 1, 0))  # here the q value is zero since it is just the AR model\nresults_AR = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(), label='original')\nplt.plot(results_AR.fittedvalues, color='blue', label='predictions')\nplt.legend(loc='best')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-train_log_diff.dropna())**2))\nplt.show()","f6affb2b":"AR_predict = results_AR.predict(start=\"2019-03-19 01:00:00\", end=\"2019-03-19 01:33:00\")\nAR_predict = AR_predict.cumsum().shift().fillna(0)\nAR_predict1 = pd.Series(np.ones(valid.shape[0]) * np.log(valid['feature_2'])[0], index = valid.index)\nAR_predict1 = AR_predict1.add(AR_predict,fill_value=0)\nAR_predict = np.exp(AR_predict1)","567a9c59":"plt.plot(valid['feature_2'], label = \"Valid\")\nplt.plot(AR_predict, color = 'red', label = \"Predict\")\nplt.legend(loc= 'best')\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(AR_predict, valid['feature_2']))\/valid.shape[0]))\nplt.show()","22b00b31":"# MA model\nARIMA = ARIMA(Train_log, order=(0, 1, 2))  # here the p value is zero since it is just the MA model\nresults_MA = ARIMA.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(), label='original')\nplt.plot(results_MA.fittedvalues, color='red', label='prediction')\nplt.legend(loc='best')\nplt.show()","7bcbf0eb":"MA_predict=results_MA.predict(start=\"2019-03-19 01:00:00\", end=\"2019-03-19 01:33:00\")\nMA_predict=MA_predict.cumsum().shift().fillna(0)\nMA_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['feature_2'])[0], index = valid.index)\nMA_predict1=MA_predict1.add(MA_predict,fill_value=0)\nMA_predict = np.exp(MA_predict1)","c1cebd96":"plt.plot(valid['feature_2'], label = \"Valid\")\nplt.plot(MA_predict, color = 'red', label = \"Predict\")\nplt.legend(loc= 'best')\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, valid['feature_2']))\/valid.shape[0]))\nplt.show()","5a74719f":"from statsmodels.tsa.arima_model import ARIMA\n\n# ARIMA model\nmodel = ARIMA(Train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(),  label='original')\nplt.plot(results_ARIMA.fittedvalues, color='red', label='predicted')\nplt.legend(loc='best')\nplt.show()","27d65662":"# Rescaling the values to original\ndef check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['feature_2'])[0], index = given_set.index)\n    predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_log)\n    \n    plt.plot(given_set['feature_2'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['feature_2']))\/given_set.shape[0]))\n    plt.show()","7023427b":"def check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n    \n    plt.plot(given_set['feature_2'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['feature_2']))\/given_set.shape[0]))\n    plt.show()","63f8faf0":"ARIMA_predict_diff=results_ARIMA.predict(start=\"2019-03-19 01:00:00\", end=\"2019-03-19 01:33:00\")","60bbb7ff":"check_prediction_diff(ARIMA_predict_diff, valid)","584729f6":"import statsmodels.api as sm\n\n# SARIMAX model\ny_hat_avg = valid.copy()\nSARIMA = sm.tsa.statespace.SARIMAX(df_train.feature_2, order=(1,0,1),seasonal_order=(0,1,1,7))\nfit1 = SARIMA.fit()\ny_hat_avg['SARIMA'] = fit1.predict(start=\"2019-03-19 01:00:00\", end=\"2019-03-19 01:33:00\", dynamic=True)\nplt.figure(figsize=(16,8))\nplt.plot( Train['feature_2'], label='Train')\nplt.plot(valid['feature_2'], label='Valid')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')\nplt.show()\n","bbe513de":"#calculating rmse value\nrmse = sqrt(mean_squared_error(valid.feature_2, y_hat_avg.SARIMA))\nprint('RMSE value is :',rmse)","67aa6e66":"\npredict=fit1.predict(start=\"2019-03-19 01:34:00\", end=\"2019-03-19 02:36:20\", dynamic=True)","57958584":"df_test['feature_2']=predict","00b734d0":"df_test.tail()","a3814943":"submission = df_test.drop(['time','feature_1','Datetime','year','month','day','Hour','seconds','minute'],axis=1)","41601dd4":"ids = df_test['id']\nfeature_2 = df_test['feature_2']","22aeee59":"# set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({'Id' : ids, 'Feature_2' : feature_2})\noutput.to_csv('Submission.csv', index=False)","af176f4f":"# [__Visualizing the dataset__](http:\/\/)","96f7dae2":"# [Sarimax Model](http:\/\/)\n","7f5ad83d":"# [Plotting ACF & PACF](http:\/\/)","bd869a4e":"# Validating the AR model","abe806df":"# [Data Spliting](http:\/\/)","4767ab79":"This looks so messy. hence no insight hear also so it was getting harder to make assumptions.\n\n","89f692a9":"we can see only 2 hours are present have no much diffrence in values. it wont give us much insights\n\nnow let's try to at understand data by grouping Month and minute","3558e1c7":"# [Make a Time Series Stationary](http:\/\/)","7040d035":"# [Creating a dataframe for Submission](http:\/\/)","af0cffe5":"only one year data given that is 2019\n\nnow let's see how our data is distributed based on time hours","a6fdfc4e":"# Predicting using SARIMAX model","8310839d":"# Predicting using AR model","f7dd6f4e":"# Predicting using MA model","e746b0b6":"# Predicting using ARIMA","9c8e23fd":"# Validating the predictions","14861f9d":"![Imgur](https:\/\/i.imgur.com\/UgzDMlj.gif)","4eb6bb81":"we can see that time is changeing by 15 min with respect to that our Fature 2 is going upwards.\n\nnow lets try to plot joint plot for both fetures using seaborn library","4675cee9":"it seeames there are some outliers are present. but only one is present so i am leaveing this as it is.\n\nour data is yearly distributed or only one year we can check it by ploting","4423d303":"# [Building AR, ARIMA Models](http:\/\/)","a2c61f0e":"# Validating the MA model"}}