{"cell_type":{"abacc8da":"code","41c61aa2":"code","53cec428":"code","0ceafdfb":"code","440f8ef3":"code","31c49a53":"code","5bb20f07":"code","8313e3eb":"code","e1e41d41":"code","4859dee0":"code","bacbd6f5":"code","9075ec03":"code","de6383b7":"code","c9167679":"code","0973c5c1":"code","961d6a97":"code","f2de2fb9":"code","dbc44cbd":"code","bb46f51b":"code","1c638483":"code","59ea7654":"code","21601408":"code","d945a4aa":"code","7b88be26":"code","993ba2f1":"code","39d6d02d":"code","5105264e":"code","c7f3fe13":"code","1ca7b7c4":"code","ec0b7788":"code","ca1aac75":"code","6a463572":"code","e87faf7e":"code","793d8263":"code","760b5d0a":"code","4be6d61c":"code","d0c50a3f":"code","ceb9f484":"code","e8614b02":"code","3807496a":"code","b9ce5ea4":"code","836c8134":"code","25ccc7e6":"code","30dca6f4":"code","c51aedc6":"code","d1d15337":"code","0baba71b":"code","c0f51e1f":"code","ef6b83eb":"code","d7fad3e3":"code","cda935d4":"code","955791aa":"code","76fc1da5":"code","17cdb92c":"code","cc5bf5a7":"code","2aa1c8d5":"code","8cc0f9d2":"code","65593a77":"code","1ce6cf22":"code","4da1e32f":"code","2e7504cb":"code","d953b1ea":"code","9c2e8985":"code","72d513cb":"code","55a46887":"code","66603f72":"code","9b8fb05d":"code","78f99018":"code","4b3516a5":"code","893945c4":"code","e0ad68b3":"code","d3201dd8":"code","d71af379":"code","9c0b2d1e":"code","1d6c91fb":"code","c1a0e609":"code","5dff70b1":"code","bc95311c":"code","71bb26bf":"code","e0801f4c":"markdown","08f29b66":"markdown","0594032d":"markdown","e96d5bed":"markdown","d806f8de":"markdown","a1340640":"markdown","e6838317":"markdown","f494c122":"markdown","08f4f773":"markdown","7ea0d171":"markdown","88f54187":"markdown","7f8961a3":"markdown","87f24767":"markdown","f8a2c679":"markdown","538c449d":"markdown","d4895aa2":"markdown","74cab696":"markdown","2dedafeb":"markdown","6f473f7e":"markdown","5b41a685":"markdown","861bbcf1":"markdown","23565a54":"markdown","7b017928":"markdown","3f410e95":"markdown","c31a790a":"markdown","f5f90e4d":"markdown","ed06b7fc":"markdown","d354e0cb":"markdown","a4e9ee21":"markdown","156e2c7e":"markdown","2c776fec":"markdown","8aea4a69":"markdown"},"source":{"abacc8da":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom learntools.core import *\n\npd.pandas.set_option('display.max_columns', None)\n\n# train_data = pd.read_csv('..\/input\/train.csv')\n# test_data = pd.read_csv('..\/input\/test.csv')\n\n# # Drop houses where the target is missing\n# train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# target = train_data.SalePrice\n\n# X_train = train_data.drop(['SalePrice'], axis=1)","41c61aa2":"dataset = pd.read_csv('..\/input\/train.csv')\ndataset.shape","53cec428":"dataset.head()","0ceafdfb":"features_with_na = [features for features in dataset.columns if dataset[features].isnull().sum() > 1]\n\nfor feature in features_with_na:\n    print(feature, np.round(dataset[feature].isnull().mean(), 4), '% missing values')","440f8ef3":"for feature in features_with_na:\n    data =  dataset.copy()\n    \n    # Putting 1 where there's a missing valu and 0 otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # Calculating Mean sale price where there are missing values\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","31c49a53":"# If data type of a feature is not object, it means it's a numerical feature\nnumerical_features = [features for features in dataset.columns if dataset[features].dtype != 'O']\n\n# Counting the numerical features\nprint('No. of Numerical Features:', len(numerical_features))\n\n# Let's have a look at these features\ndataset[numerical_features].head()","5bb20f07":"# Let's see how many year columns do we have\nyear_cols = [features for features in dataset.columns if 'Year' in features or 'Yr' in features]\n\nprint(str(len(year_cols)) + ' year columns')\n\ndataset[year_cols].head()","8313e3eb":"# Let's see the no of unique features in year cols\n[print(feature, dataset[feature].unique()) for feature in year_cols]","e1e41d41":"# Let's look at the relationship between year sold and sale price\ndata.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('YrSold')\nplt.ylabel('Median House Price')\nplt.title('House Price VS Year Sold')\nplt.show()","4859dee0":"for feature in year_cols:\n    if feature != 'YrSold':\n        data['Difference'] = data['YrSold'] - data[feature]\n        print('\\n')\n        print(data[['YrSold', feature, 'Difference']].head())\n        print('\\n')\n        plt.scatter(data['Difference'], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.title('YrSold - ' + feature )\n        plt.show()","bacbd6f5":"discrete_features = [feature for feature in numerical_features if len(data[feature].unique()) < 25 and feature not in year_cols + ['id']]\nprint('No of discrete features: {}'.format(len(discrete_features)))\n\ndata[discrete_features].head()","9075ec03":"# Let's look at the relationship between discrete features and sale price\nfor feature in discrete_features:\n    data = dataset.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Median House Price')\n    plt.title('House Price VS {}'.format(feature))\n    plt.show()","de6383b7":"continuous_features = [feature for feature in numerical_features if feature not in discrete_features + year_cols + ['Id']]\nprint('No of continuous features: {}'.format(len(continuous_features)))\n\ndata[continuous_features].head()","c9167679":"# Let's look at the relationship between discrete features and sale price\nfor feature in continuous_features:\n    data = dataset.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.title(feature)\n    plt.show()","0973c5c1":"# Logarithimic transformation, Log normal distribution\nfor feature in continuous_features:\n    data = dataset.copy()\n    \n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        plt.scatter(data[feature], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('Sale Price')\n        plt.title(feature)\n        plt.show()\n","961d6a97":"# Outliers\nfor feature in continuous_features:\n    data = dataset.copy()\n    \n#     if 0 in data[feature].unique():\n#         pass\n#     else:\n    data[feature] = np.log1p(data[feature])\n    data.boxplot(column=feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","f2de2fb9":"# Categorical features\ncategorical_features = [feature for feature in dataset.columns if data[feature].dtypes == 'O']\nprint('No of categorical features: {}'.format(len(continuous_features)))\n\ndata[categorical_features].head()","dbc44cbd":"# no of categories in categorical features\nfor feature in categorical_features:\n    print(\"The feature {} has {} categories\".format(feature, len(dataset[feature].unique())))","bb46f51b":"for feature in categorical_features:\n    data = dataset.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Sale Price')\n    plt.title(feature)\n    plt.show()","1c638483":"null_value_cat_features = [feature for feature in categorical_features if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes=='O']\n\nfor feature in null_value_cat_features:\n    print(\"{}: {}% missing values\".format(feature, np.round(dataset[feature].isnull().mean(), 4)))","59ea7654":"# Replace missing values with a new category 'Missing'\ndef replace_cat_missing_vals(dataset, null_value_cat_features):\n    data = dataset.copy()\n    data[null_value_cat_features] = data[null_value_cat_features].fillna('Missing')\n    return data\n\ndataset = replace_cat_missing_vals(dataset, null_value_cat_features)\ndataset[null_value_cat_features].isnull().sum()","21601408":"dataset.head()","d945a4aa":"null_value_numerical_features = [feature for feature in numerical_features if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes !='O']\n\nfor feature in null_value_numerical_features:\n    print(\"{}: {}% missing values\".format(feature, np.round(dataset[feature].isnull().mean(), 4)))","7b88be26":"# Replace numerical features missing values using median\nfor feature in null_value_numerical_features:\n    median_value = dataset[feature].median()\n    \n    # create a new feature column that is 1 if nan value for a feature and 0 if not \n    dataset[feature+'_nan'] = np.where(dataset[feature].isnull(), 1, 0)\n    dataset[feature].fillna(median_value, inplace=True)\n    \ndataset[null_value_numerical_features].isnull().sum()\ndataset.head(10)","993ba2f1":"# Removing YrSold column from year columns\ntemp_year_cols = year_cols.copy()\ntemp_year_cols.remove('YrSold')\n\n# Difference of year features from year in which the house was sold\nfor feature in temp_year_cols:\n    dataset[feature] = dataset['YrSold'] - dataset[feature]\n\ndataset[temp_year_cols].head()","39d6d02d":"# select only those skewed features that do not have 0 value\nnon_zero_continuous_features = [feature for feature in continuous_features if not dataset[feature].isin([0]).any().any()]\n# non_zero_continuous_features.remove('SalePrice')\nprint(non_zero_continuous_features)","5105264e":"# converting skewed features to log normal distribution\n\nfor feature in non_zero_continuous_features:\n    dataset[feature] = np.log(dataset[feature])\n    \ndataset.head()","c7f3fe13":"for feature in categorical_features:\n    temp = dataset.groupby(feature)['SalePrice'].count()\/len(dataset)\n    temp_df = temp[temp>0.01].index\n    dataset[feature] = np.where(dataset[feature].isin(temp_df), dataset[feature], 'rare_var')\n    \ndataset.head(35)","1ca7b7c4":"dataset.groupby(['MSZoning'])['SalePrice'].mean()","ec0b7788":"# convert categorical features to numbers to be understood by the model using label encoding\nfrom sklearn import preprocessing\n\nlabel_encoder = preprocessing.LabelEncoder()\nfor feature in categorical_features:\n    dataset[feature] = label_encoder.fit_transform(dataset[feature])\n    dataset[feature].unique()\n\ndataset[categorical_features].head()\n# for feature in categorical_features:\n#     labels_ordered=dataset.groupby([feature])['SalePrice'].mean().sort_values().index\n#     labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n#     dataset[feature]=dataset[feature].map(labels_ordered)","ca1aac75":"dataset.head()","6a463572":"features_to_be_scaled = [feature for feature in dataset.columns if feature not in ['Id', 'SalePrice']]\n\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(dataset[features_to_be_scaled])\nscaler.transform(dataset[features_to_be_scaled])","e87faf7e":"data = pd.concat(\n    [dataset[['Id', 'SalePrice']].reset_index(drop=True), \n    pd.DataFrame(scaler.transform(dataset[features_to_be_scaled]), \n    columns=features_to_be_scaled)], \n    axis=1)\n\ndata.head()","793d8263":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\ny_train = data[['SalePrice']]\nx_train = data.drop(['Id', 'SalePrice'], axis=1)\n\nx_train.head()","760b5d0a":"### Apply Feature Selection\n# first, I specify the Lasso Regression model, and I\n# select a suitable alpha (equivalent of penalty).\n# The bigger the alpha the less features that will be selected.\n\n# Then I use the selectFromModel object from sklearn, which\n# will select the features which coefficients are non-zero\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0))  # remember to set the seed, the random state in this function\nfeature_sel_model.fit(x_train, y_train)","4be6d61c":"feature_sel_model.get_support()","d0c50a3f":"\n# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feat = x_train.columns[(feature_sel_model.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((x_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n    np.sum(feature_sel_model.estimator_.coef_ == 0)))","ceb9f484":"x_train = x_train[selected_feat]\nx_train.head()","e8614b02":"test_dataset = pd.read_csv('..\/input\/test.csv')\ntest_dataset.shape","3807496a":"test_dataset.head()","b9ce5ea4":"features_with_na = [features for features in test_dataset.columns if test_dataset[features].isnull().sum() > 1]\n\nfor feature in features_with_na:\n    print(feature, np.round(test_dataset[feature].isnull().mean(), 4), '% missing values')","836c8134":"# If data type of a feature is not object, it means it's a numerical feature\nnumerical_features = [features for features in test_dataset.columns if test_dataset[features].dtype != 'O']\n\n# Counting the numerical features\nprint('No. of Numerical Features:', len(numerical_features))\n\n# Let's have a look at these features\ndataset[numerical_features].head()","25ccc7e6":"# Let's see how many year columns do we have\nyear_cols = [features for features in test_dataset.columns if 'Year' in features or 'Yr' in features]\n\nprint(str(len(year_cols)) + ' year columns')\n\ntest_dataset[year_cols].head()","30dca6f4":"# Let's see the no of unique features in year cols\n[print(feature, test_dataset[feature].unique()) for feature in year_cols]","c51aedc6":"for feature in year_cols:\n    if feature != 'YrSold':\n        test_dataset['Difference'] = test_dataset['YrSold'] - test_dataset[feature]\n        print('\\n')\n        print(test_dataset[['YrSold', feature, 'Difference']].head())","d1d15337":"discrete_features = [feature for feature in numerical_features if len(test_dataset[feature].unique()) < 25 and feature not in year_cols + ['id']]\nprint('No of discrete features: {}'.format(len(discrete_features)))\n\ntest_dataset[discrete_features].head()","0baba71b":"continuous_features = [feature for feature in numerical_features if feature not in discrete_features + year_cols + ['Id']]\nprint('No of continuous features: {}'.format(len(continuous_features)))\n\ntest_dataset[continuous_features].head()","c0f51e1f":"for feature in continuous_features:\n    data = test_dataset.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.title(feature)\n    plt.show()","ef6b83eb":"# Logarithimic transformation, Log normal distribution\nfor feature in continuous_features:\n    data = test_dataset.copy()\n    data[feature] = np.log1p(data[feature])\ndata[continuous_features].head()","d7fad3e3":"# Outliers\nfor feature in continuous_features:\n    data = test_dataset.copy()\n    data[feature] = np.log1p(data[feature])\n    data.boxplot(column=feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","cda935d4":"# Categorical features\ncategorical_features = [feature for feature in test_dataset.columns if data[feature].dtypes == 'O']\nprint('No of categorical features: {}'.format(len(continuous_features)))\n\ndata[categorical_features].head()","955791aa":"# no of categories in categorical features\nfor feature in categorical_features:\n    print(\" {} has {} categories\".format(feature, len(test_dataset[feature].unique())))","76fc1da5":"null_value_cat_features = [feature for feature in categorical_features if test_dataset[feature].isnull().sum()>1 and test_dataset[feature].dtypes=='O']\n\nfor feature in null_value_cat_features:\n    print(\"{}: {}% missing values\".format(feature, np.round(test_dataset[feature].isnull().mean(), 4)))","17cdb92c":"# Replace missing values with a new category 'Missing'\ntest_dataset = replace_cat_missing_vals(test_dataset, null_value_cat_features)\ntest_dataset[null_value_cat_features].isnull().sum()","cc5bf5a7":"test_dataset.head()","2aa1c8d5":"null_value_numerical_features = [feature for feature in numerical_features if test_dataset[feature].isnull().sum()>1 and test_dataset[feature].dtypes !='O']\n\nfor feature in null_value_numerical_features:\n    print(\"{}: {}% missing values\".format(feature, np.round(test_dataset[feature].isnull().mean(), 4)))","8cc0f9d2":"# Replace numerical features missing values using median\nfor feature in null_value_numerical_features:\n    median_value = test_dataset[feature].median()\n    \n    # create a new feature column that is 1 if nan value for a feature and 0 if not \n    test_dataset[feature+'_nan'] = np.where(test_dataset[feature].isnull(), 1, 0)\n    test_dataset[feature].fillna(median_value, inplace=True)\n    \ntest_dataset[null_value_numerical_features].isnull().sum()\ntest_dataset.head(10)","65593a77":"# Removing YrSold column from year columns\ntemp_year_cols = year_cols.copy()\ntemp_year_cols.remove('YrSold')\n\n# Difference of year features from year in which the house was sold\nfor feature in temp_year_cols:\n    test_dataset[feature] = dataset['YrSold'] - test_dataset[feature]\n\ntest_dataset[temp_year_cols].head()","1ce6cf22":"# select only those skewed features that do not have 0 value\nnon_zero_continuous_features = [feature for feature in continuous_features if not test_dataset[feature].isin([0]).any().any()]\n# non_zero_continuous_features.remove('SalePrice')\nprint(non_zero_continuous_features)","4da1e32f":"# converting skewed features to log normal distribution\n\nfor feature in non_zero_continuous_features:\n    test_dataset[feature] = np.log(test_dataset[feature])\n    \ntest_dataset.head()","2e7504cb":"test_dataset = test_dataset.drop(['Difference'], axis=1)\ntest_dataset.head()","d953b1ea":"for feature in categorical_features:\n    temp = test_dataset.groupby(feature).count()\/len(test_dataset)\n    temp_df = temp[temp>0.01].index\n    test_dataset[feature] = np.where(test_dataset[feature].isin(temp_df), test_dataset[feature], 'rare_var')\n    \ntest_dataset.head()","9c2e8985":"# convert categorical features to numbers to be understood by the model using label encoding\nfrom sklearn import preprocessing\n\nlabel_encoder = preprocessing.LabelEncoder()\n\nfor feature in categorical_features:\n    test_dataset[feature] = label_encoder.fit_transform(test_dataset[feature])\n    test_dataset[feature].unique()\n\ntest_dataset[categorical_features].head()","72d513cb":"features_to_be_scaled = [feature for feature in test_dataset.columns if feature not in ['Id']]\n\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(test_dataset[features_to_be_scaled])\nscaler.transform(test_dataset[features_to_be_scaled])","55a46887":"data = pd.concat(\n    [test_dataset['Id'].reset_index(drop=True), \n    pd.DataFrame(scaler.transform(test_dataset[features_to_be_scaled]), \n    columns=features_to_be_scaled)], \n    axis=1)\n\ndata.head()","66603f72":"#drop nan columns\nnan_cols = [feature for feature in data.columns if 'nan' in feature]\n\nprint(nan_cols)","9b8fb05d":"data = data.drop(nan_cols, axis=1)\ndata.head(10)","78f99018":"train_data = dataset.copy()\n#drop nan columns\nnan_cols = [feature for feature in train_data.columns if 'nan' in feature]\n\nprint(nan_cols)","4b3516a5":"train_data = train_data.drop(nan_cols, axis=1)\ntrain_data.head(10)","893945c4":"x_test = data[selected_feat]\nx_test.head()","e0ad68b3":"# from sklearn.impute import SimpleImputer\n\n# imputed_X_train_plus = X_train.copy()\n# imputed_X_test_plus = test_data.copy()\n\n# cols_with_missing = (col for col in X_train.columns \n#                                  if X_train[col].isnull().any())\n# for col in cols_with_missing:\n#     imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n#     imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# # Imputation\n# my_imputer = SimpleImputer()\n# imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(imputed_X_train_plus.select_dtypes(exclude=['object'])))\n# imputed_X_test_plus = pd.DataFrame(my_imputer.transform(imputed_X_test_plus.select_dtypes(exclude=['object'])))\n","d3201dd8":"# # \"cardinality\" means the number of unique values in a column.\n# low_cardinality_cols = [cname for cname in imputed_X_train_plus.columns if \n#                                 imputed_X_train_plus[cname].nunique() < 10 and\n#                                 imputed_X_train_plus[cname].dtype == \"object\"]\n# numeric_cols = [cname for cname in imputed_X_train_plus.columns if \n#                                 imputed_X_train_plus[cname].dtype in ['int64', 'float64']]\n# my_cols = low_cardinality_cols + numeric_cols\n# train_predictors = imputed_X_train_plus[my_cols]\n# test_predictors = imputed_X_test_plus[my_cols]\n\n# one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n# one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n# final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n#                                                                     join='left', \n#                                                                     axis=1)","d71af379":"# from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n# from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n\n# cols_to_use = ['YearBuilt', 'MSSubClass', 'LotArea']\n\n# def get_some_data():\n#     data = pd.read_csv('..\/input\/train.csv')\n#     y = data.SalePrice\n#     X = data[cols_to_use]\n#     my_imputer = SimpleImputer()\n#     imputed_X = my_imputer.fit_transform(X)\n#     return imputed_X, y\n    \n\n# X, y = get_some_data()\n# my_model = GradientBoostingRegressor()\n# my_model.fit(X, y)\n# my_plots = plot_partial_dependence(my_model, \n#                                    features=[0,2], \n#                                    X=X, \n#                                    feature_names=cols_to_use, \n#                                    grid_resolution=10)\n","9c0b2d1e":"from sklearn.ensemble import RandomForestRegressor\n\n# To improve accuracy, create a new Random Forest model which you will train on all training data\nrf_model_on_full_data = RandomForestRegressor(random_state=1)\n\n# fit rf_model_on_full_data on all data from the training data\nrf_model_on_full_data.fit(x_train, y_train)\n","1d6c91fb":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000)\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(x_train, y_train, verbose=False)","c1a0e609":"# make predictions with XGBoost\npredictions = my_model.predict(x_test)\noutput = pd.DataFrame({'Id': data.Id,\n                      'SalePrice': predictions})\nprint(output)\noutput.to_csv('submission.csv', index=False)","5dff70b1":"# # make predictions with Random Forest\n# test_preds = rf_model_on_full_data.predict(x_test)\n\n# # The lines below shows how to save predictions in format used for competition scoring\n# # Just uncomment them.\n\n# output = pd.DataFrame({'Id': data.Id,\n#                       'SalePrice': test_preds})\n# print(output)\n# output.to_csv('submission.csv', index=False)","bc95311c":"# from sklearn.pipeline import make_pipeline\n# from sklearn.impute import SimpleImputer\n\n# # make predictions with Random Forest using pipeline\n# my_pipeline = make_pipeline(SimpleImputer(), RandomForestRegressor())\n\n# my_pipeline.fit(final_train, target)\n# predictions = my_pipeline.predict(final_test)\n\n# output = pd.DataFrame({'Id': test_data.Id,\n#                       'SalePrice': predictions})\n# print(output)\n# output.to_csv('submission.csv', index=False)","71bb26bf":"# from sklearn.pipeline import make_pipeline\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import cross_val_score\n\n# scores = cross_val_score(my_pipeline, final_train, target, scoring='neg_mean_absolute_error')\n# print(scores)\n# print('Mean Absolute Error %2f' %(-1 * scores.mean()))","e0801f4c":"# 1. Exploratory Data Analysis (EDA)","08f29b66":"# Creating a Model For the Competition\n\nBuild a Random Forest model and train it on all of **X** and **y**.  ","0594032d":"**Handling Missing Valuse for Temporal Features**","e96d5bed":"# Interpreting Partial Dependence Plots","d806f8de":"The above analysis shows that the greater the missing values, the higher the price. Hence, cannot drop these instead, we need to impute them.","a1340640":"**Handling Missing Valuse for Numerical Features**","e6838317":"# Handling Missing Data Using Imputation","f494c122":"# Test Your Work\nAfter filling in the code above:\n1. Click the **Commit and Run** button. \n2. After your code has finished running, click the small double brackets **<<** in the upper left of your screen.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n3. Go to the output tab at top of your screen. Select the button to submit your file to the competition.  \n4. If you want to keep working to improve your model, select the edit button. Then you can change your model and repeat the process.\n\nCongratulations, you've started competing in Machine Learning competitions.\n\n# Continuing Your Progress\nThere are many ways to improve your model, and **experimenting is a great way to learn at this point.**\n\nThe best way to improve your model is to add features.  Look at the list of columns and think about what might affect home prices.  Some features will cause errors because of issues like missing values or non-numeric data types. \n\nLevel 2 of this micro-course will teach you how to handle these types of features. You will also learn to use **xgboost**, a technique giving even better accuracy than Random Forest.\n\n\n# Other Micro-Courses\nThe **[Pandas Micro-Course](https:\/\/kaggle.com\/Learn\/Pandas)** will give you the data manipulation skills to quickly go from conceptual idea to implementation in your data science projects. \n\nYou are also ready for the **[Deep Learning](https:\/\/kaggle.com\/Learn\/Deep-Learning)** micro-course, where you will build models with better-than-human level performance at computer vision tasks.","08f4f773":"**Handle rare Categorical Features**","7ea0d171":"# Feature Engineering","88f54187":"**Handling Missing Valuse for Numerical Features**","7f8961a3":"# Feature Scaling","87f24767":"**Handle Missing Values For Categorical features**","f8a2c679":"# Pipeline","538c449d":"# Cross-Validation","d4895aa2":"---\n**[Machine Learning Micro-Course Home Page](https:\/\/www.kaggle.com\/learn\/machine-learning)**\n\n","74cab696":"# Make Predictions\nRead the file of \"test\" data. And apply your model to make predictions","2dedafeb":"# Introduction\nMachine learning competitions are a great way to improve your data science skills and measure your progress. \n\nIn this exercise, you will create and submit predictions for a Kaggle competition. You can then improve your model (e.g. by adding features) to improve and see how you stack up to others taking this micro-course.\n\nThe steps in this notebook are:\n1. Build a Random Forest model with all of your data (**X** and **y**)\n2. Read in the \"test\" data, which doesn't include values for the target.  Predict home values in the test data with your Random Forest model.\n3. Submit those predictions to the competition and see your score.\n4. Optionally, come back to see if you can improve your model by adding features or changing your model. Then you can resubmit to see how that stacks up on the competition leaderboard.","6f473f7e":"# Using Categorical Data with One Hot Encoding","5b41a685":"# 3. Feature Selection","861bbcf1":"# 2. Feature Engineering","23565a54":"**Handle rare Categorical Features**\n\nRemove categories that are less than 1% of the total observations in a feature and categorize them as 'rare_var'","7b017928":"**Test Data**\n1. EDA\n2. Feature Engineering","3f410e95":"# XGBoost ","c31a790a":"# 1. EDA","f5f90e4d":"**Handle Missing Values For Categorical features**","ed06b7fc":"**[Machine Learning Micro-Course Home Page](https:\/\/www.kaggle.com\/learn\/machine-learning)**\n\n---\n","d354e0cb":"**Train Data**","a4e9ee21":"**Feature Scaling**","156e2c7e":"**Handling Missing Valuse for Temporal Features**","2c776fec":"# Random Forest","8aea4a69":"# Basic Setup"}}