{"cell_type":{"4afa1203":"code","c8fabeb3":"code","3310f314":"code","2452bc51":"code","c30f9f19":"code","203088d9":"code","dc5d6128":"code","c5cde7ae":"code","d977caa6":"code","49454c9d":"code","2b292840":"markdown","6132a14f":"markdown","278aa66a":"markdown","58ce6755":"markdown","90abc267":"markdown","6a812dc6":"markdown","f6ce00a7":"markdown"},"source":{"4afa1203":"from keras.layers import Input, Dense, Dropout, Conv1D, Flatten, MaxPooling1D, Concatenate, Multiply, BatchNormalization\nfrom keras.models import Model, Sequential\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras import regularizers\nfrom sklearn.model_selection import KFold \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom keras.callbacks import Callback\nfrom sklearn.linear_model import LogisticRegression, BayesianRidge\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.manifold import TSNE\nimport lightgbm as lgb\nimport math\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nimport pandas as pd \nimport numpy as np\nfrom sklearn.metrics import roc_curve, auc\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport matplotlib\nfrom sklearn.naive_bayes import GaussianNB\nnp.random.seed(203)\nimport math\nfrom scipy.stats import ks_2samp\nfrom sklearn.ensemble import RandomForestRegressor\nfrom keras.callbacks import ModelCheckpoint\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c8fabeb3":"targets = pd.read_csv(\"..\/input\/y_train.csv\")\ndf_train = pd.read_csv(\"..\/input\/X_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/X_test.csv\")","3310f314":"targets = pd.get_dummies(targets, columns=[\"surface\"])\nY = targets.drop([\"series_id\", \"group_id\"], axis=1).values\nX = df_train.drop([\"row_id\", \"series_id\", \"measurement_number\"], axis=1).values\ntest = df_test.drop([\"row_id\", \"series_id\", \"measurement_number\"], axis=1).values\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = X.reshape((3810, 128, 10, 1))\nX = X.transpose(0, 2, 1, 3)\ntest = scaler.transform(test)\ntest = test.reshape((3816, 128, 10, 1))\ntest = test.transpose(0, 2, 1, 3)","2452bc51":"#Every column gets the smoothness features, except for the ones in the list below\nfor col in df_train.drop([\"row_id\", \"series_id\", \"measurement_number\"], axis=1).columns:\n    df_train[\"diff_\"+col] = df_train[col]-df_train[col].shift(1)\n    df_test[\"diff_\"+col] = df_test[col]-df_test[col].shift(1)\n    df_train[\"ma_\"+col] = np.square(abs(df_train[col]-df_train[col].rolling(8).mean()))\n    df_test[\"ma_\"+col] = np.square(abs(df_test[col]-df_test[col].rolling(8).mean()))\n\n#The first 8 measurement (0-7) contain data from the previous series id (because of the rolling mean), so they have to be dropped\ndf_train = df_train[df_train[\"measurement_number\"]>7]\ndf_test = df_test[df_test[\"measurement_number\"]>7]\n\naggs = {\n    \"diff_orientation_X\":\"std\",\n    \"diff_orientation_Y\":\"std\",\n    \"diff_orientation_Z\":\"std\",\n    \"diff_orientation_W\":\"std\",\n    \"diff_angular_velocity_X\":\"std\",\n    \"diff_angular_velocity_Y\":\"std\",\n    \"diff_angular_velocity_Z\":\"std\",\n    \"diff_linear_acceleration_X\":\"std\",\n    \"diff_linear_acceleration_Y\":\"std\",\n    \"diff_linear_acceleration_Z\":\"std\",\n    \"ma_orientation_X\":\"sum\",\n    \"ma_orientation_Y\":\"sum\",\n    \"ma_orientation_Z\":\"sum\",\n    \"ma_orientation_W\":\"sum\",\n    \"ma_angular_velocity_X\":\"sum\",\n    \"ma_angular_velocity_Y\":\"sum\",\n    \"ma_angular_velocity_Z\":\"sum\",\n    \"ma_linear_acceleration_X\":\"sum\",\n    \"ma_linear_acceleration_Y\":\"sum\",\n    \"ma_linear_acceleration_Z\":\"sum\"\n}\n\ntrain_smooth = df_train.groupby(\"series_id\").agg(aggs)\ntest_smooth = df_test.groupby(\"series_id\").agg(aggs)\ntrain_smooth.reset_index(inplace=True)\ntest_smooth.reset_index(inplace=True)","c30f9f19":"print(train_smooth.shape) #should be (3810, 20), because 10 columns got 2 additional features each\ntrain_smooth.head()","203088d9":"X_smooth = train_smooth.drop([\"series_id\"], axis=1).values\ntest_smooth = test_smooth.drop([\"series_id\"], axis=1).values\n\nscaler = StandardScaler()\nX_smooth = scaler.fit_transform(X_smooth)\ntest_smooth = scaler.transform(test_smooth)","dc5d6128":"def init_model():\n    \n    #These features occur frequently throughout, so for easu of use, it's easier to change them up here.\n    FIRST = 30 #20\n    SECOND = 20 #10\n    HEIGHT1 = 4 #4\n    HEIGHT2 = 3 #4\n    DROPOUT = 0.5\n    STRIDES = None\n    PS = 5\n    \n    input1 = Input(shape=(128, 1))\n    a = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input1)\n    a = BatchNormalization()(a)\n    a = MaxPooling1D(strides=STRIDES, pool_size=PS)(a)\n    a = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(a)\n    a = BatchNormalization()(a)\n    a = MaxPooling1D(strides=STRIDES, pool_size=PS)(a)\n    a = Flatten()(a)\n    a = Dropout(DROPOUT)(a)\n\n    input2 = Input(shape=(128, 1))\n    b = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input2)\n    b = BatchNormalization()(b)\n    b = MaxPooling1D(strides=STRIDES, pool_size=PS)(b)\n    b = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(b)\n    b = BatchNormalization()(b)\n    b = MaxPooling1D(strides=STRIDES, pool_size=PS)(b)\n    b = Flatten()(b)\n    b = Dropout(DROPOUT)(b)\n\n    input3 = Input(shape=(128, 1))\n    c = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input3)\n    c = BatchNormalization()(c)\n    c = MaxPooling1D(strides=STRIDES, pool_size=PS)(c)\n    c = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(c)\n    c = BatchNormalization()(c)\n    c = MaxPooling1D(strides=STRIDES, pool_size=PS)(c)\n    c = Flatten()(c)\n    c = Dropout(DROPOUT)(c)\n\n    input4 = Input(shape=(128, 1))\n    d = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input4)\n    d = BatchNormalization()(d)\n    d = MaxPooling1D(strides=STRIDES, pool_size=PS)(d)\n    d = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(d)\n    d = BatchNormalization()(d)\n    d = MaxPooling1D(strides=STRIDES, pool_size=PS)(d)\n    d = Flatten()(d)\n    d = Dropout(DROPOUT)(d)\n\n    input5 = Input(shape=(128, 1))\n    e = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input5)\n    e = BatchNormalization()(e)\n    e = MaxPooling1D(strides=STRIDES, pool_size=PS)(e)\n    e = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(e)\n    e = BatchNormalization()(e)\n    e = MaxPooling1D(strides=STRIDES, pool_size=PS)(e)\n    e = Flatten()(e)\n    e = Dropout(DROPOUT)(e)\n\n    input6 = Input(shape=(128, 1))\n    f = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input6)\n    f = BatchNormalization()(f)\n    f = MaxPooling1D(strides=STRIDES, pool_size=PS)(f)\n    f = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(f)\n    f = BatchNormalization()(f)\n    f = MaxPooling1D(strides=STRIDES, pool_size=PS)(f)\n    f = Flatten()(f)\n    f = Dropout(DROPOUT)(f)\n\n    input7 = Input(shape=(128, 1))\n    g = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input7)\n    g = BatchNormalization()(g)\n    g = MaxPooling1D(strides=STRIDES, pool_size=PS)(g)\n    g = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(g)\n    g = BatchNormalization()(g)\n    g = MaxPooling1D(strides=STRIDES, pool_size=PS)(g)\n    g = Flatten()(g)\n    g = Dropout(DROPOUT)(g)\n\n    input8 = Input(shape=(128, 1))\n    h = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input8)\n    h = BatchNormalization()(h)\n    h = MaxPooling1D(strides=STRIDES, pool_size=PS)(h)\n    h = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(h)\n    h = BatchNormalization()(h)\n    h = MaxPooling1D(strides=STRIDES, pool_size=PS)(h)\n    h = Flatten()(h)\n    h = Dropout(DROPOUT)(h)\n\n    input9 = Input(shape=(128, 1))\n    i = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input9)\n    i = BatchNormalization()(i)\n    i = MaxPooling1D(strides=STRIDES, pool_size=PS)(i)\n    i = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(i)\n    i = BatchNormalization()(i)\n    i = MaxPooling1D(strides=STRIDES, pool_size=PS)(i)\n    i = Flatten()(i)\n    i = Dropout(DROPOUT)(i)\n\n    input10 = Input(shape=(128, 1))\n    j = Conv1D(FIRST, HEIGHT1, activation=\"relu\", kernel_initializer=\"uniform\")(input10)\n    j = BatchNormalization()(j)\n    j = MaxPooling1D(strides=STRIDES, pool_size=PS)(j)\n    j = Conv1D(SECOND, HEIGHT2, activation=\"relu\", kernel_initializer=\"uniform\")(j)\n    j = BatchNormalization()(j)\n    j = MaxPooling1D(strides=STRIDES, pool_size=PS)(j)\n    j = Flatten()(j)\n    j = Dropout(DROPOUT)(j)\n    \n    input11 = Input(shape=(20,))\n    k = Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")(input11)\n    k = Dropout(0.25)(k)\n\n    merged = Concatenate()([a, b])\n    merged = Concatenate()([merged, c])\n    merged = Concatenate()([merged, d])\n    merged = Concatenate()([merged, e])\n    merged = Concatenate()([merged, f])\n    merged = Concatenate()([merged, g])\n    merged = Concatenate()([merged, h])\n    merged = Concatenate()([merged, i])\n    merged = Concatenate()([merged, j])\n    merged = Concatenate()([merged, k])\n    merged = Dense(30, activation = \"relu\", kernel_initializer=\"uniform\")(merged)\n    merged = Dropout(0.25)(merged)\n    \n    output = Dense(9, activation=\"softmax\", kernel_initializer=\"uniform\")(merged)\n    model = Model([input1, input2, input3, input4, input5, input6, input7, input8, input9, input10, input11], output)\n    return model","c5cde7ae":"te = [test[:,i] for i in range(10)]\nte.append(test_smooth)\n\nfolds = StratifiedKFold(n_splits=15, shuffle=True, random_state=0)\noof = np.zeros((len(X), 9))\ntest_predictions = np.zeros((len(test), 9))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X,np.argmax(Y, axis=1))):\n    print(fold_)\n    X_train, X_test = X[trn_idx], X[val_idx]\n    y_train, y_test = Y[trn_idx], Y[val_idx]\n    X_smooth_train, X_smooth_test = X_smooth[trn_idx], X_smooth[val_idx]\n    \n    #The model needs a list of inputs. The code beneath this creates a column-input for each unique column in the training set and appends it to the list. \n    X_tr = [X_train[:,i] for i in range(10)]\n    X_tr.append(X_smooth_train)\n    X_te = [X_test[:,i] for i in range(10)]\n    X_te.append(X_smooth_test)\n    \n    model = init_model()\n    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n    model.fit(X_tr, y_train, validation_data = (X_te, y_test), epochs=100, shuffle=True, class_weight=\"balanced\")\n    pre = model.predict(X_te)\n    oof[val_idx] = model.predict(X_te)\n    test_predictions+= model.predict(te)\/20","d977caa6":"predictions = np.argmax(test_predictions, axis=1)\ntrue = np.argmax(y_test, axis=1)\n\nlabels = {\n    0:\"carpet\",\n    1:\"concrete\",\n    2:\"fine_concrete\",\n    3:\"hard_tiles\",\n    4:\"hard_tiles_large_space\",\n    5:\"soft_pvc\",\n    6:\"soft_tiles\",\n    7:\"tiled\",\n    8:\"wood\"\n}\n\npre = []\nfor p in predictions:\n    pre.append(labels[p])","49454c9d":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub[\"surface\"] = np.array(pre)\nsub.to_csv(\"submission.csv\", index=False)","2b292840":"Please consider upvoting this kernel if you find it helpful in any way. And don't hesitate to ask any questions on why I'm doing what I'm doing. And finally (most importantly), please point out if I'm being dumb anywhere below so that I can fix it :).","6132a14f":"Now, to the fun part, building and training a Convolutional Neural Network and reformatting the input so that the CNN can train on it.","278aa66a":"It turns out the LB score is almost always better with a higher number of folds. (Be warned, this might take a few hours to train).","58ce6755":"In the following code block, I'm using some pandas tricks to obtain the aforementioned \"smoothness\" features from the test and train dataframes.","90abc267":"Just a quick check to ensure that the features turned out okay.","6a812dc6":"Initially, I wanted to see how much predictive power the series had without any feature engineering. My plan on doing this was to create a Neural Net with multiple inputs (One for each 128 observation feature). With quite a lot of parameter tuning and regularization, the model was able to consistently achieve a CV accuracy between 61 and 67.\n\nI then realized that to the model, certain textures were very similar (it kept on confusing the same classes). I wanted more features to be able to distinguish between these classes with a higher accuracy. Therefore, my goal was to create new features that aimed to establish the \"smoothness\" of all the readings. My hypthesis was that the smoothness of readings would help distinguish between similar classes. What are the \"smoothness\" features though? Firstly, the standard deviation of the differences between consecutive readings (a perfectly straight line will have a standard deviation of 0 because the differences are constant), and secondly, for more polynomial looking lines, the RMSE from the moving average of the line (If a set of readings has a lot of spikes, ie. isn't smooth, then the error wil be large).\n\nNow, moving on to the actual code. In this first section, I'm just extracting all the values from the pandas dataframe and reshaping the resulting numpy array into its individual series (Plus rescaling).","f6ce00a7":"The class probability predictions need to be converted to a surface text prediction. The code below does exactly that. "}}