{"cell_type":{"5572b93b":"code","af99bf3f":"code","f1d393a2":"code","511e0856":"code","0b905cc6":"code","36ecf0f3":"code","f35259e5":"code","830126af":"code","32eb688a":"code","217e481a":"code","56b487c1":"code","3423d009":"code","1af10c53":"code","e8f237ec":"code","92e85d2a":"code","0ab0237e":"code","e1e39d58":"code","cf0d8578":"code","2b086d8f":"code","42f7fc6c":"code","2f19989e":"code","3c0bf689":"code","b286e0c5":"markdown","14c4d9ba":"markdown","ff705ab1":"markdown","3288e6bb":"markdown","8b5671cf":"markdown","a01318bf":"markdown","5dec61c1":"markdown","039eb209":"markdown","23bccd56":"markdown","6f588194":"markdown"},"source":{"5572b93b":"# Libreries\n\nimport numpy as np\nimport pandas as pd\nimport os\n\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline","af99bf3f":"# Data path\nlabels = pd.read_csv('..\/input\/train.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')\ntrain_path = '..\/input\/train\/train\/'\ntest_path = '..\/input\/test\/test\/'\n","f1d393a2":"print('Num train samples:{0}'.format(len(os.listdir(train_path))))\nprint('Num test samples:{0}'.format(len(os.listdir(test_path))))","511e0856":"labels.head()","0b905cc6":"labels['has_cactus'].value_counts()","36ecf0f3":"lab = 'Has cactus','Hasn\\'t cactus'\ncolors=['green','brown']\n\nplt.figure(figsize=(7,7))\nplt.pie(labels.groupby('has_cactus').size(), labels=lab,\n        labeldistance=1.1, autopct='%1.1f%%',\n        colors=colors,shadow=True, startangle=140)\nplt.show()","f35259e5":"fig,ax = plt.subplots(1,5,figsize=(15,3))\n\nfor i, idx in enumerate(labels[labels['has_cactus']==1]['id'][-5:]):\n  path = os.path.join(train_path,idx)\n  ax[i].imshow(cv2.imread(path)) # [...,[2,1,0]]","830126af":"fig,ax = plt.subplots(1,5,figsize=(15,3))\n\nfor i, idx in enumerate(labels[labels['has_cactus']==0]['id'][-5:]):\n  path = os.path.join(train_path,idx)\n  ax[i].imshow(cv2.imread(path)) # [...,[2,1,0]]","32eb688a":"# Libreries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom sklearn.model_selection import train_test_split","217e481a":"## Parameters for model\n\n# Hyper parameters\nnum_epochs = 25\nnum_classes = 2\nbatch_size = 128\nlearning_rate = 0.002\n\n# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","56b487c1":"# data splitting\ntrain, val = train_test_split(labels, stratify=labels.has_cactus, test_size=0.1)\ntrain.shape, val.shape","3423d009":"train['has_cactus'].value_counts()","1af10c53":"val['has_cactus'].value_counts()","e8f237ec":"# NOTE: class is inherited from Dataset\nclass MyDataset(Dataset):\n    def __init__(self, df_data, data_dir = '.\/', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name)\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","92e85d2a":"# Image preprocessing\ntrans_train = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.Pad(32, padding_mode='reflect'),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\ntrans_valid = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.Pad(32, padding_mode='reflect'),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\n# Data generators\ndataset_train = MyDataset(df_data=train, data_dir=train_path, transform=trans_train)\ndataset_valid = MyDataset(df_data=val, data_dir=train_path, transform=trans_valid)\n\nloader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\nloader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size\/\/2, shuffle=False, num_workers=0)","0ab0237e":"# NOTE: class is inherited from nn.Module\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        # ancestor constructor call\n        super(SimpleCNN, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=2)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=2)\n        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=2)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.bn5 = nn.BatchNorm2d(512)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.avg = nn.AvgPool2d(4)\n        self.fc = nn.Linear(512 * 1 * 1, 2) # !!!\n   \n    def forward(self, x):\n        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x)))) # first convolutional layer then batchnorm, then activation then pooling layer.\n        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n        x = self.pool(F.leaky_relu(self.bn4(self.conv4(x))))\n        x = self.pool(F.leaky_relu(self.bn5(self.conv5(x))))\n        x = self.avg(x)\n        #print(x.shape) # lifehack to find out the correct dimension for the Linear Layer\n        x = x.view(-1, 512 * 1 * 1) # !!!\n        x = self.fc(x)\n        return x","e1e39d58":"model = SimpleCNN().to(device)","cf0d8578":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)","2b086d8f":"\n# Train the model\ntotal_step = len(loader_train)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(loader_train):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))","42f7fc6c":"# Test the model\nmodel.eval()  # eval mode (batchnorm uses moving mean\/variance instead of mini-batch mean\/variance)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in loader_valid:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n          \n    print('Test Accuracy of the model on the 1750 validation images: {} %'.format(100 * correct \/ total))\n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')","2f19989e":"# generator for test data \ndataset_valid = MyDataset(df_data=sub, data_dir=test_path, transform=trans_valid)\nloader_test = DataLoader(dataset = dataset_valid, batch_size=32, shuffle=False, num_workers=0)","3c0bf689":"model.eval()\n\npreds = []\nfor batch_i, (data, target) in enumerate(loader_test):\n    data, target = data.cuda(), target.cuda()\n    output = model(data)\n\n    pr = output[:,1].detach().cpu().numpy()\n    for i in pr:\n        preds.append(i)\n\nsub['has_cactus'] = preds\nsub.to_csv('sub.csv', index=False)","b286e0c5":"**Simple custom generator**","14c4d9ba":"**Important note:** You may notice that in lines with # !!! there is not very clear 128 \\* 11 \\* 11. This is the dimension of the picture before the FC layers (H x W x C), then you have to calculate it manually (in Keras, for example, .Flatten () does everything for you). However, there is one life hack \u2014 just make print (x.shape) in forward () (commented out line). You will see the size (batch_size, C, H, W) - you need to multiply everything except the first (batch_size), this will be the first dimension of Linear (), and it is in C H W that you need to \"expand\" x before feeding to Linear ().","ff705ab1":"**Model**","3288e6bb":"**Hasn't cactus**","8b5671cf":"** Has cactus**","a01318bf":"**convolutional neural network on pytorch from scratch**","5dec61c1":"**CSV submission**","039eb209":"\n**EDA** (Exploratory Data Analysis).<P>\nThe purpose of EDA is:\n+ Look at the data\n+ Understand the distribution of two classes (hasn't cactus \/ has cactus)\n+ Look at some features of the image (distribution of RGB channels, average brightness, etc.)","23bccd56":"Checking label distribution(must be 1:3)","6f588194":"**Accuracy Check**"}}