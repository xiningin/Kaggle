{"cell_type":{"cf8015cf":"code","d29bbfd0":"code","f84a18de":"code","c4457d04":"code","d6db8d3d":"code","73b902d7":"code","c07eef27":"code","c026fcf5":"code","26ae9d9d":"code","ba3084a6":"code","8550ee67":"code","70064e60":"code","e2c0052c":"code","62b6bc3f":"code","18d2cc80":"code","4096bddf":"code","74a431d4":"code","d9ad4808":"code","8f4295d2":"code","b66ced4c":"code","1f43d5d8":"code","7bd58473":"code","cf8577a4":"code","368cf2d9":"code","cbead80b":"code","5e6035cf":"code","7536b133":"code","414350c4":"code","d1989d78":"code","fcef83f2":"code","b7d5cd05":"code","e08443ae":"code","d8ceae0d":"code","fd31adb1":"code","70a07e63":"code","b1784e6b":"code","d6afef35":"code","e6e41d9b":"code","6e7ddb93":"code","e1a33493":"code","7e555356":"code","5202c43a":"code","1154d7ba":"code","b90fa704":"code","5854be2e":"code","01603afd":"code","6eb690be":"code","54492dcf":"code","ef6b95e3":"code","8399b418":"code","9b053c81":"code","b2ddb618":"code","367d607c":"code","6141ea17":"code","c879e7fb":"code","942d02dc":"code","32bdf3ba":"code","e622fc70":"code","4bdcce0f":"code","f097f62f":"code","c38dd748":"code","3667b331":"code","d3d7a123":"code","90722d24":"code","3cf5d797":"code","d45309e0":"code","0375a945":"markdown","1c572611":"markdown","b56ef3c0":"markdown","b18bb3f0":"markdown","e6069be2":"markdown","3614eb23":"markdown","e4567886":"markdown","e58ed260":"markdown","045f09c2":"markdown","3412c3a6":"markdown","0cd82875":"markdown","8576ca8c":"markdown","94529244":"markdown","a41d687f":"markdown","e02a5102":"markdown","a536c8df":"markdown","bfc57004":"markdown","bab100b1":"markdown","7d2ac5fc":"markdown","53d2f70b":"markdown","b5e20b9f":"markdown","8c9e43fa":"markdown","ddf4d95e":"markdown","a12759d4":"markdown","97462280":"markdown","10f1db03":"markdown","502e8ad9":"markdown","3d6e5589":"markdown","298ed78a":"markdown","8e97c3e2":"markdown","bf458a10":"markdown"},"source":{"cf8015cf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy.stats import norm\nfrom scipy import stats\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('fivethirtyeight')","d29bbfd0":"data = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv', encoding = 'unicode_escape') ","f84a18de":"data.head()","c4457d04":"data.info()","d6db8d3d":"data.shape","73b902d7":"data.describe()","c07eef27":"data.var()","c026fcf5":"sns.pairplot(data, size=5);","26ae9d9d":"sns.displot(data, x=\"Age\", hue=\"Age\")\nplt.show()","ba3084a6":"sns.countplot(x=\"Gender\", data=data)\nplt.show()","8550ee67":"sns.scatterplot(data=data, x=\"Age\", y=\"Annual Income (k$)\", hue=\"Annual Income (k$)\", \n                size=\"Annual Income (k$)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","70064e60":"sns.scatterplot(data=data, x=\"Age\", y=\"Spending Score (1-100)\", hue=\"Spending Score (1-100)\", \n                size=\"Spending Score (1-100)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","e2c0052c":"sns.scatterplot(data=data, x=\"Annual Income (k$)\", y=\"Spending Score (1-100)\", hue=\"Spending Score (1-100)\", \n                size=\"Spending Score (1-100)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","62b6bc3f":"sns.scatterplot(data=data, x=\"Spending Score (1-100)\", y=\"Annual Income (k$)\", hue=\"Spending Score (1-100)\", \n                size=\"Spending Score (1-100)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","18d2cc80":"sns.scatterplot(data=data, x=\"CustomerID\", y=\"Annual Income (k$)\", hue=\"Annual Income (k$)\", \n                size=\"Annual Income (k$)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","4096bddf":"sns.scatterplot(data=data, x=\"CustomerID\", y=\"Spending Score (1-100)\", hue=\"Spending Score (1-100)\", \n                size=\"Spending Score (1-100)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","74a431d4":"corr_matrix = data.corr()\nf, ax = plt.subplots(figsize=(25, 15))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr_matrix, cmap=cmap, vmax=.5, annot=True, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","d9ad4808":"new_df = data.copy()","8f4295d2":"# This function will calculate the IQR for us and save the values that is higher or lower as follwow\ndef IQR(column_name):\n    Q1 = new_df[column_name].quantile(0.25)\n    Q3 = new_df[column_name].quantile(0.75)\n    IQR = Q3 - Q1\n    upper_limit = Q3 + 1.5 * IQR\n    lower_limit = Q1 - 1.5 * IQR\n    values_upper = new_df[new_df[column_name] > upper_limit]\n    values_lower = new_df[new_df[column_name] < lower_limit]\n    \n    return values_upper, values_lower, upper_limit, lower_limit","b66ced4c":"# this Function will check if the returned shape from IQR is higher than zero \n# why zero! cos the output will be for example like this (2,63) that means there are 2 rows contains outliers \n# and if it more than zero it will show us this rows\ndef upper(column_name):\n    if values_upper.shape[0] > 0:\n        print(\"Outliers upper than the higher limit: \")\n        return new_df[new_df[column_name] > upper_limit]\n    else:\n        print(\"There are no values higher than the upper limit!\")","1f43d5d8":"# same as above but for lower values\ndef lower(column_name):\n    if values_lower.shape[0] > 0:\n        print(\"Outliers lower than the higher limit: \")\n        return new_df[new_df[column_name] < lower_limit]\n    else:\n        print(\"There are no values lower than the lower limit!\")","7bd58473":"# this function will delete any outliers upper or lower the limit\ndef outliers_del(column_name):\n    # we will make new_df global to consider the global variable not the local\n    global new_df\n    new_df = new_df[new_df[column_name] < upper_limit]\n    new_df = new_df[new_df[column_name] > lower_limit]\n    print(\"the old data shape is :\", data.shape)\n    print(\"the new data shape is :\", new_df.shape)","cf8577a4":"# this function is for ploting the data \ndef plot(column_name):\n    plt.style.use('fivethirtyeight')\n    plt.figure(figsize=(16,5))\n    #plt.subplot(1,2,1)\n    # we will use fit norm to draw the normal distibutions that the data sould be it will be in black \n    #sns.distplot(data[column_name], fit=norm)\n    plt.subplot(1,2,1)\n    sns.boxplot(data[column_name],palette=\"rocket\")\n    plt.show()","368cf2d9":"def outlier_compare(column_name):\n    plt.style.use('fivethirtyeight')\n    plt.figure(figsize=(25,15))\n    plt.subplot(2,2,1)\n    sns.boxplot(data[column_name], palette=\"rocket\")\n    plt.subplot(2,2,2)\n    sns.boxplot(new_df[column_name], palette=\"rocket\")\n    plt.show()","cbead80b":"Upper_Outliers_columns = []\nLower_Outliers_columns = []\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfor column in new_df:\n    if new_df[column].dtype in numeric_dtypes:\n        values_upper, values_lower, upper_limit, lower_limit = IQR(column)\n        if values_upper.shape[0] > 0:\n            Upper_Outliers_columns.append(column)\n        if values_lower.shape[0] > 0:\n            Lower_Outliers_columns.append(column)","5e6035cf":"print('Columns upper the limit is: ', Upper_Outliers_columns)\nprint('Columns lower the limit is: ', Lower_Outliers_columns)","7536b133":"plot('Annual Income (k$)')","414350c4":"values_upper, values_lower, upper_limit, lower_limit = IQR('Annual Income (k$)')","d1989d78":"upper('Annual Income (k$)')","fcef83f2":"lower('Annual Income (k$)')","b7d5cd05":"outliers_del('Annual Income (k$)')","e08443ae":"outlier_compare('Annual Income (k$)')","d8ceae0d":"from scipy.stats import skew\n\nskewness_list = {}\nfor i in new_df:\n    if new_df[i].dtype != \"object\":\n        skewness_list[i] = skew(new_df[i])\n\nskewness = pd.DataFrame({'Skew' :skewness_list})\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(15,9))\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Skewness', fontsize=15)\nplt.xticks(rotation='90')\nplt.bar(range(len(skewness_list)), list(skewness_list.values()), align='center')\nplt.xticks(range(len(skewness_list)), list(skewness_list.keys()))\n\nplt.show()","fd31adb1":"skewness_list","70a07e63":"X = new_df.copy()","b1784e6b":"X = X.drop('CustomerID', axis=1)","d6afef35":"X = X.drop('Gender', axis=1)","e6e41d9b":"# Models\nfrom sklearn.cluster import KMeans\n# Encoders\nfrom sklearn.preprocessing import OneHotEncoder\n# Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n# Cols transform\nfrom sklearn.compose import make_column_transformer\n# Pipeline\nfrom sklearn.pipeline import make_pipeline\n# interactive diagrams of Pipelines \nfrom sklearn import set_config\nset_config(display='diagram')","6e7ddb93":"#OHE = OneHotEncoder()\nMMS = MinMaxScaler()","e1a33493":"column_trans = make_column_transformer(\n    #(OHE, ['Gender']),\n    (MMS, ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']),\n    remainder='passthrough')","7e555356":"column_trans.fit_transform(X);","5202c43a":"k_range = range(1,10)\nsse = []\nfor i in k_range:\n    Kmeans = KMeans(n_clusters=i, random_state=0)\n    pipe = make_pipeline(column_trans, Kmeans)\n    pipe.fit_predict(X)\n    sse.append(pipe[1].inertia_)","1154d7ba":"sse","b90fa704":"plt.plot(k_range, sse)\nplt.plot(range(1,10),sse, linewidth=4, markersize=12,marker='o')\nplt.xlabel('Number of K')\nplt.ylabel('SSE')\nplt.show()","5854be2e":"Kmeans = KMeans(n_clusters=5, init = \"k-means++\", random_state = 0)","01603afd":"pipe = make_pipeline(column_trans, Kmeans)","6eb690be":"pipe","54492dcf":"clusters = pipe.fit_predict(X)","ef6b95e3":"X[\"Cluster\"] = clusters","8399b418":"X.head()","9b053c81":"# pip install plotly==5.3.1","b2ddb618":"# ploting in 3D space\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport plotly.offline as py\n\n# 3d scatterplot using plotly\nScene = dict(xaxis = dict(title  = 'Age'),yaxis = dict(title  = 'Spending Score'),\n             zaxis = dict(title  = 'Annual Income'))\n\nlabels = pipe[1].labels_\ntrace = go.Scatter3d(x=X.iloc[:, 0], y=X.iloc[:, 1], z=X.iloc[:, 2], \n                     mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))\n\nlayout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)\ndata = [trace]\nfig = go.Figure(data = data, layout = layout)\nfig.show()","367d607c":"Income_Score_X = X[['Annual Income (k$)', 'Spending Score (1-100)']]","6141ea17":"Income_Score_X = MMS.fit_transform(Income_Score_X)","c879e7fb":"Income_Score_X_df = pd.DataFrame(Income_Score_X, columns = ['Annual Income (k$)','Spending Score (1-100)'])","942d02dc":"k_range = range(1,10)\nsse = []\nfor i in k_range:\n    Kmeans = KMeans(n_clusters=i, random_state=0)\n    Kmeans.fit_predict(Income_Score_X)\n    sse.append(Kmeans.inertia_)","32bdf3ba":"sse","e622fc70":"plt.plot(k_range, sse)\nplt.plot(range(1,10),sse, linewidth=4, markersize=12,marker='o')\nplt.xlabel('Number of K')\nplt.ylabel('SSE')\nplt.show()","4bdcce0f":"Kmeans = KMeans(n_clusters=5, random_state=0)","f097f62f":"clusters = Kmeans.fit_predict(Income_Score_X)","c38dd748":"Income_Score_X_df['Clusters'] = clusters","3667b331":"sns.scatterplot(data=Income_Score_X_df, x=\"Annual Income (k$)\", y=\"Spending Score (1-100)\", hue=\"Clusters\")\nsns.scatterplot(Kmeans.cluster_centers_[:,0],Kmeans.cluster_centers_[:,1],  label = \"centroids\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","d3d7a123":"plt.figure(figsize=(10,10))\nsns.lmplot(x='Annual Income (k$)',y='Spending Score (1-100)',data=Income_Score_X_df,hue='Clusters',fit_reg=False, legend=False)\nsns.scatterplot(Kmeans.cluster_centers_[:,0],Kmeans.cluster_centers_[:,1],  label = \"centroids\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","90722d24":"from sklearn.metrics import silhouette_score","3cf5d797":"labels = Kmeans.labels_","d45309e0":"silhouette_score(Income_Score_X, labels)","0375a945":"### Understanding our data","1c572611":"### Outliers Detection\n\nwe have a various methods to detect the outliers i am going to use IQR here this method works fine for me but \n\nyou can try other methods like \n\n            1- Z-score method\n            2. Robust Z-score\n            3. I.Q.R method\n            4. Winterization method(Percentile Capping)\n            5. DBSCAN Clustering\n            6. Isolation Forest\n            7. Visualizing the data\n            \nIQR stands for \"Inter Quartiles Range\"\n\nthis method depends on two values \n    \n    Q1 >> which represents a quarter of the way through the list of all data usually this value is 0.25 \n    \n    Q3 >> which represents three-quarters of the way through the list of all data usually this value is 0.75 \n    \nhow IQR works :\n    well first it sorts the data and finds its median \n    then seperate the numbers before the median and finds its own median \"Q1\"  and also seperates the numbers \n    after the total medain and finds its own median \"Q3\"\n    \n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/1a\/Boxplot_vs_PDF.svg\/1200px-Boxplot_vs_PDF.svg.png\">\n\nthen we will take the diffrance between Q3 and Q1","b56ef3c0":"we dont have any missing data and aslo we have only one object column","b18bb3f0":"#### well we will use all feature except Gender ","e6069be2":"### Take a quick look at our data","3614eb23":"#### Annual Income and Spending Score ","e4567886":"##### Elbow method","e58ed260":"### Loading the data","045f09c2":"## Data visualization ","3412c3a6":"we will try to find some interseting shapes and data distributions ","0cd82875":"## Pipeline","8576ca8c":"5 is the best number of K","94529244":"## Data preperation","a41d687f":"###### Annual Income (k$)","e02a5102":"#### But before getting our hands dirty lets define some functions that we will use a lot like \n    \"IQR\" to calculate the IQR for us \n    \"Upper and Lower\" to fetch upper values and lower values that contain outliers \n    \"outliers_del\" to delete them \n    \"Plot\" function to plot the curves \n    \"outlier_compare\" to compare the data before deleting outliers and correct the skewness and after\n    \nI will write a comment for each function when creating it","a536c8df":"### Skewness","bfc57004":"i think we have outliers here in Annual Income column","bab100b1":"##### Annual Income and Spending Score ","7d2ac5fc":"# Our methodology\n\n\n## Data visualization \n\n    Loading the data\n    Take a quick look at our data \n    Understanding our data\n    Finding the correlations\n    \n## Data preperation \n\n    Outliers detection\n    Skweness correction\n    \n## Droping Cols \n\n    we will drop some columns that we dont need\n\n\n## Pipeline\n\n    Feature scaling\n    \n## Modeling\n\n    Building the model\n    \n## Feature Selection\n   \n\n    \n## Testing our model\n\n    Evaluate the model ","53d2f70b":"## Modeling","b5e20b9f":"If you find the above plot is not clear you could use the following code","8c9e43fa":"## Droping Cols ","ddf4d95e":"#### we i think we found some good distributions","a12759d4":"I will do feature selection and choose Annual Income and Spending Score to performe clusters on those two only why!\n\nwell if you go back to Understanding our data section above you will see that those two feature have a good \ndistribution on the scatter plot so i will pick them up!","97462280":"## Evaluate Our Model with silhouette_score","10f1db03":"### Find the correlations","502e8ad9":"##### Annual Income and Spending Score based on CustomerID","3d6e5589":"Well we will use pipeline to keep out data as it is without changing cos when you do some preprocess steps without pipeline it change the original data frame for example if you did get_dummies to hot encode a cat columns it will add new columns to your data but pipeline didnot!\n\nso we will do only feature scaling but you can add many steps to your pipeline and it will become easy to read and more professional to use!\n\n##### Feature Scaling\n\nMachine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled.\n\nwe have three methods in sklearn \n\nMinMaxScaler(feature_range = (0, 1)) will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion).\n\nStandardScaler() will transform each value in the column to range about the mean 0 and standard deviation 1, ie, each value will be normalised by subtracting the mean and dividing by standard deviation. Use StandardScaler if you know the data distribution is normal.\n\nIf there are outliers, use RobustScaler(). Alternatively you could remove the outliers and use either of the above 2 scalers (choice depends on whether data is normally distributed)\n\nWe delete most outliers earlier so we can use MinMaxScaler or StandardScaler\n\n\n","298ed78a":"### Importing needed libraries","8e97c3e2":"\n<img src=\"https:\/\/analyticsindiamag.com\/wp-content\/uploads\/2020\/03\/segment-customers-segmentation-classification-marketing-market-targeting-audience-target-crm-concept_t20_wlz29V.jpg\">","bf458a10":"##### Annual Income and Spending Score based on Age"}}