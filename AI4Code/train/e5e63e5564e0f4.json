{"cell_type":{"d529e337":"code","397bb1b5":"code","a91027a9":"code","9ba8fdf0":"code","9ae0c846":"code","b796b3a0":"code","3e9cd377":"code","68a82c59":"code","b2a31105":"code","4ca3c156":"code","ecfa0183":"code","1703e2b0":"code","a626db92":"code","fcd16022":"code","f4bb3798":"code","64ddc1b2":"code","b3b3d7b9":"code","4c3b6118":"code","1b472774":"code","8d1128b6":"code","f79cb1fd":"code","f49b6c62":"code","bc5b7316":"code","50d982a1":"code","55eb9182":"code","63723543":"code","b9d10654":"code","7654b700":"code","2cc69b89":"code","87942ec4":"code","524a466d":"code","efd31ed7":"code","0a93d77b":"code","9415725a":"code","cd97124a":"code","d8dbcf4b":"code","24c51c66":"code","bd4b59be":"code","e5d21000":"code","95a6e2bd":"code","02685a05":"code","fa8b6966":"code","a517f289":"code","978a62a4":"code","5430a909":"code","deb72ff9":"code","3ed8b806":"markdown","7cbbe764":"markdown","4c01040c":"markdown","0aa500bb":"markdown","b269b893":"markdown","2aa2b4ec":"markdown","2a212dcc":"markdown","8e473881":"markdown","230e00e7":"markdown","4381c15f":"markdown","3662712f":"markdown","eb32297b":"markdown","b8387ac4":"markdown","dbe1fe81":"markdown","98891e59":"markdown","1082ee2d":"markdown","07e53f28":"markdown","4825600a":"markdown","1dd5c02a":"markdown","49b337a5":"markdown","ac6fd3e7":"markdown","02f93901":"markdown","aa5984f9":"markdown","92bd4a87":"markdown","aa43fbcc":"markdown","40d37667":"markdown","fc6350ab":"markdown","79f08689":"markdown","8d4ccda3":"markdown","47c6517a":"markdown","1cf66aa0":"markdown","13da024c":"markdown","b2626b12":"markdown","56b879a8":"markdown","3953c5b4":"markdown","511c07be":"markdown","a496d283":"markdown","10be143f":"markdown","736e5c4f":"markdown","ab4781b0":"markdown","45c853b2":"markdown","bd845454":"markdown","9243bd2a":"markdown","fec6353f":"markdown","44e50541":"markdown","7d0cfc02":"markdown","dcc6ff23":"markdown","12200484":"markdown","8d8b0500":"markdown","c5d5d78d":"markdown","5e8a9941":"markdown","3cc87d70":"markdown"},"source":{"d529e337":"!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java","397bb1b5":"import os\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","a91027a9":"!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch","9ba8fdf0":"COVID_INDEX = '..\/input\/luceneindexcovidparagraph20200410\/lucene-index-covid-paragraph-2020-04-10'","9ae0c846":"searcher = pysearch.SimpleSearcher(COVID_INDEX)","b796b3a0":"def get_articles(query):\n    hits = searcher.search(query)\n    #print(len(hits))\n    # Prints the first 10 hits\n    return hits","3e9cd377":"query = 'range of incubation periods for COVID-19'\nhits = get_articles(query)\nfor i in range(0, 10):\n    #print some relevant fields\n    print(f'{i+1} {hits[i].docid} {hits[i].score} {hits[i].lucene_document.get(\"title\")} {hits[i].lucene_document.get(\"doi\")}')","68a82c59":"hits[0].contents.split('\\n')","b2a31105":"import json\ndef get_para_results(query):\n    hits = searcher.search(query,10) \n    print(len(hits))\n    temp = {} # to store the doi of the articles being returned so we know if the article is repeated\n    i = 0\n    output = []\n    while i<len(hits) and i<10:\n        outJson = {}\n        outJson['rank'] = i+1\n        # check if the current article has a paragraph returned or not ('has_full_text' in the dataset)\n        if '.' in hits[i].docid:\n            doc_id = hits[i].docid.split('.')[0]\n            para_id = hits[i].docid.split('.')[1]\n            doi = hits[i].lucene_document.get('doi')\n            paragraph = {}\n            paragraph['score'] = hits[i].score\n            paragraph['text'] = hits[i].contents.split('\\n')[-1] # get the last element, since the contents are sorted as [title, abstract, paragraph]\n            paragraph['id'] = para_id\n            # check if the doi (same article) has not appeared before in the list\n            if doi not in temp:\n                outJson['abstract'] = hits[i].lucene_document.get('abstract') # include abstract if new article\n                article_data = json.loads(searcher.doc(doc_id).lucene_document().get('raw')) # get all the relevant data from the dataset \n                if 'body_text' in article_data:\n                    outJson['body_text'] = article_data['body_text'] # include 'body_text' in case needed later\n                temp[doi] = i\n            outJson['paragraphs'] = []\n            outJson['paragraphs'].append(paragraph)\n        else:\n            # no paragraph present, which means article does not have full text available\n            outJson['abstract'] = hits[i].lucene_document.get('abstract')\n            outJson['score'] = hits[i].score\n        outJson['title'] = hits[i].lucene_document.get('title')\n        outJson['sha'] = hits[i].lucene_document.get('sha')\n        outJson['doi'] = hits[i].lucene_document.get('doi')\n        output.append(outJson)\n        i+=1\n    return output","4ca3c156":"query = 'range of incubation periods for COVID-19'\ni = 1\nfor item in get_para_results(query):\n    if i>10:\n        break\n    print(item)\n    i+=1","ecfa0183":"def information_retrieval(file_name, topk = 10):\n\n    with open(file_name) as f:\n        json_file = json.load(f)\n    subtasks = json_file[\"sub_task\"]\n    \n    all_results = []\n    data_for_qa = []\n    for item in subtasks:\n        questions = item[\"questions\"]\n        for query in questions:\n            result_item = {\"question\" : query}\n            retri_result = get_para_results(query)\n            result_item[\"data\"] = retri_result\n\n            qa_item = {\"question\": query}\n            context = []\n            titles = []\n            doi = []\n            count = 1\n            for item in retri_result:\n                if count>topk:\n                    break\n                if 'abstract' in item and len(item['abstract']) > 0:\n                    context.append(item['abstract'])\n                    doi.append(item[\"doi\"])\n                    titles.append(item[\"title\"])\n                    count+=1\n                if 'paragraphs' in item:\n                    context.append(item['paragraphs'][0]['text'])   \n                    doi.append(item[\"doi\"])\n                    titles.append(item[\"title\"])\n                    count+=1\n\n            qa_item[\"data\"] = {\"answer\": \"\", \"context\": context, \"doi\": doi, \"titles\": titles}\n\n            all_results.append(result_item)\n            data_for_qa.append(qa_item)\n\n    return data_for_qa\n\ndef parse_ir_results(query, retri_result, topk = 10):\n    all_results = []\n    data_for_qa = []\n    qa_item = {\"question\": query}\n    result_item = {\"question\" : query}\n    result_item[\"data\"] = retri_result\n    context = []\n    titles = []\n    doi = []\n    count = 1\n    for item in retri_result:\n        if count>topk:\n            break\n        if 'abstract' in item and len(item['abstract']) > 0:\n            context.append(item['abstract'])\n            doi.append(item[\"doi\"])\n            titles.append(item[\"title\"])\n            count+=1\n        if 'paragraphs' in item:\n            context.append(item['paragraphs'][0]['text'])   \n            doi.append(item[\"doi\"])\n            titles.append(item[\"title\"])\n            count+=1\n    qa_item[\"data\"] = {\"answer\": \"\", \"context\": context, \"doi\": doi, \"titles\": titles}\n\n    all_results.append(result_item)\n    data_for_qa.append(qa_item)    \n\n    return all_results, data_for_qa\n\n    \ndef information_retrieval_query(query):\n\n    retri_result = get_para_results(query)\n    all_results, data_for_qa = parse_ir_results(query, retri_result ,topk = 20)\n    \n    return all_results, data_for_qa","1703e2b0":"### 3.1 install the prerequisite\nimport os\nimport sys\nimport json\n\n!pip uninstall tensorflow -y\n!pip uninstall tensorflow-gpu -y\n!pip install tensorflow==1.13.1\n!pip install caireCovid==0.1.8","a626db92":"import tensorflow as tf\nimport caireCovid\nfrom caireCovid import QaModule\nfrom caireCovid.qa_utils import stop_words\nimport math","fcd16022":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","f4bb3798":"### 3.2 Check all version\nprint(tf.__version__)","64ddc1b2":"# QA System\nclass QA_System():\n    def _init_(self):\n        # Load the QA models. Please refer to [Github](https:\/\/github.com\/yana-xuyan\/caireCovid) for details.\n        self.model = QaModule(['mrqa', 'biobert'], [\"\/kaggle\/input\/pretrained-qa-models\/mrqa\/1564469515\", \"\/kaggle\/input\/pretrained-qa-models\/biobert\/1585470591\"], \\\n                              \"\/kaggle\/input\/xlnetlargecased\/xlnet_cased_L-24_H-1024_A-16\/spiece.model\", \"\/kaggle\/input\/pretrained-qa-models\/bert_config.json\", \\\n                              \"\/kaggle\/input\/bert-base-cased\/vocab.txt\")\n    def getAnswer(self, query):\n        _, data_for_qa = information_retrieval_query(query)\n        answers =  self.model.getAnswers(data_for_qa)\n        return answers\n    def getAnswers(self, filename):\n        _, data_for_qa = information_retrieval(query)\n        answers = self.model.getAnswers(data_for_qa)\n        return answers\n    def makeFormatAnswers(self, answers):\n        format_answers = []\n        for i in range(len(answers[0]['data']['answer'])):\n                format_answer = {}\n                format_answer['question'] = answers[0]['question']\n                format_answer['answer'] = answers[0]['data']['answer'][i]\n                format_answer['context'] = answers[0]['data']['context'][i]\n                format_answer['doi'] = answers[0]['data']['doi'][i]\n                format_answer['title'] = answers[0]['data']['title'][i]\n                format_answer[\"confidence\"] = answers[0]['data']['confidence'][i]\n                format_answer[\"raw\"] = answers[0]['data']['raw'][i]\n                format_answers.append(format_answer)\n        return format_answers\n\ndef get_QA_answer_api(query):\n    url = \"http:\/\/eez114.ece.ust.hk:5000\/query_qa\"\n    payload = \"{\\n\\t\\\"text\\\": \\\"\"+query+\"\\\"\\n}\"\n    headers = {\n        'Content-Type': \"application\/json\",\n        'cache-control': \"no-cache\",\n        'Postman-Token': \"696fa512-5fed-45ca-bbe7-b7a1b4d19fe4\"\n    }\n    response = requests.request(\"POST\", url, data=payload, headers=headers)\n    response = response.json()\n    return response","b3b3d7b9":"import argparse\nimport sys\nimport pandas as pd\nimport csv\nimport requests\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.tokenize import sent_tokenize # use sentence tokenize\nfrom IPython.core.display import display, HTML","4c3b6118":"from nltk import word_tokenize, pos_tag, sent_tokenize\nfrom caireCovid.qa_utils import stop_words\nstop_words.append('including')\n\ndef rankAnswers(answers):\n    for item in answers:\n        query = item[\"question\"]\n        context = item['context']\n        # make new query with only n. and adj.\n        tokens = word_tokenize(query.lower())\n        tokens = [word for word in tokens if word not in stop_words]\n        tagged = pos_tag(tokens)\n        query_token = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n\n        text = context.lower()\n        count = 0\n        text_words = word_tokenize(text)\n        for word in text_words:\n            if word in query_token:\n                count += 1\n            \n        match_number = 0\n        for word in query_token:\n            if word == 'covid-19':\n                continue\n            if word in text_words:\n                match_number += 1\n        matching_score = count \/ (1 + math.exp(-len(text_words)+50))\/ 5 + match_number*10\n        item['matching_score'] = matching_score\n        item['rerank_score'] = matching_score + 0.5 * item['confidence']\n    \n    # sort QA results\n    answers.sort(key=lambda k: k[\"rerank_score\"], reverse=True)\n#     print([item['rerank_score'] for item in answers])\n    return answers\n\ndef highlight_qaresult(qaresult):\n    if qaresult == []:\n        print('API broken')\n        return 1\n    ## tokenize query\n    query = qaresult[0]['question']\n    query_tokens = word_tokenize(query.lower())\n    query_tokens = [word for word in query_tokens if word not in stop_words]\n    tagged = pos_tag(query_tokens)\n    query_tokens = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n\n    ## highlihgt answer\n    for i in range(len(qaresult)):\n        context_1 = \"<style type='text\/css'>mark { background-color:yellow; color:black; } <\/style>\"\n        golden = qaresult[i]['answer']\n        context = qaresult[i]['context']\n        context_sents = sent_tokenize(context)\n        golden_sents = sent_tokenize(golden)\n        for sent in context_sents:\n            if sent not in golden:\n                context_1 += sent\n            else:\n                context_1 += \"<mark>\"\n                for word in sent.split():\n                    word_tokens = word_tokenize(word)\n                    if len(word_tokens) > 1:\n                        for j in word_tokens:\n                            if j.lower() in query_tokens:\n                                context_1 = context_1 + \"<b>\" + j + \"<\/b>\"\n                            else:\n                                context_1 = context_1 + j\n                        context_1 = context_1 + \" \"\n                    else:\n                        for j in word_tokens:\n                            if j.lower() in query_tokens:\n                                context_1 = context_1 + \"<b>\" + j + \" <\/b>\"\n                            else:\n                                context_1 = context_1 + j + \" \"\n                context_1 += \" <\/mark>\"\n        qaresult[i]['context'] = context_1\n    return qaresult\n\ndef display_QA(result):\n    result = highlight_qaresult(result)\n    pdata = []\n    count = 0\n    for i in range(len(result)):\n        count += 1\n        line = []\n        context_1 = \"<div> \"\n        context = result[i]['context']\n        context_1 = context_1 + context\n        context_1 += \" <\/div>\"\n        line.append(context_1)\n        context_2 = '<a href= \"https:\/\/doi.org\/'\n        context_2 += result[i]['doi']\n        context_2 += ' target=\"_blank\">'\n        context_2 += result[i]['title']\n        context_2 += '<\/a>'\n        line.append(context_2)\n        pdata.append(line)\n        if count > 5:\n            break\n    df = pd.DataFrame(pdata, columns = ['QA results', 'title'])\n    df = df.style.set_properties(**{'text-align': 'left','mark-color': 'red'})\n    display(df)","1b472774":"!pip install easydict\n!pip install covidSumm==0.1.4\n!pip install fairseq","8d1128b6":"import covidSumm\nimport requests\nimport json\nimport os\nimport argparse","f79cb1fd":"from covidSumm.abstractive_utils import get_ir_result, result_to_json, get_qa_result\nfrom covidSumm.abstractive_model import abstractive_summary_model\nfrom covidSumm.abstractive_config import set_config\nfrom covidSumm.abstractive_bart_model import *","f49b6c62":"def get_summary_list(article_list, abstractive_model):\n    summary_list = []\n    for i in range(len(article_list)):\n        article = article_list[i]\n        summary_results = abstractive_model.generate_summary(article)\n        result = \"\"\n        for item in summary_results:\n            result += item.replace('\\n', ' ')\n        summary_list.append(result)\n    return summary_list\n\ndef get_answer_summary(query, abstractive_model):\n    paragraphs_list = get_qa_result(query, topk = 3)\n    answer_summary_list = abstractive_model.generate_summary(paragraphs_list)\n    answer_summary = \"\"\n    for item in answer_summary_list:\n        answer_summary += item.replace('\\n', ' ')\n    answer_summary_json = {}\n    answer_summary_json['summary'] = answer_summary\n    answer_summary_json['question'] = query\n    return answer_summary_json\n\ndef get_article_summary(query, abstractive_summary_model):\n    article_list, meta_info_list = get_ir_result(query, topk = 10)  \n    summary_list = get_summary_list(article_list, abstractive_summary_model)\n    summary_list_json = []\n    \n    for i in range(len(summary_list)):\n        json_summary = {}\n        json_summary = result_to_json(meta_info_list[i], summary_list[i])\n        summary_list_json.append(json_summary)\n\n    return summary_list_json\n\ndef get_bart_answer_summary_from_qa(query, qa_result, bart_model):\n    # we select top3\n    paragraphs_list = []\n    topk = 3\n\n    for i in range(topk):\n        if 'context' in qa_result[i].keys():\n            one_line = {}\n            one_line['src'] = qa_result[i]['context']\n            one_line['tgt'] = \"\"\n            paragraphs_list.append(one_line)\n    \n    answer_summary_list = bart_model.bart_generate_summary(paragraphs_list)\n    answer_summary_result = \"\"\n    for item in answer_summary_list:\n        answer_summary_result += item.replace('\\n', ' ')\n    \n    answer_summary_json = {}\n    answer_summary_json['summary'] = answer_summary_result\n    answer_summary_json['question'] = query\n    return answer_summary_json","bc5b7316":"args = set_config()\nargs['model_path'] = '\/kaggle\/input\/carieabssummmodel\/'\nsummary_model_1 = abstractive_summary_model(config = args)","50d982a1":"model_path = \"\/kaggle\/input\/bartsumm\/bart.large.cnn\"\nsummary_model_2 = Bart_model(model_path)","55eb9182":"from IPython.core.display import display, HTML\nimport pandas as pd\n\ndef display_summary(ans_summary_json, model_type):\n    question = ans_summary_json['question']\n    text = ans_summary_json['summary']\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+question+'<\/div>'\n    display(HTML(question_HTML))\n\n    execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>' + model_type + ' Abstractive Summary:<\/b>: '+text+'<\/div>'\n    display(HTML(execSum_HTML))\n\ndef display_article_summary(result, query):\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+query+'<\/div>'\n    pdata = []\n    abstract = \"\"\n    summary = \"\"\n    for i in range(len(result)):\n        if 'abstract' in result[i].keys():\n            line = []\n            context_2 = '<a href= \"https:\/\/doi.org\/'\n            context_2 += result[i]['doi']\n            context_2 += ' target=\"_blank\">'\n            context_2 += result[i]['title']\n            context_2 += '<\/a>'\n            line.append(context_2)\n            \n            abstract = \"<div> \" \n            abstract += result[i]['abstract']\n            abstract += \" <\/div>\"\n            line.append(abstract)\n            summary = \"<div> \" + result[i]['summary'] + \" <\/div>\"\n            line.append(summary)\n\n\n            pdata.append(line)\n    display(HTML(question_HTML))\n    df = pd.DataFrame(pdata, columns = ['Title','Abstract','Summary'])\n    HTML(df.to_html(render_links=True, escape=False))\n#     display(HTML(df.to_html(render_links=True, escape=False)))\n    df = df.style.set_properties(**{'text-align': 'left'})\n    display(df)\n","63723543":"query = \"How incubation period for COVID-19 varies across age?\"","b9d10654":"def run_example(query):\n    # Given one query, we retrieve the relevant paragraphs and feed the (paragraph, query) pairs into the QA system \n    qa_result = get_QA_answer_api(query)\n    # Answer Reranking\n    qa_result = rankAnswers(qa_result)\n    \n    # Input \"summary_model_2\" is the BART summarization model.\n    # Function \"get_bart_answer_summary\" is loaded from covidSumm.abstractive_bart_model\n    # Given one query, we take top-3 reranked paragraphs from the QA module and summarize them into one paragraph\n    answer_summary_2 = get_bart_answer_summary_from_qa(query, qa_result, summary_model_2)\n    display_summary(answer_summary_2, 'BART')\n    display_QA(qa_result)","7654b700":"run_example(query)","2cc69b89":"query = \"What do we know about covid-19 genetics?\"\nrun_example(query)","87942ec4":"query = \"What do we know about covid-19 origin?\"\nrun_example(query)","524a466d":"query = \"How incubation period for COVID-19 varies across health status?\"\nrun_example(query)","efd31ed7":"query = \"How is prevalence of asymptomatic shedding and transmission?\"\nrun_example(query)","0a93d77b":"query = \"What do we know about covid-19 evolution?\"\nrun_example(query)","9415725a":"query = \"What do we know about covid-19 management measures at the human-animal interface?\"\nrun_example(query)","cd97124a":"query = \"What do we know about the Real-time tracking of whole genomes of covid-19 and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics?\"\nrun_example(query)","d8dbcf4b":"query = \"What do we know about the Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to track variations of the covid-19 over time?\"\nrun_example(query)","24c51c66":"query = \"How is the access to geographic and temporal diverse sample sets to understand covid-19 geographic distribution and genomic differences, and determine whether there is more than one strain incirculation?\"\nrun_example(query)","bd4b59be":"query = \"Do we have any evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding)?\"\nrun_example(query)","e5d21000":"query = \"Does covid-19 infected livestock serve as a reservoir after the epidemic appears to be over?\"\nrun_example(query)","95a6e2bd":"query = \"Could farmers be infected by covid-19?\"\nrun_example(query)","02685a05":"query = \"What do we know of farmers role in the covid-19 origin?\"\nrun_example(query)","fa8b6966":"query = \"What do we know of surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia?\"\nrun_example(query)","a517f289":"query = \"What do we know about the experimental infections of covid-19 to test host range for this pathogen?\"\nrun_example(query)","978a62a4":"query = \"What evidence do we have of animal host(s) and evidences of continued spill-over to humans?\"\nrun_example(query)","5430a909":"query = \"What do we know of socioeconomic and behavioral risk factors for covid-19?\"\nrun_example(query)","deb72ff9":"query = \"what do we know about sustainable covid-19 risk reduction strategies?\"\nrun_example(query)","3ed8b806":"# Could farmers be infected by covid-19?","7cbbe764":"# How is prevalence of asymptomatic shedding and transmission?","4c01040c":"# What do we know about covid-19 evolution?","0aa500bb":"### Please try it!","b269b893":"# What do we know about covid-19 genetics?","2aa2b4ec":"# Let's try now","2a212dcc":"The first two are title and abstract, and the last element is the matched paragraph. We can see that the paragraph matched is actually quite good at answering the query. The next step would be to get an answer in a concised form by passing the matched paragraphs to a QA model.","8e473881":"# What do we know about the Real-time tracking of whole genomes of covid-19 and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics?","230e00e7":"For the question answering (QA) module, we have leveraged the BioBERT QA model which is finetuned on the SQuAD dataset and [our generalized QA model](http:\/\/https:\/\/github.com\/yana-xuyan\/caireCovid) for MRQA@EMNLP 2019 Shared Task[1]. Instead of fine-tuning the QA models on COVID-19 related datasets, we focus more on maintaining the generalization ability of our system so it can be easily applied to other similar tasks. For the MRQA model, we utilized six datasets, which vary from each other in terms of data source, context lengths, whether multi-hop reasoning is needed, strategies for data augmentation to reduce overfitting to the training data in order to enable generalization to out-of-domain data. Multi-task learning over six datasets is used to fine-tune large pre-trained language model XLNet[2] and it helped achieve promising results. To make the answers more readable, instead of providing small spans of answers, we provide the whole sentences and the surrounding context.  \n\nTo better evaluate the question answering results, we leverage the prediction probability of the QA models as the confidence score. The final answers of our system are re-ranked using this score as one of the factors, which will be talked about later in Section 2.2.\n \n\n[1]Su, Dan, et al. \"Generalizing Question Answering System with Pre-trained Language Model Fine-tuning.\" Proceedings of the 2nd Workshop on Machine Reading for Question Answering. 2019.\n[2]Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for language understanding.\" Advances in neural information processing systems. 2019.","4381c15f":"# Does covid-19 infected livestock serve as a reservoir after the epidemic appears to be over?","3662712f":"# 2. Relevant Snippet Selector\n## 2.1 Question Answering","eb32297b":"# 1. Document retrieval\n## 1.1 Query Paraphrasing","b8387ac4":"# What do we know about the Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to track variations of the covid-19 over time?","dbe1fe81":"Following the lucene+answerini information retrieval as described in: https:\/\/github.com\/castorini\/anserini\/blob\/master\/docs\/experiments-covid.md\n\nSetting up JAVA sdk 11 first:","98891e59":"In response to the COVID-19 pandemic, a lot of scholarly articles have been published recently and made freely available. At the same time, there are emerging requests from both the medical research community and the broader society to find answers to various questions regarding COVID-19. A system that can provide reliable answers to the COVID-19 related questions from the latest academic resources is crucial, especially for the medical community in the current time-critical race to treat patients and to find a cure for the virus. \n \nTo address the aforementioned requests by the medical community, we propose a machine learning-based system that uses state-of-the-art natural language processing(NLP) question answering(QA) techniques combined with summarization for mining the available scientific literature. The system is an end-to-end neural network-based open-domain QA system that can answer COVID-19 related questions, such as those questions proposed in the COVID-19 Kaggle task. Through our system, users can get two versions of the outcome:\n1. A ranked list of relevant snippets from the literature given a query;\n1. A fluent summary of the relevant results. We provide the paragraph-level summaries, which takes the paragraphs where the top three relevant snippets are located as input, to enable a more efficient way of understanding of the content.\n \nOur system consists of three different modules: **1) Document Retriever, 2) Relevant Snippet Selector, and 3) Multi-Document Summarizer**. The first module pre-processes a user\u2019s query and retrieves the most relevant k number of academic publications. The second module outputs a list of the most relevant answer snippets from the retrieved documents. It also highlights the relevant keywords. The last module is for generating the second output, namely a concise summary of the top-ranked retrieved relevant paragraphs in the two previous modules.\n\nWe have launched our [CAiRE-Covid website](https:\/\/caire.ust.hk\/covid), which showcases our results for each user query in real-time, so people can further experiment with our system.\n","1082ee2d":"Automatic text summarization is a common problem in machine learning and natural language processing (NLP). Basically there are two main types of how to summarize text in NLP:\n* **Extraction-based summarization**, which involves pulling key phrases from the source document and combining them to make a summary, and;\n* **Abstraction-based summarization**, which creates new phrases and sentences that relay the most useful information from the original text \u2014 just like humans do. \nIn general, the abstractive method is a much harder task but performs better than an extractive method.\n\nIn our project, considering the requirements that people may still want to further read each paragraph containing the predicted QA answer spans, we summarize the top-k  (top-3) paragraphs that QA module passes, to generate a **paragraph-level abstractive summary**. \n\nOur model is based on two different abstractive summarization models: [Unilm](https:\/\/github.com\/microsoft\/unilm\/tree\/master\/s2s-ft) and [BART](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/bart), both of which have obtained SOTA results on the summarization tasks ([CNN\/DM datasets](https:\/\/cs.nyu.edu\/~kcho\/DMQA\/), and [XSUM](https:\/\/github.com\/EdinburghNLP\/XSum\/tree\/master\/XSum-Dataset) data). UniLM model is a unified pre-trained model for language understanding and generation. BART is a sequence-to-sequence model trained with denoising as a pre-training objective for language generation, translation, and comprehension.\n\nWe fine-tuned the UniLM model using [SumOnGraph](https:\/\/github.com\/coshiang\/SumOnGraph) biology dataset which includes literature for 5 types of diseases including Cancer, Cardiovascular Disease, Diabetes, Allergy, and Obesity. Original data is from PubMed which is a free resource supporting the search and retrieval of biomedical and life sciences literature with the aim of improving health\u2013both globally and personally. We used the BART model fine-tuned on CNN\/DailMail dataset. \n\nWe generate a summary for each answer-related paragraph from the QA module, then concatenate them directly to form our final **paragraph-level** answer summary.\n\n*(Actually we also implement **article-level summary**, even though in this Kaggle project, for the simplicity and legibility, we only display **the paragraph-level results** of the summarization models. It takes the whole article as input, and generate a summary for each sections (eg. Introductions section, Methodologies section)  of the articles, and then concatenate them together as a more fine-grained **article-level summary**, as complementary to the abstracts. You can refer to this [github repository](https:\/\/github.com\/Iamfinethanksu\/covidSumm) for more details.)*\n\nIf anyone is interested in our article-level results, please utilize ```summary = get_article_summary(query, abstractive_summary_model)``` to obtain the results from our system. ","07e53f28":"# What do we know about the experimental infections of covid-19 to test host range for this pathogen?","4825600a":"* install the prerequisite packages\n(The covidSumm packages are from [here](https:\/\/github.com\/Iamfinethanksu\/covidSumm).","1dd5c02a":"# Do we have any evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding)?","49b337a5":"# What do we know about covid-19 management measures at the human-animal interface?","ac6fd3e7":"## 1.2 Search Engine","02f93901":"We can see some repititions in the results above. This can either be due to multiple paragraphs in the same article being matched with the query, or one article appearing more than once in the CORD-19 dataset, due to different sources. Let's now try printing out the actual paragraph that is being matched with each of the returned hits. First we print out contents of the indexing of the first hit for example:","aa5984f9":"# What do we know about covid-19 origin? ","92bd4a87":"Let's write some code to print the results, which are top 10 articles matching a given query, along with the best matched paragraph. We are printing some of the fields corresponding to each article, a complete list of fields can be found [here](https:\/\/github.com\/castorini\/anserini\/blob\/master\/src\/main\/java\/io\/anserini\/index\/generator\/CovidGenerator.java#L46).","aa43fbcc":"### 1.2.2 example for using Anserini\nHere we type query 'range of incubation periods for COVID-19' in the search engine and it will return the top10 revelant items Anserini gets in the dataset.","40d37667":"* We initiate our **Summerization model BART**.","fc6350ab":"In this project, for sake of efficiency, ***we build an API for the Search Engine module to integrate it with the Question Answering module into this system***.","79f08689":"# what do we know about sustainable covid-19 risk reduction strategies?","8d4ccda3":"Write down whatever question you are interested in below. For example:","47c6517a":"# System Architecture Overview","1cf66aa0":"To accelerate our QA system, instead of running corresponding process in notebook, we leverage an API to make better use of the local GPUs. We also make our QA system public to the community by making python package for downloading with the following command:\n> pip install caireCovid\n\nFor anyone who may be interested in the implementation details, lease refer to our [Github repository](https:\/\/github.com\/yana-xuyan\/caireCovid). Some examples and other useful resources are also available there.","13da024c":"The objective of this sub-module is to break down a user\u2019s query and rephrase complex query sentences into several shorter and simpler questions that convey the same meaning.  In this way,  the search engine and the question answering modules will be able to find more relevant and less redundant results. ","b2626b12":"# What evidence do we have of animal host(s) and evidences of continued spill-over to humans?","56b879a8":"# Project Description","3953c5b4":"# How is the access to geographic and temporal diverse sample sets to understand covid-19 geographic distribution and genomic differences, and determine whether there is more than one strain incirculation?","511c07be":"# What do we know of surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia?","a496d283":"We can try running the function for the previous query, and check if the results are what we want.","10be143f":"# What do we know of farmers role in the covid-19 origin?","736e5c4f":"![image.png](attachment:image.png)\nIn this section, we will elaborate on the building blocks of each module in our system.\n \n1) Document Retriever\n+ **Query Paraphrasing**: It converts a long\/complicated query from a user to several shorter and simpler questions for search;\n+ **Search Engine**: We use Anserini with Lucene to retrieve publications from the candidate pool with high coverage. \n \n2) Relevant Snippet Selector: \n+ **Question Answering(QA)**: This sub-module looks for and integrates evidence from one or multiple paragraphs. We leverage an ensemble of two neural-based QA models which are pre-trained on SQuAD style QA datasets. Here we consider the QA module as a supporting fact selector to provide relevant snippets from the retrieved documents.\n+ **Answer Re-ranking & Highlight Generation**:  We rerank the retrieved result by a word matching score based on part-of-speech tagging as well as the QA system confidence score. We also highlight the answer span in order to enable easier reading of the QA results.\n\n3) Multi-document Summarizer:\n+ **Abstractive Summarization**: Another output of our system is an abstractive summary that synthesizes the answer from multiple retrieved snippets. This step aims to generate short pieces of fluent summaries based on the top relevant results. Using the neural-based summarizer, we generate summaries to improve the legibility of the results and help the user to have an overview of the relevant snippets in a short time.","ab4781b0":"Getting the pyserini library, which is anserini wrapped with python:","45c853b2":"We use [Anserini](https:\/\/github.com\/castorini\/anserini) to create the search engine to retrieve a preliminary candidate set of documents. Anserini is an information retrieval module wrapped around the open source search engine **Lucene**. Although Lucene has been used widely to build industry search engine applications, its complex indexing and lack of documentation for ad hoc experimentation and testing on standard test sets, has made it less popular in the information retrieval community. Anserini uses the Lucene indexing to create an easy-to-understand module. Standard ranking algorithms(e.g. bag of words, BM25) have been implemented in the module, which enables us to use Lucene for our application. Thanks to Jimmy Lin, we make this platform based on his [notebook](https:\/\/github.com\/castorini\/anserini-notebooks\/blob\/master\/pyserini_covid19_paragraph.ipynb). Since the disk is not large enough for saving the whole dataset with index and other models, we use his API to get the information retrieval results.","bd845454":"We can build the lucene index of the COVID-19 dataset from scratch, or get one of the pre-built indexes. Using the paragraph indexing which indexes each paragraph of an article (already uploaded the index as a dataset to use), can be downloaded from: https:\/\/www.dropbox.com\/s\/ivk87journyajw3\/lucene-index-covid-paragraph-2020-04-10.tar.gz","9243bd2a":"* Now we initiate our **Summerization model UniLM**.","fec6353f":"# 3. Abstractive Summerization","44e50541":"### 1.2.1 install Python dependencies and pre-built index","7d0cfc02":"# How incubation period for COVID-19 varies across health status?","dcc6ff23":"We now make another function to get the best matched paragraphs results from the dataset to our given query. If the article in concern does not have full text available then only the abstract is indexed. Since we know the doi field is unique to each article, we check if the article has already appeared before in the list returned. To avoid repetitions, we only include the 'abstract' and the 'body_text' fields form the dataset if the article is new and not a repeated article from before. The function is shown below:","12200484":"* What do we know about covid-19 genetics?\n* What do we know about covid-19 origin? \n* What do we know about covid-19 evolution?\n* What do we know about covid-19 management measures at the human-animal interface?\n* What do we know about the Real-time tracking of whole genomes of covid-19 and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics?\n* What do we know about the Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to track variations of the covid-19 over time?\n* How is the access to geographic and temporal diverse sample sets to understand covid-19 geographic distribution and genomic differences, and determine whether there is more than one strain incirculation?\n* Do we have any evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding)?\n* Does covid-19 infected livestock serve as a reservoir after the epidemic appears to be over?\n    * Could farmers be infected by covid-19?\n    * What do we know of farmers role in the covid-19 origin?\n    * What do we know of surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia?\n    * What do we know about the experimental infections of covid-19 to test host range for this pathogen?\n    * What evidence do we have of animal host(s) and evidences of continued spill-over to humans?\n    * What do we know of socioeconomic and behavioral risk factors for covid-19?\n    * what do we know about sustainable covid-19 risk reduction strategies?","8d8b0500":"The indexing is done based on each paragraph merged with the title and abstract. Given an article with id *doc_id*, the index will be as follows:\n* *doc_id* : title + abstract\n* *doc_id.00001* : title + abstract + 1st paragraph\n* *docid.00002*: title + abstract + 2nd paragraph\n* *docid.00003*: title + abstract + 3rd paragraph","c5d5d78d":"## 2.2 Highlights Generation","5e8a9941":"# What do we know of socioeconomic and behavioral risk factors for covid-19?","3cc87d70":"In this part, we introduce the word matching highlight strategy. There are two main components in this part: (1) word matching score calculate. (2) rerank and display. The input is the Question Answering result and for the output we display the most relevant paragraph with the highlighted answer.\n+ **POS-tag based scoring:** \nWe calculate a similarity score between a QA result and a given query based on keyword matching. To obtain this score, we first select important keywords based on POS-tagging - we consider words with {NN(noun), VB (verb), JJ (adjective)} POS-tags to be important keywords. Based on the set of filtered keywords, we count the word-match between QA-result keywords and query keywords. Higher the count is, more similar the QA-result and the query are. To penalize the matching scores of short retrieved paragraphs, we normalize them with sigmoid value computed from paragraph length. Moreover, we reward the paragraphs with more diverse keywords from query, which is the major factor of matching scores.\n\n+ **rerank and display:**\nThe re-ranking score is based on both the word matching score above and the confidence score from the QA system. The QA results are again ranked and displayed. "}}