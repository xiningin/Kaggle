{"cell_type":{"021eaeb4":"code","882f2dd5":"code","4d66e7dd":"code","72f86f80":"code","7db8d1bb":"code","9d4e0abf":"code","b8f8849c":"code","c148227a":"code","c010f316":"code","652a8185":"code","188b137a":"code","4c45088d":"code","8eefca0e":"code","dd8209cd":"code","73c1c5c9":"code","21243b27":"code","f14010e6":"code","e9bbbd0a":"code","99f93904":"markdown","f4eb17a5":"markdown","f84ed42c":"markdown","919ca05d":"markdown","e26d919d":"markdown","e1aa6857":"markdown","a8dee3cf":"markdown","13875be3":"markdown","d833b7e3":"markdown","b57351f0":"markdown","e5a3be12":"markdown","a25ea51d":"markdown","7b523ce5":"markdown","da2cf18d":"markdown","dfe6c367":"markdown","704ced10":"markdown","4aea76b8":"markdown","133a0baf":"markdown"},"source":{"021eaeb4":"import pandas as pd\nimport numpy as np\nfrom random import randint, gauss\nimport matplotlib.pyplot as plt","882f2dd5":"x = np.random.uniform(0, 100, size= 30)\ny = 10 + 2*x\nplt.scatter(x, y)","4d66e7dd":"fit = lambda x,b,m: b + m*x\n\nplt.scatter(x, y)\nfor i in range(3):\n    plt.plot(x, fit(x,randint(0,10), randint(0,5)))","72f86f80":"def calculate_rss(x, y, b, m, fit):\n    residuals = y - fit(x,b,m)\n    rss = np.sum(residuals**2)\n    return rss","7db8d1bb":"m_list = []\nb_list = []\nrss_list = []\n\nfor m in list(range(0,41)):\n    for b in list(range(0,41)):\n        m_list.append(m)\n        b_list.append(b)\n        rss_list.append(calculate_rss(x, y, b, m, fit))\n        \nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(projection='3d')\nax.scatter(m_list, b_list, rss_list)\nax.scatter(m_list[np.argmin(rss_list)], b_list[np.argmin(rss_list)], np.min(rss_list), color='red')\nax.set_xlabel('m', fontsize=20)\nax.set_ylabel('b', fontsize=20)\nax.set_zlabel('RSS', fontsize=20)\nplt.show()","9d4e0abf":"m_fit = (np.sum((x - np.mean(x))*(y - np.mean(y))))\/np.sum((x-np.mean(x))**2)\nb_fit = np.mean(y)-m_fit*np.mean(x)\n\nprint(\"Fit coefs: m={} and b={}\".format(m_fit, b_fit))","b8f8849c":"plt.scatter(x, y)\nplt.plot(x, fit(x, b_fit, m_fit), color=\"red\")\nprint(\"RSS:{}\".format(calculate_rss(x, y, b_fit, m_fit, fit)))","c148227a":"x = np.random.uniform(0, 100, size= 30)\nepsilon = [gauss(0,10) for xi in x]\ny = 10 + 2*x + epsilon\n\nplt.scatter(x, y)","c010f316":"m_list = []\nb_list = []\nrss_list = []\n\nfor m in list(range(0,41)):\n    for b in list(range(0,41)):\n        m_list.append(m)\n        b_list.append(b)\n        rss_list.append(calculate_rss(x, y, b, m, fit))\n        \nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(projection='3d')\nax.scatter(m_list, b_list, rss_list)\nax.scatter(m_list[np.argmin(rss_list)], b_list[np.argmin(rss_list)], np.min(rss_list), color='red')\nax.set_xlabel('m', fontsize=20)\nax.set_ylabel('b', fontsize=20)\nax.set_zlabel('RSS', fontsize=20)\nplt.show()","652a8185":"m_fit = (np.sum((x - np.mean(x))*(y - np.mean(y))))\/np.sum((x-np.mean(x))**2)\nb_fit = np.mean(y)-m_fit*np.mean(x)\n\nprint(\"Fit coefs: m={} and b={}\".format(m_fit, b_fit))","188b137a":"plt.scatter(x, y)\nplt.plot(x, fit(x, b_fit, m_fit), color=\"red\")\n\nprint(\"RSS:{}\".format(calculate_rss(x, y, b_fit, m_fit, fit)))","4c45088d":"x = np.random.uniform(0, 100, size= 30)\ny = 10 + 2*x**2\nplt.scatter(x, y)","8eefca0e":"m_list = []\nb_list = []\nrss_list = []\n\nfor m in list(range(0,41)):\n    for b in list(range(0,41)):\n        m_list.append(m)\n        b_list.append(b)\n        rss_list.append(calculate_rss(x, y, b, m, fit))\n        \nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(projection='3d')\nax.scatter(m_list, b_list, rss_list)\nax.scatter(m_list[np.argmin(rss_list)], b_list[np.argmin(rss_list)], np.min(rss_list), color='red')\nax.set_xlabel('m', fontsize=20)\nax.set_ylabel('b', fontsize=20)\nax.set_zlabel('RSS', fontsize=20)\nplt.show()","dd8209cd":"m_fit = (np.sum((x - np.mean(x))*(y - np.mean(y))))\/np.sum((x-np.mean(x))**2)\nb_fit = np.mean(y)-m_fit*np.mean(x)\n\nprint(\"Fit coefs: m={} and b={}\".format(m_fit, b_fit))","73c1c5c9":"plt.scatter(x, y)\nplt.plot(x, fit(x, b_fit, m_fit), color=\"red\")","21243b27":"x = np.random.uniform(0, 100, size= 30)\ny = 10 + 2*x\n\nm_fit = (np.sum((x - np.mean(x))*(y - np.mean(y))))\/np.sum((x-np.mean(x))**2)\nb_fit = np.mean(y)-m_fit*np.mean(x)","f14010e6":"y_predicted = fit(x, b_fit, m_fit)","e9bbbd0a":"var_mean = np.var(y)\nvar_fit = np.sum((y - y_predicted)**2)\/len(y)\nr_squared = (var_mean - var_fit)\/var_mean\nr_squared","99f93904":"Ainda assim, nota-se que a linha da regress\u00e3o est\u00e1 em um meio termo entre os valores reais e os previstos em praticamente todo o espa\u00e7o.","f4eb17a5":"Mas se queremos uma reta que minimize o erro na predi\u00e7\u00e3o (ou seja, reduza o RSS) n\u00f3s podemos utilizar abordagens como c\u00e1lculo para encontra a equa\u00e7\u00e3o que leve a taxa de varia\u00e7\u00e3o do RSS para 0. Caso queira, [este v\u00eddeo](https:\/\/youtu.be\/DSQ2plMtbLc) mostra uma das maneiras de se chegar ao resultado esperado usando as derivadas parciais de b e m na equa\u00e7\u00e3o do RSS.\n\nIntuitivamente, o que queremos \u00e9 encontrar os coeficientes que minimizem o RSS. A figura abaixo mostra diferentes permuta\u00e7\u00f5es de $b$ e $m$, e qual o RSS correspondente. Quanto mais azul claro, menor o RSS, e pintamos de vermelho a combina\u00e7\u00e3o de RSS m\u00ednimo. ","f84ed42c":"# N\u00e3o Linearidade\n\nOs coeficientes que s\u00e3o estimados pelo m\u00e9todo dos m\u00ednimos quadrados seguem uma equa\u00e7\u00e3o linear, logo, se sua fun\u00e7\u00e3o objetivo seguir uma fun\u00e7\u00e3o n\u00e3o linear \u00e9 esperado que a regress\u00e3o linear n\u00e3o seja o melhor m\u00e9todo para se realizar as previs\u00f5es, contudo, algumas vezes os dados conseguem ser em parte bem explicados com essa regress\u00e3o, isto porque na minimiza\u00e7\u00e3o dos res\u00edduos ser\u00e1 desenvolvida uma linha que \"equilibre\" as predi\u00e7\u00f5es, n\u00e3o sendo o ideal, mas melhor que um modelo de *dummy* como o de classe majorit\u00e1ria.\n\nVejamos como exemplo uma distribui\u00e7\u00e3o com $y = 10 + 2x^2$","919ca05d":"# Avalia\u00e7\u00e3o da regress\u00e3o\n\nRegress\u00f5es s\u00e3o amplamente utilizadas a muito tempo e por \u00e1reas que n\u00e3o necessariamente conhecem muito de aprendizado de m\u00e1quina, e por isso foram estudadas v\u00e1rias maneiras de se avaliar o qu\u00e3o bom um modelo de regress\u00e3o linear \u00e9 para uma distribui\u00e7\u00e3o de dados.","e26d919d":"# Ru\u00eddo\n\nNa vida real, ainda que muitos fen\u00f4menos apresentem uma tend\u00eancia linear, raramente s\u00e3o *exatamente* lineares, isto \u00e9, existe algum ru\u00eddo dado pela aleatoriedade das observa\u00e7\u00f5es que pode demonstrar uma tend\u00eancia linear, mas cujo modelo n\u00e3o ter\u00e1 uma soma dos res\u00edduos igual \u00e0 zero.\n\nA figura abaixo mostra um exemplo deste fen\u00f4meno. Utilizamos a mesma forma de gerar os dados x e y, tal como a fun\u00e7\u00e3o $10 + 2x$, contudo adicionamos um ru\u00eddo gaussiano de m\u00e9dia 0 e desvio padr\u00e3o igual \u00e0 10. Como mencionado anteriormente, ru\u00eddo tamb\u00e9m \u00e9 tipicamente representado pela letra grega $\\epsilon$ (epsilon)","e1aa6857":"Dado que modificamos um pouco a fun\u00e7\u00e3o objetivo $y$, a melhor escolha de par\u00e2metros ent\u00e3o ser\u00e1 um pouco deslocada, devido o ru\u00eddo encontrado. Isso quer dizer que n\u00e3o necessariamente os coeficientes que definiam a fun\u00e7\u00e3o original ser\u00e3o os coeficientes \u00f3timos.","a8dee3cf":"O objetivo da regress\u00e3o linear \u00e9 encontrar a linha que melhor se ajuste \u00e0 estes dados. Poder\u00edamos come\u00e7ar por uma abordagem ing\u00eanua, usando valores aleat\u00f3rios de $b$ e $m$ em um intervalo para ver se chegamos em algum resultado interessante.","13875be3":"E assim derivamos o modelo linear que representa $y$ apenas com os dados de $x$ e $y$ e nenhuma informa\u00e7\u00e3o a mais.","d833b7e3":"Note que o fato de elevarmos ao quadrado o $x$ implica que nenhum um modelo linear precisa de coeficientes muito maiores para tentar explicar a distribui\u00e7\u00e3o de maneira linear.","b57351f0":"Note que os coeficientes encontrados foram justamente os que utilizamos para criar a fun\u00e7\u00e3o objetivo y, e isto est\u00e1 correto, pois n\u00e3o h\u00e1 nenhum ru\u00eddo em $y$ e neste caso pode se dizer que $y$ = $\\hat{y}$\n\nUtilizando estes coeficientes podemos ent\u00e3o gerar a linha correta que descreve estes dados:","e5a3be12":")N\u00e3o \u00e9 dif\u00edcil notar que a melhor reta encontrada \u00e9 a azul, pois se aproxima mais da distribui\u00e7\u00e3o real. H\u00e1 uma maneira de quantificar o quanto uma observa\u00e7\u00e3o est\u00e1 pr\u00f3xima da do modelo linear criado, e esta maneira se chama *soma dos quadrados dos res\u00edduos* (do ingl\u00eas vem a sigla RSS). O res\u00edduo \u00e9 o quanto uma aproxima\u00e7\u00e3o foi diferente do valor real de um ponto, e no RSS pegamos um ponto para calcularmos o res\u00edduo, elevamos ao quadrado, e fazemos o mesmo para todos os outros pontos para somarmos e termos a medida RSS.\n\n$$\nRSS = (y_1 - (\\hat{b}+\\hat{m}x_1))^2 + (y_2 - (\\hat{b}+\\hat{m}x_2))^2 + \\dots + (y_n - (\\hat{b}+\\hat{m}x_n))^2\n$$\n\nO chap\u00e9u em $\\hat{b}$ e $\\hat{m}$ \u00e9 aquele recurso que usamos da estat\u00edstica para dizer que os coeficientes s\u00e3o estimados, e n\u00e3o necessariamente os reais.\n\nA equa\u00e7\u00e3o do RSS pode ser vista tamb\u00e9m pelo algoritmo abaixo.","a25ea51d":"## $R^2$\n\nR quadrado \u00e9 uma medida para identificar o quanto uma predi\u00e7\u00e3o feita pela regress\u00e3o \u00e9 melhor que apenas chutar a m\u00e9dia. Isto \u00e9 feito identificando o quanto a vari\u00e2ncia dos dados explicados pelo modelo \u00e9 melhor do que a vari\u00e2ncia baseada na m\u00e9dia. Usando os dados que fizemos nosso primeiro teste ($y = 10 + 2x$) teremos que a vari\u00e2ncia do modelo explica 100% da vari\u00e2ncia dos dados, ou seja, \u00e9 100% melhor do que uma estima\u00e7\u00e3o pela m\u00e9dia.\n","7b523ce5":"# Regress\u00e3o Linear\n\nA arte de desenhar uma linha para representar um conjunto de dados chama a aten\u00e7\u00e3o de matem\u00e1ticos a muito tempo. Uma das manobras mais belas desenvolvidas e atualmente muito utilizadas no ramo de aprendizado de m\u00e1quina \u00e9 a regress\u00e3o linear simples e multivariada explicar o comportamento de dados que seguem uma tend\u00eancia linear.","da2cf18d":"# Bibliotecas","dfe6c367":"Note que os coeficientes mudaram um pouco, mas ainda s\u00e3o bem pr\u00f3ximos dos originais *dado que o ru\u00eddo adicionado tem m\u00e9dia 0, o que n\u00e3o ocorre sempre*.\n\nTamb\u00e9m podemos notar que a nova reta n\u00e3o se ajusta totalmente aos dados, mas \u00e9 o melhor modelo linear para este problema.","704ced10":"# Regress\u00e3o Linear Simples\n\nUma regress\u00e3o, como dito, \u00e9 uma t\u00e9cnica para tentar explicar sua distribui\u00e7\u00e3o de dados por um modelo linear AKA uma linha. \u00c9 extremamente importante que os dados que se procura predizer sigam essa tend\u00eancia linear, pois caso contr\u00e1rio a regress\u00e3o linear pode n\u00e3o ser a melhor abordagem. A tend\u00eancia linear significa que a fun\u00e7\u00e3o objetivo que o modelo de aprendizado de m\u00e1quina est\u00e1 buscando calcular \u00e9 do tipo:\n\n$$\ny = b + mx + \\epsilon\n$$\n\nOnde $\\epsilon$ \u00e9 um erro\/ru\u00eddo que idealmente teria m\u00e9dia 0 (um ru\u00eddo distribu\u00eddo aleatoriamente). O resto segue a equa\u00e7\u00e3o de uma reta, com $b$ \u00e9 o coeficiente linear (o $y$ quando $x$ \u00e9 0) e $m$ \u00e9 o coeficiente angular (a taxa em que $x$ cresce quando $y$ aumenta em uma unidade). \n\nPara ilustrar, veja uma distribui\u00e7\u00e3o com tend\u00eancia linear. Neste caso s\u00e3o tirados 30 valores entre 0 e 100 para definir as caracter\u00edsticas (X), e todas as caracter\u00edsticas levam \u00e0 uma resposta y que segue a f\u00f3rmula $y = 10 + 2x$ (veja que a fun\u00e7\u00e3o \u00e9 a fun\u00e7\u00e3o objetivo $y$ \u00e9 linear sem ru\u00eddo nenhum neste exemplo).","4aea76b8":"Neste caso, j\u00e1 come\u00e7amos a perceber que tem alguma coisa errada se olharmos como algumas permuta\u00e7\u00f5es de par\u00e2metros se saem:","133a0baf":"Por\u00e9m \u00e9 invi\u00e1vel fazer permuta\u00e7\u00f5es infinitas para cada novo problema, e por isso precisamos usar o RSS.\n\nFazendo o c\u00e1lculo de minimizar o RSS chegamos \u00e0 uma equa\u00e7\u00e3o utilizada para diferentes fins na estat\u00edstica, e neste caso, para gerar a regress\u00e3o linear: o *M\u00e9todo dos M\u00ednimos Quadrados*. Pelo fato da regress\u00e3o linear ser baseada neste m\u00e9todo, ela tamb\u00e9m \u00e9 comumente chamada de *Least Squares regression line*. Resumidamente, ao fim do c\u00e1lculo de minimiza\u00e7\u00e3o do RSS chegamos aos coeficientes:\n\n$$\n\\hat{m} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n$$\n\n$$\n\\hat{b} = \\bar{y} - \\hat{m}\\bar{x}\n$$\n\nCada um destes coeficientes podem ser calculados em apenas uma linha de c\u00f3digo:"}}