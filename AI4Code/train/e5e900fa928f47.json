{"cell_type":{"45bc629e":"code","dd3ad64e":"code","68d60d07":"code","b8340e38":"code","49245145":"code","04e1d535":"code","c20e6acc":"code","9aadbba8":"code","9e89eba3":"code","32e76572":"code","43911bcb":"code","10e0ff05":"code","04d8f383":"code","1705fdd1":"code","e699728e":"code","004cb69f":"code","459d629d":"code","f11f5bd8":"code","4611784a":"code","33bf14d9":"code","68255400":"code","a3f8927a":"code","e8862737":"code","a38e830e":"code","a41057d3":"code","3afbd86e":"code","d66382fc":"code","25221755":"code","4ae47fe3":"code","69b75b5b":"code","b9cf3ac2":"code","2a0f91cc":"code","7a27b944":"code","7f892ae6":"code","6c6c28bd":"code","2bcdb6cb":"markdown","9a70a383":"markdown","325347b9":"markdown","bb5b2892":"markdown","ed389a02":"markdown","cdeb69a2":"markdown","286c3091":"markdown","7e0bf66f":"markdown","9f3d186b":"markdown","5b5a4841":"markdown","cfeaa757":"markdown","b04f9183":"markdown","a4fb8d08":"markdown","f4c8af9f":"markdown","ba73d136":"markdown","0745e958":"markdown","aab3f189":"markdown","032fb67c":"markdown","9e8c6e84":"markdown","4984b6b3":"markdown","e13fd674":"markdown","7c88599e":"markdown","45271d20":"markdown","90ecb006":"markdown","966f000d":"markdown","544d355c":"markdown","84e48d04":"markdown","ca0c496d":"markdown"},"source":{"45bc629e":"# import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dd3ad64e":"# import data modelling libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","68d60d07":"# load the dataset\ndata= pd.read_csv(\"..\/input\/banking-dataset-classification\/new_train.csv\")\n\n# check shape of dataset\nprint(\"shape of the data:\", data.shape)\ndata.head()","b8340e38":"# check data types of all columns\ndata.dtypes","49245145":"data.isnull().sum()","04e1d535":"# target class count\ndata[\"y\"].value_counts()","c20e6acc":"sns.countplot(data[\"y\"])\nplt.title(\"target variable\")","9aadbba8":"# percentage of class present in target variable(y) \nprint(\"percentage of NO and YES\\n\",data[\"y\"].value_counts()\/len(data)*100)","9e89eba3":"# indentifying the categorical variables\ncat_var= data.select_dtypes(include= [\"object\"]).columns\nprint(cat_var)\n\n# plotting bar chart for each categorical variable\nplt.style.use(\"ggplot\")\n\nfor column in cat_var:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    data[column].value_counts().plot(kind=\"bar\")\n    plt.xlabel(column)\n    plt.ylabel(\"number of customers\")\n    plt.title(column)","32e76572":"# replacing \"unknown\" with the mode\nfor column in cat_var:\n    mode= data[column].mode()[0]\n    data[column]= data[column].replace(\"unknown\", mode)","43911bcb":"# indentifying the numerical variables\nnum_var= data.select_dtypes(include=np.number)\nnum_var.head()","10e0ff05":"# plotting histogram for each numerical variable\nplt.style.use(\"ggplot\")\nfor column in [\"age\", \"duration\", \"campaign\"]:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    sns.distplot(data[column], kde=True)\n    plt.title(column)","04d8f383":"data.drop(columns=[\"pdays\", \"previous\"], axis=1, inplace=True)","1705fdd1":"plt.style.use(\"ggplot\")\nfor column in cat_var:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    sns.countplot(data[column], hue=data[\"y\"])\n    plt.title(column)    \n    plt.xticks(rotation=90)","e699728e":"data.describe()","004cb69f":"# compute interquantile range to calculate the boundaries\nlower_boundries= []\nupper_boundries= []\nfor i in [\"age\", \"duration\", \"campaign\"]:\n    IQR= data[i].quantile(0.75) - data[i].quantile(0.25)\n    lower_bound= data[i].quantile(0.25) - (1.5*IQR)\n    upper_bound= data[i].quantile(0.75) + (1.5*IQR)\n    \n    print(i, \":\", lower_bound, \",\",  upper_bound)\n    \n    lower_boundries.append(lower_bound)\n    upper_boundries.append(upper_bound)","459d629d":"lower_boundries","f11f5bd8":"upper_boundries","4611784a":"# replace the all the outliers which is greater then upper boundary by upper boundary\nj = 0\nfor i in [\"age\", \"duration\", \"campaign\"]:\n    data.loc[data[i] > upper_boundries[j], i] = int(upper_boundries[j])\n    j = j + 1  ","33bf14d9":"# without outliers\ndata.describe()","68255400":"#categorical features\ncat_var","a3f8927a":"# check categorical class\nfor i in cat_var:\n    print(i, \":\", data[i].unique())","e8862737":"# initializing label encoder\nle= LabelEncoder()\n\n# iterating through each categorical feature and label encoding them\nfor feature in cat_var:\n    data[feature]= le.fit_transform(data[feature])","a38e830e":"# label encoded dataset\ndata.head()","a41057d3":"# feature variables\nx= data.iloc[:, :-1]\n\n# target variable\ny= data.iloc[:, -1]","3afbd86e":"plt.figure(figsize=(15,7))\nsns.heatmap(data.corr(), annot=True)","d66382fc":"#initialising oversampling\nsmote= SMOTETomek(0.75)\n\n#implementing oversampling to training data\nx_sm, y_sm= smote.fit_sample(x,y)\n\n# x_sm and y_sm are the resampled data\n\n# target class count of resampled dataset\ny_sm.value_counts()","25221755":"x_train, x_test, y_train, y_test= train_test_split(x_sm, y_sm, test_size=0.2, random_state=42)","4ae47fe3":"# selecting the classifier\nlog_reg= LogisticRegression()\n\n# selecting hyperparameter tuning\nlog_param= {\"C\": 10.0**np.arange(-2,3), \"penalty\": [\"l1\", \"l2\"]}\n\n# defining stratified Kfold cross validation\ncv_log= StratifiedKFold(n_splits=5)\n\n# using gridsearch for respective parameters\ngridsearch_log= GridSearchCV(log_reg, log_param, cv=cv_log, scoring= \"f1_macro\", n_jobs=-1, verbose=2)\n\n# fitting the model on resampled data\ngridsearch_log.fit(x_train, y_train)\n\n# printing best score and best parameters\nprint(\"best score is:\" ,gridsearch_log.best_score_)\nprint(\"best parameters are:\" ,gridsearch_log.best_params_)","69b75b5b":"# checking model performance\ny_predicted= gridsearch_log.predict(x_test)\n\ncm= confusion_matrix(y_test, y_predicted)\nprint(cm)\nsns.heatmap(cm, annot=True)\nprint(accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))","b9cf3ac2":"# random forest\nrf= RandomForestClassifier()\n\nrf_param= { \n           \"n_estimators\": [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n           \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n#            \"max_depth\": [4,5,6,7,8],\n           \"max_depth\": [int(x) for x in np.linspace(start=5, stop=30, num=6)],\n           \"min_samples_split\": [5,10,15,100],\n           \"min_samples_leaf\": [1,2,5,10],\n           \"criterion\":['gini', 'entropy'] \n          }\n\ncv_rf= StratifiedKFold(n_splits=5)\n\nrandomsearch_rf= RandomizedSearchCV(rf, rf_param, cv=cv_rf, scoring= \"f1_macro\", n_jobs=-1, verbose=2, n_iter=10)\n\nrandomsearch_rf.fit(x_train, y_train)\n\nprint(\"best score is:\", randomsearch_rf.best_score_)\nprint(\"best parameters are:\", randomsearch_rf.best_params_)","2a0f91cc":"# checking model performance\ny_predicted_rf= randomsearch_rf.predict(x_test)\n\nprint(confusion_matrix(y_test, y_predicted_rf))\nsns.heatmap(confusion_matrix(y_test, y_predicted_rf), annot=True)\nprint(accuracy_score(y_test, y_predicted_rf))\nprint(classification_report(y_test, y_predicted_rf))","7a27b944":"test_data= pd.read_csv(\"..\/input\/banking-dataset-classification\/new_test.csv\")\ntest_data.head()","7f892ae6":"# predicting the test data\ny_predicted= randomsearch_rf.predict(test_data)\ny_predicted","6c6c28bd":"# dataset of predicted values for target variable y\nprediction= pd.DataFrame(y_predicted, columns=[\"y_predicted\"])\nprediction_dataset= pd.concat([test_data, prediction], axis=1)\nprediction_dataset","2bcdb6cb":"### Handling imbalanced dataset\nSince the class distribution in the target variable is ~89:11 indicating an imbalance dataset, we need to resample it.","9a70a383":"### Observation :\n- As we can see from the histogram, the features `age`, `duration` and `campaign` are heavily skewed and this is due to the presence of outliers as seen in the boxplot for these features. \n- Looking at the plot for `pdays`, we can infer that majority of the customers were being contacted for the first time because as per the feature description for `pdays` the value 999 indicates that the customer had not been contacted previously. \n","325347b9":"Random Forest classifier has given the best metric score on the validation data.","bb5b2892":"### Logistic Regression","ed389a02":"### Handling Outliers\nOutliers cause significant impact on the Mean and Variance.It becomes necessary to treat the outliers.","cdeb69a2":"### check for class imbalance","286c3091":"Since, \n- for `age` the lower boundary (9.5) < minimum value (17)\n- for `duration` and `campaigh` the lower boundaries are negative (-221.0), (-2.0) resp.<br>\nreplacing outliers with the lower boundary is not required","7e0bf66f":"### Encoding Categorical Features\nMachine learning algorithm can only read numerical values. It is therefore essential to encode categorical features into numerical values","9f3d186b":"### Univariate analysis of Numerical columns","5b5a4841":"The missing values in some columns have been represented as `unknown`. `unknown` represents missing data.","cfeaa757":"### Bivariate Analysis of  Categorical Columns","b04f9183":"There are no features that are highly correlated and inversely correlated. If we had, we could have written the condition that if the correlation is higher than 0.8 (or can be any threshold value depending on the domain knowledge) and less than -0.8, we could have drop those features. Because those correlated features would have been doing the same job.","a4fb8d08":"## Exploratory Data Analysis\n### univariate analysis of categorical variables","f4c8af9f":"`age` `duration` and `campaign` are skewed towards right, we will compute the IQR and replace the outliers with the lower and upper boundaries","ba73d136":"### Observations:\n\n- Customers having administrative jobs form the majority amongst those who have subscirbed to the term deposit.\n- They are married \n- They hold a university degree\n- They do not hold a credit in default\n- Housing loan doesn't seem a priority to check for since an equal number of customers who have and have not subscribed to it seem to have subscribed to the term deposit.\n- Cell-phones should be the preferred mode of contact for contacting customers.","0745e958":"# About Dataset\nThere has been a revenue decline in a Portuguese Bank and they would like to know what actions to take. After investigation, they found that the root cause was that their customers are not investing enough for long term deposits. So the bank would like to identify existing customers that have higher chance to subscribe for a long term deposit and focus marketing efforts on such customers.","aab3f189":"### Observations :\n- The top three professions that our customers belong to are - administration, blue-collar jobs and technicians.\n- A huge number of the customers are married.\n- Majority of the customers do not have a credit in default\n- Many of our past customers have applied for a housing loan but very few have applied for personal loans.\n- Cell-phones seem to be the most favoured method of reaching out to customers.\n- Many customers have been contacted in the month of **May**.\n- The plot for the target variable shows heavy imbalance in the target variable. ","032fb67c":"### Prediction on the Test dataset\nWe have to perform the same preprocessing operations on the test data that we have performed on the train data. But here we already have preprocessed data which is present in the csv file new_test.csv","9e8c6e84":"Since `pdays` and `previous` consist majorly only of a single value, their variance is quite less and hence we can drop them since technically will be of no help in prediction.","4984b6b3":"### Splitting resampled data in train and test data","e13fd674":"### Random Forest","7c88599e":"### check missing data \nOne of the main steps in data preprocessing is handling missing data. Missing data means absence of observations in columns that can be caused while procuring the data, lack of information, incomplete results etc. Feeding missing data to your machine learning model could lead to wrong prediction or classification. Hence it is necessary to identify missing values and treat them.","45271d20":"The class distribution in the target variable is ~89:11 indicating an imbalance dataset","90ecb006":"## Gridsearch and hyperparameter tuning","966f000d":"### Separating independent and dependent variables","544d355c":"After replacing the outliers with the upper boundary, the maximum values has been changed without impacting any other parameters like mean, standard deviation and quartiles.","84e48d04":"### Checking Correlation of feature variables","ca0c496d":"Features like `job` `education` `month` `day_of_week ` has so many categories, we will Label Encode them as One Hot Encoding would create so many columns"}}