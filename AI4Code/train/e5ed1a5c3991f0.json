{"cell_type":{"a7fc7c46":"code","ffbb2708":"code","79f8a473":"code","5a53b7f7":"code","7c3b8272":"code","db188185":"code","615265d2":"code","a012b95c":"code","47f422d0":"code","cd6c68ea":"code","3e2be959":"code","c70582d6":"code","9429a40f":"code","c3ca57fb":"code","bb3eddf0":"code","6fa0fc27":"code","62cd40e5":"code","f3e2195a":"code","d20df002":"code","a272d87a":"code","5aa092a2":"code","54910972":"code","f3bb2704":"code","1e00bd1f":"code","8abb1e26":"code","f48d46f4":"code","5c0fe01e":"code","3ef01ad1":"code","bb87286f":"code","caf450d6":"code","c051ecda":"code","394af784":"code","04d17c41":"code","a27f3c6e":"code","184459be":"code","89f57370":"code","eb5c68f8":"code","3db711de":"code","b83a6cb7":"code","e8ff36eb":"code","54ccddf1":"code","d8c77132":"code","6efb9e01":"code","1f9d43f9":"code","009cc050":"code","c04e75e4":"code","6abd6cd1":"code","4ed78f35":"code","8e3c6262":"code","87b42267":"code","c1dac4a6":"code","3610e113":"code","d4da94d4":"code","3f8d8f75":"code","1b04272a":"code","92ce833e":"code","52b718ca":"code","8ca01883":"code","55961d3a":"code","2912d0d4":"code","cf548b1a":"code","ff7e3af0":"code","cfa6662b":"code","13e502da":"code","0ef1c072":"code","01468e7a":"code","5352cb77":"code","cc63a762":"code","d9840174":"markdown","dcf1a844":"markdown","bf2307dd":"markdown","0b19b9ea":"markdown","79aba79e":"markdown","ad122ad3":"markdown","7b61b8bd":"markdown","3b905337":"markdown","20671e27":"markdown","6cdc5e8a":"markdown","d2a42df2":"markdown","e6d490cb":"markdown","1487f1d1":"markdown","47b2379c":"markdown","518abd6e":"markdown","d73275a9":"markdown","a8c35305":"markdown","3f778837":"markdown","6a27f68c":"markdown","82dca166":"markdown","04777c56":"markdown","97262941":"markdown","40f0a01e":"markdown","d0b1d5c7":"markdown","fdad9774":"markdown","094fce11":"markdown","59d1a1ad":"markdown","066e0fbe":"markdown","578a0b59":"markdown","1113cf3f":"markdown"},"source":{"a7fc7c46":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.style as style\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import cross_val_score, train_test_split,KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nimport warnings\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nstyle.use('fivethirtyeight')\nwarnings.filterwarnings('ignore')","ffbb2708":"pd.set_option('display.max_rows',300)\npd.set_option('display.max_columns',300)","79f8a473":"hp = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhp_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","5a53b7f7":"hp.head()","7c3b8272":"hp_test.head()","db188185":"hp.shape,hp_test.shape","615265d2":"hp.info()","a012b95c":"hp_test.info()","47f422d0":"def missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    if df.isnull().sum().sum() != 0:\n        total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n        percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n        return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n    else:\n        print (f'Congrats, No null values in your dataframe')","cd6c68ea":"missing_percentage(hp)","3e2be959":"missing_percentage(hp_test)","c70582d6":"f,(ax1,ax2) = plt.subplots(nrows=1,ncols=2)\nf.set_figheight(7)\nf.set_figwidth(16)\nsns.heatmap(hp.isnull(),ax=ax1,cbar=False, yticklabels=False,cmap='viridis')\nsns.heatmap(hp_test.isnull(),ax=ax2,cbar=False, yticklabels=False,cmap='viridis')\nax1.set_title('Train Data')\nax2.set_title('Test Data')","9429a40f":"nnan_col = [\"BsmtQual\",'BsmtCond','BsmtFinType1','BsmtFinType2','BsmtExposure','GarageQual','GarageFinish','GarageType',\n            'GarageCond','FireplaceQu','Fence','Alley','MiscFeature','PoolQC']","c3ca57fb":"def fill_fun(df,columns_list):\n    for col in columns_list:\n        df[col].fillna(value='NA',inplace = True)","bb3eddf0":"fill_fun(hp,nnan_col)\nfill_fun(hp_test,nnan_col)","6fa0fc27":"def fill_missing(df):\n    for col in df.columns:\n        if df[col].dtypes == 'O':\n            df[col].fillna(value=df[col].mode(dropna=True)[0],inplace=True)\n        else:\n            df[col].fillna(value=df[col].median(),inplace=True)\n","62cd40e5":"fill_missing(hp)\nfill_missing(hp_test)","f3e2195a":"hp.GarageYrBlt = hp.GarageYrBlt.fillna(value=0.0)\nhp_test.GarageYrBlt = hp_test.GarageYrBlt.fillna(value=0.0)","d20df002":"missing_percentage(hp)","a272d87a":"missing_percentage(hp_test)","5aa092a2":"# c_columns = ['MSSubClass', 'OverallQual' , 'OverallCond' ]","54910972":"# def change_columns(df):\n#     column_wrong_type = c_columns\n#     for col in column_wrong_type:\n#         df[col]=df[col].astype(str)","f3bb2704":"# change_columns(hp)\n# change_columns(hp_test)","1e00bd1f":"def plotting_3_chart(df, feature):\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(15,10))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n    ","8abb1e26":"plotting_3_chart(hp, 'SalePrice')","f48d46f4":"stats.kurtosis(hp.SalePrice)","5c0fe01e":"## Plot fig sizing. \nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(hp.drop(columns=['Id']).corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(hp.drop(columns=['Id']).corr(), cmap=sns.diverging_palette(20, 220, n=200), mask = mask, annot=True, center = 0, cbar=False);\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","3ef01ad1":"feat_corr = abs(hp.corr().SalePrice).sort_values(ascending=False)[1:]\nfeat_corr","bb87286f":"def outliers_nan(df):\n    dff = df.drop(columns=['Id','SalePrice'])\n    for col in dff.columns:\n        if dff[col].dtypes != 'O':\n            IQR = np.percentile(dff[col],75) - np.percentile(dff[col],25)\n            upper_limit = np.percentile(dff[[col]],75)+(3*IQR)\n            lower_limit = np.percentile(dff[[col]],25)-(3*IQR)\n            df[col] = dff[col].apply(lambda x: np.nan if x > upper_limit or x < lower_limit else x) \n","caf450d6":"outliers_nan(hp)","c051ecda":"hp.isnull().sum().sort_values()","394af784":"hp_combined = pd.concat([hp,hp_test],join='inner')","04d17c41":"y = hp.SalePrice","a27f3c6e":"hp_combined = hp_combined[hp.columns[:-1]]","184459be":"hp_combined.head()","89f57370":"hp_combined.info()","eb5c68f8":"hp_combined_dum = pd.get_dummies(hp_combined, drop_first=True)\nhp_combined_dum.shape","3db711de":"X_train = hp_combined_dum.iloc[0:hp.shape[0],:]\nX_test =  hp_combined_dum.iloc[hp.shape[0]:,:]\ny= hp.SalePrice\nX_train['SalePrice']=y","b83a6cb7":"L = []\nfor col in X_train.columns:\n    try:\n        if (abs(X_trian.corr().SalePrice[col])>0.5):\n            L.append(col)\n    except:\n        L.append(col)\n        \n        \nL.remove('Id')\nc=L","e8ff36eb":"X_train = X_train[L].dropna()\nL.remove('SalePrice')","54ccddf1":"X_test = X_test[L]\ny = X_train[['SalePrice']]\nX_train.drop(columns='SalePrice', inplace=True)","d8c77132":"X_train.shape,X_test.shape,y.shape","6efb9e01":"ss = StandardScaler()\nX_train_ss = ss.fit_transform(X_train)\nX_test_ss = ss.fit_transform(X_test)","1f9d43f9":"lr_model = LinearRegression()\nlr_model.fit(X_train_ss,y)","009cc050":"cross_val_score(lr_model,X_train_ss,y).mean()","c04e75e4":"ls_model = Lasso(alpha=5)\ncross_val_score(ls_model.fit(X_train_ss,y),X_train_ss,y).mean()","6abd6cd1":"lscv_model = LassoCV()","4ed78f35":"cross_val_score(lscv_model.fit(X_train_ss,y),X_train_ss,y).mean()","8e3c6262":"rg_model = Ridge(alpha=5)","87b42267":"cross_val_score(rg_model.fit(X_train_ss,y),X_train_ss,y).mean()","c1dac4a6":"rgcv_model = RidgeCV(alphas=np.arange(0.1,10,0.1))","3610e113":"cross_val_score(rgcv_model.fit(X_train_ss,y),X_train_ss,y).mean()","d4da94d4":"encv_model = ElasticNetCV()","3f8d8f75":"cross_val_score(encv_model.fit(X_train_ss,y),X_train_ss,y).mean()","1b04272a":"rf_model = RandomForestRegressor(max_depth=15, random_state=101)","92ce833e":"cross_val_score(rf_model.fit(X_train_ss,y),X_train_ss,y).mean()","52b718ca":"grrf = GridSearchCV(rf_model, param_grid={'n_estimators':np.arange(1,50,1),'max_depth':np.arange(1,50,1)},\n                   n_jobs=-1,verbose=1)","8ca01883":"# grrf.fit(X_train_ss,y)","55961d3a":"\n# cross_val_score(grrf,X_train_ss,y,cv=5).mean()","2912d0d4":"knn_model= KNeighborsRegressor(n_neighbors=5)","cf548b1a":"cross_val_score(knn_model.fit(X_train_ss,y),X_train_ss,y).mean()","ff7e3af0":"from sklearn.svm import SVR","cfa6662b":"svm_model=SVR()","13e502da":"cross_val_score(svm_model.fit(X_train_ss,y),X_train_ss,y).mean()","0ef1c072":"submit = pd.DataFrame(columns=['Id','SalePrice'])","01468e7a":"submit.Id = hp_test.Id\nsubmit.SalePrice =( rgcv_model.predict(X_test_ss))\nsubmit.head()","5352cb77":"submit.to_csv('submission_rgcv_Final_t.csv',index=False)","cc63a762":"pd.read_csv('submission_rgcv_Final_t.csv')","d9840174":"<div class=\"alert alert-block alert-success\">\n\n- The best model is the LassoCV model, it scored 0.9101783479689268 on a cross validation. This result satisfys the original objective in the problem statments, since it is more than 0.9 accuracy score on cross validation scores mean.\n- LassoCV was able to score the best, because it was able to eleminate un-important features from the models.\n- Removing the outliers has positivly impaced the scores of the models. \n- Doing the grid search enhanced the results of the Random forest and the SVM Regressors. However, the score for the LassoCV is still better. This is essentially because in LassoCV there is a built in optimizer for the alpha value (kind of grid search).\n- It's recommended to blend the results of more than one model together to improve the accuracy.","dcf1a844":"<div class=\"alert alert-block alert-info\">\n    \n   ### Training Dataset\n   we have **3 columns** with **float64** data type.<br>\n   we have **35 columns** with **int64** data type.<br>\n   we have **43 columns** with **object** data type.<br>\n   Total = **81** column","bf2307dd":"<div class=\"alert alert-block alert-info\">\n    \n   ### Training Dataset\n   we have **11 columns** with **float64** data type.<br>\n   we have **23 columns** with **int64** data type.<br>\n   we have **46 columns** with **object** data type.<br>\n    Total = **80** column","0b19b9ea":"# Conclusion & Recommendations.","79aba79e":"## Kaggle Kernal Link\n","ad122ad3":"https:\/\/www.kaggle.com\/bilalyussef\/kernel8011b2c7e5\/edit","7b61b8bd":"<div class=\"alert alert-block alert-success\">\n    \n   For most citizens around the globe, buying a house is a lifetime project that affects their lifes from that point onward. Despite the crucial importance of this task, there aren't many predictive models to assess the price of a house given its features.<br>\n    \n   The aim of this project is **to devolope a predictive model to estimate the price of a house in Ames, Iowa in the United States**, Based on data and features giving about the house.<br>\n    \n   Through this project, different models will be implemented for the sake of sale price prediction. We'll start with the **simple Linear regression** then we will use **regulerization techniques** to help reduce the expected overfitting of the simple linear regression. will use **Lasso, Ridge and Elastic Net** teqniques, with and without CV (cross validation) to penelize the overfitted features. We'll be using also other advanced Regression teqniques such as **Random Forest Regressor, Support Vector Machine (SVM) regressor and the K Nearest Neighbor Regressor.** Inaddition, the Grid Search teqnique will be used to optimize the parameters in the models.\n   \n   At the End of this project, the model with the highest score will be chosen. The success of the project will be based on scoring at least 0.9 in the test split of the data.<br>\n   \n   A model with at least 90% accuracy would be beneficial for both people working in the houses sales sector and ordinary people who wants to buy new house.","3b905337":"# Lasso ","20671e27":"<div class=\"alert alert-block alert-info\">\n    \n  #### We can coclude the following from the above graphs.\n  - We have got many outliers in the target variable.\n  - We can see also that the Sale Price is not normally distibuted (referring to the Q-Q plot above). The Sale Price data is skewed to the right, we can see also the value of the kurtosis that indicates heavy tailed data.","6cdc5e8a":"## Combining the two datasets together to generate dummies.","d2a42df2":"<div class=\"alert alert-block alert-warning\">\n\n#### **Note: Houses that has no garages are assigned a zero value in the GarageYrBlt column","e6d490cb":"<div class=\"alert alert-block alert-warning\">\n    \n   ### Outliers, were assigned a NaN value and it will be dealt with later on. ","1487f1d1":"## Type changing","47b2379c":"## Obsevations","518abd6e":"# KNN","d73275a9":"# SVM","a8c35305":"# Ridge","3f778837":"## Percentage of Missing Values.","6a27f68c":"# TBA TEAM <br><br><br>\n<font size=\"4\">\n    \n- Bilal Yussef.<br><br><br>\n- Talal Al-Mutairi.<br><br><br>\n- Abdulrahman Al-Salamah.<br><br><br>\n<\/font>","82dca166":"## Let's now detect and deal with outliers\n- We'll be using the Tukey's method to detect the outliers.<br>\nLink: __[Tukey's Fence](http:\/\/http:\/\/sphweb.bumc.bu.edu\/otlt\/mph-modules\/bs\/bs704_summarizingdata\/bs704_summarizingdata7.html)__","04777c56":"<div class=\"alert alert-block alert-warning\">\n    \n   ### We'll be removing any column that has no significant correlation with the target variable (SalePrice). ","97262941":"## Visualizing the missing data","40f0a01e":"<div class=\"alert alert-block alert-success\">\n    \n   ### Now that we have no missing values, we can start investigating our data.\n   - We'll start with the Target Feature **(SalePrice)**","d0b1d5c7":"<div class=\"alert alert-block alert-danger\">\n   \n   - Many columns has NaN values.<br><br>\n   - Refering to the data description, it was found that the NaN values in many columns are not actually missing data. Rather they are NaNs either because the feature is not applicable **(i.e. BsmtQual includeds NaN values because some houses do not have basements. Consequently, it a value for basement quality can not be reported).**<br><br>\n   - The features like (BsmtQual) were searched, investegated and then a list of all such columns were saved in a list named **nnan_col** (Not NaN columns) .<br><br>\n   - also it was found that 3 catogrical variables were assigned a numercal values (ratings variables), and thus it was found as either float64 or int64. These columns were saved in a list named c_columns (catogrical columns).","fdad9774":"## Problem Statment","094fce11":"<div class=\"alert alert-block alert-info\">\n   \n   - The highest correlation is **0.88** between the **GarageCars** and  **GarageArea**.\n   - There is also high correlation between the **TotRmsAbvGrd** and **GrLivArea**, **0.83**.\n   - For the Target variable (SalePrice), some of the features has high correlation with the target while other has low correlation. below is a list of all the values of the correlations for the target sorted from highest to lowest","59d1a1ad":"<div class=\"alert alert-block alert-info\">\n    \n   we have **1460 rows** and **81 columns** in the **training dataset**.<br>\n   we have **1459 rows** and **80 columns** in the **testing dataset**.","066e0fbe":"# Randomforest","578a0b59":"# ElasticNet","1113cf3f":"# House Price Predictions Project"}}