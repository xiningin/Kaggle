{"cell_type":{"991798d7":"code","5b646ac2":"code","780e1743":"code","71d1b28d":"code","551294c1":"code","60c6c797":"code","909e0b5e":"code","f4f55528":"code","ec8b38b5":"code","9b14f425":"code","c77bdd6e":"code","d4266af0":"code","5c8159d5":"code","5b5f72d7":"code","b8e1ca81":"code","494b0d58":"code","5d0eb3ee":"code","404f5578":"markdown","e1f54904":"markdown","6278e07a":"markdown","2bd32ec0":"markdown","05cc7adf":"markdown","78f50c6c":"markdown","3defe3fd":"markdown"},"source":{"991798d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b646ac2":"import matplotlib.pyplot as plt\nimport PIL\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping","780e1743":"# pixels range from 0 to 255 so dividing by 255 so value lie between 0 and 1\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain = train_datagen.flow_from_directory(\n    directory = \"..\/input\/capital-alphabets-28x28\/train\",\n    target_size=(300, 300))","71d1b28d":"val = train_datagen.flow_from_directory(\n    directory = \"..\/input\/capital-alphabets-28x28\/validation\",\n    target_size=(300, 300))","551294c1":"test = val = train_datagen.flow_from_directory(\n    directory = \"..\/input\/capital-alphabets-28x28\/test\",\n    target_size=(300, 300))","60c6c797":"model = tf.keras.models.Sequential([\n    #input shape is the desired size of the image 300x300 with 3 bytes color\n    # This is the first convolution\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # The second convolution\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The third convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fourth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fifth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(512, activation='relu'),\n    # 26 output neuron.\n    tf.keras.layers.Dense(26, activation='softmax')\n])","909e0b5e":"model.summary()","f4f55528":"# Compiling model \nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","ec8b38b5":"# extracting values from val test generator\nX_val,y_val = next(val)","9b14f425":"checkpointer = EarlyStopping(monitor = 'val_accuracy', verbose = 1, restore_best_weights=True,mode=\"max\",patience = 25)\n# checkpointer to monitor accuracy and preventing overfit","c77bdd6e":"history = model.fit(\n            train ,\n            steps_per_epoch =len(train)\/\/32, #batch size is 32\n            epochs=1000,\n            verbose=1,\n            validation_data=(X_val,y_val),\n            callbacks = [checkpointer])","d4266af0":"training_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, val_loss, 'b-')\nplt.legend(['Training Loss', 'Val Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","5c8159d5":"training_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_accuracy) + 1)\n\n# Visualize loss history\n\nplt.plot(epoch_count, training_accuracy, 'r--')\nplt.plot(epoch_count, val_accuracy, 'b-')\nplt.legend(['Training Accuracy', 'Val Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.show()","5b5f72d7":"# saving the model\nmodel.save(\"alpharecognition.h5\")","b8e1ca81":"print(f\"Test accuracy = {model.evaluate(test)[1]}\")","494b0d58":"x_test,y_test = next(test)\npredict = model.predict(x_test)","5d0eb3ee":"figure = plt.figure(figsize=(20, 8))\nfor i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(x_test[index]))\n    predict_index = np.argmax(predict[index])\n    true_index = np.argmax(y_test[index])\n    ax.set_title(\"{} ({})\".format(chr(predict_index+65), \n                                  chr(true_index+65)),\n                                  color=(\"green\" if predict_index == true_index else \"red\"))","404f5578":"## Test Predictions Visualization","e1f54904":"## Test Accuracy","6278e07a":"## Model Creation","2bd32ec0":"> ###  If you find this notebook helpful, do consider to upvote.","05cc7adf":"## Processing Datasets","78f50c6c":"## Importing Libraries","3defe3fd":"## Model's Performance"}}