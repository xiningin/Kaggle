{"cell_type":{"24925664":"code","9a8c7a0a":"code","b60a9143":"code","f31434d2":"code","2772815c":"code","09cc7686":"code","20dbd772":"code","3329a9bc":"code","9ad33c7e":"code","785f9fea":"code","f97ddd9c":"code","cdce7f95":"code","657dcb29":"code","d0e01a3b":"code","81859442":"code","1b80d173":"code","06c847e8":"code","7920964b":"code","01a9891a":"code","a63b3c20":"code","940897d7":"code","82e8a67b":"code","8347564a":"markdown"},"source":{"24925664":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a8c7a0a":"!pip install --upgrade transformers\n!pip install simpletransformers\n# memory footprint support libraries\/code\n!ln -sf \/opt\/bin\/nvidia-smi \/usr\/bin\/nvidia-smi\n!pip install gputil\n!pip install psutil\n!pip install humanize","b60a9143":"import psutil\nimport humanize\nimport os\nimport GPUtil as GPU\n\nGPUs = GPU.getGPUs()\ngpu = GPUs[0]\ndef printm():\n    process = psutil.Process(os.getpid())\n    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\nprintm()","f31434d2":"import numpy as np\nimport pandas as pd\n#from google.colab import files\nfrom tqdm import tqdm\nimport warnings\nwarnings.simplefilter('ignore')\nimport gc\nfrom scipy.special import softmax\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nimport sklearn\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport re\nimport random\nimport torch\npd.options.display.max_colwidth = 200\n\ndef seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n\nseed_all(2)","2772815c":"train=pd.read_csv('\/kaggle\/input\/train-data\/Train_BNBR.csv')\ntest=pd.read_csv('\/kaggle\/input\/health-data\/Test_health.csv')\nsub = pd.read_csv('\/kaggle\/input\/health-data\/ss_health.csv')","09cc7686":"train.head()","20dbd772":"test.head()","3329a9bc":"from sklearn.preprocessing import LabelEncoder","9ad33c7e":"train['label'] = train['label'].astype('category')  ","785f9fea":"train['label'] = train['label'].cat.codes\ntrain.head()","f97ddd9c":"train.label.value_counts()","cdce7f95":"print(train['text'].apply(lambda x: len(x.split())).describe())","657dcb29":"print(test['text'].astype(str).apply(lambda x: len(x)).describe())","d0e01a3b":"train1=train.drop(['ID'],axis=1)\ntest1=test.drop(['ID'],axis=1)\ntest1['target']=0","81859442":"import math","1b80d173":"#%%writefile setup.sh\n\n#git clone https:\/\/github.com\/NVIDIA\/apex\n#pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/apex","06c847e8":"#!sh setup.sh","7920964b":"%%time\nerr=[]\ny_pred_tot=[]\n\nfold=StratifiedKFold(n_splits=20, shuffle=True, random_state= 2)\ni=1\nfor train_index, test_index in fold.split(train1,train1['label']):\n    train1_trn, train1_val = train1.iloc[train_index], train1.iloc[test_index]\n    model = ClassificationModel('roberta', 'roberta-base', use_cuda= False,num_labels= 4, args={'train_batch_size':32,\n                                                                         'reprocess_input_data': True,\n                                                                         'overwrite_output_dir': True,\n                                                                         \"fp16\": False,\n                                                                         #\"fp16_opt_level\": \"O2\",\n                                                                         'do_lower_case': True,\n                                                                         'num_train_epochs': 4,\n                                                                         'max_seq_length': 128,\n                                                                         #\"adam_epsilon\": 1e-4,\n                                                                         #\"warmup_ratio\": 0.06,\n                                                                         'regression': False,\n                                                                         #'lr_rate_decay': 0.4,\n                                                                         'manual_seed': 2,\n                                                                         \"learning_rate\":1e-4,\n                                                                         'weight_decay':0,\n                                                                         \"save_eval_checkpoints\": False,\n                                                                         \"save_model_every_epoch\": False,\n                                                                         \"silent\": True})\n    model.train_model(train1_trn)\n    raw_outputs_val = model.eval_model(train1_val)[1]\n    raw_outputs_val = softmax(raw_outputs_val,axis=1)   #[:,1]  \n    print(f\"Log_Loss: {log_loss(train1_val['label'], raw_outputs_val)}\")\n    err.append(log_loss(train1_val['label'], raw_outputs_val)) \n    raw_outputs_test = model.eval_model(test1)[1]\n    raw_outputs_test = softmax(raw_outputs_test,axis=1)  #[:,1]  \n    y_pred_tot.append(raw_outputs_test)\nprint(\"Mean LogLoss: \",np.mean(err))","01a9891a":"final=np.mean(y_pred_tot, 0)\nfinal= pd.DataFrame(final)\nfinal.head()","a63b3c20":"# Current best Mean LogLoss:  0.3973383729609846 (20fold_rbb_1_1e4_wd_0_32_128_epoch4_true), LB = 0.3726","940897d7":"final.to_csv('20fold_rbb_1_1e4_wd_0_32_128_epoch4_clean_data.csv',index=False)","82e8a67b":"#files.download(\"20fold_rbb_1_1e4_wd_0_32_40_epoch4_clean_data.csv\")","8347564a":"# Roberta Large Model 1"}}