{"cell_type":{"3bbff7db":"code","c5da0a2a":"code","325b368c":"code","354f1405":"code","8709dc16":"code","a4b68d2e":"code","1c3e6918":"code","55b1c47a":"code","b2e7dab9":"code","df511dec":"code","db17e5cf":"code","1fbf54f9":"code","4f8ab846":"code","0f6c1739":"code","df05a16a":"code","d77a29e0":"code","b38e3ea0":"code","43c80c3f":"code","f6c7e9b9":"code","182dc1fd":"code","f89134eb":"code","a74ba304":"code","d42d233d":"code","3b1d61f8":"code","31fde599":"code","2f752cce":"code","3820d704":"code","2c6ae168":"code","7bc64fea":"code","cd8f8fc7":"code","99e171af":"code","5c4f20d6":"code","0075be99":"code","75e98bf7":"code","8472d013":"code","4c84c948":"code","2d95f38b":"code","d9e16fed":"code","6cc8b688":"code","39753451":"code","bf234d07":"code","4f4c1fd8":"code","93595c3e":"code","a55566aa":"code","20207c0c":"code","8c048ba9":"code","4d35cadc":"code","18a644d8":"code","ed9fec6d":"code","306cef77":"code","f8dacd32":"code","03896c90":"markdown","12d872bd":"markdown","3c514dcd":"markdown","e0339c70":"markdown","bfd35c6f":"markdown","4d3a3682":"markdown","79ea1206":"markdown","2ce81fd3":"markdown","ab984796":"markdown","34133801":"markdown","513502d9":"markdown","a8357acd":"markdown","7ce57f9b":"markdown","83671798":"markdown","8f856edc":"markdown"},"source":{"3bbff7db":"#Ignoring unnecessory warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#for large and multi-dimensional arrays\nimport numpy as np\n\n#for data manipulation and analysis\nimport pandas as pd\n\n#Natural language processing tool-kit\n#NLTK is a leading platform for building Python programs to work with human language data\nimport nltk\n\n#Stopwords corpus\nfrom nltk.corpus import stopwords\n\n#stemmer\nfrom nltk.stem import PorterStemmer\n\n#for Bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#for TF-IDF\n# from sklearn.feature_extraction.text import TfidVectorizer\n\n#for Word2vec\nfrom gensim.models import Word2Vec\n\nimport re\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting","c5da0a2a":"from gensim.models import Word2Vec, word2vec\nimport logging\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.pipeline import Pipeline\nimport sqlite3\nimport re\nfrom tqdm import tqdm","325b368c":"logging.basicConfig(level=logging.INFO)\n%matplotlib inline","354f1405":"# Load the punkt tokenizer used for splitting reviews into sentences\ntokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')","8709dc16":"df = pd.read_csv('..\/input\/reviewdata\/Review.csv');\n#Getting the head of data\n\nprint(df.head());","a4b68d2e":"print(df.columns);","1c3e6918":"#Remove neutral reviews\ndf_l = df[df['Score']!=3]","55b1c47a":"#Converting score values into class label either Positive or Negative\n\ndef partition(x):\n    if x < 3:\n        return 'negative'\n    return 'positive'\n\nscore_upd = df_l['Score']\nt = score_upd.map(partition)\ndf_l['Score'] = t","b2e7dab9":"print(df_l.head())","df511dec":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","db17e5cf":"plotPerColumnDistribution(df.head(), 10, 5)","1fbf54f9":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","4f8ab846":"plotScatterMatrix(df, 12, 10)","0f6c1739":"#Dropping duplicates\nfinal_data = df_l.drop_duplicates(subset={\"UserId\", \"ProfileName\", \"Time\", \"Text\"})","df05a16a":"#Helpfulness numerator should always be less than Denominator\nfinal = final_data[final_data['HelpfulnessNumerator'] <= final_data['HelpfulnessDenominator']]\n\nfinal_X = final['Text']\nfinal_Y = final['Score']","d77a29e0":"# find sentences containing HTML tags\ni = 0;\nfor sent in final_X.values:\n    if (re.findall('<.*?>',sent)):\n        print(i)\n        print(sent)\n        break;\n    i +=1","b38e3ea0":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = set(stopwords.words('english')) #set of stopwords\nsno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemming\n\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr,' ',sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return cleaned\nprint(stop)","43c80c3f":"#code for implementing step-by-step the checks mentioned in the pre-process\ni = 0\nstr1 = ' '\nfinal_string = []\nall_positive_words = [] #store words from +ve reviews here\nall_negative_words = [] #store words from -ve reviews here\ns = ''\nfor sent in final_X.values:\n    filtered_sentence=[]\n    sent = cleanhtml(sent) #remove HTML tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if ((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n                if(cleaned_words.lower() not in stop):\n                    s = (sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final_Y.values)[i]=='positive':\n                        all_positive_words.append(s) #list of all words that are positve\n                    if (final_Y.values)[i]=='negative':\n                        all_negative_words.append(s) #list of all words that are negative\n                else:\n                    continue\n            else:\n                continue\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    final_string.append(str1)\n    i +=1\n# print(final_string)\n","f6c7e9b9":"final_X = final_string","182dc1fd":"#Bag of Words\ncount_vect = CountVectorizer()\nfinal_counts = count_vect.fit_transform(final_X)","f89134eb":"type(final_counts)","a74ba304":"final_counts.shape","d42d233d":"len(all_positive_words)","3b1d61f8":"freq_dist_positive = nltk.FreqDist(all_positive_words)\nfreq_dist_negative = nltk.FreqDist(all_negative_words)\nprint(\"Most Common Positive words : \",freq_dist_positive.most_common(20))\nprint(\"Most Common Negative Words : \",freq_dist_negative.most_common(20))","31fde599":"#bi_gram\ncount_vect = CountVectorizer(ngram_range=(1,2))\nfinal_bigram_counts = count_vect.fit_transform(final_X)","2f752cce":"final_bigram_counts.shape","3820d704":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nfinal_tf_idf = tf_idf_vect.fit_transform(final['Text'].values)","2c6ae168":"final_tf_idf.shape","7bc64fea":"connection = sqlite3.connect('..\/input\/reviewsdata\/data.sqlite')\nreviews = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\", connection)\nconnection.close()","cd8f8fc7":"reviews.hist('Score')","99e171af":"reviews['Class'] = 1 * (reviews['Score']>3)","5c4f20d6":"reviews.head(n = 2)","0075be99":"reviews.sort_values('ProductId', axis=0, inplace=True)","75e98bf7":"train_size = int(len(reviews) * 0.5)\ntrain_reviews = reviews.iloc[:train_size,:]\ntest_reviews = reviews.iloc[train_size:,:]","8472d013":"test_remove = np.logical_or(test_reviews['ProductId'].isin(train_reviews['ProductId']),\n                          test_reviews['UserId'].isin(train_reviews['UserId']))\ntest_reviews = test_reviews[np.logical_not(test_remove)]","4c84c948":"print('Training set contains {:d} reviews.'.format(len(train_reviews)))\nprint('Test set contains {:d} reviews ({:d} removed).'.format(len(test_reviews), sum(test_remove)))","2d95f38b":"n_pos_train = sum(train_reviews['Class'] == 1)\nprint('Training set contains {:.2%} positive reviews'.format(n_pos_train\/len(train_reviews)))\nn_pos_test = sum(test_reviews['Class'] == 1)\nprint('Test set contains {:.2%} positive reviews'.format(n_pos_test\/len(test_reviews)))","d9e16fed":"del reviews","6cc8b688":"def review_to_wordlist(review, remove_stopwords=False):\n    \"\"\"\n    Convert a review to a list of words. Removal of stop words is optional.\n    \"\"\"\n    # remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n    \n    # convert to lower case and split at whitespace\n    words = review_text.lower().split()\n    \n    # remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n\n    return words","39753451":"def review_to_sentences(review, tokenizer, remove_stopwords=False):\n    \"\"\"\n    Split review into list of sentences where each sentence is a list of words.\n    Removal of stop words is optional.\n    \"\"\"\n    # use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n\n    # each sentence is furthermore split into words\n    sentences = []    \n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n            \n    return sentences","bf234d07":"train_sentences = []  # Initialize an empty list of sentences\nfor review in train_reviews['Text']:\n    train_sentences += review_to_sentences(review, tokenizer)","4f4c1fd8":"train_sentences[0]","93595c3e":"model_name = 'train_model'\n# Set values for various word2vec parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 3       # Number of threads to run in parallel\ncontext = 10          # Context window size\ndownsampling = 1e-3   # Downsample setting for frequent words\nif not os.path.exists(model_name): \n    # Initialize and train the model (this will take some time)\n    model = word2vec.Word2Vec(train_sentences, workers=num_workers, \\\n                vector_size=num_features, min_count = min_word_count, \\\n                window = context, sample = downsampling)\n\n    # If you don't plan to train the model any further, calling \n    # init_sims will make the model much more memory-efficient.\n    model.init_sims(replace=True)\n\n    # It can be helpful to create a meaningful model name and \n    # save the model for later use. You can load it later using Word2Vec.load()\n    model.save(model_name)\nelse:\n    model = Word2Vec.load(model_name)","a55566aa":"del train_sentences","20207c0c":"model.wv.doesnt_match(\"banana apple orange sausage\".split())","8c048ba9":"model.wv.doesnt_match(\"vanilla chocolate cinnamon dish\".split())","4d35cadc":"model.wv.most_similar(\"great\")","18a644d8":"model.wv.most_similar(\"great\")","ed9fec6d":"model.wv.most_similar(positive=['woman', 'king'], negative=['man'])","306cef77":"model.wv.similar_by_vector(model.wv['beer'] - model.wv['alcohol'])","f8dacd32":"vocab = model.wv.index_to_key\nprint(len(vocab))","03896c90":"**Labelling good and bad reviews**","12d872bd":"Amazon Fine Food Reviews Analysis\n\nData Source: https:\/\/www.kaggle.com\/subhomoy23\/reviewdata\n\nThe Amazon Fine Food Reviews dataset consists of revieews of fine foods from Amazon.\n\nObjective:\n\nGiven a review, determine whether the review is positive or negative.","3c514dcd":"**Training a word2vec model**","e0339c70":"**Distribution graphs** (histogram\/bar graph) of sampled columns:","bfd35c6f":"**EXPLORATORY DATA ANALYSIS**\n\nPreprocessing-\nWe cannot work wit the text data in machine learning so we need to convert them into numerical vectors. This project has all techniques for conversion.\n\nText data needs to be cleaned and encoded to numerical values before giving them to machine learning models, this process of cleaning and encoding is called as Text Preprocessing.\nUnderstanding the data - See what s the data is all about, what should be considered for cleaning for data (Punctuations, stopwords etc..)\n\nBasic Cleaning - We will see what parameters need to be considered for cleaning of data (like Punctuations, stopwords) and its code.\n\nTechniques for Encoding - All the popular techniques that are used for encoding that I personally came across.\nBag of Words\nBinary Bag of Words\nBigram, Ngram\nTF-IDF(Term Frequency - Inverse Document Frequency)\nWord2Vec\nAvg-Word2Vec\nTF-IDF Word2Vec\n\nData Cleaning: Deduplication \nReviews may have duplicate entries. Hence it is necessary to remove duplicates on order to get unblased results for the analysis of the data.\n","4d3a3682":"**Bi-Gram BoW**\n\nConsidering pair of words for creating dictionary is BI-GRAM, TRI-GRAM means three consecutive words so as NGram.\n\nCountVectorizer has a parameter ngram_range is assigned to (1,2) it considers Bi-gram BoW, but this massively increases our dictionary size.","79ea1206":"**Split into training and test sets**\n\nSplit the data set into training and test sets. To ensure independence of training and test set and generalizability of the model, make sure that no product (identified by ProductId) and no user (identified by UserId) is present in both training and test set.\n\nThis is implemented by first sorting the data set by ProductId, splitting into equally sized training and test set (no shuffling!) and lastly removing any reviews from the test set where either user or product ID also appears in the training set.","2ce81fd3":"Converting all words to lowercase and removing punctuations and html tags if any\n\nStemming- Converting the words into their base word or stem word (Ex-tastefully, tasty, these words are converted to stem word called 'tasty'). This reduces the vector dimension because we don t consider all similar words\nStopwords- Stopwords are the unnecessary words that even if they are removed the sentiment of the sentence dosent change\n\nEx - Tis pasta is so tasty ==> pasta tasty (Thi, is, so are stopwords so they are removed)\n\nTo see all the stopwords see the below code cell.","ab984796":"**Drawbacksof BoW\/ Binary BoW**\n\nOur main objective in doing these text to vector encodings is that similar meaning text vectors should be close to each other, but in some cases this may not be possible for BoW.\n\nFor example, if we consider 2 reviews, like 'This pie is very tasty' and 'This pie is not tasty' after stopwords removal both sentences will be converted to 'pie tasty', so both are giving exact same meaning.\n\nThe main problem is here we are not considering the front and back words related to every word, here comes Bigram and Ngram techniques.","34133801":"**Reading Data**","513502d9":"**TF-IDF** \n\nTF-IDF stands for \u201cTerm Frequency \u2014 Inverse Document Frequency\u201d. This is a technique to quantify words in a set of documents. We generally compute a score for each word to signify its importance in the document.","a8357acd":"**Preparing review text**\n\nConvert each review in the training set to a list of sentences where each sentence is in turn a list of words. Besides splitting reviews into sentences, non-letters and (optionally) stop words are removed and all words coverted to lower case.","7ce57f9b":"**Bag of words**\n\nIn BoW we construnct a dictionary that contains set of unique words from our text review dataset. The frequency of the word is counted here. If they are n unique words in our dictionary then for every sentece or review the vector will be of length n and count of word from review is stored as its particular location in vector. The vector will be higly sparse.\n\nUsing scikit-learn's CountVectorize we can get the BoW and check out all the parameters it consists of.\n\n**Binary Bag Of Words**\n\nin binary BoW, we don't count the frequency of word, we just place 1 if the word appears in the review or else 0. In CountVectorize there is a parameter binary equals with true this makes BoW to binary BoW.","83671798":"The vector representations for words allow us to explore what the model learned. Using the distance between embedded words, we can find that are similar or dissimilar to one another.","8f856edc":"If we see the Score column, it has values 1,2,3,4,5. Considering 1,2 as negative reviews and 4,5 as positive reviews. For Score = 3 we will consider it as Neutral review and lets delete the rows that are neutral, so that we cand predict either Positive or Negative.\n\nHelpfulnessNumerator says about number of people found that review usefull and HelpfulnessDenominator is about usefull review count + not so usefull count.So, from this we can see that HelpfulnessNumerator is always less than or equal to HelpfulnessDenominator."}}