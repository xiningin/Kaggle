{"cell_type":{"f931e37e":"code","a9665d55":"code","7813e335":"code","70aaed44":"code","8bfb7c7f":"code","fb62f3ce":"code","d11d783b":"code","3808f61f":"code","791dfb7a":"code","4ab10bcd":"code","37f4e2fe":"code","2e8b28e7":"code","b9254a32":"code","c654be8c":"code","52ba0aac":"code","ec0ca88e":"code","80f7b03b":"code","0b0e104f":"code","10d92140":"code","41db4e16":"markdown","0c701213":"markdown","e8b91fd9":"markdown","02991151":"markdown","dfc616b5":"markdown","df32cfc5":"markdown"},"source":{"f931e37e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport math","a9665d55":"train = pd.read_csv(\"..\/input\/liverpool-ion-switching\/train.csv\")\n\n\nprint(train.shape)\n\n# df  =train.copy()\n# data=df.values\n","7813e335":"test = pd.read_csv(\"..\/input\/liverpool-ion-switching\/test.csv\")\nprint(test.shape)","70aaed44":"n_groups = 100\n# df[\"group\"] = 0\n# for i in range(n_groups):\n#     ids = np.arange(i*50000, (i+1)*50000)\n#     df.loc[ids,\"group\"] = i\n\ntrain['group'] = 0\ntrain.loc[:500000, 'group'] = 1 \ntrain.loc[500000:500000*2, 'group'] = 2\ntrain.loc[500000*2:500000*3, 'group'] = 3\ntrain.loc[500000*3:500000*4, 'group'] = 4\ntrain.loc[500000*4:500000*5, 'group'] = 5\ntrain.loc[500000*5:500000*6, 'group'] = 6\ntrain.loc[500000*6:500000*7, 'group'] = 7\ntrain.loc[500000*7:500000*8, 'group'] = 8\ntrain.loc[500000*8:500000*9, 'group'] = 9\ntrain.loc[500000*9:500000*10, 'group'] = 10\n\n\ntest['group'] = 0\ntest.loc[:500000, 'group'] = 11 \ntest.loc[500000:500000*2, 'group'] = 12\ntest.loc[500000*2:500000*3, 'group'] = 13\ntest.loc[500000*3:500000*4, 'group'] = 14","8bfb7c7f":"test.nunique()","fb62f3ce":"train.head()","d11d783b":"### I oerwrite the original time column, to make it easy to \"plug and play\" into other code that expect the time col \ntest[\"time\"] = test.groupby(\"group\").cumcount()+1\ntrain[\"time\"] = train.groupby(\"group\").cumcount()+1\n\ndisplay(test.groupby(\"group\")[\"time\"].head(2))\ndisplay(test.groupby(\"group\")[\"time\"].tail(2))","3808f61f":"test","791dfb7a":"train.describe()","4ab10bcd":"# for i in range(n_groups):\n#     sub = df[df.group == i]\n#     signals = sub.signal.values\n#     imax, imin = math.floor(np.max(signals)), math.ceil(np.min(signals))\n#     signals = (signals - np.min(signals))\/(np.max(signals) - np.min(signals))\n#     signals = signals*(imax-imin)\n#     df.loc[sub.index,\"open_channels\"] = [0,] + list(np.array(signals[:-1],np.int))","37f4e2fe":"from scipy.stats import zscore\ntest[\"signal_zscore\"] = test.groupby([\"group\"])[\"signal\"].transform(lambda x : zscore(x,ddof=1))\ntrain[\"signal_zscore\"] = train.groupby([\"group\"])[\"signal\"].transform(lambda x : zscore(x,ddof=1))","2e8b28e7":"train.tail()","b9254a32":"arbitrary_time_start = pd.to_datetime(1490195805403502912, unit='ns')\nprint(arbitrary_time_start)\n\ntest[\"datetime\"] = arbitrary_time_start + pd.to_timedelta(test[\"time\"]*10,unit=\"ms\")\ntrain[\"datetime\"] = arbitrary_time_start + pd.to_timedelta(train[\"time\"]*10,unit=\"ms\")\n\ntest.tail()","c654be8c":"train.head()","52ba0aac":"train.to_csv(\"train_ionChannels.csv.gz\",index=False,compression=\"gzip\")\ntest.to_csv(\"test_ionChannels.csv.gz\",index=False,compression=\"gzip\")","ec0ca88e":"## We are not going to bother with sample submission, let's just test this smoothing method against the labels in the training data. Should be fine.\n# print(data[:5,1])\n# print(data[:5,2])","80f7b03b":"# prediction = np.array(df.open_channels, np.int)\n# print(prediction[:5])","0b0e104f":"# print(prediction.shape)\n# prediction.tail()","10d92140":"# #To check I am working the metrics right ;-)\n# gd=[1,2,3,4,5,6,7,8,9,0]\n# pr=[1,2,3,4,5,6,7,8,8,0]\n\n# from sklearn.metrics import cohen_kappa_score , accuracy_score ,f1_score\n\n# print(\"Regular Cohen's Kappa\", cohen_kappa_score(np.asarray(data[:,2],np.int),np.array(df.open_channels, np.int),weights=\"quadratic\"))\n# print(\"Quadratic Cohen's Kappa\", cohen_kappa_score(np.asarray(data[:,2],np.int),np.array(df.open_channels, np.int)))\n# print(\"Accuracy\", accuracy_score(data[:,2],np.array(df.open_channels, np.int)))\n# print(\"f1\", f1_score(data[:,2],np.array(df.open_channels, np.int)))\n# print(\"test Accuracy\", accuracy_score(gd,pr))","41db4e16":"##### add arbitrary start time\n* each step is 0.1 ms (10khz measurement).  We may want to arbitrarily scale up due to worries about precision , although it shouldn;'t be ap roblem.  The real times are important for easy feature engineering using biological prior knowledge","0c701213":"## Features:\n* Z-score per group\n* coiuld add more , e.g. change vs minmax. \n\n* e.g. `df.groupby('batch')['signal'].rolling(r).mean().reset_index()['signal']`","e8b91fd9":"##### Goal: \n* Predict the number of open_channels present, based on electrophysiological signal data. (at each time step and for each time series). \n\n* IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\n\nI'll do: \n\n* Simple baseline around min\/max\/average of having X channels opened. \n* Add the groups to segment the data (ech group is a seperate experiment) , for evaluation and further features ","02991151":"## New Times\n* make new running \"time\" index pergroup = time that has passed, within the batch\/group**\n*  https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.core.groupby.GroupBy.cumcount.html#pandas.core.groupby.GroupBy.cumcount\n\n* We can also add a real datetime and add these values to it with `pd.to_TimeDelta()` , for time series methods that expect it, based on this ","dfc616b5":"Looks good!\nSo get the metrics, F1, Kappa, Quadratic Kappa and Accuracy;","df32cfc5":"#### forked code after htis. commented out for now"}}