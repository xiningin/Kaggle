{"cell_type":{"bb92cd32":"code","688d3288":"code","4b52313a":"code","2ffcdc90":"code","922069ba":"code","eb67a242":"code","d53647a0":"code","905d3e3c":"code","22fabfa0":"code","78f2f20e":"code","ba4f1ffc":"code","fa5eca08":"code","0c748458":"code","6eead30d":"code","5363322b":"code","5421c77b":"code","6c781a3e":"code","e7b9f848":"code","2b00345f":"markdown","17d6c247":"markdown","bae59962":"markdown","b24feb53":"markdown","fbb005b2":"markdown","1d1b86bf":"markdown","18f9dde9":"markdown","2b9ac9a1":"markdown","f86c9cd3":"markdown","44b5b417":"markdown","2858f6bc":"markdown","4cfdb6f2":"markdown","e43fa27f":"markdown","531285e9":"markdown","323d83be":"markdown","86688fca":"markdown"},"source":{"bb92cd32":"# MNIST\nfrom keras.datasets import mnist\nfrom matplotlib import pyplot\n# load dataset\n(trainX, trainy), (testX, testy) = mnist.load_data()\n# summarize loaded dataset\nprint('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\nprint('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n# plot first few images\nfor i in range(9):\n\t# define subplot\n\tpyplot.subplot(330 + 1 + i)\n\t# plot raw pixel data\n\tpyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n# show the figure\npyplot.show()","688d3288":"# Fashion MNIST\n# example of loading the fashion mnist dataset\nfrom matplotlib import pyplot\nfrom keras.datasets import fashion_mnist\n# load dataset\n(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n# summarize loaded dataset\nprint('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\nprint('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n# plot first few images\nfor i in range(9):\n\t# define subplot\n\tpyplot.subplot(330 + 1 + i)\n\t# plot raw pixel data\n\tpyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n# show the figure\npyplot.show()","4b52313a":"#CIFAR-10\n# example of loading the cifar10 dataset\nfrom matplotlib import pyplot\nfrom keras.datasets import cifar10\n# load dataset\n(trainX, trainy), (testX, testy) = cifar10.load_data()\n# summarize loaded dataset\nprint('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\nprint('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n# plot first few images\nfor i in range(9):\n\t# define subplot\n\tpyplot.subplot(330 + 1 + i)\n\t# plot raw pixel data\n\tpyplot.imshow(trainX[i])\n# show the figure\npyplot.show()","2ffcdc90":"import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","922069ba":"# Model \/ data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") \/ 255\nx_test = x_test.astype(\"float32\") \/ 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","eb67a242":"model = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()","d53647a0":"batch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)","905d3e3c":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","22fabfa0":"#Adam\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","78f2f20e":"#RMSprop\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"RMSprop\", metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","ba4f1ffc":"#Adagrad\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"Adagrad\", metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","fa5eca08":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# The dimensions of our input image\nimg_width = 180\nimg_height = 180\n# Our target layer: we will visualize the filters from this layer.\n# See `model.summary()` for list of layer names, if you want to change this.\nlayer_name = \"conv3_block4_out\"","0c748458":"# Build a ResNet50V2 model loaded with pre-trained ImageNet weights\nmodel = keras.applications.ResNet50V2(weights=\"imagenet\", include_top=False)\n\n# Set up a model that returns the activation values for our target layer\nlayer = model.get_layer(name=layer_name)\nfeature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)","6eead30d":"def compute_loss(input_image, filter_index):\n    activation = feature_extractor(input_image)\n    # We avoid border artifacts by only involving non-border pixels in the loss.\n    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n    return tf.reduce_mean(filter_activation)","5363322b":"@tf.function\ndef gradient_ascent_step(img, filter_index, learning_rate):\n    with tf.GradientTape() as tape:\n        tape.watch(img)\n        loss = compute_loss(img, filter_index)\n    # Compute gradients.\n    grads = tape.gradient(loss, img)\n    # Normalize gradients.\n    grads = tf.math.l2_normalize(grads)\n    img += learning_rate * grads\n    return loss, img","5421c77b":"def initialize_image():\n    # We start from a gray image with some random noise\n    img = tf.random.uniform((1, img_width, img_height, 3))\n    # ResNet50V2 expects inputs in the range [-1, +1].\n    # Here we scale our random inputs to [-0.125, +0.125]\n    return (img - 0.5) * 0.25\n\n\ndef visualize_filter(filter_index):\n    # We run gradient ascent for 20 steps\n    iterations = 30\n    learning_rate = 10.0\n    img = initialize_image()\n    for iteration in range(iterations):\n        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n\n    # Decode the resulting input image\n    img = deprocess_image(img[0].numpy())\n    return loss, img\n\n\ndef deprocess_image(img):\n    # Normalize array: center on 0., ensure variance is 0.15\n    img -= img.mean()\n    img \/= img.std() + 1e-5\n    img *= 0.15\n\n    # Center crop\n    img = img[25:-25, 25:-25, :]\n\n    # Clip to [0, 1]\n    img += 0.5\n    img = np.clip(img, 0, 1)\n\n    # Convert to RGB array\n    img *= 255\n    img = np.clip(img, 0, 255).astype(\"uint8\")\n    return img","6c781a3e":"#Let's try it out with filter 0 in the target layer:\n\nfrom IPython.display import Image, display\n\nloss, img = visualize_filter(0)\nkeras.preprocessing.image.save_img(\"0.png\", img)\n\n\"\"\"\nThis is what an input that maximizes the response of filter 0 in the target layer would\nlook like:\n\"\"\"\n\ndisplay(Image(\"0.png\"))","e7b9f848":"# Compute image inputs that maximize per-filter activations for the first 64 filters of our target layer\nall_imgs = []\nfor filter_index in range(64):\n    print(\"Processing filter %d\" % (filter_index,))\n    loss, img = visualize_filter(filter_index)\n    all_imgs.append(img)\n\n# Build a black picture with enough space for our 8 x 8 filters of size 128 x 128, with a 5px margin in between\nmargin = 5\nn = 8\ncropped_width = img_width - 25 * 2\ncropped_height = img_height - 25 * 2\nwidth = n * cropped_width + (n - 1) * margin\nheight = n * cropped_height + (n - 1) * margin\nstitched_filters = np.zeros((width, height, 3))\n\n# Fill the picture with our saved filters\nfor i in range(n):\n    for j in range(n):\n        img = all_imgs[i * n + j]\n        stitched_filters[\n            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n            (cropped_height + margin) * j : (cropped_height + margin) * j\n            + cropped_height,\n            :,\n        ] = img\nkeras.preprocessing.image.save_img(\"stiched_filters.png\", stitched_filters)\n\nfrom IPython.display import Image, display\n\ndisplay(Image(\"stiched_filters.png\"))","2b00345f":"## Simple MNIST ConvNet\n\n* Title: Simple MNIST convnet\n* Author: [fchollet](https:\/\/twitter.com\/fchollet)\n* Date created: 2015\/06\/19\n* Last modified: 2020\/04\/21\n* Description: A simple convnet that achieves ~99% test accuracy on MNIST.","17d6c247":"Image classification models see the world by decomposing their inputs over a \"vector basis\" of texture filters such as these.  \nSee also [this old blog post](https:\/\/blog.keras.io\/how-convolutional-neural-networks-see-the-world.html)\nfor analysis and interpretation.\n\"\"\"","bae59962":"### Build a feature extraction model","b24feb53":"### Setup","fbb005b2":"### Setup","1d1b86bf":"## Set up the end-to-end filter visualization loop\nOur process is as follows:\n- Start from a random image that is close to \"all gray\" (i.e. visually netural)\n- Repeatedly apply the gradient ascent step function defined above\n- Convert the resulting input image back to a displayable form, by normalizing it,\ncenter-cropping it, and restricting it to the [0, 255] range.","18f9dde9":"## What do ConvNets learn?\n\n* Title: Visualizing what convnets learn\n* Author: [fchollet](https:\/\/twitter.com\/fchollet)\n* Date created: 2020\/05\/29\n* Last modified: 2020\/05\/29\n* Description: Displaying the visual patterns that convnet filters respond to.","2b9ac9a1":"### Compare different optimizers\n\nDuring the previous tutorial, we discovered that the Adaptive Gradients Estimation (or Adam) optimizer shows good performance.  \n\nLet's compare the performance of our model using some more optimizers trained for 10 epochs.","f86c9cd3":"### Introduction\nIn this example, we look into what sort of visual patterns image classification models\nlearn. We'll be using the `ResNet50V2` model, trained on the ImageNet dataset.\nOur process is simple: we will create input images that maximize the activation of\nspecific filters in a target layer (picked somewhere in the middle of the model: layer\n`conv3_block4_out`). Such images represent a visualization of the\npattern that the filter responds to.","44b5b417":"### Prepare the data","2858f6bc":"### Set up the gradient ascent process\nThe \"loss\" we will maximize is simply the mean of the activation of a specific filter in\nour target layer. To avoid border effects, we exclude border pixels.\n","4cfdb6f2":"## Visualizing the datasets","e43fa27f":"### Evaluate the trained model","531285e9":"Our gradient ascent function simply computes the gradients of the loss above\nwith regard to the input image, and update the update image so as to move it\ntowards a state that will activate the target filter more strongly.","323d83be":"### Build and train the model","86688fca":"### Visualize the first 64 filters in the target layer\nNow, let's make a 8x8 grid of the first 64 filters\nin the target layer to get of feel for the range\nof different visual patterns that the model has learned."}}