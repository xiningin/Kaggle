{"cell_type":{"6f96d2e1":"code","ef12dbf8":"code","5fe8685c":"code","cac383c2":"code","997a2931":"code","68e79053":"code","ca1de511":"code","06d96273":"code","e015a93c":"code","04794d32":"code","7da8b752":"code","fb9287c5":"code","14eca1e0":"code","451f8dc7":"code","3e49d9df":"code","3f37ecb5":"code","14acdd7d":"code","cb7d70dc":"code","30a03fd6":"code","771bb054":"code","eaf18804":"code","49ba7371":"code","93e523d2":"markdown","1d5858a2":"markdown","16458af6":"markdown","78df318b":"markdown","3fc6d647":"markdown","258f3891":"markdown","fa75a135":"markdown","e8325a1a":"markdown","e8b01712":"markdown","390a0b53":"markdown"},"source":{"6f96d2e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef12dbf8":"df_train = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_test.csv\")","5fe8685c":"import matplotlib.pyplot as plt\nfrom skimage.feature import hog\nfrom skimage import data,exposure\nimport cv2\nfrom sklearn import svm\n%matplotlib inline\nnp.random.seed(1)","cac383c2":"train_x = df_train[df_train.columns[1::]].to_numpy()           \ntrain_y = df_train[df_train.columns[0]].to_numpy()\n\ntest_x = df_test[df_test.columns[1::]].to_numpy() \ntest_y = df_test[df_test.columns[0]].to_numpy()               \n\nprint(\"SUMMARY OF DATA:\")\n\nprint(\"train_x shape: \" + str(train_x.shape))\nprint(\"train_y shape: \" + str(train_y.shape))\nprint(\"test_x shape: \" + str(test_x.shape))\nprint(\"test_y shape: \" + str(test_y.shape))","997a2931":"train_x = train_x\/255\ntest_x = test_x\/255","68e79053":"index = 4\nplt.imshow(train_x[index].reshape(28, 28),cmap='gray')\nplt.show()","ca1de511":"from skimage.feature import hog\nfrom skimage import data,exposure\nimport cv2\ndef get_hog(image):\n    fd,hog_image=hog(image.reshape(28,28),pixels_per_cell=(2,2),\n                        cells_per_block=(1, 1),visualize=True,feature_vector=False)\n    return(fd,hog_image)\n\ndef show_hog(image,hog_image):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n    ax1.axis('off')\n    ax1.imshow(image.reshape(28, 28), cmap=plt.cm.gray)\n    ax1.set_title('Input image')\n    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n\n    ax2.axis('off')\n    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n    ax2.set_title('Histogram of Oriented Gradients')\n    plt.show()","06d96273":"df_htrain = pd.read_csv(\"..\/input\/sl-hog-features\/Hog_Features.csv\")\ndf_htest = pd.read_csv(\"..\/input\/sl-hog-features\/Hog_Features_test.csv\")\nhtrain = df_htrain[df_htrain.columns[1::]].to_numpy()\nhtest = df_htest[df_htest.columns[1::]].to_numpy()\nprint(\"Training Data size : \",htrain.shape)\nprint(\"Test Data size : \",htest.shape)","e015a93c":"a,b=get_hog(train_x[17])\nshow_hog(train_x[17],b)\na=a.reshape(-1)\na=a.reshape(1,len(a))\nprint(a.shape)","04794d32":"from sklearn.decomposition import PCA\npca = PCA(n_components=30)\nhtrain_pca = pca.fit_transform(htrain)","7da8b752":"htest_pca=pca.transform(htest)\nprint(htest_pca.shape)","fb9287c5":"htrain_pca.shape","14eca1e0":"clf = svm.SVC(kernel='linear') # rbf Kernel\n\n#Train the model using the training sets\nclf.fit(htrain_pca,train_y)\n\n#Predict the response for test dataset\ny_pred = clf.predict(htest_pca)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(test_y, y_pred, average='weighted'))\nprint(\"Precision: \",metrics.precision_score(test_y,y_pred, average='weighted'))\nprint(\"Recall: \",metrics.recall_score(test_y,y_pred,average = 'weighted'))","451f8dc7":"clf = svm.SVC(kernel='poly') # rbf Kernel\n\n#Train the model using the training sets\nclf.fit(htrain_pca,train_y)\n\n#Predict the response for test dataset\ny_pred = clf.predict(htest_pca)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(test_y, y_pred, average='weighted'))\nprint(\"Precision: \",metrics.precision_score(test_y,y_pred, average='weighted'))\nprint(\"Recall: \",metrics.recall_score(test_y,y_pred,average = 'weighted'))","3e49d9df":"clf = svm.SVC(kernel='sigmoid') # rbf Kernel\n\n#Train the model using the training sets\nclf.fit(htrain_pca,train_y)\n\n#Predict the response for test dataset\ny_pred = clf.predict(htest_pca)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(test_y, y_pred, average='weighted'))\nprint(\"Precision: \",metrics.precision_score(test_y,y_pred, average='weighted'))\nprint(\"Recall: \",metrics.recall_score(test_y,y_pred,average = 'weighted'))","3f37ecb5":"clf = svm.SVC(kernel='rbf') # rbf Kernel\n\n#Train the model using the training sets\nclf.fit(htrain_pca,train_y)\n\n#Predict the response for test dataset\ny_pred = clf.predict(htest_pca)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(test_y, y_pred, average='weighted'))\nprint(\"Precision: \",metrics.precision_score(test_y,y_pred, average='weighted'))\nprint(\"Recall: \",metrics.recall_score(test_y,y_pred,average = 'weighted'))","14acdd7d":"y_pred = clf.predict(htrain_pca)\nprint(\"Training Set Metrics : \")\nprint(\"Accuracy:\",metrics.accuracy_score(train_y, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(train_y, y_pred, average='weighted'))\nprint(\"Precision: \",metrics.precision_score(train_y,y_pred, average='weighted'))\nprint(\"Recall: \",metrics.recall_score(train_y,y_pred,average = 'weighted'))","cb7d70dc":"x=[]\ny=[]\ny2=[]\nfor i in range(1,10):\n    clf = svm.SVC(kernel='rbf',C=i)\n    clf.fit(htrain_pca,train_y)\n    y_pred = clf.predict(htest_pca)\n    x.append(i)\n    y.append(metrics.f1_score(test_y, y_pred, average='weighted'))\n    y2_pred = clf.predict(htrain_pca)\n    y2.append(metrics.f1_score(train_y, y2_pred, average='weighted'))\nplt.subplot(1,2,1)\nplt.plot(x,y2)\nplt.title(\"F1 Score as a funtion of C(Regularization Parameter)) for training data\")\nplt.xlabel(\"C\")\nplt.ylabel(\"F1 Score\")\n\nplt.subplot(1,2,2)\nplt.plot(x,y)\nplt.title(\"F1 Score as a funtion of C(Regularization Parameter)) for test data\")\nplt.xlabel(\"C\")\nplt.ylabel(\"F1 Score\")\nplt.show()","30a03fd6":"plt.plot(x,y2)\nplt.title(\"F1 Score as a funtion of C(Regularization Parameter)) for training data\")\nplt.xlabel(\"C\")\nplt.ylabel(\"F1 Score\")\nplt.show()\n\nplt.plot(x,y)\nplt.title(\"F1 Score as a funtion of C(Regularization Parameter)) for test data\")\nplt.xlabel(\"C\")\nplt.ylabel(\"F1 Score\")\nplt.show()","771bb054":"from sklearn.neighbors import KNeighborsClassifier","eaf18804":"x=[]\ny=[]\nfor i in range(2,50,5):\n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(htrain_pca,train_y)\n    y_pred = neigh.predict(htest_pca)\n    x.append(i)\n    y.append(metrics.f1_score(test_y, y_pred, average='weighted'))\nplt.plot(x,y)\nplt.title(\"F1 Score as a funtion of K(No of Neighbors)\")\nplt.xlabel(\"K\")\nplt.ylabel(\"F1 Score\")\nplt.show()","49ba7371":"m=max(y)\nindex = y.index(m)\nprint(\"Max value of F1 Score occurs at K = \",x[index])\nprint(\"The Evaluation metrics are : \")\nneigh = KNeighborsClassifier(n_neighbors=x[index])\nneigh.fit(htrain_pca,train_y)\ny_pred = neigh.predict(htest_pca)\nprint(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(test_y, y_pred, average='weighted'))\nprint(\"Precision: \",metrics.precision_score(test_y,y_pred, average='weighted'))\nprint(\"Recall: \",metrics.recall_score(test_y,y_pred,average = 'weighted'))","93e523d2":"# **Sign Language Letter Detection**:\nIn this work, we propose a method to detect American Sign Language by analyzing sign language MNIST image dataset. We will use the pixel level features in each image as well as HOG Features, to be followed with feature selection to improve the performance. Finally we will print different evaluation metrics for the classifier model such as Accuracy, F1 Score, Precision, Recall and plot ROC curves for all the classes.","1d5858a2":"The summary of the dataset is printed below:","16458af6":"A sample training image is shown below.","78df318b":"# KNN Classifier","3fc6d647":"* Sklearn has an inbuilt module which transforms the given data set into the required number of components.\n* We are resolving the HOG feature vector into 30 components from an initial feature vector length of 1764 values.\n* Classification is done using an SVM classifier using rbf kernel","258f3891":"1. Each image has a size of 28x28, which when flattened gives 784 pixel values.\n2. The training set has 27455 examples.\n3. The test set has 7172 examples for evaluation.","fa75a135":"# Normalizing Pixel Values\n* The pixel values will each be in the range(0-255)\n* Normalizing each pixel value to be in the range (0,1)","e8325a1a":"# **Feature Extraction Using HOG Features**\n* The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image.","e8b01712":"# Getting the HOG features which were previously generated on the Train and test data","390a0b53":"# Feature Selection Using PCA "}}