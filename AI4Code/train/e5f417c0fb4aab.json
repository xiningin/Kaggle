{"cell_type":{"1f9b668c":"code","315a841b":"code","3fe5142c":"code","be24f24b":"code","3779704b":"code","4346f994":"code","c5bc158e":"code","6c78cc9f":"markdown","8162ea5e":"markdown","6ef85a84":"markdown"},"source":{"1f9b668c":"# import standard libraries\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection, metrics\nimport seaborn as sns\n\n# show all columns\npd.set_option('max_columns', None)","315a841b":"# read the data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n\n# drop id columns from train and test sets\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","3fe5142c":"# histogramse for all variables with KDE\nplt.figure(figsize=(24, 6*(104\/4)))\nfor i in range(len(train.columns.tolist())):\n    plt.subplot(26, 4, i+1)\n    if i <= 99:\n        sns.histplot(train[f'f{i}'], kde=True)\n    else:\n        sns.histplot(train['loss'], kde=True)\nplt.show()","be24f24b":"# correlation matrix with heat map\ncorr = train.corr()\nplt.figure(figsize=(20, 20))\nsns.heatmap(corr)\nplt.show()","3779704b":"# the ten highest correlated features for each feature\ncols = train.columns.tolist()\nfor col in cols:\n    print(col)\n    print(corr[col].sort_values(ascending=False)[1:11])\n    print('=======================')","4346f994":"# extract X and y for training set\nX = train.drop('loss', axis=1).values\ny = train['loss'].values","c5bc158e":"# optimized hyperparameters\nparams = {\n        \"min_child_weight\": 638.7295413674256,\n        \"num_leaves\": 32,\n        \"reg_alpha\": 0.7635991288488166,\n        \"reg_lambda\": 93.08626337603258\n        }\n\n# construct the model\nmodel= lgbm.LGBMRegressor(\n                       **params,\n                       objective='rmse',\n                       metric='rmse',\n                       subsample=0.7,\n                       learning_rate=0.03,\n                       n_estimators=10000,\n                       n_jobs=-1\n                       )\n\n# construct KFold cross validation\nn_splits=5\nkf = model_selection.KFold(n_splits=n_splits)\n\n# initiate lists to save folds scores\nscores_train = []\nscores_valid = []\n\n# initiate zeros array for test data predictions\npreds_test_array = np.zeros((test.shape[0], ))\n\n# KFold cross validation \nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n\n    print(f\"Fold {fold+1} -------------->\")\n    x_train, y_train = X[train_idx], y[train_idx]\n    x_valid, y_valid = X[valid_idx], y[valid_idx]\n\n    y_train_log = y_train\n    y_valid_log = y_valid\n    \n    # fit the model\n    model.fit(\n            x_train, y_train_log,\n            eval_set=[(x_valid,y_valid_log)],\n            verbose=100,\n            early_stopping_rounds=100\n            )\n\n    # clip the results so that the minimum and maximum values are 0 and 50, respectively\n    preds_train = np.clip(model.predict(x_train), 0, 50)\n    preds_valid = np.clip(model.predict(x_valid), 0, 50)\n    preds_test = np.clip(model.predict(test), 0, 50)\n    \n    # add the predictions of each fold to the array\n    preds_test_array += preds_test \/ n_splits\n    \n    # find both train and test rsme and observe if there is overfitting\n    score_train = np.sqrt(metrics.mean_squared_error(y_train, preds_train))\n    score_valid = np.sqrt(metrics.mean_squared_error(y_valid, preds_valid))\n    \n    # print the fold score\n    print(score_valid)\n    \n    # append the fold score\n    scores_train.append(score_train)\n    scores_valid.append(score_valid)\n\nprint('Mean train score =', np.mean(scores_train), 'STD train =', np.std(scores_train, ddof=1))\nprint('Mean valid score =', np.mean(scores_valid), 'STD valid =', np.std(scores_valid, ddof=1))\n\n# populate the submission dataframe\nsample.iloc[:, 1] = preds_test_array\nsample.to_csv('lgbm_base_model_submission.csv', index=False)","6c78cc9f":"Train and validation RMSE's are so close, hence the odds of overfitting is small.","8162ea5e":"# Simple EDA","6ef85a84":"# LightGBM with Optimized Hyperparameters"}}