{"cell_type":{"4722e551":"code","38cd8955":"code","356765b6":"code","e8c9d86f":"code","d05834c1":"code","28c30f03":"code","3bc40c95":"code","b12c1d3f":"code","f396d71e":"code","9dad024b":"code","1b7ad4d0":"code","ff4ab297":"code","e345ba13":"code","d0ccfa47":"code","435943d5":"code","b738e659":"code","eb7e9e75":"code","a2452563":"code","fda05cb6":"code","301d1838":"code","8f2fe5b0":"code","6f15601a":"code","545fbd95":"code","43fb74e1":"code","f757eabc":"code","e11f973a":"code","33f86f1f":"code","7ce351c2":"code","404f6180":"code","7bdce9bb":"code","2c1e3518":"code","a86bfa8e":"code","e0c3ea7b":"code","8d705244":"code","6b8d2cae":"code","888cf82a":"code","d435389e":"code","2597cdab":"code","71baf861":"code","6016478f":"code","87afbb72":"code","606acbf1":"code","664795d5":"markdown","84427989":"markdown","a7ea8183":"markdown","f6d432f2":"markdown","ec304cef":"markdown","77239377":"markdown","3df036e9":"markdown","cbc2d13e":"markdown","eaea1784":"markdown","4012dcfe":"markdown","c5531408":"markdown","2373defd":"markdown","e8b3e937":"markdown","125b73de":"markdown","f820a8d1":"markdown"},"source":{"4722e551":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport gc\nimport psutil\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, Normalizer,MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom optuna.integration import LightGBMPruningCallback\n\n# get skewed features to impute median instead of mean\nfrom scipy.stats import skew\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor, XGBRFRegressor\n\nimport itertools\nimport optuna\nfrom lightgbm import LGBMClassifier,LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","38cd8955":"train_data = pd.read_csv('..\/input\/titanic-create-folds\/TITANIC_Folds.csv')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","356765b6":"train_data.head(2)","e8c9d86f":"test_data.head(2)","d05834c1":"train_data.shape,test_data.shape","28c30f03":"train_data.info()","3bc40c95":"test_data.info()","b12c1d3f":"train_data.isna().sum()","f396d71e":"test_data.isna().sum()","9dad024b":"train_data.drop('Cabin',axis= 1,inplace= True)\ntest_data.drop('Cabin',axis= 1,inplace= True)","1b7ad4d0":"train_data.isna().sum()","ff4ab297":"test_data.isna().sum()","e345ba13":"train_data['Age'].median(),test_data['Fare'].median(),train_data['Embarked'].mode()[0]","d0ccfa47":"# 'Age' and 'Embarked' columns of train_data has misssing values\ntrain_data['Age'] = train_data['Age'].fillna(int(train_data['Age'].mean()))\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\n# 'Age' and 'Fare' columns of test_data has missing value\ntest_data['Age'] = test_data['Age'].fillna(int(test_data['Age'].mean()))\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())","435943d5":"train_data.isna().sum()","b738e659":"test_data.isna().sum()","eb7e9e75":"list(zip(train_data.columns, train_data.dtypes, train_data.nunique()))","a2452563":"list(zip(test_data.columns, test_data.dtypes, test_data.nunique()))","fda05cb6":"train_data['isTrain'] = 1\ntest_data['isTrain'] = 0","301d1838":"tt = pd.concat([train_data,test_data])\ntt.head(2)","8f2fe5b0":"train_data.shape, test_data.shape, tt.shape","6f15601a":"list(zip(tt.columns, tt.dtypes, tt.nunique())) ","545fbd95":"tt.drop(['Name','Ticket'],axis=1,inplace = True)","43fb74e1":"list(zip(tt.columns, tt.dtypes, tt.nunique())) #","f757eabc":"tt.shape","e11f973a":"tt = pd.get_dummies(tt,columns= ['Sex','Embarked'],drop_first = True)","33f86f1f":"tt.shape","7ce351c2":"train_data = tt[tt.isTrain == 1]\ntest_data = tt[tt.isTrain == 0]","404f6180":"train_data.head(2)","7bdce9bb":"test_data.head(2)","2c1e3518":"train_data.info()","a86bfa8e":"test_data.info()","e0c3ea7b":"test_data.drop(['Survived','isTrain','fold'],axis=1,inplace = True)\ntrain_data.drop(['isTrain'],axis=1,inplace= True)","8d705244":"test_data.info()","6b8d2cae":"train_data.info()","888cf82a":"useful_features = test_data.drop('PassengerId',axis=1).columns.tolist()\nuseful_features  # these are the features on which our model will be trained","d435389e":"train_data[useful_features].shape, train_data.shape, test_data.shape, test_data[useful_features].shape","2597cdab":"test = test_data[useful_features]","71baf861":"my_folds = train_data.copy()","6016478f":"train_data = pd.read_csv('..\/input\/titanic-create-folds\/TITANIC_Folds.csv')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","87afbb72":"train_data.drop('Cabin',axis= 1,inplace= True)\ntest_data.drop('Cabin',axis= 1,inplace= True)\ntrain_data['Age'] = train_data['Age'].fillna(int(train_data['Age'].mean()))\ntest_data['Age'] = test_data['Age'].fillna(int(test_data['Age'].mean()))\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\ntrain_data['isTrain'] = 1\ntest_data['isTrain'] = 0\ntt = pd.concat([train_data,test_data])\ntt.drop(['Name','Ticket'],axis=1,inplace = True)\ntt = pd.get_dummies(tt,columns= ['Sex','Embarked'],drop_first = True)\ntrain_data = tt[tt.isTrain == 1]\ntest_data = tt[tt.isTrain == 0]\ntest_data.drop(['Survived','isTrain','fold'],axis=1,inplace = True)\ntrain_data.drop(['isTrain'],axis=1,inplace= True)\n\n# Note don't drop PassengerId column of train_data and test_data as it is used later. Instead drop it only for test and my_folds\nuseful_features = test_data.drop('PassengerId',axis=1).columns.tolist() #########################################\ntest = test_data[useful_features]\nmy_folds = train_data.copy()","606acbf1":"test.shape, my_folds.shape, useful_features","664795d5":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will not aim at converting object type columns to integer or float type because ML model can only interpret numerical values.\n<br> object type generally contains string so we can either convert them into classes by <i>one hot encoding<\/i> or we can do <i>ordinal encoding<\/i>\n<br>For example if we have a column of T-Shirt sizes whoose values are 'S','M','L','XL','XXL', here there is a sense of order like 'S' is smaller than 'M' so here we can do <i>ordinal encoding<\/i>\n<br>While if the column is 'gender' which contains 'Male' and 'Female' then there is no sence of order, so here we can do one hot encoding.<\/p> \n","84427989":"<a id=\"4\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">CONCLUSION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This was the PREPROCESSING for Level1 Round1. Means we are going to use this PREPROCESSING throughout the Round1 of Level1. So we will now find optimal hyperparameters of all models, train our models and make predictions, all on this PREPROCESSING. \n<br> That is all for now, If you have any doubt feel free to ask me in the comment. If you like my effort please do <b>UPVOTE<\/b>, it really keeps me motivated. <\/p>\n\n**<span style=\"color:#444160;\"> Thanks!<\/span>**\n<a id=\"5\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">END<\/p>\n    <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","a7ea8183":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will now use pd.get_dummies() method of pandas to create onehot encoding of all the desired categorical columns here 'Sex' and 'Embarked'.\n<br>Notice: here we did drop_first= True to remove redundant columns.\nFor examample 'Sex' column has two categories 'Male' and 'Female' so two new columns are created, but do we really need two column to tell if it is 'Male' or 'Female'. Actually no one column is enough because if 'Male' we will label it as 1 and if 'Female' we will label it as 0. Second column is just the repetition of same information. Hence we drop first. So for 5 categories column is replaced by 4 new columns instead of 5 new column when we do drop_first = True<\/p> ","f6d432f2":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will focus on columns with Object data type. There are total 4 columns :- 'Name', 'Sex', 'Ticket', 'Embarked'. 'Name' and 'Ticket' columns have many different types of string [total 1307]  which makes sense as People have different names. If we just do one hot encoding of it then our dataset will become very large and very sprse. There are other ways of dealing with it and probably we can extract features from there names like sir-name, last name but for now we will simply drop them.<\/p> ","ec304cef":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">PREPROCESSING<\/p>\n\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Here we will go one step at a time expalining everything. From next time we will use single cell to do all the PREPROCESSING<\/p> ","77239377":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Now for 'Age' and 'Fare' column we fill with median of all the age and 'Fare' values respectively, for 'Embarked' we fill with mode of all the 'Embarked' value since it is a string so we take mode.<br> <b>[Note:- to avoid data leakage we do all filling of missing values before concating train and test set for further preprocessing like one hot encoding.]<\/b><\/p> ","3df036e9":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Let's first see if there is any missing value<\/p> ","cbc2d13e":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We have PREPROCESSED our data now we will remove unnecessary columns.<\/p> ","eaea1784":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We now split it back into train and test set.<\/p> ","4012dcfe":"\n<a id=\"0\"><\/a>\n# <p style=\"background-color:#FFCC70;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:5px 5px;\">LEVEL1 ROUND1 PREPROCESSING<br><p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">INTRODUCTION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This is a part of the notebook series <i>\"My_Complete_Pipeline_for_any_ML_Competition\"<\/i> where we are building complete pipeline.<br><br> \n\ud83d\udcccLink of first notebook of the series <a href=\"https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition\">https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition<\/a><br>\n\ud83d\udcccLink of notebook where we have created folds <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-create-folds\">https:\/\/www.kaggle.com\/raj401\/titanic-create-folds<\/a><br>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\n    If you like my effort please do <b><span style=\"color:crimson; font-size:20px\">UPVOTE\ud83d\udc4d<\/span><\/b>, it really keeps me motivated. <\/p>\n\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">In the earlier notebook we have modified our training set by adding new column named 'fold' and then saved it as <i>TITANIC_folds.csv<\/i>. In this notebook we will use this modified training set instead of original training set and do PREPROCESSING for LEVEL1 ROUND1. I am providing the link of <i>TITANIC_folds.csv<\/i> you can just add it to your notebook and you are good to go.<b>\n<br>[Make sure you have added it before moving further. If you have TITANIC_Create_Folds notebook you can also add that notebook instead.]<br><\/b>\n\ud83d\udcccLink of Dataset containing <i>TITANIC_folds.csv<\/i> <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets\">https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets<\/a><br><\/p> \n\n\n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. READ DATASETS](#2)\n\n* [3. PREPROCESSING](#3)\n   * [3.1 WITH EXPLANATION](#3)\n   * [3.2 QUICK](#3.2)\n    \n    \n* [4. CONCLUSION](#4)\n    \n* [5. END](#5)\n\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">IMPORTING LIBRARIES<\/p>","c5531408":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Since no of missing value in 'Cabin' column is quite large compared to total no of rows so it is better to drop 'Cabin', Later we may try to do some imputation based on other columns but for now we will simply drop it.<\/p> ","2373defd":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will first concat our train and test set before dealing with categorical featues. The reason is Let say in training set 'col1' has only two types of value 'A' and 'B' while test set has 3 types of categories 'A' , 'C', 'D' in 'col1'. If we separatley create one hot encoding they will give wrong results because they are incompatible. So when we concat them first and then do one hot encoding then both assumes there are 4 types of categories 'A', 'B', 'C', 'D'.\n<br><b>Note:- We can think it as a general rule, \n<br>Fill missing value for train and test set separately. While if it has categorial column do one hot encoding by first concatenating train and test set.<\/b>\n Now after doing one hot encoding we need to split it back in train-test so for this we create a new column 'isTrain' before concatenating and label training rows as 1 and test rows as 0<\/p> ","e8b3e937":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Let us first use .info() method as it gives us a lot of information about our dataset.<\/p> ","125b73de":"<a id=\"3.2\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Note we did data cleaning and feature engineering step by step. From next time we  will do all the above steps in a single cell.\n<br>Like this:-<\/p> ","f820a8d1":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">READ DATASETS<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\nEither you can add the dataset whoose link I have given above or if you have TITANIC_Create_Folds notebook you can add it's output from Add data option. Both contains TITANIC_Folds.csv (modified train set).\nNow read it as train_data.\n<\/p>"}}