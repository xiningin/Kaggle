{"cell_type":{"993e4524":"code","522787e7":"code","eb0d5e24":"code","ccde91dd":"code","ba171c26":"code","7b3705f6":"code","f678b304":"code","a5728f2e":"code","4ab38a93":"code","e4c7e72b":"code","6646a525":"code","c4fea403":"code","09a99d2a":"code","665fca98":"markdown","9e3e8b7c":"markdown","03e155f7":"markdown","4aa5c2a2":"markdown","85eccc8a":"markdown","70640f2c":"markdown","207133ad":"markdown","7e0be75b":"markdown","1673df20":"markdown","a988f3e4":"markdown","76b3e887":"markdown","27e9401c":"markdown","80705c18":"markdown","dfb454b2":"markdown"},"source":{"993e4524":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","522787e7":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('..\/input\/Churn_Modelling.csv')\ndataset.describe()\ndataset.head(10)","eb0d5e24":"#The first model, which include all the features\nX1 = dataset.iloc[:,3:13].values\ny  = dataset.iloc[:,13].values\n#The subdataset for the second classifier, which has high salaries\nX2 = dataset[['CreditScore','Age','EstimatedSalary','IsActiveMember']].values\n#The third subdataset for the third classifier, which are lazy to change or close the bank accounts\nX3 = dataset[['CreditScore','Age','Balance','IsActiveMember']].values","ccde91dd":"# Encoding categorical data,\n# Here only Country and Gender feature need to be encode\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX1[:, 1] = labelencoder_X_1.fit_transform(X1[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX1[:, 2] = labelencoder_X_2.fit_transform(X1[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX1 = onehotencoder.fit_transform(X1).toarray()\nX1 = X1[:, 1:] #Avoid dummy variable traps","ba171c26":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size = 0.2, random_state = 0)\nX2_train, X2_test = train_test_split(X2, test_size = 0.2, random_state = 0)\nX3_train, X3_test = train_test_split(X3, test_size = 0.2, random_state = 0)","7b3705f6":"# Feature Scaling\n# As we can see from the data, there are difference in scale of data in each catagory in the set.\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX1_train = sc.fit_transform(X1_train)\nX1_test = sc.transform(X1_test)\nX2_train = sc.fit_transform(X2_train)\nX2_test = sc.transform(X2_test)\nX3_train = sc.fit_transform(X3_train)\nX3_test = sc.transform(X3_test)","f678b304":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense","a5728f2e":"#Our first classifier\ndef build_classifier1():\n    classifier1 = Sequential()\n    classifier1.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n    classifier1.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier1.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier1","4ab38a93":"def build_classifier2():\n    classifier2 = Sequential()\n    classifier2.add(Dense(units = 2, kernel_initializer = 'uniform', activation = 'relu', input_dim = 4))\n    classifier2.add(Dense(units = 2, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier2.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier2","e4c7e72b":"#Now we going to see the results of the first model!\nclassifier1 = KerasClassifier(build_fn = build_classifier1, batch_size = 10, epochs = 100)\naccuracies1 = cross_val_score(estimator = classifier1, X = X1_train, y = y_train, cv = 10, n_jobs = -1)\nmean1 = accuracies1.mean()\nvariance1 = accuracies1.std()","6646a525":"#Next we going to see the results of the second model!\nclassifier2 = KerasClassifier(build_fn = build_classifier2, batch_size = 10, epochs = 100)\naccuracies2 = cross_val_score(estimator = classifier2, X = X2_train, y = y_train, cv = 10, n_jobs = -1)\nmean2 = accuracies2.mean()\nvariance2 = accuracies2.std()\n","c4fea403":"#Last we going to test the results of the third model!\nclassifier3 = KerasClassifier(build_fn = build_classifier2, batch_size = 10, epochs = 100)\naccuracies3 = cross_val_score(estimator = classifier2, X = X3_train, y = y_train, cv = 10, n_jobs = -1)\nmean3 = accuracies3.mean()\nvariance3 = accuracies3.std()","09a99d2a":"print('The accuracy of our first model is:  ',mean1)\nprint('The vairance of accuracy is:         ',variance1)\nprint('The accuracy of our second model is: ',mean2)\nprint('The vairance of accuracy is:         ',variance2)\nprint('The accuracy of our third model is:  ',mean3)\nprint('The vairance of accuracy is:         ',variance3)","665fca98":"We can improve the model by testing other **batch-size**, **epoch**, or **optimizer** by using **GridSearchCV**, but it is going to be quite suffuring since the time running could be hours!<br\/>","9e3e8b7c":"As can we seen, the dataset shows some bank customer's information such as **CreditScore**, **Geography**,** Balance**, *and so on*.\nWe could use all of the information to create a classifier to predict the exiting of customers in next period. However,from my respective, with more input information, we can get better insights, but it does not mean we can have the better results. Sometimes, with so many inputs, some key factors will be faded, and noise could be created.\n\nIn my opinion, one of the main key features is the **Age**. Normally, when people get older, they will turn into stability, and do not prefer to change much. As the youths,there are many interesting programs from the banks to actract the new generation clients. The second factor is the **CreditScore**. Credit Score could tell a lot about the customers. With higher score, the customers will get more benefits from the banks, and also the banks want to keep them, then the possibility they will stay long with bank is higher. \n\nBesides, there could be 2 kinds of customer who will stay long at the bank. The first one is those who have high **EstimatedSalary**. For this group, \nthose clients are usually seeking for the bank to trust to put those money. As long as no problems occurs, they could stay faithfully long there. For this model,I will choose **CreditScore**, **Age**, **EstimatedSalary**, and **IsActiveMember** to build the classifier\n\nThe second group of customers, are those who are lazy to change or close the bank accounts. They have very high possibility of being inactive,\nsince they dont care much about the bank credit. they are also could low balance. Therefore, the third model I want to build will include \n**CreditScore**, **Age**, **Balance**, and **IsActiveMember**.\n\nOther factors I did not mention has reasons. Firsly, most of the countries we can see is europian countries, therefore there would be not significantly different here. Second, gender nowadays does not play much important role in comparation between male and female. Tenure, Number of products, and credit card, are not important in my classifiers, thus both of them could have a long time in the bank, have few banks accounts and credits or using cash only.\n\n**Now, Let's go to verify the prediction!!!**","03e155f7":"**To sum up:** <br\/>\nI am quite satified with the results. First, I could successful build the classifier to tell whether the clients still stay or not. Second,\nI tested my predicts, that I just focused on some main keys to build the model.\nHowever, in the code, there are some places such as writing the function classifier, I was not able to optimize and simplify it better.\n","4aa5c2a2":"Here I would build 2 types of classifier for our 3 models. The first classifier is for our first model, where we are going to take accounts in all features. While the second one we only care about 4 input factors. The number of hidden units I usually choose is **half** of the input variables.","85eccc8a":"As can we see from a result, for the first model, we can improve the accuracy up to 85.18% by using **epoch** = 200, and the **optimizer** for compiling is **rmsprop**.","70640f2c":"'''<br\/>\nparameters = {'batch_size': [10, 20],\n              'epochs': [100, 200],\n              'optimizer': ['adam', 'rmsprop']}\n\ngrid_search = GridSearchCV(estimator = classifier1,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)<br\/>\ngrid_search2 = GridSearchCV(estimator = classifier2,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\n\n#Now we going to seethe results of the first model!\ngrid_search1 = grid_search.fit(X1_train, y_train)<br\/>\nbest_parameters1 = grid_search1.best_params_<br\/>\nbest_accuracy1 = grid_search1.best_score_<br\/>\n#Now we going to seethe results of the sencond model!<br\/>\ngrid_search2 = grid_search2.fit(X2_train, y_train)<br\/>\nbest_parameters2 = grid_search2.best_params_<br\/>\nbest_accuracy2 = grid_search2.best_score_<br\/>\n#Now we going to seethe results of the last model!<br\/>\ngrid_search3 = grid_search2.fit(X3_train, y_train)<br\/>\nbest_parameters3 = grid_search2.best_params_<br\/>\nbest_accuracy3 = grid_search2.best_score_<br\/>\n'''","207133ad":"In the function above, I have created 2 hidden layers with 6 nodes, and the activation function I used is rectified linear unit (**relu**). However, for the output layer, since the result we expect is that the clients will stay or not, thus **sigmoid** is the better choice.<br\/>\nOn the other hand, for the compile method, since we used **sigmoid** as the output activation, the loss method which will fit to this choice would be **binary_crossentropy**.<br\/>\nSimilarily, we create the second classifier for our 2 models.","7e0be75b":"<h5> **Build ANN** <\/h5>\nNow, we have finished preprocessed our data. We are going to build our classifiers! <br\/>\nAs we know that, for each model, different datasets, there will be various parameters would fit, and give the best results. it would not be wise to apply once method to all. <br\/>\nTherefore, to help us verify this idea, I will use GridSearchCV to test some parameters, such as batch-size, epoch, and optimizer method.<br\/>\n**However**, due to the limit time running in the Kaggle, I was not able to test GridSearchCV here, I will provide my own result running on my laptop below.\n<br\/>Thus, I will used **K_fold Cross Validation** instead, to see the accuracies between our models.","1673df20":"**Here is the most waiting part of all!  Let's see the accuracies...**","a988f3e4":"<h4> **Data preprocessing**<\/h4>","76b3e887":"**Conclusion:**<br\/>\nAs we can see from the results,  The first model, which involve all of the features has won the battle! They have the highest accuracy compared to two others. However, the difference between accuracies between them is not so significant, from 3 to 5%. On the other, the variance of these models are also similar, but it is not a big duel.<br\/>\n\n\n\n\n","27e9401c":"**Things done!** Now we are going to test our classifiers for our models. **Let's go!!**","80705c18":"The dataset presents the information of bank customers.<br\/>\nThe output is to build a classifier which can determine whether they will leave or not?<br\/>\nFirst of all, let's take some look inside the dataset to gain the some first insights.","dfb454b2":"![GridSearchCV.png](attachment:GridSearchCV.png)"}}