{"cell_type":{"304b13ec":"code","7c2a5620":"code","a09d87b4":"code","059b5cdc":"code","df7389f6":"code","4c30550c":"code","bacbfcb7":"code","5e685450":"code","4367a5b4":"code","516a30fd":"code","21f0ef59":"code","6f2f12cb":"code","ba2f5404":"code","c1310753":"code","6c8efef6":"code","1eee2d47":"code","e194244e":"code","86a456fb":"code","ffe13e8b":"code","1f76d385":"code","6f2c7294":"code","7c92d238":"code","e63d8133":"code","81db1e6a":"code","dece5dc5":"code","09794fdf":"code","89864266":"code","6d2ded75":"code","6de73e45":"code","383ddeb2":"code","bfe51020":"code","b69e33bb":"code","03d89386":"code","ab21b31c":"code","fd921ad0":"code","f308ffb4":"code","a09a94a2":"code","9065860a":"code","5225954b":"code","9f06fa2e":"code","6490df14":"code","25c8e49e":"code","0a70e26f":"code","6931cf33":"code","06b06e04":"code","9f919145":"code","cb1b95fc":"code","2b2193da":"code","9a7d53a8":"code","7b531cfd":"code","5680474b":"code","d7149d96":"code","b80e1924":"code","b54a10a2":"code","7677a7de":"code","a013faa5":"code","51b99702":"code","afa0f2db":"code","b018faaa":"code","12408e2b":"code","618caa8e":"code","0cd20c9e":"markdown","ea40c7f3":"markdown","5a969c2d":"markdown","d827f68b":"markdown","187a0579":"markdown","465739bb":"markdown","9e441633":"markdown","53f2b679":"markdown","baaa819e":"markdown","38058595":"markdown","52ca0259":"markdown","f557bf38":"markdown","8cab0163":"markdown","215a84e0":"markdown","56ba342f":"markdown","343e1881":"markdown","64002525":"markdown","b850e68f":"markdown","362132a4":"markdown","2bc17e50":"markdown","914977d3":"markdown","4d9d4b2b":"markdown","d171029a":"markdown","73c2a409":"markdown","3cd667aa":"markdown","4de1a280":"markdown","9be1db6c":"markdown","d3e7ff28":"markdown","705265c0":"markdown","3105547a":"markdown","0bd8eca7":"markdown","fc6e101e":"markdown","0bc2bf6b":"markdown","d7f32bed":"markdown","9e837d15":"markdown","41e52c57":"markdown","f0408d73":"markdown","8fbe133b":"markdown"},"source":{"304b13ec":"#!pip install h5py==2.9.0 numpy==1.17.0 tqdm==4.19.5 matplotlib==2.2.3 seaborn==0.9.0 scipy==1.4.1 scikit-learn==0.19.2 tensorflow-gpu==1.15.4 Keras==2.2.4 Keras-Applications==1.0.8 Keras-Preprocessing==1.1.0 --force\n","7c2a5620":"import pandas as pd\nimport numpy as np\n#import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","a09d87b4":"\"\"\"\n    Models used in experiments\n\"\"\"\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D, concatenate, BatchNormalization, Activation, add\nfrom tensorflow.keras.models import Model, model_from_json\n#from keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\n\n\ndef UNetDS64(length, n_channel=1):\n    \"\"\"\n        Deeply supervised U-Net with kernels multiples of 64\n    \n    Arguments:\n        length {int} -- length of the input signal\n    \n    Keyword Arguments:\n        n_channel {int} -- number of channels in the output (default: {1})\n    \n    Returns:\n        keras.model -- created model\n    \"\"\"\n    \n    x = 64\n\n    inputs = Input((length, n_channel))\n    conv1 = Conv1D(x,3, activation='relu', padding='same')(inputs)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Conv1D(x,3, activation='relu', padding='same')(conv1)\n    conv1 = BatchNormalization()(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(conv1)\n\n    conv2 = Conv1D(x*2,3, activation='relu', padding='same')(pool1)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Conv1D(x*2,3, activation='relu', padding='same')(conv2)\n    conv2 = BatchNormalization()(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(conv2)\n\n    conv3 = Conv1D(x*4,3, activation='relu', padding='same')(pool2)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Conv1D(x*4,3, activation='relu', padding='same')(conv3)\n    conv3 = BatchNormalization()(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(conv3)\n\n    conv4 = Conv1D(x*8,3, activation='relu', padding='same')(pool3)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Conv1D(x*8,3, activation='relu', padding='same')(conv4)\n    conv4 = BatchNormalization()(conv4)\n    pool4 = MaxPooling1D(pool_size=2)(conv4)\n\n    conv5 = Conv1D(x*16, 3, activation='relu', padding='same')(pool4)\n    conv5 = BatchNormalization()(conv5)\n    conv5 = Conv1D(x*16, 3, activation='relu', padding='same')(conv5)\n    conv5 = BatchNormalization()(conv5)\n    \n    level4 = Conv1D(1, 1, name=\"level4\")(conv5)\n\n    up6 = concatenate([UpSampling1D(size=2)(conv5), conv4], axis=2)\n    conv6 = Conv1D(x*8, 3, activation='relu', padding='same')(up6)\n    conv6 = BatchNormalization()(conv6)\n    conv6 = Conv1D(x*8, 3, activation='relu', padding='same')(conv6)\n    conv6 = BatchNormalization()(conv6)\n    \n    level3 = Conv1D(1, 1, name=\"level3\")(conv6)\n\n    up7 = concatenate([UpSampling1D(size=2)(conv6), conv3], axis=2)\n    conv7 = Conv1D(x*4, 3, activation='relu', padding='same')(up7)\n    conv7 = BatchNormalization()(conv7)\n    conv7 = Conv1D(x*4, 3, activation='relu', padding='same')(conv7)\n    conv7 = BatchNormalization()(conv7)\n    \n    level2 = Conv1D(1, 1, name=\"level2\")(conv7)\n\n    up8 = concatenate([UpSampling1D(size=2)(conv7), conv2], axis=2)\n    conv8 = Conv1D(x*2, 3, activation='relu', padding='same')(up8)\n    conv8 = BatchNormalization()(conv8)\n    conv8 = Conv1D(x*2, 3, activation='relu', padding='same')(conv8)\n    conv8 = BatchNormalization()(conv8)\n    \n    level1 = Conv1D(1, 1, name=\"level1\")(conv8)\n\n    up9 = concatenate([UpSampling1D(size=2)(conv8), conv1], axis=2)\n    conv9 = Conv1D(x, 3, activation='relu', padding='same')(up9)\n    conv9 = BatchNormalization()(conv9)\n    conv9 = Conv1D(x, 3, activation='relu', padding='same')(conv9)\n    conv9 = BatchNormalization()(conv9)\n\n    out = Conv1D(1, 1, name=\"out\")(conv9)\n\n    model = Model(inputs=[inputs], outputs=[out, level1, level2, level3, level4])\n    \n    \n\n    return model\n\n\n","059b5cdc":"df_train = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')","df7389f6":"df_train.shape","4c30550c":"df_train.head()","bacbfcb7":"df_train.time_step.value_counts()","5e685450":"unique_breaths = df_train['breath_id'].unique()\nnum_breaths = len(unique_breaths)\nprint(num_breaths)","4367a5b4":"df_train['breath_id'][:500].plot();","516a30fd":"breath_lengths = df_train[['id','breath_id']].groupby('breath_id').count()['id']\nbreath_lengths.unique()","21f0ef59":"BREATH_LENGTH = breath_lengths.unique()[0]","6f2f12cb":"r_c_std_in_breaths = df_train[['breath_id','R','C']].groupby('breath_id').std()\nprint(r_c_std_in_breaths['R'].unique())\nprint(r_c_std_in_breaths['C'].unique())","ba2f5404":"r_values = df_train[['breath_id', 'R']].groupby('breath_id').mean()['R']\nprint(r_values)\nprint()\nprint('Unique values:')\nprint(r_values.value_counts())\n\nr_unique = np.sort(r_values.unique()).astype(int)","c1310753":"c_values = df_train[['breath_id', 'C']].groupby('breath_id').mean()['C']\nprint(c_values)\nprint()\nprint('Unique values:')\nprint(c_values.value_counts())\n\nc_unique = np.sort(c_values.unique()).astype(int)","6c8efef6":"rc_values = np.array([\n    [r, c, len(df_train[(df_train['R'] == r) & (df_train['C'] == c)])\/\/BREATH_LENGTH] \n    for r in r_unique \n    for c in c_unique\n])\n\nx = range(len(rc_values))\nplt.bar(x, rc_values[:,2])\nplt.xticks(x, [str(r) + '_' + str(c) for r, c in rc_values[:,:2] ])\nplt.xlabel('R_C')\nplt.ylabel('Number counts')\nplt.show()","1eee2d47":"first_breath  = df_train[df_train['breath_id'] == 1]\nsecond_breath = df_train[df_train['breath_id'] == 2]\n\nx = range(BREATH_LENGTH)\nt1 = first_breath['time_step']\nt2 = second_breath['time_step']\nplt.plot(x, t1)\nplt.plot(x, t2, ls = '--')","e194244e":"(max(t1) - min(t1)) \/ BREATH_LENGTH","86a456fb":"plt.plot(t1.values - t2.values);","ffe13e8b":"# Pressure in first breath\nplt.plot(df_train.pressure[:800])","1f76d385":"plt.plot(df_train.pressure[:800])","6f2c7294":"plt.plot(df_train.u_in[:80])","7c92d238":"plt.plot(df_train.u_in[:1000])","e63d8133":"plt.plot(df_train.u_out[:1000])","81db1e6a":"plt.boxplot(df_train.u_in);","dece5dc5":"pressao = plt.boxplot(df_train.pressure);","09794fdf":"percentiles = [item.get_ydata()[1] for item in pressao['whiskers']]","89864266":"percentiles","6d2ded75":"list1 = df_train.u_in.tolist()","6de73e45":"#list1","383ddeb2":"#input_minima = np.min(list1)\n#input_maxima = np.max(list1)\n#print(\"Minimum value of X\",input_minima)\n#print(\"Maximum values of X\",input_maxima)","bfe51020":"#list1 = pd.Series(list1)\n#list1 -= input_minima                       # normalizing\n#list1 \/= (input_maxima-input_minima)","b69e33bb":"plt.plot(list1[:160])","03d89386":"n = 80\nX = [list1[i:i + n] for i in range(0, len(list1), n)]","ab21b31c":"#print(\"Minimum value of X Normalized\",np.min(X))\n#print(\"Maximum values of X Normalized\",np.max(X))\nplt.plot(X[0])","fd921ad0":"#X","f308ffb4":"list2 = df_train.pressure.tolist()","a09a94a2":"#list2","9065860a":"#output_minima = np.min(list2)\n#output_maxima = np.max(list2)\n#print(\"Minimum value of Y\",output_minima)\n#print(\"Maximum values of Y\",output_maxima)","5225954b":"#list2 = pd.Series(list2)\n#list2 -= output_minima                       # normalizing\n#list2 \/= (output_maxima-output_minima)","9f06fa2e":"plt.plot(list2[:160])","6490df14":"i = 0\nY = [list2[i:i + n] for i in range(0, len(list2), n)]\n","25c8e49e":"#print(\"Minimum value of Y Normalized\",np.min(Y))\n#print(\"Maximum values of Y Normalized\",np.max(Y))\nplt.plot(Y[0])","0a70e26f":"#Y","6931cf33":"from sklearn.model_selection import train_test_split\n","06b06e04":"X_train, X_val, y_train, y_val = train_test_split(X,Y)","9f919145":"X_train = np.array(X_train)\nX_val = np.array(X_val)\ny_train = np.array(y_train)\ny_val = np.array(y_val)","cb1b95fc":"print(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","2b2193da":"from tqdm import tqdm\nimport tensorflow as tf\nACCELERATOR_TYPE = 'GPU'\n\nif ACCELERATOR_TYPE == 'GPU':\n    strategy = tf.distribute.MirroredStrategy()\n    print(\"GPU\")","9a7d53a8":"def prepareLabel(Y):\n\n    \"\"\"\n    Prepare label for deep supervised pipeline\n    \n    Returns:\n        dictionary -- dictionary containing the 5 level ground truth outputs of the network\n    \"\"\"\n    \n    def approximate(inp,w_len):\n        \"\"\"\n        Downsamples using taking mean over window\n        \n        Arguments:\n            inp {array} -- signal\n            w_len {int} -- length of window\n\n        Returns:\n            array -- downsampled signal\n        \"\"\"\n        \n        op = []\n        \n        for i in range(0,len(inp),w_len):\n        \n            op.append(np.mean(inp[i:i+w_len]))\n            \n        return np.array(op)\n\n    out = {}\n    out['out'] = []\n    out['level1'] = []\n    out['level2'] = []\n    out['level3'] = []\n    out['level4'] = []\n    \n    \n    for y in tqdm(Y,desc='Preparing Label for DS'):\n    \n                                                                    # computing approximations\n        cA1 = approximate(np.array(y).reshape(length), 2)\n\n        cA2 = approximate(np.array(y).reshape(length), 4)\n\n        cA3 = approximate(np.array(y).reshape(length), 8)\n\n        cA4 = approximate(np.array(y).reshape(length), 16)\n        \n\n\n                                                                    # populating the labels for different labels\n        out['out'].append(np.array(y.reshape(length,1)))\n        out['level1'].append(np.array(cA1.reshape(length\/\/2,1)))\n        out['level2'].append(np.array(cA2.reshape(length\/\/4,1)))\n        out['level3'].append(np.array(cA3.reshape(length\/\/8,1)))\n        out['level4'].append(np.array(cA4.reshape(length\/\/16,1)))\n\n    out['out'] = np.array(out['out'])                                # converting to numpy array\n    out['level1'] = np.array(out['level1'])\n    out['level2'] = np.array(out['level2'])\n    out['level3'] = np.array(out['level3'])\n    out['level4'] = np.array(out['level4'])\n    \n\n    return out","7b531cfd":"import os\nimport pickle \n\nlength = 80\nmodel = UNetDS64(length, n_channel=1)\nmdlName1 = 'UNetDS64'\n\ntry:                                                        # create directory to save training model\n    os.makedirs('models')\nexcept:\n    pass\n\ntry:                                                        # create directory to save training history\n    os.makedirs('History')\nexcept:\n    pass\n\ndef train_approximation_network(model,X_train, X_val, y_train, y_val):\n    for foldname in range(10):\n\n            print('----------------')\n            print('Training Fold {}'.format(foldname+1))\n            print('----------------')\n\n          \n\n            Y_train = prepareLabel(y_train)                                         # prepare labels for training deep supervision\n\n            Y_val = prepareLabel(y_val)                                             # prepare labels for training deep supervision\n\n\n\n            mdl1 = model          # create approximation network\n\n                                                                                # loss = mse, with deep supervision weights\n            mdl1.compile(loss='mean_absolute_error',optimizer='adam',metrics=['mean_squared_error'], loss_weights=[1., 0.9, 0.8, 0.7, 0.6])                                                         \n\n            # Reduce Learning Rate\n            lr = ReduceLROnPlateau(monitor=\"val_out_loss\", factor=0.85, \n                               patience=7, verbose=1)\n            # Checkpoint callbakc\n            checkpoint1_ = ModelCheckpoint(os.path.join('models','{}_model1_fold{}.h5'.format(mdlName1,foldname)), verbose=1, monitor='val_out_loss',save_best_only=True, mode='auto')  \n                                                                            \n            # Early Stopping to avoid overfitting\n            es = EarlyStopping(monitor=\"val_out_loss\", patience=30, \n                           verbose=1, mode=\"min\", \n                           restore_best_weights=True)\n            # train approximation network for 100 epochs\n            history1 = mdl1.fit(X_train,{'out': Y_train['out'], 'level1': Y_train['level1'], 'level2':Y_train['level2'], 'level3':Y_train['level3'] , 'level4':Y_train['level4']},epochs=100,batch_size=512,\n                                validation_data=(X_val,{'out': Y_val['out'], 'level1': Y_val['level1'], 'level2':Y_val['level2'], 'level3':Y_val['level3'] , 'level4':Y_val['level4']}),callbacks=[lr, checkpoint1_, es],verbose=1)\n\n            pickle.dump(history1.history, open('History\/{}_model1_fold{}.p'.format(mdlName1,foldname),'wb'))    # save training history\n\n\n            mdl1 = None                                             # garbage collection\n\n            #time.sleep(300)                                         # pause execution for a while to free the gpu'''\n    \n","5680474b":"train_approximation_network(model, X_train, X_val, y_train, y_val)","d7149d96":"fold_val_loss = []\nimport pickle\n# for fold in training history\nfor fold in range(10):\n    # open and load pickle file as bytes\n    file = open('.\/History\/UNetDS64_model1_fold{}.p'.format(fold), 'rb')\n    history = pickle.load(file)\n    \n    # print best score and epoch for each fold\n    print(\"Fold: \", fold)\n    print(\"Best model scored: {} \\nin epoch: {}\".format(np.min(history['val_loss']),np.argmin(history['val_loss'])),\"\\n\")\n    \n    # append validation loss to select best weights\n    fold_val_loss.append((np.min(history['val_loss']),np.argmin(history['val_loss'])))\n    \n    \n    # Plot trainig history loss and val_loss \n    plt.plot(history['loss'][:])\n    plt.plot(history['val_loss'][:])\n    plt.title('model loss')\n    plt.ylabel('MAE loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    \n","b80e1924":"\nfold_loss = [i[0] for i in fold_val_loss]            # list of val_loss per fold\nmin_loss = np.min(fold_val_loss)                     # get min val_loss\nmin_loss_fold = np.argmin(fold_loss)                 # get min val_los fold\n_, min_loss_epoch = fold_val_loss[min_loss_fold-1]   # get min val_loss epoch\nprint(\"Best combination in fold {}, val loss: {}, epoch: {}\".format(min_loss_fold,min_loss,min_loss_epoch))    # Print best model information for prediciton","b54a10a2":"df_test = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')","7677a7de":"list1 = df_test.u_in.tolist()\nn = 80\nX_test = [list1[i:i + n] for i in range(0, len(list1), n)]\nplt.plot(X_test[0])","a013faa5":"import os\nmdl1 = UNetDS64(80)                                             # creating approximation network\npath = \".\/models\"\nmdl1.load_weights(os.path.join(path,'UNetDS64_model1_fold{}.h5'.format(min_loss_fold)))   # loading weights\nY_test_pred_approximate = mdl1.predict(X_test,verbose=1)            # predicting approximate abp waveform","51b99702":"Y_test_pred_approximate = np.array(Y_test_pred_approximate[0])","afa0f2db":"samples, _,_ = Y_test_pred_approximate.shape\npressure = []\nfor signal in range(samples):\n    breath_pressure = [j for i in Y_test_pred_approximate[signal] for j in i]\n    pressure.extend(breath_pressure)","b018faaa":"sub = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')","12408e2b":"sub.pressure = pressure\nsub.head()","618caa8e":"sub.to_csv('approximation_submission.csv', index=False)","0cd20c9e":"Load submission csv","ea40c7f3":"we note a strong correlation between te columns *pressure* and *u_in*","5a969c2d":"Select each individual breath from output signal","d827f68b":"### Create submission file","187a0579":"Are there outliers in the dataset?","465739bb":"The two time series for the first two breaths are not perfectly aligned","9e441633":"## Approximation Model 1 Training\/Validation History","53f2b679":"# Google Brain - Ventilator Pressure Prediction \n### 1D-Unet with Deep Supervision Approach\n\nVladimir Sim\u00f5es da Luz Junior\n\n[LinkedIn](https:\/\/www.linkedin.com\/in\/vladimir-simoes-da-luz-junior\/)\n\n[GitHub](https:\/\/www.linkedin.com\/in\/vladimir-simoes-da-luz-junior\/)\n","baaa819e":"Load test set","38058595":"Normalize input","52ca0259":"### Now preparing our Y *pressure* array","f557bf38":"One time step seems to correspond to about\n\n","8cab0163":"Assign predicted values to *pressure* column","215a84e0":"There is about a factor two scatter in the various R\/C combinations.\n\nFor R = 20 we see C = 50 most often, for R = 5, 50 we see C = 10 most often.\n\n","56ba342f":"So does C:\n","343e1881":"## Predict on test set","64002525":"### Fisrt we will prepare our X input *u_in* array","b850e68f":"Train and Validation data split","362132a4":"Select each individual breath from input signal","2bc17e50":"## R and C\nR and C values are constant within each breath (having zero standard deviation)\n\n","914977d3":"## Models \n### Approximation Network - UNetDS64","4d9d4b2b":"This solution makes reference to [PPG2ABP](https:\/\/arxiv.org\/abs\/2005.01669), that have used a 1 dimensional U-Net with deep supervision to predict the arterial blood pressure waveform from the photopletysmography signal.\n\nWe have selected each individual breath from the Google Brain - VPP dataset as one single 1D input array containing the *u_in* time series. The model architecture encodes the feature map from the *u_in* breath signal and decode the feature into the *pressure* signal from the ventilator. ","d171029a":"R has only three distinct values:","73c2a409":"Normalize output","3cd667aa":"Save submission csv","4de1a280":"## Time steps in individual breaths\nTake a look at time sampling for the first two breaths. Looks like pretty uniform sampling in time.\n\n","9be1db6c":"## Load training data and first exploratory data analysis","d3e7ff28":"Instantiate model architecture with best fold weights and predict over test dataset","705265c0":"### Which fold weight lead to the lowest error metric?","3105547a":"## Libraries","0bd8eca7":"### Fitting loop","fc6e101e":"## From that we can train our 1D-Unet based on the *u_in* and *pressure* time series","0bc2bf6b":"## What about the target vector \"pressure\"","d7f32bed":"### GPU accelerator config","9e837d15":"### Deep Supervision configs","41e52c57":"Select each individual breath","f0408d73":"Concatenate results into a single list","8fbe133b":"## Train Approximation Network"}}