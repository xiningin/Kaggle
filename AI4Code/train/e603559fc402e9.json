{"cell_type":{"c3fa7936":"code","91a59a28":"code","d21f17c8":"code","05659f03":"code","954038cc":"code","1ee1930e":"code","b415c03f":"code","8e426294":"code","3501195c":"markdown","175d9332":"markdown","c91635bd":"markdown","1e9f0759":"markdown","2b831bee":"markdown","be176beb":"markdown","4f1ba6b8":"markdown","806f8ef0":"markdown"},"source":{"c3fa7936":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Path of file to read\ndata_path = '..\/input\/college-basketball-players-20092021\/CollegeBasketballPlayers2009-2021.csv'\ntotal_ball_data = pd.read_csv(data_path, low_memory = False)\n\n# Only include rows in which a player was drafted, and before 2021 (2021 will be used as test data)\nball_data_1 = total_ball_data.loc[total_ball_data['pick'] >= 1]\nball_data = ball_data_1.loc[ball_data_1['year'] < 2021]","91a59a28":"# Shows a preview of data\nball_data.head()","d21f17c8":"# Creates a target object for the pick and calls it y\ny = ball_data.pick\n\n# Creates X by using specific statistics\nfeatures = ['Min_per', 'ftr', 'pts', 'ORB_per', 'DRB_per', 'blk_per', 'stl_per', 'dporpag', 'porpag', 'AST_per', 'TO_per', 'FT_per']\nX = ball_data[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n","05659f03":"# Specify a Random Forest Model\nball_model = DecisionTreeRegressor(max_leaf_nodes=100, splitter='random', random_state=1, max_features='auto')\n# Fit the model\nball_model.fit(train_X, train_y)\n\n# Make predictions on validation data and calculate the mean absolute error\nval_preds = ball_model.predict(val_X)\nval_mae = mean_absolute_error(val_preds, val_y)\nprint(\"Validation MAE: {:,.5f}\".format(val_mae))","954038cc":"# Test data, using 2021\nball_data_2021 = ball_data_1.loc[ball_data_1['year'] == 2021]\ntest_X_2021 = ball_data_2021[features]\n\n# Make predictions\ntest_preds_2021 = ball_model.predict(test_X_2021)\n\n# Find MAE\ntest_mae_2021 = mean_absolute_error(test_preds_2021, ball_data_2021.pick)\n\n# Save predictions in right format\noutput = pd.DataFrame({'Name': ball_data_2021.player_name, 'Pick': test_preds_2021, 'Actual Pick' : ball_data_2021.pick})\noutput.to_csv('predictions2021.csv', index=False)\n\nprint(\"2021 Test MAE: {:,.5f}\".format(test_mae_2021))","1ee1930e":"from xgboost import XGBRegressor\n\n# Create model\nxgb = XGBRegressor(n_estimators=1200, learning_rate=0.106, max_depth=10, reg_lambda=1.1,tree_method='approx',random_state=1)\n\n# Fit model\nxgb.fit(train_X, train_y)\n\n# Get predictions\nxgb_preds = xgb.predict(val_X)\n\n# Get MAE\nxgb_mae = mean_absolute_error(xgb_preds, val_y)\n\nprint(\"XGB Validation MAE: {:,.5f}\".format(xgb_mae))","b415c03f":"# Make predictions\ntest_preds_xgb = xgb.predict(test_X_2021)\n\n# Find MAE\ntest_mae_xgb = mean_absolute_error(test_preds_xgb, ball_data_2021.pick)\n\nprint(\"2021 XGB Test MAE: {:,.5f}\".format(test_mae_xgb))","8e426294":"# Save predictions in right format\noutput = pd.DataFrame({'Name': ball_data_2021.player_name, 'Pick': test_preds_xgb, 'Actual Pick' : ball_data_2021.pick})\noutput.to_csv('xgbpredictions2021.csv', index=False)","3501195c":"Now, we run our model on the test data from 2021, getting an MAE of 13.7. This is a good sign, since it is possible for the test MAE to be much higher than the validation MAE due to errors such as overfitting.","175d9332":"The XGBoost method does show some signs of improvement, but for true improvement to this model I need to find the right combination of statistics to use. Also, for this to actually be useful, I would like to extrapolate this to an NBA context (in other words, using college stats to predict a player's NBA success).\nAny feedback would be appreciated!","c91635bd":"After fitting the data to the model and running the model on the prediction data, our mean absolute error is around 15. This means that our predictions for the pick, are on average 15 off from reality. There's definitely room for improvement here, and I would like to compare this value to the MAE after I find the correct combination of statistics to use.","1e9f0759":"Now, I would like to use XGBoost to potentially create a more accurate model:\n","2b831bee":"We start with the boiler plate: importing all necessary libraries and functions such as mean_absolute_error. Additionally, we read in the data using pandas. I decide to focus only on players who were actually drafted into the NBA. I also only look at players from 2009-2020, opting to use 2021 as a test data set.\n","be176beb":"We set the draft pick as the target, but this time we choose different stats than version 1. \nWe use: ","4f1ba6b8":"**Background**\n\nThe NBA Draft is one of the most unpredictable events in sports. Even advanced analytics of the modern era fail to stop teams from making inaccurate picks. For instance, let's look back at the 2018 draft class. Deandre Ayton and Marvin Bagley III were taken over Luka Doncic and Trae Young!\n\nOf course, there's no solution that will completely eliminate bad picks. However, I hope to use my College Basketball Players 2009-2021 dataset to predict what pick a player should be taken. In the future, I would like to incorporate the players' NBA data in order to predict stats such as winshares or PER based on college stats. ","806f8ef0":"Now, I will use this model to make the same predictions on the 2021 NBA draft class:"}}