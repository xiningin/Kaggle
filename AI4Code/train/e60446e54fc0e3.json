{"cell_type":{"8f736e3f":"code","98192655":"code","d8bdba9a":"code","ee04ad7d":"code","90012331":"code","e1bbafb3":"code","2da34b6b":"code","0afe4f8a":"code","faa27599":"code","7a305091":"code","ecf56235":"code","2f5613cc":"code","17706720":"code","f2be23fe":"code","a72fe9bc":"code","45e56dcc":"code","1320750c":"code","22cce2e3":"code","7f9b2308":"code","98fff426":"code","63413091":"code","cde1ad7a":"code","fc567886":"code","d0571c2e":"code","9b7093a7":"code","a51d2d34":"code","96194151":"code","5199bc79":"code","ad16295e":"code","c877983a":"code","b29de290":"code","1896d037":"code","31d766e5":"code","042da2d1":"code","65a75e77":"code","08985675":"code","07bf9e8f":"code","372efd13":"code","d5828847":"code","2f8bfefc":"code","19634c6c":"code","dd633350":"code","0c981fa3":"code","438d6a97":"code","b746f6d6":"code","81b6b981":"code","048a439b":"code","adffed07":"code","22a3529b":"code","d995e6b2":"code","b591acb9":"code","26892bf0":"code","636ae631":"code","93cbb7e8":"code","f343d2e8":"code","663dfd53":"code","543fa351":"markdown","44f23e70":"markdown","d0af6868":"markdown","f1f5e8a0":"markdown","e463986f":"markdown","eaa0818f":"markdown","66999101":"markdown","5322d4a2":"markdown","d9eeccb1":"markdown","da0e1b21":"markdown","16a2b6fb":"markdown","258fb8a3":"markdown","cc5a459c":"markdown","24a2e23c":"markdown","7f82d049":"markdown","b3124af8":"markdown","0152c5bf":"markdown","c64063ab":"markdown","6490806b":"markdown","8b8ed69e":"markdown"},"source":{"8f736e3f":"!pip install -q ipyplot\n!pip install -q imbalanced-learn\n!pip install -q delayed\n!pip install -q lightly==1.1.16\n!pip install -q ttach","98192655":"import sys\nsys.path.append(\"..\/input\/timm-pytorch-image-models\/pytorch-image-models-master\")\n\nimport gc\nimport math\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nimport time\nimport warnings\nimport json\nimport ipyplot\nimport plotly.express as px\nimport seaborn as sns\nimport ttach\n\nfrom collections import Counter, defaultdict\nfrom contextlib import contextmanager\n\nimport albumentations as A\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport scipy.stats as stats\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport albumentations as A\n\nimport math\nimport torchvision\nimport lightly\n\nfrom albumentations.pytorch import ToTensorV2\nfrom functools import partial\nfrom pathlib import Path\nfrom PIL import Image\nfrom scipy.special import softmax\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\nfrom torch.optim import SGD, Adam, AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import StratifiedKFold\nfrom typing import Optional, Tuple\nfrom glob import glob\n\nfrom imblearn.over_sampling import SMOTE\n\nimport timm\nwarnings.filterwarnings('ignore')","d8bdba9a":"class CFG:\n    class_num = 10\n    seed = 0 # seed\u5024\n    num_workers = 2 #\u4e26\u5217\u5b9f\u884c\u3059\u308b\u6570\n    batch_size =  128 #\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n    size = 224 #224 # \u30ea\u30b5\u30a4\u30ba\u3057\u305f\u5f8c\u306e\u753b\u50cf\u306e\u30b5\u30a4\u30ba\n    epochs = 100 #\u30a8\u30dd\u30c3\u30af\u6570\n    ssl_batch_size = 64#512 \n    ssl_epochs = 20 #\u30a8\u30dd\u30c3\u30af\u6570\n    ssl_input_size = 224\n    model_lr = 1e-4 # \u5b66\u7fd2\u7387 5\n    T_max = 20 # \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570\n    min_lr = 1e-5 # \u5b66\u7fd2\u7387\u306e\u6700\u5c0f\u5024 6\n    weight_decay = 1e-5 # \u5b66\u7fd2\u6e1b\u8870\u5024 6\n    max_grad_norm = 1000 # \u52fe\u914d\u306e\u6700\u5927\u30ce\u30eb\u30e0\n    print_freq = 1000 # \u5b66\u7fd2\u7d50\u679c\u3092\u8868\u793a\u3059\u308b\u983b\u5ea6\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # CPU of GPU\n    MODEL_NAME = \"resnet18d\"#'efficientnet_b0'#\"visformer_tiny\"#'vit_base_patch16_224'#\"resnet34\" # \u30e2\u30c7\u30eb\u540d\n    Version = \"V2\"# model save\u6642\u306eversion","ee04ad7d":"def init_logger(log_file='train.log'):\n    \"\"\"Output Log.\"\"\"\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\ndef seed_torch(seed=0):\n    \"\"\"Fixed seed value.\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nLOGGER = init_logger()\nseed_torch(seed=CFG.seed)","90012331":"def asMinutes(s):\n    \"\"\"Convert Seconds to Minutes.\"\"\"\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    \"\"\"Accessing and Converting Time Data.\"\"\"\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value.\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","e1bbafb3":"class TrainDataset(Dataset):\n    \"\"\"Dataset used for training.\"\"\"\n    def __init__(self, df, transform=None):\n        #self.df = df['img_path']\n        self.df = df['img']\n        self.labels = df['label'].values\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        #image = np.array(Image.open(self.df.iloc[idx]))\n        image = np.array(self.df.iloc[idx]).reshape(32,32,3)\n        \n        if self.transform:\n            augmented = self.transform(image=image) # augmentation\n            image = augmented['image']\n\n        label = self.labels[idx] # \u6b63\u89e3\u30e9\u30d9\u30eb\n        return image, label","2da34b6b":"class TestDataset(Dataset):\n    \"\"\"Dataset used for inference.\"\"\"\n    def __init__(self, df, transform=None):\n        #self.df = df['img_path']\n        self.df = df['image_id']\n        \n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = np.array(Image.open(f'..\/input\/animal-imbalance-classification\/test_images\/{self.df.iloc[idx]}'))\n        #image = np.array(self.df.iloc[idx]).reshape(32,32,3)\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n\n        return image","0afe4f8a":"def get_transforms(*, data, size, norms):\n    if data == 'train':\n        return A.Compose([\n            # \u5e7e\u4f55\u5909\u63db\u7cfb\n            A.HorizontalFlip(p=0.5), \n            A.VerticalFlip(p=0.5),\n            A.CenterCrop(width=24, height=24, p=0.5),\n            \n            # \u9ad8\u5ea6\u5e7e\u4f55\u5909\u63db\u7cfb\n            A.ShiftScaleRotate(p=0.5), \n             #A.RandomGridShuffle(p=0.5),\n            \n            #A.OneOf([\n            #    A.OpticalDistortion(p=0.5),# \u5149\u5b66\u7684\u6b6a\u307f\n            #    A.GridDistortion(p=0.5),\n            #    A.ElasticTransform(p=0.5), #\u5f3e\u6027\u5909\u63db\n            #], p=0.5),\n\n            # \u307c\u304b\u3057&\u30ce\u30a4\u30ba\u7cfb\n            #A.OneOf([\n            #    A.MotionBlur(p=0.5),\n            #    A.MedianBlur(p=0.5),\n            #    A.GaussianBlur(p=0.5),\n            #    A.GaussNoise(p=0.5),\n            #], p=0.5),\n            \n            # \u8272(\u8272\u76f8,\u5f69\u5ea6,\u8f1d\u5ea6)\u5909\u63db\u7cfb\n            A.HueSaturationValue(p=0.5), \n            A.RandomBrightnessContrast(p=0.5),\n\n            A.Resize(size, size), # \u30ea\u30b5\u30a4\u30ba            \n            #Dropout\u7cfb\n            A.Cutout(p=0.5,max_h_size=int(size * 0.1), max_w_size=int(size * 0.1), num_holes=5), \n           \n            A.Normalize(mean=[norms[0], norms[0], norms[0]], std=[norms[1], norms[1], norms[1]], max_pixel_value=255.0, p=1.0), # \u6b63\u898f\u5316\n            #x_new = (x - mean) \/ std mean=0.5, std=0.5-> x_new = 2x - 1\n            #[0, 1] \u7bc4\u56f2\u306e\u5024\u3092 [-1, 1] \u7bc4\u56f2\u306e\u5024\u306b\u5909\u63db\u3059\u308b\u51e6\u7406\n            ToTensorV2(), # Tensor\u578b\u306b\u5909\u63db\n        ])\n\n    elif data == 'valid':\n        return A.Compose([\n            #A.ChannelShuffle(p=1),\n            #A.ToGray(p=1),\n            A.Resize(size, size),\n            A.Normalize(mean=[norms[0], norms[0], norms[0]], std=[norms[1], norms[1], norms[1]], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(),\n        ])","faa27599":"# we use a pretrained resnet for this tutorial to speed\n# up training time but you can also train one from scratch\n# Do not use pretrained Model\n#resnet = torchvision.models.resnet18(pretrained=False)\n#backbone = nn.Sequential(*list(resnet.children())[:-1])\n\n#\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb https:\/\/docs.lightly.ai\/tutorials\/package\/tutorial_simsiam_esa.html\n    \ndef self_supervised_learning(backbone, num_features, ssl_train=False):\n    # dimension of the embeddings\n    num_ftrs = num_features#512\n    # dimension of the output of the prediction and projection heads\n    out_dim = proj_hidden_dim = num_features#512\n    # the prediction head uses a bottleneck architecture\n    pred_hidden_dim = 128\n    # use 2 layers in the projection head#\n    num_mlp_layers = 2\n    \n    path_to_data = '..\/input\/animal-imbalance-classification\/test_images'\n\n    # define the augmentations for self-supervised learning\n    collate_fn = lightly.data.ImageCollateFunction(\n        input_size=CFG.ssl_input_size,\n        # require invariance to flips and rotations\n        hf_prob=0.5,\n        vf_prob=0.5,\n        rr_prob=0.5,\n        # satellite images are all taken from the same height\n        # so we use only slight random cropping\n        min_scale=0.5,\n        # use a weak color jitter for invariance w.r.t small color changes\n        cj_prob=0.2,\n        cj_bright=0.1,\n        cj_contrast=0.1,\n        cj_hue=0.1,\n        cj_sat=0.1,\n    )\n\n    # create a lightly dataset for training, since the augmentations are handled\n    # by the collate function, there is no need to apply additional ones here\n    dataset_train_simsiam = lightly.data.LightlyDataset(\n        input_dir=path_to_data\n    )\n\n    # create a dataloader for training\n    dataloader_train_simsiam = torch.utils.data.DataLoader(\n        dataset_train_simsiam,\n        batch_size=CFG.ssl_batch_size,\n        shuffle=True,\n        collate_fn=collate_fn,\n        drop_last=True,\n        num_workers=CFG.num_workers\n    )\n\n    # create the SimSiam model using the backbone from above\n    model = lightly.models.SimSiam(\n        backbone,\n        num_ftrs=num_ftrs,\n        proj_hidden_dim=pred_hidden_dim,\n        pred_hidden_dim=pred_hidden_dim,\n        out_dim=out_dim,\n        num_mlp_layers=num_mlp_layers\n    )\n    \n    if ssl_train==False:\n        return model\n        \n    # SimSiam uses a symmetric negative cosine similarity loss\n    criterion = lightly.loss.SymNegCosineSimilarityLoss()\n\n    # scale the learning rate\n    lr = 0.05 * CFG.ssl_batch_size \/ 256\n    # use SGD with momentum and weight decay\n    optimizer = torch.optim.SGD(\n        model.parameters(),\n        lr=lr,\n        momentum=0.9,\n        weight_decay=5e-4\n    )\n    \n    #model.load_state_dict(torch.load(f'..\/input\/models\/ssl_200_efficientnet_b0_V9_180.pth'))#, map_location=DEVICE)\n    model.to(CFG.device)\n\n    avg_loss = 0.\n    avg_output_std = 0.\n    print('*******Self-Supervised Learning*******')\n    for e in tqdm(range(CFG.ssl_epochs)):\n        for (x0, x1), _, _ in dataloader_train_simsiam:\n            # move images to the gpu\n            x0 = x0.to(CFG.device)\n            x1 = x1.to(CFG.device)\n\n            # run the model on both transforms of the images\n            # the output of the simsiam model is a y containing the predictions\n            # and projections for each input x\n            y0, y1 = model(x0, x1)\n\n            # backpropagation\n            loss = criterion(y0, y1)\n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # calculate the per-dimension standard deviation of the outputs\n            # we can use this later to check whether the embeddings are collapsing\n            output, _ = y0\n            output = output.detach()\n            output = torch.nn.functional.normalize(output, dim=1)\n\n            output_std = torch.std(output, 0)\n            output_std = output_std.mean()\n\n            # use moving averages to track the loss and standard deviation\n            w = 0.9\n            avg_loss = w * avg_loss + (1 - w) * loss.item()\n            avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n            del x0, x1, y0, y1, output, output_std\n            \n        # the level of collapse is large if the standard deviation of the l2\n        # normalized output is much smaller than 1 \/ sqrt(dim)\n        collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n        # print intermediate results\n        print(f'[Epoch {e:3d}] 'f'Loss = {avg_loss:.2f} | 'f'Collapse Level: {collapse_level:.2f} \/ 1.00')\n        if e % 20 ==0:\n            torch.save(model.state_dict(), f'.\/ssl_{CFG.ssl_epochs}_{CFG.MODEL_NAME}_{CFG.Version}_{e}.pth') # \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58 \n\n    torch.save(model.state_dict(), f'.\/ssl_{CFG.ssl_epochs}_{CFG.MODEL_NAME}_{CFG.Version}.pth') # \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58 \n    \n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return model","7a305091":"# use timm model no pretrained\nclass BaseModel(nn.Module):\n    def __init__(self, model_name=CFG.MODEL_NAME, n_class=10, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=n_class) # model load(pretrained=False: \u4e8b\u524d\u5b66\u7fd2\u306a\u3057)                \n        \n    def forward(self, x):\n        output = self.model(x)\n        return output\n    \n    #https:\/\/dajiro.com\/entry\/2020\/07\/24\/161040\n    #https:\/\/teyoblog.hatenablog.com\/entry\/2021\/02\/21\/200938\n    #resnet\u7cfb\u3060\u3068fc,efficientnet\u306fclassifier,ViT\u306fhead\n    def freeze(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        # To unfreeze the last layer\n        for param in self.model.classifier.parameters():\n            param.requires_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.model.parameters():\n            param.requires_grad = True","ecf56235":"def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    #avg_score = AverageMeter()\n\n    model.train() # \u5b66\u7fd2\u30e2\u30fc\u30c9\n    start = end = time.time()\n    global_step = 0\n\n    for step, (images, labels) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(CFG.device) # \u753b\u50cf\u3092cpu\u304b\u3089gpu\u30d8\n        labels = labels.to(CFG.device) # \u6b63\u89e3\u30e9\u30d9\u30eb\u3092cpu\u304b\u3089gpu\u30d8\n        batch_size = labels.size(0) \n        y_preds = model(images) # \u4e88\u6e2c\u30e9\u30d9\u30eb\n        loss = criterion(y_preds, labels) # loss\u306e\u8a08\u7b97\n\n        losses.update(loss.item(), batch_size) \n        loss.backward() # \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u52fe\u914d\u3092\u8a08\u7b97\n        \n        del loss \n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        optimizer.step() # \u30e2\u30c7\u30eb\u66f4\u65b0\n        optimizer.zero_grad() # \u52fe\u914d\u306e\u521d\u671f\u5316\n        global_step += 1\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(\n                   epoch+1, step, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses,\n                   remain=timeSince(start, float(step+1)\/len(train_loader)),\n                   grad_norm=grad_norm,\n                   ))\n\n        del y_preds, images, labels, batch_size\n    del batch_time, data_time, scores, model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return losses.avg","2f5613cc":"def valid_fn(valid_loader, model, criterion):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    # switch to evaluation mode\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (images, labels) in enumerate(valid_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(CFG.device)\n        labels = labels.to(CFG.device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n        #with torch.inference_mode():\n            y_preds = model(images)\n            loss = criterion(y_preds, labels)\n            losses.update(loss.item(), batch_size)\n            \n            preds.append(y_preds.softmax(1).to(\"cpu\").numpy().argmax(1))\n            del loss\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n                print('EVAL: [{0}\/{1}] '\n                    'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                    'Elapsed {remain:s} '\n                    'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                    .format(\n                    step, len(valid_loader), batch_time=batch_time,\n                    data_time=data_time, loss=losses,\n                    remain=timeSince(start, float(step+1)\/len(valid_loader)),\n                    ))\n        del y_preds, images, labels, batch_size\n    del batch_time, data_time, scores, model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    preds = np.concatenate(preds)\n\n    return losses.avg, preds","17706720":"DATA_PATH = \"..\/input\/animal-imbalance-classification\/\"\n\ntrain_imgs = sorted(glob(os.path.join(DATA_PATH, 'train_images\/*.png')))\ntest_imgs = sorted(glob(os.path.join(DATA_PATH, 'test_images\/*.png')))\n\ntrain_img_df = pd.DataFrame(train_imgs, columns=['img_path'])\ntest_img_df = pd.DataFrame(test_imgs, columns=['img_path'])\n\ntrain_img_df['image_id'] = [os.path.basename(img) for img in train_imgs] \ntrain_label_df = pd.read_csv('..\/input\/animal-imbalance-classification\/train.csv')\ntrain_df = pd.merge(train_img_df, train_label_df, on='image_id', how='left')\nX_train = np.array([np.array(Image.open(img)) for img in train_df['img_path']])","f2be23fe":"test_img_df['image_id'] =  [os.path.basename(img) for img in test_imgs]\ntest_img_df","a72fe9bc":"from sklearn.linear_model import LogisticRegression\nfrom imblearn.under_sampling import InstanceHardnessThreshold, NeighbourhoodCleaningRule\n\n\niht = NeighbourhoodCleaningRule(\n    #random_state=0, cv=3, \n    n_jobs=-1,\n    sampling_strategy=[0,1,3,4,5,7,8],\n    #{0:500,1:500,2:500,3:500,4:500,5:500,6:500,7:500,8:500,9:500}\n    )\n    \nX_train_res, y_train_res = iht.fit_resample(X_train.reshape(X_train.shape[0], -1), train_df['label'])","45e56dcc":"#func = np.vectorize(lambda x: np.array(Image.open(x)), otypes=[np.array()])\n#X_train = fun(train_df['img_path'])\nX_train = np.array([np.array(Image.open(img)) for img in train_df['img_path']])\nsmote = SMOTE(sampling_strategy={0:5000,1:5000,2:5000,3:5000,4:5000,5:5000,6:5000,7:5000,8:5000,9:5000}, random_state=71)\nX_train_res, y_train_res = smote.fit_resample(X_train.reshape(X_train.shape[0], -1), train_df['label'])","1320750c":"train_aug_df = pd.DataFrame([list(img.reshape(1, -1)) for img in X_train_res], columns=['img'])\ntrain_aug_df['label'] = y_train_res\ntrain_aug_df[:5]\n#train_aug_df.to_csv('.\/train_aug.csv', index=False)","22cce2e3":"#columns = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\ntarget = train_aug_df['label'].value_counts()\nfig = px.pie(target, values='label', names=target.index, hole=.4, width=500, height=500)\nfig.update_traces(textinfo='value+label', pull=0.01)\nfig.show() #2:bird 6:frog 9:truck","7f9b2308":"fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=71)\ncv = list(fold.split(train_aug_df, train_aug_df['label']))\ncv","98fff426":"#1:automobile,8:ship\u306emisslabel\u304c\u591a\u305d\u3046\n#2:'bird',6:'frog',9:'truck'\u304c500\u500b\nipyplot.plot_class_tabs(train_df['img_path'], train_df['label'], max_imgs_per_tab=10, img_width=100, force_b64=True)","63413091":"class EarlyStopping:\n    def __init__(self, patience=10, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n        \n    def __call__(self, val_loss, model):\n\n        score = val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            #self.save_checkpoint(val_loss, model)\n            \n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            #self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","cde1ad7a":"def linear_combination(x, y, epsilon):\n    return (1 - epsilon) * x + epsilon * y\n\ndef reduce_loss(loss, reduction='mean'):\n    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss","fc567886":"class LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, epsilon=0.1, reduction='mean'):\n        super().__init__()\n        self.epsilon = epsilon\n        self.reduction = reduction\n\n    def forward(self, preds, target):\n        n = preds.size()[-1]\n        log_preds = F.log_softmax(preds, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n        return linear_combination(nll, loss\/n, self.epsilon)","d0571c2e":"class FocalLoss(nn.Module):\n\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n\n    def forward(self, input, target):\n        logp = self.ce(input, target)\n        p = torch.exp(-logp)\n        loss = (1 - p) ** self.gamma * logp\n        return loss.mean()","9b7093a7":"def train_loop(train, fold, class_num=None, norms=(0.5, 0.5), ssl_model=True, ssl_train=False, freeze=False):\n    #\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5f62\u304c\u56fa\u5b9a\u306e\u3068\u304d\u3001GPU\u5074\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8a08\u7b97\u3092\u6700\u9069\u5316\u3057\u9ad8\u901f\u5316\u304c\u671f\u5f85\n    #\u901a\u5e38\u306eCNN\u306e\u3088\u3046\u306b\u30c7\u30fc\u30bf\u5165\u529b\u30b5\u30a4\u30ba\u304c\u6700\u521d\u3084\u9014\u4e2d\u3067\u5909\u5316\u3057\u306a\u3044\u5834\u5408\u306b\u6709\u52b9\n    #\u305f\u3060\u3057\u3001\u8a08\u7b97\u306e\u518d\u73fe\u6027\u304c\u306a\u304f\u306a\u308b\u306e\u3067\u6ce8\u610f\n    base = BaseModel(CFG.MODEL_NAME, class_num)\n    n_features = base.model.num_features #backbone.num_features #self.model.fc.in_features\n    \n    if ssl_model:\n        backbone = nn.Sequential(*list(base.model.children())[:-1])\n        model = self_supervised_learning(backbone, n_features, ssl_train=ssl_train)\n    \n    for i, (train_index, valid_index) in enumerate(fold):\n        if i ==0:\n            continue\n        train_dataset = TrainDataset(train.loc[train_index].reset_index(drop=True), \n                                     transform=get_transforms(data='train', size=CFG.size, norms=norms)) # \u5b66\u7fd2\u7528\u306edataset\u3092\u4f5c\u6210\n        valid_dataset = TrainDataset(train.loc[valid_index].reset_index(drop=True), \n                                     transform=get_transforms(data='valid', size=CFG.size, norms=norms)) # \u8a55\u4fa1\u7528\u306edataset\u3092\u4f5c\u6210\n        \n        #pin_memory=True->CPU\u306e\u30e1\u30e2\u30ea\u9818\u57df\u304c\u30da\u30fc\u30b7\u30f3\u30b0\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u306a\u308a\u3001\u9ad8\u901f\u5316\u304c\u671f\u5f85\n        #num_workers=2 \u30df\u30cb\u30d0\u30c3\u30c1\u306e\u53d6\u308a\u51fa\u3057\u65b9\u304cSingle-process\u304b\u3089Malti-process\u306b\u306a\u308a\u3001\u9ad8\u901f\u5316\u304c\u671f\u5f85\n        train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, \n                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)# \u5b66\u7fd2\u7528\u306edatasets\u306ebatch\u3092\u4f5c\u6210\n        valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, \n                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False) # \u8a55\u4fa1\u7528\u306edatasets\u306ebatch\u3092\u4f5c\u6210\n        \n        if ssl_model:           \n            model.load_state_dict(torch.load(f'..\/input\/models\/ssl_100_resnet18d_V1_100.pth'))#, map_location=DEVICE)\n            print('********SSL_model Loaded*********')\n            #get_scatter_plot_with_thumbnails()\n            #model.classifier = nn.Linear(n_features, n_class) # \u51fa\u529b\u5c64\u306e\u6b21\u5143\u3092\u30af\u30e9\u30b9\u6570\u306b\u5909\u66f4\n            model = model.backbone\n            model.add_module('flatten', nn.Flatten())\n            model.add_module('fc', nn.Linear(in_features=n_features, out_features=class_num))\n        else:\n            model= BaseModel(CFG.MODEL_NAME, class_num)\n            print('********Scratch_model*********')\n        \n        # \u6700\u7d42\u5c64\u4ee5\u5916\u306e\u91cd\u307f\u56fa\u5b9a\n        if freeze:\n            #model.freeze()\n        \n            for param in model.parameters():\n                param.requires_grad = False\n\n            # To unfreeze the last layer\n            for param in model.fc.parameters():#classifier\n                param.requires_grad = True\n            \n        model.to(CFG.device)\n        \n        # Adam \u306f\u52fe\u914d\u3092\u79fb\u52d5\u5e73\u5747\u3068\u5b66\u7fd2\u7387\u306e\u8abf\u6574\u304b\u3089\u306a\u308a\u307e\u3059 (Momentum\u3068RMSProp)\n        optimizer = Adam(model.parameters(), lr=CFG.model_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        #criterion = nn.CrossEntropyLoss() # loss function\n        #criterion = LabelSmoothingCrossEntropy()\n        criterion = FocalLoss()\n        \n        val_loss = []\n        best_score = 0.\n        best_loss = np.inf\n        early_stopping = EarlyStopping(patience=20, verbose=True)\n        \n        for epoch in tqdm(range(CFG.epochs)):\n            start_time = time.time()\n            avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler) # \u5b66\u7fd2\n            avg_val_loss, predict = valid_fn(valid_loader, model, criterion) # \u8a55\u4fa1\u7528\u306e\u63a8\u8ad6\n            val_loss.append(avg_val_loss)\n            valid_labels = train.loc[valid_index, \"label\"].values # \u4e88\u6e2c\u30e9\u30d9\u30eb\n\n            scheduler.step() \n            macro_f1_score = f1_score(valid_labels, predict, average='macro')\n\n            elapsed = time.time() - start_time\n            LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n            LOGGER.info(f'Epoch {epoch+1} - macro_f1: {macro_f1_score}')\n            \n            early_stopping(macro_f1_score, model) # \u6700\u826f\u30e2\u30c7\u30eb\u306a\u3089\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u4fdd\u5b58\n            if early_stopping.early_stop: \n                break\n                \n            if macro_f1_score > best_score:\n                best_score = macro_f1_score\n                LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n                torch.save(model.state_dict(), f'.\/{CFG.MODEL_NAME}_fold{i}_best_{CFG.Version}.pth') # \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\n                print(f'save: .\/{CFG.MODEL_NAME}_fold{i}_best_{CFG.Version}.pth')\n                valid_folds_predict = predict\n\n            del avg_loss, avg_val_loss, valid_labels, predict, macro_f1_score\n            gc.collect()\n            torch.cuda.empty_cache()\n        \n        #break\n        \n    return valid_folds_predict, val_loss","a51d2d34":"#\u3000koko\n#ModuleAttributeError: 'Sequential' object has no attribute 'freeze'\npreds, val_loss = train_loop(train_aug_df, fold=cv, class_num=CFG.class_num, \n                             ssl_model=True, ssl_train=False,freeze=False)","96194151":"def get_scatter_plot_with_thumbnails():\n    \"\"\"Creates a scatter plot with image overlays\n    \"\"\"\n    # for plotting\n    import os\n    from PIL import Image\n\n    import matplotlib.pyplot as plt\n    import matplotlib.offsetbox as osb\n    from matplotlib import rcParams as rcp\n\n    # for resizing images to thumbnails\n    import torchvision.transforms.functional as functional\n\n    # for clustering and 2d representations\n    from sklearn import random_projection\n    # for the scatter plot we want to transform the images to a two-dimensional\n    # vector space using a random Gaussian projection\n    projection = random_projection.GaussianRandomProjection(n_components=2)\n    embeddings_2d = projection.fit_transform(embeddings)\n\n    # normalize the embeddings to fit in the [0, 1] square\n    M = np.max(embeddings_2d, axis=0)\n    m = np.min(embeddings_2d, axis=0)\n    embeddings_2d = (embeddings_2d - m) \/ (M - m)\n    \n    # initialize empty figure and add subplot\n    fig = plt.figure(figsize=(12,12))\n    fig.suptitle('SimSiam Scatter Plot')\n    ax = fig.add_subplot(1, 1, 1)\n    # shuffle images and find out which images to show\n    shown_images_idx = []\n    shown_images = np.array([[1., 1.]])\n    iterator = [i for i in range(embeddings_2d.shape[0])]\n    np.random.shuffle(iterator)\n    for i in iterator:\n        # only show image if it is sufficiently far away from the others\n        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n        if np.min(dist) < 1.5e-3:\n            continue\n        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n        shown_images_idx.append(i)\n\n    # plot image overlays\n    for idx in shown_images_idx:\n        thumbnail_size = int(rcp['figure.figsize'][0] * 5.)\n        path = os.path.join(path_to_data, filenames[idx])\n        img = Image.open(path)\n        img = functional.resize(img, thumbnail_size)\n        img = np.array(img)\n        img_box = osb.AnnotationBbox(\n            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n            embeddings_2d[idx],\n            pad=0.2,\n        )\n        ax.add_artist(img_box)\n\n    # set aspect ratio\n    ratio = 1. \/ ax.get_data_ratio()\n    ax.set_aspect(ratio, adjustable='box')\n\n\n# get a scatter plot with thumbnail overlays\n#get_scatter_plot_with_thumbnails()","5199bc79":"def get_image_as_np_array(filename: str):\n    \"\"\"Loads the image with filename and returns it as a numpy array.\n\n    \"\"\"\n    img = Image.open(filename)\n    return np.asarray(img)[...,:3]\n\n\ndef get_image_as_np_array_with_frame(filename: str, w: int = 5):\n    \"\"\"Returns an image as a numpy array with a black frame of width w.\n\n    \"\"\"\n    img = get_image_as_np_array(filename)\n    ny, nx, _ = img.shape\n    # create an empty image with padding for the frame\n    framed_img = np.zeros((w + ny + w, w + nx + w, 3))\n    framed_img = framed_img.astype(np.uint8)\n    # put the original image in the middle of the new one\n    framed_img[w:-w, w:-w] = img\n    return framed_img\n\n\ndef plot_nearest_neighbors_3x3(example_image: str, i: int):\n    \"\"\"Plots the example image and its eight nearest neighbors.\n\n    \"\"\"\n    n_subplots = 9\n    # initialize empty figure\n    fig = plt.figure()\n    fig.suptitle(f\"Nearest Neighbor Plot {i + 1}\")\n    \n    example_idx = filenames.index(example_image)\n    # get distances to the cluster center\n    distances = embeddings - embeddings[example_idx]\n    distances = np.power(distances, 2).sum(-1).squeeze()\n    # sort indices by distance to the center\n    nearest_neighbors = np.argsort(distances)[:n_subplots]\n    # show images\n    for plot_offset, plot_idx in enumerate(nearest_neighbors):\n        ax = fig.add_subplot(3, 3, plot_offset + 1)\n        # get the corresponding filename\n        fname = os.path.join(path_to_data, filenames[plot_idx])\n        if plot_offset == 0:\n            ax.set_title(f\"Example Image\")\n            plt.imshow(get_image_as_np_array_with_frame(fname))\n        else:\n            plt.imshow(get_image_as_np_array(fname))\n        # let's disable the axis\n        plt.axis(\"off\")","ad16295e":"# show example images for each cluster\nexample_images = [\n    'image_0.png',\n    'image_1.png',\n    'image_2.png',\n    'image_3.png',\n    'image_4.png',\n    'image_5.png',\n]\n\nfor i, example_image in enumerate(example_images):\n    plot_nearest_neighbors_3x3(example_image, i)","c877983a":"for (train_index, valid_index) in cv:\n    print(classification_report(train_aug_df.iloc[valid_index]['label'].values, preds))\n    break","b29de290":"def visualize_confusion_matrix(y_true,\n                               pred_label,\n                               ax: Optional[plt.Axes] = None,\n                               labels: Optional[list] = None,\n                               conf_options: Optional[dict] = None,\n                               plot_options: Optional[dict] = None) -> Tuple[plt.Axes, np.ndarray]:\n\n    _conf_options = {\n        \"normalize\": \"true\",\n    }\n    if conf_options is not None:\n        _conf_options.update(conf_options)\n\n    _plot_options = {\n        \"cmap\": \"Blues\",\n        \"annot\": True\n    }\n    if plot_options is not None:\n        _plot_options.update(plot_options)\n\n    conf = confusion_matrix(y_true=y_true,\n                            y_pred=pred_label,\n                            **_conf_options)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(conf, ax=ax, **_plot_options)\n    ax.set_ylabel(\"Label\")\n    ax.set_xlabel(\"Predict\")\n\n    if labels is not None:\n        ax.set_yticklabels(labels)\n        ax.set_xticklabels(labels)\n        ax.tick_params(\"y\", labelrotation=0)\n        ax.tick_params(\"x\", labelrotation=90)\n\n    return ax, conf","1896d037":"for (train_index, valid_index) in cv:\n    visualize_confusion_matrix(train_aug_df.iloc[valid_index]['label'].values, preds, conf_options={ \"normalize\": None }, plot_options={ \"fmt\": \"4d\" });\n    break","31d766e5":"from pylab import rcParams\n\n#dataset_show = TrainDataset(train_aug_df.sample(1000))\ndataset_show = TestDataset(test_img_df.sample(1000))\n\nrcParams['figure.figsize'] = 20,10\n\nfor i in range(10):\n    f, axarr = plt.subplots(1,5)\n    for p in range(5):\n        idx = np.random.randint(0, len(dataset_show))\n        #img, label = dataset_show[idx]\n        img = dataset_show[idx]\n        #axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())\n        axarr[p].imshow(img)\n        axarr[p].set_title(str(label))","042da2d1":"def inference(test, class_num=10, n=0, norms=(0.5, 0.5), tta=False):\n    \n    #base = BaseModel(CFG.MODEL_NAME, class_num, pretrained=False)\n    model = BaseModel(CFG.MODEL_NAME, class_num, pretrained=False)\n    \"\"\"\n    if CFG.MODEL_NAME =='visformer_tiny':\n        model = base\n    else:        \n        n_features = base.model.num_features #backbone.num_features #self.model.fc.in_features\n        backbone = nn.Sequential(*list(base.model.children())[:-1])\n        model = backbone\n        model.add_module('flatten', nn.Flatten())\n        model.add_module('fc', nn.Linear(in_features=n_features, out_features=class_num))\n    \"\"\"\n    model.load_state_dict(torch.load(f'{MODEL_PATH}')) # \u91cd\u307f\u306e\u8aad\u307f\u8fbc\u307f\n    model.to(CFG.device)\n    \n    test_dataset = TestDataset(test, transform=get_transforms(data='valid', size=CFG.size, norms=norms))\n    test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n    \n    all_predicts = []\n    model.eval() # \u63a8\u8ad6\u30e2\u30fc\u30c9\n    \n    if tta:\n        # \u63a8\u8ad6\u6642\u306b\u884c\u3046Augmentation\u5b9a\u7fa9\n        tta_transforms = ttach.Compose([\n            ttach.HorizontalFlip(),\n            ttach.VerticalFlip(),\n            ])\n        \n        model = ttach.ClassificationTTAWrapper(model, tta_transforms)\n\n    with torch.no_grad(): # \u52fe\u914d\u306e\u8a08\u7b97\u3092\u3057\u306a\u3044\n        for i, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n            images = images.to(CFG.device)\n            y_preds = model(images)\n                \n            #predicts = y_preds.softmax(1).to(\"cpu\").numpy().argmax(1)\n            predicts = y_preds.softmax(1).to(\"cpu\").numpy()\n            \n            all_predicts.append(predicts)\n    \n    predicts = np.concatenate(all_predicts)\n        \n    return predicts","65a75e77":"# model_5fold\n#CFG.MODEL_NAME = 'efficientnet_b0'\nCFG.MODEL_NAME = 'resnet18d'\nmodel_list = [\n'..\/input\/models\/resnet18d_fold0_best_V2.pth',\n#'..\/input\/models\/resnet18d_fold1_best_V2.pth',\n#'..\/input\/models\/resnet18d_fold2_best_V2.pth',\n#'..\/input\/models\/resnet18d_fold3_best_V1.pth',\n#'..\/input\/models\/resnet18d_fold4_best_V1.pth',\n#'..\/input\/models\/efficientnet_b0_fold0_best_V8.pth'\n    \n]\n\nsubmit = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))\n\npredicts = []\nfor path in model_list:\n    MODEL_PATH = path\n    predict = inference(submit, class_num=CFG.class_num, n=0, tta=False)\n    predicts.append(predict)","08985675":"# \u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\nmodel_name_list = [\n 'visformer_tiny',\n 'efficientnet_b0',\n 'resnet18d',\n]\nmodel_list = [\n    '..\/input\/models\/visformer_tiny_fold0_best_V10.pth',\n    '..\/input\/models\/efficientnet_b0_fold0_best_V10.pth',\n    '..\/input\/models\/resnet18d_fold0_best_V1.pth',\n]\n\n\npredicts = []\nfor (train_index, valid_index) in cv:\n    for path, model_name in zip(model_list, model_name_list):\n        MODEL_PATH = path\n        CFG.MODEL_NAME = model_name\n        predict= inference(train_aug_df.iloc[valid_index], class_num=CFG.class_num, n=0, tta=False)\n        predicts.append(predict)\n    break","07bf9e8f":"import scipy.stats as stats\n\nmode_val, mode_num = stats.mode(predicts, axis=0)\nprint(mode_val)","372efd13":"test_img_df['label']","d5828847":"#0.80\nsns.histplot(data=pd.DataFrame(predicts[0].argmax(1), columns=['pred']));","2f8bfefc":"vc = train_aug_df['label'].value_counts()\nvc","19634c6c":"def min_max_p(p):\n  #\u6700\u5c0f\u5024\u306e\u8a08\u7b97\n  mean = p.mean(axis=0)\n  #\u6700\u5927\u5024\u306e\u8a08\u7b97\n  std = p.std(axis=0)\n  #\u6b63\u898f\u5316\u306e\u8a08\u7b97\n  min_max_p = (p - mean) \/ std\n  return min_max_p\n\ndef fix_proba(df, columns):\n    for col in columns:\n        if col ==\"bird\" or col ==\"frog\" or col ==\"truck\":\n            df[col] = df[col] * 4.9\n    return df\n\ncolumns = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n#columns = range(10)\n#tmp = pd.DataFrame(predicts[0], columns=columns)\ntmp = pd.read_csv(\"..\/input\/models\/predict_0.csv\")\n#norm_df = min_max_p(tmp)\nnorm_df = fix_proba(tmp,columns)\n\nnorm_df = norm_df.rename(columns={\"airplane\":0,\"automobile\":1,\"bird\":2,\"cat\":3,\"deer\":4,\"dog\":5,\"frog\":6,\"horse\":7,\"ship\":8,\"truck\":9})","dd633350":"sns.histplot(data=norm_df[range(10)].idxmax(1));","0c981fa3":"col_name = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\npreprocess_df = pd.DataFrame(predicts[0], columns=col_name)\n#preprocess_df.to_csv('.\/predict_0.csv')\npreprocess_df ##2:bird 6:frog 9:truck","438d6a97":"col = 'bird'\nids = preprocess_df[preprocess_df[col]>0.20][col].sort_values(ascending=True).index\nplot_img = np.array([f'..\/input\/animal-imbalance-classification\/test_images\/image_{id}.png' for id in ids])\npreprocess_df[preprocess_df[col]>0.20][col].sort_values(ascending=True)","b746f6d6":"ipyplot.plot_images(plot_img, force_b64=True, max_images=100)","81b6b981":"#test_img_df['label'] = predicts[0]\nsubmit['label'] = norm_df[range(10)].idxmax(1)\n#submit['label'] = predicts[0].argmax(1)\n#test_img_df['label'] = mode_val.flatten() #\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\n#submit = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n#submit = pd.merge(submit['image_id'], test_img_df.drop('img_path',axis=1), on='image_id', how='left')\n#submit.to_csv('030_submission.csv', index=False) # csv\u3092\u66f8\u304d\u51fa\u3059","048a439b":"public_df = pd.read_csv('..\/input\/models\/public_test.csv')\nprivate_df = pd.read_csv('..\/input\/models\/private_test.csv')\ntrue_df = pd.concat([public_df, private_df], axis=0).reset_index()","adffed07":"print(classification_report(true_df['label'].values,submit['label'].values,))","22a3529b":"for (train_index, valid_index) in cv:\n    visualize_confusion_matrix(true_df['label'].values,submit['label'].values,conf_options={ \"normalize\": None }, plot_options={ \"fmt\": \"4d\" });\n    break\nprint(f1_score(submit['label'].values, true_df['label'].values, average='macro'))","d995e6b2":"#efficientnet_b0_fold0_best_V10.pth:014\n#visformer_tiny_fold0_best_V1.pth:015\n#visformer_tiny_fold1_best_V1.pth:016\n#visformer_tiny_fold2_best_V1.pth:017\n#visformer_tiny_fold3_best_V1.pth:018\n#visformer_tiny_fold4_best_V1.pth:019\n#visformer_tiny \u591a\u6570\u6c7a_V1.pth:020\n#resnet18d_fold0_best_V1.pth:021\n#resnet18d_fold1_best_V1.pth:022\n#resnet18d_fold2_best_V1.pth:023\n#resnet18d_fold3_best_V1.pth:024\n#resnet18d_fold0~2 \uff13\u3064_V1.pth:025\n\n#resnet18d_fold0_best_V3 channnelshift.pth:026\n#resnet18d_fold0_best_V2 gray.pth:027\n#resnet18d_fold0_best_V3 channnelshift 10\u591a\u6570\u6c7a.pth:028\n#resnet18d_best_V1 ssl 5\u591a\u6570\u6c7a.pth:029\n#resnet34d_fold4_best_V1 .pth:030","b591acb9":"submit['label'][:20]","26892bf0":"from joblib import Parallel, delayed\nimport itertools\nfrom pulp import *","636ae631":"def postprocess(args, i):\n    \n    problem = LpProblem('ice', LpMaximize)\n\n    # \u6c7a\u5b9a\u5909\u6570\u3092\u5b9a\u7fa9\n    x_num =  [LpVariable(f'x_num_{i}', lowBound=0) for i in range(10)]\n    x_prob_max = LpVariable('x_prob__max', lowBound=0, upBound=1)\n    \n    x_prob_max = 0\n    label = 0\n    pred = []\n    for i, prob in enumerate(args, -1):\n        if x_prob_max < prob:\n            x_prob_max = prob\n            label = i\n\n    pred.append(label)\n    \n    # \u76ee\u7684\u95a2\u6570\u3092\u8a2d\u5b9a\n    problem += x_prob_max\n    \n    # \u5236\u7d04\u3092\u8a2d\u5b9a\n    problem += x_num[0] <= 1100\n    problem += x_num[1] <= 1100\n    problem += x_num[2] >=  600\n    problem += x_num[3] <= 1100\n    problem += x_num[4] <= 1100\n    problem += x_num[5] <= 1100\n    problem += x_num[6] >= 850\n    problem += x_num[7] <= 1100\n    problem += x_num[8] <= 1100\n    problem += x_num[9] >= 850\n\n    # \u6700\u9069\u5316\n    problem.solve()\n\n    # \u7d50\u679c\u306e\u8868\u793a\n    print(x_num[0],\n          x_num[1].value(),\n          x_num[2].value(),\n          x_num[3].value(),\n          x_num[4].value(),\n          x_num[5].value(),\n          x_num[6].value(),\n          x_num[7].value(),\n          x_num[8].value(),\n          x_num[9].value(),\n         )","93cbb7e8":"# \u5b9f\u884c\n#prob_mat_df = pd.DataFrame(oof)\n#prob_mat_df['bcid'] = train['bcid']\nprob_mat_df = preprocess_df.copy()\nprocessed = Parallel(n_jobs=-1, prefer='threads')([delayed(postprocess)(args, i) for i, args in enumerate(prob_mat_df.itertuples())])\nprocessed.sort(key=lambda x: x[1])\nprocessed_data = [t[0] for t in processed]\npred = np.array(list(itertools.chain.from_iterable(processed_data)))","f343d2e8":"before_score = f1_score(train['class_id'], np.argmax(oof.drop(columns=['bcid']).values, axis=1), average='macro')\nafter_score = f1_score(train['class_id'], pred, average='macro')\nprint(f'oof_score: {before_score} -> {after_score}')","663dfd53":"pd.read_csv()","543fa351":"# Dataset","44f23e70":"# train & valid function","d0af6868":"## Augmentation\u7d50\u679c\u78ba\u8a8d","f1f5e8a0":"### train, valid\u5206\u5272","e463986f":"# Model","eaa0818f":"# \u524d\u51e6\u7406","66999101":"#### \u753b\u50cf\u30b5\u30a4\u30ba\u306e\u78ba\u8a8d\u3068\u6570\u4f8b\u3092\u8868\u793a","5322d4a2":"# Utils","d9eeccb1":"{'0': 'airplane',\n '1': 'automobile',\n '2': 'bird',\n '3': 'cat',\n '4': 'deer',\n '5': 'dog',\n '6': 'frog',\n '7': 'horse',\n '8': 'ship',\n '9': 'truck'}","da0e1b21":"# \u5b66\u7fd2","16a2b6fb":"visformer_tiny_fold0_best_V1:0.7413\u21d2002_sub:0.5523  model_lr=1e-4 min_lr=1e-5 weight_decay=1e-5\n\nvisformer_tiny_fold0_best_V2:0.5643\u21d2003_sub: 0.3899train_augmentation\n\nefficientnet_b0_fold0_best_V1:0.6450\u21d2004_sub:0.5091  efficientnet_b0 \u5b66\u7fd2\u56de\u6570\u5897\u3084\u3057\u3066\u3082\u305d\u3053\u307e\u3067\u7cbe\u5ea6\u3042\u304c\u3089\u306a\u305d\u3046\n\nefficientnet_b0_fold0_best_V2:0.5253\u21d2005_sub:0.3768  train_augmentation \u307c\u304b\u3057\u30fb\u30ce\u30a4\u30ba\u7cfb\u9664\u3044\u305f\n\nefficientnet_b0_fold0_best_V3:0.4477\u21d2006_sub: 0.3301 train_augmentation+SSL(size32bat126epo10)+size(32) \u307c\u304b\u3057\u30fb\u30ce\u30a4\u30ba\u7cfb\u9664\u3044\u305f\n\nefficientnet_b0_fold0_best_V4:0.6975\u21d2007_sub:0.5463  train_augmentation+SSL(size32bat512_0.14epo50)+size(224) \u307c\u304b\u3057\u30fb\u30ce\u30a4\u30ba\u7cfb\u9664\u3044\u305f\n\u2605ssl_epoch100 0.14\u3088\u308a\u4e0b\u304c\u308b\u304b\u3082 epoch\u5897\u3084\u305b\u307e\u307e\u3060\u307e\u3060\u7cbe\u5ea6\u3042\u304c\u308a\u305d\u3046\n\u2605Vit\u3067\u3084\u308b\n\nefficientnet_b0_fold0_best_V5:0.64415\u21d2008_sub: train_augmentation+Modelsize224bat128\u2605epo10\n\nefficientnet_b0_fold0_best_V6:0.7805\u21d2009_sub: train_augmentation+Modelsize224bat128\u2605epo100_43\n\nefficientnet_b0_fold0_best_V7:0.6372\u21d2\u00d7: train_augmentation+SSL0.03(size32bat512epo200)+Modelsize32bat512\u2605epo10 \u307c\u304b\u3057\u30fb\u30ce\u30a4\u30ba\u7cfb\u9664\u3044\u305f\n\u2605\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u304c\u5927\u304d\u3044\u306e\u3067T_max=20\u3078\uff01Label_smoothing=0.1 earlystopping=10\u21d220\n\nefficientnet_b0_fold0_best_V8:0.8553\u21d2011_sub: train_augmentation+SSL0.03(size32bat512epo200)+Modelsize32bat512\u2605epo100_62 \u307c\u304b\u3057\u30fb\u30ce\u30a4\u30ba\u7cfb\u9664\u3044\u305f\n\n0.8559 0.8466\n\nadversarial\nverticalflip\u6d88\u3059\n\ntrain\u306b\u30ab\u30e9\u30fc\u30ce\u30a4\u30ba\u304c\u5165\u3063\u3066\u3044\u308b\uff01\n\n\u82e6\u624b\u30bf\u30a4\u30d7\u306f\uff1f\nCNN\u3068transformer\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\nfinetuning\u6642\u306b\u91cd\u307f\u3092\u30d5\u30ea\u30fc\u30ba\u3059\u308b\uff1f\nhttps:\/\/github.com\/qubvel\/ttach\n\n\u30fb\u81ea\u5df1\u6559\u5e2b\u6709\u5b66\u7fd2\u306e\u89e3\u8aac\nhttps:\/\/speakerdeck.com\/sansandsoc\/simsiam-exploring-simple-siamese-representation-learning?slide=21\nhttps:\/\/www.guruguru.science\/competitions\/17\/discussions\/c068adce-7425-4876-a3b5-255725b9e03c\/\nDINO+vit_small \u306f\u91cd\u307f\u51cd\u7d50\u3057\u3066\u8ffd\u52a0\u3057\u305f3\u5c64\u306eMLP\u3060\u3051\u3092\u5b66\u7fd2\u3059\u308b\u3068\u3046\u307e\u304f\u3044\u3063\u305f\u3002\u91cd\u307f\u51cd\u7d50\u3057\u306a\u3044\u3068\u6027\u80fd\u5168\u7136\u6539\u5584\u3057\u306a\u3044\nDINO\u306epretrain\u306fepoch300\u4ee5\u4e0a\u56de\u3055\u306a\u3044\u3068\u5287\u7684\u306a\u6539\u5584\u306f\u3057\u306a\u3044\nSimSiam \u3092\u4f7f\u3063\u3066\u308b\u304c\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\u3002\u30b9\u30af\u30e9\u30c3\u30c1\u306eresnet34d\u3068\u6027\u80fd\u5909\u308f\u3089\u306a\u3044\u306f\u305a\n\nSelf-Supervised Learning \u3067\u4e8b\u524d\u5b66\u7fd2\u3059\u308b\u3068\u306f\u3044\u3048\u3001 scratch \u304b\u3089ViT\u7cfb\u3084\u308b\u306e\u306f\u30c7\u30fc\u30bf\u304c\u5927\u91cf\u306b\u306a\u3044\u3068\u304d\u3064\u3044\u304b\n\u5927\u91cf\u306e\u30c7\u30fc\u30bf\u304c\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u3068\u3044\u3046\u306e\u306f\uff0cimagenet\u306e\u3088\u3046\u306a\u5e45\u5e83\u3044\u753b\u50cf\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u304c\u3042\u308b\u3088\u3046\u306a\u72b6\u6cc1\u3067\n\u4eca\u56de\u306e\u3088\u3046\u306a\u30c9\u30e1\u30a4\u30f3\u304c\u56fa\u5b9a\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u3060\u3068ViT\u7cfb\u3067\u3082\u554f\u984c\u306a\u3044\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\n\n\u3061\u306a\u307f\u306b2\u5c64\u306e\u7dda\u5f62\u30e2\u30c7\u30eb\u3067\u3082\nCV0.6939 LB0.6909\u306b\u306a\u3063\u305f\u306e\u30673MLP\u306e\u52b9\u679c\u3088\u308a\u3082ViT\u306e\u4e8b\u524d\u5b66\u7fd2\u304c\u3081\u3061\u3083\u304f\u3061\u3083\u52b9\u3044\u3066\u3044\u308b\u611f\u3058\u304c\u3057\u307e\u3059(\u5f53\u305f\u308a\u524d\u3067\u3059\u304c\n\n\u904e\u5b66\u7fd2\u5bfe\u7b56\n\u4eca\u56de\u306e\u3088\u3046\u306b\u9ad8\u30054000\u679a\u306e\u753b\u50cf\u30670\u304b\u3089\u5b66\u7fd2\u3059\u308b\u5834\u5408\u3001resnet\u306e\u3088\u3046\u306adeep network\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u304c\u6570\u767e\u4e07\u301c\u6570\u5343\u4e07\u500b\u3042\u308b\u306e\u3067\u3001\u7c21\u5358\u306b\u904e\u5b66\u7fd2\uff08overfitting\uff09\u3092\u8d77\u3053\u3057\u3066\u3057\u307e\u3044\u307e\u3059\u3002\u30e2\u30c7\u30eb\u306e\u6c4e\u5316\u6027\u80fd\uff08\uff1d\u672a\u77e5\u306e\u30c7\u30fc\u30bf\u306b\u5f37\u304f\u3059\u308b\uff09\u3092\u6539\u5584\u3059\u308b\u305f\u3081\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u5f31\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u304c\u3064\u304b\u3048\u308b\u3002\nhttps:\/\/www.guruguru.science\/competitions\/17\/discussions\/a39d588e-aff2-4728-8323-b07f15563552\/","258fb8a3":"# pulp\u306b\u3088\u308b\u5f8c\u51e6\u7406","cc5a459c":"Epoch 58 - avg_train_loss: 1.0140  avg_val_loss: 0.9204  time: 11s\nEpoch 58 - macro_f1: 0.8322873755336369\nEpoch 58 - Save Best Score: 0.8323 Model","24a2e23c":"## \u81ea\u5df1\u6559\u5e2b\u6709\u5b66\u7fd2\u7d50\u679c\u306e\u53ef\u8996\u5316","7f82d049":"# Config\n","b3124af8":"# install library","0152c5bf":"# Data\u306e\u78ba\u8a8d","c64063ab":"# \u63a8\u8ad6","6490806b":"# \u5b66\u7fd2\u7d50\u679c\u306e\u53ef\u8996\u5316","8b8ed69e":"# Submission"}}