{"cell_type":{"9779a11d":"code","f28a1772":"code","e4e4adc4":"code","18e91e54":"code","9419e87d":"code","b3cafd39":"code","6a69d4ee":"code","c11f6be3":"code","1e00e946":"code","109b6cac":"code","30b14596":"code","f9a2e72e":"code","af896767":"code","6055bebd":"code","b5e47b62":"markdown","d60acaad":"markdown","c137bec5":"markdown","89c4db40":"markdown","818e548d":"markdown","b99571c1":"markdown","f05036e0":"markdown","0bb872b5":"markdown","c44b7c5d":"markdown","cf92891d":"markdown"},"source":{"9779a11d":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, RocCurveDisplay\nfrom sklearn.model_selection import GridSearchCV\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_rows', 300)","f28a1772":"PATH = '..\/input\/catalanmushrooms'\ntrain = pd.read_csv(PATH+'\/train.csv', index_col=\"Id\")\ntest = pd.read_csv(PATH+'\/test.csv', index_col=\"Id\")\n\ntrain.drop(columns=['bruises'], axis=1, inplace=True)\ntest.drop(columns=['bruises'], axis=1, inplace=True)\n\nX = train.drop(columns=['poisonous'])\nX = X[test.columns]\ny = train['poisonous']\nsample_results = pd.read_csv(PATH+'\/sample_results.csv')\n\n# The only missing values are in stalk.root as ? values. How should we handle these?\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)","e4e4adc4":"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nX_train_enc = enc.fit_transform(X_train)\nX_test_enc = enc.transform(X_test)","18e91e54":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(random_state=0)\nlr_model.fit(X_train_enc, y_train)\n\ny_pred_lr = lr_model.predict(X_test_enc)\ncm = confusion_matrix(y_test, y_pred_lr)\n\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_lr)\nroc_display = RocCurveDisplay(fpr=fpr_lr, tpr=tpr_lr).plot()\n\nthresholds_lr","9419e87d":"# specify range of hyperparameters to tune\nhyper_params = {\n    'penalty': ['l1', 'l2'],\n    'C' : np.logspace(-4, 4, 20)\n}\n\n# perform grid search\nlr_model_cv = GridSearchCV(estimator = LogisticRegression(solver='liblinear'), \n                            param_grid = hyper_params, \n                            scoring= 'roc_auc', \n                            cv = 4, \n                            verbose = 1,\n                            return_train_score=True)\n\nlr_model_cv.fit(X_train_enc, y_train)\n\ny_pred_lr_cv = lr_model_cv.predict(X_test_enc)\ncm = confusion_matrix(y_test, y_pred_lr_cv)\n\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr_lr_cv, tpr_lr_cv, thresholds_knn_cv = roc_curve(y_test, y_pred_lr_cv)\nroc_display = RocCurveDisplay(fpr=fpr_lr_cv, tpr=tpr_lr_cv).plot()\n\nlr_model_cv.best_params_","b3cafd39":"from sklearn.svm import SVC\n\nsvc_model = SVC(kernel='rbf', random_state=0)\nsvc_model.fit(X_train_enc, y_train)\n# y_pred_svm = svc_model.decision_function(X_test)\n\ny_pred_svc = svc_model.predict(X_test_enc)\ncm = confusion_matrix(y_test, y_pred_svc)\n\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr_svc, tpr_svc, thresholds_svc = roc_curve(y_test, y_pred_svc)\nroc_display = RocCurveDisplay(fpr=fpr_svc, tpr=tpr_svc).plot()\n\nthresholds_svc","6a69d4ee":"# specify range of hyperparameters to tune\nhyper_params = {\n    'C': [0.1,1, 10, 100], \n    'gamma': [1,0.1,0.01,0.001],\n    'kernel': ['rbf', 'poly', 'sigmoid']\n}\n\n# perform grid search\nsvc_model_cv = GridSearchCV(estimator = SVC(), \n                            param_grid = hyper_params, \n                            scoring= 'roc_auc', \n                            cv = 4, \n                            verbose = 1,\n                            return_train_score=True)\n\nsvc_model_cv.fit(X_train_enc, y_train)\n\ny_pred_svc_cv = svc_model_cv.predict(X_test_enc)\ncm = confusion_matrix(y_test, y_pred_svc_cv)\n\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr_svc_cv, tpr_svc_cv, thresholds_knn_cv = roc_curve(y_test, y_pred_svc_cv)\nroc_display = RocCurveDisplay(fpr=fpr_svc_cv, tpr=tpr_svc_cv).plot()\n\nsvc_model_cv.best_params_","c11f6be3":"from sklearn.neighbors import KNeighborsClassifier\n\n\nknn_model = KNeighborsClassifier(n_neighbors=6)\nknn_model.fit(X_train_enc, y_train)\n\ny_pred_knn = knn_model.predict(X_test_enc)\ncm = confusion_matrix(y_test, y_pred_knn)\n\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_pred_knn)\nroc_display = RocCurveDisplay(fpr=fpr_knn, tpr=tpr_knn).plot()\n\nthresholds_knn","1e00e946":"# specify range of hyperparameters to tune\nhyper_params = {\n    'n_neighbors': [3, 6, 9, 12],\n    'weights': ['uniform', 'distance'],\n    'p': [1, 2]\n}\n\n# perform grid search\nknn_model_cv = GridSearchCV(estimator = KNeighborsClassifier(), \n                            param_grid = hyper_params, \n                            scoring= 'roc_auc', \n                            cv = 4, \n                            verbose = 1,\n                            return_train_score=True)      \n\n\nknn_model_cv.fit(X_train_enc, y_train)\n\ny_pred_knn_cv = knn_model_cv.predict(X_test_enc)\ncm = confusion_matrix(y_test, y_pred_knn_cv)\n\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr_knn_cv, tpr_knn_cv, thresholds_knn_cv = roc_curve(y_test, y_pred_knn_cv)\nroc_display = RocCurveDisplay(fpr=fpr_knn_cv, tpr=tpr_knn_cv).plot()\n\nknn_model_cv.best_params_  # {'n_neighbors': 12, 'p': 1, 'weights': 'distance'}","109b6cac":"from sklearn.ensemble import RandomForestClassifier\n\n# specify range of hyperparameters to tune\nhyper_params = {\n    'max_depth': [12, 14, 16, 17, 18]\n}\n\n# perform grid search\nrf_model_cv = GridSearchCV(estimator = RandomForestClassifier(), \n                            param_grid = hyper_params, \n                            scoring= 'roc_auc', \n                            cv = 4, \n                            verbose = 1,\n                            return_train_score=True)      \n\n\nrf_model_cv.fit(X_train_enc, y_train)\n\ny_pred_rf_cv = rf_model_cv.predict(X_test_enc)\ncm = confusion_matrix(y_test, y_pred_rf_cv)\n\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_pred_rf_cv)\nroc_display = RocCurveDisplay(fpr=fpr_rf, tpr=tpr_rf).plot()\n\nrf_model_cv.best_params_","30b14596":"from sklearn.tree import DecisionTreeClassifier\n\n# specify range of hyperparameters to tune\nhyper_params = {\n    'max_depth': [5, 7, 9, 11, 13, 15],\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random']\n}\n\n# perform grid search\ndt_model_cv = GridSearchCV(estimator = DecisionTreeClassifier(), \n                            param_grid = hyper_params, \n                            scoring= 'roc_auc', \n                            cv = 4, \n                            verbose = 1,\n                            return_train_score=True)      \n\n\ndt_model_cv.fit(X_train_enc, y_train)\n\ny_pred_dt = dt_model_cv.predict(X_test_enc)\ncm = confusion_matrix(y_test, y_pred_dt)\n\ncm_display = ConfusionMatrixDisplay(cm).plot()\n\nfpr_dt, tpr_dt, thresholds = roc_curve(y_test, y_pred_dt)\nroc_display = RocCurveDisplay(fpr=fpr_dt, tpr=tpr_dt).plot()\n\ndt_model_cv.best_params_","f9a2e72e":"auc_lr = auc(fpr_lr, tpr_lr)\nauc_lr_cv = auc(fpr_lr_cv, tpr_lr_cv)\nauc_svc = auc(fpr_svc, tpr_svc)\nauc_svc_cv = auc(fpr_svc_cv, tpr_svc_cv)\n\nauc_rf = auc(fpr_rf, tpr_rf)\n# auc_rf_cv = auc(fpr_rf_cv, tpr_rf_cv)\nauc_knn = auc(fpr_knn, tpr_knn)\nauc_knn_cv = auc(fpr_knn_cv, tpr_knn_cv)\n\nauc_dt = auc(fpr_dt, tpr_dt)","af896767":"plt.figure()\n#plt.plot(fpr_svc, tpr_svc, color=\"blue\", lw=2, label=\"Support Vector Classifier (AUC = %0.2f)\" % auc_svc)\nplt.plot(fpr_svc_cv, tpr_svc_cv, color=\"blue\", lw=2, label=\"Support Vector Classifier CV (AUC = %0.2f)\" % auc_svc_cv)\n\n#plt.plot(fpr_lr, tpr_lr, color=\"darkorange\", lw=2, label=\"Logistic Regression (AUC = %0.2f)\" % auc_lr)\nplt.plot(fpr_lr_cv, tpr_lr_cv, color=\"darkorange\", lw=2, label=\"Logistic Regression CV (AUC = %0.2f)\" % auc_lr_cv)\n\nplt.plot(fpr_rf, tpr_rf, color=\"green\", lw=2, label=\"Random Forest (AUC = %0.2f)\" % auc_rf)\n#plt.plot(fpr_rf_cv, tpr_rf_cv, color=\"green\", lw=2, label=\"Random Forest CV (AUC = %0.2f)\" % auc_rf_cv)\n\n#plt.plot(fpr_knn, tpr_knn, color=\"red\", lw=2, label=\"KNN (AUC = %0.2f)\" % auc_knn)\nplt.plot(fpr_knn_cv, tpr_knn_cv, color=\"red\", lw=2, label=\"KNN CV (AUC = %0.2f)\" % auc_knn_cv)\n\nplt.plot(fpr_dt, tpr_dt, color=\"red\", lw=2, label=\"Decision Tree CV (AUC = %0.2f)\" % auc_dt)\n\n\n\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()","6055bebd":"# Save the test data predictions for submission\nsubmission_predictions = dt_model_cv.predict(enc.transform(test))\nsubmission = pd.DataFrame({'Id': test.index, 'poisonous': submission_predictions})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"submission.csv has been saved.\")","b5e47b62":"## Classification KPIs\nWhen classifying data the standard KPIs are:\n\n* TP - True Positives: The number of samples correctly classified as positive\n* FP - False Positives: The number of samples incorrectly classified as positive\n* TN - True Negatives: The number of samples correctly classified as false\n* FN - False Negatives: The number of samples incorrectly classified as false\n\nWhen combined, these KPIs can tell us how accurate our classification model is.","d60acaad":"\n## Classification metrics\nTypically, these KPIs are combined into the following metrics.\n\n\n${Sensitivity = True Positive Rate = \\frac{T_p}{T_p + F_n}}$\n\nSensitivity is the proportion of true positives which have been correctly classified relative to the total number of positives classified (correctly & incorrectly) by the model. \n\n\n${False Positive Rate = 1 - Sensitivity  = \\frac{F_p}{F_p + T_n}}$\n\nThe False Positive Rate is the proportion of negative samples which have been incorrectly classified as positive.\n\n\n${Specificity = \\frac{T_n}{T_n + F_p}}$\n\nSpecificity is the proportion of true negatives which have been correctly classified relative to the total number of negatives classified (correctly & incorrectly) by the model. \n\n\n${Precision = \\frac{T_p}{T_p + F_p}}$\n\nPrecision is the proportion of true positives which have been correctly classified relative to the number of true positives which exist in the data set.\nThis is preferrable to the False Positive Rate when the number of positive and negative samples in the data set differ greatly.\n","c137bec5":"### Support Vector Classifier","89c4db40":"## AUC - Area Under the Curve\nThe AUC is area under the ROC Curve. It allows us to summarise the performance of the True Positive to False Negative ratio for the entire model. \nThis is useful when comparing different models for the same classification task.","818e548d":"# ROC curves\n\nROC stands for Receiver Operating Characteristic. This name is not intuitive so we will endeavour to explain the concept simply.","b99571c1":"## Feature encoding","f05036e0":"### K-Nearest Neighbours","0bb872b5":"### Random Forest","c44b7c5d":"## The purpose of the ROC curve\n\nTypically, classification models incorporate some threshold above and below which the samples are classified as either positive or negative.\nHowever, choosing the correct threshold depends on the use-case of the model. \nIn some cases it may be acceptable to have incorrect classifications on either side for the sake of the overall accuracy of the model. However, in other cases, such as the poisonous mushroom data set which we will be using in the notebook, it is preferrable to always classify the positive samples correctly, at the expense of an increased number of incorrectly classified negative samples. e.g. If we incorrectly classify a poisonous mushroom as edible, we will kill someone, whereas if we incorrectly classify an edible mushroom as poisonous, we will just have wasted a mushroom.\nTo accomplish this we must choose a threshold weighted towards the True Positive Rate rather than the False Positive Rate.\n\nThe ROC curve plots the True Positive Rate vs False Positive rate per threshold. This allows us to sample a distribution of thresholds and choose the best threshold value for our use-case from the plot.","cf92891d":"### Logistic Regression"}}