{"cell_type":{"6f9352de":"code","c1aee470":"code","6f55efa9":"code","0a8681c5":"code","9fe00abd":"code","f368cfa0":"code","c40f4e66":"code","75c77fb7":"code","798efef0":"code","fa79618b":"code","98616721":"code","a68f6f5c":"code","090e953a":"code","0780aaae":"code","91d68d83":"code","ce4d3166":"code","12ac483d":"code","bcd71380":"code","592f9304":"code","f730c67c":"code","b230e659":"code","15380697":"code","9d2a52a0":"code","85017b12":"code","5b232f8a":"markdown","b0fb383a":"markdown","99bec5f6":"markdown","80bc68d5":"markdown","a1f64c78":"markdown","615d5fdd":"markdown","e13659c3":"markdown","ecb1cc27":"markdown","93f55008":"markdown","8382a684":"markdown","95df114f":"markdown","ff90938e":"markdown","9dab06c0":"markdown","f8f3a0e6":"markdown","de1819d8":"markdown","38f712f4":"markdown","d7e6a44f":"markdown","c239fe45":"markdown","ba5c97c9":"markdown","441754a6":"markdown","dc720495":"markdown","877a5496":"markdown","caf05c22":"markdown","eab40b28":"markdown","1027c459":"markdown","488475d4":"markdown","4d814249":"markdown"},"source":{"6f9352de":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n%matplotlib inline","c1aee470":"heart_df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\nheart_df.head(10)","6f55efa9":"heart_df.describe()","0a8681c5":"## Checking for Null values\n\nheart_df.info()","9fe00abd":"## Checking for all the unique values against the attributes\n\nfor i, col in enumerate(heart_df.columns):\n    print(heart_df.columns[i], '::\\n', heart_df[str(col)].unique(), '\\n\\n')","f368cfa0":"## Checking for the class imbalance of the Target Variable\n\nsb.countplot(heart_df.HeartDisease)\nheart_df.HeartDisease.value_counts()","c40f4e66":"## Checking for the distributions of the quantitative attributes\n\nfor col in heart_df.select_dtypes(exclude = 'object').columns.drop(['FastingBS', 'HeartDisease']):  ## Removing FastingBS & HeartDisease attributes as they are ordinal variables with values of 1 & 0\n    plt.figure(figsize = [5, 5])\n    plt.hist(heart_df[str(col)], bins = 30)\n    plt.axvline(x = heart_df[str(col)].mean(), color = 'red', ls = '--', lw = 2)\n    plt.axvline(x = heart_df[str(col)].median(), color = 'orange', ls = '-.', lw = 2)\n    plt.xlabel(str(col))\n    plt.ylabel('Count')\n    plt.show()","75c77fb7":"heart_df.head(10)","798efef0":"## Label encode the Str attributes\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nheart_df['Sex'] = le.fit_transform(heart_df['Sex'])\nheart_df['ChestPainType'] = le.fit_transform(heart_df['ChestPainType'])\nheart_df['RestingECG'] = le.fit_transform(heart_df['RestingECG'])\nheart_df['ExerciseAngina'] =  le.fit_transform(heart_df['ExerciseAngina'])\nheart_df['ST_Slope'] = le.fit_transform(heart_df['ST_Slope'])\n\nheart_df.head(10)","fa79618b":"## Checking for outliers\n\nfor col in heart_df.columns[:-1]:\n    sb.boxplot(heart_df[col]);\n    plt.show()","98616721":"## Capping `RestingBP`\n\nlower = heart_df['RestingBP'].quantile(0.25) - 1.5 * (heart_df['RestingBP'].quantile(0.75) - \n                                                      heart_df['RestingBP'].quantile(0.25))\nupper = heart_df['RestingBP'].quantile(0.25) + 1.5 * (heart_df['RestingBP'].quantile(0.75) - \n                                                      heart_df['RestingBP'].quantile(0.25))\n\nheart_df['RestingBP'] = np.where(heart_df['RestingBP'] > upper, upper, \n                                 np.where(heart_df['RestingBP'] < lower, lower, heart_df['RestingBP']))\n\nsb.boxplot(heart_df['RestingBP']);\nplt.show()","a68f6f5c":"## Capping `Cholesterol`\n\nlower = heart_df['Cholesterol'].quantile(0.25) - 1.5 * (heart_df['Cholesterol'].quantile(0.75) - \n                                                        heart_df['Cholesterol'].quantile(0.25))\nupper = heart_df['Cholesterol'].quantile(0.25) + 1.5 * (heart_df['Cholesterol'].quantile(0.75) - \n                                                        heart_df['Cholesterol'].quantile(0.25))\n\nheart_df['Cholesterol'] = np.where(heart_df['Cholesterol'] > upper, upper, \n                                   np.where(heart_df['Cholesterol'] < lower, lower, heart_df['Cholesterol']))\n\nsb.boxplot(heart_df['Cholesterol']);\nplt.show()","090e953a":"## Capping `MaxHR`\n\nlower = heart_df['MaxHR'].quantile(0.25) - 1.5 * (heart_df['MaxHR'].quantile(0.75) - \n                                                  heart_df['MaxHR'].quantile(0.25))\nupper = heart_df['MaxHR'].quantile(0.25) + 1.5 * (heart_df['MaxHR'].quantile(0.75) - \n                                                  heart_df['MaxHR'].quantile(0.25))\n\nheart_df['MaxHR'] = np.where(heart_df['MaxHR'] > upper, upper, \n                             np.where(heart_df['MaxHR'] < lower, lower, heart_df['MaxHR']))\n\nsb.boxplot(heart_df['MaxHR']);\nplt.show()","0780aaae":"## Capping `Oldpeak`\n\nlower = heart_df['Oldpeak'].quantile(0.25) - 1.5 * (heart_df['Oldpeak'].quantile(0.75) - \n                                                    heart_df['Oldpeak'].quantile(0.25))\nupper = heart_df['Oldpeak'].quantile(0.25) + 1.5 * (heart_df['Oldpeak'].quantile(0.75) - \n                                                    heart_df['Oldpeak'].quantile(0.25))\n\nheart_df['Oldpeak'] = np.where(heart_df['Oldpeak'] > upper, upper, \n                               np.where(heart_df['Oldpeak'] < lower, lower, heart_df['Oldpeak']))\n\nsb.boxplot(heart_df['Oldpeak']);\nplt.show()","91d68d83":"heart_df.head(10)","ce4d3166":"## Plotting the Heatmap to showcase the correlations among the attributes and that of the Target variable\n\nplt.figure(figsize = [10, 8])\nsb.heatmap(heart_df.corr(), cmap = 'viridis_r', annot = True);\nplt.show()","12ac483d":"## Splitting the data into Train & Test\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, Y_train, Y_test = train_test_split(heart_df.iloc[:, :-1].values, heart_df.iloc[:, -1].values, \n                                                    test_size = 0.3, random_state = 123)\n\nx_train.shape, x_test.shape, Y_train.shape, Y_test.shape","bcd71380":"## Normalizing the data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nsc_x_train = scaler.fit_transform(x_train)\nsc_x_test = scaler.transform(x_test)\n\nsc_x_train, x_train, sc_x_test","592f9304":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import recall_score, f1_score, classification_report\n\nlr_model = LogisticRegression(solver = 'newton-cg', penalty = 'l2', C = 0.1, random_state = 123)\n\nY_pred = lr_model.fit(sc_x_train, Y_train).predict(sc_x_test)\n\nprint('The Recall score for the LogisticRegression model: ', recall_score(Y_test, Y_pred))\nprint('The corresponding F1-score: ', f1_score(Y_test, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(Y_test, Y_pred))","f730c67c":"from sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, min_samples_leaf = 10, \n                                  min_samples_split = 2, random_state = 123)\n\nY_pred = dt_model.fit(sc_x_train, Y_train).predict(sc_x_test)\n\nprint('The Recall score for the DecisionTree model: ', recall_score(Y_test, Y_pred))\nprint('The corresponding F1-score: ', f1_score(Y_test, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(Y_test, Y_pred))","b230e659":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(criterion = 'entropy', max_depth = None, min_samples_leaf = 3, \n                                  min_samples_split = 10, n_estimators = 15, n_jobs = 1, random_state = 123)\n\nY_pred = rf_model.fit(sc_x_train, Y_train).predict(sc_x_test)\n\nprint('The Recall score for the RandomForest model: ', recall_score(Y_test, Y_pred))\nprint('The corresponding F1-score: ', f1_score(Y_test, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(Y_test, Y_pred))","15380697":"from sklearn.ensemble import AdaBoostClassifier\n\nab_model = AdaBoostClassifier(learning_rate = 0.1, n_estimators = 50)\n\nY_pred = ab_model.fit(sc_x_train, Y_train).predict(sc_x_test)\n\nprint('The Recall score for the Adaboost model: ', recall_score(Y_test, Y_pred))\nprint('The corresponding F1-score: ', f1_score(Y_test, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(Y_test, Y_pred))","9d2a52a0":"import xgboost\nfrom xgboost.sklearn import XGBClassifier\n\nxg_model = XGBClassifier(booster = 'gbtree', learning_rate = 0.01, max_depth = 3, n_estimators = 500, n_jobs = 1, \n                         verbosity = 0, random_state = 123)\n\nY_pred = xg_model.fit(sc_x_train, Y_train).predict(sc_x_test)\n\nprint('The Recall score for the XGBoost model: ', recall_score(Y_test, Y_pred))\nprint('The corresponding F1-score: ', f1_score(Y_test, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(Y_test, Y_pred))","85017b12":"from sklearn.svm import SVC\n\nsvc_model = SVC(C = 1, gamma = 0.1, random_state = 123)\n\nY_pred = svc_model.fit(sc_x_train, Y_train).predict(sc_x_test)\n\nprint('The Recall score for the SVM model: ', recall_score(Y_test, Y_pred))\nprint('The corresponding F1-score: ', f1_score(Y_test, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(Y_test, Y_pred))","5b232f8a":"##### Decision Trees","b0fb383a":"GridSearch has been performed for this and we have arrived at the following hyperparams to produce the best metrics (Accuracy):\n\n- Best metrics: `0.88934` using `{'criterion': 'entropy', 'max_depth': 'None', 'min_samples_leaf': '3', 'min_samples_split': '10', 'n_estimators': '15', 'n_jobs': '1'}`\n\nSince it takes much time, it has been omitted from the runtime.","99bec5f6":"##### Support Vector","80bc68d5":"##### XGBoost","a1f64c78":"### Building the Models\n\n - Use GridSearch to figure out the best set of hyperparameters to use","615d5fdd":"GridSearch has been performed for this and we have arrived at the following hyperparams to produce the best metrics (Accuracy):\n\n- Best metrics: `0.842684` using `{'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}`\n\nSince it takes much time, it has been omitted from the runtime.","e13659c3":"GridSearch has been performed for this and we have arrived at the following hyperparams to produce the best metrics (Accuracy):\n\n- Best metrics: `0.867551` using `{'learning_rate': 0.1, 'n_estimators': 50}`\n\nSince it takes much time, it has been omitted from the runtime.","ecb1cc27":"GridSearch has been performed for this and we have arrived at the following hyperparams to produce the best metrics (Accuracy):\n\n- Best metrics: `0.882226` using `{'C': 1, 'gamma': 0.1}`\n\nSince it takes much time, it has been omitted from the runtime.","93f55008":"**Given the above `Recall` scores (As it is a Healthcare problem, Recall is considered to be the best metric solver in the Healthcare predictions, as we need to keep the margin of error really really small), as well as the corresponding F1 scores, we consider using `Support Vector models` here....**","8382a684":"Looking at the above map, it seems there are some less significant attributes related to the Target variable but we will not drop the same because they have some quantifiable relationships with other significant attributes.\n\nWe will go ahead with modelling using all of them!","95df114f":"GridSearch has been performed for this and we have arrived at the following hyperparams to produce the best metrics (Accuracy):\n\n- Best metrics: `0.873777` using `{'booster': 'gbtree', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'n_jobs': 1}`\n\nSince it takes much time, it has been omitted from the runtime.","ff90938e":"#### Read in and inspect the data","9dab06c0":"##### Random Forest","f8f3a0e6":"Looking at the above plots, it is clear that the attributes `RestingBP`, `Cholesterol`, `MaxHR` and `Oldpeak` contain outliers, so we would do best to cap them (we are not removing them since we are taking into account all the attributes may be a good contributor to the target). ","de1819d8":"Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5 CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n\nTherefore this notebook will attempt to do this with relative high amount of accuracy & recall.","38f712f4":"Looking at the above counts there's not much of a class imbalance, so we need not apply any sampling methods.","d7e6a44f":"##### Logistic Regression","c239fe45":"#### Necessary Imports","ba5c97c9":"GridSearch has been performed for this and we have arrived at the following hyperparams to produce the best score (Accuracy):\n\n- Best metrics: `0.864438` using `{'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}`\n\nSince it takes much time, it has been omitted from the runtime.","441754a6":" - **Age:** age of the patient [years]\n - **Sex:** sex of the patient [M: Male, F: Female]\n - **ChestPainType:** chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n - **RestingBP:** resting blood pressure [mm Hg]\n - **Cholesterol:** serum cholesterol [mm\/dl]\n - **FastingBS:** fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n - **RestingECG:** resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n - **MaxHR:** maximum heart rate achieved [Numeric value between 60 and 202]\n - **ExerciseAngina:** exercise-induced angina [Y: Yes, N: No]\n - **Oldpeak:** oldpeak = ST [Numeric value measured in depression]\n - **ST_Slope:** the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n - **HeartDisease:** output class [1: heart disease, 0: Normal]","dc720495":"Looking at the above, there are no missing values for any of the attributes in the dataset.","877a5496":"There's not much skewness among the distributions as seen above so, we are not going for any numerical transformations.","caf05c22":"##### AdaBoost","eab40b28":"Looking at the above, we can see that the `Sex`, `ChestPainType`, `RestingECG`, `ExerciseAngina` and `ST_Slope` contain ordinal values in **Char** and **Str** format, so we would do best to encode them to numerical ordinal values..","1027c459":"### EDA\n\nExplore the dataset and perform the wrangling as required....","488475d4":"### Introduction","4d814249":"#### Attribute Information"}}