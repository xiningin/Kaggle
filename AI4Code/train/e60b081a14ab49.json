{"cell_type":{"d15decaf":"code","5ba88cbe":"code","dcb77d0d":"code","d72a093a":"code","391ac597":"code","b319de92":"code","3714e8d4":"code","8f455610":"code","8137773a":"code","fc10336f":"code","acbaf027":"code","9e8fceb1":"code","a2426933":"code","a2dda080":"code","ecd0e10d":"code","033b3d01":"code","76c0680f":"code","974f4e2c":"code","753021d5":"code","d8ee453d":"code","65fcb42e":"code","bddc12ef":"code","2aaa7470":"code","7cc09cad":"code","3f38bd81":"code","d92189e1":"code","4c0e96a9":"code","1ba1eafe":"code","b4aaadbe":"code","4b6d6ebd":"code","a3dabffd":"code","d3f8bcf5":"code","27ab6a6f":"code","6b6085fa":"code","ab010bbe":"code","5a9ca9d1":"code","0980c066":"code","b08dee0d":"code","9c61f966":"code","a8e39796":"code","8b5063fe":"code","8161a219":"code","56141400":"code","4391175a":"code","220351ac":"code","8d0fa507":"code","954087ff":"code","2b034966":"code","e3c398e8":"code","338f0c0e":"code","90d9be6b":"code","5cdf9d95":"code","18aec9e5":"code","8e6ae2ee":"code","cf00fca0":"code","890f1182":"code","3880b887":"code","764ab1ae":"code","20e4bc4d":"code","534f437d":"code","e29b3233":"code","b25e84a3":"code","a6cb5bb1":"code","b18392aa":"code","c66306f9":"code","604684dc":"code","ad324e13":"code","f258c7be":"code","38faf701":"code","3c80e1c3":"code","a0d7f0d3":"code","956e1f4d":"code","5f2262ed":"code","d157fea8":"code","a9b63fc6":"code","fb4d2972":"code","45bf130a":"code","a6b57f00":"code","efbaa921":"markdown","c00c17b2":"markdown","d6dc58ba":"markdown","afa97ab2":"markdown","845221b6":"markdown","ed7cff7b":"markdown","79ba6e0a":"markdown","a6747d37":"markdown","30ad0cf2":"markdown","41eabe30":"markdown","32d797f5":"markdown","0a5425cc":"markdown","6c21d4fc":"markdown","dce24243":"markdown","351b8116":"markdown","796c71d7":"markdown","e53f4741":"markdown","90d688dd":"markdown","9145d956":"markdown","c44212a6":"markdown","2c03bbdf":"markdown","831a94ce":"markdown","a2610ecf":"markdown","9a938803":"markdown","cf438c7d":"markdown","684f9e4f":"markdown","7a2bed21":"markdown","c2e73a14":"markdown","60266ce1":"markdown","1d33a0b4":"markdown"},"source":{"d15decaf":"# Importing the required libraries\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt, seaborn as sns\n%matplotlib inline\n\n#importing required libaries ofr model building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Importing required packages for visualization\nfrom IPython.display import Image  \nfrom six import StringIO\nfrom sklearn.tree import export_graphviz\nimport  graphviz\n\n# Importing required packages for Model evaluation\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.model_selection import GridSearchCV","5ba88cbe":"# Reading the csv file and putting it into 'df' object.\ndf = pd.read_csv('..\/input\/heartcsv\/heart_v2.csv')","dcb77d0d":"df.columns","d72a093a":"df.info()","391ac597":"df.head()","b319de92":"df['heart disease'].value_counts()","3714e8d4":"# Check how Age is related to Heart disease\nplt.figure(figsize=(20, 7))\nplt.subplot(1,3,1)\nsns.boxplot(x = 'heart disease', y = 'age', data = df)\nplt.title(\"age vs heart disease\")\nplt.subplot(1,3,2)\nsns.boxplot(x = 'heart disease', y = 'cholestrol', data = df)\nplt.title(\"Total Time cholestrol on Website vs heart disease\")\nplt.subplot(1,3,3)\nsns.boxplot(x = 'heart disease', y = 'BP', data = df)\nplt.title(\"BP vs heart disease\")\nplt.show()\nsns.catplot(x = \"sex\", hue = \"heart disease\", data = df, kind = \"count\") \nplt.title(\"Gender vs heart disease\")\nplt.show()\n","8f455610":"# Dropping feature variable to X\nX = df.drop('heart disease',axis=1)\n\n# Putting response variable to y\ny = df['heart disease']","8137773a":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\nX_train.shape, X_test.shape","fc10336f":"dt = DecisionTreeClassifier(max_depth=3)","acbaf027":"dt.fit(X_train, y_train)","9e8fceb1":"!pip install pydotplus","a2426933":"import  pydotplus","a2dda080":"# plotting tree with max_depth=3\ndot_data = StringIO()  \n\nexport_graphviz(dt, out_file=dot_data, filled=True, rounded=True,\n                feature_names=X.columns, \n                class_names=['No Disease', \"Disease\"])\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())\n","ecd0e10d":"y_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)","033b3d01":"print(\"Train score accuracy\")\nprint(accuracy_score(y_train, y_train_pred))\nconfusion_matrix(y_train, y_train_pred)","76c0680f":"print(\"Test score accuracy\")\nprint(accuracy_score(y_test, y_test_pred))\nconfusion_matrix(y_test, y_test_pred)","974f4e2c":"print(\"Train score classification report\")\nprint (classification_report(y_train, y_train_pred))","753021d5":"print(\"Test score classification report\")\nprint (classification_report(y_test, y_test_pred))","d8ee453d":"# function to graph a decision tree\ndef get_dt_graph(dt_classifier):\n    dot_data = StringIO()\n    export_graphviz(dt_classifier, out_file=dot_data, filled=True,rounded=True,\n                    feature_names=X.columns, \n                    class_names=['Disease', \"No Disease\"])\n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    return graph","65fcb42e":"# function to perfrom model evalution\ndef evaluate_model(dt_classifier):\n    y_train_pred=dt_classifier.predict(X_train)\n    y_test_pred=dt_classifier.predict(X_test)\n    print(\"Train set performance\")\n    print(\"Train Accuracy :\",accuracy_score(y_train, y_train_pred))\n    print(\"Train Confusion Matrix:\",confusion_matrix(y_train, y_train_pred))\n    print(\"-\"*50)\n    print(\"Train set performance\")\n    print(\"Test Accuracy :\", accuracy_score(y_test, y_test_pred))\n    print(\"Test Confusion Matrix:\", confusion_matrix(y_test, y_test_pred))","bddc12ef":"evaluate_model(dt)","2aaa7470":"gph = get_dt_graph(dt)\nImage(gph.create_png())","7cc09cad":"dt_default = DecisionTreeClassifier(random_state=42)\ndt_default.fit(X_train, y_train)\n","3f38bd81":"gph = get_dt_graph(dt_default)\nImage(gph.create_png())","d92189e1":"evaluate_model(dt_default)","4c0e96a9":"dt_depth = DecisionTreeClassifier(max_depth=3, random_state=42)\ndt_depth.fit(X_train, y_train)","1ba1eafe":"gph = get_dt_graph(dt_depth) \nImage(gph.create_png())","b4aaadbe":"evaluate_model(dt_depth)","4b6d6ebd":"dt_min_split = DecisionTreeClassifier(min_samples_split=20,random_state=42)\ndt_min_split.fit(X_train, y_train)","a3dabffd":"gph = get_dt_graph(dt_min_split) \nImage(gph.create_png())","d3f8bcf5":"evaluate_model(dt_min_split)","27ab6a6f":"dt_min_leaf = DecisionTreeClassifier(min_samples_leaf=20, random_state=42)\ndt_min_leaf.fit(X_train, y_train)","6b6085fa":"gph = get_dt_graph(dt_min_leaf)\nImage(gph.create_png())","ab010bbe":"evaluate_model(dt_min_leaf)","5a9ca9d1":"dt_min_leaf_entropy = DecisionTreeClassifier(min_samples_leaf=20, random_state=42, criterion=\"entropy\")\ndt_min_leaf_entropy.fit(X_train, y_train)","0980c066":"gph = get_dt_graph(dt_min_leaf_entropy)\nImage(gph.create_png())","b08dee0d":"evaluate_model(dt_min_leaf_entropy)","9c61f966":"dt = DecisionTreeClassifier(random_state=42)","a8e39796":"# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [2, 3, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'criterion': [\"gini\", \"entropy\"]\n}\n\n# We will get a total of 50 candidate models","8b5063fe":"# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=dt, \n                           param_grid=params, \n                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")","8161a219":"%%time\ngrid_search.fit(X_train, y_train)","56141400":"score_df = pd.DataFrame(grid_search.cv_results_)\nscore_df.head()","4391175a":"score_df.shape","220351ac":"#Chekding the top 5 combinations\nscore_df.nlargest(5,\"mean_test_score\")","8d0fa507":"grid_search.best_score_","954087ff":"grid_search.best_estimator_","2b034966":"#Decision tree with best estimators\ndt_best = grid_search.best_estimator_","e3c398e8":"evaluate_model(dt_best)","338f0c0e":"gph = get_dt_graph(dt_best)\nImage(gph.create_png())","90d9be6b":"from sklearn.ensemble import RandomForestClassifier","5cdf9d95":"X_train.head()","18aec9e5":"y_train.head()","8e6ae2ee":"# instantiate a random forest classifier\nrf = RandomForestClassifier(random_state=42, n_estimators=10, max_depth=3)\n# n_estimator= number of trees","cf00fca0":"rf.fit(X_train, y_train)","890f1182":"# SEE ALL TREES INFO\nrf.estimators_","3880b887":"rf.estimators_[0]","764ab1ae":"# PLOT SAMPLE DECIOSION TREE\nsample_tree = rf.estimators_[4]","20e4bc4d":"gph = get_dt_graph(sample_tree)\nImage(gph.create_png(), width=700, height=700)","534f437d":"#Another tree - showing diversity\ngph = get_dt_graph(rf.estimators_[2])\nImage(gph.create_png(), width=700, height=700)","e29b3233":"rf = RandomForestClassifier(random_state=42, n_estimators=10, max_depth=3, oob_score=True)\nrf.fit(X_train,y_train)","b25e84a3":"rf.oob_score_","a6cb5bb1":"evaluate_model(rf)","b18392aa":"classifier_rf = RandomForestClassifier(random_state=42, n_jobs=-1)","c66306f9":"# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [1, 2, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'max_features': [2,3,4],\n    'n_estimators': [10, 30, 50, 100, 200]\n}","604684dc":"# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=classifier_rf, param_grid=params, \n                          cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")","ad324e13":"%%time\ngrid_search.fit(X,y)","f258c7be":"rf_best = grid_search.best_estimator_","38faf701":"rf_best","3c80e1c3":"evaluate_model(rf_best)","a0d7f0d3":"sample_tree = rf_best.estimators_[0]","956e1f4d":"gph = get_dt_graph(sample_tree)\nImage(gph.create_png())","5f2262ed":"    gph = get_dt_graph(rf_best.estimators_[10])\nImage(gph.create_png())","d157fea8":"classifier_rf = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=5, n_estimators=100, oob_score=True)","a9b63fc6":"classifier_rf.fit(X_train, y_train)","fb4d2972":"classifier_rf.feature_importances_","45bf130a":"imp_df = pd.DataFrame({\n    \"Varname\": X_train.columns,\n    \"Imp\": classifier_rf.feature_importances_\n})","a6b57f00":"imp_df.sort_values(by=\"Imp\", ascending=False)","efbaa921":"## Classification model to predict Heart Disease","c00c17b2":"Train Test split","d6dc58ba":"#### Best estimator and score","afa97ab2":"## Hyper parameter tunning - This is done to restrict our decision tree from Over fitting.","845221b6":"## Evaluating model performance","ed7cff7b":"### Model 3: Decision tree by setting one of the hyper-parameters - Specifying minimum samples before split\nThis tells about the minimum no. of samples required to split an internal node. If an integer value is taken then consider min_samples_split as the minimum no. If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split. By default, it takes the value \"2\".","79ba6e0a":"### Hyper Parameter Tuning using Grid Search Cross Validation**","a6747d37":"### This clearly shows that the model has been over fitted. Train accuracy is 100% and test accuracy is 62%. Almost all nodes have single sample size.","30ad0cf2":"### Model 1: Decision tree without setting any hyper-parameters","41eabe30":"## Model Building","32d797f5":"#### chekding the results of grid search","0a5425cc":"## Building the decision tree\nUsing all default parameters except the depth As we want to restrict the dept size to 3 to prevent over fitting of the data.","6c21d4fc":"### Model 5: Decision tree by setting one of the hyper-parameters - Using Entropy instead of Gini as Criterion\nIt defines the homogeneity metric to measure the quality of a split. Sklearn supports \u201cGini\u201d criteria for Gini Index & \u201centropy\u201d for Information Gain. By default, it takes the value of \u201cGini\u201d.","dce24243":"## Variable importance in RandomForest and Decision trees","351b8116":"We see that the graph is exaclty the same when gini and entropy is used.","796c71d7":"### Checking the Data Imbalance","e53f4741":"### Model 2: Decision tree by setting one of the  hyper-parameters - Max depth of the tree\nThe max_depth parameter denotes the maximum depth of the tree. It can take any integer value or None. If None, then nodes are expanded until all leaves contain just one data point (leading to overfitting) or until all leaves contain less than \"min_samples_split\" samples. By default, it takes \u201cNone\u201d value.","90d688dd":"Few of the hyperparameters which we are going to apply are\n- max_depth\n- min_samples_split\n- min_samples_leaf\n- criterion (Gini\/IG or entropy).\n\nLet us understand how they impact the model performance.","9145d956":"## Final decision Tree with hyper parameter tuning ","c44212a6":"## Grid search for hyper-parameter tuning","2c03bbdf":"### EDA","831a94ce":"### Read and understand the data set","a2610ecf":"### Creating helper functions to evaluate model performance and create graph decision tree","9a938803":"## Out of Bag score","cf438c7d":"#### Testing the helper functions created","684f9e4f":"### Inference:\n1. Patinets with higher age are likely to get more effected.\n2. Patients with high cholestrol have more chances of getting heart disease\n3. Patients with high BP have more chances of getting heart disease and Its not that significant as the medians are almost the same\n4. Males are effected more than females\n","7a2bed21":"# Using Random Forest Classifier","c2e73a14":"Please note that even though we observe that there are few outliers in the data we chose not to drop them and these are medical records and it may vary from person to person and outliers may exists.","60266ce1":"### Model 4: Decision tree by setting one of the hyper-parameters -Specifying minimum samples in leaf node\nThe minimum number of samples required to be at a leaf node. If an integer value is taken then consider min_samples_leaf as the minimum no. If float, then it shows the percentage. By default, it takes the value \"1\".","1d33a0b4":"**Model Objective:** predict whether a person has heart disease or not. Based on the values that various attributes such as gender, age, cholesterol,predict whether a person has heart disease or not. \n\n**Model Approach:** To build a decsion tree which can classify and generate labels stating if a paitent has heart desease or not.The decision trees try to make a prediction and output a flowchart-like diagram. The leaf nodes (bottom) are labelled \u2018Disease\u2019 (indicating that the person has heart disease) or \u2018No Disease\u2019 (which means the person does not have heart disease).\n\n**Data understanding:**\n- Heart disease: 0 - indicates no heart disease and 1 indicated heart disease\n- sex           :0- female and 1 indicated male"}}