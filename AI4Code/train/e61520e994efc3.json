{"cell_type":{"c6bda275":"code","3cd69191":"code","5c7f2a81":"code","3cd0c97e":"code","2a4f1e80":"code","83377367":"code","ccdeb5ba":"code","f0f37b53":"code","110be89a":"code","13d884de":"code","fc88b590":"code","0b3148a3":"code","7e5a04ca":"code","928a8f59":"code","d431e877":"code","4326c43d":"code","eaf63d04":"code","76fdb73e":"code","3a099768":"code","105310b3":"code","178cf579":"code","f052fc3f":"code","dab0e6cf":"code","f971ebc8":"code","70871118":"code","b13f26cb":"code","0614cf92":"code","4083f414":"code","fe24a58a":"code","6706a0cd":"code","02378967":"code","e3ead414":"code","f1749b8a":"code","df51b713":"code","311ae882":"code","ad7402d6":"code","19633931":"code","bdc9c4b0":"code","b2b27cb3":"code","794c1f39":"code","f9f5f878":"markdown","e8275722":"markdown","21d16632":"markdown","51dd049d":"markdown","cd4f2167":"markdown","7f3b760b":"markdown","2744b0f7":"markdown","2847fba4":"markdown","d18a582a":"markdown","c2554178":"markdown","834373b1":"markdown","53c3195c":"markdown","7e53b6be":"markdown","48653036":"markdown","dabd5b9c":"markdown","121c907d":"markdown","32a4ae1f":"markdown","5ae8695d":"markdown"},"source":{"c6bda275":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.models import Model\nfrom keras import regularizers","3cd69191":"# Function to display images\ndef disp_img(img):\n    n = 10  \n    plt.figure(figsize=(20, 4))\n    for i in range(n):\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(img[i].reshape(28, 28))\n        plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)","5c7f2a81":"np.random.seed(42)","3cd0c97e":"temp = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf01 = temp[temp['label'] < 2]\ndel temp\n\ndf_train = df01[df01['label'] == 1].drop(['label'],axis = 1)  #Train data is \"1\" only\ntrain = df_train.values\ntrain = train.astype('float32') \/ 255.\n\ndf_test = df01.drop(['label'],axis = 1).head(10)  #Test data is \"0\" and \"1\"\ntest = df_test.values\ntest = test.astype('float32') \/ 255.\ndel df01","2a4f1e80":"print(train.shape)","83377367":"disp_img(train)","ccdeb5ba":"print(test.shape)","f0f37b53":"disp_img(test)","110be89a":"def create_fullautoencoder(train, opt, loss):\n    input_img = Input(shape=(784, ))\n    encoded = Dense(32, activation='relu')(input_img)\n    decoded = Dense(784, activation='sigmoid')(encoded)\n    autoencoder = Model(input_img, decoded)\n    autoencoder.compile(optimizer=opt, loss=loss)\n    return(autoencoder)","13d884de":"autoencoder1 = create_fullautoencoder(train, 'sgd', 'binary_crossentropy')\nautoencoder1.fit(train, train, epochs=50, batch_size=256, shuffle=True)","fc88b590":"autoencoder1.summary()","0b3148a3":"decoded1 = autoencoder1.predict(test)\ndisp_img(test)\ndisp_img(decoded1)","7e5a04ca":"autoencoder2 = create_fullautoencoder(train, 'adagrad', 'binary_crossentropy')\nautoencoder2.fit(train, train, epochs=50, batch_size=256, shuffle=True)","928a8f59":"decoded2 = autoencoder2.predict(test)\ndisp_img(test)\ndisp_img(decoded2)","d431e877":"autoencoder3 = create_fullautoencoder(train, 'adadelta', 'binary_crossentropy')\nautoencoder3.fit(train, train, epochs=50, batch_size=256, shuffle=True)","4326c43d":"decoded3 = autoencoder3.predict(test)\ndisp_img(test)\ndisp_img(decoded3)","eaf63d04":"autoencoder4 = create_fullautoencoder(train, 'rmsprop', 'binary_crossentropy')\nautoencoder4.fit(train, train, epochs=50, batch_size=256, shuffle=True)","76fdb73e":"decoded4 = autoencoder4.predict(test)\ndisp_img(test)\ndisp_img(decoded4)","3a099768":"autoencoder5 = create_fullautoencoder(train, 'adam', 'binary_crossentropy')\nautoencoder5.fit(train, train, epochs=50, batch_size=256, shuffle=True)","105310b3":"decoded5 = autoencoder5.predict(test)\ndisp_img(test)\ndisp_img(decoded5)","178cf579":"autoencoder6 = create_fullautoencoder(train, 'nadam', 'binary_crossentropy')\nautoencoder6.fit(train, train, epochs=50, batch_size=256, shuffle=True)","f052fc3f":"decoded6 = autoencoder6.predict(test)\ndisp_img(test)\ndisp_img(decoded6)","dab0e6cf":"autoencoder7 = create_fullautoencoder(train, 'adam', 'mean_squared_error')\nautoencoder7.fit(train, train, epochs=50, batch_size=256, shuffle=True)","f971ebc8":"decoded7 = autoencoder7.predict(test)\ndisp_img(test)\ndisp_img(decoded7)","70871118":"autoencoder8 = create_fullautoencoder(train, 'adam', 'binary_crossentropy')\nautoencoder8.fit(train, train, epochs=50, batch_size=256, shuffle=True)","b13f26cb":"decoded8 = autoencoder8.predict(test)\ndisp_img(test)\ndisp_img(decoded8)","0614cf92":"autoencoder9 = create_fullautoencoder(train, 'adam', 'kullback_leibler_divergence')\nautoencoder9.fit(train, train, epochs=50, batch_size=256, shuffle=True)","4083f414":"decoded9 = autoencoder9.predict(test)\ndisp_img(test)\ndisp_img(decoded9)","fe24a58a":"def create_sparseautoencoder(train, opt, loss):\n    input_img = Input(shape=(784, ))\n    encoded = Dense(32, activation='relu', \n                    activity_regularizer=regularizers.l1(10e-5))(input_img)\n    decoded = Dense(784, activation='sigmoid')(encoded)\n    autoencoder = Model(input_img, decoded)\n    autoencoder.compile(optimizer=opt, loss=loss)\n    return(autoencoder)","6706a0cd":"autoencoder10 = create_sparseautoencoder(train, 'adam', 'binary_crossentropy')\nautoencoder10.fit(train, train, epochs=50, batch_size=256, shuffle=True)","02378967":"autoencoder10.summary()","e3ead414":"decoded10 = autoencoder10.predict(test)\ndisp_img(test)\ndisp_img(decoded10)","f1749b8a":"def create_deepautoencoder(train, opt, loss):\n    input_img = Input(shape=(784,))\n    encoded = Dense(128, activation='relu')(input_img)\n    encoded = Dense(64, activation='relu')(encoded)\n    encoded = Dense(32, activation='relu')(encoded)\n    decoded = Dense(64, activation='relu')(encoded)\n    decoded = Dense(128, activation='relu')(decoded)\n    decoded = Dense(784, activation='sigmoid')(decoded)\n    autoencoder = Model(input_img, decoded)\n    autoencoder.compile(optimizer=opt, loss=loss)\n    return(autoencoder)","df51b713":"autoencoder11 = create_deepautoencoder(train, 'adam', 'binary_crossentropy')\nautoencoder11.fit(train, train, epochs=50, batch_size=256, shuffle=True)","311ae882":"autoencoder11.summary()","ad7402d6":"decoded11 = autoencoder11.predict(test)\ndisp_img(test)\ndisp_img(decoded11)","19633931":"def create_deepautoencoder(train, opt, loss):\n    input_img = Input(shape=(28, 28, 1))\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n    encoded = MaxPooling2D((2, 2), padding='same')(x)\n    \n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(16, (3, 3), activation='relu')(x)\n    x = UpSampling2D((2, 2))(x)\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n    \n    autoencoder = Model(input_img, decoded)\n    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n    return(autoencoder)","bdc9c4b0":"train = np.reshape(train, (len(train), 28, 28, 1))\ntest = np.reshape(test, (len(test), 28, 28, 1))\n\nautoencoder12 = create_deepautoencoder(train, 'adam', 'binary_crossentropy')\nautoencoder12.fit(train, train, epochs=50, batch_size=256, shuffle=True)","b2b27cb3":"autoencoder12.summary()","794c1f39":"decoded12 = autoencoder12.predict(test)\ndisp_img(test)\ndisp_img(decoded12)","f9f5f878":"### Convolutional autoencoder\nThis is an auto-encoder that uses CNN.  \n\nCNN even goes so far as to restore the \"0\". Therefore, a simple black and white image like MNIST may be difficult to detect anomalies, but may be suitable for anomaly detection of fine details like cracks in concrete.","e8275722":"# Anomaly detection by autoencoder with Keras\n\nI'm a beginner in deep learning, so I'm sharing what I've learned.  I used [this blog](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html) as a reference.\n\nThe train data should be trained by autoencoder with \"1\" only, and the test data should be \"0\" and \"1\" only. For the restored image, check that \"0\" is not restored. Check the difference from the original image, and \"0\" can be detected as abnormal data.\n## Setup","21d16632":"### KL divergence\nIt didn't work at all.","51dd049d":"#### RMSprop\nA method that solves the shortcomings of Adagrad by accumulating exponential moving averages.  \n**Good job! As hoped, we can determine that the \"0\" is an anomaly because we can reproduce the \"1\" and not the \"0\".**","cd4f2167":"#### Adam\nIt's a combination of RMSprop and Momentum, but with some differences.  \n**Good job! As hoped, we can determine that the \"0\" is an anomaly because we can reproduce the \"1\" and not the \"0\".**","7f3b760b":"In this case, the parameters of the optimizer were left at the default settings, but you may get different results if you adjust them.","2744b0f7":"## Fully-connected neural layer\nIt was best to use binary cross-entropy as the loss function and adam as the optimization method.","2847fba4":"#### Adagrad\nA method to automatically determine the learning rate.  Adagrad didn't work either...","d18a582a":"### Deep autoencoder\nSuccessful anomaly detection.","c2554178":"## Sparse autoencoder\nAdds sparse regularization to a simple autoencoder.  \n\nFor fully connected neural layers, the best results were obtained using binary cross entropy as the loss function and adam as the optimization method, so we will use this combination in the future. \n\nSuccessful anomaly detection.","834373b1":"Test data is **\"0\" and \"1\"**","53c3195c":"### Difference of restored image by loss function\nThe optimization method is fixed to adam, and the differences in the restored images due to the loss function are compared.\n#### MSE\nMean square error. Successful anomaly detection.","7e53b6be":"#### Adadelta\nPrevent rapid and monotonous decline in learning rate with AdaGrad's developmental system.  ","48653036":"#### Nadam\nAdam's take on Nesterov's Accelerated Gradient Method. This is ok too.","dabd5b9c":"### Differences in restored images due to optimization methods\nThe loss function is fixed to binary cross-entropy, and the differences between the restored images from the optimization methods are compared.\n#### SGD\nStochastic Gradient Descent. Easy, but zigzagging is a violent movement and in some cases inefficient.  \nSGD can't even reproduce \"1\"...","121c907d":"Train data is **\"1\"** only","32a4ae1f":"## Data Preparation\nUse MNIST data from Digit Recognizer.","5ae8695d":"#### Binary crossentropy\nAgain, we will run it again to compare it with the other loss functions.  \nSuccessful anomaly detection."}}