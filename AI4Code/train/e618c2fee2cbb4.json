{"cell_type":{"551da13f":"code","d1e63c9f":"code","77572cef":"code","556c4c22":"code","165ebd85":"code","887eb3bb":"code","b5e8eec9":"code","a637263c":"code","8e26bcd7":"code","37d7d419":"code","df204ddc":"code","959c18f4":"code","08bf536e":"code","2bc9a36f":"code","c792f9fe":"code","0954b71e":"code","36904c61":"code","fcd75262":"code","2f0f54f6":"code","cbb0226b":"code","f129b0e8":"code","489ced98":"code","7816bb20":"code","8c8b3b78":"markdown","2da4ba8e":"markdown","32dd2513":"markdown","9e2df78c":"markdown","ee9fa918":"markdown","7b323f10":"markdown","71d2891f":"markdown","6a7435e8":"markdown","14f8588f":"markdown","afb9d628":"markdown","bf2fcece":"markdown","d4650305":"markdown","aaa09bc8":"markdown","d5f7fde3":"markdown","dcfb97d8":"markdown","224cb787":"markdown","4bab539b":"markdown","420b2ce4":"markdown","576804e4":"markdown","375a4fdc":"markdown"},"source":{"551da13f":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]\n","d1e63c9f":"data = pd.read_csv('..\/input\/creditcard.csv',sep=',')\n\nprint(data.columns)","77572cef":"data1= data.sample(frac = 0.1,random_state=1)\n\ndata1.shape","556c4c22":"data.describe()","165ebd85":"data.shape","887eb3bb":"data.isnull().values.any()","b5e8eec9":"data.head()","a637263c":"count_classes = pd.value_counts(data['Class'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Transaction Class Distribution\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","8e26bcd7":"Fraud = data[data['Class']==1]\n\nNormal = data[data['Class']==0]","37d7d419":"Fraud.shape","df204ddc":"Normal.shape","959c18f4":"Fraud.Amount.describe()","08bf536e":"Normal.Amount.describe()","2bc9a36f":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(Fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(Normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","c792f9fe":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time of transaction vs Amount by class')\nax1.scatter(Fraud.Time, Fraud.Amount)\nax1.set_title('Fraud')\nax2.scatter(Normal.Time, Normal.Amount)\nax2.set_title('Normal')\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()\n","0954b71e":"data1.shape","36904c61":"data1.hist(figsize=(20,20))\nplt.show()","fcd75262":"Fraud = data1[data1['Class']==1]\nValid = data1[data1['Class']==0]\noutlier_fraction = len(Fraud)\/float(len(Valid))","2f0f54f6":"print(outlier_fraction)\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\nprint(\"Valid Cases : {}\".format(len(Valid)))","cbb0226b":"correlation_matrix = data1.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\nplt.show()","f129b0e8":"columns = data1.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\nX = data1[columns]\nY = data1[target]\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","489ced98":"classifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n                                       contamination=outlier_fraction,random_state=state, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=outlier_fraction),\n    \"Support Vector Machine\":OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n                                         max_iter=-1, random_state=state)\n   \n}","7816bb20":"n_outliers = len(Fraud)\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_prediction = clf.negative_outlier_factor_\n    elif clf_name == \"Support Vector Machine\":\n        clf.fit(X)\n        y_pred = clf.predict(X)\n    else:    \n        clf.fit(X)\n        scores_prediction = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(Y,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(Y,y_pred))","8c8b3b78":"Do fraudulent transactions occur more often during certain time frame ? Let us find out with a visual representation.","2da4ba8e":"## Model Prediction","32dd2513":"Determine the number of fraud and valid transactions in the dataset.","9e2df78c":"## Preprocessing \n\n### Import Libraries ","ee9fa918":"How different are the amount of money used in different transaction classes?","7b323f10":"## Credit Card Fraud Detection\n\n## Problem Statement:\n\nThe Credit Card Fraud Detection Problem includes modeling past credit card transactions with the knowledge of the ones that turned out to be fraud. This model is then used to identify whether a new transaction is fraudulent or not. Our aim here is to detect 100% of the fraudulent transactions while minimizing the incorrect fraud classifications.\n\n#### DataSet : \n\nThe dataset that is used for credit card fraud detection is derived from the following Kaggle URL :\n\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n","71d2891f":"Fit the model","6a7435e8":"Correlation Matrix","14f8588f":"The above correlation matrix shows that none of the V1 to V28 PCA components have any correlation to each other however if we observe Class has some form positive and negative correlations with the V components but has no correlation with Time and Amount.","afb9d628":"Get all the columns from the dataframe","bf2fcece":"#### Observations :\n- Isolation Forest detected 73 errors versus Local Outlier Factor detecting 97 errors vs. SVM detecting 8516 errors\n- Isolation Forest has a 99.74% more accurate than LOF of 99.65% and SVM of 70.09\n- When comparing error precision & recall for 3 models , the Isolation Forest performed much better than the LOF as we can see that the detection of fraud cases is around 27 % versus LOF detection rate of just 2 % and SVM of 0%.\n- So overall Isolation Forest Method performed much better in determining the fraud cases which is around 30%.\n- We can also improve on this accuracy by increasing the sample size or use deep learning algorithms however at the cost of computational expense.We can also use complex anomaly detection models to get better accuracy in determining more fraudulent cases","d4650305":"## Exploratory Data Analysis","aaa09bc8":"Let us now check the missing values in the dataset","d5f7fde3":"Define the outlier detection methods","dcfb97d8":"Plot histogram of each parameter","224cb787":"Let's have a more graphical representation of the data","4bab539b":"Doesn't seem like the time of transaction really matters here as per above observation.\nNow let us take a sample of the dataset for out modelling and prediction","420b2ce4":"#### Observations\n\n- The data set is highly skewed, consisting of 492 frauds in a total of 284,807 observations. This resulted in only 0.172% fraud cases. This skewed set is justified by the low number of fraudulent transactions.\n\n- The dataset consists of numerical values from the 28 \u2018Principal Component Analysis (PCA)\u2019 transformed features, namely V1 to V28. Furthermore, there is no metadata about the original features provided, so pre-analysis or feature study could not be done.\n\n- The \u2018Time\u2019 and \u2018Amount\u2019 features are not transformed data.\n\n- There is no missing value in the dataset.","576804e4":"Determine the number of fraud and valid transactions in the entire dataset.","375a4fdc":"Now let us print the outlier fraction and no of Fraud and Valid Transaction cases "}}