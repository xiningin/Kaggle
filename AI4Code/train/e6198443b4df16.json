{"cell_type":{"78790f0b":"code","d01cb2ae":"code","25350321":"code","01c7aaca":"code","0344ff34":"code","fa664c25":"code","13f7ced0":"code","2bba91eb":"code","26819840":"code","b110d8e9":"code","aaf14600":"code","1125a18f":"code","ba6949e5":"code","52a8dd84":"code","c29b3817":"code","b45ef809":"code","88ed27c0":"code","de4ddb9c":"code","290c8714":"code","e1b46485":"code","707e40a7":"code","b3580bb4":"code","8b6fcebc":"code","18725bed":"code","a9a74278":"code","9415074e":"code","f5bd6dd6":"code","7e54d4a3":"code","77348aa9":"code","2f5e0294":"code","c86af110":"code","d24716ed":"code","0b5b48ae":"code","97327395":"code","e4e76c55":"code","e29a52f3":"code","a3c0ddeb":"code","fd6e5ae7":"code","2de8976e":"code","dfc53621":"code","b86c985e":"code","f64c0fd0":"code","2a704f98":"code","e57ddf63":"code","8798c15d":"code","9a557ff0":"code","d03125ce":"code","dc72b9a9":"code","55abe528":"code","92955987":"code","769d4563":"code","36f54e6d":"code","65ccda82":"code","606b1d9c":"code","5c1d8a0a":"code","2f4a341d":"code","dd3dcf7f":"code","318e726e":"code","71ebd7c6":"code","8b9ae994":"markdown","5ea609f2":"markdown"},"source":{"78790f0b":"\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\n\n\n# Load the dataset\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# Head train data\ntrain.shape","d01cb2ae":"# Head train data\ntest.shape","25350321":"# Get all the numeric variables\nnumeric_features= train.select_dtypes(include= [np.number])\nnumeric_features.head()","01c7aaca":"# Get all categorical variables\ncategorical_features= train.select_dtypes(include= [np.object])\ncategorical_features.head()","0344ff34":"# Skewness plot\nsns.distplot(train.skew(), color=\"blue\", axlabel= \"Skewness\")","fa664c25":"# Kurtosis plot\nplt.figure(figsize=(12,8))\nsns.distplot(train.kurt(), color= \"r\", axlabel=\"Kurtosis\",\n            norm_hist= False, kde=True, rug= False)\n\nplt.show()","13f7ced0":"# Histogram of target variable\nplt.figure(figsize=(8,6))\nplt.xlabel(\"Sale Price\", fontsize=15)\nplt.ylabel(\"Frequency\", fontsize=15)\nplt.title(\"Sale Price Frequency Distribution\", fontsize=15)\nplt.hist(train[\"SalePrice\"], color= \"blue\")\nplt.show()","2bba91eb":"# Log normal histogram of target variable\ntarget= np.log(train[\"SalePrice\"])\nplt.figure(figsize=(8,6))\nplt.xlabel(\"Sale Price\", fontsize=15)\nplt.ylabel(\"Frequency\", fontsize=15)\nplt.title(\"Log Normal Sale Price Frequency Distribution\", fontsize=15)\nplt.hist(target, color= \"blue\")\nplt.show()","26819840":"# Boxplot of OverallQuality and SalePrice\nplt.figure(figsize=(14,12))\nsns.boxplot(y=\"SalePrice\", x=\"OverallQual\", data= train)\nplt.title(\"Overall Quality vs SalePrice\", fontsize=15)\nplt.xlabel(\"Overall Quality\", fontsize=15)\nplt.ylabel(\"Sale Price\", fontsize=15)\nplt.show()","b110d8e9":"# Get correlation of numeric variables\ncorrelation= numeric_features.corr()\ncorrelation[\"SalePrice\"].sort_values(ascending=False)*100","aaf14600":"# Correlation Heat Map (Seaborn library)\nf, ax= plt.subplots(figsize=(14,12))\nplt.title(\"Correlation of Numeric Featuer with Sale Price\", y=1, size=16)\nsns.heatmap(correlation, square= True, vmax=0.8)","1125a18f":"# Zoomed Heat Map\nk= 11\ncols = correlation.nlargest(k,'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(train[cols].values.T)\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)\n\n","ba6949e5":"# We can see the multicollinearity between GarageCars and GarageArea, TotalBsmtSF and 1stflrSF, TotRmsAbvGrd and GrLivArea\n\n# train[\"GarageCars\"].corr(train[\"SalePrice\"]), train[\"GarageArea\"].corr(train[\"SalePrice\"]),\n# train[\"TotalBsmtSF\"].corr(train[\"SalePrice\"]), train[\"1stFlrSF\"].corr(train[\"SalePrice\"])\n# train[\"TotRmsAbvGrd\"].corr(train[\"SalePrice\"]), train[\"GrLivArea\"].corr(train[\"SalePrice\"])","52a8dd84":"# Drop correlated variables\ntrain.drop([\"GarageArea\", \"1stFlrSF\", \"TotRmsAbvGrd\"], axis=1, inplace=True)\ntest.drop([\"GarageArea\", \"1stFlrSF\", \"TotRmsAbvGrd\"], axis=1, inplace=True)","c29b3817":"# Plot missing values\nplt.figure(figsize=(15,5))\nmissing= train.isnull().sum()\nmissing= missing[missing > 0]\nmissing.sort_values(ascending= False, inplace=True)\nplt.xlabel(\"Features\", fontsize=15)\nplt.ylabel(\"Number of missing values\", fontsize=15)\nplt.title(\"Number of missing data by feature\", fontsize=15)\nmissing.plot(kind=\"bar\")\n\nplt.show()","b45ef809":"# Get total and percent missing values\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)*100\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","88ed27c0":"# Delete missing values variables from the dataset\n\ntrain.drop([\"PoolQC\",\"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\"], axis=1,inplace=True)\ntest.drop([\"PoolQC\",\"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\"], axis=1,inplace=True)\n\n# correlation= numeric_features.corr()\n# correlation[\"SalePrice\"].sort_values(ascending=False)*100","de4ddb9c":"# Check the shape of train and test data\ntrain.shape, test.shape","290c8714":"# Delet numeric features which are not correlated from the train data\ntrain.drop([\"GarageYrBlt\", \"MasVnrArea\", \"Fireplaces\", \"BsmtFinSF1\", \"LotFrontage\", \"WoodDeckSF\", \"2ndFlrSF\",\n\"OpenPorchSF\", \"HalfBath\", \"LotArea\", \"BsmtFullBath\", \"BsmtUnfSF\", \"BedroomAbvGr\", \"ScreenPorch\",\n\"PoolArea\", \"MoSold\", \"3SsnPorch\", \"BsmtFinSF2\", \"BsmtHalfBath\", \"MiscVal\", \"Id\", \"LowQualFinSF\",\n\"YrSold\", \"OverallCond\", \"MSSubClass\", \"EnclosedPorch\", \"KitchenAbvGr\"], axis=1, inplace=True)","e1b46485":"# Delet numeric features which are not correlated from the test data\ntest.drop([\"GarageYrBlt\", \"MasVnrArea\", \"Fireplaces\", \"BsmtFinSF1\", \"LotFrontage\", \"WoodDeckSF\", \"2ndFlrSF\",\n\"OpenPorchSF\", \"HalfBath\", \"LotArea\", \"BsmtFullBath\", \"BsmtUnfSF\", \"BedroomAbvGr\", \"ScreenPorch\",\n\"PoolArea\", \"MoSold\", \"3SsnPorch\", \"BsmtFinSF2\", \"BsmtHalfBath\", \"MiscVal\", \"Id\", \"LowQualFinSF\",\n\"YrSold\", \"OverallCond\", \"MSSubClass\", \"EnclosedPorch\", \"KitchenAbvGr\"], axis=1, inplace=True)","707e40a7":"# Check shape of train and test\ntrain.shape, test.shape","b3580bb4":"# Head of categorical features\ncategorical_features.head()","8b6fcebc":"# Get the remaining numeric variables\nnumeric_features= train.select_dtypes(include= [np.number])\nnumeric_features.head()","18725bed":"# Get the correlation\ncorrelation= numeric_features.corr()\ncorrelation[\"SalePrice\"].sort_values(ascending=False)*100","a9a74278":"# Pairplot of numeric variables\n# Pairplot of numeric variable\nsns.set(style=\"ticks\", color_codes=True)\ncolumns= [\"SalePrice\", \"OverallQual\", \"GrLivArea\", \"GarageCars\", \"TotalBsmtSF\",\"FullBath\", \n         \"YearBuilt\", \"YearRemodAdd\"]\nsns.pairplot(train[columns], size=2, kind= \"scatter\", diag_kind=\"kde\")\nplt.show()","9415074e":"# Get the correlation plot of numeric variable with the target variable\nfig, ((ax1, ax2), (ax3, ax4),(ax5,ax6))= plt.subplots(nrows=3, ncols=2, figsize=(14,10))\n\nsns.regplot(x=train[\"OverallQual\"], y=train[\"SalePrice\"], scatter= True, fit_reg= True, ax=ax1)\n\nsns.regplot(x=train[\"YearBuilt\"], y=train[\"SalePrice\"], scatter=True, fit_reg= True, ax=ax2)\n\nsns.regplot(x=train[\"YearRemodAdd\"], y=train[\"SalePrice\"], scatter=True, fit_reg=True, ax=ax3)\n\nsns.regplot(x=train[\"TotalBsmtSF\"], y=train[\"SalePrice\"], scatter=True, fit_reg=True, ax=ax4)\n\nsns.regplot(x=train[\"GrLivArea\"], y=train[\"SalePrice\"], scatter=True, fit_reg=True, ax=ax5)\n\nsns.regplot(x=train[\"FullBath\"], y=train[\"SalePrice\"], scatter=True, fit_reg=True, ax=ax6)\n\n# sns.regplot(x=train[\"GarageCars\"], y=train[\"SalePrice\"], scatter=True, fit_reg=True, ax=ax7)\n\nplt.show()\n\n# sns.regplot(x=train[\"OverallQual\"], y=train[\"SalePrice\"], fit_reg=False)\n","f5bd6dd6":"# Check missing values in categorical variable\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)*100\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(11)","7e54d4a3":"# Outlier detection\nplt.figure(figsize=(10,8))\nplt.title(\"Scatterplot of GrLivArea and SalePrice\", fontsize=15)\nplt.xlabel(\"GrLivArea\", fontsize=15)\nplt.ylabel(\"SalePrice\", fontsize=15)\nplt.scatter(x= train.GrLivArea, y= train.SalePrice)\n\n# Remove outlier\ntrain.drop(train[(train['GrLivArea'] >4000) & (train['SalePrice']<300000)].index,inplace = True)","77348aa9":"for col in categorical_features.columns.tolist():\n    if categorical_features[col].dtype == 'object':\n        sns.countplot(col, data=categorical_features)\n        plt.xticks(rotation=55)\n        plt.show()","2f5e0294":"# for col in categorical_features.columns.tolist():\n#     if categorical_features[col].dtype == 'object':\n#         sns.countplot(col, data=categorical_features)\n#         plt.xticks(rotation=55)\n#         plt.show()\n        ","c86af110":"# Again check the missing values in remaining variables\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)*100\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(4)","d24716ed":"# Since the feature has most SBrkr we will consider this\ntrain[\"Electrical\"].isnull().sum()\ntrain[\"Electrical\"].value_counts()\ntrain[\"Electrical\"]= train[\"Electrical\"].fillna(train[\"Electrical\"].mode()[0])\ntest[\"Electrical\"]= test[\"Electrical\"].fillna(test[\"Electrical\"].mode()[0])","0b5b48ae":"# MasVnrType: We will consider mode i.e we will fill None\ntrain[\"MasVnrType\"].isnull().sum()\ntrain[\"MasVnrType\"].value_counts()\ntrain[\"MasVnrType\"]= train[\"MasVnrType\"].fillna(train[\"MasVnrType\"].fillna(\"None\"))\ntest[\"MasVnrType\"]= test[\"MasVnrType\"].fillna(test[\"MasVnrType\"].fillna(\"None\"))","97327395":"# BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, \n# NaN means that there is no basement.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train[col]= train[col].fillna(\"None\")\n\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    test[col]= test[col].fillna(\"None\")","e4e76c55":"# GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    train[col]= train[col].fillna(\"None\")\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    test[col]= test[col].fillna(\"None\")\n","e29a52f3":"# MSZoning: Fill mode (RL)\ntest[\"MSZoning\"].value_counts()\ntest[\"MSZoning\"].isnull().sum()\ntest['MSZoning'] = test['MSZoning'].fillna(test['MSZoning'].mode()[0])","a3c0ddeb":"# Functional : data description says NA means typical\ntest[\"Functional\"].value_counts()\ntest[\"Functional\"].isnull().sum()\ntest[\"Functional\"]= test[\"Functional\"].fillna(test[\"Functional\"].fillna(\"Typ\"))","fd6e5ae7":"# Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . \n# Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling.\n# We can then safely remove it.\n\ntrain= train.drop([\"Utilities\"], axis=1)\ntest= test.drop([\"Utilities\"], axis=1)","2de8976e":"# Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value.\n# We will just substitute in the most common string\n\ntest['Exterior1st'] = test['Exterior1st'].fillna(test['Exterior1st'].mode()[0])\n\ntest['Exterior2nd'] = test['Exterior2nd'].fillna(test['Exterior2nd'].mode()[0])","dfc53621":"# SaleType : Fill in again with most frequent which is \"WD\"\n\ntest[\"SaleType\"]= test[\"SaleType\"].fillna(test[\"SaleType\"].mode()[0])","b86c985e":"# GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage\ntest[\"GarageCars\"]= test[\"GarageCars\"].fillna(test[\"GarageCars\"].fillna(0))","f64c0fd0":"# TotalBsmtSF: missing values are likely zero for having no basement\ntest[\"TotalBsmtSF\"]= test[\"TotalBsmtSF\"].fillna(test[\"TotalBsmtSF\"].fillna(0))","2a704f98":"# KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the \n# missing value in KitchenQual.\ntest['KitchenQual'] = test['KitchenQual'].fillna(test['KitchenQual'].mode()[0])","e57ddf63":"# Check shape of the data after remvoing variables\ntrain.shape, test.shape","8798c15d":"t= train.select_dtypes(include=[np.object]).columns\nt","9a557ff0":"# Label Encoding some categorical variables that may contain information in their ordering set\nfrom sklearn.preprocessing import LabelEncoder\n# cols = ('BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n#         'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n#         'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n#         'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSZoning', 'LandContour', 'LotConfig',\n#         'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'YearBuilt', )\n\n\ncols= ('Street', 'LotShape', 'LandContour', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n       'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n       'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\n       'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n       'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'SaleType', 'SaleCondition', 'MSZoning')\n# Process columns, apply LabelEncoder to categories features\nfor c in cols:\n    lbl= LabelEncoder()\n    lbl.fit(list(train[c].values))\n    train[c]= lbl.transform(list(train[c].values))\n    \nfor c in cols:\n    lbl= LabelEncoder()\n    lbl.fit(list(test[c].values))\n    test[c]= lbl.transform(list(test[c].values))\n\n# Shape\nprint('Shape all_data: {}'.format(train.shape))\nprint('Shape all_data: {}'.format(test.shape))","d03125ce":"correlation_one= train.corr()\ncorrelation_one[\"SalePrice\"].sort_values(ascending=False)*100\n# correlation_one[\"SalePrice\"].sort_values(ascending=False)*100","dc72b9a9":"# Drop uncorrelated variables\ntrain.drop([\"Foundation\", \"CentralAir\", \"GarageCond\", \"Electrical\", \"PavedDrive\",\n           \"RoofStyle\", \"SaleCondition\", \"Neighborhood\", \"GarageQual\", \"HouseStyle\",\n           \"RoofMatl\", \"ExterCond\", \"Functional\", \"Exterior2nd\", \"Exterior1st\", \"BsmtCond\",\n           \"Condition1\", \"BsmtFinType2\", \"LandSlope\", \"Street\", \"MasVnrType\", \"LandContour\",\n           \"Condition2\", \"SaleType\", \"LotConfig\", \"BldgType\", \"BsmtFinType1\", \"Heating\",\n           \"LotShape\", \"BsmtExposure\", \"HeatingQC\", \"GarageType\", \"GarageFinish\", \"MSZoning\"], axis=1, inplace=True)\n\ntest.drop([\"Foundation\", \"CentralAir\", \"GarageCond\", \"Electrical\", \"PavedDrive\",\n           \"RoofStyle\", \"SaleCondition\", \"Neighborhood\", \"GarageQual\", \"HouseStyle\",\n           \"RoofMatl\", \"ExterCond\", \"Functional\", \"Exterior2nd\", \"Exterior1st\", \"BsmtCond\",\n           \"Condition1\", \"BsmtFinType2\", \"LandSlope\", \"Street\", \"MasVnrType\", \"LandContour\",\n           \"Condition2\", \"SaleType\", \"LotConfig\", \"BldgType\", \"BsmtFinType1\", \"Heating\",\n           \"LotShape\", \"BsmtExposure\", \"HeatingQC\", \"GarageType\", \"GarageFinish\", \"MSZoning\"], axis=1, inplace=True)","55abe528":"correlation_one= train.corr()\ncorrelation_one[\"SalePrice\"].sort_values(ascending=False)*100\n\n# SalePrice   : What is the price of a house\n# OverallQual : Overall material and finish quality\n# GrLivArea   : Above grade Ground living area square feet\n# TotalBsmtSF : Total Square feet of basement area\n# GarageCars  : Size of garage in car capacity\n# FullBath    : Full bathroom above grade\n# YearBuilt   : Original Construction date\n# YearRemodAdd: Remodel date\n# KitchenQual : Kitchen Quality\n# BsmtQual    : Height of the basement\n# ExterQual   : Exterior material Quality","92955987":"# Model Building\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.cross_validation import train_test_split","769d4563":"# Reshape the data\ntrain.shape, test.shape\ntest.drop(test.index[1458], inplace=True)\ntrain.shape, test.shape","36f54e6d":"# Prepare data for modeling\nX= train.drop(\"SalePrice\", axis=1)\ny= train.SalePrice\n\n# Split the data and check the shape of the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","65ccda82":"# Instantiate the model\nlinreg= LinearRegression()\n\n# Fit the model\nlinreg.fit(X_train, y_train)\n\n# Make predictions\npred_test = linreg.predict(X_test)","606b1d9c":"# Get the coefficient and intercept of the model\nprint(linreg.intercept_)\nprint(linreg.coef_)","5c1d8a0a":"# Pair the feature name with the coefficient\nlist(zip(train, linreg.coef_))","2f4a341d":"# Print R square\nprint(\"R Square \", linreg.score(X_test, y_test))\n# So, in our model, 81.51% of the variability in Y can be explained using X.","dd3dcf7f":"# Mean Absolute Deviation\nprint(\"MAE:\", metrics.mean_absolute_error(pred_test, y_test))","318e726e":"# Mean Square Error\nprint(\"MSE:\", metrics.mean_squared_error(pred_test, y_test))","71ebd7c6":"# Root Mean square Error\nprint(\"RMSE:\", np.sqrt(metrics.mean_squared_error(pred_test, y_test)))\n# Our model was able to predict the value of every house in the test set within $30430 of the real price.","8b9ae994":"# Import Libraries ","5ea609f2":"# Introduction\n**In this notebook you will find EDA and Multiple LInear Regression Analysis of House Prices for beginners in a very simple and clear manner**\n\n# Overview\n\n**This is an exploratory data analysis on the House Prices Kaggle Competition found at**\n\n**https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques**\n\n** Feel free to provide suggestion and comment on the script. Your valuable suggestion are always welcome **\n\n# A very simple tutorial for beginners to start data exploration and data modelling\n- Explore the data\n- EDA\n- Data Modeling\n- Data Validation"}}