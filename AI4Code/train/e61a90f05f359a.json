{"cell_type":{"a36438a1":"code","dff217e0":"code","f43f8fa3":"code","d030f414":"code","e1c0c61e":"code","f984b68c":"code","bfb4e445":"code","3bc2d389":"code","95ee2ecf":"code","dfd5bf41":"code","39be5b5f":"code","8b1804e1":"code","d72956d1":"code","2b8c83b0":"code","b6fee71b":"code","20a5f786":"code","a61daf68":"code","50a492fc":"code","c22096a3":"code","c705b0d1":"code","5ff1b315":"code","6c5b5b4f":"code","8bab05e2":"code","3559d213":"code","b37993fd":"code","bdf0c3c1":"code","aeaee69e":"code","17ba8591":"code","852a6a2c":"code","291b2c02":"code","ef84f484":"code","748f3ac1":"markdown","0f11af69":"markdown","168cdb05":"markdown","fe0c0853":"markdown","e587d5e1":"markdown","37e02ada":"markdown","e4dc2e84":"markdown","bca63866":"markdown","a625e590":"markdown","622084e4":"markdown","a4d50209":"markdown","91813284":"markdown","4e2c5a5b":"markdown","b7b4e902":"markdown","71e1f6d6":"markdown","c9e7319b":"markdown","c917d727":"markdown"},"source":{"a36438a1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\nimport missingno as msno\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nfrom sklearn.preprocessing import RobustScaler","dff217e0":"path = '..\/input\/tabular-playground-series-nov-2021\/'\ntrain = pd.read_csv(path+'train.csv')\ntest = pd.read_csv(path+'test.csv')","f43f8fa3":"print(train.shape)\ntrain.head()","d030f414":"train.describe()","e1c0c61e":"test.head()","f984b68c":"test.describe()","bfb4e445":"for col in train.columns:\n    msg = 'column: {:>10}\\t Percent of NaN value: {:.2f}%'.format(col, 100 * (train[col].isnull().sum() \/ train[col].shape[0]))\n    print(msg)","3bc2d389":"msno.matrix(df=train.iloc[:, :], figsize=(8, 8), color=(0.8, 0.5, 0.2))","95ee2ecf":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\n\ntrain['target'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Pie plot - target')\nax[0].set_ylabel('')\nsns.countplot('target', data=train, ax=ax[1])\nax[1].set_title('Count plot - target')\n\nplt.show()","dfd5bf41":"fig, ax = plt.subplots(10, 10, figsize=(50, 50))\nfor i in range(0, 10):\n  for j in range(0, 10):\n    col = f\"f{10*i + j}\"\n    print(f\"...{col}\", end=\" \")\n    sns.kdeplot(train[train['target'] == 1][col], ax=ax[i][j])\n    sns.kdeplot(train[train['target'] == 0][col], ax=ax[i][j])\n  print(\"\")\nplt.legend(['target == 1', 'target == 0'])\nplt.show()","39be5b5f":"colormap = plt.cm.RdBu\nplt.figure(figsize=(500, 500))\nplt.title('Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(), linewidths=0.1, vmax=1.0,\n           square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"size\": 16})","8b1804e1":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nX_test = test.drop(['id'], axis=1)","d72956d1":"X_train","2b8c83b0":"X_test","b6fee71b":"# Robust Scaler\nRS = RobustScaler().fit(X_train)\nX_train = RS.transform(X_train)\nX_test = RS.transform(X_test)","20a5f786":"import tensorflow as tf\n\nfrom sklearn.model_selection import StratifiedKFold","a61daf68":"class ResBlock(tf.keras.layers.Layer):\n    def __init__(self, n, dropout_rate=0.1):\n\n        super(ResBlock, self).__init__()\n\n        self.dense1 = tf.keras.layers.Dense(1024*n, activation='relu', kernel_regularizer='l2')\n        self.dense2 = tf.keras.layers.Dense(512*n, activation='relu', kernel_regularizer='l2')\n\n        self.dense_res = tf.keras.layers.Dense(512*n, activation='relu')\n\n        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, inputs, training):\n        \n        y1 = self.dense1(inputs)\n        y1 = self.dropout1(y1)\n\n        y2 = self.dense2(y1)\n        y2 = self.dropout2(y2)\n\n        y_res = self.dense_res(inputs)\n\n        return y2 + y_res","50a492fc":"feat_dim = 1024\nnum_blocks=0\nn = 1\ndropout_rate = 0.1\n\ndef build_model():\n    inputs = tf.keras.Input(shape=(100,))\n\n    x = tf.keras.layers.Dense(feat_dim)(inputs)\n\n    for k in range(num_blocks):\n        resBlock = ResBlock(n, dropout_rate)\n        x = resBlock(x)\n\n    for k in range(1, 9, 2):\n        resBlock = ResBlock(1\/(2**k), dropout_rate)\n        x = resBlock(x)\n\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    return model","c22096a3":"model = build_model()\nmodel.summary()","c705b0d1":"# Thanks to Kaggle Notebook \"TensorFlow Transformer - [0.112]\"\nimport math\n\nLR_START = 1e-6\nLR_MAX = 6e-4\nLR_MIN = 1e-6\nLR_RAMPUP_EPOCHS = 0\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = 420\nSTEPS = [60,120,240]\n\n\ndef lrfn(epoch):\n    if epoch<STEPS[0]:\n        epoch2 = epoch\n        EPOCHS2 = STEPS[0]\n    elif epoch<STEPS[0]+STEPS[1]:\n        epoch2 = epoch-STEPS[0]\n        EPOCHS2 = STEPS[1]\n    elif epoch<STEPS[0]+STEPS[1]+STEPS[2]:\n        epoch2 = epoch-STEPS[0]-STEPS[1]\n        EPOCHS2 = STEPS[2]\n    \n    if epoch2 < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch2 + LR_START\n    elif epoch2 < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS2 - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch2 - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index \/ decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n    return lr\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n          format(lr_y[0], max(lr_y), lr_y[-1]))\nplt.xlabel('Epoch',size=14)\nplt.ylabel('Learning Rate',size=14)\nplt.show()\ndel lr_callback","5ff1b315":"X_test.shape[0]","6c5b5b4f":"EPOCH = 60\nBATCH_SIZE = 2**14\n\nnum_fold = 10 # 20\n\nskf = StratifiedKFold(num_fold, shuffle=True, random_state=21) \n\nprediction = np.zeros((X_test.shape[0], 1)) # for Ensenble\n\ni = 0\nfor train_index, valid_index in skf.split(X_train, y_train):\n    i += 1\n    \n    print('\\n', '='*15, '>>>', f'Fold {i}', '<<<', '='*15)\n\n    model = build_model()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['AUC'])\n    \n    save_path = '.\/'\n    checkpoint_folderpath = save_path + f\"weights\/{i}\/\"\n    checkpoint_filepath = save_path + f\"weights\/{i}\/weights\"\n    if os.path.isdir(checkpoint_folderpath):\n        print(f\"Loading Fold #{i} Weights\")\n        model.load_weights(checkpoint_filepath)\n\n    #lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n    sv = tf.keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_auc', verbose=1, save_best_only=True,\n            save_weights_only=True, mode='max', save_freq='epoch', options=None)\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=5)\n    \n    history = model.fit(X_train[train_index], y_train[train_index], verbose=1,\n                              validation_data=(X_train[valid_index], y_train[valid_index]),\n                              epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[sv, early_stop]) # lr_callback, \n    # Prediction \n    print(f'Fold {i} ~ Predicting Test...')\n    prediction = np.concatenate((prediction, model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)), axis=1)\n    \n    del model\n    del optimizer\n    # del lr_callback\n    del sv\n    # early_stop","8bab05e2":"prediction.shape","3559d213":"prediction = prediction[:, 1:]\nprediction.shape","b37993fd":"# Already Done on Training Cell\n\"\"\"\nprediction = np.zeros((X_test.shape[0], 1))\nfor i in range(1, 10+1):\n    print('\\n', '='*15, '>>>', f'Fold {i}', '<<<', '='*15)\n\n    model = build_model()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['AUC'])\n    \n    save_path = '.\/'\n    checkpoint_folderpath = save_path + f\"weights\/{i}\/\"\n    checkpoint_filepath = save_path + f\"weights\/{i}\/weights\"\n    if os.path.isdir(checkpoint_folderpath):\n        print(f\"Loading Fold #{i} Weights\")\n        model.load_weights(checkpoint_filepath)\n    else:\n        print(\"=>ERROR<=\"*15)\n        break\n    \n    y_pred = model.predict(X_test, verbose=1, batch_size=BATCH_SIZE)\n    prediction += y_pred\n\nprediction \/= 10\n\"\"\"","bdf0c3c1":"prediction_mean = np.sum(prediction, axis=-1)\nprediction_median = np.median(prediction, axis=-1)","aeaee69e":"prediction_mean.shape","17ba8591":"prediction_median.shape","852a6a2c":"submission = pd.read_csv(path+'sample_submission.csv')","291b2c02":"submission[\"target\"] = prediction_mean\nsubmission.to_csv(f'submission_mean.csv', index=False)","ef84f484":"submission[\"target\"] = prediction_median\nsubmission.to_csv('submission_median.csv', index=False)\n\n# median -> submit\nsubmission.to_csv('submission.csv', index=False)","748f3ac1":"## Target Label Check","0f11af69":"# Normalizing ~ Robust Scaler","168cdb05":"Result: Just Do It","fe0c0853":"## Null Data Check","e587d5e1":"# Train Model & Prediction","37e02ada":"Scaling : Robust Scaler for dealing with outliers","e4dc2e84":"# Check Dataset","bca63866":"# EDA","a625e590":"Result : No Null Data","622084e4":"# Build Model","a4d50209":"# ResDNN = ResNet + DNN\nResidual Blocks","91813284":"# Feature Engineering","4e2c5a5b":"Check Distribution of Features\n\nNothing special\n\nno cumulative features","b7b4e902":"I will tune num_blocks and num_fold","71e1f6d6":"Result: Nothing Special except the distibution ~> need to scale ... I will chooose Robust Scaler because it is need to be deal with outliers.\n\nFeature Names are also no evidence for looking for ","c9e7319b":"# OOF Ensemble","c917d727":"# Create Submission File"}}