{"cell_type":{"41de310e":"code","6c889655":"code","b0ecf4d7":"code","e29bf082":"code","1a3aa859":"code","055d14e0":"code","c1235c13":"code","2a4493df":"code","1bf71fa5":"code","ec322474":"code","0a26663d":"markdown","6a74081c":"markdown","62760758":"markdown","30c1a11c":"markdown","635319ac":"markdown","2a128dd3":"markdown","37153ac9":"markdown","5a6a7b86":"markdown","61b6e1d4":"markdown","97c3f480":"markdown","0ffd03a0":"markdown","4ff65827":"markdown","11385440":"markdown","5b3bf736":"markdown","b6df83f8":"markdown","7f74390b":"markdown","5891cf15":"markdown","0107e84a":"markdown","b8879914":"markdown","1ad5472b":"markdown","51d4a161":"markdown"},"source":{"41de310e":"# Pandas pour manipuler les tableaux de donn\u00e9es\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 500)\n\n# scikit learn pour les outils de machine learning\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\n# librairies pour la visualisation de donn\u00e9es\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n# et quelques options visuelles\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.set(rc={'figure.figsize':(15,8)})","6c889655":"data=pd.read_csv('..\/input\/titanicdata2\/dataForML.csv',index_col=0)\ndata.head(2)","b0ecf4d7":"data=pd.concat([data, pd.get_dummies(data[\"Pclass\"], prefix='_Pclass_')], axis=1)\ndata=pd.concat([data, pd.get_dummies(data[\"Title\"], prefix='_Title_')], axis=1)\ndata=pd.concat([data, pd.get_dummies(data[\"Embarked\"], prefix='_Embarked_')], axis=1)\ndata=pd.concat([data, pd.get_dummies(data[\"Ticket\"], prefix='_Ticket_')], axis=1)\ndata=pd.concat([data, pd.get_dummies(data[\"LastNameNum\"], prefix='_LastNameNum_')], axis=1)\ndata = data.drop(['AgeBin','FareBin','Title','Embarked','LastNameNum','Ticket'],axis=1)\n\ny = data.loc['train','Survived'].astype('int')\nX = data.loc['train'].drop(['Survived','PassengerId'],axis=1)\nX_cible =  data.loc['test'].drop(['Survived','PassengerId'],axis=1)","e29bf082":"# apr\u00e8s quelques tests je me limite \u00e0 supprimer 'isEnfant'\nX=X.drop(['isEnfant'], axis=1)\nX_cible=X_cible.drop(['isEnfant'], axis=1)","1a3aa859":"# Modules KERAS\nimport keras\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.models import Sequential\nfrom numpy.random import seed\nfrom tensorflow import set_random_seed\n\n# d\u00e9finition de n_cols : la dimension des donn\u00e9es d'entr\u00e9e\nn_cols = X.shape[1]\n\n# d\u00e9finition du seed (graine des fonctions d'al\u00e9as) afin de pouvoir reproduire \u00e0 l'identique les r\u00e9sultats des calculs\nseed(42)\nset_random_seed(42)\n\n# Architecture du mod\u00e8le, un ecouche d'entr\u00e9e, une de sortie et deux couches \"profondes\"\n# cette architecture est un peu empirique, il n'y a pas vraiment de r\u00eagles permettant de d\u00e9finir\n# la meilleur architecture\nnb_neurones=47\nmodel = Sequential()\nmodel.add(Dense(nb_neurones, activation='linear', input_shape = (n_cols,)))\nmodel.add(Dense(nb_neurones, activation='linear')) \nmodel.add(Dense(nb_neurones, activation='linear')) \n\n# option dropout, cette option \u00e9vite le surajustement (overfitting) en injectant des al\u00e9as sur les donn\u00e9es\n# d'entr\u00e9e (une partie des donn\u00e9es est al\u00e9atoirement fix\u00e9e \u00e0 zero)\nmodel.add(Dropout(0.1))\n    \n# couche de sortie\nmodel.add(Dense(1, activation='sigmoid'))  # output layer\n\n# compilation du mod\u00e8le\nmodel.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n\n# ex\u00e9cution du mod\u00e8le\nhistory = model.fit(X,y,nb_epoch=200,batch_size=512, validation_split=0.2, verbose=0)","055d14e0":"plt.plot(history.history['acc'],'red',label='pr\u00e9cision sur le jeu d\\'entrainement')\nplt.plot(history.history['val_acc'],'blue',label='pr\u00e9cision sur le jeu de validation')\nplt.legend()","c1235c13":"model.fit(X,y,nb_epoch=400,verbose=0,batch_size=512)","2a4493df":"y_cible = model.predict(X_cible).reshape(-1,)\ny_cible = np.rint(y_cible)","1bf71fa5":"y_cible.sum()","ec322474":"sample=pd.DataFrame()\nsample['PassengerId'] = data.loc['test','PassengerId']\nsample['Survived'] = y_cible\nsample.Survived=sample.Survived.astype(int)\nsample=sample.reset_index(drop=True)\nsample.to_csv('submission.csv', index=False)","0a26663d":"1. Il faut tout d'abord relire les donn\u00e9es enregistr\u00e9es (en csv) lors de la premi\u00e8re partie (voir ici https:\/\/www.kaggle.com\/djousto\/titanic-first-chapter ) sous '..\/input\/dataForML.csv' ","6a74081c":"- parametre : SibSp\n- parametre : isEnfant\n- parametre : Parch\n- parametre : TicketSibling\n- parametre : isParent","62760758":"Pour les valeurs qualitatives nominales (sans notion de rang dans les valeurs), certains algorithmes comme les arbres de d\u00e9cisions peuvent les traiter telles quelles, mais les algorithmes type r\u00e9gression logistique vont essayer de les interpr\u00e9ter comme des grandeurs ordinales. Pour ces algorithmes on va devoir utiliser la m\u00e9thode  \"d'encodage \u00e0 chaud\" ou \"hot encoding\".\n\n**Explication :** Un encodage \u00e0 chaud cr\u00e9e de nouvelles colonnes (binaires), indiquant la pr\u00e9sence de chaque valeur possible \u00e0 partir des donn\u00e9es d'origine. Exemple : <img src=\"https:\/\/img.over-blog-kiwi.com\/2\/10\/64\/42\/20190302\/ob_5e7622_mtimfxh.png\">\n\nLes valeurs des donn\u00e9es d'origine sont Rouge, Jaune et Vert. Nous cr\u00e9ons une colonne s\u00e9par\u00e9e pour chaque valeur possible. Partout o\u00f9 la valeur originale \u00e9tait Rouge, nous avons mis un 1 dans la colonne Rouge.\n\nPour KERAS \u00e0 priori c'est pr\u00e9f\u00e9rable aussi, nous allons donc effectuer cette transformation pour les variables qualitatives.","30c1a11c":"    tree=ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n           max_depth=22, max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0003, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n           oob_score=False, random_state=42, verbose=0, warm_start=False)","635319ac":"<DIV STYLE=\"page-break-before:always\"><\/DIV> \n\n# 2. Pr\u00e9diction avec KERAS","2a128dd3":"Chargement des librairies et d\u00e9finition de certains param\u00e8tres.","37153ac9":"Affichage du graphique du calcul de pr\u00e9diction, au fil des it\u00e9rations. En rouge la pr\u00e9cision sur le jeu d'entrainement et en bleu la pr\u00e9cision sur le jeu de test.","5a6a7b86":"Pour un bon apprentissage automatique il est n\u00e9cessaire de bien pr\u00e9parer les donn\u00e9es (ce que nous avons fait dans la 1ere partie) puis de s\u00e9lectionner les variables les plus pertinentes et de supprimer les autres, sans quoi l'algorithme va apprendre sur de mauvaises bases et donner des r\u00e9sultats moyens.\n\nLes algorithmes classiques permettent assez facilement de s\u00e9lectionner les variables, car ils renvoient le \"poids\" de chaque variable apr\u00e8s ajustement de l'algorithme.\n\nL'un des inconv\u00e9nients des r\u00e9seaux de neurones est qu'ils sont assez opaques dans leur fonctionnement et qu'on ne peut pas vraiment savoir ce qu'ils font de chaque variable et quelle est la pertinence de chacune. \n\nOn va donc dans un premier temps utiliser un algorithme de type arbre de d\u00e9cision pour supprimer les variables \"parasites\" c'est \u00e0 dire qui ont un impact n\u00e9gatif sur le score, puis nous utiliseront KERAS que nous entrainerons sur ce jeu de variables r\u00e9duit.","61b6e1d4":"Score obtenu :\n0.81818","97c3f480":"<DIV STYLE=\"page-break-before:always\"><\/DIV>\n\n# 3. Publication de la solution","0ffd03a0":"Soyons fous, nous allons utiliser une technique de pointe pour l'apprentissage automatique : les r\u00e9seaux de neurones, ou apprentissage profond.\n\nL'apprentissage profond (plus pr\u00e9cis\u00e9ment \u00ab apprentissage approfondi \u00bb, et en anglais **deep learning, deep structured learning, hierarchical learning**) est un ensemble de m\u00e9thodes d'apprentissage automatique tentant de mod\u00e9liser avec un haut niveau d\u2019abstraction des donn\u00e9es gr\u00e2ce \u00e0 des architectures articul\u00e9es de diff\u00e9rentes transformations non lin\u00e9aires. \n\nIl existe \u00e9videmment des impl\u00e9mentations en Python, la plus utilis\u00e9e \u00e9tant 'Tensorflow'. TensorFlow est un outil open source d'apprentissage automatique d\u00e9velopp\u00e9 par Google. Le code source a \u00e9t\u00e9 ouvert le 9 novembre 2015 par Google et publi\u00e9 sous licence Apache. TensorFlow est l'un des outils les plus utilis\u00e9s en IA dans le domaine de l'apprentissage machine\n\nTensorflow reste n\u00e9anmoins assez complexe d'utilisation, il existe heureusement des API de haut niveau permettant de masquer largement la complexit\u00e9 de Tensorflow, l'une des API les plus utilis\u00e9es en Python est Keras. Keras est une biblioth\u00e8que open source \u00e9crite en python.\n\nElle a \u00e9t\u00e9 initialement \u00e9crite par Fran\u00e7ois Chollet, un ing\u00e9nieur fran\u00e7ais, travaillant pour Google. ","4ff65827":"<DIV STYLE=\"page-break-before:always\"><\/DIV>\n\n# 4. Le\u00e7ons retenues","11385440":"## 2.3. Entrainement du r\u00e9seau de neurones","5b3bf736":"## 2.2.  S\u00e9lection des variables","b6df83f8":"# 1. Lecture des donn\u00e9es","7f74390b":"**1. La pr\u00e9paration des donn\u00e9es** \n\nC'est la phase la plus longue et la plus d\u00e9terminante pour la performance finale du l'ensemble. Cette pr\u00e9paration consiste non seulement \u00e0 nettoyer les donn\u00e9es, remplir les donn\u00e9es manquantes, les donn\u00e9es aberrantes, normaliser et centrer les donn\u00e9es etc ... mais surtout, l'un des secrets est de s'impr\u00e9gnier du probl\u00e8me pour imaginer quels param\u00e8tres vont aider la machine \u00e0 pr\u00e9voir les donn\u00e9es de test. En l'occurence il faut se mettre \u00e0 la place des passagers et comprendre ce qui va influer sur la survie. Comme on peut le lire dans les archives, priorit\u00e9 a \u00e9t\u00e9 donn\u00e9e aux femmes et aux enfants. Je suis donc parti du principe qu'une femme qui a des enfants aura plus de chances de survivre mais aura aussi une destin\u00e9e li\u00e9e \u00e0 celle de ses enfants, il est \u00e9vident qu'elle ne quittera pas le navire sans ses enfants. Il a donc fallu cr\u00e9er de nouvelles variables (childrenDied ....) aidant les algorithmes \u00e0 prendre une d\u00e9cision comme l'ont fait les passagers.\n\n**2. Les al\u00e9as sont in\u00e9luctables** \n\nMalgr\u00e9 toutes les optimisations, je me suis heurt\u00e9 \u00e0 un mur des performances. Il est tr\u00e8s difficile de d\u00e9passer 81 ou 82%. Ceci est d\u00fb au fait que lors du naufrage une panique totale s'est mise en place, pour preuve, un grand nombre de canots de sauvetage sont partis \u00e0 vide.\nDe ce fait, m\u00eame si une priorit\u00e9 a \u00e9t\u00e9 donn\u00e9e aux femmes et enfants, c'est en grande partie la chance qui a fait que tel ou tel passager \u00e0 surv\u00e9cu, et quel que soit l'algorithme, il est impossible de mod\u00e9liser la chance (sans quoi on pourrait pr\u00e9dire les chiffres de la loterie)\n\n**3. Algorithme de pr\u00e9diction** \n\nLa plupart des algorithmes test\u00e9s donnent au final une pr\u00e9cision assez similaire, proche des 80%, j'ai choisi de retenir le r\u00e9seau de neurones mais un arbre de d\u00e9cisions (for\u00eats al\u00e9atoires en l'occurence) peut mener aux m\u00eames r\u00e9sultats. La diff\u00e9rence se fera sur l'optimisation des diff\u00e9rences solutions, ce qui passe par\n- l'optimisation des variables d'entr\u00e9e\n- l'optimisation des param\u00e8tres de l'algorithme, ce qui peut \u00eatre assez empirique et assez long en fonction des algorithmes, seule l'exp\u00e9rience du datascientist permettra de faire la diff\u00e9rence sur cette phase.","5891cf15":"<p style=\"text-align: center;font-size:48px;margin:50px;font-family:Impact, Charcoal, sans-serif;\">KAGGLE Challenge : Titanic<\/p>\n<p style=\"text-align: center;font-size:36px;margin:50px;font-family:Impact, Charcoal, sans-serif;\">2eme partie : Pr\u00e9diction<\/p>","0107e84a":"Une fois que les r\u00e9sultats semblent satisfaisants on peut r\u00e9-\u00e9x\u00e9cuter l'algorithme sur le jeu de donn\u00e9es complet","b8879914":"Et enfin ... soumission de la solution. Il s'agit pour cela de soumettre un fichier CSV. Comme indiqu\u00e9 sur le site KAGGLE ce fichier doit r\u00e9pondre aux exigences suivantes\n***\n>    The file should have exactly 2 columns:\n>\n>        PassengerId (sorted in any order)\n>        Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n>\n>    PassengerId,Survived\n>     892,0\n>     893,1\n>     894,0\n>     Etc.\n***","1ad5472b":"## 2.1. Pr\u00e9paration des donn\u00e9es","51d4a161":"Cet algorithme est utilis\u00e9 \u00e0 de multiples reprises en supprimant un \u00e0 un chaque variable et en comparant les r\u00e9sultats, en on d\u00e9duit que les variables suivantes sont plut\u00f4t \"n\u00e9fastes\":"}}