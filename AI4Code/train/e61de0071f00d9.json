{"cell_type":{"e7314137":"code","2076c88c":"code","22f65fa3":"code","d16b758e":"code","8ec5997c":"code","8a3c975a":"code","8d10ac07":"code","bdb01379":"code","97e8d75c":"code","b6fc7dd9":"code","5eb7f933":"code","43851ee6":"code","681e6c8f":"code","4543051c":"code","00d89b88":"code","64131281":"code","e21641d6":"code","02e19020":"code","c343c3b5":"code","e0942253":"code","6ef9da0c":"code","20648ac9":"code","5a19e7c6":"code","b33e5bb8":"code","6228152d":"code","1f6d6786":"code","7fd6c936":"code","ec109b5f":"code","62a4796f":"code","9835c2bf":"code","8fa88e7c":"code","9aa5be11":"code","baf01f30":"code","6fe279fd":"code","3ffcb453":"code","165f5785":"code","bce1aef0":"code","7db55539":"code","9018b624":"code","41e7c230":"code","7452ef3d":"code","ae2b8686":"code","4b69b87a":"code","1a4dd0dd":"code","f8b47b46":"code","d759d1f1":"code","a90d93f8":"code","ebb5a51b":"code","4e9d8e43":"code","74d88a96":"code","a1bc7b40":"code","a013a8ca":"code","8daee286":"code","e6006374":"code","25aeca98":"code","208940c1":"code","95af7a67":"code","f0c5dc58":"code","ad9e16b7":"code","8d0cd9c2":"code","eefdc526":"markdown","dc213415":"markdown","5ce80620":"markdown","1890ef18":"markdown","d3164bc3":"markdown","0ff6b8bf":"markdown","ccb473a8":"markdown","da4b3cd2":"markdown","f4c73b57":"markdown","7bd77b2f":"markdown","9db19a3c":"markdown","a8d7ddd0":"markdown","160bf683":"markdown","f895a345":"markdown","dc46c580":"markdown","10530713":"markdown","3679e818":"markdown","2c7c4ebb":"markdown","40e9d371":"markdown","ad5080c6":"markdown","c890aed3":"markdown","48d3b9d0":"markdown","a2b6183f":"markdown","3ec59493":"markdown","90698ba5":"markdown"},"source":{"e7314137":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport seaborn as sns\nfrom IPython.display import YouTubeVideo\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nimport networkx as nx\nimport PIL\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2076c88c":"# video level feature file\nprint(os.listdir(\"..\/input\/video-sample\/video\/\"))\n# frame level features file\nprint(os.listdir(\"..\/input\/frame-sample\/frame\/\"))","22f65fa3":"labels_df = pd.read_csv('..\/input\/label_names_2018.csv', error_bad_lines= False)","d16b758e":"labels_df.shape","8ec5997c":"labels_df.head()","8a3c975a":"print(\"Total nubers of labels in sample dataset: %s\" %(len(labels_df['label_name'].unique())))","8d10ac07":"vocab = pd.read_csv('..\/input\/vocabulary.csv')","bdb01379":"vocab.head()","97e8d75c":"vocab.shape","b6fc7dd9":"video_files = [\"..\/input\/video-sample\/video\/{}\".format(i) for i in os.listdir(\"..\/input\/video-sample\/video\/\")]\nprint(video_files)","5eb7f933":"vid_ids = []\nlabels = []\nmean_rgb = []\nmean_audio = []","43851ee6":"for file in video_files:\n    for example in tf.python_io.tf_record_iterator(file):\n        tf_example = tf.train.Example.FromString(example)\n        \n        vid_ids.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n        labels.append(tf_example.features.feature['labels'].int64_list.value)\n        mean_rgb.append(tf_example.features.feature['mean_rgb'].float_list.value)\n        mean_audio.append(tf_example.features.feature['mean_audio'].float_list.value)\n        \nprint('Number of videos in Sample data set: %s' % str(len(vid_ids)))\nprint('Picking a youtube video id: %s' % vid_ids[13])\nprint('List of label ids for youtube video id %s, are - %s' % (vid_ids[13], str(labels[13])))\nprint('First 20 rgb feature of a youtube video (',vid_ids[13],'): are - %s' % str(mean_rgb[13][:20]))","681e6c8f":"len(vid_ids)","4543051c":"vid_ids","00d89b88":"labels","64131281":"len(labels)","e21641d6":"mean_audio","02e19020":"len(mean_audio)","c343c3b5":"len(mean_audio[0])","e0942253":"len(mean_rgb)","6ef9da0c":"len(mean_rgb[0])","20648ac9":"labels_name = []\nfor row in labels:\n    n_labels = []\n    for label_id in row:\n        # some labels ids are missing so have put try\/except\n        try:\n            n_labels.append(str(labels_df[labels_df['label_id']==label_id]['label_name'].values[0]))\n        except:\n            continue\n    labels_name.append(n_labels)\n\nprint('List of label names for youtube video id %s, are - %s' % (vid_ids[13], str(labels_name[13])))","5a19e7c6":"labels_name[143]","b33e5bb8":"from collections import Counter\nimport operator\n\nall_labels = []\nfor each in labels_name:\n    all_labels.extend(each)\n    \nlabels_count_dict = dict(Counter(all_labels))","6228152d":"labels_count_dict","1f6d6786":"labels_count_df = pd.DataFrame.from_dict(labels_count_dict, orient= 'index').reset_index()","7fd6c936":"labels_count_df.shape","ec109b5f":"labels_count_df.columns = ['label', 'count']\nsorted_labels_count_df = labels_count_df.sort_values('count', ascending= False)","62a4796f":"sorted_labels_count_df.head()","9835c2bf":"TOP = 25\nTOP_labels = list(sorted_labels_count_df['label'])[:TOP]\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(y='label', x='count', data=sorted_labels_count_df.iloc[0:TOP, :])\nplt.title('Top {} labels with sample count'.format(TOP))","8fa88e7c":"common_occur_top_label_dict = {}\nfor row in labels_name:\n    for label in row:\n        if label in TOP_labels:\n            c_labels = [label + \"|\" + x for x in row if x != label]\n            for c_label in c_labels:\n                common_occur_top_label_dict[c_label] = common_occur_top_label_dict.get(c_label, 0) + 1\n                \n# Putting these into a dataframe\ncommon_occur_top_label_df = pd.DataFrame.from_dict(common_occur_top_label_dict, orient= 'index').reset_index()\ncommon_occur_top_label_df.columns = ['common_label', 'count']\nsorted_common_occur_top_label_df = common_occur_top_label_df.sort_values('count', ascending=False)\n\n\n# plotting 25 common occurs labels from top labels\nTOP = 25\nfig, ax = plt.subplots(figsize=(10,7))\nsns.barplot(y='common_label', x='count', data=sorted_common_occur_top_label_df.iloc[0:TOP, :])\nplt.title('Top {} common occur labels with sample count'.format(TOP))","9aa5be11":"sorted_common_occur_top_label_df.head","baf01f30":"top_coocurance_label_dict = {}\nfor row in labels_name:\n    for label in row:\n        if label in TOP_labels:\n            top_label_siblings = [x for x in row if x != label]\n            for sibling in top_label_siblings:\n                if label not in top_coocurance_label_dict:\n                    top_coocurance_label_dict[label] = {}\n                top_coocurance_label_dict[label][sibling] = top_coocurance_label_dict.get(label, {}).get(sibling, 0) + 11","6fe279fd":"from_label = []\nto_label = []\nvalue = []\nfor key, val in top_coocurance_label_dict.items():\n    for key2, val2 in val.items():\n        from_label.append(key)\n        to_label.append(key2)\n        value.append(val2)","3ffcb453":"df = pd.DataFrame({ 'from': from_label, 'to': to_label, 'value': value})\nsorted_df = df.sort_values('value', ascending=False)\nsorted_df = sorted_df.iloc[:50, ]","165f5785":"df","bce1aef0":"node_colors = ['turquoise', 'turquoise', 'green', 'crimson', 'grey', 'turquoise', 'turquoise', \n'grey', 'skyblue', 'crimson', 'yellow', 'green', 'turquoise', \n'skyblue', 'skyblue', 'green', 'green', 'lightcoral', 'grey', 'yellow', \n'turquoise', 'skyblue', 'orange', 'green', 'skyblue', 'green', 'turquoise']","7db55539":"len(node_colors)","9018b624":"df = sorted_df\nG= nx.from_pandas_edgelist(df, 'from', 'to', 'value', create_using=nx.Graph())","41e7c230":"plt.figure(figsize = (10,10))\nnx.draw(G, pos=nx.circular_layout(G), node_size=1000, with_labels=True, node_color=node_colors)","7452ef3d":"nx.draw_networkx_edge_labels(G, pos=nx.circular_layout(G), edge_labels=nx.get_edge_attributes(G, 'value'))","ae2b8686":"plt.title('Network graph representing the co-occurance between the categories', size=20)\nplt.show()","4b69b87a":"df = sorted_df\nG= nx.from_pandas_edgelist(df, 'from', 'to', 'value', create_using=nx.Graph())\nplt.figure(figsize = (10,10))\nnx.draw(G, pos=nx.circular_layout(G), node_size=1000, with_labels=True, node_color=node_colors)\nnx.draw_networkx_edge_labels(G, pos=nx.circular_layout(G), edge_labels=nx.get_edge_attributes(G, 'value'))\nplt.title('Network graph representing the co-occurance between the categories', size=20)\nplt.show()","1a4dd0dd":"frame_files = [\"..\/input\/frame-sample\/frame\/{}\".format(i) for i in os.listdir(\"..\/input\/frame-sample\/frame\/\")]\nfeat_rgb = []\nfeat_audio = []","f8b47b46":"frame_files","d759d1f1":"for file in frame_files:\n    for example in tf.python_io.tf_record_iterator(file):        \n        tf_seq_example = tf.train.SequenceExample.FromString(example)\n        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n        sess = tf.InteractiveSession()\n        rgb_frame = []\n        audio_frame = []\n        # iterate through frames\n        for i in range(n_frames):\n            rgb_frame.append(tf.cast(tf.decode_raw(\n                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8)\n                           ,tf.float32).eval())\n            audio_frame.append(tf.cast(tf.decode_raw(\n                    tf_seq_example.feature_lists.feature_list['audio'].feature[i].bytes_list.value[0],tf.uint8)\n                           ,tf.float32).eval())\n\n\n        sess.close()\n        feat_rgb.append(rgb_frame)\n        feat_audio.append(audio_frame)\n        break","a90d93f8":"feat_audio","ebb5a51b":"feat_rgb","4e9d8e43":"print(\"No. of videos %d\" % len(feat_rgb))\nprint('The first video has %d frames' %len(feat_rgb[0]))\nprint(\"Max frame length is: %d\" % max([len(x) for x in feat_rgb]))","74d88a96":"from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import TensorBoard\nfrom keras.models import load_model\nfrom keras.models import Model\nfrom keras.utils.vis_utils import plot_model\nimport operator\nimport time \nimport gc\nimport os","a1bc7b40":"train_video_rgb = []\ntrain_video_audio = []\ntrain_frame_rgb = []\ntrain_frame_audio = []\ntrain_labels = []\n\nval_video_rgb = []\nval_video_audio = []\nval_frame_rgb = []\nval_frame_audio = []\nval_labels = []","a013a8ca":"def create_train_dev_dataset(video_rgb, video_audio, frame_rgb, frame_audio, labels):\n    shuffle_indices = np.random.permutation(np.arange(len(labels)))\n    video_rgb_shuffled = video_rgb[shuffle_indices]\n    video_audio_shuffled = video_audio[shuffle_indices]\n    frame_rgb_shuffled = frame_rgb[shuffle_indices]\n    frame_audio_shuffled = frame_audio[shuffle_indices]\n    labels_shuffled = labels[shuffle_indices]\n    \n    dev_idx = max(1, int(len(labels_shuffled) * validation_split_ratio))\n    \n#     del video_rgb\n#     del video_audio\n#     del frame_rgb\n#     del frame_audio\n#     gc.collect()\n    \n    train_video_rgb, val_video_rgb = video_rgb_shuffled[:-dev_idx], video_rgb_shuffled[-dev_idx:]\n    train_video_audio, val_video_audio = video_audio_shuffled[:-dev_idx], video_audio_shuffled[-dev_idx:]\n    \n    train_frame_rgb, val_frame_rgb = frame_rgb_shuffled[:-dev_idx], frame_rgb_shuffled[-dev_idx:]\n    train_frame_audio, val_frame_audio = frame_audio_shuffled[:-dev_idx], frame_audio_shuffled[-dev_idx:]\n    \n    train_labels, val_labels = labels_shuffled[:-dev_idx], labels_shuffled[-dev_idx:]\n    \n    del video_rgb_shuffled, video_audio_shuffled, frame_rgb_shuffled, frame_audio_shuffled, labels_shuffled\n    gc.collect()\n    \n    return (train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, \n            val_video_audio, val_frame_rgb, val_frame_audio, val_labels)","8daee286":"max_frame_rgb_sequence_length = 10\nframe_rgb_embedding_size = 1024\n\nmax_frame_audio_sequence_length = 10\nframe_audio_embedding_size = 128\n\nnumber_dense_units = 1000\nnumber_lstm_units = 100\nrate_drop_lstm = 0.2\nrate_drop_dense = 0.2\nactivation_function='relu'\nvalidation_split_ratio = 0.2\nlabel_feature_size = 10","e6006374":"sample_length = 1000\n\nvideo_rgb = np.random.rand(sample_length, 1024)\nvideo_audio = np.random.rand(sample_length, 128)\n\nframe_rgb = np.random.rand(sample_length, 10, 1024)\nframe_audio = np.random.rand(sample_length, 10, 128)\n\n# Here I have considered i have only 10 labels\nlabels = np.zeros([sample_length,10])\nfor i in range(len(labels)):\n    j = random.randint(0,9)\n    labels[i][j] = 1 ","25aeca98":"def create_model(video_rgb, video_audio, frame_rgb, frame_audio, labels):\n    \"\"\"Create and store best model at `checkpoint` path ustilising bi-lstm layer for frame level data of videos\"\"\"\n    train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, val_frame_rgb, val_frame_audio, val_labels = create_train_dev_dataset(video_rgb, video_audio, frame_rgb, frame_audio, labels) \n    \n    # Creating 2 bi-lstm layer, one for rgb and other for audio level data\n    lstm_layer_1 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n    lstm_layer_2 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n    \n    # creating input layer for frame-level data\n    frame_rgb_sequence_input = Input(shape=(max_frame_rgb_sequence_length, frame_rgb_embedding_size), dtype='float32')\n    frame_audio_sequence_input = Input(shape=(max_frame_audio_sequence_length, frame_audio_embedding_size), dtype='float32')\n    \n    frame_x1 = lstm_layer_1(frame_rgb_sequence_input)\n    frame_x2 = lstm_layer_2(frame_audio_sequence_input)\n    \n    # creating input layer for video-level data\n    video_rgb_input = Input(shape=(video_rgb.shape[1],))\n    video_rgb_dense = Dense(int(number_dense_units\/2), activation=activation_function)(video_rgb_input)\n    \n    video_audio_input = Input(shape=(video_audio.shape[1],))\n    video_audio_dense = Dense(int(number_dense_units\/2), activation=activation_function)(video_audio_input)\n    \n    # merging frame-level bi-lstm output and later passed to dense layer by applying batch-normalisation and dropout\n    merged_frame = concatenate([frame_x1, frame_x2])\n    merged_frame = BatchNormalization()(merged_frame)\n    merged_frame = Dropout(rate_drop_dense)(merged_frame)\n    merged_frame_dense = Dense(int(number_dense_units\/2), activation=activation_function)(merged_frame)\n    \n    # merging video-level dense layer output\n    merged_video = concatenate([video_rgb_dense, video_audio_dense])\n    merged_video = BatchNormalization()(merged_video)\n    merged_video = Dropout(rate_drop_dense)(merged_video)\n    merged_video_dense = Dense(int(number_dense_units\/2), activation=activation_function)(merged_video)\n    \n    # merging frame-level and video-level dense layer output\n    merged = concatenate([merged_frame_dense, merged_video_dense])\n    merged = BatchNormalization()(merged)\n    merged = Dropout(rate_drop_dense)(merged)\n     \n    merged = Dense(number_dense_units, activation=activation_function)(merged)\n    merged = BatchNormalization()(merged)\n    merged = Dropout(rate_drop_dense)(merged)\n    preds = Dense(label_feature_size, activation='sigmoid')(merged)\n    \n    model = Model(inputs=[frame_rgb_sequence_input, frame_audio_sequence_input, video_rgb_input, video_audio_input], outputs=preds)\n    print(model.summary())\n    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n    \n    STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n\n    checkpoint_dir = 'checkpoints\/' + str(int(time.time())) + '\/'\n\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n\n    bst_model_path = checkpoint_dir + STAMP + '.h5'\n    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n    tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs\/{}\".format(time.time()))\n    \n    model.fit([train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio], train_labels,\n              validation_data=([val_frame_rgb, val_frame_audio, val_video_rgb, val_video_audio], val_labels),\n              epochs=200, batch_size=64, shuffle=True, callbacks=[early_stopping, model_checkpoint, tensorboard])    \n    return model","208940c1":"len(video_audio)","95af7a67":"labels","f0c5dc58":"model = create_model(video_audio=video_audio, video_rgb=video_rgb, frame_audio=frame_audio, frame_rgb=frame_rgb, \n                     labels = labels)","ad9e16b7":"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","8d0cd9c2":"model_img = plt.imread('model_plot.png')\n","eefdc526":"## Exploring the frame level of the video.","dc213415":"### Exploring the most common labels.","5ce80620":"### Exploring the video data","1890ef18":"## Creating a dataset of random values having the same size and dimension as the training data set to test the data","d3164bc3":"### Extract the values from the tfrecords","0ff6b8bf":"## Reference: https:\/\/www.kaggle.com\/amansrivastava\/exploration-bi-lstm-model","ccb473a8":"### Within these top 25 labels, explore the most common ones","da4b3cd2":"## Bi-LSTM Multilabel classification","f4c73b57":"## Check the vocabulary file. It has the description of the video indices and the descriptions for the videos.","7bd77b2f":"## Defining the Model architecture","9db19a3c":"### Looking at the distribution of top 25 labels","a8d7ddd0":"## Exploratory data analysis ","160bf683":"#### Since the frames are sequential in nature, we use a LSTM to extract this type of information and merge this with the video data. These will be passed through the sigmoid layer with units equal to the number of features.","f895a345":"### Mapping the label ids with the label names","dc46c580":"### Position the nodes on a circle","10530713":"## Creating a train, dev test by combining video_rgb, video_audio, frame_rgb, frame_audio and labels","3679e818":"## Import libraries","2c7c4ebb":"### label_names_2018.csv contains the mapping between the label ids and the label names","40e9d371":"## Training model","ad5080c6":"### The Video Game | Game is the highest occuring label in the data","c890aed3":"### Create a graph using the columns of the dataframe df","48d3b9d0":"#### Game label has the most number of examples while Recipe has the least","a2b6183f":"### Using checkpoint to store the best model and use it for future","3ec59493":"### Labels count dictionary","90698ba5":"## Creating a Network Graph to explore the relations amongst the TOP labels"}}