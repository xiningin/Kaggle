{"cell_type":{"2e4712ad":"code","7f05c3f5":"code","b7eee85b":"code","58c3b295":"code","ab25f4db":"code","d73b39ca":"code","ffd6ddb1":"code","631eb37e":"code","00f98f61":"code","cce98e96":"code","bea181f7":"code","0ede450c":"code","29e289c6":"code","5f2a550d":"code","a809d192":"code","2f5c31d0":"code","1fbb4063":"code","ff56d078":"code","395f34cb":"code","c0d9f176":"code","ac95079b":"code","68d263cf":"code","200cf811":"code","64c4602f":"markdown","11d707e3":"markdown","73c0093e":"markdown","377a4c34":"markdown","b0d07f52":"markdown","1a95791c":"markdown","3be0cfae":"markdown","f1599b60":"markdown","e37890ec":"markdown","4810f485":"markdown","99ecdee8":"markdown","3999e546":"markdown","d76b7a5b":"markdown","436928b4":"markdown","c85e1223":"markdown","64c52fc2":"markdown","0cd3bdbe":"markdown","029e6a97":"markdown","c38b6368":"markdown","58ab5968":"markdown","6074fd9d":"markdown","fe0252e0":"markdown","0c640e55":"markdown","2e6f1b2d":"markdown"},"source":{"2e4712ad":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport time\nfrom PIL import Image\nimport PIL.Image\nimport IPython.display as display\nimport tensorflow_hub as hub","7f05c3f5":"def tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)>3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)","b7eee85b":"content = '..\/input\/final-dataset-image\/style transfer\/image-2.png'\nstyle = '..\/input\/final-dataset-image\/style transfer\/style.jpg'","58c3b295":"def load_image(image_path):\n    max_dim=512\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_image(img, channels=3)# decodes the image into a tensor\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]# broadcasting the image array so that it has a batch dimension\n\n    return img","ab25f4db":"def imshow(image, title=None):\n  if len(image.shape) > 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)","d73b39ca":"# plot images\ncontent_image = load_image(content)\nstyle_image = load_image(style)\n\nplt.figure(figsize=(15,10))\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')","ffd6ddb1":"print(content_image.shape)\nprint(style_image.shape)","631eb37e":"# load image stylization module\nhub_model = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/2')\n\n# stylize image\nstylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)","00f98f61":"plt.figure(figsize=(15,10))\nplt.subplot(1, 3, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 3, 2)\nimshow(style_image, 'Style Image')\n\nplt.subplot(1, 3, 3)\nimshow(stylized_image, 'Stylized Image')","cce98e96":"# Define the content image representation and load the model\nx=tf.keras.applications.vgg19.preprocess_input(content_image*255)# needs preprocessing for the model to be initialized\nx=tf.image.resize(x, (256,256))# the vgg19 model takes images in 256\nvgg_model=tf.keras.applications.VGG19(include_top=False, weights='imagenet')\nvgg_model.trainable=False\nvgg_model.summary()","bea181f7":"content_layers=['block4_conv2']\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']","0ede450c":"# build the model\ndef my_model(layer_names):\n    # Retrieve the output layers corresponding to the content and style layers\n    vgg_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n    vgg_model.trainable = False\n    outputs = [vgg_model.get_layer(name).output for name in layer_names]\n    model=tf.keras.Model([vgg_model.input], outputs)\n    return model","29e289c6":"style_extractor = my_model(style_layers)\nstyle_outputs = style_extractor(style_image*255)","5f2a550d":"def gram_matrix(input_tensor): # input_tensor is of shape ch, n_H, n_W\n    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n    input_shape = tf.shape(input_tensor)\n    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32) # Unrolls n_H and n_W\n    return result\/(num_locations)","a809d192":"class entire_model(tf.keras.models.Model):\n    def __init__(self, style_layers, content_layers):\n        super(entire_model, self).__init__()\n        self.vgg=my_model(style_layers + content_layers)\n        self.style_layers=style_layers\n        self.content_layers=content_layers\n        self.num_style_layers=len(style_layers)\n        self.vgg.trainable=False\n\n    def call(self, inputs):\n        inputs=inputs*255.0 # Scale back the pixel values\n        preprocessed_input=tf.keras.applications.vgg19.preprocess_input(inputs)\n        outputs=self.vgg(preprocessed_input)# Pass the preprocessed input to my_model\n\n        # Separate the representations of style and content\n        style_outputs, content_outputs=(outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n        # Calculate the gram matrix for each layer in the style output. This will be the final style representation\n        style_outputs=[gram_matrix(layer) for layer in style_outputs]\n\n        # Store the content and style representation in dictionaries in a layer by layer manner\n        content_dict = {content_name:value\n                    for content_name, value\n                    in zip(self.content_layers, content_outputs)}\n\n        style_dict = {style_name:value\n                  for style_name, value\n                  in zip(self.style_layers, style_outputs)}\n\n        return {'content': content_dict, 'style': style_dict}\n        # Returns a dict of dicts with content and style representations, i.e., gram matrix of the style_layers and\n        # the content of the content_layers","2f5c31d0":"# Now we extract the style and content features by calling the above class\nextractor=entire_model(style_layers, content_layers)\nstyle_targets = extractor(style_image)['style']\ncontent_targets = extractor(content_image)['content']\n\nresults = extractor(tf.constant(content_image))","1fbb4063":"# Define a tf.Variable to contain the image to optimize\ngenerate_image = tf.Variable(content_image)\n# Since this is a float image, define a function to keep the pixel values between 0 and 1\ndef clip_0_1(image):\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)","ff56d078":"style_weight=1e3\ncontent_weight=1e4\n\n# Custom weights for different style layers\nstyle_weights = {'block1_conv1': 0.7,\n                 'block2_conv1': 0.19,\n                 'block3_conv1': 0.24,\n                 'block4_conv1': 0.11,\n                 'block5_conv1': 0.26}\n# style_weights = {'block1_conv1': 0.3,\n#                  'block2_conv1': 0.45,\n#                  'block3_conv1': 0.15,\n#                  'block4_conv1': 0.05,\n#                  'block5_conv1': 0.05}","395f34cb":"opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)","c0d9f176":"def total_cost(outputs):\n    style_outputs=outputs['style']\n    content_outputs=outputs['content']\n    style_loss=tf.add_n([style_weights[name]*tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n                        for name in style_outputs.keys()])\n    style_loss*=style_weight\/len(style_layers)# Normalize\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n                             for name in content_outputs.keys()])\n    content_loss*=content_weight\/len(content_layers)\n    loss=style_loss+content_loss\n    return loss","ac95079b":"@tf.function()\ndef train_step(image):\n    with tf.GradientTape() as tape:\n        outputs = extractor(image)\n        loss = total_cost(outputs)\n\n    grad = tape.gradient(loss, image)\n    opt.apply_gradients([(grad, image)])\n    image.assign(clip_0_1(image))","68d263cf":"num_iterations=20\nfor i in range(num_iterations):\n    train_step(generate_image)\n    if(i%2==0):\n        plt.figure(figsize=(12,12))\n        plt.subplot(1, 3, 1)\n        imshow(content_image, 'Original Image')\n        plt.subplot(1, 3, 2)\n        imshow(style_image, 'Style Image')\n        plt.subplot(1, 3, 3)\n        imshow(np.squeeze(generate_image.read_value(), 0), 'New Image - Step'+str(i))","200cf811":"epochs = 10\nsteps_per_epoch = 100\n\nstep = 0\nfor n in range(epochs):\n  for m in range(steps_per_epoch):\n    step += 1\n    train_step(generate_image)\n    print(\"-\", end='')\n  display.clear_output(wait=True)\n  display.display(tensor_to_image(generate_image))\n  print(\"Train step: {}\".format(step))","64c4602f":"# Build Model","11d707e3":"Let's run 20 iteration","73c0093e":"Let's make a function **load_image** to load the input images and limit its maximum dimension to 512 pixels","377a4c34":"Chooose the content and style layers","b0d07f52":"# Define content and style representations","1a95791c":"# Visualize Image","3be0cfae":"Now make a function **imshow** to plot image","f1599b60":"Create an optimizer. The paper recommends LBFGS, but Adam works okay, too","e37890ec":"Let's first assign the content and style image path ","4810f485":"To optimize this, use a weighted combination of the two losses to get the total loss","99ecdee8":"# Import Libraries","3999e546":"Use the intermediate layers of the model to get the content and style representations of the image.","d76b7a5b":"Use tf.GradientTape to update the image","436928b4":"So finally we can see our results","c85e1223":"# Fast Style Transfer using TF-Hub","64c52fc2":"Let's first see the style transfer result by using TF-Hub model","0cd3bdbe":"So our model is working let's run for 10 epochs and 100 iterations","029e6a97":"# Calculate style","c38b6368":"When called on an image, this model returns the gram matrix (style) of the style_layers and content of the content_layers","58ab5968":"# Extract style and content","6074fd9d":"- Compute the gram matrix.Suppose we want to calculate the style loss on a layer. To do this, the first thing we must do is flatten our layer. This is a good thing, as the Gram Matrix calculation will not change based on the size of the layer.So, suppose we do a flatten on a layer of 3 filters. Well, the Gram Matrix shows the similarity between the filters\n- Einsum allows defining Tensors by defining their element-wise computation.\n- This computation is defined by equation, a shorthand form based on Einstein summation.","fe0252e0":"# What is Neural Style Transfer\nBasically, in Neural Style Transfer we have two images- style and content. We need to copy the style from the style image and apply it to the content image. By, style we basically mean, the patterns, the brushstrokes, etc.If you want to deep dive how neural style transfers work, then click on this [link](https:\/\/towardsdatascience.com\/how-do-neural-style-transfers-work-b76de101eb3)","0c640e55":"# Run gradient descent","2e6f1b2d":"Build a model that returns the style and content tensors"}}