{"cell_type":{"b9e02119":"code","5d9c402f":"code","566094bc":"code","02e5e1b0":"code","29b9502b":"code","be98400b":"code","1e34d2b9":"code","ecc9280c":"code","ca7c41b9":"code","6ba4ebca":"code","006ed82f":"code","57e15420":"code","6c78761c":"code","517bf4cf":"code","c017e048":"code","62e15ced":"code","b0bf18c5":"code","e12590e9":"code","3c80b392":"code","1c4d7a3e":"code","e5e0d13d":"code","dadeeef3":"code","930162c4":"code","22585e8b":"code","2204431d":"code","6e33bd71":"code","0334c84e":"code","e5177482":"code","2679ed4c":"code","4c4e9f4d":"code","186c1d66":"code","a2b9c00e":"code","a08c3137":"code","72b74143":"code","f75c5be6":"code","af4db6bb":"code","d166a300":"code","da235cd3":"code","e9dba919":"markdown","4761144f":"markdown","78552a77":"markdown","5f8ef83c":"markdown","826a1f85":"markdown","75848db9":"markdown","ae396c0f":"markdown","34a63d1e":"markdown","6588fd6d":"markdown","c77f463c":"markdown","9c533eac":"markdown","e37cb7bc":"markdown","e72a66ca":"markdown","06523363":"markdown","184b4691":"markdown","bbf0dd42":"markdown","10e21004":"markdown"},"source":{"b9e02119":"import pandas as pd\nimport os\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt","5d9c402f":"dataset_path = '\/kaggle\/input\/car-crashes-severity-prediction\/'\n\ndf_test = pd.read_csv(os.path.join(dataset_path, 'test.csv'))\ndf_train = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\ndf_weather = pd.read_csv(os.path.join(dataset_path, 'weather-sfcsv.csv'))\nprint(\"The shape of the dataset is {}.\\n\\n\".format(df_train.shape))\ndf_train.head()","566094bc":"df_train.drop(columns='ID').describe()","02e5e1b0":"df_test.drop(columns='ID').describe()","29b9502b":"#descriptive statistics summary for Severity \ndf_train['Severity'].describe()","be98400b":"#Severity histogram\ndf_train.Severity.value_counts().sort_values().plot(kind = 'bar')\nplt.xlabel('Class')\nplt.ylabel('Number of class value')\nplt.xticks(rotation=360)\nplt.show()","1e34d2b9":"def show_missing(df):\n    #Shows percentage of null values in each column\n    pd.options.display.max_rows = None\n    display(((df.isnull().sum()\/len(df))*100))","ecc9280c":"show_missing(df_train)","ca7c41b9":"# Get the boolean variables for processing\nbool_variables =  [ i for i in df_train.columns if df_train.dtypes[i]=='bool']\nbool_variables","6ba4ebca":"# Change boolean type to int\ndf_train[['Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Stop','Amenity']]=df_train[['Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Stop','Amenity']].astype(int)\ndf_test[['Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Stop','Amenity']]=df_test[['Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Stop','Amenity']].astype(int)","006ed82f":"#Exploring weather data\ndf_weather.head()","57e15420":"df_weather.info()","6c78761c":"#Create 'Day\/Night' column \ndf_weather['Day\/Night'] = np.where(df_weather.Hour >= 12, 0, 1)","517bf4cf":"#Change the type of 'timestamp' column to datetime\ndf_train['timestamp'] =  pd.to_datetime(df_train['timestamp'])\ndf_test['timestamp'] =  pd.to_datetime(df_test['timestamp'])","c017e048":"#Create Year, Month, Day, and Hour Features in the training set\ndf_train['Year'] = pd.DatetimeIndex(df_train['timestamp']).year\ndf_train['Month'] = pd.DatetimeIndex(df_train['timestamp']).month\ndf_train['Day'] = pd.DatetimeIndex(df_train['timestamp']).day\ndf_train['Hour'] = pd.DatetimeIndex(df_train['timestamp']).hour","62e15ced":"#Create Year, Month, Day, and Hour Features in the test set\ndf_test['Year'] = pd.DatetimeIndex(df_test['timestamp']).year\ndf_test['Month'] = pd.DatetimeIndex(df_test['timestamp']).month\ndf_test['Day'] = pd.DatetimeIndex(df_test['timestamp']).day\ndf_test['Hour'] = pd.DatetimeIndex(df_test['timestamp']).hour","b0bf18c5":"#Merge train and test datasets with weather dataset on Year, Month, Day, and Hour columns\n#Duplicates should be dropped from the weather data\ndf_train_combined = df_train.merge(df_weather.drop_duplicates(subset=['Year','Month','Day','Hour']),on=['Year','Month','Day','Hour'], how='left')\ndf_test_combined = df_test.merge(df_weather.drop_duplicates(subset=['Year','Month','Day','Hour']),on=['Year','Month','Day','Hour'], how='left')","e12590e9":"#Create polar coordinate using latitude and longitue features\ndf_train_combined['r'] = np.sqrt(df_train_combined['Lat']**2 + df_train_combined['Lng'])\ndf_test_combined['r'] = np.sqrt(df_test_combined['Lat']**2 + df_test_combined['Lng'])","3c80b392":"#Create cartesian coordinates using latitude and longitue features\ndf_train_combined['x'] = np.cos(df_train_combined['Lat']) * np.cos(df_train_combined['Lng'])\ndf_train_combined['y'] = np.cos(df_train_combined['Lat']) * np.sin(df_train_combined['Lng'])\n\ndf_test_combined['x'] = np.cos(df_test_combined['Lat']) * np.cos(df_test_combined['Lng'])\ndf_test_combined['y'] = np.cos(df_test_combined['Lat']) * np.sin(df_test_combined['Lng'])","1c4d7a3e":"df_train_combined.info()","e5e0d13d":"show_missing(df_train_combined)","dadeeef3":"df_train_combined['Precipitation(in)'].describe()","930162c4":"df_train_combined['Wind_Chill(F)'].describe()","22585e8b":"def impute_null(df):\n    #This function imputes the null value by filling mean and mode for numerical and categorial features respectively\n    #using the training data\n    cat_v=  [ i for i in df.columns if df.dtypes[i]=='object' if df[i].isnull().values.any()]\n    num_v = [ i for i in df.columns if df.dtypes[i]=='float64' if df[i].isnull().values.any()]\n    for i in num_v:\n        df[i].fillna(df_train_combined[i].mean(), inplace =True)\n    for i in cat_v:\n        df[i].fillna(df_train_combined[i].mode()[0], inplace =True)","2204431d":"#Impute null-values for the training set\nimpute_null(df_train_combined)","6e33bd71":"show_missing(df_train_combined)","0334c84e":"#Impute null-values for the test set\nimpute_null(df_test_combined)","e5177482":"show_missing(df_test_combined)","2679ed4c":"#Encoding 'Weather_Condition' column\nle = LabelEncoder()\nle.fit(df_train_combined['Weather_Condition'])\ndf_train_combined['Weather_Condition'] = le.transform(df_train_combined['Weather_Condition'])\ndf_test_combined['Weather_Condition'] = le.transform(df_test_combined['Weather_Condition'])","4c4e9f4d":"#Calculate correlation between Sevirity and all features to determine which features will be excluded\ndf_train_combined.corr()['Severity'].sort_values(ascending = False)","186c1d66":"#DROP columns that doesn't make contribution\ndf_train_combined = df_train_combined.drop(['timestamp','Precipitation(in)','Give_Way','Bump','Visibility(mi)','Selected','No_Exit','Roundabout','Side'],axis = 1)\ndf_test_combined = df_test_combined.drop(['timestamp','Precipitation(in)','Give_Way','Bump','Visibility(mi)','Selected','No_Exit','Roundabout','Side'],axis = 1)","a2b9c00e":"df_train_combined['Severity'].value_counts()","a08c3137":"df_train_combined[\"Severity\"].value_counts() * 100 \/ len(df_train_combined)","72b74143":"from sklearn.model_selection import train_test_split\n#Split the \"training\" set into training and validation sets with 0.8:0.2 ratio. \nX = df_train_combined.drop(columns=['ID', 'Severity'])\ny = df_train_combined['Severity']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 10, stratify = y)","f75c5be6":"from sklearn.ensemble import RandomForestClassifier\n\n# Create an instance of the classifier\nclassifier = RandomForestClassifier(max_depth=2, random_state=0)\n\n# Train the classifier\nclassifier = classifier.fit(X_train, y_train)\n","af4db6bb":"print(\"The accuracy of the classifier on the validation set is \", (classifier.score(X_val, y_val)))","d166a300":"X_test = df_test_combined.drop(columns=['ID'])\n\ny_test_predicted = classifier.predict(X_test)\n\ndf_test_combined['Severity'] = y_test_predicted\n\ndf_test_combined.head()","da235cd3":"#Generate the submission file. The submission file needs the columns `ID` and `Severity` only\ndf_test_combined[['ID', 'Severity']].to_csv('\/kaggle\/working\/submission.csv', index=False)","e9dba919":"### Reading Data","4761144f":"# Exploratory Data Analysis","78552a77":"## Importing libraries\n\nWe'll use `pandas` to load and manipulate the data. Other libraries will be imported in the relevant sections.","5f8ef83c":"We've got 6407 examples in the dataset with 14 featues, 1 ID, and the `Severity` of the crash.","826a1f85":"## Data Splitting","75848db9":"**This shows that we should use a stratified splitting to maintain classes precentages**","ae396c0f":"# Data Preprocessing","34a63d1e":"We conclude that Precipitation(in) column should be excluded ","6588fd6d":"# Submission File Generation","c77f463c":"Now let's test our classifier on the validation dataset and see the accuracy.","9c533eac":"The output shows desciptive statistics for the numerical features, `Lat`, `Lng`, `Distance(mi)`, and `Severity`.","e37cb7bc":"Now there is no null-values in the training data.","e72a66ca":"### Feature Engineering","06523363":"Now there is no null-values in the training data.","184b4691":"**This shows that there is no null-values in any coulmn.**","bbf0dd42":"### Feature Encoding","10e21004":"# Model Training "}}