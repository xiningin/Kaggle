{"cell_type":{"7da2927c":"code","065dc9df":"code","4853b17b":"code","54d3dbaf":"code","b7fd8e9d":"code","5b9d0d80":"code","63b5eef0":"code","a7c501a3":"code","2a1cb5d6":"code","48db7d0c":"code","43f4db47":"code","df4ea214":"code","5a2d7bdc":"code","4c3d3350":"code","b29c1bec":"code","aac28180":"code","ba0e48d2":"code","cf028699":"markdown","5f72e70c":"markdown","e232a723":"markdown","e3dea2b2":"markdown","24abd20e":"markdown","db302e74":"markdown","4f55b19d":"markdown","b780806c":"markdown","a6a65321":"markdown","03dc0031":"markdown"},"source":{"7da2927c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport zipfile #unzip embedddings\nimport re\nimport difflib\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","065dc9df":"INPUT_PATH = \"..\/input\"\nOUTPUT_PATH = \"..\/output\"\nTRAIN_FILE = \"train.csv\"\nTEST_FILE = \"test.csv\"\nCORRECTED_TEST_FILE = \"corrected_test.csv\"\nCORRECTED_TRAIN_FILE = \"corrected_train.csv\"\nGLOVE_EMBEDDING = \"embeddings\/glove.840B.300d\/glove.840B.300d.txt\"","4853b17b":"#extract data from a csv file in a DataFrame\ndef load_data(csv_path = os.path.join(INPUT_PATH,TRAIN_FILE)):\n    return pd.read_csv(csv_path)\n\n#nested function returning True if len(X) is in [r_min, r_max] window, False otherwise\ndef len_in_range(r_min, r_max):\n    def in_range(X):\n        if len(X)>=r_min and len(X)<=r_max:\n            return True\n        else:\n            return False\n    return in_range\n\n#nested function returning True if the prefix of oov (out of vocabulary) word \n#and voc word are the same, False otherwise\ndef prefix_comparator(oov, prefix_length = 1):\n    def prefix_checker(voc):\n        if voc[:prefix_length].lower() == oov[:prefix_length].lower():\n            return True\n        else:\n            return False\n    return prefix_checker\n\n#compares oov_word to the vocabulary. If the similarity is over the ratio_treshold, the function will\n#return the corresponding token in the vocabulary, otherwise the unknown_token value\n#other parameters: \n#    hard_check: If hard_check is set to \"False\", function will replace ratio_treshold by \n#                min((len(oov_word)-1)\/len(oov_word), ratio_threshold)\n\ndef oov_checker(oov_word, vocab_list, unknown_token = 'UNK', ratio_threshold = 0.9, hard_check = False):\n    best_ratio = 0.\n    best_voc = unknown_token    \n    if hard_check:\n        best_treshold = ratio_threshold\n    else:\n        best_treshold = min((len(oov_word)-1)\/len(oov_word), ratio_threshold)\n        \n    for voc in vocab_list:\n        ratio = difflib.SequenceMatcher(None, oov_word, voc).ratio()\n        if ratio >= best_treshold and ratio > best_ratio:\n            best_ratio = ratio\n            best_voc = voc \n\n    return best_voc, best_ratio\n\n#Generates a dict of tupples in which the keys are words in oov_list and the tupple contains\n#the closest vocabulary word to the oov word and the associated score\n#Only the words in vocabulary which 1.length is in [len(oov_word)-len_window, len(oov_word)+len_window]\n#and 2. share the same prefix as oov_word are considered during the screening\ndef correction_score_generator(oov_list, vocabulary, unknown_token = 'UNK', len_window = 1):\n    correction_list = {}\n    if type(vocabulary) == list:\n        vocab_list = vocabulary\n    else:\n        vocab_list = [*vocabulary]\n        \n    sorted_vocab_list = sorted(vocab_list, key=len)\n    sorted_oov_list = sorted(oov_list, key=len)\n    length = 0 \n    for oov in sorted_oov_list:\n        if length != len(oov):\n            length = len(oov)\n            min_len = length - len_window\n            max_len = length + len_window\n            vocab_window = len_in_range(min_len, max_len)\n            filtered_vocab_list = list(filter(lambda X: vocab_window(X), sorted_vocab_list))\n        prefix_comp = prefix_comparator(oov)\n        filtered_vocab = list(filter(lambda X: prefix_comp(X), filtered_vocab_list))\n        correction_list[oov] = oov_checker(oov, filtered_vocab, unknown_token, ratio_threshold = 0.)\n    return correction_list\n\n#Takes sentences list and return a corrected version of it based on a correction_dict and a treshold:\n#1. scans each word of each sentence\n#2. checks if the word is present in the embedding dict\n#3. if not, checks if the word is present in the correction_dict and compares treshold with\n#   the score of the proposed correction, if it exists\n#4. if 2. and 3. are not positive, replace the unknown word by the \"UNK\" token\n#5. returns the corrected sentence\ndef sentence_correcter(sentences, embeddings, correction_dict, threshold = 0.9):\n    corrected_sentences = []\n    unknown = \"UNK\"\n    for sentence in sentences:\n        corrected_sentence = []\n        for word in sentence:\n            if word in embeddings: \n                corrected_sentence.append(word)\n            else:\n                try:\n                    if correction_dict[word][1] >= threshold:\n                        corrected_sentence.append(correction_dict[word][0])\n                    else:\n                        corrected_sentence.append(unknown)\n                except KeyError:\n                    corrected_sentence.append(unknown)\n        corrected_sentences.append(corrected_sentence)\n    \n    return corrected_sentences\n\ndef de_tokenize(sentences_list):\n    de_sentences_list = []\n    for sentence in sentences_list:\n        de_sentence = \"\"\n        for word in sentence:\n            de_sentence += word + \" \"\n        de_sentences_list.append(de_sentence)\n        \n    return de_sentences_list","54d3dbaf":"#Only extract GloVe embedddings as a first approach\ndef glove_embeddings(gloveFile = os.path.join(INPUT_PATH, GLOVE_EMBEDDING), extract = -1):\n\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    \n    embeddings_index = {}\n    f = open(gloveFile,'r', encoding=\"utf8\")\n    increment = 0\n    for line in f:\n        word, vect = get_coefs(*line.split(\" \"))\n        embeddings_index[word] = vect\n        if increment == extract - 1:\n            break\n        elif extract != -1:\n            increment += 1           \n    return embeddings_index\n\n#Returns a list of lists containing the tokenized version of the sentences contained in the sentences_list\ndef tokenize(sentences_list):\n    return [re.findall(r\"[\\w]+|[']|[.,!?;]\", x) for x in sentences_list]\n\n#Return a dict containing as keys all unique words from a tokenized sentences list, and as value the \n#number of times these words appears in the sentences corpus\ndef get_vocab(sentences):\n    \"\"\"\n    :param sentences: a list of list of words\n    :return: a dictionary of words and their frequency \n    \"\"\"\n    vocab={}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] +=1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n#Finds words in common between a given embedding and the vocabulary\ndef compare_vocab_and_embeddings(vocab, embeddings_index):\n    \"\"\"\n    :params vocab: our corpus vocabulary (a dictionary of word frquencies)\n            embeddings_index: a genim object containing loaded embeddings.\n    :returns in_common: words in common,\n             in_common_freq: total frequency in the corpus vocabulary of \n                             all words in common\n             oov: out of vocabulary words\n             oov_frequency: total frequency in vocab of oov words\n    \"\"\"\n    oov=[]\n    in_common=[]\n    in_common_freq = 0\n    oov_freq = 0\n\n    for word in vocab:\n        if word in embeddings_index:\n            in_common.append(word)\n            in_common_freq += vocab[word]\n        else: \n            oov.append(word)\n            oov_freq += vocab[word]\n    \n    print('Found embeddings for {:.2%} of vocab'.format(len(in_common) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(in_common_freq \/ (in_common_freq + oov_freq)))\n\n    return sorted(in_common)[::-1], sorted(oov)[::-1], in_common_freq, oov_freq\n\n#Returns the list of out-of-vocabulary words sorted by their frequency \n# stated in the vocab dict\ndef sort_oov_words(oov, vocab, threshold = 0.5, min_len = 5):\n    # Sort oov words by their frequency in the text\n    sorted_oov= sorted(oov, key =lambda x: vocab[x], reverse=True )\n    nr_tokens = 0\n    i = 0\n    ratio = 0.\n    pruned_sorted_oov = []\n    # Show oov words and their frequencies\n    if (len(sorted_oov)>0):\n        for word in sorted_oov:\n            if len(word) >= min_len:\n                if  re.search(r'[0-9]+', word, flags=0) == None:\n                    nr_tokens +=vocab[word]\n                    pruned_sorted_oov.append(word)\n        print(\"Total number of oov instances: {}\".format(nr_tokens))\n        for word in pruned_sorted_oov:\n            i += 1\n            #print(\"%s\\t%s\"%(word, vocab[word]))\n            ratio += vocab[word]\n            if ratio\/nr_tokens >= threshold:\n                break       \n    else:\n        print(\"No words were out of vocabulary.\")\n    print(\"Number of oov words selected: {}\/{} corresponding to {} instances\".format(i, len(pruned_sorted_oov),ratio))  \n    return pruned_sorted_oov[:i]","b7fd8e9d":"#extracts train data from train.csv\ntrain_data = load_data()\ntrain_data.head()","5b9d0d80":"questions_list = train_data[\"question_text\"].values","63b5eef0":"embeddings = glove_embeddings()\nprint(\"The GloVe embedding contains {} unique tokens\".format(len(embeddings.keys())))","a7c501a3":"tokenized_questions = tokenize(questions_list)\ntoken_dict = get_vocab(tokenized_questions)\nprint(\"The training dataset contains {} unique tokens\".format(len(token_dict)))","2a1cb5d6":"in_common, oov, _, _ = compare_vocab_and_embeddings(token_dict, embeddings)\noov_words = sort_oov_words(oov, token_dict, threshold = 0.2)","48db7d0c":"correction_scored_dict = correction_score_generator(oov_words, embeddings)","43f4db47":"corrected_questions = sentence_correcter(tokenized_questions, embeddings, correction_scored_dict, threshold = 0.8)\nde_corrected_questions = de_tokenize(corrected_questions)\ncorrected_train_data = train_data.copy()\ncorrected_train_data['corrected_question_text'] = de_corrected_questions\ncorrected_train_data.to_csv(CORRECTED_TRAIN_FILE, index = False)","df4ea214":"#extracts train data from train.csv\ntest_data = load_data(csv_path = os.path.join(INPUT_PATH,TEST_FILE))\ntest_data.head()","5a2d7bdc":"test_questions_list = test_data[\"question_text\"].values","4c3d3350":"tokenized_test_questions = tokenize(test_questions_list)\ntoken_test_dict = get_vocab(tokenized_test_questions)\nprint(\"The test dataset contains {} unique tokens\".format(len(token_test_dict)))","b29c1bec":"test_in_common, test_oov, _, _ = compare_vocab_and_embeddings(token_test_dict, embeddings)\ntest_oov_words = sort_oov_words(test_oov, token_test_dict, threshold = 0.2)","aac28180":"test_correction_scored_dict = correction_score_generator(test_oov_words, embeddings)","ba0e48d2":"corrected_test_questions = sentence_correcter(tokenized_test_questions, embeddings, test_correction_scored_dict, threshold = 0.8)\nde_corrected_test_questions = de_tokenize(corrected_test_questions)\ncorrected_test_data = test_data.copy()\ncorrected_test_data['corrected_question_text'] = de_corrected_test_questions\ncorrected_test_data.to_csv(CORRECTED_TEST_FILE, index = False)","cf028699":"Extraction of questions from the train_data","5f72e70c":"Tokenisation of the questions and creation of a vocabulary list","e232a723":"Compares vocabulary and embeddings to return the complete oov words list, then sort and returns a subset of oov words wich represents the top 20% of all mispelled instances present in the question list","e3dea2b2":"(inspired by https:\/\/www.kaggle.com\/alhalimi\/tokenization-and-word-embedding-compatibility\/notebook)","24abd20e":"Returns a list of correted train questions based on the correction dict and the embeddings, de-tokenize it and save it in an external file for further use","db302e74":"# Data preprocessing","4f55b19d":"Generates a dict containing the oov_words selected at the previous step and their \"closest relative\" in the embedding list, together with their similarity score","b780806c":"# Toolbox\n(in-house)","a6a65321":"Repeat the same process with the test questions","03dc0031":"Extraction of pre-trained GloVe embedding"}}