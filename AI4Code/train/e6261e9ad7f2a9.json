{"cell_type":{"e299cdd4":"code","ec6702d7":"code","301ebaf4":"code","53c1d7de":"code","9415b4cb":"code","3069c471":"code","5168faa4":"code","488b0d9d":"code","95c933ee":"code","c10f8c78":"code","16963712":"code","82c78e83":"code","6c947a1b":"code","096e468c":"code","c547caec":"code","078feb19":"code","f2433062":"code","d31a2ad2":"code","f7979b5c":"code","27e0fa50":"code","c0ba10c9":"code","8a515ee5":"markdown","af203fca":"markdown"},"source":{"e299cdd4":"# Import Libraries for reading data and computation\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\n#Import Libraries for train test split\nfrom sklearn.model_selection import train_test_split\n\n#Import Library to handle missing values\nfrom sklearn.impute import SimpleImputer\n\n# Import XGBoost module\nfrom xgboost import XGBClassifier\n\n# Confusion matrix to evaluate performance\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# AUC score to evaluate performance\nfrom sklearn.metrics import roc_auc_score\n\n# Feature scaling\nfrom sklearn import preprocessing","ec6702d7":"# Read data\nX_full = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv',index_col='id')\nX_test_full = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', index_col='id')","301ebaf4":"# Get Matrix of features (X) and Target(Y)\ny = X_full[X_full.columns[-1]] # Target\nX = X_full.drop(X_full.columns[-1], axis=1) # Features","53c1d7de":"# Making a new feature out of missing values\n# Ref: https:\/\/www.kaggle.com\/virasydoriak\/simple-logistic-regression-very-fast-with-sklearn\n# Ref: https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/discussion\/270206\n\nX['n_missing'] = X.isna().sum(axis=1)\nX_test_full['n_missing'] = X_test_full.isna().sum(axis=1)","9415b4cb":"# Shapes of all tables for reference\ndef data_table_summary(X,y,X_test):\n    \"\"\" Gives a summary of the number of rows, columns  and NaN values per dataset\"\"\"\n    print(\"Shapes of all datasets: \\n \\b Shape of y = %s \\n Shape of X = %s \\n Shape of X_test = %s\"\n          %(y.shape, X.shape, X_test.shape))\n    print(\"\\nNaN values per dataset: \\n NaN vals in y = %d \\n NaN vals in X = %d \\n NaN vals in X_test = %d\"\n         %(np.count_nonzero(np.isnan(y)),np.count_nonzero(np.isnan(X)),np.count_nonzero(np.isnan(X_test))))","3069c471":"data_table_summary(X,y,X_test_full)","5168faa4":"# Split training data into training and validation sets.\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0)","488b0d9d":"# Imputation of missing values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nX_train_im = pd.DataFrame(imputer.fit_transform(X_train))\nX_valid_im = pd.DataFrame(imputer.transform(X_valid))\n\nX_train_im.columns = X_train.columns\nX_valid_im.columns = X_valid.columns","95c933ee":"# Feature Scaling - used standard scaler here.\nscaler = preprocessing.StandardScaler()\nX_train_sc = scaler.fit_transform(X_train_im)\nX_valid_sc = scaler.transform(X_valid_im)","c10f8c78":"# # Training the XGBoost classifier on the Training set\n# classifier = XGBClassifier(random_state  = 1)\n\n# #Train-test split, evaluation metric and early stopping\n# #Ref: https:\/\/towardsdatascience.com\/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n# eval_set = [(X_train_sc, y_train), (X_valid_sc, y_valid)]\n# eval_metric = [\"auc\",\"error\"]\n# %time classifier.fit(X_train_sc, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=True)","16963712":"# #Plotting evaaluation results:\n# #Ref:https:\/\/stackoverflow.com\/questions\/51900874\/how-to-plot-xgboost-evaluation-metrics\n# results = classifier.evals_result()\n# epochs = len(results['validation_0']['error'])\n# x_axis = range(0, epochs)","82c78e83":"# fig, ax = plt.subplots()\n# ax.plot(x_axis, results['validation_0']['error'], label='Train')\n# ax.plot(x_axis, results['validation_1']['error'], label='Test')\n# ax.legend()\n# plt.ylabel('Classification error')\n# plt.title('XGBoost classification error')\n# plt.show()","6c947a1b":"# Fine tuning XGBoost model\n# Ref: https:\/\/www.kaggle.com\/mustafacicek\/tps-09-21-xgboost-0-81785\nclassifier = XGBClassifier(random_state  = 1, silent = False, scale_pos_weight = 1, learning_rate=0.06,\n                           colsample_bytree = 0.7,subsample = 0.8, objective = 'binary:logistic',\n                           eval_metric = 'error',n_estimators= 1000, reg_alpha = 3.2, reg_lambda = 0.15, \n                           max_depth=6, gamma=1,tree_method = 'gpu_hist', )","096e468c":"# Making prediction on validation set\nclassifier.fit(X_train_sc, y_train)\ny_pred = classifier.predict(X_valid_sc)","c547caec":"#Making the Confusion Matrix\ncm = confusion_matrix(y_valid, y_pred)\nprint(cm)\naccuracy_score(y_valid, y_pred)","078feb19":"# Making prediction of probabilities on validation set\ny_pred_prob = classifier.predict_proba(X_valid_sc)[:,1]","f2433062":"# Calculating AUC score\nauc_score = roc_auc_score(y_valid, y_pred_prob)\nauc_score","d31a2ad2":"#Imputation\nfinal_X_test = pd.DataFrame(imputer.transform(X_test_full))\nfinal_X_test.columns = X_test_full.columns","f7979b5c":"# Feature scaling on test set\nfinal_X_test_sc = scaler.transform(final_X_test)","27e0fa50":"# Get the test predictions\npreds_test_proba = classifier.predict_proba(final_X_test_sc)[:,1]\npreds_test_proba","c0ba10c9":"# Save test predictions to file\noutput = pd.DataFrame({'id': X_test_full.index,\n                       'claim': preds_test_proba})\noutput.to_csv('submission.csv', index=False)","8a515ee5":"# Setup -> Load data -> Preprocess -> Evaluate -> Train -> check score -> fine tune ","af203fca":"# Preprocessing the test data."}}