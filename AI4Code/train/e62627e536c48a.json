{"cell_type":{"d423de06":"code","5562af32":"code","e05312a3":"code","71a2fe34":"code","0260b9c7":"code","4e54838c":"code","5e338a74":"code","88acffc3":"code","dafe8b65":"code","93ebab03":"code","c98b01ce":"code","1f1ad565":"code","74c4a2cb":"code","a4f4b1bc":"code","005a98ba":"code","dced4e34":"code","9ad6d377":"code","574a0b21":"code","57257ef5":"code","24c71d29":"code","cbb672cb":"code","e3c92cda":"code","8d841062":"code","e36fee1d":"code","88cbeaf2":"code","0c1d4a4b":"code","443f936e":"code","f8326627":"code","cb99744a":"code","6a6986b3":"code","79bdee0c":"code","66b37b4e":"code","4b1e42f6":"code","34bced11":"code","2fe1e6ab":"markdown","a782b43c":"markdown","c0db31b8":"markdown","e8907e65":"markdown","ff5be118":"markdown","97116e64":"markdown","45f60a3b":"markdown","9e6883f5":"markdown","863f8e66":"markdown","9faa4398":"markdown","14bc5f49":"markdown","6150d9ad":"markdown","da8e5408":"markdown"},"source":{"d423de06":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings","5562af32":"df=pd.read_csv(\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndf.head()","e05312a3":"df.rename(columns={'Chance of Admit ':'chance_of_admit'},inplace=True)\ndf.head()","71a2fe34":"df.insert(8,\"chances\",0,True)\ndf.head()","0260b9c7":"df.loc[df['chance_of_admit']> 0.5, ['chances']] = '1'","4e54838c":"df.dtypes","5e338a74":"df.isnull().sum()","88acffc3":"df.describe()","dafe8b65":"df.info()","93ebab03":"df.shape","c98b01ce":"chances= df[\"chance_of_admit\"].values\ncategory = []\nfor num in chances:\n    if num <= 0.5:\n        category.append(\"Low\")\n    else:\n        category.append(\"High\")","1f1ad565":"[(i, category.count(i)) for i in set(category)]\nplt.figure(figsize=(10, 6))\nsns.countplot(category, palette=\"muted\")","74c4a2cb":"plt.figure(figsize=(12, 6))\nsns.heatmap(df.corr(), annot=True)","a4f4b1bc":"df1=df.drop(['Serial No.', 'University Rating','Research','chances'], axis=1)","005a98ba":"f,ax=plt.subplots(1,3,figsize=(25,5))\nbox1=sns.boxplot(data=df1[\"CGPA\"],ax=ax[0],color='m')\nax[0].set_xlabel('CGPA')\nbox1=sns.boxplot(data=df1[\"LOR \"],ax=ax[1],color='m')\nax[1].set_xlabel('LOR')\nbox1=sns.boxplot(data=df1[\"SOP\"],ax=ax[2],color='m')\nax[2].set_xlabel('SOP')","dced4e34":"f,ax=plt.subplots(1,2,figsize=(25,5))\nbox1=sns.boxplot(data=df1[\"TOEFL Score\"],ax=ax[0],color='m')\nax[0].set_xlabel('TOEFL Score')\nbox1=sns.boxplot(data=df1[\"GRE Score\"],ax=ax[1],color='m')\nax[1].set_xlabel('GRE Score')","9ad6d377":"import matplotlib.pyplot as plt\nimport seaborn as sns\ndf1.hist (bins=10,figsize=(20,20))\nplt.show ()","574a0b21":"sns.pairplot(df1)","57257ef5":"sns.pairplot(df,hue = 'chances', vars = ['GRE Score','TOEFL Score','SOP','LOR ','CGPA'] )","24c71d29":"fig=plt.figure(figsize=(10,6))\nsns.countplot('University Rating',data=df )\nplt.tight_layout()\nplt.show()","cbb672cb":"fig=plt.figure(figsize=(8,6))\nsns.countplot('Research',hue='chances',data=df )\nplt.tight_layout()\nplt.show()","e3c92cda":"ax = sns.distplot(df['chance_of_admit'], rug=True, hist=True)","8d841062":"ax = sns.violinplot(x=\"University Rating\", y=\"chance_of_admit\", data=df, palette=\"muted\")","e36fee1d":"df.head(1)","88cbeaf2":"df2015 = df[df.year == 2015]\ntrace1 = go.Box(    y=df2015[\"total_score\"],name = 'total score of universities in 2015',\nmarker = dict(        color = 'rgb(200, 10, 10)',\n    )\n)\ntrace2 = go.Box(\n    y=df2015[\"research\"],\n    name = 'research of universities in 2015',\n    marker = dict(\n        color = 'rgb(10, 200, 10)',\n    )\n)\n    \ntrace3 = go.Box(\n    y=df2015[\"citations\"],\n    name = 'citations of universities in 2015',\n    marker = dict(color = 'rgb(10, 10, 200)',\n    )\n)\ndata = [trace1,trace2,trace3]\niplot(data)","0c1d4a4b":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\nfrom pandas.plotting import scatter_matrix","443f936e":"x = df.iloc[:, 1:8] \ny=df['chances'].astype(int)\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=3)","f8326627":"seed=7\nmodels = []\nmodels.append(('RF',RandomForestClassifier()))\nmodels.append(('SVM',SVC()))\nmodels.append(('LR',LogisticRegression()))\nmodels.append(('NB',GaussianNB()))\n# Evaluating each models in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10,random_state=seed)\n    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" %(name, cv_results.mean(), cv_results.std())\n    print(msg)","cb99744a":"logistic = LogisticRegression()\nlogistic.fit(x_train,y_train)\ny_pred=logistic.predict(x_test)\nprint(classification_report(y_test,y_pred))\naccuracy1=logistic.score(x_test,y_test)\nprint (accuracy1*100,'%')\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot= True)","6a6986b3":"classifier=SVC()\nclassifier.fit(x_train,y_train)\nsvm_predict=classifier.predict(x_test)\nprint(classification_report(y_test,svm_predict))\naccuracy2=classifier.score(x_test,y_test)\nprint(accuracy2*100,'%')\ncm = confusion_matrix(y_test, svm_predict)\nsns.heatmap(cm, annot= True)","79bdee0c":"ran_class=RandomForestClassifier()\nran_class.fit(x_train,y_train)\nran_predict=ran_class.predict(x_test)\nprint(classification_report(y_test,ran_predict))\naccuracy3=ran_class.score(x_test,y_test)\nprint(accuracy3*100,'%')\ncm = confusion_matrix(y_test, ran_predict)\nsns.heatmap(cm, annot= True)","66b37b4e":"# Defining the decision tree algorithm\nfrom sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier()\ndtree.fit(x,y)\n\nprint('Decision Tree Classifer Created')","4b1e42f6":"!pip install pydotplus","34bced11":"# Import necessary libraries for graph viz\n!pip install --upgrade scikit-learn==0.20.3\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\n\n# Visualize the graph\ndot_data = StringIO()\nexport_graphviz(dtree, out_file=dot_data, feature_names=x.columns,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","2fe1e6ab":"**Importing Dataset**","a782b43c":"**Pairplot**","c0db31b8":"**Countplot**","e8907e65":"**Logistic Regression**","ff5be118":"**Support vector machine**","97116e64":"**Distplot**","45f60a3b":"**Histogram**","9e6883f5":"**Boxplot**","863f8e66":"**Random forest classifier**","9faa4398":"**Probelm Statement**:-\n\nPrediction of Graduate Admissions from an Indian perspective for Masters Programs. \n\nThe parameters included are :\n\n1)GRE Scores ( out of 340 )\n\n2)TOEFL Scores ( out of 120 )\n\n3)University Rating ( out of 5 )\n\n4)Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) \n\n5)Undergraduate GPA ( out of 10 ) \n\n6)Research Experience ( either 0 or 1 )\n\n7)Chance of Admit ( ranging from 0 to 1 ) is the Target Variable\n\nModels:- \n\n1)Logistic Regression \n\n2)Random Forest \n\n3)Support Vector Machine\n\n","14bc5f49":"**Train test split**","6150d9ad":"**Heatmap**","da8e5408":"**Violinplot**"}}