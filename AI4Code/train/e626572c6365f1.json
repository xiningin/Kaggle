{"cell_type":{"bf9c4d7f":"code","c18ef2ab":"code","0f8e62ae":"code","b83ff22a":"code","2b8d748f":"code","27dd0f64":"code","bfd1de25":"code","8ef5c3e6":"code","4d97f759":"code","1c75f6f5":"code","9ab24d06":"code","1a01cf10":"code","8d3bb151":"code","ada5eeb1":"code","46bab1ab":"code","61d62177":"code","31467427":"code","d5549e46":"code","97b51fe9":"code","a558c898":"code","e5ca4f84":"code","16d56c78":"code","b0611942":"code","dbb3160a":"code","fd59af51":"code","e8a8a5b9":"code","12eca40e":"code","1db8854e":"code","6fbc8af1":"code","968a92e4":"code","66c55c56":"code","69ff090f":"code","43f82fe0":"code","69aa7ab3":"code","84bc86a6":"code","b2f8f190":"code","24b62d48":"code","070023e6":"code","4dd4bad8":"code","0cad582a":"code","d97621a3":"code","bf8b4aa7":"code","581000c8":"code","3b4a495f":"code","9d6e13c2":"code","630d17d9":"code","cb40bf3d":"code","018b033f":"code","dde79c2c":"code","e1467c98":"code","cb9a82a9":"code","0d0fc989":"code","b5003751":"code","dae3499e":"code","09d19e53":"code","7ebb4aad":"code","1e240cf2":"code","ac42e2a9":"code","d824b1af":"code","2602f30f":"code","1ec47a93":"code","dc540603":"code","7052a80f":"code","d840fabf":"code","9de5eb29":"markdown","06fb6ef9":"markdown","5dae41ee":"markdown","89522a3f":"markdown","87cd7d6a":"markdown","6aea4876":"markdown","8c4a9ae0":"markdown","86f7d658":"markdown","a5000269":"markdown","937a1690":"markdown","91c64c24":"markdown","d4e66b03":"markdown","4a5d7fed":"markdown","d94ab69c":"markdown","52760cc1":"markdown","5546b148":"markdown","423e5291":"markdown","b408f49b":"markdown","25f1e0dd":"markdown","e90f4896":"markdown","26c272fc":"markdown","4b500316":"markdown","f5d1ac37":"markdown","1ab8c48c":"markdown","b6ee1c0e":"markdown","43c7b405":"markdown","264e4f83":"markdown","55cfe436":"markdown","95ba439f":"markdown","198840ac":"markdown","dd50f8de":"markdown"},"source":{"bf9c4d7f":"# Arrays \/ dataframes\nimport numpy as np\nimport pandas as pd\n\n# Plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport matplotlib.patches as mpatches\nimport contextily # for basemaps\n\n# Spatial data management\nimport geopandas as gpd\nimport pyproj # projection info\nfrom shapely.geometry import Polygon # shapely geometry object\nfrom descartes import PolygonPatch # for plotting shapely geometries\n\n# Spatial analysis\nfrom pointpats.centrography import hull, minimum_bounding_circle # enclosing geometries\nfrom pointpats import random, PointPattern, quadrat_statistics, ripley\nfrom sklearn.cluster import DBSCAN # point clustering\nfrom pysal.lib import weights # spatial weights\nfrom pysal.explore import esda # spatial autocorrelation","c18ef2ab":"df = pd.read_csv('..\/input\/earthquake\/earthquake.csv')\ndf.info()","0f8e62ae":"df.head()\n\n# We have features describing the location of each event:\n# lat, long, country, city, area\n\n# Then we have features describing the earthquake:\n# direction, dist,depth, xm, md, richter, mw, ms, mb","b83ff22a":"# What values do we have for direction?\n\ndf['direction'].unique()","2b8d748f":"# It may be easier to manage direction data as azimuths\n\nazimuths = {'north': 0.0,\n            'north_east': 45.0,\n            'east': 90.0,\n            'south_east': 135.0,\n            'south': 180.0,\n            'south_west': 225.0,\n            'west': 270.0,\n            'north_west': 315.0}\n\ndf['dir_azimuth'] = df['direction'].map(azimuths)\ndf[['direction', 'dir_azimuth']].head()","27dd0f64":"# As seen above with df.info(), numeric columns are already stored as np.float64. We\n# can use df.corr() to view correlations between numeric features\n\nfig, ax = plt.subplots(1, figsize=(12,5))\n\n# Also drop ID column, not really relevant (its date\/time based ID)\nsns.heatmap(df.drop(columns='id').corr(), annot=True, ax=ax, cmap='icefire')\nplt.show()","bfd1de25":"# See how dates and times are currently formatted\ndf[['date', 'time']].head()\n\n# Months and days are zero padded, and years and 4 digits\n# Minutes and seconds are zero padded, and with 'AM' is looks like a 12hr clock","8ef5c3e6":"# It's not clear if hours are zero padded, though. Can only see 12:__:__ in the head\n# Parse time values to check...\n\ntime_split = df['time'].str.split(':')\n\n# Find all unique first values from split lists (ex.: [12, 17, 44 AM])\nhours = time_split.str[0]\nhours.unique()\n\n# Can now see that\n# 1) the hours are NOT zero padded, and\n# 2) the only hours in the data are 12 and 1, which means either all earthquakes\n#    happen at 12\/1 AM or PM (unlikely) or there's some sort of problem with the\n#    time data. Maybe this is the time that earthquakes got recorded in the database\n#    the night following the event or something?","4d97f759":"# Define a new df column with the hours only\n\ndf['hour'] = df['time'].str.split(':').str[0]\n\n# Use .loc[] to index df where length of the hour column is 1\n# In these rows, add a zero to the beginning of the time column\n# to zero-pad the hour\n\ndf.loc[df['hour'].str.len()==1, 'time'] = '0'+df['time']\n\n# View new set of unique hours within the time column to confirm zero-padding\n\ndf['time'].str.split(':').str[0].unique()","1c75f6f5":"# Concatenate the date and time columns to a datetime column\n\ndf['datetime'] = df['date'] + ' ' + df['time']\ndf['datetime'].head()","9ab24d06":"# Convert this to an actual pandas datetime and drop the other date \/ time columns\n\ndf['datetime'] = pd.to_datetime(df['datetime'], format='%Y.%m.%d %H:%M:%S %p')\ndf.drop(columns=['date', 'time', 'hour'], inplace=True)\ndf['datetime'].head()","1a01cf10":"fig = plt.figure(figsize=(12,18))\n\ncols = ['dist', 'depth', 'xm', 'md', 'richter', 'mw', 'ms', 'mb', 'dir_azimuth']\n\ni = 1\nfor col in cols:\n    plt.subplot(5, 2, i)\n    sns.histplot(data=df, x=col, kde=True)\n    i += 1\n\nplt.subplots_adjust(hspace=0.5, wspace=0.2)\nplt.show()","8d3bb151":"fig, ax = plt.subplots(1, figsize=(10,6))\nsns.countplot(data=df, y='country', ax=ax)\nplt.title('Earthquake count by country')\nplt.show()","ada5eeb1":"# Start by simply looking at number of earthquakes each year in dataset\nyear_quakes = df.groupby(df['datetime'].dt.year).size()\n\n# Also calculate the ten-year moving average\ntenyear_MA = year_quakes.rolling(10).mean()\n\n# Generate time series lineplot\nfig, ax = plt.subplots(1, figsize=(12,5))\nsns.lineplot(ax=ax, x=year_quakes.index, y=year_quakes.values, label='Annual earthquakes')\nsns.lineplot(ax=ax, x=year_quakes.index, y=tenyear_MA, label='Ten-year moving average')\nax.set_title('Earthquakes per year')\nax.set(xlabel='Year', ylabel='Recorded earthquakes')\nax.grid(True)\nplt.show()","46bab1ab":"# What about monthly trends in earthquakes?\n\nmonth_quakes = df.groupby(df['datetime'].dt.month).size()\n\nfig, ax = plt.subplots(1, figsize=(12,4))\nsns.lineplot(ax=ax, x=month_quakes.index, y=month_quakes.values)\nax.set_title('Cumulative earthquakes per month')\nax.set(xlabel='Month', ylabel='Recorded Earthquakes')\nplt.xticks(ticks=range(1, 13))\nax.grid(True)\nplt.show()","61d62177":"border0_url = 'https:\/\/raw.githubusercontent.com\/uyasarkocal\/borders-of-turkey\/master\/lvl0-TR.geojson'\nborder1_url = 'https:\/\/raw.githubusercontent.com\/uyasarkocal\/borders-of-turkey\/master\/lvl1-TR.geojson'\n\nborder0 = gpd.read_file(border0_url)\nborder1 = gpd.read_file(border1_url)\n\n# Check coordinate systems to ensure consistency\nprint(f'border0 CRS: {border0.crs}\\n'\n      f'border1 CRS: {border1.crs}\\n')","31467427":"# Plot border data\n\nfig, ax = plt.subplots(1, figsize=(14,14))\nborder1.plot(ax=ax, color='None', edgecolor=(0.3, 0.3, 0.3), linewidth=0.75)\nborder0.plot(ax=ax, color='None', edgecolor='black', linewidth=1.5);\nplt.grid(True)\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()","d5549e46":"# Convert our earthquake dataframe to a geodataframe\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['long'], df['lat']))\n\n# Define CRS as WGS 84, presumed from lat\/long coordinates\ngdf = gdf.set_crs(epsg=4326)\n\nprint(f'gdf CRS: {gdf.crs}')\nprint(f'gdf total_bounds: {gdf.total_bounds}')","97b51fe9":"# Plot data over the Turkey country border\nfig, ax = plt.subplots(1, figsize=(12,12))\ngdf.plot(ax=ax, markersize=0.5, color='r')\nborder0.plot(ax=ax, color='None', edgecolor='k', linewidth=1)\nborder1.plot(ax=ax, color='None', edgecolor='k', linewidth=0.5)\n\n# Also use contextily to add a basemap for additional geographic context\ncontextily.add_basemap(ax=ax, crs='epsg:4326')\n\nplt.title('Earthquakes 1910-2017, Turkey')\nplt.grid(True)\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n# We can see the geographic extent described in the dataset description now.\n# There definitely seems to be some clustering of points too","a558c898":"# It also looks like there's some sort of a cutoff of data density around longitude\n# 23, is this due to some limit on the data record west of this parallel? Or is this\n# some natural phenomena?\n\nfig, ax = plt.subplots(1, figsize=(12,12))\ngdf.plot(ax=ax, markersize=1, color='r', alpha=0.3)\nborder0.plot(ax=ax, color='None', edgecolor='k', alpha=0.5, linewidth=1)\nborder1.plot(ax=ax, color='None', edgecolor='k', alpha=0.5, linewidth=0.5)\ncontextily.add_basemap(ax=ax, crs='epsg:4326', alpha=0.2)\nplt.axvline(x=23, linestyle='--', linewidth=2, color='blue')\nplt.title('Earthquakes 1910-2017, Turkey')\nplt.grid(True)\nplt.text(23.5, 31, '<- Possible data constraint\/limit near Longitude 23?')\nplt.show()","e5ca4f84":"plates_url = 'https:\/\/raw.githubusercontent.com\/fraxen\/tectonicplates\/master\/GeoJSON\/PB2002_plates.json'\nall_plates = gpd.read_file(plates_url)\n\n# This dataset covers the whole earth. Let's filter it with the bounding box of the\n# earthquake data\nplates = all_plates.cx[gdf.total_bounds[0]:gdf.total_bounds[2], gdf.total_bounds[1]:gdf.total_bounds[3]]","16d56c78":"fig, ax = plt.subplots(1, figsize=(12,12))\n\ngdf.plot(ax=ax, markersize=0.5, color='r')\nplates.plot(ax=ax, color='None', edgecolor='k', linewidth=2, linestyle='--')\n\n# Restrict plot to bounds of dataset\nax.set_xlim(gdf.total_bounds[0], gdf.total_bounds[2])\nax.set_ylim(gdf.total_bounds[1], gdf.total_bounds[3])\n\ncontextily.add_basemap(ax=ax, crs='epsg:4326')\nplt.title('Tectonic plates')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n# On first glance, it appears that the earthquake points tend to cluster along\n# tectonic plate intersections. No surprise there","b0611942":"# Use pyproj to get info on WGS84\npyproj.CRS('epsg:4326')","dbb3160a":"# Define PROJECTED_CRS string variable to be used throughout rest of notebook\n\nPROJECTED_CRS = 'ESRI:102422'\npyproj.CRS(PROJECTED_CRS)","fd59af51":"gdf_prj = gdf.to_crs(crs=PROJECTED_CRS)\nborder0_prj = border0.to_crs(crs=PROJECTED_CRS)\nplates_prj = plates.to_crs(crs=PROJECTED_CRS)\n\nprint(gdf_prj['geometry'].head())\nprint('\\nCRS Info:\\n', gdf_prj.crs)","e8a8a5b9":"# Set random seed\nnp.random.seed(1)","12eca40e":"# Define array of point coordinates\nearthquake_pts = np.array([gdf_prj['geometry'].x, gdf_prj['geometry'].y]).T\n\n# Define point pattern object using coordinates\nearthquake_pp = PointPattern(earthquake_pts)\nearthquake_pp.summary()","1db8854e":"# Get convex hull of point pattern\nearthquake_hull = Polygon(hull(earthquake_pts))","6fbc8af1":"fig, ax = plt.subplots(1, figsize=(8,6))\ngdf_prj.plot(ax=ax, markersize=1, color='red')\n            \n# Use descartes PolygonPatch to convert the Shapely object\nax.add_patch(PolygonPatch(earthquake_hull, fc='None'))\nplt.axis('equal')\nplt.title('Earthquake points and their convex hull')\nplt.show()","968a92e4":"# Generate random points within bounds of the convex hull. This returns a NumPy\n# array of coordinates\n\nrandom_pts = random.poisson(earthquake_hull, size=gdf_prj.shape[0])\n\n# Also generate random points within the rectangular bounding box of the dataset\nrandom_pts_rect = random.poisson(gdf_prj.total_bounds, size=gdf_prj.shape[0])","66c55c56":"# Plot the random points and the hull\n\nfig, ax = plt.subplots(1, figsize=(8,6))\nplt.scatter(random_pts[:,0], random_pts[:,1], s=1)\nax.add_patch(PolygonPatch(earthquake_hull, fc='None'))\nplt.axis('equal')\nplt.title('Random points within dataset\\'s convex hull')\nplt.show()","69ff090f":"fig, ax = plt.subplots(1, figsize=(8,6))\nplt.scatter(random_pts_rect[:,0], random_pts_rect[:,1], s=1)\nplt.axis('equal')\nplt.title('Random points within dataset\\'s bounding box')\nplt.show()","43f82fe0":"# Define function that will produce a quadrat count plot and print out information\n# about that quadrat statistics\n\ndef produce_qstats(point_pattern, nx, ny, title='Quadrat Count', plot=True):\n    # Rectangular quadrats\n    qstat = quadrat_statistics.QStatistic(point_pattern, nx=nx, ny=ny)\n    \n    if plot:\n        qstat.plot(title)\n\n    print('How likely is it that this point distribution is an outcome\\n'\n      'of complete spatial randomness (CSR)?\\n\\n'\n      'Chi-squared p-value of distribution:\\n'\n     f'{round(qstat.chi2_pvalue, 5)}')\n\n    if qstat.chi2_pvalue < 0.05:\n        print('\\np-value is less than 0.05\\nReject null hypothesis of CSR')\n    else: print('\\np-value is greater than or equal to 0.05'\n                '\\nFail to reject null hypothesis of CSR')","69aa7ab3":"# First, run a quadrat count on the rectangularly-bound random point pattern\nproduce_qstats(random_pts_rect, 8, 8, 'Random points within bounding box: quadrat count')","84bc86a6":"produce_qstats(earthquake_pts, 8, 8, 'Earthquake points: quadrat count')","b2f8f190":"produce_qstats(random_pts, 8, 8, 'Random points within convex hull: quadrat count')","24b62d48":"produce_qstats(random_pts_rect, 50, 50, plot=False)","070023e6":"produce_qstats(earthquake_pts, 50, 50, plot=False)","4dd4bad8":"produce_qstats(random_pts, 50, 50, plot=False)","0cad582a":"# Run g_test() on the earthquake points at 200 distance intervals. Compare this\n# to G statistic of 100 random point distributions. Restrict analysis to the\n# convex hull. Keep the simulation results for visualization.\n\ng_test = ripley.g_test(earthquake_pts, support=200,\n                       n_simulations=100, hull=earthquake_hull,\n                       keep_simulations=True)","d97621a3":"fig, ax = plt.subplots(1, figsize=(12,6))\n\n# Identfy the max and min simulations for random points G function\nmin_sim = np.min(g_test.simulations, axis=0)\nmax_sim = np.max(g_test.simulations, axis=0)\n\n# Plot these as a simluation envelope, coloring plot between max\/min\nfill = ax.fill_between(g_test.support, y1=max_sim, y2=min_sim, alpha=0.5)\n\n# Plot the observed G function\nobserved = plt.plot(g_test.support, g_test.statistic,\n                    marker='x', color='orangered', label='observed')\n\n# Limit x axis to where both curves approach 1\nax.set_xlim(0,18000)\n\nax.legend([fill, observed[0]], ['Simulation Envelope', 'Observed'])\nplt.xlabel('r (distance, meters)')\nplt.ylabel('G(r)')\nplt.title('Earthquake points: Ripley\\'s G function')\nplt.show()","bf8b4aa7":"# Plot kernel density estimation plot to visualize where point density\n# is concentrated\n\nfig, ax = plt.subplots(1, figsize=(12,8))\nsns.kdeplot(x=gdf_prj['geometry'].x, y=gdf_prj['geometry'].y,\n            ax=ax, shade=True, cmap='flare_r', alpha=0.7)\nborder0_prj.plot(ax=ax, fc='None', ec='white')\nax.axis('equal')\ncontextily.add_basemap(ax=ax, crs=PROJECTED_CRS)\nplt.title('Earthquake points: KDE plot')\nplt.show()","581000c8":"# Run sklearn's DBSCAN to find clusters, set maximum distance to 15 kilometers\n# Minimum number of samples in a cluster is 100\n\ndbscan = DBSCAN(eps=15000, min_samples=100)\ndbscan.fit(earthquake_pts)","3b4a495f":"# See unique labels, noise is labeled -1, clusters have unique labels >= 0\nnp.unique(dbscan.labels_)","9d6e13c2":"# Define function to plot dbscan results on a map, will reuse this a few times\n\ndef plot_dbscan(points, dbscan, title, pt_sizer=1, plot_circles=False):\n    # Index noise and clusters out of the dbscan points\n    noise = points[dbscan.labels_ == -1]\n    clusters = points[dbscan.labels_ != -1]\n    \n    # Plot country border and tectonic plates\n    fig, ax = plt.subplots(1, figsize=(12,8))\n    border0_prj.plot(ax=ax, fc='None', ec='k', linewidth=1.5)\n    plates_prj.plot(ax=ax, fc='None', ec='b', linestyle='-.')\n    \n    # Allow relative point size adjustment with pt_sizer argument\n    sns.scatterplot(x=noise[:,0], y=noise[:,1], ax=ax, alpha=1, s=2*pt_sizer, color='gray')\n    sns.scatterplot(x=clusters[:,0], y=clusters[:,1], ax=ax, s=4*pt_sizer, color='red')\n    \n    # Option to plot a minimum bounding circle around each cluster\n    if plot_circles:\n        for label in np.unique(dbscan.labels_):\n            if label != -1:\n                cluster_points = points[dbscan.labels_ == label]\n                # Get minimum bounding circle using pointpats.centrography.minimum_bounding_circle()\n                (center_x, center_y), radius = minimum_bounding_circle(cluster_points)\n                # Create matplotlib patch\n                circle_patch = mpatches.Circle((center_x, center_y), radius=radius, fc='None', ec='yellow', linewidth=2)\n                ax.add_patch(circle_patch)\n\n    ax.axis('equal')\n    \n    # Limit bounds of plot to earthquake data\n    ax.set_xlim(gdf_prj.total_bounds[0], gdf_prj.total_bounds[2])\n    ax.set_ylim(gdf_prj.total_bounds[1], gdf_prj.total_bounds[3])\n\n    # Manually prepare legend items\n    border0_l = mlines.Line2D([], [], color='k', linewidth=1.5, label='Turkey Border')\n    plates_l = mlines.Line2D([], [], color='b', linestyle='-.', label='Tectonic Plates')\n    noise_l = mlines.Line2D([], [], marker='.', linewidth=0, markersize=4, \n                            color='gray', label='Noise')\n    \n    if plot_circles:\n        # Draw yellow circle around red point for legend\n        mec = 'yellow'\n    else:\n        mec = 'None'\n        \n    clusters_l = mlines.Line2D([], [], marker='.', linewidth=0,\n                               markersize=12, color='red', markeredgecolor=mec,\n                               label='DBSCAN Clusters')\n    # Define legend\n    plt.legend(handles=[border0_l, plates_l, noise_l, clusters_l])\n    \n    plt.title(title)\n    \n    # Add basemap\n    contextily.add_basemap(ax=ax, crs=PROJECTED_CRS, alpha=0.5)\n    \n    plt.show()","630d17d9":"plot_dbscan(earthquake_pts, dbscan,\n            title='DBSCAN: 15km max distance, 100 point minimum',\n            plot_circles=True)","cb40bf3d":"dbscan = DBSCAN(eps=25000, min_samples=200)\ndbscan.fit(earthquake_pts)\nnp.unique(dbscan.labels_)","018b033f":"plot_dbscan(earthquake_pts, dbscan,\n            title='DBSCAN: 25km max Distance, 200 point minimum',\n            plot_circles=True)","dde79c2c":"# Calculate spatial weights, which describe spatial relationships between the\n# data, using k-nearest neighbors\n\n# Choose k based on simple rule of thumb -> sqrt(n), which is ~155 in this case\nk = round(np.sqrt(gdf_prj.shape[0]))\n\nweights_knn = weights.KNN.from_dataframe(gdf_prj, k=k)","e1467c98":"# Calculate spatial lag of xm (highest magnitude) feature\n\ngdf_prj['xm_slag'] = weights.spatial_lag.lag_spatial(weights_knn, gdf_prj['xm'])","cb9a82a9":"# Standardize xm and xm_slag to center on mean\n\ngdf_prj['xm_std'] = (gdf_prj['xm'] - gdf_prj['xm'].mean())\/gdf_prj['xm'].std()\ngdf_prj['xm_slag_std'] = (gdf_prj['xm_slag'] - gdf_prj['xm_slag'].mean())\/gdf_prj['xm_slag'].std()","0d0fc989":"# Calculate Moran's I statistic using pysal.explore.esda.Moran()\n\nmoran = esda.Moran(gdf_prj['xm'], weights_knn)","b5003751":"# Print information about Moran's statistic\nprint('Null hypothesis:\\n'\n      'CSR with no spatial autocorrelation\\n'\n      '===================================\\n'\n     f'Moran\\'s I: {round(moran.I, 4)}\\n\\n'\n     f'p-value based on simulations: {moran.p_sim}')\n\nif moran.p_sim < 0.05:\n    print('--> Reject null hypothesis')\nelse:\n    print('-->Fail to reject null hypothesis')","dae3499e":"fig, ax = plt.subplots(1, figsize=(8,8))\nsns.regplot(x=gdf_prj['xm_std'], y=gdf_prj['xm_slag_std'], ax=ax,\n            scatter_kws={'s':1}, line_kws={'color':'red'})\nax.axis('equal')\nax.set_xlim(-6,6)\nax.set_ylim(-6,6)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.axhline(y=0, color='k', linewidth=0.5)\nplt.xlabel('xm (standardized)')\nplt.ylabel('xm spatial lag (standardized)')\nplt.title(f'xm Moran\\'s I: {round(moran.I, 5)}')\nplt.show()","09d19e53":"# Whats the most severe earthquake in the dataset?\n\nmax_xm = gdf_prj['xm'].max()\ngdf_prj.loc[gdf_prj['xm']==max_xm]","7ebb4aad":"# Earthquakes with magnitudes 6 or greater are generally considered \"severe\"\n# Filter these out\n\nsevere_prj = gdf_prj[gdf_prj['xm'] > 6]\nsevere_prj.info()","1e240cf2":"# Drop the spatial lag variables and associates standardized variables as they no\n# longer apply to the new set of points\n\nsevere_prj = severe_prj.drop(columns=['xm_slag', 'xm_std', 'xm_slag_std'])","ac42e2a9":"# Min-Max Scale the xm feature to get range of 0 - 1\n# will use this to dynamically size points based on the their xm value\nsizes = (severe_prj['xm'] - severe_prj['xm'].min())\/(severe_prj['xm'].max() - severe_prj['xm'].min())\n\n# Add to all the sizes so smallest is 0.2, then spread the range further\n# with multiplication\/exponentiation\nsizes = ((sizes+0.2)*20)**2\n\nsizes.min(), sizes.max()","d824b1af":"fig, ax = plt.subplots(1, figsize=(12,8))\nplates_prj.plot(ax=ax, color='None', edgecolor='k', linewidth=2, linestyle='--')\n\n# Plot using previously computed dynamic point sizes\nsevere_prj.plot(ax=ax, markersize=sizes, fc='r', ec='k', alpha=0.6)\n\nax.axis('equal')\n# Restrict plot to bounds of dataset with 50km buffer\nax.set_xlim(severe_prj.total_bounds[0]-50000, severe_prj.total_bounds[2]+50000)\nax.set_ylim(severe_prj.total_bounds[1]-50000, severe_prj.total_bounds[3]+50000)\n\ncontextily.add_basemap(ax=ax, crs=PROJECTED_CRS)\n\nplt.title('Severe earthquakes, tectonic plates')\nplt.show()","2602f30f":"# Generate a point coordinate array from severe earthquake points\nsevere_pts = np.array([severe_prj['geometry'].x, severe_prj['geometry'].y]).T\n\n# Define convex hull of severe_pts\nsevere_hull = Polygon(hull(severe_pts))","1ec47a93":"# Run g_test() on the severe earthquake points at 100 distance intervals. Compare this\n# to G statistic of 200 random point distributions. Restrict analysis to the\n# convex hull. Keep the simluation results for visualization.\n\nsevere_g_test = ripley.g_test(severe_pts, support=100,\n                              n_simulations=200, hull=severe_hull,\n                              keep_simulations=True)","dc540603":"fig, ax = plt.subplots(1, figsize=(12,6))\n\n# Identfy the max and min simulations for random points G function\nmin_sim = np.min(severe_g_test.simulations, axis=0)\nmax_sim = np.max(severe_g_test.simulations, axis=0)\n\n# Plot these as a simluation envelope, coloring plot between max\/min\nfill = ax.fill_between(severe_g_test.support, y1=max_sim, y2=min_sim, alpha=0.5)\n\n# Plot the observed G function\nobserved = plt.plot(severe_g_test.support, severe_g_test.statistic,\n                    marker='x', color='orangered', label='observed')\n\n# Limit x axis to where both curves approach 1\nax.set_xlim(0,180000)\n\nax.legend([fill, observed[0]], ['Simulation Envelope', 'Observed'])\nplt.xlabel('Distance (meters)')\nplt.ylabel('G Function')\nplt.title('G Function Plot: Severe Earthquakes')\nplt.show()","7052a80f":"# While playing with the DBSCAN parameters, it was generally \"harder\" to identify\n# clusters\n\ndbscan = DBSCAN(eps=40000, min_samples=4)\ndbscan.fit(severe_pts)\nnp.unique(dbscan.labels_)","d840fabf":"plot_dbscan(severe_pts, dbscan,\n            title='DBSCAN: Severe earthquakes, 40km max distance, 4 point minimum',\n            pt_sizer=10)","9de5eb29":"### Notes on this G test\n\nNotice the width of the simulation envelope, where *n*=147 relative to the previous G test, where *n*=24007. Although the observed pattern is still convincingly above the simulation envelope at distances from 10km - 100km, suggesting non-random clustering, it's less obvious than when *n* was much larger.","06fb6ef9":"### Notes on the above histograms\n- Most of the magnitude values have large counts of values at 0.0. The dataset description explains that 0.0 magnitude values means a given magnitude type was not recorded for that earthquake\n- Notably, it appears that ```xm``` does not have 0.0 values. ```df.info()``` showed us that no null values were present for that column either. Perhaps this is the most useful feature to generalize earthquake severity, since ```richter``` has a very high count of 0.0 values\n- For direction values, NE, SE, SW, and NW and considerably more common that N, S, E, and W. Perhaps this has to do with the orientation of local fault lines?","5dae41ee":"The Moran's *I* value is **greater than 0**, which describes positive spatial autocorrelation (clustering) of ```xm```. It's not all that high of a positive value (perfect clustering has a Moran's *I* of 1), but the Moran's *I* value on its own does not imply anything about statistical significance, so it may be incorrect to call this \"weak\" spatial autocorrelation as you would with a Pearson correlation coefficient.\n\n### Moran's plot\nThis phonomena can also be visualized with a Moran's plot. The upper right and lower left quadrants of the plot display clustering. The upper right quadrant shows high ```xm``` values that are surrounded by other high ```xm``` values, and the lower left quadrant shows low ```xm``` values that are surrounded by other low ```xm``` values.","89522a3f":"# Format datetime column","87cd7d6a":"# References\nAgain, my primary resource was:\n- Sergio J. Rey, Dani Arribas-Bel, and Levi J. Wolf [*Geographic Data Science with Python*](https:\/\/geographicdata.science\/book\/intro.html)\n\nOthers:\n- [Point Pattern Analysis, Topic 5, Geog 418\/518, September 7, 2020](https:\/\/storymaps.arcgis.com\/stories\/2d7ebcc8ae5c4df3b8ad56a3e9a82317)\n- DongMei Chen and Arthur Getis: [Point Pattern Analysis (PPA)](http:\/\/ceadserv1.nku.edu\/longa\/\/geomed\/ppa\/doc\/html\/ppa.html)\n- Jose Parreno Garcia: [Point Pattern Analysis](https:\/\/joparga3.github.io\/spatial_point_pattern)\n- The [pointpats library documentation](https:\/\/pointpats.readthedocs.io\/en\/latest\/)\n- [PySAL Notebooks](https:\/\/pysal.org\/notebooks\/intro.html)","6aea4876":"# View Data Geographically\n\nThere's a [GitHub repo from uyasarkocal](https:\/\/github.com\/uyasarkocal\/borders-of-turkey) that contains three GeoJSON files with Turkey's borders at various levels (country, district, and city). This will help give some geographic context to the data.","8c4a9ae0":"### Notes on Correlations\n\nSome moderate to strong correlations are present within the various magnitude types. These are generally positively correlated with one another *e.g.*, higher surface wave magnitudes (```ms```) are correlated with higher body wave magnitudes (```mb```).\n\nThere's also two interesting negative correlations:\n- Between ```depth``` and ```lat```: Higher depth values (deeper quakes) are weakly correlated with lower latitudes (further south)\n- Between ```md``` and ```richter```: Md is based on \"duration of shaking\", so longer duration quakes (?) are weakly correlated with lower Richter scale values","86f7d658":"# Potential next steps \/ To-Dos\n- Use DBSCAN clusters and\/or KDE patterns with demographic data to identify communities that face particular earthquake risk and thus may require additional focus on earthquake-safe infrastructure\/emergency planning\n- Dig deeper into time series patterns. Is there any merit to the apparent seasonality shown in the above time series plots?\n- Reference any other datasets available for earthquakes in this area to confirm or challenge any patterns uncovered here\n- Apply some sort of time series regression model to try to forecast earthquakes (although this [may be impossible](https:\/\/www.usgs.gov\/faqs\/can-you-predict-earthquakes?qt-news_science_products=0#qt-news_science_products))\n- Explore association between earthquake depth or direction with local tectonic plate movement data\n- Evaluate how much distance to tectonic plate boundaries correlates with earthquake frequency and\/or severity\n- Look at historic damage data following earthquakes to predict damage costs based on earthquake parameters or proximity to earthquake center, etc.","a5000269":"### Ripley's G for severe earthquakes","937a1690":"# Explore Data Temporally","91c64c24":"We can use this weight matrix to calculate spatial lag for variables. Spatial lag is described in [Geographic Data Science with Python](https:\/\/geographicdata.science\/book\/notebooks\/06_spatial_autocorrelation.html):\n> The spatial lag operator is one of the most common and direct applications of spatial weights matrices (\ud835\udc16\u2019s) in spatial analysis. The mathematical definition is the product of \ud835\udc16 and the vector of a given variable. Conceptually, the spatial lag captures the behavior of a variable in the immediate surroundings of each location; in that respect, it is akin to a local smoother of a variable.","d4e66b03":"### Notes this G test\n\nThe curve of the G function for the earthquake points sits reliably above the simulation envelope which provides evidence of spatial clustering. Note the tightness of the simulation envelope--all 100 simulations fell within that range. If our point pattern had fewer points, the simulated random patterns would have fewer points and the simulation envelope would generally be wider (this is demonstrated below on a G test with a subset of the data). The increasing tightness of this envelope with increasing size is a good representation of the [law of large numbers](https:\/\/en.wikipedia.org\/wiki\/Law_of_large_numbers).","4a5d7fed":"We can see above that the earthquake with the highest ```xm``` value occurred in late December 1939 in the city of Erzincan. Brief research reveals that this event was the \"most severe natural loss of life in Turkey in the 20th century, with with 32,968 dead, and some 100,000 injured\" [(Wikipedia)](https:\/\/en.wikipedia.org\/wiki\/1939_Erzincan_earthquake). ","d94ab69c":"# Point pattern analysis: spatial dispersion\n\nAlthough the data certainly looks clustered, let's evaluate the degree of clustering with spatial statistics. We want to compare the point distribution of the earthquake data to a random point distribution. Conceptually this is referred to as [*Complete Spatial Randomness* (CSR)](https:\/\/en.wikipedia.org\/wiki\/Complete_spatial_randomness). Our intent is to **reject the null hypothesis of CSR** if we want to say anything significant about the spatial component of the data.","52760cc1":"### Ripley's G test\n\nRipley's G function is a more robust test to evaluate the randomness or clustering of a point pattern. At various distances *r*, it assesses what proportion of the points have their nearest neighbor within that distance. The function G(*r*) therefore ranges from 0 to 1. We can compare G(*r*) for our point distribution to simulated random point distributions to see if our point pattern deviates from the random patterns.","5546b148":"# Point pattern analysis: identifying clusters\n### Kernel density estimation","423e5291":"Luckily, users in the [discussion for this dataset](https:\/\/www.kaggle.com\/caganseval\/earthquake\/discussion\/72492) posted some helpful comments, pointing to the [dataset's original website](http:\/\/www.koeri.boun.edu.tr\/sismo\/2\/earthquake-catalog\/) for some additional metadata on some of the less recognizable features for those who aren't familiar with seismology (like myself):\n\n**xm**: Largest magnitude value in specified magnitude values (MD, ML, Mw, Ms and Mb)\n\nmd, mw, ms, mb: Magnitude types\n- **MD**: Duration\n- **Mw**: Moment\n- **Ms**: Surface wave\n- **Mb**: Body-wave\n0.0 (zero) means no calculation for that type of magnitude.\n\nUSGS has a [helpful reference](https:\/\/www.usgs.gov\/natural-hazards\/earthquake-hazards\/science\/magnitude-types?qt-science_center_objects=0#qt-science_center_objects) that describes the different magnitude types.","b408f49b":"Notice how the bounding geometry influences the quadrat statistics. The random points bound by the dataset's convex hull could not be called \"random\" when using regularly spaced rectangular quadrats, which is a bit counterintuitive. Similar results occur when using 50 rows and 50 columns of equally spaced rectangular quadrats.\n\n### 50x50 Quadrats\nWon't bother plotting because it's impossible to see anything","25f1e0dd":"# Earthquakes in Turkey 1910 - 2017: Exploratory data analysis, spatial statistics, spatial clustering, mapping\n\nThis notebook is pretty heavily commented and marked down, so hopefully it can function like a tutorial as well. Please comment if you have any suggestions for improvements!\n\n***A CRITICAL RESOURCE:\nMuch of the point pattern analysis and spatial statistics are adaptations of the workflows and code snippets outlined in\n[Geographic Data Science with PySAL and the PyData Stack](https:\/\/geographicdata.science\/book\/intro.html) by Sergio J. Rey, Dani Arribas-Bel, & Levi J. Wolf. Check out their fantastic free resource!***","e90f4896":"# A closer look at severe earthquakes","26c272fc":"# Spatial autocorrelation of earthquake severity\n\nThe website [Spatial Data Science with R](https:\/\/rspatial.org\/raster\/analysis\/3-spauto.html) provides a useful description of spatial autocorrelation:\n> Measures of spatial autocorrelation describe the degree two which observations (values) at spatial locations (whether they are points, areas, or raster cells), are similar to each other. So we need two things: observations and locations.\n\nWe could look at the spatial autocorrelation of any of the features in this dataset. One feature of particular interest may be the intensity\/severity of the earthquakes, as described by the ```xm``` feature.","4b500316":"A simple way to test against CSR is with a **quadrat analysis**, where you simply split the points into regularly spaced bins and count the number of points in each bin. You can compare these counts to an expected count of points in each bin under CSR. It's simple and easy to understand, but selecting the number of quadrats to split a distribution into is arbitrary and can have a huge effect on results. Furthermore, it's not optimized to deal with irregularly shaped study areas like the convex hull defined above. Thus the method is not very robust, but it's a good place to start nonetheless.\n\n### 8x8 Quadrats","f5d1ac37":"# Imports","1ab8c48c":"### Tectonic plates\n\nLet's pull in GeoJSON data showing the location of earth's tectonic plates. This is from [fraxen's GitHub repo](https:\/\/github.com\/fraxen\/tectonicplates).","b6ee1c0e":"# Projecting the data\n\nThus far, all the spatial data has been in the WGS 84 coordinate system, where (x, y) locations are defined with (long, lat) coordinates. WGS 84 is convenient for global mapping and is one of the de facto coordinate systems for distributing location data over the web. However, measuring distances or calculating areas using WGS 84 possesses a number of problems:\n\n- Distances of degrees are not intuitive like miles, kilometers, etc.\n- The distance between parallels at the equator is greater than the distance between parallels away from the equator. These distances approach zero toward the poles.\n- Actual distance measurements with geographic coordinate systems are *geodetic distances*, which are closer to ellipsoidal arc lengths (or great circle distances) rather that Cartesian distances. In other words, Euclidean distances between geographic coordinates do not follow the ellipsoidal surfuce defined in the coordinate system.\n- PySAL and other analytical tools are designed for projected spatial data, and thus use Cartesian distance formulas (*e.g.*, Euclidean distance).\n\n[projpicker](https:\/\/projpicker.readthedocs.io\/en\/latest\/) is a great tool to use to identify which coordinate system to use for a given study area. If this dataset was limited to Turkey I'd use a Turkish coordinate system, but unfortunately it stretches beyond the border significantly. The study area passes through multiple UTM zones and is on the edges of larger continental coordinate systems, so there aren't many great options.\n\nI ended up choosing WGS 84 ARC System Zone 2 (ESRI:102422), an equidistant cylindrical projection. It aligns grid north with true north well. Other options, such as [EPSG:5636](https:\/\/epsg.io\/5636) or [EPSG:6931](https:\/\/epsg.io\/6931), may have been more appropriate, but those projections rotate the orientation of the data which gets a bit confusing cartographically.","43c7b405":"### Map earthquake severity with dynamic point sizing","264e4f83":"# Feature Correlations","55cfe436":"### DBSCAN\n\nDBSCAN is a commonly used point cluster algorithm in point pattern analysis. Here's a good description from [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/DBSCAN):\n\n> Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander and Xiaowei Xu in 1996. It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.","95ba439f":"# Read data","198840ac":"### Notes on earthquakes-per-year\n\nThe plot above shows earthquakes picking up in the early 60's and generally increasing since then. This would be very strange for natural phenomena. Are more earthquakes occurring, or are more being recorded in the data? My hunch is the latter. Performing analyses on the whole dataset may not be the best approach if the data is incomplete. Spatial analysis of events generally assume that *all* events within a given subset of time and space are recorded included in the data. This dataset almost certainly does not have a complete record, and any conclusions drawn from it should therefore be taken with a grain of salt. We could filter out any data prior to 1970 or so to try and analyze a seemingly more complete data record. But we'll keep everything for now, as this is more of an exploratory exercise.","dd50f8de":"# Explore data features individually"}}