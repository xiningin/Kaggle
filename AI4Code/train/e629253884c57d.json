{"cell_type":{"409b1933":"code","ecbb9a2a":"code","69979ad6":"code","d8934ce7":"code","c2d15ad3":"code","207b5300":"code","21642528":"code","006bcb05":"code","96d54c8f":"code","b0b7e508":"code","3109fb4b":"code","b66c80a4":"code","747b8f59":"code","e8a61ab7":"code","457f0b90":"code","96ed7690":"code","2500f98f":"code","5fce2bca":"code","7ade9fc8":"code","a8787e8a":"code","c1c07e97":"code","eb339478":"code","f9651046":"code","d17cdf5b":"code","5838b3b2":"code","a8690b50":"code","d372d775":"code","2e95228c":"code","3140f5af":"code","a9372d5a":"code","533441a2":"code","46b4e857":"code","46df3fa4":"code","91248570":"code","7719d804":"code","2a8b174e":"code","d6471497":"code","a8de519d":"code","0cf38a99":"code","8669fb98":"code","1254004a":"code","d1f543f8":"code","402f7771":"code","67841338":"code","2adbecb9":"code","f409df33":"code","54c8dccf":"code","83c3b6ab":"code","fa767e00":"code","faea8689":"code","56169a59":"code","7af2fb63":"code","5ea46281":"code","4795cee4":"code","0d62ddb9":"code","2b74212c":"code","076158c2":"code","e2a0784e":"code","aed4c08d":"code","08aa3765":"code","4b6c9bf8":"code","5d1809da":"code","17ad52f9":"code","1f5dba77":"code","2189cbbc":"code","dd99dc96":"code","c7650418":"code","c145315f":"code","d6b47c8b":"code","9ec4b12c":"code","f0c55d4c":"code","58981850":"code","384aef11":"code","9fa1ea9d":"code","a9843715":"code","65e5fe08":"code","c29aa19f":"code","490c9260":"code","484cc08b":"code","450b7b29":"code","7122d110":"code","2a8d0400":"code","9a140c64":"code","2a83dba3":"code","b271932e":"code","877568a6":"code","5f1e42fe":"code","159f79cf":"code","06f506d8":"code","7a680b6a":"code","4514e3ae":"code","440674b3":"code","62eb2bb1":"code","d41df1d6":"code","e06cf2ec":"code","269b341d":"code","59a88e43":"code","8d5e065b":"code","b3744828":"code","541f8e6a":"code","c35a654a":"code","ca946d50":"code","8767ce3e":"code","1c303f27":"code","dbda0813":"code","dfb07f28":"code","b0620328":"code","03335029":"code","3e2ab6fa":"code","06f6f0c6":"markdown","07ab1c92":"markdown","9d99aa50":"markdown","816498a8":"markdown","0243626b":"markdown","c02c5d45":"markdown","adf8e3b9":"markdown","16df0fd8":"markdown","ddb876d2":"markdown","49a19e4b":"markdown","7ad3c8b8":"markdown","c013e018":"markdown","c6867e11":"markdown","2f915c49":"markdown","9873fd5a":"markdown","3ba4d014":"markdown","ca8a5cc9":"markdown","60d0240e":"markdown","00f4da04":"markdown","6561b512":"markdown","bd31bea8":"markdown","10abdad5":"markdown","055167df":"markdown","7a2a18ff":"markdown","ff052567":"markdown","d9247f4f":"markdown","23888d05":"markdown","023dd847":"markdown","65ac58f4":"markdown","32d31b11":"markdown","00239c3a":"markdown","00cde4db":"markdown","37f8eaca":"markdown","2ddc3644":"markdown","dff25e71":"markdown","0a83edf7":"markdown","e43289ba":"markdown","8699a2d1":"markdown","88a1c438":"markdown","502097be":"markdown","1178fcbe":"markdown","bf3b2b31":"markdown","0b8fcc29":"markdown","ee641358":"markdown","2f6cf1c9":"markdown","13f2d1e6":"markdown","6bd0bd2d":"markdown","aaf66ab5":"markdown","89cc919c":"markdown","17d6fd95":"markdown","007cab9f":"markdown","17b10081":"markdown","2daf0446":"markdown","7df61137":"markdown","5ba1670b":"markdown","0de7dfee":"markdown","fd8dfb56":"markdown","a2c96395":"markdown","2a32cead":"markdown","e94464c1":"markdown"},"source":{"409b1933":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nida-competition'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecbb9a2a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom scipy import stats","69979ad6":"# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/nida-competition1\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/nida-competition1\/test.csv\")\n\n# preview train data\ntrain_df.head()","d8934ce7":"#preview test data\ntest_df.head()","c2d15ad3":"data = train_df\ndata.info()\n\n#Looking at basic descriptive statistics for numeric data \ndata.describe() ","207b5300":"#Looking at basic descriptive statistics for non-numeric data \ndata.describe(include=np.object)","21642528":"plt.style.use('dark_background')\nplt.figure(figsize=(7,3))\n\nsns.barplot(data['y'].value_counts(),data['y'].value_counts().index,data=data,palette=\"Set2\")\nplt.title('y is imbalance')\nplt.tight_layout()","006bcb05":"categori=['job','age', 'marital', 'education', 'default','housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\nfor col in categori:\n    plt.figure(figsize=(10,4))\n    sns.barplot(data[col].value_counts(),data[col].value_counts().index,data=data)\n    plt.title(col)\n    plt.tight_layout()","96d54c8f":"#test set\ncategori=['job','age', 'marital', 'education', 'default','housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\nfor col in categori:\n    plt.figure(figsize=(10,4))\n    sns.barplot(test_df[col].value_counts(),test_df[col].value_counts().index,data=test_df)\n    plt.title(col+' (test set)')\n    plt.tight_layout()","b0b7e508":"categori=['job', 'marital', 'education', 'default','housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n\nfor cat in categori:\n    x=data.groupby([cat,'y']).y.count()\n    print(x)\n    count=len(data[data[cat] == 'unknown'])\n    unknown_percentage=count\/len(data.index)*100\n    print('% of unknown value:',unknown_percentage)\n    print()","3109fb4b":"categori=['job', 'marital', 'education', 'default','housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n\n# Build a function to show categorical values disribution\ndef plot_bar(column):\n    # temp df \n    temp_1 = pd.DataFrame()\n    # count categorical values\n    temp_1['No_deposit'] = data[data['y'] == 'no'][column].value_counts()\n    temp_1['Yes_deposit'] = data[data['y'] == 'yes'][column].value_counts()\n    temp_1.plot(kind='bar')\n    plt.xlabel(f'{column}')\n    plt.ylabel('Number of clients')\n    plt.title('Distribution of {} and deposit'.format(column))\n    plt.show();\n\nfor cat in categori:\n    plot_bar(cat)   ","b66c80a4":"numerical_variables = ['age','campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx','cons.conf.idx','euribor3m',\n                      'nr.employed','duration']\ndata[numerical_variables].describe()","747b8f59":"#train set >>binsint, default 10 >> Change bins\nage=[10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\ndata.age.plot(kind='hist',bins=400,color=\"Aquamarine\",xticks=age,fontsize=10)\nplt.title('Age')","e8a61ab7":"# let check campaign field now and it is positively skewed..\n#train set\ndata.campaign.plot(kind='hist',bins=100)\nplt.title('Campaign')\ndata['campaign'].unique()","457f0b90":"#test set\ntest_df.campaign.plot(kind='hist',bins=100)\nplt.title('Campaign (test set)')","96ed7690":"#train set\ndata.groupby(['campaign','y']).y.count()","2500f98f":"#train set\nprint('Train set')\nprint('max:',data.campaign.max())\nprint('mean:',data.campaign.mean())\nprint('median:',data.campaign.median())\nprint('unique:',data.campaign.unique())\n\nprint('====================================')\nprint('Y=yes for campaign > 10 :' , data[(data['campaign'] > 10) & (data['y'] =='yes')].age.count())\nprint('Y=yes for campaign < 10 :' , data[(data['campaign'] <= 10) & (data['y'] =='yes')].age.count())\nprint('Y=yes for campaign = 1 :' , data[(data['campaign'] == 1) & (data['y'] =='yes')].age.count())\nprint('====================================')\nprint('Y=no for campaign > 10 :' , data[(data['campaign'] > 10) & (data['y'] =='no')].age.count())\nprint('Y=no for campaign < 10 :' , data[(data['campaign'] <= 10) & (data['y'] =='no')].age.count())\nprint('Y=no for campaign = 1 :' , data[(data['campaign'] == 1) & (data['y'] =='no')].age.count())\n\n#test set\nprint('====================================')\nprint('Train set')\nprint('max:',test_df.campaign.max())\nprint('mean:',test_df.campaign.mean())\nprint('median:',test_df.campaign.median())\nprint('unique:',test_df.campaign.unique())\n","5fce2bca":"def drawhist(data,feature):\n    plt.hist(data[feature])\n    plt.title(feature)\n","7ade9fc8":"#Train set\ndrawhist(data,'pdays')","a8787e8a":"#Exclude '999'\nplt.hist(data.loc[data.pdays != 999, 'pdays'])\nplt.title('pdays excluding \"999\" ')\nplt.show()\n","c1c07e97":"#test set\nplt.hist(test_df['pdays'])\nplt.title('pdays (test set)')\nplt.show()\n","eb339478":"#test set\nplt.hist(test_df.loc[test_df.pdays != 999, 'pdays'])\nplt.title('pdays excluding \"999\" (test set)')\nplt.show()","f9651046":"#train set\nprevious=[1,2,3,4,5,6,7]\ndata.previous.plot(kind='hist',bins=20,color=\"Aquamarine\",xticks=previous,fontsize=10)\nplt.title('previous')\nplt.show()\n\ndata['previous'].unique()","d17cdf5b":"#test set\nprevious=[1,2,3,4,5,6,7]\ntest_df.previous.plot(kind='hist',bins=20,color=\"Aquamarine\",xticks=previous,fontsize=10)\nplt.title('previous (test set)')\nplt.show()\n\ntest_df['previous'].unique()","5838b3b2":"#train set\nemp=data['emp.var.rate']\nemp.plot(kind='hist',bins=20,color=\"Aquamarine\",fontsize=10)\nplt.title('emp.var.rate')\nplt.show()","a8690b50":"#test set\nemp=test_df['emp.var.rate']\nemp.plot(kind='hist',bins=20,color=\"Aquamarine\",fontsize=10)\nplt.title('emp.var.rate (test_set)')\nplt.show()","d372d775":"#train set\ncon=data['cons.price.idx']\ncon.plot(kind='hist',bins=50,color=\"Aquamarine\",fontsize=10)\nplt.title('cons.price.idx')\nplt.show()","2e95228c":"#test set\ncon=test_df['cons.price.idx']\ncon.plot(kind='hist',bins=50,color=\"Aquamarine\",fontsize=10)\nplt.title('cons.price.idx (test set)')\nplt.show()","3140f5af":"#train set\ndata.euribor3m.plot(kind='hist',bins=100,color=\"Aquamarine\",fontsize=10)\nplt.title('euribor3m')\nplt.show()","a9372d5a":"#test set\ntest_df.euribor3m.plot(kind='hist',bins=100,color=\"Aquamarine\",fontsize=10)\nplt.title('euribor3m (test set)')\nplt.show()","533441a2":"#train set\nnr=data['nr.employed']\nnr.plot(kind='hist',bins=10,color=\"Aquamarine\",fontsize=10)\nplt.title('nr.employed')\nplt.show()","46b4e857":"#test set\nnr=test_df['nr.employed']\nnr.plot(kind='hist',bins=10,color=\"Aquamarine\",fontsize=10)\nplt.title('nr.employed (test set)')\nplt.show()  ","46df3fa4":"#train set\ndata.duration.plot(kind='hist',bins=10,color=\"Aquamarine\",fontsize=10)\nplt.title('duration')\nplt.show()","91248570":"#train set\ntest_df.duration.plot(kind='hist',bins=10,color=\"Aquamarine\",fontsize=10)\nplt.title('duration (test set)')\nplt.show()","7719d804":"categori=[ 'job','marital', 'education', 'default','housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\nprint('% of unknown value')\nfor cat in categori:\n    count=len(data[data[cat] == 'unknown'])\n    unknown_percentage=count\/len(data.index)*100 \n    print(cat,':',round(unknown_percentage,2))\n","2a8b174e":"def cross_tab(data,f1,f2):\n    # find no of unique values in jobs colums\n    jobs=list(data[f1].unique())\n    # find no of unique values in education columns\n    edu=list(data[f2].unique())\n    dataframes=[]\n    for e in edu:\n        dfe=data[data[f2]==e]\n        dfejob=dfe.groupby(f1).count()[f2]\n        dataframes.append(dfejob)\n       \n    xx=pd.concat(dataframes,axis=1)\n    xx.columns=edu\n    xx=xx.fillna(0)\n    return xx","d6471497":"#train set\ncross_tab(data,'job','education')","a8de519d":"#test set\ncross_tab(test_df,'job','education')","0cf38a99":"#train set\ndata['job'][data['age']>60].value_counts()\n\n#test set\ntest_df['job'][test_df['age']>60].value_counts()","8669fb98":"data.loc[(data['age']>60) & (data['job']=='unknown'), 'job'] = 'retired'\ndata.loc[(data['education']=='unknown') & (data['job']=='admin.'), 'education'] = 'university.degree'\ndata.loc[(data['education']=='unknown') & (data['job']=='blue-collar'), 'education'] = 'basic.9y'\ndata.loc[(data['education']=='unknown') & (data['job']=='entrepreneur'), 'education'] = 'university.degree'\ndata.loc[(data['education']=='unknown') & (data['job']=='housemaid'), 'education'] = 'basic.4y'\ndata.loc[(data['education']=='unknown') & (data['job']=='management'), 'education'] = 'university.degree'\ndata.loc[(data['education']=='unknown') & (data['job']=='self-employed'), 'education'] = 'university.degree'\ndata.loc[(data['education']=='unknown') & (data['job']=='services'), 'education'] = 'high.school'\ndata.loc[(data['education']=='unknown') & (data['job']=='technician'), 'education'] = 'professional.course'\n\n\n\ndata.loc[(data['job'] == 'unknown') & (data['education']=='basic.4y'), 'job'] = 'blue-collar'\ndata.loc[(data['job'] == 'unknown') & (data['education']=='basic.6y'), 'job'] = 'blue-collar'\ndata.loc[(data['job'] == 'unknown') & (data['education']=='basic.9y'), 'job'] = 'blue-collar'\ndata.loc[(data['job']=='unknown') & (data['education']=='high.school'), 'job'] = 'admin.'\ndata.loc[(data['job'] == 'unknown') & (data['education']=='university.degree'), 'job'] = 'admin.'\ndata.loc[(data['job']=='unknown') & (data['education']=='professional.course'), 'job'] = 'technician'\n","1254004a":"#test set\ntest_df.loc[(test_df['age']>60) & (test_df['job']=='unknown'), 'job'] = 'retired'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='admin.'), 'education'] = 'university.degree'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='blue-collar'), 'education'] = 'basic.9y'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='entrepreneur'), 'education'] = 'university.degree'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='housemaid'), 'education'] = 'basic.4y'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='management'), 'education'] = 'university.degree'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='self-employed'), 'education'] = 'university.degree'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='services'), 'education'] = 'high.school'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='technician'), 'education'] = 'professional.course'\n\n\n\ntest_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.4y'), 'job'] = 'blue-collar'\ntest_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.6y'), 'job'] = 'blue-collar'\ntest_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.9y'), 'job'] = 'blue-collar'\ntest_df.loc[(test_df['job']=='unknown') & (test_df['education']=='high.school'), 'job'] = 'admin.'\ntest_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='university.degree'), 'job'] = 'admin.'\ntest_df.loc[(test_df['job']=='unknown') & (test_df['education']=='professional.course'), 'job'] = 'technician'\n","d1f543f8":"#train set\ncross_tab(data,'job','education')","402f7771":"#test set\ncross_tab(test_df,'job','education')","67841338":"#train set\njobhousing1=cross_tab(data,'job','housing')\nprint(jobhousing1)\n\n#test set\njobhousing2=cross_tab(test_df,'job','housing')\nprint(jobhousing2)","2adbecb9":"def fillhousing(data,jobhousing):\n    \"\"\"Function for imputation via cross-tabulation to fill missing values for the 'housing' categorical feature\"\"\"\n    jobs=['housemaid','services','admin.','blue-collar','technician','retired','management','unemployed','self-employed','entrepreneur','student']\n    house=[\"no\",\"yes\"]\n    for j in jobs:\n        #Here we are taking value in which housing is unknow and job value is known\n        ind=data[np.logical_and(np.array(data['housing']=='unknown'),np.array(data['job']==j))].index\n        mask=np.random.rand(len(ind))<((jobhousing.loc[j]['no'])\/(jobhousing.loc[j]['no']+jobhousing.loc[j]['yes']))\n        ind1=ind[mask]\n        ind2=ind[~mask]\n        data.loc[ind1,\"housing\"]='no'\n        data.loc[ind2,\"housing\"]='yes'\n    return data","f409df33":"#train set\ndata=fillhousing(data,jobhousing1)\njobhousing1=cross_tab(data,'job','housing')\nprint(jobhousing1)\n\n#test set\ntest_df=fillhousing(test_df,jobhousing2)\njobhousing2=cross_tab(test_df,'job','housing')\nprint(jobhousing2)","54c8dccf":"#train set\njobloan1=cross_tab(data,'job','loan')\nprint(jobloan1)\n\n#test set\njobloan2=cross_tab(test_df,'job','loan')\nprint(jobloan2)","83c3b6ab":"def fillloan(data,jobloan):\n    \"\"\"Function for imputation via cross-tabulation to fill missing values for the 'loan' categorical feature\"\"\"\n    jobs=['housemaid','services','admin.','blue-collar','technician','retired','management','unemployed','self-employed','entrepreneur','student']\n    loan=[\"no\",\"yes\"]\n    for j in jobs:\n        ind=data[np.logical_and(np.array(data['loan']=='unknown'),np.array(data['job']==j))].index\n        mask=np.random.rand(len(ind))<((jobloan.loc[j]['no'])\/(jobloan.loc[j]['no']+jobloan.loc[j]['yes']))\n        ind1=ind[mask]\n        ind2=ind[~mask]\n        data.loc[ind1,\"loan\"]='no'\n        data.loc[ind2,\"loan\"]='yes'\n    return data","fa767e00":"#train set\ndata=fillloan(data,jobloan1)\njobloan1=cross_tab(data,'job','loan')\nprint(jobloan1)\n\n#test set\ntest_df=fillloan(test_df,jobloan2)\njobloan2=cross_tab(data,'job','loan')\nprint(jobloan2)","faea8689":"data['default'].unique()","56169a59":"numerical_variables = ['age', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx','cons.conf.idx','euribor3m',\n                      'nr.employed']\n\n#train set\ndata[numerical_variables].describe()\n","7af2fb63":"\n#test set\ntest_df[numerical_variables].describe()","5ea46281":"#train set\npday_poutcome=cross_tab(data,'pdays','poutcome')\nprint(pday_poutcome)\ndata.head()","4795cee4":"#test set\npday_poutcome=cross_tab(test_df,'pdays','poutcome')\nprint(pday_poutcome)\ntest_df.head()","0d62ddb9":"#creating a new column named \"pdays2\" based on the value in \"pdays\" column \ndef function (row):\n    if(row['pdays']==999):\n        return 0;\n    return 1;\n\n#train set\ndata['pdays2']=data.apply(lambda row: function(row),axis=1)\n\n#test set\ntest_df['pdays2']=test_df.apply(lambda row: function(row),axis=1)\n\n#changing the value 999 in pdays column to value 30 \ndef function1 (row):\n    if(row['pdays']==999):\n        return 30;\n    return row['pdays'];\n\n#train set\ndata['pdays']=data.apply(lambda row: function1(row),axis=1)\n\n#test set\ntest_df['pdays']=test_df.apply(lambda row: function1(row),axis=1)","2b74212c":"#changing the type of pdays to int\n#train set\ndata['pdays']=data['pdays'].astype(int)\npd.set_option('display.max_columns', None)\ndata.head()","076158c2":"#changing the type of pdays to int\n#test set\ntest_df['pdays']=test_df['pdays'].astype(int)\npd.set_option('display.max_columns', None)\ntest_df.head()","e2a0784e":"#Using log transform\n#train_set\ndata['age']=np.log(data.age)\n\n#test_set\ntest_df['age']=np.log(test_df.age)","aed4c08d":"#train set >>binsint, default 10 >> Change bins\nage=[1,1.5,2,2.5,3,3.5,4,4.5,5]\ndata.age.plot(kind='hist',bins=200,color=\"Cyan\",xticks=age,fontsize=10)\nplt.title('Age')","08aa3765":"def binning(dataframe,featureName):\n    print (featureName)\n    g1 = 1\n    g2 = 10\n \n    dataframe.loc[(dataframe[featureName] <= g1), featureName] = 1\n    dataframe.loc[(dataframe[featureName] > g1) & (dataframe[featureName] <= g2), featureName] = 2\n    dataframe.loc[(dataframe[featureName] > g2), featureName] = 3 \n    dataframe[featureName].head()","4b6c9bf8":"#train set\nbinning(data,'campaign')","5d1809da":"#train set\ncampaign=[1,2,3]\ndata.campaign.plot(kind='hist',bins=10,xticks=campaign)\nplt.title('Campaign')\ndata['campaign'].unique()","17ad52f9":"#test set\nbinning(test_df,'campaign')","1f5dba77":"data.head()","2189cbbc":"#Importing SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import SMOTENC\n\n#Create the oversampler\nX = data.drop(['y'], axis=1)\ny = data['y']\n\nsmotenc = SMOTENC([1,2,3,4,5,6,7,8,9,11,14],random_state = 101)\nX_oversample, y_oversample = smotenc.fit_resample(X, y)\n","dd99dc96":"data=pd.concat([X_oversample,y_oversample], axis=1)","c7650418":"plt.style.use('dark_background')\nplt.figure(figsize=(7,3))\n\nsns.barplot(data['y'].value_counts(),data['y'].value_counts().index,data=data,palette=\"Set2\")\nplt.title('y is imbalance')\nplt.tight_layout()","c145315f":"#MY NOTE \n\n# fit_transform():\n#sklearn's transform's fit() just calculates the parameters (e.g. \u03bc \u03bc  and \u03c3 \u03c3  in case of StandardScaler) and saves them as \n#an internal objects state.Afterwards, you can call its transform() method to apply the transformation to a particular set of examples. \n#fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x x , \n#but it also returns a transformed x\u2032 x' . Internally, it just calls first fit() and then transform() on the same data.\n\n#MinMaxScalar():\n#>> X_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\n#>> X_scaled = X_std * (max - min) + min\n\n\n# Processing choices: https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing\n\n\n\n#-------------------------------------\n#Column : 12>pdays \/ 13>previous \/ 15>emp.var.rate \/ 16>cons.price.idx \/ 17>cons.conf.idx \/ 18>euribor3m \/ 19>nr.employed\n\n#train set\nidx_numeric=[12,13,15,16,17,18,19]\nscaler = MinMaxScaler()\ndata[data.columns[idx_numeric]] = scaler.fit_transform(data[data.columns[idx_numeric]])\n\n#Map to binary\ndata['poutcome'] = data['poutcome'].map({'failure': -1,'nonexistent': 0,'success': 1})\ndata['default'] = data['default'].map({'yes': -1,'unknown': 0,'no': 1})\ndata['housing'] = data['housing'].map({'yes': -1,'unknown': 0,'no': 1})\ndata['loan'] = data['loan'].map({'yes': -1,'unknown': 0,'no': 1})\ndata['y'] = data['y'].map({'yes': 1,'no': 0})\n\n\n\n\n#data_clean = pd.get_dummies(data,columns=nominal)\ndata.head()","d6b47c8b":"#test set\nidx_numeric=[12,13,15,16,17,18,19]\nscaler = MinMaxScaler()\ntest_df[test_df.columns[idx_numeric]] = scaler.fit_transform(test_df[test_df.columns[idx_numeric]])\n\n#Map to binary\ntest_df['poutcome'] = test_df['poutcome'].map({'failure': -1,'nonexistent': 0,'success': 1})\ntest_df['default'] = test_df['default'].map({'yes': -1,'unknown': 0,'no': 1})\ntest_df['housing'] = test_df['housing'].map({'yes': -1,'unknown': 0,'no': 1})\ntest_df['loan'] = test_df['loan'].map({'yes': -1,'unknown': 0,'no': 1})\n\ntest_df.head()","9ec4b12c":"from sklearn.preprocessing import OrdinalEncoder\nenc = OrdinalEncoder()\nenc.fit(data[['campaign','job','marital','education','contact','month','day_of_week']])\ndata[['campaign','job','marital','education','contact','month','day_of_week']] = enc.transform(data[['campaign','job','marital','education','contact','month','day_of_week']])\n\n","f0c55d4c":"data.head()","58981850":"enc.fit(test_df[['campaign','job','marital','education','contact','month','day_of_week']])\ntest_df[['campaign','job','marital','education','contact','month','day_of_week']] = enc.transform(test_df[['campaign','job','marital','education','contact','month','day_of_week']])\n","384aef11":"test_df.head()","9fa1ea9d":"# create X (features) and y (response)\nX_oversample = data.drop(['y'], axis=1)\ny_oversample = data['y']","a9843715":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np","65e5fe08":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n","c29aa19f":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","490c9260":"# split data into train and test sets\nseed = 42\ntest_size = 0.3\n\nX_train, X_test, y_train, y_test = train_test_split(X_oversample, y_oversample, test_size=test_size, random_state=seed)","484cc08b":"model = XGBClassifier(eval_metric='auc',seed=42)\nmodel.fit(X_train, y_train)","450b7b29":"# make predictions for test data\ny_pred = model.predict(X_test)\n","7122d110":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nrfc_cv_score = cross_val_score(model, X_oversample, y_oversample, cv=10, scoring='roc_auc')\n\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test, y_pred))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nprint(\"=== All AUC Scores ===\")\nprint(rfc_cv_score)\nprint('\\n')\nprint(\"=== Mean AUC Score ===\")\nprint(\"Mean AUC Score - XGBoost Classifier: \", rfc_cv_score.mean())","2a8d0400":"from datetime import datetime\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nfrom sklearn import feature_selection","9a140c64":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \n\nparams_dist={'colsample_bytree': [0.6, 1.0],\n                                        'gamma': [0.5,2, 5],\n                                        'learning_rate': [0.01, 0.1, 0.3],\n                                        'max_depth': [ 6, 12],\n                                        'min_child_weight': [1, 5, 10],\n                                        'subsample': [0.6, 1.0],\n                                        'early_stopping_rounds':[42]}\n\nskf = StratifiedKFold(n_splits=3, shuffle = True, random_state = 42)\n\n#Create a based model\nXGBgridCV = XGBClassifier(objective='binary:logistic',base_score=0.12, booster='gbtree',\n                                            colsample_bytree=1,\n                                           eval_metric='auc',\n                                           importance_type='gain',\n                                           min_child_weight=1,\n                                           subsample=1,\n                                           tree_method='gpu_hist',\n                                           verbosity=0,n_estimators=1000,\n                                           scale_pos_weight=7 ,seed=42)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = XGBgridCV, param_grid=params_dist, \n                           cv = skf.split(X_train,y_train), n_jobs = 10, verbose = 3)  \n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)","2a83dba3":"grid_search.best_estimator_","b271932e":"grid_search.best_params_","877568a6":"final_model=grid_search.best_estimator_\n\n# make predictions for test data\ny_pred2 = final_model.predict(X_test)\npredictions2 = [round(value) for value in y_pred2]","5f1e42fe":"rfc_cv_score2 = cross_val_score(final_model, X_oversample, y_oversample, cv=10, scoring='roc_auc')\n\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test, y_pred2))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(classification_report(y_test, y_pred2))\nprint('\\n')\nprint(\"=== All AUC Scores ===\")\nprint(rfc_cv_score2)\nprint('\\n')\nprint(\"=== Mean AUC Score ===\")\nprint(\"Mean AUC Score - XGBClassifier: \", rfc_cv_score2.mean())","159f79cf":"test_df['y'] = final_model.predict_proba(test_df)[:, 1]\n\nsubmission = test_df[['y']]\n\nsubmission.index = np.arange(1, len(submission)+1) \nsubmission.index.name='Id'\n\nsubmission.to_csv(\"submission full XGB.csv\", index=True)\nsubmission.head()\n\nsubmission[submission['y'] >0.5]","06f506d8":"from sklearn import feature_selection\nsfm = feature_selection.SelectFromModel(final_model, threshold=0.0003)\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\nfeatureList =[];\nfor feature_list_index in sfm.get_support(indices=True):\n    temp = X_train.columns[feature_list_index]\n    featureList.append(temp)\n\nprint(featureList)\n    \nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)\n","7a680b6a":"X_important_train=X_train[['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'pdays2']]","4514e3ae":"feat_importances = pd.Series(grid_search.best_estimator_.feature_importances_, index=X_oversample.columns)\nfeat_importances.nlargest(30).plot(kind='barh',figsize=(10,10))\nplt.show()","440674b3":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \n\nparams_dist={'colsample_bytree': [0.6, 1.0],\n                                        'gamma': [4, 7],\n                                        'learning_rate': [0.05, 0.2],\n                                        'max_depth': [3,5],\n                                        'min_child_weight': [5, 10],\n                                        'subsample': [0.6, 1.0],\n                                        'early_stopping_rounds':[42], }\n \nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n\n#Create a based model\nXGBgridCV = XGBClassifier(objective='binary:logistic',base_score=0.12, booster='gbtree',\n                                            colsample_bytree=1,\n                                           eval_metric='auc',\n                                           importance_type='gain',\n                                           min_child_weight=1,\n                                           subsample=1,\n                                           tree_method='gpu_hist',\n                                           verbosity=0,n_estimators=10000,\n                                           scale_pos_weight=7 ,seed=42)\n\n\n# Instantiate the grid search model\ngrid_search2 = GridSearchCV(estimator = XGBgridCV, param_grid=params_dist, \n                           cv = skf.split(X_important_train,y_train), n_jobs = 10, verbose = 3)  \n\n# Fit the grid search to the data\ngrid_search2.fit(X_important_train, y_train)","62eb2bb1":"grid_search2.best_params_\n","d41df1d6":"grid_search2.best_estimator_\n","e06cf2ec":"final_model2=grid_search2.best_estimator_\n\n# make predictions for test data\ny_pred3 = final_model2.predict(X_test)\npredictions = [round(value) for value in y_pred]\n\nrfc_cv_score3 = cross_val_score(final_model2, X_oversample, y_oversample, cv=10, scoring='roc_auc')\n\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test, y_pred3))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(classification_report(y_test, y_pred3))\nprint('\\n')\nprint(\"=== All AUC Scores ===\")\nprint(rfc_cv_score2)\nprint('\\n')\nprint(\"=== Mean AUC Score ===\")\nprint(\"Mean AUC Score - XGBClassifier: \", rfc_cv_score3.mean())","269b341d":"X_important_test=test_df[['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']]","59a88e43":"test_df['y'] = final_model2.predict_proba(X_important_test)[:, 1]\n\nsubmission = test_df[['y']]\n\nsubmission.index = np.arange(1, len(submission)+1) \nsubmission.index.name='Id'\n\nsubmission.to_csv(\"submission reduced XGB.csv\", index=True)\nsubmission.head()\n\nsubmission[submission['y'] >0.5]","8d5e065b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nida-competition'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3744828":"#Generic mathematics calculation \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom scipy import stats\n\n#Resolving Imbalance data\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import SMOTENC\n\n#Building model selection, train&test split\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n#Building pipeline of classifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier","541f8e6a":"# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/nida-competition1\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/nida-competition1\/test.csv\")\n\n# # preview train data\n# train_df.head()\n\ndata = train_df","c35a654a":"#SMOTE\n#Create the oversampler\nX = data.drop(['y'], axis=1)\ny = data['y']\n\nsmotenc = SMOTENC([1,2,3,4,5,6,7,8,9,14],random_state = 101)\nX_oversample, y_oversample = smotenc.fit_resample(X, y)\n\n#concatinate train data for data manupulation\ndata = pd.concat([X_oversample, y_oversample], axis=1)\n\n# ##>> Knowing each categorical features\n# categori=['job','age', 'marital', 'education', 'default','housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome','y']\n# for col in categori:\n#     plt.figure(figsize=(11,6))\n#     sns.barplot(data[col].value_counts(),data[col].value_counts().index,data=data)\n#     plt.title(col)\n#     plt.tight_layout()\n    \n# ## Looking at the relationship between catagorical features and dependent variable, 'Y'\n# for cat in categori:\n#     x=data.groupby([cat,'y']).y.count()\n#     print(x)\n#     count=len(data[data[cat] == 'unknown'])\n#     unknown_percentage=count\/len(data.index)*100\n#     print('% of unknown value:',unknown_percentage)\n#     print()\n\n##>>> Categorical features disribution\n\n# # Build a function to show categorical values disribution\n# def plot_bar(column):\n#     # temp df \n#     temp_1 = pd.DataFrame()\n#     # count categorical values\n#     temp_1['No_deposit'] = data[data['y'] == 'no'][column].value_counts()\n#     temp_1['Yes_deposit'] = data[data['y'] == 'yes'][column].value_counts()\n#     temp_1.plot(kind='bar')\n#     plt.xlabel(f'{column}')\n#     plt.ylabel('Number of clients')\n#     plt.title('Distribution of {} and deposit'.format(column))\n#     plt.show();\n\n# for cat in categori:\n#     plot_bar(cat)  \n\n# ## >> Knowing each numerical features\n# numerical_variables = ['age','campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx','cons.conf.idx','euribor3m',\n#                       'nr.employed','duration']\n# data[numerical_variables].describe()\n\n\n# Data Cleaning\n\n##>> Catagorical features\n'''Convert Duration Call into 5 category'''\ndef duration(data):\n    data.loc[data['duration'] <= 102, 'duration'] = 1\n    data.loc[(data['duration'] > 102) & (data['duration'] <= 180)  , 'duration'] = 2\n    data.loc[(data['duration'] > 180) & (data['duration'] <= 319)  , 'duration'] = 3\n    data.loc[(data['duration'] > 319) & (data['duration'] <= 645), 'duration'] = 4\n    data.loc[data['duration']  > 645, 'duration'] = 5\n    return data\n\ncategori=['job', 'marital', 'education', 'default','housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n# print('% of unknown value')\n# for cat in categori:\n#     count=len(data[data[cat] == 'unknown'])\n#     unknown_percentage=count\/len(data.index)*100 \n# #     print(cat,':',round(unknown_percentage,2))\n\n# >> Data imputation #######################################################\n\ndef cross_tab(data,f1,f2):\n    # find no of unique values in jobs colums\n    jobs=list(data[f1].unique())\n    # find no of unique values in education columns\n    edu=list(data[f2].unique())\n    dataframes=[]\n    for e in edu:\n        dfe=data[data[f2]==e]\n        # https:\/\/www.youtube.com\/watch?v=qy0fDqoMJx8 for groupby operation\n        #https:\/\/www.youtube.com\/watch?v=hfDXRyYIFkk grupby count\n        #https:\/\/data36.com\/pandas-tutorial-2-aggregation-and-grouping\/\n        dfejob=dfe.groupby(f1).count()[f2]\n        dataframes.append(dfejob)\n        #https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.concat.html\n    xx=pd.concat(dataframes,axis=1)\n    xx.columns=edu\n    xx=xx.fillna(0)\n    return xx\n\n# >>> Jobs & Education #######################################################\n#train set\ncross_tab(data,'job','education')\n\n#train set\ndata['job'][data['age']>60].value_counts()\n\n#test set\ntest_df['job'][test_df['age']>60].value_counts()\n\n#train set\n#data.loc[(data['age']>60) & (data['job']=='unknown'), 'job'] = 'retired'\n#data.loc[(data['education']=='unknown') & (data['job']=='management'), 'education'] = 'university.degree'\n#data.loc[(data['education']=='unknown') & (data['job']=='services'), 'education'] = 'high.school'\n#data.loc[(data['education']=='unknown') & (data['job']=='housemaid'), 'education'] = 'basic.4y'\n\n\n#data.loc[(data['job'] == 'unknown') & (data['education']=='basic.4y'), 'job'] = 'blue-collar'\n#data.loc[(data['job'] == 'unknown') & (data['education']=='basic.6y'), 'job'] = 'blue-collar'\n#data.loc[(data['job'] == 'unknown') & (data['education']=='basic.9y'), 'job'] = 'blue-collar'\n#data.loc[(data['job']=='unknown') & (data['education']=='professional.course'), 'job'] = 'technician'\n\n\ndata.loc[(data['age']>60) & (data['job']=='unknown'), 'job'] = 'retired'\ndata.loc[(data['education']=='unknown') & (data['job']=='admin.'), 'education'] = 'university.degree'\ndata.loc[(data['education']=='unknown') & (data['job']=='blue-collar'), 'education'] = 'basic.9y'\ndata.loc[(data['education']=='unknown') & (data['job']=='entrepreneur'), 'education'] = 'university.degree'\ndata.loc[(data['education']=='unknown') & (data['job']=='housemaid'), 'education'] = 'basic.4y'\ndata.loc[(data['education']=='unknown') & (data['job']=='management'), 'education'] = 'university.degree'\ndata.loc[(data['education']=='unknown') & (data['job']=='self-employed'), 'education'] = 'university.degree'\ndata.loc[(data['education']=='unknown') & (data['job']=='services'), 'education'] = 'high.school'\ndata.loc[(data['education']=='unknown') & (data['job']=='technician'), 'education'] = 'professional.course'\n\n\n\ndata.loc[(data['job'] == 'unknown') & (data['education']=='basic.4y'), 'job'] = 'blue-collar'\ndata.loc[(data['job'] == 'unknown') & (data['education']=='basic.6y'), 'job'] = 'blue-collar'\ndata.loc[(data['job'] == 'unknown') & (data['education']=='basic.9y'), 'job'] = 'blue-collar'\ndata.loc[(data['job']=='unknown') & (data['education']=='high.school'), 'job'] = 'admin.'\ndata.loc[(data['job'] == 'unknown') & (data['education']=='university.degree'), 'job'] = 'admin.'\ndata.loc[(data['job']=='unknown') & (data['education']=='professional.course'), 'job'] = 'technician'\n\n\n#test_df.loc[(test_df['age']>60) & (test_df['job']=='unknown'), 'job'] = 'retired'\n#test_df.loc[(test_df['education']=='unknown') & (test_df['job']=='management'), 'education'] = 'university.degree'\n#test_df.loc[(test_df['education']=='unknown') & (test_df['job']=='services'), 'education'] = 'high.school'\n#test_df.loc[(test_df['education']=='unknown') & (test_df['job']=='housemaid'), 'education'] = 'basic.4y'\n\n\n#test_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.4y'), 'job'] = 'blue-collar'\n#test_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.6y'), 'job'] = 'blue-collar'\n#test_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.9y'), 'job'] = 'blue-collar'\n#test_df.loc[(test_df['job']=='unknown') & (test_df['education']=='professional.course'), 'job'] = 'technician'\n\n\ntest_df.loc[(test_df['age']>60) & (test_df['job']=='unknown'), 'job'] = 'retired'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='admin.'), 'education'] = 'university.degree'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='blue-collar'), 'education'] = 'basic.9y'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='entrepreneur'), 'education'] = 'university.degree'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='housemaid'), 'education'] = 'basic.4y'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='management'), 'education'] = 'university.degree'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='self-employed'), 'education'] = 'university.degree'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='services'), 'education'] = 'high.school'\ntest_df.loc[(test_df['education']=='unknown') & (test_df['job']=='technician'), 'education'] = 'professional.course'\n\n\n\ntest_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.4y'), 'job'] = 'blue-collar'\ntest_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.6y'), 'job'] = 'blue-collar'\ntest_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='basic.9y'), 'job'] = 'blue-collar'\ntest_df.loc[(test_df['job']=='unknown') & (test_df['education']=='high.school'), 'job'] = 'admin.'\ntest_df.loc[(test_df['job'] == 'unknown') & (test_df['education']=='university.degree'), 'job'] = 'admin.'\ntest_df.loc[(test_df['job']=='unknown') & (test_df['education']=='professional.course'), 'job'] = 'technician'\n\n#train set\n# print(cross_tab(data,'job','education'))\n\n#test set\n# print(cross_tab(test_df,'job','education'))\n\n#train set\njobhousing1=cross_tab(data,'job','housing')\n# print(jobhousing1)\n\n#test set\njobhousing2=cross_tab(test_df,'job','housing')\n# print(jobhousing2)\n\ndef fillhousing(data,jobhousing):\n    \"\"\"Function for imputation via cross-tabulation to fill missing values for the 'housing' categorical feature\"\"\"\n    jobs=['housemaid','services','admin.','blue-collar','technician','retired','management','unemployed','self-employed','entrepreneur','student']\n    house=[\"no\",\"yes\"]\n    for j in jobs:\n        #Here we are taking value in which housing is unknow and job value is known\n        ind=data[np.logical_and(np.array(data['housing']=='unknown'),np.array(data['job']==j))].index\n        mask=np.random.rand(len(ind))<((jobhousing.loc[j]['no'])\/(jobhousing.loc[j]['no']+jobhousing.loc[j]['yes']))\n        ind1=ind[mask]\n        ind2=ind[~mask]\n        data.loc[ind1,\"housing\"]='no'\n        data.loc[ind2,\"housing\"]='yes'\n    return data\n\n#train set\ndata=fillhousing(data,jobhousing1)\njobhousing1=cross_tab(data,'job','housing')\n# print(jobhousing1)\n\n#test set\ntest_df=fillhousing(test_df,jobhousing2)\njobhousing2=cross_tab(test_df,'job','housing')\n# print(jobhousing2)\n\n# >>>Jobs & loan  #######################################################\n\n#train set\njobloan1=cross_tab(data,'job','loan')\n# print(jobloan1)\n\n#test set\njobloan2=cross_tab(test_df,'job','loan')\n# print(jobloan2)\n\ndef fillloan(data,jobloan):\n    \"\"\"Function for imputation via cross-tabulation to fill missing values for the 'loan' categorical feature\"\"\"\n    jobs=['housemaid','services','admin.','blue-collar','technician','retired','management','unemployed','self-employed','entrepreneur','student']\n    loan=[\"no\",\"yes\"]\n    for j in jobs:\n        ind=data[np.logical_and(np.array(data['loan']=='unknown'),np.array(data['job']==j))].index\n        mask=np.random.rand(len(ind))<((jobloan.loc[j]['no'])\/(jobloan.loc[j]['no']+jobloan.loc[j]['yes']))\n        ind1=ind[mask]\n        ind2=ind[~mask]\n        data.loc[ind1,\"loan\"]='no'\n        data.loc[ind2,\"loan\"]='yes'\n    return data\n\n#train set\ndata=fillloan(data,jobloan1)\njobloan1=cross_tab(data,'job','loan')\n# print(jobloan1)\n\n#test set\ntest_df=fillloan(test_df,jobloan2)\njobloan2=cross_tab(data,'job','loan')\n# print(jobloan2)\n\n### >>> Default #######################################################\n\ndata['default'].unique()\n\nnumerical_variables = ['age','campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx','cons.conf.idx','euribor3m',\n                      'nr.employed']\n\n# #train set\n# data[numerical_variables].describe()\n\n# #test set\n# test_df[numerical_variables].describe()\n\n# def drawhist(data,feature):\n#     plt.hist(data[feature])\n    \n# #train set\n# print('train set hist - pdays')\n# drawhist(data,'pdays')\n# plt.show()\n\n# plt.hist(data.loc[data.pdays != 999, 'pdays'])\n# plt.show()\n\n## Pdays #######################################################\n\n#test set\n# print('test set hist - pdays')\n# drawhist(test_df,'pdays')\n# plt.show()\n\n# plt.hist(test_df.loc[test_df.pdays != 999, 'pdays'])\n# plt.show()\n\n#NOT INCLUDED\nimport statistics\nx=sum(data.loc[data.pdays != 999, 'pdays'])\ny=len(data.loc[data.pdays != 999, 'pdays'])\naverage=x\/y\n\n# print(x,y)\n# print(average)\n\nmode=statistics.mode(data.loc[data.pdays != 999, 'pdays'])\n# print(mode)\n\nmedian=statistics.median(data.loc[data.pdays != 999, 'pdays'])\n# print(median)\n\nmode_all=statistics.mode(data['pdays'])\n# print(mode_all)\n\nmedian_all=statistics.median(data['pdays'])\n# print(median_all)\n\n#MY NOTE\n#train set\npday_poutcome=cross_tab(data,'pdays','poutcome')\n# print(pday_poutcome)\n# data.head()\n\n#test set\npday_poutcome=cross_tab(test_df,'pdays','poutcome')\n# print(pday_poutcome)\n# test_df.head()\n\n#creating a new column named \"pdays2\" based on the value in \"pdays\" column \ndef function (row):\n    if(row['pdays']==999):\n        return 0;\n    return 1;\n\n#train set\ndata['pdays2']=data.apply(lambda row: function(row),axis=1)\n\n#test set\ntest_df['pdays2']=test_df.apply(lambda row: function(row),axis=1)\n\n#changing the value 999 in pdays column to value 30 \ndef function1 (row):\n    if(row['pdays']==999):\n        return 30;\n    return row['pdays'];\n\n#train set\ndata['pdays']=data.apply(lambda row: function1(row),axis=1)\n\n#test set\ntest_df['pdays']=test_df.apply(lambda row: function1(row),axis=1)\n\n#changing the type of pdays to int\n#train set\ndata['pdays']=data['pdays'].astype(int)\npd.set_option('display.max_columns', None)\n# data.head()\n\n#changing the type of pdays to int\n#test set\ntest_df['pdays']=test_df['pdays'].astype(int)\npd.set_option('display.max_columns', None)\n# test_df.head()\n\n##>>> Age ##########################################################\n\n#train set\n# # Check outlier if any for Numberic column.\n# data.age.plot(kind='box')\n# # There are outlier and check max age and age greated than 90\n\n#test set\n# # Check outlier if any for Numberic column.\n# test_df.age.plot(kind='box')\n# # There are outlier and check max age and age greated than 90\n\n# #Max age >>train set\n# print(data.age.max())\n\n# #Max age >> test set\n# print(test_df.age.max())\n\n# #Age more than 80 >> train set\n# data[data['age'] > 80].head(5)\n\n\n# #Age more than 80 >> test set\n# test_df[test_df['age'] > 80].head(5)\n\n#MY NOTE\n\n# #No. of customers who are over 80 >> train set\n# data['age'].groupby(data['age'] > 80).count()\n\n# #No. of customers who are over 80 >> test set\n# test_df['age'].groupby(test_df['age'] > 80).count()\n\n\n# #MY NOTE \n# #train set >>binsint, default 10 >> Change bins\n# data.age.plot(kind='hist',bins=400)\n\n#DROP Outlier\n#train set\ndata=data.drop(data[data.age>85].index)\n\n#train set\ndata['age'].unique()\n\n#test set\ntest_df['age'].unique()\n\n# #train set\n# data.age.plot(kind='kde')\n\n# #test set\n# data.age.plot(kind='kde')\n\ndef binning(dataframe,featureName):\n#     print (featureName)\n    q1 = dataframe[featureName].quantile(0.25)\n    q2 = dataframe[featureName].quantile(0.50)\n    q3 = dataframe[featureName].quantile(0.75)\n    dataframe.loc[(dataframe[featureName] <= q1), featureName] = 1\n    dataframe.loc[(dataframe[featureName] > q1) & (dataframe[featureName] <= q2), featureName] = 2\n    dataframe.loc[(dataframe[featureName] > q2) & (dataframe[featureName] <= q3), featureName] = 3\n    dataframe.loc[(dataframe[featureName] > q3), featureName] = 4 \n#     print (q1, q2, q3)\n    \n#MY NOTE >>> NEW Binning \n\ndef binning2(dataframe,featureName):\n#     print (featureName)\n    g1 = 27\n    g2 = 35\n    g3 = 60\n    dataframe.loc[(dataframe[featureName] <= g1), featureName] = 1\n    dataframe.loc[(dataframe[featureName] > g1) & (dataframe[featureName] <= g2), featureName] = 2\n    dataframe.loc[(dataframe[featureName] > g2) & (dataframe[featureName] <= g3), featureName] = 3\n    dataframe.loc[(dataframe[featureName] > g3), featureName] = 4 \n#     dataframe[featureName].head()\n    \n#train set\nbinning2(data,'age')\ndata.head()\n\n#test set\nbinning2(test_df,'age')\ntest_df.head()\n\n\n#Campaign ####################################################\n\n# let check campaign field now and it is positively skewed..\n#train set\n# data.campaign.plot(kind='hist')\n\n# #test set\n# test_df.campaign.plot(kind='hist')\n\n# #train set \n# data.campaign.plot(kind='box')\n# # lot of exreme values.\n\n# #test set\n# test_df.campaign.plot(kind='box')\n\n# #MY NOTE\n# #train set\n# data.campaign.plot(kind='hist',bins=200)\n\n# #test set\n# test_df.campaign.plot(kind='hist',bins=200)\n\n# #train set\n# print('max:',data.campaign.max())\n# print('mean:',data.campaign.mean())\n# print('median:',data.campaign.median())\n# print('unique:',data.campaign.unique())\n\n# #????????\n# print('Y=1 for campaign > 10 :' , data[(data['campaign'] > 10) & (data['y'] ==1)].age.count())\n# print('Y=1 for campaign < 10 :' , data[(data['campaign'] <= 10) & (data['y'] ==1)].age.count())\n# print('Y=1 for campaign = 1 :' , data[(data['campaign'] == 1) & (data['y'] ==1)].age.count())\n\n# #test set\n# print('max:',test_df.campaign.max())\n# print('mean:',test_df.campaign.mean())\n# print('median:',test_df.campaign.median())\n# print('unique:',test_df.campaign.unique())\n\n# #train set\n# data.groupby(['campaign','y']).y.count()\n\n# #train set\n# data['campaign'].describe()\n\n# #test set\n# test_df['campaign'].describe()\n\n# #train set\n# q1 = data['campaign'].quantile(0.25)\n# q2 = data['campaign'].quantile(0.50)\n# q3 = data['campaign'].quantile(0.75)\n\n# print(q1)\n# print(q2)\n# print(q3)\n\n# iqr = q3-q1 #Interquartile range\n\n# extreme_low_campaign = q1-1.5*iqr\n# extreme_high_capmaign = q3+1.5*iqr\n\n# print (extreme_low_campaign)\n# print (extreme_high_capmaign)\n\n# #test set\n# q1 = test_df['campaign'].quantile(0.25)\n# q2 = test_df['campaign'].quantile(0.50)\n# q3 = test_df['campaign'].quantile(0.75)\n\n# print(q1)\n# print(q2)\n# print(q3)\n\n# iqr = q3-q1 #Interquartile range\n\n# extreme_low_campaign = q1-1.5*iqr\n# extreme_high_capmaign = q3+1.5*iqr\n\n# print (extreme_low_campaign)\n# print (extreme_high_capmaign)\n\n#train set\nbinning(data,'campaign')\n\n#test set\nbinning(test_df,'campaign')\n\n# Standardizing data #########################################\n\n# >> Standardizing the data\n\n# Mapping binary data and create dummy variables --> --> -->\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# fit_transform():\n#sklearn's transform's fit() just calculates the parameters (e.g. \u03bc \u03bc  and \u03c3 \u03c3  in case of StandardScaler) and saves them as \n#an internal objects state.Afterwards, you can call its transform() method to apply the transformation to a particular set of examples. \n#fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x x , \n#but it also returns a transformed x\u2032 x' . Internally, it just calls first fit() and then transform() on the same data.\n\n#MinMaxScalar():\n#>> X_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\n#>> X_scaled = X_std * (max - min) + min\n\n\n# Processing choices: https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing\n\n\n\n#-------------------------------------\n#>>> Column : 0>age \/ 10>duration \/11>campaign \/ 12>pdays \/ 13>previous \/ 15>emp.var.rate \/ 16>cons.price.idx \/ \n#17>cons.conf.idx \/ 18>euribor3m \/ 19>nr.employed\n\n#train set\nidx_numeric=[0,10,11,12,13,15,16,17,18,19]\nscaler = MinMaxScaler()\ndata[data.columns[idx_numeric]] = scaler.fit_transform(data[data.columns[idx_numeric]])\n\n#Map to binary\ndata['poutcome'] = data['poutcome'].map({'failure': -1,'nonexistent': 0,'success': 1})\ndata['default'] = data['default'].map({'yes': -1,'unknown': 0,'no': 1})\ndata['housing'] = data['housing'].map({'yes': -1,'unknown': 0,'no': 1})\ndata['loan'] = data['loan'].map({'yes': -1,'unknown': 0,'no': 1})\n\n# One hot encoding of nominal varibles\nnominal = ['job','marital','education','contact','month','day_of_week']\ndata_clean = pd.get_dummies(data,columns=nominal)\ndata_clean['y']=data_clean['y'].map({'yes': 1,'no': 0})\ndata_clean.head()\n\n#test set\nidx_numeric=[0,10,11,12,13,15,16,17,18]\nscaler = MinMaxScaler()\ntest_df[test_df.columns[idx_numeric]] = scaler.fit_transform(test_df[test_df.columns[idx_numeric]])\n\n#Map to binary\ntest_df['poutcome'] = test_df['poutcome'].map({'failure': -1,'nonexistent': 0,'success': 1})\ntest_df['default'] = test_df['default'].map({'yes': -1,'unknown': 0,'no': 1})\ntest_df['housing'] = test_df['housing'].map({'yes': -1,'unknown': 0,'no': 1})\ntest_df['loan'] = test_df['loan'].map({'yes': -1,'unknown': 0,'no': 1})\n\n# One hot encoding of nominal varibles\nnominal = ['job','marital','education','contact','month','day_of_week']\ntest_df_clean = pd.get_dummies(test_df,columns=nominal)\n\n\ndata_clean.shape\n\ntest_df_clean.shape\n","ca946d50":"# create X (features) and y (response)\nX_oversample = data.drop(['y'], axis=1)\ny_oversample = data['y']","8767ce3e":"# #test set\n# from sklearn.preprocessing import MinMaxScaler\n# idx_numeric=[0,10,11,12,13,15,16,17,18]\n# scaler = MinMaxScaler()\n# test_df[test_df.columns[idx_numeric]] = scaler.fit_transform(test_df[test_df.columns[idx_numeric]])\n\n\n# #Map to binary\n# test_df['poutcome'] = test_df['poutcome'].map({'failure': -1,'nonexistent': 0,'success': 1})\n# test_df['default'] = test_df['default'].map({'yes': -1,'unknown': 0,'no': 1})\n# test_df['housing'] = test_df['housing'].map({'yes': -1,'unknown': 0,'no': 1})\n# test_df['loan'] = test_df['loan'].map({'yes': -1,'unknown': 0,'no': 1})\n\n# # One hot encoding of nominal varibles\n# nominal = ['job','marital','education','contact','month','day_of_week']\n# test_df_clean = pd.get_dummies(test_df,columns=nominal)\n\n# # test_df_clean.info()\n# # print(np.isnan(test_df_clean.values.any()))\n# # print(np.isnan(test_df.values.any()))\n\n# print(test_df.head())\n# print(test_df.tail())\n\n# print(test_df_clean.isnull().values.any())\n\n# for i in test_df_clean.columns:\n    \n#     if test_df_clean[i].isnull().any() == True:\n#         print(i, test_df_clean[i].isnull().any())\n#         print(test_df_clean[i].describe)\n        \n# # df = pd.DataFrame(numbers,columns=['set_of_numbers'])\n# test_df_clean.describe(include = ['poutcome'])\n# df.describe(include=['category'])\n\n# data_clean.shape\n\ntest_df_clean.shape  ","1c303f27":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n# create X (features) and y (response)\nX_oversample = data_clean.drop(['y'], axis=1)\ny_oversample = data_clean['y']\n\n# use train\/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X_oversample, y_oversample, test_size=0.2, random_state=2)\n","dbda0813":"##Building pipeline of classifiers\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\n##Building pipeline of classifiers #####################################################\n# set all CPU\nn_jobs = -1\nrandom_state = 11\n# LogisticRegression\npipe_lr = Pipeline([('lr', LogisticRegression(random_state=random_state, n_jobs=n_jobs, max_iter=500))])\n\n# RandomForestClassifier\npipe_rf = Pipeline([('rf', RandomForestClassifier(random_state=random_state, oob_score=True, n_jobs=n_jobs))])\n\n# KNeighborsClassifier\npipe_knn = Pipeline([('knn', KNeighborsClassifier(n_jobs=n_jobs))])\n\n# DecisionTreeClassifier\npipe_dt = Pipeline([('dt', DecisionTreeClassifier(random_state=random_state, max_features='auto'))])\n\n# BaggingClassifier\n# note we use SGDClassifier as classier inside BaggingClassifier\npipe_bag = Pipeline([('bag',BaggingClassifier(base_estimator=SGDClassifier(random_state=random_state, n_jobs=n_jobs, max_iter=1500),\\\n                                              random_state=random_state,oob_score=True,n_jobs=n_jobs))])\n# SGDClassifier\npipe_sgd = Pipeline([('sgd', SGDClassifier(random_state=random_state, n_jobs=n_jobs, max_iter=1500))])\n\n# XGBClassifier\npipe_xgb = Pipeline([('xgb', XGBClassifier(booster = 'gbtree'))])\n\n## Set parameters for Grid Search #######################################################\n\n# set number \ncv = StratifiedKFold(shuffle=True, n_splits=10, random_state=random_state)\n# set for LogisticRegression\ngrid_params_lr = [{\n                'lr__penalty': ['l2'],\n                'lr__C': [0.3, 0.6, 0.7],\n                'lr__solver': ['sag']\n                }]\n# set for RandomForestClassifier\ngrid_params_rf = [{\n                'rf__criterion': ['entropy'],\n                'rf__min_samples_leaf': [80, 100],\n                'rf__max_depth': [25, 27],\n                'rf__min_samples_split': [3, 5,7],\n                'rf__n_estimators' : [60,100,200],\n                'rf__max_features': ['auto']\n                }]\n# set for KNeighborsClassifier\ngrid_params_knn = [{'knn__n_neighbors': [16,17,18]}]\n\n# set for DecisionTreeClassifier\ngrid_params_dt = [{\n                'dt__max_depth': [8, 10],\n                'dt__min_samples_leaf': [1, 3, 5, 7]\n                  }]\n# set for BaggingClassifier\ngrid_params_bag = [{'bag__n_estimators': [10, 15, 20,100,200]}]\n\n# set for SGDClassifier\ngrid_params_sgd = [{\n                    'sgd__loss': ['log', 'huber'],\n                    'sgd__learning_rate': ['adaptive'],\n                    'sgd__eta0': [0.001, 0.01, 0.1],\n                    'sgd__penalty': ['l1', 'l2', 'elasticnet'], \n                    'sgd__alpha':[0.1, 1, 5, 10]\n                    }]\n\n#Set for XGBClassifier\ngrid_params_xgb = [{\n                    'xgb__eta' : [0.01, 0.3,0.7, 1.0],\n                    'xgb__max_depth' : [6, 8, 10 ,15],\n                    'xgb__lambda' : [1],\n                    'xgb__alpha'  : [0]  \n                    }]\n\n\n## Grid search objects ########################################################\n\n# for LogisticRegression\ngs_lr = GridSearchCV(pipe_lr, param_grid=grid_params_lr,\n                     scoring='accuracy', cv=cv) \n# for RandomForestClassifier\ngs_rf = GridSearchCV(pipe_rf, param_grid=grid_params_rf,\n                     scoring='accuracy', cv=cv)\n# for KNeighborsClassifier\ngs_knn = GridSearchCV(pipe_knn, param_grid=grid_params_knn,\n                     scoring='accuracy', cv=cv)\n# for DecisionTreeClassifier\ngs_dt = GridSearchCV(pipe_dt, param_grid=grid_params_dt,\n                     scoring='accuracy', cv=cv)\n# for BaggingClassifier\ngs_bag = GridSearchCV(pipe_bag, param_grid=grid_params_bag,\n                     scoring='accuracy', cv=cv)\n# for SGDClassifier\ngs_sgd = GridSearchCV(pipe_sgd, param_grid=grid_params_sgd,\n                     scoring='accuracy', cv=cv)\n# for XGBClassifier (made by myself)\ngs_xgb = GridSearchCV(pipe_xgb, param_grid = grid_params_xgb, scoring ='accuracy', cv=cv)\n\n# models that we iterate over\nclass_list = [ gs_xgb, gs_rf, gs_knn, gs_dt, gs_bag, gs_sgd,gs_lr]\n# dict for later use \nmodel_dict = {0:'XGboost', 1:'RandomForest', 2:'Knn', 3:'DesionTree', 4:'Bagging with SGDClassifier', 5:'SGD Class', 6:'Logistic_reg'}\n\n## Function to iterate over models and obtain results ######################################\n# set empty dicts and list\n\nimport time\n\nresult_acc = {}\nresult_auc = {}\nmodels = []\n\nfor index, model in enumerate(class_list):\n        start = time.time()\n        print('\\n')\n        print('Start New Model - Estimator is {}'.format(model_dict[index]))\n        model.fit(X_train, y_train)\n        print('---------------------------------------------')\n        print('best params {}'.format(model.best_params_))\n        print('best score is {}'.format(model.best_score_))\n        auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n        print('---------------------------------------------')\n        print('ROC_AUC is {} and accuracy rate is {}'.format(auc, model.score(X_test, y_test)))\n        end = time.time()\n        print('It lasted for {} sec'.format(round(end - start, 3)))\n        print(' End Model ')\n        print('\\n')\n\n        models.append(model.best_estimator_)\n        result_acc[index] = model.best_score_\n        result_auc[index] = auc\n\n\"\"\" Model performance during Grid Search \"\"\"\npd.DataFrame(list(zip(model_dict.values(), result_acc.values(), result_auc.values())), columns=['Model', 'Accuracy_rate','Roc_auc_rate'])","dfb07f28":"##Random Forest (Intensive GridSearchCV)\nimport time\n\ntic = time.perf_counter()\n\nrfc = RandomForestClassifier(random_state =42,n_jobs = 4)\n\nparam_grid = { \n    'n_estimators': [200,300,400,500,1000],\n#     'max_features': ['auto', 'sqrt', 'log2'],\n    'max_features': ['auto'],\n    'max_depth' : [8,10,12,15,20,25],\n#     'criterion' :['gini', 'entropy']\n    'criterion' :['gini','entropy']\n}\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10)\nCV_rfc.fit(X_train, y_train)\n\nprint(CV_rfc.best_params_)\nprint(CV_rfc.best_estimator_)\n\ntoc = time.perf_counter()\nprint(f\"executing time in {toc - tic:0.4f} seconds\")","b0620328":"\n#Put the best RF parameters (Full Model)\n\n##2nd submission model\nmodels = RandomForestClassifier(criterion='gini', random_state=42, oob_score=True, n_jobs=-1, max_depth=35, min_samples_leaf=80, min_samples_split=3, n_estimators=1000)\n\n##1st submission model\n# models = RandomForestClassifier(criterion='entropy', random_state=42, oob_score=True, n_jobs=-1, max_depth=15,\/\n#                                     min_samples_leaf=80, min_samples_split=3, n_estimators=200,max_features = 'auto')\n\nmodels.fit(X_train, y_train)\n\nauc = roc_auc_score(y_test, models.predict_proba(X_test)[:,1])\nprint('fullModel AUC = ',auc)\n\n# for feature in zip(X_test.columns, models.feature_importances_):\n#     print(feature)\n\nfeat_importances = pd.Series(models.feature_importances_, index=X_train.columns)\n# feat_importances.nlargest(40).plot(kind='bar')\n\n##Fit to test set\n\n# models = RandomForestClassifier(criterion='gini', random_state=42, oob_score=True, n_jobs=-1, max_depth=25, min_samples_leaf=80, min_samples_split=3, n_estimators=500)\n# models.fit(X_train, y_train)\n\nif 'y' in test_df_clean.columns:\n    del test_df_clean['y']\ntest_df1 = test_df_clean\n\ntest_result = test_df1\n\ntest_result['y'] = models.predict_proba(test_df1)[:, 1]\n\nsubmission = test_result[['y']]\n\nsubmission.index = np.arange(1, len(submission)+1) \nsubmission.index.name='Id'\n\nsubmission.to_csv(\"submissionRF5-fullmodel.csv\", index=True)","03335029":"#### Feature Importance ##################################################\nimport time\nfrom sklearn.feature_selection import SelectFromModel\n# Thrshold = [0.01]\n\nThrshold = [0.01,0.005,0.004,0.003,0.002,0.001,0.0005,0.0001,0.00005,0.00001,0]\n# sfm = SelectFromModel(models, threshold=0.0020)\n\nfor i in Thrshold:\n    tic = time.perf_counter()\n    sfm = SelectFromModel(models, threshold=i)\n\n    # Train the selector\n    sfm.fit(X_train, y_train)\n    print('--------------------------------------------')\n    featureList =[];\n    for feature_list_index in sfm.get_support(indices=True):\n        temp = X_train.columns[feature_list_index]\n        featureList.append(temp)\n\n    print(featureList)\n    \n    X_important_train = sfm.transform(X_train)\n    X_important_test = sfm.transform(X_test)\n\n    # Create a new random forest classifier for the most important features\n    clf_important = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n\n    # Train the new classifier on the new dataset containing the most important features\n    clf_important.fit(X_important_train, y_train)\n\n    y_pred = clf_important.predict(X_important_test)\n\n    y_important_pred = clf_important.predict(X_important_test)\n    \n    auc = roc_auc_score(y_test, clf_important.predict_proba(X_important_test)[:,1])\n    print('No of features = ',X_important_test.shape)\n    print('Threshold = ', i)\n    print('AUC score = ',auc)\n    \n    toc = time.perf_counter()\n    print(f\"executing time in {(toc - tic)\/60:0.4f} minutes\")\n    print('--------------------------------------------')","3e2ab6fa":"# Create a selector object that will use the random forest classifier to identify\n# features that have an importance of more than 0.0005 as per result of random Threshold above\nimport time\nfrom sklearn.feature_selection import SelectFromModel\n\ntic = time.perf_counter()\n\nsfm = SelectFromModel(models, threshold=0.0002)\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\nfeatureList =[];\nfor feature_list_index in sfm.get_support(indices=True):\n    temp = X_train.columns[feature_list_index]\n    featureList.append(temp)\n\nprint(featureList)\n    \nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)\n\n# Create a new random forest classifier for the most important features\nclf_important = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n\n# Train the new classifier on the new dataset containing the most important features\nclf_important.fit(X_important_train, y_train)\n\ny_pred = clf_important.predict(X_important_test)\n\ny_important_pred = clf_important.predict(X_important_test)\n\nauc = roc_auc_score(y_test, clf_important.predict_proba(X_important_test)[:,1])\nprint(auc)\n\ntoc = time.perf_counter()\nprint(f\"executing time in {toc - tic:0.4f} seconds\")\n# ---------------------------------------------------\n\n\nif 'y' in test_df_clean.columns:\n    del test_df_clean['y']\n\ntest_df1 = test_df_clean\n\ntest_df2 = test_df1[featureList]\n\ntest_df1['y'] = clf_important.predict_proba(test_df2)[:, 1]\nsubmission = test_df1[['y']]\n\nsubmission.index = np.arange(1, len(submission)+1) \nsubmission.index.name='Id'\n\nsubmission.to_csv(\"submissionRF5_reduced_model.csv\", index=True)","06f6f0c6":"### 11. Final Prediction -- reduced model","07ab1c92":"# INTENSIVE RANDOM FOREST ","9d99aa50":"### >> Data preprocessing","816498a8":"**Todo**\n1. Data collection\n2. Data cleaning\n3. Feature engineering\n4. Model training\n5. Model evaluation (Cross validation)","0243626b":"### >>> previous","c02c5d45":"### >>> Check missing value","adf8e3b9":"### >>> Categorical features disribution","16df0fd8":"### From previous results of data overview, features with 'unknown\/missing values are as follows:\n### 'education' , 'job' , 'housing' , 'loan' , 'default' , 'maritial'","ddb876d2":"### 10. Final model evaluation","49a19e4b":"### >>> nr.employed\t","7ad3c8b8":"### Import libaries","c013e018":"### >>> Mapping binary data and encoding","c6867e11":"## 2. Data cleaning\n","2f915c49":"## XGBoosting","9873fd5a":"### >> Data imputation","3ba4d014":"### >>>Age","ca8a5cc9":"### >> Data overview","60d0240e":"### >>> emp.var.rate","00f4da04":"### >> Knowing each numerical features","6561b512":"> ## Create Model","bd31bea8":"### >> Outlier and distribution check for numerical features","10abdad5":"# Precheck all Classifiers (takes time for 3 hours ++)","055167df":"### >>> Jobs & Housing","7a2a18ff":"### 5. Tuning Hyperparameters","ff052567":"### >>> Jobs & Education","d9247f4f":"### 7. Final prediction - Full model","23888d05":"### Feature Importance ","023dd847":"### Run RandomForest - Full Model ","65ac58f4":"### >>> Campaign","32d31b11":"### 2.Train XGBoost Model","00239c3a":"### >>>Campaign","00cde4db":"### From above table, the majority of the values for 'pdays' are missing. The majority of these missing values occur when the 'poutcome' is 'non-existent'. This means that the majority of the values in 'pdays' are missing because the customer was never contacted before. \n#### ('999' means customer was not previously contacted)\n\n### >>> To deal with this variable, the numerical variable 'pdays' is removed and categorical variables with categories \"pdays\" and \"pdays2\" is replaced.","37f8eaca":"### >>> duration","2ddc3644":"## Intensive GridSearchCV for RF (takes time for 3 hours ++)","dff25e71":"### >> Looking at 'Y' --> Data is highly imbalance","0a83edf7":"### 9. Tune Hyperparameter for final model","e43289ba":"## Create Model","8699a2d1":"### >> Standardizing the data","88a1c438":"**Todo**\n1. Data collection\n2. Data cleaning\n3. Feature engineering\n4. Model training\n5. Model evaluation (Cross validation)","502097be":"### >>>cons.price.idx","1178fcbe":"### 1. Create model","bf3b2b31":"### 6. Fit model and evaluate","0b8fcc29":"### >>> euribor3m\t","ee641358":"### Run RF with Reduced Model \/ Get Final Result","2f6cf1c9":"### 4. Evaluating Performance","13f2d1e6":"### >> Numerical features","6bd0bd2d":"### >>>Jobs & loan","aaf66ab5":"### >>>Using SMOTEnc to deal with inbalance data","89cc919c":"### >>> Default","17d6fd95":"## 1. Data collection & Data Overview\n###   >> Data reading","007cab9f":"# Team : \u0e40\u0e14\u0e47\u0e01\u0e2b\u0e25\u0e31\u0e07\u0e2b\u0e49\u0e2d\u0e07   (BADS\/ DS6)\n# Member :    1. Mintra Sojiphan          6220422057\n#             2. Amorn Tayakee            6220422058\n#             3. Nopparoek Pimsan         6220422059\n#             4. Traithep Junthep         6220422061\n#             5. Werapat Jintanachaiwat   6220422071","17b10081":"### >> Catagorical features","2daf0446":"### >> Looking at the relationship between catagorical features and dependent variable, 'Y'","7df61137":"## 1. Data collection & Data Overview\n###   >> Data reading","5ba1670b":"### >> Knowing each categorical features","0de7dfee":"### >>> Age","fd8dfb56":"### >>> pdays","a2c96395":"### 3. XGBoost Prediction","2a32cead":"### >>> Pdays \n#### >> missing Values are encoded as '999'. From the above table, it is clear that only 'pdays' has missing values. ","e94464c1":"### 8. Features importance"}}