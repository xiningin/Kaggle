{"cell_type":{"35159c93":"code","9b80560f":"code","b326b427":"code","ae1c7966":"code","fc138b39":"code","83213fd6":"code","2c3fd5fc":"code","66d26663":"code","fd97b9e5":"code","dbba4bfa":"code","e2fd39e4":"code","bf8dd111":"code","22189553":"code","fc0c9e17":"code","77af5aa0":"code","5ecf2bf6":"code","20355b5e":"code","90d8ac26":"code","81086b58":"code","36876dcc":"code","6b01c56c":"code","03934f1d":"code","dd55328e":"code","e16e97e0":"code","f749c9b6":"code","db57ec96":"code","b3285941":"code","543368ce":"code","bf9115ae":"code","e4f840d0":"code","ca0e5a57":"code","08daf3bd":"code","8a8fc971":"code","f714e35d":"code","bf2defbb":"code","69e07b62":"code","56f88be8":"code","bc4e5dc2":"code","c65b1072":"code","0fe33da3":"markdown","2749f946":"markdown","7b60b9ad":"markdown","697591db":"markdown","b9109e22":"markdown","33163db3":"markdown","42222499":"markdown","b934733b":"markdown","ea45b04a":"markdown"},"source":{"35159c93":"## Reference - AV Guided Hack Session.","9b80560f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\n\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import KFold, train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom lightgbm import LGBMRegressor\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# using classifier chains\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.naive_bayes import GaussianNB\nfrom scipy.sparse import csr_matrix","b326b427":"train = pd.read_csv('..\/input\/topic-modeling-for-research-articles-20\/Train.csv')\ntest = pd.read_csv('..\/input\/topic-modeling-for-research-articles-20\/Test.csv')\nss = pd.read_csv('..\/input\/topic-modeling-for-research-articles-20\/SampleSubmission.csv')\ntags = pd.read_csv('..\/input\/topic-modeling-for-research-articles-20\/Tags.csv')\npath = \".\/\"","ae1c7966":"ID_COL = 'id'\n\nTARGET_COLS = ['Analysis of PDEs', 'Applications',\n               'Artificial Intelligence', 'Astrophysics of Galaxies',\n               'Computation and Language', 'Computer Vision and Pattern Recognition',\n               'Cosmology and Nongalactic Astrophysics',\n               'Data Structures and Algorithms', 'Differential Geometry',\n               'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n               'Information Theory', 'Instrumentation and Methods for Astrophysics',\n               'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n               'Optimization and Control', 'Representation Theory', 'Robotics',\n               'Social and Information Networks', 'Statistics Theory',\n               'Strongly Correlated Electrons', 'Superconductivity',\n               'Systems and Control']\n\nTOPIC_COLS = ['Computer Science', 'Mathematics', 'Physics', 'Statistics']","fc138b39":"train.head(3)\ntest.head(3)\ntags.head(3)","83213fd6":"print(f'\\n->->->| Train contains {train.shape[0]} samples and {train.shape[1]} variables')\nprint(f'\\n->->->| Test contains {test.shape[0]} samples and {test.shape[1]} variables')\nprint(f'\\n->->->| Number of distinct SUB TOPICS is {len(TARGET_COLS)}')\n\nfeatures = [c for c in train.columns if c not in [ID_COL] + TARGET_COLS]\nprint(f'\\n->->->| The dataset contains {len(features)} features')","2c3fd5fc":"len(TARGET_COLS)","66d26663":"100 * (train[TARGET_COLS].sum()\/(train.shape[0])).sort_values(ascending=False)","fd97b9e5":"# Null Values\nnull_values_per_variable = 100 * (train.isnull().sum()\/train.shape[0]).round(3)#.reset_index()\nnull_values_per_variable.sort_values(ascending=False)","dbba4bfa":"train[TOPIC_COLS].sum().sort_values(ascending=False)","e2fd39e4":"100 * (train[TARGET_COLS].sum()\/(train.shape[0])).sort_values(ascending=False)","bf8dd111":"from wordcloud import WordCloud, STOPWORDS\n\nwc = WordCloud(stopwords = set(list(STOPWORDS) + ['inside']), random_state = 42)","22189553":"fig, axes = plt.subplots(2, 2, figsize=(20, 12))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i, sub_topic_name in enumerate(['Machine Learning', 'Artificial Intelligence', 'Computer Vision and Pattern Recognition', 'Robotics']):\n    sub_topic = train[train[sub_topic_name] == 1]\n    op = wc.generate(str(sub_topic['ABSTRACT']))\n    _ = axes[i].imshow(op)\n    _ = axes[i].set_title(sub_topic_name.upper(), fontsize=24)\n    _ = axes[i].axis('off')\n    \n_ = plt.suptitle('TOP WORDS FOR A GIVEN SUBTOPIC', fontsize=30)","fc0c9e17":"fig, axes = plt.subplots(2, 2, figsize=(20, 12))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i, sub_topic_name in enumerate(TOPIC_COLS):\n    sub_topic = train[train[sub_topic_name] == 1]\n    op = wc.generate(str(sub_topic['ABSTRACT']))\n    _ = axes[i].imshow(op)\n    _ = axes[i].set_title(sub_topic_name.upper(), fontsize=24)\n    _ = axes[i].axis('off')\n_ = plt.suptitle('TOP WORDS FOR A GIVEN TOPIC', fontsize=30)","77af5aa0":"!pip install --upgrade transformers ","5ecf2bf6":"!pip install --upgrade simpletransformers ","20355b5e":"train['labels'] = list(zip(train['Analysis of PDEs'].tolist(),\n                        train['Applications'].tolist(),\n                        train['Artificial Intelligence'].tolist(),\n                        train['Astrophysics of Galaxies'].tolist(),\n                        train['Computation and Language'].tolist(),\n                        train['Computer Vision and Pattern Recognition'].tolist(),\n                        train['Cosmology and Nongalactic Astrophysics'].tolist(),\n                        train['Data Structures and Algorithms'].tolist(),\n                        train['Differential Geometry'].tolist(),\n                        train['Earth and Planetary Astrophysics'].tolist(),\n                        train['Fluid Dynamics'].tolist(),\n                        train['Information Theory'].tolist(),\n                        train['Instrumentation and Methods for Astrophysics'].tolist(),\n                        train['Machine Learning'].tolist(),\n                        train['Materials Science'].tolist(),\n                        train['Methodology'].tolist(),\n                        train['Number Theory'].tolist(),\n                        train['Optimization and Control'].tolist(),\n                        train['Representation Theory'].tolist(),\n                        train['Robotics'].tolist(),\n                        train['Social and Information Networks'].tolist(),\n                        train['Statistics Theory'].tolist(),\n                        train['Strongly Correlated Electrons'].tolist(),\n                        train['Superconductivity'].tolist(),\n                        train['Systems and Control'].tolist()\n                        ))\ntrain['text'] = train['ABSTRACT'].apply(lambda x: x.replace('\\n', ' '))\ncols = ['text','labels']","90d8ac26":"trn, val = train_test_split(train, test_size=0.22, random_state=2000)","81086b58":"# preprocessing\ndef preprocess(text):\n    processed_text =text.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n    # Remove punctuation\n    processed_text = processed_text.str.replace(r'[^\\w\\d\\s]', ' ')\n    \n    # Replace whitespace between terms with a single space\n    processed_text = processed_text.str.replace(r'\\s+', ' ')\n    \n    # Remove leading and trailing whitespace\n    processed_text = processed_text.str.replace(r'^\\s+|\\s+?$', '')\n    \n    # change words to lower case - Hello, HELLO, hello are all the same word\n    processed_text = processed_text.str.lower()\n    \n    return processed_text\n\n#processed_text  = preprocess(all_text)\n\ndef get_texts(df):\n    #texts = df['ABSTRACT'].apply(preprocess)\n    df['ABSTRACT'] = preprocess(df['ABSTRACT'])\n    #texts = texts.values.tolist()\n    return df","36876dcc":"import torch, gc\ntrain = get_texts(train)\ntest = get_texts(test)\n\ntrn = get_texts(trn)\nval = get_texts(val)\n\ncols = ['text','labels']\n\n\ngc.collect()\ntorch.cuda.empty_cache()","6b01c56c":"%%time\nfrom simpletransformers.classification import MultiLabelClassificationModel\nimport pandas as pd\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\ntrain_df = pd.DataFrame(trn[cols]) #pd.DataFrame(trn[cols], columns=[cols])\neval_df = pd.DataFrame(val[cols])\n\n# Use bert-large-cased with 5 epochs - Kaggle Disk space goes out with large cased.\n\n# Create a MultiLabelClassificationModel\nmodel = MultiLabelClassificationModel('bert', 'bert-base-cased', num_labels=25, args={'reprocess_input_data': True, \n                                                                                      'gradient_accumulation_steps':16, 'learning_rate': 3e-5, \n                                                                                      'max_seq_length': 128,\n                                                                                     'overwrite_output_dir': True, 'num_train_epochs': 3})\n# Train the model\nmodel.train_model(train_df[cols]) #,eval_df=eval_df[cols])","03934f1d":"result, model_outputs, wrong_predictions = model.eval_model(eval_df)\nresult","dd55328e":"def get_best_thresholds(true, preds):\n    thresholds = [i\/100 for i in range(100)]\n    best_thresholds = []\n    for idx in range(25):\n        f1_scores = [f1_score(true[:, idx], (preds[:, idx] > thresh) * 1) for thresh in thresholds]\n        best_thresh = thresholds[np.argmax(f1_scores)]\n    \n        best_thresholds.append(best_thresh)\n    return best_thresholds","e16e97e0":"best_thresholds = get_best_thresholds(val[TARGET_COLS].values, model_outputs)\nbest_thresholds\nfor i, thresh in enumerate(best_thresholds):\n    model_outputs[:, i] = (model_outputs[:, i] > thresh) * 1\n\nf1_score(val[TARGET_COLS], model_outputs, average='micro')","f749c9b6":"to_predict = test.ABSTRACT.apply(lambda x: x.replace('\\n', ' ')).tolist()\npreds, outputs = model.predict(to_predict)\noutputs_p = outputs\n\n# Save the probabilities to blend later\npd.DataFrame(outputs,columns=TARGET_COLS).to_csv(path + 'Simple_transformers_Prob.csv')\n\nfor i, thresh in enumerate(best_thresholds):\n    outputs[:, i] = (outputs[:, i] > thresh) * 1","db57ec96":"# Download file function\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","b3285941":"create_download_link(ss, filename = 'hacklive_fastai.csv')","543368ce":"!pip install scikit-multilearn","bf9115ae":"vec = CountVectorizer(max_features=10000)\n_ = vec.fit(list(train['ABSTRACT']) + list(test['ABSTRACT']))\n\ntrn_abs = vec.transform(trn['ABSTRACT'])\nval_abs = vec.transform(val['ABSTRACT'])\ntst_abs = vec.transform(test['ABSTRACT'])\nprint(trn_abs.shape, val_abs.shape, tst_abs.shape)\n\ntrn2 = np.hstack((trn_abs.toarray(), trn[TOPIC_COLS]))\nval2 = np.hstack((val_abs.toarray(), val[TOPIC_COLS]))\ntst2 = np.hstack((tst_abs.toarray(), test[TOPIC_COLS]))\n\nprint(trn2.shape, val2.shape, tst2.shape)","e4f840d0":"trn2 = csr_matrix(trn2.astype('int16'))\nval2 = csr_matrix(val2.astype('int16'))\ntst2 = csr_matrix(tst2.astype('int16'))","ca0e5a57":"\n# initialize classifier chains multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = ClassifierChain(LogisticRegression(C = 12, n_jobs=-1))\n\n# train\nprint('Start Training...')\nclassifier.fit(trn2, trn[TARGET_COLS])\n\n# predict\nprint('Start Predicting...')\nval_preds = classifier.predict_proba(val2)\n\nval_preds = val_preds.toarray()\nbest_thresholds = get_best_thresholds(val[TARGET_COLS].values, val_preds)","08daf3bd":"val_preds = classifier.predict_proba(val2)\nbest_thresholds = get_best_thresholds(val[TARGET_COLS].values, val_preds.toarray())\nval_preds = val_preds.toarray()","8a8fc971":"for i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\n\nf1_score(val[TARGET_COLS], val_preds, average='micro')","f714e35d":"preds_test = classifier.predict_proba(tst2)\npreds_test = preds_test.toarray()\npreds_test_p = preds_test\n\nfor i, thresh in enumerate(best_thresholds):\n    preds_test[:, i] = (preds_test[:, i] > thresh) * 1\n\nss[TARGET_COLS] = preds_test\ncreate_download_link(ss, filename = 'hacklive_NLP_count_added_topics_ClassifierChain1.csv')","bf2defbb":"p1 = pd.DataFrame(outputs_p,columns=TARGET_COLS)\np2 = pd.DataFrame(preds_test_p,columns=TARGET_COLS)","69e07b62":"preds_test_Fin = p1\nfor label in TARGET_COLS:\n    preds_test_Fin[label] = (0.40*p1[label])+(0.60*p2[label])","56f88be8":"for i, thresh in enumerate(best_thresholds):\n  #print(\"COL : \" + str(i))\n  preds_test_Fin.iloc[:,i] =  (preds_test_Fin.iloc[:,i] > 0.25) * 1\n\nss[TARGET_COLS] = preds_test\ncreate_download_link(ss, filename = 'hacklive_NLP_count_added_topics_ClassifierChain1.csv')","bc4e5dc2":"ss.to_csv(\"submission.csv\")","c65b1072":"# finish","0fe33da3":"## Classifier Chains and Logistic Regression","2749f946":"## Read the data and basic EDA","7b60b9ad":"### Analytics Vidhya - HackLive 3 - 71.05% LB Score- Rank 50 Simple Transformers and Classifier Chains - Solution","697591db":"Topic Modeling for Research Articles 2.0 Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process.\n\nEarlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.\n\nGiven the abstracts for a set of research articles, predict the tags for each article included in the test set. Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics:\n\nComputer Science\n\nMathematics\n\nPhysics\n\nStatistics\n\nList of possible tags are as follows:\n\n[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]","b9109e22":"## MODEL BUILDING","33163db3":"## Simple Transformers","42222499":"## Ensemble","b934733b":"* ### Target Distribution","ea45b04a":"PROBLEM STATEMENT"}}