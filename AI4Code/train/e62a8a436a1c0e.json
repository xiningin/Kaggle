{"cell_type":{"0626d4d6":"code","9976042b":"code","3ea25be7":"code","ca986992":"code","7eacc888":"code","01d394e3":"code","73b812f5":"code","eb209c5c":"code","51caca30":"code","1c2b7734":"code","0e8e6c7a":"code","144fa433":"code","46b9ff2a":"code","c6aca89c":"code","1c340569":"code","7cf3e990":"code","0c659b24":"code","0436271a":"code","e755673a":"code","99fef5fe":"code","8bf5d5dc":"code","4e76af8b":"code","cdf9ea83":"code","6d891700":"code","edbb1c49":"code","466d12f6":"code","f831b1db":"code","09bfcfeb":"code","4345e1b3":"code","089ced32":"code","183f70ea":"markdown","d7d6bd2a":"markdown","9435c61e":"markdown","ca05be2c":"markdown","9276cfdc":"markdown","169b2428":"markdown","16654a31":"markdown","6b431917":"markdown","1ff8dbfc":"markdown","ebcfc190":"markdown"},"source":{"0626d4d6":"#Importing what we are going to use\nimport pandas as pd, numpy as np, seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.base import clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.preprocessing import RobustScaler,PowerTransformer, OrdinalEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import ElasticNetCV, Lasso, LinearRegression \nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor","9976042b":"#importing our training and testing data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","3ea25be7":"sns.scatterplot(x='GrLivArea', y='SalePrice' , data=train)","ca986992":"train = train[train.loc[:, 'GrLivArea'] < 4000] #deleting the outliers tht we've talked about.\n#meringing the two data sets together so we preprocess them together\ny  = train['SalePrice']\ntrain.drop('SalePrice', axis=1, inplace=True)\ndata = train.append(test)","7eacc888":"sns.distplot(y, fit=norm) #as you can see the target value is skewed now let's log it ","01d394e3":"y = np.log(y) \nsns.distplot(y, fit=norm)#it's so much better now","73b812f5":"#Checking how much missing values for each column we have\ncol_nnan = pd.DataFrame(data.isna().sum()).sort_values(0, ascending=False)\ncol_nnan.iloc[:34]","eb209c5c":"data['MasVnrType'].fillna('None+', inplace=True)\ndata['MasVnrArea'].fillna(data['MasVnrArea'].mean(), inplace=True)\ndata['BsmtFinType1'].fillna('None', inplace=True)\ndata['BsmtFinType2'].fillna('None', inplace=True)\ndata['BsmtCond'].fillna('None', inplace=True)\ndata['BsmtQual'].fillna('None', inplace=True)\ndata['BsmtExposure'].fillna('None', inplace=True)\ndata['Fence'].fillna('None', inplace=True)\ndata['LotFrontage'].fillna(0, inplace=True)\ndata['FireplaceQu'].fillna('None', inplace=True)\ndata['Alley'].fillna('None', inplace=True)\ndata['PoolQC'].fillna('None', inplace=True)\ndata['MiscFeature'].fillna('None', inplace=True)\ndata[['GarageCond', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageType']]\nfor col in ['GarageCond', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageType']:\n    if 'int' in str(data[col].dtype) or 'float' in str(data[col].dtype):\n        data[col].fillna(0, inplace=True)\n    else:\n        data[col].fillna('None', inplace=True)\nfor col in ['MSZoning', 'BsmtHalfBath', 'BsmtFullBath', 'Functional', 'BsmtUnfSF', 'SaleType', 'BsmtFinSF1', 'KitchenQual', 'BsmtFinSF2', 'GarageCars', 'TotalBsmtSF', 'Electrical', 'Exterior2nd', 'Exterior1st', 'GarageArea']:\n    if 'float' in str(data[col].dtype) or 'int' in str(data[col].dtype) and data[col].value_counts().shape[0] > 500:\n        data[col].fillna(data[col].mean(), inplace=True)\n    elif 'object' in str(data[col].dtype):\n        data[col].fillna(data[col].mode()[0], inplace=True)# mode function have in it's output's index 0 the most frequent value","51caca30":"data['Utilities'].value_counts()","1c2b7734":"data.drop(['Utilities', 'Id'], axis=1, inplace=True)\n#it looks like Utilities doesn't contain useful information since all values are the same except for one\n#Id either doesn't represent a useful information since it's a non repetitive personal information","0e8e6c7a":"print('We have in our Data {} Features, {} Samples, {} NaNs'.format(data.shape[1], data.shape[0], data.isna().sum().sum()))","144fa433":"data['YearBuiltCategorie'] = pd.cut(data['YearBuilt'], bins=[-np.inf, 1900, 1950, 2000, +np.inf], labels=[0, 1, 2, 3])\n#generating new column from YearBuilt column as follows if the value in YearBuilt between -inifiti and 1900 then\n#it's correspondance in the new column it's 0 \n#else if it's between 1900 and 1950 then it's 1 else if it's between 1950 and 2000 then it's 2\n#else 3 (between 2000 and +infiniti)","46b9ff2a":"#getting the categorical columns\nnumeric_cols = ['LotFrontage', 'LotArea', 'BsmtFinSF2', 'LowQualFinSF', 'YrSold', 'YearBuilt', 'BsmtFullBath', 'BsmtHalfBath', 'YearRemodAdd', 'GarageCars',  'Fireplaces', 'TotRmsAbvGrd', 'KitchenAbvGr', 'BedroomAbvGr', 'HalfBath', 'FullBath', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageYrBlt', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal', 'PoolArea']\ncategorical_cols = []\nfor col in data.columns:\n    if col not in numeric_cols:\n        categorical_cols.append(col)","c6aca89c":"#Finding for the skewed features so we make them normal like as we talked about in the previous cells\nskewed_cols = (np.abs(data[numeric_cols].skew()) > 0.5)\nskewed_cols = np.array(skewed_cols.keys())[np.array(skewed_cols.values)]","1c340569":"#Normal Like the Skewed features\npws = PowerTransformer()\ndata.loc[:, skewed_cols] = pws.fit_transform(data.loc[:, skewed_cols])","7cf3e990":"#O_cols contains all the columns that their values should be ordered because they have information in the order\n#Now we encode the categories of these columns manually because OrdinalEncoder\/LabelEncoder will miss to code them in order\nO_cols =  ['MoSold', 'OverallCond', 'YearBuiltCategorie' ,'OverallQual', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtFinType1','BsmtCond', 'BsmtExposure', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond']\ndata.replace({\n              'ExterQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4,\n                            'Ex': 5},\n              'ExterCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4,\n                            'Ex': 5},\n    \n              'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4, \n                           'Ex': 5},\n              'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4,\n                           'Ex': 5},\n              'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n              'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4,\n                               'ALQ': 5, 'GLQ': 6},\n              'BsmtFinType2': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4,\n                               'ALQ': 5, 'GLQ': 6},\n    \n              'HeatingQC': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4,\n                            'Ex': 5},\n    \n              'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4,\n                              'Ex': 5},\n    \n              'FireplaceQu': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4,\n                              'Ex': 5},\n              \n              'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4,\n                             'Ex': 5},\n              'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA':3, 'Gd': 4,\n                             'Ex': 5},\n              'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n    \n              'PavedDrive': {'None': 0, 'N': 1, 'P': 2, 'Y': 3},\n    \n              'PoolQC': {'None': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n    \n              'Fence': {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},\n    \n              'Functional': {'None': 0, 'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4,\n                             'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8}\n    \n             }, inplace=True)\n\n\n#Finding the categorical columns that their values have no information in ordering them and then Label Encoding them\nOB_cols = []\nfor col in categorical_cols:\n    if col not in O_cols:\n        OB_cols.append(col)\nenc = OrdinalEncoder()\ndata[OB_cols] = enc.fit_transform(np.c_[data[OB_cols]])","0c659b24":"train = data.iloc[:len(y)].copy()\ntest = data.iloc[len(y):].copy()","0436271a":"RMSLE =  lambda y_true, y_pred : np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))#This will calculate our RMSLError\ndef cvoRMSLE(model, X, y, cv=5, shuffle=True): #it performs Cross Validation on the given data with 5-Folds as default\n    kfolds = KFold(n_splits=cv, shuffle=shuffle)\n    scores = []\n    for training_indices, testing_indices in kfolds.split(X, y):\n        model.fit(X[training_indices], np.c_[y[training_indices]])\n        scores.append(RMSLE(y[testing_indices], model.predict(X[testing_indices])))\n    return np.array(scores)","e755673a":"ElasticNet = make_pipeline(RobustScaler(), ElasticNetCV(alphas=[1, 2, 3, 4, 5, 6, 7, 8, .1, .2, .3, .4, .6, .7, .8, .9, 1e-2, 1e-4, 1e-3, 5e-4]))\nElasticNet.fit(train.values, np.c_[y])\nbest_alpha = ElasticNet[1].alpha_ \n#after finding the best alpha w search arround that alpha for better accuracy\n\nElasticNet = make_pipeline(RobustScaler(), ElasticNetCV(alphas=[best_alpha, best_alpha*.9, best_alpha*.8, best_alpha*.7, best_alpha*.6, \n                                                                best_alpha*1.05, best_alpha*1.2, best_alpha*1.5, best_alpha*2]))\nprint(cvoRMSLE(ElasticNet, train.values, np.c_[y], shuffle=False).mean())\nbest_alpha = ElasticNet[1].alpha_\n\n","99fef5fe":"lasso = make_pipeline(RobustScaler(), Lasso(alpha=best_alpha))\nlasso.fit(train.values, np.c_[y])\ncvoRMSLE(lasso, train.values, np.c_[y]).mean()","8bf5d5dc":"KRR = make_pipeline(RobustScaler(), KernelRidge())\nKRR.fit(train.values, np.c_[y])\ncvoRMSLE(KRR, train.values, np.c_[y], shuffle=False).mean()","4e76af8b":"LR = LinearRegression()\ncvoRMSLE(LR, train.values, np.c_[y], shuffle=False).mean()#0.1208868807912646\n","cdf9ea83":"CatReg = CatBoostRegressor(verbose=0, iterations=2500)\ncvoRMSLE(CatReg, train.values, np.c_[y], shuffle=False).mean()","6d891700":"class Stack_Models(object):\n    def __init__(self, base_models=[]):\n        self.base_models = [clone(model) for model in base_models]\n        self.k = None\n    \n    def fit(self, X, y, cv=5):\n        self.k = cv\n        kfold = KFold(n_splits=self.k, shuffle=True)\n        generated_data = np.zeros((len(X), len(self.base_models)))\n        for i in range(len(self.base_models)):\n            model = self.base_models[i]\n            for training_indices, testing_indices in kfold.split(X, y):\n                X_train, y_train = X[training_indices], y[training_indices]\n                X_test, y_test = X[testing_indices], y[testing_indices]\n                model.fit(X_train, y_train) \n                generated_data[testing_indices, i] = model.predict(X_test).reshape(-1)\n            model.fit(X, y)\n        return np.column_stack(generated_data)\n        \n    def predict(self, X):\n        data = np.column_stack([ model.predict(X) for model in self.base_models])\n        return data\n        ","edbb1c49":"stacking = Stack_Models(base_models=[KRR, LR, CatReg, lasso, ElasticNet])","466d12f6":"new_data = stacking.fit(train.values, np.c_[y]).T","f831b1db":"#Now we are going to train the data comming from the first layer onto two models (meta models)\n#Model 1\ncat = CatBoostRegressor(verbose=0)\ncat.fit(new_data, np.c_[y])\nprint(cvoRMSLE(cat, new_data, np.c_[y]).mean())\n#Model 2\nxgbr = XGBRegressor(objective='reg:squarederror')\nxgbr.fit(new_data, np.c_[y])\nprint(cvoRMSLE(xgbr, new_data, np.c_[y]).mean())","09bfcfeb":"test_new_data = stacking.predict(test.values)","4345e1b3":"y_pred = (np.exp(cat.predict(test_new_data)) + np.exp(xgbr.predict(test_new_data)))\/2","089ced32":"y_pred.mean()","183f70ea":"Now we are done with preprocessing,that wasn't that hard wasn't it?. Now let's split our data!","d7d6bd2a":"<b><h4>Now if you don't know Cross Validation or you don't know how it works I really encourage you to watch this video for better understanding about what we're going to talk about in the next cells : \n    https:\/\/youtu.be\/fSytzGwwBVw<\/h4><\/b>","9435c61e":"4. Now we are going to impute the features that have missing values which means replacing the missing values by another value other than NaN, Notice in this case that most of columns that have NaN means it lacks that thing (the NON-Exictence of that thing) for example the columns PoolQC have lot of NaNs because of houses deosn't have a Pool.","ca05be2c":"1.     Before I start doing some code, I want to say that I've learnt a lot of new ideas from this competition and from the notebooks that people have shared so I want to thank everyone who shared his notebook in this competition specially these two guys : <br>\n   <b>Serigne CISSE<\/b> : https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard <br>\n   <b>Julien COHEN SOLAL <\/b> : https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset \n  <br>\nNow let's start with our workd :)) ENJOY.","9276cfdc":"5. Now Let's Start With the Linear Regression Models <br>\nNotice [NameModel]CV means that we can fine tune the model directly no need to make a separated grid\/random search to find the optimal parameter's values.","169b2428":"3. Now we check on the Distribution of the target value, since later we are going to use Linear Models it's advisable to take a look at the target distribution and if you have found that the distribution is far away from being normally distibuted ( we have skeweed distribution in this case) try to fix that by applying np.log(x) (because all values are positif here if you had in other case negativ and positiv values try sklearn.preprocessing.PowerTransformer(default=\u2019yeo-johnson\u2019)) which makes the distribution normal-like => which makes values closer to each other that means really huge and small values (outliers) make less negativ effect on the generalization of the linear model which proves the accuracy, we are going in later cells to do the same think on some skewed features but instead of using log(x) transformation we are going to apply PowerTransformer which uses Yeo-Johnson transformation as default and transforms negativ and positiv vaules.","16654a31":"Anyways Thank you so much for sticking to the end of the notebook I really appreciate it. tell me if you found that helpful, the models aren't fine tuned properly for simplicity reasons, I consider myself a beginner so if you have anything that I might did better just tell me in the comments, hope to see you in a future notebooks. <b> KEEP LEARNING :))<\/b>","6b431917":"Now We Are Going to Do what is called \"Stacking\" it's a type of ensembeling models to get better performance so there is two type of stacking here we are going to work with type two which uses K-Folds, Great Source to get the Big Picture of these two types :<br>\nStacking Type One (Stacking with a holdout dataset) : https:\/\/youtu.be\/enEerl0feRo<br>\nStacking Type Two (Stacking with K-Folds) :  https:\/\/youtu.be\/WpbGXGgWyVg","1ff8dbfc":"If you are thinking that how we are going to get the real prediction since we are going to train the models on the logged [log(y)] version of the target,the answer is that's true we are going the train on log(y) BUT we can use the inverse function of log(x) which is e^x (np.exp(x)) to get the real prediction => <b>log(y) = w <=> y = exp(w)<\/b>","ebcfc190":"2. The author of the dataset recommended removing any sample that have value GrLivArea > 4000, why? because they are considered as outliers, cause as you can see after the value 4000 in GrLivArea there are samples with high living area and relativily cheap price and houses with really high prices that are far from the other points we don't want our model to generalize on these outliers so we just remove them, I'm sure if you dig into other columns you will find cases like this but removing all of them will affect negativly the performance of the model we deleted those 4 samples because they were like extrem outliers so to speak. => when removing outliers make sure to remove some outliers not a lot relative to the size of the dataset, when I started this competition I wasn't aware of these outliers then after deleting them I advanced about 200 place in the LB (LeaderBoard) that's how powerful deleting outliers is."}}