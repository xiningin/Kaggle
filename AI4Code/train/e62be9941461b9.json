{"cell_type":{"1e4c4a1a":"code","d7cf96bc":"code","211e61ba":"code","f1546c3b":"code","89d2faa4":"code","c4c02c7a":"code","307ccc51":"code","20198bed":"code","0d92744d":"code","14053394":"code","d1d3146d":"markdown","0606443d":"markdown","411e66f8":"markdown","6ea08551":"markdown","8e8ce191":"markdown","20a28689":"markdown","5087e310":"markdown"},"source":{"1e4c4a1a":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport sklearn.datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","d7cf96bc":"X, y = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)","211e61ba":"X","f1546c3b":"# Making X two-dimensional\nX = X.loc[:, ['sepal length (cm)', 'petal length (cm)']]\n\n# Scaling X to center it at the origin\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)","89d2faa4":"X","c4c02c7a":"# Applying PCA without dimensionality reduction\npca = PCA(n_components=2)\nX_pca = pd.DataFrame(pca.fit_transform(X), index=X.index, columns=[\"PC1\", \"PC2\"])","307ccc51":"X_pca","20198bed":"# \nX_eigenvectors = pd.DataFrame(np.linalg.svd(np.cov(X.T))[0])\nX_eigenvectors","0d92744d":"pca_eigenvectors = pd.DataFrame(np.linalg.svd(np.cov(X_pca.T))[0])\npca_eigenvectors","14053394":"plt.figure(figsize=(20, 10))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X['sepal length (cm)'], X['petal length (cm)'])\nplt.arrow(\n    x=0,\n    y=0,\n    dx=X_eigenvectors.iloc[0, 0],\n    dy=X_eigenvectors.iloc[1, 0],\n    width=0.05,\n    color='red',\n    label=\"PC1\"\n)\nplt.arrow(\n    x=0,\n    y=0,\n    dx=X_eigenvectors.iloc[0, 1],\n    dy=X_eigenvectors.iloc[1, 1],\n    width=0.05,\n    color='yellow',\n    label=\"PC2\"\n)\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.xlabel(\"Sepal Length\")\nplt.ylabel(\"Petal Length\")\nplt.title(\"Some Two-Dimensional Data\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_pca['PC1'], X_pca['PC2'])\nplt.arrow(\n    x=0,\n    y=0,\n    dx=pca_eigenvectors.iloc[0, 0],\n    dy=pca_eigenvectors.iloc[1, 0],\n    width=0.05,\n    color='red',\n    label=\"PC1\"\n)\nplt.arrow(\n    x=0,\n    y=0,\n    dx=pca_eigenvectors.iloc[0, 1],\n    dy=pca_eigenvectors.iloc[1, 1],\n    width=0.05,\n    color='yellow',\n    label=\"PC2\"\n)\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"The Same Data After PCA\")\nplt.legend()\n\nplt.show()","d1d3146d":"# Applying PCA","0606443d":"# Visualizing PCA\n\nThe principal components are calculated by finding the eigenvectors of the covariance matrix for the data.  \nWe can calculate the eigenvectors using singular value decomposition (SVD), since the covariance matrix always satisfied symmetric positive semi-definiteness.\n\nLet's calculate them and plot them on a scatterplot both before and after PCA is applied.","411e66f8":"# YouTube Tutorial Included!  \n  \n***\n  \nThis notebook was made to accompany a YouTube video that I made for my channel.  \n  \nIf you want an in-depth explanation of the steps taken, you can check out the video here:  \nhttps:\/\/youtu.be\/QGLH-FUafLI","6ea08551":"# Preprocessing","8e8ce191":"# Understanding Principal Component Analysis\n\nThis will be a quick demo of how principal components can be calculated.  \n  \nWe will not be using PCA for dimensionality reduction today, but rather just using it to get an intuitive sense of the process.","20a28689":"# Getting Started","5087e310":"We can see that the principal components are used as the new basis for the feature space of our data.  \n  \nPCA redefines the way we look at the space by generating new feautures that point in the direction of greatest variance in the data."}}