{"cell_type":{"ba1286af":"code","ee3fb431":"code","f6c139fe":"code","fb31a83f":"code","08ab3464":"code","b09b1e7f":"code","5e7cc394":"code","2c64929c":"code","b7104f94":"code","c5e599e2":"code","10060474":"code","41b857d5":"code","c47c5d74":"code","3452bf7d":"code","d16cc3fc":"code","60c92a37":"code","16010bc3":"markdown","ae756ef4":"markdown","70a66d30":"markdown"},"source":{"ba1286af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ee3fb431":"data = pd.read_csv('..\/input\/adult.csv')\n# Delete rows with missing data\ndata = data.loc[(data['workclass']!='?') & (data['occupation']!='?') & (data['native.country']!='?')]","f6c139fe":"def mapping(column_str):\n    data_ = data[column_str]\n    list_ = list(set(data_))\n    map_ = {}\n    for i in range(len(list_)):\n        map_[list_[i]] = i\n    return data_.map(map_)\n\n# Mapping non-numeric data to numeric data\nnon_numeric = [\"workclass\", \"marital.status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native.country\", \"income\"]\nfor column in non_numeric:\n    data[column] = mapping(column)","fb31a83f":"X = data[['age','workclass', 'education.num', 'marital.status','occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country']]\nY = data[\"income\"]","08ab3464":"# split data to train and test\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25, random_state=2019)","b09b1e7f":"#LR\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(train_X, train_Y)\npred_Y = lr.predict(test_X)\nprint('LR:')\nprint('accuracy %s' % metrics.accuracy_score(test_Y, pred_Y))\nprint('F1_score %s' % metrics.f1_score(test_Y, pred_Y))\nprint('auc %s' % metrics.roc_auc_score(test_Y, pred_Y))","5e7cc394":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nk_range = np.arange(1, 31)\naccuracy_scores = []\nF1_scores = []\nauc_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(train_X, train_Y)\n    pred_Y = knn.predict(test_X)\n    accuracy_scores.append(metrics.accuracy_score(test_Y, pred_Y))\n    F1_scores.append(metrics.f1_score(test_Y, pred_Y))\n    auc_scores.append(metrics.roc_auc_score(test_Y, pred_Y))\nprint('KNN:')\nprint('accuracy : %s' % accuracy_scores.index(max(accuracy_scores)), max(accuracy_scores))\nprint('F1_score : %s' % F1_scores.index(max(F1_scores)), max(F1_scores))\nprint('auc : %s' % auc_scores.index(max(auc_scores)), max(auc_scores))","2c64929c":"#plot k and metrics in knn model\n#accuracy\nplt.figure(1)\nplt.plot(k_range, accuracy_scores)\nplt.xlabel('K')\nplt.ylabel('Accuracy')\n#F1_score\nplt.figure(2)\nplt.plot(k_range, F1_scores)\nplt.xlabel('K')\nplt.ylabel('F1_scores')\n#auc\nplt.figure(3)\nplt.plot(k_range, auc_scores)\nplt.xlabel('K')\nplt.ylabel('auc')","b7104f94":"# Decision Tree\nfrom sklearn import tree\ndeep_range = np.arange(1,21)\naccuracy_scores = []\nF1_scores = []\nauc_scores = []\nfor max_deep in deep_range:\n    dt = tree.DecisionTreeClassifier(max_depth=max_deep)\n    dt = dt.fit(train_X, train_Y)\n    pred_Y = dt.predict(test_X)\n    accuracy_scores.append(metrics.accuracy_score(test_Y, pred_Y))\n    F1_scores.append(metrics.f1_score(test_Y, pred_Y))\n    auc_scores.append(metrics.roc_auc_score(test_Y, pred_Y))\nprint('Decision Tree:')\nprint('accuracy : %s' % accuracy_scores.index(max(accuracy_scores)), max(accuracy_scores))\nprint('F1_score : %s' % F1_scores.index(max(F1_scores)), max(F1_scores))\nprint('auc : %s' % auc_scores.index(max(auc_scores)), max(auc_scores))","c5e599e2":"#plot max_depth in decision tree model and metrics\n#accuracy\nplt.figure(1)\nplt.plot(deep_range, accuracy_scores)\nplt.xlabel('max_depth')\nplt.ylabel('Accuracy')\n#F1_score\nplt.figure(2)\nplt.plot(deep_range, F1_scores)\nplt.xlabel('max_depth')\nplt.ylabel('F1_scores')\n#auc\nplt.figure(3)\nplt.plot(deep_range, auc_scores)\nplt.xlabel('max_depth')\nplt.ylabel('auc')","10060474":"#NB\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB().fit(train_X, train_Y)\npred_Y = nb.predict(test_X)\nprint('NB:')\nprint('accuracy %s' % metrics.accuracy_score(test_Y, pred_Y))\nprint('F1_score %s' % metrics.f1_score(test_Y, pred_Y))\nprint('auc %s' % metrics.roc_auc_score(test_Y, pred_Y))","41b857d5":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=80, max_features='auto', max_depth=15)\nrf = rf.fit(train_X, train_Y)\npred_Y = rf.predict(test_X)\nprint('Random Forest:')\nprint('accuracy %s' % metrics.accuracy_score(test_Y, pred_Y))\nprint('F1_score %s' % metrics.f1_score(test_Y, pred_Y))\nprint('auc %s' % metrics.roc_auc_score(test_Y, pred_Y))","c47c5d74":"#GB\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(n_estimators=150, learning_rate=0.2, max_depth=4)\ngbc = gbc.fit(train_X, train_Y)\npred_Y = gbc.predict(test_X)\nprint('GBC:')\nprint('accuracy %s' % metrics.accuracy_score(test_Y, pred_Y))\nprint('F1_score %s' % metrics.f1_score(test_Y, pred_Y))\nprint('auc %s' % metrics.roc_auc_score(test_Y, pred_Y))","3452bf7d":"#SVM\nfrom sklearn.svm import SVC\nsvm = SVC()\nsvm = svm.fit(train_X, train_Y)\npred_Y = svm.predict(test_X)\nprint('SVM:')\nprint('accuracy %s' % metrics.accuracy_score(test_Y, pred_Y))\nprint('F1_score %s' % metrics.f1_score(test_Y, pred_Y))\nprint('auc %s' % metrics.roc_auc_score(test_Y, pred_Y))","d16cc3fc":"#Neural Networks\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,BatchNormalization\nmodel=Sequential([\n    BatchNormalization(input_shape=(12,)),\n    Dense(16,activation='relu'),\n    Dropout(0.5),\n    Dense(8,activation='relu'),\n    Dropout(0.5),\n    Dense(4,activation='relu'),\n    Dense(1,activation='sigmoid'),\n])\nmodel.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x=train_X, y=train_Y, batch_size=256, epochs=30)","60c92a37":"y_pred = np.squeeze(model.predict_classes(test_X))\nprint('Nerual Network:')\nprint('accuracy %s' % metrics.accuracy_score(test_Y, pred_Y))\nprint('F1_score %s' % metrics.f1_score(test_Y, pred_Y))\nprint('auc %s' % metrics.roc_auc_score(test_Y, pred_Y))","16010bc3":"#### Data preprocessing\n- Delete rows with missing data\n- Mapping non-numeric data to numeric data","ae756ef4":"#### Build and train the model\n- using a verity of traditional models\n- metrics: accuracy, F1_score, auc","70a66d30":"#### Compare different models\n|model|accuracy|F1_score|auc|\n|---|---\n|LR|0.82|0.88|0.71|\n|KNN|0.84|0.90|0.78|\n|Decision Tree|0.85|0.90|0.78|\n|NB|0.79|0.87|0.64|\n|Random Forest|0.85|0.91|0.77|\n|GB|0.86|0.91|0.79|\n|SVM|0.81|0.88|0.67|\n|Neural Networks|0.81|0.88|0.67|\n\nGBC get the best scores in all these three metrics."}}