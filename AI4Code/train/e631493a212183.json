{"cell_type":{"4a3b32c7":"code","e9a115a1":"code","6752ffdc":"code","3c7e9125":"code","ad51ef19":"code","2e9907cb":"code","e8b9a95e":"code","c07ee099":"code","dcb29f96":"code","8284002d":"code","b8aa817c":"code","2fd8135a":"code","e01caa7a":"code","e8cf9106":"code","526f2d37":"code","fc9ccedd":"code","afbbc863":"code","83f8f26a":"code","cb8751ad":"code","f536cbd3":"code","f7e76eb4":"code","70f369f6":"code","0c3c9c57":"code","a5a5b5f2":"code","0d678d3b":"code","bac65b1b":"code","9c847150":"code","c1581a3a":"code","65218745":"code","b9f7e0df":"code","f2883964":"code","43b085d5":"code","23f08381":"code","a405c281":"code","e91d555b":"code","c1d93714":"code","54c35b38":"code","275cb89e":"code","ab833057":"code","aad95b38":"code","c0a2f0c4":"code","eab4390d":"code","944a8289":"code","f8fa5588":"code","411a2677":"code","ee7d046a":"code","ccd83b65":"code","e6e9da3c":"code","0ed1c9b6":"code","089cb8a3":"code","bcee80c7":"code","03d4f3e8":"code","f19e9110":"code","e406a93d":"code","7e5b5e42":"code","0f102ccc":"code","dd02504f":"code","078c5f83":"code","27e60e85":"code","a9c4e75e":"code","9c38a8ec":"code","64f05724":"code","786b6baf":"code","5e5ba34c":"code","8e310cfb":"code","e3d90729":"code","219d2eac":"code","7e026c0c":"code","6cb20cc6":"code","87ec1430":"code","1e2d0b35":"code","abfab121":"code","86fd7303":"code","2db6f66a":"code","7e93f9b3":"code","a9685f19":"code","d6f1db85":"code","6b18729e":"code","0acb1a03":"code","b89809a2":"code","b7170990":"code","3c3ab75a":"code","38a953ce":"code","b866c9ac":"code","be366140":"code","d9f900e7":"code","22c847d9":"code","67802d75":"code","17e4beb1":"code","c79d5cd3":"code","02b8f831":"code","ba52e709":"code","e7c3ca01":"code","c427513b":"code","85252088":"code","29c55670":"code","d927bbb3":"code","72105c0e":"code","3ee99b31":"code","28cbac62":"code","e8754d75":"code","b9bce68a":"code","6d9124f8":"code","518cb096":"code","61c98ad3":"code","ec523576":"code","a855d20c":"code","e17f6449":"code","b2db0769":"code","2a8b6f6d":"code","db5ced1a":"code","c214e809":"code","fb67052d":"code","3359f547":"markdown","dabc6214":"markdown","dc778443":"markdown","a46e1341":"markdown","8b3d4059":"markdown","b784fc8e":"markdown","88565cf1":"markdown","c90bbae2":"markdown","587929cb":"markdown","40f19a38":"markdown","fd8a984b":"markdown","c8def354":"markdown","6fccddd8":"markdown","2fb830cf":"markdown","b804174c":"markdown","9b3cb8f9":"markdown","3fe6beb3":"markdown","72448502":"markdown","15151b2f":"markdown","8192d623":"markdown","44a6cb96":"markdown","ca0cacbc":"markdown","b03156d2":"markdown","1c0899eb":"markdown","f943bf80":"markdown","2215f3b0":"markdown","c7cca63d":"markdown","64dc9185":"markdown","3d84c019":"markdown","873094ce":"markdown","4d55f530":"markdown","26b727ad":"markdown","4f07f7e1":"markdown","fcfb9b49":"markdown","425c8b99":"markdown","f9c37bfa":"markdown","34786146":"markdown","ce8cd581":"markdown","fbf21b67":"markdown"},"source":{"4a3b32c7":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9a115a1":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import VarianceThreshold\n","6752ffdc":"# Loading the data\ndf = pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/train.csv')\ndf.shape","3c7e9125":"# Check the presence of null data\n[col for col in df.columns if df[col].isnull().sum() > 0] ","ad51ef19":"# Split the data into train and test data\nx_train, x_test, y_train, y_test = train_test_split(\n    df.drop(labels = ['TARGET'], axis = 1),\n    df['TARGET'],\n    test_size = 0.3,\n    random_state = 0\n)","2e9907cb":"constant_features = [\n    features for features in x_train.columns if x_train[features].std() == 0\n]\nlen(constant_features)","e8b9a95e":"x_train.drop(labels = constant_features, axis = 1, inplace=True)\nx_test.drop(labels = constant_features, axis = 1, inplace = True)","c07ee099":"x_train, x_test, y_train, y_test = train_test_split(\n    df.drop(labels = ['TARGET'], axis = 1),\n    df['TARGET'],\n    test_size = 0.3,\n    random_state = 0\n)","dcb29f96":"# Create a empty list\nquasi_constant_feat = []\n\n# Loop for searching all the columns in the data\nfor feature in x_train.columns:\n    \n    # find the predominant value\n    predominant = (x_train[feature].value_counts() \/ np.float(\n        len(x_train))).sort_values(ascending=False).values[0]\n    \n    # evaluate predominant feature\n    if predominant > 0.999:\n        quasi_constant_feat.append(feature)\n\nlen(quasi_constant_feat)","8284002d":"x_train.drop(labels = quasi_constant_feat, axis = 1, inplace=True)\nx_test.drop(labels = quasi_constant_feat, axis = 1, inplace = True)\nprint(x_train.shape, x_test.shape)","b8aa817c":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop(labels=['TARGET'], axis=1),\n    df['TARGET'],\n    test_size=0.3,\n    random_state=0)\nprint(X_train.shape, X_test.shape)","2fd8135a":"# Create a empty list for duplicated features\nduplicated_feat = []\nfor i in range(0, len(X_train.columns)):\n\n    col_1 = X_train.columns[i]\n\n    for col_2 in X_train.columns[i + 1:]:\n\n        # if the features are duplicated\n        if X_train[col_1].equals(X_train[col_2]):\n\n            # and then append the duplicated one to a list\n            duplicated_feat.append(col_2)","e01caa7a":"duplicated_features = set(duplicated_feat)\nprint(len(duplicated_features))","e8cf9106":"X_train.drop(labels = duplicated_features, axis = 1, inplace=True)\nX_test.drop(labels = duplicated_features, axis = 1, inplace = True)\nprint(X_train.shape, X_test.shape)","526f2d37":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip')\ndata.shape","fc9ccedd":"data.head()","afbbc863":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","83f8f26a":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","cb8751ad":"corrmat = X_train.corr()\nfig, ax = plt.subplots()\nfig.set_size_inches(16,16)\nsns.heatmap(corrmat)","f536cbd3":"grouped_feature_ls = []\ncorrelated_groups = []\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","f7e76eb4":"corr_features = correlation(X_train, 0.9) # filter for all the features with correlation more than 0.9\ncorrelated_features = set(corr_features) # Set statement is used to identify the unique feature in the list\nprint(len(correlated_features)) # length of set of correlated features","70f369f6":"# Dropping all the correlated features from the data\nX_train.drop(labels=correlated_features, axis=1, inplace=True)\nX_test.drop(labels=correlated_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","0c3c9c57":"from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile","a5a5b5f2":"df = pd.read_csv('\/kaggle\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip')\ndf.shape","0d678d3b":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(df.select_dtypes(include=numerics).columns)\ndf = df[numerical_vars]\ndf.shape","bac65b1b":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop(labels=['target', 'ID'], axis=1),\n    df['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","9c847150":"mi = mutual_info_classif(X_train.fillna(0), y_train)\nmi","c1581a3a":"# let's add the variable names and order the features\n# according to the MI for clearer visualisation\nmi = pd.Series(mi)\nmi.index = X_train.columns\nmi.sort_values(ascending=False)","65218745":"# and now let's plot the ordered MI values per feature\nmi.sort_values(ascending=False).plot.bar(figsize=(20, 8))","b9f7e0df":"# here I will select the top 10 features\n# which are shown below\nsel_ = SelectKBest(mutual_info_classif, k=10).fit(X_train.fillna(0), y_train)\nX_train.columns[sel_.get_support()]","f2883964":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.shape","43b085d5":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","23f08381":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['SalePrice'], axis=1),\n    data['SalePrice'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","a405c281":"# determine the mutual information\nmi = mutual_info_regression(X_train.fillna(0), y_train)\nmi = pd.Series(mi)\nmi.index = X_train.columns\nmi.sort_values(ascending=False)","e91d555b":"mi.sort_values(ascending=False).plot.bar(figsize=(20,8))","c1d93714":"# here I will select the top 10 percentile\nsel_ = SelectPercentile(mutual_info_regression, percentile=10).fit(X_train.fillna(0), y_train)\nX_train.columns[sel_.get_support()]","54c35b38":"from sklearn.feature_selection import chi2","275cb89e":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata.shape","ab833057":"# the categorical variables in the titanic are PClass, Sex and Embarked\n# first I will encode the labels of the categories into numbers\n\n# for Sex \/ Gender\ndata['Sex'] = np.where(data.Sex == 'male', 1, 0)\n\n# for Embarked\nordinal_label = {k: i for i, k in enumerate(data['Embarked'].unique(), 0)}\ndata['Embarked'] = data['Embarked'].map(ordinal_label)\n\n# PClass is already ordinal","aad95b38":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data[['Pclass', 'Sex', 'Embarked']],\n    data['Survived'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","c0a2f0c4":"f_score = chi2(X_train.fillna(0), y_train)\nf_score","eab4390d":"pvalues = pd.Series(f_score[1])\npvalues.index = X_train.columns\npvalues.sort_values(ascending = True)","944a8289":"from sklearn.feature_selection import f_classif, f_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile","f8fa5588":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip')\ndata.shape","411a2677":"data.head()","ee7d046a":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","ccd83b65":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","e6e9da3c":"univariate = f_classif(X_train.fillna(0), y_train)\nunivariate","0ed1c9b6":"# let's add the variable names and order it for clearer visualisation\nunivariate = pd.Series(univariate[1])\nunivariate.index = X_train.columns\nunivariate.sort_values(ascending=False, inplace=True)","089cb8a3":"# and now let's plot the p values\nunivariate.sort_values(ascending=False).plot.bar(figsize=(20, 8))","bcee80c7":"# here I will select the top 10 features\nsel_ = SelectKBest(f_classif, k=10).fit(X_train.fillna(0), y_train)\nX_train.columns[sel_.get_support()]","03d4f3e8":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.shape","f19e9110":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","e406a93d":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['SalePrice'], axis=1),\n    data['SalePrice'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","7e5b5e42":"univariate = f_regression(X_train.fillna(0), y_train) # fill all null values with 0 for now\nunivariate = pd.Series(univariate[1])\nunivariate.index = X_train.columns\nunivariate.sort_values(ascending=False, inplace=True)","0f102ccc":"univariate.sort_values(ascending=False).plot.bar(figsize=(20,8))","dd02504f":"# here I will select the top 10 percentile\nsel_ = SelectPercentile(f_regression, percentile=10).fit(X_train.fillna(0), y_train)\nX_train.columns[sel_.get_support()]","078c5f83":"X_train = sel_.transform(X_train.fillna(0))\nX_train.shape","27e60e85":"from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.metrics import roc_auc_score, mean_squared_error","a9c4e75e":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip')\ndata.shape","9c38a8ec":"data.head()","64f05724":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","786b6baf":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","5e5ba34c":"# build decision tree for each feature \nroc_values = []\nfor feature in X_train.columns:\n    clf = DecisionTreeClassifier()\n    clf.fit(X_train[feature].fillna(0).to_frame(), y_train)\n    y_scored = clf.predict_proba(X_test[feature].fillna(0).to_frame())\n    roc_values.append(roc_auc_score(y_test, y_scored[:, 1]))","8e310cfb":"roc_values = pd.Series(roc_values)\nroc_values.index = X_train.columns\nroc_values.sort_values(ascending=False)","e3d90729":"# and now let's plot\nroc_values.sort_values(ascending=False).plot.bar(figsize=(20, 8))","219d2eac":"# a roc auc value of 0.5 indicates random decision\n# let's check how many features show a roc-auc value\n# higher than random\nlen(roc_values[roc_values > 0.5])","7e026c0c":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.shape","6cb20cc6":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","87ec1430":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['SalePrice'], axis=1),\n    data['SalePrice'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","1e2d0b35":"# loop to build a tree, make predictions and get the mse\n# for each feature of the train set\nmse_values = []\nfor feature in X_train.columns:\n    clf = DecisionTreeRegressor()\n    clf.fit(X_train[feature].fillna(0).to_frame(), y_train)\n    y_scored = clf.predict(X_test[feature].fillna(0).to_frame())\n    mse_values.append(mean_squared_error(y_test, y_scored))","abfab121":"# let's add the variable names and order it for clearer visualisation\nmse_values = pd.Series(mse_values)\nmse_values.index = X_train.columns\nmse_values.sort_values(ascending=False)","86fd7303":"mse_values.sort_values(ascending=False).plot.bar(figsize=(20,8))","2db6f66a":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS","7e93f9b3":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip', nrows = 30000)\ndata.shape","a9685f19":"data.head()","d6f1db85":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","6b18729e":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","0acb1a03":"# find and remove correlated features\n# in order to reduce the feature space a bit\n# so that the algorithm takes shorter\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(X_train, 0.8)\nprint('correlated features: ', len(set(corr_features)) )","b89809a2":"# removed correlated  features\nX_train.drop(labels=corr_features, axis=1, inplace=True)\nX_test.drop(labels=corr_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","b7170990":"sfs1 = SFS(RandomForestClassifier(n_jobs=4), \n           k_features=5, \n           forward=True, \n           floating=False, \n           verbose=2,\n           scoring='roc_auc',\n           cv=3)\n\nsfs1 = sfs1.fit(np.array(X_train.fillna(0)), y_train)","3c3ab75a":"selected_feat= X_train.columns[list(sfs1.k_feature_idx_)]\nselected_feat","38a953ce":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.shape","b866c9ac":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","be366140":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['SalePrice'], axis=1),\n    data['SalePrice'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","d9f900e7":"# find and remove correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(X_train, 0.8)\nprint('correlated features: ', len(set(corr_features)) )","22c847d9":"# removed correlated  features\nX_train.drop(labels=corr_features, axis=1, inplace=True)\nX_test.drop(labels=corr_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","67802d75":"X_train.fillna(0, inplace=True)","17e4beb1":"# step forward feature selection\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(RandomForestRegressor(), \n           k_features=5, \n           forward=True, \n           floating=False, \n           verbose=2,\n           scoring='r2',\n           cv=3)\n\nsfs1 = sfs1.fit(np.array(X_train), y_train)","c79d5cd3":"X_train.columns[list(sfs1.k_feature_idx_)]","02b8f831":"from sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel","ba52e709":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip')\ndata.shape","e7c3ca01":"data.head()","c427513b":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","85252088":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","29c55670":"# linear models benefit from feature scaling\n\nscaler = StandardScaler()\nscaler.fit(X_train.fillna(0))","d927bbb3":"# l1 penalty is used for LASSO fitting\nsel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver='liblinear'))\nsel_.fit(scaler.transform(X_train.fillna(0)), y_train)","72105c0e":"# this command let's me visualise those features that were kept\nsel_.get_support()","3ee99b31":"# Now I make a list with the selected features\nselected_feat = X_train.columns[(sel_.get_support())]\n\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n    np.sum(sel_.estimator_.coef_ == 0)))","28cbac62":"# the number of features which coefficient was shrank to zero:\nnp.sum(sel_.estimator_.coef_ == 0)","e8754d75":"# we can identify the removed features like this:\nremoved_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\nremoved_feats","b9bce68a":"# we can then remove the features from the training and testing set\n# like this\nX_train_selected = sel_.transform(X_train.fillna(0))\nX_test_selected = sel_.transform(X_test.fillna(0))\nprint(\"Before Lasso Regularization :\", X_train.shape, X_test.shape)\nprint(\"After Lasso Regularization :\", X_train_selected.shape, X_test_selected.shape)","6d9124f8":"from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import roc_auc_score","518cb096":"# load dataset\ndata = pd.read_csv('\/kaggle\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip', nrows=50000)\ndata.shape","61c98ad3":"data.head()","ec523576":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","a855d20c":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","e17f6449":"sel_ = SelectFromModel(RandomForestClassifier(n_estimators=100))\nsel_.fit(X_train.fillna(0), y_train)","b2db0769":"# this command let's me visualise those features that were selected.\nsel_.get_support()","2a8b6f6d":"# let's add the variable names and order it for clearer visualisation\nselected_feat = X_train.columns[(sel_.get_support())]\nlen(selected_feat)","db5ced1a":"# let's display the list of features\nselected_feat","c214e809":"pd.Series(sel_.estimator_.feature_importances_.ravel()).hist()","fb67052d":"print('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients greater than the mean coefficient: {}'.format(\n    np.sum(sel_.estimator_.feature_importances_ > sel_.estimator_.feature_importances_.mean())))","3359f547":"So there are 38 constant feature columns with same value in all the data out of 370 columns. So in a single step we are able to identify 38 features with zero variance which does not provide any predictive information of output variable. So we can straight away delete these columns. In the below statement we delete all the 38 constant feature columns.  ","dabc6214":"<a id=\"correlation\"><\/a>\n## 4. Correlation\n\nCorrelation is a statistical technique that can show whether and how strongly pairs of variables are **Linearly** related. There are several different correlation techniques. Some of the most important correlation is **Pearson Correlation**. **Pearson correlation** technique works best with **linear** relationships: as one variable gets larger, the other gets larger (or smaller) in direct proportion. It does not work well with curvilinear relationships (in which the relationship does not follow a straight line).\n\nCorrelation Coefficient value varies from -1 to +1, where:\n1. 1 is a perfect positive correlation\n2. 0 is no correlation\n3. -1 is a perfect negative correlation\n\n* Correlation is Positive when the values increase together\n* Correlation is Negative when one value decreases as the other increases\n\nBelow is the example of various relationships between two variables. As we observe when two variables are perfectly linearly correlated then it's value is +1 or -1. Also we see below that correlation coefficent works only for Linear relationships. For non-linear relationships its value is 0. \n\n![1920px-Correlation_examples2.svg.png](attachment:1920px-Correlation_examples2.svg.png)\n\nCorrelation works for quantifiable data in which numbers are meaningful, usually quantities of some sort. It cannot be used for purely categorical data, such as gender, brands purchased, or favorite color.\n\nSo in this tutorial we will consider only the numerical features of data. Otherwise, this step has to be done post feature engineering once all the categorical features are encoded.","dc778443":"110 of 114 features have predictive power higher than 0.5.  ","a46e1341":"<a id=\"const\"><\/a>\n## 1. Constant Features\n\nIn this method we remove the feature columns which have same value in all the data i.e., where variance is zero. Because the feature columns with zero variance in the data will have zero predictive power of output variable.\n\nBelow we will study how to identify the feature columns with same value in all the data. Once identified we remove those columns from the data as a preprocessing step before modelling.","8b3d4059":"<a id=\"conclusion\"><\/a>\n## Conclusion\n\nThis notebook will not be possible if not because of - [Soledad Galli](https:\/\/www.udemy.com\/course\/feature-selection-for-machine-learning\/)'s amazing Udemy course. All the credits goes to her for developing such an amazing course.\n\nThanks for reading the whole notebook. Will add more techqnies on **Hybrid Methods** and **Advanced methods** **(Work in Progress)**.","b784fc8e":"<a id=\"embeded\"><\/a>\n## C. Embeded Methods\n\nEmbeded methods are all group of techniques which perform feature selection as part of the model construction process. The example of this approach is the **LASSO** method for constructing a linear model, which penalizes the regression coefficients with an L1 penalty, shrinking many of them to zero. Any features which have non-zero regression coefficients are 'selected' by the LASSO algorithm.\n\nIn this section  we will study below methods under embeded methods - \n1. Lasso Method\n2. Tree Method\n\nSimilar approach can be applied many other machine learning algorithms to identify the best features in the dataset.\n\n**Regularization Methods** -There are 3 types of Regularization methods . In this tutorial we will study **LASSO Method**\n1. The L1 Regularization (also called as LASSO)\n2. The L2 Regularization (also called as Ridge)\n3. The L1\/L2 Regularization (also called Elastic Net)\n\n**Lasso Method -** is a method of adding penalty on the different parameters of the model to reduce the freedom of the model. Hence, the model is less likely to overfit to the noise of training data and will improve the generalization abilities of the model for linear models. Though originally defined for linear regression, lasso regularization is easily extended to a wide variety of statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators, in a straightforward fashion.\n\nSo we can fit Linear and Logistic regression with Lasso regularization, helps us to eliminate the features with coeffiecents zero.","88565cf1":"<a id=\"uni2\"><\/a>\n## 8. Univariate roc-auc or mse\n\nThis procedure works as follows:\n\n- First, it builds one decision tree per feature, to predict the target\n- Second, it makes predictions using the decision tree and the mentioned feature\n- Third, it ranks the features according to the machine learning metric (roc-auc or mse)\n- It selects the highest ranked features\n\n**Note :** We can use other machine learning algorithms too other than decision tree in the above procedure.","c90bbae2":"### 9.2 Forward feature selection for Regression","587929cb":"<a id=\"mi\"><\/a>\n## 5. Mutual Information(MI)\n\nThe mutual information of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the \"amount of information\" obtained about one random variable through observing the other random variable. \n\nFor example, if X and Y are independent, then knowing X does not give any information about Y and vice versa, so their mutual information is zero. At the other extreme, if X is a deterministic function of Y and Y is a deterministic function of X then all information conveyed by X is shared with Y: knowing X determines the value of Y and vice versa. So in summary - \n1. MI Measures the mutual depency of two variables\n2. MI determines how similar the p(X,Y) is to product of p(X) and p(Y). Basically measures how dependent both the variables are.\n3. If X and Y are independent, then MI is zero\n\nIdeally feature selection is done post feature engineering once one-hot encoding of categorical variable is done. So for this purpose we include the numerical features of paribas dataset. In this tutorial we will study how to calculate the Mutual Information (MI) for both **[Classification](http:\/\/)** and **[Regression](http:\/\/)** problems.","40f19a38":"<a id=\"intro\"><\/a>\n## Feature Selecion Methods\nIn machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:\n* simplification of models to make them easier to interpret by researchers\/users,\n* shorter training times,\n* to avoid the curse of dimensionality,\n* enhanced generalization by reducing overfitting (formally, reduction of variance)\n\nFeature selection Methods can be divided into 3 main categories.\n1. Filter Methods\n2. Wrapper Methods\n3. Embeded Methods\n\n![feature%20selection%20methods.jpg](attachment:feature%20selection%20methods.jpg)\n\nIn addition to above techniques, there are few more techniques developed like **Hyrid Methods** and **Advanced Methods**. In this tutorial we will learn breifly about various techniques using all the different methods mentioned. In this tutorial, we have used various datasets from different kaggle competitions like - parbas data, house price data, santander data, titanic.","fd8a984b":"We cannot use this method always as it takes Linear Assumption between feature and output variable. But we can use Log feature and test for the Linear relationship with output variable.","c8def354":"In Regression the smaller the mse, the better the model performance is. So in this case, we need to select from the right to the left.","6fccddd8":"### 5.2 MI for Regression\n\nBelow is code for identifying the mutual infromation for a regression problem between each feature and the output variable. For this purpose we used Housing pricing dataset. ","2fb830cf":"As we observe there are 141 features out of 370 total feature columns with more than 99.9% same value in all the data. So this simple test tells us that around 40% of the feature columns are not needed in building the machine learning model. So we can straight away delete all the 146 columns of data. So in the below code we straight away delete all these 146 columns from train and test data.","b804174c":"So here we identify 38 which are highly correlated with other variables (>0.9). So including these features will not add any additional information to the machine learning modelling. Ideally we should exclude all these features. The threshold can be changed from 0.9 to any number based on the business scenario. Below we will exclude all the correlated features. ","9b3cb8f9":"<a id=\"filter\"><\/a>\n## A. Filter Methods - Basic Methods & Statistical Measures\n\n**Filter Methods -** In this method we use different statistical measures to identify the correlation or predictive power of a single feature with output variable. The tests are done in a univariate manner i.e., in the absence of other features. So the filter methods does not consider the inter-dependency or interactions within the features. There several scoring methods we use for feature selection. Some of the methods are mentioned below. \n\nThere are various procedures for feature selecton under Filter Methods. Below are some of the basic methods and statistical procedures to identify good predictive features. We will study each one of them one-by-one in this tutorial.\n\n**Basic Methods**\n\n1. Constant Features\n2. Quasi-constant features\n3. Duplicate features\n\n**Statistical Methods**\n1. Correlation\n2. Mutual Information\n3. Chi-Square | Fischer Square\n4. Univariate feature selection\n5. Univariate roc-auc | mse\n       \nThere are several advantages of using filter methods for feature selection. Filter methods can be used as first hand tool to understand predictive power of features before going into Embeded \/ Hybrid \/ advanced tools. Mainly this is first step of feature selection pipeline.\n\nBelow are  different Filter methods based on the type of data available. We will discuss further some of the methods in this tutorial.\n\n![Filter%20Methods.png](attachment:Filter%20Methods.png)\n\n**Advantages**\n* They rely on the characteristics of the data\n* They do not use any machine Learning Algorithm\n* Model agnostic - They do not depend on the machine learning model we use\n* Tends to be less computationally expensive\n* Lower performance compared to Wrapper Methods\n* Relevant for quick screening and removal of irrelevant features\n\nA typical algorithm consists of two steps.\n1. Rank features according to the statistical method or criteria. Each feature is ranked independent of other features.\n2. Select the highest ranking features.","3fe6beb3":"Here again, there seem to be a bunch of features towards the left with pvalues above 0.05, which are candidates to be removed, as this means that the features do not statistically significantly discriminate the target.","72448502":"<a id=\"quasi\"><\/a>\n## 2. Quasi - Constant Features\n\nThese are the feature columns where most of the data has the same value. So in this section we consider the threshold as 99% ie., a column is quasi-constant feature column if more than 99% of the data has same value. We exclude all the quasi constant features as part of the pre-processing step. This is an important step when we have data with thousands of features and in these cases there is a high chance of having lot of constant features and quasi-constant features. It is ideal to test and remove these as they will have almost zero predictability of output variabe (Y).\n\nBelow we will study how to identify the quasi-constant features and once identified delete those columns from original data. ","15151b2f":"### 7.1 ANOVA for Classification","8192d623":"We used **Set** statement here because a set will always contain the unique values. As we observe here there are 65 duplicate columns in the data. We can drop all these columns as our pre-processing step. Below is the code to drop all the duplicated columns contained in **duplicated_features**.","44a6cb96":"<a id=\"wrapper\"><\/a>\n## B. Wrapper Methods\n\nThere are different procedures to select features under Wrapper Methods. They are - \n1. Forward Selection - Add one feature at a time recursively\n2. Backward Selection - Removes one feature at a time recursively\n3. Exhaustive Search - searches across all possible feature combinations\n\n**Procedure**\n \n1. Search for the subset of features\n2. Build the Machine Learning Model on the selected feature Subset\n3. Evaluate Model Performance\n4. Repeat\n\n![Feature_selection_Wrapper_Method.png](attachment:Feature_selection_Wrapper_Method.png)\n\n**Step Forward Feature Selection**\n\nForward selection, which involves starting with no variables in the model, testing the addition of each variable using a chosen model fit criterion, adding the variable (if any) whose inclusion gives the most statistically significant improvement of the fit, and repeating this process until none improves the model to a statistically significant extent.\n\n**Step Backward Feature Selection**\n\nBackward elimination, which involves starting with all candidate variables, testing the deletion of each variable using a chosen model fit criterion, deleting the variable (if any) whose loss gives the most statistically insignificant deterioration of the model fit, and repeating this process until no further variables can be deleted without a statistically insignificant loss of fit.\n\n**Exhaustive Search**\n\nThis method searches across all possible feature combinations. Its aim is to find the best performing feature subset\u2014we can say it\u2019s a brute-force evaluation of feature subsets. It creates all the subsets of features from 1 to N, with N being the total number of features, and for each subset, it builds a machine learning algorithm and selects the subset with the best performance.The parameters that you can play with here are the 1 and N, which can be described as the minimum number of features and the maximum number of features. \n\n**Important :**\n* The Optimal features depends on the Machine Learning Algorithm used\n* They are able to detect the interactions between the variables\n* Find the optimal feature subset for the desired classifier\n* Very computationally expensive\n* Usually provide the best performing feature subset for a given machine learning algorithm\n* This selection procedure is called greedy, because it evaluates all possible feature combinations.\n* In general this is a unfeasable solution because of its computational cost\n\nThere is a special package for python that implements this type of feature selection: mlxtend.\n\nIn the mlxtend implementation of the step forward feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features. \n\nAs this is a exhaustive algorithm and computationally expensive, we will look into only forward feature selection procedure with maximum iterations of 5. Similar code can be applied to backward selection and exhaustive search.","ca0cacbc":"### 5.1 MI for Classification\n\nBelow is code for identifying the mutual infromation for a classification problem between each feature and the output variable. ","b03156d2":"### 7.2 ANOVA for Regression","1c0899eb":"### 8.1 Univariate roc-auc for Classification","f943bf80":"<div align='center'><font size=\"6\" color=\"#075b8c\"> A beginner's guide to Feature Selection Methods <\/font><\/div>","2215f3b0":"### 8.2 Univariate roc-auc for Regression","c7cca63d":"<a id=\"tree\"><\/a>\n## 11. Tree Method (random forest)\n\nIn this procdure we use random forest method to understand feature importance. Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. Random Forests and decision trees in general give preference to features with high cardinality\n\nBelow we will see how to use random forest classifier to get feature importance using **SelectFromModel**.\n","64dc9185":"<a id=\"ffs\"><\/a>\n## 9. Forward featrue selection ","3d84c019":"<a id=\"uni\"><\/a>\n## 7. Univariate Feature Selection\n\nUnivariate feature selection works by selecting the best features based on univariate statistical tests (**ANOVA**). The methods based on F-test estimate the degree of **linear dependency** between two random variables. They assume a **linear relationship** between the feature and the target. These methods also assume that the variables follow a **Gaussian distribution**.\n\nHere we will assume that the features have linear relationship with target variable, and that they are normally distributed. But when or if you choose to implement these selection procedure for your features, you will have to check that this is the case, to make sure you are implementing the right method.\n\nIdeally feature selection is done post feature engineering once one-hot encoding of categorical variable is done. So for this purpose we include the numerical features of paribas dataset. In this tutorial we will study how to calculate the Mutual Information (MI) for both **[Classification](http:\/\/)** and **[Regression](http:\/\/)** problems.","873094ce":"The smaller the p_value, the more significant the feature is to predict the target, in this case Survival in the titanic. Then, from the above data, Sex is the most important feature, then PClass then Embarked. \n","4d55f530":"<a id=\"chi2\"><\/a>\n## 6. Fischer Score | chi square\n\nIt compares the observed distribution of the different classes of target Y among the different categories of the feature, against the expected distribution of the target classes, regardless of the feature categories. \n\n1. Measures the dependency of 2 variables\n2. Suited for **Categorical Variables**\n3. Target should be **binary**\n4. Variable values should be non-negative, and typically boolean, frequencies or count\n5. it compares the observed distribution class with the different labels against the expected one, would there be no labels ","26b727ad":"Above is the correlation heatmap of all the numerical featuers in the paribas dataset. Now out of this correlation map, let's identify the features with higher correlation and exclude those features. In the exercise we will set a threshold of 0.9 and exclude all the features with correlation more than 0.9. ","4f07f7e1":"Compared to total number of features, we can see that there are very few features that contribute to most of the prediction of target. There are few features with almost zero mutual information (MI). So in general we select the top 10 or top 20 features from the above graph and build the model. For selecting the top 10 or top 20 we use **SelectKBest** method from sklearn.","fcfb9b49":"In this tutorial we will cover different feature selection methods. I will keep adding more methods in the future.\n\n- [Overview of Feature Selection](#intro)\n- [Filter Methods](#filter)\n    1. [constant features](#const)\n    2. [Qausi-constant features](#quasi)\n    3. [Duplicate features](#duplicate)\n    4. [Correlation](#correlation)\n    5. [Mutual Information](#mi)\n    6. [Fisher score | chi-square](#chi2)\n    7. [Univariate feature selection](#uni)\n    8. [Univariate roc-auc or mse](#uni2)\n- [Wrapper Methods](#wrapper)\n    9. [Forward Feature Selection](#ffs)\n- [Embeded Method](#embeded)\n   10. [Lasso Method](#lasso)\n   11. [Tree Method](#tree)\n- [Conclusion](#conclusion)","425c8b99":"### 9.1 Forward feature selection for Classification","f9c37bfa":"Lower the p-value, the most predictive is the feature. There are a few features that do not seem to have predictive power according to this test, which are those on the left side of plot with pvalues above 0.05. These features with pvalue > 0.05 are indeed not important. However, the test assumes a **linear relationship**, so it might also be the case that the feature is related to the target but not in a non-linear manner.\n\nSo futher analysis is needed if we want to know the true nature of the relationship between feature and target.\n\nWe use **SelectKBest** method to get top 10 or top 20 features or top 10 percentile features from the data. ","34786146":"<a id=\"lasso\"><\/a>\n## 10. Lasso Method","ce8cd581":"<a id=\"duplicate\"><\/a>\n## 3. Duplicated Features\n\nSometimes there is a chance that duplicate features exist in the data. Usually this can happen when we are one-hot encoding the data. So it is always preferable to delete the duplicate features in the data. This may not happen always but it is ideal to test this and if any duplicate features exist then drop them. Below is the code to identify the deuplicate features. As we see below that there are duplicate features in the data.","fbf21b67":"So here we can increase the number of trees in the random forest classifier to get better results. Among all the methods this will be one of the best methods to use to get the feature importance."}}