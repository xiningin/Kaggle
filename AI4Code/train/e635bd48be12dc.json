{"cell_type":{"c7cb856c":"code","a2d893f6":"code","8db0c176":"code","888db5ac":"code","e72a560c":"code","d65c6a1d":"code","afa6bde8":"code","677a1e42":"code","ba89ff8f":"code","70c1d5e0":"code","01f5dd49":"code","00da1d28":"code","c1872fbc":"code","951eddd7":"code","b9d023a5":"code","85548d6d":"code","89ab29ca":"code","d5ce048d":"code","32f6e711":"code","35605b66":"code","8b7984f3":"code","34c0f4b0":"code","009310a9":"code","0cef4d0b":"code","e58f9c6d":"code","a57015b5":"code","2a7cbd5f":"code","200e26c8":"code","063fe58d":"code","7e000b84":"code","32e27162":"code","57e13721":"code","a897db0d":"code","683687a7":"code","08fecfc7":"code","61b84d0b":"code","c7b21934":"code","33e51ff4":"code","c1fa9d3b":"code","a0bfeafb":"code","911bed70":"code","99eb5147":"code","8d1b16d6":"markdown","41681ab3":"markdown","fea94b20":"markdown","1560bfdd":"markdown","9d709180":"markdown","19d77762":"markdown","3f09da74":"markdown","8e81f3e8":"markdown","04aa0ede":"markdown","e7c10a8f":"markdown","701a48cb":"markdown","7fb87f2e":"markdown","ce336b35":"markdown","e4fad3c3":"markdown","ec92c83f":"markdown","978eb142":"markdown"},"source":{"c7cb856c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2d893f6":"## Importing the required Libraries\n\nimport numpy as np\nimport pandas as pd;\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8db0c176":"train=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_=len(train)\nprint(train_)","888db5ac":"titanic=pd.concat([train,test],ignore_index='True')\ntitanic.head()","e72a560c":"titanic.info()","d65c6a1d":"titanic.isnull().sum()","afa6bde8":"sns.barplot(data=titanic,x='Pclass',y='Survived',hue='Sex')","677a1e42":"titanic['Sex']=titanic['Sex'].apply({'male':0,'female':1}.get)","ba89ff8f":"sns.pairplot(titanic,\n           x_vars=['Pclass','Sex','Age','Fare','Survived'],\n           y_vars=['Pclass','Sex','Age','Fare','Survived'],\n           diag_kind='kde')","70c1d5e0":"sns.heatmap(titanic.isnull(),cbar=False).set_title(\"Missing Values Heatmap\")","01f5dd49":"titanic.nunique()","00da1d28":"titanic.describe()","c1872fbc":"titanic.groupby(['Cabin','Survived'])['Pclass'].sum()","951eddd7":"titanic['Cabin'].unique()","b9d023a5":"titanic['Cabin']=titanic['Cabin'].replace(np.nan,'N')\ntitanic","85548d6d":"titanic['Deck']=titanic['Cabin'].str[:1]\ntitanic","89ab29ca":"sns.displot(titanic,x='Deck',hue='Survived',multiple='dodge')","d5ce048d":"titanic['Deck'].value_counts().sort_values(ascending=False)","32f6e711":"titanic.groupby(['Deck','Pclass']).count()","35605b66":"titanic.groupby(['Pclass','Sex','Embarked'])['Age'].mean()","8b7984f3":"sns.catplot(data=titanic,x='Embarked',kind='count')","34c0f4b0":"titanic[titanic['Embarked'].isnull()]","009310a9":"titanic['Embarked'].fillna('S',inplace=True)\ntitanic.tail()","0cef4d0b":"titanic[titanic['Fare'].isnull()]","e58f9c6d":"k=titanic.groupby(['Pclass'])['Fare'].mean()\ntitanic['Fare']=titanic['Fare'].replace(np.nan,k[3])","a57015b5":"titanic.groupby(['Embarked','Sex','Pclass'])['Age'].mean()","2a7cbd5f":"titanic['Age']=titanic['Age'].fillna(titanic.groupby(['Embarked','Sex','Pclass'])['Age'].transform('mean'))","200e26c8":"titanic.info()","063fe58d":"titanic['Name_pre']=titanic[\"Name\"].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n","7e000b84":"print(titanic['Name_pre'].value_counts())","32e27162":"titanic['Name_pre'] = titanic['Name_pre'].apply({\n    'Mr': 'Mr',\n    'Miss': 'Miss',\n    'Mrs': 'Mrs',\n    'Master': 'Master',\n    'Dr': 'Dr',\n    'Rev': 'Rev',\n    'Don': 'Royalty',\n    'Col': 'Officer',\n    'Jonkheer': 'Mr',\n    'Major': 'Officer',\n    'Ms': 'Miss',\n    'Mlle': 'Miss',\n    'the Countess': 'Royalty',\n    'Capt': 'Officer',\n    'Dona': 'Royalty',\n    'Lady': 'Royalty',\n    'Sir': 'Mr',\n    #'Mme': 'Mrs',\n}.get)\ntitanic['Name_pre'].value_counts()","57e13721":"titanic['Family']=titanic['SibSp']+titanic['Parch']+1","a897db0d":"titanic.drop(['Name','SibSp','Parch','Ticket','Cabin','Deck'],axis=1,inplace=True)","683687a7":"emb = pd.get_dummies(titanic['Embarked'], prefix='Embarked')\ntitl = pd.get_dummies(titanic['Name_pre'], prefix='Name_pre')\nsex = pd.get_dummies(titanic['Sex'], prefix='Sex')\n#deck=pd.get_dummies(titanic['Deck'], prefix='Deck')\n#age = pd.get_dummies(titanic['Age-range'], prefix='age')\n\n# Concatenating dummy variables to dataframe\ntitanic = pd.concat([titanic, emb,titl, sex], axis=1)\n\n# Droping intial variables\ntitanic.drop(['Embarked','Name_pre', 'Sex'], axis=1, inplace=True)","08fecfc7":"train=titanic.loc[:train_-1, :]","61b84d0b":"test=titanic.loc[train_:, :]\ntest=test.drop('Survived',axis=1)","c7b21934":"train_x=train.drop('Survived',axis=1)\ntrain_y=train['Survived']","33e51ff4":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(train_x,train_y,test_size=0.2,random_state=0)","c1fa9d3b":"from sklearn.ensemble import RandomForestClassifier\nscores={}\nfor i in range(1,8):\n    model=RandomForestClassifier(n_estimators=200*i,random_state=0)\n    model.fit(X_train,Y_train)\n    score=model.score(X_test,Y_test)\n    scores[200*i]=score\nprint('Score for diffrent values of n_estimators')\nfor i,j in scores.items():\n    print(i, ' : ', j)","a0bfeafb":"model_=RandomForestClassifier(n_estimators=1000,random_state=0)\nmodel_.fit(train_x,train_y)\nprediction=model_.predict(test)","911bed70":"submit=pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':prediction})\nsubmit['Survived']=submit['Survived'].astype('int')\nsubmit.reset_index(inplace=True)\nsubmit=submit.drop('index',axis=1)\nsubmit","99eb5147":"submit.to_csv('titanic_submit.csv',index=False)","8d1b16d6":"# Visualization","41681ab3":"### From the above entries we can see that Survived column contains 418 null values that is from the test dataset so nothing to do with that \n","fea94b20":"### Splitting the train and test dataset and applying the machine learning algorithms","1560bfdd":"### Taking the insights of the titanic data and Exploring the data","9d709180":"# Insights from he data","19d77762":"### Applying Random Forest Classifier on the train and test data to predict the survival rate ","3f09da74":"# Implementing Random forest Classifier","8e81f3e8":"- This notebook is on the survival of the passengers of the titanic accident that happened in 15th April 1921 in the off the coast of Newfoundland in the North Atlantic.\n- We are predicting the number of people survided in the accident which type of people like upper lower or middle based on the deck and the room allottment\n- ![image.png](attachment:7c885e56-5d8d-407c-879b-f560d2a439b2.png)","04aa0ede":"# Read The dataset","e7c10a8f":"# Importing Datasets\n##### Reading the datasets from kaggle: \n##### Training and tesing data both.\nConcatinating both datasetsinto a single dataset \"titanic\" and performing the EDA, Stored the length of the train dataset in \"train_\" so that we can split the dataset into train and test later on for prediction","701a48cb":"### Checking for null values in the table by visualizing it also","7fb87f2e":"#### Reducing the Name prefix","ce336b35":"### Printing the number of rows in trian data","e4fad3c3":"### Seprating the train and test data","ec92c83f":"# Imputation of Null Vlaues","978eb142":"### Making the dummy variables of the certains columns"}}