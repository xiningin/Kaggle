{"cell_type":{"47448bb0":"code","fef3c131":"code","4effab8d":"code","e45d5759":"code","d3e99a68":"code","25567a85":"code","f8e1708f":"code","d030f9d3":"code","d1f6ee0d":"code","7e7c1ae6":"code","066281e2":"code","83a05099":"markdown","153ac359":"markdown","f9673aec":"markdown","f5363117":"markdown","ea0ce425":"markdown","30d0236a":"markdown","4a0e821c":"markdown","072f486c":"markdown","76aa9846":"markdown","e92f2b91":"markdown"},"source":{"47448bb0":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', -1)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.models import load_model\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\n#import sqlite3\nfrom tqdm import tqdm\nimport re\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.model_selection import train_test_split","fef3c131":"filtered_data = pd.read_csv(\"..\/input\/instruments\/reviews.csv\")\n\n# Give reviews with Score>3 a positive rating(1), and reviews with a score<3 a negative rating(0).\ndef partition(x):\n    if x < 3:\n        return 0\n    return 1\n\ndef findMinorClassPoints(df):\n    posCount = int(df[df['overall']==1].shape[0]);\n    negCount = int(df[df['overall']==0].shape[0]);\n    if negCount < posCount:\n        return negCount\n    return posCount\n\n#changing reviews with score less than 3 to be positive and vice-versa\nactualScore = filtered_data['overall']\npositiveNegative = actualScore.map(partition) \nfiltered_data['overall'] = positiveNegative\n\n#Performing Downsampling\n# samplingCount = findMinorClassPoints(filtered_data)\n# postive_df = filtered_data[filtered_data['overall'] == 1].sample(n=5000)\n# negative_df = filtered_data[filtered_data['overall'] == 0].sample(n=5000)\n\n# filtered_data = pd.concat([postive_df, negative_df])\n\nprint(\"Number of data points in our data\", filtered_data.shape)\nfiltered_data.head(3)","4effab8d":"\n#Deduplication of entries\nfinal=filtered_data.drop_duplicates(subset={\"reviewerID\",\"asin\",\"reviewerName\",\"reviewText\",\"unixReviewTime\"}, keep='first', inplace=False)\nfinal.shape\n\n#Preprocessing\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\npreprocessed_reviews = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(final['reviewText'].values):\n    sentence = re.sub(r\"http\\S+\", \"\", str(sentence))\n    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    # sentance = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n    preprocessed_reviews.append(sentence.strip())\n    \n## Similartly you can do preprocessing for review summary also.\ndef concatenateSummaryWithText(str1, str2):\n    return str1 + ' ' + str2\n\npreprocessed_summary = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(final['summary'].values):\n    sentence = re.sub(r\"http\\S+\", \"\", str(sentence))\n    #sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    # sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n    preprocessed_summary.append(sentence.strip())\n    \npreprocessed_reviews = list(map(concatenateSummaryWithText, preprocessed_reviews, preprocessed_summary))\nfinal['CleanedText'] = preprocessed_reviews\nfinal['CleanedText'] = final['CleanedText'].astype('str')","e45d5759":"X = final['CleanedText']\ny = final['overall']","d3e99a68":"del final\ndel preprocessed_reviews\ndel preprocessed_summary\ndel filtered_data","25567a85":"X_t, X_test, y_t, y_test = train_test_split(X, y, test_size=0.30, stratify=y, shuffle=True)\nX_train, X_cv, y_train, y_cv = train_test_split(X_t, y_t, test_size=0.30, stratify=y_t, shuffle=True)\nprint(\"Shape of Input  - Train:\", X_train.shape)\nprint(\"Shape of Output - Train:\", y_train.shape)\nprint(\"Shape of Input  - CV   :\", X_cv.shape)\nprint(\"Shape of Output - CV   :\", y_cv.shape)\nprint(\"Shape of Input  - Test :\", X_test.shape)\nprint(\"Shape of Output - Test :\", y_test.shape)","f8e1708f":"tokenize = Tokenizer(num_words=5000)\ntokenize.fit_on_texts(X_train)\n\nX_train_new = tokenize.texts_to_sequences(X_train)\nX_cv_new = tokenize.texts_to_sequences(X_cv)\nX_test_new = tokenize.texts_to_sequences(X_test)\n\nprint(X_train_new[1])\nprint(len(X_train_new))","d030f9d3":"# truncate and\/or pad input sequences\nmax_review_length = 1000\nX_train_new = sequence.pad_sequences(X_train_new, maxlen=max_review_length)\nX_cv_new = sequence.pad_sequences(X_cv_new, maxlen=max_review_length)\nX_test_new = sequence.pad_sequences(X_test_new, maxlen=max_review_length)\n\nprint(X_train_new.shape)\nprint(X_train_new[1])","d1f6ee0d":"# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()\n    \nn_epochs = 5\nbatchsize = 512\n\nfinal_output = pd.DataFrame(columns=[\"Model\", \"Architecture\",\n                                     \"TRAIN_LOSS\", \"TEST_LOSS\", \"TRAIN_ACC\", \"TEST_ACC\"]);","7e7c1ae6":"# create the model\nembed_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(5000, embed_vector_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"***********************************************\")\nprint(\"Printing the Model Summary\")\nprint(model.summary())\nprint(\"***********************************************\")\n\n\n\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model\n\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('my_model.h5')","066281e2":"m_hist = model.fit(X_train_new, y_train, epochs=n_epochs, \n                   batch_size=batchsize, verbose=1, validation_data=(X_cv_new, y_cv))\n\nscore = model.evaluate(X_test_new, y_test, batch_size=batchsize)\nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfinal_output = final_output.append({\"Model\": 1,\n                                    \"Architecture\": 'Embedding-LSTM-Sigmoid', \n                                    \"TRAIN_LOSS\": '{:.5f}'.format(m_hist.history[\"loss\"][n_epochs-1]),\n                                    \"TEST_LOSS\": '{:.5f}'.format(score[0]),\n                                    \"TRAIN_ACC\": '{:.5f}'.format(m_hist.history[\"acc\"][n_epochs-1]),\n                                    \"TEST_ACC\": '{:.5f}'.format(score[1])}, ignore_index=True)\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch')\nax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,n_epochs+1))\n\nvy = m_hist.history['val_loss']\nty = m_hist.history['loss']\nplt_dynamic(x, vy, ty, ax)","83a05099":"# Amazon Reviews Analysis\n\n\nData Source: http:\/\/jmcauley.ucsd.edu\/data\/amazon\/ <br>\n\n\nThe Amazon Reviews dataset consists of reviews of various product categories from Amazon.<br>\n\nAttribute Information:\n\n1. reviewerId - unqiue identifier for the user\n2. asin - id of product\n3. reviewerName - name of reviewer\n4. Helpful\/1 - number of users who found the review helpful\n5. Helpful\/0 - number of users who found the review not helpful\n6. reviewText - Actual review\n7. overall - rating between 1 and 5\n8. Summary - brief summary of the review\n9. UnixReviewTime - Unix Timestamp when the review was submitted\n10. ReviewTime - Normal Day\/Time\n\n\n#### Objective:\nGiven a review, determine whether the review is positive (rating of 4 or 5) or negative (rating of 1 or 2).\n\n<br>\n[Q] How to determine if a review is positive or negative?<br>\n<br> \n[Ans] We could use Score\/Rating. A rating of 4 or 5 can be cosnidered as a positive review. A rating of 1 or 2 can be considered as negative one. A review of rating 3 is considered nuetral and such reviews are ignored from our analysis. This is an approximate and proxy way of determining the polarity (positivity\/negativity) of a review.","153ac359":"Segregating the input and output data from the dataset.\n\nWe will be using the Cleaned Text i.e preprocessed data from the dataset and score for that text","f9673aec":"### Splitting the data","f5363117":"This is just to give batch input to the RNN","ea0ce425":"## Data Preprocessing","30d0236a":"The dataset is available in two forms\n1. .csv file\n2. SQL  Database\n\nWe are using the csv dataset\n<br> \n\nHere as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score is above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\".","4a0e821c":"### Model ( Embedding -> LSTM -> Output(Sigmoid) ) ","072f486c":"## Loading the data","76aa9846":"### Tokenizing the dataset","e92f2b91":"### Padding the dataset"}}