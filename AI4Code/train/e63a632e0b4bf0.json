{"cell_type":{"7f75f506":"code","489522e8":"code","75540b2e":"code","1f32984f":"code","8958852a":"code","ff676c51":"code","9a3d07d7":"code","5d4a3949":"markdown","f6996ca2":"markdown","5e0401d3":"markdown","f353746c":"markdown","ce29e8b3":"markdown"},"source":{"7f75f506":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","489522e8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline","75540b2e":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))\n\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) \nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n\nall_data = pd.get_dummies(all_data)\nall_data = all_data.fillna(all_data.mean())\n\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice","1f32984f":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","8958852a":"#model_ridge = Ridge()\n\n#alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n#cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            #for alpha in alphas]\n\n#cv_ridge = pd.Series(cv_ridge, index = alphas)\n#cv_ridge.plot(title = \"Validation - Just Do It\")\n#plt.xlabel(\"alpha\")\n#plt.ylabel(\"rmse\")\n\n#cv_ridge.min()","ff676c51":"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\nrmse_cv(model_lasso).mean()\ncoef = pd.Series(model_lasso.coef_, index = X_train.columns)","9a3d07d7":"preds = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\n\nlasso_preds = np.expm1(model_lasso.predict(X_test))\npreds = lasso_preds\n\nsolution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\nsolution.to_csv(\"submission.csv\", index = False)","5d4a3949":"# **Lasso Regression**","f6996ca2":"### Lasso Linear Model","5e0401d3":"Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. \n\nThis modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.\n\n**Loss function = OLS + alpha * summation (squared coefficient values)**\n\nIn the above loss function, alpha is the parameter we need to select. \n\nA low alpha value can lead to over-fitting, whereas a high alpha value can lead to under-fitting.","f353746c":"Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is a modification of linear regression.\n\nIn Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients.\n\nThe loss function for Lasso Regression can be expressed as below:\n\n**Loss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)**\n\nIn the above loss function, alpha is the penalty parameter we need to select. \n\nUsing an l1 norm constraint forces some weight values to zero to allow other coefficients to take non-zero values.","ce29e8b3":"# Ridge Regression"}}