{"cell_type":{"65eaaae0":"code","b8cccd98":"code","2d2033fa":"code","a20cd616":"code","5393c51d":"code","68df2471":"code","539c2ad9":"code","6aea6cd2":"code","c4940fe8":"code","eefb0860":"code","de3cfcf0":"code","5b40a21e":"markdown","898fb7bc":"markdown","b35dc70f":"markdown","1089801a":"markdown","db81c1ad":"markdown","7c515a09":"markdown","fbbe2ae7":"markdown","17ba8976":"markdown"},"source":{"65eaaae0":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy import linalg as LA\nimport os","b8cccd98":"dirname = '\/kaggle\/input\/olivetti'\nfileName = 'olivetti_faces.npy'\nfaces = np.load(os.path.join(dirname,fileName))","2d2033fa":"faces.shape","a20cd616":"plt.imshow(faces[0], cmap='gray')","5393c51d":"avgFace = np.average(faces, axis=0)\nprint('avgFace.shape:',avgFace.shape)\nplt.imshow(avgFace, cmap='gray')","68df2471":"#Subtracting mean from all the samples\nX = faces\nX = X.reshape((X.shape[0], X.shape[1]**2))\nX = X - np.average(X, axis=0)","539c2ad9":"plt.imshow(avgFace + X[0].reshape(64,64), cmap='gray')","6aea6cd2":"XtX = np.matmul(X.T, X)\nprint('XtX.shape:', XtX.shape)","c4940fe8":"C = (1\/X.shape[0])*XtX","eefb0860":"w, v = LA.eig(C)","de3cfcf0":"numComponents = 100\nV = v[:,0:numComponents]\nV.shape\n\nperson_id = 25\nfig,a =  plt.subplots(1,2)\nimport numpy as np\nx = np.arange(1,5)\na[0].imshow(avgFace + X[person_id].reshape(64,64), cmap='gray')\na[0].set_title('Original')\n\nalphas = np.dot(V.T, X[person_id])\na[1].imshow(avgFace + np.dot(V, alphas).reshape(64,64), cmap='gray')\na[1].set_title('Compressed:' + str(numComponents))\nplt.show()","5b40a21e":"Finding eigen vectors (which are also the orthonormal directions) of the covariance matrix.","898fb7bc":"* Suppose we had 1 million gray scale images each of size 64X64. We would have to store $(10^6)(64)(64) = 4.096 X10^9$ values.\n* But if we do PCA and pick only the top r eigen vectors then:\n<br>\nwe need to store only the r eigen vectors(of dim 4096) and r $\\alpha$ for each of the one million images: $(4096*r + 10^6 * r)$\n<br\/>\nIf r = 100 then we need to store only about $~10^8$ values. That means, we will need 10 times lesser memory to store all the images.","b35dc70f":"## PCA allows us to compress the data by only storing the 'important' information ","1089801a":"### Computing covariance matrix","db81c1ad":"Expressing any samples as a linear combination of the eigen vectors:<br\/>\n$x_i = \\alpha_1 v_1 + \\alpha_2 v_2 + ................+\\alpha_d v_d$\n<br\/>\nTo find $\\alpha_1$:\n<br\/>\n$v_1^T x_i = \\alpha_1(v_1^T  v_1) + \\alpha_2(v_1^T v_2)  +.........+ \\alpha_d(v_1^T v_d)$\n<br\/>\n$v_1^T x_i = \\alpha_1.1 + \\alpha_2.0 +............+\\alpha_d.0$     $\\;\\;\\;\\;\\;\\;$  (Since $v_1$, $v_2$,....$v_d$ are orthonormal to each other.)\n<br\/>\n$\\implies \\alpha_1 = v_1^T x_i$\n<br\/>\nSimilarly all other $\\alpha$ can also be found out.\n<br\/>\n<br\/>\n<br\/>\n* Instead of including all the eigen vectors, we can take only a small fraction of them and the reconstructed image will be a good match with the origianl image.\n<br\/>\n* Suppose we take only 100 eigen vectors (out of 4096 in this case) corresponding to top 100 eigen values:","7c515a09":"### Average face","fbbe2ae7":"## In this notebook image compression using PCA is demonstrated.","17ba8976":"* We can see that the reconstructed image is very similar to the original image."}}