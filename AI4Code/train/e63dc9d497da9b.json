{"cell_type":{"a26001c1":"code","91e5341a":"code","659622cd":"code","aa591b96":"code","59af6ce8":"code","4cf1ee94":"code","dbf04412":"code","75a18b5a":"code","36684ce4":"code","bd465d65":"code","45f5fb17":"code","1bfb90b4":"code","629132f2":"code","1ec8e493":"code","864ed3d1":"code","7df7c3c7":"code","fb2a2167":"code","b4a6a6cf":"code","afe8f996":"code","43cd6797":"code","de5236d4":"code","26401255":"code","6b135489":"code","cff032be":"code","2b782151":"code","818f8df2":"code","d88485ce":"code","efc69068":"markdown","23ce9878":"markdown","77161065":"markdown","ef5ea48b":"markdown","dc1faf4c":"markdown","37285849":"markdown","0770a2a4":"markdown","7b7b0e57":"markdown","922c74bc":"markdown","9950dd9f":"markdown","79863d78":"markdown","10a1c11e":"markdown","f949a9df":"markdown","c487b931":"markdown","4a99b19d":"markdown","33a6b169":"markdown","f2e81cf0":"markdown","1be2ccc8":"markdown","7cacbde0":"markdown","0dc92416":"markdown","96e0e8a6":"markdown","e79ac8e5":"markdown","e3737c7e":"markdown","1acba241":"markdown","3b165f3a":"markdown","513457d9":"markdown","dc8bda14":"markdown","fb468204":"markdown","98b96488":"markdown","a55b6ab4":"markdown"},"source":{"a26001c1":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport pandas as pd \nimport os\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,r2_score\nfrom sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nimport warnings\nfrom imblearn.over_sampling import SMOTE\nwarnings.filterwarnings('ignore')","91e5341a":"df = pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-additional-full.csv', sep = ';')\ndf.head()","659622cd":"df.info()","aa591b96":"for col in df.columns:\n    print()\n    if df[col].dtype == 'object':\n        print(f'Name of Column is: {col} and unique values are: {df[col].unique()}')","59af6ce8":"#this function returns categorical variables\ndef return_categorical(df):\n\n  categorical_columns = [column_name for column_name in df if df[column_name].dtype == 'O']\n  return categorical_columns\n\n#this function returns numerical variables\ndef return_numerical(df):\n\n  return list(set(df.columns) - set(return_categorical(df)))\n\n\ndef check_normal(df):\n  fig, axes = plt.subplots(1,len(return_numerical(df)), figsize =(70, 10))\n\n  for i,numeric_column_name in enumerate(list(set(df.columns) -set(return_categorical(df)))):\n\n    sns.distplot(df[numeric_column_name], ax=axes[i]);\n    plt.title(f'Distribution of {numeric_column_name}');\n    \ndef classifier(clf, x_train,x_test,y_train,y_test):\n    y_test_pred = clf.predict(x_test)\n    y_train_pred = clf.predict(x_train)\n\n    accuracy_test = accuracy_score(y_test,y_test_pred)\n    accuracy_train =  accuracy_score(y_train,y_train_pred)\n    \n    roc_test = roc_auc_score(y_test, y_test_pred, multi_class='ovr')\n    roc_train = roc_auc_score(y_train, y_train_pred, multi_class='ovr')\n    \n    print('Train accuracy is:',accuracy_train )\n    print('Test accuracy is:',accuracy_test )\n    print()\n    print('Train ROC is:', roc_train)\n    print('Test ROC is:',roc_test )\n    \n    # Fscore, precision and recall on test data\n    f1 = f1_score(y_test, y_test_pred)\n    precision = precision_score(y_test, y_test_pred)\n    recall = recall_score(y_test, y_test_pred) \n    print()\n    print(\"F score is:\",f1 )\n    print(\"Precision is:\",precision)\n    print(\"Recall is:\", recall)\n  \n\ndef random_search(clf,params, x_train,x_test,y_train,y_test):\n    \n    random_search = RandomizedSearchCV(estimator= clf, param_distributions=params, scoring='roc_auc', cv=5)\n    random_search.fit(x_train, y_train)\n    optimal_model = random_search.best_estimator_\n\n    print(\"Best parameters are: \", random_search.best_params_)\n    print()\n    print(\"Best estimator is: \", random_search.best_estimator_)\n    print()\n    print('Scores and accuracies are:')\n    print()\n    classifier(optimal_model, x_train,x_test,y_train,y_test)","4cf1ee94":"check_normal(df)","dbf04412":"\n# plotting graphs for all categorical columns\nfor col in return_categorical(df):\n    counts = df[col].value_counts().sort_index()\n    if len(counts) > 10:\n      fig = plt.figure(figsize=(30, 10))\n    else:\n      fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    counts.plot.bar(ax = ax, color='steelblue')\n    ax.set_title(col + ' counts')\n    ax.set_xlabel(col) \n    ax.set_ylabel(\"Frequency\")\nplt.show()","75a18b5a":"corr = df.corr()\ncorr_greater_than_75 = corr[corr>=.75]\ncorr_greater_than_75","36684ce4":"#Visualising by heatmap\nplt.figure(figsize=(12,8))\nsns.heatmap(corr_greater_than_75, cmap=\"Reds\", annot = True);","bd465d65":"df['pdays'].unique()","45f5fb17":"df['job'] = df['job'].apply(lambda x: -1 if x=='unknown' or x=='unemployed' else (15 if x=='entrepreneur' else (8 if x == 'blue-collar' else ( 6 if x=='technician' or x=='services' or  x=='admin.' or x=='management' else (4 if x== 'self-employed' or x=='student' else (2 if x=='housemaid' or x=='retired' else None) )))))\ndf['housing'] = df['housing'].apply(lambda x: 0 if x=='no' else (1 if x=='yes' else -1))\ndf['loan'] = df['loan'].apply(lambda x: 0 if x=='no' else (1 if x=='yes' else -1))\ndf['y'] = df['y'].apply(lambda x: 0 if x=='no' else (1 if x=='yes' else -1))\ndf['default'] = df['default'].apply(lambda x: 0 if x=='no' else (1 if x=='yes' else -1))\ndf['poutcome'] = df['poutcome'].apply(lambda x: 0 if x=='failure' else (2 if x=='failure' else -1))\ndf['pdays'] = df['pdays'].apply(lambda x: 0 if x==999 else(20 if x<=10 else(6 if x<=20 else 3)))","1bfb90b4":"df.drop(['day_of_week', 'contact', 'month'], axis=1, inplace = True)","629132f2":"df  = pd.get_dummies(df, drop_first = True)","1ec8e493":"x = df.drop(\"y\", axis=1)\ny = df['y']\nx.sample()\n\nx_train,x_test,y_train,y_test = train_test_split(x,y, random_state=42)","864ed3d1":"smote = SMOTE()\n\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(x_train, y_train)\n\nprint('Original dataset shape', len(x_train))\nprint('Resampled dataset shape', len(x_smote))","7df7c3c7":"s = StandardScaler()","fb2a2167":"knn = KNeighborsClassifier(n_neighbors = 20)\nknn.fit( s.fit_transform(x_train), y_train)\n\nclassifier(knn, s.fit_transform(x_smote),s.transform(x_test), y_smote,y_test)","b4a6a6cf":"error_rate = []\nfor i in range(1,40):\n knn = KNeighborsClassifier(n_neighbors=i)\n knn.fit( s.fit_transform(x_train), y_train)\n pred_i = knn.predict(s.transform(x_test))\n error_rate.append(np.mean(pred_i != y_test))\n\nacc = []\nfor i in range(1,40):\n    neigh = KNeighborsClassifier(n_neighbors = i).fit(s.fit_transform(x_train), y_train)\n    yhat = neigh.predict(s.transform(x_test))\n    acc.append(metrics.accuracy_score(y_test, yhat))\n    ","afe8f996":"# Visualising the tuning parameters\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate)))","43cd6797":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),acc,color = 'blue',linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('accuracy vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nprint(\"Maximum accuracy:-\",max(acc),\"at K =\",acc.index(max(acc)))","de5236d4":"knn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit( s.fit_transform(x_smote), y_smote)\n\nclassifier(knn, s.fit_transform(x_smote),s.transform(x_test),y_smote,y_test)","26401255":"# bagging classifier\nmodel = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors = 10),\n                          n_estimators = 15)\nclassifier(model.fit( s.fit_transform(x_smote), y_smote), s.fit_transform(x_smote),s.transform(x_test),y_smote,y_test)","6b135489":"dtree = DecisionTreeClassifier(random_state=0)\ndtree.fit(x_train, y_train)\nclassifier(dtree, x_train,x_test,y_train,y_test)","cff032be":"param_grid = {'max_depth':np.arange(3,20),\n              'min_samples_split': np.arange(3,20,1),\n             'min_samples_leaf':np.arange(3,30),\n              'min_samples_split' : np.arange(3,30),\n              'criterion': ('gini', 'entropy')}\n\n\n\nrandom_search(DecisionTreeClassifier(random_state=0),param_grid, x_train,x_test,y_train,y_test)","2b782151":"# I have chosen tuned hyperparameters here\n\n\nkfold = model_selection.KFold(n_splits = 3)\n\n# bagging classifier\nmodel = BaggingClassifier(base_estimator = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=25, max_depth=8, criterion='gini'),n_estimators = 500,)\n\nclassifier(model.fit(x_train, y_train), x_train,x_test,y_train,y_test)","818f8df2":"rforest = RandomForestClassifier(random_state=0)\nclassifier(rforest.fit(x_train, y_train), x_train,x_test,y_train,y_test)","d88485ce":"params = {'n_estimators' : np.arange(100,1000, 100),\n              'max_depth' : np.arange(3,20,1),\n              'min_samples_split' : np.arange(3,20,1),\n              'min_samples_leaf' : np.arange(3,20,1),\n         'max_features': ('sqrt', 'log2'), 'criterion': ('gini', 'entropy')}\n\nrandom_search(RandomForestClassifier(random_state=0),params, x_train,x_test,y_train,y_test)","efc69068":"# Checking Categorical Columns","23ce9878":"# Modeling with KNN","77161065":"# Modeling with Decession Tree","ef5ea48b":"# Modeling value of K gives least error","dc1faf4c":"* None of the feature are following a Normal Distribution","37285849":"# Bagging Knn as base Model","0770a2a4":"# One hot encoding for categorical features ","7b7b0e57":"## Table Content\n* 1. Importing Modules\n\n* 2. Loading Data\n\n* 3. Data PreProcessing And Visualizations\n\n* 4. Utility Functions\n\n* 5. Data Balancing\n\n* 6. Modelling And Optimizing The Models\n\n* 7. Results And Conclusion","922c74bc":"# Train_Test_Split","9950dd9f":"## Modefing the column on the basis of importance weighs","79863d78":"# Visualising Categorical Variabels","10a1c11e":"* 999 represents no contact has been done, thus i'll be assigning it a very less weight.","f949a9df":"# Scaling and Optimising","c487b931":"# Tuning hyper parameters for Decission tree","4a99b19d":"# Checking Unique values","33a6b169":"* Not using balanced data with tree based models because they can handle imbalace pretty well so whynot.","f2e81cf0":"# Importing Modules","1be2ccc8":"# Decission Tree Base Bagging","7cacbde0":"# Tuning hyper perameters by using Knn ","0dc92416":"* Some correlation can be seen here between emp. var rate and nr.employed.\n* Also, euribor3m and emp.var rate.\n* However, I would not be removing any. As I am gonna mostly be training on tree models which doesn't require much preprocessing.","96e0e8a6":"# Modeling with Random Forest","e79ac8e5":"# Data Preprocessing\n* Data Visualisation","e3737c7e":"* Bagging with Decision tree is performing the best according to recall and roc.\n* For a credit insurance problem, I would want to go with recall here.","1acba241":"# Loading Data","3b165f3a":"# Tuning hyper parameters for Random Forest","513457d9":"# Balancing data by using SMOTE","dc8bda14":"# Droping columns","fb468204":"## Utility Function","98b96488":"# Checing highly co-related column","a55b6ab4":"* imbalanced categorical data observed"}}