{"cell_type":{"3d60cc62":"code","796d0692":"code","5ea02c38":"code","ff2a90e3":"code","6b51cc11":"code","a45a6ca7":"code","ad20e89c":"code","16d71be0":"code","7847c26a":"code","a6259d5f":"code","63ebdeda":"code","b2881efd":"code","21982f55":"code","4faa575d":"code","94347243":"code","90730bc8":"code","212f7738":"code","f8b61aa2":"code","2b19f50f":"code","2a12aa3d":"code","5e710366":"code","6ff590c1":"code","e5b6556e":"code","939b913a":"code","8c3f9b32":"code","aaa97ce0":"code","99bf15df":"code","036f82dc":"code","a917aecd":"code","d140dfb8":"code","5f748f54":"code","fdf9de85":"code","9b09f6de":"code","0de45018":"code","81b92f6a":"code","6ddf4263":"code","58283e55":"code","d36bf83c":"code","111fd65b":"code","f5fce0dc":"code","4cc4bbb3":"code","ef33ecbf":"code","452deae9":"code","cc734f33":"code","3c638255":"code","9b054a05":"code","9d5ccab2":"markdown","75ee07f7":"markdown","7dae2d2d":"markdown"},"source":{"3d60cc62":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom keras.utils import to_categorical\nfrom keras.datasets import mnist\nfrom keras.models import Sequential , Model\nfrom keras.layers import Dense, Dropout,Input,BatchNormalization , Add\nfrom keras.optimizers import SGD\nfrom keras.utils import plot_model\nfrom matplotlib import pyplot as plt\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","796d0692":"(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\ntrain_images = train_images.reshape((60000, 784))\ntest_images = test_images.reshape((10000, 784))\n\n# Normalize pixel values to be between 0 and 1\ntrain_images, test_images = train_images \/ 255.0, test_images \/ 255.0\n\n\nprint('train shape : ',train_images.shape)\nprint('test shape : ',test_images.shape)","5ea02c38":"es = EarlyStopping(monitor='val_loss',\n                   mode='auto',\n                   verbose=1,\n                   patience=7)\n\nlrr= ReduceLROnPlateau(monitor='val_loss',\n                       factor=0.1, \n                       patience=3, \n                       min_lr=1e-9)","ff2a90e3":"encoder_input = Input(784,name = 'input_encoder')\nencoder = Dense(1024, activation='tanh',name = 'encoder_layer1')(encoder_input)\nencoder = BatchNormalization(name = 'encoder_layer2')(encoder)\nencoder = Dropout(0.3 , name = 'encoder_layer3')(encoder)\nencoder = Dense(128, activation='relu',name = 'encoder_layer4')(encoder)\nencoder = Dense(64, activation='relu',name = 'encoder_layer5')(encoder)\nencoder_output = Dense(64, activation='relu',name = 'output_encoder')(encoder)\n\nEncoder = Model(inputs= [encoder_input], outputs=[encoder_output],name = 'Encoder')\nEncoder.summary()","6b51cc11":"decoder_input = Input(64,name = 'input_decoder')\ndecoder = Dense(64, activation='tanh',name = 'decoder_layer1')(decoder_input)\ndecoder = Dense(64, activation='tanh',name = 'decoder_layer2')(decoder)\ndecoder = BatchNormalization(name = 'decoder_layer3')(decoder)\ndecoder = Dropout(0.2 , name = 'decoder_layer4')(decoder)\ndecoder = Dense(128, activation='relu',name = 'decoder_layer5')(decoder)\ndecoder = Dense(1024, activation='relu',name = 'decoder_layer6')(decoder)\ndecoder_output = Dense(784,activation = 'sigmoid',name ='output_layer')(decoder)\n\nDecoder = Model(inputs= [decoder_input], outputs=[decoder_output],name = 'Decoder')\nDecoder.summary()","a45a6ca7":"autoencoder_input = Input(784,name = 'input_autoencoder')\nlatent = Encoder(autoencoder_input)\ndecoded_data = Decoder(latent)\nautoencoder = Model(inputs= [autoencoder_input], outputs=[decoded_data],name = 'AutoEncoder')\n\nautoencoder.summary()","ad20e89c":"plot_model(autoencoder, show_shapes=True)","16d71be0":"sgd = SGD(lr=0.01, momentum=0.99)\nautoencoder.compile(optimizer=sgd, loss='mae')","7847c26a":"history = autoencoder.fit(train_images[:5000], train_images[:5000],\n                    batch_size = 128,\n                    validation_split = 0.2,\n                              epochs= 100,\n                              verbose=1,\n                              callbacks=[es , lrr])","a6259d5f":"val_loss = history.history['val_loss']\nloss = history.history['loss']\n\nplt.plot(val_loss)\nplt.plot(loss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Val error','Train error'], loc='upper right')\nplt.savefig('plot_error.png')\nplt.show()","63ebdeda":"input_model = Input(64,name = 'input_layer')\n\nclassifier = Dense(64, activation='relu')(input_model)\nclassifier = Dense(32, activation='relu')(classifier)\noutput = Dense(10, activation='softmax')(classifier)\n\nclassifier = Model(inputs= [input_model], outputs=[output])\nclassifier.summary()","b2881efd":"sgd = SGD(lr=0.01, momentum=0.9)\nclassifier.compile(optimizer=sgd, loss='categorical_crossentropy' , metrics = ['accuracy'])","21982f55":"es = EarlyStopping(monitor='val_loss',\n                   mode='auto',\n                   verbose=1,\n                   patience=5)\n\nlrr= ReduceLROnPlateau(monitor='val_loss',\n                       factor=0.01, \n                       patience=2, \n                       min_lr=1e-8)","4faa575d":"Encoder.summary()","94347243":"train_label = to_categorical(train_labels[:5000])\nlatent_vector = Encoder.predict(train_images[:5000])\nhistory = classifier.fit(latent_vector,train_label,\n               batch_size = 128,\n               validation_split = 0.2,\n               epochs= 100,\n               verbose=1,\n               callbacks=[es , lrr])","90730bc8":"val_loss = history.history['val_loss']\nloss = history.history['loss']\n\nplt.plot(val_loss)\nplt.plot(loss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Val error','Train error'], loc='upper right')\nplt.savefig('plot_error.png')\nplt.show()","212f7738":"val_accuracy = history.history['val_accuracy']\naccuracy = history.history['accuracy']\n\nplt.plot(val_accuracy)\nplt.plot(accuracy)\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend(['Val accuracy','Train accuracy'], loc='upper right')\nplt.savefig( 'plot_accuracy.png')\nplt.show()","f8b61aa2":"print(train_labels[5001:5010])\n\n# plot images\nfor i in range(5001,5010):\n    img = train_images[i].reshape(28,28)\n    # define subplot\n    j = i - 5001\n    plt.subplot(330 + 1 + j)\n    # plot raw pixel data\n    plt.imshow(img,cmap=plt.get_cmap('gray'))\n    \n# show the figure\nplt.show()","2b19f50f":"latent_vector = Encoder.predict(train_images[5001:5010])\ny_pred = classifier.predict(latent_vector)\npred = np.argmax(y_pred, axis=1)","2a12aa3d":"score = metrics.accuracy_score(train_labels[5001:5010], pred)\nround(score*100,2)","5e710366":"print('Classification Report')\nprint(classification_report(train_labels[5001:5010], pred))","6ff590c1":"latent_vector = Encoder.predict(test_images)\ny_pred = classifier.predict(latent_vector)\npred = np.argmax(y_pred, axis=1)","e5b6556e":"score = metrics.accuracy_score(test_labels, pred)\nround(score*100,2)","939b913a":"print('Classification Report')\nprint(classification_report(test_labels, pred))","8c3f9b32":"y_true = list(range(0,10))\ny_pred = list(range(0,10))\ndata = confusion_matrix(test_labels, pred)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, cmap=\"Blues\",fmt=\"d\", annot=True,annot_kws={\"size\": 16})# font size","aaa97ce0":"Decoder.summary()","99bf15df":"plt.figure(figsize = (10,5))\n\nlatent_vector_test =np.ones([1,64])\nprint(latent_vector_test)\nrebuild_image = Decoder.predict(latent_vector_test)\nrebuild_image = rebuild_image.reshape(28,28)\n# define subplot\nplt.subplot(120 + 1 + 0)\n# plot raw pixel data\nplt.imshow(rebuild_image,cmap=plt.get_cmap('gray'))\n\n\nlatent_vector_test =np.zeros([1,64])\nprint(latent_vector_test)\nrebuild_image = Decoder.predict(latent_vector_test)\nrebuild_image = rebuild_image.reshape(28,28)\n# define subplot\nplt.subplot(120 + 1 + 1)\n# plot raw pixel data\nplt.imshow(rebuild_image,cmap=plt.get_cmap('gray'))","036f82dc":"plt.figure(figsize = (15,10))\nfor i in range(9):\n    latent_vector_test =np.random.rand(1,64)\n    rebuild_image = Decoder.predict(latent_vector_test)\n    rebuild_image = rebuild_image.reshape(28,28)\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # plot raw pixel data\n    plt.imshow(rebuild_image,cmap=plt.get_cmap('gray'))","a917aecd":"autoencoder_input = Input(784,name = 'input_autoencoder')\n\nout1_encoder = Dense(1024, activation='relu',name = 'encoder_layer1')(autoencoder_input)\nout2_encoder = Dense(512, activation='relu',name = 'encoder_layer2')(out1_encoder)\nout2_encoder = BatchNormalization(name = 'encoder_layer3')(out2_encoder)\nout3_encoder = Dense(256, activation='relu',name = 'encoder_layer4')(out2_encoder)\nout4_encoder = Dense(64, activation='relu',name = 'encoder_layer5')(out3_encoder)\n\nin1_decoder = Dense(64, activation='relu',name = 'decoder_layer1')(out4_encoder)\nin2_decoder = Add()([in1_decoder,out4_encoder])\nin3_decoder = Dense(256, activation='relu',name = 'decoder_layer2')(in2_decoder)\nin3_decoder = Add()([in3_decoder,out3_encoder])\nin4_decoder = Dense(512, activation='relu',name = 'decoder_layer3')(in3_decoder)\nin4_decoder = Add()([in4_decoder,out2_encoder])\nin5_decoder = Dense(1024, activation='relu',name = 'decoder_layer4')(in4_decoder)\nin5_decoder = Add()([in5_decoder,out1_encoder])\n\nautoencoder_output = Dense(784,activation = 'sigmoid',name = 'output_autoencoder')(in5_decoder)\n\nAutoEncoder = Model(inputs= [autoencoder_input], outputs=[autoencoder_output],name = 'AutoEncoder')\nAutoEncoder.summary()","d140dfb8":"plot_model(AutoEncoder, show_shapes=True)","5f748f54":"sgd = SGD(lr=0.01, momentum=0.99)\nAutoEncoder.compile(optimizer=sgd, loss='mae')","fdf9de85":"es = EarlyStopping(monitor='val_loss',\n                   mode='auto',\n                   verbose=1,\n                   patience=7)\n\nlrr= ReduceLROnPlateau(monitor='val_loss',\n                       factor=0.1, \n                       patience=3, \n                       min_lr=1e-9)","9b09f6de":"history = AutoEncoder.fit(train_images[:5000], train_images[:5000],\n                    batch_size = 128,\n                    validation_split = 0.2,\n                              epochs= 100,\n                              verbose=1,\n                              callbacks=[es , lrr])","0de45018":"val_loss = history.history['val_loss']\nloss = history.history['loss']\n\nplt.plot(val_loss)\nplt.plot(loss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Val error','Train error'], loc='upper right')\nplt.savefig('plot_error.png')\nplt.show()","81b92f6a":"input_model = Input(64,name = 'input_layer')\n\nclassifier = Dense(64, activation='relu')(input_model)\nclassifier = Dense(32, activation='relu')(classifier)\noutput = Dense(10, activation='softmax')(classifier)\n\nclassifier = Model(inputs= [input_model], outputs=[output])\nclassifier.summary()","6ddf4263":"sgd = SGD(lr=0.01, momentum=0.9)\nclassifier.compile(optimizer=sgd, loss='categorical_crossentropy' , metrics = ['accuracy'])","58283e55":"es = EarlyStopping(monitor='val_loss',\n                   mode='auto',\n                   verbose=1,\n                   patience=5)\n\nlrr= ReduceLROnPlateau(monitor='val_loss',\n                       factor=0.01, \n                       patience=2, \n                       min_lr=1e-8)","d36bf83c":"layer_names = []\nfor layer in AutoEncoder.layers[0:6]:\n    layer_names.append(layer.name)\nlayer_names    ","111fd65b":"Encoder = Model(inputs= AutoEncoder.inputs, outputs= AutoEncoder.layers[5].output,name = 'Encoder')\nEncoder.summary()","f5fce0dc":"plot_model(Encoder, show_shapes=True)","4cc4bbb3":"train_label = to_categorical(train_labels[:5000])\n\nlatent_vector = Encoder.predict(train_images[:5000])\nhistory = classifier.fit(latent_vector,train_label,\n               batch_size = 128,\n               validation_split = 0.2,\n               epochs= 100,\n               verbose=1,\n               callbacks=[es , lrr])","ef33ecbf":"latent_vector = Encoder.predict(test_images)\ny_pred = classifier.predict(latent_vector)\npred = np.argmax(y_pred, axis=1)","452deae9":"score = metrics.accuracy_score(test_labels, pred)\nround(score*100,2)","cc734f33":"print('Classification Report')\nprint(classification_report(test_labels, pred))","3c638255":"y_true = list(range(0,10))\ny_pred = list(range(0,10))\ndata = confusion_matrix(test_labels, pred)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, cmap=\"Blues\",fmt=\"d\", annot=True,annot_kws={\"size\": 16})# font size","9b054a05":"# MAE\nimport tensorflow as tf\n\ny_true = np.ones([1,10])\ny_pred = np.zeros([1,10])\n\nprint(y_true)\nprint(y_pred)\n\n# Using 'auto'\/'sum_over_batch_size' reduction type.\nmae = tf.keras.losses.MeanAbsoluteError()\nprint(mae(y_true, y_pred).numpy())\n","9d5ccab2":"# simple AutoEncoder (Vanilla autoencoders)","75ee07f7":"# Advanced AutoEncoder","7dae2d2d":"# play with Decoder !"}}