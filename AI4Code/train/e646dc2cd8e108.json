{"cell_type":{"8887e607":"code","1ba4826c":"code","0ce0c124":"code","979fcee4":"code","fa9c82e5":"code","6e21543c":"code","a5c53888":"code","956e4716":"code","e8bd80fb":"code","0c9ad66e":"code","bc53425e":"code","9b0a1675":"code","0d4fb72a":"code","b033c197":"code","91fd089a":"code","9de5f1e3":"code","0a9b229d":"code","48283960":"code","e0c53fbb":"code","7d042e94":"code","9c99c1c0":"code","6f2dfce7":"code","10a12b79":"code","d1affd08":"code","ee4bc999":"code","eea36047":"markdown","eb51038e":"markdown","96a7f06c":"markdown","bdc21109":"markdown","871ba7b8":"markdown","a9cb81bc":"markdown","b4f62370":"markdown","708b376f":"markdown"},"source":{"8887e607":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, warnings,json\nwarnings.filterwarnings('ignore')\n\n# Plot\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# Transformer Model\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel\n\n# SKLearn Library\nfrom sklearn.model_selection import train_test_split\n\n# Garbage Collector\nimport gc\n\nos.environ[\"WANDB_API_KEY\"] = \"0\"","1ba4826c":"# Initialize TPU\n\ndef Init_TPU():  \n\n    try:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        REPLICAS = strategy.num_replicas_in_sync\n        print(\"Connected to TPU Successfully:\\n TPUs Initialised with Replicas:\",REPLICAS)\n        \n        return strategy\n    \n    except ValueError:\n        \n        print(\"Connection to TPU Falied\")\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n        \n        return strategy\n    \nstrategy=Init_TPU()","0ce0c124":"# Define Dataset Path\npath = '..\/input\/contradictory-my-dear-watson\/'","979fcee4":"# Load Training Data\ntrain_url = os.path.join(path,'train.csv')\ntrain_data = pd.read_csv(train_url, header='infer')\n\n# Sample Submission\nsample_sub_url = os.path.join(path,'sample_submission.csv')\nsample_sub = pd.read_csv(sample_sub_url, header='infer')\n\n# Load Test Data\ntest_url = os.path.join(path,'test.csv')\ntest_data = pd.read_csv(test_url, header='infer')","fa9c82e5":"print(\"Total Records: \", train_data.shape[0])","6e21543c":"#Inspect\ntrain_data.head()","a5c53888":"# Records per Label\nprint(\"Records per Label: \\n\", train_data.groupby('label').size())","956e4716":"# Records per Languages\nprint(\"Records per Language: \\n\", train_data.groupby('language').size())","e8bd80fb":"# Pie Chart\ndf = pd.DataFrame({\"count\": train_data.language.value_counts() })\nfig = px.pie(df, values='count', names=df.index, title='Language Count %',\n             labels={'index':'lang'}, color_discrete_sequence=px.colors.diverging.Earth)\nfig.update_traces(textinfo='percent')\nfig.show()","0c9ad66e":"# Bar Plot - Label Count per Language \nfig, ax = plt.subplots(figsize=(20,10))\ntrain_data.groupby(['language','label']).count()['premise'].unstack().plot(ax=ax,kind='bar', cmap='cividis')\nplt.grid(color='gray',linestyle='--',linewidth=0.2)\nax.set_facecolor('#d8dcd6')\nplt.title(\"Label Count per Language\", fontsize='18')\nplt.xticks(rotation=45)","bc53425e":"# Garbage Collection\ngc.collect()","9b0a1675":"# Transformer Model Name\ntransformer_model = 'jplu\/tf-xlm-roberta-large'\n\n# Define Tokenizer\ntokenizer = XLMRobertaTokenizer.from_pretrained(transformer_model)","0d4fb72a":"# Checking the output of tokenizer\ntokenizer.convert_tokens_to_ids(list(tokenizer.tokenize(\"Elementary, My Dear Watson!\")))","b033c197":"# Create seperate list from Train & Test Dataframes with only Premise & Hypothesis\ntrain = train_data[['premise','hypothesis']].values.tolist()\ntest = test_data[['premise','hypothesis']].values.tolist()","91fd089a":"# Define Max Length\nmax_len = 80   # << change if you wish\n\n# Encode the training & test data \ntrain_encode = tokenizer.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\ntest_encode = tokenizer.batch_encode_plus(test, pad_to_max_length=True, max_length=max_len)","9de5f1e3":"# Split the Training Data into Training (90%) & Validation (10%)\n\ntest_size = 0.1  # << change if you wish\nx_train, x_val, y_train, y_val = train_test_split(train_encode['input_ids'], train_data.label.values, test_size=test_size)\n\n\n# Split Test Data\nx_test = test_encode['input_ids']","0a9b229d":"#garbage collect\ngc.collect()","48283960":"# Loading Data Into TensorFlow Dataset\nAUTO = tf.data.experimental.AUTOTUNE\nbatch_size = 16 * strategy.num_replicas_in_sync\n\ntrain_ds = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\nval_ds = (tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size).prefetch(AUTO))\n\ntest_ds = (tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size))","e0c53fbb":"# Garbage Collection\ngc.collect()","7d042e94":"def build_model(strategy,transformer):\n    with strategy.scope():\n        transformer_encoder = TFXLMRobertaModel.from_pretrained(transformer)  #Pretrained BERT Transformer Model\n        \n        input_layer = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        \n        sequence_output = transformer_encoder(input_layer)[0]\n        \n        cls_token = sequence_output[:, 0, :]\n        \n        output_layer = Dense(3, activation='softmax')(cls_token)\n        \n        model = Model(inputs=input_layer, outputs=output_layer)\n        \n        model.compile(\n            Adam(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        \n        return model\n    \n\n# Applying the build model function\nmodel = build_model(strategy,transformer_model)","9c99c1c0":"# Model Summary\nmodel.summary()","6f2dfce7":"# Train the Model\n\nepochs = 30  # < change if you wish\nn_steps = len(train_data) \/\/ batch_size \n\nmodel.fit(train_ds, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds,\n          epochs = epochs)","10a12b79":"# Garbage Collection\ngc.collect()","d1affd08":"prediction = model.predict(test_ds, verbose=0)\nsample_sub['prediction'] = prediction.argmax(axis=1)","ee4bc999":"sample_sub.to_csv(\"submission.csv\", index=False)","eea36047":"# Data","eb51038e":"# Prediction & Saving","96a7f06c":"# Build & Train Model\n\nNow we shall build a model with the pre-trained BERT transformer model into Keras Functional Model","bdc21109":"# Data Prep\n\nIn this step we shall use a pre-trained Transformer model from HuggingFace to encode the input before it is fed to the model\n\n* For details about XLM-RoBERTa model check [here](https:\/\/huggingface.co\/transformers\/model_doc\/xlmroberta.html#)\n\n**Model = xlm-roberta-large**\n","871ba7b8":"# EDA","a9cb81bc":"### Prefetching\n\nPrefetching overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.\n\nThe tf.data API provides the tf.data.Dataset.prefetch transformation. It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to tf.data.experimental.AUTOTUNE which will prompt the tf.data runtime to tune the value dynamically at runtime.","b4f62370":"# Libraries","708b376f":"This notebook serves the dual purpose of being the official submission & a tutorial for NLI (Natural Language Inference). Natural Language Inferencing (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related.\n\nIn this notebook, we shall have a go at the [Contradictory, My Dear Watson](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/overview\/description) competition dataset by performing a simple EDA & then build\/train a model with a pre-trained BERT Model for encoding different languages. The whole thing will be implemented using Tensorflow \/ Keras.\n\nThe training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text.\n\nI shall keep the notebook well commented & organized for easy reading :-)."}}