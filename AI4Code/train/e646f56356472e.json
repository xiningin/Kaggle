{"cell_type":{"ad433a10":"code","6cdfacb0":"code","136a2e27":"code","14bc324d":"code","1faafa16":"code","dfd52cdb":"code","fe594f66":"code","9771009d":"code","71d1d15f":"code","e5f6534e":"code","9dbca44c":"code","1bc0aa2f":"code","fe212ecc":"code","fda2dbee":"code","0d2bbcf1":"code","3eaca0af":"code","56da6db4":"markdown","68f11a23":"markdown","995d787e":"markdown","a5d3ddfd":"markdown","59791c98":"markdown","8b44dd2e":"markdown","5b2736cf":"markdown","7fdf9bc0":"markdown","f4f267b3":"markdown"},"source":{"ad433a10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6cdfacb0":"import random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom IPython.display import Image, display\nfrom tensorflow.keras.preprocessing.image import load_img\nimport PIL\nfrom PIL import ImageOps\n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras import layers","136a2e27":"input_dir = \"..\/input\/offroad-terrain-attention-region-images\/TrainingImages\/TrainingImages\/OriginalImages\"\ntarget_dir = \"..\/input\/offroad-terrain-attention-region-images\/TrainingImages\/TrainingImages\/EnumMasks\/png_0_1\"\nimg_size = (160, 160)\nnum_classes = 3\nbatch_size = 16\n\ninput_img_paths = sorted(\n    [\n        os.path.join(input_dir, fname)\n        for fname in os.listdir(input_dir)\n        if fname.endswith(\".jpg\")\n    ]\n)\ntarget_img_paths = sorted(\n    [\n        os.path.join(target_dir, fname)\n        for fname in os.listdir(target_dir)\n        if fname.endswith(\".png\") and not fname.startswith(\".\")\n    ]\n)\n\nprint(\"Number of samples:\", len(input_img_paths))","14bc324d":"# Display image #21 It was FudanPed00021.png  but only 21 is required though the image below is FudanPed00022.png  \ni = 1\nfigure, ax = plt.subplots(nrows=1,ncols=2,figsize=(8,8))\nax.ravel()[0].imshow(mpimg.imread(input_img_paths[i]))\nax.ravel()[0].set_title(\"Original image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\n#ax.ravel()[2].imshow(PIL.ImageOps.autocontrast(load_img(target_img_paths[i])))\n#ax.ravel()[2].set_title(\"Contrast of mask\")\n#ax.ravel()[2].set_axis_off()\nplt.tight_layout()","1faafa16":"# Display image #21 It was FudanPed00021.png  but only 21 is required though the image below is FudanPed00022.png  \ni = 21\nfigure, ax = plt.subplots(nrows=1,ncols=2,figsize=(8,8))\nax.ravel()[0].imshow(mpimg.imread(input_img_paths[i]))\nax.ravel()[0].set_title(\"Original image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\n#ax.ravel()[2].imshow(PIL.ImageOps.autocontrast(load_img(target_img_paths[i])))\n#ax.ravel()[2].set_title(\"Contrast of mask\")\n#ax.ravel()[2].set_axis_off()\nplt.tight_layout()","dfd52cdb":"class OffDataset(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n        for j, path in enumerate(batch_input_img_paths):\n            img = load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n        for j, path in enumerate(batch_target_img_paths):\n            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = np.expand_dims(img, 2)\n            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n            y[j] -= 1\n        return x, y","fe594f66":"def get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model","9771009d":"# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nmodel = get_model(img_size, num_classes)\nmodel.summary()","71d1d15f":"val_samples = 77 # 85% Training -- 15%(1108) Validation- 77 is 15%  \nrandom.Random(129).shuffle(input_img_paths) #Original 7390(samples) Here 1822(24,65%)\nrandom.Random(129).shuffle(target_img_paths)#samples: 515\ntrain_input_img_paths = input_img_paths[:-val_samples]\ntrain_target_img_paths = target_img_paths[:-val_samples]\nval_input_img_paths = input_img_paths[-val_samples:]\nval_target_img_paths = target_img_paths[-val_samples:]\n\n# Instantiate data Sequences for each split\ntrain_gen = OffDataset(\n    batch_size, img_size, train_input_img_paths, train_target_img_paths\n)\nval_gen = OffDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)","e5f6534e":"# We use the \"sparse\" version of categorical_crossentropy\n# because our target data is integers.\nmodel.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"pets_segmentation.h5\", save_best_only=True)\n]\n\nepochs = 30\nmodelunet=model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)","9dbca44c":"# summarize history for accuracy\nplt.plot(modelunet.history['accuracy'])\nplt.plot(modelunet.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()\n# summarize history for loss\nplt.plot(modelunet.history['loss'])\nplt.plot(modelunet.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()","1bc0aa2f":"# Generate predictions for all images in the validation set\nval_gen = OffDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)\nval_preds = model.predict(val_gen)","fe212ecc":"def display_mask(i):\n    \"\"\"Quick utility to display a model's prediction.\"\"\"\n    mask = np.argmax(val_preds[i], axis=-1)\n    mask = np.expand_dims(mask, axis=-1)\n    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n    return img","fda2dbee":"# Display image #10\ni = 10\nfigure, ax = plt.subplots(nrows=1,ncols=3,figsize=(8,5))\nax.ravel()[0].imshow(mpimg.imread(val_input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(val_target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\nax.ravel()[2].imshow(display_mask(i))\nax.ravel()[2].set_title(\"Predicted mask \")\nax.ravel()[2].set_axis_off()\nplt.tight_layout()","0d2bbcf1":"# Display image #11\ni = 11\nfigure, ax = plt.subplots(nrows=1,ncols=3,figsize=(8,5))\nax.ravel()[0].imshow(mpimg.imread(val_input_img_paths[i]))\nax.ravel()[0].set_title(\"Orginal image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(val_target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\nax.ravel()[2].imshow(display_mask(i))\nax.ravel()[2].set_title(\"Predicted mask \")\nax.ravel()[2].set_axis_off()\nplt.tight_layout()","3eaca0af":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar\u00edlia Prata, @mpwolke Was here')","56da6db4":"#Codes by Ammar Alhaj Ali https:\/\/www.kaggle.com\/ammarnassanalhajali\/image-segmentation-with-a-u-net-and-keras","68f11a23":"#Another one","995d787e":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTd0EUL6_kTfFAALXTx2NlFZu0NfrR17LM7Dw&usqp=CAU)multivu.com","a5d3ddfd":"Off_Road Dataset (Off-Road Terrain Attention Region Images)","59791c98":"#History for Accuracy","8b44dd2e":"#TRAINING","5b2736cf":"#Inference","7fdf9bc0":"#Display sample of Image Dataset","f4f267b3":"#Build the U-Net Model Architecture"}}