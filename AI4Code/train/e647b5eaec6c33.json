{"cell_type":{"6841c47d":"code","a1d25a97":"code","b7abaff5":"code","e7c6228f":"code","9cfa437c":"code","8ff0ad67":"code","63aab9a4":"code","0a77ce8e":"code","c5c497d3":"code","5bb5c1d1":"code","4736a013":"code","8ca3e439":"code","20de07ae":"code","54d46103":"code","8c2859b7":"code","737ba6bc":"code","0597a8c1":"code","327a27ee":"code","9cdcfabb":"code","52fd5856":"code","ff3619d8":"code","5721d6ed":"code","ed1748ab":"code","ebb94a17":"code","44efe459":"code","904e1855":"markdown","cef74fe2":"markdown","4f355072":"markdown","09b41893":"markdown","cc2d452a":"markdown","2d24b4eb":"markdown","f7e24f8f":"markdown","1df63456":"markdown","a68cc571":"markdown","7ac93e29":"markdown","1fd0d1b0":"markdown","2d50fa70":"markdown","20f9e688":"markdown","4e905f2d":"markdown","1a46dc2c":"markdown","7e28c783":"markdown","fa0e9438":"markdown","7a052a5a":"markdown","484bfe14":"markdown","6fdd40fa":"markdown","0c0318c5":"markdown","57f8edff":"markdown","9c3d5a8c":"markdown","33ba85b7":"markdown","a813907a":"markdown","12a136d9":"markdown","4990fa40":"markdown","183fd112":"markdown","a6b59b13":"markdown"},"source":{"6841c47d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1d25a97":"from IPython.display import Image\nurl = 'http:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/56\/Kosaciec_szczecinkowaty_Iris_setosa.jpg'\nImage(url,width=800, height=800)","b7abaff5":"url = 'http:\/\/upload.wikimedia.org\/wikipedia\/commons\/4\/41\/Iris_versicolor_3.jpg'\nImage(url,width=800, height=800)","e7c6228f":"url = 'http:\/\/upload.wikimedia.org\/wikipedia\/commons\/9\/9f\/Iris_virginica.jpg'\nImage(url,width=800, height=800)","9cfa437c":"import seaborn as sns\niris=sns.load_dataset(\"iris\")\niris.head()","8ff0ad67":"iris.info()\n#The data is not so long, just 150 row with 5 columns only one of which is a string, which is our target variable","63aab9a4":"iris.describe(include=\"all\")","0a77ce8e":"sns.pairplot(iris,hue=\"species\")","c5c497d3":"import matplotlib.pyplot as plt\n%matplotlib inline","5bb5c1d1":"setosa=iris[iris[\"species\"]==\"setosa\"]\nplt.figure(figsize=(15,10))\nsns.kdeplot(setosa[\"sepal_width\"],setosa[\"sepal_length\"],cmap=\"plasma\",shade=True,shade_lowest=False)","4736a013":"versicolor=iris[iris[\"species\"]==\"versicolor\"]\nplt.figure(figsize=(15,10))\nsns.kdeplot(versicolor[\"sepal_width\"],versicolor[\"sepal_length\"],cmap=\"BuPu\",shade=True,shade_lowest=False)","8ca3e439":"virginica=iris[iris[\"species\"]==\"virginica\"]\nplt.figure(figsize=(15,10))\nsns.kdeplot(virginica[\"sepal_width\"],virginica[\"sepal_length\"],cmap=\"YlGnBu\",shade=True,shade_lowest=False)","20de07ae":"X=iris.drop(\"species\",axis=1)\ny=iris[\"species\"]\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)","54d46103":"from sklearn.svm import SVC\nmodel=SVC()\nmodel.fit(X_train,y_train)","8c2859b7":"predictions=model.predict(X_test)\npredictions","737ba6bc":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\n#here we get the classification report to learn how accurate our model is","0597a8c1":"print(confusion_matrix(y_test,predictions))","327a27ee":"from sklearn.model_selection import GridSearchCV","9cdcfabb":"param_grid={\"C\":[1,10,100,100],\"gamma\":[1,0.1,0.01,0.001,0.0001]} \n#here we select values for grid search to try\ngrid=GridSearchCV(SVC(),param_grid,verbose=2)","52fd5856":"grid.fit(X_train,y_train)\n# we apply it to our training data to see the best C and gamma values\n#grid.fit() will find the best combination of C and gamma values for our model","ff3619d8":"grid.best_params_","5721d6ed":"grid.best_estimator_","ed1748ab":"grid_predictions=grid.predict(X_test) \n#Now we predict with this readjustment","ebb94a17":"print(classification_report(y_test,grid_predictions))","44efe459":"print(confusion_matrix(y_test,grid_predictions))","904e1855":"*3. The Iris Virginica:","cef74fe2":"*SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n\n*we can play with some of these parameters of the algorithm to predict better if the algorithm performs bad\n\n*C as a parameter of SVC model controls the cost of misclassification; a large C value gives us low bias and high variance.Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance\n\n*gamma as a parameter; large gamma value leads to high bias and low variance in the model or vice versa.","4f355072":"# 2.Overall Information About the Support Vector Machines Model","09b41893":"*We get the almost the same results, but in large datasets, the grid search will result in better results","cc2d452a":"*After training our model,we will predict the test dataset","2d24b4eb":"# 2.Exploratory Data Analysis","f7e24f8f":"*We have to split our data before applying the algorithm","1df63456":"*Here we call our SVM algorithm and apply it for the training dataset","a68cc571":"# 3.Splitting Data and Training the Algorithm","7ac93e29":"From the pairplot above, \n\n    *we see that setosa type is highly separable from the other species, its petal length and sepal length is longer than the others.\n    \n    *Virginica has wider petal width\n    \n    *Sepal width of virginica and versicolor is pretty close to each other","1fd0d1b0":"# 1.Information About the Dataset:","2d50fa70":"# 4.Evaluation of the Performance of Our Model","20f9e688":"*2.The Iris Versicolor","4e905f2d":"*There is a good correlation between sepal width and length of setosa as it is seen above","1a46dc2c":"*We can adjust C and gamma parameters with a grid research instead of trying one by one","7e28c783":"*Our model has pretty good prediction with precission,accuracy,recall and F1 scores over %95 for both 0 and 1's for all types","fa0e9438":"*The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Sir Ronald Fisher in the 1936 as an example of discriminant analysis. \n\n\n*The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor), so 150 total samples. Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.There are three types of this flower as follows:","7a052a5a":"*Now we will visualize our data in order to get better insights ","484bfe14":"* we will create a kde plot of sepal_length versus sepal width for setosa species of flower","6fdd40fa":"*They are supervised machine learning models with associated algorithms that analyze data and recognize patterns used for classification and regression analysis\n\n*This notebook will focus for classification\n\n*SVM training algorithm builds a model that assigns new examples or at least test data points into one category or the other making it a non-probabilistic binary linear classifier\n\n*The separate categories are divided by a clear gap as wide as possible and new examples are predicted to belong to a category based on which side of the gap they fall on\n\n*The algorith draw a separating hyperplane between the two classes. But this separating line should be the best option. The algorithm chooses a hyperplane that maximizes the margin between the classes which touch the end points of the both classes, which are also known as the support vectors.\n\n*This separation can also be extended to nonlinear data","0c0318c5":"*According to confusion matrix;there is only one false prediction,and these results are also very good\n","57f8edff":"*Now we will evaluate again how the algorithm performs with the new parameters of the grid search","9c3d5a8c":"# 5. Grid Search for Better Predictions:","33ba85b7":"* The two types has also good correlation between their sepal width and length, but not as high as setosa type","a813907a":"*These results  above as the best parameter and estimator show that {'C': 1, 'gamma': 0.1} are the best combination of C and gamma values for our data for training the algorithm","12a136d9":"*we use seaborn to get the famoues iris data","4990fa40":"*1.The Iris Setosa:","183fd112":"*GridSearchCV(estimator, param_grid, , scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2n_jobs', error_score=nan, return_train_score=False) |\n| Exhaustive search over specified parameter values for an estimator.","a6b59b13":"*We will create the same plot for other types as well in the following plots:"}}