{"cell_type":{"1d4fc3a2":"code","be8e4ac6":"code","5b07d59a":"code","5c96fabd":"code","db6bdf66":"code","40aba48d":"code","79c047ca":"code","a26c8fac":"code","8fe60484":"code","7514ad9d":"code","94de78d5":"code","4a4101a0":"code","d1277fbe":"code","4e646c62":"code","25a88d92":"code","9335a04e":"code","06806826":"code","7e833996":"markdown","51d494c9":"markdown","eea86075":"markdown","d034a983":"markdown","52e51b7a":"markdown","181a4108":"markdown","41072646":"markdown","a1abf7fe":"markdown","ffaa3e79":"markdown"},"source":{"1d4fc3a2":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder,RobustScaler, PowerTransformer, PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, ElasticNet, LassoLars, Lasso, RidgeCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.feature_selection import SelectFromModel\n\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt  \n\nimport warnings\nwarnings.filterwarnings('ignore')","be8e4ac6":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest  = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsub   = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","5b07d59a":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","5c96fabd":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","db6bdf66":"sns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","40aba48d":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\nsns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","79c047ca":"y_train = train.SalePrice.values\ntrain.drop(['SalePrice'], axis=1, inplace=True)\nprint(f\"training size is : {train.shape}\")\nprint(f\"test size is : {train.shape}\")","a26c8fac":"train_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :train_na})\nmissing_data.head(20)","8fe60484":"def imputer(all_data):\n    all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\n    for col in ( \"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\\\n                'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\\\n               'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\\\n               \"MasVnrType\",'MSSubClass'\n               ):\n        all_data[col] = all_data[col].fillna('None')\n\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars',\\\n               'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\\\n               \"MasVnrArea\",\n               ):\n        all_data[col] = all_data[col].fillna(0)\n\n    for col in ('MSZoning',\"Functional\",'Electrical','KitchenQual','Exterior1st','Exterior2nd',\\\n                'SaleType'\n               ):\n        all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n\n    all_data = all_data.drop(['Utilities'], axis=1)\n    return all_data\n\ntrain = imputer(train)\ntest  = imputer(test)","7514ad9d":"def eng(all_data):\n    \n    all_data['exists_garage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['exists_bsmt']   = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data[\"OverallGrade\"]  = all_data[\"OverallQual\"] * all_data[\"OverallCond\"]\n    all_data['Total_Bath']    = all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])\n    all_data[\"SimplOverallCond\"] = all_data.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                           4 : 2, 5 : 2, 6 : 2, # average\n                                                           7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                          })\n    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n    all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n    all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n    all_data['MoSold'] = all_data['MoSold'].astype(str)\n    all_data['OverallQual']=all_data['OverallQual'].astype(str)\n    all_data['GarageYrBlt']=all_data['GarageYrBlt'].astype(str)\n    all_data['GarageCars']=all_data['GarageCars'].astype(str)\n    all_data['BedroomAbvGr']=all_data['BedroomAbvGr'].astype(str)\n    all_data['HalfBath']=all_data['HalfBath'].astype(str)\n    return all_data\n\ntrain = eng(train)\ntest  = eng(test)","94de78d5":"object_feats  = train.dtypes[train.dtypes == \"object\"].index.tolist()\nnumeric_feats = train.dtypes[train.dtypes != \"object\"].index.tolist()","4a4101a0":"def skew_elimination(all_data):\n    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n    skewness = skewness[abs(skewness) > 0.75]\n    skewed_features = skewness.index\n    all_data[skewed_features] = np.log1p(all_data[skewed_features])\n    return all_data\n\ntrain = skew_elimination(train)\ntest = skew_elimination(test)","d1277fbe":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), object_feats),\n        ('num', RobustScaler() , numeric_feats)\n    ])","4e646c62":"xg = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nclf_xg = Pipeline(steps=[\n                    ('pre', preprocessor),\n                    ('poly', PolynomialFeatures(2)),\n                    ('selection', SelectFromModel(estimator=RandomForestRegressor(n_estimators=300, random_state=1))),\n                    ('xg', xg),\n                    ])\n\nclf_xg.fit(train, y_train)","25a88d92":"lg = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=6, min_sum_hessian_in_leaf = 11)\nclf_lg = Pipeline(steps=[\n                    ('pre', preprocessor),\n                    ('lg', lg),\n                    ])\n\nclf_lg.fit(train, y_train)","9335a04e":"estimators = [('forest', RandomForestRegressor(n_estimators=300,random_state=1)),\n                ('kernel_ridge', KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)),\n              ('Boosting', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber',random_state=1)),\n              ('elasticnet', ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)),\n              ( 'lasso', Lasso(alpha =0.0005, random_state=1)),\n             ]\nstack_reg = StackingRegressor(estimators = estimators, final_estimator = RandomForestRegressor(n_estimators = 400 ,random_state=1), n_jobs=-1)\nclf = Pipeline(steps=[\n                    ('pre', preprocessor),\n                    ('stack', stack_reg),\n                    ])\nclf.fit(train, y_train)\n\npredictions = 0.5*np.expm1(clf.predict(test)) + 0.25*np.expm1(clf_xg.predict(test))+ 0.25*np.expm1(clf_lg.predict(test))","06806826":"sub['SalePrice'] = predictions\nsub.to_csv('submission.csv',index=False)","7e833996":"# Importing and Saving Data","51d494c9":"# Eliminating Skewness of features","eea86075":"# Training and Predicting","d034a983":"**Non-normality disappears after log(1+x) transformation**","52e51b7a":"# Independent variables Imputing","181a4108":"# **Importing libraries**","41072646":"# Target value transformation","a1abf7fe":"# Outliers","ffaa3e79":"# Feature engineering"}}