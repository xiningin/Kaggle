{"cell_type":{"5d61e60d":"code","4480f8c7":"code","5f6efaa0":"code","fb9b27d1":"code","916c88ce":"code","acdda432":"code","d391d03a":"code","c5f262dc":"code","55f8ae36":"code","9bae0cb2":"code","9157d3b5":"code","ae2ba085":"code","be15b307":"code","2f616da2":"code","75edfeec":"code","9e379b75":"code","9055ee14":"code","c1379a16":"code","591c6a8a":"code","ffabf127":"code","b559875f":"code","cfd404ab":"code","e3bd9f57":"code","e403f651":"code","c7ced243":"code","5596aa53":"code","99519df7":"code","d2786003":"code","f8768e70":"code","bd75b102":"code","80d2b1c3":"code","bc6a4b3f":"code","26cd6d26":"code","b1b31ddd":"code","63857176":"code","21e473a9":"markdown","8ded6879":"markdown","8641b011":"markdown","e5a49d76":"markdown","a3fed795":"markdown","ed88d366":"markdown","5c2b990a":"markdown","52572bd1":"markdown","b600a478":"markdown","bf3c6a10":"markdown","ffdaee6f":"markdown","fb158aca":"markdown","3af87388":"markdown","21388104":"markdown","5e9091a4":"markdown","a915d8b4":"markdown","4af4cacc":"markdown","b24bb70e":"markdown","1a24c2b0":"markdown","c38b9c5f":"markdown","5f44b421":"markdown","d9be1289":"markdown"},"source":{"5d61e60d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\nimport matplotlib.image as implt\nfrom PIL import Image \nimport seaborn as sns\nimport cv2 as cs2\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')","4480f8c7":"train_path = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\"\ntest_path = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\"\n\ntrain_horses = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\/horses\"\ntest_horses = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/horses\"\n\ntrain_humans = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\/humans\"\ntest_humans = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/humans\"","5f6efaa0":"# VISUALIZATION\ncategory_names = os.listdir(train_path) # output: ['humans', 'horses']\nnb_categories = len(category_names) # output: 2\ntrain_images = []\n\nfor category in category_names:\n    folder = train_path + \"\/\" + category\n    train_images.append(len(os.listdir(folder)))\n\nsns.barplot(y=category_names, x=train_images).set_title(\"Number Of Training Images Per Category\");","fb9b27d1":"test_images = []\nfor category in category_names:\n    folder = test_path + \"\/\" + category\n    test_images.append(len(os.listdir(folder)))\n\nsns.barplot(y=category_names, x=train_images).set_title(\"Number Of Testing Images Per Category\");","916c88ce":"img1 = implt.imread(train_horses + \"\/horse01-0.png\")\nimg2 = implt.imread(train_humans + \"\/human01-00.png\")\n\nplt.subplot(1, 2, 1)\nplt.title('horse')\nplt.imshow(img1)       \nplt.subplot(1, 2, 2)\nplt.title('human')\nplt.imshow(img2)\nplt.show()","acdda432":"img_size = 50\nhumans_train = []\nhorses_train = []\nlabel = []\n\nfor i in os.listdir(train_humans): # all train human images\n    if os.path.isfile(train_path + \"\/humans\/\" + i): # check image in file\n        humans = Image.open(train_path + \"\/humans\/\" + i).convert(\"L\") # converting grey scale \n        humans = humans.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        humans = np.asarray(humans)\/255 # bit format\n        humans_train.append(humans)\n        label.append(1)\n        \nfor i in os.listdir(train_horses): # all train horse images\n    if os.path.isfile(train_path + \"\/horses\/\" + i): # check image in file\n        horses = Image.open(train_path + \"\/horses\/\" + i).convert(\"L\") # converting grey scale \n        horses = horses.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        horses = np.asarray(horses)\/255 # bit format\n        horses_train.append(horses)\n        label.append(0)","d391d03a":"x_train = np.concatenate((humans_train,horses_train),axis=0) # training dataset\nx_train_label = np.asarray(label) # label array containing 0 and 1\nx_train_label = x_train_label.reshape(x_train_label.shape[0],1)\n\nprint(\"humans:\",np.shape(humans_train) , \"horses:\",np.shape(horses_train))\nprint(\"train_dataset:\",np.shape(x_train), \"train_values:\",np.shape(x_train_label))","c5f262dc":"img_size = 50\nhumans_test = []\nhorses_test = []\nlabel = []\n\nfor i in os.listdir(test_humans): # all train human images\n    if os.path.isfile(test_path + \"\/humans\/\" + i): # check image in file\n        humans = Image.open(test_path + \"\/humans\/\" + i).convert(\"L\") # converting grey scale \n        humans = humans.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        humans = np.asarray(humans)\/255 # bit format\n        humans_test.append(humans)\n        label.append(1)\n        \nfor i in os.listdir(test_horses): # all train horse images\n    if os.path.isfile(test_path + \"\/horses\/\" + i): # check image in file\n        horses = Image.open(test_path + \"\/horses\/\" + i).convert(\"L\") # converting grey scale \n        horses = horses.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        horses = np.asarray(horses)\/255 # bit format\n        horses_test.append(horses)\n        label.append(0)","55f8ae36":"x_test = np.concatenate((humans_test,horses_test),axis=0) # training dataset\nx_test_label = np.asarray(label) # label array containing 0 and 1\nx_test_label = x_test_label.reshape(x_test_label.shape[0],1)\n\nprint(\"humans:\",np.shape(humans_test) , \"horses:\",np.shape(horses_test))\nprint(\"test_dataset:\",np.shape(x_test), \"test_values:\",np.shape(x_test_label))","9bae0cb2":"x = np.concatenate((x_train,x_test),axis=0) # count: 1027+256= 1283  | train_data\n# x.shape: \n#   output = (1283,50,50)\ny = np.concatenate((x_train_label,x_test_label),axis=0) # count: 1027+256= 1283 | test_data\nx = x.reshape(x.shape[0],x.shape[1]*x.shape[2]) # flatten 3D image array to 2D, count: 50*50 = 2500\nprint(\"images:\",np.shape(x), \"labels:\",np.shape(y))","9157d3b5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\n\nprint(\"Train Number: \", number_of_train)\nprint(\"Test Number: \", number_of_test)","ae2ba085":"x_train = X_train.T\nx_test = X_test.T\ny_train = y_train.T\ny_test = y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","be15b307":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01) # np.full((row,column),value)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","2f616da2":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -(1-y_train)*np.log(1-y_head)-y_train*np.log(y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]  # x_train.shape[1]  is for scaling, x_train.shape[1] = 1090\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\n        \"derivative_weight\": derivative_weight,\n        \"derivative_bias\": derivative_bias\n    }\n    return cost,gradients","75edfeec":"def update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        \n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 50 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n        # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","9e379b75":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    # if z is bigger than 0.5, our prediction is human (y_head=1)\n    # if z is smaller than 0.5, our prediction is horse (y_head=0)\n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","9055ee14":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    # initialize\n    dimension = x_train.shape[0] # 2500\n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    test_acc_lr = round((100 - np.mean(np.abs(y_prediction_test - y_test)) * 100),2)\n    train_acc_lr = round((100 - np.mean(np.abs(y_prediction_train - y_train))*100),2)\n    \n    # Print train\/test Errors\n    print(\"train accuracy: %\", train_acc_lr)\n    print(\"test accuracy: %\", test_acc_lr)\n    return train_acc_lr, test_acc_lr","c1379a16":"#you can adjust learning_rate and num_iteration to check how the result is affected\n#(for learning rate, try exponentially lower values:0.001 etc.) \ntrain_acc_lr, test_acc_lr = logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.001, num_iterations = 2000)","591c6a8a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\ntest_acc_logregsk = round(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)* 100, 2)\ntrain_acc_logregsk = round(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)* 100, 2)","ffabf127":"# with GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\n    \"C\": np.logspace(-4, 4, 20),\n    \"penalty\": [\"l1\",\"l2\"]\n}\nlg=LogisticRegression(random_state=42)\nlog_reg_cv=GridSearchCV(lg,grid,cv=10,n_jobs=-1,verbose=2)\nlog_reg_cv.fit(x_train.T,y_train.T)\nprint(\"accuracy: \", log_reg_cv.best_score_)","b559875f":"models = pd.DataFrame({\n    'Model': ['LR without sklearn','LR with sklearn','LR with GridSearchCV' ],\n    'Train Score': [train_acc_lr, train_acc_logregsk, \"-\"],\n    'Test Score': [test_acc_lr, test_acc_logregsk, log_reg_cv.best_score_*100]\n})\nmodels.sort_values(by='Test Score', ascending=False)","cfd404ab":"x_train.shape[0]","e3bd9f57":"def initialize_parameters_and_layer_sizes_NN(x_train,y_train):\n    parameters = {\n        \"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1, # we multiply by 0.1 to get a small number\n        \"bias1\": np.zeros((3,1)),\n        \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n        \"bias2\": np.zeros((y_train.shape[0],1))\n    }\n    return parameters","e403f651":"def forward_propagation_NN(x_train,parameters):\n    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    cache = {\n        \"Z1\": Z1,\n        \"A1\": A1,\n        \"Z2\": Z2,\n        \"A2\": A2\n    }\n    \n    return A2, cache ","c7ced243":"def compute_cost_NN(A2,Y,parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost","5596aa53":"def backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","99519df7":"def update_parameters_NN(parameters, grads, learning_rate = 0.08):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","d2786003":"def predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1])) # x_test.shape[1]: 193\n    # if z is bigger than 0.5 our prediction is human\n    # if z is smaller than 0.5 our prediction is horse\n    for i in range(A2.shape[1]):\n        if A2[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","f8768e70":"def two_layer_neural_network(x_train,y_train,x_test,y_test,num_iterations):\n    cost_list = []\n    index_list = []\n    # initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train,y_train)\n    \n    for i in range(0,num_iterations):\n        # forward propagation\n        A2,cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2,y_train,parameters)\n        # backward propagation\n        grads = backward_propagation_NN(parameters,cache,x_train,y_train)\n        # update parameters\n        parameters = update_parameters_NN(parameters,grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters","bd75b102":"parameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)","80d2b1c3":"# Reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","bc6a4b3f":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\n\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    # units: number of nodes \n    classifier.add(Dense(units= 8, kernel_initializer=\"uniform\", activation=\"relu\", input_dim=x_train.shape[1]))\n    classifier.add(Dense(units= 4, kernel_initializer=\"uniform\", activation=\"relu\", input_dim=x_train.shape[1]))\n    classifier.add(Dense(units= 1, kernel_initializer=\"uniform\", activation=\"sigmoid\")) # end node\n    classifier.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return classifier","26cd6d26":"classifier = KerasClassifier(build_fn=build_classifier, epochs=100)\naccuracies = cross_val_score(estimator=classifier, X=x_train, y=y_train, cv=5)\nmean = accuracies.mean()","b1b31ddd":"print(\"Accuracies: \", accuracies)","63857176":"print(\"Accuracy mean: \",mean)","21e473a9":"# Backward propagation\n* As you know backward propagation means derivative.","8ded6879":"Scaling down the train set images, we have:\n\nhumans and horses arrays of (527,50,50) sizes ---> total train_set (1027,50,50)\nLabel array of corresponding label values, label 1 for human, label 0 for horse ---> total train_set_label (1027,1)","8641b011":"There are 2 types of images in this data set. These are human and horse pictures. Here I will train my program using the deep learning methods. Eventually, the machine will have learned the separation between man and horse.\n\n## <font color=\"blue\">Content<\/font>\n* [Introduction](#1)\n* [Importing Libraries](#2)\n* [Exploring the Dataset](#3)\n* [Processing Dataset](#4)\n* Logistic Regression\n    * [Logistic Regression without sklearn](#5)\n    * [Logistic Regression with sklearn](#6)\n* [Layer Neural Network](#7)","e5a49d76":"The whole picture:","a3fed795":"# Importing Libraries <a id=\"2\"><\/a>\nThe first thing we need to do is import the libraries.","ed88d366":"# **INTRODUCTION** <a id=\"1\"><\/a>\n* Machine Learning refers to a machine learning to use big data sets instead of hard coded rules.\n* Machine Learning allows computers to learn on their own. This type of learning takes advantage of the computing power of modern computers that can easily handle large data sets.\n\n## **Supervised and Unsupervised Learning**\n* **Supervised Learning involves using tagged data sets with inputs and expected outputs.** <br>\nAs you train an AI using supervised learning, you give it an input and say the expected output.\nIf the output produced by AI is wrong, it adjusts its calculations. This process is repeatedly done over the dataset until it minimizes the error rate of Artificial Intelligence.\nAn example of supervised learning is the weather-determining Artificial Intelligence. Learns to forecast the weather using historical data. These training data include inputs (pressure, humidity, wind speed) and outputs (temperature).\n* **Unsupervised Learning is the task of machine learning using data sets that do not have a specific structure.** <br>\nIf you train an AI using unsupervised learning, you allow Artificial Intelligence to make a logical classification of data.\nAn example of unsupervised learning is an example of artificial intelligence that predicts for an e-commerce website. Because here it is not learned using a labeled input and output data set.\nInstead, it will create its own classification using input data. It will tell you which types of users can buy more different products.\n\n## **What is Deep Learning and How Does It Work?**\n* **Deep Learning:**<br>\n    Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n* **Why deep learning:** <br>\n    When the amounth of data is increased, machine learning techniques are insufficient in terms of performance and deep learning gives better performance like accuracy.\n![Deep Learning & Machine Learining](https:\/\/www.inteliment.com\/wp-content\/uploads\/2019\/09\/ml.jpg)\nDeep Learning is a machine learning method. It allows us to train artificial intelligence to predict outputs with a given dataset. Both supervised and unsupervised learning can be used to train artificial intelligence.\n<br>\n\nI will try to explain the logic of working with the example of flight ticket price prediction with Deep Learning. In this example, we will use supervised learning.\nWhen estimating flight ticket prices, let's say we want to use the following entries (for now we consider one-way flights):\n* Departure Airport\n* Arrival Airport\n* Departure Date\n* Company\n\n## **Neural Networks**\nArtificial neural networks are made up of neurons, just like the human brain. All neurons are interconnected and affect the output.\n![Neural Networks](https:\/\/miro.medium.com\/max\/2636\/1*3fA77_mLNiJTSgZFhYnU0Q.png)\nNeurons are divided into three different layers:\n* Entry Layer\n* Hidden Layers\n* Output Layer\n\n**The input layer** receives the input data. In our example, there are four neurons in the entry layer: Departure Airport, Arrival Airport, Departure Date and Company. The input layer sends the entries to the first hidden layer.\n\n**Hidden layers** perform mathematical calculations in our inputs. One of the challenges in creating artificial neural networks is to decide on the number of hidden layers as well as the number of neurons for each layer.\n\n\u201cDeep\u201d in Deep Learning means having more than one hidden layer.\n\n**The output layer** returns output data. In our example, it gives us a price estimate.\n\n### So how is the price prediction done?\nThis is where Deep Learning begins.\n\nEach link between neurons is associated with a \"weight\". This weight determines the importance of the input value. The first weights are set randomly.\nWhen estimating the price of an airplane ticket, one of the most important (weight) factors is the departure date. Therefore, the departure date neuron connections will have a great weight.\n\n![](https:\/\/www.gormanalysis.com\/blog\/neural-networks-a-worked-example_files\/nnets-example_sketch1.jpg)\n\nEach neuron has an Activation Function. One of the objectives of the activation function is to \"standardize\" the outputs from the neuron.\n\nAfter a data set passes through all layers of the neural network, it returns as a result from the output layer.","5c2b990a":"## Forward propagation\n* Forward propagation is almost same with logistic regression.\n* The only difference is we use tanh function and we make all process twice.\n* Also numpy has tanh function. So we do not need to implement it.\n![](https:\/\/machinelearningknowledge.ai\/wp-content\/uploads\/2019\/10\/Feed-Forward-Neural-Network.gif)","52572bd1":"# Exploring the Dataset <a id=\"3\"><\/a>","b600a478":"We combine image strings and tags and flatten 'x':","bf3c6a10":"# Logistic Regression without sklearn <a id=\"5\"><\/a>\n![](https:\/\/d2o2utebsixu4k.cloudfront.net\/media\/images\/9a57ce9a-b10c-4ed0-9729-50d979af0a6f.jpg)\n\n\"logistic_regression\" uses following functions:\n* initialize_weights_and_bias : has initial values of weights and bias which will be updated later\n* sigmoid : activation function that limit the output to a range between 0 and 1\n* forward_backward propogation : is used to calculate cost function(error) and gradient descent(to learn proper weights and bias values that minimize the error)\n* update : updating learning parameters 'w' and 'b' to find best values of them for better training\n* predict : uses x_test as input for forward propogation","ffdaee6f":"# Update Parameters","fb158aca":"# Logistic Regression with sklearn <a id=\"6\"><\/a>\nWe will have the accuracy values of Logistic Regression using sklearn. And bonus, I will compare accuracies of two different Linear Models with LR w\/o sklearn:\n* Logistic Regression\n* Perceptron","3af87388":"# L Layer Neural Network\n![](https:\/\/www.forexmt4indicators.com\/wp-content\/uploads\/2014\/10\/NN1__1.gif)","21388104":"# Loss function and Cost function\n* Loss and cost functions are same with logistic regression\n* Cross entropy function","5e9091a4":"# Create Model","a915d8b4":"Next step, we need to determine the amount of data for train and test. You can modify test_size and see how it affects the accuracy. Let's split!\n","4af4cacc":"Then we need to take transpose of all matrices. The purpose of it, quoted from [here](https:\/\/www.quora.com\/Why-do-we-transpose-matrices-in-machine-learning) is: \"In python, it is often the case that transposing will enable you to have the data in a given shape that might make it easier to use whatever framework or algorithm\"","b24bb70e":"# Layer Neural Network <a id=\"7\"><\/a>","1a24c2b0":"# Prediction with learnt parameters weight and bias","c38b9c5f":"Scaling down the train set images, we have:\n\nhumans and horses arrays of (128,50,50) sizes ---> total train_set (256,50,50)\nLabel array of corresponding label values, label 1 for human, label 0 for horse ---> total train_set_label (256,1)","5f44b421":"# Processing Dataset <a id=\"4\"><\/a>\nNow we need to modify images. The dataset contains different sizes of RGB color images. First we should resize all the images, second sonvert images to grayscale. Depending on purpose, you can go for RGB images. But grey scale has just one dimension while RGB image has 3, and helps you to avoid false classification and complexities.","d9be1289":"Sigmoid \n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-05edc1873d0103e36064862a45566dba)"}}