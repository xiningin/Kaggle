{"cell_type":{"11fafac0":"code","6a88ffbc":"code","5a59563e":"code","dde95488":"code","23097234":"code","a163507f":"code","c38d9434":"code","ec7a2596":"code","f7ca9a77":"code","3b2fa434":"code","5d2b5def":"code","6ea6bfaa":"code","911270c0":"code","ee10a960":"code","2781a764":"code","8f9ef63d":"code","a31572e7":"code","65517857":"code","d7305dfa":"code","64705b46":"code","b53ae717":"code","79d633a8":"code","658ec214":"code","f0ddb76c":"markdown","1bc2ecbc":"markdown","8cf414f0":"markdown","159328d1":"markdown","c866c676":"markdown","d52aa3e7":"markdown","155cf29a":"markdown","096377d7":"markdown","f3d61241":"markdown","949b50f9":"markdown","c44770ad":"markdown","a645e485":"markdown","20933b84":"markdown","d7ce98f6":"markdown","d16a4c73":"markdown","8b110cdb":"markdown","fed5d9d6":"markdown","70bb2fcb":"markdown"},"source":{"11fafac0":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\nimport random\n\nfrom fastprogress import master_bar, progress_bar\nfrom IPython.display import display\n\nimport warnings\nwarnings.filterwarnings('ignore')","6a88ffbc":"SEED = 7\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\n\nnp.random.seed(SEED)\nrandom.seed(SEED)","5a59563e":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nprint(df.shape)\ndf.head()","dde95488":"# Minor preprocessing\ndf['Time'] = df['Time'] \/ 3600 % 24","23097234":"df['Class'].value_counts(normalize=True)","a163507f":"def plot_scatter(X, y, mode='TSNE', fname='file.png'):\n    if mode == 'TSNE':\n        X_r = TSNE(n_components=2, random_state=SEED).fit_transform(X)\n    elif mode == 'PCA':\n        X_r = PCA(n_components=2, random_state=SEED).fit_transform(X)\n    elif mode == 'TSVD':\n        X_r = TruncatedSVD(n_components=2, random_state=SEED).fit_transform(X)\n    else:\n        print('[ERROR]: Please select a valid mode')\n        return\n        \n    traces = []\n    traces.append(go.Scatter(x=X_r[y == 0, 0], y=X_r[y == 0, 1], mode='markers', showlegend=True, name='Non Fraud'))\n    traces.append(go.Scatter(x=X_r[y == 1, 0], y=X_r[y == 1, 1], mode='markers', showlegend=True, name='Fraud'))\n\n    layout = dict(title=f'{mode} plot')\n    fig = go.Figure(data=traces, layout=layout)\n    py.iplot(fig, filename=fname)","c38d9434":"fraud = df.loc[df['Class'] == 1]\nnon_fraud = df.loc[df['Class'] == 0].sample(3000)\n\nnew_df = pd.concat([fraud, non_fraud]).sample(frac=1.).reset_index(drop=True)\ny = new_df.pop('Class')","ec7a2596":"plot_scatter(new_df, y, mode='TSNE', fname='tsne1.png')","f7ca9a77":"plot_scatter(new_df, y, mode='PCA', fname='pca1.png')","3b2fa434":"plot_scatter(new_df, y, mode='TSVD', fname='tsvd1.png')","5d2b5def":"def get_dls(data, batch_sz, n_workers, valid_split=0.2):\n    d_size = len(data)\n    ixs = np.random.permutation(range(d_size))\n\n    split = int(d_size * valid_split)\n    train_ixs, valid_ixs = ixs[split:], ixs[:split]\n\n    train_sampler = SubsetRandomSampler(train_ixs)\n    valid_sampler = SubsetRandomSampler(valid_ixs)\n\n    # Input and output data should be same\n    ds = TensorDataset(torch.from_numpy(data).float(), torch.from_numpy(data).float())\n\n    train_dl = DataLoader(ds, batch_sz, sampler=train_sampler, num_workers=n_workers)\n    valid_dl = DataLoader(ds, batch_sz, sampler=valid_sampler, num_workers=n_workers)\n\n    return train_dl, valid_dl","6ea6bfaa":"def train(epochs, model, train_dl, valid_dl, optimizer, criterion, device):\n    model = model.to(device)\n\n    mb = master_bar(range(epochs))\n    mb.write(['epoch', 'train loss', 'valid loss'], table=True)\n\n    for ep in mb:\n        model.train()\n        train_loss = 0.\n        for train_X, train_y in progress_bar(train_dl, parent=mb):\n            train_X, train_y = train_X.to(device), train_y.to(device)\n            train_out = model(train_X)\n            loss = criterion(train_out, train_y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            mb.child.comment = f'{loss.item():.4f}'\n\n        with torch.no_grad():\n            model.eval()\n            valid_loss = 0.\n            for valid_X, valid_y in progress_bar(valid_dl, parent=mb):\n                valid_X, valid_y = valid_X.to(device), valid_y.to(device)\n                valid_out = model(valid_X)\n                loss = criterion(valid_out, valid_y)\n                valid_loss += loss.item()\n                mb.child.comment = f'{loss.item():.4f}'\n\n        mb.write([f'{ep+1}', f'{train_loss\/len(train_dl):.6f}', f'{valid_loss\/len(valid_dl):.6f}'], table=True)","911270c0":"class AutoEncoder(nn.Module):\n    def __init__(self, f_in):\n        super().__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(f_in, 100),\n            nn.Tanh(),\n            nn.Dropout(0.2),\n            nn.Linear(100, 70),\n            nn.Tanh(),\n            nn.Dropout(0.2),\n            nn.Linear(70, 40)\n        )\n        self.decoder = nn.Sequential(\n            nn.ReLU(inplace=True),\n            nn.Linear(40, 40),\n            nn.Tanh(),\n            nn.Dropout(0.2),\n            nn.Linear(40, 70),\n            nn.Tanh(),\n            nn.Dropout(0.2),\n            nn.Linear(70, f_in)\n        )\n\n    def forward(self, x):\n        return self.decoder(self.encoder(x))","ee10a960":"EPOCHS = 10\nBATCH_SIZE = 512\nN_WORKERS = 0\n\nmodel = AutoEncoder(30)\ncriterion = F.mse_loss\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","2781a764":"X = df.drop('Class', axis=1).values\ny = df['Class'].values\n\nX = MinMaxScaler().fit_transform(X)\nX_nonfraud = X[y == 0]\nX_fraud = X[y == 1]\ntrain_dl, valid_dl = get_dls(X_nonfraud[:5000], BATCH_SIZE, N_WORKERS)","8f9ef63d":"train(EPOCHS, model, train_dl, valid_dl, optimizer, criterion, device)","a31572e7":"with torch.no_grad():\n    model.eval()\n    non_fraud_encoded = model.encoder(torch.from_numpy(X_nonfraud).float().to(device)).cpu().numpy()\n    fraud_encoded = model.encoder(torch.from_numpy(X_fraud).float().to(device)).cpu().numpy()\n\nnrows = 3000\nsample_encoded_X = np.append(non_fraud_encoded[:nrows], fraud_encoded, axis=0)\nsample_encoded_y = np.append(np.zeros(nrows), np.ones(len(fraud_encoded)))","65517857":"plot_scatter(sample_encoded_X, sample_encoded_y, mode='TSNE', fname='tsne2.png')","d7305dfa":"plot_scatter(sample_encoded_X, sample_encoded_y, mode='PCA', fname='pca2.png')","64705b46":"plot_scatter(sample_encoded_X, sample_encoded_y, mode='TSVD', fname='tsvd2.png')","b53ae717":"def print_metric(model, df, y, scaler=None):\n    X_train, X_val, y_train, y_val = train_test_split(df, y, test_size=0.2, shuffle=True, random_state=SEED, stratify=y)\n    mets = [accuracy_score, precision_score, recall_score, f1_score]\n\n    if scaler is not None:\n        X_train = scaler.fit_transform(X_train)\n        X_val = scaler.transform(X_val)\n\n    model.fit(X_train, y_train)\n    train_preds = model.predict(X_train)\n    train_probs = model.predict_proba(X_train)[:, 1]\n    val_preds = model.predict(X_val)\n    val_probs = model.predict_proba(X_val)[:, 1]\n\n    train_met = pd.Series({m.__name__: m(y_train, train_preds) for m in mets})\n    train_met['roc_auc'] = roc_auc_score(y_train, train_probs)\n    val_met = pd.Series({m.__name__: m(y_val, val_preds) for m in mets})\n    val_met['roc_auc'] = roc_auc_score(y_val, val_probs)\n    met_df = pd.DataFrame()\n    met_df['train'] = train_met\n    met_df['valid'] = val_met\n\n    display(met_df)","79d633a8":"encoded_X = np.append(non_fraud_encoded, fraud_encoded, axis=0)\nencoded_y = np.append(np.zeros(len(non_fraud_encoded)), np.ones(len(fraud_encoded)))","658ec214":"clf = LogisticRegression(random_state=SEED)\nprint('Metric scores for original data:')\nprint_metric(clf, X, y)\nprint('Metric score for encoded data:')\nprint_metric(clf, encoded_X, encoded_y)","f0ddb76c":"## AutoEncoder Architecture\n\nAutoencoder consists of two sub-models: Encoder and Decoder. Encoder learns to extract features from the input in order to represent the same data in different dimensional space. Decoder learns to decode these features to construct back the input data. Usually, autoencoder is used to extract features which have lower dimension than the input, but the main idea here is to try if autoencoder can extract features and learn to represent a class of data from the other one.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png\" width=\"500\"><\/img>","1bc2ecbc":"Plot is not that good maybe because PCA is good but it's linear so it can't interpret complex polynomial relations among independent variables.","8cf414f0":"We will use only a sample of non_fraud cases and let our autoencoder learn how to reconstruct it. This way our encoder will learn insights of data of one class and will help distinguish it from different class.","159328d1":"**Add seed for code reproducibility**","c866c676":"## Load the data","d52aa3e7":"TSNE was able cluster most of the data seperately.","155cf29a":"Looks like the model is not optimal. Maybe the final classifier is not a robust one or our autoencoder needs to have some different architecture and required better hyperparameters. But still results are close to the baseline and TSNE, PCA and LDA created a good plot to distinguish between the classes even when our data was highly imbalanced using the encoded features. Also, the dataset was highly imbalanced, so this technique is worth trying with the datasets which are not highly imbalanced.","096377d7":"## Visualize the data with TSNE and PCA\n\n### TSNE\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. If you want to learn how PCA works behind the scenes check out this [video](https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM).\n\n**NOTE:** Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. You can read more about it [here](https:\/\/distill.pub\/2016\/misread-tsne\/).\n\n### PCA\nPrincipal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize. PCA is sensitive to outliers, they should be removed. If you want to learn how PCA works behind the scenes check out this [video](https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ).\n\n### TruncatedSVD\nTruncated Singular Value Decomposition (SVD) is a matrix factorization technique that factors a matrix M into the three matrices U, \u03a3, and V. This is very similar to PCA, excepting that the factorization for SVD is done on the data matrix, whereas for PCA, the factorization is done on the covariance matrix. Typically, SVD is used under the hood to find the principle components of a matrix.","f3d61241":"## Plotly for data visualization","949b50f9":"**Let's try with a simple model**","c44770ad":"Let's train our model. It will take only a few seconds to train on a GPU.","a645e485":"## Load required libraries","20933b84":"Dataset is highly imbalanced. We have only 0.1727% fraud cases and 99.8273% of non_fraud cases. Generally these datasets are just like that because fraud cases are very rare but still they cost banks a lot so we need some way to predict fraud cases even if we have less data for that class.","d7ce98f6":"# Autoencoders\nAutoencoders are an unsupervised learning technique in which we leverage neural networks for the task of representation learning. Specifically, we'll design a neural network architecture such that we impose a \nbottleneck in the network which forces a compressed knowledge representation of the original input. The main idea is to pass the input and train the neural-net to generate the input itself. In short the input and output is same. This helps in feature extraction the data representation and reducing the number of inputs to train the model. We will get in more detail later in this kernel.","d16a4c73":"We need to encode the data using only the Encoder because that's the part of the model responsible to extract features.","8b110cdb":"For the sake of visualization let's take a sample of non_fraud cases and all the fraud cases.","fed5d9d6":"**Create DataLoaders**","70bb2fcb":"**First let's see how much data we have for each class**"}}