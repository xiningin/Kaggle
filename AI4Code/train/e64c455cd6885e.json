{"cell_type":{"e496bcf5":"code","5e6dcb8f":"code","6dd1f216":"code","0e3def10":"code","a4a5e14e":"code","c9359c1f":"code","b09e7d1d":"code","dce37e6a":"code","ec064402":"code","25cace45":"code","d999833b":"code","a7b117bd":"markdown","ab433e4c":"markdown","a6e83e78":"markdown","7a716666":"markdown","4ac1f5c1":"markdown","69f6d0f2":"markdown","87aa3cb3":"markdown","03186cf7":"markdown","a6aaa827":"markdown","386f051a":"markdown","96d8f752":"markdown"},"source":{"e496bcf5":"import h2o\nh2o.init()","5e6dcb8f":"train_data = h2o.import_file('..\/input\/titanic\/train.csv')\ntest_data  = h2o.import_file('..\/input\/titanic\/test.csv')","6dd1f216":"train_data[\"Survived\"] = train_data[\"Survived\"].asfactor()","0e3def10":"train_data","a4a5e14e":"predictors = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Age\", \"Fare\"]\ntarget     = \"Survived\"","c9359c1f":"from h2o.estimators.gbm import H2OGradientBoostingEstimator\n\nclassifier  =  H2OGradientBoostingEstimator(nfolds =    5,\n                                            ntrees =   15,\n                                            seed   =    1,\n                                            max_depth = 4)\n\nclassifier.train(predictors, target, training_frame = train_data)","b09e7d1d":"predictions = classifier.predict(test_data)\npredictions","dce37e6a":"discrimination_threshold = 0.5\nSurvived = ((predictions[\"p1\"] > discrimination_threshold )*1).set_names(['Survived'])","ec064402":"test_with_predictions = test_data.cbind(Survived)\ntest_with_predictions","25cace45":"submission = test_with_predictions[:,[\"PassengerId\",\"Survived\"]]\nh2o.export_file(submission, path = \"submission.csv\", force = True)","d999833b":"h2o.cluster().shutdown()","a7b117bd":"create a list of the features we are interested in","ab433e4c":"Let us take a look at the `train_data`","a6e83e78":"Convert the `Survived` column in the `train_data` frame to be categorical, indicating to the estimator that this is a classification problem. As for the other categorical features, H2O automatically takes care of them.","7a716666":"Read in the data as a [H2OFrame](https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-py\/docs\/frame.html), the primary data store for H2O. For examples of munging with H2O see the [Data Manipulation](https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-munging.html) page.","4ac1f5c1":"![](https:\/\/raw.githubusercontent.com\/Carl-McBride-Ellis\/images_for_kaggle\/main\/H2O_ai_logo.png)\n# H2O.ai Gradient boosting classifier\nIn this notebook we shall be using the Gradient boosting classifier ([`H2OGradientBoostingEstimator`](https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/gbm.html)) from [H2O.ai](https:\/\/www.h2o.ai\/)\n\nTo learn more about H2O.ai see:\n* [H2O.ai Overview](https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/index.html)\n* [H2O.ai Tutorials](https:\/\/docs.h2o.ai\/h2o-tutorials\/latest-stable\/index.html)\n\nFirstly, import `h2o` and start a local H2O server","69f6d0f2":"We shall be using the [H2O Gradient Boosting Machine](https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/gbm.html). Note that it is [extremely easy to overfit the Titanic dataset](https:\/\/www.kaggle.com\/carlmcbrideellis\/overfitting-and-underfitting-the-titanic), so we shall only use 15 trees, having a maximum depth of 4","87aa3cb3":"# Create a `submission.csv` for scoring by kaggle","03186cf7":"we shall now [combine](https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-munging\/combining-columns.html) our class predictions with the `test_data`","a6aaa827":"we are interested in the `p1` class, which we shall convert from a probability to a binary classification, here setting the [discrimination threshold](https:\/\/www.kaggle.com\/carlmcbrideellis\/discrimination-threshold-false-positive-negative) to be 0.5","386f051a":"We have finished, and shall now shut down our H2O instance","96d8f752":"Let us take a look at the predictions"}}