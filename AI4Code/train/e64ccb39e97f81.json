{"cell_type":{"50334b61":"code","b301e4dd":"code","9e111bac":"code","49e2f098":"code","1b337b0c":"code","ec1bb1a1":"code","e8ae2675":"code","1eb03070":"code","b5902167":"code","46ac4433":"code","20e6d1e0":"code","38eb076f":"markdown","ec1928ba":"markdown","63e8879a":"markdown","6bbf5ec8":"markdown","a5f12ea2":"markdown","9fa82e20":"markdown","ca94f823":"markdown","bc69428e":"markdown","dcc970a3":"markdown","f52ef8a5":"markdown"},"source":{"50334b61":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nimport wave\nimport pylab\nfrom pathlib import Path\nfrom scipy import signal\nfrom scipy.io import wavfile\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# Set paths to input and output data\nINPUT_DIR = '\/kaggle\/input\/free-spoken-digits\/free-spoken-digit-dataset-master\/recordings\/'\nOUTPUT_DIR = '\/kaggle\/working\/'\n\n# Print names of 10 WAV files from the input path\nparent_list = os.listdir(INPUT_DIR)\nfor i in range(10):\n    print(parent_list[i])","b301e4dd":"# Plot first 5 WAV files as a waveform and a frequency spectrum\nfor i in range(5): \n    signal_wave = wave.open(os.path.join(INPUT_DIR, parent_list[i]), 'r')\n    sample_rate = 16000\n    sig = np.frombuffer(signal_wave.readframes(sample_rate), dtype=np.int16)\n\n    plt.figure(figsize=(12,12))\n    plot_a = plt.subplot(211)\n    plot_a.set_title(parent_list[i])\n    plot_a.plot(sig)\n    plot_a.set_xlabel('sample rate * time')\n    plot_a.set_ylabel('energy')\n\n    plot_b = plt.subplot(212)\n    plot_b.specgram(sig, NFFT=1024, Fs=sample_rate, noverlap=900)\n    plot_b.set_xlabel('Time')\n    plot_b.set_ylabel('Frequency')\n\nplt.show()","9e111bac":"# Utility function to get sound and frame rate info\ndef get_wav_info(wav_file):\n    wav = wave.open(wav_file, 'r')\n    frames = wav.readframes(-1)\n    sound_info = pylab.frombuffer(frames, 'int16')\n    frame_rate = wav.getframerate()\n    wav.close()\n    return sound_info, frame_rate\n\n# For every recording, make a spectogram and save it as label_speaker_no.png\nif not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-images')):\n    os.mkdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n    \nfor filename in os.listdir(INPUT_DIR):\n    if \"wav\" in filename:\n        file_path = os.path.join(INPUT_DIR, filename)\n        file_stem = Path(file_path).stem\n        target_dir = f'class_{file_stem[0]}'\n        dist_dir = os.path.join(os.path.join(OUTPUT_DIR, 'audio-images'), target_dir)\n        file_dist_path = os.path.join(dist_dir, file_stem)\n        if not os.path.exists(file_dist_path + '.png'):\n            if not os.path.exists(dist_dir):\n                os.mkdir(dist_dir)\n            file_stem = Path(file_path).stem\n            sound_info, frame_rate = get_wav_info(file_path)\n            pylab.specgram(sound_info, Fs=frame_rate)\n            pylab.savefig(f'{file_dist_path}.png')\n            pylab.close()\n\n# Print the ten classes in our dataset\npath_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images'))\nprint(\"Classes: \\n\")\nfor i in range(10):\n    print(path_list[i])\n    \n# File names for class 1\npath_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images\/class_1'))\nprint(\"\\nA few example files: \\n\")\nfor i in range(10):\n    print(path_list[i])","49e2f098":"# Declare constants\nIMAGE_HEIGHT = 256\nIMAGE_WIDTH = 256\nBATCH_SIZE = 32\nN_CHANNELS = 3\nN_CLASSES = 10\n\n# Make a dataset containing the training spectrograms\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n                                             batch_size=BATCH_SIZE,\n                                             validation_split=0.2,\n                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n                                             shuffle=True,\n                                             color_mode='rgb',\n                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n                                             subset=\"training\",\n                                             seed=0)\n\n# Make a dataset containing the validation spectrogram\nvalid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n                                             batch_size=BATCH_SIZE,\n                                             validation_split=0.2,\n                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n                                             shuffle=True,\n                                             color_mode='rgb',\n                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n                                             subset=\"validation\",\n                                             seed=0)","1b337b0c":"plt.figure(figsize=(12, 12))\nfor images, labels in train_dataset.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")\nplt.show()","ec1bb1a1":"# Function to prepare our datasets for modelling\ndef prepare(ds, augment=False):\n    # Define our one transformation\n    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1.\/255)])\n    flip_and_rotate = tf.keras.Sequential([\n        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n    ])\n    \n    # Apply rescale to both datasets and augmentation only to training\n    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n    return ds\n\ntrain_dataset = prepare(train_dataset, augment=False)\nvalid_dataset = prepare(valid_dataset, augment=False)","e8ae2675":"# Create CNN model\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\nmodel.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n\n# Compile model\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.RMSprop(),\n    metrics=['accuracy'],\n)\n\n# Train model for 10 epochs, capture the history\nhistory = model.fit(train_dataset, epochs=10, validation_data=valid_dataset)","1eb03070":"# Plot the loss curves for training and validation.\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values)+1)\n\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b5902167":"# Plot the accuracy curves for training and validation.\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nepochs = range(1, len(acc_values)+1)\n\nplt.figure(figsize=(8,6))\nplt.plot(epochs, acc_values, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","46ac4433":"# Compute the final loss and accuracy\nfinal_loss, final_acc = model.evaluate(valid_dataset, verbose=0)\nprint(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))","20e6d1e0":"# Clean the output dir\nimport shutil\nshutil.rmtree('\/kaggle\/working\/audio-images')","38eb076f":"![](https:\/\/i.imgur.com\/OX1ADxu.png)\n\n# Introduction\n\nWith the advances in deep learning and neural networks, it is now possible to create simple yet powerful networks able to perform classification, forecasting and object detection tasks that were previously highly complex and sometimes even infeasible. In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They're very powerful at recognizing and classifiying objects and patterns in images when presented with suitable data and a very specific task. However, it turns out at these CNN's are not limited to problems originiating in computer vision, but can also be applied to non-image problems. For instance, a sound can be converted to a spectogram, which is a chart that shows the amount of each frequency at each time in an audio file. It has  been shown that this approach can beat state-of-the-art environmental sound detection models. In this notebook, we'll use CNN's to classify spoken words from the Audio MNIST dataset. The dataset contains 3000 audio files, where a human pronounces one of ten digits (0-9).\n\nKey takeaways from this notebook:\n\n* We convert 3000 MNIST audio files to spectograms.\n* We use Tensorflow's API to create a CNN with multiple 2D convolution layers.\n* The network prove excellent at classifying the spectograms in 10 distinct classes. \n* Final validation accuracy is about 95%.\n\nFirst load the data, then print example files and then plot WAV files.","ec1928ba":"Let's quickly visualize a few of the generated spectograms and their labels.","63e8879a":"# Convert audio files to spectograms\n\nWe can now do the actual conversion of every audio sample. The code below uses the wave and pylab library to extract sound information and frame rate from each sample and save the result as a spectrogram. After this we've simply converted our audio problem to a image problem.","6bbf5ec8":"# Preparing the data\n\nWe can now load the spectrograms into memory. We use the image_dataset_from_directory utility to generate the datasets, and we use Keras image preprocessing layers for image standardization and data augmentation. The validation set is what will ultimately be our benchmark when becomes to performance and accuracy of our classifier. The batch size is set fairly low for now (32) for all images to fit in memory. The seed is for reproducibility.\n\nMore information: https:\/\/keras.io\/examples\/vision\/image_classification_from_scratch\/","a5f12ea2":"We can compute the final loss and accuracy score on our valid dataset using the evaluate() function.","9fa82e20":"# Modelling\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*ytBUCmhkAucJ5imsNfAyfQ.png)\n\nKeras is one of the most popular deep learning libraries in Python for research and development because of its simplicity and ease of use. It uses the Tensorflow backend to build both shallow and deep models without much hazzle. Since there's a lot of data available here, my belief was that neural network were suiteable. We'll go through why the settings and hyperparameters are set the way they are. See more at https:\/\/keras.io\/api\/\n\nHere we make a neural network using Conv2D and MaxPooling2D layers to downsample the input images into smaller convolutions, which can be seen as a window of the input image. Combining multiple of these convolutions we are able to capture important features in the image such as edges, contours and colors. By iterating over batches of input images and associated labels, we can assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other.\n\nMore here: https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53","ca94f823":"# Evaluation\n\nTo evaluate our CNN, we'll look at the loss and accuracy scores to see how well training's progressed and check if there's any underfit\/overfit. Evaluations are based on both the training and the validation set. It seems that after 8 epochs the loss and accuracy for the validation set plateaus just around the 0.10 and 0.95 mark, respectively.","bc69428e":"After 10 epochs, the training accuracy reaches 99% and validation accuracy reaches 95%.","dcc970a3":"Before we can build our model and start training, we need to apply one simple augmentation the dataset and that is rescaling. We rescale an input in the (0, 255) range to be in the (0,1) range.","f52ef8a5":"You can clearly see the difference in the energy distribution when different words are being pronounced. These distinct characteristics also show up in the spectograms of the recordings, which will enable us transform what was originally an audio problem to an image problem."}}