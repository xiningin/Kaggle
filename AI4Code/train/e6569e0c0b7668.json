{"cell_type":{"4d8b9488":"code","ad979298":"code","5c909ede":"code","ae8d5d95":"code","cdcaab5e":"code","fb16fef0":"code","392220b2":"code","dea34ad2":"code","7e96a5db":"code","fe298ff8":"code","663ef87f":"code","0ef0221e":"code","5647d27d":"code","91231354":"code","70947b18":"code","5b569006":"code","5c4cb8cd":"code","08070b9d":"code","fc15a6aa":"code","4c63e781":"code","ef36e12f":"code","b57378e6":"code","d2debeb2":"code","4c94a15c":"code","c0c30845":"code","600edc16":"code","c1db1645":"code","5e406569":"code","c749e78a":"code","43066d48":"code","ecfe6606":"code","f7c53442":"code","7624fa26":"code","ddcb84e5":"code","a488e10b":"code","d0bb29c1":"code","2d2afeca":"markdown","67f21541":"markdown","38c0a23f":"markdown","d8d99ccc":"markdown","8dee7df5":"markdown","3762cea9":"markdown","bcd7e11a":"markdown","deba25f5":"markdown"},"source":{"4d8b9488":"# Import libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import utils\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn('DelftStack')\nwarnings.warn('Do not show this message')","ad979298":"#Import dataset\ndd = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndd.head()","5c909ede":"# convert 0 values to meaning for all cloumns except 'Outcome', and 'Pregnancies'\nfor col in dd.drop(['Outcome', 'Pregnancies'], axis=1):\n    val = dd[col].mean()\n    dd[col] = dd[col].replace(0, val)","ae8d5d95":"dd.head()","cdcaab5e":"dd.dtypes","fb16fef0":"#check for missing values\ndd.isnull().sum()","392220b2":"# outcome distribution\noutcome = dd['Outcome'].value_counts().to_frame()\nprint(outcome)\noutcome['Outcome'].plot(kind='pie',\n                            figsize=(15, 6),\n                            autopct='%1.1f%%', \n                            startangle=90, \n                            labels=None,         \n                            pctdistance=1.12,    \n                            )\n\nplt.title('Outcome Distribution', y=1.2) \nplt.axis('equal') \nplt.legend(labels=outcome.index, loc='upper left') \nplt.show()","dea34ad2":"group_names = ['20\\'s', '30\\'s', '40\\'s', '50\\'s', '60\\'s', '70\\'s']\ndd['AgeBD'] = pd.cut(dd['Age'], bins=[20,29,39,49,59,69,79], labels=group_names, include_lowest=True )\n\ngroup_names_bmi = ['Underweight', 'Healthy', 'Overweight', 'Obese1','Obese2', 'Obese3']\ndd['BMIBD'] = pd.cut(dd['BMI'], bins=[0,18.5, 24.9, 29.9, 34.9, 39.9, 50 ], labels=group_names_bmi,\n                   include_lowest=True  )\n\n\ndd","7e96a5db":"print('Age Break Down')\nprint(dd.AgeBD.value_counts())\nplt.figure(figsize = (10,8))\npyplot.bar(group_names, dd[\"AgeBD\"].value_counts())\n\npyplot.xlabel(\"Age Group\")\npyplot.ylabel(\"count\")\npyplot.title(\"Age Break Down \")","fe298ff8":"bmibd = dd.BMIBD.value_counts().to_frame()\nbmibd.index\n\n\nprint('BMI Break Down')\nprint(dd.BMIBD.value_counts())\nplt.figure(figsize = (10,8))\npyplot.bar(bmibd.index, dd[\"BMIBD\"].value_counts())\n\npyplot.xlabel(\"BMI Group\")\npyplot.ylabel(\"count\")\npyplot.title(\"BMI Break Down \")","663ef87f":"# find the mean values based on outcomes \ngrouped = dd.groupby('Outcome').mean()\ngrouped","0ef0221e":"#plot corrlation using heatmap\nplt.figure(figsize=(20,10))\nsns.heatmap(dd.corr(),annot= True, linewidths=2, cmap = \"YlGnBu\")","5647d27d":"plt.figure(figsize = (10,8))\nsns.scatterplot(x = 'Glucose', y = 'BloodPressure', hue = \"Outcome\",\n                data = dd, alpha = 1)\nplt.ylim(40, )\nplt.xlim(60, )","91231354":"plt.figure(figsize = (10,8))\nsns.scatterplot(x = 'BMI', y = 'Age', hue = \"Outcome\",\n                data = dd, alpha = 1)\nplt.xlim(15, 60)\nplt.ylim(20,70)","70947b18":"plt.figure(figsize = (10,8))\nsns.scatterplot(x = 'Glucose', y = 'Age', hue = \"Outcome\",\n                data = dd, alpha = 1)\nplt.ylim(20,70)","5b569006":"plt.figure(figsize = (10,8))\nsns.scatterplot(x = 'Glucose', y = 'Age', hue = \"Outcome\",\n                data = dd, alpha = 1)\nplt.ylim(20,80)","5c4cb8cd":"print('Mean Value for Pregnancies',grouped['Pregnancies'])\nplt.figure(figsize=(10,5))\nsns.boxplot(y=dd['Pregnancies'],x=dd['Outcome'])\nplt.tight_layout()\nplt.show()","08070b9d":"print('Mean Value for Insulin',grouped['Insulin'])\nplt.figure(figsize=(10,5))\nsns.boxplot(y=dd['Insulin'],x=dd['Outcome'])\nplt.tight_layout()\nplt.show()","fc15a6aa":"plt.figure(figsize = (10,8))\nsns.scatterplot(x='Glucose', y= 'Insulin', hue = \"Outcome\", data=dd)\nplt.ylim(0,400)\nplt.xlim(50,200)","4c63e781":"plt.figure(figsize = (10,8))\nsns.scatterplot(x='BMI', y= 'Glucose', hue = \"Outcome\", data=dd)\nplt.ylim(50,200)\nplt.xlim(15,55)","ef36e12f":"# feature selection \nx = dd.drop(['Outcome','AgeBD', 'BMIBD'], axis=1)\ny = dd['Outcome']","b57378e6":"# feature scaling \nsc = StandardScaler()\nX = sc.fit_transform(x)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.15,random_state = 5)","d2debeb2":"X_test.shape","4c94a15c":"# Build model, compile model, train and validate model preformace\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(300, input_dim=8, activation='relu'),\n  tf.keras.layers.Dropout(.3),\n  tf.keras.layers.Dense(150, activation='sigmoid'),\n  tf.keras.layers.Dropout(.2),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhisto = model.fit(X_train, y_train, epochs=50, batch_size=64, verbose = 1, validation_data=(X_test, y_test))","c0c30845":"# graph model accuracy of training and testing subsets \nplt.figure(figsize=(15,5))\nplt.plot(histo.history['accuracy'])\nplt.plot(histo.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.ylim(.6,)\nplt.show()","600edc16":"# graph the loss of both training and testing subsets \nplt.figure(figsize=(15,5))\nplt.plot(histo.history['loss']) \nplt.plot(histo.history['val_loss']) \nplt.title('Model loss') \nplt.ylabel('Loss') \nplt.xlabel('Epoch') \nplt.legend(['Train', 'Test'], loc='upper left') \nplt.show()","c1db1645":"# Hyperparameter Tuning for classifier models \nmodel={\n    'svm':{ 'model':SVC(),'params':{ 'C':[0.1,1, 10, 100],'gamma': [1,0.1,0.01,0.001],\n                                    'kernel':['rbf', 'poly', 'sigmoid']}\n    },\n    'Randomforest':{ 'model':RandomForestClassifier(),'params':{'n_estimators':range(1,50),\n                                                                'criterion':['gini','entropy']}\n    },\n    'Logistic':{'model':LogisticRegression(),'params':{ 'C': range(1,10)}\n    },\n    'decision_tree': {'model': DecisionTreeClassifier(),'params': { 'n_neighbors': range(1,50),\n                                                                   'criterion': ['gini','entropy']}\n    },\n    'KNN':{'model':KNeighborsClassifier(),'params':{'n_neighbors' : range(1,50)}\n    }\n}","5e406569":"scores = []\nfor model_name, mp in model.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], return_train_score=False)","c749e78a":"clf.fit(X_train, y_train)","43066d48":"scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n})\n    \nmodel_pref = pd.DataFrame(scores,columns=['model','best_params','best_score'])","ecfe6606":"# best model for hyperparameter tuning given the parameters above, \nmodel_pref","f7c53442":"reg = LogisticRegression()\nreg.fit(X_train, y_train)   \ny_pred=reg.predict(X_test)\nprint(accuracy_score(y_test,y_pred)*100)","7624fa26":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_ped2 = knn.predict(X_test)\nprint(accuracy_score(y_test,y_ped2)*100)","ddcb84e5":"svm = SVC()\nsvm.fit(X_train, y_train)\ny_ped3 = svm.predict(X_test)\nprint(accuracy_score(y_test,y_ped3)*100)","a488e10b":"rfd = RandomForestClassifier()\nrfd.fit(X_train, y_train)\ny_ped4 = rfd.predict(X_test)\nprint(accuracy_score(y_test,y_ped4)*100)","d0bb29c1":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ny_ped5 = dtc.predict(X_test)\nprint(accuracy_score(y_test,y_ped5)*100)","2d2afeca":"Finding: \n1. Higher glucose levels regardless of age increases chance of developing diabetes. \n2. Maintaining glucose levels below 100 can be a good preventative measure one can take to decrease chance of developing diabetes.","67f21541":"Findings: \n\n1. Age by it self isn't the best indicator of diabetes. \n2. BMI is a strong indicator of diabetes in comparison to age. \n3. Also someone with an BMI of 25 to 30 and is above age 40 can still develop diabetes. ","38c0a23f":"Now lets look and compare models without hyperparameter tuning.","d8d99ccc":"Finding \n\n1. There seem to be a positive correlation between insilin and glucose.\n2. When both insilin and glucose levels are high that shows a strong indication of diabetes.  ","8dee7df5":"Finding \n1. BMI seems to be not a reliable indicator of diabetes. Reason being Body mass index (BMI) is a measurement of your overall body mass but isn't a measurement of what the mass consist of. \n2. On the other hands glucose levels does seem to be a reliable indicator of diabetes.","3762cea9":"Finding:\n\nIn outcome 1 where person has Diabetes, the mean value of pregnancy is 4.865 and has a larger box plot. Indicating having more pregnancies can increase chance of developing diabetes. \n","bcd7e11a":"After looking at the models above you might ask yourself why does the classifier models without hyperparameter \ntuning perform better than the models with parameter tuning?\n\nThe simple answer is the parameters you give to the GridSearchCV. GridSearchCV loops through predefined parameters and fit your estimators (models) on your training set. So, in the end, you select the best parameters from the predefined parameters you set.","deba25f5":"Findings:\n1. Higher glucose levels can be a good indicator of diabetes.\n2. High bloodpressure levels and High glucose levels can be a good indicator of diabetes together. \n3. High or low bloodpressure level with a glucose level below 100 can be a good indicator of not having diabetes. "}}