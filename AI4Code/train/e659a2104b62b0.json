{"cell_type":{"4cfd4650":"code","19430048":"code","362a1c18":"code","5c0c0dfa":"code","2aa43d05":"code","736525d2":"code","9c6a8a71":"code","f1a8dc76":"code","df7af3bd":"code","f1f946e4":"code","10d7d29f":"code","d6c3fa3d":"code","e3719d2c":"code","b31587eb":"code","83b5703d":"code","a15be7ee":"code","ba61ffa9":"code","0a112ed6":"code","63d765f3":"code","1b5a2fcb":"code","b24a1b4c":"code","698ea7c3":"code","b1e81482":"code","4110ccce":"code","18a99f0b":"code","29ada593":"code","a609cd05":"code","34b3fda1":"code","c998b6cb":"code","858097c0":"code","b38000a9":"code","a6acd0f4":"code","dbff8f8a":"code","fe971bec":"code","5f6aa93f":"code","7e8300c0":"code","91f6300c":"code","84b7e936":"code","809afbbb":"code","bf17fe84":"code","87dc05eb":"code","9a1328fe":"code","f9b8a535":"code","80119267":"code","5151038d":"code","39e4ec4f":"markdown","261a12ec":"markdown","db54489e":"markdown","8c5b041a":"markdown","43e4c8ea":"markdown","119eec62":"markdown","6e29bc61":"markdown","6ed41491":"markdown","90e54727":"markdown","085156f7":"markdown","375c3468":"markdown","2512c56e":"markdown","1439350a":"markdown","6c50d435":"markdown","b822991f":"markdown"},"source":{"4cfd4650":"import os\nimport re\nimport nltk\nimport string\nimport spacy\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom numpy.random import seed\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout \nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.random import set_seed\n\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\npd.set_option('display.max_columns', None)","19430048":"!python -m spacy download en_vectors_web_lg\n!python -m spacy link en_vectors_web_lg en_vectors_web_lg_link","362a1c18":"nlp = spacy.load('en_vectors_web_lg_link') #('en_core_web_sm')","5c0c0dfa":"data = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv', encoding ='windows-1251')[['v1', 'v2']]\ndata.columns = ['label', 'msg']\ndata.head()","2aa43d05":"df_else, validation_df  = train_test_split(data,\n                                test_size=0.25,\n                                random_state = 101)","736525d2":"df_train, df_test  = train_test_split(df_else,\n                                test_size=0.25,\n                                random_state = 101)","9c6a8a71":"df_train","f1a8dc76":"def del_punct(text):\n    chars = []\n    for char in text:\n        if char not in string.punctuation:\n            chars.append(char)\n        else:\n            chars.append(' ')\n    return ''.join(chars)\n\ndef text_preparation(text: str) -> str:\n    text = text.lower()\n    text = del_punct(text)\n    doc = nlp(text)\n    text = ' '.join([\n            token.lemma_ \n                for token in doc \n                if token.text not in nlp.Defaults.stop_words \n        ])\n    \n    text = re.sub(r'\\d+', ' somenumbers ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text\n     \ndef processing(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df['msg_len'] = df['msg'].apply(len)\n    df['msg'] = df['msg'].apply(text_preparation)\n    return df\n\ndf_train_p = processing(df_train)\ndf_test_p = processing(df_test)\ndf_train_p.head()","df7af3bd":"datasets = {\n    'train'        : df_train, \n    'test'         : df_test, \n    'train + test' : df_else, \n    'validation'   : validation_df\n}\n\nfor dataset_name, dataset in datasets.items():\n    print('\\n' + dataset_name + ':')\n    display(pd.DataFrame(dataset['label'].value_counts()))","f1f946e4":"g = sns.FacetGrid(df_train_p, hue='label', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'msg_len')\nplt.legend()\nplt.show()","10d7d29f":"def text_for_cloud(label):\n    text = ' '.join(df_train_p['msg'][df_train_p['label'] == label].to_list())\n    text = text.replace('somenumbers', '')\n    return text\n\nfor label in ['spam', 'ham']:\n    wordcloud = WordCloud(\n        max_font_size=500,\n        max_words=100,\n        background_color=\"white\"\n    ).generate(text_for_cloud(label))\n\n    plt.figure(figsize=(12, 8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(f'{label.capitalize()} messages wordcloud', fontsize=20)\n    plt.show()","d6c3fa3d":"bow_transformer = CountVectorizer(max_features = 1500).fit(df_train_p['msg'])\nbow_train = bow_transformer.transform(df_train_p['msg'])\nbow_train.shape","e3719d2c":"tfidf_transformer = TfidfTransformer().fit(bow_train)\ntrain_tfidf = tfidf_transformer.transform(bow_train)\n\nbow_test = bow_transformer.transform(df_test_p['msg'])\ntest_tfidf = tfidf_transformer.transform(bow_test)","b31587eb":"X_train = pd.DataFrame.sparse.from_spmatrix(train_tfidf)\nX_train.columns = bow_transformer.get_feature_names()\nX_train['msg_len'] = df_train_p['msg_len'].values\n\nX_test = pd.DataFrame.sparse.from_spmatrix(test_tfidf) \nX_test.columns = bow_transformer.get_feature_names()\nX_test['msg_len'] = df_test_p['msg_len'].values\n\ny_train = df_train_p['label']\ny_test = df_test_p['label']","83b5703d":"scaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_sc = scaler.transform(X_train.values)\nX_test_sc = scaler.transform(X_test.values)","a15be7ee":"input_width = len(X_train.columns)\ninput_width","ba61ffa9":"def dim_red_analysis(n_epochs):\n    seed(101)\n    set_seed(101)\n\n    encoder = Sequential()\n    encoder.add(Dense(units = 256, activation = 'relu', input_shape = [input_width]))\n    encoder.add(Dropout(0.2))\n    encoder.add(Dense(units = 16, activation = 'relu'))\n    encoder.add(Dense(units = 2, activation = 'relu'))\n\n    decoder = Sequential()\n    decoder.add(Dense(units = 16, activation = 'relu', input_shape = [2]))\n    decoder.add(Dense(units = 256, activation = 'relu'))\n    decoder.add(Dense(units = input_width, activation = 'relu'))\n\n    autoencoder = Sequential([encoder, decoder])\n\n    autoencoder.compile(loss = 'mse', optimizer = SGD(lr = 10))\n    \n    autoencoder.summary()\n\n    autoencoder.fit(\n        X_train_sc, \n        X_train_sc, \n        epochs = n_epochs,\n        validation_data=(X_test, X_test)\n    )\n    \n    if n_epochs > 1:\n        histo = pd.DataFrame(autoencoder.history.history)\n        for metric in ['loss', 'val_loss']:\n            plt.title(metric)\n            histo[metric].plot()\n            plt.show()\n        \n    encoded_2dim = encoder.predict(X_train_sc)\n    encoded_2dim = pd.DataFrame(encoded_2dim)\n    encoded_2dim['y'] = df_train['label'].values\n\n    plt.figure(figsize = (12, 8))\n    sns.scatterplot(data = encoded_2dim, x = 0, y = 1, hue = 'y', palette = 'magma')\n    plt.show()","0a112ed6":"dim_red_analysis(0)","63d765f3":"dim_red_analysis(1)","1b5a2fcb":"dim_red_analysis(3)","b24a1b4c":"dim_red_analysis(83)","698ea7c3":"def eval_result(model, X_test, y_test):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        pred = model.predict(X_test)\n        print(classification_report(y_test, pred, target_names = ['Ham', 'Spam']))\n        display(pd.DataFrame(confusion_matrix(y_test, pred), \n                         columns = ['Predicted Ham', 'Predicted Spam'],\n                         index = ['Ham', 'Spam']))\n        \n        print(f'Accuracy: {round(accuracy_score(y_test, pred), 5)}')\n        if hasattr(model, 'feature_importances_'):\n            features = pd.DataFrame({\n                'Variable'  :X_test.columns,\n                'Importance':model.feature_importances_\n            })\n            features.sort_values('Importance', ascending=False, inplace=True)\n            display(features.head(20))","b1e81482":"nb_model = MultinomialNB()\nnb_model.fit(X_train, y_train)\neval_result(nb_model, X_test, y_test)","4110ccce":"dtc = DecisionTreeClassifier(random_state = 1)\ndtc.fit(X_train, y_train)\neval_result(dtc, X_test, y_test)","18a99f0b":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train, y_train)\neval_result(rfc, X_test, y_test)","29ada593":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(X_train, y_train)\neval_result(gbc, X_test, y_test)","a609cd05":"X_train_v = pd.DataFrame([nlp(msg).vector for msg in df_train['msg']])\nX_test_v = pd.DataFrame([nlp(msg).vector for msg in df_test['msg']])","34b3fda1":"X_train_v.shape","c998b6cb":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train_v, y_train)\neval_result(rfc, X_test_v, y_test)","858097c0":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(X_train_v, y_train)\neval_result(gbc, X_test_v, y_test)","b38000a9":"sid = SentimentIntensityAnalyzer()","a6acd0f4":"X_train_sa = pd.DataFrame([sid.polarity_scores(msg) for msg in df_train['msg']])\nX_test_sa = pd.DataFrame([sid.polarity_scores(msg) for msg in df_test['msg']])\nX_train_sa","dbff8f8a":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train_sa, y_train)\neval_result(rfc, X_test_sa, y_test)","fe971bec":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(X_train_sa, y_train)\neval_result(gbc, X_test_sa, y_test)","5f6aa93f":"LDA = LatentDirichletAllocation(n_components=12,random_state=1)\nLDA.fit(bow_train)","7e8300c0":"for index,topic in enumerate(LDA.components_):\n    print(f'The top 10 words for topic #{index}')\n    print([bow_transformer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n    print('\\n')","91f6300c":"topic_results_train = pd.DataFrame(LDA.transform(bow_train))\ntopic_results_test = pd.DataFrame(LDA.transform(bow_test))\nprint(f'train shape: {topic_results_train.shape}, test shape: {topic_results_test.shape}')","84b7e936":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(topic_results_train, y_train)\neval_result(rfc, topic_results_test, y_test)","809afbbb":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(topic_results_train, y_train)\neval_result(gbc, topic_results_test, y_test)","bf17fe84":"X_train_full = pd.concat([X_train, X_train_v, X_train_sa], axis=1)\nX_test_full = pd.concat([X_test, X_test_v, X_test_sa], axis=1)","87dc05eb":"X_train_full.shape","9a1328fe":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train_full, y_train)\neval_result(rfc, X_test_full, y_test)","f9b8a535":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(X_train_full, y_train)\neval_result(gbc, X_test_full, y_test)","80119267":"df_val_p = processing(validation_df)\n\nbow_val = bow_transformer.transform(df_val_p['msg'])\nval_tfidf = tfidf_transformer.transform(bow_val)\n\nX_val = pd.DataFrame.sparse.from_spmatrix(val_tfidf)\nX_val['msg_len'] = df_val_p['msg_len'].values\n\ny_val = df_val_p['label']\n\nX_val_v = pd.DataFrame([nlp(msg).vector for msg in validation_df['msg']])\nX_val_sa = pd.DataFrame([sid.polarity_scores(msg) for msg in validation_df['msg']])\nX_val_full = pd.concat([X_val, X_val_v, X_val_sa], axis=1)\nX_val_full.shape","5151038d":"eval_result(gbc, X_val_full, y_val)","39e4ec4f":"# Modelling with TF-IDF","261a12ec":"# Dimensionality reduction using autoencoder","db54489e":"# Scaling","8c5b041a":"# Sentiment Analysis","43e4c8ea":"We can see, that classes are clearly separable with just small overlapping","119eec62":"# Validation","6e29bc61":"Didn't use topic modelling, because it worsens the result. The model of choice is GBC.","6ed41491":"# Word2Vec + TF-IDF + Sentiment data","90e54727":"# Data preparation ","085156f7":"Spam messages tend to be longer","375c3468":"# Creating Bag of Words (lemmatized)","2512c56e":"# Topic modelling","1439350a":"# Splits","6c50d435":"# EDA","b822991f":"# Word2Vec"}}