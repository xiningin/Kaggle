{"cell_type":{"ea11457c":"code","3a259e45":"code","6ea2fa1a":"code","c8354d4f":"code","1e668d95":"code","c86ec537":"code","4b22e1fb":"code","c3efeecf":"code","bc0dd480":"code","985637e2":"code","3621ddd4":"code","a6c84a22":"code","4394093c":"code","2eb62488":"code","d7f7677e":"code","3ac29b1f":"code","2f2f79aa":"code","ebd29f38":"code","620c3a03":"code","29ffdf67":"code","59985a96":"code","8f48651d":"code","ff72bfc1":"code","68c52fac":"code","2c17e133":"code","b8644239":"code","ee695db4":"code","d8449b7d":"code","99c760ba":"code","58020904":"code","0e3285af":"code","cbd40a69":"code","5c0ea44f":"code","c3c61b98":"code","45aacd9d":"code","ac997e57":"code","1059fbed":"code","2d9cfa36":"code","563709bd":"code","cbd24e4e":"code","79344000":"code","a4f2e80a":"code","ff93dfba":"code","be2fb067":"code","f7424e48":"code","9d565280":"code","c7f2904e":"code","46afaac2":"code","1ef4d645":"code","2a919955":"code","b55f83db":"code","4f0cc9d8":"code","0044f467":"code","ca580b1a":"code","f5d48881":"code","602fbe5e":"code","76cdded9":"code","8acb1254":"code","049eb3c0":"code","66a83601":"markdown"},"source":{"ea11457c":"# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","3a259e45":"# Import train & test data \ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv') # example of what a submission should look like","6ea2fa1a":"# View the training data\ntrain.head(15)","c8354d4f":"train.Age.plot.hist()","1e668d95":"# View the test data (same columns as the training data)\ntest.head() # head = view first 5 lines","c86ec537":"# View the example submisison dataframe\ngender_submission.head()","4b22e1fb":"train.describe()","c3efeecf":"# Plot graphic of missing values\nmissingno.matrix(train, figsize = (30,10))","bc0dd480":"# Alternatively, you can see the number of missing values like this\ntrain.isnull().sum()","985637e2":"df_bin = pd.DataFrame() # for discretised continuous variables\ndf_con = pd.DataFrame() # for continuous variables","3621ddd4":"# How many people survived?\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived', data=train);\nprint(train.Survived.value_counts())","a6c84a22":"# Let's add this to our subset dataframes\ndf_bin['Survived'] = train['Survived']\ndf_con['Survived'] = train['Survived']","4394093c":"\nsns.distplot(train.Pclass)","2eb62488":"# How many missing variables does Pclass have?\ntrain.Pclass.isnull().sum()","d7f7677e":"df_bin['Pclass'] = train['Pclass']\ndf_con['Pclass'] = train['Pclass']","3ac29b1f":"# add Sex to the subset dataframes\ndf_bin['Sex'] = train['Sex']\ndf_bin['Sex'] = np.where(df_bin['Sex'] == 'female', 1, 0) # change sex to 0 for male and 1 for female\n\ndf_con['Sex'] = train['Sex']","2f2f79aa":"def plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    \"\"\"\n    Function to plot counts and distributions of a label variable and \n    target variable side by side.\n    ::param_data:: = target dataframe\n    ::param_bin_df:: = binned dataframe for countplot\n    ::param_label_column:: = binary labelled column\n    ::param_target_column:: = column you want to view counts and distributions\n    ::param_figsize:: = size of figure (width, height)\n    ::param_use_bin_df:: = whether or not to use the bin_df, default False\n    \"\"\"\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});","ebd29f38":"# Add SibSp to subset dataframes\ndf_bin['SibSp'] = train['SibSp']\ndf_con['SibSp'] = train['SibSp']","620c3a03":" #Visualise the counts of SibSp and the distribution of the values\n# against Survived\nplot_count_dist(train, \n                bin_df=df_bin, \n                label_column='Survived', \n                target_column='SibSp', \n                figsize=(20, 10))","29ffdf67":"# Add Parch to subset dataframes\ndf_bin['Parch'] = train['Parch']\ndf_con['Parch'] = train['Parch']","59985a96":"# Visualise the counts of Parch and the distribution of the values\n# against Survived\nplot_count_dist(train, \n                bin_df=df_bin,\n                label_column='Survived', \n                target_column='Parch', \n                figsize=(20, 10))","8f48651d":"# Add Fare to sub dataframes\ndf_con['Fare'] = train['Fare'] \ndf_bin['Fare'] = pd.cut(train['Fare'], bins=5) # discretised","ff72bfc1":"# Add Embarked to sub dataframes\ndf_bin['Embarked'] = train['Embarked']\ndf_con['Embarked'] = train['Embarked']","68c52fac":"# Remove Embarked rows which are missing values\nprint(len(df_con))\ndf_con = df_con.dropna(subset=['Embarked'])\ndf_bin = df_bin.dropna(subset=['Embarked'])\nprint(len(df_con))","2c17e133":"# One-hot encode binned variables\none_hot_cols = df_bin.columns.tolist()\none_hot_cols.remove('Survived')\ndf_bin_enc = pd.get_dummies(df_bin, columns=one_hot_cols)\n\ndf_bin_enc.head()","b8644239":"# One hot encode the categorical columns\ndf_embarked_one_hot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_one_hot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_plcass_one_hot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')","ee695db4":"# Combine the one hot encoded columns with df_con_enc\ndf_con_enc = pd.concat([df_con, \n                        df_embarked_one_hot, \n                        df_sex_one_hot, \n                        df_plcass_one_hot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","d8449b7d":" #Seclect the dataframe we want to use first for predictions\nselected_df = df_con_enc","99c760ba":"# Split the dataframe into data and labels\nX_train = selected_df.drop('Survived', axis=1) # data\ny_train = selected_df.Survived # labels","58020904":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","0e3285af":"# Logistic Regression\nstart_time = time.time()\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))","cbd40a69":"# k-Nearest Neighbours\nstart_time = time.time()\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\nknn_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))","5c0ea44f":"# Gaussian Naive Bayes\nstart_time = time.time()\ntrain_pred_gaussian, acc_gaussian, acc_cv_gaussian = fit_ml_algo(GaussianNB(), \n                                                                      X_train, \n                                                                      y_train, \n                                                                           10)\ngaussian_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gaussian)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gaussian_time))","c3c61b98":"# Linear SVC\nstart_time = time.time()\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\nlinear_svc_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=linear_svc_time))","45aacd9d":"# Stochastic Gradient Descent\nstart_time = time.time()\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\nsgd_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=sgd_time))","ac997e57":"# Decision Tree Classifier\nstart_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                X_train, \n                                                                y_train,\n                                                                10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","1059fbed":"# Gradient Boosting Trees\nstart_time = time.time()\ntrain_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                       X_train, \n                                                                       y_train,\n                                                                       10)\ngbt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))","2d9cfa36":"# Define the categorical features for the CatBoost model\ncat_features = np.where(X_train.dtypes != np.float)[0]\ncat_features","563709bd":"# Use the CatBoost Pool() function to pool together the training data and categorical feature labels\ntrain_pool = Pool(X_train, \n                  y_train,\n                  cat_features)","cbd24e4e":"# CatBoost model definition\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=['Accuracy'],\n                                    loss_function='Logloss')\n\n# Fit CatBoost model\ncatboost_model.fit(train_pool,\n                   plot=True)\n\n# CatBoost accuracy\nacc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)","79344000":"# How long will this take?\nstart_time = time.time()\n\n# Set params for cross-validation as same as initial model\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=True)\n\n# How long did it take?\ncatboost_time = (time.time() - start_time)\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)","a4f2e80a":"# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(\"Accuracy: {}\".format(acc_catboost))\nprint(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))\nprint(\"Running Time: {}\".format(datetime.timedelta(seconds=catboost_time)))","ff93dfba":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_knn, \n        acc_log,  \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt,\n        acc_catboost\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","be2fb067":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,      \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt,\n        acc_cv_catboost\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","f7424e48":"# Feature Importance\ndef feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp\n    #plt.savefig('catboost_feature_importance.png')","9d565280":"# Plot the feature importance scores\nfeature_importance(catboost_model, X_train)","c7f2904e":"metrics = ['Precision', 'Recall', 'F1', 'AUC']\n\neval_metrics = catboost_model.eval_metrics(train_pool,\n                                           metrics=metrics,\n                                           plot=True)\n\nfor metric in metrics:\n    print(str(metric)+\": {}\".format(np.mean(eval_metrics[metric])))","46afaac2":"# One hot encode the columns in the test data frame (like X_train)\ntest_embarked_one_hot = pd.get_dummies(test['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","1ef4d645":"# Combine the test one hot encoded columns with test\ntest = pd.concat([test, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","2a919955":"# Create a list of columns to be used for the predictions\nwanted_test_columns = X_train.columns\nwanted_test_columns","b55f83db":"# Make a prediction using the CatBoost model on the wanted columns\npredictions = catboost_model.predict(test[wanted_test_columns])","4f0cc9d8":"# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","0044f467":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions # our model predictions on the test dataset\nsubmission.head()","ca580b1a":"# What does our submission have to look like?\ngender_submission.head()","f5d48881":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['Survived'] = submission['Survived'].astype(int)\nprint('Converted Survived column to integers.')","602fbe5e":"# How does our submission dataframe look?\nsubmission.head()","76cdded9":"# Are our test and submission dataframes the same length?\nif len(submission) == len(test):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","8acb1254":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmission.to_csv('..\/catboost_submission.csv', index=False)\nprint('Submission CSV is ready!')","049eb3c0":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"..\/catboost_submission.csv\")\nsubmissions_check","66a83601":"\n\n**Can we predict who would've survived the Titanic?**\n\nThis notebook goes through a basic exploratory data analysis of the Kaggle Titanic dataset with Python\nAlthough this notebook works towards creating a Kaggle submission, it should not be taken as exhaustive list of things to do with a dataset. It has been setup as an introduction to get you started with exploratory data analysis (EDA).\n\nThere are challenges and extensions listed throughout. I encourage you to take the foundations here and build upon them.\n\nIf you complete the challenges or get a better score than the one listed at the bottom of this notebook, tweet me and I'llgive your work a shout out.\n\nIf you're interested, there's a video walkthrough of this notebook available on my YouTube: https:\/\/youtu.be\/f1y9wDDxWnA\n\nKeep learning,\n\nDaniel\n\nStep 0: Why EDA?\nNot all data comes in a neat little package ready to be modelled by the latest and greatest machine learning models.\n\nMost of the time, you'll get a dataset you don't know much about. So before you can make any solid predictions, you'll to find out more.\n\nThis is where EDA comes in.\n\nThe main thing to remember is the first word. Exploratory. You're trying to figure out more about the data so you can build a model the best way you can.\n\nYou'll usually do this when you first look at a dataset but it'll continually happen as you learn more. EDA is an iterative process. There's no one way to do it either. It'll vary with each new dataset but there are some things you'll find yourself doing more often, we'll talk about those in this notebook and in a future blog post.\n\nStep 1: Download the data\nYou can download the data for this notebook here: https:\/\/www.kaggle.com\/c\/titanic\/data\n\nStep 2: Work through the notebook\nFirst we will import all the relevant dependencies we need.\nIf you don't have any of these, the notebook will throw an error. The error will likely tell you what you don't have. Then you'll have to install it.\n\nYou can usually figure out how to install it by Googling: \"how to install [the thing you don't have]\".\n\nPS If you have any questions, feedback, advice or bug fixes, please let me know."}}