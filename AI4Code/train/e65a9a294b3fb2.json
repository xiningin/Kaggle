{"cell_type":{"79b9cfe5":"code","809a9cf7":"code","a947dd95":"code","2bfbef01":"code","ee98b787":"code","c6a1986e":"code","6bc2d1aa":"code","51ce3f53":"code","8fc3be73":"code","d2d22fca":"code","36357249":"code","55989c4e":"code","d7507873":"code","3cdd6e66":"code","22c3079e":"code","7b734aa8":"code","7bd478d4":"code","efe1217e":"code","51cea9e7":"code","b4068547":"code","dabce8fc":"code","f4c35e36":"code","d7ac8631":"code","1abb3897":"code","3890cdae":"code","fb8443c2":"code","33b95dad":"code","15da5270":"code","1792bc34":"code","7e28d965":"code","f09dec87":"code","e4c87640":"code","eec57d64":"code","57726ea2":"code","411d026e":"code","6e705c0c":"code","4facba83":"code","fac4e9e3":"markdown","650bff04":"markdown","e7535d72":"markdown","013cab6d":"markdown","28930f7c":"markdown","7ec4312e":"markdown","b182aebf":"markdown","e62c3ba7":"markdown","afdbe189":"markdown","bf8a0436":"markdown","bffbc32c":"markdown","37680807":"markdown","54b6e4fc":"markdown","c4588c1d":"markdown","2f6f7686":"markdown","32d695ea":"markdown","773d4d2a":"markdown","dbb9380d":"markdown","b88464c0":"markdown","59fdda4e":"markdown","3662eaa5":"markdown","71b6ab1f":"markdown","2ca1138d":"markdown","a01bf715":"markdown"},"source":{"79b9cfe5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","809a9cf7":"data = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines=True)\ny_data = data.is_sarcastic.values\ntitles = data.headline.values\ndata.head()","a947dd95":"from nltk import word_tokenize\n\ntitles_tokenized = []\nfor title in titles:\n    titles_tokenized.append(word_tokenize(title))","2bfbef01":"titles_an = [] #alphanumeric\nfor title in titles_tokenized:\n    words = [word for word in title if word.isalpha()]\n    titles_an.append(words)","ee98b787":"titles_an[0]","c6a1986e":"from nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\ntitles_preprocessed = []\nfor title in titles_an:\n    stemmed = [porter.stem(word) for word in title]\n    titles_preprocessed.append(stemmed)","6bc2d1aa":"titles_preprocessed[0]","51ce3f53":"word_list = []\n\nfor title in titles_preprocessed:\n    for word in title:\n        word_list.append(word)","8fc3be73":"from collections import Counter #Here, we create a counter\nfreq_list = Counter(word_list)","d2d22fca":"dictionary = freq_list.most_common(10000)","36357249":"dictionary = list(zip(*dictionary))[0]","55989c4e":"nums = range(0,10000)","d7507873":"word_int = dict(zip(dictionary, nums))","3cdd6e66":"x_data = []\n\nfor title in titles_preprocessed:\n    x_data.append([word_int[x] for x in title if x in word_int.keys()])","22c3079e":"import numpy as np\n\nx = np.array(x_data)\ntest_data = x[:5000]\ntrain_data = x[5000:]\n\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\nfrom keras.utils.np_utils import to_categorical\n\ny = np.asarray(y_data).astype('float32')\ny_test = y[:5000]\ny_train = y[5000:]","7b734aa8":"x_data = x_train[:5000]\nx_half_train = x_train[5000:]\n\ny_data = y_train[:5000]\ny_half_train = y_train[5000:]","7bd478d4":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(32, activation='relu', input_shape = (10000,)))\nmodel.add(layers.Dense(4, activation='relu'))\nmodel.add(layers.Dense(1, activation = 'softmax'))\n\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","efe1217e":"history = model.fit(x_half_train, y_half_train, epochs = 25, batch_size = 300, validation_data=(x_data, y_data))\n","51cea9e7":"results = model.evaluate(x_test, y_test)\nresults","b4068547":"import matplotlib.pyplot as plt\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","dabce8fc":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(32, activation='relu', input_shape = (10000,)))\nmodel.add(layers.Dense(4, activation='relu'))\nmodel.add(layers.Dense(1, activation = 'softmax'))\n\nmodel.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])","f4c35e36":"history = model.fit(x_half_train, y_half_train, epochs = 25, batch_size = 410, validation_data=(x_data, y_data))\n","d7ac8631":"results = model.evaluate(x_test, y_test)\nresults","1abb3897":"import matplotlib.pyplot as plt\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","3890cdae":"#Now, let us define our model\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(32, activation='relu', input_shape = (10000,)))\nmodel.add(layers.Dense(4, activation='relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])","fb8443c2":"history = model.fit(x_half_train, y_half_train, epochs = 10, batch_size = 512, validation_data=(x_data, y_data))\n","33b95dad":"results = model.evaluate(x_test, y_test)\nresults","15da5270":"import matplotlib.pyplot as plt\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","1792bc34":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud","7e28d965":"# Stopwords\nstops=stopwords.words(\"english\")","f09dec87":"#Create Corpus\n\ncorpus=[]\n\nfor i in range(data.shape[0]):\n    headline = re.sub(\"[^a-zA-Z]\",\" \",data[\"headline\"][i] )\n    headline=headline.lower()\n    headline=headline.split()\n    ps=PorterStemmer()\n    headline=[ps.stem(word) for word in headline if not word in set(stops)]\n    headline='  '.join(headline)\n    corpus.append(headline)","e4c87640":"corpus","eec57d64":"#Check out the generating WordCloud process \ncorpus_str=str(corpus)\ncloud = WordCloud(relative_scaling = 1.0,background_color = \"white\").generate(corpus_str)\n\nplt.figure(figsize = (12,10))\nplt.imshow(cloud)\nplt.axis(\"off\")\nplt.show()","57726ea2":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\nimport json\n\n#Non-Sarcastic vs Sarcastic\nsns.countplot(data['is_sarcastic'],palette=\"RdBu_r\")\nplt.title(\"Non-Sarcastic Text vs Sarcastic Text\")\nprint(\"No of Non-Sarcastic news: \",data['is_sarcastic'].value_counts()[0])\nprint(\"No of Sarcastic news: \",data['is_sarcastic'].value_counts()[1])","411d026e":"from wordcloud import WordCloud\n#We use WordCloud to check recurrent words, related to sarcastic text\nfig,ax = plt.subplots(figsize=(12,20))\nplt.axis('off')\n\nplt.subplot(2,1,1)\ntext = \" \".join(data[data['is_sarcastic']==0]['headline'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1200,height=1000).generate(text)\nplt.title(\"WordCloud for Non-Sarcastic text\")\nplt.axis('off')\nplt.imshow(wordcloud)","6e705c0c":"from wordcloud import WordCloud\n#We use WordCloud to check recurrent words, related to sarcastic text\nfig,ax = plt.subplots(figsize=(12,20))\nplt.axis('off')\n\n\nplt.subplot(2,1,2)\ntext = \" \".join(data[data['is_sarcastic']==1]['headline'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1200,height=1000).generate(text)\nplt.title(\"WordCloud for Sarcastic text\")\nplt.axis('off')\nplt.imshow(wordcloud)","4facba83":"#Check the average of such recurrent words\nfig,ax = plt.subplots(figsize=(16,8))\nfig.suptitle(\"AVERAGE WORD LENGTH IN A TEXT\")\n\nplt.subplot(1,2,1)\nplt.title(\"Non-Sarcastic Text\")\nword_length = data[data['is_sarcastic']==0]['headline'].str.split().apply(lambda x : [len(i) for i in x] )\nsns.distplot(word_length.map(lambda x: np.mean(x)),kde=True)\n\nplt.subplot(1,2,2)\nplt.title(\"Sarcastic Text\")\nword_length = data[data['is_sarcastic']==1]['headline'].str.split().apply(lambda x : [len(i) for i in x] )\nsns.distplot(word_length.map(lambda x: np.mean(x)),kde=True)","fac4e9e3":"# CHECK OUT RESULT, WORSE THAN THE FIRST MODEL, AS WE HAVE A HIGHER LOSS THAN BEFORE AND ACCURACY STILL THE SAME","650bff04":"# CHECKING OUT RESULTS, WE LOWERED OUR LOSS AND HIGHERED OUR ACCURACY!!!","e7535d72":"# CHECK OUT IF THE TOKENIZATION HAS BEEN IMPLEMENTED CORRECTLY ","013cab6d":"# if you are a more visual person like me, see the model's result by graph","28930f7c":"# RUN MODEL, DEFINING EPOCHS, BATCH_SIZE AND VALIDATION_DATA","7ec4312e":"# Using the corpus we created earlier, we generate a WordCloud process, in general","b182aebf":"# Usind the generator model, to create a wordcloud picture for sarcastic text","e62c3ba7":"# Usind the generator model, to create a wordcloud picture for non-sarcastic text","afdbe189":"# Then we create an array for x_data, considering the most frequent 10000 words, that we would split later, randomly, into test_data and train_data, and y_data consists of is.sarcastic.values","bf8a0436":"# IN ORDER TO CLASSIFY BETTER AND RUN A GOOD MODEL, WE TOKENIZE","bffbc32c":"# CREATE FIRST FEEDFORWARD MODEL","37680807":"# we finally split the data, into train and test","54b6e4fc":"# LOADING THE DATASET","c4588c1d":"# LAST TRY, AND HOPEFULLY BEST ONE, WE RUN OUR MODEL, CHANGING SOFTMAX WITH SIGMOID","2f6f7686":"# WE CHECK OUT THE MODEL\n# we have low loss and not a good accuracy, meaning we can do better!! Let's change hyperparameters, optimizers, loss function and activation ","32d695ea":"# WE CHANGED OUR EPOCH INTO 10 AND BATCH_SIZE INTO 512","773d4d2a":"# USE OF COUNTER AND DICTIONARY","dbb9380d":"# DATA ANALYSIS\n# let's do some exploration analysis, to see frequency and mode of words, used for sarcastic news and not","b88464c0":"# I hope that sharing my school project with you, I helped someone\n\n# thank you!\n\n# #staysafe","59fdda4e":"# AGAIN, GRAPH","3662eaa5":"# SECOND TRY - RUNNING SECOND MODEL, CHANGED OPTIMIZER AND LOSS","71b6ab1f":"# THEN, WE STEM TO SHORTEN, FINDING THE ROOT OF THE WORDS AND NORMALIZING PHRASES ","2ca1138d":"# Now, let's check out average lenght of the news, when is sarcastic and is not","a01bf715":"# CHECK WITH GRAPHS, AS WELL, TO SEE THE DIFFERENCE EVEN MORE"}}