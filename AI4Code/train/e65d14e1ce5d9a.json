{"cell_type":{"aa2e37a2":"code","48c3c96f":"code","b958e274":"code","0c320a6e":"code","38c2f773":"code","77c321a6":"code","bcc89f82":"code","59e26a02":"code","bb216e16":"code","f9476e1e":"code","5d6dce63":"code","4d85c19c":"code","50241729":"code","3c9b25cb":"code","2938990c":"code","af491741":"code","b9459509":"code","9b6a2a2c":"code","10c8e1be":"code","88e85c23":"code","5187bbd1":"code","5c92f9f0":"code","1c84b90d":"code","83449bea":"code","e331e674":"code","65eb5138":"code","fc06bc60":"code","065a26c7":"code","364e7775":"code","47bdec50":"code","ef39a166":"code","e4b281d4":"code","a586b324":"markdown","8c80c9ba":"markdown","08fd7370":"markdown","8a1ec759":"markdown","eddca698":"markdown","1558233d":"markdown","b243a464":"markdown","558e9b82":"markdown","d150d2dc":"markdown","841650f1":"markdown","e041dd2f":"markdown","be4070d5":"markdown","c1491019":"markdown","f932fb0f":"markdown","bbcdbe36":"markdown","7100302e":"markdown","91d02879":"markdown","b771898e":"markdown","8911027a":"markdown","7cc1dddb":"markdown","c17c2b3f":"markdown","48a46eee":"markdown","36eddc95":"markdown","2bb4ff55":"markdown","1300b85e":"markdown","ac88e489":"markdown","1b747b4b":"markdown"},"source":{"aa2e37a2":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image,ImageFilter","48c3c96f":"df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nX_test_main = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","b958e274":"df.head()","0c320a6e":"Y = df.iloc[:,0]\nX = df.iloc[:,1:]","38c2f773":"X = X.to_numpy()\nY = Y.to_numpy()\nX_test_main = X_test_main.to_numpy()","77c321a6":"X = X.reshape(-1,28,28)\nX_test_main = X_test_main.reshape(-1,28,28)\nprint(X.shape)\nprint(X_test_main.shape)","bcc89f82":"w=14\nh=14\nfig=plt.figure(figsize=(w,h))\ncolumns = 4\nrows = 5\nfor i in range(1, rows*columns+1):\n    img1 = X[i+random.randrange(1,300)]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img1)\nplt.show()","59e26a02":"X = np.expand_dims(X,axis=-1)\nX_test_main = np.expand_dims(X_test_main,axis=-1)\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size= 0.2,shuffle=True)","bb216e16":"print(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\nprint(X_test_main.shape)","f9476e1e":"X_train = X_train\/255.\nX_test = X_test\/255.\nX_test_main = X_test_main\/255.\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_test_main.shape)","5d6dce63":"def one_hottie(labels,C):\n    \"\"\"\n    One hot Encoding is used in multi-class classification problems to encode every label as a vector of binary values\n        eg. if there are 3 class as 0,1,2\n            one hot vector for class 0 could be : [1,0,0]\n                           then class 1: [0,1,0]\n                           and class 2: [0,0,1]\n    We need this encoding in out labels for the model learns to predict in a similar way.\n    \n    Without it,if only integer values are used in labels,it could affect model in different ways,\n        such as predicting a class that does not exist.\n        \n    \"\"\"\n    One_hot_matrix = tf.one_hot(labels,C)\n    return tf.keras.backend.eval(One_hot_matrix)\n\nY_test_later = Y_test.copy()\nY_train = one_hottie(Y_train, 10)\nY_test = one_hottie(Y_test, 10)\nprint (\"Y shape: \" + str(Y_train.shape))\nprint (\"Y test shape: \" + str(Y_test.shape))","4d85c19c":"# def res_net_block(input_data, filters=[128], conv_size=[3,5]):\n#     x = tf.keras.layers.Conv2D(filters[0], conv_size[0], activation='relu', padding='same')(input_data)\n#     x = tf.keras.layers.BatchNormalization()(x)\n#     x = tf.keras.layers.Conv2D(filters[0], conv_size[1], activation=None, padding='same')(x)\n#     x = tf.keras.layers.BatchNormalization()(x)\n#     x = tf.keras.layers.Add()([x, input_data])\n#     x = tf.keras.layers.Activation('relu')(x)\n#     return x","50241729":"model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, 3, activation='relu', input_shape=(28,28,1),padding=\"same\"),\n    tf.keras.layers.MaxPool2D(strides=2),\n    \n    \n    tf.keras.layers.Conv2D(128, 3, activation='relu',padding=\"same\"),\n    tf.keras.layers.MaxPool2D(strides=2),\n    \n    tf.keras.layers.Dropout(0.2),\n        \n    tf.keras.layers.Conv2D(256, 3, activation='relu',padding=\"same\"),\n    tf.keras.layers.MaxPool2D(strides=2),\n    \n    tf.keras.layers.Conv2D(256, 3, activation='relu',padding=\"same\"),\n    tf.keras.layers.MaxPool2D(strides=2),\n        \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(100,kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n    \n    tf.keras.layers.Dense(50,kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n        \n    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(0.01) ,activation='softmax')\n])","3c9b25cb":"model.summary()","2938990c":"# initial_learning_rate = 0.001 #initial rate\n# # Rate decay with exponential decay\n# # new rate = initial_learning_rate * decay_rate ^ (step \/ decay_steps)\n\n# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate,\n#     decay_steps=800,\n#     decay_rate=0.5,\n#     staircase=True)","af491741":"model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.006),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy','Recall','Precision'])\n\nresult = model.fit(x=X_train,y=Y_train,batch_size=64,epochs=20,verbose=1,shuffle=False,initial_epoch=0,\n                   validation_split=0.1)","b9459509":"plt.plot(result.history['acc'], label='train')\nplt.plot(result.history['val_acc'], label='valid')\nplt.legend(loc='upper left')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()\nplt.plot(result.history['loss'], label='train')\nplt.plot(result.history['val_loss'], label='test')\nplt.legend(loc='upper right')\nplt.title('Model Cost')\nplt.ylabel('Cost')\nplt.xlabel('Epoch')\nplt.show()","9b6a2a2c":"model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0001),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy','Recall','Precision'])\n\nresult = model.fit(x=X_train,y=Y_train,batch_size=64,epochs=40,verbose=1,shuffle=False,initial_epoch=20,\n                   validation_split=0.1)","10c8e1be":"plt.plot(result.history['acc'], label='train')\nplt.plot(result.history['val_acc'], label='valid')\nplt.legend(loc='upper left')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()\nplt.plot(result.history['loss'], label='train')\nplt.plot(result.history['val_loss'], label='test')\nplt.legend(loc='upper right')\nplt.title('Model Cost')\nplt.ylabel('Cost')\nplt.xlabel('Epoch')\nplt.show()","88e85c23":"model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00006),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy','Recall','Precision'])\n\nresult = model.fit(x=X_train,y=Y_train,batch_size=64,epochs=60,verbose=1,shuffle=False,initial_epoch=40,\n                   validation_split=0.1)","5187bbd1":"check = model.evaluate(X_test,Y_test)","5c92f9f0":"preds = model.predict_classes(X)\npreds.shape","1c84b90d":"# X = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\n# Y_test = X.iloc[:,0]\n# Y_test = Y_test.to_numpy()","83449bea":"conf = tf.math.confusion_matrix(preds,Y)","e331e674":"with tf.Session() as session:\n    print(conf.eval())","65eb5138":"train_gen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n                                                            zoom_range=0.20,\n                                                            width_shift_range=0.2,\n                                                            height_shift_range=0.2,\n                                                            shear_range=0.20,\n                                                            horizontal_flip=False,\n                                                            brightness_range=[0.1,1],\n                                                            rescale=1.\/255)\ntest_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255)","fc06bc60":"model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00005),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy','Recall','Precision'])\n\nresult = model.fit(train_gen.flow(X_train*255,Y_train,batch_size=64),\n                   validation_data = test_gen.flow(X_test*255,Y_test,batch_size=16),\n                   epochs=70,\n                   verbose=1)","065a26c7":"preds = model.predict_classes(X_test_main)","364e7775":"preds.shape","47bdec50":"arr = [x for x in range(1,28001)]\nlabel = pd.DataFrame(arr,columns = [\"ImageId\"])\nlabel[\"Label\"] = pd.DataFrame(preds)\nlabel.head()","ef39a166":"label.to_csv('Y_test.csv',header=True,index = False)","e4b281d4":"model.save(\"saved_model\")","a586b324":"**We have hit 99.3\n% now**\n* So lets see where things went wrong with a confusion matrix","8c80c9ba":"**Lets Import our Tools**","08fd7370":"See those spikes, that tells me that learning rate was a bit higher.","8a1ec759":"A conv2d block = \"**CONV2D(number of filters,size of filters) -> ReLU -> MAXPOOL2D**\"\n\n*INPUT DATA -> CONV2D(64,3) -> ReLU -> CONV2D BLOCK(128,3) -> CONV2D BLOCK(256,3) -> CONV2D BLOCK(256,3) -> \nFlatten out the ouput -> DENSE(100) -> DROPOUT(0.4) -> ReLU -> DENSE(50) -> ReLU -> DROPOUT(0.4) -> DENSE(10) -> OUTPUT DATA(that is kinda of likeliness of each particular class being correct) -> SOFTMAX FOR CLASSIFICATION\n\nBETTER TO CHECK THIS OUT AT MODEL SUMMARY","eddca698":"* For the 1st train, I'll boost the training with a 0.006 learning rate in just 20 epochs\n* Actually less than that were required, i just made sure it gets a little stable","1558233d":"**And our Data**","b243a464":"As you can see I included rescale in the data generator, but the images are already normalized. So we denormalize it before passing it to generator","558e9b82":"**2nd TRaining**\nNow we start form 99% but taking this to 100 would be hard with this model\n* Also we need to reduce the loss a lot. Being such a simple model the loss is still high","d150d2dc":"# BUILDING THE MODEL","841650f1":"***3rd Training***\nWe reduce the learning rate further to check more performance can be extracted here","e041dd2f":"So right now we already hit 98 but that is expected as the data is just too simple","be4070d5":"**SO 10 classes, One hot matrix encoding**","c1491019":"**Lets have a look at our data**","f932fb0f":"**And convert them to numpy for preprocessing**","bbcdbe36":"One more training. This time with data Augmentation","7100302e":"**For simplicity , We won't be varying the learning rate with scheduler, instead we will just train it multiple times**","91d02879":"## MODEL SUMMARY","b771898e":"**Lets use a sequential for real fast assemble of layers**","8911027a":"\n\n**And WHen you check out the training and validation set combined**","7cc1dddb":"As the data is quite simple, we won't need any complex model, so here is just a simple good enough model","c17c2b3f":"**So We split the data into X and Y**","48a46eee":"**So we barely touched 99.1 here**\n* I saw a variance problem here so i actually reduced dropouts.","36eddc95":"LETS CHECK OUT THE PLOTS\n* As you can see, it hits the 96+ immediately and gets stable there\n* So now we will reduce the learning rate by a lot, and train the model for real","2bb4ff55":"**So we will reshape it into proper image shapes as we will be using convolutional networks**","1300b85e":"1. **Alright then lets normalize the images and go to the model**","ac88e489":"# Make predictions","1b747b4b":"# Preprocessing"}}