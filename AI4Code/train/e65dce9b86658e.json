{"cell_type":{"652f0a46":"code","8a1f9f82":"code","ec3bcadf":"code","106c923e":"code","69525464":"code","2cfb9e62":"code","59e265f6":"code","2786d868":"code","ef040209":"code","93bd08bb":"code","94cd01b3":"code","4514167d":"code","0f91aff7":"code","c732a01b":"code","040f280b":"code","46a99ef0":"code","145189e3":"code","93f2ff5a":"code","4d8e73c3":"code","59b6cfe3":"code","370212f6":"code","75f616bd":"code","75232f88":"code","a6835943":"code","40ccfbb7":"code","0dc7fb1e":"code","e6c0782f":"code","31a55dae":"code","6fb315b1":"code","f363fe52":"code","1375169b":"code","36daac00":"code","52fec7d4":"code","b2335d7a":"code","75ef014f":"code","8c846775":"code","adcfea2f":"code","3a08b077":"code","1e9108ed":"code","7b1d1c06":"markdown","1ac43131":"markdown","1c0baa37":"markdown","6ab3cf30":"markdown","f0887de8":"markdown","b9cf9dd4":"markdown","71327202":"markdown","cc81fa9c":"markdown","48f42943":"markdown","06dab03e":"markdown","61cdf4fc":"markdown","b5f2f260":"markdown","230602f8":"markdown","dc7f5127":"markdown","a2cc51b5":"markdown","138c1c4c":"markdown","1b85f303":"markdown","9a90b55b":"markdown","9b5a5a3b":"markdown","e959dcc2":"markdown","6e575687":"markdown","12f6feb5":"markdown","6cf5f55a":"markdown","e306921c":"markdown","a425b9ab":"markdown","27f2e2aa":"markdown","0c3ada5f":"markdown","0e064893":"markdown","4e25a6c4":"markdown","31135238":"markdown","2e92253d":"markdown","8b2a9bfd":"markdown","9b4e1de5":"markdown","e6903f4b":"markdown","df5286e1":"markdown","6adfdc65":"markdown","21cff266":"markdown","259c7a98":"markdown","6338b63b":"markdown","1d4f74ea":"markdown","c3955f7f":"markdown","af37ea4a":"markdown","2dc9914a":"markdown","89a925bc":"markdown","74c3e5cc":"markdown","e3c1cd37":"markdown","670875f5":"markdown","2d385585":"markdown"},"source":{"652f0a46":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style()\n\n# to divide our data into train and validation set\nfrom sklearn.model_selection import train_test_split\n#to encode our labels\nfrom tensorflow.keras.utils import to_categorical\n#to build our model \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout\n# Our optimizer options\nfrom keras.optimizers import RMSprop\nfrom keras.optimizers import Adam\n#Callback options\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\n#importing image data generator for data augmentation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n#for the final prediction report\nfrom sklearn.metrics import classification_report ,confusion_matrix","8a1f9f82":"train =pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest=pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ny_train=train['label']\nprint(f'The size of training data is {train.shape[0]} and that of test data is {test.shape[0]}')","ec3bcadf":"y_train","106c923e":"x_train=train.drop('label',axis=1).values\nx_train","69525464":"plt.imshow(x_train[41999].reshape(28,28),cmap='binary')\nplt.show()","2cfb9e62":"x_train=x_train.reshape(42000,28,28)","59e265f6":"x_train.shape","2786d868":"\nx_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.10,random_state=19)","ef040209":"print(f'The training set has {x_train.shape[0]} images and the validation set has {x_val.shape[0]} images')","93bd08bb":"y_cat_train=to_categorical(y_train,num_classes=10)\ny_cat_val=to_categorical(y_val,num_classes=10)","94cd01b3":"random_image=x_train[78]\nrandom_label=y_train[78]\nplt.imshow(random_image,cmap='binary')\nplt.title(random_label,fontsize=20,weight='bold',color='red')\nplt.show()","4514167d":"random_image.max()","0f91aff7":"random_image.min()","c732a01b":"x_train_scaled=x_train\/255\nx_val_scaled=x_val\/255","040f280b":"plt.imshow(x_train_scaled[78],cmap='binary')\nplt.title(y_train[78],fontsize=20,weight='bold',color='red')\nplt.show()","46a99ef0":"x_train_final = x_train_scaled.reshape(x_train_scaled.shape[0],28,28,1)\nx_val_final = x_val_scaled.reshape(x_val_scaled.shape[0],28,28,1)","145189e3":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","93f2ff5a":"model=Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(5,5),strides=(1,1),input_shape=(28,28,1),padding='Same',activation='relu'))\nmodel.add(Conv2D(filters=32,kernel_size=(5,5),strides=(1,1),input_shape=(28,28,1),padding='Same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),input_shape=(28,28,1),padding='Same',activation='relu'))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),input_shape=(28,28,1),padding='Same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10,activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","4d8e73c3":"\nearly_stop= EarlyStopping(monitor='val_loss',patience=2)","59b6cfe3":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","370212f6":"image_gen=ImageDataGenerator(rotation_range=10,width_shift_range=0.1,height_shift_range=0.1,shear_range=0.1,zoom_range=0.1,fill_mode='nearest')","75f616bd":"train_image_gen=image_gen.fit(x_train_final)","75232f88":"train_image_gen","a6835943":"epoch= 15 #30 gave me the best accuracy!","40ccfbb7":"model.fit_generator(image_gen.flow(x_train_final,y_cat_train),epochs=epoch,validation_data=(x_val_final,y_cat_val),callbacks=[learning_rate_reduction])","0dc7fb1e":"metrics=pd.DataFrame(model.history.history)\nmetrics","e6c0782f":"metrics[['loss' , 'val_loss']].plot()\nplt.show()","31a55dae":"metrics[['accuracy' , 'val_accuracy']].plot()\nplt.show()","6fb315b1":"model.evaluate(x_val_final, y_cat_val, verbose=0)","f363fe52":"y_pred=model.predict_classes(x_val_final)\ny_pred","1375169b":"\nprint('Classification Report')\nprint(classification_report(y_val,y_pred))\nprint('\\n')\nprint('Confusion Matrix')\nprint(confusion_matrix(y_val,y_pred))","36daac00":"np.random.seed(22)\nrandom_selection=np.random.randint(0,4201,size=1)\nrandom_sample=x_val_final[random_selection]\nprint('Prediction:')\nprint(model.predict_classes(random_sample.reshape(1,28,28,1))[0])\nplt.imshow(random_sample.reshape(28,28),cmap='binary')\nplt.show","52fec7d4":"np.random.seed(20)\nrandom_selection=np.random.randint(0,4201,size=1)\nrandom_sample=x_val_final[random_selection]\nprint('Prediction:')\nprint(model.predict_classes(random_sample.reshape(1,28,28,1))[0])\nplt.imshow(random_sample.reshape(28,28),cmap='binary')\nplt.show","b2335d7a":"np.random.seed(19)\nrandom_selection=np.random.randint(0,4201,size=1)\nrandom_sample=x_val_final[random_selection]\nprint('Prediction:')\nprint(model.predict_classes(random_sample.reshape(1,28,28,1))[0])\nplt.imshow(random_sample.reshape(28,28),cmap='binary')\nplt.show","75ef014f":"# Reshaping test data\nx_test = test.values\nx_test = x_test.reshape(28000,28,28)\nx_test.shape","8c846775":"x_test_scaled=x_test\/255","adcfea2f":"test_predictions=model.predict_classes(x_test_scaled.reshape(28000,28,28,1))","3a08b077":"test_predictions","1e9108ed":"output=pd.DataFrame({'ImageId':test.index+1,'Label':test_predictions})\noutput.to_csv('new_submission.csv',index=False)","7b1d1c06":"<a id=\"section12\"><\/a>\n## Accuracy and Loss plots","1ac43131":"So we are now on the last part ! Let's first make our test data all ready and then we will submit our predictions !","1c0baa37":"Let's see how much data we have now in train and validation set !","6ab3cf30":"Let's set our features aside and see how it looks!","f0887de8":"![Image Recognier](https:\/\/miro.medium.com\/max\/372\/1*AO2rIhzRYzFVQlFLx9DM9A.png)","b9cf9dd4":"<a id=\"section4\"><\/a>\n## Encoding the Label column\n","71327202":"Let's just see a random image from our dataset ( I just used 41999 randomly , you can use any number).\n- We will use the imshow(image show) function of matplotlib library here , you can use cmap as binary or Greys (they will get you the same image)\n- we will reshape it to a image size that is 28 * 28 so that we can plot it from 728.","cc81fa9c":"Now we will reshape the whole x_train dataset.","48f42943":"Okay so we are almost done , but to process the image we need to tell our model that our images are grey-scale , and we will do it by adding a 1 to the shape of our dataset.","06dab03e":"<a id=\"section8\"><\/a>\n## Building our model","61cdf4fc":"<a id=\"section6\"><\/a>\n## Building our CNN Model\n","b5f2f260":"<a id=\"toc_section\"><\/a>\n## Table of Contents\n* [Introduction](#top_section)\n* [Importing all the Required Libraries and Data](#section1)\n* [Making our Data ready to be used](#section2)\n    - [Shaping our data](#section3)\n    - [Scaling our data](#section4)\n    - [Encoding our data](#section5)\n* [Building our CNN Model](#section6)\n    - [Choosing an Optimizer](#section7)\n    - [Building our model](#section8)\n    - [Data Augmentation](#section9)\n    - [Training our model](#section10)\n    - [Accuracy and Loss plots](#section11)\n    - [Accuracy report](#section12)\n* [Submission & Some Last Words](#sectionlst)\n","230602f8":"### For beginners:\n> What is data Augmentation\n<br>\n\n- Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n","dc7f5127":"# Some last words:\n\n### Thank you for reading! I'm still a beginner and want to improve myself in every way I can. So if you have any ideas to feedback please let me know in the comments section!\n\n\n<div align='center'><font size=\"5\" color=\"#000000\"><b>And again please vote if you liked this notebook so it can reach more people, Thanks!<\/b><\/font><\/div>","a2cc51b5":"<a id=\"section1\"><\/a>\n## Importing the libraries and Data\n","138c1c4c":"Ler's see our label column ! Basically it is the the column which labels our image as the number it is recognized as.","1b85f303":"<a id=\"section11\"><\/a>\n## Training our Model !","9a90b55b":"<a id=\"section3\"><\/a>\n## Scaling our data\n","9b5a5a3b":"So now that we know what data augmentation is , let's implement it to make our model even better !","e959dcc2":"So the long part is done , but just to be sure that we did not eventually overfit the data , let's plot the graph for it!","6e575687":"<a id=\"section2\"><\/a>\n## Making our Data to be used !\n","12f6feb5":"<a id=\"section10\"><\/a>\n## Data Augmentation","6cf5f55a":"<a id=\"section7\"><\/a>\n## Choosing an optimizer\n","e306921c":"Let's set our RMSprop then :\n- Setting learning rate to 0.001\n- The value of rho to 0.9\n- Epsilon to 10^-8\n- Decay to zero\n\nYou can also read more about RMSprop [here](https:\/\/towardsdatascience.com\/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a).","a425b9ab":"<a id=\"section13\"><\/a>\n## Accuracy Report","27f2e2aa":"# <a id=\"top_section\"><\/a>\n\n<div align='center'><font size=\"6\" color=\"#000000\"><b> Image Recognition using CNN (explained !) <\/b><\/font><\/div>\n<hr>\n<div align='center'><font size=\"5\" color=\"#000000\">Introduction<\/font><\/div>\n<hr>\n\nWhen I started doing this problem my main goal was getting experience and to be able to explain whatver I learnt on the way. I'm still learning and trying to improve my skills, so every feedback and advices are welcome !\n<br><br>\nIn this notebook I will try to show how to build a Digit Recognizer using Convolutional Neural Network(CNN) and will try to explain each step. If you are a beginner then this is the Notebook for you !\n\n### Here are the things I will try to cover in this Notebook:\n\n- What does our data look like ?\n- What model should we choose or build?\n- Which optimization technique to use?\n- What is data augmentation ? Will it help in increasing the accuracy of our model?\n- Is it possible to achieve 100% accuracy?\n\n### If you like this kernel feel free to upvote and leave feedback, thanks!","0c3ada5f":"![](https:\/\/image.slidesharecdn.com\/pacpresentation-111117202719-phpapp02\/95\/packaging-your-story-social-media-for-b2bs-59-728.jpg?cb=1328235125)","0e064893":"Let's start this notebook with importing all the necessary libraries we would need and the dataset.","4e25a6c4":"Now let's plot a random image along with it's label.","31135238":"<a id=\"section9\"><\/a>\n## Choosing a callback\n","2e92253d":"But won't it affect the image? Same question was mine when i first thought about it but , NO , it won't ! Don't believe me ? See for yourself !","8b2a9bfd":"Now let's call this image_gen we created !","9b4e1de5":"As our label column has 10 possible values(0-9) we will encode it using the to_categorical function from the keras.\n- We will use num_classes as 10 , as that is how many possible values the column can have.","e6903f4b":"<a id=\"section3\"><\/a>\n## Shaping our data\n","df5286e1":"I am using ImageDataGenerator function from keras here for data augmentation ,and have set the range for different features as below:\n- Rotation : 10\n- Width : 10% \n- Height : 10%\n- Sheer : 10 %\n- Zoom : 10%","6adfdc65":"So it's all set now let's build our model shall we?\nI tried various models and this one gave me the best results so i will be using this one !\nIf you play with it a little bit you can get even better results , atleast that's how i got it !\nHere's a tip if you're a beginner: <br>\n> Start with a base model , and change a little bit and continue doing so , play with the base model and stop when you get better results.","21cff266":"There are two call-back techniques that I tried :\n- [Earlystop](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping) : A basic technique which stops training when a monitored metric has stopped improving.\n- [ReduceLRonPlateau](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ReduceLROnPlateau) : A more advance technique than early stop which reduces learning rate when a metric has stopped improving.","259c7a98":"Looks pretty good , now let's see our model's accuracy report !","6338b63b":"So we have come a long way ! CONGRATS , we have almost completed our model.\nHere comes the most boring , time-taking and the longest part , i.e., training the model.\nSit back , go for a walk , talk to your friends or maybe just watch netflix and let the model train !","1d4f74ea":"So there are a lot of optimizers out there and the most famous ones are Adam and RMSprop , \n> So what should we choose?\n\nI tried both and RMSprop gave me better results for my model so I will be using RMSprop here , but you can also try Adam optimizer it's good too and maybe if you are using a different model Adam optimizer could work better for you model !","c3955f7f":"Let's be sure that we did it right :P","af37ea4a":"Let's see what is the range of our data for the images. Simply the (max-min) value is range.","2dc9914a":"Clearly 255 is the range , so we will now use this value to scale our data ! ","89a925bc":"<a id=\"sectionlst\"><\/a>\n#  Submission\n\n\n### [Back To Table of Contents](#toc_section)","74c3e5cc":"Let's convert these predictions into a submiitable csv file.","e3c1cd37":"![Data splitting chart](https:\/\/i.stack.imgur.com\/pXAfX.png)","670875f5":"Let's split our data into training and validation set , we will use the train_test_split function of sklearn library for this step.","2d385585":"![Thank you for reading](https:\/\/i.chzbgr.com\/full\/8599480832\/hC37E5E67\/thank-you-very-much)"}}