{"cell_type":{"8d1e179d":"code","3c3b1754":"code","080251fe":"code","0138592d":"code","2d244f5e":"code","c4bd5193":"code","39f71c7d":"code","b15a2acc":"code","baa09f45":"code","601e65f7":"code","6f78a2fd":"code","2c81322b":"code","61f8b854":"code","0d8e3274":"code","05f47367":"code","af366f97":"code","70739ce3":"code","fceab712":"code","cd615520":"code","74e752ad":"code","4474aafa":"code","c35ba7d3":"code","a7184539":"code","8f91fc98":"code","be0a4029":"code","3733ece3":"code","bbca34b4":"code","bde0f16c":"code","7f80b089":"code","dfe38889":"code","89ccb67e":"code","504914be":"code","299bfeb1":"code","d6c2efbb":"code","c0efa7a8":"code","446d84db":"code","2229085d":"code","1f5f2cfe":"code","f282ad36":"code","b8258fe8":"code","17896fad":"code","f1bdeefb":"code","ff6a1570":"code","f1f80317":"code","9804265b":"code","ef0ce17c":"code","81cc9a21":"code","759c52ba":"code","7e8b9494":"code","86c68fcb":"code","f38ea4ba":"code","f3e95a97":"code","66d8f9e9":"code","61fe3360":"code","a536b3c0":"code","e4ad34db":"code","c9feb8e9":"code","1c5e9acc":"code","91d3de98":"code","59ed7cc5":"code","a84c04f2":"code","93f79f6c":"code","88ade60b":"code","82a94e6b":"code","24898bea":"code","ee9e20dd":"code","96ae51a0":"code","27dc4eab":"code","c00c0afd":"code","30fb0119":"code","87aec38b":"code","5a8a10f8":"code","16b30c4a":"code","233abbed":"code","a972b819":"code","aaf983cc":"code","14d0054e":"code","19e0eb24":"code","e7a48c1d":"code","1ed5cea9":"code","3e3b93c4":"code","7b639dfc":"code","c6f88a29":"code","18013847":"code","d09e7757":"code","83af2270":"code","455e7448":"code","97238871":"code","fe591033":"code","95ec239f":"code","e372ee34":"markdown"},"source":{"8d1e179d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()","3c3b1754":"# Example\n# Converts to log1p(count)\n# Print original count back using expm1\nprint('Test log and exp')\ntest_count = 100\nprint('original value', test_count)\nx = np.log1p(test_count) # log (x+1)\nprint('log1p', x)\nprint('expm1', np.expm1(x)) # exp(x) - 1","080251fe":"df = pd.read_csv('..\/input\/bike-sharing-demand\/train.csv',parse_dates=['datetime'],index_col=0)\ndf_test = pd.read_csv('..\/input\/bike-sharing-demand\/test.csv',parse_dates=['datetime'],index_col=0)\n","0138592d":"df.head()","2d244f5e":"columns = ['count', 'season', 'holiday', 'workingday', 'weather', 'temp',\n       'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'dayofweek','hour']","c4bd5193":"# We need to convert datetime to numeric for training.\n# Let's extract key features into separate numeric columns\ndef add_features(df):\n    df['year'] = df.index.year\n    df['month'] = df.index.month\n    df['day'] = df.index.day\n    df['dayofweek'] = df.index.dayofweek\n    df['hour'] = df.index.hour","39f71c7d":"# Add New Features\nadd_features(df)\nadd_features(df_test)","b15a2acc":"df.head()","baa09f45":"# Need to predict the missing data\nplt.title('Rental Count - Gaps')\ndf['2011-01':'2011-02']['count'].plot()\nplt.show()","601e65f7":"# Rentals change hourly!\nplt.plot(df['2011-01-01']['count'])\nplt.xticks(fontsize=14, rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Rental Count')\nplt.title('Hourly Rentals for Jan 01, 2011')\nplt.show()","6f78a2fd":"# Seasonal\nplt.plot(df['2011-01']['count'])\nplt.xticks(fontsize=14, rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Rental Count')\nplt.title('Jan 2011 Rentals (1 month)')\nplt.show()","2c81322b":"group_hour = df.groupby(['hour'])\naverage_by_hour = group_hour['count'].mean()","61f8b854":"plt.plot(average_by_hour.index,average_by_hour)\nplt.xlabel('Hour')\nplt.ylabel('Rental Count')\nplt.xticks(np.arange(24))\nplt.grid(True)\nplt.title('Average Hourly Rental Count')","0d8e3274":"# Year to year trend\nplt.plot(df['2011']['count'],label='2011')\nplt.plot(df['2012']['count'],label='2012')\nplt.xticks(fontsize=14, rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Rental Count')\nplt.title('2011 and 2012 Rentals (Year to Year)')\nplt.legend()\nplt.show()","05f47367":"plt.plot(df['2011']['count'].map(np.log1p),label='2011')\nplt.plot(df['2012']['count'].map(np.log1p),label='2012')\nplt.xticks(fontsize=14, rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Log(Rental Count)')\nplt.title('2011 and 2012 Rentals (Year to Year)')\nplt.legend()\nplt.show()","af366f97":"plt.boxplot([df['count']], labels=['count'])\nplt.title('Box Plot - Count')\nplt.ylabel('Target')\nplt.grid(True)","70739ce3":"# Let's see how the data distribution changes with log1p\n# Evenly distributed\nplt.boxplot([df['count'].map(np.log1p)], labels=['log1p(count)'])\nplt.title('Box Plot - log1p(Count)')\nplt.ylabel('Target')\nplt.grid(True)","fceab712":"df[\"count\"] = df[\"count\"].map(np.log1p)","cd615520":"df.head()","74e752ad":"df_test.head()","4474aafa":"df.dtypes","c35ba7d3":"group_year_month = df.groupby(['year','month'])","a7184539":"average_year_month = group_year_month['count'].mean()","8f91fc98":"average_year_month","be0a4029":"for year in average_year_month.index.levels[0]:\n    plt.plot(average_year_month[year].index,average_year_month[year],label=year)\n    \nplt.legend()    \nplt.xlabel('Month')\nplt.ylabel('Count')\nplt.grid(True)\nplt.title('Average Monthly Rental Count for 2011, 2012')\nplt.show()","3733ece3":"group_year_hour = df.groupby(['year','hour'])\naverage_year_hour = group_year_hour['count'].mean()\nfor year in average_year_hour.index.levels[0]:\n    #print (year)\n    #print(average_year_month[year])\n    plt.plot(average_year_hour[year].index,average_year_hour[year],label=year)\n    \nplt.legend()    \nplt.xlabel('Hour')\nplt.ylabel('Count')\nplt.xticks(np.arange(24))\nplt.grid(True)\nplt.title('Average Hourly Rental Count - 2011, 2012')","bbca34b4":"group_workingday_hour = df.groupby(['workingday','hour'])\naverage_workingday_hour = group_workingday_hour['count'].mean()","bde0f16c":"for workingday in average_workingday_hour.index.levels[0]:\n    #print (year)\n    #print(average_year_month[year])\n    plt.plot(average_workingday_hour[workingday].index,average_workingday_hour[workingday],\n             label=workingday)\n    \nplt.legend()    \nplt.xlabel('Hour')\nplt.ylabel('Count')\nplt.xticks(np.arange(24))\nplt.grid(True)\nplt.title('Average Hourly Rental Count by Working Day')\nplt.show()","7f80b089":"# Let's look at correlation beween features and target\ndf.corr()['count']","dfe38889":"# Any relation between temperature and rental count?\nplt.scatter(x=df.temp,y=df[\"count\"])\nplt.grid(True)\nplt.xlabel('Temperature')\nplt.ylabel('Count')\nplt.title('Temperature vs Count')\nplt.show()","89ccb67e":"# Any relation between humidity and rental count?\nplt.scatter(x=df.humidity,y=df[\"count\"],label='Humidity')\nplt.grid(True)\nplt.xlabel('Humidity')\nplt.ylabel('Count')\nplt.title('Humidity vs Count')\nplt.show()","504914be":"# Save all data\ndf.to_csv('bike_all.csv',index=True,index_label='datetime',columns=columns)","299bfeb1":"# Training = 70% of the data\n# Validation = 30% of the data\n# Randomize the datset\nnp.random.seed(5)\nl = list(df.index)\nnp.random.shuffle(l)\ndf = df.loc[l]","d6c2efbb":"rows = df.shape[0]\ntrain = int(.7 * rows)\ntest = rows-train","c0efa7a8":"rows, train, test","446d84db":"columns","2229085d":"# Write Training Set\ndf.iloc[:train].to_csv('bike_train.csv'\n                          ,index=False,header=False\n                          ,columns=columns)","1f5f2cfe":"# Write Validation Set\ndf.iloc[train:].to_csv('bike_validation.csv'\n                          ,index=False,header=False\n                          ,columns=columns)","f282ad36":"# Test Data has only input features\ndf_test.to_csv('bike_test.csv',index=True,index_label='datetime')","b8258fe8":"print(','.join(columns))","17896fad":"# Write Column List\nwith open('bike_train_column_list.txt','w') as f:\n    f.write(','.join(columns))","f1bdeefb":"# Install xgboost in notebook instance.\n#### Command to install xgboost\n!pip install xgboost==0.90","ff6a1570":"import sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# XGBoost \nimport xgboost as xgb","f1f80317":"column_list_file = 'bike_train_column_list.txt'\ntrain_file = 'bike_train.csv'\nvalidation_file = 'bike_validation.csv'\ntest_file = 'bike_test.csv'","9804265b":"columns = ''\nwith open(column_list_file,'r') as f:\n    columns = f.read().split(',')","ef0ce17c":"# Specify the column names as the file does not have column header\ndf_train = pd.read_csv(train_file,names=columns)\ndf_validation = pd.read_csv(validation_file,names=columns)","81cc9a21":"df_train.head()","759c52ba":"X_train = df_train.iloc[:,1:] # Features: 1st column onwards \ny_train = df_train.iloc[:,0].ravel() # Target: 0th column\n\nX_validation = df_validation.iloc[:,1:]\ny_validation = df_validation.iloc[:,0].ravel()","7e8b9494":"# XGBoost Training Parameter Reference: \n#   https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.md\n#regressor = xgb.XGBRegressor(max_depth=5,eta=0.1,subsample=0.7,num_round=150)\nregressor = xgb.XGBRegressor(max_depth=5,n_estimators=150)","86c68fcb":"regressor","f38ea4ba":"regressor.fit(X_train,y_train, eval_set = [(X_train, y_train), (X_validation, y_validation)])","f3e95a97":"eval_result = regressor.evals_result()","66d8f9e9":"training_rounds = range(len(eval_result['validation_0']['rmse']))","61fe3360":"print(training_rounds)","a536b3c0":"plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\nplt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\nplt.grid(True)\nplt.xlabel('Iteration')\nplt.ylabel('RMSE')\nplt.title('Training Vs Validation Error')\nplt.legend()\nplt.show()","e4ad34db":"xgb.plot_importance(regressor)\nplt.show()","c9feb8e9":"# Verify Quality using Validation dataset\n# Compare actual vs predicted performance with dataset not seen by the model before\ndf = pd.read_csv(validation_file,names=columns)","1c5e9acc":"df.head()","91d3de98":"df.shape","59ed7cc5":"X_test = df.iloc[:,1:]\nprint(X_test[:5])","a84c04f2":"result = regressor.predict(X_test)","93f79f6c":"result[:5]","88ade60b":"df['count_predicted'] = result","82a94e6b":"# Negative Values are predicted\ndf['count_predicted'].describe()","24898bea":"df[df['count_predicted'] < 0]","ee9e20dd":"df['count_predicted'].hist()\nplt.title('Predicted Count Histogram')\nplt.show()","96ae51a0":"def adjust_count(x):\n    if x < 0:\n        return 0\n    else:\n        return x","27dc4eab":"df['count_predicted'] = df['count_predicted'].map(adjust_count)","c00c0afd":"df[df['count_predicted'] < 0]","30fb0119":"df['count'] = df['count'].map(np.expm1)\ndf['count_predicted'] = df['count_predicted'].map(np.expm1)","87aec38b":"# Actual Vs Predicted\nplt.plot(df['count'], label='Actual')\nplt.plot(df['count_predicted'],label='Predicted')\nplt.xlabel('Sample')\nplt.ylabel('Count')\nplt.xlim([100,150])\nplt.title('Validation Dataset - Predicted Vs. Actual')\nplt.legend()\nplt.show()","5a8a10f8":"# Over prediction and Under Prediction needs to be balanced\n# Training Data Residuals\nresiduals = (df['count'] - df['count_predicted'])\n\nplt.hist(residuals)\nplt.grid(True)\nplt.xlabel('Actual - Predicted')\nplt.ylabel('Count')\nplt.title('Residuals Distribution')\nplt.axvline(color='r')\nplt.show()","16b30c4a":"value_counts = (residuals > 0).value_counts(sort=False)\nprint(' Under Estimation: {0:0.2f}'.format(value_counts[True]\/len(residuals)))\nprint(' Over  Estimation: {0:0.2f}'.format(value_counts[False]\/len(residuals)))","233abbed":"print(\"RMSE: {0:0.2f}\".format(mean_squared_error(df['count'],df['count_predicted'])**.5))","a972b819":"# RMSlE - Root Mean Squared Log Error\n# RMSLE Metric is used by Kaggle for this competition\n\n# RMSE Cost Function - Magnitude of difference matters\n\n# RMSLE cost function - \"Only Percentage difference matters\"\n\n# Reference:Katerina Malahova, Khor SoonHin \n# https:\/\/www.slideshare.net\/KhorSoonHin\/rmsle-cost-function\ndef compute_rmsle(y_true, y_pred):\n    if type(y_true) != np.ndarray:\n        y_true = np.array(y_true)\n        \n    if type(y_pred) != np.ndarray:\n        y_pred = np.array(y_pred)\n     \n    return(np.average((np.log1p(y_pred) - np.log1p(y_true))**2)**.5)","aaf983cc":"print('RMSLE')\nprint(compute_rmsle(100,50),\n      compute_rmsle(1000,500),\n      compute_rmsle(10000,5000))","14d0054e":"print('RMSLE')\nprint(compute_rmsle(100,25),\n      compute_rmsle(1000,250),\n      compute_rmsle(10000,2500))","19e0eb24":"print('RMSE')\nprint(mean_squared_error([100],[50])**.5,\n      mean_squared_error([1000],[500])**.5, \n      mean_squared_error([10000],[5000])**.5)","e7a48c1d":"print('RMSE')\nprint(mean_squared_error([100],[25])**.5,\n      mean_squared_error([1000],[250])**.5, \n      mean_squared_error([10000],[2500])**.5)","1ed5cea9":"print(\"RMSLE: {0}\".format(compute_rmsle(df['count'],df['count_predicted'])))","3e3b93c4":"# Prepare Data for Submission to Kaggle\ndf_test = pd.read_csv(test_file,parse_dates=['datetime'])","7b639dfc":"df_test.head()","c6f88a29":"X_test =  df_test.iloc[:,1:] # Exclude datetime for prediction","18013847":"X_test.head()","d09e7757":"result = regressor.predict(X_test)","83af2270":"result[:5]","455e7448":"np.expm1(result)","97238871":"# Convert result to actual count\ndf_test[\"count\"] = np.expm1(result)","fe591033":"df_test[df_test[\"count\"] < 0]","95ec239f":"df_test[['datetime','count']].to_csv('predicted_count_log.csv',index=False)","e372ee34":"## Training and Validation Set\n### Target Variable as first column followed by input features\n### Training, Validation files do not have a column header"}}