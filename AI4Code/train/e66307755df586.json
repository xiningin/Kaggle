{"cell_type":{"ed49351f":"code","e055c5cf":"code","93c07e3d":"code","6543e4d8":"code","77fdf775":"code","c1739fa1":"code","d71c7e02":"code","59f8ae48":"code","b03b7933":"code","bd1b4a2f":"code","5cb3e8c5":"code","a369b857":"code","32907856":"code","10e9351e":"code","3d539543":"code","a367087d":"code","3a3fd887":"code","4fb6b0eb":"code","514f9ccf":"code","2ef14fd6":"code","61bce883":"code","2ace05ca":"code","d364a78c":"code","deb50940":"code","f46b1ad8":"code","773e5f39":"code","f05b13b6":"code","8dae7b0d":"code","d34ec962":"code","60a2528a":"code","483d7f50":"code","b773dcb5":"code","1a60257b":"code","2f40f0e8":"code","4fcfffb5":"code","577ed7d6":"code","d3d2db6e":"code","5775a3e2":"code","19f762fb":"code","5fbabfce":"code","08c7ba5c":"code","fdc0d874":"code","c4cf0303":"code","daf5fa8c":"code","0f345088":"code","957f4d62":"code","610d93dc":"code","ce84521f":"code","bd3690d1":"code","86e11d4f":"code","dc3fe58c":"code","9676681d":"code","e8238133":"code","6977aaa4":"code","be37294d":"code","a2f94979":"code","bc8ca1a3":"code","d48e2d01":"code","49c01343":"code","b31a1ca9":"code","ddc730c5":"markdown","85e6bb5b":"markdown","1d20708b":"markdown","00989bf1":"markdown","0b3c4dba":"markdown","76203e60":"markdown","f8591346":"markdown","4c7499a2":"markdown","81af6d00":"markdown","d4d2aace":"markdown","f90000c7":"markdown","565bcb14":"markdown","3135c07d":"markdown","4684d278":"markdown","2863ddb9":"markdown","8af88093":"markdown","459097f6":"markdown","d20e763f":"markdown","c3ace4d8":"markdown","7f20ddc2":"markdown","ae9ffba2":"markdown","472cb8ee":"markdown","9c5233c6":"markdown","df306b06":"markdown","a3a3397e":"markdown","6021f5d2":"markdown","2b24bd56":"markdown","8895f34f":"markdown","31414abb":"markdown","98a1a370":"markdown"},"source":{"ed49351f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.impute import KNNImputer\nimport os\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n","e055c5cf":"warnings.simplefilter(action = 'ignore',category = FutureWarning)\nwarnings.simplefilter(\"ignore\",category = ConvergenceWarning)\nnp.warnings.filterwarnings('ignore')\npd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)\npd.set_option('display.float_format',lambda x: '%.3f' % x)","93c07e3d":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","6543e4d8":"# To perform preprocessing on both datasets, we need to combine both datasets\ndf = train.append(test).reset_index()","77fdf775":"df.head()","c1739fa1":"df.shape","d71c7e02":"df.info()","59f8ae48":"def cat_summary(data):\n    cat_names = [col for col in data.columns if len(data[col].unique()) < 10 and col not in \"Sale Price\"]\n    value_counts= 0\n    for col in cat_names:\n        print(pd.DataFrame({col: data[col].value_counts(),\n                           \"Ratio\": 100 * data[col].value_counts()\/ len (data)}), end = \"\\n\\n\\n\")\n        value_counts += 1\n        sns.countplot(x = col, data = data)\n        plt.show()\n    print(\"We have \" +  str(value_counts) + \" categorical variables.\")\n","b03b7933":"cat_summary(df)","bd1b4a2f":"like_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) < 20] # Categorical variables with numeric values","5cb3e8c5":"num_names = [col for col in df.columns if df[col].dtypes != 'O' and col not in [\"index\",\"Id\"] and col not in like_num] # numerical variables","a369b857":"def hist_for_nums(data, numeric_cols):\n    col_counter = 0\n    \n    for col in numeric_cols:\n        data[col].hist() # histogram grafigi ciktisi\n        plt.xlabel(col) # degisken ismini x eksenine yaz\n        plt.title(col) # degisken ismini basliga yaz\n        plt.show() # ciktiyi goster\n        \n        col_counter += 1 \n        \n    print(col_counter, \"variables have been plotted\")\n    \nrcParams['figure.figsize'] = 8,6","32907856":"hist_for_nums(df, num_names)","10e9351e":"rcParams['figure.figsize'] = 20,15 # set figure size","3d539543":"corr_matrix = df[num_names].corr()\nsns.heatmap(corr_matrix, annot = True)\nplt.show()\n","a367087d":"df[\"New_Age\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\ndf[\"New_Year_Ren\"] = df[\"YrSold\"] - df[\"YearRemodAdd\"]\n\ndf.loc[(df['New_Age'] <= 10) & (df['New_Year_Ren'] <= 10),'New_Build_Cond'] = \"Excellent\"\ndf.loc[(df['New_Age'] >= 10) & (df['New_Year_Ren'] >= 10),'New_Build_Cond'] = \"Very Good\"\ndf.loc[(df['New_Age'] >= 20) & (df['New_Year_Ren'] >= 10),'New_Build_Cond'] = \"Good\"\ndf.loc[(df['New_Age'] >= 30) & (df['New_Year_Ren'] >= 10),'New_Build_Cond'] = \"Fair\"\ndf.loc[(df['New_Age'] >= 50) & (df['New_Year_Ren'] >= 10),'New_Build_Cond'] = \"Poor\"\ndf.loc[(df['New_Age'] >= 10) & (df['New_Year_Ren'] < 10),'New_Build_Cond'] = \"Very Good\"\ndf.loc[(df['New_Age'] >= 20) & (df['New_Year_Ren'] < 10),'New_Build_Cond'] = \"Good\"\ndf.loc[(df['New_Age'] >= 30) & (df['New_Year_Ren'] < 10),'New_Build_Cond'] = \"Fair\"\ndf.loc[(df['New_Age'] >= 50) & (df['New_Year_Ren'] < 10),'New_Build_Cond'] = \"Poor\"","3a3fd887":"dl = [\"YrSold\",\"YearBuilt\",\"YearRemodAdd\"]\ndf.drop(dl,axis = 1,inplace = True)","4fb6b0eb":"## This part is taken from datajameson's \"Advance Regression House Price prediction(Top 2 %)\" notebook \ndf['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\ndf['Total_sqr_footage'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] +\n                           df['1stFlrSF'] + df['2ndFlrSF'])\n\ndf['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                         df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n\ndf['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] +\n                        df['EnclosedPorch'] + df['ScreenPorch'] +\n                        df['WoodDeckSF'])\n\ndf['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\ndf['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\ndf['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n\ndf['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\ndf['Condition1_'] = df['Condition1'].apply(lambda x: 1 if x == \"Norm\" else 0)\n\ndf['SaleCondition_'] = df['SaleCondition'].apply(lambda x: 1 if x == \"Normal\" else 0)\n\ndf['GarageCond_'] = df['GarageCond'].apply(lambda x: 1 if x == \"TA\" else 0)\n\ndf['GarageQual_'] = df['GarageQual'].apply(lambda x: 1 if x == \"TA\" else 0)\n\ndf['Electrical_'] = df['Electrical'].apply(lambda x: 1 if x == \"SBrkr\" else 0)\n\ndf['Heating_'] = df['Heating'].apply(lambda x: 1 if x == \"GasA\" else 0)\n","514f9ccf":"df.drop(['Condition1', 'SaleCondition', 'Electrical', 'Heating', 'GarageQual', 'GarageCond'], axis=1, inplace=True)","2ef14fd6":"dl_1 = [\"TotalBsmtSF\",\"1stFlrSF\",\"2ndFlrSF\",\"BsmtFinSF1\",\n        \"BsmtFinSF2\",\"FullBath\",\"HalfBath\",\"BsmtFullBath\",\"BsmtHalfBath\",\n        \"OpenPorchSF\",\"3SsnPorch\",\"EnclosedPorch\",\"ScreenPorch\",\"WoodDeckSF\",\n        \"PoolArea\",\"GarageArea\",\"TotalBsmtSF\",\"Fireplaces\"]\ndf.drop(dl_1,axis = 1,inplace = True)","61bce883":"def missing_values_table(dataframe):\n    variables_with_na = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0] # variables with missing values\n\n    n_miss = dataframe[variables_with_na].isnull().sum().sort_values(ascending=False) # number of missing values in variables\n    ratio = (dataframe[variables_with_na].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False) #ratio of missing values in variables\n\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    sns.barplot(x = missing_df.index, y = 'n_miss', data=missing_df)\n    plt.xticks(rotation = 90)\n    plt.show()\n\n    return missing_df\nrcParams['figure.figsize'] = 11.7,8.27 # set figure size","2ace05ca":"missing_values_table(df)","d364a78c":"df.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"], axis = 1, inplace = True)","deb50940":"garage = list(df.columns[df.columns.str.contains('Gar',regex = True)])\ngarage\ngar_col_list = [col for col in df.columns if df[col].dtypes == 'O' and col in garage]","f46b1ad8":"for col in gar_col_list:\n    df[col].fillna(\"No Garage\",inplace = True)","773e5f39":"basement = [\"BsmtCond\",\"BsmtExposure\",\"BsmtQual\",\"BsmtFinType1\",\"BsmtFinType2\"]\nfor col in basement:\n    df[col].fillna(\"No Basement\",inplace = True)","f05b13b6":"df[\"FireplaceQu\"].fillna(\"No Fireplace\",inplace = True)","8dae7b0d":"missing_values_table(df)","d34ec962":"cat_cols = [col for col in df.columns if df[col].dtypes == 'O'] # categorical variables\ndfc = df[cat_cols] # categorical variables are assigned to dfc\ndf.drop(cat_cols,axis = 1,inplace = True) # categorical variables are dropped","60a2528a":"dfs = df[[\"SalePrice\"]] # Target variable selected\ndf.drop(\"SalePrice\",axis = 1,inplace = True) # Target variable is dropped from the dataset","483d7f50":"num_cols = [col for col in df.columns if df[col].dtypes != 'O' and col not in [\"Id\"]] # numerical variables are assigned to num_cols","b773dcb5":"imputer = KNNImputer(n_neighbors = 10)  # KNN imputer is assigned to imputer, Number of neighboring samples set as 10.\ndf_filled = imputer.fit_transform(df) # transformed version of df is assigned to df_filled\ndf = pd.DataFrame(df_filled,columns = df.columns)","1a60257b":"def outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.10)\n    quartile3 = dataframe[variable].quantile(0.90)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    \n    return low_limit, up_limit","2f40f0e8":"rcParams['figure.figsize'] = 9,6\ndef has_outliers(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    if dataframe[(dataframe[variable] < low_limit) | (dataframe[variable] > up_limit)].any(axis=None):\n        sns.boxplot(x=dataframe[variable])\n        plt.show()\n        print(variable, \"yes\")","4fcfffb5":"for i in num_cols:\n    has_outliers(df, i)","577ed7d6":"def replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nfor i in num_cols:\n    replace_with_thresholds(df, i)","d3d2db6e":"for i in num_cols:\n    has_outliers(df, i)","5775a3e2":"# Target variable and categorical variables are concatenated\ndf = pd.concat([df,dfs],axis = 1)\ndf = pd.concat([df,dfc],axis = 1)","19f762fb":"cat_cols = [col for col in df.columns if df[col].dtypes =='O']","5fbabfce":"def one_hot_encoder(dataframe,categorical_cols,nan_as_category=True):\n    original_columns = list(dataframe.columns)\n    dataframe = pd.get_dummies(dataframe,columns = categorical_cols,dummy_na = nan_as_category,drop_first = True)\n    new_columns = [c for c in dataframe.columns if c not in original_columns]\n    return dataframe,new_columns\n","08c7ba5c":"df,new_cols_ohe = one_hot_encoder(df,cat_cols)","fdc0d874":"df.drop([\"index\",\"Id\"],axis=1,inplace=True)","c4cf0303":"train_df = df[df['SalePrice'].notnull()]\n\nX = train_df.drop('SalePrice',axis = 1)\ny = train_df[[\"SalePrice\"]]","daf5fa8c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)\nmodels = [('LR', LinearRegression()),\n          ('Ridge', Ridge()),\n          ('Lasso', Lasso()),\n          ('ElasticNet', ElasticNet()),\n          ('XGB', XGBRegressor()),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"RandomForest\", RandomForestRegressor())]","0f345088":"results = []\nnames = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    result = np.sqrt(mean_squared_error(y_test, y_pred))\n    results.append(result)\n    names.append(name)\n    msg = \"%s: %f\" % (name, result)\n    print(msg)","957f4d62":"xgb = XGBRegressor().fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","610d93dc":"xgb_params = {\"learning_rate\": [0.1,0.01,0.5],\n              \"max_depth\": [5,8,15,20],\n              \"n_estimators\": [100,200,500,1000],\n              \"colsample_bytree\": [0.4,0.7,1]}","ce84521f":"xgb_cv_model = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train,y_train)\nxgb_cv_model.best_params_","bd3690d1":"# Final Model\nxgb_tuned = XGBRegressor(**xgb_cv_model.best_params_).fit(X_train,y_train)\ny_pred = xgb_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","86e11d4f":"lgb_model = LGBMRegressor()","dc3fe58c":"lgbm_params = {\"learning_rate\": [0.01,0.001,0.1,0.5,1],\n               \"n_estimators\": [200,500,1000,5000],\n               \"max_depth\": [6,8,10,15,20],\n               \"colsample_bytree\": [1,0.8,0.5,0.4]}","9676681d":"lgbm_cv_model = GridSearchCV(lgb_model,\n                             lgbm_params,\n                             cv = 10,\n                             n_jobs = -1,\n                             verbose = 2).fit(X_train,y_train)\n","e8238133":"lgbm_cv_model.best_params_","6977aaa4":"# Final Model\nlgbm_tuned = LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train,y_train)\ny_pred = lgbm_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))\n","be37294d":"rf_model = RandomForestRegressor(random_state = 42).fit(X_train,y_train)\ny_pred = rf_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","a2f94979":"rf_params = {\"max_depth\": [3,5,8,10,15,None],\n              \"max_features\": [5,10,15,20,50,100],\n              \"n_estimators\": [200,500,1000],\n              \"min_samples_split\": [2,5,10,20,30,50]}","bc8ca1a3":"rf_cv_model = GridSearchCV(rf_model,rf_params,cv = 10,n_jobs = -1,verbose = 1).fit(X_train,y_train)\nrf_cv_model.best_params_","d48e2d01":"# Final Model\nrf_tuned = RandomForestRegressor(**rf_cv_model.best_params_).fit(X_train,y_train)\ny_pred = rf_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","49c01343":"tuned_models = [(\"LightGBM_1\",lgbm_tuned),\n                (\"XGBM\",xgb_tuned),\n                (\"RandomForest\",rf_tuned)]","b31a1ca9":"for name,model in tuned_models:\n    y_pred = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n    msg = \"%s: (%f)\" % (name,rmse)\n    print(msg)\n","ddc730c5":"# 4. Overview","85e6bb5b":"* __It seems that most of the variables are not normally distributed__","1d20708b":"# 8. Outlier Analysis","00989bf1":"# Introduction","0b3c4dba":"# 7. Missing Data Analysis ","76203e60":"## 11.1 XGB ","f8591346":"## 11.3 Random Forest","4c7499a2":"* __Since we concatenated train and test datasets into one, target variable(\"SalePrice\") seems like it contains missing values. Missing values of SalePrice comes from the test dataset.__","81af6d00":"# 11. Parameter Optimization","d4d2aace":"* __We can apply KNN imputer for numerical variables__","f90000c7":"# 10. Model","565bcb14":"* __The na values of the variables specified in the Garage and Basement lists and FireplaceQu means that those variables do not have the corresponding object. So these can be changed to \"No Garage\", \"No Basement\" and \"No Fireplace\" respectively.__","3135c07d":"# 3. Read Train-Test Data","4684d278":"# 5. Exploratory Data Analysis (EDA)","2863ddb9":"* I started with feature engineering in order to apply preprocessing operations to those variables as well","8af88093":"* __For the rest of the variables, we can apply modes of those variables or simply leave it as it is. By doing so, we can let models to handle the missing values.__","459097f6":"* __Variables PoolQC, MiscFeature, Alley, Fence contains mostly missing observations. According to Description.txt, Na values in these variables mean that the corresponding object is absent.__\n\n* __For example, Na value in the Fence variable means that there is no fence in that house. The absence of fences in the vast majority of homes means that this variable has no effect on estimating the house price, so it can be removed from the dataset.__","d20e763f":"## 11.4 Tuned Models","c3ace4d8":"## 11.2 LightGBM","7f20ddc2":".# 1. Import Libraries","ae9ffba2":"* We have 82 different variables with 2919 observations","472cb8ee":"* We get all categorical variables with their number of observations and their ratios.","9c5233c6":"In this notebook, I will try to apply some basic data preprocessing such as missing data analysis, outlier analysis encoding, as well as feature engineering and model tuning operations.","df306b06":"## 5.2 Numerical Variable Analysis","a3a3397e":"# 9. Encoding","6021f5d2":"## 5.1 Categorical Variable Analysis","2b24bd56":"In kaggle, the competition of house price estimation is described as follows:\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","8895f34f":"# 6. Feature Engineering","31414abb":"# 2. Settings","98a1a370":"__Correlation Matrix__"}}