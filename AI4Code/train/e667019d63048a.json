{"cell_type":{"eb24f53f":"code","bab6ff4c":"code","09394253":"code","b07a5d42":"code","34f1c67b":"code","ab1ea17b":"code","7363a035":"code","6a592448":"code","1bb1364b":"code","b8b1aa2a":"code","aa12aea9":"code","3b142838":"code","92258cb4":"code","99c892eb":"code","ed4c5e4e":"code","5ec268e2":"code","ccc3d094":"code","1719babc":"code","e8e21a30":"code","9fb8702b":"code","d72e0c7a":"code","dd283eba":"code","8a892f19":"markdown","1a01e231":"markdown","fe9125e8":"markdown","5b16160e":"markdown","26bd55c1":"markdown","26780b5a":"markdown","8fa4550f":"markdown","16c0981c":"markdown","a5e8bbe9":"markdown","cf82f568":"markdown","75363fab":"markdown","f17273a3":"markdown","0f82ab73":"markdown","62011d3f":"markdown","6de3be55":"markdown","4d8779f7":"markdown","52afc8f4":"markdown"},"source":{"eb24f53f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datatable as dt\n\nimport os","bab6ff4c":"%%time\n\nexample_sample_submission = dt.fread(\"..\/input\/jane-street-market-prediction\/example_sample_submission.csv\").to_pandas()\nexample_test = dt.fread(\"..\/input\/jane-street-market-prediction\/example_test.csv\").to_pandas()\nfeatures = dt.fread(\"..\/input\/jane-street-market-prediction\/features.csv\").to_pandas()\ntrain = dt.fread(\"..\/input\/jane-street-market-prediction\/train.csv\").to_pandas()","09394253":"display(example_sample_submission.head())\ndisplay(example_test.head())\ndisplay(features.head())\ndisplay(train.head())","b07a5d42":"total_by_date = train.assign(total=lambda x:1).loc[:,['date','total']].groupby(\"date\").count().reset_index()\nfig,ax = plt.subplots(figsize = (15,5))\nsns.lineplot(data=total_by_date, x='date', y='total', ax=ax)","34f1c67b":"def plot_over_time(data, cols, ylabel):\n    fig,ax = plt.subplots(figsize = (15,5))\n\n    for col in cols:\n        sns.lineplot(data=data, x='date', y=col, ax=ax, label=col)\n\n    ax.legend()\n    ax.set(xlabel='date', ylabel=ylabel)\n\ndef plot_in_groups(data, cols, n=5):\n    while len(cols) > 0:\n        plot_over_time(data, cols[:n], ylabel='missing_count')\n        cols = cols[n:]\n        \nfeatures = [col for col in train.columns if \"feature\" in col]\nfeatures_resp_weight = features + ['resp', 'weight']\n\nmissing_over_time = train[['date']+features_resp_weight]\\\n    .groupby('date')\\\n    .apply(lambda df: df.isnull().sum()\/len(df))\\\n    .drop(columns=['date'])\\\n    .reset_index()\n\nmax_missing_count = missing_over_time.drop(columns=['date']).max(axis=0).drop(['resp', 'weight'])","ab1ea17b":"plot_in_groups(missing_over_time,max_missing_count[max_missing_count<0.01].index)","7363a035":"plot_in_groups(missing_over_time,max_missing_count[(max_missing_count>=0.01) & (max_missing_count<0.05)].index)","6a592448":"plot_in_groups(missing_over_time,max_missing_count[(max_missing_count>=0.05) & (max_missing_count<0.10)].index)","1bb1364b":"plot_in_groups(missing_over_time, max_missing_count[max_missing_count>=0.10].index)","b8b1aa2a":"plot_over_time(missing_over_time, ['resp'], ylabel='missing_count')","aa12aea9":"plot_over_time(missing_over_time, ['weight'], ylabel='missing_count')","3b142838":"# Arbitrary maximum number of unique values to consider the feature categorical\nmax_categories_count = 50\n\nfeatures_nunique = {feature:train[feature].nunique() for feature in features}\n\ncategorical_features = [feature for (feature, nunique) in features_nunique.items() if nunique <= max_categories_count]\nnumerical_features = [feature for (feature, nunique) in features_nunique.items() if nunique > max_categories_count]\n\nprint(\"Categorical Features:\")\nprint(\", \".join(categorical_features))\n\nprint(\"\\nNumerical Features:\")\nprint(\", \".join(numerical_features))","92258cb4":"feature_0_mean = train[['date','feature_0']].replace({'feature_0': {-1:0}}).groupby('date').mean().reset_index()\n\nfig, ax = plt.subplots(figsize = (15,5))\n\nsns.lineplot(data=feature_0_mean, x='date', y='feature_0', ax=ax)\nax.set(xlabel='date', ylabel='positive class proportion')","99c892eb":"feature_mean_over_time = train[['date']+numerical_features]\\\n    .groupby('date')\\\n    .mean()\\\n    .reset_index()\n\nmax_mean = feature_mean_over_time.drop(columns=['date']).max(axis=0)","ed4c5e4e":"plot_in_groups(feature_mean_over_time,max_mean[max_mean<1].index)","5ec268e2":"plot_in_groups(feature_mean_over_time,max_mean[(max_mean>=1) & (max_mean<5)].index)","ccc3d094":"plot_in_groups(feature_mean_over_time,max_mean[(max_mean>=5) & (max_mean<10)].index)","1719babc":"plot_in_groups(feature_mean_over_time,max_mean[max_mean>=10].index)","e8e21a30":"plot_data = train.assign(agg_date=lambda df: (df.date\/10).round())\nfig,ax = plt.subplots(figsize = (15,5))\n\nsns.boxplot(x=\"agg_date\", y=\"resp\", data=plot_data, ax=ax, whis=[1,99])","9fb8702b":"fig,ax = plt.subplots(figsize = (15,5))\n\nsns.boxplot(x=\"agg_date\", y=\"resp\", data=plot_data, ax=ax, whis=[1,99], showfliers=False)","d72e0c7a":"plot_data = train.assign(agg_date=lambda df: (df.date\/10).round())\nfig,ax = plt.subplots(figsize = (15,5))\n\nsns.boxplot(x=\"agg_date\", y=\"weight\", data=plot_data, ax=ax, whis=[1,99])","dd283eba":"plot_data = train.assign(agg_date=lambda df: (df.date\/10).round())\nfig,ax = plt.subplots(figsize = (15,5))\n\nsns.boxplot(x=\"agg_date\", y=\"weight\", data=plot_data, ax=ax, whis=[0,95], showfliers=False)","8a892f19":"### Categorical Features","1a01e231":"Nothing noteworthy here. The rows are roughly evenly spread across all dates.","fe9125e8":"What we want to see here is if there are major shifts in the missing count. It is noticeable that right before day 300 several features showed a big decrease in missing counts, but this shouldn't be a problem.\n<br><br>From the plots we can see that this is not the case and we can safely use all features with regard to missing count.\n<br><br>The target `resp` and `weight` don't have any missing values, as expected.","5b16160e":"<a class=\"anchor\" id=\"target_value_over_time\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Target Values Over Time<\/center>\n\nHere we will evaluate the distribution of the target over time.","26bd55c1":"In the first plot (including outliers) we can see that historicaly the target (`resp`) values range from roughly -0.6 to 0.5. In the second plot (that disregards the first and last percentile), we can see that 98% of the values falls roughly within the [-0.1, 0.1] range.\n<br><br>\nOne important thing to notice is that the target values are most of the time roughly symetrical around 0. This allows us to use classification algorithms without worrying about class balancing.\n<br><br>\nLastly, I wonder whether this variable is the percentual change of some sort of stock.","26780b5a":"<a class=\"anchor\" id=\"rows_over_time\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Rows Count Over Time<\/center>\n\nHere we will evaluate the number of examples available for each date.","8fa4550f":"The mean values of the numerical features remains resonably stable over time, with no major shifts.\n<p> It woud be ideal to evaluate the distribution of the features over time, but this will be left to a future version.","16c0981c":"<a class=\"anchor\" id=\"value_over_time\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Features Values Over Time<\/center>\n\nHere we will evaluate the mean value of each feature over time.","a5e8bbe9":"The feature_0 distribution over time is fairly stable, we should be able to use it safely.","cf82f568":"## <center style=\"background-color:Gainsboro; width:40%;\">Summary<\/center>\n* [Introduction](#introduction)\n* [Reading Data](#reading_data)\n* [Rows Count Over Time](#rows_over_time)\n* [Null Counts by Feature Over Time](#missing_over_time)\n* [Features Values Over Time](#value_over_time)\n* [Conclusion](#conclusion)","75363fab":"### Numerical Features","f17273a3":"<a class=\"anchor\" id=\"introduction\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Introduction<\/center>\n\nThis notebook evalutes the features and target behaviour over time. I want to evaluate here if there are major shifts in missing count or features\/target distribution. This is important because we want to avoid our model to learn from patterns that are no long applicable.","0f82ab73":"<a class=\"anchor\" id=\"missing_over_time\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Null Counts by Feature Over Time<\/center>\n\nHere we will evaluate the proportion of null values for each feature over time.","62011d3f":"<a class=\"anchor\" id=\"conclusion\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Conclusion<\/center>\n\nThis initial analysis shows that it is fairly safe to use all the features provided, once we didn't detect any major shift in their behaviour over time.","6de3be55":"In the first plot (including outliers) we can see that `weight` values can go as high as 170 or so, but the second plot (excluding the 5% highest values of each period) shows that the majority of the values (95%) falls within the range [0,16].\n<br><br>\nThis might be the stock value, which is why it is as much important as `resp` in regard to the evaluation.","4d8779f7":"<a class=\"anchor\" id=\"reading_data\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Reading Data<\/center>\n","52afc8f4":"<a class=\"anchor\" id=\"weight_value_over_time\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Weight Values Over Time<\/center>\n\nHere we will evaluate the distribution of the weight over time."}}