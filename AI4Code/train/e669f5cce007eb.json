{"cell_type":{"59227a71":"code","5d16b864":"code","3c6eb41a":"code","170d1f41":"code","b9c803ce":"code","1cdf9e92":"code","0d83042a":"code","ca985ef0":"code","699b0283":"code","c3816c79":"code","0c8bea60":"code","9e7c7a0b":"code","9caf9659":"code","9529cca5":"code","4b0b259a":"code","04e709fe":"code","766edbe1":"code","0b4e47a4":"code","4c1882e4":"code","86bfff88":"code","c240d461":"code","9c00e88d":"code","40824884":"code","f9b9a246":"code","781b7755":"code","bfe3d13a":"markdown","1896b974":"markdown","32fece62":"markdown","abd04e5c":"markdown","542adc6b":"markdown","db6c2df3":"markdown"},"source":{"59227a71":"import numpy as np \nimport pandas as pd \nimport os","5d16b864":"import tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n\nclass AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https:\/\/arxiv.org\/pdf\/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch size * en_seq_len * latent_dim\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>', U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n            if verbose:\n                print('Ws+Uh>', Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","3c6eb41a":"import re\n\nlines = open('..\/input\/chatbot-data\/cornell movie-dialogs corpus\/movie_lines.txt', encoding='utf-8',\n             errors='ignore').read().split('\\n')\n\nconvers = open('..\/input\/chatbot-data\/cornell movie-dialogs corpus\/movie_conversations.txt', encoding='utf-8',\n             errors='ignore').read().split('\\n')\n","170d1f41":"len(lines)","b9c803ce":"\nexchn = []\nfor conver in convers:\n    exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())\n\ndiag = {}\nfor line in lines:\n    diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]\n\n\n\n## delete\ndel(lines, convers, conver, line)\n\n\n\nquestions = []\nanswers = []\n\nfor conver in exchn:\n    for i in range(len(conver) - 1):\n        questions.append(diag[conver[i]])\n        answers.append(diag[conver[i+1]])\n        \n        \n        \n\n## delete\ndel(diag, exchn, conver, i)\n\n\n###############################\n#        max_len = 13         #\n###############################\n\nsorted_ques = []\nsorted_ans = []\nfor i in range(len(questions)):\n    if len(questions[i]) < 13:\n        sorted_ques.append(questions[i])\n        sorted_ans.append(answers[i])\n\n\n\n###############################\n#                             #\n###############################\n\n\n\n\ndef clean_text(txt):\n    txt = txt.lower()\n    txt = re.sub(r\"i'm\", \"i am\", txt)\n    txt = re.sub(r\"he's\", \"he is\", txt)\n    txt = re.sub(r\"she's\", \"she is\", txt)\n    txt = re.sub(r\"that's\", \"that is\", txt)\n    txt = re.sub(r\"what's\", \"what is\", txt)\n    txt = re.sub(r\"where's\", \"where is\", txt)\n    txt = re.sub(r\"\\'ll\", \" will\", txt)\n    txt = re.sub(r\"\\'ve\", \" have\", txt)\n    txt = re.sub(r\"\\'re\", \" are\", txt)\n    txt = re.sub(r\"\\'d\", \" would\", txt)\n    txt = re.sub(r\"won't\", \"will not\", txt)\n    txt = re.sub(r\"can't\", \"can not\", txt)\n    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n    return txt\n\nclean_ques = []\nclean_ans = []\n\nfor line in sorted_ques:\n    clean_ques.append(clean_text(line))\n        \nfor line in sorted_ans:\n    clean_ans.append(clean_text(line))\n\n\n\n## delete\ndel(answers, questions, line)\n\n\n###############################\n#                             #\n###############################\n\n\nfor i in range(len(clean_ans)):\n    clean_ans[i] = ' '.join(clean_ans[i].split()[:11])\n\n\n\n###############################\n#                             #\n###############################\n\ndel(sorted_ans, sorted_ques)\n\n\n## trimming\nclean_ans=clean_ans[:30000]\nclean_ques=clean_ques[:30000]\n## delete\n\n\n###  count occurences ###\nword2count = {}\n\nfor line in clean_ques:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\nfor line in clean_ans:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n\n## delete\ndel(word, line)\n\n\n###  remove less frequent ###\nthresh = 5\n\nvocab = {}\nword_num = 0\nfor word, count in word2count.items():\n    if count >= thresh:\n        vocab[word] = word_num\n        word_num += 1\n        \n## delete\ndel(word2count, word, count, thresh)       \ndel(word_num)        \n\n\n\nfor i in range(len(clean_ans)):\n    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'\n\n\n\ntokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\nx = len(vocab)\nfor token in tokens:\n    vocab[token] = x\n    x += 1\n    \n    \n\nvocab['cameron'] = vocab['<PAD>']\nvocab['<PAD>'] = 0\n\n## delete\ndel(token, tokens) \ndel(x)\n\n### inv answers dict ###\ninv_vocab = {w:v for v, w in vocab.items()}\n\n\n\n## delete\ndel(i)\n\n\n\nencoder_inp = []\nfor line in clean_ques:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])\n        \n    encoder_inp.append(lst)\n\ndecoder_inp = []\nfor line in clean_ans:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])        \n    decoder_inp.append(lst)\n\n### delete\ndel(clean_ans, clean_ques, line, lst, word)\n\n\n\n\n\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nencoder_inp = pad_sequences(encoder_inp, 13, padding='post', truncating='post')\ndecoder_inp = pad_sequences(decoder_inp, 13, padding='post', truncating='post')\n\n\n\n\ndecoder_final_output = []\nfor i in decoder_inp:\n    decoder_final_output.append(i[1:]) \n\ndecoder_final_output = pad_sequences(decoder_final_output, 13, padding='post', truncating='post')\n\n\ndel(i)\n\n","1cdf9e92":"# decoder_final_output, decoder_final_input, encoder_final, vocab, inv_vocab\n\nVOCAB_SIZE = len(vocab)\nMAX_LEN = 13\n\nprint(decoder_final_output.shape, decoder_inp.shape, encoder_inp.shape, len(vocab), len(inv_vocab), inv_vocab[0])","0d83042a":"inv_vocab[16]","ca985ef0":"#print(len(decoder_final_input), MAX_LEN, VOCAB_SIZE)\n#decoder_final_input[0]\n#decoder_output_data = np.zeros((len(decoder_final_input), MAX_LEN, VOCAB_SIZE), dtype=\"float32\")\n#print(decoder_output_data.shape)\n#decoder_final_input[80]","699b0283":"from tensorflow.keras.utils import to_categorical\ndecoder_final_output = to_categorical(decoder_final_output, len(vocab))","c3816c79":"decoder_final_output.shape","0c8bea60":"\nembeddings_index = {}\nwith open('..\/input\/glove6b50d\/glove.6B.50d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\nprint(\"Glove Loded!\")\n","9e7c7a0b":"\nembedding_dimention = 50\ndef embedding_matrix_creater(embedding_dimention, word_index):\n    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n          # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\nembedding_matrix = embedding_matrix_creater(50, word_index=vocab)    \n","9caf9659":"del(embeddings_index)","9529cca5":"embedding_matrix.shape","4b0b259a":"embedding_matrix[0]","04e709fe":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Dropout, Attention","766edbe1":"embed = Embedding(VOCAB_SIZE+1, \n                  50, \n                  \n                  input_length=13,\n                  trainable=True)\n\nembed.build((None,))\nembed.set_weights([embedding_matrix])\n","0b4e47a4":"enc_inp = Input(shape=(13, ))","4c1882e4":"#embed = Embedding(VOCAB_SIZE+1, 50, mask_zero=True, input_length=13)(enc_inp)\nenc_embed = embed(enc_inp)\nenc_lstm = Bidirectional(LSTM(400, return_state=True, dropout=0.05, return_sequences = True))\n\nencoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)\n\nstate_h = Concatenate()([forward_h, backward_h])\nstate_c = Concatenate()([forward_c, backward_c])\n\nenc_states = [state_h, state_c]\n\n\ndec_inp = Input(shape=(13, ))\ndec_embed = embed(dec_inp)\ndec_lstm = LSTM(400*2, return_state=True, return_sequences=True, dropout=0.05)\noutput, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n\n# attention\nattn_layer = AttentionLayer()\nattn_op, attn_state = attn_layer([encoder_outputs, output])\ndecoder_concat_input = Concatenate(axis=-1)([output, attn_op])\n\n\ndec_dense = Dense(VOCAB_SIZE, activation='softmax')\nfinal_output = dec_dense(decoder_concat_input)\n\nmodel = Model([enc_inp, dec_inp], final_output)\n\nmodel.summary()","86bfff88":"import keras\nimport tensorflow as tf","c240d461":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","9c00e88d":"model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs=10, batch_size=24, validation_split=0.15)","40824884":"model.save('chatbot.h5')\nmodel.save_weights('chatbot_weights.h5')","f9b9a246":"enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])\n\n\ndecoder_state_input_h = tf.keras.layers.Input(shape=( 400 * 2,))\ndecoder_state_input_c = tf.keras.layers.Input(shape=( 400 * 2,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n\ndecoder_outputs, state_h, state_c = dec_lstm(dec_embed , initial_state=decoder_states_inputs)\n\n\ndecoder_states = [state_h, state_c]\n\n#decoder_output = dec_dense(decoder_outputs)\n\ndec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],\n                                      [decoder_outputs] + decoder_states)\n","781b7755":"\nfrom keras.preprocessing.sequence import pad_sequences\nprint(\"##########################################\")\nprint(\"#       start chatting ver. 1.0          #\")\nprint(\"##########################################\")\n\n\nprepro1 = \"\"\nwhile prepro1 != 'q':\n    \n    prepro1 = input(\"you : \")\n    try:\n        prepro1 = clean_text(prepro1)\n        prepro = [prepro1]\n        \n        txt = []\n        for x in prepro:\n            lst = []\n            for y in x.split():\n                try:\n                    lst.append(vocab[y])\n                except:\n                    lst.append(vocab['<OUT>'])\n            txt.append(lst)\n        txt = pad_sequences(txt, 13, padding='post')\n\n\n        ###\n        enc_op, stat = enc_model.predict( txt )\n\n        empty_target_seq = np.zeros( ( 1 , 1) )\n        empty_target_seq[0, 0] = vocab['<SOS>']\n        stop_condition = False\n        decoded_translation = ''\n\n\n        while not stop_condition :\n\n            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + stat )\n\n            ###\n            ###########################\n            attn_op, attn_state = attn_layer([enc_op, dec_outputs])\n            decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])\n            decoder_concat_input = dec_dense(decoder_concat_input)\n            ###########################\n\n            sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n\n            sampled_word = inv_vocab[sampled_word_index] + ' '\n\n            if sampled_word != '<EOS> ':\n                decoded_translation += sampled_word           \n\n\n            if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n                stop_condition = True\n\n            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n            empty_target_seq[ 0 , 0 ] = sampled_word_index\n            stat = [ h , c ] \n\n        print(\"chatbot attention : \", decoded_translation )\n        print(\"==============================================\")\n\n    except:\n        print(\"sorry didn't got you , please type again :( \")\n\n\n","bfe3d13a":"# Data Preprocess","1896b974":"# Glove Embedding","32fece62":"# Attention Class","abd04e5c":"# inferece","542adc6b":"# Attention Inference\n","db6c2df3":"# Model"}}