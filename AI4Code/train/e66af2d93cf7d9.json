{"cell_type":{"a2fdab2f":"code","1b2725c1":"code","1d1b96f8":"code","c425c140":"code","843f3bc9":"code","827eca91":"code","af2ff657":"code","2a16f08d":"code","1aceed42":"code","223e66c4":"code","6d8ac833":"code","e2447dd7":"code","803b30ff":"code","53aa8f27":"code","c7c460c3":"code","94b2e8b8":"code","a473b570":"code","6c69d335":"code","69ba8458":"code","052d9de6":"code","292c5469":"code","208a7b9d":"code","d519cf55":"code","08ca6d88":"code","0925bf4e":"code","8a5cfc8a":"code","8a5e6301":"code","8278e2b5":"code","851ee8db":"code","774474ed":"code","018b700b":"code","71229d9f":"code","484391f7":"code","2f56e3c0":"code","e515c8ee":"code","4962bd5d":"code","492966c1":"code","72f0deda":"code","3bc37c34":"code","ff532970":"code","800dec90":"code","59f720ab":"code","f9d5bf75":"code","37ffcae8":"code","3016a7b4":"code","278ba8b0":"code","8ff19654":"code","3f9dd309":"code","a07eaa14":"markdown","33ca4077":"markdown","30bce48a":"markdown","190fc7d8":"markdown","5aead942":"markdown","db575322":"markdown","58702c29":"markdown","8307bf2f":"markdown","e23db423":"markdown","c1e41ff4":"markdown","3600742d":"markdown","d0eff545":"markdown","245782b9":"markdown","68d4c525":"markdown","b11395d2":"markdown","bfc477c2":"markdown"},"source":{"a2fdab2f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nimport os\nimport glob\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nimport gc\n\nfrom sklearn.model_selection import train_test_split, KFold\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","1b2725c1":"class Config:\n    data_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n    seed = 42","1d1b96f8":"train = pd.read_csv(Config.data_dir + 'train.csv')\ntrain.head()","c425c140":"train.stock_id.unique()","843f3bc9":"test = pd.read_csv(Config.data_dir + 'test.csv')\ntest.head()","827eca91":"display(train.groupby('stock_id').size())\n\nprint(\"\\nUnique size values\")\ndisplay(train.groupby('stock_id').size().unique())","af2ff657":"def get_trade_and_book_by_stock_and_time_id(stock_id, time_id=None, dataType = 'train'):\n    book_example = pd.read_parquet(f'{Config.data_dir}book_{dataType}.parquet\/stock_id={stock_id}')\n    trade_example =  pd.read_parquet(f'{Config.data_dir}trade_{dataType}.parquet\/stock_id={stock_id}')\n    if time_id:\n        book_example = book_example[book_example['time_id']==time_id]\n        trade_example = trade_example[trade_example['time_id']==time_id]\n    book_example.loc[:,'stock_id'] = stock_id\n    trade_example.loc[:,'stock_id'] = stock_id\n    return book_example, trade_example","2a16f08d":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef calculate_wap1(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2'] + df['ask_size2']\n    \n    x = (a1\/b1 + a2\/b2)\/ 2\n    \n    return x\n\n\ndef calculate_wap2(df):\n        \n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']\n    \n    x = (a1 + a2)\/ b\n    return x\n\ndef realized_volatility_per_time_id(file_path, prediction_column_name):\n\n    stock_id = file_path.split('=')[1]\n\n    df_book = pd.read_parquet(file_path)\n    df_book['wap1'] = calculate_wap1(df_book)\n    df_book['wap2'] = calculate_wap2(df_book)\n\n    df_book['log_return1'] = df_book.groupby(['time_id'])['wap1'].apply(log_return)\n    df_book['log_return2'] = df_book.groupby(['time_id'])['wap2'].apply(log_return)\n    df_book = df_book[~df_book['log_return1'].isnull()]\n\n    df_rvps =  pd.DataFrame(df_book.groupby(['time_id'])[['log_return1', 'log_return2']].agg(realized_volatility)).reset_index()\n    df_rvps[prediction_column_name] = 0.6 * df_rvps['log_return1'] + 0.4 * df_rvps['log_return2']\n\n    df_rvps['row_id'] = df_rvps['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_rvps[['row_id',prediction_column_name]]","1aceed42":"def get_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     #min_size = ('size', 'min'),\n                                                     #min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df","223e66c4":"def get_stock_stat(stock_id : int, dataType = 'train'):\n    \n    book_subset, trade_subset = get_trade_and_book_by_stock_and_time_id(stock_id, dataType=dataType)\n    book_subset.sort_values(by=['time_id', 'seconds_in_bucket'])\n\n    ## book data processing\n    \n    book_subset['bas'] = (book_subset[['ask_price1', 'ask_price2']].min(axis = 1)\n                                \/ book_subset[['bid_price1', 'bid_price2']].max(axis = 1)\n                                - 1)                               \n\n    \n    book_subset['wap1'] = calculate_wap1(book_subset)\n    book_subset['wap2'] = calculate_wap2(book_subset)\n    \n    book_subset['log_return_bid_price1'] = np.log(book_subset['bid_price1'].pct_change() + 1)\n    book_subset['log_return_ask_price1'] = np.log(book_subset['ask_price1'].pct_change() + 1)\n    # book_subset['log_return_bid_price2'] = np.log(book_subset['bid_price2'].pct_change() + 1)\n    # book_subset['log_return_ask_price2'] = np.log(book_subset['ask_price2'].pct_change() + 1)\n    book_subset['log_return_bid_size1'] = np.log(book_subset['bid_size1'].pct_change() + 1)\n    book_subset['log_return_ask_size1'] = np.log(book_subset['ask_size1'].pct_change() + 1)\n    # book_subset['log_return_bid_size2'] = np.log(book_subset['bid_size2'].pct_change() + 1)\n    # book_subset['log_return_ask_size2'] = np.log(book_subset['ask_size2'].pct_change() + 1)\n    book_subset['log_ask_1_div_bid_1'] = np.log(book_subset['ask_price1'] \/ book_subset['bid_price1'])\n    book_subset['log_ask_1_div_bid_1_size'] = np.log(book_subset['ask_size1'] \/ book_subset['bid_size1'])\n    \n\n    book_subset['log_return1'] = (book_subset.groupby(by = ['time_id'])['wap1'].\n                                  apply(log_return).\n                                  reset_index(drop = True).\n                                  fillna(0)\n                                 )\n    book_subset['log_return2'] = (book_subset.groupby(by = ['time_id'])['wap2'].\n                                  apply(log_return).\n                                  reset_index(drop = True).\n                                  fillna(0)\n                                 )\n    \n    stock_stat = pd.merge(\n        book_subset.groupby(by = ['time_id'])['log_return1'].agg(realized_volatility).reset_index(),\n        book_subset.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return2'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_bid_price1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_ask_price1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_bid_size1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_return_ask_size1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_ask_1_div_bid_1'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat = pd.merge(\n        stock_stat,\n        book_subset.groupby(by = ['time_id'])['log_ask_1_div_bid_1_size'].agg(realized_volatility).reset_index(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    \n    \n    stock_stat['stock_id'] = stock_id\n    \n    # Additional features that can be added. Referenced from https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-train\/data\n    \n    # trade_subset_agg = get_agg_info(trade_subset)\n    \n    #     stock_stat = pd.merge(\n    #         stock_stat,\n    #         trade_subset_agg,\n    #         on = ['stock_id', 'time_id'],\n    #         how = 'left'\n    #     )\n    \n    ## trade data processing \n    \n    return stock_stat\n\ndef get_data_set(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df","6d8ac833":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","e2447dd7":"def plot_feature_importance(df, model):\n    feature_importances_df = pd.DataFrame({\n        'feature': df.columns,\n        'importance_score': model.feature_importances_\n    })\n    plt.rcParams[\"figure.figsize\"] = [10, 5]\n    ax = sns.barplot(x = \"feature\", y = \"importance_score\", data = feature_importances_df)\n    ax.set(xlabel=\"Features\", ylabel = \"Importance Score\")\n    plt.xticks(rotation=45)\n    plt.show()\n    return feature_importances_df","803b30ff":"book_stock_1, trade_stock_1 = get_trade_and_book_by_stock_and_time_id(1, 5)\ndisplay(book_stock_1.shape)\ndisplay(trade_stock_1.shape)","53aa8f27":"book_stock_1.head()","c7c460c3":"trade_stock_1.head()","94b2e8b8":"%%time\ntrain_stock_stat_df = get_data_set(train.stock_id.unique(), dataType = 'train')\ntrain_stock_stat_df.head()","a473b570":"train_data_set = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntrain_data_set.head()","6c69d335":"train_data_set.info()","69ba8458":"%%time\ntest_stock_stat_df = get_data_set(test['stock_id'].unique(), dataType = 'test')\ntest_stock_stat_df","052d9de6":"test_data_set = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest_data_set.fillna(-999, inplace=True)\ntest_data_set","292c5469":"train_data_set.to_pickle('train_features_df.pickle')\ntest_data_set.to_pickle('test_features_df.pickle')","208a7b9d":"x = gc.collect()","d519cf55":"X_display = train_data_set.drop(['stock_id', 'time_id', 'target'], axis = 1)\nX = X_display.values\ny = train_data_set['target'].values\n\nX.shape, y.shape","08ca6d88":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=Config.seed, shuffle=False)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","0925bf4e":"rs = Config.seed","8a5cfc8a":"import optuna\nfrom optuna.samplers import TPESampler\n\ndef objective(trial, data=X, target=y):\n    \n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=rs, shuffle=False)\n    \n    param = {\n        'tree_method':'gpu_hist', \n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)}\n    \n    model = XGBRegressor(**param)\n    \n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n    model.fit(X_train ,y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n    \n    preds = model.predict(X_test)\n    \n    rmspe = rmspe(y_test, preds)\n    \n    return rmspe","8a5e6301":"study = optuna.create_study(sampler=TPESampler(), direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective, n_trials=1000, gc_after_trial=True)","8278e2b5":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","851ee8db":"optuna.visualization.plot_optimization_history(study)","774474ed":"optuna.visualization.plot_param_importances(study)","018b700b":"best_xgbparams = study.best_params\nbest_xgbparams","71229d9f":"xgb = XGBRegressor(**best_xgbparams, tree_method='gpu_hist')","484391f7":"%%time\nxgb.fit(X_train ,y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n\npreds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 5)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 5)\nprint(f'Performance of the Tuned XGB prediction: R2 score: {R2}, RMSPE: {RMSPE}')","2f56e3c0":"def objective(trial):\n    \n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=rs, shuffle=False)\n    valid = [(X_test, y_test)]\n    \n    param = {\n        \"device\": \"gpu\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 500),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4000),\n#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100000, 700000),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)}\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n    model = LGBMRegressor(**param)\n    \n    model.fit(X_train, y_train, eval_set=valid, verbose=False, callbacks=[pruning_callback], early_stopping_rounds=100)\n\n    preds = model.predict(X_test)\n    \n    rmspe = rmspe(y_test, preds)\n    return rmspe","e515c8ee":"study = optuna.create_study(sampler=TPESampler(), direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective, n_trials=1000, gc_after_trial=True)","4962bd5d":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","492966c1":"optuna.visualization.plot_optimization_history(study)","72f0deda":"optuna.visualization.plot_param_importances(study)","3bc37c34":"best_lgbmparams = study.best_params\nbest_lgbmparams","ff532970":"lgbm = LGBMRegressor(**best_lgbmparams, device='gpu')","800dec90":"%%time\nlgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, early_stopping_rounds=100)\n\npreds = xgb.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds), 6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the Tuned LIGHTGBM prediction: R2 score: {R2}, RMSPE: {RMSPE}')","59f720ab":"def_xgb = XGBRegressor(tree_method='gpu_hist', random_state = rs, n_jobs= - 1)\n\ndef_lgbm = LGBMRegressor(device='gpu', random_state=rs)","f9d5bf75":"from sklearn.ensemble import StackingRegressor\n\n\nestimators = [('def_xgb', def_xgb),\n              ('def_lgbm', def_lgbm),\n              ('tuned_xgb', xgb)]\n\nclf = StackingRegressor(estimators=estimators, final_estimator=lgbm, verbose=1)","37ffcae8":"%%time\nclf.fit(X_train, y_train)","3016a7b4":"preds = clf.predict(X_test)\nR2 = round(r2_score(y_true = y_test, y_pred = preds),6)\nRMSPE = round(rmspe(y_true = y_test, y_pred = preds), 6)\nprint(f'Performance of the STACK prediction: R2 score: {R2}, RMSPE: {RMSPE}')","278ba8b0":"test_data_set_final = test_data_set.drop(['stock_id', 'time_id'], axis = 1)\n\ny_pred = test_data_set_final[['row_id']]\nX_test = test_data_set_final.drop(['row_id'], axis = 1)","8ff19654":"X_test","3f9dd309":"y_pred = y_pred.assign(target = clf.predict(X_test))\ny_pred.to_csv('submission.csv',index = False)","a07eaa14":"# Stacking Regressor","33ca4077":"### Imports","30bce48a":"#### Most of the feature engineering code","190fc7d8":"#### Feature engineering","5aead942":"#### Plotting","db575322":"### Helper Functions","58702c29":"### Preparing Train and Test set for training and prediction with the desired features\nThe following cell takes around 25 mins for execution. You can also use the pickled data from the notebook output and build on that","8307bf2f":"# Submission","e23db423":"Wouldnt be possible without these amazing notebooks\n\n* https:\/\/www.kaggle.com\/abhishek1aa\/feature-engineering-xgboost-lgbm-baseline\/notebook\n* https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-train\/data\n* https:\/\/www.kaggle.com\/konradb\/we-need-to-go-deeper","c1e41ff4":"#### Metric","3600742d":"#### Storing for later usages. Processing time for features took 25 mins\nYou can directly use this from the notebook output and build on that","d0eff545":"# Optuna Tuned LGBM","245782b9":"### Example of book and trade data","68d4c525":"### Config","b11395d2":"#### File reading","bfc477c2":"# Optuna Tuned XGBoost"}}