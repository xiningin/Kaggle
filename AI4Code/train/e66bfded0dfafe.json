{"cell_type":{"0589bd7b":"code","d72b5f72":"code","99e5f6d0":"code","d03888a3":"code","d1bf8e63":"code","8d83f2b6":"code","18f3d7ca":"code","b19ef620":"code","f57119bc":"code","dabd87ba":"code","85a6a204":"code","28dae9f7":"code","cac4c4b1":"code","120f63af":"code","60bc8f68":"code","af496e58":"code","a5e6a534":"code","f65dc232":"markdown","60752db5":"markdown","c336f9a8":"markdown","edfd75d9":"markdown"},"source":{"0589bd7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d72b5f72":"from __future__ import absolute_import, division, print_function\nimport tensorflow.compat.v1 as tf\n#import tensorflow as tf\ntf.enable_eager_execution()\n\n\nimport time","99e5f6d0":"# Adding file path\nfile_path = '..\/input\/stop-words-in-28-languages\/english.txt'\n\n# Reading the file data, and then decoding to py2 compat.\ntext_data = open(file_path, 'rb').read().decode(encoding='utf-8')","d03888a3":"# The unique characters in the file\nvocab_unique = sorted(set(text_data))\n\n# Unique characters to Indices Mapping\nchar2ind = {u:i for i, u in enumerate(vocab_unique)}\nind2char = np.array(vocab_unique)\n\ntext_to_int = np.array([char2ind[c] for c in text_data])","d1bf8e63":"# The maximum length sentence we want for a single input in characters\nseq_len = 50\ntext_for_epoch = len(text_data)\/\/seq_len\n\n# Creating training targets\ntarget_dataset = tf.data.Dataset.from_tensor_slices(text_to_int)\n\nfor i in target_dataset.take(5):\n    print(ind2char[i.numpy()])","8d83f2b6":"sequence = target_dataset.batch(seq_len+1, drop_remainder=True)\n\ndef splitting_target_input(chunk):\n    text_input = chunk[:-1]\n    text_target = chunk[1:]\n    return text_input, text_target\n\ndataset = sequence.map(splitting_target_input)","18f3d7ca":"for input_example, target_example in  dataset.take(1):\n    print ('Input data: ', repr(''.join(ind2char[input_example.numpy()])))\n    print ('Target data:', repr(''.join(ind2char[target_example.numpy()])))","b19ef620":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(ind2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(ind2char[target_idx])))","f57119bc":"# Batch size \nBATCH_SIZE = 64\nsteps_per_epoch = text_for_epoch\/\/BATCH_SIZE\nBUFFER_SIZE = 10000\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","dabd87ba":"vocab_size = len(vocab_unique)\nembedding_dim = 256\nrnn_units = 1024\nrnn = tf.keras.layers.CuDNNGRU \n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n                              batch_input_shape=[BATCH_SIZE, None]),\n    rnn(rnn_units,\n        return_sequences=True, \n        recurrent_initializer='glorot_uniform',\n        stateful=True),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n\nmodel.summary()","85a6a204":"for input_example_batch, target_example_batch in dataset.take(1):\n    example_batch_predictions = model(input_example_batch)","28dae9f7":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","cac4c4b1":"# Train the Model\n\ndef loss_func(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss_func(target_example_batch, example_batch_predictions)\n\nmodel.compile(\n    optimizer = tf.train.AdamOptimizer(),\n    loss = loss_func)","120f63af":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","60bc8f68":"EPOCHS=10\n\nhistory = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])","af496e58":"tf.train.latest_checkpoint(checkpoint_dir)\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n                              batch_input_shape=[1, None]),\n    rnn(rnn_units,\n        return_sequences=True, \n        recurrent_initializer='glorot_uniform',\n        stateful=True),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n\n\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))\n\n\nmodel.summary()","a5e6a534":"# Number of characters to generate\nmax_words = 350\n\n# Starting String\nstart_string=u\"ANTONIO: \"\n\n\n\ninput_eval = [char2ind[s] for s in start_string]\ninput_eval = tf.expand_dims(input_eval, 0)\n\n\ngen_text = []\n\ntemperature = 1.0\n\n\nmodel.reset_states()\nfor i in range(max_words):\n    predictions = model(input_eval)\n\n    predictions = tf.squeeze(predictions, 0)\n\n\n    predictions = predictions \/ temperature\n    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n\n  \n    input_eval = tf.expand_dims([predicted_id], 0)\n\n    gen_text.append(ind2char[predicted_id])\n    \nprint(start_string + ''.join(gen_text))","f65dc232":"#Codes by https:\/\/www.kaggle.com\/anilreddy8989\/word-guess-with-rnn","60752db5":"#Package Imports","c336f9a8":"#From now Only Errors. I saved the script for another Dataset. ","edfd75d9":"#Data Processing"}}