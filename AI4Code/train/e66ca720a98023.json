{"cell_type":{"14f2de5b":"code","c0af1977":"code","0f251ff0":"code","38b6bb57":"code","13b78b09":"code","dad3e609":"code","a85bb74a":"code","6aadbdf2":"code","21bca4c8":"code","86f029af":"code","aea6b6ad":"code","e9326f46":"code","493a3995":"code","73544274":"code","a5a53f5d":"code","3d6a5898":"code","0b2b71d4":"code","28858aff":"code","62815965":"code","4a2584cf":"code","fed06526":"code","a2d13e05":"code","24f93ba2":"code","eb0847a9":"code","bd9502f8":"code","7fafcf40":"code","f0895a07":"code","4f4f8128":"markdown","bfc727e3":"markdown","a7ddee3b":"markdown","efa8c048":"markdown","77436296":"markdown","d1ff347f":"markdown","c2076090":"markdown","2c023656":"markdown","f5128ea4":"markdown","59d9d30d":"markdown","d2e3cb4b":"markdown","0b9cad81":"markdown","14423d66":"markdown","bbb058bd":"markdown","ecbc8ad0":"markdown","b1f1fe28":"markdown","7b2c2470":"markdown","f7a33904":"markdown","f1d998fc":"markdown","7de126d2":"markdown","67597119":"markdown","c1fd1f38":"markdown","52f73a1f":"markdown"},"source":{"14f2de5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c0af1977":"df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","0f251ff0":"df.head()","38b6bb57":"df.info()","13b78b09":"corrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True);","dad3e609":"dict = {}\nfor i in list(df.columns):\n    dict[i] = df[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).transpose()","a85bb74a":"#features = columns for column in df.columns if column not in \"Outcome\"\ncon_cols = list(df.drop('Outcome',axis=1).columns)\ntarget = ['Outcome']\nprint(\"Feature columns: \",con_cols)\nprint(\"Target columns: \",target)\ndf.head()","6aadbdf2":"df.isnull().sum()","21bca4c8":"fig, axs = plt.subplots(9)\nfig.set_figwidth(8)\nfig.set_figheight(45)\ni=0\nfor col in df.columns:\n    sns.boxplot(y=df[col], ax=axs[i])\n    i=i+1","86f029af":"fig, axs = plt.subplots(9)\nfig.set_figwidth(8)\nfig.set_figheight(45)\ni=0\nfor col in df.columns:\n    sns.histplot(x=df[col], ax=axs[i],kde=True)\n    i=i+1","aea6b6ad":"df.skew(axis = 0, skipna = True)","e9326f46":"df.drop(df[df[\"Pregnancies\"] > 14].index,inplace=True)\ndf.drop(df[df[\"Glucose\"] < 50].index,inplace=True)\ndf.drop(df[df[\"BloodPressure\"] > 120].index,inplace=True)\ndf.drop(df[df[\"SkinThickness\"] > 80].index,inplace=True)\ndf.drop(df[df[\"Insulin\"] > 600].index,inplace=True)\ndf.drop(df[df[\"BMI\"] > 55].index,inplace=True)\ndf.drop(df[df[\"DiabetesPedigreeFunction\"] > 2].index,inplace=True)\ndf.drop(df[df[\"Age\"] > 70].index,inplace=True)\n\nprint(\"Shape of dataset: \", df.shape)","493a3995":"# for Insulin\nfig, axs = plt.subplots(2)\nsns.kdeplot(df['Insulin'],color='Purple',fill=True, ax=axs[0])\n# Removing the skewness using a log function and checking the distribution again\ndf['Insulin'] = df['Insulin'].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df['Insulin'],color='Orange',fill=True, ax=axs[1])\ndf['Insulin'].skew(axis = 0, skipna = True)","73544274":"# for DiabetesPedigreeFunction\nfig, axs = plt.subplots(2)\nsns.kdeplot(df['DiabetesPedigreeFunction'],color='Purple',fill=True, ax=axs[0])\n# Removing the skewness using a log function and checking the distribution again\ndf['DiabetesPedigreeFunction'] = df['DiabetesPedigreeFunction'].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df['DiabetesPedigreeFunction'],color='Orange',fill=True, ax=axs[1])\ndf['DiabetesPedigreeFunction'].skew(axis = 0, skipna = True)","a5a53f5d":"# for Age\nfig, axs = plt.subplots(2)\nsns.kdeplot(df['Age'],color='Purple',fill=True, ax=axs[0])\n# Removing the skewness using a log function and checking the distribution again\ndf['Age'] = df['Age'].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df['Age'],color='Orange',fill=True, ax=axs[1])\ndf['Age'].skew(axis = 0, skipna = True)","3d6a5898":"# creating a copy of dataframe\ndf1 = df\n\n# separating the features and target \nX = df1.drop(['Outcome'],axis=1)\ny = df1[['Outcome']]\nfeature_cols = list(X.columns)\n","0b2b71d4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","28858aff":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[feature_cols] = sc.fit_transform(X_train[feature_cols])\nX_test[feature_cols] = sc.transform(X_test[feature_cols])","62815965":"X_train.head","4a2584cf":"# Base Models\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Ensembling and Boosting\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# Metrics\nfrom sklearn.metrics import accuracy_score,classification_report\n\n# Hyper-parameter tuning\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","fed06526":"models = [\n    ('SVC', SVC()),\n    ('DecisionTreeClassifier',DecisionTreeClassifier()),\n    ('KNeighborsClassifier',KNeighborsClassifier()),\n    ('LogisticRegression',LogisticRegression()),\n    ('RandomForestClassifier',RandomForestClassifier()),\n    ('GradientBoostingClassifier',GradientBoostingClassifier())\n]\n\n\nprint(\"The accuracy scores of the models are :\")\nfor model_name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(model_name, \": \", accuracy_score(y_test,y_pred))","a2d13e05":"def AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test):\n\n    # initialize grid search\n    grid = GridSearchCV(\n    estimator=classifier, param_grid=param_grid, scoring=\"accuracy\", verbose=1,\n    n_jobs=1,cv=5 )\n\n    # fit the model and extract best score\n    grid.fit(X_train, y_train)\n\n    print(\"Support Vector Classifier: \", grid.best_score_)\n    print(\"Best parameters set:\")\n    print(grid.best_params_)\n\n    # Getting Accuracy\n    y_pred = grid.best_estimator_.predict(X_test)\n    print(\"Classification Report\")\n    print(classification_report(y_test, y_pred))\n    return grid.best_estimator_;","24f93ba2":"# define the model\nclassifier = DecisionTreeClassifier()\n\n# define a grid of parameters\nparam_grid = {'criterion':['gini','entropy'],\n              'splitter':['best','random'],\n              'max_depth':[2,3,4,5,6,7,8],\n              'max_features':['auto','sqrt','log2'],\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","eb0847a9":"# define the model\nclassifier = KNeighborsClassifier()\n\n# define a grid of parameters\nparam_grid = {'n_neighbors':[2,3,4,5,6,7,8],\n              'weights':['uniform','distance'],\n              'algorithm':['auto','ball_tree','kd_tree','brute'],\n              'leaf_size':[26,27,28,29,30,31]\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","bd9502f8":"# define the model\nclassifier = SVC()\n\n# define a grid of parameters\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['linear','rbf']\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","7fafcf40":"# define the model\nclassifier = LogisticRegression()\n\n# define a grid of parameters\nparam_grid = {'C': np.logspace(-4, 4, 50),\n              'penalty': ['l1', 'l2']\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","f0895a07":"# define the model\nclassifier = RandomForestClassifier()\n\n# define a grid of parameters\nparam_grid = {'bootstrap': [True, False],\n              'max_depth': [10, 40, None],\n              'min_samples_leaf': [1, 2, 4],\n              'min_samples_split': [2, 5, 10],\n              'n_estimators': [200, 600, 1400]\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","4f4f8128":"Base Modeling","bfc727e3":"Separating into features and targets","a7ddee3b":"Removing the outliers","efa8c048":"Splitting the dataset into the Training set and Test set","77436296":"Packages","d1ff347f":"Decision Tree Classifier tuning","c2076090":"K Neighbors Classifier tuning","2c023656":"Making features model ready","f5128ea4":"# Data Preprocessing","59d9d30d":"Box Plot","d2e3cb4b":"Feature Scaling","0b9cad81":"# Understanding the data","14423d66":"LogisticRegression Classifier tuning","bbb058bd":"Skewness along the index axis","ecbc8ad0":"RandomForestClassifier tuning","b1f1fe28":"Histogram","7b2c2470":"SV Classifier tuning","f7a33904":"Reading Data","f1d998fc":"Checking for Missing Values","7de126d2":"# Hyperparameter tuning using GridSearchCV","67597119":"# Modeling","c1fd1f38":"Removing the skewness","52f73a1f":"Defining Function for Hyperparameter tuning using Grid Search CV"}}