{"cell_type":{"d0c90cda":"code","ec59e7c9":"code","ef9cb88d":"code","6f1b9015":"code","49758e35":"code","b85c38e3":"code","973eb2f1":"code","f4743b38":"code","21763aa8":"code","539b73c9":"code","54f15e9d":"code","41c3961a":"code","961f5c1b":"code","441f73d8":"code","2afeeaca":"code","7ae9946d":"code","07637c72":"code","e8d581eb":"code","e07f72df":"code","121731ae":"code","80f7d973":"code","03363e27":"code","597b0063":"code","a7e1e20a":"code","74f3413a":"code","2676f38a":"code","2d217bea":"code","0b1c6558":"code","e19244f3":"code","c5cd078b":"code","8a935314":"code","6a6ac359":"code","0927f4b0":"code","b60e1893":"code","cf14e7ee":"code","d0cf2d7f":"code","66050d72":"code","b3dc0a4c":"code","4600a46a":"code","f0c8b7af":"code","f5dfa546":"code","04f78230":"code","73541203":"code","ba0414f5":"code","f4f62911":"code","92979afc":"code","4cc5ea1f":"code","6fd513ec":"code","0a44045e":"code","8160973e":"code","9dad9b82":"code","3155fae8":"code","bcde0d40":"code","10d29e29":"code","940b84d6":"code","1b688421":"code","c68f516e":"code","10ae46df":"code","e120f83f":"code","34ddb9f0":"code","8860a02c":"code","4955a506":"code","19e122df":"code","62fcf067":"code","9f3f2683":"markdown","05f6dc2b":"markdown","2c856a32":"markdown","9b50b6f2":"markdown","867e7189":"markdown","83a30ad0":"markdown","9ddf30a4":"markdown","dfb648f4":"markdown","d62f7ce8":"markdown","3585cd99":"markdown","36832d54":"markdown","6ea0e625":"markdown","2bf5a7f2":"markdown","2df4bca7":"markdown","8ccb4df9":"markdown","7fdc9234":"markdown","8460cb25":"markdown","b0b52466":"markdown","4caccc74":"markdown","613357cf":"markdown","48a9c093":"markdown","c023d269":"markdown","273888b5":"markdown","94fb35ae":"markdown","26996639":"markdown","10be398b":"markdown","9efb8421":"markdown","39ef08fc":"markdown","a037535e":"markdown","636e601b":"markdown","4feeb204":"markdown","88b502f6":"markdown","4af3e8c9":"markdown","ed3705d6":"markdown","4e9f3d7c":"markdown","771fdf6c":"markdown","3602c055":"markdown","4d147224":"markdown","218fb95e":"markdown","7a58026d":"markdown","ed9bc531":"markdown","5b556999":"markdown","4eff4143":"markdown","c1b5bdf3":"markdown","25b75a62":"markdown","0a0b290e":"markdown","286e06a9":"markdown","7265013a":"markdown","abea4a91":"markdown"},"source":{"d0c90cda":"import pandas as pd\nimport numpy as np","ec59e7c9":"df=pd.read_csv(\"..\/input\/restaurant\/Rest.csv\")\ndf.head()","ef9cb88d":"import re\nimport nltk                                 ## Importing natural language tool kit\nnltk.download('stopwords')                  ## downloading Stop words from NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer  ## Importing stemmmer to find the root word\ncorp = []\n","6f1b9015":"for i in range(0, 1000):\n  review = re.sub('[^a-zA-Z]', ' ', df['Review'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')                 ## We have to keep 'NOT'as it plays a vital in classyfing negative review\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  corp.append(review)","49758e35":"print(corp)","b85c38e3":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = pd.DataFrame(cv.fit_transform(corp).toarray())\ny = df.iloc[:, -1].values","973eb2f1":"X.head(5)","f4743b38":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","21763aa8":"from sklearn.naive_bayes import GaussianNB,MultinomialNB\nnb = GaussianNB()\nmnb=MultinomialNB()\nnb.fit(X_train, y_train)","539b73c9":"y_pred = nb.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","54f15e9d":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\ncm = confusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))\nacc_nb=accuracy_score(y_test, y_pred)\nacc_nb","41c3961a":"from sklearn.model_selection import cross_val_score\nnb_Kfold_accu = cross_val_score(estimator = nb, X = X_train, y = y_train, cv = 10)\nnb_Kfold_accu=nb_Kfold_accu.mean()\nprint(\"Accuracy: {:.2f} %\".format(nb_Kfold_accu*100))","961f5c1b":"mnb.fit(X_train, y_train)","441f73d8":"y_pred = mnb.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","2afeeaca":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\ncm = confusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))\nacc_mnb=accuracy_score(y_test, y_pred)\nacc_mnb\ncm","7ae9946d":"from sklearn.model_selection import cross_val_score\nmnb_Kfold_accu = cross_val_score(estimator = mnb, X = X_train, y = y_train, cv = 10)\nmnb_Kfold_accu=mnb_Kfold_accu.mean()\nprint(\"Accuracy: {:.2f} %\".format(mnb_Kfold_accu*100))\n","07637c72":"model_compare=pd.DataFrame({'Models':['GuassianNB','Multinomial'],\n               'Accuracy Score':[acc_nb,acc_mnb],\n            'K Fold Accuracy':[nb_Kfold_accu,mnb_Kfold_accu]})\nprint(model_compare)","e8d581eb":"from sklearn.model_selection import GridSearchCV\nparameters = [{'alpha': [1,10,50,100], 'fit_prior': [True,False]}]       ","e07f72df":"grid_search = GridSearchCV(estimator = mnb,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","121731ae":"parameters = [{'alpha': [10,11,12,13], 'fit_prior': [True]}]   \ngrid_search = GridSearchCV(estimator = mnb,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","80f7d973":"parameters = [{'alpha': [10.90,10.95,11,11.05,11.1,11.15,11.20], 'fit_prior': [True]}]   \nmnb_grid_search = GridSearchCV(estimator = mnb,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nmnb_grid_search = mnb_grid_search.fit(X_train, y_train)\nmnb_best_accuracy = mnb_grid_search.best_score_\nmnb_best_parameters = mnb_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","03363e27":"mnb_y_pred=mnb_grid_search.predict(X_test)\nmnb_y_pred","597b0063":"mnb_cm = confusion_matrix(y_test, mnb_y_pred)\nprint(classification_report(y_test, mnb_y_pred))\nacc_mnb=accuracy_score(y_test, mnb_y_pred)\nacc_mnb","a7e1e20a":"from sklearn.metrics import roc_auc_score\nmnb_roc=roc_auc_score(y_test,mnb_y_pred)\nmnb_roc","74f3413a":"from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression(random_state = 0)\nlg.fit(X_train, y_train)","2676f38a":"from sklearn.model_selection import cross_val_score\nlg_Kfold_accu = cross_val_score(estimator = lg, X = X_train, y = y_train, cv = 5)\nlg_Kfold_accu=lg_Kfold_accu.mean()\nprint(\"Accuracy: {:.2f} %\".format(lg_Kfold_accu*100))","2d217bea":"lg_y_pred = lg.predict(X_test)\nprint(np.concatenate((lg_y_pred.reshape(len(lg_y_pred),1), y_test.reshape(len(y_test),1)),1))","0b1c6558":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\nlg_cm = confusion_matrix(y_test, lg_y_pred)\nprint(classification_report(y_test, lg_y_pred))\nacc_lg=accuracy_score(y_test, lg_y_pred)\nacc_lg\n","e19244f3":"parameters = [{'penalty': [11,12,'elasticnet'], 'C': [1,10,50,100,200]},\n              {'tol': [0.001,0.0001,0.00001]}]\nlg_grid_search = GridSearchCV(estimator = lg,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nlg_grid_search = lg_grid_search.fit(X_train, y_train)\nlg_best_accuracy = lg_grid_search.best_score_\nbest_parameters = lg_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","c5cd078b":"parameters = [{'tol': [0.01,0.001,0.0012,0.0013,0.0014]}]\nlg_grid_search = GridSearchCV(estimator = lg,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nlg_grid_search = lg_grid_search.fit(X_train, y_train)\nlg_best_accuracy = lg_grid_search.best_score_\nbest_parameters = lg_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","8a935314":"parameters = [{'tol': [0.1,0.01,0.001]}]\nlg_grid_search = GridSearchCV(estimator = lg,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nlg_grid_search = lg_grid_search.fit(X_train, y_train)\nlg_best_accuracy = lg_grid_search.best_score_\nbest_parameters = lg_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","6a6ac359":"paramters=[int(x) for x in np.linspace(start = 0.0001, stop = 0.001, num = 1)]","0927f4b0":"lg_grid_search = GridSearchCV(estimator = lg,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nlg_grid_search = lg_grid_search.fit(X_train, y_train)\nlg_best_accuracy = lg_grid_search.best_score_\nbest_parameters = lg_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","b60e1893":"lg_y_pred = lg_grid_search.predict(X_test)\nlg_cm = confusion_matrix(y_test, lg_y_pred)\nprint(classification_report(y_test, lg_y_pred))\nacc_lg=accuracy_score(y_test, lg_y_pred)\nacc_lg*100\n","cf14e7ee":"lg_roc=roc_auc_score(y_test,lg_y_pred)\nlg_roc","d0cf2d7f":"from sklearn.ensemble import RandomForestClassifier\nrc = RandomForestClassifier(random_state = 0)\nrc.fit(X_train, y_train)","66050d72":"from sklearn.model_selection import cross_val_score\nrc_Kfold_accu = cross_val_score(estimator = rc, X = X_train, y = y_train, cv = 10)\nrc_Kfold_accu=rc_Kfold_accu.mean()\nprint(\"Accuracy: {:.2f} %\".format(rc_Kfold_accu*100))","b3dc0a4c":"rc_y_pred = rc.predict(X_test)\nprint(np.concatenate((rc_y_pred.reshape(len(rc_y_pred),1), y_test.reshape(len(y_test),1)),1))","4600a46a":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\ncm = confusion_matrix(y_test, rc_y_pred)\nprint(classification_report(y_test, rc_y_pred))\nrc_acc=accuracy_score(y_test, rc_y_pred)\nrc_acc","f0c8b7af":"params = [{'n_estimators':[100,200,300,400,500], 'criterion':['entropy','gini'],\n              'max_depth':[3,4,5]}]\nrc_grid_search = GridSearchCV(estimator = rc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nrc_grid_search = rc_grid_search.fit(X_train, y_train)\nrc_best_accuracy = rc_grid_search.best_score_\nbest_parameters = rc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","f5dfa546":"params = [{'n_estimators':[300,310,320,330,340,350,360,370,380,390], 'criterion':['entropy'],\n              'max_depth':[5,5.5,6]}]\nrc_grid_search = GridSearchCV(estimator = rc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nrc_grid_search = rc_grid_search.fit(X_train, y_train)\nrc_best_accuracy = rc_grid_search.best_score_\nbest_parameters = rc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","04f78230":"params = [{'n_estimators':[390,400,450,500], 'criterion':['entropy'],\n              'max_depth':[6,6.5,7,7.5,8]}]\nrc_grid_search = GridSearchCV(estimator = rc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nrc_grid_search = rc_grid_search.fit(X_train, y_train)\nrc_best_accuracy = rc_grid_search.best_score_\nbest_parameters = rc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","73541203":"params = [{'n_estimators':[390,391,392,393,394,395,396,397,398,399], 'criterion':['entropy'],\n              'max_depth':[8,9,10,11]}]\nrc_grid_search = GridSearchCV(estimator = rc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nrc_grid_search = rc_grid_search.fit(X_train, y_train)\nrc_best_accuracy = rc_grid_search.best_score_\nbest_parameters = rc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","ba0414f5":"rc_y_pred = rc_grid_search.predict(X_test)\nrc_cm = confusion_matrix(y_test, rc_y_pred)\nprint(classification_report(y_test, rc_y_pred))\nacc_rc=accuracy_score(y_test, rc_y_pred)\nacc_rc*100\n","f4f62911":"rc_roc=roc_auc_score(y_test,rc_y_pred)\nrc_roc","92979afc":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(random_state = 0)\ndtc.fit(X_train, y_train)","4cc5ea1f":"from sklearn.model_selection import cross_val_score\ndtc_Kfold_accu = cross_val_score(estimator = dtc, X = X_train, y = y_train, cv = 10)\ndtc_Kfold_accu=dtc_Kfold_accu.mean()\nprint(\"Accuracy: {:.2f} %\".format(dtc_Kfold_accu*100))","6fd513ec":"dtc_y_pred = dtc.predict(X_test)\ndtc_y_pred","0a44045e":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\ncm = confusion_matrix(y_test, dtc_y_pred)\nprint(classification_report(y_test, dtc_y_pred))\ndtc_acc=accuracy_score(y_test, dtc_y_pred)\ndtc_acc","8160973e":"params = [{ 'criterion':['entropy','gini'],\n            'max_depth':[3,4,5],'splitter':[\"best\",\"random\"],\n           'max_features' :[\"auto\", \"sqrt\", \"log2\"],\n          'min_samples_split':[2,3,4],\n          'ccp_alpha':[0.1,0.01,0.001,0.0001]}]\ndtc_grid_search = GridSearchCV(estimator = dtc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ndtc_grid_search = dtc_grid_search.fit(X_train, y_train)\ndtc_best_accuracy = dtc_grid_search.best_score_\nbest_parameters = dtc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","9dad9b82":"params = [{ 'criterion':['gini'],\n            'max_depth':[4.5,5,5.5,6,6.5],'splitter':[\"best\"],\n           'max_features' :[\"auto\"],\n          'min_samples_split':[1,2],\n          'ccp_alpha':[0.001,0.0012,0.0013,0.0014,0.0015,0.0016]}]\ndtc_grid_search = GridSearchCV(estimator = dtc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ndtc_grid_search = dtc_grid_search.fit(X_train, y_train)\ndtc_best_accuracy = dtc_grid_search.best_score_\nbest_parameters = dtc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","3155fae8":"dtc_y_pred = dtc_grid_search.predict(X_test)\ndtc_cm = confusion_matrix(y_test, dtc_y_pred)\nprint(classification_report(y_test, dtc_y_pred))\nacc_dtc=accuracy_score(y_test, dtc_y_pred)\nacc_dtc*100\n","bcde0d40":"dtc_roc=roc_auc_score(y_test,dtc_y_pred)\ndtc_roc","10d29e29":"from sklearn.svm import SVC\nsvc = SVC(random_state = 0)\nsvc.fit(X_train, y_train)","940b84d6":"svc_Kfold_accu = cross_val_score(estimator = svc, X = X_train, y = y_train, cv = 10)\nsvc_Kfold_accu=svc_Kfold_accu.mean()\nprint(\"Accuracy: {:.2f} %\".format(svc_Kfold_accu*100))","1b688421":"svc_y_pred = svc.predict(X_test)\nsvc_y_pred","c68f516e":"cm = confusion_matrix(y_test, svc_y_pred)\nprint(classification_report(y_test, svc_y_pred))\nsvc_acc=accuracy_score(y_test, svc_y_pred)\nsvc_acc","10ae46df":"params = [{ 'C':[1,10,50,100,150,200],\n            'kernel':['rbf','linear'],'gamma':[\"scale\",\"auto\"],\n           'tol' :[0.001,0.0001,0.00001]}]\nsvc_grid_search = GridSearchCV(estimator = svc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nsvc_grid_search = svc_grid_search.fit(X_train, y_train)\nsvc_best_accuracy = svc_grid_search.best_score_\nbest_parameters = svc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","e120f83f":"params = [{ 'C':[150,160,170,180,190],\n            'kernel':['rbf'],'gamma':[\"auto\"],\n           'tol' :[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009]}]\nsvc_grid_search = GridSearchCV(estimator = svc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nsvc_grid_search = svc_grid_search.fit(X_train, y_train)\nsvc_best_accuracy = svc_grid_search.best_score_\nbest_parameters = svc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","34ddb9f0":"params = [{ 'C':[150,151,152,153,154,155,156,157,158,159],\n            'kernel':['rbf'],'gamma':[\"auto\"],\n           'tol' :[0.002,0.0021,0.0022,0.0023,0.0024,0.0025,0.0026,0.0027,0.0028,0.0029]}]\nsvc_grid_search = GridSearchCV(estimator = svc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nsvc_grid_search = svc_grid_search.fit(X_train, y_train)\nsvc_best_accuracy = svc_grid_search.best_score_\nbest_parameters = svc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","8860a02c":"params = [{ 'C':[148,149,150,150.1,150.2,150.3,150.4,150.5],\n            'kernel':['rbf'],'gamma':[\"auto\"],\n           'tol' :[0.015,0.016,0.017,0.018,0.019,0.002]}]\nsvc_grid_search = GridSearchCV(estimator = svc,\n                           param_grid = params,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\nsvc_grid_search = svc_grid_search.fit(X_train, y_train)\nsvc_best_accuracy = svc_grid_search.best_score_\nbest_parameters = svc_grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","4955a506":"svc_y_pred = svc_grid_search.predict(X_test)\nsvc_cm = confusion_matrix(y_test, svc_y_pred)\nprint(classification_report(y_test, svc_y_pred))\nacc_svc=accuracy_score(y_test, svc_y_pred)\nacc_svc*100\n","19e122df":"svc_roc=roc_auc_score(y_test,svc_y_pred)\nsvc_roc","62fcf067":"model_acc_comp=pd.DataFrame({'Models':['Multinomial_NB','Logistic Regression','Random Forest','Decision Tree',\n                                       'Support Vector Machin'],\n               'Accuracy Score':[acc_mnb,acc_lg,acc_rc,acc_dtc,acc_svc]})\nprint(model_acc_comp)","9f3f2683":"### **Prediction after Hyper Parammer tuning**","05f6dc2b":"##                          CAPSTONE_PROJECT_2_CLASSIFY_RESTAURANT_REVIEWS-NLP","2c856a32":"### Evaluation of model using Confusion Matrix","9b50b6f2":"### **K Fold Validation for model**","867e7189":"### Predicting the Test set results","83a30ad0":"### Model Comparison","9ddf30a4":"## ***3>> Building Random Forest Classifier Model***","dfb648f4":"### ***As we observe above,Multinomial model gives better accuracy then Guassain ,we willl proceed with Multinomial model and we will try to do hyper paramter tuning for the same***","d62f7ce8":"## ***5>> Building Support Vector Machine-Classifier Model***","3585cd99":"**As we can observe Multinomial Naives Bayes model has a better accuaracy compared to all ,comming to the independent feature we have only independent feature and one dependent feature,we have to retain both**","36832d54":"***Basically  we have used 5 Classification models and there Best Accuracy of the models are as follows:***\n\n1. Multinomial_NB Model:        **78.0 %**\n    2. Logistic Regression  Model : **77.6 %**\n    3. Random Forest Model :    **75.20 %**\n    4. Decision Tree Model :    **52.80 %**\n    5. Support Vector Machine : **76.80 %**","6ea0e625":"### ***Using Naive Bayes as the first Classficaion model first on the Training set as it uses conditional probability***","2bf5a7f2":"### **Hyper Tuning of Parameters**","2df4bca7":"### ***Now Creating loop for each row to clean every word in row***","8ccb4df9":"### **Predicting results**","7fdc9234":"### **K Fold validaiton for performance of model**","8460cb25":"### **Prediction and Evaluation after Hyper Parammer tuning**","b0b52466":"### Evaluation of model using Confusion Matrix","4caccc74":"### **K Fold cross validation for performance of the model**","613357cf":"### **K Fold Cross Validation**","48a9c093":"### Splitting dataset into the Training set and Test set","c023d269":"### **K Fold cross validation for performance of the model**","273888b5":"### Evaluation of model using Confusion Matrix","94fb35ae":"## ***4>> Building Decision Tree Classifier Model***","26996639":"### ***Cleaning the texts***","10be398b":"### **Checking Accuracy after hypertuning**","9efb8421":"### 12. Now, compare the models and pick the ideal one. \n\n### 13. Try to Predict the target with maximum independent features. ","39ef08fc":"# Conclusion","a037535e":"### **Hyper Tuning of Parameters**","636e601b":"### **1]Problem Statement 1:**                                       \n  ***Perform Basic EDA a. Boxplot b. Histogram \u2013 Distribution of Target Variable c. Distribution Plot \u2013 Target Variable d. Aggregation for all numerical Columns e. Unique Values across all columns f. Duplicate values across all columns g. Correlation \u2013 Heatmap h. Regression Plot i. Bar Plot j. Pair plot P.s: All the above charts should be plotted against every independent variable vs Target Variable. (Except b and c)***\n\n###  **2] Statement-2. Drop all duplicate rows**\n\n### **3] Statement-3. Drop all non-essential feature**","4feeb204":"### 7. Use one-hot encoding to convert the categorical variables into numerical ones, if required. \n### 8. Build a model of choice \u2013 Classification problem statement, hence build a classification model first and calculate Confusion Matrix, AUC, F1 Score, Precision, Recall and Accuracy. \n### 9. Build at least a minimum of 4 different Regression models. All the models should use K-Fold cross Validation to train the model with at least 5-fold cross validation. \n### 10. Compare the error and pick the ideal one with least errors. \n### 11. Run hyperparameter tuning on all the models and pick the best parameters (A minimum of 2 Parameters should be tuned) and picked.","88b502f6":"### **Hyper Paramter Tuning**","4af3e8c9":"## **4] Statement-4:: Perform basic text processing on the data.**\n## **5] Staement-5: Tokenize and stem the data, so that it can be used for classification.**\n## **6] Statement-6: Use NER if required**","ed3705d6":"### **Solution:-** \n\nAs data has only 2 rows,one dependent and one independent we dont have numerical data for Exploratory Data Analysis\nso all the above mentioned plots and comparison will not be possible, as well as no duplicate rows because chance is that texts might be same but different persion, and no NON -ESSENTIAL features","4e9f3d7c":"***As a result, we can say, Multinomial Na\u00efve Bayes is the best fit to this dataset, gives model prediction performance with accuracy of 78%, logistic Regression and Random forest model are fair enough that are working very well on this Dataset. It gives approx. 77% and 75% accuracy respectively which is quite good***","771fdf6c":"### Evaluation of model using Confusion Matrix","3602c055":"### ***Creating the Bag of Words model***","4d147224":"## ***1>> Naive Bayes Classification Model-GuassinaNB***","218fb95e":"### **Hyper Tuning of Parameters**","7a58026d":"### Evaluation of model after tuning paramters","ed9bc531":"### Evaluation of model using Confusion Matrix","5b556999":"### ***Lets now compare the accuracies of all the models***","4eff4143":"##  ***2>> Building Logistic Regression Model***","c1b5bdf3":"### **Hyper_Parameter Tuning**","25b75a62":"**Dataset Description**\n\nThe Dataset contains 1000 reviews in text format in a \u2015.tsv\u2016 This dataset is used for training the models It is split into 75%  training data and 25% test data. The dataset contains two columns. First column contains the text reviews given by different users which are related to the food of the restaurant  as well  as the  overall review of the restaurant. The  second column contains  the sentiment i.e.  if the  review is positive  or negative.i,e 1  indicates that the review is positive and 0 indicates the review is negative. \n\nThe dataset is imported and converted and into a Pandas Dataframe. The model we are building should predict if the review is positive or negative. The dataset is cleaned from 1000 reviews and the reviews which are not proper are discarded from the dataset and then the Dataframe is then served as input to the Count Vectorizer","0a0b290e":"### **Prediction and Evaluation after Hyper Parammer tuning**","286e06a9":"**Abstract:**\n\nThis Project is a classification machine learning model which uses NLP to classify restaurant reviews(whether liked or not liked).The reviews can be anything which are related to the food of the restaurant,staff of the restaurant and also overall review of the restaurant.The Project uses 5 models namely  **i) Multinommial Naive bayes ii) Logistic Regression Model iii) Random Forest Classifier iv) Decision Tree Classifier v) Support Vector Machine(SVM) algorithm** for classifying the reviews.The classified  reviews  are  helpful  for the  restaurant to analyze  their shortcomings improve the quality of food and service in the restaurant.","7265013a":"### Evaluation of model using Confusion Matrix","abea4a91":"### Using Multinomial Naive bayes model"}}