{"cell_type":{"17a0ebbf":"code","2414f333":"code","d644a1c4":"code","58bdf254":"code","a4cc6d99":"code","0957f17a":"code","251f7a2e":"code","10bdcd50":"code","94d6339e":"code","4e0f3408":"code","a232cb2d":"code","b9974eff":"code","c9797b23":"code","3cbc995d":"code","e41a1d36":"code","be7fb5c7":"markdown","08b03d38":"markdown","05191b8f":"markdown","5392837b":"markdown","70fcbe15":"markdown","bb922d24":"markdown","6832360d":"markdown"},"source":{"17a0ebbf":"#importing libraries\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom IPython.display import IFrame\nimport warnings\n%matplotlib inline\n\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"..\/input\/netflix-shows\/netflix_titles.csv\")\n\n#let's look at the profile report\nfrom pandas_profiling import ProfileReport\nimport panel as pn\nProfileReport(df, title=\"Pandas Profiling Report\")","2414f333":"#plotly libraries\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\ncf.go_offline()\nimport plotly.express as px\nimport plotly\ncols = plotly.colors.DEFAULT_PLOTLY_COLORS\n\n#number of shows\/movies across countries\nfrom tabulate import tabulate\nimport seaborn as sns\n\ndef split(df,col):\n    arr = []\n    for index, row in df[df[col].notnull()].iterrows():\n        txt = row[col].split(\",\")\n        for i in txt:\n            if i!=\"\":\n                arr.append([i.strip(),row[\"type\"],row[\"show_id\"]])\n    \n    return arr\n\n#cleaning country and listed_in variables as it has multiple values\ndf_country = pd.DataFrame(split(df,\"country\"), columns=[\"country\",\"type\",\"show_id\"])\ngrp = df_country.groupby([\"country\"])[\"show_id\"].count().reset_index()\nlst = list(grp.sort_values(\"show_id\",ascending=False).head(10)[\"country\"])\ngrp_country = df_country[df_country[\"country\"].isin(lst)].groupby([\"country\",\"type\"]) \\\n.count().reset_index().sort_values([\"country\",\"type\"])\n\ndf_listedIn = pd.DataFrame(split(df,\"listed_in\"), columns=[\"listed_in\",\"type\",\"show_id\"])\ngrp = df_listedIn.groupby([\"listed_in\"])[\"show_id\"].count().reset_index()\nlst = list(grp.sort_values(\"show_id\",ascending=False).head(10)[\"listed_in\"])\ngrp_listedIn = df_listedIn[df_listedIn[\"listed_in\"].isin(lst)].groupby([\"listed_in\",\"type\"]) \\\n.count().reset_index().sort_values([\"listed_in\",\"type\"])\n\nfig = make_subplots(rows=1, cols=2)\ngrp = grp_country[grp_country[\"type\"]==\"Movie\"]\nfig.add_trace(\n    go.Bar(x=grp[\"country\"], y=grp[\"show_id\"],name='Movies',marker_color=cols[0]),\n    row=1, col=1\n)\ngrp = grp_country[grp_country[\"type\"]==\"TV Show\"]\nfig.add_trace(\n    go.Bar(x=grp[\"country\"], y=grp[\"show_id\"],name='Series',marker_color=cols[1]),\n    row=1, col=1\n)\ngrp = grp_listedIn[grp_listedIn[\"type\"]==\"Movie\"]\nfig.add_trace(\n    go.Bar(x=grp[\"listed_in\"], y=grp[\"show_id\"],showlegend=False,marker_color=cols[0]),\n    row=1, col=2\n)\ngrp = grp_listedIn[grp_listedIn[\"type\"]==\"TV Show\"]\nfig.add_trace(\n    go.Bar(x=grp[\"listed_in\"], y=grp[\"show_id\"],showlegend=False,marker_color=cols[1]),\n    row=1, col=2\n)\nfig.update_layout(template=\"plotly\",title=\"Movies\/Series across different Countries and Genres\")\niplot(fig)","d644a1c4":"dff = pd.merge(df_country,df[['release_year','show_id']],how='inner',on='show_id')\ndff = dff[(dff['release_year'].notnull())&(dff['release_year'] < 2020)&(dff['release_year'] > 2000)]\nlst = list(dff.groupby('country')['show_id'].count().reset_index().sort_values(by='show_id',ascending=False).head(10)['country'])\ndff = dff[dff['country'].isin(lst)].groupby(['release_year','country','type'])['show_id'].count().reset_index().sort_values(by=[\"release_year\",\"show_id\"],ascending=[True,False])\ndff.columns = ['Years', 'Country', 'Type', 'Count']\nfig1 = px.line(dff, x='Years', y='Count', color='Country',facet_col='Type')\nfig1.update_layout(title='Movie\/Shows released across years', xaxis=dict(title=''), xaxis2=dict(title=''))\nfig1.for_each_annotation(lambda a: a.update(text=a.text.replace(\"Type=\", \"\")))\n\ndff = pd.merge(df_country,df[['date_added','show_id']],how='inner',on='show_id')\ndff['date_added'] = pd.to_datetime(dff['date_added'])\ndff = dff[(dff['date_added'].notnull())&(dff['date_added'] < '1\/1\/2020')&(dff['date_added'] > '1\/1\/2000')]\ndff = dff[dff['country'].isin(lst)].groupby([dff.date_added.dt.year,'country','type'])['show_id'].count().reset_index().sort_values(by=[\"date_added\",\"show_id\"],ascending=[True,False])\ndff.columns = ['Years', 'Country', 'Type', 'Count']\n\nfig2 = px.line(dff, x='Years', y='Count', color='Country',facet_col='Type')\nfig2.update_layout(title='Movie\/Shows added to Netflix', xaxis=dict(title=''), xaxis2=dict(title=''))\nfig2.for_each_annotation(lambda a: a.update(text=a.text.replace(\"Type=\", \"\")))\nfig1.show()\nfig2.show()","58bdf254":"dff = pd.merge(df_country,df_listedIn[[\"listed_in\",\"show_id\"]], how='inner',on='show_id')\ndff = dff.groupby(['country','listed_in','type'])['show_id'].count().reset_index()\nlst = ['United States', 'India', 'United Kingdom', 'Canada']\n\ndff = dff.sort_values(by=['country', 'type','show_id'],ascending=[False, False, False])\n\nfig1 = px.bar(dff[(dff['country'].isin(lst))&(dff['type']=='Movie')], x='listed_in', y='show_id', facet_col='country',color='country')\nfig1.update_layout(title='', xaxis=dict(title=''), xaxis2=dict(title=''), xaxis3=dict(title=''), \n                   xaxis4=dict(title=''), xaxis5=dict(title=''), yaxis=dict(title=''), showlegend=False)\nfig1.for_each_annotation(lambda a: a.update(text=a.text.replace(\"country=\", \"\")))\n\nfig2 = px.bar(dff[(dff['country'].isin(lst))&(dff['type']=='TV Show')], x='listed_in', y='show_id', facet_col='country',color='country')\nfig2.update_layout(title='', xaxis=dict(title=''), xaxis2=dict(title=''), xaxis3=dict(title=''), \n                   xaxis4=dict(title=''),xaxis5=dict(title=''), yaxis=dict(title=''), showlegend=False)\nfig2.for_each_annotation(lambda a: a.update(text=\"\"))\nfig1.update_layout(\n    autosize=False,\n    width=780,\n    height=400,)\nfig2.update_layout(\n    autosize=False,\n    width=780,\n    height=400,)\nfig1.show()\nfig2.show()","a4cc6d99":"#importing necessary libraries\/functions\nfrom nltk.util import ngrams\nimport nltk, re, string, gensim\nfrom nltk.corpus import stopwords, wordnet as wn\nfrom nltk.stem import WordNetLemmatizer \nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom collections import defaultdict\nfrom gensim import corpora\nfrom nltk.stem import SnowballStemmer\n\nps = SnowballStemmer('english')\nlemmatizer = WordNetLemmatizer() \n\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\n\n\n#Words from corpus - dictionary \nwords = set(nltk.corpus.words.words())\n\n\n#Text cleaning - tokenization, remove special characters, punctions, meaningless words etc.\ndef txt_clean(txt):\n    tokens = nltk.word_tokenize(txt.lower())\n    tokens_clean = [w for w in tokens if w.isalpha() and w in words] \n    return tokens_clean\n\n#create unigram\/bigram\/trigram words, remove stopwords, lemmatize\ndef lemmatize_stem(tokens, ngram_type=None):\n    \n    bigram = gensim.models.Phrases(tokens, min_count=2, threshold=100)\n    bigram_tokens = [bigram[tokens[w]] for w in range(len(tokens))]\n    trigram = gensim.models.Phrases(bigram[tokens],threshold=100)\n    trigram_tokens = [trigram[tokens[w]] for w in range(len(tokens))]\n    \n    tokens_clean = []\n    if ngram_type == \"bigram\":\n        tokens_c = bigram_tokens\n    elif ngram_type == \"trigram\":\n        tokens_c = trigram_tokens\n    else:\n        tokens_c = tokens\n\n    for i in range(len(tokens)-1):\n        txt = tokens_c[i]\n        txt_above5 = [k for k in txt if len(k)>=5 and k not in gensim.parsing.preprocessing.STOPWORDS]\n        lemma_txt = [lemmatizer.lemmatize(w,pos=tag_map[tg[0]]) for w,tg in nltk.pos_tag(txt_above5)]\n        stem_txt = [w for w in lemma_txt]\n        tokens_clean.append(stem_txt)\n\n    dictionary = corpora.Dictionary(tokens_clean)\n    corpus = [dictionary.doc2bow(text) for text in tokens_clean]\n    \n    return dictionary, corpus, tokens_clean\n\nunseen_len = int(round(0.10 * len(df),0))\nunseen_data = df[\"description\"].sample(unseen_len) #random sample\ntxt_data = df[\"description\"].drop(unseen_data.index)\n\n#Tokenize and clean\ntokens = list(txt_data.apply(lambda x: txt_clean(x)))\n\n#Chose bigram - it had the best performace (tried both unigram and trigram)\ndictionary, corpus, tokens_clean = lemmatize_stem(tokens, \"bigram\")\n\nfrom gensim.models import CoherenceModel\n\n#Choosing best parameter and topics\ntopics_arr = [20, 40, 60, 80]\nlearning_decay = [0.5, 0.7, 0.9]\nminimum_probability= [0.01, 0.05, 0.08]\n\ndef best_params(topic_num, min_probability, decay):\n    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = topic_num, id2word=dictionary, passes=15, minimum_probability=min_probability, decay=decay)\n    # Compute Coherence Score\n    coherence_model_lda = CoherenceModel(model=ldamodel, texts = tokens_clean, dictionary=dictionary, coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n    return [topic_num, min_probability, decay, coherence_lda]\n\n\n#Parameter Tuning - this'll take some time to run\nparams = []\nfor t in topics_arr:\n    for p in minimum_probability:\n        for l in learning_decay:\n            val = best_params(t,p,l)\n            params.append(val)\n\n\n#Chosen from the above function (The number of topics and decay shouldn't be too high or too low)\ntopic_num = 40\nmin_probability = 0.05\nlearning_decay = 0.5\n\n\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = topic_num, id2word=dictionary, passes=15, minimum_probability=min_probability, decay=learning_decay)\n\ntopics = ldamodel.print_topics(num_words=10)\n\n#Evaluation\n#Perplexity - how probable some new unseen data is given the model that was learned earlier.\nprint('Perplexity: ', ldamodel.log_perplexity(corpus))\n\n#Coherence - measure of the degree of semantic similarity between high scoring words in each topic (and then average across topics)\ncoherence_model_lda = CoherenceModel(model=ldamodel, texts = tokens_clean, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score: ', coherence_lda)\nprint(\"\\nSample of Topics:\")\nfor i,j in ldamodel.show_topics(formatted=True,num_words= 10):\n    print(\"Topic-{} => {}\".format(i,j))","0957f17a":"#Description-topic distributions for our training set  - It lists top 4 keywords and Dominant topic for each sentence\narr = []\nfor i, j in enumerate(ldamodel[corpus]):\n    if len(j) > 0:\n        max_val = sorted([w[1] for w in j],reverse=True)[0]\n        max_topic = [w[0] for w in j if w[1]==max_val][0]\n        keywords = ldamodel.show_topic(max_topic,topn=4)\n        keywords = [k[0] for k in keywords]\n        description = txt_data.iloc[i]\n        arr.append([description,  \",\".join(keywords), max_topic, round(max_val,2),])\n\nlda_distribution = pd.DataFrame(arr, columns=['Description', 'Top Keywords', 'Dominant Topic', 'Probability'])\nlda_distribution.head()","251f7a2e":"#interactive word visualization\nimport pyLDAvis.gensim\nlda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\npyLDAvis.save_html(lda_display, 'lda.html')\n\n#Not displaying the output - this for some reason messes up other visualizations","10bdcd50":"#let's predict the topics for unseen data\nunseen_clean = unseen_data.apply(lambda x: txt_clean(x))\n    \narr = []\nfor i in unseen_clean:\n    lemma_txt = [lemmatizer.lemmatize(w,pos=tag_map[tg[0]]) for w,tg in nltk.pos_tag(i)]\n    lemma_txt2 = [w for w in lemma_txt if w not in gensim.parsing.preprocessing.STOPWORDS]\n    arr2 = ldamodel[dictionary.doc2bow(lemma_txt2)]\n    max_arr2 = sorted([x[1] for x in arr2],reverse=True)[:3]\n    sel_arr2 = [list(x) for x in arr2 if x[1] in max_arr2][:3]\n    sel_arr2 = sum(sel_arr2,[])\n    if len(sel_arr2) != 6:\n        sel_arr2.extend([\"None\"]*(6-len(sel_arr2)))\n    \n    sel_arr2.extend([i])\n    arr.append(sel_arr2)\n\nunseen_df = pd.DataFrame(arr, columns = [\"topic_1\", \"topic_1_prob\", \"topic_2\", \"topic_2_prob\", \"topic_3\", \"topic_3_prob\", 'Tokens']).round(2)\nunseen_df[\"Description\"] = list(unseen_data)\ncols = list(unseen_df.columns)\ncols = cols[-1:] + [cols[-2]] + cols[:-2]\nunseen_df = unseen_df[cols]\nunseen_df.head()","94d6339e":"from gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\n#We can use the same train and test from our LDA model - txt_data (train) and unseen_data (test)\n\n#text cleaning - remove stopwords and meaningless words\ndef clean(x):\n    tokens = nltk.word_tokenize(x.lower())\n    tokens_clean = list(set([w for w in tokens if w.isalpha() and w not in gensim.parsing.preprocessing.STOPWORDS]))\n    return tokens_clean\n\n#clean train data\ntxt_clean = list(txt_data.apply(lambda x: clean(x)))\n\n#bigram-tokenize\nbigram = gensim.models.Phrases(txt_clean, min_count=2, threshold=100)\nbigram_tokens = [bigram[txt_clean[w]] for w in range(len(txt_clean))]\n\nmodel = Word2Vec(bigram_tokens, size=300, min_count=1, workers=3, window=5, sg=1)\n\nprint(\"Let's see some sample word similarities (according to train-set):\\n\")\nprint(\"Similar words to 'Crime':\")\nprint(model.wv.similar_by_word(\"crime\"),\"\\n\")\nprint(\"Similar words to 'Love':\")\nprint(model.wv.similar_by_word(\"love\"),\"\\n\")\nprint(\"These are based out of our train-set so some of it may not be true in real-life\")","4e0f3408":"#Word2Vec to Sentence-Vector Representation\ndef get_feat(x):\n    arr = []\n    for i in x:\n        arr.append(model.wv[i])\n    \n    arr = list(np.mean(arr, axis=0))\n    return arr\n\n#Each row in txt_data is represented by 300 features (Size parameter) from Word2Vec\nX = pd.Series(bigram_tokens).apply(lambda x: get_feat(x))\nX = np.array(list(X))\nX = X\/X.max(axis=0)\n\n#scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)","a232cb2d":"#Elbow method to determine the number of clusters\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,10) \nfor k in K: \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k).fit(X) \n    kmeanModel.fit(X)     \n      \n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) \/ X.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_, \n                 'euclidean'),axis=1)) \/ X.shape[0] \n    mapping2[k] = kmeanModel.inertia_ \n\n\ndf_elbow = pd.DataFrame(list(zip(K, distortions, inertias)), columns=[\"K\",\"Distortions\",\"Inertia\"])\n\nfig = make_subplots(rows=1, cols=2)\nfig.add_trace(\n    go.Scatter(x=df_elbow[\"K\"], y=df_elbow[\"Distortions\"], name=\"Distortion\"),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_elbow[\"K\"], y=df_elbow[\"Inertia\"], name=\"Inertia\"),\n    row=1, col=2\n)\nfig.update_layout(template=\"plotly\",title=\"Elbow method - Distortions and Inertia\", xaxis_title=\"K\", xaxis2_title=\"K\")\n\n\n# fig, ax  = plt.subplots(1,2)\n# sns.lineplot(x=\"K\", y=\"Distortions\", data=df_elbow,ax=ax[0])\n# sns.lineplot(x=\"K\", y=\"Inertia\", data=df_elbow,ax=ax[1])\n# plt.suptitle(\"The Elbow method using Distortion and Inertia\");","b9974eff":"#clustering\nkmeans = KMeans(n_clusters=4, random_state=0).fit(X)\ncenters = kmeans.labels_\ndf_X  = pd.DataFrame(X)\ndf_X[\"clusters\"] = list(centers)\n\n#concat with orginal dataframe\ndf_concat = pd.concat([txt_data.reset_index(),pd.Series(bigram_tokens,name=\"bigram_tokens_clean\"),df_X],axis=1).set_index('index')\ndf_concat = df_concat.join(df[[\"show_id\",\"title\",\"listed_in\",\"country\"]])","c9797b23":"#Cluster Visualization using PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(df_concat[np.arange(0,100)])\ndf_concat['PC1'] = X_pca[:,0]\ndf_concat['PC2'] = X_pca[:,1]\n\n#import seaborn as sns\n#mpl.rcParams['figure.dpi']= 150\nfig = px.scatter(df_concat, x=\"PC1\", y=\"PC2\", color=\"clusters\",\n                 size=len('bigram_tokens_clean'), hover_data=['title'])\n\n\n\nfig.show()","3cbc995d":"# WordCloud Visualization of each cluster\nfrom wordcloud import WordCloud, STOPWORDS \n\ndef clust_words(cluster_num):\n    lst = list(df_concat[df_concat[\"clusters\"]==cluster_num][\"bigram_tokens_clean\"])\n    reviews = \"\"\n    for i in lst:\n        for t in i:\n            if len(t) >= 5:\n                reviews += \" \" + t + \" \"\n\n    reviews = reviews.strip()\n    return reviews\n\n\nlength = len(df_concat[\"clusters\"].unique())\nstopwords = set(STOPWORDS) \nwrdcld = WordCloud(width = 200, height =300, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 4)\n\nfig, ax = plt.subplots(1,length,figsize = (24,20))\n#plt.figure(figsize = (50, 50), facecolor = None) \n\nfor i in range(length):\n    reviews = clust_words(i)\n    wordcloud = wrdcld.generate(reviews)\n    ax[i].imshow(wordcloud) \n    ax[i].axis(\"off\") \n\nplt.show();","e41a1d36":"#TSNE Visualization - Visualization of higher dimension models to 2-dimension\nfrom sklearn.manifold import TSNE\nembeddings_ak = model.wv[model.wv.vocab]\ntsne_ak_2d = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3500, random_state=32)\nembeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings_ak)\n\nimport matplotlib.cm as cm\ndef tsne_plot_2d(label, embeddings, words=[], a=1):\n    plt.figure(figsize=(20, 8))\n    colors = cm.rainbow(np.linspace(0, 1, 1))\n    x = embeddings[:,0]\n    y = embeddings[:,1]\n    plt.scatter(x, y, c=colors, alpha=a, label=label)\n    for i, word in enumerate(words):\n        plt.annotate(word, alpha=0.3, xy=(x[i], y[i]), xytext=(5, 2), \n                     textcoords='offset points', ha='right', va='bottom', size=10)\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.savefig(\"hhh.png\", format='png', dpi=200, bbox_inches='tight')\n    plt.show()\n\n\ntsne_plot_2d('Netflix review', embeddings_ak_2d, a=0.1)","be7fb5c7":"<a id=\"section4\"><\/a>\n### Word Embeddings (Word2Vec)\n\nWe are going to train Word2Vec model based on descriptions and also perform Clustering on the generated features. Word2Vec is a actually a 2 layer neural network that can be used to find similarity between words like \"Man\" and \"King\", \"Woman\" and \"Queen\" rtc. and thus can represent a word in vector format. \n\n<b>Steps:<\/b>  \n- Train\/Test Split\n- Preprocessing\n    - Text cleaning - remove special characters, punctuations etc.\n    - Remove stop-words\n    - Create bigram\/trigram tokens\n- Training + Clustering","08b03d38":"#### Summary\n\nWe've trained the model and have generated predictions for our test set. Also, we've generated topic distributions for each description in our training set. Let's see what we've got:\n\n- The topic number is 40 - So 5k rows (train set) are represented with 40 topics along with probabilities; an \"unseen\" description can be represented with sum of \"n\" number of topics (from the 40). And, each topic is not unique so we can't define a topic - they've overlaps (Check out sample-topics and try to distinguish between topics)\n- Coherence score is good but with the unseen data prediction the probability distributions of topics are low. And, the word distribution within those topics are high (as we saw in above bullet\n- These topic probabaility distributions per description can be used as feature in clustering algorithm to see what all topics can be grouped together (or) can be used in predictive modelling for predicting ratings\/duration etc.","05191b8f":"<a id=\"section2\"><\/a>\n### Visualizations (Interactive)\n\nLet's do some basic analysis before proceeding to topic modeling","5392837b":"#### Summary\n\nUsing Word2Vec we've converted sentences to vector of 300 features (Word -> Vectors; Sentence -> Sum of word vectors). And, we clustered the features to get 4 groups containing potential similar titles in each cluster (based on the descriptions alone). Next, we've generated WordCloud of words within each cluster and the t-SNE visualization of the all reviews. Let's see what we've got:\n\n- The Clusters are clearly separated but have little overlaps - the titles within each cluster have similar descriptions based on Word2Vec. So, we can use this to build recommendations - for example: people who like some movies in Cluster 0 can be be recommended all other movies (based on the distance) in Cluster 0 based on euclidean distance.\n- In WordCloud visuals we can see words like \"Young\", \"family\", \"friend\", \"World\" are overlapping across clusters but we can still distinguish between the clusters. The overlap is expected on a word-level but we have to mainly look from the conext of the sentence (description)","70fcbe15":"## Netflix Movie\/Shows - Topic Modeling\n> <span style='font-family:cursive'> If a group of people where asked to guess the genre(s) for the same movie\/tv show - Do you think the answers will be same?<\/span>\n\nIn this notebook, we'll be analyzing Netflix dataset (https:\/\/www.kaggle.com\/shivamb\/netflix-shows) to create topics\/groups based on descriptions. Given a new movie, can we predict the topic probabilities (This'll be useful to assign Genres, recommend movies\/shows etc.)\n\n<b>Note<\/b>: The summaries\/views presented in this notebook are based on the above dataset and NOT to be mixed with the current Netflix shows\/series\n\n### Index\n1. [Load Dataset](#section1)\n2. [Interactive Visualizations](#section2)\n3. [Topic Modeling](#section3)\n4. [Word Embedding](#section4)","bb922d24":"#### Summary (infered from above plots):\n\n- Netflix has more viewers in US and India as compared to other countries\n- International movies\/series, Dramas and comedies seems to be the prefered genre\n- The growth of netflix seems to be more prominent after 2016 and more people prefer watching latest movies\/shows (2015 and beyond)\n- Documentries and Family movies also seem to have a good following among the countries that have high viewership\n- Cult classic movies\/shows and Sci-Fi shows doesn't seem to attract viewers\n\nThis gives us a taste of the likes\/dislikes of our Netflix audience.\n\n<a id=\"section3\"><\/a>\n### Topic Modelling (LDA)\n\nWe are going to train LDA (Latent Dirichlet allocation) model based on title descriptions and determine the topic(s) of \"unseen\" descriptions.\n\n<b>Steps:<\/b>  \n- Train\/Test Split\n    - We need to split the data into train\/test set in order to test our LDA model\n    - 90:10 - Split \"descriptions\"\n- Preprocessing\n    - Text cleaning - remove special characters, punctuations etc.\n    - Remove stop-words\n    - Create bigram\/trigram tokens\n    - Lemmatization\n- Training and Evaluation (Coherence\/Perplexity)\n- Test on the \"unseen\" data","6832360d":"<a id=\"section1\"><\/a>\n### Load dataset"}}