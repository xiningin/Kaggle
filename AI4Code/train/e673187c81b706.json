{"cell_type":{"d0281fd7":"code","7721c2c0":"code","48ca97f4":"code","9a481b11":"code","2c72ea12":"code","28ea4d96":"code","5d0fc04c":"code","15146019":"code","d73a54fb":"code","49bc4806":"code","b6eaaf93":"code","c1f91b8d":"code","baf37d5b":"code","4782bafb":"markdown","8590a3bc":"markdown","a743e53a":"markdown","0784e80e":"markdown","a57b5afd":"markdown","f10b86ae":"markdown","ac04c9a4":"markdown","86345515":"markdown","50bff16b":"markdown","c061ba56":"markdown","0ba983ef":"markdown"},"source":{"d0281fd7":"!ls '..\/input'","7721c2c0":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import metrics\n\nfrom sklearn.utils import class_weight\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\n\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport pandas as pd","48ca97f4":"train_loc = '..\/input\/specimages\/train_test_split\/train\/'\ntest_loc = '..\/input\/specimages\/train_test_split\/val\/'","9a481b11":"trdata = ImageDataGenerator()\ntraindata = trdata.flow_from_directory(directory=train_loc, target_size=(224,224))\ntsdata = ImageDataGenerator()\ntestdata = tsdata.flow_from_directory(directory=test_loc, target_size=(224,224))","2c72ea12":"diagnosis_csv = '..\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/patient_diagnosis.csv'\ndiagnosis = pd.read_csv(diagnosis_csv, names=['pId', 'diagnosis'])\ndiagnosis.head()","28ea4d96":"categories = diagnosis['diagnosis'].unique()\ncategories","5d0fc04c":"vgg16 = VGG16(weights='imagenet')\nvgg16.summary()\n\nx  = vgg16.get_layer('fc2').output\nprediction = Dense(8, activation='softmax', name='predictions')(x)\n\nmodel = Model(inputs=vgg16.input, outputs=prediction)","15146019":"for layer in model.layers:\n    layer.trainable = False\n\nfor layer in model.layers[-20:]:\n    layer.trainable = True\n    print(\"Layer '%s' is trainable\" % layer.name)  ","d73a54fb":"opt = Adam(lr=0.000001)\nmodel.compile(optimizer=opt, loss=categorical_crossentropy, \n              metrics=['accuracy', 'mae'])\nmodel.summary()","49bc4806":"checkpoint = ModelCheckpoint(\"vgg16_base_res.h5\", monitor='val_accuracy', verbose=1, \n                             save_best_only=True, save_weights_only=False, mode='auto')\nearly = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')","b6eaaf93":"counter = Counter(traindata.classes)                       \nmax_val = float(max(counter.values()))   \nclass_weights = {class_id : max_val\/num_images for class_id, num_images in counter.items()}\nclass_weights","c1f91b8d":"hist = model.fit(traindata, steps_per_epoch=traindata.samples\/\/traindata.batch_size, validation_data=testdata, \n                 class_weight=class_weights, validation_steps=testdata.samples\/\/testdata.batch_size, \n                 epochs=110,callbacks=[checkpoint,early])","baf37d5b":"plt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='val')\nplt.title('VGG16: Loss and Validation Loss (0.000001 = Adam LR)')\nplt.legend();\nplt.show()\n\nplt.plot(hist.history['accuracy'], label='train')\nplt.plot(hist.history['val_accuracy'], label='val')\nplt.title('VGG16: Accuracy and Validation Accuracy (0.000001 = Adam LR)')\nplt.legend();\nplt.show()\n\nplt.plot(hist.history['mae'], label='train')\nplt.plot(hist.history['val_mae'], label='val')\nplt.title('VGG16: MAE and Validation MAE (0.000001 = Adam LR)')\nplt.legend();\nplt.show()","4782bafb":"Now, considering our data is imbalanced, we need to compute the class weight for each category\/class.","8590a3bc":"We see that slowly but surely our model is learning our data. Validation accuracy is increasing steadily while mean absolute error is decreasing. However, we should be aware that loss is increasing. In this context, but maybe we can just ditch loss since we are already looking at mae.\n\nAnd that's that! If you're following this series, thank you so much! Please upvote if this helped you in any way.\n","a743e53a":"Now we also want to save the model if the validation accuracy improved. However, we don't want to waste computational resource if we're seeing the lack of improvement after 20 epochs.\n\nWe then define the checkpoint and early stopping for our model.","0784e80e":"Use Adam as our optimiser and set the learning rate to 0.000001 because again we're trying to recognise signals. Additionally it's not a normal sound wave either cause we're using breathing sounds. So the model has to be very sensitive to small details.","a57b5afd":"Use the weights from imagenet to get the pretrained model. The prediction is set to 8 output because we have 8 classes.","f10b86ae":"Finally everything's set, let's start training!\n\nNotice that steps_per_epoch and validation_steps were calculated instead of just simply setting a random value. The reason is we need to see the optimal way to getting these values. We calculate this by dividing samples \/ batch_size.","ac04c9a4":"This is the final part of my CNN series for detecting respiratory diseases using respiratory sound. On this kernel, we use the images created from audio to feed to a convolutional 2d model, in this case, I'm using VGG16.\n\nIf you missed out on how we manipulate and transformed out data, you can look through these kernels:\n- Part 1: [Slice audio into subslices based on the txt files](https:\/\/www.kaggle.com\/danaelisanicolas\/cnn-part-1-create-subslices-for-each-sound)\n- Part 2: [Splitting to train and test](https:\/\/www.kaggle.com\/danaelisanicolas\/cnn-part-2-split-to-train-and-test)\n- Part 3: [Convert audio to spectrogram images](https:\/\/www.kaggle.com\/danaelisanicolas\/cnn-part-3-create-spectrogram-images)\n\nWithout further adeu, let's start!","86345515":"And lastly we want to visualise how our model is learning. Plot the loss, accuracy, and mae as follows:","50bff16b":"Load the train and test data using ImageDataGenerator flow_from_directory. Target size set to 224x224 given that this is the input requirement of VGG16.","c061ba56":"Now consider that normally, imagenet is recognising objects. We have spectrogram images which has signals. So we set 20 out of 23 layers to trainable.","0ba983ef":"Import the necessary libraries. Here we're using TensorFlow Keras. Again as mentioned, we'll use VGG16."}}