{"cell_type":{"1c039114":"code","0c205d0c":"code","9e27b95e":"code","0104c19d":"code","70d1134d":"code","d9c67374":"code","3388111f":"code","fe16e46b":"code","ee481f5a":"code","4d519512":"code","991d56cc":"code","31f9e19b":"code","4b42f5af":"code","cd2a54ba":"code","0dc5a30a":"code","6da35368":"code","1ef2ce84":"code","1f39da15":"code","aad364ca":"code","c4733041":"code","1d8f1466":"code","265c47fa":"code","e49fcc36":"code","dcba18a1":"code","fda7546a":"code","15135857":"code","c772c24d":"code","8b78f093":"code","4f5e046d":"code","fb6e807d":"code","1fd04f28":"code","3778fcc0":"code","60c65fbd":"code","3b8b79e1":"code","a8c2b9fc":"code","b81d483e":"code","fe447040":"code","fc1c4590":"code","7a454588":"code","4b4d7e5a":"code","6a621558":"code","0d8166c0":"code","a80e6923":"code","b6e0c3a3":"code","2a6a85a4":"code","0e178883":"code","1a23072f":"code","548f568b":"code","6f4fb244":"code","8cf86878":"code","8b67fa2c":"code","80c85976":"code","8f9388e6":"code","4ff5cfd6":"code","fca2e5aa":"code","dceb321b":"code","55fb1671":"code","f263a6b4":"code","cee7c9da":"code","8b7a337e":"code","474cf3bd":"code","21ef7bc6":"code","2b51687a":"code","a05269a7":"code","72b78268":"code","3953ff5c":"code","a2232426":"code","564280d3":"code","d3ae8ca4":"code","ecfe8236":"code","9a3ae37a":"code","ade0e3b6":"code","b3789051":"code","9e45387a":"code","009be2d2":"code","adca5f7f":"code","c4b90552":"code","5dce6e79":"code","ce077220":"code","5b0cf651":"code","93443dc5":"code","64ec1642":"code","459ae10d":"code","6a03351a":"code","8adc3959":"code","15905292":"code","2b797de1":"code","36a198e0":"code","630b84c0":"code","04deb733":"code","f42df05e":"code","c1eb8668":"code","bc364126":"code","5fe1ba33":"code","0e923f3a":"code","d66820cf":"code","2a4ffac4":"code","bafd42eb":"code","0dee58c7":"code","fb521fa5":"code","5a20d72a":"code","b3e3c7b6":"markdown","26ee8576":"markdown","12db381f":"markdown","6da1ece7":"markdown","e0986487":"markdown","daf13891":"markdown","ba51d3c4":"markdown","61bb84a7":"markdown","5c2792c6":"markdown","a43815e5":"markdown","f7a94859":"markdown","7926033c":"markdown","ba4d452c":"markdown","d6c453d8":"markdown","20389e3f":"markdown","85b4151b":"markdown","68ae3063":"markdown","516a8a70":"markdown","ed18a86d":"markdown","148d9d36":"markdown","a9be270d":"markdown","b4f6a81e":"markdown","0d650031":"markdown","1ff2d55b":"markdown","6d94e5c7":"markdown","21c55db0":"markdown","6018af97":"markdown","aee7e4e6":"markdown","c9d6ee33":"markdown","b843ca81":"markdown","9a9aeb77":"markdown","b83cc37d":"markdown","37a3d2dd":"markdown","7a3b27be":"markdown","70b3c35b":"markdown","83ff2828":"markdown","99c39cc0":"markdown","1143d929":"markdown","58b1ea82":"markdown","5875b0a0":"markdown","e9f4e68a":"markdown","ae14ff09":"markdown","3ccb43fc":"markdown","09fbb31b":"markdown","6d304916":"markdown","76d75361":"markdown","11e8dfa6":"markdown","fb3a883d":"markdown","3e232628":"markdown","46f52aa5":"markdown","d5e1d6dd":"markdown","d3af3488":"markdown","dc3be0fe":"markdown","06764409":"markdown","d9e7795e":"markdown","577e6abb":"markdown","697c09cd":"markdown","fb05a726":"markdown","e6d53dcd":"markdown","02a8e899":"markdown","2a827dc4":"markdown","628c3fc6":"markdown","97ce1030":"markdown","6a7f620b":"markdown","aaa1c030":"markdown","02fbbca6":"markdown","5827a9f3":"markdown","58bb69ea":"markdown","46861507":"markdown","d1c326c0":"markdown","51f5149f":"markdown","5fbafc01":"markdown","41e6aa22":"markdown","a27763df":"markdown","b3b388ae":"markdown","12985d98":"markdown","d4c6e2df":"markdown","75c2cbd5":"markdown","9623353b":"markdown","8279a5cf":"markdown","1f5d1242":"markdown","b207c7fb":"markdown","ff0b5047":"markdown","5e37c6b6":"markdown","a117e3e2":"markdown","7b4bd287":"markdown","81a96f56":"markdown","0e6d9191":"markdown","5c88c5d7":"markdown","a31bb7f0":"markdown","41b5d8ad":"markdown","898d7fa1":"markdown","6c8caacd":"markdown","fa5fe17f":"markdown","a9481c6d":"markdown","85e82ccd":"markdown","4c3548ea":"markdown","3c43d2e0":"markdown","7fa91463":"markdown","28f4fe25":"markdown","bbd0c2aa":"markdown","196b796a":"markdown","f4d653dc":"markdown","f34bd55e":"markdown","0bb09213":"markdown","4493331e":"markdown","c9a7a459":"markdown","732008fb":"markdown","6384cc49":"markdown","f978f8a6":"markdown","eaac6128":"markdown","73f30582":"markdown","55a52b09":"markdown","11f9877b":"markdown","f295b58e":"markdown"},"source":{"1c039114":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","0c205d0c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.set(style=\"whitegrid\")","9e27b95e":"import warnings\n\nwarnings.filterwarnings('ignore')","0104c19d":"data = '\/kaggle\/input\/income-classification\/income_evaluation.csv'\n\ndf = pd.read_csv(data)","70d1134d":"# print the shape\nprint('The shape of the dataset : ', df.shape)","d9c67374":"df.head()","3388111f":"col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n\ndf.columns = col_names\n\ndf.columns","fe16e46b":"df.info()","ee481f5a":"df.dtypes","4d519512":"df.describe()","991d56cc":"df.describe().T","31f9e19b":"df.describe(include='all')","4b42f5af":"### 4.7 Check for missing values <a class=\"anchor\" id=\"4.7\"><\/a>\n\n\n- In Python missing data is represented by two values:\n\n   - **None** : None is a Python singleton object that is often used for missing data in Python code.\n\n   - **NaN** : NaN is an acronym for Not a Number. It is a special floating-point value recognized by all systems   that use the standard IEEE floating-point representation.\n\n- There are different methods in place on how to detect missing values.\n\n\n#### Pandas isnull() and notnull() functions \n\n- Pandas offers two functions to test for missing values - **isnull()** and **notnull()**. \n\n- These are simple functions that return a boolean value indicating whether the passed in argument value is in fact missing data.\n\n\nBelow, I will list some useful commands to deal with missing values.\n\n\n#### Useful commands to detect missing values \n\n- **df.isnull()**\n\nThe above command checks whether each cell in a dataframe contains missing values or not. If the cell contains missing value, it returns True otherwise it returns False.\n\n- **df.isnull().sum()**\n\nThe above command returns total number of missing values in each column in the dataframe.\n\n- **df.isnull().sum().sum()**\n\nIt returns total number of missing values in the dataframe.\n\n\n- **df.isnull().mean()**\n\nIt returns percentage of missing values in each column in the dataframe.\n\n\n- **df.isnull().any()**\n\nIt checks which column has null values and which has not. The columns which has null values returns TRUE and FALSE otherwise.\n\n- **df.isnull().any().any()**\n\nIt returns a boolean value indicating whether the dataframe has missing values or not. If dataframe contains missing values it returns TRUE and FALSE otherwise.\n\n- **df.isnull().values.any()**\n\nIt checks whether a particular column has missing values or not. If the column contains missing values, then it returns TRUE otherwise FALSE.\n\n- **df.isnull().values.sum()**\n\nIt returns the total number of missing values in the dataframe.\n","cd2a54ba":"# check for missing values\n\ndf.isnull().sum()","0dc5a30a":"#assert that there are no missing values in the dataframe\n\nassert pd.notnull(df).all().all()","6da35368":"def initial_eda(df):\n    if isinstance(df, pd.DataFrame):\n        total_na = df.isna().sum().sum()\n        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n        print(\"Total NA Values : %d \" % (total_na))\n        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n        col_name = df.columns\n        dtyp = df.dtypes\n        uniq = df.nunique()\n        na_val = df.isna().sum()\n        for i in range(len(df.columns)):\n            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n        \n    else:\n        print(\"Expect a DataFrame but got a %15s\" % (type(df)))\n","1ef2ce84":"initial_eda(df)","1f39da15":"categorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :\\n\\n', categorical)","aad364ca":"df[categorical].head()","c4733041":"for var in categorical: \n    \n    print(df[var].value_counts())","1d8f1466":"for var in categorical:\n    \n     print(df[var].value_counts()\/np.float(len(df)))","265c47fa":"# check for missing values\n\ndf['income'].isnull().sum()","e49fcc36":"# view number of unique values\n\ndf['income'].nunique()","dcba18a1":"# view the unique values\n\ndf['income'].unique()","fda7546a":"# view the frequency distribution of values\n\ndf['income'].value_counts()","15135857":"# view percentage of frequency distribution of values\n\ndf['income'].value_counts()\/len(df)","c772c24d":"# visualize frequency distribution of income variable\n\nf,ax=plt.subplots(1,2,figsize=(18,8))\n\nax[0] = df['income'].value_counts().plot.pie(explode=[0,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Income Share')\n\n\n#f, ax = plt.subplots(figsize=(6, 8))\nax[1] = sns.countplot(x=\"income\", data=df, palette=\"Set1\")\nax[1].set_title(\"Frequency distribution of income variable\")\n\nplt.show()","8b78f093":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(y=\"income\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of income variable\")\nplt.show()","4f5e046d":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.countplot(x=\"income\", hue=\"sex\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of income variable wrt sex\")\nplt.show()","fb6e807d":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.countplot(x=\"income\", hue=\"race\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of income variable wrt race\")\nplt.show()","1fd04f28":"# check number of unique labels \n\ndf.workclass.nunique()","3778fcc0":"# view the unique labels\n\ndf.workclass.unique()","60c65fbd":"# view frequency distribution of values\n\ndf.workclass.value_counts()","3b8b79e1":"# replace '?' values in workclass variable with `NaN`\n\ndf['workclass'].replace(' ?', np.NaN, inplace=True)","a8c2b9fc":"# again check the frequency distribution of values in workclass variable\n\ndf.workclass.value_counts()","b81d483e":"f, ax = plt.subplots(figsize=(10, 6))\nax = df.workclass.value_counts().plot(kind=\"bar\", color=\"green\")\nax.set_title(\"Frequency distribution of workclass variable\")\nax.set_xticklabels(df.workclass.value_counts().index, rotation=30)\nplt.show()","fe447040":"f, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"workclass\", hue=\"income\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of workclass variable wrt income\")\nax.legend(loc='upper right')\nplt.show()","fc1c4590":"f, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"workclass\", hue=\"sex\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of workclass variable wrt sex\")\nax.legend(loc='upper right')\nplt.show()","7a454588":"# check number of unique labels\n\ndf.occupation.nunique()","4b4d7e5a":"# view unique labels\n\ndf.occupation.unique()\n","6a621558":"# view frequency distribution of values\n\ndf.occupation.value_counts()","0d8166c0":"# replace '?' values in occupation variable with `NaN`\n\ndf['occupation'].replace(' ?', np.NaN, inplace=True)\n","a80e6923":"# again check the frequency distribution of values\n\ndf.occupation.value_counts()","b6e0c3a3":"# visualize frequency distribution of `occupation` variable\n\nf, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"occupation\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of occupation variable\")\nax.set_xticklabels(df.occupation.value_counts().index, rotation=30)\nplt.show()","2a6a85a4":"# check number of unique labels\n\ndf.native_country.nunique()","0e178883":"# view unique labels \n\ndf.native_country.unique()\n","1a23072f":"# check frequency distribution of values\n\ndf.native_country.value_counts()\n","548f568b":"# replace '?' values in native_country variable with `NaN`\n\ndf['native_country'].replace(' ?', np.NaN, inplace=True)","6f4fb244":"# again check the frequency distribution of values\n\ndf.native_country.value_counts()","8cf86878":"# visualize frequency distribution of `native_country` variable\n\nf, ax = plt.subplots(figsize=(16, 12))\nax = sns.countplot(x=\"native_country\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of native_country variable\")\nax.set_xticklabels(df.native_country.value_counts().index, rotation=90)\nplt.show()","8b67fa2c":"df[categorical].isnull().sum()","80c85976":"# check for cardinality in categorical variables\n\nfor var in categorical:\n    \n    print(var, ' contains ', len(df[var].unique()), ' labels')","8f9388e6":"numerical = [var for var in df.columns if df[var].dtype!='O']\n\nprint('There are {} numerical variables\\n'.format(len(numerical)))\n\nprint('The numerical variables are :\\n\\n', numerical)","4ff5cfd6":"df[numerical].head()","fca2e5aa":"df[numerical].isnull().sum()","dceb321b":"df['age'].nunique()","55fb1671":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nax = sns.distplot(x, bins=10, color='blue')\nax.set_title(\"Distribution of age variable\")\nplt.show()","f263a6b4":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nx = pd.Series(x, name=\"Age variable\")\nax = sns.distplot(x, bins=10, color='blue')\nax.set_title(\"Distribution of age variable\")\nplt.show()","cee7c9da":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nx = pd.Series(x, name=\"Age variable\")\nax = sns.kdeplot(x, shade=True, color='red')\nax.set_title(\"Distribution of age variable\")\nplt.show()","8b7a337e":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nax = sns.boxplot(x)\nax.set_title(\"Visualize outliers in age variable\")\nplt.show()","474cf3bd":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.boxplot(x=\"income\", y=\"age\", data=df)\nax.set_title(\"Visualize income wrt age variable\")\nplt.show()","21ef7bc6":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.boxplot(x=\"income\", y=\"age\", hue=\"sex\", data=df)\nax.set_title(\"Visualize income wrt age and sex variable\")\nax.legend(loc='upper right')\nplt.show()","2b51687a":"plt.figure(figsize=(8,6))\nax = sns.catplot(x=\"income\", y=\"age\", col=\"sex\", data=df, kind=\"box\", height=8, aspect=1)\nplt.show()","a05269a7":"plt.figure(figsize=(12,8))\nsns.boxplot(x ='race', y=\"age\", data = df)\nplt.title(\"Visualize age wrt race\")\nplt.show()","72b78268":"# plot correlation heatmap to find out correlations\n\ndf.corr().style.format(\"{:.4}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)","3953ff5c":"sns.pairplot(df)\nplt.show()","a2232426":"sns.pairplot(df, hue=\"income\")\nplt.show()","564280d3":"sns.pairplot(df, hue=\"sex\")\nplt.show()","d3ae8ca4":"X = df.drop(['income'], axis=1)\n\ny = df['income']","ecfe8236":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","9a3ae37a":"# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","ade0e3b6":"categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n\ncategorical","b3789051":"numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n\nnumerical","9e45387a":"# print percentage of missing values in the categorical variables in training set\n\nX_train[categorical].isnull().mean()","009be2d2":"# print categorical variables with missing data\n\nfor col in categorical:\n    if X_train[col].isnull().mean()>0:\n        print(col, (X_train[col].isnull().mean()))","adca5f7f":"# impute missing categorical variables with most frequent value\n\nfor df2 in [X_train, X_test]:\n    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\n    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)\n    df2['native_country'].fillna(X_train['native_country'].mode()[0], inplace=True)    ","c4b90552":"# check missing values in categorical variables in X_train\n\nX_train[categorical].isnull().sum()","5dce6e79":"# check missing values in categorical variables in X_test\n\nX_test[categorical].isnull().sum()","ce077220":"# check missing values in X_train\n\nX_train.isnull().sum()","5b0cf651":"# check missing values in X_test\n\nX_test.isnull().sum()","93443dc5":"# preview categorical variables in X_train\n\nX_train[categorical].head()","64ec1642":"# import category encoders\n\nimport category_encoders as ce","459ae10d":"# encode categorical variables with one-hot encoding\n\nencoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n                                 'race', 'sex', 'native_country'])\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","6a03351a":"X_train.head()","8adc3959":"X_train.shape","15905292":"X_test.head()","2b797de1":"X_test.shape","36a198e0":"cols = X_train.columns\n","630b84c0":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n","04deb733":"X_train = pd.DataFrame(X_train, columns=[cols])","f42df05e":"X_test = pd.DataFrame(X_test, columns=[cols])","c1eb8668":"# import Random Forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n# instantiate the classifier \n\nrfc = RandomForestClassifier(random_state=0)\n\n\n\n# fit the model\n\nrfc.fit(X_train, y_train)\n\n\n\n# Predict the Test set results\n\ny_pred = rfc.predict(X_test)\n\n\n\n# Check accuracy score \n\nfrom sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","bc364126":"# instantiate the classifier with n_estimators = 100\n\nrfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nrfc_100.fit(X_train, y_train)\n\n\n\n# Predict on the test set results\n\ny_pred_100 = rfc_100.predict(X_test)\n\n\n\n# Check accuracy score \n\nprint('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred_100)))","5fe1ba33":"# create the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nclf.fit(X_train, y_train)\n","0e923f3a":"# view the feature scores\n\nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\nfeature_scores","d66820cf":"# Creating a seaborn bar plot\n\nf, ax = plt.subplots(figsize=(30, 24))\nax = sns.barplot(x=feature_scores, y=feature_scores.index, data=df)\nax.set_title(\"Visualize feature scores of the features\")\nax.set_yticklabels(feature_scores.index)\nax.set_xlabel(\"Feature importance score\")\nax.set_ylabel(\"Features\")\nplt.show()\n","2a4ffac4":"# drop the least important feature from X_train and X_test\n\nX_train = X_train.drop(['native_country_41'], axis=1)\n\nX_test = X_test.drop(['native_country_41'], axis=1)\n","bafd42eb":"# instantiate the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nclf.fit(X_train, y_train)\n\n\n# Predict on the test set results\n\ny_pred = clf.predict(X_test)\n\n\n\n# Check accuracy score \n\nprint('Model accuracy score with native_country_41 variable removed : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","0dee58c7":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\n","fb521fa5":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","5a20d72a":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","b3e3c7b6":"#### Findings\n\n- We can see that the dataset contains 9 character variables and 6 numerical variables.\n\n- `income` is the target variable.\n\n- There are no missing values in the dataset. I will explore this later,","26ee8576":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n\n1.\t[The problem statement](#1)\n1.\t[Import libraries](#2)\n1.\t[Import dataset](#3)\n1.\t[Exploratory data analysis](#4)\n1.  [Explore categorical variables](#5)\n1.  [Explore numerical variables](#6)\n1.  [Declare feature vector and target variable](#7)\n1.\t[Split data into separate training and test set](#8)\n1.\t[Feature Engineering](#9)\n1.  [Feature Scaling](#10)\n1.\t[Random Forest Classifier with default parameters](#11)\n1.\t[Random Forest Classifier with 100 Decision Tress](#12)\n1.\t[Find important features with Random Forest model](#13)\n1.\t[Visualize feature scores of the features](#14)\n1.\t[Build the Random Forest model on selected features](#15)\n1.\t[Confusion matrix](#16)\n1.\t[Classification report](#17)\n1.\t[Results and conclusion](#18)\n","12db381f":"#### Visualize `workclass` variable wrt `sex` variable","6da1ece7":"#### Explore `workclass` variable","e0986487":"### 4.9 Functional approach to EDA <a class=\"anchor\" id=\"4.9\"><\/a>\n\n- An alternative approach to EDA is to write a function that presents initial EDA of dataset.\n\n- We can write such a function as follows :-","daf13891":"That is the end of this kernel. I hope you find it useful and enjoyable.\n\nYour feedback and comments are most welcome.","ba51d3c4":"#### Visualize `workclass` variable wrt `income` variable","61bb84a7":"Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\n\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making. \n\n\nWe have another tool called `Confusion matrix` that comes to our rescue.","5c2792c6":"### Types of variables\n\n- In this section, I segregate the dataset into categorical and numerical variables. \n\n- There are a mixture of categorical and numerical variables in the dataset. \n\n- Categorical variables have data type object. Numerical variables have data type int64.\n\n- First of all, I will explore categorical variables.","a43815e5":"Now, I will build the random forest model again and check accuracy.","f7a94859":"### 9.1 Display categorical variables in training set\n","7926033c":"## 2. Import libraries <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n","ba4d452c":"## 5. Explore Categorical Variables <a class=\"anchor\" id=\"5\"><\/a>\n\n\n[Back to Table of Contents](#0.1)","d6c453d8":"We can see that there are 1843 values encoded as `?` in occupation variable. I will replace these `?` with `NaN`.","20389e3f":"We can see that there are no missing values in the `income` target variable.","85b4151b":"#### Explore `native_country` variable","68ae3063":"### 4.2 Preview the dataset <a class=\"anchor\" id=\"4.2\"><\/a>","516a8a70":"#### Explore `occupation` variable","ed18a86d":"## 6. Explore Numerical Variables <a class=\"anchor\" id=\"6\"><\/a>\n\n\n[Back to Table of Contents](#0.1)","148d9d36":"We can see that native_country column contains relatively large number of labels as compared to other columns. I will check for cardinality after train-test split.","a9be270d":"#### Interpretation\n\n\n- We can see that whites make more money than non-whites in both the income categories.","b4f6a81e":"## 10. Feature Scaling <a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)","0d650031":"Here, I have build the Random Forest Classifier model with default parameter of `n_estimators = 10`. So, I have used 10 decision-trees to build the model. Now, I will increase the number of decision-trees and see its effect on accuracy.","1ff2d55b":"## 15. Build the Random Forest model on selected features <a class=\"anchor\" id=\"15\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nNow, I will drop the least important feature `native_country_41` from the model, rebuild the model and check its effect on accuracy.","6d94e5c7":"#### Interpretation\n\n\n- We can see that workers make less than equal to 50k in most of the working categories.\n\n- But this trend is more appealing in Private `workclass` category.","21c55db0":"### 4.3 Rename column names <a class=\"anchor\" id=\"4.3\"><\/a>\n\nWe can see that the dataset does not have proper column names. The column names contain underscore. We should give proper names to the columns. I will do it as follows:-","6018af97":"### 9.4 Encode categorical variables\n","aee7e4e6":"We can use Pandas series object to get an informative axis label as follows :-","c9d6ee33":"## 11. Random Forest Classifier model with default parameters <a class=\"anchor\" id=\"11\"><\/a>\n\n\n[Back to Table of Contents](#0.1)","b843ca81":"## 7. Declare feature vector and target variable <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","9a9aeb77":"We now have X_train dataset ready to be fed into the Random Forest classifier. We will do it as follows.","b83cc37d":"#### Detect outliers in `age` variable with boxplot","37a3d2dd":"### 5.5 Percentage of frequency distribution of values","7a3b27be":"## 1. The problem statement <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\nIn this kernel, I try to make predictions where the prediction task is to determine whether a person makes over 50K a year. I implement Random Forest Classification with Python and Scikit-Learn. So, to answer the question, I build a Random Forest classifier to predict whether a person makes over 50K a year.\n\nI have used the **Income classification data set** for this project.\n","70b3c35b":"We can plot the bars horizontally as follows :-","83ff2828":"We can see that there are lots of outliers in `age` variable.","99c39cc0":"We can see that the above `df.describe().T` command presents statistical properties in horizontal form.","1143d929":"#### Comment\n\n- Now, we can see that there are several variables like `workclass`, `occupation` and `native_country` which contain missing values. \n\n- Generally, the missing values are coded as `NaN` and python will detect them with the usual command of df.isnull().sum().\n\n- But, in this case the missing values are coded as `?`. Python fail to detect these as missing values because it does not consider `?` as missing values. \n\n- So, I have to replace `?` with `NaN` so that Python can detect these missing values.\n\n- I will explore these variables and replace `?` with `NaN`.","58b1ea82":"#### Visualize relationship between `race` and `age`","5875b0a0":"### 4.1  View dimensions of dataset <a class=\"anchor\" id=\"4.1\"><\/a>","e9f4e68a":"There are 2 unique values in the `income` variable.","ae14ff09":"We can see that `age` is slightly positively skewed.","3ccb43fc":"## 12. Random Forest Classifier model with 100 Decision Trees  <a class=\"anchor\" id=\"12\"><\/a>\n\n\n\n[Back to Table of Contents](#0.1)","09fbb31b":"## 18. Results and Conclusion <a class=\"anchor\" id=\"18\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\n1.\tIn this project, I build a Random Forest Classifier to predict the income of a person. I build two models, one with 10 decision-trees and another one with 100 decision-trees. \n2.\tThe model accuracy score with `10 decision-trees is 0.8446` but the same with `100 decision-trees is 0.8521`. So, as expected accuracy increases with number of decision-trees in the model.\n3.\tI have used the Random Forest model to find only the important features, build the model using these features and see its effect on accuracy. \n4.\tI have removed the `native_country_41` variable from the model, rebuild it and checked its accuracy. The `accuracy of the model with native_country_41 variable removed is 0.8544`. So, we can see that the model accuracy has been improved with `native_country_41` variable removed from the model.\n5.\tConfusion matrix and classification report are another tool to visualize the model performance. They yield good performance.\n\n","6d304916":"### 5.3 Summary of categorical variables \n\n- There are 9 categorical variables in the dataset.\n\n- The categorical variables are given by `workclass`, `education`, `marital_status`, `occupation`, `relationship`, `race`, `sex`, `native_country` and `income`.\n\n- `income` is the target variable.","76d75361":"#### Visualize `income` wrt `race`","11e8dfa6":"### 5.4 Frequency distribution of categorical variables \n\nNow, we will check the frequency distribution of categorical variables.","fb3a883d":"#### View the distribution of `age` variable","3e232628":"#### Interpretation\n\n- I have removed the `native_country_41` variable from the model, rebuild it and checked its accuracy. \n\n- The accuracy of the model now comes out to be 0.8544. \n\n- The accuracy of the model with all the variables taken into account is 0.8521. \n\n- So, we can see that the model accuracy has been improved with `native_country_41` variable removed from the model.","46f52aa5":"Now, I will use the feature importance variable to see feature importance scores.","d5e1d6dd":"We can see that there are 583 values encoded as `?` in native_country variable. I will replace these `?` with `NaN`.","d3af3488":"## 14. Visualize feature scores of the features <a class=\"anchor\" id=\"14\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nNow, I will visualize the feature scores with matplotlib and seaborn.","dc3be0fe":"#### Interpretation\n\nWe can see that there are no missing values in the dataset.","06764409":"- The above `df.describe()` command presents statistical properties in vertical form.\n\n- If we want to view the statistical properties in horizontal form, we should run the following command.","d9e7795e":"We can see that the most important feature is `fnlwgt` and least important feature is `native_country_41`.","577e6abb":"### 4.5 Check the data types of columns <a class=\"anchor\" id=\"4.5\"><\/a>\n\n- The above `df.info()` command gives us the number of filled values along with the data types of columns.\n\n- If we simply want to check the data type of a particular column, we can use the following command.","697c09cd":"#### Plot pairwise relationships in dataset","fb05a726":"### 6.3 Summary of numerical variables\n\n- There are 6 numerical variables.\n\n- These are given by `age`, `fnlwgt`, `education_num`,`capital_gain`, `capital_loss` and `hours_per_week`.\n\n- All of the numerical variables are of discrete data type.","e6d53dcd":"#### Interpretation\n\n\n- We can see that males make more money than females in both the income categories.","02a8e899":"#### Interpretation\n\n- We can see that there is no strong correlation between variables.","2a827dc4":"## 8. Split data into separate training and test set <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","628c3fc6":"### 4.4 View summary of dataset <a class=\"anchor\" id=\"4.4\"><\/a>","97ce1030":"#### Explore `age` variable","6a7f620b":"We can see that there are no missing values in the numerical variables.","aaa1c030":"## 4. Exploratory data analysis <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\nNow, I will explore the data to gain insights about the data. ","02fbbca6":"Similarly, I will take a look at the X_test set.","5827a9f3":"- Now, we can see that there are no values encoded as `?` in the workclass variable.\n\n- I will adopt similar approach with `occupation` and `native_country` column.","58bb69ea":"We can see that there are 1836 values encoded as `?` in workclass variable. I will replace these `?` with `NaN`.","46861507":"#### Interpretation\n\n\n- We can see that there are lot more private workers than other category of workers.","d1c326c0":"The model accuracy score with 10 decision-trees is 0.8446 but the same with 100 decision-trees is 0.8521. So, as expected accuracy increases with number of decision-trees in the model.","51f5149f":"### 5.7 Check missing values in categorical variables ","5fbafc01":"#### Interpretation\n\n- Senior people make more money than younger people.","41e6aa22":"#### Interpretation\n\n- As expected, younger people make less money as compared to senior people.","a27763df":"[Go to Top](#0)","b3b388ae":"#### Interpretation\n\n- The above command does not throw any error. Hence, it is confirmed that there are no missing or negative values in the dataset.\n\n- All the values are greater than or equal to zero excluding character values.","12985d98":"* We now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called **feature scaling**. We will do it as follows.","d4c6e2df":"## 17. Classification Report <a class=\"anchor\" id=\"17\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model. I have described these terms in later.\n\nWe can print a classification report as follows:-","75c2cbd5":"We can shade under the density curve and use a different color as follows:-","9623353b":"### 6.4 Check missing values in numerical variables ","8279a5cf":"#### Visualize `income` wrt `age` and `sex` variable","1f5d1242":"We can see that there are no missing values in X_train and X_test.","b207c7fb":"## 9. Feature Engineering  <a class=\"anchor\" id=\"9\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n- **Feature Engineering** is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. \n\n- I will carry out feature engineering on different types of variables.\n\n- First, I will display the categorical and numerical variables in training set separately.","ff0b5047":"### 5.8 Number of labels: Cardinality \n\n- The number of labels within a categorical variable is known as **cardinality**. \n\n- A high number of labels within a variable is known as **high cardinality**. \n\n- High cardinality may pose some serious problems in the machine learning model. So, I will check for high cardinality.","5e37c6b6":"#### Find out the correlations","a117e3e2":"#### Interpretation\n\n\n- We can see that there are more male workers than female workers in all the working category.\n\n- The trend is more appealing in Private sector.","7b4bd287":"The two unique values are `<=50K` and `>50K`.","81a96f56":"### 9.2 Display numerical variables in training set\n","0e6d9191":"### 5.6 Explore the variables ","5c88c5d7":"#### Interpretation\n\n- Whites are more older than other groups of people.","a31bb7f0":"#### Visualize `workclass` variable","41b5d8ad":"<a class=\"anchor\" id=\"0\"><\/a>\n# Random Forest Classifier with Feature Importance\n\n\nHello friends,\n\n\nRandom Forest is a supervised machine learning algorithm which is based on ensemble learning. In this kernel, I build two Random Forest Classifier models to predict whether a person makes over 50K a year, one with 10 decision-trees and another one with 100 decision-trees. The expected accuracy increases with number of decision-trees in the model. I have demonstrated the **feature selection process** using the Random Forest model to find only the important features, rebuild the model using these features and see its effect on accuracy. I have used the **Income classification data set** for this project.","898d7fa1":"Now, we can see that `workclass`, `occupation` and `native_country` variable contains missing values.","6c8caacd":"### 6.2 Preview the numerical variables","fa5fe17f":"Here, **y_test** are the true class labels and **y_pred** are the predicted class labels in the test-set.","a9481c6d":"#### Explore `income` target variable ","85e82ccd":"## 16. Confusion matrix <a class=\"anchor\" id=\"16\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nA confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\n\nFour types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n\n**True Positives (TP)** \u2013 True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n\n**True Negatives (TN)** \u2013 True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n\n**False Positives (FP)** \u2013 False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**\n\n\n\n**False Negatives (FN)** \u2013 False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**\n\n\n\nThese four outcomes are summarized in a confusion matrix given below.\n","4c3548ea":"**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated.**","3c43d2e0":"### 4.8 Check with ASSERT statement <a class=\"anchor\" id=\"4.8\"><\/a>\n\n\n- We must confirm that our dataset has no missing values.\n\n- We can write an **Assert statement** to verify this.\n\n- We can use an assert statement to programmatically check that no missing, unexpected 0 or negative values are present.\n\n- This gives us confidence that our code is running properly.\n\n- **Assert statement** will return nothing if the value being tested is true and will throw an AssertionError if the value is false.\n\n- Asserts\n\n   - assert 1 == 1 (return Nothing if the value is True)\n\n   - assert 1 == 2 (return AssertionError if the value is False)","7fa91463":"### 5.2 Preview categorical variables ","28f4fe25":"We can see that there are 32561 instances and 15 attributes in the data set.","bbd0c2aa":"We can see that `United-States` dominate amongst the `native_country` variables.","196b796a":"### 6.1  Find numerical variables ","f4d653dc":"### 9.3 Engineering missing values in categorical variables","f34bd55e":"#### Interpretation\n\n- We can see that `age` and `fnlwgt` are positively skewed.\n\n- The variable `education_num` is negatively skewed while `hours_per_week` is normally distributed.\n\n- There exists weak positive correlation between `capital_gain` and `education_num` (correlation coefficient=0.1226). ","0bb09213":"## 3. Import dataset <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)\n","4493331e":"### 6.5 Explore numerical variables","c9a7a459":"### 5.1 Find categorical variables ","732008fb":"We can see that from the initial 14 columns, we now have 105 columns in training set.","6384cc49":"### 4.6 View statistical properties of dataset <a class=\"anchor\" id=\"4.6\"><\/a>","f978f8a6":"#### Visualize `income` wrt `sex` variable","eaac6128":"As a final check, I will check for missing values in X_train and X_test.","73f30582":"#### Explore relationship between `age` and `income` variables","55a52b09":"#### Important points to note\n\n\n- The above command `df.describe()` helps us to view the statistical properties of numerical variables. It excludes character variables.\n\n- If we want to view the statistical properties of character variables, we should run the following command -\n\n        `df.describe(include=['object'])`\n\n- If we want to view the statistical properties of all the variables, we should run the following command -\n\n        `df.describe(include='all')`","11f9877b":"## 13. Find important features with Random Forest model <a class=\"anchor\" id=\"13\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nUntil now, I have used all the features given in the model. Now, I will select only the important features, build the model using these features and see its effect on accuracy. \n\n\nFirst, I will create the Random Forest model as follows:-","f295b58e":"#### Interpretation\n\n\n- The above plot confirms that the most important feature is `fnlwgt` and least important feature is `native_country_41`."}}