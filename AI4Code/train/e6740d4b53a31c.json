{"cell_type":{"255a40dd":"code","6339fd6f":"code","c8ebbf6b":"code","04638467":"code","71cd01ff":"code","3f8b4a4a":"code","9620073f":"code","fe74b300":"code","1b0ee454":"code","18a7f93d":"code","5b3be4a6":"code","62267095":"code","bc418578":"code","5de940ba":"code","e7b4281a":"code","a1ba993d":"code","67459755":"code","e4ceae28":"code","245687bf":"code","4a0970d8":"code","8cbfd4db":"code","6598c124":"code","b73da2a4":"code","4a0b03ac":"code","b74f439b":"code","59c9cd78":"code","dcdd287b":"code","95fda4ae":"code","f81e2734":"code","a5042923":"code","7cb72e3f":"code","5203711c":"code","4713139b":"code","ff6d76db":"code","2ddd3038":"code","a062a5c3":"code","cbaf63d6":"code","ab459574":"code","1a1d46ee":"code","64482fad":"code","aa1d86e2":"code","c4900d70":"code","b28f75f4":"code","0106f418":"code","5cba3331":"code","baf46fb0":"code","c1649e5e":"code","11c8f993":"code","bfc5120a":"code","f6e06001":"code","57800776":"code","da12f73e":"code","ec595d4d":"code","72851756":"code","9aa992c1":"code","7d127e6b":"code","55d072fb":"code","bf3e5df7":"code","ddfdc34e":"code","9a4abc89":"code","6b875b95":"code","b3f908f9":"code","715a8254":"code","916d37c3":"code","0c2d19e6":"code","d9bc9b10":"code","b2206f66":"code","49ad9682":"code","34081603":"code","7a2280ff":"code","8f20e52a":"code","be03f65c":"code","0cfdd692":"code","e27067e6":"code","9e4e45ac":"code","62fa0540":"code","afa028a2":"code","de890e5f":"code","02713be2":"code","cc2363a4":"code","40ed12c4":"code","404fe3ee":"code","afaa789a":"code","64523a40":"code","22cf976c":"code","29c46696":"code","2720717e":"code","5079af70":"code","6f5a55ed":"code","2d7694a5":"code","c263948e":"code","7a411626":"code","ebf40470":"code","8e302895":"code","3ae17a87":"code","1a92d8fb":"code","1c7128eb":"code","7cdb2713":"code","8af69033":"code","a1e3f260":"code","76b476c9":"code","215c55f1":"code","ae1507a8":"code","f950af07":"code","1691f289":"code","c7a9fb7c":"code","0ca2adad":"code","db19120b":"markdown","cb3472f6":"markdown","34b88375":"markdown","265548c8":"markdown","9880c054":"markdown","15f0d785":"markdown","ee7dec00":"markdown","82371ea9":"markdown","28ac50e6":"markdown","92a2bce0":"markdown","b83933b8":"markdown","25d70027":"markdown","f454637d":"markdown","613e7f9f":"markdown","5131be4a":"markdown","e83b4a2d":"markdown","ef445696":"markdown","eb7320ed":"markdown","43431090":"markdown","2a2ac5d6":"markdown","09f8af49":"markdown","6bbd0966":"markdown","5abaf3b5":"markdown","e4f752a6":"markdown","b06dfef0":"markdown","ad1352d8":"markdown","57117100":"markdown","32b364fd":"markdown","c709e8a3":"markdown","866d3639":"markdown","a5ec28c1":"markdown","ff57c4dc":"markdown","912c0eaf":"markdown","2d44e48f":"markdown","4a8c0a40":"markdown","d537b601":"markdown"},"source":{"255a40dd":"#importing the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","6339fd6f":"data_place = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndata_place.head(10)","c8ebbf6b":"#Details about the data\ndata_place.info()\n#Totally there are 14 variables and we can remove the serial number variable as it is not important","04638467":"data_place.shape","71cd01ff":"# now checking the missing value imputation\ndata_place.isna().sum()","3f8b4a4a":"#We can fill the salary missing values as zero because the students who are not placed has givven as missing values\ndata_place[\"salary\"]=data_place[\"salary\"].fillna(0.0)\ndata_place.isna().sum()","9620073f":"data_place.drop(\"sl_no\",axis=1,inplace=True)\ndata_place.describe()","fe74b300":"data_place.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","1b0ee454":"data_place.skew()# there is no problem of skewness","18a7f93d":"data_place.dtypes","5b3be4a6":"sns.barplot(\"gender\",\"salary\",data=data_place)#we can say that the salary for male is more compared to the female","62267095":"\nprint(data_place.groupby('status')['gender'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['ssc_b'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['hsc_b'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['hsc_s'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['degree_t'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['workex'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['specialisation'].value_counts(normalize=True))","bc418578":"sns.countplot(\"gender\", hue=\"status\", data=data_place)\nplt.show()\n","5de940ba":"sns.barplot(\"gender\",\"salary\",hue=\"status\",data=data_place)\n#male candidates got high paid jobs than females","e7b4281a":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"gender\", data=data_place)\nplt.show()","a1ba993d":"sns.barplot(\"ssc_p\",\"status\",data=data_place)\n#We can say that on an average if a person takes above 50 percent in ssc he may get placed","67459755":"sns.countplot(\"ssc_b\",hue=\"status\",data=data_place)\n# we can say that the students in central board are placed more but the ssc education doesnt much effect the placement","e4ceae28":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"ssc_b\", data=data_place)\nplt.show()\n# the students studied in central board are got high paid salaries","245687bf":"sns.lineplot(\"ssc_p\", \"salary\", hue=\"ssc_b\", data=data_place)\nplt.show()\n#From this plot we can say that the candidates from central board with\n# ssc percentage with an average of 60 are getting highest paid job","4a0970d8":"sns.barplot(\"hsc_p\",\"status\",data=data_place)\n# we can say that on an average if student gets 50 percentage, there is possibility of getting placed","8cbfd4db":"sns.countplot(\"hsc_b\", hue=\"status\", data=data_place)\nplt.show()\n#In HSC other board students are placed more","6598c124":"sns.boxplot(\"salary\", \"hsc_b\", data=data_place)\n# The salary for central board candidates are high","b73da2a4":"sns.countplot(\"hsc_s\", hue=\"status\", data=data_place)\nplt.show()\n#Arts students placed ratio is low","4a0b03ac":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"hsc_b\", data=data_place)\nplt.show()\n# Salary for Central board candidates are high","b74f439b":"sns.lineplot(\"hsc_p\", \"salary\", hue=\"hsc_b\", data=data_place)\nplt.show()\n# A candidate from HSC central board with 60 percentage are getting highest paid job ","59c9cd78":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"hsc_s\", data=data_place)\nplt.show()\n#The salary for the science students in HSC is more","dcdd287b":"#Kernel-Density Plot\nsns.kdeplot(data_place.degree_p[ data_place.status==\"Placed\"])\nsns.kdeplot(data_place.degree_p[ data_place.status==\"Not Placed\"])\nplt.legend([\"Placed\", \"Not Placed\"])\nplt.xlabel(\"Under Graduate Percentage\")\nplt.show()\n# the placed rate will be high if the degree percentage is around 55 percentage","95fda4ae":"sns.countplot(\"degree_t\", hue=\"status\", data=data_place)\nplt.show()\n#commerce&Mmt students are more placed","f81e2734":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"degree_t\", data=data_place)\nplt.show()\n# we can say that the students in Sci-tech get high paid jobs but Comm&mgmt students are getting good jobs","a5042923":"sns.countplot(\"workex\",hue=\"status\",data=data_place)\n#So the students with experience are getting placed and their chance of not getting place is less","7cb72e3f":"sns.barplot(\"salary\",\"workex\",data=data_place)\n#Workexperinced candidates are getting high paid jobs","5203711c":"sns.barplot(\"etest_p\",\"status\",data=data_place)\n#So this feature doesnt effect placement","4713139b":"sns.lineplot(\"etest_p\",\"salary\",data=data_place)\n# so this doesnt effect the salary also","ff6d76db":"sns.barplot(\"specialisation\",\"salary\",hue=\"status\",data=data_place)\n#mkt and finance students are getting highly paid  jobs","2ddd3038":"sns.countplot(\"specialisation\",hue=\"status\",data=data_place)\n#Market and finance candidates are getting placed more","a062a5c3":"sns.barplot(\"mba_p\",\"status\",data=data_place)\n#this doesnt effect status","cbaf63d6":"sns.lineplot('mba_p',\"salary\",data=data_place)\n#doesnt effect salary ","ab459574":"data_new = data_place.drop([\"hsc_b\",\"ssc_b\",\"salary\"],axis=1)\ndata_new.info()","1a1d46ee":"data_new[\"gender\"].value_counts()","64482fad":"data_new.loc[data_new['gender']=='M','gender']= 0\n\ndata_new.loc[data_new['gender']=='F','gender']= 1\n\ndata_new[\"gender\"] = data_new[\"gender\"].astype(int)\n\n\ndata_new[\"hsc_s\"] = data_new.hsc_s.map({\"Commerce\":0,\"Science\":1,\"Arts\":2})\ndata_new[\"degree_t\"] = data_new.degree_t.map({\"Comm&Mgmt\":0,\"Sci&Tech\":1, \"Others\":2})\ndata_new[\"workex\"] = data_new.workex.map({\"No\":0, \"Yes\":1})\ndata_new[\"status\"] = data_new.status.map({\"Not Placed\":0, \"Placed\":1})\ndata_new[\"specialisation\"] = data_new.specialisation.map({\"Mkt&HR\":0, \"Mkt&Fin\":1})","aa1d86e2":"data_new.dtypes","c4900d70":"#importing the necessary libraries\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, fbeta_score, confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#NAIVE_BAYES MODEL\nfrom sklearn.naive_bayes import GaussianNB\n\n#SVC \nfrom sklearn.svm import SVC\n\n#XGBOOST\nfrom xgboost import XGBClassifier\nimport pandas as pd","b28f75f4":"data_new.dtypes","0106f418":"cor=data_new.corr()\nplt.figure(figsize=(14,6))\nsns.heatmap(cor,annot=True)","5cba3331":"#from correlation plot we can remove the hsc_s and degree_t\ndata_new.drop([\"hsc_s\",\"degree_t\"],axis=1,inplace=True)","baf46fb0":"x = data_new.drop(\"status\", axis =1).values#independent variable\n\ny = data_new[\"status\"].values #dependant variable\n#train and test data split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 42)\n\nprint(x_train.shape, x_test.shape)\n#NOTE:\n#.values will store the values in the form of array\n#if you not give x will store the values in series","c1649e5e":"#to feed the random state\nseed = 42\n\n#prepare models\nmodels = []\nmodels.append((\"LR\", LogisticRegression()))\nmodels.append((\"LDA\", LinearDiscriminantAnalysis()))\nmodels.append((\"KNN\", KNeighborsClassifier()))\nmodels.append((\"CART\", DecisionTreeClassifier()))\nmodels.append((\"NB\", GaussianNB()))\nmodels.append((\"RF\", RandomForestClassifier()))\nmodels.append((\"SVM\", SVC(gamma = 'auto')))\nmodels.append((\"XGB\", XGBClassifier()))\n#appending all the models with their names","11c8f993":"import warnings \nwarnings.filterwarnings(\"ignore\")# to avoid the warnings in our data-set\nresult = []\nnames = []\nscoring = 'recall'\nseed = 42\n\nfor name, model in models:\n    kfold = KFold(n_splits = 5, random_state =seed)# 5 split of data (value of k)\n    cv_results = cross_val_score(model, x_train, y_train, cv = kfold, scoring = scoring)\n    result.append(cv_results)\n    names.append(name)\n    msg = (name, cv_results.mean(), cv_results.std())\n    print(msg)","bfc5120a":"#boxplot results for choosing our algorithm\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,4))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(1,1,1)\nplt.boxplot(result)\nax.set_xticklabels(names)\nplt.show()","f6e06001":"#precion\nimport warnings \nwarnings.filterwarnings(\"ignore\")# to avoid the warnings in our data-set\nresult1 = []\nnames = []\nscoring = 'precision'\nseed = 42\n\nfor name, model in models:\n    kfold = KFold(n_splits = 5, random_state =seed)# 5 split of data (value of k)\n    cv_results = cross_val_score(model, x_train, y_train, cv = kfold, scoring = scoring)\n    result1.append(cv_results)\n    names.append(name)\n    msg1 = (name, cv_results.mean(), cv_results.std())\n    print(msg1)\n#first one is mean value of a model, next one is the std deviation","57800776":"#boxplot results for choosing our algorithm\nfig = plt.figure(figsize = (8,4))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(1,1,1)\nplt.boxplot(result1)\nax.set_xticklabels(names)\nplt.show()","da12f73e":"# default scoring is a accuracy\nimport warnings \nwarnings.filterwarnings(\"ignore\")# to avoid the warnings in our data-set\nresult2 = []\nnames = []\nseed = 42\n\nfor name, model in models:\n    kfold = KFold(n_splits = 5, random_state =seed)# 5 split of data (value of k)\n    cv_results = cross_val_score(model, x_train, y_train, cv = kfold)\n    result2.append(cv_results)\n    names.append(name)\n    msg1 = (name, cv_results.mean(), cv_results.std())\n    print(msg1)\n#first one is mean value of a model, next one is the std deviation","ec595d4d":"fig = plt.figure(figsize = (8,4))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(1,1,1)\nplt.boxplot(result2)\nax.set_xticklabels(names)\nplt.show()","72851756":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(criterion='entropy')\ndtree.fit(x_train, y_train)\ny_pred = dtree.predict(x_test)","9aa992c1":"print(classification_report(y_test,y_pred))","7d127e6b":"#Using Random Forest Algorithm\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)","55d072fb":"print(classification_report(y_test,y_pred))","bf3e5df7":"# creating confusion matrix heatmap\n\nconf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","ddfdc34e":"\nfrom sklearn.ensemble import ExtraTreesClassifier\nx2= data_new.drop(\"status\",axis=1)\ny= data_new[\"status\"]\n\nmodel = ExtraTreesClassifier(n_estimators =5, criterion = 'entropy')\n\nmodel.fit(x2,y)\n\nfi = model.feature_importances_\n\nprint(fi)","9a4abc89":"fi_df = pd.DataFrame({'fi':fi, \"feature\":x2.columns})\nfi_df.head()","6b875b95":"fi_df.sort_values([\"fi\"],ascending = False)","b3f908f9":"x2_col = fi_df[fi_df[\"fi\"]>0.05]\nx2_col","715a8254":"#now we are going to extract only these features\nx2 = x2[x2_col[\"feature\"]]\nx2.head()","916d37c3":"#now with these values of x and y we are going to build the model\nfrom sklearn.preprocessing import MinMaxScaler\n\nstd_data = MinMaxScaler()\nstd_data = std_data.fit_transform(x2)\nstd_data = pd.DataFrame(std_data, columns =x2.columns)\nstd_data.head()","0c2d19e6":"x_train, x_test, y_train, y_test = train_test_split(std_data,y, test_size = 0.25, random_state = 100)\nmodel1 = RandomForestClassifier().fit(x_train,y_train)\n\ny_pred = model1.predict(x_test)\n\nprint(classification_report(y_test,y_pred))","d9bc9b10":"# creating confusion matrix heatmap\n\nconf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","b2206f66":"from sklearn.model_selection import RandomizedSearchCV\n#no of trees in randomforest\n#n_estimators = [100,200,500]\n\n#no of features to consider at every split\nmax_features = ['auto','sqrt']\n\n#max number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10,110,11)]\n\n#minimum no of samples required at each node\nmin_samples_leaf = [1,2,4]\n\n\nrandom_grid = {'max_features':max_features,'max_depth':max_depth,'min_samples_leaf':min_samples_leaf}\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator=rf,param_distributions = random_grid, n_iter= 100, cv=3,verbose=1,n_jobs=2,random_state=11)\n\nrf_random.fit(x_train,y_train)","49ad9682":"rf_random.best_estimator_","34081603":"newmod = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=4, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=200,\n                       n_jobs=None, oob_score=False, random_state=42,\n                       verbose=0, warm_start=False).fit(x_train,y_train)","7a2280ff":"y_pred = newmod.predict(x_test)\nprint(classification_report(y_test,y_pred))","8f20e52a":"data_n= data_place\ndata_n.dtypes","be03f65c":"data_n[\"gender\"] = data_n.gender.map({\"M\":0,\"F\":1})\ndata_n[\"ssc_b\"] = data_n.ssc_b.map({\"Others\":0,\"Central\":1})\ndata_n[\"hsc_b\"] = data_n.hsc_b.map({\"Others\":0,\"Central\":1})\ndata_n[\"hsc_s\"] = data_n.hsc_s.map({\"Commerce\":0,\"Science\":1,\"Arts\":2})\ndata_n[\"degree_t\"] = data_n.degree_t.map({\"Comm&Mgmt\":0,\"Sci&Tech\":1, \"Others\":2})\ndata_n[\"workex\"] = data_n.workex.map({\"No\":0, \"Yes\":1})\ndata_n[\"status\"] = data_n.status.map({\"Not Placed\":0, \"Placed\":1})\ndata_n[\"specialisation\"] = data_n.specialisation.map({\"Mkt&HR\":0, \"Mkt&Fin\":1})\n","0cfdd692":"cor=data_n.corr()\nplt.figure(figsize=(14,6))\nsns.heatmap(cor,annot=True)","e27067e6":"# Seperating Features and Target\nX = data_n[[ 'ssc_p', 'hsc_p', 'hsc_s', 'degree_p',  'workex','etest_p', 'specialisation', 'mba_p',]]\ny = data_n['status']","9e4e45ac":"# Let us now split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify =y)","62fa0540":"from sklearn.metrics import make_scorer, accuracy_score,precision_score\nfrom sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nrandomForestFinalModel = RandomForestClassifier(n_estimators=200,criterion='gini',\n max_depth= 4 ,\n max_features= 'auto',random_state=42)\nrandomForestFinalModel.fit(X_train, y_train)\npredictions_rf = randomForestFinalModel.predict(X_test)\n\nprint(classification_report(y_test,predictions_rf))","afa028a2":"conf_mat = pd.DataFrame(confusion_matrix(y_test, predictions_rf))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","de890e5f":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\ndf = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndf.head(10)","02713be2":"df[\"salary\"]=df[\"salary\"].fillna(0.0)\ndf.isna().sum()","cc2363a4":"X1 = df.drop(['status',\"salary\"], axis = 1)\ny1 = df.status\n\nfrom sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\n\n#placed as 1, not placed as 0\ny1 = encoder.fit_transform(y1)\nX1 = pd.get_dummies(X1)","40ed12c4":"from sklearn.model_selection import train_test_split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X1, y1, test_size= 0.3, random_state=41)","404fe3ee":"knn = KNeighborsClassifier(n_neighbors= 5 )\nknn.fit(X_train2, y_train2)\ny_pred2 = knn.predict(X_test2)\n","afaa789a":"print(accuracy_score(y_test2,y_pred2))\n\nprint(classification_report(y_test2, y_pred2))","64523a40":"conf_mat = pd.DataFrame(confusion_matrix(y_test2, y_pred2))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","22cf976c":"dataset = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndataset","29c46696":"X=dataset.iloc[:,[2,4]].values # X contain columns hsc_p and ssc_p\nY=dataset.iloc[:,12].values.reshape(-1,1)","2720717e":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)","5079af70":"#fitting multiple linear regression to the training set\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,Y_train)\n\n#predicting the test result\ny_pred=regressor.predict(X_test)","6f5a55ed":"#Let\u2019s check out the coefficients for the predictors:\nregressor.coef_","2d7694a5":"regressor.intercept_","c263948e":"from sklearn.metrics import r2_score\nr2_score(Y_test, y_pred)","7a411626":"X1=dataset.iloc[:,[2,7]].values\nY1=dataset.iloc[:,12].values.reshape(-1,1)","ebf40470":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X1,Y1,test_size=0.2,random_state=0)","8e302895":"#fitting multiple linear regression to the training set\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,Y_train)\n\n#predicting the test result\ny_pred=regressor.predict(X_test)\n","3ae17a87":"#Let\u2019s check out the coefficients for the predictors:\nregressor.coef_","1a92d8fb":"regressor.intercept_","1c7128eb":"from sklearn.metrics import r2_score\nr2_score(Y_test, y_pred)","7cdb2713":"X2=dataset.iloc[:,[4,7]].values\nY2=dataset.iloc[:,12].values.reshape(-1,1)","8af69033":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X2,Y2,test_size=0.2,random_state=0)\n\n#fitting multiple linear regression to the training set\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,Y_train)\n\n#predicting the test result\ny_pred=regressor.predict(X_test)\n","a1e3f260":"#Let\u2019s check out the coefficients for the predictors:\nregressor.coef_\n","76b476c9":"regressor.intercept_","215c55f1":"from sklearn.metrics import r2_score\nr2_score(Y_test, y_pred)","ae1507a8":"X3=dataset.iloc[:,[2,4,7]].values # X contain columns hsc_p and ssc_p\nY3=dataset.iloc[:,12].values.reshape(-1,1)","f950af07":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X3,Y3,test_size=0.2,random_state=0)\n\n#fitting multiple linear regression to the training set\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,Y_train)\n\n#predicting the test result\ny_pred=regressor.predict(X_test)","1691f289":"regressor.intercept_","c7a9fb7c":"r2_score(Y_test, y_pred)","0ca2adad":"regressor.coef_","db19120b":"So we have more samples of male than female and male are getting high paid jobs compared to the female","cb3472f6":"# HSC_P, DEGREE_P VS MBA_P","34b88375":"# BASED ON THE CORRELATION AND PREVIOUS STEPS WE HAVE SELECTED THE IMPORTANT FEATURES","265548c8":"# ENCODING","9880c054":"# Univariate analysis","15f0d785":"# Analysing which model will be best for this data-set","ee7dec00":"# The regression equation is \n\nY= 38.56 + 0.13(ssc_p) + 0.225(hsc_p)\n\nRscore is 17%","82371ea9":"# KNN CLASSIFICATION","28ac50e6":"# NOTE:\n\n1) GENDER: we can say that male students are placed more than females but in non-placed status also male has high ratio\n\n2) SSC_B and HSC_B: we can say that this fetaure may not be important\n","92a2bce0":"#For accuracy the best model is Random-forest","b83933b8":"# Feature: ssc_p (Secondary Education percentage), ssc_b (Board Of Education)","25d70027":"# Bi-variate and multi-variate analysis","f454637d":"# SSC_P, DEGREE_P vs MBA_P","613e7f9f":"# MBA percantage","5131be4a":"# EMPLOYABILITY TEST PERCENTAGE","e83b4a2d":"#For precision is DECISION-TREE","ef445696":"# SSC_P and HSC_P vs MBA_P","eb7320ed":"# The regression equation is \n\nY= 0.0814(ssc_p)+ 0.10799(hsc_p)+ 0.17898(degree_p)","43431090":"# DECISION TREE","2a2ac5d6":"# GENDER VS SALARY","09f8af49":"# ETS Method","6bbd0966":"# HSC_P, SSC_P, DEGREE_P vs MBA_P","5abaf3b5":"# HYPER-PARAMETER TUNING","e4f752a6":"# EDA and VISUALISATION","b06dfef0":"# Degree percentage and Degree Specialisation","ad1352d8":"# The regression equation is\nY= 44.04 + 0.138(hsc_p) + 0.225(degree_p)","57117100":"# BASED ON DIFFERENT CATEGORICAL VARIABLES SEEING THE STATUS OF THE CANDIDATES","32b364fd":"# GENDER vs STATUS","c709e8a3":"# MULTIPLE LINEAR REGRESSION","866d3639":"# FEATURE SELECTION:\n The feature which is not important are \nsalary, hsc_b and ssc_b","a5ec28c1":"# CONCLUSION:\n\nThus we can say that the model built with ssc_p and degree_p has good rscore value compared to other models.\n\nFor this data-set,\n\nSo we can say that in order to predict mba percentage, the ssc percentage and degree percentage is enough rather than using\n\nhsc percentage feature","ff57c4dc":"# POST GRADUATE SPECIALISATION ","912c0eaf":"#FOR recall the best model is NAIVEBAYES","2d44e48f":"# CONCLUSION:\n\nTHUS WE CAN SAY THAT RANDOM-FOREST WILL BE THE BEST MODEL COMPARED TO OTHER MODELS FOR THIS DATA-SET WHICH GIVES BETTER SOLUTION","4a8c0a40":"# Feature: hsc_p (Higher Secondary Education percentage), hsc_b (Board Of Education), hsc_s (Specialization in Higher Secondary Education)","d537b601":"# The regression equation is \n\nY = 39.6580 + 0.123(ssc_p) +0.215(degree_p)"}}