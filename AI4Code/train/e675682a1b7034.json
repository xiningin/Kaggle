{"cell_type":{"867c772b":"code","50822c4a":"code","d55b2963":"code","dfd4615d":"code","5f89131e":"code","a8f3da7f":"code","4a8fdece":"code","0069966b":"code","8a91a9a0":"code","b827224a":"code","1eb2edd2":"code","30f50dab":"code","cdef52c4":"code","f3199840":"code","c8a34f3c":"code","1bd9235a":"code","4f00d3b8":"code","66d17bf4":"code","9ee8ffcd":"code","d460e79a":"code","821e479b":"code","bc0a1cd5":"code","3320ccbf":"code","3c4a3fda":"code","80c44a0e":"code","90e206e5":"code","950292d9":"code","07b9e698":"code","a1843998":"code","d11415a8":"code","ba40abb0":"code","6f8e67d4":"code","8908bcc2":"code","6734295f":"code","0bb2df0e":"code","6b377bd9":"code","cfbc6c2a":"code","89e9ead4":"code","0689584b":"code","7a36f733":"code","95d62f44":"code","1a655e25":"code","2eaf202d":"code","c7a7934d":"code","992747f7":"code","2ca025e8":"code","737a85fa":"code","db06c93e":"code","31bcced4":"code","9a0fcad3":"code","c554aaf4":"code","a591660b":"code","9704f0e8":"code","ba84f25c":"code","2c45c1c5":"code","a006e48f":"code","ad7301b7":"code","a57f29d2":"code","0156056a":"code","59e07aea":"code","d568662a":"code","f4a6ab02":"code","3bc6fe1a":"code","34e1257f":"code","30741c89":"code","4e8239ed":"code","3ba2b9c2":"code","0dc34224":"code","7f2ff894":"code","ac94126b":"code","947c4f52":"code","db25342c":"code","e69fd03f":"code","9cf93059":"code","91c8532a":"code","00d26992":"code","66d27158":"code","128acb71":"code","fca68b12":"code","965595e3":"code","613e1ae5":"code","6835bf9d":"markdown","8748d054":"markdown","64d62262":"markdown","9318fabf":"markdown","905a02a2":"markdown","fedc26fe":"markdown","9154bd97":"markdown","ba33a118":"markdown","ffc75f2c":"markdown","fc6a8981":"markdown","d8ef16db":"markdown","5ff313bb":"markdown","856f4486":"markdown","a81781eb":"markdown","979e386a":"markdown","a82f65c0":"markdown","52aaab1f":"markdown","4455e95c":"markdown","5d5ca185":"markdown","97ec87c3":"markdown","0295e20b":"markdown","b568fa05":"markdown"},"source":{"867c772b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","50822c4a":"# Reading the dataset\ntrain = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\n\nhouses = pd.concat([train,test], axis=0, ignore_index=True)","d55b2963":"# Inspect the shape of the dataset\nhouses.shape","dfd4615d":"# Inspect the different columsn in the dataset\nhouses.columns","5f89131e":"# Let's take a look at the first few rows\nhouses.head()","a8f3da7f":"# Check the summary of the dataset\nhouses.describe()","4a8fdece":"# Summary of the dataset: \nprint(houses.info())","0069966b":"# All numeric (float and int) variables in the dataset\nhouses_numeric = houses.select_dtypes(include=['float64', 'int64'])\nhouses_numeric.head()","8a91a9a0":"# dropping the columns we want to treat as categorical variables\nhouses_numeric = houses_numeric.drop(['MSSubClass', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', \n                                    'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                                   'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', \n                                   'MoSold', 'YrSold'], axis=1)\nhouses_numeric.head()","b827224a":"# Pairwise scatter plot\n\n#plt.figure(figsize=(20, 10))\n#sns.pairplot(houses_numeric)\n#plt.show()","1eb2edd2":"# Correlation matrix\ncor = houses_numeric.corr()\ncor","30f50dab":"# Figure size\nplt.figure(figsize=(18,10))\n\n# Heatmap\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\nplt.show()","cdef52c4":"# Check the number of missing values in each column\n\nhouses.isnull().sum()","f3199840":"# Let's now check the percentage of missing values in each column\n\nround(100*(houses.isnull().sum()\/len(houses.index)), 2)","c8a34f3c":"##Repalcing NA values with meaningful values as NA means the facility is not present\n\n#NA in Alley column means No Alley\nhouses['Alley'].fillna('No Alley', inplace=True)\n\nhouses['MasVnrType'].fillna('None', inplace=True) \n\n#NA in FireplaceQu column means No Fireplace\nhouses['FireplaceQu'].fillna('No Fireplace', inplace=True)\n\n#NA in PoolQC column means No Pool\nhouses['PoolQC'].fillna('No Pool', inplace=True) \n\n#NA in Fence column means No Fence\nhouses['Fence'].fillna('No Fence', inplace=True) ","1bd9235a":"houses['MasVnrArea'].fillna(0, inplace=True) \nhouses['LotFrontage'].fillna(0, inplace=True) ","4f00d3b8":"houses['GarageType'].fillna('No Garage', inplace=True) \nhouses['GarageFinish'].fillna('No Garage', inplace=True) \nhouses['GarageQual'].fillna('No Garage', inplace=True) \nhouses['GarageCond'].fillna('No Garage', inplace=True) ","66d17bf4":"# Dropping \"FireplaceQu\" having 47% missing values\nhouses.drop(['MiscFeature'], axis = 1, inplace = True)","9ee8ffcd":"# Get the value counts of all the columns\n\n#for column in houses:\n#    if round(100*(houses[column].astype('category').value_counts()\/len(houses.index)), 2)[0] >90:\n #       print('___________________________________________________')","d460e79a":"## Dropping highly skewed features\nhouses.drop(['LandSlope','LowQualFinSF', 'BsmtHalfBath', \n         'ScreenPorch', 'PoolArea', 'MiscVal'], axis = 1, inplace = True)","821e479b":"# Check the number of null values again\nhouses.isnull().sum()","bc0a1cd5":"# Fill the empty values with median values\nhouses= houses.apply(lambda x: x.fillna(x.value_counts().index[0]))","3320ccbf":"# Check the number of null values again\nhouses.isnull().sum()","3c4a3fda":"print(len(houses.index))\nprint(len(houses.index)\/1460)","80c44a0e":"# Let's look at the dataset again\n\nhouses.head()","90e206e5":"houses.drop(['Id','MSZoning'], 1, inplace = True)","950292d9":"houses.head(20)","07b9e698":"#converting from int type to object to treat the variables as categorical variables\n\nhouses['MSSubClass'] = houses['MSSubClass'].astype('object')\nhouses['OverallQual'] = houses['OverallQual'].astype('object')\nhouses['OverallCond'] = houses['OverallCond'].astype('object')\nhouses['BsmtFullBath'] = houses['BsmtFullBath'].astype('object')\n#houses['BsmtHalfBath'] = houses['BsmtHalfBath'].astype('object')\nhouses['FullBath'] = houses['FullBath'].astype('object')\nhouses['HalfBath'] = houses['HalfBath'].astype('object')\nhouses['BedroomAbvGr'] = houses['BedroomAbvGr'].astype('object')\nhouses['KitchenAbvGr'] = houses['KitchenAbvGr'].astype('object')\nhouses['TotRmsAbvGrd'] = houses['TotRmsAbvGrd'].astype('object')\nhouses['Fireplaces'] = houses['Fireplaces'].astype('object')\nhouses['GarageCars'] = houses['GarageCars'].astype('object')","a1843998":"# subset all categorical variables\nhouse_categorical = houses.select_dtypes(include=['object'])\nhouse_categorical.head()","d11415a8":"# convert into dummies\nhouse_dummies = pd.get_dummies(house_categorical, drop_first=True)\nhouse_dummies.head()","ba40abb0":"# drop categorical variables \nhouses = houses.drop(list(house_categorical.columns), axis=1)","6f8e67d4":"houses = pd.concat([houses, house_dummies], axis=1)","8908bcc2":"houses.shape","6734295f":"CurrentDate = 2020\n\nhouses['Age of House'] = CurrentDate - houses['YearBuilt']\nhouses.drop(['YearBuilt'], 1, inplace = True)\n\nhouses['last remodeled age'] = CurrentDate - houses['YearRemodAdd']\nhouses.drop(['YearRemodAdd'], 1, inplace = True)\n\nhouses['Last deal of the house'] = CurrentDate - houses['YrSold']\nhouses.drop(['YrSold'], 1, inplace = True)\n\nhouses['Garage Age'] = CurrentDate - houses['GarageYrBlt']\nhouses.drop(['GarageYrBlt'], 1, inplace = True)","0bb2df0e":"houses.shape","6b377bd9":"houses.head()","cfbc6c2a":"train_data = houses.iloc[:train.shape[0]]\ntrain_data['SalePrice'].tail()","89e9ead4":"train_data","0689584b":"test_data = houses.iloc[train.shape[0]:]\ntest_data =test_data.drop(['SalePrice'], 1)\n#test_data['SalePrice'].head()","7a36f733":"# Put all the feature variables in X\n\nX = train_data.drop(['SalePrice'], 1)\nX.head()","95d62f44":"# Put the target variable in y\n\ny = train_data['SalePrice']\n\ny.head()","1a655e25":"# target variable: price of car\nsns.distplot(train_data['SalePrice'])\nplt.show()","2eaf202d":"y = np.log1p(y)","c7a7934d":"# target variable: price of car\nsns.distplot(y)\nplt.show()","992747f7":"houses_numeric.head()","2ca025e8":"# Import the StandardScaler()\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a scaling object\nscaler = StandardScaler()\n\n# Create a list of the variables that you need to scale\nvarlist = ['LotFrontage','LotArea','MasVnrArea','TotalBsmtSF',\n       '1stFlrSF','2ndFlrSF','GrLivArea','GarageArea','WoodDeckSF','OpenPorchSF']\n\n# Scale these variables using 'fit_transform'\nX[varlist] = scaler.fit_transform(X[varlist])\ntest_data[varlist] = scaler.transform(test_data[varlist])","737a85fa":"test_data.head()","db06c93e":"X.head()","31bcced4":"# split into train and test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","9a0fcad3":"X_train","c554aaf4":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","a591660b":"#checking the value of optimum number of parameters\nprint(model_cv.best_estimator_)\nprint(model_cv.best_params_)\nprint(model_cv.best_score_)","9704f0e8":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=1000]\ncv_results","ba84f25c":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\nplt.figure(figsize=(16,5))\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","2c45c1c5":"print(model_cv.best_estimator_)\nprint(model_cv.best_params_)\nprint(model_cv.best_score_)","a006e48f":"alpha = 10\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\nridge.coef_","ad7301b7":"#lets predict the R-squared value of test and train data\nfrom sklearn import metrics\n\ny_train_pred = ridge.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))","a57f29d2":"type(ridge.coef_)","0156056a":"coeffs = np.squeeze (np.asarray(ridge.coef_))\nprint(coeffs)","59e07aea":"X_train.columns","d568662a":"ridge_features = pd.Series(coeffs, index = X_train.columns)\nridge_features.abs().sort_values(ascending=False)","f4a6ab02":"alpha = 10\n\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train,y_train)\npreds1 = ridge.predict(test_data)\nfinal_predictions_ridge = np.exp(preds1)","3bc6fe1a":"ridge_values = pd.DataFrame({'Id': test_data.index , \n                             'SalePrice_predicted':final_predictions_ridge })\nridge_values.head()","34e1257f":"lasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","30741c89":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","4e8239ed":"#find out the R-squared value of the lasso model\nmodel_cv1 = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv1.fit(X_train, y_train)","3ba2b9c2":"# cv results\ncv_results1 = pd.DataFrame(model_cv1.cv_results_)\ncv_results1","0dc34224":"# plotting CV results\nplt.figure(figsize=(16,4))\n\nplt.plot(cv_results1[\"param_alpha\"], cv_results1[\"mean_test_score\"])\nplt.plot(cv_results1[\"param_alpha\"], cv_results1[\"mean_train_score\"])\nplt.xlabel('number of features')\nplt.ylabel('r-squared')\nplt.title(\"Optimal Number of Features\")\nplt.legend(['test score', 'train score'], loc='upper right')","7f2ff894":"#checking the value of optimum number of parameters\nprint(model_cv.best_estimator_)\nprint(model_cv.best_params_)\nprint(model_cv.best_score_)","ac94126b":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.figure(figsize=(16,5))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","947c4f52":"alpha = 0.0001\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train, y_train) ","db25342c":"#predicting the R-squared value of test and train data\ny_train_pred = lasso.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))","e69fd03f":"alpha = 0.0001\n\nlasso = Lasso(alpha=alpha)\n\nlasso.fit(X_train,y_train)\npreds = lasso.predict(test_data)\nfinal_predictions_lasso =np.exp(preds)","9cf93059":"lasso.coef_","91c8532a":"coeffs1 = np.squeeze (np.asarray(lasso.coef_))\nprint(coeffs1)","00d26992":"lasso_features = pd.Series(coeffs1, index = X_train.columns)\nlasso_features.abs().sort_values(ascending=False)","66d27158":"lasso_values = pd.DataFrame({'Id': test_data.index ,#'SalePrice_actual': np.exp(y_test), \n                             'SalePrice_predicted':final_predictions_lasso })\nlasso_values.head(8)","128acb71":"sub = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/sample_submission.csv')","fca68b12":"sub.head()","965595e3":"sub['SalePrice'] = np.exp(preds)\nsub.head()","613e1ae5":"sub.to_csv('submission.csv')","6835bf9d":"### Ridge Regression","8748d054":"This is quite hard to read, and we can rather plot correlations between variables. Also, a heatmap is pretty useful to visualise multiple correlations in one plot.","64d62262":"#### Creating dummy variables","9318fabf":"### 1. Data Understanding and Exploration\n\nLet's first import the required libraries and have a look at the dataset and understand the size, attribute names etc.","905a02a2":"We will create a new feature `\"Age Of House\"` to check how old the particular house is.","fedc26fe":"## 4. Model Building and Evaluation\n\nLet's start building the model. The first step to model building is the usual test-train split. So let's perform that","9154bd97":"### Dummy variable creation\n\nThe next step is to deal with the categorical variables present in the dataset. So first take a look at which variables are actually categorical variables.","ba33a118":"Let's now make a pairwise scatter plot and observe linear relationships.","ffc75f2c":"## 3. Data Preparation \n\n\n#### Data Preparation\n\nLet's now prepare the data and build the model.\n","fc6a8981":"Sales price is skewed to a side. Hence taking log of sales price to normally distribute the data.","d8ef16db":"from the above graph and the bestparam score we got optimum lambda to be 10","5ff313bb":"Now, clearly the variables `ID` won't be of any use in the analysis, so it's best that we drop this variable.","856f4486":"## 2. Data Cleaning\n\nLet's now conduct some data cleaning steps. \n\n### Missing Values","a81781eb":"### Scaling\n\nLets scale the features.","979e386a":"## Lasso","a82f65c0":"`\"Sales Price\"` is **normally distributed** over the data.","52aaab1f":"#### Data Exploration\nWe'll first subset the list of all (independent) numeric variables, and then make a **pairwise plot**.","4455e95c":"Here we can see highly skewed features having only one values (Above 90%). Hence dropping the columns.\n\nTaking into consideration that `\"Utilities\"` are main features for a House, retaining the feature.","5d5ca185":"from the above graph and the bestparam score,Lambda = 0.0001 is optimal value.","97ec87c3":"All of the columns do not need to be scaled.Hence scaling particular columns.","0295e20b":"As you can see that more than 90% values in `\"MiscFeature\"` is missing. Hence dropping the column.","b568fa05":"Some of the features are ordinal type. Converting these features from \"Int\" to \"Object\" so that they can be treated as categorical variables."}}