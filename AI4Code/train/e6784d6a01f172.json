{"cell_type":{"002aa86c":"code","dca49ed0":"code","2ffc03af":"code","f974fc11":"code","4523ef6c":"code","782e3584":"code","65555eed":"code","8e157b22":"code","0656b171":"code","75816357":"code","ab339cca":"code","29439bd2":"code","7ab21047":"code","c8f8f1aa":"code","1e5b15c4":"code","106de6ae":"code","c4b66fda":"code","f7ffd84b":"code","b6dc8684":"code","c64df7bd":"code","67e9d4b5":"code","360d9c92":"code","fbd4f3b3":"code","90a4e934":"code","456574da":"code","44a22505":"code","53fa2f31":"code","f4d241fd":"code","5a56f874":"code","d9afce5b":"code","b3cb9f3f":"code","43749e47":"code","e8560ec3":"code","555da51f":"code","6be235bf":"code","29b90ad7":"code","83a73e04":"code","46591277":"code","caa6b737":"code","bb07f402":"code","ede5e775":"code","1f1d174d":"code","f8ff309a":"code","7c73bb35":"code","26b257ad":"code","2100db7c":"code","8f9211f5":"code","678ec94d":"code","f2b6d292":"code","939ea731":"code","37a28dcb":"code","cc612a4f":"code","f5f013b6":"code","7adba3ef":"code","132875d5":"code","e766b9ab":"code","ffb05203":"code","119b8d67":"code","b08717fa":"code","731df9e0":"markdown","813b0560":"markdown","81a89b63":"markdown","0f2c7d5a":"markdown","4aaffeac":"markdown","0b9eeee8":"markdown","fbe8c961":"markdown","0b11aae2":"markdown","1e2750cb":"markdown","e2a46490":"markdown","bf7151cd":"markdown","b664f4f6":"markdown","e05004eb":"markdown","4fe32b44":"markdown","3246c3e4":"markdown","f8583776":"markdown","c78b3837":"markdown","90f09004":"markdown","ec77767f":"markdown","3a9f8d70":"markdown","7c0fe426":"markdown","ea0d1de7":"markdown","2eb09ed1":"markdown","e1249a08":"markdown","3e0e4356":"markdown","0499d3aa":"markdown","d0635a99":"markdown","80173777":"markdown","db763e2d":"markdown","e93296ee":"markdown","2469b881":"markdown","2303d075":"markdown","37af71c2":"markdown","ee432b18":"markdown","dc3eec97":"markdown","a9424a11":"markdown","d490d327":"markdown","df46cb31":"markdown","cef32ae5":"markdown","ef4e22ee":"markdown","adde5d7b":"markdown","b90872bc":"markdown","a95150af":"markdown"},"source":{"002aa86c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport warnings\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom scipy.stats.stats import pearsonr\nfrom scipy.stats import norm\nfrom collections import Counter\nfrom sklearn.linear_model import LinearRegression,LassoCV, Ridge, LassoLarsCV,ElasticNetCV\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\nwarnings.filterwarnings('ignore')\nsns.set(style='white', context='notebook', palette='deep')\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline","dca49ed0":"# Load train and Test set\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","2ffc03af":"# Check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n# Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the 'Id' column since it's unnecessary for the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n# Check data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","f974fc11":"train.head()","4523ef6c":"test.head()","782e3584":"# Getting Description\ntrain['SalePrice'].describe()","65555eed":"# Plot Histogram\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","8e157b22":"# Checking Categorical Data\ntrain.select_dtypes(include=['object']).columns","0656b171":"# Checking Numerical Data\ntrain.select_dtypes(include=['int64','float64']).columns","75816357":"cat = len(train.select_dtypes(include=['object']).columns)\nnum = len(train.select_dtypes(include=['int64','float64']).columns)\nprint('Total Features: ', cat, 'categorical', '+',\n      num, 'numerical', '=', cat+num, 'features')","ab339cca":"# Correlation Matrix Heatmap\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","29439bd2":"# Top 10 Heatmap\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","7ab21047":"most_corr = pd.DataFrame(cols)\nmost_corr.columns = ['Most Correlated Features']\nmost_corr","c8f8f1aa":"# Overall Quality vs Sale Price\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","1e5b15c4":"# Living Area vs Sale Price\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'], kind='reg')","106de6ae":"# Removing outliers manually (Two points in the bottom right)\ntrain = train.drop(train[(train['GrLivArea']>4000) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","c4b66fda":"# Living Area vs Sale Price\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'], kind='reg')","f7ffd84b":"# Garage Area vs Sale Price\nsns.boxplot(x=train['GarageCars'], y=train['SalePrice'])","b6dc8684":"# Removing outliers manually (More than 4-cars, less than $300k)\ntrain = train.drop(train[(train['GarageCars']>3) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","c64df7bd":"# Garage Area vs Sale Price\nsns.boxplot(x=train['GarageCars'], y=train['SalePrice'])","67e9d4b5":"# Garage Area vs Sale Price\nsns.jointplot(x=train['GarageArea'], y=train['SalePrice'], kind='reg')","360d9c92":"# Removing outliers manually (More than 1000 sqft, less than $300k)\ntrain = train.drop(train[(train['GarageArea']>1000) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","fbd4f3b3":"# Garage Area vs Sale Price\nsns.jointplot(x=train['GarageArea'], y=train['SalePrice'], kind='reg')","90a4e934":"# Basement Area vs Sale Price\nsns.jointplot(x=train['TotalBsmtSF'], y=train['SalePrice'], kind='reg')","456574da":"# First Floor Area vs Sale Price\nsns.jointplot(x=train['1stFlrSF'], y=train['SalePrice'], kind='reg')","44a22505":"# Total Rooms vs Sale Price\nsns.boxplot(x=train['TotRmsAbvGrd'], y=train['SalePrice'])","53fa2f31":"# Total Rooms vs Sale Price\nvar = 'YearBuilt'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","f4d241fd":"# Combining Datasets\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"Train data size is : {}\".format(train.shape))\nprint(\"Test data size is : {}\".format(test.shape))\nprint(\"Combined dataset size is : {}\".format(all_data.shape))","5a56f874":"# Find Missing Ratio of Dataset\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","d9afce5b":"# Percent missing data by feature\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","b3cb9f3f":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","43749e47":"# Check if there are any missing values left\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","e8560ec3":"all_data['MSSubClass'].describe()","555da51f":"#MSSubClass =The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","6be235bf":"all_data['KitchenQual'].unique()","29b90ad7":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# Process columns and apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# Check shape        \nprint('Shape all_data: {}'.format(all_data.shape))","83a73e04":"# Adding Total Square Feet feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","46591277":"# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n\ny_train = train.SalePrice.values\n\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","caa6b737":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skewed Features' :skewed_feats})\nskewness.head()","bb07f402":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    all_data[feat] += 1","ede5e775":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","1f1d174d":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","f8ff309a":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","7c73bb35":"# Cross-validation with k-folds\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","26b257ad":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.2, gamma=0.0, \n                             learning_rate=0.05, max_depth=6, \n                             min_child_weight=1.5, n_estimators=7200,\n                             reg_alpha=0.9, reg_lambda=0.6,\n                             subsample=0.2,seed=42, silent=1,\n                             random_state =7)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","2100db7c":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","8f9211f5":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","678ec94d":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\"Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f2b6d292":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, clf in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(clf)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","939ea731":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","37a28dcb":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","cc612a4f":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","f5f013b6":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","7adba3ef":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","132875d5":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.10 + lgb_train_pred*0.20 ))","e766b9ab":"# Example\nStacked = 1\/(0.1077)\nXGBoost = 1\/(0.1177)\nLGBM = 1\/(0.1159)\nSum = Stacked + XGBoost + LGBM\nStacked = Stacked\/Sum\nXGBoost = XGBoost\/Sum\nLGBM = LGBM\/Sum\nprint(Stacked, XGBoost, LGBM)","ffb05203":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*Stacked +\n               xgb_train_pred*XGBoost + lgb_train_pred*LGBM))","119b8d67":"ensemble = stacked_pred*Stacked + xgb_pred*XGBoost + lgb_pred*LGBM","b08717fa":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","731df9e0":"For our models, we are going to use lasso, elastic net, kernel ridge, gradient boosting, XGBoost, and LightGBM regression.","813b0560":"Again with the bottom two data-points. Let's remove those outliers.","81a89b63":"1. Let's engineer one feature to combine square footage, this may be useful later on.","0f2c7d5a":"## 2. Analyzing the Test Variable (Sale Price)\n\nLet's check out the most interesting feature in this study: Sale Price. Important Note: This data is from Ames, Iowa. The location is extremely correlated with Sale Price. (I had to take a double-take at a point, since I consider myself a house-browsing enthusiast)","4aaffeac":"## Introduction\n\nThis notebook is going to be focused on solving the problem of predicting house prices for house buyers and house sellers.\n\nA house value is simply more than location and square footage. Like the features that make up a person, an educated party would want to know all aspects that give a house its value. \n\nWe are going to take advantage of all of the feature variables available to use and use it to analyze and predict house prices.\n\nWe are going to break everything into logical steps that allow us to ensure the cleanest, most realistic data for our model to make accurate predictions from.\n\n1. Load Data and Packages\n2. Analyzing the Test Variable (Sale Price)\n3. Multivariable Analysis\n4. Impute Missing Data and Clean Data\n5. Feature Transformation\/Engineering\n6. Modeling and Predictions\n","0b9eeee8":"With 81 features, how could we possibly tell which feature is most related to house prices? Good thing we have a correlation matrix. Let's do it!","fbe8c961":"So, the average is a 57 type. What does that mean? Is a 90 type 3 times better than a 30 type? This feature was interpreted as numerical when it is actually categorical. The types listed here are codes, not values. Thus, we need to feature transformation with this and many other features.","0b11aae2":"### Imputing Missing Values\n* PoolQC : data description says NA means \"No Pool\"\n* MiscFeature : data description says NA means \"no misc feature\"\n* Alley : data description says NA means \"no alley access\"\n* Fence : data description says NA means \"no fence\"\n* FireplaceQu : data description says NA means \"no fireplace\"\n* LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n* GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with \"None\".\n* GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0.\n* BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath: Replacing missing data with 0.\n* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there isn't a basement.\n* MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\n* MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'.\n* Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.\n* Functional : data description says NA means typical.\n* Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n* KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\n* Exterior1st and Exterior2nd : Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n* SaleType : Fill in again with most frequent which is \"WD\"\n* MSSubClass : Na most likely means No building class. We can replace missing values with None\n\n","1e2750cb":"4-car garages result in less Sale Price? That doesn't make much sense. Let's remove those outliers.","e2a46490":"### LightGBM","bf7151cd":"Looks good.","b664f4f6":"Since our lasso model performed the best, we'll use it as a meta-model.","e05004eb":"### Ensemble Prediction\nNote: To get our weights for each model, we'll take the inverse of each regressor and average it out of 100%","4fe32b44":"Everything looks fine here.","3246c3e4":"### Stacked models","f8583776":"Here, we stack the models to average their scores.","c78b3837":"In our previous example, we could tell that our categories don't follow a particular order. What about categories that do? Let's take a look at \"Kitchen Quality\".","90f09004":"Nice! We got a 0.02 point increase in the Pearson-R Score.","ec77767f":"Checking performance of base models by evaluating the cross-validation RMSLE error.","3a9f8d70":"What! People pay more for better quality? Nothing new here. Let's move on.","7c0fe426":"It makes sense that people would pay for the more living area. What doesn't make sense is the two datapoints in the bottom-right of the plot. \n\n We need to take care of this! What we will do is remove these outliers manually. ","ea0d1de7":"It seems like houses with more than 11 rooms come with a $100k off coupon. It looks like an outlier but I'll let it slide.","2eb09ed1":"## 3. Multivariable Analysis\n\nLet's check out all the variables! There are two types of features in housing data, categorical and numerical. \n\nCategorical data is just like it sounds. It is in categories. It isn't necessarily linear, but it follows some kind of pattern. For example, take a feature of \"Downtown\". The response is either \"Near\", \"Far\", \"Yes\", and \"No\".  Back then, living in downtown usually meant that you couldn't afford to live in uptown. Thus, it could be implied that downtown establishments cost less to live in. However, today, that is not the case. (Thank you, hipsters!) So we can't really establish any particular order of response to be \"better\" or \"worse\" than the other.\n\nNumerical data is data in number form. (Who could have thought!) These features are in a linear relationship with each other. For example, a 2,000 square foot place is 2 times \"bigger\" than a 1,000 square foot place. Plain and simple. Simple and clean.","e1249a08":"Here we average ENet, GBoost, KRR, and lasso. We'll add in XGBoost and LightGBM later.","3e0e4356":"## 5. Feature Transformation\/Engineering\n\nLet's take a look at some features that may be misinterpreted to represent something it's not.\n\nMSSubClass: Identifies the type of dwelling involved in the sale.\t\n*         20\t1-STORY 1946 & NEWER ALL STYLES\n*         30\t1-STORY 1945 & OLDER\n*         40\t1-STORY W\/FINISHED ATTIC ALL AGES\n*         45\t1-1\/2 STORY - UNFINISHED ALL AGES\n*         50\t1-1\/2 STORY FINISHED ALL AGES\n*         60\t2-STORY 1946 & NEWER\n*         70\t2-STORY 1945 & OLDER\n*         75\t2-1\/2 STORY ALL AGES\n*         80\tSPLIT OR MULTI-LEVEL\n*         85\tSPLIT FOYER\n*         90\tDUPLEX - ALL STYLES AND AGES\n*        120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n*        150\t1-1\/2 STORY PUD - ALL AGES\n*        160\t2-STORY PUD - 1946 & NEWER\n*        180\tPUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n*        190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n","0499d3aa":"### Submission","d0635a99":"From looking at the head of both sets, we can see that the only difference in features is \"Sale Price\". This makes sense because we are trying to predict it!","80173777":"# Predicting House Prices with Machine Learning\n**Eric Kim, B.A.** - **August 2017**","db763e2d":"## 0. Understanding the Client and their Problem\n\nA benefit to this study is that we can have two clients at the same time! (Think of being a divorce lawyer for both interested parties) However, in this case, we can have both clients with no conflict of interest!\n\nClient Housebuyer: This client wants to find their next dream home with a reasonable price tag. They have their locations of interest ready. Now, they want to know if the house price matches the house value. With this study, they can understand which features (ex. Number of bathrooms, location, etc.) influence the final price of the house. If all matches, they can ensure that they are getting a fair price.\n\nClient Houseseller: Think of the average house-flipper. This client wants to take advantage of the features that influence a house price the most. They typically want to buy a house at a low price and invest on the features that will give the highest return. For example, buying a house at a good location but small square footage. The client will invest on making rooms at a small cost to get a large return. ","e93296ee":"With an average house price of $180921, it seems like I should relocated to Iowa!","2469b881":"### XGBoost","2303d075":"Although it seems like house prices decrease with age, we can't be entirely sure. Is it because of inflation or stock market crashes? Let's leave the years alone.","37af71c2":"That looks much better. Note: removal of data is totally discretionary and may or may not help in modeling. Use at your own preference.","ee432b18":"It's a nice overview, but oh man is that a lot of data to look at. Let's zoom into the top 10 features most related to Sale Price.","dc3eec97":"Looks like a normal distribution? Not quite! Looking at the kurtosis score, we can see that there is a very nice peak. However, looking at the skewness score, we can see that the sale prices deviate from the normal distribution. Going to have to fix this later! We want our data to be as \"normal\" as possible.","a9424a11":"Here, data_description.txt comes to the rescue again!\n\nKitchen Quality:\n*        Ex:\tExcellent\n*        Gd:\tGood\n*        TA:\tTypical\/Average\n*        Fa:\tFair\n*        Po:\tPoor\n\nIs a score of \"Gd\" better than \"TA\" but worse than \"Ex\"? I think so, let's encode these labels to give meaning to their specific orders.","d490d327":"Only 0.01 point Pearson-R Score increase, but looks much better!","df46cb31":"## 6. Modeling and Predictions","cef32ae5":"Well, the most correlated feature to Sale Price is... Sale Price?!? Of course. For the other 9, they are as listed. Here is a short description of each. (Thank you, data_description.txt!)\n\n1. OverallQual: Rates the overall material and finish of the house (1 = Very Poor, 10 = Very Excellent)\n2. GrLivArea: Above grade (ground) living area square feet\n3. GarageCars: Size of garage in car capacity\n4. GarageArea: Size of garage in square feet\n5. TotalBsmtSF: Total square feet of basement area\n6. 1stFlrSF: First Floor square feet\n7. FullBath: Full bathrooms above grade\n8. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n9. YearBuilt: Original construction date\n\nLet's take a look at how each relates to Sale Price and do some pre-cleaning on each feature if necessary.","ef4e22ee":"So the training set has 1460 rows and 81 features. The test set has 1459 rows and 80 features.","adde5d7b":"## 1. Loading Data and Packages","b90872bc":"## 4. Impute Missing Data and Clean Data\nImportant questions when thinking about missing data:\n* How prevalent is the missing data?\n* Is missing data random or does it have a pattern?\n\nThe answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hiding an inconvenient truth.\n\nLet's combine both training and test data into one dataset to impute missing values and do some cleaning.","a95150af":"### Fixing \"skewed\" features.\nHere, we fix all of the skewed data to be more normal so that our models will be more accurate when making predictions."}}