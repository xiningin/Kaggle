{"cell_type":{"844e3280":"code","7918151a":"code","3882e6be":"code","9e01ed2b":"code","df393d74":"code","01198bcf":"code","3b544745":"code","e9fabe3b":"code","861cf23e":"code","e0fd0ce4":"code","cea3f9b5":"code","704f853d":"code","e496e9fd":"code","dbd7b95b":"code","3dae9e05":"code","3a5352b5":"code","60f06d7f":"code","9cb89e82":"code","42319d47":"code","95c65c7f":"code","94eac6e7":"code","7645a77f":"code","7c216b28":"code","a1be736a":"code","f47a6773":"code","d31a3f26":"code","7f666d5b":"code","c1a64ee6":"code","20460659":"code","66b5d4ca":"code","4a9ed1a7":"code","70fec3ef":"code","7065dd71":"code","d5b53be7":"code","c8a9ef23":"code","236f670d":"code","63268e7a":"code","6df13b7e":"code","1e8b9061":"code","963e4d21":"code","e6db4893":"code","bb7e8864":"code","2124c615":"code","38be7f2d":"code","5cc64178":"code","f20c25ef":"code","48b49e1e":"code","4cef3d26":"code","2d96a3b7":"code","487f1457":"code","87e7acd8":"code","3f8026c8":"code","b3f4dd22":"code","c8e2a00c":"code","81c16f3b":"code","e0fd373d":"code","ba9af4e0":"code","e6af175b":"code","ffb34f77":"code","ab928b7d":"code","cedd7352":"code","4e83e207":"code","72760071":"code","c7a019c8":"code","36271585":"code","99126ce4":"code","5bc6fcfb":"code","e4e8327b":"markdown","059a856e":"markdown","3439e5b2":"markdown","d7e16b06":"markdown","df8904a1":"markdown","7cb42b3e":"markdown","0344a614":"markdown","2b4a6ebe":"markdown","36b81ca6":"markdown","5c9a5c50":"markdown","d40617d1":"markdown","18684196":"markdown","069fee25":"markdown","9dcef33d":"markdown","136d6959":"markdown","050791ad":"markdown","4e629f37":"markdown","32a1eb3c":"markdown","f13eaa7a":"markdown","2f146021":"markdown","46a5031f":"markdown","520cbf24":"markdown"},"source":{"844e3280":"%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cufflinks as cf\ncf.go_offline()\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","7918151a":"print(os.listdir(\"..\/input\"))","3882e6be":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","9e01ed2b":"start_time = time.time()\ntrain = pd.read_csv('..\/input\/application_train.csv')\ntest = pd.read_csv('..\/input\/application_test.csv')\n\nPOS_CASH_balance = pd.read_csv('..\/input\/POS_CASH_balance.csv')\nbureau_balance = pd.read_csv('..\/input\/bureau_balance.csv')\nprevious_application = pd.read_csv('..\/input\/previous_application.csv')\ninstallments_payments = pd.read_csv('..\/input\/installments_payments.csv')\ncredit_card_balance = pd.read_csv('..\/input\/credit_card_balance.csv')\nbureau = pd.read_csv('..\/input\/bureau.csv')\n\nprint('all data loaded in {:4f} sec'.format(time.time()-start_time))","df393d74":"def replace_to_nan(df):\n    df = df[df['CODE_GENDER'] != 'XNA']\n    df.replace(to_replace={'XNA': np.nan, 'XAP': np.nan}, value=None, inplace=True)\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n    return df","01198bcf":"train = replace_to_nan(train)\ntest = replace_to_nan(test)","3b544745":"print('train data shape ', train.shape)\nprint('test data shape ', test.shape)","e9fabe3b":"print('column type')\ntrain.dtypes.value_counts()","861cf23e":"TARGET = 'TARGET' \nID = 'SK_ID_CURR'\ntrain[TARGET].hist()\ntrain[TARGET].value_counts()","e0fd0ce4":"for col in train.select_dtypes(['object', 'category']).columns:\n    min_op = set(train[col]) - set(test[col])\n    if len(min_op) > 0:\n        print('{} have some row that not in test table, will treat'.format(col), min_op,'as nan')\n        for not_in_test in min_op:\n            train[col].replace(not_in_test, np.nan, inplace=True)\n    min_op = set(test[col]) - set(train[col])\n    if len(min_op) > 0:\n        print('{} have some row that not in train table, will treat'.format(col), min_op,'as nan')\n        for not_in_train in min_op:\n            test[col].replace(not_in_train, np.nan, inplace=True)","cea3f9b5":"# this function is used to get the dataframe containing column with missing value with its total and percentage\ndef missing_col(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\n    missing_col  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_col[missing_col['Total'] > 0]","704f853d":"missing_col(train)","e496e9fd":"missing_col(POS_CASH_balance)","dbd7b95b":"missing_col(bureau_balance)","3dae9e05":"missing_col(previous_application)","3a5352b5":"missing_col(installments_payments)","60f06d7f":"missing_col(credit_card_balance)","9cb89e82":"missing_col(bureau)","42319d47":"# plot distribution target=1 vs target=0\ndef plot_dist(col, train=train, target=TARGET):\n    plt.figure(figsize=(12,8))\n    target_0 = train[train[target]==0].dropna()\n    target_1 = train[train[target]==1].dropna()\n    sns.distplot(target_0[col].values, label='target: 0')\n    sns.distplot(target_1[col].values, color='red', label='target: 1')\n    plt.xlabel(col)\n    plt.legend()\n    plt.show()\n\n# plot value vs its index and TARGET\ndef plot_val_vs_idx(col, train=train, target=TARGET):\n    plt.figure(figsize=(12,8))\n    plt.scatter(range(train.shape[0]), train[col], c=train[target], cmap='viridis')\n    plt.ylabel(col)\n    plt.xlabel('index')\n    plt.colorbar()\n    plt.show()\n\ndef pie_i_plot(col, train=train, hole=0.5, title=None):\n    if title is None:\n        title = col\n    temp = train[col].value_counts()\n    df = pd.DataFrame({'labels': temp.index, 'values': temp.values})\n    df.iplot(kind='pie',labels='labels',values='values', title=title, hole = 0.5)","95c65c7f":"train.drop([ID, TARGET], axis=1).describe()","94eac6e7":"print('see their age')\n(train['DAYS_BIRTH'] \/-365).describe()","7645a77f":"train[train['DAYS_EMPLOYED'] < train['DAYS_BIRTH']]","7c216b28":"(train['DAYS_REGISTRATION'] \/-365).describe()","a1be736a":"print('check if there are some mistake on data', train[train['DAYS_EMPLOYED'] < train['DAYS_BIRTH']].shape[0])","f47a6773":"# create a new features (ESTIMATED AGE)\ntrain['EST_AGE'] = (train['DAYS_BIRTH'] \/ -365)","d31a3f26":"plot_dist('EST_AGE')","7f666d5b":"correlations = train.drop(ID, axis=1).corr()","c1a64ee6":"plt.figure(figsize=(20,20))\nsns.heatmap(correlations)","20460659":"correlations = correlations.sort_values(by=TARGET)\ncorrelations.head(10)['TARGET']","66b5d4ca":"correlations.tail(10)['TARGET']","4a9ed1a7":"del(correlations)\ngc.collect()","70fec3ef":"plt.figure(figsize=(5,5))\nsns.heatmap(train[[TARGET,'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].corr(), annot = True)","7065dd71":"for ext_col in ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']:\n    plot_dist(ext_col)","d5b53be7":"train = train.drop('EST_AGE', axis=1)","c8a9ef23":"# this plot is used to get probability of a categorical column that loan is not repaid (target=1)\ndef create_plot_prob(df, column, rotate=False, limit=None):\n    display_name = column.replace('_', ' ').title()\n    if rotate:\n        rotation = 45\n    else:\n        rotation = 0\n    if limit is not None:\n        selected = list(df[col].value_counts().head(limit).index)\n        df = df[df[col].isin(selected)]\n    grouped_data = df[[column, TARGET]].groupby(column).agg(['mean', 'count'])\n    grouped_data.reset_index(inplace=True)\n    grouped_data.columns = [column, 'mean', 'sum']\n    grouped_data = grouped_data.sort_values(by='mean')\n    sns.set(style=\"whitegrid\")\n\n    fig = plt.figure(figsize=(20, 10))\n#     plt.tight_layout()\n    ax1 = fig.add_subplot(111)\n    ax1.set_title('Occurance of Loan not repaid per {}'.format(display_name))\n    label = grouped_data[column]\n    g = sns.barplot(x=column, y='sum', data=grouped_data, order=label, ax=ax1)\n    g.set_xticklabels(labels=label, rotation=rotation)\n    g.set(xlabel=display_name, ylabel='Number of Case')\n    ax2 = ax1.twinx()\n    h = sns.pointplot(x=column, y=\"mean\", data=grouped_data, ax=ax2)\n    h.set(xlabel='index', ylabel='Loan not repaid Prior Probability')\n    ax2.grid(False)\n    plt.show()","236f670d":"for col in train.select_dtypes('object').columns:\n    create_plot_prob(train, col, limit=10)","63268e7a":"for col in train.select_dtypes(['object', 'category']).columns:\n    pie_i_plot(col, title=col)","6df13b7e":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\n\nPOS_CASH_balance = reduce_mem_usage(POS_CASH_balance)\nbureau_balance = reduce_mem_usage(bureau_balance)\ndel previous_application\ngc.collect()\n# previous_application = reduce_mem_usage(previous_application)\ninstallments_payments = reduce_mem_usage(installments_payments)\ncredit_card_balance = reduce_mem_usage(credit_card_balance)\nbureau = reduce_mem_usage(bureau)","1e8b9061":"def one_hot_encoder(df, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if str(df[col].dtype) in ['object', 'category']]\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns","963e4d21":"df = train.append(test)\ndf.head(1)","e6db4893":"for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n    df[bin_feature], uniques = pd.factorize(df[bin_feature])\ndf, cat_cols = one_hot_encoder(df, False)\n\ndf['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\ndf['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\ndf['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\ndf['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\ndf['PAYMENT_RATE'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\ndel test\ndel train\ngc.collect()","bb7e8864":"grp = credit_card_balance.groupby(ID)['SK_ID_PREV'].nunique().reset_index().rename(index=str, columns={'SK_ID_PREV':'NO_LOANS'})\ncredit_card_balance = credit_card_balance.merge(grp, on=ID, how='left')\ndel(grp)\ngc.collect()","2124c615":"# No of Installments paid per Loan per Customer \ngrp = credit_card_balance.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV'])['CNT_INSTALMENT_MATURE_CUM'].max().reset_index().rename(index = str, columns = {'CNT_INSTALMENT_MATURE_CUM': 'NO_INSTALMENTS'})\ngrp1 = grp.groupby(by = ['SK_ID_CURR'])['NO_INSTALMENTS'].sum().reset_index().rename(index = str, columns = {'NO_INSTALMENTS': 'TOTAL_INSTALMENTS'})\ncredit_card_balance = credit_card_balance.merge(grp1, on = ['SK_ID_CURR'], how = 'left')\ndel grp, grp1\ngc.collect()","38be7f2d":"#AVERAGE NUMBER OF TIMES DAYS PAST DUE HAS OCCURRED PER CUSTOMER\ndef f(DPD):\n    \n    # DPD is a series of values of SK_DPD for each of the groupby combination \n    # We convert it to a list to get the number of SK_DPD values NOT EQUALS ZERO\n    x = DPD.tolist()\n    c = 0\n    for i,j in enumerate(x):\n        if j != 0:\n            c += 1\n    \n    return c \n\ngrp = credit_card_balance.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV']).apply(lambda x: f(x.SK_DPD)).reset_index().rename(index = str, columns = {0: 'NO_DPD'})\ngrp1 = grp.groupby(by = ['SK_ID_CURR'])['NO_DPD'].mean().reset_index().rename(index = str, columns = {'NO_DPD' : 'DPD_COUNT'})\n\ncredit_card_balance = credit_card_balance.merge(grp1, on = ['SK_ID_CURR'], how = 'left')\ndel grp1\ndel grp \ngc.collect()","5cc64178":"#% of MINIMUM PAYMENTS MISSED\ndef f(min_pay, total_pay):\n    \n    M = min_pay.tolist()\n    T = total_pay.tolist()\n    P = len(M)\n    c = 0 \n    # Find the count of transactions when Payment made is less than Minimum Payment \n    for i in range(len(M)):\n        if T[i] < M[i]:\n            c += 1  \n    return (100*c)\/P\n\ngrp = credit_card_balance.groupby(by = ['SK_ID_CURR']).apply(lambda x: f(x.AMT_INST_MIN_REGULARITY, x.AMT_PAYMENT_CURRENT)).reset_index().rename(index = str, columns = { 0 : 'PERCENTAGE_MISSED_PAYMENTS'})\ncredit_card_balance = credit_card_balance.merge(grp, on = ['SK_ID_CURR'], how = 'left')\ndel grp \ngc.collect()","f20c25ef":"grp = credit_card_balance.groupby(by = ['SK_ID_CURR'])['AMT_DRAWINGS_ATM_CURRENT'].sum().reset_index().rename(index = str, columns = {'AMT_DRAWINGS_ATM_CURRENT' : 'DRAWINGS_ATM'})\ncredit_card_balance = credit_card_balance.merge(grp, on = ['SK_ID_CURR'], how = 'left')\ndel grp\ngc.collect()\n\ngrp = credit_card_balance.groupby(by = ['SK_ID_CURR'])['AMT_DRAWINGS_CURRENT'].sum().reset_index().rename(index = str, columns = {'AMT_DRAWINGS_CURRENT' : 'DRAWINGS_TOTAL'})\ncredit_card_balance = credit_card_balance.merge(grp, on = ['SK_ID_CURR'], how = 'left')\ndel grp\ngc.collect()\n\ncredit_card_balance['CASH_CARD_RATIO1'] = (credit_card_balance['DRAWINGS_ATM']\/credit_card_balance['DRAWINGS_TOTAL'])*100\ndel credit_card_balance['DRAWINGS_ATM']\ndel credit_card_balance['DRAWINGS_TOTAL']\ngc.collect()\n\ngrp = credit_card_balance.groupby(by = ['SK_ID_CURR'])['CASH_CARD_RATIO1'].mean().reset_index().rename(index = str, columns ={ 'CASH_CARD_RATIO1' : 'CASH_CARD_RATIO'})\ncredit_card_balance = credit_card_balance.merge(grp, on = ['SK_ID_CURR'], how = 'left')\ndel grp \ngc.collect()\n\ndel credit_card_balance['CASH_CARD_RATIO1']\ngc.collect()","48b49e1e":"credit_card_balance.shape","4cef3d26":"df.shape","2d96a3b7":"credit_card_balance, cat_cols = one_hot_encoder(credit_card_balance, nan_as_category=False)","487f1457":"credit_card_balance.drop(['SK_ID_PREV'], axis= 1, inplace = True)","87e7acd8":"df = df.merge(credit_card_balance, on=ID, how='left')\ndel(credit_card_balance)\ngc.collect()\ndf.shape","3f8026c8":"# NUMBER OF PAST LOANS PER CUSTOMER\ngrp = bureau[['SK_ID_CURR', 'DAYS_CREDIT']].groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT'].count().reset_index().rename(index=str, columns={'DAYS_CREDIT': 'BUREAU_LOAN_COUNT'})\nbureau = bureau.merge(grp, on = ['SK_ID_CURR'], how = 'left')","b3f4dd22":"# NUMBER OF TYPES OF PAST LOANS PER CUSTOMER\ngrp = bureau[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by = ['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(index=str, columns={'CREDIT_TYPE': 'BUREAU_LOAN_TYPES'})\nbureau = bureau.merge(grp, on = ['SK_ID_CURR'], how = 'left')","c8e2a00c":"# Is the Customer diversified in taking multiple types of Loan or Focused on a single type of loan\nbureau['AVERAGE_LOAN_TYPE'] = bureau['BUREAU_LOAN_COUNT']\/bureau['BUREAU_LOAN_TYPES']\ndel bureau['BUREAU_LOAN_COUNT'], bureau['BUREAU_LOAN_TYPES'], grp\ngc.collect()","81c16f3b":"#% OF ACTIVE LOANS FROM BUREAU DATA\nbureau['CREDIT_ACTIVE_BINARY'] = bureau['CREDIT_ACTIVE']\n\ndef f(x):\n    if x == 'Closed':\n        y = 0\n    else:\n        y = 1    \n    return y\n\nbureau['CREDIT_ACTIVE_BINARY'] = bureau.apply(lambda x: f(x.CREDIT_ACTIVE), axis = 1)\n\n# Calculate mean number of loans that are ACTIVE per CUSTOMER \ngrp = bureau.groupby(by = ['SK_ID_CURR'])['CREDIT_ACTIVE_BINARY'].mean().reset_index().rename(index=str, columns={'CREDIT_ACTIVE_BINARY': 'ACTIVE_LOANS_PERCENTAGE'})\nbureau = bureau.merge(grp, on = ['SK_ID_CURR'], how = 'left')\ndel bureau['CREDIT_ACTIVE_BINARY'], grp\ngc.collect()","e0fd373d":"bureau_balance, bureau_balance_cat = one_hot_encoder(bureau_balance, True)\nbureau, bureau_cat = one_hot_encoder(bureau, True)","ba9af4e0":"bureau_balance_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\nfor col in bureau_balance_cat:\n    bureau_balance_aggregations[col] = ['mean']\nbureau_balance_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(bureau_balance_aggregations)\nbureau_balance_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bureau_balance_agg.columns.tolist()])\nbureau = bureau.join(bureau_balance_agg, how='left', on='SK_ID_BUREAU')\nbureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\ndel bureau_balance, bureau_balance_agg\ngc.collect()","e6af175b":"bureau.shape","ffb34f77":"num_aggregations = {\n    'DAYS_CREDIT': ['mean'],\n    'DAYS_CREDIT_ENDDATE': ['mean'],\n    'DAYS_CREDIT_UPDATE': ['mean'],\n    'CREDIT_DAY_OVERDUE': ['mean'],\n    'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n    'AMT_CREDIT_SUM': ['mean',],\n    'AMT_CREDIT_SUM_DEBT': ['mean'],\n    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n    'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n    'AMT_ANNUITY': ['max', 'mean'],\n    'CNT_CREDIT_PROLONG': ['sum'],\n    'MONTHS_BALANCE_MIN': ['min'],\n    'MONTHS_BALANCE_MAX': ['max'],\n    'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n}\n# Bureau and bureau_balance categorical features\ncat_aggregations = {}\nfor cat in bureau_cat: cat_aggregations[cat] = ['mean']\nfor cat in bureau_balance_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n\nbureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\nbureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n# Bureau: Active credits - using only numerical aggregations\nactive = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\nactive_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\nactive_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\nbureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\ndel active, active_agg\ngc.collect()\n# Bureau: Closed credits - using only numerical aggregations\nclosed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\nclosed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\nclosed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\nbureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\ndel closed, closed_agg, bureau\ngc.collect()","ab928b7d":"bureau_agg.shape","cedd7352":"df.shape","4e83e207":"df = df.merge(bureau_agg, on=ID, how='left')\ndel(bureau_agg)\ngc.collect()\ndf.shape","72760071":"ins, cat_cols = one_hot_encoder(installments_payments, nan_as_category= True)\ndel installments_payments\ngc.collect()\n# Percentage and difference paid in each installment (amount paid and installment value)\nins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] \/ ins['AMT_INSTALMENT']\nins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n# Days past due and days before due (no negative values)\nins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\nins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\nins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\nins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n# Features: Perform aggregations\naggregations = {\n    'NUM_INSTALMENT_VERSION': ['nunique'],\n    'DPD': ['max', 'mean', 'sum'],\n    'DBD': ['max', 'mean', 'sum'],\n    'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n    'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n    'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n    'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n}\nfor cat in cat_cols:\n    aggregations[cat] = ['mean']\nins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\nins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n# Count installments accounts\nins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\ndel ins\ngc.collect()","c7a019c8":"df = df.merge(ins_agg, on=ID, how='left')\ndel(ins_agg)\ngc.collect()\ndf.shape","36271585":"pos, cat_cols = one_hot_encoder(POS_CASH_balance, nan_as_category= True)\ndel(POS_CASH_balance)\ngc.collect()\n# Features\naggregations = {\n    'MONTHS_BALANCE': ['max', 'mean', 'size'],\n    'SK_DPD': ['max', 'mean'],\n    'SK_DPD_DEF': ['max', 'mean']\n}\nfor cat in cat_cols:\n    aggregations[cat] = ['mean']\n\npos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\npos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n# Count pos cash accounts\npos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\ndel pos\ngc.collect()","99126ce4":"df = df.merge(pos_agg, on=ID, how='left')\ndel(pos_agg)\ngc.collect()\ndf.shape","5bc6fcfb":"df = reduce_mem_usage(df)","e4e8327b":"### Trainset correlation","059a856e":"Number of loans per customer","3439e5b2":"DAYS_EMPLOYED and DAYS_REGISTRATION also  looks pretty normal, nothing fishy for now","d7e16b06":"### BUREAU Data","df8904a1":"some column have missing value more than 50% of its data\n\nwe might need to handle this later (some option are imputation, drop the column\/row)","7cb42b3e":"## **NUMERICAL EDA**","0344a614":"as we can see above the target prediction is imbalanced\nwe have way more 0 (loan was repaid on time) than 1 (loan not repaid)","2b4a6ebe":"## Categorical features","36b81ca6":"target = 1 skews toward younger people","5c9a5c50":"Nan preprocess helper\n[Martin Kotek (Competition Host): \"Value 365243 denotes infinity in DAYS variables in the datasets, therefore you can consider them NA values. Also XNA\/XAP denote NA values.\"](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/discussion\/57247)\n","d40617d1":"EXT_SOURCE_3 seems interesting because for target 1 and 2 they skewed differently","18684196":"### categorical column distribution","069fee25":"so in this problem we have 307511 training data with 122 variables and  120 features (after we remove SK_ID_CURR and TARGET)","9dcef33d":"every \"DAYS\" feature are negative because they are recorded relative to the current loan application\n\nsee if there are any anomality in DAYS, as for others it might be hard to detect just by looking at its description above","136d6959":"apparently no mistake on DAYS_EMPLOYED vs DAYS_BIRTH","050791ad":"# Feature Engineering","4e629f37":"\nits interesting to see 3 EXTERNAL FEATURE have the lowest correlation\nlets dive a bit deeper on those features","32a1eb3c":"## credit card","f13eaa7a":"#### **check  train and or test for missing column**","2f146021":"### TARGET DISTRIBUTION","46a5031f":"DAYS_BIRTH looks pretty normal, nothing fishy","520cbf24":"# **EDA and Cleaning**"}}