{"cell_type":{"7e430d95":"code","95d40497":"code","5a48b6f3":"code","c5f26df0":"code","f351a0d4":"code","cbcfb6a2":"code","0f62f1a1":"code","ecd09140":"code","a2032047":"code","7ce54d3d":"code","342f57e9":"code","fb6bbe55":"code","d64151ed":"code","58d3f881":"code","0f14fc09":"code","386a19db":"code","4f9a5cf3":"code","0d963b12":"code","654364e3":"code","66ceba93":"code","6aa4ab7e":"code","958d7006":"code","6b50f753":"code","04ef7249":"code","b0e17731":"code","3cb286d5":"code","3aa92b71":"markdown","ca3c798f":"markdown","e8072b03":"markdown","35a1df71":"markdown","f0b4dabe":"markdown","81eb087a":"markdown","cf375060":"markdown","fc22bae3":"markdown","6de8c63d":"markdown","41c628af":"markdown","8c97269f":"markdown","8c1e7607":"markdown","fe6de6a6":"markdown","fd2fb4c0":"markdown","c57c851a":"markdown","6579ba31":"markdown","ff9fdd25":"markdown","82eadc34":"markdown","1596b58a":"markdown","229ef8b7":"markdown","f11d17a6":"markdown","3e4c7a56":"markdown","6a8de1e5":"markdown","72b203fc":"markdown","be839958":"markdown"},"source":{"7e430d95":"#### PACKAGE IMPORTS ####\n# ML design\nimport tensorflow as tf\nfrom tensorflow import keras\n!pip install -q tensorflow-text\nimport tensorflow_text as text  # text processing \/ required for BERT encoder\nimport tensorflow_hub as hub  # for BERT encoder\n\n# data handling\nimport numpy as np\nimport pandas as pd\nimport string\nimport random\n\n# file management\nimport os\nimport bz2\nimport pickle\nimport _pickle as cPickle","95d40497":"# integrations\n# mount google drive:\n# (for saving models and checkpoints)\nGDRIVE_DIR = '\/content\/gdrive\/'\nfrom google.colab import drive\ndrive.mount(GDRIVE_DIR)","5a48b6f3":"# GLOBAL VARIABLES\n\n# set author \/ file paths\nAUTHOR = 'tests'\n\n# model structure\nUSE_WORD_PATH = True\nif USE_WORD_PATH:\n    AUTHOR = AUTHOR + '_words_model\/'\nelse: PATH_EXTENSION = ''\n\n# saving models\/ checkpoints\n# (Google Drive)\nFILEPATH = GDRIVE_DIR + 'MyDrive\/Colab_Notebooks\/models\/text_generation\/' + AUTHOR\nCHECKPOINT_DIR = FILEPATH + '\/checkpoints\/'\nPREDICTION_MODEL_DIR = FILEPATH + '\/prediction_model\/'\nTRAINING_MODEL_DIR = FILEPATH + '\/training_model\/'\nPROCESSED_DATA_DIR = FILEPATH + '\/proc_data\/'\n\n# online dataset repository\nDATASETS_DIR = 'https:\/\/raw.githubusercontent.com\/mvenouziou\/text_generator\/main\/'\nDATA_FILES = ['robert_frost_collection.csv']","c5f26df0":"# GLOBAL PARAMATERS\n\nNUM_TRAILING_WORDS = 5  # for word model path\nPADDED_EXAMPLE_LENGTH = 500  # for character model path\nBATCH_SIZE = 32","f351a0d4":"def create_character_tokenizer():\n    \"\"\"\n    This function takes a list of strings as its argument. It should create \n    and return a Tokenizer according to the above specifications. \n    \"\"\"\n    \n    char_tokens = string.printable\n    filters = '#$%&()*+-\/<=>@[]^_`{|}~\\t'\n\n    # Initialize standard keras tokenizer\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n                    num_words=None,  \n                    filters=filters,\n                    lower=False,  # conversion to lowercase letters\n                    char_level=True,\n                    oov_token=None,  # drop unknown characters\n                    )\n    \n    # fit tokenizer\n    tokenizer.fit_on_texts(char_tokens)\n\n    return tokenizer","cbcfb6a2":"def make_padded_array(text_blocks, tokenizer=None, max_len=PADDED_EXAMPLE_LENGTH):\n    # Tokenizes and applies padding for uniform length\n\n    # load tokenizer if one is not supplied\n    if tokenizer is None:\n        tokenizer = create_character_tokenizer()\n\n    # tokenize\n    token_blocks = tokenizer.texts_to_sequences(text_blocks)\n\n    # zero padding\n    padded_blocks = tf.keras.preprocessing.sequence.pad_sequences(\n                        sequences=token_blocks,  # dataset\n                        maxlen=max_len, \n                        dtype='int32', \n                        padding='pre',\n                        truncating='pre', \n                        value=0.0\n                        )\n    \n    return padded_blocks","0f62f1a1":"def get_bert_encoder(seq_length=NUM_TRAILING_WORDS):\n\n    # Word Embeddings path (bert encoder)\n    encoder_url = 'https:\/\/tfhub.dev\/tensorflow\/' \\\n                        + 'small_bert\/bert_en_uncased_L-2_H-128_A-2\/1'\n    preprocessor_url = 'https:\/\/tfhub.dev\/tensorflow\/' \\\n                        + 'bert_en_uncased_preprocess\/3'\n                \n    # preprocessing layer\n    # get BERT components\n    preprocessor = hub.load(preprocessor_url)\n    bert_tokenizer = hub.KerasLayer(preprocessor.tokenize,\n                                    name='bert_tokenizer')\n    bert_packer = hub.KerasLayer(preprocessor.bert_pack_inputs,\n                                 arguments=dict(seq_length=seq_length),\n                                 name='bert_input_packer')\n    bert_encoder = hub.KerasLayer(encoder_url, trainable=False, \n                             name='BERT_encoder')\n    \n    return bert_tokenizer, bert_packer, bert_encoder","ecd09140":"# Function: loader for .csv files\ndef prepare_csv(filename, datasets_dir=DATASETS_DIR, \n                content_columns=['Name', 'Content'], shuffle_rows=True):\n    \n    # load data into DataFrame\n    dataframe = pd.read_csv(datasets_dir + filename).dropna()\n    \n    # extract titles and content\n    # note: column headings must match those below\n    if 'Name ' in dataframe.columns:  # required for the Robert Frost set\n        dataframe.rename(columns={'Name ':'Name'})\n    \n    # prepare titles\n    try: \n        dataframe['Name'] = dataframe['Name'].apply(\n                            lambda x: x.upper() + ':\\n')\n    except:\n        # no titles found\n        content_columns = ['Content']\n\n    # prepare content\n    dataframe['Content'] = dataframe['Content'].apply(\n                    lambda x: x + '\\n')\n\n    # restrict dataset\n    dataframe = dataframe[content_columns]\n\n    # shuffle entries (rows)\n    if shuffle_rows:\n        dataframe = dataframe.sample(frac=1)\n    \n    # data cleanup\n    dataframe = dataframe[content_columns]\n    \n    # merge desired text columns\n    dataframe['merge'] = dataframe[content_columns[0]]\n    for i in range(1, len(content_columns)):\n        dataframe['merge'] = dataframe['merge'] + dataframe[content_columns[i]]\n\n    # convert to list of strings\n    data_list = dataframe['merge'].tolist()\n    \n    return data_list   \n\n\n# Function: Load and standardize data files\ndef load_parse(data_list, display_samples=True):  \n\n    # remove paragraph \/ line marks and split up words  \n    tokenizer = text.WhitespaceTokenizer()\n\n    # tokenize data (outputs bytestrings)\n    cleaned_list_byte = [tokenizer.tokenize(data).numpy() for data in data_list]\n\n    # convert data back to string format\n    num_entries = len(cleaned_list_byte)\n\n    clean_list = [' '.join(map(lambda x: x.decode(), cleaned_list_byte[i])) \n                    for i in range(num_entries)]\n\n    # Display some text samples\n    if display_samples:\n        num_samples = 5\n        inx = np.random.choice(len(clean_list), num_samples, replace=False)\n        for example in np.array(clean_list)[inx]:\n            print(example)\n            print()\n\n        print('len(text_chunks):', len(clean_list))\n\n    return clean_list","a2032047":"def create_input_target_blocks(full_examples, tokenizer=None,\n                               max_len=PADDED_EXAMPLE_LENGTH,\n                               num_words=NUM_TRAILING_WORDS):\n    # converts text into sliding n-grams of words and characters\n    # returning input \/ target sets\n\n    # helper function to create word-level inputs\n    def update_word_char_lists(text, chars_list, words_list):\n        words_input = text.split(' ')  # separate words\n        words_input = words_input[-num_words-1: -1]  # get trailing words\n\n        # convert words to string (tensor)\n        words_input = ' '.join(words_input)\n\n        # add values to lists\n        chars_list.append(text)\n        words_list.append([words_input])\n        \n        return None\n\n    if tokenizer is None:\n        tokenizer = create_character_tokenizer()\n\n    blocks = []\n    for example in full_examples:      \n\n        char_block = []\n        word_block = []\n        example_length = len(example)\n\n        # small blocks at start (will be zero-padded later)\n        leading_characters = 1  # min chars to seed predictions\n        for i in range(leading_characters, example_length - max_len - 1):\n            text = example[: i]\n            update_word_char_lists(text, char_block, word_block)\n\n        # full length blocks\n        for i in range(example_length - max_len - 1):\n            # create n-gram\n            text = example[i: max_len + i]\n            update_word_char_lists(text, char_block, word_block)\n\n        # small blocks at end (will be zero-padded later)\n        for i in range(example_length - max_len - 1, example_length-1):\n            text = example[i: ]\n            update_word_char_lists(text, char_block, word_block)\n    \n        # tokenize and add pre-padding\n        char_block = make_padded_array(char_block, tokenizer, max_len=max_len)\n\n        # separate into inputs and targets\n        inputs_char = char_block[:, :-1]\n        targets_char = char_block[:, 1:]\n\n        # update blocks\n        word_block = np.array(word_block)\n        blocks.append((inputs_char, word_block, targets_char))\n\n    return blocks","7ce54d3d":"# Function: data prep to create stateful RNN batches\n# note: This will be applied separately on each example text, \n# so that RNN can reset internal state \/ distinguish between unrelated passages\n# note: This code is taken directly from Imperial College London's \n# Coursera course cited above\n\ndef preprocess_stateful(char_input, word_input, target, batch_size=BATCH_SIZE):\n\n    # Prepare input and output arrays for training the stateful RNN\n    num_examples = char_input.shape[0]\n\n    # adjust for batch size to divide evenly into sample size\n    num_processed_examples = num_examples - (num_examples % batch_size)\n    input_cropped = char_input[:num_processed_examples]\n    target_cropped = target[:num_processed_examples]\n\n    # separate out samples so rows of data match up across epochs\n    # 'steps' measures how to space them out\n    steps = num_processed_examples \/\/ batch_size  \n\n    # define reordering\n    inx = np.empty((0,), dtype=np.int32)  # initialize empty array object\n    \n    for i in range(steps):\n        inx = np.concatenate((inx, i + np.arange(0, num_processed_examples, \n                                                    steps)))\n\n    # reorder the data\n    input_char_stateful = input_cropped[inx]\n    input_word_stateful = word_input[inx]\n    target_seq_stateful = target_cropped[inx]\n\n    return input_char_stateful, input_word_stateful, target_seq_stateful","342f57e9":"def input_pipeline(data_files=DATA_FILES, verbose=True, batch_size=BATCH_SIZE, \n                   max_len=PADDED_EXAMPLE_LENGTH, num_words=NUM_TRAILING_WORDS,\n                   datasets_dir=DATASETS_DIR, saved_proc_dir=PROCESSED_DATA_DIR):\n\n    # load previously processed data (pbz2 compressed file format)\n    try:    \n        with bz2.open(saved_proc_dir + 'datafiles.pbz2', 'rb') as file:\n            data_dict = cPickle.load(file)\n\n        X_data_list = data_dict['X_data_list']\n        Y_data_list = data_dict['Y_data_list']\n\n        print('loaded saved pre-processed data')\n\n    except:       \n\n        # load data file\n        data_list = []\n        for filename in data_files:\n\n            # check file extension and select loader (csv or txt)\n            _, file_extension = os.path.splitext(filename)     \n\n            if file_extension == '.csv':   \n                data = prepare_csv(filename, \n                                datasets_dir=datasets_dir, \n                                content_columns=['Name', 'Content'], \n                                shuffle_rows=True)\n                \n            else: # file_extension == '.txt':\n                with open(filepath + '\/' + filename, 'r', encoding='utf-8') as file:\n                    data = file.readlines()\n\n            # add extracted list of texts to data list\n            data_list += data\n\n        if verbose:\n            print('PROGRESS: data_list created')\n        \n        # clean data\n        clean_list = load_parse(data_list, display_samples=False)\n        if verbose:\n            print('PROGRESS: clean_list created')\n        \n        # preprocess data\n        tokenizer = create_character_tokenizer()\n        blocks = create_input_target_blocks(full_examples=clean_list, \n                                            tokenizer=tokenizer,\n                                            max_len=max_len,\n                                            num_words=num_words)\n        if verbose:\n            print('PROGRESS: blocks created')\n        \n        # create separate input \/ target pairs for each block\n        X_data_list = []\n        Y_data_list = []\n\n        i=0\n        for block in blocks:\n            if i % 10 == 0:\n                print(f'PROGRESS: processing block {i} of {len(blocks)}')\n\n            char_input = block[0] \n            word_input = block[1] \n            target = block[2]\n\n            input_char_stateful, input_word_stateful, target_seq_stateful = \\\n                                    preprocess_stateful(char_input=char_input, \n                                                        word_input=word_input, \n                                                        target=target, \n                                                        batch_size=batch_size)\n\n            # group for model input\n            X = [input_char_stateful, input_word_stateful]\n            Y = target_seq_stateful\n\n            X_data_list.append(X)\n            Y_data_list.append(Y)\n\n            # advance index\n            i += 1\n\n        # save file (pbz2 compressed file format)\n        with bz2.BZ2File(saved_proc_dir + 'datafiles.pbz2', 'wb') as sfile:\n            cPickle.dump({'X_data_list': X_data_list, \n                          'Y_data_list': Y_data_list}, sfile)\n\n    return X_data_list, Y_data_list","fb6bbe55":"# Function: Model Definition\ndef get_training_model(use_word_path=USE_WORD_PATH,\n                       verbose=True,\n                       batch_size=BATCH_SIZE, \n                       padded_examples=PADDED_EXAMPLE_LENGTH,\n                       num_words=NUM_TRAILING_WORDS):\n    \n    \"\"\" Defines and compiles our stateful RNN model. \n    Note: batch size is required argument for stateful RNN. \"\"\"\n    \n    from keras.layers import Input, Embedding, Concatenate, Dense, GRU,\\\n                             Average, Dropout, BatchNormalization, Lambda\n\n    # parameters\n    vocab_size = len(create_character_tokenizer().word_index) + 1\n    embedding_dim = 256\n    merge_dim = 128\n    \n    # Build model\n    # define input shapes\n    input_1 = Input(shape=(None, ), #(padded_examples-1, ), \n                    batch_size=batch_size,\n                    dtype=tf.int32, \n                    name='char_input')\n    \n    input_2 = Input(shape=(), \n                    batch_size=batch_size,\n                    dtype=tf.string, \n                    name='word_input')\n\n    # travel individual paths\n    # Character Level Path\n    # ## Char: Embedding\n    x1 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n                   mask_zero=True, batch_input_shape=(batch_size, None),\n                   name='char_embedding',)(input_1)\n\n    # ## Char: GRU 1\n    x1 = GRU(units=embedding_dim, stateful=True, \n             return_sequences=True, name='char_GRU_1',)(x1)\n    x1 = Dropout(rate=.10, name='char_Dropout_1')(x1)\n    x1 = BatchNormalization(name='char_Batch_Norm_1')(x1)\n    \n    # ## Char: GRU Final --  must use output_dim = merge_dim!\n    x1 = GRU(units=merge_dim, stateful=True, \n             return_sequences=True, name='char_GRU_final',)(x1)\n    x1 = Dropout(rate=.10, name='char_Dropout_final')(x1)\n    x1 = BatchNormalization(name='char_Batch_Norm_final')(x1)\n\n    # Word Encoding Path\n    if use_word_path:\n        \n        bert_tokenizer, bert_packer, bert_encoder = get_bert_encoder()\n        x2 = bert_tokenizer(input_2)  # tokenize\n        x2 = bert_packer([x2])  # pack inputs for encoder\n        x2 = bert_encoder(x2)['sequence_output'] # encoding\n\n        # ## Word: GRU 1\n        x2 = GRU(units=32, stateful=True, \n                 return_sequences=True, name='word_GRU_1',)(x2)\n        x2 = Dropout(rate=.10, name='word_Dropout_1')(x2)\n        x2 = BatchNormalization(name='word_Batch_Norm_1')(x2)\n\n        # ## Word: Required conversion to valid merge output dim = merge_dim!\n        x2 = Dense(units=num_words, activation=None, \n                   name='word_Dense_pre_final')(x2)\n        x2 = tf.keras.layers.AveragePooling1D(\n                pool_size=5, padding='same', name='word_pooling_final')(x2)\n        x2 = Dense(units=merge_dim, activation=None, \n                   name='word_Dense_final')(x2)\n\n        # Merge Paths\n        x = Average(name='merged_layers')([x1, x2])\n\n    else:\n        x = x1  # update variable id to match next step\n    \n    # Final GRU layer\n    x = GRU(units=embedding_dim, stateful=True, \n            return_sequences=True, name='GRU_OUTPUT')(x)          \n\n    # Character prediction (logits)\n    outputs = Dense(units=vocab_size, activation=None, \n                    name='Decoding')(x)       \n    \n    # create model\n    model = keras.Model(inputs=[input_1, input_2], outputs=outputs)\n\n    if verbose:\n        print(model.summary())\n\n    return model","d64151ed":"def get_prediction_model(trained_model=None, use_word_path=USE_WORD_PATH,\n                         padded_examples=PADDED_EXAMPLE_LENGTH, verbose=False):\n    \"\"\" enforces batch size = 1, only returns last character prediction\n     and loads any saved weights \"\"\"\n\n    # set paramaters\n    batch_size=1\n\n    # create model\n    prediction_model = get_training_model(batch_size=batch_size, \n                                          use_word_path=use_word_path,\n                                          padded_examples=padded_examples,\n                                          verbose=verbose)\n\n\n    # load weights from pre-trained model\n    if trained_model is not None:        \n        trained_weights = trained_model.get_weights()\n        prediction_model.set_weights(trained_weights)\n\n    return prediction_model","58d3f881":"def compile_model(model, learning_rate):\n    \n    model.compile(optimizer=tf.keras.optimizers.Adamax(\n                                learning_rate=learning_rate),\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n                                                    from_logits=True),\n                  metrics=['sparse_categorical_accuracy', \n                        'sparse_categorical_crossentropy'],\n                 )\n    \n    return model","0f14fc09":"# checkpoint manager\ndef create_checkpoint_manager(model, checkpoint_dir=CHECKPOINT_DIR):\n\n    checkpoint = tf.train.Checkpoint(model=model)\n\n    checkpoint_manager = tf.train.CheckpointManager(\n                            checkpoint=checkpoint, \n                            directory=checkpoint_dir, \n                            max_to_keep=4, \n                            keep_checkpoint_every_n_hours=None,\n                            checkpoint_name='ckpt', \n                            step_counter=None, \n                            checkpoint_interval=None,\n                            init_fn=None\n                            )\n    \n    return checkpoint, checkpoint_manager","386a19db":"# Function: Train model\ndef train_model(model, X_data_list, Y_data_list,\n                num_epochs=1, \n                num_datasets_to_use=1,\n                checkpoint=None, \n                checkpoint_manager=None,\n                learning_rate=0.001,\n                batch_size=BATCH_SIZE, \n                filepath=FILEPATH, \n                checkpoint_dir=CHECKPOINT_DIR):\n\n    # compile model\n    model = compile_model(model, learning_rate=learning_rate)\n\n    # set checkpoint manager\n    if checkpoint is None or checkpoint_manager is None:\n        checkpoint, checkpoint_manager = \\\n                    create_checkpoint_manager(model=model, \n                                              checkpoint_dir=checkpoint_dir)\n    \n    # organize training data\n    num_blocks = len(X_data_list)\n    train_datasets_list = list(zip(X_data_list, Y_data_list))       \n    \n    # begin training loop\n    for epoch in range(num_epochs):\n\n        print(f'Epoch: {epoch}')\n\n        # shuffle dataset order\n        random.shuffle(train_datasets_list)\n        print('shuffled datasets')\n\n        for i in range(num_datasets_to_use):\n            print(f'dataset: {i}')\n\n            # select dataset\n            data = train_datasets_list[i]\n            X = data[0]\n            Y = data[1]\n\n            # train model\n            history = model.fit(x=X, y=Y,\n                                shuffle=False,\n                                epochs=1,\n                                verbose=1)\n            \n            # reset RNN hidden states\n            model.reset_states()\n\n            # save checkpoint\n            checkpoint_manager.save()\n\n        # save full model at end of each epoch\n        model.save(checkpoint_dir + 'saved_model_epoch_' + str(epoch))\n\n    return model","4f9a5cf3":"def convert_to_input(last_token, trunc_text, \n                     prepare_words=USE_WORD_PATH,\n                     max_len=PADDED_EXAMPLE_LENGTH, \n                     num_words=NUM_TRAILING_WORDS):\n    \n    # words\n    if prepare_words:\n        words_input = trunc_text.split(' ')  # separate words\n        words_input = words_input[-num_words-1:-1]  # get trailing words\n        words_input = tf.constant(' '.join(trunc_text))  # convert to tensor\n    else:\n        words_input=tf.constant(' ')\n\n    # pad token sequence\n    inputs_char=tf.constant(last_token)\n    \"\"\"\n    length = max_len - 1\n    inputs_char = tf.keras.preprocessing.sequence.pad_sequences(\n                        sequences=inputs_char,  # dataset\n                        maxlen=length, \n                        dtype='int32', \n                        padding='pre',\n                        truncating='pre', \n                        value=0.0\n                        )\n    \"\"\"\n    # create separate input \/ target pairs for each block\n    X = [inputs_char, words_input]\n\n    return X","0d963b12":"def generator(input_text, prediction_model, precision_reduction=0, \n              num_characters=250, tokenizer=None, \n              max_len=PADDED_EXAMPLE_LENGTH, num_words=NUM_TRAILING_WORDS, \n              print_result=True):\n\n    # get tokenizer (if not supplied)      \n    if tokenizer is None:\n        tokenizer = create_character_tokenizer()\n    \n    # initialize generated text\n    last_token =  tokenizer.texts_to_sequences([input_text])\n    trunc_text = input_text.upper() + ':\\n'\n    generated_text = []\n   \n    # text generation loop\n    initial_state = None\n    for _ in range(num_characters):\n\n        # prepare input for model\n        inputs = convert_to_input(last_token=last_token, \n                                  trunc_text=trunc_text,\n                                  max_len=max_len,\n                                  num_words=num_words)\n        \n        # pass forward final GRU layer state\n        GRU_layer = prediction_model.get_layer('GRU_OUTPUT')\n        GRU_layer.reset_states(initial_state)\n        \n        # run model and compute logits\n        output = prediction_model(inputs)\n        logits = output[:, -1, :]  # extract last character logits\n        logits = logits.numpy()\n       \n        # generate next character from logits distribution\n        # purturb probabilities (optional)\n        if precision_reduction != 0:\n            fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)\n            logits = logits * (1 + precision_reduction * fuzz_factor)\n\n        last_token = tf.random.categorical(logits=logits, num_samples=1)\n        last_token = last_token.numpy().tolist()\n        \n        # get input for next character prediction\n        input_text = tokenizer.sequences_to_texts(last_token)\n        input_text = input_text[0]\n\n        # record generated character\n        generated_text.append(input_text)\n\n        #  get GRU state for next character prediction\n        initial_state = GRU_layer.states[0].numpy()\n\n    # reset for next run\n    output_text = ''.join(generated_text)\n    \n    if print_result:\n        print(output_text)\n\n    return output_text","654364e3":"# Store trained model separate from checkpoints\ndef save_model(model, model_dir):\n\n    # save model\n    model.save(model_dir)\n\n    # get tokenizer\n    prediction_tokenizer = create_character_tokenizer()\n    \n    # save tokenizer\n    with open(model_dir + 'tokenizer.pickle', 'wb') as file:\n        pickle.dump(prediction_tokenizer, file, pickle.HIGHEST_PROTOCOL)\n\n    return None","66ceba93":"X_data_list, Y_data_list = input_pipeline(DATA_FILES)","6aa4ab7e":"training_model = get_training_model(verbose=True)","958d7006":"try:\n    # load from checkpoint\n    checkpoint, checkpoint_manager = \\\n        create_checkpoint_manager(model=training_model, \n                                    checkpoint_dir=CHECKPOINT_DIR)\n\n    checkpoint_manager.restore_or_initialize()\n    print('loaded checkpoint')\n\nexcept:\n    print('No matching checkpoints')","6b50f753":"num_epochs = 10\n\ntraining_model = train_model(training_model, X_data_list, Y_data_list,\n                             num_epochs=num_epochs, \n                             num_datasets_to_use=10)","04ef7249":"prediction_model = get_prediction_model(trained_model=training_model)","b0e17731":"input_text = 'She '\n\ngenerator(input_text=input_text, \n          prediction_model=prediction_model, \n          precision_reduction=0, \n          num_characters=250, \n          tokenizer=None, \n          max_len=PADDED_EXAMPLE_LENGTH, \n          num_words=NUM_TRAILING_WORDS, \n          print_result=True)","3cb286d5":"# training model\nsave_model(training_model, model_dir=TRAINING_MODEL_DIR)\n\n# prediction model\nsave_model(prediction_model, model_dir=PREDICTION_MODEL_DIR)","3aa92b71":"Text Generation RNN\n\nThis program constructs a character-level sequence model to generate text according to a character distribution learned from the dataset.\n\n- Try my web app implementation at www.communicatemission.com\/ml-projects#text_generation. (Currently, only the standard model is implemented in the app)\n\n- See more at https:\/\/raw.githubusercontent.com\/mvenouziou\/text_generator.\n\n- See credits \/attributions below\n","ca3c798f":"##### GLOBAL VARIABLES\nFile directories and hyperparameters","e8072b03":"Compiler","35a1df71":"## Text Generation in the style of Robert Frost","f0b4dabe":"Load Latest Training Checkpoint","81eb087a":"Word-Level","cf375060":"Saving Models","fc22bae3":"Generate Text","6de8c63d":"Input Pipeline","41c628af":"### Define Implementation Functions","8c97269f":"Training Model","8c1e7607":"Character-Level","fe6de6a6":"### Define Data Pre-processors","fd2fb4c0":"* Load and Process Data","c57c851a":"**Some Unique Features**\n\n*(Although very likely that others have created similar models, I personally have not seen them and these can be considered independent constructions. Citations for other content are below:)*\n\n- Option to implement either the standard linear model architecture (see credits below) or nonlinear architectures.\n\n- Nonlinear model architecture uses parallel RNN's for word-level embeddings and character-level embeddings. \n\n- Manage RNN statefulness for independent data sources. The linear models credited below use a single continuous work, which necessarily implies a dependence relation between samples \/ batches. This model implements the ability to treat independent works (individual poems, books, authors, etc.) as truly independent samples by resetting RNN states and shuffling independent data sources.\n\n- Load and prepare data from multiple CSV and text files. Each rows from a CSV and each complete TXT file are treated as independent data sources. (CSV data prep accepts titles and content.) ","6579ba31":"Train Model","ff9fdd25":"### Define Models and Training Loop","82eadc34":"Save Models","1596b58a":"Initialize Training Model","229ef8b7":"**Credits \/ Citations \/ Attributions:**\n\nLinear Model and Shared Code:\n\n- Other than items noted in previous sections, this python code and linear model structure is based heavily on code found in Imperial College London's Coursera course, \"Customising your models with Tensorflow 2\" (https:\/\/www.coursera.org\/learn\/customising-models-tensorflow2) and the Tensorflow RNN text generation documentation (https:\/\/www.tensorflow.org\/tutorials\/text\/text_generation?hl=en).\n\nNonlinear Model:\n\n- This utilizes the pretrained Small BERT word embeddings from Tensorflow Hub, which they credit to Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova's paper \"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models.\" (See https:\/\/tfhub.dev\/google\/collections\/bert\/1)\n\nWeb App The web app is built on the Anvil platform and (at the time of this writing) is hosted on Google Cloud server (CPU).\n\nAbout\n\n- LinkedIn: https:\/\/www.linkedin.com\/in\/movenouziou\/\n- GitHub: https:\/\/github.com\/mvenouziou\n\n","f11d17a6":"Create Prediction Model","3e4c7a56":"Prediction Model","6a8de1e5":"Training Loop","72b203fc":"# Implementation","be839958":"Checkpoint Manager"}}