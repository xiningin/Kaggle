{"cell_type":{"f674217e":"code","838a3a81":"code","10f7de74":"code","45d47f1b":"code","7c4c0d15":"code","c588f84c":"code","fabe075c":"code","b7ede1b0":"code","db6077a6":"code","9eb4a065":"code","9c093f5c":"code","78ba8d17":"code","704653a2":"code","79151070":"code","99eb9993":"code","4891196f":"code","cee37fe9":"code","83825819":"code","ea534905":"code","15d9a968":"code","aab7da06":"code","707ffa16":"code","63b0549c":"code","4f596833":"code","1192f456":"code","6b196bf9":"code","9dd3adbe":"code","3d212c3f":"code","f9e9f327":"code","7e99ff43":"code","1bc725fa":"code","7dcd0941":"code","3a50b7df":"code","90df453c":"code","958a49ea":"code","48f1940c":"code","c6e4f856":"code","b8439e51":"code","ff163f99":"code","541734b6":"code","373edf21":"code","b6746d48":"code","54eb55ac":"code","849fd7c3":"code","f25470ef":"code","ccd77159":"code","3c14940c":"code","59ef2cc5":"code","d940a5ed":"code","cb4de89f":"code","86fad7dd":"code","25bc6946":"code","964e9a1a":"code","4834668c":"code","3233b680":"markdown","78017dfe":"markdown","0200273f":"markdown","f8e3711d":"markdown","c7a9c06e":"markdown","ce66f48a":"markdown","6cec6181":"markdown","a059c6f2":"markdown","ee7a5bc2":"markdown","7354be3b":"markdown","a60aa3ce":"markdown","b68d9938":"markdown","e47acaeb":"markdown","49b4c2ea":"markdown","2db834dc":"markdown","a5674208":"markdown","07e4e9e2":"markdown","32150fdd":"markdown","c9ba74fb":"markdown","d29cdf6e":"markdown","5854bc30":"markdown","d5d4c568":"markdown","b33d8a3d":"markdown","87394ede":"markdown","517e9572":"markdown","788ed833":"markdown","c8a5fb70":"markdown","2a167740":"markdown","473d7931":"markdown","c9c37b76":"markdown"},"source":{"f674217e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n","838a3a81":"# You have to include the full link to the csv file containing your dataset\ncreditcard_df = pd.read_csv(r\"\/kaggle\/input\/creditcard-marketing\/4.Marketing_data.csv\")\n# CUSTID: Identification of Credit Card holder \n# BALANCE: Balance amount left in customer's account to make purchases\n# BALANCE_FREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n# PURCHASES: Amount of purchases made from account\n# ONEOFFPURCHASES: Maximum purchase amount done in one-go\n# INSTALLMENTS_PURCHASES: Amount of purchase done in installment\n# CASH_ADVANCE: Cash in advance given by the user\n# PURCHASES_FREQUENCY: How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n# ONEOFF_PURCHASES_FREQUENCY: How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n# PURCHASES_INSTALLMENTS_FREQUENCY: How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n# CASH_ADVANCE_FREQUENCY: How frequently the cash in advance being paid\n# CASH_ADVANCE_TRX: Number of Transactions made with \"Cash in Advance\"\n# PURCHASES_TRX: Number of purchase transactions made\n# CREDIT_LIMIT: Limit of Credit Card for user\n# PAYMENTS: Amount of Payment done by user\n# MINIMUM_PAYMENTS: Minimum amount of payments made by user  \n# PRC_FULL_PAYMENT: Percent of full payment paid by user\n# TENURE: Tenure of credit card service for user","10f7de74":"creditcard_df","45d47f1b":"creditcard_df.info()\n# 18 features with 8950 points  ","7c4c0d15":"creditcard_df.describe()\n# Mean balance is $1564 \n# Balance frequency is frequently updated on average ~0.9\n# Purchases average is $1000\n# one off purchase average is ~$600\n# Average purchases frequency is around 0.5\n# average ONEOFF_PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY, and CASH_ADVANCE_FREQUENCY are generally low\n# Average credit limit ~ 4500\n# Percent of full payment is 15%\n# Average tenure is 11.5 years","c588f84c":"# Let's see who made one off purchase of $40761!\ncreditcard_df[creditcard_df['ONEOFF_PURCHASES'] == creditcard_df['ONEOFF_PURCHASES'].max()]\n","fabe075c":"creditcard_df['CASH_ADVANCE'].max()","b7ede1b0":"# Let's see who made cash advance of $47137!\n# This customer made 123 cash advance transactions!!\n# Never paid credit card in full\n\ncreditcard_df[creditcard_df['CASH_ADVANCE'] == creditcard_df['CASH_ADVANCE'].max()]\n","db6077a6":"# Let's see if we have any missing data, luckily we don't!\nsns.heatmap(creditcard_df.isnull(), yticklabels = False, cbar = False, cmap=\"rainbow\")\nplt.show()\n","9eb4a065":"creditcard_df.isnull().sum()","9c093f5c":"# Fill up the missing elements with mean of the 'MINIMUM_PAYMENT' \ncreditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS'] = creditcard_df['MINIMUM_PAYMENTS'].mean()\n","78ba8d17":"# Fill up the missing elements with mean of the 'CREDIT_LIMIT' \ncreditcard_df.loc[(creditcard_df['CREDIT_LIMIT'].isnull() == True), 'CREDIT_LIMIT'] = creditcard_df['CREDIT_LIMIT'].mean()","704653a2":"sns.heatmap(creditcard_df.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\nplt.show()","79151070":"# Let's see if we have duplicated entries in the data\ncreditcard_df.duplicated().sum()","99eb9993":"# Let's drop Customer ID since it has no meaning here \ncreditcard_df.drop(\"CUST_ID\", axis = 1, inplace= True)","4891196f":"creditcard_df.head()","cee37fe9":"n = len(creditcard_df.columns)\nn","83825819":"creditcard_df.columns","ea534905":"# distplot combines the matplotlib.hist function with seaborn kdeplot()\n# KDE Plot represents the Kernel Density Estimate\n# KDE is used for visualizing the Probability Density of a continuous variable. \n# KDE demonstrates the probability density at different values in a continuous variable. \n\n# Mean of balance is $1500\n# 'Balance_Frequency' for most customers is updated frequently ~1\n# For 'PURCHASES_FREQUENCY', there are two distinct group of customers\n# For 'ONEOFF_PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY' most users don't do one off puchases or installment purchases frequently \n# Very small number of customers pay their balance in full 'PRC_FULL_PAYMENT'~0\n# Credit limit average is around $4500\n# Most customers are ~11 years tenure\n\nplt.figure(figsize=(20,60))\nfor i in range(len(creditcard_df.columns)-1):\n  plt.subplot(16, 2, i+1)\n  sns.distplot(creditcard_df[creditcard_df.columns[i]], kde_kws={\"color\": \"r\", \"lw\": 1, \"label\": \"KDE\"}, hist_kws={\"color\": \"g\"})\n  plt.title(creditcard_df.columns[i])\n\nplt.tight_layout()","15d9a968":"plt.figure(figsize = (10,5))\nax=sns.countplot(creditcard_df['TENURE'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.show()","aab7da06":"correlations = creditcard_df.corr()\n","707ffa16":"f, ax = plt.subplots(figsize = (20, 20))\nsns.heatmap(correlations, annot = True)\nplt.show()\n# 'PURCHASES' have high correlation between one-off purchases, 'installment purchases, purchase transactions, credit limit and payments. \n# Strong Positive Correlation between 'PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY'\n","63b0549c":"# Let's scale the data first\nscaler = StandardScaler()\ncreditcard_df_scaled = scaler.fit_transform(creditcard_df)","4f596833":"creditcard_df_scaled.shape","1192f456":"creditcard_df_scaled","6b196bf9":"scores_1 = []\n\nrange_values = range(1, 20)\n\nfor i in range_values:\n  kmeans = KMeans(n_clusters = i)\n  kmeans.fit(creditcard_df_scaled)\n  scores_1.append(kmeans.inertia_) \nplt.figure(figsize = (10,5))\nplt.plot(scores_1, 'bx-')\nplt.title('Finding the right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('Scores') \nplt.show()\n\n# From this we can observe that, 4th cluster seems to be forming the elbow of the curve. \n# However, the values does not reduce linearly until 8th cluster. \n# Let's choose the number of clusters to be 7.","9dd3adbe":"kmeans = KMeans(8)\nkmeans.fit(creditcard_df_scaled)\nlabels = kmeans.labels_","3d212c3f":"kmeans.cluster_centers_.shape","f9e9f327":"\ncluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [creditcard_df.columns])\ncluster_centers           ","7e99ff43":"# In order to understand what these numbers mean, let's perform inverse transformation\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns = [creditcard_df.columns])\ncluster_centers\n\n# First Customers cluster (Transactors): Those are customers who pay least amount of intrerest charges and careful with their money, Cluster with lowest balance ($104) and cash advance ($303), Percentage of full payment = 23%\n# Second customers cluster (revolvers) who use credit card as a loan (most lucrative sector): highest balance ($5000) and cash advance (~$5000), low purchase frequency, high cash advance frequency (0.5), high cash advance transactions (16) and low percentage of full payment (3%)\n# Third customer cluster (VIP\/Prime): high credit limit $16K and highest percentage of full payment, target for increase credit limit and increase spending habits\n# Fourth customer cluster (low tenure): these are customers with low tenure (7 years), low balance \n","1bc725fa":"labels.shape # Labels associated to each data point","7dcd0941":"labels.max()","3a50b7df":"labels.min()","90df453c":"y_kmeans = kmeans.fit_predict(creditcard_df_scaled)\ny_kmeans\n","958a49ea":"# concatenate the clusters labels to our original dataframe\ncreditcard_df_cluster = pd.concat([creditcard_df, pd.DataFrame({'cluster':labels})], axis = 1)\ncreditcard_df_cluster.head()","48f1940c":"# Plot the histogram of various clusters\nfor i in creditcard_df.columns:\n  plt.figure(figsize = (35, 5))\n  for j in range(8):\n    plt.subplot(1,8,j+1)\n    cluster = creditcard_df_cluster[creditcard_df_cluster['cluster'] == j]\n    cluster[i].hist(bins = 20)\n    plt.title('{}    \\nCluster {} '.format(i,j))\n  \n  plt.show()\n\n","c6e4f856":"# Obtain the principal components \npca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(creditcard_df_scaled)\nprincipal_comp","b8439e51":"# Create a dataframe with the two components\npca_df = pd.DataFrame(data = principal_comp, columns =['pca1','pca2'])\npca_df.head()","ff163f99":"# Concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","541734b6":"plt.figure(figsize=(10,10))\nax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"cluster\", data = pca_df, palette =['red','green','blue','pink','yellow','gray','purple', 'black'])\nplt.show()","373edf21":"\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom keras.optimizers import SGD\n\nencoding_dim = 7\n\ninput_df = Input(shape=(17,))\n\n\n# Glorot normal initializer (Xavier normal initializer) draws samples from a truncated normal distribution \n\nx = Dense(encoding_dim, activation='relu')(input_df)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\nencoded = Dense(10, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\nx = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(encoded)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\ndecoded = Dense(17, kernel_initializer = 'glorot_uniform')(x)\n\n# autoencoder\nautoencoder = Model(input_df, decoded)\n\n#encoder - used for our dimention reduction\nencoder = Model(input_df, encoded)\n\nautoencoder.compile(optimizer= 'adam', loss='mean_squared_error')\n","b6746d48":"creditcard_df_scaled.shape","54eb55ac":"autoencoder.fit(creditcard_df_scaled, creditcard_df_scaled, batch_size = 128, epochs = 25,  verbose = 1)","849fd7c3":"autoencoder.summary()","f25470ef":"autoencoder.save_weights('autoencoder.h5')","ccd77159":"pred = encoder.predict(creditcard_df_scaled)","3c14940c":"pred.shape","59ef2cc5":"scores_2 = []\n\nrange_values = range(1, 20)\n\nfor i in range_values:\n  kmeans = KMeans(n_clusters= i)\n  kmeans.fit(pred)\n  scores_2.append(kmeans.inertia_)\nplt.figure(figsize=(10,10))\nplt.plot(scores_2, 'bx-')\nplt.title('Finding right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('scores') \nplt.show()","d940a5ed":"plt.figure(figsize=(10,10))\nplt.plot(scores_1, 'bx-', color = 'r')\nplt.plot(scores_2, 'bx-', color = 'g')\nplt.show()","cb4de89f":"kmeans = KMeans(4)\nkmeans.fit(pred)\nlabels = kmeans.labels_\ny_kmeans = kmeans.fit_predict(creditcard_df_scaled)","86fad7dd":"df_cluster_dr = pd.concat([creditcard_df, pd.DataFrame({'cluster':labels})], axis = 1)\ndf_cluster_dr.head()","25bc6946":"pca = PCA(n_components=2)\nprin_comp = pca.fit_transform(pred)\npca_df = pd.DataFrame(data = prin_comp, columns =['pca1','pca2'])\npca_df.head()","964e9a1a":"pca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","4834668c":"plt.figure(figsize=(10,10))\nax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"cluster\", data = pca_df, palette =['red','green','blue','yellow'])\nplt.show()","3233b680":"![alt text](https:\/\/drive.google.com\/uc?id=1BOX2q8R_8E4Icb4v1tpn1eymCTJY2b5o)","78017dfe":"![alt text](https:\/\/drive.google.com\/uc?id=1EYWyoec9Be9pYkOaJTjPooTPWgRlJ_Xz)","0200273f":"![alt text](https:\/\/drive.google.com\/uc?id=1r1FjdO8duujUoI904Oy4vbza6KktxSXo)","f8e3711d":"# FIND THE OPTIMAL NUMBER OF CLUSTERS USING ELBOW METHOD","c7a9c06e":"![alt text](https:\/\/drive.google.com\/uc?id=1Yfi-dpWW3keU5RLgwAT4YmQ2rfY1GxUh)","ce66f48a":"![alt text](https:\/\/drive.google.com\/uc?id=1rBQziDU0pS1Fz0m8VQRjQuBoGFSX1Spb)","6cec6181":"![alt text](https:\/\/drive.google.com\/uc?id=1vMr3ouoZ6Pc1mba1mBm2eovlJ3tfE6JA)","a059c6f2":"![alt text](https:\/\/drive.google.com\/uc?id=1v7hJEPiigSeTTaYo0djbO-L4uEnTpcAU)","ee7a5bc2":"![alt text](https:\/\/drive.google.com\/uc?id=1uS6vsccMt3koetsp3k9cAIfbpJw7Z1J8)","7354be3b":"# IMPORT LIBRARIES AND DATASETS","a60aa3ce":"# APPLY K-MEANS METHOD","b68d9938":"![alt text](https:\/\/drive.google.com\/uc?id=1AcyUL_F9zAD2--Hmyq9yTkcA9mC6-bwg)","e47acaeb":"# APPLY PRINCIPAL COMPONENT ANALYSIS AND VISUALIZE THE RESULTS","49b4c2ea":"![alt text](https:\/\/drive.google.com\/uc?id=1EBCmP06GuRjVfPgTfH85Yhv9xIAZUj-K)","2db834dc":"Data Source: https:\/\/www.kaggle.com\/arjunbhasin2013\/ccdata","a5674208":"![alt text](https:\/\/drive.google.com\/uc?id=1xDuvEnbuNqIjX5Zng39TCfGCf-BBDGf0)","07e4e9e2":"![image.png](attachment:image.png)","32150fdd":"\n<table>\n  <tr><td>\n    <img src=\"https:\/\/drive.google.com\/uc?id=1OjWCpwRHlCSNYaJoUUd2QGryT9CoQJ5e\"\n         alt=\"Fashion MNIST sprite\"  width=\"1000\">\n  <\/td><\/tr>\n  <tr><td align=\"center\">\n    <b>Figure 1. Customers Segmentation\n  <\/td><\/tr>\n<\/table>\n","c9ba74fb":"![alt text](https:\/\/drive.google.com\/uc?id=1VvqzWWY8wFGeP4cl-rVtWVOg1P6saHfZ)","d29cdf6e":"![alt text](https:\/\/drive.google.com\/uc?id=1ppL-slQPatrmHbPBEaT3-8xNH01ckoNE)","5854bc30":"# UNDERSTAND THE THEORY AND INTUITON BEHIND K-MEANS","d5d4c568":"![alt text](https:\/\/drive.google.com\/uc?id=1xk1D5uldId0DWywRJ3-OAVBcIr5NGCq_)","b33d8a3d":"# UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE","87394ede":"# APPLY AUTOENCODERS (PERFORM DIMENSIONALITY REDUCTION USING AUTOENCODERS)","517e9572":"![alt text](https:\/\/drive.google.com\/uc?id=1Q43AkxxDy4g-zl5lIX4_PBJtTguh4Ise)","788ed833":"![alt text](https:\/\/drive.google.com\/uc?id=1LpdL0-4E9lbc4s-x6eJ5zkyIVw_OpHuJ)","c8a5fb70":"![alt text](https:\/\/drive.google.com\/uc?id=1bLRDIZRda0NSTAdcbugasIjDjvgw4JIU)","2a167740":"- The elbow method is a heuristic method of interpretation and validation of consistency within cluster analysis designed to help find the appropriate number of clusters in a dataset. \n- If the line chart looks like an arm, then the \"elbow\" on the arm is the value of k that is the best.\n- Source: \n  - https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)\n  - https:\/\/www.geeksforgeeks.org\/elbow-method-for-optimal-value-of-k-in-kmeans\/","473d7931":"# VISUALIZE AND EXPLORE DATASET","c9c37b76":"![alt text](https:\/\/drive.google.com\/uc?id=1g0tWKogvKaCrtsfzjApi6m8yGD3boy4x)"}}