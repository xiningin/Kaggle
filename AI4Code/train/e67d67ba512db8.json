{"cell_type":{"a186f88f":"code","a665f74f":"code","66e98528":"code","29a7a0b7":"code","624fbabb":"code","467a5d9b":"code","510637ff":"code","6c7ce7dc":"code","69a58039":"code","3f3494c4":"code","01ba8f2e":"code","4d9360db":"code","b7cba4f8":"code","9447a218":"code","ec2cd1e8":"code","8816e1e8":"code","6245df45":"code","ad10ef87":"code","4921122d":"code","28c80f04":"code","2ced1069":"markdown","b0b69239":"markdown","5d001e99":"markdown","ed98f27f":"markdown","a68c9df8":"markdown","4d88d9aa":"markdown","6aa81d98":"markdown","86523e9f":"markdown","abe9ce92":"markdown"},"source":{"a186f88f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a665f74f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nstyle.use('fivethirtyeight')","66e98528":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.shape","29a7a0b7":"# Scale the Amount and the time column\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nrb_scaler = RobustScaler()\n\ndf['scaled_amt'] = rb_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['scaled_time'] = rb_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n\ndf.drop(['Amount', 'Time'], axis = 1, inplace = True)\n\n\n","624fbabb":"sns.countplot(df['Class'], color = 'green')","467a5d9b":"# X = df.drop(['Class'], axis = 1)\n# y = df['Class']","510637ff":"# Create a balanced dataset with equal number of fraud and non fraud data\n# shuffel the data\ndf = df.sample(frac = 1, random_state = 1)\n\nfraud_df = df.loc[df['Class']==1]\n# pick 492 recirds of non fraud data\nnon_fraud_df = df.loc[df['Class']==0][:492]\nnormalized_df = pd.concat([fraud_df, non_fraud_df])\nnormalized_df.shape\n# again shuffel\nnew_df = normalized_df.sample(frac = 1)\n\nsns.countplot(new_df['Class'])","6c7ce7dc":"# corelation of features for new normalized, balanced data set\nplt.figure(figsize=(18, 12))\nsns.heatmap(new_df.corr(), annot=True)","69a58039":"fig, ax = plt.subplots(2, 4, figsize = (18, 10))\n\nsns.boxplot(x= 'Class', y = 'V12', data = new_df, ax = ax[0,0])\nax[0,0].set_title(\"V12 vs Class Negative Corelation\")\n\nsns.boxplot(x= 'Class', y = 'V14', data = new_df, ax = ax[0,1])\nax[0,1].set_title(\"V14 vs Class Negative Corelation\")\n\nsns.boxplot(x= 'Class', y = 'V16', data = new_df, ax = ax[0,2])\nax[0,2].set_title(\"V16 vs Class Negative Corelation\")\n\nsns.boxplot(x= 'Class', y = 'V10', data = new_df, ax = ax[0,3])\nax[0,3].set_title(\"V10 vs Class Negative Corelation\")\n\nsns.boxplot(x= 'Class', y = 'V4', data = new_df, ax = ax[1,0])\nax[1,0].set_title(\"V4 vs Class Positive Corelation\")\n\nsns.boxplot(x= 'Class', y = 'V11', data = new_df, ax = ax[1,1])\nax[1,1].set_title(\"V11 vs Class Positive Corelation\")\n\nsns.boxplot(x= 'Class', y = 'V2', data = new_df, ax = ax[1,2])\nax[1,2].set_title(\"V2 vs Class Positive Corelation\")","3f3494c4":"# remove outliers \noutline_col = ['V14', 'V16', 'V10']\nfor col in outline_col:\n    class1 = new_df[col].loc[new_df['Class']==1].values\n    q25, q75 = np.percentile(class1, 25), np.percentile(class1, 75)\n    print('{} - 25 percentile : {} 75 percent : {}'.format(col,q25, q75))\n    iqr = q75-q25\n    print('{} IQR : {}'.format(col, iqr))\n    interquartile_range = iqr*1.5\n    interquartile_range_lower,  interquartile_range_higher = q25 - interquartile_range, q75+interquartile_range\n    print('{}-Lower Range : {} Higher Range : {}'.format(col,interquartile_range_lower, interquartile_range_higher))\n    outliner = [x for x in class1 if(x < interquartile_range_lower or x> interquartile_range_higher)]\n#     print(len(outliner))\n\n    #  delete the outliners from the new df\n    new_df = new_df.drop(new_df.loc[(new_df[col]< interquartile_range_lower) | \n                                    (new_df[col] > interquartile_range_higher)].index)\n    new_df.shape","01ba8f2e":"fig, ax = plt.subplots(1, 3, figsize = (18, 10))\ni = 0\nfor i, col in zip(range(len(outline_col)),outline_col):\n    sns.boxplot(x= 'Class', y = col, data = new_df, ax = ax[i])\n    ax[i].set_title(\"{} vs Class After outliners deleted\".format(col))","4d9360db":"X = new_df.drop(['Class'], axis = 1)\ny = new_df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)\nprint('X Train Shape', X_train.shape)\nprint('Y Train Shape', y_train.shape)\nprint('X Test Shape', X_test.shape)\nprint('y Train Shape', y_test.shape)","b7cba4f8":"models = []\nmodels.append(('LSR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append((\"SVC\", SVC()))\nmodels.append(('DTC', DecisionTreeClassifier()))","9447a218":"for name, model in models:\n    model.fit(X_train, y_train)\n    prediction = model.predict(X_test)\n    score = accuracy_score(y_test, prediction)\n    print('{}Accuracy Score : {}%'.format(name, round(score, 4)*100))","ec2cd1e8":"estimator = LogisticRegression()\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n             \"penalty\": ['l1', 'l2']}\n\ngrid_search = GridSearchCV(estimator=estimator, param_grid = param_grid, cv=5,verbose=0)\ngrid_search.fit(X_train, y_train)\nlog_reg = grid_search.best_estimator_\nprint(log_reg)\n","8816e1e8":"# Hyperparameters for KNN\nestimator = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [5, 7, 10,12, 15, 20]}\n\ngrid_search = GridSearchCV(estimator=estimator, param_grid = param_grid, cv=5,verbose=0)\ngrid_search.fit(X_train, y_train)\nknn_clf = grid_search.best_estimator_\nprint(knn_clf)","6245df45":"estimator = SVC()\nparam_grid = {'kernel' : ['linear', 'rbf', 'poly'],\n             'gamma' : [0.1, 1, 10, 100],\n             'C' : [0.1, 1, 10, 100, 1000]}\n\ngrid_svc = GridSearchCV(estimator=estimator, param_grid = param_grid, cv=5,verbose=0)\ngrid_svc.fit(X_train, y_train)\nsvc_best = grid_svc.best_estimator_\nprint(svc_best)","ad10ef87":"estimator = DecisionTreeClassifier()\nparam_grid = {'max_depth' : [2,3,4,5],\n             'min_samples_split' : [2,3,4]}\n\ngrid_dtc = GridSearchCV(estimator=estimator, param_grid = param_grid, cv=5,verbose=0)\ngrid_dtc.fit(X_train,y_train)\ndtc_best = grid_dtc.best_estimator_\nprint(dtc_best)","4921122d":"from sklearn.model_selection import cross_val_score\ncv_score_hyper_params_logReg = cross_val_score(log_reg, X, y, cv = 5)\nprint('Logestic Regression Accuracy Score : {}%'.format(round(cv_score_hyper_params_logReg.mean(), 4)*100))","28c80f04":"cv_score_hyper_params_logReg = cross_val_score(knn_clf, X, y, cv = 5)\nprint('KNN Accuracy Score : {}%'.format(round(cv_score_hyper_params_logReg.mean(), 4)*100))","2ced1069":"This is Part 1, in this part I will work on \nIdentifying the ouliners - reducing them, \nFinding a classification model which will provide Accuracies on manually balanced data.\nfinding the best estimator for each of the Classification Classifier here i will use Stratified K fold Cross Validation technique","b0b69239":"You can still see few outliners since it's driven by the standard factor(1.5) which is used to derive interquartile_range.","5d001e99":"Remove the outliners from the 3 columns","ed98f27f":"I'll break it hear, in part 2 we will use the best estimators on Balanced data derived from NearMiss and SMOTE Algorithms.","a68c9df8":"Quick check on the outliners for the 3 columns","4d88d9aa":"Find the best parameters for the classifiers","6aa81d98":"now that we found the best params for the classifiers lets find the mean accuracy for top two high accuracy Classifiers by using CV","86523e9f":"BoxPlots: We will use boxplots to have a better understanding of the distribution of very high and low correlation features in fradulent and non fradulent transactions.","abe9ce92":"Just a quick look at how imbalanced the data is!"}}