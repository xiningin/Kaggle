{"cell_type":{"ed057b3d":"code","9f2fe356":"code","1fd77e2b":"code","66492e42":"code","145e1d21":"code","b443754f":"code","218ec156":"code","4c80c3bc":"code","992d1c3a":"code","395909bc":"code","90ac69b7":"code","983ae342":"code","5f913f3a":"code","6be23212":"code","392e759c":"code","81d6bbe3":"code","25f7bf7f":"code","3466f879":"code","f24a6127":"code","725a2988":"code","5d25b3f5":"code","a50b79c8":"markdown","9d3c79c7":"markdown","b9053f04":"markdown","7b0e9635":"markdown","7b7484e3":"markdown","55c51981":"markdown","c7887795":"markdown","a3bda3eb":"markdown","5a3ad506":"markdown","f6bd1963":"markdown","8d94930b":"markdown","1bb63454":"markdown","75f3b187":"markdown","d29228bd":"markdown","13b98f78":"markdown","dc8dc07a":"markdown","16081669":"markdown","695b7e56":"markdown","a51ead7d":"markdown","844c5265":"markdown","37fcc112":"markdown","22e2426d":"markdown"},"source":{"ed057b3d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore')\nFILE_PATH='\/kaggle\/input\/tabular-playground-series-may-2021\/'","9f2fe356":"# reading the training and test data\ntrain_data=pd.read_csv(FILE_PATH+'train.csv')\ntest_data=pd.read_csv(FILE_PATH+'test.csv')","1fd77e2b":"#columns of training data\ntrain_data.columns","66492e42":"# dropping the 'id' column from both train and test data\ntrain_data.drop(['id'],inplace=True,axis=1)\ntest_data.drop(['id'],inplace=True,axis=1)","145e1d21":"train_data.head(5)","b443754f":"# checking unique elements of target feature of train data\ntrain_data['target'].unique()","218ec156":"# converting the data type from 'object' to 'int'\ntrain_data['target']=train_data['target'].str[6:].astype('int')","4c80c3bc":"# features columns\nfeatures=[]\nfor i in range(0,50):\n    features.append('feature_'+str(i))\n#features","992d1c3a":"#display features and their minimum value (other than zero).\nprint('For training dataset--')\nfor fea in features:\n    if train_data[fea].min()!=0:\n        print(fea,\" \",train_data[fea].min())\nprint('For test dataset--')\nfor fea in features:\n    if test_data[fea].min()!=0:\n        print(fea,\" \",test_data[fea].min())","395909bc":"# Maximum value present in the dataset\nprint('Maximum value in training data',train_data.max().values.max())\nprint('Maximum value in test data',test_data.max().values.max())","90ac69b7":"# Finding feature that has maximum number of unique values both in training and test data\nprint('For training data--')\nmax_value=-1\nfeat=''\nfor fea in features:\n    if train_data[fea].nunique()>max_value:\n        feat=fea\n        max_value=train_data[fea].nunique()\nprint(feat,\" \",max_value)\nprint('For test data--')\nmax_value=-1\nfeat=''\nfor fea in features:\n    if test_data[fea].nunique()>max_value:\n        feat=fea\n        max_value=test_data[fea].nunique()\nprint(feat,\" \",max_value)","983ae342":"print('Value of features that is present on training data but not on test data')\nprint('-'*100)\nfor f in features:\n    train_set=set(train_data[f])\n    test_set=set(test_data[f])\n    # values present in train but not in test\n    dif_set=train_set.difference(test_set)\n    if dif_set != set():\n        print(f,'-'*10,dif_set)","5f913f3a":"print('Value of features that is present on test set but not on training set')\nprint('-'*100)\nfor f in features:\n    train_set=set(train_data[f])\n    test_set=set(test_data[f])\n    # values present in test but not in train\n    dif_set=test_set.difference(train_set)\n    if dif_set != set():\n        print(f,'-'*10,dif_set)","6be23212":"#count plot\nsns.countplot(x='target',data=train_data)","392e759c":"# Percentile of each class\nfor i in range(1,5):\n    print('Class_',i,end=' is ')\n    print(((train_data['target']==i).sum()\/train_data.shape[0])*100,'%')","81d6bbe3":"def display(feature):\n    ''' Function to display plot of a feature present both in training and test side by side'''\n    ax=[]\n    fig=plt.figure(figsize=(15,5))\n    ax.append(fig.add_subplot(1,2,1))\n    ax[-1].set_title(\"Training Data:\")\n    sns.histplot(x=feature,data=train_data,stat='density',kde =True)\n    ax.append(fig.add_subplot(1,2,2))\n    ax[-1].set_title(\"Test Data:\")\n    sns.histplot(x=feature,data=test_data,stat='density', kde=True)\n    return plt.show()","25f7bf7f":"for fea in features:\n    print('\\033[1m',fea.upper(),'\\033[0m') #\\033[1m and \\033[0m can be used to make python text bold\n    display(fea)","3466f879":"for fea in features:\n    if train_data[fea].mean() < train_data[fea].median():\n        print(fea,' is left skewed')","f24a6127":"# making a table showing the maximum occuring value and what percentage of data does it occupy on the training data\nfreq_table=pd.DataFrame()\nfreq=[]\nper=[]\nfor fea in features:\n    freq.append(train_data[fea].mode()[0])\n    per.append(((train_data[fea]==train_data[fea].mode()[0]).sum()\/train_data.shape[0])*100)\nfreq_table['Features']=features\nfreq_table['Max Occuring Value']=freq\nfreq_table['Percentage Occupied']=per\nfreq_table=freq_table.sort_values('Percentage Occupied')\nfreq_table.reset_index()","725a2988":"fig=plt.figure(figsize=(15,15))\nbarh=plt.barh(freq_table['Features'],freq_table['Percentage Occupied'])\nplt.bar_label(barh, fmt='%.01f%%')\nplt.xlabel('Percentage Occupied')","5d25b3f5":"corr=train_data.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(20, 20))\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True,cmap=\"YlGnBu\",linewidth=0.5)","a50b79c8":"All the 50 features seems to have **discrete numbers (starting from 0)**, and the target has 4 unique values but they have data type 'object'. ***It's a dataset with aim to classify the target among the 4 classes(namely Class_1, Class_2,Class_3,Class_4)***","9d3c79c7":"Let's check the distribution for our features now,","b9053f04":"**57.497%** of training data has target 'Class_2', whereas only **8.49%** of training data has target 'Class_1'. Thus, our target is bit **imbalanced.**\n","7b0e9635":"The tabular playground series are hosted by kaggle that are always more approachable compared to the their normal featured competitions. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. \n","7b7484e3":"The first thing that needs to be checked is obviously how the **target is distributed.**\n","55c51981":"We can see that very few features are present (given above) which have their minimum value less than zero. ","c7887795":"# Time for Visuals!\n","a3bda3eb":"The dataset is used for this competition is synthetic, but based on a real dataset and generated using a **CTGAN**. The original dataset deals with **predicting the category on an eCommerce product** given various attributes about the listing. Let's check them!","5a3ad506":"The value that occurs most frequently at training data is **zero**, many of which features are occupying **more than 50% of our data**. Let's visualize it to get a better idea.","f6bd1963":"Let's see how much percentage of our data has our value that is present the most at each feature","8d94930b":"From the above visual, we can easily confirm that there is **no high correlation present between any features.**","1bb63454":"# Loading Libraries","75f3b187":"The value is **not very high(compared to shape of data)**. Let's check in which feature the are maximum unique values.","d29228bd":"## Let's talk about data!","13b98f78":"# Introduction","dc8dc07a":"From the visuals, we can see that that majority of features are skewed. To check if a feature is left skewed or right skewed, we can just check the condition that for a feature to be **left skewed**, it's mean should be less than it's median, (from the figure below).\n![Skewness](https:\/\/www.statisticshowto.com\/wp-content\/uploads\/2014\/02\/pearson-mode-skewness.jpg)","16081669":"One more thing to note if one is working considering that the features are categorical is the values of features that is present of training data but not on test and vice-versa. ","695b7e56":"Thus, we can clearly infer that majority of features are **right skewed**. ","a51ead7d":"The features are **anonymized**, so it's difficult to directly get insights for each features from their names. The feature 'id' is not normally used in EDA so for now, let's remove it for now.","844c5265":"Thus, the maximum number of unique values is **71**, in training set and **65** in test set (not a large number compared to number of samples in the dataset), both present on the same feature **feature_38** . Thus, we can work thinking that all the features present are to be considered as categorical type, due to presence of discrete and finite values.","37fcc112":"At last, let's use **heatmap** to visualize the **correlation** between the features or between our features and target.","22e2426d":"Any suggestions what can I also add are most welcome and **Kindly Upvote the notebook if it is of any help!**"}}