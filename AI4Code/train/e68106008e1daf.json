{"cell_type":{"839c4f5f":"code","c1a7ec4c":"code","7e1a515c":"code","b7a5f534":"code","80f6fc53":"code","e0f687f7":"code","90ab8f88":"code","60f7dfd8":"code","e5edb12c":"code","05f1387b":"code","60c13590":"code","ad56f42f":"code","20e3e511":"code","d13c3417":"code","6b9d5198":"code","235374c2":"code","2e1d250f":"code","54e09186":"code","97e10f74":"code","c5a54a30":"code","0115e49d":"code","7a147e5d":"code","7132de18":"code","b3917216":"code","44a8d83b":"code","3f6a0d6e":"code","f2bf1f0c":"code","b7a6fc98":"code","20a18af7":"code","b75908ac":"code","69cc3972":"code","59cce74f":"code","635be4bf":"code","b13b0ea5":"code","1955a212":"code","4964ea44":"code","df59c2b4":"code","62636907":"code","83ef95da":"code","fc9c4d20":"code","b36cb2fb":"code","bd0e3bc8":"code","d7d6929c":"code","6bfcc2e6":"code","27e8060e":"code","8c8e1fc6":"code","c94d60a5":"markdown","11c83bd3":"markdown"},"source":{"839c4f5f":"!rm -r \/kaggle\/temp","c1a7ec4c":"!mkdir \/kaggle\/temp \/kaggle\/temp\/train \/kaggle\/temp\/test \/kaggle\/temp\/train_labels\n!unzip ..\/input\/data-science-bowl-2018\/stage2_test_final.zip -d \/kaggle\/temp\/test\n!unzip ..\/input\/data-science-bowl-2018\/stage1_test.zip -d \/kaggle\/temp\/test\n!unzip ..\/input\/data-science-bowl-2018\/stage1_train.zip -d \/kaggle\/temp\/train\n!unzip ..\/input\/data-science-bowl-2018\/stage1_train_labels.csv.zip \/kaggle\/temp\/masks","7e1a515c":"\n!ls \/kaggle\/temp","b7a5f534":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80f6fc53":"#!pip install Pillow\n","e0f687f7":"#!\/usr\/bin\/env python\n__author__ = \"Sreenivas Bhattiprolu\"\n__license__ = \"Feel free to copy, I appreciate if you acknowledge Python for Microscopists\"\n\n# https:\/\/www.youtube.com\/watch?v=0kiroPnV1tM\n# https:\/\/www.youtube.com\/watch?v=cUHPL_dk17E\n# https:\/\/www.youtube.com\/watch?v=RaswBvMnFxk\n\n\n\n\"\"\"\n@author: Sreenivas Bhattiprolu\n\"\"\"\nimport sklearn.model_selection\nimport tensorflow as tf\nimport os\nimport random\nimport numpy as np\nimport cv2\n \nfrom tqdm import tqdm \nimport datetime\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nimport PIL\nimport matplotlib.pyplot as plt\n\nfrom keras import backend as K\n\n#seed = 42\n#np.random.seed = seed\n\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNELS = 3\n\nLIMIAR = 0.5\n\nTRAIN_PATH = '\/kaggle\/temp\/train\/'\nTEST_PATH = '\/kaggle\/temp\/test\/'\n\n#iou_metric = sklearn.metrics.jaccard_score","90ab8f88":"\"\"\"\nHere is a dice loss for keras which is smoothed to approximate a linear (L1) loss.\nIt ranges from 1 to 0 (no error), and returns results similar to binary crossentropy\n\"\"\"\n\n\nsmooth = 1.\n\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n","60f7dfd8":"\ntrain_ids = next(os.walk(TRAIN_PATH))[1]\ntest_ids = next(os.walk(TEST_PATH))[1]\n\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n\nprint('Resizing training images and masks')\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   \n    path = TRAIN_PATH + id_\n    img = imread(path + '\/images\/' + id_ + '.png')[:,:,:IMG_CHANNELS]  \n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_train[n] = img  #Fill empty X_train with values from img\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n    for mask_file in next(os.walk(path + '\/masks\/'))[2]:\n        mask_ = imread(path + '\/masks\/' + mask_file)\n        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  \n                                      preserve_range=True), axis=-1)\n        mask = np.maximum(mask, mask_)  \n            \n    Y_train[n] = mask   \n\n\n\n","e5edb12c":"cv2.imwrite(\"\/kaggle\/working\/mask_1.png\", np.float32(Y_train[0])*255)\ncv2.imwrite(\"\/kaggle\/working\/mask_2.png\", np.float32(Y_train[1])*255)\nimg = imread(path + '\/images\/' + id_ + '.png')[:,:,:IMG_CHANNELS]  \npath = TRAIN_PATH + train_ids[0]\ncv2.imwrite(\"\/kaggle\/working\/img_1.png\",  imread(path + '\/images\/' + train_ids[0] + '.png')[:,:,:IMG_CHANNELS] )\npath = TRAIN_PATH + train_ids[1]\ncv2.imwrite(\"\/kaggle\/working\/img_2.png\",  imread(path + '\/images\/' + train_ids[1] + '.png')[:,:,:IMG_CHANNELS] )","05f1387b":"# test images\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nsizes_test = []\nprint('Resizing test images') \nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TEST_PATH + id_\n    img = cv2.imread(path + '\/images\/' + id_ + '.png', cv2.IMREAD_COLOR)[:,:,:IMG_CHANNELS]\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Done!')\n\n","60c13590":"x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X_train, Y_train, train_size = 0.8, shuffle = True)\n\nX_train = None\nY_train = None","ad56f42f":"len(test_ids)","20e3e511":"#learn.crit = MixedLoss(10.0, 2.0)\n#learn.metrics=[accuracy_thresh(0.5),dice,IoU]\n\n\n#Build the model\ninputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\ns = tf.keras.layers.Lambda(lambda x: x \/ 255)(inputs)\n\n#Contraction path\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\nc1 = tf.keras.layers.Dropout(0.1)(c1)\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\np1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\nc2 = tf.keras.layers.Dropout(0.1)(c2)\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\np2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n \nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\nc3 = tf.keras.layers.Dropout(0.2)(c3)\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\np3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n \nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\nc4 = tf.keras.layers.Dropout(0.2)(c4)\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\np4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n \nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\nc5 = tf.keras.layers.Dropout(0.3)(c5)\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n    #Expansive path \nu6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\nu6 = tf.keras.layers.concatenate([u6, c4])\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\nc6 = tf.keras.layers.Dropout(0.2)(c6)\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n \nu7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\nu7 = tf.keras.layers.concatenate([u7, c3])\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\nc7 = tf.keras.layers.Dropout(0.2)(c7)\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n \nu8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\nu8 = tf.keras.layers.concatenate([u8, c2])\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\nc8 = tf.keras.layers.Dropout(0.1)(c8)\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n \nu9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\nu9 = tf.keras.layers.concatenate([u9, c1], axis=3)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\nc9 = tf.keras.layers.Dropout(0.1)(c9)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n \noutputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n \nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-5,\n    decay_steps=10000,\n    decay_rate=0.5)\nopt = tf.keras.optimizers.Adam(learning_rate=(10e-4))\niou_metric = tf.keras.metrics.MeanIoU(num_classes=2)\n#opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=2), dice_coef])\nmodel.compile(optimizer=opt, loss=dice_coef_loss, metrics=['accuracy', iou_metric , dice_coef])\n#model.summary()\n\n\n","d13c3417":"#model.load_weights(\"\/kaggle\/working\/model_dice.h5\")\ndef reset_weights(model):\n    session = K.get_session()\n    for layer in model.layers: \n        if hasattr(layer, 'kernel_initializer'):\n            layer.kernel.initializer.run(session=session)","6b9d5198":"\n\n################################\n#Modelcheckpoint\ncheckpointer = tf.keras.callbacks.ModelCheckpoint('model_for_nuclei.h5', verbose=1, save_best_only=True)\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='val_loss'),\n        tf.keras.callbacks.TensorBoard(log_dir='logs\/body'),\n        \n]\n\nresults = model.fit(x_train, y_train, validation_split=0.1, batch_size=16, epochs=3, callbacks=callbacks)\n#while float(results.history['loss'][-1]) > -0.6:\n#    reset_weights(model)\n#    model = build()\n#    results = model.fit(x_train, y_train, validation_split=0.1, batch_size=16, epochs=3, callbacks=callbacks)\n####################################","235374c2":"model.save_weights(\"\/kaggle\/working\/model_dice.h5\")","2e1d250f":"\n\nidx = random.randint(0, len(x_train))\n\n\npreds_train = model.predict(x_train[:int(x_train.shape[0]*0.9)], verbose=1)\npreds_val = model.predict(x_train[int(x_train.shape[0]*0.9):], verbose=1)\npreds_test = model.predict(x_test, verbose=1)\n\n \npreds_train_t = (preds_train > 0.5).astype(np.uint8)\npreds_val_t = (preds_val > 0.5).astype(np.uint8)\npreds_test_t = (preds_test > 0.5).astype(np.uint8)\n\n\n# Perform a sanity check on some random training samples\nix = random.randint(0, len(preds_train_t))\nimshow(x_train[ix])\nplt.show()\nimshow(np.squeeze(y_train[ix]))\nplt.show()\nimshow(np.squeeze(preds_train_t[ix]))\nplt.show()\n\n\n# Perform a sanity check on some random validation samples\nix = random.randint(0, len(preds_val_t))\n#imshow(x_train[int(x_train.shape[0]*0.9):][ix])\nplt.show()\nimshow(np.squeeze(y_train[int(y_train.shape[0]*0.9):][ix]))\nplt.show()\nimshow(np.squeeze(preds_val_t[ix]))\nplt.show()","54e09186":"#model.save_weights(\"\/kaggle\/working\/model_dice.h5\")","97e10f74":"def build_model():\n    #Build the model\n    inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    s = tf.keras.layers.Lambda(lambda x: x \/ 255)(inputs)\n\n    #Contraction path\n    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n    c1 = tf.keras.layers.Dropout(0.1)(c1)\n    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n\n    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n    c2 = tf.keras.layers.Dropout(0.1)(c2)\n    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n \n    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n    c3 = tf.keras.layers.Dropout(0.2)(c3)\n    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n \n    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n    c4 = tf.keras.layers.Dropout(0.2)(c4)\n    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n \n    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n    c5 = tf.keras.layers.Dropout(0.3)(c5)\n    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n    #Expansive path \n    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = tf.keras.layers.concatenate([u6, c4])\n    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n    c6 = tf.keras.layers.Dropout(0.2)(c6)\n    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n \n    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = tf.keras.layers.concatenate([u7, c3])\n    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n    c7 = tf.keras.layers.Dropout(0.2)(c7)\n    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n \n    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = tf.keras.layers.concatenate([u8, c2])\n    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n    c8 = tf.keras.layers.Dropout(0.1)(c8)\n    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n \n    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n    c9 = tf.keras.layers.Dropout(0.1)(c9)\n    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n \n    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n \n    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n    loss = \"dice\"\n    if loss == \"dice\":\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 10e-4), loss=dice_coef_loss, metrics=['accuracy',iou_metric, dice_coef])\n    else:\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 10e-4), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy',iou_metric, dice_coef])\n    return model","c5a54a30":"datagen_1 = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range = 60,fill_mode = \"wrap\")\ndatagen_2 = tf.keras.preprocessing.image.ImageDataGenerator(brightness_range=[0.2,1.0], validation_split = 0.2, fill_mode = \"wrap\")\ndatagen_2_masks = tf.keras.preprocessing.image.ImageDataGenerator(validation_split = 0.2, fill_mode = \"wrap\")\ndatagen_3 = tf.keras.preprocessing.image.ImageDataGenerator(zoom_range = [0.5, 1.5],fill_mode = \"wrap\")\ndatagen_4 = tf.keras.preprocessing.image.ImageDataGenerator(vertical_flip = True,fill_mode = \"wrap\")\ndatagen_5 = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip = True, fill_mode = \"wrap\")\ndatagen_6 = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range = 60, zoom_range = [0.5, 1.5], fill_mode = \"wrap\")\ndatagen_7 = tf.keras.preprocessing.image.ImageDataGenerator(vertical_flip = True, horizontal_flip = True, rotation_range = 60, fill_mode = \"wrap\")\n\n\n#for batch in datagen.flow(x, batch_size=1,seed=1337 ):\n\n \n#data_1 = datagen_1.flow(x_train, seed=1337)\n#masks_1 = datagen_1.flow(y_train, seed=1337)\n\n#data_2 = datagen_2.flow(x_train, seed=1337)\n#masks_2 = datagen_2.flow(y_train, seed=1337)\n\n#data_3 = datagen_3.flow(x_train, seed=1337)\n#masks_3 = datagen_3.flow(y_train, seed=1337)\n","0115e49d":"logdir = os.path.join(\"logs\/head_1\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_1 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\nlogdir = os.path.join(\"logs\/head_2\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_2 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\nlogdir = os.path.join(\"logs\/head_3\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_3 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\nlogdir = os.path.join(\"logs\/head_4\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_4 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\nlogdir = os.path.join(\"logs\/head_5\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_5 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\nlogdir = os.path.join(\"logs\/head_6\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_6 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\n\nlogdir = os.path.join(\"logs\/head_7\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_7 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\n\nlogdir = os.path.join(\"logs\/head_8\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_8 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\n\nlogdir = os.path.join(\"logs\/head_9\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_9 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\n\nlogdir = os.path.join(\"logs\/head_10\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_10 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]\n\nlogdir = os.path.join(\"logs\/head_11\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  \ncallbacks_11 = [\n        tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss'),\n        \n        tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n]","7a147e5d":"model_1 = build_model()\nmodel_2 = build_model()\nmodel_3 = build_model()\nmodel_4 = build_model()\nmodel_5 = build_model()\nmodel_6 = build_model()\nmodel_7 = build_model()\nmodel_8 = build_model()\nmodel_9 = build_model()\nmodel_10 = build_model()\nmodel_11 = build_model()\n\nmodel_1.load_weights(\"\/kaggle\/working\/model_dice.h5\")\nmodel_2.load_weights(\"\/kaggle\/working\/model_dice.h5\")\nmodel_3.load_weights(\"\/kaggle\/working\/model_dice.h5\")\nmodel_4.load_weights(\"\/kaggle\/working\/model_dice.h5\")\nmodel_5.load_weights(\"\/kaggle\/working\/model_dice.h5\")\nmodel_6.load_weights(\"\/kaggle\/working\/model_dice.h5\")\nmodel_7.load_weights(\"\/kaggle\/working\/model_dice.h5\")\nmodel_8.load_weights(\"\/kaggle\/working\/model_dice.h5\")\nmodel_9.load_weights(\"\/kaggle\/working\/model_dice.h5\")\n\n    \nEPOCHS = 10\n#for batch in datagen_1.flow(x_train, batch_size=1,seed=1337 ):\n#    data_1.append(batch)\n    \n#for batch in datagen_1.flow(y_train, batch_size=1,seed=1337 ):\n#    masks_1.append(batch)\n\n \ndatagen_img = datagen_1.flow(x_train, seed = 1337)\ndatagen_masks = datagen_1.flow(y_train, seed = 1337)\ndatagen = zip(datagen_img, datagen_masks)\nresults_1 = model_1.fit(datagen, batch_size=32, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_1)\ndatagen_img = datagen_4.flow(x_train, seed = 1337)\ndatagen_masks = datagen_4.flow(y_train, seed = 1337)\ndatagen = zip(datagen_img, datagen_masks)\nresults_4 = model_4.fit(datagen, batch_size=32, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_4)\n\n\n\n","7132de18":"datagen_img = datagen_2.flow(x_train, seed = 1337)\ndatagen_masks = datagen_2_masks.flow(y_train, seed = 1337)\ndatagen = zip(datagen_img, datagen_masks)\nresults_2 = model_2.fit(datagen, batch_size=16, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_2)\ndatagen_img = datagen_5.flow(x_train, seed = 1337)\ndatagen_masks = datagen_5.flow(y_train, seed = 1337)\ndatagen = zip(datagen_img, datagen_masks)\nresults_5 = model_5.fit(datagen, batch_size=16, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_5)\n","b3917216":"\ndatagen_img = datagen_3.flow(x_train, seed = 1337)\ndatagen_masks = datagen_3.flow(y_train, seed = 1337)\ndatagen = zip(datagen_img, datagen_masks)\nresults_3 = model_3.fit(datagen, batch_size=32, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_3)\nresults_6 = model_6.fit(datagen, batch_size=32, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_6)\nresults_7 = model_7.fit(datagen, batch_size=32, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks = callbacks_7)\n","44a8d83b":"datagen_img = datagen_6.flow(x_train, seed = 1337)\ndatagen_masks = datagen_6.flow(y_train, seed = 1337)\ndatagen = zip(datagen_img, datagen_masks)\nresults_8 = model_8.fit(datagen, batch_size=16, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_8)\nresults_9 = model_9.fit(datagen, batch_size=16, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_9)","3f6a0d6e":"datagen_img = datagen_7.flow(x_train, seed = 1337)\ndatagen_masks = datagen_7.flow(y_train, seed = 1337)\ndatagen = zip(datagen_img, datagen_masks)\nresults_8 = model_10.fit(datagen, batch_size=16, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_10)\nresults_9 = model_11.fit(datagen, batch_size=16, steps_per_epoch = len(x_train)\/16, epochs=EPOCHS, callbacks=callbacks_11)","f2bf1f0c":"#model_1.predict(x_test)","b7a6fc98":"models = [ model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10, model_11]\n         \n#predictions = []\npredictions = (model_5.predict(x_test)> LIMIAR).astype(int)\nx=2\nfor model in models:\n    x+=1\n    predictions += (model.predict(x_test)>LIMIAR).astype(int)\n    model.save_weights(\"model_\"+str(x)+\".h5\")\n\npredictions_new = (predictions >= 5).astype(int)\nprint(predictions_new)\nmodels.append(model_5)","20a18af7":"corpo_predict = model.predict(x_test)\n\n# Perform a sanity check on some random training samples\nix = random.randint(0, len(corpo_predict))\nimshow(x_test[ix])\nplt.show()\nimshow(np.squeeze(y_test[ix]))\nplt.show()\nimshow(np.squeeze(predictions_new[ix]))\nplt.show()\n","b75908ac":"imshow(np.squeeze(corpo_predict[ix]))\nplt.show()","69cc3972":"#model_1.metrics_names","59cce74f":"results = []\n\nhydra_results = []\nacc = tf.keras.metrics.Accuracy()\niou = 0\ndice = 0\nx = -1\nfor image in predictions_new > 0.5:\n    x+= 1\n    if(x == 134):\n        x = 133\n    acc.update_state( y_test[x], image )\n    iou += iou_metric(y_test[x], image )\n    dice += dice_coef(tf.dtypes.cast(y_test[x], tf.float32), tf.dtypes.cast(image, tf.float32) )\n    \nx+=1\niou = iou\/x\ndice = dice\/x\n\n\nprint( str(iou) + \" \" + str(dice) + \" \" + str(acc.result().numpy()))\n#['loss', 'accuracy', 'mean_io_u', 'dice_coef']\nhydra_results.append(-dice)\nhydra_results.append(acc.result().numpy())\nhydra_results.append(iou)\nhydra_results.append(dice)\nresults.append(hydra_results)\nhydra_results = None\niou = None\ndice = None\nacc = None\nfor model in models:\n    results.append(model.evaluate(x_test, y_test, batch_size = 16))\n\nmodelos = []\naccuracy = []\niou = []\ndice = []\nx=0\nfor result in results:\n    if x == 0:\n        modelos.append(\"Hydra\")\n    else:\n        modelos.append(\"Cabe\u00e7a_\" + str(x))\n    accuracy.append(result[1])\n    iou.append(result[2])\n    dice.append(result[3])\n    x+=1\ncorpo_result = model.evaluate(x_test, y_test, batch_size = 16)\nmodelos.append(\"Corpo\")\naccuracy.append(corpo_result[1])\niou.append(corpo_result[2])\ndice.append(corpo_result[3])\nprint(dice)\ncorpo_result = None\n## Create results DataFrame\nsub = pd.DataFrame()\nsub['Models'] = modelos\nsub['Accuracy'] = accuracy\nsub['IoU'] = iou\nsub['Dice Coefficient'] = dice\nsub.to_csv('results.csv', index=False)","635be4bf":"#x_test = None\n#y_test = None\n#datagen = None\npredictions_h_t = predictions_new\n#predictions_bool = ensemble_model.predict(X_test)\n\n\n# Perform a sanity check on some random training samples\nix = random.randint(0, len(predictions_h_t))\nimshow(x_test[ix])\nplt.show()\n#imshow(np.squeeze(Y_train[ix]))\n#plt.show()\nimshow(np.squeeze(predictions_h_t[ix]))\nplt.show()\n\n\n","b13b0ea5":"\nx=0\nfor model in models:\n    x+=1\n    if x == 1:\n        predictions = model.predict(X_test)\n    else:\n        predictions += (model.predict(X_test))\n    model.save_weights(\"model_\"+str(x)+\".h5\")\n\n#predictions = predictions - model_5_predict\npredictions_new = predictions \/ len(models)","1955a212":"x_test = None\nx_train = None\ny_test = None\ny_train = None\npreds_test = predictions_new\n# Create list of upsampled test masks\npreds_test_upsampled = []\nfor i in range(len(preds_test)):\n    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n                                       (sizes_test[i][0], sizes_test[i][1]), \n                                       mode='constant', preserve_range=True))","4964ea44":"# Run length Encoding from https:\/\/www.kaggle.com\/rakhlin\/fast-run-length-encoding-python\n\nfrom skimage.morphology import label\n\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef prob_to_rles(x, cutoff=0.5):\n    lab_img = label(x > cutoff)\n    for i in range(1, lab_img.max() + 1):\n        yield rle_encoding(lab_img == i)","df59c2b4":"new_test_ids = []\nrles = []\nfor n, id_ in enumerate(test_ids):\n    rle = list(prob_to_rles(preds_test_upsampled[n]))\n    rles.extend(rle)\n    new_test_ids.extend([id_] * len(rle))","62636907":"submission_df = pd.DataFrame()\nsubmission_df['ImageId'] = new_test_ids\nsubmission_df['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsubmission_df.to_csv('sub-dsbowl2018-1.csv', index=False)\n","83ef95da":"# Run-length encoding stolen from https:\/\/www.kaggle.com\/rakhlin\/fast-run-length-encoding-python\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef prob_to_rles(x, cutoff=0.5):\n    lab_img = label(x > cutoff)\n    for i in range(1, lab_img.max() + 1):\n        yield rle_encoding(lab_img == i)","fc9c4d20":"len(test_ids)","b36cb2fb":"#new_test_ids = []\n#rles = []\n#for n, id_ in enumerate(test_ids):\n#    rle = list(prob_to_rles(preds_test_upsampled[n]))\n#    rles.extend(rle)\n#    new_test_ids.extend([id_] * len(rle))\n    ","bd0e3bc8":"len(new_test_ids)","d7d6929c":"# Create submission DataFrame#\n#sub = pd.DataFrame()\n#sub['ImageId'] = new_test_ids\n#sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n#sub.to_csv('sub-dsbowl2018-1.csv', index=False)\n","6bfcc2e6":"new_test_ids = None\nrles = None\nsub = None\npreds_test = None\npredictions_h_t = None","27e8060e":"#x = 0\n#models.append(model)\n#for model in models:\n#    if results[x][1] < 0.85:\n#        continue\n#    x+=1\n#    preds_test = model.predict(X_test)\n#    # Create list of upsampled test masks\n#    preds_test_upsampled = []\n#    for i in range(len(preds_test)):\n#        preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n#                                       (sizes_test[i][0], sizes_test[i][1]), \n#                                       mode='constant', preserve_range=True))##\n\n#    new_test_ids = []\n#    rles = []\n#    for n, id_ in enumerate(test_ids):\n#        rle = list(prob_to_rles(preds_test_upsampled[n]))\n#        rles.extend(rle)\n#        new_test_ids.extend([id_] * len(rle))\n#    \n#    # Create submission DataFrame\n#    sub = pd.DataFrame()\n#    sub['ImageId'] = new_test_ids\n#    sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n#    sub.to_csv('sub-dsbowl2018-1-'+str(x)+'.csv', index=False)\n#    \n#    new_test_ids = None\n#    rles = None\n#    sub = None\n#    preds_test = None\n#    preds_test_upsampled = None","8c8e1fc6":"#print(results[0])\n#print(model_1.metrics_names)","c94d60a5":"'''\n#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=2), dice_coef])\nEpoch 1\/25\n38\/38 [==============================] - 63s 2s\/step - loss: 0.5298 - accuracy: 0.8059 - mean_io_u: 0.3997 - val_loss: 0.3652 - val_accuracy: 0.8350 - val_mean_io_u: 0.4006\nEpoch 2\/25\n38\/38 [==============================] - 61s 2s\/step - loss: 0.2833 - accuracy: 0.8623 - mean_io_u: 0.3997 - val_loss: 0.2074 - val_accuracy: 0.9155 - val_mean_io_u: 0.4006\nEpoch 3\/25\n38\/38 [==============================] - 61s 2s\/step - loss: 0.1629 - accuracy: 0.9381 - mean_io_u: 0.3997 - val_loss: 0.1595 - val_accuracy: 0.9369 - val_mean_io_u: 0.4006\nEpoch 4\/25\n38\/38 [==============================] - 61s 2s\/step - loss: 0.1301 - accuracy: 0.9502 - mean_io_u: 0.3998 - val_loss: 0.1490 - val_accuracy: 0.9443 - val_mean_io_u: 0.4012\nEpoch 5\/25\n38\/38 [==============================] - 61s 2s\/step - loss: 0.1205 - accuracy: 0.9540 - mean_io_u: 0.4001 - val_loss: 0.1683 - val_accuracy: 0.9368 - val_mean_io_u: 0.4016\nEpoch 6\/25\n38\/38 [==============================] - 62s 2s\/step - loss: 0.1093 - accuracy: 0.9587 - mean_io_u: 0.4000 - val_loss: 0.1280 - val_accuracy: 0.9518 - val_mean_io_u: 0.4006\nEpoch 7\/25\n38\/38 [==============================] - 61s 2s\/step - loss: 0.1044 - accuracy: 0.9607 - mean_io_u: 0.3999 - val_loss: 0.1208 - val_accuracy: 0.9552 - val_mean_io_u: 0.4007\nEpoch 8\/25\n38\/38 [==============================] - 61s 2s\/step - loss: 0.1052 - accuracy: 0.9602 - mean_io_u: 0.3998 - val_loss: 0.1372 - val_accuracy: 0.9496 - val_mean_io_u: 0.4010\nEpoch 9\/25\n38\/38 [==============================] - 61s 2s\/step - loss: 0.1028 - accuracy: 0.9608 - mean_io_u: 0.4000 - val_loss: 0.1175 - val_accuracy: 0.9585 - val_mean_io_u: 0.4009\nEpoch 10\/25\n38\/38 [==============================] - 63s 2s\/step - loss: 0.0956 - accuracy: 0.9633 - mean_io_u: 0.3999 - val_loss: 0.1253 - val_accuracy: 0.9551 - val_mean_io_u: 0.4008\nEpoch 11\/25\n38\/38 [==============================] - 61s 2s\/step - loss: 0.0946 - accuracy: 0.9638 - mean_io_u: 0.3998 - val_loss: 0.1179 - val_accuracy: 0.9590 - val_mean_io_u: 0.4007'''","11c83bd3":"model.compile(optimizer='adam', loss=dice_coef_loss, metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=2), dice_coef])\nEpoch 1\/25\n38\/38 [==============================] - 62s 2s\/step - loss: -0.3905 - accuracy: 0.3727 - mean_io_u_1: 0.4026 - dice_coef: 0.3923 - val_loss: -0.5665 - val_accuracy: 0.7097 - val_mean_io_u_1: 0.5823 - val_dice_coef: 0.5638\nEpoch 2\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.6546 - accuracy: 0.8006 - mean_io_u_1: 0.6161 - dice_coef: 0.6545 - val_loss: -0.6745 - val_accuracy: 0.8156 - val_mean_io_u_1: 0.6253 - val_dice_coef: 0.6971\nEpoch 3\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.6894 - accuracy: 0.8232 - mean_io_u_1: 0.6306 - dice_coef: 0.6885 - val_loss: -0.6860 - val_accuracy: 0.8236 - val_mean_io_u_1: 0.6241 - val_dice_coef: 0.7124\nEpoch 4\/25\n38\/38 [==============================] - 60s 2s\/step - loss: -0.6971 - accuracy: 0.8283 - mean_io_u_1: 0.6326 - dice_coef: 0.6965 - val_loss: -0.6866 - val_accuracy: 0.8216 - val_mean_io_u_1: 0.6336 - val_dice_coef: 0.7092\nEpoch 5\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.6999 - accuracy: 0.8320 - mean_io_u_1: 0.6362 - dice_coef: 0.6987 - val_loss: -0.6934 - val_accuracy: 0.8269 - val_mean_io_u_1: 0.6283 - val_dice_coef: 0.7200\nEpoch 6\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.7024 - accuracy: 0.8318 - mean_io_u_1: 0.6338 - dice_coef: 0.7024 - val_loss: -0.6987 - val_accuracy: 0.8337 - val_mean_io_u_1: 0.6262 - val_dice_coef: 0.7290\nEpoch 7\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.7019 - accuracy: 0.8368 - mean_io_u_1: 0.6418 - dice_coef: 0.7010 - val_loss: -0.7090 - val_accuracy: 0.8787 - val_mean_io_u_1: 0.6576 - val_dice_coef: 0.7246\nEpoch 8\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.8265 - accuracy: 0.9308 - mean_io_u_1: 0.7406 - dice_coef: 0.8270 - val_loss: -0.8167 - val_accuracy: 0.9244 - val_mean_io_u_1: 0.7633 - val_dice_coef: 0.8225\nEpoch 9\/25\n38\/38 [==============================] - 60s 2s\/step - loss: -0.8744 - accuracy: 0.9494 - mean_io_u_1: 0.8059 - dice_coef: 0.8747 - val_loss: -0.8628 - val_accuracy: 0.9459 - val_mean_io_u_1: 0.7949 - val_dice_coef: 0.8658\nEpoch 10\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.8913 - accuracy: 0.9567 - mean_io_u_1: 0.8273 - dice_coef: 0.8913 - val_loss: -0.8707 - val_accuracy: 0.9500 - val_mean_io_u_1: 0.7842 - val_dice_coef: 0.8732\nEpoch 11\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.8966 - accuracy: 0.9591 - mean_io_u_1: 0.8247 - dice_coef: 0.8965 - val_loss: -0.8759 - val_accuracy: 0.9497 - val_mean_io_u_1: 0.8117 - val_dice_coef: 0.8767\nEpoch 12\/25\n38\/38 [==============================] - 62s 2s\/step - loss: -0.8978 - accuracy: 0.9592 - mean_io_u_1: 0.8308 - dice_coef: 0.8979 - val_loss: -0.8798 - val_accuracy: 0.9518 - val_mean_io_u_1: 0.8102 - val_dice_coef: 0.8805\nEpoch 13\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.8940 - accuracy: 0.9581 - mean_io_u_1: 0.8296 - dice_coef: 0.8938 - val_loss: -0.8787 - val_accuracy: 0.9518 - val_mean_io_u_1: 0.7983 - val_dice_coef: 0.8821\nEpoch 14\/25\n38\/38 [==============================] - 61s 2s\/step - loss: -0.9022 - accuracy: 0.9610 - mean_io_u_1: 0.8381 - dice_coef: 0.9019 - val_loss: -0.8682 - val_accuracy: 0.9501 - val_mean_io_u_1: 0.7585 - val_dice_coef: 0.8723"}}