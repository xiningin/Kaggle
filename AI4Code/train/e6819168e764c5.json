{"cell_type":{"e5acd265":"code","8d9edbcb":"code","035e973f":"code","99771a55":"code","cea0a8c8":"code","67322af2":"code","14cb281e":"code","013e3800":"code","b8964129":"code","0721d27a":"code","93dbe80e":"code","960c66eb":"code","1c124309":"code","ac219ee6":"code","593501c0":"code","8b0f61e9":"code","4086f456":"markdown","94e61b94":"markdown","f022cff5":"markdown","75fe507f":"markdown","c8a18ee2":"markdown","a4b88a11":"markdown","9e4c3f66":"markdown","e7bbc46a":"markdown","49dde9f2":"markdown","4fa7a693":"markdown","85fe5ccd":"markdown","302d7ed2":"markdown","b4914990":"markdown"},"source":{"e5acd265":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Import HTML library for displaying results nicely\nfrom IPython.core.display import display, HTML\n# json reader\nimport json\n# tqdm progress meter\nfrom tqdm import tqdm\n\n# huggingface transformers\n!pip install transformers\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom transformers import BartTokenizer, BartForConditionalGeneration","8d9edbcb":"!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","035e973f":"!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch\n# import lucene index (thanks, anserini team!)\n!wget https:\/\/www.dropbox.com\/s\/d6v9fensyi7q3gb\/lucene-index-covid-2020-04-03.tar.gz\n!tar xvfz lucene-index-covid-2020-04-03.tar.gz","99771a55":"COVID_INDEX = 'lucene-index-covid-2020-04-03\/'\n\ndef show_query(query):\n    \"\"\"HTML print format for the searched query\"\"\"\n    return HTML('<br\/><div style=\"font-family: Times New Roman; font-size: 20px;'\n                'padding-bottom:12px\"><b>Query<\/b>: '+query+'<\/div>')\n\ndef show_document(idx, doc):\n    \"\"\"HTML print format for document fields\"\"\"\n    have_body_text = 'body_text' in json.loads(doc.raw)\n    body_text = ' Full text available.' if have_body_text else ''\n    return HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + \n               f'<b>Document {idx}:<\/b> {doc.docid} ({doc.score:1.2f}) -- ' +\n               f'{doc.lucene_document.get(\"authors\")} et al. ' +\n             # f'{doc.lucene_document.get(\"journal\")}. ' +\n             # f'{doc.lucene_document.get(\"publish_time\")}. ' +\n               f'{doc.lucene_document.get(\"title\")}. ' +\n               f'<a href=\"https:\/\/doi.org\/{doc.lucene_document.get(\"doi\")}\">{doc.lucene_document.get(\"doi\")}<\/a>.'\n               + f'{body_text}<\/div>')\n\ndef show_query_results(query, searcher, top_k=10):\n    \"\"\"HTML print format for the searched query\"\"\"\n    hits = searcher.search(query)\n    display(show_query(query))\n    for i, hit in enumerate(hits[:top_k]):\n        display(show_document(i+1, hit))\n    return hits[:top_k] \n\nsearcher = pysearch.SimpleSearcher(COVID_INDEX)","cea0a8c8":"# Enter query here\nquery = ('COVID-19 incubation period in humans')\n\nhits = show_query_results(query, searcher, top_k=10)","67322af2":"model_version = 'distilbert-base-uncased'\ndo_lower_case = True\nmodel = DistilBertModel.from_pretrained(model_version)\ntokenizer = DistilBertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)","14cb281e":"def extract_distilbert(text, tokenizer, model):\n    # Convert text to IDs with special tokens specific to the distilBERT model\n    text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n    text_words = tokenizer.convert_ids_to_tokens(text_ids[0])[1:-1]\n\n    n_chunks = int(np.ceil(float(text_ids.size(1))\/510))\n    states = []\n    \n    for ci in range(n_chunks):\n        text_ids_ = text_ids[0, 1+ci*510:1+(ci+1)*510]            \n        text_ids_ = torch.cat([text_ids[0, 0].unsqueeze(0), text_ids_])\n        if text_ids[0, -1] != text_ids[0, -1]:\n            text_ids_ = torch.cat([text_ids_, text_ids[0,-1].unsqueeze(0)])\n        \n        with torch.no_grad():\n            state = model(text_ids_.unsqueeze(0))[0]\n            state = state[:, 1:-1, :]\n        states.append(state)\n\n    state = torch.cat(states, axis=1)\n    return text_ids, text_words, state[0]","013e3800":"query_ids, query_words, query_state = extract_distilbert(query, tokenizer, model)","b8964129":"my_fav_hit = 6\ndoc_json = json.loads(hits[my_fav_hit].raw)\n\nparagraph_states = []\nfor par in tqdm(doc_json['body_text']):\n    state = extract_distilbert(par['text'], tokenizer, model)\n    paragraph_states.append(state)","0721d27a":"# Compute similarity given the extracted states from sciBERT\ndef cross_match(state1, state2):\n    state1 = state1 \/ torch.sqrt((state1 ** 2).sum(1, keepdims=True))\n    state2 = state2 \/ torch.sqrt((state2 ** 2).sum(1, keepdims=True))\n    sim = (state1.unsqueeze(1) * state2.unsqueeze(0)).sum(-1)\n    return sim","93dbe80e":"# Compute similarity for each paragraph\nsim_matrices = []\nfor pid, par in tqdm(enumerate(doc_json['body_text'])):\n    sim_score = cross_match(query_state, paragraph_states[pid][-1])\n    sim_matrices.append(sim_score)","960c66eb":"paragraph_relevance = [torch.max(sim).item() for sim in sim_matrices]\n\n# Select the index of top 5 paragraphs with highest relevance\nrel_index = np.argsort(paragraph_relevance)[-5:][::-1]","1c124309":"def show_sections(section, text):\n    \"\"\"HTML print format for document subsections\"\"\"\n    return HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px; margin-left: 15px\">' + \n        f'<b>{section}<\/b> -- {text.replace(\" ##\",\"\")} <\/div>')\n\ndisplay(show_query(query))\ndisplay(show_document(my_fav_hit, hits[my_fav_hit]))\nfor ri in np.sort(rel_index):\n    display(show_sections(doc_json[\"body_text\"][ri]['section'], \" \".join(paragraph_states[ri][1])))","ac219ee6":"text = []\nfor ri in np.sort(rel_index):\n    text = text + paragraph_states[ri][1]","593501c0":"torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nsum_tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\nsum_model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\nsum_model.to(torch_device)\n# set to evaluation mode for speed and memory saving\nsum_model.eval()\n","8b0f61e9":"\narticle_input_ids = sum_tokenizer.batch_encode_plus([tokenizer.convert_tokens_to_string(text)], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n\nsummary_ids = sum_model.generate(article_input_ids,\n                             num_beams=4,\n                             length_penalty=2.0,\n                             max_length=1000,\n                             no_repeat_ngram_size=3)\n\nsummary_txt = sum_tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n\nHTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px; margin-left: 15px\">' + \n        f'<p><b>Query:<\/b> {query}<\/p> &nbsp; <p><b>Summary of results:<\/b> {summary_txt}<\/div><\/p>')","4086f456":"# Install Pyserini and download the Lucene index","94e61b94":"# Extract states from each paragraph in a specific hit","f022cff5":"# Build a cosine similarity matrix between the query and each paragraph in the hit","75fe507f":"# Summarize most relevant paragraphs with BART abstractive summarization","c8a18ee2":"# Import libraries","a4b88a11":"# Define functions for running queries and displaying results","9e4c3f66":"# Run query over Lucene index, display top hits","e7bbc46a":"# Extract states from the query","49dde9f2":"# Exploring the CORD dataset: mining insight from a mountain of literature\n\nThe speed at which COVID-19 has spread across the globe has presented a unique challenge for healthcare researchers and practitioners. The scientific community must act quickly to understand this virus and the pathology it causes so that we can develop treatments, preventative public health measures and, ultimately, a vaccine. Rapidly moving science, however, presents a new problem: a massive amount of literature produced in a short period of time such that no individual researcher can effectively digest it all with the speed required to quickly take the next step.\n\nTo help with this problem, I will attempt to build a literature mining assistant. My process for literature reading typically is as follows:\n\n1. Run a search for papers relevant to my topic of interest\n2. Read abstracts of search results and select most relevant papers for detailed reading\n3. Thoroughly read those papers to understand their conlcusions\n4. While reading, make note of references with seemingly useful further information\n5. Back to step 2 with references from step 4\n\nTo accelerate this process, I will use a combination of search tools and abstractive summarization.\n\nTo facilitate step 1, I will use the open-source search engine library [Pyserini](https:\/\/github.com\/castorini\/pyserini). Pyserini is a Python wrapper for [Anserini](http:\/\/github.com\/castorini\/Anserini), which uses the [Apache Lucene](https:\/\/lucene.apache.org\/) open source search engine framework to build and search indexes from large text databases. The Anserini team has already build an index of the CORD dataset, which I will use here (thanks, Anserini team!).\n\nTo accelerate steps 2 and 3, I will use an abstractive summarization approach. The researcher will choose a paper for summarization and the algorithm will then produce a summary of the most relevant information in the paper.\n\nThe release of the transformer model [BERT](http:\/\/arxiv.org\/abs\/1810.04805) has been lauded as the \"ImageNet moment\" of the NLP world, showing advances by almost every available metric over the previous state of the art. The model, or now set of models, is a large transformer pre-trained using a masking technique rather than typical left to right sequence learning, allowing it to learn both left to right and right to left sequence dependencies. Here I will apply the smaller, notebook-friendly distilBERT model to reduce each paragraph to a lower-dimensional vector and then find the most relevant paragraphs using [cosine similarity](https:\/\/towardsdatascience.com\/nlp-text-similarity-how-it-works-and-the-math-behind-it-a0fb90a05095). \n\nOne of the most exciting model architectures in modern machine learning are denoising autoencoders. The success of the autoencoder approach in predicting animal behavior given a set of neural recordings was what first inspired me to explore the world of machine learning and data science ([Pandarinath et al](https:\/\/www.biorxiv.org\/content\/10.1101\/152884v1)). The denoising autoencoder BART is a wonderful model for abstractive summarization, I will use it here to generate summaries of the relevant paragraphs from each search result.\n\n### Justification\nHere I have chosen to build a literature mining assistant. You might call it a human-majorly-in-the-loop algorithm. I have chosen to keep the researcher at the heart of the process as opposed to building a semantic understanding or question answering model. The power of the human mind is to make connections between loosely associated ideas, many of which will not be contained in the CORD dataset. No NLP model is yet capable of building the world-level model required to make these connections, although Yoshua Bengio's [world scope](https:\/\/medium.com\/syncedreview\/new-study-tracks-nlp-development-and-direction-through-a-world-scope-bf8023a6588f) project is aiming to do just that. If these efforts are successful the application of these models to scientific literature searches could significantly increase the efficiency of scientific research, or possibly synthesize a host of latent ideas already existing in the literature. For now, Biologists are still the best tool we have for real semantic understanding of biological literature.\n\n### Results\nThe tool works pretty well! The summaries are informative and sometimes have bits of useful unexpected information, like references to other papers or data. BERT and BART work wonderfully and the Huggingface Transformers package provides a great interface for them for Pytorch. The processing time is a bit long, but could easily be accelerated outside of the restrictions of the kaggle notebook environment. This tool could be used by researchers to increase the efficiency of their reading by quickly pulling out the real meat of the most relevant papers quickly.\n\n### Future directions:\n\nIf this were my job and I could spend more time on this project, I would love to accelerate steps 4 and 5. I would do this by detecting the references present in the relevant paragraphs, pulling full text from Pub Med, and repeating the BART\/BERT summarization process. It would be nice to rework the search to allow a PubMed search initially to expand the usefulness of this tool beyond COVID-19 research.\n\nCredit:\n\nMy work here is primarily for my own edification and I owe many thanks to the teams which have provided most of the libraries and code that have allowed me to quickly complete such a fun project. Thanks to the Anserini team for generating the Lucene index for the CORD dataset. Thanks to the authors of the BERT and BART papers and the [Huggingface transformers](https:\/\/huggingface.co\/transformers\/) team for creating a wonderful python interface for these models. Much credit to Kaggle user Dirk the Engineer for their much more thorough [notebook](https:\/\/www.kaggle.com\/dirktheeng\/anserini-bert-squad-for-semantic-corpus-search) using Anserini and BERT in a question-answering approach, which provided the primary inspiration for this work. For a much more serious attempt at completing the goals of this competition, be sure to check out their work. ","4fa7a693":"# Set up openJDK for running Pyserini","85fe5ccd":"# Function for extracting states from text using distilBERT","302d7ed2":"# Set up pre-trained distilBERT model","b4914990":"# Find indices of most relevant paragraphs"}}