{"cell_type":{"ec21f0b6":"code","49df1626":"code","f314a3d3":"code","473889d1":"code","ce32c03e":"code","2d01ea3f":"code","8c1b3d98":"code","54a937dc":"code","676f0037":"code","cf466658":"code","cabbaa36":"code","7be2883d":"code","b2e94bd0":"code","f825ebab":"code","58287f1d":"code","d7cb234e":"code","3a93ef72":"code","9b8e3f92":"code","0f1e24be":"code","1b90c8ab":"code","af7f609b":"code","9c2a1635":"code","f90abe36":"code","7923fa92":"code","689f159e":"code","06d42647":"code","b18c3444":"code","473fe3c6":"code","7b73a627":"code","ba7d5fd1":"code","4d21907f":"code","4261e2e4":"code","c7e44703":"code","08d5597d":"code","3525070b":"code","4fea4d7c":"code","4da3d528":"code","0f1e5c7e":"code","075e0a60":"code","e7fb8459":"code","45f6b5c0":"code","75aab8de":"code","6779d81c":"code","fcb5d344":"code","b629e5a6":"code","5f856c33":"code","5e0c0c7e":"code","41d355ad":"code","557861ff":"code","fe2cbee1":"code","c915aa88":"code","9f705db2":"code","06cbc9e0":"code","4428c1bf":"code","f6eb27ca":"code","ae4c7d6e":"code","7418779b":"code","11f792fa":"code","670ad098":"code","503635cd":"code","f14444cb":"code","233cc75a":"code","a2717328":"code","c4d6104c":"code","866c4960":"code","66450801":"code","afa9d9e9":"code","251299c6":"code","948dc126":"code","7a9119b4":"code","d3e31a19":"code","86ff24c8":"code","d9f9fbe1":"code","2accad0e":"code","ec45962a":"code","06ca6b7a":"code","86b88819":"code","152e7d4e":"code","9fb35b3f":"code","19ff53c5":"code","ce3947ce":"code","5415d1fa":"code","5d394486":"code","33005c75":"code","b0970cab":"code","aff50ad6":"code","80d995ed":"code","63d010d1":"code","d92adc3d":"code","3ad28b69":"code","648ec415":"code","ba6b1cb4":"code","8ff095e8":"markdown","d9d587d7":"markdown","65dbd75c":"markdown","7c6368d5":"markdown","d70e68c7":"markdown","c4b11855":"markdown","69368507":"markdown","641541d0":"markdown","b8facacc":"markdown","f2b18665":"markdown","6d66d2a2":"markdown","5ae114b7":"markdown","1c630835":"markdown","fa6a9704":"markdown","12aee193":"markdown","0a05ea39":"markdown","4e160214":"markdown","30120135":"markdown","4b6fdc94":"markdown","996a78f0":"markdown","a179bbcc":"markdown","7e7b4489":"markdown","edc2c4f2":"markdown","0009d66d":"markdown","1417bb9c":"markdown","764a2f4d":"markdown","a9bf352e":"markdown","687350ac":"markdown","7b5a9b2a":"markdown","e34f5ff6":"markdown","fd827ea2":"markdown","43351cfb":"markdown","3a4d55f2":"markdown","6faabda2":"markdown","5a7b261e":"markdown","5ff35d05":"markdown","763a8f6f":"markdown","f396170a":"markdown","ae7fa896":"markdown"},"source":{"ec21f0b6":"import pandas as pd\nimport numpy as np\nfrom statistics import mean\nimport matplotlib.pyplot as plt","49df1626":"df_read = pd.read_csv('..\/input\/mental-health-in-tech-field-eda-cleaning\/main_dummy.csv')\ndf_read = df_read.drop('Unnamed: 0', axis = 1)\ndf_read","f314a3d3":"#check col names\ndf_dummies = df_read.copy()\norig_col = list(df_dummies.columns.values)\nfor eachorig in orig_col:\n    print(eachorig)","473889d1":"print('original number of cols: ', len(orig_col))\n\n#remove the diagnosis\ndiag_list = ['mood disorder', 'anxiety disorder', 'attention deficit disorder', 'substance use disorder', 'other diagnose']\nfor eachdiag in diag_list:\n    orig_col.remove(eachdiag)\nprint('number of cols after removing diagnosis: ', len(orig_col))\n\n#remove other columns with 'If you have a mental health issue'\nremove_list = [\"If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?_Not applicable to me\", \n               \"If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?_Often\", \n               \"If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?_Rarely\", \n               \"If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?_Sometimes\", \n               \"If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Not applicable to me\", \n               \"If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Often\", \n               \"If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Rarely\", \n               \"If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Sometimes\", \n               \"Have you been diagnosed with a mental health condition by a medical professional?_Yes\", \n               'If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Not applicable to me', \n               'How willing would you be to share with friends and family that you have a mental illness?_Not applicable to me (I do not have a mental illness)']\n# remove_list = ['If yes, what condition(s) have you been diagnosed with?',\n#                 'If maybe, what condition(s) do you believe you have?', \n#                 'If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?_Not applicable to me', \n#                 'If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?_Often', \n#                 'If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?_Rarely', \n#                 'If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?_Sometimes', \n#                 'If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Not applicable to me', \n#                 'If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Often', \n#                 'If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Rarely', \n#                 'If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?_Sometimes']\n# for eachcol in orig_col:\n#     if eachcol.find('If you')>-1:\n#         print(eachcol)\n#         orig_col.remove(eachcol)\nfor eachif in remove_list:\n    if eachif in orig_col:\n        orig_col.remove(eachif)\nprint('number of cols after removing if have mental issue: ', len(orig_col))","ce32c03e":"#create dataframe with the rest of the cols for modeling\ndf_dummies = df_dummies[orig_col]","2d01ea3f":"from sklearn.preprocessing import StandardScaler","8c1b3d98":"ss = StandardScaler()\n# df_dummies['age'] = ss.fit_transform(np.array(df_dummies['age']).reshape(-1, 1))","54a937dc":"from sklearn.model_selection import train_test_split","676f0037":"cols = df_dummies.columns.values\nycol = 'Do you currently have a mental health disorder?'\n# not_x = ['If yes, what condition(s) have you been diagnosed with?',\n#          'If maybe, what condition(s) do you believe you have?', \n#          'Do you currently have a mental health disorder?']\nxcol = list()\nfor each in cols:\n    if each != ycol:\n        xcol.append(each)\ny = df_dummies[ycol]\nx = df_dummies[xcol]\ncol_test1 = x.columns.values\nx_whiten = ss.fit_transform(x)\nx = pd.DataFrame(x_whiten, columns = xcol)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)","cf466658":"x.shape","cabbaa36":"n_fold = 5\nx_folds = np.array_split(x_train, n_fold) #split training data into n_fold proportions\ny_folds = np.array_split(y_train, n_fold) #split training data into n_fold proportions\n\ncv_folds = list() #stores dataframes for each fold\nfor eachfold in range(n_fold): #for each fold\n    train_number = list(np.arange(0, n_fold)) #a list of fold numbers\n    train_number.pop(eachfold) #pop current fold number\n    df_y_ts = y_folds[eachfold] #use current fold number as testing fold, create testing y\n    df_x_ts = x_folds[eachfold] #use current fold number as testing fold, create testing x\n    \n    y_train_list = list() #stores all the df y from folds for training\n    x_train_list = list() #stores all the df x from folds for training\n    for eachnumber in train_number: #for each training fold number\n        x_train_list.append(x_folds[eachnumber]) #append the df in training fold number for x\n        y_train_list.append(y_folds[eachnumber]) #append the df in training fold number for y\n    df_x_tr = pd.concat(x_train_list) #combine all the training dfs for x into 1\n    df_y_tr = pd.concat(y_train_list) #combine all the training dfs for y into 1\n    cv_folds.append([df_x_tr, df_y_tr, df_x_ts, df_y_ts]) #append training and testing dataframe for current fold\n    \n    \n# x_trtr, x_trts, y_trtr, y_trts = train_test_split(x_train, y_train, test_size = 0.2, random_state = 0)\n# x_tr_folds = np.array_split(x_trtr, n_fold)\n# y_tr_folds = np.array_split(y_trtr, n_fold)\n# x_ts_folds = np.array_split(x_trts, n_fold)\n# y_ts_folds = np.array_split(y_trts, n_fold)","7be2883d":"from sklearn.feature_selection import chi2, SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\nimport scipy.stats","b2e94bd0":"# #chi test and p value test\n# chi, pval = chi2(x, y)\n# df_chi_p = pd.DataFrame({'Feature':xcol, 'p value':pval, 'chi2':chi})\n# # print(df_pval[df_pval['p value']<0.0001])\n# low_p = df_chi_p.loc[df_chi_p['p value']<0.0001, 'Feature']","f825ebab":"# #sort by chi2 values\n# sort_chi = df_chi_p.sort_values(by = 'chi2')\n# sort_chi","58287f1d":"# #sort by p\n# sort_p = df_chi_p.sort_values(by = 'p value', ascending = False)\n# print(list(sort_p.index) == list(sort_chi.index))\n# #sorting by p or chi gives the same result","d7cb234e":"# #plot scatter plot of all chi2 values\n# plt.scatter(sort_chi.index, sort_chi['chi2'])\n# plt.xlabel('feature index')\n# plt.ylabel('chi2 value')\n# plt.title('Chi2 vs Feature')\n# plt.show()\n# ##there are features that have chi2 values much higher than other ones","3a93ef72":"# #check which features have high chi2 values\n# high_chi = list(df_chi_p.loc[df_chi_p['chi2']> 50, 'Feature'])\n# higher_chi = list(df_chi_p.loc[df_chi_p['chi2']> 100, 'Feature'])\n# print('chi2 above 100:')\n# for eachhigher in higher_chi:\n#     print(eachhigher)\n# print('\\nfeatures in high but not in higher: ')\n# for eachhigh in high_chi:\n#     if eachhigh not in higher_chi:\n#         print(eachhigh)","9b8e3f92":"# for each in df_dummies.columns:\n#     if each.find(\n#         'Have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?') > -1:\n#         print(each)","0f1e24be":"# df_chi_p[df_chi_p['chi2']> 50]","1b90c8ab":"# #personality disorder not in the high range\n# df_chi_p[df_chi_p['Feature'] == 'other diagnose']","af7f609b":"# #critical value\n# critical = scipy.stats.chi2.ppf(1-.00005, df = (x.shape[1]-1)*(x.shape[0]-1))\n# # plt.scatter(sort_chip)","9c2a1635":"# #use chi test to select features\n# print(x.shape)\n# x_new = SelectKBest(chi2, 30).fit_transform(x, y)\n# # new_low_p = x_new.columns\n# print(x_new)","f90abe36":"# #calculate chi2 and p between each col and other cols\n# x_chi2 = list() #store lists of chi2\n# x_p = list() #store lists of p values\n# col_outlier = list() #store lists of columns that are correlated\n\n# for eachxcol in range(len(xcol)): #for each col\n    \n#     #get chi2 and p values\n#     other_col = list(x.columns.values) #get all the column names\n#     other_col.pop(eachxcol) #pop the current col name\n#     current_col = xcol[eachxcol] #let the current col name be the col to compare with other cols\n#     x_current = np.array(x[current_col]).reshape(x.shape[0], 1) #reshape current column from (x.shape[0], ) to (x.shape[0], 1) for chi2 function\n#     chi_x, p_x = chi2(x[other_col], x_current) #calculate chi2 between other cols and current col\n#     x_chi2.append(chi_x) #append chi2 results\n#     x_p.append(p_x) #append p results \n    \n#     #compare with critical value\n#     current_crit = scipy.stats.chi2.ppf(1-.00005, df = (len(other_col)-1)*(x.shape[0]-1)) #calculate critical value\n#     for eachchi in chi_x: #for each chi2 value\n#         if current_crit < eachchi: #if chi2 value smaller than critical value\n#             print('There is a significant relation between ', x_current, 'and other cols') # hypothesis rejected, there is significant relation\n\n#     #investigate chi2 outliers \n# #     z = list(abs(scipy.stats.zscore(chi_x))) #get absolute values of the z scores\n# # #     print('\\ncurrent column is: ', current_col)\n# #     current_outlier = list()\n# #     for eachz in z: #for each z score of current column\n# #         if eachz>3: #if z score is greater than 3, i.e. an outlier\n# #             outlier = other_col[z.index(eachz)] #find feature name of outlier\n# #             question_current = current_col.find('_')\n# #             question_outlier = outlier.find('_')\n# #             if question_current>-1:\n# #                 question_current = current_col[:question_current]\n# #                 if question_outlier>-1:\n# #                     question_outlier = outlier[:question_outlier]\n# #                     if question_outlier != question_current:\n# #                         current_outlier.append(outlier)\n# # #                         print('\\ncurrent column is: ', current_col)\n\n# # #                         print(outlier, eachz)\n# #     col_outlier.append(current_outlier)\n# #     col_outlier = other_col[z[z>3].index()]\n# #     print('outlier for feature ', current_col, ': ', col_outlier)\n# #     if eachxcol < 5:\n# #         plt.scatter(range(len(other_col)), chi_x)\n# #         plt.xlabel('other columns')\n# #         plt.ylabel('chi2 values')\n# #         plt.title('chi2 values between '+current_col+' and other cols')\n# #         plt.show()","7923fa92":"from sklearn.decomposition import PCA","689f159e":"#calculate variance explained\nvar_exp_list = list() #store variance explained for n features\/eigenvalues\nfor ncomp in range(1, x.shape[1]+1): #for each number of features\/eigenvalues\n    pca_comp = PCA(n_components = ncomp) #set the number of components wanted for pca\n    pca_comp.fit(x)\n    var_exp = sum(pca_comp.explained_variance_ratio_) #sum the variance explained of each component to get total variance explained\n    var_exp_list.append(var_exp) #append total variance explained in list\n    \n#plot variance explained\nplt.scatter(range(x.shape[1]), var_exp_list) #plot variance explained against number of features\nplt.xlabel('number of eigenvalues\/features')\nplt.ylabel('% of variance explained')\nplt.title('Variance explained')\n\n#find the good number of eigenvalues\nfor eachvar in var_exp_list:\n    if eachvar>0.95:\n        good_var = eachvar\n        break\ngood_n_eigen = var_exp_list.index(good_var)\nplt.scatter(good_n_eigen, good_var)\nplt.show()\nprint('number of eigenvalues above 0.95: ', good_n_eigen, good_var)","06d42647":"#get newly generated components\npca_best = PCA(n_components = good_n_eigen)\npca_best.fit(x)\npca_features = pca_best.fit_transform(x)\nsingle_best = pca_best.singular_values_\ndf_pca = pd.DataFrame(pca_features, columns = list(range(good_n_eigen)))\nprint(df_pca.shape)","b18c3444":"# pca_2 = PCA(n_components = 2)\n# pca.fit(x)\n# principal_comps = pca.fit_transform(x)\n# df_principal = pd.DataFrame(principal_comps, columns = x.columns.values)\n# # print(principal_comps.shape)\n# principal_var = pca.explained_variance_\n# principal_val = pca.singular_values_\n# print(principal_val)\n#plotting\n# fig = plt.figure()\n# ax = fig.add_subplot(111)\n# plt.scatter(df_principal['pc1'], df_principal['pc2'])\n# var_pri1_max = max(df_principal['pc1'])\n# var_pr1_min = min(df_principal['pc1'])\n# var_pri2_max = max(df_principal['pc2'])\n# var_pr2_min = min(df_principal['pc2'])\n# col_pri1_max = x.columns[list(df_principal['pc1']).index(var_pri1_max)]\n# col_pri1_min = x.columns[list(df_principal['pc1']).index(var_pri1_min)]\n# col_pri2_max = x.columns[list(df_principal['pc2']).index(var_pri2_max)]\n# col_pri2_min = x.columns[list(df_principal['pc2']).index(var_pri2_min)]\n# print(col_pri1_max, col_pri1_min, col_pri1_max, col_pri2_min)","473fe3c6":"#use random forest to select features\nn_trees = np.arange(10, 110, 10)\ndepth = [2, 5, 10, 20, 30, 40, 50, 100]\naccuracy = list()\n\nfor eachn in n_trees:\n    acc_n = list()\n    for eachd in depth:\n        acc_d = list()\n        for eachfold in cv_folds:\n            x_tr_fold = eachfold[0]\n            y_tr_fold = eachfold[1]\n            x_ts_fold = eachfold[2]\n            y_ts_fold = eachfold[3]\n            rf = RandomForestClassifier(n_estimators = eachn,\n                                       max_depth = eachd, \n                                       bootstrap = True, \n                                       random_state = 0)\n            rf.fit(x_tr_fold, y_tr_fold)\n            acc_d.append(rf.score(x_ts_fold, y_ts_fold))\n        acc_n.append(mean(acc_d))\n    accuracy.append(acc_n)","7b73a627":"#plot results\ntree_acc_list = list()\nfor eachacc in range(len(accuracy)):\n    plt.scatter(depth, accuracy[eachacc], label = n_trees[eachacc])\n    tree_acc_list.append(max(accuracy[eachacc]))\nplt.legend(title = 'number of trees')\nplt.xlabel('depth of forest')\nplt.ylabel('accuracy')\nplt.show()\n\n#find the best number of trees and depth\nbest_acc = max(tree_acc_list)\nbest_n = n_trees[tree_acc_list.index(best_acc)]\nbest_n_list = accuracy[tree_acc_list.index(best_acc)]\nbest_d = depth[best_n_list.index(best_acc)]\nprint('the highest accuracy is: ', best_acc, '\\nbest number of trees is: ', best_n, '\\nbest forest depth is: ', best_d)","ba7d5fd1":"# acc_n = accuracy[6:9]\n# count = 6\n# for eachn in acc_n:\n#     plt.scatter(depth[3:6], eachn[3:6], label = n_trees[count])\n#     count += 1\n# plt.legend(title = 'number of trees')\n# plt.xlabel('depth of forest')\n# plt.ylabel('accuracy')\n# plt.show()\n","4d21907f":"#investigate new range of ntree and depth\nn_trees_s = np.arange(95, 106)\ndepth_s = np.arange(20, 41)\naccuracy_s = list()\n\nfor eachns in n_trees_s:\n    acc_ns = list()\n    print(eachns)\n    for eachds in depth_s:\n        acc_ds = list()\n        for eachfold in cv_folds:\n            x_tr_fold = eachfold[0]\n            y_tr_fold = eachfold[1]\n            x_ts_fold = eachfold[2]\n            y_ts_fold = eachfold[3]\n            rf = RandomForestClassifier(n_estimators = eachns,\n                                       max_depth = eachds, \n                                       bootstrap = True, \n                                       random_state = 0)\n            rf.fit(x_tr_fold, y_tr_fold)\n            acc_ds.append(rf.score(x_ts_fold, y_ts_fold))\n        acc_ns.append(mean(acc_ds))\n    accuracy_s.append(acc_ns)","4261e2e4":"#plot results\nbest_acc_d = list()\nfor eachacc in range(len(accuracy_s)):\n    plt.scatter(depth_s, accuracy_s[eachacc], label = n_trees_s[eachacc])\n    best_acc_d.append(max(accuracy_s[eachacc]))\nplt.legend(title = 'number of trees')\nplt.xlabel('depth of forest')\nplt.ylabel('accuracy')\nplt.show()\n\n#get the best accuracy, number of trees and depth\nbest_acc = max(best_acc_d)\nbest_n = n_trees_s[best_acc_d.index(best_acc)]\nbest_n_list = accuracy_s[best_acc_d.index(best_acc)]\nbest_d = depth_s[best_n_list.index(best_acc)]\nprint('the highest accuracy is: ', best_acc, '\\nbest number of trees is: ', best_n, '\\nbest forest depth is: ', best_d)","c7e44703":"best_rf = RandomForestClassifier(n_estimators = best_n, \n                                 max_depth = best_d, \n                                 random_state = 0)\nbest_rf.fit(x_train, y_train)\nselect_feature = SelectFromModel(best_rf, prefit=True)\nx_new = select_feature.transform(x)\nprint('original number of features: ', x.shape[1], '\\nnumber of selected features: ', x_new.shape[1])\nfeature_index = select_feature.get_support()\nfeature_name = list(x.columns[feature_index])\nprint(feature_name)\n# importances = best_rf.feature_importances_\n# indices = np.argsort(importances)[::-1]\n\n# # Print the feature ranking\n# print(\"Feature ranking:\")\n\n# for f in range(x.shape[1]):\n#     print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))","08d5597d":"best_rf.fit(x_train[feature_name], y_train)\nprint('training accuracy with selected feature: ', best_rf.score(x_train[feature_name], y_train))\nbest_rf.fit(x_test[feature_name], y_test)\nprint('training accuracy with selected feature: ', best_rf.score(x_test[feature_name], y_test))","3525070b":"from sklearn.svm import SVC as svc","4fea4d7c":"# kernel = ['linear', 'poly', 'rbf']\nc = [0.01, 0.1, 1, 5, 10, 50, 100, 200] #c for all kernels\ngamma = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 2, 3, 4, 5] #for rbf and poly\ndegree = [2, 3, 4, 5] #for poly kernel\n# kernel = ['linear', 'poly', 'rbf']","4da3d528":"def svc_cv(cv_folds, c, kernel, gamma = [], degree = [], print_para = False):\n    accuracy = list()\n    n_fold = len(cv_folds)\n    if kernel == 'linear':\n        for eachc in c:\n            linear_c = list()\n            for eachfold in cv_folds:\n                x_tr_fold = eachfold[0]\n                y_tr_fold = eachfold[1]\n                x_ts_fold = eachfold[2]\n                y_ts_fold = eachfold[3]\n                linear = svc(C = eachc, kernel = 'linear')\n                linear.fit(x_tr_fold, y_tr_fold)\n                linear_c.append(linear.score(x_ts_fold, y_ts_fold))\n            accuracy.append(mean(linear_c))\n    elif kernel == 'poly':\n        for eachd in degree:\n            poly_acc_g = list()\n            if print_para:\n                print('working on degree: ', eachd)\n            for eachg in gamma:\n                if print_para:\n                    print('working on gamma: ', eachg)\n                poly_acc_c = list()\n                for eachc in c:\n                    if print_para:\n                        print('working on c: ', eachc)\n                    poly_acc_fold = list()\n                    for eachfold in cv_folds:\n                        x_tr_fold = eachfold[0]\n                        y_tr_fold = eachfold[1]\n                        x_ts_fold = eachfold[2]\n                        y_ts_fold = eachfold[3]\n                        poly = svc(C = eachc, gamma = eachg, degree = eachd, kernel = 'poly')\n                        poly.fit(x_tr_fold, y_tr_fold)\n                        poly_acc_fold.append(poly.score(x_ts_fold, y_ts_fold))\n                    poly_acc_c.append(mean(poly_acc_fold))\n                poly_acc_g.append(poly_acc_c)\n            accuracy.append(poly_acc_g)\n    elif kernel == 'rbf':\n        for eachg in gamma:\n            rbf_acc_c = list()\n            for eachc in c:\n                rbf_acc_fold = list()\n                for eachfold in cv_folds:\n                    x_tr_fold = eachfold[0]\n                    y_tr_fold = eachfold[1]\n                    x_ts_fold = eachfold[2]\n                    y_ts_fold = eachfold[3]\n                    rbf = svc(C = eachc, gamma = eachg, kernel = 'rbf')\n                    rbf.fit(x_tr_fold, y_tr_fold)\n                    rbf_acc_fold.append(rbf.score(x_ts_fold, y_ts_fold))\n                rbf_acc_c.append(mean(rbf_acc_fold))\n            accuracy.append(rbf_acc_c)\n    return accuracy","0f1e5c7e":"#train linear kernel\nlinear_acc = svc_cv(cv_folds, c, 'linear')\nplt.scatter(c, linear_acc)\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.title('Accuracy VS C for Linear Kernel')\nprint(max(linear_acc), c[linear_acc.index(max(linear_acc))])","075e0a60":"#focus on new range of c for linear\nc_linear = np.arange(0.005, 0.015, 0.001)\nlinear_acc = svc_cv(cv_folds, c_linear, 'linear')\n#plot\nplt.scatter(c_linear, linear_acc)\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.title('Accuracy VS C for Linear Kernel')\nplt.show()\n#test\nbest_c_linear = c_linear[linear_acc.index(max(linear_acc))]\nlinear_test = svc(C = best_c_linear, kernel = 'linear')\nlinear_test.fit(x_test, y_test)\nlinear_test_acc = linear_test.score(x_test, y_test)\nprint('testing accuracy for linear kernel is: ', linear_test_acc, '\\nbest c is: ', best_c_linear)","e7fb8459":"#train poly model\npoly_acc = svc_cv(cv_folds, c, 'poly', gamma, degree)\n# print(len(poly_acc))\nfor eachdegree in range(len(poly_acc)):\n    degree_acc = poly_acc[eachdegree]\n    for eachacc in range(len(poly_acc[eachdegree])):\n        plt.scatter(c, degree_acc[eachacc], label = gamma[eachacc])\n    plt.xlabel('c')\n    plt.ylabel('accuracy')\n    plt.legend(title = 'gamma')\n    plt.title('Accuracy for '+ str(degree[eachdegree]) + ' degree polynomial')\n    plt.show()","45f6b5c0":"#find the degree that has the highest accuracy\ndegree_max_acc = list() #store the max accuracy for each degree\nbest_g_c = list() #store the g and c values of the highest accuracy for each degree\nfor eachd in poly_acc: #for each degree\n    each_g_acc = list() #store the best accuracy for each degree\n    for eachg in eachd: #for accuracy of each g value \n        each_g_acc.append(max(eachg)) #append the highest accuracy among c for current g and degree\n    degree_max_acc.append(max(each_g_acc)) #append the highest accuracy for current degree\n    best_g = gamma[each_g_acc.index(max(each_g_acc))] #find which gamma has the highest accuracy\n    best_g_list = eachd[each_g_acc.index(max(each_g_acc))] #get accuracies of each c of current gamma\n    best_c = c[best_g_list.index(max(each_g_acc))] #get which c has the highest accuracy\n    best_g_c.append([best_g, best_c]) #append the best gamma and c values for this degree in a list\n    \nbest_degree = degree[degree_max_acc.index(max(degree_max_acc))] #get the degree that has the best accuracy\nprint('max accuracy for each degree is: ', degree_max_acc)\nprint(best_degree)\nprint(best_g_c)","75aab8de":"#new c and gamma\n# c_new1 = np.arange(5, 16)\n# gamma_new1 = np.arange(0.005, 0.015, 0.001)\n# c_new2 = np.arange(150, 260, 10)\n# gamma_new2 = np.arange(0.005, 0.015, 0.001)\n# c_new3 = np.arange(0.5, 1.5, 0.1)\n# gamma_new3 = np.arange(0.05, 0.11, 0.01)\n# c_new4 = np.arange(0.05, 0.15, 0.01)\n# gamma_new4 = np.arange(0.05, 0.15, 0.01)\nc_new1 = np.arange(95, 106)\ngamma_new1 = np.arange(0.0005, 0.0015, 0.0001)\nc_new2 = np.arange(0.5, 1.5, 0.1)\ngamma_new2 = np.arange(0.005, 0.015, 0.001)\nc_new3 = np.arange(1, 10)\ngamma_new3 = np.arange(0.005, 0.011, 0.001)\nc_new4 = np.arange(5, 15)\ngamma_new4 = np.arange(0.005, 0.015, 0.001)\n\nc_new_all = [c_new1, c_new2, c_new3, c_new4]\ngamma_new_all = [gamma_new1, gamma_new2, gamma_new3, gamma_new4]\n\n#train with new hyperparameters\npoly_all = list()\nfor eachnew in range(len(c_new_all)):\n    pca_degree = int(eachnew+2)\n    pca_degree = [pca_degree]\n    new = svc_cv(cv_folds, c_new_all[eachnew], 'poly', gamma_new_all[eachnew], pca_degree) \n    poly_all.append(new)","6779d81c":"# for eachdegree in range(len(poly_acc)):\n#     degree_acc = poly_acc[eachdegree]\n#     for eachacc in range(len(poly_acc[eachdegree])):\n#         plt.scatter(c_poly, degree_acc[eachacc], label = gamma_poly[eachacc])\n#     plt.xlabel('c')\n#     plt.ylabel('accuracy')\n#     plt.legend(title = 'gamma', loc = (1, 0))\n#     plt.title('Accuracy for '+ str(degree[eachdegree]) + ' degree polynomial')\n#     plt.show()\nfor eachdegree in range(len(poly_all)):\n    degree_acc = poly_all[eachdegree][0]\n    for eachacc in range(len(degree_acc)):\n#         print(len(c_new_pca[eachdegree]), c_new_pca[eachdegree], np.arange(0.005, 0.025, 0.005))\n#         print(len(degree_acc), len(degree_acc[eachacc]))\n#         print(len(gamma_new_pca[eachdegree]))\n        plt.scatter(c_new_all[eachdegree], degree_acc[eachacc], label = gamma_new_all[eachdegree][eachacc])\n    plt.xlabel('c')\n    plt.ylabel('accuracy')\n    plt.legend(title = 'gamma', loc = (1, 0))\n    plt.title('Accuracy for '+ str(degree[eachdegree]) + ' degree polynomial')\n    plt.show()\n    \n#find the degree that has the highest accuracy\ndegree_max_acc = list() #store the max accuracy for each degree\nbest_g_c = list() #store the g and c values of the highest accuracy for each degree\nfor eachd_list in range(len(poly_all)): #for each degree\n    eachd = poly_all[eachd_list][0]\n    each_g_acc = list() #store the best accuracy for each degree\n    for eachg in eachd: #for accuracy of each g value \n        each_g_acc.append(max(eachg)) #append the highest accuracy among c for current g and degree\n    degree_max_acc.append(max(each_g_acc)) #append the highest accuracy for current degree\n    best_g = gamma_new_all[eachd_list][each_g_acc.index(max(each_g_acc))] #find which gamma has the highest accuracy\n    best_g_list = eachd[each_g_acc.index(max(each_g_acc))] #get accuracies of each c of current gamma\n    best_c = c_new_all[eachd_list][best_g_list.index(max(each_g_acc))] #get which c has the highest accuracy\n    best_g_c.append([best_g, best_c]) #append the best gamma and c values for this degree in a list\n    \nbest_degree = degree[degree_max_acc.index(max(degree_max_acc))] #get the degree that has the best accuracy\nprint('max accuracy for each degree is: ', degree_max_acc)\nprint(best_degree)\nprint(best_g_c)","fcb5d344":"# #find the best c and gamma\n# degree_max_acc = list() #store the max accuracy for each degree\n# best_g_c = list() #store the g and c values of the highest accuracy for each degree\n# for eachd in poly_acc: #for each degree\n#     each_g_acc = list() #store the best accuracy for each degree\n#     for eachg in eachd: #for accuracy of each g value \n#         each_g_acc.append(max(eachg)) #append the highest accuracy among c for current g and degree\n#     degree_max_acc.append(max(each_g_acc)) #append the highest accuracy for current degree\n#     best_g = gamma_poly[each_g_acc.index(max(each_g_acc))] #find which gamma has the highest accuracy\n#     best_g_list = eachd[each_g_acc.index(max(each_g_acc))] #get accuracies of each c of current gamma\n#     best_c = c_poly[best_g_list.index(max(each_g_acc))] #get which c has the highest accuracy\n#     best_g_c.append([best_g, best_c]) #append the best gamma and c values for this degree in a list\n    \n# best_degree = degree[degree_max_acc.index(max(degree_max_acc))] #get the degree that has the best accuracy\n# print('max accuracy for each degree is: ', degree_max_acc)\n# print(best_degree)\n# print(best_g_c)\n\n#get testing accuracy for the best \npoly_test_accuracies = list()\nfor eachgc in range(len(best_g_c)):\n    current_g_c = best_g_c[eachgc]\n    poly = svc(C = current_g_c[1], gamma = current_g_c[0], degree = degree[eachgc], kernel = 'poly')\n    poly.fit(x_test, y_test)\n    poly_test_acc = poly.score(x_test, y_test)\n    poly_test_accuracies.append(poly_test_acc)\n#     print(current_g_c)\n# print(best_g_c)\nprint(poly_test_accuracies)\nprint('the best degree is: ', degree[poly_test_accuracies.index(max(poly_test_accuracies))], \n      '\\nthe highest testing accuracy is: ', max(poly_test_accuracies))","b629e5a6":"# #get testing accuracy for the best \n# poly_test_accuracies = list()\n# for eachgc in range(len(best_g_c)):\n#     current_g_c = best_g_c[eachgc]\n#     poly = svc(C = current_g_c[1], gamma = current_g_c[0], degree = degree[eachgc], kernel = 'poly')\n#     poly.fit(x_test, y_test)\n#     poly_test_acc = poly.score(x_test, y_test)\n#     poly_test_accuracies.append(poly_test_acc)\n# print(poly_test_accuracies)\n# print('the best degree is: ', degree[poly_test_accuracies.index(max(poly_test_accuracies))], \n#       '\\nthe highest testing accuracy is: ', max(poly_test_accuracies))","5f856c33":"#train poly model\nrbf_acc = svc_cv(cv_folds, c, 'rbf', gamma)","5e0c0c7e":"#plot accuracy\ngamma_acc_list = list()\nfor eachacc in range(len(rbf_acc)):\n    plt.scatter(c, rbf_acc[eachacc], label = gamma[eachacc])\n    gamma_acc_list.append(max(rbf_acc[eachacc]))\nmax_acc = max(gamma_acc_list)\nbest_gamma = gamma[gamma_acc_list.index(max_acc)]\nbest_gamma_acc = rbf_acc[gamma_acc_list.index(max_acc)]\nbest_c = c[best_gamma_acc.index(max_acc)]\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.legend(title = 'gamma')\nplt.title('Accuracy for rbf kernel')\nplt.show()\nprint('the highest accuracy is: ', max_acc, '\\nbest gamma is: ', best_gamma, '\\nbest c is: ', best_c)","41d355ad":"#focus on new c and gamma\nrbf_c = np.arange(1, 10)\nrbf_gamma = np.arange(0.0005, 0.0015, 0.0001)\nrbf_new = svc_cv(cv_folds, rbf_c, 'rbf', rbf_gamma)","557861ff":"#plot new rbf\ngamma_acc_list = list()\nfor eachacc in range(len(rbf_new)):\n    plt.scatter(rbf_c, rbf_new[eachacc], label = rbf_gamma[eachacc])\n    gamma_acc_list.append(max(rbf_new[eachacc]))\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.legend(title = 'gamma', loc = (1.1, 0))\nplt.title('Accuracy for rbf kernel')\nplt.show()\nmax_acc = max(gamma_acc_list)\nbest_gamma = rbf_gamma[gamma_acc_list.index(max_acc)]\nbest_gamma_acc = rbf_new[gamma_acc_list.index(max_acc)]\nbest_c = rbf_c[best_gamma_acc.index(max_acc)]\nprint('the highest accuracy is: ', max_acc, '\\nbest gamma is: ', best_gamma, '\\nbest c is: ', best_c)","fe2cbee1":"#get testing accuracy for rbf\nrbf_test = svc(C = best_c, gamma = best_gamma, kernel = 'rbf')\nrbf_test.fit(x_test, y_test)\nrbf_test_acc = rbf_test.score(x_test, y_test)\nprint('testing accuracy for rbf kernel is: ', rbf_test_acc)","c915aa88":"#change from all features to selected features\nselected_cv_folds = list()\nfor eachfold in cv_folds:\n    selected_x_tr = eachfold[0][feature_name]\n    selected_x_ts = eachfold[2][feature_name]\n    selected_cv_folds.append([selected_x_tr, eachfold[1], selected_x_ts, eachfold[3]])\nselectedx_test = x_test[feature_name]","9f705db2":"linear_acc = svc_cv(selected_cv_folds, c, 'linear')\nplt.scatter(c, linear_acc)\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.title('Accuracy VS C for Linear Kernel')\nprint('highest accuracy is: ', max(linear_acc), '\\nbest c is: ', c[linear_acc.index(max(linear_acc))])","06cbc9e0":"#focus on new range of c\nc_linear = np.arange(0.1, 1, 0.1)\nlinear_acc = svc_cv(selected_cv_folds, c_linear, 'linear')\nplt.scatter(c_linear, linear_acc)\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.title('Accuracy VS C for Linear Kernel')\n\n#best c\nbest_c_linear = c_linear[linear_acc.index(max(linear_acc))]\nprint(max(linear_acc), 'best c: ', best_c_linear)\n# print(linear_acc)\n\n#test\nlinear_test = svc(C = best_c_linear, kernel = 'linear')\nlinear_test.fit(selectedx_test, y_test)\nlinear_test_acc = linear_test.score(selectedx_test, y_test)\nprint('best testing accuracy: ', linear_test_acc)","4428c1bf":"# c_selected = [0.001, 0.01, 0.1, 1, 5, 10, 50, 100]\n# # c_selected = [0.001, 0.01, 0.1, 1]\n\n# gamma_selected = [0.001, 0.01, 0.1, 1, 5, 10]","f6eb27ca":"#train poly model\npoly_acc = svc_cv(selected_cv_folds, c, 'poly', gamma, degree)\n\n#plot accuracy\nfor eachdegree in range(len(poly_acc)):\n    degree_acc = poly_acc[eachdegree]\n    for eachacc in range(len(poly_acc[eachdegree])):\n        plt.scatter(c, degree_acc[eachacc], label = gamma[eachacc])\n    plt.xlabel('c')\n    plt.ylabel('accuracy')\n    plt.legend(title = 'gamma')\n    plt.title('Accuracy for '+ str(degree[eachdegree]) + ' degree polynomial')\n    plt.show()","ae4c7d6e":"#find the degree that has the highest accuracy\ndegree_max_acc = list() #store the max accuracy for each degree\nbest_g_c = list() #store the g and c values of the highest accuracy for each degree\nfor eachd in poly_acc: #for each degree\n    each_g_acc = list() #store the best accuracy for each degree\n    for eachg in eachd: #for accuracy of each g value \n        each_g_acc.append(max(eachg)) #append the highest accuracy among c for current g and degree\n    degree_max_acc.append(max(each_g_acc)) #append the highest accuracy for current degree\n    best_g = gamma[each_g_acc.index(max(each_g_acc))] #find which gamma has the highest accuracy\n    best_g_list = eachd[each_g_acc.index(max(each_g_acc))] #get accuracies of each c of current gamma\n    best_c = c[best_g_list.index(max(each_g_acc))] #get which c has the highest accuracy\n    best_g_c.append([best_g, best_c]) #append the best gamma and c values for this degree in a list\n    \nbest_degree = degree[degree_max_acc.index(max(degree_max_acc))] #get the degree that has the best accuracy\nprint('max accuracy for each degree is: ', degree_max_acc)\nprint(best_degree)\nprint(best_g_c)","7418779b":"#new c and gamma\n# c_new1 = np.arange(95, 106)\n# gamma_new1 = np.arange(0.005, 0.015, 0.001)\n# c_new2 = np.arange(0.005, 0.015, 0.001)\n# gamma_new2 = np.arange(0.1, 1, 0.1)\n# c_new3 = np.arange(0.5, 1.5, 0.1)\n# gamma_new3 = np.arange(0.05, 0.15, 0.01)\n# c_new4 = np.arange(35, 80, 5)\n# gamma_new4 = np.arange(0.05, 0.15, 0.01)\nc_new1 = np.arange(40, 61)\ngamma_new1 = np.arange(0.005, 0.015, 0.001)\nc_new2 = np.arange(40, 61)\ngamma_new2 = np.arange(0.005, 0.015, 0.001)\nc_new3 = np.arange(0.05, 0.15, 0.01)\ngamma_new3 = np.arange(0.05, 0.15, 0.01)\nc_new4 = np.arange(0.005, 0.015, 0.001)\ngamma_new4 = np.arange(0.05, 0.15, 0.01)\n\nc_new_selected = [c_new1, c_new2, c_new3, c_new4]\ngamma_new_selected = [gamma_new1, gamma_new2, gamma_new3, gamma_new4]\n\n#train with new hyperparameters\npoly_selected = list()\nfor eachnew in range(len(c_new_selected)):\n    pca_degree = int(eachnew+2)\n    pca_degree = [pca_degree]\n    new = svc_cv(selected_cv_folds, c_new_selected[eachnew], 'poly', gamma_new_selected[eachnew], pca_degree) \n    poly_selected.append(new)","11f792fa":"#plot new acc\nfor eachdegree in range(len(poly_selected)):\n    degree_acc = poly_selected[eachdegree][0]\n    for eachacc in range(len(degree_acc)):\n        plt.scatter(c_new_selected[eachdegree], degree_acc[eachacc], label = gamma_new_selected[eachdegree][eachacc])\n    plt.xlabel('c')\n    plt.ylabel('accuracy')\n    plt.legend(title = 'gamma', loc = (1, 0))\n    plt.title('Accuracy for '+ str(degree[eachdegree]) + ' degree polynomial')\n    plt.show()\n    \n#find the degree that has the highest accuracy\ndegree_max_acc = list() #store the max accuracy for each degree\nbest_g_c = list() #store the g and c values of the highest accuracy for each degree\nfor eachd_list in range(len(poly_selected)): #for each degree\n    eachd = poly_selected[eachd_list][0]\n    each_g_acc = list() #store the best accuracy for each degree\n    for eachg in eachd: #for accuracy of each g value \n        each_g_acc.append(max(eachg)) #append the highest accuracy among c for current g and degree\n    degree_max_acc.append(max(each_g_acc)) #append the highest accuracy for current degree\n    best_g = gamma_new_selected[eachd_list][each_g_acc.index(max(each_g_acc))] #find which gamma has the highest accuracy\n    best_g_list = eachd[each_g_acc.index(max(each_g_acc))] #get accuracies of each c of current gamma\n    best_c = c_new_selected[eachd_list][best_g_list.index(max(each_g_acc))] #get which c has the highest accuracy\n    best_g_c.append([best_g, best_c]) #append the best gamma and c values for this degree in a list\n    \nbest_degree = degree[degree_max_acc.index(max(degree_max_acc))] #get the degree that has the best accuracy\nprint('max accuracy for each degree is: ', degree_max_acc)\nprint(best_degree)\nprint(best_g_c)","670ad098":"#get testing accuracy for the best \npoly_test_accuracies = list()\nfor eachgc in range(len(best_g_c)):\n    current_g_c = best_g_c[eachgc]\n    poly = svc(C = current_g_c[1], gamma = current_g_c[0], degree = degree[eachgc], kernel = 'poly')\n    poly.fit(selectedx_test, y_test)\n    poly_test_acc = poly.score(selectedx_test, y_test)\n    poly_test_accuracies.append(poly_test_acc)\n#     print(current_g_c)\n# print(best_g_c)\nprint(poly_test_accuracies)\nprint('the best degree is: ', degree[poly_test_accuracies.index(max(poly_test_accuracies))], \n      '\\nthe highest testing accuracy is: ', max(poly_test_accuracies))","503635cd":"rbf_acc = svc_cv(selected_cv_folds, c, 'rbf', gamma)","f14444cb":"#plot accuracy\ngamma_acc_list = list()\nfor eachacc in range(len(rbf_acc)):\n    plt.scatter(c, rbf_acc[eachacc], label = gamma[eachacc])\n    gamma_acc_list.append(max(rbf_acc[eachacc]))\nmax_acc = max(gamma_acc_list)\nbest_gamma = gamma[gamma_acc_list.index(max_acc)]\nbest_gamma_acc = rbf_acc[gamma_acc_list.index(max_acc)]\nbest_c = c[best_gamma_acc.index(max_acc)]\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.legend(title = 'gamma')\nplt.title('Accuracy for rbf kernel')\nplt.show()\nprint('the highest accuracy is: ', max_acc, '\\nbest gamma is: ', best_gamma, '\\nbest c is: ', best_c)","233cc75a":"#new c and gamma\nc_rbf = np.arange(1, 10)\ngamma_rbf = np.arange(0.005, 0.015, 0.001)\nrbf_acc = svc_cv(selected_cv_folds, c_rbf, 'rbf', gamma_rbf)","a2717328":"#plot accuracy\ngamma_acc_list = list()\nfor eachacc in range(len(rbf_acc)):\n    plt.scatter(c_rbf, rbf_acc[eachacc], label = gamma_rbf[eachacc])\n    gamma_acc_list.append(max(rbf_acc[eachacc]))\nmax_acc = max(gamma_acc_list)\nbest_gamma = gamma_rbf[gamma_acc_list.index(max_acc)]\nbest_gamma_acc = rbf_acc[gamma_acc_list.index(max_acc)]\nbest_c = c_rbf[best_gamma_acc.index(max_acc)]\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.legend(title = 'gamma', loc = (1, 0))\nplt.title('Accuracy for rbf kernel')\nplt.show()\nprint('the highest accuracy is: ', max_acc, '\\nbest gamma is: ', best_gamma, '\\nbest c is: ', best_c)","c4d6104c":"#testing rbf\nrbf_test = svc(C = best_c, gamma = best_gamma, kernel = 'rbf')\nrbf_test.fit(selectedx_test, y_test)\nrbf_test_acc = rbf_test.score(selectedx_test, y_test)\nprint('the test accuracy for rbf kernel is: ', rbf_test_acc)","866c4960":"#split into train and test set\nx_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(df_pca, y, test_size = 0.2, random_state = 0)\n\n#split into cv folds\nx_folds_pca = np.array_split(x_train_pca, n_fold) #split training data into n_fold proportions\ny_folds_pca = np.array_split(y_train_pca, n_fold) #split training data into n_fold proportions\n\ncv_folds_pca = list() #stores dataframes for each fold\nfor eachfold in range(n_fold): #for each fold\n    train_number = list(np.arange(0, n_fold)) #a list of fold numbers\n    train_number.pop(eachfold) #pop current fold number\n    df_y_ts = y_folds_pca[eachfold] #use current fold number as testing fold, create testing y\n    df_x_ts = x_folds_pca[eachfold] #use current fold number as testing fold, create testing x\n    \n    y_train_list = list() #stores all the df y from folds for training\n    x_train_list = list() #stores all the df x from folds for training\n    for eachnumber in train_number: #for each training fold number\n        x_train_list.append(x_folds_pca[eachnumber]) #append the df in training fold number for x\n        y_train_list.append(y_folds_pca[eachnumber]) #append the df in training fold number for y\n    df_x_tr = pd.concat(x_train_list) #combine all the training dfs for x into 1\n    df_y_tr = pd.concat(y_train_list) #combine all the training dfs for y into 1\n    cv_folds_pca.append([df_x_tr, df_y_tr, df_x_ts, df_y_ts]) #append training and testing dataframe for current fold","66450801":"linear_pca = svc_cv(cv_folds_pca, c, 'linear')\nplt.scatter(c, linear_pca)\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.title('Accuracy VS C for Linear Kernel')\n\nlinear_acc_pca = max(linear_pca)\nbest_c_pca = c[linear_pca.index(max(linear_pca))]\nprint('highest accuracy is: ', linear_acc_pca, '\\nbest c is: ', best_c_pca)","afa9d9e9":"#new c\nc_pca = np.arange(0.005, 0.1, 0.001)\nlinear_pca = svc_cv(cv_folds_pca, c_pca, 'linear')\nplt.scatter(c_pca, linear_pca)\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.title('Accuracy VS C for Linear Kernel')\n\nlinear_acc_pca = max(linear_pca)\nbest_c_pca = c_pca[linear_pca.index(max(linear_pca))]\nprint('highest accuracy is: ', linear_acc_pca, '\\nbest c is: ', best_c_pca)","251299c6":"#test accuracy\nlinear_test_pca = svc(C = best_c_pca, kernel = 'linear')\nlinear_test_pca.fit(x_test_pca, y_test_pca)\nlinear_tsacc_pca = linear_test_pca.score(x_test_pca, y_test_pca)\nprint('the test accuracy for rbf kernel is: ', linear_tsacc_pca)","948dc126":"poly_pca = svc_cv(cv_folds_pca, c, 'poly', gamma, degree)","7a9119b4":"#plot accuracy\nfor eachdegree in range(len(poly_pca)):\n    degree_acc = poly_pca[eachdegree]\n    for eachacc in range(len(poly_pca[eachdegree])):\n        plt.scatter(c, degree_acc[eachacc], label = gamma[eachacc])\n    plt.xlabel('c')\n    plt.ylabel('accuracy')\n    plt.legend(title = 'gamma', loc = (1, 0))\n    plt.title('Accuracy for '+ str(degree[eachdegree]) + ' degree polynomial')\n    plt.show()\n    \n#find the degree that has the highest accuracy\ndegree_max_acc = list() #store the max accuracy for each degree\nbest_g_c = list() #store the g and c values of the highest accuracy for each degree\nfor eachd in poly_pca: #for each degree\n    each_g_acc = list() #store the best accuracy for each degree\n    for eachg in eachd: #for accuracy of each g value \n        each_g_acc.append(max(eachg)) #append the highest accuracy among c for current g and degree\n    degree_max_acc.append(max(each_g_acc)) #append the highest accuracy for current degree\n    best_g = gamma[each_g_acc.index(max(each_g_acc))] #find which gamma has the highest accuracy\n    best_g_list = eachd[each_g_acc.index(max(each_g_acc))] #get accuracies of each c of current gamma\n    best_c = c[best_g_list.index(max(each_g_acc))] #get which c has the highest accuracy\n    best_g_c.append([best_g, best_c]) #append the best gamma and c values for this degree in a list\n    \nbest_degree = degree[degree_max_acc.index(max(degree_max_acc))] #get the degree that has the best accuracy\nprint('max accuracy for each degree is: ', degree_max_acc)\nprint(best_degree)\nprint(best_g_c)","d3e31a19":"#new c and gamma\n# c_new1 = np.arange(5, 16)\n# gamma_new1 = np.arange(0.005, 0.015, 0.001)\n# c_new2 = np.arange(150, 260, 10)\n# gamma_new2 = np.arange(0.005, 0.015, 0.001)\n# c_new3 = np.arange(0.05, 0.15, 0.01)\n# gamma_new3 = np.arange(0.05, 0.15, 0.01)\n# c_new4 = np.arange(0.05, 0.15, 0.01)\n# gamma_new4 = np.arange(0.05, 0.15, 0.01)\nc_new1 = np.arange(95, 106)\ngamma_new1 = np.arange(0.0005, 0.0015, 0.0001)\nc_new2 = np.arange(0.1, 2, 0.1)\ngamma_new2 = np.arange(0.005, 0.015, 0.001)\nc_new3 = np.arange(40, 61)\ngamma_new3 = np.arange(0.005, 0.015, 0.001)\nc_new4 = np.arange(1, 20)\ngamma_new4 = np.arange(0.005, 0.015, 0.001)\n\nc_new_pca = [c_new1, c_new2, c_new3, c_new4]\ngamma_new_pca = [gamma_new1, gamma_new2, gamma_new3, gamma_new4]\n\n#train with new hyperparameters\npoly_pca = list()\nfor eachnew in range(len(c_new_pca)):\n    pca_degree = int(eachnew+2)\n    pca_degree = [pca_degree]\n    new = svc_cv(cv_folds_pca, c_new_pca[eachnew], 'poly', gamma_new_pca[eachnew], pca_degree) \n    poly_pca.append(new)","86ff24c8":"#plot new acc\nfor eachdegree in range(len(poly_pca)):\n    degree_acc = poly_pca[eachdegree][0]\n    for eachacc in range(len(degree_acc)):\n        plt.scatter(c_new_pca[eachdegree], degree_acc[eachacc], label = gamma_new_pca[eachdegree][eachacc])\n    plt.xlabel('c')\n    plt.ylabel('accuracy')\n    plt.legend(title = 'gamma', loc = (1, 0))\n    plt.title('Accuracy for '+ str(degree[eachdegree]) + ' degree polynomial')\n    plt.show()\n    \n#find the degree that has the highest accuracy\ndegree_max_acc = list() #store the max accuracy for each degree\nbest_g_c = list() #store the g and c values of the highest accuracy for each degree\nfor eachd_list in range(len(poly_pca)): #for each degree\n    eachd = poly_pca[eachd_list][0]\n    each_g_acc = list() #store the best accuracy for each degree\n    for eachg in eachd: #for accuracy of each g value \n        each_g_acc.append(max(eachg)) #append the highest accuracy among c for current g and degree\n    degree_max_acc.append(max(each_g_acc)) #append the highest accuracy for current degree\n    best_g = gamma_new_pca[eachd_list][each_g_acc.index(max(each_g_acc))] #find which gamma has the highest accuracy\n    best_g_list = eachd[each_g_acc.index(max(each_g_acc))] #get accuracies of each c of current gamma\n    best_c = c_new_pca[eachd_list][best_g_list.index(max(each_g_acc))] #get which c has the highest accuracy\n    best_g_c.append([best_g, best_c]) #append the best gamma and c values for this degree in a list\n    \nbest_degree = degree[degree_max_acc.index(max(degree_max_acc))] #get the degree that has the best accuracy\nprint('max accuracy for each degree is: ', degree_max_acc)\nprint(best_degree)\nprint(best_g_c)","d9f9fbe1":"#get testing accuracy for the best \npoly_test_accuracies = list()\nfor eachgc in range(len(best_g_c)):\n    current_g_c = best_g_c[eachgc]\n    poly = svc(C = current_g_c[1], gamma = current_g_c[0], degree = degree[eachgc], kernel = 'poly')\n    poly.fit(x_test_pca, y_test_pca)\n    poly_test_acc = poly.score(x_test_pca, y_test_pca)\n    poly_test_accuracies.append(poly_test_acc)\nprint(poly_test_accuracies)\nprint('the best degree is: ', degree[poly_test_accuracies.index(max(poly_test_accuracies))], \n      '\\nthe highest testing accuracy is: ', max(poly_test_accuracies))","2accad0e":"rbf_pca = svc_cv(cv_folds_pca, c, 'rbf', gamma)","ec45962a":"#plot accuracy\ngamma_acc_list = list()\nfor eachacc in range(len(rbf_pca)):\n    plt.scatter(c, rbf_pca[eachacc], label = gamma[eachacc])\n    gamma_acc_list.append(max(rbf_pca[eachacc]))\nmax_acc = max(gamma_acc_list)\nbest_gamma = gamma[gamma_acc_list.index(max_acc)]\nbest_gamma_acc = rbf_pca[gamma_acc_list.index(max_acc)]\nbest_c = c[best_gamma_acc.index(max_acc)]\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.legend(title = 'gamma', loc = (1,0))\nplt.title('Accuracy for rbf kernel')\nplt.show()\nprint('the highest accuracy is: ', max_acc, '\\nbest gamma is: ', best_gamma, '\\nbest c is: ', best_c)","06ca6b7a":"#new c and gamma\nc_rbf = np.arange(1, 10)\ngamma_rbf = np.arange(0.0005, 0.0015, 0.0001)\nrbf_pca = svc_cv(cv_folds_pca, c_rbf, 'rbf', gamma_rbf)","86b88819":"#plot accuracy\ngamma_acc_list = list()\nfor eachacc in range(len(rbf_pca)):\n    plt.scatter(c_rbf, rbf_pca[eachacc], label = gamma_rbf[eachacc])\n    gamma_acc_list.append(max(rbf_pca[eachacc]))\nmax_acc = max(gamma_acc_list)\nbest_gamma = gamma_rbf[gamma_acc_list.index(max_acc)]\nbest_gamma_acc = rbf_pca[gamma_acc_list.index(max_acc)]\nbest_c = c_rbf[best_gamma_acc.index(max_acc)]\nplt.xlabel('c')\nplt.ylabel('accuracy')\nplt.legend(title = 'gamma', loc = (1, 0))\nplt.title('Accuracy for rbf kernel')\nplt.show()\nprint('the highest accuracy is: ', max_acc, '\\nbest gamma is: ', best_gamma, '\\nbest c is: ', best_c)","152e7d4e":"#testing rbf\nrbf_test = svc(C = best_c, gamma = best_gamma, kernel = 'rbf')\nrbf_test.fit(x_test_pca, y_test_pca)\nrbf_test_acc = rbf_test.score(x_test_pca, y_test_pca)\nprint('the test accuracy for rbf kernel is: ', rbf_test_acc)","9fb35b3f":"# df_read2 = pd.read_csv('..\/input\/mental-health-in-tech-field-eda-cleaning\/main_dummy2.csv')\n# df_read2 = df_read2.drop('Unnamed: 0', axis = 1)\n# df_dummies2 = df_read2.copy()\n# print(df_dummies2.shape)","19ff53c5":"# cols2 = df_dummies2.columns.values\n# ycol2 = 'Have you ever sought treatment for a mental health issue from a mental health professional?'\n# xcol2 = list()\n# for each in cols2:\n#     if each != ycol:\n#         xcol2.append(each)\n# y2 = df_dummies2[ycol2]\n# x2 = df_dummies2[xcol2]\n# x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size = 0.2, random_state = 0)","ce3947ce":"# n_fold = 5\n# x_folds2 = np.array_split(x_train2, n_fold) #split training data into n_fold proportions\n# y_folds2 = np.array_split(y_train2, n_fold) #split training data into n_fold proportions\n\n# cv_folds2 = list() #stores dataframes for each fold\n# for eachfold in range(n_fold): #for each fold\n#     train_number = list(np.arange(0, n_fold)) #a list of fold numbers\n#     train_number.pop(eachfold) #pop current fold number\n# #     print(train_number)\n#     df_y_ts = y_folds2[eachfold] #use current fold number as testing fold, create testing y\n#     df_x_ts = x_folds2[eachfold] #use current fold number as testing fold, create testing x\n#     y_train_list = list() #stores all the df y from folds for training\n#     x_train_list = list() #stores all the df x from folds for training\n#     for eachnumber in train_number: #for each training fold number\n#         x_train_list.append(x_folds2[eachnumber]) #append the df in training fold number for x\n#         y_train_list.append(y_folds2[eachnumber]) #append the df in training fold number for y\n#     df_x_tr = pd.concat(x_train_list) #combine all the training dfs for x into 1\n#     df_y_tr = pd.concat(y_train_list) #combine all the training dfs for y into 1\n#     cv_folds2.append([df_x_tr, df_y_tr, df_x_ts, df_y_ts]) #append training and testing dataframe for current fold","5415d1fa":"# from math import exp","5d394486":"# c = [0.01, 0.1, 1, 5, 10, 50, 100, 200] #c for all kernels\n# gamma = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 2, 3, 4, 5] #for rbf and poly\n# degree = [2, 3, 4, 5] #for poly kernel","33005c75":"# linear_acc = svc_cv(cv_folds2, c, 'linear')\n# plt.scatter(c, linear_acc)\n# plt.xlabel('c')\n# plt.ylabel('accuracy')\n# plt.title('Accuracy VS C for Linear Kernel')\n# print('highest accuracy is: ', max(linear_acc), '\\nbest c is: ', c[linear_acc.index(max(linear_acc))])","b0970cab":"# rbf_acc = svc_cv(cv_folds2, c, 'rbf', gamma)","aff50ad6":"# #plot accuracy\n# gamma_acc_list = list()\n# for eachacc in range(len(rbf_acc)):\n#     plt.scatter(c, rbf_acc[eachacc], label = gamma[eachacc])\n#     gamma_acc_list.append(max(rbf_acc[eachacc]))\n# max_acc = max(gamma_acc_list)\n# best_gamma = gamma[gamma_acc_list.index(max_acc)]\n# best_gamma_acc = rbf_acc[gamma_acc_list.index(max_acc)]\n# best_c = c[best_gamma_acc.index(max_acc)]\n# plt.xlabel('c')\n# plt.ylabel('accuracy')\n# plt.legend(title = 'gamma')\n# plt.title('Accuracy for rbf kernel')\n# plt.show()\n# print('the highest accuracy is: ', max_acc, '\\nbest gamma is: ', best_gamma, '\\nbest c is: ', best_c)","80d995ed":"# #new gamma and c\n# # gamma_linear2 = np.arange(0.0001, 0.001, 0.0001)\n# c_linear2 = np.arange(50, 100)\n# rbf_acc = svc_cv(cv_folds2, c_linear2, 'rbf', [best_gamma])","63d010d1":"# #plot accuracy\n# gamma_acc_list = list()\n# for eachacc in range(len(rbf_acc)):\n#     plt.scatter(c_linear2, rbf_acc[eachacc], label = gamma[eachacc])\n#     gamma_acc_list.append(max(rbf_acc[eachacc]))\n# max_acc = max(gamma_acc_list)\n# best_gamma = gamma[gamma_acc_list.index(max_acc)]\n# best_gamma_acc = rbf_acc[gamma_acc_list.index(max_acc)]\n# best_c = c_linear2[best_gamma_acc.index(max_acc)]\n# plt.xlabel('c')\n# plt.ylabel('accuracy')\n# plt.legend(title = 'gamma')\n# plt.title('Accuracy for rbf kernel')\n# plt.show()\n# print('the highest accuracy is: ', max_acc, '\\nbest gamma is: ', best_gamma, '\\nbest c is: ', best_c)","d92adc3d":"# #test accuracy\n# rbf_test2 = svc(C = best_c, gamma = best_gamma, kernel = 'rbf')\n# rbf_test2.fit(x_test, y_test)\n# rbf_test_acc2 = rbf_test2.score(x_test, y_test)\n# print('testing accuracy is: ', rbf_test_acc2)","3ad28b69":"# poly_test2 = svc_cv(cv_folds2, c, 'poly', gamma, degree)","648ec415":"# #plot result\n# for eachdegree in range(len(poly_acc)):\n#     degree_acc = poly_acc[eachdegree]\n#     for eachacc in range(len(poly_acc[eachdegree])):\n#         plt.scatter(c, degree_acc[eachacc], label = gamma[eachacc])\n#     plt.xlabel('c')\n#     plt.ylabel('accuracy')\n#     plt.legend(title = 'gamma')\n#     plt.title('Accuracy for '+ str(degree[eachdegree]) + ' degree polynomial')\n#     plt.show()","ba6b1cb4":"# #find the best c and gamma for each degree\n# degree_max_acc = list() #store the max accuracy for each degree\n# best_g_c = list() #store the g and c values of the highest accuracy for each degree\n# for eachd in poly_acc: #for each degree\n#     each_g_acc = list() #store the best accuracy for each degree\n#     for eachg in eachd: #for accuracy of each g value \n#         each_g_acc.append(max(eachg)) #append the highest accuracy among c for current g and degree\n#     degree_max_acc.append(max(each_g_acc)) #append the highest accuracy for current degree\n#     best_g = gamma[each_g_acc.index(max(each_g_acc))] #find which gamma has the highest accuracy\n#     best_g_list = eachd[each_g_acc.index(max(each_g_acc))] #get accuracies of each c of current gamma\n#     best_c = c[best_g_list.index(max(each_g_acc))] #get which c has the highest accuracy\n#     best_g_c.append([best_g, best_c]) #append the best gamma and c values for this degree in a list\n    \n# best_degree = degree[degree_max_acc.index(max(degree_max_acc))] #get the degree that has the best accuracy\n# print('max accuracy for each degree is: ', degree_max_acc)\n# print(best_degree)\n# print(best_g_c)","8ff095e8":"# with PCA Features","d9d587d7":"chi test and p value between features","65dbd75c":"chi2 test","7c6368d5":"# SVM","d70e68c7":"rbf kernel","c4b11855":"# Question 1: have mental condition or not?","69368507":"poly kernel","641541d0":"linear kernel","b8facacc":"Poly kernel","f2b18665":"# question 2: reach for help?","6d66d2a2":"# train with selected features","5ae114b7":"# Feature Selection","1c630835":"chi test and p value between x and y","fa6a9704":"rbf kernel","12aee193":"Q1:\n\nBest Accuracy: 1.0\n\nKernel: Poly\n\nDegree: 4\n\nFeatures: all","0a05ea39":"poly kernel","4e160214":"Load Data","30120135":"# Summary","4b6fdc94":"modify training and testing sets","996a78f0":"# Get rid of unwanted columns ","a179bbcc":"split into training, testing, and CV folds","7e7b4489":"Further split training and testing data for CV folds","edc2c4f2":"Linear kernel","0009d66d":"# Prepare train and test sets","1417bb9c":"SVC training function: conducts K fold CV on given training and testing data, returns accuracy of the result\n\ninput:\n\nx_tr_folds: a list of dataframe containing n dataframes for n folds, used to fit model\n\ny_tr_folds: a list of dataframe containing n dataframes for n folds, used to fit model\n\nx_ts_folds: a dataframe of features used to calculate test accuracy\n\ny_ts_folds: an array for response variable used to calculate \n\nc: list a of values for c to tune, required for all three types of kernels\n\nkernel: string, linear, poly or rbf\n\ngamma: list of gamma values. for poly and rbf\n\ndegree: list of number of degrees, for poly\n\nOutput:\n\naccuracy: list of accuracy corresponding to the given c, gamma and degree. For linear, dimension is 1xlen(c). For poly, dimension is len(degree)x(len(gamma)xlen(c)). For rbf, dimension is len(gamma)x(len(c))","764a2f4d":"rbf kernel","a9bf352e":"linear kernel","687350ac":"PCA","7b5a9b2a":"# train with all features","e34f5ff6":"# use all features","fd827ea2":"Further split into train and test for CV","43351cfb":"random forest","3a4d55f2":"Set up parameters","6faabda2":"rbf kernel","5a7b261e":"# Whitening data","5ff35d05":"Poly kernel","763a8f6f":"Linear Kernel","f396170a":"Set up x, y, train and test data","ae7fa896":"Starting parameters"}}