{"cell_type":{"6a575491":"code","6895a7ad":"code","f4dcb52f":"code","3a01e847":"code","c405a2fa":"code","febb534c":"code","007ebb50":"code","a1c73375":"code","b01eedda":"code","bbe98cce":"code","52d877ad":"code","99c2a01e":"code","fcbaafb9":"code","c9f319c6":"code","5317d1e9":"code","574960c6":"code","59d02ad1":"code","a91cc9ad":"code","816bcda5":"code","8766f4d1":"code","c36e52e7":"code","a66ac84f":"code","7bd4089f":"code","aab12872":"code","87e850da":"code","c73284a2":"code","8fdb6d28":"code","3a22f0f6":"code","b118e2dd":"code","23217713":"code","a2901af3":"code","7a94ccea":"code","fe3b05f4":"code","21bf962c":"code","dbda4e04":"code","c43c57d4":"code","876ad528":"markdown"},"source":{"6a575491":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6895a7ad":"import pandas as pd\nimport seaborn as sb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport matplotlib.pyplot as plt","f4dcb52f":"#So now lets import our training and test datasets\ntrain=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')","3a01e847":"train.info()","c405a2fa":"#Lets check the count of null values in our training dataset\ntrain.isnull().sum()","febb534c":"#Assign y as survived column from training dataset\ny=train['Survived']","007ebb50":"#Cabin column is really intresting and can be quite usefull so we wont be deleting it and we'll replace the null values by any letter in\n#this case consider U and we will fill the null values for both train and test data\n\ntrain[\"Cabin\"].unique()\ntrain[\"Cabin\"] = train[\"Cabin\"].fillna('U')\ntrain[\"Cabin\"] = train[\"Cabin\"].apply(lambda x: x[0])\n\n\ntest[\"Cabin\"].unique()\ntest[\"Cabin\"] = test[\"Cabin\"].fillna('U')\ntest[\"Cabin\"] = test[\"Cabin\"].apply(lambda x: x[0])","a1c73375":"#Lets make us new column Family size which will give us the total number of family size for every passenger  \ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1","b01eedda":"#Name list contains the title which can be very usefull for survival prediction ,so we will extract the title from evey name and remove \n#the remaining data\n\ntrain['Name'] = train['Name'].map(lambda x: x.split(',')[1].split('.')[0].strip())\ntitles = train['Name'].unique()\n\ntest['Name'] = test['Name'].map(lambda x: x.split(',')[1].split('.')[0].strip())\ntitles1 = test['Name'].unique()","bbe98cce":"titles,titles1","52d877ad":"#Name Column\ntitle_train=pd.get_dummies(train['Name'],drop_first=True,prefix='Title')\ntrain=pd.concat([train,title_train],axis=1)\ntrain.drop(columns=['Name'],axis=1,inplace=True)\n\ntitle_test=pd.get_dummies(test['Name'],drop_first=True,prefix='Title')\ntest=pd.concat([test,title_test],axis=1)\ntest.drop(columns=['Name'],axis=1,inplace=True)","99c2a01e":"#Sibsp Column\nSib_train=pd.get_dummies(train['SibSp'],drop_first=True,prefix='Sib')\ntrain=pd.concat([train,Sib_train],axis=1)\ntrain.drop(columns=['SibSp'],axis=1,inplace=True)\n\nSib_test=pd.get_dummies(test['SibSp'],drop_first=True,prefix='Sib')\ntest=pd.concat([test,Sib_test],axis=1)\ntest.drop(columns=['SibSp'],axis=1,inplace=True)","fcbaafb9":"#Cabin Column\ncabin_train=pd.get_dummies(train['Cabin'],drop_first=True,prefix='Cabin')\ntrain=pd.concat([train,cabin_train],axis=1)\ntrain.drop(columns=['Cabin'],axis=1,inplace=True)\n\ncabin_test=pd.get_dummies(test['Cabin'],drop_first=True,prefix='Cabin')\ntest=pd.concat([test,cabin_test],axis=1)\ntest.drop(columns=['Cabin'],axis=1,inplace=True)","c9f319c6":"#Parch Column\nparch_train=pd.get_dummies(train['Parch'],drop_first=True,prefix='Parch')\ntrain=pd.concat([train,parch_train],axis=1)\ntrain.drop(columns=['Parch'],axis=1,inplace=True)\n\nparch_test=pd.get_dummies(test['Parch'],drop_first=True,prefix='Parch')\ntest=pd.concat([test,parch_test],axis=1)\ntest.drop(columns=['Parch'],axis=1,inplace=True)","5317d1e9":"train.Age.min(),train.Age.max()","574960c6":"#As we can see above the value of Age column ranges from 0.42 to 80.0.So we will create a new column Age Attributes that will \n#define the attribute of person by their age\n#1) For training data:-\ncondition_train=[\n    (train.Age<10),\n    (train.Age<20),\n    (train.Age<50),\n    (train.Age<80)\n    \n]\n\nAge_attributes_train=['minor','teenage','Adult','Old']\n\ntrain['Age_attributes']=np.select(condition_train,Age_attributes_train,default='Ages')\n\n#2) For testing data:-\ncondition_test=[\n    (test.Age<10),\n    (test.Age<20),\n    (test.Age<50),\n    (test.Age<80)\n    \n]\nAge_attributes_test=['minor','teenage','Adult','Old']\n\ntest['Age_attributes']=np.select(condition_test,Age_attributes_test,default='Ages')\n","59d02ad1":"#Now we will drop the age column as we've created a new column of Age Attributes\ntrain.drop(columns=['Age'],axis=1,inplace=True)\ntest.drop(columns=['Age'],axis=1,inplace=True)","a91cc9ad":"train.isnull().sum(),   test.isnull().sum()","816bcda5":"#Feature Encoding Continues:\nAge_train=pd.get_dummies(train['Age_attributes'],drop_first=True,prefix='Ages_of_people')\nAge_test=pd.get_dummies(test['Age_attributes'],drop_first=True,prefix='Ages_of_people')\n\ntrain=pd.concat([train,Age_train],axis=1)\ntest=pd.concat([test,Age_test],axis=1)\n","8766f4d1":"pdclass_train=pd.get_dummies(train['Pclass'],drop_first=True,prefix='Class')\npdclass_test=pd.get_dummies(test['Pclass'],drop_first=True,prefix='Class')\ntrain=pd.concat([train,pdclass_train],axis=1)\ntest=pd.concat([test,pdclass_test],axis=1)\ntrain.drop(columns=['Pclass'],axis=1,inplace=True)\ntest.drop(columns=['Pclass'],axis=1,inplace=True)\nsex=pd.get_dummies(train['Sex'],drop_first=True,prefix='Sex')\nsex1=pd.get_dummies(test['Sex'],drop_first=True,prefix='Sex')\nEmbarked=pd.get_dummies(train['Embarked'],drop_first=True,prefix='Embarked')\nEmbarked1=pd.get_dummies(test['Embarked'],drop_first=True,prefix='Embarked')\ntest=pd.concat([test,sex1,Embarked1],axis=1)\ntrain=pd.concat([train,sex,Embarked],axis=1)\ntrain.drop(columns=['Sex','Embarked'],axis=1,inplace=True)  \ntest.drop(columns=['Sex','Embarked'],axis=1,inplace=True)","c36e52e7":"train.isnull().sum()","a66ac84f":"#We'll drop the age attributes now as we've created dummy variables for it\ntrain.drop(columns=['Age_attributes'],axis=1,inplace=True)\ntest.drop(columns=['Age_attributes'],axis=1,inplace=True)","7bd4089f":"test.isnull().sum()","aab12872":"#Firstly we'll have to drop Parch_9 column from test dataset as this column doesnt exist in training data\ntest.drop(columns=['Parch_9'],axis=1,inplace=True)","87e850da":"test['Fare'].fillna(test['Fare'].mean(),inplace=True)","c73284a2":"#We wont be needing Ticket column as it has lot of unique values and feature encoding wont be possible, so we'll just dropit\ntrain.drop(columns=['Ticket'],axis=1,inplace=True)\ntest.drop(columns=['Ticket'],axis=1,inplace=True)","8fdb6d28":"#From the title column we've created we have a lot of new colums from train set that aren' present in test set\n#so we will delete those columns from our train dataset\ntrain.drop(columns=['Title_Mlle','Title_Col','Title_the Countess','Title_Jonkheer','Title_Major','Title_Lady','Title_Mlle','Title_Mme','Title_Sir'],inplace=True,axis=1)","3a22f0f6":"#Cabin_T is also an extra colun in train set which isnt present in test set so we will drop that as well\ntrain.drop(columns=['Cabin_T'],axis=1,inplace=True)","b118e2dd":"test.rename(columns={'Title_Dona':'Title_Don'},inplace=True)\ntrain.drop(columns=['Survived'],axis=1,inplace=True)","23217713":"#Alright our both datasets are ready\n#We'll just take a look at their shape\ntrain.shape,test.shape","a2901af3":"#This is he model that worked best for me and got me good accuracy\n#You can use it as well  :)\nleaderboard_model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=None,\n                                           n_jobs=-1,\n                                           verbose=1)\nleaderboard_model.fit(train,y)\nleaderboard_model.score(train,y)","7a94ccea":"#0.85 now thats a great accuracy for a begineer\n#Make sure to carefully follow the steps throghout to get this accuracy","fe3b05f4":"y_pred=leaderboard_model.predict(test)","21bf962c":"df_gendercsv=pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':y_pred})","dbda4e04":"df_gendercsv.to_csv('gender.csv',index=False)","c43c57d4":"#Thats it guys thanks for sticking throghout\n#Bye for now","876ad528":"> #Feature Encoding by using Get dummies.You can use one hot encoding as well"}}