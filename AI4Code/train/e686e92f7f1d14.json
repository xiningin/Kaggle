{"cell_type":{"0aed459c":"code","04f14810":"code","b7d5653c":"code","060e5b10":"code","6d858cab":"code","38565f95":"code","931a307b":"code","e907381f":"code","106b4dae":"code","7d0ae48f":"code","bc557113":"code","e915f2a3":"code","17ef21f6":"code","300a1b0d":"code","4d4c5c21":"code","fa7f1533":"code","2604a232":"code","bc0ee1d9":"code","edef8027":"code","5cd08e23":"code","8363fd3a":"code","cb7af2b6":"markdown","bb6616a3":"markdown","2b5746b6":"markdown","6167e2ac":"markdown","d1636ee7":"markdown","52e48d93":"markdown","d121e16b":"markdown","4794143f":"markdown","5d8c266e":"markdown","fc34e924":"markdown","b04a7426":"markdown","455cca69":"markdown","09ee8050":"markdown","91099744":"markdown","6729eb5a":"markdown","d974c832":"markdown","20beb3e7":"markdown","701c4226":"markdown","2b94bf32":"markdown","280d1e65":"markdown","9ff21aea":"markdown","93883ff3":"markdown","896b3877":"markdown","e0cc97a6":"markdown","4978cb20":"markdown"},"source":{"0aed459c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n#import packages\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","04f14810":"#import helper packages from keras\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array","b7d5653c":"#test image 1\ntest1 = load_img('..\/input\/flowers-recognition\/flowers\/daisy\/100080576_f52e8ee070_n.jpg')\nplt.imshow(test1)\nx = img_to_array(test1)\nprint(type(x))\nprint(x.shape)\n\n","060e5b10":"test2 = load_img('..\/input\/flowers-recognition\/flowers\/dandelion\/1128626197_3f52424215_n.jpg')\nplt.imshow(test2)\nprint(img_to_array(test2).shape)\n","6d858cab":"test3 = load_img('..\/input\/flowers-recognition\/flowers\/rose\/12572786553_634868f7f2_n.jpg')\nplt.imshow(test3)\nprint(img_to_array(test3).shape)","38565f95":"test4 = load_img('..\/input\/flowers-recognition\/flowers\/sunflower\/12471791574_bb1be83df4.jpg')\nplt.imshow(test4)\nprint(img_to_array(test4).shape)","931a307b":"test5 = load_img('..\/input\/flowers-recognition\/flowers\/tulip\/113291410_1bdc718ed8_n.jpg')\nplt.imshow(test5)\nprint(img_to_array(test5).shape)","e907381f":"import os\n\nnumber_classes = {'daisy': len(os.listdir('..\/input\/flowers-recognition\/flowers\/daisy\/')),\n'dandelion': len(os.listdir('..\/input\/flowers-recognition\/flowers\/dandelion\/')),\n'rose': len(os.listdir('..\/input\/flowers-recognition\/flowers\/rose\/')),\n'sunflower': len(os.listdir('..\/input\/flowers-recognition\/flowers\/sunflower\/')),\n'tulip' : len(os.listdir('..\/input\/flowers-recognition\/flowers\/tulip\/'))  }\n\nplt.bar(number_classes.keys(), number_classes.values(), width = .5)\nplt.title(\"Number of Images by Class\")\nplt.xlabel('Class Name')\nplt.ylabel('# Images')\n\n#uneven distribution\n#resample for equal distributions?","106b4dae":"# Dask is used for paralell computing\nfrom dask import bag\nfrom dask import diagnostics\nfrom PIL import Image\n\ndirectories = {'daisy': '..\/input\/flowers-recognition\/flowers\/daisy\/',\n'dandelion':'..\/input\/flowers-recognition\/flowers\/dandelion\/',\n'rose': '..\/input\/flowers-recognition\/flowers\/rose\/',\n'sunflower': '..\/input\/flowers-recognition\/flowers\/sunflower\/',\n'tulip' : '..\/input\/flowers-recognition\/flowers\/tulip\/'  }\n\ndef get_dims(file):\n    '''Returns dimenstions for an RBG image'''\n    im = Image.open(file)\n    arr = np.array(im)\n    h,w,d = arr.shape\n    return h,w\n\nfor n,d in directories.items():\n    filepath = d\n    filelist = [filepath + f for f in os.listdir(filepath)]\n    dims = bag.from_sequence(filelist).map(get_dims)\n    with diagnostics.ProgressBar():\n        dims = dims.compute()\n        dim_df = pd.DataFrame(dims, columns=['height', 'width'])\n        sizes = dim_df.groupby(['height', 'width']).size().reset_index().rename(columns={0:'count'})\n        sizes.plot.scatter(x='width', y='height');\n        plt.title('Image Sizes (pixels) | {}'.format(n))","7d0ae48f":"train_datagen = ImageDataGenerator(rescale = 1.\/255, validation_split = 0.2)\n\ntrain_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(256, 256),\n    batch_size=10,\n    class_mode = 'categorical',\n    subset='training') # set as training data\n\nvalid_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(256, 256),\n    batch_size=10,\n    class_mode = 'categorical',\n    subset='validation') # set as training data","bc557113":"inputs = keras.Input(shape = (256,256,3))\nconv = keras.layers.Conv2D(filters = 32,padding = 'same', kernel_size = (3,3) , activation = 'relu')(inputs)\nconv = keras.layers.Conv2D(filters = 32, kernel_size = (3,3) , activation = 'relu')(inputs)\npool = keras.layers.MaxPooling2D(pool_size = (2,2))(conv)\nflat = keras.layers.Flatten()(pool)\ndense = keras.layers.Dense(64, activation = 'relu')(flat)\noutput = keras.layers.Dense(5, activation = 'softmax')(dense)\n\nmodel = keras.Model(inputs, output)\n\noptimizer = keras.optimizers.SGD(learning_rate=0.002, momentum=0.8)\nloss = keras.losses.CategoricalCrossentropy(from_logits=True)\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\nprint(model.summary())\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=10,\n    validation_data=valid_generator,\n    validation_steps=50\n)","e915f2a3":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   rotation_range=40,\n                                   width_shift_range=0.4,\n                                   height_shift_range=0.4,\n                                   shear_range=0.2,\n                                   zoom_range=0.3,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest',\n                                   validation_split = 0.2)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(256, 256),\n    batch_size=10,\n    class_mode = 'categorical',\n    subset='training', shuffle = True, seed = 69420) # set as training data\n\nvalid_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(256, 256),\n    batch_size=10,\n    class_mode = 'categorical',\n    subset='validation', shuffle = True, seed = 69420) # set as training data","17ef21f6":"inputs = keras.Input(shape = (256,256,3))\nconv = keras.layers.Conv2D(filters = 32,padding = 'same', kernel_size = (3,3) , activation = 'relu')(inputs)\nconv = keras.layers.Conv2D(filters = 32, kernel_size = (3,3) , activation = 'relu')(inputs)\npool = keras.layers.MaxPooling2D(pool_size = (2,2))(conv)\nflat = keras.layers.Flatten()(pool)\ndense = keras.layers.Dense(64, activation = 'relu')(flat)\noutput = keras.layers.Dense(5, activation = 'softmax')(dense)\n\nmodel = keras.Model(inputs, output)\n\noptimizer = keras.optimizers.SGD(learning_rate=0.002, momentum=0.8)\nloss = keras.losses.CategoricalCrossentropy(from_logits=True)\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\nprint(model.summary())\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=10,\n    validation_data=valid_generator,\n    validation_steps=50\n)","300a1b0d":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   rotation_range=40,\n                                   width_shift_range=0.4,\n                                   height_shift_range=0.4,\n                                   shear_range=0.2,\n                                   zoom_range=0.3,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest',\n                                   validation_split = 0.2)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(256, 256),\n    batch_size = 32,\n    class_mode = 'categorical',\n    subset='training', shuffle = True, seed = 69420) # set as training data\n\nvalid_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(256, 256),\n    batch_size = 32,\n    class_mode = 'categorical',\n    subset='validation', shuffle = True, seed = 69420) # set as training data\n\ninputs = keras.Input(shape = (256,256,3))\nconv = keras.layers.Conv2D(filters = 32,padding = 'same', kernel_size = (3,3) , activation = 'relu')(inputs)\nconv = keras.layers.Conv2D(filters = 32, kernel_size = (3,3) , activation = 'relu')(inputs)\npool = keras.layers.MaxPooling2D(pool_size = (2,2))(conv)\nflat = keras.layers.Flatten()(pool)\ndense = keras.layers.Dense(64, activation = 'relu')(flat)\noutput = keras.layers.Dense(5, activation = 'softmax')(dense)\n\nmodel_small = keras.Model(inputs, output)\n\n#optimizer = keras.optimizers.SGD(learning_rate=0.002, momentum=0.8)\nloss = keras.losses.CategoricalCrossentropy(from_logits=True)\n\nmodel_small.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n\nprint(model_small.summary())\n\nhistory = model_small.fit(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=10,\n    validation_data=valid_generator,\n    validation_steps=20\n)","4d4c5c21":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   rotation_range=40,\n                                   width_shift_range=0.4,\n                                   height_shift_range=0.4,\n                                   shear_range=0.2,\n                                   zoom_range=0.3,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest',\n                                   validation_split = 0.2)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(256, 256),\n    batch_size = 32,\n    class_mode = 'categorical',\n    subset='training', shuffle = True, seed = 69420) # set as training data\n\nvalid_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(256, 256),\n    batch_size = 32,\n    class_mode = 'categorical',\n    subset='validation', shuffle = True, seed = 69420) # set as training data\n\ninputs = keras.Input(shape = (256,256,3))\nconv1 = keras.layers.Conv2D(filters = 32, padding = 'same', kernel_size = (3,3) , activation = 'relu')(inputs)\nconv2 = keras.layers.Conv2D(filters = 32, kernel_size = (3,3) , activation = 'relu')(conv1)\npool1 = keras.layers.MaxPooling2D(pool_size = (2,2))(conv2)\ndrop1 = keras.layers.Dropout(0.3)(pool1)\n\nconv3 = keras.layers.Conv2D(filters = 64, padding = 'same', kernel_size = (3,3) , activation = 'relu')(drop1)\nconv4 = keras.layers.Conv2D(filters = 64, kernel_size = (3,3) , activation = 'relu')(conv3)\npool2 = keras.layers.MaxPooling2D(pool_size = (2,2))(conv4)\ndrop2 = keras.layers.Dropout(0.3)(pool2)\n\n\nconv5 = keras.layers.Conv2D(filters = 128, padding = 'same', kernel_size = (3,3) , activation = 'relu')(drop2)\nconv6 = keras.layers.Conv2D(filters = 128, kernel_size = (3,3) , activation = 'relu')(conv5)\npool3 = keras.layers.MaxPooling2D(pool_size = (2,2))(conv6)\n\n\nflat = keras.layers.Flatten()(pool3)\n\ndense1 = keras.layers.Dense(512, activation = 'relu')(flat)\ndrop4 = keras.layers.Dropout(0.3)(dense1)\ndense2 = keras.layers.Dense(128, activation = 'relu')(drop4)\noutput = keras.layers.Dense(5, activation = 'softmax')(dense2)\n\nmodel_full = keras.Model(inputs, output)\n\noptimizer = keras.optimizers.SGD(learning_rate=0.002, momentum=0.8)\nloss = keras.losses.CategoricalCrossentropy(from_logits=False)\n\nmodel_full.compile(optimizer = optimizer, loss=loss, metrics=['accuracy'])\n\nprint(model_full.summary())\n\nhistory = model_full.fit(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=10,\n    validation_data=valid_generator,\n    validation_steps=20\n)","fa7f1533":"from tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.xception import preprocess_input\nfrom tensorflow.keras.applications.xception import decode_predictions","2604a232":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   rotation_range=40,\n                                   width_shift_range=0.4,\n                                   height_shift_range=0.4,\n                                   shear_range=0.2,\n                                   zoom_range=0.3,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest',\n                                   validation_split = 0.2)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(299, 299), #need to use 299 here because of the requirements of the Xception model\n    batch_size=32,\n    class_mode = 'categorical',\n    subset='training', shuffle = True, seed = 69420) # set as training data\n\nvalid_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(299, 299), #need to use 299 here because of the requirements of the Xception model\n    batch_size=32,\n    class_mode = 'categorical',\n    subset='validation', shuffle = True, seed = 69420) # set as validation data","bc0ee1d9":"base_model = Xception(weights = 'imagenet', include_top = False, input_shape = (299,299,3))\n\nbase_model.trainable = False\n\ninputs = keras.Input(shape=(299,299,3))\nbase = base_model(inputs, training = False)\nvectors = keras.layers.GlobalAveragePooling2D()(base)\n\nhidden1 = keras.layers.Dense(512, activation = 'relu')(vectors)\nhidden2 = keras.layers.Dense(128, activation = 'relu')(hidden1)\nhidden3 = keras.layers.Dense(32, activation = 'relu')(hidden2)\n\noutputs = keras.layers.Dense(5, activation = 'softmax')(hidden3)\n\npreTrainModel = keras.Model(inputs, outputs)\noptimizer = keras.optimizers.SGD(learning_rate=0.002, momentum=0.8)\nloss = keras.losses.CategoricalCrossentropy(from_logits=False)\n\npreTrainModel.compile(optimizer = optimizer, loss=loss, metrics=['accuracy'])\n\nprint(preTrainModel.summary())\n\nhistory = preTrainModel.fit(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=10,\n    validation_data=valid_generator,\n    validation_steps=20\n)","edef8027":"def build_model(learning_rate = 0.002, momentum = 0.8):\n    base_model = Xception(weights = 'imagenet', include_top = False, input_shape = (299,299,3))\n\n    base_model.trainable = False\n\n    inputs = keras.Input(shape=(299,299,3))\n    base = base_model(inputs, training = False)\n    vectors = keras.layers.GlobalAveragePooling2D()(base)\n\n    hidden1 = keras.layers.Dense(512, activation = 'relu')(vectors)\n    hidden2 = keras.layers.Dense(128, activation = 'relu')(hidden1)\n    hidden3 = keras.layers.Dense(32, activation = 'relu')(hidden2)\n\n    outputs = keras.layers.Dense(5, activation = 'softmax')(hidden3)\n\n    preTrainModel = keras.Model(inputs, outputs)\n    optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n    loss = keras.losses.CategoricalCrossentropy(from_logits=False)\n\n    preTrainModel.compile(optimizer = optimizer, loss=loss, metrics=['accuracy'])\n\n    return preTrainModel\n\nscores = {}\nfor lr in [0.1, 0.01, 0.005]:\n    for m in [0.25, 0.5, 0.75]:\n        \n        history = build_model(lr, m).fit(train_generator,\n                                        steps_per_epoch=100,\n                                        epochs=5,\n                                        validation_data=valid_generator,\n                                        validation_steps=20\n                                        )\n        scores[lr,m] = history.history;","5cd08e23":"#scores\nfor lr,m in scores:\n    avgAcc = np.mean(scores[lr, m]['accuracy']).round(3)\n    avgLoss = np.mean(scores[lr, m]['loss']).round(3)\n    avgValAcc = np.mean(scores[lr, m]['val_accuracy']).round(3)\n    avgValLoss = np.mean(scores[lr, m]['val_loss']).round(3)\n    print(f'Learning rate: {lr}, \\t Momentum: {m}, \\t Average Accuracy: {avgAcc} , \\t Average Val Accuracy: {avgValAcc} , \\t Average Loss: {avgLoss} , \\t Average Val Loss: {avgValLoss}')\n    \n    \nplt.plot(scores[0.01,0.75]['accuracy'], label='accuracy')\nplt.plot(scores[0.01,0.75]['val_accuracy'], label='validation accuracy')\nplt.xticks(np.arange(10))\nplt.legend()\n","8363fd3a":"from tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.xception import preprocess_input\nfrom tensorflow.keras.applications.xception import decode_predictions\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   rotation_range=40,\n                                   width_shift_range=0.4,\n                                   height_shift_range=0.4,\n                                   shear_range=0.2,\n                                   zoom_range=0.3,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest',\n                                   validation_split = 0.2)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(299, 299), #need to use 299 here because of the requirements of the Xception model\n    batch_size=32,\n    class_mode = 'categorical',\n    subset='training', shuffle = True, seed = 69420) # set as training data\n\nvalid_generator = train_datagen.flow_from_directory(\n    '..\/input\/flowers-recognition\/flowers\/',\n    target_size=(299, 299), #need to use 299 here because of the requirements of the Xception model\n    batch_size=32,\n    class_mode = 'categorical',\n    subset='validation', shuffle = True, seed = 69420) # set as validation data\n\nbase_model = Xception(weights = 'imagenet', include_top = False, input_shape = (299,299,3))\n\nbase_model.trainable = False\n\ninputs = keras.Input(shape=(299,299,3))\nbase = base_model(inputs, training = False)\nvectors = keras.layers.GlobalAveragePooling2D()(base)\n\nhidden1 = keras.layers.Dense(512, activation = 'relu')(vectors)\nhidden2 = keras.layers.Dense(128, activation = 'relu')(hidden1)\nhidden3 = keras.layers.Dense(32, activation = 'relu')(hidden2)\n\noutputs = keras.layers.Dense(5, activation = 'softmax')(hidden3)\n\nfinalModel = keras.Model(inputs, outputs)\noptimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.75)\nloss = keras.losses.CategoricalCrossentropy(from_logits=False)\n\nfinalModel.compile(optimizer = optimizer, loss=loss, metrics=['accuracy'])\n\nprint(finalModel.summary())\n\ncheckpoint = keras.callbacks.ModelCheckpoint(\n    'xception_fin_{epoch:02d}_{val_accuracy:.3f}.h5',\n    save_best_only=True,\n    monitor='val_accuracy',\n    mode='max'\n)\n\nhistory = finalModel.fit(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=30,\n    validation_data=valid_generator,\n    validation_steps=20,\n    callbacks=[checkpoint]\n\n)","cb7af2b6":"Based on above graphs, we'll resize images to 200x200. We want to resize images to the smallest dimension, in order to not increase the size of any images. Increasing the size of an image requires stretching the pixels in the images, and this will create noise in the images. The 200x200 resize reduces the size of most images, and only increases the height dimension of a few images. This should be a decently good dimension to resize to.\n\n### Note:\nAfter a few iterations of model training, the images were instead resized to 256x256 in order to allow more training data to be retained after resizing.","bb6616a3":"# Try Pre-Trained models available within Keras\n\nThere are various pre-trained image classification models available within Keras, most of them have been pre-trained on the ImageNet benchmarking set. \n\nThis link covers some of these models in a bit more detail: https:\/\/www.pyimagesearch.com\/2017\/03\/20\/imagenet-vggnet-resnet-inception-xception-keras\/\nThe keras documentation for the models available can be found here: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/\n\nFor this project we're going to go with Xception, besides the fact that it was covered during class lectures, it's also one of the smaller models in terms of parameters to load, this is important for deploying.","2b5746b6":"### learning rate and momentum","6167e2ac":"# Import data with augmentations","d1636ee7":"### test image 5 - tulip\n","52e48d93":"# use keras to import and process images","d121e16b":"# Change Batch Size","4794143f":"### test image 1 - daisy","5d8c266e":"Final parameters: Learning Rage = 0.01, Momentum = 0.75","fc34e924":"# tune parameters of 'final' model","b04a7426":"# Create image import flow","455cca69":"### test image 4 - sunflower","09ee8050":"### test image 3 - rose","91099744":"With the above distribution of targets we're not going to resample the targets, the distribution is near-uniform enough.","6729eb5a":"## import packages","d974c832":"# Add more layers to model and retrain\n\n## Try out multiple Conv layers, with dropout and pooling in between to extract more features","20beb3e7":"# Train Final Model\n## Run the final model based on Xception with Learning Rate = 0.01 and Momentum = 0.75\n\n### Run model for 30 epochs as final training\n\n### Additional parameters and tweaks: \n* Data Augmentation\n* Shuffle input data","701c4226":"## Import packages required for Exception pre-trained model","2b94bf32":"## Load all images - no resize","280d1e65":"## Pull in some test images to make sure everything works and get a feel for the data","9ff21aea":"## Class Distribution","93883ff3":"# Create, Fit and Evaluate Baseline Model","896b3877":"# Rerun previous model on augmentated data","e0cc97a6":"## Import Data using flow_from_directory and ImageDataGenerator","4978cb20":"### test image 2 - dandelion"}}