{"cell_type":{"6a8502ca":"code","699a6313":"code","1274b759":"code","f115f619":"code","af89e403":"code","7c91b681":"code","2147c9b3":"code","47edf4fe":"code","59827a1a":"code","97d302a7":"code","22de514b":"code","354223a5":"code","a0914f83":"code","139a7b30":"code","08cff0c7":"code","02bb6b75":"code","d9c63c43":"code","6e1c0d66":"code","96ad974c":"code","99dfd276":"code","080c85f2":"code","f7bcba13":"code","743b2e0f":"code","84d05956":"markdown","da10792a":"markdown","8095bbe9":"markdown"},"source":{"6a8502ca":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom bs4 import BeautifulSoup             \nfrom nltk.corpus import stopwords # Import the stop word list\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU,Conv1D,MaxPooling1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import load_model","699a6313":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubmit_template = pd.read_csv('..\/input\/sample_submission.csv', header = 0)","1274b759":"train.head()","f115f619":"list_sentences = train[\"comment_text\"]\nlist_sentences_test = test[\"comment_text\"]","af89e403":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features,char_level=True)","7c91b681":"tokenizer.fit_on_texts(list(list_sentences))","2147c9b3":"list_tokenized = tokenizer.texts_to_sequences(list_sentences)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","47edf4fe":"maxlen = 500\nX_t = pad_sequences(list_tokenized, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","59827a1a":"totalNumWords = [len(one_comment) for one_comment in list_tokenized]\nplt.hist(totalNumWords)\nplt.show()","97d302a7":"inp = Input(shape=(maxlen, ))\ninp","22de514b":"embed_size = 240\nx = Embedding(len(tokenizer.word_index)+1, embed_size)(inp)","354223a5":"x = Conv1D(filters=100,kernel_size=4,padding='same', activation='relu')(x)","a0914f83":"x=MaxPooling1D(pool_size=4)(x)","139a7b30":"x = Bidirectional(GRU(60, return_sequences=True,name='lstm_layer',dropout=0.2,recurrent_dropout=0.2))(x)","08cff0c7":"x = GlobalMaxPool1D()(x)","02bb6b75":"x = Dense(50, activation=\"relu\")(x)","d9c63c43":"x = Dropout(0.2)(x)\nx = Dense(6, activation=\"sigmoid\")(x)","6e1c0d66":"model = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                 metrics=['accuracy'])","96ad974c":"model.summary()","99dfd276":"X_train, X_test, y_train, y_test = train_test_split(X_t, train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]], test_size = 0.10, random_state = 42)","080c85f2":"batch_size = 32\nepochs = 6\nmodel.fit(X_train,y_train, batch_size=batch_size, epochs=epochs,validation_data=(X_test,y_test),verbose=2)","f7bcba13":"y_submit = model.predict(X_te,batch_size=batch_size,verbose=1)","743b2e0f":"y_submit[np.isnan(y_submit)]=0\nsample_submission = submit_template\nsample_submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_submit\nsample_submission.to_csv('submission.csv', index=False)","84d05956":"Just in case you are wondering, the reason why I used 500 is because most of the number of characters in a sentence falls within 0 to 500:","da10792a":"Finally, we can start buliding our model.\n\nFirst, we set up our input layer. As mentioned in the Keras documentation, we have to include the shape for the very first layer and Keras will automatically derive the shape for the rest of the layers.","8095bbe9":"Then we pass it to the max pooling layer that applies the max pool operation on a window of every 4 characters. And that is why we get an output of (num of sentences X 125 X 100) matrix."}}