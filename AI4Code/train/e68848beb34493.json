{"cell_type":{"a98d0614":"code","8684c94a":"code","894f6285":"code","8eb463bc":"code","5b275c10":"code","f9f6cc84":"code","f2827ac5":"code","2d7a0cd6":"code","98b4f827":"code","2462e0fc":"code","e1717206":"code","b32d2bfc":"code","776058f9":"code","e2d32db6":"code","0bdef5fe":"code","fe88ddbc":"code","2b400e66":"code","a9de2e88":"code","3e29f535":"code","093b4f0e":"code","1dbce5c5":"code","5d6ee3b0":"code","6b94f031":"code","dc804bd3":"code","111ca5e8":"code","5d82b3dd":"code","a684e0cf":"code","ee3f8e79":"code","edf8b885":"code","329a709e":"code","153a221f":"code","dbc5983c":"code","71213863":"code","d08724ed":"code","482eb775":"code","3a581c8a":"code","c26dee9e":"code","f68d847b":"code","62a463fb":"code","c5681aa4":"code","655d3b93":"code","43241b31":"code","b30310af":"code","be725e15":"code","913732ad":"code","35b903bb":"code","9821908c":"code","8a561c48":"code","3281bb9e":"code","f82a755f":"code","5afef079":"code","062cf9d3":"code","8f6f85c7":"code","245e016b":"code","208a2dec":"code","1a989e87":"markdown","1e562a2a":"markdown","fe405f7e":"markdown","936520a0":"markdown","520d531a":"markdown","2bc515a1":"markdown","2c51a699":"markdown","73485ce3":"markdown","f07fb3f5":"markdown"},"source":{"a98d0614":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","8684c94a":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nfrom sklearn.cluster import KMeans\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)","894f6285":"# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))","8eb463bc":"# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv(data_dir + '\/train.csv')\n    test = pd.read_csv(data_dir + '\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","5b275c10":"# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    # \u304d\u3063\u3068100\u79d2\u523b\u307f\u306f\u3044\u3044\u7d50\u679c\u3092\u5f97\u308c\u306a\u304b\u3063\u305f\u306e\u3060\u308d\u3046\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n","f9f6cc84":"# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    #\n    # \u3053\u3053\u304b\u30b9\u30b3\u30a2\u306b\u76f8\u5f53\u5f71\u97ff\u3057\u3066\u308b\u3068\u8a18\u8f09\u3057\u3066\u308b\n    # \u306a\u3093\u3067\u3053\u308c\u304c\u52b9\u3044\u3066\u308b\u304b\u306f\u3044\u307e\u306e\u3068\u3053\u308d\u4e0d\u660e\n    # \u304a\u3063\u3066\u8abf\u3079\u308b\u3053\u3068\u306b\u3059\u308b\u3002\n    #\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        # \u5dee\u5206\u3092\u52d5\u3044\u305f\u5f8c\u306e\u4fa1\u683c\u3067\u5272\u3063\u3066*100\u3057\u3066\u308b\n        # \u5909\u52d5\u984d\u3092\u5909\u52d5\u5f8c\u306e\u4fa1\u683c\u3067\u5272\u308b\u3053\u3068\u3067\u6bd4\u7387\u306b\u3057\u3066\u3044\u308b(\u5c0f\u3055\u3044\u4fa1\u683c\u3060\u3068\u3053\u306e\u5024\u306f\u304a\u304a\u304d\u304f\u306a\u308b)\n        val = (df_diff\/price[1:])*100\n        # \u305d\u308c\u306bvol\u3092\u304b\u3051\u3066\u308b\u306e\u3067\u5909\u52d5\u6bd4\u7387\u306b\u5927\u304d\u3055\u3092\u304b\u3051\u308b\u306e\u3067\u3053\u306e\u5024\u304c\u5927\u304d\u3044\u3068\u5024\u306e\u6bd4\u7387\u304c\u5927\u304d\u304f\u52d5\u3044\u305f\u3053\u3068\u306b\u306a\u308b\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    # time_id\u306b\u5bfe\u5fdc\u3059\u308bdf\u3092\u629c\u304d\u51fa\u3059\u3002\n    # time_id\u306b\u7d71\u8a08\u6307\u6a19\u306a\u306e\u3067\u30ea\u30fc\u30af\u3057\u3066\u3044\u308b\n    \n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]\n        \n        # power\u3063\u3066\u547c\u3070\u308c\u308b\u6307\u6a19\u3092\u5f97\u308b\u3002\n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        \n        # \u5e73\u5747\u4ee5\u4e0a\u306eprice\u306e\u5408\u8a08\u3068\u5e73\u5747\u4ee5\u4e0b\u306eprice\u306e\u5408\u8a08\u5024\n        # \u5168\u304f\u3082\u3063\u3066\u3044\u3089\u306a\u3044\u30c7\u30fc\u30bf\u306b\u3057\u304b\u898b\u3048\u306a\u3044\u3002\n        # \u5916\u308c\u5024\u306e\u5f71\u97ff\u5f15\u304f\u6c17\u304c\u3059\u308b\u3057\n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        \n        #\n        # \u6b63\u306e\u5dee\u5206\u306e\u5408\u8a08\u5024\u3068\u8ca0\u306e\u5dee\u5206\u306e\u5408\u8a08\u5024\n        # \u3044\u308b\u306e\u304b\u3053\u308c\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        \n        \n        # \u504f\u5dee\u306e\u4e2d\u592e\u5024\n        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))  \n        # \u4fa1\u683c\u306e\u4e8c\u4e57\u306e\u5e73\u5747\u5024\n        energy = np.mean(df_id['price'].values**2)\n        # \u7b2c3-\u7b2c\uff11\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # size\u306b\u5bfe\u3057\u3066\u3082\u3046\u3048\u3068\u540c\u69d8\u306e\u3053\u3068\n        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n        \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150','time_id'], axis = 1, inplace = True)\n    \n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n","f2827ac5":"# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n#     vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility',\n#                 'log_return1_realized_volatility_600', 'log_return2_realized_volatility_600', \n#                 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n# #                 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n#                 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n# #                 'log_return1_realized_volatility_100', 'log_return2_realized_volatility_100', \n#                 'trade_log_return_realized_volatility',\n#                 'trade_log_return_realized_volatility_600', \n#                 'trade_log_return_realized_volatility_400',\n# #                 'trade_log_return_realized_volatility_300',\n# #                 'trade_log_return_realized_volatility_100',\n#                 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","2d7a0cd6":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","98b4f827":"# \u91cd\u8981\u5ea6\u89e3\u6790\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df","2462e0fc":"def calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    \n    return mean_df","e1717206":"# \u753b\u50cf\u4fdd\u5b58\u7528\nimport matplotlib.pyplot as plt\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(8, 12)):\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()","b32d2bfc":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","776058f9":"train.shape","e2d32db6":"pd.to_pickle(train,'train(307)_notau_noKNN.pkl')\npd.to_pickle(test,'test(307)_notau_noKNN.pkl')","0bdef5fe":"train = pd.read_pickle('.\/train(307)_notau_noKNN.pkl')\ntest = pd.read_pickle('test(307)_notau_noKNN.pkl')\ntrain.shape","fe88ddbc":"test.shape","2b400e66":"# making agg features\n\n# time_id\u6bce\u306estockid\u6bce\u306etarget\u5909\u6570\u306e\u4e00\u89a7\ntrain_p = pd.read_csv(data_dir + '\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n# \ncorr = train_p.corr()\n\n# stick_id\u306e\u76f8\u95a2\u4fc2\u6570\u306eindex\u3001\u3064\u307e\u308astockid\nids = corr.index\n\n# 7\u306b\u95a2\u3057\u3066\u306f\u8abf\u3079\u308b(\u30b7\u30eb\u30a8\u30c3\u30c8\u56f3\u3068\u304b\u3067)\n# time_id\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3068\u3057\u305f\u6642\u306estock_id\u76f8\u95a2\u4fc2\u6570\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n# print(kmeans.labels_)\n\n\n# l\u306brange(7)\u3068\u7b49\u3057\u3044\u30af\u30e9\u30b9\u30bf\u3092\u64ae\u3063\u305f\u6642\u306estock_id\u3092\u683c\u7d0d\u3057\u3066\u308b\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    # stock_id\u304c\u6307\u5b9a\u3055\u308c\u3066\u308b\u30af\u30e9\u30b9\u30bf\u3068\u540c\u3058\u3082\u306e(\u5168\u4f53\u30b5\u30f3\u30d7\u30eb)\u3092\u5f15\u3044\u3066\u304f\u308b\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    # time_id\u6bce\u306b\u5e73\u5747\u5024\u3092\u53d6\u308b(\u7570\u306a\u308bstock_id\u3067\u3082\u30af\u30e9\u30b9\u30bf\u304c\u540c\u3058\u306a\u3082\u306e\u540c\u58eb\u306e\u5e73\u5747\u5024\u306b\u306a\u308b)\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    \n    #\n    # stock_id\u305d\u306e\u3082\u306e\u306b\u3044\u307f\u304c\u306a\u304f\u306a\u3063\u305f\u306e\u3067\u6ce8\u610f\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\n    # stock_id\u3092\u30af\u30bf\u30b9\u30bfid\u306b\u5909\u66f4\u3057\u3066\u305d\u306e\u5f8c\u306bc1?\u3063\u3066\u3044\u3046\u306e\u3092\u3064\u3051\u3066\u308b\n    #\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","a9de2e88":"matTest = []\nmat = []\nkmeans = []","3e29f535":"#mat2 #= mat1.pivot(index='time_id', columns='stock_idmat2\n# \u4f55\u3067\u305d\u3093\u306a\u3053\u3068\u3057\u305f\u3093\u3084\u7b11\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","093b4f0e":"# \u3053\u306e\u66f8\u304d\u65b9\u4fbf\u5229\u3001\u899a\u3048\u3066\u304a\u304f\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","1dbce5c5":"# \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u7d50\u679c\u306e\u4f7f\u3046\u7279\u5fb4\u91cf\u3060\u3051\u629c\u304d\u51fa\u3057\u3066\u304d\u3066\u308b\uff0810\/314\uff09\n# 2.5\u306b\u304b\u3093\u3057\u3066\u306f\u30af\u30e9\u30b9\u30bf\u306e\u4e2d\u304c\u3042\u307e\u308a\u306b\u5c0f\u3055\u3044\u306e\u3067\u9664\u304f\n# \u306a\u305c\u3001\u3053\u306e\u7279\u5fb4\u91cf\u306b\u3057\u305f\u306e\u304b\u306f\u4e0d\u660e\u306e\u305f\u3081\u3001\u8ffd\u52a0\u5b9f\u9a13\u304c\u5fc5\u8981\n# log_return2\u3092\u524a\u3063\u305f\u7406\u7531\u306f\u308f\u304b\u3089\u3093\uff08\u304a\u305d\u3089\u304f\u76f8\u95a2\u304c\u4f3c\u3061\u3083\u3046\u304b\u3089\uff09\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_2c1',\n     'log_return1_realized_volatility_3c1',     \n     'log_return1_realized_volatility_4c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_2c1',\n     'total_volume_mean_3c1', \n     'total_volume_mean_4c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_2c1',\n     'trade_size_mean_3c1', \n     'trade_size_mean_4c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_2c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_2c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_2c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_2c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_2c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_2c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n] ","5d6ee3b0":"# train\u3068\u304f\u3063\u3064\u3051\u308b\ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')","6b94f031":"train.shape","dc804bd3":"train.head()","111ca5e8":"test.head()","5d82b3dd":"test = pd.merge(test,mat2[nnn],how='left',on='time_id')","a684e0cf":"train.shape","ee3f8e79":"test.shape","edf8b885":"test.head()","329a709e":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","153a221f":"seed = 29\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} ","dbc5983c":"from sklearn.model_selection import GroupKFold\ngain_importance_list = []\nsplit_importance_list = []\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # \u3053\u306e\u66f8\u304d\u65b9\u3059\u308b\u3053\u3068\u3067\u3001\u5168\u30c7\u30fc\u30bf\u3092OOf\u306b\u3057\u3066rmspe\u304c\u6c42\u3081\u3089\u308c\u308b\u3001\n    # \u899a\u3048\u3066\u304a\u3044\u305f\u65b9\u304c\u3044\u3044\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) \/ 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","71213863":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean groupkfold 352\u3000KNN notau.csv', index=False)","d08724ed":"train = pd.read_pickle('.\/train(307)_notau_noKNN.pkl')\ntest = pd.read_pickle('.\/test(307)_notau_noKNN.pkl')","482eb775":"# \u3053\u3063\u3061\u306f\u4fa1\u683c\u304c\u52d5\u304f\u3069\u3046\u3053\u3046\u3067\u306f\u306a\u304f\u3066\u3001\u53d6\u5f15\u56de\u6570\u306e\u5408\u8a08\u306e\u5024\n# \u5358\u4f4d\u3092\u305d\u308d\u3048\u308b\u306e\u3068\u3001\u50be\u5411\u304c\u306a\u3093\u3068\u306a\u304f\u307f\u3048\u308b\u306e\u304b\uff01\n\ntrain['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\ntrain['size_tau2_450'] = np.sqrt( 0.25\/ train['trade_order_count_sum'] )\ntest['size_tau2_450'] = np.sqrt( 0.25\/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\ntrain['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\ntest['size_tau2_150'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_600-450'] = train['size_tau2_450'] - train['size_tau2']\ntest['size_tau2_600-450'] = test['size_tau2_450'] - test['size_tau2']\ntrain['size_tau2_600-300'] = train['size_tau2_300'] - train['size_tau2']\ntest['size_tau2_600-300'] = test['size_tau2_300'] - test['size_tau2']\ntrain['size_tau2_600-150'] = train['size_tau2_150'] - train['size_tau2']\ntest['size_tau2_600-150'] = test['size_tau2_150'] - test['size_tau2']\n\ntrain['size_tau2_450-300'] = train['size_tau2_300'] - train['size_tau2_450']\ntest['size_tau2_450-300'] = test['size_tau2_300'] - test['size_tau2_450']\ntrain['size_tau2_450-150'] = train['size_tau2_150'] - train['size_tau2_450']\ntest['size_tau2_450-150'] = test['size_tau2_150'] - test['size_tau2_450']\ntrain['size_tau2_300-150'] = train['size_tau2_150'] - train['size_tau2_300']\ntest['size_tau2_300-150'] = test['size_tau2_150'] - test['size_tau2_300']","3a581c8a":"pd.to_pickle(train, 'train(317)_tau_noKNN.pkl')\npd.to_pickle(test, 'test(317)_tau_noKNN.pkl')","c26dee9e":"train.shape","f68d847b":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","62a463fb":"x.shape","c5681aa4":"seed = 29\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} ","655d3b93":"oof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score\n\ngain_importance_list = []\nsplit_importance_list = []\n\nfrom sklearn.model_selection import GroupKFold\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # \u3053\u306e\u66f8\u304d\u65b9\u3059\u308b\u3053\u3068\u3067\u3001\u5168\u30c7\u30fc\u30bf\u3092OOf\u306b\u3057\u3066rmspe\u304c\u6c42\u3081\u3089\u308c\u308b\u3001\n    # \u899a\u3048\u3066\u304a\u3044\u305f\u65b9\u304c\u3044\u3044\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) \/ 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","43241b31":"train.shape","b30310af":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean kfold 317 tau.csv', index=False)","be725e15":"train.shape","913732ad":"# making agg features\n\n# time_id\u6bce\u306estockid\u6bce\u306etarget\u5909\u6570\u306e\u4e00\u89a7\ntrain_p = pd.read_csv(data_dir + '\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n# \ncorr = train_p.corr()\n\n# stick_id\u306e\u76f8\u95a2\u4fc2\u6570\u306eindex\u3001\u3064\u307e\u308astockid\nids = corr.index\n\n# 7\u306b\u95a2\u3057\u3066\u306f\u8abf\u3079\u308b(\u30b7\u30eb\u30a8\u30c3\u30c8\u56f3\u3068\u304b\u3067)\n# time_id\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3068\u3057\u305f\u6642\u306estock_id\u76f8\u95a2\u4fc2\u6570\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n# print(kmeans.labels_)\n\n\n# l\u306brange(7)\u3068\u7b49\u3057\u3044\u30af\u30e9\u30b9\u30bf\u3092\u64ae\u3063\u305f\u6642\u306estock_id\u3092\u683c\u7d0d\u3057\u3066\u308b\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    # stock_id\u304c\u6307\u5b9a\u3055\u308c\u3066\u308b\u30af\u30e9\u30b9\u30bf\u3068\u540c\u3058\u3082\u306e(\u5168\u4f53\u30b5\u30f3\u30d7\u30eb)\u3092\u5f15\u3044\u3066\u304f\u308b\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    # time_id\u6bce\u306b\u5e73\u5747\u5024\u3092\u53d6\u308b(\u7570\u306a\u308bstock_id\u3067\u3082\u30af\u30e9\u30b9\u30bf\u304c\u540c\u3058\u306a\u3082\u306e\u540c\u58eb\u306e\u5e73\u5747\u5024\u306b\u306a\u308b)\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    \n    #\n    # stock_id\u305d\u306e\u3082\u306e\u306b\u3044\u307f\u304c\u306a\u304f\u306a\u3063\u305f\u306e\u3067\u6ce8\u610f\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\n    # stock_id\u3092\u30af\u30bf\u30b9\u30bfid\u306b\u5909\u66f4\u3057\u3066\u305d\u306e\u5f8c\u306bc1?\u3063\u3066\u3044\u3046\u306e\u3092\u3064\u3051\u3066\u308b\n    #\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","35b903bb":"matTest = []\nmat = []\nkmeans = []\n#mat2 #= mat1.pivot(index='time_id', columns='stock_idmat2\n# \u4f55\u3067\u305d\u3093\u306a\u3053\u3068\u3057\u305f\u3093\u3084\u7b11\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\n# \u3053\u306e\u66f8\u304d\u65b9\u4fbf\u5229\u3001\u899a\u3048\u3066\u304a\u304f\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","9821908c":"# \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u7d50\u679c\u306e\u4f7f\u3046\u7279\u5fb4\u91cf\u3060\u3051\u629c\u304d\u51fa\u3057\u3066\u304d\u3066\u308b\uff0810\/314\uff09\n# 2.5\u306b\u304b\u3093\u3057\u3066\u306f\u30af\u30e9\u30b9\u30bf\u306e\u4e2d\u304c\u3042\u307e\u308a\u306b\u5c0f\u3055\u3044\u306e\u3067\u9664\u304f\n# \u306a\u305c\u3001\u3053\u306e\u7279\u5fb4\u91cf\u306b\u3057\u305f\u306e\u304b\u306f\u4e0d\u660e\u306e\u305f\u3081\u3001\u8ffd\u52a0\u5b9f\u9a13\u304c\u5fc5\u8981\n# log_return2\u3092\u524a\u3063\u305f\u7406\u7531\u306f\u308f\u304b\u3089\u3093\uff08\u304a\u305d\u3089\u304f\u76f8\u95a2\u304c\u4f3c\u3061\u3083\u3046\u304b\u3089\uff09\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_2c1',\n     'log_return1_realized_volatility_3c1',     \n     'log_return1_realized_volatility_4c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_2c1',\n     'total_volume_mean_3c1', \n     'total_volume_mean_4c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_2c1',\n     'trade_size_mean_3c1', \n     'trade_size_mean_4c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_2c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_2c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_2c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_2c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_2c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_2c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_2c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_450_0c1',\n     'size_tau2_450_1c1',\n     'size_tau2_450_2c1',\n     'size_tau2_450_3c1',\n     'size_tau2_450_4c1',\n     'size_tau2_300_0c1',\n     'size_tau2_300_1c1',\n     'size_tau2_300_2c1', \n     'size_tau2_300_3c1', \n     'size_tau2_300_4c1', \n     'size_tau2_150_0c1',\n     'size_tau2_150_1c1',\n     'size_tau2_150_2c1', \n     'size_tau2_150_3c1', \n     'size_tau2_150_4c1',          \n     'size_tau2_600-450_0c1',\n     'size_tau2_600-450_1c1',\n     'size_tau2_600-450_2c1',\n     'size_tau2_600-450_3c1',\n     'size_tau2_600-450_4c1',\n     'size_tau2_600-300_0c1',\n     'size_tau2_600-300_1c1',\n     'size_tau2_600-300_2c1',\n     'size_tau2_600-300_3c1',\n     'size_tau2_600-300_4c1',\n     'size_tau2_600-150_0c1',\n     'size_tau2_600-150_1c1',\n     'size_tau2_600-150_2c1',\n     'size_tau2_600-150_3c1',\n     'size_tau2_600-150_4c1',          \n     'size_tau2_450-300_0c1',\n     'size_tau2_450-300_1c1',\n     'size_tau2_450-300_2c1',\n     'size_tau2_450-300_3c1',\n     'size_tau2_450-300_4c1',\n     'size_tau2_450-150_0c1',\n     'size_tau2_450-150_1c1',\n     'size_tau2_450-150_2c1',\n     'size_tau2_450-150_3c1',\n     'size_tau2_450-150_4c1',            \n     'size_tau2_300-150_0c1',\n     'size_tau2_300-150_1c1',\n     'size_tau2_300-150_2c1',\n     'size_tau2_300-150_3c1',\n     'size_tau2_300-150_4c1',            \n      ] ","8a561c48":"# train\u3068\u304f\u3063\u3064\u3051\u308b\ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","3281bb9e":"train.shape","f82a755f":"pd.to_pickle(train, 'train(412)_tau_KNN.pkl')\npd.to_pickle(test, 'test(412)_tau_KNN.pkl')","5afef079":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","062cf9d3":"oof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score\n\ngain_importance_list = []\nsplit_importance_list = []\n\nfrom sklearn.model_selection import GroupKFold\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # \u3053\u306e\u66f8\u304d\u65b9\u3059\u308b\u3053\u3068\u3067\u3001\u5168\u30c7\u30fc\u30bf\u3092OOf\u306b\u3057\u3066rmspe\u304c\u6c42\u3081\u3089\u308c\u308b\u3001\n    # \u899a\u3048\u3066\u304a\u3044\u305f\u65b9\u304c\u3044\u3044\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) \/ 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","8f6f85c7":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean kfold 412 tau KNN.csv', index=False)","245e016b":"mean_gain_df.set_index('feature_names').filter(like='tau', axis=0).sort_values('importance', ascending=False).head(15)","208a2dec":"test['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","1a989e87":"## \u8003\u5bdf\n\nCV : 0.21742  \n\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0tau\u306a\u3057\u3000: 0.22057  \n\n\u5fae\u5c0f\u3060\u304c\u52b9\u679c\u3042\u308a?  \nfeature importance\u3092\u307f\u308b\u3068size_tau2\u304c\u3044\u3044\u304b\u3082\uff1f\uff1f  \n600-450\u3082\u3061\u307e\u3061\u307e\u52b9\u3044\u3066\u308b\u304b\u3082\u3057\u308c\u306a\u3044\n","1e562a2a":"## \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0TAU","fe405f7e":"# group_kfold\u306b\u3064\u3044\u3066\n## \u76ee\u7684\ntime_id\u306e\u7d71\u8a08\u6307\u6a19\u3092\u7528\u3044\u3066\u3044\u308b\u305f\u3081\u3001time_ID\u6bce\u306b\u3057\u306a\u3044\u3068\u30ea\u30fc\u30af\u304c\u767a\u751f\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044  \n\u3082\u3057CV\u304c\u4f4e\u304f\u3001LB\u304c\u9ad8\u3044\u5834\u5408\u53ef\u80fd\u6027\u306f\u9ad8\u3044\u3002  \n\u904e\u9069\u5408\u3055\u305b\u305f\u304f\u306a\u3044\u306e\u3067\u3069\u306e\u7a0b\u5ea6\u306e\u5f71\u97ff\u304b\u63a2\u308b\n\n## \u8ffd\u52a0\nstock_ID\u306e\u7d71\u8a08\u6307\u6a19\u3082\u6c42\u3081\u3066\u3044\u308b\u304c\u3001\u3053\u308c\u304c\u30ea\u30fc\u30af\u306b\u306a\u308b\u304b\u306f\u304b\u306a\u308a\u30b0\u30ec\u30fc  \nstock_ID\u81ea\u4f53\u306f\u672a\u6765\u306e\u60c5\u5831\u3092\u542b\u3080\u304c\u3001\u9298\u67c4\u4e8b\u306b\u7279\u5fb4\u304c\u3042\u308b\u306e\u306f\u306a\u3093\u3089\u9593\u9055\u3044\u3067\u306f\u306a\u3044\u3057\u3001\u305d\u3082\u305d\u3082\u30c7\u30fc\u30bf\u304c\u306a\u3044\u306e\u3067\u305d\u3046\u3059\u308b\u3057\u304b\u306a\u3044\u3002\n\n### \u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\n- tau\u306f\u306a\u3057\n- \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u6307\u6a19\u306a\u3057\n","936520a0":"# \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\n\n\u304b\u306a\u308a\u7d30\u304b\u3044\u3068\u3053\u308d\u306b\u306a\u308b\u306e\u3067\u3001\u3067\u304d\u308c\u3070\u6bce\u56de\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3079\u304d\u306a\u3093\u3060\u3051\u308c\u3069\u3082\u4e00\u56de\u7f6e\u3044\u3066\u304a\u304f\u3002\nseed = 29\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} \n\n    \u3053\u308c\u304c\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u3066\u3044\u308b\u5024\u3089\u3057\u3044\u306e\u3067\u4eca\u56de\u306f\u3053\u308c\u3092\u4f7f\u3046","520d531a":"# TAU\u306b\u3064\u3044\u3066\n\ntau1 : train['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\n\u306b\u95a2\u3057\u3066\u306f\u5358\u7d14\u306a\u7d71\u8a08\u6307\u6a19\u306e\u305f\u3081\u3001LGBM\u30e2\u30c7\u30eb\u306b\u306f\u52b9\u679c\u304c\u306a\u3044  \nfeature_importance\u3092\u307f\u3066\u3082\u5168\u304f\u52b9\u3044\u3066\u3044\u306a\u3044  \n\n\ntau2 : \ntrain['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )  \ntrain['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )  \n\u306b\u95a2\u3057\u3066\u306f\u5358\u4f4d\u6642\u9593\u3042\u305f\u308a\u306e\u6307\u6a19\u306b\u306a\u308b\u306e\u3067\u5909\u308f\u308b\u6c17\u304c\u3059\u308b  \n150\u79d2\u7d4c\u904e\u5f8c\u306e\u53d6\u5f15\u5408\u8a08\u6570\u306e\u5024\u3092600\u79d2\u5358\u4f4d\u306b\u306a\u304a\u3057\u3066\u3044\u308b\u304b\u3089\u3001\u63a8\u5b9a\u53d6\u5f15\u5408\u8a08\u6570\u306b\u306a\u308b\u306f\u305a  \n\u3053\u308c\u304c\u5b9f\u969b\u306e\u53d6\u5f15\u5408\u8a08\u6570\u3068\u9055\u3046\u5834\u5408\u3001\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u304c\u6fc0\u3057\u304f\u52d5\u3044\u3066\u63a8\u5b9a\u3068\u7570\u306a\u3063\u305f\u3088\u3063\u3066\u3044\u3046\u6307\u6a19\u306b\u306a\u308b\u306f\u305a  \n\n\u53d6\u5f15\u56de\u6570\u306b\u3053\u3060\u308f\u308b\u5fc5\u8981\u6027\u3063\u3066\u3044\u3046\u306e\u306f\u4f55\u6545\u306a\u3093\u3060\u308d\u3046...  \n\u666e\u901a\u306blog_return\u3068\u304b\u3067\u3082\u3044\u3044\u3093\u3058\u3083\u306a\u3044\u304b\u306a\u3063\u3066\u3044\u3046\u6c17\u304c\u3059\u308b\u3051\u3069\n\n## \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u6307\u6a19\u306a\u3057","2bc515a1":"## \u8003\u5bdf\nCV : 0.22507  \ntau\u306a\u3057\u3000: 0.2253  \n\n\u52b9\u679c\u306a\u3057....  \nfeture_importance\u3067\u307f\u3066\u3082\u52b9\u679c\u306f\u5168\u304f\u306a\u3044\u3002  ","2c51a699":"# \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u6307\u6a19\u306b\u3064\u3044\u3066\n\n\u4eca\u56de\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306ftime_id\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3068\u3057\u3066\u3001stock_id\u6bce\u306e\u76ee\u7684\u5909\u6570\u306e\u5024\u3092stock_id\u6bce\u306b\u76f8\u95a2\u95a2\u4fc2\u3092\u51fa\u3057\u3066\u3044\u308b\u3002  \n\u3059\u306a\u308f\u3061\u3001\u76f8\u95a2\u306e\u5024\u304c\u4f3c\u3066\u3044\u308bstock_id\u306f\u76ee\u7684\u5909\u6570\u306e\u5206\u5e03\u304c\u4f3c\u3066\u3044\u308b\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308b\u3002  \n\u5206\u5e03\u304c\u4f3c\u3066\u3044\u308b\u3068\u3044\u3046\u3053\u3068\u306f\u3001\u5404\u30af\u30e9\u30b9\u30bf\u6bce\u306b\u66f8\u304f\u6307\u6a19\u306e\u5e73\u5747\u3092\u53d6\u3063\u3066time_id\u6bce\u306b\u5404\u30af\u30e9\u30b9\u30bf\u306e\u6307\u6a19\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4ee3\u5165\u3059\u308b\u3053\u3068\u3067\u3001\u65b0\u305f\u306a\u7279\u5fb4\u91cf\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3068\u3044\u3046\u8003\u3048\u65b9\u3002  \n\ntau\u306f\u4e00\u65e6\u306a\u3057","73485ce3":"## \u8003\u5bdf\nCV : 0.22013  \n\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u6307\u6a19\u3092\u5165\u308c\u308b\u3053\u3068\u3067cv\u306f\u304b\u306a\u308a\u6539\u5584\u3055\u308c\u305f\u3002\n\u3057\u304b\u3057\u306a\u304c\u3089feature importance\u3092\u307f\u3066\u3082\u305d\u3053\u307e\u3067\u52b9\u679c\u304c\u3042\u308b\u3088\u3046\u306b\u306f\u3071\u3063\u3068\u307f\u898b\u3048\u306a\u304b\u3063\u305f\u3002  \n\u305f\u307e\u305f\u307e\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u7d50\u679c\u3068\u30de\u30c3\u30c1\u3057\u3066\u308b\u306e\u304b\u3001\u305d\u308c\u3068\u3082\u4e0a\u304c\u308b\u306e\u304b...\n\n\u306a\u3057\u3000CV : 0.2253  \n\n~\u3063\u3066\u3053\u3068\u306f\u3060\u3088\u3002time_id\u6bce\u306b\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u5834\u5408\u3001\u3069\u3046\u306a\u308b\uff1f\uff1f\uff1f  \ntime_id 1, 2\u306b\u76f8\u95a2\u304c\u898b\u3089\u308c\u3066\u7d71\u8a08\u6307\u6a19\u3092\u51fa\u3057\u305f\u5834\u5408\u30a4\u30e1\u30fc\u30b8\u7684\u306b\u306f  \n\u4f55\u3082\u306a\u3044time_id  \n\u52d5\u304ftime_id  \n\u307f\u305f\u3044\u306a\u611f\u3058\u306b\u307e\u3068\u307e\u3063\u3066\u304f\u308c\u3066\u3001\u5b9f\u969b\u306e\u8a55\u4fa1\u3067\u3082\u3044\u3044\u611f\u3058\u306b\u307e\u3068\u307e\u3063\u3066\u304f\u308c\u308b\u306e\u3067\u306f\u306a\u3044\u304b\uff1f\uff1f\uff1f  \n\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u306e\u5927\u5909\u305d\u3046\u3060\u3051\u3069\u3084\u308b\u4fa1\u5024\u306f\u3042\u308a\u305d\u3046~  \ntarget\u304ctime_id\u6bce\u306e\u6307\u6a19\u3060\u304b\u3089\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306f\u3067\u304d\u306a\u3044\u308f....  \nstock_id\u4ee5\u5916\u306b\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u304c\u610f\u5473\u3042\u308a\u305d\u3046\u306a\u7279\u5fb4\u91cf\u306f\u898b\u5f53\u305f\u3089\u306a\u3044\u304b\u3089\u306a\u3093\u3068\u3082\u3044\u3048\u3093","f07fb3f5":"# \u81ea\u4f5c\u30ab\u30fc\u30cd\u30eb\n\u78ba\u304b\u3081\u305f\u3044\u3053\u3068\n- groupk_fold\u306e\u52b9\u679c\n- \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u6307\u8a55\u306e\u52b9\u679c\n- \u9069\u5207\u306a\u79d2\u6570\u533a\u5207\u308a(\u304a\u305d\u3089\u304f150.300.450)\n- \u7279\u5fb4\u91cf\u306esum. mean, std\u3042\u305f\u308a\u306e\u3044\u3089\u306a\u3044\u6307\u6a19\u306e\u524a\u6e1b"}}