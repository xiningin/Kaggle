{"cell_type":{"26f065e9":"code","0fffdb93":"code","c79a011f":"code","195f43d9":"code","92ed8727":"code","eefe0ecb":"code","ca3e91fa":"code","58ca08f7":"code","30d7ccaa":"code","b6220b09":"code","ede2b709":"code","000b7339":"code","5702d9e1":"code","e24633a7":"code","b2cf8f22":"code","e696595a":"code","a0bf3f18":"code","751920dc":"code","213ad2ba":"code","ff4df90d":"code","1a0b8617":"code","68dd675a":"code","7e78d4ac":"code","62140b6d":"code","73ab1948":"code","409c58a6":"code","82a61749":"code","3a281481":"code","58f3bf97":"code","f4ae59b9":"code","090a67a7":"code","d335dd9f":"code","78790074":"code","f2554cb1":"code","9d6fd102":"code","b2b79999":"code","9516efe0":"code","a5f7f5fe":"code","efce0350":"code","203bd055":"markdown","84ec10af":"markdown","fef7f001":"markdown","b56448dc":"markdown","ccad6e07":"markdown","4a8b2776":"markdown","d8bb0af6":"markdown","cc53fdeb":"markdown","9c1ffc58":"markdown","bb68b7be":"markdown","8cad2c58":"markdown","245a9cec":"markdown","731936be":"markdown","16aca967":"markdown","aa3724d1":"markdown","70b1d6b6":"markdown","6e140bff":"markdown","0e7ca3bf":"markdown","d0de88f1":"markdown","420a5a4a":"markdown","ee86a06f":"markdown","ca2be6da":"markdown","b4d52ed5":"markdown","1d631ec7":"markdown","25217d98":"markdown"},"source":{"26f065e9":"# Check number of images\n!find ..\/input\/digikala-color-classification\/train -type f | wc -l","0fffdb93":"# Check paths (labels)\n!ls -a ..\/input\/digikala-color-classification\/train","c79a011f":"!pip install natsort","195f43d9":"import cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\nfrom random import choice","92ed8727":"root = '..\/input\/digikala-color-classification\/train'\n\ndef generate_random_img_label():\n    paths = list(Path(root).rglob('*.jp*g'))\n    path = choice(paths)\n    img = cv2.imread(path.as_posix())\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    label = path.parent.stem\n    return img, label\n\ndef crop_image(img):\n    \"\"\"\n    This is a function that crops extra white background\n    around product.\n    Src:\n        https:\/\/stackoverflow.com\/questions\/64046602\/how-can-i-crop-an-object-from-surrounding-white-background-in-python-numpy\n    \"\"\"\n    mask = img!=255\n    mask = mask.any(2)\n    mask0,mask1 = mask.any(0),mask.any(1)\n    colstart, colend = mask0.argmax(), len(mask0)-mask0[::-1].argmax()+1\n    rowstart, rowend = mask1.argmax(), len(mask1)-mask1[::-1].argmax()+1\n    return img[rowstart:rowend, colstart:colend]\n\n\nimages = list(Path(root).rglob('*.jp*g'))\n\nfor i in range(2):\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.suptitle('Horizontally stacked subplots')\n    rnd = choice(images)\n    # Open image and make into Numpy array\n    img = cv2.imread(rnd.as_posix())\n    ax1.imshow(img)\n\n    img = crop_image(img)\n    ax2.imshow(img)\n    plt.show()","eefe0ecb":"def hsv_histogram(img):\n    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    h = hsv[..., 0]\n    return np.bincount(h.ravel(), minlength=256)\n\nimg, label = generate_random_img_label()\nimg = crop_image(img)\nplt.imshow(img)\nplt.show()\n\nhist = hsv_histogram(img)\nplt.plot(hist[1:])\nplt.title(label)\nplt.show()","ca3e91fa":"# Let's bring the color labels\nfrom os import listdir\nfrom natsort import natsorted\n\npaths  = listdir('..\/input\/digikala-color-classification\/train')\nlabels = {path: i for i, path in enumerate(natsorted(paths))}\nlabels","58ca08f7":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# find all images\nroot = '..\/input\/digikala-color-classification\/train'\npaths = natsorted(list(Path(root).rglob('*.jp*g')), key=lambda x: x.parent.stem)\n\n# dataframe for saving features\ndf = pd.DataFrame(columns=[i for i in range(255)])\ncol_labels = []\n\n# loop over all items and calculate the images histogram.\nfor i, path in enumerate(paths):\n    img = cv2.imread(path.as_posix())\n    label = labels[path.parent.stem]\n    img = crop_image(img)\n    hist = hsv_histogram(img)\n    df = df.append(pd.DataFrame(hist[1:]).T, ignore_index=True)\n    col_labels.append(label) # append labels to insert after the loop\n    \ndf['labels'] = col_labels # Insert the labels","30d7ccaa":"df.head()","b6220b09":"df_copy = df.copy()\nY = df_copy.pop('labels')\nX = df_copy.copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1989)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform (X_test)","ede2b709":"classifiers = [\n    RandomForestClassifier,\n    BaggingClassifier,\n    DecisionTreeClassifier,\n    SVC,\n    KNeighborsClassifier,\n    RidgeClassifier\n]\n\nparams = {\n    RandomForestClassifier.__name__: dict(n_estimators=150, \n                                          max_depth=5, \n                                          min_samples_split=5,\n                                          n_jobs=-2),\n    BaggingClassifier.__name__: {},\n    SVC.__name__: {},\n    RidgeClassifier.__name__: {},\n    DecisionTreeClassifier.__name__: {},\n    KNeighborsClassifier.__name__: dict(metric='manhattan', \n                                        n_neighbors=len(labels))\n}","000b7339":"for classifier in classifiers:\n    Classifier = classifier(**params[classifier.__name__])\n    Classifier.fit(X_train, y_train)\n    y_pred = Classifier.predict(X_test)\n    print(classifier.__name__)\n    print('*' * 20)\n    print(classification_report(y_test, y_pred, target_names=list(labels.keys())))","5702d9e1":"!pip install natsort\n!pip install tf-explain\n!pip install -U tensorboard-plugin-profile\n\nfrom os.path import isdir, dirname, abspath, join, sep\nfrom random import shuffle, choice, uniform\nfrom sklearn.metrics import confusion_matrix\nfrom os import makedirs, listdir\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom natsort import natsorted\nfrom pathlib import Path\nimport matplotlib as mpl\nfrom io import BytesIO\nimport seaborn as sns\nfrom tqdm import tqdm\nimport numpy as np\nimport itertools\nimport cv2\nimport os\n\nfrom tensorflow.keras.applications import (DenseNet201, InceptionV3, MobileNetV2,\n                                           ResNet101, Xception, EfficientNetB7,\n                                           VGG19, NASNetLarge)\nfrom tensorflow.keras.applications import (densenet, inception_v3, mobilenet_v2, \n                                           resnet, xception, efficientnet, \n                                           vgg19, nasnet)\nfrom tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, EarlyStopping, \n                                        LearningRateScheduler)\n\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import Sequential,Model\nfrom tensorflow.keras import backend as K\nimport tf_explain as tfe\nimport tensorflow as tf\nimport sklearn\n\nplt.rcParams['figure.dpi'] = 100\nmpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","e24633a7":"class Params:\n    root = 'drive\/MyDrive'\n    data_path = '..\/input\/digikala-color-classification\/train'\n    path_to_save = join(root, '.\/logs')\n    autotune = tf.data.experimental.AUTOTUNE\n    batch_size = 16\n    epochs = 50\n    lr = 1e-2\n    decay = 1e-6\n    momentum = 0.95\n    nesterov = True\n    block_to_gradcam = \"block14_sepconv2_act\"\n    model = 'xception'\n    \nif not isdir(Params.root):\n    makedirs(Params.root)\n\nif not isdir(Params.path_to_save):\n    makedirs(Params.path_to_save)\n","b2cf8f22":"sorted_paths = natsorted(listdir(Params.data_path))\n\nlabels = {v:k for k, v in enumerate(sorted_paths)}\nrev_labels = {v:k for k,v in labels.items()}\nclass_names = list(labels.keys())\n\n# print(labels)\n\nprint(\"number of images per each label\")\nfor item in sorted_paths:\n    label_path = Path(join(Params.data_path, item))\n    num_images = list(label_path.rglob('*.jp*g'))\n    print(f\"'{item}': {len(num_images)}\")","e696595a":"models = {\n    'densenet': DenseNet201,\n    'xception': Xception,\n    'inceptionv3': InceptionV3,\n    'effecientnetb7': EfficientNetB7,\n    'vgg19': VGG19,\n    'nasnetlarge': NASNetLarge,\n    'mobilenetv2': MobileNetV2,\n    'resnet': ResNet101\n}\n\n# To use => myNet = models['densenet']()\n\npreprocess_pipeline = {\n    'densenet': densenet.preprocess_input,\n    'xception': xception.preprocess_input,\n    'inceptionv3': inception_v3.preprocess_input,\n    'effecientnetb7': efficientnet.preprocess_input,\n    'vgg19': vgg19.preprocess_input,\n    'nasnetlarge': nasnet.preprocess_input,\n    'mobilenetv2': mobilenet_v2.preprocess_input,\n    'resnet': resnet.preprocess_input\n}\n","a0bf3f18":"import albumentations as A\nfrom albumentations import OneOf, Compose","751920dc":"def aug_flip(p=0.5):\n    return OneOf([\n        A.Flip(),\n        A.Transpose(),\n        A.RandomRotate90(),\n        A.ShiftScaleRotate()\n    ],\n        p=p)\n\ndef aug_blur(p=0.5):\n    return OneOf([\n        A.MotionBlur(),\n        A.MedianBlur(blur_limit=3),\n        A.Blur(blur_limit=3),\n        # A.GlassBlur()\n        ],\n        p=p)\n\ndef aug_brightness(p=0.5):\n    # To do: Why brightness augmentations turns image to black.\n    return OneOf([\n#         A.CLAHE(),\n        A.HueSaturationValue(),\n        A.RandomBrightness(),\n        A.RandomContrast(),\n        # A.RandomBrightnessContrast(brightness_limit=0.2,contrast_limit=0.2,brightness_by_max=False)\n    ], \n        p=p)\n\ndef aug_noise(p=0.5):\n    return OneOf([\n        A.IAAAdditiveGaussianNoise(),\n        A.GaussNoise(),\n        # A.MultiplicativeNoise()\n    ], \n        p=p)\n\ndef aug_distortion(p=0.5):\n    return OneOf([\n        A.OpticalDistortion(),\n        A.GridDistortion()\n    ], \n        p=p)\n        \ndef aug_iaa(p=0.5):\n    return OneOf([\n        A.IAAEmboss(),\n        A.IAAPerspective(),\n        A.IAAFliplr(),\n        A.IAAFlipud(),\n        A.IAASharpen(),\n        A.IAASuperpixels()\n    ],\n        p=p)\n\ndef aug_nature(p=0.5):\n    return OneOf([\n        A.RandomRain(),\n        A.RandomFog(),\n        A.RandomShadow(),\n        A.RandomSunFlare()\n    ],\n        p=p)\n\n# def aug_color(p=0.5):\n#     OneOf([\n#         A.ColorJitter(),\n#         A.Solarize(),\n#         A.Posterize(),\n#     ],\n#         p=p)\n\n\ndef strong_aug(image, all_p=0.5, flip_p=0.5, blur_p=0.5, distort_p=.0, iaa_p=.0, brightness_p=.0, noise_p=.0, color_p=0.0):\n    aug = Compose([\n        aug_flip(p=flip_p), \n        aug_distortion(p=distort_p),\n        aug_iaa(p=iaa_p),\n        # aug_nature(p=nature_p),\n        aug_brightness(p=brightness_p),\n        aug_noise(p=noise_p),\n        aug_blur(p=blur_p),\n        # aug_color(p=color_p)\n    ], \n        p=all_p)\n    result = aug(image=image)\n    return result['image']","213ad2ba":"AUG_PROBABILITIES = {\n    'all_p': 1.0,\n    'flip_p': 0.8,\n    'blur_p': 0.3,\n    'distort_p': 0.0,\n    # 'nature_p': 0.0,\n    'iaa_p': 0.0, # to use imgaug\n    'brightness_p': 0.0,\n    'noise_p': 0.0,\n    # 'color_p': 0.0\n}\n\n\ndef crop_image(img):\n    \"\"\"\n    This is a function that crops extra white background\n    around product.\n    Src:\n        https:\/\/stackoverflow.com\/questions\/64046602\/how-can-i-crop-an-object-from-surrounding-white-background-in-python-numpy\n    \"\"\"\n    mask = img!=255\n    mask = mask.any(2)\n    mask0,mask1 = mask.any(0),mask.any(1)\n    colstart, colend = mask0.argmax(), len(mask0)-mask0[::-1].argmax()+1\n    rowstart, rowend = mask1.argmax(), len(mask1)-mask1[::-1].argmax()+1\n    return img[rowstart:rowend, colstart:colend]\n\ndef process_train(path):\n    \"\"\"\n    Load file from path, process and return image and its\n    corresponding label. (includes augmentation)\n    \n    \"\"\"\n    path = path.numpy().decode('utf-8') # only available with tf.py_function\n    image = load_img(path, target_size=(224, 224))\n    image = img_to_array(image, dtype=np.float32)\n    # image = crop_image(image) # removes as much white space as possible\n    # image = cv2.resize(image, (224, 224))\n    image = strong_aug(image, **AUG_PROBABILITIES) \n    image = preprocess_pipeline[Params.model](image) # preprocess corresponding to its deep model.\n    \n    # Create one-hot encoded label.\n    path = Path(path).parent.stem\n    size = len(list(labels.keys()))\n    label = np.zeros(size, dtype=np.float32)\n    label[labels[path]] = 1\n    return image, label\n\ndef process_val(path):\n    \"\"\"\n    Load file from path, process and return image and its\n    corresponding label. (No augmentation included)\n    \n    \"\"\"\n    path = path.numpy().decode('utf-8')\n    image = load_img(path, target_size=(224, 224))\n    image = img_to_array(image, dtype=np.float32)\n    # image = crop_image(image)\n    # image = cv2.resize(image, (224, 224))\n    image = preprocess_pipeline[Params.model](image)\n     \n    size = len(list(labels.keys()))\n    path = Path(path).parent.stem\n    label = np.zeros(size, dtype=np.float32)\n    label[labels[path]] = 1\n    return image, label\n\ndef generator(pattern, validation_ratio, reshuffle_each_iteration=False, include_stats=True):\n    \"\"\"\n    Creates two generators in tf.data.Dataset dtype for train and validation \n    \n    \"\"\"\n    reader = tf.data.Dataset.list_files(pattern)\n    n_data = reader.cardinality().numpy()\n    val_size = int(n_data * validation_ratio)\n    train_ds = reader.skip(val_size)\n    val_ds = reader.take(val_size)\n    \n    train_labels_quota = {k: 0 for k in list(labels.keys())}\n    val_labels_quota = {k: 0 for k in list(labels.keys())}\n    \n    train_ds = train_ds.map(lambda x: tf.py_function(process_train, \n                                                     [x], \n                                                     [tf.float32, tf.float32]), \n                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    \n    val_ds = val_ds.map(lambda x: tf.py_function(process_val, \n                                                 [x], \n                                                 [tf.float32, tf.float32]), \n                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    \n    if include_stats:\n        for \u0640, label in train_ds:\n            l = np.argmax(label.numpy())\n            train_labels_quota[rev_labels[l]] += 1\n\n        for _, label in val_ds:\n            l = np.argmax(label.numpy())\n            val_labels_quota[rev_labels[l]] += 1\n\n    return train_ds, val_ds, {'train': train_labels_quota, 'val': val_labels_quota}\n\n# Let's check if it works","ff4df90d":"pattern = Params.data_path + '\/*\/*.jp*g'\ntrain_ds, val_ds, labels_quota = generator(pattern=pattern, validation_ratio=0.2, include_stats=True)","1a0b8617":"print('Number of Samples for each label')\nfor (k1, v1), (k2, v2) in zip(labels_quota['train'].items(), labels_quota['val'].items()):\n    print(f'{k1}: train: {v1} - val: {v2}')\n    \nfor image, label in train_ds.take(1):\n    print(label)\n    print(list(labels.keys())[np.argmax(label)])\n    img = image.numpy() # convert EagerTensor to numpy\n\n    \"\"\"\n    In order to plot the image in matplotlib.pyplot, we need 0 < image values < 1:\n    1. we shift the values to values more than 0.\n    2. We then normalize the values between [0, 1] # src: https:\/\/stackoverflow.com\/a\/1735122\/6118987\n    \n    \"\"\"\n    if img.min() < 0:\n        img += abs(img.min())\n\n    img *= (1.0\/img.max()) # src: https:\/\/stackoverflow.com\/a\/1735122\/6118987\n\n    print(img.min())\n    print(img.max())\n    plt.imshow(img)\n    plt.show()\n\n# print(image.numpy())","68dd675a":"# Number of train\/validation images\nprint(train_ds.cardinality().numpy())\nprint(val_ds.cardinality().numpy())","7e78d4ac":"\"\"\"\nexample\n\ntf.data shuffle\ntf.data.batch\n\n\"\"\"\ntrain_batches = train_ds.shuffle(100, reshuffle_each_iteration=False).batch(Params.batch_size)\n\nfor image, label in train_batches.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","62140b6d":"\"\"\"\nClassification metrics for tensorboard\n\nSrc:\n    https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data\n\n\"\"\"\n\nMETRICS = [\n    tf.keras.metrics.TruePositives(name='tp'),\n    tf.keras.metrics.FalsePositives(name='fp'),\n    tf.keras.metrics.TrueNegatives(name='tn'),\n    tf.keras.metrics.FalseNegatives(name='fn'), \n    tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name='recall'),\n    tf.keras.metrics.AUC(name='auc'),\n    tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n    \n]\n\n\ndef create_model(optimizer, name='resnet', loss='categorical_crossentropy', blocks_to_train=[]):\n    \"\"\"\n    Creates model based on the input name and freezes `blocks_to_train` blocks.\n    Args: \n        optimizer(tf.keras.optimizers): initialized tensorflow optimizers.\n        name(str): one of the keys in the `models` list.\n        blocks_to_train: name of the blocks to freeze, if not given all the \n        layers will be trainable.\n        loss: sets loss\n        \n    \"\"\"\n    base_model = models[name](include_top=False, weights='imagenet')\n    # model = Model(base_model.inputs, base_model.layers[-1].output)\n\n    if bool(blocks_to_train):\n        for block in blocks_to_train:\n            for layer in base_model.layers:\n                if block in layer.name:\n                    layer.trainable = True\n                else:\n                    layer.trainable = False\n    else:\n        for layer in base_model.layers:\n            layer.trainable = True\n            \n    x = GlobalAveragePooling2D()(base_model.layers[-1].output)\n    x = Dense(1024, activation='relu')(x)\n    output = Dense(12, activation='softmax')(x)\n\n    model = Model(base_model.inputs, output)\n    model.compile(loss=loss,\n                  optimizer=optimizer,\n                  metrics=METRICS)\n    return model\n\n\ndef scheduler(epoch):\n    # Every 10 epochs, the learning rate is reduced to 1\/10 of the original\n    if epoch == 10:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr * 0.1)\n\n    if epoch == 100:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr * 0.1)\n\n    return K.get_value(model.optimizer.lr)\n\ndef generate_path(path_to_output, last_run=False):\n    \"\"\"\n    Creates new path and returns the address.\n    Notes:\n        Sometimes accidently it happens that you overwrite your previous models. so\n        this function is designed to create a new path for each run.\n    \"\"\"\n    if not isdir(path_to_output):\n        makedirs(path_to_output)\n    \n    runs = natsorted([path for path in listdir(path_to_output) if path.startswith(\"run_tf_data\")])\n    if last_run:\n        if not bool(runs):\n            path = join(path_to_output, \"run_tf_data_1\")\n        else:\n            path = join(path_to_output, runs[-1])\n\n        return path\n    if not bool(runs):\n        path = join(path_to_output, 'run_tf_data_1')\n    else:\n        f = runs[-1].rsplit(\"data_\")[1]\n        path = join(path_to_output, 'run_tf_data_' + str(int(f) + 1))\n    \n    return path","73ab1948":"def plot_to_image(figure):\n    \"\"\"\n    Saves plot as a png file.\n    src: tensorflow.org docs\n    https:\/\/stackoverflow.com\/a\/61443397\/6118987\n    https:\/\/www.tensorflow.org\/tensorboard\/image_summaries\n    \n    \"\"\"\n    # save plot to png file\n    buf = BytesIO()\n    plt.savefig(buf, format='png')\n\n    # Closing the figure prevents it from being displayed directly inside\n    # the notebook.\n    # plt.close(figure)\n\n    buf.seek(0)\n    \n    image = tf.image.decode_png(buf.getvalue(), channels=4)\n    \n    # Add the batch dimension\n    image = tf.expand_dims(image, 0)\n    buf.close()\n\n    return image\n\ndef image_grid(data, labels, class_names):\n    \"\"\"\n    Saves all images as a grid of class_names * class_names.\n    src: tensorflow docs\n    \"\"\"\n    assert data.ndim == 4\n    figure = plt.figure(figsize=(10,10))\n    \n    num_images = data.shape[0]\n    size = int(np.ceil(np.sqrt(num_images)))\n    # class_names[int(tf.argmax(y[0]))\n    for i in range(num_images):\n        plt.subplot(size, size, i+1, title=class_names[int(tf.argmax(labels[i]))])\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        \n        # Grayscale images\n        if data.shape[3] == 1:\n            plt.imshow(data[i], cmap=plt.cm.binary)\n        else:\n            plt.imshow(data[i])\n\n    return figure\n    \ndef get_confusion_matrix(y_labels, logits, class_names):\n    \"\"\"\n    Confusion matrix calculation.\n    Src: Tensorflow docs\n    \"\"\"\n    preds = np.argmax(logits, axis=1)\n    labels = np.argmax(y_labels, axis=1).numpy()\n    cm = confusion_matrix(\n        labels,\n        preds, \n        labels=np.arange(len(class_names))\n    )\n\n    return cm\n\ndef plot_confusion_matrix(cm, class_names):\n    \"\"\"\n    Saves the confusion matrix as image.\n    Src: Tensorflow docs\n    \"\"\"\n    size=len(class_names)\n    figure=plt.figure(figsize=(size, size))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(\"Confusion Matrix\")\n    plt.colorbar()\n    indices = np.arange(len(class_names))\n    \n    plt.xticks(indices, class_names, rotation=45)\n    plt.yticks(indices, class_names)\n    labels=np.around(\n        cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], decimals=2)\n\n    threshold = cm.max() \/ 2.0\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = \"white\" if cm[i, j] > threshold else \"black\"\n        plt.text(j, i, labels[i, j], horizontalalignment=\"center\", color=color)\n\n    plt.tight_layout()\n    plt.xlabel(\"True Label\")\n    plt.ylabel(\"Prediction label\")\n    cm_image = plot_to_image(figure)\n    \n    return  cm_image\n\n","409c58a6":"from datetime import datetime\n\nmain_path = generate_path(Params.path_to_save)\ntb_logdir = f'{main_path}\/acc_loss'\nimg_logdir = f'{main_path}\/image\/'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_logdir)\ncm_writer = tf.summary.create_file_writer(img_logdir+'\/cm')\n\ndef log_confusion_matrix(epoch, logs):\n    test_pred_raw = model.predict(np.array(val_ds_x))\n    test_pred = np.argmax(test_pred_raw, axis=1)\n    \n    test_labels = np.argmax(val_ds_y, axis=1)\n    \n    # Calculate the confusion matrix\n    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\n    # Log the confusion matrix as an image summary.\n    figure = plot_confusion_matrix(cm, class_names=class_names)\n    cm_image = plot_to_image(figure)\n    plt.close('all')\n    # Log the confusion matrix as an image summary.\n    with cm_writer.as_default():\n        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n\ncm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)","82a61749":"# !rm -r drive\/MyDrive\/logs\/*","3a281481":"# Configure to performance\npattern = Params.data_path + '\/*\/*.jp*g'\ntrain_ds, val_ds, num_samples = generator(pattern=pattern, validation_ratio=0.2, include_stats=True)\n\n# Get train_size before split to batches and also using reeat function.\ntrain_size = tf.data.experimental.cardinality(train_ds).numpy() \n\n\"\"\"\nNote:\n    If tf.data.Dataset.batch().repeat() => we have to define `steps_per_epoch` argument in `.fit` function.\n    If tf.data.Dataset.batch() => you can use .fit() with no `steps_per_epoch` argument.\n    \n\"\"\"\n\ntrain_ds = train_ds.batch(Params.batch_size).repeat()\ntrain_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)","58f3bf97":"# Keras.fit does not accept generators anymore. \n# so let's keep the validation in separate list.\n\nval_ds_x = []\nval_ds_y = []\n\nfor _, (val_x, val_y) in enumerate(val_ds):\n    val_ds_x.append(val_x)\n    val_ds_y.append(val_y)\n\nval_data = (tf.convert_to_tensor(val_ds_x, dtype=tf.float32), \n            tf.convert_to_tensor(val_ds_y, dtype=tf.float32))","f4ae59b9":"val_data[0].numpy().shape","090a67a7":"# Check for image and label shape.\n\nitem, label =  next(iter(train_ds))\nprint(item.shape)\nprint(label.shape)","d335dd9f":"# Set up the GradCam callback arguments\n# Choosing 10 green labeled objects\n\nvalidation_class_green = (\n    np.array(\n       [img for img, label in zip(val_ds_x, val_ds_y) if \\\n        np.all(np.argmax(label) == labels['green'])][0:10]),None\n)\n\n# Define tf-explain instance for GradCam Callbacks on one og classes (green)\ntfe_callback = tfe.callbacks.GradCAMCallback(validation_class_green, \n                                             class_index=0,\n                                             output_dir=join(main_path, 'image', 'grad_cam'),\n                                             layer_name=Params.block_to_gradcam) # MobileNetV2 block_1_project\n\n# Define Optimizer\noptimizer = SGD(lr=Params.lr, \n                decay=Params.decay, \n                momentum=Params.momentum, \n                nesterov=Params.nesterov)\n\n# define the model\nmodel = create_model(optimizer, name=Params.model, blocks_to_train=[])","78790074":"# In order to use tensorboard magi\n# %load_ext tensorboard\n# Start TensorBoard.\n# %tensorboard --logdir drive\/MyDrive\/logs\/run_tf_data_1\/ \n\n# Train the classifier.\nprint(f'train size: {sum(num_samples[\"train\"].values())}')\nprint(f'val size: {val_ds.cardinality()}')\nprint(\"*\"* 20)\nprint('Number of Samples for each label')\nprint(\"*\" * 20)\nfor (k1, v1), (k2, v2) in zip(num_samples['train'].items(), num_samples['val'].items()):\n    print(f'{k1}: train: {v1} - val: {v2}')\n    \nhistory = model.fit(train_ds,\n                    epochs=Params.epochs,\n                    callbacks=[tensorboard_callback, cm_callback,tfe_callback],\n                    steps_per_epoch=train_size\/\/Params.batch_size,\n                    validation_data=val_data,\n)\n","f2554cb1":"# https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data#evaluate_metrics\n# Plot loss\n\ndef plot_loss(history, label, n):\n    # Use a log scale on y-axis to show the wide range of values.\n    plt.semilogy(history.epoch, history.history['loss'],\n               color=colors[n], label='Train ' + label)\n    plt.semilogy(history.epoch, history.history['val_loss'],\n               color=colors[n], label='Val ' + label,\n               linestyle=\"--\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \nplot_loss(history, Params.model, 0)","9d6fd102":"def plot_metrics(history):\n    metrics = ['loss', 'prc', 'precision', 'recall']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric],color=colors[0], \n                 linestyle=\"--\", label='Val')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n\n        plt.legend()\n\nplot_metrics(history)","b2b79999":"# https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data#evaluate_metrics\n# sns.color_palette(\"mako\", as_cmap=True)\ntest_predictions_baseline = model.predict(val_data[0].numpy(), batch_size=Params.batch_size)\n\n## CHECK\ndef plot_cm(label_matrix, predictions):\n    \n    preds = np.argmax(predictions, axis=1)\n    labels_ = np.argmax(label_matrix, axis=1)\n    \n    cm = confusion_matrix(labels_, \n                          preds,\n                          labels=np.arange(len(list(labels.keys()))))\n    plt.figure(figsize=(8,8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"icefire_r\")\n    indices = np.arange(len(class_names))\n    plt.xticks(indices, class_names, rotation=45)\n    plt.yticks(indices, class_names)\n    plt.title('Confusion matrix')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\nbaseline_results = model.evaluate(val_data[0].numpy(), \n                                  val_data[1].numpy(),\n                                  batch_size=Params.batch_size, \n                                  verbose=0)\n\nfor name, value in zip(model.metrics_names, baseline_results):\n    print(name, ': ', value)\nprint()\n\nplot_cm(val_data[1].numpy(), test_predictions_baseline)\n","9516efe0":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/miscellaneous\/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py\n# https:\/\/datascience.stackexchange.com\/questions\/77112\/how-to-create-roc-auc-curves-for-multi-class-text-classification-problem-in-py\n#         RocCurveDisplay\n\ndef plot_roc(name, labels\u0640, predictions, **kwargs):\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    temp = list(labels.keys())\n    \n    for i, item in enumerate(temp):\n        fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(labels\u0640[:, i], predictions[:, i])\n        roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n    \n    fig, axes = plt.subplots(4, 3, figsize=(17, 15))\n    \n    \n    for i, item in enumerate(temp):\n        ax = axes[i%4][i%3]\n        ax.plot(100*fpr[i], 100*tpr[i], label=f'ROC curve {item} (area = {roc_auc[i]:.2f})', linewidth=2, **kwargs)\n        ax.set_xlabel('False positives [%]')\n        ax.set_ylabel('True positives [%]')\n        ax.legend(loc=\"lower right\")\n#         ax.plot([0, 0], [100, 20], 'k--')\n        ax.set_xlim([-0.5,30])\n        ax.set_ylim([80,100.5])\n        ax.grid(True)\n        s = plt.gca()\n        s.set_aspect('equal')\n\n    \nplot_roc(\"Test Baseline\",\n         val_data[1].numpy(), \n         test_predictions_baseline,\n         color=colors[0], \n         linestyle='--')\n","a5f7f5fe":"# Use this answer\n# https:\/\/stackoverflow.com\/a\/45335434\n\ndef plot_prc(name, labels_, predictions, **kwargs):\n    \n    precision = dict()\n    recall = dict()\n    precision_recall_curve = dict()\n    temp = list(labels.keys())\n    fig, axes = plt.subplots(4, 3, figsize=(17, 15))\n    \n    for i, item in enumerate(temp):\n        precision[i], recall[i], _ = sklearn.metrics.precision_recall_curve(labels_[:, i], predictions[:, i])\n        ax = axes[i%4][i%3]\n        ax.plot(recall[i], precision[i], lw=2, label=item, **kwargs)\n\n        ax.set_xlabel('Recall')\n        ax.set_ylabel('Precision')\n        ax.legend(loc='lower left')\n\n        ax.grid(True)\n        s = plt.gca()\n        s.set_aspect('equal')\n\nplot_prc(\"Test Baseline\",\n         val_data[1].numpy(),\n         test_predictions_baseline,\n         color=colors[0],\n         linestyle='--')\n\n\n\n","efce0350":"# !zip -r tfdata.zip .\/drive\/MyDrive\/logs\/\n# !rm -r drive\/MyDrive\/logs\/*","203bd055":"# Plot Loss ","84ec10af":"# Method 2: Use Deep learning models to extract features and classify the items according to colors","fef7f001":"# Other approaches to image augmentations\n\n### keras preprossing layers\n\nhttps:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation#use_keras_preprocessing_layers\n\n### tf.image\n\nhttps:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation#data_augmentation_3\n","b56448dc":"# Various augmentation techniques using *Albumentation*\n\n- Flip\n- Transpose\n- Rotate\n- Blur (Median - Motion - Glasss)\n- Brightness \n- Noise (Gaussian - Additive - Multiplicative)\n- Distortion (Optical - Grid)\n- Nature inspired (Fog - Rain - Shadow)\n\n\nTutorial: https:\/\/albumentations.ai\/docs\/getting_started\/image_augmentation\/\n\n","ccad6e07":"# Model Evaluation\nIn this part we check these items:\n- **Plot loss for both train and validation.**\n- **Plot precision, recall and area under the curve.**\n- **Plot Confusion matrix based on True Positive\/False Positive and True Negative\/False Negative.**\n- **Plot ROC curve and Area Under Precision-Recall Curve.**","4a8b2776":"# Plot Area Under the Curve","d8bb0af6":"# Data Generator Utils","cc53fdeb":"# TensorBoard Utils","9c1ffc58":"# Training and Transfer Learning Utils","bb68b7be":"# Let's check out classifiers accuracy on hsv format `h channel` histogram","8cad2c58":"# Plot ROC\n\n### Src:\n### https:\/\/stackoverflow.com\/a\/45335434\n### https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data#evaluate_metrics","245a9cec":"# Method 1: Extract hsv histogram (hue channel provides color variety).\n\n[HSV format description](`https:\/\/en.wikipedia.org\/wiki\/HSL_and_HSV`)","731936be":"# Heatmap indicating Multi-Label Confusion Matrix\n\n#### Src: https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data#evaluate_metrics","16aca967":"# Preprocessing Operations","aa3724d1":"### Plotting confusion matrix on tensorboard\n\n![image](https:\/\/github.com\/tensorflow\/tensorboard\/blob\/master\/docs\/images\/images_cm.png?raw=1)","70b1d6b6":"**Successfully `crop_image` can crop the extra white space around the product.**","6e140bff":"# Tensorboard Confusion Matrix\n\n### Epoch 110\n![image.png](attachment:7615a8f7-0c4b-44ae-a360-d9333e58a04d.png)\n\n# GradCam Stripe Images\n\n### Epoch 110\n![image.png](attachment:a39d061c-9cd6-4298-b687-a56359638640.png)\n","0e7ca3bf":"the output is a string typed Tensor. What we are going to do is to produce the image and label. image is going to be accessible by using the image address and its corresponding parent directory would be its label.\n\nNote that `tf.data.Dataset` is going to be run in graph mode and we wouldn't be able to use the simple functions on tensor input. after a little bit of research, I found [this stackoverflow link](https:\/\/stackoverflow.com\/questions\/62079198\/how-to-apply-map-function-to-the-tf-tensor) which suggests `tf.py_function` to overcome this issue.","d0de88f1":"# Crop extra white space around the images\n\nthe function `crop image` is based on [this stackoverflow answer](https:\/\/stackoverflow.com\/questions\/64046602\/how-can-i-crop-an-object-from-surrounding-white-background-in-python-numpy) to my question.","420a5a4a":"# tf_explain Callbacks\n\nWe use tf-explain **GradCam callbacks** to visualize that how good the model can **extract the related features**.\n\n![Pishi](https:\/\/tf-explain.readthedocs.io\/en\/latest\/_images\/grad_cam.png)\n\nSource codes:\n- https:\/\/medium.com\/analytics-vidhya\/tf-explain-working-283a311f1276\n- https:\/\/tf-explain.readthedocs.io\/en\/latest\/methods.html","ee86a06f":"### Set custom callback to write confusion matrix at the end of each validation step on validation data\n\nSources:\n- https:\/\/towardsdatascience.com\/exploring-confusion-matrix-evolution-on-tensorboard-e66b39f4ac12\n- https:\/\/www.tensorflow.org\/tensorboard\/image_summaries","ca2be6da":"# Digikala Products color classification\nIn this notebook we are given over 6240 images of Digikala products and the task is to find a model that can detect the color of each object best.\n\nThere are 12 classes for these corresponding colors:\n- black\n- blue\n- brown\n- green\n- grey\n- orange\n- pink\n- purple\n- red\n- silver\n- white\n- yellow\n\nThese are what we will cover in this notebook:\n\n-  **Extract hsv histogram, use it as features and train `sklearn` classifiers.**\n-  **Use `tf.data` as data generator and practice some of usefule methods.**\n-  **Use `Tensorboard` to track metrics like accuracy, precision, loss, auc-roc curve and etc.**\n- **Save confusion matrix plot and GradCam output (using `tf-explain`) in tensorboard images to get better insights about models.**\n-  **How to apply numpy operations on `tf.data` using tf.py_function.**\n- **Use `albumentations` for more strong and professional augmentations.**","b4d52ed5":"# Plot Loss\/Precision\/Recall\/mean AUC","1d631ec7":"# Initialize deep models and their corresponding preprocessing function in a *dict*","25217d98":"### validation data can not be generators anymore according to:\n\n- [source 1: tensorflow docs](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model#fit)\n- [Source 2: stackoverflow comments ](https:\/\/stackoverflow.com\/a\/60003165\/6118987)\n\n> Validation data \n\n>  validation_data could be:\n- tuple (x_val, y_val) of Numpy arrays or tensors.\n- tuple (x_val, y_val, val_sample_weights) of Numpy arrays.\n\n>  Note that validation_data does not support all the data types that are supported in x, eg, dict, generator or keras.utils.Sequence."}}