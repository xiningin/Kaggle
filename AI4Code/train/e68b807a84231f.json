{"cell_type":{"ea23cd01":"code","875189e7":"code","4a659768":"code","fe28a772":"code","edf32547":"code","038f299a":"code","ac3893f2":"code","d7c40e92":"code","c5feac45":"code","6e11f117":"code","5ae4eb9a":"code","0749f9d1":"code","2865c5be":"code","2d6d3484":"code","33de5418":"code","22abb73b":"code","0c816015":"code","95f60a33":"markdown","da626179":"markdown","69aac396":"markdown","c5d816a9":"markdown","faa9d4ce":"markdown","1236539f":"markdown","c0b4663d":"markdown","47b66493":"markdown","217e332c":"markdown","9820ddb6":"markdown","c872efe5":"markdown","7b35143d":"markdown","b00f66c7":"markdown","2029804e":"markdown","342171b1":"markdown","b608315d":"markdown","371532b2":"markdown","4b1925a2":"markdown"},"source":{"ea23cd01":"# !pip uninstall tensorflow protobuf --yes\n# !pip install tf-nightly","875189e7":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom matplotlib import dates as md\n","4a659768":"master_url_root = \"https:\/\/raw.githubusercontent.com\/numenta\/NAB\/master\/data\/\"\n\ndf_small_noise_url_suffix = \"artificialNoAnomaly\/art_daily_small_noise.csv\"\ndf_small_noise_url = master_url_root + df_small_noise_url_suffix\ndf_small_noise = pd.read_csv(df_small_noise_url)\n\ndf_daily_jumpsup_url_suffix = \"artificialWithAnomaly\/art_daily_jumpsup.csv\"\ndf_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\ndf_daily_jumpsup = pd.read_csv(df_daily_jumpsup_url)\n","fe28a772":"print(df_small_noise.head())\n\nprint(df_daily_jumpsup.head())\n","edf32547":"\ndef plot_dates_values(data):\n    dates = data[\"timestamp\"].to_list()\n    values = data[\"value\"].to_list()\n    dates = [datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\") for x in dates]\n    plt.subplots_adjust(bottom=0.2)\n    plt.xticks(rotation=25)\n    ax = plt.gca()\n    xfmt = md.DateFormatter(\"%Y-%m-%d %H:%M:%S\")\n    ax.xaxis.set_major_formatter(xfmt)\n    plt.plot(dates, values)\n    plt.show()\n\n","038f299a":"plot_dates_values(df_small_noise)\n","ac3893f2":"plot_dates_values(df_daily_jumpsup)\n","d7c40e92":"\ndef get_value_from_df(df):\n    return df.value.to_list()\n\n\ndef normalize(values):\n    mean = np.mean(values)\n    values -= mean\n    std = np.std(values)\n    values \/= std\n    return values, mean, std\n\n\n# Get the `value` column from the training dataframe.\ntraining_value = get_value_from_df(df_small_noise)\n\n# Normalize `value` and save the mean and std we get,\n# for normalizing test data.\ntraining_value, training_mean, training_std = normalize(training_value)\nlen(training_value)\n","c5feac45":"TIME_STEPS = 288\n\n\ndef create_sequences(values, time_steps=TIME_STEPS):\n    output = []\n    for i in range(len(values) - time_steps):\n        output.append(values[i : (i + time_steps)])\n    # Convert 2D sequences into 3D as we will be feeding this into\n    # a convolutional layer.\n    return np.expand_dims(output, axis=2)\n\n\nx_train = create_sequences(training_value)\nprint(\"Training input shape: \", x_train.shape)\n","6e11f117":"n_steps = x_train.shape[1]\nn_features = x_train.shape[2]\n\nkeras.backend.clear_session()\nmodel = keras.Sequential(\n    [\n        layers.Input(shape=(n_steps, n_features)),\n        layers.Conv1D(filters=32, kernel_size=15, padding='same', data_format='channels_last',\n            dilation_rate=1, activation=\"linear\"),\n        layers.LSTM(\n            units=25, activation=\"tanh\", name=\"lstm_1\", return_sequences=False\n        ),\n        layers.RepeatVector(n_steps),\n        layers.LSTM(\n            units=25, activation=\"tanh\", name=\"lstm_2\", return_sequences=True\n        ),\n        layers.Conv1D(filters=32, kernel_size=15, padding='same', data_format='channels_last',\n            dilation_rate=1, activation=\"linear\"),\n        layers.TimeDistributed(layers.Dense(1, activation='linear'))\n    ]\n)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\nmodel.summary()\n","5ae4eb9a":"history = model.fit(\n    x_train,\n    x_train,\n    epochs=200,\n    batch_size=128,\n    validation_split=0.1,\n    callbacks=[\n        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=25, mode=\"min\", restore_best_weights=True)\n    ],\n)\n","0749f9d1":"plt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.legend()\n","2865c5be":"# Get train MAE loss.\nx_train_pred = model.predict(x_train)\ntrain_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n\nplt.hist(train_mae_loss, bins=50)\nplt.xlabel(\"Train MAE loss\")\nplt.ylabel(\"No of samples\")\nplt.show()\n\n# Get reconstruction loss threshold.\nthreshold = np.max(train_mae_loss)\nprint(\"Reconstruction error threshold: \", threshold)\n","2d6d3484":"# Checking how the first sequence is learnt\nplt.plot(x_train[0])\nplt.show()\nplt.plot(x_train_pred[0])\nplt.show()\n","33de5418":"\ndef normalize_test(values, mean, std):\n    values -= mean\n    values \/= std\n    return values\n\n\ntest_value = get_value_from_df(df_daily_jumpsup)\ntest_value = normalize_test(test_value, training_mean, training_std)\nplt.plot(test_value.tolist())\nplt.show()\n\n# Create sequences from test values.\nx_test = create_sequences(test_value)\nprint(\"Test input shape: \", x_test.shape)\n\n# Get test MAE loss.\nx_test_pred = model.predict(x_test)\ntest_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\ntest_mae_loss = test_mae_loss.reshape((-1))\n\nplt.hist(test_mae_loss, bins=50)\nplt.xlabel(\"test MAE loss\")\nplt.ylabel(\"No of samples\")\nplt.show()\n\n# Detect all the samples which are anomalies.\nanomalies = (test_mae_loss > threshold).tolist()\nprint(\"Number of anomaly samples: \", np.sum(anomalies))\nprint(\"Indices of anomaly samples: \", np.where(anomalies))\n","22abb73b":"# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\nanomalous_data_indices = []\nfor data_idx in range(TIME_STEPS - 1, len(test_value) - TIME_STEPS + 1):\n    time_series = range(data_idx - TIME_STEPS + 1, data_idx)\n    if all([anomalies[j] for j in time_series]):\n        anomalous_data_indices.append(data_idx)\n","0c816015":"df_subset = df_daily_jumpsup.iloc[anomalous_data_indices, :]\nplt.subplots_adjust(bottom=0.2)\nplt.xticks(rotation=25)\nax = plt.gca()\nxfmt = md.DateFormatter(\"%Y-%m-%d %H:%M:%S\")\nax.xaxis.set_major_formatter(xfmt)\n\ndates = df_daily_jumpsup[\"timestamp\"].to_list()\ndates = [datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\") for x in dates]\nvalues = df_daily_jumpsup[\"value\"].to_list()\nplt.plot(dates, values, label=\"test data\")\n\ndates = df_subset[\"timestamp\"].to_list()\ndates = [datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\") for x in dates]\nvalues = df_subset[\"value\"].to_list()\nplt.plot(dates, values, label=\"anomalies\", color=\"r\")\n\nplt.legend()\nplt.show()\n","95f60a33":"Let's plot training and validation loss to see how the training went.\n","da626179":"## Load the data\n\nWe will use the [Numenta Anomaly Benchmark(NAB)](\nhttps:\/\/www.kaggle.com\/boltzmannbrain\/nab) dataset. It provides artifical\ntimeseries data containing labeled anomalous periods of behavior. Data are\nordered, timestamped, single-valued metrics.\n\nWe will use the `art_daily_small_noise.csv` file for training and the\n`art_daily_jumpsup.csv` file for testing. The simplicity of this dataset\nallows us to demonstrate anomaly detection effectively.\n","69aac396":"Let's overlay the anomalies on the original test data plot.\n","c5d816a9":"## Plot anomalies\n\nWe now know the samples of the data which are anomalies. With this, we will\nfind the corresponding `timestamps` from the original test data. We will be\nusing the following method to do that:\n\nLet's say time_steps = 3 and we have 10 training values. Our `x_train` will\nlook like this:\n\n- 0, 1, 2\n- 1, 2, 3\n- 2, 3, 4\n- 3, 4, 5\n- 4, 5, 6\n- 5, 6, 7\n- 6, 7, 8\n- 7, 8, 9\n\nAll except the initial and the final time_steps-1 data values, will appear in\n`time_steps` number of samples. So, if we know that the samples\n[(3, 4, 5), (4, 5, 6), (5, 6, 7)] are anomalies, we can say that the data point\n5 is an anomaly.\n","faa9d4ce":"### Timeseries data with anomalies\n\nWe will use the following data for testing and see if the sudden jump up in the\ndata is detected as an anomaly.\n","1236539f":"## Prepare training data\n\nGet data values from the training timeseries data file and normalize the\n`value` data. We have a `value` for every 5 mins for 14 days.\n\n-   24 * 60 \/ 5 = **288 timesteps per day**\n-   288 * 14 = **4032 data points** in total\n","c0b4663d":"### Compare recontruction\n\nJust for fun, let's see how our model has recontructed the first sample.\nThis is the 288 timesteps from day 1 of our training dataset.\n","47b66493":"# Timeseries anomaly detection using an Autoencoder","217e332c":"### Timeseries data without anomalies\n\nWe will use the following data for training.\n","9820ddb6":"## Objective\n\nThis script demonstrates how you can use a reconstruction autoencoder model to detect anomalies in timeseries data.\n","c872efe5":"## Setup\n","7b35143d":"## Quick look at the data\n","b00f66c7":"## Detecting anomalies\n\nWe will detect anomalies by determining how well our model can reconstruct\nthe input data.\n\n\n1.   Find MAE loss on training samples.\n2.   Find max MAE loss value. This is the worst our model has performed trying\nto reconstruct a sample. We will make this the `threshold` for anomaly\ndetection.\n3.   If the reconstruction loss for a sample is greater than this `threshold`\nvalue then we can infer that the model is seeing a pattern that it isn't\nfamiliar with. We will label this sample as an `anomaly`.\n\n","2029804e":"### Create sequences\nCreate sequences combining `TIME_STEPS` contiguous data values from the\ntraining data.\n","342171b1":"## Train the model\n\nPlease note that we are using `x_train` as both the input and the target\nsince this is a reconstruction model.\n","b608315d":"## Visualize the data\n","371532b2":"### Prepare test data\n","4b1925a2":"## Build a model\n\nWe will build a convolutional reconstruction autoencoder model. The model will\ntake input of shape `(batch_size, sequence_length, num_features)` and return\noutput of the same shape. In this case, `sequence_length` is 288 and\n`num_features` is 1.\n"}}