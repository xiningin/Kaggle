{"cell_type":{"7b2bae92":"code","15f50b23":"code","fa7330f5":"code","e0c8c784":"code","782bc7ed":"code","2742b25e":"code","08dca77a":"code","3ed104b3":"code","d362a29a":"code","81afb749":"code","6e9a5cde":"code","f884c1cc":"code","e87856e9":"code","e8f73642":"markdown","b16a5d34":"markdown","494c2c2a":"markdown","8e39b4a7":"markdown","8c0454d0":"markdown","64efd0a2":"markdown","aa04e636":"markdown","a34b238a":"markdown","2f8104f2":"markdown","db4918f1":"markdown"},"source":{"7b2bae92":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nimport warnings\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","15f50b23":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsub = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n# Preview the data\ntrain.head()","fa7330f5":"print(\"Train shape: \", train.shape, \"\\nTest shape: \", test.shape)","e0c8c784":"# Separate target from features\ny = train.target\ntrain.drop(['target','id'], axis = 1, inplace = True)\ntest.drop(['id'], axis = 1, inplace = True)\ny.head()","782bc7ed":"print(\"Train shape: \", train.shape, \"\\nTest shape: \", test.shape)","2742b25e":"from sklearn.preprocessing import OrdinalEncoder\ncat_cols = [col for col in train.columns if 'cat' in col]\n\nX = train.copy()\nX_test = test.copy()\nenc = OrdinalEncoder()\nX[cat_cols] = enc.fit_transform(train[cat_cols])\nX_test[cat_cols] = enc.transform(test[cat_cols])\nX.head()","08dca77a":"#X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","3ed104b3":"# # import machine learning libraries\n# import xgboost as xgb\n# from sklearn.metrics import mean_absolute_error\n# from sklearn.metrics import mean_squared_error\n\n# # import packages for hyperparameters tuning\n# from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 0)\n\n# space={'max_depth': hp.quniform(\"max_depth\", 2, 10, 1),\n#         'gamma': hp.uniform ('gamma', 0,3),\n#         'reg_alpha' : hp.quniform('reg_alpha', 5,50,1),\n#         'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n#         'colsample_bytree' : hp.uniform('colsample_bytree', 0.3,1),\n#         'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n#         'n_estimators' : hp.choice('n_estimators', np.arange(8000, 13001, 500, dtype=int)),\n#         'learning_rate' : hp.uniform('learning_rate', 0.01,0.75),\n#         'subsample' : hp.uniform('subsample', 0.2,1),\n#         'seed': 0\n#     }\n\n# def objective(space):\n#     clf=xgb.XGBRegressor(\n#                     n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n#                     reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n#                     colsample_bytree=int(space['colsample_bytree']))\n    \n#     evaluation = [( X_train, y_train), ( X_test, y_test)]\n    \n#     clf.fit(X_train, y_train,\n#             eval_set=evaluation, eval_metric=\"rmse\",\n#             early_stopping_rounds=10,verbose=False)\n    \n\n#     pred = clf.predict(X_test)\n#     mae = mean_absolute_error(y_test,pred)\n#     rmse = np.sqrt(mean_squared_error(y_test,pred))\n#     print(\"Mean Absolute Error:\" , mae)\n#     print(\"Root Mean Squared Error:\" , rmse)\n#     return {'loss': rmse, 'status': STATUS_OK }","d362a29a":"# warnings.filterwarnings(\"ignore\")\n# trials = Trials()\n# best = fmin(fn=objective,\n#             space=space,\n#             algo=tpe.suggest,\n#             max_evals=200, # change\n#             trials=trials)\n# print(best)","81afb749":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nwarnings.filterwarnings(\"ignore\")\n\nxgb_params = {'colsample_bytree': 0.5370475144455653,\n            'gamma': 0.44740331029919433,\n            'learning_rate': 0.41376596243713015,\n            'max_depth': 8,\n            'min_child_weight': 1.0,\n            'n_estimators': 13000,\n            'reg_alpha': 39.0,\n            'reg_lambda': 0.0557963001452326,\n            'subsample': 0.21049744017223115,\n            'booster': 'gbtree', \n            'random_state': 0,\n            'n_jobs': -1}\n\n# xgb_params = {'n_estimators': 10000,\n#               'learning_rate': 0.4221491940327969,\n#               'subsample': 0.5223967706745876,\n#               'colsample_bytree': 0.9323858528945878,\n#               'gamma': 1.2536233854787242,\n#               'min_child_weight': 5.0,\n#               'max_depth': 7,\n#               'booster': 'gbtree', \n#               'reg_lambda': 0.9703073647842214,\n#               'reg_alpha': 40,\n#               'random_state': 0,\n#               'n_jobs': -1}\n\n#Setting the kfold parameters\nkf = KFold(n_splits = 10, shuffle = True, random_state = 0)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 250)\n    \n    #Mean of the predictions\n    preds += model.predict(X_test) \/ 10 # Splits\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ 10 #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 10\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")\n","6e9a5cde":"sub.target = preds\nsub.head()","f884c1cc":"sub.to_csv(\"submission_2.csv\", index = False)","e87856e9":"# Use the model to generate predictions\n#predictions = model_2.predict(X_test)\n\n# Save the predictions to a CSV file\n#output = pd.DataFrame({'Id': X_test.index,\n#                       'target': predictions})\n#output.to_csv('submission.csv', index=False)","e8f73642":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","b16a5d34":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data.","494c2c2a":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","8e39b4a7":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","8c0454d0":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","64efd0a2":"Next, we break off a validation set from the training data.","aa04e636":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","a34b238a":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","2f8104f2":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","db4918f1":"Set up the Light GBM first with finding optimal tuning parameters"}}