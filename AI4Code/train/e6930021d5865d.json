{"cell_type":{"7195626c":"code","8ce0e331":"code","768ff501":"code","91452e95":"code","2ea3f7b0":"code","ee177bd7":"code","d53573ec":"code","a67f9d26":"code","d6a605a0":"code","f0429db2":"code","6d7b1dea":"code","1d9d94a3":"code","03694edc":"code","2e926a17":"code","809a9e5f":"code","ce9bf0cf":"code","ac70dc35":"code","27d364d4":"code","d235cb18":"code","d44b6964":"code","10a989e1":"code","c0b6a178":"code","4af18be7":"code","e8e314f1":"code","c5941617":"code","99ab43ec":"code","d422b9a2":"code","832bc220":"code","af503e5e":"code","a6b26535":"code","eff13863":"code","d96857f6":"code","feb06609":"code","be498151":"code","d2dfebcd":"code","c6980abd":"code","3708b885":"code","ba12fdd8":"code","8b14263d":"code","adcddd49":"code","970e79a0":"code","86445c28":"code","c26db093":"code","4bec0882":"code","2f7ac343":"code","c57a18d8":"code","85cd21a7":"code","ec18bb9e":"code","a72d8f68":"code","452dfd83":"code","cf9a74c7":"code","1e74be63":"code","ed780123":"code","133c6a3d":"code","c14b8c88":"code","7e614a89":"code","8cff12dc":"code","fb2ade5b":"code","d9d1e6cf":"code","bb3308e4":"code","e447a3d2":"code","7eaac9cf":"code","33a0482f":"code","53afef93":"code","9dd8e374":"code","488397e9":"code","c58ba7b7":"code","b84fe57a":"markdown","00b27643":"markdown","b995b98d":"markdown","b80ccdbe":"markdown","d044dd36":"markdown","0046abec":"markdown","abc7e8e1":"markdown"},"source":{"7195626c":"%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport time\nimport math\nimport tqdm as tqdm\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    TRAIN_CSV = os.path.join(dirname, 'train.csv')\n    TEST_CSV = os.path.join(dirname, 'test.csv')\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nprint(TRAIN_CSV)\nprint(TEST_CSV)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","8ce0e331":"train_df = pd.read_csv(TRAIN_CSV)\nprint(train_df.head())\n\ntest_df = pd.read_csv(TEST_CSV)\nprint(test_df.head())\n\ntrain_labels = train_df['label']\nprint(train_labels)\n\ntrain_df = train_df.drop(['label'], axis=1)\nprint(train_df.head())\n\nprint(\"Number of training example: {}\".format(len(train_df)))\nprint(\"Number of testing example: {}\".format(len(test_df)))","768ff501":"train_labels.value_counts()","91452e95":"plt.rcParams['figure.figsize'] = (8, 6)\nplt.bar(train_labels.value_counts().index, train_labels.value_counts())\nplt.xticks(np.arange(train_labels.nunique()))\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.show()","2ea3f7b0":"transform = transforms.Compose([\n    #transforms.ToPILImage(),\n    #transforms.RandomAffine(degrees=15, translate=(0.15, 0.15), scale=(0.8, 1.2)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])","ee177bd7":"class DS(Dataset):\n    def __init__(self, imgs, labels=None, transform=None):\n        self.imgs = imgs\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, index):\n        image = self.imgs.iloc[index].values.astype(np.uint8).reshape((28, 28))\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.labels is not None:\n            label = self.labels.iloc[index]\n        else:\n            return image       \n        return image, label","d53573ec":"features_train, features_test, targets_train, targets_test = train_test_split(train_df,train_labels, test_size = 0.1,\n                                                                             random_state = 42)","a67f9d26":"trainset = DS(features_train, targets_train, transform)\nvalidset = DS(features_test, targets_test, transform)\ntestset = DS(test_df, transform=transform)","d6a605a0":"bs = 64","f0429db2":"train_loader = DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(validset, batch_size=bs, shuffle=True, num_workers=4)\ntest_loader = DataLoader(testset, batch_size=bs, shuffle=False, num_workers=4)","6d7b1dea":"def plot_img(imgs, labels, preds=None, is_pred=False, columns = 4):\n    assert(len(imgs) > 0 and len(labels) > 0 and len(imgs) == len(labels))\n    fig = plt.figure(figsize=(8,8))\n    rows = math.ceil(len(imgs)\/columns) \n    \n    for i in range(1, columns*rows +1):\n        fig.add_subplot(rows, columns, i)\n        if is_pred:\n            plt.title(\"Pred:: \" + str(labels[i-1].item()) + \"\/Act:: \" + str(train_labels[i-1].item()))\n        else:\n            plt.title(\"Pred:: \" + str(labels[i-1].item()))\n        plt.axis('off')\n        plt.imshow(imgs[i-1].numpy().reshape(28,28), cmap='gray')\n    plt.show()","1d9d94a3":"imgs, labels = next(iter(train_loader))\nplot_img(imgs[:20], labels[:20])","03694edc":"def accuracy(output, target, is_test=False):\n    global total\n    global correct\n    batch_size = target.size(0)\n    total += batch_size\n    \n    _, pred = torch.max(output, 1)\n    if is_test:\n        preds.extend(pred)\n    correct += (pred == target).sum()\n    return 100 * float(correct) \/ float(total)","2e926a17":"class AvgStats(object):\n    def __init__(self):\n        self.reset()\n        \n    def reset(self):\n        self.losses =[]\n        self.precs =[]\n        self.its = []\n        \n    def append(self, loss, prec, it):\n        self.losses.append(loss)\n        self.precs.append(prec)\n        self.its.append(it)","809a9e5f":"def save_checkpoint(model, is_best, filename='.\/checkpoint.pth.tar'):\n    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n    if is_best:\n        torch.save(model.state_dict(), filename)  # save checkpoint\n    else:\n        print (\"=> Validation Accuracy did not improve\")","ce9bf0cf":"def load_checkpoint(model, filename = '.\/checkpoint.pth.tar'):\n    sd = torch.load(filename, map_location=lambda storage, loc: storage)\n    names = set(model.state_dict().keys())\n    for n in list(sd.keys()):\n        if n not in names and n+'_raw' in names:\n            if n+'_raw' not in sd: sd[n+'_raw'] = sd[n]\n            del sd[n]\n    model.load_state_dict(sd)","ac70dc35":"class CLR(object):\n    def __init__(self, optim, bn, base_lr=1e-7, max_lr=100):\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.optim = optim\n        self.bn = bn - 1\n        ratio = self.max_lr\/self.base_lr\n        self.mult = ratio ** (1\/self.bn)\n        self.best_loss = 1e9\n        self.iteration = 0\n        self.lrs = []\n        self.losses = []\n        \n    def calc_lr(self, loss):\n        self.iteration +=1\n        if math.isnan(loss) or loss > 4 * self.best_loss:\n            return -1\n        if loss < self.best_loss and self.iteration > 1:\n            self.best_loss = loss\n            \n        mult = self.mult ** self.iteration\n        lr = self.base_lr * mult\n        \n        self.lrs.append(lr)\n        self.losses.append(loss)\n        \n        return lr\n        \n    def plot(self, start=10, end=-5):\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Losses\")\n        plt.plot(self.lrs[start:end], self.losses[start:end])\n        plt.xscale('log')\n        \n        \n    def plot_lr(self):\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Learning Rate\")\n        plt.plot(self.lrs)\n        plt.yscale('log')","27d364d4":"def update_lr(optimizer, lr):\n    for g in optimizer.param_groups:\n        g['lr'] = lr","d235cb18":"def update_mom(optimizer, mom):\n    for g in optimizer.param_groups:\n        g['momentum'] = mom","d44b6964":"class AdaptiveConcatPool2d(nn.Module):\n    def __init__(self, sz=1):\n        super().__init__()\n        self.adavgp = nn.AdaptiveAvgPool2d(sz)\n        self.adamaxp = nn.AdaptiveMaxPool2d(sz)\n        \n    def forward(self, x):\n        x = torch.cat([self.adavgp(x), self.adamaxp(x)], 1)\n        x = x.view(x.size(0),-1)\n        return x","10a989e1":"class CustomClassifier(nn.Module):\n    def __init__(self, in_features, intermed_bn= 512, out_features=10, dout=0.25):\n        super().__init__()\n        self.fc_bn0 = nn.BatchNorm1d(in_features)\n        self.dropout0 = nn.Dropout(dout)\n        self.fc0 = nn.Linear(in_features, intermed_bn, bias=True)\n        self.fc_bn1 = nn.BatchNorm1d(intermed_bn, momentum=0.01)\n        self.dropout1 = nn.Dropout(dout * 2)\n        self.fc1 = nn.Linear(intermed_bn, out_features, bias=True)\n        \n    def forward(self, x):\n        x = self.fc_bn0(x)\n        x = self.dropout0(x)\n        x = F.relu(self.fc0(x))\n        x = self.fc_bn1(x)\n        x = self.dropout1(x)\n        x = self.fc1(x)\n        return x","c0b6a178":"class OneCycle(object):\n    def __init__(self, nb, max_lr, momentum_vals=(0.95, 0.85), prcnt= 10, div=10, use_cosine=False):\n        self.nb = nb\n        self.div = div\n        self.high_lr = max_lr\n        self.low_mom = momentum_vals[1]\n        self.high_mom = momentum_vals[0]\n        self.use_cosine = use_cosine\n        if self.use_cosine:\n            self.prcnt = 0\n        else:\n            self.prcnt = prcnt\n        self.iteration = 0\n        self.lrs = []\n        self.moms = []\n        if self.use_cosine:\n            self.step_len =  int(self.nb \/ 4)\n        else:\n            self.step_len =  int(self.nb * (1- prcnt\/100)\/2)\n        \n    def calc(self):\n        if self.use_cosine:\n            lr = self.calc_lr_cosine()\n            mom = self.calc_mom_cosine()\n        else:\n            lr = self.calc_lr()\n            mom = self.calc_mom()\n        self.iteration += 1\n        return (lr, mom)\n        \n    def calc_lr(self):\n        if self.iteration ==  0:\n            self.lrs.append(self.high_lr\/self.div)\n            return self.high_lr\/self.div\n        elif self.iteration == self.nb:\n            self.iteration = 0\n            self.lrs.append(self.high_lr\/self.div)\n            return self.high_lr\/self.div\n        elif self.iteration > 2 * self.step_len:\n            ratio = (self.iteration - 2 * self.step_len) \/ (self.nb - 2 * self.step_len)\n            #lr = self.high_lr * ( 1 - 0.99 * ratio)\/self.div\n            lr = (self.high_lr \/ self.div) * (1- ratio * (1 - 1\/self.div))\n        elif self.iteration > self.step_len:\n            ratio = 1- (self.iteration -self.step_len)\/self.step_len\n            lr = self.high_lr * (1 + ratio * (self.div - 1)) \/ self.div\n        else :\n            ratio = self.iteration\/self.step_len\n            lr = self.high_lr * (1 + ratio * (self.div - 1)) \/ self.div\n        self.lrs.append(lr)\n        return lr\n\n    def calc_mom(self):\n        if self.iteration == 0:\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        elif self.iteration == self.nb:\n            self.iteration = 0\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        elif self.iteration > 2 * self.step_len:\n            mom = self.high_mom\n        elif self.iteration > self.step_len:\n            ratio = (self.iteration -self.step_len)\/self.step_len\n            mom = self.low_mom + ratio * (self.high_mom - self.low_mom)\n        else :\n            ratio = self.iteration\/self.step_len\n            mom = self.high_mom - ratio * (self.high_mom - self.low_mom)\n        self.moms.append(mom)\n        return mom\n\n    def calc_lr_cosine(self):\n        if self.iteration ==  0:\n            self.lrs.append(self.high_lr\/self.div)\n            return self.high_lr\/self.div\n        elif self.iteration == self.nb:\n            self.iteration = 0\n            self.lrs.append(self.high_lr\/self.div)\n            return self.high_lr\/self.div\n        elif self.iteration > self.step_len:\n            ratio = (self.iteration -self.step_len)\/(self.nb - self.step_len)\n            lr = (self.high_lr\/self.div) + 0.5 * (self.high_lr - self.high_lr\/self.div) * (1 + math.cos(math.pi * ratio))\n        else :\n            ratio = self.iteration\/self.step_len\n            lr = self.high_lr - 0.5 * (self.high_lr - self.high_lr\/self.div) * (1 + math.cos(math.pi * ratio))\n        self.lrs.append(lr)\n        return lr\n\n    def calc_mom_cosine(self):\n        if self.iteration == 0:\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        elif self.iteration == self.nb:\n            self.iteration = 0\n            self.moms.append(self.high_mom)\n            return self.high_mom\n        elif self.iteration > self.step_len:\n            ratio = (self.iteration -self.step_len)\/(self.nb - self.step_len)\n            mom = self.high_mom - 0.5 * (self.high_mom - self.low_mom) * (1 + math.cos(math.pi * ratio))\n        else :\n            ratio = self.iteration\/self.step_len\n            mom = self.low_mom + 0.5 * (self.high_mom - self.low_mom) * (1 + math.cos(math.pi * ratio))\n        self.moms.append(mom)\n        return mom","4af18be7":"train_stats = AvgStats()\ntest_stats = AvgStats()","e8e314f1":"total = 0\ncorrect = 0","c5941617":"train_loss = 0\ntest_loss = 0\nbest_acc = 0\ntrn_losses = []\ntrn_accs = []\nval_losses = []\nval_accs = []\npreds = []","99ab43ec":"def train(model, epoch=0, use_cycle = False):\n    model.train()\n    global best_acc\n    global trn_accs, trn_losses\n    is_improving = True\n    counter = 0\n    running_loss = 0.\n    avg_beta = 0.98\n            \n    for i, (input, target) in enumerate(train_loader):\n        bt_start = time.time()\n        input, target = input.to(device), target.to(device)\n        var_ip, var_tg = Variable(input), Variable(target)\n        \n        if use_cycle:    \n            lr, mom = onecycle.calc()\n            update_lr(optimizer, lr)\n            update_mom(optimizer, mom)\n        \n        output = model(var_ip)\n        loss = criterion(output, var_tg)\n            \n        running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n        smoothed_loss = running_loss \/ (1 - avg_beta**(i+1))\n        \n        trn_losses.append(smoothed_loss)\n            \n        # measure accuracy and record loss\n        prec = accuracy(output.data, target)\n        trn_accs.append(prec)\n\n        train_stats.append(smoothed_loss, prec, time.time()-bt_start)\n        if prec > best_acc :\n            best_acc = prec\n            save_checkpoint(model, True)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()           ","d422b9a2":"def test(model):\n    with torch.no_grad():\n        model.eval()\n        global val_accs, val_losses\n        running_loss = 0.\n        avg_beta = 0.98\n        for i, (input, target) in enumerate(valid_loader):\n            bt_start = time.time()\n            input, target = input.to(device), target.to(device)\n            var_ip, var_tg = Variable(input), Variable(target)\n            output = model(var_ip)\n            loss = criterion(output, var_tg)\n\n            running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n            smoothed_loss = running_loss \/ (1 - avg_beta**(i+1))\n\n            # measure accuracy and record loss\n            prec = accuracy(output.data, target, is_test=True)\n            test_stats.append(loss.item(), prec, time.time()-bt_start)\n\n            val_losses.append(smoothed_loss)\n            val_accs.append(prec)\n            \n        return running_loss","832bc220":"def fit(model, scheduler= None, use_cycle = False):\n    print(\"Epoch\\tTrain loss\\tValidn loss\\tTrain acc\\tValidn acc\")\n    for j in range(epoch):\n        train(model, j, use_cycle=use_cycle)\n        loss = test(model)\n        if scheduler is not None:\n            scheduler.step(loss)\n        print(\"{}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\"\n              .format(j+1, trn_losses[-1], val_losses[-1], trn_accs[-1], val_accs[-1]))","af503e5e":"model = models.resnet18(pretrained=True)\nmodel","a6b26535":"model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nmodel.avgpool = AdaptiveConcatPool2d()\nmodel.fc = CustomClassifier(in_features=model.fc.in_features*2, out_features=10)","eff13863":"for param in model.parameters():\n    param.require_grad = False\n    \nfor param in model.fc.parameters():\n    param.require_grad = True\n    \nmodel = model.to(device)","d96857f6":"criterion = nn.CrossEntropyLoss()","feb06609":"optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)","be498151":"clr = CLR(optimizer, len(train_loader))","d2dfebcd":"save_checkpoint(model, True, 'before_clr.pth.tar')","c6980abd":"def lr_find(model):\n    t = tqdm.tqdm(train_loader, leave=False, total=len(train_loader))\n    running_loss = 0.\n    avg_beta = 0.98\n    model.train()\n    for i, (input, target) in enumerate(t):\n        input, target = input.to(device), target.to(device)\n        var_ip, var_tg = Variable(input), Variable(target)\n        output = model(var_ip)\n        loss = criterion(output, var_tg)\n\n        running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n        smoothed_loss = running_loss \/ (1 - avg_beta**(i+1))\n        t.set_postfix(loss=smoothed_loss)\n\n        lr = clr.calc_lr(smoothed_loss)\n        if lr == -1 :\n            break\n        update_lr(optimizer, lr)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","3708b885":"lr_find(model)","ba12fdd8":"clr.plot()","8b14263d":"load_checkpoint(model, 'before_clr.pth.tar')","adcddd49":"epoch = 40","970e79a0":"optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)","86445c28":"#sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, min_lr=1e-4)","c26db093":"onecycle = OneCycle(int(len(train_loader) * epoch \/bs), 1e-2, use_cosine=True)","4bec0882":"fit(model, scheduler=None, use_cycle=True)","2f7ac343":"save_checkpoint(model, True, 'before_clr_unfreeze.pth.tar')","c57a18d8":"for param in model.parameters():\n    param.require_grad = True","85cd21a7":"optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)","ec18bb9e":"clr = CLR(optimizer, len(train_loader), base_lr=1e-30)","a72d8f68":"lr_find(model)","452dfd83":"clr.plot(start=0, end=-1)","cf9a74c7":"load_checkpoint(model, 'before_clr_unfreeze.pth.tar')","1e74be63":"optimizer = torch.optim.SGD(model.parameters(), lr=1e-30, momentum=0.9, weight_decay=1e-4)","ed780123":"#sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, min_lr=1e-24)","133c6a3d":"onecycle = OneCycle(int(len(train_loader) * epoch \/bs), 1e-29, use_cosine=True)","c14b8c88":"fit(model, scheduler=None, use_cycle=True)","7e614a89":"ep_losses = []\nfor i in range(0, len(train_stats.losses), len(train_loader)):\n    if i != 0 :\n        ep_losses.append(train_stats.losses[i])\n        \nep_lossesv = []\nfor i in range(0, len(test_stats.losses), len(valid_loader)):\n    if(i != 0):\n        ep_lossesv.append(test_stats.losses[i])","8cff12dc":"ep_accs = []\nfor i in range(0, len(train_stats.precs), len(train_loader)):\n    if i != 0 :\n        ep_accs.append(train_stats.precs[i])\n        \nep_accsv = []\nfor i in range(0, len(test_stats.precs), len(valid_loader)):\n    if(i != 0):\n        ep_accsv.append(test_stats.precs[i])","fb2ade5b":"plt.xlabel(\"Iterations\")\nplt.ylabel(\"Accuracy\")\nplt.yticks(np.arange(0, 100))\nplt.plot(ep_accs, 'r', label='Train')\nplt.plot(ep_accsv, 'b', label='Valid')\nplt.legend()","d9d1e6cf":"plt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.yticks(np.arange(0, 100, step=10))\nplt.plot(ep_losses, 'r', label='Train')\nplt.plot(ep_lossesv, 'b', label='Valid')\nplt.legend()","bb3308e4":"preds = []","e447a3d2":"with torch.no_grad():\n    model.eval()\n    for i, input in enumerate(test_loader):\n        input = input.to(device)\n        var_ip = Variable(input)\n        output = model(var_ip)\n        _, pred = torch.max(output, 1)\n        preds.extend(pred.tolist())","7eaac9cf":"len(preds)","33a0482f":"preds","53afef93":"import csv","9dd8e374":"!ls","488397e9":"sb = open('final_sub.csv', 'w', newline='')\nwriter = csv.writer(sb)\nwriter.writerow([\"ImageId\", \"Label\"])\n\nfor i, pred in enumerate(preds):\n    writer.writerow([i+1, pred])\n    \nsb.close()","c58ba7b7":"from IPython.display import FileLink\nFileLink('final_sub.csv')","b84fe57a":"# Train and Test Loops","00b27643":"# Transfer Learning","b995b98d":"# Initialize","b80ccdbe":"# Load Dataset","d044dd36":"# Plot Images","0046abec":"Implementatation of Cyclic Learning Rate and One cycle Policy @ https:\/\/github.com\/nachiket273\/One_Cycle_Policy","abc7e8e1":"# Helper Functions"}}