{"cell_type":{"4ca75904":"code","02c05134":"code","39ccd1da":"code","4866b2ff":"code","e9fd2092":"code","e419fa51":"code","11b1d7ff":"code","df2f6b1b":"code","f8eeefc4":"code","3221bf0a":"code","f66bc9b0":"code","db27d28a":"code","f13cf4b0":"code","2766c644":"code","08eb9cc8":"code","8e92439f":"code","177f7817":"code","37898317":"code","3b89794f":"code","8568725e":"code","ed0fb127":"code","facdfcaa":"code","6710a10b":"code","2c4a39f2":"code","ff55ab8a":"code","1c2d0e6f":"code","85359092":"code","3be0b5d6":"code","598eecd1":"code","e40602ff":"code","a5db40eb":"code","64d88a50":"code","24711648":"code","ccb4af88":"code","64ca8140":"code","8c6e8636":"code","4f29f149":"code","0885fdf3":"code","88dfce19":"code","91e3429f":"code","0e742f91":"code","def2180e":"code","8ce6132e":"code","d9485c78":"code","429e2415":"code","25fbedc1":"code","fc5d8faa":"code","6bd76db8":"code","0b64c83b":"code","590ba1b2":"code","1630b32a":"code","86afeeaf":"code","92779b95":"code","68d6d1df":"code","6f455202":"code","598a6bff":"code","dc487980":"code","015ea8f0":"markdown"},"source":{"4ca75904":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore scikit\/sns warnings\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #limit float outputs to 3","02c05134":"test = pd.read_csv(\"..\/input\/test.csv\", index_col = 0)\ntrain = pd.read_csv(\"..\/input\/train.csv\", index_col = 0)\ntrain.head()","39ccd1da":"sns.lmplot(data = train, x = \"GrLivArea\", y = \"SalePrice\") #can see outliers that affect the model\nplt.ylabel(\"Sale Price\")\nplt.xlabel(\"Living Area\")","4866b2ff":"#remove outliers - very large\ntrain = train.drop(train[(train[\"GrLivArea\"] > 4000) & (train[\"SalePrice\"] < 300000)].index)\nsns.lmplot(data = train, x = \"GrLivArea\", y = \"SalePrice\")\nplt.ylabel(\"Sale Price\")\nplt.xlabel(\"Living Area\")","e9fd2092":"#now to look at saleprice\nplt.boxplot(train[\"SalePrice\"])","e419fa51":"sns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice']) #get normal dist parameters\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best') #plot distribution details\nplt.ylabel('Frequency')\nplt.title('Sale Price distribution')\n\nfig = plt.figure() #qqplot\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show() #data is heavily right skewed, can correct","11b1d7ff":"#apply log(1+x) to sale price\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\nsns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice']) #get normal dist parameters\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best') #plot distribution details\nplt.ylabel('Frequency')\nplt.title('Sale Price distribution')\n\nfig = plt.figure() #qqplot\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show() #skew is gone\n\ny_train = train[\"SalePrice\"] #assign normalised y","df2f6b1b":"plt.boxplot(train[\"SalePrice\"]) #skew has been removed","f8eeefc4":"#combine test and train\nall_data = pd.concat([train.drop([\"SalePrice\"], axis = 1), test])\n\n#look at na values\nall_data.describe()","3221bf0a":"na_ratios = all_data.isnull().sum() \/ len(all_data)\nna_ratios = na_ratios.drop(na_ratios[na_ratios == 0].index).sort_values(ascending = False)\nna_ratios = pd.DataFrame({\"Missing Proportion\": na_ratios})\nna_ratios = na_ratios.drop(na_ratios[na_ratios[\"Missing Proportion\"] <0.005].index)\nna_ratios","f66bc9b0":"#the amount of missing data for \"Garage\", \"Bsmt\" and \"Mas\" are each roughly the same, clearing up plot\nna_ratios = na_ratios.drop(index=[\"GarageFinish\", \"GarageYrBlt\", \"GarageQual\", \"GarageCond\"])\nna_ratios = na_ratios.drop(index=[\"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\"])\nna_ratios = na_ratios.drop(index=[\"MasVnrType\"])\n\nplt.subplots(figsize=(12, 12))\nplt.xticks(rotation = \"90\")\nsns.barplot(x=na_ratios.index, y = na_ratios[\"Missing Proportion\"])\nplt.xlabel(\"Features\", fontsize = 15)\nplt.title(\"Proportion of Missing Data for Each Feature\", fontsize = 18)\nplt.ylabel(\"Missing Proportion\", fontsize = 15)","db27d28a":"plt.subplots(figsize=(20,15))\nsns.heatmap(all_data.corr(), vmax = 0.9, square = True, annot = True) #correlation plot for various features","f13cf4b0":"plt.subplots(figsize=(20,15))\nsns.heatmap(train.corr(), vmax = 0.9, square = True, annot = True) #correlation plot with saleprice","2766c644":"na_ratios = all_data.isnull().sum()\nna_ratios = na_ratios.drop(na_ratios[na_ratios == 0].index)\nna_ratios = pd.DataFrame({\"Missing Count\": na_ratios})\nprint(na_ratios) #all missing values\nna_ratios.to_csv(\"NA_Ticklist.csv\")","08eb9cc8":"all_data[\"MSZoning\"] = all_data[\"MSZoning\"].fillna(\"RL\") #fill na with most common value\nall_data[\"MSZoning\"].value_counts()","8e92439f":"#basing the lotfrontage on the building subclass - lotfrontage related to building type\nlf_est = all_data[[\"LotFrontage\", \"MSSubClass\"]].pivot_table(\"LotFrontage\", \"MSSubClass\", aggfunc = \"median\")\nlf_est = lf_est.fillna(np.nanmedian(all_data[\"LotFrontage\"]))\nlf_est = pd.Series(lf_est[\"LotFrontage\"], index = lf_est.index)\nlf_est","177f7817":"lf_na_index = all_data[\"LotFrontage\"][all_data[\"LotFrontage\"].isnull()].index\nlf_na_ests = all_data[\"MSSubClass\"][lf_na_index].map(lf_est)\nall_data[\"LotFrontage\"][lf_na_index] = lf_na_ests\nall_data[[\"LotFrontage\", \"MSSubClass\"]].ix[lf_na_index].head() #check for correct mapping\n#could also use groupby on neighbourhood and use those medians\nall_data[\"LotFrontage\"] = all_data[\"LotFrontage\"].fillna(np.nanmedian(all_data[\"LotFrontage\"]))","37898317":"#following NAs need to be convered to \"none\" or 0 according to data descriptions\nna_means_none_features = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\n                         \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\",\n                         \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\nall_data[na_means_none_features] = all_data[na_means_none_features].fillna(\"None\")","3b89794f":"all_data[\"Utilities\"] = all_data[\"Utilities\"].fillna(\"AllPub\") #fill with most common value\nall_data[\"Utilities\"].value_counts()","8568725e":"all_data[\"Exterior1st\"] = all_data[\"Exterior1st\"].fillna(\"VinylSd\")#fill with most common value\nall_data[\"Exterior1st\"].value_counts()","ed0fb127":"all_data[\"Exterior2nd\"] = all_data[\"Exterior2nd\"].fillna(\"VinylSd\") #fill with most common value\nall_data[\"Exterior2nd\"].value_counts()","facdfcaa":"fill_feature = \"MasVnrType\"\nall_data[fill_feature] = all_data[fill_feature].fillna(\"None\") #missing value likely means none\nall_data[fill_feature].value_counts()\nfill_feature = \"MasVnrArea\"\nall_data[fill_feature] = all_data[fill_feature].fillna(0) #as previous","6710a10b":"#bsmtqual means no basement, so fill other variables appropriately\nfill_feature = [\"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"BsmtFullBath\", \"BsmtHalfBath\", \"TotalBsmtSF\"]\nall_data[fill_feature] = all_data[fill_feature].fillna(0)\nall_data[fill_feature].head()","2c4a39f2":"fill_feature = \"SaleType\"\nall_data[fill_feature] = all_data[fill_feature].fillna(\"WD\") #replace with most common value\nall_data[fill_feature].value_counts()","ff55ab8a":"fill_feature = \"Electrical\"\nall_data[fill_feature] = all_data[fill_feature].fillna(\"SBrkr\") #replace with most common value\nall_data[fill_feature].value_counts()","1c2d0e6f":"fill_feature = \"KitchenQual\"\nall_data[fill_feature] = all_data[fill_feature].fillna(\"TA\") #replace with most common value\nall_data[fill_feature].value_counts()","85359092":"fill_feature = \"Functional\"\nall_data[fill_feature] = all_data[fill_feature].fillna(\"Typ\") #replace with most common value\nall_data[fill_feature].value_counts()","3be0b5d6":"fill_feature = [\"GarageCars\", \"GarageArea\"]\nall_data[fill_feature] = all_data[fill_feature].fillna(0) #replace with most common value\nall_data[fill_feature].head()","598eecd1":"#replace garageyrblt nas based on OverallQual\ngarageYrMap = all_data.pivot_table(\"GarageYrBlt\", \"OverallQual\")\ngarageYrMap = pd.Series(garageYrMap[\"GarageYrBlt\"],\n                       index = garageYrMap.index)\nqual_to_garage_map = all_data[\"OverallQual\"].map(garageYrMap)\ngarageYrBlt_nas = all_data[\"GarageYrBlt\"].isnull().index\nall_data[\"GarageYrBlt\"][all_data[\"GarageYrBlt\"].isnull()] = qual_to_garage_map[all_data[\"GarageYrBlt\"].isnull()]\ngarageYrMap","e40602ff":"na_ratios = all_data.isnull().sum()\nna_ratios = na_ratios.drop(na_ratios[na_ratios == 0].index)\nna_ratios = pd.DataFrame({\"Missing Count\": na_ratios})\nprint(na_ratios) #all missing values filled","a5db40eb":"numerical_features = ['LotFrontage',\n 'LotArea',\n 'OverallQual',\n 'YearBuilt',\n 'MasVnrArea',\n 'BsmtFinSF1',\n 'BsmtFinSF2',\n 'BsmtUnfSF',\n 'TotalBsmtSF',\n '1stFlrSF',\n '2ndFlrSF',\n 'GrLivArea',\n 'FullBath',\n 'TotRmsAbvGrd',\n 'Fireplaces',\n 'GarageYrBlt',\n 'GarageCars',\n 'GarageArea',\n 'WoodDeckSF',\n 'OpenPorchSF',\n 'EnclosedPorch',\n 'ScreenPorch',\n 'TotalSF'] #list of all the features that will be considered numerical or ordered categorical\n\nunordered_cat_features = ['MSSubClass',\n 'MSZoning',\n 'Street',\n 'Alley',\n 'LotShape',\n 'LandContour',\n 'LotConfig',\n 'LandSlope',\n 'Neighborhood',\n 'Condition1',\n 'Condition2',\n 'BldgType',\n 'HouseStyle',\n 'OverallCond',\n 'RoofStyle',\n 'RoofMatl',\n 'Exterior1st',\n 'Exterior2nd',\n 'MasVnrType',\n 'ExterQual',\n 'ExterCond',\n 'Foundation',\n 'BsmtQual',\n 'BsmtCond',\n 'BsmtExposure',\n 'BsmtFinType1',\n 'BsmtFinType2',\n 'Heating',\n 'HeatingQC',\n 'CentralAir',\n 'Electrical',\n 'LowQualFinSF',\n 'BsmtFullBath',\n 'BsmtHalfBath',\n 'HalfBath',\n 'BedroomAbvGr',\n 'KitchenAbvGr',\n 'KitchenQual',\n 'Functional',\n 'FireplaceQu',\n 'GarageType',\n 'GarageFinish',\n 'GarageQual',\n 'GarageCond',\n 'PavedDrive',\n '3SsnPorch',\n 'PoolArea',\n 'PoolQC',\n 'Fence',\n 'MiscFeature',\n 'MiscVal',\n 'MoSold',\n 'YrSold',\n 'SaleType',\n 'SaleCondition'] #list of all the features that will be considered unordered categorical - see data_description.txt","64d88a50":"all_data['MSZoning'] = all_data['MSZoning'].apply(str) #encode mssubclass as categorical","24711648":"sns.distplot(all_data[\"LowQualFinSF\"])\nall_data[\"LowQualFinSF\"][all_data[\"LowQualFinSF\"] > 0] = 1\nall_data[\"LowQualFinSF\"] = all_data[\"LowQualFinSF\"].fillna(0) #almost all of the data is at 0, so encode as categorical varialbe (0 or >0)","ccb4af88":"def cat_pivot(feature):\n    \"\"\"\n    feature: a feature found in all_data\n    \n    returns: a pivot table based on the median value for saleprice for each category\n    \"\"\"\n    \n    ms_saleprice = pd.concat([all_data.ix[train.index][feature], y_train], axis = 1)\n    return ms_saleprice.pivot_table(\"SalePrice\", feature, aggfunc = \"median\").sort_values(\"SalePrice\")\n\ndef lin_plot(feature):\n    \"\"\"\n    feature: a feature found in all_data\n    \n    returns: a seaborn lmplot to assess the correlation of a variable with sale price\n    \"\"\"\n        \n    lf_saleprice = pd.concat([all_data.ix[train.index][feature], y_train], axis = 1)\n    return sns.lmplot(data = lf_saleprice, x = feature, y = \"SalePrice\") ","64ca8140":"cat_pivot(\"LowQualFinSF\")","8c6e8636":"sns.distplot(all_data[\"3SsnPorch\"])\nall_data[\"3SsnPorch\"][all_data[\"3SsnPorch\"] > 0] = 1\nall_data[\"3SsnPorch\"] = all_data[\"3SsnPorch\"].fillna(0) #almost all of the data is at 0, so encode as categorical varialbe (0 or >0)","4f29f149":"cat_pivot(\"3SsnPorch\")","0885fdf3":"sns.distplot(all_data[\"PoolArea\"])\nall_data[\"PoolArea\"][all_data[\"PoolArea\"] > 0] = 1\nall_data[\"PoolArea\"] = all_data[\"PoolArea\"].fillna(0) #almost all of the data is at 0, so encode as categorical varialbe (0 or >0)","88dfce19":"cat_pivot(\"PoolArea\")","91e3429f":"sns.distplot(all_data[\"MiscVal\"])\nall_data[\"MiscVal\"][all_data[\"MiscVal\"] > 0] = 1 #almost all of the data is at 0, so encode as categorical varialbe (0 or >0)","0e742f91":"cat_pivot(\"MiscVal\")","def2180e":"all_data['MSSubClass'] = all_data['MSSubClass'].apply(str) #encode mssubclass as categorical\n\ncat_pivot(\"MSSubClass\") #demonstrating that this is a categorical variable - ordered by saleprice","8ce6132e":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nlin_plot(\"TotalSF\") #create new feature - the total surface area of all floors, is well correlated with saleprice","d9485c78":"#check for skew in numerical or ordered categorical variables\nskewed_features = all_data[numerical_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_features})\nskewness","429e2415":"lin_plot(\"LotArea\") #lot area before scaling","25fbedc1":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features]) #can also log transform        ","fc5d8faa":"lin_plot(\"LotArea\") #lot area post scaling","6bd76db8":"#can make plots of each variable\n\n#for feature in numerical_features:\n#    lin_plot(feature)\n\n#for feature in unordered_cat_features:\n#    cat_pivot(feature)","0b64c83b":"from sklearn.preprocessing import OneHotEncoder\n\nall_numerical_features = all_data[numerical_features]\n\nunordCatOHE = OneHotEncoder(sparse = False) #use onehotencoder to deal with categorical variables - drop = \"first\" not available in this kernel\nall_categorical_features = pd.DataFrame(unordCatOHE.fit_transform(all_data[unordered_cat_features]), index = all_data.index)\n\nall_data_features = pd.concat([all_numerical_features,\n                               all_categorical_features],\n                               axis = 1) #create a new dataframe only containing the useful features\n\nX_train = all_data_features.ix[train.index]\nX_test = all_data_features.ix[test.index] #create new train and test datasets for use in modelling below","590ba1b2":"def preds_to_output(preds):\n    \"\"\"\n    preds: predictions of saleprice for the test data\n    \n    returns: nothing - writes submission.csv in the same format as sample_submission.csv\n    \"\"\"\n    output = pd.DataFrame({\n        \"Id\": X_test.index,\n        \"SalePrice\": preds\n    })\n\n    output.to_csv(\"submission.csv\", index = False)\n    \n    print(\"CSV written successfully\")","1630b32a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\n\nrtree = RandomForestRegressor()\nfitted_model = rtree.fit(X_train, y_train)\npreds = np.expm1(rtree.predict(X_test))\n\ndef cross_val(model):\n    return -np.mean(cross_val_score(model, X_train, y_train, cv = 5, scoring = \"neg_mean_absolute_error\"))\n    \ncross_val(rtree) #demonstrating cross validation method with randomforestregressor","86afeeaf":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(n_estimators = 3000, #100-1000\n    learning_rate = 0.005, #increase while decreasing n_trees\n    max_depth = 5, #increase incrementally by 1; default 6, increasing can lead to overfit\n    colsample_bytree = 0.3, # 0.3 to 0.8\n    gamma = 0) #0, 1 or 5\n\nmodel.fit(X_train, y_train)\ncross_val(model)\nxgb_preds = np.expm1(model.predict(X_test)) #store the predictions for xgbregressor\npreds_to_output(xgb_preds)","92779b95":"cross_val(model)","68d6d1df":"importances = pd.DataFrame({'Variable':X_test.columns,\n              'Importance':model.feature_importances_}).sort_values('Importance', ascending=False)\n\nplt.subplots(figsize=(15, 15))\nplt.xticks(rotation = \"90\")\nsns.barplot(data = importances[importances[\"Importance\"] > 0.004], x = \"Variable\", y = \"Importance\") #plot the importance of variables according to the xgbregressor model","6f455202":"from sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import RobustScaler\n\nrobScale = RobustScaler()\nrobScaleXtrain = robScale.fit_transform(X_train)\nrobScaleXtest = robScale.transform(X_test)\n\nmodel=Lasso(alpha = 0.00001, random_state=1)\nmodel.fit(robScaleXtrain, y_train)\n\nlasso_preds = np.expm1(model.predict(robScaleXtest)) #store the predictions from the lasso model\npreds_to_output(lasso_preds)","598a6bff":"cross_val(model)","dc487980":"preds_to_output(((lasso_preds + xgb_preds) \/ 2)) #take an average of the xgbregressor and lasso models for final submission (~0.)","015ea8f0":"Attempt at Kaggle's house prices competition (https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview) using XGBoost and lasso models; implemented via jupyter notebooks.\n\nTodo:\n\n1. Work on stacked regression - https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n2. Further visualisation and feature engineering"}}