{"cell_type":{"b5852529":"code","2647da6d":"code","62ae8766":"code","33fb9be2":"code","fa7cac47":"code","c7ec03fd":"code","b76c3259":"code","69db3165":"code","166aef77":"code","f4f4f9f0":"code","8c9aee74":"code","f81facec":"code","588c5ecf":"code","bcfcf91b":"code","291c92df":"code","3763802b":"code","6118e0f6":"code","2b5f0f6f":"code","0756c0bc":"code","1aa48d03":"code","2878157b":"code","3534e661":"code","dde62ad0":"code","4347fa4a":"code","c379695d":"code","3ac5e8cf":"code","030d92a8":"code","9f99f6ee":"code","2b2c8a7a":"code","ac434b2f":"markdown","7fd9da63":"markdown","3efb95e6":"markdown","b3949032":"markdown","139ed166":"markdown","e6517e47":"markdown","c8ac75b0":"markdown","b67857be":"markdown","050abadc":"markdown","fd6101a9":"markdown"},"source":{"b5852529":"import pandas as pd","2647da6d":"DATA_PATH = '\/kaggle\/input\/lish-moa\/'","62ae8766":"train_features = pd.read_csv(DATA_PATH + 'train_features.csv', index_col=False)\ntrain_targets = pd.read_csv(DATA_PATH + 'train_targets_scored.csv', index_col=False)\ntrain_targets_ns = pd.read_csv(DATA_PATH + 'train_targets_nonscored.csv', index_col=False)\ntest_features = pd.read_csv(DATA_PATH + 'test_features.csv', index_col=False)\n\nsubmission = pd.read_csv(DATA_PATH + 'sample_submission.csv', index_col=False)","33fb9be2":"# Reset values in sample submission\nsubmission.loc[:, submission.columns != 'sig_id'] = 0","fa7cac47":"import tensorflow as tf\nfrom sklearn.metrics import log_loss","c7ec03fd":"device_name = tf.test.gpu_device_name()\n\nif \"GPU\" not in device_name:\n    print(\"GPU device not found, using CPU\")\nelse:\n    print('Found GPU at: {}'.format(device_name))","b76c3259":"# Make a copy and start working on it\nstd_train_features = train_features.copy()\nstd_test_features = test_features.copy()","69db3165":"top_feats = [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n             30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52,\n             53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,\n             75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97,\n             98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n             116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134,\n             137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155,\n             156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173,\n             174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n             192, 193, 194, 195, 196, 198, 199, 200, 201, 203, 204, 205, 206, 207, 209, 210, 211, 212,\n             213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231,\n             232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250,\n             251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268,\n             269, 270, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287,\n             288, 289, 290, 291, 292, 293, 295, 296, 297, 299, 301, 302, 303, 304, 305, 306, 307, 308,\n             309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327,\n             328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345,\n             346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n             364, 365, 366, 367, 368, 369, 370, 371, 372, 375, 376, 377, 378, 379, 380, 381, 382, 383,\n             384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n             403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,\n             422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 435, 436, 437, 438, 439, 440,\n             441, 443, 444, 445, 446, 447, 448, 449, 450, 451, 454, 455, 457, 458, 459, 460, 461, 462,\n             463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n             482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 499, 501,\n             502, 503, 504, 506, 507, 508, 510, 511, 512, 513, 514, 515, 516, 519, 520, 521, 522, 523,\n             524, 525, 526, 527, 528, 529, 531, 532, 533, 535, 536, 537, 539, 540, 541, 542, 543, 544,\n             545, 546, 547, 548, 550, 551, 552, 553, 555, 558, 560, 561, 562, 563, 566, 567, 568, 569,\n             570, 571, 572, 573, 574, 575, 576, 578, 579, 581, 582, 583, 584, 585, 586, 587, 588, 589,\n             590, 591, 592, 593, 594, 595, 596, 597, 598, 600, 601, 602, 603, 607, 608, 609, 610, 612,\n             613, 614, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631,\n             632, 633, 634, 635, 636, 637, 638, 639, 640, 642, 643, 644, 645, 646, 647, 648, 649, 650,\n             651, 652, 653, 655, 656, 657, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670,\n             671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n             689, 690, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 705, 706, 708, 709, 710,\n             711, 712, 714, 715, 717, 718, 719, 721, 722, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n             733, 734, 735, 736, 738, 739, 740, 741, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752,\n             753, 754, 755, 756, 757, 758, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771,\n             772, 773, 774, 775, 776, 777, 778, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790,\n             791, 793, 794, 795, 796, 797, 798, 799, 801, 802, 803, 804, 805, 806, 807, 809, 810, 812,\n             814, 815, 816, 817, 818, 819, 820, 822, 823, 824, 826, 827, 828, 829, 830, 831, 832, 833,\n             835, 836, 838, 839, 840, 841, 842, 843, 846, 847, 848, 849, 851, 852, 853, 855, 856, 857,\n             859, 860, 861, 862, 863, 865, 867, 868, 869, 870, 871, 872, 873, 874, 875]\nprint(len(top_feats))","166aef77":"# Note that cp_type is removed later\ndef filter_top_features(std_train_features, std_test_features):\n    top_feat_columns = std_train_features.columns[top_feats]\n    std_train_features = std_train_features.loc[:, std_train_features.columns.isin(top_feat_columns)]\n    std_test_features = std_test_features.loc[:, std_test_features.columns.isin(top_feat_columns)]\n    return std_train_features, std_test_features","f4f4f9f0":"std_train_features, std_test_features = filter_top_features(std_train_features, std_test_features)","8c9aee74":"std_train_features","f81facec":"from sklearn.decomposition import PCA","588c5ecf":"# Get column names that we should preprocess later (before we add features with PCA)\npreprocess_columns = std_train_features.columns.difference(['sig_id', 'cp_type', 'cp_time', 'cp_dose'])","bcfcf91b":"\"\"\"\n# GENES\nGENES = [col for col in std_train_features.columns if col.startswith('g-')]\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(std_train_features[GENES]), pd.DataFrame(std_test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:std_train_features.shape[0]]; test2 = data2[-std_test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\nstd_train_features = pd.concat((std_train_features, train2), axis=1)\nstd_test_features = pd.concat((std_test_features, test2), axis=1)\n\"\"\"\n\n# CELLS\nCELLS = [col for col in std_train_features.columns if col.startswith('c-')]\nn_comp = 15\n\ndata = pd.concat([pd.DataFrame(std_train_features[CELLS]), pd.DataFrame(std_test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:std_train_features.shape[0]]; test2 = data2[-std_test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\nstd_train_features = pd.concat((std_train_features, train2), axis=1)\nstd_test_features = pd.concat((std_test_features, test2), axis=1)","291c92df":"from sklearn import preprocessing\nimport numpy as np","3763802b":"# Create a scaler with sklearn\nscaler = preprocessing.StandardScaler().fit(std_train_features.loc[:, preprocess_columns])\n\n#std_train_features.loc[:, preprocess_columns] = scaler.transform(std_train_features.loc[:, preprocess_columns])\n#std_test_features.loc[:, preprocess_columns] = scaler.transform(std_test_features.loc[:, preprocess_columns])","6118e0f6":"# Transform cp_type column\nstd_train_features.loc[:, 'cp_type'] = std_train_features.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\nstd_test_features.loc[:, 'cp_type'] = std_test_features.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n\n# Transform cp_time column\nstd_train_features.loc[:, 'cp_time'] = std_train_features.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\nstd_test_features.loc[:, 'cp_time'] = std_test_features.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n\n# Transform cp_dose column\nstd_train_features.loc[:, 'cp_dose'] = std_train_features.loc[:, 'cp_dose'].map({'D1': 3, 'D2': 4})\nstd_test_features.loc[:, 'cp_dose'] = std_test_features.loc[:, 'cp_dose'].map({'D1': 3, 'D2': 4})","2b5f0f6f":"import tensorflow_addons as tfa\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nimport tensorflow.keras.backend as K\n\nfrom sklearn.model_selection import StratifiedKFold","0756c0bc":"class ValLossDisplay(tf.keras.callbacks.Callback):\n    def __init__(self, loss_index='val_loss'):\n        self.best_loss = None\n        self.loss_index = loss_index\n        super().__init__()\n    \n    def on_epoch_end(self, epoch, logs=None):\n        if self.best_loss is None or self.best_loss > logs[self.loss_index]:\n            self.best_loss = logs[self.loss_index]\n        print(\"\\r\",'Epoch: %d - Loss: %.6f (best: %.6f)' % (epoch, logs[self.loss_index], self.best_loss), ' '*5, end='')\n    \n    def on_train_end(self, logs=None):\n        print(\"\\r\",'Loss: %.6f' % (self.best_loss), ' '*40)","1aa48d03":"class SGDRScheduler(tf.keras.callbacks.Callback):\n    '''Cosine annealing learning rate scheduler with periodic restarts.\n    # Usage\n        ```python\n            schedule = SGDRScheduler(min_lr=1e-5,\n                                     max_lr=1e-2,\n                                     steps_per_epoch=np.ceil(epoch_size\/batch_size),\n                                     lr_decay=0.9,\n                                     cycle_length=5,\n                                     mult_factor=1.5)\n            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n        ```\n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size\/batch_size)`. \n        lr_decay: Reduce the max_lr after the completion of each cycle.\n                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n        cycle_length: Initial number of epochs in a cycle.\n        mult_factor: Scale epochs_to_restart after each full cycle completion.\n    # References\n        Blog post: jeremyjordan.me\/nn-learning-rate\n        Original paper: http:\/\/arxiv.org\/abs\/1608.03983\n    '''\n    def __init__(self,\n                 min_lr,\n                 max_lr,\n                 steps_per_epoch,\n                 lr_decay=1,\n                 cycle_length=10,\n                 mult_factor=2):\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.lr_decay = lr_decay\n\n        self.batch_since_restart = 0\n        self.next_restart = cycle_length\n\n        self.steps_per_epoch = steps_per_epoch\n\n        self.cycle_length = cycle_length\n        self.mult_factor = mult_factor\n\n        self.history = {}\n\n    def clr(self):\n        '''Calculate the learning rate.'''\n        fraction_to_restart = self.batch_since_restart \/ (self.steps_per_epoch * self.cycle_length)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n        return lr\n\n    def on_train_begin(self, logs={}):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.max_lr)\n\n    def on_batch_end(self, batch, logs={}):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        self.batch_since_restart += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_epoch_end(self, epoch, logs={}):\n        '''Check for end of current cycle, apply restarts when necessary.'''\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n            self.next_restart += self.cycle_length\n            self.max_lr *= self.lr_decay\n            self.best_weights = self.model.get_weights()\n\n    def on_train_end(self, logs={}):\n        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n        self.model.set_weights(self.best_weights)","2878157b":"def build_model(n_features, n_targets):\n    layers = [\n        tf.keras.layers.Input(n_features),\n        tf.keras.layers.Dense(512),\n        tf.keras.layers.PReLU(),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(512),\n        tf.keras.layers.PReLU(),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(n_targets, activation='sigmoid')\n    ]\n\n    optimizer = tfa.optimizers.AdamW(lr=1e-4, weight_decay=1e-5, clipvalue=700)\n    \n    model = tf.keras.Sequential(layers)\n    model.compile(\n        optimizer=optimizer,\n        loss='binary_crossentropy'\n    )\n\n    return model","3534e661":"# For debug purposes, display summary of the model\nn_features = len(top_feats) - 2\nn_targets = len(list(train_targets.columns)) - 1\nmodel = build_model(n_features, n_targets)\nmodel.summary()","dde62ad0":"# Values for training\nEPOCHS = 50\nBATCH_SIZE = 128\nNUM_SPLITS = 7\nEARLY_STOP = 8 # Patience value for early stop\nREDUCE_LR = 3\nRANDOM_SEEDS = range(7)","4347fa4a":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","c379695d":"def train_and_predict(std_train_features, train_targets, std_test_features, submission, oof_df):\n    tf.random.set_seed(42)\n    \n    # Remove the ID column for targets\/labels of model training\n    X_train = std_train_features.loc[:, std_train_features.columns != 'sig_id']\n    y_train = train_targets.loc[:, train_targets.columns != 'sig_id']\n    X_test = std_test_features.loc[:, std_test_features.columns != 'sig_id']\n\n    # Values for model creation\n    n_features = X_train.shape[1]\n    n_targets = y_train.shape[1]\n\n    # Init vars\n    oof_indices = std_train_features['sig_id']\n    history = []\n\n    # Average of several seeds\n    for seed in RANDOM_SEEDS:\n\n        # Apply k-folds\n        kfold = MultilabelStratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=seed)\n    \n        for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n            print('** SEED %d - FOLD %d **' % (seed, fold + 1))\n\n            X_kfold_train, X_kfold_valid = X_train.values[train_index], X_train.values[valid_index]\n            y_kfold_train, y_kfold_valid = y_train.values[train_index], y_train.values[valid_index]\n\n            # Callbacks for early stopping\n            displayer = ValLossDisplay()\n            \n            epoch_size = len(X_train)\n            eas = EarlyStopping(monitor='val_loss', patience=EARLY_STOP, min_delta=1e-5,\n                                verbose=1, mode='min', baseline=None, restore_best_weights=True)\n            lr_sched = SGDRScheduler(min_lr=1e-5,\n                                     max_lr=1e-2,\n                                     steps_per_epoch=np.ceil(epoch_size\/BATCH_SIZE),\n                                     lr_decay=0.85,\n                                     mult_factor=1.5)\n            \n            checkpoint_path = '\/checkpoint_%d_%d' % (seed, fold)\n            m_ckpt = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss',\n                                                        verbose=0, save_best_only=True, save_weights_only=True)\n            eas = EarlyStopping(monitor='val_loss', patience=EARLY_STOP,\n                                verbose=1, mode='min', baseline=None, restore_best_weights=True)\n            callbacks = [displayer, lr_sched, m_ckpt, eas]\n\n            # Create model\n            model = build_model(n_features, n_targets)\n\n            kfold_history = model.fit(\n                X_kfold_train,\n                y_kfold_train,\n                epochs=EPOCHS,\n                batch_size=BATCH_SIZE,\n                validation_data=(X_kfold_valid, y_kfold_valid),\n                verbose=0,\n                callbacks=callbacks\n            )\n\n            history.append(min(kfold_history.history['val_loss']))\n\n            # Restore best weights\n            model.load_weights(checkpoint_path)\n\n            # Stores validation data prediction at respective indices\n            oof_df.loc[oof_df['sig_id'].isin(oof_indices[valid_index]), oof_df.columns != 'sig_id'] += model.predict(X_kfold_valid)\n\n            # adds up test prediction per fold\n            kfold_pred = model.predict(X_test.values)\n            submission.loc[submission['sig_id'].isin(std_test_features['sig_id']), submission.columns != 'sig_id'] += kfold_pred\n    \n    overall_loss = sum(history) \/ len(history)\n    print('Overall Val. Loss: %.6f' % overall_loss)\n    \n    submission.loc[submission['sig_id'].isin(std_test_features['sig_id']), submission.columns != 'sig_id'] \/= (NUM_SPLITS * len(RANDOM_SEEDS))\n    oof_df.loc[:, oof_df.columns != 'sig_id'] \/= len(RANDOM_SEEDS)\n    \n    return submission, oof_df","3ac5e8cf":"# Init dataframes\nsubmission.loc[submission['sig_id'].isin(std_test_features['sig_id']), submission.columns != 'sig_id'] = 0\noof_df = train_targets.copy()\noof_df.loc[:, train_targets.columns != 'sig_id'] = 0\n\n# Data to train model when cp_type=trt_cp\ngr0_std_train_features = std_train_features[std_train_features['cp_type'] == 0].reset_index(drop=True)\ngr0_std_test_features = std_test_features[std_test_features['cp_type'] == 0].reset_index(drop=True)\ngr0_train_targets = train_targets.loc[train_targets['sig_id'].isin(gr0_std_train_features['sig_id'])].reset_index(drop=True)\ndel gr0_std_train_features['cp_type']\ndel gr0_std_test_features['cp_type']\n\n# Training and prediction when cp_type=trt_cp\nsubmission, oof_df = train_and_predict(gr0_std_train_features, gr0_train_targets, gr0_std_test_features, submission, oof_df)\n\n# Fix values to zero when cp_type=ctl_vehicle\nsubmission.loc[std_test_features['cp_type']==1, submission.columns != 'sig_id'] = 0\noof_df.loc[std_train_features['cp_type']==1, oof_df.columns != 'sig_id'] = 0","030d92a8":"def logloss_metric(y_true, y_pred):\n    metrics = []\n    for _target in y_true.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","9f99f6ee":"logloss_score = logloss_metric(\n    train_targets.loc[:, train_targets.columns != 'sig_id'],\n    oof_df.loc[:, oof_df.columns != 'sig_id']\n)\n\nprint('OOF Metric: %.6f' % logloss_score)","2b2c8a7a":"submission.to_csv('submission.csv', index=False)","ac434b2f":"### Categorical data","7fd9da63":"## Prepare data","3efb95e6":"# Model","b3949032":"### Top features","139ed166":"# Submit predictions","e6517e47":"### Normalization (not used)","c8ac75b0":"## Define model and callbacks","b67857be":"## Prepare environment","050abadc":"### PCA features","fd6101a9":"## Training"}}