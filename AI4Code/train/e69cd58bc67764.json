{"cell_type":{"81ca21a3":"code","5fafc45e":"code","b24c8c9b":"code","1a72f222":"code","64a09755":"code","4f065e71":"code","25503001":"code","c504196a":"code","b100b4fb":"code","2ec8fe0f":"code","51f4dd3a":"code","f3e8d9eb":"code","1c3d2b23":"code","eece828b":"code","9f8f5d81":"code","fab8edef":"code","a3480aed":"markdown","561f9231":"markdown","bafb389e":"markdown","af1862ec":"markdown","de752aef":"markdown","54c67cea":"markdown"},"source":{"81ca21a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# Any results you write to the current directory are saved as output.","5fafc45e":"# in case you get csv files, you may use pd.read_csv(). Here we are using a popular regression dataset\n# already available in scikit learn.\nfrom sklearn.datasets import load_boston\ndataset = load_boston()\ndf = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ntarget = dataset.target\ndf.head(5)","b24c8c9b":"df.info() # this is mainly to check for any null values in the dataset","1a72f222":"# This is used to randomly split data \nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid = train_test_split(df,target,test_size=0.2,random_state=100)","64a09755":"# its always good to get idea of dimensions of dataset that you are working with.\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_valid.shape)\nprint(Y_valid.shape)\nprint(X_train.head())","4f065e71":"# This besides helping in getting a glimpse of the dataset also helps to detect any outliers for\n# all attributes in one go.\nX_train.describe()","25503001":"# This gives a relation between every pair of columns.\ncorr = X_train.corr()\nsns.heatmap(corr)\nprint(corr)","c504196a":"for column in dataset.feature_names:\n    plt.scatter(X_train[column], Y_train)\n    plt.xlabel(column)\n    plt.ylabel(\"target\")\n    plt.show()","b100b4fb":"# training a model using the training data.\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,Y_train)","2ec8fe0f":"# predicting the results using the trained model.\nY_pred = lin_reg.predict(X_valid)\nerror = mean_squared_error(Y_valid, Y_pred)\nprint(error)","51f4dd3a":"# this model shows that our model produces results(in blue) that have the same pattern as shown by the dataset\nplt.scatter(X_valid['LSTAT'], Y_valid,  color='black')\nplt.scatter(X_valid['LSTAT'], Y_pred, color='blue')\nplt.xlabel('LSTAT')\nplt.ylabel('target')\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","f3e8d9eb":"# dropping unnecessary columns\nprint(X_train.columns)\nX_train_new = X_train.drop(\"CHAS\", axis = 1)\nX_valid_new = X_valid.drop(\"CHAS\", axis = 1)\nprint(X_train_new.columns)","1c3d2b23":"# training the model again on edited training dataset\nlin_reg.fit(X_train_new,Y_train)","eece828b":"# predicting validation results using trained model again.\nY_pred = lin_reg.predict(X_valid_new)\nerror = mean_squared_error(Y_valid, Y_pred)\nprint(error)","9f8f5d81":"# currently using default parameters.\ntree_reg = RandomForestRegressor(n_estimators = 10, random_state = 100)\ntree_reg.fit(X_train, Y_train)","fab8edef":"# similar to linear regression model, get validation results using random forest model\nY_pred = tree_reg.predict(X_valid)\nerror = mean_squared_error(Y_valid, Y_pred)\nprint(error)","a3480aed":"Correlation matrix among the variables is used to get relation between every pair of variables. This is done so that we can edit (add and remove) dimensions of the data that we find can produce better results.","561f9231":"After looking over the data, we clearly observe that some of the variables such as LSTAT show some relation with the target whereas others like CHAS do not show direct relation with the target.","bafb389e":"We see that such easy observations and manipulation of data lead to improvement in error. SImilarly, we can play with the data in several other ways to get even better results.","af1862ec":"Since, we saw from the analysis that 'CHAS' is not closesly related to the target, so removing that variable can possibly help us train a better model.","de752aef":"Now, we will apply various machine learning models using our inferences from the basi data analysis. Also, we will compare results of those ML algorithms on this dataset.","54c67cea":"Finally, we apply a more complex non-linear regression model. This will fit the patterns of the data even better producing better results."}}