{"cell_type":{"941f6398":"code","a2f4123f":"code","e9989578":"code","fe277b01":"code","6f74d3ed":"code","1a8389c8":"code","5c9b15c8":"code","48a422d8":"code","88e7a0b0":"code","1a694df1":"code","8b42afce":"code","51a7bab6":"code","81aab0bf":"markdown"},"source":{"941f6398":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a2f4123f":"from sklearn.metrics import mean_absolute_error, precision_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nprint(\"Imported\")","e9989578":"path = \"..\/input\/heart.csv\"\ndados = pd.read_csv(path)\n\ndados.columns","fe277b01":"#Transformando os dados de chest pain type em novos atributos\nchest_pain_type_attr = pd.get_dummies(dados[\"cp\"], prefix=\"chest_pain_type\")\n\ncolunas = dados.columns.drop('cp')\ndados = dados[colunas]\n\ndados = dados.join(chest_pain_type_attr)\n\ndados.columns\n","6f74d3ed":"#Using \"weka.attributeSelection.InfoGainAttributeEval\" on \"WEKA\", de following attributes are irrelevant to classification\ncolunas = dados.columns.drop(['chol', 'fbs', 'trestbps', 'restecg'])\n\n#dados = dados[colunas]\nprint(dados.columns)","1a8389c8":"thal_attr = pd.get_dummies(dados[\"thal\"], prefix=\"thal\")\n\ncolunas = dados.columns.drop('thal')\ndados = dados[colunas]\n\ndados = dados.join(thal_attr)\n\ndados.columns","5c9b15c8":"y = dados['target']\n\ncolumns = dados.columns.drop('target')\n\nX = dados[columns]\n\nprint(\"Classes: \" + str(y.unique()) )\nprint(\"Columns: \" + str(X.columns) )\n","48a422d8":"#Split Dataset\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=1, train_size=0.66)\n\nprint(\"Dataset Splited\")","88e7a0b0":"model = MLPClassifier(random_state=1, learning_rate_init=0.0003, max_iter=3000, activation='relu',\n                      hidden_layer_sizes=(300))\nprint(model)\nprint(\"\\n --> Model Created <--\")\nmodel.fit(X_train, y_train)\nprint(\"\\n --> MODEL FITTED <--\")\n\n\ny_pred = model.predict(X_val)\nprint(\"\\n --> y PREDICTED <--\")\n\nmae = mean_absolute_error(y_pred, y_val)\nprint(\"MAE: \" + str(mae))\nprint(\"Score: \" + str(precision_score(y_val, y_pred)) )","1a694df1":"model = KNeighborsClassifier(n_neighbors=23, weights='distance', p=1 )\n\nprint(model)\nprint(\"\\n --> Model Created <--\")\nmodel.fit(X_train, y_train)\nprint(\"\\n --> MODEL FITTED <--\")\n\ny_pred = model.predict(X_val)\nprint(\"\\n --> y PREDICTED <--\")\n\nmae = mean_absolute_error(y_pred, y_val)\nprint(\"MAE: \" + str(mae))\nprint(\"Score: \" + str(precision_score(y_val, y_pred)) )","8b42afce":"model = DecisionTreeClassifier(criterion='gini')\n\nprint(model)\nprint(\"\\n --> Model Created <--\")\nmodel.fit(X_train, y_train)\nprint(\"\\n --> MODEL FITTED <--\")\n\ny_pred = model.predict(X_val)\nprint(\"\\n --> y PREDICTED <--\")\n\nmae = mean_absolute_error(y_pred, y_val)\nprint(\"MAE: \" + str(mae))\nprint(\"Score: \" + str(precision_score(y_val, y_pred)) )","51a7bab6":"from sklearn import svm\n\nmodel = svm.SVC(kernel=\"linear\")\n\nprint(model)\nprint(\"\\n --> Model Created <--\")\nmodel.fit(X_train, y_train)\nprint(\"\\n --> MODEL FITTED <--\")\n\ny_pred = model.predict(X_val)\nprint(\"\\n --> y PREDICTED <--\")\n\nmae = mean_absolute_error(y_pred, y_val)\nprint(\"MAE: \" + str(mae))\nprint(\"Score: \" + str(precision_score(y_val, y_pred)) )","81aab0bf":"Fim"}}