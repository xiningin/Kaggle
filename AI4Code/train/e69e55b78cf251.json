{"cell_type":{"7b42c668":"code","69fa332b":"code","18af8261":"code","02782107":"code","17fe2a44":"code","108813b2":"code","30dd5f4c":"code","be867e9c":"code","3d68af2d":"code","18d0d522":"code","3496dc5a":"code","f020967b":"code","9e734273":"code","c884c68c":"code","3255205d":"code","7983ba2b":"code","35d47797":"code","3913e6d4":"code","d23631f6":"code","c0b71686":"code","4da87235":"code","bb603016":"code","fe808150":"code","154cc859":"code","8e0aca97":"code","724137dc":"code","60128782":"code","a9b03da0":"code","43808521":"code","ee429b45":"code","ee6c889f":"code","8614cb9d":"code","51ad6e0c":"code","56eba0f1":"code","dd073b00":"code","c1da827f":"code","94e88a90":"code","6e6cfaab":"code","92be689f":"code","75d214f3":"code","5f528907":"code","7fc85348":"code","ab0ccaed":"code","c37321e3":"code","5fc3b60c":"code","a23f3196":"code","e4b300c7":"code","8e457354":"code","1cfcde67":"code","6fa86f8e":"code","517feff9":"code","3920eaa5":"code","63089210":"code","744c806c":"code","0fd66059":"code","4bf35ed7":"code","56296ba1":"code","a58782c5":"code","173418d8":"code","33af93b6":"code","24fb97a6":"code","6cd98172":"code","eaa6b4a8":"code","4a26e1a6":"code","2857cfd7":"code","bb8e99f3":"code","5ea5cbc4":"code","b62c56db":"code","f0f0cdcc":"code","ca617ae7":"code","ab22f4ca":"code","94ff02cf":"code","71fcf01c":"markdown","118e4904":"markdown","ff12ba26":"markdown","2cbddbd2":"markdown","d7f92671":"markdown","6bd7cd0a":"markdown","e0938aba":"markdown","0a9e9918":"markdown","e1c41a32":"markdown","ebbf1f7c":"markdown","cd2bf3a9":"markdown","e2fd3a49":"markdown","40bb629e":"markdown","00f4e3dc":"markdown","cc7c158d":"markdown","07d87046":"markdown","35876cd4":"markdown","2da0f697":"markdown","0e4d2c92":"markdown","945a1dda":"markdown","8c5298f7":"markdown","1344e538":"markdown","0fe68b59":"markdown","c0825a19":"markdown","78a00abe":"markdown","e6d65518":"markdown","4e6f6c89":"markdown","c6f35a98":"markdown","2a44a21b":"markdown","2c71ba78":"markdown","55da0d83":"markdown","46f6b67c":"markdown","b91f4724":"markdown","145d2f6f":"markdown","f3427bc4":"markdown","34d92f19":"markdown","9cd0b533":"markdown","f871f28a":"markdown","8c067482":"markdown","fd607f7e":"markdown","c823d9ea":"markdown","20a6572a":"markdown","0c596551":"markdown","556974ba":"markdown","fbda42a4":"markdown","96fb6375":"markdown","e90ca720":"markdown","4a78c590":"markdown","42d040ec":"markdown","eb2aaf1c":"markdown","8a5a8350":"markdown","e5614604":"markdown","4fcb1860":"markdown","3f475375":"markdown","d7ce3afa":"markdown","a90621ae":"markdown","cd4af8bf":"markdown","7ffab2d8":"markdown","b28c28c9":"markdown","cfba46f3":"markdown","07ab6a52":"markdown","aba81e2f":"markdown","faea778f":"markdown","7f2c3457":"markdown","bdb658ee":"markdown","aac87f67":"markdown","5dd0a2f9":"markdown","3222e85a":"markdown","839ded27":"markdown","d788b0a5":"markdown","b3206162":"markdown","8ff0357c":"markdown","d9e9e779":"markdown","85ffbc31":"markdown","ee1fbfa9":"markdown","0588af4f":"markdown","1bc11aa4":"markdown","33c47245":"markdown","46fa4dec":"markdown","625f52b1":"markdown","ee229f4f":"markdown"},"source":{"7b42c668":"from pathlib import Path\nimport os \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf \nfrom scipy import stats\n#-------------------------------------------------------------------------\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n#-------------------------------------------------------------------------\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n#-------------------------------------------------------------------------\nfrom sklearn.decomposition import PCA\n#-------------------------------------------------------------------------\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n#-------------------------------------------------------------------------\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score, roc_auc_score\nfrom sklearn.metrics import classification_report","69fa332b":"data_path =Path(r\"..\/input\/heart-disease-uci\/heart.csv\")\ndata = pd.read_csv(data_path)","18af8261":"data.head()","02782107":"data.isnull().sum()","17fe2a44":"all_features = ['cp','restecg', 'slope',\n                'ca', 'thal',\n                'age', 'trestbps',\n                'chol', 'thalach', 'oldpeak']\n\nplt.figure(figsize=(20,15))\n\nfor ind, feat in enumerate(all_features):\n    \n    plt.subplot(4,4,ind+1)\n    sns.boxplot(y=feat,data=data)","108813b2":"data.info()","30dd5f4c":"categorical_featurs = ['sex', 'cp', 'fbs','restecg', 'exang', 'slope', 'ca', 'thal']\n\nnumerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']","be867e9c":"# utilities\n\ndef plot_histogram(column):\n    have_the_disease = data[data['target']==1][column].value_counts()\n    no_disease  = data[data['target']==0][column].value_counts()\n    df = pd.DataFrame([have_the_disease, no_disease])\n    df.index = ['Have the disease','No disease']\n    df.plot(kind='bar',stacked=True, title=str(column))\n    \n    \ndef categorical_data_analysis(feature):\n    \n    print(data[feature].value_counts())\n    print(\"---------------------------------\")\n\n    print(\"{} - have the disease\".format(feature))\n    print(data[data['target']==1][feature].value_counts())\n    print('--------------------------------------')\n\n\n    print(\"{} - do not have the disease\".format(feature))\n    print(data[data['target']==0][feature].value_counts())\n    print('--------------------------------------')\n\n    plot_histogram(feature)\n    \ndef numerical_data_analysis(feature): \n    \n    print(\"- Statistics of the {} feature for the people  have the disease\".format(feature))\n    print(data[data['target']==1][feature].describe())\n    print('-------------------------------------------')\n\n    print(\"- Statistics of the trestbps feature for the people do not have the disease\")\n    print(data[data['target']==0][feature].describe())\n    print('-------------------------------------------')\n\n\n    plt.hist(data[data['target']==1][feature],\n            color='red',\n            edgecolor='black',\n            alpha=0.4,\n            label='{} - have the disease'.format(feature))\n\n    plt.hist(data[data['target']==0][feature],\n            color='blue',\n            edgecolor='black',\n            alpha=0.4,\n            label='{} - dont have the disease'.format(feature))\n\n    plt.grid(True)\n    plt.legend(loc='upper right')\n    ","3d68af2d":"numerical_data_analysis('age')","18d0d522":"categorical_data_analysis('sex')","3496dc5a":"data['cp'].value_counts()\nprint(\"---------------------------------\")\n\nplot_histogram('cp')","f020967b":"numerical_data_analysis('trestbps')","9e734273":"numerical_data_analysis('chol')","c884c68c":"#The outlayer\n\nchol_1 = data[data['target']==1]['chol'] \nchol_1[chol_1 > 500]","3255205d":"categorical_data_analysis('fbs')","7983ba2b":"categorical_data_analysis('restecg')","35d47797":"numerical_data_analysis('thalach')","3913e6d4":"categorical_data_analysis('exang')","d23631f6":" numerical_data_analysis('oldpeak')","c0b71686":"categorical_data_analysis('slope')","4da87235":"categorical_data_analysis('ca')","bb603016":"categorical_data_analysis('thal')","fe808150":"#I will use here Spearman\u2019s ","154cc859":"categorical_featurs = ['sex', 'cp', 'fbs','restecg', 'exang', 'slope', 'ca', 'thal']\n\nnumerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n\nfeatures = ['age', 'cp', 'chol', 'restecg', 'thalach', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n\ncorr = data[features].corr(method ='spearman')\ncorr.style.background_gradient(cmap='coolwarm')","8e0aca97":"data_cleaned = data[(np.abs(stats.zscore(data)) < 3).all(axis=1)]","724137dc":"all_features = ['cp','restecg', 'slope',\n                'ca', 'thal',\n                'age', 'trestbps',\n                'chol', 'thalach', 'oldpeak']\n\nplt.figure(figsize=(20,15))\n\nfor ind, feat in enumerate(all_features):\n    \n    plt.subplot(4,4,ind+1)\n    sns.boxplot(y=feat,data=data_cleaned)","60128782":"ages = set(np.array(data_cleaned['age']))\nprint(ages)","a9b03da0":"bins = [25, 40, 60, 79]\n\n# now we record for each data points which bin it fails into\nwhich_bin = np.digitize(data_cleaned['age'], bins=bins)","43808521":"which_bin","ee429b45":"binned_ages = pd.Series(which_bin)","ee6c889f":"# removing the pld age column\nready_for_preprocessing_data = data_cleaned.drop('age', axis=1)","8614cb9d":"# adding the new age column\nready_for_preprocessing_data = ready_for_preprocessing_data.assign(age=binned_ages.values)","51ad6e0c":"categorical_featurs = ['sex', 'cp', 'fbs','restecg', 'exang', 'slope', 'ca', 'thal', 'age']\nnumerical_features = ['trestbps', 'chol', 'thalach', 'oldpeak']","56eba0f1":"X = ready_for_preprocessing_data.drop('target', axis=1)\ny = ready_for_preprocessing_data['target']","dd073b00":"# pipeline for numerical data as discribed before\nnumerica_pipline = Pipeline([\n   (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n   (\"norm\", MinMaxScaler()),\n   (\"selection\", RFE(LogisticRegression(max_iter=10000), n_features_to_select=4))\n])\n\n\npreprocessing_pipeline = ColumnTransformer([\n    ('num', numerica_pipline, numerical_features),\n    ('cat', OneHotEncoder(), categorical_featurs)\n])\n\nX_prepered = preprocessing_pipeline.fit_transform(X, y)","c1da827f":"X_prepered.shape","94e88a90":"pca_3 = PCA(n_components=3)\npca_3.fit(X_prepered)\nX_pca_3 = pca_3.transform(X_prepered)","6e6cfaab":"pca_df_3 = pd.DataFrame(X_pca_3, columns={'feature1', 'feature2', 'feature3'})\npca_df_3 = pca_df_3.assign(target=y.values)","92be689f":"class1_features = pca_df_3[pca_df_3['target']==1]\nclass0_features = pca_df_3[pca_df_3['target']==0]","75d214f3":"x_class1 = class1_features['feature1']\ny_class1 = class1_features['feature2']\nz_class1 = class1_features['feature2']\n\nx_class0 = class0_features['feature1']\ny_class0 = class0_features['feature2']\nz_class0 = class0_features['feature2']\n\n\nfig = plt.figure(figsize=(30, 30))\n\n\nax = fig.add_subplot(231, projection = '3d')\nax.scatter(x_class1, y_class1, z_class1, 'or', s=150, label='Class_1')\nax.scatter(x_class0, y_class0, z_class0, 'ob', s=150, label='Class_0')\nax.view_init(0, 90)\nplt.legend()\n\nax = fig.add_subplot(232, projection = '3d')\nax.scatter(x_class1, y_class1, z_class1, 'or', s=150, label='Class_1')\nax.scatter(x_class0, y_class0, z_class0, 'ob', s=150, label='Class_0')\nax.view_init(0, 180)\nplt.legend()\n\nax = fig.add_subplot(233, projection = '3d')\nax.scatter(x_class1, y_class1, z_class1, 'or', s=150, label='Class_1')\nax.scatter(x_class0, y_class0, z_class0, 'ob', s=150, label='Class_0')\nax.view_init(270, 270)\nplt.legend()","5f528907":"pca_2 = PCA(n_components=2)\npca_2.fit(X_prepered)\nX_pca_2 = pca_2.transform(X_prepered)\n\npca_df_2 = pd.DataFrame(X_pca_2, columns={'feature1', 'feature2'})\npca_df_2 = pca_df_2.assign(target=y.values)\n\nclass1_features_2 = pca_df_2[pca_df_3['target']==1]\nclass0_features_2 = pca_df_2[pca_df_3['target']==0]\n\nplt.plot(class1_features_2['feature1'], class1_features_2['feature2'], 'or', label='Class_1')\nplt.plot(class0_features_2['feature1'], class0_features_2['feature2'], 'ob', label='Class_0')\n\nplt.grid(True)\nplt.legend()","7fc85348":"x_train, x_test, y_train, y_test = train_test_split(X_prepered,\n                                                    y,\n                                                    test_size=0.15,\n                                                    shuffle=True,\n                                                    random_state=0)","ab0ccaed":"lr_model = LogisticRegression(max_iter=1000, random_state=0)\nlr_model.fit(x_train , y_train)\nlr_train_score = lr_model.score(x_train , y_train)\n\nprint(\"Logistic Regression Training Score: {:.3F}\".format(lr_train_score))\n\n\n# LogisticRegression evaluated using shuffle-split cross-validation \nlr_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.15, n_splits=5, random_state=0)\nlr_val_scores = cross_val_score(lr_model, x_train , y_train, cv=lr_shuffle_split)\nprint(\"Logistic Regression Cross validation Score: {:.3F}\".format(np.mean(lr_val_scores)))","c37321e3":"# overfitting\n\ndt_model = DecisionTreeClassifier(random_state=0)\ndt_model.fit(x_train , y_train)\ndt_train_score = dt_model.score(x_train , y_train)\n\nprint(\"DecisionTree Classifier Training Score: {:.3F}\".format(dt_train_score))\n\n\n# DecisionTreeClassifier evaluated using shuffle-split cross-validation \ndt_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=5, random_state=0)\ndt_val_scores = cross_val_score(dt_model, x_train , y_train, cv=dt_shuffle_split)\nprint(\"DecisionTree Classifier Cross validation Score: {:.3F}\".format(np.mean(dt_val_scores)))","5fc3b60c":"rf_model = RandomForestClassifier(n_estimators=100,\n                                 max_leaf_nodes=10,\n                                 bootstrap=False,\n                                 max_samples=100,\n                                 n_jobs=-1,\n                                 random_state=0)\n\nrf_model.fit(x_train , y_train)\nrf_train_score = rf_model.score(x_train , y_train)\n\nprint(\"Random Forest Training Score: {:.3F}\".format(rf_train_score))\n\n\n# RandomForestClassifier evaluated using shuffle-split cross-validation \nrf_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=5, random_state=0)\nrf_val_scores = cross_val_score(rf_model, x_train , y_train, cv=rf_shuffle_split)\nprint(\"Random Forest Cross validation Score: {:.3F}\".format(np.mean(rf_val_scores)))","a23f3196":"# Hard Voting \nv_log_clf = LogisticRegression(max_iter=1000, random_state=0)\nv_random_forest = RandomForestClassifier()\nv_dt = DecisionTreeClassifier(random_state=0)\n\n\nhard_voting_model = VotingClassifier(estimators=[(\"lr\",v_log_clf),\n                                            (\"rf\",v_random_forest),\n                                            (\"DT\",v_dt)],\n                               voting='hard')\n\nhard_voting_model.fit(x_train , y_train)\nhard_voting_train_score = hard_voting_model.score(x_train , y_train)\n\nprint(\"Hard Voting Classifiers Training Score: {:.3F}\".format(hard_voting_train_score))\n\n\n# VotingClassifier evaluated using shuffle-split cross-validation \nhard_voting_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=5, random_state=0)\nhard_voting_val_scores = cross_val_score(hard_voting_model, x_train , y_train, cv=hard_voting_shuffle_split)\nprint(\"Hard Voting Classifiers Cross validation Score: {:.3F}\".format(np.mean(hard_voting_val_scores)))","e4b300c7":"# Soft Voting\nv_log_clf = LogisticRegression(max_iter=1000, random_state=0)\nv_random_forest = RandomForestClassifier()\nv_dt = DecisionTreeClassifier(random_state=0)\n\n\nsoft_voting_model = VotingClassifier(estimators=[(\"lr\",v_log_clf),\n                                            (\"rf\",v_random_forest),\n                                            (\"DT\",v_dt)],\n                               voting='soft')\n\nsoft_voting_model.fit(x_train , y_train)\nsoft_voting_train_score = soft_voting_model.score(x_train , y_train)\n\nprint(\"Soft Voting Classifiers Training Score: {:.3F}\".format(soft_voting_train_score))\n\n\n# VotingClassifier evaluated using shuffle-split cross-validation \nsoft_voting_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=5, random_state=0)\nsoft_voting_val_scores = cross_val_score(soft_voting_model, x_train , y_train, cv=soft_voting_shuffle_split)\nprint(\"Soft Voting Classifiers Cross validation Score: {:.3F}\".format(np.mean(soft_voting_val_scores)))","8e457354":"rf_bagging_model = BaggingClassifier(base_estimator=RandomForestClassifier(),\n                                     n_estimators=100,\n                                     bootstrap=True, #The difference between baggong and pasting\n                                     max_samples=100,\n                                     n_jobs=-1,\n                                     random_state=0)\n\nrf_bagging_model.fit(x_train , y_train)\nrf_bagging_train_score = rf_bagging_model.score(x_train , y_train)\n\nprint(\"RandomForest Bagging Classifier Training Score: {:.3F}\".format(rf_bagging_train_score))\n\n\n# BaggingClassifier evaluated using shuffle-split cross-validation \nrf_bagging_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=5, random_state=0)\nrf_baggign_val_scores = cross_val_score(rf_bagging_model, x_train , y_train, cv=rf_bagging_shuffle_split)\nprint(\"RandomForest Bagging Classifier Cross validation Score: {:.3F}\".format(np.mean(rf_baggign_val_scores)))","1cfcde67":"rf_pasting_model = BaggingClassifier(base_estimator=RandomForestClassifier(max_depth=8,\n                                                                             max_leaf_nodes=10,\n                                                                             n_estimators=100,\n                                                                             n_jobs=-1,\n                                                                             random_state=0),\n                                     n_estimators=100,\n                                     bootstrap=False, #The difference between baggong and pasting\n                                     max_samples=100,\n                                     n_jobs=-1,\n                                     random_state=0)\n\nrf_pasting_model.fit(x_train , y_train)\nrf_pasting_train_score = rf_pasting_model.score(x_train , y_train)\n\nprint(\"RandomForest pasting Classifier Training Score: {:.3F}\".format(rf_pasting_train_score))\n\n\n# BaggingClassifier evaluated using shuffle-split cross-validation \nrf_pasting_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=5, random_state=0)\nrf_pasting_val_scores = cross_val_score(rf_pasting_model, x_train , y_train, cv=rf_pasting_shuffle_split)\nprint(\"RandomForest pasting Classifier Cross validation Score: {:.3F}\".format(np.mean(rf_pasting_val_scores)))","6fa86f8e":"rf_adaboost_model = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=5,\n                                                                             max_leaf_nodes=10,\n                                                                             n_estimators=100,\n                                                                             n_jobs=-1,\n                                                                             random_state=0),\n                                       \n                                       n_estimators=100,\n                                       learning_rate=0.1,\n                                       algorithm='SAMME.R',\n                                       random_state=0)\n\nrf_adaboost_model.fit(x_train , y_train)\nrf_adaboost_train_score = rf_adaboost_model.score(x_train , y_train)\n\nprint(\"RandomForestClassifier AdaBoost Classifier Training Score: {:.3F}\".format(rf_adaboost_train_score))\n\n\n# AdaBoostClassifier evaluated using shuffle-split cross-validation \nrf_adaboost_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=5, random_state=0)\nrf_adaboost_val_scores = cross_val_score(rf_adaboost_model, x_train , y_train, cv=rf_adaboost_shuffle_split)\nprint(\"RandomForestClassifier AdaBoost Classifier Cross validation Score: {:.3F}\".format(np.mean(rf_adaboost_val_scores)))","517feff9":"g_boosting_model = GradientBoostingClassifier(n_estimators=500,\n                                             learning_rate=0.01,\n                                             random_state=0,\n                                             max_depth=5)\n\ng_boosting_model.fit(x_train , y_train)\ng_boosting_train_score = g_boosting_model.score(x_train , y_train)\n\nprint(\"Gradient Boosting Classifier Training Score: {:.3F}\".format(g_boosting_train_score))\n\n\n# GradientBoostingClassifier evaluated using shuffle-split cross-validation \ng_boosting_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.2, n_splits=5, random_state=0)\ng_boosting_val_scores = cross_val_score(g_boosting_model, x_train , y_train, cv=g_boosting_shuffle_split)\nprint(\"Gradient Boosting Classifier Cross validation Score: {:.3F}\".format(np.mean(g_boosting_val_scores)))\n\n# overfitting","3920eaa5":"inputs = tf.keras.layers.Input(shape=(30,))\n\nx = tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal',\n                               kernel_regularizer=tf.keras.regularizers.L2())(inputs)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.4)(x)\n\nx = tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer='he_normal',\n                               kernel_regularizer=tf.keras.regularizers.L2())(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.4)(x)\n\n\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nnn_model = tf.keras.Model(inputs, output)","63089210":"nn_model.compile(optimizer='rmsprop',\n                 loss='categorical_crossentropy',\n                 metrics=[\"acc\"])","744c806c":"tf.keras.utils.plot_model(nn_model, show_shapes=True, rankdir=\"LR\")","0fd66059":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor='acc',\n                                                  patience=10,\n                                                  restore_best_weights=True)","4bf35ed7":"history = nn_model.fit(x_train, y_train, epochs=400,\n                       batch_size=64,\n                       callbacks=[early_stopping])","56296ba1":"nn_train_acc = nn_model.evaluate(x_train, y_train)[1]\nnn_test_acc = nn_model.evaluate(x_test, y_test)[1]","a58782c5":"train_scores = [lr_train_score, dt_train_score,\n                rf_train_score, hard_voting_train_score,\n                soft_voting_train_score, \n                rf_bagging_train_score, rf_pasting_train_score,\n                rf_adaboost_train_score, g_boosting_train_score]\n\n\ncross_val_scores = [lr_val_scores, dt_val_scores,\n                    rf_val_scores, hard_voting_val_scores,\n                    soft_voting_val_scores, rf_baggign_val_scores,\n                    rf_pasting_val_scores, rf_adaboost_val_scores,\n                    g_boosting_val_scores]","173418d8":"mean_cross_val_scores = []\n\nfor i in cross_val_scores: \n    mean_cross_val_scores.append(np.mean(i))\n","33af93b6":"models = [lr_model, dt_model, rf_model,\n          hard_voting_model, soft_voting_model,\n          rf_bagging_model, rf_pasting_model,\n          rf_adaboost_model, g_boosting_model]\n\n\ntest_scores = []\n\nfor model in models:\n    test_score = model.score(x_test, y_test)\n    test_scores.append(test_score)","24fb97a6":"models = pd.DataFrame({\n    'Model': [ 'Logistic Regression','Decision Tree Classifier',\n               'Random Forest',\n               'Hard Voting Classifiers', 'Soft Voting Classifiers',\n               'Bagging Classifier using RandomForestClassifier',\n               'Pasting Classifier using RandomForestClassifier',\n               'AdaBoost Classifier using RandomForestClassifier',\n               'Gradient Boosting Classifier'],\n    \n    \n    'Training Accuracy': train_scores,\n    \n    \n    'Cross Validation Accuracy': mean_cross_val_scores,\n\n    'Testset Accuracy': test_scores})\n\n","6cd98172":"models.loc[len(models.index)] = ['Neural Network', nn_train_acc, '-', nn_test_acc]\n\nmodels","eaa6b4a8":"pca = PCA(n_components=10)\npca.fit(x_train)\nx_train_pca = pca.transform(x_train)\nx_test_pca = pca.transform(x_test)","4a26e1a6":"pca_lr_model = LogisticRegression(max_iter=1000, random_state=0)\npca_lr_model.fit(x_train_pca , y_train)\nPca_lr_train_score = pca_lr_model.score(x_train_pca , y_train)\n\nprint(\"Logistic Regression Training Score: {:.3F}\".format(Pca_lr_train_score))\n\n\n# LogisticRegression evaluated using shuffle-split cross-validation \npca_lr_shuffle_split = StratifiedShuffleSplit(train_size=0.8, test_size=0.15, n_splits=5, random_state=0)\npca_lr_val_scores = cross_val_score(pca_lr_model, x_train_pca , y_train, cv=pca_lr_shuffle_split)\nprint(\"Logistic Regression Cross validation Score: {:.3F}\".format(np.mean(pca_lr_val_scores)))","2857cfd7":"pca_test_score = pca_lr_model.score(x_test_pca, y_test)\npca_test_score","bb8e99f3":"cf_matrix = confusion_matrix(y_test, lr_model.predict(x_test))\nsns.heatmap(cf_matrix, annot=True)","5ea5cbc4":"print(\"The number of True Positive is: {}\".format(cf_matrix[0, 0]))\nprint(\"The number of True Negative is: {}\".format(cf_matrix[1, 1]))\nprint(\"The number of False Positive is: {}\".format(cf_matrix[0, 1]))\nprint(\"The number of False Negative is: {}\".format(cf_matrix[1, 0]))","b62c56db":"Logistic_Regression_Precision_score = precision_score(y_test,\n                                                      lr_model.predict(x_test))\n\nprint(\" Logistic Regression Precision score: {}\".format(Logistic_Regression_Precision_score))","f0f0cdcc":"Logistic_Regression_recall_score = recall_score(y_test,\n                                                lr_model.predict(x_test))\n\nprint(\" Logistic Regression Recall score: {}\".format(Logistic_Regression_recall_score))","ca617ae7":"Logistic_Regression_f1_score = f1_score(y_test,\n                                        lr_model.predict(x_test))\n\nprint(\" Logistic Regression F1 score: {}\".format(Logistic_Regression_f1_score))","ab22f4ca":"Logistic_Regression_Average_Precision_score = average_precision_score(y_test,\n                                                                      lr_model.predict_proba(x_test)[:,1])\n\nprint(\"Logistic Regression Average Precision score: {}\".format(Logistic_Regression_Average_Precision_score))","94ff02cf":"Logistic_Regression_ROC_AUC_score = roc_auc_score(y_test,\n                                                  lr_model.predict_proba(x_test)[:,1])\n\nprint(\"Logistic Regression ROC AUC Score: {}\".format(Logistic_Regression_ROC_AUC_score))","71fcf01c":"**Welcome everyone**\n\nIn this notebook I will do some analysis to understand the data and get a good idea about the features\nthat I will use to train a model that predict the Heart Disease. \n\n**I will pretend to be machine learning model**\n\nFirst of all I'm not a doctor, my major is industrial engineering and I have no idea about these features that \nI'm going to use so I will pretend to be machine learning model thak looks to the data as numbers \nfind a good representations that will help it learn and trying to find a mapping between the the x and y. \n\nalso I will try to understand the Heart Disease from the data as a ML model, let's see if I can :) also I will try \nto measure my perfomance on the test set it will be interesting thing to try because my background is industrial\nengineering not medicine. \n\nLet us start with some analysis to understand the data. ","118e4904":"## 5.2- (Trying) to train the model after applying PCA ","ff12ba26":"As we can see the people have the disease tend to have a lower serum cholesterol in mg\/dl\nthe figure may confuse you but the x axis is chol and the y axis count the number of people with that chol. \nalso we notice an outlier in the people have the disease that will negatively affect the training so \nin the data preperation section I will get red of that. ","2cbddbd2":"As we can see from  the table above the models achieve good testing result. The highest testset Accuracy \nwas for the Logistic Regression and Hard Voting Classifiers I think that because the data is very simple and\nsmall to use a neural network. the second best scores were for Random Forest, Bagging, Pasting, and Neural Network. \n\nIn the next step I will choose one or two of these models to further investigation and analysis. \n\nI will choose the Logistic Regression and the Random Forest\t","d7f92671":"The following Machine Learning algorithms will be tested: \n    \n    1- Logistic Regression\n    2- DecisionTree Classifier\n    3- Ensemble Methods:\n        a- Random Forest\n        b- Voting Classifiers\n        c- bagging and Pasting\n        d- Boosting","6bd7cd0a":"## 3.2- DecisionTree Classifier","e0938aba":"The data will be splitted to train and test sets and the test set will be used for the final model testing.","0a9e9918":"## 2.2- Feature engineering ","e1c41a32":"## 3.1- Logistic Regression","ebbf1f7c":"#### 3.3.4.1- AdaBoost","cd2bf3a9":"In this section data will be prepared by transfirming the row data we have to useful representation can be \nused by the models to perform the classification. \n\n- I will use **Discretization** for the age feature. <br>\n- **One-hot encoding** for the categorical features. <br>\n- And for the numerizal featurs: **extract polynoial features**, **feature selection** and then **normalization** <br>","e2fd3a49":"# 6- Conclusion","40bb629e":"After reducing the dimensionality of dataset the Cross validation Score was increazed by about 2% but\nthe testset score was decreased by about 2% ","00f4e3dc":"## 5.1- comparison","cc7c158d":"## 1.1- First look","07d87046":"And by now the ages are ready for one_hot encoding ","35876cd4":" I can not identify the types in name, and what the dataset creator is meant by chest pain type. \n\nbut as I said previously I want to **think like a ML model** and identify patterns from row data, so from the above figure \nI can conclude that the type 0 means no pain science the majority of the people have no disease are having the type 0 pain\nand some of the people have the disease do not have does not feel any type of pain. \n\nAnd the most frequent pain type between people have the disease is type 2. ","2da0f697":"As we can see from the figures above the data is not linearly separable. This will help us alot to choose the right \nalgorithms.","0e4d2c92":"### 1.6.2- sex","945a1dda":"### 1.6.13- thal - thal: 3 = normal; 6 = fixed defect; 7 = reversable defect  ","8c5298f7":"## 1.5- utility functions","1344e538":"# 3- Classification models & Kfold cross validation.","0fe68b59":"Blood pressure (BP) is the pressure of circulating blood against the walls of blood vessels.\nMost of this pressure results from the heart pumping blood through the circulatory system.\nWhen used without qualification, the term \"blood pressure\" refers to the pressure in the large arteries.\nBlood pressure is usually expressed in terms of the systolic pressure (maximum pressure during one heartbeat) \nover diastolic pressure (minimum pressure between two heartbeats) in the cardiac cycle. It is measured in millimeters\nof mercury (mmHg) above the surrounding atmospheric pressure.\n\n**Source:** <a href=\"https:\/\/en.wikipedia.org\/wiki\/Blood_pressure\">wikipedia-Blood pressure\/<a>","c0825a19":"unfortunately the number of observations for restecg (class2) is very small it's just 4 observations :(\nthe model will not generalize well in this class, while the numbers in the other two classes is balanced and that's good. ","78a00abe":"Some of the machine learning problems involve a dataset with a very high dimensionality,\nwhich will extremely slowdown the learning process, and make the learning harder for the algorithm this problem \nis called Curse of dimensionality. Dimensionality reduction one of the techniques that is widely used to solve the\nproblem but by reducing the dimensionality, we will lose some of the information that contained the dataset,\nthe benefit will be to speed up the learning process.\n\nPrinciple Component Analysis aims to reducing the dimensionality of the data while preserving the largest amount\nof information (variance) presented in the dataset.\n\nyou can learn more about Principle Component Analysis (PCA) here: <a href=\"https:\/\/www.kaggle.com\/general\/253753\">Principle Component Analysis (PCA) fully Explained!!\/<a>","e6d65518":"### 3.3.4- Boosting","4e6f6c89":"as we can see from  the figure and the discribtive statistics the people have the disease tend to have\na lower resting blood pressure. ","c6f35a98":"#### 3.3.3.1- bagging","2a44a21b":"Features ('cp', 'thal', 'trestbps', 'chol', 'thalach', 'oldpeak') indicate some outliers \nI will deal with that in the data preprocessing section.","2c71ba78":"## 3.3- Ensemble Methods","55da0d83":"**Thank you for reading, I hope you enjoyed and benefited from it.** <br>\n \n**If you have any questions or notes please leave it in the commont section.** <br>\n\n**If you like this notebook please press upvote and thanks again.** <br>","46f6b67c":"Heart Disease UCI dataset was used in this notebook, first we performed an exploratory data analysis to analyze\nand understand the dataset we have. T have started with a visualization of the missing data, then I start to analyze the \nfeatures one by one. \n\n\nThe mean and the standard deviation of the ages for the people have and dont have the disease  are\nso close for both, and from the means and the histogram we can say that the younger people are more susceptible to disease. \nThe number of males in this dataset is more than twice the number of females, and the number of males that have the\ndisease is higher than females that have the disease the number of males that do not have the disease is higher than\nfemales that have the disease. The most frequent pain type between people have the disease is type 2.\n\n\nthe people havethe disease tend to have a lower resting blood pressure and tend to have a lower serum cholesterol in mg\/dl.\nthe number of people that have the disease and fasting blood sugar > 120 mg\/dl (class 1) is almost equal to the number of\npeople that do not have the disease and fasting blood sugar < 120 mg\/dl (class 0).\n\n\nAlso for the people have the disease the most resting electrocardiographic results type was type 1, and they have number of\nmajor vessels 0 (class 0), also they have the (class 2) of the slope of the peak exercise ST segment\nwhile most of the people dont have the disease have the (class 1)\n\n\nThe correlation between the valriables and the target was studied using Spearman\u2019s \u03c1 and most of the variables was related the \ntarget, some them are positively related and other are negatively.\n\n\nThe data was cleaned from the outliers then  prepared for the macine learning models, the age feature was binned, other numerical features passed through  a pipeline that extract new features then select the best of them using the RFE and \nfinnaly normalized. For the categorical data I used the one_hot encoder to encode them. \n\n\nSeveral Machine learning model were tested the highest testset Accuracy was for the Logistic Regression\nand Hard Voting Classifiers I think that because the data is very simple and small to use a neural network.\nthe second best scores were for Random Forest, Bagging, Pasting, and Neural Network.\n\n\nFinally the Logistic regreesion model was chosen for further analysis and the following metrics were computed: precision_score,\nrecall_score, f1_score, average_precsions_score, roc_auc_score and all of them shown a very good results, all of them\nwere higher than 95%. ","b91f4724":"### 2.2.1- Discretization","145d2f6f":"people have the disease have higher mean maximum heart rate achieved of 158.47 also they have a higher min and max \nof (96, 202). \n\nfrom the figure above the distribution of the (thalach - dont have the disease) have a higer deiviation than the \ndistribution of the (thalach - have the disease) where most of the obsevations concentrated around  the mean. \n\nThe two distribution are difeerent in parameters so it will help the calssification model during training. ","f3427bc4":"correlation: is a number between \u20131.0 and +1.0. positive corelation meaning both variables move in the same direction\nif we increase one of them the other one also increase and if we decrease one of them the other one will increase. While negative correlations means that the both variables are moveing in opposite  direction. <br>\n\n- Positive Correlation: both variables change in the same direction.<br>\n- Neutral Correlation: No relationship in the change of the variables.<br>\n- Negative Correlation: variables change in opposite directions.<br>\n    \nIt is important to note that correlation doesn't imply causation. \nCorrelation quantifies the strength of the relationship between the features of a dataset. <br>\n\n\n**Source:** <a href=\"https:\/\/re-thought.com\/exploring-correlation-in-python\/\">Exploring Correlation in Python: Pandas, SciPy\/<a>","34d92f19":"### 3.3.3- bagging and Pasting","9cd0b533":"There are several statistics that you can use to quantify correlation, three of them: <br>\n\n- Pearson\u2019s r. Can be used to summarize the strength of the linear relationship between two data samples.\nTo use Pearson both valriables must be numenrical and normally distributed. Pearson\u2019s correlation coefficient\ncalculated as the covariance of  the two variables divided by the product of the standard deviation of each data sample.<br>\n\n- Spearman\u2019s \u03c1 (rho): does not assume that data is from a specific distribution, also does not require the variables to be normally distributed nor the relationship between them to be linear, and does not  require the variables to be numerical,\nSpearman\u2019s rank correlation tests whether two ordinal or\/and quantitative variables are dependent.<br>\n\nSpearman's correlation coefficient, (\u03c1, also signified by rs) measures the strength and direction of the association between\ntwo ranked variables. this coefficient is between -1 and +1 The higher the magnitude of \u03c1 (in the positive or negative directions),\nthe stronger the relationship.<br>\n\n\n- Kendall\u2019s \u03c4 (tau) <br>\n\nCorrelation can be useful in data analysis and modelling to better understand the relationships between variables. \nSo it is important that you have a good understanding of it before you attempt a data analysis or modelling.<br>\n\n\nfor more information visit: <a href=\"https:\/\/re-thought.com\/exploring-correlation-in-python\/\">Exploring Correlation in Python: Pandas, SciPy\/<a>","f871f28a":"as we can see all the variables are related with the target variable either positively negatively.\n\n\nIf you notice the analysis in the previos section I taked about each valriable if it will have \na large or small contribution in the training process based on charts in the previous sections. \n\n\nIn the previous section I said that the variables (thal, ca, slope, thalach, cp) will have a good contribution\nduring the training process and as we can see they have the highest Spearman's correlation coefficient with the target \nvalue. All these are assumptions we will se if that true during the feature selection and training processes.\n","8c067482":"we now have less outliers as we can see from the boxplots, I will not remove the remained outliers due to the small amount \nof data we have. ","fd607f7e":"### 3.3.2- Voting Classifiers","c823d9ea":"### 1.6.1- Age","20a6572a":"Firs of all the number of people with  fasting blood sugar > 120 mg\/dl (class 1) is very small compared to number of people\nfasting blood sugar < 120 mg\/dl (class 0)\n\nsecond the numbers are so closed to each others, where the number of people that have the disease and fasting blood sugar > 120 mg\/dl \n(class 1) is almost equal to the number of people that do not have the disease and fasting blood sugar < 120 mg\/dl \n(class 0). \n\n\nSo I think that this feature will be useless to the training model, this feature will not help the model \nto classify a person to target 1 or 0. \n\nI will use the Feature Selection functions in sklearn to see if my hypothesis is right or not. ","0c596551":"Here we will partition the input range into fixed number of bins, then a data point will be represented\nby which bin it fails into.\n\nAs we can see the ages is between 29 and 77 I will create three bins the first one include the ages between 20 and 39, \nthe second one contain ages between 40 and 59, the third one contain ages between 60 and 79. ","556974ba":"For trying and to get more insights I will reduce the feature space for the data and train the model again to see\nif that will have any impact on the accuracy. ","fbda42a4":"## 2.3- visualization after compressing the data using PCA","96fb6375":"You can read more about serum cholesterol at <a href=\"https:\/\/www.healthline.com\/health\/serum-cholesterol\">healthline- What Is Serum Cholesterol and Why Is It Important?\/<a>\n","e90ca720":"# 5- Final Analysis and comparison.","4a78c590":"### 1.6.8- thalach - maximum heart rate achieved ","42d040ec":"### 2.2.2- Preprocessing input pipeline ","eb2aaf1c":"The last 5 features I will talk about them briefly because they need an expert in medical sciences.\n\n\nIn general for the last 3 categorical features the number of observations is imabalnced, such as \n(thal) feature where there are just 4 observations of the (class 0) we can remove them from the dataset\nbefore training because they will affect the training negatively. And for the number of major vessels (ca) feature, most of the people have the disease have number of major vessels 0 (class 0)\nthis will be helpful during the training process. \n\n\nFor the (the slope of the peak exercise ST segment) feature most of the people that have the disease have the (class 2) of this feature \nwhile most of the people dont have the disease have the (class 1) of this feature. This also will be helpful during training. ","8a5a8350":"## 1.4- data types","e5614604":"# 1- Understand the data ","4fcb1860":"### 1.6.9- exang - exercise induced angina  ","3f475375":"## 5.3- Further analysis","d7ce3afa":"### 1.6.3- cp - chest pain type","a90621ae":"The data does not contain any missing values so in this section I'll get rid of outliers","cd4af8bf":"I will Neural network that consists of 3 layers with different number of neurons, the atication fumction will be relu \nand for the final layer will be sigmoid because we have 2 classes, BatchNormalization layers and Dropout layers after \nthe Dense layers. ","7ffab2d8":"### 1.6.12- ca - number of major vessels (0-3) colored by flourosopy ","b28c28c9":"## 1.7- correlations","cfba46f3":"# 7- Thank you ","07ab6a52":"# 4- Deep learning  classification model.","aba81e2f":"## 1.3- Check for missing values","faea778f":"### 1.6.7- restecg - resting electrocardiographic results (values 0,1,2)","7f2c3457":"## 2.1- data cleaning ","bdb658ee":"### 1.6.5- chol - serum cholestoral in mg\/dl   ","aac87f67":"## 1.6- take alook at the features one by one","5dd0a2f9":"As we can see from the statistics of the ages for the people have and dont have the disease\nthat the mean and the standard deviation are so close for both\n\nand from the means and the histogram we can say that the younger people are more susceptible to disease !!!\n\nAnd due to the little difference between them I can say that the ML model will not will not benefit much from \nthis feature so I will use **(binning)** to try to get a useful info for the model. ","3222e85a":"## 1.2- Check for missing values\n\nThere are no missing values in the data so we cak continue ","839ded27":"#### 3.3.3.1- pasting","d788b0a5":"### 1.6.6- fbs - fasting blood sugar > 120 mg\/dl ","b3206162":"### 1.6.4- trestbps - resting blood pressure  ","8ff0357c":"#### 3.3.4.2- GradientBoostingClassifier","d9e9e779":"### 1.6.11- slope - the slope of the peak exercise ST segment ","85ffbc31":"As we can see:\n    \n- The number of males in this dataset is more than twice the number of females, \n- The number of males that have the disease is higher than females that have the disease\n- The number of males that do not have the disease is higher than females that have the disease\n\nSo we may encounter some **bayes problems**. ","ee1fbfa9":"### 1.6.10- oldpeak - oldpeak = ST depression induced by exercise relative to rest  ","0588af4f":"**Contents:**\n    \n1- Understand the data. <br>\n2- Data preperation and visualization using PCA. <br>\n3- Classification models & Kfold cross validation. <br>\n4-Deep learning input pipeline and classification model.<br>\n5- Final Analysis and comparison.<br>\n6- Conclusion.<br>\n7- Thank you.<br>","1bc11aa4":"Features:\n    \n- age <br>\n- sex <br>\n- chest pain type (4 values) <br>\n- resting blood pressure <br>\n- serum cholestoral in mg\/dl <br>\n- fasting blood sugar > 120 mg\/dl <br>\n- resting electrocardiographic results (values 0,1,2) <br>\n- maximum heart rate achieved <br>\n- exercise induced angina <br>\n- oldpeak = ST depression induced by exercise relative to rest <br>\n- the slope of the peak exercise ST segment <br>\n- number of major vessels (0-3) colored by flourosopy <br>\n- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect <br>\n\nThe number of training datais very small so it will be challenge. I will use Kfold cross validation. ","33c47245":"# 2- Data preperation and visualization using PCA.","46fa4dec":"### 3.3.1- Random Forest","625f52b1":"## 2.4- Splitting the data","ee229f4f":"The following code will: \n    \n- For each column, it first computes the Z-score of each value in the column, relative to the column mean and standard deviation.\n\n- It then takes the absolute Z-score because the direction does not matter, only if it is below the threshold.\n\n- all(axis=1) ensures that for each row, all column satisfy the constraint.\n\n- Finally, the result of this condition is used to index the dataframe.\n\n\n**Source:** <a href=\"https:\/\/stackoverflow.com\/questions\/23199796\/detect-and-exclude-outliers-in-pandas-data-frame\">Detect and exclude outliers in Pandas data frame\/<a>"}}