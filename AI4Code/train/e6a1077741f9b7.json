{"cell_type":{"ab8d6816":"code","cb814f41":"code","e4f09239":"code","7f3a9ab9":"code","85ab1c13":"code","e8086aa8":"code","b1ed5d02":"code","18cac60e":"code","b3584afa":"code","c858edc7":"code","45bfe539":"code","708c1fd8":"code","3a0e2835":"code","37454353":"code","5118ec2a":"code","f4d8b51e":"code","5429751a":"code","eb07408c":"code","9186abbd":"code","9f99a431":"code","791f4dab":"code","30babf90":"code","2ebd27d7":"code","1d7450ac":"code","89198357":"code","02267142":"code","5e2cdfea":"code","3c33cc29":"code","f2fb4696":"code","354920da":"code","877efbbb":"code","be1534e6":"code","8e39eb30":"code","328cfe02":"code","a235798f":"code","259ab99d":"code","ca2c8ab6":"code","89c5c246":"code","e685b88d":"code","fbdde7cb":"code","aefb0501":"code","fc060846":"code","d98804df":"code","960aa0fe":"code","9f3c128e":"code","728741a6":"code","3fe5fc5a":"code","e6ca5aea":"code","656d74f9":"code","f51da3b8":"code","d6437af7":"code","d0d6911f":"code","434516cb":"code","4066826d":"code","1163c01e":"code","5afbe45e":"code","6308b31a":"code","fcced69d":"code","ceb120fd":"code","4c1e184f":"markdown","ab151603":"markdown","5f413522":"markdown","fb0fa181":"markdown","26b5d81a":"markdown","f731abb5":"markdown","e19abc20":"markdown","9e3552b7":"markdown","b41e957d":"markdown","992247dd":"markdown","30771b5e":"markdown","6326f401":"markdown"},"source":{"ab8d6816":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nimport time\nimport spacy\nfrom scipy.misc import imread\nfrom sklearn.utils import resample\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot\ntrain_df = pd.read_csv('..\/input\/kidney_disease.csv')\ndf = pd.read_csv('..\/input\/kidney_disease.csv')\nstart_time = time.time()\nend = time.time()\nprint(\"Time taken in reading the input files is {}.\".format(end - start_time))\ntrain_df.head()\nprint(\"Number of rows in train dataset {} \".format(train_df.shape[0]))\n\ntrain_df.describe()\n\n# Data Preprocessing\n\ndf[['htn','dm','cad','pe','ane']] = df[['htn','dm','cad','pe','ane']].replace(to_replace={'yes':1,'no':0})\ndf[['rbc','pc']] = df[['rbc','pc']].replace(to_replace={'abnormal':1,'normal':0})\ndf[['pcc','ba']] = df[['pcc','ba']].replace(to_replace={'present':1,'notpresent':0})\ndf[['appet']] = df[['appet']].replace(to_replace={'good':1,'poor':0,'no':np.nan})\ndf['classification'] = df['classification'].replace(to_replace={'ckd':1.0,'ckd\\t':1.0,'notckd':0.0,'no':0.0})\ndf.rename(columns={'classification':'class'},inplace=True)\ndf['pe'] = df['pe'].replace(to_replace='good',value=0) # Not having pedal edema is good\ndf['appet'] = df['appet'].replace(to_replace='no',value=0)\ndf['cad'] = df['cad'].replace(to_replace='\\tno',value=0)\ndf['dm'] = df['dm'].replace(to_replace={'\\tno':0,'\\tyes':1,' yes':1, '':np.nan})\n#df.drop('id',axis=1,inplace=True)\n\nfrom sklearn.model_selection import train_test_split\nsns.countplot(x = 'age', hue = 'appet', data = df)\nfor i in ['rc','wc','pcv']:\n    df[i] = df[i].str.extract('(\\d+)').astype(float)\nfor i in ['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','rc','wc','pcv']:\n    df[i].fillna(df[i].mean(),inplace=True)\ndf = df.dropna(axis=0)\ndf['class'].value_counts()\ndf.head()\ndf.info()\nvalues = df.values\n#Data Augmentation\nn_iterations = 10000\nn_size = int(len(df) * 0.50)\nprint(n_size)\n# run bootstrap\nstats = list()\nfor i in range(n_iterations):\n# prepare train and test sets\n    train = resample(values, n_samples=n_size)\n    test = np.array([x for x in values if x.tolist() not in train.tolist()])\n# fit model\nmodel = DecisionTreeClassifier()\nmodel.fit(train[:,:-1], train[:,-1])\n# evaluate model\npredictions = model.predict(test[:,:-1])\nscore = accuracy_score(test[:,-1], predictions)\n\n","cb814f41":"plt.figure(figsize=(12,3))\nfeatures = train.columns.values.tolist()\nimportance = clf_best.feature_importances_.tolist()\nfeature_series = pd.Series(data=importance,index=features)\nfeature_series.plot.bar()\nplt.title('Feature Importance')","e4f09239":"#another algorithm compile 2 and 3 below\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nimport time\nimport spacy\nfrom scipy.misc import imread\nfrom sklearn.utils import resample\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot\ntrain_df = pd.read_csv('..\/input\/kidney_disease.csv')\ndf = pd.read_csv('..\/input\/kidney_disease.csv')\nstart_time = time.time()\nend = time.time()\nprint(\"Time taken in reading the input files is {}.\".format(end - start_time))\ntrain_df.head()\nprint(\"Number of rows in train dataset {} \".format(train_df.shape[0]))\n\ntrain_df.describe()\n\n# Data Preprocessing\n\ndf[['htn','dm','cad','pe','ane']] = df[['htn','dm','cad','pe','ane']].replace(to_replace={'yes':1,'no':0})\ndf[['rbc','pc']] = df[['rbc','pc']].replace(to_replace={'abnormal':1,'normal':0})\ndf[['pcc','ba']] = df[['pcc','ba']].replace(to_replace={'present':1,'notpresent':0})\ndf[['appet']] = df[['appet']].replace(to_replace={'good':1,'poor':0,'no':np.nan})\ndf['classification'] = df['classification'].replace(to_replace={'ckd':1.0,'ckd\\t':1.0,'notckd':0.0,'no':0.0})\ndf.rename(columns={'classification':'class'},inplace=True)\ndf['pe'] = df['pe'].replace(to_replace='good',value=0) # Not having pedal edema is good\ndf['appet'] = df['appet'].replace(to_replace='no',value=0)\ndf['cad'] = df['cad'].replace(to_replace='\\tno',value=0)\ndf['dm'] = df['dm'].replace(to_replace={'\\tno':0,'\\tyes':1,' yes':1, '':np.nan})\n#df.drop('id',axis=1,inplace=True)\n\nfrom sklearn.model_selection import train_test_split\nsns.countplot(x = 'age', hue = 'appet', data = df)\nfor i in ['rc','wc','pcv']:\n    df[i] = df[i].str.extract('(\\d+)').astype(float)\nfor i in ['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','rc','wc','pcv']:\n    df[i].fillna(df[i].mean(),inplace=True)\ndf = df.dropna(axis=0)\ndf['class'].value_counts()\ndf.head()\ndf.info()\nvalues = df.values\n","7f3a9ab9":"#Run this next to first for MLP\n#Binary Classifier\n# Import libraries for data wrangling, preprocessing and visualization\nimport numpy \n#import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n# Importing libraries for building the neural network\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n# Select the columns to use for prediction in the neural network\nprediction_var = ['sg','al','su','bgr','sc','pot','pcv','wc','rc','dm']\nX = df[prediction_var].values\nY = df['class'].values\n# Diagnosis values are strings. Changing them into numerical values using LabelEncoder.\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n# Baseline model for the neural network. We choose a hidden layer of 10 neurons. The lesser number of neurons helps to eliminate the redundancies in the data and select the more important features.\ndef create_baseline():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, input_dim=10, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model. We use the the logarithmic loss function, and the Adam gradient optimizer.\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n# Evaluate model using standardized dataset. \nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Results:%.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","85ab1c13":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import learning_curve\ntrain_sizes = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\nprediction_var = ['sg','al','su','bgr','sc','pot','pcv','wc','rc','dm']\nX = df[prediction_var].values\ny = df['class'].values\ntrain_sizes, train_scores, validation_scores = learning_curve(estimator = LinearRegression(),X=X,y=y,train_sizes = train_sizes,cv = 5,scoring = 'neg_mean_squared_error')\nprint('Training scores:\\n\\n', train_scores)\nprint('\\n', '-' * 70) # separator to make the output easy to read\nprint('\\nValidation scores:\\n\\n', validation_scores)\ntrain_scores_mean = -train_scores.mean(axis = 1)\nvalidation_scores_mean = -validation_scores.mean(axis = 1)\nprint('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\nprint('\\n', '-' * 20) # separator\nprint('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn')\nplt.plot(train_sizes, train_scores_mean, label = 'Training error')\nplt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\nplt.ylabel('MSE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\nplt.legend()\nplt.ylim(0.0,1)","e8086aa8":"#Run this next to first for MLP\n#Binary Classifier\n# Import libraries for data wrangling, preprocessing and visualization\nimport numpy \n#import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n# Importing libraries for building the neural network\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n# Read data file\ndf = pd.read_csv('..\/input\/kidney_disease.csv')\nstart_time = time.time()\nend = time.time()\nprint(\"Time taken in reading the input files is {}.\".format(end - start_time))\ntrain_df.head()\nprint(\"Number of rows in train dataset {} \".format(train_df.shape[0]))\n\ntrain_df.describe()\n\n# Data Preprocessing\n\ndf[['htn','dm','cad','pe','ane']] = df[['htn','dm','cad','pe','ane']].replace(to_replace={'yes':1,'no':0})\ndf[['rbc','pc']] = df[['rbc','pc']].replace(to_replace={'abnormal':1,'normal':0})\ndf[['pcc','ba']] = df[['pcc','ba']].replace(to_replace={'present':1,'notpresent':0})\ndf[['appet']] = df[['appet']].replace(to_replace={'good':1,'poor':0,'no':np.nan})\ndf['classification'] = df['classification'].replace(to_replace={'ckd':1.0,'ckd\\t':1.0,'notckd':0.0,'no':0.0})\ndf.rename(columns={'classification':'class'},inplace=True)\ndf['pe'] = df['pe'].replace(to_replace='good',value=0) # Not having pedal edema is good\ndf['appet'] = df['appet'].replace(to_replace='no',value=0)\ndf['cad'] = df['cad'].replace(to_replace='\\tno',value=0)\ndf['dm'] = df['dm'].replace(to_replace={'\\tno':0,'\\tyes':1,' yes':1, '':np.nan})\n#df.drop('id',axis=1,inplace=True)\n\n#from sklearn.model_selection import train_test_split\n#sns.countplot(x = 'age', hue = 'appet', data = df)\nfor i in ['rc','wc','pcv']:\n    df[i] = df[i].str.extract('(\\d+)').astype(float)\nfor i in ['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','rc','wc','pcv']:\n    df[i].fillna(df[i].mean(),inplace=True)\ndf = df.dropna(axis=0)\ndf['class'].value_counts()\ndf.head()\ndf.info()\nvalues = df.values\ndf = pd.read_csv('..\/input\/kidney_disease.csv')\nstart_time = time.time()\nend = time.time()\nprint(\"Time taken in reading the input files is {}.\".format(end - start_time))\ntrain_df.head()\nprint(\"Number of rows in train dataset {} \".format(train_df.shape[0]))\n\ntrain_df.describe()\n\n# Data Preprocessing\n\ndf[['htn','dm','cad','pe','ane']] = df[['htn','dm','cad','pe','ane']].replace(to_replace={'yes':1,'no':0})\ndf[['rbc','pc']] = df[['rbc','pc']].replace(to_replace={'abnormal':1,'normal':0})\ndf[['pcc','ba']] = df[['pcc','ba']].replace(to_replace={'present':1,'notpresent':0})\ndf[['appet']] = df[['appet']].replace(to_replace={'good':1,'poor':0,'no':np.nan})\ndf['classification'] = df['classification'].replace(to_replace={'ckd':1.0,'ckd\\t':1.0,'notckd':0.0,'no':0.0})\ndf.rename(columns={'classification':'class'},inplace=True)\ndf['pe'] = df['pe'].replace(to_replace='good',value=0) # Not having pedal edema is good\ndf['appet'] = df['appet'].replace(to_replace='no',value=0)\ndf['cad'] = df['cad'].replace(to_replace='\\tno',value=0)\ndf['dm'] = df['dm'].replace(to_replace={'\\tno':0,'\\tyes':1,' yes':1, '':np.nan})\n#df.drop('id',axis=1,inplace=True)\n\nfrom sklearn.model_selection import train_test_split\nsns.countplot(x = 'age', hue = 'appet', data = df)\nfor i in ['rc','wc','pcv']:\n    df[i] = df[i].str.extract('(\\d+)').astype(float)\nfor i in ['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','rc','wc','pcv']:\n    df[i].fillna(df[i].mean(),inplace=True)\ndf = df.dropna(axis=0)\ndf['class'].value_counts()\ndf.head()\ndf.info()\nvalues = df.values\n#data = pd.read_csv('..\/input\/kidney_disease.csv', header=0)\n#seed = 5\n#numpy.random.seed(seed)\n# Column Unnamed : 32 holds only null values, so it is of no use to us. We simply drop that column.\n#data.drop(\"Unnamed: 32\",axis=1,inplace=True)\n#data.drop(\"id\", axis=1, inplace=True)\n# Select the columns to use for prediction in the neural network\nprediction_var = ['sg','al','su','bgr','sc','pot','pcv','wc','rc','dm']\nX = df[prediction_var].values\nY = df['class'].values\n# Diagnosis values are strings. Changing them into numerical values using LabelEncoder.\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n# Baseline model for the neural network. We choose a hidden layer of 10 neurons. The lesser number of neurons helps to eliminate the redundancies in the data and select the more important features.\ndef create_baseline():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, input_dim=10, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model. We use the the logarithmic loss function, and the Adam gradient optimizer.\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n# Evaluate model using standardized dataset. \nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Results:%.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n","b1ed5d02":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score, ShuffleSplit, cross_validate\n\n# Preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, Imputer, LabelEncoder\n\n# Metrics :\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score, classification_report\n\nprint(\"Setup complete...\")\nlabels = df.columns[:]\nX = df[labels]\nprint(X)\ny = df['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n#model = XGBClassifier()\n\n#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('ABR', AdaBoostRegressor()))\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test).astype(int)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(name,accuracy)\n    ","18cac60e":"#print(score)\nstats.append(score)\n# plot scores\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95\np = ((1.0-alpha)\/2.0) * 100\nlower = max(0.0, numpy.percentile(stats, p))\np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, numpy.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","b3584afa":"print(train)\nprint(test)\n","c858edc7":"print(train.shape)\nprint(test.shape)","45bfe539":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score\nfrom imblearn.over_sampling import SMOTE\n#loans = pd.read_csv('..\/lending-club-data.csv.zip')\n#loans.iloc[0]\n#loans.bad_loans.value_counts()\n#loans = loans[~loans.payment_inc_ratio.isnull()]\n#model_variables = ['grade', 'home_ownership','emp_length_num', 'sub_grade','short_emp',\n          #  'dti', 'term', 'purpose', 'int_rate', 'last_delinq_none', 'last_major_derog_none',\n           # 'revol_util', 'total_rec_late_fee', 'payment_inc_ratio', 'bad_loans']\n\n#loans_data_relevent = loans[model_variables]\n#loans_relevant_enconded = pd.get_dummies(loans_data_relevent)\n#training_features, test_features, \\\n#training_target, test_target, = train_test_split(loans_relevant_enconded.drop(['bad_loans'], axis=1),\n #                                              loans_relevant_enconded['bad_loans'],\n  #                                             test_size = .1,\n   #                                            random_state=12)\nsm = SMOTE(random_state=12, ratio = 1.0)\nx_res, y_res = sm.fit_sample(training_features, training_target)\n#print training_target.value_counts(), np.bincount(y_res)\nx_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res,\n                                                    y_res,\n                                                    test_size = .1,\n                                                    random_state=12)\n","708c1fd8":"#oversampling\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score, ShuffleSplit, cross_validate\n\n# Preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, Imputer, LabelEncoder\n\n# Metrics :\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score, classification_report\n\nprint(\"Setup complete...\")\n\n\ndf = pd.read_csv('..\/input\/kidney_disease.csv')\n\ntarget_count = df.classification.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');\n# Data Preprocessing\n\ndf[['htn','dm','cad','pe','ane']] = df[['htn','dm','cad','pe','ane']].replace(to_replace={'yes':1,'no':0})\ndf[['rbc','pc']] = df[['rbc','pc']].replace(to_replace={'abnormal':1,'normal':0})\ndf[['pcc','ba']] = df[['pcc','ba']].replace(to_replace={'present':1,'notpresent':0})\ndf[['appet']] = df[['appet']].replace(to_replace={'good':1,'poor':0,'no':np.nan})\ndf['classification'] = df['classification'].replace(to_replace={'ckd':1.0,'ckd\\t':1.0,'notckd':0.0,'no':0.0})\ndf.rename(columns={'classification':'class'},inplace=True)\ndf['pe'] = df['pe'].replace(to_replace='good',value=0) # Not having pedal edema is good\ndf['appet'] = df['appet'].replace(to_replace='no',value=0)\ndf['cad'] = df['cad'].replace(to_replace='\\tno',value=0)\ndf['dm'] = df['dm'].replace(to_replace={'\\tno':0,'\\tyes':1,' yes':1, '':np.nan})\n#df.drop('id',axis=1,inplace=True)\n\nfrom sklearn.model_selection import train_test_split\nsns.countplot(x = 'age', hue = 'appet', data = df)\nfor i in ['rc','wc','pcv']:\n    df[i] = df[i].str.extract('(\\d+)').astype(float)\nfor i in ['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','rc','wc','pcv']:\n    df[i].fillna(df[i].mean(),inplace=True)\ndf = df.dropna(axis=0)\ndf['class'].value_counts()\ndf.head()\ndf.info()\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Remove 'id' and 'target' columns\nlabels = df.columns[1:]\nprint(labels)\nX = df[labels]\nprint(X)\ny = df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1, random_state=1)\nprint(\"Training Dataset size:\",X_train)\n#model = XGBClassifier()\n#model.fit(X_train, y_train)\n#y_pred = model.predict(X_test)\n\n#accuracy = accuracy_score(y_test, y_pred)\n#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('ABR', AdaBoostRegressor()))\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test).astype(int)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(name,accuracy)","3a0e2835":"#run with feature which reduce the classifier performane\n#model = XGBClassifier()\n#model.fit(X_train[['age']], y_train)\n#y_pred = model.predict(X_test[['age']])\n\n#accuracy = accuracy_score(y_test, y_pred)\n#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","37454353":"from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","5118ec2a":"#count_class_0, count_class_1 = df.classification.value_counts()\ncount_class_0=target_count[0]\ncount_class_1=target_count[1]\ndf_class_0 = df[df['class'] == 0]\ndf_class_1 = df[df['class'] == 1]\nprint(df_class_1.count())\n# Divide by class\n","f4d8b51e":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n#print(df_class_1_over)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\nprint(df_test_over)\nprint(df_test_over[df_test_over['class'] == 0])\n#print(df_test_over[df_test_over['class'] == 1])\nprint('Random over-sampling:')\n#print(df_test_over.class.value())\n#print(df.info())\n#print(df_test_over.class.value_counts())\n\ndf_test_over['class'].value_counts().plot(kind='bar', title='Count (target)');","5429751a":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score, ShuffleSplit, cross_validate\n\n# Preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, Imputer, LabelEncoder\n\n# Metrics :\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score, classification_report\n\nprint(\"Setup complete...\")\nmethod_names = []\nmethod_scores = []\nlabels = df.columns[:]\nX = df[labels]\nprint(X)\ny = df['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n#model = XGBClassifier()\n\n#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('ABR', AdaBoostRegressor()))\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test).astype(int)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(name,accuracy)\n    method_names.append(name)\n    method_scores.append(accuracy)\n    \n    #print(name,accuracy_score(y_test, pred))\n    \n   # model.fit(X_train, y_train)\n    #if model='XGB':\n        #big_X = x_train[feature_columns_to_use].append(x_train[feature_columns_to_use])\n        #big_X_imputed = DataFrameImputer().fit_transform(big_X)\n    #pred = model.predict(y_test).astype(int)\n    #print(name, accuracy_score(y_test, pred))","eb07408c":"plt.figure(figsize=(15,10))\nplt.ylim([0.20,1])\nplt.bar(method_names,method_scores,width=0.5)\nplt.xlabel('Method Name')\nplt.ylabel('Method Accuracy')","9186abbd":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')\nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\n\nplot_2d_space(X_ros, y_ros, 'Random over-sampling')\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')","9f99a431":"#RNN\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_dim=26))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer='nadam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])  \nprint(X_train.shape)\nmodel.fit(X_train,y_train,verbose=1,shuffle=True, nb_epoch=3,batch_size=100,validation_split=0.4)\n\n# Read test data and evaluate results\n\n\nscore = model.evaluate(X_test,y_test, batch_size=16)\nprint(\"LOSS\")\nprint(score[0])\nprint(\"precision\")\nprint(score[1])","791f4dab":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nencoder = LabelEncoder()\nencoder.fit(y)\nencoded_Y = encoder.transform(y)\ndef create_baseline():\n    # create model\n    model = Sequential()\n    model.add(Dense(5, input_dim=10, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model. We use the the logarithmic loss function, and the Adam gradient optimizer.\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","30babf90":"# The GRU architecture\nregressorGRU = Sequential()\n# First GRU layer with Dropout regularisation\nregressorGRU.add(GRU(units=26, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Second GRU layer\nregressorGRU.add(GRU(units=26, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Third GRU layer\nregressorGRU.add(GRU(units=26, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Fourth GRU layer\nregressorGRU.add(GRU(units=26, activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# The output layer\nregressorGRU.add(Dense(units=1))\n# Compiling the RNN\nregressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n# Fitting to the training set\nregressorGRU.fit(X_train,y_train,epochs=50,batch_size=150)","2ebd27d7":"# using a different dataset with 8000 records\n","1d7450ac":"predicted_stock_price = regressor.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)","89198357":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report,accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ndef auc_scorer(clf, X, y, model): # Helper function to plot the ROC curve\n    if model=='RF':\n        fpr, tpr, _ = roc_curve(y, clf.predict_proba(X)[:,1])\n    elif model=='SVM':\n        fpr, tpr, _ = roc_curve(y, clf.decision_function(X))\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure()    # Plot the ROC curve\n    plt.plot(fpr, tpr, label='ROC curve from '+model+' model (area = %0.3f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr,tpr,roc_auc\n\n# from subprocess import check_output\n# print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","02267142":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nimport time\nimport spacy\nfrom scipy.misc import imread\ntrain_df = pd.read_csv('..\/input\/kidney_disease.csv')\ndf = pd.read_csv('..\/input\/kidney_disease.csv')\nstart_time = time.time()\nend = time.time()\nprint(\"Time taken in reading the input files is {}.\".format(end - start_time))\ntrain_df.head()\nprint(\"Number of rows in train dataset {} \".format(train_df.shape[0]))\n\ntrain_df.describe()\n\n","5e2cdfea":"# Map text to 1\/0 and do some cleaning\ndf[['htn','dm','cad','pe','ane']] = df[['htn','dm','cad','pe','ane']].replace(to_replace={'yes':1,'no':0})\ndf[['rbc','pc']] = df[['rbc','pc']].replace(to_replace={'abnormal':1,'normal':0})\ndf[['pcc','ba']] = df[['pcc','ba']].replace(to_replace={'present':1,'notpresent':0})\ndf[['appet']] = df[['appet']].replace(to_replace={'good':1,'poor':0,'no':np.nan})\ndf['classification'] = df['classification'].replace(to_replace={'ckd':1.0,'ckd\\t':1.0,'notckd':0.0,'no':0.0})\ndf.rename(columns={'classification':'class'},inplace=True)\ndf['pe'] = df['pe'].replace(to_replace='good',value=0) # Not having pedal edema is good\ndf['appet'] = df['appet'].replace(to_replace='no',value=0)\ndf['cad'] = df['cad'].replace(to_replace='\\tno',value=0)\ndf['dm'] = df['dm'].replace(to_replace={'\\tno':0,'\\tyes':1,' yes':1, '':np.nan})\ndf.drop('id',axis=1,inplace=True)\ncorr = df.corr()\nplt.figure(figsize = (15,15))\nsns.heatmap(data = corr, annot = True, square = True, cbar = True)\n","3c33cc29":"# Further cleaning\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.grid()\nplt.title(\"Number of Missing Values\")\nplt.savefig('missing.png')\n","f2fb4696":"df.head()\ndf.info()","354920da":"from sklearn.model_selection import train_test_split\nsns.countplot(x = 'age', hue = 'appet', data = df)\nfor i in ['rc','wc','pcv']:\n    df[i] = df[i].str.extract('(\\d+)').astype(float)\nfor i in ['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','rc','wc','pcv']:\n    df[i].fillna(df[i].mean(),inplace=True)\n\n","877efbbb":"df2 = df.dropna(axis=0)\ndf2['class'].value_counts()","be1534e6":"corr_df = df2.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_df, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title('Correlations between different predictors')\nplt.show()","8e39eb30":"x_train, x_test, y_train, y_test = train_test_split(df2.iloc[:,:-1], df2['class'], \n                                                    test_size = 0.50, random_state=44,\n                                                   stratify= df2['class'] )\nprint(x_train.shape)\nprint(x_test.shape)\n#df['rbc'] = pd.to_numeric(df['rbc'],errors='coerce')\n#df['wc'] = pd.to_numeric(df['wc'],errors='coerce')\n#df['pc'] = pd.to_numeric(df['pc'],errors='coerce')\n#df['pcc'] = pd.to_numeric(df['pcc'],errors='coerce')\n#df['ba'] = pd.to_numeric(df['ba'],errors='coerce')\n#df['htn'] = pd.to_numeric(df['htn'],errors='coerce')\n#df['pcv'] = pd.to_numeric(df['pcv'],errors='coerce')\n#df['rc'] = pd.to_numeric(df['rc'],errors='coerce')\n#df['cad'] = pd.to_numeric(df['cad'],errors='coerce')\n#df['dm'] = pd.to_numeric(df['dm'],errors='coerce')\n#df.dtypes\ny_train.value_counts()\n","328cfe02":"#df2 = df.dropna(axis=0)\n#no_na = df2.index.tolist()\n#some_na = df.drop(no_na).apply(lambda x: pd.to_numeric(x,errors='coerce'))\n#some_na = some_na.fillna(0) # Fill up all Nan by zero.\n\n#x_test = some_na.iloc[:,:-1]\n#y_test = some_na['class']\n#y_true = y_test\n#lr_pred = clf_best.predict(x_test)\n#print(classification_report(y_true, lr_pred))\n\n#confusion = confusion_matrix(y_test, lr_pred)\n#print('Confusion Matrix:')\n#print(confusion)\n\n#print('Accuracy: %3f' % accuracy_score(y_true, lr_pred))\n# Determine the false positive and true positive rates\n#fpr,tpr,roc_auc = auc_scorer(clf_best, x_test, y_test, 'RF')","a235798f":"import xgboost as xgb\nmodel = xgb.XGBClassifier(objective ='binary:logistic', colsample_bytree=0.3, learning_rate=0.1,max_depth=8, n_estimators=50)\nmodel.fit(x_train, y_train)","259ab99d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report,accuracy_score\n\nmethod_names = []\nmethod_scores = []\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train) #Fitting\nprint(\"Logistic Regression Classification Test Accuracy {}\".format(log_reg.score(x_test,y_test)))\nmethod_names.append(\"Logistic Reg.\")\nmethod_scores.append(log_reg.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = log_reg.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","ca2c8ab6":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report,accuracy_score\n\ndec_tree = DecisionTreeClassifier()\ndec_tree.fit(x_train,y_train)\nprint(\"Decision Tree Classification Score: \",dec_tree.score(x_test,y_test))\nmethod_names.append(\"Decision Tree\")\nmethod_scores.append(dec_tree.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = dec_tree.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","89c5c246":"plt.figure(figsize=(15,10))\nplt.ylim([0.85,1])\nplt.bar(method_names,method_scores,width=0.5)\nplt.xlabel('Method Name')\nplt.ylabel('Method Score')","e685b88d":"tuned_parameters = [{'n_estimators':[7,8,9,10,11,12,13,14,15,16],'max_depth':[2,3,4,5,6,None],\n                     'class_weight':[None,{0: 0.33,1:0.67},'balanced'],'random_state':[42]}]\nclf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=10,scoring='f1')\nclf.fit(x_train, y_train)\n\n\nprint(\"Detailed classification report:\")\ny_true, lr_pred = y_test, clf.predict(x_test)\nprint(classification_report(y_true, lr_pred))\n\nconfusion = confusion_matrix(y_test, lr_pred)\nprint('Confusion Matrix:')\nprint(confusion)\n\n# Determine the false positive and true positive rates\n#fpr,tpr,roc_auc = auc_scorer(clf, x_test, y_test, 'RF')\n\nprint('Best parameters:')\nprint(clf.best_params_)\nclf_best = clf.best_estimator_\n","fbdde7cb":"plt.figure(figsize=(12,3))\nfeatures = x_test.columns.values.tolist()\nimportance = clf_best.feature_importances_.tolist()\nfeature_series = pd.Series(data=importance,index=features)\nfeature_series.plot.bar()\nplt.title('Feature Importance')","aefb0501":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train) #Fitting\nprint(\"Logistic Regression Classification Test Accuracy {}\".format(log_reg.score(x_test,y_test)))\nmethod_names.append(\"Logistic Reg.\")\nmethod_scores.append(log_reg.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = log_reg.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","fc060846":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score, ShuffleSplit, cross_validate\n\n# Preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, Imputer, LabelEncoder\n\n# Metrics :\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score, classification_report\n\nprint(\"Setup complete...\")\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('ABR', AdaBoostRegressor()))\n\nfor name, model in models:\n    model.fit(x_train, y_train)\n    #if model='XGB':\n        #big_X = x_train[feature_columns_to_use].append(x_train[feature_columns_to_use])\n        #big_X_imputed = DataFrameImputer().fit_transform(big_X)\n    pred = model.predict(x_test).astype(int)\n    print(name, accuracy_score(y_test, pred))","d98804df":"from sklearn.preprocessing import LabelEncoder\n\nsns.heatmap(df.isnull(),cmap=\"viridis\",cbar=False,yticklabels=False)\nx_train, xtest, y_train, y_test = train_test_split(df.iloc[:,:-1], df['classification'], \n                                                    test_size = 0.33, random_state=44,\n                                                   stratify= df['classification'] )\n\n\n\n\n    ","960aa0fe":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.title(\"Number of Missing Values\")\nplt.savefig('missing_updated.png')","9f3c128e":"from sklearn.preprocessing import LabelEncoder\nimport glob\nimport keras as k\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nimport matplotlib.pyplot as plt\n\n#columns_to_retain = [\"sg\", \"al\", \"sc\", \"hemo\",\"pcv\", \"wbcc\", \"rbcc\", \"htn\", \"classification\"]\n#olumns_to_retain = df.columns\n#df = df.drop([col for col in df.columns if not col in columns_to_retain], axis=1)\n    # now drop the rows with na values\n#df = df.dropna(axis=0)\n\n#for column in df.columns:\n   # if df[column].dtype == np.number:\n   #     continue\n    #df[column] = LabelEncoder().fit_transform(df[column])\n\n#y = df.classification.values\n#x = df.drop([\"classification\"],axis=1)\n#from sklearn.preprocessing import LabelEncoder\n\n#for i in ['rbc','pc','pcc','ba','htn','dm','cad','appet','pe','ane','classification']:\n   # df[i] = df[i].map({'3+': 3})\n #   df[i] = LabelEncoder().fit_transform(df[i])\n\n#df2 = df.dropna(axis=0)\n#df2['class'].value_counts()\n#x_train = df.drop(['id','classification'],axis=1)\n#y_train = df['classification']\n#x_train, x_test, y_train, y_test = train_test_split(df2.iloc[:,:-1], df2['classification'], \n #                                                   test_size = 0.33, random_state=44,\n  #                                                 stratify= df2['classification'] )\n#x_scaler = MinMaxScaler()\n#x_scaler.fit(x)\n#column_names = x.columns\n#x[column_names] = x_scaler.transform(x)\n\n#x_train,  x_test, y_train, y_test = train_test_split(x, y, test_size=5, shuffle=True)\n\n#optimizer = k.optimizers.Adam()\n#checkpoint = ModelCheckpoint(\"ckd.best.model\", monitor=\"loss\",\n                                # mode=\"min\", save_best_only=True, verbose=0)\n#model = k.models.Sequential()\n#model.add(Dense(256, input_dim=len(x.columns),kernel_initializer=k.initializers.random_normal(seed=13), activation=\"relu\"))\n#from sklearn.model_selection import train_test_split\n#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)\n\n\n","728741a6":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score, ShuffleSplit, cross_validate\n\n# Preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, Imputer, LabelEncoder\n\n# Metrics :\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score, classification_report\n\nprint(\"Setup complete...\")\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('ABR', AdaBoostRegressor()))\n\nfor name, model in models:\n    model.fit(x_train, y_train)\n    pred = model.predict(x_test).astype(int)\n    print(name, accuracy_score(y_test, pred))","3fe5fc5a":"\ncorr_df = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_df, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title('Correlations between different predictors')\nplt.show()","e6ca5aea":"svc = SVC()\n\nparams = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\n\nclf = GridSearchCV(svc, param_grid = params, scoring = 'accuracy', cv = 10, verbose = 2)\n\nclf.fit(x_train, y_train)\nclf.best_params_\nmodel = Sequential()\nmodel.add(Dense(100,input_dim=X.shape[1],activation='relu'))\nmodel.add(Dense(50,activation='relu'))\nmodel.add(Dense(25,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X,Y,epochs=100,batch_size=40,validation_split=.2,verbose=2)\nmodel.summary()","656d74f9":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f51da3b8":"scores = model.evaluate(X,Y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","d6437af7":"print(X_train.shape)\nprint(X_test.shape)","d0d6911f":"y_train.value_counts()","434516cb":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report,accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ndef auc_scorer(clf, X, y, model): # Helper function to plot the ROC curve\n    if model=='RF':\n        fpr, tpr, _ = roc_curve(y, clf.predict_proba(X)[:,1])\n    elif model=='SVM':\n        fpr, tpr, _ = roc_curve(y, clf.decision_function(X))\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure()    # Plot the ROC curve\n    plt.plot(fpr, tpr, label='ROC curve from '+model+' model (area = %0.3f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr,tpr,roc_auc\n\n# from subprocess import check_output\n# print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","4066826d":"tuned_parameters = [{'n_estimators':[7,8,9,10,11,12,13,14,15,16],'max_depth':[2,3,4,5,6,None],\n                     'class_weight':[None,{0: 0.33,1:0.67},'balanced'],'random_state':[42]}]\nclf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=10,scoring='f1')\nclf.fit(x_train, y_train)\n\nprint(\"Detailed classification report:\")\ny_true, lr_pred = y_test, clf.predict(x_test)\nprint(classification_report(y_true, lr_pred))\n\nconfusion = confusion_matrix(y_test, lr_pred)\nprint('Confusion Matrix:')\nprint(confusion)\n\n# Determine the false positive and true positive rates\nfpr,tpr,roc_auc = auc_scorer(clf, x_test, y_test, 'RF')\n\nprint('Best parameters:')\nprint(clf.best_params_)\nclf_best = clf.best_estimator_\n","1163c01e":"plt.figure(figsize=(12,3))\nfeatures = x_test.columns.values.tolist()\nimportance = clf_best.feature_importances_.tolist()\nfeature_series = pd.Series(data=importance,index=features)\nfeature_series.plot.bar()\nplt.title('Feature Importance')","5afbe45e":"list_to_fill = x_test.columns[feature_series>0.03]\nprint(list_to_fill)","6308b31a":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.callbacks import EarlyStopping\nfrom keras import optimizers\nfrom keras import regularizers\nBATCH_SIZE = 40\nEPOCHS = 50\nLEARNING_RATE = 0.001\nDATASET_SIZE = 400\nmodel = Sequential()\nprint(x_test.columns[feature_series>0.03].shape[0])\nmodel.add(Dense(100,input_dim=x_test.columns[feature_series>0.03].shape[0],activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(50,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(25,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1))\n\nadam = optimizers.adam(lr=LEARNING_RATE)\nmodel.compile(loss='mse', optimizer=adam, metrics=['mae'])\nprint('Dataset size: %s' % DATASET_SIZE)\nprint('Epochs: %s' % EPOCHS)\nprint('Learning rate: %s' % LEARNING_RATE)\nprint('Batch size: %s' % BATCH_SIZE)\nprint('Input dimension: %s' % x_test.shape[1])\nprint('Features used: %s' % x_test.columns[feature_series>0.03])","fcced69d":"model.summary()","ceb120fd":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom keras import metrics\nmax_len = 24\ndef get_cnn_model_v1():   \n    model = Sequential()\n    # we start off with an efficient embedding layer which maps\n    # our vocab indices into embedding_dims dimensions\n    # 1000 is num_max\n    model.add(Embedding(400,\n                        20,\n                        input_length=max_len))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(64,\n                     3,\n                     padding='valid',\n                     activation='relu',\n                     strides=1))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(256))\n    model.add(Dropout(0.2))\n    model.add(Activation('relu'))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    model.summary()\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc',metrics.binary_accuracy])\n    return model\n\n\ndef check_model(model,x_train,y_train):\n    model.fit(X,y,batch_size=40,epochs=10,verbose=1,validation_split=0.5)\n\nm = get_cnn_model_v1()\ncheck_model(m,x_train, y_train)\n \n","4c1e184f":"## Cleaning and preprocessing of data for training a classifier","ab151603":"## Load Modules and helper functions","5f413522":"## Split the set for training models further into a (sub-)training set and testing set.","fb0fa181":"## Examine correlations between different features","26b5d81a":"##### Building a Deep Learning model","f731abb5":"## Check the portion of rows with NaN\n- Now the data is cleaned with improper values labelled NaN. Let's see how many NaNs are there.\n- Drop all the rows with NaN values, and build a model out of this dataset (i.e. df2)","e19abc20":"# Predicting Chronic Kidney Disease based on health records\nThe proposed health record of chronic kideny failure is taken from UCI Machine learning dataset.\nIt consists of 400 instances and 25 attributes\n![![image.png](attachment:image.png)](http:\/\/)\n\n\n## Summary of Results\nThe predictive analytics was done using various classifiers like  LogisticRegression\nLinearDiscriminantAnalysis\nKNeighborsClassifier\nDecisionTreeClassifier\nGaussianNB\nSVM\nRandomForestRegressor\nXGBClassifier\nGradientBoostingRegressor\nAdaBoostRegressor\nand the best classifier with high accuracy was evaluated\nThe sample size for the classifier is analysed using Learning curve","9e3552b7":"## Make predictions with the best model selected above\nI filled in all NaN with 0 and pass it to the trained classifier. The results are as follows:\n- True positive = 180\n- True negative = 35\n- False positive = 0\n- False negative = 27\n----\n- Accuracy = 88.8%\n- ROC AUC = 99.2%","b41e957d":"## Examine feature importance\nSince I pruned the forest (*max_depth*=2) and decrease the number of trees (*n_estimators*=8), not all features are used.","992247dd":"## Load files","30771b5e":"## Choosing parameters with GridSearchCV with 10-fold cross validations.\n(Suggestion for next time: try using Bayesian model selection method)","6326f401":"## Next, I examine the rest of the dataset (with missing values across the rows)\nAre there correlations between occurence of missing values in a row? The plot suggests, seems no."}}