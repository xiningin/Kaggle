{"cell_type":{"70a99688":"code","8c459995":"code","2fb85e0d":"code","5acb5521":"code","0c79511c":"code","547eb94f":"code","b603ca51":"code","3014483b":"code","4291e13e":"code","8d12b223":"code","dc6b3ee1":"code","e508c738":"code","e898f782":"code","43301852":"code","d8bee408":"code","88ad22b6":"code","9e8c3588":"markdown","0e4baa69":"markdown","7fe3aa86":"markdown","ca9615fe":"markdown","964bc2b3":"markdown","4cb10fc4":"markdown","40c18d7d":"markdown","61c1be7b":"markdown","2de52526":"markdown","3d429a08":"markdown","2898cd9d":"markdown","524720e4":"markdown"},"source":{"70a99688":"import numpy as np \nimport pandas as pd\nimport pandas_profiling\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom statistics import mean \n\n# Data Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","8c459995":"df_train.profile_report(style={'full_width': True})","2fb85e0d":"# Embarked is missing 2 data entries so let's fill those with the Mode value \ndf_embarked = df_train['Embarked'].copy()\ndf_embarked.fillna(df_embarked.mode(), inplace=True)\ndf_train['EmbarkedFilled'] = df_embarked\nsns.barplot(x=\"EmbarkedFilled\", y=\"Survived\", hue=\"Sex\", data=df_train)","5acb5521":"sns.barplot(x='Pclass', y='Survived', hue=\"Sex\", data=df_train)","0c79511c":"# Age is interesting so let's do some light transformation and look at the Age column via age groups\ndf_age = df_train['Age'].copy()\n\n# Age column is missing some data approximately 20% so let's do some pre-processing and fill in the missing data with the median age.\nmedian_age = df_age[pd.notna(df_age)].median()\ndf_age.fillna(median_age, inplace=True)\n\n# Next let's decide a real life grouping for the ages:\n# baby- 0-1; toddler- 1-3; preschool- 3-5; gradeschooler- 5-12; teen- 12-18; young adult- 18-21; adult- 21-54; senior- 55+ \nbins = (-1, 1, 3, 5, 12, 18, 21, 54, 100)\nage_groups = pd.cut(df_age, bins=bins, labels=['baby', 'todd', 'pre', 'grade', 'teen', 'ya', 'adult', 'senior'])\n# Add it to the original df so we can graph\ndf_train['AgeGroup'] = age_groups\n\n# Graph the age grouping\nsns.barplot(x='AgeGroup', y='Survived', hue='Sex', data=df_train)","547eb94f":"# Given there are 248 unique fare amounts paid that may be too disperse and noisy let's group them into bins based on the quantiles.\n# I was trying to think of ways to naturally group them similar to how we did with ages but there's no obvious way. For example if there\n# were price tiers that we can derive from the fare amounts that would've helped but we don't have any indication of a tier. I mean who\n# was setting the pricing on this trip sheeesh. \n\ndf_fare = df_train['Fare'].copy()\nbins = (-1, 8, 14, 31, 600)\ndf_train['FareGroup'] = pd.cut(df_fare, bins=bins)\nsns.barplot(x='FareGroup', y='Survived', hue='Sex', data=df_train)\n","b603ca51":"sns.pointplot(x='Parch', y='Survived', hue='Sex', data=df_train)","3014483b":"sns.pointplot(x='SibSp', y='Survived', hue='Sex', data=df_train)","4291e13e":"def fillAges(df):\n    '''Fill in missing data for Age column by using the median'''\n    df_age = pd.DataFrame(df['Age'])\n    imputer = SimpleImputer(strategy='median')\n    df['Age'] = imputer.fit_transform(df_age)\n    return df\n    \ndef fillEmbarked(df):\n    '''Fill in missing data for the Embarked column by using the mode'''\n    df_embarked = pd.DataFrame(df['Embarked'])\n    imputer = SimpleImputer(strategy='most_frequent')\n    df['Embarked'] = imputer.fit_transform(df_embarked)\n    return df\n    \ndef addAgeGroup(df):\n    '''Group ages in bins and create a new series for it in the dataframe'''\n    # baby- 0-1; toddler- 1-3; preschool- 3-5; gradeschooler- 5-12; teen- 12-18; young adult- 18-21; adult- 21-54; senior- 55+ \n    bins = (-1, 1, 3, 5, 12, 18, 21, 54, 100)\n    age_group = pd.cut(df['Age'], bins=bins, labels=['baby', 'todd', 'pre', 'grade', 'teen', 'ya', 'adult', 'senior'])\n    df['AgeGroup'] = age_group\n    return df\n\ndef addFareGroup(df):\n    '''Group fare in bins and create a new series it in the dataframe'''\n    bins = (-1, 8, 14, 31, 600)\n    df['FareGroup'] = pd.cut(df_fare, bins=bins, labels=['1','2','3','4'])\n    return df\n\ndef addTitle(df):\n    '''Add title column based on the Name column'''\n    name = df['Name'].copy()\n    df['Title'] = name.apply(lambda x: x.split(', ')[1].split(' ')[0])\n    return df\n\ndef dropUnused(df, cols_to_keep):\n    '''Drop the unused columns'''\n    return df[cols_to_keep]\n\ndef clean(df, cols_to_keep):\n    '''Clean the data and include the columns we dont want to drop in the end'''\n    df = fillAges(df)\n    df = fillEmbarked(df)\n    df = addAgeGroup(df)\n    df = addFareGroup(df)\n    df = addTitle(df)\n    df = dropUnused(df, cols_to_keep)\n    return df\n\ndf_train = clean(df_train, ['AgeGroup', 'Title', 'FareGroup', 'Embarked', 'Parch', 'SibSp', 'Sex', 'Pclass', 'Survived'])\ndf_test = clean(df_test, ['AgeGroup', 'Title', 'FareGroup', 'Embarked', 'Parch', 'SibSp', 'Sex', 'Pclass', 'PassengerId']) # We need to keep the PassengerId for submission","8d12b223":"def encode(df_train, df_test, features):\n    '''Encode the categorical columns'''\n    \n    # combine so that we can fit over the training and test data to ensure best results for transform\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    for feature in features:\n        lbl_encoder = LabelEncoder()\n        lbl_encoder = lbl_encoder.fit(df_combined[feature])\n        df_train[feature] = lbl_encoder.transform(df_train[feature])\n        df_test[feature] = lbl_encoder.transform(df_test[feature])\n        \n    return (df_train, df_test)\n\n\ncategorical_features = ['Embarked', 'Sex', 'Title', 'AgeGroup', 'FareGroup']\ndf_train, df_test = encode(df_train, df_test, categorical_features)","dc6b3ee1":"df_train.profile_report(style={'full_width': True})","e508c738":"X_train_all = df_train.drop(['Survived'], axis=1)\ny_train_all = df_train['Survived']\n\ntest_size = 0.25\nX_train, X_test, y_train, y_test = train_test_split(X_train_all, y_train_all, test_size=test_size, random_state=1)","e898f782":"def simple_predict(X_train, y_train, X_test, y_test, pred_models):\n    models = []\n    score_results = []\n\n    for m in prediction_models:\n        model = eval(m)()\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        models.append(m)\n        score_results.append(accuracy_score(y_test, y_pred))\n        \n    return pd.DataFrame({'model': models, 'result': score_results})\n\npredict(X_train, y_train, X_test, y_test, ['RandomForestClassifier', 'SGDClassifier', 'GradientBoostingClassifier'])\n\n","43301852":"def cross_validation(X_all, y_all, pred_models):\n    '''Cross validate across all the different classifiers. By default use 4 folds'''\n    models = []\n    result_1 = []\n    result_2 = []\n    result_3 = []\n    result_4 = []\n    means = []\n    \n    for m in pred_models:\n        model = eval(m)()\n        results = cross_val_score(model, X_all, y_all, cv=4, scoring='accuracy')\n        models.append(m)\n        result_1.append(results[0])\n        result_2.append(results[1])\n        result_3.append(results[2])\n        result_4.append(results[3])\n        means.append(mean(results))\n        \n    return pd.DataFrame({'model': models, \n                         'result1': result_1,\n                         'result2': result_2,\n                         'result3': result_3,\n                         'result4': result_4, \n                         'mean': means})\n\ncross_validation(X_train_all, y_train_all, ['RandomForestClassifier', 'SGDClassifier', 'GradientBoostingClassifier'])\n        \n","d8bee408":"# Separate the passenger Ids for submission\npass_id_series = df_test['PassengerId']\ndf_test = df_test.drop(['PassengerId'], axis=1)\n\n# Predict\nrfc = RandomForestClassifier()\nrfc.fit(X_train_all, y_train_all)\npredictions = rfc.predict(df_test)\n\n# Setup final data frame for csv output\ndf_output = pd.DataFrame({'PassengerId': pass_id_series, 'Survived': predictions})\nprint(df_output.head())","88ad22b6":"# output to csv\ndf_output.to_csv('submission.csv', index=False)","9e8c3588":"## Data Visualization\nLet's dive a little deeper to see if we can identify stronger patterns and relationships by using graphs.","0e4baa69":"## Predict Test Data\nNow that we have our results from the classifiers above. Let's now predict the test data using the winning model. ","7fe3aa86":"# Conclusion\nWhile this was a good first attempt at creating a prediction model for the Titanic competition I feel like there is still more I could've done to improve the accuracy of the prediction. There are still some strategies left on the table in terms of *parameter tuning*, comparing more models  and creating custom ensemble models. I also could've invested in more graphs to reveal more depth on feature correlation to our prediction target as well. Instinctively I also feel like there is a win somewhere with a feature *interaction*, combining 2 or more features. One day I'll come back and see if I can improve it more. \n\nIf you made it here thanks for following along I hope this proved helpful to you.","ca9615fe":"## Sanity Check \nAs a sanity check let's make sure all the values are cleaned, pre-processed and encoded. If not then we missed a step. ","964bc2b3":"# Feature Engineering","4cb10fc4":"# Introduction\nNever was a fan of Titanic movie so I don't have too much pre-knowledge of it but familiar with the event of course. Dived a little deeper for more context and understanding here [Titanic Encyclopedia Entry](https:\/\/www.encyclopedia-titanica.org\/titanic\/). The goal of this notebook is to build a machine learning model that accurately predicts who survived and who did not survive based on statistical data provided on the passengers who were aboard the Titanic. ","40c18d7d":"## Model Selection\nFor the selection strategy we're going to keep it light and use the default paramters for the models. We're going to first do a plain comparison between each model's predict on the test data. Then we're going to run cross-validation on each of the models for a higher rate of confidence on the winning model. ","61c1be7b":"## Cleaning & Pre-Processing\n\nBefore we start feature engineering we must first decide on each of the columns we want to use and if we want to generate new features based on the given features available, such as the name title from the name column. In addition we must fill in any missing data for any of the features we will use. Lastly we must ensure that we perform the same cleaning on the test data as well. \n\nThe features we will be going with are: **AgeGroup**, **Title**, **FareGroup**, **Embarked**, **Pclass** and **Sex**. The columns that have missing data are **AgeGroup** and **Embarked** while the **Title**, **AgeGroup** and **FareGroup** need to be transformed. In addition let's transform the **Pclass** column by converting the string value into it's numerical value. The remaining columns in the data will be dropped. ","2de52526":"## Test Data\nNow that we have cleaned the data, perform pre-processing and some minor feature engineering strategies let move on to modeling. But first we must prepare the test data we will be using for hyperparamter tuning and model prediction validation.","3d429a08":"## Encoding\nNow that we cleaned the data let's encode the categorical features so that we can move on to modeling. The categorical features left are **Embarked**, **Title**, **FareGroup**, **AgeGroup**, and **Sex**. \n\nTo decide on the best encoding method for each feature let's think about each at a high-level. **Embarked** has a cardinality of 3, **Title** has a cardinality of 17, **Sex** has a cardinality of 2, **AgeGroup** with a cardinality of 8 and **FareGroup** with a cardinality of 6. The **Embarked** and **Sex** feature has no natural ordering usually **Title** does but under the circumstances of the Titanic it may have less importance. **FareGroup** and **AgeGroup** have been transformed into bins with labels so we can leverage the labels directly in order to encode. ","2898cd9d":"# Data Exploration\nLet's explore the data at a high-level to understand what we're working with, anticipate any cleaning we would have to do, and build a general instinct for what features we should leverage to create a prediction. I also used the following resource to understand what each of the columns [Titanic Meta](http:\/\/campus.lakeforest.edu\/frank\/FILES\/MLFfiles\/Bio150\/Titanic\/TitanicMETA.pdf).\n\nThe profile below gives us plenty of information about the data we're looking at. We can see that we're missing data for the **Age**, **Cabin** and the **Embarked** columns so we may have to do some cleaning there. There are 5 columns that are categorical which potentially means we will need to do some encoding as well. The **Cabin** and the **Ticket** columns have a high cardinality so we know type of encoding we may need to use since they're both categorical. The correlation graphs give us a high-level of where we should look for features. \n\nTaking a closer look we can also see off rip that the **Ticket** and **PassengerId** columns wouldn't prove too useful for our predictions because there's no pattern there it causes too much noise. I would also say the same for the **Name** column but we can see an underlying pattern in the titles: *Mr.*, *Mrs.* *Master*, etc. The **Cabin**  column is interesting because we can extrapolate a pattern if we only looked at the *letter* in the cabin number unfortunately though **Cabin** is missing over 3\/4s of its values. My decision here would be too also disregard this column as well. ","524720e4":"# Modeling\n"}}