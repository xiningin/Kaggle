{"cell_type":{"02ebfd82":"code","81d0930d":"code","3f512704":"code","2e2c3ceb":"code","70126144":"code","f1770b6e":"code","85fe9de2":"code","cd7eee98":"code","b0cbe410":"code","5c4c5dfe":"code","854078fe":"code","fbbb7b23":"code","8db31737":"code","6c468437":"code","aadd38c5":"code","91e3d4b7":"code","b990c1af":"code","605a411e":"code","f4c14a25":"code","d7f78951":"code","475cbdfe":"code","f6bfbd57":"code","fe0695c3":"code","f10673ba":"code","66372c28":"code","cb02d069":"code","1ae11b2e":"code","ea2054c9":"code","6abc04e9":"code","ba240e5b":"code","883be61d":"code","4f7b366c":"code","23fc881e":"code","63312d35":"code","7caec4b6":"code","22f52776":"code","47524ffc":"code","3df3b7d4":"code","4ce08f91":"code","e8e7dc4f":"code","3c8ba410":"markdown","f9228299":"markdown","71699642":"markdown","9ec706b5":"markdown","dac59897":"markdown","5e7a36f5":"markdown","d5b5bb12":"markdown","92f0b57d":"markdown"},"source":{"02ebfd82":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","81d0930d":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n# Data dictionary can be found here: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data","3f512704":"train.shape","2e2c3ceb":"test.shape","70126144":"train.columns","f1770b6e":"#Separate and drop the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","85fe9de2":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.distplot(train['SalePrice'])","cd7eee98":"f, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(train.corr(), vmax=.8, square=True)","b0cbe410":"k = 10 #number of variables for heatmap\ncols = train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","5c4c5dfe":"train_T = train[['SalePrice','OverallQual','GrLivArea','GarageCars','TotalBsmtSF',\n                 '1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt']]\nsns.set()\nsns.pairplot(train_T, size = 2.5)\nplt.show()","854078fe":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\ntrain_T = train[['SalePrice','OverallQual','GrLivArea','GarageCars','TotalBsmtSF',\n                 '1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt']]\nsns.set()\nsns.pairplot(train_T, size = 2.5)\nplt.show()","fbbb7b23":"# Credit to Kaggle Kernal: Stacked Regressions to predict House Prices\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","8db31737":"# Credit to Kaggle Kernal: Stacked Regressions to predict House Prices\n\n#We use the numpy function log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","6c468437":"# Separate SalePrice from the rest of the dataset\ny_train = train['SalePrice'].values\ntrain.drop('SalePrice', axis=1, inplace=True)","aadd38c5":"# Combine datasets to perform transformations\n\nntrain = train.shape[0]\nntest = test.shape[0]\ncombined = pd.concat([train, test])","91e3d4b7":"combined.shape","b990c1af":"missing = combined.isnull().sum()\nmissing = missing[missing != 0].sort_values(ascending=False)\nmissing","605a411e":"# Apply Nones to missing values\ncombined['PoolQC'] = combined['PoolQC'].fillna('None') # No entries treated as no pool\ncombined['MiscFeature'] = combined['MiscFeature'].fillna('None') # No entries treated as none\ncombined['Alley'] = combined['Alley'].fillna('None') # No entries treated as having no alley\ncombined['Fence'] = combined['Fence'].fillna('None') # No entries treated as having no fence\ncombined['FireplaceQu'] = combined['FireplaceQu'].fillna('None') # No entries treated as having no fireplace\ncombined['GarageType'] = combined['GarageType'].fillna('None') # No entries treated as having no Garage\ncombined['GarageFinish'] = combined['GarageFinish'].fillna('None') # No entries treated as having no Garage\ncombined['GarageQual'] = combined['GarageQual'].fillna('None') # No entries treated as having no Garage\ncombined['GarageCond'] = combined['GarageCond'].fillna('None') # No entries treated as having no Garage\ncombined['BsmtExposure'] = combined['BsmtExposure'].fillna('None') # No entries treated as having no Basement\ncombined['BsmtCond'] = combined['BsmtCond'].fillna('None') # No entries treated as having no Basement\ncombined['BsmtQual'] = combined['BsmtQual'].fillna('None') # No entries treated as having no Basement\ncombined['BsmtFinType2'] = combined['BsmtFinType2'].fillna('None') # No entries treated as having no Basement\ncombined['BsmtFinType1'] = combined['BsmtFinType1'].fillna('None') # No entries treated as having no Basement\ncombined['MasVnrType'] = combined['MasVnrType'].fillna('None') # No entries treated as having no masonry veneer\ncombined['MSSubClass'] = combined['MSSubClass'].fillna('None') # No entries treated as having no building class","f4c14a25":"# Apply zeroes to missing values\ncombined['GarageYrBlt'] = combined['GarageYrBlt'].fillna(0) # No entries treated as having no Garage-related numerics\ncombined['GarageCars'] = combined['GarageCars'].fillna(0) # No entries treated as having no Garage-related numerics\ncombined['GarageArea'] = combined['GarageArea'].fillna(0) # No entries treated as having no Garage-related numerics\ncombined['BsmtFullBath'] = combined['BsmtFullBath'].fillna(0) # No entries treated as having no Basement-related numerics\ncombined['BsmtHalfBath'] = combined['BsmtHalfBath'].fillna(0) # No entries treated as having no Basement-related numerics\ncombined['BsmtFinSF1'] = combined['BsmtFinSF1'].fillna(0) # No entries treated as having no Basement-related numerics\ncombined['BsmtFinSF2'] = combined['BsmtFinSF2'].fillna(0) # No entries treated as having no Basement-related numerics\ncombined['BsmtUnfSF'] = combined['BsmtUnfSF'].fillna(0) # No entries treated as having no Basement-related numerics\ncombined['TotalBsmtSF'] = combined['TotalBsmtSF'].fillna(0) # No entries treated as having no Basement-related numerics\ncombined['MasVnrArea'] = combined['MasVnrArea'].fillna(0) # No entries treated as having no masonry veneer-related numerics","d7f78951":"# Apply stats to missing values\ncombined['LotFrontage'] = combined.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median())) # No entries given median of their respective neighborhood\ncombined['MSZoning'] = combined['MSZoning'].fillna(combined['MSZoning'].mode()[0]) # No entries given mode of MSZoning\ncombined['Electrical'] = combined['Electrical'].fillna(combined['Electrical'].mode()[0]) # No entries given mode of Electrical\ncombined['KitchenQual'] = combined['KitchenQual'].fillna(combined['KitchenQual'].mode()[0]) # No entries given mode of KitchenQual\ncombined['Exterior1st'] = combined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0]) # No entries given mode of Exterior1st\ncombined['Exterior2nd'] = combined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0]) # No entries given mode of Exterior2nd\ncombined['SaleType'] = combined['SaleType'].fillna(combined['SaleType'].mode()[0]) # No entries given mode of SaleType","475cbdfe":"# Apply strings to missing values\ncombined['Functional'] = combined['Functional'].fillna('Typ') # No entries treated as having 'Typ'","f6bfbd57":"# Drop features for remaining features\ncombined.drop(['Utilities'], axis=1, inplace=True)","fe0695c3":"missing = combined.isnull().sum()\nmissing = missing[missing != 0].sort_values(ascending=False)\nmissing","f10673ba":"combined['MSSubClass'] = combined['MSSubClass'].apply(str)","66372c28":"# Decided to leave out YrSold and OverallCond from LabelEncoder\n\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'MoSold')\n\nfor c in cols:\n    le = LabelEncoder() \n    le.fit(list(combined[c].values)) \n    combined[c] = le.transform(list(combined[c].values))","cb02d069":"combined.shape","1ae11b2e":"# Credit to Kaggle Kernal: Reach Top 10% With Simple Model On Housing Prices\n\nnumeric_feats = (combined.dtypes[combined.dtypes != \"object\"]).index\n\n# Check the skew of all numerical features\nskewed_feats = combined[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(15)","ea2054c9":"# Credit to Kaggle Kernal: Reach Top 10% With Simple Model On Housing Prices\n\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #combined[feat] += 1\n    combined[feat] = boxcox1p(combined[feat], lam)","6abc04e9":"combined = pd.get_dummies(combined)\ncombined.shape","ba240e5b":"train = combined[:ntrain]\ntest = combined[ntrain:]","883be61d":"from sklearn.linear_model import ElasticNet, ElasticNetCV, Lasso, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error","4f7b366c":"# Credit to Kaggle Kernal: Reach Top 10% With Simple Model On Housing Prices\n\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","23fc881e":"# Credit to Kaggle Kernal: Reach Top 10% With Simple Model On Housing Prices\n\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","63312d35":"# Credit to Kaggle Kernal: Reach Top 10% With Simple Model On Housing Prices\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = rmsle_cv(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7caec4b6":"# Credit to Kaggle Kernal: Reach Top 10% With Simple Model On Housing Prices\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","22f52776":"# Credit to Kaggle Kernal: Reach Top 10% With Simple Model On Housing Prices\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","47524ffc":"RFR = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100)\nRFR.fit(train.values, y_train)  \nscore = rmsle_cv(RFR)\nprint(\"Random Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3df3b7d4":"LassoMd = lasso.fit(train.values,y_train)\nENetMd = ENet.fit(train.values,y_train)\nKRRMd = KRR.fit(train.values,y_train)\nGBoostMd = GBoost.fit(train.values,y_train)","4ce08f91":"finalMd = (np.expm1(LassoMd.predict(test.values)) + np.expm1(ENetMd.predict(test.values)) + np.expm1(KRRMd.predict(test.values)) + np.expm1(GBoostMd.predict(test.values))) \/ 4\nfinalMd","e8e7dc4f":"submission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = finalMd\nsubmission.to_csv('submission.csv', index=False)","3c8ba410":"# Transformations","f9228299":"## Cross Validation","71699642":"## Target Transformation\n### Skew","9ec706b5":"## Feature Re-typing","dac59897":"## Feature Skew","5e7a36f5":"## Outliers","d5b5bb12":"# Modelling","92f0b57d":"## Missing Data"}}