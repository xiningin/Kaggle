{"cell_type":{"76e4be1c":"code","1930b082":"code","ea75e954":"code","a87978c7":"code","855caa1d":"code","00718bf7":"code","ba4c80b2":"code","4122eee2":"code","c6ecb89a":"code","7976a99a":"code","229914d6":"code","067c1c7b":"code","e07f4fea":"code","03201ce9":"code","e30d53a1":"code","93433817":"code","777a9a4c":"code","c38786fe":"code","a6828654":"code","97a506dc":"code","7eec95e6":"code","c59e565d":"code","3b395727":"code","f09765be":"code","48f6f060":"code","5f5c1642":"code","3f99c750":"code","1e401590":"code","a2cfad1c":"code","9db2c7aa":"code","375ba517":"code","a9eedea2":"markdown","3a15cc51":"markdown","0909138e":"markdown","71f64ed6":"markdown","74d14b88":"markdown","d5898bbf":"markdown","95b9e48a":"markdown","47f8789f":"markdown","7b9032c0":"markdown","7c3d8f6f":"markdown","0eb08f21":"markdown","4d33b392":"markdown","92604ff8":"markdown","1d55b2ac":"markdown","c8d26f07":"markdown","77c4591e":"markdown","0e504ccb":"markdown","d1992230":"markdown","888280f3":"markdown","2e7034dc":"markdown","f685f98b":"markdown","d344f0fd":"markdown","60c31d1e":"markdown","b96783ed":"markdown","19320ed2":"markdown","5f17abbd":"markdown","3d9fb4cd":"markdown","c1bb0abf":"markdown","e49ae1ba":"markdown","b55b5517":"markdown","48b2c6f7":"markdown","f24eaa9f":"markdown","49b9885c":"markdown"},"source":{"76e4be1c":"!pip install -q efficientnet\n!pip install -q tf-explain","1930b082":"import os\nimport cv2\nimport time\nimport numpy as np\nimport pandas as pd\nfrom datetime import date\n\nfrom scipy.stats import rankdata\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom kaggle_datasets import KaggleDatasets\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Dropout\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import optimizers\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nfrom tf_explain.core.grad_cam import GradCAM\n\n# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n# policy = mixed_precision.Policy('mixed_bfloat16')\n# mixed_precision.set_policy(policy)\n\nimport efficientnet.tfkeras as efn","ea75e954":"SEED = 2020\n\nLABEL_SMOOTHING = 0.05\nWEIGHT_DECAY = 0\n\nIMG_R = 256\nIMG_C = 250\nIMG_N = 224\n\nROT = 180.0\nSHR = 2.0\nHZOOM = 6.0\nWZOOM = 6.0\nHSHIFT = 8.0\nWSHIFT = 8.0\n\nR, C = 5,5","a87978c7":"DATA_PATH = '..\/input\/siim-isic-melanoma-classification'\nMODEL_PATH = '..\/input\/melanoma-efficientnet-b6-tpu-tta-saved-models'\nTRAIN_IMG_PATH = '..\/input\/siim-isic-melanoma-classification\/jpeg\/train'\nTEST_IMG_PATH = '..\/input\/siim-isic-melanoma-classification\/jpeg\/test'\n\ntrain_df = pd.read_csv(os.path.join(DATA_PATH,'train.csv'))\ntest_df = pd.read_csv(os.path.join(DATA_PATH,'test.csv'))\n\nsgkf = pd.read_csv('..\/input\/siimisic-stratified-group-kfold-traindata\/train_StratifiedGroupK(5)Fold(SEED2020)(Group_sex_anatomsite_target).csv')\n\nval_pred = pd.read_csv(f'{MODEL_PATH}\/{[filename for filename in os.listdir(MODEL_PATH) if \"VALPREDS\" in filename][0]}')\n\ntrain_pred = pd.read_csv(f'{MODEL_PATH}\/{[filename for filename in os.listdir(MODEL_PATH) if \"TRAINPREDS\" in filename][0]}')\ntrain_logs = pd.read_csv(f'{MODEL_PATH}\/{[filename for filename in os.listdir(MODEL_PATH) if \"TRAINING_LOGS\" in filename][0]}')\n\ntest_pred = pd.read_csv(f'{MODEL_PATH}\/{[filename for filename in os.listdir(MODEL_PATH) if \"TESTPREDS_2\" in filename][0]}')\ntest_pred_mean = pd.read_csv(f'{MODEL_PATH}\/{[filename for filename in os.listdir(MODEL_PATH) if \"TESTPREDS_MEAN\" in filename][0]}')\ntest_pred_noaug = pd.read_csv(f'{MODEL_PATH}\/{[filename for filename in os.listdir(MODEL_PATH) if \"TESTPREDS_NOAUG\" in filename][0]}')\ntest_pred_rank = pd.read_csv(f'{MODEL_PATH}\/{[filename for filename in os.listdir(MODEL_PATH) if \"TESTPREDS_RANK\" in filename][0]}')\n\nFOLD = int(os.listdir(MODEL_PATH)[0].split(\"_\")[-1][0])\nprint(\"Fold: \",FOLD)\n\nEFNMODEL = [filename for filename in os.listdir(MODEL_PATH) if 'hdf5' in filename][0].split(\"_\")[2][-1]\nprint(\"EfficientNet Model Num: \",EFNMODEL)\n\nval_merged = sgkf[sgkf.fold==FOLD].merge(val_pred,on='image_name')\ntrain_merged = sgkf[sgkf.fold!=FOLD].merge(train_pred,on='image_name')","855caa1d":"modelName = f'EfficientNetB{EFNMODEL}'\n\nMODEL_WEIGHTS = [filename for filename in os.listdir(MODEL_PATH) if 'hdf5' in filename][0]\n\n# model_input = tf.keras.Input(shape=(IMG_N, IMG_N, 3), name='imgInput')\n    \nconstructor = getattr(efn, modelName)\n\nbase_model = constructor(include_top=False,\n                         weights='imagenet',\n                         input_shape=(IMG_N, IMG_N, 3),\n                         pooling='avg')\n\noutput = tf.keras.layers.Dense(1, activation='sigmoid',dtype='float32')(base_model.output)\n\nmodel = tf.keras.Model(base_model.input, output, name=modelName)\n        \n# model.compile(\n#     optimizer='adam',\n#     loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING),\n#     metrics=[\n#         'binary_accuracy',\n#         tf.keras.metrics.AUC(name='auc'),\n#         tf.keras.metrics.Recall(name='recall'),\n#         tf.keras.metrics.TruePositives(name=\"TP\"),\n#         tf.keras.metrics.FalseNegatives(name=\"FN\"),\n#         tf.keras.metrics.FalsePositives(name=\"FP\")\n#     ]\n# )\n\nmodel.summary()\n\nmodel.load_weights(f'{MODEL_PATH}\/{MODEL_WEIGHTS}')","00718bf7":"def getImage(image_name,train=True):\n    PATH = TRAIN_IMG_PATH if train else TEST_IMG_PATH\n    \n    image = cv2.imread(f'{PATH}\/{image_name}.jpg')\n    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    \n    # Preprocessing\n    image = cv2.resize(image,(IMG_R,IMG_R))\n    image = tf.image.central_crop(image,IMG_C\/IMG_R).numpy()\n    image = cv2.resize(image,(IMG_N,IMG_N))\n    \n#     print(image)\n    \n    return image\n\ndef overlay_heatmap(heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_JET):\n    # apply the supplied color map to the heatmap and then\n    # overlay the heatmap on the input image\n    heatmap = cv2.applyColorMap(heatmap, colormap)\n    output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n    # return a 2-tuple of the color mapped heatmap and the output,\n    # overlaid image\n    return heatmap,output","ba4c80b2":"fig, ax = plt.subplots(2,2,figsize=(20,15))\n\nval_merged['ranks'] = rankdata(val_merged.target_y.values)\/len(val_merged.target_y.values)\ntrain_merged['ranks'] = rankdata(train_merged.target_y.values)\/len(train_merged.target_y.values)\n\n\n# sns.kdeplot(val_merged[val_merged.target_x==1].ranks,label='malignant(Rank wise)',shade=True,ax=ax[0,0])\n# sns.kdeplot(val_merged[val_merged.target_x==0].ranks,label='beingn(Rank wise)',shade=True,ax=ax[0,0])\n# sns.kdeplot(val_merged.target_y,label='Combined',shade=True,ax=ax[0,0])\nsns.kdeplot(val_merged[val_merged.target_x==1].target_y,label='malignant',shade=True,ax=ax[0,0])\nsns.kdeplot(val_merged[val_merged.target_x==0].target_y,label='benign',shade=True,ax=ax[0,0])\nax[0,0].set_title(f'Distribution of Validation set Predictions (FOLD:{FOLD})')\n\n# sns.kdeplot(train_merged.target_y,label='Combined',shade=True,ax=ax[0,1])\nsns.kdeplot(train_merged[train_merged.target_x==1].target_y,label='malignant',shade=True,ax=ax[0,1])\nsns.kdeplot(train_merged[train_merged.target_x==0].target_y,label='benign',shade=True,ax=ax[0,1])\nax[0,1].set_title(f'Distribution of Training set Predictions (FOLD:{FOLD})')\n\nfpr, tpr, thresholds = roc_curve(val_merged.target_x, val_merged.target_y)\nroc_auc = roc_auc_score(val_merged.target_x, val_merged.target_y)\nax[1,0].plot(fpr,tpr,linestyle='--')\nax[1,0].set_title(f\"ROC Curve of Validation set predictions (AUC: {roc_auc})\")\n\nfpr, tpr, thresholds = roc_curve(train_merged.target_x, train_merged.target_y)\nroc_auc = roc_auc_score(train_merged.target_x, train_merged.target_y)\nax[1,1].plot(fpr,tpr,linestyle='--')\nax[1,1].set_title(f\"ROC Curve of Training set predictions (AUC: {roc_auc})\")","4122eee2":"fig, ax = plt.subplots(1,3,figsize=(20,5))\n\nsns.kdeplot(test_pred_mean.target,shade=True,ax=ax[0])\nax[0].set_title(f'Distribution of Test set (TTA-Mean) Predictions')\n\nsns.kdeplot(test_pred_noaug.target,shade=True,ax=ax[1])\nax[1].set_title(f'Distribution of Test set (No-AUG) Predictions')\n\nsns.kdeplot(test_pred_rank.target,shade=True,ax=ax[2])\nax[2].set_title(f'Distribution of Test set (TTA-Mean by Ranks) Predictions')","c6ecb89a":"def displayImages(df,target):\n    \n    image_names = list(set(df.image_name).intersection(set(df[df.target_x==target].image_name)))\n    print(\"Total Images: \",len(image_names))\n    \n    r, c = min(int(np.ceil(len(image_names)\/R)),R), C\n    \n    if r==1:\n        r=2\n    \n    fig, ax = plt.subplots(r,c,figsize=(c*4*2,r*6))\n\n    layer_name = 'top_conv'\n\n    # fig.suptitle(f'GridCAMs of layer: {layer_name} of train images',fontsize=15)\n\n    explainer = GradCAM()\n\n    i = 0\n\n    for image_name in np.random.choice(image_names,replace=False,size=min(r*c,len(image_names))):\n\n        prediction = df[df.image_name==image_name]['target_y'].values[0]\n        actual = df[df.image_name==image_name]['target_x'].values[0]\n\n        color = 'green' if int(prediction>=0.5)==actual else 'red'\n\n        title = f'Image Name: {image_name}\\nActual: {actual}\\nPrediction: {prediction}'\n\n        image = getImage(image_name)\n\n        heatmap1 = explainer.explain(([image\/255.0], None), model, layer_name = layer_name, class_index=0)\n\n        heatmap2, output = overlay_heatmap(heatmap1,image)\n\n        output = np.hstack([image,output])\n\n        ax[i\/\/c,i%c].imshow(output)\n        ax[i\/\/c,i%c].axis('off')\n        ax[i\/\/c,i%c].set_title(title,fontsize=15,color=color)\n\n        i += 1","7976a99a":"displayImages(train_merged,1)","229914d6":"displayImages(train_merged,0)","067c1c7b":"displayImages(val_merged,1)","e07f4fea":"displayImages(val_merged,0)","03201ce9":"layer_name = 'top_conv'\nexplainer = GradCAM()\n\npatient_id = np.random.choice(val_merged.patient_id.unique(),size=1,replace=False)[0]\n\nx = val_merged[val_merged.patient_id == patient_id].sort_values(['age_approx','anatom_site_general_challenge','image_name'])\n\nr, c = int(np.ceil(x.shape[0]\/5)), 5\n\nfig, ax = plt.subplots(r,c, figsize=(c*4*2,6*r))\n\n# fig = fig.suptitle(f'{patient_id} Sex: {x.sex.values[0]}',fontsize=20)\nprint(f'{patient_id} Sex: {x.sex.values[0]}')\n\nfor i, image_name in enumerate(x.image_name.values):\n    \n    prediction = val_pred[val_pred.image_name==image_name]['target'].values[0]\n    actual = train_df[train_df.image_name==image_name]['target'].values[0]\n    age = train_df[train_df.image_name==image_name]['age_approx'].values[0]\n    site = train_df[train_df.image_name==image_name]['anatom_site_general_challenge'].values[0]\n    \n    color = 'green' if int(prediction>=0.5)==actual else 'red'\n    \n    title = f'Image Name: {image_name}\\nActual: {actual}\\nPrediction: {prediction}\\nAge:{age}\\nAnatom site:{site}'\n    \n    image = getImage(image_name)\n    \n    heatmap1 = explainer.explain(([image\/255.0], None), model, layer_name = layer_name, class_index=0)\n    \n    heatmap2, output = overlay_heatmap(heatmap1,image)\n    \n    output = np.hstack([image,output])\n    \n    if r>1:\n        ax[i\/\/c,i%c].imshow(output)\n        ax[i\/\/c,i%c].set_title(title,color=color)\n        ax[i\/\/c,i%c].axis('off')\n    else:\n        ax[i%c].imshow(output)\n        ax[i%c].set_title(title,color=color)\n        ax[i%c].axis('off')\n\n# plt.savefig(f'{x.target.sum()}_{patient_id}_{x.sex.values[0]}.png')\n\nplt.show()","e30d53a1":"val_merged.anatom_site_general_challenge.value_counts()","93433817":"def displayAnatomsite(df,site,target,layer_name = 'top_conv'):\n    image_names = list(df[(df.target_x==target) & (df.anatom_site_general_challenge==site)].image_name)\n    print(\"Total Images: \",len(image_names))\n    \n    if len(image_names)==0:\n        return\n    \n    r, c = 5, 5\n    \n    r, c = min(int(np.ceil(len(image_names)\/5)),5), 5\n    if r==1:\n        r=2\n\n    fig, ax = plt.subplots(r,c,figsize=(c*4*2,r*6))\n\n    # fig.suptitle(f'GridCAMs of layer: {layer_name} of train images',fontsize=15)\n\n    explainer = GradCAM()\n\n    i = 0\n\n    for image_name in np.random.choice(image_names,replace=False,size=min(r*c,len(image_names))):\n\n        prediction = df[df.image_name==image_name]['target_y'].values[0]\n        actual = df[df.image_name==image_name]['target_x'].values[0]\n\n        color = 'green' if int(prediction>=0.5)==actual else 'red'\n\n        title = f'Image Name: {image_name}\\nActual: {actual}\\nPrediction: {prediction}'\n\n        image = getImage(image_name)\n\n        heatmap1 = explainer.explain(([image\/255.0], None), model, layer_name = layer_name, class_index=0)\n\n        heatmap2, output = overlay_heatmap(heatmap1,image)\n\n        output = np.hstack([image,output])\n\n        ax[i\/\/c,i%c].imshow(output)\n        ax[i\/\/c,i%c].axis('off')\n        ax[i\/\/c,i%c].set_title(title,fontsize=15,color=color)\n\n        i += 1","777a9a4c":"displayAnatomsite(val_merged,'torso',1)","c38786fe":"displayAnatomsite(val_merged,'torso',0)","a6828654":"displayAnatomsite(val_merged,'lower extremity',1)","97a506dc":"displayAnatomsite(val_merged,'lower extremity',0)","7eec95e6":"displayAnatomsite(val_merged,'upper extremity',1)","c59e565d":"displayAnatomsite(val_merged,'upper extremity',0)","3b395727":"displayAnatomsite(val_merged,'head\/neck',1)","f09765be":"displayAnatomsite(val_merged,'head\/neck',0)","48f6f060":"displayAnatomsite(val_merged,'palms\/soles',1)","5f5c1642":"displayAnatomsite(val_merged,'palms\/soles',0)","3f99c750":"displayAnatomsite(val_merged,'oral\/genital',1)","1e401590":"displayAnatomsite(val_merged,'oral\/genital',0)","a2cfad1c":"layer_name = 'top_conv'\n\nimage_names = list(val_merged[(val_merged.target_x==1)&(val_merged.target_y<0.5)].image_name)\nprint(\"Total Images: \",len(image_names))\n\nif len(image_name)!=0:\n\n    r, c = 5, 5\n\n    r, c = min(int(np.ceil(len(image_names)\/5)),5), 5\n    if r==1:\n        r=2\n\n    fig, ax = plt.subplots(r,c,figsize=(c*4*2,r*6))\n\n    # fig.suptitle(f'GridCAMs of layer: {layer_name} of train images',fontsize=15)\n\n    explainer = GradCAM()\n\n    i = 0\n\n    for image_name in np.random.choice(image_names,replace=False,size=min(r*c,len(image_names))):\n\n        prediction = val_merged[val_merged.image_name==image_name]['target_y'].values[0]\n        actual = val_merged[val_merged.image_name==image_name]['target_x'].values[0]\n\n        color = 'green' if int(prediction>=0.5)==actual else 'red'\n\n        title = f'Image Name: {image_name}\\nActual: {actual}\\nPrediction: {prediction}'\n\n        image = getImage(image_name)\n\n        heatmap1 = explainer.explain(([image\/255.0], None), model, layer_name = layer_name, class_index=0)\n\n        heatmap2, output = overlay_heatmap(heatmap1,image)\n\n        output = np.hstack([image,output])\n\n        ax[i\/\/c,i%c].imshow(output)\n        ax[i\/\/c,i%c].axis('off')\n        ax[i\/\/c,i%c].set_title(title,fontsize=15,color=color)\n\n        i += 1","9db2c7aa":"layer_name = 'top_conv'\n\nimage_names = list(val_merged[(val_merged.target_x==0)&(val_merged.target_y>=0.5)].image_name)\nprint(\"Total Images: \",len(image_names))\n\nif len(image_name)!=0:\n    \n    r, c = 5, 5\n\n    r, c = min(int(np.ceil(len(image_names)\/5)),5), 5\n    if r==1:\n        r=2\n\n    fig, ax = plt.subplots(r,c,figsize=(c*4*2,r*6))\n\n    # fig.suptitle(f'GridCAMs of layer: {layer_name} of train images',fontsize=15)\n\n    explainer = GradCAM()\n\n    i = 0\n\n    for image_name in np.random.choice(image_names,replace=False,size=min(r*c,len(image_names))):\n\n        prediction = val_merged[val_merged.image_name==image_name]['target_y'].values[0]\n        actual = val_merged[val_merged.image_name==image_name]['target_x'].values[0]\n\n        color = 'green' if int(prediction>=0.5)==actual else 'red'\n\n        title = f'Image Name: {image_name}\\nActual: {actual}\\nPrediction: {prediction}'\n\n        image = getImage(image_name)\n\n        heatmap1 = explainer.explain(([image\/255.0], None), model, layer_name = layer_name, class_index=0)\n\n        heatmap2, output = overlay_heatmap(heatmap1,image)\n\n        output = np.hstack([image,output])\n\n        ax[i\/\/c,i%c].imshow(output)\n        ax[i\/\/c,i%c].axis('off')\n        ax[i\/\/c,i%c].set_title(title,fontsize=15,color=color)\n\n        i += 1","375ba517":"image_names = test_pred_mean.image_name.values\n\nr, c = min(int(np.ceil(len(image_names)\/R)),R), C\n    \nif r==1:\n    r=2\n\nfig, ax = plt.subplots(r,c,figsize=(c*4*2,r*6))\n\nlayer_name = 'top_conv'\n\n# fig.suptitle(f'GridCAMs of layer: {layer_name} of train images',fontsize=15)\n\nexplainer = GradCAM()\n\ni = 0\n\nfor image_name in np.random.choice(image_names,replace=False,size=min(r*c,len(image_names))):\n\n    prediction = test_pred_mean[test_pred_mean.image_name==image_name]['target'].values[0]\n    rank = test_pred_rank[test_pred_rank.image_name==image_name]['target'].values[0]\n\n    title = f'Image Name: {image_name}\\nRank: {rank}\\nPrediction: {prediction}'\n\n    image = getImage(image_name,train=False)\n\n    heatmap1 = explainer.explain(([image\/255.0], None), model, layer_name = layer_name, class_index=0)\n\n    heatmap2, output = overlay_heatmap(heatmap1,image)\n\n    output = np.hstack([image,output])\n\n    ax[i\/\/c,i%c].imshow(output)\n    ax[i\/\/c,i%c].axis('off')\n    ax[i\/\/c,i%c].set_title(title,fontsize=15)\n\n    i += 1","a9eedea2":"### GridCAMs of TP + FN in Validation Images","3a15cc51":"#### Lower Extremity (TN + FP)","0909138e":"## Necessary Functions","71f64ed6":"### GridCAMs of Testset Images","74d14b88":"## Model to Interpret","d5898bbf":"## Introduction\n\n---\n\n![](https:\/\/miro.medium.com\/max\/10096\/1*B7T0sSVNPCNtdpyMkA7lGA.jpeg)\n\n---\n\n### TF-explain\n\n[tf-explain](https:\/\/tf-explain.readthedocs.io\/en\/latest\/) offers interpretability methods for Tensorflow 2.0 to ease neural network\u2019s understanding. With either its core API or its tf.keras callbacks, you can get a feedback on the training of your models.\n\nWe will use **Core API** in this notebook to visualize **What our Model is seeing in the Image to take the decision.**\n\n*(You can use it as a Callback during model training)*\n\n\n### GradCams\n\n---\n\n**Gradient-weighted Class Activation Mapping(Grad-CAM)** is a method that extracts gradients from a convolutional neural network's final convolutional layer(mostly) and uses this information to highlight regions most responsible for the predicted probability the image belongs to a predefined class.\n\nThe steps of Grad-CAM include extracting the gradients with subsequent global average pooling. A ReLU activation function is added to only depict the regions of the image that have a positive contribution to the predefined class. The resulting attention map can be plotted over the original image and can be interpreted as a visual tool for identifying regions the model \u2018looks at\u2019 to predict if an image belongs to a specific class. Readers interested in the mathematical theory behind Grad-CAM are encouraged to read the paper by Selvaraju et al. via https:\/\/arxiv.org\/abs\/1610.02391.\n\nThis notebook demonstrates the relative ease of implementing Grad-CAM with our basic model using **tf-explain**.\n\n---\n\n<font size=5 color='red'>Please Upvote, If you find it useful!!<\/font>","95b9e48a":"## Helpful Resources\n\n---\n\n* [tf-explain docs](https:\/\/tf-explain.readthedocs.io\/en\/latest\/)\n* [Demystifying Convolutional Neural Networks using GradCam](https:\/\/towardsdatascience.com\/demystifying-convolutional-neural-networks-using-gradcam-554a85dd4e48)\n* [Grad-CAM class activation visualization | keras.io](https:\/\/keras.io\/examples\/vision\/grad_cam\/)\n* [Grad-CAM: Visualize class activation maps with Keras, TensorFlow, and Deep Learning](https:\/\/www.pyimagesearch.com\/2020\/03\/09\/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning\/)\n* [Introducing tf-explain, Interpretability for TensorFlow 2.0](https:\/\/www.sicara.ai\/blog\/2019-07-31-tf-explain-interpretability-tensorflow)\n* [Interpreting Tensorflow models with tf-explain](https:\/\/gilberttanner.com\/blog\/interpreting-tensorflow-model-with-tf-explain)","47f8789f":"### GridCAMs of TP + FN of Training Images","7b9032c0":"### If you observe something useful, please mention it in the comment section!\n*(I have used 0.5 as the threshold value to decide TP,FP,TN,FN. Which is not the ideal way as the evaluation metric is AUC)*\n\n<font size=5 color='red'>Please Upvote, If you find it useful!!<\/font>","7c3d8f6f":"#### Lower Extremity (TP + FN)","0eb08f21":"## Hyperparameters used to train the Model","4d33b392":"#### Oral\/Genital (TN + FP)","92604ff8":"### GridCAMs of Different Anatom Sites from Validation-set","1d55b2ac":"#### Palms\/Soles (TN + FP)","c8d26f07":"### GridCAMs of TN + FP in Validation Images","77c4591e":"#### Head\/Neck (TN + FP)","0e504ccb":"#### False Negatives in Validation set","d1992230":"## Necessary Libraries","888280f3":"#### Head\/Neck (TP + FN)","2e7034dc":"#### Torso (TN + FP)","f685f98b":"#### Oral\/Genital (TP + FN)","d344f0fd":"### Distributions of Predictions","60c31d1e":"### GridCAMs of Benign Predictions(TN + FP) of Training Images","b96783ed":"#### Torso (TP+FN)","19320ed2":"## Data Loading","5f17abbd":"#### Palms\/Soles (TP + FN)","3d9fb4cd":"## \ud83d\udca1 Key Takeaways\n\n---\n\n* We can see for many images **Hairs** are influencing the prediction.\n* Model is focusing on **Dark Corners** in the images. *(Maybe cropping the image(random or center) will help the model to focus on central region more)*","c1bb0abf":"#### False Positives in Validation set","e49ae1ba":"### GridCAMs of a Random Patient from Validation-set","b55b5517":"## Model Interpretation","48b2c6f7":"#### Upper Extremity (TN + FP)","f24eaa9f":"#### Upper Extremity (TP + FN)\n","49b9885c":"### Mistakes made by the Model"}}