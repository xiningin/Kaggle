{"cell_type":{"8b9c3bbc":"code","5206dbc1":"code","fb5155b4":"code","aab1b279":"code","0a80ff50":"code","a54a2276":"code","13801e92":"code","09180c98":"code","1bf93a29":"code","f16a8d3d":"code","4c75485b":"code","a6f1eb32":"code","7efd269d":"code","d66c9819":"code","286e0a5c":"code","b422dcbf":"code","21f5e701":"code","b3c21dab":"code","582ba1b5":"markdown","c14f488f":"markdown","b4165818":"markdown","6b24bf5e":"markdown"},"source":{"8b9c3bbc":"from IPython.display import Image\nImage(filename='..\/input\/sign-language-mnist\/amer_sign2.png')","5206dbc1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns","fb5155b4":"df_tr = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_train.csv')\ndf_tr.head()","aab1b279":"# load test set\ndf_te = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_test.csv')\n\n# Create train\/test\n\nX_test = df_te.iloc[:,1:].values\ny_test = df_te[['label']].values\n\nprint('X_tr shape', X_test.shape, X_test.dtype)\nprint('y_te shape', y_test.shape, y_test.dtype)\n\nX_test = (X_test - 128)\/255","0a80ff50":"def show_img(img, df):\n    \n    # Take the label\n    label = df['label'][img]\n    \n    # Take the pixels\n    pixels = df.iloc[img, 1:]\n\n    # The pixel intensity values are integers from 0 to 255\n    pixels = np.array(pixels, dtype='uint8')\n\n    # Reshape the array into 28 x 28 array (2-dimensional array)\n    pixels = pixels.reshape((28, 28))\n\n    # Plot\n    plt.title('Label is {label}'.format(label=label))\n    plt.imshow(pixels, cmap='gray')\n    plt.show()\n","a54a2276":"show_img(90, df_tr)","13801e92":"list_data = [df_tr, df_te]\nfig,axes = plt.subplots(nrows=1,ncols=2,figsize=(15,6))\n\nfor data, ax, names in zip(list_data, axes.ravel(), ['train', 'test']):\n    sns.countplot(data['label'], palette='rocket', ax=ax)\n    ax.set_title(\"Frequency for each letter in the {} dataset\".format(names))\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_xticklabels(['A','B','C','D','E','F','G','H','I','K','L','M','N','O','P','Q','R','S',\n                            'T','U','V','W','X','Y'])\n\nplt.tight_layout()","09180c98":"# Create train\/test\n\nX = df_tr.iloc[:,1:].values\ny = df_tr[['label']].values\n\nfrom sklearn.model_selection import train_test_split\n\nX_tr, X_v, y_tr, y_v = train_test_split(X, y, test_size=0.2, random_state=14)\n\nprint('X_tr shape', X_tr.shape, X_tr.dtype)\nprint('X_v shape', X_v.shape, X_v.dtype)\nprint('y_tr shape', y_tr.shape, y_tr.dtype)\nprint('y_v shape', y_v.shape, y_v.dtype)\n\nX_tr = (X_tr - 128)\/255\nX_v = (X_v - 128)\/255","1bf93a29":"# X_tr and y_tr to right shape for CNN\n\ntrain_x = X_tr.reshape(-1,28,28,1) \ntrain_y = y_tr.reshape(-1,1) \n\n# val_x and val_y\nval_x = X_v.reshape(-1,28,28,1)\nval_y = y_v.reshape(-1,1)\n\nX_test = X_test.reshape(-1, 28, 28, 1)\n\nprint(train_x.shape)\nprint(val_x.shape)","f16a8d3d":"from sklearn.preprocessing import LabelBinarizer\n\nlb=LabelBinarizer()\ny_tr= lb.fit_transform(y_tr)\ny_v= lb.fit_transform(y_v)\ny_test= lb.fit_transform(y_test)","4c75485b":"# With data augmentation to prevent overfitting\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,  \n        rotation_range=10,  \n        zoom_range = 0.1, \n        width_shift_range=0.1, \n        height_shift_range=0.1,  \n        horizontal_flip=False, \n        vertical_flip=False)  \n\n\ndatagen.fit(train_x)","a6f1eb32":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Convolutional network \nmodel = keras.Sequential()\n\nmodel.add(keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, activation='relu', input_shape=(28, 28, 1)))\nmodel.add(keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, activation='relu'))\n\nmodel.add(keras.layers.Conv2D(filters=64, kernel_size=2, strides=1, activation='relu'))\nmodel.add(keras.layers.Conv2D(filters=64, kernel_size=2, strides=1, activation='relu'))\nmodel.add(keras.layers.MaxPool2D(pool_size=2))\n\nmodel.add(keras.layers.Conv2D(filters=128, kernel_size=2, strides=1, activation='relu'))\nmodel.add(keras.layers.Conv2D(filters=128, kernel_size=2, strides=1, activation='relu'))\nmodel.add(keras.layers.MaxPool2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(keras.layers.Conv2D(filters=256, kernel_size=2, strides=1, activation='relu'))\nmodel.add(keras.layers.Conv2D(filters=256, kernel_size=2, strides=1, activation='relu'))\nmodel.add(keras.layers.MaxPool2D(pool_size=2))\n\nmodel.add(keras.layers.Flatten())\n\nmodel.add(keras.layers.Dense(512,activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(keras.layers.Dense(256,activation='relu'))\n\nmodel.add(keras.layers.Dense(24, activation='softmax'))\n\nprint(model.summary())\n\n# Compile the model\nmodel.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc'])\n\n\nearly_stopping = [EarlyStopping(patience=3, monitor='val_loss'), ReduceLROnPlateau(patience=2), \n                  ModelCheckpoint(filepath='ASL_MNIST_CNN_temp.h5', save_best_only=True)]","7efd269d":"history = model.fit(datagen.flow(train_x, y_tr, batch_size = 250), epochs = 100, validation_data = (val_x, y_v) , callbacks = early_stopping)\n\nmodel.save('ASL_MNIST_CNN.h5') # Saves architecture and weights\nprint('Model Saved')","d66c9819":"test_loss, test_acurracy = model.evaluate(X_test, y_test)\nprint('Test loss: {:.2f}, accuracy: {:.2f}%'.format(test_loss, test_acurracy*100))","286e0a5c":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create two plots: one for the loss value, one for the accuracy\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n\n# Plot accuracy values\nax1.plot(history.history['loss'], label='train loss')\nax1.plot(history.history['val_loss'], label='val loss')\nax1.set_title('Validation loss {:.3f} (mean last 3)'.format(\n    np.mean(history.history['val_loss'][-3:]) # last three values\n))\nax1.set_xlabel('epoch')\nax1.set_ylabel('loss value')\nax1.legend()\n\n# Plot accuracy values\nax2.plot(history.history['acc'], label='train acc')\nax2.plot(history.history['val_acc'], label='val acc')\nax2.set_title('Validation accuracy {:.3f} (mean last 3)'.format(\n    np.mean(history.history['val_acc'][-3:]) # last three values\n))\nax2.set_xlabel('epoch')\nax2.set_ylabel('accuracy')\nax2.legend()\nplt.show()","b422dcbf":"predictions = model.predict_classes(X_test)\npredictions\n# Remove one hot encoding from the target\ny_test_=np.argmax(y_test, axis=1)\ny_test_[1]","21f5e701":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test_, predictions, target_names = ['A','B','C','D','E','F','G','H','I','K','L','M','N','O','P','Q','R','S',\n                        'T','U','V','W','X','Y']))","b3c21dab":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nmatrix = confusion_matrix(y_true=y_test_, y_pred=predictions)\n\nplt.figure(figsize = (20,15))\nax = sns.heatmap(matrix,cmap= \"Blues\", linecolor = 'black' , linewidth = 0, annot = True, fmt='', xticklabels=['A','B','C','D','E','F','G','H','I','K','L','M','N','O','P','Q','R','S',\n                        'T','U','V','W','X','Y'], yticklabels=['A','B','C','D','E','F','G','H','I','K','L','M','N','O','P','Q','R','S',\n                        'T','U','V','W','X','Y']);\nax.set(xlabel='Classified as', ylabel='True label')\nplt.show()","582ba1b5":"# Quick EDA","c14f488f":"# Sign Language MNIST Dataset\n\n## ASL\nAmerican Sign Language (ASL) is a complete, natural language that has the same linguistic properties as spoken languages, with grammar that differs from English. ASL is expressed by movements of the hands and face. It is the primary language of many North Americans who are deaf and hard of hearing, and is used by many hearing people as well.\n\n## The dataset\nThe original MNIST image dataset of handwritten digits is a popular benchmark for image-based machine learning methods but researchers have renewed efforts to update it and develop drop-in replacements that are more challenging for computer vision and original for real-world applications. As noted in one recent replacement called the Fashion-MNIST dataset, the Zalando researchers quoted the startling claim that \"Most pairs of MNIST digits (784 total pixels per sample) can be distinguished pretty well by just one pixel\". To stimulate the community to develop more drop-in replacements, the Sign Language MNIST is presented here and follows the same CSV format with labels and pixel values in single rows. The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (excluding J and Z which require motion).\n\nThe dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2\u2026.pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. The original hand gesture image data represented multiple users repeating the gesture against different backgrounds. The Sign Language MNIST data came from greatly extending the small number (1704) of the color images included as not cropped around the hand region of interest. To create new data, an image pipeline was used based on ImageMagick and included cropping to hands-only, gray-scaling, resizing, and then creating at least 50+ variations to enlarge the quantity.","b4165818":"--- \n\n# Loading the datasets ","6b24bf5e":"\n## Content of the notebook:\n\n* Loading dataset.\n* Creation and training of the Convolutional Neural Netwok\n"}}