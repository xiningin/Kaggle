{"cell_type":{"41dad529":"code","379bef91":"code","b49de752":"code","379f9ccd":"code","d3148915":"code","fbc9e6e5":"code","52e7e861":"code","a9c88e31":"code","4f62900a":"code","be07161a":"code","dda99531":"code","47978a6c":"code","0d290aee":"code","717a9fcd":"code","69f341da":"code","43a3a883":"code","b6a9ce4d":"code","7ba55e6a":"markdown","3dfa26ea":"markdown","f3a233a8":"markdown","8e976f0d":"markdown","c7b8c5f2":"markdown","5ca3449f":"markdown","4986bf0d":"markdown","74bc2c9c":"markdown","f5e0b68e":"markdown","e2f22d21":"markdown","5775f8a6":"markdown","dba2f319":"markdown","45605601":"markdown","eed7a9e8":"markdown","921b16aa":"markdown","27b0b6a0":"markdown","ac7a6479":"markdown","c49996ad":"markdown","160c2742":"markdown","b1153adb":"markdown","d33f6037":"markdown"},"source":{"41dad529":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","379bef91":"data = pd.read_csv('..\/input\/column_2C_weka.csv')\ndata.sample(5)","b49de752":"data.info()","379f9ccd":"# Investigate the correlation between the features:\ndata.corr()","d3148915":"# Visulization of the features:\ncolor_list = ['orange' if i=='Abnormal' else 'blue' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'], c=color_list, figsize= [12,12], diagonal='hist', alpha=0.3, s = 200, marker = '.')\nplt.show()","fbc9e6e5":"# Change classes to Abnormal = 1, Normal = 0:\ndata['class'] = [1 if each == 'Abnormal' else 0 for each in data['class']]\ndata.sample(5)","52e7e861":"y = data['class'].values\nx_data = data.drop(['class'], axis=1)","a9c88e31":"x = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))\nx.head()","4f62900a":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=1)","be07161a":"print('x_train shape: ', x_train.shape)\nprint('y_train shape: ', y_train.shape)\nprint('x_test shape: ', x_test.shape)\nprint('y_test shape: ', y_test.shape)","dda99531":"from sklearn.neighbors import KNeighborsClassifier\n\n# Creating model with the k value of 3\nknn = KNeighborsClassifier(n_neighbors = 3)\n\n# Training the model\nknn.fit(x_train, y_train)","47978a6c":"# Predicting our y values using our KNN model and x_test:\nprediction = knn.predict(x_test)\n\n# Comparing y_prediction and y_test values:\ndatashow = {'y_prediction': prediction, 'y_test': y_test}\nd_new = pd.DataFrame(datashow)\nd_new.T   # For the ease of reading I implemented transpose of our dataset.","0d290aee":"print('Score of the model for k=3: ', knn.score(x_test, y_test))","717a9fcd":"score_list=[]\nfor each in range(1,25):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2 = knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\nplt.figure(figsize=(10,5))\nplt.plot(range(1,25), score_list)\nplt.xlabel('k values range')\nplt.ylabel('Score values')\nplt.show()","69f341da":"# Retraining and predicting our dataset with the best k value (k=19):\nknn3 = KNeighborsClassifier(n_neighbors=19)\nknn3.fit(x_train,y_train)\ny_prediction = knn3.predict(x_test)\nprint('Score of the model for k=19: ', knn3.score(x_test, y_test))","43a3a883":"d = {'y_prediction': y_prediction, 'y_test': y_test}\ndata01 = pd.DataFrame(data=d)\ndata01.T","b6a9ce4d":"correct = 0\nfalse = 0\nfor each in range(1,len(data01)):\n    if data01.y_test[each] == data01.y_prediction[each]:\n        correct = correct + 1\n    else:\n        false = false + 1\n\nprint('correct predictions = ', correct)\nprint('false predictions = ', false)","7ba55e6a":"**14. Comparing Test and Prediction Values:**","3dfa26ea":"**6. Exclude y Values from Dataset**","f3a233a8":"**KNN Classification Model for Biomechanical Features Classification of Patients**\n\nKNN = K Neirest Neighbour\n\nSteps of KNN algorithm:\n1. Chosing a k value.\n2. Finding the k value of nearest neighbors to our data point (which we want to predict) \n3. In what classes do these neighbors belong to? \n4. Predict our data point according to dominance of the neighbor classification.\n\nEuclidean principle is used to find nearest neighbors:\neuclidean distance = square root((x2-x1)\u02c62 + (y2-y1)\u02c62)\n\nThe most important progress in KNN is normalization of the values (Other wise big difference between the data points' x and y values can not be seen easly).\n\nIn order to use sklearn library for KNN we can follow these steps:\n1. Import libraries\n2. Import dataset\n3. Clean and fill dataset\n4. Visualize dataset\n5. Change the classification to binary form (0 or 1)\n6. Exlude y values from the dataset\n7. Implement normalization\n8. Split the dataset to train and test\n9. Train the model\n10. Predict the x_test\n11. Analyse the score of our prediction\n12. Investigate to find the best k value possible","8e976f0d":"So we can see our model's accuracy is not great. Let's see if there are any possibility for a better model.","c7b8c5f2":"**1. Importing Libraries:**","5ca3449f":"**7. Implementing Normalization**","4986bf0d":"**10. Predicting the x_test values**","74bc2c9c":"**4. Visualizing Dataset:**","f5e0b68e":"**5. Changing Classes to Binary Form:**","e2f22d21":"Let's count how many of our predictions are correct and how many of them are false.","5775f8a6":"We don't have any NaN values. So we don't need to clean or fill the dataset.","dba2f319":"From the correlation one can see easyly pelvic_incidence and lumbar_lordosis_angel features are highly correlated (0.71).","45605601":"**8. Splitting Dataset into Train and Test**","eed7a9e8":"**2. Importing Dataset:**","921b16aa":"From the graph above we can see the best value for k is around 18-21.","27b0b6a0":"**9. Training KNN Model**","ac7a6479":"**3. Cleaning and Filling Dataset:**","c49996ad":"With the help of a better k value our model's accuracy is raised from 75% to 80%.","160c2742":"**12. Investigating the Best k Value:**","b1153adb":"**11. Analysing the Score of the Prediction**","d33f6037":"**13. Final model with the best k value:**"}}