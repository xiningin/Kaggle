{"cell_type":{"068a3078":"code","b878a6f8":"code","dc785685":"code","bb8a9504":"code","452f9358":"code","d750d671":"code","92dd2a2f":"code","6d4bc944":"code","cbc58f4b":"code","2c513268":"code","bb8a8e64":"code","96b63446":"code","8d48d354":"code","cda33117":"code","140a9229":"code","1678908c":"code","9e630d1c":"code","b1d02d96":"code","44a3b666":"code","6f227f2c":"code","89a8b359":"code","e7fd1b95":"markdown","3f3a354f":"markdown","df02d278":"markdown","15ca6220":"markdown","79a4cb66":"markdown","804f53d4":"markdown","4f20e4b5":"markdown","569cf771":"markdown","ab59e0b2":"markdown","5bbcbb49":"markdown","eed98ede":"markdown"},"source":{"068a3078":"class Config:\n    name_v1 = \"lstm\"\n    BATCH_SIZE = 512\n    EPOCHS = 300\n    n_fold = 3\n    n_a = 128\n    seeds = [2021]\n    target_col = \"pressure\"\n    debug = False\n    cv_shuffel = False","b878a6f8":"import os\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import RobustScaler\n\nfrom lightgbm import LGBMModel\nfrom matplotlib_venn import venn2\nfrom tqdm import tqdm","dc785685":"# import sys\n# import importlib\n# sys.path.append('..\/..\/useful_modules')\n\n# from logger_m import Logger\n# import cv_m\n# from reduce_memory_m import reduce_mem_usage\n# from utilities_m import Util","bb8a9504":"import os\nimport logging\nimport datetime\n\nclass Logger:\n    \"\"\"save log\"\"\"\n    def __init__(self, path):\n        self.general_logger = logging.getLogger(path)\n        stream_handler = logging.StreamHandler()\n        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n        if len(self.general_logger.handlers) == 0:\n            self.general_logger.addHandler(stream_handler)\n            self.general_logger.addHandler(file_general_handler)\n            self.general_logger.setLevel(logging.INFO)\n\n    def info(self, message):\n        # display time\n        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n\n    @staticmethod\n    def now_string():\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))","452f9358":"import numpy as np\nimport pandas as pd\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2 \n    dfs = []\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    dfs.append(df[col].astype(np.int8))\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    dfs.append(df[col].astype(np.int16))\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    dfs.append(df[col].astype(np.int32))\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    dfs.append(df[col].astype(np.int64) ) \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    dfs.append(df[col].astype(np.float16))\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    dfs.append(df[col].astype(np.float32))\n                else:\n                    dfs.append(df[col].astype(np.float64))\n        else:\n            dfs.append(df[col])\n    \n    df_out = pd.concat(dfs, axis=1)\n    if verbose:\n        end_mem = df_out.memory_usage().sum() \/ 1024**2\n        num_reduction = str(100 * (start_mem - end_mem) \/ start_mem)\n        print(f'Mem. usage decreased to {str(end_mem)[:3]}Mb:  {num_reduction[:2]}% reduction')\n    return df_out","d750d671":"import joblib\n\nclass Util:\n    \"\"\"save & load\"\"\"\n    @classmethod\n    def dump(cls, value, path):\n        joblib.dump(value, path, compress=True)\n\n    @classmethod\n    def load(cls, path):\n        return joblib.load(path)","92dd2a2f":"from sklearn import model_selection\nimport numpy as np\n\nclass GroupKFold_:\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X=None, y=None, group=None):\n        if self.shuffle == False:\n            self.random_state = None\n        \n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = group.unique()\n        for tr_group_idx, va_group_idx in kf.split(unique_ids):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(group.isin(tr_group))[0]\n            val_idx = np.where(group.isin(va_group))[0]\n            yield train_idx, val_idx # returns a generator","6d4bc944":"INPUT = \"..\/input\/ventilator-pressure-prediction\/\"\nEXP = \".\/\"\nEXP_MODEL = os.path.join(EXP, \"model\")\nEXP_FIG = os.path.join(EXP, \"fig\")\nEXP_PREDS = os.path.join(EXP, \"preds\")\n\n# make dirs\nfor d in [EXP_MODEL, EXP_FIG, EXP_PREDS]:\n    os.makedirs(d, exist_ok=True)\n    \n# utils\nlogger = Logger(EXP)\nwarnings.filterwarnings(\"ignore\")\nsns.set(style='whitegrid')","cbc58f4b":"train = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\ntest = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\nsample_submission = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))\n\nif Config.debug:\n    np.random.seed(Config.seeds[0])\n    train = train[train[\"breath_id\"].isin(np.random.choice(train[\"breath_id\"].unique(), 100))].reset_index(drop=True)\n    test = test[test[\"breath_id\"].isin(np.random.choice(test[\"breath_id\"].unique(), 100))].reset_index(drop=True)\n    sample_submission = sample_submission[sample_submission[\"id\"].isin(test[\"id\"].tolist())].reset_index(drop=True)\n    \nprint(f\"Train shape: {train.shape}, test shape: {test.shape}, submission shape: {sample_submission.shape}\")","2c513268":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] \/df['count']\n    \n    #df['u_in_lag']=0\n    #df['u_in_lag2']=0\n    #for i in range(df.shape[0]):\n        #if df['breath_id'][i]==df['breath_id'][i+1]:\n        #    df['u_in_lag'][i+1]=df['u_in'][i]\n        #else:\n        #    df['u_in_lag'][i+1]=0\n        #if df['breath_id'][i]==df['breath_id'][i+2]:\n        #    df['u_in_lag'][i+2]=df['u_in'][i]\n        #else:\n        #    df['u_in_lag'][i+2]=0\n        #if i\/10000==round(i\/10000):\n        #    print(i)\n    \n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['u_in_lag'] = df['u_in_lag']*df['breath_id_lagsame']\n    df['u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['u_in_lag2'] = df['u_in_lag2']*df['breath_id_lag2same']\n    df['u_out_lag2'] = df['u_out'].shift(2).fillna(0)\n    df['u_out_lag2'] = df['u_out_lag2']*df['breath_id_lag2same']\n    #df['u_in_lag'] = df['u_in'].shift(2).fillna(0)\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['RC'] = df['R']+df['C']\n    df = pd.get_dummies(df)\n    return df","bb8a8e64":"def gkf(X, group, n_splits, random_state, shuffle):\n    gkf = GroupKFold_(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n    return list(gkf.split(X, group=group))","96b63446":"# preprocess\nprint(\"# ============= # Preprocess # ============= #\")\ntrain_x = add_features(train)\ntest_x = add_features(test)\ntrain_x = reduce_mem_usage(train_x)\ntest_x = reduce_mem_usage(test_x)\ntrain_y = train_x[['pressure']]\nfeatures = [col for col in train_x.columns if col not in ['pressure','id', 'breath_id','one','count','breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same','u_out_lag2']]","8d48d354":"print(\"# ============= # RobustScaler # ============= #\")\nRS = RobustScaler()\ntrain_x = RS.fit_transform(train_x[features])\ntest_x = RS.transform(test_x[features])\ntrain_x = pd.DataFrame(data=train_x, columns=features)\ntest_x = pd.DataFrame(data=test_x, columns=features)\nprint(train_x.shape)\nprint(test_x.shape)","cda33117":"# cv_index = [folds, (train\/evaluate), index]\nprint(\"# ============= # Cross validation # ============= #\")\n# importlib.reload(cv_m)\ncv_index=gkf(train, group=train[\"breath_id\"],n_splits=Config.n_fold, random_state=Config.seeds[0], shuffle=Config.cv_shuffel)\nprint(f\"Cv list shape: {np.shape(cv_index)}\")\nprint(f\"Train len: {len(cv_index[0][0])}\")\nprint(f\"Evaluation len: {len(cv_index[0][1])}\")","140a9229":"import gc\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold","1678908c":"def create_model(strategy, tr_x, n_a):   \n    np.random.seed(Config.seeds[0])\n    tf.random.set_seed(Config.seeds[0])\n\n#     with strategy.scope():\n    model = Sequential([\n        tf.keras.layers.Input(shape=tr_x.shape[-2:]),\n#         tf.keras.layers.Bidirectional(keras.layers.LSTM(300, return_sequences=True)),\n#         tf.keras.layers.Bidirectional(keras.layers.LSTM(250, return_sequences=True)),\n#        tf.keras.layers.Bidirectional(keras.layers.LSTM(150, return_sequences=True)),\n        tf.keras.layers.Bidirectional(keras.layers.LSTM(n_a, return_sequences=True)), # Return hidden state output in each time step \"return_sequences=True\".\n        tf.keras.layers.Dense(n_a, activation='gelu'),\n        tf.keras.layers.Dense(1),\n    ])\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mae') #0.002\n    \n    return model\n\ndef plot_hist(hist):\n    plt.plot(hist.history[\"loss\"])\n    plt.plot(hist.history[\"val_loss\"])\n    plt.title(\"Model Performance\")\n    plt.ylabel(\"MAE\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","9e630d1c":"# # Detect hardware, return appropriate distribution strategy\n# print(tf.version.VERSION)\n# try: # detect TPU\n#     tpu = None\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# except ValueError: # detect GPU(s) and enable mixed precision\n#     strategy = tf.distribute.MirroredStrategy() # works on GPU and multi-GPU\n#     policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n#     tf.config.optimizer.set_jit(True) # XLA compilation\n#     tf.keras.mixed_precision.experimental.set_policy(policy)\n#     print('Mixed precision enabled')\n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","b1d02d96":"%%time\nTx = 80\nn_a = Config.n_a # number of dimensions for the hidden state of each LSTM cell.\ntest_preds = []\nmodels = []\ntest_x = test_x.values.reshape(-1, 80, test_x[features].shape[-1])\n\nfor fold, (train_idx, eval_idx) in enumerate(cv_index):\n    \n    K.clear_session()\n    print(f\"\\nFOLD: {fold}\")\n\n    # train\n    tr_x = train_x.iloc[train_idx]\n    tr_y = train[\"pressure\"].iloc[train_idx]\n    # evaluating\n    vl_x = train_x.iloc[eval_idx]\n    vl_y = train[\"pressure\"].iloc[eval_idx]\n\n    # reshape for lstm\n    tr_x = tr_x.values.reshape(-1, Tx, tr_x.shape[-1])\n    tr_y = tr_y.values.reshape(-1, Tx)\n    vl_x = vl_x.values.reshape(-1, Tx, vl_x.shape[-1])\n    vl_y = vl_y.values.reshape(-1, Tx)\n\n    # model creation\n    checkpoint_filepath = f\".\/checkpoint\/checkpoint_sv.hdf5\"\n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.55, patience=8, verbose=0)\n    es = EarlyStopping(monitor=\"val_loss\", patience=50, verbose=0, mode=\"min\", restore_best_weights=True)\n    sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath,\n            monitor='val_loss',\n            verbose=0, \n            save_best_only=True,\n            save_weights_only=False,\n            mode='auto',\n            save_freq='epoch'\n        )\n\n    model = create_model(None, tr_x, n_a)\n\n    # model train\n    history = model.fit(tr_x, tr_y,\n                    validation_data=(vl_x, vl_y),\n                    epochs=Config.EPOCHS,\n                    batch_size=Config.BATCH_SIZE,\n                    verbose= \"auto\",\n                    callbacks = [lr, es, sv])\n\n\n    # model predict\/test\n    y_true = vl_y.squeeze().reshape(-1, 1)\n    y_pred = np.array(model.predict(vl_x, verbose=1, batch_size=Config.BATCH_SIZE)).squeeze().reshape(-1, 1)\n    score = mean_absolute_error(y_true, y_pred)\n    print(f\"OOF MAE Fold {1}: {score}\")\n\n    del tr_x, tr_y, vl_x, vl_y\n    _ = gc.collect()\n\n    models.append(model)\n    a0 = np.zeros((len(test_x), n_a))\n    c0 = np.zeros((len(test_x), n_a))\n    test_preds.append(np.array(model.predict(test_x, batch_size=Config.BATCH_SIZE, verbose=1)).squeeze().reshape(-1, 1).squeeze())\n\n    plot_hist(history)\n\n    if fold == 0:\n        dot_img_file = '.\/fig\/model.png'\n        tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)","44a3b666":"max_pressure = 64.82099173863948\nmin_pressure = -1.8957442945646408\ndiff_pressure = 0.07030215\n\nsample_submission[\"pressure\"] = np.median(np.vstack(test_preds), axis=0)    \nsample_submission[\"pressure\"] = np.round((sample_submission.pressure - min_pressure)\/diff_pressure) * diff_pressure + min_pressure\nsample_submission.pressure = np.clip(sample_submission.pressure, min_pressure, max_pressure)\nsample_submission.to_csv('.\/submission.csv', index=False)\nsample_submission.head()","6f227f2c":"# # return_sequences=True --> in each time step returns the hidden state\n# # return_state=True --> will provide access to the hidden state output (state_h) and the cell state (state_c).  lstm1, state_h, state_c = LSTM(1, return_state=True)\n# def create_desglosed_model(m, Tx, hot_v, n_a):\n#     outputs = []\n#     X = tf.keras.layers.Input(shape=(Tx, hot_v))\n#     a0 = tf.keras.layers.Input(shape=(n_a,), name='a0')\n#     c0 = tf.keras.layers.Input(shape=(n_a,), name='c0')\n#     forward_h = a0\n#     forward_c = c0\n#     backward_h = a0\n#     backward_c = c0\n\n#     reshaper = tf.keras.layers.Reshape((1, hot_v))  \n#     bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_a, return_state=True), input_shape=(m, Tx, n_a))\n#     dense1 = tf.keras.layers.Dense(n_a, activation='gelu')\n#     dense2 = tf.keras.layers.Dense(1)\n    \n#     for t in range(Tx):\n#         x = X[:,t,:]\n#         x = reshaper(x)\n        \n#         lstm_out, forward_h, forward_c, backward_h, backward_c = bilstm(x, initial_state=[forward_h, forward_c, backward_h, backward_c])\n#         lstm_out = dense1(lstm_out)\n#         out = dense2(lstm_out)\n#         outputs.append(out) \n    \n#     model = tf.keras.models.Model(inputs=[X,a0,c0],outputs=outputs)\n#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mae') #0.002\n#     return model","89a8b359":"# Tx = 80\n# n_a = Config.n_a # number of dimensions for the hidden state of each LSTM cell.\n# test_preds = []\n# models = []\n# test_x = test_x.values.reshape(-1, 80, test_x.shape[-1])\n\n# for fold, (train_idx, eval_idx) in enumerate(cv_index):\n    \n#     K.clear_session()\n#     print(f\"\\nFOLD: {fold}\")\n\n#     # train\n#     tr_x = train_x.iloc[train_idx]\n#     tr_y = train[\"pressure\"].iloc[train_idx]\n#     # evaluating\n#     vl_x = train_x.iloc[eval_idx]\n#     vl_y = train[\"pressure\"].iloc[eval_idx]\n\n#     # reshape for lstm\n#     tr_x = tr_x.values.reshape(-1, Tx, tr_x.shape[-1])\n#     tr_y = tr_y.values.reshape(-1, Tx)\n#     vl_x = vl_x.values.reshape(-1, Tx, vl_x.shape[-1])\n#     vl_y = vl_y.values.reshape(-1, Tx)\n\n#     # model creation\n#     checkpoint_filepath = f\".\/checkpoint\/checkpoint_sv.hdf5\"\n#     lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.55, patience=8, verbose=0)\n#     es = EarlyStopping(monitor=\"val_loss\", patience=50, verbose=0, mode=\"min\", restore_best_weights=True)\n#     sv = keras.callbacks.ModelCheckpoint(\n#             checkpoint_filepath,\n#             monitor='val_loss',\n#             verbose=0, \n#             save_best_only=True,\n#             save_weights_only=False,\n#             mode='auto',\n#             save_freq='epoch'\n#         )\n\n# #     model = create_model(None, tr_x, n_a)\n\n#     model = create_desglosed_model(len(tr_x), len(tr_x[0]), len(tr_x[0][0]), n_a)\n\n#     # model train\n# #     history = model.fit(tr_x, tr_y,\n# #                     validation_data=(vl_x, vl_y),\n# #                     epochs=Config.EPOCHS,\n# #                     batch_size=Config.BATCH_SIZE,\n# #                     verbose= \"auto\",\n# #                     callbacks = [lr, es, sv])\n#     a0 = np.zeros((len(tr_x), n_a))\n#     c0 = np.zeros((len(tr_x), n_a))\n#     vl_a0 = np.zeros((len(vl_x), n_a))\n#     vl_c0 = np.zeros((len(vl_x), n_a))\n\n#     history = model.fit([tr_x,a0,c0], tr_y,\n#                     validation_data=([vl_x,vl_a0,vl_c0], vl_y),\n#                     epochs=Config.EPOCHS,\n#                     batch_size=Config.BATCH_SIZE,\n#                     verbose= \"auto\",\n#                     callbacks = [lr, es, sv])\n\n#     # model predict\/test\n#     y_true = vl_y.squeeze().reshape(-1, 1)\n#     y_pred = np.array(model.predict([vl_x,vl_a0,vl_c0], verbose=1, batch_size=Config.BATCH_SIZE)).squeeze().reshape(-1, 1)\n#     score = mean_absolute_error(y_true, y_pred)\n#     print(f\"OOF MAE Fold {1}: {score}\")\n\n#     del tr_x, tr_y, vl_x, vl_y\n#     _ = gc.collect()\n\n#     models.append(model)\n#     a0 = np.zeros((len(test_x), n_a))\n#     c0 = np.zeros((len(test_x), n_a))\n#     test_preds.append(np.array(model.predict([test_x,a0,c0], batch_size=Config.BATCH_SIZE, verbose=1)).squeeze().reshape(-1, 1).squeeze())\n\n#     plot_hist(history)\n\n#     if fold == 0:\n#         dot_img_file = '.\/fig\/model.png'\n#         tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)","e7fd1b95":"dot_img_file = '.\/fig\/model.png'\ntf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)","3f3a354f":"## SetUp","df02d278":"# Bi directional lstm model with initial state","15ca6220":"## Funcs","79a4cb66":"## Building model","804f53d4":"## Load Data","4f20e4b5":"## Config","569cf771":"# Useful modules","ab59e0b2":"## Main","5bbcbb49":"## Library","eed98ede":"## Feature Enginnering"}}