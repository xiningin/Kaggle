{"cell_type":{"c3b8ed1a":"code","a19bf047":"code","750f8fcd":"code","d9d6904c":"code","b4d5d335":"code","3cfdef7a":"code","5a2de24f":"code","e7be1e90":"code","81c45b61":"code","d8c9133c":"code","7eecd7c2":"code","ed1919ba":"code","70dda156":"markdown","3e3d98b8":"markdown","26215475":"markdown","2c38d34c":"markdown","1a1b0378":"markdown","4c9b3f23":"markdown","8ed9a210":"markdown","e6adc8d7":"markdown","81f3bdf5":"markdown","77bd9e5e":"markdown","c4bacecb":"markdown","28c009ba":"markdown"},"source":{"c3b8ed1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a19bf047":"data=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\nprint(data.head())\ntext_corpus=data['text']\nprint(text_corpus)\ntrain_data=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","750f8fcd":"import re\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom wordcloud import WordCloud\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nstop=set(stopwords.words('english'))\nimport tensorflow as tf\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom sklearn.manifold import TSNE\nfrom tqdm import tqdm\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout,Conv1D,Flatten,Dropout,Activation,MaxPooling1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n","d9d6904c":"print(data.shape[0])\nprint(data.shape[1])\n\nprint(train_data.shape[0])\nprint(test_data.shape[0])\n\nprint(train_data.columns)\nreal_tweet=train_data[train_data['target']==1].shape[0]\nfalse_tweet= train_data[train_data['target']==0].shape[0]\nprint(\"Real tweets:\",real_tweet)\nprint(\"Fake tweets:\",false_tweet)\n\ndef word_freq(data):\n    return len(data)\n\nreal_tweet_length= train_data[train_data['target']==1]['text'].str.split().map(lambda x: word_freq(x))\nfalse_tweet_length=train_data[train_data['target']==0]['text'].str.split().map(lambda x: word_freq(x))\nprint(real_tweet_length)\nprint(false_tweet_length)\n\ndef draw_countoftweets(real_tweet,false_tweet):\n    plt.rcParams['figure.figsize']=(10,10)\n    plt.bar(0,real_tweet,width=0.7,label='Real Tweets',color='Green')\n    plt.legend()\n    plt.bar(2,false_tweet,width=0.7,label='False Tweets',color='Red')\n    plt.legend()\n    plt.ylabel('Count of Tweets')\n    \n    plt.title('Types of Tweets')\n    plt.show()\n\ndraw_countoftweets(real_tweet,false_tweet)\n\ndef draw_words_in_tweet(real_tweet_length,false_tweet_length):\n    figs,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    ax1.hist=plt.hist(real_tweet_length,color='Blue')\n    ax1.set_title('Real Tweets')\n    ax2.hist=plt.hist(false_tweet_length,color='Red')\n    ax2.set_title('False Tweets')\n    figs.suptitle('Words in tweet')\n    plt.show()\n    \ndraw_words_in_tweet(real_tweet_length,false_tweet_length)\nreal_avg_tweet_len= train_data[train_data['target']==1]['text'].str.split().apply(lambda x:[len(i) for i in x])\nreal_avg_tweet_len=real_avg_tweet_len.map(lambda x : np.mean(x))\nfalse_avg_tweet_len= train_data[train_data['target']==0]['text'].str.split().apply(lambda x:[len(i) for i in x])\nfalse_avg_tweet_len= false_avg_tweet_len.map(lambda x: np.mean(x))\nreal_tweet_mention=train_data[train_data['target']==1]['text'].apply(lambda x: len([j for j in str(x) if j=='@']))\nfalse_tweet_mention=train_data[train_data['target']==0]['text'].apply(lambda x : len([j for j in str(x) if j=='@']))\nreal_stopword_count=train_data[train_data['target']==1]['text'].apply(lambda x: len([j for j in str(x) if j in stop]))\nfalse_stopword_count=train_data[train_data['target']==0]['text'].apply(lambda x: len([j for j in str(x) if j in stop]))\ndef fig_avg_plot(real_avg_tweet_len,false_avg_tweet_len):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(real_avg_tweet_len,ax=ax1,color='Blue')\n    ax1.set_title('Real Tweet Length')\n    sns.distplot(false_avg_tweet_len,ax=ax2,color='Red')\n    ax2.set_title('False Tweet Length')\n    fig.suptitle('Average Length of Words in Tweet')\n    plt.show()\n\n\nfig_avg_plot(real_avg_tweet_len,false_avg_tweet_len)\n\ndef fig_tweet_mention_plot(real_tweet_mention,false_tweet_mention):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(real_tweet_mention,ax=ax1,color='Blue')\n    ax1.set_title('Real Tweet Mention')\n    sns.distplot(false_tweet_mention,ax=ax2,color='Red')\n    ax2.set_title('False Tweet Mention')\n    fig.suptitle('Tweet Mentions')\n    plt.show()\n\nprint(real_tweet_mention)\nprint(real_stopword_count)\nprint(false_stopword_count)\n#fig_tweet_mention_plot(real_tweet_mention,false_tweet_mention)","b4d5d335":"def corpus(data,target):\n    corpus_l=[]\n    for i in (train_data[train_data['target']==1]['text'].str.split()):\n        for j in i:\n            corpus_l.append(j)\n    return corpus_l\n\n\ndef analyse_most_common(data):\n    count=Counter(data)\n    mostcommon_words= count.most_common()\n    y=[]\n    x=[]\n    for word,count in mostcommon_words[:150]:\n        if word not in stop:\n            x.append(word)\n            y.append(count)\n    \n    sns.barplot(x=y,y=x)\n\n\n\ndef display_cloud(data):\n    wordcloud=WordCloud(width=500,height=700,stopwords=stop,background_color='white',min_font_size=5).generate(str(data))\n    plt.figure(figsize=(15,7))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()\n\nreal_tweet_corpus= corpus(train_data,1)\nfalse_tweet_corpus=corpus(train_data,0)\n#print(false_tweet_corpus)\nanalyse_most_common(real_tweet_corpus)\nanalyse_most_common(false_tweet_corpus)\ndisplay_cloud(real_tweet_corpus)\ndisplay_cloud(false_tweet_corpus)","3cfdef7a":"def remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\ndef clean_data(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n\ntrain_data['text']=train_data['text'].apply(lambda x: remove_url(x))\n#print(cleaned_data)\ntrain_data['text']=train_data['text'].apply(lambda x: clean_data(x))\nprint(train_data.head())\ntrain_data['text']=train_data['text'].apply(lambda x: remove_html(x))\ntrain_data['text']=train_data['text'].apply(lambda x: remove_punctuations(x))\n#train_data['text']=train_data['text'].apply(lambda x: stem_words(x))\nprint(train_data.head())\n\n\n\n    \n\n    ","5a2de24f":"def vectorize(data):\n    cv=CountVectorizer()\n    fit_data_cv=cv.fit_transform(data)\n    return fit_data_cv,cv\ndef tfidf(data):\n    tfidfv=TfidfVectorizer()\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_cv,tfidfv\n\ndef dimen_reduc_plot(test_data,test_label,option):\n    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n    pca=SparsePCA(n_components=2,random_state=42)\n    if(option==1):\n        tsvd_result=tsvd.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        \n        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label        )\n        \n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_Tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSVD\")\n        plt.show()\n    if(option==2):\n        tsne_result=tsne.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"PCA\")\n        plt.show() \n    if(option==3):\n        pca_result=pca.fit_transform(test_data.toarray())\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSNE\")\n        plt.show()\n        \ndata_vect=train_data['text'].values\ndata_vect_real=train_data[train_data['target']==1]['text'].values\ntarget_vect=train_data['target'].values\ntarget_data_vect_real=train_data[train_data['target']==1]['target'].values\ndata_vect_false=train_data[train_data['target']==0]['text'].values\ntarget_data_vect_false=train_data[train_data['target']==0]['target'].values\ntrain_data_cv,cv= vectorize(data_vect)\nreal_tweet_train_data_cv,cv=vectorize(data_vect_real)\nprint(train_data.head())\ndimen_reduc_plot(train_data_cv,target_vect,1)\ndimen_reduc_plot(real_tweet_train_data_cv,target_data_vect_real,1)\ndimen_reduc_plot(train_data_cv,target_vect,3)\ndimen_reduc_plot(real_tweet_train_data_cv,target_data_vect_real,3)\ndimen_reduc_plot(train_data_cv,target_vect,2)\ndimen_reduc_plot(real_tweet_train_data_cv,target_data_vect_real,2)\n\n        ","e7be1e90":"def add_corpus(data):\n    corpus=[]\n    for i in tqdm(data):\n        words=[word.lower() for word in word_tokenize(i)]\n        corpus.append(words)\n    return corpus\ndef create_glove_embedding(data):\n    embedding_map={}\n    file=open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r')\n    for  f in file:\n        values=f.split(' ')\n        word=values[0]\n        coef=np.asarray(values[1:],dtype='float32')\n        embedding_map[word]=coef\n    file.close()\n    return embedding_map\ndef  embedding_preprocess(data,target):\n    #max_word_length=1000\n    max_sequence_length=100\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(data)\n    sequences=tokenizer.texts_to_sequences(data)\n    \n    word_idx=tokenizer.word_index\n    data_pad=pad_sequences(sequences,padding=\"post\",maxlen=max_sequence_length)\n    label=to_categorical(np.asarray(target))\n    print(len(word_idx))\n    print(\"Data Length\")\n    print(data_pad)\n    print(\"Target Length\")\n    print(label.shape)\n    emb_dim=data.get('a').shape[0]\n    print(emb_dim)\n    num_length=len(word_idx)+1\n    emb_mat=np.zeros((num_length,emb_dim))\n    for word,idx in tqdm(word_idx.items()):\n        if idx > num_length:\n            continue\n        elif idx < num_length:\n            emb_vector=data.get(word)\n            if emb_vector is not None: \n                emb_mat[idx]=emb_vector\n    \n    return emb_mat,word_idx,data_pad,num_length\n    \nlines_without_stopwords=[] \nfor line in train_data['text'].values: \n    line = line.lower()\n    line_by_words = re.findall(r'(?:\\w+)', line, flags = re.UNICODE) \n    new_line=[]\n    for word in line_by_words:\n        if word not in stop:\n            new_line.append(word)\n    lines_without_stopwords.append(new_line)\ntexts = lines_without_stopwords\n    \ncorpus_train_data=add_corpus(train_data['text'])\nprint(\"corpus created\")\n\ntargets=train_data['target']\nembedding_map= create_glove_embedding(texts)\nprint(\"Embedding matrix created\")\nemb_mat,word_idx,pad_data,num_words=embedding_preprocess(embedding_map,targets)\nprint(pad_data.shape)\n\nprint(\"Visualise embedded vectors\")\nplt.plot(emb_mat[10])\nplt.plot(emb_mat[20])\nplt.plot(emb_mat[50])\nplt.title(\"Embedding Vectors\")\nplt.show()\n","81c45b61":"def split(train_tweet,test_tweet,random_state,test_size):\n    X_train,X_test,Y_train,Y_test= train_test_split(train_tweet,test_tweet,test_size=test_size)\n    return X_train,X_test,Y_train,Y_test\n\n\nprint(emb_mat.shape)\nprint(pad_data.shape)\nlabel=to_categorical(np.asarray(train_data['target']))\nprint(to_categorical(np.asarray(train_data['target'])).shape)\n\ntrain_tweet=pad_data[:train_data.shape[0]]\ntest_tweet=pad_data[train_data.shape[0]:]\nprint(train_tweet.shape)\nprint(test_tweet.shape)\nX_train,X_test,Y_train,Y_test=split(train_tweet,label,42,0.2)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(train_tweet)\n","d8c9133c":"\nmodel=Sequential([Embedding(num_words,100,input_length=100,weights=[emb_mat],trainable=False),\n                  Conv1D(128,5,activation='relu'),\n                  Dropout(0.2),\n                  MaxPooling1D(pool_size=3),\n                  \n                 LSTM(100),\n                 Dense(4,activation='relu'),\n                  Dense(4,activation='relu'),\n                  \n                 Dense(2,activation='sigmoid')])\noptimizer=Adam(learning_rate=1e-3)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.summary()\n","7eecd7c2":"model.fit(X_train,Y_train,verbose=2,epochs=40,batch_size=4,validation_data=(X_test,Y_test))","ed1919ba":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ny_pre=model.predict(test_tweet)\n","70dda156":"Preliminary submission: Requires lot of improvements.","3e3d98b8":"Split in to train test sets:\n\nUse train_test_split module from sklearn","26215475":"Using Conv1D and LSTM architecture Layers:Intial Model","2c38d34c":"Deep Embedding Layer using Keras (best practise):\n\nAcknowledgment: https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html","1a1b0378":"Initial data installation","4c9b3f23":"This is an intial draft tutorial and brief work-through of Glove based embedding models in tweet analysis.The following notebooks and repositories have been helpful and contains good information and presentation.\n\n\n\nAcknowledgements and Github\n\n\nIntial Draft of the work inspired from the following kernels:\n\n\n1.https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n\n\n2.https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n\n\n3.https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n\n4.https:\/\/www.kaggle.com\/stacykurnikova\/using-glove-embedding\n\n\n\nThe following githubs\/links have been useful:\n\n\n1.https:\/\/github.com\/stanfordnlp\/GloVe\n\n\n2.https:\/\/github.com\/scikit-learn\/scikit-learn\n\n\n3.https:\/\/github.com\/tensorflow\/tensorflow\n\n4.https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n\n\n\nThis is an introductory work on tweet analysis with preprocessing, and embeddings only and computed with a deep learning layer. The work is in intial phases and comments are not yet updated.","8ed9a210":"EDA 2-WordCloud and 1-gram analysis of most common words.","e6adc8d7":"Data Cleaning Extensively: Url, Unicode Emojis,Special Characters, Punctuations (Stemming,Lemmatizing is avoided for loss of semantic generality)\n\nAcknowledgment:https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","81f3bdf5":"Import Libraries \n","77bd9e5e":"EDA with metrics like word count,average word length,special characters.\n\nAcknowledgements:\n\nhttps:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert","c4bacecb":"Vectorization Analysis - Training for TFIDF and CountVectorizer \n\nChecking the Vectors in Low Dimension(2) using TSNE, PCA(Sparse) and SVD\n\nAcknowledgemnt : https:\/\/towardsdatascience.com\/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b","28c009ba":"Embedding using Glove with padded preprocessing:\n\nAcknowledgment: https:\/\/www.kaggle.com\/stacykurnikova\/using-glove-embedding\n\nhttps:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html"}}