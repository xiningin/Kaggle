{"cell_type":{"bd8cc07c":"code","4b660117":"code","d34fc43b":"code","9b89a6f4":"code","ee56fde7":"code","0ec2f5a0":"code","699adf27":"code","11d326e2":"code","036979dd":"code","47c30dbe":"code","b9fc1220":"markdown","3d4a79ac":"markdown","3efc012a":"markdown"},"source":{"bd8cc07c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b660117":"import datetime\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n%matplotlib inline","d34fc43b":"# load data\n\nbreast = load_breast_cancer()\n\nX, y = breast.data, breast.target\n\nfeature_name = breast.feature_names","9b89a6f4":"# split the datasets\n\nX_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","ee56fde7":"# transform the datasets\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)","0ec2f5a0":"# setting the parameters\n\nboost_round = 50\nearly_stop_round = 10 # if data's behavier not improve in this number of round, then stop. To avoid over-fitting\n\nparams = {\n    'boosting_type':'gbdt',\n    'objective':'regression',\n    'metrics':{'12', 'auc'},\n    'num_leaves':31,\n    'learning_rate':0.05,\n    'feature_fraction':0.9,\n    'bagging_fraction':0.8,\n    'bagging_freq':5,\n    'verbose':1\n}","699adf27":"# training and setting early-stop\nresults = {}\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=boost_round,\n                valid_sets=(lgb_eval, lgb_train),\n                valid_names=('validate','train'),\n                early_stopping_rounds=early_stop_round,\n                evals_result=results)","11d326e2":"# predictions\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ny_pred    ","036979dd":"# Model metrics\nlgb.plot_metric(results)\nplt.show()","47c30dbe":"# plot the most importance feature\nlgb.plot_importance(gbm, importance_type='split')\nplt.show()","b9fc1220":"All this code is from https:\/\/www.bilibili.com\/video\/BV1Ca4y1t7DS?p=19. It is a very useful video to learn ensemble model of ML.\n\nFirst, import the packages","3d4a79ac":"Now the training of the LGB model is complete.","3efc012a":"# LightGBM"}}