{"cell_type":{"7833b983":"code","bd0fc6c4":"code","b6724b6f":"code","73ad335c":"code","f2365f98":"code","9460b583":"code","016bf265":"code","ad259666":"code","65e4059e":"code","1bbcf827":"code","f8a7f576":"code","94858545":"code","7cedec1f":"code","5c29eefd":"markdown","886c36a6":"markdown","c9aba344":"markdown","24e4972a":"markdown","4ef558f8":"markdown","7829d58e":"markdown","dcff7db0":"markdown","555d0e32":"markdown"},"source":{"7833b983":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\nimport time\nfrom sklearn.datasets import fetch_20newsgroups\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom tqdm.auto import tqdm\ntqdm.pandas()","bd0fc6c4":"dataset = fetch_20newsgroups(subset='all')\n\nX = pd.Series(dataset['data'])\ny = pd.Series(dataset['target'])","b6724b6f":"fig, ax = plt.subplots(figsize=(20, 6))\nfig.suptitle('Target Class Distribution', fontsize=24)\ny.apply(lambda i: dataset['target_names'][i]).value_counts().plot.pie()","73ad335c":"fig, ax = plt.subplots(figsize=(20, 6))\nwordcloud = WordCloud(height=600, width=2000, stopwords=STOPWORDS).generate(' '.join((X.values)))\nax.imshow(wordcloud)\nax.set_title('Word Cloud', fontsize=24)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)","f2365f98":"fig, ax = plt.subplots(5, 4, figsize=(20, 14))\nfig.suptitle('Class wise multiset of words', fontsize=24)\nfor i in range(20):\n    u = Counter((' '.join((X[y != i].values))).split())\n    a = Counter((' '.join((X[y == i].values))).split())\n    wordcloud = WordCloud(height=600, width=1000, stopwords=STOPWORDS).generate_from_frequencies((a-u))\n    ax[i\/\/4][i%4].imshow(wordcloud)\n    ax[i\/\/4][i%4].set_title(f'{dataset[\"target_names\"][i]}', fontsize=18)\n    ax[i\/\/4][i%4].xaxis.set_visible(False)\n    ax[i\/\/4][i%4].yaxis.set_visible(False)    ","9460b583":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.metrics import roc_auc_score\n\nskf = StratifiedKFold(shuffle=True, random_state=19)\n\nroc_auc_scores = []\ntime_taken = []\nn_features = []\n\ndef train_model():\n    model = DecisionTreeClassifier(class_weight='balanced', random_state=19)\n    scores = []\n\n    for train_index, test_index in tqdm(skf.split(X, y)):\n        X_train, X_test = X_vect[train_index], X_vect[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        model.fit(X_train, y_train)\n        scores.append(roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr', average='weighted'))\n\n    feats = X_vect.shape[1]\n    score = round(np.mean(scores), 5)*100\n        \n    return feats, score","016bf265":"start = time.time()\n\nX_vect = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2)).fit_transform(X)\nfeats, score = train_model()\n\nend = time.time()\nt = end - start\n\nn_features.append(feats)\ntime_taken.append(t)\nroc_auc_scores.append(score)\n\nprint('Mean score', score)\nprint('Features used', feats)","ad259666":"import re\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\nstart = time.time()\n\nX = X.progress_apply(lambda text: re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-Za-z]\", \" \", text)))\nX = X.progress_apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\nX_vect = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2)).fit_transform(X)\nfeats, score = train_model()\n\nend = time.time()\nt = end - start\n\nn_features.append(feats)\ntime_taken.append(t)\nroc_auc_scores.append(score)\n\nprint('Mean score', score)\nprint('Features used', feats)","65e4059e":"from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nstart = time.time()\n\nX = X.progress_apply(lambda x: ' '.join(wordnet_lemmatizer.lemmatize(word).lower() for word in x.split()))\nX_vect = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2)).fit_transform(X)\nfeats, score = train_model()\n\nend = time.time()\nt = end - start\n\nn_features.append(feats)\ntime_taken.append(t)\nroc_auc_scores.append(score)\n\nprint('Mean score', score)\nprint('Features used', feats)","1bbcf827":"class RemoveIrrelevantFeatures():\n\n    def __init__(self, problem_type, random_state=None):\n        self.problem_type = problem_type\n        self.random_state = random_state\n        if problem_type not in ['regression', 'classification']:\n            raise Exception('Invalid problem type')\n        \n    def fit(self, X, y):\n        if self.problem_type == 'regression':\n            model = DecisionTreeRegressor(random_state=self.random_state)\n        else:\n            model = DecisionTreeClassifier(class_weight='balanced', random_state=self.random_state)\n        model.fit(X, y)\n        self.support = (model.feature_importances_ > 0)\n        self.indices = np.where(self.support)[0]\n        \n    def transform(self, X, y=None):\n        return X[:, self.indices]\n    \n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X, y)\n    \n    def get_support(self):\n        return self.support","f8a7f576":"start = time.time()\n\npipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2))),\n    ('feature_selector', RemoveIrrelevantFeatures(problem_type='classification', random_state=19)),\n])\nX_vect = pipeline.fit_transform(X, y)\nfeats, score = train_model()\n\nend = time.time()\nt = end - start\n\nn_features.append(feats)\ntime_taken.append(t)\nroc_auc_scores.append(score)\n\nprint('Mean score', score)\nprint('Features used', feats)","94858545":"for i in range(3):\n    n_features[i] -= n_features[i+1]","7cedec1f":"fig, ax = plt.subplots(1, 2, figsize=(20, 8))\nax = ax.tolist()\nlabels = ['Noise', 'Un Normalized Text', 'Irrelevant Features', 'Important Features']\nexplode = (0, 0, 0, 0.1)\nax[0].pie(n_features, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax[0].set_title('Feature Analysis', fontsize=18)\nax[0].axis('equal')\nax[0].legend()\n\n\nt = ['Raw Text', 'Cleaned Text', 'Normalized Text', 'Selected Features']\n\ncolor = 'tab:orange'\nax[1].set_ylabel('Time Taken', color=color, fontsize=12)\nax[1].plot(t, time_taken, color=color)\nax[1].tick_params(axis='y', labelcolor=color)\n\nax.append(ax[1].twinx())\n\ncolor = 'tab:blue'\nax[2].set_ylabel('AUROC', color=color, fontsize=12)\nax[2].plot(t, roc_auc_scores, color=color)\nax[2].tick_params(axis='y', labelcolor=color)\n\nax[1].set_title('Comparison: Time taken and AUROC', fontsize=18)\nplt.show()","5c29eefd":"# Performance Comparison","886c36a6":"# Natural Language Processing\n\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n\nSince, text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n\nAs it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications.\n\nEmbeddings are the texts converted into numbers and there may be different numerical representations of the same text. The different types of word embeddings can be broadly classified into two categories-\n\n* Frequency based Embedding\n    * Binary Vector\n    * Count Vector\n    * TF-IDF Vector\n* Prediction based Embedding\n    * Continuous Bag of Words\n    * Skip Gram\n\nReference: [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/word-embeddings-count-word2veec\/)","c9aba344":"## Text Cleaning\n\nAny piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n\nFor example \u2013 language stopwords (commonly used words of a language \u2013 is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\nA general approach for text cleaning is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary.","24e4972a":"# Dataset Exploration\n\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. \n\nThe data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware \/ comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale \/ soc.religion.christian). \n\n> The `sklearn.datasets.fetch_20newsgroups` function is a data fetching \/ caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the `~\/scikit_learn_data\/20news_home` folder and calls the `sklearn.datasets.load_files` on either the training or testing set folder, or both of them","4ef558f8":"## Text Normalization\n\nAnother type of textual noise is about the multiple representations exhibited by single word.\n\nFor example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model.\n\nThe most common lexicon normalization practices are :\n\n* Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word.\n* Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).","7829d58e":"## Feature Selection\n\nFeature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.\n\nIrrelevant or partially relevant features can negatively impact model performance.\nHaving irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n\nFeature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\n","dcff7db0":"* Cleaning and Normalization reduce the time taken as well as improve the model's capability of distinguishing between classes\n* Feature Selection reduces the time taken by a huge margin but also there is very slight decrease in AUROC\n* For this case, as the pie chart depicts:\n    * 2% features are actually noise\n    * 3.4% features are due to Un normalised data\n    * 95.3% features are irrelevant\n    * Only 0.2% features are relevant\n    ","555d0e32":"## TF-IDF on uncleaned Data"}}