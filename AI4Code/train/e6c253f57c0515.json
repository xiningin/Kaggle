{"cell_type":{"21e24fd2":"code","8b96cf19":"code","3c2e115b":"code","fc1beab2":"code","e04749ba":"code","17fd7897":"code","6ff0e2ec":"code","762e0b37":"code","6712da22":"code","8faa227f":"code","19a3aa59":"code","5eb6eaab":"code","aacef157":"code","6ae35476":"code","e625d878":"code","7ebb72d1":"code","8f4d5fb6":"code","99078428":"code","91fdfa3d":"code","a8031bad":"code","a8ec547c":"markdown","243ec648":"markdown","cf8cf78b":"markdown","c9d11677":"markdown","2be46d7b":"markdown","073fbfcd":"markdown","4dd8eff6":"markdown","9f426ff9":"markdown","6da8854b":"markdown","2bdfece0":"markdown","cee5480c":"markdown","f6db738e":"markdown","d21771d9":"markdown","576a7562":"markdown","da7b7ef6":"markdown"},"source":{"21e24fd2":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', -1)\nimport matplotlib.pyplot as plt\nimport re\nimport pickle\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import word_tokenize\nimport nltk\n","8b96cf19":"from gensim.models import KeyedVectors\nmodel = KeyedVectors.load_word2vec_format(\"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\", binary=True)\n# Gensim install and documentation:\n#    https:\/\/radimrehurek.com\/gensim\/install.html\n#    http:\/\/mccormickml.com\/2016\/04\/12\/googles-pretrained-word2vec-model-in-python\/\n# Word2Vec tutorials:\n#  Skip-gram model: similar to autoencoder, goal is to learn weights (word vectors) in an unsupervised way\n#    The unsupervised task is, given a word, predict surrounding words\n#    http:\/\/mccormickml.com\/2016\/04\/19\/word2vec-tutorial-the-skip-gram-model\/\n#  Negative sampling: Same as before but make it feasible to train (300M weights reduced by\n#    sub-sampling frequent words and using 'negative sampling' optimization)\n#    http:\/\/mccormickml.com\/2017\/01\/11\/word2vec-tutorial-part-2-negative-sampling\/\n#  Applying Word2Vec to recommenders and advertising\n#    http:\/\/mccormickml.com\/2018\/06\/15\/applying-word2vec-to-recommenders-and-advertising\/\n# Word2Vec applications:\n#    predict age\/gender for a specific random blog\n#    https:\/\/github.com\/sunyam\/Blog_Authorship","3c2e115b":"# Find the path to Chatbot Data files\n# This search is useful because the spelling is slightly different in the Workspace\n# For example 'Chatbot Data' in the Workspace translates into 'chatbot-data' in the path\nimport os\nprint(\"Files in '..\/input':\")\nfor entry in os.scandir('..\/input'):\n    print(entry.name)\nprint('')\nprint(\"Files in '..\/input\/chatbot-data':\")\nfor entry in os.scandir('..\/input\/chatbot-data'):\n    print(entry.name)\nprint('')\nprint(\"Files in '..\/input\/chatbot-data\/cornell_movie_dialogs_corpus':\")\nfor entry in os.scandir('..\/input\/chatbot-data\/cornell_movie_dialogs_corpus'):\n    print(entry.name)\nprint('')\nprint(\"Files in '..\/input\/chatbot-data\/cornell_movie_dialogs_corpus\/cornell movie-dialogs corpus':\")\nfor entry in os.scandir('..\/input\/chatbot-data\/cornell_movie_dialogs_corpus\/cornell movie-dialogs corpus'):\n    print(entry.name)","fc1beab2":"# Let's read the movie titles files and see whether we can find 'The Godfather'\ntitles = pd.read_csv('..\/input\/chatbot-data\/cornell_movie_dialogs_corpus\/cornell movie-dialogs corpus\/movie_titles_metadata.txt'\n                     , header = None, names = ['movieID', 'movie title', 'movie year', 'IMDB rating', 'IMDB votes', 'genres']\n                     , sep = '\\+\\+\\+\\$\\+\\+\\+', engine = 'python')\ntitles['movieID'].apply(lambda x: x.strip())\ntitles[titles['movie title'].str.contains('godfather')]","e04749ba":"# Let's read the characters file and see whether we can find 'The Godfather' characters\ncharacters = pd.read_csv('..\/input\/chatbot-data\/cornell_movie_dialogs_corpus\/cornell movie-dialogs corpus\/movie_characters_metadata.txt'\n                         , header = None, names = ['characterID', 'character name', 'movieID', 'movie title', 'gender', 'position in credits'] \n                         , sep = '\\+\\+\\+\\$\\+\\+\\+', engine = 'python')\ncharacters['characterID'] = characters['characterID'].str.strip()\ncharacters['movieID'] = characters['movieID'].str.strip()\ncharacters['gender'] = characters['gender'].str.strip().str.lower()\ncharacters[characters['movieID'] == 'm203'].head(5)","17fd7897":"# Gender information is available for a third of the characters\n# and for those, we have twice more male than female\ncharacters['gender'].value_counts()","6ff0e2ec":"# Let's read the dialog lines and see whether we can find the opening 'Godfather' scene (monologue not included unfortunately)\nlines = pd.read_csv('..\/input\/chatbot-data\/cornell_movie_dialogs_corpus\/cornell movie-dialogs corpus\/movie_lines.txt'\n                    , header = None, names = ['lineID', 'characterID',  'movieID', 'character name', 'line text'] \n                    , sep = '\\+\\+\\+\\$\\+\\+\\+', engine = 'python')\nlines['lineID'] = lines['lineID'].str.strip()\nlines['characterID'] = lines['characterID'].str.strip()\nlines['movieID'] = lines['movieID'].str.strip()\nlines[lines['movieID'] == 'm203'].sort_values(by=['lineID']).head(5)","762e0b37":"# Let's join character gender information to the lines table\n# Note that gender information is not available for Bonasera\nlines2 = lines.join(characters.set_index('characterID'), on='characterID', rsuffix='_char')\nlines2.head(5)\nlines2 = lines2.drop(columns = ['character name_char', 'movieID_char'])\nlines2[lines2['movieID'] == 'm203'].sort_values(by=['lineID']).head(5)\n","6712da22":"# Let's remove lines with missing gender information, and replace special characters with a space\nlines2 = lines2.drop(lines2[(lines2.gender != 'm') & (lines2.gender != 'f')].index)\nlines2['line text'] = lines2['line text'].str.replace(\"[^a-zA-Z0-9 ']\", \" \", regex = True).fillna(\"\")\nlines2[lines2['movieID'] == 'm203'].sort_values(by=['lineID']).head(5)","8faa227f":"# Women account for 30% of lines, men for 70%\nlines2['gender'].value_counts()","19a3aa59":"# We'll use below function to transform a full sentence into a vector\n#   The function first lists vectors associated to each word\n#   and then returns the average vector for the full sentence\n# The resulting vector has 300 dimensions (0 to 299). Each dimension has a value between -1 and +1\n# In the example below, dimansion 79 has the highest value 0.16\ndef transform_sentence(line):\n    transform_sentence_result = [model[w] for w in word_tokenize(line) if w in model]\n    if len(transform_sentence_result) == 0:\n        return np.array([0.0]*300)\n    else:\n        return pd.Series(transform_sentence_result).mean(axis=0)\npd.Series(transform_sentence('Bonasera we know each other for years')).sort_values()","5eb6eaab":"# Which word contributes most to the sentence?\n# In our example, dimension 79 has the highest value 0.16\n# Which word contributes most to that dimension 79?\ndef most_significant_word(line):\n    # Which dimension \"a\" has the highest value?\n    a = pd.Series([model[w] for w in word_tokenize(line) if w in model]).mean(axis=0).argmax()\n    most_significant_word = ''\n    # Which word contributes most to that dimension?\n    significance = 0\n    for w in word_tokenize(line):\n        if w in model:\n            if model[w][a] > significance:\n                most_significant_word = w\n                significance = model[w][a]\n    return most_significant_word\nmost_significant_word('Bonasera we know each other for years')","aacef157":"# The example below for the word 'Corleone' shows that\n# dimension 125 has the minimum value -0.68\n# dimension 61 has the maximum value 0.60\n# mean value is close to zero\npd.DataFrame(model['Corleone']).agg(['min', 'idxmin', 'mean', 'max', 'idxmax'])","6ae35476":"# Let's see how lines 104504 & 104505 look like once vectorized\nlines3 = lines2.apply(lambda x: transform_sentence(x['line text']), axis=1, result_type='expand')\nlines3 = lines3[lines3.max(axis=1) != 0]\nlines3 = lines3.join(lines2['gender'])\nlines3.loc[104504:104506,:]","e625d878":"# Let's fit a logistic regression classifier with the vectorized sentences\nX = lines3.drop(['gender'], axis=1)\ny = lines3['gender']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.60)\nlr = LogisticRegression(solver='lbfgs', class_weight='balanced')\nlr.fit(X_train, y_train)\n# Function below provides a report including accuracy score, confusion matrix and classification report\ndef report(clf, X, y):\n    acc = accuracy_score(y_true=y, \n                         y_pred=clf.predict(X))\n    cm = pd.DataFrame(confusion_matrix(y_true=y, \n                                       y_pred=clf.predict(X)), \n                      index=clf.classes_, \n                      columns=clf.classes_)\n    rep = classification_report(y_true=y, \n                                y_pred=clf.predict(X))\n    return '{:.3f}\\n\\n{}\\n\\n{}'.format(acc, cm, rep)\n# Let's print the report\nprint()\nprint('Logistic Regression')\nprint(report(lr, X_test, y_test))\n","7ebb72d1":"for strategy in ['stratified', 'most_frequent', 'uniform']:\n    dummy = DummyClassifier(strategy=strategy)\n    dummy.fit(X_train, y_train)\n    print('')\n    print('dummy', strategy)\n    print(report(dummy, X_test, y_test))","8f4d5fb6":"# Let's see how our two lines 104504 & 104506 look like once counted (1 if the word is present, 0 if not)\ncorpus = lines2['line text']\nvectorizer = CountVectorizer(binary=True, min_df=0.001, max_df=0.999)\nvectorizer.fit(corpus)\nX2 = pd.DataFrame(vectorizer.transform(corpus).A, columns = vectorizer.get_feature_names(), index=lines2.index)\ny2 = lines2['gender']\nX2.loc[104504:104506,:]","99078428":"# Let's fit a logistic regression classifier with the counting, and print the report\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.60)\nlr2 = LogisticRegression(solver='lbfgs', class_weight='balanced')\nlr2.fit(X2_train, y2_train)\nprint()\nprint('Logistic Regression - CountVectorizer')\nprint(report(lr2, X2_test, y2_test))","91fdfa3d":"lr3 = LogisticRegression(solver='lbfgs', class_weight='balanced')\nlr3.fit(X, y)\nprobas = pd.DataFrame(lr.predict_proba(X), \n                         columns=['P({})'.format(X) for X in lr.classes_], \n                         index=X.index)\nprobas = probas.join(lines2)\nprobas.sort_values('P(f)').head(3)","a8031bad":"probas.sort_values('P(m)').head(3)\n# No comment ;-)","a8ec547c":"### IN PARTICULAR, IMPORT AND LOAD GOOGLE PRE-TRAINED WORD2VEC MODEL (USING GENSIM PACKAGE)","243ec648":"### LET'S SEE HOW WORD2VEC PRE-TRAINED MODEL WORKS\n#### Google's WORD2VEC is a model that associates 30M words with as many vectors","cf8cf78b":"# MOVIE DIALOGS ANALYSIS\n##\n## Given a text line, can we predict who is talking: a man or a woman?\n### We'll use the \"Chatbot Data\" dataset of 617 movies, 9 000 characters, 300 000 text lines\n### We'll analyze those text lines using Google's Word2Vec model that associates 30M words to as many vectors","c9d11677":"## 1. IMPORT","2be46d7b":"## 4. VALIDATE\n### COULD WE GET THE SAME RESULT BY JUST THROWING A DICE?\n#### \u201cstratified\u201d: generates predictions by respecting the training set\u2019s class distribution.\n#### \u201cmost_frequent\u201d\/\"prior\": always predicts the most frequent label in the training set.\n#### \u201cuniform\u201d: generates predictions uniformly at random.","073fbfcd":"### COULD WE GET BETTER RESULTS WITH COUNTVECTORIZER?","4dd8eff6":"## 3. FIT","9f426ff9":"### LOGISTIC REGRESSION CLASSIFIER","6da8854b":"### Most masculine lines","2bdfece0":"## 5. CONCLUSION: WHICH SENTENCES MAKE THE MOST DIFFERENCE?","cee5480c":"### Most feminine lines","f6db738e":"### CountVectorizer actually stands the comparison with Word2Vec","d21771d9":"## 3.PREPROCESS[](http:\/\/)","576a7562":"## 2. EXPLORE: IS THE GODFATHER IN THE DATASET?","da7b7ef6":"### LET'S GET WHAT WE NEED IN ONE PLACE AND CLEAN"}}