{"cell_type":{"f8d56109":"code","ac0fd82b":"code","12755163":"code","5be7f590":"code","ef6e57e8":"code","9e28a535":"code","b139c3e7":"code","d399d2e4":"code","0e5cbcc1":"code","3033bb20":"code","d26474ab":"code","4e62dd3f":"code","ad969e93":"code","87de0b42":"code","a01dcb8e":"code","61c302c6":"code","4c4341f1":"code","18bfa5d6":"code","93a9c723":"code","d9a19e4d":"code","6ed8459f":"code","7f7e695f":"code","0493f0e9":"code","8b36b929":"code","aed59620":"code","9164c578":"code","1f924584":"code","18a70c9b":"code","bf49ac8e":"code","fe96c1e4":"code","2a56207e":"code","7f5b915a":"code","db95bf2d":"code","a0119e85":"code","7e17ecfa":"code","125f937c":"code","63552559":"code","de5bed1c":"code","dd7bf1ae":"code","9a45b4df":"code","f1222c8f":"code","4b52db7a":"code","758ed2dc":"code","da31ffc8":"code","c116e385":"code","6dca0bcd":"code","6f6bbd19":"code","7899f20b":"code","1c7b9efd":"code","208a8d10":"markdown","7d92eb22":"markdown","161fb5ac":"markdown","03eb935d":"markdown","038966cb":"markdown","3ef9b9b7":"markdown"},"source":{"f8d56109":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ac0fd82b":"# All imports\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom pandas.plotting._matplotlib import scatter_matrix as sm\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder","12755163":"# Loading Data into data frame\nheart_df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","5be7f590":"# Displaying top 5 rows from the data frame\nheart_df.head() #defaulted to 5 rows","ef6e57e8":"# checking the total number of columns and rows\nheart_df.shape","9e28a535":"# From the above output, we know that there are 303 records(rows) and 14 attributes\/features(columns)","b139c3e7":"# Alternatively we can use the below command for checking the same\nheart_df","d399d2e4":"# We can see that some rows are masked which prevents the full view of the data.","0e5cbcc1":"# checking for duplicates\nheart_df.drop_duplicates() # drops duplicates, if any\nheart_df.shape","3033bb20":"# We see that the number of rows and columns remain same, we confirmed that there were no duplicates","d26474ab":"# Let's check for total null values in each column\nheart_df.isnull().sum()","4e62dd3f":"# Let's check for the full information about the dataset including non-nulls and \n# data-types\nheart_df.info()","ad969e93":"# Since data-types are int64 and float64, it's presumed that there are no non-numerical\n# data present in the given dataset. Yet to confirm it, let's check for it with below command\nheart_df.isna().sum()","87de0b42":"# Now, let's describe the data to see the IQR, mean, standard deviation, min, max values\nheart_df.describe()","a01dcb8e":"# Let's rename the columns for better understanding\nheart_df = heart_df.rename({'cp': 'chest_pain', 'trestbps': 'resting_blood_pressure', \n                            'chol': 'cholestrol', 'fbs': 'fasting_blood_sugar',\n                            'restecg': 'rest_ECG', 'thal': 'thallium_stress', 'target': 'is_heart_disease'}, axis=1)","61c302c6":"# Let's check the unique values in each column\nheart_df.nunique()","4c4341f1":"for c in heart_df.columns:\n    print (\"----\", c ,\"---\")\n    print (heart_df[c].value_counts())","18bfa5d6":"# From above output, we found that some columns contain categorical data.\n# let's encode the categorical data.\nheart_df.head()","93a9c723":"plt.figure(figsize=(15,7))\nax = sns.heatmap(heart_df.corr(),annot=True, cmap=\"YlGnBu\", linewidths=.5)","d9a19e4d":"# From the heatmap above, we understand that chest_pain, thalach and slope are highly correlated.","6ed8459f":"# Boxplot visualization\nplt.figure(figsize=(15,7))\nheart_df.boxplot()\nplt.xticks(rotation=45)\nplt.show()","7f7e695f":"# Out of 14 features, 7 features seems to have outliers in the data set.\n# cholestrol contains values that are higher when compared to other features. ","0493f0e9":"# Clear visualization of the outliers\nplt.title('Box plot summary of cholestrol')\nheart_df['cholestrol'].plot(kind='box',figsize=(6,8))\nplt.yticks(range(200,600,50))\nplt.grid()\nplt.show()","8b36b929":"# outlier range for cholestrol\nupper_limit_chol = heart_df['cholestrol'].quantile(0.99)\nlower_limit_chol = heart_df['cholestrol'].quantile(0.01)\nprint(\"upper_limit_chol\",upper_limit_chol)\nprint(\"lower_limit_chol\",lower_limit_chol)","aed59620":"#trimming outliers in cholestrol\nheart_df = heart_df[(heart_df['cholestrol'] <= upper_limit_chol) & (heart_df['cholestrol'] >= lower_limit_chol)]\nheart_df.describe()","9164c578":"# Visualizing the boxplot for trimmed data\nplt.title('Box plot summary of cholestrol')\nheart_df['cholestrol'].plot(kind='box',figsize=(6,8))\nplt.yticks(range(200,600,50))\nplt.grid()\nplt.show()","1f924584":"# As the trimmed data looks good, let's move on to the next attribute to be handled.\nplt.title('Box plot summary of thalach')\nheart_df['thalach'].plot(kind='box',figsize=(6,8))\nplt.grid()\nplt.show()","18a70c9b":"# outlier range for thalach\nupper_limit_thalach = heart_df['thalach'].quantile(0.99)\nlower_limit_thalach = heart_df['thalach'].quantile(0.01)\nprint(\"upper_limit_thalach\",upper_limit_thalach)\nprint(\"lower_limit_thalach\",lower_limit_thalach)","bf49ac8e":"#trimming outliers in thalach\nheart_df = heart_df[(heart_df['thalach'] <= upper_limit_thalach) & (heart_df['thalach'] >= lower_limit_thalach)]\nheart_df.describe()","fe96c1e4":"# As the trimmed data looks good, let's move on to the next attribute to be handled.\nplt.title('Box plot summary of thalach')\nheart_df['thalach'].plot(kind='box',figsize=(6,8))\nplt.grid()\nplt.show()","2a56207e":"# So, we have trimmed thalach too!\n# Visualizing the boxplot for trimmed data\nplt.title('Box plot summary of resting_blood_pressure')\nheart_df['resting_blood_pressure'].plot(kind='box',figsize=(6,8))\nplt.grid()\nplt.show()","7f5b915a":"# checking outliers in resting_blood_pressure\nupper_limit_RBP = heart_df['resting_blood_pressure'].quantile(0.99)\nlower_limit_RBP = heart_df['resting_blood_pressure'].quantile(0.01)\nprint(\"upper_limit_RBP\",upper_limit_RBP)\nprint(\"lower_limit_RBP\",lower_limit_RBP)","db95bf2d":"#trimming outliers in resting_blood_pressure\nheart_df = heart_df[(heart_df['resting_blood_pressure'] <= upper_limit_RBP) \n  & (heart_df['resting_blood_pressure'] >= lower_limit_RBP)]\nheart_df.describe()","a0119e85":"# As the trimmed data looks good, let's move on to the next attribute to be handled.\nplt.title('Box plot summary of Resting_Blood_Pressure')\nheart_df['resting_blood_pressure'].plot(kind='box',figsize=(6,8))\nplt.grid()\nplt.show()","7e17ecfa":"# Now the data looks more clean! Let's check for skewness!","125f937c":"#finding the distribution of the data using Visualization\nheart_df.plot(kind='density', subplots=True, layout=(7,2),sharex=False,sharey=False,fontsize=15, figsize=(20,20))\nplt.suptitle(\"PDF\", y=1.00, fontweight='bold', fontsize=20)\nplt.subplots_adjust(hspace=1.5,wspace=0.5)\nplt.show()","63552559":"heart_df.diff().hist(color=\"k\", alpha=0.5, layout=(7,2),sharex=False,sharey=False, figsize=(15,15));\nplt.suptitle(\"Histogram\", y=1.00, fontweight='bold', fontsize=20)\nplt.subplots_adjust(hspace=0.5,wspace=0.5)\nplt.show()","de5bed1c":"# Skipping standardization and normalization as we have already treated the outliers. \n# Also the data is in the range easy to handle.","dd7bf1ae":"heart_df.head()","9a45b4df":"\n## ***************Next Steps: perform Feature Selection*************** ##\nX_cat = heart_df[['sex','chest_pain','fasting_blood_sugar','rest_ECG','exang','slope','ca','thallium_stress']] #categorical variables\nX_num = heart_df.drop(X_cat.columns, axis=1)\nX_num.drop('is_heart_disease',axis = 1,inplace=True)\n\ny = heart_df.iloc[:,-1]\nX_cat ","f1222c8f":"X_num # numerical variables","4b52db7a":"## *************** Method 1a:Feature Selection - chi2 *************** ##\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#apply SelectKBest class to extract top 10 best features using chi2\nBestFeatures = SelectKBest(score_func=chi2, k='all')\nfit = BestFeatures.fit(X_cat,y)\nchi2_df_scores = pd.DataFrame(fit.scores_)\nchi2_df_columns = pd.DataFrame(X_cat.columns)\n\n#concatenating df_scores and df_columns dataframes for better visualization\nchi2_feature_Scores = pd.concat([chi2_df_columns,chi2_df_scores],axis=1) # feature scores using chi2\nchi2_feature_Scores.columns = ['Ranked Attributes\/Features','Score'] \nprint('##TOP chi2 FEATURES')\nchi2_feature_Scores # Score value is directly proportional to the feature importance","758ed2dc":"## *************** Method 1b:Feature Selection - ANOVA *************** ##\nfrom sklearn.feature_selection import f_classif\n\n#apply SelectKBest class to extract top 10 best features using ANOVA F-measure via f_classif\nBestFeatures = SelectKBest(score_func=f_classif, k='all')\nfit = BestFeatures.fit(X_num,y)\nanova_df_scores = pd.DataFrame(fit.scores_)\nanova_df_columns = pd.DataFrame(X_num.columns)\n\n#concatenating anova_df_scores and anova_df_columns dataframes for better visualization\nanova_feature_Scores = pd.concat([anova_df_columns,anova_df_scores],axis=1) # feature scores obtained by ANOVA F-measure\nanova_feature_Scores.columns = ['Ranked Attributes\/Features','Score'] \nprint('##TOP Anova FEATURES')\nanova_feature_Scores # Score value is directly proportional to the feature importance","da31ffc8":"frames = [chi2_feature_Scores,anova_feature_Scores] #features from chi2 and anova with ranking\nresult = pd.concat(frames) # concatinating features from chi2 and anova with ranking\nresult #features in score order","c116e385":"chi2_anova_top_10 = result.nlargest(10,'Score') # combined top_10 features \nprint('##TOP 10 FEATURES - chi2_anova_top_10')\nchi2_anova_top_10","6dca0bcd":"X=heart_df[['oldpeak','ca','thalach','chest_pain','exang','age','slope','sex','thallium_stress','resting_blood_pressure']]\nY=heart_df['is_heart_disease']\nX","6f6bbd19":"Y #target variable is_heart_disease","7899f20b":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,mean_squared_error, classification_report,accuracy_score,f1_score,precision_score,recall_score,roc_curve,roc_auc_score\n\nmodel_RF = RandomForestClassifier(n_estimators=30)\ntrain_X, test_X, train_y, test_y = train_test_split(X, Y,test_size=0.2,random_state=100)\nmodel_RF.fit(train_X, train_y)\nprediction_RF = model_RF.predict(test_X)\n\n#Score calculation for RandomForest\nacc_RF = accuracy_score(test_y,prediction_RF)\ntrainscore_RF =  model_RF.score(train_X,train_y)\ntestscore_RF =  model_RF.score(test_X,test_y) \nconf_matrix_RF = confusion_matrix(test_y,prediction_RF)\n\nprint(\"Training Accuracy: \", trainscore_RF*100,'\\n')\nprint(\"Test Accuracy: \", testscore_RF*100,'\\n')\nprint(\"Classification report: \\n\", classification_report(test_y,prediction_RF))\nprint('Confusion Matrix : \\n', conf_matrix_RF)\n    \n# drawing confusion matrix\nsns.heatmap(conf_matrix_RF, center = True , annot=True, fmt=\"d\" ,cmap=\"RdYlBu\")\nplt.show()","1c7b9efd":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel_KNN = KNeighborsClassifier(n_neighbors = 5)\nmodel_KNN.fit(train_X,train_y)\nprediction_KNN = model_KNN.predict(test_X)  # Make predictions\n\n#Score calculation for KNN\nacc_KNN = accuracy_score(test_y,prediction_KNN)\ntrainscore_KNN =  model_KNN.score(train_X,train_y)\ntestscore_KNN =  model_KNN.score(test_X,test_y) \nconf_matrix_KNN = confusion_matrix(test_y,prediction_KNN)\n\nprint(\"Training Accuracy: \", trainscore_KNN*100,'\\n')\nprint(\"Test Accuracy: \", testscore_KNN*100,'\\n')\nprint(\"Classification report: \\n\", classification_report(test_y,prediction_KNN))\nprint('Confusion Matrix : \\n', conf_matrix_KNN)\n    \n# drawing confusion matrix\nsns.heatmap(conf_matrix_KNN, center = True , annot=True, fmt=\"d\" ,cmap=\"RdYlBu\")\nplt.show()","208a8d10":"#Feature Selection","7d92eb22":"### Model - KNN Clustering","161fb5ac":"###Model - RandomForest","03eb935d":"# Data Exploration and Visualization","038966cb":"### We have data of both categorical and numerical inputs and the output is categorical. So let's apply chi2 and ANOVA feature selection techniques to select the top 10 features. We will use the top 10 features obtained from this selection technique to the models.","3ef9b9b7":"# Data pre-processing - outlier cleaning"}}