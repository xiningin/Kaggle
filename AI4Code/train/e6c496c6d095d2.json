{"cell_type":{"0da0db16":"code","0e972d71":"code","59f6a8c1":"code","170da0f2":"code","73241294":"code","eb7b2bc6":"code","8256a5ad":"code","25091d9a":"code","5854372b":"code","7396528f":"code","ee6e3445":"code","bf1c8f74":"code","261e448a":"code","433a9698":"code","35446d46":"code","1bb03dc9":"code","7ba0a376":"code","6134a7e2":"code","70bd0448":"code","d2726d58":"code","7d971945":"code","79325151":"code","ae19590c":"code","8a15e648":"markdown","ce51f761":"markdown","c82ac065":"markdown","ba9861c2":"markdown","daa3821c":"markdown","44d01974":"markdown","26584562":"markdown"},"source":{"0da0db16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0e972d71":"df = pd.read_csv('..\/input\/Admission_Predict.csv')","59f6a8c1":"df.head(10)","170da0f2":"# Check null values in the dataset - pretty clean\ndf.isnull().sum()","73241294":"df.columns","eb7b2bc6":"df.describe()\ndf_features = df.drop(['Chance of Admit ','Serial No.'],axis=1)\ndf_target = df['Chance of Admit ']","8256a5ad":"# Prepare training and test dataset\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(df_features,df_target,test_size=0.2,random_state=123)","25091d9a":"print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)","5854372b":"## We can see, data is not standardized, it's not required if we are building decision tress\ndt = DecisionTreeRegressor(criterion='mse',max_depth=2,splitter='best',min_samples_split=2)\ndt_fit = dt.fit(X_train,Y_train)","7396528f":"from sklearn.metrics import mean_squared_error,r2_score\ny_pred = dt_fit.predict(X_test)\nrmse = np.sqrt(mean_squared_error(Y_test, y_pred))\nprint(rmse,r2_score(Y_test,y_pred))","ee6e3445":"predictions = pd.DataFrame(y_pred,Y_test).reset_index()\npredictions.columns = ['Predictions','Actual']\npredictions.head(20)","bf1c8f74":"dt.feature_importances_","261e448a":"df_features.columns","433a9698":"#### Most Important Features 1) CGPA 2) GRE Score","35446d46":"from sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(dt, X_train, Y_train, cv=10)","1bb03dc9":"np.mean(cv_scores)","7ba0a376":"from numpy.random import randint\nparams = {\"max_depth\":[1,5],\n         \"max_features\":randint(1,7,size=4), # Use maximum 4 features \n         \"min_samples_leaf\":randint(1,4,size=3)}","6134a7e2":"from sklearn.model_selection import RandomizedSearchCV\nrandom_search = RandomizedSearchCV(estimator = dt,param_distributions = params,n_iter=15,cv=5)","70bd0448":"# Fit the random search on dataset\nrandom_search.fit(df_features, df_target)","d2726d58":"print(random_search.best_estimator_)","7d971945":"print(random_search.best_params_)","79325151":"print(random_search.best_score_)","ae19590c":"#### We get an improved accuracy over previous cross validation score of 67% to 72%\n#### We can train a random forest regressor to improve overall accuracy","8a15e648":"#### Main parameters to tune in decision trees are \n##### 1. max_depth - the depth of the tree - can be checked by comparing test and train accuracy\n##### 2.min_samples_tree \n##### 3. min sample in leaf","ce51f761":"### See Important Features","c82ac065":"#### As we can see th model doesn't perform well with our specified set of parameters","ba9861c2":"### The following notebook builds a decision tree regression model with hyperparameter tuning of the tree implemented at the last part","daa3821c":"\n### Cross Validation","44d01974":"### Hyperparameter Tuning","26584562":"#### See predictions"}}