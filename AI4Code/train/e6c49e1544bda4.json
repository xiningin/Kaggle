{"cell_type":{"bdd1f504":"code","33a26064":"code","cad9a7dd":"code","71567edd":"code","db229c1f":"code","ff48451c":"code","23dc4d70":"code","617d6dd8":"code","723f7247":"code","afa9fef4":"code","335140eb":"code","03ed45e8":"code","7c810096":"markdown"},"source":{"bdd1f504":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33a26064":"#We import tensorflow library for deep neural network.\nimport seaborn as sns\nimport tensorflow as tf","cad9a7dd":"#we prepare our directories\nmother_directory = os.getcwd()\ndata_directory = \"..\/input\/early-diabetes-classification\/diabetes_data.csv\"","71567edd":"#This is a function that changes the catregorical columns into one hot encoded columns.\ndef categorical_to_binary(data,categorical_dict):\n    s\u00fctun_list = []\n    for s\u00fctun in data.columns:\n        if type(data[s\u00fctun].iloc[0]) is str:\n            \u00e7e\u015fitler = data[s\u00fctun].unique()\n            s\u00fctun_list.append(s\u00fctun)\n            for ka\u015f in range(len(\u00e7e\u015fitler)-1):\n                categorical_dict[\u00e7e\u015fitler[ka\u015f]] = []\n                for j in range(len(data[s\u00fctun])):\n                    if data[s\u00fctun].iloc[j] == \u00e7e\u015fitler[ka\u015f]:\n                        categorical_dict[\u00e7e\u015fitler[ka\u015f]].append(1)\n                    else:\n                        categorical_dict[\u00e7e\u015fitler[ka\u015f]].append(0)\n\n    return s\u00fctun_list","db229c1f":"#Here we grab our data into a dataframe.\ndata = pd.read_csv(data_directory,delimiter=\";\")","ff48451c":"data.head()","23dc4d70":"#we turn categorical into one hot encoded columns\ncategorical_dict = {}\nliste_s\u00fctun = categorical_to_binary(data,categorical_dict)","617d6dd8":"#we change the data into one hot encoded\nfor name in categorical_dict.keys():\n    data[name] = categorical_dict[name]\nfor k in liste_s\u00fctun:\n    data = data.drop(k,axis=1)","723f7247":"#Heat map to look at the correlations between the data categories\ntc = data.corr()\nsns.heatmap(tc,annot = False,cmap=\"coolwarm\")","afa9fef4":"data.info()","335140eb":"#Here we prepare data adn train it on the neural network we defined.\nmalum = data[\"obesity\"].copy()\ndata = data.drop(\"obesity\",axis=1)\nX = data.iloc[:,:].values\ny = malum.values\n\n#Here split our data into train_test datasets\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.15,random_state = 10)\nprint(X_train)\nprint(y_train)\nyedek = y_test.copy()\n\n#here we standardize the data into the same interval so that a category wouldnt overrepresent in the decision process.\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\n\n#We define our neural network and compile it\nclassifier = tf.keras.models.Sequential()\nclassifier.add(tf.keras.layers.Dense(units = 128, activation = \"relu\"))\nclassifier.add(tf.keras.layers.Dropout(0.2))\nclassifier.add(tf.keras.layers.Dense(units = 32, activation = \"relu\"))\nclassifier.add(tf.keras.layers.Dropout(0.1))\nclassifier.add(tf.keras.layers.Dense(units = 1,activation = \"sigmoid\"))\nclassifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\" , metrics=[\"accuracy\"])\n\n#Here we train our model.\nclassifier.fit(X_train,y_train,epochs = 100,validation_data = (X_test,y_test))\n#This the inference phase.We try our model on test data.\ny_pred = classifier.predict(X_test)\n\ny_pred = (y_pred > 0.5)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n#With this confusion matrix \ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\naccuracy_score(y_test,y_pred)","03ed45e8":"#This the inference phase.We try our model on test data.\ny_pred = classifier.predict(X_test)\n\ny_pred = (y_pred > 0.5)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n#With this confusion matrix \ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\naccuracy_score(y_test,y_pred)","7c810096":"#This is the Citation of the data that is sed in this notebook.\nCitation\nIslam M.M.F., Ferdousi R., Rahman S., Bushra H.Y. (2020) Likelihood Prediction of Diabetes at Early Stage Using Data Mining Techniques. In: Gupta M., Konar D., Bhattacharyya S., Biswas S. (eds) Computer Vision and Machine Intelligence in Medical Image Analysis. Advances in Intelligent Systems and Computing, vol 992. Springer, Singapore. https:\/\/doi.org\/10.1007\/978-981-13-8798-2_12\n\nLicense\nLicense was not specified, yet data is free and public.\n\nSplash banner\nIcon by Freepik."}}