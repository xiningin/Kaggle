{"cell_type":{"238ffd08":"code","04cb43d4":"code","5b48a88a":"code","50699bd0":"code","71a9819a":"code","dc9c3e33":"code","c22f199b":"code","4072a459":"code","76fbc8a1":"code","32ddc396":"code","f6f5bce9":"code","2f271476":"code","8b9a566b":"code","b6577f94":"code","6636866e":"code","f6fe626a":"code","faddaeb2":"code","5f85b9e9":"code","242f1ed0":"markdown","c4dcc66d":"markdown","475f031d":"markdown","6d8ef7ba":"markdown","af06e5e3":"markdown","b28c1c33":"markdown","629fdc51":"markdown","77d837d4":"markdown","c36db24f":"markdown","aa9785bb":"markdown","113f74cd":"markdown","bb263ea2":"markdown"},"source":{"238ffd08":"import pandas as pd\n\ndf_train = pd.read_csv('..\/input\/figure-eight-medical-sentence-summary\/train.csv')\ndf_test = pd.read_csv('..\/input\/figure-eight-medical-sentence-summary\/test.csv')\n\ndf_train.head(20)","04cb43d4":"df_train['relation'].unique()","5b48a88a":"import numpy as np\n\nx_train = df_train['sentence'].as_matrix()\ny_train = df_train['relation'].as_matrix()\n\nfrom sklearn.preprocessing import label_binarize\n\ny_train = label_binarize(y_train, classes=df_train['relation'].unique())\n\nprint(x_train[:10])\nprint(y_train[:10])","50699bd0":"x_train_sub_list = []\n\nfor i, row in df_train.iterrows():\n    pos_t1 = row['sentence'].find(row['term1'])\n    pos_t2 = row['sentence'].find(row['term2'])    \n    \n    if pos_t1 < pos_t2:\n        len_t1 = len(row['term1'])\n        x_train_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n    else:\n        len_t2 = len(row['term2'])\n        x_train_sub_list.append(row['sentence'][pos_t2+len_t2:pos_t1])\n        \nx_train_sub = np.array(x_train_sub_list)\n\nprint(x_train_sub[:10])","71a9819a":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\ndef my_tokenizer_pos(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    return [pos[1] for pos in pos_tags]\n\n# testando nossa fun\u00e7\u00e3o:\n\nfor x in x_train_sub[:10]:\n    print(my_tokenizer_pos(x))","dc9c3e33":"stopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer_bow(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas","c22f199b":"from sklearn.decomposition import TruncatedSVD\n\nclass SVDDimSelect(object):\n    def fit(self, X, y=None):        \n        try:\n            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]\/2))\n            self.svd_transformer.fit(X)\n        \n            cummulative_variance = 0.0\n            k = 0\n            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:\n                cummulative_variance += var\n                if cummulative_variance >= 0.5:\n                    break\n                else:\n                    k += 1\n                \n            self.svd_transformer = TruncatedSVD(n_components=k)\n        except Exception as ex:\n            print(ex)\n            \n        return self.svd_transformer.fit(X)\n    \n    def transform(self, X, Y=None):\n        return self.svd_transformer.transform(X)\n        \n    def get_params(self, deep=True):\n        return {}","4072a459":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport scipy\n\nclf = OneVsRestClassifier(LogisticRegression(random_state=0, solver='lbfgs', \n                                             multi_class='multinomial'))\n\n\nmy_pipeline = Pipeline([\n                        ('union', FeatureUnion([('bow', TfidfVectorizer(tokenizer=my_tokenizer_bow)),\\\n                                                ('pos', Pipeline([('pos-vect', CountVectorizer(tokenizer=my_tokenizer_pos)), \\\n                                                         ('pos-tfidf', TfidfTransformer())]))\n                                               ])),\\\n                       ('svd', SVDDimSelect()), \\\n                       ('clf', clf)])\n\npar = {'clf__estimator__C' : np.logspace(-4, 4, 20)}\n\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='f1_weighted', n_jobs=1, n_iter=20)","76fbc8a1":"print(x_train_sub.shape)\nprint(y_train.shape)\n\nprint(x_train_sub[:10])\nprint(y_train[:10])","32ddc396":"hyperpar_selector.fit(X=x_train_sub[:500], y=y_train[:500])","f6f5bce9":"x_test = df_test['sentence'].as_matrix()\ny_test = df_test['relation'].as_matrix()\n\ny_test = label_binarize(y_test, classes=df_train['relation'].unique())\n\nx_test_sub_list = []\n\nfor i, row in df_test.iterrows():\n    pos_t1 = row['sentence'].find(row['term1'])\n    len_t1 = len(row['term1'])\n    \n    pos_t2 = row['sentence'].find(row['term2'])    \n    \n    \n    if pos_t1 < pos_t2:\n        len_t1 = len(row['term1'])\n        x_train_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n    else:\n        len_t2 = len(row['term2'])\n        x_train_sub_list.append(row['sentence'][pos_t2+len_t2:pos_t1])\n    \n\nx_test_sub = np.array(x_test_sub_list)","2f271476":"y_predicted = hyperpar_selector.predict(x_test)","8b9a566b":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_predicted, target_names=df_train['relation'].unique()))","b6577f94":"f = open('..\/input\/dbpedia-with-entity-relations-in-portuguese\/DBpediaRelations-PT-0.2.txt', 'r')\n\nline = f.readline()\n\ndf_dict = {'sentence':[], 'term1':[], 'term2':[], 'relation':[]}\n\nwhile line:    \n    if len(line) > 1:\n        line_vals = line.split(':')\n        if len(line_vals) >= 2:                        \n            if line_vals[0].strip() == 'SENTENCE':\n                df_dict['sentence'].append(' '.join(line_vals[1:]))\n            elif line_vals[0].strip() == 'ENTITY1':\n                df_dict['term1'].append(' '.join(line_vals[1:]))\n            elif line_vals[0].strip() == 'ENTITY2':\n                df_dict['term2'].append(' '.join(line_vals[1:]))\n            elif line_vals[0].strip() == 'REL TYPE':\n                df_dict['relation'].append(' '.join(line_vals[1:]))\n    line = f.readline()\n    \nf.close()","6636866e":"import pandas as pd\ndf = pd.DataFrame.from_dict(df_dict)\n\ndf.head(10)\n#df.describe()","f6fe626a":"!python -m spacy download pt_core_news_sm","faddaeb2":"import pt_core_news_sm\n\nnlp = pt_core_news_sm.load()","5f85b9e9":"import nltk\nfrom nltk.stem import RSLPStemmer\nstopwords_list = stopwords.words('portuguese')\n\nstemmer = nltk.stem.RSLPStemmer()\nstemmer.stem('gatinho')","242f1ed0":"<p>Agora vamos definir duas fun\u00e7\u00f5es de tokeniza\u00e7\u00e3o: uma para tokenizar bag-of-words e outra para tokenizar os POS tags<\/p>","c4dcc66d":"<p>\n<\/p>","475f031d":"<p>Agora vamos treinar os algoritmos<\/p>","6d8ef7ba":"<p>Transformamos as senten\u00e7as e tipos de relacionamento em matrizes numpy. Tamb\u00e9m binarizamos os r\u00f3tulos dos relacionamentos, para utilizarmos no nosso classificador logo mais.<\/p>","af06e5e3":"<p>Como n\u00e3o temos os tipos das entidades, mas sabemos que se trata de nomes de medicamentos e doen\u00e7as na maioria dos casos, n\u00e3o utilizaremos o tipo das entidades como atributos, mas utilizaremos os POS tags de todas as palavras entre as entidades. Vamos criar outras matrizes com esses atributos. <\/p>\n\n<p>Para os POS Tags, vamos fazer algo parecido ao chunking sugerido em https:\/\/courses.cs.washington.edu\/courses\/cse517\/13wi\/slides\/cse517wi13-RelationExtraction.pdf, mas ao inv\u00e9s de usar chunking, vamos criar 3-grams desses POS tags para simplificar.<\/p>","b28c1c33":"<p>Vamos reaproveitar a classe para sele\u00e7\u00e3o de atributos usando SVD.<\/p>","629fdc51":"<p>Agora vamos criar nosso Pipeline. Em resumo, vamos usar o TFIDF Vectorizer e o nosso POS Tagger em paralelo, e depois juntar os atributos para redimensionar usando o SVD.<\/p>","77d837d4":"<h2>2. M\u00e9todos para identifica\u00e7\u00e3o de relacionamentos<\/h2>\n\n<p>Os m\u00e9todos mais comuns para identificar relacionamentos entre entidades s\u00e3o:<\/p>\n\n* **Padr\u00f5es codificados manualmente**: Basta criar padr\u00f5es usando express\u00f5es regulares, por exemplo, para identificar que duas entidades se relacionam. Assim como em \"X mora em Y\" pode ser um padr\u00e3o para identificar o relacionamento (X, mora_em, Y) entre uma entidade X do tipo PESSOA e uma entidade Y do tipo LOCALIDADE.\n* **M\u00e9todos bootstraping**: Com poucos dados, procura por ocorr\u00eancias de duas entidades em que j\u00e1 se conhece o relacionamento (no Google, por exemplo), e usa os modelos encontrados como modelos para o mesmo relacionamento entre outras entidades.\n* **M\u00e9todos supervisionados**: Com base num corpus anotado com relacionamentos, criar modelos que 1) detecte quando existe o relacionamento entre duas entidades e 2) classifique o tipo de relacionamento entre elas. \n\n<p>Nesta aula, vamos ver um m\u00e9todo supervisionado para classificar o relacionamento entre entidades, usando t\u00e9cnicas que j\u00e1 utilizamos em aulas anteriores.<\/p>\n\n<p>Para isso, utilizaremos alguns atributos mais comuns para o problema, como:<\/p>\n\n* Bag of Words\/LSA\n* Flags indicadores dos tipos das entidades\n* N\u00famero de palavras entre as duas entidades\n* Flag indicando se o texto de uma entidade \u00e9 composto pelo texto da outra\n* POS tags\n* etc\n\n","c36db24f":"<p><b>Exerc\u00edcio 7:<\/b> Treine um modelo de extra\u00e7\u00e3o de relacionamentos em Portugu\u00eas, utilizando o corpus extra\u00eddo do DBPedia e com relacionamentos entre pares de entidades anotadas.<\/p>","aa9785bb":"<h2>3. Criando um Modelo Supervisionado<\/h2>\n<p> Vamos utilizar o corpus [Figure Eight: Medical Sentence Summary](https:\/\/www.kaggle.com\/kmader\/figure-eight-medical-sentence-summary), que possui diversas senten\u00e7as extra\u00eddas do PubMed, com entidades anotadas, assim como seus tipos de relacionamento.<\/p>","113f74cd":"<h1 align=\"center\"> Aplica\u00e7\u00f5es em Processamento de Linguagem Natural <\/h1>\n<h2 align=\"center\"> Aula 07 - Extra\u00e7\u00e3o de Informa\u00e7\u00e3o (Parte 2)<\/h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.<\/h3>","bb263ea2":"<h2>1. Extra\u00e7\u00e3o de Relacionamentos<\/h2>\n<p>A extra\u00e7\u00e3o de relacionamentos consiste em identificar a liga\u00e7\u00e3o entre diversas entidades nomeadas no texto. Isso envolve mencionar qual \u00e9 o tipo da liga\u00e7\u00e3o entre duas entidades. Considere o exemplo de senten\u00e7a abaixo.<\/p>\n\n<p>\"Carlos Alberto de Nogueira \u00e9 o morador mais antigo da Rua Pra\u00e7a da Alegria.\"<\/p>\n\n<p>Temos as entidades:<\/p>\n\n* Carlos Alberto de Nogueira (PESSOA)\n* Rua Pra\u00e7a da Alegria (LOCALIDADE)\n\n<p>Essas mesmas entidades est\u00e3o relacionadas da seguinte forma:<\/p>\n\n[Carlos Alberto de Nogueira (PERSON); morador mais antigo; Rua Pra\u00e7a da Alegria (LOCALIDADE)]\n\n\n<p>Um dos mais famosos exemplos de sistema de reconhecimento \u00e9 o [Never-Ending Language Learning (NELL)](http:\/\/rtw.ml.cmu.edu\/), projeto desenvolvido pela Universidade Carnigie Mellon, com participa\u00e7\u00e3o do Google e inclusive de pesquisadores brasileiros financiados pelo CNPq. Esse projeto consiste em extrair relacionamentos de milh\u00f5es de p\u00e1ginas da internet, criando uma gigantesca base de conhecimento.<\/p>"}}