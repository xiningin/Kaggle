{"cell_type":{"fe8bc45d":"code","a207ab32":"code","56c26e17":"code","c0f43e51":"code","021285f2":"code","8561dc0b":"code","a84d995c":"code","5dc4d6cd":"code","fc071276":"code","4e00a466":"code","cb9d46e4":"code","89ad3c2c":"code","a65d3a42":"code","4a3e1b9d":"code","263c5366":"markdown","5e7cb5cd":"markdown","fdc4faeb":"markdown","1413bdb6":"markdown","596e6525":"markdown"},"source":{"fe8bc45d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n\nimport shap\nimport gc\n\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a207ab32":"# import pandas as pd\ncompas_scores_raw = pd.read_csv(\"..\/input\/compass\/compas-scores-raw.csv\")\ncox_violent_parsed = pd.read_csv(\"..\/input\/compass\/cox-violent-parsed.csv\")\ncox_violent_parsed_filt = pd.read_csv(\"..\/input\/compass\/cox-violent-parsed_filt.csv\")\n","56c26e17":"print (type(compas_scores_raw))\ncompas_scores_raw.head()","c0f43e51":"TARGET_COL = \"Two_yr_Recidivism\"","021285f2":"df = pd.read_csv(\"..\/input\/compass\/propublicaCompassRecividism_data_fairml.csv\/propublica_data_for_fairml.csv\")\n\nprint(df.shape)\ndisplay(df.columns)\ndf.head()","8561dc0b":"data = df.drop([TARGET_COL],axis=1)\ny = df[TARGET_COL]","a84d995c":"# Here is a way to select these columns using the column names\n    \n#feature_columns = ['Number_of_Priors', 'score_factor','Age_Above_FourtyFive', 'Age_Below_TwentyFive', 'African_American','Asian', 'Hispanic', 'Native_American', 'Other', 'Female',       'Misdemeanor']\nfeature_columns = ['Number_of_Priors', 'score_factor','Age_Above_FourtyFive', 'Age_Below_TwentyFive', 'Misdemeanor']\n\ndata = df[feature_columns].values\ny = df['Two_yr_Recidivism'].values","5dc4d6cd":"#Create train and validation set\ntrain_x, valid_x, train_y, valid_y = train_test_split(data, y, test_size=0.25, shuffle=True, stratify=y, random_state=42)","fc071276":"print (\"test\")\nprint (type(train_x))\nprint (\"Shape of training input : \",train_x.shape)\nprint (\"Shape of training output : \",train_y.shape)","4e00a466":"from sklearn.neighbors import KNeighborsClassifier\n# Set up the K-Nearest neighbor model using the k nearest neighbors. Change the value of n_neighbors\nknn_model = KNeighborsClassifier(n_neighbors=5)\n# Train the model on the iris data\nknn_model.fit(train_x, train_y)\nscore = knn_model.score(valid_x, valid_y)\nprint (\"The score for this model is \", score)","cb9d46e4":"from sklearn.svm import SVC\n\n# Set up SVM model with a given kernel and c parameter\nsvm_model = SVC(C=1.0, kernel='linear')         # linear SVM\n#svm_model = SVC(C=10.0, kernel='rbf')           # non-linear SVM\n\n# Train the model on the iris data\nsvm_model.fit(train_x, train_y)\nscore = svm_model.score(valid_x, valid_y)\nprint (\"The score for this model is \", score)","89ad3c2c":"# This is the actual score used in the origianl notebook\ny_pred = svm_model.predict(valid_x)\nscore = roc_auc_score(valid_y, y_pred)\nprint(\"Overall AUC on validation: {:.3f}\" .format(score))","a65d3a42":"from sklearn.tree import DecisionTreeClassifier\nDT_model = DecisionTreeClassifier()\n\n# Train the model on the iris data\nDT_model.fit(train_x, train_y)\nscore = DT_model.score(valid_x, valid_y)\nprint (\"The score for this model is \", score)","4a3e1b9d":"from sklearn import tree\n\ntree.plot_tree(DT_model, max_depth=3);","263c5366":"### Getting started with feature importance and fairness using ML & SHAP\n\n\n* We'll look at feature importance according to ML models. \n* We can use multiple methods - e.g. permutation importance, shapley, and the  difference in feature importance from different models","5e7cb5cd":"## We'll start with the naive fairML subset of the data. Very simple\n* **Target column**: `Two_yr_Recidivism` = recividism (any) within 2 years\n* Note that we have fewer variables and features here.\n","fdc4faeb":"# Section 2: Support Vector Machine (SVM)\n\nSVM is another algorithm for classifying data. It tries to divide the data up using lines, sometimes straight linear lines and sometimes curved lines.\n\nSVM tries to find the best lines - actually, a plane in multiple dimensions - to divide the data up into the known categories.\n## Linear SVM Classification\n\n\n","1413bdb6":"### Gradient Boosting Classsifer\nThe original code used LightGBC to classify the data.  \nhttps:\/\/towardsdatascience.com\/understanding-gradient-boosting-machines-9be756fe76ab\n\nWe can substitute other learning methods in place of this. I added KNN, SVM and Decision Trees below.\n","596e6525":"# Section 3: Decision Trees \nThis classification method tries to break the classification task into a series of decisions structured as a tree.\n> Like SVMs, Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are powerful algorithms, capable of fitting complex datasets.\n- From *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems*, 2nd Edition by Aur\u00e9lien G\u00e9ron, ISBN-13: 978-1492032649"}}