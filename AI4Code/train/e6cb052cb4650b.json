{"cell_type":{"8ea59283":"code","7b4827d6":"code","f5e0db47":"code","22dabf7e":"code","0ff622b2":"code","6b1bff93":"code","f2853923":"code","46d2fe67":"code","aaef7558":"code","611c4ef4":"code","933c4427":"code","312ef12a":"code","d412fc03":"code","5b64304b":"code","db59dfa5":"code","f82b2e76":"code","3e3aa60a":"code","2fe07acd":"code","1136281d":"code","597fb40b":"code","ed4578a4":"code","32137aa3":"code","456bfef4":"code","3bc8fa2c":"code","01354bb2":"code","696a1629":"code","7fc6e0e0":"code","74bda900":"code","254a863b":"code","e94382dd":"code","31d5ce36":"code","61b39b3d":"code","9999ce76":"code","6dfcfed8":"code","145fe555":"code","ff160063":"code","c18a68c3":"code","d1eb909d":"code","9bdb628c":"code","d789c5c8":"code","306e3beb":"code","c363d08c":"code","fb829874":"code","0340a810":"code","67744b27":"code","ec9493e6":"code","f1750124":"code","293dbff7":"code","efc92f90":"code","06ea4fab":"code","e19c0a93":"code","2ba16b33":"code","22b152f3":"code","8e517618":"code","0228c3b8":"code","570a3792":"code","cec44067":"markdown","8759e9d0":"markdown","25cf2b98":"markdown","d0aaa634":"markdown","f61eff0c":"markdown","adb882c3":"markdown","fd395e8c":"markdown","de944b9d":"markdown","04f6142b":"markdown","890b67b4":"markdown","283c5500":"markdown","093881ba":"markdown","492e26ba":"markdown","0170d7ee":"markdown","2d014225":"markdown","b80030eb":"markdown","c6aa5f68":"markdown","481f96e5":"markdown","b3f85930":"markdown"},"source":{"8ea59283":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7b4827d6":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","f5e0db47":"df_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")","22dabf7e":"df_train.head()","0ff622b2":"df_train.describe()","6b1bff93":"df_train_corr = df_train.corr()\nplt.figure(figsize=(18,10))\nsns.heatmap(df_train_corr)","f2853923":"plt.figure(figsize=(14,8))\nsns.scatterplot(data=df_train,x='GrLivArea',y='SalePrice')","46d2fe67":"df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n\n#Check the graphic again\nplt.figure(figsize=(14,8))\nsns.scatterplot(data=df_train,x='GrLivArea',y='SalePrice')","aaef7558":"plt.figure(figsize=(14,8))\nsns.boxplot(data=df_train,y='SalePrice',x='OverallQual')","611c4ef4":"plt.figure(figsize=(14,8))\nsns.lineplot(data=df_train,x='YearBuilt',y='SalePrice')","933c4427":"from scipy.stats import norm, skew\nfrom scipy import stats\n\nplt.figure(figsize=(14,7))\nsns.distplot(df_train['SalePrice'] , fit=norm)","312ef12a":"(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.figure(figsize=(14,7))\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","d412fc03":"df_train['SalePrice'] = np.log1p(df_train['SalePrice'])\nplt.figure(figsize=(14,7))\nsns.distplot(df_train['SalePrice'] , fit=norm)","5b64304b":"(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.figure(figsize=(14,7))\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","db59dfa5":"is_null = pd.DataFrame((100*df_train.isna().sum())\/len(df_train),columns={'isNull'})\nnull_values = is_null.sort_values(by='isNull',ascending=False).head(19)\nnull_values.head(19)","f82b2e76":"df_test=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3e3aa60a":"id_train = df_train.pop('Id')\nid_test = df_test.pop('Id')","2fe07acd":"test_is_null = pd.DataFrame((100*df_test.isna().sum())\/len(df_test),columns={'isNull'})\ntest_null_values = test_is_null.sort_values(by='isNull',ascending=False).head(33)\ntest_null_values.head(19)","1136281d":"plt.figure(figsize=(10,6))\nax = sns.barplot(x = test_null_values.index,y = test_null_values.isNull)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()","597fb40b":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny_train = df_train.SalePrice.values\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","ed4578a4":"all_data_is_null = pd.DataFrame((100*all_data.isna().sum())\/len(all_data),columns={'isNull'})\nall_data_null_values = all_data_is_null.sort_values(by='isNull',ascending=False).head(33)\nall_data_null_values.head(19)","32137aa3":"for col in ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','MSSubClass']:\n    all_data[col] = all_data[col].fillna('None')","456bfef4":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","3bc8fa2c":"for col in ['GarageCond','GarageFinish','GarageType','GarageQual','BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtCond']:\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ['GarageYrBlt','GarageCars','GarageArea','TotalBsmtSF','BsmtUnfSF','BsmtFullBath','BsmtHalfBath','BsmtFinSF1','BsmtFinSF2']:\n    all_data[col] = all_data[col].fillna(0).astype(int)","01354bb2":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","696a1629":"all_data['Functional'] = all_data['Functional'].fillna(all_data['Functional'].mode()[0])","7fc6e0e0":"print(df_train['Utilities'].unique())\ndf_train[df_train['Utilities'] == 'NoSeWa']","74bda900":"all_data.drop('Utilities', axis = 1, inplace = True)","254a863b":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","e94382dd":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","31d5ce36":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0).astype(int)","61b39b3d":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","9999ce76":"all_data_is_null = pd.DataFrame((100*all_data.isna().sum())\/len(all_data),columns={'isNull'})\nall_data_null_values = all_data_is_null.sort_values(by='isNull',ascending=False).head(33)\nall_data_null_values.head(5)","6dfcfed8":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","145fe555":"all_data = pd.get_dummies(all_data)","ff160063":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","c18a68c3":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","d1eb909d":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","9bdb628c":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","d789c5c8":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","306e3beb":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","c363d08c":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","fb829874":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","0340a810":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","67744b27":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ec9493e6":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f1750124":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","293dbff7":"averaged_models = AveragingModels(models = (ENet, GBoost, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","efc92f90":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","06ea4fab":"averaged_models.fit(train.values, y_train)\nstacked_train_pred = averaged_models.predict(train.values)\nstacked_pred = np.expm1(averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","e19c0a93":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","2ba16b33":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","22b152f3":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","8e517618":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","0228c3b8":"sub = pd.DataFrame()\nsub['Id'] = id_test\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","570a3792":"!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"Message\"","cec44067":"No more annoying outliers, we can continue with our discovery of the data. Looking at the OverallQual next.","8759e9d0":"Exterior 1 and 2 only have one missing value we can replace them with the most occuring value.","25cf2b98":"Next we need to convert the wrongly classified columns from numerical to categorical.","d0aaa634":"For the venir we can assume that if they are missing, they there were none used. We can then just fill the missing venir area with 0.","f61eff0c":"The data is skewed to the right, using log is an easy fix to get a normal distributed dataset.","adb882c3":"Since general living area only takes into account the ground floor, one should take into account the basements and 1st\/ 2nd floor to the total size of the house.\n","fd395e8c":"The higher the quality the more expensive the house. Which is to be expected.\nWe can also observe how the YearBuilt affects pricing.","de944b9d":"Next we have a look at the missing data, and try to feature engineer what we can.","04f6142b":"We find that there is a strong corrolation between OverallQual, GrLivArea and the SalePrice which makes sense. The better and bigger the house the more expensive it would be.","890b67b4":"Again, as stated in the dataset the missing data is none or 0 depending on the type of the column.","283c5500":"According to the dataset, if Functional is missing we should replace it with Typ","093881ba":"Reading though multiple submission, I found that there are some outliers in the data that are very extreme. The strongest ones are in the GrLivArea.","492e26ba":"For the following columns, the dataset tells us that they are all missing data are none.","0170d7ee":"The same can be done for KitchenQual, SaleType and MSZoning","2d014225":"We can replace the missing Electrical values with the most occuring value.","b80030eb":"Since the utilities is the same in all the train cases but one, we can just drop this column.","c6aa5f68":"The people that bought these houses got a bargain. However, we cannot use these values in our data since they are not a representation of how SalePrice evolves with GrLivArea. So we can just drop them from the training set.","481f96e5":"Much better.","b3f85930":"Assuming that the houses have the same size in the same neighborhood, we can take the median of the Lotfrontage in the same neighborhood and give that value to the houses with missing lotfrontage values."}}