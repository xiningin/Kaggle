{"cell_type":{"408165da":"code","280d0d88":"code","73dec802":"code","84e869c3":"code","ae789c8e":"code","630cac34":"code","8aec3f6d":"code","119184b0":"code","41ab2292":"code","63f6dd84":"code","4572b77b":"markdown","8354a002":"markdown","f3f3b8aa":"markdown","2c8320b2":"markdown","322d6ac3":"markdown","90526486":"markdown","d044ff07":"markdown","226d96a1":"markdown","00721029":"markdown","8a114b52":"markdown","d3f46af6":"markdown"},"source":{"408165da":"import pandas as pd\nimport numpy as np\nimport scipy.linalg as la\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom warnings import filterwarnings\nimport math\n\nfilterwarnings('ignore') # Hehe\n\n\n# Load dataset\nfilepath = '..\/input\/homebrew-beer-recipes\/brewers_friend_database.csv'\ndata = pd.read_csv(filepath)\n\nprint('Ready.')","280d0d88":"print(\"Dataset size: \",data.shape)\ndata.head()","73dec802":"to_replace = {\n    \"German Pils\": \"German Pilsner (Pils)\",\n    \"Oktoberfest\/M\u00e4rzen\": \"M\u00e4rzen\",\n    \"California Common Beer\": \"California Common\",\n    \"Weizen\/Weissbier\": \"Weissbier\",\n    \"Scottish Export 80\/-\": \"Scottish Export\",\n    \"Standard\/Ordinary Bitter\": \"Ordinary Bitter\",\n    \"Wee Heavy\": \"Strong Scotch Ale\",\n    \"Light American Lager\": \"American Light Lager\",\n    \"American Wheat Beer\": \"American Wheat or Rye Beer\",\n    \"Special\/Best\/Premium Bitter\": \"Best Bitter\"\n}\n\ndrop_styles = [\"No Profile Selected\", \"Dry Mead\", \"Semi-Sweet Mead\", \"Common Cider\", \"Other Fruit Melomel\", \"Experimental Beer\", \"Mixed Style Beer\"]\n\nfor key in to_replace:\n    data['style'].replace(key, to_replace[key], inplace = True)\n    \nfor s in drop_styles:\n    data.drop(data[data['style'] == s].index, inplace = True)\n    \nprint(\"Ready.\")","84e869c3":"styles_count = data['style'].value_counts() # Frequency of each class\nstyles = list(styles_count.keys()) # Complete list of styles\npopularity = data['style'].value_counts(normalize = True) # Proportion of each class\ntop_ten = popularity[:10] # Top ten most common styles\ntop_ten_styles = list(top_ten.keys()) # Class names\n\n# Print results\nprint('Total styles: ',len(styles))\nprint(\" \")\nprint('Top ten styles:')\nprint(top_ten)","ae789c8e":"top_ten_data = data[ data['style'].isin( top_ten_styles ) ]\n\nfig1 = plt.figure(1)\nfig1.set_figheight(10)\nfig1.set_figwidth(10)\nax = fig1.add_subplot(111, projection = '3d')\n\nclean_data = pd.DataFrame(columns = data.columns) # New dataframe without outliers\nfor s in top_ten_styles:\n    temp = top_ten_data.loc[ top_ten_data['style'] == s ] # Subset of each style\n    temp = temp.loc[(abs(zscore(temp[['color','abv','ibu']])) < 1).all(axis = 1)].sample(frac = 0.20) # Remove outliers and subsample\n    clean_data = clean_data.append(temp) # Add to new dataframe\n    ax.scatter(temp['color'], temp['abv'], temp['ibu'], label = s) # Plot new subset\n\nax.view_init(30, -60)\nax.set_xlabel('Color')\nax.set_ylabel('ABV')\nax.set_zlabel('IBU')\nplt.legend(loc = 2);","630cac34":"fig2 = plt.figure(figsize=(20,10))\nsns.pairplot(clean_data, hue = \"style\");\n","8aec3f6d":"fig3, ax3 = plt.subplots(3,1, figsize=(20,20))\n\nfor idx, v in enumerate(['ibu', 'abv', 'color']):\n    f = sns.violinplot(ax = ax3[idx], x = \"style\", y = v, data = clean_data)\n    f.grid()\n    f.set_xticklabels(ax3[idx].xaxis.get_majorticklabels(), rotation = 30);","119184b0":"columns = ['ibu','abv','color']\n\ncov_mat_cols = [\"s{}{}\".format(j + 1,k + 1) for j in range(3) for k in range(3)] # Columns for the covariance matrix\nmean_mat_cols = [\"u_{}\".format(v) for v in columns] # Columns for the average matrix\n\nnew_cols = ['Style', 'Freq'] + mean_mat_cols + cov_mat_cols # Columns of the resulting table\n\ndistribution = pd.DataFrame(columns = new_cols) # Result to export\nctr = 0 # Dataframe row counter\nfor s in styles: \n    temp = data.loc[ data['style'] == s ] # Instead of doing a groupby, I find this, more clear and easy to follow\n    temp = temp.loc[(abs(zscore(temp[columns])) < 2).all(axis = 1)] # Remove outliers\n    \n    freq = len(temp.index) # Number of recipes\n    avgs = temp.mean() # Average\n    cov = temp.cov() # Covariance matrix\n    \n    if freq < 100: # Discard if less than 100 rows\n        continue\n        \n    if la.det(cov) < 0.01: # Covariance matrix should be reversible (may fail)\n        print(\"Covariance matrix for style\", s, \"is singular.\")        \n        continue\n\n    inv_cov = pd.DataFrame(la.inv(cov.values), cov.columns, cov.index) # The inverse of the covariance matrix \n    \n    # Build the resulting dataframe row to row\n    row = (s, freq) # Style and frequency\n    for v in columns:\n        row = row + (avgs[v],) # Add average\n    for i, vi in inv_cov.items(): # For each row\n        for j, vj in vi.items(): # For each column\n            row = row + (vj, ) # Add the indexes of the inverse of the covariance matrix\n    \n    distribution = distribution.append( pd.DataFrame([row], columns = new_cols, index = [ctr]) )\n    ctr = ctr + 1\n\ndistribution\n","41ab2292":"# Mahalanobis distance\ndef mahalanobis(x, dist): \n    x_mu = x - dist.mean() # x - u\n    cov = dist.cov() # Covariance matrix of the dataset\n    cov_inv = pd.DataFrame(la.inv(cov.values), cov.columns, cov.index) # Inverse of the previuos matrix\n    m = x_mu.dot(cov_inv).dot(x_mu.T) # Quadratic Mahalanobis equation\n    return math.sqrt(m)","63f6dd84":"# Nuevo estilo de cerveza (aleatorio)\nsx = pd.Series([5.36, 25.21, 10.61], index = ['abv', 'ibu', 'color'])\n\nresults = {}\nfor s in top_ten_styles:\n    temp = clean_data.loc[ clean_data['style'] == s ]\n    m = mahalanobis(sx, temp)\n    results[s] = m\n    \n# Imprimir datos ordenados\nfor k, v in sorted(results.items(), key = lambda item: item[1]):\n    print(\"{}: {:.2f}\".format(k,v)) ","4572b77b":"The list indicate that it is likely that the craft beer we are analyzing correspond to the *American Light Lager* style.","8354a002":"## Analysis of data distribution over space\n\nIt is observed that for the numerical variables, at least for the top ten most common styles, the distribution is almost normal. The following violin plots allows for a better data viewing.\n","f3f3b8aa":"## Style classification\n\nThis table allows to estimate a craft beer style according to it properties (ABV, IBU and Color) using the Mahalanobis distance formula between the attribute vector and the average values of each style.\n\nThe Mahalanobis distance between an observation and a distribution is:\n\n$d_m(\\vec{x}, \\vec\\mu) = \\sqrt{(\\vec{x}-\\vec\\mu)^T \\sum^{-1}(\\vec{x}-\\vec{\\mu})}$\n\n\n","2c8320b2":"## Exploratory data analysis\n\nThere are 203156 observations containing 2 cathegorical attributes and 3 numeric atributes, whose descriptions are the following:\n\n**IBU:** International Bitterness Units.   \n**ABV:** Alcohol By Volume.    \n**Color:** Lovibond scale is used.  ","322d6ac3":"# Craft beer style classifier expert\n\n**Abstract** In this work it is proposed a simple classifier algorithm to determine the closest beer style given its color, IBU and ABV.  \nThis the final work for the posgraduate course *\"Visual Analysis for Big Data\"*.  \n\n**Author**: Dr. Mat\u00edas Micheletto.  \nICIC, CONICET - Universidad Nacional del Sur.  \nEmail: [matias.micheletto@uns.edu.ar](mailto:matias.micheletto@uns.edu.ar)  \n\n","90526486":"## Most popular styles\n\nMost present style correspond to *American IPA*, with a proportion of 15.3%.  \nFor the visual analysis, the top ten most popular styles will be used.","d044ff07":"## Obtaining the approximate distribution\n\nWe need to calculate frequency, average and covariance matrix for each style from the dataset. All data is used discarding outliers that fall outside the limit where $|z| > 2 \\cdot \\sigma$","226d96a1":"## Sparse data matrix\n\nA projection of the same data, but using 2D planes. It can be seen that there's a lot of data overlapping.\n\n","00721029":"## Style labels preprocessing\n\n### The following classes are equivalent and will be combined:  \n\"German Pils\" -> \"German Pilsner (Pils)\"  \n\"Oktoberfest\\\/M\\u00e4rzen\" -> \"M\\u00e4rzen\"  \n\"California Common Beer\" -> \"California Common\"  \n\"Weizen\\\/Weissbier\" -> \"Weissbier\"  \n\"Scottish Export_80\\\/-\" -> \"Scottish Export\"  \n\"Standard\\\/Ordinary Bitter\" -> \"Ordinary Bitter\"  \n\"Wee Heavy\" -> \"Strong Scotch Ale\"  \n\"Light American Lager\" -> \"American Light Lager\"  \n\n### The following classes must be deleted:\n\"No Profile Selected\"  \n\"Dry Mead\"  \n\"Semi-Sweet Mead\"  \n\"Common Cider\"  \n\"Other Fruit Melomel\"  \n\n### Styles with the following names require an additional processing:  \n\"Experimental Beer\"  \n\"Mixed Style Beer\"  \n","8a114b52":"## Example\n\nSuppose we want to determine the style of a craft beer which has the following properties:\n\n**ABV** = 5.36%  \n**IBU** = 25.21  \n**Color** = 10.61\u00b0L  \n\nThe following procedure determines the probability that this recipe corresponds to each style, according to the Mahalanobis classifier:","d3f46af6":"## Sparse data visualization\n\nAs a first approximation to visualize the variable space, the 10 most common styles are plotted. To reduce the amount of data, outliers are removed (z<$\\sigma$) and the set is subsampled to 20%. It is observed that the data are grouped in relatively separate clusters, which would indicate that there is a relationship between the variables and the different styles."}}