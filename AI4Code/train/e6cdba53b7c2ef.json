{"cell_type":{"b817f1a2":"code","5051308f":"code","5837b137":"code","b099717d":"code","fadc448d":"code","4812e1b3":"code","710bf9b1":"code","138009d2":"code","4328dea1":"code","096e8b32":"code","69c4bc3a":"code","e16e6a63":"code","981f4833":"code","9cba2fa6":"code","7b359e18":"code","662c1f7b":"code","4e72b14e":"code","5d248bf6":"code","3cfa5dc8":"code","7e5c5f0c":"code","d920177a":"code","98a5fb07":"markdown","4c13b4e0":"markdown","07c7dff6":"markdown","b307f263":"markdown","f96ab1f6":"markdown","7026eb93":"markdown","df707c9c":"markdown","8d22b25d":"markdown"},"source":{"b817f1a2":"%matplotlib inline\n\nfrom os import listdir, makedirs\nfrom os.path import isfile, join, basename, splitext, isfile, exists\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm_notebook\n\nimport tensorflow as tf\nimport keras.backend as K\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dropout, Dense, Flatten, BatchNormalization\nfrom keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import Concatenate, Average, Maximum, CuDNNLSTM, CuDNNGRU, Bidirectional, TimeDistributed\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.engine.input_layer import Input\nfrom keras.models import load_model\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('precision', 30)\nnp.set_printoptions(precision = 30)\n\nnp.random.seed(7723)\ntf.set_random_seed(1090)","5051308f":"%%time\ntrain_df = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int8, 'time_to_failure': np.float32})","5837b137":"train_df.head()","b099717d":"X_train = train_df.acoustic_data.values\ny_train = train_df.time_to_failure.values","fadc448d":"ends_mask = np.less(y_train[:-1], y_train[1:])\nsegment_ends = np.nonzero(ends_mask)\n\ntrain_segments = []\nstart = 0\nfor end in segment_ends[0]:\n    train_segments.append((start, end))\n    start = end\n    \nprint(train_segments)","4812e1b3":"plt.title('Segment sizes')\n_ = plt.bar(np.arange(len(train_segments)), [ s[1] - s[0] for s in train_segments])","710bf9b1":"class EarthQuakeRandom(keras.utils.Sequence):\n\n    def __init__(self, x, y, x_mean, x_std, segments, ts_length, batch_size, steps_per_epoch):\n        self.x = x\n        self.y = y\n        self.segments = segments\n        self.ts_length = ts_length\n        self.batch_size = batch_size\n        self.steps_per_epoch = steps_per_epoch\n        self.segments_size = np.array([s[1] - s[0] for s in segments])\n        self.segments_p = self.segments_size \/ self.segments_size.sum()\n        self.x_mean = x_mean\n        self.x_std = x_std\n\n    def get_batch_size(self):\n        return self.batch_size\n\n    def get_ts_length(self):\n        return self.ts_length\n\n    def get_segments(self):\n        return self.segments\n\n    def get_segments_p(self):\n        return self.segments_p\n\n    def get_segments_size(self):\n        return self.segments_size\n\n    def __len__(self):\n        return self.steps_per_epoch\n\n    def __getitem__(self, idx):\n        segment_index = np.random.choice(range(len(self.segments)), p=self.segments_p)\n        segment = self.segments[segment_index]\n        end_indexes = np.random.randint(segment[0] + self.ts_length, segment[1], size=self.batch_size)\n\n        x_batch = np.empty((self.batch_size, self.ts_length))\n        y_batch = np.empty(self.batch_size, )\n\n        for i, end in enumerate(end_indexes):\n            x_batch[i, :] = self.x[end - self.ts_length: end]\n            y_batch[i] = self.y[end - 1]\n            \n        x_batch = (x_batch - self.x_mean)\/self.x_std\n\n        return np.expand_dims(x_batch, axis=2), y_batch","138009d2":"t_segments = [train_segments[i] for i in [ 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]\nv_segments = [train_segments[i] for i in [ 0, 1, 2, 3]]","4328dea1":"x_sum = 0.\ncount = 0\n\nfor s in t_segments:\n    x_sum += X_train[s[0]:s[1]].sum()\n    count += (s[1] - s[0])\n\nX_train_mean = x_sum\/count\n\nx2_sum = 0.\nfor s in t_segments:\n    x2_sum += np.power(X_train[s[0]:s[1]] - X_train_mean, 2).sum()\n\nX_train_std =  np.sqrt(x2_sum\/count)\n\nprint(X_train_mean, X_train_std)","096e8b32":"train_gen = EarthQuakeRandom(\n    x = X_train, \n    y = y_train,\n    x_mean = X_train_mean, \n    x_std = X_train_std,\n    segments = t_segments,\n    ts_length = 150000,\n    batch_size = 64,\n    steps_per_epoch = 400\n)\n\nvalid_gen = EarthQuakeRandom(\n    x = X_train, \n    y = y_train,\n    x_mean = X_train_mean, \n    x_std = X_train_std,\n    segments = v_segments,\n    ts_length = 150000,\n    batch_size = 64,\n    steps_per_epoch = 400\n)","69c4bc3a":"def CnnRnnModel():\n    i = Input(shape = (150000, 1))\n    \n    x = Convolution1D( 8, kernel_size = 10, strides = 10, activation='relu')(i)\n    x = Convolution1D(16, kernel_size = 10, strides = 10, activation='relu')(x)\n    x = Convolution1D(16, kernel_size = 10, strides = 10, activation='relu')(x)\n    x = CuDNNGRU(24, return_sequences = False, return_state = False)(x)\n    y = Dense(1)(x)\n\n    return Model(inputs = [i], outputs = [y])","e16e6a63":"model = CnnRnnModel()\nmodel.compile(loss='mean_absolute_error', optimizer='adam')\nmodel.summary()","981f4833":"hist = model.fit_generator(\n    generator =  train_gen,\n    epochs = 50, \n    verbose = 0, \n    validation_data = valid_gen,\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience = 5, verbose = 1),\n        ModelCheckpoint(filepath='cnn_rnn.h5', monitor='val_loss', save_best_only=True, verbose=1)]\n)","9cba2fa6":"plt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n_= plt.legend(['Train', 'Test'], loc='upper left')","7b359e18":"def load_test(ts_length = 150000):\n    base_dir = '..\/input\/test\/'\n    test_files = [f for f in listdir(base_dir) if isfile(join(base_dir, f))]\n\n    ts = np.empty([len(test_files), ts_length])\n    ids = []\n    \n    i = 0\n    for f in tqdm_notebook(test_files):\n        ids.append(splitext(f)[0])\n        t_df = pd.read_csv(base_dir + f, dtype={\"acoustic_data\": np.int8})\n        ts[i, :] = t_df['acoustic_data'].values\n        i = i + 1\n\n    return ts, ids","662c1f7b":"test_data, test_ids = load_test()","4e72b14e":"X_test = ((test_data - X_train_mean)\/ X_train_std).astype('float32')\nX_test = np.expand_dims(X_test, 2)\nX_test.shape","5d248bf6":"model = load_model('cnn_rnn.h5')","3cfa5dc8":"y_pred = model.predict(X_test)","7e5c5f0c":"submission_df = pd.DataFrame({'seg_id': test_ids, 'time_to_failure': y_pred[:, 0]})","d920177a":"submission_df.to_csv(\"submission.csv\", index=False)","98a5fb07":"Load and normalize the test data","4c13b4e0":"Train the model with early stopping","07c7dff6":"Use convolutional layers to learn the features and reduce the time sequence length ","b307f263":"We could use any segments for training \/ validation","f96ab1f6":"Find complete segments in the training data (time to failure goes to zero)","7026eb93":"The generator samples randomly from the segmens without crossing the boundaries","df707c9c":"Load best model and predict","8d22b25d":"I think it does not make big difference but lets not leak into the validation data and calculate mean and standrad deviation on the training data only."}}