{"cell_type":{"7b59ee12":"code","6c43bbea":"code","f5a2dead":"code","b8177fe6":"code","4efe093e":"code","365c99b4":"code","943f31c9":"code","0d169006":"code","5ad76676":"code","967f3e00":"code","c43233ef":"code","108b40fa":"code","a3980cc8":"markdown","484e4b0d":"markdown","af7c4fe2":"markdown","e4498496":"markdown","5a6171c2":"markdown","62e608bb":"markdown","48ef056a":"markdown","236d5dde":"markdown","cbc61e71":"markdown","c0e787e9":"markdown"},"source":{"7b59ee12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c43bbea":"data = pd.read_csv('..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv')\ndata.tail()","f5a2dead":"#scatter plot\ncolor_list = ['red' if i =='Abnormal' else 'green'  for i in data.loc[:, 'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'], c= color_list, figsize = [15,15], diagonal = 'hist', alpha=0.5, s =200, marker = '*', edgecolor = \"black\")\nplt.show()\n","b8177fe6":"#N = data[data.class == \"Normal\"]\n#A = data[data.class == \"Abnormal\"]","4efe093e":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx, y = data.loc[:, data.columns != 'class'], data.loc[:, 'class']\nknn.fit(x, y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))\n","365c99b4":"# train test split\nfrom sklearn.model_selection import train_test_split \nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size = 0.3, random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx, y = data.loc[:, data.columns != 'class'], data.loc[:, 'class']\nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)\nprint('With KNN (K=3) accuracy is: {}', knn.score(x_test, y_test))# accuracy","943f31c9":"#Model Complexity\nneig = np.arange(1,25)\ntrain_accuracy=[]\ntest_accuracy=[]\n\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    #Fit with knn\n    knn.fit(x_train, y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    #test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n# Plot\nplt.figure(figsize=[13, 8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","0d169006":"# SVM\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\nsvm.fit(x_train, y_train)\n# TEST\nprint(\"Print Accuracy of SVM Algorithm:\", svm.score(x_test, y_test))","5ad76676":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\n# TEST\nprint(\"Print Accuracy of Naive Bayes Algorithm:\", nb.score(x_test, y_test))","967f3e00":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\n# TEST\nprint(\"Print Accuracy of Decision Tree Algorithm:\", dt.score(x_test, y_test))","c43233ef":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)\n# TEST\nprint(\"Print Accuracy of Random Forest Algorithm:\", rf.score(x_test, y_test))","108b40fa":"y_pred = rf.predict(x_test)\ny_true = y_test\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_pred)\n#Visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","a3980cc8":"<a id = \"3\"><\/a><br>\n# 3. **Supports Vector Machine(SVM) Classification**\n* We can use this algorithm to divide our data into two different clusters.\n* **SVM ==>** finds best decision boundary. It allows me to draw the best line.\n* SVM will find the maximum margin for me.\n* **Maximum margin:** the sum of the distances of my data closest to the line.\n* **Support Vector:** Two closest points affecting maximum margin.\n* My goal is to maximize the margin.","484e4b0d":"***59 :*** TN ==> True negative\n\n***7  :*** FP ==> False positive(type1 error)\n\n***7  :*** FN ==> False negative(type2 error)\n\n***20 :*** TP ==> True positive","af7c4fe2":" <a id = \"2\"><\/a><br>\n # 2. **K-Nearest Neighbors (KNN) Classification**\n* KNN: K nearest labeled data points.\n* Classification method.\n* First we need to train our data. Train = fit\n* fit(): fits the data, train the data.\n* predict() : predicts the data\n* x: features y: target variables(normal, abnormal)\n* n_neighbors: K. In this example it is 3. It means that look at the 3 closest labeled data points.","e4498496":"<a id = \"7\"><\/a><br>\n# 7. **Evaluation Classification Models :Confusion Matrix**\n* In this algorithm, I correctly estimated how much of the normal and abnormal data or abnormal data estimated how much of the normal data.\n","5a6171c2":"<a id = \"5\"><\/a><br>\n# 5. **Decision Tree Classification**\n* **CART :** Classification And Regression Tree\n* On the basis of the Decision Trees are **splits**.\n* It draws the boundary in the best separation.","62e608bb":"* K is called a hyperparameter. We need to choose is that gives best performance.\n* If k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n* If k is big, model that is less complex model can lead to underfit.","48ef056a":"<a id = \"6\"><\/a><br>\n# 6. **Random Forest Classification**\n* ***ensamble learning model :*** We combine several decision tree algorithms and get 1 Random Forest model.\n* We have a data and seperate according to train and test.\n* We select n sample in train part and this is call **subsample.**\n* We train the subsamples with the decision tree.\n* I train my subsamples with n decision trees. that is, I have one random forest model consisting of n decision trees.","236d5dde":"* We fit the data and predict it with KNN.\n* We predict correct or what is our accuracy or the accuracy is best metric to evaluate or result?\n\nWe need to split our data train and test sets.\n* train: use train set by fitting\n* test: make prediction on test set.\n* With train and test sets, fitted data the tested data are completely different.\n* train_test_split(x,y,test_size = 0.3, random_state = 1)\n  * x:features\n  * y: target variables(normal, abnormal)\n  * test_size: percentage of test size.0.3 ==> test size = 30% and train size = 70%\n  * random_state:sets a seed. If this seed is same number, train_test_split() produce exact same split at each time.\n* fit(x_train, y_train): fit on train sets\n* score(x_test, y_test):predict and give accuracy on test sets.","cbc61e71":"<a id = \"1\"><\/a><br>\n# 1. **INTRODUCTON**\n\nContent: \n\n1. [Introduction](#1)\n1. [K-Nearest Neighbors (KNN) Classification](#2)\n1. [Support Vector Machine(SVM) Classification](#3)\n1. [Naive Bayes Classification](#4) \n1. [Decision Tree Classification](#5)\n1. [Random Forest Classification](#6)\n1. [Evaluation Classification Models :Confusion Matrix](#7)\n1. [Regression](#8)\n1. [Cross Validation (CV)](#9)\n1. [ROC Curve](#10)\n1. [Hyperparameter Tuning](#11)\n1. [Pre-processing Data](#12)","c0e787e9":"<a id = \"4\"><\/a><br>\n# 4. **Naive Bayes Classification**\n* Naive Bayes algorithm is used to find which label belongs to any point.\n* **Formula:** P (Math | X) = P (X | Math) * P (Math) \/ P (X)\n\n  * **P (Math | X)** : probability of Math according to X\n  * **P (X | Math):** likelihood\n  * **P (Math) :** prior probability \n  * **P (X) :** marginal likehood\n"}}