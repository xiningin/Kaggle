{"cell_type":{"35d19104":"code","1ae95e4d":"code","fcc1278c":"code","329803e2":"code","5ba82189":"code","9e1ee78e":"code","a11d2a03":"code","5e25494f":"code","23a2e88b":"code","4e260f33":"code","484d9834":"code","45e9ac58":"code","0a0eb05e":"code","afaf73e6":"code","708aba0f":"code","28596726":"code","171be221":"code","667cbd87":"code","9783571c":"code","ee59a846":"code","0da6686c":"code","b136013d":"code","86caf267":"markdown","949b781a":"markdown","f6066fd6":"markdown","f450a1fb":"markdown","6f839d16":"markdown","5887d59b":"markdown","9656603d":"markdown","a40df61b":"markdown"},"source":{"35d19104":"import pandas as pd\nimport itertools\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nimport random\nimport re","1ae95e4d":"# tennis.csv contains 8 online articles about tennis\ndf = pd.read_csv(\"..\/input\/reports\/tennis.csv\")\ndf","fcc1278c":"# Show the 8 articles\nfor art in df['article_text']:\n    print(f'{art}\\n\\n')","329803e2":"sentences = []\nfor s in df['article_text']:\n    sentences.append(sent_tokenize(s))\n\n# Extract the longest sentence of each text to be able to check,\n# if the algorithm performs better then just picking the longest\n# sentence\nlongest_sentences = []\n    \nfor item in sentences:\n    sorteditems = sorted(item, key=len)\n    longest_sentences.append((sorteditems[-1]))\n\nlongest_sentences","5ba82189":"# Flatten nested list of sentences \n# to have a single list of all sentences\nsentences = list(itertools.chain(*sentences))\nsentences[:5]","9e1ee78e":"clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\nclean_sentences = [s.lower() for s in clean_sentences]\nstop_words = stopwords.words('english')\n\ndef remove_stopwords(words: list) -> str:\n    \"\"\"Remove stopwords from a list of words and return the remaining words\n       as a string joint by the space char. \n    \"\"\"\n    sentence = \" \".join([word for word in words if word not in stop_words])\n    return sentence\n\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\nprint(sentences[:5])\nclean_sentences[:5]","a11d2a03":"import pickle\n\nwith open('..\/input\/pickle-fastext-embedding\/cc.en.300.pkl','rb') as infile:\n    word_embeddings = pickle.load(infile)\n\ntype(word_embeddings)","5e25494f":"word_embeddings['federer']","23a2e88b":"word_embeddings['Federer']","4e260f33":"word_embeddings['world']","484d9834":"sentence_vectors = []\nwords_not_in_word_embeddings = set()\n\ndef get_vectors_from_word_embeddings(w):\n    try:\n        vec = word_embeddings[w]\n    except KeyError:\n        vec = np.zeros((300,))\n        words_not_in_word_embeddings.add(w)\n    return vec\n        \nfor sentence in clean_sentences:\n    if len(sentence) != 0:\n        word_list = sentence.split()\n        vec_list = []\n        for w in word_list:\n            vec = get_vectors_from_word_embeddings(w)\n            vec_list.append(vec)\n        v = sum(vec_list)\/(len(word_list)+0.001)\n    else:\n        v = np.zeros((300,))\n    sentence_vectors.append(v)","45e9ac58":"# Check which words were not in the dictionary\nwords_not_in_word_embeddings","0a0eb05e":"print(len(sentences))\nprint(len(clean_sentences))\nlen(sentence_vectors)","afaf73e6":"from sklearn.metrics.pairwise import cosine_similarity\n\n# Spoiler alert: 119\ndataset_length = len(sentences)\n\nsimilarity_matrix = np.zeros([dataset_length, dataset_length])\nfor i in range(dataset_length):\n    for j in range(dataset_length):\n        # ignore the diagonal\n        if i != j:\n            similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]","708aba0f":"print(cosine_similarity(sentence_vectors[1].reshape(1,300), sentence_vectors[7].reshape(1,300))[0,0])\ntype(sentence_vectors[1])","28596726":"# 119x119 Matrix\nsimilarity_matrix[118][117]","171be221":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(similarity_matrix)\nscores = nx.pagerank(nx_graph)","667cbd87":"scores","9783571c":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)","ee59a846":"print(len(ranked_sentences))\nranked_sentences[:5]","0da6686c":"# ranked_sentences contains the highest ranked sentence for every text. It is a list of tuples (score, sentence)\nfor sentence in ranked_sentences:\n    print(sentence[0], len(sentence[1]))","b136013d":"from termcolor import colored\nfor i, article in enumerate(df['article_text']):\n    print(colored((\"ARTICLE:\".center(50)),'yellow'))\n    print('\\n')\n    print(colored((article),'blue'))\n    print('\\n')\n    print(colored((\"SUMMARY:\".center(50)),'green'))\n    print('\\n')\n    print(colored((f\"Summary in Text? {ranked_sentences[i][1] in article}\".center(50)),'green'))\n    print('\\n')\n    print(colored((f'{ranked_sentences[i][1]} - Score: {ranked_sentences[i][0]}'),'cyan'))\n    print('\\n')","86caf267":"## Tokenize the texts into sentences\n\nUsing: [nltk.tokenize](https:\/\/www.nltk.org\/api\/nltk.tokenize.html) - split the texts into lists of sentences and create a single list of all the sentences. ","949b781a":"## Load a word embedings dictionary\n\nThe project uses [fastText's](https:\/\/fasttext.cc\/docs\/en\/crawl-vectors.html) pre-trained word vectors for English, trained on Common Crawl and Wikipedia using `fastText`. The model was trained using `CBOW` with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives.\n\nThe dictionary `word_embeddings` contains a word as key and 300 numeric vector values as value.","f6066fd6":"## Compute a vector for every sentence\n\nIn the last step the dictionary `word_embeds` has been created. It contains 300 vectors for every occuring word. In the following step 300 vectors are composed (by summing the word vectors) on the sentence level. For normalization the vector values of the sentence are divided by the number of words in the sentence.","f450a1fb":"## Cleaning the sentences\n\n* replace every non alphabetic character with a space\n* lowercase all the words\n* remove all stopwords as per `nltk.corpus.stopwords`","6f839d16":"\ud83e\udd28 - Okay, something ain't right. The matching of the ranked sentences and the articles does not seem to work well.","5887d59b":"**Note:** This notebook is following the article (https:\/\/thecleverprogrammer.com\/2020\/08\/24\/summarize-text-with-machine-learning\/) and the very similar notebook (https:\/\/www.kaggle.com\/gauravduttakiit\/summarize-text-with-machine-learning\/). \n\nBut it seems both have borrowed from this article https:\/\/appliedmachinelearning.blog\/2019\/12\/31\/extractive-text-summarization-using-glove-vectors\/.\n\nThe goal is to extract the most significant sentences from a text as a summary for the text. To determine the significance the sentences are ranked by the TextRank algorithm (based on PageRank). \n\n## Processes:\n\n* Text cleaning\n* Assignment of embedding vectors of remaining words coming from fasttext precomputed model (alternatively use GloVe as the original articles did)\n* Computing of embedding vectors per sentence\n* Computer cosine_similarity for each sentence (of all texts) with every other sentence\n* Use similarity matrix data structure as input to the TextRank algorithm\n\n## Findings\n\nThe main issue with the article by `thecleverprogrammer` and the related notebook is, that they throw together all the sentences of all input text and then compute the relevance over them. The way they implement the process makes it impossible to related a sentence back to the text it was coming from. The original article only computes the TextRank for sentences, that belong to a specific text.\n\nThe major flaw makes the whole notebook a bit pointless. It can be remedied by going back to determine the highest scoring sentence in TextRank per sentence. \n\nBesides there were some minor problems, that can have an influence on the result.\n\n* There is a difference in unrecognized words between the GloVe and the fasttext dictionary. No too suprising because they have different text basis. But the discovery made me aware of an interesting detail. All words have been lowercased before the lookup in GloVe or fasttext. Typically names (i.e. named entities) of players like `nishikori` have not been found. But it turned out, that `Nishikori` is in the dictionary. On the other hand it turned out that lowercased names like `federer` and `sharapova` are in the dictionary. The uppercased version is as well in the dictionary, e.g. `Federer` and `Sharapova`, with very different values for the embedding vectors.\n\n**Research question(s):** How does lowercasing influence the computation of the `TextRank`? How many words are not in the dictionary, if no lowercasing happens?\n\n## Extractive Approach\n\nThe Extractive approach takes sentences directly from the document according to a scoring function to form a cohesive summary. This method works by identifying the important sections of the text cropping and assembling parts of the content to produce a condensed version.\n\nThis project summarizes a text using the TextRank algorithm, which is an extractive and unsupervised machine learning algorithm. [NetworkX PageRank](https:\/\/networkx.org\/documentation\/networkx-1.2\/reference\/generated\/networkx.pagerank.html)\n\n## Steps\n\n* Tokenize texts into sentences to obtain\n* Cleaning the sentences (remove punctuation, case(?), stopwords)\n* Load a word embedings dictionary from [fasttext](https:\/\/fasttext.cc\/docs\/en\/crawl-vectors.html)\n* Compute vectors for every sentence\n* Compute a similarity matrix\n* Apply PageRank algorithm to find the most important sentence per text","9656603d":"## Compute a similarity matrix\n\nUse `cosine_similarity` from `sklearn` ([Cosine similarity](https:\/\/scikit-learn.org\/stable\/modules\/metrics.html#cosine-similarity)) to compute how similar \n1 sentence to every other sentence in the dataset is.\n\n### XXX reshape\n\nI am not 100% sure, why we are applying `reshape`. The high level explanation I found elsewhere is, that\n`scikit-learn` requires the reshaped version of the array.\n\n```python\n  >>> sentence_vectors[0]\n  ... array([-4.61076051e-02,...], dtype=float32)\n  >>> sentence_vectors[0].reshape(1,300)\n  ... array([[-4.61076051e-02,...]], dtype=float32)\n```","a40df61b":"## Apply PageRank algorithm\n\nRunning the PageRank algorithm on the similarity matrix, which determines the most relevant sentence in an article. [NetworkX PageRank](https:\/\/networkx.org\/documentation\/networkx-1.2\/reference\/generated\/networkx.pagerank.html)\n\n### XXX from score to ranked sentences\n\nThe whole networkx part is a bit unclear to me in the moment."}}