{"cell_type":{"f5764f73":"code","78133437":"code","17dbe3f5":"code","660de857":"code","d28e7def":"code","c9272a70":"code","9cc16c1d":"code","db1acd74":"code","ca39a086":"code","d8814fe5":"code","a3c10b85":"code","7be15968":"code","de8c48bd":"code","2e5ecba1":"code","e333b752":"markdown","12ca4d2e":"markdown","17198992":"markdown","9312d991":"markdown","52006797":"markdown","22d6c139":"markdown","39b991a2":"markdown","a00c1d43":"markdown","fe08dc73":"markdown","20b12635":"markdown"},"source":{"f5764f73":"import pandas as pd\npd.options.mode.chained_assignment = None \nimport numpy as np\nimport re\nimport nltk\n\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\nSTOP_WORDS = nltk.corpus.stopwords.words()","78133437":"! ls \"..\/input\/nlp-getting-started\"","17dbe3f5":"train=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsubmission=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","660de857":"train.head()","d28e7def":"print(train.shape)","c9272a70":"def clean(text):\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', text).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)  \n            \n    sentence = \" \".join(sentence)\n    return sentence","9cc16c1d":"train = train.dropna(axis=0)\ntrain=train.reset_index()\nfor i in range (train.shape[0]):\n    train.at[i,'text']=clean(train.loc[i,'text'])","db1acd74":"corpus=[]\n\nfor i in range(train.shape[0]):\n    corpus.append(train['text'][i].split(\" \"))\n    ","ca39a086":"print(corpus[:3])","d8814fe5":"model = word2vec.Word2Vec(corpus, size=100, window=10, min_count=35, workers=4)","a3c10b85":"model.wv['news']","7be15968":"def drawing(model):\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","de8c48bd":"drawing(model)","2e5ecba1":"model.most_similar('news')","e333b752":"Load libraries : ","12ca4d2e":"#### This kernel will be devided into the following parts\n\n1. Data Exploration\n2. Data Preprocessing\n3. Data Vizualization ","17198992":"Create Corpus ","9312d991":"Can Using Word2Vec to get Simialr words ","52006797":"#### Hello everyone , \n In this kernel we will go together into  ****Visualization**** Disaster Tweets data to learn how words related together using t-SNE natural language processing NLP techniques.\n \n** Before any thing , What is T-SNE ?\n** \n\n T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n \n [To read more](https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding)","22d6c139":"## Word 2 Vec\n\nWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space\n\nin this example have 100 dims","39b991a2":"**2. Data Preprocessing**","a00c1d43":"**1. Data Exploration**","fe08dc73":"* clean nan values\n* delete columns not help in training or viusalize like id,location and keyword \n* convert letters in text to lowercase \n* remove numbers and symobls \n*  take toknize of text \n* Create Corpus of Training","20b12635":"## congratulation \n> ## Happy End"}}