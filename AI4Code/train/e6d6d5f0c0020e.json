{"cell_type":{"0e6bdfa3":"code","a75227ab":"code","3f383a5d":"code","48d72415":"code","98bcd094":"markdown","40707705":"markdown","92fc26b2":"markdown","aef04c4b":"markdown","6f7ab765":"markdown"},"source":{"0e6bdfa3":"import os\nfrom h5py import File as h5File\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport nibabel as nib\nfrom torch.utils.data import Dataset\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms","a75227ab":"class TReNDS_dataset(Dataset):\n    def __init__(\n            self, \n            mat_folder, \n            sbm_path, \n            train_scores_path=None, \n            fnc_path=None, \n            ICN_numbers_path=None,\n            mask_path=None,\n            transform=None\n        ):\n        super().__init__()  # Inherit from Dataset torch class, therefore initialize it\n        print('Loading dataset...')\n        # Load data\n        # Store the paths to the .mat file as a dictionary {patientID: complete_path_to_file}\n        self.mat_paths = {int(filename.split('.')[0]): os.path.join(mat_folder, filename) for filename in os.listdir(mat_folder)}\n\n        if fnc_path:\n            fnc = pd.read_csv(fnc_path)  # There are no NaN values here (ref: https:\/\/www.kaggle.com\/rftexas\/trends-in-dept-understanding-eda-lgb-baseline)\n            self.fnc = {Id: np.array(fnc.loc[fnc['Id'] == Id]).squeeze()[1:] for Id in self.mat_paths.keys()}\n        else:\n            self.fnc = None\n\n        sbm = pd.read_csv(sbm_path)  # There are no NaN values here (as before)\n        self.sbm = {Id: np.array(sbm.loc[sbm['Id'] == Id]).squeeze()[1:] for Id in self.mat_paths.keys()}\n\n        ICN_num = pd.read_csv(ICN_numbers_path)  # There are no NaN values\n        self.ICN_num = np.array(ICN_num['ICN_number']).squeeze()\n\n        # Check if dataset is for training or for submission\n        if train_scores_path:\n            train_scores = pd.read_csv(train_scores_path)\n            train_scores.fillna(train_scores.mean(), inplace=True)  # Look for NaN values and replace them with column mean\n            self.labels = {Id: np.array(train_scores.loc[train_scores['Id'] == Id]).squeeze()[1:] for Id in self.mat_paths.keys()}\n        else:\n            self.labels = None\n\n        # Test code to verify if there are all the labels for each type of data\n        # fnc_keys = list(fnc['Id'])\n        # sbm_keys = list(sbm['Id'])\n        # print(len(pt_keys), len(fnc_keys), len(sbm_keys))\n        # fnc_missing = []\n        # sbm_missing = []\n        # for k in pt_keys:\n        #     if k not in fnc_keys:\n        #         fnc_missing.append(k)\n        #     if k not in sbm_keys:\n        #         sbm_missing.append(k)\n        # print(fnc_missing, sbm_missing)\n\n        self.mask = np.array(nib.load(mask_path))\n\n        # Prepare num_to_id in order to address the indexes required from torch API\n        self.__num_to_id = {i: k for i, k in enumerate(self.mat_paths.keys())}\n        # Create reverse order to have control over dataset patients IDs and indexes\n        self.id_to_num = {k: i for i, k in self.__num_to_id.items()}\n\n        print('Dataset loaded!')\n\n        self.transform = transform\n\n    def __len__(self):\n        # Return the length of the dataset\n        return len(self.mat_paths.keys())\n\n    def __getitem__(self, item):\n        # Get the ID corresponding to the item (an index) that torch is looking for.\n        ID = self.__num_to_id[item]\n\n        # Retrieve all information from the Dataset initialization\n        \n        # brain = torch.load(self.mat_paths[ID]) # This is the loading in case the tensors has been saved as torch ones.\n        # This is the loading in case of saving the tensors as compressed bytes numpy array\n        # brain = np.copy(np.frombuffer(zlib.decompress(open(self.pt_paths[ID], 'rb').read()), dtype='float64').reshape(53, 52, 63, 53))  \n        # brain = None  # In case of shallow networks which doesn't need the brain images\n        brain: np.ndarray = np.array(h5File(self.mat_paths[ID], 'r')['SM_feature'], dtype='float32')  # Load as float32, I think float64 is not needed.\n\n        sbm = self.sbm[ID]\n        # Create sample\n        sample = {\n            'ID': ID,\n            'sbm': sbm,\n            'brain': brain\n        }\n        if self.fnc:\n            sample['fnc'] = self.fnc[ID]\n        # Add labels to the sample if the dataset is the training one.\n        if self.labels:\n            sample['label'] = self.labels[ID]\n\n        # Transform sample (if defined)\n        return self.transform(sample) if self.transform is not None else sample","3f383a5d":"class ToTensor:\n    def __call__(self, sample):\n        sbm = torch.tensor(sample['sbm']).float()\n        ID = sample['ID']\n\n        new_sample = {**sample, 'sbm': sbm, 'ID': ID}\n\n        if sample['brain'] is not None:\n            # Define use of brain images - which are not necessary for shallow networks\n            new_sample['brain'] = torch.tensor(sample['brain']).float()\n        sample_keys = list(sample.keys())\n        # These are labels which can be skept in case they are not needed.\n        tries_keys = ['fnc', 'label']\n        for tk in tries_keys:\n            if tk in sample_keys:\n                new_sample[tk] = torch.tensor(sample[tk]).float()\n\n        return new_sample","48d72415":"# Define all the needed paths\nbase_path = '\/kaggle\/input\/trends-assessment-prediction\/'  # In case the working directory is different from the playing one\ntrain_mat_folder = os.path.join(base_path, 'fMRI_train')\ntest_pt_folder = os.path.join(base_path, 'fMRI_test')\nfnc_path = os.path.join(base_path, 'fnc.csv')\nsbm_path = os.path.join(base_path, 'loading.csv')\nICN_num_path = os.path.join(base_path, 'ICN_numbers.csv')\ntrain_scores_path = os.path.join(base_path, 'train_scores.csv')\nmask_path = os.path.join(base_path, 'fMRI_mask.nii')\n\n# Define transformations\ntrans = transforms.Compose([ToTensor()])\n\ndataset = TReNDS_dataset(train_mat_folder, sbm_path, train_scores_path, fnc_path, ICN_num_path, mask_path, transform=trans)\ndataloader = DataLoader(dataset, batch_size=24, shuffle=True, pin_memory=True, num_workers=1)\n\nfor batch in tqdm(dataloader, desc='Reading dataset...'):\n    brain = batch['brain']  # Notice that this is still in CPU. If needed, it can be sent to the CUDA device with .to('cuda:0')\n    print(brain.shape)\n    break","98bcd094":"# Imports\nDefine the imports that will be needed later.","40707705":"# Define the real loader\nDefine the code that will be necessary to load the entire dataset. I left the loadings of the ICN numbers and of the mask, even though I will not use it while retrieving the data later.\n\nDatasets in PyTorch are indexed, which means that PyTorch shuffles and selects which index it wants to load, and let the user use them in order to retrieve the right element.\n\n## __init__\nIn the __init__ part there is an initialization of the parameters, which is done once in a time. Notice that I transform the pandas matrices into dictionary, as explained in the __getitem__ section.\n\n## __len__\nWe need to define this method in order to let the Dataset API know the dimension of the dataset. In our case, this is the length of the keys of the files in training data.\n\n## __getitem__\nIn this method I retrieve and build the actual data for the generator. Notice that this is a concurrent method (num_workers > 0) and therefore I cannot do operations that are not multithreading-compliant - like, loading the whole matrix from pandas and then retrieving a row directly. This is why I loaded the entire fnc and sbm datasets as dictionary, which are multithreading ready. Reading one file at a time is good also.\n\n__getitem__ requires to address the item with an index. This is why I prepared also a index_to_ID in order to retrieve the ID of a row from an index.","92fc26b2":"# Actual use of the dataset\nHere I define the right use of the dataset. You will notice that the first dimension of the brain matrix is the batch dimension.","aef04c4b":"I decided to create this dataset for the first timers in loading giant datasets. I used PyTorch Dataset API in order to create a fast, thread-safe and reliable generator for the dataset.","6f7ab765":"# Custom transformations\nPyTorch let the user prepare some custom transformations which are applied to the whole dataset at each call. I put an example, which transforms each element in a torch tensor."}}