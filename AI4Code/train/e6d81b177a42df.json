{"cell_type":{"219033df":"code","e51a8c38":"code","7f1163e2":"code","01c495d4":"code","d6204293":"code","d059d977":"code","381cfd36":"code","d1fa0e51":"code","293ad989":"code","88bdd327":"code","e1300a8b":"code","4f345e82":"markdown","33157530":"markdown","c471fdea":"markdown","adb06e1c":"markdown","f92cfbca":"markdown","75c6253f":"markdown","ef81c478":"markdown","0f12625d":"markdown","0de2cb53":"markdown"},"source":{"219033df":"import gym\nimport numpy as np\nimport random as rd\n#\u6ce8\u518c\u6e38\u620f\u73af\u5883\n#setup game environment\nenv = gym.make('FrozenLake8x8-v0')\n#\u5b9a\u4e49Q\u503c\u8868\uff0c\u521d\u59cb\u503c\u8bbe\u4e3a0\n#intial Q-table, initial data as 0\nQ = np.zeros([env.observation_space. n,env.action_space.n])\n#\u8bbe\u7f6e\u5b66\u4e60 \u53c2\u6570\n#set up hyper-parameter\nlearningRate = 0.85\ndiscountFactor = 0.95\n#Learning rate\/alpha: mathematically shown using the symbol alpha\n#Discount rate\/gemma: represented with the symbol Gemma\n\n#\u5b9a\u4e49\u4e00\u4e2a\u6570\u7ec4\uff0c\u7528\u4e8e\u4fdd\u5b58\u6bcf\u4e00\u56de\u5408\u5f97\u5230\u7684\u5956\u52b1\n#hold all the rewards will get from each episode. \n#This will be so we can see how our game scores change over time\nrewardList = []\nprint(Q)","e51a8c38":"def train():\n    for i_episodes in range(20000): ##for each time step within an episode\n    #\u91cd\u7f6e\u6e38\u620f\u73af\u5883\n    #for each episode\n    #we're going to first rest the state of the environment back to the starting state\n      s = env.reset()\n      i = 0\n    # \u5b66\u4e60 Q-Table\n      while i < 2000:\n        i += 1\n        #\u4f7f\u7528\u5e26\u63a2\u7d22(\u03b5\u8d2a\u5fc3\u7b97\u6cd5)\u7684\u7b56\u7565\u9009\u62e9\u52a8\u4f5c\n        #use epsilon greedy strategy\n        a = epsilon_greedy(Q, s, i_episodes)\n        #\u6267\u884c\u52a8\u4f5c\uff0c\u5e76\u5f97\u5230\u65b0\u7684\u73af\u5883\u72b6\u6001\u3001\u5956\u52b1\u7b49\n        #new state, reward for that, whether the action in our episode, and diagnostic information\n        observation, reward, done, info = env.step(a)\n        #\u66f4\u65b0Q\u503c\u8868\n        #update Q-table\n        Q[s, a] = (1-learningRate) * Q[s, a] + learningRate * ( reward + discountFactor * np.max(Q[observation,:]))\n        s = observation\n        #'done' check whether or not our episode is finished\n        if done:\n            break","7f1163e2":"def epsilon_greedy(q_table, s, num_episodes):\n    #for each time step within an episode we set our exploration rate threshold\n    rand_num = rd.randint(0, 20000)\n    if rand_num > num_episodes:\n        #\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\n        #the agaent will explore the environment and simple action randomly\n        action = rd.randint(0, 3)\n    else :\n        #\u9009\u62e9\u4e00\u4e2a\u6700\u4f18\u7684\u52a8\u4f5c\n        #the agent will exploit the environment and choose the action\n        #has the highest key value in the Q-table for the current state\n        action = np.argmax(q_table[s, :])\n    return action","01c495d4":"#contains everything that happens for a single timestamp within each spisode\ndef test():\n    for i_episodes in range(100):\n        #\u91cd\u7f6e\u6e38\u620f\u73af\u5883\n        #reset environment\n        s = env.reset()\n        i = 0\n        total_reward = 0\n        while i < 500:\n            i += 1\n            #\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\n            #choose an action\n            a = np.argmax(Q[s, :])\n            #\u6267\u884c\u52a8\u4f5c\uff0c\u5e76\u5f97\u5230\u65b0\u7684\u73af\u5883\u72b6\u6001\u3001\u5956\u52b1\u7b49\n            #act new action and update\n            observation, reward, done, info = env.step(a)\n            #\u53ef\u89c6\u5316\u6e38\u620f\u753b\u9762(\u91cd\u7ed8\u4e00\u5e27\u753b\u9762)\n            env.render()\n            #\u8ba1\u7b97\u5f53\u524d\u56de\u5408\u7684\u603b\u5956\u52b1\u503c\n            #update the rewards from current episode by adding the reward we received\n            total_reward += reward\n            s = observation\n            if done:\n                break\n        rewardList.append(total_reward)","d6204293":"train()\ntest()","d059d977":"print(\"Final Q-Table Values: \")\nprint(Q)","381cfd36":"print(\"Success rate: \" + str(sum(rewardList)\/len(rewardList)))","d1fa0e51":"import matplotlib.pyplot as plt\nplt.plot(range(len(rewardList)), rewardList)","293ad989":"#The result not that good, so I'm going to change the hyper-parameter\n\nlearningRate = 0.05\ndiscountFactor = 0.99\nrewardList = []","88bdd327":"train()\ntest()","e1300a8b":"print(\"Success rate: \" + str(sum(rewardList)\/len(rewardList)))\nplt.plot(range(len(rewardList)), rewardList)","4f345e82":"### Frozen Lake game\n![frozen](https:\/\/www.pandotrip.com\/wp-content\/uploads\/2015\/11\/Chaqmaqtin-Photo-by-Matthieu-Paley2-980x711.jpg)\n\n> \"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"\n> \n> The surface is described using a grid like the following:\n> \n>SFFF       (S: starting point, safe)\n>\n>FHFH       (F: frozen surface, safe)\n>\n>FFFH       (H: hole, fall to your doom)\n>\n>HFFG       (G: goal, where the frisbee is located)\n\n> \n> The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n![f2](https:\/\/www.deeplearningwizard.com\/deep_learning\/deep_reinforcement_learning_pytorch\/images\/gridworld_examples.png)","33157530":"### Content\n\n**Reinforcement Learning**\n> * What is Reinforcement learning?\n> * Two bases of RL\n> * Develope History\n> * Terms in reinforcement learning\n> * Flappy Bird - example of reinforcement learning\n> * The mainstream algorithm of reinforcement learning\n\n**Markov Decision Process**\n>* Why MDP\n>* Markove Chain\n>* Famous example for Markove Chain\/Process\n\n**The Bellman equation**\n\n**Q-Learing**\n> * What is Q-learning\n> * Why Q-learning\n> * What is Q-learning table?\n> * The mathematic theory of Q-learning\n> * The algorithm of Q-learning\n\n**Frozen Lake game**","c471fdea":"### reference\n[1] Exact DP: Dynamic p.Programming and optimal control, Vol. 1 (2017), Vol. 2 (2012)\n\n[2] https:\/\/iq.opengenus.org\/introduction-to-q-learning-and-reinforcement-learning\/\n\n[3] https:\/\/towardsdatascience.com\/simple-reinforcement-learning-q-learning-fcddc4b6fe56\n\n[4] https:\/\/zhuanlan.zhihu.com\/p\/24808797\n\n[5] https:\/\/www.zhihu.com\/question\/26408259\n\n[6] https:\/\/zhuanlan.zhihu.com\/p\/28084942\n\n[7] https:\/\/zhuanlan.zhihu.com\/p\/56425081\n\n[8] https:\/\/www.zhihu.com\/question\/41775291","adb06e1c":"# FrozenLake\ud83e\uddca - Reinforcement&Q-learning\n\n> A simple explanation for Reinforcement learning and Q-learning, with a famous example, a game - Frozen Lake for further explanation. ","f92cfbca":"### The Bellman equation\n\n> **How do we choose which action to perform in this case?**\n> Assume we already know the expected reward. Then we choose the sequence of action which will eventually generate the highest reward.\n> \n> **Q-Value (a.k.a. Quality Value)** is the name given to the cumulative reward we will receive.\n> It can be mathematically stated as follows:\n\n![b1](https:\/\/iq.opengenus.org\/content\/images\/2019\/09\/1-1.PNG)\n\n> The equation above conveys that the Q-Value yielded from being at state s and selecting action a, is the immediate reward received, r(s, a), added to the highest Q-Value possible from state s' (the state the agent ended up after taking action a from state s).\n> \n> \u03b3 is the discount factor, which controls the importance of long-term rewards vs. the immediate one.\n> \n> This is the famous Bellman equation, its Wikipedia page provides a thorough explanation of how its mathematically derived.\n> \n> This equation is quite powerful and is of huge importance to us due to 2 important characteristics:\n> \n> * While we are still preserving the Markov States assumptions, the recursive nature of the Bellman Equation allows rewards from future states to propagate to far-off previous states.\n> \n> * It is not necessary that we know the true Q-Values when starting. Since the equation is a recursive one, we can guess arbitrary values and it will eventually converge to the real ones.","75c6253f":"### Markov Decision Process\n\n#### Why MDP\n> The goal of an agent and an MDP is to maximize it\u2019s cumulative for awards.\n\n> It gives us a way to formalize sequential decision making. This formalization is the basis for problems that are solved with reinforcement learning.\n\n![image.png](https:\/\/drive.google.com\/uc?id=1DrT1YWdQFDdUAOLYEKmyv8kQkmI-hBvK)\n\n> *In reinforcement learning, The Markov Decision Process (MDP) describes the completely observable environment, that is, the observed state content completely determines the characteristics required for decision making. Almost all reinforcement learning problems can be converted to MDP.\n\n#### Markove Chain\n> Markov process, also known as the Markov Chain, is a memory-free random process, which can be represented by a tuple <S,P>, where S is a finite number of state sets, and P is the state transition probability matrix.\n\n> In reinforcement learning, each state the agent is in is a direct consequence of the previous state and the action previously performed.\n>\n> The previous state is also a similar consequence of the one before that, and on it goes until we are left with the initial state.\n>\n> Each one of these steps, and their order, hold information about the current state - and therefore have direct control on which action the Agent should choose next.\n>\n> But we are faced with a problem - the further we go, the more information the Agent needs to save and process at every given step it takes.\n>\n> This can quickly become unfeasible, due to the inability to perform calculations.\n> \n> As a solution to this problem, we assume that all states are Markov States, that is, any state solely depends on the state that came before it (the action performed), and the transition from that state to the current one (the reward given).\n> \n> Have a look at these 2 Tic-Tac-Toe games:\n![m2](https:\/\/iq.opengenus.org\/content\/images\/2019\/09\/1-1.jpeg)\n\n> The final state reached in both the games is exactly the same, but the steps taken to reach there are different for both.\n> \n> In both cases, the blue player must capture the top-right cell, or he will lose.\n> All we needed in order to find this was the last state, nothing else.\n> \n> Note that, **when using the Markov assumption, data is being lost** - this becomes important in complex games such as Go or Chess, where the order of the moves hold implicit information about the opponent's strategy or way of thinking.\n> \n> Nevertheless, the Markov states assumption is fundamental when attempting to calculate long-term strategies.\n\n#### Famous example for Markove Chain\/Process\n\n> It is little bit long and someone already know MDP, so read from here:\nhttps:\/\/www.52coding.com.cn\/2017\/08\/18\/RL%20-%20Markov%20Decision%20Processes\/\n![m1](https:\/\/drive.google.com\/uc?id=1Y_v61gBrv8Rmzl9m-cvIotp0FBvqFzyd)","ef81c478":"### Reinforcement learning\n> To know the Q-learning, we should first know something about reinforcement learning. \n> \n#### What is Reinforcement learning?\n> Reinforcement Learning is defined as a Machine Learning method that is concerned with how software agents should take actions in an environment.\n> It is a class of algorithms.\n\n![image.png](https:\/\/drive.google.com\/uc?id=1-PqwGaynrpJmvZ7aZyf7HVpurZJbhly1)\n\n#### Two bases of RL\n> * AI\/RL = artificial intelligence \/ reinforcement learning: Learning through data\/experience, simulation, model-free methods, feature-based representations\n> * Decision\/Control\/DP = Dynamic programming: Principle of Optimality; Markov decision problem; POMDP; policy iteration\/value iteration\n\n#### Develope History\n> * Optimal control (Bellman, Shannon, and other 1950s)\n> * Al\/Rl and decision\/control\/ DP ideas meet (late 80s-early 90s)\n> * First success, backgammon program (Tesauro, 1992, 1996)\n> * Algorithmic progress, analysis, applications (mid 90s)\n> * Machine learning, big data, robotics, deep neural networks (mid 00s)\n> * AlphaGo and Alphazero (Deep mind, google)\n\n#### Terms in reinforcement learning\n> * Agent: It is an assumed entity which performs actions in an environment to gain some reward.\n> * Environment (e): A scenario that an agent has to face.\n> * Action: The action agent takes\n> * Reward (R): An immediate return given to an agent when he or she performs specific action or task.\n> * State (s): State refers to the current situation returned by the environment.\n> * Policy (\u03c0): It is a strategy which applies by the agent to decide the next action based on the current state.\n> * Value (V): It is expected long-term return with discount, as compared to the short-term reward.\n> * Value Function: It specifies the value of a state that is the total amount of reward. It is an agent which should be expected beginning from that state.\n> * Model of the environment: This mimics the behavior of the environment. It helps you to make inferences to be made and also determine how the environment will behave.\n> * Model based methods: It is a method for solving reinforcement learning problems which use model-based methods.\n> * Q value or action value (Q): Q value is quite similar to value. The only difference between the two is that it takes an additional parameter as a current action.\n\n![rl1](https:\/\/iq.opengenus.org\/content\/images\/2019\/09\/1-2.jpg)\n#### Flappy Bird - example of reinforcement learning\n> * Agent: The bird, the role of machine\n> * Action: The way to make the bird fly\n> * Environment: The whole process of the game where have all kinds of water pipes\n> * Rewards: The farther you fly, the more points you earn\n\n![rl2](https:\/\/www.raspberrypi.org\/app\/uploads\/2019\/12\/flappy-bird-1.jpg)\n\n> The biggest difference you'll find between reinforcement learning and supervised learning and unsupervised learning is that it doesn't require a lot of \"data feeding.\" It's about trying to learn certain skills on your own.\n\n#### The mainstream algorithm of reinforcement learning\n![image.png](https:\/\/drive.google.com\/uc?id=1AztxUGZ4WB3fT7OAlagJmfCSvK3LitT2)\n> Model-free vs. Model-based learning\n\n> *An important difference between the two classifications is whether an agent can fully understand or learn the model of its environment\n\n> * Model-based learning has an advanced knowledge of the environment, so planning can be considered in advance. However, the disadvantage is that if the Model is inconsistent with the real world, it will not perform well in the actual use scenarios.\n> * Model-free gives up Model learning and is less efficient than the former, but it is easier to implement and adjust to a good state in real situations. Lest the model learning approach become more popular, it is more widely developed and tested.","0f12625d":"Much better:)","0de2cb53":"### Q-learning\n\n#### 1. What is Q-learning\n> Q-learning is a kind of reinforcement learning technique used for learning the optimal policy in a MDP), is a value-based method of supplying information to inform which action an agent should take.\n\n#### 2. Why Q-learning\n> The objective of Q-learning is to find a policy that is optimal, in the sense that the expected return over all successive time steps is the maximum achievable. In other word, the goal of Q-learning is to find the optimal policy by learning the optimal two values for each state action period.\n\n#### 3. What is Q-learning table?\n> Q-table is just a fancy name for a simple lookup Table. We calculate the maximum expected future rewards for each state. Basically, this chart will guide us on the best course of action in each state.\n\n> In the Q table, the columns are actions and the rows are states.\n\n![image.png](https:\/\/drive.google.com\/uc?id=1UXKQVszbn75QyHjnsDEOAt6hQJsDnS4J)\n> Each Q score will be the maximum expected future reward that the robot will receive when it takes the action in that state. This is an iterative process because we need to improve q-Table with each iteration.\n\n> To learn each value of the Q-table, we use the Q-Learning algorithm.\n\n> And we will update the value using the Bellman equation\n\n#### 4. The mathematic theory of Q-learning\n> Bellman euqation\n> Using above function, we get the Q value of the cell in the table.\n\n![2](https:\/\/drive.google.com\/uc?id=18vf-Pd3QRptq3xhHKd9mDbcqgpyKkHeD)\n\n#### 5. The algorithm of Q-learning\n![3](https:\/\/drive.google.com\/uc?id=1ahn24Cj0XeY87wHuQK2_YWKBO86f0dhl)\n> **4.1 Initialize the Q-value**\n> * We'll start by building a Q table. We have n columns, where n = actions. We have m rows, where m is equal to the number of states. \n> * We initialize the value to 0.\n> \n> **4.2&4.3 Select and execute the action**\n> * The combination of these steps takes place in an indefinite period of time. This means that the step runs until we stop training, or the training loop stops, as defined in the code.\n> * We will select the action (a) in the state based on q-Table. However, as mentioned earlier, when the series originally started, each Q was 0.\n> * So now the concept of exploring and developing tradeoffs comes into play.\n> * We're going to use something called epsilon's greed strategy.\n> * At first, the epsilon interest rate will be higher. The robot will explore the environment and randomly select actions. The logic behind this is that robots know nothing about the environment.\n> * As robots explore the environment, epsilon rates decrease and robots begin to take advantage of the environment.\n> * During the exploration, the robot gradually became more confident in estimating Q values.\n> * For the robot example, there are four operations to choose from: up, down, left, and right. We start training now - our robot knows nothing about the environment. So the robot chooses random action, right.\n> * We can now update the Q value with the Bellman equation to start and move to the right.\n> \n> **4.4\/4.5 evaluation**\n> * Now we take action and observe the results and rewards. We need to update the function Q (s, A).\n\n![q](https:\/\/iq.opengenus.org\/content\/images\/2019\/09\/1.gif)"}}