{"cell_type":{"cd92d528":"code","7fa04b60":"code","91b7b2cb":"code","8eb6c3a4":"code","d15dbf4c":"code","91219e58":"code","990813ae":"code","6ed9abb2":"code","2da5ee05":"code","b49c8e6e":"code","0c926a92":"code","cbb62336":"code","8c1b0eca":"code","83ff4eba":"code","5b26ef45":"markdown","2b5900cb":"markdown","a91b7952":"markdown","a08c4a21":"markdown","61035e9b":"markdown","f104695c":"markdown","609013ea":"markdown","a3f10cc1":"markdown","3d753302":"markdown","ba4ddf88":"markdown","673d3b70":"markdown","a2f33605":"markdown","a7ddd77a":"markdown","d18f749b":"markdown","9f00ab55":"markdown","7aea8184":"markdown","acdd1a3c":"markdown","9f7edb5d":"markdown","6ac3444b":"markdown","c98efcdf":"markdown","08606a8f":"markdown","e43af8e7":"markdown","a3d8b2cc":"markdown","118e754e":"markdown","61c83de9":"markdown","b33cc219":"markdown","381c4cbe":"markdown","1f03a0b0":"markdown","9e9dd5de":"markdown"},"source":{"cd92d528":"!pip install openpyxl\n!pip install wordcloud\n# GDAL Important library for many geopython libraries\n!apt install gdal-bin python-gdal python3-gdal -y --quiet\n# Install rtree - Geopandas requirment\n!apt install python3-rtree -y --quiet\n# Install Geopandas\n#!pip install git+git:\/\/github.com\/geopandas\/geopandas.git --quiet\n!pip install geopandas\n# Install descartes - Geopandas requirment\n!pip install descartes --quiet\n# Install Pysal\n!pip install pysal --quiet\n# Install splot --> pysal\n!pip install splot --quiet\n# Install mapclassify\n!pip install mapclassify --quiet\n!pip install folium\n\n# Modules for data processing\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport sys\nfrom datetime import datetime\nimport calendar\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# REFERENCE: https:\/\/www.kaggle.com\/dmitryuarov\/eda-covid-19-impact-on-digital-learning\nSTATE_ABBR = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ', 'Arkansas': 'AR',\n    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'District of Columbia': 'DC', 'District Of Columbia': 'DC',\n    'Florida': 'FL', 'Georgia': 'GA', 'Guam': 'GU', 'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL',\n    'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME',\n    'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n    'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN',\n    'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virgin Islands': 'VI', 'Virginia': 'VA', 'Washington': 'WA',\n    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n}\nSTATE_NAME = dict([(y, x) for x, y in STATE_ABBR.items()])\n\n# ########################################################################\n# ### analyze_dataset\n# ########################################################################\n# #\n# # Simple function to help quickly analyze information & usability of\n# # a dataset. Provides information about shape, null values, unique\n# # values & basic statistical features.\n# #\n# # Inputs:\n# #   1.  df_path (string) -> Dataset path (if available)\n# #   2.  df (pandas dataframe) -> Dataset (if available)\n# #   3.  direct_df (boolean) -> Whether dataset path or dataset is\n# #       being provided\n# #   4.  processing_func (function) -> If dataset needs to be processed\n# #       before analyzing\n# #   5.  Other arguments for pd.read_csv(...) if dataset path is being\n# #       provided\n# #\n# # Return:   Either dataframe itself (if path provided) or head of\n# #           dataframe (if dataframe provided)\n# #\n# ########################################################################\n\n# def analyze_dataset(df_path = None, df = None, direct_df = False, processing_func = lambda x: x, **read_csv_args):\n    \n#     if(direct_df == False):\n#         df = pd.read_csv(df_path, **read_csv_args)\n#     df = processing_func(df)\n    \n#     num_rows, num_cols = df.shape\n#     dtypes = dict(df.dtypes.items())\n#     print(\"*****************\")\n#     print(\"Basic Info:\")\n#     print(\"*****************\\n\")\n#     print(f\"Shape of Dataset: {num_rows} rows, {num_cols} cols\")\n#     print(\"Columns:\")\n#     for col_idx, col in enumerate(df.columns):\n#         print(f\"\\t{col_idx+1}. {col}\\n\\t\\t\\t\\t\\t\\t\\t\\t{dtypes[col]}\")\n    \n#     print(\"\\n\\n\\n*****************\")\n#     print(\"Null Values:\")\n#     print(\"*****************\\n\")\n#     nulls = pd.isnull(df).sum()\n#     print(f\"Total Nulls: {nulls.sum()}\")\n#     nulls = nulls[nulls > 0]\n#     nulls = list(sorted(nulls.items(), key = lambda x: x[1], reverse = True))\n#     print(\"Columns with missing values:\")\n#     for col_idx, (col_name, col_missin_num) in enumerate(nulls):\n#         print(f\"\\t{col_idx + 1}. {col_name}\\n\\t\\t\\t\\t\\t\\t\\t\\t{col_missin_num} missing ({col_missin_num \/ num_rows * 100:.1f}%)\")\n    \n#     print(\"\\n\\n\\n*****************\")\n#     print(\"Column-specific:\")\n#     print(\"*****************\\n\")\n#     print(\"Unique values in columns:\")\n#     idx = 1\n#     for col in df.columns:\n#         nunique = df[col].nunique()\n#         if(nunique < 10):\n#             unique_vals = [\"'\" + str(x) + \"'\" for x in df[col].unique()]\n#             print(f\"{idx}. {col} has {nunique} unique values\")\n#             idx += 1\n#             print(f\"\\t[ {', '.join(unique_vals)} ]\")\n#     print(\"\\n\\nStatistical Features:\")\n#     print(df.describe())\n    \n#     print(\"\\n\\n\")\n#     if(direct_df == True):\n#         return df.head()\n#     else:\n#         return df\n\n########################################################################\n### load_main_dataset\n########################################################################\n#\n# Function to load the main processed & merged dataset for\n# engagement. Has options to merge selected datasets. The processing\n# has been done separately in another function `process_main_dataset`\n#\n# Inputs:\n#   1.  whether_merge_district (boolean) -> Whether to merge districts\n#       data\n#   2.  whether_merge_products (boolean) -> Whether to merge products\n#       data\n#   3.  whether_merge_dates (boolean) -> Whether to merge dates data\n#\n# Return:   Engagement data merged with other relevant datasets\n#\n########################################################################\n\ndef load_main_dataset(whether_merge_districts = True, whether_merge_products = True, whether_merge_dates = True):\n    def reduce_dtype_size(df):\n        numeric_cols = [x for x in df.columns if (df[x].dtype != object) & ('datetime' not in str(df[x].dtype))]\n        for numeric_col in numeric_cols:\n            if('float' in str(df[numeric_col].dtype)):\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'float')\n            elif(('uint' in str(df[numeric_col].dtype)) | ('bool' in str(df[numeric_col].dtype))):\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'unsigned')\n            else:\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'signed')\n        return df\n    \n    def merge_districts_data(engagement_data):\n        districts_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/districts_data.csv')\n        districts_data = reduce_dtype_size(districts_data)\n        merged_engagement_data = pd.merge(engagement_data, districts_data, how = 'left', on = 'district_id')\n        return merged_engagement_data\n\n    def merge_products_data(engagement_data):\n        products_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/products_data.csv')\n        products_data = reduce_dtype_size(products_data)\n        merged_engagement_data = pd.merge(engagement_data, products_data, how = 'left', left_on = 'lp_id', right_on = 'LP ID')\n        merged_engagement_data = merged_engagement_data.drop('LP ID', axis = 1)\n        return merged_engagement_data\n\n    def merge_dates_data(engagement_data):\n        dates_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/dates_data.csv', parse_dates = ['date'])\n        dates_data = reduce_dtype_size(dates_data)\n        merged_engagement_data = pd.merge(engagement_data, dates_data, how = 'left', left_on = 'time', right_on = 'date')\n        merged_engagement_data = merged_engagement_data.drop('date', axis = 1)\n        return merged_engagement_data\n\n    engagement_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/engagement_data.csv', parse_dates = ['time'])\n    engagement_data = reduce_dtype_size(engagement_data)\n    if(whether_merge_districts == True):\n        engagement_data = merge_districts_data(engagement_data)\n    if(whether_merge_products == True):\n        engagement_data = merge_products_data(engagement_data)\n    if(whether_merge_dates == True):\n        engagement_data = merge_dates_data(engagement_data)\n    \n    return engagement_data\n\n\n# ########################################################################\n# ### process_main_dataset\n# ########################################################################\n# #\n# # Function to process the main processed & save it for\n# # loading later from another function `load_main_dataset`.\n# #\n# # Inputs:\n# #   1.  whether_load_url_html_data (boolean) -> Whether to process\n# #       and save URL's HTML data\n# #\n# # Return:   None\n# #\n# ########################################################################\n\n# def process_main_dataset(whether_load_url_html_data = False):\n#     def get_all_na_idx(df):\n#         all_na_idx = df.isnull().all(axis=1)\n#         return all_na_idx[all_na_idx == True].keys()\n\n#     def add_dummys(df, dummy_cols, remove_orig_dummy_cols = False):\n#         dummy_df = df[dummy_cols]\n#         dummy_df = pd.get_dummies(dummy_df)\n        \n#         df = pd.concat([df, dummy_df], axis = 1)\n#         if(remove_orig_dummy_cols == True):\n#             df = df.drop(dummy_cols, axis = 1)\n        \n#         return df\n\n#     def reduce_dtype_size(df):\n#         numeric_cols = [x for x in df.columns if (df[x].dtype != object) & ('datetime' not in str(df[x].dtype))]\n#         for numeric_col in numeric_cols:\n#             if('float' in str(df[numeric_col].dtype)):\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'float')\n#             elif(('uint' in str(df[numeric_col].dtype)) | ('bool' in str(df[numeric_col].dtype))):\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'unsigned')\n#             else:\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'signed')\n#         return df\n    \n#     def load_districts_data():\n        \n#         def districts_data_preprocessing(districts_data):\n            \n#             def process_lower_upper_bounds(df_series):\n#                 processed_lower_series = []\n#                 processed_upper_series = []\n                \n#                 for row in df_series:                \n#                     if(pd.isnull(row) == True):\n#                         processed_lower_series.append(row)\n#                         processed_upper_series.append(row)\n#                     else:\n#                         assert(len(row[1:-1].split(', ')) == 2)\n#                         lower_val, upper_val = row[1:-1].split(', ')\n#                         lower_val = float(lower_val)\n#                         upper_val = float(upper_val)\n#                         processed_lower_series.append(lower_val)\n#                         processed_upper_series.append(upper_val)\n                \n#                 return processed_lower_series, processed_upper_series\n            \n#             for col in ['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw']:\n#                 lower_series, upper_series = process_lower_upper_bounds(districts_data[col])\n#                 districts_data[col + '_lower_bound'] = pd.Series(lower_series, index = districts_data.index)\n#                 districts_data[col + '_upper_bound'] = pd.Series(upper_series, index = districts_data.index)\n#                 districts_data[col + '_bound_avg'] = pd.Series(np.add(lower_series, upper_series) \/ 2.0, index = districts_data.index)\n            \n#             districts_data = districts_data.drop(['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw'], axis = 1)\n#             return districts_data\n        \n#         districts_data = pd.read_csv('\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\n        \n#         districts_data = districts_data_preprocessing(districts_data)\n#         districts_data = districts_data.drop(get_all_na_idx(districts_data.drop('district_id', axis = 1))).reset_index(drop = True)\n#         districts_data = add_dummys(districts_data, ['locale'], remove_orig_dummy_cols = False)\n        \n#         #all_states = districts_data['state'].unique()\n#         #district_id_state_map = dict(districts_data[['district_id', 'state']].values)\n#         #state_district_id_map = dict([(x, [y for y in district_id_state_map if district_id_state_map[y] == x]) for x in all_states])\n#         #districts_data = districts_data.drop('state', axis = 1)\n        \n#         districts_data = reduce_dtype_size(districts_data)\n#         return districts_data\n\n#     districts_data = load_districts_data()\n    \n#     # URL Information Extraction\n#     #   For July 2021\n#     #   Using similarweb.com\n#     #   Avg Duration - in seconds\n#     #   Total Visits - in 1000s\n#     def load_url_html_data():\n#         url_html_dict = {}\n\n#         all_html_content = \"\"\n#         with open(f'.\/Data\/url_info\/combined_url_info_data.txt', 'r') as html_file:\n#             all_html_content = html_file.read()\n\n#         tot_num_files = len([x for x in all_html_content.split('---') if len(x.strip()) != 0])\n#         print(f\"Total No. of files: {tot_num_files}\\n\\n\")\n\n#         for html_content_idx, html_content in enumerate(all_html_content.split('---')):\n            \n#             html_content = html_content.strip()\n#             if(len(html_content) == 0):\n#                 print(\"ERROR: URL not found\")\n#                 sys.exit(\"\")\n            \n#             url_name = html_content.split('<')[0].strip()\n#             html_content = '<'.join(html_content.split('<')[1:])\n#             print(f\"{html_content_idx + 1}. File: {url_name}\\n\")\n#             if(url_name in url_html_dict):\n#                 print(\"ERROR: Name already exists\")\n#                 sys.exit(\"\")\n#             url_html_dict[url_name] = {}\n            \n#             # Global Rank\n#             global_rank = re.findall('\\\"GlobalRank\":\\[\\d+,\\d+,-?\\d+,\\d+\\]', html_content)\n#             if(len(global_rank) != 1):\n#                 print(\"ERROR: Global Rank\")\n#                 print(global_rank)\n#                 sys.exit(\"\")\n#             global_rank = int(global_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['global_rank'] = global_rank\n\n#             # Country\n#             if('<img class=\"websiteRanks-titleIconImg\" src=\"\/images\/flags-svg\/flag-icon-us.svg\">' in html_content):\n#                 country = 'USA'\n#             else:\n#                 print(\"Country: Not USA!\")\n#                 country = 'Not_USA'\n#             url_html_dict[url_name]['country'] = country\n            \n#             # Country Rank\n#             country_rank = re.findall('\\\"CountryRanks\":\\{\"\\d+\":\\[\\d+,\\d+,-?\\d+,\\d+\\]\\}', html_content)\n#             if(len(country_rank) != 1):\n#                 print(\"ERROR: Country Rank\")\n#                 print(country_rank)\n#                 sys.exit(\"\")\n#             country_rank = int(country_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['country_rank'] = country_rank\n            \n#             # Category\n#             category = re.findall('<a class=\"websiteRanks-nameText\" data-analytics-category=\"Internal Link\" data-analytics-label=\"Category Rank\/.+\" href=\"\/top-websites\/category\/.+\" itemprop=\"significantLink\">.+<\/a>', html_content)\n#             if(len(category) != 1):\n#                 print(\"ERROR: Category\")\n#                 print(category)\n#                 url_html_dict[url_name]['main_category'] = np.nan\n#                 url_html_dict[url_name]['sub_category'] = np.nan\n#             else:\n#                 category = category[0].split('Category Rank\/')[1].split('\"')[0]\n#                 main_category = category.split('\/')[0]\n#                 url_html_dict[url_name]['main_category'] = main_category\n#                 if(len(category.split('\/')) != 1):\n#                     sub_category = category.split('\/')[-1]\n#                 else:\n#                     sub_category = \"\"\n#                 url_html_dict[url_name]['sub_category'] = sub_category\n\n#             # Category Rank\n#             category_rank = re.findall('\\\"CategoryRank\\\":\\[\\d+,\\d+,-?\\d+,\\d+\\]', html_content)\n#             if(len(category_rank) != 1):\n#                 print(\"ERROR: Category Rank\")\n#                 print(category_rank)\n#                 sys.exit(\"\")\n#             category_rank = int(category_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['category_rank'] = category_rank\n\n#             # Total Visits\n#             total_visits = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">.*\\d+[KMB]<\/span>', html_content)\n#             if(len(total_visits) != 1):\n#                 print(\"ERROR: Total Visits\")\n#                 print(total_visits)\n#                 url_html_dict[url_name]['total_visits'] = np.nan\n#             else:\n#                 total_visits = total_visits[0].split('>')[1].split('<')[0]\n#                 units = total_visits[-1]\n#                 total_visits = float(''.join([x for x in total_visits if x.isdigit()]))\n#                 if(units == 'K'):\n#                     total_visits = total_visits * 1\n#                 elif(units == 'M'):\n#                     total_visits = total_visits * 1000\n#                 elif(units == 'B'):\n#                     total_visits = total_visits * 1000000\n#                 url_html_dict[url_name]['total_visits'] = total_visits\n\n#             # Avg Duration\n#             avg_duration = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+:\\d+:\\d+<\/span>', html_content)\n#             if(len(avg_duration) != 1):\n#                 print(\"ERROR: Avg Duration\")\n#                 print(avg_duration)\n#                 url_html_dict[url_name]['avg_duration'] = np.nan\n#             else:\n#                 avg_duration = avg_duration[0].split('>')[1].split('<')[0]\n#                 hr_val, min_val, sec_val = avg_duration.split(':')\n#                 avg_duration = 3600 * int(hr_val) + 60 * int(min_val) + int(sec_val)\n#                 url_html_dict[url_name]['avg_duration'] = avg_duration\n\n#             # Page Visits\n#             page_visits = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+\\.\\d+<\/span>', html_content)\n#             if(len(page_visits) != 1):\n#                 print(\"ERROR: Page Visits\")\n#                 print(page_visits)\n#                 url_html_dict[url_name]['page_visits'] = np.nan\n#             else:\n#                 page_visits = float(page_visits[0].split('>')[1].split('<')[0])\n#                 url_html_dict[url_name]['page_visits'] = page_visits\n\n#             # Bounce Rate\n#             bounce_rate = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+.\\d+%<\/span>', html_content)\n#             if(len(bounce_rate) != 1):\n#                 print(\"ERROR: Bounce Rate\")\n#                 print(bounce_rate)\n#                 url_html_dict[url_name]['bounce_rate'] = np.nan\n#             else:\n#                 bounce_rate = float(bounce_rate[0].split('>')[1].split('%')[0])\n#                 url_html_dict[url_name]['bounce_rate'] = bounce_rate\n            \n#             # Description\n#             description = re.findall('<p itemprop=\"description\" class=\"websiteHeader-companyDescription js-companyDescription\">.+<\/p>', html_content)\n#             if(len(description) != 1):\n#                 url_html_dict[url_name]['description'] = np.nan\n#             else:\n#                 description = description[0].split('>')[1].split('<')[0]\n#                 url_html_dict[url_name]['description'] = description\n\n#         url_html_df = pd.DataFrame.from_dict(url_html_dict, orient = 'index').reset_index(drop = False)\n#         url_html_df.columns = ['URL'] + [*url_html_df.columns][1:]\n\n#         url_html_df['global_rank'] = url_html_df['global_rank'].replace({0: np.nan})\n#         url_html_df['country_rank'] = url_html_df['country_rank'].replace({0: np.nan})\n#         url_html_df['category_rank'] = url_html_df['category_rank'].replace({0: np.nan})\n\n#         additional_data = pd.read_csv('.\/Data\/url_info\/url_info_mobile.csv')\n#         url_html_df = pd.concat([url_html_df, additional_data], axis = 0)\n\n#         def find_subpage_level(url):\n#             level = url.split(':\/\/')[1]\n#             level = level.split('\/')\n#             level = [x for x in level if len(x.strip()) > 0]\n#             return len(level) - 1\n#         url_html_df['URL_subpage_level'] = url_html_df['URL'].apply(find_subpage_level)\n#         url_html_df['URL_subpage_visits'] = url_html_df.apply(lambda x: x['total_visits'] * (((100 - x['bounce_rate']) \/ 100) ** x['URL_subpage_level']), axis = 1)\n#         url_html_df['URL_page_duration'] = url_html_df['avg_duration'] \/ url_html_df['page_visits']\n#         url_html_df['URL_subpage_total_browsing_days'] = url_html_df['URL_subpage_visits'] * url_html_df['URL_page_duration'] \/ 60 \/ 60 \/ 24\n#         url_html_df['URL_subpage_avg_browsing_days'] = url_html_df['URL_subpage_total_browsing_days'] \/ 31\n#         url_html_df.to_csv('.\/Data\/url_info\/final_url_data.csv', index = False)\n    \n#     def load_products_data():\n    \n#         def products_data_preprocessing(products_data):\n\n#             products_data['Primary Category'] = products_data['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if pd.isna(x) == False else np.nan)\n#             products_data['Primary Category'] = products_data['Primary Category'].map({'LC': 'LC', 'CM': 'CM', 'SDO': 'SDO', 'LC\/CM\/SDO': 'Other'})\n#             products_data['Primary Essential Function'] = products_data['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if pd.isna(x) == False else np.nan)\n            \n#             def sector_map(sectors):\n                \n#                 sector_prek12 = []\n#                 sector_higher_ed = []\n#                 sector_corporate = []\n                \n#                 for sector in sectors:\n#                     if(pd.isna(sector) == True):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12; Higher Ed'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12; Higher Ed; Corporate'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(1)\n#                     elif(sector == 'Corporate'):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(1)\n#                     elif(sector == 'Higher Ed; Corporate'):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(1)\n#                     else:\n#                         print(f\"***\\nUnknown sector detected! {sector}\\n***\")\n                \n#                 return sector_prek12, sector_higher_ed, sector_corporate\n            \n#             sector_prek12, sector_higher_ed, sector_corporate = sector_map(products_data['Sector(s)'])\n#             products_data = products_data.assign(Sector_prek12 = sector_prek12)\n#             products_data = products_data.assign(Sector_higher_ed = sector_higher_ed)\n#             products_data = products_data.assign(Sector_corporate = sector_corporate)\n            \n#             products_data['Primary Essential Function'] = products_data['Primary Essential Function'].replace({\"Sites, Resources & References\": \"Sites, Resources & Reference\"})\n            \n#             # Correcting small mistakes\n#             products_data['URL'] = products_data['URL'].replace({'https:\/\/fligprid.com': 'https:\/\/flipgrid.com'})\n\n#             if(whether_load_url_html_data == True):\n#                 load_url_html_data()\n#             url_info_data = pd.read_csv('.\/Data\/url_info\/final_url_data.csv')\n#             url_info_data.columns = ['mainURL_' + x if x != 'URL' else x for x in url_info_data.columns]\n#             products_data = pd.merge(products_data, url_info_data, how = 'left', on = 'URL')\n            \n#             products_data['mainURL_country_rank'] = products_data.apply(lambda x: x if x['mainURL_country'] == 'USA' else np.nan, axis = 1)\n            \n#             return products_data\n        \n#         products_data = pd.read_csv('.\/Data\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')\n        \n#         products_data = products_data_preprocessing(products_data)\n#         products_data = add_dummys(products_data, ['Sector(s)', 'Primary Category', 'Primary Essential Function', 'mainURL_main_category'], remove_orig_dummy_cols = False)\n        \n#         products_data = reduce_dtype_size(products_data)\n#         return products_data\n\n#     products_data = load_products_data()\n\n#     # Not focusing on vacation dates since time-analysis is not priority\n\n#     def load_dates_data():\n#         days_2020 = pd.date_range(datetime(2020, 1, 1), datetime(2020, 12, 31))\n#         dates_data = pd.DataFrame.from_dict({'date': days_2020})\n        \n#         dates_data['month'] = dates_data['date'].apply(lambda x: x.month)\n#         dates_data['day'] = dates_data['date'].apply(lambda x: x.day)\n#         dates_data['day_of_week'] = dates_data['date'].apply(lambda x: calendar.day_name[x.weekday()])\n#         dates_data['is_weekend'] = dates_data['day_of_week'].apply(lambda x: 1 if (x == 'Saturday') | (x == 'Sunday') else 0)\n        \n#         us_holidays = pd.read_csv('.\/Data\/US Holiday Dates (2004-2021).csv', usecols = ['Date', 'Holiday'], parse_dates = ['Date'])\n#         dates_data = pd.merge(dates_data, us_holidays, how = 'left', left_on = 'date', right_on = 'Date')\n#         dates_data = dates_data.drop('Date', axis = 1)\n#         dates_data['is_holiday'] = dates_data['Holiday'].apply(lambda x: 0 if pd.isnull(x) == True else 1)\n        \n#         dates_data = add_dummys(dates_data, ['day_of_week', 'Holiday'], remove_orig_dummy_cols = False)\n#         dates_data = reduce_dtype_size(dates_data)\n#         return dates_data\n\n#     dates_data = load_dates_data()\n\n#     def load_engagement_data():\n\n#         engagement_data = pd.DataFrame()\n#         districts = []\n        \n#         for x in os.listdir('.\/Data\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/'):\n#             data_x = pd.read_csv(f'.\/Data\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/{x}', parse_dates = ['time'])\n#             engagement_data = pd.concat([engagement_data, data_x], axis = 0)\n#             districts.extend([int(x.split('.')[0])] * data_x.shape[0])\n        \n#         engagement_data['district_id'] = pd.Series(districts, index = engagement_data.index)\n        \n#         top_products_id = list(products_data['LP ID'].unique())\n#         districts_id = list(districts_data['district_id'].unique())\n#         engagement_data = engagement_data[engagement_data['lp_id'].isin(top_products_id)]\n#         engagement_data = engagement_data[engagement_data['district_id'].isin(districts_id)]\n        \n#         same_url_map = {\n#             33562: 75206,\n#             87841: 35971\n#         }\n#         engagement_data['lp_id'] = engagement_data['lp_id'].replace(same_url_map)\n#         engagement_data = engagement_data.groupby(['time', 'lp_id', 'district_id'])[['pct_access', 'engagement_index']].aggregate(np.nansum).reset_index()\n        \n#         engagement_data = reduce_dtype_size(engagement_data)\n#         return engagement_data\n\n#     engagement_data = load_engagement_data()\n\n#     # # Saving all datasets\n#     districts_data.to_csv('.\/Data\/Processed_Dataset\/districts_data.csv', index = False)\n#     products_data.to_csv('.\/Data\/Processed_Dataset\/products_data.csv', index = False)\n#     dates_data.to_csv('.\/Data\/Processed_Dataset\/dates_data.csv', index = False)\n#     engagement_data.to_csv('.\/Data\/Processed_Dataset\/engagement_data.csv', index = False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nCOLOR_WHITE = '#F8F1FF'\nCOLOR_BLACK = '#231942'\nCOLOR_DARK_BLUE = '#156BB7'\nCOLOR_LIGHT_BLUE = '#63D1DF'\nCOLOR_GREEN = '#30DB8D'\nCOLOR_DARK_GREEN = '#0DAB6C'\nCOLOR_ORANGE = '#FBAB60'\nCOLOR_YELLOW = '#F8E16C'\nCOLOR_RED = '#DA4167'\n\nPLOT_THEME_LIGHT = {\n    'text': COLOR_BLACK,\n    'axis': COLOR_BLACK,\n    'subtitle': COLOR_DARK_BLUE,\n    'color+1': COLOR_DARK_GREEN,\n    'color+2': COLOR_YELLOW,\n    'color+3': COLOR_ORANGE,\n    'color+4': COLOR_DARK_BLUE,\n    'color-1': COLOR_RED,\n    'bg': COLOR_LIGHT_BLUE,\n    'inv': COLOR_WHITE,\n    'color+1_lower': '#064B30',\n    'color+1_higher': '#2EEFA2',\n    'gray': '#676076',\n}\nPLOT_THEME_LIGHT['groups'] = [PLOT_THEME_LIGHT[x] for x in ['color+1', 'color-1', 'color+3', 'color+4', 'color+2']]\n\nPLOT_THEME_DARK = {\n    'text': COLOR_WHITE,\n    'axis': COLOR_WHITE,\n    'subtitle': COLOR_LIGHT_BLUE,\n    'color+1': COLOR_GREEN,\n    'color+2': COLOR_YELLOW,\n    'color+3': COLOR_ORANGE,\n    'color+4': COLOR_LIGHT_BLUE,\n    'color-1': COLOR_RED,\n    'bg': COLOR_DARK_BLUE,\n    'inv': COLOR_BLACK,\n    'color+1_lower': '#188B57',\n    'color+1_higher': '#86EABD',\n    'gray': '#D6CFDB',\n}\nPLOT_THEME_DARK['groups'] = [PLOT_THEME_DARK[x] for x in ['color+1', 'color-1', 'color+3', 'color+4', 'color+2']]\n\ndef create_fig(nrows = 1, ncols = 1, width = 10, height = 5):\n    fig, ax = plt.subplots(nrows, ncols, figsize = (width, height))\n    return fig, ax\n\ndef remove_spines(ax, theme = {}):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_color(theme['axis'])\n    ax.spines['left'].set_color(theme['axis'])\n    return ax\n\ndef remove_all_spines(ax):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return ax\n\ndef set_titles_and_labels(fig, ax, fig_title = \"\", title = \"\", xlabel = \"\", ylabel = \"\", theme = {}):\n    fig.suptitle(fig_title, fontsize = 30, color = theme['text'])\n    ax.set_title(title, fontsize = 20, color = theme['subtitle'])\n    ax.set_xlabel(xlabel, fontsize = 15, color = theme['text'])\n    ax.set_ylabel(ylabel, fontsize = 15, color = theme['text'])\n    return fig, ax\n\ndef set_ticks(ax, theme):\n    ax.tick_params(axis = 'x', colors = theme['axis'])\n    ax.tick_params(axis = 'y', colors = theme['axis'])\n    return ax\n\ndef set_xticklabels(ax, labels, rotate_x = 0, theme = {}):\n    ax.set_xticks(np.arange(len(labels)))\n    ax.set_xticklabels(labels, color = theme['text'], rotation = rotate_x)\n    return ax\n\ndef set_yticklabels(ax, labels, rotate_y = 0, theme = {}):\n    ax.set_yticks(np.arange(len(labels)))\n    ax.set_yticklabels(labels, color = theme['text'], rotation = rotate_y)\n    return ax\n\ndef set_bg(fig, ax, theme):\n    fig.set_facecolor(theme['bg'])\n    ax.set_facecolor(theme['bg'])\n    return fig, ax\n\ndef select_theme(theme):\n    if(theme == 'DARK'):\n        return PLOT_THEME_DARK\n    else:\n        return PLOT_THEME_LIGHT\n\ndef set_legend(ax, theme):\n    ax.legend(loc = 'best')\n    return ax\n\ndef plot_decoration():\n    return \"\"\"\n    fig, ax = set_bg(fig, ax, theme); ax = set_ticks(ax, theme); ax = remove_spines(ax, theme); fig, ax = set_titles_and_labels(fig, ax, suptitle, title, xlabel, ylabel, theme);\n    \"\"\".strip()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_lineplot(x_vals, y_vals, width = 15, height = 7, labels = [], suptitle = \"Lineplot\", title = \"Demo\", xlabel = \"\", ylabel = \"\", theme = 'DARK', x_names = [], rotate_x = 0):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    if(len(y_vals) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n    else:\n        group_colors = theme['groups'][:len(y_vals)]\n    for y_val_idx, y_val in enumerate(y_vals):\n        if(len(labels) == 0):\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = f'line #{y_val_idx + 1}')\n        else:\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = labels[y_val_idx])\n    ax = set_legend(ax, theme)\n    ax = set_xticklabels(ax, x_names, rotate_x = rotate_x, theme = theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_barplot(x_names, y_vals, cats = [], width = 15, height = 7, suptitle = \"Barplot\", title = \"Demo\", xlabel = '', ylabel = '', theme = 'DARK', rotate_x = 0, rotate_y = 0):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    x_vals = np.arange(len(x_names))\n    if(len(cats) > 0):\n        uniq_cats = list(sorted(pd.Series(cats).unique()))\n        if(len(uniq_cats) > len(theme['groups'])):\n            group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\n        else:\n            group_colors = theme['groups'][:len(uniq_cats)]\n        group_colors = [group_colors[uniq_cats.index(x)] for x in cats]\n    else:\n        if(len(y_vals) > len(theme['groups'])):\n            group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n        else:\n            group_colors = theme['groups'][:len(y_vals)]\n    ax.bar(x_vals, y_vals, color = group_colors)\n    ax = set_xticklabels(ax, x_names, rotate_x = rotate_x, theme = theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_scatterplot(x_vals, y_vals, cats = [1], width = 15, height = 7, suptitle = \"Scatterplot\", title = 'Demo', xlabel = '', ylabel = '', theme = 'DARK', annotate = False, annotate_texts = []):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    no_cats_passed = False\n    if(len(cats) == 1):\n        cats = np.ones(len(x_vals))\n        no_cats_passed = True\n    uniq_cats = pd.Series(cats).unique()\n    if(len(uniq_cats) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\n    else:\n        group_colors = theme['groups'][:len(uniq_cats)]\n    for cat_idx, cat in enumerate(uniq_cats):\n        ax.scatter(x_vals[cats == cat], y_vals[cats == cat], color = group_colors[cat_idx], label = cat)\n    if(annotate == True):\n        for idx in range(len(x_vals)):\n            ax.annotate(annotate_texts[idx], (x_vals[idx], y_vals[idx]), color = theme['text'])\n    if(no_cats_passed == False):\n        ax = set_legend(ax, theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_us_map(state_vals_df, title, val_col, val_label, range_min_val = 0, range_max_val = 1, theme = 'DARK', state_col = 'STATE_ABBR'):\n    \n    theme = select_theme(theme)\n\n    layout = dict(\n        font_family = 'Source Sans Pro',\n        font_color = theme['text'],\n        title_text = title,\n        # To change\n        title_font = dict(\n            family = \"Source Sans Pro\",\n            size = 25,\n            color = theme['axis']\n        ),\n        geo_scope = 'usa',\n        paper_bgcolor = theme['bg'],\n        geo_bgcolor = theme['bg'],\n        geo = dict(\n            landcolor = theme['inv'],\n            subunitcolor = theme['gray'],\n            lakecolor = theme['bg'],\n        ),\n    )\n\n    fig = px.choropleth(\n        state_vals_df,\n        locations = state_col,\n        color = val_col,\n        color_continuous_scale = [theme['color+1_lower'], theme['color+1_higher']],\n        range_color = (range_min_val, range_max_val),\n        locationmode = \"USA-states\",\n        labels = {val_col : val_label, state_col: 'State'},\n    )\n\n    fig.update_layout(layout)\n    fig.update_layout(margin = {\"r\": 0, \"l\": 0, \"b\": 15})\n    fig.show()\n    \nDATA = load_main_dataset()","7fa04b60":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport sys\nimport time\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nfrom scipy.stats import pearsonr\n\nimport geopandas as gpd\nimport libpysal as lps\nimport esda\nfrom esda.moran import Moran_Local\nfrom esda.moran import Moran\nfrom splot.esda import plot_moran, moran_scatterplot, lisa_cluster\nimport seaborn as sns\nimport folium\n\ndef load_covid19_severity_data():\n    df = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/OxCGRT_latest.csv', usecols = ['CountryName', 'RegionName', 'Date', 'Jurisdiction', 'ConfirmedCases', 'ConfirmedDeaths'], parse_dates = ['Date'])\n    df = df[(df['CountryName'] == 'United States') & (df['Jurisdiction'] == 'STATE_TOTAL')]\n    df = df.drop(['CountryName', 'Jurisdiction'], axis = 1)\n    df = df[df['RegionName'] != 'Washington DC'].reset_index(drop = True)\n    df['Month'] = df['Date'].apply(lambda x: x.month)\n\n    us_state_pop_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/nst-est2020.csv', usecols = ['NAME', 'POPESTIMATE042020', 'POPESTIMATE2020'])\n    us_state_pop_data.columns = ['RegionName', 'Population_April1_2020', 'Population_July1_2020']\n    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n    days_in_months = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n    middays = [31 \/\/ 2]\n    for month_idx in range(1, 12):\n        middays.append(np.sum(days_in_months[: month_idx - 1]) + days_in_months[month_idx] \/\/ 2)\n    for month_idx in range(12):\n        us_state_pop_data[month_idx + 1] = us_state_pop_data['Population_July1_2020'] + (us_state_pop_data['Population_July1_2020'] - us_state_pop_data['Population_April1_2020']) \/ (183 - 92) * (middays[month_idx] - 183)\n    us_state_pop_data = us_state_pop_data.drop(['Population_April1_2020', 'Population_July1_2020'], axis = 1)\n    us_state_pop_data = pd.melt(us_state_pop_data, id_vars = 'RegionName', var_name = 'Month', value_name = 'Population')\n\n    df = pd.merge(df, us_state_pop_data, how = 'left', on = ['RegionName', 'Month'])\n    df = df.groupby(['RegionName', 'Month'])[['ConfirmedCases', 'ConfirmedDeaths', 'Population']].aggregate(np.nanmean).reset_index()\n    df['death_to_cases_ratio'] = (df['ConfirmedDeaths'] \/ (df['ConfirmedCases'] + 0.0000000000001))\n    df['cases_to_pop_ratio'] = (df['ConfirmedCases'] \/ (df['Population'] + 0.0000000000001))\n    df['covid19_severity_index'] = (2 * df['death_to_cases_ratio'] + df['cases_to_pop_ratio']) \/ 3.0\n    \n    #df = df.drop(['ConfirmedCases', 'ConfirmedDeaths', 'Population', 'death_to_cases_ratio', 'cases_to_pop_ratio'], axis = 1)\n    return df\n\ncovid19_severity_data = load_covid19_severity_data()\ncovid19_severity_data['RegionName'] = covid19_severity_data['RegionName'].replace(STATE_ABBR)\n\nengagement_data = DATA.groupby(['state', 'month'])[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\n\nstate_vals_df = covid19_severity_data.copy()\ntitle = \"Covid Severity - Statewise Timeline\"\nval_col = \"covid19_severity_index\"\nval_label = \"Severity Index\"\nrange_min_val = 0.005\nrange_max_val = 0.05\ntheme = 'DARK'\n\n# state_vals_df = covid19_severity_data\n# title = \"Covid Severity\"\n# val_col = \"ConfirmedCases\"\n# val_label = \"Confirmed Cases\"\n# range_min_val = 1500\n# range_max_val = 2500000\n# theme = 'DARK'\n\nstate_vals_df['Month'] = state_vals_df['Month'].apply(lambda x: ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][x - 1])\n\ntheme = select_theme(theme)\n\nlayout = dict(\n    font_family = 'Source Sans Pro',\n    font_color = theme['text'],\n    title_text = title,\n    # To change\n    title_font = dict(\n        family = \"Source Sans Pro\",\n        size = 25,\n        color = theme['axis']\n    ),\n    geo_scope = 'usa',\n    paper_bgcolor = theme['bg'],\n    geo_bgcolor = theme['bg'],\n    geo = dict(\n        landcolor = theme['inv'],\n        subunitcolor = theme['gray'],\n        lakecolor = theme['bg'],\n    ),\n)\n\nfig = px.choropleth(\n    state_vals_df,\n    locations = 'RegionName',\n    color = val_col,\n    color_continuous_scale = [theme['color-1'], theme['color+1_higher']],\n    range_color = (range_min_val, range_max_val),\n    locationmode = \"USA-states\",\n    labels = {val_col : val_label, 'RegionName': 'State'},\n    animation_frame = 'Month'\n)\n\nfig.update_layout(layout)\nfig.update_layout(margin = {\"r\": 0, \"l\": 0, \"b\": 15})\nfig.show()","91b7b2cb":"temp_data = covid19_severity_data.groupby('RegionName')['covid19_severity_index'].aggregate(np.nanmean).reset_index(drop = False)\n\nnbr_gdf = gpd.read_file('\/kaggle\/input\/learnplatform-analysis-data\/tl_2020_us_state.shp')[['NAME', 'geometry']]\nexclude_cols = ['Hawaii', 'United States Virgin Islands', 'Commonwealth of the Northern Mariana Islands', 'Guam', 'American Samoa', 'Puerto Rico', 'Alaska']\nnbr_gdf = nbr_gdf[~nbr_gdf['NAME'].isin(exclude_cols)]\nnbr_gdf['NAME'] = nbr_gdf['NAME'].replace(STATE_ABBR)\nnbr_final = nbr_gdf.merge(temp_data, how = 'inner', right_on = 'RegionName', left_on = 'NAME')\n\nw =  lps.weights.Queen.from_dataframe(nbr_final)\nw.transform = 'r'\nnbr_final['weighted_covid19_severity_index'] = lps.weights.lag_spatial(w, nbr_final['covid19_severity_index'])\ny = nbr_final.covid19_severity_index\nmoran = esda.Moran(y, w)\nplot_moran(moran, zstandard=True, figsize=(12,6))\nplt.ylabel('Spatial Lag of Covid19 Severity Index')\nplt.xlabel('Covid19 Severity Index')\nplt.tight_layout()\nplt.show()","8eb6c3a4":"m_local = Moran_Local(y, w)\nP_VALUE = 0.05\nfig, ax = plt.subplots(figsize=(10,10))\nmoran_scatterplot(m_local, p=P_VALUE, ax=ax)\nax.set_xlabel('Covid19 Severity Index', fontsize = 10)\nax.set_ylabel('Spatial Lag of Covid19 Severity Index', fontsize = 10)\nplt.text(1.35, 0.5, 'HH', fontsize=25)\nplt.text(1.65, -0.8, 'HL', fontsize=25)\nplt.text(-1.5, 0.6, 'LH', fontsize=25)\nplt.text(-1.3, -0.8, 'LL', fontsize=25)\nplt.show()","d15dbf4c":"fig, ax = plt.subplots(figsize=(12,10))\nlisa_cluster(m_local, nbr_final, p=P_VALUE, ax=ax)\nnbr_final.boundary.plot(ax=ax)\nplt.title('Spatial Autocorrelation Map for Covid19 Severity Index', fontsize = 20)\nplt.tight_layout()\nplt.show()","91219e58":"covid_engagement_data = pd.merge(engagement_data, covid19_severity_data, how = 'inner', left_on = ['state', 'month'], right_on = ['RegionName', 'Month'])\nstatewise_covid_index = covid_engagement_data.groupby('state')['covid19_severity_index'].aggregate(np.nanmean).reset_index(drop = False)\n\nsel_states = ['WA', 'UT', 'VA', 'NJ', 'NY', 'MA']\n# sel_states = covid_engagement_data['state'].unique()\n\ndatas = {}\nfor state in sel_states:\n    datas[state] = covid_engagement_data[covid_engagement_data['state'] == state]\n\n# First set up the figure, the axis, and the plot element we want to animate\nfig, ax = plt.subplots(1, 1, figsize = (12, 5))\n\nax.set_xlim(( 0, 12))\nax.set_ylim((0, 0.07))\nax.set_xticks(np.arange(0, 12))\nax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nax.set_title(\"Covid19 Severity Index Timeline (for certain states)\")\nax.set_ylabel(\"Covid19 Severity Index\")\nax.set_xlabel(\"2020\")\n\nlines = {}\nfor state in sel_states:\n    lines[state] = ax.plot([], [], lw=2, label = state)[0]\n\nax.legend(loc = 'upper right')\n\n# initialization function: plot the background of each frame\ndef init():\n    for state in sel_states:\n        lines[state].set_data([], [])\n    return set(lines.values())\n\n# animation function. This is called sequentially\ndef animate(i):\n    for state in sel_states:\n        y = datas[state][datas[state]['month'] <= i + 1]['covid19_severity_index'].values\n        x = np.arange(0, y.shape[0])\n        lines[state].set_data(x, y)\n    return set(lines.values())\n\n# call the animator. blit=True means only re-draw the parts that have changed.\nanim = animation.FuncAnimation(fig, animate, init_func = init,\n                               frames = 12, interval = 500, blit=True)\n\n# equivalent to rcParams['animation.html'] = 'html5'\nrc('animation', html='html5')\n\nplt.close()\n\nanim","990813ae":"def plot_lines(state, corr, pval, ax, ax_idx = 0):\n    x = covid_engagement_data[covid_engagement_data['state'] == state]['covid19_severity_index']\n    x = (x - np.mean(x)) \/ np.std(x)\n    y = covid_engagement_data[covid_engagement_data['state'] == state]['engagement_index']\n    y = (y - np.mean(y)) \/ np.std(y)\n    \n    x_vals = np.arange(12)\n    y_vals = [x, y]\n    labels = ['Covid19 Severity', 'Student Engagement']\n    title = f\"{state}\\nCorrelation={corr:.2f}, pval={pval:.2f}\"\n    xlabel = \"\"\n    ylabel = \"Normalized Values\"\n    theme = 'DARK'\n    x_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n    rotate_x = 90\n\n    theme = select_theme(theme)\n    if(len(y_vals) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n    else:\n        group_colors = theme['groups'][:len(y_vals)]\n    for y_val_idx, y_val in enumerate(y_vals):\n        if(len(labels) == 0):\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = f'line #{y_val_idx + 1}')\n        else:\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = labels[y_val_idx])\n#     ax = set_legend(ax, theme)\n    if(ax_idx == 0):\n        ax.legend(loc = 'upper left')\n    if(ax_idx in [4, 5]):\n        ax = set_xticklabels(ax, x_names, rotate_x = rotate_x, theme = theme)\n    else:\n        ax.set_xticks([])\n    if(ax_idx in [0, 2, 4]):\n        ax.set_ylabel(ylabel, fontsize = 15, color = theme['text'])\n    if(ax_idx in [1, 3, 5]):\n        ax.set_yticks([])\n    ax.set_title(title, fontsize = 15, color = theme['subtitle'])\n    ax = remove_spines(ax, theme)\n    ax = set_ticks(ax, theme)\n    ax.set_facecolor(theme['bg'])\n    return ax\n#     plot_lineplot(np.arange(12), [x, y], suptitle = f\"Covid19 Severity vs. Engagement for {state}\", title = f\"Correlation={corr:.2f}, pval={pval:.2f}\", height = 7, width = 15, labels = ['Covid19 Severity', 'Student Engagement'], x_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n\nfig, ax = plt.subplots(3, 2, figsize = (20, 15), sharey = True)\n    \ncorrs = {}\ncorrs_pval = {}\nplot_cnt = 0\nfor state in covid_engagement_data['state'].unique():\n    state_data = covid_engagement_data[covid_engagement_data['state'] == state][['covid19_severity_index', 'engagement_index']].dropna()\n    if(state_data.shape[0] >= 6):\n        corrs[state], corrs_pval[state] = pearsonr(state_data['covid19_severity_index'], state_data['engagement_index'])\n        if(corrs_pval[state] < 0.05):\n            if(state not in ['CA', 'IN', 'NC', 'TN', 'TX', 'VA']):\n                continue\n            ax[int(plot_cnt \/\/ 2)][int(plot_cnt % 2)] = plot_lines(state, corrs[state], corrs_pval[state], ax[int(plot_cnt \/\/ 2)][int(plot_cnt % 2)], ax_idx = plot_cnt)\n            plot_cnt += 1\n            if(plot_cnt == 6):\n                break\nfig.set_facecolor(theme['bg'])\nfig.suptitle(f\"Covid19 Severity vs. Engagement\", fontsize = 30, color = theme['text'], y = 0.95)\nplt.show()","6ed9abb2":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport sys\nimport time\n\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import pearsonr\n\nWHO_PANDEMIC_ANNOUNCEMENT_DATE = datetime(2020, 3, 11)\n\nrelevant_cols = [\n    'STATE',\n    'POSTCODE',\n    'STEMERG',\n    'CLSCHOOL',\n    'STAYHOME',\n    'STAYHOMENOGP',\n    'FM_ALL',\n    'FM_EMP',\n]\n\ndata = pd.read_excel('\/kaggle\/input\/learnplatform-analysis-data\/COVID-19 US state policy database 3_29_2021.xlsx', usecols = relevant_cols)\ndata = data.drop([0, 1, 2, 3], axis = 0)\ndata = data[pd.isnull(data['STATE']) == False]\ndata = data.replace({0: np.nan})\nfor col in data.columns[2:]:\n    data[col] = data[col].astype(np.datetime64)\n\ndef find_earliest_date(row, cols_to_compare = []):\n    return min(*[row[x] for x in cols_to_compare])\ndata['STAYHOME'] = data.apply(find_earliest_date, axis = 1, cols_to_compare = ['STAYHOME', 'STAYHOMENOGP'])\ndata = data.drop('STAYHOMENOGP', axis = 1)\ndata['FM_ALL'] = data.apply(find_earliest_date, axis = 1, cols_to_compare = ['FM_ALL', 'FM_EMP'])\ndata = data.drop('FM_EMP', axis = 1)\n\ndates_data = data.copy()\n\nfor col in data.columns[2:]:\n    data[col] = data[col].apply(lambda x: ((x - WHO_PANDEMIC_ANNOUNCEMENT_DATE).days) \/ 7)\n\nusecols = [\n    'CountryName',\n    'RegionName',\n    'Jurisdiction',\n    'C1_School closing',\n    'C2_Workplace closing',\n    'C3_Cancel public events',\n    'C4_Restrictions on gatherings',\n    'C5_Close public transport',\n    'C6_Stay at home requirements',\n    'C7_Restrictions on internal movement',\n    'C8_International travel controls',\n    'E1_Income support',\n    'E2_Debt\/contract relief',\n    'H1_Public information campaigns',\n    'H2_Testing policy',\n    'H3_Contact tracing',\n    'H6_Facial Coverings',\n    'H7_Vaccination policy',\n    'H8_Protection of elderly people',\n]\ndf = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/OxCGRT_latest.csv', usecols = usecols)\ndf = df[(df['CountryName'] == 'United States') & (df['Jurisdiction'] == 'STATE_TOTAL')]\ndf = df.drop(['CountryName', 'Jurisdiction'], axis = 1)\ndf = df[df['RegionName'] != 'Washington DC'].reset_index(drop = True)\ndf = df.groupby('RegionName')[[x for x in df.columns if x != 'RegionName']].aggregate(np.nanmean).reset_index(drop = False)\n\ndf['RegionName'] = df['RegionName'].replace(STATE_ABBR)\n\nfrom sklearn.decomposition import PCA\n\npolicy_metrics_names = []\npolicy_metrics = []\n\nC_cols = [x for x in df.columns if x.startswith('C')]\nE_cols = [x for x in df.columns if x.startswith('E')]\nH_cols = [x for x in df.columns if x.startswith('H')]\n\nfor feat_idx, feats in enumerate([C_cols, E_cols, H_cols]):\n\n    data_for_pca = df[feats]\n    for col in data_for_pca.columns:\n        data_for_pca[col] = (data_for_pca[col] - data_for_pca[col].mean()) \/ (data_for_pca[col].std() + 0.0000000000001)\n\n    pca = PCA(n_components = 1)\n    pca_orig_data = pca.fit_transform(data_for_pca)\n    for pca_idx in range(1):\n        pca_data = pca.components_[pca_idx]\n        expl_var = np.sum(pca.explained_variance_ratio_[:pca_idx + 1]) * 100\n        if(pca_data.sum() < 0):\n            pca_data = -pca_data\n        \n        policy_metrics.append(pca_orig_data[:, pca_idx])\n        if(feat_idx == 0):\n            policy_metrics_names.append(f'C_{pca_idx+1}')\n        elif(feat_idx == 1):\n            policy_metrics_names.append(f'E_{pca_idx+1}')\n        elif(feat_idx == 2):\n            policy_metrics_names.append(f'H_{pca_idx+1}')\n\n        if(pca_idx == 0):\n            if(feat_idx == 0):\n                cats = [0, 0, 0, 0, 1, 1, 1, 1]\n                col_name = 'Containment & Closure'\n            elif(feat_idx == 1):\n                cats = [0, 0]\n                col_name = \"Economic Response\"\n            elif(feat_idx == 2):\n                cats = [0, 0, 0, 1, 1, 1]\n                col_name = \"Health Systems\"\n            \n            plot_barplot(\n                [x for x in feats], pca_data,\n                width = 15, height = 7,\n                suptitle = f\"{col_name}\", title = f\"PCA Component {pca_idx + 1}, Explained Variance = {expl_var:.2f}%\",\n                xlabel = \"Features\", ylabel = \"Weight\",\n                rotate_x = 60,\n                theme = 'DARK',\n                cats = cats\n            )\n        \n        if(expl_var >= 70):\n            break","2da5ee05":"policy_data_metrics = pd.DataFrame(np.asarray(policy_metrics).transpose(), columns = policy_metrics_names)\n#print(data.shape, social_vulnerability_metrics.shape)\npolicy_data = pd.concat([df[['RegionName']], policy_data_metrics], axis = 1)\npolicy_data = policy_data.groupby('RegionName')[policy_metrics_names].aggregate(np.nanmean).reset_index(drop = False)\ndata = pd.merge(data, policy_data, how = 'inner', left_on = 'POSTCODE', right_on = 'RegionName')\n\nengagement_data = DATA.groupby('state')[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\npolicy_engagement_data = pd.merge(engagement_data, data, how = 'inner', left_on = 'state', right_on = 'POSTCODE')\n\nfig, ax = plt.subplots(1, 4, figsize = (25, 8))\n#for col in ['STEMERG', 'CLSCHOOL', 'STAYHOME', 'FM_ALL', 'C_1', 'E_1', 'H_1']:\nfor col_idx, col in enumerate(['STEMERG', 'CLSCHOOL', 'STAYHOME', 'FM_ALL']):\n    col_data = policy_engagement_data[['engagement_index', col]].dropna()\n    \n    sns.regplot(\n        x = col_data['engagement_index'],\n        y = col_data[col],\n        ax = ax[col_idx],\n        color = COLOR_GREEN,\n    )\n\n    theme = select_theme('DARK')\n    for a in ax:\n        fig, a = set_bg(fig, a, theme)\n        a = set_ticks(a, theme)\n        a = remove_spines(a, theme)\n    corr, corr_pval = pearsonr(col_data['engagement_index'], col_data[col])\n    ax[col_idx].set_title(f\"{col}\\nCorrelation: {corr:.2f}, pval={corr_pval:.2f}\", fontsize = 15, color = theme['subtitle'])\n    ax[col_idx].set_xlabel(col, fontsize = 10, color = theme['text'])\n    if(col_idx == 0):\n        ax[col_idx].set_ylabel(\"engagement_index\", fontsize = 10, color = theme['text'])\n    else:\n        ax[col_idx].set_ylabel(\"\")\nfig.suptitle(\"Government Response Features\", fontsize = 20, color = theme['text'])\nplt.show()\n\nfig, ax = plt.subplots(1, 3, figsize = (20, 8))\nfor col_idx, col in enumerate(['C_1', 'E_1', 'H_1']):\n    col_data = policy_engagement_data[['engagement_index', col]].dropna()\n    \n    sns.regplot(\n        x = col_data['engagement_index'],\n        y = col_data[col],\n        ax = ax[col_idx],\n        color = COLOR_GREEN,\n    )\n\n    theme = select_theme('DARK')\n    for a in ax:\n        fig, a = set_bg(fig, a, theme)\n        a = set_ticks(a, theme)\n        a = remove_spines(a, theme)\n    corr, corr_pval = pearsonr(col_data['engagement_index'], col_data[col])\n    ax[col_idx].set_title(f\"{col}\\nCorrelation: {corr:.2f}, pval={corr_pval:.2f}\", fontsize = 15, color = theme['subtitle'])\n    ax[col_idx].set_xlabel(col, fontsize = 10, color = theme['text'])\n    if(col_idx == 0):\n        ax[col_idx].set_ylabel(\"engagement_index\", fontsize = 10, color = theme['text'])\n    else:\n        ax[col_idx].set_ylabel(\"\")\nfig.suptitle(\"OxCGRT Indicators\", fontsize = 20, color = theme['text'])\nplt.show()","b49c8e6e":"from datetime import datetime, timedelta\nbefore_after_reopening_pct_access = []\nbefore_after_reopening_engagement_index = []\nbefore_after_reopening_rel_dates = []\nfor state in dates_data['POSTCODE'].unique():\n    school_closing_date = dates_data[dates_data['POSTCODE'] == state].iloc[0]['CLSCHOOL']\n    min_date = school_closing_date - timedelta(30)\n    max_date = school_closing_date + timedelta(30)\n    state_data = DATA[(DATA['time'] < max_date) & (DATA['time'] > min_date)]\n    state_data = state_data[state_data['is_weekend'] == 0]\n    if(state_data.shape[0] == 0):\n        continue\n    state_data = state_data[state_data['state'].replace(STATE_ABBR) == state]\n    state_data = state_data.groupby(['state', 'time'])[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\n    state_data['relative_time'] = state_data['time'].apply(lambda x: (x - school_closing_date).days)\n    if(state_data.shape[0] == 0):\n        continue\n    def normalize(x):\n        return (x - x.mean()) \/ (x.std() + 0.00000000000000000000001)\n    before_after_reopening_pct_access.append(normalize(state_data['pct_access']))\n    before_after_reopening_engagement_index.append(normalize(state_data['engagement_index']))\n    before_after_reopening_rel_dates.append(state_data['relative_time'])\n\ntot_days = np.median([len(x) for x in before_after_reopening_pct_access])\nhalf_days = tot_days \/\/ 2\n\navg = []\nfor rel_date_idx in range(-29, 30):\n    val_sum = 0\n    val_cnt = 0\n    for idx in range(len(before_after_reopening_pct_access)):\n        for date_idx in range(len(before_after_reopening_rel_dates[idx])):\n            if(before_after_reopening_rel_dates[idx][date_idx] == rel_date_idx):\n                val_sum += before_after_reopening_pct_access[idx][date_idx]\n                val_cnt += 1\n    if(val_cnt == 0):\n        print(\"ERROR!\")\n    else:\n        avg.append(val_sum \/ val_cnt)\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 10))\nfor idx in range(len(before_after_reopening_pct_access)):\n    sns.lineplot(x = before_after_reopening_rel_dates[idx], y = before_after_reopening_pct_access[idx].values, ax = ax, color = COLOR_GREEN, alpha = 0.5)\nsns.lineplot(x = np.arange(-29, 30), y = avg, ax = ax, color = COLOR_LIGHT_BLUE, alpha = 1, lw = 3, label = 'Average')\n\ntheme = select_theme('DARK')\nsuptitle = \"Trend of pct_access around School Closing\"\ntitle = \"Sharp decrease in engagement when schools closed\"\nxlabel = \"Relative Days from School Closing\"\nylabel = \"pct_access\"\nax.legend(loc = 'best')\nexec(plot_decoration())\nfrom PIL import Image\nimg = Image.open('\/kaggle\/input\/learnplatform-analysis-data\/drop_infographic.png')\n# img = img.resize((150, 200))\nheight = img.size[1]\nfig.figimage(img, 550, fig.bbox.ymax - 400)\nplt.show()","0c926a92":"avg = []\nfor rel_date_idx in range(-29, 30):\n    val_sum = 0\n    val_cnt = 0\n    for idx in range(len(before_after_reopening_engagement_index)):\n        for date_idx in range(len(before_after_reopening_rel_dates[idx])):\n            if(before_after_reopening_rel_dates[idx][date_idx] == rel_date_idx):\n                val_sum += before_after_reopening_engagement_index[idx][date_idx]\n                val_cnt += 1\n    if(val_cnt == 0):\n        print(\"ERROR!\")\n    else:\n        avg.append(val_sum \/ val_cnt)\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 10))\nfor idx in range(len(before_after_reopening_engagement_index)):\n    sns.lineplot(x = before_after_reopening_rel_dates[idx], y = before_after_reopening_engagement_index[idx].values, ax = ax, color = COLOR_GREEN, alpha = 0.5)\nsns.lineplot(x = np.arange(-29, 30), y = avg, ax = ax, color = COLOR_LIGHT_BLUE, alpha = 1, lw = 3, label = 'Average')\n    \ntheme = select_theme('DARK')\nsuptitle = \"Trend of engagement_index around School Closing\"\ntitle = \"\"\nxlabel = \"Relative Days from School Closing\"\nylabel = \"engagement_index\"\nax.legend(loc = 'best')\nexec(plot_decoration())\nplt.show()","cbb62336":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport sys\n\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\nfrom scipy.stats import pearsonr\n\ndates_avail = [\n    (8, 19),\n    (8, 21),\n    (8, 28),\n    (9, 3),\n    (9, 11),\n    (9, 18),\n    (9, 25),\n    (10, 2),\n    (10, 5),\n    (10, 9),\n    (10, 16),\n    (10, 23),\n    (10, 30),\n    (11, 6),\n    (11, 13),\n    (11, 20),\n    (11, 25),\n    (12, 4),\n    (12, 11),\n    (12, 18),\n    (12, 23)\n]\n\nall_data = pd.DataFrame()\nfor date in dates_avail:\n    df = pd.read_excel('\/kaggle\/input\/learnplatform-analysis-data\/School_Reopening_Plans (copy).xlsx', sheet_name = f'Map Data {date[0]}.{date[1]}.20', usecols = ['State_Abbr', 'Status'])\n    df['Date'] = pd.Series([datetime(2020, date[0], date[1])] * df.shape[0], index = df.index)\n    all_data = pd.concat([all_data, df], axis = 0).reset_index(drop = True)\n\nall_data['Updated_Status'] = all_data['Status'].replace({\n    '  No order': 'No order',\n    'No order in effect': 'No order',\n    'Partial closure in effect': 'Partial closure',\n    'Full closure in effect': 'Full closure',\n    'State ordered closure in effect (including states where openings are delayed)': 'Full closure',\n    'State-ordered regional closure in effect': 'Partial closure',\n    'Only hybrid or remote instruction allowed': 'Partial closure',\n    'State-ordered in-person instruction available part-time or full-time*': 'Partial open',\n    'Varies by school\/district\/dependent on local health authorities': 'No order'\n})\n\ndef define_reopening_score(status):\n    if(status == 'No order'):\n        return 0\n    elif(status == 'Full closure'):\n        return -1\n    elif(status == 'Partial closure'):\n        return -0.3\n    elif(status == 'Partial open'):\n        return 0.3\n    elif(status == 'Ordered open'):\n        return 1\n    else:\n        print(\"Found unknown value\")\n\nall_data['Reopening_Score'] = all_data['Updated_Status'].apply(define_reopening_score)\nall_data = all_data.groupby('State_Abbr')['Reopening_Score'].aggregate(np.nanmean).reset_index(drop = False)\n\nredblue_states = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/Political_RedBlueState.csv')\nreopening_political_data = pd.merge(all_data, redblue_states, how = 'inner', on = 'State_Abbr')\n\nengagement_data = DATA.groupby('state')[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\nreopening_engagement_data = pd.merge(engagement_data, all_data, how = 'inner', left_on = 'state', right_on = 'State_Abbr')\ncorr, corr_pval = pearsonr(reopening_engagement_data['engagement_index'], reopening_engagement_data['Reopening_Score'])\n\nplot_us_map(reopening_political_data, f\"School Reopening Scores Map: Correlation={corr:.2f}, pval={corr_pval:.2f}\", 'Reopening_Score', 'Score', -1, 1, theme = 'DARK', state_col = 'State_Abbr')","8c1b0eca":"from IPython.display import HTML, display\nimport tabulate\n\nredblue_states = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/Political_RedBlueState.csv')\npolicy_political_data = pd.merge(policy_engagement_data, redblue_states, how = 'inner', left_on = 'state', right_on = 'State_Abbr')\n\n# plot_us_map(reopening_political_data, \"Political Alignment\", 'WonByBiden', 'Won By Biden', 0, 1, theme = 'DARK', state_col = 'State_Abbr')\n\nstate_vals_df = reopening_political_data\ntitle = \"Political Landscape - Red & Blue States\"\nval_col = 'WonByBiden'\nval_label = 'Won By Biden'\nrange_min_val = 0\nrange_max_val = 1\ntheme = 'LIGHT'\nstate_col = 'State_Abbr'\n    \ntheme = select_theme(theme)\n\nlayout = dict(\n    font_family = 'Source Sans Pro',\n    font_color = theme['text'],\n    title_text = title,\n    # To change\n    title_font = dict(\n        family = \"Source Sans Pro\",\n        size = 25,\n        color = theme['axis']\n    ),\n    geo_scope = 'usa',\n    paper_bgcolor = theme['bg'],\n    geo_bgcolor = theme['bg'],\n    geo = dict(\n        landcolor = theme['inv'],\n        subunitcolor = theme['gray'],\n        lakecolor = theme['bg'],\n    ),\n)\n\nfig = px.choropleth(\n    state_vals_df,\n    locations = state_col,\n    color = val_col,\n    color_continuous_scale = ['red', 'blue'],\n    range_color = (range_min_val, range_max_val),\n    locationmode = \"USA-states\",\n    labels = {val_col : val_label, state_col: 'State'},\n)\n\nfig.update_layout(layout)\nfig.update(layout_coloraxis_showscale=False)\nfig.update_layout(margin = {\"r\": 0, \"l\": 0, \"b\": 15})\nfig.show()","83ff4eba":"political_corr = [\n    ['FEATURE', 'CORRELATION WITH POLITICS', 'CORRELATION P-VALUE']\n]\nfor col in ['STEMERG', 'CLSCHOOL', 'STAYHOME', 'FM_ALL', 'C_1', 'E_1', 'H_1']:\n    col_data = policy_political_data[['WonByBiden', col]].dropna()\n    row = []\n    row.append(f\"Government Policy - {col}\")\n    corr, corr_pval = pearsonr(col_data['WonByBiden'], col_data[col])\n    row.append(f\"{corr:.2f}\")\n    row.append(f\"{corr_pval:.2f}\")\n    political_corr.append(row)\n\npolitical_corr.append(['', '', ''])\n\nengagement_data = DATA.groupby('state')[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\npolitical_engagement_data = pd.merge(engagement_data, redblue_states, how = 'inner', left_on = 'state', right_on = 'State_Abbr')\n\nrow = []\nrow.append(\"Reopening Score\")\ncorr, corr_pval = pearsonr(reopening_political_data['Reopening_Score'], reopening_political_data['WonByBiden'])\nrow.append(f\"{corr:.2f}\")\nrow.append(f\"{corr_pval:.2f}\")\npolitical_corr.append(row)\n\npolitical_corr.append(['', '', ''])\n\nrow = []\nrow.append(\"pct_access\")\ncorr, corr_pval = pearsonr(political_engagement_data['pct_access'], political_engagement_data['WonByBiden'])\nrow.append(f\"{corr:.2f}\")\nrow.append(f\"{corr_pval:.2f}\")\npolitical_corr.append(row)\n\nrow = []\nrow.append(\"engagement_index\")\ncorr, corr_pval = pearsonr(political_engagement_data['engagement_index'], political_engagement_data['WonByBiden'])\nrow.append(f\"{corr:.2f}\")\nrow.append(f\"{corr_pval:.2f}\")\npolitical_corr.append(row)\n\ndisplay(HTML(tabulate.tabulate(political_corr, tablefmt='html')))","5b26ef45":"<div>\n<font size='+1'>\n    When comparing the correlations of these government policy features with student engagement, it was found that only Health Systems seemed to have a statistically significant correlation of around 0.5. Other government responses seemed to  not have much effect.\n<\/font>\n<\/div>","2b5900cb":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>1 PM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    The lunch break - an oasis in the desert of online classes. Just like a lost survivor in an oasis, Grace was now refreshed and determined to survive. The <b>Physical Education & Health<\/b> class stood front of her, but she was prepared.\n    <br><br>\n    \"Actually, I like PE,\" she said. \"It's fun since many of my classmates' parents also attend this class.\" \"Really,\" I asked with a tinge of surprise. \"What do they do?\"\n    <br><br>\n    \"They do whatever we do - meditation, yoga, games, exercise,\" she replied. \"They say it helps them de-stress while working at home. It gives them the chance to take a break to take care of their health.\"\n<\/font>\n<\/span>\n<\/div>","a91b7952":"<div>\n<font size='+1'>\n    Looking at the timeline, there are certain patterns that can be seen. To discover these patterns, <font color='#FD7121'>Spatial Autocorrelation<\/font> can be used. By comparing a state's value with its neighbouring state's values, the following Moran's Scatterplot can be found -\n<\/font>\n<\/div>","a08c4a21":"![LearnPlatform_Experimental(7).png](attachment:bc9685c4-3131-4421-b578-0be70aced7c1.png)","61035e9b":"<div>\n<font size='+1'>\n    Now, let us check for any relationship between the Covid19 Severity Index & Student Engagement. The timeline of Covid19 Severity Index was compared with the timeline of Student Engagement for all states. The below states are ones where the correlation was extremely high -\n<\/font>\n<\/div>","f104695c":"<div>\n<font size='+1'>\n    It is interesting to see an upward spike for severaly affected states near September while a downward spike for least affected states around the same time. In fact, only 3 other states showed this upward spike and almost all of them had high Covid19 Severity Index values before September itself. Maybe the severely-hit states were not able to control the cases and the situation got out of hand, while most other states were able to keep their cases under control.\n<\/font>\n<\/div>","609013ea":"<div>\n<font size='+1'>\n    Firstly, let us try and quantify how the government had responded to the emergency. For this, we would be using two different kinds of features. Firstly we compare the dates when different declarations were made (like school closing, state emergency) with March 11, 2020 as the reference date (when WHO declared the pandemic). States which made these important declarations early would be scored high.\n    <br><br>\n    Secondly, we use OxCGRT indicators. Since there are many indicators, we can repeat the process of using the first PCA component to simplify our analysis. Based on the components, we can get an idea about what each PCA component actually represents -\n    <ul>\n        <li><b>Containment & Closure<\/b> - High value indicates good government response with regards to social distancing<\/li>\n        <li><b>Economic Response<\/b> - High value indicates more income support & debt relief<\/li>\n        <li><b>Health Systems<\/b> - High value indicates good public campaigning<\/li>\n    <\/ul>\n<\/font>\n<\/div>","a3f10cc1":"<div>\n    <font size='+2' color='#0DAB6C'>\n        <b>Detailed Analysis<\/b>\n    <\/font>\n<\/div>","3d753302":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>3 PM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    It had finally come - the last hour of online classes! The last hour was generally reserved for two optional subjects which the students could choose for half an hour each. Grace had firstly opted for <b>Arts<\/b>.\n    <br><br>\n    She absolutely loves it and why shouldn't she? She gets to learn skills like acting & photography (which is where her future ambitions have stemmed from) along with other exciting hobbies like music, dance and ceramic.\n    <br><br>\n    But obviously, all of this becomes fun when you actually do it along with your friends. Not when you watch someone else do it through a laptop screen. The online Arts class was incredibly underwhelming for Grace. \"Oh, how I miss the Arts classes back in school!\" she would cry. \"I don't want to paint or act all alone, I want my friends here. Oh, when will we be able to go back to school?\"\n<\/font>\n<\/span>\n<\/div>","ba4ddf88":"<div>\n    <font size='+2' color='#0DAB6C'>\n        <b>Detailed Analysis<\/b>\n    <\/font>\n<\/div>","673d3b70":"<div>\n<font size='+1'>\n    When comparing the political factors with different government policies, it was found that the stay-at-home, Economic Response & Health Systems policies of the state government were influenced by whether the state was Red or Blue. Further even the reopening of schools seems to be a politically influenced decision.\n    <br><br>\n    However, political factors do not seem to be affecting Student Engagement.<\/font>\n<\/div>","a2f33605":"<div>\n<font size='+1'>\n    Finally, we can identify these states on a map -\n<\/font>\n<\/div>","a7ddd77a":"<div>\n    <font size='+2' color='#0DAB6C'>\n        <b>Detailed Analysis<\/b>\n    <\/font>\n<\/div>","d18f749b":"<div>\n<font size='+1'>\n    Another idea that I worked on was the impact of closing schools on student engagement. To do this, I checked the engagement levels for each states 1 month before and after schools were closed in that state. The results showed that contrary to my intuition, a sharp decrease was observed in pct_access around the time schools were closed. For engagement_index, the decrease is not obvious.\n<\/font>\n<\/div>","9f00ab55":"<div>\n<font size='+1'>\n    Interestingly, a very high negative correlation was observed (implying engagement increased as covid19 severity decreased). Also a very clear trend can be seen above - as earlier noticed, student engagement took a drop around July due to vacations and later increased in September, while Covid19 Severity increased around June and like most other states (as shown in the previous graph) took a drop near September. This unique trend might be causing extremely high correlations for these states.\n<\/font>\n<\/div>","7aea8184":"<div>\n<font size='+1'>\n    Indeed, this pandemic has made all of us realize the importance of health. Not only in terms of staying safe, but also in terms of taking care of the body and mind. Working from home can be stressful, unless you have a Grace in your house.\n    <br><br>\n    And it makes me reflect the past one and a half year. How millions of lives were impacted by this virus. People losing loved ones, losing jobs, losing money, losing life. It will surely be one of the toughest times of my lifetime.\n    <br><br>\n    And for Grace? She might not realize this, but she is losing the blossoming years of her childhood. Instead of growing up among her friends under the sun, she is growing up alone under our roof.\n    <br><br>\n    Covid19 has impacted several lives, and it is important to understand just how much. So I decided to analyze the impact Covid19 has had on the digital engagement of children. This is what I observed -\n<\/font>\n<\/div>","acdd1a3c":"<div align='center'>\n    <font size='+2' color='#75D345'>\n        <a style=\"background-color:#63D1DF; padding: 15px;\" href='https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-4-finally-done' target=\"_blank\">Next: <b>Finally Done!<\/b><\/a>\n    <\/font>\n<\/div>","9f7edb5d":"<div>\n<font size='+1'>\n    Let us now deal with states & time together. The following is an animation showing how covid19 severity changed with time for the 3 most and least affected states.\n<\/font>\n<\/div>","6ac3444b":"<div>\n<font size='+1'>\n    As can be seen, regions around New York have high severity while regions around Washington have lower severity. Vermont, despite being in a hotspot, still has managed to keep its severity index low.\n<\/font>\n<\/div>","c98efcdf":"<div align='center'>\n    <font size='+3' color='#75D345'>\n        <b>A Day of Digital Learning<\/b>\n    <\/font>\n    <br>\n    <font size='+2.5' color='#FBAB60'>\n        <b>Part 3: Sleepy Noons<\/b>\n    <\/font>\n    <br>\n    <br>\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-1-wake-up\">\n        <font size='+1'>\n            &#9202; Wake Up!\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-2-morning-mania\">\n        <font size='+1'>\n            &#127748; Morning Mania\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-3-sleepy-noons\">\n        <font size='+1'>\n            <b>&#127774; Sleepy Noons<\/b>\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-4-finally-done\">\n        <font size='+1'>\n            &#127769; Finally Done!\n        <\/font>\n    <\/a>\n<\/div>","08606a8f":"![LearnPlatform_Experimental(6).png](attachment:8f46580b-f422-44eb-a45c-c72e1cff2412.png)","e43af8e7":"<div>\n<font size='+1'>\n    Finally, government policy and response is not straightforward. It is fair to say that a lot of dirty politics would have been involved in each decision. This is because the covid19 pandemic was seen by many politicians as a basis to argue and debate and forward their own propoganda. Hence, the effect of the Political Landscape must also be checked. Here the red & blue state status will be used as a political feature.\n<\/font>\n<\/div>","a3d8b2cc":"<div>\n<font size='+1'>\n    From this, we understand that there is some correlation of the state's values with its neightbouring states' values. To further analyze, we can identify four kind of states -\n    <ol>\n        <li><b>HH<\/b> - State has high severity index & also neighbouring states have high severity index<\/li>\n        <li><b>HL<\/b> - State has high severity index but neighbouring states have low severity index<\/li>\n        <li><b>LH<\/b> - State has low severity index but neighbouring states have high severity index<\/li>\n        <li><b>LL<\/b> - State has low severity index & also neighbouring states have low severity index<\/li>\n    <\/ol>\n<\/font>\n<\/div>","118e754e":"<div>\n<font size='+1'>\n    How sleepy do you have to be to confuse geography to history? They are completely different subjects!\n    <br><br>\n    Geography mostly deals with spatial analysis while history deals with temporal analysis. There is no way you should confuse that - it's like confusing between time and area! They are separate quantities and have separate units of measurement. You don't involve space when talking about time and don't involve time when talking about space...\n    <br><br>\n    Unless you perform what is called a <b>spatio-temporal analysis<\/b>. Here is what I found out -\n<\/font>\n<\/div>","61c83de9":"![LearnPlatform_Experimental(19).png](attachment:12681e82-39d1-48c2-9aec-fe03cd000d95.png)","b33cc219":"<div>\n<font size='+1'>\n    That is a question which millions of people debate over. That is a question that the government has to face a lot of heat for. That is a question on the lips of every student, teacher & parent.\n    <br><br>\n    The pandemic has complicated the entire situation and it seems the answer to the above question is not as straightforward as one would expect. Government policies need to consider several factors relating health, finance & administration before taking a decision on reopening schools.\n    <br><br>\n    This made me wonder whether government policies would have any impact on student engagement? Whether the government's past measures and future plans have any effect on the present student engagement? I decided to check this out and here is what I found -\n<\/font>\n<\/div>","381c4cbe":"<div>\n<font size='+1'>\n    Since we are analysing the impact of school closing on engagement levels, why not try and see whether plans to reopen schools has any impact on student engagement. After receiving permission, I have used a really nice dataset from EdWeek which tracks the school reopening plan for different states.\n    <br><br>\n    Using this dataset, I came up with a <b>Reopening Score<\/b> which represents whether states are eager to reopen schools. Higher the Reopening Score, more eager are the states to reopen schools.\n    <br><br>\n    After plotting the scores on a map and finding its correlation with student engagement, it was however observed that there is no statistically significant correlation between the two.\n<\/font>\n<\/div>","1f03a0b0":"<div>\n<font size='+1'>\n    Firstly, let us look at the impact of Covid19 on the USA. One way to represent this impact is what I call <b>Covid19 Severity Index<\/b> which tries to measure the severity of Covid19 in a state.\n    <br><br>\n    It is given by the formula -\n    <br>\n    <b>Covid19 Severity Index = ((2 x #Deaths \/ #Cases) + (1 x #Cases \/ Population)) \/ 3<\/b>\n    <br>\n    Basically a weighted sum of Death-to-cases ratio & Cases-to-Population ratio. Here the first term is being weighed twice as much as the second term, since cases converting into deaths is much more severe than population converting into cases.\n    <br><br>\n    Using this newly generated metric, let us analyze how sever Covid has been among different states with time -\n<\/font>\n<\/div>","9e9dd5de":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>2 PM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    The twin effect of having lunch and doing relaxing meditation and exercises now had Grace as sleepy as a hibernating bear. Every once a while, I made sure to tap the pen loudly on my table - her sleeping head would suddenly jerk, her eyes would open and the sad reality in front of her laptop screen would chase away her dreams.\n    <br><br>\n    \"Which class is going on?\" I asked, just to make her move some part of her face. \"<b>Social Studies<\/b>,\" came a grim low-tone reply. \"What are they teaching right now?\" I asked, just to make her look at the screen once.\n    <br><br>\n    \"I think history, yeah maybe history,\" she said, trying to let some of her screen's light into her eye. I nodded and turned around to continue my work. \"So can anyone tell me where on this map is Nevada?\" came a sound from Grace's laptop. I smiled.\n<\/font>\n<\/span>\n<\/div>"}}