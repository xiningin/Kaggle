{"cell_type":{"a1ba9b70":"code","1e9603fb":"code","ea9eebf2":"code","1545ae14":"code","6395d56c":"code","e8174a47":"code","8c44acbd":"code","57a1d241":"code","d372d2e2":"code","35be1da2":"code","8c62a44f":"code","b7bc00b2":"code","1706f154":"code","25a206f9":"code","c39c84e9":"code","74a6d7bc":"code","ddc5f930":"code","80100285":"code","a775f87e":"code","240adbc9":"code","3fdc8138":"code","011b5bfc":"code","3128d766":"code","b4a24992":"code","239be408":"code","ce524a04":"code","def13eab":"markdown","431297bc":"markdown","b512e80a":"markdown","4fea2b60":"markdown","9bc8b5ed":"markdown","f7404c29":"markdown","55b26513":"markdown","22f827df":"markdown","18dec01e":"markdown","cf86f607":"markdown","0a06f7ca":"markdown","0bcb4adb":"markdown","db45f6ad":"markdown","6d0cc147":"markdown","090aa097":"markdown","c60efd87":"markdown","f12db3c8":"markdown","c4cb293f":"markdown","70794234":"markdown"},"source":{"a1ba9b70":"# Set your own project id here\nPROJECT_ID = 'bigquery-bikes' # a string, like 'kaggle-bigquery-240818'\n\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=PROJECT_ID, location=\"\")\ndataset = client.create_dataset('model_dataset', exists_ok=True)\n\nfrom google.cloud.bigquery import magics\nfrom kaggle.gcp import KaggleKernelCredentials\nmagics.context.credentials = KaggleKernelCredentials()\nmagics.context.project = PROJECT_ID","1e9603fb":"%load_ext google.cloud.bigquery","ea9eebf2":"# We would like to split the dataset into a training dataset and a test dataset.\n# Ideally, the ratio between training and test data should be in the order of 80:20, or 2\/3 : 1\/3.\n# I would sort data by ascending datetime and use the older values in the training dataset and the most recent values in the test set.\n# Depending on the data available, I'd try to gather at least one full year of data in the training dataset (to try to capture seasonality).","1545ae14":"# create a reference to our table\ntable = client.get_table(\"bigquery-public-data.austin_bikeshare.bikeshare_trips\")\n\n# look at five rows from our dataset\nclient.list_rows(table, max_results=5).to_dataframe()","6395d56c":"%%bigquery training_dataset\nSELECT\n  IFNULL(start_station_name, \"\") as start_station_name,\n  TIMESTAMP_TRUNC(start_time, HOUR) as start_hour,\n  COUNT(1) as num_rides\nFROM\n  `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nWHERE\n  start_time < '2018-01-01'\nGROUP BY start_station_name, start_hour\nORDER BY start_station_name, start_hour   ","e8174a47":"training_dataset.head()","8c44acbd":"training_dataset.info()","57a1d241":"training_dataset['start_station_name'].nunique()","d372d2e2":"training_dataset['start_station_name'].value_counts()","35be1da2":"# There are names of stations that look like types of use more than real station names: eg. \"Repair Shop\", \"Re-branding\", \"Customer Service\",\n#\"Marketing Event\", \"Stolen\".\n# also, there are mobile stations at different locations. Maybe we could group these data altogether and discard them from the model? ","8c62a44f":"training_dataset['start_hour'].unique()","b7bc00b2":"%%bigquery\nCREATE OR REPLACE MODEL`model_dataset.bike_trips`\nOPTIONS(model_type='linear_reg') AS \nSELECT\n  COUNT(1) as label,\n  IFNULL(start_station_name, \"\") as start_station_name,\n  TIMESTAMP_TRUNC(start_time, HOUR) as start_hour\nFROM\n  `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nWHERE\n  start_time < '2018-01-01'\nGROUP BY start_station_name, start_hour \nORDER BY start_station_name, start_hour ","1706f154":"%%bigquery\nSELECT\n  *\nFROM ML.EVALUATE(MODEL `model_dataset.bike_trips`, (\n  SELECT  \n    COUNT(1) as label,\n    IFNULL(start_station_name, \"\") as start_station_name,\n    TIMESTAMP_TRUNC(start_time, HOUR) as start_hour\n  FROM\n    `bigquery-public-data.austin_bikeshare.bikeshare_trips`\n  WHERE\n      start_time > '2018-01-01'\nGROUP BY start_station_name, start_hour\nORDER BY start_station_name, start_hour ))","25a206f9":"## The poor performance might come from:\n\n## * Issues in the training and test datasets, such as too little data, missing data, or erroneous data. We may have to discard stations with too\n## little data or outliers.\n\n##* A change in patterns of usage between the training period and the test period, that could result from changes in the context (eg: new tariff scheme,\n## introduction of biking incentives, opening of new stations...). For example, a steep increase in usage, as more and more people adopt the bike sharing \n## system and new stations are introduced.\n\n## * A change in patterns of usage between the training period and the test period, since we're dealing here with a growing network (of bike sharing):\n## Starting with a dozen of stations in 2013, the network has more than 80 stations in 2018.\n## Therefore, as more and more stations open, the bike sharing network might get denser, and people adopt new stations that are closer. We might observe\n## a decline in daily average in older stations that are close to newer stations.\n\n## * Linear model might not be appropriate.\n\n## * Events occurring at different dates each year: Are they any events that might significantly impact shared bike usage? If so, such events\n## might not be identified in our variables, since dates differ each year.\n\n## * The number of variables might be too low, and we might need more features in our model to explain patterns.","c39c84e9":"%%bigquery\n\nSELECT AVG(ROUND(predicted_label)) as predicted_avg_riders, \n       AVG(label) as true_avg_riders\nFROM\nML.PREDICT(MODEL `model_dataset.bike_trips`, (\nSELECT COUNT(1) as label,\n       start_station_name,\n       TIMESTAMP_TRUNC(start_time, HOUR) as start_hour\nFROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nWHERE start_time >= \"2018-01-01\" AND start_time < \"2019-01-01\"\n  AND start_station_name = \"22nd & Pearl\"\nGROUP BY start_station_name, start_hour\n))","74a6d7bc":"%%bigquery avg_daily_rides_per_date\nSELECT\n  TIMESTAMP_TRUNC(start_time, DAY) as start_date,\n  COUNT(1) \/ COUNT(DISTINCT(IFNULL(start_station_name, \"\"))) as avg_daily_rides\nFROM\n  `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nGROUP BY start_date\nORDER BY start_date","ddc5f930":"avg_daily_rides_per_date.set_index('start_date', inplace = True)","80100285":"import matplotlib.pyplot as plt\nfig, axes = plt.subplots(figsize=(12,3))\naxes.plot(avg_daily_rides_per_date.index, avg_daily_rides_per_date['avg_daily_rides'])\naxes.set_ylabel('Average daily rides per station')\naxes.set_title('Average daily rides per station over the study period');\n","a775f87e":"## The pattern in 2018 looks very different from the pattern during the other years. There is a sustained high level of daily rides during a few months from\n## mid-February to mid-May (and to a lesser extend in September-October) that was not observed during previous years.","240adbc9":"## Please see my post in the Kaggle Learn Forums for ideas of improvements: https:\/\/www.kaggle.com\/learn-forum\/103648.\n## Also below is the code to plot the daily riders for each year of the study period for the following datasets:\n## 1\/ dataset:  stations existing prior to 2018, that are not temporary (i.e. not 'Mobile Station...', 'MapJam...'), and that have more than 100 riders\n## per year\n## 2\/ dataset:  stations new in 2018, that are not temporary (i.e. not 'Mobile Station...', 'MapJam...'), and that have more than 100 riders per year","3fdc8138":"# Extract the first dataset (stations existing prior to 2018...)\n%%bigquery overall_daily_riders_re_sampled\nSELECT\n  EXTRACT(YEAR from start_time) as year,\n  EXTRACT(MONTH from start_time) as month,\n  EXTRACT(DAY from start_time) as day,\n  EXTRACT(DAYOFYEAR FROM start_time) as day_of_year,\n  COUNT(1) as num_rides\nFROM\n  `bigquery-public-data.austin_bikeshare.bikeshare_trips` \nWHERE (start_station_name NOT IN ('10th & Red River',\n                                 '11th & Salina',\n                                 '11th & Salina ',\n                                 '21st & Speedway @PCL',\n                                 '21st & University',\n                                 '22nd & Pearl',\n                                 '23rd & Rio Grande',\n                                 '23rd & San Jacinto @ DKR Stadium',\n                                 '5th & Campbell', '6th & Chalmers',\n                                 '6th & Chalmers ', '8th & Lavaca',\n                                 'Dean Keeton & Speedway',\n                                 'Dean Keeton & Speedway ',\n                                 'Dean Keeton & Whitis',\n                                 \"Eeyore's 2017\", \"Eeyore's 2018\",\n                                 'Hollow Creek & Barton Hills',\n                                 'Lake Austin & Enfield', 'Lake Austin Blvd @ Deep Eddy',\n                                 'Lakeshore & Pleasant Valley', 'Lakeshore @ Austin Hostel',\n                                 'Nash Hernandez @ RBJ South', 'Nueces & 26th', \n                                 'Red River\/Cesar Chavez @ The Fairmont',\n                                 'Repair Shop', 'Rio Grande & 28th',\n                                 'Rosewood & Angelina', 'Rosewood & Chicon',\n                                 'South Congress @ Bouldin Creek'))\n  AND (start_station_name NOT IN ('6th & Chalmers ', '6th & Congress', 'Customer Service',\n       'East 7th & Pleasant Valley', \"Eeyore's 2017\", \"Eeyore's 2018\",\n       'Main Office', 'MapJam at French Legation',\n       'MapJam at Pan Am Park', 'MapJam at Scoot Inn', 'Marketing Event',\n       'Re-branding', 'Repair Shop', 'Shop', 'Stolen'))\n  AND (start_station_name NOT IN ('MapJam at Hops & Grain Brewery', 'Mobile Station',\n'Mobile Station @ Bike Fest',\n       'Mobile Station @ Boardwalk Opening Ceremony',\n       'Mobile Station @ Unplugged'))\nGROUP BY year, month, day, day_of_year\nORDER BY year, day_of_year ","011b5bfc":"# Store results in dataframes and add dates (there should be a simpler way to code it...)\nfrom datetime import datetime\nimport numpy as np\n\ndaily_riders = dict()\nfor y in range(2013,2020):\n    # extract 1 year\n    daily_riders[y] = overall_daily_riders_re_sampled[overall_daily_riders_re_sampled['year']==y]\n    # retrieve dates\n    daily_riders[y]['date'] = '1901-01-01'\n    for i in range(len(daily_riders[y])):\n        daily_riders[y]['date'].iloc[i] =datetime(daily_riders[y]['year'].iloc[i], daily_riders[y]['month'].iloc[i], daily_riders[y]['day'].iloc[i])","3128d766":"# plot results for 2018\ny = 2018\nimport matplotlib.pyplot as plt\nplt.plot(daily_riders[y]['date'],daily_riders[y]['num_rides'])","b4a24992":"# plot for all the year using plotly\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nfrom plotly import subplots\n\npath = \"\" # insert path here\nexport_filename =  path+\"daily_riders_over_the_years.html\"\noverall_title = 'Daily riders from selected start stations (existing prior to 2018, not temporary, with more than 100 riders per year)'\n\nfig = subplots.make_subplots(rows=7, cols=1)\n\nfor i in range(7):\n    y = 2013 + i\n    trace = go.Bar(\n        x=daily_riders[y]['date'],\n        y=daily_riders[y]['num_rides'],\n        #marker = dict(color =daily_riders[y]['colour']),\n        name = y\n    )\n    \n    fig.append_trace(trace, i+1, 1)\n\ny_max = max([daily_riders[y]['num_rides'].max() for y in range(2013,2020)])\nfig['layout'].update(title=overall_title)\nfig['layout']['yaxis1'].update(range=[0, y_max])\n\nfor i in range(7):\n    y = 2013 + i\n    fig['layout']['xaxis'+str(i+1)].update(range=[str(y)+'-01-01', str(y)+'-12-31'])\n    fig['layout']['yaxis'+str(i+1)].update(range=[0, y_max], title = str(y))\n\npyo.plot(fig)\n#pyo.plot(fig,filename = export_filename)","239be408":"# Extract the second dataset (stations new in 2018...)\n# same but \"IN\" instead of \"NOT In\" in the first WHERE condition\n\"\"\"\nWHERE (start_station_name IN ('10th & Red River',\n                                 '11th & Salina',\n                                 '11th & Salina ',\n                                 '21st & Speedway @PCL',\n                                 '21st & University',\n                                 '22nd & Pearl',\n                                 '23rd & Rio Grande',\n                                 '23rd & San Jacinto @ DKR Stadium',\n                                 '5th & Campbell', '6th & Chalmers',\n                                 '6th & Chalmers ', '8th & Lavaca',\n                                 'Dean Keeton & Speedway',\n                                 'Dean Keeton & Speedway ',\n                                 'Dean Keeton & Whitis',\n                                 \"Eeyore's 2017\", \"Eeyore's 2018\",\n                                 'Hollow Creek & Barton Hills',\n                                 'Lake Austin & Enfield', 'Lake Austin Blvd @ Deep Eddy',\n                                 'Lakeshore & Pleasant Valley', 'Lakeshore @ Austin Hostel',\n                                 'Nash Hernandez @ RBJ South', 'Nueces & 26th', \n                                 'Red River\/Cesar Chavez @ The Fairmont',\n                                 'Repair Shop', 'Rio Grande & 28th',\n                                 'Rosewood & Angelina', 'Rosewood & Chicon',\n                                 'South Congress @ Bouldin Creek'))\n\"\"\"","ce524a04":"# to plot, follow the same steps as above. Just change the formula for y_max:\n# y_max = max([daily_riders[y]['num_rides'].max() for y in range(2018,2020)])","def13eab":"# Stocking rental bikes\n\n![bike rentals](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a0\/Bay_Area_Bike_Share_launch_in_San_Jose_CA.jpg\/640px-Bay_Area_Bike_Share_launch_in_San_Jose_CA.jpg)\n\nYou stock bikes for a bike rental company in Austin, ensuring stations have enough bikes for all their riders. You decide to build a model to predict how many riders will start from each station during each hour, capturing patterns in seasonality, time of day, day of the week, etc.\n\nTo get started, create a project in GCP and connect to it by running the code cell below. Make sure you have connected the kernel to your GCP account in Settings.","431297bc":"Write your query below:","b512e80a":"What you should see here is that the model is underestimating the number of rides by quite a bit. \n\n## 7) Exercise: Average daily rides per station\n\nEither something is wrong with the model or something surprising is happening in the 2018 data. \n\nWhat could be happening in the data? Write a query to get the average number of riders per station for each year in the dataset and order by the year so you can see the trend. You can use the `EXTRACT` method to get the day and year from the start time timestamp. (You can read up on EXTRACT [in this lesson in the Intro to SQL course](https:\/\/www.kaggle.com\/dansbecker\/order-by)). ","4fea2b60":"# 9) Next steps\n\nGiven what you've learned, what improvements do you think you could make to your model? Share your ideas on the [Kaggle Learn Forums](https:\/\/www.kaggle.com\/learn-forum)! (I'll pick a couple of my favorite ideas & send the folks who shared them a Kaggle t-shirt. :)","9bc8b5ed":"Write your query below:","f7404c29":"Write your query below:","55b26513":"## Model creation\n\nNow it's time to turn this data into a model. You'll use the `CREATE MODEL` statement that has a structure like: \n\n```sql\nCREATE OR REPLACE MODEL`model_dataset.bike_trips`\nOPTIONS(model_type='linear_reg') AS \n-- training data query goes here\nSELECT ...\n    column_with_labels AS label\n    column_with_data_1 \n    column_with_data_2\nFROM ... \nWHERE ... (Optional)\nGROUP BY ... (Optional)\n```\n\nThe `model_type` and `optimize_strategy` shown here are good parameters to use in general for predicting numeric outcomes with BQML.\n\n**Tip:** Using ```CREATE OR REPLACE MODEL``` rather than just ```CREATE MODEL``` ensures you don't get an error if you want to run this command again without first deleting the model you've created.","22f827df":"## 2) Exercise: Query the training data\n\nWrite the query to retrieve your training data. The fields should be:\n1. The start_station_name\n2. A time trips start, to the nearest hour. Get this with `TIMESTAMP_TRUNC(start_time, HOUR) as start_hour`\n3. The number of rides starting at the station during the hour. Call this `num_rides`.\nSelect only the data before 2018-01-01 (so we can save data from 2018 as testing data.)","18dec01e":"## 8) What do your results tell you?\n\nGiven the daily average riders per station over the years, does it make sense that the model is failing?","cf86f607":"## 3) Exercise: Create and train the model\n\nBelow, write your query to create and train a linear regression model on the training data.","0a06f7ca":"## Training data\n\nFirst, you'll write a query to get the data for model-building. You can use the public Austin bike share dataset from the `bigquery-public-data.austin_bikeshare.bikeshare_trips` table. You predict the number of rides based on the station where the trip starts and the hour when the trip started. Use the `TIMESTAMP_TRUNC` function to truncate the start time to the hour.","0bcb4adb":"## 4) Exercise: Model evaluation\n\nNow that you have a model, evaluate it's performance on data from 2018. \n\n\n> Note that the ML.EVALUATE function will return different metrics depending on what's appropriate for your specific model. You can just use the regular ML.EVALUATE funciton here. (ROC curves are generally used to evaluate binary problems, not linear regression, so there's no reason to plot one here.)","db45f6ad":"### This exercise is designed to pair with [this tutorial](https:\/\/www.kaggle.com\/rtatman\/bigquery-machine-learning-tutorial). If you haven't taken a look at it yet, head over and check it out first. (Otherwise these exercises will be pretty confusing!) -- Rachael ","6d0cc147":"You'll want to inspect your data to ensure it looks like what you expect. Run the line below to get a quick view of the data, and feel free to explore it more if you'd like (if you don't know how to do that, the [Pandas micro-course](https:\/\/www.kaggle.com\/learn\/pandas)) might be helpful.","090aa097":"You should see that the r^2 score here is negative. Negative values indicate that the model is worse than just predicting the mean rides for each example.\n\n## 5) Theories for poor performance\n\nWhy would your model be doing worse than making the most simple prediction based on historical data?","c60efd87":"## 6) Exercise: Looking at predictions\n\nA good way to figure out where your model is going wrong is to look closer at a small set of predictions. Use your model to predict the number of rides for the 22nd & Pearl station in 2018. Compare the mean values of predicted vs actual riders.","f12db3c8":"Write your query below:","c4cb293f":"## Linear Regression\n\nYour dataset is quite large. BigQuery is especially efficient with large datasets, so you'll use BigQuery-ML (called BQML) to build your model. BQML uses a \"linear regression\" model when predicting numeric outcomes, like the number of riders.\n\n## 1) Training vs testing\n\nYou'll want to test your model on data it hasn't seen before (for reasons described in the [Intro to Machine Learning Micro-Course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning). What do you think is a good approach to splitting the data? What data should we use to train, what data should we use for test the model?","70794234":"Write your query below:"}}