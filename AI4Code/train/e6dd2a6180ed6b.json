{"cell_type":{"75b48731":"code","20d2751b":"code","374773e9":"code","0c5fbf4a":"code","3e8e6012":"code","7b7ca3ed":"code","737de244":"code","ea3a3b50":"code","dcd51311":"code","e4d90651":"code","255f2c0e":"code","2a7e71bb":"code","682370fb":"code","57626bdc":"code","eb0e9b81":"code","de71a168":"code","480fdd38":"markdown","f974b950":"markdown","8877a3fa":"markdown","4c52ca6e":"markdown","ef61b366":"markdown","10e13f3d":"markdown","9c64d5d7":"markdown","ea3cc9d2":"markdown","83ea0a83":"markdown","20c0d934":"markdown","638c55b2":"markdown","a2bffbd8":"markdown","b0e2da30":"markdown","4fbeeff9":"markdown","7c5961db":"markdown","4a1726fe":"markdown","d0628587":"markdown"},"source":{"75b48731":"import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=300, n_features=1, n_informative=1, noise=10, random_state=7)\nplt.scatter(X, y, color='blue')\nplt.title('Generated pseudo-data', fontsize=16)\nplt.xlabel('x', fontsize=13)\nplt.ylabel('y', fontsize=13)\nplt.show()","20d2751b":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\nprint(f\"Number of data points in X_train: {len(X_train)}\")\nprint(f\"Number of data points in X_test: {len(X_test)}\")\nprint(f\"Number of data points in y_train: {len(y_train)}\")\nprint(f\"Number of data points in y_test: {len(y_test)}\")","374773e9":"p, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(14, 5))\nax1.scatter(X_train, y_train, color='blue')\nax1.set_title('Training data', fontsize=16)\nax2.scatter(X_test, y_test, color='blue')\nax2.set_title('Testing data', fontsize=16)\np.show()","0c5fbf4a":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)   # Note, X_test and y_test are NOT referenced here!\n","3e8e6012":"y_pred = lr.predict(X_test)\nplt.plot(X_test, y_pred, color='red', linewidth=3)\nplt.title('Linear regression result', fontsize=16)\nplt.xlabel('x', fontsize=13)\nplt.ylabel('y', fontsize=13)\nplt.show()","7b7ca3ed":"from IPython.display import display, Math\ndisplay(Math(r'y = {:.2f}x + {:.2f}'.format(lr.coef_[0], lr.intercept_)))","737de244":"plt.scatter(X_test, y_test, color='blue')\nmax_x = max(X_test)\nmin_x = min(X_test)\nplt.plot(X_test, y_pred, color='red', linewidth=3)\nplt.title('Fitted linear regression', fontsize=16)\nplt.xlabel('x', fontsize=13)\nplt.ylabel('y', fontsize=13)\nplt.show()","ea3a3b50":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nprint('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\nprint('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))","dcd51311":"from sklearn.datasets import make_sparse_uncorrelated\n\nX, y = make_sparse_uncorrelated(n_samples=20000, n_features=100, random_state=7)\n\nfig, ax = plt.subplots(3,3, figsize = (10,10))\nfor i in range(3):\n    for j in range(3):\n        ax[i,j].set_title(f\"Dimension {i*3+j+1}\")\n        ax[i,j].scatter(X[:,i*3+j], y, s=1)\nplt.tight_layout()\nplt.show()","e4d90651":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\nprint(f\"Number of data points in X_train: {len(X_train)}\")\nprint(f\"Number of data points in X_test: {len(X_test)}\")\nprint(f\"Number of data points in y_train: {len(y_train)}\")\nprint(f\"Number of data points in y_test: {len(y_test)}\")","255f2c0e":"from sklearn.linear_model import SGDRegressor\nlr = SGDRegressor(verbose=2)\nlr.fit(X_train, y_train)   # Note, X_test and y_test are NOT referenced here!","2a7e71bb":"y_pred = lr.predict(X_test)\n\n# The mean squared error\nprint('Mean squared error: %.2f'\n      % mean_squared_error(y_test, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f'\n      % r2_score(y_test, y_pred))","682370fb":"import numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\n\ndef true_fun(X):\n    return np.sin(1.5 * np.pi * X)\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i],\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n\n    X_test = np.linspace(0, 1, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    plt.title(\"Degree {}\\nMSE = {:.2e}(+\/- {:.2e})\".format(\n        degrees[i], -scores.mean(), scores.std()))\nplt.show()","57626bdc":"from sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.utils.validation import check_random_state\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import RidgeCV\n\n# Load the faces datasets\ndata = fetch_olivetti_faces()\ntargets = data.target\n\n# assemble training and test sets\ndata = data.images.reshape((len(data.images), -1))\ntrain = data[targets < 30]\ntest = data[targets >= 30]  # Test on independent people\n\n# Pick 5 random people from the test set to render later\nn_faces = 5\nrng = check_random_state(4)\nface_ids = rng.randint(test.shape[0], size=(n_faces, ))\ntest = test[face_ids, :]\nprint(\"All the training faces:\")\n\n# render some sample faces\nimage_shape = (64, 64)\nplt.figure(figsize=(10,30))\nfor i in range(len(train)):\n    sub = plt.subplot(30, 10, i+1)\n    sub.set_axis_off()\n    sub.imshow(train[i].reshape(image_shape),\n               cmap=plt.cm.gray,\n               interpolation=\"nearest\")\nplt.tight_layout()\nplt.show()","eb0e9b81":"# cut the pictures in half\nn_pixels = data.shape[1]\nX_train = train[:, :(n_pixels + 1) \/\/ 2] # Upper half of the faces\ny_train = train[:, n_pixels \/\/ 2:] # Lower half of the faces\nX_test = test[:, :(n_pixels + 1) \/\/ 2] # Upper half of the faces\ny_test = test[:, n_pixels \/\/ 2:] # Lower half of the faces\nprint(f\"There are {len(X_train)} training values\")\nprint(f\"There are {len(X_test)} testing values\")\n\n# render some sample faces\nimage_shape = (32, 64)\nplt.figure(figsize=(25,40))\n#plt.suptitle(\"Sample Faces\", size=16)\nfor i in range(len(X_test)):\n    sub = plt.subplot(len(X_test)\/10+1, 10, i+1)\n    sub.set_axis_off()\n    sub.imshow(X_test[i].reshape(image_shape),\n               cmap=plt.cm.gray,\n               interpolation=\"nearest\")\nplt.show()\n","de71a168":"# Define estimators\nESTIMATORS = {\n    \"Linear regression\": LinearRegression(),\n    \"RidgeCV\": RidgeCV(),\n    \"Random Forest\": RandomForestRegressor(n_estimators=20, max_depth=10, max_features=10),\n    \"Neural network\": MLPRegressor(hidden_layer_sizes=(100, 30, 10, 5)),\n}\n\n# Fit estimators\ny_test_predict = dict()\nfor name, estimator in ESTIMATORS.items():\n    print(f\"Now training: {name}\")\n    estimator.fit(X_train, y_train)\n    y_test_predict[name] = estimator.predict(X_test)\n    print(f\"Mean squared error: {mean_squared_error(y_test, y_test_predict[name])}\")\n    print(f\"R2: {r2_score(y_test, y_test_predict[name], multioutput='variance_weighted')}\")\n    print()\n\n# Plot the completed faces\nimage_shape = (64, 64)\nn_cols = 1 + len(ESTIMATORS)\nplt.figure(figsize=(2. * n_cols, 2.26 * n_faces))\nplt.suptitle(\"Face completion with multi-output estimators\", size=16)\nfor i in range(n_faces):\n    # render the true face\n    true_face = np.hstack((X_test[i], y_test[i]))\n    if i:\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1)\n    else:\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1,\n                          title=\"true faces\")\n    sub.axis(\"off\")\n    sub.imshow(true_face.reshape(image_shape),\n               cmap=plt.cm.gray,\n               interpolation=\"nearest\")\n    \n    # render the generated faces\n    for j, est in enumerate(ESTIMATORS):\n        completed_face = np.hstack((X_test[i], y_test_predict[est][i]))\n        if i:\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j)\n        else:\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j,\n                              title=est)\n        sub.axis(\"off\")\n        sub.imshow(completed_face.reshape(image_shape),\n                   cmap=plt.cm.gray,\n                   interpolation=\"nearest\")\nplt.show()","480fdd38":"Let's train! This time we'll use a stochastic linear regression model, since we have so many data points and there's a risk that the resultant matrix would be non-invertible.","f974b950":"## Linear Regression Examples\n\nNote: this notebook works only with Python >= 3.6.\n\n(Credit to https:\/\/tutorials.technology\/tutorials\/19-how-to-do-a-regression-with-sklearn.html)","8877a3fa":"### First example: simple linear regression\n\nFirst, we're going to generate some pseudo-random data that is roughly linear.  Scikit-learn provides a function that generates this kind of data.","4c52ca6e":"Now, we'll create the LinearRegression object and train it with our training data.  Note, we are not using the test data at all yet.","ef61b366":"Now, we'll split the x and y data into training sets and test sets.  We'll take out 20% of our data and reserve it for test data.","10e13f3d":"### Second example: a dataset with 100 features!\n\nSo far, all of this could easily be done in Excel.  Let's try something harder! How about our example where we had 100 variables and 20,000 data points?  Let's generate some data in a similar way. Since plotting a 100-dimensional space is quite difficult, we'll plot only 9 '2-D projections' of the first 9 dimensions below.\n\nWe'll use \"make_sparse_uncorrelated\" to generate our data, which has the interesting property that only the first four features are useful (i.e. linear).","9c64d5d7":"(Why do these look like blobs, and not lines?  Remember, we are projecting a 100-dimensional dataset onto 2 dimensions. We shouldn't expect to see a line in one of these slices.)\n\nNow, split the data as before.","ea3cc9d2":"### Fourth example: Linear regression applied to face prediction\n\nWe can even apply linear regression, as well as other models, to pixel data.  Let's see if we can train different types of models on a selection of faces.  Given the upper half of a new face, can the model predict the bottom half?","83ea0a83":"Now let's plot the line and the test data together:","20c0d934":"### Third example: Underfitting, overfitting and \"just right\"\n\nThis example involves generating some noisy data that approximates a sin curve, and increasing the degree of the polynomial and see how it affects the MSE.","638c55b2":"We'll plot the training data and testing data separately so you can see the distribution:","a2bffbd8":"Now let's take a look at our MSE and R^2.","b0e2da30":"We can find out the exact parameters used for this line formula:","4fbeeff9":"Not bad. Remember, the training routine **never saw** any of these test data points.\n\nWe have a formal way of quantifying the quality of predictions, by using the mean squared error and coefficient of determination (R^2):","7c5961db":"Now, predict the test values from this new model, and plot the resulting line:","4a1726fe":"That mean squared error looks huge, but remember the scale of the y axis we are dealing with.  The R^2 value indicates a good fit (where 1.0 is perfect).","d0628587":"**Moral:** the mean squared errors and r^2 values don't necessarily tell you the whole story. Always double-check the results!"}}