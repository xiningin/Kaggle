{"cell_type":{"e669bb13":"code","852d4ff7":"code","84f5c61c":"code","0dd5233f":"code","ecc95d41":"code","8e616b14":"code","dd318ad6":"code","10a53677":"code","5d0cb6c9":"code","8ee93c6d":"code","247edeb6":"code","efa0dfec":"code","b4ad6564":"code","d818bffd":"code","863027cb":"code","7f232258":"code","2199d937":"code","a7793eac":"markdown","15fad8b1":"markdown","b4617fc1":"markdown"},"source":{"e669bb13":"from sklearn.datasets import load_wine","852d4ff7":"wine = load_wine()","84f5c61c":"import pandas as pd\nfeatures = pd.DataFrame(data=wine['data'],columns=wine['feature_names'])\nfeatures.head()","0dd5233f":"print(wine)","ecc95d41":"print(wine.DESCR)","8e616b14":"X = wine.data\ny = wine.target\nfeatures = wine.feature_names\nprint(len(features))","dd318ad6":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfor i in range(13):\n  sns.boxplot(x=y,y=X[:,i])\n  plt.ylabel(features[i])\n  plt.show()","10a53677":"for i in range(13):\n  plt.hist(X[:,i],edgecolor='black')\n  plt.title(features[i])\n  plt.show()","5d0cb6c9":"target_name = wine.target_names\nprint(target_name)","8ee93c6d":"X.shape","247edeb6":"y.shape","efa0dfec":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=32)","b4ad6564":"X_train.shape","d818bffd":"y_train.shape","863027cb":"svm = SVC()\nsvm.fit(X_train,y_train)\nsvm.score(X_test,y_test)","7f232258":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = { 'C':[0.1,1,100,1000],'kernel':['rbf','poly','sigmoid','linear'],'degree':[1,2,3,4,5,6]}\ngrid = GridSearchCV(SVC(),param_grid)\ngrid.fit(X_train,y_train)","2199d937":"print(grid.best_params_)\nprint(grid.score(X_test,y_test))","a7793eac":"# Hyper-parameter Tuning","15fad8b1":"**SVM**\n\nSVM is the extremely popular algorithm. In this notebook I try to give a explanation for how it works, how we do a hyper-parameter tuning and give a example using python sklearn library.\n\n\nSVM stands for Support Vector Machine. It is a Supervised Machine Learning algorithm. It is used for both classification and regression problem. It uses a process called kernel strategy to modify your data and based on these changes finds the perfect boundary between the possible results.\n\nMost of the times we get linear data but usually things are not that simple. Let\u2019s take an example of classification with non-linear data :\n \nNow, to classify this type of data we add a third dimension to this two-dimension plot. We rule that it be calculated a certain way that is convenient for us: z = x\u00b2 + y\u00b2 (you\u2019ll notice that\u2019s the equation for a circle). It give us a three dimension space. Since we are in three dimensions now, the hyperplane is a plane parallel to the x axis at a certain z (let\u2019s say z = 1). Now, we convert it again in two dimensions.\nIt looks like this :\n\n![image.png](attachment:image.png)\n \nAnd here we go! Our decision boundary is a circumference of radius 1, which separates both tags using SVM.\n","b4617fc1":"Here, we use wine dataset, In this dataset we have to find the class of Wine. \n\nIn this notebook we learn how to implement our SVM model and how to tune our hyper-parameters.\nHere is the code link\nLet\u2019s Start\nWe take the Wine dataset to perform Support Vector Classifier.\n\uf0d8\tHere is dataset information:\nInput variables ( based on physicochemical tests ):\n        1.\tAlcohol\n        2.\tMalic acid\n        3.\tAsh\n        4.\tAlcalinity of ash\n        5.\tMagnesium\n        6.\tTotal phenols\n        7.\tFlavanoids\n        8.\tNon flavonoids phenols\n        9.\tProanthocyanins\n        10. Color Intensity\n        11. Hue\n        12. od280\/od315_of_diluted_wines\n        13. Proline\nOutput variable:\n        14. Class\nLibraries used \u2013\n\n        o\tPandas\n        \n        o\tNumpy\n        \n        o\tMatplotlib\n        \n        o\tSeaborn\n        \n        o\tSklearn\n        \n        o\tGrid Search CV\n"}}