{"cell_type":{"a90d57e1":"code","80dc3eb6":"code","8d6c89ec":"code","9c9785d3":"code","c44704b2":"code","0ba31ae7":"code","636077ce":"code","5a290dbc":"code","129caf17":"code","8e479482":"code","038cc19e":"code","25e20f8a":"code","e6cedd01":"code","09a3cfd7":"code","b54563f3":"code","60306ce9":"code","59792e9a":"code","0c75e8b9":"code","117bd927":"code","ac64fa6d":"code","ae890286":"code","92aedafe":"code","dc6bb799":"code","a12cb6c2":"code","17581738":"code","83dd02f3":"code","2be48c41":"code","77d5ed0a":"code","75c4ee4a":"code","5d3e7b61":"code","45d53b81":"code","71851893":"code","590839b0":"code","64c5d8a7":"code","341160c5":"markdown","b49b9a19":"markdown","b2d9631f":"markdown","17f059ba":"markdown","f18e2cbe":"markdown","e24ce34c":"markdown","de8987f9":"markdown","1c7c72f2":"markdown","a345098a":"markdown","697ee234":"markdown","6fbcb8d4":"markdown","65868374":"markdown","9aff2446":"markdown","1f9c8abc":"markdown","a00436f7":"markdown","4a197a51":"markdown","d90460bb":"markdown","0c7291b0":"markdown","65a2477c":"markdown"},"source":{"a90d57e1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","80dc3eb6":"# Read Data\ndf = pd.read_csv(\"..\/input\/voice.csv\")","8d6c89ec":"# First 5 Rows of Data\ndf.head()","9c9785d3":"df.columns","c44704b2":"df.info()","0ba31ae7":"sns.pairplot(df, hue='label', vars=['skew', 'kurt',\n       'sp.ent', 'sfm', 'mode','meanfun',\n       'meandom','dfrange'])\nplt.show()","636077ce":"sns.countplot(df.label)\nplt.show()","5a290dbc":"sns.scatterplot(x = 'skew', y = 'kurt', hue = 'label', data = df)\nplt.show()","129caf17":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(), annot=True, linewidth=.5, fmt='.2f', linecolor = 'grey')\nplt.show()","8e479482":"X = df.drop(['label'],axis=1)\ny = df.label","038cc19e":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)","25e20f8a":"# Import SVM\nfrom sklearn.svm import SVC\n\nsvm = SVC()\nsvm.fit(X_train, y_train)","e6cedd01":"y_pred = svm.predict(X_test)","09a3cfd7":"from sklearn.metrics import confusion_matrix, classification_report","b54563f3":"cm = confusion_matrix(y_test, y_pred)","60306ce9":"sns.heatmap(cm, annot=True, cmap=\"Paired_r\", linewidth=2, linecolor='w', fmt='.0f')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Value')\nplt.show()","59792e9a":"print(\"Test Accuracy: {:.2f}%\".format(svm.score(X_test, y_test)*100))","0c75e8b9":"# Normalization\nX = (X - np.min(X)) \/ (np.max(X) - np.min(X)).values","117bd927":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","ac64fa6d":"svm.fit(X_train, y_train)","ae890286":"y_pred = svm.predict(X_test)","92aedafe":"cm = confusion_matrix(y_test,y_pred)","dc6bb799":"sns.heatmap(cm, annot=True, fmt='.0f', cmap='brg_r')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Value')\nplt.show()","a12cb6c2":"print(\"Test Accuracy: {:.2f}%\".format(svm.score(X_test, y_test)*100))","17581738":"param_grid = {'C':[0.1, 1, 10, 100], 'gamma':[1, 0.1, 0.01, 0.001], 'kernel' : ['rbf', 'poly', 'sigmoid', 'linear']}","83dd02f3":"from sklearn.model_selection import GridSearchCV","2be48c41":"grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=4)","77d5ed0a":"grid.fit(X_train, y_train)","75c4ee4a":"print(\"Best Parameters: \",grid.best_params_)","5d3e7b61":"grid_pred = grid.predict(X_test)","45d53b81":"cmNew = confusion_matrix(y_test, grid_pred)","71851893":"sns.heatmap(cmNew, annot=True, fmt='.0f', cmap='gray_r')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Value')\nplt.show()","590839b0":"print(\"Test Accuracy: {:.2f}%\".format(grid.score(X_test, y_test)*100))","64c5d8a7":"print(classification_report(y_test, grid_pred))","341160c5":"We'll use 70% of our data to train our model and we'll test it with 30% of the data.","b49b9a19":"Also there is 'degree' parameters. It is used for 'poly' kernel to define degree of polynomial kernel. It is 3 by default.","b2d9631f":"Also SVM uses a technique called **kernel trick** to transform the data. If datapoints have low dimensional space and it wouldn't be able to draw a hyperplane it tries to add a new dimension to data.\n<br>\n<img src=\"https:\/\/qph.fs.quoracdn.net\/main-qimg-8a4a30421342fedb9bdda38fbd2529a8\" \/>\nNow I'll explain some parameters in SVM and we'll try to use SVM to classify voices according to features.","17f059ba":"## CONTENT\n<br>\n\n1. [What is Support Vector Machine(SVM)](#1)\n1. [SVM Parameters](#2)\n1. [Import Libraries and Read Data](#3)\n1. [Visualize Data](#4)\n1. [Create and Evaluate Model](#5)","f18e2cbe":"# SUPPORT VECTOR MACHINE","e24ce34c":"**Kernel**\n<br>\nYou can choose the kernel type used by SVM. It can be \u2018linear\u2019, \u2018rbf\u2019, \u2018poly\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019.\nAnd yes, answer is still same 'it depends your data'.\n<br>\n<img src=\"http:\/\/dataaspirant.com\/wp-content\/uploads\/2017\/01\/Iris_Petal_Svm.png\" \/>","de8987f9":"Our test score incresed a little bit again and we reach **97.79%** of accuracy.","1c7c72f2":"**Gamma Parameter**\n<br>\nIt is kernel coefficient. You use it if you choose 'rbf', 'poly' or 'sigmoid' as a kernel.","a345098a":"## <a id=3><\/a> Import Libraries and Read Data","697ee234":"**Thank you. If you like it please upvote and I will be happy to hear your comments and feedbacks**","6fbcb8d4":"## <a id=4><\/a>Visualize Data","65868374":"## <a id=5><\/a>Create and Evaluate Model","9aff2446":"Let's fit our model.","1f9c8abc":"## <a id=1><\/a>What is Support Vector Machine (SVM)","a00436f7":"Our accuracy is not well and as you can see confusion matrix above our prediction is not good. So let's try to improve our model. At first we'll normalize our data after that we'll apply some parameter optimizations.","4a197a51":"[](http:\/\/)SVM or Support Vector Machine algorithm tries to draw a hyperplane between two classes to seperate them. There can be many hyperplane but SVM's purpose is finding maximum margin or finding maximum distance between data points from each classes. This data points are nearest data points to hyperlane from each classes. Let's look image below it explains better.\n<br>\n<img src=\"https:\/\/lh6.googleusercontent.com\/r0dB9ntNr6FWOOLf6GqVUF72K4iBV_oR7IgAl3RO61WpDnIpgkwNhmjxjtMwNIN-23MMlJAnTFe0a2ZqXxMNF0WursGwV5bHaqRMmiCyEyH21k4e6Tj5DFBr2ck4DMgS-FkNz5fl\" width=400 \/>","d90460bb":"**C Parameter**\n<br>\nC parameter controls trade-off between training points.\n- Small C: Large margin\n- Large C: Small margin, it has potential to overfit.\n<br>\nIf you ask which is better to use, answer is 'it depends on your data'. It would be better if you try different C values to find best score.\n<br>\n<img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2018\/07\/svm-parameter-c-example.png\" \/>","0c7291b0":"Wow! Our score increase to 97.27% and all we did is normalize the data! We can see importance of normalizaton in here. Let's try to find best parameters for our model. ","65a2477c":"## <a id=2><\/a>SVM - Parameters"}}