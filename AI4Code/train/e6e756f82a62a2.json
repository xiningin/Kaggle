{"cell_type":{"a4f8865c":"code","66968e21":"code","0d3f59c9":"code","b98cf7fd":"code","53f51a70":"code","7bf6f800":"code","e88e07d3":"code","f2c02a9f":"code","20a2ed2e":"code","1161c3ba":"code","53c4bd83":"code","8f3e759a":"code","a6069294":"code","0d762ffb":"code","d4dc5f7a":"code","af43a360":"code","b4eb37c2":"code","017488dc":"markdown","917d54e3":"markdown","76b6676e":"markdown","a26eb45a":"markdown"},"source":{"a4f8865c":"#Author: Magda Monteiro\n#Date: July, 2021\n#Purpose: perform data cleaning and treatment using pandas and statistical functions\n#This data is from Kaggle.com\n\n#--\n\n'''To perform an exploratory data analysis, we first need to clean and treatment the data so that it becomes consistent. This is one of the most important steps if you want to make quality decisions. I made this kaggle notebook using functions from python libraries to process and analyze the data of a company that had the objective of retaining its customers.'''\n\n#--\n\n#So, let's begin!\n\n#first import the librarys of funcions \n#give a alias for the library\n\nimport pandas as pd\nimport statistics as sts\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n#later import the data\n#give a name for your data. In this case im using \"dataset\" like name\n#the name of the file that im using is \"..\/input\/dados-1\/Churn.csv\"\n\ndataset = pd.read_csv(\"..\/input\/dados-1\/Churn.csv\", sep = \";\")\n\n#for view the data,use the funcion:\n\ndataset.head()\n\n","66968e21":"#to view the total of lines and columns, use the funcion:\n\ndataset.shape","0d3f59c9":"#for rename for the columns, use the funcion:\n\ndataset.columns=[\"Id\", \"Score\",\"State\",\"Gender\",\"Age\",\"Patrimony\",\"Balance\", \"Products\",\"Temcartcredito\",\"Active\",\"Wage\",\"Exited\" ]\n\n#to view \n\ndataset.head()","b98cf7fd":"#some states are incorrectly named and in this case we will only need the PR, SC and RS state data.\n#we can see the \"state\" column, assigning a variable to the formula and then calling that variable\n\ngrouped=dataset.groupby(['State']).size()\ngrouped","53f51a70":"#to correct this, we will substitute the values for the mode\n#since the state that is most repeated is RS, we will replace the data that is inconsistent with it,\n#using the function below:\n\ndataset.loc[dataset['State'].isin(['TD','SP','RP']),'State']='RS'\n\n#to view \n\ngrouped=dataset.groupby(['State']).size()\ngrouped\n\n ","7bf6f800":"#in the case of data referring to age, we need it to be within the range of 0 to 120 years\n#first we will check for occurrences of values outside this range\n\ndataset.loc[(dataset['Age'] < 0) | (dataset['Age'] > 120)]","e88e07d3":"#now, we calculate the median to replace the data that out of range.\n#the median is less likely to have nonstandard values.\n#for this, we will use the statistics library function\n\nmedian=sts.median(dataset['Age'])\n\n#to replace the data that out of range:\n\ndataset.loc[(dataset['Age'] < 0) | (dataset['Age'] > 120)]=median\n\n#to check if there are still values for the age range:\n\ndataset.loc[(dataset['Age'] < 0) | (dataset['Age'] > 120)]","f2c02a9f":"#to identify duplicate data, we search by ID\n\ndataset[dataset.duplicated(['Id'],keep=False)]","20a2ed2e":"# to delete duplicate ID\n\ndataset.drop_duplicates(subset=\"Id\",keep=\"first\",inplace=True)\n\n#to check if there are duplicate ID\n\ndataset[dataset.duplicated(['Id'],keep=False)]\n\n","1161c3ba":"#to view the total of lines and columns,after delete duplicate ID\n#note that before there were 999 lines and now it has 995\ndataset.shape","53c4bd83":"#In order for the variable \"gender\" to enter the analysis, it needs to be transformed into a numeric variable. For that we will apply this function:\n\ndataset[\"Gender\"] = dataset[\"Gender\"].astype('category')\ndataset.insert(0, 'GenderCode', dataset.Gender.cat.codes)\ndataset.dtypes\n","8f3e759a":"#After, drop the columns \"Gender\" and \"Id\" \ndataset.drop(['Gender','Id'], axis='columns', inplace=True)\ndataset.head()\n\n","a6069294":"#to see the variables that relate to each other, we can to use the functions below:\n\n\nfig, ax = plt.subplots(figsize=(11,11)) \nsns.heatmap(dataset.corr(),annot=True, ax=ax)\nplt.show()","0d762ffb":"#I used the \"crosstab\" function to relate the \"Exited\" and \"Age\" columns of the \"dataset\" DataFrame , and find out what is the frequency of the \"Exited\" 0 and 1 in the ages.\nfrequency = pd.crosstab(dataset.Age, dataset.Exited)\nfrequency\n","d4dc5f7a":"#I renamed the columns to identify that the value 0 is the customers exited and the value 1 the ones that didn't exited\nfrequency.columns = [\"customers_exited\", \"customers_not_exit\"]\n","af43a360":"#I added the information to the \"dataset\" dataframe.\ntabela_d = pd.concat([dataset,frequency], axis=1)\ntabela_d","b4eb37c2":"plt.figure(figsize=(18,9))\nplt.bar(tabela_d['Age'],tabela_d['customers_exited'])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Customers_Exited\")\nplt.show()\n","017488dc":"In this graph, we can see that the largest number of customers who left the company are between 30 and 45 years old.","917d54e3":"The more they have values close to one and the lighter colors, the more the variables correlate. Here we can see that there are few strong correlations between the variables, but the one that has the greatest relationship with the \"Exited\" variable is the \"Age\" variable. That is, among the variables, the one that has the greatest relationship with the exited of clients is age.","76b6676e":"## Exploratory Data Analysis","a26eb45a":"The more they have values close to one and the lighter colors, the more the variables correlate. Here we can see that there are few strong correlations between the variables, but the one that has the greatest relationship with the \"Exited\" variable is the \"Age\" variable. That is, among the variables, the one that has the greatest relationship with the exited of clients is age."}}