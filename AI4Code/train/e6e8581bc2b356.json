{"cell_type":{"77e1087f":"code","92c85b82":"code","4ee05c82":"code","68e3b705":"code","a59793fe":"code","f49a8093":"code","4ac1b9bf":"code","01705a9e":"code","813f4f4b":"code","4dc0871e":"code","3163b21b":"code","0a62ab09":"code","e069ee53":"code","fb5c7e9b":"code","6941a79d":"code","e1844fc4":"code","538415dc":"code","0e5d9893":"code","d0f5eef1":"code","435491be":"code","94f2b6a9":"code","82fc0a9c":"code","ee53b58b":"code","3bbdb3b9":"code","13585e03":"code","d26c3506":"code","2dcff0a4":"code","a6c41f6a":"code","197c8a82":"code","bc3732ae":"code","16590f10":"code","b2b4a87e":"code","77a42795":"code","b130a1c6":"code","e6f75053":"code","5ed481ce":"code","d979d443":"code","b4edc13a":"code","35b5df3b":"code","181c72ac":"code","5a30afb7":"code","6997c7a2":"code","33e4a0ed":"code","c6bdfc8c":"code","48648d1f":"code","b1eb82c8":"code","315903f7":"code","bac37502":"code","126d97b1":"code","97934108":"code","8fef5a99":"code","c0ae7a7d":"code","a931115d":"code","7864bdc5":"code","38d3ed0e":"code","2837cf23":"code","a9f56571":"code","09a3258a":"code","b19ab163":"code","16f6f7d4":"code","a63a1c28":"code","408cbd5f":"code","ff2c094b":"code","30b74cdc":"code","18b33026":"code","818f971d":"code","109735b0":"code","f220b12d":"code","84e3a7df":"code","a8029651":"code","b7329b9c":"code","156fc32e":"code","4531017e":"markdown","6bab90bb":"markdown","8cb1e226":"markdown","ed8f0413":"markdown","360ea2e2":"markdown","34c12ddd":"markdown","d83f5e31":"markdown","f159a565":"markdown","1fc1711e":"markdown","2d3148bb":"markdown","fad10cc1":"markdown"},"source":{"77e1087f":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv', index_col = 'PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv', index_col = 'PassengerId')","92c85b82":"train.shape","4ee05c82":"test.shape","68e3b705":"train.tail()","a59793fe":"test.head()","f49a8093":"train.isna().sum()","4ac1b9bf":"test.isna().sum()","01705a9e":"train.dtypes.unique()","813f4f4b":"test.dtypes.unique()","4dc0871e":"train.select_dtypes(include = ['object']).describe()","3163b21b":"train.drop('Survived', axis = 1).select_dtypes(exclude = ['object']).describe()","0a62ab09":"target = train.Survived.copy()\ntarget","e069ee53":"target.isna().any()","fb5c7e9b":"target.loc[target == 1].size \/ target.size","6941a79d":"target.describe()","e1844fc4":"train.drop('Survived', axis = 1).columns.equals(test.columns)","538415dc":"pd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nsns.set_style('whitegrid')","0e5d9893":"plt.figure(figsize = (16, 6))\nsns.countplot(x = train.Survived, palette = 'Purples_r')","d0f5eef1":"def plot_grid(data, fig_size, grid_size, plot_type, target = ''):\n    \"\"\"\n    Custom function for plotting grid of plots.\n    It takes: DataFrame of data, size of a grid, type of plots, string name of target variable;\n    And it outputs: grid of plots.\n    \"\"\"\n    fig = plt.figure(figsize = fig_size)\n    if plot_type == 'histplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.histplot(data[column_name], kde = True, color = 'blueviolet', stat = 'count')\n    if plot_type == 'boxplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.boxplot(x = data[column_name], color = 'blueviolet')\n    if plot_type == 'countplot':\n        target = data[target]\n        for i, column_name in enumerate(data.drop(target.name, axis = 1).columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.countplot(x = data[column_name], hue = target, palette = 'Purples_r')\n            plot.legend(loc = 'upper right', title = target.name)\n    plt.tight_layout()","435491be":"plot_grid(train.drop('Survived', axis = 1), (16, 6), (2,3), 'histplot')","94f2b6a9":"pd.pivot_table(train, index = 'Survived', values = ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass'], aggfunc = 'mean')","82fc0a9c":"plot_grid(train.select_dtypes(exclude = 'object').drop(['Fare', 'Age'], axis = 1), (16, 6), (1, 3), 'countplot', 'Survived')","ee53b58b":"print(f\"{pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train, index = 'Survived', columns = 'SibSp', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train, index = 'Survived', columns = 'Parch', values = 'Name', aggfunc ='count')}\")","3bbdb3b9":"plt.figure(figsize = (16, 6))\nsns.heatmap(train.corr(), \n            annot = True,\n            fmt = '.2f',\n            square = True,\n            cmap = \"Purples_r\", \n            mask = np.triu(train.corr()))","13585e03":"plot_grid(train.drop('Survived', axis = 1), (16, 6), (2,3), 'boxplot')","d26c3506":"plot_grid(pd.concat([train.select_dtypes(include = 'object').drop(['Name', 'Ticket', 'Cabin'], axis = 1), target], axis = 1), (16, 6), (2,1), 'countplot', 'Survived')","2dcff0a4":"print(f\"{pd.pivot_table(train, index = 'Survived', columns = 'Sex', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values = 'Name', aggfunc ='count')}\")","a6c41f6a":"train.select_dtypes(include = 'object').nunique().sort_values(ascending = False)","197c8a82":"train_test = pd.concat([train.drop('Survived', axis = 1), test], keys = ['train', 'test'], axis = 0)\nmissing_values = pd.concat([train_test.isna().sum(),\n                            (train_test.isna().sum() \/ train_test.shape[0]) * 100], axis = 1, \n                            keys = ['Values missing', 'Percent of missing'])\nmissing_values.loc[missing_values['Percent of missing'] > 0].sort_values(ascending = False, by = 'Percent of missing').style.background_gradient('Purples')","bc3732ae":"train_cleaning = train.drop('Survived', axis = 1).copy()\ntest_cleaning = test.copy()\n\ntrain_cleaning['Cabin'].fillna('none', inplace = True)\ntest_cleaning['Cabin'].fillna('none', inplace = True)\n\ntrain_cleaning['Ticket'].fillna('none', inplace = True)\ntest_cleaning['Ticket'].fillna('none', inplace = True)\n\ntrain_cleaning['Age'].fillna(train_cleaning['Age'].median(), inplace = True)\ntest_cleaning['Age'].fillna(train_cleaning['Age'].median(), inplace = True)\n\ntrain_cleaning['Embarked'] = train_cleaning.groupby('Pclass').Embarked.apply(lambda x: x.fillna(x.mode()[0]))\ntrain_cleaning['Fare'] = train_cleaning.groupby('Pclass').Fare.apply(lambda x: x.fillna(x.median()))\nfor i in train.Pclass.unique():\n    test_cleaning.loc[test.Pclass == i, 'Embarked'] = test_cleaning.loc[test.Pclass == i, 'Embarked'].fillna(train.loc[train.Pclass == i].Embarked.mode()[0])\n    test_cleaning.loc[test.Pclass == i, 'Fare'] = test_cleaning.loc[test.Pclass == i, 'Fare'].fillna(train.loc[train.Pclass == i].Fare.median())","16590f10":"train_cleaning.isnull().sum().max() + test_cleaning.isnull().sum().max()","b2b4a87e":"train_test_cleaning = pd.concat([train_cleaning, test_cleaning], keys = ['train', 'test'], axis = 0)\ntrain_test_cleaning","77a42795":"train_test_cleaning['CabinLetter'] = train_test_cleaning.Cabin.str.split().apply(lambda x: x[-1][0].strip().lower() if x[0] != 'none' else np.nan)","b130a1c6":"train_test_cleaning.xs('train').groupby('Pclass').CabinLetter.apply(lambda x: x.value_counts().index[0])","e6f75053":"train_cleaning_new = train_test_cleaning.xs('train').copy()\ntest_cleaning_new = train_test_cleaning.xs('test').copy()\n\ntrain_cleaning_new['CabinLetter'] = train_cleaning_new.groupby('Pclass')['CabinLetter'].apply(lambda x: x.fillna(x.mode()[0]))\n\nfor i in train.Pclass.unique():\n    test_cleaning_new.loc[test_cleaning_new.Pclass == i, 'CabinLetter'] = test_cleaning_new.loc[test_cleaning_new.Pclass == i, 'CabinLetter'].fillna(train_cleaning_new.loc[train_cleaning_new.Pclass == i].CabinLetter.mode()[0])\n    \ntrain_test_cleaning = pd.concat([train_cleaning_new, test_cleaning_new], keys = ['train', 'test'], axis = 0)","5ed481ce":"train_test_cleaning['CabinNumbers'] = train_test_cleaning.Cabin.apply(lambda x: int(x[1:]) if x != 'none' else 0)\n\ntrain_test_cleaning['TicketNumbers'] = train_test_cleaning.Ticket.apply(lambda x: int(x) if x.isnumeric() else 0 if x == 'none' else int(x.split(' ')[-1]) if (x.split(' ')[-1]).isnumeric() else 0)\ntrain_test_cleaning['TicketLetters'] = train_test_cleaning.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.', '').replace('\/', '').lower() \n                                                                        if len(x.split(' ')[:-1]) > 0 else 'none')\ntrain_test_cleaning['TicketIsNumeric'] = train_test_cleaning.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n\ntrain_test_cleaning['FamilySize'] = train_test_cleaning.SibSp + train_test_cleaning.Parch + 1\ntrain_test_cleaning['FamilySize'] = train_test_cleaning['FamilySize'].apply(lambda x: 'no family' if (x == 1)\n                                                                            else 'medium' if (x == 2 or x == 3)\n                                                                            else 'large')\n\n# train_test_cleaning['AgeGroup'] = train_test_cleaning['Age'].apply(lambda x: 'infant' if (x < 1) \n#                                                                    else 'child' if (x >= 1 and x <= 11)                                                                    \n#                                                                    else 'teen' if (x >= 12 and x <= 17)\n#                                                                    else 'adult' if (x >= 18 and x <= 64)\n#                                                                    else 'adult+')\n\n# train_test_cleaning['Surname'] = train_test_cleaning['Name'].apply(lambda x: x.split(',')[0].lower())\ntrain_test_cleaning['Embarked'] = train_test_cleaning['Embarked'].str.lower()","d979d443":"train_test_cleaning","b4edc13a":"train_cleaning_target_cleaned = pd.concat([train_test_cleaning.xs('train'), target], axis = 1)\ntrain_cleaning_target_cleaned","35b5df3b":"print(f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'CabinLetter', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', values = 'TicketNumbers', aggfunc = (lambda x: x.mode()[0]))} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'TicketIsNumeric', values = 'Name', aggfunc ='count')} \\n\\n\" +\n#       f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'AgeGroup', values = 'Name', aggfunc ='count')} \\n\\n\" +\n      f\"{pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'FamilySize', values = 'Name', aggfunc ='count')}\")","181c72ac":"pd.pivot_table(train_cleaning_target_cleaned, index = 'Survived', columns = 'TicketLetters', values = 'Name', aggfunc = 'count')","5a30afb7":"train_cleaning_target_cleaned.select_dtypes(include = 'object').nunique().sort_values(ascending = False)","6997c7a2":"plot_grid(train_cleaning_target_cleaned.drop(['Survived', 'Pclass', 'TicketIsNumeric', 'SibSp', 'Parch'], axis = 1), (16, 6), (2, 3), 'histplot')","33e4a0ed":"plot_grid(train_cleaning_target_cleaned.drop(['Name', 'Ticket', 'Cabin', 'Age', 'Fare', 'TicketNumbers', 'TicketLetters', 'CabinNumbers'],\n                                             axis = 1), (16, 6), (3, 3), 'countplot', 'Survived')","c6bdfc8c":"# 'Age', 'Fare', 'TicketNumbers', 'CabinNumbers'\nfig, axs = plt.subplots(2, 2, figsize = (16, 6))\nsns.histplot(hue = train_cleaning_target_cleaned.Survived, x = train_cleaning_target_cleaned.Age, palette = {0 : 'black', 1 : 'purple'}, ax = axs[0][0])\naxs[0][0].set_title('Age distribution')\nsns.histplot(hue = train_cleaning_target_cleaned.Survived, x = train_cleaning_target_cleaned.Fare, palette = {0 : 'black', 1 : 'purple'}, ax = axs[0][1])\naxs[0][1].set_title('Fare distribution')\n\nsns.histplot(hue = train_cleaning_target_cleaned.Survived, x = train_cleaning_target_cleaned.TicketNumbers, palette = {0 : 'black', 1 : 'purple'}, ax = axs[1][0])\naxs[1][0].set_title('TicketNumbers distribution')\nsns.histplot(hue = train_cleaning_target_cleaned.Survived, x = train_cleaning_target_cleaned.CabinNumbers, palette = {0 : 'black', 1 : 'purple'}, ax = axs[1][1])\naxs[1][1].set_title('CabinNumbers distribution')\nplt.tight_layout()","48648d1f":"plt.figure(figsize = (16,6))\nsns.heatmap(train_cleaning_target_cleaned.corr(),\n            annot = True,\n            fmt = '.2f',\n            square = True,\n            cmap = \"Purples_r\",\n            mask = np.triu(train_cleaning_target_cleaned.corr()))","b1eb82c8":"to_drop = ['Name',\n           'Ticket',\n           'Cabin']\n\ntrain_test_cleaned = train_test_cleaning.drop(to_drop, axis = 1).copy()\ntrain_test_cleaned","315903f7":"label_cols = ['TicketLetters', 'Sex', 'Pclass', 'TicketIsNumeric', 'FamilySize']#'Surname', \nonehot_cols = ['CabinLetter', 'Embarked']\nnumerical_cols = ['Age', 'SibSp', 'Parch', 'Fare', 'TicketNumbers', 'CabinNumbers']#'Pclass'","bac37502":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# One-hot encoding\ntrain_test_onehot = pd.get_dummies(train_test_cleaned[onehot_cols])\nX_train_full_onehot, X_test_onehot = train_test_onehot.xs('train').reset_index(), train_test_onehot.xs('test').reset_index()\n\nX_train_full, X_test = train_test_cleaned.xs('train'), train_test_cleaned.xs('test')\n# Label encoding\nX_train_full_labeled = pd.DataFrame()\nX_test_labeled = pd.DataFrame()\nfor col in label_cols:\n    encoder = LabelEncoder()\n    encoder.fit(X_train_full[col])\n    \n    encoded_train = pd.Series(encoder.transform(X_train_full[col]), name = col)\n    X_train_full_labeled = pd.concat([X_train_full_labeled, encoded_train], axis = 1)\n    \n    encoded_test = pd.Series(encoder.transform(X_test[col]), name = col)\n    X_test_labeled = pd.concat([X_test_labeled, encoded_test], axis = 1)\n# Numerical features scaling\nscaler = StandardScaler()\nscaler.fit(X_train_full[numerical_cols])\nX_train_full_scaled = pd.DataFrame(scaler.transform(X_train_full[numerical_cols]), columns = numerical_cols)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test[numerical_cols]), columns = numerical_cols)\n# Concatenating it all together\nX_train_full = pd.concat([X_train_full_onehot, \n                          X_train_full_labeled, \n                          X_train_full_scaled], axis = 1)\nX_train_full.set_index('PassengerId', inplace = True)\nX_test = pd.concat([X_test_onehot, \n                    X_test_labeled, \n                    X_test_scaled], axis = 1)\nX_test.set_index('PassengerId', inplace = True)\nX_train_full","126d97b1":"X_test","97934108":"y_train_full = target\ny_train_full","8fef5a99":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import train_test_split\n\ntf.random.set_seed(1)","c0ae7a7d":"early_stopping = keras.callbacks.EarlyStopping(\n    patience = 10,#100 80 40 20 10\n    min_delta = 0.001,\n    restore_best_weights = True,\n)\n\nk = 5\nhistory = pd.DataFrame(columns = ['ValAccuracy', 'TrainAccuracy', 'StoppedEpoch'], index = range(k))\n\nfor fold in range(k):\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify = y_train_full, train_size = 0.8)\n    \n    model = keras.Sequential([layers.BatchNormalization(input_shape = [X_train_full.shape[1]]),\n                              layers.Dense(units = 16, activation = 'relu'),\n                              layers.Dropout(rate = 0.1),\n                              \n                              layers.BatchNormalization(),\n                              layers.Dense(units = 16, activation = 'relu'),\n                              layers.Dropout(rate = 0.1),\n                              \n                              layers.BatchNormalization(),\n                              layers.Dense(units = 1, activation = 'sigmoid')])\n    \n    model.compile(optimizer = 'adam',\n                  loss = 'binary_crossentropy',\n                  metrics = ['binary_accuracy'])\n    \n    model.fit(X_train, y_train,\n              validation_data = (X_valid, y_valid),\n              batch_size = 512,\n              epochs = 1000,\n              callbacks = [early_stopping],\n              verbose = 0,)\n    \n    history.loc[fold, 'ValAccuracy'] = model.history.history['val_binary_accuracy']\n    history.loc[fold, 'TrainAccuracy'] = model.history.history['binary_accuracy']\n    history.loc[fold, 'StoppedEpoch'] = early_stopping.stopped_epoch","a931115d":"fig, axs = plt.subplots(k, figsize = (16, 32))\nfig.suptitle(f'Train and validation accuracy for {k}-fold validation\\n\\n', fontsize = 16)\nfor i in range(k):\n    sns.lineplot(data = history.loc[i, 'ValAccuracy'], ax = axs[i], color = 'red')\n    sns.lineplot(data = history.loc[i, 'TrainAccuracy'], ax = axs[i], color = 'blue')\n    axs[i].legend(['Validation', 'Train'])\n    axs[i].set_ylabel('Accuracy')\n    axs[i].set_xlabel('Epochs')\n    \nplt.tight_layout()","7864bdc5":"history.StoppedEpoch.mean()","38d3ed0e":"model = keras.Sequential([layers.BatchNormalization(input_shape = [X_train_full.shape[1]]),\n                              layers.Dense(units = 16, activation = 'relu'),\n                              layers.Dropout(rate = 0.1),\n                              \n                              layers.BatchNormalization(),\n                              layers.Dense(units = 16, activation = 'relu'),\n                              layers.Dropout(rate = 0.1),\n                              \n                              layers.BatchNormalization(),\n                              layers.Dense(units = 1, activation = 'sigmoid')])\n\nmodel.compile(optimizer = 'adam',\n              loss = 'binary_crossentropy',\n              metrics = ['binary_accuracy'])\n\nhistory = model.fit(X_train_full, y_train_full,\n                    batch_size = 512,\n                    epochs = 33,\n                    verbose = 0)","2837cf23":"print(f\"Train mean: {np.mean(history.history['binary_accuracy'])}\"+\"\\n\"+\n      f\"Train std: {np.std(history.history['binary_accuracy'])}\")","a9f56571":"predictions_nn = model.predict(X_test)","09a3258a":"predictions_nn[predictions_nn > 0.5] = 1\npredictions_nn[predictions_nn <= 0.5] = 0","b19ab163":"predictions_nn[predictions_nn == 1].size","16f6f7d4":"predictions_nn[predictions_nn == 0].size","a63a1c28":"predictions_nn.flatten().astype('int64')","408cbd5f":"from sklearn.model_selection import cross_val_score, cross_validate\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","ff2c094b":"def test_estimators(X, y, estimators, labels, cv):\n    ''' \n    A function for testing multiple estimators.\n    It takes: full train data and target, list of estimators, \n              list of labels or names of estimators,\n              cross validation splitting strategy;\n    And it returns: a DataFrame of table with results of tests\n    '''\n    result_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        est_name = label\n        result_table.loc[row_index, 'Model Name'] = est_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv = cv,\n                                    n_jobs = -1)\n\n        result_table.loc[row_index, 'Test accuracy'] = cv_results['test_score'].mean()\n        result_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        result_table.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    result_table.sort_values(by=['Test accuracy'], ascending = False, inplace = True)\n\n    return result_table","30b74cdc":"X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify = y_train_full, train_size = 0.1)\ny_train","18b33026":"lr = LogisticRegression()\ndt = DecisionTreeClassifier(random_state = 1)\nrf = RandomForestClassifier()\n# xgb = XGBClassifier()\nlgbm = LGBMClassifier()\ncb = CatBoostClassifier(allow_writing_files = False, logging_level = 'Silent')\n\nestimators = [lr,\n              dt,\n              rf,\n              lgbm, \n              cb]\n#               xgb]\n\nlabels = ['LogRegression',\n          'DecisionTree',\n          'RandomForest',\n          'LGBM',\n          'CatBoost']\n#           'XGB']\n\nresults = test_estimators(X_train, y_train, estimators, labels, cv = 10)\nresults.style.background_gradient(cmap = 'Purples')","818f971d":"cb.fit(X_train_full, y_train_full)\nlgbm.fit(X_train_full, y_train_full)","109735b0":"predictions_cb = cb.predict(X_test)\npredictions_lgbm = lgbm.predict(X_test)","f220b12d":"submission = pd.DataFrame()","84e3a7df":"submission['PassengerId'] = X_test.index\nsubmission['pr_nn'] = predictions_nn.flatten().astype('int64')\nsubmission['pr_cb'] = predictions_cb\nsubmission['pr_lgbm'] = predictions_lgbm","a8029651":"submission[[col for col in submission.columns if col.startswith('pr_')]].sum(axis = 1).value_counts()","b7329b9c":"submission['Survived'] = (submission[[col for col in submission.columns if col.startswith('pr_')]].sum(axis=1) >= 2).astype(int)\nsubmission","156fc32e":"submission[['PassengerId', 'Survived']].to_csv('submission.csv', index = False)","4531017e":"# Introduction\nHello!\n\nIn this kernel you will find my approach to \"Tabular Playground Series - Apr 2021\" competition using neural network.","6bab90bb":"# Table of contents:\n\n1. Meeting our data\n\n2. Visualization and data analysis\n\n3. Data cleaning\n\n4. Feature engineering and encoding\n\n5. Creating and evaluating a model\n\n    5.1 Neural network\n\n    5.2 Other models\n\n    5.3 Voting ensemble","8cb1e226":"Taking a sample to save some time.","ed8f0413":"# 5.1 Neural network","360ea2e2":"# 4. Feature engineering and encoding","34c12ddd":"# 2. Visualization and data analysis","d83f5e31":"# 1. Meeting our data","f159a565":"# 5.2 Other models","1fc1711e":"# 3. Data cleaning","2d3148bb":"# 5. Creating and evaluating a model","fad10cc1":"# 5.3 Voting ensemble"}}