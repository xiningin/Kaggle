{"cell_type":{"043e4507":"code","e6a7e9d2":"code","74a014c7":"code","740ef1bf":"code","5ceaccbe":"code","5bf64afd":"code","3f920b30":"code","d3842b2d":"code","e4729674":"code","4009f33c":"code","6b9a8610":"code","9037914b":"code","e98e7945":"code","374d4b1b":"code","d32384d7":"code","dfaecad4":"code","25fb1f39":"code","c64cceba":"code","90dc7da3":"code","b9c306b1":"code","d6c987c0":"code","be5e538f":"code","78cec532":"code","641c11b4":"code","1bf2b02e":"code","c0c92471":"code","ffcee2b4":"code","28fee2e1":"code","686cfad4":"code","ff31d529":"code","55ff37ee":"code","1431f8ad":"markdown","94b60807":"markdown","33ccbe6e":"markdown","c0ee72c0":"markdown","a1041c30":"markdown","b69e90e4":"markdown","2dc6d133":"markdown","b0e9b13f":"markdown"},"source":{"043e4507":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\n\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')","e6a7e9d2":"os.chdir(\"\/home\/leon\/Documents\/projects\")","74a014c7":"true = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\nfake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")","740ef1bf":"true.head()","5ceaccbe":"fake.head()","5bf64afd":"fake.iloc[0]['text']","3f920b30":"true['label']  = pd.Series([0] * len(true)) # add label column | 0 == True, 1 == Fake\nfake['label']  = pd.Series([1] * len(fake))\n\ntrue['label'] = pd.Categorical(true['label']) # make label categorical\nfake['label'] = pd.Categorical(fake['label'])","d3842b2d":"full = true.append(fake) # create single df","e4729674":"full","4009f33c":"def remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers: word_tokenize('ebrahim^hazrati')'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n\n#remove one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\ndef full_clean(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%","6b9a8610":"full['clean'] = [full_clean(i) for i in full['text']]","9037914b":"full.to_csv('fake_real.csv')","e98e7945":"full = pd.read_csv(\"..\/input\/fakereal\/fake_real.csv\")","374d4b1b":"articles = full['clean'].dropna().to_list()\narticles[:2]","d32384d7":"plt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(articles))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","dfaecad4":"import tensorflow as tf\n\nfrom tensorflow import keras\n\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\nfrom keras import optimizers\n\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split","25fb1f39":"def balanced_subsample(y, size=None):\n    \n    '''Sample from data and keep the classes balanced'''\n\n    subsample = []\n\n    if size is None:\n        n_smp = y.value_counts().min()\n    else:\n        n_smp = int(size \/ len(y.value_counts().index))\n\n    for label in y.value_counts().index:\n        samples = y[y == label].index.values\n        index_range = range(samples.shape[0])\n        indexes = np.random.choice(index_range, size=n_smp, replace=False)\n        subsample += samples[indexes].tolist()\n\n    return subsample","c64cceba":"full.head()","90dc7da3":"# sample 10% of the data\nsample = balanced_subsample(full['label'], len(full)*0.1) \n\n# extract the indices picked by the function to create subsample\nsample = full.iloc[sample, :]  ","b9c306b1":"text = sample['clean'].astype(str)\ndocs = text.to_list()\n\nlabels = pd.array(sample['label'])","d6c987c0":"print(len(docs))\n# print('\\n')\nprint(len(labels))","be5e538f":"# check number of unique words to estimate a reasonable vocab size\n\none_str = ''.join(text)\nunique_words = Counter(one_str.split())\nlen(unique_words) ","78cec532":"vocab_size = 46856  # 20000 for the entire dataset\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs[:1]) # list of lists","641c11b4":"x = []\n\nfor i in encoded_docs:\n    x.append(len(i))\n    \nprint(\"In my sample the largest document has\", max(x), \"words.\")","1bf2b02e":"# now every document will be represented by a vector of the same length: 3000 values \/ 'words'\n\nmax_length = 2996\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)\n\n","c0c92471":"X = padded_docs\ny = sample['label']","ffcee2b4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","28fee2e1":"model = Sequential()\nmodel.add(Embedding(input_dim = vocab_size, output_dim = 32, input_length = max_length))\n\nmodel.add(Bidirectional(LSTM(64, activation='linear')))\nmodel.add(Dense(32, activation='linear'))\nmodel.add(Dense(1, activation='sigmoid'))\n   \nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n   \nmodel.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\nprint(model.summary())","686cfad4":"# fit the model\nhistory = model.fit(X_train, y_train, epochs=5, verbose=1, batch_size=30, validation_split = 0.2)\n# evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","ff31d529":"import matplotlib.pyplot as plt\n\nplt.style.use(\"fivethirtyeight\")","55ff37ee":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","1431f8ad":"## Apply the cleaning functions on text and export the cleaned text to save memory.","94b60807":"# Text classification using LSTM\n\nIn this notebook I will make use of Long Short Term Memory (LSTM) architecture to classify newspaper articles as either real or fake, using Keras. ","33ccbe6e":"## Load in cleaned data","c0ee72c0":"## Create single, labeled dataframe\n\nTo merge the two dataframes I add a label column to each and stack the frames on top of each other. This way I combined the data and added the labels.","a1041c30":"## Exploration","b69e90e4":"We can see The United States and especially Trump is a common theme in the articles. International affairs (north korea) and U.S. elections (hillary clinton) are also featured often.  ","2dc6d133":"# Cleaning\n\n### The following functions clean the text data (punctuation, stopwords, make lowercase etc.) \n\nThe nice thing is that the very last function 'full_clean' can be adapted by removing or adding the other functions.\n","b0e9b13f":"Unfortunately my computer cannot handle the full data, so I needed to subsample and work with only 10% of the data."}}