{"cell_type":{"518f61cf":"code","f24d470f":"code","ea3a8fa8":"code","e2837a45":"code","e82c3c3c":"code","1ee9e32b":"code","4bd65e93":"code","a15b98c5":"code","b14eb724":"code","13c8603f":"code","34e018a5":"code","b0f81f36":"code","2ba59af4":"code","5255e6c1":"code","2ea821ac":"code","4f3ced86":"code","3f024757":"code","c3b086d1":"code","ecd5171f":"code","c0b17bdf":"code","bb3d89c7":"code","d016363e":"code","38f0182d":"code","71316be5":"code","56476113":"code","c05b902e":"code","42c9adb5":"code","b089ef1c":"code","8a6aecf9":"code","7e2f3c8c":"code","66737b98":"code","73dedbcd":"code","56fa388d":"code","1ddbd886":"code","423b015c":"code","76b78064":"code","4e377a3f":"code","735e454c":"code","5cae8337":"code","068ed25e":"code","e78f7900":"code","7db6dc60":"code","69c406de":"code","d710f219":"code","95db2059":"code","bc69a350":"markdown","57a3c157":"markdown","26229d89":"markdown","8eb5210e":"markdown","c9ce10eb":"markdown","ed05b737":"markdown","d89ed39b":"markdown"},"source":{"518f61cf":"import os\nimport string\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\nimport seaborn as sns\n\nfrom nltk.stem import WordNetLemmatizer\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n\nfrom collections import defaultdict","f24d470f":"lemmatizer=WordNetLemmatizer()","ea3a8fa8":"nlp=English()\nstop_words=nlp.Defaults.stop_words\n\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\nprint(nlp.pipe_names)","e2837a45":"train=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain=train[['id', 'excerpt', 'target']]\ntrain.head()","e82c3c3c":"print(\"Number Of Records:\", train.id.nunique())","1ee9e32b":"train.target.describe()","4bd65e93":"plt.plot(train.target.sort_values().values)","a15b98c5":"\nsns.displot(data=train, x='target', bins=100)\nplt.title(\"Reading Ease\")\nplt.xticks(np.arange(-4, 2, 0.5))\nplt.figure(figsize=(10, 5))\nplt.show()","b14eb724":"sns.boxplot(data=train, x='target')","13c8603f":"class Vocab:\n    def __init__(self, passages):\n        self.passages=passages\n        self.word2id={}\n        self.id2word={}\n        self.vocab_freq={}\n        self.vocab_=[]\n    def build(self):\n        for passage in self.passages:\n            for word in nlp(passage):\n                if word.is_stop or word.is_punct or word.like_num or len(word.text.strip())<=1:\n                    continue\n                word=word.lower_.strip()\n                word=lemmatizer.lemmatize(word)\n                if word not in self.vocab_freq:\n                    self.vocab_freq[word]=0\n                self.vocab_freq[word]+=1\n        for idx, word in enumerate(self.vocab_freq.keys()):\n            self.word2id[word]=idx\n            self.id2word[idx]=word\n            self.vocab_.append(word)\n            \n    def __len__(self):\n        return len(self.vocab_)","34e018a5":"%%time\nvocab=Vocab(train.excerpt.values)\nvocab.build()","b0f81f36":"print(\"Vocab Size:\", len(vocab))","2ba59af4":"df=pd.DataFrame.from_dict({\"word\": list(vocab.vocab_freq.keys()),\"freq\": list(vocab.vocab_freq.values())})\ndf.head()","5255e6c1":"df.freq.describe()","2ea821ac":"plt.plot(df.freq.sort_values().values)","4f3ced86":"plt.plot(np.log(df.freq).sort_values().values)","3f024757":"plt.plot((df[df.freq>5].freq.cumsum()).sort_values().values)","c3b086d1":"def get_frequency_distribution(passage):\n    freq_dist=defaultdict(int)\n    for word in nlp(passage):\n        if word.is_stop or word.is_punct or word.like_num or len(word.text.strip())<=1:\n            continue\n        word=word.lower_.strip()\n        word=lemmatizer.lemmatize(word)\n        if word in vocab.vocab_freq:\n            freq_dist[vocab.vocab_freq[word]]+=1\n    return freq_dist","ecd5171f":"train['freq_dist'] = train.excerpt.apply(get_frequency_distribution)\ntrain.head()","c0b17bdf":"def get_bin_num(freq):\n    if freq <= 20:\n        return 0\n    elif freq <= 50:\n        return 1\n    return 2\ndf['bin_num']=df.freq.apply(get_bin_num)\ndf.head()","bb3d89c7":"def bin_distribution(freq_dist):\n    total_cnt=0\n    bins=np.zeros(3)\n    for key,value in freq_dist.items():\n        total_cnt+=value\n        if key <= 20:\n            bins[0]+=value\n        elif key <= 50:\n            bins[1]+=value\n        else:\n            bins[2]+=value\n    for i in range(3):\n        bins[i]\/=total_cnt\n        bins[i]=round(bins[i], 2)\n    return bins","d016363e":"train['bin_distribution']=train.freq_dist.apply(bin_distribution)\ntrain['bin1'] = train.bin_distribution.apply(lambda lst: lst[0])\ntrain['bin2'] = train.bin_distribution.apply(lambda lst: lst[1])\ntrain['bin3'] = train.bin_distribution.apply(lambda lst: lst[2])\n\n\ntrain.head()","38f0182d":"sns.countplot(data=df, x='bin_num')\n","71316be5":"fig, ax=plt.subplots(1, 3, sharey=True, figsize=(10, 4))\n\nsns.histplot(train, x='bin1', ax=ax[0])\nsns.histplot(train, x='bin2', ax=ax[1])\nsns.histplot(train, x='bin3', ax=ax[2])\n\nax[0].set_title('Bin1')\nax[1].set_title('Bin2')\nax[2].set_title('Bin3')\n\nplt.show()","56476113":"fig, ax=plt.subplots(3, 1, sharex=True, figsize=(10, 7))\n\nsns.boxplot(data=train, x='bin1', ax=ax[0])\nsns.boxplot(data=train, x='bin2', ax=ax[1])\nsns.boxplot(data=train, x='bin3', ax=ax[2])\n\nplt.show()","c05b902e":"train.head()","42c9adb5":"_, ax=plt.subplots(2, 3, figsize=(15, 6))\n\nsns.histplot(data=train, x='bin1', y='target', ax=ax[0][0])\nsns.lineplot(data=train, x='bin1', y='target', ax=ax[1][0])\n\nsns.histplot(data=train, x='bin2', y='target', ax=ax[0][1])\nsns.lineplot(data=train, x='bin2', y='target', ax=ax[1][1])\n\nsns.histplot(data=train, x='bin3', y='target', ax=ax[0][2])\nsns.lineplot(data=train, x='bin3', y='target', ax=ax[1][2])\n","b089ef1c":"import logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","8a6aecf9":"!pip install pyldavis","7e2f3c8c":"from gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nfrom gensim.models import TfidfModel\n\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\npyLDAvis.enable_notebook()","66737b98":"docs=[]\nfor passage in train.excerpt.values:\n    doc=[]\n    for word in nlp(passage):\n        if word.is_stop or word.is_punct or word.like_num or len(word.text.strip())<=1:\n            continue\n        word=word.lower_.strip()\n        word=lemmatizer.lemmatize(word)\n        doc.append(word)\n    docs.append(doc)","73dedbcd":"dictionary=Dictionary(docs)\ncorpus=[dictionary.doc2bow(doc) for doc in docs]\n#tfidfModel=TfidfModel(corpus)\n#corpus=tfidfModel[corpus]\nprint(\"Number Of Unique Tokens:\", len(dictionary))\nprint(\"Number Of Documents\", len(corpus))","56fa388d":"id2word={}\nfor word, idx in dictionary.token2id.items():\n    id2word[idx]=word","1ddbd886":"#ldaModel=LdaModel(corpus, num_topics=30,passes=30,iterations=500,eval_every=1,id2word=id2word)\n#gensimvis.prepare(ldaModel, corpus, dictionary)","423b015c":"import torch\nimport torch.nn as nn","76b78064":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, phase):\n        self.df=df\n        self.phase=phase\n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        X=torch.tensor(row.bin_distribution, dtype=torch.float32)\n        if self.phase=='train':\n            y=torch.tensor(row.target, dtype=torch.float32)\n            return  X, y\n        return X\n    def __len__(self):\n        return len(self.df)","4e377a3f":"class Model(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear=nn.Linear(in_features, out_features)\n    def forward(self, x):\n        out=self.linear(x)\n        return out","735e454c":"train_dataset=Dataset(train, phase='train')\ntrain_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=1000, shuffle=True)\n","5cae8337":"def train_epoch(model, optimizer, mse_loss):\n    epoch_loss=0.0\n    model.train()\n    for X, y in train_dataloader:\n        y_hat=model(X)\n        y_hat=torch.clip(y_hat, -3.6, 1.6)\n        loss=mse_loss(y_hat, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss+=loss.item()\n    epoch_loss\/=len(train_dataloader)\n    return epoch_loss","068ed25e":"models=[]\nfor i in range(7):\n    model=Model(3, 1)\n    mse_loss=torch.nn.MSELoss()\n    optimizer=torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n    loss=0.0\n    for e in range(120):\n        loss=train_epoch(model, optimizer, mse_loss)\n        if (e+1)%10==0:\n            print(\"Epoch:{} | Loss:{:.3f}\".format(e+1, loss))\n    print(\"Loss At the End of the Model Iteration {} is :{:.3f}\".format(i+1,loss))\n    models.append(model)","e78f7900":"test_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_df.head()","7db6dc60":"test_df['freq_dist'] = test_df.excerpt.apply(get_frequency_distribution)\ntest_df['bin_distribution']=test_df.freq_dist.apply(bin_distribution)\n\n\ntest_df.head()","69c406de":"test_dataset=Dataset(test_df, phase='test')\ntest_dataloader=torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1000)\n\npreds=[]\nfor X in test_dataloader:\n    batch_size=X.shape[0]\n    y=torch.zeros(batch_size)\n    for model in models:\n        model.eval()\n        with torch.no_grad():\n            y_hat=model(X)\n            y_hat=y_hat.view(-1)\n            \n            y+=y_hat\n    y\/=len(models)\n    preds += list(y.numpy())\n","d710f219":"test_df['target']=preds\ntest_df[['id', 'target']].to_csv('submission.csv', index=False)","95db2059":"test_df.head()","bc69a350":"# Modeling","57a3c157":"As Expected we can see from the line graph that \n 1. ease of reading got reduced as the proporation of rare words in the text increases\n 2. ease of reading got increase as the proporation of more common words increase in the text.","26229d89":"# Analysis of the topics with Topic Modelling LDA","8eb5210e":"1. long-tail distribution of words\n\n2. like Sparsity Exists till 14000 words and the frequency starts to raise gradually till 25000 and starts a sudden raise\n\n3. 75% of the words had frequency <= 5","c9ce10eb":"The Distributions of 3 bins are different\n\nDoes these contribute to the reading easy\n\nHypothesis is that:\n1. High Distribution of Rare words may be difficult to read and viceversa\n2. High Distribution of Common Words are easy to read and viceversa.","ed05b737":"bins:\nbin1 --> [0, 20]\n\nbin2 --> [21,50]\n\nbin3 --> [51, 100]\n\nbin4 --> [101, 500]\n\nbin5 --> [>500]","d89ed39b":"Target is following normal distribution with the values ranging between -3.5, 1.5\n\n50% of the passages have the reading ease of <= -0.9"}}