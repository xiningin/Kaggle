{"cell_type":{"1c3b6a97":"code","ed7911fa":"code","a2ff864d":"code","5e3d11de":"code","338980e3":"code","910fc5a5":"code","bdcb73a8":"code","b1fe038d":"code","655837e9":"code","b391e133":"code","b5d4d4d7":"code","6d87f3a8":"code","c073d38a":"code","21996dc3":"code","7249f24e":"code","39f1ea53":"code","dc4e2235":"code","a8bb29d4":"code","6c682cdc":"code","41de358e":"markdown","6d48708b":"markdown","fd9e30b3":"markdown","3544f22b":"markdown"},"source":{"1c3b6a97":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LSTM, MaxPooling1D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport re\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","ed7911fa":"pip install konlpy","a2ff864d":"train = pd.read_csv('..\/input\/boaz-nlp-competition\/train.csv')\ntest = pd.read_csv('..\/input\/boaz-nlp-competition\/test.csv')\nsub = pd.read_csv('..\/input\/boaz-nlp-competition\/sample_submission.csv')","5e3d11de":"train.head()","338980e3":"print('Train Data: ', train.shape)\nprint('Test Data: ', test.shape)","910fc5a5":"# \uac04\ub2e8\ud55c \ubd88\ud544\uc694 \ubb38\uc790, \ub2e8\uc5b4\ub4e4\uc744 \uc9c0\uc6cc\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4.\n\ndef cleaning_text(x):\n    x = re.sub('[^\\w\\s]', '', x)\n    x = re.sub('[\\rob\\r]', '', x)\n    return re.sub('[\u3131-\u314e\u314f-\u3163]', '', x)\n\ntrain['\uae30\uc5c5\ud55c\uc904\ud3c9'] = train['\uae30\uc5c5\ud55c\uc904\ud3c9'].apply(cleaning_text)\ntest['\uae30\uc5c5\ud55c\uc904\ud3c9'] = test['\uae30\uc5c5\ud55c\uc904\ud3c9'].apply(cleaning_text)","bdcb73a8":"# \ud1a0\ud06c\ub098\uc774\uc800\ub294 Okt\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\nimport konlpy.tag\nokt = konlpy.tag.Okt()","b1fe038d":"def tokenize(x):\n    return okt.morphs(x)","655837e9":"%%time\ntrain['text_token'] = train['\uae30\uc5c5\ud55c\uc904\ud3c9'].apply(tokenize)\ntest['text_token'] = test['\uae30\uc5c5\ud55c\uc904\ud3c9'].apply(tokenize)","b391e133":"# \uc784\ubca0\ub529\uc740 Fasttext\ub97c \uc774\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\nfrom gensim.models.fasttext import FastText","b5d4d4d7":"%%time\n\nft_model = FastText(train['text_token'],window = 3, min_count = 3, size = 200, iter = 100) ","6d87f3a8":"def vectorize_data(data, vocab: dict) -> list:\n    keys = list(vocab.keys())\n    filter_unknown = lambda word: vocab.get(word, None) is not None\n    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n    vectorized = list(map(encode, data))\n    return vectorized\n\ntrain_data = train['text_token']\ntest_data = test['text_token']\n\ninput_length = 128\ntrain_pad = pad_sequences(\n    sequences=vectorize_data(train_data, vocab=ft_model.wv.vocab),\n    maxlen=input_length, padding='post')\ntest_pad = pad_sequences(\n    sequences=vectorize_data(test_data, vocab=ft_model.wv.vocab),\n    maxlen=input_length, padding='post')","c073d38a":"X_train, X_test, y_train, y_test = train_test_split(\n    train_pad,\n    train['\ud3c9\uc810'],\n    test_size=0.2,\n    shuffle=True,\n    random_state=42)","21996dc3":"# \ubaa8\ub378\uc740 \ucf00\ub77c\uc2a4 \uae30\ubc18\uc758 \uae30\ubcf8\uc801\uc778 LSTM \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\ndef LSTM_model(embedding_matrix):\n    \n    input_text = Input(shape=(X_train.shape[1],))\n    \n    embedded = Embedding(\n        input_dim = embedding_matrix.shape[0],\n        output_dim = embedding_matrix.shape[1], \n        weights = [embedding_matrix])(input_text)\n    \n    LSTM_layer = LSTM(256, return_sequences = True)(embedded)\n    maxpooling1 = MaxPooling1D(2,None,padding='same')(LSTM_layer)\n    drop_out1 = Dropout(0.3)(maxpooling1)\n    flat = Flatten()(drop_out1)\n    dense1 = Dense(64, activation='relu')(flat)\n    drop_out2 = Dropout(0.3)(dense1)\n    dense2 = Dense(64, activation='relu')(flat)\n    drop_out3 = Dropout(0.3)(dense2)\n    dense3 = Dense(32, activation='relu')(drop_out3)\n    drop_out4 = Dropout(0.3)(dense3)\n    output = Dense(1, activation='sigmoid')(drop_out4)\n    \n    model = Model(inputs=input_text, outputs=output)\n    model.summary()\n    return model","7249f24e":"LSTM_model = LSTM_model(\n    embedding_matrix=ft_model.wv.vectors)","39f1ea53":"from keras.utils import plot_model\nplot_model(LSTM_model,show_shapes=True, show_layer_names=True)","dc4e2235":"early_stopping = EarlyStopping(monitor='val_loss', mode='min',patience=3)\n\nLSTM_model.compile(\n    Adam(1e-4),\n    loss=\"binary_crossentropy\",\n    metrics=['acc'])\n\nhistory = LSTM_model.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_test, y_test),\n    batch_size=128,\n    callbacks=[early_stopping],\n    epochs=10)","a8bb29d4":"pred = LSTM_model.predict([test_pad]) ","6c682cdc":"sub['\ud3c9\uc810'] = pred\nsub['\ud3c9\uc810'] = sub['\ud3c9\uc810'].apply(lambda x: 1 if x >= 0.5 else 0)\n\nsub.to_csv('Baseline_Submission.csv', index = False)","41de358e":"## Embedding","6d48708b":"## Modeling","fd9e30b3":"## Konlpy Tokenizing","3544f22b":"# 2021 \uaca8\uc6b8\ubc29\ud559 \uc790\uc5f0\uc5b4 \uc2a4\ud130\ub514 \uc2e4\uc2b5<br>\n\n### \uc7a1\ud50c\ub798\ub2db \uae30\uc5c5 \ub9ac\ubdf0\ub97c \ud06c\ub864\ub9c1\ud55c \ub370\uc774\ud130\ub85c \uc9c4\ud589\ud558\ub294 \uac10\uc131\ubd84\uc11d \ub300\ud68c\uc785\ub2c8\ub2e4! \ud83d\ude4c <br>\n\n### \uc2a4\ud130\ub514 \uc8fc\ucc28\ubcc4 \uacc4\ud68d \uc0c1 \uc6b0\uc218\uc791 \ubc1c\ud45c\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \ucd5c\uc885 \ucee4\ub110\uc744 \uacf5\uc720\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc9c4\ud589\ud560 \uc608\uc815\uc785\ub2c8\ub2e4! <br>\n\n### \uac04\ub2e8\ud55c Baseline \ucee4\ub110\uc785\ub2c8\ub2e4! <br>\n\n### EDA\ub85c \ud14d\uc2a4\ud2b8\uc640 \ub098\uba38\uc9c0 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ud2b9\uc9d5\uc744 \ud30c\uc545\ud558\uba74\uc11c \ubc30\uc6e0\ub358 \uac83\ub4e4\uc744 \uc2e4\uc2b5\ud574\ubcf4\uc2dc\uba74 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2f9 <br>"}}