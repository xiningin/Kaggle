{"cell_type":{"5bdda7c7":"code","a2e38061":"code","13f55de4":"code","2040c933":"code","b2d8d54d":"code","defd6992":"code","65196470":"code","3ef45cbf":"code","e66a35f2":"code","b0ab4bb6":"code","332e7ec1":"code","26207a66":"code","a02cf6ed":"code","80fbfc56":"code","b0f33168":"code","f3888790":"code","e29ff0f6":"code","49d355c1":"code","00eb14fe":"code","cd7547cf":"code","4b2cc067":"code","f6a49395":"markdown","48030820":"markdown","20e8bd2b":"markdown"},"source":{"5bdda7c7":"!pip install anyascii\n!pip install contractions","a2e38061":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom anyascii import anyascii\nfrom contractions import fix\n\nimport re\nimport nltk\nfrom nltk.tokenize import TweetTokenizer, word_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import TFBertModel\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(1)\ntf.random.set_seed(1)","13f55de4":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntrain_df.head()","2040c933":"train_df = train_df[['text', 'target']]\ntrain_df.head()","b2d8d54d":"train_df.isna().sum()","defd6992":"X, y = train_df.text.to_list(), train_df.target.values","65196470":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)","3ef45cbf":"def clean_text(text):\n    \n    # remove url links\n    pattern = r\"http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n    text = re.sub(pattern, \" \", text).strip() \n    \n    # remove tweeter handles\n    pattern = r\"@[\\w_]+\"\n    text = re.sub(pattern, \" \", text).strip()\n    \n    # replace flight codes\n    pattern = r\"[a-zA-Z]{2,}[0-9]+:?\"\n    text = re.sub(pattern, \" \", text).strip()\n    \n    # remove distance and times\n    pattern = r\"[0-9]+[a-zA-Z]{1,}\"\n    text = re.sub(pattern, \" \", text).strip()\n\n    # remove money and time\n    pattern = r\"\\$?[0-9]+:?\"\n    text = re.sub(pattern, \" \", text).strip()\n    \n    # remove ... or --\n    pattern = r\"[.]{2,}|[-]{2,}\"\n    text = re.sub(pattern, \" \", text).strip()\n    \n    # remove special handles\/??\/| patterns\n    pattern = r\"@[\\w']+|(\\|)|[?]{2,}|(\\\\n){1,}\"\n    text = re.sub(pattern, \" \", text).strip()\n    \n    # convert in ascii\n    text = anyascii(text)\n    \n    text = text.replace('\\n',' ').replace('U\/',' ').replace('U_',' ').strip()\n\n    # remove ampersand words\n    pattern = r\"&[\\w]+;?\"\n    text = re.sub(pattern, \" \", text).strip()\n    \n    # fix contractions\n    text = fix(text)\n\n    return text","e66a35f2":"X_train_cleaned = [clean_text(text) for text in X_train]\nX_test_cleaned = [clean_text(text) for text in X_test]","b0ab4bb6":"max_len = 70\n\nbert_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(bert_name, \n                                          add_special_tokens=True, \n                                          do_lower_case=True, \n                                          max_length=max_len, \n                                          pad_to_max_length=True)","332e7ec1":"def bert_encoder(text):\n#     text = text.numpy().decode('utf-8')\n    encoded = tokenizer.encode_plus(text, add_special_tokens=True,\n                                    max_length=max_len,\n                                    padding='max_length',\n                                    truncation=True,\n                                    return_attention_mask=True,\n                                    return_token_type_ids=True)\n    \n    return encoded['input_ids'], encoded['token_type_ids'], encoded['attention_mask']","26207a66":"train_bert = np.array([bert_encoder(text) for text in X_train_cleaned])\ntrain_label = np.array(y_train)\ntrain_label = keras.utils.to_categorical(train_label, num_classes=2).astype('int64')\n\ntest_bert = np.array([bert_encoder(text) for text in X_test_cleaned])\ntest_label = np.array(y_test)\ntest_label = keras.utils.to_categorical(test_label, num_classes=2).astype('int64')","a02cf6ed":"train_bert.shape","80fbfc56":"train_text, train_seg, train_masks = np.split(train_bert, 3, axis=1)\ntest_text, test_seg, test_masks = np.split(test_bert, 3, axis=1)\n\ntrain_text = train_text.squeeze()\ntrain_seg = train_seg.squeeze()\ntrain_masks = train_masks.squeeze()\n\ntest_text = test_text.squeeze()\ntest_seg = test_seg.squeeze()\ntest_masks = test_masks.squeeze()","b0f33168":"def map_features(input_ids, attention_masks, token_type_ids, y):\n    return {'input_ids':input_ids, 'attention_masks':attention_masks, 'token_type_ids':token_type_ids}, y","f3888790":"train_ds = tf.data.Dataset.from_tensor_slices((train_text, train_masks, train_seg, train_label))\ntrain_ds = train_ds.map(map_features).shuffle(100).batch(32)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((test_text, test_masks, test_seg, test_label))\ntest_ds = test_ds.map(map_features).batch(32)","e29ff0f6":"tf.random.set_seed(1)\n\nbert_model = TFBertForSequenceClassification.from_pretrained(bert_name)\n\noptimizer = keras.optimizers.Adam(learning_rate=2e-5)\nloss = keras.losses.BinaryCrossentropy(from_logits=True)\n\nbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\nbert_model.summary()","49d355c1":"history = bert_model.fit(train_ds, epochs=2, validation_data=test_ds)","00eb14fe":"sub_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\", usecols=['id', 'text'])\n# sub_df\n\nsub_texts = sub_df.text.to_list()\n\nsub_texts_cleaned = [clean_text(text) for text in sub_texts]\nsub_texts_seqs = np.array([bert_encoder(text) for text in sub_texts_cleaned])\n\n# sub_texts_seqs.shape\n\nsub_txt, sub_seg, sub_masks = np.split(sub_texts_seqs, 3, axis=1)\n\nsub_txt = sub_txt.squeeze()\nsub_masks = sub_masks.squeeze()\nsub_seg = sub_seg.squeeze()","cd7547cf":"predictions = bert_model.predict((sub_txt, sub_masks, sub_seg))\npredictions = predictions.logits.argmax(axis=1)\npredictions.shape","4b2cc067":"# save predictions in csv file\n\nsub_df['target'] = predictions\nsub_df.drop('text', 1, inplace=True)\nsub_df.to_csv(\"submission.csv\", index=False)","f6a49395":"## Using Pretrained Model","48030820":"## Cleaning Texts","20e8bd2b":"### Get Predictions"}}