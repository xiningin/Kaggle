{"cell_type":{"1f445eee":"code","70089ade":"code","2119fda7":"code","bbd55e99":"code","4d914cef":"code","ae81a89f":"code","1c84ca41":"code","95b61f3c":"code","14f08ae9":"code","c11c73da":"code","73bd9cbb":"code","2a64d385":"code","ec3d7336":"code","c3bc9803":"code","531a0659":"code","e3c1476d":"code","42529fa8":"code","5977096a":"code","6677a62d":"code","74baddf5":"code","ebd5c595":"code","c50e1bb4":"code","e96bfa2b":"code","44dfbb85":"code","2d3733ad":"code","bb835df6":"code","aa849c02":"code","2388230d":"code","1b562fe9":"code","30c332ab":"code","65dc50da":"code","bb9768d7":"code","9fecf457":"code","ada0337e":"code","06cc7508":"code","f676e684":"code","6989861d":"code","3c9e63b7":"code","465c165f":"code","928833e4":"code","78446322":"code","5e9430a5":"code","02a46703":"code","76108090":"code","16b5e577":"code","c3e75c32":"code","8887cdcb":"code","055b3d8d":"markdown","a5a11a0f":"markdown","34f93276":"markdown","d8b01954":"markdown","74e95d24":"markdown","7f708071":"markdown","bb5e4345":"markdown","6773f522":"markdown","c2fcbecc":"markdown","f56c6267":"markdown","eadcee53":"markdown","36ebfb52":"markdown","1c5ffad0":"markdown","84dcc252":"markdown","3f8e7b3e":"markdown","9a1d4edb":"markdown","32bdfc0b":"markdown","b32245ae":"markdown","7a805b2c":"markdown","2072abca":"markdown"},"source":{"1f445eee":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport matplotlib.pyplot as plt # data visualization library\n%matplotlib inline\nimport seaborn as sns\n\nimport re\nimport nltk\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score, average_precision_score, recall_score\n\n\nfrom nltk.tokenize import word_tokenize\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer #word stemmer class\nlemma = WordNetLemmatizer()\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk import FreqDist","70089ade":"#select 5000 rows\ndf = pd.read_csv('\/kaggle\/input\/vehicle\/vehicle.csv', nrows= 5000)\ndf.sample(5)","2119fda7":"words = set(nltk.corpus.words.words())","bbd55e99":"def normalizer(blogs):\n    blogs = \" \".join(filter(lambda x: x[0]!= '@' , blogs.split()))\n    blogs = re.sub('[^a-zA-Z]', ' ', blogs)\n    blogs = blogs.lower()\n    blogs = re.sub(' +', ' ', blogs).strip()\n    blogs = blogs.split()\n    blogs = [word for word in blogs if not word in set(stopwords.words('english'))]\n    blogs = [lemma.lemmatize(word) for word in blogs]\n    \n    blogs = \" \".join(blogs)\n    return blogs","4d914cef":"df['normalized_text'] = df.text.apply(normalizer)","ae81a89f":"df.head()","1c84ca41":"# Remove Non-English Words from Normalized text\ndef remove_non_english_words(blog):\n    return \" \".join(w for w in nltk.wordpunct_tokenize(blog) if w.lower() in words or not w.isalpha())\n\ndf['normalized_text'] = df.normalized_text.apply(remove_non_english_words)","95b61f3c":"df.head()","14f08ae9":"# all tweets \nall_words = \" \".join(df.normalized_text)","c11c73da":"wordcloud = WordCloud(height=2000, width=2000, stopwords=STOPWORDS, background_color='white')\nwordcloud = wordcloud.generate(all_words)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","73bd9cbb":"##  create another dataframe dfT having only the columns needed for creating label\ndfT=df[['gender', 'age', 'topic', 'sign']]","2a64d385":"## Convert age from int type into String\ndfT['age']=dfT['age'].astype('str')","ec3d7336":"## Create a 2D Matrix 'm' which is list of list contaning 'gender', 'age', 'topic', 'sign' for each row\nm=[]                              # 2D Matrix having list of list\nfor i in range(dfT.shape[0]):\n    g=[]                          # 1D list of 'gender', 'age', 'topic', 'sign'\n    for j in range(dfT.shape[1]):\n        g.append(dfT.iloc[i][j])\n    m.append(g)","c3bc9803":"#Add a column called labels\ndf['labels']=m","531a0659":"df.head()","e3c1476d":"final_df = df[['normalized_text', 'labels']]","42529fa8":"final_df.head()","5977096a":"# Lets Check Distribution of Labels\nfinal_df['labels'].astype('str').value_counts()","6677a62d":"## Check for Null Values\nfinal_df.isna().sum()","74baddf5":"# No Null Values","ebd5c595":"X = final_df['normalized_text']\ny = final_df['labels']","c50e1bb4":"X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.25)","e96bfa2b":"# Consider only those rows which occur more than 15% and less than 80 %, also restrict features to 100\n\nvectorizer = CountVectorizer(ngram_range = (1,2), stop_words=stopwords.words('english'), \n                             min_df = 0.15, max_df = 0.8, max_features = 100)","44dfbb85":"# transform the X data to document_term_matrix\n\nX_train_dtm = vectorizer.fit_transform(X_train)\nX_test_dtm = vectorizer.transform(X_test)\nX_train_dtm","2d3733ad":"# check the vocabulary( First 15 features)\nvectorizer.get_feature_names()[:10]","bb835df6":"\nprint(X_train_dtm )","aa849c02":"# examine vocabulary and document term matrix together\npd.DataFrame(X_train_dtm.toarray(), columns = vectorizer.get_feature_names())\n","2388230d":"\nprint(X_train_dtm )","1b562fe9":"# examine vocabulary and document term matrix together\npd.DataFrame(X_test_dtm.toarray(), columns = vectorizer.get_feature_names())","30c332ab":"dfT = df[['gender', 'age', 'topic', 'sign']]","65dc50da":"dfT['age'] = dfT['age'].astype('str')","bb9768d7":"keys=[] \nvalues=[] \n\nfor i in range(dfT.shape[1]): # iterate through all the colummns        \n    for j in range(dfT.iloc[:,i].value_counts().shape[0]): # iterate through all the rows of value_counts of that column\n        keys.append(dfT.iloc[:,i].value_counts().index[j])         \n        values.append(dfT.iloc[:,i].value_counts().iloc[j])","9fecf457":"dictionary = dict(zip(keys,values))","ada0337e":"print(dictionary)","06cc7508":"from sklearn.preprocessing import MultiLabelBinarizer \nmlb = MultiLabelBinarizer(classes=sorted(dictionary.keys()))\ny_train_mlb = mlb.fit_transform(y_train)\ny_test_mlb = mlb.transform(y_test)","f676e684":"y_train_mlb[0]","6989861d":"y_test_mlb[0]","3c9e63b7":"y_train.iloc[1]","465c165f":"mlb.inverse_transform(y_train_mlb)[1]","928833e4":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver='lbfgs', multi_class='ovr')\novr = OneVsRestClassifier(lr)\n\novr.fit(X_train_dtm, y_train_mlb)\ny_pred_ovr_test = ovr.predict(X_test_dtm)\n#y_proba_ovr = ovr.predict_proba(X_test_dtm)\ny_pred_ovr_test","78446322":"y_pred_ovr_train = ovr.predict(X_train_dtm)\ny_pred_ovr_train","5e9430a5":"def print_scores(actual, predicted, averaging_type):\n    print('\\nAVERAGING TYPE==> ',averaging_type)\n    print('F1 score: ',f1_score(actual,predicted, average=averaging_type))\n    print('Average Precision Score: ',average_precision_score(actual,predicted, average=averaging_type))\n    print('Average Recall Score: ',recall_score(actual,predicted, average=averaging_type))","02a46703":"print('--------------------------TRAIN SCORES--------------------------------')\nprint('Accuracy score: ',accuracy_score(y_train_mlb, y_pred_ovr_train))\nprint_scores(y_train_mlb, y_pred_ovr_train, 'micro')\nprint_scores(y_train_mlb, y_pred_ovr_train, 'macro')\nprint_scores(y_train_mlb, y_pred_ovr_train, 'weighted')","76108090":"print('--------------------------TEST SCORES--------------------------------')\nprint('Accuracy score: ',accuracy_score(y_test_mlb, y_pred_ovr_test))\nprint_scores(y_test_mlb, y_pred_ovr_test, 'micro')\nprint_scores(y_test_mlb, y_pred_ovr_test, 'macro')\nprint_scores(y_test_mlb, y_pred_ovr_test, 'weighted')","16b5e577":"five_pred = y_pred_ovr_test[:5]\nfive_actual = y_test_mlb[:5]","c3e75c32":"five_actual = mlb.inverse_transform(five_actual)\nfive_actual","8887cdcb":"five_pred = mlb.inverse_transform(five_pred)\nfive_pred","055b3d8d":"# 1. Load the dataset ","a5a11a0f":"##### Test Document Term Matrix","34f93276":"#### a. Label columns to merge: \u201cgender\u201d, \u201cage\u201d, \u201ctopic\u201d, \u201csign\u201d","d8b01954":"# 7. Transform the labels - (7.5 points)\nAs we have noticed before, in this task each example can have multiple tags. To deal with\nsuch kind of prediction, we need to transform labels in a binary form and the prediction will be\na mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\n\n#### a. Convert your train and test labels using MultiLabelBinarizer","74e95d24":"# Create a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label. Check below image for reference (5 points)","7f708071":"# 2. Preprocess rows of the \u201ctext\u201d column \n#### a. Remove unwanted characters\n#### b. Convert text to lowercase\n#### c. Remove unwanted spaces\n#### d. Remove stopwords","bb5e4345":"##### b. Print the term-document matrix","6773f522":"##### Result as expected","c2fcbecc":"##### Train Score","f56c6267":"##### Train Document Term Matrix","eadcee53":"#### b. After completing the previous step, there should be only two columns in your data frame i.e. \u201ctext\u201d and \u201clabels\u201d as shown in the below image","36ebfb52":"# 3. As we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points)","1c5ffad0":"# 5. Vectorize the features \n#### a. Create a Bag of Words using count vectorizer\n#### i. Use ngram_range=(1, 2)\n##### ii. Vectorize training and testing features\n","84dcc252":"# 10. Print true label and predicted label for any five examples","3f8e7b3e":"# 8. Choose a classifier \n","9a1d4edb":"# 9. Fit the classifier, make predictions and get the accuracy \n","32bdfc0b":"##### Test Scores","b32245ae":"##### Lets verify one single row of train set after MLB conversion","7a805b2c":"####  Word Cloud of all the normlized text","2072abca":"# 4. Separate features and labels, and split the data into training and testing "}}