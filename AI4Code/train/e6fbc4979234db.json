{"cell_type":{"19082e42":"code","f57cdcc8":"code","4c06d3b4":"code","8ef9ea9c":"code","fa883913":"code","52a8d049":"code","e8b75b19":"code","b756ea59":"code","5ca54590":"code","8be0831d":"code","152486f5":"code","5dbc3338":"code","1828fca7":"code","f5dba6be":"code","30888e44":"code","9267a633":"code","f150af72":"code","0e5ff326":"code","d62f7368":"code","d98328ab":"code","0dac865a":"code","4d86aee6":"code","eb437e2e":"code","c026bb3a":"code","9a83ae1d":"code","53635033":"code","3cf87efa":"code","e2a6cb7a":"code","bc58e129":"code","d482b366":"code","02dca1f5":"code","8c1b0175":"code","55622527":"code","5e5f76fc":"code","dc2ba7e0":"code","a0550fae":"code","9cbee36a":"code","1b4e29ad":"code","98407d33":"markdown","fecf2045":"markdown","7a9e212f":"markdown","5f27c66a":"markdown","75c51474":"markdown","f6e77261":"markdown","92d3039c":"markdown","5835d691":"markdown","fc6a03d7":"markdown","51598baf":"markdown","e1bf6fb2":"markdown","1223cd30":"markdown","a72797cd":"markdown","1e0e180e":"markdown","1be9ae07":"markdown","431f772c":"markdown","712f7e07":"markdown","59b43ec7":"markdown","58469c8f":"markdown","e7fa20f1":"markdown","8b0adc4a":"markdown","f2d90217":"markdown","4841769d":"markdown","7841987b":"markdown","a53ecd30":"markdown","5cb151a4":"markdown","c1d916ae":"markdown","03d0f064":"markdown"},"source":{"19082e42":"# required pacakges to be imported\nimport pandas as pd\nimport matplotlib as plt\nimport numpy as np\nfrom pandas import DataFrame\nfrom datetime import date\nfrom matplotlib import style\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize']=10,6\nimport math\n# importing the dataset from local storage\n#data = pd.read_csv(\"C:\\\\Users\\\\KANDIRAJU\\\\Downloads\\\\time-series-datasets\\\\Electric_Production.csv\")\n#data.head()\ndata = pd.read_csv(\"..\/input\/Electric_Production.csv\")\ndata.head()","f57cdcc8":"# renaming the column names as per my convenience ( this is optional if you wish to perform) \ndata.rename(columns = {'DATE' : 'date', 'Value' : 'value'}, inplace=True)\ndata.head()","4c06d3b4":"# to plot a graph, index has to be set. it is not possible to plot the graph without index.\ndata.set_index('date', inplace=True)\ndata.head()\nplt.xlabel(\"date\")\nplt.ylabel(\"value\")\nplt.title(\"production graph\")\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\nplt.plot(data); \n# we will notice that the x axis is messed up, this is because, it plotted all the date points and the numbers got overlapped.","8ef9ea9c":"rolmean = data.rolling(window=12).mean()\nprint(rolmean.head(20))","fa883913":"std = data.rolling(window=12).std()\nprint(std.head(20))","52a8d049":"# plot rolling statistics\noriginal_data = plt.plot(data, color='blue',label='original data')\nmean = plt.plot(rolmean,color ='red',label='rolling mean')\nstd = plt.plot(std,color ='black',label='standard deviation')\nplt.title(\"mean, std & original data\")\nplt.xlabel(\"date\")\nplt.ylabel(\"value\")\nplt.legend()\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\nplt.show(block =False)","e8b75b19":"# perform dickey fuller test (ADFT)\nfrom statsmodels.tsa.stattools import adfuller \nadft = adfuller(data['value'],autolag='AIC')\n# output for dft will give us without defining what the values are.\n#hence we manually write what values does it explains using a for loop\noutput = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])\nfor key,values in adft[4].items():\n    output['critical value (%s)'%key] =  values\nprint(output)","b756ea59":"data_logscale = np.log(data)\n# logarithmic function is used to scale the data to a certain extent.\nplt.plot(data_logscale)\nplt.title(\"Log scale\")\nplt.xlabel(\"date\")\nplt.ylabel(\"value\")\n#plt.legend()\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\nplt.show(block =False)","5ca54590":"#determining the rolling mean(average) for the log data. Perform the same steps which are performed on the data before.\nmoving_average = data_logscale.rolling(window=12).mean()\n#print(rolmean_log)\n#determining the standard deviation ( same steps! )\nstd_dev = data_logscale.rolling(window=12).std()\n#print(std_log)\nplt.plot(moving_average, color='red')\nplt.plot(data_logscale, color='blue')\nplt.plot(std_dev, color='black');","8be0831d":"movingavg_logscale = data_logscale - moving_average\nmovingavg_logscale.head(15)","152486f5":"movingavg_logscale.dropna(inplace=True)\nmovingavg_logscale.head(10)","5dbc3338":"\nfrom statsmodels.tsa.stattools import adfuller \ndef test_stationarity(timeseries):\n    \n    #determining the rolling statistics for timeseries\n    \n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    #plotting the rolling statistics for timeseries\n    \n    timeseries_original =plt.plot(timeseries, color='blue',label = 'original graph')\n    timeseries_mean =plt.plot(movingAverage, color='red',label = 'movingAverage')\n    timeseries_std =plt.plot(movingSTD, color='black',label = 'movingSTD')\n    plt.legend(loc='best')\n    plt.title(\"rolling mean & standard deviation of timeseries\")\n    plt.show(block=False)\n    \n    #perform dickey fuller test\n    \n    print(\"results of dickey fuller test\")\n    adft = adfuller(data['value'],autolag='AIC')\n    \n    # output for dft will give us without defining what the values are.\n    #hence we manually write what values does it explains using a for loop\n    \n    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])\n    for key,values in adft[4].items():\n        output['critical value (%s)'%key] =  values\n    print(output)\n    ","1828fca7":"from pylab import rcParams\nrcParams['figure.figsize'] = 10,6\n# we are going to use the function here.\ntest_stationarity(movingavg_logscale)","f5dba6be":"print(data_logscale.head())","30888e44":"weighted_average = data_logscale.ewm(halflife=12, min_periods=0,adjust=True).mean()\nprint(weighted_average.head())","9267a633":"\nplt.plot(data_logscale)\nplt.plot(weighted_average, color='red')\nplt.xlabel(\"date\")\nplt.ylabel(\"value\")\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\n#plt.legend()\nplt.show(block =False)","f150af72":"logScale_weightedMean = data_logscale-weighted_average\n# use the same function defined above and pass the object into it.\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\ntest_stationarity(logScale_weightedMean)","0e5ff326":"data_log_shift = data_logscale - data_logscale.shift()\nplt.xlabel(\"date\")\nplt.ylabel(\"value\")\nplt.title(\"shifted timeseries\")\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\nplt.plot(data_log_shift)","d62f7368":"# We are dropping the NaN values, and the data_log_shift value here is 'd'\ndata_log_shift.dropna(inplace=True)\n# using the same fuction call and plotting the graph.\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\ntest_stationarity(data_log_shift)","d98328ab":"# next is to segregate the differentiated values to decompose, we use seasonal decompose method from stats model.\n# !pip install statsmodels\n# !pip install --upgrade patsy\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n#decomposition = seasonal_decompose(data_logscale,model='additive', freq=12).plot()\n\n# plotting the graphs induvidually\ndecomposition = seasonal_decompose(data_logscale,model='additive', freq=12)\ntrend = decomposition.trend\nseasonality =decomposition.seasonal\n# ensure that the residual method is just \" resid \"\n# check the values inside the subplots are 411,412,413,414 which mean, there are 4 graphs in total(1st number in the value)\nresidual =decomposition.resid\nplt.subplot(411)\nplt.plot(data_logscale,label= 'original')\nplt.legend(loc='best')\nplt.plot()\nplt.subplot(412)\nplt.plot(trend,label= 'trend')\nplt.legend(loc='best')\nplt.plot()\nplt.subplot(413)\nplt.plot(seasonality,label= 'seasonality')\nplt.legend(loc='best')\nplt.plot()\nplt.subplot(414)\nplt.plot(residual,label= 'residual')\nplt.legend(loc='best')\nplt.plot()\nplt.tight_layout()\n\n","0dac865a":"decomposed_logdata = residual\ndecomposed_logdata.dropna(inplace=True)\ntest_stationarity(decomposed_logdata)\n","4d86aee6":"# plot acf  and pacf graphs ( auto corellation function and partially auto corellation function )\n# to find 'p' from p,d,q we need to use, PACF graphs and for 'q' use ACF graph\nfrom statsmodels.tsa.stattools import acf,pacf\n# we use d value here(data_log_shift)\nacf = acf(data_log_shift, nlags=15)\npacf= pacf(data_log_shift, nlags=15,method='ols')\n\n# ols stands for ordinary least squares used to minimise the errors\n\n# 121 and 122 makes the data to look side by size \n\n#plot PACF\nplt.subplot(121)\nplt.plot(acf) \nplt.axhline(y=0,linestyle='-',color='blue')\nplt.axhline(y=-1.96\/np.sqrt(len(data_log_shift)),linestyle='--',color='black')\nplt.axhline(y=1.96\/np.sqrt(len(data_log_shift)),linestyle='--',color='black')\nplt.title('Auto corellation function')\nplt.tight_layout()\n\n\n#plot ACF\nplt.subplot(122)\nplt.plot(pacf) \nplt.axhline(y=0,linestyle='-',color='blue')\nplt.axhline(y=-1.96\/np.sqrt(len(data_log_shift)),linestyle='--',color='black')\nplt.axhline(y=1.96\/np.sqrt(len(data_log_shift)),linestyle='--',color='black')\nplt.title('Partially auto corellation function')\nplt.tight_layout()\n\n","eb437e2e":"# in order to find the p and q values from the above graphs,\n#  we need to check,where the graph cuts off the origin or drops to zero for the first time\n# from the above graphs the p and q values arw merely close to 2 where the graph cuts off the orgin ( draw the line to x axis)\n# now we have p,d,q values. So now we can substitute in the ARIMA model and lets see the output.","c026bb3a":"from statsmodels.tsa.arima_model import ARIMA\n\n# calculating the AR model\nmodel = ARIMA(data_logscale, order =(2,1,0))\n# consider MA as 0 in MA_model\nAR_result = model.fit()\nplt.plot(data_log_shift)\nplt.plot(AR_result.fittedvalues, color='red')\nplt.title(\"sum of squares of residuals\")\nprint('RSS : %f' %sum((AR_result.fittedvalues-data_log_shift[\"value\"])**2))","9a83ae1d":"# less the RSS more effective the model is","53635033":"# calculating the MA model\nmodel = ARIMA(data_logscale, order =(2,1,0))\n# consider MA as 0 in MA_model\nMA_result = model.fit()\nplt.plot(data_log_shift)\nplt.plot(MA_result.fittedvalues, color='red')\nplt.title(\"sum of squares of residuals\")\nprint('RSS : %f' %sum((MA_result.fittedvalues-data_log_shift[\"value\"])**2))\n","3cf87efa":"# There is no need of finding the AR and MA values, this is just for our referrence, we already know the values of p,d,q\n# you can simply plot the ARIMA model and check for the results.\n","e2a6cb7a":"# calculating the ARIMA model\nmodel = ARIMA(data_logscale, order =(3,1,3))\nARIMA_result = model.fit()\nplt.plot(data_log_shift)\nplt.plot(ARIMA_result.fittedvalues, color='red')\nplt.title(\"sum of squares of residuals\")\nprint('RSS : %f' %sum((ARIMA_result.fittedvalues-data_log_shift[\"value\"])**2))","bc58e129":"# we founded the predicted values in the above code and we need to print the values in the form of series\nARIMA_predicts = pd.Series(ARIMA_result.fittedvalues,copy=True)\nARIMA_predicts.head()","d482b366":"# finding the cummulative sum\nARIMA_predicts_cumsum = ARIMA_predicts.cumsum()\nprint(ARIMA_predicts_cumsum.head())","02dca1f5":"ARIMA_predicts_log = pd.Series(data_logscale['value'],index =data_logscale.index)\nARIMA_predicts_log = ARIMA_predicts_log.add(ARIMA_predicts_cumsum,fill_value=0)\nprint(ARIMA_predicts_log.head())","8c1b0175":"# converting back to the exponential form results in getting back to the original data.\nARIMA_final_preditcs = np.exp(ARIMA_predicts_log)\nrcParams['figure.figsize']=10,10\nplt.plot(data)\nplt.plot(ARIMA_predicts_cumsum)","55622527":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize']=10,10\nplt.plot(ARIMA_predicts_cumsum)\nplt.plot(data)\n","5e5f76fc":"#future prediction\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize']=15,10\nARIMA_result.plot_predict(1,500)\nx=ARIMA_result.forecast(steps=200)","dc2ba7e0":"# from the above graph, we calculated the future predictions till 2024\n# the greyed out area is the confidence interval wthe predictions will not cross that area.\n","a0550fae":"# check the predicted values for ARIMA_result.plot_predict(1,500)\nARIMA_result.forecast(steps=200)","9cbee36a":"# Finally we calculated the units(value) of electricity is consumed in the coming future using time series analysis.","1b4e29ad":"# **********                                 THE END                                                        **************** #","98407d33":"\nThe Dickey Fuller test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. The null and alternate hypothesis of this test are:\n\n**Null Hypothesis:** The series has a unit root (value of a =1)\n\n**Alternate Hypothesis:** The series has no unit root.\n\nIf we fail to reject the null hypothesis, we can say that the series is non-stationary. This means that the series can be linear or difference stationary (we will understand more about difference stationary in the next section).","fecf2045":"We see that the p value is greater than 0.05 so we cannot reject the **Null hypothesis**. Also the test statistics is greater  than the critical values. so the data is non stationary","7a9e212f":"We see the spread of the data evenly compaered to the mean of the data. The scale on the y axis changed as we have taken the log of it. But still data is not stationary","5f27c66a":"# ARIMA Model","75c51474":"**ARIMA**, short for **\u2018Auto Regressive Integrated Moving Average\u2019** is actually a class of models that \u2018explains\u2019 a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.\n\nAny \u2018**non-seasonal**\u2019 time series that exhibits patterns and is not a random white noise can be modeled with **ARIMA** models.\n\nAn **ARIMA** model is characterized by 3 terms: p, d, q\n\nwhere,\n\n**AR:** Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n\n**I:** Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n\n**MA:** Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. More precisely, it \n\nIf a time series, has seasonal patterns, then you need to add seasonal terms and it becomes SARIMA, short for \u2018Seasonal ARIMA\u2019. More on that once we finish **ARIMA**.","f6e77261":"The reason why the window is considered as 12 beacuse, we have to calculate the mean for every one year which is for 12 months from this data we see that we have only the first month data of every yearto make it as an year, we consider all the 1st month of every year and count them to 12 months which makes one year.\n\nCalculation of rolling mean : since the window is 12 ( 12 months ) the values of top 11 months are considered and you will get NaN values for the top 12.","92d3039c":"Perform dickey fuller test (ADFT) once again. This is the actual code for dickey fuller test. We have to perform this function everytime to check whether the data is stationary or not.","5835d691":"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. Time series are widely used for non-stationary data, like economic, weather, stock price, and retail sales in this post. We will demonstrate different approaches for forecasting electric consumption time series. \n","fc6a03d7":"From the above graph we observed that the data attained stationartiy. We also see that the test statistics and critial value is relatively equal","51598baf":"**Pandas** : Pandas is an open source library, providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. What\u2019s cool about Pandas is that,  it takes data (like a CSV or TSV file, or a SQL database) and creates a Python object with rows and columns called Data frame that looks very similar to table in a statistical software (think Excel or SPSS for example).\n\n**Numpy** : Numpy is a general-purpose array-processing package. It provides a high-performance multidimensional array object, and tools for working with these arrays. It is the fundamental package for scientific computing with Python.\n\n**Matplotlib** : matplotlib.pyplot is a plotting library used for 2D graphics in python programming language. It can be used in python scripts, shell, web application servers and other graphical user interface toolkits. It supports a very wide variety of graphs and plots namely - histogram, bar charts, power spectra, error charts etc. It is used along with NumPy to provide an environment that is an effective open source alternative for MatLab.\n\n\n\n","e1bf6fb2":"Rolling mean or avergae is a calculation to analyze data points by creating a series of averages of different subsets of the full data set. It is also called a moving mean ","1223cd30":"**Determining the rolling mean\n**\n","a72797cd":"Over here, the time series is shifted by 1, which means, the time is differentiated by 1 ( finding the 'd' value). In ARIMA model, I stands for integration and that's what we calculated here.","1e0e180e":"Now substrcting the log scale with the moving average. There is nothing specific to perform this subtraction. You can even try by changing the logarithmic function.\nAs the data is not stationary, this is one of the steps to perform the test to ensure that the data is stationary\n\n\n","1be9ae07":"We define a function to test the stationarity with respect to time, hence we take timeseries. The reason to define it in the form of a function is because, this function can be used in the coming ADCF tests as well. It becomes easy to use the same function and check the stationarity rather then writing the code again and again.","431f772c":"## **ADF (Augmented Dickey Fuller) Test**\n","712f7e07":"**Determining the standard deviation**\n\nStandard deviation is the measure of dispersion of a set of data from its mean. It measures the absolute variability of a distribution\n","59b43ec7":"**Stationarity : An important module in time series analyis. **","58469c8f":"## **Time series Analysis **\n\n1. A time series is a sequence where a metric is recorded over regular time intervals.\n2. A time series (x\u2081, \u2026, x\u2091) is assumed to be a sequence of real values taken at successive equally spaced points in time, from time t=1 to time t=e.\n3. Depending on the frequency, a time series can be of yearly (ex: annual budget), quarterly (ex: expenses), monthly (ex: air traffic), weekly (ex: sales qty), daily (ex: weather), hourly (ex: stocks price), minutes (ex: inbound calls in a call canter) and even seconds wise (ex: web traffic).\n4. Forecasting is the final step where you want to predict the future values the series is going to take.\n\n\nFor a deailed information on Time series, you can go through this link.\n\nhttps:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/","e7fa20f1":"Previously we subtracted data_logscale with moving average, now take the same log_scale and subtract with weighted_average\n","8b0adc4a":"Previously if we observe, the mean and std has high values and hence there was a spread in the data, now as we have sclaed the data, the values are relatively to the same scale. Let's drop the NaN values which make the data noise.","f2d90217":"As discussued above, we got some NaN values, this is because, as we are claculating the mean for every 12 months Hence it takes the values of first 12 months and that is the reason why the first 11 months are NaN. It depends on how many months you want to calculate. Based on that, you can take the window size.","4841769d":"We see that the data is not stationary, in order to check the stationarity, perform dickey fuller test. Before doing that, let us understand what Stationarity is all about.","7841987b":"After defining the function, use the movingavg_logscale which is subtracted from 2 values in the above cell. You can use any function call by taking the difference or the using different log to remove the stationarity","a53ecd30":"The exponential moving average (EMA) is a weighted average of the last n prices, where the weighting decreases exponentially with each previous price\/period. In other words, the formula gives recent prices more weight than past prices.\n\n","5cb151a4":"A time series has stationarity if a shift in time doesn\u2019t cause a change in the shape of the distribution. Basic properties of the distribution like the mean , variance and covariance are constant over time. Stationarity can be defined in precise mathematical terms, but for our purpose we mean a flat looking series, without trend, constant variance over time, a constant autocorrelation structure over time and no periodic fluctuations (seasonality).\n","c1d916ae":"For further information on ADCF, you can refer this article.\n\nhttps:\/\/www.stata.com\/manuals13\/tsdfuller.pdf","03d0f064":"One of the module is completed as we came to a conclusion. We need to check the weighted average, to understand the trend of the data in timeseries. Take the previous log data nd perform the following operation."}}