{"cell_type":{"2b65360e":"code","08083f12":"code","5b44aa76":"code","63c95913":"code","fa7c5eeb":"code","5ea9d441":"code","5cda8cad":"code","a9048bc1":"code","1c917816":"code","4dd0da86":"code","30349062":"code","20fef0bb":"code","6b3ea54d":"code","f0cad588":"code","36c6fa72":"code","aa89f0bd":"code","3fac8b0d":"code","6f636c47":"code","f3bae975":"code","4328e1e2":"code","a86eb415":"code","d3c7c840":"code","5d477281":"code","00a12dc2":"code","9ed9ea27":"code","d5375740":"code","da0c228a":"code","7a5cbc34":"code","29508f0e":"code","6a696422":"code","c3989cca":"code","57f3fb9e":"code","810a680c":"markdown","0a6ca599":"markdown","9b178dc4":"markdown","e2eef0ff":"markdown"},"source":{"2b65360e":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","08083f12":"# Accuring Data\ny = pd.read_csv('..\/input\/titanic1\/train.csv')\nX = pd.read_csv('..\/input\/titanic1\/test.csv')","5b44aa76":"# Analyze by describing data\ny.info()","63c95913":"y.describe()","fa7c5eeb":"y.head(8)","5ea9d441":"total = y.isnull().sum().sort_values(ascending=False)\npercent_1 = y.isnull().sum()\/y.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","5cda8cad":"y.columns.values","a9048bc1":"# SibSp and Parch\ndata = [y,X]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ny['not_alone'].value_counts()","1c917816":"axes = sns.factorplot('relatives','Survived',data=y, aspect = 2.5, )","4dd0da86":"# Missing Data \"Cabin\"\n\nimport re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [y,X]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature\ny = y.drop(['Cabin'], axis=1)\nX = X.drop(['Cabin'], axis=1)","30349062":"# \"Age\"\ndata = [y, X]\n\nfor dataset in data:\n    mean = y[\"Age\"].mean()\n    std = X[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = y[\"Age\"].astype(int)\ny[\"Age\"].isnull().sum()","20fef0bb":"# \"Embarked\"\ny['Embarked'].describe()","6b3ea54d":"common_value = 'S'\ndata = [y, X]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","f0cad588":"# Converting Feature\ny.info()","36c6fa72":"# \"Name\"\ndata = [y, X]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ny = y.drop(['Name'], axis=1)\nX =X.drop(['Name'], axis=1)","aa89f0bd":"# \"Gender\"\ngenders = {\"male\": 0, \"female\": 1}\ndata = [y, X]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","3fac8b0d":"# \"Fare\"\ndata = [y, X]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","6f636c47":"# \"Ticket\"\ny = y.drop(['Ticket'], axis=1)\nX = X.drop(['Ticket'], axis=1)","f3bae975":"# \"Emberked\"\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [y,X]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","4328e1e2":"# \"Age\"\ndata = [y,X]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed \ny['Age'].value_counts()","a86eb415":"y.head()","d3c7c840":"data = [y,X]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","5d477281":"# Creating New Feature \"Age Times Class\"\ndata = [y,X]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n\n# \"Fare Per Person\"\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n\ny.head(10)","00a12dc2":"# ML Model\nX_train = y.drop(\"Survived\", axis=1)\nY_train = y[\"Survived\"]\nX_test  = X.drop(\"PassengerId\", axis=1).copy()","9ed9ea27":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth=2, min_samples_leaf=5) \ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_train)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)","d5375740":"# Linear Support Vector Machine\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_train)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n","da0c228a":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_train)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","7a5cbc34":"results = pd.DataFrame({\n    'Model': ['Decision Tree', 'Logistic Regression','Support Vector Machines','Random Forest'],\n    'Score': [acc_decision_tree,acc_log,acc_linear_svc,acc_random_forest]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","29508f0e":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","6a696422":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","c3989cca":"# AdaBoost Classification\nfrom sklearn import model_selection\nfrom sklearn.ensemble import AdaBoostClassifier","57f3fb9e":"seed = 7\nnum_trees = 30\n\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n# create the sub models\nestimators = []\n\nmodel1 = LogisticRegression()\nestimators.append(('logreg', model1))\n\nmodel2 = DecisionTreeClassifier()\nestimators.append(('decision_tree', model2))\n\nmodel3 = LinearSVC()\nestimators.append(('linear_svc', model3))\n\nmodel4 = RandomForestClassifier()\nestimators.append(('random_forest',model4))\n\n# create the ensemble model\nensemble = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(ensemble, X_train, Y_train, cv=kfold)\n\nprint(results)\nprint(\"max: \", results.max())\nprint(\"min: \", results.min())\nprint(\"mean: \", results.mean())\n\n# We get a mean estimate of classification accuracy.\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (results.mean(), results.std() * 2))","810a680c":"seed = 7\nnum_trees = 30\n\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold)","0a6ca599":"# Data PreProcessing","9b178dc4":"2. Use the titanic dataset you used in Q1. Now use AdaBoost ensemble method to create classification model. Compare the accuracy of this model with the ensemble model obtained in Q1.","e2eef0ff":"# 1. Use titanic dataset. Do the required pre-processing in order to clean your data and scale the data in a range. Your task is to predict the survival chance of the passengers boarded on ship. Build the ensemble model using decision tree, logistic regressor and support vector machine classifiers. Make the final prediction by combining the output of all three classifiers together. Analyze the prediction ability of your model"}}