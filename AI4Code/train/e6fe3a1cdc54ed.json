{"cell_type":{"c1805608":"code","24eb9122":"code","21a296c1":"code","2a6d796e":"code","4255ff6a":"code","146b7e70":"code","8002afa7":"code","3921d8c9":"code","c90f0718":"code","41fabef2":"code","ff1c5813":"code","34a2bbbd":"code","23babf1a":"code","99b31a0a":"code","d026ecc7":"code","52915d7d":"code","304b09a3":"code","b35b64f5":"code","b6dd38e1":"code","9ca7d1f5":"code","15262c18":"code","ffd82fdc":"code","1fd6b9ee":"code","8055b7c7":"code","c4fa6410":"code","f4573b22":"code","290d8b89":"code","7928b6d8":"code","27501a4b":"code","a6066301":"code","dc3b60b3":"code","c8e88314":"code","f9d8b76d":"code","9193e055":"code","8f0e937b":"code","8f10af0d":"markdown","0d7571f7":"markdown","0274b386":"markdown","0ac9882b":"markdown","2b86c06d":"markdown","82b6a528":"markdown","531a41fc":"markdown","96867705":"markdown","0209db24":"markdown","d59bea3c":"markdown","3069fe0e":"markdown","29b0f983":"markdown","dfb85120":"markdown","906786f2":"markdown","a5790466":"markdown","c8c25cff":"markdown","874db7a1":"markdown","def1ef39":"markdown"},"source":{"c1805608":"pip install -U lightautoml","24eb9122":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport time\nimport random\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import LocalOutlierFactor\n\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21a296c1":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\", low_memory=False)#, nrows=10000)\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","2a6d796e":"test.info(memory_usage=\"deep\")","4255ff6a":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","146b7e70":"fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(18, 8), gridspec_kw={'width_ratios': [2, 1]})\n\nbars = axs[0].bar(train[\"target\"].value_counts().sort_index().index,\n                  train[\"target\"].value_counts().sort_index().values,\n                  color=colors,\n                  edgecolor=\"black\")\naxs[0].set_title(\"Target distribution\", fontsize=20, pad=15)\naxs[0].set_ylabel(\"Count\", fontsize=14, labelpad=15)\naxs[0].set_xlabel(\"Target label\", fontsize=14, labelpad=10)\naxs[0].bar_label(bars, train[\"target\"].value_counts().sort_index().values,\n                 padding=3, fontsize=12)\naxs[0].bar_label(bars, [f\"{x:2.1f}%\" for x in train[\"target\"].value_counts().sort_index().values\/2000],\n                 padding=-20, fontsize=12)\naxs[0].margins(0.025, 0.06)\naxs[0].grid(axis=\"y\")\n\npie = axs[1].pie(train[\"target\"].value_counts(sort=False).sort_index().values,\n                 labels=train[\"target\"].value_counts(sort=False).sort_index().index,\n                 colors=colors,\n                 rotatelabels=True,\n                 textprops={\"fontsize\": 14})\naxs[1].axis(\"equal\")\nplt.show();","8002afa7":"df = pd.DataFrame()\ndf[\"id\"] = train[\"id\"]\ndf[\"target\"] = train[\"target\"]\ndf[\"id\"] = pd.cut(df[\"id\"], np.arange(0, 201000, 1000), right=False)\nvalues = df.groupby(\"id\")[\"target\"].value_counts(sort=False).values\nclasses = [\"Class_\" + str(x) for x in np.arange(9)]\n\ncols = 3\nrows = 3\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=False)\n\nplt.subplots_adjust(hspace = 0.35)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        axs[r, c].plot(np.arange(0, 200, 1),\n                       [values[x] for x in np.arange(i, 1800, 9)],\n                       color=colors[i])\n        axs[r, c].set_title(classes[i], fontsize=12, pad=5)\n        axs[r, c].set_xticks(np.arange(0, 250, 50))\n        axs[r, c].set_xticklabels([str(int(x))+\"k\" for x in axs[r, c].get_xticks()])\n        axs[r, c].set_xlabel(\"Dataframe id\")\n        axs[r, c].set_ylabel(\"Class labels qty per 1k rows\")\n        axs[r, c].set_ylim(0, 320)\n        i+=1\nfig.suptitle(\"Class labels distribution in the train dataset\", fontsize=20)\nplt.show();","3921d8c9":"def make_data_plots(df, i=0):\n    \"\"\"\n    Makes value distribution histogram plots for a given dataframe features\n    \"\"\"\n    columns = df.drop([\"target\", \"id\"], axis=1).columns.values\n\n    cols = 3\n    rows = (len(columns) - i) \/\/ cols + 1\n\n    fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=True)\n    plt.subplots_adjust(hspace = 0.2)\n    for r in np.arange(0, rows, 1):\n        for c in np.arange(0, cols, 1):\n            if i >= len(columns):\n                axs[r, c].set_visible(False)\n            else:\n                axs[r, c].hist(df[columns[i]].values, bins = 30,\n                               color=random.choice(colors),\n                               edgecolor=\"black\")\n                axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            i+=1","c90f0718":"make_data_plots(train)","41fabef2":"def make_nonzero_data_plots(df, i=0):\n    \"\"\"\n    Makes nonzero value distribution histogram plots for a given dataframe features\n    \"\"\"    \n    \n    columns = df.drop([\"target\", \"id\"], axis=1).columns.values\n\n    cols = 3\n    rows = (len(columns) - i) \/\/ cols + 1\n\n    fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=False)\n    plt.subplots_adjust(hspace = 0.2)\n    for r in np.arange(0, rows, 1):\n        for c in np.arange(0, cols, 1):\n            if i >= len(columns):\n                axs[r, c].set_visible(False)\n            else:\n                axs[r, c].hist(df[df[columns[i]] > 0][columns[i]].values, bins = 30,\n                               color=random.choice(colors),\n                               edgecolor=\"black\")\n                axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            i+=1","ff1c5813":"make_nonzero_data_plots(train)","34a2bbbd":"x = -1*np.arange(len(test.drop([\"id\"], axis=1).columns))\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars = ax.barh(x+0.2, train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).values \/ train.shape[0],\n               height=0.4, color=\"cornflowerblue\", label=\"Train dataset\", edgecolor=\"black\")\nbars2 = ax.barh(x-0.2, test.drop([\"id\"], axis=1).astype(bool).sum(axis=0).values \/ test.shape[0],\n                height=0.4, color=\"palevioletred\", label=\"Test dataset\", edgecolor=\"black\")\nax.set_title(\"Fraction of nonzero values in the both datasets\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Fraction of nonzero values\", fontsize=20, labelpad=15)\nax.set_xticks(np.arange(0, 0.8, 0.05))\nax.set_yticks(x)\nax.set_yticklabels(list(test.drop([\"id\"], axis=1).columns.values))\nax.tick_params(axis=\"x\", labelsize=15)\nax.tick_params(axis=\"y\", labelsize=14)\nax.grid(axis=\"x\")\nax.legend(fontsize=15)\nax2 = ax.secondary_xaxis('top')\nax2.set_xticks(np.arange(0, 0.8, 0.05))\nax2.set_xlabel(\"Fraction of nonzero values\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.05, 0.01)","23babf1a":"pca = PCA(n_components=2)\nX_reduced = pd.DataFrame(pca.fit_transform(train.drop([\"id\", \"target\"], axis=1)))\n\nfig, ax = plt.subplots(figsize=(16, 9))\nax.scatter(X_reduced[0], X_reduced[1], c=LabelEncoder().fit_transform(train[\"target\"]), cmap=\"tab10\")\nax.set_title(\"Scatter plot of the original train set reduced to 2 dimensions\", fontsize=20, pad=15)\nplt.show();","99b31a0a":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(train.drop([\"id\", \"target\"], axis=1))\n\npca = PCA(n_components=2)\nX_reduced = pd.DataFrame(pca.fit_transform(X_scaled))\n\nfig, ax = plt.subplots(figsize=(16, 9))\nax.scatter(X_reduced[0], X_reduced[1], c=LabelEncoder().fit_transform(train[\"target\"]), cmap=\"tab10\")\nax.set_title(\"Scatter plot of the scaled train set reduced to 2 dimensions\", fontsize=20, pad=15)\nplt.show();","d026ecc7":"iso_forest = IsolationForest(n_jobs=-1, random_state=42, n_estimators=3000)\niso_forest.fit(train.drop([\"id\", \"target\"], axis=1))\nscores = iso_forest.decision_function(train.drop([\"id\", \"target\"], axis=1))\nprint(f\"Anomalies found in the train dataset: {(scores < 0).sum()}\")\nto_drop = train.loc[scores < 0].index","52915d7d":"# Target distribution in detected anomalies\ntrain.loc[(scores < 0), \"target\"].value_counts()","304b09a3":"# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(train.drop([\"id\", \"target\"], axis=1))\n# lof = LocalOutlierFactor(n_jobs=-1)\n# lof.fit(X_scaled)\n# scores = lof.negative_outlier_factor_\n# score_threshold = -1.93\n# to_drop = train.loc[scores < score_threshold].index\n# print(f\"Anomalies found in the train dataset: {(scores < score_threshold).sum()}\")","b35b64f5":"# # Target distribution in detected anomalies\n# train.loc[(scores < score_threshold), \"target\"].value_counts()","b6dd38e1":"# Dropping anomaly rows detected with Isolation Forest\ntrain.drop(axis=0, index=set(to_drop), inplace=True)\ntrain.shape","9ca7d1f5":"train.drop([\"id\", \"target\"], axis=1).duplicated(keep=False).sum()","15262c18":"def delete_duplicates(df):\n    \"\"\"\n    Finds duplicates in a given DataFrame and deletes rows with identical features values but different target. \n    \"\"\"\n    \n    # Copying duplicate rows in a new dataset and getting their indices\n    idx = df.drop([\"id\", \"target\"], axis=1).duplicated(keep=False)\n    duplicates = df.loc[idx == True].copy()\n    features = [x for x in duplicates.columns if \"feature\" in x]\n    idx = duplicates[\"id\"]\n    \n    # Checking if which rows with equal feature values have different target\n    indx_to_drop = []\n    for index in idx:\n        for row in idx:\n            if (row != index) and (row not in indx_to_drop):\n                if duplicates.loc[index, features].equals(duplicates.loc[row, features]):\n                    if duplicates.loc[index, \"target\"] != duplicates.loc[row, \"target\"]:\n    #                     print(f\"Found duplicates with different targets: {index} - {duplicates.loc[index, 'target']} and {row} - {duplicates.loc[row, 'target']}\")\n                        indx_to_drop.append(index)\n                        indx_to_drop.append(row)\n    #                 else:\n    #                     print(f\"Found duplicates with the same target: {index} and {row}\")\n    \n    # Reporting results\n    print(f\"There are {len(duplicates['id'])} duplicated rows in the dataset.\")\n    print(f\"{len(set(indx_to_drop))} of them have different target. They will be deleted from the dataset.\")\n    print(f\"The datatframe has {len(df['id'])} rows.\")\n    df.drop(axis=0, index=set(indx_to_drop), inplace=True)\n    print(f\"After duplicated deletion there are {len(df['id'])} rows.\")\n    \n    return df","ffd82fdc":"train = delete_duplicates(train)","1fd6b9ee":"# Target feature encoding\nencoder = LabelEncoder()\ntrain[\"target\"] = encoder.fit_transform(train[\"target\"])\ntrain[\"target\"].value_counts(sort=False)","8055b7c7":"def transform_dataset(data):\n    \"\"\"\n    Adds new custom features and transforms original features into custom categories\n    \"\"\"\n    \n    # Copying features in a temporary dataset which will be transformed with MinMaxScaler\n    df = data[[x for x in data.columns if \"feature_\" in x]].copy()\n    \n    # Adding custom features\n    data[\"feature_75\"] = df.max(axis=1)\n    data[\"feature_76\"] = df.mean(axis=1)\n    data[\"feature_77\"] = df.median(axis=1)\n    data[\"feature_78\"] = df.nunique(axis=1)\n    data[\"feature_79\"] = (df == 0).astype(int).sum(axis=1)\n    data[\"feature_80\"] = (df != 0).sum(axis=1)\n    data[\"feature_81\"] = (df == 0).astype(int).sum(axis=1) \/ 75\n    data[\"feature_82\"] = (df != 0).sum(axis=1) \/ 75\n    \n    \n    # Scaling original features and adding new features basing on them\n    scaled_df = pd.DataFrame(index = data.index.values, columns = df.columns.values)\n    for col in df.columns.values:\n        scaler = MinMaxScaler()\n        scaled_df[col] = scaler.fit_transform(np.array(df[col]).reshape(-1, 1))\n    data[\"feature_83\"] = scaled_df.mean(axis=1)\n    data[\"feature_84\"] = (scaled_df == 1).sum(axis=1)\n    \n    # Cutting original features into custom intevals [0, 1), [1, 15), [15, 30) ...\n    intervals = np.insert(np.arange(15, 370, 15), 0, [0, 1])\n    intervals_text = pd.cut(train[\"feature_0\"], intervals, right=False).value_counts().sort_index().index.astype(\"string\")\n    map_dict = dict(zip(intervals_text, list(np.arange(len(intervals_text)))))\n    for i, column in enumerate(data.drop([\"id\"], axis=1).columns):\n        if ((i < 75) and (column!=\"target\")):\n            data[column] = pd.cut(data[column], intervals, right=False).astype(\"string\")\n            data[column].replace(map_dict, inplace=True)\n    \n    return data","c4fa6410":"# Transforming both datasets and making value distribution plots for new custom features of the train dataset\ntrain_data = transform_dataset(train.copy())\ntest_data = transform_dataset(test.copy())\nmake_data_plots(train_data, i=75)","f4573b22":"# LightAutoML parameters\nN_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 10 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 6 * 3600 # Time in seconds for automl run\nTARGET_NAME = \"target\"","290d8b89":"task = Task(\"multiclass\")\n\nroles = {\"target\": TARGET_NAME,\n         \"drop\": \"id\"}","7928b6d8":"%%time\n\nmodel = TabularUtilizedAutoML(task = task,\n                              verbose=1,\n                              timeout = TIMEOUT,\n                              cpu_limit = N_THREADS,\n                              reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n#                               general_params = {'use_algos': [['linear_l2', 'lgb_tuned', 'cb_tuned'], ['lgb', 'linear_l2']]})\n                              general_params = {'use_algos': [['linear_l2', 'lgb_tuned', 'cb_tuned'], ['lgb', 'cb']]})\n\ntrain_preds = model.fit_predict(train_data, roles = roles)\nprint(f\"Train data score is {log_loss(train_data['target'].values, train_preds.data)}\")","27501a4b":"conf_mx = confusion_matrix(train_data[\"target\"], np.array([np.argmax(x) for x in train_preds.data]).reshape(-1,1))\nconf_mx","a6066301":"fig, ax = plt.subplots(figsize=(10, 10))\nax.matshow(conf_mx, cmap=plt.cm.get_cmap(\"viridis\"))","dc3b60b3":"# Extracting and sorting feature importances from LightAutoML\nfeature_importances = model.get_feature_scores('fast', silent=False)\nindices = feature_importances[\"Feature\"].str.split(\"_\", expand=True)[1].astype(int).sort_values(ascending=True).index\nfeature_importances = feature_importances.reindex(indices).reset_index(drop=True)","c8e88314":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = feature_importances[\"Feature\"]\ndf[\"Importance\"] = feature_importances[\"Importance\"]\ndf.loc[:74, \"Color\"] = \"cornflowerblue\"\ndf.loc[75:, \"Color\"] = \"palevioletred\"\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nlegend_lines = [Line2D([0], [0], color=\"cornflowerblue\", lw=10),\n                Line2D([0], [0], color=\"palevioletred\", lw=10)]\n\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars1 = ax.barh(x, df[\"Importance\"], height=height,\n                color=df[\"Color\"], edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nax.legend(legend_lines, [\"Original features\", \"Custom features\"], fontsize=15, loc=1, bbox_to_anchor=(0, 0, 1, 0.97))\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","f9d8b76d":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = feature_importances.loc[:75, \"Feature\"]\ndf[\"Importance\"] = feature_importances.loc[:75, \"Importance\"]\ndf.sort_values(\"Importance\", axis=0, ascending=True, inplace=True)\n\n\nheight = 0.7\n\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(16,8))\n\nbars1 = axs[0].barh(df[\"Feature\"].iloc[-15:], df[\"Importance\"].iloc[-15:],\n                    height=height,\n                    color=\"mediumorchid\",\n                    edgecolor=\"black\")\naxs[0].set_title(\"Top 15 most important original features\", fontsize=15)\naxs[0].set_xlabel(\"Feature importance\", fontsize=15, labelpad=10)\n\nbars2 = axs[1].barh(train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:],\n                    train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).values[-15:] \/ train.shape[0],\n                    height=height,\n                    color=\"mediumseagreen\",\n                    edgecolor=\"black\")\naxs[1].set_title(\"Top 15 original features with the most nonzero values\", fontsize=15)\naxs[1].set_xlabel(\"Fraction of nonzero values\", fontsize=15, labelpad=10)\naxs[1].bar_label(bars2, \n                 [\"Top 15 important feature\" if x in list(df[\"Feature\"].iloc[-15:]) else \"\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:]],\n                 padding=-175, fontsize=12, color=\"white\", weight=\"bold\")\n\nfig.suptitle(\"Top 15 original features with the most importance and nonzero values\", fontsize=20)\nplt.show();","9193e055":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = feature_importances.loc[:75, \"Feature\"]\ndf[\"Importance\"] = feature_importances.loc[:75, \"Importance\"]\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\ncolors = [\"mediumorchid\" if x in df[\"Feature\"].iloc[-15:] else \"mediumseagreen\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:]]\n\nheight = 0.7\n\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(16,8))\n\nbars1 = axs[0].barh(df[\"Feature\"].iloc[-15:], df[\"Importance\"].iloc[-15:],\n                    height=height,\n                    color=\"mediumorchid\",\n                    edgecolor=\"black\")\naxs[0].set_title(\"Top 15 least important original features\", fontsize=15)\naxs[0].set_xlabel(\"Feature importance\", fontsize=15, labelpad=10)\n\nbars2 = axs[1].barh(train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).index[-15:],\n                    train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).values[-15:] \/ train.shape[0],\n                    height=height,\n                    color=\"lightcoral\",\n                    edgecolor=\"black\")\naxs[1].set_title(\"Top 15 original features with the least nonzero values\", fontsize=15)\naxs[1].set_xlabel(\"Fraction of nonzero values\", fontsize=15, labelpad=10)\naxs[1].bar_label(bars2, [\"Top 15 least important feature\" if x in list(df[\"Feature\"].iloc[-15:]) else \"\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).index[-15:]],\n                 padding=-215, fontsize=12, color=\"white\", weight=\"bold\")\n\nfig.suptitle(\"Top 15 original features with the least importance and nonzero values\", fontsize=20)                \nplt.show();","8f0e937b":"preds = model.predict(test_data.drop(\"id\", axis=1))\npredictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions = pd.concat([predictions, pd.DataFrame(preds.data, columns=[\"Class_\" + str(x) for x in np.arange(1, 10, 1)])], axis=1)\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","8f10af0d":"# **Data import**","0d7571f7":"Let's check if these classes are evenly distributed in the dataset.","0274b386":"## Detecting anomalies with LocalOutlierFactor","0ac9882b":"## Fraction of nonzero values in the both datasets","2b86c06d":"## Detecting anomalies with IsolationForest","82b6a528":"## Original features value distribution in the train dataset","531a41fc":"As you can see, the data does not have distinct clusters after reducing dimensions.","96867705":"There are some duplicates in the dataset. If they have identical feature but different target values it may decrease machine learning performance.","0209db24":"# **Data analysis**","d59bea3c":"## Target distribution","3069fe0e":"# **Predictions and submission**","29b0f983":"## Original features nonzero value distribution in the train dataset","dfb85120":"# **Feature importances**","906786f2":"# **Data preparation**","a5790466":"# **Confusion matrix**","c8c25cff":"So it looks like class labels are distributed pretty evenly across the dataset.","874db7a1":"# **Machine Learning**","def1ef39":"## PCA"}}