{"cell_type":{"9b5e2ede":"code","7a046f77":"code","ab2bebfb":"code","8547bffb":"code","6f88f80d":"code","a5c1fc71":"code","46ba219b":"code","28bf9747":"code","85710046":"code","10855f3c":"code","06a31332":"code","dd6adca5":"code","be754b85":"code","4d50ec85":"markdown","1299cdb7":"markdown","1abe28b9":"markdown","5ae7f200":"markdown","b4194819":"markdown","9bead0de":"markdown","f05da113":"markdown","968673a4":"markdown","6bccf38a":"markdown","3e678bd7":"markdown","0271c4bb":"markdown","e2726c02":"markdown","fe0b27b6":"markdown","89adec5b":"markdown","bef14dad":"markdown","81eb3c52":"markdown","cf52f0d1":"markdown","c026b148":"markdown","459e24f7":"markdown","2d6b6984":"markdown","88f088e3":"markdown","28381a72":"markdown","b75a30b8":"markdown","486c212f":"markdown","ef026e98":"markdown","d76197b8":"markdown","28d27613":"markdown","30fa08b2":"markdown","e162dabe":"markdown","55a442c8":"markdown","25af37f1":"markdown","d90540bc":"markdown","7d3bf132":"markdown","8124bd03":"markdown","2a70802f":"markdown","ec34e8a0":"markdown","a97b1926":"markdown"},"source":{"9b5e2ede":"# Import Libraries \nimport random, re, math\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom kaggle_datasets import KaggleDatasets\nprint('Tensorflow version ' + tf.__version__)\nfrom sklearn.model_selection import KFold\nfrom functools import partial\nimport keras\nimport pandas as pd, numpy as np\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf, re, math\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\n## Install Efficient Net \n!pip install -q efficientnet >> \/dev\/null\nimport efficientnet.tfkeras as efn\n\n","7a046f77":"import warnings\nfrom IPython.display import YouTubeVideo\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nfrom IPython.display import HTML\nfrom IPython.display import YouTubeVideo\n\nYouTubeVideo('MXxN4fv01c8', width=800, height=300)","ab2bebfb":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","8547bffb":"from kaggle_datasets import KaggleDatasets\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH_ORI = KaggleDatasets().get_gcs_path('cassava-leaf-disease-classification')\nGCS_PATH_NEW = KaggleDatasets().get_gcs_path('tfrecords10foldcv')\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [256, 256]\nCLASSES = ['0', '1', '2', '3', '4']\nEPOCHS = 5","6f88f80d":"## Code Credits : https:\/\/www.kaggle.com\/wuliaokaola\/getting-started-tpus-new-tfrecords\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset\n\n\ndef data_augment(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    return image, label\n\ndef get_training_dataset(dataset, do_aug = True): # trainingfiles changed to dataset\n    #dataset = load_dataset(training_files, labeled=True)  \n    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)  \n    if do_aug: dataset = dataset.map(transform ,num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef get_validation_dataset(validation_files , ordered=False):\n    dataset = load_dataset(validation_files, labeled=True, ordered=ordered) \n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\n\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH_ORI + '\/test_tfrecords\/ld_test*.tfrec')\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","a5c1fc71":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n     # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","46ba219b":"def transform(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 20. * tf.random.normal([1],dtype='float32')\n    shr = 8. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n     # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),label","28bf9747":"print(\"Displaying Image with Augmentations\")\nrow = 3; col = 4;\nfoldno = list(np.arange(10))\ntraining_example =  tf.io.gfile.glob(GCS_PATH_NEW + '\/Id_train00-2140.tfrec')\n\nall_elements = get_training_dataset(load_dataset(training_example),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","85710046":"print(\"Displaying Image with Augmentations\")\n\nrow = 3; col = 4;\nfoldno = list(np.arange(10))\ntraining_example =  tf.io.gfile.glob(GCS_PATH_NEW + '\/Id_train01-2140.tfrec')\n\nall_elements = get_training_dataset(load_dataset(training_example),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","10855f3c":"print(\"Displaying Image with Augmentations\")\n\nrow = 3; col = 4;\nfoldno = list(np.arange(10))\ntraining_example =  tf.io.gfile.glob(GCS_PATH_NEW + '\/Id_train09-2139.tfrec')\n\nall_elements = get_training_dataset(load_dataset(training_example),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","06a31332":"\ndef build_model():\n    with strategy.scope():       \n        #img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.efn.EfficientNetB0.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n\n        base_model = efn.EfficientNetB0(weights='imagenet', include_top=False)\n        base_model.trainable = True\n\n        model = tf.keras.Sequential([\n            base_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax')  \n        ])\n        opt = tf.keras.optimizers.Adam(lr=0.00001)\n        model.compile(\n        optimizer=opt,\n        loss='sparse_categorical_crossentropy',  \n        metrics=['sparse_categorical_accuracy'])\n        return model\n    ","dd6adca5":"# Code Credits : https:\/\/www.kaggle.com\/cdeotte\/how-to-compete-with-gpus-workshop\n\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n","be754b85":"FOLDS =  10\nfoldno = np.arange(10)\nmodels = []\nhistories = []\nfor fold in foldno:\n    training_files = tf.io.gfile.glob([GCS_PATH_NEW + '\/Id_train%.2i*.tfrec'%x for x in foldno if x != fold])\n    NUM_TRAINING_IMAGES = count_data_items(training_files)\n\n    validation_files = tf.io.gfile.glob([GCS_PATH_NEW + '\/Id_train%.2i*.tfrec'%x for x in foldno if x == fold])\n    train_dataset = load_dataset(training_files)                              #get_training_dataset(training_files)\n    valid_dataset = get_validation_dataset(validation_files)                              #get_validation_dataset(validation_files)\n    NUM_VALIDATION_IMAGES = count_data_items(validation_files)\n    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\n    VALID_STEPS = NUM_VALIDATION_IMAGES \/\/ BATCH_SIZE\n    model = build_model()\n    history = model.fit(get_training_dataset(train_dataset), \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    epochs=EPOCHS,\n                    validation_data=valid_dataset,\n                    validation_steps=VALID_STEPS , \n                    callbacks = [lr_callback])\n    \n    models.append(model)\n    path  = f'Cassava_Model_EFFNETB0:_Fold:{fold}.hdf5'\n    model.save(path)\n    histories.append(history)\n    \n    print(\" \")\n","4d50ec85":"### 5.1 Code for Augmentations","1299cdb7":"<a id=\"3\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>3. Setting Up TPU Environment <\/center><h2>\n","1abe28b9":"### 6.3 Training and Validating Model :","5ae7f200":"The core idea is since we are using Transfer Learning , if we use very large learning rate initially , That will cause drastic updates in the weights of pretrained Efficient Net ! We want to utilize our pretrained model hence it is better to start with smaller Learning Rate like 1e-5 and gradually move towards 1e-3 , Refer This [video](https:\/\/www.youtube.com\/watch?v=DEuvGh4ZwaY&t=2706s) for more on this !","b4194819":"<a id=\"5\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>5. Defining Augmentations  <\/center><h2>\n","9bead0de":"<a id=\"1\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>1. Preface and About the Kernel<\/center><h2>\n","f05da113":"<a id=\"4\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>4. Loading Competition Data  <\/center><h2>\n","968673a4":"As Pointed Out Earlier in Some Discussions There was problem in Tf Records Format . So I created A New Dataset in Tfrecords Format , I created a 10 folds Dataset using this [Script](https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-disease-stratified-tfrecords-256x256) , Stratified based on targets , For verifying Exact Stratification across various Folds , Refer this [Notebook](https:\/\/www.kaggle.com\/sayedathar11\/cassava-leaf-disease-stratified-tfrecords-256x256)","6bccf38a":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h2>\n\n* [1 Preface ](#1)\n* [2. What is TPU ](#2)\n* [3. Setting Up the Environment ](#3)\n* [4. Loading Competition Data  ](#4)\n* [5. Defining Augmentations ](#5)\n* [6. Modelling ](#6)\n* [7. Some Pointers to Improve Model's Performance  ](#7)\n\n\n\n  \n","3e678bd7":"This is how a TPU v3 looks like(Image Credits Wikipedia) .\n![tpuv3.jpg](attachment:tpuv3.jpg)","0271c4bb":"### 4.1 Loading Data from Tfrecords Format ","e2726c02":"### 6.2 Defining Learning Rate Schedule ","fe0b27b6":"When using TPU Datasets needs to be stored on Google Cloud Storage bucket , as TPUs are available on Google Cloud Platform . We need to specify the GCS path to load Data .","89adec5b":"**Note : That I have created Image of size 256 * 256 if you want to experiment with different Image sizes , just use the above script which was use for 10 Fold Cv Generation and set Size Parameter there to size you want**","bef14dad":"First We have to define the distribution of strategy for Tensorflow . So that TPU can use this and distribute Jobs accordingly. The below code does exactly the same","81eb3c52":"Now we need to Assemble Dataset From Tfrecords so that is in the form that we can feed to our Model ! The Below Code Does the same ! \nPS : **I have hidden the code , you can just read Explanantion of Each Functions Otherwise You Can Just Go through Explanation of Each Functions given below.**","cf52f0d1":"### 5.2 Displaying Some Augmentations Example","c026b148":"### Teaming UP :\n**If You have Good Experience in Computer Vision and are looking for Team Member to Ace this competition I am available \ud83d\ude0a , I would be Glad to Learn From You and also teach you some of the techniques I have learnt**","459e24f7":"### 6.4 Conclusions:\n\n1. With just 5 Epochs we reach pretty good accuracy of 84-85 % .\n2. Using TPU Speeds up things like crazy for Example Training Such a Model without FineTuning the EfficientNet layer could take minutes for each epochs , **Using TPU we completed Each Epochs within 30 seconds**.\n3. Use of TPUs Make K-Fold Cross Validation very much feasible however we must prepare Data in Tfrecords Format before Hand .\n4. Augmentations become much more faster while using TPUs.","2d6b6984":"### 4.2 Explanation of Each Functions:\n1. `decode_image` :Takes Image which is in Tfrecords format and decodes it into a Tensor , It also normalizes the tensor by scaling values between 0-1 across all 3 Channels . Note that this function is insantiated by read_tfrecord Function which is explained next.\n\n2. `read_tfrecord`: It takes Tfrecord Example and it returns (image , label) pair if we have training data and (image , id) pair , if we have test data.\n\n3. `load_dataset`: It is used Load Training or Test Dataset appropriately . If it loads training data we also do some image Augmentations on our Training Data so that we have more samples of images .\n\n4. `get_training_dataset`: It is used to fetch training dataset by giving appropriate path of Tfrecords in the directory ! Note that this function insantiates `load_dataset` function which uses it to load training dataset.\n\n5. `get_validation_dataset`: In our Case it loads Each Validation Folds appropriately using `load_dataset` utility.\n\n6. `get_test_dataset`: It is used to get test_dataset.\n\n7. `count_data_items` : Usually in each Tfrecords Files we have multiple items for example if we have a Tf record file named `Id_train00-2140.tfrec` , Then this file has 2140 Data items . This code counts all such data items in all tfrecords either train or validation and return their sum.","88f088e3":"1. This Kernel deals with how to Train Deep Learning Models on TPU.\n2. I was learning how to use TPU after discovering about them while doing Flowers Classification using TPU [here](https:\/\/www.kaggle.com\/c\/tpu-getting-started\/) \n3. In this kernel I have Also Demonstrated K Fold Cv and Augmentations.\n4. I have trained \/ FineTuned Efficient Net B0.\n5. Please Feel free to point out some improvement tips .","28381a72":"We will use code from Chirs Deotte's [Kernel](https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96) for Data Augmentation . As Pointed out in this Kernel using `tensorflow.data.Dataset` for Data Augmentations , does Data Augmentation on TPU \/ GPU which offers Speed up as compared to traditional method of Doing Data Augmentations on a CPU.","b75a30b8":"<a id=\"6\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>6. Modelling   <\/center><h2>\n","486c212f":"**Note : Since we are using TPU we need to define our model inside a context manager which is `strategy.scope` , What this context manager does is effectively it tells the TPU how to distribute Training on different Cores**","ef026e98":"### 4.3 Conclusion For Data Loaders :\n\nAs a concluding point for Data Loaders Functions You can learn more about them in Tfrecords Basic [Kernel](https:\/\/www.kaggle.com\/ryanholbrook\/tfrecords-basics) and getting started kernel [here](https:\/\/www.kaggle.com\/ryanholbrook\/create-your-first-submission) . **I would Emphasize to go over this kernels also so that you can get familiar with workflow and connect dots between these 2 , This will help in future to create DataLoading Pipeline From Scratch**","d76197b8":"<a id=\"2\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>2. What is TPU ?<\/center><h2>\n","28d27613":"<a id=\"7\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>7.Some Pointers on Improving Models Performance<\/center><h2>\n","30fa08b2":"We will have following augmentations.\n1. Rotate by 20 degrees\n2. Shear by 8\n3. zoom by some factor \n4. height and width shift by 16","e162dabe":"A Notable Parameter is Batch_Size , Note that Batch size is set to 16 * Number of Replicas , In Case of our TPU Number of Replicas are 8. So Batch Size becomes 128.","55a442c8":"1. A TPU can be thought as of Multiple GPUS clubbed in One Machine .\n2. TPU can be used for parallelising Training of Machine Learning Models.\n3. A Typical TPU has 8 cores and each core is like a GPU .\n\nTo Know more about TPU you can refer the below video .","25af37f1":"We can see 8 replicas are return which means that there are 8 cores present Each core is kind of GPU.**NOTE: If Your Notebook Outputs 1 it means you have not Enabled TPU , Enabling TPU is just like Enabling GPU just go to Accelerator Tab and select TPU v3-8**","d90540bc":"For the above model an epoch takes **less than 30s** and within **5 Epochs**  we are able to reach an accuracy of **~84 % on Validation Data** ! This is absolutely good as we train the entire model from scratch! Just unhide the Output Cell of the Model if you want to see the results for each Folds .","7d3bf132":"### 6.1 Defining Model Architecture ","8124bd03":"1. Trying out other variations of Efficient Net like B1 , B2 , .. B6.\n2. Trying out some more technqiues of Fine Tuning Like Feature Extraction , Freezing Initial Layers and Tuning Last Few Layers.\n3. Adding Dropouts and Regularization like L1 or L2 Regularization.\n4. Trying out different Learning Rate Schedules and adding some more callbacks like Early Stopping.\n5. Experimenting with some more augmentations other than those define here ! \n6. Ensembling the predictions of various EfficientNet models and Also various CNNs **[This should be done at the end of competition after trying out single best models]**","2a70802f":"Even though EfficientNet B0 used here performs quite well , it can be improved tremendously well by following some of the listed tips.\n","ec34e8a0":"When we are using TPU's , Dataset are often serialized into Tfrecords Format , You can know more about Tfrecords [here](https:\/\/www.youtube.com\/watch?v=KgjaC9VeOi8&list=PLqFaTIg4myu-1c3ygYzakW8-hNzQG59-5) , \n**In a Nutshell Tfrecords allows distributing various jobs accross cores and helps parallelizing tasks and speeding up Data Loading and Augmentation process**","a97b1926":"1. For Modelling we Will Use **EfficientNetB0** , You can learn more about EfficientNet [Here](https:\/\/www.youtube.com\/watch?v=4U2WO8ObGGU&t=379s) .\n\n2. We will use Transfer Learning to Fine tune  Layers of EfficientNet !\n\n3. Another thing can be using Various Efficient Architectures like B1, B2 .. ,B6 and selecting what suits the problem best.\n\n4. Also Since we have Power of TPU one can Also Train EfficientNet from Scratch !\n"}}