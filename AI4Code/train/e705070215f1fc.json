{"cell_type":{"97089f88":"code","c759fc6d":"code","a15defa6":"code","3ab484e9":"code","b80fbb9a":"code","1b827003":"code","cb3d42fe":"code","12d1ebc1":"code","778ca862":"code","03f2e3ea":"code","b736b586":"code","eeb5b3d8":"code","7501bc09":"code","880284ec":"code","484e920d":"code","e74557bd":"code","e1987a94":"code","89134f46":"code","12503f72":"code","4036ba80":"code","29d5adf7":"code","0a01fa57":"code","1015c9ce":"code","1515336a":"code","5ab1bcf7":"code","ba541582":"code","60f10673":"code","2db98957":"code","450f5989":"code","26bda799":"code","96551459":"code","e2905b11":"code","0cedb93c":"code","55106bef":"code","cc1cbb3e":"code","8dedcd0c":"code","466aadf6":"code","f25c1d31":"code","1e602d91":"code","b320548c":"code","aaaba1b4":"code","36d51eeb":"code","60435443":"code","3c32d94f":"code","37dc4655":"code","8e9d95c3":"code","ae83daf4":"code","07fb559a":"code","4de259ed":"code","eec23b31":"code","fff62846":"code","63c25bef":"code","87fc33cf":"code","63332da8":"code","3bb737c6":"code","9974c2e1":"code","3af52fb3":"code","f4fa780b":"code","feda56f0":"code","2c488aa7":"code","d7407269":"code","51fd2982":"code","b4c864e7":"code","b4606fa9":"code","32bb23ae":"code","79f690d2":"code","be0343c7":"code","825108e9":"code","239a36ec":"code","bdcc3d90":"code","eaffbac1":"code","56ae44bc":"code","359c0513":"code","9d75cbb2":"code","417af951":"code","c2a172fa":"code","f35bc083":"code","89b28372":"code","1e633f58":"code","fa758516":"code","be11db6c":"markdown","a1d62b5e":"markdown","02686768":"markdown","f4576f94":"markdown","9d513990":"markdown","30ce0519":"markdown","399eaa26":"markdown","1f4f6292":"markdown","9d85d57d":"markdown","6d0dcc91":"markdown","58558c30":"markdown","e4802807":"markdown","e7b2b797":"markdown","45cc6ff1":"markdown","67e06b3f":"markdown","eaf0eb7f":"markdown"},"source":{"97089f88":"import riiideducation\n# import dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score, train_test_split, validation_curve, ShuffleSplit, RandomizedSearchCV\nfrom sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt","c759fc6d":"# load all data\ntrain = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',\n                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n                   nrows=5000000,\n                   dtype={'timestamp': 'int64',\n                          'user_id': 'int32',\n                          'content_id': 'int16',\n                          'content_type_id': 'int8',\n                          'task_container_id': 'int16',\n                          'answered_correctly':'int8',\n                          'prior_question_elapsed_time': 'float32',\n                          'prior_question_had_explanation': 'boolean'}\n                   )","a15defa6":"n = len(train)\nn","3ab484e9":"train.head()","b80fbb9a":"#reading in question df\nquestions_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv')","1b827003":"questions_df['num_tags'] = questions_df['tags'].apply(lambda x:len(x.split()) if pd.notna(x) else 0)","cb3d42fe":"questions_df.head()","12d1ebc1":"questions_df = questions_df[['question_id','part','num_tags']]","778ca862":"#reading in lecture df\nlectures_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/lectures.csv')","03f2e3ea":"lectures_df.head()","b736b586":"lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n\nlectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])","eeb5b3d8":"lectures_df.head()","7501bc09":"part_lectures_columns = [column for column in lectures_df.columns if column.startswith('part')]\n\ntypes_of_lectures_columns = [column for column in lectures_df.columns if column.startswith('type_of_')]","880284ec":"part_lectures_columns","484e920d":"# merge lecture features to train dataset\ntrain_lectures = train[train.content_type_id == True].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')","e74557bd":"train_lectures.head()","e1987a94":"print(len(train_lectures), len(train_lectures)\/n)","89134f46":"# collect per user stats\nuser_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()","12503f72":"# add boolean features\nfor column in user_lecture_stats_part.columns:\n    bool_column = column + '_boolean'\n    user_lecture_stats_part[bool_column] = (user_lecture_stats_part[column] > 0).astype(int)","4036ba80":"user_lecture_stats_part","29d5adf7":"#clearing memory\ndel(train_lectures)","0a01fa57":"#removing True or 1 for content_type_id which are not questions\n\ntrain = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)","1015c9ce":"train[(train.task_container_id == 9999)].tail()","1515336a":"train[(train.content_type_id == False)].task_container_id.nunique()","5ab1bcf7":"#saving value to fillna\nelapsed_mean = train.prior_question_elapsed_time.mean()","ba541582":"group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\ngroup1.columns = ['avg_questions']\ngroup2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\ngroup2.columns = ['avg_questions']\ngroup3 = group1 \/ group2","60f10673":"group3['avg_questions_seen'] = group3.avg_questions.cumsum()","2db98957":"group3.iloc[10].avg_questions_seen","450f5989":"results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_final.columns = ['answered_correctly_user']\n\nresults_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_final.columns = ['explanation_mean_user']","26bda799":"results_u2_final.explanation_mean_user.describe()","96551459":"train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')","e2905b11":"results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\nresults_q_final.columns = ['quest_pct']","0cedb93c":"results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\nresults_q2_final.columns = ['count']","55106bef":"question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","cc1cbb3e":"question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","8dedcd0c":"question2.quest_pct = round(question2.quest_pct,5)","466aadf6":"display(question2.head(), question2.tail())","f25c1d31":"train.head()","1e602d91":"len(train)","b320548c":"train.answered_correctly.mean()","aaaba1b4":"prior_mean_user = results_u2_final.explanation_mean_user.mean()\nprior_mean_user","36d51eeb":"train.loc[(train.timestamp == 0)].answered_correctly.mean()","60435443":"train.loc[(train.timestamp != 0)].answered_correctly.mean()","3c32d94f":"train.drop(['timestamp', 'content_type_id', 'question_id', 'part', 'num_tags'], axis=1, inplace=True)","37dc4655":"len(train)","8e9d95c3":"train.head()","ae83daf4":"validation = train.groupby('user_id').tail(5)\ntrain = train[~train.index.isin(validation.index)]\nlen(train) + len(validation)","07fb559a":"len(validation)","4de259ed":"validation.answered_correctly.mean()","eec23b31":"train.answered_correctly.mean()","fff62846":"results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_val.columns = ['answered_correctly_user']\n\nresults_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_val.columns = ['explanation_mean_user']","63c25bef":"X = train.groupby('user_id').tail(18)\ntrain = train[~train.index.isin(X.index)]\nlen(X) + len(train) + len(validation)","87fc33cf":"X.answered_correctly.mean()","63332da8":"train.answered_correctly.mean()","3bb737c6":"results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_X.columns = ['answered_correctly_user']\n\nresults_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_X.columns = ['explanation_mean_user']","9974c2e1":"#clearing memory\ndel(train)","3af52fb3":"X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nX = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")\n\nX = pd.merge(X, user_lecture_stats_part, on=['user_id'], how=\"left\")","f4fa780b":"validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nvalidation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\nvalidation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")\n\nvalidation = pd.merge(validation, user_lecture_stats_part, on=['user_id'], how=\"left\")","feda56f0":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\n\nX.prior_question_had_explanation.fillna(False, inplace = True)\nvalidation.prior_question_had_explanation.fillna(False, inplace = True)\n\nvalidation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\nX[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])","2c488aa7":"X.columns","d7407269":"#reading in question df\n#question2 = pd.read_csv('\/kaggle\/input\/question2\/question2.csv)","51fd2982":"content_mean = question2.quest_pct.mean()\n\nquestion2.quest_pct.mean()\n#there are a lot of high percentage questions, should use median instead?","b4c864e7":"#filling questions with no info with a new value\nquestion2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n\n\n#filling very hard new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n\n#filling very easy new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)","b4606fa9":"X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nvalidation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nX.part = X.part - 1\nvalidation.part = validation.part - 1","32bb23ae":"X.head()","79f690d2":"y = X['answered_correctly']\nX = X.drop(['answered_correctly'], axis=1)\nX.head()\n\ny_val = validation['answered_correctly']\nX_valdf = validation.drop(['answered_correctly'], axis=1)","be0343c7":"X_train = X[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n       'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n       'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n       'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean', 'num_tags',]]\nX_val = X_valdf[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n               'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n               'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n               'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n               'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean', 'num_tags',]]","825108e9":"# Filling with 0.5 for simplicity; there could likely be a better value\nX_train['answered_correctly_user'].fillna(0.65,  inplace=True)\nX_train['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX_train['quest_pct'].fillna(content_mean, inplace=True)\nX_train['num_tags'].fillna(0, inplace=True)\nX_train['part'].fillna(4, inplace = True)\nX_train['avg_questions_seen'].fillna(1, inplace = True)\nX_train['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX_train['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n\nX_train['part_1'].fillna(0, inplace = True)\nX_train['part_2'].fillna(0, inplace = True)\nX_train['part_3'].fillna(0, inplace = True)\nX_train['part_4'].fillna(0, inplace = True)\nX_train['part_5'].fillna(0, inplace = True)\nX_train['part_6'].fillna(0, inplace = True)\nX_train['part_7'].fillna(0, inplace = True)\nX_train['type_of_concept'].fillna(0, inplace = True)\nX_train['type_of_intention'].fillna(0, inplace = True)\nX_train['type_of_solving_question'].fillna(0, inplace = True)\nX_train['type_of_starter'].fillna(0, inplace = True)\n#X_train['part_1_boolean'].fillna(0, inplace = True)\n#X_train['part_2_boolean'].fillna(0, inplace = True)\n#X_train['part_3_boolean'].fillna(0, inplace = True)\n#X_train['part_4_boolean'].fillna(0, inplace = True)\n#X_train['part_5_boolean'].fillna(0, inplace = True)\n#X_train['part_6_boolean'].fillna(0, inplace = True)\n#X_train['part_7_boolean'].fillna(0, inplace = True)\nX_train['type_of_concept_boolean'].fillna(0, inplace = True)\nX_train['type_of_intention_boolean'].fillna(0, inplace = True)\nX_train['type_of_solving_question_boolean'].fillna(0, inplace = True)\nX_train['type_of_starter_boolean'].fillna(0, inplace = True)","239a36ec":"X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\nX_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX_val['quest_pct'].fillna(content_mean,  inplace=True)\n\nX_val['part'].fillna(4, inplace = True)\nX_val['avg_questions_seen'].fillna(1, inplace = True)\nX_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)\nX_val['num_tags'].fillna(0, inplace=True)\nX_val['part_1'].fillna(0, inplace = True)\nX_val['part_2'].fillna(0, inplace = True)\nX_val['part_3'].fillna(0, inplace = True)\nX_val['part_4'].fillna(0, inplace = True)\nX_val['part_5'].fillna(0, inplace = True)\nX_val['part_6'].fillna(0, inplace = True)\nX_val['part_7'].fillna(0, inplace = True)\nX_val['type_of_concept'].fillna(0, inplace = True)\nX_val['type_of_intention'].fillna(0, inplace = True)\nX_val['type_of_solving_question'].fillna(0, inplace = True)\nX_val['type_of_starter'].fillna(0, inplace = True)\n#X_val['part_1_boolean'].fillna(0, inplace = True)\n#X_val['part_2_boolean'].fillna(0, inplace = True)\n#X_val['part_3_boolean'].fillna(0, inplace = True)\n#X_val['part_4_boolean'].fillna(0, inplace = True)\n#X_val['part_5_boolean'].fillna(0, inplace = True)\n#X_val['part_6_boolean'].fillna(0, inplace = True)\n#X_val['part_7_boolean'].fillna(0, inplace = True)\nX_val['type_of_concept_boolean'].fillna(0, inplace = True)\nX_val['type_of_intention_boolean'].fillna(0, inplace = True)\nX_val['type_of_solving_question_boolean'].fillna(0, inplace = True)\nX_val['type_of_starter_boolean'].fillna(0, inplace = True)","bdcc3d90":"X_train_values = X_train.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_train_scaled = min_max_scaler.fit_transform(X_train_values)\nx_train_scaled","eaffbac1":"X_val_values = X_val.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_val_scaled = min_max_scaler.fit_transform(X_val_values)\nx_val_scaled","56ae44bc":"y_train = y.values\ny_val = y_val.values","359c0513":"x_train_scaled.shape, len(y_train)","9d75cbb2":"# Define simple fully_connected model\ndim = x_train_scaled.shape[1]\ninputs = tf.keras.Input(shape=(dim,))\nx = tf.keras.layers.Dense(16, activation='relu')(inputs)\nx = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(16, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\nout = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel_DNN = tf.keras.Model(inputs=inputs, outputs = out)\nmodel_DNN.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0002), loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.Precision()])\nmodel_DNN.summary()","417af951":"# Train\nepoch_num = 20\nhistory = model_DNN.fit(x_train_scaled, np.array(y_train), validation_data=(x_val_scaled, np.array(y_val)), batch_size=64, epochs=epoch_num)","c2a172fa":"_ = plt.plot(history.epoch, history.history['loss'], c='r')\n_ = plt.plot(history.epoch, history.history['val_loss'], c='g')\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"Loss\")\n#_ = plt.xlim(0, 1)  # limit axes for better visualization\n#_ = plt.ylim(0, 0.1)","f35bc083":"y_pred = model_DNN.predict(x_val_scaled)\ny_true = np.array(y_val)\nroc_auc_score(y_true, y_pred)","89b28372":"env = riiideducation.make_env()","1e633f58":"iter_test = env.iter_test()","fa758516":"for (test_df, sample_prediction_df) in iter_test:\n    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n    \n    test_df = pd.merge(test_df, user_lecture_stats_part, on=['user_id'], how=\"left\")\n    test_df['part_1'].fillna(0, inplace = True)\n    test_df['part_2'].fillna(0, inplace = True)\n    test_df['part_3'].fillna(0, inplace = True)\n    test_df['part_4'].fillna(0, inplace = True)\n    test_df['part_5'].fillna(0, inplace = True)\n    test_df['part_6'].fillna(0, inplace = True)\n    test_df['part_7'].fillna(0, inplace = True)\n    test_df['type_of_concept'].fillna(0, inplace = True)\n    test_df['type_of_intention'].fillna(0, inplace = True)\n    test_df['type_of_solving_question'].fillna(0, inplace = True)\n    test_df['type_of_starter'].fillna(0, inplace = True)\n    #test_df['part_1_boolean'].fillna(0, inplace = True)\n    #test_df['part_2_boolean'].fillna(0, inplace = True)\n    #test_df['part_3_boolean'].fillna(0, inplace = True)\n    #test_df['part_4_boolean'].fillna(0, inplace = True)\n    #test_df['part_5_boolean'].fillna(0, inplace = True)\n    #test_df['part_6_boolean'].fillna(0, inplace = True)\n    #test_df['part_7_boolean'].fillna(0, inplace = True)\n    test_df['type_of_concept_boolean'].fillna(0, inplace = True)\n    test_df['type_of_intention_boolean'].fillna(0, inplace = True)\n    test_df['type_of_solving_question_boolean'].fillna(0, inplace = True)\n    test_df['type_of_starter_boolean'].fillna(0, inplace = True)\n    test_df['num_tags'].fillna(0, inplace = True)\n    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n    test_df['part'] = test_df.part - 1\n\n    test_df['part'].fillna(4, inplace = True)\n    test_df['avg_questions_seen'].fillna(1, inplace = True)\n    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    test_df_train = test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n            'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n            'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n            'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n            #'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n            'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean','num_tags',]]\n    \n    test_df_values = test_df_train.values #returns a numpy array\n    min_max_scaler = preprocessing.MinMaxScaler()\n    test_df_values = min_max_scaler.fit_transform(test_df_values)\n\n    test_df['answered_correctly'] =  model_DNN.predict(test_df_values)\n    #print(test_df)\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","be11db6c":"Modeling with DNN","a1d62b5e":"Merging Data","02686768":"Training Dataset","f4576f94":"# smaller learning rate showed better curve, but larger one e.g. 0.001 has better result","9d513990":"use lecture","30ce0519":"Affirmatives (True) for content_type_id are only for those with a different type of content (lectures). These are not real questions.","399eaa26":"Extracting Training Data","1f4f6292":"# select columns for modeling","9d85d57d":"Making Predictions for New Data","6d0dcc91":"Data Exploration","58558c30":"Does it make sense to use last questions as validation? Why is the rate of correct answers so low? I am convinced there is a better way to match the test data.","e4802807":"Creating Validation Set (Most Recent Answers by User)","e7b2b797":"# add num_tags as an additional feature","45cc6ff1":"2% is lecture","67e06b3f":"Reading Data and Importing Libraries","eaf0eb7f":"Data Normalization"}}