{"cell_type":{"6e21171c":"code","350c7faa":"code","7d9f1755":"code","c713dec9":"code","0ba0bcf7":"code","8a9e1b1d":"code","2e1796be":"code","a4a16ccf":"code","44819190":"code","c218141b":"code","c71e348e":"code","43a83ba8":"code","41196da3":"code","c2dbe3e3":"code","557facb4":"code","2dd75b2e":"code","89d7d7a3":"code","45dff71b":"markdown","79d0fece":"markdown","fca8c79a":"markdown","e5a37a04":"markdown","902b7afc":"markdown","825b5b61":"markdown","c2441e45":"markdown","4cf506ba":"markdown","2cf75858":"markdown","64cfbe85":"markdown","93b414a1":"markdown","ae570fd7":"markdown","7c449e01":"markdown","a51b2df5":"markdown"},"source":{"6e21171c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","350c7faa":"!wget http:\/\/www.robots.ox.ac.uk\/~vgg\/data\/flowers\/102\/102flowers.tgz\n!tar -xzf 102flowers.tgz\n!rm 102flowers.tgz\n!wget http:\/\/www.robots.ox.ac.uk\/~vgg\/data\/flowers\/102\/imagelabels.mat","7d9f1755":"import os\nimport numpy as np\nimport scipy.io\nimport cv2\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split","c713dec9":"img_labels = scipy.io.loadmat(\"imagelabels.mat\")\nimg_labels = img_labels[\"labels\"]\nimg_labels = img_labels[0]\nfor i in range(len(img_labels)):\n  img_labels[i] = img_labels[i] - 1","0ba0bcf7":"train_x = []\ntrain_y = []\ndir = \"jpg\/\"\nfor imgs in os.listdir(dir):\n  img_num = int(imgs[7:11])-1\n  train_y.append(img_labels[img_num])\n  image = cv2.imread(os.path.join(dir, imgs))\n  resized = cv2.resize(image, (150,150))\n  normalized_img = cv2.normalize(resized, None, alpha=0, beta=1, \n                            norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n  train_x.append(normalized_img)\ntrain_x = np.array(train_x)","8a9e1b1d":"trainx, valx, trainy, valy = train_test_split(train_x, train_y, test_size=0.15, random_state=10)","2e1796be":"print('Training Dataset Shape: \u00ad{}'.format(trainx.shape))\nprint('No. of Training Dataset Labels: {}'.format(len(trainy)))","a4a16ccf":"training_images= trainx\/255.0\ntest_images=valx\/255.0\n\ntraining_images = trainx.reshape((6960,150,150,3))\nvalx = valx.reshape((1229,150,150,3))\nprint('Training Dataset Shape: \u00ad{}'.format(trainx.shape))\nprint('No. of Training Dataset Labels: {}'.format(len(trainy)))\nprint('Test Dataset Shape: {}'.format(valx.shape))\nprint('No. of Test Dataset Labels: {}'.format(len(valy)))","44819190":"trainy = to_categorical(trainy)\nvaly = to_categorical(valy)","c218141b":"# fn to create weights\ndef create_weights(shape):\n    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n # fn to create biases\ndef create_biases(size):\n    return tf.Variable(tf.constant(0.05, shape=[size]))\n\n# fn to create convolutional layer\ndef create_convolutional_layer(input,num_input_channels, conv_filter_size, num_filters):  \n    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n    biases = create_biases(num_filters)\n \n    layer = tf.nn.conv2d(input=input, filter=weights,strides=[1, 1, 1, 1],padding='SAME')\n    layer += biases\n    layer = tf.nn.relu(layer)  \n    layer = tf.nn.max_pool(value=layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n    \n    return layer\n\n# fn to create flatten layer\ndef create_flatten_layer(layer):\n    layer_shape = layer.get_shape()\n    num_features = layer_shape[1:4].num_elements()\n    layer = tf.reshape(layer, [-1, num_features])\n \n    return layer\n \n # fn to create fully connected layers\ndef create_fc_layer(input, num_inputs, num_outputs, use_relu=True):\n    weights = create_weights(shape=[num_inputs, num_outputs])\n    biases = create_biases(num_outputs)\n    \n    if use_relu:\n        layer = tf.add(tf.matmul(input, weights), biases)\n        layer = tf.nn.relu(layer)\n    else:\n        layer = tf.add(tf.matmul(input, weights), biases, name='y_preds')\n\n    return layer","c71e348e":"## INITIALIZING CONSTANTS\nx = tf.placeholder(tf.float32, shape=[None, 150,150,3], name='x')\ny = tf.placeholder(tf.float32, shape=[None, 102], name='y')\nNUM_EPOCHS = 2\nBATCH_SIZE = 500\nKEEP_PROB = 0.5","43a83ba8":"## BUILDING CNN\n\n# Adding 1st convolutional layer\n\nblock1_conv1 = create_convolutional_layer(input=x, num_input_channels=3, conv_filter_size=3, num_filters=64)\nblock1_conv2 = create_convolutional_layer(input=block1_conv1, num_input_channels=64,conv_filter_size=3, num_filters=128) \nbatch1 = tf.layers.batch_normalization(block1_conv2) \ndrop1 = tf.nn.dropout(batch1, KEEP_PROB)\n\n# Adding 2nd convolutional layer\nblock2_conv1 = create_convolutional_layer(input=drop1, num_input_channels=128, conv_filter_size=3, num_filters=128)\nblock2_conv2 = create_convolutional_layer(input=block2_conv1, num_input_channels=128, conv_filter_size=3, num_filters=256)\nbatch2 = tf.layers.batch_normalization(block2_conv2) \ndrop2 = tf.nn.dropout(batch2, KEEP_PROB)\n\n# Adding 3rd convolutional layer\nblock3_conv1 = create_convolutional_layer(input=drop2, num_input_channels=256, conv_filter_size=3, num_filters=256)\nblock3_conv2 = create_convolutional_layer(input=block3_conv1, num_input_channels=256, conv_filter_size=3, num_filters=512)\nbatch3 = tf.layers.batch_normalization(block3_conv2) \ndrop3 = tf.nn.dropout(batch3, KEEP_PROB)\nlayer_flat = create_flatten_layer(drop3)\n\nlayer_fc1 = create_fc_layer(input=layer_flat, num_inputs=layer_flat.get_shape()[1:4].num_elements(), num_outputs=1024, use_relu=True)\nbatch5 = tf.layers.batch_normalization(layer_fc1)\ndrop5 = tf.nn.dropout(batch5, KEEP_PROB)\n\n#output layer\ny_preds = create_fc_layer(input=drop5, num_inputs=1024, num_outputs=102, use_relu=False)","41196da3":"## CALCULATING COST AND ACCURACY\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_preds, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\ncorrect_pred = tf.equal(tf.argmax(y_preds, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')","c2dbe3e3":"init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(NUM_EPOCHS):\n      for batch in range(int(len(trainx)\/BATCH_SIZE)):\n        batch_x = trainx[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(trainx))]\n        batch_y = trainy[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(trainy))]\n\n        opt = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n\n      for batch in range(int(len(valx)\/BATCH_SIZE)):\n        val_batch_x = valx[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(valx))]\n        val_batch_y = valy[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(valy))]\n\n        val_loss, val_acc= sess.run([cost, accuracy], feed_dict={x: val_batch_x, y: val_batch_y})\n        \n      print(\"Epoch \"+str(epoch+1)+\": Train Loss= \"+\"{:.4f}\".format(loss)+\"   Train Accuracy= \" +  \"{:.4f}\".format(acc)+\n              \"   Valid Loss= \"+\"{:.4f}\".format(val_loss)+\"   Valid Accuracy= \" + \"{:.4f}\".format(val_acc))\n\n    ## SAVING THE MODEL\n    os.mkdir('\/model5')\n    tf.saved_model.simple_save(sess, '\/model5', inputs={\"x\": x}, outputs={\"y_preds\": y_preds})\n    print('--- MODEL SAVED ---')","557facb4":"graph = tf.Graph()\nwith graph.as_default():\n    with tf.Session(graph=graph) as sess:\n        tf.saved_model.loader.load(sess, [\"serve\"], '\/model5')\n\n        x = graph.get_tensor_by_name('x:0')\n        y_preds = graph.get_tensor_by_name('y_preds:0')\n        \n        y_true = []\n        preds = []\n        for batch in range(int(len(valx)\/BATCH_SIZE)):\n          batch_x = valx[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(valx))]\n          batch_y = valy[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(valy))]\n\n          y_true.append(batch_y)\n          preds.append(sess.run(y_preds, feed_dict={x: batch_x}))\n\n        y_true = np.stack(np.array(y_true), axis=0)\n        preds = np.stack(np.array(preds), axis=0)","2dd75b2e":"loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = tf.cast(preds, tf.float32), \n                                                              labels=tf.cast(y_true, tf.float32)))\ncorrect = tf.equal(np.argmax(preds, axis=2), np.argmax(y_true, axis=2))\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))","89d7d7a3":"with tf.Session() as sess:\n    print('Loss :',loss.eval())\n    print('Accuracy :', accuracy.eval())","45dff71b":"## Pre-Processing\n\nLoading labels:","79d0fece":"Then initializing constants which will be used further like Batch size and Epochs.","fca8c79a":"## Inferencing the saved model","e5a37a04":"# Multiclass classification using Tensorflow","902b7afc":"## Building the model","825b5b61":"## Future Implementation\nAs this is just a basic model for learning phase, these things can be further done to improve effeciency:\n\n* As dataset was small, so need of data augumentation.\n* Finding more architectures to improve the accuracy.","c2441e45":"Reshaping and exploring data again:","4cf506ba":"## Training and saving model:","2cf75858":"## Understanding Dataset\n\nThe dataset which we will work on is 102 flower classification.\n\nThis dataset contains flowers of 102 categories, each class consisting of between 40 and 258 images. As classes were quite many so accordingly dataset was quite less which was a total of 8,189 images.\n\nThe data format is simple, a directory containing images and a .mat file containing labels.\n\n### Loading Datase\nYou can use this code to download and extract data:","64cfbe85":"#### Accuracy is very low because of less training epoches because I wrote this code to show just the impletation part, Increase them to get better results.","93b414a1":"Converting to categorical:","ae570fd7":"As always we will start with importing needed libraries:","7c449e01":"Loading images and converting them to NumPy array:","a51b2df5":"Splitting data in training and testing sets:"}}