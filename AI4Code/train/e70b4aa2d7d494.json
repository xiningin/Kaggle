{"cell_type":{"9e1f418c":"code","df1a7fea":"code","0e1e7cd2":"code","872badbd":"code","529f45a2":"code","ab385ef4":"code","7b4326d7":"code","fba5c718":"code","3847f28d":"code","5e896c91":"code","a850031d":"code","8f1d6660":"code","63fff00c":"code","6bb33679":"code","2613b8eb":"code","00b47def":"code","f1214849":"code","69e274ea":"code","7f74c23c":"code","85bdbe89":"code","b4a1ef2d":"code","ffaff19b":"code","bd2058d7":"code","9c96fb4e":"code","1ad60744":"code","efd04aa6":"code","4356652d":"code","d75a3a2d":"code","8c43a680":"code","bb9a5be7":"code","0cc1ff59":"markdown","3c5cb6e5":"markdown","0f459d7d":"markdown","bd4314ae":"markdown","e995ab4e":"markdown","3ee1c285":"markdown","bb1e8615":"markdown","808abc55":"markdown","b5c35164":"markdown","d3ed1703":"markdown","9f154be8":"markdown","02ab8bd0":"markdown","a9f526d7":"markdown","8ab8bb43":"markdown","91694b28":"markdown"},"source":{"9e1f418c":"MAX_SAMPLE = None # set a small number for experimentation, set None for production.","df1a7fea":"!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl","0e1e7cd2":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrandom.seed(123)\nnp.random.seed(456)","872badbd":"COMPUTE_EVAL = False","529f45a2":"if COMPUTE_EVAL:\n    df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\n    df['split'] = np.random.randn(df.shape[0], 1)\n\n    msk = np.random.rand(len(df)) <= 0.9\n\n    train = df[msk]\n    train2 = train\n    val = df[~msk]\nelse:\n    train_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\n    train = pd.read_csv(train_path)\n    train = train[:MAX_SAMPLE]\n\npaper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\npapers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","ab385ef4":"if COMPUTE_EVAL:\n    sample_submission = val\n    paper_test_folder = paper_train_folder\nelse: \n    sample_submission_path = '..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv'\n    sample_submission = pd.read_csv(sample_submission_path)\n\n    paper_test_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/test'\n\nfor paper_id in sample_submission['Id']:\n    with open(f'{paper_test_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper\n    ","7b4326d7":"all_labels = set()\n\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')","fba5c718":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt","3847f28d":"literal_preds = []\n\nfor paper_id in sample_submission['Id']:\n    paper = papers[paper_id]\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = totally_clean_text(text_1)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_text(label))\n    \n    literal_preds.append('|'.join(labels))\n","5e896c91":"literal_preds[:5]","a850031d":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nPREDICT_BATCH = 64000 \n\n#SciBERT finetuned without validation set\nPRETRAINED_PATH = '..\/input\/coleridgescibertfinetunedsmall\/output'\nTEST_INPUT_SAVE_PATH = '.\/input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\nTRAIN_PATH = '..\/input\/coleridgescibertfinetunedsmall\/train_ner.json'\nVAL_PATH = '..\/input\/coleridgescibertfinetunedsmall\/train_ner.json'\n\n#SciBERT\n# PRETRAINED_PATH = '..\/input\/coleridgescibert\/output'\n# TEST_INPUT_SAVE_PATH = '.\/input_data'\n# TEST_NER_DATA_FILE = 'test_ner_input.json'\n# TRAIN_PATH = '..\/input\/coleridgescibert\/train_ner.json'\n# VAL_PATH = '..\/input\/coleridgescibert\/train_ner.json'\n\n#Normal BERT\n# PRETRAINED_PATH = '..\/input\/coleridge-bert-models\/output'\n# TEST_INPUT_SAVE_PATH = '.\/input_data'\n# TEST_NER_DATA_FILE = 'test_ner_input.json'\n# TRAIN_PATH = '..\/input\/coleridge-bert-models\/train_ner.json'\n# VAL_PATH = '..\/input\/coleridge-bert-models\/train_ner.json'\n\nPREDICTION_SAVE_PATH = '.\/pred'\nPREDICTION_FILE = 'test_predictions.txt'","8f1d6660":"train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","63fff00c":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences","6bb33679":"test_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in sample_submission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n        \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words)\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')","2613b8eb":"os.environ[\"MODEL_PATH\"] = f\"{PRETRAINED_PATH}\"\nos.environ[\"TRAIN_FILE\"] = f\"{TRAIN_PATH}\"\nos.environ[\"VALIDATION_FILE\"] = f\"{VAL_PATH}\"\nos.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}\/{TEST_NER_DATA_FILE}\"\nos.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"","00b47def":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp \/kaggle\/input\/coleridge-packages\/my_seqeval.py .\/\n\n# make necessart directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)","f1214849":"def bert_predict():\n    !python ..\/input\/kaggle-ner-utils\/kaggle_run_ner.py \\\n    --model_name_or_path \"$MODEL_PATH\" \\\n    --train_file \"$TRAIN_FILE\" \\\n    --validation_file \"$VALIDATION_FILE\" \\\n    --test_file \"$TEST_FILE\" \\\n    --output_dir \"$OUTPUT_DIR\" \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict","69e274ea":"bert_outputs = []\n\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n    # write data rows to input file\n    with open(f'{TEST_INPUT_SAVE_PATH}\/{TEST_NER_DATA_FILE}', 'w') as f:\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n            json.dump(row, f)\n            f.write('\\n')\n    \n    # remove output dir\n    !rm -r \"$OUTPUT_DIR\"\n    \n    # do predict\n    bert_predict()\n    \n    # read predictions\n    with open(f'{PREDICTION_SAVE_PATH}\/{PREDICTION_FILE}') as f:\n        this_preds = f.read().split('\\n')[:-1]\n        bert_outputs += [pred.split() for pred in this_preds]","7f74c23c":"# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]\n\ndel test_rows","85bdbe89":"bert_dataset_labels = [] # store all dataset labels for each publication\n\nfor length in paper_length:\n    labels = set()\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n        curr_phrase = ''\n        for word, tag in zip(sentence, pred):\n            if tag == 'B': # start a new phrase\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n                curr_phrase = word\n            elif tag == 'I' and curr_phrase: # continue the phrase\n                curr_phrase += ' ' + word\n            else: # end last phrase (if any)\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n        # check if the label is the suffix of the sentence\n        if curr_phrase:\n            labels.add(curr_phrase)\n            curr_phrase = ''\n    \n    # record dataset labels for this publication\n    bert_dataset_labels.append(labels)\n    \n    del test_sentences[:length], bert_outputs[:length]","b4a1ef2d":"bert_dataset_labels[:5]","ffaff19b":"def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union\n\nfiltered_bert_labels = []\n\nfor labels in bert_dataset_labels:\n    filtered = []\n    \n    for label in sorted(labels, key=len):\n        label = clean_text(label)\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n            filtered.append(label)\n    \n    filtered_bert_labels.append('|'.join(filtered))","bd2058d7":"filtered_bert_labels[:5]","9c96fb4e":"final_predictions = []\nliteral_counter = 0\nfor literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n    if bert_pred:\n        final_predictions.append(bert_pred)\n    else:\n        literal_counter = literal_counter + 1\n        #print(\"we used Literal Matching\")\n        final_predictions.append(literal_match)\n        \nprint(\"literal counts: \", str(literal_counter))","1ad60744":"sample_submission['PredictionString'] = final_predictions\nsample_submission.head()","efd04aa6":"# t = {'cleaned_label': sample_submission['cleaned_label'], 'Prediction_string': final_predictions}\n# df = pd.DataFrame(t)\n# #dict(list(t.items())[:0])\n# print(df.loc[2,:])\n# print(len(sample_submission['pub_title']))","4356652d":"def getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.PredictionString.split('|'), row[col]))\n        return 2*n \/ (len(row.PredictionString.split('|')) + len(row[col]))\n    return f1score\n\ndef my_jaccard(strs): \n    str1, str2 = strs\n    temp_list = []\n    for sentence in str1.lower().split('|'):\n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        d = float(len(c)) \/ (len(a) + len(b) - len(c))\n        temp_list.append(d)\n    return sum(temp_list) \/ len(temp_list)   ","d75a3a2d":"if COMPUTE_EVAL:\n    getMetric_score = sample_submission.apply(getMetric('cleaned_label'), axis=1)\n    print('getMetric_score =', getMetric_score.mean())\n    my_jaccard_score = sample_submission[['PredictionString', 'cleaned_label']].apply(my_jaccard, axis=1)\n    print('my_jaccard_score =', my_jaccard_score.mean())\n    ","8c43a680":"#sample_submission.head()","bb9a5be7":"sample_submission.to_csv(f'submission.csv', index=False)","0cc1ff59":"### Transform data to NER format","3c5cb6e5":"# Bert prediction","0f459d7d":"# Aggregate final predictions and write submission file","bd4314ae":"# Load data","e995ab4e":"### Do predict and collect results","3ee1c285":"### Create a knowledge bank","bb1e8615":"# Install packages","808abc55":"### Restore Dataset labels from predictions","b5c35164":"### Matching on test data","d3ed1703":"### Paths and Hyperparameters","9f154be8":"Group by publication, training labels should have the same form as expected output.","02ab8bd0":"### Filter based on Jaccard score and clean","a9f526d7":"# Literal matching","8ab8bb43":"This notebook gives a simple combination of literal matching and Named Entity Recognition using BERT (base model from huggingface).\n\nThe training phase of the BERT model was done in another kernel: Pytorch BERT for Named Entity Recognition.","91694b28":"# Import"}}