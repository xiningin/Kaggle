{"cell_type":{"bc21c4db":"code","bba15461":"code","4200d767":"code","6213b2a7":"code","bbd73841":"code","10f0a6b4":"code","8fb6cfa6":"code","ce5274b6":"code","51524c3e":"code","75b95864":"code","5c597085":"code","923b03ec":"code","bbb996ea":"code","62c81ce4":"code","031eafd5":"code","8ab1d054":"code","cc31fb36":"code","5b1b00c9":"code","72d286b7":"code","ab394d27":"code","32939db5":"code","9bbc5596":"code","20b76da5":"code","cf668e6d":"code","6b26cd65":"code","c06490a4":"code","d3f3ab7a":"code","44392e69":"code","28820dbf":"code","927ec1c6":"code","85f46306":"code","98b41d0c":"code","0fbac376":"code","6cff34a6":"code","da093aae":"code","df3f1ade":"code","42f8631f":"code","d6e0f940":"code","567d094d":"code","3ab3c8c5":"code","e67715d9":"code","067a6338":"code","7e354e9c":"code","62e439ad":"code","1bbee203":"code","aeae8478":"code","f7574552":"code","2723f991":"code","9d0fa11d":"code","59449911":"code","bcec7e5d":"code","56d20a4e":"code","76753e73":"code","87c4d884":"code","2afd3c6d":"code","b562f6f4":"code","214264a0":"code","6210c2d4":"code","b7ffbcae":"code","b6f5b30d":"code","00f3e2f7":"code","e9fd9687":"code","ab922461":"code","614003fc":"code","41e39f75":"code","354f37d1":"code","e1b5b760":"code","eab229ab":"code","abd40166":"code","3d3bdce0":"code","63d656d7":"code","60f21ab2":"code","64d20259":"code","71f72802":"code","d8223b48":"code","a4b017ae":"code","a1e43339":"code","1503bb07":"code","94857bf4":"code","75702053":"code","42a3f28b":"code","5557fb9a":"markdown","409a8082":"markdown","22c767ab":"markdown","6b593508":"markdown","276cd04e":"markdown","2d795e9e":"markdown","9016fe6a":"markdown","f87ddead":"markdown","d2faab99":"markdown","9c4c4f67":"markdown","d265a89a":"markdown","34db54a9":"markdown","86e6c63c":"markdown","615e4ac4":"markdown","e1229c89":"markdown","7969094d":"markdown","da3ba9a7":"markdown","d5bb0855":"markdown","accda3b8":"markdown","72d1d62e":"markdown","18799eb5":"markdown","cdbb9268":"markdown","d5722fcd":"markdown","7d6d216b":"markdown","1887dd5f":"markdown","4cf29898":"markdown","3b47c472":"markdown","964b1fb6":"markdown","d3a722d2":"markdown","210dad56":"markdown","983541a9":"markdown","ca60e58e":"markdown","f44ba6bd":"markdown","3ef12d7e":"markdown","de7d592b":"markdown","119035af":"markdown","8b736955":"markdown","0a9a5195":"markdown","fd571ebb":"markdown","960a442b":"markdown"},"source":{"bc21c4db":"#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","bba15461":"#import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n#take a look at the training data\ntrain.describe(include=\"all\")","4200d767":"#get a list of the features within the dataset\nprint(train.columns)","6213b2a7":"#see a sample of the dataset to get an idea of the variables\ntrain.sample(5)","bbd73841":"#see a summary of the training dataset\ntrain.describe(include = \"all\")","10f0a6b4":"#check for any other unusable values\nprint(pd.isnull(train).sum())","8fb6cfa6":"train.hist(bins=10,figsize=(9,7),grid=False);","ce5274b6":"g = sns.FacetGrid(train, col=\"Sex\", row=\"Survived\", margin_titles=True)\ng.map(plt.hist, \"Age\",color=\"purple\");","51524c3e":"g = sns.FacetGrid(train, hue=\"Survived\", col=\"Pclass\", margin_titles=True,\n                  palette={1:\"seagreen\", 0:\"gray\"})\ng=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();","75b95864":"g = sns.FacetGrid(train, hue=\"Survived\", col=\"Sex\", margin_titles=True,\n                palette=\"Set1\",hue_kws=dict(marker=[\"^\", \"v\"]))\ng.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('Survival by Gender , Age and Fare');","5c597085":"train.Embarked.value_counts().plot(kind='bar', alpha=0.55)\nplt.title(\"Passengers per boarding location\");","923b03ec":"sns.set(font_scale=1)\ng = sns.factorplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\",\n                    data=train, saturation=.5,\n                    kind=\"bar\", ci=None, aspect=.6)\n(g.set_axis_labels(\"\", \"Survival Rate\")\n    .set_xticklabels([\"Men\", \"Women\"])\n    .set_titles(\"{col_name} {col_var}\")\n    .set(ylim=(0, 1))\n    .despine(left=True))  \nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How many Men and Women Survived by Passenger Class');","bbb996ea":"import matplotlib.pyplot as plt\nax = sns.boxplot(x=\"Survived\", y=\"Age\", \n                data=train)\nax = sns.stripplot(x=\"Survived\", y=\"Age\",\n                   data=train, jitter=True,\n                   edgecolor=\"gray\")\nplt.title(\"Survival by Age\",fontsize=12);","62c81ce4":"train.Age[train.Pclass == 1].plot(kind='kde')    \ntrain.Age[train.Pclass == 2].plot(kind='kde')\ntrain.Age[train.Pclass == 3].plot(kind='kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\")\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'),loc='best') ;","031eafd5":"sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","8ab1d054":"sns.stripplot(x=\"Survived\", y=\"Age\", data=train, jitter=True)","cc31fb36":"sns.factorplot(\"Pclass\", \"Survived\", hue = \"Sex\", data = train)\nplt.show()","5b1b00c9":"pd.crosstab([train[\"Sex\"], train[\"Survived\"]], train[\"Pclass\"], \n            margins = True).style.background_gradient(cmap = \"summer_r\")","72d286b7":"#draw a bar plot of survival by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n#print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","ab394d27":"#draw a bar plot of survival by Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n#print percentage of people by Pclass that survived\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","32939db5":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\n#I won't be printing individual percent values for all of these.\nprint(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)","9bbc5596":"#draw a bar plot for Parch vs. survival\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()","20b76da5":"#sort the ages into logical categories\ntrain[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","cf668e6d":"train[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n#calculate percentages of CabinBool vs. survived\nprint(\"Percentage of CabinBool = 1 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of CabinBool = 0 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\nplt.show()","6b26cd65":"test.describe(include=\"all\")","c06490a4":"#we'll start off by dropping the Cabin feature since not a lot more useful information can be extracted from it.\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)","d3f3ab7a":"#we can also drop the Ticket feature since it's unlikely to yield any useful information\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)","44392e69":"#now we need to fill in the missing values in the Embarked feature\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","28820dbf":"#replacing the missing values in the Embarked feature with S\ntrain = train.fillna({\"Embarked\": \"S\"})","927ec1c6":"#create a combined group of both datasets\ncombine = [train, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","85f46306":"#replace various titles with more common names\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","98b41d0c":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","0fbac376":"# fill missing age with mode age group for each title\nmr_age = train[train[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train[train[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train[train[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train[train[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train[train[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train[train[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\n#I tried to get this code to work with using .map(), but couldn't.\n#I've put down a less elegant, temporary solution for now.\n#train = train.fillna({\"Age\": train[\"Title\"].map(age_title_mapping)})\n#test = test.fillna({\"Age\": test[\"Title\"].map(age_title_mapping)})\n\nfor x in range(len(train[\"AgeGroup\"])):\n    if train[\"AgeGroup\"][x] == \"Unknown\":\n        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n        \nfor x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]","6cff34a6":"#map each Age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\ntrain.head()\n\n#dropping the Age feature for now, might change\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","da093aae":"#drop the name feature since it contains no more useful information.\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","df3f1ade":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","42f8631f":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","d6e0f940":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","567d094d":"#check train data\ntrain.head()","3ab3c8c5":"#check test data\ntest.head()","e67715d9":"from sklearn.model_selection import train_test_split\n\nX = train.drop(['Survived', 'PassengerId'], axis=1)\nY = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.22, random_state = 0)","067a6338":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred_NB = gaussian.predict(x_test)\nacc_gaussian = round(accuracy_score(y_pred_NB, y_test) * 100, 2)\nprint(acc_gaussian)","7e354e9c":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_NB))","62e439ad":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_NB)","1bbee203":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred_lg = logreg.predict(x_test)\nacc_logreg = round(accuracy_score(y_pred_lg, y_test) * 100, 2)\nprint(acc_logreg)","aeae8478":"print(classification_report(y_test, y_pred_lg))","f7574552":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_lg)","2723f991":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred_svm = svc.predict(x_test)\nacc_svc = round(accuracy_score(y_pred_svm, y_test) * 100, 2)\nprint(acc_svc)","9d0fa11d":"print(classification_report(y_test, y_pred_svm))","59449911":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_svm)","bcec7e5d":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred_svc = linear_svc.predict(x_test)\nacc_linear_svc = round(accuracy_score(y_pred_svc, y_test) * 100, 2)\nprint(acc_linear_svc)","56d20a4e":"print(classification_report(y_test, y_pred_svc))","76753e73":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_svc)","87c4d884":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred_per = perceptron.predict(x_test)\nacc_perceptron = round(accuracy_score(y_pred_per, y_test) * 100, 2)\nprint(acc_perceptron)","2afd3c6d":"print(classification_report(y_test, y_pred_per))","b562f6f4":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_per)","214264a0":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred_dt = decisiontree.predict(x_test)\nacc_decisiontree = round(accuracy_score(y_pred_dt, y_test) * 100, 2)\nprint(acc_decisiontree)","6210c2d4":"print(classification_report(y_test, y_pred_dt))","b7ffbcae":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_dt)","b6f5b30d":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred_rf = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred_rf, y_test) * 100, 2)\nprint(acc_randomforest)","00f3e2f7":"print(classification_report(y_test, y_pred_rf))","e9fd9687":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_rf)","ab922461":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred_knn = knn.predict(x_test)\nacc_knn = round(accuracy_score(y_pred_knn, y_test) * 100, 2)\nprint(acc_knn)","614003fc":"print(classification_report(y_test, y_pred_knn))","41e39f75":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_knn)","354f37d1":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred_sgd = sgd.predict(x_test)\nacc_sgd = round(accuracy_score(y_pred_sgd, y_test) * 100, 2)\nprint(acc_sgd)","e1b5b760":"print(classification_report(y_test, y_pred_sgd))","eab229ab":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_sgd)","abd40166":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred_gbc = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred_gbc, y_test) * 100, 2)\nprint(acc_gbk)","3d3bdce0":"print(classification_report(y_test, y_pred_gbc))","63d656d7":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred_gbc)","60f21ab2":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","64d20259":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","71f72802":"#Visualizing the model's ROC curve (**source for graph code given below the plot)\nfrom sklearn.metrics import roc_curve, auc\nlogreg.fit(x_train, y_train)\n\n\n \n# Determine the false positive and true positive rates\nFPR, TPR, _ = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])\n \n# Calculate the AUC\n\nroc_auc = auc(FPR, TPR)\nprint ('ROC AUC: %0.3f' % roc_auc )\n \n# Plot of a ROC curve\nplt.figure(figsize=(10,10))\nplt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Test Sample Performance)')\nplt.legend(loc=\"lower right\")\nplt.show()","d8223b48":"#Visualizing the model's ROC curve (**source for graph code given below the plot)\nfrom sklearn.metrics import roc_curve, auc\ngaussian.fit(x_train, y_train)\n\n\n \n# Determine the false positive and true positive rates\nFPR, TPR, _ = roc_curve(y_test, gaussian.predict_proba(x_test)[:,1])\n \n# Calculate the AUC\n\nroc_auc = auc(FPR, TPR)\nprint ('ROC AUC: %0.3f' % roc_auc )\n \n# Plot of a ROC curve\nplt.figure(figsize=(10,10))\nplt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Test Sample Performance)')\nplt.legend(loc=\"lower right\")\nplt.show()","a4b017ae":"from sklearn.metrics import roc_curve, auc\ndecisiontree.fit(x_train, y_train)\n\n\n \n# Determine the false positive and true positive rates\nFPR, TPR, _ = roc_curve(y_test, decisiontree.predict_proba(x_test)[:,1])\n \n# Calculate the AUC\n\nroc_auc = auc(FPR, TPR)\nprint ('ROC AUC: %0.3f' % roc_auc )\n \n# Plot of a ROC curve\nplt.figure(figsize=(10,10))\nplt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Test Sample Performance)')\nplt.legend(loc=\"lower right\")\nplt.show()","a1e43339":"from sklearn.metrics import roc_curve, auc\nrandomforest.fit(x_train, y_train)\n\n\n \n# Determine the false positive and true positive rates\nFPR, TPR, _ = roc_curve(y_test, randomforest.predict_proba(x_test)[:,1])\n \n# Calculate the AUC\n\nroc_auc = auc(FPR, TPR)\nprint ('ROC AUC: %0.3f' % roc_auc )\n \n# Plot of a ROC curve\nplt.figure(figsize=(10,10))\nplt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Test Sample Performance)')\nplt.legend(loc=\"lower right\")\nplt.show()","1503bb07":"from sklearn.metrics import roc_curve, auc\nknn.fit(x_train, y_train)\n\n\n \n# Determine the false positive and true positive rates\nFPR, TPR, _ = roc_curve(y_test, knn.predict_proba(x_test)[:,1])\n \n# Calculate the AUC\n\nroc_auc = auc(FPR, TPR)\nprint ('ROC AUC: %0.3f' % roc_auc )\n \n# Plot of a ROC curve\nplt.figure(figsize=(10,10))\nplt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Test Sample Performance)')\nplt.legend(loc=\"lower right\")\nplt.show()","94857bf4":"from sklearn.metrics import roc_curve, auc\ngbk.fit(x_train, y_train)\n\n\n \n# Determine the false positive and true positive rates\nFPR, TPR, _ = roc_curve(y_test, gbk.predict_proba(x_test)[:,1])\n \n# Calculate the AUC\n\nroc_auc = auc(FPR, TPR)\nprint ('ROC AUC: %0.3f' % roc_auc )\n \n# Plot of a ROC curve\nplt.figure(figsize=(10,10))\nplt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Test Sample Performance)')\nplt.legend(loc=\"lower right\")\nplt.show()","75702053":"from sklearn.metrics import roc_curve, roc_auc_score\n# Instantiate the classfiers and make a list\nclassifiers = [LogisticRegression(), \n               GaussianNB(), \n               KNeighborsClassifier(), \n               DecisionTreeClassifier(),\n               RandomForestClassifier(),\n              GradientBoostingClassifier()]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(x_train, y_train)\n    y_pred = model.predict_proba(x_test)[::,1]\n    \n    fpr, tpr, _ = roc_curve(y_test,  y_pred)\n    auc = roc_auc_score(y_test, y_pred)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)","42a3f28b":"fig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","5557fb9a":"I decided to use the Gradient Boosting Classifier model for the testing data.","409a8082":"### Ticket Feature","22c767ab":"As the ROC curve for gradient Boosting is the best so we use the output with Gradient Boosting.","6b593508":"## Sources:\n* [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n* [Scikit-Learn ML from Start to Finish](https:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish?scriptVersionId=320209)\n# \n\nAny and all feedback is welcome! ","276cd04e":"## 4) Data Visualization\nIt's time to visualize our data so we can see whether our predictions were accurate! ","2d795e9e":"### Sex Feature","9016fe6a":"### Cabin Feature","f87ddead":"## 7) Creating Submission File\nIt's time to create a submission.csv file to upload to the Kaggle competition!","d2faab99":"As predicted, females have a much higher chance of survival than males. The Sex feature is essential in our predictions.","9c4c4f67":"## 3) Data Analysis\nWe're going to consider the features in the dataset and how complete they are. ","d265a89a":"# Titanic Survival Predictions (Beginner)\nI am a newbie to data science and machine learning, and will be attempting to work my way through the Titanic: Machine Learning from Disaster dataset. Please consider upvoting if this is useful to you! :)\n\n### Contents:\n1. Import Necessary Libraries\n2. Read In and Explore the Data\n3. Data Analysis\n4. Data Visualization\n5. Cleaning Data\n6. Choosing the Best Model\n7. Creating Submission File\n\nAny and all feedback is welcome! ","34db54a9":"### Age Feature","86e6c63c":"* **Numerical Features:** Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n* **Categorical Features:** Survived, Sex, Embarked, Pclass\n* **Alphanumeric Features:** Ticket, Cabin\n\n#### What are the data types for each feature?\n* Survived: int\n* Pclass: int\n* Name: string\n* Sex: string\n* Age: float\n* SibSp: int\n* Parch: int\n* Ticket: string\n* Fare: float\n* Cabin: string\n* Embarked: string\n\nNow that we have an idea of what kinds of features we're working with, we can see how much information we have about each of them.\n","615e4ac4":"### Age Feature","e1229c89":"### Sex Feature","7969094d":"### Embarked Feature","da3ba9a7":"### Splitting the Training Data\nWe will use part of our training data (22% in this case) to test the accuracy of our different models.","d5bb0855":"### Name Feature\nWe can drop the name feature now that we've extracted the titles.","accda3b8":"In general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)","72d1d62e":"### Testing Different Models\n\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n\nFor each model, we set the model, fit it with 78% of our training data, predict for 22% of the training data and check the accuracy.","18799eb5":"People with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children.","cdbb9268":"## 1) Import Necessary Libraries\nFirst off, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn.","d5722fcd":"## 5) Cleaning Data\nTime to clean our data to account for missing values and unnecessary information!","7d6d216b":"### SibSp Feature","1887dd5f":"### Fare Feature\nIt's time separate the fare values into some logical groups as well as filling in the single missing value in the test dataset.","4cf29898":"## 6) Choosing the Best Model","3b47c472":"People with a recorded Cabin number are, in fact, more likely to survive. (66.6% vs 29.9%)","964b1fb6":"Next we'll fill in the missing values in the Age feature. Since a higher percentage of values are missing, it would be illogical to fill all of them with the same value (as we did with Embarked). Instead, let's try to find a way to predict the missing ages. ","d3a722d2":"If you've come this far, congratulations and thank you for reading! \n\n*If you use any part of this notebook in a published kernel, credit (you can simply link back here) would be greatly appreciated. :)*","210dad56":"As predicted, people with higher socioeconomic class had a higher rate of survival. (62.9% vs. 47.3% vs. 24.2%)","983541a9":"### Embarked Feature","ca60e58e":"It's clear that the majority of people embarked in Southampton (S). Let's go ahead and fill in the missing values with S.","f44ba6bd":"Let's compare the accuracies of each model!","3ef12d7e":"### Parch Feature","de7d592b":"#### Some Observations:\n* There are a total of 891 passengers in our training set.\n* The Age feature is missing approximately 19.8% of its values. I'm guessing that the Age feature is pretty important to survival, so we should probably attempt to fill these gaps. \n* The Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. We'll probably drop these values from our dataset.\n* The Embarked feature is missing 0.22% of its values, which should be relatively harmless.","119035af":"Babies are more likely to survive than any other age group. ","8b736955":"We can see that except for the abovementioned missing values, no NaN values exist.","0a9a5195":"## 2) Read in and Explore the Data \nIt's time to read in our training and testing data using `pd.read_csv`, and take a first look at the training data using the `describe()` function.","fd571ebb":"### Looking at the Test Data\nLet's see how our test data looks!","960a442b":"### Pclass Feature"}}