{"cell_type":{"f57e76e4":"code","a08df883":"code","82ccc4d0":"code","990ea34b":"code","996f4691":"code","bd272091":"code","03405963":"code","efb9fef0":"code","e24025e4":"code","88cebcce":"code","a515a385":"code","a7f2bbcf":"code","7a104b1e":"code","2c70da2c":"code","985a6188":"code","0d366d44":"code","f0e0e946":"code","829578a7":"code","b472ef7e":"code","1dabbb98":"code","347561a3":"code","eb5429a5":"code","3bee1a69":"code","df835245":"code","faa516a6":"code","1a09e344":"code","4ba5b587":"code","093a0e91":"code","6ec58582":"code","9fc8b274":"code","b0b75659":"code","dbb79f96":"code","7605a7df":"code","49bfb092":"code","acef5907":"code","65eb3c85":"markdown"},"source":{"f57e76e4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm.autonotebook import tqdm","a08df883":"import json\nimport re\nfrom wordcloud import WordCloud,STOPWORDS\nfrom collections import Counter\nfrom nltk.probability import FreqDist\nfrom functools import partial\nimport matplotlib.pyplot as plt","82ccc4d0":"# https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_preprocess\/3\n!pip install tensorflow_text\nimport tensorflow_text as text","990ea34b":"preprocessor = hub.load(\"https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_preprocess\/3\")\n\n# Step 1: tokenize batches of text inputs.\ntext_inputs = [tf.keras.layers.Input(shape=(), dtype=tf.string),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0...] # This SavedModel accepts up to 2 text inputs.\ntokenize = hub.KerasLayer(preprocessor.tokenize)\ntokenized_inputs = [tokenize(segment) for segment in text_inputs]\n\n# Step 2 (optional): modify tokenized inputs.\npass\n\n# Step 3: pack input sequences for the Transformer encoder.\nseq_length = 128 \u00a0# Your choice here.\nbert_pack_inputs = hub.KerasLayer(\n\u00a0 \u00a0 preprocessor.bert_pack_inputs,\n\u00a0 \u00a0 arguments=dict(seq_length=seq_length)) \u00a0# Optional argument.\nencoder_inputs = bert_pack_inputs(tokenized_inputs)\n","996f4691":"# reading csv files and train & test file paths\ntrain_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","bd272091":"import re\nimport string\n\ndef text_cleaning(text):\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()    \n    return text","03405963":"train_df.head()","efb9fef0":"test_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntest_df","e24025e4":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","88cebcce":"%%time\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","a515a385":"%%time\ntqdm.pandas()   #tqdm is used to show any code running with a progress bar. \ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","a7f2bbcf":"%%time\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","7a104b1e":"words =list( train_df['cleaned_label'].values)\nstopwords=['ourselves', 'hers','the','of','and','in', 'between', 'yourself', 'but', 'again','of', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\nsplit_words=[]\nfor word in words:\n    lo_w=[]\n    list_of_words=str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist","2c70da2c":"#len(np.unique(allwords)) #187\n#np.unique(allwords)","985a6188":"mostcommon = FreqDist(allwords).most_common(100)\nwordcloud = WordCloud(width=1600, height=800, background_color='black', stopwords=STOPWORDS).generate(str(mostcommon))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Top 100 Most Common Words in cleaned_label', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()\n\nmostcommon_small = FreqDist(allwords).most_common(25)\nx, y = zip(*mostcommon_small)\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.tight_layout(pad=0)\nplt.title('Freq of 25 Most Common Words in cleaned_label', fontsize=60)\nplt.show()","0d366d44":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","f0e0e946":"labels = train_df['cleaned_label'].tolist()\nprint(len(labels))\nlabels[:5]","829578a7":"sentences = train_df['text'].tolist()\nprint(len(sentences))","b472ef7e":"tokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nprint(len(word_index))","1dabbb98":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\nprint(reverse_word_index[3])\n\ndef decode_sentence(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","347561a3":"#vocab_size = 10000\nvocab_size = len(word_index)\nembedding_dim = 16\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size = 20000","eb5429a5":"train_sentences = train_df['text'].tolist()\n#train_sentences[1]","3bee1a69":"test_sentences = sample_sub['text'].tolist()\n#test_sentences[1]","df835245":"train_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","faa516a6":"print(decode_sentence(train_padded[0]))\n#print(train_sentences[0])\n#print(train_labels[0])","1a09e344":"train_labels = train_df[\"cleaned_label\"].tolist()","4ba5b587":"train_labels_sequences = tokenizer.texts_to_sequences(train_labels)\n#train_labels_sequences","093a0e91":"# Need this block to get it to work with TensorFlow 2.x\nimport numpy as np\ntrain_padded = np.array(train_padded)\ntrain_labels = np.array(train_labels_sequences)\ntest_padded = np.array(test_padded)\n#test_labels = np.array(test_labels)","6ec58582":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(train_padded,train_labels,test_size=0.1, random_state=0)","9fc8b274":"print(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)","b0b75659":"from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","dbb79f96":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n","7605a7df":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(train_padded)","49bfb092":"svc = make_pipeline(StandardScaler(), SVC(kernel='linear'))\nsvc.fit(train_padded,train_labels)","acef5907":"predict = svc.predict(test_padded)\npredict","65eb3c85":"# Data Preprocessing"}}