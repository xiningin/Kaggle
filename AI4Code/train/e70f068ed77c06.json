{"cell_type":{"8b209208":"code","25bfac1d":"code","fe0ff0da":"code","5ea69700":"markdown","168255ba":"markdown","03df959d":"markdown","ed5067fa":"markdown"},"source":{"8b209208":"%%writefile otm.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","25bfac1d":"%%writefile \"anti_otm.py\"\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\na1, a2 = None, None\nlast_action = None # track my action.\n\n\n###########################################\n# Original agent with modifications marked ->\n###########################################\n\ndef anti_transition_agent(observation, configuration):\n    global T, P, a1, a2, last_action\n    if observation.step > 1:\n        a1 = last_action   # on me only; take mirrored view on game\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            probs = P[a1,:]\n            \n            probs += 0.63 * np.roll(probs, 1)    # This is the magic addition of phase\n            \n            result = (int(probs.argmax()) + 1) % 3   # Changed to argmax instead of stochastic\n        else:\n            result = int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = last_action    # on me only\n        result = int(np.random.randint(3))\n        \n    result = (result + 1) % 3  # beat what he would have done\n        \n    last_action = result\n        \n    return result","fe0ff0da":"from kaggle_environments import evaluate, make, utils\nenv = make(\"rps\", debug=True)\n\nnum_win=0\nnum_loss=0\nnum_matches=0\n\nfor _ in range(50):\n    env.reset()\n    result=env.run([\"anti_otm.py\", \"otm.py\"])\n    reward=result[-1][0][\"observation\"][\"reward\"]\n    if reward>20:\n        num_win+=1\n    if reward<-20:\n        num_loss+=1\n    num_matches+=1\n    \n    print(f\"{reward:+4.0f}, {num_matches:2d} matches, {num_win\/num_matches:5.1%} win, {num_loss\/num_matches:5.1%} loss\")","5ea69700":"# Anti Opponent Transition Matrix\n\nChanges are marked with comments. Opponents actions are completely ignored. The anti-agent starts being effective only if you add the phase.","168255ba":"# Beating a stochastic agent\n\nHere I present how you can beat stochastic agent for which you know the exact algorithm.\n\nThe first step is to copy the agent. Then you mirror the logic, by making it see what it would have seen as an enemy. This usually means, you feed *your own last action* instead of the lastOpponentAction into the logic. Moreover, you store the opponent action as \"your own action\" in your history - but here the agent does not store this history anyway. For deterministic bots without randomness you could already get a perfect score (e.g. [Anti Statistical](https:\/\/www.kaggle.com\/superant\/anti-statistical)).\n\nFor a stochastic bot you may already have a tiny edge over the original, but you can make it much bigger by rotating the probability action distribution clockwise (see [Geometry of RPS](https:\/\/www.kaggle.com\/c\/rock-paper-scissors\/discussion\/210305)). Rotating is very similar to adding a random variable where one of three probabilities is zero.\n\nIn the latest version I write this on probabilities directly, rather than using complex numbers. \n\nThe rotation and the magic number which is used here before argmax is: `probs += 0.63 * np.roll(probs, 1)`. Seeing this in terms of complex number multiplication on the representations, this value is suspiciously close to \\\\(\\exp(2\\pi i \\cdot \\frac{1}{9})\\\\)\n\nSeveral interesting questions arise:\n\n* Why is the phase shift so sensitive and why is `0.63` a good value for different stochastic agents? Or can you find a better value?\n* Can you do better than a constant shift?\n* Can you use this technique without knowing the opponent's code?\n* Can you detect whether the opponent is vulnerable to such a tactics (i.e. detect if he is OTM) without trying out (inducing) this strategy?","03df959d":"# Original Opponent Transition Matrix\n\nHere is the original [Opponent Transition Matrix](https:\/\/www.kaggle.com\/group16\/rps-opponent-transition-matrix): ","ed5067fa":"# Evaluation"}}