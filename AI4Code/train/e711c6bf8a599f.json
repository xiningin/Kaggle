{"cell_type":{"cf3e8e5f":"code","31545518":"code","862f6ae4":"code","9d7e0663":"code","0258d2c2":"code","4f7f028e":"code","15ca7015":"code","e134c80f":"code","05f26555":"code","13fd672a":"code","a3b09683":"code","38be820d":"code","f08cce90":"code","2e046d77":"code","025f7cf5":"code","19901f6c":"code","7c53a4f8":"markdown","bbf0a591":"markdown","548690ea":"markdown","44be83cb":"markdown","c294eedd":"markdown","e9c50d5e":"markdown","a1db8c1d":"markdown","fd0f802d":"markdown","c8c80140":"markdown","768893eb":"markdown","11a1bce4":"markdown","2585d25b":"markdown"},"source":{"cf3e8e5f":"!pip install -U git+https:\/\/github.com\/lyft\/nuscenes-devkit","31545518":"from copy import deepcopy\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset, Box, Quaternion, view_points\n\n%matplotlib inline","862f6ae4":"df = pd.read_csv('..\/input\/lyftpredictionvisualization\/lyft.csv')","9d7e0663":"df.head()","0258d2c2":"# some black magic, see https:\/\/www.kaggle.com\/seshurajup\/starter-lyft-level-5-av-dataset-from-github#625566\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_images images\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_maps maps\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_lidar lidar\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_data data","4f7f028e":"lyft = LyftDataset(data_path='.', json_path='data', verbose=False)","15ca7015":"def get2Box(boxes_list, names_list, token, scores=None):\n    '''Given a list of boxes in `x,y,z,w,l,h,yaw` format, returns them in `Box` format\n    \n    Args:\n    boxes_list: a list of boxes in [x, y, z, w, l, h, yaw] format\n    names_list: classes the boxes belong to\n    token: token of the sample the boxes belong to\n    scores: predicted confidence scores, only for predicted boxes, \n    '''\n    boxes = []\n    for idx in range(len(boxes_list)):\n        center = boxes_list[idx, :3] # x, y, z\n        yaw = boxes_list[idx, 6]\n        size = boxes_list[idx, 3:6] # w, l, h\n        name = names_list[idx]\n        detection_score = 1.0 # for ground truths \n        if scores is not None:\n            detection_score = scores[idx]\n        quat = Quaternion(axis=[0, 0, 1], radians=yaw)\n        box = Box(\n            center=center,\n            size=size,\n            orientation=quat,\n            score=detection_score,\n            name=name,\n            token=token\n        )\n        boxes.append(box)\n    return boxes\n","e134c80f":"def get_pred_gt(pred_df, idx): \n    '''Given an index `idx`, this function reads ground truth and predicted strings and returns\n    corresponding boxes in `Box` format'''\n    \n    sample_token = pred_df.iloc[idx]['Id']\n    \n    string = pred_df.iloc[idx]['GroundTruthString'].split()\n    gt_objects = [string[x:x+8] for x in range(0, len(string), 8)]\n    \n    string = pred_df.iloc[idx]['PredictionString'].split()\n    pred_objects = [string[x:x+9] for x in range(0, len(string), 9)]\n    \n    # str -> float, in x,y,z,w,l,h,yaw format\n    gt_boxes = np.array([list(map(float, x[0:7])) for x in gt_objects])\n    gt_class = np.array([x[7] for x in gt_objects])\n    \n    pred_scores = np.array([float(x[0]) for x in pred_objects])\n    pred_boxes = np.array([list(map(float, x[1:8])) for x in pred_objects])\n    pred_class = np.array([x[8] for x in pred_objects])\n    \n    # x,y,z,w,l,h,yaw -> Box instance\n    predBoxes = get2Box(pred_boxes, pred_class, sample_token, scores=pred_scores)\n    gtBoxes = get2Box(gt_boxes, gt_class, sample_token)\n    \n    return predBoxes, gtBoxes ","05f26555":"def glb_to_sensor(box, sample_data):\n    '''Get a box from global frame to sensor's frame of reference '''\n    \n    box = box.copy() # v.imp\n    cs_record = lyft.get('calibrated_sensor', sample_data['calibrated_sensor_token'])\n    pose_record = lyft.get('ego_pose', sample_data['ego_pose_token'])\n    \n    # global to ego \n    box.translate(-np.array(pose_record['translation']))\n    box.rotate(Quaternion(pose_record['rotation']).inverse)\n    \n    # ego to sensor\n    box.translate(-np.array(cs_record['translation']))\n    box.rotate(Quaternion(cs_record['rotation']).inverse)\n    \n    return box","13fd672a":"# get predicted and ground boxes for each sample in `Box` format\npred_boxes = []\ngt_boxes = []\nfor idx in range(len(df)):\n    pBoxes, gBoxes = get_pred_gt(df, idx)\n    pred_boxes.append(pBoxes)\n    gt_boxes.append(gBoxes)","a3b09683":"# let's take a peek\nidx = 0\npred_boxes[idx][0]","38be820d":"gt_boxes[idx][0]","f08cce90":"idx = 0 # change this to visualize other samples\nsample_token = df.iloc[idx]['Id']\n\nsample = lyft.get('sample', sample_token)\nsample_data = lyft.get('sample_data', sample['data']['LIDAR_TOP'])\npath = sample_data['filename']\nlidar_points = np.fromfile(path, dtype=np.float32, count=-1).reshape([-1, 5])[:, :4]\n\n_, ax = plt.subplots(1, 1, figsize=(9, 9))\n\n# create colors based on the distance of the point from lidar\naxes_limit=40\n\ndists = np.sqrt(np.sum(lidar_points[:, :2] ** 2, axis=1))\ncolors = np.minimum(1, dists \/ axes_limit \/ np.sqrt(2))\nax.scatter(lidar_points[:, 0], lidar_points[:, 1], c=colors, s=0.2)\nax.plot(0, 0, \"x\", color=\"red\") # plot lidar location\n\n# Limit visible range.\nax.set_xlim(-axes_limit, axes_limit)\nax.set_ylim(-axes_limit, axes_limit)\n\n# plot the ground truths\nfor box in gt_boxes[idx]:\n    box = glb_to_sensor(box, sample_data)\n    c = np.array([255, 158, 0 ]) \/ 255.0 # Orange\n    box.render(ax, view=np.eye(4), colors=(c, c, c))\n\n# plot the predicted boxes\nfor box in pred_boxes[idx]:\n    box = glb_to_sensor(box, sample_data)\n    c = np.array([0, 0, 230]) \/ 255.0 # Blue\n    box.render(ax, view=np.eye(4), colors=(c, c, c))\n\n# gotta invert for consistency with lyft's inbuilt plots\nplt.gca().invert_yaxis()\nplt.gca().invert_xaxis()\nplt.show()","2e046d77":"lyft.render_sample_data(sample_data['token'])","025f7cf5":"def get_lines(boxes, name):\n    '''Takes in boxes, extracts edges and returns `go.Scatter3d` object for those lines'''\n    \n    x_lines = []\n    y_lines = []\n    z_lines = []\n\n    def f_lines_add_nones():\n        x_lines.append(None)\n        y_lines.append(None)\n        z_lines.append(None)\n\n    ixs_box_0 = [0, 1, 2, 3, 0]\n    ixs_box_1 = [4, 5, 6, 7, 4]\n\n    for box in boxes:\n        box = glb_to_sensor(box, sample_data)\n        points = view_points(box.corners(), view=np.eye(3), normalize=False)\n        x_lines.extend(points[0, ixs_box_0])\n        y_lines.extend(points[1, ixs_box_0])\n        z_lines.extend(points[2, ixs_box_0])\n        f_lines_add_nones()\n        x_lines.extend(points[0, ixs_box_1])\n        y_lines.extend(points[1, ixs_box_1])\n        z_lines.extend(points[2, ixs_box_1])\n        f_lines_add_nones()\n        for i in range(4):\n            x_lines.extend(points[0, [ixs_box_0[i], ixs_box_1[i]]])\n            y_lines.extend(points[1, [ixs_box_0[i], ixs_box_1[i]]])\n            z_lines.extend(points[2, [ixs_box_0[i], ixs_box_1[i]]])\n            f_lines_add_nones()\n\n    lines = go.Scatter3d(x=x_lines, y=y_lines, z=z_lines, mode=\"lines\", name=name)\n    return lines","19901f6c":"idx = 0 # change this to visualize other samples\nsample_token = df.iloc[idx]['Id']\n\nsample = lyft.get(\"sample\", sample_token)\nsample_data = lyft.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\npath = sample_data['filename']\nlidar_points = np.fromfile(path, dtype=np.float32, count=-1).reshape([-1, 5])[:, :4]\n\n# plot the points\ndf_tmp = pd.DataFrame(lidar_points[:, :3], columns=[\"x\", \"y\", \"z\"])\ndf_tmp[\"norm\"] = np.sqrt(np.power(df_tmp[[\"x\", \"y\", \"z\"]].values, 2).sum(axis=1))\nscatter = go.Scatter3d(\n    x=df_tmp[\"x\"],\n    y=df_tmp[\"y\"],\n    z=df_tmp[\"z\"],\n    mode=\"markers\",\n    marker=dict(size=1, color=df_tmp[\"norm\"], opacity=0.8),\n)\n\ngt_lines = get_lines(gt_boxes[idx], 'gt_boxes')\npred_lines = get_lines(pred_boxes[idx], 'pred_boxes')\nfig = go.Figure(data=[scatter, gt_lines, pred_lines])\nfig.update_layout(scene_aspectmode=\"data\")\nfig.show()","7c53a4f8":"In this notebook, I've shared my prediction visualization code. The code expects a csv file with predicted and ground truth boxes in `PredictionString` format (as used by `train.csv` and `sample_submission.csv`, see [here](https:\/\/www.kaggle.com\/c\/3d-object-detection-for-autonomous-vehicles\/data) for more information)\n\nHow this works:\n\n* We convert the predicted and ground truth strings to [Box](https:\/\/github.com\/lyft\/nuscenes-devkit\/blob\/master\/lyft_dataset_sdk\/utils\/data_classes.py#L478) format\n* Use matplotlib for BEV visualization\n* Use plotly for 3D interactive visualization\n\nI'm all open for new suggestions, drop a comment :)","bbf0a591":"When visualizing above 3d plot, for a mouse: you can use scroll to zoom in\/out, use left button to rotate the plot, use right button to translate the plot. This plot works smoothly when kernel is in edit mode, the rendered version after kernel commit is a bit laggy.","548690ea":"That's all for the BEV visualization, let's do 3D interactive visualization","44be83cb":"# `string` -> `Box`","c294eedd":"# Some utility functions","e9c50d5e":"# 3D Interactive visualization","a1db8c1d":"I've uploaded a sample csv file for demo, here's what it looks like:","fd0f802d":"# BEV Visualization","c8c80140":"# Visualizing the predictions","768893eb":"The Orange ones are the ground truths and theh blue ones are the predictions, we can plot ground truths using lyft's inbuilt render_sample_data function","11a1bce4":"<font color='red'>Do upvote if you liked this kernel :) <\/font> \ud83d\ude07","2585d25b":"Here `GroundTruthString` is the ground truth string and `PredictionString` is the predicted string for the corresponding sample token `Id`, both the strings follow the format of `PredictionString` in `train.csv`"}}