{"cell_type":{"c1ada470":"code","6b279533":"code","4028b640":"code","43d5b3a3":"code","fcac31b6":"code","c722e91a":"code","054a40a0":"code","7ce93c73":"code","f7e4d476":"code","79f00190":"code","694e0abc":"code","9bfb460d":"code","fc8af3ac":"code","8ee27f46":"code","67222b87":"code","89f62140":"code","bb31a2eb":"code","b547ddc7":"code","cb9dc6d3":"code","d89863c9":"code","1fa62c69":"code","c07523e1":"code","faaeb1ac":"code","d4aeb261":"code","6dfa5f10":"code","05d9ebd2":"code","02e7e0eb":"code","6deb70e4":"code","ef9106cd":"code","7dbaf6a4":"code","1d7ee884":"code","618ad850":"code","8d0f050e":"code","784f867d":"code","5f329863":"code","f6fb20e3":"code","08d0ed8b":"code","5e8d2cda":"code","93197adc":"code","6514eff9":"code","77655cb8":"code","8dda36ab":"code","3188b6bb":"code","ee1b09dc":"code","0806d03d":"code","50e964ed":"code","3d9f6fda":"code","ca857205":"code","cd14e2cb":"code","fad15d63":"code","4824a46d":"code","e9051872":"code","b8fc35bd":"code","90566e5d":"code","73682a57":"code","a060b88d":"code","d6523dda":"code","03d75179":"code","094c0972":"code","ebb5915a":"markdown","8b02764e":"markdown"},"source":{"c1ada470":"# Sources\n# https:\/\/www.kaggle.com\/dimitreoliveira\/model-stacking-feature-engineering-and-eda\n# https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n\nfrom sklearn.metrics import mean_squared_error as mse\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\n\nfrom sklearn.metrics import confusion_matrix, make_scorer\nimport shap\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\ncats = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","6b279533":"train.head()","4028b640":"cats.head()","43d5b3a3":"shops.head()","fcac31b6":"items.head()","c722e91a":"test.head()","054a40a0":"train.groupby(['shop_id'])['item_cnt_day'].sum().plot(kind='bar', figsize=(15,5))","7ce93c73":"train.groupby(['item_id'])['item_cnt_day'].sum().sort_values(ascending=False)","f7e4d476":"train = train.query('item_price > 0')","79f00190":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","694e0abc":"train = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","9bfb460d":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median\n\nmedian","fc8af3ac":"print(shops['shop_name'].duplicated().sum())\nprint(shops['shop_id'].duplicated().sum())","8ee27f46":"group = train.groupby('item_id').agg({'item_price': ['median'], 'item_cnt_day':['sum']})\ngroup.columns = ['median_price', 'sum_sales']\ngroup.reset_index(inplace=True)\ngroup[['median_price', 'sum_sales']].sort_values(by=['sum_sales'], ascending=False).round(1).head(50).set_index('median_price').plot(kind='bar', figsize=(20,8))","67222b87":"group[['item_id', 'median_price', 'sum_sales']].sort_values(by=['sum_sales'], ascending=False).round(1).head(10).set_index('median_price')","89f62140":"group['total_item_revenue'] = group['median_price']*group['sum_sales']\ngroup[['item_id', 'total_item_revenue']].sort_values(by=['total_item_revenue'], ascending=False).round(1).head(50).set_index('item_id').plot(kind='bar', figsize=(20,8))","bb31a2eb":"# Most sold item: \nprint(items[['item_name', 'item_id']][items['item_id'] == 20949])\n# Free translation: Branded package T-shirt 1C white (34 * 42)\nprint('-'*75)\n# Most revenue generated by a single item:\nprint(items[['item_name', 'item_id']][items['item_id'] == 6675])\n# Sony Playstation 4 (500 Gb) Black","b547ddc7":"matrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \n\nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\n\nprint(matrix)","cb9dc6d3":"group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\nprint(group)\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","d89863c9":"group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']}).unstack(fill_value=0).stack()\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\ngroup","1fa62c69":"gun = group.groupby('date_block_num').agg({'shop_id': ['unique']})\ngun.columns = ['unique']\ngun.reset_index(inplace=True)","c07523e1":"cl = group['shop_id'].unique()\nsmoplt = pd.DataFrame(1, index=np.sort(cl), columns=np.arange(34))\n\nfor i in range(34):\n    gun_ar = gun['unique'][gun['unique'].index == i].to_numpy()\n    ca = gun_ar[0].astype(np.int64)\n    vals = np.setdiff1d(cl, ca)\n    for j in vals:\n        smoplt.loc[smoplt[i].index == j, [i]] = 0","faaeb1ac":"fig, ax = plt.subplots(figsize=(15,15))\n\nsns.heatmap(smoplt, cbar=False, ax=ax, robust=True, cmap=\"GnBu\", linewidths=1, linecolor='#000000')\n\n# Based on the heatmap, its safe to assume the stores 8, 9, 13, 17, 20, 23, 29, 30, 32, 33, 40, 43 and 55 will have no sales on the 34 month.","d4aeb261":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","6dfa5f10":"matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month","05d9ebd2":"matrix","02e7e0eb":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\n\nmatrix = matrix.drop(['shop_name', 'item_name', 'item_category_name'], axis=1)\nmatrix = matrix[['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_cnt_month']]","6deb70e4":"matrix","ef9106cd":"matrix_train = matrix[matrix['date_block_num'] < 34]","7dbaf6a4":"matrix_train","1d7ee884":"sell_month = matrix_train.astype('float64').groupby('date_block_num')['item_cnt_month'].sum()\n\nsell_month.plot()","618ad850":"sell_month.sort_values(ascending=False)\n\n# The months with peak sales are 11 and 23. Both correspond to December. The predicted month 34 correspond to November.\n# Based on the past 2 years, its expected to have a slight increase of sales when compared to month 33.","8d0f050e":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\nmatrix = lag_feature(matrix, [1,2], 'item_cnt_month')\n\nmatrix","784f867d":"group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2], 'date_item_avg_item_cnt')\n\nmatrix","5f329863":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\n\n\nmatrix","f6fb20e3":"group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\n\nmatrix","08d0ed8b":"group = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)","5e8d2cda":"matrix","93197adc":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","6514eff9":"group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)","77655cb8":"matrix","8dda36ab":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","3188b6bb":"matrix.info()","ee1b09dc":"train = matrix[matrix['date_block_num'] < 34]\ntest = matrix[matrix['date_block_num'] == 34]\n\ntrain.drop('date_block_num', axis=1, inplace=True)\ntest.drop(['date_block_num', 'item_cnt_month'], axis=1, inplace=True)\n\ny = train['item_cnt_month']\nX = train.drop('item_cnt_month', axis=1)","0806d03d":"'''\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,shuffle=True, stratify=y , random_state=42)\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_test, y_test)], \n    verbose=True, \n    early_stopping_rounds = 3)\n'''","50e964ed":"'''\nplot_features(model, (10,14))\n\n# Highest importance features:\n# date_item_avg_item_cnt\n# date_shop_cat_avg_item_cnt\n# date_cat_avg_item_cnt\n# item_cnt_month_lag_2\n# \n# Lowest importance features:\n# date_shop_revenue\n# lag_6 features\n# lag_3 features\n# date_item_avg_item_price\n# delta features\n# date_shop_avg_item_cnt\n# date_avg_item_cnt\n# month\n# days\n'''","3d9f6fda":"scorer = make_scorer(mse)\n\nlinreg = LinearRegression()\nelnet = ElasticNet(random_state=42)\ndectree = DecisionTreeRegressor(random_state=42)\nforest = RandomForestRegressor(random_state=42)\nadab = AdaBoostRegressor(random_state=42)\ngb = XGBRegressor(eval_metric=scorer, random_state=42)\n\n\n# Create the train_test_split for model evaluation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,shuffle=True, stratify=y , random_state=42)","ca857205":"def bayes_search(model, param_grid):\n\n    # Initialize the cross validation method\n    n_iter = 5\n    cv = StratifiedKFold(n_splits=n_iter, shuffle=True, random_state=42)\n\n    # Execute the bayes search\n    bsearch = BayesSearchCV(model, param_grid, n_iter=n_iter, scoring=scorer, cv=cv, n_jobs=-1, verbose=True).fit(X,y)\n    # Print the values to be used in each parameter for best result in the final fitting\n    print(' ',bsearch.best_score_)\n    print(' ',bsearch.best_params_)\n    \n    return None","cd14e2cb":"'''\n# Searching for LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\ny_pred = linreg.predict(X_test)\nresult = mse(y_pred, y_test, squared=False)\nprint(result)\n'''\n# Result:\n# rmse: 0.7568274","fad15d63":"# Searching for ElasticNet\n\n'''\n# Define the parameters to be tested in the bayes search\nparam_grid = {'alpha': Real(0.1, 1, prior='log-uniform'),\n              'l1_ratio': Real(0, 1),\n              'max_iter': Integer(50, 2000),\n              }\n\nbayes_search(elnet, param_grid)\n\n\n# Results:\n# ([('alpha', 0.6185535843131738), ('l1_ratio', 0.9459955732871277), ('max_iter', 1511)])\n\nelnet = ElasticNet(alpha=0.618, l1_ratio=0.945, max_iter=1511, random_state=42)\nelnet.fit(X_train, y_train)\ny_pred = elnet.predict(X_test)\nresult = mse(y_pred, y_test, squared=False)\nprint(result)\n'''\n\n# Result:\n# rmse: 1.0813551","4824a46d":"# Searching for DecisionTreeRegressor\n\n'''\n# Define the parameters to be tested in the bayes search\nparam_grid = {\n              'max_leaf_nodes': Integer(10, 200),\n              }\n\nbayes_search(dectree, param_grid)\n\n\n# Results:\n# ([('max_leaf_nodes', 54)])\n\ndectree = DecisionTreeRegressor(max_leaf_nodes=54, n_jobs=-1, random_state=42, verbose=True)\ndectree.fit(X_train, y_train)\ny_pred = dectree.predict(X_test)\nresult = mse(y_pred, y_test, squared=False)\nprint(result)\n'''\n\n# Result:\n# rmse: 0.7187903167054043","e9051872":"# Searching for RandomForestRegressor\n'''\n\n# Define the parameters to be tested in the bayes search\nparam_grid = {'n_estimators': Integer(100, 1000),\n              #'max_leaf_nodes': Integer(10, 200),\n              }\n\nbayes_search(forest, param_grid)\n\n\n# Results:\n# ([('max_leaf_nodes', 54), ('n_estimators', 200)])\n\nforest = RandomForestRegressor(max_leaf_nodes=54, n_estimators=200, random_state=42, n_jobs=-1, verbose=True)\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\nresult = mse(y_pred, y_test, squared=False)\nprint(result)\n'''\n\n# Result\n# rmse: 0.7028950041966859","b8fc35bd":"# Searching for AdaBoostRegressor\n'''\n\n# Define the parameters to be tested in the bayes search\nparam_grid = {'n_estimators': Integer(50, 500),\n              'learning_rate': Real(0.01, 1, prior='log-uniform')\n              }\n\nbayes_search(adab, param_grid)\n\n# Results:\n# ([('n_estimators', 200), ('learning_rate', 0.05)])\n\nadab = AdaBoostRegressor(learning_rate=0.1, n_estimators=200, random_state=42)\nadab.fit(X_train, y_train)\ny_pred = adab.predict(X_test)\nresult = mse(y_pred, y_test, squared=False)\nprint(result)\n'''\n\n# Result\n# rmse: 0.9776510406903495","90566e5d":"# Searching for XGBoost\n\n'''\n# Define the parameters to be tested in the bayes search\nparam_grid = {'max_depth': Integer(1, 90),\n              'learning_rate': Real(0.01, 1, prior='log-uniform'),\n              'n_estimators': Integer(50, 200)\n             }\n\nbayes_search(gb, param_grid)\n\n\n# Results:\n# ([('max_depth', 8), ('n_estimators', 200), ('min_child_weight', 300), ('colsample_bytree', 0.8), ('subsample', 0.8), ('eta', 0.3)])\n\n\ngb = XGBRegressor(colsample_bytree=0.8, max_depth=8, n_estimators=200, eta=0.3, subsample=0.8, n_jobs=-1, random_state=42)\ngb.fit(X_train, y_train, verbose=True)\ny_pred = gb.predict(X_test)\nresult = mse(y_pred, y_test, squared=False)\nprint(result)\n\n'''\n# Result:\n# rmse: 0.6327427","73682a57":"# Out of all models tried, XGBRegressor got the better result\n\ngb = XGBRegressor(colsample_bytree=0.8, max_depth=8, n_estimators=200, eta=0.3, subsample=0.8, n_jobs=-1, random_state=42)\ngb.fit(X, y,\n    eval_metric=\"rmse\", \n    eval_set=[(X, y), (X_test, y_test)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ncols_when_model_builds = gb.get_booster().feature_names\ntest = test[cols_when_model_builds]\ny_pred = gb.predict(test)","a060b88d":"pd.Series(y_pred).describe()","d6523dda":"plot_features(gb, (10,14))","03d75179":"test = test.reset_index()\n\nsubmission = pd.DataFrame({\n    \"ID\": test.reset_index().index, \n    \"item_cnt_month\": y_pred\n})","094c0972":"submission.to_csv('xgb_submission.csv', index=False)","ebb5915a":"The data is spread among 3 different tables. cats, shops and train.\nNot only that, but the data in train and test are given as a daily count, instead of monthly. Since the objective is to predict sales for the 34th month, it makes sense reorganizing the values monthly","8b02764e":"First, we'll check feature importance and drop the less relevant ones"}}