{"cell_type":{"7d6cd4e6":"code","6de85bce":"code","ef3c7db0":"code","90a7051a":"code","34540e8c":"code","28248717":"code","091b4564":"code","b08bcdb0":"code","8c861bd7":"code","ce06e3fc":"code","2d8e73fe":"code","e41bde8d":"code","9327ff27":"code","35f215f5":"markdown","a4731cb9":"markdown","8ebb8a4f":"markdown","98056af2":"markdown","519b8e68":"markdown","afc710e0":"markdown","2888358e":"markdown","69164964":"markdown"},"source":{"7d6cd4e6":"import os\nfrom pathlib import Path\n\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\n\n# --- setup ---\npd.set_option('max_columns', 50)","6de85bce":"import zarr\n\nimport l5kit\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.evaluation import write_pred_csv\n\nprint(\"l5kit version:\", l5kit.__version__)","ef3c7db0":"import torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Subset\nfrom torchvision.models import resnet18\nfrom torch import nn\nfrom typing import Dict","90a7051a":"# --- Model utils ---\nclass LyftMultiModel(nn.Module):\n\n    def __init__(self, cfg: Dict, num_modes=3):\n        super().__init__()\n\n        # TODO: support other than resnet18?\n        backbone = resnet18(pretrained=False, progress=True)\n        self.backbone = backbone\n\n        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n        num_in_channels = 3 + num_history_channels\n\n        self.backbone.conv1 = nn.Conv2d(\n            num_in_channels,\n            self.backbone.conv1.out_channels,\n            kernel_size=self.backbone.conv1.kernel_size,\n            stride=self.backbone.conv1.stride,\n            padding=self.backbone.conv1.padding,\n            bias=False,\n        )\n\n        # This is 512 for resnet18 and resnet34;\n        # And it is 2048 for the other resnets\n        backbone_out_features = 512\n\n        # X, Y coords for the future positions (output shape: Bx50x2)\n        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n        num_targets = 2 * self.future_len\n\n        # You can add more layers here.\n        self.head = nn.Sequential(\n            # nn.Dropout(0.2),\n            nn.Linear(in_features=backbone_out_features, out_features=4096),\n        )\n\n        self.num_preds = num_targets * num_modes\n        self.num_modes = num_modes\n\n        self.logit = nn.Linear(4096, out_features=self.num_preds + num_modes)\n\n    def forward(self, x):\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x)\n\n        x = self.backbone.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        x = self.head(x)\n        x = self.logit(x)\n\n        # pred (bs)x(modes)x(time)x(2D coords)\n        # confidences (bs)x(modes)\n        bs, _ = x.shape\n        pred, confidences = torch.split(x, self.num_preds, dim=1)\n        pred = pred.view(bs, self.num_modes, self.future_len, 2)\n        assert confidences.shape == (bs, self.num_modes)\n        confidences = torch.softmax(confidences, dim=1)\n        return pred, confidences\n\n    ","34540e8c":"# --- Utils ---\nimport yaml\n\n\ndef save_yaml(filepath, content, width=120):\n    with open(filepath, 'w') as f:\n        yaml.dump(content, f, width=width)\n\n\ndef load_yaml(filepath):\n    with open(filepath, 'r') as f:\n        content = yaml.safe_load(f)\n    return content\n\n\nclass DotDict(dict):\n    \"\"\"dot.notation access to dictionary attributes\n\n    Refer: https:\/\/stackoverflow.com\/questions\/2352181\/how-to-use-a-dot-to-access-members-of-dictionary\/23689767#23689767\n    \"\"\"  # NOQA\n\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    ","28248717":"# Referred https:\/\/www.kaggle.com\/pestipeti\/pytorch-baseline-inference\ndef run_prediction(predictor, data_loader):\n    predictor.eval()\n\n    pred_coords_list = []\n    confidences_list = []\n    timestamps_list = []\n    track_id_list = []\n\n    with torch.no_grad():\n        dataiter = tqdm(data_loader)\n        for data in dataiter:\n            image = data[\"image\"].to(device)\n            # target_availabilities = data[\"target_availabilities\"].to(device)\n            # targets = data[\"target_positions\"].to(device)\n            pred, confidences = predictor(image)\n\n            pred_coords_list.append(pred.cpu().numpy().copy())\n            confidences_list.append(confidences.cpu().numpy().copy())\n            timestamps_list.append(data[\"timestamp\"].numpy().copy())\n            track_id_list.append(data[\"track_id\"].numpy().copy())\n    timestamps = np.concatenate(timestamps_list)\n    track_ids = np.concatenate(track_id_list)\n    coords = np.concatenate(pred_coords_list)\n    confs = np.concatenate(confidences_list)\n    return timestamps, track_ids, coords, confs\n","091b4564":"# --- Lyft configs ---\ncfg = {\n    'format_version': 4,\n    'model_params': {\n        'model_architecture': 'resnet50',\n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n\n    'raster_params': {\n        'raster_size': [448, 448],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n\n    'train_data_loader': {\n        'key': 'scenes\/train.zarr',\n        'batch_size': 12,\n        'shuffle': True,\n        'num_workers': 4\n    },\n\n    'valid_data_loader': {\n        'key': 'scenes\/validate.zarr',\n        'batch_size': 32,\n        'shuffle': False,\n        'num_workers': 4\n    },\n    \n    'test_data_loader': {\n        'key': 'scenes\/test.zarr',\n        'batch_size': 8,\n        'shuffle': False,\n        'num_workers': 4\n    },\n\n    'train_params': {\n        'max_num_steps': 10000,\n        'checkpoint_every_n_steps': 5000,\n\n        # 'eval_every_n_steps': -1\n    }\n}\n","b08bcdb0":"flags_dict = {\n    \"debug\": False,\n    # --- Data configs ---\n    \"l5kit_data_folder\": \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\",\n    # --- Model configs ---\n    \"pred_mode\": \"multi\",\n    # --- Training configs ---\n    \"device\": \"cuda:0\",\n    \"out_dir\": \"results\/multi_train\",\n    \"epoch\": 2,\n    \"snapshot_freq\": 50,\n}","8c861bd7":"flags = DotDict(flags_dict)\nout_dir = Path(flags.out_dir)\nos.makedirs(str(out_dir), exist_ok=True)\nprint(f\"flags: {flags_dict}\")\nsave_yaml(out_dir \/ 'flags.yaml', flags_dict)\nsave_yaml(out_dir \/ 'cfg.yaml', cfg)\ndebug = flags.debug","ce06e3fc":"# set env variable for data\nl5kit_data_folder = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\nos.environ[\"L5KIT_DATA_FOLDER\"] = l5kit_data_folder\ndm = LocalDataManager(None)\n\nprint(\"Load dataset...\")\ndefault_test_cfg = {\n    'key': 'scenes\/test.zarr',\n    'batch_size': 32,\n    'shuffle': False,\n    'num_workers': 4\n}\ntest_cfg = cfg.get(\"test_data_loader\", default_test_cfg)\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\ntest_path = test_cfg[\"key\"]\nprint(f\"Loading from {test_path}\")\ntest_zarr = ChunkedDataset(dm.require(test_path)).open()\nprint(\"test_zarr\", type(test_zarr))\ntest_mask = np.load(f\"{l5kit_data_folder}\/scenes\/mask.npz\")[\"arr_0\"]\ntest_agent_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\ntest_dataset = test_agent_dataset\nif debug:\n    # Only use 100 dataset for fast check...\n    test_dataset = Subset(test_dataset, np.arange(100))\ntest_loader = DataLoader(\n    test_dataset,\n    shuffle=test_cfg[\"shuffle\"],\n    batch_size=test_cfg[\"batch_size\"],\n    num_workers=test_cfg[\"num_workers\"],\n    pin_memory=True,\n)\n\nprint(test_agent_dataset)\nprint(\"# AgentDataset test:\", len(test_agent_dataset))\nprint(\"# ActualDataset test:\", len(test_dataset))","2d8e73fe":"device = torch.device(flags.device)\n\nif flags.pred_mode == \"multi\":\n    predictor = LyftMultiModel(cfg)\nelse:\n    raise ValueError(f\"[ERROR] Unexpected value flags.pred_mode={flags.pred_mode}\")\n\npt_path = \"\/kaggle\/input\/lyft-prediction-public-models\/multi_mode_448px.pth\"\nprint(f\"Loading from {pt_path}\")\n\nimport copy\nstate_dict = torch.load(pt_path)\nstate_dict_v2 = copy.deepcopy(state_dict)\n\nfor key in state_dict:\n    new_key = key[10:]\n    state_dict_v2[new_key] = state_dict_v2.pop(key)\n\npredictor.load_state_dict(state_dict_v2)\npredictor.to(device)","e41bde8d":"# --- Inference ---\ntimestamps, track_ids, coords, confs = run_prediction(predictor, test_loader)","9327ff27":"csv_path = \"submission.csv\"\nwrite_pred_csv(\n    csv_path,\n    timestamps=timestamps,\n    track_ids=track_ids,\n    coords=coords,\n    confs=confs)\nprint(f\"Saved to {csv_path}\")","35f215f5":"# Main script\n\nNow finished defining all the util codes. Let's start writing main script to train the model!","a4731cb9":"## Prepare model & optimizer","8ebb8a4f":"# Environment setup\n\n - Please add [pestipeti\/lyft-l5kit-unofficial-fix](https:\/\/www.kaggle.com\/pestipeti\/lyft-l5kit-unofficial-fix) as utility script.\n    - Official utility script \"[philculliton\/kaggle-l5kit](https:\/\/www.kaggle.com\/mathurinache\/kaggle-l5kit)\" does not work with pytorch GPU.","98056af2":"## Configs","519b8e68":"## Model\n\npytorch model definition. Here model outputs both **multi-mode trajectory prediction & confidence of each trajectory**.","afc710e0":"# Inference!","2888358e":"## Loading data\n\nHere we will only use the first dataset from the sample set. (sample.zarr data is used for visualization, please use train.zarr \/ validate.zarr \/ test.zarr for actual model training\/validation\/prediction.)<br\/>\nWe're building a `LocalDataManager` object. This will resolve relative paths from the config using the `L5KIT_DATA_FOLDER` env variable we have just set.","69164964":"# Lyft: Prediction with multi-mode confidence\n\nThank you for the original training and inference kernel [@corochann](https:\/\/www.kaggle.com\/corochann), don't forget to upvote those!  \nThis kernel here is just a demonstration of the capabilities of these kernels below. With just very few changes in the training routine (e.g. increasing the rasterizer to 448 x 448 px) the model achieves a public LB score of 33.953, which is a 28% reduction to the original model (original LB score: 47.068).\n- https:\/\/www.kaggle.com\/corochann\/lyft-training-with-multi-mode-confidence  \n- https:\/\/www.kaggle.com\/corochann\/lyft-prediction-with-multi-mode-confidence  "}}