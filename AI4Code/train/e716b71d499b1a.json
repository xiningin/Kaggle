{"cell_type":{"ba1d8cb1":"code","c5bc6a6e":"code","e2dd2146":"code","032abb90":"code","45c9f074":"code","63fc1cd1":"code","09ea7d68":"code","6e64f245":"code","7e0cffb7":"code","876188f1":"code","9a1ccbe7":"code","9de7d2d4":"code","208a9255":"markdown","4e8cbe3f":"markdown","c659d2eb":"markdown","bae5567e":"markdown","cc90ad3f":"markdown","666ee9a4":"markdown","e68f1c5b":"markdown","e9f792c5":"markdown","4d8eedcd":"markdown","add2d0c6":"markdown","5193ebf7":"markdown"},"source":{"ba1d8cb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom tqdm import tnrange, tqdm, tqdm_notebook\nimport numpy as np, pandas as pd\nimport os\nprint(\"..\\input\")\n\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn')\nsns.set(font_scale=1)","c5bc6a6e":"mu = 0.497073351173995\nsigma = 0.8105721059584674\n\n# Generate three normal distributions:\nnorm1 = np.sort(np.random.normal(mu, sigma, 400000))\nnorm2 = np.sort(np.random.normal(mu, sigma, 400000))\nnorm3 = np.sort(np.random.normal(mu, sigma, 400000))\n\n# Plor graphs of their paiwise difference:\nfig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.distplot(norm1-norm2, ax=ax[0])\nsns.distplot(norm2-norm3, ax=ax[1])\nsns.distplot(norm1-norm3, ax=ax[2])\n\nax[2].set_xlabel(\"norm1-norm2\")\nax[1].set_xlabel(\"norm2-norm3\")\nax[0].set_xlabel(\"norm1-norm3\")","e2dd2146":"categ_2 = np.hstack((np.zeros(3800),np.ones(6500)))\ncateg_3 = np.hstack((np.zeros(3500),np.ones(6000), np.ones(800)*-1))\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(categ_2, ax=ax[0])\nsns.distplot(categ_3, ax=ax[1])\n                    \nax[1].set_xlabel(\"3 categories\")\nax[0].set_xlabel(\"2 categories\")","032abb90":"# Lets try to add some noise to categories:\nnoise_categ_2 = categ_2 + np.random.normal(loc=0.0, scale=categ_2.mean(), size=categ_2.shape)\nnoise_categ_3 = categ_3 + np.random.normal(loc=0.0, scale=categ_3.mean(), size=categ_3.shape)\n\n# And plot:\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(noise_categ_2, ax=ax[0])\nsns.distplot(noise_categ_3, ax=ax[1])\n                    \nax[1].set_xlabel(\"3 categories\")\nax[0].set_xlabel(\"2 categories\")","45c9f074":"# Now we will try to substract the normality:\ndiff_categ_2 = np.sort(noise_categ_2) - np.sort(np.random.normal(noise_categ_2.mean(), noise_categ_2.std(), 10300))\ndiff_categ_3 = np.sort(noise_categ_3) - np.sort(np.random.normal(noise_categ_3.mean(), noise_categ_3.std(), 10300))\n\n# And plot:\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(diff_categ_2, ax=ax[0])\nsns.distplot(diff_categ_3, ax=ax[1])\n                    \nax[1].set_xlabel(\"Denoised 3 categories\")\nax[0].set_xlabel(\"Denoised 2 categories\")","63fc1cd1":"train_file = '..\/input\/train.csv'\ntest_file = '..\/input\/test.csv'\n\ntrain = pd.read_csv(train_file, index_col='ID_code')\nX_test = pd.read_csv(test_file, index_col='ID_code')","09ea7d68":"y_train = train.iloc[:,0]\nX_train = train.iloc[:,1:]","6e64f245":"X_all = pd.concat([X_test, X_train])","7e0cffb7":"for col in tqdm_notebook(X_all.columns):\n    mu = X_all[col].mean()\n    sigma = X_all[col].std()\n    minus = np.sort(np.random.normal(mu, sigma, 400000))\n    ser = X_all[col].sort_values(ascending=True) - minus\n    X_all[col] = ser[X_all.index]","876188f1":"test = X_all.loc[X_test.index,:].reset_index(drop=True)\ntrain = X_all.loc[X_train.index,:].reset_index(drop=True)\n\ntrain0 = train[y_train.values==0].copy()\ntrain1 = train[y_train.values==1].copy()","9a1ccbe7":"# CALCULATE MEANS AND STANDARD DEVIATIONS\ns = [0]*200\nm = [0]*200\nfor i in range(200):\n    s[i] = np.std(train['var_'+str(i)])\n    m[i] = np.mean(train['var_'+str(i)])\n    \n# CALCULATE PROB(TARGET=1 | X)\ndef getp(i,x):\n    c = 3 #smoothing factor\n    a = len( train1[ (train1['var_'+str(i)] > x-s[i]\/c)&(train1['var_'+str(i)]<x+s[i]\/c) ] ) \n    b = len( train0[ (train0['var_'+str(i)] > x-s[i]\/c)&(train0['var_'+str(i)]<x+s[i]\/c) ] )\n    if a+b<500: return 0.1 #smoothing factor\n    # RETURN PROBABILITY\n    return a \/ (a+b)\n    # ALTERNATIVELY RETURN ODDS\n    # return a \/ b\n    \n# SMOOTH A DISCRETE FUNCTION\ndef smooth(x,st=1):\n    for j in range(st):\n        x2 = np.ones(len(x)) * 0.1\n        for i in range(len(x)-2):\n            x2[i+1] = 0.25*x[i]+0.5*x[i+1]+0.25*x[i+2]\n        x = x2.copy()\n    return x","9de7d2d4":"# DRAW PLOTS, YES OR NO\nPicture = True\n# DATA HAS Z-SCORE RANGE OF -4.5 TO 4.5\nrmin=-5; rmax=5; \n# CALCULATE PROBABILITIES FOR 501 BINS\nres=501\n# STORE PROBABILITIES IN PR\npr = 0.1 * np.ones((200,res))\npr2 = pr.copy()\nxr = np.zeros((200,res))\nxr2 = xr.copy()\nct2 = 0\nfor j in tnrange(50, desc='1st loop'):\n    if Picture: plt.figure(figsize=(15,8))\n    for v in tnrange(4, desc='1st loop'):\n        ct = 0\n        # CALCULATE PROBABILITY FUNCTION FOR VAR\n        for i in np.linspace(rmin,rmax,res):\n            pr[v+4*j,ct] = getp(v+4*j,m[v+4*j]+i*s[v+4*j])\n            xr[v+4*j,ct] = m[v+4*j]+i*s[v+4*j]\n            xr2[v+4*j,ct] = i\n            ct += 1\n        if Picture:\n            # SMOOTH FUNCTION FOR PRETTIER DISPLAY\n            # BUT USE UNSMOOTHED FUNCTION FOR PREDICTION\n            pr2[v+4*j,:] = smooth(pr[v+4*j,:],res\/\/10)\n            # DISPLAY PROBABILITY FUNCTION\n            plt.subplot(2, 4, ct2%4+5)\n            plt.plot(xr[v+4*j,:],pr2[v+4*j,:],'-')\n            plt.title('P( t=1 | var_'+str(v+4*j)+' )')\n            xx = plt.xlim()\n            # DISPLAY TARGET DENSITIES\n            plt.subplot(2, 4, ct2%4+1)\n            sns.distplot(test['var_'+str(v+4*j)], label = 'test')\n            sns.distplot(train0['var_'+str(v+4*j)], label = 't=0')\n            sns.distplot(train1['var_'+str(v+4*j)], label = 't=1')\n            plt.title('var_'+str(v+4*j))\n            plt.legend()\n            plt.xlim(xx)\n            plt.xlabel('')\n        if (ct2%8==0): print('Showing vars',ct2,'to',ct2+7,'...')\n        ct2 += 1\n    if Picture: plt.show()","208a9255":"# What happens when you're normal\n\nLets generate two normal distributions and subtract one from the other:","4e8cbe3f":"# Categories?: Display Target Density and Target Probability\nAs described by Chris: \"Below are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`.\" \n\nNote how the shape and the range of graphs are consistnent. All we need to do is to sort them in the right order. Does not it remind you of the last Santander? I am pretty sure the correct order can be restored.","c659d2eb":"# Load our data","bae5567e":"# Substracting normality?","cc90ad3f":"The shapes look familiar.","666ee9a4":"# Afterthoughts\nI have not tried to go beyond this, so I am not even sure that this works. But I will certainly try. All comments are welcome. So if you have reasons to think this is bollocks, please say so.","e68f1c5b":"Two things to note:\n1. There is single peak at zero.\n2. Substracting one sorted array from another is probably not the best way to \"substract normality\".\n\nIt maybe Ok for exploration, but for the actual data there should be a cleaner way to do this.","e9f792c5":"\n# Subtracting normality\n\nLately, I was gazing at the beatiful graphs created by Chris Deotte once again:\nhttps:\/\/www.kaggle.com\/cdeotte\/modified-naive-bayes-santander-0-899\/notebook\n\n...thinking that they have told us to look for weird ubnormal stuff. To do so we have to substract normality, so here we go.","4d8eedcd":"As you see substracting sorted array is not the best way to restore original categories. But categorical data with added noise looks different after the trick. Lets try doing it to our data.","add2d0c6":"# A conspiracy theory\nWhat if I would want to hide that my data is categorical? I could add gaussian noise to category values.","5193ebf7":"# Statistical Functions\nBelow are functions to calcuate various statistical things."}}