{"cell_type":{"07a9e8b2":"code","3890c16e":"code","e46bb63d":"code","4fc63774":"code","cc2b6ac4":"code","35d6b97a":"code","f831ebcb":"code","d872b21b":"code","75d1ee85":"code","bf00a708":"code","e363f5b4":"code","4d9aac6d":"code","9c3733f5":"code","9152cbdf":"code","b1daa741":"code","bca991d2":"code","193a82e3":"code","7acca759":"code","137c2635":"code","672087b6":"code","555dff22":"code","13c93fdb":"code","07094441":"code","b8577f9a":"code","9030089c":"code","f9d34ffb":"code","a42430d3":"code","f66d3871":"code","a7383ec5":"markdown","75fa42b1":"markdown","236198f7":"markdown","2139b0df":"markdown","73af0a4f":"markdown","eb2f86f0":"markdown","7c9198fa":"markdown","92a7b070":"markdown","b18c0740":"markdown","6fccfee3":"markdown","a29301f9":"markdown","5790c103":"markdown","440b77eb":"markdown","870687df":"markdown","3c9872c4":"markdown","9d742eec":"markdown","9ba4539c":"markdown","a81484c2":"markdown","00ea4a44":"markdown","8c5f368f":"markdown","1024772d":"markdown","f2a69632":"markdown","22e815f8":"markdown","b3e6c328":"markdown","19186c73":"markdown","5d18dded":"markdown","fd43018e":"markdown","947527e9":"markdown","b7e53884":"markdown","4426d2a3":"markdown","087c18d0":"markdown","035c281b":"markdown","6af7d7e0":"markdown","10ac062d":"markdown","068fa143":"markdown","527350ad":"markdown","b2f2ecba":"markdown","9f7a25a8":"markdown","6594824f":"markdown"},"source":{"07a9e8b2":"import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv('..\/input\/star-dataset\/6 class csv.csv')\ndata\n","3890c16e":"data.describe()","e46bb63d":"data['Spectral Class'].unique()","4fc63774":"data['Star color'].unique()","cc2b6ac4":"data['Star type'].replace([0,1,2,3,4,5],['Red Dwarf','White Dwarf','Brown Dwarf','Main Sequence','Supergiants','Hypergiants'],inplace=True)\n\ndata['Star color'].replace(['Blue White','Blue white','Blue-white','Blue white '],['Blue-White','Blue-White','Blue-White','Blue-White'],inplace=True)\ndata['Star color'].replace(['Yellowish White'],['Yellowish-White'],inplace=True)\ndata['Star color'].replace(['Pale yellow orange'],['Pale-Yellow-Orange'],inplace=True)\ndata['Star color'].replace(['yellow-white'],['Yellow-White'],inplace=True)\ndata['Star color'].replace(['white'],['White'],inplace=True)\ndata['Star color'].replace(['yellowish'],['Yellowish'],inplace=True)\ndata['Star color'].replace(['Blue '],['Blue'],inplace=True)\ndata['Star color'].unique()","35d6b97a":"data['Star type'].replace([0,1,2,3,4,5],['Red Dwarf','White Dwarf','Brown Dwarf','Main Sequence','Supergiants','Hypergiants'],inplace=True)","f831ebcb":"def chi_square(df, col1, col2):    \n    #---create the contingency table---\n    df_cont = pd.crosstab(index = df[col1], columns = df[col2])\n    \n    #---calculate degree of freedom---\n    degree_f = (df_cont.shape[0]-1) * (df_cont.shape[1]-1)\n    #---sum up the totals for row and columns---\n    df_cont.loc[:,'Total']= df_cont.sum(axis=1)\n    df_cont.loc['Total']= df_cont.sum()\n  \n    #---create the expected value dataframe---\n    df_exp = df_cont.copy()    \n    df_exp.iloc[:,:] = np.multiply.outer(df_cont.sum(1).values,df_cont.sum().values) \/ df_cont.sum().sum()            \n    \n    # calculate chi-square values\n    df_chi2 = ((df_cont - df_exp)**2) \/ df_exp    \n    df_chi2.loc[:,'Total']= df_chi2.sum(axis=1)\n    df_chi2.loc['Total']= df_chi2.sum()\n    \n    #---get chi-square score---   \n    chi_square_score = df_chi2.iloc[:-1,:-1].sum().sum()\n\n    #---calculate the p-value---\n    from scipy import stats\n    p = stats.distributions.chi2.sf(chi_square_score, degree_f)\n\n    return chi_square_score, degree_f, p\n","d872b21b":"\nchi_score, degree_f, p = chi_square(data,'Spectral Class','Star type')\nprint('Spectral Class vs Star type')\nprint(f'Chi2_score: {chi_score}, Degrees of freedom: {degree_f}, p-value: {p}\\n')\n\nchi_score, degree_f, p = chi_square(data,'Star color','Star type')\nprint('Star color vs Star type')\nprint(f'Chi2_score: {chi_score}, Degrees of freedom: {degree_f}, p-value: {p}\\n')\n\nchi_score, degree_f, p = chi_square(data,'Star color','Spectral Class')\nprint('Star color vs Spectral Class')\nprint(f'Chi2_score: {chi_score}, Degrees of freedom: {degree_f}, p-value: {p}\\n')","75d1ee85":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,7))\nsns.boxplot(x=\"Star type\", y=\"Temperature (K)\", data=data, palette=\"Set3\", linewidth=0.5)\nsns.swarmplot(x=\"Star type\", y=\"Temperature (K)\", data=data, size=3.5)\nplt.show()\nplt.figure(figsize=(10,7))\nsns.boxplot(x=\"Star type\", y=\"Luminosity(L\/Lo)\", data=data, palette=\"Set3\", linewidth=0.5)\nsns.swarmplot(x=\"Star type\", y=\"Luminosity(L\/Lo)\", data=data, size=1.5)\nplt.show()\nplt.figure(figsize=(10,7))\nsns.boxplot(x=\"Star type\", y=\"Radius(R\/Ro)\", data=data, palette=\"Set3\", linewidth=0.5)\nsns.swarmplot(x=\"Star type\", y=\"Radius(R\/Ro)\", data=data, size=1.5)\nplt.show()\nplt.figure(figsize=(10,7))\nsns.boxplot(x=\"Star type\", y=\"Absolute magnitude(Mv)\", data=data, palette=\"Set3\", linewidth=0.5)\nsns.swarmplot(x=\"Star type\", y=\"Absolute magnitude(Mv)\", data=data, size=3.5)\nplt.show()","bf00a708":"def comp_df(data, col_num, col_categ):    \n    #---create the contingency table---\n    df = pd.DataFrame(data, columns=[col_num, col_categ])\n    \n    uniq = df[col_categ].unique()\n    data_comp = pd.DataFrame()\n    \n    #---form a table where the columns will be categorical features, and the values will be numerical--\n    for value in uniq:\n        data_uniq = df.loc[df[col_categ] ==  value]\n        data_uniq.reset_index(drop=True, inplace=True)\n        data_comp[value] = data_uniq[col_num]\n    \n    #---fill in gaps with median values---\n    data_comp = data_comp.fillna(data_comp.median(axis=0), axis=0)\n    return data_comp","e363f5b4":"#check how it works\ncomp_df(data,'Temperature (K)','Spectral Class')","4d9aac6d":"def f_score_anova(data):\n    # number of items in each group\n    n = data.shape[0]\n    # number of groups\n    k = data.shape[1]\n    # number of observations\n    observation_size = n*k\n\n    # group means and overall mean\n    data.loc['Group Means'] = data.mean()\n    overall_mean = data.iloc[-1].mean()\n    \n    # calculate Sum of squares of all observation, SS_total = SS_between + SS_within\n    SS_total = (((data.iloc[:-1] - overall_mean)**2).sum()).sum()\n    # calculate sum of squares within is the sum of squared deviations of scores around their group\u2019s mean\n    SS_within = (((data.iloc[:-1] - data.iloc[-1])**2).sum()).sum()\n    # calculate the sum of squares of the group means from the overall mean:\n    SS_between = (n * (data.iloc[-1] - overall_mean)**2).sum()\n\n    # compute the various degrees of freedoms\n    df_total = observation_size - 1       \n    df_within = observation_size - k      \n    df_between = k - 1\n    print('df1 between =', df_between)\n    print('df2 within =', df_within)\n    \n    # compute the various mean squared values\n    mean_sq_between = SS_between \/ (k - 1)\n    mean_sq_within = SS_within \/ (observation_size - k)\n\n    # calculate the F-value, which is the ratio of two variances\n    F = mean_sq_between \/ mean_sq_within\n\n    return F","9c3733f5":"data_comp = comp_df(data,'Radius(R\/Ro)','Star type')\nF = f_score_anova(data_comp)\nprint('F = ', '%.2f' % F)\n\n# also we can compare our def f_score_anova with using the Stats module to calculate f-score\nimport scipy.stats as stats\nfvalue, pvalue = stats.f_oneway(*data_comp.iloc[:-1,0:6].T.values)\nprint('F (Stats module)=','%.2f' % fvalue, 'p-value (Stats module)', pvalue)\n","9152cbdf":"numeric_columns_name = [c for c in data.columns if data[c].dtype.name != 'object']\n\nfor value in numeric_columns_name:\n    print(value,'vs Star type:')\n    data_comp = comp_df(data,value,'Star type')\n    F = f_score_anova(data_comp)\n    print('F = ', '%.2f' % F)\n    print()","b1daa741":"for value in numeric_columns_name:\n    print(value,'vs Spectral Class:')\n    data_comp = comp_df(data,value,'Spectral Class')\n    F = f_score_anova(data_comp)\n    print('F = ', '%.2f' % F)\n    print()","bca991d2":"plt.figure(figsize=(10,7))\nsns.boxplot(x=\"Spectral Class\", y=\"Temperature (K)\", data=data, palette=\"Set3\", linewidth=0.5)\nsns.swarmplot(x=\"Spectral Class\", y=\"Temperature (K)\", data=data, size=1.7)\nplt.show()","193a82e3":"for value in numeric_columns_name:\n    print(value,'vs Star color:')\n    data_comp = comp_df(data,value,'Star color')\n    F = f_score_anova(data_comp)\n    print('F = ', '%.2f' % F)\n    print()","7acca759":"plt.figure(figsize=(15,7))\nsns.boxplot(x=\"Star color\", y=\"Luminosity(L\/Lo)\", data=data, palette=\"Set3\", linewidth=0.5)\nplt.show()","137c2635":"sns.heatmap(data[['Temperature (K)', 'Luminosity(L\/Lo)', 'Radius(R\/Ro)', 'Absolute magnitude(Mv)']].corr(), annot=True)","672087b6":"sns.heatmap(data[['Temperature (K)', 'Luminosity(L\/Lo)', 'Radius(R\/Ro)', 'Absolute magnitude(Mv)']].corr(method='spearman'), annot=True)","555dff22":"sns.pairplot(data)","13c93fdb":"from sklearn.linear_model import LinearRegression\n\ndef calculate_vif(df, features):\n    vif, tolerance = {}, {}\n    # all the features that you want to examine \n    for feature in features:\n    # extract all the other features you will regress against\n        X = [f for f in features if f != feature] \n        X, y = df[X], df[feature]\n        \n        # extract r-squared from the fit\n        r2 = LinearRegression().fit(X, y).score(X, y)\n        # calculate tolerance \n        tolerance[feature] = 1 - r2\n        \n        # calculate VIF\n        vif[feature] = 1\/(tolerance[feature])\n\n        #return VIF DataFrame\n    return pd.DataFrame({'VIF': vif, 'Tolerance': tolerance})","07094441":"calculate_vif(df=data, features= ['Temperature (K)', 'Luminosity(L\/Lo)', 'Radius(R\/Ro)', 'Absolute magnitude(Mv)'])","b8577f9a":"data.hist(bins=50 , figsize=(15,8))","9030089c":"data['LOG(Luminosity(L\/Lo))'] = np.log10(data['Luminosity(L\/Lo)'])\ndata['LOG(Radius(R\/Ro))'] = np.log10(data['Radius(R\/Ro)'])\ndata['LOG(Temperature (K))'] = np.log10(data['Temperature (K)'])","f9d34ffb":"data.hist(column = ['LOG(Temperature (K))','LOG(Luminosity(L\/Lo))', 'LOG(Radius(R\/Ro))', 'Absolute magnitude(Mv)'],bins=50 , figsize=(15,8))","a42430d3":"g =sns.relplot(\n    data=data,\n    x=\"Temperature (K)\", y=\"Luminosity(L\/Lo)\",\n    hue=\"Spectral Class\", style=\"Star type\",size=\"Radius(R\/Ro)\", sizes = (50,400)\n)\n\ng.fig.set_figheight(10)\ng.fig.set_figwidth(20)","f66d3871":"g =sns.relplot(\n    data=data,\n    x=\"Temperature (K)\", y=\"LOG(Luminosity(L\/Lo))\",\n    hue=\"Spectral Class\", style=\"Star type\",size=\"Radius(R\/Ro)\", sizes = (50,400)\n)\n\ng.fig.set_figheight(10)\ng.fig.set_figwidth(20)","a7383ec5":"And another one categorical variable:our target **Star type** is a nominal variable (contains values that have no intrinsic ordering).  \nLet's restore the original name so that there is no ordering illusion.","75fa42b1":"The features seem to correlate with the target. Let's check it using ANOVA.","236198f7":"Also let's check the Spearman\u2019s Rank Correlation Coefficient","2139b0df":"> <div class=\"alert alert-block alert-info\">\n<b>Conclusions :<\/b> <br>\n    1) Spectral Class and Star color are correlated with target and could be included for training in our model. <br>\n    2) Spectral Class and Star color are correlated with each other, therefore we will need to exclude some of them from the learning model. About this below. <\/div>","73af0a4f":"#### Categorial and numerical variables (ANOVA method)","eb2f86f0":"We have some sets of categorial vs numerical variables and let's write a function that implements the ANOVA method by hand  \nFirstly we write a function that will build a table for one set (categorial vs numerical)","7c9198fa":"And now we see Hertzsprung\u2013Russell diagram, where also contains information about Luminosity, Temerature, Spectral class, Radius and target: Star type.  \n","92a7b070":"### Dependence of variables","b18c0740":"#### Categorial variables  \nWe have the next 3 continuous variables including target: Star type (target), Star color, Spectral Class  \nUsing **chi-square** to determine if they are dependent on each other","6fccfee3":"> <div class=\"alert alert-block alert-info\">\n<b>Conclusions :<\/b> <br> All numerical features are correlated with target and could be included for training in our model. <\/div>","a29301f9":"Using Chi-square Distibution Table (for example: [Chi-square Table](https:\/\/www.mathsisfun.com\/data\/chi-square-table.html)) we can see that for p = 0.05 the critical chi-square region is:\n1) Spectral Class vs Star type: 43.77  \n2) Star color vs Star type: 79.08  \n3) Star color vs Spectral Class: about 90  \n\nIt mean that chi-square values is greater than critical chi-square, it therefore the null hypothesis is rejected and the alternate hypothesis is accepted: **the 2 categorical variables being compared are dependent on each other**.\n\nAlso we can see that calculated p-vaules less than critical p-values (p = 0.05)\n","5790c103":"So our continuous variables has monotonic relationships.  \nLook at pairwise dependence of numerical variables using Seaborn module","440b77eb":"And one plot for max F-score","870687df":"We observe **high degree** linear relationship between:  \n* Absolute magnitude(Mv) and Luminosity(L\/Lo)\n* Radius(R\/Ro) and Absolute magnitude(Mv)\n* Radius(R\/Ro) and Luminosity(L\/Lo)  \n\nAnd **moderate degree** linear relationship between:\n* Absolute magnitude(Mv) and Temperature (K)\n* Luminosity(L\/Lo) and Temperature (K)","3c9872c4":"Look at **numerical columns**:","9d742eec":"There is variance in all our different groups (you can verify this by looking at the [F Table](http:\/\/www.socr.ucla.edu\/Applets.dir\/F_Table.html))","9ba4539c":"We see that a significant part of the Temperature, Luminosity and Radius indicators are in the range close to zero relative to the maximum values.\nAdd \u0441ommon logarithm of the Temperature, Luminosity and Radius to our dataset","a81484c2":"For information:\n* 1 \u2014 features are not correlated\n* VIF = [1,5] \u2014 features are moderately correlated\n* VIF > 5 \u2014 features are highly correlated\n* VIF > 10 \u2014 high correlation between features and is cause for concern\n\n","00ea4a44":"It is useful to visualize the distribution of the data using a Boxplot. We are interested in the following sets:  \n1) Star type vs Absolute magnitude(Mv)  \n2) Star type vs Radius(R\/Ro)  \n3) Star type vs Luminosity(L\/Lo)  \n4) Star type vs Temperature (K)","8c5f368f":"#### Numeric variables  \nWe have the next 4 continuous variables: Temperature (K), Luminosity(L\/Lo), Radius(R\/Ro), Absolute magnitude(Mv).  \nUsing seaborn let's see Pearson correlation of continuous variables","1024772d":"##### Multicollinearity for numerical variables  \nTo exclude some of numerical variables using VIF \u2014 Variance Inflation Factor.  \nVIF allows you to determine the strength of the correlation between the various independent variables. It is calculated by taking a variable and regressing it against every other variables.","f2a69632":"![eso0728c.jpeg](attachment:05548c78-1b43-49e8-a0ac-e2742e98735c.jpeg)","22e815f8":"## Data analysis  \n\nLoad the dataset and look at the data:","b3e6c328":"Build a graph using seaborn.replot. This allows you to display a lot of information at once.","19186c73":"> <div class=\"alert alert-block alert-info\">\n<b>Conclusions :<\/b> <br> Our variables (not target) depend on each other, therefore we will need to exclude some of them from the learning model <\/div>","5d18dded":"Let's check all sets \"Categorial and numerical variables\"","fd43018e":"Firstly look how numerical variables are distributed","947527e9":"Next check how numerical features are correlated with other categorial variables: Spectral Class and Star color","b7e53884":"## About dataset\nThe dataset contains information about:\n1. Absolute Temperature (in K)\n2. Relative Luminosity (L\/Lo)\n3. Relative Radius (R\/Ro)\n4. Absolute Magnitude (Mv)\n5. Star Color (white,Red,Blue,Yellow,yellow-orange etc)\n6. Spectral Class (O,B,A,F,G,K,,M)\n7. Star Type **(Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , SuperGiants, HyperGiants)**  \n\nLo = 3.828 x 10^26 Watts (Avg Luminosity of Sun)  \nRo = 6.9551 x 10^8 m (Avg Radius of Sun)","4426d2a3":"Now we write a function that will implements the method ANOVA","087c18d0":"And one plot for max F-score","035c281b":"Using F Distibution Table (for example: [F Table](http:\/\/www.socr.ucla.edu\/Applets.dir\/F_Table.html)) we can see that for alpha = 0.05 the critical F region is about 2.2141\n\nIt mean that our F-values (F=1113,91) is greater than critical F-value, it therefore the null hypothesis is rejected and the alternate hypothesis is accepted: **the 2 categorical variables being compared are dependent on each other**.\n\nAlso we can see that calculated p-vaules less than critical p-values (p = 0.05)","6af7d7e0":"Look at **categorical columns**:","10ac062d":"**Continuation:** In the next notebook we will start machine learning.","068fa143":"### Dataset visualization","527350ad":"# STAR DATASET (1) Analysis","b2f2ecba":"We see that most of the values are lost closer to zero, which makes it impossible to look at the whole picture.  \nLet's use the previously calculated logarithms.","9f7a25a8":"We see that the values are duplicated in various ways. Let's bring everything to uniformity using **pandas**:","6594824f":"> <div class=\"alert alert-block alert-info\">\n<b>Conclusions :<\/b> <br> So our features are moderately correlated, but not highly correlated. \u0410or now, let's hold all the features for the learning models. <\/div>"}}