{"cell_type":{"995c9563":"code","efc6b265":"code","6e3da0c8":"code","305d9c43":"code","6355ca74":"code","d285a6fb":"code","10aab084":"code","e5295297":"code","5a82972d":"code","38b7fd09":"code","eb479c2f":"code","ca6b7ec2":"code","78212418":"code","e57b1ca2":"code","c86c3565":"code","ef54a98f":"code","be723597":"code","78039791":"code","eb4f226f":"code","a5561e7e":"code","f92253fe":"code","18c57bfc":"code","f5174ff7":"code","d453f949":"code","60f59acb":"code","17f1f06d":"code","53000123":"code","b2aa6691":"code","6aa980f3":"code","ada1f1fe":"code","8a63b1a0":"code","e86c6d39":"code","5abe4762":"code","27d835ee":"code","0e6e4ea3":"code","57fb821d":"code","eba8d16e":"code","8fdc3aa7":"code","6dd7dadf":"code","ebae46ad":"code","9ba2ce02":"code","bf20b7e1":"code","d2aa8e71":"code","c90719c0":"code","9ac92002":"code","96e81d1a":"code","01a45819":"code","c954c792":"code","3e57e545":"code","fc2e8363":"code","03c7b986":"code","0e8749a1":"code","97517410":"code","afe4d0ff":"code","6d76b60c":"code","1c913eee":"code","0c54418b":"code","69ded9de":"code","2496a0cf":"code","d7f441f1":"code","91748c25":"code","59792137":"code","e650d546":"code","3daa567a":"code","963874db":"code","544c7674":"code","f2b22aa2":"code","fcd4ac83":"code","854c3f22":"code","028d96cb":"code","5902938d":"code","72e97cfc":"code","73587d5b":"code","381eab5d":"code","1c1bb9a1":"code","2cebe36c":"code","298e4c9c":"code","5c1eec50":"code","be04e073":"code","70566dc3":"code","a13d93e9":"code","ae433deb":"code","149d49cf":"code","04857234":"code","a9c9227c":"markdown","f94ea99b":"markdown","bb5448c4":"markdown","41ab1169":"markdown","e8a61081":"markdown","c6040d46":"markdown","f10857a0":"markdown","70b20b70":"markdown","ea2cd902":"markdown","ecbcb08b":"markdown","8caac673":"markdown","402c836e":"markdown","1d3caeea":"markdown","5c01b8a0":"markdown","70e25dba":"markdown","077cc934":"markdown","5a888527":"markdown"},"source":{"995c9563":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","efc6b265":"## \ub370\uc774\ud130 \uc124\uba85\n\n# SalePrice - \ubd80\ub3d9\uc0b0\uc758 \ud310\ub9e4 \uac00\uaca9(\ub2ec\ub7ec)\uc785\ub2c8\ub2e4. \uc774\uac83\uc740 \uc608\uce21\ud558\ub824\ub294 \ub300\uc0c1 \ubcc0\uc218\uc785\ub2c8\ub2e4.\n# MSSubClass : \uac74\ubb3c \ud074\ub798\uc2a4\n# MSZoning : \uc77c\ubc18 zoning \ubd84\ub958\n# LotFrontage : \ubd80\ub3d9\uc0b0\uc5d0 \uc5f0\uacb0\ub41c \uac70\ub9ac\uc758 \uc120\ud615 \ud53c\ud2b8\n# LotArea : \ud3c9\ubc29 \ud53c\ud2b8 \ub2e8\uc704\uc758 \ubd80\uc9c0 \ud06c\uae30\n# \uac70\ub9ac : \ub3c4\ub85c \uc811\uadfc \uc720\ud615\n# \uace8\ubaa9 : \uace8\ubaa9 \uc811\uadfc \ubc29\uc2dd\n# LotShape : \uc18d\uc131\uc758 \uc77c\ubc18\uc801\uc778 \ubaa8\uc591\n# LandContour : \ubd80\ub3d9\uc0b0\uc758 \ud3c9\ud0c4\ub3c4\n# \uc720\ud2f8\ub9ac\ud2f0 : \uc0ac\uc6a9 \uac00\ub2a5\ud55c \uc720\ud2f8\ub9ac\ud2f0 \uc720\ud615\n# LotConfig : \ub85c\ud2b8 \uad6c\uc131\n# LandSlope : \uc18d\uc131\uc758 \uae30\uc6b8\uae30\n# \uc778\uadfc \uc9c0\uc5ed : Ames \uc2dc \uacbd\uacc4 \ub0b4\uc758 \ubb3c\ub9ac\uc801 \uc704\uce58\n# \uc870\uac741 : \uac04\uc120\ub3c4\ub85c \ub610\ub294 \ucca0\ub3c4\uc640 \uc778\uc811\n# \uc870\uac742 : \uac04\uc120\ub3c4\ub85c \ub610\ub294 \ucca0\ub3c4\uc640\uc758 \uadfc\uc811\uc131(\ucd08\uac00 \uc788\ub294 \uacbd\uc6b0)\n# BldgType : \uc8fc\uac70 \uc720\ud615\n# HouseStyle : \uc8fc\uac70 \uc2a4\ud0c0\uc77c\n# OverallQual : \uc804\uccb4 \uc7ac\uc9c8\uacfc \ub9c8\uac10 \ud488\uc9c8\n# OverallCond : \uc804\uccb4 \uc0c1\ud0dc \ub4f1\uae09\n# \uc5f0\ub3c4 : \uc6d0\ub798 \uac74\uc124 \ub0a0\uc9dc\n# YearRemodAdd : \ub9ac\ubaa8\ub378\ub9c1 \ub0a0\uc9dc\n# RoofStyle : \uc9c0\ubd95 \uc720\ud615\n# RoofMatl : \uc9c0\ubd95 \uc7ac\ub8cc\n# \uc678\ubd801\ucc28 : \uc8fc\ud0dd\uc758 \uc678\ubd80 \ud53c\ubcf5\n# \uc678\ubd802\ucc28 : \uc8fc\ud0dd\uc758 \uc678\ubd80 \ud53c\ubcf5\uc7ac(2\uac1c \uc774\uc0c1\uc758 \uc7ac\ub8cc\uc778 \uacbd\uc6b0)\n# MasVnrType : \uc11d\uc870 \ubca0\ub2c8\uc5b4 \uc720\ud615\n# MasVnrArea : \uc11d\uc870 \ubca0\ub2c8\uc5b4\ud310 \uba74\uc801(\uc81c\uacf1\ud53c\ud2b8)\n# ExterQual : \uc678\uc7a5\uc7ac \ud488\uc9c8\n# ExterCond : \uc678\uc7a5\uc7ac\uc758 \ud604\ud669\n# \uae30\ucd08 : \uae30\ucd08\uc758 \uc885\ub958\n# BsmtQual : \uc9c0\ud558\uc2e4 \ub192\uc774\n# BsmtCond : \uc9c0\ud558\uc2e4\uc758 \uc77c\ubc18 \uc0c1\ud0dc\n# BsmtExposure : \ud30c\uc5c5 \ub610\ub294 \uc815\uc6d0 \uc218\uc900\uc758 \uc9c0\ud558 \ubcbd\n# BsmtFinType1 : \uc9c0\ud558\uc2e4 \ub9c8\uac10\uba74\uc801 \ud488\uc9c8\n# BsmtFinSF1 : 1\uc885 \uc81c\uacf1\ud53c\ud2b8 \uc644\uc131\n# BsmtFinType2 : \ub450 \ubc88\uc9f8 \uc644\uc131 \uc601\uc5ed\uc758 \ud488\uc9c8(\uc788\ub294 \uacbd\uc6b0)\n# BsmtFinSF2 : \uc720\ud615 2 \uc644\uc131 \ud3c9\ubc29 \ud53c\ud2b8\n# BsmtUnfSF : \uc9c0\ud558\uc2e4\uc758 \ubbf8\uc644\uc131 \ud3c9\ubc29 \ud53c\ud2b8\n# TotalBsmtSF : \uc9c0\ud558 \uba74\uc801\uc758 \ucd1d \ud3c9\ubc29 \ud53c\ud2b8\n# \ub09c\ubc29 : \ub09c\ubc29\uc758 \uc885\ub958\n# HeatingQC : \ub09c\ubc29 \ud488\uc9c8 \ubc0f \uc0c1\ud0dc\n# CentralAir : \uc911\uc559 \uc5d0\uc5b4\ucee8\n# \uc804\uae30 : \uc804\uae30 \uc2dc\uc2a4\ud15c\n# 1stFlrSF : 1\uce35 \ud3c9\ubc29\ud53c\ud2b8\n# 2ndFlrSF : 2\uce35 \ud3c9\ubc29\ud53c\ud2b8\n# LowQualFinSF : \uc800\ud488\uc9c8 \ub9c8\uac10 \ud3c9\ubc29 \ud53c\ud2b8(\ubaa8\ub4e0 \uce35)\n# GrLivArea : \uc9c0\uc0c1(\uc9c0\uc0c1) \uac70\uc2e4 \uba74\uc801 \ud3c9\ubc29\ud53c\ud2b8\n# BsmtFullBath : \uc9c0\ud558 \uc804\uccb4 \uc695\uc2e4\n# BsmtHalfBath : \uc9c0\ud558 \ubc18 \ud654\uc7a5\uc2e4\n# FullBath : \ub4f1\uae09 \uc774\uc0c1\uc758 \uc804\uccb4 \uc695\uc2e4\n# HalfBath : \ub4f1\uae09 \uc774\uc0c1\uc758 \ubc18\uc695\n# \uce68\uc2e4 : \uc9c0\ud558\uce35 \uc774\uc0c1\uc758 \uce68\uc2e4 \uc218\n# \uc8fc\ubc29 : \uc8fc\ubc29 \uc218\n# KitchenQual : \uc8fc\ubc29 \ud488\uc9c8\n# TotRmsAbvGrd : \ub4f1\uae09 \uc774\uc0c1\uc758 \ucd1d \ubc29(\ud654\uc7a5\uc2e4 \uc81c\uc678)\n# \uae30\ub2a5 : \ud648 \uae30\ub2a5 \ub4f1\uae09\n# \ubcbd\ub09c\ub85c : \ubcbd\ub09c\ub85c\uc758 \uc218\n# FireplaceQu : \ubcbd\ub09c\ub85c \ud488\uc9c8\n# GarageType : \ucc28\uace0 \uc704\uce58\n# GarageYrBlt : \ucc28\uace0 \uac74\uc124 \uc5f0\ub3c4\n# GarageFinish : \ucc28\uace0 \uc778\ud14c\ub9ac\uc5b4 \ub9c8\uac10\n# GarageCars : \ucc28\uace0\uc758 \ucc28\uace0 \ud06c\uae30\n# GarageArea : \ud3c9\ubc29 \ud53c\ud2b8\uc758 \ucc28\uace0 \ud06c\uae30\n# GarageQual : \ucc28\uace0 \ud488\uc9c8\n# GarageCond : \ucc28\uace0 \uc0c1\ud0dc\n# PavedDrive : \ud3ec\uc7a5\ub41c \ucc28\ub3c4\n# WoodDeckSF : \ud3c9\ubc29 \ud53c\ud2b8\uc758 \ubaa9\uc7ac \ub370\ud06c \uba74\uc801\n# OpenPorchSF : \ud3c9\ubc29 \ud53c\ud2b8\uc758 \uc624\ud508 \ubca0\ub780\ub2e4 \uc601\uc5ed\n# EnclosedPorch : \ud3c9\ubc29 \ud53c\ud2b8\uc758 \ub2eb\ud78c \ubca0\ub780\ub2e4 \uc601\uc5ed\n# 3SsnPorch : \uc81c\uacf1\ud53c\ud2b8\uc758 3\uacc4\uc808 \ubca0\ub780\ub2e4 \uba74\uc801\n# ScreenPorch : \ud3c9\ubc29 \ud53c\ud2b8\uc758 \uc2a4\ud06c\ub9b0 \ubca0\ub780\ub2e4 \uba74\uc801\n# PoolArea : \ud3c9\ubc29 \ud53c\ud2b8\uc758 \uc218\uc601\uc7a5 \uba74\uc801\n# PoolQC : \uc218\uc601\uc7a5 \ud488\uc9c8\n# \uc6b8\ud0c0\ub9ac : \uc6b8\ud0c0\ub9ac \ud488\uc9c8\n# MiscFeature : \ub2e4\ub978 \ubc94\uc8fc\uc5d0\uc11c \ub2e4\ub8e8\uc9c0 \uc54a\ub294 \uae30\ud0c0 \uae30\ub2a5\n# MiscVal : \uae30\ud0c0 \uae30\ub2a5\uc758 $\uac12\n# MoSold : \uc6d4 \ud310\ub9e4\n# YrSold : \ub144\ub3c4 \ud310\ub9e4\n# SaleType : \ud310\ub9e4 \uc720\ud615\n# SaleCondition : \ud310\ub9e4 \uc870\uac74","6e3da0c8":"## import some necessary librairies\n\nimport numpy as np #linear algebra\nimport pandas as pd # data processing, csv file I\/O (e.g. pd.read_csv)\n\n%matplotlib inline      \n##notebook\uc744 \uc2e4\ud589\ud55c \ube0c\ub77c\uc6b0\uc800\uc5d0\uc11c \ubc14\ub85c \uadf8\ub9bc\uc744  \uc218 \uc788\uac8c \ud574\uc8fc\ub294 \uac83\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\n\n\n","305d9c43":"## \ub370\uc774\ud130 \ud504\ub808\uc784 \ud615\ud0dc\ub85c \ub370\uc774\ud130 \uac00\uc838\uc624\uae30\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","6355ca74":"train.head(5)","d285a6fb":"train.shape","10aab084":"test.head(5)","e5295297":"#check the numbers of samples and features\n\nprint(\"The train data size before dropping Id feature is : {}\".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {}\".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the 'Id' column since it's unnecessary for the prediction process.\ntrain.drop(\"Id\", axis=1, inplace=True)\ntest.drop(\"Id\", axis=1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {}\".format(train.shape))\nprint(\"The test data size after dropping Id feature is : {}\".format(test.shape))","5a82972d":"### Documentation for the Ames Housing Data indicates that there are outliers present in the training data","38b7fd09":"## \uc774\uc0c1\uce58 \ud655\uc778\n\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('Saleprice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","eb479c2f":"## \uc624\ub978\ucabd \uc544\ub798\uc5d0 2\uac1c\uc758 \uc774\uc0c1\uce58 \ud655\uc778 -> \ub9e4\uc6b0 \ub0ae\uc740 saleprice\n## \ud574\ub2f9 \uc774\uc0c1\uce58 \uc0ad\uc81c","ca6b7ec2":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000)&(train['SalePrice']<300000)].index)\n\n#check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","78212418":"## Note:\n## \uc774\uc0c1\uac12 \uc81c\uac70\ub294 \ud56d\uc0c1 \uc548\uc804\ud569\ub2c8\ub2e4. \n## \uc6b0\ub9ac\ub294 \uc774 \ub450 \uac00\uc9c0\uac00 \ub9e4\uc6b0 \uac70\ub300\ud558\uace0 \ub9e4\uc6b0 \ub098\uc058\uae30 \ub54c\ubb38\uc5d0 \uc0ad\uc81c\ud558\uae30\ub85c \uacb0\uc815\ud588\uc2b5\ub2c8\ub2e4(\ub9e4\uc6b0 \uc800\ub834\ud55c \uac00\uaca9\uc5d0 \ub9e4\uc6b0 \ud070 \uc601\uc5ed).\n\n## \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \ub2e4\ub978 \uc774\uc0c1\uac12\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n## \uadf8\ub7ec\ub098 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0 \uc774\uc0c1\uce58\uac00 \uc788\ub294 \uacbd\uc6b0 \ubaa8\ub4e0 \ud56d\ubaa9\uc744 \uc81c\uac70\ud558\uba74 \ubaa8\ub378\uc5d0 \ub098\uc05c \uc601\ud5a5\uc744 \uc904 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \uadf8\uac83\ub4e4\uc744 \ubaa8\ub450 \uc81c\uac70\ud558\ub294 \ub300\uc2e0 \uc77c\ubd80 \ubaa8\ub378\uc744 \uacac\uace0\ud558\uac8c \ub9cc\ub4e4 \uac83\uc785\ub2c8\ub2e4. \n## \uc774\uc5d0 \ub300\ud574\uc11c\ub294 \uc774 \ub178\ud2b8\ubd81\uc758 \ubaa8\ub378\ub9c1 \ubd80\ubd84\uc744 \ucc38\uace0\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.","e57b1ca2":"### SalePrice is the variable we need to predict.\n### So let's do some analysis on this variable first\n\nsns.distplot(train['SalePrice'], fit=norm); #fit=norm \uc815\uaddc\ubd84\ud3ec \uace1\uc120 \ub098\ud0c0\ub0a8\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu,sigma))\n\n# Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:2f})'.format(mu, sigma)],\n          loc='best')  ## legend \uac00\uc7a5 \uc801\ud569\ud55c \uc790\ub9ac\uc5d0\nplt.ylabel('Freqency')\nplt.title('SalePrice distribution')\n\n# Get also the QQ-plot\n# QQ-plot: \uc815\uaddc\ubaa8\uc9d1\ub2e8 \uac00\uc815\uc744 \ud558\ub294 \ubc29\ubc95 \uc911 \ud558\ub098\ub85c \uc218\uc9d1 \ub370\uc774\ud130\ub97c \ud45c\uc900\uc815\uaddc\ubd84\ud3ec\uc758 \ubd84\uc704\uc218\uc640 \ube44\uad50\ud558\uc5ec \uadf8\ub9ac\ub294 \uadf8\ub798\ud504\n# \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\uc131\uc744 \ub530\ub978\ub2e4\uba74 \uc9c1\uc120\uc758 \ud615\ud0dc\ub85c \uadf8\ub824\uc9c0\uac8c \ub428\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","c86c3565":"## The target variable is right skewed\n## As (linear) models love normally distributed data, we need to transform this variable and make it more normally distributed.","ef54a98f":"# We use the numpy funtion log1p which applies log(1+x) to all elements of the column\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# Check the new distribution\nsns.distplot(train['SalePrice'], fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu,sigma))\n\n# Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})'.format(mu, sigma)],\n          loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","be723597":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train,test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all data size is : {}\".format(all_data.shape))","78039791":"## \uacb0\uce21\uac12 \ud655\uc778\n\nall_data_na = (all_data.isnull().sum()\/len(all_data))*100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio': all_data_na})\nmissing_data.head(20)","eb4f226f":"## \uacb0\uce21\uce58 \uc2dc\uac01\ud654\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y = all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","a5561e7e":"## Data Correlation\n\n# Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","f92253fe":"### - PoolQC : data description says NA means \"No Pool\". \n### That make sense, given the huge ratio of missing value(+99%) and majority of houses have no Pool at all in general\n\nall_data['PoolQC'] = all_data['PoolQC'].fillna(\"None\")","18c57bfc":"### MiscFeature: data description says NA means \"no misc feature\"\n\nall_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')","f5174ff7":"### Alley: data description says NA means \"no alley access\"\n\nall_data['Alley'] = all_data['Alley'].fillna(\"None\")","d453f949":"### Fence: data description says NA means \"no fence\"\n\nall_data['Fence'] = all_data['Fence'].fillna(\"None\")","60f59acb":"### FireplaceQu: data description says NA means \"no fireplace\"\n\nall_data['FireplaceQu'] = all_data['FireplaceQu'].fillna(\"None\")","17f1f06d":"### LotFrontage : \uc9d1 \uc18d\uc131\uc5d0 \uc5f0\uacb0\ub41c \uac01 \uac70\ub9ac\uc758 \uba74\uc801\uc740 \uc774\uc6c3\uc758 \ub2e4\ub978 \uc9d1\uacfc \uba74\uc801\uc774 \ube44\uc2b7\ud560 \uac00\ub2a5\uc131\uc774 \ub192\uc73c\ubbc0\ub85c \n### \uc774\uc6c3\uc758 LotFrontage \uc911\uc559\uac12\uc73c\ub85c \uacb0\uce21\uac12 \ucc44\uc6b0\uae30\n\n#Group by neighborhood and fill in missing value by the medain LotFrontage of all the neighborhood\n\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","53000123":"### GargeType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n\nfor col in ('GarageType', 'GarageFinish','GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","b2aa6691":"### GarageYrBit, GarageArea and GarageCars: Replacing missing data with 0 (Since No garage = no cars in such garage)\n\nfor col in ('GarageYrBlt','GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","6aa980f3":"### BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath \n## : missing values are likely zero for having no basement\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","ada1f1fe":"### BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2\n### : For all these categorical basement-related features, NaN means that there is no basement\n\nfor col in ('BsmtQual', 'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","8a63b1a0":"### MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses.\n### We can fill 0 for the area and None for the type.\n\nall_data['MasVnrType'] = all_data[\"MasVnrType\"].fillna('None')\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)","e86c6d39":"### MSZoning (The general zoning classification) : 'RL' is by far the most common value.\n### So we can fill in missing values with 'RL'\n\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","5abe4762":"### Utilities: For this categorical feature all records are 'AIIPub', except for one 'NoSeWa' and 2 NA\n### Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling.\n### We can then safely remove it\n \nall_data = all_data.drop(['Utilities'], axis=1)","27d835ee":"train['Utilities'].value_counts()","0e6e4ea3":"# Functional: data description says NA means typical\n\nall_data['Functional'] = all_data['Functional'].fillna('Typ')","57fb821d":"# Electrical: It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","eba8d16e":"# KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\n\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","8fdc3aa7":"# Exterior1st and Exterior2nd: Again Both Exterior 1& 2 have only one missing value.\n# We wiil just substitute in the most common string\n\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","6dd7dadf":"# SaleType: Fill in again with most frequent which is \"WD\"\n\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","ebae46ad":"# MSSubClass: Na most likely means No building class. We can replace missing values with None\n\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna('None')","9ba2ce02":"# Check remaining missing values if any\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'Missing Ratio': all_data_na})\nmissing_data.head()","bf20b7e1":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","d2aa8e71":"from sklearn.preprocessing import LabelEncoder\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(all_data[c].values))\n    all_data[c] = lbl.transform(list(all_data[c].values))\n    \n#shape\nprint('Shape all_data: {}'.format(all_data.shape))","c90719c0":"# Adding total sqfootage feature\n\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","9ac92002":"# Skewed features\n\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew': skewed_feats})\nskewness.head(10)","96e81d1a":"#### Box Cox Transformation of (highly) skewed features\n\n### 1+x\uc758 Box-Cox \ubcc0\ud658\uc744 \uacc4\uc0b0\ud558\ub294 scipy \ud568\uc218 boxcox1p\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n### \u03bb=0\uc744 \uc124\uc815\ud558\ub294 \uac83\uc740 \ubaa9\ud45c \ubcc0\uc218\uc5d0 \ub300\ud574 \uc704\uc5d0\uc11c \uc0ac\uc6a9\ud55c log1p\uc640 \ub3d9\uc77c\ud569\ub2c8\ub2e4.","01a45819":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","c954c792":"### Getting dummy categorical features\n\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","3e57e545":"### Getting the new train and test sets\n\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","fc2e8363":"### import librairies\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","03c7b986":"### Define a cross validation strategy\n\n### Sklearn\uc758 cross_val_score \ud568\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \n### \uadf8\ub7ec\ub098 \uc774 \ud568\uc218\uc5d0\ub294 \uc154\ud50c \uc18d\uc131\uc774 \uc5c6\uc73c\ubbc0\ub85c \uad50\ucc28 \uac80\uc99d \uc804\uc5d0 \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc11e\uae30 \uc704\ud574 \ud55c \uc904\uc758 \ucf54\ub4dc\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.","0e8749a1":"# Validation function\nn_folds = 5  ## \ub370\uc774\ud130 \ubd84\ud560 \uc218\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\n\n#cross_val_score\uac19\uc740 \uacbd\uc6b0, \uc778\uc790\uc5d0 'neg'\ub77c\ub294 \ub2e8\uc5b4\ub97c \ubd99\uc5ec\uc8fc\uc5b4\uc57c \ud558\ub294\ub370 \uc774\ub294 Negative(\uc74c\uc218)\ub97c \ub73b\ud55c\ub2e4. \uc65c \uc74c\uc218\ub97c \ubd99\uc5ec\uc8fc\ub294 \uac78\uae4c? \uadf8 \uc774\uc720\ub294 \ubc14\ub85c cross_val_score\ub098 GridSearchCV \ud568\uc218\uc758 \uacbd\uc6b0 score\ub97c \ucd9c\ub825\ud574\uc904 \ub54c \uac00\uc7a5 \ub192\uc740 \uc218\uce58\uc758 score\ub97c \ucd9c\ub825\ud558\ub3c4\ub85d \ub418\uc5b4\uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uc608\ub97c \ub4e4\uc5b4, cross_val_score\ub85c \uad50\ucc28\uac80\uc99d\uc744 3\ubc88 \uc2e4\uc2dc\ud55c \ud6c4 \uac01\uac01\uc758 MSE\uac12\uc774 [3, 5, 7] \uc774\ub77c\ub294 \uacb0\uacfc\uac12\uc774 \ub098\uc654\ub2e4. \uc774 \ub54c cross_val_score\uc758 \uac00\uc7a5 \ub192\uc740 \uc218\uce58\uc758 score\ub97c \ucd9c\ub825\ud558\ub294 \ud2b9\uc131\ub300\ub85c \ud55c\ub2e4\uba74 MSE\uac12\uc740 \ub0ae\uc744\uc218\ub85d \uc608\uce21\uc131\ub2a5\uc774 \uc88b\ub2e4\ub294 \uc758\ubbf8\uc784\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 '7'\uc774\ub77c\ub294 \uac00\uc7a5 \uc548\uc88b\uc740 MSE\uac12\uc744 \ucd9c\ub825\ud558\uac8c \ub41c\ub2e4. \ub530\ub77c\uc11c \uacb0\uacfc\uac12\ub4e4\uc758 \uc21c\uc704\ub97c \ubc14\uafd4\uc8fc\uae30 \uc704\ud574\uc11c neg(negative=\uc74c\uc218)\ub97c \ubd99\uc5ec -1\uc744 \uacf1\ud574\uc8fc\uac8c \ub418\ub294 \uac83\uc774\ub2e4. \n#\ubc29\uae08 \ub4e0 \uc608\uc2dc\ub85c \ub4e0\ub2e4\uba74 [3, 5, 7] \uc5d0\ub2e4\uac00 \uac01\uac01 -1\uc744 \uacf1\ud574\uc8fc\uc5b4 [-3, -5, -7]\uc774 \ub418\uba74\uc11c \uc774 \uc138\uac1c\uc758 \uc218\uce58 \uc911 \uac00\uc7a5 \ud070 \uac12\uc740 -3\uc774\ubbc0\ub85c cross_val_score\ub294 -3\uc774\ub77c\ub294 \uacb0\uacfc\ubb3c\uc744 \ucd9c\ub825\ud558\uac8c \ub41c\ub2e4.(\uadf8\ub798\uc11c \uba85\ud655\ud55c metric \uc9c0\ud45c\ub97c \ucd9c\ub825\ud558\ub824\uba74 \ub2e4\uc2dc -1\uc744 \uacf1\ud574\uc8fc\uc5b4 \ucd9c\ub825\uc2dc\ucf1c\uc57c \ud55c\ub2e4.)","97517410":"### Lasso Regression : \uc774 \ubaa8\ub378\uc740 \uc774\uc0c1\uac12\uc5d0 \ub9e4\uc6b0 \ubbfc\uac10\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc6b0\ub9ac\ub294 \uadf8\uac83\ub4e4\uc5d0 \ub300\ud574 \ub354 \uac15\ub825\ud558\uac8c \ub9cc\ub4e4 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4. \n### \uc774\ub97c \uc704\ud574 \ud30c\uc774\ud504\ub77c\uc778\uc5d0\uc11c sklearn\uc758 Robustscaler() \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","afe4d0ff":"## RobustScaler: \ud2b9\uc131\ub4e4\uc774 \uac19\uc740 \uc2a4\ucf00\uc77c\uc744 \uac16\uac8c \ub418\uc9c0\ub9cc \ud3c9\uade0\ub300\uc2e0 \uc911\uc559\uac12\uc744 \uc0ac\uc6a9 ==> \uadf9\ub2e8\uac12\uc5d0 \uc601\ud5a5\uc744 \ubc1b\uc9c0 \uc54a\uc74c\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.0005, random_state=1))","6d76b60c":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","1c913eee":"### Elastic Net Regression: again made robust to outliers\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","0c54418b":"### Kernel Ridge Regression : X\ub97c \ucee4\ub110\ud568\uc218\ub97c \ud65c\uc6a9\ud558\uc5ec Mapping\ud55c \ubcc0\uc218\ub97c \ud65c\uc6a9\ud558\uc5ec \ud30c\ub77c\ubbf8\ud130\ub97c \ucd94\uc815\ud558\ub294 \uac83\n### Mapping\ub41c Value\ub97c \uac00\uc9c0\uace0 Ridge Regression\uc744 \uc218\ud589\ud558\ub294 \uac83\n\n# kernel: \uc54c\uace0\ub9ac\uc998\uc5d0 \uc0ac\uc6a9\ud560 \ucee4\ub110\uc744 \uacb0\uc815('linear','poly','rbf','sigmoid','precomputed')\n# degree: polynomial \ud568\uc218\uc758 \ucc28\uc218\n# gamma: kernel\uc758 \uac8c\uc218\n# coef0: kernel \ud568\uc218\uc758 \ub3c5\ub9bd\uc801\uc778 \uad6c\uac04\n# max_iter: \ubc18\ubcf5\ud69f\uc218 \uc81c\ud55c \uac12\n\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","69ded9de":"### Gradient Boosting Regression: with huber loss that makes it robust to outliers\n### Gradient Boosting Regression\uc740 \uc5ec\ub7ec\uac1c\uc758 decision tree\ub97c \ubb36\uc5b4 \uac15\ub825\ud55c model\uc744 \ub9cc\ub4dc\ub294 \uc559\uc0c1\ube14 \uae30\ubc95\n### random forest\uc640 \ub2ec\ub9ac gradient boosting model\uc740 \uc774\uc804 tree\uc758 \uc624\ucc28\ub97c \ubcf4\uc644\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c tree\ub97c \ub9cc\ub4e0\ub2e4\n\n# learning_rate\ub97c \ub192\uc774\uba74 \ubcf4\uc815\uc744 \uac15\ud558\uac8c \ud558\uae30 \ub54c\ubb38\uc5d0 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc744 \ub9cc\ub4e0\ub2e4.\n# n_estimator \uac12\uc744 \ud0a4\uc6b0\uba74 ensemble\uc5d0 \ud2b8\ub9ac\uac00 \ub354 \ub9ce\uc774 \ucd94\uac00\ub418\uc5b4 \ubaa8\ub378\uc758 \ubcf5\uc7a1\ub3c4\uac00 \ucee4\uc9c0\uace0 train \uc138\ud2b8\ub97c \ub354 \uc815\ud655\ud558\uac8c fitting\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4,\n                                  max_features='sqrt', min_samples_leaf=15, min_samples_split=10,\n                                  loss='huber',random_state=5)","2496a0cf":"### XGBoost\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n                            learning_rate=0.05, max_depth=3,\n                            min_child_weight=1.7817, n_estimators=2200,\n                            reg_alpha=0.4640, reg_lambda=0.8571,\n                            subsample=0.5213, silent=1,\n                            random_state=7, nthread=-1)","d7f441f1":"### LightGBM\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","91748c25":"### Base models scores\n\n#### Let's see how these base models perform on the data by evaluating the cross-validation rmsle error","59792137":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e650d546":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3daa567a":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","963874db":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","544c7674":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f2b22aa2":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","fcd4ac83":"### Simplest Stacking approach: Averaging base models\n\n### We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse","854c3f22":"## Averaged base models class\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","028d96cb":"#### Averaged base models score\n\n### We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.","5902938d":"averaged_models = AveragingModels(models=(ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","72e97cfc":"### It seems even the simplest stacking approach really improve the score . \n### This encourages us to go further and explore a less simple stacking approch.","73587d5b":"### Less simple Stacking : \uba54\ud0c0 \ubaa8\ub378 \ucd94\uac00\n# \uc774 \uc811\uadfc \ubc29\uc2dd\uc5d0\uc11c\ub294 \ud3c9\uade0 \uae30\ubc18 \ubaa8\ub378\uc5d0 \uba54\ud0c0 \ubaa8\ub378\uc744 \ucd94\uac00\ud558\uace0 \uc774\ub7ec\ud55c \uae30\ubcf8 \ubaa8\ub378\uc758 out-of-fold \uc608\uce21\uc744 \uc0ac\uc6a9\ud558\uc5ec \uba54\ud0c0 \ubaa8\ub378\uc744 \ud6c8\ub828\ud569\ub2c8\ub2e4.\n\n# \ud6c8\ub828 \ubd80\ubd84\uc5d0 \ub300\ud55c \uc808\ucc28\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \uc124\uba85\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n# 1. \ucd1d \ud6c8\ub828 \uc138\ud2b8\ub97c \ub450 \uac1c\uc758 \ubd84\ub9ac\ub41c \uc138\ud2b8\ub85c \ub098\ub215\ub2c8\ub2e4(\uc5ec\uae30\uc11c\ub294 train \ubc0f .holdout ).\n\n# 2. \uccab \ubc88\uc9f8 \ubd80\ubd84\uc5d0\uc11c \uc5ec\ub7ec \uae30\ubcf8 \ubaa8\ub378 \ud559\uc2b5(train)\n\n# 3.\ub450 \ubc88\uc9f8 \ubd80\ubd84\uc5d0\uc11c \uc774\ub7ec\ud55c \uae30\ubcf8 \ubaa8\ub378\uc744 \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4(\ud640\ub4dc\uc544\uc6c3).\n\n# 4. 3)\uc758 \uc608\uce21(out-of-folds \uc608\uce21\uc774\ub77c\uace0 \ud568)\uc744 \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uace0 \uc62c\ubc14\ub978 \uc751\ub2f5(\ubaa9\ud45c \ubcc0\uc218)\uc744 \ucd9c\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \uba54\ud0c0 \ubaa8\ub378\uc774\ub77c\uace0 \ud558\ub294 \ub354 \ub192\uc740 \uc218\uc900\uc758 \ud559\uc2b5\uc790\ub97c \ud6c8\ub828\uc2dc\ud0b5\ub2c8\ub2e4.\n\n### \ucc98\uc74c \uc138 \ub2e8\uacc4\ub294 \ubc18\ubcf5\uc801\uc73c\ub85c \uc218\ud589\ub429\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 5\uacb9 \uc313\uae30 \ub97c \uc608\ub85c \ub4e4\uba74 \uba3c\uc800 \ud6c8\ub828 \ub370\uc774\ud130\ub97c 5\uacb9\uc73c\ub85c \ub098\ub215\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c 5\ubc88\uc758 \ubc18\ubcf5 \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uac01 \ubc18\ubcf5\uc5d0\uc11c \ubaa8\ub4e0 \uae30\ubcf8 \ubaa8\ub378\uc744 4\uacb9\uc73c\ub85c \ud6c8\ub828\ud558\uace0 \ub098\uba38\uc9c0 \uc811\uae30(\ud640\ub4dc\uc544\uc6c3 \ud3f4\ub4dc)\ub97c \uc608\uce21\ud569\ub2c8\ub2e4.\n\n### \ub530\ub77c\uc11c 5\ubc88\uc758 \ubc18\ubcf5 \ud6c4\uc5d0 \uc804\uccb4 \ub370\uc774\ud130\uac00 out-of-folds \uc608\uce21\uc744 \uc5bb\ub294 \ub370 \uc0ac\uc6a9\ub418\uba70 4\ub2e8\uacc4\uc5d0\uc11c \uba54\ud0c0 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uae30 \uc704\ud55c \uc0c8\ub85c\uc6b4 \uae30\ub2a5\uc73c\ub85c \uc0ac\uc6a9\ud560 \uac83\uc784\uc744 \ud655\uc2e0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n### \uc608\uce21 \ubd80\ubd84\uc758 \uacbd\uc6b0 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uae30\ubcf8 \ubaa8\ub378\uc758 \uc608\uce21\uc744 \ud3c9\uade0\ud654\ud558\uace0 \uba54\ud0c0 \uae30\ub2a5\uc73c\ub85c \uc0ac\uc6a9\ud588\uc73c\uba70 \ucd5c\uc885 \uc608\uce21\uc740 \uba54\ud0c0 \ubaa8\ub378\ub85c \uc218\ud589\ub429\ub2c8\ub2e4.","381eab5d":"### Stacking averaged Models Class\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","1c1bb9a1":"### Stacking Averaged models Score\n\n### (\ub3d9\uc77c\ud55c \uc218\uc758 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec) \ub450 \uac00\uc9c0 \uc811\uadfc \ubc29\uc2dd\uc744 \ube44\uad50\ud560 \uc218 \uc788\ub3c4\ub85d Enet KRR\uacfc Gboost\ub97c \ud3c9\uade0\ud55c \ub2e4\uc74c lasso meta-model\ub85c \ucd94\uac00\ud569\ub2c8\ub2e4","2cebe36c":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","298e4c9c":"## Ensembling StackedRegressor, XGBoost and LightGBM\n\n### We add XGBoost and LightGBM to the StackedRegressor defined previously","5c1eec50":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","be04e073":"#### Final Training and Prediction\n\n### StackedRegressor:\n\nstacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","70566dc3":"### XGBoost:\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","a13d93e9":"### LightGBM:\n\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","ae433deb":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","149d49cf":"### Ensemble Prediction:\n\nensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","04857234":"# Submission\n\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv', index=False)","a9c9227c":"### Features engineering","f94ea99b":"#### Transforming some numerical variables that are really categorical","bb5448c4":"#### Label Encoding some categorical variables that may contain information in their ordering set","41ab1169":"#### \uacb0\uce21\uac12 \ub300\uccb4\n\n##### We impute them by proceeding sequentially through features with missing values","e8a61081":"#### Outliers","c6040d46":"## More features engineering","f10857a0":"### Modelling","70b20b70":" #### Base models","ea2cd902":"### Target Variable","ecbcb08b":"#### Stacking models","8caac673":"#### It remains no missing value","402c836e":"#### Adding one more important feature","1d3caeea":"##### let's first concatenate the train and test data in the same dataframe","5c01b8a0":"##### Since area related features are very important to determine house prices\n#####  ,we add one more feature which is the total area of basement, first and second floor areas of each house","70e25dba":"### Data Processing","077cc934":"#### Log-transformation of the target variable","5a888527":"##### The skew seems now corrected and the data appears more normally distributed"}}