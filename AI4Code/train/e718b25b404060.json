{"cell_type":{"31218806":"code","3689629a":"code","35506be9":"code","06e57c30":"code","5ff5c7ca":"code","cc92500b":"code","26a22112":"code","75d3344b":"code","7b2c0c4c":"code","e80376a9":"code","470a2b33":"code","b08b3d86":"code","f0bbbc08":"code","4e36ace7":"markdown","0fc902ea":"markdown","833ddb52":"markdown","86cfe3e5":"markdown","6a417eff":"markdown","42866461":"markdown","4876c454":"markdown","1dec3f6c":"markdown","3b0e2fce":"markdown","51e9b5bc":"markdown","0e50e0d0":"markdown","881f2c0d":"markdown","ef73ad4f":"markdown","e903b968":"markdown"},"source":{"31218806":"import numpy as np\nimport pandas as pd\n\npd.reset_option('^display.', silent=True)\npd.set_option('mode.chained_assignment', None)\n\n# Load the full dataset\ndf = pd.read_csv('\/kaggle\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv')\n\n# List the first five passengers and their fate\ndf.head()","3689629a":"# Check for null values\nnull_value_stats = df.isnull().sum(axis=0)\nnull_value_stats[null_value_stats != 0]","35506be9":"# Print the data types\nprint(df.dtypes)\n\n# Delete unused variables\ndf = df.drop(['PassengerId', 'Country', 'Firstname', 'Lastname'],axis=1)\n\n# Save indices of categorial features (Sex, category)\ncategorical_features_indices = np.where(df.dtypes != np.int64)[0]\n\n# Show the final dataframe\ndf.head()","06e57c30":"from sklearn.model_selection import train_test_split\n\n# Split X and y\nX = df.drop('Survived', axis=1)\ny = df.Survived\n\n# Make a train and validation set of the data\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, stratify=y, random_state=0)","5ff5c7ca":"from catboost import CatBoostClassifier, Pool, cv\nfrom sklearn.metrics import accuracy_score\n\nmodel = CatBoostClassifier(custom_loss=['Accuracy'],\n                           random_seed=0,\n                           verbose=200)\n\nmodel.fit(X_train,\n          y_train,\n          cat_features=categorical_features_indices,\n          eval_set=(X_val, y_val),\n          plot=True);","cc92500b":"# Set log loss as the log function\ncv_params = model.get_params()\ncv_params.update({ 'loss_function': 'Logloss' })\n\ncv_data = cv(\n    Pool(X, y, cat_features=categorical_features_indices),\n    cv_params,\n    fold_count=5,\n    plot=True\n)","26a22112":"# Print the best validation accuracy and its iteration\nprint('Best validation accuracy score: {:.2f}\u00b1{:.2f} at step {}'.format(\n    np.max(cv_data['test-Accuracy-mean']),\n    cv_data['test-Accuracy-std'][np.argmax(cv_data['test-Accuracy-mean'])],\n    np.argmax(cv_data['test-Accuracy-mean'])\n))","75d3344b":"from time import time\nparams = {\n    'iterations': 500,\n    'learning_rate': 0.1,\n    'eval_metric': 'Accuracy',\n    'random_seed': 0,\n    'verbose': 200,\n    'use_best_model': False\n}\n\ntrain_pool = Pool(X_train, y_train, cat_features=categorical_features_indices)\nvalidate_pool = Pool(X_val, y_val, cat_features=categorical_features_indices)\n\nno_stop_model = CatBoostClassifier(**params)\nt0 = time()\nno_stop_model.fit(train_pool, eval_set=validate_pool)\nprint(\"Training time no stopping:\", round(time()-t0, 4), \"s\")","7b2c0c4c":"params.update({\n    'od_type': 'Iter',\n    'od_wait': 40\n})\nearly_stop_model = CatBoostClassifier(**params)\nt0 = time()\nearly_stop_model.fit(train_pool, eval_set=validate_pool);\nprint(\"Training time early stopping:\", round(time()-t0, 4), \"s\")","e80376a9":"print(f'No stop model tree count: {no_stop_model.tree_count_}')\nprint(f'No stop model validation accuracy: {accuracy_score(y_val, no_stop_model.predict(X_val))}')\nprint()\nprint(f'Early stop model tree count: {early_stop_model.tree_count_}')\nprint(f'Early stop model validation accuracy: {accuracy_score(y_val, early_stop_model.predict(X_val))}')","470a2b33":"params = {\n    'iterations': 5,\n    'eval_metric': 'Accuracy',\n    'random_seed': 0,\n    'verbose': 200\n}\nmodel_snapshot = CatBoostClassifier(**params)\nmodel_snapshot.fit(train_pool, eval_set=validate_pool, save_snapshot=True)\n\nparams.update({\n    'iterations': 10,\n    'learning_rate': 0.1,\n})\nmodel_snapshot = CatBoostClassifier(**params)\nmodel_snapshot.fit(train_pool, eval_set=validate_pool, save_snapshot=True)","b08b3d86":"model = CatBoostClassifier(iterations=50, random_seed=0, logging_level='Silent').fit(train_pool)\nfeature_importances = model.get_feature_importance(train_pool)\nfeature_names = X_train.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    print('{}: {}'.format(name, score))","f0bbbc08":"predictions = model.predict(X_val)\npredictions_probs = model.predict_proba(X_val)\nprint(f'Predictions of classes: {predictions[:10]}')\nprint(f'Prediction of probs: {predictions_probs[:10]}')","4e36ace7":"# Feature importance\n\nSince CatBoost is a tree-based library, it comes with a feature importance attribute out of the box. This is great for understanding why the model predicts the way it does and can also help you select the best features, if you were to re-train the model. Apperantly **Age** is slightly more important that **Sex**, whereas **Category** is of less significance.","0fc902ea":"# Model training\n\nNow let's create the model itself: We would go here with default parameters (as they provide a really good baseline almost all the time), the only thing should specify here is custom_loss parameter, as this would give us an ability to see what's going on in terms of this competition metric - accuracy, as well as to be able to watch for logloss, as it would be more smooth on dataset of such size.\n\nWe'll make the model and fit it on the data. We should see some cool plots from the process.[](http:\/\/)","833ddb52":"# Model predictions\n\nLet's find the survivors. CatBoost offers the well-known predict() and predict_proba() methods to make class and probability predictions. Apparently, our model thinks that the first 10 passengers in the validation set all died. Here, passenger 10 came closest to surviving with a probability of 32%.","86cfe3e5":"# The snapshot feature\n\nIt's possible to save snapshots at each iteration with CatBoost. You can use it for recovering training after an interruption or for starting training with previous results. During the training, CatBoost makes these snapshots \u2014 backup copies of intermediate results. The next time you train, it will simply pick up where it left. In this case, the completed iterations of building trees don't need to be repeated.","6a417eff":"# Explicit model params\n\nFor convenience, you can define the params explicitly and easily change them before modelling. The below code sets some useful parameters and fits the model without plotting. It uses a learning rate of 0.1 and 500 iterations. If you have a validation set, you should set *use_best_model* to True during training. This way, the resulting tress ensemble is shrinking to the best iteration.","42866461":"Lastly we'll split the dataset in two parts - a training and a validation set.","4876c454":"# Model cross-validation\n\nWe can run cross-validation as well with some nice plots. This tells CatBoost to record the log loss values at each iteartion. The use a fold count here of 5 with shuffling on. You can set verbose here too to get more details. When you run the block, it will plot the graphs live, which is pretty cool. We use the params_update() method here to change the previous parameters.","1dec3f6c":"# Feature preparation\n\nFirst let's check for any null values in the data.","3b0e2fce":"![](https:\/\/i.imgur.com\/vdca02S.png)\n\n# Introduction\n\nI've recently being playing around with CatBoost and thought it has some pretty cool features that would be fun to try on this dataset. CatBoost's an open-source ML library that uses gradient boosting on decision trees and has a rich toolkit for things like GPU-based training, model analysis, live metrics and visualization. In this nootebook we'll look at CatBoost using the Estonia dataset.\n\nAbout the data: The datset contains details, like names, age, gender and fate of the 989 passengers abord the MS Estonia at the night of the sinking, 28 September 1994. The MS Estonia was a cruise ferry built in 1979\/80 and used to the capricious wheater of the Baltic Sea, but at the night of the 28 a mixture of mechanical failures and poor decision-making collided, which led to her sinking. The diasater claimed 852 lives, which are all accounted for the in data.\n\nNote: Unfortunealy CatBoost plots do not work on Kaggle, but if you run the notebook they do.\n\nFind CatBoost at: https:\/\/catboost.ai\/\n\nTechnical introduction: https:\/\/catboost.ai\/news\/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus\n\nAlso check out F\u00e9lix Revert's excellent article on CatBoost: https:\/\/towardsdatascience.com\/why-you-should-learn-catboost-now-390fb3895f76","51e9b5bc":"# Early stopping\n\nCatBoost offers a **overfitting-detector** (early stopping) feature to stop training, when validation accuracy stops improving beyond a certain iteration. Here we use the iteration count and wait a maximum of 40 iterations.","0e50e0d0":"As you can see, it is possible to watch our model learn through some nice plots. You can enable logging_level=Verbose for fit() if you'd like more output. We're presented with logloss first for the train and validation set, with the training decreasing for all iterations and the validation increasing after iteration 100.\n\nIf we look at accuracy, with a default CatBoost configuration we can see that the best accuracy value of 0.88 (on training set) was acheived about iteration 400. The validation accuracy speaks about iteration 250, then it starts overfitting the data.","881f2c0d":"Lucky for us, there weren't any. Next let's see what kind of data types we're dealing with. We should either exclude or encode the categorial columns. We'll also delete some of the variables that are not usable for modelling, like name, passenger id and country. These features have no say in predicting the probability for survival. If they have, likely some bias has crept in.","ef73ad4f":"Notice we cut the training time down by 1\/10 by enabling early stopping! Lets see how the two models compare in terms of complexity (tree count) and performance (accuracy). It turns out, that you actually get more for less in this case.","e903b968":"The cv_data variable contains the result of the CV process. Let's view the mean test accuracy and the best iteration. Overfitting sets in as well when using CV, as the best iteration when measuring validation accuracy was at iteration 162. This means that we should ideally had stopped training here, since the validation error does not improve beyond this point. Another apporach is early stopping, that stops training prematurely when validation accuracy does not improve. We will see that next."}}