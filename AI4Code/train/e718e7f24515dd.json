{"cell_type":{"5fe0967e":"code","056df714":"code","97c8ce5e":"code","11c91c22":"code","3af56154":"code","b0717474":"code","59a6c077":"code","1ed86d66":"code","5455520c":"code","58e50973":"code","166c19ae":"code","d5462409":"code","56c8aa25":"markdown","5d6ab9c6":"markdown","9961ea50":"markdown"},"source":{"5fe0967e":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.initializers import he_normal","056df714":"# Loading the data\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)\nprint(train_data[0])","97c8ce5e":"# For shits and giggles - decoding words from numeric back to english\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\ndecoded_review","11c91c22":"# Preparing data\n# Encoding the integer sequences into a binary matrix\ndef vectorize_sequences(sequences, dimension  = 10000):\n    # Create an all-zero matrix of shape (len(sequences), dimension)\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        # Sets specific indicies of results[i] to 1s\n        results[i, sequence] = 1.\n    return results\n\n# Vectorize data\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\n# Vectorize targets\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","3af56154":"x_train[0]","b0717474":"y_train","59a6c077":"# Function to plot learning curves\ndef lc():\n    H = model.history.history\n    fig = plt.figure(figsize = (19, 6))\n    plt.subplot(121)\n    plt.plot(H['accuracy'], label = 'acc')\n    \n    try:\n        plt.plot(H['val_accuracy'], label = 'val_acc')\n    except:\n        pass\n    \n    plt.grid(); plt.legend()\n    \n    plt.subplot(122)\n    plt.plot(H['loss'], label = 'loss')\n    \n    try:\n        plt.plot(H['val_loss'], label = 'val_loss')\n    except:\n        pass\n    \n    plt.grid(); plt.legend()\n    plt.show()","1ed86d66":"# Train model on 15000 samples and validate on 10000\nmodel = Sequential()\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nmodel.fit(x_train, y_train, epochs = 20, batch_size = 512, validation_split = 0.4, verbose = 0)\n\n# Learning curves\nlc()\n\n# Overfitting","5455520c":"# Train model on 4 epochs and full train data and evaluate on test data\nmodel = Sequential()\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nmodel.fit(x_train, y_train, epochs = 4, batch_size = 512, verbose = 0)\n\nresults = model.evaluate(x_test, y_test)\nprint(f'Test data loss: {results[0]}\\nTest data accuracy: {results[1]}')\n\n# Learning curves\nlc()","58e50973":"def create_model(layers, act = 'relu', output_act = 'sigmoid', loss = 'binary_crossentropy'):\n    init = he_normal(seed = 666)\n    model = Sequential()\n    for layer in layers:\n        model.add(Dense(layer, activation = act, kernel_initializer = init))\n    model.add(Dense(1, activation = output_act, kernel_initializer = init))\n    \n    model.compile(optimizer = 'rmsprop', loss = loss, metrics = ['accuracy'])\n    return model\n\ndef evaluate():\n    results = model.evaluate(x_test, y_test)\n    print(f'Test data loss: {results[0]}\\nTest data accuracy: {results[1]}')\n    return results","166c19ae":"# Setting up experiments\n# Dict key notation: [hidden layers], activation of hidden layers, activation of output layer, loss function\nexperiments = {'[16], relu, sigm, bc': create_model([16]),\n               '[16, 16], relu, sigm, bc': create_model([16, 16]),\n               '[16, 16, 16], relu, sigm, bc': create_model([16, 16, 16]),\n               '[32], relu, sigm, bc': create_model([32]),\n               '[32, 32], relu, sigm, bc': create_model([32, 32]),\n               '[32, 32, 32], relu, sigm, bc': create_model([32, 32, 32]),\n               '[64], relu, sigm, bc': create_model([64]),\n               '[64, 64], relu, sigm, bc': create_model([64, 64]),\n               '[64, 64, 64], relu, sigm, bc': create_model([64, 64, 64]),\n               '[16, 16], relu, sigm, mse': create_model([16, 16], loss = 'mse'),\n               '[16, 16], tanh, sigm, bc': create_model([16, 16], act = 'tanh')}\n\n# Dictionary to store evaluation results\nresults = dict()\n\n# Loo through all experiments\nfor experiment in experiments:\n    print(experiment)\n    \n    model = experiments[experiment]\n    checkpoint = ModelCheckpoint('best_model.hdf5', save_best_only = True, save_weights_only = True)\n    model.fit(x_train, y_train, epochs = 10, batch_size = 512, validation_split = 0.4, verbose = 0, callbacks = [checkpoint])\n    \n    # Load best weights\n    model.load_weights('best_model.hdf5')\n    \n    results[experiment] = evaluate()\n    lc()","d5462409":"fig = plt.figure(figsize = (19, 6))\nfor r in results:\n    plt.bar(r, results[r][1], label = r)\nplt.xticks([], [])\nplt.legend(loc = 4); plt.grid()\nplt.show()","56c8aa25":"### Further experiments","5d6ab9c6":"### Basic model","9961ea50":"### Back to the basics\n\nThis kernel is no more than my homework on IMDB dataset. Here I want to run some experiments using very simple ANN architecture with Dense layers.\n\nI'm not chasing for high accuracies here, all I want is to look how will behave models with different parameters."}}