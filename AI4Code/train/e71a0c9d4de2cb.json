{"cell_type":{"fba8e0d0":"code","4c827778":"code","a88eaf03":"code","07c3aeb6":"code","820a5641":"code","9443c37d":"code","1505bcd3":"code","ea9afe65":"code","4de0dbf7":"code","20f0a5cc":"code","71a4a1e6":"code","cf835a4c":"code","ab852f51":"code","cc931c36":"code","9a70d336":"code","df2e0029":"code","146de98e":"code","426a545b":"code","107c6276":"code","27e76eba":"code","12ae58d7":"code","bf878ebd":"code","335cbd52":"code","807bd5df":"code","5a00151b":"code","2c077252":"code","d343793e":"code","27a6c5aa":"code","02e20471":"code","f88a5981":"code","b1652824":"code","f50d7f63":"code","4d30d043":"code","25630fca":"code","7582e9e7":"code","50fafe5c":"code","8460cf98":"code","92a5a12c":"code","f1391a35":"code","10fb5f7b":"code","af340128":"code","00dd8759":"code","8741bcdd":"code","c70c5a60":"code","e11dc577":"code","538ed1da":"code","b0b91cca":"code","e2693d78":"code","222529e9":"code","4f7d5fac":"code","50af4e80":"code","f533ffd1":"code","9d64178e":"code","a7d9b879":"code","63a78825":"code","eac86c23":"code","75500068":"code","41d4daa2":"code","a4883307":"code","0838a62b":"code","50aa3773":"code","7eb903bd":"code","ebcd8093":"code","e9287828":"code","58e316bd":"code","e0435edf":"code","19ae6e86":"code","2291e601":"code","8f3f9378":"code","eca29340":"code","361cb01a":"code","07c35254":"code","78ab0ff7":"code","07950f2e":"code","f95e9a49":"markdown","6d431c6d":"markdown","249c4426":"markdown","f26b95ed":"markdown"},"source":{"fba8e0d0":"!ls -la ..\/input\/inference-best-lb","4c827778":"import sys\n\n# for kaggle kernel\n# add datasets iterative-stratification and umaplearn\n\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nsys.path.append('..\/input\/umaplearn\/umap')\n%mkdir model\n%mkdir interim\n\nfrom scipy.sparse.csgraph import connected_components\nfrom umap import UMAP\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold, RepeatedMultilabelStratifiedKFold\n\nimport numpy as np\nimport scipy as sp\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\nimport time\n# import joblib\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.manifold import TSNE\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nprint(f\"is cuda available: {torch.cuda.is_available()}\")\n\nimport warnings\n# warnings.filterwarnings('ignore')\n\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nDEFAULT_SEED = 512\nseed_everything(seed_value=DEFAULT_SEED)","a88eaf03":"# file name prefix\nNB = '101'\n\nIS_TRAIN = False ################################################################\n\nMODEL_DIR = \"..\/input\/503-203-tabnet-with-nonscored-features-train\/model\" # \"..\/model\"\nINT_DIR = \"interim\" # \"..\/interim\"\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0","07c3aeb6":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","820a5641":"# test_features_dummy = pd.read_csv('..\/input\/dummytestfeatures\/test_features_dummy.csv')\n# test_features = pd.concat([test_features, test_features_dummy]).reset_index(drop=True)","9443c37d":"from sklearn.preprocessing import QuantileTransformer\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nfor col in (GENES + CELLS):\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = pd.concat([train_features, test_features])[col].values.reshape(vec_len+vec_len_test, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')        \n\n    train_features[col] = transformer.transform(train_features[col].values.reshape(vec_len, 1)).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","1505bcd3":"# GENES\nn_comp = 50\nn_dim = 15\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n\nif IS_TRAIN:\n    pca = PCA(n_components=n_comp, random_state=DEFAULT_SEED).fit(train_features[GENES])\n    umap = UMAP(n_components=n_dim, random_state=DEFAULT_SEED).fit(train_features[GENES])\n    pd.to_pickle(pca, f\"{MODEL_DIR}\/{NB}_pca_g.pkl\")\n    pd.to_pickle(umap, f\"{MODEL_DIR}\/{NB}_umap_g.pkl\")\nelse:\n    pca = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_pca_g.pkl\")\n    umap = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_umap_g.pkl\")\n    \ndata2 = pca.transform(data[GENES])\ndata3 = umap.transform(data[GENES])\n\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain3 = data3[:train_features.shape[0]]\ntest3 = data3[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntrain3 = pd.DataFrame(train3, columns=[f'umap_G-{i}' for i in range(n_dim)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest3 = pd.DataFrame(test3, columns=[f'umap_G-{i}' for i in range(n_dim)])\n\ntrain_features = pd.concat((train_features, train2, train3), axis=1)\ntest_features = pd.concat((test_features, test2, test3), axis=1)\n\n#CELLS\nn_comp = 15\nn_dim = 5\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n\n\nif IS_TRAIN:\n    pca = PCA(n_components=n_comp, random_state=DEFAULT_SEED).fit(train_features[CELLS])\n    umap = UMAP(n_components=n_dim, random_state=DEFAULT_SEED).fit(train_features[CELLS])\n    pd.to_pickle(pca, f\"{MODEL_DIR}\/{NB}_pca_c.pkl\")\n    pd.to_pickle(umap, f\"{MODEL_DIR}\/{NB}_umap_c.pkl\")\nelse:\n    pca = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_pca_c.pkl\")\n    umap = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_umap_c.pkl\")   \n\ndata2 = pca.transform(data[CELLS])\ndata3 = umap.transform(data[CELLS])\n\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain3 = data3[:train_features.shape[0]]\ntest3 = data3[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntrain3 = pd.DataFrame(train3, columns=[f'umap_C-{i}' for i in range(n_dim)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest3 = pd.DataFrame(test3, columns=[f'umap_C-{i}' for i in range(n_dim)])\n\ntrain_features = pd.concat((train_features, train2, train3), axis=1)\ntest_features = pd.concat((test_features, test2, test3), axis=1)\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]","ea9afe65":"from sklearn.feature_selection import VarianceThreshold\n\nif IS_TRAIN:\n    var_thresh = VarianceThreshold(threshold=0.5).fit(train_features.iloc[:, 4:])\n    pd.to_pickle(var_thresh, f\"{MODEL_DIR}\/{NB}_variance_thresh0_5.pkl\")\nelse:\n    var_thresh = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_variance_thresh0_5.pkl\")\n                                \ndata = train_features.append(test_features)\ndata_transformed = var_thresh.transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint(train_features.shape)\nprint(test_features.shape)","4de0dbf7":"train = train_features[train_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","20f0a5cc":"train.to_pickle(f\"{INT_DIR}\/{NB}_train_preprocessed.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}_test_preprocessed.pkl\")","71a4a1e6":"# file name prefix\nNB = '203'\n\n# IS_TRAIN = True\n\n# MODEL_DIR = \"model\" # \"..\/model\"\n# INT_DIR = \"interim\" # \"..\/interim\"\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0\n\n# model hyper params\nHIDDEN_SIZE = 2048\n\n# training hyper params\nEPOCHS = 15\nBATCH_SIZE = 2048\nNFOLDS = 10 # 10\nNREPEATS = 1\nNSEEDS = 5 # 5\n\n# Adam hyper params\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 1e-5\n\n# scheduler hyper params\nPCT_START = 0.2\nDIV_FACS = 1e3\nMAX_LR = 1e-2","cf835a4c":"def process_data(data):    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\ndef calc_valid_log_loss(train, target, target_cols):\n    y_pred = train[target_cols].values\n    y_true = target[target_cols].values\n    \n    y_true_t = torch.from_numpy(y_true.astype(np.float64)).clone()\n    y_pred_t = torch.from_numpy(y_pred.astype(np.float64)).clone()\n    \n    return torch.nn.BCELoss()(y_pred_t, y_true_t).to('cpu').detach().numpy().copy()","ab852f51":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size=HIDDEN_SIZE):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n               \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n                \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","cc931c36":"def run_training(train, test, trn_idx, val_idx, feature_cols, target_cols, fold, seed):\n    \n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    train_df = train_.loc[trn_idx,:].reset_index(drop=True)\n    valid_df = train_.loc[val_idx,:].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=len(feature_cols),\n        num_targets=len(target_cols),\n    )\n    \n    model.to(DEVICE)\n       \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=PCT_START, div_factor=DIV_FACS, \n                                              max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    best_loss_epoch = -1\n    \n    if IS_TRAIN:\n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n\n            if valid_loss < best_loss:            \n                best_loss = valid_loss\n                best_loss_epoch = epoch\n                oof[val_idx] = valid_preds\n                model.to('cpu')\n                torch.save(model.state_dict(), f\"{MODEL_DIR}\/{NB}_nonscored_SEED{seed}_FOLD{fold}_.pth\")\n                model.to(DEVICE)\n\n            if epoch % 10 == 0 or epoch == EPOCHS-1:\n                print(f\"seed: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}, best_loss: {best_loss:.6f}, best_loss_epoch: {best_loss_epoch}\")                           \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=len(feature_cols),\n        num_targets=len(target_cols),\n    )\n    \n    model.load_state_dict(torch.load(f\"{MODEL_DIR}\/{NB}_nonscored_SEED{seed}_FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    if not IS_TRAIN:\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        oof[val_idx] = valid_preds\n\n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","9a70d336":"def run_k_fold(train, test, feature_cols, target_cols, NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    mskf = RepeatedMultilabelStratifiedKFold(n_splits=NFOLDS, n_repeats=NREPEATS, random_state=None)\n    \n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n        oof_, pred_ = run_training(train, test, t_idx, v_idx, feature_cols, target_cols, f, seed)\n        \n        predictions += pred_ \/ NFOLDS \/ NREPEATS\n        oof += oof_ \/ NREPEATS\n        \n    return oof, predictions","df2e0029":"def run_seeds(train, test, feature_cols, target_cols, nfolds=NFOLDS, nseed=NSEEDS):\n    seed_list = range(nseed)\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    time_start = time.time()\n\n    for seed in seed_list:\n\n        oof_, predictions_ = run_k_fold(train, test, feature_cols, target_cols, nfolds, seed)\n        oof += oof_ \/ nseed\n        predictions += predictions_ \/ nseed\n        print(f\"seed {seed}, elapsed time: {time.time() - time_start}\")\n\n    train[target_cols] = oof\n    test[target_cols] = predictions","146de98e":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","426a545b":"train = pd.read_pickle(f\"{INT_DIR}\/101_train_preprocessed.pkl\")\ntest = pd.read_pickle(f\"{INT_DIR}\/101_test_preprocessed.pkl\")","107c6276":"train_trainbook = pd.read_pickle(\"..\/input\/503-203-tabnet-with-nonscored-features-train\/interim\/101_train_preprocessed.pkl\")\ntest_trainbook = pd.read_pickle(\"..\/input\/503-203-tabnet-with-nonscored-features-train\/interim\/101_test_preprocessed.pkl\")","27e76eba":"train_trainbook.head()","12ae58d7":"train.head()","bf878ebd":"test_trainbook.head()","335cbd52":"test.head()","807bd5df":"# remove nonscored labels if all values == 0\ntrain_targets_nonscored = train_targets_nonscored.loc[:, train_targets_nonscored.sum() != 0]\nprint(train_targets_nonscored.shape)\n\ntrain = train.merge(train_targets_nonscored, on='sig_id')","5a00151b":"target = train[train_targets_nonscored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfeature_cols = [c for c in process_data(train).columns if c not in target_cols and c not in ['kfold','sig_id']]","2c077252":"run_seeds(train, test, feature_cols, target_cols)","d343793e":"print(f\"train shape: {train.shape}\")\nprint(f\"test  shape: {test.shape}\")\nprint(f\"features : {len(feature_cols)}\")\nprint(f\"targets  : {len(target_cols)}\")","27a6c5aa":"valid_loss_total = calc_valid_log_loss(train, target, target_cols)\nprint(f\"CV loss: {valid_loss_total}\")","02e20471":"train.to_pickle(f\"{INT_DIR}\/{NB}_train_nonscored_pred.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}_test_nonscored_pred.pkl\")","f88a5981":"valid_results = train_targets_nonscored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_nonscored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","b1652824":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","f50d7f63":"from pytorch_tabnet.tab_model import TabNetRegressor","4d30d043":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","25630fca":"# file name prefix\nNB = '503'\nNB_PREV = '203'\n\n# IS_TRAIN = False\n\n# MODEL_DIR = \"..\/input\/moa503\/503-tabnet\" # \"..\/model\"\n# INT_DIR = \"..\/input\/moa503\/203-nonscored-pred\" # \"..\/interim\"\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0\n\n# model hyper params\n\n# training hyper params\n# EPOCHS = 25\n# BATCH_SIZE = 256\nNFOLDS = 10 # 10\nNREPEATS = 1\nNSEEDS = 3 # 5\n\n# Adam hyper params\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 1e-5\n\n# scheduler hyper params\nPCT_START = 0.2\nDIV_FACS = 1e3\nMAX_LR = 1e-2","7582e9e7":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","50fafe5c":"# test_features_dummy = pd.read_csv('..\/input\/dummytestfeatures\/test_features_dummy.csv')\n# test_features = pd.concat([test_features, test_features_dummy]).reset_index(drop=True)","8460cf98":"print(\"(nsamples, nfeatures)\")\nprint(train_features.shape)\nprint(train_targets_scored.shape)\nprint(train_targets_nonscored.shape)\nprint(test_features.shape)\nprint(sample_submission.shape)","92a5a12c":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","f1391a35":"from sklearn.preprocessing import QuantileTransformer\n\nuse_test_for_preprocessing = False\n\nfor col in (GENES + CELLS):\n\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        if use_test_for_preprocessing:\n            raw_vec = pd.concat([train_features, test_features])[col].values.reshape(vec_len+vec_len_test, 1)\n            transformer.fit(raw_vec)\n        else:\n            raw_vec = train_features[col].values.reshape(vec_len, 1)\n            transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl') \n\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n\n\n    train_features[col] = transformer.transform(train_features[col].values.reshape(vec_len, 1)).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","10fb5f7b":"# GENES\n\nn_comp = 90\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=42).fit(data[GENES])\n    pd.to_pickle(fa, f'{MODEL_DIR}\/{NB}_factor_analysis_g.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}\/{NB}_factor_analysis_g.pkl')\n    \ndata2 = (fa.transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\n\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=42).fit(data[CELLS])\n    pd.to_pickle(fa, f'{MODEL_DIR}\/{NB}_factor_analysis_c.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}\/{NB}_factor_analysis_c.pkl')\n\ndata2 = (fa.transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","af340128":"# features_g = list(train_features.columns[4:776])\n# train_ = train_features[features_g].copy()\n# test_ = test_features[features_g].copy()\n# data = pd.concat([train_, test_], axis = 0)\n# km = KMeans(n_clusters=35, random_state=123).fit(data)","00dd8759":"# km.predict(data)","8741bcdd":"# km.labels_","c70c5a60":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        \n        if IS_TRAIN:\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n            pd.to_pickle(kmeans, f\"{MODEL_DIR}\/{NB}_kmeans_{kind}.pkl\")\n        else:\n            kmeans = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_kmeans_{kind}.pkl\")\n            \n        train[f'clusters_{kind}'] = kmeans.predict(train_)\n        test[f'clusters_{kind}'] = kmeans.predict(test_)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)","e11dc577":"print(train_features.shape)\nprint(test_features.shape)","538ed1da":"def fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n#         df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n#         df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n#         df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)","b0b91cca":"print(train_features.shape)\nprint(test_features.shape)","e2693d78":"remove_vehicle = True\n\nif remove_vehicle:\n    trt_idx = train_features['cp_type']=='trt_cp'\n    train_features = train_features.loc[trt_idx].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[trt_idx].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[trt_idx].reset_index(drop=True)\nelse:\n    pass","222529e9":"# train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n# target = train[train_targets_scored.columns]\ntarget = train[train_targets_scored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","4f7d5fac":"print(target.shape)\nprint(train_features.shape)\nprint(test_features.shape)\nprint(train.shape)\nprint(test.shape)","50af4e80":"train_nonscored_pred = pd.read_pickle(f'{INT_DIR}\/{NB_PREV}_train_nonscored_pred.pkl')\ntest_nonscored_pred = pd.read_pickle(f'{INT_DIR}\/{NB_PREV}_test_nonscored_pred.pkl')","f533ffd1":"# remove nonscored labels if all values == 0\ntrain_targets_nonscored = train_targets_nonscored.loc[:, train_targets_nonscored.sum() != 0]\n\n# nonscored_targets = [c for c in train_targets_nonscored.columns if c != \"sig_id\"]","9d64178e":"train = train.merge(train_nonscored_pred[train_targets_nonscored.columns], on='sig_id')\ntest = test.merge(test_nonscored_pred[train_targets_nonscored.columns], on='sig_id')","a7d9b879":"from sklearn.preprocessing import QuantileTransformer\n\nnonscored_target = [c for c in train_targets_nonscored.columns if c != \"sig_id\"]\n\nfor col in (nonscored_target):\n\n    vec_len = len(train[col].values)\n    vec_len_test = len(test[col].values)\n#     raw_vec = pd.concat([train, test])[col].values.reshape(vec_len+vec_len_test, 1)\n    raw_vec = train[col].values.reshape(vec_len, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')        \n\n    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","63a78825":"feature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]\nlen(feature_cols)","eac86c23":"num_features=len(feature_cols)\nnum_targets=len(target_cols)","75500068":"import torch\nimport torch.nn as nn\nfrom pytorch_tabnet.metrics import Metric\n\nclass LabelSmoothing(nn.Module):\n    \"\"\"\n    NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0, n_cls=2):\n        \"\"\"\n        Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing + smoothing \/ n_cls\n        self.smoothing = smoothing \/ n_cls\n\n    def forward(self, x, target):\n        probs = torch.nn.functional.sigmoid(x,)\n        # ylogy + (1-y)log(1-y)\n        #with torch.no_grad():\n        target1 = self.confidence * target + (1-target) * self.smoothing\n        #print(target1.cpu())\n        loss = -(torch.log(probs+1e-15) * target1 + (1-target1) * torch.log(1-probs+1e-15))\n        #print(loss.cpu())\n        #nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        #nll_loss = nll_loss.squeeze(1)\n        #smooth_loss = -logprobs.mean(dim=-1)\n        #loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n    \nclass SmoothedLogLossMetric(Metric):\n    \"\"\"\n    BCE with logit loss\n    \"\"\"\n    def __init__(self, smoothing=0.001):\n        self._name = f\"{smoothing:.3f}\" # write an understandable name here\n        self._maximize = False\n        self._lossfn = LabelSmoothing(smoothing)\n\n    def __call__(self, y_true, y_score):\n        \"\"\"\n        \"\"\"\n        y_true = torch.from_numpy(y_true.astype(np.float32)).clone()\n        y_score = torch.from_numpy(y_score.astype(np.float32)).clone()\n#         print(\"smoothed log loss metric: \", self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy())\n        return self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy().take(0)\n    \nclass LogLossMetric(Metric):\n    \"\"\"\n    BCE with logit loss\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        self._name = f\"{smoothing:.3f}\" # write an understandable name here\n        self._maximize = False\n        self._lossfn = LabelSmoothing(smoothing)\n\n    def __call__(self, y_true, y_score):\n        \"\"\"\n        \"\"\"\n        y_true = torch.from_numpy(y_true.astype(np.float32)).clone()\n        y_score = torch.from_numpy(y_score.astype(np.float32)).clone()\n#         print(\"log loss metric: \", self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy())\n        return self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy().take(0)","41d4daa2":"def process_data(data):\n#     data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2, 0: 0, 1: 1, 2: 2})\n    data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1, 0: 0, 1: 1})   \n    return data\n\ndef run_training_tabnet(train, test, trn_idx, val_idx, feature_cols, target_cols, fold, seed, filename=\"tabnet\"):\n    \n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    train_df = train_.loc[trn_idx,:].reset_index(drop=True)\n    valid_df = train_.loc[val_idx,:].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n        \n    model = TabNetRegressor(n_d=32, n_a=32, n_steps=1, lambda_sparse=0,\n                            cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n                            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                            mask_type='entmax',  # device_name=DEVICE,\n                            scheduler_params=dict(milestones=[100, 150], gamma=0.9),#)\n                            scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,\n                            verbose=10,\n                            seed = seed)\n    \n    loss_fn = LabelSmoothing(0.001)\n#     eval_metric = SmoothedLogLossMetric(0.001)\n#     eval_metric_nosmoothing = SmoothedLogLossMetric(0.)\n       \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    \n    if IS_TRAIN:\n#         print(\"isnan\", np.any(np.isnan(x_train)))\n        model.fit(X_train=x_train, y_train=y_train,\n                  eval_set=[(x_valid, y_valid)], eval_metric=[LogLossMetric, SmoothedLogLossMetric],\n                  max_epochs=200, patience=50, batch_size=1024, virtual_batch_size=128,\n                    num_workers=0, drop_last=False, loss_fn=loss_fn\n                  )\n        model.save_model(f\"{MODEL_DIR}\/{NB}_{filename}_SEED{seed}_FOLD{fold}\")\n            \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    \n    model = TabNetRegressor(n_d=32, n_a=32, n_steps=1, lambda_sparse=0,\n                            cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n                            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                            mask_type='entmax',  # device_name=DEVICE,\n                            scheduler_params=dict(milestones=[100, 150], gamma=0.9),#)\n                            scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,\n                            verbose=10,\n                            seed = seed)\n    \n    model.load_model(f\"{MODEL_DIR}\/{NB}_{filename}_SEED{seed}_FOLD{fold}.model\")\n\n    valid_preds = model.predict(x_valid)\n\n    valid_preds = torch.sigmoid(torch.as_tensor(valid_preds)).detach().cpu().numpy()\n    oof[val_idx] = valid_preds\n        \n    predictions = model.predict(x_test)\n    predictions = torch.sigmoid(torch.as_tensor(predictions)).detach().cpu().numpy()\n    \n    return oof, predictions","a4883307":"def run_k_fold(train, test, feature_cols, target_cols, NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state = seed)\n    \n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n        oof_, pred_ = run_training_tabnet(train, test, t_idx, v_idx, feature_cols, target_cols, f, seed)\n        \n        predictions += pred_ \/ NFOLDS \/ NREPEATS\n        oof += oof_ \/ NREPEATS\n        \n    return oof, predictions\n\ndef run_seeds(train, test, feature_cols, target_cols, nfolds=NFOLDS, nseed=NSEEDS):\n    seed_list = range(nseed)\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    time_start = time.time()\n\n    for seed in seed_list:\n\n        oof_, predictions_ = run_k_fold(train, test, feature_cols, target_cols, nfolds, seed)\n        oof += oof_ \/ nseed\n        predictions += predictions_ \/ nseed\n        print(f\"seed {seed}, elapsed time: {time.time() - time_start}\")\n\n    train[target_cols] = oof\n    test[target_cols] = predictions","0838a62b":"train.to_pickle(f\"{INT_DIR}\/{NB}_pre_train.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}_pre_test.pkl\")","50aa3773":"run_seeds(train, test, feature_cols, target_cols, NFOLDS, NSEEDS)","7eb903bd":"train.to_pickle(f\"{INT_DIR}\/{NB}_train.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}_test.pkl\")","ebcd8093":"# train[target_cols] = np.maximum(PMIN, np.minimum(PMAX, train[target_cols]))\nvalid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","e9287828":"sub6 = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub6.to_csv('submission.csv', index=False)","58e316bd":"sub6","e0435edf":"import glob\n","19ae6e86":"!mkdir -p \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/gen-efficientnet-pretrained\/tf_efficientnet_*.pth \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/deepinsight-resnest-v2-resnest50-output\/resnest50_fast_2s2x40d-9d126481.pth \/root\/.cache\/torch\/hub\/checkpoints\/\n!ls -la \/root\/.cache\/torch\/hub\/checkpoints\/\n","2291e601":"!python ..\/input\/inference-best-lb\/deepinsight-resnest-lightning-v2-inference.py\nsub5 = pd.read_csv('submission_resnest_v2.csv')","8f3f9378":"!python ..\/input\/inference-best-lb\/deepinsight-efficientnet-lightning-v7-b3-inference.py\nsub4 = pd.read_csv('.\/submission_effnet_v7_b3.csv')","eca29340":"! python ..\/input\/inference-best-lb\/simple-nn-new-split-inference.py\nsub3 = pd.read_csv('.\/submission.csv')","361cb01a":"test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n!python ..\/input\/inference-best-lb\/2heads-ResNest-inference.py\nsub2 = pd.read_csv('.\/submission.csv')","07c35254":"!python ..\/input\/inference-best-lb\/3stage-nn-inference.py\nsub1 = pd.read_csv('.\/submission_2stageNN_with_ns_oldcv_0.01822.csv')","78ab0ff7":"!python ..\/input\/inference-best-lb\/simple-nn-old-split-inference.py\nsub7 = pd.read_csv('submission_script_simpleNN_oldcv_0.01836.csv')","07950f2e":"submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nsubmission.iloc[:, 1:] = 0\nsubmission.iloc[:, 1:] = (sub1.iloc[:,1:]*0.37 + sub3.iloc[:,1:]*0.1 + sub4.iloc[:,1:]*0.18 +sub5.iloc[:,1:]*0.15)*0.9 + sub6.iloc[:,1:]*0.1 + sub7.iloc[:,1:]*0.09 + sub2.iloc[:,1:]*0.09\nsubmission.to_csv('submission.csv', index=False)","f95e9a49":"## 503-203-tabnet-with-nonscored-features-10fold3seed","6d431c6d":"## 203-101-nonscored-pred-2layers.ipynb","249c4426":"### non-scored labels prediction","f26b95ed":"## 101-preprocess.ipynb"}}