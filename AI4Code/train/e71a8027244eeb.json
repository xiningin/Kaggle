{"cell_type":{"bd50fefc":"code","9fc23ce9":"code","753e1ef2":"code","104c4083":"code","eb94df20":"code","919f0c6e":"code","d69b4d19":"code","608b5bbb":"code","990996d4":"code","7f58a058":"code","546d94b4":"code","183ceb83":"code","e676cfcf":"code","e1377e5a":"code","2627be12":"code","b3f1b524":"code","a1ebec15":"code","47928382":"code","6e3fe312":"code","a0c774d4":"code","1450081b":"code","205479d7":"code","0be8650c":"markdown"},"source":{"bd50fefc":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps=PorterStemmer()","9fc23ce9":"train_df=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\n","753e1ef2":"train_df.isnull().sum()","104c4083":"X = train_df.drop([\"id\"],axis=1)\nX.head()","eb94df20":"key = X[\"keyword\"].value_counts().index[0]\nprint(key)\nloc = X[\"location\"].value_counts().index[0]\nprint(loc)","919f0c6e":"train_df['keyword'] = train_df['keyword'].fillna(train_df['keyword'].value_counts().idxmax())\ntrain_df['location'] = train_df['location'].fillna(train_df['location'].value_counts().idxmax())\ntrain_df.head()\n","d69b4d19":"test_df['keyword'] = test_df['keyword'].fillna(test_df['keyword'].value_counts().idxmax())\ntest_df['location'] = test_df['location'].fillna(test_df['location'].value_counts().idxmax())\ntest_df.isnull().sum()\n","608b5bbb":"X=train_df.iloc[:,1:4]\ny=train_df.iloc[:,4:]\nX[\"text\"]=X[\"text\"].apply(lambda x:\" \".join(x.lower() for x in x.split())) # Lower words\nX[\"text\"]=X[\"text\"].str.replace(\"[^\\w\\s]\",\" \") # Clear \".,!#\"\nX[\"text\"]=X[\"text\"].str.replace(\"\\d\",\" \") # Clear Number\nX[\"text\"]=X[\"text\"].str.replace(\"https\",\" \") # Clear https\n\n# Stopwords\nsw=stopwords.words(\"english\")\nX[\"text\"]=X[\"text\"].apply(lambda x:\" \".join(x for x in x.split() if x not in sw))\n# Lemmi\nfrom textblob import Word\nnltk.download(\"wordnet\")\nX[\"text\"]=X[\"text\"].apply(lambda x:\" \".join([Word(x).lemmatize() for x in x.split()]))\nX[\"text\"][:5]\n\n","990996d4":"# For Test Data\ntest_x=test_df.iloc[:,1:4]\ntest_y=test_df.iloc[:,4:]\n\ntest_x[\"text\"]=test_x[\"text\"].apply(lambda x:\" \".join(x.lower() for x in x.split() ))\ntest_x[\"text\"]=test_x[\"text\"].str.replace(\"[^\\w\\s]\",\" \")\ntest_x[\"text\"]=test_x[\"text\"].str.replace(\"\\d\",\" \")\ntest_x[\"text\"]=test_x[\"text\"].apply(lambda x:\" \".join(x for x in x.split() if x not in sw))\ntest_x[\"text\"]=test_x[\"text\"].str.replace(\"https\",\" \") # Clear https\n\n#Lemmi\ntest_x[\"text\"]=test_x[\"text\"].apply(lambda x:\" \".join([Word(x).lemmatize() for x in x.split()]))\ntest_x[\"text\"][:5]\n\n\n","7f58a058":"X['sentence']=X['keyword']+\" \"+X[\"text\"]\ntrain_text = X['sentence'].values\nprint(train_text)\n\n","546d94b4":"test_x['sentence']=test_x['keyword']+\" \"+test_x[\"text\"]\ntest_text=test_x['sentence'].values\nprint(test_text)","183ceb83":"x_train,x_test,y_train,y_test=train_test_split(train_text,y,test_size=0.2,random_state=1)","e676cfcf":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7)\nvectorizer.fit(x_train)\nprint(x_train)\nx_train_tfidf=vectorizer.transform(x_train).toarray()\nx_test_tfidf=vectorizer.transform(x_test).toarray()\ntest_last=vectorizer.transform(test_text).toarray()\n\n\n\n","e1377e5a":"from sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\nbn=BernoulliNB(alpha=0.2)\nbn.fit(x_train_tfidf,y_train)\ny_pred=bn.predict(x_test_tfidf)\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nf1=f1_score(y_test,y_pred)\nprint(f1)\ncr=classification_report(y_test,y_pred)\nprint(cr)\naccuracy=cross_val_score(bn,x_test_tfidf,y_test,cv=10).mean()\nprint(accuracy)\n","2627be12":"prediction1=bn.predict(test_last)\ndata = {'id':test_df[\"id\"],'target':prediction1}\noutput = pd.DataFrame(data, columns = ['id','target'])\noutput.index = test_df.index\n\noutput.to_csv(\"submission.csv\", index = False)    \n\n\na = pd.read_csv(\"submission.csv\")\na","b3f1b524":"pip install catboost","a1ebec15":"from catboost import CatBoostClassifier\n\ncb=CatBoostClassifier()\ncb.fit(x_train_tfidf,y_train)\ny_pred=cb.predict(x_test_tfidf)\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nf1=f1_score(y_test,y_pred)\nprint(f1)\ncr=classification_report(y_test,y_pred)\nprint(cr)\naccuracy=cross_val_score(bn,x_test_tfidf,y_test,cv=10).mean()\nprint(accuracy)","47928382":"catb_params={\"iterations\":[200,500,750],\n             \"learning_rate\":[0.01,0.05,0.1],\n             \"depth\":[3,5,8]                  \n            }\nfrom sklearn.model_selection import GridSearchCV","6e3fe312":"cat_b=CatBoostClassifier()\ncatb_model=GridSearchCV(cat_b,catb_params,cv=5,n_jobs=1,verbose=2)\ncatb_model.fit(x_train_tfidf,y_train)\n","a0c774d4":"catb_model.best_params_","1450081b":"cb=CatBoostClassifier(iterations=750,learning_rate=0.05,depth=5)\ncb.fit(x_train_tfidf,y_train)\ny_pred=cb.predict(x_test_tfidf)\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nf1=f1_score(y_test,y_pred)\nprint(f1)\ncr=classification_report(y_test,y_pred)\nprint(cr)\naccuracy=cross_val_score(bn,x_test_tfidf,y_test,cv=10).mean()\nprint(accuracy)","205479d7":"prediction1=bn.predict(test_last)\ndata = {'id':test_df[\"id\"],'target':prediction1}\noutput = pd.DataFrame(data, columns = ['id','target'])\noutput.index = test_df.index\n\noutput.to_csv(\"submission.csv\", index = False)    \n\n\na = pd.read_csv(\"submission.csv\")\na","0be8650c":"# US\u0130NG  CATBOOST\n"}}