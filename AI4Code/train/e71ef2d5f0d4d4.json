{"cell_type":{"5c5265bc":"code","b24d4ea6":"code","961dd685":"code","04528275":"code","8103f993":"code","fb662a69":"code","c4f451b6":"code","f347d912":"code","8f720306":"code","2e7780a0":"code","041366a8":"code","069c0b4b":"code","b8b13655":"code","950f527a":"code","e5f00a27":"code","e2a217b3":"code","89c7f732":"code","5331dfd7":"code","5b460e4a":"code","f294ca99":"code","7b43778b":"code","55d48442":"code","d938a6fd":"code","e9cb6bdc":"code","8e7ed3b6":"code","f2e266be":"code","4828334a":"code","a487a319":"code","3c6a9ea0":"code","12285f82":"code","86f6acdb":"code","f5fa0c2d":"code","3619c87b":"code","fac204f0":"code","0d137e6c":"code","cbdf71c4":"code","90633cde":"code","9e767443":"code","742db177":"code","3e427d10":"code","e6463f41":"code","6b5752be":"code","b78290a5":"code","15a7855a":"code","87fab038":"code","207b06f0":"code","16b808d0":"code","0cb65653":"code","201c57f7":"code","e4bc931b":"code","18c2c7f0":"code","8c7afd30":"code","ebc284b3":"markdown","39af6a02":"markdown","e904dc05":"markdown","059e12c7":"markdown","8854e535":"markdown","3bae504e":"markdown","311d03ef":"markdown","b6cf8cb5":"markdown","95d02d72":"markdown","ab498db7":"markdown","f563e62a":"markdown","f57012d0":"markdown","4a64587f":"markdown","2d83add6":"markdown","04c843eb":"markdown","dc9ab69a":"markdown","33e60a4e":"markdown","8102b947":"markdown","7061eb9f":"markdown","c21e5a8b":"markdown","e58ebcd6":"markdown","71c3269f":"markdown","90db469b":"markdown","7ace5178":"markdown","62bd4de1":"markdown","ced907e7":"markdown","eef4005c":"markdown","606c4999":"markdown"},"source":{"5c5265bc":"import pandas as pd          \nimport numpy as np          # For mathematical calculations\nimport matplotlib.pyplot as plt  # For plotting graphs\nfrom datetime import datetime    # To access datetime\nfrom pandas import Series        # To work on series\n%matplotlib inline\nimport warnings                   # To ignore the warnings\nwarnings.filterwarnings(\"ignore\")","b24d4ea6":"# Now let\u2019s read the data\ncandies=pd.read_csv(\"..\/input\/candy_production.csv\")","961dd685":"candies_original=candies.copy()","04528275":"candies.columns","8103f993":"candies.dtypes","fb662a69":"candies.shape","c4f451b6":"candies.head()","f347d912":"candies.tail()","8f720306":"candies['observation_date'] = pd.to_datetime(candies.observation_date,format='%Y-%m-%d')  \ncandies_original['observation_date'] = pd.to_datetime(candies_original.observation_date,format='%Y-%m-%d')","2e7780a0":"#  let\u2019s extract the year, month and day from the observation_date\nfor i in (candies,candies_original):\n    i['year']=i.observation_date.dt.year \n    i['month']=i.observation_date.dt.month \n    i['day']=i.observation_date.dt.day","041366a8":"candies.head()","069c0b4b":"candies.index = candies['observation_date'] # indexing the Datetime to get the time period on the x-axis.\nts = candies['IPG3113N']\nplt.figure(figsize=(16,8))\nplt.plot(ts, label='% Candy Production')\nplt.title('Candy Production')\nplt.xlabel(\"Time(year)\")\nplt.ylabel(\"% Candy Production\")\nplt.legend(loc='best')","b8b13655":"# let\u2019s look at yearly production count.\nplt.figure(figsize=(16,8))\ncandies.groupby('year')['IPG3113N'].mean().plot.bar()","950f527a":"# let\u2019s look at monthly production count.\nplt.figure(figsize=(16,8))\ncandies.groupby('month')['IPG3113N'].mean().plot.bar()","e5f00a27":"# Let\u2019s look at the monthly mean of each year separately.\n\ntemp=candies.groupby(['year','month'])['IPG3113N'].mean()\ntemp.plot(figsize=(15,5), title= 'production Count(Monthwise)', fontsize=14)","e2a217b3":"train=candies.ix[:'2011-10-01']\nvalid=candies.ix['2011-11-01':]","89c7f732":"train.head()","5331dfd7":"train.IPG3113N.plot(figsize=(15,8), title= 'Candy Production', fontsize=14, label='train')\nvalid.IPG3113N.plot(figsize=(15,8), title= 'Candy Production', fontsize=14, label='valid')\nplt.xlabel(\"observation_date\")\nplt.ylabel(\"production count\")\nplt.legend(loc='best')\nplt.show()","5b460e4a":"# predictions using naive approach for the validation set.\ndd= np.asarray(train['IPG3113N'])\ny_hat = valid.copy()\ny_hat['naive'] = dd[len(dd)-1]\nplt.figure(figsize=(12,8))\nplt.plot(train.index, train['IPG3113N'], label='Train')\nplt.plot(valid.index,valid['IPG3113N'], label='Valid')\nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast')\nplt.legend(loc='best')\nplt.title(\"Naive Forecast\")\nplt.show()","f294ca99":"# RMSE(Root Mean Square Error) to check the accuracy of our model on validation data set.\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(valid['IPG3113N'], y_hat.naive))\nprint(rms)","7b43778b":"# last 5 observations.\ny_hat_avg = valid.copy()\ny_hat_avg['moving_avg_forecast'] = train['IPG3113N'].rolling(5).mean().iloc[-1] # average of last 5 observations.\nplt.figure(figsize=(15,5)) \nplt.plot(train['IPG3113N'], label='Train')\nplt.plot(valid['IPG3113N'], label='Valid')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 5 observations')\nplt.legend(loc='best')\nplt.show()","55d48442":"# last 7 observations.\ny_hat_avg = valid.copy()\ny_hat_avg['moving_avg_forecast'] = train['IPG3113N'].rolling(7).mean().iloc[-1] # average of last 7 observations.\nplt.figure(figsize=(15,5)) \nplt.plot(train['IPG3113N'], label='Train')\nplt.plot(valid['IPG3113N'], label='Valid')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 7 observations')\nplt.legend(loc='best')\nplt.show()","d938a6fd":"# last 50 observations.\ny_hat_avg = valid.copy()\ny_hat_avg['moving_avg_forecast'] = train['IPG3113N'].rolling(50).mean().iloc[-1] # average of last 50 observations.\nplt.figure(figsize=(15,5)) \nplt.plot(train['IPG3113N'], label='Train')\nplt.plot(valid['IPG3113N'], label='Valid')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 50 observations')\nplt.legend(loc='best')\nplt.show()","e9cb6bdc":"rms = sqrt(mean_squared_error(valid['IPG3113N'], y_hat_avg.moving_avg_forecast))\nprint(rms)","8e7ed3b6":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\ny_hat_ex = valid.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train['IPG3113N'])).fit(smoothing_level=0.6,optimized=False)\ny_hat_ex['SES'] = fit2.forecast(len(valid))\nplt.figure(figsize=(16,8))\nplt.plot(train['IPG3113N'], label='Train')\nplt.plot(valid['IPG3113N'], label='Valid')\nplt.plot(y_hat_ex['SES'], label='SES')\nplt.legend(loc='best')\nplt.show()","f2e266be":"rms = sqrt(mean_squared_error(valid['IPG3113N'], y_hat_ex['SES']))\nprint(rms)","4828334a":"import statsmodels.api as sm\nsm.tsa.seasonal_decompose(train['IPG3113N']).plot()\nresult = sm.tsa.stattools.adfuller(train['IPG3113N'])\nplt.show()","a487a319":"y_hat_ex = valid.copy()\n\nfit1 = Holt(np.asarray(train['IPG3113N'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_ex['Holt_linear'] = fit1.forecast(len(valid))\n\nplt.figure(figsize=(16,8))\nplt.plot(train['IPG3113N'], label='Train')\nplt.plot(valid['IPG3113N'], label='Valid')\nplt.plot(y_hat_ex['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","3c6a9ea0":"rms = sqrt(mean_squared_error(valid['IPG3113N'], y_hat_ex.Holt_linear))\nprint(rms)","12285f82":"y_hat_win = valid.copy()\nfit1 = ExponentialSmoothing(np.asarray(train['IPG3113N']) ,seasonal_periods=25 ,trend='add', seasonal='add',).fit()\ny_hat_win['Holt_Winter'] = fit1.forecast(len(valid))\nplt.figure(figsize=(16,8))\nplt.plot( train['IPG3113N'], label='Train')\nplt.plot(valid['IPG3113N'], label='Valid')\nplt.plot(y_hat_win['Holt_Winter'], label='Holt_Winter')\nplt.legend(loc='best')\nplt.show()","86f6acdb":"rms = sqrt(mean_squared_error(valid['IPG3113N'], y_hat_win.Holt_Winter))\nprint(rms)","f5fa0c2d":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(24).mean()\n    rolstd = timeseries.rolling(24).std()\n    \n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","3619c87b":"plt.figure(figsize=(16,8))\ntest_stationarity(candies_original['IPG3113N'])","fac204f0":"train_log = np.log(train['IPG3113N'])\nvalid_log = np.log(valid['IPG3113N'])\n\nmoving_avg = train_log.rolling(24).mean()\nplt.figure(figsize=(16,8))\nplt.plot(train_log)\nplt.plot(moving_avg, color = 'red')\nplt.show()","0d137e6c":"train_log_moving_avg_diff = train_log - moving_avg","cbdf71c4":"train_log_moving_avg_diff.dropna(inplace = True)\nplt.figure(figsize=(16,8))\ntest_stationarity(train_log_moving_avg_diff)","90633cde":"train_log_diff = train_log - train_log.shift(1)\nplt.figure(figsize=(16,8))\ntest_stationarity(train_log_diff.dropna())","9e767443":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(pd.DataFrame(train_log).IPG3113N.values, freq = 24)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.figure(figsize=(16,8))\nplt.subplot(411)\nplt.plot(train_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","742db177":"# Let\u2019s check stationarity of residuals.\n\ntrain_log_decompose = pd.DataFrame(residual)\ntrain_log_decompose['date'] = train_log.index\ntrain_log_decompose.set_index('date', inplace = True)\ntrain_log_decompose.dropna(inplace=True)\nplt.figure(figsize=(16,8))\ntest_stationarity(train_log_decompose[0])","3e427d10":"from statsmodels.tsa.stattools import acf, pacf\nlag_acf = acf(train_log_diff.dropna(), nlags=25)\nlag_pacf = pacf(train_log_diff.dropna(), nlags=25, method='ols')","e6463f41":"# ACF plot\nplt.figure(figsize=(16,8))\nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\nplt.show()","6b5752be":"# PACF plot\nplt.figure(figsize=(16,8))\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.show()","b78290a5":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel = ARIMA(train_log, order=(2, 1, 0))  # here the q value is zero since it is just the AR model\nresults_AR = model.fit(disp=-1)  \nplt.figure(figsize=(16,8))\nplt.plot(train_log_diff.dropna(), label='original')\nplt.plot(results_AR.fittedvalues, color='red', label='predictions')\nplt.legend(loc='best')\nplt.show()","15a7855a":"# First step would be to store the predicted results as a separate series and observe it.\nAR_predict=results_AR.predict(start=\"2011-11-01\", end=\"2017-08-01\")\nAR_predict=AR_predict.cumsum().shift().fillna(0)\nAR_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['IPG3113N'])[0], index = valid.index)\nAR_predict1=AR_predict1.add(AR_predict,fill_value=0)\nAR_predict = np.exp(AR_predict1)\n\nplt.figure(figsize=(16,8))\nplt.plot(valid['IPG3113N'], label = \"Valid\")\nplt.plot(AR_predict, color = 'red', label = \"Predict\")\nplt.legend(loc= 'best')\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(AR_predict, valid['IPG3113N']))\/valid.shape[0]))\nplt.show()","87fab038":"\nmodel = ARIMA(train_log, order=(0, 1, 2))  # here the p value is zero since it is just the MA model\nresults_MA = model.fit(disp=-1)\nplt.figure(figsize=(16,8))\nplt.plot(train_log_diff.dropna(), label='original')\nplt.plot(results_MA.fittedvalues, color='red', label='prediction')\nplt.legend(loc='best')\nplt.show()","207b06f0":"MA_predict=results_MA.predict(start=\"2011-11-01\", end=\"2017-08-01\")\nMA_predict=MA_predict.cumsum().shift().fillna(0)\nMA_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['IPG3113N'])[0], index = valid.index)\nMA_predict1=MA_predict1.add(MA_predict,fill_value=0)\nMA_predict = np.exp(MA_predict1)\n\nplt.figure(figsize=(16,8))\nplt.plot(valid['IPG3113N'], label = \"Valid\")\nplt.plot(MA_predict, color = 'red', label = \"Predict\")\nplt.legend(loc= 'best')\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, valid['IPG3113N']))\/valid.shape[0]))\nplt.show()","16b808d0":"model = ARIMA(train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \n\nplt.figure(figsize=(16,8))\nplt.plot(train_log_diff.dropna(),  label='original')\nplt.plot(results_ARIMA.fittedvalues, color='red', label='predicted')\nplt.legend(loc='best')\nplt.show()","0cb65653":"# Let\u2019s define a function which can be used to change the scale of the model to the original scale.\n\ndef check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['IPG3113N'])[0], index = given_set.index)\n    predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_log)\n    \n    plt.figure(figsize=(16,8))\n    plt.plot(given_set['IPG3113N'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['IPG3113N']))\/given_set.shape[0]))\n    plt.show()","201c57f7":"def check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n    \n    plt.figure(figsize=(16,8))\n    plt.plot(given_set['IPG3113N'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['IPG3113N']))\/given_set.shape[0]))\n    plt.show()","e4bc931b":"ARIMA_predict_diff=results_ARIMA.predict(start=\"2011-11-01\", end=\"2017-08-01\")\n\ncheck_prediction_diff(ARIMA_predict_diff, valid)","18c2c7f0":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\ny_hat_avg = valid.copy()\nfit1 = SARIMAX(train['IPG3113N'], order=(2, 1, 4),seasonal_order=(0,1,1,7),enforce_stationarity=False,enforce_invertibility=False).fit()\ny_hat_ex['SARIMA'] = fit1.predict(start=\"2011-11-01\", end=\"2017-08-01\", dynamic=True)\nplt.figure(figsize=(16,8))\nplt.plot( train['IPG3113N'], label='Train')\nplt.plot(valid['IPG3113N'], label='Valid')\nplt.plot(y_hat_ex['SARIMA'], label='SARIMA')\nplt.legend(loc='best')\nplt.show()","8c7afd30":"# Let\u2019s check the rmse value for the validation part.\n\nrms = sqrt(mean_squared_error(valid['IPG3113N'], y_hat_ex.SARIMA))\nprint(rms)","ebc284b3":"Let\u2019s make a copy of data so that even if we do changes in these dataset we do not lose the original dataset.","39af6a02":"### AR model\n\nThe autoregressive model specifies that the output variable depends linearly on its own previous values.\n","e904dc05":"Let\u2019s look at the data types of each feature.","059e12c7":"### Splitting the data into training and validation part","8854e535":"## SARIMAX\n\nSARIMAX model takes into account the seasonality of the time series. So we will build a SARIMAX model on the time series.\n","3bae504e":"## 6.  ARIMA\nAs our time series is non stationary, we have to apply differencing to reduce possibles trend and seasonality.","311d03ef":"### Naive","b6cf8cb5":"### MA model\n\nThe moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term.\n","95d02d72":"## Table of Contents:\n1.  Understanding Time Series\n2. Import libraries and  loading the data\n3. Dataset Structure and Content\n4.  Feature Extraction\n5. EDA - Plots (trend? seasonality?)\n6. Simple models: Average, Naive\n7. Exponential models:Holt,  Holt Winters\n8.  ARIMA\n9. SARIMAX","ab498db7":"### Removing Seasonality","f563e62a":"### Holt\u2019s Winter method","f57012d0":"### Components of a Time Series\n**1. Trend :** Trend is a general direction in which something is developing or changing.\n\n**2. Seasonality :** The pattern is repeating at regular time interval which is known as the seasonality. Any predictable change or pattern in a time series that recurs or repeats over a specific time period can be said to be seasonality.\n","4a64587f":" ### Simple Exponential Smoothing","2d83add6":"#### Considering validate set,  the Moving Average method showed better performance at some important metrics like RMSE and MAPE","04c843eb":"Let\u2019s predict the values for validation set.","dc9ab69a":"## 4.  Feature Extraction","33e60a4e":"## 6. Simple models: Naive,Moving Average","8102b947":"## 5. EDA - Plots (trend? seasonality?)","7061eb9f":"Forecasting the time series using ARIMA\n\n* First of all we will fit the ARIMA model on our time series for that we have to find the optimized values for the p,d,q parameters.\n\n* To find the optimized values of these parameters, we will use ACF(Autocorrelation Function) and PACF(Partial Autocorrelation Function) graph.\n\n* ACF is a measure of the correlation between the TimeSeries with a lagged version of itself.\n\n* PACF measures the correlation between the TimeSeries with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons.\n","c21e5a8b":"## 3. Dataset Structure and Content","e58ebcd6":"### Moving Average","71c3269f":"## 2. Import libraries and  loading the data","90db469b":"See the shape of the dataset.","7ace5178":"## 1. Introduction to Time Series\n<b style=\"color:green\">Time Series is generally data which is collected over time and is dependent on it.<\/b>\n\n**Formal definition of Time Series:**\n\nA series of data points collected in time order is known as a time series. Most of business houses work on time series data to analyze sales number for the next year, website traffic, count of traffic, number of calls received, etc. Data of a time series can be used for forecasting.\n\n<b style=\"color:red\">Not every data collected with respect to time represents a time series.<\/b>\n\nSome of the examples of time series are:\n* Stock Price :\n* Passenger Count of an airlines :\n* Temperature over time :\n* Number of visitors in a hotel","62bd4de1":"## 7. Exponential models: Ses, Holt,  Holt Winters","ced907e7":"### Combined model","eef4005c":"let\u2019s have a look at the features in the dataset.","606c4999":" ### Holt\u2019s Linear Trend Model"}}