{"cell_type":{"d0533560":"code","138ca3dd":"code","7110b96c":"code","0746c7f5":"code","cd5106bf":"code","ff8c11ab":"code","bdf18be1":"code","52548e27":"code","1f7e555c":"code","7b10e593":"code","c93cf0a1":"code","a778fefa":"code","068ef4c9":"code","23b592ea":"code","f6db8ff3":"code","20ba314d":"code","39d67c86":"code","d4f850de":"code","8a2599b9":"markdown","671e18e3":"markdown","57593e21":"markdown","fdf57aa2":"markdown","36775682":"markdown","75d6121f":"markdown","d196128e":"markdown","19a0151a":"markdown","8f9bab93":"markdown","17d1b046":"markdown","d6c56152":"markdown","2383c80e":"markdown","9d3c2d80":"markdown","cdc34a46":"markdown","38ad6bd7":"markdown"},"source":{"d0533560":"import collections\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom transformers import TFXLMRobertaModel,AutoTokenizer","138ca3dd":"base_model = \"..\/input\/jplu-tf-xlm-roberta-base-2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model, add_special_tokens=True)","7110b96c":"# Not running across all folds in this data. Just using a really convoluted way to split the data\ndf = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")\nfolds = 4\ndf[\"kfold\"] = -1\n\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=df.language.values)):\n    df.loc[v_, 'kfold'] = f\n    \nfold_val = 3\ntrain = df[df['kfold']!=fold_val]\nvalid = df[df['kfold']==fold_val]\n\n# Coverting the dataframe to list of dictionaries.\n# NOT IDEAL. But this was the best way to get the data closer to the \n# Keras example. Please let me know if I can do it in a better way.\ntrain_records = train.to_dict('records')\nvalid_records = valid.to_dict('records')\n\nprint (f'Totally there are {len(train_records)} training records and {len(valid_records)} validation records.')","0746c7f5":"max_length = 384\ndoc_stride = 128","cd5106bf":"class ChaiiExample:\n    def __init__(self, question, context, start_char_idx=0, answer_text='', id=''):\n        self.question = question\n        self.context = context\n        self.start_char_idx = start_char_idx\n        self.answer_text = answer_text\n        self.id = id\n\n        # The tokenizer returns us a list of features from a single long doc. \n        # This is enabled by setting the return_overflowing_tokens=True \n\n        # It is required to find put in which features, the answer actually is\n        # present. The answer requires start and end positions, but we have\n        # split the document! \n        # To map the relative answer positions with the original,\n        # we need 'offsets' , obtained by setting return_offsets_mapping=True\n\n        # Padded to max length\n        self.tokenized_sample = tokenizer(\n            question, context, max_length=max_length, stride=doc_stride,\n            truncation=\"only_second\",\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\"\n        )\n    \n \n    def prepare_train_sample(self):\n        # Retrive the features\n        sample_mapping = self.tokenized_sample.pop(\"overflow_to_sample_mapping\")\n        # Retrive the offsets\n        offset_mapping = self.tokenized_sample.pop(\"offset_mapping\")\n\n        # Obtaining start and end positions of the answer\n        self.tokenized_sample[\"start_positions\"] = []\n        self.tokenized_sample[\"end_positions\"] = []\n\n        for i, offsets in enumerate(offset_mapping):\n            input_ids = self.tokenized_sample[\"input_ids\"][i]\n            # Assume that the answer is not present here. So, tokenize\n            # label these impossible answers with the CLS token\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n\n            # Grab the sequence corresponding to that sample. \n            # Sequence id's help to know what is the context and \n            # what is the question (basically sets 0 to question \n            # and 1 to context - based on the order we used when \n            # calling the tokenizer)\n            sequence_ids = self.tokenized_sample.sequence_ids(i)\n\n            # Start\/end character index of the answer in the text.\n            answer_start = self.start_char_idx\n            answer_end = answer_start + len(self.answer_text)\n\n            # Start token index of the current span in the context.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the context.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Check the position of the answer in the context\n            # 1. If answer is out of span, then mark the feature with CLS index\n            if not (offsets[token_start_index][0] <= answer_start and\\\n                    offsets[token_end_index][1] >= answer_end):\n                self.tokenized_sample[\"start_positions\"].append(cls_index)\n                self.tokenized_sample[\"end_positions\"].append(cls_index)\n            # 2. Else move the token_start_index and token_end_index to the two \n            #    ends of the answer.\n            else:\n                while token_start_index < len(offsets) and\\\n                      offsets[token_start_index][0] <= answer_start:\n                    token_start_index += 1\n                self.tokenized_sample[\"start_positions\"].append(token_start_index - 1)\n                \n                while offsets[token_end_index][1] >= answer_end:\n                    token_end_index -= 1\n                self.tokenized_sample[\"end_positions\"].append(token_end_index + 1)\n\n            # Note that there maybe an edge case where the answer could go after \n            # the last offset if the answer is the last word.\n\n        self.split_the_dicts()\n\n    def split_the_dicts(self):\n        # Splits the list of dictionary values into separate dictionaries\n        if isinstance(self.tokenized_sample['input_ids'][0],list):\n            keys = self.tokenized_sample.keys()\n            vals = zip(*[self.tokenized_sample[k] for k in keys])\n            self.tokenized_sample = [dict(zip(keys, v)) for v in vals]\n\n\n    def prepare_validation_sample(self):\n        # Retrive the features\n        sample_mapping = self.tokenized_sample.pop(\"overflow_to_sample_mapping\")\n\n        # Keep the example_id that gave us this feature.\n        # We will aslo store the offset mappings.\n        self.tokenized_sample[\"id\"] = []\n\n        for i in range(len(self.tokenized_sample[\"input_ids\"])):\n            # Grab the sequence corresponding to that example\n            sequence_ids = self.tokenized_sample.sequence_ids(i)\n            context_index = 1 \n\n            self.tokenized_sample[\"id\"].append(self.id)\n\n            # Set to None the offset_mapping that are not part of the context \n            # so it's easy to determine if a token\n            # position is part of the context or not.\n            self.tokenized_sample[\"offset_mapping\"][i] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(self.tokenized_sample[\"offset_mapping\"][i])\n            ]\n\n        self.split_the_dicts()\n","ff8c11ab":"def create_chaii_examples(raw_data, train_flag=True):\n    chaii_examples = []\n    for data in raw_data:\n        if train_flag:\n            chaii = ChaiiExample(data['question'],data['context'],\\\n                                 data['answer_start'],data['answer_text'])\n            chaii.prepare_train_sample()\n        else:\n            # Keeping track of id for validation post processing\n            chaii = ChaiiExample(data['question'],data['context'],id=data['id'])\n            chaii.prepare_validation_sample()\n\n        # Since we are creating multiple features from the same document,\n        # it is essential to take care while adding the record for training\n        if isinstance(chaii.tokenized_sample, dict):\n            chaii_examples.append(chaii.tokenized_sample)\n        else:\n            chaii_examples.extend(chaii.tokenized_sample)\n\n    return chaii_examples","bdf18be1":"def create_inputs_targets(chaii_examples):\n    # Creating the training data dictionary\n    dataset_dict = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"start_positions\": [],\n        \"end_positions\": [],\n    }\n    \n    for item in chaii_examples:\n        for key in dataset_dict:\n            dataset_dict[key].append(item[key])\n\n    # Converting all to numpy array\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_positions\"], dataset_dict[\"end_positions\"]]\n    return x, y","52548e27":"def create_eval_inputs(chaii_examples):\n    # The evaluation consists of only the inputs\n    # The targets are directly taken from the validation records\n    dataset_dict = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"id\": [],\n        \"offset_mapping\": [],\n    }\n    \n    for item in chaii_examples:\n        for key in dataset_dict:\n            dataset_dict[key].append(item[key])\n\n    x = [\n        np.array(dataset_dict[\"input_ids\"]),\n        np.array(dataset_dict[\"attention_mask\"]),\n    ]\n    # No y is returned here\n    return x","1f7e555c":"train_chaii_examples = create_chaii_examples(train_records)\nx_train, y_train = create_inputs_targets(train_chaii_examples)\nprint(f\"{len(train_chaii_examples)} training points created.\")\n\neval_chaii_examples = create_chaii_examples(valid_records, train_flag=False)\nx_eval = create_eval_inputs(eval_chaii_examples)\nprint(f\"{len(eval_chaii_examples)} evaluation points created.\")","7b10e593":"def create_model():\n    ## This code is directly from the Keras example ##\n    encoder = TFXLMRobertaModel.from_pretrained(base_model)\n    input_ids = layers.Input(shape=(max_length,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_length,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model","c93cf0a1":"model = create_model()","a778fefa":"model.summary()","068ef4c9":"def post_process_results(all_pred_start, all_pred_end, original_records, hf_features, n_best_size = 20, max_answer_length = 30):\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k['id']: i for i, k in enumerate(original_records)}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(hf_features):\n        features_per_example[example_id_to_index[feature[\"id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n    min_null_score = None\n\n    for idx, example in enumerate(original_records):\n        feature_indices = features_per_example[idx]\n\n        valid_answers = []\n        context = example['context']\n\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # Get the predictions of the model for this feature\n            pred_start = all_pred_start[feature_index]\n            pred_end = all_pred_end[feature_index]\n\n            # Offset to map the position of predicted span with the span of the \n            # answer in the context\n            offset_mapping = hf_features[feature_index][\"offset_mapping\"]\n\n            # Update minimum null prediction.\n            cls_index = hf_features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = pred_start[cls_index] + pred_end[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n\n            # Go through all possibilities for the `n_best_size` greater start and end.\n            start_indexes = np.argsort(pred_start)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(pred_end)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": pred_start[start_index] + pred_end[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # The final answer\n        predictions[example[\"id\"]] = best_answer[\"text\"]\n\n    return predictions\n            ","23b592ea":"class JaccardScore(keras.callbacks.Callback):\n    def __init__(self, x_eval):\n        self.x_eval = x_eval\n        \n    def safe_div(self, x,y):\n        if y == 0:\n            return 1\n        return x \/ y\n\n    def jaccard(self, str1, str2): \n        a = set(str1.split()) \n        b = set(str2.split())\n        c = a.intersection(b)\n        return self.safe_div(float(len(c)) , (len(a) + len(b) - len(c)))\n\n    def get_jaccard_score(self, y_true,y_pred):\n        assert len(y_true)==len(y_pred)\n        score=0.0\n        for i in range(len(y_true)):\n            score += self.jaccard(y_true[i], y_pred[i])\n            \n        return score\n\n    def on_epoch_end(self, epoch, logs=None):\n        y_pred = []\n        y_true = []\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        # The post processing function to select only the best output\n        predictions = post_process_results(pred_start, pred_end, \\\n                                           valid_records, eval_chaii_examples)\n        for idx, record in enumerate(valid_records):\n            y_true.append(record['answer_text'])\n            y_pred.append(predictions[record['id']])\n\n        score = self.get_jaccard_score(y_true,y_pred)\n        epoch_jaccard = score\/len(predictions)\n\n        print(f\"epoch={epoch+1}, jaccard score={epoch_jaccard:.2f}\")","f6db8ff3":"epochs = 1 # training only for one epoch to quickly check the flo\nbatch_size = 4\n\njaccard_score_callback = JaccardScore(x_eval)\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=epochs,\n    verbose=2,\n    batch_size=batch_size,\n    callbacks=[jaccard_score_callback],\n)","20ba314d":"test = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\ntest_records = test.to_dict('records')\n\ntest_chaii_examples = create_chaii_examples(test_records, train_flag=False)\nx_test = create_eval_inputs(test_chaii_examples)\nprint(f\"{len(test_chaii_examples)} evaluation points created.\")","39d67c86":"pred_start, pred_end = model.predict(x_test)\npredictions = post_process_results(pred_start, pred_end, test_records, test_chaii_examples)","d4f850de":"submit_df = pd.DataFrame({'id': list(predictions.keys()), 'PredictionString': list(predictions.values())})\nsubmit_df.to_csv('submission.csv', index=False)\nsubmit_df.head()","8a2599b9":"### **Output post-processing** \n\n* The output is one logit for each feature and each token. One can take `argmax` *(index of the max element)* to filter the result. \n* But, in order to avoid cases where the *start_index* is lesser than *end_index*, it is advised to use score obtained by **adding** the start and end logits.\n* Also to avoid checking over all the predictions, only the top-k predictions will be picked. This can be controlled using a parameter. In this notebook, it is called `n_best_size`.  \n* Also, it is imperative to create a mapping between the validation input and predictions to enable this processing. Here is where the `id` stored in the validation comes in handy.","671e18e3":"### **Loading the data**","57593e21":"## Closing notes\n\n- Training takes a veeeerrrry long time.\ud83d\ude41\n- Comments on various things that can make this notebook better are always welcome. One is surely getting rid of record conversion.\n- Despite a bad score, I think I am better equipped on 4 things I set out to learn *(not sure of the last one though!)*. \n\n","fdf57aa2":"## **Setup**","36775682":"**Dealing with very long documents.** \n- Usually, in other NLP tasks, the long documents are truncated, when they are longer than the model maximum sentence length.\n- However, in Question-Answering, removing part of the the *context* might result in losing the answer we are looking for. \n- To deal with this, we will allow one (long) example in our dataset to give several input features, each of length shorter than the maximum length of the model *(in this notebook, set as the `max_length` hyper-parameter)*. \n- Also, just in case the answer lies at the point we split a long context, we allow some overlap between the features we generate controlled by the hyper-parameter `doc_stride`.\n- We do allow for truncation BUT only the context.","75d6121f":"## **Submit** \n\nNot a very good score. In fact, not at all upto the baseline at all!\ud83d\ude22","d196128e":" ## **Dealing with Data**\n \n Basic things like splitting the data into folds and pre-processing is performed here","19a0151a":"### **Creating the model** \nCreate the Question-Answering Model using HF and Keras Functional API","8f9bab93":"### **Preprocessing the data** \n\n\nTwo steps happen in this section -\n\n1.   Go through each record and create `ChaiiExample` object from each.\n2. Include the HF method to deal with long documents\n\n","17d1b046":"### **Create the Jaccard evaluation callback**\n\nWidely used in many NLP tasks. Think of it as the metric that evaluates how close you are to the exact answer. The higher it is, the better fares the model.\n\n**Jaccard similarity coefficient score.**\n\nDefinition - \n\n> The Jaccard index or Jaccard similarity coefficient,is the size of the intersection divided by the size of the union of two label sets. It is used to compare set of predicted labels for a sample to the corresponding set of labels in true labels.\n\n\n","d6c56152":"Helper functions to create the training and validation data from the `ChaiiExample` class.","2383c80e":"### **Notebook Details**\n\nMostly an amalgamation of two excellent resouces - \n\n- The [Keras SQuAD](https:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/) example\n- The [HuggingFace SQuAD](https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb) walkthrough\n\nAlso, I really liked the notebook series from [Juli\u00e1n Peller](https:\/\/www.kaggle.com\/julian3833\/1-the-competition-qa-for-qa-noobs) - used to understand the whole rigmarole of QA in NLP and Shahules's [kernel](https:\/\/www.kaggle.com\/shahules\/chaii-custom-qa-training-pytorch) which had PyTorch implementation with HF additions. \n\nOn the EDA side I really liked the word clouds from [Shivam Ralli](https:\/\/www.kaggle.com\/hoshi7\/chaii-interactive-wordclouds) and [AK Nain](https:\/\/www.kaggle.com\/aakashnain\/chaii-explore-the-data).\n\nThat being said, all the notebooks are awesome!\ud83d\ude42","9d3c2d80":"## **Model Creation, Post-processing and Evaluation**","cdc34a46":"## **Train and Evaluate**\n\nFinally, never thought I will get here!","38ad6bd7":"## **Introduction**\n\nHindi and Tamil are most widely spoken languages in India. Even if not going by the data **(*sacrilageous remark on Kaggle*)**  I have not personally enountered any one in Bangalore *(Karnataka, India)* who does not speak either of the two languages.\n\n\nSince the spread of these langauges is far and wide, it is very practical that a competition with Hindi and Tamil at the helm is introduced. Looking forward to learning a lot!\n\n\nThis notebook is an earnest attempt at learning 4 things that are very new to me -\n\n*   Natural Language Processing\n*   Keras\n*   HuggingFace\n*   Tamil and Hindi *(I can croon some melodies in both but god help the one who wants to talk with me using these languages)*"}}