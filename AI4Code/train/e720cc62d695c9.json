{"cell_type":{"6088437c":"code","c1ebff76":"code","3b66b513":"code","3b8be251":"code","e0ded478":"code","395cfea6":"code","db3f5428":"code","ccd5b0f1":"code","d07956d3":"code","86e21115":"code","c7091f69":"code","7043e489":"code","d08f18e4":"code","82447177":"code","4da39525":"code","427cc68c":"code","171f20e4":"code","b72b9dad":"code","bacb5ae0":"code","9ac32907":"code","75697865":"code","3ad2e554":"code","889beaff":"code","e35be501":"code","ff9f3658":"code","682138d7":"code","0988afe6":"code","9d4a2c5b":"code","e4eac364":"code","041cd4d6":"code","f50d791f":"code","368820f7":"code","25bfa938":"code","318008a7":"code","ea8d1b54":"code","bd46fec2":"code","56dd0c02":"code","20555459":"code","1ace2852":"code","690fa904":"code","078b03d1":"code","5622072b":"code","f485e0b5":"code","c7e2c258":"code","b6d9792a":"code","5893a4bb":"code","ca1942bb":"code","f4a8e6fe":"code","9cc1f693":"code","f61dca75":"code","6a0327ee":"code","4a0426ef":"code","73c7c6fe":"code","3dc99edf":"code","1592a0f3":"code","3cab8cfb":"code","1686898a":"markdown","fe020d51":"markdown","a55b4cdd":"markdown","4b98b561":"markdown","b069e1e3":"markdown","7be51931":"markdown","db45c94c":"markdown","9821b53a":"markdown","57e732ec":"markdown","f9ff4c0c":"markdown","b4d685eb":"markdown","279db0d1":"markdown","7abc76f1":"markdown","bb1336f0":"markdown","695703c6":"markdown","a2d69b2e":"markdown","12e34ae0":"markdown","bde4046c":"markdown","06783771":"markdown","13f02ee3":"markdown","05707a27":"markdown","01eee6bf":"markdown","26b69ade":"markdown","5595cfa4":"markdown","bfd46456":"markdown","6f0d0498":"markdown","190a141e":"markdown","d70ea429":"markdown","01992eee":"markdown","9c6c1411":"markdown","a08981b5":"markdown","1ea0aa8e":"markdown","61b26fe0":"markdown","ece64949":"markdown","41c0abb8":"markdown","3ab4b5de":"markdown","9115e740":"markdown","d31d999a":"markdown","e6abdc2f":"markdown","5f66df35":"markdown","7f3f8f28":"markdown","6293fbb1":"markdown","e193d064":"markdown","ff029aa2":"markdown","71329ed5":"markdown","86777ab7":"markdown","52ce568d":"markdown","8884da92":"markdown","ba97d6e6":"markdown","b32d3a42":"markdown","ed1e1c56":"markdown"},"source":{"6088437c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1ebff76":"# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3b66b513":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ny_sample =  pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","3b8be251":"train_data.head(10)","e0ded478":"train_data.info()","395cfea6":"import seaborn as sns\nsns.countplot(x = \"Survived\",data = train_data)","db3f5428":"train_data.groupby('Survived').mean()","ccd5b0f1":"train_data.describe()","d07956d3":"train_data.groupby(train_data['Age'].isnull()).mean()","86e21115":"age_survived = train_data[train_data['Survived'] == 1]['Age'].dropna()\nage_died = train_data[train_data['Survived'] == 0]['Age'].dropna()\nsns.distplot(age_survived, kde = False, bins = 40)\nsns.distplot(age_died, kde = False, bins = 40)\nplt.legend(['Did not survive', 'Survived'])\nplt.title('Overlaid histogram for Age')\nplt.show()","c7091f69":"fare_survived = train_data[train_data['Survived'] == 1]['Fare'].dropna()\nfare_died = train_data[train_data['Survived'] == 0]['Fare'].dropna()\nsns.distplot(fare_survived, kde = False, bins = 20)\nsns.distplot(fare_died, kde = False, bins = 20)\nplt.legend(['Did not survive', 'Survived'])\nplt.title('Overlaid histogram for Fare')\nplt.show()","7043e489":"sns.heatmap(train_data[['Fare', 'Pclass']].corr(),annot = True)","d08f18e4":"train_data.groupby('Pclass').mean()","82447177":"sns.catplot(x='Pclass', y='Survived', data=train_data, kind='point', aspect=2)","4da39525":"sns.catplot(x='SibSp', y='Survived', data=train_data, kind='point', aspect=2)","427cc68c":"sns.catplot(x='Parch', y='Survived', data=train_data, kind='point', aspect=2)","171f20e4":"import seaborn as sns\nsns.boxplot(x = 'Parch', y = 'Age', data = train_data, palette = 'hls')","b72b9dad":"train_data.groupby('Parch').mean()","bacb5ae0":"def age_aprox(cols):\n    age = cols[0]\n    parch = cols[1]\n    \n    if pd.isnull(age):\n        if parch == 0:\n            return train_data[train_data['Parch'] == 0]['Age'].mean()\n        elif parch == 1:\n            return train_data[train_data['Parch'] == 1]['Age'].mean()\n        elif parch == 2:\n            return train_data[train_data['Parch'] == 2]['Age'].mean()\n        elif parch == 3:\n            return  train_data[train_data['Parch'] == 3]['Age'].mean()\n        elif parch == 4:\n            return train_data[train_data['Parch'] == 4]['Age'].mean()\n        else:\n            return train_data['Age'].mean()\n    else:\n        return age","9ac32907":"train_data['Age'] = train_data[['Age','Parch']].apply(age_aprox, axis = 1)\ntest_data['Age'] = test_data[['Age','Parch']].apply(age_aprox, axis = 1)","75697865":"train_data['Family_count'] = train_data['SibSp'] +train_data['Parch']\n#train_data.drop(['SibSp','Parch'], inplace = True,axis  = 1)\n","3ad2e554":"test_data['Family_count'] = test_data['SibSp'] +test_data['Parch']\n#test_data.drop(['SibSp','Parch'], inplace = True,axis  = 1)","889beaff":"round((train_data['Cabin'].isnull().sum()\/len(train_data))*100,2)","e35be501":"sns.catplot(x='Sex', y='Survived', data=train_data, kind='point', aspect=2)","ff9f3658":"sns.catplot(x='Embarked', y='Survived', data=train_data, kind='point', aspect=2)","682138d7":"train_data.pivot_table('Survived',index = 'Sex',columns = 'Embarked',aggfunc= 'count')","0988afe6":"train_data['Name'].head(10)","9d4a2c5b":"# Apply regex per name\n# Use function : Series.str.extract()\nfor name in train_data['Name']:\n    train_data['Title'] = train_data['Name'].str.extract('([A-Za-z]+)\\.',expand=True)    # Regex to get title : ([A-Za-z]+)\\.\n    test_data['Title'] = test_data['Name'].str.extract('([A-Za-z]+)\\.',expand=True) ","e4eac364":"train_data.groupby('Title').count()['PassengerId']","041cd4d6":"test_data.groupby('Title').count()['PassengerId']","f50d791f":"gender = {'male':1,'female': 0}\ntrain_data['Sex'] = train_data['Sex'].map(gender)\ntest_data['Sex'] = test_data['Sex'].map(gender)","368820f7":"train_data.head(5)","25bfa938":"train_data['Embarked'].describe()","318008a7":"train_data['Embarked'].fillna('S', inplace = True)\ntest_data['Embarked'].fillna('S', inplace = True)","ea8d1b54":"title = {'Capt':'Others','Col':'Others','Countess':'Others','Don':'Others', 'Dr':'Others','Jonkheer':'Others', 'Lady':'Others', 'Major':'Others',\n        'Mlle':'Others', 'Mme':'Others', 'Ms':'Miss','Rev': 'Others','Sir':'Others','Dona': 'Others'}","bd46fec2":"train_data.replace({'Title':title},inplace=True)\ntest_data.replace({'Title':title},inplace=True)","56dd0c02":"train_data.groupby('Title').count()['PassengerId']","20555459":"features = [\"Pclass\", \"Sex\", \"SibSp\",\"Parch\",\"Age\",\"Embarked\"]\ny = train_data[\"Survived\"]\nX = pd.get_dummies(train_data[features])\ntitanic_test = pd.get_dummies(test_data[features])\nprint(X.shape)\nprint(y.shape)\n#print(X_test.shape)","1ace2852":"sns.heatmap(X.corr(),annot = True)","690fa904":"from sklearn.model_selection import train_test_split, cross_val_score","078b03d1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)","5622072b":"for dataset in [y_train,y_val,y_test]:\n    print(round(len(dataset)\/len(y),2))","f485e0b5":"from sklearn.model_selection import  cross_val_score","c7e2c258":"from sklearn.ensemble import RandomForestClassifier\nrf_basic = RandomForestClassifier()\nscores  = cross_val_score(rf_basic,X_train,y_train, cv = 5)\nscores","b6d9792a":"from pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf_basic.get_params())","5893a4bb":"from sklearn.model_selection import GridSearchCV","ca1942bb":"def print_results(results):\n    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+\/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))","f4a8e6fe":"rf_grid = RandomForestClassifier()\nparameters = {\n    'n_estimators': [5, 50, 100, 200],\n    'max_depth': [2, 10, 20, None],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [1, 2, 4],\n    'min_samples_split': [2, 5, 10],\n}\n\ncv = GridSearchCV(rf_grid, parameters, cv=5,verbose = 2)\n#cv.fit(X_train, y_train)","9cc1f693":"#print_results(cv)","f61dca75":"from sklearn.metrics import accuracy_score, precision_score, recall_score","6a0327ee":"rf_best = RandomForestClassifier(n_estimators= 50, max_depth = 10, max_features = 'auto', min_samples_leaf =4, min_samples_split = 10)\nrf_best.fit(X_train,y_train)","4a0426ef":"mdl = rf_best\ny_pred = mdl.predict(X_val)\naccuracy = round(accuracy_score(y_val, y_pred), 3)\nprecision = round(precision_score(y_val, y_pred), 3)\nrecall = round(recall_score(y_val, y_pred), 3)\nprint('MAX DEPTH: {} \/ # OF EST: {} -- A: {} \/ P: {} \/ R: {}'.format(mdl.max_depth,\n                                                                         mdl.n_estimators,\n                                                                         accuracy,\n                                                                         precision,\n                                                                         recall))","73c7c6fe":"y_pred = rf_best.predict(X_test)\naccuracy = round(accuracy_score(y_test, y_pred), 3)\nprecision = round(precision_score(y_test, y_pred), 3)\nrecall = round(recall_score(y_test, y_pred), 3)\nprint('MAX DEPTH: {} \/ # OF EST: {} -- A: {} \/ P: {} \/ R: {}'.format(rf_best.max_depth,\n                                                                     rf_best.n_estimators,\n                                                                     accuracy,\n                                                                     precision,\n                                                                     recall))","3dc99edf":"from sklearn.ensemble import RandomForestClassifier\nrf_best.fit(X, y)","1592a0f3":"predictions = rf_best.predict(titanic_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","3cab8cfb":"output","1686898a":"There is a strong correlation between Cabin_group_G and Pclass. Lets drop Cabin_Group_G","fe020d51":"Since 77% of the data is missing we can drop this feature","a55b4cdd":"## Cabin","4b98b561":"# Converting categorical features into dummy indicators","b069e1e3":"* We can use the **Parent Children** feature to estimate age","7be51931":"## Name","db45c94c":"# Titanic Dataset Challenge\n###  Predict which passengers survived the Titanic shipwreck","9821b53a":"# Exploratory Data Analysis\n### What are some interesting questions we can answer by just plotting the data?\n\n* What is the ratio of passengers who survived vs did not survive?\n* How many passengers who survived were men vs women\n* How many passengers who survived were young vs old\n* What was the Port of Embarkation for passengers who survived?\n* What was the family size of the passengers who survived?\n\nThe above questions are just to find out if there are any good predictor features. Are the features correlated to the target variable? Are there any correlation between the features? If so, we want to remove them.\n\nFirst things first, look at the data!","57e732ec":"* Could we combine SibSp and Parch? It it better to combine features if they have a similar trend so that the model has fewer things to look at.","f9ff4c0c":"# Grid Search CV","b4d685eb":"## Exploring Categorical Features\nExplore Sex, Cabin, Embarked","279db0d1":"### Feature Reference\n- **Name** (str) - Name of the passenger\n- **Pclass** (int) - Ticket class\n- **Sex** (str) - Sex of the passenger\n- **Age** (float) - Age in years\n- **SibSp** (int) - Number of siblings and spouses aboard\n- **Parch** (int) - Number of parents and children aboard\n- **Ticket** (str) - Ticket number\n- **Fare** (float) - Passenger fare\n- **Cabin** (str) - Cabin number\n- **Embarked** (str) - Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","7abc76f1":"## Sex","bb1336f0":"# Evaluate result on validation set","695703c6":"### Inference:\n* There are a total of 12 columns and 891 rows\n* **Name, Sex, Ticket, Cabin, Embarked** are object datatypes\n* **Age, Fare** are floats\n* Rest of the features are integers. We can convert all of them to float datatype before feeding it into the model\n* **Age, Cabin, Embarked** have missing values","a2d69b2e":"# Checking for independence between features","12e34ae0":"# Final model selection and evaluation on test set","bde4046c":"### Inference:\n* There is not much we can infer from here except that the passengers who survived had a higher Fare than those who didn't. Could this be a good estimator? Let's keep looking!","06783771":"# Impute continous missing values","13f02ee3":"### What is the goal of this analysis?\n- To predict which passengers survived the Titanic shipwreck. Looks like a classification problem.","05707a27":"## Embarked","01eee6bf":"## Age","26b69ade":"## SibSp","5595cfa4":"We will try adjusting the following set of hyperparameters:\n* n_estimators = number of trees in the foreset\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min number of data points placed in a node before the node is split\n* min_samples_leaf = min number of data points allowed in a leaf node\n* bootstrap = method for sampling data points (with or without replacement)","bfd46456":"### What is the nature of the target variable \u2013 continuous\/categorical?\n* The target variable is a binary (categorical) variable where 0 = No and 1 = Yes","6f0d0498":"## Age\n* Are the Age values missing in random or in a systematic way?","190a141e":"### Inference:\n* Only 38% of passengers survived\n* The average Age of passengers in the ship were 30 years: ranging 0 - 80\n* Fare ranges from 0 - 512. We might have to scale them so they have a similar range\n\n### Let's look at continous column...","d70ea429":"# Basic model using cross-validation","01992eee":"## Name","9c6c1411":"# Split into Train, Validation, Test","a08981b5":"* Passenger with more number of siblings are less likely to survive","1ea0aa8e":"## Embarked","61b26fe0":"### What can we infer by just looking at it?\n* There are 4 categorical features - **Survived, PClass, Sex, Embarked**\n* There are 4 numeric features - **Age, SibSp, Parch, Fare**\n* **Cabin** is alpha-numeric and has a lot of missing values\n* **PassengerID, Name, Ticket** are all fields for identification and may not contribute much to predicting survival\n* However **Name** has Titles which might prove useful to predict Survival\n","ece64949":"## SibSp and Parch","41c0abb8":"## Fare","3ab4b5de":"## Parch","9115e740":"![Atlantic%20liner%20'Titanic'%20%28Br,%201912%29%20sinking,%20bow%20first,%201912,%20with%20eight%20full%20lifeboats%20nearby%20and%20an%20iceberg%20in%20the%20distance_banner.jpg](attachment:Atlantic%20liner%20'Titanic'%20%28Br,%201912%29%20sinking,%20bow%20first,%201912,%20with%20eight%20full%20lifeboats%20nearby%20and%20an%20iceberg%20in%20the%20distance_banner.jpg)","d31d999a":"* Passengers at port C are more likely to survive. Why though? Are they correlated with another feature?","e6abdc2f":"* Sex is a good estimator of survival\n* Women were more likely to survive than men","5f66df35":"## Inference\n* Embarked may be correlated to Sex.\n* We see passengers from Port S are less likely to survive, could it be because there are more men than women who embarked at Port S?\n* We will choose to include Embarked still ","7f3f8f28":"## Inference\n* We have extracted the titles from the name column\n* Looking at the count, it looks like some titles are less frequent than the others. We can group them up to under the title \"Others\"","6293fbb1":"## Read the Data","e193d064":"### Inference:\n* Looks like **Pclass** and **Fare** are correlated, first class has higher fare than third class. We don't want any features to be correlated in our model.\n* We will choose not to include Fare as one of the features","ff029aa2":"## Sex","71329ed5":"## Exploring Continous Features\nExplore Age, Fare, Pclass, SibSp, Parch","86777ab7":"* For First class, roughly 63% survived. Vertical bars represent errors.\n* For Third class, roughly 25% survived\n* This could be a good estimator of survival ","52ce568d":"## Pclass","8884da92":"* The distribution for Age for **Survived** vs **Did not survive** are relatively similar","ba97d6e6":"## Impute categorical missing values","b32d3a42":"# Deploying the model","ed1e1c56":"### Inference:\n* Except for the peak at the beginning, the distribution for \"Survive\" vs \"Did not survive\" is similar.\n* Previously when we saw the average we saw a drastic difference in Fare for those who survived and those who didn't. We now know that it was caused by an outlier."}}