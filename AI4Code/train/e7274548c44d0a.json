{"cell_type":{"c3c5c2bc":"code","c2460713":"code","9b41d05b":"code","695583cb":"code","6ba86912":"code","5439317d":"code","02cc53ee":"code","00b60b1a":"code","89186cbb":"code","77c06e9c":"code","6ad3fce8":"code","632b52a5":"code","d62e0798":"code","c78c8d51":"code","ab1576c0":"code","d8622e70":"code","863277dc":"code","d53d1020":"code","d4aadf38":"code","451ab328":"code","01462da4":"code","bd127498":"code","ad0c1464":"code","44df51d6":"code","a6d40d4b":"code","13938b08":"code","0bf560c8":"code","93573d26":"code","fe1cf078":"code","4c394ac9":"code","b3a76b81":"code","b9910ed5":"code","2d27f7bf":"code","957a6bff":"code","4a316565":"code","41ac87c7":"code","e2a801b0":"code","b50e7aac":"code","0b0c74ab":"code","092cb61a":"code","c99a879f":"code","f30b8cd6":"code","290c1dc0":"code","3ccd9b98":"code","3054e782":"code","3999a788":"code","4c8b9101":"code","53007bbb":"code","37bd4f55":"code","378818ee":"code","a3ebd994":"code","26cd9c6f":"code","9a37d0f3":"code","1b0338f0":"code","f7c6b3d8":"code","9ae90561":"code","bdaf7627":"code","f31df4d7":"code","65380e7d":"code","d36b3e7e":"code","04fe57bc":"code","49e9fea2":"code","fdafa073":"code","79a025c3":"markdown","3db304b6":"markdown","9382e0f7":"markdown","a65ee4c1":"markdown","c9789e5d":"markdown","42c2a19c":"markdown","4995f5e5":"markdown","6e292017":"markdown","9c16357f":"markdown","11822632":"markdown","408bd95a":"markdown"},"source":{"c3c5c2bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2460713":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom matplotlib.pyplot import figure\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n","9b41d05b":"stock_data= pd.read_csv(\"..\/input\/dataset\/NSE-TATAGLOBAL.csv\")\nstock_data.head()","695583cb":"stock_data.describe()","6ba86912":"stock_data.dtypes","5439317d":"stock_data.shape","02cc53ee":"stock_data.info()","00b60b1a":"print(stock_data.columns)","89186cbb":"print(stock_data.describe)","77c06e9c":"#select numeric columns from data \nstock_data_numeric= stock_data.select_dtypes(include=[np.number])\nnumeric_cols=stock_data_numeric.columns.values\nprint(numeric_cols)","6ad3fce8":"#select non numeric data from dataset \nstock_data_non_numeric= stock_data.select_dtypes(exclude=[np.number])\nnon_numeric_cols= stock_data_non_numeric.columns.values\nprint(non_numeric_cols)","632b52a5":"cols_hm=stock_data.columns[:30]\ncolours=['#000099','#ffff00']\nsns.heatmap(stock_data[cols_hm].isnull(),cmap=sns.color_palette(colours))","d62e0798":"#missing values in dataset through percentage list \nfor col in stock_data.columns:\n    pct_missing=np.mean(stock_data[col].isnull())\n    print('{} - {}%'.format(col,round(pct_missing*100)))","c78c8d51":"stock_data['Date'].unique()","ab1576c0":"stock_data['Open'].unique()","d8622e70":"stock_data['High'].unique()","863277dc":"stock_data['Low'].unique()","d53d1020":"stock_data['Last'].unique()","d4aadf38":"stock_data['Close'].unique()","451ab328":"stock_data['Total Trade Quantity'].unique()","01462da4":"stock_data['Turnover (Lacs)'].unique()","bd127498":"##lets calculate the percentage of High category \nstock_data.High.value_counts(normalize=True)\n\n#plot the graph of perctanges of High category \nstock_data.High.value_counts(normalize=True).plot.barh()\nplt.show()","ad0c1464":"stock_data.columns","44df51d6":"stock_data.columns=['Date','Open','High','Low','Last','Close','Total_Trade_Quantity','Turnover_Lacs']\nstock_data.head(5)","a6d40d4b":"##lets calculate the percentage of Low category \nstock_data.Low.value_counts(normalize=True)\n\n#plot the graph of perctanges of Low category \nstock_data.Low.value_counts(normalize=True).plot.barh()\nplt.show()","13938b08":"#lets calculate the percentage of each Total_Trade_Quantity Category \nstock_data.Total_Trade_Quantity.value_counts(normalize=True)\n\n#plot the graph of percentages of Total_Trade_Quantity category \nstock_data.Total_Trade_Quantity.value_counts(normalize=True).plot.barh()\nplt.show()","0bf560c8":"#lets calculate the percentage of each Turnover_Lacs\nstock_data.Turnover_Lacs.value_counts(normalize=True)\n\n#plot the graph of percentages of Turnover_Lacs category\nstock_data.Turnover_Lacs.value_counts(normalize=True).plot.barh()\nplt.show()","93573d26":"plt.figure(figsize=(16,6))\n#storing heatmap object in a variable to easily access it whenever required to include more features (such as title )\n#set the range of values to be displayed on the colormap from -1 to 1 and set the annotatio to true to display correlation values to heatmap \nheatmap=sns.heatmap(stock_data.corr(),vmin=-1,vmax=1,annot=True)\n#giving title to the heatmap \nheatmap.set_title('Correlation Heatmap',fontdict={'fontsize':12},pad=12)","fe1cf078":"#featuring correlating with Total_Trade_Quantity\nplt.figure(figsize=(8,12))\nheatmap=sns.heatmap(stock_data.corr()[['Total_Trade_Quantity']].sort_values(by='Total_Trade_Quantity',ascending=False),vmin=-1,vmax=1,annot=True,cmap='BrBG')\nheatmap.set_title('Features correlating with Total_Trade_Quantity',fontdict={'fontsize':18},pad=16)","4c394ac9":"import matplotlib.pyplot as plt \nimport numpy as np \n\n#creating dataset\nnp.random.seed(10)\nstock_data=np.random.normal(100,20,200)\n\nfig=plt.figure(figsize=(10,7))\nplt.boxplot(stock_data)\nplt.show()","b3a76b81":"import pandas as pd \nimport matplotlib.pyplot as plt\ndf= pd.read_csv(\"..\/input\/dataset\/NSE-TATAGLOBAL.csv\")\n#we are considering close column as target variable \n#first we will set index as date\ndf['Date']=pd.to_datetime(df.Date,format='%Y-%m-%d')\n#df.index=df['Date']\n\n#plotting of graph \nplt.figure(figsize=(16,8))\nplt.plot(df['Close'],label='Close Price History')\n","b9910ed5":"!pip install -U Tensorflow ","2d27f7bf":"from pandas_datareader import data\nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport datetime as dt\nimport urllib.request , json \nimport os \nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler","957a6bff":"df=df.sort_values('Date')\ndf.head()","4a316565":"plt.figure(figsize=(18,9))\nplt.plot(range(df.shape[0]),(df['Low']+df['High'])\/2.0)\nplt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\nplt.xlabel('Date',fontsize=18)\nplt.ylabel('Mid price',fontsize=18)\nplt.show()","41ac87c7":"df1=df.reset_index()['Close']\nprint(df1)","e2a801b0":"plt.plot(df1)","b50e7aac":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler(feature_range=(0,1))\ndf1=scaler.fit_transform(np.array(df1).reshape(-1,1))\n","0b0c74ab":"#now we will print df1 and check the transformation \nprint(df1)","092cb61a":"train_size=int(len(df1)*(0.65))\ntest_size=len(df1)-train_size\ntrain_data,test_data=df1[0:train_size,:],df1[train_size:len(df1),:1]","c99a879f":"train_size,test_size","f30b8cd6":"import numpy \ndef create_dataset(dataset,time_step=1):\n    dataX,dataY=[],[]\n    for i in range(len(dataset)-time_step-1):\n        a= dataset[i:(i+time_step),0]\n        dataX.append(a)\n        dataY.append(dataset[i+time_step,0])\n        return numpy.array(dataX),numpy.array(dataY)","290c1dc0":"#reshape into X=t,t+1,t+2,t+3 and Y=t+4\ntime_step=100\nX_train,y_train=create_dataset(train_data,time_step)\nX_test,ytest=create_dataset(test_data,time_step)","3ccd9b98":"print(X_train.shape),print(y_train.shape)","3054e782":"print(X_test.shape),print(ytest.shape)","3999a788":"#reshape input to be [samples ,time steps,features] which is required for LSTM \nX_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)","4c8b9101":"## creating stacked LSTM model \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM","53007bbb":"model= Sequential()\nmodel.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\nmodel.add(LSTM(50,return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1))\nmodel.compile(loss=\"mean_squared_error\",optimizer=\"adam\")","37bd4f55":"model.summary()","378818ee":"model.summary()","a3ebd994":"model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1)","26cd9c6f":"import tensorflow as tf","9a37d0f3":"tf.__version__","1b0338f0":"train_predict=model.predict(X_train)\ntest_predict=model.predict(X_test)","f7c6b3d8":"#transform back to original form \ntrain_predict=scaler.inverse_transform(train_predict)\ntest_predict=scaler.inverse_transform(test_predict)","9ae90561":"#calculate RMSE performance metrics\nimport math \nfrom sklearn.metrics import mean_squared_error\nmath.sqrt(mean_squared_error(y_train,train_predict))","bdaf7627":"#test data RMSE \nmath.sqrt(mean_squared_error(y_train,train_predict))","f31df4d7":"##shifting train production for plotting \nlook_back=100\ntrainPredictPlot= numpy.empty_like(df1)\ntrainPredictPlot[:, :]=np.nan\ntrainPredictPlot[look_back:len(train_predict)+look_back,:]=train_predict\n#shift test predictions for plotting \ntestPredictPlot=numpy.empty_like(df1)\ntestPredictPlot[:,:]=numpy.nan\ntestPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict\n#plot baseline and predictions \nplt.plot(scaler.inverse_transform(df1))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","65380e7d":"len(test_data)","d36b3e7e":"x_input=test_data[341:].reshape(1,-1)\nx_input.shape","04fe57bc":"temp_input=list(x_input)\ntemp_input=temp_input[0].tolist()","49e9fea2":"temp_input","fdafa073":"# demonstrate prediction for next 10 days\nfrom numpy import array\n\nlst_output=[]\nn_steps=100\ni=0\nwhile(i<30):\n    \n    if(len(temp_input)>100):\n        #print(temp_input)\n        x_input=np.array(temp_input[1:])\n        print(\"{} day input {}\".format(i,x_input))\n        x_input=x_input.reshape(1,-1)\n        x_input = x_input.reshape((1,371,1))\n        #print(x_input)\n        yhat = model.predict(x_input, verbose=0)\n        print(\"{} day output {}\".format(i,yhat))\n        temp_input.extend(yhat[0].tolist())\n        temp_input=temp_input[1:]\n        #print(temp_input)\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    else:\n        x_input = x_input.reshape((1, n_steps,1))\n        yhat = model.predict(x_input, verbose=0)\n        print(yhat[0])\n        temp_input.extend(yhat[0].tolist())\n        print(len(temp_input))\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    \n\nprint(lst_output)","79a025c3":"# Splitting dataset into test and train dataset ","3db304b6":"**Since we know that we can't use column names with space in between them to plot various graphs therefore we have renamed them to visualize them better.**","9382e0f7":"as we know that close is our target column therefore we will plot the graph against it \n","a65ee4c1":"# Demonstrate prediction for next 10 years","c9789e5d":"# Prediction and performance metrics","42c2a19c":"# Importing Libraries ","4995f5e5":"# Now lets check missing data in dataset through heatmap","6e292017":"**Finding missing values through percentage list ****","9c16357f":"we will now plot the graph to predict the estimated values and its accurate values ","11822632":"# Exploratory Data Analysis ","408bd95a":"This shows that our data don't have missing values in it, since we dont have missing data in our dataset therefore we will not elimate any column or feature from dataset "}}