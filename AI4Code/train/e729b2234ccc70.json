{"cell_type":{"423efb80":"code","18bfbfd6":"code","4d2f95b4":"code","6cb19c47":"code","ccb534af":"code","a64ab702":"code","563ab1bb":"code","a44ff413":"code","ecc635dc":"code","78569448":"code","b63954a2":"code","73b1ab5a":"markdown","b377ef85":"markdown","dce99d88":"markdown","ff2eaf42":"markdown","81c27891":"markdown","0f553315":"markdown","96208718":"markdown","c1f70dae":"markdown","1bf11aee":"markdown","61ea32f0":"markdown","52de89a6":"markdown"},"source":{"423efb80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","18bfbfd6":"# Import libraries\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport os\n\nhtml_pages = {}\n\n# The dataset includes webpages from different days for different companies.\nfor page_name in os.listdir('..\/input'):\n    page_path = f'..\/input\/{page_name}'\n    page_file = open(page_path, 'r')\n    html_data = BeautifulSoup(page_file)\n    # The .html file contains a 'news-table' section that holds all the news headlines\n    html_page = html_data.find(id=\"news-table\")\n    # Add the page to our dictionary\n    html_pages[page_name] = html_page","4d2f95b4":"#html_pages","6cb19c47":"amzn = html_pages['amzn_24jul.html']\n# All the headlines in the .html file are stored as table rows and are tagged as <tr>\namzn_tr = amzn.findAll('tr') #Remember this is a beautiful soup now. So, we can directly find <tr> tag.\n\nfor i, tr in enumerate(amzn_tr):\n    # <a> in these .html pages contain the news headlines.\n    hyperlinks = tr.a.get_text()\n    # <td> in these .html page contain the time stamp.\n    headlines = tr.td.get_text()\n    print(f'{i}:')\n    print(hyperlinks)\n    print(headlines)\n    \n    #if i == 5:\n    #    break","ccb534af":"news = []\n\nfor page_name, news_table in html_pages.items():\n    for x in news_table.findAll('tr'):\n        text = x.get_text()\n        headline = x.a.get_text()\n        # Split date-time, because some elements just have time, while others have time and date as well.\n        date_td = x.td.text.split()\n        # If only time\n        if len(date_td) == 1:\n            time = date_td[0]\n        # If both time and Date\n        else:\n            date = date_td[0]\n            time = date_td[1]\n        # Separate the stock symbol    \n        stock_symbol = page_name.split('_')[0]\n        # Join everything\n        news.append([stock_symbol, date, time, headline])\n        \nnews[:5] # print first five news headlines","a64ab702":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nnew_words = {\n    'crushes': 10,\n    'beats': 5,\n    'misses': -5,\n    'trouble': -10,\n    'falls': -100\n}\n\nanalyser = SentimentIntensityAnalyzer()\n\nanalyser.lexicon.update(new_words)","563ab1bb":"columns = ['stock_symbol', 'date', 'time', 'headline']\n# Load list of the news into a pandas DataFrame\nnews_headlines = pd.DataFrame(news, columns = columns)\n# Go through each headline and evaluate it's polarity. \nsenti_scores = [analyser.polarity_scores(headline) for headline in news_headlines.headline.values]\n# Convert it into a DataFrame for easy joining.\nsentiment = pd.DataFrame(senti_scores)\n# Join headline and its sentiment score.\nnews_headlines = pd.concat([news_headlines, sentiment], axis = 1)\n# Date column is converted from string to datetime\nnews_headlines['date'] = pd.to_datetime(news_headlines.date).dt.date\nnews_headlines.head(10)","a44ff413":"import matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n%matplotlib inline\n# Group by Date and Company\nmean_c = news_headlines.groupby(['date', 'stock_symbol']).mean()\n\nmean_c = mean_c.unstack(level=1)\nmean_c = mean_c.xs('compound', axis=1)\n\nmean_c.plot.bar();","ecc635dc":"# Number of news headlines\ndirty_news = news_headlines.headline.count()\n# Drop duplicates headlines\ncleaned_news = news_headlines.drop_duplicates(['stock_symbol', 'headline'])\n# Number of headlines after dropping \nclean_news = cleaned_news.headline.count()\n\nprint(dirty_news)\nprint(clean_news)","78569448":"single_day = cleaned_news.set_index(['stock_symbol', 'date'])\nsingle_day = single_day.loc['amzn']\n\n# Set day to January of 2019\nsingle_day = single_day.loc['2019-07-24']\n\n# Convert the datetime string to just the time since it is just one day.\nsingle_day['time'] = pd.to_datetime(single_day['time'])\nsingle_day['time'] = single_day.time.dt.time\n\n# Set the index to time and sort by it\nsingle_day.set_index('time', inplace=True)\nsingle_day=single_day.sort_index(ascending=True)\nsingle_day.head()","b63954a2":"# Drop the columns which are useless\nplot_day = single_day.drop(['headline', 'compound'], axis=1)\n# Give names to the sentiments\nplot_day.columns = ['negative', 'positive', 'neutral']\n# Plot a stacked bar chart\nplot_day.plot.bar(stacked = True, \n                  figsize=(10, 6), \n                  title = \"Sentiment Analysis for AMAZON on 2019-07-24\", \n                  color = [\"red\", \"blue\", \"green\"])\nplt.legend(bbox_to_anchor=(1.2, 0.5))\nplt.ylabel(\"scores\");","73b1ab5a":"To finish the project, lets try to explore just one stock on any given day (just a single day), cause this is what one should be interested in.\nAnd don't forget to use 'cleaned_news'.","b377ef85":"Let's try the same with all the webpages in the dataset.","dce99d88":"Trying to perdict the price of a certain stock has always been a challenging problem. This kernel tries to preict the behavior of a stock from news headlines. The stock price and related news for any trading company can be taken from [FINVIZ](http:\/\/finviz.com). \nI have used [Beautiful Soup](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/) for the analysis. 'Beautiful Soup is a python library for pulling data out of HTML and XML files.'","ff2eaf42":"Seems like news regarding Amazon were mostly 'positive' or 'neutral' on July 24, 2019. \nLet's check the stock price for the same day.\n![AMZN.PNG](attachment:AMZN.PNG)","81c27891":"You can display the html pages that we have extracted so far.","0f553315":"I have used VADER to perform sentiment analysis, because it doesn't require any training data. \nWe can add new words to the VADER Lexicon text file which are specific to our problem. In our case, since we are working with stock market, adding words like 'falls', 'trouble', etc. with an associated positive or negative score should be good practice. ","96208718":"We can now visualize the sentiment scores using a plot.","c1f70dae":"Now we can explore the material of these headlines. What's inside them!\nYou may want to check these links for a better understanding of the elements of a HTML page.\n\n[a: The Anchor element](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTML\/Element\/a)\n\n[td: The Table Data Cell element](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTML\/Element\/td)","1bf11aee":"Now we have updated the lexicons in the VADER model, we can try to evaluate the polarity of the news headlines.","61ea32f0":"Try to plot the sentiment scores","52de89a6":"Seems like Tesla had some trouble on November 22. Further exploration reveals that there were only 5 headlines on that day. And 2 of them were almost similar.\nWe can further check and try to remove similar headlines from the dataset."}}