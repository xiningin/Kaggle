{"cell_type":{"abdd0773":"code","c3540677":"code","6c5c36ed":"code","913b153b":"code","38a60f33":"code","863b38d1":"code","d96627ad":"code","09e677ec":"code","348e4caa":"code","2be854d3":"code","11bad8d5":"code","6614ac79":"code","1eaa8566":"code","b1bbda63":"code","a5683537":"code","e7601938":"code","05a95315":"code","ca408dc8":"code","54fc0013":"code","3d539fc6":"code","b9ea51c8":"code","8ac17edf":"code","bae9f0af":"code","5021df74":"code","5ea0bafb":"markdown","0d104441":"markdown","bac63d47":"markdown","b1f4e9da":"markdown","f7f3d17c":"markdown","06eb4f84":"markdown","5b80771c":"markdown"},"source":{"abdd0773":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3540677":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom urllib.parse import urlparse\nfrom nltk.corpus import stopwords \nimport re\nimport nltk\nfrom wordcloud import WordCloud\nnltk.download(\"stopwords\")\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score, recall_score, f1_score\nfrom lightgbm import LGBMClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GRU, Embedding\nfrom tensorflow.keras.optimizers import Adam,SGD,RMSprop\nimport optuna\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')","6c5c36ed":"data = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndata.head()","913b153b":"print(\"Null Values\\n\"+str(data.isnull().sum()))\nprint(\"\\nData Shape: \" , data.shape)","38a60f33":"def find_num_words(data):\n    wordlist = []\n    splitting_list = data.str.split()\n    for i in range(len(splitting_list)):\n        for j in range(len(splitting_list[i])):\n            wordlist.append(splitting_list[i][j])\n            \n    wordset = set(wordlist)\n    \n    print(\"Number of unique words:\",len(wordset),\"Number of words:\",len(wordlist))\n\nfind_num_words(data.headline)","863b38d1":"print(data.is_sarcastic.value_counts() ,\"\\n\")\n\nfig, ax = plt.subplots(1,2, figsize=(19, 5))\nf1 = sns.countplot(data.is_sarcastic, ax = ax[0]);\nf1.set_title(\"Count of real and fake data\")\nf1.set_ylabel(\"Count\")\nf1.set_xlabel(\"Target\")\n\nf2 = plt.pie(data[\"is_sarcastic\"].value_counts().values,explode=[0,0],labels=data.is_sarcastic.value_counts().index, autopct='%1.1f%%')\nfig.show()","d96627ad":"def get_netloc(url):\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n    return netloc\n\nlink = []\nfor i in range(len(data.article_link)):\n    link.append(get_netloc(data.article_link[i]))\n    \ndata[\"link\"] = link","09e677ec":"data.link = data[\"link\"].str.replace(\"local.theonion.com\",\"www.theonion.com\").replace(\"politics.theonion.com\",\"www.theonion.com\").replace(\"entertainment.theonion.com\",\"www.theonion.com\").replace(\"sports.theonion.com\",\"www.theonion.com\").replace(\"ogn.theonion.com\",\"www.theonion.com\")\ndata.link = data.link.str.replace('www.huffingtonpost.comhttp:','www.huffingtonpost.com').replace('www.huffingtonpost.comhttps:','www.huffingtonpost.com')\ndata.link.value_counts()","348e4caa":"ax = sns.countplot(x=\"link\",  hue='is_sarcastic', data=data, palette=\"pastel\")\nplt.title(\"Distribution of The Sarcasm According to Site\");","2be854d3":"#Removing punctiation marks\ndef remove_punctuations(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n#Removing special characters\ndef remove_specialchars(text):\n    return re.sub(\"[^a-zA-Z]\",\" \",text)\n\n#Removal of stopwords and lemmatization\ndef remove_stopwords_and_lemmatization(text):\n    final_text = []\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    \n    for word in text:\n        if word not in set(stopwords.words('english')):\n            lemma = nltk.WordNetLemmatizer()\n            word = lemma.lemmatize(word) \n            final_text.append(word)\n    return \" \".join(final_text)\n\n\n#Total function\ndef cleaning(text):\n    text = remove_punctuations(text)\n    text = remove_specialchars(text)\n    text = remove_stopwords_and_lemmatization(text)\n    return text\n\ndata['headline']=data['headline'].apply(cleaning)","11bad8d5":"text = \" \".join(review for review in data.headline)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))\n\n# Generate a word cloud image\nwordcloud = WordCloud(background_color=\"white\").generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6614ac79":"data.head()","1eaa8566":"sentences = data[\"headline\"].values.tolist()\ntarget = data[\"is_sarcastic\"].values.tolist()","b1bbda63":"print(sentences[:2])\nprint(target[:2])","a5683537":"num_words = 1000\ntokenizer = Tokenizer(num_words=num_words)\n\ntokenizer.fit_on_texts(sentences)\ntokens = tokenizer.texts_to_sequences(sentences)\n\nnumTokens = [len(token) for token in tokens]\nnumTokens = np.array(numTokens)\nprint(\"Tokens'mean\",np.mean(numTokens))\nprint(\"Max\", np.max(numTokens))\nprint(\"Argmax\", np.argmax(numTokens))","e7601938":"#Longest one\nsentences[7302]","05a95315":"max_tokens = int(np.mean(numTokens) + 2*np.std(numTokens))\nprint(\"Calculated maximum number of tokens :\",max_tokens)\nprint(\"What percentage of the data fits this average? :\",np.sum(numTokens < max_tokens) \/ len(numTokens))","ca408dc8":"padding_data = pad_sequences(tokens, maxlen=max_tokens)\nprint(padding_data.shape)\nprint(len(target))","54fc0013":"X_train, X_test, y_train, y_test = train_test_split(padding_data, data.is_sarcastic, random_state=1, test_size=0.15, stratify=data.is_sarcastic)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.1, stratify=y_train)","3d539fc6":"def objective(trial):\n    # Parameters\n    L2 = trial.suggest_float(\"l\", 1e-5, 1e-2, log=True)\n    EMBEDDING_SIZE = trial.suggest_int(\"embedding_size\", 10, 50, step=10)\n    BATCH_SIZE = trial.suggest_int(\"batch_size\", 16, 64, step=8)\n    EPOCHS = trial.suggest_int(\"epochs\", 10,30, step=10)\n    LR = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n    OPT = trial.suggest_categorical(\"optimizer\", [Adam, SGD, RMSprop])\n    \n\n    # Model\n    \"\"\"base_model = model(include_top=False, weights=\"imagenet\", pooling=\"max\")\n    for layer in base_model.layers:\n        layer.trainable = False\n    x = base_model.output\n    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\"\"\"\n    \n    model = Sequential()\n    model.add(Embedding(input_dim = num_words,\n                        output_dim=EMBEDDING_SIZE,\n                        input_length=max_tokens,\n                        name='embedding_layer'))\n    \n    model.add(GRU(units=128, return_sequences=True, kernel_regularizer = tf.keras.regularizers.l2(l=L2)))\n    model.add(GRU(units=64,return_sequences=True, kernel_regularizer = tf.keras.regularizers.l2(l=L2)))\n    model.add(GRU(units=32))\n    model.add(Dense(16, activation=\"relu\"))\n    model.add(Dense(1, activation='sigmoid'))\n\n \n    model.compile(optimizer=OPT(lr=LR), loss='binary_crossentropy', metrics=['accuracy'])\n\n    H = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)\n\n    val_loss, val_acc = model.evaluate(X_test,y_test)\n    \n    return val_loss # optuna de\u011ferlendirmesini en k\u00fc\u00e7\u00fck de\u011fere g\u00f6re yap\u0131yor o y\u00fczden loss d\u00f6nd\u00fcrd\u00fcm. Accuracy d\u00f6nd\u00fcrd\u00fc\u011f\u00fcmde acc b\u00fcy\u00fcd\u00fck\u00e7e bunu k\u00f6t\u00fc bir \u015feymi\u015f gibi alg\u0131lad\u0131.\n\n\n# Default Sampler is Tree-structured Parzen Estimator algorithm\n\nstudy = optuna.create_study()\nstart = time.time()\nstudy.optimize(objective, n_trials=5)\nend = time.time()","b9ea51c8":"best_params = study.best_params\nprint(best_params)\nprint(\"model took %0.2f seconds to train\" % (end - start))","8ac17edf":"EMBEDDING_SIZE = 40\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim = num_words,\n                    output_dim=EMBEDDING_SIZE,\n                    input_length=max_tokens,\n                    name='embedding_layer'))\nmodel.add(GRU(units=128, return_sequences=True, kernel_regularizer = tf.keras.regularizers.l2(l=1.367675698620851e-05)))\nmodel.add(GRU(units=64,return_sequences=True, kernel_regularizer = tf.keras.regularizers.l2(l=1.367675698620851e-05)))\nmodel.add(GRU(units=32))\n#model.add(Dense(16, activation=\"relu\"))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","bae9f0af":"model.compile(loss=\"binary_crossentropy\", optimizer = RMSprop(lr=8.74355057651944e-05), metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=40)","5021df74":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","5ea0bafb":"# Model Building","0d104441":"In the graph below, we examined which sites the real and fake news were obtained from. We used urlparse to get the site name from url.","bac63d47":"# Visualization\nIn the first two visualization, we examine the distribution of real and fake data. First graph is a countplot from seaborn. It shows both real and fake news numbers with columnns. Second graph is a pie graph. If we use pie graph, we can examine percentages of distributions.","b1f4e9da":"# Gather Data\nNews Headlines dataset for Sarcasm Detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). They collect real (and non-sarcastic) news headlines from HuffPost.\n\nEach record consists of three attributes:\n* is_sarcastic: 1 if the record is sarcastic otherwise 0\n* headline: the headline of the news article\n* article_link: link to the original news article. Useful in collecting supplementary data","f7f3d17c":"## Tokenizing","06eb4f84":"# Preparing Data","5b80771c":"In this dataset there is not any null value and there is 28619 sentence."}}