{"cell_type":{"5bdc037a":"code","6242d110":"code","f1030436":"code","ab04e942":"code","7a11f0d1":"code","d42a70e1":"code","dbe10fee":"code","158b6d61":"code","5dd763c5":"code","80bc989e":"code","00f68f3f":"code","2fb9544c":"code","be1a2f91":"code","a511960a":"code","99a7ea34":"code","fab31d8b":"code","283e9a50":"code","5b6080e7":"code","9622c55c":"code","354f75a8":"code","e12a9fcd":"code","575c362e":"code","59d02ada":"code","38b69617":"code","48131526":"code","deaee7a4":"code","d1e7cfe6":"code","e46a32ff":"code","8ffea3cd":"code","84a80960":"code","de6015ef":"code","f469a670":"code","c9e4e2be":"code","79cb709b":"code","5d951f9b":"code","2419cddc":"code","397925ae":"code","636226b8":"code","3b487edf":"code","7409050f":"code","62aede25":"code","b778fd7a":"code","b646f2f0":"code","e0216e61":"code","3a48f515":"code","cdf64032":"code","056ba484":"code","30824a2c":"code","5aaa944b":"code","01a60e81":"code","c1ff7b4d":"code","f3fc477b":"code","bebf57b3":"code","bb81ffe6":"code","98300b6e":"code","b38a0b9a":"code","434e7332":"code","9b34763b":"code","900f6228":"code","f2267437":"code","8e43a661":"markdown","91163e26":"markdown"},"source":{"5bdc037a":"!wget https:\/\/github.com\/Chaogan-Yan\/DPABI\/raw\/master\/Templates\/ch2better.nii","6242d110":"import numpy as np # linear algebra\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\nimport h5py\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nmask_filename = '..\/input\/trends-assessment-prediction\/fMRI_mask.nii'\nsubject_filename = '..\/input\/trends-assessment-prediction\/fMRI_train\/10004.mat'\nsmri_filename = 'ch2better.nii'\nmask_niimg = nl.image.load_img(mask_filename)\n\ndef load_subject(filename, mask_niimg):\n    \"\"\"\n    Load a subject saved in .mat format with\n        the version 7.3 flag. Return the subject\n        niimg, using a mask niimg as a template\n        for nifti headers.\n        \n    Args:\n        filename    <str>            the .mat filename for the subject data\n        mask_niimg  niimg object     the mask niimg object used for nifti headers\n    \"\"\"\n    subject_data = None\n    with h5py.File(subject_filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_niimg = nl.image.new_img_like(mask_niimg, subject_data, affine=mask_niimg.affine, copy_header=True)\n    return subject_niimg\nsubject_niimg = load_subject(subject_filename, mask_niimg)\nprint(\"Image shape is %s\" % (str(subject_niimg.shape)))\nnum_components = subject_niimg.shape[-1]\nprint(\"Detected {num_components} spatial maps\".format(num_components=num_components))","f1030436":"nlplt.plot_prob_atlas(subject_niimg, bg_img=smri_filename, view_type='filled_contours', draw_cross=False, title='All %d spatial maps' % num_components, threshold='auto')\n","ab04e942":"grid_size = int(np.ceil(np.sqrt(num_components)))\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*10, grid_size*10))\n[axi.set_axis_off() for axi in axes.ravel()]\nrow = -1\nfor i, cur_img in enumerate(nl.image.iter_img(subject_niimg)):\n    col = i % grid_size\n    if col == 0:\n        row += 1\n    nlplt.plot_stat_map(cur_img, bg_img=smri_filename, title=\"IC %d\" % i, axes=axes[row, col], threshold=3, colorbar=False)\n","7a11f0d1":"import matplotlib.pyplot as plt\n\nfrom nilearn import datasets\nhaxby_dataset = datasets.fetch_haxby()   # load dataset\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is at: %s' %\n      haxby_dataset.func[0])  # 4D data\n\n# Build the mean image because we have no anatomic data\nfrom nilearn import image\nfunc_filename = haxby_dataset.func[0]\nmean_img = image.mean_img(func_filename)\n\nz_slice = -14\n\nfig = plt.figure(figsize=(4, 5.4), facecolor='k')\n\nfrom nilearn.plotting import plot_anat, show\ndisplay = plot_anat(mean_img, display_mode='z', cut_coords=[z_slice],\n                    figure=fig)\nmask_vt_filename = haxby_dataset.mask_vt[0]\nmask_house_filename = haxby_dataset.mask_house[0]\nmask_face_filename = haxby_dataset.mask_face[0]\ndisplay.add_contours(mask_vt_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['red'])\ndisplay.add_contours(mask_house_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['blue'])\ndisplay.add_contours(mask_face_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['limegreen'])\n\n# We generate a legend using the trick described on\n# http:\/\/matplotlib.sourceforge.net\/users\/legend_guide.httpml#using-proxy-artist\nfrom matplotlib.patches import Rectangle\np_v = Rectangle((0, 0), 1, 1, fc=\"red\")\np_h = Rectangle((0, 0), 1, 1, fc=\"blue\")\np_f = Rectangle((0, 0), 1, 1, fc=\"limegreen\")\nplt.legend([p_v, p_h, p_f], [\"vt\", \"house\", \"face\"])\n\nshow()","d42a70e1":"from nilearn import datasets\n\n# haxby dataset to have EPI images and masks\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is at: %s' %\n      haxby_dataset.func[0])  # 4D data\n\nhaxby_anat_filename = haxby_dataset.anat[0]\nhaxby_mask_filename = haxby_dataset.mask_vt[0]\nhaxby_func_filename = haxby_dataset.func[0]\n\n# one motor contrast map from NeuroVault\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","dbe10fee":"from nilearn import plotting\n\n# Visualizing t-map image on EPI template with manual\n# positioning of coordinates using cut_coords given as a list\nplotting.plot_stat_map(stat_img,\n                       threshold=3, title=\"plot_stat_map\",\n                       cut_coords=[36, -27, 66])","158b6d61":"view = plotting.view_img(stat_img, threshold=3)\n# In a Jupyter notebook, if ``view`` is the output of a cell, it will\n# be displayed below the cell\nview","5dd763c5":"plotting.plot_glass_brain(stat_img, title='plot_glass_brain',\n                          threshold=3)","80bc989e":"plotting.plot_anat(haxby_anat_filename, title=\"plot_anat\")","00f68f3f":"plotting.plot_roi(haxby_mask_filename, bg_img=haxby_anat_filename,\n                  title=\"plot_roi\")","2fb9544c":"# Import image processing tool\nfrom nilearn import image\n\n# Compute the voxel_wise mean of functional images across time.\n# Basically reducing the functional image from 4D to 3D\nmean_haxby_img = image.mean_img(haxby_func_filename)\n\n# Visualizing mean image (3D)\nplotting.plot_epi(mean_haxby_img, title=\"plot_epi\")","be1a2f91":"\n# haxby dataset to have anatomical image, EPI images and masks\n#haxby_dataset = datasets.fetch_haxby()\nhaxby_anat_filename = haxby_dataset.anat[0]\nhaxby_mask_filename = haxby_dataset.mask_vt[0]\nhaxby_func_filename = haxby_dataset.func[0]\n\n# localizer dataset to have contrast maps\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","a511960a":"plotting.plot_stat_map(stat_img, display_mode='ortho',\n                       cut_coords=[36, -27, 60],\n                       title=\"display_mode='ortho', cut_coords=[36, -27, 60]\")\n\n","99a7ea34":"plotting.plot_stat_map(stat_img, display_mode='z', cut_coords=5,\n                       title=\"display_mode='z', cut_coords=5\")","fab31d8b":"plotting.plot_stat_map(stat_img, display_mode='x',\n                       cut_coords=[-36, 36],\n                       title=\"display_mode='x', cut_coords=[-36, 36]\")","283e9a50":"plotting.plot_stat_map(stat_img, display_mode='y', cut_coords=1,\n                       title=\"display_mode='y', cut_coords=1\")","5b6080e7":"plotting.plot_stat_map(stat_img, display_mode='z',\n                       cut_coords=1, colorbar=False,\n                       title=\"display_mode='z', cut_coords=1, colorbar=False\")","9622c55c":"plotting.plot_stat_map(stat_img, display_mode='xz',\n                       cut_coords=[36, 60],\n                       title=\"display_mode='xz', cut_coords=[36, 60]\")","354f75a8":"plotting.plot_stat_map(stat_img, display_mode='yx',\n                       cut_coords=[-27, 36],\n                       title=\"display_mode='yx', cut_coords=[-27, 36]\")","e12a9fcd":"plotting.plot_stat_map(stat_img, display_mode='yz',\n                       cut_coords=[-27, 60],\n                       title=\"display_mode='yz', cut_coords=[-27, 60]\")","575c362e":"plotting.plot_stat_map(stat_img, display_mode='tiled',\n                       cut_coords=[36, -27, 60],\n                       title=\"display_mode='tiled'\")","59d02ada":"from nilearn import image\n\n# Compute voxel-wise mean functional image across time dimension. Now we have\n# functional image in 3D assigned in mean_haxby_img\nmean_haxby_img = image.mean_img(haxby_func_filename)","38b69617":"display = plotting.plot_anat(mean_haxby_img, title=\"add_edges\")\n\n# We are now able to use add_edges method inherited in plotting object named as\n# display. First argument - anatomical image  and by default edges will be\n# displayed as red 'r', to choose different colors green 'g' and  blue 'b'.\ndisplay.add_edges(haxby_anat_filename)","48131526":"# As seen before, we call the `plot_anat` function with a background image\n# as first argument, in this case again the mean fMRI image and argument\n# `cut_coords` as list for manual cut with coordinates pointing at masked\n# brain regions\ndisplay = plotting.plot_anat(mean_haxby_img, title=\"add_contours\",\n                             cut_coords=[-34, -39, -9])\n# Now use `add_contours` in display object with the path to a mask image from\n# the Haxby dataset as first argument and argument `levels` given as list\n# of values to select particular level in the contour to display and argument\n# `colors` specified as red 'r' to see edges as red in color.\n# See help on matplotlib.pyplot.contour to use more options with this method\ndisplay.add_contours(haxby_mask_filename, levels=[0.5], colors='r')\n\n","deaee7a4":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"add_contours with filled=True\",\n                             cut_coords=[-34, -39, -9])\n\n# By default, no color fillings will be shown using `add_contours`. To see\n# contours with color fillings use argument filled=True. contour colors are\n# changed to blue 'b' with alpha=0.7 sets the transparency of color fillings.\n# See help on matplotlib.pyplot.contourf to use more options given that filled\n# should be True\ndisplay.add_contours(haxby_mask_filename, filled=True, alpha=0.7,\n                     levels=[0.5], colors='b')","d1e7cfe6":"display = plotting.plot_anat(mean_haxby_img, title=\"add_markers\",\n                             cut_coords=[-34, -39, -9])\n\n# Coordinates of seed regions should be specified in first argument and second\n# argument `marker_color` denotes color of the sphere in this case yellow 'y'\n# and third argument `marker_size` denotes size of the sphere\ncoords = [(-34, -39, -9)]\ndisplay.add_markers(coords, marker_color='y', marker_size=100)","e46a32ff":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"adding a scale bar\",\n                             cut_coords=[-34, -39, -9])\ndisplay.annotate(scalebar=True)","8ffea3cd":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"adding a scale bar\",\n                             cut_coords=[-34, -39, -9])\ndisplay.annotate(scalebar=True, scale_size=25, scale_units='mm')","84a80960":"from nilearn import datasets\n\nrest_dataset = datasets.fetch_development_fmri(n_subjects=20)\nfunc_filenames = rest_dataset.func\nconfounds = rest_dataset.confounds\n","de6015ef":"# Import dictionary learning algorithm from decomposition module and call the\n# object and fit the model to the functional datasets\nfrom nilearn.decomposition import DictLearning\n\n# Initialize DictLearning object\ndict_learn = DictLearning(n_components=8, smoothing_fwhm=6.,\n                          memory=\"nilearn_cache\", memory_level=2,\n                          random_state=0)\n# Fit to the data\ndict_learn.fit(func_filenames)\n# Resting state networks\/maps in attribute `components_img_`\n# Note that this attribute is implemented from version 0.4.1.\n# For older versions, see the note section above for details.\ncomponents_img = dict_learn.components_img_\n\n# Visualization of functional networks\n# Show networks using plotting utilities\nfrom nilearn import plotting\n\nplotting.plot_prob_atlas(components_img, view_type='filled_contours',\n                         title='Dictionary Learning maps')","f469a670":"# Import Region Extractor algorithm from regions module\n# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n# maps, less the threshold means that more intense non-voxels will be survived.\nfrom nilearn.regions import RegionExtractor\n\nextractor = RegionExtractor(components_img, threshold=0.5,\n                            thresholding_strategy='ratio_n_voxels',\n                            extractor='local_regions',\n                            standardize=True, min_region_size=1350)\n# Just call fit() to process for regions extraction\nextractor.fit()\n# Extracted regions are stored in regions_img_\nregions_extracted_img = extractor.regions_img_\n# Each region index is stored in index_\nregions_index = extractor.index_\n# Total number of regions extracted\nn_regions_extracted = regions_extracted_img.shape[-1]\n\n# Visualization of region extraction results\ntitle = ('%d regions are extracted from %d components.'\n         '\\nEach separate color of region indicates extracted region'\n         % (n_regions_extracted, 8))\nplotting.plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n                         title=title)","c9e4e2be":"# First we need to do subjects timeseries signals extraction and then estimating\n# correlation matrices on those signals.\n# To extract timeseries signals, we call transform() from RegionExtractor object\n# onto each subject functional data stored in func_filenames.\n# To estimate correlation matrices we import connectome utilities from nilearn\nfrom nilearn.connectome import ConnectivityMeasure\n\ncorrelations = []\n# Initializing ConnectivityMeasure object with kind='correlation'\nconnectome_measure = ConnectivityMeasure(kind='correlation')\nfor filename, confound in zip(func_filenames, confounds):\n    # call transform from RegionExtractor object to extract timeseries signals\n    timeseries_each_subject = extractor.transform(filename, confounds=confound)\n    # call fit_transform from ConnectivityMeasure object\n    correlation = connectome_measure.fit_transform([timeseries_each_subject])\n    # saving each subject correlation to correlations\n    correlations.append(correlation)\n\n# Mean of all correlations\nimport numpy as np\nmean_correlations = np.mean(correlations, axis=0).reshape(n_regions_extracted,\n                                                          n_regions_extracted)","79cb709b":"title = 'Correlation between %d regions' % n_regions_extracted\n\n# First plot the matrix\ndisplay = plotting.plot_matrix(mean_correlations, vmax=1, vmin=-1,\n                               colorbar=True, title=title)\n\n# Then find the center of the regions and plot a connectome\nregions_img = regions_extracted_img\ncoords_connectome = plotting.find_probabilistic_atlas_cut_coords(regions_img)\n\nplotting.plot_connectome(mean_correlations, coords_connectome,\n                         edge_threshold='90%', title=title)","5d951f9b":"# First, we plot a network of index=4 without region extraction (left plot)\nfrom nilearn import image\n\nimg = image.index_img(components_img, 4)\ncoords = plotting.find_xyz_cut_coords(img)\ndisplay = plotting.plot_stat_map(img, cut_coords=coords, colorbar=False,\n                                 title='Showing one specific network')","2419cddc":"# For this, we take the indices of the all regions extracted related to original\n# network given as 4.\nregions_indices_of_map3 = np.where(np.array(regions_index) == 4)\n\ndisplay = plotting.plot_anat(cut_coords=coords,\n                             title='Regions from this network')\n\n# Add as an overlay all the regions of index 4\ncolors = 'rgbcmyk'\nfor each_index_of_map3, color in zip(regions_indices_of_map3[0], colors):\n    display.add_overlay(image.index_img(regions_extracted_img, each_index_of_map3),\n                        cmap=plotting.cm.alpha_cmap(color))\n\nplotting.show()","397925ae":"from nilearn import datasets\n\n# By default 2nd subject will be fetched\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First anatomical nifti image (3D) located is at: %s' %\n      haxby_dataset.anat[0])\nprint('First functional nifti image (4D) is located at: %s' %\n      haxby_dataset.func[0])","636226b8":"from nilearn.image.image import mean_img\n\n# Compute the mean EPI: we do the mean along the axis 3, which is time\nfunc_filename = haxby_dataset.func[0]\nmean_haxby = mean_img(func_filename)\n\nfrom nilearn.plotting import plot_epi, show\nplot_epi(mean_haxby)","3b487edf":"from nilearn.masking import compute_epi_mask\nmask_img = compute_epi_mask(func_filename)\n\n# Visualize it as an ROI\nfrom nilearn.plotting import plot_roi\nplot_roi(mask_img, mean_haxby)","7409050f":"from nilearn.masking import apply_mask\nmasked_data = apply_mask(func_filename, mask_img)\n\n# masked_data shape is (timepoints, voxels). We can plot the first 150\n# timepoints from two voxels\n\n# And now plot a few of these\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(7, 5))\nplt.plot(masked_data[:150, :2])\nplt.xlabel('Time [TRs]', fontsize=16)\nplt.ylabel('Intensity', fontsize=16)\nplt.xlim(0, 150)\nplt.subplots_adjust(bottom=.12, top=.95, right=.95, left=.12)\n\nshow()","62aede25":"from nilearn import datasets\n\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","b778fd7a":"fsaverage = datasets.fetch_surf_fsaverage()","b646f2f0":"from nilearn import surface\n\ntexture = surface.vol_to_surf(stat_img, fsaverage.pial_right)","e0216e61":"from nilearn import plotting\n\nplotting.plot_surf_stat_map(fsaverage.infl_right, texture, hemi='right',\n                            title='Surface right hemisphere', colorbar=True,\n                            threshold=1., bg_map=fsaverage.sulc_right)\n\n","3a48f515":"plotting.plot_glass_brain(stat_img, display_mode='r', plot_abs=False,\n                          title='Glass brain', threshold=2.)\n\nplotting.plot_stat_map(stat_img, display_mode='x', threshold=1.,\n                       cut_coords=range(0, 51, 10), title='Slices')\n","cdf64032":"big_fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\nbig_texture = surface.vol_to_surf(stat_img, big_fsaverage.pial_right)\n\nplotting.plot_surf_stat_map(big_fsaverage.infl_right,\n                            big_texture, hemi='right', colorbar=True,\n                            title='Surface right hemisphere: fine mesh',\n                            threshold=1., bg_map=big_fsaverage.sulc_right)\n\n\nplotting.show()","056ba484":"view = plotting.view_surf(fsaverage.infl_right, texture, threshold='90%',\n                          bg_map=fsaverage.sulc_right)\n\n# In a Jupyter notebook, if ``view`` is the output of a cell, it will\n# be displayed below the cell\nview","30824a2c":"view = plotting.view_img_on_surf(stat_img, threshold='90%')\n# view.open_in_browser()\n\nview","5aaa944b":"from nilearn import datasets\nprint('Datasets are stored in: %r' % datasets.get_data_dirs())","01a60e81":"motor_images = datasets.fetch_neurovault_motor_task()\nmotor_images.images","c1ff7b4d":"tmap_filename = motor_images.images[0]\n\nfrom nilearn import plotting\nplotting.plot_stat_map(tmap_filename)","f3fc477b":"plotting.plot_stat_map(tmap_filename, threshold=3)","bebf57b3":"rsn = datasets.fetch_atlas_smith_2009()['rsn10']\nrsn","bb81ffe6":"from nilearn import image\nprint(image.load_img(rsn).shape)","98300b6e":"first_rsn = image.index_img(rsn, 0)\nprint(first_rsn.shape)","b38a0b9a":"plotting.plot_stat_map(first_rsn)","434e7332":"for img in image.iter_img(rsn):\n    # img is now an in-memory 3D img\n    plotting.plot_stat_map(img, threshold=3, display_mode=\"z\", cut_coords=1,\n                           colorbar=False)","9b34763b":"selected_volumes = image.index_img(rsn, slice(3, 5))","900f6228":"for img in image.iter_img(selected_volumes):\n    plotting.plot_stat_map(img)","f2267437":"# Load 4D probabilistic atlases\nfrom nilearn import datasets\n\n# Harvard Oxford Atlasf\nharvard_oxford = datasets.fetch_atlas_harvard_oxford('cort-prob-2mm')\nharvard_oxford_sub = datasets.fetch_atlas_harvard_oxford('sub-prob-2mm')\n\n# Multi Subject Dictionary Learning Atlas\nmsdl = datasets.fetch_atlas_msdl()\n\n# Smith ICA Atlas and Brain Maps 2009\nsmith = datasets.fetch_atlas_smith_2009()\n\n# ICBM tissue probability\nicbm = datasets.fetch_icbm152_2009()\n\n# Allen RSN networks\nallen = datasets.fetch_atlas_allen_2011()\n\n# Pauli subcortical atlas\nsubcortex = datasets.fetch_atlas_pauli_2017()\n\n# Visualization\nfrom nilearn import plotting\n\natlas_types = {'Harvard_Oxford': harvard_oxford.maps,\n               'Harvard_Oxford sub': harvard_oxford_sub.maps,\n               'MSDL': msdl.maps, 'Smith 2009 10 RSNs': smith.rsn10,\n               'Smith2009 20 RSNs': smith.rsn20,\n               'Smith2009 70 RSNs': smith.rsn70,\n               'Smith2009 20 Brainmap': smith.bm20,\n               'Smith2009 70 Brainmap': smith.bm70,\n               'ICBM tissues': (icbm['wm'], icbm['gm'], icbm['csf']),\n               'Allen2011': allen.rsn28,\n               'Pauli2017 Subcortical Atlas': subcortex.maps,\n               }\n\nfor name, atlas in sorted(atlas_types.items()):\n    plotting.plot_prob_atlas(atlas, title=name)\n\n# An optional colorbar can be set\nplotting.plot_prob_atlas(smith.bm10, title='Smith2009 10 Brainmap (with'\n                                           ' colorbar)',\n                         colorbar=True)\nprint('ready')\nplotting.show()","8e43a661":"# Hope you guys find this notebook useful.\n# If you like this notebook, please upvote.***\ud83d\udc4d","91163e26":"#  Fetch brain development functional datasets"}}