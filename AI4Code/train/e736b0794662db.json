{"cell_type":{"c6042c9a":"code","8db0ac7d":"code","400f2960":"code","23d024b4":"code","271d17dc":"code","2b1f7615":"code","6fbba2d0":"code","4266632c":"code","d9694069":"code","16fd5ed5":"code","e33549be":"code","d0250ff9":"code","fea04836":"code","f10d081c":"code","1fe7980a":"code","08e888c8":"code","64305423":"code","aae276be":"code","60867b94":"code","d2f4f03e":"code","d8e9f2b8":"code","ead69786":"code","5d7a4e83":"code","41f0489d":"code","7841cfa9":"code","b50966d6":"code","5d2ac31c":"code","5aef9e32":"markdown","48a6852d":"markdown","f5e3c6df":"markdown","2d212d4c":"markdown","dc19fe1d":"markdown","2bf543a3":"markdown","995a849d":"markdown","ec12f0e8":"markdown","68b7079a":"markdown","cb8769ae":"markdown","e2c7874f":"markdown","92119e1b":"markdown","0bc69a5f":"markdown","f9ba8030":"markdown"},"source":{"c6042c9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8db0ac7d":"!pip install tensorflow-gpu==2.0.0-beta1","400f2960":"import gc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport tensorflow as tf\nfrom tensorflow import feature_column\nimport tensorflow.keras.models\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, DenseFeatures\n\nprint(tensorflow.__version__)","23d024b4":"path = '..\/input\/'\ndf_train_id = pd.read_csv(path + 'train_identity.csv')\ndf_train_trans = pd.read_csv(path + 'train_transaction.csv')\ndf_test_id = pd.read_csv(path + 'test_identity.csv')\ndf_test_trans = pd.read_csv(path + 'test_transaction.csv')","271d17dc":"df_train_trans.head()","2b1f7615":"df_train_id.head()","6fbba2d0":"df_train = pd.merge(df_train_trans, df_train_id, on='TransactionID', how='left')\ndf_test = pd.merge(df_test_trans, df_test_id, on='TransactionID', how='left')","4266632c":"df_train.isnull().sum().sort_values(ascending=False)[:20]","d9694069":"df_test.isnull().sum().sort_values(ascending=False)[:20]","16fd5ed5":"miss_val_threshold = 0.25\n\ncol_to_del = []\n\nfor c in df_train.columns:\n    if df_train[c].isnull().sum() > df_train.shape[0]*miss_val_threshold:\n        col_to_del.append(c)\n\nfor c in df_test.columns:\n    if df_train[c].isnull().sum() > df_test.shape[0]*miss_val_threshold:\n        if c not in col_to_del:\n            col_to_del.append(c)\n        \ncol_to_del.append('TransactionID')\ncol_to_del.append('TransactionDT')","e33549be":"df_train.drop(columns=col_to_del, inplace = True)\ndf_test.drop(columns=col_to_del, inplace = True)","d0250ff9":"df_train.shape\ndf_test.shape","fea04836":"df_train.fillna(-999, inplace= True)\ndf_test.fillna(-999, inplace= True)","f10d081c":"# before optimization\ndf_train.info()\ndf_test.info()","1fe7980a":"col_int16 = ['TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2',\n           'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n           'D1', 'D10', 'D15', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\n           'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30',\n           'V31', 'V32', 'V33', 'V34', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59',\n           'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', \n           'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79',\n           'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89',\n           'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99',\n           'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109',\n           'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119',\n           'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129',\n           'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279',\n           'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289',\n           'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299',\n           'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309',\n           'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319',\n           'V320', 'V321']","08e888c8":"for c in col_int16:\n    df_train[c] = df_train[c].astype(np.int16)\n    df_test[c] = df_test[c].astype(np.int16)","64305423":"# one-hot-encoding of categorical values in train dataset\ncol_dummies = ['ProductCD', 'card4', 'card6', 'P_emaildomain']\n\nfor c in col_dummies:\n    df_train[c] = pd.get_dummies(df_train[c])\n    df_test[c] = pd.get_dummies(df_test[c])\n    \n# drop one-hot-encoded features     \ndf_train.drop(columns=col_dummies, inplace = True)  \ndf_test.drop(columns=col_dummies, inplace = True) ","aae276be":"# after optimization\ndf_train.info()\ndf_test.info()","60867b94":"# free memory\ndel df_train_id, df_train_trans, df_test_id, df_test_trans\ngc.collect()","d2f4f03e":"# we have extremely imbalanced data. isFraud: 0 (96,5%) \/ 1 (3,5%)\n# we have to make sure, that we have around the same split of classes in training and validation sets:\n\ndef splitData():\n    \n\n    # 1. split training data into classes (isFraud = 1\/0)\n    df_train_neg = df_train.loc[df_train['isFraud'] == 0]\n    df_train_pos = df_train.loc[df_train['isFraud'] == 1]\n    \n    # 2. split the classes into training and validation sets\n    split = 0.2\n    x_train_pos, x_val_pos = train_test_split(df_train_pos, test_size=split)\n    x_train_neg, x_val_neg = train_test_split(df_train_neg, test_size=split)\n\n# #     # 3. upsample minoriry class (isFalse = 1) to achieve 1:1 class distribution\n#     x_train_pos = pd.concat([x_train_pos]*27)\n\n#     # 3. downsample majority (isFraud = 0) to achieve 1:1 class distribution\n    x_train_neg = x_train_neg.sample(frac=1\/27) # trainings data\n    x_val_neg = x_val_neg.sample(frac=1\/27) # validation data (just for model fine tuning, in later versions it will be removed to use whole validation data)  \n\n    # 4. combine and reshuffle training and validation sets\n    x_train = (x_train_pos.append(x_train_neg)).sample(frac=1)\n    x_val = (x_val_pos.append(x_val_neg)).sample(frac=1)\n\n    # 5. define target values\n    y_train = x_train.pop('isFraud')\n    y_val = x_val.pop('isFraud')\n    \n    return x_train, x_val, y_train, y_val","d8e9f2b8":"def prepTrainData():\n    \n    x_train, x_val, y_train, y_val = splitData()  \n\n    # convert pandas.DataFrames to TensorFlow.Datasets to be used in DL-models\n    bs = 512    #batch size\n    sh = bs*50    #shuffle buffer\n\n    ds_train = tf.data.Dataset.from_tensor_slices((dict(x_train), y_train))\n    ds_train = ds_train.shuffle(sh)\n    ds_train = ds_train.batch(bs)\n#     ds_train = ds_train.prefetch(buffer_size=1)          # turned this off to save memory\n    \n    ds_val = tf.data.Dataset.from_tensor_slices((dict(x_val), y_val))\n    ds_val = ds_val.batch(bs)\n#     ds_val = ds_val.prefetch(buffer_size=1)              # turned this off to save memory\n    \n    return ds_train, ds_val, x_train, x_val, y_train, y_val","ead69786":"def featureColumns(df):\n    feature_columns = []\n    \n    for c in df.columns:\n        cat = feature_column.categorical_column_with_hash_bucket(\n            key=str(c),\n            hash_bucket_size=df[str(c)].nunique(),\n            dtype=tf.dtypes.int16)\n        \n        col = feature_column.embedding_column(\n            categorical_column=cat,\n            dimension=int(max(5, (df[str(c)].nunique())**0.5))) # embeddings dimension: max(5, sqrt of unique values)\n        \n        feature_columns.append(col)\n        \n    return feature_columns","5d7a4e83":"def modelGen(feature_columns):\n    model = Sequential([\n        DenseFeatures(feature_columns),\n        Dense(64, activation='relu'),\n        Dropout(rate=0.4),\n        Dense(32, activation='relu'),\n        Dropout(rate=0.4),\n        Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","41f0489d":"def loopTrain(n_models):\n    \n    predictions = pd.DataFrame()\n    \n    for i in range(0, n_models):\n        gc.collect()     # free memory \n        \n        # prepare data\n        ds_train, ds_val, x_train, x_val, y_train, y_val = prepTrainData()\n        \n#         # calculate classes distibution\n#         clw = compute_class_weight('balanced',\n#                                    np.unique(y_train),\n#                                    y_train)\n        \n#         print('class weights: ' + str(clw))\n        \n        # create fearure columns\n        feature_columns = featureColumns(x_train)\n        \n        # generat new model\n        m = modelGen(feature_columns)\n        m.optimizer.lr = 1e-2\n        \n        # training\n        m.fit(ds_train,\n              validation_data=ds_val,\n#               class_weight = {0: clw[0], 1: clw[1]}, # balance the target class\n              verbose = 1,\n              epochs = 3\n              )\n        \n        # additional metrics\n        pred_class, pred_prob = metricsSklearn(x_val, y_val, m)    # additional metrics to check model prediction power\n        \n        # predict probability and save results\n        predictions[str(i+1)] = pred_prob[:,0]\n        \n        print('Model ' + str(i+1) + ' finished training. \\n')\n        \n    predictions['isFraud'] = predictions.mean(axis=1)    # mean of all predictions\n                \n    return predictions","7841cfa9":"# predict on validation set to get ROC-AUC and check how model predicts different classes \n\ndef metricsSklearn(x_val, y_val, m):\n    \n    # convert pandas Dataframe to list\n    test_data = []\n    for c in x_val.columns:\n        test_data.append(x_val[c])\n    \n    pred_class = m.predict_classes(test_data,\n                                   batch_size=8192,\n                                   verbose = 1)\n    \n    print('true classes distribution: \\n'+ str(y_val.value_counts()))\n    \n    print('predicted classes distribution: \\n' + str(pd.Series(pred_class[:,0]).value_counts()))\n    \n    pred_prob = m.predict(test_data,\n                                 batch_size=8192,\n                                 verbose = 1)\n    \n    print(classification_report(y_val, pred_class))\n    print('ROC-AUC: ' + str(roc_auc_score(y_val, pred_prob)))\n    \n    return pred_class, pred_prob","b50966d6":"n_models = 5\n\npredictions=loopTrain(n_models)","5d2ac31c":"predictions","5aef9e32":"### MISSING DATA\nOur combined datasets have 433 features (plus the target value). Some of the features have too many missing values (up to 99%):\n- drop low quality features\n- encode missing data with '-1'","48a6852d":"The results are still very inconsistent and make no sense. Even with relatively high accuracy (for perfectly balanced classes), the model predicts poorly. What is even more strange - the very same model setup sometimes have troubles predicting isFalse=0 and then in another loop isFalse=1. Any advice on how to tune the model?","f5e3c6df":"## TENSORFLOW","2d212d4c":"#### TRAINING","dc19fe1d":"### DATA CLEANING\n\nReading CSVs into pandas DataFrame sometimes result in data type mismatch that needs to be corrected. As a bonus, correcting the data types significantly reduce the DataFrame size. In our case we achieve a reduction of 75%.<br\/><br\/>\nTrain dataset: from 815+ MB to 206+ MB.<br\/>\nTest dataset: from 695+ MB to 173+ MB.<br\/><br\/>","2bf543a3":"#### MODEL GENERATOR","995a849d":"### Your ideas are welcome! Please kindly upvote to get more attention. Thanks!","ec12f0e8":"### Your ideas are welcome! Please kindly upvote to get more attention. Thanks!","68b7079a":"## DATA","cb8769ae":"### LOAD \/ MERGE DATA","e2c7874f":"#### FEATURE COLUMNS","92119e1b":"## Tensorflow 2.0 kernel","0bc69a5f":"Everybody (including me) here played with very effective XGB and LGB kernels to solve IEEE-CIS Fraud Detection.\n\nNow, I want to try a new approach and need some help with this Tensorflow 2.0 kernel. I know it's not optimal for such problems, but I still want to try out deep learning for that.\n\nWe have two very imbalanced classes (ratio 1:27). What I tried so far:\n1. downsample the major class \n2. upsample minor class\n3. leave the classes as they are and tune the loss in Keras (specify the class weights) \n\nAll 3 approaches give relative same results: high accuracy on training and validation set, but then a very low ROC score. To eliminate the possible effect of imbalanced classes, (in this version here) I not only balanced the training set but also the validation set (class distribution for both data sets is 1:1). \n\nPROBLEM: The results are still very inconsistent and make no sense. Even with relatively high accuracy (for perfectly balanced classes), the model predicts poorly. What is even more strange - the very same model setup sometimes have troubles predicting isFalse=0 and then in another loop isFalse=1. Any advice on how to tune the model?","f9ba8030":"#### one-hot-encoding"}}