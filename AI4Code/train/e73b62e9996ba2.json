{"cell_type":{"6e543ac1":"code","2d8247e9":"code","4604acc5":"code","6385be00":"code","916bbb5a":"code","678c6d90":"code","063d7680":"code","2d2118e2":"code","41a10b82":"code","8dd33643":"code","4b5981da":"code","7423e9a8":"code","c52e1ca5":"code","2890f69e":"code","e8decb5e":"code","3237a58d":"code","27572751":"code","b3e529a8":"markdown"},"source":{"6e543ac1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d8247e9":"import random\nimport warnings\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nwarnings.filterwarnings(\"ignore\")\n","4604acc5":"seed=47","6385be00":"def evaluate_model(model, x, y):\n    y_pred_prob = model.predict(x)\n    acc = accuracy_score(y, y_pred_prob)\n    return {'accuracy' : acc}","916bbb5a":"import numpy as np\n\ndef split_sequences(sequences, n_steps):\n\tX, y = list(), list()\n\tfor i in range(len(sequences)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps\n\t\t# check if we are beyond the dataset\n\t\tif end_ix > len(sequences):\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn np.array(X), np.array(y)","678c6d90":"import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\n\n# source https:\/\/keras.io\/examples\/audio\/transformer_asr\/\n\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1, activation=\"selu\"):\n        super().__init__()\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=activation),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(dropout_rate)\n        self.dropout2 = layers.Dropout(dropout_rate)\n\n    def call(self, inputs, training):\n        attn_output = self.attn(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\nclass Transformer(keras.Model):\n    def __init__(\n            self,\n            num_hid=64,  # embed_dim - num of features\n            time_steps=1,\n            nb_classes=7,\n            num_head=2,\n            num_feed_forward=128,  # pointwise dim\n            num_layers_enc=4,\n            dropout_rate=0.1,\n            activation=\"relu\"\n    ):\n        super().__init__()\n        self.numlayers_enc = num_layers_enc\n        self.enc_input = layers.Input((time_steps, num_hid))\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward, dropout_rate, activation)\n                for _ in range(num_layers_enc)\n            ]\n        )\n        self.GlobalAveragePooling1D = layers.GlobalAveragePooling1D(data_format='channels_last')\n        self.out = layers.Dense(units=nb_classes, activation='softmax')\n\n    def call(self, inputs):\n        #x =  Time2Vector(x.shape[-1])\n        x = self.encoder(inputs)\n        x = self.GlobalAveragePooling1D(x)\n        y = self.out(x)\n        return y","063d7680":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv', sep=',')","2d2118e2":"x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type']\ny_train = y_train.apply(lambda x : x - 1)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)","41a10b82":"# https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293612\n\ndef r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\ndef fe(df):\n    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n    df['Aspect2'] = df.Aspect.map(r)\n    ### source: https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    ########\n    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n    df['Euclidean_Distance_to_Hydrolody'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df['Manhattan_Distance_to_Hydrolody'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n    return df","8dd33643":"x_train = fe(x_train)\nx_test = fe(x_test)\n\n# Summed features pointed out by @craigmthomas (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823)\nsoil_features = [x for x in x_train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in x_train.columns if x.startswith(\"Wilderness_Area\")]\n\nx_train[\"soil_type_count\"] = x_train[soil_features].sum(axis=1)\nx_test[\"soil_type_count\"] = x_test[soil_features].sum(axis=1)\n\nx_train[\"wilderness_area_count\"] = x_train[wilderness_features].sum(axis=1)\nx_test[\"wilderness_area_count\"] = x_test[wilderness_features].sum(axis=1)\n\nx_train['std'] = np.std(x_train, axis=1)\nx_test['std'] = np.std(x_test, axis=1)","4b5981da":"sc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\nx_train = x_train[:, np.newaxis, :]\nx_test = x_test[:, np.newaxis, :]\nnb_classes = train_df['Cover_Type'].nunique()\ntime_steps = 1\nnum_features = x_train.shape[-1]","7423e9a8":"num_heads=2\nnum_layers_enc=1\nnum_feed_forward=64\n\nmodel = Transformer(num_hid=num_features,\n                        time_steps=time_steps,\n                        nb_classes=nb_classes,\n                        num_head=num_heads,\n                        num_layers_enc=num_layers_enc,\n                        num_feed_forward=num_feed_forward)\n\nopt = tf.keras.optimizers.Adam()\nloss = tf.keras.losses.SparseCategoricalCrossentropy()\nmodel.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=400, batch_size=1024, verbose=1)\nprint()\nresults = model.evaluate(x_test, y_test)[1]\nprint(results)","c52e1ca5":"del train_df, x_train, y_train, x_test, y_test\ngc.collect()","2890f69e":"test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv', sep=',')\nx_test = test_df.drop(['Id', 'Soil_Type7','Soil_Type15'], axis=1)\nx_test = fe(x_test)\nx_test[\"soil_type_count\"] = x_test[soil_features].sum(axis=1)\nx_test[\"wilderness_area_count\"] = x_test[wilderness_features].sum(axis=1)\nx_test['std'] = np.std(x_test, axis=1)\nx_test = sc.transform(x_test)\nx_test = x_test[:, np.newaxis, :]","e8decb5e":"target = model.predict(x_test)\nclass_preds = np.argmax(target, axis=-1) + 1\nids = test_df['Id'].values\nsubmission = pd.DataFrame({'Id' : ids, 'Cover_Type' : class_preds})","3237a58d":"submission.head()","27572751":"submission.to_csv('submission.csv', index=False)","b3e529a8":"# Submission"}}