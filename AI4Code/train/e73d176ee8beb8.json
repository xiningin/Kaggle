{"cell_type":{"80925665":"code","1a939252":"code","fe281a99":"code","30ef768f":"code","ef30d8b6":"code","f06664e1":"code","8b5ca2b1":"code","26fa0e8b":"code","b1e50c5b":"code","950e095f":"code","0ac0274c":"code","6ec7be2c":"code","e350eef2":"code","32f954c6":"code","95f7cbcc":"code","c3e1914d":"code","247632c0":"code","acf9957d":"code","bf531a24":"code","1fd2d8b5":"code","11759eaf":"code","79a58f0c":"code","8ca97030":"code","b5557acf":"code","e23018bd":"code","40292b32":"code","6cebc818":"code","41e697ff":"code","a5f975a6":"markdown","18dc1ac1":"markdown","5efaa11d":"markdown","86b9cba9":"markdown","43d09140":"markdown","b7d4936b":"markdown","df3a606b":"markdown","8edd5327":"markdown","ce1f6db6":"markdown","ecffcc00":"markdown","ea170188":"markdown","444b01ea":"markdown","f9cf406e":"markdown","9e4cc217":"markdown","cc5ed034":"markdown","abbd3d35":"markdown","f8b73dde":"markdown","fedc614b":"markdown","69fbdebc":"markdown","788594aa":"markdown","e6a28080":"markdown","26977cf9":"markdown","2d33c4ec":"markdown","f3efea8f":"markdown","aff8c86e":"markdown","e4ae9876":"markdown","c245ec99":"markdown","bcf6ed57":"markdown","6c10676e":"markdown"},"source":{"80925665":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport numpy as np\ninit_notebook_mode(connected=True)\n\n## generate random data\nN = 50\nrandom_x = np.linspace(2, 10, N)\nrandom_y1 = np.linspace(2, 10, N)\nrandom_y2 = np.linspace(2, 10, N)\n\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\", name=\"Actual Data\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\", name=\"Model\")\nlayout = go.Layout(title=\"2D Data Repersentation Space\", xaxis=dict(title=\"x2\", range=(0,12)), \n                   yaxis=dict(title=\"x1\", range=(0,12)), height=400, \n                   annotations=[dict(x=5, y=5, xref='x', yref='y', text='This 1D line is the Data Manifold (where data resides)',\n                   showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                   ax=-120, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8)])\nfigure = go.Figure(data = [trace1], layout = layout)\niplot(figure)","1a939252":"random_y3 = [2 for i in range(100)]\nrandom_y4 = random_y2 + 1\ntrace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\ntrace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\nlayout = go.Layout(xaxis=dict(title=\"x1\", range=(0,12)), yaxis=dict(title=\"x2\", range=(0,12)), height=400,\n                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"2D Data Repersentation Space\", showlegend=False)\ndata = [trace1, trace2, trace3, trace4]\nfigure = go.Figure(data = data, layout = layout)\niplot(figure)\n\n\n\n#################\n\nrandom_y3 = [2 for i in range(100)]\nrandom_y4 = random_y2 + 1\ntrace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\ntrace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\nlayout = go.Layout(xaxis=dict(title=\"u1\", range=(1.5,12)), yaxis=dict(title=\"u2\", range=(1.5,12)), height=400,\n                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"Latent Distance View Space\", showlegend=False)\ndata = [trace1, trace2, trace3, trace4]\nfigure = go.Figure(data = data, layout = layout)\niplot(figure)","fe281a99":"import matplotlib.pyplot as plt \nimport numpy as np\nfs = 100 # sample rate \nf = 2 # the frequency of the signal\nx = np.arange(fs) # the points on the x axis for plotting\ny = [ np.sin(2*np.pi*f * (i\/fs)) for i in x]\n\n% matplotlib inline\nplt.figure(figsize=(15,4))\nplt.stem(x,y, 'r', );\nplt.plot(x,y);","30ef768f":"## load the libraries \nfrom keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom numpy import argmax, array_equal\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom imgaug import augmenters\nfrom random import randint\nimport pandas as pd\nimport numpy as np","ef30d8b6":"### read dataset \ntrain = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\ntrain_x = train[list(train.columns)[1:]].values\ntrain_y = train['label'].values\n\n## normalize and reshape the predictors  \ntrain_x = train_x \/ 255\n\n## create train and validation datasets\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n\n## reshape the inputs\ntrain_x = train_x.reshape(-1, 784)\nval_x = val_x.reshape(-1, 784)","f06664e1":"## input layer\ninput_layer = Input(shape=(784,))\n\n## encoding architecture\nencode_layer1 = Dense(1500, activation='relu')(input_layer)\nencode_layer2 = Dense(1000, activation='relu')(encode_layer1)\nencode_layer3 = Dense(500, activation='relu')(encode_layer2)\n\n## latent view\nlatent_view   = Dense(10, activation='sigmoid')(encode_layer3)\n\n## decoding architecture\ndecode_layer1 = Dense(500, activation='relu')(latent_view)\ndecode_layer2 = Dense(1000, activation='relu')(decode_layer1)\ndecode_layer3 = Dense(1500, activation='relu')(decode_layer2)\n\n## output layer\noutput_layer  = Dense(784)(decode_layer3)\n\nmodel = Model(input_layer, output_layer)","8b5ca2b1":"model.summary()","26fa0e8b":"model.compile(optimizer='adam', loss='mse')\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\nmodel.fit(train_x, train_x, epochs=20, batch_size=2048, validation_data=(val_x, val_x), callbacks=[early_stopping])","b1e50c5b":"preds = model.predict(val_x)","950e095f":"from PIL import Image \nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(val_x[i].reshape(28, 28))\nplt.show()","0ac0274c":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(preds[i].reshape(28, 28))\nplt.show()","6ec7be2c":"## recreate the train_x array and val_x array\ntrain_x = train[list(train.columns)[1:]].values\ntrain_x, val_x = train_test_split(train_x, test_size=0.2)\n\n## normalize and reshape\ntrain_x = train_x\/255.\nval_x = val_x\/255.","e350eef2":"train_x = train_x.reshape(-1, 28, 28, 1)\nval_x = val_x.reshape(-1, 28, 28, 1)","32f954c6":"# Lets add sample noise - Salt and Pepper\nnoise = augmenters.SaltAndPepper(0.1)\nseq_object = augmenters.Sequential([noise])\n\ntrain_x_n = seq_object.augment_images(train_x * 255) \/ 255\nval_x_n = seq_object.augment_images(val_x * 255) \/ 255","95f7cbcc":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x[i].reshape(28, 28))\nplt.show()","c3e1914d":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x_n[i].reshape(28, 28))\nplt.show()","247632c0":"# input layer\ninput_layer = Input(shape=(28, 28, 1))\n\n# encoding architecture\nencoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\nencoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\nencoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\nencoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\nencoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\nlatent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n\n# decoding architecture\ndecoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)\ndecoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\ndecoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\ndecoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\ndecoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\ndecoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\noutput_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n\n# compile the model\nmodel_2 = Model(input_layer, output_layer)\nmodel_2.compile(optimizer='adam', loss='mse')","acf9957d":"model_2.summary()","bf531a24":"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\nhistory = model_2.fit(train_x_n, train_x, epochs=10, batch_size=2048, validation_data=(val_x_n, val_x), callbacks=[early_stopping])","1fd2d8b5":"preds = model_2.predict(val_x_n[:10])\nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(preds[i].reshape(28, 28))\nplt.show()","11759eaf":"def dataset_preparation(n_in, n_out, n_unique, n_samples):\n    X1, X2, y = [], [], []\n    for _ in range(n_samples):\n        ## create random numbers sequence - input \n        inp_seq = [randint(1, n_unique-1) for _ in range(n_in)]\n        \n        ## create target sequence\n        target = inp_seq[:n_out]\n    \n        ## create padded sequence \/ seed sequence \n        target_seq = list(reversed(target))\n        seed_seq = [0] + target_seq[:-1]  \n        \n        # convert the elements to categorical using keras api\n        X1.append(to_categorical([inp_seq], num_classes=n_unique))\n        X2.append(to_categorical([seed_seq], num_classes=n_unique))\n        y.append(to_categorical([target_seq], num_classes=n_unique))\n    \n    # remove unnecessary dimention\n    X1 = np.squeeze(np.array(X1), axis=1) \n    X2 = np.squeeze(np.array(X2), axis=1) \n    y  = np.squeeze(np.array(y), axis=1) \n    return X1, X2, y\n\nsamples = 100000\nfeatures = 51\ninp_size = 6\nout_size = 3\n\ninputs, seeds, outputs = dataset_preparation(inp_size, out_size, features, samples)\nprint(\"Shapes: \", inputs.shape, seeds.shape, outputs.shape)\nprint (\"Here is first categorically encoded input sequence looks like: \", )\ninputs[0][0]","79a58f0c":"def define_models(n_input, n_output):\n    ## define the encoder architecture \n    ## input : sequence \n    ## output : encoder states \n    encoder_inputs = Input(shape=(None, n_input))\n    encoder = LSTM(128, return_state=True)\n    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n    encoder_states = [state_h, state_c]\n\n    ## define the encoder-decoder architecture \n    ## input : a seed sequence \n    ## output : decoder states, decoded output \n    decoder_inputs = Input(shape=(None, n_output))\n    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n    decoder_dense = Dense(n_output, activation='softmax')\n    decoder_outputs = decoder_dense(decoder_outputs)\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n    \n    ## define the decoder model\n    ## input : current states + encoded sequence\n    ## output : decoded sequence\n    encoder_model = Model(encoder_inputs, encoder_states)\n    decoder_state_input_h = Input(shape=(128,))\n    decoder_state_input_c = Input(shape=(128,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n\n    return model, encoder_model, decoder_model\n\nautoencoder, encoder_model, decoder_model = define_models(features, features)","8ca97030":"encoder_model.summary()","b5557acf":"decoder_model.summary()","e23018bd":"autoencoder.summary()","40292b32":"autoencoder.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nautoencoder.fit([inputs, seeds], outputs, epochs=1)","6cebc818":"def reverse_onehot(encoded_seq):\n    return [argmax(vector) for vector in encoded_seq]\n\ndef predict_sequence(encoder, decoder, sequence):\n    output = []\n    target_seq = np.array([0.0 for _ in range(features)])\n    target_seq = target_seq.reshape(1, 1, features)\n\n    current_state = encoder.predict(sequence)\n    for t in range(out_size):\n        pred, h, c = decoder.predict([target_seq] + current_state)\n        output.append(pred[0, 0, :])\n        current_state = [h, c]\n        target_seq = pred\n    return np.array(output)","41e697ff":"for k in range(5):\n    X1, X2, y = dataset_preparation(inp_size, out_size, features, 1)\n    target = predict_sequence(encoder_model, decoder_model, X1)\n    print('\\nInput Sequence=%s SeedSequence=%s, PredictedSequence=%s' \n          % (reverse_onehot(X1[0]), reverse_onehot(y[0]), reverse_onehot(target)))","a5f975a6":"noise\ub97c \ucd94\uac00\ud558\uba74,","18dc1ac1":"\ub9cc\ub4e0 model\uc758 \uc694\uc57d\uc785\ub2c8\ub2e4.","5efaa11d":"\uac80\uc99d \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc608\uce21\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.","86b9cba9":"## (KOR ver.)How Autoencoders work - Understanding the math and implementation\n\n### Contents \n\n<ul>\n<li>1. Introduction<\/li>\n<ul>\n    <li>1.1 What are Autoencoders ? <\/li>\n    <li>1.2 How Autoencoders Work ? <\/li>\n<\/ul>\n<li>2. Implementation and UseCases<\/li>\n<ul>\n    <li>2.1 UseCase 1: Image Reconstruction <\/li>\n    <li>2.2 UseCase 2: Noise Removal <\/li>\n    <li>2.3 UseCase 3: Sequence to Sequence Prediction <\/li>\n<\/ul>\n<\/ul>\n\n<br>\n\n## 1. Introduction\n## 1.1 What are Autoencoders \n\nAutoencoders\ub294 output\uc774 input\uacfc \ub3d9\uc77c\ud55c \ud2b9\uc218\ud55c \uc720\ud615\uc758 neural network architectures\uc785\ub2c8\ub2e4. Autoencoders\ub294 input data\uc758 low level\uc758 representations\uc744 \ud559\uc2b5\ud558\uae30 \uc704\ud574 un-supervised \ubc29\uc2dd\uc73c\ub85c \ud6c8\ub828\ub429\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \uc774\ub7ec\ud55c low level features\ub294 \uc2e4\uc81c \ub370\uc774\ud130\ub97c \ud22c\uc601\ud558\uae30 \uc704\ud574 \ub2e4\uc2dc \ubcc0\ud615\ub429\ub2c8\ub2e4. Autoencoders\ub294 \ub124\ud2b8\uc6cc\ud06c\uac00 \uc785\ub825\uc744 \uc608\uce21\ud558\ub3c4\ub85d \uc694\uccad\ud558\ub294 regression task\uc785\ub2c8\ub2e4 (\uc989, identity unction \ubaa8\ub378\ub9c1). \uc774\ub7ec\ud55c \ub124\ud2b8\uc6cc\ud06c\ub294 \uc911\uac04\uc5d0 \uba87 \uac1c\uc758 \ub274\ub7f0\uc758 \ubcd1\ubaa9 \ud604\uc0c1(bottleneck)\uc774 \uc2ec\ud558\uae30 \ub54c\ubb38\uc5d0 \uc785\ub825\uc744 \ub514\ucf54\ub354\uac00 \uc6d0\ub798 \uc785\ub825\uc744 \uc7ac\ud604\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 low-dimensional \ucf54\ub4dc\ub85c \uc555\ucd95\ud558\ub294 \ud6a8\uacfc\uc801\uc778 \ud45c\ud604\uc744 \uc0dd\uc131\ud574\uc57c\ud569\ub2c8\ub2e4.\n\n\uc804\ud615\uc801\uc778 autoencoder \uad6c\uc870\ub294 3\uac1c\uc758 \uc8fc\uc694 \uc694\uc18c\ub85c \uad6c\uc131:\n\n- **Encoding Architecture :** Encoding \uad6c\uc870\ub294 \ub178\ub4dc(node) \uc218\uac00 \uac10\uc18c\ud558\ub294 \uc77c\ub828\uc758 \uacc4\uce35(layers)\uc73c\ub85c \uad6c\uc131\ub418\uba70 \uad81\uadf9\uc801\uc73c\ub85c latent view representation\uc73c\ub85c \ucd95\uc18c\ub429\ub2c8\ub2e4.\n- **Latent View Repersentation :** Latent view\ub294 \uc785\ub825\uc774 \uac10\uc18c\ud558\uace0 \uc815\ubcf4\uac00 \ubcf4\uc874\ub418\ub294 \uac00\uc7a5 \ub0ae\uc740 \uc218\uc900\uc758 \uacf5\uac04\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n- **Decoding Architecture :** Decoding \uad6c\uc870\ub294 \uc778\ucf54\ub529 \uad6c\uc870\uc640 \uac70\uc6b8\uc5d0 \ubc18\uc0ac\ub41c \ub4ef\ud55c \ubaa8\uc2b5\uc744 \ub744\uc9c0\ub9cc \ubaa8\ub4e0 \ub808\uc774\uc5b4\uc758 \ub178\ub4dc \uc218\uac00 \uc99d\uac00\ud558\uc5ec \uad81\uadf9\uc801\uc73c\ub85c \uc720\uc0ac\ud55c (\uac70\uc758) \uc785\ub825\uc744 \ucd9c\ub825\ud569\ub2c8\ub2e4.  \n\n![](https:\/\/i.imgur.com\/Rrmaise.png)\n\n\uace0\ub3c4\ub85c fine tunning \ub41c autoencoder \ubaa8\ub378\uc740 \uccab \ubc88\uc9f8 \uacc4\uce35(layer)\uc5d0\uc11c \uc804\ub2ec \ub41c \ub3d9\uc77c\ud55c \uc785\ub825\uc744 \uc7ac\uad6c\uc131 \ud560 \uc218 \uc788\uc5b4\uc57c\ud569\ub2c8\ub2e4. \uc774 kernel\uc5d0\uc11c\ub294 autoencoder\uc758 \uc791\ub3d9\uacfc \uadf8 \uad6c\ud604\uc5d0 \ub300\ud574 \uc124\uba85\ud558\uaca0\uc2b5\ub2c8\ub2e4. Autoencoder\ub294 \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uc640 \ud568\uaed8 \uad11\ubc94\uc704\ud558\uac8c \uc0ac\uc6a9\ub418\uba70 \uc77c\ubd80 \uc0ac\uc6a9 \uc0ac\ub840\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\n- Dimentionality Reduction(\ucc28\uc6d0 \ucd95\uc18c)\n- Image Compression(\uc774\ubbf8\uc9c0 \uc555\ucd95)   \n- Image Denoising(\uc774\ubbf8\uc9c0 \ub178\uc774\uc988 \uc81c\uac70)   \n- Image Generation(\uc774\ubbf8\uc9c0 \uc0dd\uc131)\n- Feature Extraction(feature \ucd94\ucd9c)  \n\n## 1.2 How Autoencoders work \n\n\uba3c\uc800 usecase\ub97c \uc124\uba85\ud558\uae30\uc5d0 \uc55e\uc11c autoencoder\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uc218\ud559\uc744 \uc774\ud574 \ud574\uc57c \ud569\ub2c8\ub2e4. Autoencoder\uc758 \uae30\ubcf8 \uc544\uc774\ub514\uc5b4\ub294 \ub192\uc740 \uc218\uc900\uc758 \ucc28\uc6d0 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ub0ae\uc740 \uc218\uc900\uc758 \uc7ac\ud604(representation)\uc744 \ud559\uc2b5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, encoding \uc808\ucc28\ub97c \uc774\ud574\ud574\ubd05\uc2dc\ub2e4. Data representation space(\ub370\uc774\ud130 \ud45c\uc2dc \uacf5\uac04)(\ub370\uc774\ud130\ub97c \ud45c\uc2dc\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 N \ucc28\uc6d0 \uacf5\uac04)\uc744 \uace0\ub824\ud558\uace0 x1\uacfc x2\uc758 \ub450 \ubcc0\uc218\uc5d0 \uc758\ud574 \ud45c\uc2dc\ub418\ub294 \ub370\uc774\ud130 \ud3ec\uc778\ud2b8\ub97c \uc0dd\uac01\ud574\ubd05\uc2dc\ub2e4. Data manifold\ub294 \uc2e4\uc81c \ub370\uc774\ud130\uac00 \uc874\uc7ac\ud558\ub294 data representation spce \ub0b4\ubd80\uc758 \uacf5\uac04\uc785\ub2c8\ub2e4.","43d09140":"\uc774 \uad6c\ud604\uc5d0\uc11c\ub294 10 epoch \ub9cc \ud559\uc2b5\uc744 \uc2dc\ucf30\uc9c0\ub9cc, \ub354 \ub9ce\uc740 epoch\uc5d0 \ub300\ud574 \uc774 \ub124\ud2b8\uc6cc\ud06c\ub97c \ud559\uc2b5\ud55c\ub2e4\uba74 \ub354 \ub098\uc740 \uc608\uce21 \uacb0\uacfc\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (e.g. 500 ~ 1000 epochs)\n\n\n## 2.3 UseCase 3: Sequence to Sequence Prediction using AutoEncoders\n\n\n\ub2e4\uc74c usecase\ub294 sequence \ub300 sequence \uc608\uce21\uc785\ub2c8\ub2e4. \uc774\uc804 \uc608\uc81c\uc5d0\uc11c\ub294 \uae30\ubcf8\uc801\uc73c\ub85c 2 \ucc28\uc6d0 \ub370\uc774\ud130\uc778 \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774\ubc88 \uc608\uc81c\uc5d0\uc11c\ub294 1 \ucc28\uc6d0\uc778 sequence \ub370\uc774\ud130\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Sequence \ub370\uc774\ud130\uc758 \uc608\ub85c\ub294 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc640 \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774 usecase\ub294 \uae30\uacc4 \ubc88\uc5ed(Machine translation)\uc5d0 \uc801\uc6a9\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \uc608\uc81c\uc758 CNN\uacfc \ub2ec\ub9ac \uc774 uscase\uc5d0\uc11c\ub294 LSTM\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\n\uc774 \uc139\uc158\uc758 \ucf54\ub4dc \ub300\ubd80\ubd84\uc740 Jason Brownie\uac00 \uc6b4\uc601\ud558\ub294 \ube14\ub85c\uadf8 \uac8c\uc2dc\ubb3c\uc744 \ucc38\uc870\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n- Reference : https:\/\/machinelearningmastery.com\/develop-encoder-decoder-model-sequence-sequence-prediction-keras\/\n\n#### Autoencoder Architecture  \n\n\uc774 usecase\uc758 \uad6c\uc870\ub294 \uc18c\uc2a4(source) sequence\ub97c \uc778\ucf54\ub529\ud558\ub294 \uc778\ucf54\ub354\uc640 \uc778\ucf54\ub529 \ub41c \uc18c\uc2a4 sequence\ub97c \ub514\ucf54\ub529\ud558\ub294 \ub514\ucf54\ub354\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uba3c\uc800 \uc774 \uad6c\uc870\uc5d0\uc11c \uc0ac\uc6a9\ub420 LSTM\uc758 \ub0b4\ubd80 precess\ub97c \uc774\ud574\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n- Long Short-Term Memory, or LSTM, \uc740 \ub0b4\ubd80 \uac8c\uc774\ud2b8(gate)\ub85c \uad6c\uc131\ub41c \uc21c\ud658 \uc2e0\uacbd\ub9dd\uc785\ub2c8\ub2e4. \n- \ub2e4\ub978 \uc21c\ud658 \uc2e0\uacbd\ub9dd\uacfc \ub2ec\ub9ac \ub124\ud2b8\uc6cc\ud06c\uc758 \ub0b4\ubd80 \uac8c\uc774\ud2b8\ub97c \uc0ac\uc6a9\ud558\uba74 \uc2dc\uac04\uc5d0 \ub530\ub978 \uc5ed\uc804\ud30c \ub610\ub294 BPTT\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \uc131\uacf5\uc801\uc73c\ub85c \ud559\uc2b5\uc2dc\ud0a4\uace0 \uae30\uc6b8\uae30 \uc18c\uc2e4 \ubb38\uc81c(vanishing gradients problem)\ub97c \ubc29\uc9c0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- LSTM \uacc4\uce35\uc5d0\uc11c LSTM \uba54\ubaa8\ub9ac \ub2e8\uc704\uc758 \uc218\ub97c \uc815\uc758 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uacc4\uce35 \ub0b4\uc758 \uac01 \ub2e8\uc704 \ub610\ub294 \uc140\uc740 \ub0b4\ubd80 \uba54\ubaa8\ub9ac\/\uc140 \uc0c1\ud0dc(c)\ub97c \uac00\uc9c0\uba70 \uc885\uc885 h\ub77c\ub294 \ud45c\ud604\uc744 \uc0ac\uc6a9\ud558\ub294 \uc228\uaca8\uc9c4 \uc0c1\ud0dc\ub97c \ucd9c\ub825\ud569\ub2c8\ub2e4.\n- Keras\ub97c \uc0ac\uc6a9\ud558\uba74 LSTM \uacc4\uce35\uc758 \ucd9c\ub825 \uc0c1\ud0dc\uc640 LSTM \uacc4\uce35\uc758 \ud604\uc7ac \uc0c1\ud0dc\uc5d0 \ubaa8\ub450 \uc561\uc138\uc2a4 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774\uc81c LSTM \ub808\uc774\uc5b4\ub85c \uad6c\uc131\ub41c sequence\ub97c \ud559\uc2b5\ud558\uace0 \uc0dd\uc131\ud558\uae30 \uc704\ud55c autoencoder \uad6c\uc870\ub97c \uc0dd\uc131\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub450 \uac00\uc9c0 \uad6c\uc131 \uc694\uc18c\uac00 \uc874\uc7ac\ud569\ub2c8\ub2e4. \n\n- Sequence\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uace0 LSTM\uc758 \ud604\uc7ac \uc0c1\ud0dc\ub97c \ucd9c\ub825\uc73c\ub85c \ubc18\ud658\ud558\ub294 \uc778\ucf54\ub354 \uad6c\uc870  \n- Sequence \ubc0f \uc778\ucf54\ub354 LSTM \uc0c1\ud0dc\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uace0 \ub514\ucf54\ub529 \ub41c \ucd9c\ub825 sequence\ub97c \ubc18\ud658\ud558\ub294 \ub514\ucf54\ub354 \uad6c\uc870\n- \ubcf4\uc774\uc9c0 \uc54a\ub294 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc608\uce21\uc744 \uc0dd\uc131\ud558\ub294 \ub3d9\uc548 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d LSTM\uc758 \uc228\uae40 \ubc0f \uba54\ubaa8\ub9ac \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud558\uace0 \uc561\uc138\uc2a4\ud569\ub2c8\ub2e4. \n\n\uba3c\uc800 \uace0\uc815 \uae38\uc774\uc758 \ubb34\uc791\uc704 sequence\ub97c \ud3ec\ud568\ud558\ub294 sequence \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc0dd\uc131\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubb34\uc791\uc704 sequence\ub97c \uc0dd\uc131\ud558\ub294 \ud568\uc218\ub97c \uc815\uc758\ud560 \uac83\uc785\ub2c8\ub2e4.\n\n- X1\uc740 \ub09c\uc218\ub97c \ud3ec\ud568\ud558\ub294 \uc785\ub825 sequence\ub97c \ud45c\uc2dc\ud569\ub2c8\ub2e4.\n- X2\ub294 sequence\uc758 \ub2e4\ub978 \uc694\uc18c\ub97c \uc7ac\ud604\ud558\uae30 \uc704\ud574 \uc2dc\ub4dc(seed)\ub85c \uc0ac\uc6a9\ub418\ub294 \ud328\ub529(padding) \ub41c sequence\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n- y\ub294 \ud0c0\uac9f sequence \ub610\ub294 \uc2e4\uc81c sequence\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \n","b7d4936b":"\ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc608\uce21\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","df3a606b":"Input sequence \uae30\ubc18\uc73c\ub85c sequence\ub97c \uc608\uce21\ud558\ub294 \ud568\uc218\ub97c \uc791\uc131\ud574\ubcf4\uc790.","8edd5327":"\uadf8\ub7ec\ub098 \uc5ec\uae30\uc11c \ud575\uc2ec\uc801\uc778 \uc9c8\ubb38\uc740 \uc5b4\ub5a4 \ub17c\ub9ac \ub610\ub294 \uc5b4\ub5a4 \uaddc\uce59\uc73c\ub85c, \uc810 B\ub294 A\uc640 \uac01\ub3c4 L\ub85c \ud45c\ud604 \ub420 \uc218 \uc788\ub294\uc9c0\uc785\ub2c8\ub2e4. \ub610\ub294 \ub2e4\ub978 \uc6a9\uc5b4\ub85c B, A \ubc0f L \uc0ac\uc774\uc758 \ubc29\uc815\uc2dd\uc740 \uc5b4\ub5bb\uac8c \ud45c\ud604\ub429\ub2c8\uae4c? \ub2f5\uc740 \"\uc815\ud574\uc9c4 \ubc29\uc815\uc2dd\uc740 \uc5c6\uc9c0\ub9cc \ube44\uc9c0\ub3c4 \ud559\uc2b5\uacfc\uc815\uc744 \ud1b5\ud574 \ucd5c\uc0c1\uc758 \ubc29\uc815\uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4.\" \uc785\ub2c8\ub2e4. \uac04\ub2e8\ud788 \ub9d0\ud574\uc11c \ud559\uc2b5 \uacfc\uc815\uc740 B\ub97c A\uc640 L\uc758 \ud615\ud0dc\ub85c \ubcc0\ud658\ud558\ub294 \uaddc\uce59\/\ubc29\uc815\uc2dd\uc73c\ub85c \uc815\uc758 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Autoencoder \uad00\uc810\uc5d0\uc11c \uc774 \uacfc\uc815\uc744 \uc774\ud574\ud569\uc2dc\ub2e4.\n\n\uc228\uaca8\uc9c4 \ub808\uc774\uc5b4(hidden layer)\uac00 \uc5c6\ub294 autoencoder\ub97c \uace0\ub824\ud558\uba74 \uc785\ub825 x1 \ubc0f x2\ub294 \ub354 \ub0ae\uc740 repersentation d\ub85c \uc778\ucf54\ub529 \ub41c \ub2e4\uc74c x1 \ubc0f x2\uc5d0 \ucd94\uac00\ub85c \ud22c\uc601\ub429\ub2c8\ub2e4.\n\n![](https:\/\/i.imgur.com\/lfq4eEy.png)\n\n<br>\n**Step1 : Repersent the points in Latent View Space**   \n\n\ub370\uc774\ud130 \ud45c\ud604 \uacf5\uac04\uc5d0\uc11c \uc810 A\uc640 B\uc758 \uc88c\ud45c\uac00 \ub2e4\uc74c\uacfc \uac19\uc740 \uacbd\uc6b0:\n\n- Point A : (x1A, x2A)  \n- Point B : (x1B, x2B)   \n\n\uc7a0\uc7ac \ubdf0 \uacf5\uac04\uc5d0\uc11c\uc758 \uc88c\ud45c\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\n(x1A, x2A) ---> (0, 0)  \n(x1B, x2B) ---> (u1B, u2B)  \n\n- Point A : (0, 0)  \n- Point B : (u1B, u2B)   \n\n\uc5ec\uae30\uc11c u1B\uc640 u2B\ub294 \uc810\uacfc \uae30\uc900\uc810 \uc0ac\uc774\uc758 \uac70\ub9ac \ud615\ud0dc\ub85c \ud45c\ud604 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\nu1B = x1B - x1A  \nu2B = x2B - x2A\n\n**Step2 : Represent the points with distance d and angle L **    \n\n\uc774\uc81c u1B\uc640 u2B\ub294 \uac70\ub9ac d\uc640 \uac01\ub3c4 L\uc758 \uc870\ud569\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc774\uac83\uc744 \uac01\ub3c4 L\ub9cc\ud07c, \uc218\ud3c9\ucd95 \ucabd\uc73c\ub85c \ud68c\uc804\ud558\uba74 L\uc740 0\uc774\ub429\ub2c8\ub2e4. \uc989, \n\n**=> (d, L)**     \n**=> (d, 0)**   (after rotation)   \n\n\uc774\uac83\uc740 \uc778\ucf54\ub529 \ud504\ub85c\uc138\uc2a4\uc758 \ucd9c\ub825\uc774\uba70 \ub370\uc774\ud130\ub97c \ub0ae\uc740 \ucc28\uc6d0\uc73c\ub85c \ud45c\uc2dc\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \uacc4\uce35\uc758 \uac00\uc911\uce58\uc640 \ud3b8\ud5a5\uc744 \uac16\ub294 \uc2e0\uacbd\ub9dd\uc758 \uae30\ubcf8 \ubc29\uc815\uc2dd\uc744 \uc0c1\uae30\ud558\uba74,\n\n**=> (d, 0) = W. (u1B, u2B)**    \n==> (encoding)    \n\n\uc5ec\uae30\uc11c W\ub294 \uc740\ub2c9\uce35\uc758 \uac00\uc911\uce58 \ud589\ub82c\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c \ub514\ucf54\ub529 \ud504\ub85c\uc138\uc2a4\ub294 \uc778\ucf54\ub529 \ud504\ub85c\uc138\uc2a4\uc758 \ubbf8\ub7ec \uc774\ubbf8\uc9c0\ub77c\ub294 \uac83\uc744 \uc54c\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n**=> (u1B, u2B) = Inverse (W) . (d, 0)**    \n==> (decoding)  \n\n\ucd95\uc18c \ub41c \ud615\ud0dc\uc758 \ub370\uc774\ud130 (x1, x2)\ub294 \uc778\ucf54\ub529 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \uc5bb\uc740 \uc7a0\uc7ac \ubdf0 \uacf5\uac04\uc5d0\uc11c (d, 0)\uc785\ub2c8\ub2e4. \ub9c8\ucc2c\uac00\uc9c0\ub85c \ub514\ucf54\ub529 \uad6c\uc870\ub294 \uc774 \ud45c\ud604\uc744 \uc6d0\ub798 \ud615\uc2dd (u1B, u2B)\uc73c\ub85c \ub2e4\uc2dc \ubcc0\ud658 \ud55c \ub2e4\uc74c (x1, x2)\ub85c \ubcc0\ud658\ud569\ub2c8\ub2e4. \uc911\uc694\ud55c \uc810\uc740 \ub370\uc774\ud130 \uc720\ud615\uc5d0 \ub530\ub77c \uaddc\uce59\/\ud559\uc2b5 \uae30\ub2a5\/\uc778\ucf54\ub529 \ub514\ucf54\ub529 \ubc29\uc815\uc2dd\uc774 \ub2e4\ub97c \uc218 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 2 \ucc28\uc6d0 \uacf5\uac04\uc5d0\uc11c \ub2e4\uc74c \ub370\uc774\ud130\ub97c \uace0\ub824\ud574\ubcf4\uc2ed\uc1fc.\n\n\n## Different Rules for Different data\n\n\ubaa8\ub4e0 \uc720\ud615\uc758 \ub370\uc774\ud130\uc5d0 \ub3d9\uc77c\ud55c \uaddc\uce59\uc744 \uc801\uc6a9 \ud560 \uc218\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc774\uc804 \uc608\uc5d0\uc11c \uc120\ud615 \ub370\uc774\ud130 \ub9e4\ub2c8 \ud3f4\ub4dc\ub97c \ud55c \ucc28\uc6d0\uc73c\ub85c \ud22c\uc601\ud558\uace0 \uac01\ub3c4 L\uc744 \uc81c\uac70\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ub370\uc774\ud130 \ub9e4\ub2c8 \ud3f4\ub4dc\ub97c \uc81c\ub300\ub85c \ud22c\uc601 \ud560 \uc218 \uc5c6\ub294 \uacbd\uc6b0\uc5d0\ub294 \uc5b4\ub5bb\uac8c \ud574\uc57c\ud569\ub2c8\uae4c? \uc608\ub97c \ub4e4\uc5b4, \ub2e4\uc74c \ub370\uc774\ud130 \ub9e4\ub2c8 \ud3f4\ub4dc\ubcf4\uae30\ub97c \uace0\ub824\ud558\uc2ed\uc2dc\uc624.","ce1f6db6":"Decoder \uad6c\uc870 \uc694\uc57d","ecffcc00":"\uc774\uc81c autoencoder\ub97c \uc704\ud55c \ubaa8\ub378 \uad6c\uc870\ub97c \uc0dd\uc131\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \ubb38\uc81c\uc5d0 \ub300\ud574 \uc5b4\ub5a4 \uc720\ud615\uc758 \ub124\ud2b8\uc6cc\ud06c\ub97c \ub9cc\ub4e4\uc5b4\uc57c\ud558\ub294\uc9c0 \uc774\ud574\ud574\ubd05\uc2dc\ub2e4.\n\n**Encoding Architecture:**   \n\n\uc778\ucf54\ub529 \uad6c\uc870\ub294 3 \uac1c\uc758 Convolutional Layers\uc640 3 \uac1c\uc758 Max Pooling Layers\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. Relu\ub294 Convolutional Layers\uc5d0\uc11c \ud65c\uc131\ud654 \ud568\uc218\ub85c \uc0ac\uc6a9\ub418\uba70 \ud328\ub529\uc740 \"same\"\uc73c\ub85c \uc124\uc815\ud569\ub2c8\ub2e4. Max Pooling Layers\uc758 \uc5ed\ud560\uc740 \uc774\ubbf8\uc9c0 \ucc28\uc6d0\uc744 \ub2e4\uc6b4 \uc0d8\ud50c\ub9c1(down sampling)\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774 layer\ub294 \ucd08\uae30 \ud45c\ud604\uc758 \uacb9\uce58\uc9c0 \uc54a\ub294 \ud558\uc704 \uc601\uc5ed\uc5d0 \ucd5c\ub300 \ud544\ud130(max filter)\ub97c \uc801\uc6a9\ud569\ub2c8\ub2e4.\n\n**Decoding Architecture:**   \n\n\ub514\ucf54\ub529 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c\ub3c4 \uc720\uc0ac\ud558\uac8c, Convolutional Layers\ub294 \uc778\ucf54\ub529 \uad6c\uc870\uc640 \ub3d9\uc77c\ud55c \ucc28\uc6d0(\uc5ed\ubc29\ud5a5 \ubc29\uc2dd)\uc744 \uac16\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 3 \uac1c\uc758 Max Pooling Layers \ub300\uc2e0 3 \uac1c\uc758 \uc5c5 \uc0d8\ud50c\ub9c1(up-sampling) \ub808\uc774\uc5b4\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4. \ud65c\uc131\ud654 \ud568\uc218\ub294 \ub3d9\uc77c\ud558\uba70(relu) Convolutional Layers\uc758 \ud328\ub529\ub3c4 \ub3d9\uc77c\ud569\ub2c8\ub2e4. \uc5c5 \uc0d8\ud50c\ub9c1 \ub808\uc774\uc5b4\uc758 \uc5ed\ud560\uc740 \uc785\ub825 \ubca1\ud130\uc758 \ucc28\uc6d0\uc744 \ub354 \ub192\uc740 \ud574\uc0c1\ub3c4\/\ucc28\uc6d0\uc73c\ub85c \uc5c5 \uc0d8\ud50c\ub9c1\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. Max Pooling \uc791\uc5c5\uc740 \ube44\uac00\uc5ed\uc801\uc774\uc9c0\ub9cc \uac01 pooling \uc601\uc5ed \ub0b4\uc5d0\uc11c \ucd5c\ub300 \uac12\uc758 \uc704\uce58\ub97c \u200b\u200b\uae30\ub85d\ud558\uc5ec \ub300\ub7b5\uc801\uc778 \uc5ed(inverse)\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Up-sampling layer\ub294 \uc774 \uc18d\uc131\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub0ae\uc740 \ucc28\uc6d0\uc758 \ud2b9\uc9d5 \uacf5\uac04\uc5d0\uc11c \uc7ac\uad6c\uc131\uc744 \ud22c\uc601(project)\ud569\ub2c8\ub2e4.  \n\n","ea170188":"\n### Excellent References\n\n1. https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/unsupervised-deep-learning-computer-vision\/\n2. https:\/\/towardsdatascience.com\/applied-deep-learning-part-3-autoencoders-1c083af4d798\n3. https:\/\/blog.keras.io\/building-autoencoders-in-keras.html\n4. https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/demo\/autoencoder.html  \n5. https:\/\/machinelearningmastery.com\/develop-encoder-decoder-model-sequence-sequence-prediction-keras\/\n\n\nThanks for viewing the kernel, **please upvote** if you liked it. ","444b01ea":"\uc870\uae30 \uc911\uc9c0 \ucf5c\ubc31(callback)\uc73c\ub85c \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0b5\ub2c8\ub2e4. \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \uc5bb\uace0 \uc2f6\uc73c\uba74 \uc801\ud78c epoch\ubcf4\ub2e4 \ub354 \ud070 \uc218\ub85c \ud559\uc2b5\uc2dc\ud0a4\uc138\uc694.","f9cf406e":"noise \ucd94\uac00\ud558\uae30 \uc804,","9e4cc217":"\ub2e4\uc74c\uc73c\ub85c \uc870\uae30 \uc911\uc9c0 \ucf5c\ubc31(early stop callback)\uc73c\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\ud569\ub2c8\ub2e4.","cc5ed034":"### Generate some predictions","abbd3d35":"\ub2e4\uc74c\uc740 autoencoder \uad6c\uc870\uc5d0 \ub300\ud55c \uc694\uc57d\uc785\ub2c8\ub2e4.","f8b73dde":"\uc774\ub7ec\ud55c \uc720\ud615\uc758 \ub370\uc774\ud130\uc5d0\uc11c \ud575\uc2ec \ubb38\uc81c\ub294 \uc815\ubcf4 \uc190\uc2e4\uc5c6\uc774 \ub2e8\uc77c \ucc28\uc6d0\uc758 \ub370\uc774\ud130 \ud504\ub85c\uc81d\uc158\uc744 \uc5bb\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc720\ud615\uc758 \ub370\uc774\ud130\ub97c latent space\uc5d0 projection\ud558\uba74 \ub9ce\uc740 \uc815\ubcf4\uac00 \uc190\uc2e4\ub418\uace0 \uc774\ub97c \ubcc0\ud615\ud558\uc5ec \uc6d0\ub798 \ubaa8\uc591\uc73c\ub85c \ud22c\uc0ac\ud558\ub294 \uac83\uc774 \uac70\uc758 \ubd88\uac00\ub2a5\ud569\ub2c8\ub2e4. \uc544\ubb34\ub9ac \ub9ce\uc740 \ud3c9\ud589\uc774\ub3d9(shift)\uc640 \ud68c\uc804(rotation)\uc774 \uc801\uc6a9\ub418\uc5b4\ub3c4 \uc6d0\ubcf8 \ub370\uc774\ud130\ub294 \ubcf5\uad6c \ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub807\ub2e4\uba74 \uc2e0\uacbd\ub9dd\uc740 \uc774 \ubb38\uc81c\ub97c \uc5b4\ub5bb\uac8c \ud574\uacb0\ud574\uc57c \ud560\uae4c\uc694? \uc9c1\uad00\uc801\uc73c\ub85c \uc0dd\uac01\ud574\ubcf4\uba74, \ub9e4\ub2c8 \ud3f4\ub4dc \uacf5\uac04(manifold space)\uc5d0\uc11c \uc2ec\uce35 \uc2e0\uacbd\ub9dd\uc740 \uc120\ud615 \ub370\uc774\ud130 \ud3f4\ub4dc \ubdf0(fold view)\ub97c \uc5bb\uae30 \uc704\ud574 \uacf5\uac04(space)\uc744 \uad6c\ubd80(bend)\ub9ac\ub294 \uc18d\uc131\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. Autoencoder \uad6c\uc870\ub294 \uc228\uaca8\uc9c4 \ub808\uc774\uc5b4\uc5d0 \uc774 \uc18d\uc131\uc744 \uc801\uc6a9\ud558\uc5ec \uc7a0\uc7ac \ubdf0 \uacf5\uac04(latent view space)\uc5d0\uc11c \ub0ae\uc740 \uc218\uc900\uc758 \ud45c\ud604\uc744 \ud559\uc2b5 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ub2e4\uc74c \uc774\ubbf8\uc9c0\ub294 \uc774 \uc18d\uc131\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n![](https:\/\/i.imgur.com\/gKCOdiL.png)\n\n\uba3c\uc800 \uc774\ubbf8\uc9c0\uc5d0\uc11c \ud2b9\uc9d5\uc744 \ud559\uc2b5 \ud55c \ub2e4\uc74c \ucd9c\ub825\uacfc \ub3d9\uc77c\ud55c \uc774\ubbf8\uc9c0\ub97c \ud22c\uc0ac\ud558\ub294 Keras\ub97c \uc0ac\uc6a9\ud558\uc5ec autoencoder\ub97c \uad6c\ud604\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n## 2. Implementation\n\n## 2.1 UseCase 1 : Image Reconstruction\n\n1. \ud544\uc694\ud55c library\ub4e4\uc744 \ubd88\ub7ec\uc635\ub2c8\ub2e4.\n","fedc614b":"\ub2e4\uc74c\uc73c\ub85c Keras\uc5d0\uc11c \ubaa8\ub378\uc758 \uad6c\uc870\ub97c \uc0dd\uc131\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","69fbdebc":"\uc774\uc81c Adam Optimizer \ubc0f Categorical Cross Entropy loss \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec autoencoder \ubaa8\ub378\uc744 \ud559\uc2b5 \ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","788594aa":"**Predicted : Autoencoder Output**","e6a28080":"Autoencoder \uad6c\uc870 \uc694\uc57d","26977cf9":"\uc774 \ub370\uc774\ud130\ub97c \ud45c\uc2dc\ud558\uae30 \uc704\ud574 \ud604\uc7ac X\uc640 Y\uc758 2 \ucc28\uc6d0\uc744 \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc774 \uacf5\uac04\uc758 \ucc28\uc6d0\uc744 \ub354 \ub0ae\uc740 \ucc28\uc6d0(\uc989, 1D)\uc73c\ub85c \uc904\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ub2e4\uc74c\uacfc \uac19\uc774 \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n- Line \uc0c1\uc758 \uae30\uc900\uc810: A  \n- \uc218\ud3c9\ucd95\uc73c\ub85c \ubd80\ud130 \uac01\ub3c4: L\n\n\n\uadf8\ub807\ub2e4\uba74 line A \uc704\uc758 \ub2e4\ub978 \uc9c0\uc810(\uc608: B)\uc740 A\ub85c\ubd80\ud130 \uac70\ub9ac \"d\" \ubc0f \uac01\ub3c4 L\uc758 \uad00\uc810\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","2d33c4ec":"### Model \uad6c\uc870\ub97c \uc0b4\ud3b4\ubcf4\uc790\n\nEncoder \uad6c\uc870 \uc694\uc57d","f3efea8f":"### 3. Create Autoencoder architecture\n\n\uc774 \uc139\uc158\uc5d0\uc11c\ub294 autoencoder \uad6c\uc870\ub97c \uc791\uc131\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc778\ucf54\ub529 \ubd80\ubd84\uc740 2000, 1200 \ubc0f 500 \ub178\ub4dc\uac00 \uc788\ub294 3\uac1c\uc758 \ub808\uc774\uc5b4\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uc778\ucf54\ub529 \uc544\ud0a4\ud14d\ucc98\ub294 10\uac1c\uc758 \ub178\ub4dc\ub85c \uad6c\uc131\ub41c latent view space\uc5d0 \uc5f0\uacb0\ub418\uace0 500, 1200 \ubc0f 2000 \ub178\ub4dc\uac00 \uc788\ub294 \ub514\ucf54\ub529 \uc544\ud0a4\ud14d\ucc98\uc5d0 \uc5f0\uacb0\ub429\ub2c8\ub2e4. \ucd5c\uc885 \uacc4\uce35\uc740 \uc785\ub825 \uacc4\uce35\uacfc \uac19\uc740 \uc218\uc758 \ub178\ub4dc\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.","aff8c86e":"\ub530\ub77c\uc11c 20\uac1c\uc758 epoch\ub85c \ud6c8\ub828 \ub41c autoencoder\uac00 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ub9e4\uc6b0 \uc798 \uc7ac\uad6c\uc131 \ud560 \uc218 \uc788\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. autoencoder\uc758 \ub2e4\ub978 \uc0ac\uc6a9 \uc0ac\ub840\ub97c \uc0b4\ud3b4 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. Image denoising \ub610\ub294 removal of noise from the image\ub77c \ubd88\ub9ac\ub294 \uc774\ubbf8\uc9c0 \ub178\uc774\uc988\uc81c\uac70.\n\n## 2.2 UseCase 2 - Image Denoising\n\nAutoencoder\ub294 \ub9e4\uc6b0 \uc720\uc6a9\ud569\ub2c8\ub2e4. Autoencoder\uc758 \ub610 \ub2e4\ub978 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc778 \uc774\ubbf8\uc9c0 \ub178\uc774\uc988 \uc81c\uac70\uc5d0 \ub300\ud574 \uc0b4\ud3b4 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0\ub294 \ub370\uc774\ud130\uc5d0 \ub178\uc774\uc988\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc73c\uba70 autoencoder\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud574\ub2f9 \uc774\ubbf8\uc9c0\uc758noise\ub97c \uc81c\uac70 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc2e4\uc81c\ub85c \uad6c\ud604\ud574\ubd05\uc2dc\ub2e4. \uba3c\uc800 \uc774\ubbf8\uc9c0 \ud53d\uc140\uc744 \uad6c\uc131\ud558\ub294 train_x \ubc0f val_x \ub370\uc774\ud130\ub97c \uc900\ube44\ud569\ub2c8\ub2e4.\n\n![](https:\/\/www.learnopencv.com\/wp-content\/uploads\/2017\/11\/denoising-autoencoder-600x299.jpg)","e4ae9876":"\uc6d0\ubcf8 \uc774\ubbf8\uc9c0\uc640 \uc608\uce21 \uc774\ubbf8\uc9c0\ub97c \uc2dc\uac01\ud654 \ud574\ubd05\uc2dc\ub2e4.\n\n**Inputs: Actual Images**","c245ec99":"### 2. Dataset Prepration \n\n\ub370\uc774\ud130 \uc138\ud2b8\ub97c \ubd88\ub7ec\uc624\uace0, \uc608\uce21 \ubcc0\uc218\uc640 \ub300\uc0c1\uc744 \ubd84\ub9ac\ud558\uace0, \uc785\ub825\uc744 \uc815\uaddc\ud654\ud569\ub2c8\ub2e4.","bcf6ed57":"\uc774 autoencoder \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c\ub294 convolution \ub124\ud2b8\uc6cc\ud06c\uac00 \uc774\ubbf8\uc9c0 \uc785\ub825\uacfc \uc798 \uc791\ub3d9\ud558\uae30 \ub54c\ubb38\uc5d0 convolutional layer\ub97c \ucd94\uac00 \ud560 \uac83\uc785\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uc5d0 convolution\uc744 \uc801\uc6a9\ud558\uae30 \uc704\ud574 28 * 28 \ud589\ub82c\uc758 \ud615\ud0dc\ub85c \uc785\ub825\uc744 \uc7ac\uad6c\uc131\ud569\ub2c8\ub2e4. CNN\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc800\uc790\uc758 \uc774\uc804 [kernel](https:\/\/www.kaggle.com\/shivamb\/a-very-comprehensive-tutorial-nn-cnn)\uc744 \ucc38\uace0\ud558\uc138\uc694.  ","6c10676e":"### Noisy Images \n\n\uc774\ubbf8\uc9c0\uc5d0 \uc758\ub3c4\uc801\uc73c\ub85c \ub178\uc774\uc988\ub97c \ub3c4\uc785 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ubcc0\ud615\uc73c\ub85c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uac15(augmentation)\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218\uc788\ub294 imaug \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubcc0\ud615 \uc911 \ud558\ub098\ub294 \ub178\uc774\uc988 \ucd94\uac00\uc785\ub2c8\ub2e4. \uc774\ubbf8\uc9c0\uc5d0 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ub178\uc774\uc988\ub97c \ucd94\uac00 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uba74,\n\n- Salt and Pepper Noise  \n- Gaussian Noise  \n- Periodic Noise  \n- Speckle Noise  \n\nImpulse \ub178\uc774\uc988\ub77c\uace0\ub3c4 \ud558\ub294 salt and pepper \ub178\uc774\uc988\ub97c \uc774\ubbf8\uc9c0\uc5d0 \uc801\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \ub178\uc774\uc988\ub294 \uc774\ubbf8\uc9c0 \uc2e0\ud638\uc5d0 \ub0a0\uce74\ub86d\uace0 \uac11\uc791\uc2a4\ub7ec\uc6b4 \uc7a5\uc560\ub97c \uc720\ubc1c\ud569\ub2c8\ub2e4. \ub4dc\ubb3c\uac8c \ud770\uc0c9 \ubc0f \uac80\uc815\uc0c9 \ud53d\uc140\ub85c \uc774\ubbf8\uc9c0\uc5d0 \ud45c\ud604\ub418\uae30\ub3c4 \ud569\ub2c8\ub2e4."}}