{"cell_type":{"d8aab22c":"code","77786869":"code","259c1e13":"code","f40aaac3":"code","3c168960":"code","a9d69873":"code","995f24e0":"code","34cb1267":"code","1e2687e6":"code","c01db525":"code","7ae17433":"code","c2dc9e82":"code","0033813f":"code","a21a91e3":"code","3763f289":"code","58264201":"code","9df2b8cd":"code","2f8e1521":"code","15ddfb18":"code","41d437bd":"code","46c941c2":"code","6b42e788":"code","3c06a342":"code","7f9af37a":"code","8302904c":"code","ce9bdd84":"code","ca69769d":"code","23e5680f":"code","fd65a8fd":"code","13f71d75":"code","f6b2b0b5":"code","4a0e1ce7":"code","1a5c8b41":"code","0c6f5cce":"code","5233fa86":"code","b6852c63":"code","cef6d63d":"code","eee6bde3":"code","7fd143fe":"code","6ec5e07f":"code","b69d5101":"code","514d5241":"code","1408cefc":"code","01003242":"code","dc2d1b0c":"code","fea1ffb6":"code","e4a433df":"code","44c4f1bf":"code","f654cf7e":"code","2194a6d5":"code","1fa39325":"code","69d9782b":"code","3842584f":"code","6008a69c":"code","079cd9ff":"code","d4be352e":"code","4b4493b8":"code","752ab874":"code","a477b484":"code","73ce8069":"code","7a685243":"code","1108c9cc":"code","8b6b6826":"code","fdd8e1f7":"code","b5e62fee":"code","e7a177cb":"code","72db4a48":"code","2b999700":"code","24ad1b5e":"code","00b713c2":"code","330872b6":"code","d4327b48":"code","69708437":"code","c86d4252":"code","495f1723":"code","fbc120be":"code","22117148":"code","1e5fb623":"code","b5c7f821":"code","b83bd4d0":"code","2313d533":"code","3ce98f10":"code","64101d42":"code","14169c6c":"code","2bf13452":"code","c3a5f4d9":"code","41bbe01f":"code","6b70bcd5":"code","2ea1b219":"code","211216f7":"code","7478d183":"code","c143ec8a":"markdown","4914b6eb":"markdown","140f007e":"markdown","b97b161f":"markdown","9ea5a3e8":"markdown","2e97850a":"markdown","5317f61c":"markdown","959984ad":"markdown","404e2545":"markdown","34dce74f":"markdown","2a8f4691":"markdown","91842f29":"markdown","2ef2a33d":"markdown","dc94d142":"markdown","420de188":"markdown","47603559":"markdown","4c4e6b42":"markdown","e4966e2e":"markdown","9012d9cc":"markdown","1fc517ba":"markdown","6abf1bd1":"markdown","ae3eb01d":"markdown","89b982f4":"markdown","a19c2bfd":"markdown","32318565":"markdown","b8732c9d":"markdown","5e9a4e1c":"markdown","9046e046":"markdown","33a3a16b":"markdown","f8a1db83":"markdown","c2c9ab42":"markdown","52031658":"markdown","3a92e743":"markdown","4a6fba8c":"markdown","3add5b93":"markdown","0a7c23f8":"markdown","02ef1d7f":"markdown","e7ec7e7f":"markdown","1fba1cba":"markdown","488650ac":"markdown","c026e6d4":"markdown","21b62af4":"markdown","ca25e3a1":"markdown","d5bb8c62":"markdown","bf0b7e97":"markdown","c5c50010":"markdown","ac9bd6db":"markdown","6064f3a5":"markdown","7984bc9a":"markdown","9432832a":"markdown","6752c49c":"markdown","82b0cc77":"markdown","d7d14541":"markdown","bce546ff":"markdown","c9628b4c":"markdown","de037c20":"markdown","a2c81211":"markdown","0941c76e":"markdown","9fb37aee":"markdown","5291d984":"markdown","67d33bab":"markdown","4ef179c7":"markdown","7bb8e22f":"markdown","ca1e6856":"markdown","55f07acc":"markdown","0830c0ad":"markdown","ae963c0e":"markdown","724fd54f":"markdown","5d01cd06":"markdown","eaa03783":"markdown","540d7166":"markdown","e7c89a9f":"markdown","0e5465e5":"markdown","8fdd2e09":"markdown","93a802c3":"markdown","109ef409":"markdown","2a2a974b":"markdown","5c37a647":"markdown","024ea340":"markdown"},"source":{"d8aab22c":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Data Manipulation and Treatment\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n#Plotting and Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nfrom scipy import stats\nimport itertools\n\n#Scikit-Learn for Modeling\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics","77786869":"def str_to_date(date):\n    return datetime.strptime(date, '%Y-%m-%d').date()","259c1e13":"#The training Set\ndf_train = pd.read_csv(\"..\/input\/ml-training-vlib\/train.csv\",sep=',', parse_dates=['Date']\n                       , date_parser=str_to_date,\n                       low_memory = False)\n\n\n#Additional Information on those stores \ndf_store = pd.read_csv(\"..\/input\/ml-training-vlib\/store.csv\"\n                       , low_memory = False)","f40aaac3":"df_train.head(20) ","3c168960":"#df_train[df_train[\"Store\"] == 1]","a9d69873":"df_train.dtypes,print (\"The Train dataset has {} Rows and {} Variables\".format(str(df_train.shape[0]),str(df_train.shape[1])))","995f24e0":"df_store.head()","34cb1267":"df_store.dtypes ,print (\"The Store dataset has {} Rows (which means unique Shops) and {} Variables\".format(str(df_store.shape[0]),str(df_store.shape[1]))) ","1e2687e6":"df_train.isna().sum()","c01db525":"print ()\nprint (\"-Durant ces 2 ann\u00e9es, {} est le nombre de jours que les magasins ont ferm\u00e9s.\".format(df_train[(df_train.Open == 0)].count()[0]))\nprint ()\nprint (\"-Parmi ces fermetures, {} sont dues aux vacances scolaires. \" .format(df_train[(df_train.Open == 0) & (df_train.SchoolHoliday == 1)&(df_train.StateHoliday == '0') ].count()[0]))\nprint ()\nprint (\"-et {} sont dues aux vacances 'publique'.\".format(df_train[(df_train.Open == 0) &\n         ((df_train.StateHoliday == 'a') |\n          (df_train.StateHoliday == 'b') | \n          (df_train.StateHoliday == 'c'))].count()[0]))\nprint ()\nprint (\"-Il est int\u00e9r\u00e9ssant de noter, {} jours de fermetures que ne sont pas expliqu\u00e9s (aucune vacances d'annonc\u00e9es).\".format(df_train[(df_train.Open == 0) &\n         (df_train.StateHoliday == \"0\")\n         &(df_train.SchoolHoliday == 0)].count()[0]))\nprint ()","7ae17433":"df_train[(df_train.Open == 0)].Sales.mean()","c2dc9e82":"df_train = df_train.drop(df_train[(df_train.Open == 0) & (df_train.Sales == 0)].index)","0033813f":"df_train = df_train.reset_index(drop=True) #making sure the indexes are back to [0,1,2,3 etc.] ","a21a91e3":"print (\"Our new training set has now {} rows \".format(df_train.shape[0]))","3763f289":"df_train.Sales.describe() ","58264201":"# Pourquoi?\ndf_train[(df_train.Open == 1) & (df_train.Sales == 0)]","9df2b8cd":"df_train=df_train.drop(df_train[(df_train.Open == 1) & (df_train.Sales == 0)].index)\ndf_train = df_train.reset_index(drop=True) ","2f8e1521":"fig, axes = plt.subplots(1, 2, figsize=(17,3.5))\naxes[0].boxplot(df_train.Sales, showmeans=True,vert=False)\naxes[0].set_xlim(0,max(df_train[\"Sales\"]+1000))\naxes[0].set_title('Boxplot For Sales Values')\naxes[1].hist(df_train.Sales, cumulative=False, bins=20)\naxes[1].set_title(\"Sales histogram\")\naxes[1].set_xlim((min(df_train.Sales), max(df_train.Sales)))\n\n{\"Mean\":np.mean(df_train.Sales),\"Median\":np.median(df_train.Sales)}\n","15ddfb18":"fig, axes = plt.subplots(1, 2, figsize=(17,3.5))\naxes[0].boxplot(df_train.Sales, showmeans=True,vert=False)\naxes[0].set_xlim(0,max(df_train[\"Sales\"]+1000))\naxes[0].set_title('Boxplot For Sales Values')\naxes[1].hist(np.log(df_train.Sales), cumulative=False, bins=20)\naxes[1].set_title(\"Sales histogram\")\naxes[1].set_xlim((min(np.log(df_train.Sales)), max(np.log(df_train.Sales))))\n\n{\"Mean\":np.mean(df_train.Sales),\"Median\":np.median(df_train.Sales)}","41d437bd":"print (\"{0:.2f}% of the time Rossman are actually having big sales day (considered outliers).\".format(df_train[df_train.Sales>14000].count()[0]\/df_train.shape[0]*100))","46c941c2":"df_train.Customers.describe()    ","6b42e788":"fig, axes = plt.subplots(1, 2, figsize=(17,3.5))\naxes[0].boxplot(df_train.Customers, showmeans=True,vert=False)\naxes[0].set_xlim(0,max(df_train[\"Customers\"]+100))\naxes[0].set_title('Boxplot For Customer Values')\naxes[1].hist(df_train.Customers, cumulative=False, bins=20)\naxes[1].set_title(\"Customers histogram\")\naxes[1].set_xlim((min(df_train.Customers), max(df_train.Customers)))\n\n{\"Mean\":np.mean(df_train.Customers),\"Median\":np.median(df_train.Customers)}","3c06a342":"print (\"{0:.2f}% of the time Rossman are actually having customers more than usual (considered outliers).\".format(df_train[df_train.Customers>1500].count()[0]\/df_train.shape[0]*100))","7f9af37a":"df_train[df_train.Customers>7000]","8302904c":"stats.pearsonr(df_train.Customers, df_train.Sales)[0]","ce9bdd84":"df_store.count(0)\/df_store.shape[0] * 100","ca69769d":"df_store[pd.isnull(df_store.CompetitionDistance)] ","23e5680f":"df_store_check_distribution=df_store.drop(df_store[pd.isnull(df_store.CompetitionDistance)].index)\nfig, axes = plt.subplots(1, 2, figsize=(17,3.5))\naxes[0].boxplot(df_store_check_distribution.CompetitionDistance, showmeans=True,vert=False,)\naxes[0].set_xlim(0,max(df_store_check_distribution.CompetitionDistance+1000))\naxes[0].set_title('Boxplot For Closest Competition')\naxes[1].hist(df_store_check_distribution.CompetitionDistance, cumulative=False, bins=30)\naxes[1].set_title(\"Closest Competition histogram\")\naxes[1].set_xlim((min(df_store_check_distribution.CompetitionDistance), max(df_store_check_distribution.CompetitionDistance)))\n{\"Mean\":np.nanmean(df_store.CompetitionDistance),\"Median\":np.nanmedian(df_store.CompetitionDistance),\"Standard Dev\":np.nanstd(df_store.CompetitionDistance)}#That's what i thought, very different values, let's see why ","fd65a8fd":"df_store['CompetitionDistance'].fillna(df_store['CompetitionDistance'].median(), inplace = True)","13f71d75":"# remplir avec la m\u00e9diane versus 0\ndf_store.CompetitionOpenSinceMonth.fillna(0, inplace = True)\ndf_store.CompetitionOpenSinceYear.fillna(0,inplace=True)","f6b2b0b5":"df_store[pd.isnull(df_store.Promo2SinceWeek)]","4a0e1ce7":"df_store[pd.isnull(df_store.Promo2SinceWeek)& (df_store.Promo2==0)]","1a5c8b41":"df_store.Promo2SinceWeek.fillna(0,inplace=True)\ndf_store.Promo2SinceYear.fillna(0,inplace=True)\ndf_store.PromoInterval.fillna(0,inplace=True)","0c6f5cce":"df_store.count(0)\/df_store.shape[0] * 100","5233fa86":"#Left-join the train to the store dataset since .Why?\n#Because you want to make sure you have all events even if some of them don't have their store information ( which shouldn't happen)\ndf_train_store = pd.merge(df_train, df_store, how = 'left', on = 'Store')\ndf_train_store.head() \nprint (\"The Train_Store dataset has {} Rows and {} Variables\".format(str(df_train_store.shape[0]),str(df_train_store.shape[1]))) \n","b6852c63":"df_train_store","cef6d63d":"df_train_store['SalesperCustomer']= df_train_store['Sales']\/df_train_store['Customers']","eee6bde3":"df_train_store['SalesperCustomer']","7fd143fe":"fig, axes = plt.subplots(2, 3,figsize=(17,10) )\npalette = itertools.cycle(sns.color_palette(n_colors=4))\nplt.subplots_adjust(hspace = 0.28)\n#axes[1].df_train_store.groupby(by=\"StoreType\").count().Store.plot(kind='bar')\naxes[0,0].bar(df_store.groupby(by=\"StoreType\").count().Store.index,df_store.groupby(by=\"StoreType\").count().Store,color=[next(palette),next(palette),next(palette),next(palette)])\naxes[0,0].set_title(\"Number of Stores per Store Type \\n Fig 1.1\")\naxes[0,1].bar(df_train_store.groupby(by=\"StoreType\").sum().Sales.index,df_train_store.groupby(by=\"StoreType\").sum().Sales\/1e9,color=[next(palette),next(palette),next(palette),next(palette)])\naxes[0,1].set_title(\"Total Sales per Store Type (in Billions) \\n Fig 1.2\")\naxes[0,2].bar(df_train_store.groupby(by=\"StoreType\").sum().Customers.index,df_train_store.groupby(by=\"StoreType\").sum().Customers\/1e6,color=[next(palette),next(palette),next(palette),next(palette)])\naxes[0,2].set_title(\"Total Number of Customers per Store Type (in Millions) \\n Fig 1.3\")\naxes[1,0].bar(df_train_store.groupby(by=\"StoreType\").sum().Customers.index,df_train_store.groupby(by=\"StoreType\").Sales.mean(),color=[next(palette),next(palette),next(palette),next(palette)])\naxes[1,0].set_title(\"Average Sales per Store Type \\n Fig 1.4\")\naxes[1,1].bar(df_train_store.groupby(by=\"StoreType\").sum().Customers.index,df_train_store.groupby(by=\"StoreType\").Customers.mean(),color=[next(palette),next(palette),next(palette),next(palette)])\naxes[1,1].set_title(\"Average Number of Customers per Store Type \\n Fig 1.5\")\naxes[1,2].bar(df_train_store.groupby(by=\"StoreType\").sum().Sales.index,df_train_store.groupby(by=\"StoreType\").SalesperCustomer.mean(),color=[next(palette),next(palette),next(palette),next(palette)])\naxes[1,2].set_title(\"Average Spending per Customer in each Store Type \\n Fig 1.6\")\nplt.show()","6ec5e07f":"StoretypeXAssortment = sns.countplot(x=\"StoreType\",hue=\"Assortment\",order=[\"a\",\"b\",\"c\",\"d\"], data=df_store,palette=sns.color_palette(\"Set2\", n_colors=3)).set_title(\"Number of Different Assortments per Store Type\")\ndf_store.groupby(by=[\"StoreType\",\"Assortment\"]).Assortment.count()","b69d5101":"df_train_store['Month']=df_train_store.Date.dt.month\ndf_train_store['Year']=df_train_store.Date.dt.year","514d5241":"sns.factorplot(data = df_train_store, x =\"Month\", y = \"Sales\", \n               col = 'Promo', # per store type in cols\n               hue = 'Promo2',\n               row = \"Year\"\n              ,sharex=False)\n","1408cefc":"sns.factorplot(data = df_train_store, x =\"Month\", y = \"SalesperCustomer\", \n               col = 'Promo', # per store type in cols\n               hue = 'Promo2',\n               row = \"Year\"\n              ,sharex=False)","01003242":"#df_train_store.columns","dc2d1b0c":"#sns.factorplot(data = df_train_store, x =\"Month\", y = \"Customers\", \n#               col = 'Promo', # per store type in cols\n#               hue = 'Promo2',\n#               row = \"Year\"\n#              ,sharex=False)","fea1ffb6":"sns.factorplot(data = df_train_store, x =\"DayOfWeek\", y = \"Sales\",\n                hue='Promo'\n              ,sharex=False)","e4a433df":"#33 Stores are opened on Sundays\nprint (\"Number of Stores opened on Sundays:{}\" .format(df_train_store[(df_train_store.Open == 1) & (df_train_store.DayOfWeek == 7)]['Store'].unique().shape[0]))","44c4f1bf":"len(df_train_store[\"Store\"].unique())","f654cf7e":"#df_train_store[(df_train_store.Open == 1) & (df_train_store.DayOfWeek == 7)]","2194a6d5":"df_train_store['CompetitionDist_Cat'] = pd.cut(df_train_store['CompetitionDistance'], 5)","1fa39325":"#pd.cut(df_train_store['CompetitionDistance'], 5)","69d9782b":"df_train_store.groupby(by=\"CompetitionDist_Cat\").Sales.mean()","3842584f":"df_train_store.groupby(by=\"CompetitionDist_Cat\").Customers.mean()","6008a69c":"del df_train_store[\"CompetitionDist_Cat\"]","079cd9ff":"# Cr\u00e9ation de variable jour\ndf_train_store['Day']=df_train_store.Date.dt.day\ndel df_train_store[\"Date\"]","d4be352e":"df_train_store['StoreType'].isnull().any(),df_train_store['Assortment'].isnull().any(),df_train_store['StateHoliday'].isnull().any()","4b4493b8":"df_train_store[\"StoreType\"].value_counts(),df_train_store[\"Assortment\"].value_counts(),df_train_store[\"StateHoliday\"].value_counts()","752ab874":"df_train_store['StateHoliday'] = df_train_store['StateHoliday'].astype('category')\ndf_train_store['Assortment'] = df_train_store['Assortment'].astype('category')\ndf_train_store['StoreType'] = df_train_store['StoreType'].astype('category')\ndf_train_store['PromoInterval']= df_train_store['PromoInterval'].astype('category')","a477b484":"df_train_store['StateHoliday_cat'] = df_train_store['StateHoliday'].cat.codes\ndf_train_store['Assortment_cat'] = df_train_store['Assortment'].cat.codes\ndf_train_store['StoreType_cat'] = df_train_store['StoreType'].cat.codes\ndf_train_store['PromoInterval_cat'] = df_train_store['PromoInterval'].cat.codes","73ce8069":"df_train_store['StateHoliday_cat'] = df_train_store['StateHoliday_cat'].astype('float')\ndf_train_store['Assortment_cat'] = df_train_store['Assortment_cat'].astype('float')\ndf_train_store['StoreType_cat'] = df_train_store['StoreType_cat'].astype('float')\ndf_train_store['PromoInterval_cat'] = df_train_store['PromoInterval_cat'].astype('float')","7a685243":"df_train_store.dtypes","1108c9cc":"df_correlation=df_train_store[['Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo',\n        'SchoolHoliday',\n       'CompetitionDistance', 'CompetitionOpenSinceMonth',\n       'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n       'Promo2SinceYear', 'SalesperCustomer', 'Month', 'Year',\n       'Day', 'StateHoliday_cat', 'Assortment_cat', 'StoreType_cat',\n       'PromoInterval_cat']]","8b6b6826":"df_correlation=df_correlation.drop('Open', axis = 1)","fdd8e1f7":"upper_triangle = np.zeros_like(df_correlation.corr(), dtype = np.bool)\nupper_triangle[np.triu_indices_from(upper_triangle)] = True #make sure we don't show half of the other triangle\nf, ax = plt.subplots(figsize = (15, 10))\nsns.heatmap(df_correlation.corr(),ax=ax,mask=upper_triangle,annot=True, fmt='.2f',linewidths=0.5,cmap=sns.diverging_palette(10, 133, as_cmap=True))","b5e62fee":"del df_train_store['Assortment_cat']\ndel df_train_store['StoreType_cat']\ndel df_train_store['PromoInterval_cat']","e7a177cb":"df_train_store['CompetitionOpenSince'] = np.where((df_train_store['CompetitionOpenSinceMonth']==0) & (df_train_store['CompetitionOpenSinceYear']==0) , 0,(df_train_store.Month - df_train_store.CompetitionOpenSinceMonth) + \n                                       (12 * (df_train_store.Year - df_train_store.CompetitionOpenSinceYear)) )","72db4a48":"df_train_store['CompetitionOpenSince']","2b999700":"del df_train_store['CompetitionOpenSinceYear']\ndel df_train_store['CompetitionOpenSinceMonth']\n","24ad1b5e":"#df_train_store[\"is_holiday_state\"] = df_train_store['StateHoliday'].map({\"0\": 0, \"a\": 1, \"b\": 1, \"c\": 1})\n#del df_train_store['StateHoliday_cat']\n#del df_train_store['StateHoliday']","00b713c2":"df_train_store=pd.get_dummies(df_train_store, columns=[\"Assortment\", \"StoreType\",\"PromoInterval\"], prefix=[\"is_Assortment\", \"is_StoreType\",\"is_PromoInteval\"])\ndf_train_store.columns","330872b6":"df_train_store","d4327b48":"def rmspe(y, yhat):\n    rmspe = np.sqrt(np.mean( (y - yhat)**2 ))\n    return rmspe","69708437":"features = df_train_store.drop(['Customers', 'Sales', 'SalesperCustomer'], axis = 1) \n# Une r\u00e8gle empirique est de transformer ma valeur cible en log si je vois que les valeurs sont tr\u00e8s dispers\u00e9es, ce qui est le cas\n# et bien s\u00fbr les ramener \u00e0 leurs vraies valeurs avec np.exp\ntargets=np.log(df_train_store.Sales)\n","c86d4252":"X_train, X_train_test, y_train, y_train_test = model_selection.train_test_split(features, targets, test_size=0.20, random_state=15)","495f1723":"def function_class(data_train, data_test, clf):\n    clf.fit(data_train, y_train)\n    yhat  = clf.predict(data_test)\n    error = rmspe(y_train_test,yhat)  \n    return clf, error","fbc120be":"from sklearn.dummy import DummyRegressor\ndummmy = DummyRegressor(strategy='median')\n\nclf_dummy, error_dummy = function_class(X_train, X_train_test, dummmy)\nerror_dummy","22117148":"rfr = RandomForestRegressor(n_estimators = 10, n_jobs = -1)\nclf_rf, error_rf = function_class(X_train, X_train_test, rfr)\nerror_rf","1e5fb623":"#X_train","b5c7f821":"from sklearn.feature_selection import RFECV\n\n# Serialization\nfrom sklearn.externals import joblib \n\ndef neg_rmspe(y, yhat):\n    rmspe = np.sqrt(np.mean( (y - yhat)**2 ))*-1\n    return rmspe\n\nscoring_new = metrics.make_scorer(neg_rmspe)\n\ndef rfecv_step(step):\n    rfecv = RFECV(estimator =  RandomForestRegressor(n_estimators = 5, n_jobs = -1), step = step, cv = 2,\n                  scoring = scoring_new, n_jobs = -1, verbose = 10)\n    x_select = rfecv.fit_transform(X_train, y_train)\n    joblib.dump(rfecv, 'selection_features_' + str(step) + '_.pkl')\n        \n    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n    print(\"Rmpse with the features selected: %f\" % rfecv.grid_scores_.max())\n    \n    return x_select\n\nx_select = rfecv_step(25)","b83bd4d0":"rfecv_1 = joblib.load('selection_features_' + str(25) + '_.pkl')\n\nrfr = RandomForestRegressor(n_estimators = 10, n_jobs = -1)\nclf_rf, error_rf = function_class(rfecv_1.transform(X_train), rfecv_1.transform(X_train_test), rfr)\nerror_rf","2313d533":"params = {'max_depth':(10,20),\n         'n_estimators':(10,25)}\nscoring_fnc = metrics.make_scorer(rmspe)\ngrid = model_selection.RandomizedSearchCV(estimator = rfr, param_distributions = params,\n                                          cv = 3, verbose = 10, n_jobs = -1, scoring = scoring_new ) \ngrid.fit(rfecv_1.transform(X_train), y_train)","3ce98f10":"grid.best_params_,grid.best_score_\n#MY BEST PARAMS ARE :n_estimators=128,max_depth=20,min_samples_split=10","64101d42":"#with the optimal parameters i got let's see how it behaves with the validation set\nrfr_val=RandomForestRegressor(n_estimators=20, \n                             criterion='mse', \n                             max_depth=10, \n                             min_samples_split=10, \n                             n_jobs=-1,\n                             random_state=35, \n                             verbose=0)\nmodel_RF_test = rfr_val.fit(X_train,y_train)\nyhat = model_RF_test.predict(X_train_test)","14169c6c":"importances = rfr_val.feature_importances_\n\nstd = np.std([rfr_val.feature_importances_ for tree in rfr_val.estimators_],\n             axis=0)\nindices = np.argsort(importances)\npalette1 = itertools.cycle(sns.color_palette())\n# Store the feature ranking\nfeatures_ranked=[]\nfor f in range(X_train.shape[1]):\n    features_ranked.append(X_train.columns[indices[f]])\n# Plot the feature importances of the forest\n\nplt.figure(figsize=(10,15))\nplt.title(\"Feature importances\")\nplt.barh(range(X_train.shape[1]), importances[indices],\n            color=[next(palette1)], align=\"center\")\nplt.yticks(range(X_train.shape[1]), features_ranked)\nplt.ylabel('Features')\nplt.ylim([-1, X_train.shape[1]])\nplt.show()","2bf13452":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(gpu_id = 0, tree_method = 'gpu_hist')\nclf_xgb, error_xgb = function_class(X_train, X_train_test, xgb)\nerror_xgb","c3a5f4d9":"params = {'n_estimators':(100,200,300,400,500),\n         'colsample_bytree':(0.6,0.7,0.8,0.9,1),\n         'learning_rate':(0.1,0.01,1,0.001)}\nscoring_fnc = metrics.make_scorer(rmspe)\ngrid = model_selection.RandomizedSearchCV(estimator = XGBRegressor(gpu_id = 0, tree_method = 'gpu_hist'), param_distributions = params,\n                                          cv = 3, verbose = 10, n_jobs = -1, scoring = scoring_new ) \ngrid.fit(rfecv_1.transform(X_train), y_train)","41bbe01f":"grid.best_params_,grid.best_score_","6b70bcd5":"xgb = XGBRegressor(gpu_id = 0, tree_method = 'gpu_hist', colsample_bytree=0.9, learning_rate=1, n_estimators = 400 )\nclf_xgb, error_xgb = function_class(X_train, X_train_test, xgb)\nerror_xgb","2ea1b219":"from catboost import CatBoostRegressor\n\nctb = CatBoostRegressor(task_type=\"GPU\", devices='0:1', verbose = 0)\nclf_ctb, error_ctb = function_class(X_train, X_train_test, ctb)\nerror_ctb","211216f7":"params = {'iterations':(500, 750, 1000, 2000),\n         'depth':(10,20,25),\n         'learning_rate':(0.1,0.01,1,0.001)}\nscoring_fnc = metrics.make_scorer(rmspe)\ngrid = model_selection.RandomizedSearchCV(estimator = CatBoostRegressor(task_type=\"GPU\", devices='0:1', verbose = 0), param_distributions = params,\n                                          cv = 3, verbose = 10, n_jobs = -1, scoring = scoring_new ) \ngrid.fit(rfecv_1.transform(X_train), y_train)","7478d183":"grid.best_params_,grid.best_score_","c143ec8a":"# I. Importation des packages","4914b6eb":"- La meilleure fa\u00e7on d'\u00e9valuer la performance d'un type de magasin est de voir quelles sont les ventes par client afin de tout normaliser et d'obtenir le magasin qui fait que ses clients d\u00e9pensent le plus en moyenne.\n\n- Comparons d'abord les ventes totales de chaque type de magasin, ses ventes moyennes et voyons ensuite comment cela change lorsque nous ajoutons les clients \u00e0 l'\u00e9quation :\n","140f007e":"- Nous constatons un changement spectaculaire lorsque nous comparons le fait d'avoir une promotion \"Promo\" = 1 \u00e0 l'absence de promotion \"Promo\" = 0 et que nous pouvons conclure qu'un magasin qui a une promotion un jour donn\u00e9 change consid\u00e9rablement son chiffre d'affaires.\n\n- Mais, \u00e9tonnamment, lorsque nous v\u00e9rifions de mani\u00e8re plus granulaire la variable \"Promo2\" (indiquant une promotion continue en bleu ou en orange), nous constatons qu'en g\u00e9n\u00e9ral, lorsqu'il n'y a pas de promotion cons\u00e9cutive, les magasins ont tendance \u00e0 vendre plus qu'avec une promotion cons\u00e9cutive. C'est probablement une solution qu'ils mettent en place pour traiter les magasins dont les ventes sont tr\u00e8s faibles au d\u00e9part. Et en effet, en v\u00e9rifiant les ventes par client par rapport \u00e0 la promotion, nous comprenons que ces magasins souffrent initialement de faibles ventes et que la promotion continue montre une augmentation tremblante du pouvoir d'achat des clients.\n\n- Si l'on regarde les ann\u00e9es, on constate une l\u00e9g\u00e8re augmentation d'une ann\u00e9e sur l'autre, mais pas de changement majeur entre 2013 et 2015, et on observe en fait un sch\u00e9ma tr\u00e8s similaire au fil des mois, avec des pics importants d'abord autour de la p\u00e9riode de P\u00e2ques en mars et avril, puis en \u00e9t\u00e9 en mai, juin et juillet et enfin autour de la p\u00e9riode de No\u00ebl en novembre et d\u00e9cembre.\n","b97b161f":"## Question 23:\n- Utiliser https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.corr.html pour cr\u00e9er la heatmap \u00e0 partir de la matrice des corr\u00e9lations\n- Analyses les corr\u00e9lations les plus \u00e9lev\u00e9es","9ea5a3e8":"1) `CompetitionDistance`:","2e97850a":"## Question 15:\n- Remplacer les valeurs manquantes de Promo2SinceWeek, Promo2SinceYear & PromoInterval par z\u00e9ros\n- Afficher les valeurs manquantes de df_store afin de v\u00e9rifier que tout est \u00e0 100%","5317f61c":"## Question 13:\n- Remplacer les valeurs manquantes par la m\u00e9diane","959984ad":"Comme nous avons besoin de variables num\u00e9riques \u00e0 la fois pour notre analyse de corr\u00e9lation et pour alimenter les mod\u00e8les bas\u00e9s sur les arbres de d\u00e9cision, nous devons transformer ce qui n'est pas num\u00e9rique en une repr\u00e9sentation num\u00e9rique. ","404e2545":"## Feature Engineering","34dce74f":"## Variable - Competition Distance","2a8f4691":"Nous avons encore StoreType, Assortment et StateHoliday comme Obejcts ; nous devons les convertir en cat\u00e9gories num\u00e9riques : Mais nous devons d'abord nous assurer que nous n'avons pas Nan avant de faire ces transformations, sinon Nan sera \u00e9gal \u00e0 -1","91842f29":"## Question 30:\n- Utiliser RandomizedSearchCV. Remplir le param\u00e9tre scoring, n_jobs, verbose, cv (3), estimator et param_distributions\n- fitter le mod\u00e8le sur X_train transform\u00e9 avec notre algorithme de s\u00e9lection de variables","2ef2a33d":"- Ce cas est assez simple, toutes les valeurs manquantes proviennent de champs o\u00f9 `Promo2`=0 ce qui signifie qu'il n'y a pas d'activit\u00e9s promotionnelles continues pour ces magasins.\n- L'absence de promotion signifie que ces champs doivent \u00e9galement \u00eatre \u00e0 0 puisqu'ils sont li\u00e9s \u00e0 Promo2.","dc94d142":"## Question 27:\n- Utiliser un random forest avec comme param\u00e8tres n_estimators = 10, n_jobs = -1","420de188":"## Question 22:\n- D\u00e9couper la variable CompetitionDistance qui est continue en 5 parties - Cat\u00e9gorielle CompetitionDist_Cat","47603559":"- Avant de d\u00e9cider comment traiter cette question, nous savons qu'il existe une infinit\u00e9 de moyens de combler les valeurs manquantes.\n- L'approche la plus courante et la plus simpliste consiste \u00e0 la remplir avec la moyenne ou la m\u00e9diane de cette variable.\n- Examinons rapidement ces mesures.","4c4e6b42":"Le jeu de donn\u00e9es train.csv contient une date. Lors de l'import avec pandas, cette date est reconnu en tant qu'OBJECT. Afin de pouvoir l'utiliser de fa\u00e7on optimale et de pouvoir r\u00e9aliser du feature engineering sur celle ci, nous allons sp\u00e9cifier directement dans l'importaion que la colonne 'Date' est une date au format yyyy-mm-jj. Pour cela nous allons cr\u00e9\u00e9r une fonction permettant de parser la date directement lors de l'importation du fichier.","e4966e2e":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQA1NImk-thpL2U9_rwsOf-SAQw55Ed3HvaOy7J1sdGjvpdq7Lx)","9012d9cc":"## D\u00e9finir la m\u00e9trique\nIl est difficile de mesurer la qualit\u00e9 d'un mod\u00e8le donn\u00e9 sans quantifier ses performances par rapport \u00e0 la formation et aux tests. Cela se fait g\u00e9n\u00e9ralement \u00e0 l'aide d'un certain type de mesure des performances. Pour ce projet, nous utiliserons le score RMSE (Root Mean Square Error) - Erreur quadratique moyenne - fourni comme mesure d'\u00e9valuation pour la comp\u00e9tition. Si le r\u00e9sultat est inf\u00e9rieur \u00e0 10%, cela signifie que la pr\u00e9diction est de tr\u00e8s bonne qualit\u00e9 et que c'est notre objectif dans ce projet.","1fc517ba":"## Question 34:\n- R\u00e9entrainer XGB avec les nouveaux param\u00e8tres\n- Si certains param\u00e8tres ont atteints des bornes sup ou max relanc\u00e9 RandomizedSearchCV en adaptant les param\u00e8tres","6abf1bd1":"# IV. Analyse des corr\u00e9lations","ae3eb01d":"# II. Importation des jeux de donn\u00e9es","89b982f4":"## Question 7:\n- Afficher les statistques baisques de df_train sur la variables Sales\n- Que remarquez vous?","a19c2bfd":"## Question 16:\n- Merger les deux jeux de donn\u00e9e sur la variable cl\u00e9 Store","32318565":"## Mod\u00e9lisation\nDans cette section du projet, nous allons d\u00e9velopper un entra\u00eenement et un ajustement de RF en utilisant GridSearch pour l'optimisation des hyperparam\u00e8tres afin de faire une pr\u00e9diction.\nEnsuite, nous ferons des \u00e9valuations pr\u00e9cises de la performance de chaque mod\u00e8le (RandomForestRegressor vs XGBoostRegressor) en utilisant la biblioth\u00e8que sklearn nous aider \u00e0 \u00e9valuer quel mod\u00e8le est le plus appropri\u00e9.","b8732c9d":"## III.2 Analyse de df_store\n## Question 13:\n- Afficher le pourcentage de valeur manquantes pour chaque variables","5e9a4e1c":"- Les variables \"Promo2SinceWeek\", \"Promo2SinceYear\" et \"PromoInterval\" ont un taux de remplissage de 51% car elles sont en fait des valeurs NULL, car il n'y a pas de promotion continue pour ces magasins. \n\n- Pour les variables \"CompetitionOpenSinceMonth\" et \"CompetitionOpenSinceYear\", ce sont essentiellement des donn\u00e9es manquantes que nous traitons ici (taux de remplissage de 68,25 %), ce qui signifie que nous avons la distance la plus proche du concurrent mais que nous manquons les informations sur la date \u00e0 laquelle il a r\u00e9ellement ouvert \u00e0 c\u00f4t\u00e9 du magasin Rossman.","9046e046":"## Question 17:\n- Calculer la nouvelle variable SalesperCustomer correspondant au nombre de ventes normalis\u00e9s par clients","33a3a16b":"## Question 4:\n- Afficher le nombre de jours ou l'on a un magasin ferm\u00e9\n- Afficher le nombre de jours ou l'on a un magasin ferm\u00e9 le jour de vacances scolaire mais pas des vacances \"publique\"\n- Afficher le nombre de jours ou l'on a un magasin ferm\u00e9 coincidant avec des vacances \"publique\"\n- Afficher le nombre de jours ou l'on a un magasin ferm\u00e9 coincidant avec des jours ou il n'y avait aucune vacances (Scolaire ou \"publique\")","f8a1db83":"- Nous pouvons clairement voir ici que la plupart des magasins ont soit un type d'assortiment \"a\", soit un type d'assortiment \"c\".\n- Il est int\u00e9ressant de noter que le magasin de type d, qui a la moyenne de ventes par client la plus \u00e9lev\u00e9e, a en fait un type d'assortiment \"c\" plus \u00e9lev\u00e9 que les autres, ce qui explique tr\u00e8s probablement cette moyenne \u00e9lev\u00e9e de ventes par client.\n- Un autre facteur important est le fait que le magasin de type b est le seul \u00e0 avoir le type d'assortiment b et qu'un grand nombre d'entre eux sont en fait des \"extra\" et, comme le montrent les figures 1.4 et 1.5, c'est celui qui a le plus grand nombre de clients et de ventes. Cette formule d'extra est probablement le juste milieu pour les clients entre un assortiment pas trop vari\u00e9 comme l'assortiment C et un assortiment pas trop basique comme l'assortiment \"a\" et c'est ce qui explique le trafic \u00e9lev\u00e9 dans ce magasin.","c2c9ab42":"## Question 1:\n- Importer train.csv et stocker le dans df_train, sp\u00e9cifier le nom de la colonne contenant la date et ajouter la fonction avec le param\u00e8tre date_parser\n- Importer store.csv et stocker le dans df_store","52031658":"## Question 9:\n- Afficher la boxplot et la distribution pour la variable \u00e0 pr\u00e9dire","3a92e743":"## Question 8:\n- Supprimer les lignes avec des magasins ouvert et des ventes nulles","4a6fba8c":"2) `CompetitionOpenSinceMonth` and `CompetitionOpenSinceYear`?","3add5b93":"## Question 19:\n- A votre avis en \u00e9tudiant la r\u00e9partition des gamme - Assortment et les ventes, le panier moyen et le nombre de client par type de magasin que peut on en d\u00e9duire?\n- En g\u00e9n\u00e9ral que retrouve t'on le plus en terme de gamme dans les magasins?\n- Mettre en avant le type de gamme permettant d'avoir un panier \u00e9l\u00e9v\u00e9\n- Mettre en avant le type de gamme permettant d'avoir un nombre de clients \u00e9lev\u00e9s\n- Mettre en avant le type de gamme permettant d'avoir un nombre de ventes \u00e9lev\u00e9es","0a7c23f8":"# Machine Learning","02ef1d7f":"- Puisque nous ne disposons d'aucune information sur ces valeurs manquantes et d'aucune mani\u00e8re pr\u00e9cise de les remplir.\n- Un moyen cr\u00e9atif pourrait \u00eatre d'appliquer un algorithme de classification multilabel et de s'entra\u00eener sur les champs non Nan, puis de pr\u00e9dire ce qui pourrait \u00eatre tr\u00e8s probablement le mois et l'ann\u00e9e pour ces champs. Mais cette approche est trop longue sur le plan des calculs.\n- C'est pourquoi ces champs vont \u00eatre assign\u00e9s \u00e0 0.\n","e7ec7e7f":"## Question 21:\n- Analyser la variable Promo par jours\n- Quelles informations peut on en tirer?","1fba1cba":"* Id - an Id that represents a (Store, Date) duple within the test set\n* Store - a unique Id for each store\n* Sales - the turnover for any given day (this is what you are predicting)\n* Customers - the number of customers on a given day\n* Open - an indicator for whether the store was open: 0 = closed, 1 = open\n* StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n* SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n* StoreType - differentiates between 4 different store models: a, b, c, d\n* Assortment - describes an assortment level: a = basic, b = extra, c = extended\n* CompetitionDistance - distance in meters to the nearest competitor store\n* CompetitionOpenSince[Month\/Year] - gives the approximate year and month of the time the nearest competitor was opened\n* Promo - indicates whether a store is running a promo on that day\n* Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n* Promo2Since[Year\/Week] - describes the year and calendar week when the store started participating in Promo2\n* PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store","488650ac":"# Pr\u00e9diction des ventes\n\nRossmann g\u00e8re plus de 3 000 pharmacies dans 7 pays europ\u00e9ens. Actuellement, les directeurs des magasins Rossmann sont charg\u00e9s de pr\u00e9voir leurs ventes quotidiennes jusqu'\u00e0 six semaines \u00e0 l'avance. Les ventes des magasins sont influenc\u00e9es par de nombreux facteurs, notamment les promotions, la concurrence, les vacances scolaires et nationales, la saisonnalit\u00e9 et la localit\u00e9. Comme des milliers de g\u00e9rants pr\u00e9disent les ventes en fonction de leur situation particuli\u00e8re, la pr\u00e9cision des r\u00e9sultats peut \u00eatre tr\u00e8s variable.\n\n\n## Objectifs:\n\n- Exploration de donn\u00e9es (outliers, missing values etc.).\n- Analyse des corr\u00e9lation.\n- Entrainer un mod\u00e8le \n- Evaluer le mod\u00e8le\n- Choisir le meilleur mod\u00e8le","c026e6d4":"## III.1 Analyse de df_train","21b62af4":"## Question 24:\n- Utiliser get_dummies sur les variables \"Assortment\", \"StoreType\",\"PromoInterval\"","ca25e3a1":"- Il a fallu v\u00e9rifier certaines exceptions (les valeurs aberrantes) dans le boxplot pour voir si les donn\u00e9es saisies \u00e9taient erron\u00e9es, mais il s'av\u00e8re que ce grand nombre de ventes certains jours s'explique soit par des objectifs promotionnels, le type de magasin \u00e9tant grand et populaire, soit par le fait qu'il n'a pas assez de concurrence et qu'il est le monopole dans sa r\u00e9gion.\n\n- Une mesure importante \u00e0 toujours v\u00e9rifier lorsque l'on examine une distribution est la comparaison entre la moyenne et la m\u00e9diane et la distance qui les s\u00e9pare. Comme nous le voyons ici, une moyenne de 6955 contre 6369 en m\u00e9diane est un tr\u00e8s bon signe qu'il n'y a pas de valeurs extravagantes affectant la distribution g\u00e9n\u00e9rale des ventes.","d5bb8c62":"# III.3 Analyse crois\u00e9e df_train & df_store","bf0b7e97":"# Question 10:\n- Afficher le pourcentage de valeurs sup\u00e9rier strict \u00e0 14 000\n- A votre avis comment expliquer les ventes sup\u00e9rieurs \u00e0 14k? Faut il les supprimer?","c5c50010":"- Nos 5 variables les plus importantes sont :\n\n 1-Distance du concurrent : Cela a en effet un impact important sur les ventes d'un magasin comme nous l'avons vu pr\u00e9c\u00e9demment dans notre EDA, alors que la concurrence est tr\u00e8s loin ; les magasins ont tendance \u00e0 vendre beaucoup plus.\n \n 2-Promotion : La promotion est primordiale pour qu'un magasin augmente ses ventes, elle permet de casser les prix et donc d'int\u00e9resser plus de clients \u00e0 l'achat.\n \n 3-Magasin : Le magasin lui-m\u00eame repr\u00e9sente un identificateur unique pour l'algorithme permettant de reconna\u00eetre quel magasin a quels attributs et de mieux prendre en compte les pr\u00e9visions de ces m\u00eames magasins dans un futur proche.\n \n 4-CompetitionOpenSince : la fusion de cette variable a pay\u00e9 et nous a permis de donner des pr\u00e9visions plus pr\u00e9cises des ventes en fonction du moment de l'ouverture de ces concurrents.\n \n 5-DayofWeek : Comme nous l'avons dit, au cours d'une semaine, le sch\u00e9ma varie beaucoup si c'est un dimanche ou un lundi (comme nous l'avons vu dans notre EDA) par exemple et chaque jour de la semaine a ses propres attributs et propri\u00e9t\u00e9s qui permettent de savoir combien nous allons vendre.\n","ac9bd6db":"## Question 12:\n- Afficher le pourcentage d'observations avec Customers sup\u00e9rieur \u00e0 1500\n- Afficher la valeur de l'observation avec la valeurs Customers sup\u00e9rieur \u00e0 7000\n- Calculer la corr\u00e9lation lin\u00e9aire (pearson) entre Customers et Sales","6064f3a5":"- Nous constatons une distribution tr\u00e8s biais\u00e9e \u00e0 droite pour cette variable avec une diff\u00e9rence significative entre la moyenne et la m\u00e9diane. Ceci est d\u00fb \u00e0 la dispersion des donn\u00e9es avec un \u00e9cart-type de 7659, sup\u00e9rieur \u00e0 la moyenne et \u00e0 la m\u00e9diane.\n- Il est pr\u00e9f\u00e9rable, d'un point de vue r\u00e9aliste, d'introduire la valeur m\u00e9diane dans les trois magasins Nan plut\u00f4t que la moyenne, car celle-ci est biais\u00e9e par ces valeurs aberrantes.","7984bc9a":"# III. Exploration de donn\u00e9es","9432832a":"- Nous pouvons d'abord voir le 0,82 entre les clients et les ventes, ce qui sugg\u00e8re qu'ils sont positivement corr\u00e9l\u00e9s comme nous l'avons indiqu\u00e9 plus haut dans l'analyse.\n\n- Il est int\u00e9ressant de constater que les ventes par client et la promotion (0,28) sont en fait positivement corr\u00e9l\u00e9es, puisque l'organisation d'une promotion augmente ce chiffre.\n\n- Les ventes par client sont \u00e9galement en corr\u00e9lation avec la distance de concurrence (0,21), de mani\u00e8re positive, comme je l'ai dit, plus la distance de concurrence est grande, plus nous r\u00e9alisons de ventes par client, ce qui est logique, plus notre concurrence est forte, plus Rossman peut atteindre une position de monopole dans la r\u00e9gion.\n\n- De plus, l'effet de la promotion2 sur les ventes par client, comme nous l'avons dit plus haut (0,22), a provoqu\u00e9 un changement dans le mod\u00e8le d'achat et l'a augment\u00e9 lorsque la promotion continue \u00e9tait appliqu\u00e9e.\n\n- Enfin, nous pouvons voir que StoreType joue un r\u00f4le majeur dans les ventes par client (0,44), ce qui est probablement d\u00fb \u00e0 l'encodage de la variable type de magasin qui sugg\u00e8re que les cat\u00e9gories \u00e9lev\u00e9es comme d qui est \u00e9gal \u00e0 4 ont un poids plus \u00e9lev\u00e9. Il faut garder en t\u00eate qu'ici nous somme sur une corr\u00e9lation lin\u00e9aire et il est n\u00e9cessaire de prendre du recul vis \u00e0 vis de ces r\u00e9sultats. Pour l'analyse des corr\u00e9lations des cat\u00e9gorielles il serait n\u00e9c\u00e9ssaire de se pencher sur le Khi-deux (permettant de mettre en avant l'intensit\u00e9 de relation entre cat\u00e9gories)","6752c49c":"## Question 20:\n- Analyser les graphiques\n- A t'on les m\u00eames tendances d'une ann\u00e9e \u00e0 l'autre?\n- La variable Promo \u00e0 t'elle un impact?\n- Promo2? Que peut on en d\u00e9duire? A t'on les r\u00e9sultats attendu?","82b0cc77":"# Question 11:\n- Afficher les statistiques basiques pour la variables Customers\n- Afficher la boxplot & la distribution \n- Comparer les deux distributions entre Customers & Sales (Comparaison visuel)","d7d14541":"## Question 18:\n- Analyser les graphiques\n- Trouver le type de magasin avec le plus grand nombre d'apparition dans notre jeu de donn\u00e9es\n- Trouver le type de magasin avec le plus grand nombre de vente\n- Trouver le type de magasin avec le panier moyen le plus \u00e9lev\u00e9\n- Est ce coh\u00e9rent avec les informations que vous avez sur les diff\u00e9rents type de magasins?","bce546ff":"## Question 29:\n- Importer le mod\u00e8le sauvegarder\n- Transformer la donn\u00e9e X_train et X_train_test afin de passer 25 variables \u00e0 N (N correspondant au nombre de features optimale selon notre algorithme de s\u00e9lection de variables)\n- Afficher l'erreur sur la donn\u00e9e de test","c9628b4c":"## Question 33:\n- Activer le GPU dans settings sur votre droite. Accelerator -> GPU\n- Refaire le pipeline avec un mod\u00e8le Lin\u00e9aire https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV + https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV\n- R\u00e9aliser le pipeline avec XGBoostRegressor","de037c20":"- Ici les meilleurs r\u00e9sultats se trouvent aux bornes de ce que nous avons sp\u00e9cifi\u00e9s dans params. Afin de trouver les meilleurs param\u00e8tres il serait n\u00e9c\u00e9ssaire d'augmenter le nombre de param\u00e8tres \u00e0 test\u00e9s. Cependant cela prendrait trop de temps et cela demande une machine plus puissante et accessible pendant plusieurs minutes voir plusieurs heures...","a2c81211":"- Ce jeu de formation nous montre que le magasin de type A est celui qui compte le plus grand nombre de succursales, de ventes et de clients dans les quatre types de magasins diff\u00e9rents. Mais cela ne signifie pas que c'est le Storetype le plus performant.\n\n- Si l'on examine les ventes moyennes et le nombre de clients, on constate qu'en fait, c'est le magasin de type B qui a enregistr\u00e9 les ventes moyennes les plus \u00e9lev\u00e9es et le nombre moyen de clients le plus \u00e9lev\u00e9. On pourrait supposer que si B ne compte que 17 magasins mais un nombre moyen de ventes et de clients si \u00e9lev\u00e9 qu'il s'agit probablement d'hyper succursales Rossman alors que A serait plus petit en taille mais beaucoup plus pr\u00e9sent.\n\n- \u00c9tonnamment, c'est le magasin de type D qui a la d\u00e9pense moyenne la plus \u00e9lev\u00e9e par client, ce qui s'explique probablement par une distance moyenne de concurrence plus \u00e9lev\u00e9e que les autres, ce qui signifie que chaque client ach\u00e8tera plus puisqu'il sait qu'il n'y a pas beaucoup de magasins similaires dans les environs.\n\n- Ce qui nous aiderait \u00e0 mieux comprendre ce qui se passe, c'est de rechercher d'autres variables expliquant ce comportement, comme les assortiments, la concurrence et les promotions.","0941c76e":"# Question 25:\n- S\u00e9parer notre jeu de donn\u00e9es en 2 \u00e9chantillons train & test - 80% \/ 20%","9fb37aee":"## Question 31:\n- Utiliser un random forest avec les param\u00e8tres suivant n_estimators=128, criterion='mse', max_depth=20, min_samples_split=10, n_jobs=-1,random_state=35, verbose=0","5291d984":"Le minimum est \u00e0 zero. Ce qui veut dire qu'il y a des magasins qui \u00e9taient ouvert mais qui n'ont g\u00e9n\u00e9r\u00e9s aucun chiffre d'affaire. Nous partirons de principe que ces \u00e9v\u00e9nements sont des outliers. Il est possible que la variable Open soit erron\u00e9 quelques fois.","67d33bab":"## Question 5:\n- V\u00e9rifier que le vente sont \u00e0 z\u00e9ro lorsque le magasin est ferm\u00e9","4ef179c7":"- Nous pouvons observer des tendances similaires dans la colonne des clients et dans celle des ventes. En fait, notre facteur de corr\u00e9lation de 0,82 explique qu'il existe une forte corr\u00e9lation positive entre les ventes et les clients. En g\u00e9n\u00e9ral, plus vous avez de clients dans un magasin, plus vos ventes de la journ\u00e9e sont \u00e9lev\u00e9es.\n\n- Nous constatons qu'un jour donn\u00e9, il y a eu un grand nombre de clients dans un magasin, en raison d'une grande promotion. Ces valeurs sp\u00e9cifiques affectent la moyenne, ce qui explique la diff\u00e9rence entre une moyenne de 762 et une m\u00e9diane de 676.\n\n- On observe une asym\u00e9trie \u00e0 droite dans les deux distributions en raison du faible nombre de valeurs aberrantes, mais la repr\u00e9sentation \u00e9lev\u00e9e de chaque valeur aberrante seule pousse la distribution vers la droite, comme le montrent les deux histogrammes, ce qui se produit g\u00e9n\u00e9ralement lorsque la moyenne est sup\u00e9rieure \u00e0 la m\u00e9diane.","7bb8e22f":"# Question 3:\n- Afficher le nombre de valeurs manquantes pour df_train","ca1e6856":"## Question 2:\n- Afficher les 5 premi\u00e8res lignes de df_train\n- Afficher le nombre de lignes et de colonnes de df_train\n- Idem pour df_store","55f07acc":"## Question 14:\n- Remplir les valeurs manquantes par z\u00e9ro","0830c0ad":"## Question 32:\n- Afficher les variables importantes du mod\u00e8le pr\u00e9c\u00e9dent","ae963c0e":"# Question 26:\n- D\u00e9finir une fonction prenant en input un classifier, entrainant le classifier et sortant de fa\u00e7on automatique la m\u00e9trique\n- Utiliser DummyRegressor pour avoir une baseline","724fd54f":"\u00c9tant donn\u00e9 que l'association de 0,1,2,3 \u00e0 des variables cat\u00e9gorielles telles que StoreType,Assortment,StateHoliday affecte le biais de l'algorithme (0 compterait moins de 3 alors qu'en r\u00e9alit\u00e9, le StoreType a et le StoreType c devraient \u00eatre trait\u00e9s de mani\u00e8re \u00e9gale). Nous allons simplement le convertir en cat\u00e9gorique maintenant pour les besoins de l'analyse de corr\u00e9lation, puis utiliser la fonction get_dummies pour les coder de mani\u00e8re binaire.","5d01cd06":"> ## Question 35:\n- R\u00e9aliser la m\u00eame chose avec CatBoost - Utilisation du GPU + RandomizedSearchCV\n- Relancer le mod\u00e8le avec les hyperparam\u00e8tres obtenus","eaa03783":"- Puisque les variables `CompetitionOpenSinceYear` et `CompetionOpenSinceMonth` ont la m\u00eame signification sous-jacente, les fusionner en une seule variable que nous appelons \"Comp\u00e9titionOuverte\" permet \u00e0 l'algorithme de comprendre plus facilement le mod\u00e8le et cr\u00e9e moins de branches et donc des arbres complexes.","540d7166":"-`StateHoliday` n'est pas tr\u00e8s important \u00e0 distinguer (quel type de vacances) et peut \u00eatre fusionn\u00e9 dans une variable binaire appel\u00e9e `is_holiday_state`.","e7c89a9f":"Ce qui est int\u00e9ressant de tracer, c'est l'effet de la distance de concurrence la plus proche sur les ventes, pour voir si celui qui a une concurrence tr\u00e8s lointaine r\u00e9alise effectivement plus de ventes que celui qui a une concurrence proche. Comme la \"distance de concurrence\" est une variable continue, nous devons d'abord la convertir en une variable cat\u00e9gorielle avec 5 cat\u00e9gories diff\u00e9rents (j'ai choisi ce nombre en regardant la distribution et pour garder l'esth\u00e9tique).","0e5465e5":"## Variable - Assortments\nComme nous l'avons indiqu\u00e9 dans la description, les assortiments ont trois types et chaque magasin a un type et un type d'assortiment d\u00e9finis : \n- a\" signifie \"choses de base\n- \"b\" signifie \"extra\".\n- \"c\" signifie des choses \u00e9tendues, donc la plus grande vari\u00e9t\u00e9 de produits.\n\nCe qui pourrait \u00eatre int\u00e9ressant, c'est de voir la relation entre un type de magasin et son type d'assortiment respectif.","8fdd2e09":"- Nous constatons d\u00e9j\u00e0 une grande diff\u00e9rence, m\u00eame au niveau de la semaine (du lundi au vendredi), lorsque nous s\u00e9parons la promotion et l'absence de promotion, et qu'il n'y a pas de promotion pendant le week-end.\n\n- On peut comprendre que le dimanche ait un pic aussi \u00e9lev\u00e9, car tr\u00e8s peu de magasins ouvrent le dimanche (seulement 33) ; si quelqu'un a besoin de quelque chose d'urgent et n'a pas le temps de le recevoir en semaine, il devra faire un certain trajet pour se rendre dans ceux qui sont ouverts, m\u00eame si ce n'est pas pr\u00e8s de chez lui. Cela signifie que ces 33 magasins ouverts le dimanche repr\u00e9sentent en fait la demande potentielle si tous les Rossman Stores \u00e9taient ferm\u00e9s le dimanche. Cela nous montre clairement combien il est important que les magasins soient ouverts le dimanche.\n\n- Apr\u00e8s avoir essay\u00e9 d'examiner le comportement des ventes sur une semaine au fil des ann\u00e9es et des mois, j'ai conclu que le sch\u00e9ma ne change pas, ce qui signifie qu'il y a toujours un pic le lundi avec les promotions, un petit pic le vendredi avant le week-end et un gros pic le dimanche \u00e0 cause des magasins fer","93a802c3":"# Random Forest\n\n**Ses avantages:**\n    \n- Les temps de parcours al\u00e9atoires des for\u00eats sont assez rapides, et ils sont capables de traiter des donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es et manquantes.\n- Le processus consistant \u00e0 faire la moyenne ou \u00e0 combiner les r\u00e9sultats de diff\u00e9rents arbres de d\u00e9cision permet de surmonter le probl\u00e8me du sur apprentissage.\n- Ils ne n\u00e9cessitent pas non plus de pr\u00e9paration des donn\u00e9es d'entr\u00e9e. Il n'est pas n\u00e9cessaire de mettre les donn\u00e9es \u00e0 l'\u00e9chelle.\n\n**Ses inconv\u00e9nients:**\n- Le principal inconv\u00e9nient des For\u00eats al\u00e9atoires est la taille du mod\u00e8le. Vous pourriez facilement vous retrouver avec une for\u00eat qui prend des centaines de m\u00e9gaoctets de m\u00e9moire et qui est lente \u00e0 \u00e9valuer.\n- Elles deviennent un peu plus difficiles \u00e0 interpr\u00e9ter que les arbres de d\u00e9cision ordinaires, puisque nous construisons des for\u00eats de plus de 50 arbres de d\u00e9cision et plus en utilisant la recherche par grille.","109ef409":"## Question 28:\n- Utiliser la fonction de s\u00e9lection de variable RFECV https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html. Avec comme param\u00e8tre 1\n- Cette fonction maximise un m\u00e9trique, hors dans notre cas le meilleur mod\u00e8le est celui qui minimise la m\u00e9trique. Proposer une \u00e9volution de rmpse afin de cadrer avec la fonction","2a2a974b":"## Question 6:\nPour \u00e9viter tout biais, tout bruit dans notre jeu de donn\u00e9es, nous allons supprimer cette information. De plus cela r\u00e9duira le nombre de lignes \u00e0 traiter par notre mod\u00e8le.\n\n- Supprimer les jours ou les magasins sont ferm\u00e9s","5c37a647":"3) `Promo2SinceWeek`, `Promo2SinceYear` and `PromoInterval` ?","024ea340":"## Variable - Promotion\nVoyons comment la promotion affecte les ventes globales de Rossman en examinant quand il y a une promotion et quand il n'y en a pas pendant ces 3 ann\u00e9es. Cela nous permet d'abord de voir l'impact de la promotion et aussi de voir l'\u00e9volution des ventes sur des ann\u00e9es sp\u00e9cifiques (donc les tendances d'une ann\u00e9e donn\u00e9e) et l'augmentation progressive des ventes de 2013 \u00e0 2015 :"}}