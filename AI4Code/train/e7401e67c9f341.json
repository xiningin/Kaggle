{"cell_type":{"1701b78f":"code","ceaf1668":"code","1d515c67":"code","12c53057":"code","c71ecf12":"code","c9eb6a91":"code","8ca50972":"code","67756cb5":"code","a4d33321":"code","0ae88382":"code","812df4b2":"code","a76dd1ea":"code","5c71e1ae":"code","4231a87c":"code","6270229c":"code","284d67be":"code","822992a1":"code","d6423af4":"code","61843642":"code","815bca7d":"code","07f0197f":"code","05471330":"code","43ebd0bf":"code","6fd1e7f9":"code","7dd19632":"code","f534c180":"code","81e5607b":"code","01b26118":"code","42a4f85f":"code","7f8834ce":"code","a8250459":"code","9cadc731":"code","f1a2175a":"code","94ff7598":"code","e95512ea":"code","7f97e418":"code","9726f931":"code","bc703647":"code","f86f0b14":"code","38570c1a":"code","b2d80951":"code","dc49a9e8":"code","9ecd1df9":"code","8fe0f6c8":"code","b57cc592":"code","c4301134":"code","71aa79c6":"code","d602443e":"code","b053f299":"code","563ae35c":"code","ba90bb86":"code","ed411c5c":"code","258107d1":"code","12dc30ef":"code","a22122c8":"code","ac615008":"code","c16d6ad3":"code","bb004820":"code","a5897677":"code","278eaa29":"code","928c59ef":"code","960516eb":"code","0175be71":"code","f6976469":"code","423af51a":"code","61d7c4e3":"code","0bcbb910":"code","56304691":"code","7cc1009d":"code","77e7faeb":"code","57c5e347":"code","8a406f15":"code","c6a56acd":"code","e8f69b79":"code","e6b655cb":"code","30728d8b":"code","d99107de":"code","b107af65":"code","b12d969f":"code","3957e6d2":"markdown","85816e1f":"markdown","f2d61511":"markdown","8a2d0aa0":"markdown","92f44466":"markdown","bc26b97c":"markdown","d5714fb6":"markdown","72fddb32":"markdown","f27041c8":"markdown","4defd8c6":"markdown","b5ff2032":"markdown","c7ed0ecd":"markdown","7283f5b8":"markdown","a5cb9736":"markdown","907bce09":"markdown","4fd52ffb":"markdown","6f68b12e":"markdown","1411532e":"markdown","51bb0298":"markdown","0e785357":"markdown","bb0fd923":"markdown","5afdac17":"markdown","b32deb14":"markdown","02e42ee5":"markdown","0b98ecec":"markdown","fe11f532":"markdown","20b5a3cc":"markdown","986f53d8":"markdown","5a3abf65":"markdown","47437a64":"markdown","c4ff5ba1":"markdown","fbdf1d8e":"markdown","258fbde5":"markdown","b7d4701c":"markdown","242463e2":"markdown","e27fa83e":"markdown","f8d535f8":"markdown","8c29fe1c":"markdown","9bc21d10":"markdown","cafd8583":"markdown"},"source":{"1701b78f":"## import standard libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm","ceaf1668":"## import data and names files\ndata = pd.read_csv('..\/input\/sample-dataset-for-ols-reg-model-statistics\/imports-85.csv')\nnames = pd.read_csv('..\/input\/sample-dataset-for-ols-reg-model-statistics\/imports-85_names.csv',  sep='_')  ## default separator does not work","1d515c67":"data.head(2)","12c53057":"names.values","c71ecf12":"header_names = ['Symboling', 'Normalized_losses', 'Brand', 'Fuel_type', 'Aspiration', 'Num_of_doors', 'Body_style', 'Drive_wheels', 'Engine_location', 'Wheel_base', 'Length', 'Width', 'Height', 'Curb_weight', 'Engine_type', 'Num_of_cylinders', 'Engine_size', 'Fuel_system', 'Bore', 'Stroke', 'Compression_ratio', 'Horsepower', 'Peak_rpm', 'City_mpg', 'Highway_mpg', 'Price']\ndata = pd.read_csv('..\/input\/sample-dataset-for-ols-reg-model-statistics\/imports-85.csv', names=header_names)\ndata.head(3)","c9eb6a91":"def missing_table_creator(data):\n    missing_values, number_of_missings, perc_of_missing, type_of_data = [], [], [], []\n    m=0   \n    for j in range(len(data)):\n        for i in range(len(data.columns)):\n            if data.values[j, i]==\"?\":\n                missing_values.append(data.columns[i])             \n    unique_list = np.unique(list(missing_values))\n    for i in unique_list:\n        number_of_missings.append(missing_values.count(i))\n        perc_of_missing.append(str(np.round(int(number_of_missings[m])\/int(len(data)) * 100, 2))+\"%\")\n        m+=1\n    for i in unique_list:\n        type_of_data.append(type(data[i][0]))\n    missings_table = {\"Column's name\": unique_list, \"Number of missings\": number_of_missings, \"Percent of missings\": perc_of_missing, \"Type\": type_of_data}\n    missings_table = pd.DataFrame(missings_table)\n    return missings_table","8ca50972":"mst = missing_table_creator(data)\nmst","67756cb5":"np.unique(list(data.Normalized_losses))","a4d33321":"normalized_losses_grouped = data.groupby(by='Normalized_losses').count()\nnormalized_losses_list = np.unique(normalized_losses_grouped.index)","0ae88382":"for i in range(len(normalized_losses_list)):\n    if normalized_losses_list[i]==\"?\":\n        normalized_losses_list[i]=400","812df4b2":"for i in range(len(normalized_losses_list)):\n    normalized_losses_list[i]=int(normalized_losses_list[i])","a76dd1ea":"norm_loss_sort = {\"Args\": normalized_losses_list, \"Values\": normalized_losses_grouped.values[:,0]}\nnorm_loss_sort = pd.DataFrame(norm_loss_sort)\nnorm_loss_sort = norm_loss_sort.sort_values(\"Args\")","5c71e1ae":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nax1.title.set_text(\"Distribution of Normalized losses compared wtih normal distribution curve\")\nsns.distplot(normalized_losses_grouped.values[:,0], fit=norm, kde=True, ax=ax1, color='gray')\nax2.title.set_text(\"Plot of Normalized losses with N\/A value changed into 333\")\nax2.scatter(norm_loss_sort.values[:,0], norm_loss_sort.values[:,1], color='gray')","4231a87c":"data.Normalized_losses = data.Normalized_losses.replace({\"?\": 400})","6270229c":"missing_table_creator(data)","284d67be":"## check for type of data\nfor i in missing_table_creator(data)[\"Column's name\"]:\n    print(data[str(i)].head(4))","822992a1":"np.unique(data.Num_of_doors)","d6423af4":"data.Num_of_doors = data.Num_of_doors.replace({\"four\": 1, \"two\": 0})","61843642":"data[data.Num_of_doors==\"?\"]","815bca7d":"## sedans should have 4 doors\ndata.Num_of_doors = data.Num_of_doors.replace({\"?\": 1})","07f0197f":"missing_table_creator(data)","05471330":"bore_mean = np.round(data[data.Bore!=\"?\"].Bore.astype(\"float32\").mean(), 2)\ndata.Bore = data.Bore.replace({\"?\": bore_mean})","43ebd0bf":"hpr_mean = np.round(data[data.Horsepower!=\"?\"].Horsepower.astype(\"float32\").mean(), 2)\ndata.Horsepower = data.Horsepower.replace({\"?\": hpr_mean})","6fd1e7f9":"peak_mean = np.round(data[data.Peak_rpm!=\"?\"].Peak_rpm.astype(\"float32\").mean(), 2)\ndata.Peak_rpm = data.Peak_rpm.replace({\"?\": peak_mean})","7dd19632":"stroke_mean = np.round(data[data.Stroke!=\"?\"].Stroke.astype(\"float32\").mean(), 2)\ndata.Stroke = data.Stroke.replace({\"?\": stroke_mean})","f534c180":"price_mean = np.round(data[data.Price!=\"?\"].Price.astype(\"float32\").mean(), 2)\ndata.Price = data.Price.replace({\"?\": price_mean})","81e5607b":"missing_table_creator(data)","01b26118":"## data without missing values\ndata.head()","42a4f85f":"## split data into train and labels sets\ntrain = data.drop(columns='Price')\nlabels = data.Price","7f8834ce":"## name of columns for below dictionaries\nnames.values[50:80]","a8250459":"## dictionaries for categorical features\nbrand_dict = dict(zip(list(np.unique(data.Brand)), [2, 2, 2, 1, 2, 1, 0, 2, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 0, 1, 0, 2])) ##brands spltted into three categories: cheap, more expensive and premium\nfuel_dict = dict(zip(list(np.unique(data.Fuel_type)), list(range(len(data.groupby(by=['Fuel_type']).count().iloc[:,0])))))\naspiration_dict = dict(zip(list(np.unique(data.Aspiration)), list(range(len(data.groupby(by=['Aspiration']).count().iloc[:,0])))))\nbody_dict = dict(zip(list(np.unique(data.Body_style)), [3, 1, 0, 2, 3]))\nwheels_dict = dict(zip(list(np.unique(data.Drive_wheels)), list(range(len(data.groupby(by=['Drive_wheels']).count().iloc[:,0])))))\nengine_loc_dict = dict(zip(list(np.unique(data.Engine_location)), list(range(len(data.groupby(by=['Engine_location']).count().iloc[:,0])))))\nengine_type_dict = dict(zip(list(np.unique(data.Engine_type)), list(range(len(data.groupby(by=['Engine_type']).count().iloc[:,0])))))\ncylinders_dict = dict(zip(list(np.unique(data.Num_of_cylinders)), [8, 5, 4, 6, 3, 12, 2]))\nfuel_system_dict = dict(zip(list(np.unique(data.Fuel_system)), list(range(len(data.groupby(by=['Fuel_system']).count().iloc[:,0])))))","9cadc731":"## mapping dictionaries and creating numerical values instead of categoricals\ntrain[\"brand\"] = data.Brand.map(brand_dict)\ntrain[\"fuel_type\"] = data.Fuel_type.map(fuel_dict)\ntrain[\"aspiration\"] = data.Aspiration.map(aspiration_dict)\ntrain[\"body_style\"] = data.Body_style.map(body_dict)\ntrain[\"drive_wheels\"] = data.Drive_wheels.map(wheels_dict)\ntrain[\"engine_location\"] = data.Engine_location.map(engine_loc_dict)\ntrain[\"engine_type\"] = data.Engine_type.map(engine_type_dict)\ntrain[\"cylinders\"] = data.Num_of_cylinders.map(cylinders_dict)\ntrain[\"fuel_system\"] = data.Fuel_system.map(fuel_system_dict)\ntrain = train.drop(columns=['Brand', 'Fuel_type', 'Aspiration', 'Body_style', 'Drive_wheels', 'Engine_location', 'Engine_type', 'Num_of_cylinders', 'Fuel_system'])","f1a2175a":"import statsmodels.api as sm","94ff7598":"x = sm.add_constant(train.astype('float64'))\nx = np.array(x).astype(\"float64\")\ny = np.array(labels).astype(\"float64\")\nxnames = list(train.astype('float64').columns)\nxnames.insert(0, 'Intercept')\nfirst_model = sm.OLS(y, x)\nfirst_results = first_model.fit()\nprint(first_results.summary(xname=xnames, yname='Price'))","e95512ea":"from sklearn.model_selection import train_test_split","7f97e418":"#n=int(input(\"Please enter n: \"))\nr_sq_mean = []\nfor i in range(150):\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n    model = sm.OLS(y_train, x_train)\n    results = model.fit()\n    pred = results.predict(x_test)\n    r_sq = 1 - sum((pred - y_test)**2) \/ sum((y_test - y_test.mean())**2)\n    r_sq_mean.append(r_sq)\nprint(np.mean(r_sq_mean))","9726f931":"corr = train.astype(\"float64\").corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nf, ax = plt.subplots(figsize=(22, 22))\nsns.heatmap(corr, cbar=True, annot=True, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.8, cbar_kws={\"shrink\": .5}, ax=ax)\nax.arrow(17, 15.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(16, 14.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(15, 13.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(11.5, 10, 0, 1, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(8, 6.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(7, 5.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(6, 4.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(9, 7.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(26, 24.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(11, 9.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(5, 3.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(4, 2.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')\nax.arrow(4, 2.5, -1, 0, head_width=0.5, head_length=0.7, width=.06, fc='b', ec='b')","bc703647":"x = sm.add_constant(train.drop(columns=['Num_of_doors', 'Wheel_base', 'Length', 'Width', 'Height', 'Curb_weight', 'Bore', 'Compression_ratio', 'Peak_rpm', 'City_mpg', 'Highway_mpg', 'fuel_system']).astype(\"float64\"))\ny = np.array(labels).astype(\"float64\")\nxnames = list(train.drop(columns=['Num_of_doors', 'Wheel_base', 'Length', 'Width', 'Height', 'Curb_weight', 'Bore', 'Compression_ratio', 'Peak_rpm', 'City_mpg', 'Highway_mpg', 'fuel_system']).astype(\"float64\").columns)\nxnames.insert(0, 'Intercept')\n#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\nsecond_model = sm.OLS(y, x)\nsecond_results = second_model.fit()\nprint(second_results.summary(xname=xnames, yname='Price'))","f86f0b14":"corr = train.drop(columns=['Num_of_doors', 'Wheel_base', 'Length', 'Width', 'Height', 'Curb_weight', 'Bore', 'Compression_ratio', 'Peak_rpm', 'City_mpg', 'Highway_mpg', 'fuel_system']).astype(\"float64\").corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nf, ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(corr, cbar=True, annot=True, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=1.2, cbar_kws={\"shrink\": .5}, ax=ax)\n\nax.arrow(6, 4.5, -1, 0, head_width=0.3, head_length=0.4, width=.06, fc='b', ec='b')\nax.arrow(4.5, 3, 0, 1, head_width=0.3, head_length=0.4, width=.06, fc='b', ec='b')\nax.arrow(11, 9.5, -1, 0, head_width=0.3, head_length=0.4, width=.06, fc='b', ec='b')","38570c1a":"corr = train.drop(columns=['drive_wheels', 'Horsepower', 'Num_of_doors', 'Wheel_base', 'Length', 'Width', 'Height', 'Curb_weight', 'Bore', 'Compression_ratio', 'Peak_rpm', 'City_mpg', 'Highway_mpg', 'fuel_system']).astype(\"float64\").corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nf, ax = plt.subplots(figsize=(16, 16))\nf.suptitle(\"Final plot of correlations. \\n14 from 25 parameters have been deleted.\", fontsize=20)\nsns.heatmap(corr, cbar=True, annot=True, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=1.2, cbar_kws={\"shrink\": .5}, ax=ax)","b2d80951":"train_after_del = train.drop(columns=['drive_wheels', 'Horsepower', 'Num_of_doors', 'Wheel_base', 'Length', 'Width', 'Height', 'Curb_weight', 'Bore', 'Compression_ratio', 'Peak_rpm', 'City_mpg', 'Highway_mpg', 'fuel_system']).astype(\"float64\")","dc49a9e8":"x = sm.add_constant(train_after_del)\ny = np.array(labels).astype(\"float64\")\nxnames = list(train_after_del.columns)\nxnames.insert(0, 'Intercept')\n#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nthird_model = sm.OLS(y, x)\nthird_results = third_model.fit()\nprint(third_results.summary(xname=xnames, yname='Price'))","9ecd1df9":"print(\"Despite we have deleted many features, the condition number is still too big:\", np.linalg.cond(third_results.model.exog))","8fe0f6c8":"train_after_del.head()","b57cc592":"## standarization for normalized losses column\nnorm_loss_std = (train_after_del.Normalized_losses - train_after_del.Normalized_losses.mean())\/train_after_del.Normalized_losses.std()\ntrain_after_del['Normalized_losses'] = train_after_del['Normalized_losses'].map(dict(zip(list(train_after_del.Normalized_losses), norm_loss_std)))\n## standarization for engine size column\nengine_size_std = (train_after_del.Engine_size - train_after_del.Engine_size.mean())\/train_after_del.Engine_size.std()\ntrain_after_del['Engine_size'] = train_after_del['Engine_size'].map(dict(zip(list(train_after_del.Engine_size), engine_size_std)))\n## standarization for stroke column\nstroke_std = (train_after_del.Stroke - train_after_del.Stroke.mean())\/train_after_del.Stroke.std()\ntrain_after_del['Stroke'] = train_after_del['Stroke'].map(dict(zip(list(train_after_del.Stroke), stroke_std)))\n## standarization for cylinders column\ncylinders_std = (train_after_del.cylinders - train_after_del.cylinders.mean())\/train_after_del.cylinders.std()\ntrain_after_del['cylinders'] = train_after_del['cylinders'].map(dict(zip(list(train_after_del.cylinders), cylinders_std)))\n## standarization for symboling column\nsymboling_std = (train_after_del.Symboling - train_after_del.Symboling.mean())\/train_after_del.Symboling.std()\ntrain_after_del['Symboling'] = train_after_del['Symboling'].map(dict(zip(list(train_after_del.Symboling), symboling_std)))\n## standarization for body style column\nbody_std = (train_after_del.body_style - train_after_del.body_style.mean())\/train_after_del.body_style.std()\ntrain_after_del['body_style'] = train_after_del['body_style'].map(dict(zip(list(train_after_del.body_style), body_std)))\n## standarization for brand column\nbrand_std = (train_after_del.brand - train_after_del.brand.mean())\/train_after_del.brand.std()\ntrain_after_del['brand'] = train_after_del['brand'].map(dict(zip(list(train_after_del.brand), brand_std)))","c4301134":"train_after_del.head()","71aa79c6":"x = sm.add_constant(train_after_del)\ny = np.array(labels).astype(\"float64\")\nxnames = list(train_after_del.columns)\nxnames.insert(0, 'Intercept')\n#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nfourth_model = sm.OLS(y, x)\nfourth_results = fourth_model.fit()\nprint(fourth_results.summary(xname=xnames, yname='Price'))","d602443e":"print(\"After standarizing we obtain the condition number:\", np.round(np.linalg.cond(fourth_results.model.exog), 4), \"and there is no warning regarding this parameter anymore.\")","b053f299":"xnames = list(train.astype('float64').columns)\nxnames.insert(0, 'Intercept')\nalpha = 0.05\nfirst_model_pv = {\"Features\": xnames, \"p-value\": np.round(first_results.pvalues, 6)} ## take features with p-values from the first model\nfirst_model_pv = pd.DataFrame(first_model_pv) ## convert into df type\ncols_to_delete = first_model_pv[first_model_pv[\"p-value\"]>alpha] ## take exceeding p-values only\ncols_to_delete","563ae35c":"## dropping all above features\ntrain_cols_del_pv = train.drop(columns=list(cols_to_delete.Features)[1:]).astype('float64')\ntrain_cols_del_pv.head()","ba90bb86":"## standarization for normalized losses column\nnorm_loss_std = (train_cols_del_pv.Normalized_losses - train_cols_del_pv.Normalized_losses.mean())\/train_cols_del_pv.Normalized_losses.std()\ntrain_cols_del_pv['Normalized_losses'] = train_cols_del_pv['Normalized_losses'].map(dict(zip(list(train_cols_del_pv.Normalized_losses), norm_loss_std)))\n## standarization for engine size column\nengine_size_std = (train_cols_del_pv.Engine_size - train_cols_del_pv.Engine_size.mean())\/train_cols_del_pv.Engine_size.std()\ntrain_cols_del_pv['Engine_size'] = train_cols_del_pv['Engine_size'].map(dict(zip(list(train_cols_del_pv.Engine_size), engine_size_std)))\n## standarization for stroke column\nbore_std = (train_cols_del_pv.Bore - train_cols_del_pv.Bore.mean())\/train_cols_del_pv.Bore.std()\ntrain_cols_del_pv['Bore'] = train_cols_del_pv['Bore'].map(dict(zip(list(train_cols_del_pv.Bore), bore_std)))\n## standarization for cylinders column\ncylinders_std = (train_cols_del_pv.cylinders - train_cols_del_pv.cylinders.mean())\/train_cols_del_pv.cylinders.std()\ntrain_cols_del_pv['cylinders'] = train_cols_del_pv['cylinders'].map(dict(zip(list(train_cols_del_pv.cylinders), cylinders_std)))\n## standarization for symboling column\npeak_std = (train_cols_del_pv.Peak_rpm - train_cols_del_pv.Peak_rpm.mean())\/train_cols_del_pv.Peak_rpm.std()\ntrain_cols_del_pv['Peak_rpm'] = train_cols_del_pv['Peak_rpm'].map(dict(zip(list(train_cols_del_pv.Peak_rpm), peak_std)))\n## standarization for body style column\nbody_std = (train_cols_del_pv.body_style - train_cols_del_pv.body_style.mean())\/train_cols_del_pv.body_style.std()\ntrain_cols_del_pv['body_style'] = train_cols_del_pv['body_style'].map(dict(zip(list(train_cols_del_pv.body_style), body_std)))\n## standarization for brand column\nbrand_std = (train_cols_del_pv.brand - train_cols_del_pv.brand.mean())\/train_cols_del_pv.brand.std()\ntrain_cols_del_pv['brand'] = train_cols_del_pv['brand'].map(dict(zip(list(train_cols_del_pv.brand), brand_std)))","ed411c5c":"x = sm.add_constant(train_cols_del_pv)\nx = np.array(x).astype(\"float64\")\ny = np.array(labels).astype(\"float64\")\nxnames = list(train.drop(columns=list(cols_to_delete.Features)[1:]).columns)\nxnames.insert(0, 'Intercept')\n#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nmodel_due_to_pv = sm.OLS(y, x)\nresults_due_to_pv = model_due_to_pv.fit()\nprint(results_due_to_pv.summary(xname=xnames, yname='Price'))","258107d1":"corr1 = train_cols_del_pv.corr()\nmask1 = np.zeros_like(corr1, dtype=np.bool)\nmask1[np.triu_indices_from(mask1)] = True\n\ncorr2 = train_after_del.corr()\nmask2 = np.zeros_like(corr2, dtype=np.bool)\nmask2[np.triu_indices_from(mask2)] = True\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 18))\nsns.heatmap(corr1, cbar=True, annot=True, mask=mask1, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=1.2, cbar_kws={\"shrink\": .5}, ax=ax1)\nax1.title.set_text(\"OLS Model: columns deleted due to p-value\\n11 from 25 variables left\\n R-squared: 0.854\")\nsns.heatmap(corr2, cbar=True, annot=True, mask=mask1, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=1.2, cbar_kws={\"shrink\": .5}, ax=ax2)\nax2.title.set_text(\"OLS Model: columns deleted due colinearity\\n11 from 25 variables left\\n R-squared: 0.849\")","12dc30ef":"#n=int(input(\"Please enter n: \\n\"))\nn=70\nx1 = sm.add_constant(train_cols_del_pv)\nx1 = np.array(x1).astype(\"float64\")\nx2 = sm.add_constant(train_after_del)\nx2 = np.array(x2).astype(\"float64\")\ny = np.array(labels).astype(\"float64\")\nglobal_mean = [[], []]\nfor j in range(n):\n    r_sq_mean = [[],[]]\n    for i in range(500):\n        x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y, test_size=0.2)\n        x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y, test_size=0.2)\n        model1 = sm.OLS(y1_train, x1_train)\n        model2 = sm.OLS(y2_train, x2_train)\n        results1 = model1.fit()\n        results2 = model2.fit()\n        pred1 = results1.predict(x1_test)\n        pred2 = results2.predict(x2_test)\n        r_sq1 = 1 - sum((pred1 - y1_test)**2) \/ sum((y1_test - y1_test.mean())**2)\n        r_sq2 = 1 - sum((pred2 - y2_test)**2) \/ sum((y2_test - y2_test.mean())**2)\n        r_sq_mean[0].append(r_sq1)\n        r_sq_mean[1].append(r_sq2)\n    if j%4==0:\n        print(\"Epoch\", j+1, \"- R-sqared for variables selected due to p-value:\", np.round(np.mean(r_sq_mean[0]), 5), \"- R-squared for variables selected due to colinearity:\", np.round(np.mean(r_sq_mean[1]), 5))\n    global_mean[0].append(np.round(np.mean(r_sq_mean[0])-np.mean(r_sq_mean[1]), 5))\nfor i in range(len(global_mean[0])):\n    if global_mean[0][i]>=0:\n        global_mean[1].append(1)\n    else:\n        global_mean[1].append(0)\nglobal_mean = np.array(global_mean)\nprint(\"\\nModel with variables selected due to p-value was better in\", np.round(np.sum(global_mean[1])\/n * 100, 2), \"% during\", n, \"epochs.\")\nprint(\"\\nAvearge difference between two r-square parameters is\", np.round(np.mean(global_mean[0]), 5))","a22122c8":"print(\"Error term for the first approach:\" ,np.round(fourth_results.resid.mean(), 10))\nprint(\"Error term for the second approach:\" ,np.round(results_due_to_pv.resid.mean(), 10))","ac615008":"from statsmodels.stats.diagnostic import normal_ad, lilliefors\nfrom statsmodels.stats.stattools import jarque_bera\nfrom scipy.stats import shapiro, kstest","c16d6ad3":"anderson_darling_test1 = normal_ad(fourth_results.resid)\nshapiro_wilk_test1 = shapiro(fourth_results.resid)\nlilliefors_test1 = lilliefors(fourth_results.resid)\nkolmogorov_smirnov_test1 = kstest(fourth_results.resid, 'norm')\njarque_bera_test1 = jarque_bera(fourth_results.resid)[:2]\n\narr1 = np.array(anderson_darling_test1+shapiro_wilk_test1+lilliefors_test1+kolmogorov_smirnov_test1+jarque_bera_test1).reshape(5,2)\nresidual_results_table1 = {\"Test statistic\": arr1[:,0], \"P-value\": np.round(arr1[:,1], 6)}\nresidual_results_df1 = pd.DataFrame(residual_results_table1, index=[\"Anderson-Darlin\", \"Shapiro-Wilk\", \"Lilliefors\", \"Kolmogorov-Smirnov\", \"Jarque-Bera\"])\nresidual_results_df1","bb004820":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\nres1 = fourth_results.resid\nres2 = results_due_to_pv.resid\nsm.qqplot(res1, line='s', ax=ax1)\nax1.title.set_text(\"Q-Q plot for the first approach\")\nsm.qqplot(res2, line='s', ax=ax2)\nax2.title.set_text(\"Q-Q plot for the second approach\")\nplt.show()","a5897677":"anderson_darling_test2 = normal_ad(results_due_to_pv.resid)\nshapiro_wilk_test2 = shapiro(results_due_to_pv.resid)\nlilliefors_test2 = lilliefors(results_due_to_pv.resid)\nkolmogorov_smirnov_test2 = kstest(results_due_to_pv.resid, 'norm')\njarque_bera_test2 = jarque_bera(results_due_to_pv.resid)[:2]\n\narr2 = np.array(anderson_darling_test2+shapiro_wilk_test2+lilliefors_test2+kolmogorov_smirnov_test2+jarque_bera_test2).reshape(5,2)\nresidual_results_table2 = {\"Test statistic\": arr2[:,0], \"P-value\": np.round(arr2[:,1], 6)}\nresidual_results_df2 = pd.DataFrame(residual_results_table2, index=[\"Anderson-Darlin\", \"Shapiro-Wilk\", \"Lilliefors\", \"Kolmogorov-Smirnov\", \"Jarque-Bera\"])\nresidual_results_df2","278eaa29":"from statsmodels.stats.diagnostic import het_breuschpagan\nfrom statsmodels.stats.diagnostic import het_goldfeldquandt\nfrom statsmodels.stats.diagnostic import het_white","928c59ef":"x = sm.add_constant(train_after_del)\nx = np.array(x).astype(\"float64\")\ny = np.array(labels).astype(\"float64\")\nhomo_breusche_pagan = het_breuschpagan(fourth_results.resid, x)\nhomo_goldfeld_quandt = het_goldfeldquandt(y, x, alternative='two-sided')\nhomo_white = het_white(fourth_results.resid, x)","960516eb":"arr3 = np.array(homo_breusche_pagan[:2]+homo_goldfeld_quandt[:2]+homo_white[:2]).reshape(3,2)\nhomo_results_table = {\"Test statistic\": arr3[:,0], \"P-value\": np.round(arr3[:,1], 6)}\nhomo_results_df = pd.DataFrame(homo_results_table, index=[\"Breusche-Pagan\", \"Goldfeld-Quandt\", \"White\"])\nhomo_results_df","0175be71":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 9))\nax1.scatter(fourth_results.predict(x), fourth_results.resid, c='gray', label='Residuals')\nax1.plot(np.linspace(50, 46450, 1000), np.zeros(1000), c='black', label=\"y=0 line\")\nax1.title.set_text(\"Predictions vs. Residuals\")\nax1.legend()\nax1.set_xlabel(\"Preditions\")\nax1.set_ylabel(\"Residuals\")\nax2.scatter(range(len(fourth_results.resid)), fourth_results.predict(x), c='g', label='Predictions')\nax2.scatter(range(len(fourth_results.resid)), y, c='b', label='True Values')\nax2.title.set_text(\"Prediction vs. True Values\")\nax2.set_xlabel(\"Number of a sample\")\nax2.set_ylabel(\"Price\")\nax2.legend()","f6976469":"from statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.stats.diagnostic import acorr_breusch_godfrey, acorr_ljungbox","423af51a":"durbin_watson_test = durbin_watson(fourth_results.resid)\nbreusch_godfrey_test = acorr_breusch_godfrey(fourth_results)[:2]\nljung_box_test = acorr_ljungbox(fourth_results.resid, boxpierce=True, lags=1)","61d7c4e3":"dw_df = {\"Test statistic\": durbin_watson_test}\ndw_df = pd.DataFrame(dw_df, index=[\"Durbin-Watson test\"])\ndw_df","0bcbb910":"bg_df = {\"Test statistic\": breusch_godfrey_test[0], \"p-values\": breusch_godfrey_test[1]}\nbg_df = pd.DataFrame(bg_df, index=[\"Breusch\u2013Godfrey test\"])\nbg_df","56304691":"arr4 = np.array(ljung_box_test).reshape(2,2)\n\nlj_df = {\"Test statistic\": arr4[:,0], \"p-value\": np.round(arr4[:,1], 8)}\nlj_df = pd.DataFrame(lj_df, index=[\"Ljung-Box test\", \"Box-Pierce test\"])\nlj_df","7cc1009d":"from statsmodels.stats.diagnostic import linear_rainbow, linear_harvey_collier","77e7faeb":"rainbow = linear_rainbow(fourth_results)\nrainbow_df = {\"Test statistic\": rainbow[0], \"p-value\": rainbow[1]}\nrainbow_df = pd.DataFrame(rainbow_df, index=[\"Rainbow test\"])\nrainbow_df","57c5e347":"harvey_collier = linear_harvey_collier(fourth_results)","8a406f15":"from statsmodels.stats.outliers_influence import variance_inflation_factor","c6a56acd":"multicoll = [variance_inflation_factor(np.array(train_after_del), i) for i in range(train_after_del.shape[1])]\nmulticoll_df = {\"Results\": multicoll}\nmulticoll_df = pd.DataFrame(multicoll_df, index=train_after_del.columns)\nmulticoll_df","e8f69b79":"print(\"The interpretation is the following: for Engine size variable we have the variance inflation factor equal to\", multicoll_df.Results[2], \"taking the square root, we get, that this means that the standard error for the coefficient of that predictor variable is\", np.round(np.sqrt(multicoll_df.Results[2]), 3), \"times larger than if that predictor variable had 0 correlation with the other predictor variables.\" )","e6b655cb":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nmulticoll2 = [variance_inflation_factor(np.array(train.astype('float64')), i) for i in range(train.shape[1])]\nmulticoll_df_ = {\"All variables\": multicoll2}\nmulticoll_df_ = pd.DataFrame(multicoll_df_, index=train.columns)\nmulticoll_df_","30728d8b":"cond_num = {\"1st Approach\": np.linalg.cond(fourth_results.model.exog), \"2nd Approach\": np.linalg.cond(results_due_to_pv.model.exog), \"All variables\": np.linalg.cond(first_results.model.exog)}\ncond_num = pd.DataFrame(cond_num, index=[\"Condition Number\"])\ncond_num","d99107de":"n=251\nplt.figure(figsize=(35,15))\n\nfor i in train_after_del.columns:\n    plt.subplot(n)\n    n = n + 1\n    x = train_after_del[str(i)]\n    x = np.array(x)\n    y = labels.astype('float64')\n    z = np.linspace(np.min(x)-0.5, np.max(x)+0.5, 1000)\n    line = fourth_results.params[0] + z*fourth_results.params[str(i)]\n    plt.scatter(x, y)\n    plt.plot(z, line)\n    plt.title(str(i))\n    if n>255:\n        break\nplt.show()\n\nn=251\nplt.figure(figsize=(35,15))\n\nfor i in train_after_del.columns[5:]:\n    plt.subplot(n)\n    n = n + 1\n    x = train_after_del[str(i)]\n    x = np.array(x)\n    y = labels.astype('float64')\n    z = np.linspace(np.min(x)-0.5, np.max(x)+0.5, 1000)\n    line = fourth_results.params[0] + z*fourth_results.params[str(i)]\n    plt.scatter(x, y)\n    plt.plot(z, line)\n    plt.title(str(i))\n    if n>255:\n        break\nplt.show()","b107af65":"from statsmodels.stats.outliers_influence import OLSInfluence\nfrom statsmodels.graphics.regressionplots import plot_leverage_resid2\nfrom yellowbrick.regressor import CooksDistance","b12d969f":"x = sm.add_constant(train_after_del)\nx_arr = np.array(x).astype(\"float64\")\ny = np.array(labels).astype(\"float64\")\nlist_=[]\n\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 9))\n\nx_train, x_test, y_train, y_test = train_test_split(x_arr, y, test_size=0.2)\nfinal_model = sm.OLS(y_train, x_train)\nxnames = list(train_after_del.astype('float64').columns)\nxnames.insert(0, 'Intercept')\nfinal_results = final_model.fit(xname=xnames, yname='Price')\nols_inf = OLSInfluence(final_results)\npred = final_results.predict(x_test)\nr_sq = 1 - sum((pred - y_test)**2) \/ sum((y_test - y_test.mean())**2)\nax1.plot(range(len(pred)), np.sort(pred), c='g', label='Predictions')\nax1.plot(range(len(pred)), np.sort(y_test), c='b', label='True Values')\nax1.set_xlabel(\"Sorted samples\")\nax1.set_ylabel(\"Price\")\nax1.legend()\nplot_leverage_resid2(final_results, ax=ax2)\n\nvisualizer = CooksDistance()\nvisualizer.fit(x_train, y_train)\nvisualizer.show(ax=ax3)\n\nprint(\"Final linear function is following:\")\nfor i in range(len(final_results.params)):\n    list_.append(str(np.round(final_results.params[i])) + \" * \" + str(x.columns[i]))\nprint('The final formula for the fitted line is:\\n\\n', '  +  '.join([lst for lst in list_]))\nprint(\"\\nTest R-square:\", r_sq)\nprint(\"\\nCondition number:\", np.linalg.cond(final_results.model.exog), '\\n')\nprint(final_results.summary())\nprint(\"\\nIn the below one can find another parameters that can be found in OLS Influence summary table:\\n\")\nprint(ols_inf.summary_table())","3957e6d2":"Based on p-values for all three tests, we can reject the $H_0$ hypothesis, which says that there is homoscedasticity. It can be verifed also by looking on the below plots. On the left one, we can see that errors are unstable and not symetric with any horizontal line. On the right one, we can observe how predictions fit the actual prices.","85816e1f":"Let us check, what we are left with and look on the correlation's plot again. It look much better, but still **horsepower** parameter seems to be not neccessary. It's hard to think that this information is not crucial for any car fan, but I think that it can be combine somehow by **engine size** and **aspiration** variables. We can also try to erase the parameter **drive wheel**, which hopefully, can be combine by **body style, engine size** and **location**.","f2d61511":"4. Autocorrelation of error terms.\n\nObservations of the error term should be uncorrelated with each other. In other words, one observation of the error term should not predict the next observation. For instance, if the error for one observation is positive and that systematically increases the probability that the following error is positive, that is a positive correlation. If the subsequent error is more likely to have the opposite sign, that is a negative correlation. This problem is known autocorrelation. We use two tests to verify the $H_0$ hypothesis, that there is no correlation betweeen error terms.","8a2d0aa0":"Before checking if assumptions of the linear regression model are satisfied, we can try another approach with selecting parameters that should be used. We can look once again on the first model, and check **p-values** for each variable. Assuming $\\alpha=0.05$ we can say that all parameters with $\\text{p-value}>\\alpha$ are statistically not neccessary and try delete them.","92f44466":"Let's start with only non-numerical feture, that is number of doors. What's going, we are looking for unique values, convert into numbers, and after checking that both missing values belong to sedan body style, wlog we can assume, that sedans have four doors.","bc26b97c":"The minus sign in the second case, means that without rounding this model underpredicts values, but it happens at most on $10^{-11}$ level.","d5714fb6":"## 5. Final observations and parameters.","72fddb32":"## 4. Checking assumptions for OLS Regression Model. ","f27041c8":"To decide which data should we work with, we can run the below. It takes two datasets separately, splits both into train and test sets, fits the models on train data, predicts values on x_test and finally compares them with y_test (it counts R-squared for each epoch\/model). Results are printed and kept in **global_mean** list. Finally we compute how often the model trained on the first data (newer) is better than the second one - trained on the data from the first approach.","4defd8c6":"1. The error term has a population mean of zero.\n\nIt simply \nmeans, we want to check if a vector of residues has a mean equal to zero. We want to avoid the case, once the model under- or overpredicts values.","b5ff2032":"It means, that probably all values in the dataset are of the string type and have to be converted into ints\/floats during processing. There is some theories how to handle with missing values. One says, that in case when the number of N\/A's exceeds 55-65% of all observations in a column, and this feature does not seem to be significant, we can concider deleting entire feature. Because of very small number of records (205 rows), I decided not to delete any row\/column contains N\/A value. For **Normalized_losses** one fifth of values are N\/A's. We can reassign (gruop) them into a new value and look, if it's distinguishable on the distribution and scatter plots:","c7ed0ecd":"Taking $\\alpha=0.05$ (or even $\\alpha=0.01$), p-values for all tests (especially for Shapiro-Wilk which is known as most precise) are smaller than $\\alpha$, so there is an evidence to reject $H_0$ hypothesis, which says, that error term is normally distributed. Furthermore, we can look on the **Q-Q plot**, that compares two sets of quantiles against one another. On the below, the closer points look like a straight line, the closer is the error term to be normally distributed. Both approaches are shown in the below, one can see that in the first case, points cover the red line a bit better and more symetric (please take a look on Y-axis in both cases).","7283f5b8":"All **?** signes are replaced, which can be checked by running the below:","a5cb9736":"6. Multicollinearity.\n\nThe above error arises due to the perfect correlation in data. It means, that there still can be a problem with multicollinearity. We can check that by Variance Inflation Factor test, as in the below:","907bce09":"First warning is on the bottom of the model summary. Since the condition number is huge, we have too many linearly dependent variables probably. There is no suprise that's happens. One can easily find a realtion between **length** and **width** of a car with its **wheel base, self weight, fuel usage, engine size,** and hence also with **bore and stroke, number of cylinders, horsepower's number**. On the other hand, **fuel type** is strongly correlated with **compression ratio**. **Height** is related with **body style**, which is somehow related with **doors number**. **Fuel usage** in a **city** is colinear with **highway's one**. Since we want to predict the price of a car, we should think what kind of parameters are thought as neccessary when someone wants to measure the price. On the below plot I signed all parameters that show strong colinearity with another ones.","4fd52ffb":"As we can see in **Normalized_losses** column, missing values are marked by **\"?\"**  sign. Since it's not very comfortable to me to work with the **names** (description) file, I created own table, that contain all nesseccary informations about N\/A's.","6f68b12e":"On the above one could observe, we get quite nice $R^2$. However, the set is not splitted on train and test part, so that the model cannot be checked on 'unknown' data. In the below, we can check, how the model behaves, by taking the mean of r^2 after running n-times on randomly splitted data.","1411532e":"2. The error term is normally distributed.\n\nWe use five tests to check that assumption: Kolmogorov-Smirnov, Shapiro\u2013Wilk, Anderson\u2013Darling, Jarque\u2013Bera and  Lilliefors.","51bb0298":"For Durbin-Watson test we use a significance table (https:\/\/www3.nd.edu\/~wevans1\/econ30331\/Durbin_Watson_tables.pdf). For $\\alpha=0.05$ and population around 200, the boundaries are following:\n$\\\\ \\text{dL} = 1.665, \\ \\text{dU} = 1.874 $. Since the test statistic is equal to $1.112$, there is statistical evidence that the error terms are positively autocorrelated.","0e785357":"Roughly speaking, the Rainbow test is checking if some good linear fit can be achieved on a subsample in the \"middle\" of the data, even if the true relationship is non-linear. The $H_0$ hypothesis is rejected whenever the overall fit is significantly worse than the fit for the subsample. For alpha equal to 0.01, 0.05 or even 0.1, p-value is still greater, so there is no reason to reject the hypothesis regarding the linearity.","bb0fd923":"The reason may follows from the fact that values are not standarized yet, so that the model gets too big numbers at the beginning. We standarize parameters: **normalized losses, engine size, stroke, symboling, body style, brand** and **cylinders**. Let us see that no dummy update is needed, since all numerical variables are binary or already have required order.","5afdac17":"3. The error term has a constant variance - homoscedasticity.\n\nThe variance of the errors should be consistent for all observations. In other words, the variance does not change for each observation or for a range of observations.","b32deb14":"Next tool which can be helpful graphical visualiation of data. On the below we can see (literally), how each feature fits with price variable. On each plot, there is the fitted line, calculated by the model.","02e42ee5":"As we on both plots, assigned number **400** can be easily distinguish from the rest values, so it can be used in the original file too.","0b98ecec":"As we can see above, there are only three continuous variables left, which is not lucky case for linear regression model. Most of continuous features have been deleted due to very strong collinearity with another variables. Anyway, as we can see on the below, predicted prices are not as bad as one could assume they should be, especially after whole analysis. On the middle plot we can see, only few records have quite high laverage value, the rest is centered which means the number of outliers is quite small. On the right plot, we measure Cook's distance - observations with high value of this measure should be considered deeper in the analysis. To get precise values of Cook's distance (and another paramters listed below) we can use OLS Influence library.","fe11f532":"## Conclusion.\n\nAs we see, there is some conditions the data has to satisfy, to make linear regression model validate, and make our considerations accurate. From the above, we can see that OLS linear regression model is probably not the best one to choose. Based on that result, we could check how another regression models would work here (starting with Weigthed Least Squares (WLS) Regression Model). Anyway, each model has different assumpitions that dataset should satisfy, so once we are facing noised data with high linear corelations, many categorical features, small number of records and so on, preparing data and finding appropriate model can be not trivial task. Anyway, I hope that some considerations written here will be helpful somehow for you.","20b5a3cc":"Almost each time, once the above has been runnuning, new data were giving better $R^2$ value. On the other hand, the average difference between both cases is around $0.005$, so for rounding up to 2 digits after a coma, we would yield the mean difference equal to $0.01$. However, deeper looking into another model's parameters, makes the first approach a bit better in my opinion. Details will be provided below, once we are checking, if assumptions, that guarantee OLS regression model is working properly, are satisfied.","986f53d8":"The global parameters, that describes multicollinearity is the condition number and it can be found in the OLS model summary. We can verify three values of it:","5a3abf65":"Once missing values are managed and all categorical features are changed into numerical ones, we can start with looking for correlations between parameters. After checking column's names, it seems we have a lot of useless (dependent) paramters, that have to be deleted. Our goal is to find the most significant parameters that are enough to our prediction task. We can start with running first OLS regression model (to check how it behaves), then look on the correlation's plot.","47437a64":"Once the homoscedasticity has to be rejected, one can try use WLS model (Weighted Least Squares) instead of OLS (Ordinary Least Squares) one. It has different assumptions to satisfy, I will try to test it on this dataset in the future. Anyway, now we are going to check next point.","c4ff5ba1":"## 3. Second approach - selecting features due to p-value. ","fbdf1d8e":"The Q-Q plot has explanation once looking on the below table (for the second approach). We can see that in this case, p-values are even smaller in each test, so it means the first approach 'is closer' to have the error term normally distributed.","258fbde5":"Breusch-Godfrey, Ljung-Box and Box-Pierce tests also show that there is correlation between the error term. It can follows from the fact, that there are independent variable missing from the model. The positive autocorrelation increases the variance of the coefficient estimates and estimated standard errors given by ordinary least squares will be smaller than the true values. It can make the crucial parameters like t-statistic, F-statistic, R-squared and adjusted R-squared inaccurate. These symptoms can be fatal for any model, so like in the previous assumption. Either model should be changed or we are facing problematic data.\n\nSo far one could observe, that most of assumptions are not satisfied or, at least, partially not satisfied. So that, one could ask, if it is a good idea to run linear regression model on this data. In other words, one wants to check if the data has linear structure. To satisfy this assumption, the correctly specified model must just fit the linear pattern.\n\n5. Linear structure - model is linear in the coefficients and the error term.\n\nObviously, in general, especially in real projects, this assumption should be checked as the first one. Without linearity in data, it's hardly possible to get accurate and true results on the output. However, my goal here is showing and analysing methods for checking all these assumptions, and in my opinion, it's still interesting (from the learning's point of view, not the best predition's one), to look what can happen once some assumption is not satisfied, why that occurs and so on. Going back for the linear structre, again we use statistical tests to verify that.","b7d4701c":"After standarization and running model on new data, we can check, if the left variables are colinear or not. What is interesting, from the correlation plot, it seems that features in new data are more linearly dependent than previously (condition number is higher in the second model). Moreover in both models there are 11 features left. However, $R^2$ score is a bit higher in the new model, $0.854$ vs. $0.849$. ","242463e2":"As on the below, we are left with features od percentage leve of N\/A's arounf 1-2%. We will go through all particularly.","e27fa83e":"One should be careful once looking on the **Type** column. It says, that all listed features have a string type of data, which is not true:","f8d535f8":"The results of the Variance Inflation Factor test are not perfect, but they are the best we have obtained. On the below we can check reults, once testing the dataset before selecting best features.","8c29fe1c":"### 1. First touch of a data - importing and handling with missing values.","9bc21d10":"## 2. First approach - reducing colinearity and OLS model training.","cafd8583":"In the above we can see that **names** file is rather a description file. We can find here names for columns (as by default there are no headers in data file) and the last 'table' is the information regarding missing values. So that we can import data once again with known header as below."}}