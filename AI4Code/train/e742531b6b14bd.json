{"cell_type":{"dbbeb0fb":"code","db507fd2":"code","e049ca48":"code","2f67cc7b":"code","02e7240f":"code","3bf5baed":"code","530f91e5":"code","7e927117":"code","66eacea3":"code","0f072c49":"code","e5bdc8d4":"code","b7d36861":"code","7765d9b8":"code","f6f2f5b9":"code","ae3d64c0":"code","58c7de17":"code","452342a0":"code","920ef0b6":"code","d81279b9":"code","a94f9b1b":"code","9ad3a644":"code","dfd07f7c":"code","5e9e2097":"code","6a518e2c":"code","a00abb5d":"code","81dd5136":"code","f5c42045":"markdown","5159d7a3":"markdown","35df27af":"markdown","5e712226":"markdown","7777b252":"markdown","305152eb":"markdown","3d48b4ed":"markdown","e3af1ed4":"markdown","98691cad":"markdown","882bb6f5":"markdown","1f0ae6dc":"markdown","ec764944":"markdown","23b40646":"markdown","32065254":"markdown"},"source":{"dbbeb0fb":"# I don't like warning messages too much\nimport warnings\nwarnings.filterwarnings(\"ignore\")","db507fd2":"import os\nimport gc\nimport cv2\nimport glob\nimport h5py\nimport shutil\nimport itertools\nimport random as rn\n\nimport imgaug as aug\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom pathlib import Path\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport imgaug.augmenters as iaa\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout\nfrom keras.layers import Input, Flatten, BatchNormalization, Lambda\nfrom keras.layers import CuDNNGRU, CuDNNLSTM, Bidirectional, LSTM, GRU\nfrom keras.layers import Add, Concatenate, Reshape\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import to_categorical\n\nfrom keras import backend as K\nimport tensorflow as tf\n\n\ncolor = sns.color_palette()\n%matplotlib inline\n%config InlineBackend.figure_format=\"svg\"","e049ca48":"## I said it earlier as well, do everything you can to make your results reproducible. It matters!!\n\n# Set the seed for hash based operations in python\nos.environ['PYTHONHASHSEED'] = '0'\n\nseed=1234\n\n# set the seed for random number generator\nrn.seed(seed)\n\n# Set the numpy seed\nnp.random.seed(seed)\n\n# Set the random seed in tensorflow at graph level\ntf.set_random_seed(seed)\n\n# We will be using CuDNN implementation of RNNs which already is non-reproducible\n# So I am trying to get results as close as possible on different runs\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n                              inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\n# Make the augmentation sequence deterministic\naug.seed(seed)","2f67cc7b":"# path to the data directory\ndata_dir = Path(\"..\/input\/captcha\/captcha\/\")\n\n# getting list of all images\nimages = list(data_dir.glob(\"*.png\"))\nprint(\"Number of images found: \", len(images))","02e7240f":"# Let's take a look at some samples first. \n# Always look at your data!\nsample_images = images[:4]\n\nf,ax = plt.subplots(2,2, figsize=(5,3))\nfor i in range(4):\n    img = imread(sample_images[i])\n    print(\"Shape of image: \", img.shape)\n    ax[i\/\/2, i%2].imshow(img)\n    ax[i\/\/2, i%2].axis('off')\nplt.show()","3bf5baed":"# make a set of all unique characters. Letter is a wrong\n# word to use here but I am gonna use it for now.\nletters = set()\n\n# A list to store the max length for each catcha\nlengths = []\n\n# Iterate over each image. The name of the image is the \n# text ccontained in it. \nfor image in images:\n    image_name = str(image.name).split(\".\")[0]\n    lengths.append(len(image_name))\n    for ch in image_name:\n        letters.add(ch)\n\n# Sort the letters        \nletters = sorted(letters)\nprint(\"Number of unqiue letters in the whole dataset: \", len(letters))\nprint(\"Maximum length of any captcha: \", max(Counter(lengths).keys()))\nprint(\"\\nAll letters to be considered: \")\nprint(letters)","530f91e5":"dataset = []\n\nfor image in images:\n    image_path = str(image)\n    label = str(image.name).split(\".\")[0]\n    dataset.append((image_path, label))\n\ndataset = pd.DataFrame(dataset, columns=[\"img_path\", \"label\"], index=None)\ndataset = dataset.sample(frac=1.).reset_index(drop=True)\nprint(\"Total number of samples in the dataset: \", len(dataset))\ndataset.head(10)","7e927117":"# split into train and validation sets\ntraining_data, validation_data = train_test_split(dataset, test_size=0.1, random_state=seed)\n\ntraining_data = training_data.reset_index(drop=True)\nvalidation_data = validation_data.reset_index(drop=True)\n\nprint(\"Number of training samples: \", len(training_data))\nprint(\"Number of validation samples: \", len(validation_data))","66eacea3":"# function to create labels from text\ndef text_to_labels(text):\n    return list(map(lambda x: letters.index(x), text))\n\n# function to convert labels back to texts\ndef labels_to_text(label):\n    return ''.join(list(map(lambda x: letters[int(x)], label)))\n\n# sanity-check for letters\ndef is_valid_str(s):\n    for ch in s:\n        if not ch in letters:\n            return False\n    return True","0f072c49":"def build_data(df, resize=True, img_height=24, img_width=72):\n    \"\"\"This function reads samples from a dataframe and store\n    the image values and labels in two separate arrays.\n    \n    Args:\n        df        : dataframe from which we want to read the data\n        resize    : whether to resize images or not\n        img_weidth: width of images to be considered\n        img_height: height of images to be considered\n        \n    Returns:\n        images    : numpy array of images\n        labels    : numpy array of encoded labels\n    \"\"\"\n    n = len(df)\n    images = np.zeros((n, img_height, img_width), dtype=np.float32)\n    labels = [0]*n\n    for i in range(n):\n        img = cv2.imread(df[\"img_path\"][i])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        \n        if resize:\n            img = cv2.resize(img, (img_width, img_height))\n        \n        img = (img\/255.).astype(np.float32)\n        label = df[\"label\"][i]\n        \n        # only add to if all the charaters are valid\n        if is_valid_str(label):\n            images[i, :, :] = img\n            labels[i] = label\n    \n    return images, np.array(labels)","e5bdc8d4":"# Building training data\ntraining_images, training_labels = build_data(training_data)\nprint(\"Number of training images: \", training_images.shape)\nprint(\"Number of training labels: \", training_labels.shape)","b7d36861":"# Building validation data\nvalidation_images, validation_labels = build_data(validation_data)\nprint(\"Number of validation images: \", validation_images.shape)\nprint(\"Number of validation labels: \", validation_labels.shape)","7765d9b8":"# Check some samples again\nf,ax = plt.subplots(4,2, figsize=(8,5))\nfor i in range(4):\n    ax[i\/\/2, i%2].imshow(training_images[i], cmap='gray')\n    ax[i\/\/2, i%2].set_title(training_labels[i])\n    ax[i\/\/2, i%2].axis('off')\n\nfor i in range(4, 8):\n    ax[i\/\/2, i%2].imshow(validation_images[i], cmap='gray')\n    ax[i\/\/2, i%2].set_title(validation_labels[i])\n    ax[i\/\/2, i%2].axis('off')\n    \nplt.show()","f6f2f5b9":"def data_generator(df, \n                   batch_size, \n                   img_width, \n                   img_height, \n                   downsample_factor, \n                   max_text_len, \n                   is_validation_data=False):\n    \"\"\"This is a data generator which yields batches \n    of (image, label) pairs.\n    \n    Args:\n        df                : training or validation dataframe\n        batch_size        : batch size ti be used during training\n        img_width         : width of images to be considered  \n        img_height        : height of images to be considered\n        downsample_factor : by what factor the CNN has downsampled the images\n        max_text_len      : maximum length of the text in your data\n        is_validation_data: is the data being considered a validation data?\n        \n    Returns:\n        inputs: numpy array containg inputs that are required for the final model\n        outputs: a dummy array of zeros \n    \"\"\"\n    n = len(df)\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n    nb_batches = int(np.ceil(n\/batch_size))\n    \n    if not is_validation_data:\n        images, texts = training_images, training_labels\n    else:\n        images, texts = validation_images, validation_labels\n    \n    batch_images = np.ones((batch_size, img_width, img_height, 1), dtype=np.float32)\n    batch_labels = np.ones((batch_size, max_text_len), dtype=np.float32)\n    input_length = np.ones((batch_size, 1), dtype=np.int64) * \\\n                                            (img_width \/\/ downsample_factor - 2)\n    label_length = np.zeros((batch_size, 1), dtype=np.int64)\n    \n    while True:\n        for i in range(nb_batches):\n            idx_to_consider = indices[i*batch_size:(i+1)*batch_size]\n            \n            for j, idx in enumerate(idx_to_consider):\n                img = images[idx].T\n                img = np.expand_dims(img, axis=-1)\n                text = texts[idx]\n                \n                if is_valid_str(text):\n                    label = text_to_labels(texts[idx])\n                    batch_images[j] = img\n                    batch_labels[j] = label\n                    label_length[j] = len(text)\n\n            inputs = {\n            'input_data': batch_images,\n            'input_label': batch_labels,\n            'input_length': input_length,\n            'label_length': label_length,\n            }\n            \n            outputs = {'ctc_loss': np.zeros([batch_size], dtype=np.float32)}\n            yield inputs, outputs","ae3d64c0":"# batch size to be used for training\nbatch_size = 32\n\n# image dimensions\nimg_width=72\nimg_height=24 \n\n# by what factor the image has been downsampled by the CNN part?\ndownsample_factor=4\n\n# maximum length of any text in the data\nmax_text_len=4","58c7de17":"# Get a generator object for the training data\ntrain_data_generator = data_generator(training_data, \n                                      batch_size=batch_size, \n                                      img_width=img_width, \n                                      img_height=img_height, \n                                      downsample_factor=downsample_factor, \n                                      max_text_len=max_text_len, \n                                      is_validation_data=False)\n\n# Get a generator object for the validation data \nvalid_data_generator = data_generator(validation_data, \n                                      batch_size=batch_size, \n                                      img_width=img_width, \n                                      img_height=img_height, \n                                      downsample_factor=downsample_factor, \n                                      max_text_len=max_text_len, \n                                      is_validation_data=True)","452342a0":"# A handy-dandy function for checking the generator output\n# always sanity-check your data before passing it to the model\ndef visualize_data_gen_output(data_gen, samples_to_visualize=2):\n    for i, (inp, out) in enumerate(data_gen):\n        print('Text generator output (data which will be fed into the neutral network):')\n        print('1)the_input (image)')\n        img = (inp['input_data'][i, :, :, 0]*255).astype(np.uint8)\n        plt.imshow(img.T, cmap='gray')\n        plt.show()\n        print(f\"2) the_labels(captcha) {labels_to_text(inp['input_label'][i])} is encoded as {list(map(int, inp['input_label'][i]))}\") \n        print(f\"3) input_length (width of image that is fed to the network after CNN): {inp['input_length'][i][0]} == (72\/4 - 2)\")\n        print(f\"4) label_length (length of captcha): {inp['label_length'][i][0]}\")\n        print(\" \")\n        if i==samples_to_visualize:\n            break","920ef0b6":"visualize_data_gen_output(train_data_generator)","d81279b9":"visualize_data_gen_output(valid_data_generator)","a94f9b1b":"# We will be using this loss function as the output\n# The loss function in model.compile(..) will be a dummy one\n# This is different from a normal scenario where you pass an actual \n# loss function when you compile the model\ndef ctc_lambda_func(args):\n    y_pred, labels, input_length, label_length = args\n    # the 2 is critical here since the first couple outputs of the RNN\n    # tend to be garbage:\n    y_pred = y_pred[:, 2:, :]\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)","9ad3a644":"def build_model():\n    # Inputs to the model\n    input_img = Input(shape=(img_width, img_height, 1), name='input_data', dtype='float32')\n    labels = Input(name='input_label', shape=[max_text_len], dtype='float32')\n    input_length = Input(name='input_length', shape=[1], dtype='int64')\n    label_length = Input(name='label_length', shape=[1], dtype='int64')\n    \n    # Convolution part for feaure extraction\n    x = Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same', name='Conv1')(input_img)\n    x = MaxPooling2D((2,2), name='pool1')(x)\n    x = Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same', name='Conv2')(x)\n    x = MaxPooling2D((2,2), name='pool2')(x)\n    \n    # Reshape the features for passing to RNN\n    # We have used two max pool with pool size and strides of 2. Hence, downsampled is 4x smaller\n    # Also, the number of filters in the last layer is 64.\n    new_shape = ((img_width \/\/ 4), (img_height \/\/ 4)*64)\n    x = Reshape(target_shape=new_shape, name='reshape')(x)\n    x = Dense(64, activation='relu', name='dense1')(x)\n    \n    # RNNs\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True,  name='lstm_1'), name='bi_1')(x)\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True,  name='lstm_2'), name='bi_2')(x)\n    \n    # final part\n    x = Dense(len(letters)+1, activation='softmax', name='dense2', kernel_initializer='he_normal')(x)\n    \n    # Get the CTC loss and represent it in a layer\n    output = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc_loss')([x, labels, input_length, label_length])\n    \n    # define the final model\n    model = Model([input_img, labels, input_length, label_length], output, name='ocr_model_v1')\n    \n    # optimizer\n    sgd = SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n    \n    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n    # this is the reason we have this ctc_loss array of zeros in our generator\n    model.compile(loss={'ctc_loss': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n    return model","dfd07f7c":"model = build_model()\nmodel.summary()","5e9e2097":"# things required for starting the training \nnb_epochs = 50\nnb_train_steps = training_data.shape[0] \/\/ batch_size\nnb_validation_steps = validation_data.shape[0] \/\/ batch_size\nes = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nckpt = ModelCheckpoint(filepath='ocr_v2.h5', save_best_only=True, monitor='val_loss')","6a518e2c":"# Train the model\nhistory = model.fit_generator(train_data_generator, \n                    epochs=nb_epochs, \n                    steps_per_epoch=nb_train_steps, \n                    validation_data=valid_data_generator, \n                    validation_steps=nb_validation_steps,\n                    callbacks=[es, ckpt])","a00abb5d":"# A utility to decode the output of the network\ndef decode_batch_predictions(pred):\n    pred = pred[:, 2:]\n    input_len = np.ones(pred.shape[0])*pred.shape[1]\n    \n    # Use greedy search. For complex tasks, you can use beam search\n    results = K.get_value(K.ctc_decode(pred, \n                                   input_length=input_len,\n                                   greedy=True)[0][0])\n    \n    # Iterate over the results and get back the text\n    texts = []\n    for res in results:\n        outstr = ''\n        for c in res:\n            if c < len(letters):\n                outstr += letters[c]\n        texts.append(outstr)\n    \n    # return final text results\n    return texts","81dd5136":"# Get the input output layer and define a Keras function\n# It is similar to getting layers in tensorflow and \n# passing the information to the session.\noutput_func = K.function([model.get_layer(name='input_data').input],\n                        [model.get_layer(name='dense2').output])\n\n\n#  Let's check results on some validation samples\nfor p, (inp_value, _) in enumerate(valid_data_generator):\n    bs = inp_value['input_data'].shape[0]\n    X_data = inp_value['input_data']\n    labels = inp_value['input_label']\n    \n    preds = output_func([X_data])[0]\n    pred_texts = decode_batch_predictions(preds)\n    \n    \n    orig_texts = []\n    for label in labels:\n        text = ''.join(list(map(lambda x: letters[int(x)], label)))\n        orig_texts.append(text)\n        \n    for i in range(bs):\n        print(f'GT: {orig_texts[i]} \\t Predicted: {pred_texts[i]}')\n    break","f5c42045":"### 10. Testing the model","5159d7a3":"So, we have around `10K` samples of data in total. The height, width and depth of images in the dataset are `24`, `72`, and `3` respectively. Becaue for a Captcha color isn't an important property, hence we will be using `grayscale` images for our use-case. Also, the images are already very small in size, so we won't be resizing them to some other size but if you want to experiemnt with that as well, feel free to do so. Everything is modular and requires only few changes.\n\nBefore that we need to know:\n* What all `unique` alphanumeric characters are present in the images?\n* What is the maximum length of any captcha? (This is needed to encode captcha to vectors)\n\nLet's find out.","35df27af":"### 5. Some helper functions for pre-processing and post-processing","5e712226":"Now that we have everything in place. It's time to start work towards a model building process but before that, we need to define a data generator. Pay attention to the details of the generator here. It is not a trivial generator. Don't worry about all the things that are defined inside the generator for now, you will get them fully once we have built our final model.\n\n### 7. Data generator ","7777b252":"### 4. Separate out training and validation datasets. ","305152eb":"### 6. Store the data in memory(if you can!)","3d48b4ed":"`pandas` dataframe is my favorite data structure to work with. We will make a dataframe that contains the path to the images and the corresponding labels\/texts in the images.","e3af1ed4":"### 8. CTC loss function","98691cad":"**Hooray!!** We have successfully build a captcha cracker with minimal efforts. That's it folks! I hope you have enjoyed this. If yes, please don't forget to **upvote** to show your appreciation.","882bb6f5":"### 3. Load the data ","1f0ae6dc":"Hello everyone! As usual, I was looking for some cool datasets that have been uploaded recently on Kaggle and I came across this dataset. I have been thinking to work on a similar kind of problem for long and I finally selected this one. Today we will be building an OCR using deep learning. Well, all the ideas presented here aren't new and much of inspiration for this comes from [this example](https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/image_ocr.py) provided in the `Keras` repository.","ec764944":"### 1. Import the required libraries","23b40646":"### 2. Reproducibility isn't a requirement, it's a necessity!","32065254":"### 9. Model "}}