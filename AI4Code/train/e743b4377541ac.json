{"cell_type":{"a2980aa5":"code","d8e9168a":"code","76a5bc91":"code","5ae222ea":"code","dee0c4ba":"code","1599d92a":"code","4aac4493":"code","5087463e":"code","1d554d87":"code","1590e4d5":"code","eb736333":"code","e071ec04":"code","2717a921":"code","134297a4":"code","4547d929":"code","9bc41dfb":"code","447508a2":"code","ce87b042":"code","fa6fb771":"markdown","787fcd20":"markdown","59e8cc85":"markdown","4316d5be":"markdown","7a06c17c":"markdown","f32e3366":"markdown","e184b14a":"markdown","be0296e5":"markdown","122d4868":"markdown"},"source":{"a2980aa5":"# Check nvcc version\n!nvcc -V\n# Check GCC version\n!gcc --version","d8e9168a":"# install mmcv-full thus we could use CUDA operators\n!pip install mmcv-full","76a5bc91":"# install dependencies: (use cu101 because colab has CUDA 10.1)\n!pip install -U torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n\n# Install mmdetection\n!rm -rf mmdetection\n!git clone https:\/\/github.com\/open-mmlab\/mmdetection.git\n%cd mmdetection\n\n!pip install -e .\n\n# install Pillow 7.0.0 back in order to avoid bug in colab\n!pip install Pillow==7.0.0","5ae222ea":"import copy\nimport os.path as osp\nfrom os import listdir\nimport xml\nfrom xml.dom.minidom import parse\nimport xml.dom.minidom\n\nimport mmcv\nimport numpy as np\n\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset","dee0c4ba":"# Check Pytorch installation\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\n# Check MMDetection installation\nimport mmdet\nprint(mmdet.__version__)\n\n# Check mmcv installation\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())","1599d92a":"!mkdir checkpoints\n!wget -c https:\/\/open-mmlab.s3.ap-northeast-2.amazonaws.com\/mmdetection\/v2.0\/mask_rcnn\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth \\\n      -O checkpoints\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth","4aac4493":"# Let's take a look at the dataset image\nimport mmcv\nimport matplotlib.pyplot as plt\n\nimg = mmcv.imread('\/kaggle\/input\/robomaster-final\/robomaster_Final Tournament\/image\/VW_CH3ENTERPRIZEVsTaurus_BO2_1_10.jpg')\nplt.figure()\nplt.imshow(mmcv.bgr2rgb(img))\nplt.show()","5087463e":"from sklearn.model_selection import train_test_split\nimport codecs\nfrom os import listdir\n\nimg_list = listdir('\/kaggle\/input\/robomaster-final\/robomaster_Final Tournament\/image')\n\ntrain_img, test_img = train_test_split(img_list, test_size=0.4, random_state=42)\ntest_img, val_img = train_test_split(test_img, test_size=0.4, random_state=42)\nwith codecs.open('train.txt', 'w') as f:\n    for l in train_img:\n        f.write(l.strip('.jpg')+'\\n')\nwith codecs.open('test.txt', 'w') as f:\n    for l in test_img:\n        f.write(l.strip('.jpg')+'\\n')\nwith codecs.open('val.txt', 'w') as f:\n    for l in val_img:\n        f.write(l.strip('.jpg')+'\\n')","1d554d87":"!cat val.txt","1590e4d5":"import copy\nimport os.path as osp\n\nimport mmcv\nimport numpy as np\n\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\n\ndef get_bboxes(path):\n    bboxes = []\n    DOMTree = xml.dom.minidom.parse(path)\n    annotation = DOMTree.documentElement\n\n    objects = annotation.getElementsByTagName(\"object\")\n\n    for obj in objects:\n        coor = obj.getElementsByTagName('bndbox')\n        xmin = int(float(coor[0].getElementsByTagName('xmin')[0].childNodes[0].data))\n        ymin = int(float(coor[0].getElementsByTagName('ymin')[0].childNodes[0].data))\n        xmax = int(float(coor[0].getElementsByTagName('xmax')[0].childNodes[0].data))\n        ymax = int(float(coor[0].getElementsByTagName('ymax')[0].childNodes[0].data))\n        bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return bboxes\n\ndef get_labels(path):\n    labels = []\n    DOMTree = xml.dom.minidom.parse(path)\n    annotation = DOMTree.documentElement\n\n    objects = annotation.getElementsByTagName(\"object\")\n\n    for obj in objects:\n        label = obj.getElementsByTagName('name')[0].childNodes[0].data\n        labels.append(label)\n\n    return labels\n\n@DATASETS.register_module()\nclass RMDataset8(CustomDataset):\n\n    CLASSES = ('car', 'watcher', 'base', 'armor')\n\n    def load_annotations(self, ann_file):\n        cat2label = {k: i for i, k in enumerate(self.CLASSES)}\n        # load image list from file\n        ann_file = ann_file[31:]\n        print(ann_file)\n        image_list = mmcv.list_from_file(ann_file)\n    \n        data_infos = []\n        # convert annotations to middle format\n        for image_id in image_list:\n            filename = f'{self.img_prefix}\/{image_id}.jpg'\n            image = mmcv.imread(filename)\n            height, width = image.shape[:2]\n    \n            data_info = dict(filename=f'{image_id}.jpg', width=width, height=height)\n    \n            # load annotations\n            label_prefix = self.img_prefix.replace('image', 'image_annotation')\n            anno_path = osp.join(label_prefix, f'{image_id}.xml')\n    \n            bbox_names = get_labels(anno_path)\n            bboxes = get_bboxes(anno_path)\n    \n            gt_bboxes = []\n            gt_labels = []\n            gt_bboxes_ignore = []\n            gt_labels_ignore = []\n    \n            # filter 'DontCare'\n            for bbox_name, bbox in zip(bbox_names, bboxes):\n                if bbox_name in cat2label:\n                    gt_labels.append(cat2label[bbox_name])\n                    gt_bboxes.append(bbox)\n                else:\n                    gt_labels_ignore.append(-1)\n                    gt_bboxes_ignore.append(bbox)\n\n            data_anno = dict(\n                bboxes=np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n                labels=np.array(gt_labels, dtype=np.long),\n                bboxes_ignore=np.array(gt_bboxes_ignore,\n                                       dtype=np.float32).reshape(-1, 4),\n                labels_ignore=np.array(gt_labels_ignore, dtype=np.long))\n\n            data_info.update(ann=data_anno)\n            data_infos.append(data_info)\n\n        return data_infos","eb736333":"from mmcv import Config\ncfg = Config.fromfile('.\/configs\/faster_rcnn\/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py')","e071ec04":"from mmdet.apis import set_random_seed\ndataset = 'RMDataset8'\n\n# Modify dataset type and path\ncfg.dataset_type = dataset\ncfg.data_root = '\/kaggle\/input\/robomaster-final\/'\n\ncfg.data.test.type = dataset\ncfg.data.test.data_root = '\/kaggle\/input\/robomaster-final\/'\ncfg.data.test.ann_file = 'test.txt'\ncfg.data.test.img_prefix = 'robomaster_Final Tournament\/image'\n\ncfg.data.train.type = dataset\ncfg.data.train.data_root = '\/kaggle\/input\/robomaster-final\/'\ncfg.data.train.ann_file = 'train.txt'\ncfg.data.train.img_prefix = 'robomaster_Final Tournament\/image'\n\ncfg.data.val.type = dataset\ncfg.data.val.data_root = '\/kaggle\/input\/robomaster-final\/'\ncfg.data.val.ann_file = 'val.txt'\ncfg.data.val.img_prefix = 'robomaster_Final Tournament\/image'\n\n# modify num classes of the model in box head\ncfg.model.roi_head.bbox_head.num_classes = 4\n# We can still use the pre-trained Mask RCNN model though we do not need to\n# use the mask branch\ncfg.load_from = 'checkpoints\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n\n# Set up working dir to save files and logs.\ncfg.work_dir = '.\/tutorial_exps'\n\n# The original learning rate (LR) is set for 8-GPU training.\n# We divide it by 8 since we only use one GPU.\ncfg.optimizer.lr = 0.02 \/ 8\ncfg.lr_config.warmup = None\ncfg.log_config.interval = 10\ncfg.total_epochs = 2\n\n# Change the evaluation metric since we use customized dataset.\ncfg.evaluation.metric = 'mAP'\n# We can set the evaluation interval to reduce the evaluation times\ncfg.evaluation.interval = 12\n# We can set the checkpoint saving interval to reduce the storage cost\ncfg.checkpoint_config.interval = 1\n\n# Set seed thus the results are more reproducible\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)\n\n\n# We can initialize the logger for training and have a look\n# at the final config used for training\nprint(f'Config:\\n{cfg.pretty_text}')","2717a921":"from mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\n\n\n# Build dataset\ndatasets = [build_dataset(cfg.data.train)]\n\n# Build the detector\nmodel = build_detector(\n    cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n# Add an attribute for visualization convenience\nmodel.CLASSES = datasets[0].CLASSES\n\n# Create work_dir\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","134297a4":"from pathlib import Path\nimport zipfile\n\nckpt_root = Path('\/kaggle\/working\/mmdetection\/checkpoints')\nwith zipfile.ZipFile('ckpt.zip', 'w') as z:\n    for img_name in ckpt_root.iterdir():\n        z.write(img_name)","4547d929":"!ls \/kaggle\/input\/rm-detection\/mmdetection\/checkpoints","9bc41dfb":"from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n\nimg = '\/kaggle\/input\/robomaster-final\/robomaster_Final Tournament\/image\/VW_CH3ENTERPRIZEVsTaurus_BO2_1_12.jpg'\n\nmodel.cfg = cfg\nresult = inference_detector(model, img)\nshow_result_pyplot(model, img, result, score_thr=0.3)","447508a2":"import cv2\n\nvideo_path = \"\/kaggle\/input\/robomaster-final\/video.mp4\"\nvideo = cv2.VideoCapture(video_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\n_, frame = video.read()\nout = cv2.VideoWriter('output.avi', fourcc, 24, (frame.shape[1], frame.shape[0]))\n\ncnt = 0\nwhile True:\n    _, frame = video.read()\n    print(cnt)\n    cnt += 1\n    if frame is None:\n        break\n    result = inference_detector(model, img)\n    img = model.show_result(img, result, score_thr=0.3, show=False)\n    out.write(img)\n    \nvideo.release()\nout.release()","ce87b042":"!ls","fa6fb771":"### Modify the config\n\nIn the next step, we need to modify the config for the training.\nTo accelerate the process, we finetune a detector using a pre-trained detector.","787fcd20":"### Understand the log\nFrom the log, we can have a basic understanding the training process and know how well the detector is trained.\n\nFirstly, the ResNet-50 backbone pre-trained on ImageNet is loaded, this is a common practice since training from scratch is more cost. The log shows that all the weights of the ResNet-50 backbone are loaded except the `conv1.bias`, which has been merged into `conv.weights`.\n\nSecond, since the dataset we are using is small, we loaded a Mask R-CNN model and finetune it for detection. Because the detector we actually using is Faster R-CNN, the weights in mask branch, e.g. `roi_head.mask_head`, are `unexpected key in source state_dict` and not loaded.\nThe original Mask R-CNN is trained on COCO dataset which contains 80 classes but KITTI Tiny dataset only have 3 classes. Therefore, the last FC layer of the pre-trained Mask R-CNN for classification has different weight shape and is not used.\n\nThird, after training, the detector is evaluated by the default VOC-style evaluation. The results show that the detector achieves 54.1 mAP on the val dataset,\n not bad!","59e8cc85":"## Test the trained detector\n\nAfter finetuning the detector, let's visualize the prediction results!","4316d5be":"## Install MMDetection","7a06c17c":"Given a config that trains a Faster R-CNN on COCO dataset, we need to modify some values to use it for training Faster R-CNN on KITTI dataset.","f32e3366":"### Train a new detector\n\nFinally, lets initialize the dataset and detector, then train a new detector!","e184b14a":"## Train a detector on customized dataset\n\nTo train a new detector, there are usually three things to do:\n1. Support a new dataset\n2. Modify the config\n3. Train a new detector\n\n","be0296e5":"# Download model","122d4868":"### Support a new dataset\n\nThere are three ways to support a new dataset in MMDetection: \n  1. reorganize the dataset into COCO format.\n  2. reorganize the dataset into a middle format.\n  3. implement a new dataset.\n\nUsually we recommend to use the first two methods which are usually easier than the third.\n\nIn this tutorial, we gives an example that converting the data into the format of existing datasets like COCO, VOC, etc. Other methods and more advanced usages can be found in the [doc](https:\/\/mmdetection.readthedocs.io\/en\/latest\/tutorials\/new_dataset.html#).\n\nFirstly, let's download a tiny dataset obtained from [KITTI](http:\/\/www.cvlibs.net\/datasets\/kitti\/eval_object.php?obj_benchmark=3d). We select the first 75 images and their annotations from the 3D object detection dataset (it is the same dataset as the 2D object detection dataset but has 3D annotations). We convert the original images from PNG to JPEG format with 80% quality to reduce the size of dataset."}}