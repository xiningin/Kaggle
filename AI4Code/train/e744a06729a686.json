{"cell_type":{"ce48c6b7":"code","267b7c5a":"code","0ff83bde":"code","3e137e39":"code","efb8a49f":"code","f88d0353":"code","7ecdc6ad":"code","e3eec625":"markdown","632ebdec":"markdown","57cde87a":"markdown","99db2e7d":"markdown"},"source":{"ce48c6b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","267b7c5a":"data_train=pd.read_csv('\/kaggle\/input\/cs-challenge\/training_set.csv')\ndata_test=pd.read_csv('\/kaggle\/input\/cs-challenge\/test_set.csv')\ndf_train=data_train.copy()\ndf_test=data_test.copy()","0ff83bde":"def fill_nan(df) :\n    '''Remplacement des donn\u00e9es manquantes. On suppose la continuit\u00e9 des grandeurs sur une m\u00eame \u00e9olienne\n    car ph\u00e9nom\u00e8ne physique'''\n    \n    df_clean=pd.DataFrame()\n    for mac_code in ['WT1', 'WT2', 'WT3', 'WT4'] :\n        wt=df[df[\"MAC_CODE\"]==mac_code].sort_values(\"Date_time\").fillna(method=\"ffill\")\n        df_clean=pd.concat([df_clean,wt]).sort_values(\"ID\")\n    \n    return df_clean\n\ndf_clean=fill_nan(df_train)","3e137e39":"def variance_threshold(df, threshold=2.0) :\n    '''S\u00e9lectionne les colonnes de df dont la variance >= threshold. Elimine les colonnes constantes. '''\n    return np.array(df.columns)[df.std()>=threshold]\n    \ndef speed_transform(df) :\n    '''On \u00e9l\u00e8ve les vitesses aux cubes car puissance r\u00e9cup\u00e9rable dans \u00e9olienne proportionnelle \u00e0 la vitesse turbine au cube.'''\n    speed_columns=[\"Rotor_speed\", \"Rotor_speed_min\", \"Rotor_speed_max\", \"Generator_converter_speed\",\"Generator_converter_speed_min\",\"Generator_converter_speed_max\", \"Generator_speed\", \"Generator_speed_min\", \"Generator_speed_max\" ]\n    for column in speed_columns :\n        df[column+\"^3\"] = (np.array(df[column]))**3\n        df=df.drop(column, axis=1)\n    return df\n\ndef selectKBest(df,k=10) :\n    '''Etude de corr\u00e9lation avec la colonne target et s\u00e9lection des k meilleures colonnes'''\n    corr_target={np.abs(df[column].corr(df[\"TARGET\"])) : str(column) for column in df.columns}\n    corr=list(corr_target.keys())\n    corr.sort(reverse=True)\n    \n    selected_features=[corr_target[element] for element in corr[:k]]\n    \n    return selected_features\n \ndef remove_duplicates(df, selected_features) :\n    '''Suppression des colonnes fortement corr\u00e9l\u00e9es entre elles. Elles font doublons.'''\n    \n    for k in range(2,3) :\n        selected=selected_features[k]\n        corr_selected={column : np.abs(df[selected].corr(df[column])) for column in df.columns}\n        duplicates=[column for column in df.columns if corr_selected[column] >=0.95]\n        \n        if \"TARGET\" in duplicates :\n            duplicates.remove(\"TARGET\")\n            \n        duplicates.remove(selected)\n        df=df.drop(duplicates, axis=1)\n    \n    return list(df.columns)\n    \ndef features_selection(df) :\n    \n    df=df.drop([\"ID\", \"MAC_CODE\"], axis=1) #Suppression des colonnes inutiles\n    selected_features=variance_threshold(df) #Suppression des donn\u00e9es constantes\n    df=df[selected_features]                \n    df=speed_transform(df)               #El\u00e9vation des vitesses au cube\n    selected_features=selectKBest(df)\n    df=df[selected_features]\n    selected_features=remove_duplicates(df, selected_features)\n    print(\"Selected features : \", selected_features)\n    \n    return selected_features, df[selected_features]\n    \n    \nselected_features, df_clean = features_selection(df_clean)","efb8a49f":"from sklearn.model_selection import train_test_split\n#Dernier pr\u00e9-traitement des donn\u00e9es : la normalisation et s\u00e9paration du dataset d'entra\u00eenement en train et test\n\ndef normalize(df) :\n    ''' Normalisation des donn\u00e9es pour qu'une colonne ne soit pas pr\u00e9pond\u00e9rante sur une autre.'''\n    \n    for column in df.columns :\n        df[column]=(df[column]-df[column].mean())\/df[column].std()\n    \n    return df\n\nY=df_clean[\"TARGET\"]\nX=normalize(df_clean.drop(\"TARGET\", axis=1))\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.20, random_state=40)","f88d0353":"from sklearn.linear_model import Ridge, SGDRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom  sklearn.metrics import mean_absolute_error\n\ndef ridge_regression(X_train, X_test, Y_train, Y_test) :\n    reg=Ridge(alpha=30000)\n    reg.fit(X_train, Y_train)\n    \n    #Evaluation\n    Y_pred_train=reg.predict(X_train)\n    Y_pred_test=reg.predict(X_test)\n    \n    print(\"MAE train : \", mean_absolute_error(Y_pred_train, Y_train))\n    print(\"MAE test : \", mean_absolute_error(Y_pred_test, Y_test))\n    \n    return reg \n\ndef SGD_regression(X_train, X_test, Y_train, Y_test) :\n    reg=SGDRegressor(alpha=0.26)\n    reg.fit(X_train, Y_train)\n    \n    #Evaluation\n    Y_pred_train=reg.predict(X_train)\n    Y_pred_test=reg.predict(X_test)\n    \n    print(\"MAE train : \", mean_absolute_error(Y_pred_train, Y_train))\n    print(\"MAE test : \", mean_absolute_error(Y_pred_test, Y_test))\n    \n    return reg\n\ndef random_forest_regression(X_train, X_test, Y_train, Y_test) :\n    reg=RandomForestRegressor(n_estimators=50)\n    reg.fit(X_train, Y_train)\n    \n    #Evaluation\n    Y_pred_train=reg.predict(X_train)\n    Y_pred_test=reg.predict(X_test)\n    \n    print(\"MAE train : \", mean_absolute_error(Y_pred_train, Y_train))\n    print(\"MAE test : \", mean_absolute_error(Y_pred_test, Y_test))\n    \n    return reg\n\ndef MLP_regression(X_train, X_test, Y_train, Y_test) :\n    reg=MLPRegressor(hidden_layer_sizes=(10,5,5), max_iter=200, random_state=5)\n    reg.fit(X_train, Y_train)\n    \n    #Evaluation\n    Y_pred_train=reg.predict(X_train)\n    Y_pred_test=reg.predict(X_test)\n    \n    print(\"MAE train : \", mean_absolute_error(Y_pred_train, Y_train))\n    print(\"MAE test : \", mean_absolute_error(Y_pred_test, Y_test))\n    \n    return reg\n\nreg=random_forest_regression(X_train, X_test, Y_train, Y_test)","7ecdc6ad":"def preprocessing_validation(df, selected_features) :\n    selected_features.remove(\"TARGET\")\n    df=fill_nan(df)\n    df=speed_transform(df)\n    ids_val=df[\"ID\"]\n    X_realworld=normalize(df[selected_features])\n    \n    return ids_val, X_realworld, df\n\ndef prediction_to_csv(filename,ids, model, X_real) :\n    results=pd.DataFrame()\n    results[\"ID\"]=ids\n    results[\"TARGET\"]=model.predict(X_real)\n    \n    results.to_csv(str(filename)+\".csv\", index=False)\n    \nids_val, X_realworld, df_new=preprocessing_validation(df_test, selected_features)\nprediction_to_csv(\"results_RF9\", ids_val, reg, X_realworld)","e3eec625":"## 0. Chargement des donn\u00e9es","632ebdec":"## 2. S\u00e9lection des features","57cde87a":"## 1. Pr\u00e9-traitement des donn\u00e9es","99db2e7d":"## 3. Mod\u00e8les de Machine Learning"}}