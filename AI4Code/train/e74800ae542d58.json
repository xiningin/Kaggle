{"cell_type":{"23053ae6":"code","a4f306d2":"code","885379d2":"code","7345011c":"code","078474a7":"code","3eea3958":"code","1e84e81b":"code","e94f72dc":"code","d6fe9ab3":"code","224fd6fc":"code","5cee6c9b":"code","580eec36":"code","05c3bc95":"code","634b542d":"code","7a89c9e2":"code","847eafb2":"code","8873485c":"code","cc427ba0":"code","56d6e938":"code","3d71567c":"code","21ee240c":"code","83805796":"code","2a68cc46":"code","cb3de161":"code","eff1686f":"code","67abcaf1":"code","424a227d":"code","e998e058":"code","5632a025":"code","344ec0f8":"code","5cbdd5e6":"code","82590693":"code","4b64a12f":"code","0c7a6a9d":"code","34b70452":"code","ec18b0e0":"code","f513f5a2":"code","435ecf1a":"code","9aa5aad8":"code","26331a94":"code","b9f92ba8":"code","0a1f5e0a":"code","82c343d3":"code","9f45764e":"code","b492f777":"code","dae69667":"code","55f05681":"code","be1177fb":"code","bba2df85":"code","a64863c8":"code","2c714fc7":"code","7883e3db":"code","44179eae":"code","6326e7a6":"code","525d97d5":"code","7a9f0f28":"code","8b099db4":"code","3abf6456":"code","19369870":"code","dfbcdc2b":"code","0f0b901d":"code","ab58e12a":"code","92949c3b":"code","fa9979a4":"code","18339a4e":"code","5f8ac516":"code","433bff38":"code","59e498ca":"code","86131723":"code","91c84540":"code","87d1d4d9":"code","58c4ac08":"code","bd4c95a6":"code","71831917":"code","5aacee00":"code","be8675fb":"code","25d514c7":"code","8852b25e":"code","98c835a8":"code","ec502b7e":"code","a5cfd07e":"code","7c63486a":"code","bcb037e1":"code","7fe9e3f8":"code","66ee65de":"code","5cbf5f47":"markdown","ed18ca12":"markdown","116a190f":"markdown","06f4b294":"markdown","c77cefae":"markdown","5c6d50f7":"markdown","0b048bad":"markdown","a2e3f533":"markdown","91e622bf":"markdown","57a04a24":"markdown","5f874ffd":"markdown","eaadabe8":"markdown","ecbdcd83":"markdown","187e51d1":"markdown","0708d787":"markdown","b425d270":"markdown","77a6508d":"markdown","d29ead28":"markdown","484df48d":"markdown","94210477":"markdown","68fc9ac4":"markdown","b06214bf":"markdown","6af964d7":"markdown"},"source":{"23053ae6":"# GoogleNews-vectors-negative300\n!wget --header=\"Host: doc-0k-64-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/70.0.3538.102 Safari\/537.36\" --header=\"Accept: text\/html,application\/xhtml+xml,application\/xml;q=0.9,image\/webp,image\/apng,*\/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Cookie: AUTH_sl8scr3r78dtlbjs5mvoco4ilf6h1a3b=09929041593969215305|1542391200000|409ktbjcsj48hp7irn60farm0jmshpq0; NID=144=iUZLDGG1-D1dE5x5RLpA2pGNWimx6WHQ-g4sz0bLPUDSHeHy6sI6XhiSEJ2z4YqrP-S_-rirUP-2C75qLXmC01uAkbOsJZvJPogLAa1uKW1i2UjM8beCIYenF3TWqmPWZLW38-CRyh-9yKI_elj6rjlnuezxRYPHc6BmXZe5RDA\" --header=\"Connection: keep-alive\" \"https:\/\/doc-0k-64-docs.googleusercontent.com\/docs\/securesc\/sr3ne958tilodvhhec79bitjd2u2bnlr\/j8ko7ih5d7a02ae9qfsd9j8545eqhnlp\/1542391200000\/06848720943842814915\/09929041593969215305\/0B7XkCwpI5KDYNlNUTTlSS21pQmM?e=download\" -O \"GoogleNews-vectors-negative300.bin.gz\" -c","a4f306d2":"# loading libraries and data\n\n%matplotlib inline\n\nimport sqlite3                          # for sql database\nimport pandas as pd\nimport numpy as np\nimport nltk                             # nltk:- Natural Language Processing Toolkit\nimport string\nimport re\nimport io\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport pickle\ndef saveindisk(obj,filename):\n    pickle.dump(obj,open(filename+\".p\",\"wb\"), protocol=4)\ndef openfromdisk(filename):\n    temp = pickle.load(open(filename+\".p\",\"rb\"))\n    return temp\n\ncon = sqlite3.connect(\"..\/input\/database.sqlite\")\n\n# Filtering only positive and negative reviews that is\n# not taking into consideration those reviews with score = 3\ndf = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3\n\"\"\", con)\n\n\n# Give reviews with score > 3 to be positive rating and reviews with a score < 3 as a negative\ndef polarity(x):\n    if x < 3:\n        return 'Negative'\n    else:\n        return 'Positive'\ndf[\"Score\"] = df[\"Score\"].map(polarity) # map is use to assign in all the Score","885379d2":"df.head() # top 5 values","7345011c":"df.describe()","078474a7":"df.shape\ndf['Score'].size","3eea3958":"df['Score'].value_counts()","1e84e81b":"df.duplicated(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}).value_counts() # checking duplicates","e94f72dc":"display= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\nORDER BY ProductID\n\"\"\", con)\ndisplay\n\n","d6fe9ab3":"df1 =  df.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep=\"first\") # Deleting all the duplicates","224fd6fc":"size_diff = df1['Id'].size\/df['Id'].size","5cee6c9b":"filtered_data2 = df1[df1.HelpfulnessNumerator <= df1.HelpfulnessDenominator]","580eec36":"# A regular expression (or RE) specifies a set of strings that matches it; the functions in this module let you check if a particular string matches a given regular expression (or if a given regular expression matches a particular string, which comes down to the same thing).\nimport re\n# cleaning html symbols from the sentence\ndef cleanhtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\n","05c3bc95":"# cleaning punctuations from the sentence\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned","634b542d":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = stopwords.words('english')\nprint(stop)","7a89c9e2":"from nltk.stem import SnowballStemmer # Stemmers remove morphological affixes from words, leaving only the word stem.\nsnow = SnowballStemmer('english') \nprint(snow.stem('tasty'))","847eafb2":"i = 0\nstring1 = ' '\nfinal_string = []\nall_positive_words = []                   # store words from +ve reviews here\nall_negative_words = []                   # store words from -ve reviews here.\ns = ''\n\nfor sent in filtered_data2['Text'].values:\n    filtered_sentence = []\n    sent = cleanhtml(sent)\n    sent = cleanpunc(sent)\n    for w in sent.split():\n        if((w.isalpha()) and (len(w)>2)):  \n            if(w.lower() not in stop):    # If it is a stopword\n                s = (snow.stem(w.lower())).encode('utf8')\n                filtered_sentence.append(s)\n                if (filtered_data2['Score'].values)[i] == 'Positive':\n                    all_positive_words.append(s)\n                if(filtered_data2['Score'].values)[i] == 'Negative':\n                    all_negative_words.append(s)\n            else:\n                continue\n        else:\n            continue \n    string1 = b\" \".join(filtered_sentence) \n    final_string.append(string1)\n    i += 1","8873485c":"i = 0\nstring1 = ' '\nfinal_string_nostem = []\ns = ''\n\nfor sent in filtered_data2['Text'].values:\n    filtered_sentence=[]\n    sent = cleanhtml(sent)\n    sent = cleanpunc(sent)\n    for w in sent.split():\n        if((w.isalpha()) and (len(w)>2)):  \n            if(w.lower() not in stop):\n                s = w.lower().encode('utf8')\n                filtered_sentence.append(s)\n            else:\n                continue\n        else:\n            continue \n    string1 = b\" \".join(filtered_sentence)     \n    final_string_nostem.append(string1)\n    i += 1","cc427ba0":"from collections import Counter\nprint(\"Number of positive words:\",len(all_positive_words))\nprint(\"Number of negative words:\", len(all_negative_words))","56d6e938":"filtered_data2['CleanedText'] = final_string\nfiltered_data2['CleanedText_NoStem'] = final_string_nostem\nfiltered_data2.head(3)\n","3d71567c":"filtered_data2['CleanedText_NoStem'][1]","21ee240c":"from sklearn.feature_extraction.text import CountVectorizer","83805796":"filtered_data2['CleanedText'].values","2a68cc46":"uni_gram = CountVectorizer()\nuni_gram_vectors = uni_gram.fit_transform(filtered_data2['CleanedText'].values)","cb3de161":"uni_gram_vectors.shape[1]","eff1686f":"uni_gram_vectors[0]","67abcaf1":"type(uni_gram_vectors)","424a227d":"from sklearn.decomposition import TruncatedSVD\n\ntsvd_uni = TruncatedSVD(n_components=2)\ntsvd_uni_vec = tsvd_uni.fit_transform(uni_gram_vectors)","e998e058":"tsvd_uni.explained_variance_ratio_[:].sum()","5632a025":"# Perplexity = 40\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tsvd_uni_vec.shape[0]), n_samples)\nsample_features = tsvd_uni_vec[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=40)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","344ec0f8":"# Perplexity = 30\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tsvd_uni_vec.shape[0]), n_samples)\nsample_features = tsvd_uni_vec[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=20)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","5cbdd5e6":"bi_gram = CountVectorizer(ngram_range=(1,2))\nbi_gram_vectors = bi_gram.fit_transform(filtered_data2['CleanedText'].values)","82590693":"bi_gram_vectors.shape ","4b64a12f":"type(bi_gram_vectors)","0c7a6a9d":"from sklearn.decomposition import TruncatedSVD\nsample_points = filtered_data2.sample(1100)\n\nbi_gram = CountVectorizer(ngram_range=(1,2))\nbi_gram_vectors = bi_gram.fit_transform(sample_points['CleanedText'])\ntsvd_bi = TruncatedSVD(n_components=2)\ntsvd_bi_vec = tsvd_bi.fit_transform(bi_gram_vectors)","34b70452":"tsvd_bi.explained_variance_ratio_[:].sum()","ec18b0e0":"# Perplexity = 30\n\nfrom sklearn.manifold import TSNE\nfrom time import time\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tsvd_bi_vec.shape[0]), n_samples)\nsample_features = tsvd_bi_vec[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=30)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","f513f5a2":"# Perplexity = 20\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tsvd_bi_vec.shape[0]), n_samples)\nsample_features = tsvd_bi_vec[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=20)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","435ecf1a":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(ngram_range=(1,2))\ntfidf_vec = tfidf.fit_transform(filtered_data2['CleanedText'])","9aa5aad8":"tfidf_vec.shape","26331a94":"features = tfidf.get_feature_names()\nfeatures[190000:190010]","b9f92ba8":"def top_tfidf_feats(row, features, top_n=25):\n    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n    topn_ind = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ind]\n    df = pd.DataFrame(top_feats,columns = ['feature', 'tfidf'])\n    return df\ntop_tfidfs = top_tfidf_feats(tfidf_vec[1000,:].toarray()[0],features, 20)","0a1f5e0a":"from sklearn.decomposition import TruncatedSVD\n\ntsvd_tfidf = TruncatedSVD(n_components=2)\ntsvd_tfidf_vec = tsvd_tfidf.fit_transform(tfidf_vec)","82c343d3":"tsvd_tfidf.explained_variance_ratio_[:].sum() ","9f45764e":"# Perplexity = 20\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tsvd_tfidf_vec.shape[0]), n_samples)\nsample_features = tsvd_tfidf_vec[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=20)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","b492f777":"# Perplexity = 30 \n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tsvd_tfidf_vec.shape[0]), n_samples)\nsample_features = tsvd_tfidf_vec[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=30)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","dae69667":"# Perplexity = 40\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tsvd_tfidf_vec.shape[0]), n_samples)\nsample_features = tsvd_tfidf_vec[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=40)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","55f05681":"final_string = []\nfor sent in filtered_data2['CleanedText'].values:\n    sent = str(sent)\n    sentence = []\n    for word in sent.split():\n        sentence.append(word)\n    final_string.append(sentence)","be1177fb":"import gensim # In gensim a corpus is simply an object which, when iterated over, returns its documents represented as sparse vectors.\nw2v_model = gensim.models.Word2Vec(final_string,min_count=5,size=50, workers=-1)","bba2df85":"w2v_vocub = w2v_model.wv.vocab\nlen(w2v_vocub)","a64863c8":"w2v_model.wv.most_similar('tast')","2c714fc7":"avg_vec = []\nfor sent in final_string:\n    cnt = 0\n    sent_vec = np.zeros(50)\n    for word in sent:\n        try:\n            wvec = w2v_model.wv[word]\n            sent_vec += wvec\n            cnt += 1\n        except: \n            pass\n    sent_vec \/= cnt\n    avg_vec.append(sent_vec)","7883e3db":"avg_vec = np.array(avg_vec)\navg_vec.shape","44179eae":"avg_vec","6326e7a6":"np.any(np.isnan(avg_vec))","525d97d5":"np.all(np.isfinite(avg_vec))","7a9f0f28":"col_mean = np.nanmean(avg_vec, axis=0)","8b099db4":"inds = np.where(np.isnan(avg_vec))\ninds","3abf6456":"avg_vec[inds] = np.take(col_mean, inds[1])\n","19369870":"np.any(np.isnan(avg_vec))","dfbcdc2b":"from sklearn import preprocessing\navg_vec_norm = preprocessing.normalize(avg_vec)","0f0b901d":"# Perplexity = 30\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, avg_vec.shape[0]), n_samples)\nsample_features = avg_vec[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=30)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","ab58e12a":"# Perplexity = 20\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, avg_vec_norm.shape[0]), n_samples)\nsample_features = avg_vec_norm[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=20)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","92949c3b":"from gensim.models import KeyedVectors\nw2v_model_google = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)","fa9979a4":"w2v_model_google.wv[\"word\"].size","18339a4e":"import random\navg_vec_google = []\ndatapoint = 3000\nsample_cols = random.sample(range(1, datapoint), 1001)\n\nfor sent in filtered_data2['CleanedText_NoStem'].values[sample_cols]:\n    cnt = 0\n    sent_vec = np.zeros(300)\n    sent = sent.decode(\"utf-8\") \n    for word in sent.split():\n        try:\n            wvec = w2v_model_google.wv[word]\n            sent_vec += wvec\n            cnt += 1\n        except: \n            pass\n    sent_vec \/= cnt\n    avg_vec_google.append(sent_vec)\navg_vec_google = np.array(avg_vec_google)","5f8ac516":"np.any(np.isnan(avg_vec_google))","433bff38":"np.all(np.isfinite(avg_vec_google))","59e498ca":"col_mean = np.nanmean(avg_vec_google, axis=0)","86131723":"inds = np.where(np.isnan(avg_vec_google)) \ninds","91c84540":"avg_vec_google[inds] = np.take(col_mean, inds[1])\n","87d1d4d9":"avg_vec_google","58c4ac08":"from sklearn import preprocessing\navg_vec_google_norm = preprocessing.normalize(avg_vec_google)","bd4c95a6":"# Perplexity = 20\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, avg_vec_google.shape[0]), n_samples)\nsample_features = avg_vec_google[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=20)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","71831917":"# Perplexity = 30\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, avg_vec_google.shape[0]), n_samples)\nsample_features = avg_vec_google[sample_cols]\nsample_class = filtered_data2['Score'][sample_cols]\n\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=30)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","5aacee00":"# Perplexity = 50\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, avg_vec_google.shape[0]), n_samples)\nsample_features = avg_vec_google[sample_cols]\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=50)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","be8675fb":"datapoints = 3000\nsample_cols = random.sample(range(1, datapoints), 1001)","25d514c7":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(ngram_range=(1,2))\ntfidf_vec_ns = tfidf.fit_transform(filtered_data2['CleanedText_NoStem'].values[sample_cols])\ntsvd_tfidf_ns = TruncatedSVD(n_components=2)\ntsvd_tfidf_vec_ns = tsvd_tfidf_ns.fit_transform(tfidf_vec_ns)\nfeatures = tfidf.get_feature_names()","8852b25e":"tfidf_w2v_vec_google = []\nreview = 0\n\nfor sent in filtered_data2['CleanedText_NoStem'].values[sample_cols]:\n    cnt = 0 \n    weighted_sum  = 0\n    sent_vec = np.zeros(300)\n    sent = sent.decode(\"utf-8\") \n    for word in sent.split():\n        try:\n            wvec = w2v_model_google.wv[word] \n            tfidf = tfidf_vec_ns[review,features.index(word)]\n            sent_vec += (wvec * tfidf)\n            weighted_sum += tfidf\n        except:\n            pass\n    sent_vec \/= weighted_sum\n    tfidf_w2v_vec_google.append(sent_vec)\n    review += 1","98c835a8":"len(tfidf_w2v_vec_google)","ec502b7e":"len(tfidf_w2v_vec_google[0])","a5cfd07e":"tfidf_w2v_vec_google[0]","7c63486a":"from sklearn import preprocessing\ntfidf_w2v_vec_google_norm = preprocessing.normalize(tfidf_w2v_vec_google)","bcb037e1":"# Perplexity = 20\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tfidf_w2v_vec_google_norm.shape[0]), n_samples)\nsample_features = tfidf_w2v_vec_google_norm[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=20)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","7fe9e3f8":"# Perplexity = 35\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tfidf_w2v_vec_google_norm.shape[0]), n_samples)\nsample_features = tfidf_w2v_vec_google_norm[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=35)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","66ee65de":"# Perplexity = 50\n\nfrom sklearn.manifold import TSNE\nimport random\n\nn_samples = 1000\nsample_cols = random.sample(range(1, tfidf_w2v_vec_google_norm.shape[0]), n_samples)\nsample_features = tfidf_w2v_vec_google_norm[sample_cols]\n\nsample_class = filtered_data2['Score'][sample_cols]\nsample_class = sample_class[:,np.newaxis]\nmodel = TSNE(n_components=2,random_state=0,perplexity=50)\n\nembedded_data = model.fit_transform(sample_features)\nfinal_data = np.concatenate((embedded_data,sample_class),axis=1)\ntsne_data = pd.DataFrame(data=final_data,columns=[\"Dim1\",\"Dim2\",\"label\"])\n\nsns.FacetGrid(tsne_data,hue=\"label\",size=6).map(plt.scatter,\"Dim1\",\"Dim2\").add_legend()\nplt.show()","5cbf5f47":"###  Data Deduplication","ed18ca12":"# Text Preprocessing: Stemming, stop-word removal and Lemmatization.\nNow that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n\nAfter which we collect the words used to describe positive and negative reviews","116a190f":"# Storing our preprocessed data in Database","06f4b294":"## *7) Tf-idf W2Vec*\n","c77cefae":"###  Stemming\n* A word stem is part of a word. It is sort of a normalization idea, but linguistic.\nFor example, the stem of the word waiting is wait.\n\n![alt text](https:\/\/pythonspot-9329.kxcdn.com\/wp-content\/uploads\/2016\/08\/word-stem.png.webp)","5c6d50f7":"# *6) Using Google's Trained W2Vec on Google News*","0b048bad":"## *2) Text Preprocessing*","a2e3f533":"## *1) Data Cleaning*","91e622bf":"## *3) tf-idf*\n\n* TF*IDF is an information retrieval technique that weighs a term's frequency (TF) and its inverse document frequency (IDF). Each word or term has its respective TF and IDF score. The product of the TF and IDF scores of a term is called the TF*IDF weight of that term\n\n![alt text](https:\/\/1.bp.blogspot.com\/-tnzPA6dDtTU\/Vw6EWm_PjCI\/AAAAAAABDwI\/JatHtUJb4fsce9E-Ns5t02_nakFtGrsugCLcB\/s1600\/%25E8%259E%25A2%25E5%25B9%2595%25E5%25BF%25AB%25E7%2585%25A7%2B2016-04-14%2B%25E4%25B8%258A%25E5%258D%25881.39.07.png)","57a04a24":"###  Helpfullness Numerator Greater than Helpfullness Denominator\n\n* HelpfulnessNumerator = (Yes) Review is good or not\n* HelpfulnessDenominator = (Yes + No) Review is good or not","5f874ffd":"### Punctuations Removal","eaadabe8":"### HTML Tag Removal","ecbdcd83":"# Postive and Negative words in reviews","187e51d1":"# Cleaned text Without Stemming for Google trained W2Vec","0708d787":"# Amazon Fine Food Reviews Analysis\nData Source: https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\nThe Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.<br>\n\nNumber of reviews: 568,454<br>\nNumber of users: 256,059<br>\nNumber of products: 74,258<br>\nTimespan: Oct 1999 - Oct 2012<br>\nNumber of Attributes\/Columns in data: 10 \n\nAttribute Information:\n\n1. Id\n2. ProductId - unique identifier for the product\n3. UserId - unqiue identifier for the user\n4. ProfileName\n5. HelpfulnessNumerator - number of users who found the review helpful\n6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n7. Score - rating between 1 and 5\n8. Time - timestamp for the review\n9. Summary - brief summary of the review\n10. Text - text of the review\n\n\n#### Objective:\nGiven a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).\n","b425d270":"# Ways to convert text to vector\n## *1) Uni-gram BOW*","77a6508d":"# Conclusion:-\n* As it is clearly visible from the overlapping area of the graph that bag of word and tf-IDF not classify the reviews positive or negative, while tfidf w2vec gives the more clear result in classifying the reviews as positive or negative.","d29ead28":"# Preprocessing on all the reviews","484df48d":"# References:-\nhttps:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/","94210477":"## *2) Bi-gram BOW*","68fc9ac4":"## *5) Avg Word2Vec*\n\n* It is the average of Word2Vec","b06214bf":"### Stopwords\n* A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n\n![alt text](https:\/\/www.geeksforgeeks.org\/wp-content\/uploads\/Stop-word-removal-using-NLTK.png)","6af964d7":"### *4) Word2Vec*\n\n* word2vec is an algorithm for constructing vector representations of words, also known as word embeddings. The vector for each word is a semantic description of how that word is used in context, so two words that are used similarly in text will get similar vector represenations. Once you map words into vector space, you can then use vector math to find words that have similar semantics."}}