{"cell_type":{"4638ca7d":"code","67f3ec48":"code","a2d5252e":"code","bd019d8a":"code","16735124":"code","93035776":"code","120b7f5e":"code","37b7931c":"code","3357b19c":"code","922dd89f":"code","c263ab4c":"code","7a1c6a04":"code","018520ac":"code","de9fe96f":"code","4e49a653":"code","328dbb5f":"code","e69e26ff":"code","f4582d4e":"code","c9469976":"code","d090ae27":"code","95fbd808":"code","d991dae6":"code","16be233e":"code","d6db238c":"code","7bb1724a":"code","69f31e21":"code","543ec908":"code","609dcee1":"code","ef7f1458":"code","ae859b38":"code","540d412c":"code","eef57ba5":"code","7b014374":"code","ed5d67cb":"code","c9dd9455":"code","4a429ffb":"code","c51fc70f":"code","a766f601":"code","30723618":"code","b5585445":"code","c5738c92":"code","167273e2":"markdown","de97db96":"markdown","77d833a4":"markdown","ee2aadbf":"markdown","098ecc46":"markdown","eba401f2":"markdown","98ebe89b":"markdown","aa39a23b":"markdown","8cd50a97":"markdown","ab16f93b":"markdown","a3f1dd44":"markdown","9897e3bb":"markdown","3b4c48c0":"markdown","bd854fdd":"markdown","ab9a4fee":"markdown","cfc1a88a":"markdown","032ab3d9":"markdown","1f276ae8":"markdown","58c70e0e":"markdown","35660096":"markdown","1620c453":"markdown","919964a0":"markdown","a66387f0":"markdown","7a7e8549":"markdown","57c2275f":"markdown","d31a3035":"markdown","2dd4ff77":"markdown","a988ea14":"markdown","9577b721":"markdown","103ecdde":"markdown"},"source":{"4638ca7d":"## Importing required libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Basic Statistics\nfrom scipy import stats\nfrom scipy.stats import norm\n\n# For scaling the data\nfrom sklearn.preprocessing import StandardScaler\n\n# For splitting the dataset into train and test\nfrom sklearn.model_selection import train_test_split\n\n# Light GBM model\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn import preprocessing\n\n# XGBoost model\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\n\n# For calculating accuracy values\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# For ignoring the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","67f3ec48":"# Fetching the data to pandas dataframes\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","a2d5252e":"# Concatenating training and test data into a single dataframe\ndata = pd.concat([train, test], sort=False)\ndata = data.reset_index(drop=True)\ndata.head()","bd019d8a":"print('Train data : ', train.shape)\nprint('Test data : ' , test.shape)\nprint('Complete data : ', data.shape)\n","16735124":"train['SalePrice'].describe()","93035776":"## Checking the distribution of target variable\nplt.figure(figsize=(12,6))\nsns.distplot(train['SalePrice']);\n\n## QQ-plot ( normal probability plot )\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","120b7f5e":"#applying log transformation\ntrain['SalePrice'] = np.log(train['SalePrice'])\n#transformed histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","37b7931c":"# Checking the missing values\ntotal_missing = data.isnull().sum().sort_values(ascending=False)\npercent_missing = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total_missing, percent_missing], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","3357b19c":"#correlation matrix\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(corrmat, vmax=.8,cmap=\"Blues\", square=True);","922dd89f":"plt.figure(figsize=(7,20))\nsns.heatmap(train.corr()[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).head(50), vmin=-1, annot=True);","c263ab4c":"# Dropping columns\ndata.drop([\"GarageArea\"], axis = 1, inplace = True)","7a1c6a04":"# Categorical features\ndata[\"GarageType\"]   = data[\"GarageType\"].fillna(\"None\")\ndata[\"GarageFinish\"] = data[\"GarageFinish\"].fillna(\"None\")\ndata[\"GarageQual\"] = data[\"GarageQual\"].fillna(\"None\")\ndata[\"GarageCond\"] = data[\"GarageCond\"].fillna(\"None\")\n\n# Numerical features\ndata[\"GarageYrBlt\"]  = data[\"GarageYrBlt\"].fillna(0)\ndata[\"GarageCars\"] = data[\"GarageCars\"].fillna(0)","018520ac":"# Categorical features\ndata[\"BsmtQual\"] = data[\"BsmtQual\"].fillna(\"None\")\ndata[\"BsmtCond\"] = data[\"BsmtCond\"].fillna(\"None\")\ndata[\"BsmtExposure\"] = data[\"BsmtExposure\"].fillna(\"None\")\ndata[\"BsmtFinType1\"] = data[\"BsmtFinType1\"].fillna(\"None\")\ndata[\"BsmtFinType2\"] = data[\"BsmtFinType2\"].fillna(\"None\")\n\n\n# Numerical features\ndata[\"BsmtFinSF1\"]  = data[\"BsmtFinSF1\"].fillna(0)\ndata[\"BsmtFinSF2\"]  = data[\"BsmtFinSF2\"].fillna(0)\ndata[\"BsmtUnfSF\"]   = data[\"BsmtUnfSF\"].fillna(0)\ndata[\"TotalBsmtSF\"] = data[\"TotalBsmtSF\"].fillna(0)\ndata[\"BsmtFullBath\"] = data[\"BsmtFullBath\"].fillna(0)\ndata[\"BsmtHalfBath\"] = data[\"BsmtHalfBath\"].fillna(0)","de9fe96f":"data[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)","4e49a653":"# Filling None as a value for categroical values\ndata[\"Alley\"] = data[\"Alley\"].fillna(\"None\")\ndata[\"PoolQC\"] = data[\"PoolQC\"].fillna(\"None\")\ndata[\"Fence\"]  = data[\"Fence\"].fillna(\"None\")\ndata[\"MiscFeature\"] = data[\"MiscFeature\"].fillna(\"None\")\ndata[\"FireplaceQu\"]  = data[\"FireplaceQu\"].fillna(\"None\")\n\n# After checking the data in the respective columns i felt that filling the missing values with Mode will provide much \n# better insights than filling the values with 0 or median since most of the values are biased towards a single value\n\ndata['SaleType']    = data['SaleType'].fillna(data['SaleType'].mode()[0])\ndata[\"Electrical\"]  = data.groupby(\"YearBuilt\")['Electrical'].transform(lambda x: x.fillna(x.mode()[0]))\ndata['KitchenQual']  = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\ndata['MSZoning']  = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\ndata['Utilities'] = data['Utilities'].fillna(data['Utilities'].mode()[0])\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\ndata['LotFrontage'].interpolate(method='linear',inplace=True)\ndata[\"Functional\"]   = data[\"Functional\"].fillna(\"Typ\")\n\ndata[\"SalePrice\"] = data[\"SalePrice\"].fillna(0)","328dbb5f":"# Checking if any null value is remaining in the dataset\ndata.isnull().sum().sort_values(ascending = False).head(15)","e69e26ff":"# Finding out the columns with integer datatype\nint_type_variables = [column for column in train.columns if train[column].dtype in ['int64']]\nint_type_variables","f4582d4e":"# Creating a function to plot the numerical variables with the target variable\ndef plotting_numerical(data):\n    for col in int_type_variables:\n        if col != 'SalePrice':\n            print(col)\n            print(data[col].dtype)\n    \n            plt.figure(figsize=(10, 10))\n            ax = sns.scatterplot(x=col, y='SalePrice', data=data)\n            plt.show();","c9469976":"sns.set_style('whitegrid')\nplotting_numerical(train)","d090ae27":"# Changing the datatype of integer columns to object \nlist_of_columns_for_datatype_change=['MSSubClass','OverallQual','OverallCond','BsmtFullBath','GarageCars','BsmtHalfBath','FullBath',\n                                     'HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','MoSold','YrSold']\ndata[list_of_columns_for_datatype_change]=data[list_of_columns_for_datatype_change].apply(lambda column:column.astype('object'))","95fbd808":"# Preprocessing the data by label encoding all the categorical variables in data\n# Creating a list of object type variables\nobj_type_variables = [column for column in data.columns if data[column].dtype in ['object']]\n\n# Label Encoding all the categorical variables\nle = preprocessing.LabelEncoder()\nfor li in obj_type_variables:\n    le.fit(list(set(data[li])))\n    data[li] = le.transform(data[li])\n\n# Splitting the test and train data as orignal\ntrain, test = data[:len(train)], data[len(train):]","d991dae6":"train.drop(train[(train['BsmtFinSF1']>5000)].index, inplace=True)\ntrain.drop(train[(train['TotalBsmtSF']>6000)].index, inplace=True)\ntrain.drop(train[(train['1stFlrSF']>4000)].index, inplace=True)\ntrain.drop(train[(train['MiscVal']>8000)].index, inplace=True)\ntrain.drop(train[(train['OpenPorchSF']>500) & (train['SalePrice']<250000)].index, inplace=True)\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)\n","16be233e":"# Creating the variables for model fitting\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\n\n# Test variable\ntest = test.drop(columns=['SalePrice', 'Id'])","d6db238c":"train.dtypes.value_counts()","7bb1724a":"## Training the XGboost regression model\nXgb_model = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n## Fitting the model\nXgb_model.fit(X, y)\n\n## Calculating r2 score for the model\nr2_score(Xgb_model.predict(X), y)","69f31e21":"predictions = Xgb_model.predict(test)","543ec908":"predictions.size","609dcee1":"sample_submission=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission=pd.DataFrame({\"Id\":sample_submission['Id'],\n                         \"SalePrice\":predictions})\nsubmission.to_csv('submission_xgb.csv',index=False)","ef7f1458":"# For calculating accuracy values\nfrom sklearn.metrics import mean_squared_error, r2_score\nkfold = KFold(n_splits=5, random_state = 2020, shuffle = True)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nmodel_lgb.fit(X, y)\nr2_score(model_lgb.predict(X), y)","ae859b38":"pred = model_lgb.predict(test)","540d412c":"sample_submission=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission=pd.DataFrame({\"Id\":sample_submission['Id'],\n                         \"SalePrice\":predictions})\nsubmission.to_csv('submission_lgb.csv',index=False)","eef57ba5":"# Gradient Boosting Regressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nmodel_gbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\nmodel_gbr.fit(X, y)\nr2_score(model_gbr.predict(X), y)","7b014374":"prediction_gbr = model_gbr.predict(test)","ed5d67cb":"sample_submission=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission=pd.DataFrame({\"Id\":sample_submission['Id'],\n                         \"SalePrice\":prediction_gbr})\nsubmission.to_csv('submission_gbr.csv',index=False)","c9dd9455":"# Random Forest Regressor\nmodel_rf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\nmodel_rf.fit(X, y)\nr2_score(model_rf.predict(X), y)","4a429ffb":"prediction_rf = model_rf.predict(test)","c51fc70f":"sample_submission=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission=pd.DataFrame({\"Id\":sample_submission['Id'],\n                         \"SalePrice\":prediction_rf})\nsubmission.to_csv('submission_rf.csv',index=False)","a766f601":"# Stack up all the models above, optimized using xgboost\nstacked_models = StackingCVRegressor(regressors=(Xgb_model, model_lgb, model_gbr, model_rf),\n                                meta_regressor=Xgb_model,\n                                use_features_in_secondary=True)\n\nstacked_models.fit(X, y)","30723618":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return (((0.1 * model_gbr.predict(X)) + \\\n            (0.2 * Xgb_model.predict(X)) + \\\n            (0.25 * model_lgb.predict(X)) + \\\n            (0.1 * model_rf.predict(X)) + \\\n            (0.35 * stacked_models.predict(np.array(X)))))\n\n","b5585445":"final_predictions  = blended_predictions(test)","c5738c92":"sample_submission=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission=pd.DataFrame({\"Id\":sample_submission['Id'],\n                         \"SalePrice\":final_predictions})\nsubmission.to_csv('submission_stack.csv',index=False)","167273e2":"As none of the above mentioned 6 variables have evident correlation with the target variable, we probably should remove them but first we will try to create any feature that can be generated out of them. \n\nFor an example if poolQc is null e, we can probably create a binary column that states if the house has a pool (assuming null value for poolQc means that the house doesn't have a pool) or not and then we will remove the columns with null values\n\nAlso from the above correlation plot we can infer that two pairs of columns (GarageCars and GarageArea) and (TotalBsmntSF and 1stFlrSF) are highly correlated and as GarageCars is showing better correlation with the target variable we can remove GarageArea column while for the second pair both \nTotalBsmntSF and 1stFlrSF show almost equal correlation with the target variable, however Basement size has mostly nothing to do with the size of 1st floor, hence we will keep both the variables","de97db96":"As we can not tamper with the test set, we should only remove the outliers from the training set, hence splitting the data to training and test set as orignal after performing label encoding on the data\n\n**Label Encoding refers to converting the labels into numeric form so as to convert it into the machine-readable form.**","77d833a4":"Similarly to fill all the columns related to Basement area. If there is a null value for any basement related features I am assuming that the house doesn't have a basement and hence filling the value with None for categorical and 0 for numerical features","ee2aadbf":"![](http:\/\/)![](http:\/\/)As we can see that the maximum value of target variable is much higher than 75% percentile value, that means the **data is skewed.**\n\nLet us examine further","098ecc46":"In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. ... Sometimes missing values are caused by the researcher\u2014for example, when data collection is done improperly or mistakes are made in data entry.\n\nWe have to fill the missing values with some value or we can remove the rows which contain missing values but that will result in loss of data and will not be helpful in modelling part ","eba401f2":"As we can see from the plots above, we can see that the columns:\n* BsmntFinSF1\n* TotalBsmntSF\n* 1stFlrSF\n* GrlivArea\n* OpenporchSF\n* MIscVal\n\nHave outlier values, we will remove these values in order to make better predictions\n\nOne more interesting thing that we can notice from the graphs above is that there are certain columns which have datatype integer but are behaving as categorical features namelyy :\n* MSSubClass\n* OverallQual\n* OverallCond\n* BsmntFullBath\n* BsmntHalfBath\n* FullBath\n* HalfBath\n* BedroomAbvGr\n* KitchenAbvGr\n* TotRmsAbvGr\n* Fireplaces\n* GarageCars\n* MoSold\n* YrSold\n\nWe will change the datatypes of the above metnioned columns to objects to use them further in the modelling section\n    ","98ebe89b":"## Giving weightages to the models based on their accuracy","aa39a23b":"# EDA, Feature Engineering and Classification using python\n\n### Tushar Pasricha - 29th April 2020\n\n\nIn this competition we are provided with data describing the characterstics of the house and our goal is to predict the house Sale price. The models are evaluated on the Root-Mean-Squared-Error (RMSE) between the log of the SalePrice predicted by our model, and the log of the actual SalePrice\n\n## Lets Begin","8cd50a97":"### The skew seems now corrected and the data appears more normally distributed.\nThis will certainly help in model prediction, as the target variable is now treated we can move forward with analysing the features that will be helpful in training the model","ab16f93b":"## Stacking up all the models","a3f1dd44":"## Random Forest","9897e3bb":"## Gradient Boosting ","3b4c48c0":"As the target variable is something we need to predict, it is almost always helpful analysing the target variable first.<br>\n\nWe will see if the target variable is following a normal distribution, as most of the models have an assumption that the target follows a normal distribution.\nIf the distribution is not in form of a normal distribution we can treat it by using **Log Transformation** \n\nThe log transformation is, arguably, the most popular among the different types of transformations used to transform skewed data to approximately conform to normality. If the original data follows a log-normal distribution or approximately so, then the log-transformed data follows a normal or near normal distribution.","bd854fdd":"## Missing Values","ab9a4fee":"### Removing the outliers ","cfc1a88a":"**As we can see from the plot above, the target variable doesn't follow normal distribution curve and is positively skewed as suspected. We need to transform the data to make it more normally distributed**\n\nLet me first explain what does skewness means : <br>\nSkewness defines the lack of symmetry in data. It is the measure of degree of asymmetry of a distribution\n\n**Regression has the following assumptions:**\n    1. Linear relationship\n    2. Multivariate normality\n    3. No or little multicollinearity\n    4. No auto-correlation\n    5. Homoscedasticity (all variables have same variance)\n    \n\nMultivariate normality means that regression requires all its variables to be normal. By having skewed data we violate the assumption of normality.\n\n**The violations can impact regression in the following ways:**\n    1. Disproportionate influence on parameter estimates: Parameter estimation is based on the minimization of squared error.\n        observations in skewed data will make a disproportionate effect on the parameter estimates. \n    2. Due to the skewness of data Confidence Intervals (statistics) can be either too wide or too narrow\n        as they are based on the assumption of normally distributed errors.","032ab3d9":"As the first 7 values have a very large number of null values, we should probably remove them.\nFor doing the same we should first check the correlation between the respective features and the target variable, if they have a very high correlation then we might have to impute the values else we can directly remove them\n","1f276ae8":"# Modelling","58c70e0e":"## Data Description\nI have created a list of the columns that are provided to us with their most probable descriptions and the assumptions i will be making while exploring the data :\n\n* MSSubClass: Identifies the type of dwelling involved in the sale.\n* MSZoning: Identifies the general zoning classification of the sale.\n* LotFrontage: Linear feet of street connected to property.\n* LotArea: Lot size in square feet.\n* Street: Type of road access to property.\n* Alley: Type of alley access to property.\n* LotShape: General shape of property.\n* LandContour: Flatness of the property.\n* Utilities: Type of utilities available.\n* LotConfig: Lot configuration.\n* LandSlope: Slope of property.\n* Neighborhood: Physical locations within Ames city limits.\n* Condition1: Proximity to various conditions.\n* Condition2: Proximity to various conditions (if more than one is present).\n* BldgType: Type of dwelling.\n* HouseStyle: Style of dwelling.\n* OverallQual: Rates the overall material and finish of the house.\n* OverallCond: Rates the overall condition of the house.\n* YearBuilt: Original construction date.\n* YearRemodAdd: Remodel date (same as construction date if no remodeling or additions).\n* RoofStyle: Type of roof.\n* RoofMatl: Roof material.\n* Exterior1st: Exterior covering on house.\n* Exterior2nd: Exterior covering on house (if more than one material).\n* MasVnrType: Masonry veneer type (None - None).\n* MasVnrArea: Masonry veneer area in square feet.\n* ExterQual: Evaluates the quality of the material on the exterior\/\n* ExterCond: Evaluates the present condition of the material on the exterior.\n* Foundation: Type of foundation.\n* BsmtQual: Evaluates the height of the basement (NA - No Basement).\n* BsmtCond: Evaluates the general condition of the basement (NA - No Basement).\n* BsmtExposure: Refers to walkout or garden level walls (NA - No Basement).\n* BsmtFinType1: Rating of basement finished area (NA - No Basement).\n* BsmtFinSF1: Type 1 finished square feet.\n* BsmtFinType2: Rating of basement finished area (if multiple types) (NA - No Basement).\n* BsmtFinSF2: Type 2 finished square feet.\n* BsmtUnfSF: Unfinished square feet of basement area.\n* TotalBsmtSF: Total square feet of basement area.\n* Heating: Type of heating.\n* HeatingQC: Heating quality and condition.\n* CentralAir: Central air conditioning.\n* Electrical: Electrical system.\n* 1stFlrSF: First Floor square feet.\n* 2ndFlrSF: Second floor square feet.\n* LowQualFinSF: Low quality finished square feet (all floors).\n* GrLivArea: Above grade (ground) living area square feet.\n* BsmtFullBath: Basement full bathrooms.\n* BsmtHalfBath: Basement half bathrooms.\n* FullBath: Full bathrooms above grade.\n* HalfBath: Half baths above grade.\n* Bedroom: Bedrooms above grade (does NOT include basement bedrooms).\n* Kitchen: Kitchens above grade.\n* KitchenQual: Kitchen quality.\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms).\n* Functional: Home functionality (Assume typical unless deductions are warranted).\n* Fireplaces: Number of fireplaces.\n* FireplaceQu: Fireplace quality (NA - No Fireplace).\n* GarageType: Garage location (NA - No Garage).\n* GarageYrBlt: Year garage was built.\n* GarageFinish: Interior finish of the garage (NA - No Garage).\n* GarageCars: Size of garage in car capacity.\n* GarageArea: Size of garage in square feet.\n* GarageQual: Garage quality (NA - No Garage).\n* GarageCond: Garage condition (NA - No Garage).\n* PavedDrive: Paved driveway.\n* WoodDeckSF: Wood deck area in square feet.\n* OpenPorchSF: Open porch area in square feet.\n* EnclosedPorch: Enclosed porch area in square feet.\n* 3SsnPorch: Three season porch area in square feet.\n* ScreenPorch: Screen porch area in square feet.\n* PoolArea: Pool area in square feet.\n* PoolQC: Pool quality (NA - No Pool).\n* Fence: Fence quality (NA - No Fence).\n* MiscFeature: Miscellaneous feature not covered in other categories (NA - None).\n* MiscVal: Value of miscellaneous feature.\n* MoSold: Month Sold (MM).\n* YrSold: Year Sold (YYYY).\n* SaleType: Type of sale.\n* SaleCondition: Condition of sale.","35660096":"## First we should check the target variable","1620c453":"**There are a lot of categorical and numerical columns, also we can see that some of the columns also have null values.<br>\nWe should treat the data for null values by filling categorical variables by 'None' or by imputing the rows having null values on basis of their percentage, however for numerical features we can impute the data by either 0,mean,median or mode values. **\n\n**I personally prefer to fill the values using either MEDIAN values or 0 as they are not impacted much by outliers and mostly helpful in training of the model**","919964a0":"### Checking the correlation of features with the target variable","a66387f0":"# Light GBM model","7a7e8549":"Let us first fill all the columns related to garage area. If there is a null value for any garage related features I am assuming that the house doesn't have a garage and hence filling the value with None for categorical and 0 for numerical features","57c2275f":"While modelling, we can give extra weightage to the columns or features that have a very high correlation with the target variable","d31a3035":"### Filling the missing values with data","2dd4ff77":"# XGBoost model","a988ea14":"# Checking the features","9577b721":"# Outliers\n\n### Now the next step of our exploration will be checking the outliers in the data and removing them.\n\nAn outlier is an observation that lies an abnormal distance from other values in a random sample from a population. ... Examination of the data for unusual observations that are far removed from the mass of data. These points are often referred to as outliers.\n\nFor finding the outlier values we will first plot all the numeric columns with our target Variable in order to find the values that are not following the general trend and then we will remove them for better prediction accuracy","103ecdde":"## Basic Data exploration"}}