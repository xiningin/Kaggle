{"cell_type":{"974cc2bc":"code","ad3c0500":"code","0180d59e":"code","861c0ee3":"code","4e6cb080":"code","cd54c61c":"code","00003979":"code","ac4508f1":"code","722cef4f":"code","14a45a21":"code","ab7ee2fa":"code","3f7ab6bd":"code","d5c4a2d3":"code","8a429a89":"code","16f314de":"code","793476a4":"code","e6cb0190":"code","158a2cd8":"code","6c29c2bd":"code","b8b3c541":"code","79356986":"code","1775bc81":"code","1887aae7":"code","e72cfc10":"code","bad59270":"code","f5fbd6a5":"code","2d7d01ff":"code","6aad0264":"code","aa041e4f":"code","1a47fdd4":"code","b03fd638":"code","f6dd3d04":"markdown","e0714efd":"markdown","c1b851be":"markdown","ed32e09f":"markdown","d5ab5792":"markdown","886ce24f":"markdown","529eefb5":"markdown","dfc69225":"markdown","1b1f137e":"markdown","81a12e5c":"markdown","d630c31e":"markdown","36bed070":"markdown","1f48e07b":"markdown","bea7b465":"markdown","ff218a57":"markdown","69a2c73a":"markdown","06d69de1":"markdown","341f05a1":"markdown","63b09d0b":"markdown","7a16d8c0":"markdown","12f6493a":"markdown","5534434c":"markdown","754b2061":"markdown","b1ed2427":"markdown","55a0f714":"markdown"},"source":{"974cc2bc":"import os\nimport json\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport seaborn as sns\nfrom ast import literal_eval\nimport matplotlib.pyplot as plt\nfrom pandas.io.json import json_normalize\nfrom sklearn.metrics import mean_squared_error\n\n%matplotlib inline\npd.options.display.max_columns = 999","ad3c0500":"def add_time_features(df):\n    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')\n    df['year'] = df['date'].apply(lambda x: x.year)\n    df['month'] = df['date'].apply(lambda x: x.month)\n    df['day'] = df['date'].apply(lambda x: x.day)\n    df['weekday'] = df['date'].apply(lambda x: x.weekday())\n    \n    return df","0180d59e":"def load_df(file_name = 'train_v2.csv', nrows = None):\n    USE_COLUMNS = [\n        'channelGrouping', 'date', 'device', 'fullVisitorId', 'geoNetwork',\n        'socialEngagementType', 'totals', 'trafficSource', 'visitId',\n        'visitNumber', 'visitStartTime', 'customDimensions']\n\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    df = pd.read_csv('..\/input\/{}'.format(file_name),\n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, nrows=nrows, usecols=USE_COLUMNS)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n        \n    # Normalize customDimensions\n    df['customDimensions']=df['customDimensions'].apply(literal_eval)\n    df['customDimensions']=df['customDimensions'].str[0]\n    df['customDimensions']=df['customDimensions'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)\n\n    column_as_df = json_normalize(df['customDimensions'])\n    column_as_df.columns = [f\"customDimensions.{subcolumn}\" for subcolumn in column_as_df.columns]\n    df = df.drop('customDimensions', axis=1).merge(column_as_df, right_index=True, left_index=True)\n    return df","861c0ee3":"train = load_df(\"..\/input\/train_v2.csv\", nrows=1000000)\ntest = load_df(\"..\/input\/test_v2.csv\", nrows=1000000)","4e6cb080":"train.head().T","cd54c61c":"print('TRAIN SET')\nprint('Rows: %s' % train.shape[0])\nprint('Columns: %s' % train.shape[1])\nprint('Features: %s' % train.columns.values)\nprint()\nprint('TEST SET')\nprint('Rows: %s' % test.shape[0])\nprint('Columns: %s' % test.shape[1])\nprint('Features: %s' % test.columns.values)","00003979":"train = add_time_features(train)\ntest = add_time_features(test)\n# Convert feature types.\ntrain[\"totals.transactionRevenue\"] = train[\"totals.transactionRevenue\"].astype('float')\ntrain['totals.hits'] = train['totals.hits'].astype(float)\ntest['totals.hits'] = test['totals.hits'].astype(float)\ntrain['totals.pageviews'] = train['totals.pageviews'].astype(float)\ntest['totals.pageviews'] = test['totals.pageviews'].astype(float)","ac4508f1":"gp_fullVisitorId_train = train.groupby(['fullVisitorId']).agg('sum')\ngp_fullVisitorId_train.head()","722cef4f":"# Train\ngp_fullVisitorId_train = train.groupby(['fullVisitorId']).agg('sum')\ngp_fullVisitorId_train['fullVisitorId'] = gp_fullVisitorId_train.index\ngp_fullVisitorId_train['mean_hits_per_day'] = gp_fullVisitorId_train.groupby(['day'])['totals.hits'].transform('mean')\ngp_fullVisitorId_train['mean_pageviews_per_day'] = gp_fullVisitorId_train.groupby(['day'])['totals.pageviews'].transform('mean')\ngp_fullVisitorId_train['sum_hits_per_day'] = gp_fullVisitorId_train.groupby(['day'])['totals.hits'].transform('sum')\ngp_fullVisitorId_train['sum_pageviews_per_day'] = gp_fullVisitorId_train.groupby(['day'])['totals.pageviews'].transform('sum')\ngp_fullVisitorId_train = gp_fullVisitorId_train[['fullVisitorId', 'mean_hits_per_day', 'mean_pageviews_per_day', 'sum_hits_per_day', 'sum_pageviews_per_day']]\ntrain = train.join(gp_fullVisitorId_train, on='fullVisitorId', how='inner', rsuffix='_')\ntrain.drop(['fullVisitorId_'], axis=1, inplace=True)\n\n# Test\ngp_fullVisitorId_test = test.groupby(['fullVisitorId']).agg('sum')\ngp_fullVisitorId_test['fullVisitorId'] = gp_fullVisitorId_test.index\ngp_fullVisitorId_test['mean_hits_per_day'] = gp_fullVisitorId_test.groupby(['day'])['totals.hits'].transform('mean')\ngp_fullVisitorId_test['mean_pageviews_per_day'] = gp_fullVisitorId_test.groupby(['day'])['totals.pageviews'].transform('mean')\ngp_fullVisitorId_test['sum_hits_per_day'] = gp_fullVisitorId_test.groupby(['day'])['totals.hits'].transform('sum')\ngp_fullVisitorId_test['sum_pageviews_per_day'] = gp_fullVisitorId_test.groupby(['day'])['totals.pageviews'].transform('sum')\ngp_fullVisitorId_test = gp_fullVisitorId_test[['fullVisitorId', 'mean_hits_per_day', 'mean_pageviews_per_day', 'sum_hits_per_day', 'sum_pageviews_per_day']]\ntest = test.join(gp_fullVisitorId_test, on='fullVisitorId', how='inner', rsuffix='_')\ntest.drop(['fullVisitorId_'], axis=1, inplace=True)","14a45a21":"train.head()","ab7ee2fa":"time_agg = train.groupby('date')['totals.transactionRevenue'].agg(['count', 'sum'])\nyear_agg = train.groupby('year')['totals.transactionRevenue'].agg(['sum'])\nmonth_agg = train.groupby('month')['totals.transactionRevenue'].agg(['sum'])\nday_agg = train.groupby('day')['totals.transactionRevenue'].agg(['sum'])\nweekday_agg = train.groupby('weekday')['totals.transactionRevenue'].agg(['count','sum'])","3f7ab6bd":"plt.figure(figsize=(20,7))\nplt.ticklabel_format(axis='y', style='plain')\nplt.ylabel('Sum transactionRevenue', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.scatter(time_agg.index.values, time_agg['sum'])\nplt.show()","d5c4a2d3":"plt.figure(figsize=(20,7))\nplt.ticklabel_format(axis='y', style='plain')\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.scatter(time_agg.index.values, time_agg['count'])\nplt.show()","8a429a89":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(20,7))\nax1.scatter(year_agg.index.values, year_agg['sum'])\nax1.locator_params(nbins=2)\nax1.ticklabel_format(axis='y', style='plain')\nax1.set_xlabel('Year', fontsize=12)\n\nax2.scatter(month_agg.index.values, month_agg['sum'])\nax2.locator_params(nbins=12)\nax2.ticklabel_format(axis='y', style='plain')\nax2.set_xlabel('Month', fontsize=12)\n\nax3.scatter(day_agg.index.values, day_agg['sum'])\nax3.locator_params(nbins=10)\nax3.ticklabel_format(axis='y', style='plain')\nax3.set_xlabel('Day', fontsize=12)\n\nax4.scatter(weekday_agg.index.values, weekday_agg['sum'])\nax4.locator_params(nbins=7)\nax4.ticklabel_format(axis='y', style='plain')\nax4.set_xlabel('Weekday', fontsize=12)\n\nplt.tight_layout()\nplt.show()","16f314de":"# Drop column that exists only in train data\ntrain = train.drop(['trafficSource.campaignCode'], axis=1)\n# Input missing transactionRevenue values\ntrain[\"totals.transactionRevenue\"].fillna(0, inplace=True)\n\ntest_ids = test[\"fullVisitorId\"].values","793476a4":"# Unwanted columns\nunwanted_columns = ['channelGrouping', 'customDimensions.index', 'customDimensions.value', 'fullVisitorId',\n                   'visitId', 'visitNumber', 'visitStartTime',\n                   'device.browser', 'device.browserSize', 'device.browserVersion',\n                   'device.deviceCategory', 'device.flashVersion',\n                   'device.language', 'device.mobileDeviceBranding',\n                   'device.mobileDeviceInfo', 'device.mobileDeviceMarketingName',\n                   'device.mobileDeviceModel', 'device.mobileInputSelector',\n                   'device.operatingSystem', 'device.operatingSystemVersion',\n                   'device.screenColors', 'device.screenResolution', 'geoNetwork.city',\n                   'geoNetwork.cityId', 'geoNetwork.continent', 'geoNetwork.country',\n                   'geoNetwork.latitude', 'geoNetwork.longitude', 'geoNetwork.metro',\n                   'geoNetwork.networkDomain', 'geoNetwork.networkLocation',\n                   'geoNetwork.region', 'geoNetwork.subContinent',       \n                   'totals.sessionQualityDim', 'trafficSource.adContent',\n                   'trafficSource.adwordsClickInfo.adNetworkType',\n                   'trafficSource.adwordsClickInfo.criteriaParameters',\n                   'trafficSource.adwordsClickInfo.gclId',\n                   'trafficSource.adwordsClickInfo.page',\n                   'trafficSource.adwordsClickInfo.slot', 'trafficSource.campaign',\n                   'trafficSource.isTrueDirect', 'trafficSource.keyword',\n                   'trafficSource.medium', 'trafficSource.referralPath',\n                   'trafficSource.source']\n\ntrain = train.drop(unwanted_columns, axis=1)\ntest = test.drop(unwanted_columns, axis=1)\n# Constant columns\nconstant_columns = [c for c in train.columns if train[c].nunique()<=1]\nprint('Columns with constant values: ', constant_columns)\ntrain = train.drop(constant_columns, axis=1)\ntest = test.drop(constant_columns, axis=1)\n# Columns with more than 50% null data\nhigh_null_columns = [c for c in train.columns if train[c].count()<=len(train) * 0.5]\nprint('Columns more than 50% null values: ', high_null_columns)\ntrain = train.drop(high_null_columns, axis=1)\ntest = test.drop(high_null_columns, axis=1)","e6cb0190":"print('TRAIN SET')\nprint('Rows: %s' % train.shape[0])\nprint('Columns: %s' % train.shape[1])\nprint('Features: %s' % train.columns.values)\n\nprint()\nprint('TEST SET')\nprint('Rows: %s' % test.shape[0])\nprint('Columns: %s' % test.shape[1])\nprint('Features: %s' % test.columns.values)","158a2cd8":"train.head()","6c29c2bd":"categorical_features = ['device.isMobile','year', 'month', 'weekday', 'day']\ntrain = pd.get_dummies(train, columns=categorical_features)\ntest = pd.get_dummies(test, columns=categorical_features)","b8b3c541":"# align both data sets (by outer join), to make they have the same amount of features,\n# this is required because of the mismatched categorical values in train and test sets\ntrain, test = train.align(test, join='outer', axis=1)\n\n# replace the nan values added by align for 0\ntrain.replace(to_replace=np.nan, value=0, inplace=True)\ntest.replace(to_replace=np.nan, value=0, inplace=True)","79356986":"X_train = train[train['date']<=datetime.date(2017, 5, 31)]\nX_val = train[train['date']>datetime.date(2017, 5, 31)]","1775bc81":"# Get labels\nY_train = X_train['totals.transactionRevenue'].values\nY_val = X_val['totals.transactionRevenue'].values\nX_train = X_train.drop(['totals.transactionRevenue'], axis=1)\nX_val = X_val.drop(['totals.transactionRevenue'], axis=1)\ntest = test.drop(['totals.transactionRevenue'], axis=1)\n# Log transform the labels\nY_train = np.log1p(Y_train)\nY_val = np.log1p(Y_val)","1887aae7":"reduce_features = ['date']\nX_train = X_train.drop(reduce_features, axis=1)\nX_val = X_val.drop(reduce_features, axis=1)\ntest = test.drop(reduce_features, axis=1)","e72cfc10":"X_train = X_train.astype('float32')\nX_val = X_val.astype('float32')\ntest = test.astype('float32')","bad59270":"X_train.head()","f5fbd6a5":"params = {\n\"objective\" : \"regression\",\n\"metric\" : \"rmse\", \n\"num_leaves\" : 500,\n\"min_child_samples\" : 20,\n\"learning_rate\" : 0.005,\n\"bagging_fraction\" : 0.6,\n\"feature_fraction\" : 0.7,\n\"bagging_frequency\" : 1,\n\"bagging_seed\" : 1,\n\"lambda_l1\": 3,\n'min_data_in_leaf': 70\n}","2d7d01ff":"lgb_train = lgb.Dataset(X_train, label=Y_train)\nlgb_val = lgb.Dataset(X_val, label=Y_val)\nmodel = lgb.train(params, lgb_train, 10000, valid_sets=[lgb_train, lgb_val], early_stopping_rounds=100, verbose_eval=100)","6aad0264":"# Make prediction on validation data.\nval_predictions = model.predict(X_val, num_iteration=model.best_iteration)\n# Get min and max values of the predictions and labels.\nmin_val = max(max(val_predictions), max(Y_val))\nmax_val = min(min(val_predictions), min(Y_val))\n# Create dataframe with validation predicitons and labels.\nval_df = pd.DataFrame({\"Label\":Y_val})\nval_df[\"Prediction\"] = val_predictions\n# Plot data\nsns.set(style=\"darkgrid\")\nsns.jointplot(y=\"Label\", x=\"Prediction\", data=val_df, kind=\"reg\", color=\"m\", height=10)\nplt.plot([min_val, max_val], [min_val, max_val], 'm--')\nplt.show()","aa041e4f":"val_predictions[val_predictions<0] = 0\nmse = mean_squared_error(val_predictions, Y_val)\nrmse = np.sqrt(mean_squared_error(val_predictions, Y_val))\n\nprint('Model validation metrics')\nprint('MSE: %.2f' % mse)\nprint('RMSE: %.2f' % rmse)","1a47fdd4":"lgb.plot_importance(model, figsize=(15, 10))\nplt.show()","b03fd638":"predictions = model.predict(test, num_iteration=model.best_iteration)\n\nsubmission = pd.DataFrame({\"fullVisitorId\":test_ids})\npredictions[predictions<0] = 0\nsubmission[\"PredictedLogRevenue\"] = predictions\nsubmission = submission.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsubmission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"]\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head(10)","f6dd3d04":"Again we had higher frequency at a similar time period.","e0714efd":"### Agregated features.","c1b851be":"### This is our new data with some cleaning and engineering.","ed32e09f":"#### Here is sum of our tagert feature \"transactionRevenue\" through the time.","d5ab5792":"### Feature engineering","886ce24f":"Seems we had more transactions on late 2016 and early 2017, date features seems to be a good addition to our model.","529eefb5":"### Model\n* Now let's to use the famous LGBM to model our data.","dfc69225":"### About the train data","1b1f137e":"### About the engineered time features\n* Year: It seem transactions had a large increase from 2016 to 2017\n* Month: Lager transaction on december seems ok, but about months but im not sure why high values on april and august (maybe because of easter (april) or Tax-free weekend, back-to-school season(august)?)\n* Day: Here it seems that not really important is going on, seems this features can be discarded.\n* Weekday: Something strange is going on here, seems that weekends have less transactions?","81a12e5c":"### Feature importance","d630c31e":"### The let's do some cleaning","36bed070":"### This is how our data looks like","1f48e07b":"Function to load and convert files borrowed from this [kernel](https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook), thanks!","bea7b465":"#### Let's take a look at other time features.","ff218a57":"### Exploratory data analysis","69a2c73a":"#### And here count of our target feature \"transactionRevenue\".","06d69de1":"### Drop unwanted columns","341f05a1":"### Split data in train and validation by date","63b09d0b":"#### Let's take a look at our target value through the time.","7a16d8c0":"### Model metrics","12f6493a":"## LGBM - Google Analytics Customer Revenue Prediction\n* Note: this is just a starting point, there's a lot of work to be done.*\n* I also have a [deep learning](https:\/\/www.kaggle.com\/dimitreoliveira\/deep-learning-keras-ga-revenue-prediction) version of this code, this one is supposed to be a comparation between the models.\n* I'm new to LGBM if you have any tip or correction please let me know.","5534434c":"### Auxiliar functions","754b2061":"### Dependencies","b1ed2427":"### Let's have a look at the our model prediction on the validation set against the labels.\n* Each point is a value from the data (axis x = label, axis y = prediction).\n* The dashed line would be the perfect values (prediction = labels).\n* The continuous line would be a linear regression.","55a0f714":"### One-hot encode categorical data"}}