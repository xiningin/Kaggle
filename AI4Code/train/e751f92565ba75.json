{"cell_type":{"2be93e17":"code","f9e1908a":"code","68f6388e":"code","9c860670":"code","9032d56e":"code","c084bfbb":"code","36cea591":"code","e09f6201":"code","765634f5":"code","a25a8e54":"code","8389d7ef":"code","368cf46c":"code","31664250":"code","f66ade4e":"code","1d024971":"code","beb4d7b5":"code","29f295a1":"code","3139525a":"code","4454434b":"code","c894cb7d":"code","fbc20b2c":"code","de0e910a":"code","2728afa3":"code","29a14073":"code","9e065752":"code","e6611d57":"code","932f96da":"code","553c9b38":"code","773fa0f6":"code","ab8ae2d3":"code","728b230d":"code","ab80820d":"code","9b1004ea":"code","3e382500":"code","39fc3336":"code","5dc4d094":"code","f204cdbf":"code","19fa5aac":"markdown","8bd51c9d":"markdown","4b43c3d6":"markdown","420c668d":"markdown","4ccea9f0":"markdown","8a9d20cb":"markdown","824b5eb8":"markdown","fcb53e46":"markdown","c0d64cf3":"markdown","8f287396":"markdown","24c799a8":"markdown","2649fb01":"markdown","34df3723":"markdown","c59a30ac":"markdown","7274327e":"markdown","6f89de14":"markdown","b603471d":"markdown","0da37a25":"markdown"},"source":{"2be93e17":"# Libraries CPU\nimport wandb     ### comment when Internet OFF\nimport cv2\nimport os\nimport gc\nimport random\nimport tqdm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits import mplot3d\nimport sys\nsys.path = ['..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master'\n           ] + sys.path\n\n# Libaries GPU\nimport cudf\nimport cupy\nimport cuml\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\n# Pytorch & Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import Compose, Resize, Normalize, HorizontalFlip, VerticalFlip,\\\n                            Rotate, CenterCrop\n\n\nfrom efficientnet_pytorch import EfficientNet\nfrom transformers import AutoTokenizer\nfrom torchvision.models import resnet34, resnet50\n\n# Environment check\nos.environ[\"WANDB_SILENT\"] = \"true\"      ### comment when Internet OFF\n\n# Secrets \ud83e\udd2b\n### comment when Internet OFF\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\n# Color scheme\nmy_colors = [\"#EDAC54\", \"#F4C5B7\", \"#DD7555\", \"#B95F18\", \"#475A20\"]\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device available now:', device)\n\n# Base paths\ntrain_base = \"..\/input\/shopee-product-matching\/train_images\/\"\ntest_base = \"..\/input\/shopee-product-matching\/test_images\/\"","f9e1908a":"def set_seed(seed = 1234):\n    '''Sets the seed of the entire notebook.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","68f6388e":"! wandb login $secret_value_0     ### comment when Internet OFF","9c860670":"# ---- Set COMPUTE_CV value ----\nCOMPUTE_CV = True\n\n# Switch to False if test.csv has more than 3 values\n### check out Chris's notebook for more info on this\ntest = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n\nif len(test)>3: \n    COMPUTE_CV = False","9032d56e":"if COMPUTE_CV == True:\n    # === CPU data ===\n    # Read in data\n    data = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")    \n    # Set a \"filepath\" column\n    data[\"filepath\"] = train_base + data[\"image\"]\n    # Map on for each product all `posting_id` that are labeled as the same\n    ### this way we create a \"target\" column (ONLY FOR TRAIN)\n    group_dicts = data.groupby('label_group')[\"posting_id\"].unique().to_dict()\n    data['target'] = data[\"label_group\"].map(group_dicts)\n    \n    # === GPU data ===\n    data_gpu = cudf.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")    \n    data_gpu[\"filepath\"] = train_base + data_gpu[\"image\"]\n\nelse:\n    # === CPU data ===\n    data = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\n    data[\"filepath\"] = test_base + data[\"image\"]\n    # No Target Here\n    \n    # === GPU data ===\n    data_gpu = cudf.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")    \n    data_gpu[\"filepath\"] = test_base + data_gpu[\"image\"]","c084bfbb":"# # === OPTIONAL ===\n# # Increase 2.05 times the amount of data\n# data = pd.concat([data, data, data.loc[:2000]], axis=0)\n# data_gpu = cudf.concat([data_gpu, data_gpu, data_gpu.loc[:2000]], axis=0)","36cea591":"# Let's look at it\ndata.head(2)","e09f6201":"# Save data to W&B Artifacts\n### comment when Internet OFF\nrun = wandb.init(project='shopee-kaggle', name='original_data')\nartifact = wandb.Artifact(name='original', \n                          type='dataset')\n\nartifact.add_file(\"..\/input\/shopee-preprocessed-data\/train.parquet\")\nartifact.add_file(\"..\/input\/shopee-preprocessed-data\/test.parquet\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","765634f5":"def F1_score(target_column, pred_column):\n    '''Returns the F1_score for each row in the data.\n    Remember: The final score is the mean F1 score.\n    target_column: the name of the column that contains the target\n    pred_column: the name of the column that contains the prediction\n    '''\n    \n    def get_f1(row):\n        # Find the common values in target and prediction arrays.\n        intersection = len( np.intersect1d(row[target_column], row[pred_column]) )\n        # Computes the score by following the formula\n        f1_score = 2 * intersection \/ (len(row[target_column]) + len(row[pred_column]))\n        \n        return f1_score\n    \n    return get_f1","a25a8e54":"run = wandb.init(project='shopee-kaggle', name='metric_baseline')\n\ndata_baseline = data.copy()\n\n# Create artificial prediction column\n### based on image_phash - all images with the same image_phash are the same\ngroup_baseline = data_baseline.groupby(\"image_phash\")[\"posting_id\"].unique().to_dict()\ndata_baseline['preds'] = data_baseline[\"image_phash\"].map(group_baseline)\n\n# Get F1 score for each row\ndata_baseline['F1'] = data_baseline.apply(F1_score(target_column=\"target\", pred_column=\"preds\"), axis=1)\nprint('CV score for baseline = {:.3f}'.format(data_baseline[\"F1\"].mean()))\nwandb.log({\"Baseline CV Score\" : data_baseline[\"F1\"].mean()})\n\nwandb.finish()","8389d7ef":"class ShopeeDataset(Dataset):\n    \n    def __init__(self, csv, train):\n        self.csv = csv.reset_index()\n        self.train = train\n        \n        # Instantiate one of the tokenizer classes of the library from BERT\n        self.tokenizer = AutoTokenizer.from_pretrained('..\/input\/bert-base-uncased')\n        # Image Augmentation\n        self.transform = Compose([VerticalFlip(p=0.5),\n                                  HorizontalFlip(p=0.5),\n                                  Resize(256, 256),\n                                  Normalize(),\n                                 ])\n        \n    def __len__(self):\n        return len(self.csv)\n    \n    \n    def __getitem__(self, index):\n        '''Read in image & title as PyTorch Dataset.\n        Return the transformed image and text ids and mask.'''\n            \n        # Read in image and text data\n        image = cv2.imread(self.csv[\"filepath\"][index])\n        text = self.csv[\"title\"][index]\n        \n        # Transform image & transpose channels [color, height, width]\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_transf = self.transform(image=image)[\"image\"].astype(np.float32)\n        image_transf = torch.tensor(image_transf.transpose(2, 0, 1))\n        \n        # Tokenize the text using BERT\n        text_token = self.tokenizer(text, padding=\"max_length\",\n                                    truncation=True, max_length=16,\n                                    return_tensors=\"pt\")\n        input_ids = text_token[\"input_ids\"][0]\n        attention_mask = text_token[\"attention_mask\"][0]\n        \n        # Return dataset info\n        ### if \"test\", we won't have label_group available\n        if self.train == True:\n            label_group = torch.tensor(self.csv[\"label_group\"][index])\n            return image_transf, input_ids, attention_mask, label_group\n        \n        else:\n            return image_transf, input_ids, attention_mask","368cf46c":"# Compute dataloader for test data\ndataset_data = ShopeeDataset(csv=data, train=False)\ndata_loader = DataLoader(dataset_data, batch_size=16,\n                         num_workers=4)\n\nprint(\"Dataset Len: {:,}\".format(len(dataset_data)), \"\\n\" +\n      \"Image Shape [0]: {}\".format(dataset_data[0][0].shape), \"\\n\" +\n      \"input_ids [0]: {}\".format(dataset_data[0][1]), \"\\n\" +\n      \"attention_mask [0]: {}\".format(dataset_data[0][2]))","31664250":"# Extract Efficientnet and put model on GPU\nmodel_effnet = EfficientNet.from_name(\"efficientnet-b2\").cuda()\nmodel_effnet.load_state_dict(torch.load(\"..\/input\/efficientnet-pytorch\/efficientnet-b2-27687264.pth\"))\n\n# model_resnet = resnet50(pretrained = False).cuda()\n# model_resnet.load_state_dict(torch.load('..\/input\/pretrained-pytorch-models\/resnet50-19c8e357.pth'))","f66ade4e":"# Extract embeddings of the image (the EffnetB0 representation)\nembeddings = []\n\n# We aren't training, only extracting the representation\nwith torch.no_grad():\n    for image, ids, mask in tqdm.tqdm(data_loader):\n        # Don't forget to append the image to .cuda() as well\n        image = image.cuda()\n        ids = ids.detach().numpy()\n        mask = mask.detach().numpy()\n        \n        img_embeddings = model_effnet(image)\n        img_embeddings = img_embeddings.detach().cpu().numpy()\n        # Add information from ids and mask as well\n        img_embeddings = np.hstack((img_embeddings, ids, mask))\n        embeddings.append(img_embeddings)\n        \n\n# Concatenate all embeddings\nall_image_embeddings = np.concatenate(embeddings)\nprint(\"image_embeddings shape: {:,}\/{:,}\".format(all_image_embeddings.shape[0], all_image_embeddings.shape[1]))\n\n# Save it to a binary file in NumPy .npy format.\n# np.save(\"image_embeddings\", all_image_embeddings)","1d024971":"# Read in image_embeddings\n# all_image_embeddings = np.load(\"..\/input\/shopee-preprocessed-data\/image_embeddings.npy\")\n\n# Save image_embeddings to W&B\n### comment when Internet OFF\nrun = wandb.init(project='shopee-kaggle', name='image_embeddings')\nartifact = wandb.Artifact(name='image_embeddings', \n                          type='dataset')\n\nartifact.add_file(\"..\/input\/shopee-preprocessed-data\/image_embeddings.npy\")\n\nwandb.log_artifact(artifact)\nwandb.log({\"Length of Image embeddings\" : all_image_embeddings.shape[1],\n           \"Width of Image embeddings\" : all_image_embeddings.shape[0]})\nwandb.finish()","beb4d7b5":"# Clean memory\ndel model_effnet\n_ = gc.collect()","29f295a1":"run = wandb.init(project='shopee-kaggle', name='image_predictions')    ### comment when Internet OFF","3139525a":"# Create the model instance\nif len(data) > 3:\n    knn_model = NearestNeighbors(n_neighbors=50)\n    wandb.log({\"n_neighbors\" : 50})     ### comment when Internet OFF\nelse:\n    knn_model = NearestNeighbors(n_neighbors=2)\n    wandb.log({\"n_neighbors\" : 2})      ### comment when Internet OFF\n    \n# Train the model\nknn_model.fit(all_image_embeddings)","4454434b":"# Creating the splits, to prevent memory errors\n### more info on this in Chris's notebook\npredictions = []\nCHUNK = 1024 * 4  ### 4096\n\nSPLITS = len(all_image_embeddings) \/\/ CHUNK\nif len(all_image_embeddings) % CHUNK != 0: SPLITS += 1\nprint(\"Total Splits:\", SPLITS)\n\n\n# Making the prediction\nprint(\"Finding Similar Images ...\")\n\nfor no in range(SPLITS):\n    \n    a = no * CHUNK\n    b = (no+1) * CHUNK\n    b = min(b, len(all_image_embeddings))\n    print(\"CHUNK:\", a, \"-\", b)\n    \n    distances, indices = knn_model.kneighbors(all_image_embeddings[a:b,])\n    \n    for k in range(b-a):\n        index = np.where(distances[k, ] < 6.0)[0]\n        split = indices[k, index]\n        pred = data.iloc[split][\"posting_id\"].values\n        \n        predictions.append(pred)\n\n        \n# Clean environment\ndel knn_model, distances, indices\n_ = gc.collect()","c894cb7d":"# Add predictions to dataframe\ndata['img_pred'] = predictions\ndata.head(3)","fbc20b2c":"### comment when Internet OFF\nwandb.finish()","de0e910a":"# Create dataframe\nimg_embeddings_df = pd.DataFrame(all_image_embeddings)\n\n# Separating out the features\nX = img_embeddings_df.values\n# Standardizing the features\nX = StandardScaler().fit_transform(X)\n\n# Separating out the target\ny = data[\"label_group\"]\n\n\n# PCA\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(X)\n# pca.explained_variance_ratio_.sum()\n\nprincipalDf = pd.DataFrame(data = principalComponents,\n                           columns = ['pc_1', 'pc_2', 'pc_3'])\nfinalDf = pd.concat([principalDf, y], axis = 1)","2728afa3":"# Plot\nfig = plt.figure(figsize=(20, 15))\nax = plt.axes(projection='3d')\n\nax.scatter3D(finalDf['pc_1'], finalDf['pc_2'], finalDf['pc_3'], c=finalDf['label_group'], cmap='BrBG')\nax.set_title('Image Embeddings: 3D Cluster', size=20);","29a14073":"del img_embeddings_df, X, pca, principalDf, finalDf, all_image_embeddings\n_ = gc.collect()","9e065752":"# Extract the Tf-Idf Matrix\n# TODO: Extract more features & add preprocessing from notebook I\ntf_idf = TfidfVectorizer(stop_words='english', binary=True, max_features=25000)\ntext_embeddings = tf_idf.fit_transform(data_gpu[\"title\"]).toarray()\n\nprint(\"Text Embeddings Matrix format: {:,}\/{:,}\".format(text_embeddings.shape[0], text_embeddings.shape[1]))","e6611d57":"# Save image_embeddings to W&B\n### comment when Internet OFF\nrun = wandb.init(project='shopee-kaggle', name='text_embeddings')\nartifact = wandb.Artifact(name='text_embeddings', \n                          type='dataset')\n\nartifact.add_file(\"..\/input\/shopee-preprocessed-data\/text_embeddings.npy\")\n\nwandb.log_artifact(artifact)\nwandb.log({\"Length of Text embeddings\" : text_embeddings.shape[1],\n           \"Width of Text embeddings\" : text_embeddings.shape[0]})\nwandb.finish()","932f96da":"def find_matches_cupy(X, posting_ids, threshold):\n    # TODO: to be developed\n    # https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/230486\n    X = cp.array(X)\n    N = X.shape[1]\n    matches = []\n\n    for i in tqdm(range(N)):\n        v = X[:, i].reshape(-1, 1)\n        thresholded_bool = cp.linalg.norm(v - X, axis=0) < threshold\n        thresholded_ix = cp.argwhere(thresholded_bool).squeeze(-1)\n        thresholded_ix = thresholded_ix.get()\n        match = \" \".join(posting_ids[thresholded_ix])\n        matches.append(match)\n\n    return matches","553c9b38":"# Creating the splits, to prevent memory errors\n### more info on this in Chris's notebook\npredictions = []\nCHUNK = 1024 * 4  ### 4096\n\nSPLITS = len(text_embeddings) \/\/ CHUNK\nif len(text_embeddings) % CHUNK != 0: SPLITS += 1\nprint(\"Total Splits:\", SPLITS)\n\n\n# Making the prediction\nprint(\"Finding Similar Titles ...\")\n\nfor no in range(SPLITS):\n    \n    a = no * CHUNK\n    b = (no+1) * CHUNK\n    b = min(b, len(text_embeddings))\n    print(\"CHUNK:\", a, \"-\", b)\n    \n    # Cosine similarity distance\n    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        index = cupy.where(cts[k,] > 0.7)[0]\n        index = cupy.asnumpy(index)\n        pred = data.iloc[index][\"posting_id\"].values\n        \n        predictions.append(pred)\n\n        \n# Clean environment\ndel tf_idf, text_embeddings\n_ = gc.collect()","773fa0f6":"# Add predictions to dataframe\ndata['title_pred'] = predictions\ndata.head(3)","ab8ae2d3":"from cuml.experimental.preprocessing import StandardScaler as StandardScaler_gpu\nfrom cuml.decomposition import PCA as PCA_gpu","728b230d":"# # Create dataframe\n# text_embeddings_df = cudf.DataFrame(text_embeddings)\n\n# # Separating out the features\n# X = text_embeddings_df.values\n# # Standardizing the features\n# X = StandardScaler_gpu().fit_transform(X)\n\n# # Separating out the target\n# y = data[\"label_group\"]\n\n\n# # PCA\n# pca = PCA_gpu(n_components=3)\n# principalComponents = pca.fit_transform(X)\n\n# principalDf = cudf.DataFrame(data = principalComponents,\n#                              columns = ['pc_1', 'pc_2', 'pc_3'])\n# finalDf = cudf.concat([principalDf, y], axis = 1)","ab80820d":"# All images that have the same phash are identical, so we'll add these too\nduplicate_dict = data.groupby('image_phash').posting_id.agg('unique').to_dict()\ndata['duplic_pred'] = data[\"image_phash\"].map(duplicate_dict)","9b1004ea":"def combine_predictions(row, cv=True):\n    '''Combine all predictions together.'''\n    \n    # Concatenate all predictions\n    all_preds = np.concatenate([row[\"img_pred\"],row[\"title_pred\"], row[\"duplic_pred\"]])\n    all_preds = np.unique(all_preds)\n    \n    # Return combined unique preds\n    if cv == True:\n        return all_preds\n    else:\n        return ' '.join(all_preds)","3e382500":"if COMPUTE_CV == True:\n    \n    data[\"all_preds\"] = data.apply(lambda x: combine_predictions(x, cv=True), axis=1)\n    data[\"f1\"] = data.apply(F1_score(target_column=\"target\", pred_column=\"all_preds\"), axis=1)\n    print(\"CV Score: {:.3}\".format(data[\"f1\"].mean()))\n    \n\ndata[\"matches\"] = data.apply(lambda x: combine_predictions(x, cv=False), axis=1)","39fc3336":"# Plot F1 Score on product\nplt.figure(figsize = (20, 6))\n\nplot = sns.kdeplot(x = data[\"f1\"])\nplt.title(\"F1 score Distribution\", fontsize=20)\nplt.xlabel(\"F1\", fontsize=15)\nplt.ylabel(\"\");","5dc4d094":"# --- Make a custom plot to save into W&B ---\n### comment when Internet OFF\nrun = wandb.init(project='shopee-kaggle', name='f1_final_scores')\n\n# Prepare data\ncustom_data = [[s] for s in data[\"f1\"]]\n\n# Create Table & .log() the plot\ntable = wandb.Table(data=custom_data, columns=[\"f1\"])\nwandb.log({'f1_hist': wandb.plot.histogram(table, \"f1\",\n                                           title=\"F1 score Distribution\")})\n\nwandb.finish()","f204cdbf":"data[['posting_id','matches']].to_csv('submission.csv',index=False)\nprint(\"Submission Ready :)\")","19fa5aac":"# 7. Final predictions\n\nNow that we have predictions linked to both image and title embeddings, we can combine them and create the final predictions that we'll also submit to the leaderboard.","8bd51c9d":"<img src=\"https:\/\/i.imgur.com\/TodFykz.png\">\n<center><h1>-Model Training & Submission-<\/h1><\/center>\n\n# 1. Introduction\n\ud83d\udfe2 **Goal:** Building a model that can identify which images contain the same product\/s.\n\n\ud83d\udfe0 **To consider**:\n* This competition is a little different, as it doesn't use Supervised ML Techniques, but **Unsupervised** ML Techniques.\n* The goal is to group similar products together: although we have a \"target variable\" (named `label_group`) in the `train` dataset, there can be multiple other types of groups in the `test` dataset (completely unseen during training). Hence, we can't use the `label_group` as our target (`y`) feature.\n\n<div class=\"alert alert-block alert-success\">\n<b>Inspiration:<\/b> HUGE thanks to Chris Deotte for creating a <a href=\"https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700\"> trendsetter notebook with a baseline <\/a>, so we can all get started and to zzy990106 for his <a href=\"https:\/\/www.kaggle.com\/zzy990106\/b0-bert-cv0-9\"> PyTorch version <\/a> on Chris's work.\n<p>This notebook has the purpose of going deeper with the explanations regarding the code and process and an attempt of improving the baseline score as we go along. \ud83d\ude0a<\/p>\n<\/div>\n\n### \ud83d\udcda Libraries + W&B\n\n> You can find my W&B Dashboard on this competition [here](https:\/\/wandb.ai\/andrada\/shopee-kaggle?workspace=user-andrada).","4b43c3d6":"### Bonus: 3D Plotting on Text Embeddings Clusters\n\n> We'll use PCA to downsize the data from 1000 features to only 3.","420c668d":"# 2. Load the Data\n\nLet's read the data, by always taking into account the state of the notebook (whether is in **submission** or **commiting** process).\n* For `submission`, we'll read in `test.csv` data\n* For `commiting`, we'll read in `train.csv`, so we can plot a CV score as well","4ccea9f0":"Now we can create the `dataset` and the `dataloader`. Remember, if:\n* **COMPUTE_CV == True**: `dataset_data` variable will contain `train.csv` data\n* **COMPUTE_CV == False**: `dataset_data` variable will contain `test.csv` data","8a9d20cb":"<img src=\"https:\/\/i.imgur.com\/cUQXtS7.png\">\n\n# Specs on how I trained \u2328\ufe0f\ud83c\udfa8\u00b6\n### (on my local machine)\n* Z8 G4 Workstation \ud83d\udda5\n* 2 CPUs & 96GB Memory \ud83d\udcbe\n* NVIDIA Quadro RTX 8000 \ud83c\udfae\n* RAPIDS version 0.17 \ud83c\udfc3\ud83c\udffe\u200d\u2640\ufe0f","824b5eb8":"# 4. PyTorch Dataset\n\nWe'll create a Dataset class called `ShopeeDataset` that will:\n1. Receive the metadata\n2. Read in the `image` and `title`\n3. Perform image augmentation and tokenization\n4. Return the necessary information to feed into the model afterwards\n\n\n### The Bert Tokenizer ([data from Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased\/code?datasetId=431504&sortBy=voteCount)):\n* Pretrained tokenizer that splits sentences into tokens (source from `transformers` library - [click here for more info](https:\/\/huggingface.co\/transformers\/preprocessing.html))\n* The output is as follows:\n    * `input_ids`: indices corresponding to each token in the sentence\n    * `attention_mask`: indicates to the model which tokens should be attended to, and which should not ([documentation on attention_mask here](https:\/\/huggingface.co\/transformers\/glossary.html#attention-mask))\n<img src=\"https:\/\/i.imgur.com\/3uY3YFi.png\" width=500>","fcb53e46":"# 5. Grouping using Image Embeddings\n\nNow we can safely extract the embeddings from our images using EffNet. You can find more on PyToch EfficientNet [here](https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch).\n\nThe Embeddings are actually the abstract representation of the images:\n* `input`: an image of [3, 256, 256] (3 channels, of size 256x256)\n* `output`: an array of 1000 items which is the abstract representation of the input structure (see image below)\n<img src=\"https:\/\/i.imgur.com\/PjLEVaE.png\" width=550>\n\n## I. Retrieving the embeddings\n\n> **\ud83d\udccc Note**: Because we do not have Internet access for this notebook, we need to import the EffNet model from a dataset. Nikita Kozodoi has kindly already created this for us [here](https:\/\/www.kaggle.com\/kozodoi\/efficientnet-pytorch). \n<img src=\"https:\/\/miro.medium.com\/max\/910\/1*CjpipU_oChc899f_Esjpyg.png\" width=400>","c0d64cf3":"# 6. Grouping using Text Embeddings\n\nAs we also have the `title` of the image available, it would be a shame not to use this data for predicting as well. In this part we'll create a TfIdf Vectorizer to extract these embeddings.\n\n## I. Retrieving the embeddings\n\n> A `TfIdf` Process looks like the example below:\n<img src=\"https:\/\/i.imgur.com\/W2tVXDY.png\" width=700>","8f287396":"> This is how the distribution shows in the W&B dashboard:\n<img src=\"https:\/\/i.imgur.com\/FnL9Br0.png\" width=500>\n\n## \ud83d\udce9 Submission\n\n> **\ud83d\udccc Note**: Don't forget to disable the Internet access before submitting.\n\n<div class=\"alert alert-block alert-warning\">\n<b>Note:<\/b> This notebooks uses internet to connect to the W&B Dashboard. To submit it, you'll have to set the Internet Off and to comment the lines of code that save information into the W&B Project.<\/p>\n<\/div>","24c799a8":"So, without doing anything we have a **CV score** of **0.553**.","2649fb01":"## II. Creating the predictions","34df3723":"> **\ud83d\udccc Note**: The cell below takes ~ 6 mins to run. Hence, I have saved the `image_embeddings` numpy array [here](https:\/\/www.kaggle.com\/andradaolteanu\/shopee-preprocessed-data).\n\n> What we are doing is appending to EACH batch of images (`[16, 1000]`) the `ids` extracted from BERT (`[16, 16]`) and the `masks` (`[16, 16]`) => `[16, 1032]`","c59a30ac":"## II. Creating the predictions\n\nThe competition says that \"group sizes are capped at 50, so there is no benefit to predict more than 50 matches.\" Hence, we'll create clusters of a maximum size of 50.","7274327e":"### Bonus: 3D Plotting on Image Embeddings Clusters\n\n> We'll use PCA to downsize the data from 1000 features to only 3.","6f89de14":"# 3. Competition Metric\n\nLet's now understand the competition metric. I usually like to have this down, as it is a very important part of the prediction process.\n\n*\ud83d\udccc Again, the methodology is highly inspired from [\n[PART 2] - RAPIDS TfidfVectorizer - [CV 0.700]](https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700) \ud83d\udccc*\n\n<img src=\"https:\/\/i.imgur.com\/h3oWxLT.png\" width=800>","b603471d":"> **CV Score: 0.67** with a submission score in Leaderboard of **0.66**.\n<img src=\"https:\/\/i.imgur.com\/QLtVqqq.png\" width=600>","0da37a25":"> When this notebook is commited, the `data` variable will have 34,000 rows. However, when we'll commit it, the `data` will access the 70,000 hidden rows in the `test.csv`. This means that **the amount of observations pushed through the pipeline will double**. To avoid any *memory errors*, you would want to also experiment by pushing ~ 70,000 rows as well, to **make sure your code isn't crushing** somewhere along the way."}}