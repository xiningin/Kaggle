{"cell_type":{"e6bbca1d":"code","6e2e6d71":"code","de633100":"code","c036aef2":"code","607f0118":"code","98a0f67f":"code","7383744a":"code","c794c8ce":"code","7d4a257e":"code","7b467134":"code","178acfb0":"code","876357e7":"code","220619aa":"code","b74adbd7":"code","1ecacf1f":"code","6a6efe43":"code","88687d88":"code","3aeaeeb6":"code","bbd1ae45":"code","a1d1e8d6":"code","a21b1b6f":"code","01e7a0d2":"code","2014d722":"markdown","abdcb6e1":"markdown","f81c1e92":"markdown","a8579d98":"markdown","9550fed2":"markdown","ea5de4ac":"markdown","b8c96c11":"markdown","824ea5ab":"markdown"},"source":{"e6bbca1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e2e6d71":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport mpld3\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix , roc_auc_score , recall_score, precision_score , f1_score , accuracy_score, auc","de633100":"heartdata=pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\nheartdata.head()","c036aef2":"heartdata.output.value_counts()","607f0118":"a = np.arange(0,100,10)\nage_bucket = [str(x)+'-'+str(x+10) for x in a]\n(age_bucket)\nheartdata['age_bucket'] = heartdata['age'].apply(lambda x: age_bucket[x\/\/10])\nheartdata.head()","98a0f67f":"sns.countplot(x='age_bucket', data=heartdata, hue='output', order = heartdata['age_bucket'].value_counts().index)","7383744a":"sns.catplot(x='age_bucket', data=heartdata, hue='output', col='sex', kind='count')","c794c8ce":"heartdata= heartdata.drop('age_bucket',axis=1)","7d4a257e":"c=heartdata.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(c, annot=True)\nplt.show()","7b467134":"cat_features=['caa','cp','restecg','slp','thall']\n#onehotenoding\ndf=heartdata[cat_features]\nenc = OneHotEncoder(handle_unknown='ignore')\nencod=enc.fit(df)\na=encod.transform(df).toarray()\ncols=encod.get_feature_names(cat_features)\ncols","178acfb0":"enc_df=pd.DataFrame(a, columns=cols)\nenc_df.head()","876357e7":"#drop categorical columns\nnew_df=heartdata.drop(cat_features,axis=1)\n#Join 2 dataframes\nnew_df=new_df.join(enc_df)\nnew_df.shape","220619aa":"#normalise numerical columns\nstd=['age','trtbps','chol','thalachh', 'oldpeak']\nscaler = StandardScaler(copy=False)\nscaler.fit(new_df[std])\nstdpd=scaler.transform(new_df[std])","b74adbd7":"new1_df=pd.DataFrame(stdpd,columns=std)\nnew_df=new_df.drop(std,axis=1)\nnew_df=new_df.join(new1_df)\nnew_df.shape","1ecacf1f":"labels=new_df['output']\nnew_df=new_df.drop('output',axis=1)","6a6efe43":"#splitting the dataset into train and test sets. Since its a small dataset, we use most \n#of the samples for training\n\nX_train, X_test, y_train, y_test = train_test_split(new_df, labels, test_size=0.2, random_state=42)","88687d88":"X_train.shape , X_test.shape","3aeaeeb6":"#SVC\nclf_svc=SVC(random_state=42,probability=True).fit(X_train,y_train)\ny_pred=clf_svc.predict(X_test)\nclf_svc.score(X_test,y_test)\ny_pred","bbd1ae45":"tn, fp, fn, tp=confusion_matrix(y_test, y_pred).ravel()\nprob=clf_svc.predict_proba(X_test)\nprob=np.hsplit(prob,2)[1]\nprob=np.array(prob).squeeze()","a1d1e8d6":"fpr, tpr, thresholds = metrics.roc_curve(y_test,prob)","a21b1b6f":"plt.plot(fpr,tpr)\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC: AUC = {:.4f}\".format(roc_auc_score(y_test,y_pred)))\n","01e7a0d2":"recall=recall_score(y_test,y_pred)\nprec=precision_score(y_test,y_pred)\nf1sc=f1_score(y_test,y_pred)\nacc=accuracy_score(y_test,y_pred)\n\nprint (\" Recall : {}\\n Precision : {}\\n F1 score : {}\\n Accuracy : {}\".format(recall,prec,f1sc,acc))","2014d722":"Assuming sex=0 to be female and sex=1 to be male\n1. Most of the data points are for males\n2. Females have a significant number of positive cases for all age groups\n3. In males, most positive cases are in the age group 40-60\n\nDropping the extra column 'age_bucket'","abdcb6e1":"Data processing before training a model.\n1. one hot encode the categorical features\n2. Standardise the numerical features","f81c1e92":"The above model performs quite well with a precision of 93.3% and recall of 87.5% , giving an f1 score of 90.3%\nA high f1 score in turn tells us that the model does good both in terms of recall and precision.\nIn the medical setting, recall is of utmost importance since it is important to not miss out on an acutal positive case. i.e. in this case, it becomes more important to catch the cases with a higher probability of heart attack","a8579d98":"Lets group the age groups into buckets 10-20, 20-30 and so on till 90-100, and then check the distributions in the age buckets.","9550fed2":"The dataset is an imbalanced one w.r.t the age groups i.e we dont have equal number of samples from each age group. The following are the observations:\n1. Most of the samples cover the age ranges 40-70, \n2. In most age ranges the number of positive cases (heart attacks) is more than the negative cases.\n3. In the age range 60-70, number of positive cases in less than the number of negative cases\n\nlets see the distriution between males and females","ea5de4ac":"An important step in analysing how well the model has been trained is to check for the following\n1. Accuracy\n2. Precision\n3. Recall\n4. F1score\n5. AuC\n\nWe use the probabilities output by the model, to draw the area under the ROC. This graph gives us the TPR vs FPR for various thresholds","b8c96c11":"separating the labels for the data, i.e the output column of the dataset","824ea5ab":"Lets look at the correlation between the features. The heatmap tells us the following \n1. The output or prediction is related positively to the cp, thalachh,and slp in decreasing order of importance\n2. The output or prediction is related negatively to the caa, thall, sex,age in decreasing order of importance\n3. There is some positive corelation between features like slp & thalachh, age & caa, trtbps,chol\n4. Negative correlation between age and thalachh"}}