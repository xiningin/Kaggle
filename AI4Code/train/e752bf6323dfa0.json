{"cell_type":{"808004b3":"code","37a87e22":"code","81939927":"code","562a9e55":"code","878dc0d8":"code","1f541db5":"code","6d9c685d":"code","77f6165b":"code","6c95b7bf":"code","8b75ef61":"code","adb31f19":"code","b61d6182":"code","500ac158":"code","009ed439":"code","672c20e2":"code","4f2ac4d0":"code","dfc15582":"code","14b8c499":"code","d593fb50":"code","9ad002e0":"code","11f20758":"code","3f00f1ec":"code","c9ced560":"code","cb54c005":"code","0ece0e83":"code","5e4e61d4":"code","a42e3514":"code","bfde0d52":"code","f0a8e104":"code","626e54e1":"code","b3158342":"code","82993a81":"code","e8e8c231":"code","ce1faa34":"code","ba66b2a9":"code","69f90789":"code","8f290c9f":"code","a4b54420":"markdown","7dccefae":"markdown","31fa6a48":"markdown","9d9080b7":"markdown","9c2ed23c":"markdown"},"source":{"808004b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37a87e22":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns',None)\n","81939927":"bc_df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","562a9e55":"bc_df.head(10)","878dc0d8":"bc_df.dtypes # Except for the diagnosis all the other columns are float values","1f541db5":"bc_df.drop('Unnamed: 32',axis=1,inplace=True)","6d9c685d":"bc_df.shape","77f6165b":"bc_df.isnull().sum() ","6c95b7bf":"sns.pairplot(bc_df,hue='diagnosis')","8b75ef61":"from sklearn.preprocessing import LabelEncoder","adb31f19":"le = LabelEncoder()\nbc_df['diagnosis'] = le.fit_transform(bc_df['diagnosis']) ","b61d6182":"corr_mat = bc_df.corr()","500ac158":"corr_mat['diagnosis'].sort_values(ascending=False) # Getting the highly correlated features with the target column","009ed439":"# Getting the columns that are having multi collinearity\n# Creating a dataframe with correlated column, the correlation value and the source column to which it is correlated\n# Filtering only those that are correlated more than 96%\nmulti_col_df = pd.DataFrame(columns=['corr_col','corr_val','source_col'])\nfor i in corr_mat:\n    temp_df = pd.DataFrame(corr_mat[corr_mat[i]>0.96][i])\n    temp_df = temp_df.reset_index()\n    temp_df['source_col'] = i\n    temp_df.columns = ['corr_col','corr_val','source_col']\n    multi_col_df = pd.concat((multi_col_df,temp_df),axis=0)","672c20e2":"multi_col_df[multi_col_df['corr_val']!=1]","4f2ac4d0":"# Lsting the columns with their correlation value with the target columnn in descending order\ncorr_mat['diagnosis'].sort_values(ascending=False)","dfc15582":"X = bc_df.drop(['id','diagnosis'],axis=1)\ny = bc_df['diagnosis']","14b8c499":"X_trainval, X_test, y_trainval, y_test = train_test_split(X,y,test_size=0.20,random_state=1234)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval,test_size=0.30,random_state=1234)","d593fb50":"X_train_z = zscore(X_train)  \n\nX_val_z = zscore(X_val)\n\nX_test_z = zscore(X_test)","9ad002e0":"knn_clfr = KNeighborsClassifier(n_neighbors=7,weights='distance')","11f20758":"knn_clfr.fit(X_train_z,y_train)","3f00f1ec":"y_predict = knn_clfr.predict(X_val_z)\n","c9ced560":"print(knn_clfr.score(X_train_z, y_train))\nprint(knn_clfr.score(X_val_z, y_val))\nprint(metrics.classification_report(y_val, y_predict))\nprint(metrics.confusion_matrix(y_val, y_predict))","cb54c005":"y_predict = knn_clfr.predict(X_test_z)\n","0ece0e83":"print(knn_clfr.score(X_test_z, y_test))\nprint(metrics.classification_report(y_test, y_predict))\nprint(metrics.confusion_matrix(y_test, y_predict))","5e4e61d4":"bc_df_new =  bc_df.drop(['radius_mean'], axis=1)\nbc_df_new =  bc_df.drop(['perimeter_mean'], axis=1)\nbc_df_new =  bc_df.drop(['radius_worst'], axis=1)\nbc_df_new =  bc_df.drop(['area_worst'], axis=1)\nbc_df_new =  bc_df.drop(['perimeter_se'], axis=1)","a42e3514":"X = bc_df_new.drop(['id','diagnosis'],axis=1)\ny = bc_df_new['diagnosis']","bfde0d52":"X_trainval, X_test, y_trainval, y_test = train_test_split(X,y,test_size=0.20,random_state=1234)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval,test_size=0.30,random_state=1234)","f0a8e104":"X_train_z = zscore(X_train)  \n\nX_val_z = zscore(X_val)\n\nX_test_z = zscore(X_test)","626e54e1":"knn_clfr = KNeighborsClassifier(n_neighbors=5,weights='distance')","b3158342":"knn_clfr.fit(X_train_z,y_train)","82993a81":"y_predict = knn_clfr.predict(X_val_z)\n","e8e8c231":"print(knn_clfr.score(X_train_z, y_train))\nprint(knn_clfr.score(X_val_z, y_val))\nprint(metrics.classification_report(y_val, y_predict))\nprint(metrics.confusion_matrix(y_val, y_predict))","ce1faa34":"y_predict = knn_clfr.predict(X_test_z)\n","ba66b2a9":"print(knn_clfr.score(X_test_z, y_test))\nprint(metrics.classification_report(y_test, y_predict))\nprint(metrics.confusion_matrix(y_test, y_predict))","69f90789":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# creating odd list of K for KNN\nmyList = list(range(1,50))\n\n\n# empty list that will hold cv scores\ncv_scores = []\nk_neighbors = []\n\n# perform 10-fold cross validation\nfor k in myList:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train_z, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    k_neighbors.append(k)\n\n\nMSE = [1 - x for x in cv_scores]\nmin(MSE)\nMSE.index(min(MSE))\nbest_k = myList[MSE.index(min(MSE))]\nprint (\"The optimal number of neighbors is %d\" % best_k)","8f290c9f":"%matplotlib inline \nimport matplotlib.pyplot as plt\n\nfig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] = 18\nfig_size[1] = 9\nplt.rcParams[\"figure.figsize\"] = fig_size\n\nplt.xlim(0,25)\n\n\n# plot misclassification error vs k\nplt.plot(k_neighbors, MSE)\n\n\n\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","a4b54420":"Creating a model without removing multicollinearity","7dccefae":"The pairplot above clearly shows that there are multicollinearity between several attributes, it is not good to have the multicollinearity. Also we can see that there are few variables that are somewhat good in distinguishing the classes, this can be seen in the density graph","31fa6a48":"It is clearly seen the column radius mean is highly correlated with perimeter_mean, area_mean, radius_worst, permieter_worst each above 96%. \nHaving all these column might affect the performance of the model. Similarly we have other columns with multicollinearity we have to make sure this is handled effectively. One of the effective means to handle it is to use PCA. I am just retaining the important column based on its correlation with the target columns.","9d9080b7":"There are no null values in any of the columns.","9c2ed23c":"I am still working on this, will do few more feature engineering and update"}}