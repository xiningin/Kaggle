{"cell_type":{"4788e918":"code","9214ce93":"code","09173696":"code","e33389d4":"code","edd2c489":"code","9da32091":"code","d95315da":"code","44a41f2a":"code","e9f6cc6c":"code","b6ea3d26":"code","10a64a9e":"code","b6c5be06":"code","e4f78cd0":"code","97f08f30":"code","3e6f7f79":"code","ff6ad61e":"code","6f036089":"code","4a106080":"code","573b9d8f":"code","b44a60f1":"code","90596ab6":"code","c952cfba":"code","25802358":"code","23b2ae88":"code","58d327f0":"code","9736add4":"code","816a726b":"code","cf16d3e3":"code","65f82320":"code","273cbb99":"code","7362a229":"code","4a2ee062":"code","8f618503":"code","a9a5b009":"code","59fcf074":"code","5a3125cb":"code","7753d6ce":"code","d843cd04":"code","27562529":"code","ac9bc920":"code","141dcd53":"code","615d23ee":"code","164c1012":"code","4b21097f":"code","a2027329":"code","0909b289":"code","ab4bb3d0":"code","942e2385":"code","864e5686":"code","68a2d6b1":"code","9c37dc51":"code","b3faff10":"code","bbbeda75":"code","24784842":"code","77e10b64":"code","6382a8bb":"code","5f17490c":"code","24576529":"code","2fa5d97e":"code","a7d6117c":"code","2e21d1ff":"code","f9c1b072":"code","799a68c1":"code","0a48cfb1":"code","4421fe88":"code","e46c5878":"code","677845f2":"code","7a2ee379":"code","79c4bf8b":"code","6b98e859":"markdown","86131bfa":"markdown","5d374ab7":"markdown","2950aa65":"markdown","d077e634":"markdown","a5768046":"markdown","0edec5df":"markdown","ee45039d":"markdown","fda4b899":"markdown","ec42ba95":"markdown","eb561c99":"markdown","e413f531":"markdown","ed1cff99":"markdown","bb4e08cd":"markdown","4a68082f":"markdown","0d346389":"markdown","8389a320":"markdown","1a145605":"markdown","2117b0b2":"markdown","ffa3c57e":"markdown","77b858bf":"markdown","cc4f940a":"markdown","71b2d058":"markdown","d7ec2a82":"markdown","325f0370":"markdown","5e3f4846":"markdown","e8b70015":"markdown","dcc954ee":"markdown","3ae9e218":"markdown","5a5b5a09":"markdown","19c50d13":"markdown","cb9ccb71":"markdown","8fe0fc8d":"markdown","5c03b687":"markdown","957101d0":"markdown","0b4f525a":"markdown","b11917bb":"markdown","ced25365":"markdown","0e34d884":"markdown","555533ad":"markdown","cb6f9b3d":"markdown","4d3d0045":"markdown","1f38f07a":"markdown","7bbea310":"markdown","18d29578":"markdown","b71ab89e":"markdown"},"source":{"4788e918":"# importing libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import normalize\n\nfrom keras.models import Sequential \nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers.normalization import BatchNormalization","9214ce93":"# loading the train dataset\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\nprint(\"Number of data points:\",train_df.shape[0])","09173696":"# loading the test dataset\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Number of data points:\",test_df.shape[0])","e33389d4":"# combining both the datasets for EDA\ntitanic = pd.concat([train_df, test_df], sort=False)","edd2c489":"titanic.head()","9da32091":"titanic.info()","d95315da":"titanic.describe()","44a41f2a":"titanic.isnull().sum()","e9f6cc6c":"# replacing the missing values of the Cabin column with 'unknown'\ntitanic.Cabin = titanic.Cabin.fillna(\"unknown\")","b6ea3d26":"# replacing the missing value of Embarked with the mode of the column\ntitanic.Embarked = titanic.Embarked.fillna(titanic['Embarked'].mode()[0])","10a64a9e":"# replacing the missing value of Fare with the mean of the column\ntitanic.Fare = titanic.Fare.fillna(titanic['Fare'].mean())","b6c5be06":"#using the title column to fill the age column\ntitanic['title']=titanic.Name.apply(lambda x: x.split('.')[0].split(',')[1].strip())","e4f78cd0":"newtitles={\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"}","97f08f30":"titanic['title']=titanic.title.map(newtitles)","3e6f7f79":"titanic.groupby(['title','Sex']).Age.mean()","ff6ad61e":"def newage (cols):\n    title=cols[0]\n    Sex=cols[1]\n    Age=cols[2]\n    if pd.isnull(Age):\n        if title=='Master' and Sex==\"male\":\n            return 4.57\n        elif title=='Miss' and Sex=='female':\n            return 21.8\n        elif title=='Mr' and Sex=='male': \n            return 32.37\n        elif title=='Mrs' and Sex=='female':\n            return 35.72\n        elif title=='Officer' and Sex=='female':\n            return 49\n        elif title=='Officer' and Sex=='male':\n            return 46.56\n        elif title=='Royalty' and Sex=='female':\n            return 40.50\n        else:\n            return 42.33\n    else:\n        return Age","6f036089":"titanic.Age=titanic[['title','Sex','Age']].apply(newage, axis=1)","4a106080":"titanic.groupby('Survived')['PassengerId'].count().plot.bar()","573b9d8f":"# from the above plot\nprint('Passengers that survived {} %'.format(round(titanic['Survived'].mean()*100,2)))\nprint('Passengers that did not survive {} %'.format(100 - round(titanic['Survived'].mean()*100,2)))","b44a60f1":"corr = train_df.corr()\nsns.heatmap(corr, cbar=True, annot=True, square=True, \n            fmt='.2f', annot_kws={'size': 10}, \n            yticklabels=corr.columns.values, xticklabels=corr.columns.values)","90596ab6":"n = titanic.shape[0]\nsns.pairplot(titanic[['Pclass', 'Fare', 'Age', 'SibSp', 'Parch', 'Survived']][0:n], hue='Survived', \n             vars=['Pclass', 'Fare', 'Age', 'SibSp', 'Parch'])\nplt.show()","c952cfba":"plt.figure(figsize=[12,10])\nplt.subplot(2,2,1)\nsns.barplot('Sex','Survived',data=train_df)\nplt.subplot(2,2,2)\nsns.barplot('Embarked','Survived',data=train_df)","25802358":"# taking the count of characcters in Name\ntitanic['Name1'] = titanic.Name.apply(lambda x:len(x))","23b2ae88":"titanic['HasCabin'] = titanic['Cabin'].apply(lambda x:0 if x=='unknown' else 1)","58d327f0":"titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch']","9736add4":"titanic['IsAlone'] = titanic['FamilySize'].apply(lambda x:1 if x==0 else 0)","816a726b":"titanic = titanic.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'],axis=1)","cf16d3e3":"# performing one hot encoding of the categorical features\ntitanic = pd.get_dummies(titanic)","65f82320":"# now the dataset is ready, ML model can be trained on it\ntitanic.head()","273cbb99":"corr = titanic.corr()\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr, cbar=True, annot=True, square=True, \n            fmt='.2f', annot_kws={'size': 8},\n            yticklabels=corr.columns.values, xticklabels=corr.columns.values)","7362a229":"train_len = train_df.shape[0]\ntrain=titanic[:train_len]\ntest=titanic[train_len:]","4a2ee062":"# changing the type of the class label from float to int\ntrain.Survived=train.Survived.astype('int')\ntrain.Survived.dtype","8f618503":"X_train = train.drop(\"Survived\",axis=1)\ny_train = train['Survived']\nX_test = test.drop(\"Survived\", axis=1)","a9a5b009":"# normalizing the train & test dataset\nX_train = normalize(X_train)\nX_test = normalize(X_test)","59fcf074":"print(X_train.shape)\nprint(X_test.shape)","5a3125cb":"# initializing Logistic Regression model with L2 regularisation\nlr = LogisticRegression(penalty='l2')\n\n# C values we need to try on classifier\nC = [1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\nparam_grid = {'C':C}\n\n# using GridSearchCV to find the optimal value of C\n# using roc_auc as the scoring parameter & applying 10 fold CV\ngscv = GridSearchCV(lr,param_grid,scoring='accuracy',cv=10,return_train_score=True)\n\ngscv.fit(X_train,y_train)\n\nprint(\"Best C Value: \",gscv.best_params_)\nprint(\"Best Accuracy: %.5f\"%(gscv.best_score_))","7753d6ce":"# determining optimal C\noptimal_C = gscv.best_params_['C']\n\n#training the model using the optimal C\nlrf = LogisticRegression(penalty='l2', C=optimal_C)\nlrf.fit(X_train,y_train)\n\n#predicting the class label using test data \ny_pred = lrf.predict(X_test)","d843cd04":"# confusion matrix on train data\ny_predict = lrf.predict(X_train)\ncm = confusion_matrix(y_train, y_predict)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g')","27562529":"output=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred})\noutput.to_csv('submission.csv', index=False)","ac9bc920":"# initializing Linear SVM model with L1 regularisation\nsvm = SGDClassifier(loss='hinge', penalty='l1')\n\n# C values we need to try on classifier\nalpha_values = [1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001,0.00001]\nparam_grid = {'alpha':alpha_values}\n\n# using GridSearchCV to find the optimal value of alpha\n# using roc_auc as the scoring parameter & applying 10 fold CV\ngscv = GridSearchCV(svm,param_grid,scoring='accuracy',cv=10,return_train_score=True)\n\ngscv.fit(X_train,y_train)\n\nprint(\"Best alpha Value: \",gscv.best_params_)\nprint(\"Best Accuracy: %.5f\"%(gscv.best_score_))","141dcd53":"# determining optimal alpha\noptimal_alpha = gscv.best_params_['alpha']\n\n#training the model using the optimal alpha\nsvm = SGDClassifier(loss='hinge', penalty='l1', alpha=optimal_alpha)\nsvm.fit(X_train,y_train)\n\n#predicting the class label using test data \ny_pred = svm.predict(X_test)","615d23ee":"# confusion matrix on train data\ny_predict = svm.predict(X_train)\ncm = confusion_matrix(y_train, y_predict)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g')","164c1012":"output=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred})\noutput.to_csv('submission.csv', index=False)","4b21097f":"# initializing KNN model \nknn = KNeighborsClassifier(weights='uniform')\n\n# C values we need to try on classifier\nneighbors = [5, 7, 9, 11, 15, 21, 25, 31, 35, 41, 47, 52]\nparam_grid = {'n_neighbors':neighbors}\n\n# using GridSearchCV to find the optimal value of k\n# using roc_auc as the scoring parameter & applying 10 fold CV\ngscv = GridSearchCV(knn,param_grid,scoring='accuracy',cv=10,return_train_score=True)\n\ngscv.fit(X_train,y_train)\n\nprint(\"Best k Value: \",gscv.best_params_)\nprint(\"Best Accuracy: %.5f\"%(gscv.best_score_))","a2027329":"# determining optimal neighbors\noptimal_k = gscv.best_params_['n_neighbors']\n\n#training the model using the optimal k\nknn = KNeighborsClassifier(n_neighbors=optimal_k, weights='uniform')\nknn.fit(X_train,y_train)\n\n#predicting the class label using test data \ny_pred = knn.predict(X_test)","0909b289":"# confusion matrix on train data\ny_predict = knn.predict(X_train)\ncm = confusion_matrix(y_train, y_predict)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g')","ab4bb3d0":"output=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred})\noutput.to_csv('submission.csv', index=False)","942e2385":"# initializing DT model \ndt = DecisionTreeClassifier(class_weight='balanced')\n\n# max_depth values we need to try on classifier\ndepth = [5, 7, 9, 11, 15, 21, 25, 31, 35, 41, 47, 52]\nparam_grid = {'max_depth':depth}\n\n# using GridSearchCV to find the optimal value of k\n# using roc_auc as the scoring parameter & applying 10 fold CV\ngscv = GridSearchCV(dt,param_grid,scoring='accuracy',cv=10,return_train_score=True)\n\ngscv.fit(X_train,y_train)\n\nprint(\"Best depth Value: \",gscv.best_params_)\nprint(\"Best Accuracy: %.5f\"%(gscv.best_score_))","864e5686":"# determining optimal max_depth\noptimal_depth = gscv.best_params_['max_depth']\n\n#training the model using the optimal k\ndt = DecisionTreeClassifier(max_depth=optimal_depth, class_weight='balanced')\ndt.fit(X_train,y_train)\n\n#predicting the class label using test data \ny_pred = dt.predict(X_test)","68a2d6b1":"# confusion matrix on train data\ny_predict = dt.predict(X_train)\ncm = confusion_matrix(y_train, y_predict)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g')","9c37dc51":"output=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred})\noutput.to_csv('submission.csv', index=False)","b3faff10":"rf=RandomForestClassifier(random_state=1)\n\nparams={'n_estimators': list(range(10,100,10)),\n      'max_depth':[3,4,5,6,7,8,9,10],\n      'criterion':['gini','entropy']}\n\ngscv=GridSearchCV(estimator=rf, param_grid=params, scoring='accuracy', cv=10, return_train_score=True)\ngscv.fit(X_train,y_train)\nprint(\"Best C Value: \",gscv.best_params_)\nprint(\"Best Accuracy: %.5f\"%(gscv.best_score_))","bbbeda75":"#training the model using the optimal params\nrf = RandomForestClassifier(random_state=1, criterion='gini', max_depth= 4, n_estimators=20)\nrf.fit(X_train,y_train)\n\n#predicting the class label using test data \ny_pred = rf.predict(X_test)","24784842":"# confusion matrix on train data\ny_predict = rf.predict(X_train)\ncm = confusion_matrix(y_train, y_predict)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g')","77e10b64":"output=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred})\noutput.to_csv('submission.csv', index=False)","6382a8bb":"param_grid={\n    'max_depth':list(range(2,10)),\n    'n_estimators':list(range(50,500,50)),\n}\n\nxgb = XGBClassifier(objective='binary:logistic')\nrscv = GridSearchCV(xgb, param_grid, scoring='accuracy', n_jobs=-1, return_train_score=True)\nrscv.fit(X_train, y_train)\nprint(\"Best Max_Depth:\",rscv.best_params_['max_depth'])\nprint(\"Best N estimators:\",rscv.best_params_['n_estimators'])\nprint(\"Best Accuracy: %.5f\"%(rscv.best_score_))","5f17490c":"#training the model using the optimal params\nxgb = XGBClassifier(learning_rate=0.2, max_depth=4, n_estimators=150)\nxgb.fit(X_train,y_train)\n\n#predicting the class label using test data \ny_pred = xgb.predict(X_test)","24576529":"# confusion matrix on train data\ny_predict = xgb.predict(X_train)\ncm = confusion_matrix(y_train, y_predict)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g')","2fa5d97e":"features = X_train.columns\nimportances = xgb.feature_importances_\nindices = (np.argsort(importances))\nplt.figure(figsize=(8,8))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='r', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","a7d6117c":"output=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred})\noutput.to_csv('submission.csv', index=False)","2e21d1ff":"clf1 = DecisionTreeClassifier(max_depth=5, class_weight='balanced')\nclf2 = LogisticRegression(penalty='l2',C=10)\nclf3 = XGBClassifier(learning_rate=0.2, max_depth=4, n_estimators=150)\nrf = RandomForestClassifier(criterion='entropy', max_depth= 9, n_estimators=90)\n\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=rf)\n\nsclf.fit(X_train, y_train)","f9c1b072":"#predicting the class label using test data \ny_pred = sclf.predict(X_test)","799a68c1":"# confusion matrix on train data\ny_predict = sclf.predict(X_train)\ncm = confusion_matrix(y_train, y_predict)\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g')","0a48cfb1":"output=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred})\noutput.to_csv('submission.csv', index=False)","4421fe88":"from keras.utils import to_categorical\ny_train_new = to_categorical(y_train)","e46c5878":"# using He Normalization for weights initialization\nmodel = Sequential()\n\n# Layer 1 - 64 neurons\nmodel.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\nmodel.add(BatchNormalization())\n\n# Layer 2 - 32 neurons\nmodel.add(Dense(32, activation='relu', kernel_initializer='he_normal'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.summary()\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train_new, batch_size=128, epochs=10, verbose=1)","677845f2":"y_pred = model.predict(X_test)\ny_classes = y_pred.argmax(axis=-1)","7a2ee379":"output=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_classes})\noutput.to_csv('submission.csv', index=False)","79c4bf8b":"from prettytable import PrettyTable \nx = PrettyTable()\nx.field_names = [\"Model\", \"Train Accuracy(%)\", \"Test Accuracy(%)\"]\nx.add_row(['Logistic Regression', '82.49', '77.51'])\nx.add_row(['Support Vector Machines', '77.44', '72.24'])\nx.add_row(['K Nearest Neighbor', '72.84', '70.34'])\nx.add_row(['Decision Tree', '80.80', '76.55'])\nx.add_row(['Random Forest', '83.95', '79.42'])\nx.add_row(['XGBoost', '83.61', '75.11'])\nx.add_row(['Stacking Classifier', '-', '73.68'])\nx.add_row(['Neural Networks', '81.59', ''])\nprint(x)","6b98e859":"**By Aziz Presswala**","86131bfa":"## 3. Preprocessing & Feature Engineering","5d374ab7":"**Result** - This model has an accuracy score of **77.51%** on test data","2950aa65":"### Bar Plots","d077e634":"**Result** - This model has an accuracy score of **76.55%** on test data","a5768046":"## Random Forest","0edec5df":"## Kaggle Competition","ee45039d":"### Loading the Data","fda4b899":"# Titanic : Machine Learning from Disaster","ec42ba95":"## XGBoost","eb561c99":"**Result** - This model has an accuracy score of **70.34%** on test data","e413f531":"## 5. Conclusion","ed1cff99":"### Performance Metric","bb4e08cd":"## Stacking Classifier","4a68082f":"**Result** - This model has an accuracy score of **73.68%** on test data","0d346389":"- Data is given in 2 files - train.csv (for training the model) & test.csv (for testing the accuracy of the model)\n- Given data contains 12 columns - PassengerID, PClass, SibSp, Parch, Age, Fare, Sex, Name, Ticket, Cabin, Embarked, Survived\n- Size of train.csv: 59.7KB, test.csv: 27.9KB\n- Number of Rows in train.csv: 891, test.csv: 418","8389a320":"### Correlation Matrix","1a145605":"- It is a binary classification problem.\n- Given data about the passengers, task is to predict whether a passenger will survive the disaster or not.\n- 1 if the passenger survived, 0 if did not survive.","2117b0b2":"**Result** - This model has an accuracy score of **72.24%** on test data","ffa3c57e":"- Feature - Name, a good strategy can be to take the count of number of characters\n- Feature - Cabin, a good strategy will be to make a new column HasCabin.\n- Feature - Ticket has no significance, therfore it can be dropped.\n- Feature - Sex & Embarked, we can perform one hot encoding.\n- Creating 2 new features - FamilySize (Parch + SibSp) & IsAlone.","77b858bf":"**Result** - This model has an accuracy score of **75.11%** on test data","cc4f940a":"- Columns: Age, Fare, Cabin & Embarked have missing values","71b2d058":"### Pair plots of features - [Pclass, Fare, Age, Sibsp, Parch]","d7ec2a82":"### Checking for NULL\/Missing values","325f0370":"## Neural Networks","5e3f4846":"### Splitting into Train & Test","e8b70015":"Steps:-\n1. Machine Learning Problem\n2. Exploratory Data Analysis\n3. Preprocessing & Feature Engineering\n4. Applying Machine Learning Models\n5. Conclusion","dcc954ee":"### Distribution of datapoints among output classes","3ae9e218":"## 1. Machine Learning Problem","5a5b5a09":"## Logistic Regression","19c50d13":"From the above matrix, it is evident that no feature has a high correlation with the class label - Survived.","cb9ccb71":"- 3 columns of float datatype\n- 4 columns of int datatype\n- 5 columns of string","8fe0fc8d":"Observations:-\n- Probability of survival of females is significantly more than males.\n- Probability of survival of people who embarked from Cherbourg(C) is high.","5c03b687":"## K Nearest Neighbors","957101d0":"- Accuracy (No. of correctly classified pts \/ Total no. of pts)\n- Confusion Matrix","0b4f525a":"### Data","b11917bb":"- Here we can see that the engineered features - title, Name1, HasCabin, IsAlone have a high correlation with the class label Survived.","ced25365":"### Correlation Matrix for new features","0e34d884":"### Type of ML Problem","555533ad":"## 4. Applying Machine Learning Models","cb6f9b3d":"## Decision Tree","4d3d0045":"**Result** - This model has an accuracy score of **79.42%** on test data","1f38f07a":"Observations:-\n- Passengers of Class 1 & 2 were given more preference than passengers of class 3.\n- Passengers whose Ticket Fare was more were a higher preference.\n- Passengers who did'nt have any parents\/children onboard were given a higher preference.\n- The Pclass vs Age plot can classify the datapoints to a certain extent, therefore, Age & Pclass are very important features.","7bbea310":"- From the above table we conclude that Random Forest model performs the best with 79.42% test accuracy followed by Logistic Regression & Decision Tree.","18d29578":"## 2. Exploratory Data Analysis","b71ab89e":"## Support Vector Machines"}}