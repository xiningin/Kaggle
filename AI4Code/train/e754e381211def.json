{"cell_type":{"e99b7ebf":"code","056114d7":"code","e4db92ae":"code","98c21e5d":"code","c554edc4":"code","f137a42f":"code","1679267b":"code","91cdbdf7":"code","c6486865":"code","f311acb2":"code","f9ef1501":"code","59828008":"code","b44e593f":"code","148aa244":"code","9f7657b8":"code","3c8e77da":"code","9345a28e":"code","0b1ae974":"code","5cc231a2":"code","cf0e7851":"code","f776b1f0":"code","01e0c275":"code","4abe0804":"code","3e872e2e":"code","ab127487":"code","bf940c17":"code","14c17634":"code","401af875":"code","81ab6f8e":"code","2f956db7":"code","dcad76ba":"code","b84dd8f8":"code","6b80d714":"code","c9d1a855":"code","94a213f1":"code","b0c1a24f":"code","05c42049":"code","bfd4a41b":"code","6d40b2f3":"code","ca7563f4":"markdown","7d64c9ec":"markdown","ddf33904":"markdown","8819355f":"markdown","4a01581b":"markdown","1f5965ad":"markdown","f152dd1b":"markdown","3212730c":"markdown"},"source":{"e99b7ebf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, sent_tokenize\n\nimport keras\nfrom keras.preprocessing.text import Tokenizer, one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Flatten, SpatialDropout1D, Input, Bidirectional, Dropout\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\nfrom keras.regularizers import l2\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.metrics import roc_auc_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n#constants\neng_stopwords = set(stopwords.words(\"english\"))\nfrom nltk.stem.wordnet import WordNetLemmatizer \nlem = WordNetLemmatizer()\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","056114d7":"#Importing datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","e4db92ae":"train_data = train['comment_text']\ntest_data = test['comment_text']\ntotal_data = pd.DataFrame(train_data.append(test_data), columns = ['comment_text'])\n\nprint('Total_data: ', total_data.shape)\ntotal_data.head()","98c21e5d":"y = train.iloc[:,2:]\ny.head()","c554edc4":"#checking for missing values\nprint(\"Missing values in total_data: \",total_data.isnull().sum())\nprint(\"Missing values in target variable: \",y.isnull().sum())","f137a42f":"total_data['total_length'] = total_data['comment_text'].apply(len)\ntotal_data['capitals'] = total_data['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\ntotal_data['caps_vs_length'] = total_data.apply(lambda row: float(row['capitals'])\/float(row['total_length']),\n                                axis=1)\ntotal_data['num_exclamation_marks'] = total_data['comment_text'].apply(lambda comment: comment.count('!'))\ntotal_data['num_question_marks'] = total_data['comment_text'].apply(lambda comment: comment.count('?'))\ntotal_data['num_punctuation'] = total_data['comment_text'].apply(\n    lambda comment: sum(comment.count(w) for w in '.,;:'))\ntotal_data['num_symbols'] = total_data['comment_text'].apply(\n    lambda comment: sum(comment.count(w) for w in '*&$%'))\ntotal_data['num_words'] = total_data['comment_text'].apply(lambda comment: len(comment.split()))\ntotal_data['num_unique_words'] = total_data['comment_text'].apply(\n    lambda comment: len(set(w for w in comment.split())))\ntotal_data['words_vs_unique'] = total_data['num_unique_words'] \/ total_data['num_words']\ntotal_data['num_smilies'] = total_data['comment_text'].apply(\n    lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))","1679267b":"import gensim\n#to seperate sentenses into words\ndef preprocess(comment):\n    \"\"\"\n    Function to build tokenized texts from input comment\n    \"\"\"\n    return gensim.utils.simple_preprocess(comment, deacc=True, min_len=3)","91cdbdf7":"#tokenize the comments\nall_text = total_data.comment_text.apply(lambda x: preprocess(x))","c6486865":"\ndef clean(word_list):\n    \"\"\"\n    Function to clean the pre-processed word lists \n    \n    Following transformations will be done\n    1) Stop words removal from the nltk stopword list\n    2) Lemmatization (Converting word to its root form : babies --> baby ; children --> child)\n    \"\"\"\n    #remove stop words\n    clean_words = [w for w in word_list if not w in eng_stopwords]\n\n    #Lemmatize\n    clean_words=[lem.lemmatize(word, \"v\") for word in clean_words]\n    return(clean_words) ","f311acb2":"#scale it to all text\nall_text = all_text.apply(lambda x:clean(x))","f9ef1501":"#checking number of comments clean\/unclean\nprint('Total number of comments in train data: ', train.shape[0])\nprint('Total number unclean comments train data: ', (y.sum()).sum())\n#marking comments with no labels as clean\nprint('Total number clean comments train data: ', (train.shape[0]-(y.sum()).sum()))","59828008":"#for clean comments we will make another column. We might need to remove this column in the end.\nrowsums = y.sum(axis=1)\ny['clean'] = (rowsums==0)","b44e593f":"# Checking class imbalance\nplt.figure(figsize=(8,4))\na = y.sum()\nax = sns.barplot(a.index, a.values, alpha=0.8)\nplt.title('Class Balance')\nplt.xlabel('Classes')\nplt.ylabel('# of Occurrences', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = a.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","148aa244":"y.shape[0]","9f7657b8":"#checking number of comments having multiple tags\nk = rowsums.value_counts()\nk","3c8e77da":"print('Number of comments with multiple tags: ', np.sum(k[2:7]))","9345a28e":"temp = y.iloc[:,:-1]\ncorr = temp.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True)","0b1ae974":"#checking proportion of different classes in train data\np = [i*100\/a.sum() for i in a]\n\nmapped = zip(a.index, p)\nset(mapped)","5cc231a2":"y['clean'] = [1 if i is True else 0 for i in y['clean']]","cf0e7851":"m=1\nplt.figure(figsize=(20,20))\nfor i in y.columns[:-1]:\n    plt.subplot(3,3,m)\n    subset=train[train[i]==1]\n    text=subset.comment_text.values\n    wc= WordCloud(background_color=\"black\",max_words=2000)\n    wc.generate(\" \".join(text))\n    plt.title(i, fontsize=50)\n    plt.imshow(wc.recolor(colormap= 'summer'), alpha=0.98)\n    m = m+1","f776b1f0":"'''\ngensim is python library for training word embeddings in given data\nfor more information visit: \n1. https:\/\/machinelearningmastery.com\/develop-word-embeddings-python-gensim\/\n2. http:\/\/kavita-ganesan.com\/gensim-word2vec-tutorial-starter-code\/#.XEoWKVwzbIV\n'''\nimport gensim\n\nembedding_vector_size = 100\n# now training embeddings for each word \nmodel_1 = gensim.models.Word2Vec(sentences = all_text, size=embedding_vector_size, min_count=1, window=5, workers=4 )\n\n# to get total number of unique words\nwords = list(model_1.wv.vocab)\n\nprint(\"vocab size:\", len(words))","01e0c275":"#len(sequence)\nlength = [len(x) for x in all_text]\nplt.hist(length)\nplt.xlabel('length of words')\nplt.ylabel('frequency')","4abe0804":"'''\nNow we have trained the embeddings, we now have embedding vector for each word. We will\nconvert our text training data to numeric using theseword embeddings.\nFirst, we need to make length of each input same, therefore we'll do padding. But padding happends \non numeric data, therefore we'll convert texts to sequences using tokenize() function. Then add padding\nThen we'll replace each non-zero numeric resulted from texts to sequences to its corresponding word\nembedding.\n'''\nmax_len = 130  \nmax_features = 6000\ntokenizer = Tokenizer(num_words=max_features)       #keeps 6000 most common words\ntrain_test_data = all_text                       # contains word tokens extracted from lines\ntokenizer.fit_on_texts(train_test_data)\nsequence = tokenizer.texts_to_sequences(train_test_data)\ntrain_test_data = pad_sequences(sequence, maxlen = max_len)","3e872e2e":"'''\n# Preparing embedding matrix\nvocab_size = len(tokenizer.word_index)+1\nembedding_matrix = np.zeros((vocab_size, embedding_vector_size))\n# +1 is done because i starts from 1 instead of 0, and goes till len(vocab)\nfor  word, i in tokenizer.word_index.items():\n    embedding_vector = model_1.wv[word]\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n'''","ab127487":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embedding_vector_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = model_1.wv[word]\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","bf940c17":"# Separating training and test data\nntrain = train.shape[0]\nX = train_test_data[:ntrain,:]\ntest_data = train_test_data[ntrain:,:]\nX = pd.DataFrame(X)\nX = pd.concat([X,total_data.iloc[:ntrain,1:]],axis=1)\ntest_data = pd.DataFrame(test_data)\ntest_data = pd.concat([test_data,total_data.iloc[ntrain:,1:]],axis=1)","14c17634":"remove_n = 70000\ny = y.reset_index()\ndrop_indices = np.random.choice(y['clean'].index, remove_n, replace=False)\n#df_subset = X['clean'].drop(drop_indices)\n","401af875":"X = X.reset_index()","81ab6f8e":"print('Shape before dropping rows having clean labels: ', X.shape)\nX=  X.drop(['index'],axis=1)\nX = X.drop(drop_indices, axis=0)\n\nprint('Shape after dropping rows having clean labels: ', X.shape)","2f956db7":"y1 = y.iloc[:,:-1]\ny1 = y1.drop(drop_indices)\ny1=  y1.drop(['index'],axis=1)","dcad76ba":"X_train, X_test, y_train, y_test = train_test_split(X , y1, test_size=0.2, random_state=42, shuffle=True)","b84dd8f8":"model = Sequential()\n\nmodel.add(Embedding(input_dim = max_features, output_dim = embedding_vector_size, \n                    input_length = X.shape[1], weights = [embedding_matrix]))\nmodel.add(Bidirectional(LSTM(64, dropout=0.25, recurrent_dropout=0.1, return_sequences=True)))\nmodel.add(LSTM(32, return_sequences=True, dropout=0.25,))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(16, dropout=0.25,))  # return a single vector of dimension 32\nmodel.add(Dense(10))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['acc'])\nprint(model.summary())","6b80d714":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","c9d1a855":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_test, self.y_test = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_test, verbose=0)\n            score = roc_auc_score(self.y_test, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","94a213f1":"RocAuc = RocAucEvaluation(validation_data=(X_test, y_test), interval=1)","b0c1a24f":"history = model.fit(X_train, y_train, epochs = 6, batch_size = 1000, validation_data=(X_test, y_test),\n                 callbacks=[RocAuc])","05c42049":"import matplotlib.pyplot as plt\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","bfd4a41b":"y_pred = model.predict(test_data)\ny_pred = pd.DataFrame(y_pred, columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])","6d40b2f3":"y_pred.to_csv('submission.csv', index=False)","ca7563f4":"This might not give us much idea since many comments have multile taggings","7d64c9ec":"Now we have cleaned the data, we will now try to analyze the data.","ddf33904":"Notice that number of comments are 159571 but adding total number of comments from plot shows that there are multiple tags for many comments.","8819355f":"Now we have to correct multiple tag issue by checking which tags go together more often. This can be done by checking correlation between different classes.","4a01581b":"Since we have high class imbalance, we are randomly removing 70K 'clean' labeled observations.","1f5965ad":"Multiple tags are where sum is greater than 1","f152dd1b":"\n'''\nHere we will do preprocessing\n1. Removing punctuations\n2. Lowering all words\n3. removing non-alphabet things\n4. removing stop words\n5. Tokenizing the sentence\n'''\nimport string\n\nreview_lines = list()\nlines = total_data['comment_text'].values.tolist()\n\nfor line in lines:\n    \n    '''\n    breaks line into it's sub parts like each word and comma etc,\n    https:\/\/pythonspot.com\/tokenizing-words-and-sentences-with-nltk\/\n    '''\n    tokens = word_tokenize(line)   \n    \n     #convert to lower case\n    tokens = [w.lower() for w in tokens]\n    \n    #remove punctuation from each word\n    # brief detail: https:\/\/pythonadventures.wordpress.com\/2017\/02\/05\/remove-punctuations-from-a-text\/\n    #table = str.maketrans('','', string.punctuation)\n    #stripped = [w.translate(table) for w in tokens]\n     \n    # remove remaining tokens that are not alphabetic\n    words = [w for w in tokens if w.isalpha()]\n    \n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if w not in stop_words]\n    \n    review_lines.append(words)\n","3212730c":"Feature Engineering:\n* Length of the comment - my initial assumption is that angry people write short messages\n* Number of capitals - observation was many toxic comments being ALL CAPS\n* Proportion of capitals - see previous\n* Number of exclamation marks - i observed several toxic comments with multiple exclamation marks\n* Number of question marks - assumption that angry people might not use question marks\n* Number of punctuation symbols - assumption that angry people might not use punctuation\n* Number of symbols - assumtion that words like fck or $# or sh*t mean more symbols in foul language (Thx for tip!)\n* Number of words - angry people might write short messages?\n* Number of unique words - observation that angry comments are sometimes repeated many times\n* Proportion of unique words - see previous\n* Number of (happy) smilies - Angry people wouldn't use happy smilies, right?"}}