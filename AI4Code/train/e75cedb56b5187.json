{"cell_type":{"0bbf5822":"code","cfb05249":"code","7610d095":"code","1cbb4041":"code","cbdf46d4":"code","586e856f":"code","876eb817":"code","e548ec62":"code","17c02177":"code","681ea359":"code","14ae2c21":"code","0049eaa4":"code","456ef836":"code","0fe1fb3a":"code","d5c3a7ed":"code","9227cbc6":"markdown"},"source":{"0bbf5822":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport xgboost as xgb\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.externals import joblib\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix, make_scorer\n\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam, SGD, RMSprop, Adagrad\n","cfb05249":"def plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\ndef train_to_test_split(train_x, train_y):\n    sc = MinMaxScaler()\n    train_x = sc.fit_transform(train_x)\n#     joblib.dump(sc.fit(train_x), scaler_filename) \n    return train_test_split(train_x, train_y, test_size=0.3, random_state=5)\n\n\ndef kfold(clf):\n    kf = KFold(len(dt.index), n_folds=10, shuffle=True, random_state=111)\n    outcomes = []\n    fold = 0\n    for train_index, test_index in kf:\n        fold += 1\n        X_train, X_test = dt.loc[:,'age':'thal'].values[train_index], dt.loc[:,'age':'thal'].values[test_index]\n        y_train, y_test = dt['target'].values[train_index], dt['target'].values[test_index]\n        clf.fit(X_train, y_train)\n        prediction = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, prediction)\n        outcomes.append(accuracy)\n        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))     \n    mean_outcome = np.mean(outcomes)\n    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n\n    \ndef grid_search(clf, parameters, X_train, y_train):\n    acc_scorer = make_scorer(accuracy_score)\n    grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer, n_jobs=-1, cv=5)\n    grid_obj = grid_obj.fit(X_train, y_train.values.ravel())\n    clf = grid_obj.best_estimator_\n    return(clf)\n    \n    \ndef classifier(clf):\n    \n    clf_name = clf.__class__.__name__\n    parameters = parameter_set(clf_name)\n    print(parameters)\n    # return predictions from gird search best model\n    clf = grid_search(clf, parameters, X_train, y_train)\n    \n    # fit best model\n    clf.fit(X_train, y_train.values.ravel())\n    \n    predictions = clf.predict(X_test) \n    if clf_name == 'XGBClassifier':\n        predictions = [value for value in predictions]\n    return(predictions)\n\ndef parameter_set(clf_name):\n    if clf_name == 'RandomForestClassifier':\n        parameters = {'n_estimators': [5, 10, 50, 100, 150, 200], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n#               'max_depth': list(range(2,10)), \n#               'min_samples_split': list(range(2,5)),\n#               'min_samples_leaf': list(range(1,5)),\n              'verbose': [0]\n             }\n    if clf_name == 'DecisionTreeClassifier':\n        parameters = {\n              'max_depth': list(range(2,10)), \n              'min_samples_split': list(range(2,10))\n             }\n    if clf_name == 'AdaBoostClassifier':\n        parameters = {\n            \"n_estimators\" : [5, 10, 50, 100, 150, 200],\n            \"algorithm\" :  [\"SAMME\", \"SAMME.R\"],\n            'learning_rate':[0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.3, 0.5, 0.7]\n             }\n    if clf_name == 'GradientBoostingClassifier':\n        parameters = {\n            \"loss\":[\"deviance\"],\n            \"learning_rate\": [0.075, 0.1, 0.15, 0.2, 0.3, 0.5, 0.7],\n#             \"min_samples_split\": list(range(2,5)),\n#             \"min_samples_leaf\": list(range(1,5)),\n            \"max_depth\": [3,5,8],\n            \"max_features\": [\"log2\",\"sqrt\"],\n            \"criterion\": [\"friedman_mse\",  \"mae\"],\n            \"subsample\": [0.5, 0.8, 0.9, 1.0],\n            \"n_estimators\": [5, 10, 50, 100, 150, 200]\n             }\n    if clf_name == 'XGBClassifier':\n        parameters = {\n            'learning_rate': np.linspace(0.01, 0.5, 9),\n#             'max_depth': list(range(5,10)),\n#             'min_child_weight': list(range(3,10)),\n            'gamma': np.linspace(0, 0.5, 11),\n#             'subsample': [0.8, 0.9],\n#             'colsample_bytree': [0.3, 0.4, 0.5 , 0.7, 0.8, 0.9],\n            'objective': ['binary:logistic']\n        }\n    return(parameters)","7610d095":"dt = pd.read_csv(\"..\/input\/heart.csv\")\nX_train, X_test, y_train, y_test = train_to_test_split(dt[dt.columns[0:13]] ,dt[['target']])","1cbb4041":"model = Sequential()\nmodel.add(Dense(units=512, activation='relu', input_dim=np.shape(X_train)[1]))\nmodel.add(Dense(units=512, activation='relu'))\nmodel.add(Dropout(0.3, noise_shape=None, seed=None))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dropout(0.3, noise_shape=None, seed=None))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.3, noise_shape=None, seed=None))\nmodel.add(Dense(units=16, activation='relu'))\nmodel.add(Dropout(0.3, noise_shape=None, seed=None))\nmodel.add(Dense(units=1, activation='sigmoid'))","cbdf46d4":"model.summary()","586e856f":"opt = optimizers.SGD()\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics = [\"accuracy\"])","876eb817":"result = model.fit(X_train, y_train ,epochs=100, batch_size=16, validation_split=0.1, shuffle=True)","e548ec62":"MLP_acc = accuracy_score(y_test, model.predict_classes(X_test))\nMLP_loss = log_loss(y_test, model.predict_classes(X_test))\nprint(MLP_acc)\nprint(MLP_loss)","17c02177":"plot_history(result)","681ea359":"classifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    xgb.XGBClassifier()\n]","14ae2c21":"# Logging for Visual Comparison# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Accuracy\"]\nlog = pd.DataFrame([['MLP', MLP_acc*100]],columns=log_cols)\n\nfor clf in classifiers:\n    \n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train.values.ravel())\n    print(\"=\"*30)\n    print(name)\n    \n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.2%}\".format(acc))\n    \n    log_entry = pd.DataFrame([[name, acc*100]], columns=log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\"*30)","0049eaa4":"# Grid Search\nfor clf in classifiers:\n    name = clf.__class__.__name__ + 'Grid'\n    print(\"=\"*30)\n    print(name)\n    train_predictions = classifier(clf)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.2%}\".format(acc))\n    log_entry = pd.DataFrame([[name, acc*100]], columns=log_cols)\n    log = log.append(log_entry)\nprint(\"=\"*30)\n","456ef836":"sns.set_color_codes(\"muted\")\ng=sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy')  \n\nfor p in g.patches:\n    x = p.get_x() + p.get_width() +.3\n    y = p.get_y() + p.get_height()\/2 + .1\n    g.annotate(\"%.2f %%\" % (p.get_width()), (x, y))\n\nplt.show()\n\n","0fe1fb3a":"clf = classifiers[1] \nclf.fit(X_train, y_train.values.ravel())\n# model_name = '..\/model\/'+clf.__class__.__name__+'_00001.pkl'\n# joblib.dump(clf, model_name) \n","d5c3a7ed":"print('Loading ' + clf.__class__.__name__)\n# clf = joblib.load(model_name) \n# scaler = joblib.load(scaler_filename) \n\ninput_dt = [37,1,3,130,250,1,0,187,0,2.3,0,0,1]\ninput_dt = np.array(input_dt).reshape(1, -1)\n# input_dt = scaler.transform(input_dt)\ntrain_predictions = clf.predict(input_dt)\n\n\nprint('Result is: ' + str(float(train_predictions)))","9227cbc6":"# 2019\/06\/11 Note\n- SGD is better then adam in some seed.\n\n# 2019\/07\/10 Note\n- Cross Validation With Parameter Tuning Using Grid Search.\n- Calculate the accuracy score of MLP using test dataset."}}