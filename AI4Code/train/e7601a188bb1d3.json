{"cell_type":{"0b315f2d":"code","34bdf0a7":"code","4f6a21c4":"code","b6137e6e":"code","5874ec1c":"code","fd9d69a1":"code","20448d34":"code","9c430ff4":"code","cf8db17c":"code","75a17bfa":"code","0368eb92":"code","18c1ccac":"code","5012c237":"code","1034cfd5":"code","d2e0f929":"code","522d931f":"code","f29d5467":"code","a55b92d6":"code","7ea187d8":"code","8a83b8c2":"code","2daea7ad":"code","69b7b36c":"code","a31552c2":"code","c9c37498":"code","f5f2b2b5":"code","a103d4e7":"code","0682aba7":"code","36d63f77":"code","52c55ec8":"code","91713c14":"code","47093459":"code","6c3439bb":"code","51f92627":"code","85f055f6":"code","a3a71ed2":"code","b24f076d":"markdown","7450482c":"markdown","99b728e1":"markdown","22b27d93":"markdown","d1bdab63":"markdown","4405d4d7":"markdown","db01ae04":"markdown","721cfc56":"markdown","0ee39623":"markdown","440ff46b":"markdown","23c855da":"markdown","a6904bba":"markdown"},"source":{"0b315f2d":"!pip install contractions ","34bdf0a7":"\n!pip install twikenizer\n!pip install twokenize","4f6a21c4":"import pandas as pd \npd.set_option(\"display.max_rows\", 101)\n    \nimport numpy as np \nimport contractions\nfrom bs4 import BeautifulSoup\nimport unidecode\nimport unicodedata\nimport re \nimport string\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer,word_tokenize\n\nimport twikenizer as twk\nimport twokenize\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nstopwords_english = stopwords.words('english') \nstemmer = PorterStemmer()\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")","b6137e6e":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","5874ec1c":"train.head()","fd9d69a1":"# Almost balanced test set \n\ntrain.target.value_counts()","20448d34":"train.isnull().sum()","9c430ff4":"train.drop(columns=['keyword','location','id'],axis=1,inplace=True)\ntest.drop(columns=['keyword','location','id'],axis=1,inplace=True)","cf8db17c":"train['is_test']=0\ntest['is_test']=1\n\ndf=pd.concat([train,test],ignore_index=True)","75a17bfa":"def process_tweet(tweet,lemma=True,token='twikenizer'):\n    \n    \"\"\"\n    The function can be called to pre-process the tweet to make it machine readable \n    This function does the following:\n    \n    Removes HTML tag\n    Removes @RT tags if any \n    Removes @userid if any\n    Removes hyperlinks https:\n    Removes Hashtag symbol\n    Split words such GoodNight to Good Night \n    Removes extra spaces\n    Lowercase the words \n    Expands contractions like can't to cannot\n    Removes accented characters \n    Tokenizes using one of two tokenizers ( NLTK's tweet tokenizer and twikenizer)\n    Removes punctuations \n    Removes stopwords \n    Lemmatization or Stemming done depending on preferance\n    \n    Input: Raw Tweet \n    Output : Process tweet \n    \"\"\"\n    \n    # Convert type to string \n    \n    tweet=str(tweet)\n    \n    #remove HTML tags\n    soup = BeautifulSoup(tweet)\n    tweet=soup.get_text()\n    \n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    \n    #remove the user name \n    tweet=re.sub(r'@\\S*',\"\",tweet)\n\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    \n    #Split jointed words eg GoodMorning to Good Morning\n    \n    #tweet=\" \".join(re.findall(r'[A-Z][^A-Z]*',tweet))\n    \n    #remove extra space and make lower case \n    tweet=tweet.strip().lower()\n    \n    # Remove accented characters \n    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # Expand Contractions \n    tweet=contractions.fix(tweet)\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    tweet = emoji_pattern.sub(r'', tweet) #Removing emojis\n    \n    # Tokenize the tweet \n    \n    if(token=='twikenizer'):\n        \n        token = twk.Twikenizer()\n        tweet=token.tokenize(tweet)\n        \n    elif(token=='twokenize'):\n        tweet=twokenize.tokenizeRawTweetText(tweet)\n    \n    else:\n    # tokenize tweets using NLTK\n        tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                                   reduce_len=True)\n    \n    \n        tweet = tokenizer.tokenize(tweet)\n    \n\n    \n    # remove stop words and punctuations \n    \n    tweet_clean=[]\n    for word in tweet:\n        if  word not in string.punctuation and word.isalpha() and word not in stopwords_english:\n            \n            tweet_clean.append(word)\n            \n    if(lemma==True):\n        \n        tweets=[]\n    \n        for word in tweet_clean:\n            lemma_word=lemmatizer.lemmatize(word)\n            tweets.append(lemma_word)\n    else: \n                \n        tweets=[]            \n        for word in tweet_clean:\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets.append(stem_word)\n    \n    return tweets","0368eb92":"# Before going on the process the data lets create a word cloud of the tweets to see tha major words for each catergory \n\n# Lets begin with the disaster tweets. \n\ndisaster_world_cloud=train[train.target==1]\nnormal_world_cloud=train[train.target==0]\n","18c1ccac":"disaster_world_cloud['Clean_Text']=disaster_world_cloud['text'].apply(lambda x:\" \".join(process_tweet(x)))\nnormal_world_cloud['Clean_Text']=normal_world_cloud['text'].apply(lambda x:\" \".join(process_tweet(x)))","5012c237":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt \n%matplotlib inline ","1034cfd5":"# Disaster tweet word cloud \n\nwc=WordCloud(width=5000,height=5000).generate_from_text(\" \".join(disaster_world_cloud['Clean_Text']))\nplt.figure(figsize = (10,15))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","d2e0f929":"#Non-Disaster word cloud \n\nwc=WordCloud(width=5000,height=5000).generate_from_text(\" \".join(normal_world_cloud['Clean_Text']))\nplt.figure(figsize = (10,15))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","522d931f":"df['Clean_Text']=df['text'].apply(lambda x:\" \".join(process_tweet(x)))","f29d5467":"#From the word cloud we saw that there are some single letter words in tweets like a,v we can remove them as they are like noise in the dataset \n\nwords=\" \".join(df.Clean_Text.values)\nwords=words.split()\n\nmost_freq70k=pd.Series(words).value_counts().head(70000)\n\ndf['Clean_Text']=df['Clean_Text'].apply(lambda x:\" \".join([t for t in x.split() if t in most_freq70k]))","a55b92d6":"# Splitting back to train and test\n\ntest=df[df['is_test']==1]\ntest.drop(columns=['target','is_test'],axis=1,inplace=True)\ntest.head()","7ea187d8":"train=df[df['is_test']==0]\ntrain.drop(columns=['is_test'],axis=1,inplace=True)\ntrain=train.sample(frac=1).reset_index(drop=True)\n\ntrain.head()","8a83b8c2":"from sklearn import linear_model \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nctv=CountVectorizer(tokenizer=word_tokenize,ngram_range=(3,3))\ntrain=train.sample(frac=1).reset_index(drop=True)\n\ncorpus=train['Clean_Text'].to_list()\n\n\nX=ctv.fit_transform(corpus)\ny=train.target.tolist()","2daea7ad":"#ctv.get_feature_names()\n\n#lr_scores=[]\n\n#lr_scores=model_selection.cross_val_score(lr,X.toarray(),y,cv=kfold,n_jobs=-1,scoring='accuracy')","69b7b36c":"\nlr=linear_model.LogisticRegression(random_state=101,C=0.04)\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nscores_lr=model_selection.cross_validate(lr,X.toarray(),y,cv=kfold,n_jobs=-1,scoring='accuracy',return_train_score=True)","a31552c2":"# Plot the test and train scores \n\nplt.plot(scores_lr['train_score'])\nplt.plot(scores_lr['test_score'])\nplt.ylim(0,1);\nplt.xlabel('Number of Folds');\nplt.ylabel('Accuracy');\nplt.legend();\nscores_lr['test_score']","c9c37498":"from sklearn import ensemble\n\nrandom_forest=ensemble.RandomForestClassifier(max_depth=10)\n\nscores_rf=model_selection.cross_validate(random_forest,X.toarray(),y,cv=kfold,n_jobs=-1,scoring='accuracy',return_train_score=True)\n","f5f2b2b5":"# Ploting the scores \nplt.plot(scores_rf['train_score'])\nplt.plot(scores_rf['test_score'])\nplt.ylim(0,1);\nplt.xlabel('Number of Folds');\nplt.ylabel('Accuracy');\nplt.legend();","a103d4e7":"print(\"mean accuracy on the test set using Random Forest is {0:.04f}%\".format(np.mean(scores_lr['test_score'])))","0682aba7":"from sklearn import naive_bayes\n\nnb=naive_bayes.MultinomialNB()\n\nscores_nb=model_selection.cross_validate(nb,X.toarray(),y,cv=kfold,n_jobs=-1,scoring='accuracy',return_train_score=True)\n","36d63f77":"scores_nb['train_score']","52c55ec8":"scores_nb['test_score']","91713c14":"# Lets implement the TF-IDF solution and see if we can get a improvement in the metric score using logistic regression and \n# Random forest \n\n# I found that for this dataset count vectorizer works well over TFIDF vectorizer so going to build a pipleline with \n#count vectorizer ","47093459":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfv=TfidfVectorizer(ngram_range=(3,3),tokenizer=word_tokenize)\n\nX_tfv=tfv.fit_transform(corpus)","6c3439bb":"lr_tfv=linear_model.LogisticRegression(random_state=101,C=0.04)\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\n\nscores_lr=model_selection.cross_validate(lr_tfv,X_tfv.toarray(),y,cv=kfold,n_jobs=-1,scoring='accuracy',return_train_score=True)\n\n#5 Fold test score from logistic regression using TF-IDF\nscores_lr['test_score']","51f92627":"random_forest_tfv=ensemble.RandomForestClassifier(max_depth=10)\n\nscores_rf_tfv=model_selection.cross_validate(random_forest_tfv,X_tfv.toarray(),y,cv=kfold,n_jobs=-1,scoring='accuracy',return_train_score=True)\n\n#5-Fold test score from Random forest using TF-IDF \nscores_rf_tfv['test_score']","85f055f6":"# Lets make pipeline using countvectorizer and voting classifier consisting of Logistic regression, RF and NB \n\nfrom sklearn.pipeline import Pipeline\n\nX_train=corpus[:int(X.shape[0]*0.8)]\nX_test=corpus[int(X.shape[0]*0.8):]\ny_train=y[:int(X.shape[0]*0.8)]\ny_test=y[int(X.shape[0]*0.8):]\n\nDis_pipeline=Pipeline([\n    ('ctv',CountVectorizer(tokenizer=word_tokenize,ngram_range=(3,3))),\n    ('vot',ensemble.VotingClassifier(estimators=[('lr', linear_model.LogisticRegression(random_state=101,C=0.04)), \n                                                 ('rf', ensemble.RandomForestClassifier(max_depth=10)),\n                                                  ('nb',naive_bayes.MultinomialNB())], \n                                                 voting='soft'))\n                            ])\n\nDis_pipeline.fit(X_train,y_train)","a3a71ed2":"from sklearn import metrics\nprint(\"The accuracy of disaster classification pipline is:{0:.04}%\".format(metrics.accuracy_score(y_test,Dis_pipeline.predict(X_test))*100))","b24f076d":"# Explore ","7450482c":"There where some single letter like a,e,n which we removed using better tokenization . Apart from that most words look related to disaster ","99b728e1":"# Combine the train and test data \n\n","22b27d93":"# Logistic Regression ","d1bdab63":"# Creating WordClouds ","4405d4d7":"# Read the data ","db01ae04":"# Model Exploration","721cfc56":"# Applying the process_tweet function","0ee39623":"# Cheking the same inputs with Random Forest","440ff46b":"### Lets drop the keyword and Location columns as we will try to predict the target just using the text  ","23c855da":"\n# Please Upvote if Like the notebook \n\nThis Notebooks is my attemp to begin my journey in Natural Language processing (NLP) domain . In this notebooks I am going to implement basic classification algorithm and fine tune them within hardware and time limitations . \n\n### **Point to Learn from this notebook:**<br> \n\n### **Pre-process the data to bring it to machine feedable format.**<br>\n* This step had a loop in it . So after I first pre-processed the data I go over the corpus output and see if there are any more word\/text which can be improved and thus I add more functions to process_tweet function. \n\n### **Creating you wordcloud**<br>\n* WordClouds could be one way to visualise NLP data. This allows one to see what are the words which are highlighted in each catergories.\n\n### **Going over basic Classification algorith**<br>\n* 1. This is an important step to get off the ground . Rather than fine tuninig expensive and heavy algorithms , one can just start with model which incorporate logistic regression in order to get a baseline and then slowly improve upon them via DL and transformer models \n\n### **Ensemble of models** \n* Using sklearn Voting classifier we will create the ensemble model","a6904bba":"Chossing accuracy as the metric as the dataset is pretty balanced. \n\n\nSo here in this notebooks I am going to restrict my self to Bag of words models using count vectorizer and TF-IDF. Also the hardware limitiations on Kaggle does not allow me to build large model consisting of uni-gram to tri-gram but I have played with uni ,bi and tri-grams individually and saw my accuracy rise from 53% to 61% . So I am confident that if we can combine the uni-gram to tri-gram range we might have a 1-2% increase in accuracy\n\nAlso I have not performed much model tuning due to long run time but combining model tuning along with uni-tri gram should increase the final accucary by ~2%"}}