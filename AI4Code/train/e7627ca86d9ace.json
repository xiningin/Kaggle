{"cell_type":{"44227707":"code","a37b8a05":"code","71e3180a":"code","8d88c260":"code","ef0ff328":"code","2ba66451":"code","212d17cd":"code","b210350f":"code","e3a67ec2":"code","6222228c":"code","e57066dc":"code","857561ba":"code","4f6fb45a":"code","68c59689":"code","e3ec6063":"code","42442efa":"markdown","21bb2d13":"markdown","caedae44":"markdown","db465d64":"markdown","261d8783":"markdown","75b78f71":"markdown","27fdeee5":"markdown","6a3c440b":"markdown","a657f210":"markdown"},"source":{"44227707":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport re","a37b8a05":"og_train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')\nog_train = og_train.dropna()\ntest = test.dropna()\n\nog_train = og_train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n\ntrain_og_pos = og_train[og_train['sentiment'] == 'positive'] \ntrain_og_neg = og_train[og_train['sentiment'] == 'negative']\n\ntrain_og_pos = train_og_pos.reset_index(drop=True)\ntrain_og_neg = train_og_neg.reset_index(drop=True)","71e3180a":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(og_train, test_size=0.25)\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)\n\ntrain_pos = train[train['sentiment'] == 'positive']\ntrain_neg = train[train['sentiment'] == 'negative']\n\ntrain_pos = train_pos.reset_index(drop=True)\ntrain_neg = train_neg.reset_index(drop=True)","8d88c260":"#lowercase the text\ntrain_pos_text = train_pos['text'].str.lower()\ntrain_neg_text = train_neg['text'].str.lower()\n\ntrain_pos_target_text = train_pos['selected_text'].str.lower()\ntrain_neg_target_text = train_neg['selected_text'].str.lower()\n\n\ntrain_og_pos_text = train_og_pos['text'].str.lower()\ntrain_og_neg_text = train_og_neg['text'].str.lower()\n\ntrain_og_pos_target_text = train_og_pos['selected_text'].str.lower()\ntrain_og_neg_target_text = train_og_neg['selected_text'].str.lower()","ef0ff328":"# removing special characters and numbers\ntrain_pos_text = train_pos_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntrain_neg_text = train_neg_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\n\ntrain_pos_target_text = train_pos_target_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntrain_neg_target_text = train_neg_target_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\n\n\ntrain_og_pos_text = train_og_pos_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntrain_og_neg_text = train_og_neg_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\n\ntrain_og_pos_target_text = train_og_pos_target_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntrain_og_neg_target_text = train_og_neg_target_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )","2ba66451":"# removing stopwords\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words(\"english\"))\n\ntrain_pos_text = train_pos_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\ntrain_neg_text = train_neg_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n\ntrain_pos_target_text = train_pos_target_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\ntrain_neg_target_text = train_neg_target_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n\n\ntrain_og_pos_text = train_og_pos_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\ntrain_og_neg_text = train_og_neg_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n\ntrain_og_pos_target_text = train_og_pos_target_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\ntrain_og_neg_target_text = train_og_neg_target_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))","212d17cd":"#create the matrix of featurized sentences from data using spacy \nimport spacy \nnlp = spacy.load('en_core_web_lg')\n\ndocument_pos = nlp.pipe(train_pos_text)\npos_vector = np.array([tweet.vector for tweet in document_pos])\n\ndocument_neg = nlp.pipe(train_neg_text)\nneg_vector = np.array([tweet.vector for tweet in document_neg])\n\n\ndocument_og_pos = nlp.pipe(train_og_pos_text)\nog_pos_vector = np.array([tweet.vector for tweet in document_og_pos])\n\ndocument_og_neg = nlp.pipe(train_og_neg_text)\nog_neg_vector = np.array([tweet.vector for tweet in document_og_neg])","b210350f":"#iterate through words of text and create word feature, and append sentence feature to word ft., label (Y) is whether or the not the single word is part of the selected text\ndef featurize(text, selected_text, corpus_vect):\n    labels = []\n    featurized_data = []\n    for i in range(len(text)):\n        sent_vect = corpus_vect[i]\n        target_text = selected_text[i]\n        for word in text[i].split():\n            word_vect = nlp(word).vector\n            ft_vect = np.concatenate((word_vect, sent_vect))\n            featurized_data.append(ft_vect.tolist())\n            if word in target_text:\n                labels.append(1)\n            else:\n                labels.append(0)\n    return (featurized_data, labels)\n\n(featurized_positive_X, featurized_positive_Y) = featurize(train_pos_text, train_pos_target_text, pos_vector)\n(featurized_negative_X, featurized_negative_Y) = featurize(train_neg_text, train_neg_target_text, neg_vector)\n\n\n(featurized_og_positive_X, featurized_og_positive_Y) = featurize(train_og_pos_text, train_og_pos_target_text, og_pos_vector)\n(featurized_og_negative_X, featurized_og_negative_Y) = featurize(train_og_neg_text, train_og_neg_target_text, og_neg_vector)\n","e3a67ec2":"from sklearn.linear_model import LogisticRegression\nclf_pos = LogisticRegression(random_state=0, max_iter=1000).fit(featurized_positive_X, featurized_positive_Y)\nclf_neg = LogisticRegression(random_state=0, max_iter=1000).fit(featurized_negative_X, featurized_negative_Y)\n\n#clf_og_pos = LogisticRegression(random_state=0, max_iter=1000).fit(featurized_og_positive_X, featurized_og_positive_Y)\n#clf_og_neg = LogisticRegression(random_state=0, max_iter=1000).fit(featurized_og_negative_X, featurized_og_negative_Y)","6222228c":"from sklearn.ensemble import RandomForestClassifier\nclf_pos_RF = RandomForestClassifier().fit(featurized_positive_X, featurized_positive_Y)\nclf_neg_RF = RandomForestClassifier().fit(featurized_negative_X, featurized_negative_Y)\n\nclf_og_pos_RF = RandomForestClassifier().fit(featurized_og_positive_X, featurized_og_positive_Y)\nclf_og_neg_RF = RandomForestClassifier().fit(featurized_og_negative_X, featurized_og_negative_Y)","e57066dc":"#method to build the return string, uses the first word and last word of phrase to extract that portion from the original text (og_words)\ndef buildRetString(first_word, last_word, og_words):\n    retStr = '';\n    for i in range(len(og_words)):\n        og_word = og_words[i]\n        word = og_word.lower()\n        word = re.sub(\"[^a-z\\s]\",\"\", word)\n        if word not in stopwords:\n            if word != first_word:\n                continue;\n            else:\n                temp_og_word = og_word\n                temp_word = word\n                retStr += temp_og_word\n                i += 1\n                while temp_word != last_word:\n                    temp_og_word = og_words[i]\n                    retStr += (' ' + temp_og_word)\n                    temp_word = temp_og_word.lower()\n                    temp_word = re.sub(\"[^a-z\\s]\",\"\", temp_word)\n                    i += 1\n                return retStr","857561ba":"#preprocess the validation data\nval_text = val['text'].str.lower()\nval_text = val_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\nval_text = val_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n#obtain feature vector for sentences\ndocument_val = nlp.pipe(val_text)\nval_vector = np.array([tweet.vector for tweet in document_val])\n\nselected_text_clf_probs = []\n\n#extra clfs\nselected_text_RF = []\n\n\nfor i in range(len(val_text)):\n    sent_vect = val_vector[i]\n    if val['sentiment'][i] != 'neutral' and len(val_text[i].split()) > 2:\n        temp_selected_text = []\n        probabilities_words = {}\n        \n        #extra clfs\n        probabilities_RF = {}        \n        \n        for word in val_text[i].split():\n            word_vect = nlp(word).vector\n            ft_vect = np.concatenate((word_vect, sent_vect))\n            if val['sentiment'][i] == 'positive':\n                clf = clf_pos\n                clf_RF = clf_pos_RF\n                \n            else:\n                clf = clf_neg\n                clf_RF = clf_neg_RF\n                \n            probability_class_1 = clf.predict_proba([ft_vect])[:, 1]\n            probabilities_words.update({word:(probability_class_1-0.5)})  \n            \n            #extra clfs                       \n            probability_class_1 = clf_RF.predict_proba([ft_vect])[:, 1]\n            probabilities_RF.update({word:(probability_class_1-0.5)})  \n\n        \n        words = val_text[i].split()\n        subsets = [words[m:j+1] for m in range(len(words)) for j in range(m,len(words))]\n        \n        \n        best_sum = 0;\n        best_index = -1\n        for j in range(len(subsets)):\n            current_sum = 0\n            for p in range(len(subsets[j])):\n                current_sum += probabilities_words.get(subsets[j][p])\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_index = j\n        if best_index != -1:\n            first_word = subsets[best_index][0]\n            last_word = subsets[best_index][len(subsets[best_index])-1]\n            og_words = val['text'][i].split()\n            retStr = buildRetString(first_word, last_word, og_words)  \n            #print(retStr)\n            selected_text_clf_probs.append(retStr)\n        else:\n            selected_text_clf_probs.append(val['text'][i])\n            \n        #extra clfs                 \n        best_sum = 0;\n        best_index = -1\n        for j in range(len(subsets)):\n            current_sum = 0\n            for p in range(len(subsets[j])):\n                current_sum += probabilities_RF.get(subsets[j][p])\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_index = j\n        if best_index != -1:\n            first_word = subsets[best_index][0]\n            last_word = subsets[best_index][len(subsets[best_index])-1]\n            og_words = val['text'][i].split()\n            retStr = buildRetString(first_word, last_word, og_words)  \n            #print(retStr)\n            selected_text_RF.append(retStr)\n        else:\n            selected_text_RF.append(val['text'][i])           \n\n        \n    else:\n        #neutral case\n        selected_text_clf_probs.append(val['text'][i])\n        \n        #extra clfs\n        selected_text_RF.append(val['text'][i])\n        ","4f6fb45a":"#jacard score\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\nground_truth = val['selected_text']\nprediction = pd.Series(selected_text_clf_probs)\njac_sum = 0\nfor i in range(len(prediction)):\n    jac_sum += jaccard(prediction[i], ground_truth[i])\n\nprint('score(val) LR: ', (1\/len(prediction)) * jac_sum)\n\n#jacard score\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\nground_truth = val['selected_text']\nprediction = pd.Series(selected_text_RF)\njac_sum = 0\nfor i in range(len(prediction)):\n    jac_sum += jaccard(prediction[i], ground_truth[i])\n\nprint('score(val) RF: ', (1\/len(prediction)) * jac_sum)","68c59689":"#preprocess the test data\ntest_text = test['text'].str.lower()\ntest_text = test_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntest_text = test_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n#obtain feature vector for sentences\ndocument_test = nlp.pipe(test_text)\ntest_vector = np.array([tweet.vector for tweet in document_test])\n\nselected_text_clf_probs_og = []\nfor i in range(len(test_text)):\n    sent_vect = test_vector[i]\n    if test['sentiment'][i] != 'neutral' and len(test_text[i].split()) > 2:\n        temp_selected_text = []\n        probabilities_words = {}\n        for word in test_text[i].split():\n            word_vect = nlp(word).vector\n            ft_vect = np.concatenate((word_vect, sent_vect))\n            if test['sentiment'][i] == 'positive':\n                clf = clf_og_pos_RF\n            else:\n                clf = clf_og_neg_RF\n            probability_class_1 = clf.predict_proba([ft_vect])[:, 1]\n            probabilities_words.update({word:(probability_class_1-0.5)})        \n        \n        words = test_text[i].split()\n        subsets = [words[m:j+1] for m in range(len(words)) for j in range(m,len(words))]\n        best_sum = 0;\n        best_index = -1\n        for j in range(len(subsets)):\n            current_sum = 0\n            for p in range(len(subsets[j])):\n                current_sum += probabilities_words.get(subsets[j][p])\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_index = j\n        if best_index != -1:\n            first_word = subsets[best_index][0]\n            last_word = subsets[best_index][len(subsets[best_index])-1]\n            og_words = test['text'][i].split()\n            retStr = buildRetString(first_word, last_word, og_words)  \n            selected_text_clf_probs_og.append(retStr)\n        else:\n            selected_text_clf_probs_og.append(test['text'][i])\n        \n    else:\n        #neutral case\n        selected_text_clf_probs_og.append(test['text'][i])\n    ","e3ec6063":"temp_series = pd.Series(selected_text_clf_probs_og)\nsubmission['selected_text'] = temp_series\nsubmission.to_csv('submission.csv', index=False)","42442efa":"Split train into train\/val split new train on sentiment label","21bb2d13":"Make predictions on selected text over test data, using the classifier trained over all training data","caedae44":"Compute Jacard score and extracting text using both LR and RF implementations","db465d64":"Fit model with newly formed data (LR being compared with RF in validation set only, final submission made with RF)","261d8783":"Use spacy to featurize the data in the form [(word from sentence),(entire sentence)], label is presence\/absence of the word in the selected text","75b78f71":"Extract text for validation data and evaluate extraction using Jacard score for example 'selected_text' field\n* Running both LR and RF on validation data for comparison of classifier","27fdeee5":"Preprocessing data","6a3c440b":"Read in the data, and obtain original positive\/negative sentiment labeled data for submission model","a657f210":"Print the submission"}}