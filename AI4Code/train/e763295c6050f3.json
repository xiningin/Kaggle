{"cell_type":{"c511b738":"code","9f02dde9":"code","9358a479":"code","f01eb52d":"code","954b92dd":"code","40740230":"code","6af7cc49":"code","afe84781":"code","7fcc0c21":"code","ee5e08ef":"code","f6c9e76a":"code","0923c2e9":"code","4f3f64b8":"code","d46e9ac8":"code","811e35f3":"code","cd1aef2f":"code","46baf071":"code","a542a953":"code","0b897f01":"code","36c679cc":"code","cdd27298":"code","faf4e67c":"code","d9628eff":"code","02eddb97":"code","3b32de0b":"code","4594af7a":"code","7be92c69":"code","d8f3f2db":"code","134ffefe":"code","ddbec478":"code","4ffdd55e":"code","88d9a9cb":"code","fdca1790":"code","b09876b2":"code","709e7e28":"code","5f1747e2":"code","13e6b096":"code","37ec35ef":"code","8c966c8a":"code","efe42a77":"code","6942a501":"code","d1a35132":"code","c89ef9f0":"code","b35d20e8":"code","7d2b66a3":"code","1ed425ea":"code","4bb1b9e3":"code","1c5a3d36":"code","8f297fe3":"code","fdc93173":"code","8ec33696":"code","2d3952eb":"code","3b9e26b8":"code","eea49a42":"code","079d9c73":"code","f8242b29":"code","7ea42103":"code","b9938963":"code","aa2a701d":"code","2b42d665":"code","ed6a7c9e":"code","5c64091e":"code","81a85092":"code","a2ff2a72":"code","89d5d0fb":"code","6cd3872a":"code","408bfba1":"code","73804fc3":"code","382b08cc":"code","9eb125c5":"code","4b28c9a7":"code","5d4ece12":"code","765520a5":"code","e792ad0a":"code","b5dcdf2b":"code","ff7e3001":"code","398da316":"code","b79e50fb":"code","d8dd7b22":"code","06545164":"code","e570587b":"markdown","b5b55e9f":"markdown","812a3dc3":"markdown","020a1c86":"markdown","c699edb8":"markdown","6a81c833":"markdown","659d4b8c":"markdown","c48dee47":"markdown","a6cf231b":"markdown","d2bfdc2a":"markdown","345e1c7e":"markdown","cd654134":"markdown","bc1c7790":"markdown","587c1334":"markdown","c309e6ea":"markdown","c2e5fa14":"markdown","90b9b894":"markdown","9e39632b":"markdown","68fcaf8a":"markdown","ca1022b4":"markdown","716b55a9":"markdown","d0d3c9b6":"markdown","126a6d47":"markdown","50e167b9":"markdown","8550859b":"markdown","fbdd2331":"markdown","1de03adf":"markdown","71df1abd":"markdown","efd1d098":"markdown","70285f4c":"markdown","f9aad6de":"markdown","9eb641c5":"markdown","79ef071f":"markdown","2393bc1c":"markdown","c5ce0a62":"markdown","8ea8bfd0":"markdown","410b6ba8":"markdown","d60b5105":"markdown","520c61fe":"markdown","71c8b381":"markdown","4584bfff":"markdown","2c277dd6":"markdown","b7108f72":"markdown","3843b900":"markdown","ed3a21b5":"markdown","a7b803e9":"markdown","32a6a293":"markdown","6a30dcfe":"markdown","b7548ba6":"markdown","28287dc0":"markdown","ef3d43d5":"markdown","ed19b48d":"markdown","9aef711b":"markdown","95de66ee":"markdown","e26aff44":"markdown","8d83aac6":"markdown","c62a8428":"markdown","e3913bf0":"markdown","ed0a5a25":"markdown","956d11fd":"markdown","3a1db2e0":"markdown","334e1673":"markdown","7e2c06ea":"markdown","923d7d51":"markdown","e10ebc1c":"markdown","33c28537":"markdown","35a91bbe":"markdown","e07e1554":"markdown","774f490f":"markdown","8d9dfa5d":"markdown","6efa376a":"markdown","a5b9ea54":"markdown","fd2c97f0":"markdown","f72215e9":"markdown","80947056":"markdown","f61f9eca":"markdown","6ea9d3c5":"markdown","b3eb0423":"markdown","b8083a18":"markdown","62af2060":"markdown","f8fe0cbd":"markdown","4153f295":"markdown","7cea948c":"markdown","0508aaf5":"markdown","bc68ebf0":"markdown","a33f3968":"markdown","d3ff34ee":"markdown","e79a894f":"markdown","c429b513":"markdown","7dfc98cd":"markdown","745e0ebe":"markdown","fff33543":"markdown","81c9ea9f":"markdown","c3182f72":"markdown","fb093346":"markdown","5859305b":"markdown","029420c2":"markdown","f2c9fcb7":"markdown","86caf0d2":"markdown","b1bfe384":"markdown","753ca2b5":"markdown","16451693":"markdown"},"source":{"c511b738":"import pandas as pd\nimport numpy as np\nimport os \nimport time\n\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime","9f02dde9":"data_folder = \"..\/input\/g-research-crypto-forecasting\/\"\n!ls $data_folder","9358a479":"start = time.time()\n\ncrypto_df = pd.read_csv(os.path.join(data_folder, 'train.csv'))\n\nend = time.time()\nprint(end - start) # just ouf curiosity, see how long it takes to read such a large csv file","f01eb52d":"crypto_df.shape","954b92dd":"crypto_df.head(2)","40740230":"start = crypto_df.iloc[0].timestamp.astype('datetime64[s]')\nend = crypto_df.iloc[-1].timestamp.astype('datetime64[s]')\n\nprint(f'Data from {start} until {end}')","6af7cc49":"asset_details_df = pd.read_csv(os.path.join(data_folder, 'asset_details.csv'))","afe84781":"asset_details_df","7fcc0c21":"btc_mini_df = crypto_df[crypto_df.Asset_ID == 1].iloc[-60:]","ee5e08ef":"btc_mini_df.head(4)","f6c9e76a":"btc_mini_df = btc_mini_df.set_index(\"timestamp\")","0923c2e9":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Candlestick(x=btc_mini_df.index, \n                                     open=btc_mini_df['Open'], \n                                     high=btc_mini_df['High'], \n                                     low=btc_mini_df['Low'], \n                                     close=btc_mini_df['Close'])])\nfig.show()","4f3f64b8":"btc_df = crypto_df[crypto_df.Asset_ID == 1].set_index('timestamp')\n\nbtc_df.info(show_counts =True)","d46e9ac8":"btc_df.isna().sum()","811e35f3":"# we start with 1 instead of 0 since there's nothing to compare the first entry to\n(btc_df.index[1:]-btc_df.index[:-1]).value_counts().head()","cd1aef2f":"btc_df = btc_df.reindex(range(btc_df.index[0],btc_df.index[-1]+60,60), method='pad')","46baf071":"(btc_df.index[1:]-btc_df.index[:-1]).value_counts().head()","a542a953":"btc_df['datetime'] = btc_df.apply(lambda r: np.float64(r.name).astype('datetime64[s]'), axis=1)\n\nbtc_df.set_index('datetime', inplace=True);\n\nbtc_df.head(3)","0b897f01":"btc_df.head(3)","36c679cc":"btc_mini_df = btc_df[-7201:].copy(deep=True)  # the 7201 comes from me cheating and looking ahead into the results, trying to get full days of data","cdd27298":"btc_mini_df.head(3)","faf4e67c":"btc_mini_df['time'] = btc_mini_df.apply(lambda r:r.name, axis=1) # I need to move timestamp back into a column\n\n# and parse it into year, month, day and hour\nbtc_mini_df['year'] = [btc_mini_df.iloc[i].time.year for i in range(len(btc_mini_df))]  \nbtc_mini_df['month'] = [btc_mini_df.iloc[i].time.month for i in range(len(btc_mini_df))]\nbtc_mini_df['day'] = [btc_mini_df.iloc[i].time.day for i in range(len(btc_mini_df))]\nbtc_mini_df['hour'] = [btc_mini_df.iloc[i].time.hour for i in range(len(btc_mini_df))]","d9628eff":"btc_mini_df.head(3)","02eddb97":"# average data per hour \ntmp = btc_mini_df.groupby(['year', 'month', 'day', 'hour']).mean()\n\n# restore the multilevel index created by groupby into the year, month, day, hour columns that we created earlier\ntmp.reset_index(inplace=True)","3b32de0b":"cols = ['year', 'month', 'day', 'hour', 'Close']\n\ntmp[cols].head(5)","4594af7a":"tmp = btc_mini_df['Close'] # extract only the Close price\n\nlag_df = pd.concat([tmp.shift(1, axis = 0), tmp], axis=1) # downward shift by 1 step \n\n# the original price series becomes the time t value, \n# while the downward shifted series is time t+1\nlag_df.columns = ['Close(t)', 'Close(t+1)'] \n\nlag_df.head()","7be92c69":"lag_df = pd.concat([tmp.shift(3), tmp.shift(2), tmp.shift(1), tmp], axis=1)\n\nlag_df.columns = ['Close(t-2)', 'Close(t-1)', 'Close(t)', 'Close(t+1)'] # rename columns for easier read\n\nlag_df.head()","d8f3f2db":"tmp = btc_mini_df['Close'] # extract only the Close price\n\nlag_df = tmp.shift(1) # downward shift by 1\n\nwindow = lag_df.rolling(window=2) # rolling window size of 2\nmeans = window.mean() # compute the means for the rolling windows\n\nnew_df = pd.concat([means, tmp], axis=1) # concatenate the two series vertically\n\nnew_df.columns = ['mean(t-1,t)', 't+1'] # rename columns for easier reading\n\nnew_df.head()","134ffefe":"window = tmp.expanding()\n\ndataframe = pd.concat([window.min(), window.mean(), window.max(), tmp.shift(-1)], axis=1)\n\ndataframe.columns = ['min', 'mean', 'max', 't+1']\n\nprint(dataframe.head(5))","ddbec478":"# have a look at the initial dataset again, to verify that tha dataframe above is correct\ntmp.head(3)","4ffdd55e":"# helper function to compute the log returns\ndef log_returns(series, periods = 1):\n    return np.log(series).diff(periods = periods)","88d9a9cb":"f = plt.figure(figsize = (15,4))\n\nlret_btc = log_returns(btc_mini_df.Close,1)[1:]\n\nplt.plot(lret_btc)\n\nplt.show()","fdca1790":"btc_df.head(3)","b09876b2":"#btc_df.plot(x='datetime', y='Close', figsize=(8,5))\nbtc_df.Close.plot(figsize=(20,5))\nplt.title('Evolution of BTC price')\nplt.show()","709e7e28":"print(btc_mini_df.iloc[0].time)\nprint(btc_mini_df.iloc[-2].time)\nprint(btc_mini_df.iloc[-1].time)","5f1747e2":"groups = btc_mini_df.groupby('day')\n\ndays = pd.DataFrame()\n\nfor name, group in groups:\n    if name == 21: # skip the last day, which seems to be incomplete \n        continue\n    days[name] = group.Close.values\n\ndays.plot(subplots=True, legend=False, figsize=(10,8), title='BTC price evolution throughout the day\\n2021-09-16 to 2021-09-20');","13e6b096":"btc_df.Close.hist(figsize=(8,5))\nplt.title('Histogram of closing prices for BTC')\nplt.show()","37ec35ef":"btc_df.Close.plot(kind='kde')\nplt.title('Density plot of closing prices for BTC')\nplt.show()","8c966c8a":"print(btc_df.iloc[0].name)\nprint(btc_df.iloc[-2].name)\nprint(btc_df.iloc[-1].name)","efe42a77":"btc_df.groupby(pd.Grouper(freq='A')).size()","6942a501":"groups = btc_df.groupby(pd.Grouper(freq='A')) # group by year\n\nyears = pd.DataFrame([])\n\nfor name, group in groups: # iterate through the years\n    tmp = group.groupby(pd.Grouper(freq='D')).Close.mean() # compute the daily mean\n    tmp.index = tmp.index.strftime('%m-%d') # transform the index into 'mm-dd' only\n    \n    years = years.join(tmp, rsuffix=name.year, how = \"outer\") # join together yearly series (on the 'mm-dd' index) \n    \nyears.boxplot(figsize=(8,6))\n\nplt.title('Box and whiskers plots for BTC close prices\\n years 2018 to 2020');","d1a35132":"yrs = ['Close', 'Close2019', 'Close2020']\n\nplt.matshow(years[yrs].dropna().T, interpolation=None, aspect='auto')\n\nplt.title('BTC closing daily price\\nyears 2018->2020')\nplt.ylabel('years')\nplt.xlabel('days')\nplt.show()","c89ef9f0":"norm_years=(years-years.min())\/(years.max()-years.min())\n\nplt.matshow(norm_years[yrs].dropna().T, interpolation=None, aspect='auto')\n\nplt.title('BTC closing daily price\\nyears 2018->2020\\normalized per year')\nplt.ylabel('years')\nplt.xlabel('days')\n\nplt.show()","b35d20e8":"groups = btc_df.groupby(pd.Grouper(freq='D'))\n\ndays = pd.DataFrame([])\n\nfor i, (name, group) in enumerate(groups):\n    tmp = group.Close\n    tmp.index = tmp.index.strftime('%H:%M')\n    \n    days = days.join(tmp, rsuffix=name.year, how = \"outer\")\n    \n    if i == 9:\n        print('break')\n        break\n    \nplt.matshow(days.T, interpolation=None, aspect='auto')\nplt.show()","7d2b66a3":"from pandas.plotting import lag_plot\n\nlag_plot(btc_df.groupby(pd.Grouper(freq='D')).Close.mean())\n\nplt.show()","1ed425ea":"daily_df = btc_df.groupby(pd.Grouper(freq='D')).Close.mean()\ndaily_values = pd.DataFrame(daily_df.values)\n\nlags = 8\ncolumns = [daily_values]\n\nfor i in range(1,(lags + 1)):\n    columns.append(daily_values.shift(i)) # downward shift by i positions\n\ndataframe = pd.concat(columns, axis=1)\n\ncol_names = ['t'] + ['t-'+str(l) for l in range(1,(lags + 1))]\n\ndataframe.columns = col_names\n\nplt.figure(figsize=(16,6))\n\nfor i in range(1,(lags + 1)):\n    ax = plt.subplot(240 + i)\n    ax.set_title('t vs t-' + str(i))\n    plt.scatter(x=dataframe['t'].values, y=dataframe['t-'+str(i)].values)\n\nplt.show()","4bb1b9e3":"from pandas.plotting import autocorrelation_plot\n\nbtc_days_df = btc_df.groupby(pd.Grouper(freq='D')).Close.mean()\n\nautocorrelation_plot(btc_days_df)\n\nplt.title('Autocorrelation plot\\ndaily resolution')\nplt.show()","1c5a3d36":"#btc_df.plot(x='datetime', y='Close', figsize=(8,5))\n\nmonth_sz = 60 * 24 * 30\nyear_sz = 12 * month_sz\n\nbtc_df.Close.plot(figsize=(20,5))\n\n# tail-rolling average transform\nrolling_m = btc_df.Close.rolling(window=month_sz)\nrolling_m_mean = rolling_m.mean()\nrolling_m_mean.plot(color='green')\n\nrolling_y = btc_df.Close.rolling(window=year_sz)\nrolling_y_mean = rolling_y.mean()\nrolling_y_mean.plot(color='red')\n\nplt.legend(['BTC price\/min', '30-day rolling avg', '1-year rolling avg'])\nplt.title('Evolution of BTC price')\nplt.show()","8f297fe3":"quad_data = [i**2 for i in range(1,200)]\nplt.figure(figsize=(15,6))\n\n## quadratic data plots\n# line plot\nplt.subplot(221)\nplt.plot(quad_data)\nplt.title('Made up data (qudratic)')\n\n# histogram\nplt.subplot(223)\nplt.hist(quad_data)\n\n## square root data plots\n# linear plots\nsq_data = np.sqrt(quad_data)\nplt.subplot(222)\nplt.plot(sq_data)\nplt.title('Data after transformation (squared root)')\n\n# histogram\nplt.subplot(224)\nplt.hist(sq_data)\n\nplt.show()","fdc93173":"data = btc_df.Close\nplt.figure(figsize=(15,6))\n\n## quadratic data plots\n# line plot\nplt.subplot(221)\nplt.plot(data)\nplt.title('Real data (BTC closing price)')\n\n# histogram\nplt.subplot(223)\nplt.hist(data)\n\n## square root data plots\n# linear plots\nsq_data = np.sqrt(data)\nplt.subplot(222)\nplt.plot(sq_data)\nplt.title('Data after transformation (squared root)')\n\n# histogram\nplt.subplot(224)\nplt.hist(sq_data)\n\nplt.show()","8ec33696":"data = btc_df.Close\nplt.figure(figsize=(15,6))\n\n## initial data plots\n# line plot\nplt.subplot(221)\nplt.plot(data)\nplt.title('Real data (BTC closing price)')\n\n# histogram\nplt.subplot(223)\nplt.hist(data)\n\n## square root data plots\n# linear plots\nsq_data = np.log(data)\nplt.subplot(222)\nplt.plot(sq_data)\nplt.title('Data after transformation (log transform)')\n\n# histogram\nplt.subplot(224)\nplt.hist(sq_data)\n\nplt.show()","2d3952eb":"# automatically box-cox transform a time series\nfrom scipy.stats import boxcox\n\nplt.figure(figsize=(15,6))\n\n## initial data plots\n# line plot\nplt.subplot(221)\nplt.plot(data)\nplt.title('Real data (BTC closing price)')\n\n# histogram\nplt.subplot(223)\nplt.hist(data)\n\n## transformed data plots\ntemp_df = pd.DataFrame(btc_df.Close)\ntemp_df.columns = ['Close']\n\ntemp_df['Close'], lambda_val = boxcox(temp_df['Close'])\nprint(f'best lambda: {lambda_val}')\n\n# line plot\nplt.subplot(222)\nplt.plot(temp_df['Close'])\n\n# histogram\nplt.subplot(224)\nplt.hist(temp_df['Close'])\nplt.show()","3b9e26b8":"lret_btc = log_returns(btc_df.Close,1)[1:]","eea49a42":"f = plt.figure(figsize = (15,4))\n\nplt.plot(lret_btc)\nplt.plot(lret_btc.index, [lret_btc.mean()] * len(lret_btc), color='red')\nplt.legend(['BTC log returns', 'mean'])\n\nplt.show()","079d9c73":"print(lret_btc.describe())","f8242b29":"groups = lret_btc.groupby(pd.Grouper(freq='A')) # group by year\n\nyears = pd.DataFrame([])\n\nfor name, group in groups: # iterate through the years\n    tmp = group.groupby(pd.Grouper(freq='D')).mean() # compute the daily mean\n    tmp.index = tmp.index.strftime('%m-%d') # transform the index into 'mm-dd' only\n    \n    years = years.join(tmp, rsuffix=name.year, how = \"outer\") # join together yearly series (on the 'mm-dd' index) \n    \nyears.boxplot(figsize=(8,6))\nplt.ylim([-0.0002, 0.0002])\nplt.title('Box and whiskers plots for log returns of BTC close prices\\n years 2018 to 2021');","7ea42103":"lret_btc = log_returns(btc_df,1)[1:]\n\ntemp_df = lret_btc.groupby(pd.Grouper(freq='D')).Close.mean()\n\nautocorrelation_plot(temp_df)\n\nplt.title('Autocorrelation plot\\nlog returns\\ndaily resolution')\nplt.ylim([-0.25, 0.25])\nplt.show()","b9938963":"from random import seed\nfrom random import random\n\nseed(101)\n\nvalues = [-1 if random() < 0.5 else 1] # x(0)\n\nfor i in range(1, 1000):\n    rnd_step = -1 if random() < 0.5 else 1\n    y_t = values[i-1] + rnd_step\n    values.append(y_t)\n\nplt.figure(figsize=(8,7))\n\n# linear plot\nplt.subplot(211)    \nplt.plot(values)\nplt.title('Line plot of a generated random walk time series')\n\n# correlogram\nplt.subplot(212)    \nautocorrelation_plot(values)\nplt.title('Autocorrelation plot for a generated random walk time series')\n\nplt.show()","aa2a701d":"from pandas.plotting import autocorrelation_plot\n\nbtc_days_df = btc_df.groupby(pd.Grouper(freq='D')).Close.mean()\n\nplt.figure(figsize=(8,4))\nautocorrelation_plot(btc_days_df)\n\nplt.title('Autocorrelation plot BTC price\\ndaily resolution')\nplt.show()","2b42d665":"# calculate the stationarity of our closing price data\nfrom statsmodels.tsa.stattools import adfuller\n\n# statistical test\nresult = adfuller(btc_days_df)\n\nprint(f'ADF result: {result[0]} p={result[1]:.3f}') \n\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print(f'\\t{key}: {value:.3f}')","ed6a7c9e":"diff_df = btc_days_df.diff()[1:]\n\nplt.figure(figsize=(8,7))\n\n# linear plot\nplt.subplot(211)    \nplt.plot(diff_df)\nplt.title('Line plot of the diff time series')\n\n# correlogram\nplt.subplot(212)    \nautocorrelation_plot(diff_df)\nplt.title('Autocorrelation plot for the diff time series')\nplt.ylim([-0.25, 0.25])\nplt.show()","5c64091e":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# prepare dataset\ntrain_size = int(len(btc_days_df) * 0.5)\ntrain, test = btc_days_df[0:train_size], btc_days_df[train_size:]\n\n# persistence\npreds = []\nprev = train[-1]\n\nfor i in range(len(test)):\n    preds.append(prev)\n    prev = test[i]\n    \nrmse = sqrt(mean_squared_error(test, preds))\nprint(f'Persistence model RMSE: {rmse:.3f}')","81a85092":"from statsmodels.tsa.seasonal import seasonal_decompose\n\n##whole data\ndata = btc_days_df\ndecomp = seasonal_decompose(data, model='additive')\n\nplt.figure(figsize=(20,12))\n\nplt.subplot(421)\ndata.plot()\nplt.ylabel('initial data')\nplt.title('Additive decomposition\\nWhole dataset (3.5 years)')\n\nplt.subplot(423)\ndecomp.trend.plot()\nplt.ylabel('trend')\n\nplt.subplot(425)\ndecomp.seasonal.plot()\nplt.ylabel('seasonal')\n\nplt.subplot(427)\ndecomp.resid.plot()\nplt.ylabel('residual')\n\n##small window\ndata = btc_days_df\ndecomp = seasonal_decompose(data, model='multiplicative')\n\nplt.subplot(422)\ndata.plot()\nplt.ylabel('initial data')\nplt.title('Multiplicative decomposition\\nWhole dataset (3.5 years)')\n\nplt.subplot(424)\ndecomp.trend.plot()\nplt.ylabel('trend')\n\nplt.subplot(426)\ndecomp.seasonal.plot()\nplt.ylabel('seasonal')\n\nplt.subplot(428)\ndecomp.resid.plot()\nplt.ylabel('residual')\n\nplt.show()","a2ff2a72":"from statsmodels.tsa.seasonal import seasonal_decompose\n\n##whole data\ndata = btc_days_df[:160]\ndecomp = seasonal_decompose(data, model='additive')\n\nplt.figure(figsize=(20,12))\n\nplt.subplot(421)\ndata.plot()\nplt.ylabel('initial data')\nplt.title('Additive decomposition\\nSmaller time window (0.5 years)')\n\nplt.subplot(423)\ndecomp.trend.plot()\nplt.ylabel('trend')\n\nplt.subplot(425)\ndecomp.seasonal.plot()\nplt.ylabel('seasonal')\n\nplt.subplot(427)\ndecomp.resid.plot()\nplt.ylabel('resdual')\n\n##small window\ndata = btc_days_df[:160]\ndecomp = seasonal_decompose(data, model='multiplicative')\n\nplt.subplot(422)\ndata.plot()\nplt.ylabel('initial data')\nplt.title('Multiplicative decomposition\\nSmaller time window (0.5 years)')\n\nplt.subplot(424)\ndecomp.trend.plot()\nplt.ylabel('trend')\n\nplt.subplot(426)\ndecomp.seasonal.plot()\nplt.ylabel('seasonal')\n\nplt.subplot(428)\ndecomp.resid.plot()\nplt.ylabel('resdual')\n\nplt.show()","89d5d0fb":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.optimize import curve_fit\n\ndata = btc_days_df\n\n# create our independent variable; our X values are the timesteps, \n# but we can make it just as well an array from 0 to ..., since the \n# actual value does not matter\nX = [i for i in range(0, len(data))]\nX = np.reshape(X, (len(X), -1))\n\ny = data.values # dependent variable is the closing price\n\npf = PolynomialFeatures(degree=3) # a cubic polynomial model\nXp = pf.fit_transform(X)          # transform X into a quadratic form (each row i will be: x[i]^0 x[i]^1 x[i]^2 x[i]^3) \n\n# fit the quadratic model through ordinary least squares Linear Regression\nmd2 = LinearRegression() \nmd2.fit(Xp, y)\n\ntrendp = md2.predict(Xp)\n\nplt.plot(X, y)\nplt.plot(X, trendp)\nplt.legend(['data', 'polynomial trend'])\nplt.show()","6cd3872a":"plt.figure(figsize=(8,7))\n\ndetrpoly = [y[i] - trendp[i] for i in range(0, len(y))]\n\nplt.subplot(211)    \nplt.plot(X, detrpoly)\nplt.title('polynomially detrended data')\n\n# correlogram\nplt.subplot(212)    \nautocorrelation_plot(detrpoly)\nplt.title('Autocorrelation plot for a generated random walk time series')\n\nplt.show()","408bfba1":"from sklearn.metrics import mean_squared_error, r2_score\n\nr2 = r2_score(y, trendp)\nrmse = np.sqrt(mean_squared_error(y, trendp))\nprint('r2:', r2)\nprint('rmse', rmse)","73804fc3":"tmp = btc_days_df\n\nlag_df = pd.concat([tmp.shift(7, axis = 0), tmp], axis=1)[7:] # remove the first 7 non differenced rows\n\nlag_df.columns = ['t', 't+7']\n\ndiff_data = lag_df['t+7'] - lag_df['t']\n\ndiff_data.plot()\nplt.title('Line plot of deseasonalized daily close price using differencing')\n\nplt.show()","382b08cc":"groups = btc_df.groupby(pd.Grouper(freq='A'))\n\ncount = len(groups)\n\nplt.figure(figsize=(12,8))\n\nfor i, (name, group) in enumerate(groups):\n    plt.subplot(count*100 + 10 + i+1)\n    plt.plot(group.index.values, group.Close.values)\nplt.show","9eb125c5":"groups = btc_days_df.groupby(pd.Grouper(freq='A'))\ndays_per_year = []\nfor name, group in groups:\n    days_per_year.append(len(group.values))\n\nplt.figure(figsize=(12,20))\n\nstart = 0\nyears = [2018, 2019, 2020, 2021]\ncount = len(days_per_year)\n\nlegend = ['Detrended series', 'Fitted weekly seasonality', 'Fitted monthly seasonality', 'Fitted trimester seasonality']\n\nfor i, days in enumerate(days_per_year):\n    series = detrpoly[start:start+days]\n    \n    plt.subplot(count*100 + 10 + i+1)\n    plt.title(f'Year {years[i]}')\n    \n    plt.plot(series)\n    \n    intervals = [7, 30, 90]\n    cols = ['red', 'green', 'magenta']\n\n    for cnt, inter in enumerate(intervals):\n        X = [i%inter for i in range(0, len(series))] # let's try to model a weekly seasonality as a sinusoid\n        y = series\n\n        degree = 4\n\n        coef = np.polyfit(X, y, degree)\n\n        # create curve\n        curve = list()\n        for i in range(len(X)):\n            value = coef[-1]\n            for d in range(degree):\n                value += X[i]**(degree-d) * coef[d]\n            curve.append(value)\n        \n        # plot curve over original data\n        plt.plot(curve, color=cols[cnt], linewidth=1)\n        plt.legend(legend)\n    start = start+days\nplt.show","4b28c9a7":"X = btc_days_df.values\n\ntrain_size = int(len(X) * 0.7)\ntrain, test = X[0:train_size], X[train_size:len(X)]\n\nplt.plot(train)\nplt.plot([None for i in train] + [x for x in test])\n\nplt.title('70-30% train-test splits')\nplt.show()","5d4ece12":"data = btc_days_df\n\ndf = pd.concat([data.shift(1), data], axis=1)\ndf.columns = ['t', 't+1']\n\nprint(dataframe.head(5))","765520a5":"X = df.values\n\ntrain_size = int(len(X) * 0.7)\n\ntrain, test = X[1:train_size], X[train_size:]\n\nX_train, y_train = train[:,0], train[:,1]\nX_test, y_test = test[:,0], test[:,1]","e792ad0a":"def pers_predict(x):\n    return x","b5dcdf2b":"preds = []\n\nfor x in X_test:\n    yhat = pers_predict(x)\n    preds.append(yhat)\n\nrmse = sqrt(mean_squared_error(y_test, preds))\n\nprint(f'RMSE: {rmse:.3f}')","ff7e3001":"# calculate residuals\nresiduals = [y_test[i]-preds[i] for i in range(len(preds))]\nresiduals = pd.DataFrame(residuals)\n\nresiduals.plot()\nplt.title('Residuals plot for the persistence model')\nplt.show()","398da316":"# summary statistics\nprint(residuals.describe())","b79e50fb":"import seaborn as sns\n\nsns.histplot(data = residuals, kde=True)\n\nplt.show()","d8dd7b22":"from statsmodels.graphics.gofplots import qqplot\n\nres_arr = np.array(residuals)\nqqplot(res_arr, line='r')\nplt.show()","06545164":"autocorrelation_plot(residuals)\nplt.show()","e570587b":"In section [section 6.6](#6.6-Autocorrelation-plots) I plotted the autocorrelation for BTC close price. Let's see it again.","b5b55e9f":"### 5.2 Lag features  \n\nThis is the typical way in which time series data is tranformed into a supervised learning problem.  \n\nThe idea is to predict the value at the current timestamp based on the value from the previous timestamp(s).   \n\n**Nomenclature:**  \n- because we are predicting a single variable, this is called **univariate**  \n- the number of previous time steps we use in our prediction is called the **width of the time window** or **the lag**\n- if we predict only one future time step we are doing **one-step forecast**. We can also perform **multi-step forecast** and predict multiple next steps at once \n- this method is also called **sliding window**, with a window width of 1 in our case below.\n\nInitial dataset:  \n\n| time | value |  \n| --- | ---|   \n| t1 | v1  \n| t2 | v2  \n| t3 | v3\n\nbecomes:    \n\n| x | y | \n| --- | --- |     \n| ? | v1      \n| v1 | v2  \n| v2 | v3  \n\nThe actual value for timestamp is gone from our data. We don't care about it, as long as we preserve the order. We only care about the actual value at the previous point in time.\n\nWe use pandas' dataframe.shift() function, which shifts values vertically or horizontally, fills in with NaN values and leaves the index as it is.","812a3dc3":"The naive persistence prediction model: receives the current value as input and predicts the same as output.","020a1c86":"# 1. Quick overview\n\nThis dataset was made available on Kaggle as part of the <a href='https:\/\/www.kaggle.com\/c\/g-research-crypto-forecasting\/overview'>G-Research Crypto forecasting competition<\/a>. The challenge proposed by G-Research was to predict price returns across a bundle of major cryptocurrencies for which we have approximately 3 years of historical price data.  ","c699edb8":"We see that we have 78 instances where two consecutive entries are 120 seconds apart, instead of 60 sec, and so on. \n\nBecause the gaps in data are so small, we can use a simple imputation method: fill in the missing data with the value from the most recent available minute.  \n\nThis is what the *method = 'pad'* parameter of the *reinde* function below does.    ","6a81c833":"The same transformation as above, but this time we include the 3 previous values, for each future time t+1:","659d4b8c":"The spikes past the 95% (solid grey line) and 99% (dotted grey line) confidence levels look like a statistical fluke, in this context.  \n\nI conclude that log returns are white noise and cannot be predicted.","c48dee47":"Available files (provided by the organizers of the competition):\n- **train.csv**\n- **asset_details.csv**\n- example_sample_submission.csv\n- example_test.csv\n- supplemental_train.csv\n\nOnly the first two seem interesting for now.","a6cf231b":"Observations: the price at t correlates quite strongly with the price at previous time points (from t-1 to t-8), for lower price values (approximately half of maximum price). Beyond this value, the correlation becomes weaker the more we go back in time.  \n\nWe know from the first line plot we looked at that Bitcoin price surged in 2021. Also, common knowledge of crypto assets market says that prices get very volatile when there is a lot of hype and more people get into the market. So, when prices are high, we expect a lot of volatility (more abrupt and erratic price changes across days). This is what the plots above tell us too.  \n\nSo, what we conclude: there's a strong positive relationship with prices on previous days, but not so much when the market becomes hyped and volatile.","d2bfdc2a":"In conclusion, I found out that this data is likely a random walk time series. I think that discovering this through analysis is more valuable than having read this commonly known fact directly from some article dealing with forecasting security prices over time. ","345e1c7e":"We see the same information from the line plot and no further insight.  \n","cd654134":"## 8.5 Removing seasonality   \n\n*Seasonality* is a short-term pattern that repeats itself at a fixed frequency. A one-time cycle is just that, a cycle, not seasonality. \n\nIts effect:  \n- may obscure the pattern in our data (and we remove it)\n- can be picked up by our modelling algorithm (we can use it as extra feature)  \nBoth are valid approaches.  \n\nThere are many method to deseasonalize a dataset. Not all can be used in forecasting problems. Some can only be used in analysis of past data. Something to keep in mind for other type of problems.  \n\nJust as with detrending, we can do:  \n- differencing  \n- modelling  \n\n**Differencing**\n\nIn [section 8.3](#8.3-Time-series-decomposition) we saw something that looked like weekly seasonality (easy to spot in the shorter time window plot there).  \n\nSo I'll subtract from each day the closing price of the day 1 week before.","bc1c7790":"# 8. Temporal structure of time series data\n\n## 8.1 White noise\n\nIn a [previous section](#log_returns) I computed the log returns for the BTC close price. This plot reminded me of white noise.  \n\nWe can formally investigate if it actually is white noise .  \n\nA time series is white noise if the variables are independent and identically distributed with a mean of zero.  \n\nWhite noise is important in time series forecasting for two reasons:  \n1. **Prediction**: If a time series is white noise, then it's by definition random and cannot be predicted.\n2. **Diagnosis**: The errors of a time series model should be white noise. What does this mean ? That the erros contain no information, as all the information from the time series was harnessed by the model itself. And the opposite ? If the erors are not white noise, the model can be improved further.\n\nHowever, it's generally expected that any real like time series will contains a certain amount of white noise.\n\nA series is *not* white noise if:\n- the mean is non zero\n- the variance changes over time\n- the is a significant autocorrelation (with lagged values)","587c1334":"Here's the result on our real world data:","c309e6ea":"According to the usual taxonomy of Machine Learning, we have supervised, unsupervised and reinforcement learning problems. I'm looking to apply supervised learning methods for this data.  \n\nThe data does not look like the typical supervised learning dataset. In supervised learning data, for each row we expect to see several features and a label \/ value that we're trying to predict based on those features. \n\nLet's have a look again at our dataset below. Definitely not what we need, yet.","c2e5fa14":"Null hypothesis (H0) of the ADF test: time series is non-stationary.  \nBased on the results, H0 cannot be rejected and the chances of this being a fluke are small.  \n\nThen the next interesting question is **how we can make it stationary**.  \n\nAn easy method is to **subtract the previous value** for each time step t. That would be an obvious thing to do since we know our dataset looks like a random walk and we know that we obtained a random walk by using a linear function of Close(t-1) to predict Close(t).","90b9b894":"Detrended data doesn't seem to have a trend we can spot with the naked-eye. Let's see the prediction error.","9e39632b":"### Log transform\n\nLog transforms help when the data has an exponential trend, which is often likened to a hokey stick in asset price popular terminology.  \n\nWe could say we see something like a hockey stick starting in Jan 2021. Let's explore this.","68fcaf8a":"We have approximately 3 years worth of data. This informs the type of time windows we can look at.  \n\nFor example, if we zoom out at an yearly resolution, we can only compare 2018, 2019 and 2020 (2021 is incomplete).","ca1022b4":"Examine the summary statistics of the residuals. The means is the most important.","716b55a9":"Looks like a close fit.  \n\nLet's see the detrended data.","d0d3c9b6":"We've kind of seen this before, in section 8.2, but now I have the whole procedure more formalized.  \n\nThere's no point to plot the predictions, since it will overlap on our data (shifting by one time step is not visible when we have 1300 datapoints in a small plot).\n\n**Visualize residuals**\n\nBeyond predicting and evaluating the error, we should look at the errors of each prediction. Here's why:  \n- the errors should have a mean at 0. If not, it means our model is biased (tends to err more on the positive or more on the negative side). If we have a biased model, we should know. The most naive way to correct for this is to subtract the mean of residuals from all future predictions.  \n- errors should be random (no pattern, no structure). Otherwise, it means the errors contain information. Information that should have been captured by our model. So we need to improve the model.  ","126a6d47":"Let's transform the timestamp into the index of our dataframe. This will facilitate some operations down the road.","50e167b9":"I'll do one 70-30% split","8550859b":"1. Train-test split respecting the temporal order","fbdd2331":"# 2. Dataset description","1de03adf":"## 8.3 Time series decomposition  \n\nA time series is conceptualized as having these types of components:\n1. **systematic components**\n    - **level** = overall average value \n    - **trend** = temporary upward or downward movement \n    - **seasonality** = a short-term cycle that repeats itself   \n2. **non-systematic components**\n    - **random noise**\n    \nThe 4 components are thought to combine in two possible ways into a time series:  \n- additive  \n    *Close(t) = level + trend + seasonality + noise*  \n        \n- multiplicative  \n    *Close(t) = level * trend * seasonality * noise*  \n    \nIn reality, time series data can have:\n- both additive and multiplicative components  \n- both upward and downward trends (especially security price data)  \n- non-repeating and repeating cycles","71df1abd":"### Square root\n\nSquare root helps bring the data into a linear trend and well-behaved distribution (i.e. Gaussian, uniform) *when* the data is quadratic.  \n\nHere's an example on made up data.","efd1d098":"It's hard to see if there's a seasonal componentt, constistent across all years.\n\nUsing the daily closing price, I'm going to do the following:\n- break the dataset into four years\n- try to fit seasonality models (4th degree polynomials) of 3 different frequencies: weekly, monthly, trimestrial","70285f4c":"2. Multiple train-test split respecting the temporal order\n\nsklearn provides the TimeSeriesSplit method.  \n\nWe decide on the number of splits first. Let's say we want 3 splits. TimeSeriesSplit will divide the data into 4 batches of equal size, respecting the temporal order. And the 4 batches are used for training different models in this way:\n\n![image.png](attachment:1c1773a4-d37a-4c56-b4f8-ec3911ddb153.png)\n\nThe size of the test set is contastant, while the size of the training set keep increasing.\n\nStill doesn't solve the problem I noticed with the previous train-test split method.  \n\nFurthermore, in practice, time series models are retrained as new data comes in. ","f9aad6de":"An interesting selection of 14 crypto coins. I say interesting because, as a crypto investor myself, I wonder what IOTA and Dogecoin are doing in there.","9eb641c5":"It doesn't look like the right power transformation for our data.","79ef071f":"### 6.5 Lag plots  \n\nTime series data implies a relationship between the value at a time t+1 and values and previous points in time.  \n\nThe step size we take to go back in time is called **lag** (lag of 1, lag 2 etc).  \n\nPandas provides the lag plot method. Let's examine the plot first.","2393bc1c":"## 8.4 Removing trends  \n\nTo clarify, a trend is a *systematic change* that does not seem to be periodic. In our case, a bull market or a bear market have upward \/ downward trends.  \n\nWhy we'd want to look into the trend of the dataset:  \n- can inform **which modelling algorithm** we can use\n- we could **remove** the identified trend and simplify prediction (remove information)\n- trend information can become an **extra feature** for training (add information)\n\nWe can group trends into two large groups:\n- deterministic - they are consistent(e.g. temperatures across years)\n- stochastic - they tend to swtich between up and down  \n\nWe can also call them:  \n- global trends\n- local trends (I guess the bear and the bull market would fit into a local trend)\n\nTime-series that contain a trend are labelled **non-stationary**. We can *detrend* the data and make it stationary.  \n\nThe interesting thing is that detrending \"is common when using statistical methods for time series forecasting, but does not always improve results when using machine learning models.\" (<a href='https:\/\/machinelearningmastery.com\/introduction-to-time-series-forecasting-with-python\/'>Introduction to Time Series Forecasting With Python<\/a> by Jason Brownlee).  \nSomething to keep in mind.\n\nThere are two main ways to detrend:  \n- differencing\n- model the trend \n\nIn [section 8.2](#detrend_differencing) we applied differencing to remove the trend when we assumed our data is a random walk. We were left with something that looked like white noise, which cannot be modelled.  \n\nLet's try to model the trend using a >1 degree **polynomial function**, since we know from just looking at it that our trend is not linear.  \n\nI will assume it's a cubic (3rd degree) polynomial:  \ny = a * x + b * x^2 + c * x^3 + d\n\nQuick overview of what polynomials look like:\n![image.png](attachment:35a0966e-05df-46a6-8bee-138c1df63616.png)\n\nSource: <a href='https:\/\/en.wikiversity.org\/wiki\/Algebra_II\/Polynomial_Functions'>Wikiversity<\/a>\n\nIf you have to revisit curve fitting procedure (like I had to do), <a href='https:\/\/machinelearningmastery.com\/curve-fitting-with-python\/'>this tutorial is great<\/a>.  \n\nA summary from the tutorial mentioned above:   \n\"*Curve fitting involves first defining the functional form of the mapping function (also called the basis function or objective function), then searching for the parameters to the function that result in the minimum error.\" (<a href='https:\/\/machinelearningmastery.com\/curve-fitting-with-python\/'>Curve Fitting With Python<\/a>, by Jason Brownlee)*","c5ce0a62":"We can use the **candlestick** function from the **graph_objects package** of plotly to visualize trading data as the common candlestick plot.","8ea8bfd0":"There are approximately 24 million observations in our time series.","410b6ba8":"Still not very informative. This doesn't seem to be the right way to look at assets prices and now we know.","d60b5105":"# 4. Preprocessing  \n\nDetecting features for which we have missing values is easy to do. We just look for null values per column.   ","520c61fe":"Check the output","71c8b381":"## 8.2 Random walk  \n\nFirst, let's see a perfect random walk dataset and then we'll look at our own data again.  \n\nA time series is constructed through a *random walk* process as follows:  \ny(t) = X(t-1) + rnd_step,  \nwhere rnd_step is randomly selected from {-1, 1} and os is x(0).  \n\nWe construct this dataset below.","4584bfff":"Or, more clearly:","2c277dd6":"We don't have the same number of observations for each year. 2020 has one extra day, for example. 2018 is missing a single data point. And 2021 is quite incomplete. \nJust to proceed fast through this step, I'll use a quick dirty trick and join the data for each year while filling in missing values with NaN. It's not the best thing to do, but for the scope of this analysis it's good enough. ","b7108f72":"<a id='persistence_model'><\/a>\nSadly, it looks like we get white noise when we remove non-stationarity, which we know contains no structure that we can model and use for prediction.  \n\nA naive model for white noise is:  \n*Close(t) = Close(t-1)*  \nThis is also called the **persistence model**  \nJust because we know the next value is a function of the current value but we have no way to build a model of this relationship.  \n\nThe persistence model will be the baseline for any future model.","3843b900":"<a id='non_stationary'><\/a>\nWe see that the data is non-stationary: there is an **increasing trend**, yet non-linear (the 1-year rolling average in the plot above, going up at least in the last two years) and a **seasonality** component (local ups and downs).  \n\nFurthermore, data **variance** (difference between peaks and throughs in the local oscillaions) also changes (higher in the last year). \n\nThese make it more difficult to model the data with classical statistical methods.\n\nA common wasy to remove these and to improve the signal to noise ratio are power transforms.  \n\nTwo of the commonly used power transforms are: **square root** and **log transform**. ","ed3a21b5":"If we have some domain knowledge, we can say that we're not surprised we don't see any correlation between consecutive days.","a7b803e9":"Not a lot of data is missing. We can drop those rows, impute the values etc. I'm not going to use Target, so I won't go into this right now. \n\nThe more covert missing data is when we don't have any information at all for a particular minute. Which means that **whole rows may be missing** from our dataset.  \n\nWe should have one row per minute per asset. Since we extracted the data for a single asset, we expect consecutive rows to have a difference of 60 seconds between their index values.  \n\nFirst step is to look at the **time lag between consecutive entries** in our dataset.","32a6a293":"**Modelling seasonality**\n\nLike with detrending, we can model the seasonality and subtract it from the data.  \n\nSeasonality has the shape of the sine, which can be modelled as a 4th degree polynomial:  \ny = a * x + b * x^2 + c * x^3 + d * x^4 + e","6a30dcfe":"Mean and std are almost 0. Since variance = std^2, it's also close to 0. \n\nSince we have a lot of data, we can split it into shorter time intervals and see if the summary stats change. ","b7548ba6":"The squared root doesn't seem to help because our data is not quadratic.","28287dc0":"We see a strong positive relation between Close price at t and Close price at t+1.  \n\nLet's experiment with the lag value.","ef3d43d5":"### 6.1 Line plots","ed19b48d":"The problem we see in the plot above is that matshow seems to scale the colors relative to the whole 3 years worth of data.  \nWe've seen in the box plots above that that 2021 has a much higher mean value than all previous years.  \n\nSo this upward going trend from one year to the next obscures the evolution of price throughout the year and makes it less visible.  \n\nTherefore I'll first normalize (from min 0 to max 1) the closing price for each year and replot.","9aef711b":"What I would like to find out is how to get more insight from these plots.  \n\nFor example:  \n- I see that additive and multiplicative decomposition produced similar plot. How to judge which one is better ?  \n- The seasonal components look similar, but the multiplicate is centered around 1 while the additive seems to have an average somewhere between 0 and 50  \n- The average residuals have a mean of 0 while the multiplicative have a mean of 1\n- Also, the amplitude of residuals is quite different, orders of magnitude different (4000 versus 1) My naive interpretation is that the additive model does not capture the information in the data well enough and it's even more wrong in periods of hype (start of 2018 and 2021).\n\nUnfortunately, all articles, blog posts and book chapters I found so far stop at showing these plots and briefly mentioning what trend, season and remainders mean. As I find out more about what conclusions we can draw, I will update this notebook.","95de66ee":"### 5.3 Window features  \n\nWe saw how we can add a window of a certain width as features to be used by our model to forecast the value at future time t+1.  \n\nBesides using the raw values from the time window (i.e. Close(t-1), Close(t), etc), we can also compute *summary statistics* of these values and use them as features for prediction.  \n\nThe most common aggregate value is the mean of the lag window (also called **moving average** or **rolling mean**).\n\n| time | value |  \n| --- | --- |  \n| t1 | v1  \n| t2 | v2  \n| t3 | v3  \n| t4 | v4\n\n**shifted**  \n\n| time | value | \n| --- | --- |  \n| t1 | ?  \n| t2 | v1  \n| t3 | v2  \n| t4 | v3\n\nshifted, applied **rolling mean** (aka moving average ) with a window size of 2  \n\n| time | value | \n| --- | --- |  \n| t1 | ? |  \n| t2 | ? | \n| t3 | (v1+v2) \/ 2 | \n| t4 | (v2+v3) \/ 2 |\n\nFinal dataset\n\n| time | mean | Close(t+1) |  \n| --- | --- | --- |   \nt1&emsp;$\\;$|  ?  |  v1  |  \nt2&emsp;$\\;$|  ?  |  v2  |  \nt3&emsp;$\\;$|  (v1+v2) \/ 2  |  v3  |  \nt4&emsp;$\\;$|  (v2+v3) \/ 2  |  v4  |  \n\nThe moving average can be used as a naive prediction model: predict for next day the average of the last *w* days (where *w* is the width of the moving average window). But before doing this, the theory says we are supposed to have *stationary* data (having no obvious upward or downward trend and no seasonality), so we'll need some further preprocessing down the road. ","e26aff44":"Remember the [persistence model](#persistence_model) from earlier ? It had better RMSE (1045). But of course we don't expect the trend to be a good prediction model for this problem. We just model it so we can remove it from the data and then find a good model on the detrended data. ","8d83aac6":"And that would be our training data for a supervised learning model.\n\nTrying to predict coin prices based on day and hour will likely be a poor model.  \n\nBut we can enrich these features with additional ones. Here are a few candidates:\n- Weekend or not.\n- Season of the year.\n- Business quarter of the year.  \netc  \n\nI'm not convinced this would help much, so I'll explore further ways to build features.","c62a8428":"Evaluate using root mean squared error:  \n- because squaring errors punishes more the large errors  \n- taking the square root brings it back into the same units of measurement","e3913bf0":"### 6.6 Autocorrelation plots\n\nLag plots showed a relationship between current price and price at previous time points.  \n\nWe can also quantify the relationship between the price and the actual lag value.  \n\nFor a lag=1, we can compute the correlation between current time step value and previous time step value. If we have n time steps in our data, we'll have n-1 correlation values. These values can be anywhere inthe interval [-1,1].  \n\n-1 (strongest negative correlation)  \n0 (no relationshop at all)  \n1 (strongest positive correlation)  \n\nThis computation can be done for lag=1 to any lag value we want.\n\nPandas provides the **autocorrelation_plot()** function for this.  \n\nValues outside the dotted lines are statistically significant.  \n\nFor many types of time series data this will look like a sinusoid, with a decreasing amplitude. It's not the case for asset prices (at least not for those with large liquidity  - traded by many people).  ","ed0a5a25":"To sum up, the three methods above are a few basic and common ways (by the book) in which time series are rephrased into a dataset on which we can apply supervised machine learning methods.  \n\nThat's it for feature engineering for now. I'll move on to visualizing the data.","956d11fd":"Candlestick plots look nice and fancy, but later we'll go beyond this into exploring trends, cycles and seasonality of the dataset.  \n\nFor now, it's just a way to get a quick look at our data, so we have a mental model of what we're dealing with.","3a1db2e0":"### Assets \n\nIn the list of transactions, assets are referred to by Asset_ID. Let's look into **asset_details.csv** to see what these 'assets' are.","334e1673":"Finally, the lag does not have to be linear: we don't necessarily have to use the previous k values. We can include in the lag window values from the same day last week or the same hour the day before etc. ","7e2c06ea":"This doesn't look like anything to me. The seasonality amplitude is so small, I don't think it matters if we remove it or not or if we add it as an egineered feature.","923d7d51":"#### Rephrasing the dataset into a supervised learning dataset. \n\nIn supervised learning datasets we have entries with a set of features (our **x**) and a label \/ value for our output variable which we want to predict (our **y**).  \n\nIn the case of time series, our output **y** is the price at time t. But what are the inputs for a possible prediction model ?   \n\nTo phrase it differently, for each entry in our dataset we need to have a set of feaures and a label (used for training and then outputted as a prediction of our model).  \n\nThis means we need to convert our original data:\n\n| time | value |\n| --- | --- |\n| 1514764860 | 13850.176 | \n| 1514764920 | 13828.102 |\n| 1514764980 | 13801.314 |\n| ... | ... |\n\ninto something of this form:\n\n| x | y |\n| --- | --- |\n| x1 | y1 |\n| x2 | y2 |\n| x3 | y3 |","e10ebc1c":"I don't think training on 2018-2020 data and predicting on 2021 data is a good idea ^^","33c28537":"<a href='https:\/\/stats.stackexchange.com\/questions\/101274\/how-to-interpret-a-qq-plot'>This<\/a> is a usefull discussion on how to interpret the Q-Q plot. It seems like the residuals distribution is a heavy-tail, not a normal one. For an actual prediction model, this would be something to look into and correct.","35a91bbe":"<a id='log_returns'><\/a>\n#### Log returns feature engineering","e07e1554":"The mean is not 0. The persistence model tends to predict higher values than the real ones. This means we should correct for this bias. Solutions range from subtracting 76 (the mean of residuals) for predictions to complex bias correction methods.\n\n**Residuals distribution**\n\nThe distribution should be Gaussian, otherwise it means there's something worng with our model.  ","774f490f":"We looked at the distribution of values across the whole timeframe (3 years worth of data).  \n\nBut it may be useful to examine the distribution in smaller time windows.  \n\nLet's group our data by year and visualize it as box and whisker plots.","8d9dfa5d":"A typical feature for assets price time series is the ratio between the current price and the price at the previous time point. This is usually computed as a log of the ratio in time series modelling, because that facilitates some apperations (additions, etc). In this case, they are called **log returns**.\n\nTo compute the log return, we can simply take the difference between the logarithm of two consecutive prices. ","6efa376a":"The autocorrelation plot of the closing price looks exactly like that of a random walk. Random walks are tricky to predict. Still, most assets price data looks like a random walk, from what I read.  \n\nIn [section 7](#non_stationary) I mentioned our closing price time series looking **non-stationary**. It's time to apply formal methods to test this assumption.  \n\nThe *statsmodel* library provides the *adfuller()* method which implements the *Augmented Dickey-Fuller test*.  ","a5b9ea54":"The best power transformation for our data seems to be the reciprocal square root (lambda = -0.5), according to the boxcox method.  \n\nThe distribution we obtain is the closest to a normal distribution, so far. But the trend is not linear. We see what happens with data from previous bull markets (2018 highs become more pronounced after this transformation). Too bad we don't have data from December 2017, when the actual peak of the previous bull market happened for BTC prices. My guess is it would approach the level of July 2021 in a transformed plot.  \n\nI won't explore this further for the moment, as I'm not convinced this is the right transformation we need for our data.","fd2c97f0":"How about a daily pattern ?  \n\nLet's see a heatmap of price during the day for 10 consecutive days.","f72215e9":"### 6.3 Box and whisker plots","80947056":"#### Types of features (x) we can create from time series data:  \n- **date time** features \n- **lag** features  \n- **window** features   ","f61f9eca":"**Autocorrelation plot of residuals**\n\nWe should find no autocorrelation. If there is some, we should improve our model and capture it.  \n\nAnd the plot below shows no significant autocorrelation.","6ea9d3c5":"### Automatic search for the right power transform\n\nscipy package offers a method to automatically search for the most suitable power transformation for a dataset: <a href='https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.boxcox.html'>boxcox function<\/a>.\n\n[Short overview]  \nBoxCox procedure identifies the appropriate exponent (lambda = l) to use to transform data into a well-behaved distribution.    \n\nThe returned lambda value indicates the power to which all data should be raised.  \n\nIn order to do this, the Box-Cox power transformation searches from lambda = -5 to lambda = +5 until the best value is found.  \n\nBest lambda means: the one for which the transformed data's standard deviation is the smallest. In this situation, the data has the highest likelihood \u2013 but not a guarantee \u2013 to be normally distributed.\n\nTable 1: Common Box-Cox Transformations\n\n| lambda | transformed Y |   \n| --- | --- |   \n| -2 | \tY^(-2) = 1\/Y^2\n| -1 | \tY^(-1) = 1\/Y^1\n| -0.5 | \tY^(-0.5) = 1\/(Sqrt(Y))\n| | \tlog(Y)\n| 0.5 | \tY^(0.5) = Sqrt(Y)\n| 1 | \tY^1 = Y\n| 2 | \tY^2","b3eb0423":"## 9. Model evaluation\n\nIn ML uspervised learning problems we usually use a train-test split to separate our dataset into data to use for training and data to use for testing.  \n\nHow do we do this with time series data, where we can't just do a random split, since we have to respect the order of the data ?\n\nApparently, there are three common method to split data for backtesting:\n1. Train-test split respecting the temporal order  \n2. Multiple train-test split respecting the temporal order  \n3. Walk-forward validation, where we update the model every time we reveice a new piece of data\n","b8083a18":"Let's see the linear plot again.","62af2060":"Let's see the autocorrelation for the log returns now.","f8fe0cbd":"There are 3 types of information to explore in a time series through visualization:\n\n<strong>temporal structure:<\/strong>  \n&emsp;- line plots  \n&emsp;- lag plots  \n&emsp;- autocorrelation plots  \n<strong>the distribution of observations:<\/strong>  \n&emsp;- histograms  \n&emsp;- density plots  \n<strong>the change in distribution over time intervals:<\/strong>  \n&emsp;- box and whisker plots  \n&emsp;- heatmap plots","4153f295":"# 3. Basic trading data visualization","7cea948c":"There are of course many summary statistics we can use besides the average for the previous time window.    \n\nAlso, the window does not have to have a fixed width. It can also be a forever expanding window.  \n\n| time | value |\n| --- | --- | \n| t1 | 1 | \n| t2 | 2 | \n| t3 | 3 | \n| t4 | 4 | \n| t4 | 5 |\n\n**expanding** rolling window  \n\n| # |values |\n| --- | --- |\n| 0 | 1 2 |\n| 1 | 1 2 3 |\n| 2 | 1 2 3 4 |\n| 4 | 1 2 3 4 5 |\n\nfinal engineered dataset having summay statistics (min, mean and max window values) computed for the **expanding** rolling window\n\n| # | min | mean | max | t+1 |\n| --- | --- | --- | --- | --- |\n| 0 | 1 | 1 | 1 | 2 |\n| 1 | 1 | 1.5 | 2 | 3 |\n| 2 | 1 | 2 | 3 | 4 |\n| 3 | 1 | 2.5 | 4 | 5 |","0508aaf5":"### 6.2 Histograms and density plots  \n\nSome linear time series forecasting methods assume a well-behaved distribution of observations (like a normal distribution).  \n\nBefore doing statistical tests to formally assess the assumption of normality, we can easily visualize our data as a histogram.","bc68ebf0":"[Short recap]   \nA box plot is interpretted like this:\n\n- The middle 50% of the data is contained in the block itself. The upper edge (hinge) of the box indicates the 75th percentile of the data set, and the lower hinge indicates the 25th percentile. \n- The horizontal line inside the box indicates the median value of the data.\n- If the median line within the box is not equidistant from the hinges, then the data is skewed.\n- The small horizontal ends of the vertical lines (the \"whiskers\") indicate the minimum and maximum data values, unless outliers are present in which case the whiskers extend to a maximum of 1.5 times the inter-quartile range (the hight of the box).\n- The points outside the ends of the whiskers are (suspected) outliers.  \n\nInsights from the plot above:  \n- median value for BTC changes slightly in the first 3 years\n- 2018 and 2020 are quite rich in outliers compared to the other 2\n- in 2021 BTC price spiked ","a33f3968":"**Evaluating the persistence model**\n\nThe persistence model was the naive one that predicts for the next time step the same value from the current time step. This seems like a good baselin model for security price, since the linear plot for asset price looked like a random walk anyway. And in random walk remember we  computed the next value as a linear transformation of the current value.  \n\nI'll go through formally evaluating the persistene model more like a practice for the serious models I will eventually build.  \n\n1. Transform the Closing price dataset into a supervised learning problem\n2. Establish the train and test sets\n3. Define the persistence model\n4. Make predictions and establish a baseline performance","d3ff34ee":"Great, no more time gaps in our dataset !  \n\nWe don't need the index as a timestamp anymore. For future analysis it will be easier to have it as a date.","e79a894f":"# 7. Power transforms ","c429b513":"### 5.1 Date time features\n\nAgain, our data looks like this:  \n\n| timestamp | value |\n| --- | --- |\n| 1514764860 | 13850.176 | \n| 1514764920 | 13828.102 |\n| 1514764980 | 13801.314 |\n| ... | ... |\n\nWe can transform the timestamp into day, hour, min, this way creating three new features.  \n\nWhat types of questions we can answer with this approach ? We can imagine a problem where we're trying to predict the price for a specific time on a specific day.  \n\nThis makes more sense if our data was temperatures throughout the day, where we know there are daily cycles, yearly cycles etc, rather than stocks, but we're just using this for illustration purposes for now.  ","7dfc98cd":"Difficult to tell with the naked eye. There's something called Q-Q plot that can help.  \n- it orders residuals ascendingly\n- plots them in a scatterplot against those of an ideal Gaussian distribution","745e0ebe":"**Data features**  \n\n- **timestamp** - Unix timestamps (the number of seconds elapsed since 1970-01-01 00:00:00.000 UTC). Timestamps in this dataset are multiple of 60, indicating minute-by-minute data.\n- **Asset_ID** - uniquely identifies the traded coin\n- **Count** - number of trades executed within the respective minute\n- **Open, High, Low, Close** - the usual price details for a given unit of time. \n- **Volume** - amount of units of this coin traded in the particular minute\n- **VWAP** - The average price of the asset over the time interval, weighted by volume. VWAP is an aggregated form of trade data.\n- **Target** - Residual log-returns for the asset over a 15 minute horizon <- we know this from the competition's official description.","fff33543":"# 6. Typical time series visualizations  ","81c9ea9f":"3. Walk-forward validation\n\nAgain, temporal order is respected. This is always the case with time series data.  \n\n- We start on small window of our dataset (we can chose a minimum window width to our liking). We train on this window and predict the value for the next time stamp.\n- We expand the window with the new observation, retrain, predict for the next point in time  \n- and so on...  \n\nWe can opt for an expanding window or for a sliding window (which slides 'to the left' instead of increasing in size).  \nWith this method, for 2 mil data points (our initial data size for BTC, at 1-min resolution), we'll end up training about 2 mil models. I don't know if it's feasible or we'd have to do some window jumps instead of sliding. Also, in this case, an expanding window might be computationally expensive (training of 1 mil data points), I don't know yet.  \n\nIn any case, this seems to be the way to do training in time series modeling.","c3182f72":"We see a statistically significant correlation with up tp 200-ish previous days average prices. The more we go back in time, the lower the correlation, until it starts to become slightly negative (statistically significant).  \n\nWe know markets have a global evolution (bull \/ bear market) and local trends (short term excitement for the price of an asset and then slight corrections).\n\nSo, depending on the time resolution our data has, I expect the autocorrelation plot to capture the underlying long or short term trend. ","fb093346":"Get a subset of our data (the last 60 entries for example) and restrict the analysis to one asset (which makes it a univariate analysis). ","5859305b":"Now it's easier to spot maximum and minimum per year, but we still don't see an pattern across years.","029420c2":"# 5. Feature engineering","f2c9fcb7":"### 6.4 Heat map plots  \n\nAnother way to look at this data is to plot it as a 2D plot and color-encode price values. ","86caf0d2":"The plot is a bit dense, since it contains all the data we had (almost 3 years worth of data, on a 1-minuted resolution).  \n\nBut it's apparent that we don't see a noticeable pattern (no apperent pattern that repeats itself year after year, for example).  \n\nFor time series, it can be better to look at and compare plots from the same interval, like day-to-day, year-to-year etc.\n\nLet's assume we're naive about crypto prices and we want to investigate whether there is an daily seasonality, so we want to plot it in a more informative way. ","b1bfe384":"Let's see it as a density plot too.\n","753ca2b5":"**Quick intro**\n\nI wanted to see a notebook \/ tutorial that would take me through the **basics of working with time series**.  \n\nMost notebooks I saw were either not very rigorous or they took me straight into price prediction using some methods, which is not what I needed right away.  \n\nSo I wrote my own introductory notebook. I acquired most of the information I used here through reading <a href='https:\/\/machinelearningmastery.com\/introduction-to-time-series-forecasting-with-python\/'>Introduction to Time Series Forecasting With Python<\/a> by Jason Brownlee (took a few days to read an implement).   \n\nWhat this notebook is:  \n- a good starting point for understanding time series data and how it differs from problems with other type of tabular data \n- a cookbook we can use for exploration when starting to work with a new dataset\n\nWhat this notebook is not:\n- it is not about prediction. It stops at exploration and understanding the data.\n- it's not meant for advanced practitioners of asset price prediction - unless you want to revisit some concepts.  \n\n### Contents\n\n[1. Quick overview](#1.-Quick-overview)  \n[2. Dataset description](#2.-Dataset-description)  \n[3. Basic trading data visualization](#3.-Basic-trading-data-visualization)  \n[4. Preprocessing](#4.-Preprocessing)  \n[5. Feature engineering](#5.-Feature-engineering)  \n[6. Typical time series visualizations](#6.-Typical-time-series-visualizations)  \n[7. Power transforms](#7.-Power-transforms)  \n[8. Temporal structure of time series data](#8.-Temporal-structure-of-time-series-data)  \n[9. Model evaluation](#9.-Model-evaluation)","16451693":"You've reached the temporary end. This is work in progress. There is no conclusion yet. I continue to update this notebook as I read more."}}