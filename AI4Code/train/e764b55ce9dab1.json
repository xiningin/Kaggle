{"cell_type":{"aff3317e":"code","04eb589d":"code","eb7d667d":"code","0276b9ba":"code","9c7056ec":"code","ad2f7003":"code","c4347cda":"code","44fb7a0e":"code","39a6d61a":"code","1ee2fa3d":"code","b64254ba":"code","d583f95b":"code","ae61608e":"code","08ea1bf5":"code","37bdad3f":"code","38ab9411":"markdown","f911a5bf":"markdown","acc40c53":"markdown","e74b469d":"markdown"},"source":{"aff3317e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","04eb589d":"from google.cloud import bigquery","eb7d667d":"client = bigquery.Client()","0276b9ba":"# in bighquery each dataset is contained in soe project\n# for example \"Hacker_news\" data set is contained in \"bigquery-public-data\"\n# we will tell our client to get the data from the project by using the dataset method and we will store it in our variable\ndataset_ref = client.dataset(\"hacker_news\", project = \"bigquery-public-data\")","9c7056ec":"# now that our client has fulfilled our request in bringing the dataset from projct to the dataset referece\n# now our client is sitting ideal, so we will request our client to pull the data from the reference to our dataset variable\n# using get_dataset() method\n\ndataset = client.get_dataset(dataset_ref)","ad2f7003":"# now our data is loaded into dataset variable\n# every dataset is the collection of tables, think it as a spreadsheet file containing multiple tables all composed of \n# rows and columns.","c4347cda":"# list all the tables present in the dataset usint list_tables()  method\n# remember our client is sitting ideal so we will again order him to list the tables\ntables = list(client.list_tables(dataset))","44fb7a0e":"# to print the table id present\nfor i in tables:\n    print(i.table_id)","39a6d61a":"# Similar to how we fetched a dataset, we can fetch a table. In the code cell below, we fetch the full table in \n# the hacker_news dataset.\n# remember that our client will bring every information from the reference dataset named \"dataset_ref\" as it has the \n# reference to the original dataset present in the \"bigquery-public-dataset\"\ntable_ref = dataset_ref.table('full')\n\n# now fetch the table from the reference \ntable = client.get_table(table_ref)","1ee2fa3d":"# to view the schema of the table 'full'\ntable.schema","b64254ba":"# We can use the list_rows() method to check just the first five lines of of the full table to make sure this is right.\n# (Sometimes databases have outdated descriptions, so it's good to check.) \n# This returns a BigQuery RowIterator object that can quickly be converted to a pandas DataFrame with the to_dataframe() method.","d583f95b":"client.list_rows(table, max_results = 5).to_dataframe()","ae61608e":"# Preview the first five entries in the \"by\" column of the \"full\" table\n# we can also select a particular colums to look its values\n# here we are using slicing to display only the 0th column as upper index is excluded in python list slicing\nclient.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()","08ea1bf5":"# Preview the first five entries in the \"by\" column of the \"full\" table\n# looking at only 2 columns\nclient.list_rows(table, selected_fields=table.schema[:2], max_results=5).to_dataframe()","37bdad3f":"# looking only last column\n# Preview the first five entries in the \"by\" column of the \"full\" table\nclient.list_rows(table, selected_fields=table.schema[-1:], max_results=5).to_dataframe()","38ab9411":"### The first step in the workflow is to create a Client object. As you'll soon see, this Client object will play a central role in retrieving information from BigQuery datasets.","f911a5bf":"This tells us:\n\n* the field (or column) is called by,\n* the data in this field is strings,\n* NULL values are allowed, and\n* it contains the usernames corresponding to each item's author.\n* We can use the list_rows() method to check just the first five lines of of the full table to make sure this is right. (Sometimes databases have outdated descriptions, so it's good to check.) This returns a BigQuery RowIterator object that can quickly be converted to a pandas DataFrame with the to_dataframe() method","acc40c53":"# SQL using BigQuery","e74b469d":"*Each SchemaField tells us about a specific column (which we also refer to as a field). In order, the information is:*\n\n* The name of the column\n* The field type (or datatype) in the column\n* The mode of the column ('NULLABLE' means that a column allows NULL values, and is the default)\n* A description of the data in that column\n* The first field has the SchemaField:\n\nSchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())"}}