{"cell_type":{"04a2a913":"code","c17d7d7e":"code","125ff71e":"code","816db3ab":"code","7e0c4ab8":"code","d7aa78fb":"code","4b983a86":"code","7436365a":"code","fede6161":"code","b19ff1ea":"code","27520a02":"code","0da569eb":"code","7d61e57b":"code","c0c3ff9c":"code","942b265a":"code","b244c110":"code","de9be1a2":"code","a2663c52":"code","eb732164":"markdown","df517ccd":"markdown","b1bf7f1e":"markdown","603b29b8":"markdown","0d04c4df":"markdown","a68f69c8":"markdown"},"source":{"04a2a913":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c17d7d7e":"import pandas as pd\nimport numpy as np\n\ndata=pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndata.head()","125ff71e":"data.drop(['customerID','Churn','TotalCharges'],axis=1,inplace=True)\ndata.head()","816db3ab":"from sklearn.preprocessing import LabelEncoder\n\nle=LabelEncoder()\ncat_cols=data.select_dtypes(exclude=['int','float']).columns\nenc_data=list(cat_cols)\ndata[enc_data]=data[enc_data].apply(lambda col:le.fit_transform(col))\ndata[enc_data].head()","7e0c4ab8":"from sklearn.preprocessing import StandardScaler\n\nscale=StandardScaler()\ndatas=scale.fit_transform(data)\ndata_df=pd.DataFrame(datas)\ndata_df.head()","d7aa78fb":"from sklearn.decomposition import PCA\n\npca=PCA(n_components=2)\npc=pca.fit_transform(datas)\n\npdf=pd.DataFrame(data=pc,columns=['principal component 1','principal component 2'])\npdf.head()","4b983a86":"sim_data=pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\nsim_data=sim_data[['Churn']]\nsim_data.head()\n# sim_data=pd.DataFrame(le.fit_transform(sim_data),columns=['Churn'])\n# sim_data.head()","7436365a":"data=pd.concat([pdf,sim_data],axis=1)\ndata.head()","fede6161":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = ['Yes','No']\ncolors = ['g','r']\nfor target, color in zip(targets,colors):\n    indicesToKeep = data['Churn'] == target\n    ax.scatter(data.loc[indicesToKeep, 'principal component 1']\n               , data.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","b19ff1ea":"data=pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndata.drop(['customerID','Churn','TotalCharges'],axis=1,inplace=True)\ndata.head()","27520a02":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nle=LabelEncoder()\ncat_cols=data.select_dtypes(exclude=['int','float']).columns\nenc_data=list(cat_cols)\ndata[enc_data]=data[enc_data].apply(lambda col:le.fit_transform(col))\n\n\n\nscale=StandardScaler()\ndatas=scale.fit_transform(data)\ndata_df=pd.DataFrame(datas)\n\nscore_list=[]\nfor n_clusters in range(2,15):\n        clusterer = KMeans (n_clusters=n_clusters).fit(data)\n        preds = clusterer.predict(data)\n        centers = clusterer.cluster_centers_\n\n        score = silhouette_score (data, preds, metric='euclidean')\n        score_list.append(score)\n        print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))\n\nplt.bar(range(2,15),score_list)\nplt.show()","0da569eb":"model=KMeans(n_clusters=4)\nmodel.fit(data)\nprint(model.labels_)","7d61e57b":"target=pd.DataFrame(model.labels_,columns=['target'])\ndata=pd.concat([pdf,target],axis=1)\ndata.head()","c0c3ff9c":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('Visualizing', fontsize = 20)\ntargets = [0,1,2,3]\ncolors = ['r', 'g', 'b','k']\nfor target, color in zip(targets,colors):\n    indicesToKeep = data['target'] == target\n    ax.scatter(data.loc[indicesToKeep, 'principal component 1']\n               , data.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()\n# targets=[0,1,2,3]\n# colors=['red','green','blue','black']\n# for t,c in zip(targets,colors):\n#     keep=data['target']==t\n#     plt.scatter(data.loc[keep,'principal component 1'],\n#                 data.loc[keep,'principal component 2'],c=colors,s=50)\n# plt.legend(targets)\n# plt.show()","942b265a":"from sklearn.decomposition import KernelPCA\n\npca=KernelPCA(n_components=2,kernel='rbf')\npc=pca.fit_transform(datas)\n\npdf=pd.DataFrame(data=pc,columns=['principal component 1','principal component 2'])\npdf.head()","b244c110":"data=pd.concat([pdf,sim_data],axis=1)\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = ['Yes','No']\ncolors = ['g','r']\nfor target, color in zip(targets,colors):\n    indicesToKeep = data['Churn'] == target\n    ax.scatter(data.loc[indicesToKeep, 'principal component 1']\n               , data.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()\n# targets=['Yes','No']\n# colors=['red','green']\n# for t,c in zip(targets,colors):\n#     keep=data['Churn']==t\n#     plt.scatter(data.loc[keep,'principal component 1'],\n#                 data.loc[keep,'principal component 2'],c=colors,s=50)\n# plt.legend(targets)\n# plt.show()","de9be1a2":"target=pd.DataFrame(model.labels_,columns=['target'])\ndata=pd.concat([pdf,target],axis=1)\ndata.head()","a2663c52":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0,1,2,3]\ncolors = ['r','g','b','k']\nfor target, color in zip(targets,colors):\n    indicesToKeep = data['target'] == target\n    ax.scatter(data.loc[indicesToKeep, 'principal component 1']\n               , data.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()\n\n\n# targets=[0,1,2,3]\n# colors=['red','green','blue','black']\n# for t,c in zip(targets,colors):\n#     keep=data['target']==t\n#     plt.scatter(data.loc[keep,'principal component 1'],\n#                 data.loc[keep,'principal component 2'],c=colors,s=50)\n# plt.legend(targets)\n# plt.show()","eb732164":"**PCA for Visualization**\n\nSince we have hell so many features, there will be that many dimensions while visualizing. Since we are all humans (considering Kaggle already did a captcha) and can only visualize stuff in 2D or 3D max, we need to pull the features down to 2D. That dimensionality reduction is possible by applying Principal Component Analysis(PCA) on the data.","df517ccd":"**Importing and Reading**\nHere as usual we are importing the basic libraries and also reading the data.","b1bf7f1e":"We will directly move on to our preprocessing as data analysis is covered in [this notebook](http:\/\/www.kaggle.com\/vedanth777\/telecom-customer-churn-data-preprocessing) already. So we will directly jump into our label encoding part.","603b29b8":"**Plots**\n\nThe first graphs we are going to plot is going to be for the churned and existing customers. The next two sets of graphs are going to be for the clustering of different types of customers including those existing as well as churned","0d04c4df":"As we encoded the categorical columns we need to scale the numeric columns inorder to maintain stability throughout the data as one feature maybe ranging differently than other and may indirectly have more influence over the prediction just because of the numeric range and not because of the feature actually contributing towards the actual prediction of target.","a68f69c8":"**Data Preprocessing**\n\nFirst of all, we will drop the unnecessary columns and the target column from our data."}}