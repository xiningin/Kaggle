{"cell_type":{"ceb1f919":"code","0634c0b1":"code","ff5bc6d3":"code","adfc12e9":"code","21e53396":"code","01f8dd36":"code","4cf3e2d0":"code","2778be2b":"code","5db2d09b":"code","33f38400":"code","b31ac892":"code","b920039e":"code","3c6c444e":"code","13dd1f73":"code","718ef3bd":"code","9ed120e6":"code","5882366b":"code","248f5d90":"code","e3e5c69b":"code","eeb38e84":"code","045f2357":"code","336d0090":"code","c04e98a5":"code","67661c09":"code","49b9e7bb":"code","e93ae229":"code","3f40ce07":"code","3ae274da":"code","c1dc04b1":"code","b3603b63":"code","17ff8055":"code","c5d53aa6":"code","333ed59b":"code","377051a9":"code","cbe994ac":"code","c20d7070":"code","e30e312e":"code","7c20bd78":"code","a1ac0b1a":"code","47b2e7ab":"code","b03325a7":"code","1a144625":"code","57c11850":"code","c19db3fd":"code","632de94b":"code","b1279fd2":"code","4a5d05c5":"code","ea324bac":"code","e8d7f2b9":"code","191686e1":"code","c7976aa7":"code","d8c29916":"code","5cbe5f08":"markdown","2781ee05":"markdown","a8c57b9b":"markdown","e0ba4623":"markdown","0012a286":"markdown","1b5ed9a3":"markdown","03a04c25":"markdown","4bacdcf2":"markdown","a74bd182":"markdown","218080c1":"markdown","12406259":"markdown","5a693f67":"markdown","7ce81c11":"markdown","b49561c6":"markdown","e90506d2":"markdown","879054b6":"markdown","71061023":"markdown","fc34fa61":"markdown","20a5a6fd":"markdown","8bcb2f55":"markdown","5c3a550e":"markdown","ef8e6a93":"markdown","fc43e93c":"markdown","592f1fab":"markdown","8aec729c":"markdown","1cd9b66d":"markdown","0d96e505":"markdown","9dc1dd92":"markdown","0ae3f4a5":"markdown","bce93028":"markdown","a7aa10cc":"markdown","58496793":"markdown"},"source":{"ceb1f919":"!pip install pyspark","0634c0b1":"#load_ext nb_black\nfrom pyspark.sql import SparkSession\n#session all related to df not rdd\nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\n\nspark = SparkSession.builder.getOrCreate()","ff5bc6d3":"spark","adfc12e9":"#import os\n#from os.path import isfile, join\n\n#loc = os.path.abspath(\"\")\n#data_loc = f\"{loc}\/data\/\"","21e53396":"df = spark.read.csv(\"..\/input\/paysim1\", inferSchema=True, header=True)","01f8dd36":"df.printSchema()","4cf3e2d0":"df.show(2)","2778be2b":"df = df.select(\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"isFraud\")","5db2d09b":"df.show(2)","33f38400":"df.printSchema()","b31ac892":"(df.count() , len(df.columns))","b920039e":"df.select('amount','oldbalanceOrg','newbalanceOrig','isFraud').describe().show()","3c6c444e":"# null values in each column\ndata_agg = df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns])\ndata_agg.show()","13dd1f73":"# value counts of Type column\ndf.groupBy('type').count().show()","718ef3bd":"train, test = df.randomSplit([0.7, 0.3], seed=7)","9ed120e6":"#it better be random so that we will have a better distribution and so better model.\n","5882366b":"print(f\"Train set length: {train.count()} records\")\nprint(f\"Test set length: {test.count()} records\")","248f5d90":"train.show(2)","e3e5c69b":"train.dtypes","eeb38e84":"catCols = [x for (x, dataType) in train.dtypes if dataType == \"string\"]\nnumCols = [ x for (x, dataType) in train.dtypes if (dataType == \"double\") ]\n#numCols = [ x for (x, dataType) in train.dtypes if ((dataType == \"double\") & (x != \"isFraud\")) ]\n#skip the \"isFraud\" but ","045f2357":"print(numCols)\nprint(catCols)","336d0090":"train.agg(F.countDistinct(\"type\")).show()","c04e98a5":"train.groupBy(\"type\").count().show()","67661c09":"from pyspark.ml.feature import (\n    OneHotEncoder,\n    StringIndexer,\n)","49b9e7bb":"#catCols are the cols with string\nstring_indexer = [\n    StringIndexer(inputCol=x, outputCol=x + \"_StringIndexer\", handleInvalid=\"skip\")\n    for x in catCols\n]","e93ae229":"string_indexe=string_indexer[0].fit(df).transform(df)\nstring_indexe.show()","3f40ce07":"one_hot_encoder = [\n    OneHotEncoder(\n        inputCols=[f\"{x}_StringIndexer\" for x in catCols],\n        outputCols=[f\"{x}_OneHotEncoder\" for x in catCols],\n    )\n]","3ae274da":"one_hot_encoder_df=one_hot_encoder[0].fit(string_indexe).transform(string_indexe)\none_hot_encoder_df.show()\n","c1dc04b1":"from pyspark.ml.feature import VectorAssembler","b3603b63":"assemblerInput = [x for x in numCols]\nassemblerInput += [f\"{x}_OneHotEncoder\" for x in catCols]","17ff8055":"assemblerInput","c5d53aa6":"vector_assembler = VectorAssembler(\n    inputCols=assemblerInput, outputCol=\"VectorAssembler_features\"\n)","333ed59b":"stages = []\nstages += string_indexer\nstages += one_hot_encoder\nstages += [vector_assembler]\n","377051a9":"stages","cbe994ac":"#%%time\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline().setStages(stages)\nmodel = pipeline.fit(train)\n\npp_df = model.transform(train)","c20d7070":"pp_df.select(\n    \"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"VectorAssembler_features\",\n).show(truncate=False)","e30e312e":"pp_df.show()","7c20bd78":"test.count()","a1ac0b1a":"df_test=test.where(test.isFraud == 1)","47b2e7ab":"df_test.show()","b03325a7":"from pyspark.ml.classification import LogisticRegression","1a144625":"data = pp_df.select(\n    F.col(\"VectorAssembler_features\").alias(\"features\"),\n    F.col(\"isFraud\").alias(\"label\"),\n)","57c11850":"data.show(5, truncate=False)","c19db3fd":"%%time\nmodel = LogisticRegression().fit(data)\ndata=model.transform(data)\n","632de94b":"data.show()","b1279fd2":"model = pipeline.fit(df_test)\n\npp_df_test = model.transform(df_test)","4a5d05c5":"data_test = pp_df_test.select(\n    F.col(\"VectorAssembler_features\").alias(\"features\"),\n    F.col(\"isFraud\").alias(\"label\"),\n)","ea324bac":"data_test.show(5, truncate=False)","e8d7f2b9":"model = LogisticRegression().fit(data_test)\ndata=model.transform(data_test)\ndata.show()","191686e1":"df.limit","c7976aa7":"model.summary.areaUnderROC","d8c29916":"model.summary.pr.show()","5cbe5f08":"## Data Dimensions ","2781ee05":"Dataset: https:\/\/www.kaggle.com\/ntnu-testimon\/paysim1\/data (Randomly sampled 10% of the dataset)","a8c57b9b":"these fancy is [ ] is just way to get the data in list format!! ie the output will be list!\n","e0ba4623":"## One-Hot Encoding\nOne-hot encoding is a concept every data scientist should know. I\u2019ve relied on it multiple times when dealing with missing values. It\u2019s a lifesaver!\n\nHere\u2019s the caveat \u2013 Spark\u2019s OneHotEncoder does not directly encode the categorical variable.\n> First, we need to use the String Indexer to convert the variable into numerical form and then use OneHotEncoderEstimator to encode multiple columns of the dataset.\n\nIt creates a Sparse Vector for each row.","0012a286":"**Preparing the test data by passing it to the pipeline**","1b5ed9a3":"## one-hot numeric array of the features","03a04c25":"For our instance ,we don't really need \" nameOrig \" , \" nameDest \" , \" oldbalanceDest \" , \" oldbalanceDest \" ,\" newbalanceDest \" ,we really don't need these as these are not the factors which affects the frud data or not.","4bacdcf2":"# fromat for [] is var =[ output function\/var for(loop)   ] then the output will stored as a list of item.\n","a74bd182":"Drop columns from the data :\n* In any machine learning project, we always have a few columns that are not required for solving the problem.\n\nfor instance, we can use the drop function to remove the column from the data. Use the asterisk (*) sign before the list to drop multiple columns from the dataset:\nmy_data = my_data.drop(*['Batsman', 'Bowler', 'Id'])\nmy_data.columns","218080c1":"### Train\/test split","12406259":"## Transformers and Estimators\nAs the name suggests, Transformers convert one dataframe into another either by updating the current values of a particular column (like converting categorical columns to numeric) or mapping it to some other values by using a defined logic.\n\nAn Estimator implements the fit() method on a dataframe and produces a model. For example, LogisticRegression is an Estimator that trains a classification model when we call the fit() method.\n\n\n","5a693f67":"so here is thr things we need to select data for train and test \n","7ce81c11":"we are only slecting some colum which is an alternative for droping a colum rather ,efficent.","b49561c6":"df = df.select(\"type\",\"isFraud\")\n#categorical value ","e90506d2":"## Encode Categorical Variables using PySpark\nMost machine learning algorithms accept the data only in numerical form. So, it is essential to convert any categorical variables present in our dataset into numbers.\n* String Indexing is similar to Label Encoding.","879054b6":"data weight is in fraction so \n0.7 ie 70% of the data from is being randomly selected and loaded but with a random factor seed ie 7","71061023":"What is One Hot Encoding? Why and When Do You Have to Use it?\n=> Encode categorical features as a one-hot numeric array. By default, the encoder derives the categories based on the unique values in each feature.\n\n## This is why we use one hot encoder to perform \u201cbinarization\u201d of the category and include it as a feature to train the model.\n\nhttps:\/\/hackernoon.com\/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n","fc34fa61":"random selection of data makes the good distribution of record ,leads to good model","20a5a6fd":"randomSplit:\nSeed is random number generator seed. This is important because you might want to be able to hard code the same seed for your tests so that you always get the same results in test, but in prod code replace it with current time in milliseconds or a random number from a good entropy source.\n\nBasically the splited in random way. ","8bcb2f55":"To use ML we need to conver string into values ,check the Data Type for that!","5c3a550e":"Here we seperated the numerical col and string col;","ef8e6a93":"## Now all the feature of record will be represted by the a single vector","fc43e93c":"> A pipeline allows us to maintain the data flow of all the relevant transformations that are required to reach the end result.","592f1fab":"here fun takes 2 parameters and  _StringIndexer will give the numeric value of of x as output when put like this : \nx + \"_StringIndexer\"","8aec729c":"### Vector assembling\n\nVectorAssembler:\nCombines the values of input columns into a single vector.\nhttp:\/\/spark.apache.org\/docs\/latest\/ml-features#vectorassembler\n\nIt accepts numeric, boolean and vector type columns\n","1cd9b66d":"### One hot encoding\n\nStringIndexer:\nConverts a single feature to an index feature.\nhttp:\/\/spark.apache.org\/docs\/latest\/ml-features#stringindexer\n\n\nOneHotEncoder:\nhttp:\/\/spark.apache.org\/docs\/latest\/ml-features#onehotencoder\n\nFor more info: http:\/\/spark.apache.org\/docs\/latest\/ml-features","0d96e505":"We need stages and pipeline so that we don't need to apply the indexer and encoder to to","9dc1dd92":"## Model Testing","0ae3f4a5":"### Logistic Regression","bce93028":"### Dtypes\nIn this dataset, any column of type string is treated as a categorical feature, but sometimes we might have numeric features we want treated as categorical or vice versa. We\u2019ll need to carefully identify which columns are numeric and which are categorical.","a7aa10cc":"## So we use it to conver string into numerical values.","58496793":"use agg when sum is indeed.\nF is sql function ie countDistinct\nDF goes along with sql as DF is in tabel format.\nSQL function maily applies in the record of the colum or the whole record itself.\n"}}