{"cell_type":{"9f11176b":"code","768d8032":"code","77276b52":"code","76175525":"code","5adc47b7":"code","c988a77d":"code","4d9362bb":"code","b95e0329":"code","23df0fee":"code","bb15ffa3":"code","d9fb556a":"code","14150864":"code","c820158a":"code","077ff11a":"code","ae818647":"code","7c38f6f9":"code","a3996793":"code","76424bda":"code","b1017397":"code","96a64f8c":"code","9f19d02a":"markdown","c083c935":"markdown","d7286cad":"markdown","35d144ad":"markdown","071c12b3":"markdown","9d55dcf7":"markdown","822b8ea5":"markdown","ddc5ea67":"markdown","0ee8a59b":"markdown","8e626dd1":"markdown"},"source":{"9f11176b":"# Import the required libraries\nimport cv2                         # opencv version 3.4.2\nimport numpy as np                 # numpy version 1.16.3\nimport matplotlib.pyplot as plt    # matplotlib version 3.0.3\n%matplotlib inline","768d8032":"# Load the source image\nsrc_img = cv2.imread('..\/input\/images\/images\/source.png')","77276b52":"# Function to display an image using matplotlib\ndef show_image(img, title, colorspace):\n    dpi = 96\n    figsize = (img.shape[1] \/ dpi, img.shape[0] \/ dpi)\n    fig, ax = plt.subplots(figsize = figsize, dpi = dpi)\n    if colorspace == 'RGB':\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), interpolation = 'spline16')\n    if colorspace == 'gray':\n        plt.imshow(img, cmap = 'gray')\n    plt.title(title, fontsize = 12)\n    ax.axis('off')\n    plt.show()    ","76175525":"# Display the source image\nshow_image(src_img, 'Source image containing different objects', 'RGB')","5adc47b7":"# Change colorspace from BGR to HSV\nsrc_img_hsv = cv2.cvtColor(src_img, cv2.COLOR_BGR2HSV)\n\n# Define limits of yellow HSV values\nyellow_lower = np.array([16, 100, 100])\nyellow_upper = np.array([45, 255, 255])\n\n# Filter the image and get the mask\nmask = cv2.inRange(src_img_hsv, yellow_lower, yellow_upper)\n\nshow_image(mask, 'Yellow color filter mask', 'gray')","c988a77d":"# Remove white noise\nkernel = np.ones((5, 5), np.uint8)\nopening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n\nshow_image(opening, 'Morphological opening', 'gray')","4d9362bb":"# Remove small black dots\nclosing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n\nshow_image(closing, 'Morphological closing', 'gray')","b95e0329":"# Get back the fine boundary edges using dilation\nkernel1 = np.ones((2, 2), np.uint8)\ndilation = cv2.dilate(closing, kernel1, iterations = 1)\n\nshow_image(dilation, 'Morphological dilation', 'gray')","23df0fee":"contours, _ = cv2.findContours(dilation, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n# There are 2 contours: outer one is the rectangle(ish) and inner one is the circle(ish)\n# Get the outer contour (it has larger area than the inner contour)\nc1 = max(contours, key = cv2.contourArea)\n\n# Define the bounding rectangle around the contour\nrect = cv2.minAreaRect(c1)\n\n# Get the 4 corner coordinates of the rectangle\nbox = cv2.boxPoints(rect)\nbox = np.int0(box)\n\n# Draw the bounding rectangle to show the marked object\ntemp_img = src_img.copy()\nbdg_rect = cv2.drawContours(temp_img, [box], 0, (0, 0, 255), 2)\n\nshow_image(bdg_rect, 'Marked object to be extracted', 'RGB')","bb15ffa3":"# cv2.boxPoints(rect) returns the coordinates (x, y) as the following list:\n# [[bottom right], [bottom left], [top left], [top right]]\n\nwidth = box[0][0] - box[1][0]\nheight = box[1][1] - box[2][1]\n\nsrc_pts = box.astype('float32')\ndst_pts = np.array([[width, height],\n                    [0, height],\n                    [0, 0],\n                    [width, 0]], dtype = 'float32')\n\n# Get the transformation matrix\nM = cv2.getPerspectiveTransform(src_pts, dst_pts)\n\n# Apply the perspective transformation\nwarped = cv2.warpPerspective(src_img, M, (width, height))\n\n# Save it as the query image\nquery_img = warped","d9fb556a":"# Display the query image\nshow_image(query_img, 'Query image', 'RGB')","14150864":"# Create an ORB object\norb = cv2.ORB_create()\n\n# Detect and visualize the features\nfeatures = orb.detect(query_img, None)\nf_img = cv2.drawKeypoints(query_img, features, None, color = (0, 255, 0), flags = 0)\n\nshow_image(f_img, 'Detected features', 'RGB')","c820158a":"# Function to match features and find the object\ndef match_feature_find_object(query_img, train_img, min_matches): \n    # Create an ORB object\n    orb = cv2.ORB_create(nfeatures=100000)\n    \n    features1, des1 = orb.detectAndCompute(query_img, None)\n    features2, des2 = orb.detectAndCompute(train_img, None)\n\n    # Create Brute-Force matcher object\n    bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n    matches = bf.knnMatch(des1, des2, k = 2)\n    \n    # Nearest neighbour ratio test to find good matches\n    good = []    \n    good_without_lists = []    \n    matches = [match for match in matches if len(match) == 2] \n    for m, n in matches:\n        if m.distance < 0.8 * n.distance:\n            good.append([m])\n            good_without_lists.append(m)\n         \n    if len(good) >= min_matches:\n        # Draw a polygon around the recognized object\n        src_pts = np.float32([features1[m.queryIdx].pt for m in good_without_lists]).reshape(-1, 1, 2)\n        dst_pts = np.float32([features2[m.trainIdx].pt for m in good_without_lists]).reshape(-1, 1, 2)\n        \n        # Get the transformation matrix\n        M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n               \n        # Find the perspective transformation to get the corresponding points\n        h, w = query_img.shape[:2]\n        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n        dst = cv2.perspectiveTransform(pts, M)\n        \n        train_img = cv2.polylines(train_img, [np.int32(dst)], True, (0, 255, 0), 2, cv2.LINE_AA)\n    else:\n        print('Not enough good matches are found - {}\/{}'.format(len(good), min_matches))\n            \n    result_img = cv2.drawMatchesKnn(query_img, features1, train_img, features2, good, None, flags = 2)\n    \n    show_image(result_img, 'Feature matching and object recognition', 'RGB')","077ff11a":"train_img = cv2.imread('..\/input\/images\/images\/train1.png')\nmatch_feature_find_object(query_img, train_img, 10)","ae818647":"train_img = cv2.imread('..\/input\/images\/images\/train2.png')\nmatch_feature_find_object(query_img, train_img, 10)","7c38f6f9":"train_img = cv2.imread('..\/input\/images\/images\/train3.jpg')\nmatch_feature_find_object(query_img, train_img, 10)","a3996793":"train_img = cv2.imread('..\/input\/images\/images\/train4.png')\nmatch_feature_find_object(query_img, train_img, 10)","76424bda":"train_img = cv2.imread('..\/input\/images\/images\/train5.jpg')\nmatch_feature_find_object(query_img, train_img, 10)","b1017397":"# Image without the matchbox\ntrain_img = cv2.imread('..\/input\/images\/images\/train6.jpg')\nmatch_feature_find_object(query_img, train_img, 10)","96a64f8c":"# Image without the matchbox\ntrain_img = cv2.imread('..\/input\/images\/images\/train7.jpg')\nmatch_feature_find_object(query_img, train_img, 10)","9f19d02a":"## 2. Feature detection and matching","c083c935":"There are small white noises in the image which are removed by morphological opening. Morphological closing will be used to remove the small black dots in the object.","d7286cad":"## 3. Result\n\nThe result on several images with different setup is shown below. The identified *matchbox* is marked in green color in the train images. The matched features are also shown.\n\nNote: When the the front face of the *matchbox* is not coplanar with the camera's view in the train image, then some corners of the green bounding box are slightly shifted. Higher the non-coplanarity, higher is the deviation. This is due to the requirement of cv2.findHomography() that the source and destination points should be coplanar.","35d144ad":"In the above image, the missing circle in the center is due to the presence of non-yellow concentric circles on the *matchbox*. To get the complete *matchbox*, the property of contours is exploited as shown below.","071c12b3":"The yellow *matchbox* will be extracted from the above image and its extracted features will be used to recognize this *matchbox* in other images.\n\nThere are several approaches to extract an object and it highly depends on the given image. In this case, the matchbox can be uniquely identified by its color and consequently we will use color filtering to accomplish that task. HSV (Hue, Saturation, Value) is better suited than BGR (Blue, Green, Red) values to filter an image based on color.","9d55dcf7":"The *matchbox* is marked in red in the above image. It is extracted by applying geometric transformations on the image and the result is saved as the query image.","822b8ea5":"# Object recognition using feature matching\n\nThe objective of this notebook is to use feature matching to recognize a known object in any given image by applying various image processing techniques using OpenCV.\n\nA feature is a place (at the pixel level) in an image that can be mathematically identified in another image(s). In other words, it is a specific pixel pattern which is unique to an image or an object in an image. A known object can be recognized by finding and matching its features in any given image.","ddc5ea67":"To uniquely identify the *matchbox* in any given image (train image):\n* detect and describe its features\n* detect and describe the features in the train image\n* match the features in both images based on their descriptors\n* extract good matches based on the nearest neighbour ratio\n* get the locations of matched features in both the images and apply perspective transformation to find the *matchbox*","0ee8a59b":"The visualization below shows the detected features of the *matchbox*.","8e626dd1":"## 1. Image processing and object extraction\n\nIn this section an object is extracted from the given image (source image), so as to detect its features."}}