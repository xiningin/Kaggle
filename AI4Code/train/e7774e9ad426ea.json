{"cell_type":{"3eeea3b3":"code","087dc2d2":"code","f9c1a240":"code","bc513bc7":"code","edbe1bc1":"code","1139d8e9":"code","cafbad75":"code","06780b1f":"code","b956af0f":"code","0c4bffe7":"code","c7e09157":"code","dd60e6a7":"code","7ba1fb02":"code","33fb2c55":"code","42f8ed51":"code","057c59d5":"code","ab1ae10b":"code","9052bd7b":"code","d27effa3":"code","a654ca11":"code","0b959e34":"code","85064a64":"code","ce1a9cc8":"code","f12bcae7":"code","14676c0f":"code","b75371b9":"code","43de6edc":"code","b1bc8df3":"code","db1c32d3":"code","109c3907":"code","8d504e0c":"code","ca798369":"code","88f0dac7":"code","88f89fa7":"code","71a558b1":"code","883b91c9":"code","b47893be":"code","021326ae":"code","8b7e551e":"code","83833485":"markdown","7f07918a":"markdown","3dd3541f":"markdown","2499f4f2":"markdown","c88e8c53":"markdown","ae600a32":"markdown","6c21c593":"markdown","e5a653b2":"markdown","14593803":"markdown","cf35c5b1":"markdown","6a874bd4":"markdown","3d7c01b8":"markdown","2f2095c9":"markdown","72653ef5":"markdown","1f294240":"markdown","b6745221":"markdown","f14e3e86":"markdown","14268301":"markdown","8de1cb2a":"markdown","2b2cb9cc":"markdown","d617681d":"markdown","792b1203":"markdown","51381f0d":"markdown","d36f05b8":"markdown","1e917955":"markdown","d81b71c4":"markdown","915a25ca":"markdown","473beeab":"markdown","34dc7e37":"markdown","f16cc68c":"markdown","4e5759e7":"markdown","c16b4578":"markdown","cbc248ae":"markdown","0aa0f7da":"markdown","5016fc71":"markdown"},"source":{"3eeea3b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# Extra Libs\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, precision_score, f1_score, auc, roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Default visual settings\n\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams['figure.dpi'] = 150\nbackground_color='#F5F4EF'\n\n\n#sns.palplot([\"#fbfbfb\",\"#f5f4ef\", \"#4b4b4c\",\"#8abbd0\",\"#0e4f66\",\"#244747\"])\n\n# Ignore warnings\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Print colored text \n# https:\/\/stackoverflow.com\/questions\/287871\/how-to-print-colored-text-to-the-terminal\n# Includes other color options\n\nHEADER = '\\033[95m'\nOKBLUE = '\\033[94m'\nOKCYAN = '\\033[96m'\nOKGREEN = '\\033[92m'\nWARNING = '\\033[93m'\nFAIL = '\\033[91m'\nENDC = '\\033[0m'\nBOLD = '\\033[1m'\nUNDERLINE = '\\033[4m'\n\nclass color:\n    S = BOLD + OKBLUE +  UNDERLINE   #S = Start\n    E = ENDC #E = End\n\nprint(color.S+\"Libraries loaded\"+color.E)","087dc2d2":"df = pd.read_csv('\/kaggle\/input\/water-potability\/water_potability.csv')\n\n# Show first n rows of data, default is 5\ndf.head(3).style.set_caption('Water Quality Dataset').hide_index().format(None,na_rep=\"Not Provided\")\n\n# Our 'y' (what we want to predict) is the 'Potability' column","f9c1a240":"def overview(df):\n    print(color.S+'DataFrame Overview\\n'+color.E)\n    print('DataFrame Shape: \\n',df.shape[0], 'Rows\\n',df.shape[1],' Columns\\n')\n    print('Column Headers: \\n',list(df.columns),'\\n')\n    print('Missing Values: \\n',df.isnull().sum()\/len(df))\n    print('\\nData Types:\\n',df.dtypes)","bc513bc7":"overview(df)","edbe1bc1":"# We can also view if there is a diff in missing values based on our target variable\n#print(\"Missing Values by Target Variable Class\\n\")\n#print(\"Potability == 1\\n\")\n#print(df.query(\"Potability == 1\").isnull().sum()\/len(df.query(\"Potability == 1\")))\n\n#print(\"\\nPotability == 0\\n\")\n#print(df.query(\"Potability == 0\").isnull().sum()\/len(df.query(\"Potability == 0\")))","1139d8e9":"# Isolate columns that req changing\nnull_cols = []\nfor col in df.columns:\n    if df[col].isnull().sum() > 0:\n        print(col)\n        null_cols.append(col)\n        \n# Assigned to list so can check later","cafbad75":"# Separate our variables from our y\nvariables = df.columns[:-1]\nvariables","06780b1f":"# Data\ntarget_split = pd.DataFrame(round((df.groupby(['Potability'])['Potability'].count())\/len(df),2)).T\n\n# Plot\nfig, ax = plt.subplots(1,1, figsize=(6,1.25), facecolor=background_color)\n\nax.barh(target_split.index, target_split[0], color='#244747')\nax.barh(target_split.index, target_split[1], left=target_split[0], color='#8abbd0')\n\nax.set(facecolor=background_color, xlim=[0,1], xticks=[], yticks=[])\n\nfor s in [\"top\",\"right\",\"left\",\"bottom\"]:\n            ax.spines[s].set_visible(False)\n\n# Annotations\nfor i in target_split.index:\n    ax.annotate(f\"{int(target_split[0][i]*100)}%\", xy=(target_split[0][i]\/2, i),\n                   va = 'center', ha='center',fontsize=28,color='white')\n    ax.annotate(f\"{int(target_split[1][i]*100)}%\", xy=(target_split[0][i]+target_split[1][i]\/2, i),\n                   va = 'center', ha='center',fontsize=28, color='white')\n    \n    ax.annotate(\"Non-Potable\", xy=(target_split[0][i]\/2, -0.22),\n                   va = 'center', ha='center',fontsize=8, color='white')\n    \n    ax.annotate(\"Potable\", xy=(target_split[0][i]+target_split[1][i]\/2, -0.22),\n                   va = 'center', ha='center',fontsize=8, color='white')\n\nax.text(0,0.5, \"Target Variable Split\", fontsize=14, fontweight='bold')\n\nplt.show()","b956af0f":"fig = plt.figure(figsize=(16, 2), facecolor=background_color)\n\ngs = fig.add_gridspec(1, len(df.columns)-1)\ngs.update(wspace=0.2, hspace=0.4)\n\n# Build axes for plots\n\nplot = 0\nfor row in range(0, 1):\n    for col in range(0, len(df.columns)-1):\n        locals()[\"ax\"+str(plot)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(plot)].set_facecolor(background_color)\n        locals()[\"ax\"+str(plot)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(plot)].get_yaxis().set_visible(False)\n        locals()[\"ax\"+str(plot)].set_axisbelow(True)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(plot)].spines[s].set_visible(False)\n        plot += 1\n\n# Plots\n\nplot = 0\n\nfor variable in df.columns[:-1]:\n        sns.kdeplot(df.query(\"Potability == 0\")[variable],ax=locals()[\"ax\"+str(plot)], color='#244747', shade=True, ec='black',linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        sns.kdeplot(df.query(\"Potability == 1\")[variable], ax=locals()[\"ax\"+str(plot)], color='#8abbd0',ec='black', shade=True, linewidth=1.5, alpha=0.6, zorder=3, legend=False)\n        locals()[\"ax\"+str(plot)].grid(which='major', axis='x', zorder=5, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(plot)].set_xlabel(variable)\n        plot += 1\n               \nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\n\nax0.text(Xstart, Yend+(Yend*0.15), 'Variable Distribution by Potability', fontsize=24, fontweight='bold')\n\nplt.show()","0c4bffe7":"def n_choose_k_unordered_combinations(n,k):\n    \n    # n is how many there are in total, and k is how many you'd like to chose\n    # returns number of unique combinations where order does not matter\n\n    import math\n    print(color.S+\"Unordered Combinations\"+color.E)\n    print(int(math.factorial(n)\/(math.factorial(k)*(math.factorial(n-k)))),f\"ways to choose {k} items from {n}\")\n    \n\nn_choose_k_unordered_combinations(9,2)","c7e09157":"import itertools\ncombinations = list(itertools.combinations(variables, 2))\nprint(len(combinations))\ncombinations","dd60e6a7":"fig = plt.figure(figsize=(16, 16), facecolor=background_color)\n\nplot = 0 \n\ncombinations = list(itertools.combinations(variables, 2))\n\nfor item in combinations:\n            \n        plot += 1\n        \n        plt.subplot(6 , 6, plot, facecolor=background_color)\n        plt.subplots_adjust(hspace=0.5, wspace=0.5)\n        \n        sns.scatterplot(x=item[0] ,y=item[1] , data=df, hue=df['Potability'], palette=['#244747','#8abbd0'], ec='black', lw=1.5, legend=False)\n        sns.despine()\n        \n        plt.ylabel(item[1], fontsize=8)\n        plt.xlabel(item[0], fontsize=8)\n        plt.yticks([])\n        plt.xticks([])        \n        \nfig.text(0.12,0.92,'Variable Relationships split by Potability', fontsize=24, fontweight='bold')\nplt.show()","7ba1fb02":"fig = plt.figure(figsize=(16, 16), facecolor=background_color)\n\nplot = 0 \n\nfor x in variables:\n    for y in variables[::-1]:\n        if x==y: continue\n            \n        plot += 1\n        \n        plt.subplot(9 , 9, plot, facecolor=background_color)\n        plt.subplots_adjust(hspace=1, wspace=1)\n        \n        sns.scatterplot(x=x ,y=y , data=df, hue=df['Potability'], palette=['#244747','#8abbd0'], ec='black', lw=1.5, legend=False)\n        sns.despine()\n        \n        plt.ylabel(y, fontsize=8)\n        plt.xlabel(x, fontsize=8)\n        plt.yticks([])\n        plt.xticks([])\n        \nfig.text(0.12,0.92,'Variable Relationships split by Potability', fontsize=24, fontweight='bold')\nplt.show()","33fb2c55":"fig = plt.figure(figsize=(8,4), facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.5, hspace=0.27)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\n\nsns.scatterplot(data=df, x='ph', y='Sulfate', hue='Potability',palette=['#244747','#8abbd0'], ec='black', lw=1.5, ax=ax0)\nsns.scatterplot(data=df, x='Solids', y='Sulfate', hue='Potability',palette=['#244747','#8abbd0'], ec='black', lw=1.5, legend=False, ax=ax1)\n     \nfor i in range(0, 2):\n        locals()[\"ax\"+str(i)].tick_params(axis='both', which='both', length=0)\n        locals()[\"ax\"+str(i)].set_facecolor(background_color)\n        locals()[\"ax\"+str(i)].grid(which='both', axis='both', zorder=5, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(i)].set_axisbelow(True)\n        for s in [\"top\",\"right\",\"left\",\"bottom\"]:\n            locals()[\"ax\"+str(i)].spines[s].set_visible(False)\n\nax1.set_ylabel('') # Turned off y label for second plot as is the same as the first\n\ndef x_formatter(x, pos):\n    \"\"\"The two args are the value and tick position\"\"\"\n    if x <= 0:\n        label = '0'\n\n    elif x >= 1e6:\n        label = '{:1.1f}M'.format(x*1e-6)\n        \n    else:\n        label = '{:1.0f}k'.format(x*1e-3)\n    return label\n\nax1.xaxis.set_major_formatter(x_formatter)\n\n# Legend\n\nL = ax0.legend(frameon=False,loc=\"upper center\", bbox_to_anchor=(0.45, 1.18), ncol= 2)\nplt.setp(L.texts, family='serif', size=9) \nL.get_frame().set_facecolor('none')\nL.get_texts()[0].set_text('Non-Potable')\nL.get_texts()[1].set_text('Potable')\n\nax0.text(-0.2,575,'Interesting Variable Relationships', fontsize=18, fontweight='bold')\n\nplt.show()","42f8ed51":"# Custom color map\ncolors = [\"#244747\",\"#f5f4ef\", \"#4b4b4c\",\"#8abbd0\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nfig = plt.figure(figsize=(10,10), facecolor=background_color)\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace=0.5, hspace=0.75)\nax0 = fig.add_subplot(gs[0,:])\nax1 = fig.add_subplot(gs[1,:])\n\n# Hides top right of heatmap\n#mask = np.triu(np.ones_like(corr, dtype=np.bool))\n#np.tril(corr) for hiding the lower left corner instead\n\ncorr_1 = df.query(\"Potability == 1\")[variables].corr()\ncorr_0 = df.query(\"Potability == 0\")[variables].corr()\n\n# Heatmap\nsns.heatmap(corr_1, linewidth=3, annot=True, fmt='.1%', cbar=False, cmap=colormap, vmin=-0.1, vmax=0.1, ax=ax0)\nsns.heatmap(corr_0, linewidth=3, annot=True, fmt='.1%', cbar=False, cmap=colormap, vmin=-0.1, vmax=0.1, ax=ax1)\n\nfor i in range(0, 2):\n        locals()[\"ax\"+str(i)].tick_params(axis='both', which='both', length=0)\n        locals()[\"ax\"+str(i)].set_facecolor(background_color)\n# Text\nax0.text(0,-3,'Variable Correlation: Heatmap',fontsize=24, fontweight='bold')\nax0.text(0,-1.5,'When Samples are Potable',fontsize=15, fontweight='bold')\n\nax1.text(0,-1.5,'When Samples are Non-Potable',fontsize=15, fontweight='bold')\n\n\nplt.show()","057c59d5":"fig = plt.figure(figsize=(16, 2), facecolor=background_color)\n\ngs = fig.add_gridspec(1, len(df.columns)-1)\ngs.update(wspace=0.75, hspace=0.4)\n\n# Build axes for plots\n\nplot = 0\nfor row in range(0, 1):\n    for col in range(0, len(df.columns)-1):\n        locals()[\"ax\"+str(plot)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(plot)].set_facecolor(background_color)\n        locals()[\"ax\"+str(plot)].tick_params(axis='y', left=False)\n        #locals()[\"ax\"+str(plot)].get_yaxis().set_visible(False)\n        locals()[\"ax\"+str(plot)].set_axisbelow(True)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(plot)].spines[s].set_visible(False)\n        plot += 1\n\n# Plots\n\nplot = 0\n\nfor variable in df.columns[:-1]:\n        sns.boxplot(data=df, y=df[variable],ax=locals()[\"ax\"+str(plot)], color='#244747')\n        locals()[\"ax\"+str(plot)].grid(which='both', axis='both', zorder=5, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(plot)].set_xlabel(variable)\n        locals()[\"ax\"+str(plot)].set_ylabel('')\n\n        plot += 1\n        \n        \nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\nax0.text(Xstart, Yend+(Yend*0.15), 'Variable Distribution: Outlier Detection', fontsize=24, fontweight='bold')\n\nplt.show()","ab1ae10b":"fig = plt.figure(figsize=(16, 2), facecolor=background_color)\n\ngs = fig.add_gridspec(1, len(df.columns)-1)\ngs.update(wspace=0.75, hspace=0.4)\n\n# Build axes for plots\n\nplot = 0\nfor row in range(0, 1):\n    for col in range(0, len(df.columns)-1):\n        locals()[\"ax\"+str(plot)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(plot)].set_facecolor(background_color)\n        locals()[\"ax\"+str(plot)].tick_params(axis='y', left=False)\n        #locals()[\"ax\"+str(plot)].get_yaxis().set_visible(False)\n        locals()[\"ax\"+str(plot)].set_axisbelow(True)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(plot)].spines[s].set_visible(False)\n        plot += 1\n\n# Plots\n\nplot = 0\n\nfor variable in df.columns[:-1]:\n        sns.boxenplot(data=df, y=df[variable],ax=locals()[\"ax\"+str(plot)], color='#244747')\n        locals()[\"ax\"+str(plot)].grid(which='both', axis='both', zorder=5, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(plot)].set_xlabel(variable)\n        locals()[\"ax\"+str(plot)].set_ylabel('')\n\n        plot += 1\n        \n        \nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\nax0.text(Xstart, Yend+(Yend*0.15), 'Variable Distribution: Outlier Detection', fontsize=24, fontweight='bold')\n\nplt.show()","9052bd7b":"y = df['Potability']\nX = df.drop(['Potability'], axis=1)\n\n# Split our data\nX_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","d27effa3":"for i in null_cols:\n    print(i, \"Train \",X_train[i].mean())\n    print(i, \"Test \",X_test[i].mean())","a654ca11":"for dataframe in [X_train, X_test]:\n    for column in variables:\n        column_value=X_train[column].mean()\n        dataframe[column].fillna(column_value, inplace=True)   \n        \n# Note, all of our columns are 'float64' so we can impute as above\n# If we had Categorical features, e.g. 'Object' data types, we could impute the 'Mode' (most common value)\n# For example:\n# for dataframe in [X_train, X_test]:\n#    dataframe[catergorical_variable].fillna(X_train[catergorical_variable].mode()[0], inplace=True)","0b959e34":"print(color.S+\"Null Values:\"+color.E)\nfor dataframe in [X_train, X_test]:\n    print(dataframe.isnull().sum().sum())","85064a64":"min_max_scaler = MinMaxScaler()\nX_train_min_max_scaled = min_max_scaler.fit_transform(X_train)\nX_test_min_max_scaled = min_max_scaler.transform(X_test)\n\nstandard_scaler = StandardScaler()\nX_train_stand_scaled = standard_scaler.fit_transform(X_train)\nX_test_stand_scaled = standard_scaler.transform(X_test)","ce1a9cc8":"X_train_min_max_scaled = pd.DataFrame(X_train_min_max_scaled, columns=variables)\nX_test_min_max_scaled = pd.DataFrame(X_test_min_max_scaled, columns=variables)\n\nX_train_stand_scaled = pd.DataFrame(X_train_stand_scaled, columns=variables)\nX_test_stand_scaled = pd.DataFrame(X_test_stand_scaled, columns=variables)","f12bcae7":"X_train_stand_scaled.head(3).style.set_caption('X Train: Standard Scaling')","14676c0f":"X_train_min_max_scaled.head(3).style.set_caption('X Train: Min-Max Scaling')","b75371b9":"print(color.S+\"Null Accuracy Score:\"+color.E,round(y_test.value_counts()[0]\/(y_test.value_counts().sum()),4))","43de6edc":"logistic_regression = LogisticRegression(random_state=0)\nlogistic_regression.fit(X_train_min_max_scaled, y_train)\nlog_pred_min_max = logistic_regression.predict(X_test_min_max_scaled)\n\nprint(color.S+'Model accuracy score:'+color.E,' {0:0.4f}'. format(accuracy_score(y_test, log_pred_min_max)))\nprint(color.S+'Confusion Matrix:'+color.E,'\\n',confusion_matrix(y_test, log_pred_min_max))","b1bc8df3":"np.unique(log_pred_min_max)","db1c32d3":"models = {\n    \"Logistic Regression\": LogisticRegression(random_state=0),\n    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n    \"Random Forest\": RandomForestClassifier(random_state=0),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n    \"K-Nearest Neighbours\": KNeighborsClassifier(n_neighbors=5),\n    \"SVM\": SVC()\n}","109c3907":"model_df = []\naccuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nfor model, model_launch in models.items():\n    \n    # Loop through listed models\n    model_initiated = model_launch\n    model_initiated.fit(X_train, y_train)\n    predictions = model_initiated.predict(X_test)\n    \n    # Model Names\n    model_df.append(model)\n    \n    # Model Scores\n    accuracy.append(accuracy_score(y_test, predictions))\n    precision.append(precision_score(y_test, predictions))\n    recall.append(recall_score(y_test, predictions))\n    f1.append(f1_score(y_test, predictions))\n    \n    # Model Names & Scores in to a DataFrame\n    \n    scores_df = pd.DataFrame({'Model':model_df,\n                              'Accuracy': accuracy, \n                              'Precision': precision,\n                              'Recall': recall,\n                              'F1':f1}).sort_values(by='Accuracy', ascending=False)\n    \n    \n# Stylised DataFrame to help quickly grasp top performing models across metrics\nscores_df\n\n(scores_df.style\n  .background_gradient(cmap=colormap, subset=['Accuracy','Precision','Recall','F1'])\n  .highlight_max(subset=['Accuracy','Precision','Recall','F1'], color='gold')\n  .set_caption('Model Performance with Unscaled Data')\n  .format({'Accuracy': \"{:.2%}\",\n           'Precision': \"{:.2%}\",\n           'Recall': \"{:.2%}\",\n           'F1': \"{:.2%}\"\n          }))","8d504e0c":"model_df = []\naccuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nfor model, model_launch in models.items():\n    \n    # Loop through listed models\n    model_initiated = model_launch\n    model_initiated.fit(X_train_min_max_scaled, y_train)\n    predictions = model_initiated.predict(X_test_min_max_scaled)\n    \n    # Model Names\n    model_df.append(model)\n    \n    # Model Scores\n    accuracy.append(accuracy_score(y_test, predictions))\n    precision.append(precision_score(y_test, predictions))\n    recall.append(recall_score(y_test, predictions))\n    f1.append(f1_score(y_test, predictions))\n    \n    # Model Names & Scores in to a DataFrame\n    \n    scores_df_min_max = pd.DataFrame({'Model':model_df,\n                              'Accuracy': accuracy, \n                              'Precision': precision,\n                              'Recall': recall,\n                              'F1':f1}).sort_values(by='Accuracy', ascending=False)\n    \n    \n    \n# Stylised DataFrame to help quickly grasp top performing models across metrics\nscores_df_min_max\n\n(scores_df_min_max.style\n  .background_gradient(cmap=colormap, subset=['Accuracy','Precision','Recall','F1'])\n  .highlight_max(subset=['Accuracy','Precision','Recall','F1'], color='gold')\n  .set_caption('Model Performance with Min-Max Scaled X Data')\n  .format({'Accuracy': \"{:.2%}\",\n           'Precision': \"{:.2%}\",\n           'Recall': \"{:.2%}\",\n           'F1': \"{:.2%}\"\n          }))","ca798369":"model_df = []\naccuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nfor model, model_launch in models.items():\n    \n    # Loop through listed models\n    model_initiated = model_launch\n    model_initiated.fit(X_train_stand_scaled, y_train)\n    predictions = model_initiated.predict(X_test_stand_scaled)\n    \n    # Model Names\n    model_df.append(model)\n    \n    # Model Scores\n    accuracy.append(accuracy_score(y_test, predictions))\n    precision.append(precision_score(y_test, predictions))\n    recall.append(recall_score(y_test, predictions))\n    f1.append(f1_score(y_test, predictions))\n    \n    # Model Names & Scores in to a DataFrame\n    \n    scores_df_stand = pd.DataFrame({'Model':model_df,\n                              'Accuracy': accuracy, \n                              'Precision': precision,\n                              'Recall': recall,\n                              'F1':f1}).sort_values(by='Accuracy', ascending=False)\n    \n    \n    \n# Stylised DataFrame to help quickly grasp top performing models across metrics\nscores_df_stand\n\n(scores_df_stand.style\n  .background_gradient(cmap=colormap, subset=['Accuracy','Precision','Recall','F1'])\n  .highlight_max(subset=['Accuracy','Precision','Recall','F1'], color='gold')\n  .set_caption('Model Performance with Standard Scaled X Data')\n  .format({'Accuracy': \"{:.2%}\",\n           'Precision': \"{:.2%}\",\n           'Recall': \"{:.2%}\",\n           'F1': \"{:.2%}\"\n          }))\n\n","88f0dac7":"print(color.S+'Average Scores by X Train Scaling'+color.E)\nfor i in [scores_df, scores_df_min_max, scores_df_stand]:\n    print(i.mean(),'\\n')","88f89fa7":"(scores_df_stand.style\n  .background_gradient(cmap=colormap, subset=['Accuracy','Precision','Recall','F1'])\n  .highlight_max(subset=['Accuracy','Precision','Recall','F1'], color='gold')\n  .set_caption('Model Performance with Standard Scaled X Data')\n  .format({'Accuracy': \"{:.2%}\",\n           'Precision': \"{:.2%}\",\n           'Recall': \"{:.2%}\",\n           'F1': \"{:.2%}\"\n          }))","71a558b1":"SVM = SVC(probability=True)\nRF = RandomForestClassifier(random_state=0)\nDecision_Tree = DecisionTreeClassifier(random_state=0)\nKNN = KNeighborsClassifier(n_neighbors=5)","883b91c9":"accuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nVoting_Classifier = VotingClassifier(estimators=[('SVM',SVM),('RF',RF), ('Decision Tree',Decision_Tree),('KNN',KNN)],\n                                     voting='soft',n_jobs=-1)\n\nVoting_Classifier = Voting_Classifier.fit(X_train_stand_scaled, y_train)\n\npredictions = Voting_Classifier.predict(X_test_stand_scaled)\n\n\n# Model Scores\naccuracy.append(accuracy_score(y_test, predictions))\nprecision.append(precision_score(y_test, predictions))\nrecall.append(recall_score(y_test, predictions))\nf1.append(f1_score(y_test, predictions))\n\nVoting_Class_df= pd.DataFrame({'Model':'Voting Classifier',\n                          'Accuracy': accuracy, \n                          'Precision': precision,\n                          'Recall': recall,\n                          'F1':f1}).sort_values(by='Accuracy', ascending=False)","b47893be":"(Voting_Class_df.set_index('Model').style\n  .background_gradient(cmap=colormap, subset=['Accuracy','Precision','Recall','F1'])\n  .set_caption('Voting Classifier Performance')\n  .format({'Accuracy': \"{:.2%}\",\n           'Precision': \"{:.2%}\",\n           'Recall': \"{:.2%}\",\n           'F1': \"{:.2%}\"\n          }))","021326ae":"(Voting_Class_df.append(scores_df_stand).set_index('Model').sort_values(by='Accuracy', ascending=False).style\n.background_gradient(cmap=colormap, subset=['Accuracy','Precision','Recall','F1'])\n.highlight_max(subset=['Accuracy','Precision','Recall','F1'], color='gold')\n.set_caption('Model Performance Overview')\n.format({'Accuracy': \"{:.2%}\",\n       'Precision': \"{:.2%}\",\n       'Recall': \"{:.2%}\",\n       'F1': \"{:.2%}\"\n      }))","8b7e551e":"fig, ax = plt.subplots(1,1, figsize=(4,3), facecolor=background_color)\n\nconfusion = confusion_matrix(y_test, predictions)\n    \ngroup_names = ['True Negative','False Positive','False Negative','True Positive']\ngroup_counts = ['{0:0.0f}'.format(value) for value in confusion.flatten()]\ngroup_percentages = ['{0:.1%}'.format(value) for value in confusion.flatten()\/np.sum(confusion)]\nlabels = [f'{v1}\\n\\n{v2}\\n\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nsns.heatmap(confusion, annot=labels, fmt='', cmap=colormap, linewidth=3, cbar=False,\n           xticklabels=['Predicted Negative','Predicted Positive'],\n           yticklabels=['Actual Negative', 'Actual Positive'], ax=ax)\n\nax.tick_params(which='both', axis='both', length=0)\nplt.yticks(rotation=0) \n\nax.text(-0.8,-0.3,'Voting Classifer: Confusion Matrix',fontsize=15, fontweight='bold')\n\nplt.show()","83833485":"# **Model Improvement**\n\nThe next steps in this notebook are to fine tune the models. This will likely improve our scores.\n\nI could possibily look to remove outliers in the dataset, too.\n\nAlso, it might be worth trialling SMOTE to resmaple our data.\n\n***Work in progress***","7f07918a":"Let's see how the Voting Classifier performed. Note that I used the \"Soft\" vote rather than the \"Hard\" vote.","3dd3541f":"# **Next Tasks** \n\n**Exploratory Data Analysis**\n\nThis will help us to understand the data. Are there interesting relationships between variables? Do some variables appear to be a good indiciator for what our target variable will be? \n\n**Next, let's split our data into Training & Test sets**\n\nAn essential step for any predictive Machine Learning task\n\n**Deal with Null Values**\n\nWe'll deal with them in a way that will *not cause data leakage*\n\n","2499f4f2":"# Data Checks & Cleaning\n\nTypically before starting any project it is wise to **view & assess the datasets** you are provided. Today is no different.\n\nA checklist of areas I like to initially view are:\n\n* Data shape, i.e. Number of Rows & Columns\n* Data types, i.e. Are all columns integers? Or text? etc.\n* Missing data, i.e. Null Values\n* Column names\n","c88e8c53":"The Voting Classifer performed well. With the highest F1 score so far.\n\nWe can see where the classifier went wrong (and right) by using a confusion matrix","ae600a32":"# **Splitting our data**","6c21c593":"**Model Performance Reminder**\n\nSeveral models performed relatively well, with 3 outperforming the Null Accuracy score.\n\nThe most accurate models are: SVM, Random Forest, and Gradient Boosting.\n\nHowever, KNN had the highest F1 score, so this can't be dismissed.","e5a653b2":"**What does this tell us?**\n\nThere do not seem to be clear differences between the variables when looking for Potability status.\n\nThis might suggest that no one feature on it's own can be used as a powerful indicator, though we will continue to investigate this throughout the notebook.\n\nI'll plot all of our 9 variables against one another to see if we can view anything en masse that could be worth exploring.\n\nTo do so, let's figure out how many unique pairs we'll have to plot (order does not matte, e.g. ph & hardness is the same as hardness & ph)","14593803":"View with Boxen Plots","cf35c5b1":"**Z-Standardised Data**","6a874bd4":"# **Modelling**\n\nWhe dealing with classification problems, there are often a frew questions worth bearing in mind when assessing model performance:\n\n* What is a 'good' score?\n\nFor this, I like to set an initial **baseline as the 'Null Accuracy' score**. In other words, If I just predicted the most common answer for every input, what would my accuracy score be?\n\n* Could False Positives or False Negatives have a disastrous impact?\n\nThis is more common on, say, Medical datasets. If a model predicts a False Negative it could quite literally have life-threatening effects.\n\n* Based on the above, is your chosen error-metric suitable?","3d7c01b8":"For now, I will leave the data as it is.","2f2095c9":"# **Water Quality**\n\n**The Problem: Can we predict which water samples are Potable or Non-Potable?**\n\nI've had a hiatus from Kaggle and this dataset is a nice way to try and get back in to the swing of things.\n\nAn additional goal of this notebook was to demonstrate DataFrame styling.\n\nThis is a useful tool as one can quickly highlight areas of importance in a dataframe, for example by highlighting max values etc.\n\n\n**Disclaimer**: \n\nThere has been a fair amount of discussion in relation to this dataset centred on the idea that it is not genuine. \nThrough this analysis, it was very difficult to separate the two classes (Potable & Non-Potable), and of the classifications didn't make a lot of sense given the value of other variables. \n\nHaving said that, I have decided to publish this notebook nontheless, as I feel that some of the methods and snippets of code might still be useful for some, even if the conclusions aren't of real-world value!\n\n","72653ef5":"**First: Scaling the data**\n\nThis can be done manually, or through a library like Sklearn. \n\nThere are several methods of scaling data. Two popular methods are **z-standardisation & Min-Max Scaling**.\n\nZ-standardisation represents how many standard deviations a point is from the mean.\n\nWhilst Min-Max scaling transforms the data so that it is between 0 & 1.\n\nI'll try both to see how if this influences model performance","1f294240":"Most of our samples are non-potable.\n\nWe could deal with this imbalance using techniques such as SMOTE. However, this may not be necessary.\n\nWhat about the distribution of our variables in relation to potability?","b6745221":"We observe above & below that the **scaled data out-performs the unscaled data**, on average, across every metric.\n\nThis is of course to be expected, but is nice to show it.\n\nThe **z-Standardised scaling outperforms the Min-Max scaling** on this particular dataset. Therefore, all future model tests will be done using the z-Standardised data.","f14e3e86":"**Models**\n\nWe could run through each model in turn as below, but instead, I will loop through a selection of machine learning models to which may have promise.\n\nWe can then look to fine tune these models to improve the score further.","14268301":"As we can see, the model performs well at predicting negatives, however it is poor at predicting positives.\n\n**Importance of Error Metrics**\n\nThis, along with all the model performances above, really show the importance of defining your error metric. \n\nThis is typically done by answering the questions: \n\n\"**Why do I want to predict Variable X?**\" \n\nor \n\n\"**What does success look like?**\"\n\nOnce you have thoses answers you can start to apply logic to your choice of error metric. For example, in this case one could reasonably argue that the worst case scenario would be for our model to have a high number of False Positives. If our model was to predict a water source was Potable, when in fact it was not, the unsuspecting drinker of the water could find themselves very ill. \n\nSo in that respect, we might choose a model that is perhaps more cautious in predicting Positive ('Potable' in our case) classifications.","8de1cb2a":"Convert to DataFrame","2b2cb9cc":"# **Exploratory Data Analysis**\n\nThis is a crucial step.\n\nIt aids in building understanding of your data, and can also help you to spot features that may prove valauble for your model. This can also be useful when explaining to stakeholders 'how' a model works","d617681d":"**Null Accuracy**\n\nThis can serve as a baseline for us to beat. \n\nThe Null accuracy would be the score if you always predicted the most common classification. In this case, all zeroes.","792b1203":"And to plot them","51381f0d":"We can see here that Logisitc Regression only matches the Null Accuracy score. Indeed, it predicts the negative class every time.\n\nThe code below shows all the predictions are zeroes!","d36f05b8":"Viewing the differences","1e917955":"# **Combining Models**\n\nWe see that the models above seem to perform differently across our error metrics. \n\nSVM, for example, has the highest Accuracy, but it's Recall is much lower than KNN. One way to try and get the benefits of many models is to use a voting classifier.\n\nThe idea behind the Voting Classifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.","d81b71c4":"# **Dealing with Missing Values**\n\nI will impute the missing values as simply the mean for the column. \n\nImportantly, I will use calculate the **mean of the training set only**. The reason for this is to **prevent data leakage**.\n\nIf you use the mean of the entire dataset, you are gaining information from your test data, and as such, you may overfit to our specific case. We want our model to be able to deal with unseen data.\n","915a25ca":"**Min-Max Scaled Data**","473beeab":"**What does this tell us?**\n\nWell here we can clearly see a difference between Potable & Non-Potable samples, especially with relation to ph & Sulfate.\n\nWhen water is Potable, we observe a relatively strong inverse relationship between the two variables; as ph increases, Sulfate levels decrease. This does not look to be the case with Non-Potable samples.\n\nTo a lesser extent, this looks to also be true with Solids & ph.","34dc7e37":"The models I will be trialing are listed below.\n\nFirst, I will run through the models using the unscaled data, then I will repeat using scale data.","f16cc68c":"**Unscaled Data**","4e5759e7":"**What does this tell us?**\n\nThis is a pretty overwhelming plot. It doesn't show us a whole lot as there is far too much information to take in at once. \n\nHowever, the benefit of this type of plotting can be to quickly assess, roughly, if there are certain areas we wish to 'zoom in' and focus on.","c16b4578":"It looks to have performed quite well. Let's compare this to the indivdual model performances","cbc248ae":"# **The Data**","0aa0f7da":"**What does this tell us?**\n\nAs we noticed in the scatter plots above, we now see formally that variables have differing relationships between one another when split by Potability.\n\nThis is good news from a modelling perspective. **We have uncovered some previously hidden insights in our data**.","5016fc71":"We can explicitly view these combinations if we wish:"}}