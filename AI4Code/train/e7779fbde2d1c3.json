{"cell_type":{"28ea5f9e":"code","fd20f4fc":"code","fca7b92e":"code","a60b239d":"code","b517bc94":"code","8a5bd26a":"code","b8cbb40b":"code","25faf76c":"code","7c3431dd":"code","46761396":"code","7023bba4":"code","4ca46e46":"code","b539da0b":"code","26a8d5db":"code","7736ac7e":"code","b260fe96":"code","4b777fb2":"code","7de9602d":"code","253b4eee":"code","b374aa52":"code","4cada470":"code","9f66845d":"code","87781af9":"code","c34a18ab":"code","0d74744a":"code","71ca1673":"code","efabd028":"code","13bd9275":"code","a8ec5e5f":"code","ac3f8828":"code","0ad37ac5":"code","d9dc98c5":"code","6b97f313":"code","b8d2d818":"code","ea4a4e3d":"code","52ab19db":"code","b20a2580":"code","735ac4d2":"code","e457aa65":"code","b0553681":"code","3692e7e7":"code","83c3d78d":"code","1950ee9c":"code","64047f2b":"code","b56a1edf":"code","fb715816":"code","c4d23855":"code","167d5609":"code","4da2e08e":"code","6144bc67":"code","100e2ca0":"code","a7dc9d9b":"code","9a14d47e":"code","45319be8":"code","bed2125c":"code","aee5e580":"code","26ed62b0":"code","9fe5a9d9":"code","149b4d9f":"code","23f262f3":"code","e1d597b3":"code","4e34624c":"code","a5718b26":"code","86d1745a":"code","0cdcce36":"code","002e18ef":"code","1aaf5d6c":"code","0181b2a1":"code","4e52198b":"code","8e801435":"code","ec4ec4ab":"code","7a82d5ff":"code","6ff5550d":"code","b233273f":"code","80f2c2e4":"code","d57bd894":"markdown","9768098d":"markdown","f29973e6":"markdown","8f434ed4":"markdown","f6978686":"markdown","b3aa6f4d":"markdown","53368995":"markdown","b4d8ff49":"markdown","dee838ca":"markdown","aac83e1d":"markdown","7948e2d8":"markdown","82f5f38b":"markdown"},"source":{"28ea5f9e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fd20f4fc":"from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RandomizedSearchCV,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestRegressor,BaggingRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression,LogisticRegression,Lasso, Ridge\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import r2_score,roc_auc_score,classification_report,mean_squared_error,accuracy_score,confusion_matrix","fca7b92e":"### read datasets\ntrain=pd.read_csv(\"..\/input\/loan-prediction\/train_ctrUa4K.csv\")\ntest=pd.read_csv(\"..\/input\/loan-prediction\/test_lAUu6dG.csv\")\n\ntest_cpy=test.copy()","a60b239d":"round((test.isnull().sum()\/len(test.index))*100,2)","b517bc94":"train.head()","8a5bd26a":"test.head()","b8cbb40b":"train.shape","25faf76c":"test.shape","7c3431dd":"train.info()","46761396":"train.describe()","7023bba4":"train['Loan_Status'].value_counts().plot.bar()","4ca46e46":"# Independent Variable (Categorical)\n\nplt.figure(1) \nplt.subplot(221) \ntrain['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender') \nplt.subplot(222) \ntrain['Married'].value_counts(normalize=True).plot.bar(title= 'Married') \nplt.subplot(223) \ntrain['Self_Employed'].value_counts(normalize=True).plot.bar(title= 'Self_Employed') \nplt.subplot(224) \ntrain['Credit_History'].value_counts(normalize=True).plot.bar(title= 'Credit_History') \nplt.show()","b539da0b":"# It can be inferred from the above bar plots that:\n\n#     80% applicants in the dataset are male.\n#     Around 65% of the applicants in the dataset are married.\n#     Around 15% applicants in the dataset are self employed.\n#     Around 85% applicants have repaid their debts.","26a8d5db":"plt.figure(1) \nplt.subplot(131) \ntrain['Dependents'].value_counts(normalize=True).plot.bar(figsize=(24,6), title= 'Dependents') \nplt.subplot(132) \ntrain['Education'].value_counts(normalize=True).plot.bar(title= 'Education') \nplt.subplot(133) \ntrain['Property_Area'].value_counts(normalize=True).plot.bar(title= 'Property_Area') \nplt.show()","7736ac7e":"# Following inferences can be made from the above bar plots:\n\n#     Most of the applicants don\u2019t have any dependents.\n#     Around 80% of the applicants are Graduate.\n#     Most of the applicants are from Semiurban area.","b260fe96":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(train['ApplicantIncome']); \nplt.subplot(122) \ntrain['ApplicantIncome'].plot.box(figsize=(16,5)) \nplt.show()","4b777fb2":"train.boxplot(column='ApplicantIncome', by = 'Education')\nplt.suptitle(\"\")","7de9602d":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(train['CoapplicantIncome']); \nplt.subplot(122) \ntrain['CoapplicantIncome'].plot.box(figsize=(16,5)) \nplt.show()","253b4eee":"# Let\u2019s look at the distribution of LoanAmount variable.\n\ntrain['LoanAmount'].plot.box(figsize=(16,5)) ","b374aa52":"Gender=pd.crosstab(train['Gender'],train['Loan_Status'],normalize=True) \nGender.plot(kind=\"bar\", stacked=True, figsize=(4,4))\n\n# It can be inferred that the proportion of male and female applicants \n# is more or less same for both approved and unapproved loans.","4cada470":"Married=pd.crosstab(train['Married'],train['Loan_Status'],normalize=True) \nDependents=pd.crosstab(train['Dependents'],train['Loan_Status'],normalize=True) \nEducation=pd.crosstab(train['Education'],train['Loan_Status'],normalize=True) \nSelf_Employed=pd.crosstab(train['Self_Employed'],train['Loan_Status'],normalize=True) \n\nMarried.plot(kind=\"bar\", stacked=True, figsize=(4,4)) \nplt.show() \nDependents.plot(kind=\"bar\", stacked=True) \nplt.show() \nEducation.plot(kind=\"bar\", stacked=True, figsize=(4,4)) \nplt.show() \nSelf_Employed.plot(kind=\"bar\", stacked=True, figsize=(4,4)) \nplt.show()\n\n\n#  Proportion of married applicants is higher for the approved loans.\n#  Distribution of applicants with 1 or 3+ dependents is similar across both the categories of Loan_Status.\n#  There is nothing significant we can infer from Self_Employed vs Loan_Status plot.","9f66845d":"Credit_History=pd.crosstab(train['Credit_History'],train['Loan_Status'],normalize=True) \nProperty_Area=pd.crosstab(train['Property_Area'],train['Loan_Status'],normalize=True) \nCredit_History.plot(kind=\"bar\", stacked=True, figsize=(4,4)) \nplt.show() \nProperty_Area.plot(kind=\"bar\", stacked=True) \nplt.show()\n\n#  It seems people with credit history as 1 are more likely to get their loans approved.\n#  Proportion of loans getting approved in semiurban area is higher as compared to that in rural or urban areas.","87781af9":"train.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar()","c34a18ab":"bins=[0,2500,4000,6000,81000] \ngroup=['Low','Average','High', 'Very high'] \ntrain['Income_bin']=pd.cut(train['ApplicantIncome'],bins,labels=group)\n\nIncome_bin=pd.crosstab(train['Income_bin'],train['Loan_Status'],normalize=True) \nIncome_bin.plot(kind=\"bar\", stacked=True) \nplt.xlabel('ApplicantIncome') \nP = plt.ylabel('Percentage')","0d74744a":"bins=[0,1000,3000,42000] \ngroup=['Low','Average','High'] \ntrain['Coapplicant_Income_bin']=pd.cut(train['CoapplicantIncome'],bins,labels=group)\n\nCoapplicant_Income_bin=pd.crosstab(train['Coapplicant_Income_bin'],train['Loan_Status'],normalize=True) \nCoapplicant_Income_bin.plot(kind=\"bar\", stacked=True) \nplt.xlabel('CoapplicantIncome') \nP = plt.ylabel('Percentage')\n\n\n# It shows that if coapplicant\u2019s income is less the chances of loan approval are high. \n# But this does not look right. The possible reason behind this may be that most of the applicants don\u2019t \n# have any coapplicant so the coapplicant income for such applicants is 0 and hence the loan approval is not \n# dependent on it. So we can make a new variable in which we will combine the applicant\u2019s and coapplicant\u2019s \n# income to visualize the combined effect of income on loan approval.","71ca1673":"train['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome']\n\nbins=[0,2500,4000,6000,81000] \ngroup=['Low','Average','High', 'Very high'] \ntrain['Total_Income_bin']=pd.cut(train['Total_Income'],bins,labels=group)\n\nTotal_Income_bin=pd.crosstab(train['Total_Income_bin'],train['Loan_Status'],normalize=True) \nTotal_Income_bin.plot(kind=\"bar\", stacked=True) \nplt.xlabel('Total_Income') \nP = plt.ylabel('Percentage')\n\n\n# It can be seen that the proportion of approved loans is higher for Low and Average \n# Loan Amount as compared to that of High Loan Amount which supports our hypothesis in which we considered that \n# the chances of loan approval will be high when the loan amount is less.","efabd028":"bins=[0,100,200,700] \ngroup=['Low','Average','High'] \ntrain['LoanAmount_bin']=pd.cut(train['LoanAmount'],bins,labels=group)\n\nLoanAmount_bin=pd.crosstab(train['LoanAmount_bin'],train['Loan_Status'],normalize=True) \nLoanAmount_bin.plot(kind=\"bar\", stacked=True) \nplt.xlabel('LoanAmount') \nP = plt.ylabel('Percentage')","13bd9275":"train=train.drop(['Income_bin', 'Coapplicant_Income_bin', 'LoanAmount_bin', 'Total_Income_bin', 'Total_Income'], axis=1)","a8ec5e5f":"matrix = train.corr() \nf, ax = plt.subplots(figsize=(9, 6)) \nsns.heatmap(matrix, vmax=.8, square=True, cmap=\"BuPu\");","ac3f8828":"train['LoanAmount_log'] = np.log(train['LoanAmount']) \ntrain['LoanAmount_log'].hist(bins=20) \n","0ad37ac5":"test['LoanAmount_log'] = np.log(test['LoanAmount'])","d9dc98c5":"train['Gender'].value_counts()","6b97f313":"train['Married'].value_counts()","b8d2d818":"train['Self_Employed'].value_counts()","ea4a4e3d":"train['Loan_Status'].value_counts()","52ab19db":"train['Dependents'].value_counts()\ntrain['Dependents']=train['Dependents'].fillna(0)\ntrain['Dependents'].replace({'3+':3},inplace=True)\n\ntest['Dependents']=test['Dependents'].fillna(0)\ntest['Dependents'].replace({'3+':3},inplace=True)","b20a2580":"train['Gender']=train['Gender'].map({'Male':1, 'Female': 0})\ntest['Gender']=test['Gender'].map({'Male':1, 'Female': 0})\ntrain['Gender']=train['Gender'].fillna(1).astype('int')\ntest['Gender']=test['Gender'].fillna(1).astype('int')\n\ntrain['Self_Employed']=train['Self_Employed'].map({'Yes':1, 'No': 0})\ntest['Self_Employed']=test['Self_Employed'].map({'Yes':1, 'No': 0})\ntrain['Self_Employed']=train['Self_Employed'].fillna(0).astype(int)\ntest['Self_Employed']=test['Self_Employed'].fillna(0).astype(int)\n\ntrain['Married']=train['Married'].map({'Yes':1, 'No': 0})\ntest['Married']=test['Married'].map({'Yes':1, 'No': 0})\ntrain['Married']=train['Married'].fillna(1).astype('int')\n\n\ntrain['Education']=train['Education'].map({'Graduate':1, 'Not Graduate': 0})\ntest['Education']=test['Education'].map({'Graduate':1, 'Not Graduate': 0})\n\ntrain['Loan_Status']=train['Loan_Status'].map({'Y':1, 'N': 0})","735ac4d2":"train['Dependents']=train['Dependents'].astype(int)\ntest['Dependents']=test['Dependents'].astype(int)","e457aa65":"# train_pa=pd.get_dummies(train['Property_Area'],drop_first=True)\n# test_pa=pd.get_dummies(test['Property_Area'],drop_first=True)","b0553681":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n\ntrain['Property_Area']=le.fit_transform(train['Property_Area'])\ntest['Property_Area']=le.fit_transform(test['Property_Area'])","3692e7e7":"train.drop(['Loan_ID'],axis=1,inplace=True)\ntest.drop(['Loan_ID'],axis=1,inplace=True)","83c3d78d":"train.head()","1950ee9c":"round((train.isnull().sum()\/len(train.index))*100,2)","64047f2b":"test.head()","b56a1edf":"test.isnull().sum()","fb715816":"sns.boxplot(x=train['ApplicantIncome'])","c4d23855":"sns.boxplot(x=train['CoapplicantIncome'])","167d5609":"train.Loan_Status.value_counts()","4da2e08e":"train['TotalIncome']=train['ApplicantIncome']+train['CoapplicantIncome']\ntest['TotalIncome']=test['ApplicantIncome']+test['CoapplicantIncome']\n","6144bc67":"train.head()","100e2ca0":"train['Total_Income_log'] = np.log(train['TotalIncome']) \nsns.distplot(train['Total_Income_log']); \ntest['Total_Income_log'] = np.log(test['TotalIncome'])","a7dc9d9b":"train.drop(['ApplicantIncome','CoapplicantIncome','TotalIncome','LoanAmount'],axis=1,inplace=True)\ntest.drop(['ApplicantIncome','CoapplicantIncome','TotalIncome','LoanAmount'],axis=1,inplace=True)","9a14d47e":"train.head()","45319be8":"train['LoanAmount_log']=train['LoanAmount_log'].fillna(train['LoanAmount_log'].median())\ntrain['Loan_Amount_Term']=train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mean())\ntrain['Credit_History']=train['Credit_History'].fillna(train['Credit_History'].mean())\n\ntest['LoanAmount_log']=test['LoanAmount_log'].fillna(test['LoanAmount_log'].median())\ntest['Loan_Amount_Term']=test['Loan_Amount_Term'].fillna(test['Loan_Amount_Term'].mean())\ntest['Credit_History']=test['Credit_History'].fillna(test['Credit_History'].mean())","bed2125c":"### new variable EMI\n\ntrain['EMI']=train['LoanAmount_log']\/train['Loan_Amount_Term']\ntest['EMI']=test['LoanAmount_log']\/test['Loan_Amount_Term']","aee5e580":"# Normalising continuous features\ndf = train[['EMI','Total_Income_log']]\ndf_test = test[['EMI','Total_Income_log']]\n","26ed62b0":"df.head()","9fe5a9d9":"normalized_df=(df-df.mean())\/df.std()\nnormalized_df_test=(df_test-df_test.mean())\/df_test.std()","149b4d9f":"normalized_df.head()","23f262f3":"train=train.drop(['EMI','Total_Income_log','LoanAmount_log','Loan_Amount_Term'],axis=1)\n# train_out2=train_out2.drop(['LoanAmount','Loan_Amount_Term'],axis=1)\ndf=pd.concat([normalized_df,train],axis=1)\n\ndf.head(10)\n","e1d597b3":"test=test.drop(['EMI','Total_Income_log','LoanAmount_log','Loan_Amount_Term'],axis=1)\n\ntest_df=pd.concat([normalized_df_test,test],axis=1)\n","4e34624c":"test_df.head()","a5718b26":"round((test_df.isnull().sum()\/len(test_df.index))*100,2)","86d1745a":"#set seed for same results everytime\nseed=0\nimport sklearn.ensemble as ensemble\nimport sklearn.metrics as metrics\n\nX=df.drop('Loan_Status',1)\ny=df['Loan_Status']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state =1)\n\n#declare the models\nlr = LogisticRegression()\nrf=RandomForestClassifier()\nadb=ensemble.AdaBoostClassifier()\nbgc=ensemble.BaggingClassifier()\ngnb = GaussianNB()\nknn=KNeighborsClassifier()\ndt = DecisionTreeClassifier()\nbgcl_lr = BaggingClassifier(base_estimator=lr, random_state=0)\n\n# ,ab_rf,ab_dt,ab_nb,ab_lr,bgcl_lr\n\nmodels=[lr,rf,adb,bgc,gnb,knn,dt,bgcl_lr]\nsctr,scte,auc,ps,rs,acc=[],[],[],[],[],[]\ndef ens(X_train,X_test, y_train, y_test):\n    for model in models:\n            model.fit(X_train, y_train)\n            y_test_pred = model.predict(X_test)\n            y_test_pred_new=model.predict_proba(X_test)\n            y_test_pred_new=y_test_pred_new[:,1]\n            train_score=model.score(X_train,y_train)\n            test_score=model.score(X_test,y_test)\n            p_score=metrics.precision_score(y_test,y_test_pred)\n            r_score=metrics.recall_score(y_test,y_test_pred)\n            accr=metrics.accuracy_score(y_test,y_test_pred)\n            ac=metrics.roc_auc_score(y_test,y_test_pred_new)\n            \n            sctr.append(train_score)\n            scte.append(test_score)\n            ps.append(p_score)\n            rs.append(r_score)\n            auc.append(ac)\n            acc.append(accr)\n    return sctr,scte,auc,ps,rs,acc\n\nens(X_train,X_test, y_train, y_test)\n# 'ab_rf','ab_dt','ab_nb','ab_lr','bgcl_lr'\nensemble=pd.DataFrame({'names':['Logistic Regression','Random Forest','Ada boost','Bagging',\n                                'Naive-Bayes','KNN','Decistion Tree',\n                                'bagged LR'],\n                       'auc_score':auc,'training':sctr,'testing':scte,'precision':ps,'recall':rs,'accuracy':acc})\nensemble=ensemble.sort_values(by='auc_score',ascending=False).reset_index(drop=True)\nensemble","0cdcce36":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier()\n\n","002e18ef":"rf.fit(X,y)\n\ny_test_pred_rf=rf.predict(test_df)\nfinalpred=pd.concat([test_cpy['Loan_ID'],pd.DataFrame(y_test_pred_rf,columns=['Loan_Status'])],1)\nfinalpred.to_csv(\"sub.csv\",index=False)","1aaf5d6c":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state =0)\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n\ndt = DecisionTreeClassifier()\ndt_params = {'max_depth':np.arange(1,10), 'min_samples_leaf':np.arange(2,30), 'criterion':['entropy','gini']}\nrscv = RandomizedSearchCV(dt, dt_params, cv=5, scoring='roc_auc')\nrscv.fit(X, y)\nprint(rscv.best_params_)\nrscv_best_DT=rscv.best_params_\n\nDT=DecisionTreeClassifier(**rscv_best_DT)\nDT.fit(X,y)\ny_test_pred_DT=DT.predict(test_df)\nfinalpred=pd.concat([test_cpy['Loan_ID'],pd.DataFrame(y_test_pred_DT,columns=['Loan_Status'])],1)\nfinalpred.to_csv(\"pred.csv\",index=False)","0181b2a1":"import xgboost as xgb \nfrom xgboost.sklearn import XGBClassifier","4e52198b":"xgb=XGBClassifier(learning_rate=0.09,n_estimators=125,max_depth=4,min_child_weight=4,colsample_bytree=0.5,reg_alpha=0.000001 )","8e801435":"xgb.fit(X,y)\ny_test_pred_xgb=xgb.predict(test_df)\nfinalpred=pd.concat([test_cpy['Loan_ID'],pd.DataFrame(y_test_pred_DT,columns=['Loan_Status'])],1)\nfinalpred.to_csv(\"xgb.csv\",index=False)","ec4ec4ab":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,          \n                      intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,          \n                      penalty='l2', random_state=1, solver='liblinear', tol=0.0001,          \n                      verbose=0, warm_start=False)\n\nlr.fit(X,y)\n\n\ny_test_pred_lr=lr.predict(test_df)\nfinalpred=pd.concat([test_cpy['Loan_ID'],pd.DataFrame(y_test_pred_lr,columns=['Loan_Status'])],1)\nfinalpred.to_csv(\"LR_1.csv\",index=False)","7a82d5ff":"from sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV","6ff5550d":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y == 0))) \n\nfrom imblearn.over_sampling import SMOTE\nsm=SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X, y.ravel()) \n  \nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) ","b233273f":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,          \n                      intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,          \n                      penalty='l2', random_state=1, solver='liblinear', tol=0.0001,          \n                      verbose=0, warm_start=False)\n\n\nlr.fit(X_train_res,y_train_res)\n","80f2c2e4":"y_test_pred_lr=lr.predict(test_df)\nfinalpred=pd.concat([test_cpy['Loan_ID'],pd.DataFrame(y_test_pred_lr,columns=['Loan_Status'])],1)\n\nfinalpred.to_csv(\"LR_2.csv\",index=False)","d57bd894":"### Numerical Independent Variable vs Target Variable","9768098d":"Using SMOTE for sampling","f29973e6":"### XGBOOST","8f434ed4":"### Independent Variable (Numerical)","f6978686":"### random forest","b3aa6f4d":"### Independent Variable (Ordinal)","53368995":"### Categorical Independent Variable vs Target Variable","b4d8ff49":"###  Univariate analysis","dee838ca":"### Independent Variable (Categorical)","aac83e1d":"### SMOTE oversampling with LR has worked the best --> LR2.csv file","7948e2d8":"### Logistic Regression","82f5f38b":"### Decision trees"}}