{"cell_type":{"9bd7f906":"code","bd2e1aa7":"code","c3b5f6aa":"code","95ffb4ac":"code","01a1383c":"code","e2297bac":"code","bc48b3e8":"code","70736c0e":"code","6ad0f2b5":"code","eb7e6898":"code","4b157e1b":"code","ede54430":"code","7c14c374":"code","796dcb8b":"code","3b3e4136":"code","a35a936b":"code","9d4da02a":"code","f6066aaf":"code","f3875cae":"code","fd2d0113":"code","f92d8790":"code","fb69adef":"code","80e26932":"code","751c709b":"code","4e7f458f":"code","963b891c":"code","a5ea1289":"code","789ba6b4":"code","b2221952":"code","b29dcd61":"code","aad945bb":"code","4a60e447":"markdown","74b149da":"markdown","b92b78a2":"markdown","ac9a672f":"markdown","79384318":"markdown","085a4211":"markdown","cc345d16":"markdown","0c2eba89":"markdown","0ff7f2ea":"markdown"},"source":{"9bd7f906":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Visualization\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd2e1aa7":"diabetes = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv', index_col=0, header = 0)    \n\ndiabetes['ethnicity'] = np.where(diabetes['ethnicity'] == \"African American\", 1, np.where(diabetes['ethnicity'] == \"Asian\", 2, np.where(diabetes['ethnicity'] == \"Caucasian\", 3, \n                                np.where(diabetes['ethnicity'] == \"Hispanic\", 4, np.where(diabetes['ethnicity'] == \"Native American\", 5, np.where(diabetes['ethnicity'] == \"Other\/Unknown\", 6, np.nan))))))\n# The dataset contains 13057 data points with missing values and 181 features(floats(157), integers(15) and object(7))\nprint(diabetes.columns)  \n","c3b5f6aa":"diabetes = diabetes.drop([\"h1_bilirubin_max\", \"h1_bilirubin_min\", \"h1_albumin_max\", \"h1_albumin_min\", \"h1_lactate_max\", \"h1_lactate_min\", \"h1_pao2fio2ratio_max\", \n                            \"h1_pao2fio2ratio_min\", \"h1_arterial_ph_max\", \"h1_arterial_pco2_max\", \"h1_arterial_pco2_min\", \"h1_arterial_po2_max\", \"h1_arterial_po2_min\", \n                            \"h1_hco3_max\", \"h1_hco3_min\", \"h1_wbc_max\", \"h1_wbc_min\", \"h1_calcium_max\", \"h1_calcium_min\", \"h1_platelets_max\", \"h1_platelets_min\", \n                            \"h1_bun_max\", \"h1_bun_min\", \"h1_diasbp_invasive_max\", \"h1_diasbp_invasive_min\", \"h1_sysbp_invasive_max\", \"h1_sysbp_invasive_min\",\n                           \"h1_creatinine_max\", \"h1_creatinine_min\", \"h1_mbp_invasive_max\", \"h1_mbp_invasive_min\", \"h1_arterial_ph_min\"], axis = 1)\nprint(diabetes.columns)","95ffb4ac":"# Subset only the numerical data\ndiabetes = diabetes.select_dtypes(np.number)\ndiabetes.info()\n\n# Remove data with id names\ndiabetes = diabetes.drop(['encounter_id', 'hospital_id', 'icu_id'], axis = 1)\n\n# Convert any values into float, easy to use for predictive models\ndiabetes = diabetes.apply (pd.to_numeric, errors='coerce')\ndiabetes = diabetes.astype(float)\n","01a1383c":"print(diabetes.dtypes)","e2297bac":"diabetes.head()","bc48b3e8":"print(diabetes.describe())","70736c0e":"print(\"dimension of diabetes data: {}\".format(diabetes.shape))\n# The diabetes data consists of 130157 data points and 171 features","6ad0f2b5":"## Diabetes_mellitus is the feature we are going to predict, 0 means No diabetes and 1 means diabetes. \nprint(diabetes.groupby('diabetes_mellitus').size())\n# Of the 130157 data points, 102006 have diabetes and 28151 do not have diabetes.","eb7e6898":"print(diabetes.isnull().sum())\n\n# fill missing values with mean column values (NA replaced with mean values)\ndiabetes.fillna(diabetes.mean(), inplace=True)\n# count the number of NaN values in each column\nprint(diabetes.isnull().sum())\ndiabetes","4b157e1b":"import seaborn as sns\nsns.countplot(diabetes['diabetes_mellitus'],label=\"Count\")","ede54430":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(diabetes.loc[:, diabetes.columns != 'diabetes_mellitus'], diabetes['diabetes_mellitus'], stratify=diabetes['diabetes_mellitus'], random_state=66)\nfrom sklearn.neighbors import KNeighborsClassifier\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)\nfor n_neighbors in neighbors_settings:\n    # build the model\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(knn.score(X_train, y_train))\n    # record test set accuracy\n    test_accuracy.append(knn.score(X_test, y_test))\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()","7c14c374":"# check the accuracy score of the k-nearest neighbors algorithm to predict diabetes\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'.format(knn.score(X_test, y_test)))","796dcb8b":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","3b3e4136":"tree = DecisionTreeClassifier(max_depth=3, random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","a35a936b":"# Feature importance shows how important each feature is for the decision a decision tree classifier makes\nprint(\"Feature importances:\\n{}\".format(tree.feature_importances_))\n","9d4da02a":"diabetes_features = [x for i,x in enumerate(diabetes.columns) if i!=139]\ndef plot_feature_importances_diabetes(model):\n    plt.figure(figsize=(20,20))\n    n_features = 139\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), diabetes_features)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)\nplot_feature_importances_diabetes(tree)\n# The main features (d1_hemoglobin_max, d1_glucose_max, d1_diabsp_noninvasive_min, weight, bmi) seems to be the major features contributing to predict the diabetes","f6066aaf":"# Multilayer perceptrons (MLP) \nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(random_state=42)\nmlp.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))\n# The result is not as better than decision tree model, might be due to scaling","f3875cae":"# Re-scale data (Mean of 0 and variance of 1)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\nmlp = MLPClassifier(random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n# The training model is over-fitting on re-scaling","fd2d0113":"# Tried to increase the iteration, alpha parameter to the weight of the model\nmlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n# Still the training model is over-fitting","f92d8790":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = tree.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n","fb69adef":"# method I: plt the ROC curve\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","80e26932":"diabetes_unlabelled = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv', index_col=0, header = 0)    \n\ndiabetes_unlabelled['ethnicity'] = np.where(diabetes_unlabelled['ethnicity'] == \"African American\", 1, np.where(diabetes_unlabelled['ethnicity'] == \"Asian\", 2, np.where(diabetes_unlabelled['ethnicity'] == \"Caucasian\", 3, \n                                np.where(diabetes_unlabelled['ethnicity'] == \"Hispanic\", 4, np.where(diabetes_unlabelled['ethnicity'] == \"Native American\", 5, np.where(diabetes_unlabelled['ethnicity'] == \"Other\/Unknown\", 6, np.nan))))))\ndiabetes_encounter = diabetes_unlabelled[[\"encounter_id\"]]","751c709b":"diabetes_unlabelled = diabetes_unlabelled.drop([\"h1_bilirubin_max\", \"h1_bilirubin_min\", \"h1_albumin_max\", \"h1_albumin_min\", \"h1_lactate_max\", \"h1_lactate_min\", \"h1_pao2fio2ratio_max\", \n                            \"h1_pao2fio2ratio_min\", \"h1_arterial_ph_max\", \"h1_arterial_pco2_max\", \"h1_arterial_pco2_min\", \"h1_arterial_po2_max\", \"h1_arterial_po2_min\", \n                            \"h1_hco3_max\", \"h1_hco3_min\", \"h1_wbc_max\", \"h1_wbc_min\", \"h1_calcium_max\", \"h1_calcium_min\", \"h1_platelets_max\", \"h1_platelets_min\", \n                            \"h1_bun_max\", \"h1_bun_min\", \"h1_diasbp_invasive_max\", \"h1_diasbp_invasive_min\", \"h1_sysbp_invasive_max\", \"h1_sysbp_invasive_min\",\n                           \"h1_creatinine_max\", \"h1_creatinine_min\", \"h1_mbp_invasive_max\", \"h1_mbp_invasive_min\", \"h1_arterial_ph_min\"], axis = 1)\nprint(diabetes_unlabelled.columns)","4e7f458f":"# Subset only the numerical data\ndiabetes_unlabelled = diabetes_unlabelled.select_dtypes(np.number)\ndiabetes_unlabelled.head()\n\n# Remove data with id names\ndiabetes_unlabelled = diabetes_unlabelled.drop(['encounter_id', 'hospital_id', 'icu_id'], axis = 1)\n\n# Convert any values into float, easy to use for predictive models\ndiabetes_unlabelled = diabetes_unlabelled.apply (pd.to_numeric, errors='coerce')\ndiabetes_unlabelled = diabetes_unlabelled.astype(float)\n","963b891c":"print(\"dimension of diabetes_unlabelled data: {}\".format(diabetes_unlabelled.shape))\n# The diabetes data consists of 10234 data points and 139 features","a5ea1289":"print(diabetes_unlabelled.isnull().sum())\n\n# fill missing values with mean column values (NA replaced with mean values)\ndiabetes_unlabelled.fillna(diabetes_unlabelled.mean(), inplace=True)\n# count the number of NaN values in each column\nprint(diabetes_unlabelled.isnull().sum())\ndiabetes_unlabelled","789ba6b4":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = tree.predict_proba(diabetes_unlabelled)\npreds = probs[:,1]","b2221952":"diabetes_prediction = pd.DataFrame({'diabetes_mellitus': probs[:, 0]})\ndiabetes_prediction.head()","b29dcd61":"diabetes_prediction_final = pd.concat([diabetes_encounter.reset_index(drop=True), diabetes_prediction], axis=1)\ndiabetes_prediction_final","aad945bb":"diabetes_prediction_final.to_csv('Diabetes_prediction.csv',index=False)","4a60e447":"# Deep Learning to Predict Diabetes","74b149da":"# Predicitng the unlabelledWiDS2021 dataset","b92b78a2":"# Feature engineering and selection","ac9a672f":"# K-Nearest Neighbors to Predict Diabetes","79384318":"# Data Pre-processing:","085a4211":"# Data Pre-processing:","cc345d16":"# Load dataset","0c2eba89":"# Feature Importance in Decision Trees","0ff7f2ea":"# Decision Tree Classifier"}}