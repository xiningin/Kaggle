{"cell_type":{"3d40297e":"code","f5e81f5f":"code","6a2b84a9":"code","c3898403":"code","70b6bd68":"code","f621d287":"code","aff08d23":"code","4658209e":"code","affa1143":"code","b5424443":"code","45a0215c":"code","aca005d7":"code","8313e31e":"code","813a2b89":"code","c3c02929":"code","ea0e3b94":"code","42c3089b":"code","95d24cb6":"code","aa96689c":"code","89de54e7":"code","12e98f8f":"code","38a782fe":"code","2e9caf05":"code","4c8d606b":"code","01d0736d":"code","6a9d0ff9":"code","921244d1":"markdown","4e675cf8":"markdown","730c8253":"markdown","75c5b68d":"markdown","2320831f":"markdown","0bdfe008":"markdown","0852dd9f":"markdown","038d58f5":"markdown","c462d3d9":"markdown","5891df11":"markdown","0e0ff56d":"markdown","e4924c9c":"markdown","eba0dad6":"markdown","1534151c":"markdown","01728afc":"markdown","6990066d":"markdown","251d4b68":"markdown","c163c83d":"markdown","ed0f36c9":"markdown","87baf155":"markdown"},"source":{"3d40297e":"## RUN THIS CELL TO PROPERLY HIGHLIGHT THE HEADER\nimport requests\nfrom IPython.core.display import HTML\nstyles = requests.get(\"https:\/\/raw.githubusercontent.com\/Harvard-IACS\/2018-CS109A\/master\/content\/styles\/cs109.css\").text\nHTML(styles)","f5e81f5f":"import os\nimport warnings\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pandas as pd\nfrom scipy.sparse.linalg import eigs\nimport seaborn as sns\nfrom IPython.display import display\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n%matplotlib inline\n\nwarnings.filterwarnings(\"ignore\")","6a2b84a9":"food_all = pd.read_csv('\/kaggle\/input\/intro-to-ml\/food-data.csv', index_col=0,sep=',').dropna(axis=0)\n\ndisplay(food_all)","c3898403":"### One-dimensional view (along the 'Real Coffee')\nfood_all['y'] =np.zeros(16) \n\nx = 'Real coffee'\ny = 'y'\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.regplot(x=x,\n                 y=y,\n                 data=food_all,\n                 fit_reg=True,\n                 marker=\"o\",\n                 color=\"skyblue\")\n\n# add annotations one by one with a loop\nfor i in range(food_all.shape[0]):\n    label = food_all.index[i]\n    x_pos = food_all[x][i]\n    y_pos = food_all[y][i]\n\n    ax.text(x_pos,\n            y_pos,\n            label,\n            rotation=60,\n            size='medium',\n            color='black',\n            weight='semibold')","70b6bd68":"### Two-dimensional view (along the 'Real Coffee' & 'Tea')\nx = 'Real coffee'\ny = 'Tea'\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.regplot(x=x,\n                 y=y,\n                 data=food_all,\n                 fit_reg=False,\n                 marker=\"o\",\n                 color=\"skyblue\",\n                 scatter_kws={'s': 350})\n\n# add annotations one by one with a loop\nfor i in range(food_all.shape[0]):\n    label = food_all.index[i]\n    x_pos = food_all[x][i]\n    y_pos = food_all[y][i]\n\n    ax.text(x_pos,\n            y_pos,\n            label,\n            horizontalalignment='left',\n            size='medium',\n            color='black',\n            weight='semibold')","f621d287":"### Three-dimensional view (along the 'Real Coffee', 'Tea' & Potatoes)\nm = food_all[['Real coffee', 'Tea', 'Potatoes']].values\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\nfor i in range(len(m)):\n    x = m[i, 0]\n    y = m[i, 1]\n    z = m[i, 2]\n    label = i\n    ax.scatter(x, y, z, c='skyblue', s=60, alpha=1)\n    ax.text(x,\n            y,\n            z,\n            '%s' % (food_all.index[i]),\n            horizontalalignment='left',\n            size='medium',\n            weight='semibold',\n            zorder=1,\n            color='black')\n\nax.set_xlabel('Real coffee')\nax.set_ylabel('Tea')\nax.set_zlabel('Potatoes')\n\nplt.show()","aff08d23":"### Four-dimensional view (along the 'Real Coffee', 'Tea' & Potatoes)\nm = food_all[['Real coffee', 'Tea', 'Potatoes','Frozen fish']].values\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\nimg=ax.scatter(m[:,0],m[:,1], m[:,2],c=m[:,3], label=food_all.index,cmap=plt.hot(),s=60,alpha=1)\nfig.colorbar(img)\n\nax.set_xlabel('Real coffee')\nax.set_ylabel('Tea')\nax.set_zlabel('Potatoes')\nfor i in range(len(m)):\n    ax.text( m[i,0], m[i,1],m[i,2],food_all.index[i])\nplt.show()","4658209e":"# Load dataset\nnba_data = pd.read_csv('\/kaggle\/input\/intro-to-ml\/nba.csv', index_col=0, sep=',').dropna(axis=0)\n# Reset index\nnba_data.reset_index(drop=True, inplace=True)\nnba_data.set_index('Player', inplace=True)\n\n# Select numeric columns for PCA\nnba_data = nba_data.select_dtypes(include=[np.number])\n\n# dimensions\nm, k = nba_data.shape\n\nprint(\"{} x {} table of data:\".format(m, k))\ndisplay(nba_data.head())\nprint(\"...\")","affa1143":"# First 250 players (alphabetically)\nx = nba_data.head(250).values\nx = x \/ x.std(axis=0)","b5424443":"x_centered = x - x.mean(axis=0)","45a0215c":"C = x_centered.T.dot(x_centered) \/ x_centered.shape[0]\nprint(C[:4, :4])\nprint('...')\nprint(C.shape)","aca005d7":"K = 2\nl, W = eigs(C, k=K, v0=np.ones(25))\n\neigenvectors = pd.DataFrame(W.real,\n                            columns=['Eigenvector 1', 'Eigenvector 2'],\n                            index=nba_data.columns)\n\neigenvectors","8313e31e":"eigenvalues = pd.DataFrame(l.real, index=['\u03bb1', '\u03bb2'])\neigenvalues.plot(kind='bar',\n                 title='First two Eigenvalues',\n                 rot=0,\n                 legend=False)","813a2b89":"PCs = {\n    'PC' + str(1 + i): (np.dot(x_centered, W[:, i]) \/ np.sqrt(l[i])).real\n    for i in range(W.shape[1])\n}\n\nnba_pca = pd.DataFrame(PCs, index=nba_data.head(250).index)\n\nx = 'PC1'\ny = 'PC2'\n\nfig, ax = plt.subplots(figsize=(25, 25))\nax = sns.regplot(data=nba_pca,\n                 x=x,\n                 y=y,\n                 fit_reg=False,\n                 marker=\"o\",\n                 color=\"skyblue\",\n                 scatter_kws={'s': 250})\n\n# add annotations one by one with a loop\nfor i in range(nba_pca.shape[0]):\n    label = nba_pca.index[i]\n    x_pos = nba_pca[x][i] + .06\n    y_pos = nba_pca[y][i] + .08\n\n    ax.text(x_pos,\n            y_pos,\n            label,\n            horizontalalignment='left',\n            size='large',\n            color='black',\n            weight='semibold')","c3c02929":"fig, ax = plt.subplots(1, 1, figsize=(15, 15))\nax = sns.scatterplot(data=nba_pca,\n                     x=x,\n                     y=y,\n                     marker=\"o\",\n                     s=250,\n                     hue=(nba_data['MP']),\n                     ax=ax)\n\nfor i in range(nba_pca.shape[0]):\n    label = nba_pca.index[i]\n    x_pos = nba_pca[x][i] + .06\n    y_pos = nba_pca[y][i] + .08\n\n    ax.text(x_pos,\n            y_pos,\n            label,\n            horizontalalignment='left',\n            size='medium',\n            color='black',\n            weight='semibold')\nplt.show()","ea0e3b94":"from sklearn.cluster import KMeans\nX=nba_pca\ncolors = ['black', 'blue', 'purple', 'yellow', 'red']\nkmeans = KMeans(n_clusters=5, random_state=0).fit(X)\nfig, ax = plt.subplots(1, 1, figsize=(15, 15))\nx = 'PC1'\ny = 'PC2'\nax = sns.scatterplot(data=nba_pca,\n                     x=x,\n                     y=y,\n                     marker=\"o\",\n                     s=250,\n                     hue=(kmeans.predict(X)),\n                     palette=sns.color_palette(\"hls\", 5),\n                     legend=\"full\",\n                     ax=ax)\n\nfor i in range(nba_pca.shape[0]):\n    label = nba_pca.index[i]\n    x_pos = nba_pca[x][i] + .06\n    y_pos = nba_pca[y][i] + .08\n\n    ax.text(x_pos,\n            y_pos,\n            label,\n            horizontalalignment='left',\n            size='medium',\n            color='black',\n            weight='semibold')\nplt.show()","42c3089b":"from sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\n\n# loading dataset\ndigits = load_digits()\nX = digits.data \/ 255.0\ny = digits.target\n\n# transforming into a Pandas dataframe\nfeat_cols = ['pixel_' + str(i) for i in range(X.shape[1])]\ndf = pd.DataFrame(X, columns=feat_cols)\ndf['label'] = y\ndf['label'] = df['label'].apply(lambda i: str(i))\nX, y = None, None\n\nprint(\"{} x {} table of data:\".format(df.shape[0], df.shape[1]))\ndisplay(df.head())\nprint(\"...\")","95d24cb6":"np.random.seed(42)\nrndperm = np.random.permutation(df.shape[0])\n\nplt.gray()\nfig = plt.figure(figsize=(8, 6))\nfor i in range(0, 15):\n    ax = fig.add_subplot(3,\n                         5,\n                         i + 1,\n                         title=\"\\nDigit: {}\\n\".format(\n                             str(df.loc[rndperm[i], 'label'])))\n    ax.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.matshow(df.loc[rndperm[i], feat_cols].values.reshape(\n        (8, 8)).astype(float))\nplt.show()","aa96689c":"# Count per label\ndf.label.value_counts()","89de54e7":"pca = PCA(n_components=25)\npca_result = pca.fit_transform(df[feat_cols].values)\n\nfor i in range(25):\n    df['PC' + str(1 + i)]= pca_result[:,i]\n\nplt.figure(figsize=(16, 10))\nsns.scatterplot(x=\"PC1\",\n                y=\"PC2\",\n                hue=\"label\",\n                palette=sns.color_palette(\"hls\", 10),\n                data=df.loc[rndperm, :],\n                legend=\"full\",\n                alpha=1)","12e98f8f":"plt.figure(figsize=(16, 10))\ng = sns.relplot(x=\"PC1\", y=\"PC2\",\n                 col=\"label\", hue=\"PC3\",\n                  col_wrap=4,data=df)","38a782fe":"np.random.seed(42)\nrndperm = np.random.permutation(df.shape[0])\ncolumns=[]\nfor i in range(25):\n    columns.append('PC' + str(1 + i))\nplt.gray()\nfig = plt.figure(figsize=(8, 6))\nfor i in range(0, 15):\n    ax = fig.add_subplot(3,\n                         5,\n                         i + 1,\n                         title=\"\\nDigit: {}\\n\".format(\n                             str(df.loc[rndperm[i], 'label'])))\n    ax.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.matshow(df.loc[rndperm[i], columns[:4]].values.reshape(\n        (2, 2)).astype(float))\nplt.show()","2e9caf05":"#scree plot\npca = PCA(n_components=64)\npca_result = pca.fit_transform(df[feat_cols].values)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","4c8d606b":"print(np.cumsum(pca.explained_variance_ratio_))","01d0736d":"#reconstruct from 10 components\npca = PCA(n_components=10)\npca_result = pca.fit_transform(df[feat_cols].values)\nreconst=pca.inverse_transform(pca_result)\ncolumns=[]\nfor i in range(64):\n    df['RC' + str(1 + i)]= reconst[:,i]\n    columns.append('RC' + str(1 + i))\nfig = plt.figure(figsize=(8, 6))\nfor i in range(0, 15):\n    ax = fig.add_subplot(3,\n                         5,\n                         i + 1,\n                         title=\"\\nDigit: {}\\n\".format(\n                             str(df.loc[rndperm[i], 'label'])))\n    ax.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.matshow(df.loc[rndperm[i], columns].values.reshape(\n        (8, 8)).astype(float))\nplt.show()","6a9d0ff9":"#reconstruct from 40 components\npca = PCA(n_components=40)\npca_result = pca.fit_transform(df[feat_cols].values)\nreconst=pca.inverse_transform(pca_result)\ncolumns=[]\nfor i in range(64):\n    df['RC' + str(1 + i)]= reconst[:,i]\n    columns.append('RC' + str(1 + i))\nfig = plt.figure(figsize=(8, 6))\nfor i in range(0, 15):\n    ax = fig.add_subplot(3,\n                         5,\n                         i + 1,\n                         title=\"\\nDigit: {}\\n\".format(\n                             str(df.loc[rndperm[i], 'label'])))\n    ax.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.matshow(df.loc[rndperm[i], columns].values.reshape(\n        (8, 8)).astype(float))\nplt.show()","921244d1":"Let's consider the following dataset...","4e675cf8":"## Example 1: Food Consumption in Europe","730c8253":"**Can we create a scatterplot along all four dimensions?**","75c5b68d":"Now, let's project our original dataset onto the 2-dimensional space we derived above","2320831f":"What we can do is create a scatterplot of the first and second principal component and color each of the different types of digits with a different color. If we are lucky the same type of digits will be positioned (i.e., clustered) together in groups, which would mean that the first two principal components actually tell us a great deal about the specific types of digits.","0bdfe008":"## Introduction","0852dd9f":"Finally, we estimate the covariance matrix","038d58f5":"## Example 2: NBA players\n\nThe dataset `nba.csv` is from the 2017\u201318 NBA Regular season scraped from [Basketball-reference](https:\/\/www.basketball-reference.com\/leagues\/NBA_2018.html) and comprises of traditional NBA statistics (points, rebounds, age, etc) of 466 players.\n\n","c462d3d9":"Next, we estimate the mean for each variable and subtract to center the data","5891df11":"Take the eigenvectors $w_1, w_2$ of $C$ corresponding to the largest eigenvalue $\\lambda_1$, the second largest eigenvalue $\\lambda_2$","0e0ff56d":"## Example3: Handwritten digits","e4924c9c":"Mathematically, PCA is implemented as follows:\n<br>\n\n* Given $m$ data points, $\\{x_1,x_2,...,x_m\\} \\in R^n$, with their mean $\\mu = \\frac{1}{m}\\sum_{i=1}^{m}x_i$\n* Find a direction $w \\in R^n$ where $\\|w\\| \\leq 1$\n* Such that the variance of the data along the direction $w$ is maximized \n$$\\max_{w:\\|w\\|\\leq 1}\\underbrace{\\frac{1}{m}\\sum_{i=1}^{m}\\left(w^Tx_i-w^T\\mu\\right)^2}_{\\text{variance}}$$\n* It can be easily shown that this equals\n$$w^T\\underbrace{\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i-\\mu \\right)\\left(x_i-\\mu \\right)^T\\right)}_{\\text{covariance matrix C}}w = w^TCw$$\n**So, the optimization problem becomes $$\\max_{w:\\|w\\|\\leq 1}w^TCw$$**\n* This can be formulated as an eigenvalue problem\n    * Given a symmetric matrix $C \\in R^{n\\times n}$\n    * Find a row vector $w \\in R^n$ and $\\|w\\| = 1$\n    * Such that $Cw = \\lambda w$\n* There will be multiple solutions of $w_1, w_2, ...$ (eigenvectors) with different $\\lambda_1,\\lambda_2,...$ (eigenvalues)\n    * They are ortho-normal: $w_i^Tw_i = 1, w_i^Tw_j=0$<br>\n<br>\n\n**To find the top $k$ principal components, first find the mean and covariance matrix from the data $$ \\mu = \\frac{1}{m}\\sum_{i=1}^{m}x_i \\ and \\ C = \\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i-\\mu\\right)\\left(x_i-\\mu\\right)^T$$ calculate the first $k$ eigenvectors $w_1,w_2,...,w_k$ of $C$ corresponding to the largest eigenvalues $\\lambda_1,\\lambda_2,...,\\lambda_k$.**\n\n**Then compute the reduced representation \n$$z_i = \\left(\\begin{split}w_1^T\\left(x_i-\\mu\\right)\/\\sqrt{\\lambda_1}\\cr w_2^T\\left(x_i-\\mu\\right)\/\\sqrt{\\lambda_2}\\end{split}\\right)$$**","eba0dad6":"`PCA`, can be performed in the following steps:\n\n* calculate the mean of each column\n* center the value in each column by subtracting the mean column value\n* calculate covariance matrix of centered matrix\n* calculate eigendecomposition of the covariance (eigenvectors represent the magnitude of directions or components for the reduce subspace)","1534151c":"We can also overlay additional information on the chart above. For example, we could overlay the Minutes Played over the entire season (`MP`)","01728afc":"First, we normalize the data so each variable has a unit variance","6990066d":"### Algorithm","251d4b68":"`Unsupervised models` are trained models on data without labels. Instead of telling an unsupervised algorithm what it should be looking for in the data, the algorithm does the work itself, in a sense independently finding structure within the data. It is very common in machine learning tasks involving large number of features, that one is advised to use `Principal component analysis` (a.k.a `PCA`). \n\n`PCA` is a statistical technique for reducing the number of dimensions in our original dataset whilst retaining most information. It is using the correlation between some dimensions and tries to provide a minimum number of variables that keeps the maximum amount of variation or information about how the original data is distributed. It does not do this using guesswork but using hard mathematics and it uses something known as the `eigenvalues` and `eigenvectors` of the data-matrix.\n\n`Eigenvectors` (also known as `principal component`) are basically vectors that are linearly uncorrelated and carry the  variability in our data along their direction.","c163c83d":"## Overview of PCA","ed0f36c9":"### Mathematics of PCA","87baf155":"For this example, we will use the [Scikit-Learn implementation of PCA](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)"}}