{"cell_type":{"a603dae0":"code","02921925":"code","b1316a28":"code","1d6ea75f":"code","20615dc8":"code","af725327":"code","2091b574":"code","96f3a582":"code","5c1b477e":"code","d1561044":"code","bd29caee":"code","30892bf8":"code","3f51728b":"code","64d0fbf0":"code","acb1c704":"code","4fff0182":"code","c7b923af":"code","8658d487":"code","7b841d9b":"code","24a87ceb":"code","b0c53c21":"code","70553f3e":"code","43ec3574":"markdown","563244c6":"markdown","7d9f0906":"markdown","361e07f2":"markdown","a880d8ec":"markdown","c8e2b81d":"markdown","d6da7ac3":"markdown","612c0295":"markdown"},"source":{"a603dae0":"%config Completer.use_jedi = False\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","02921925":"data = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndata.head()","b1316a28":"data.isnull().sum()","1d6ea75f":"for feature in data.columns:\n    print(feature, ':', len(data[feature].unique()))","20615dc8":"discrete_features, continuous_features = [], []\nfor feature in data.columns:\n    if feature == 'output':\n        label = [feature]\n    elif len(data[feature].unique()) < 15:\n        discrete_features.append(feature)\n    else:\n        continuous_features.append(feature)\nprint('Discrete: ', discrete_features, '\\n', 'Continuous: ', continuous_features)","af725327":"curr_data = continuous_features + ['output']\ncorrelation = data.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(correlation, annot=True)\nplt.show()","2091b574":"fig, axes = plt.subplots(3, 2, figsize=(12,12))\naxes = axes.ravel()\n\nfor idx, ax in enumerate(axes[:-1]):\n    ax.hist(data[continuous_features[idx]])\n    ax.set_title(continuous_features[idx])\n\naxes[-1].set_visible(False)\nplt.tight_layout(pad=2)\nplt.show()","96f3a582":"fig, ax = plt.subplots(len(discrete_features), 2, figsize=(12,22))\n\nfor i in range(len(discrete_features)):\n    plt.title(discrete_features[i])\n    sns.countplot(ax=ax[i, 0], x=discrete_features[i], data=data)\n    sns.countplot(ax=ax[i, 1], x=discrete_features[i], hue='output', data=data)\nfig.tight_layout(pad=1)\nplt.show()","5c1b477e":"sns.countplot(x='output', data=data)","d1561044":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import chi2, SelectFromModel, SelectKBest\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nimport xgboost","bd29caee":"best_features = SelectKBest(chi2, k=10)\nfeatures_ranking = best_features.fit(data.drop(['output'], axis=1), data['output'])\nranking_dictionary = {}\nfor i in range(len(features_ranking.scores_)):\n    ranking_dictionary[data.columns[i]] = round(features_ranking.scores_[i], 3)\n\nasc_sort = sorted(ranking_dictionary.items(), key = lambda kv:(kv[1], kv[0]))\n\nfor i, j in asc_sort:\n    print(i, ':', j)","30892bf8":"feature_model = SelectFromModel(Lasso(alpha=0.05, random_state=0))\nfeature_model.fit(data.drop(['output'], axis=1), data['output'])","3f51728b":"mask = feature_model.get_support() \nfor i in range(len(mask)):\n    if not mask[i]:\n        print(data.drop(['output'], axis=1).columns[i])","64d0fbf0":"data1 = data.drop(['slp', 'age', 'sex', 'restecg'], axis=1)\n# data1 = data\n\nprint(data1.shape)","acb1c704":"X = data1.drop(['output'], axis=1)\ny = data1['output']\nX.head()","4fff0182":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape, X_test.shape)","c7b923af":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","8658d487":"model = LogisticRegression(solver='newton-cg')\nmodel.fit(X_train, y_train)\npredicted=model.predict(X_test)\nconf = plot_confusion_matrix(model, X_test, y_test)\n# conf_map = pd.DataFrame(conf, [''])\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint(classification_report(y_test, predicted))","7b841d9b":"model = SVC(C=7, kernel='linear', gamma=0.001)\nmodel.fit(X_train, y_train)\npredicted = model.predict(X_test)\nprint(\"The accuracy of SVM is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint(classification_report(y_test, predicted))\nconf = plot_confusion_matrix(model, X_train, y_train)","24a87ceb":"model3 = RandomForestClassifier(max_depth=4)\nscore3 = cross_val_score(model3, X_train, y_train, cv=10)\nprint(score3)\nprint(score3.mean())","b0c53c21":"param_combinations = {\n    'learning_rate': np.arange(0.05, 0.4, 0.05),\n    'max_depth': np.arange(3, 10),\n    'min_child_weight': np.arange(1, 7, 2),\n    'gamma': np.arange(0.0, 0.5, 0.1),\n}\nXGB = xgboost.XGBClassifier()\nperfect_params = RandomizedSearchCV(XGB, param_distributions=param_combinations, n_iter=6, n_jobs=-1, scoring='roc_auc')\nperfect_params.fit(X, y)\nperfect_params.best_params_","70553f3e":"model5 = xgboost.XGBClassifier(min_child_weight=3, max_depth=8, learning_rate=0.05, gamma=0.4, verbosity=0, use_label_encoder=False)\nscore = cross_val_score(model5, X, y, cv=5)\nprint(score)\nprint(score.mean())","43ec3574":"No missing values found.","563244c6":"### Data Analysis","7d9f0906":"### Discrete - Continuous Data Division","361e07f2":"### Observations\n\n- There is improper distribution between the classes of **sex** feature\n- Class 0 is almost half than that of class 1\n- The chances of getting heart attack is more in **class 0** of sex feature\n- Though the data for class 0 is less the amount of high chances of heart attack is relatively high.\n- In **caa**, people with 0 major vessel, are highly prone to heart attack.\n- Comparing the **cp** feature, people with non-anginal pain have high chances of heart attack.\n- People of exercise induced angina are less prone to heart attack.","a880d8ec":"### Observations\n* There is a high correlation of `output` and `thallach`, `oldpeak`\n* **oldpeak** is negatively correlated to **output** thus higher the oldpeak lower the chances of heart attack\n\n* **age** and **thalachh** are correlated.\n* **slp** and **oldpeak** are highly correlated.","c8e2b81d":"#### Based on the outputs of chi2 test and Lasso, Columns such as `fbs`, `age`, `slp` and `restecg` are least important.","d6da7ac3":"### Feature Selection","612c0295":"### Model Testing"}}