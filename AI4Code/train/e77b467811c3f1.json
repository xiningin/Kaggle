{"cell_type":{"f37107b2":"code","471c6bd4":"code","f3cf37a0":"code","3070a031":"code","5deb6d72":"code","c39e3187":"code","6b563469":"code","40883715":"code","dc7c9dbb":"code","e05550d5":"code","ac02342a":"code","b1018297":"code","9f0eecd3":"code","533f17ad":"code","27345eec":"code","5e6d9501":"code","a3154742":"code","b01982f6":"markdown","1bc36629":"markdown","70f5433a":"markdown","b3845f85":"markdown","59ab522f":"markdown"},"source":{"f37107b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","471c6bd4":"#Importing necessary libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline","f3cf37a0":"# DataSet as bos\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n\nbos = pd.read_csv(\"\/kaggle\/input\/boston-house-prices\/housing.csv\" , header=None, delimiter=r\"\\s+\", names=column_names)\n\nbos.columns","3070a031":"bos","5deb6d72":"bos.shape","c39e3187":"x = bos.iloc[:,0:13]\ny = bos[\"MEDV\"]","6b563469":"#Establishing correlation in data\n\nimport seaborn as sns\nnames = []\n#creating a correlation matrix\n\ncorrelations = bos.corr()\nsns.heatmap(correlations,square = True, cmap = \"inferno\")\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\nplt.show()","40883715":"# Splitting dataset as Test and Train\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.33, random_state = 5)","dc7c9dbb":"from sklearn.linear_model import LinearRegression\n\n#Fitting model to train and test data to linear regression model\nlr = LinearRegression()\n\nmodel = lr.fit(x_train, y_train)","e05550d5":"pred = lr.predict(x_test)","ac02342a":"# Creating a dataframe with the values\npd.DataFrame({\"Actual\": y_test, \"Predict\": pred})","b1018297":"# Visualization of the above dataframe in a scatterplot\n\nplt.scatter(y_test, pred)\nplt.xlabel('Y test')\nplt.ylabel('X test (or) Predicted')","9f0eecd3":"# Mean Squared Error (MAE)\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score as score\n\nprint(\"Accuracy :\", score(y_test, pred))\nprint(\"Mean Squared Error :\", mse(y_test, pred))","533f17ad":"from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,RandomForestRegressor,GradientBoostingRegressor\nfrom xgboost import XGBClassifier,XGBRFRegressor,XGBRegressor\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.linear_model import LogisticRegression,LinearRegression,SGDRegressor\nfrom sklearn.svm import SVC,SVR\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom lightgbm import LGBMRegressor","27345eec":"model_a = XGBRegressor(n_estimators = 1000)\nmodel_a.fit(x_train, y_train)","5e6d9501":"pred_a = model_a.predict(x_test)\n\n\n# Comparison on prediction values before and after using XGBoost\npd.DataFrame({\"Actual\": y_test, \"Predict\":pred, \"Predict_XGBOOST\": pred_a})","a3154742":"# Mean Squared error after Boosting\n\nprint(\"Accuracy :\", score(y_test, pred_a))\nprint(\"Mean Squared Error :\", mse(y_test,pred_a))","b01982f6":"### ** The Dataset Content**\nEach record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository1): CRIM: per capita crime rate by town\n\n1. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n2. INDUS: proportion of non-retail business acres per town\n3. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n4. NOX: nitric oxides concentration (parts per 10 million)\n5. RM: average number of rooms per dwelling\n6. AGE: proportion of owner-occupied units built prior to 1940\n7. DIS: weighted distances to \ufb01ve Boston employment centers\n8. RAD: index of accessibility to radial highways\n9. TAX: full-value property-tax rate per 10,000 dollars\n10. PTRATIO: pupil-teacher ratio by town \n11. B: 1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town\n12. LSTAT: % lower status of the population\n13. MEDV: Median value of owner-occupied homes in $1000s\n\nView Dataset: https:\/\/www.kaggle.com\/vikrishnan\/boston-house-prices","1bc36629":"Additionally i have added Boosting techniques to make a clarity on difference between prediction scores","70f5433a":"*\nThus with boosting tecniques the mean squared error is highly reduced to 11.07 from 28.53\n*\n\nI hope this kernal is useful to you to learn exploratory data analysis and regression problem.\n\nIf find this notebook help you to learn, Please Upvote.\n\nThank You!!","b3845f85":"## Explore more on Regression Algorithm and Xgboost","59ab522f":"### With XGBRegressor"}}