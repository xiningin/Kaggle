{"cell_type":{"a921b975":"code","f34bcf39":"code","c9b61cdc":"code","9c44f7ef":"code","751664fe":"code","fe6a6be8":"code","18ab2efd":"code","60d4975c":"code","7ab5799d":"code","e39cd6a3":"code","06dc47a7":"code","f7dd8699":"code","00c9629c":"code","d3f85358":"code","84591374":"code","3d05bba2":"code","ef7a8186":"code","654fb119":"code","05abbe52":"code","f44eb96a":"code","693cf8ee":"code","ac8e838a":"code","f129f8fb":"code","c2d396cb":"code","bb1bfc20":"code","97ce8545":"code","fe1d26fb":"code","b8999aea":"code","55d08f1b":"code","30759717":"markdown","f181b058":"markdown","2a47083f":"markdown","62360868":"markdown","a6e1b1b2":"markdown","06c9029b":"markdown","8164870e":"markdown"},"source":{"a921b975":"!pip install neptune-client","f34bcf39":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.sparse as sp\nfrom itertools import islice, cycle, product\nfrom more_itertools import pairwise\nimport copy\n\nfrom tqdm import tqdm_notebook\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport neptune.new as neptune\nimport pickle\nNEPTUNE_API_TOKEN = pickle.load(open('..\/input\/tokens\/neptune_token.pkl', 'rb'))        \n\nimport IPython\ndef display(*dfs):\n    for df in dfs:\n        IPython.display.display(df)","c9b61cdc":"def add_nan_to_cat(df_user, feature):\n    if df_users[feature].isna().sum() != 0:\n        df_users[feature] = df_users[feature].astype('category').cat.add_categories(feature+'_nan')\n        df_users[feature] = df_users[feature].fillna(feature+'_nan')\n        \n        \ndef compate_dfs_by_id(df, df_users, feature_id):\n    interaction_users = df[feature_id].unique()\n    users = df_users[feature_id].unique()\n\n    common_users = len(np.intersect1d(interaction_users, users))\n    users_only_interaction = len(np.setdiff1d(interaction_users, users))\n    users_only_features = len(np.setdiff1d(users, interaction_users))\n    total_users = common_users + users_only_features + users_only_interaction\n\n    print(f'\u041a\u043e\u043b-\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 - {total_users}')\n    print(f'\u041a\u043e\u043b-\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 c\u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\u043c\u0438 \u0438 \u0444\u0438\u0447\u0430\u043c\u0438 - {common_users} ({common_users \/ total_users * 100:.2f}%)')\n    print(f'\u041a\u043e\u043b-\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0442\u043e\u043b\u044c\u043a\u043e c \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\u043c\u0438 - {users_only_interaction} ({users_only_interaction \/ total_users * 100:.2f}%)')\n    print(f'\u041a\u043e\u043b-\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0442\u043e\u043b\u044c\u043a\u043e c \u0444\u0438\u0447\u0430\u043c\u0438 - {users_only_features} ({users_only_features \/ total_users * 100:.2f}%)')","9c44f7ef":"df = pd.read_csv('\/kaggle\/input\/mts-ml-summer-school\/interactions.csv')\ndf_users = pd.read_csv('\/kaggle\/input\/mts-ml-summer-school\/users.csv')\ndf_items = pd.read_csv('\/kaggle\/input\/mts-ml-summer-school\/items.csv')\nsubmit = pd.read_csv('\/kaggle\/input\/mts-ml-summer-school\/sample_submission.csv')\nto_add = pd.read_csv('\/kaggle\/input\/mtc-duplicated-titles\/add_by_title.csv', index_col=0)","751664fe":"df['start_date']= pd.to_datetime(df.start_date)\nassert (df.start_date.dt.month.unique() == np.arange(1,13)).all()\n\n# \u0435\u0441\u043b\u0438 \u043f\u0430\u0440\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c-\u043a\u043d\u0438\u0433\u0430 \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u0442\u0441\u044f, \u0442\u043e \u0431\u0443\u0434\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u0430\u043a\/\u043c\u0438\u043d \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u0444\u0438\u0447\u0435\u0439\nduplicates = df.duplicated(subset=['user_id', 'item_id'], keep=False)\ndf_duplicates = df[duplicates].sort_values(by=['user_id', 'item_id'])\ndf = df[~duplicates]\ndf_duplicates = df_duplicates.groupby(['user_id', 'item_id']).agg({\n                                        'progress': 'max',\n                                        'rating': 'max',\n                                        'start_date': 'min'\n                                        })\ndf = df.append(df_duplicates.reset_index(), ignore_index=True)\n\n# df['rating_user_mean'] = df.groupby('user_id').rating.transform(np.mean)\n# df['rating_fillna_user_mean'] = np.where(df.rating.isna(), df.rating_user_mean, df.rating)\n# \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0432\u0441\u0435\u043c \u043a\u043d\u0438\u0433\u0430\u043c \u0441\u0442\u0430\u0432\u0438\u0442 \u043e\u0446\u0435\u043d\u043a\u0443 \u043d\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 3, \u0435\u0433\u043e 3 \u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043c\u0430 \u0441 5. \u0423\u0440\u043e\u0432\u043d\u044f\u0435\u043c \u0442\u0430\u043a\u0438\u0435 \u0440\u0430\u0437\u043d\u0438\u0446\u044b \u0432\u044b\u0447\u0435\u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043f\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e\n# df['rating_norm_by_user'] = df.rating  - df.groupby('user_id').rating.transform(np.mean)\n# df['progress0_rating_na'] = (df.progress == 0) & (df.rating.notna())\n# df.progress = df.progress.clip(upper=100)","fe6a6be8":"compate_dfs_by_id(df, df_users, 'user_id')\n\n# \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0432 df_users, \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u0435\u0441\u0442\u044c \u0432 interection df\nnew_users = np.setdiff1d(df.user_id.unique(), df_users.user_id.unique())\ndf_users = df_users.append(pd.DataFrame(new_users, columns=['user_id']))\nassert df_users.user_id.nunique() >= df.user_id.nunique()\n\nadd_nan_to_cat(df_users, 'age')\nadd_nan_to_cat(df_users, 'sex')\n\n","18ab2efd":"# \u0435\u0441\u043b\u0438 \u0443\u043a\u0430\u0437\u0430\u043d\u0430 \u043f\u043e\u043b\u043d\u0430\u044f \u0434\u0430\u0442\u0430, \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0433\u043e\u0434\n# tmp = '\\d\\d.\\d\\d.(\\d\\d\\d\\d)'\n# temp = df_items.year.str.extract(tmp)\n# df_items.loc[temp.dropna().index.values, 'year'] = temp.dropna().values.ravel()\n# # df_items.year.loc[temp.dropna().index]\n\n# tmp = '\\d+ \\w+ (\\d\\d\\d\\d)'\n# temp = df_items.year.str.extract(tmp)\n# df_items.loc[temp.dropna().index.values, 'year'] = temp.dropna().values.ravel()\n\n# tmp = '\\w+ (\\d\\d\\d\\d)'\n# temp = df_items.year.str.extract(tmp)\n# df_items.loc[temp.dropna().index.values, 'year'] = temp.dropna().values.ravel()\n\n\n# # df_items.year = df_items.year.replace('\u0434\u043e \u043d.\u044d.', '\u0434\u043e \u043d. \u044d.')\n# tmp = '(\u0434\u043e \u043d. \u044d.)'\n# temp = df_items.year.str.extract(tmp)\n# df_items.loc[temp.dropna().index.values, 'year'] = temp.dropna().values.ravel()\n# tmp = '(\u0434\u043e \u043d.\u044d.)'\n# temp = df_items.year.str.extract(tmp)\n# df_items.loc[temp.dropna().index.values, 'year'] = '\u0434\u043e \u043d. \u044d.'\n# df_items.loc[:, 'year'] = df_items.year.replace('\u0434\u043e \u043d. \u044d.', '0')\n\n# df_items.year = df_items.year.str.replace('\u0433.', '').str.replace('\u0433','')\n# df_items.year = df_items.year.str.replace('\u043e\u043a\u043e\u043b\u043e', '')#.str.replace('\u0434\u043e','')\n# df_items.year = df_items.year.str.replace('II \u0432. \u043d. \u044d.', '100').str.replace('III\u2014VI \u0432\u0432. \u043d. \u044d.','250').str.replace('I \u0432\u0435\u043a \u043d. \u044d.','50')\n# df_items.year = df_items.year.str.replace('XII \u0432.', '600').str.replace('\u043f\u0435\u0440\u0432\u0430\u044f \u043f\u043e\u043b\u043e\u0432\u0438\u043d\u0430 XIX dtrf', '1825')\n# df_items.year = df_items.year.str.replace('[\u0430-\u044f\u0410-\u044f]','')\n\n# df_items.year = df_items.year.str.replace('\u2014',',').str.replace('\u2013', ',')\\\n#                     .str.replace('-', ',').str.split(',', expand=True)[0].str.strip().astype('float')\n# df_items.loc[[47464, 49129], 'year'] = 2017\n\n# year_cat = pd.cut(df_items.year, bins=(-1, 500,1000,1500,1800,2025), labels=(500,1000,1500,1800,2025))\n# df_items['year_with_category'] = np.where(df_items.year > 1800, df_items.year, year_cat)\n\ndf_items.rename(columns={'id': 'item_id'}, inplace=True)\n\ncol = ['genres', 'authors', 'year']\ndf_items[col] = df_items[col].astype('category')\n","60d4975c":"CV_FOLDS = 7","7ab5799d":"class TimeRangeSplit():\n    \"\"\"\n        https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.date_range.html\n    \"\"\"\n    def __init__(self, \n                 start_date, \n                 end_date=None, \n                 freq='D', \n                 periods=None, \n                 tz=None, \n                 normalize=False, \n                 closed=None, \n                 test_size=0,\n                 train_min_date=None,\n                 filter_cold_users=True, \n                 filter_cold_items=True, \n                 filter_already_seen=True):\n        \n        self.start_date = start_date\n        if end_date is None and periods is None:\n            raise ValueError(\"Either 'end_date' or 'periods' must be non-zero, not both at the same time.\")\n\n        self.end_date = end_date\n        self.freq = freq\n        self.periods = periods\n        self.tz = tz\n        self.normalize = normalize\n        self.closed = closed\n        self.test_size = test_size\n        self.train_min_date = pd.to_datetime(train_min_date, errors='raise')\n        self.filter_cold_users = filter_cold_users\n        self.filter_cold_items = filter_cold_items\n        self.filter_already_seen = filter_already_seen\n\n        self.date_range = pd.date_range(\n            start=start_date, \n            end=end_date, \n            freq=freq, \n            periods=periods, \n            tz=tz, \n            normalize=normalize, \n            closed=closed)\n        \n        print(self.date_range)\n\n        self.max_n_splits = max(0, len(self.date_range) - 1)\n        if self.max_n_splits == 0:\n            raise ValueError(\"Provided parametrs set an empty date range.\") \n\n    def split(self, \n              df, \n              user_column='user_id',\n              item_column='item_id',\n              datetime_column='start_date',\n              fold_stats=False):\n        df_datetime = df[datetime_column]\n        if self.train_min_date is not None:\n            train_min_mask = df_datetime >= self.train_min_date\n        else:\n            train_min_mask = df_datetime.notnull()\n        date_range = self.date_range[(self.date_range >= df_datetime.min()) & \n                                     (self.date_range <= df_datetime.max())]\n        \n        for start, end in pairwise(date_range):\n            end = end + pd.Timedelta(days=self.test_size-1) if self.test_size > 0 else end\n            fold_info = {\n                'Start date': start,\n                'End date': end\n            }\n#             print(start, end)\n            train_mask = train_min_mask & (df_datetime < start)\n            train_idx = df.index[train_mask]\n            if fold_stats:\n                fold_info['Train'] = len(train_idx)\n\n            test_mask = (df_datetime >= start) & (df_datetime < end)\n            test_idx = df.index[test_mask]\n            \n            if self.filter_cold_users:\n                new = np.setdiff1d(\n                    df.loc[test_idx, user_column].unique(), \n                    df.loc[train_idx, user_column].unique())\n                new_idx = df.index[test_mask & df[user_column].isin(new)]\n                test_idx = np.setdiff1d(test_idx, new_idx)\n                test_mask = df.index.isin(test_idx)\n                if fold_stats:\n                    fold_info['New users'] = len(new)\n                    fold_info['New users interactions'] = len(new_idx)\n\n            if self.filter_cold_items:\n                new = np.setdiff1d(\n                    df.loc[test_idx, item_column].unique(), \n                    df.loc[train_idx, item_column].unique()) # not in train\n                new_idx = df.index[test_mask & df[item_column].isin(new)]\n                test_idx = np.setdiff1d(test_idx, new_idx) # in train\n                test_mask = df.index.isin(test_idx)\n                if fold_stats:\n                    fold_info['New items'] = len(new)\n                    fold_info['New items interactions'] = len(new_idx)\n\n            if self.filter_already_seen:\n                user_item = [user_column, item_column]\n                train_pairs = df.loc[train_idx, user_item].set_index(user_item).index\n                test_pairs = df.loc[test_idx, user_item].set_index(user_item).index\n                intersection = train_pairs.intersection(test_pairs)\n                test_idx = test_idx[~test_pairs.isin(intersection)] # exclude intersection\n                if fold_stats:\n                    fold_info['Known interactions'] = len(intersection)\n\n            if fold_stats:\n                fold_info['Test'] = len(test_idx)\n\n            yield (train_idx, test_idx, fold_info)\n\n    def get_n_splits(self, df, datetime_column='date'):\n        df_datetime = df[datetime_column]\n        if self.train_min_date is not None:\n            df_datetime = df_datetime[df_datetime >= self.train_min_date]\n\n        date_range = self.date_range[(self.date_range >= df_datetime.min()) & \n                                     (self.date_range <= df_datetime.max())]\n\n        return max(0, len(date_range) - 1)","e39cd6a3":"CV_FOLDS = 7\n\ndf.sort_values('start_date', inplace=True)\n\nstart_date = df['start_date'].max().normalize() - pd.Timedelta(days=7-1) # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0444\u043e\u043b\u043e\u043c \u043f\u043e 2 \u0434\u043d\u044f \u0432 \u043a\u0430\u0436\u0434\u043e\u043c\ncv = TimeRangeSplit(start_date=start_date, \n                    # periods=CV_FOLDS+1, \n                    periods=3+1,\n#                     periods = 1+1,\n                    freq='2d', test_size=1, \n                    filter_cold_users=False) # \u0432 \u0442\u0435\u0441\u0442\u0435 \u0435\u0441\u0442\u044c \u043d\u043e\u0432\u044b\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u043b\u044f \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0438 \u0442\u0435\u0441\u0442\u0430 \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u043d\u043e\u0432\u044b\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\ncvs = list(cv.split(\n    df, \n    user_column='user_id',\n    item_column='item_id',\n    datetime_column='start_date',\n    fold_stats=True\n))\n\n[cc[2] for cc in cvs]","06dc47a7":"for train_idx, val_idx, _ in cv.split(df.sort_values('start_date'), datetime_column='start_date'):\n    train, val = df.loc[train_idx], df.loc[val_idx]\n    print(train.shape, train.user_id.nunique(), end='\\t')\n    print(val.shape, val.user_id.nunique(), end='\\n------------\\n')","f7dd8699":"for train_idx, val_idx, _ in cv.split(df.sort_values('start_date'), datetime_column='start_date'):\n    train, val = df.loc[train_idx], df.loc[val_idx]\n    fs = ['user_id', 'item_id']\n    known = train.set_index(fs)\n    val = val.set_index(fs)\n    display(val.index.isin(known.index).sum())","00c9629c":"submit.shape, submit.Id.nunique()","d3f85358":"# \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430, \u0447\u0442\u043e \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043d\u0435\u0442 \u043d\u043e\u0432\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\n# for train_idx, val_idx, _ in cvs:\n#     train, val = df.loc[train_idx], df.loc[val_idx]\n#     assert len(np.setdiff1d(val.user_id.unique(), train.user_id.unique())) == 0","84591374":"train_idx, val_idx, _ = cvs[0]\ntrain, val = df.loc[train_idx], df.loc[val_idx]\n\nprint(train.start_date.max(), val.start_date.min(), val.start_date.max())","3d05bba2":"users_inv_mapping = dict(enumerate(df['user_id'].unique()))\nusers_mapping = {v: k for k, v in users_inv_mapping.items()}\n\nitems_inv_mapping = dict(enumerate(df['item_id'].unique()))\nitems_mapping = {v: k for k, v in items_inv_mapping.items()}\n\nitem_titles = pd.Series(df_items['title'].values, index=df_items['item_id']).to_dict()\n\nlen(users_mapping), len(items_mapping)","ef7a8186":"def metrics_map(val, recs):\n    users_count = val.user_id.nunique()\n    \n    assert recs.item_id.apply(type).unique()[0] == list\n    recs.loc[:, 'item_id'] = recs.item_id.apply(lambda x: (x + [-1]*10)[:10])\n    assert recs.item_id.apply(len).value_counts().shape[0]== 1\n    \n    recs = recs.explode('item_id')\n    recs['rank'] = recs.groupby('user_id').cumcount() + 1\n\n    val_recs = val.set_index(['user_id', 'item_id']).join(recs.set_index(['user_id', 'item_id']))\n    val_recs = val_recs.sort_values(by=['user_id', 'rank'])\n    val_recs['users_item_count'] = val_recs.groupby(['user_id'], sort=False)['rank'].transform(np.size)\n\n    val_recs['cumulative_rank'] = val_recs.groupby(level='user_id').cumcount() + 1\n    val_recs['cumulative_rank'] = val_recs['cumulative_rank'] \/ val_recs['rank']\n\n    mapN = (val_recs[\"cumulative_rank\"] \/ val_recs[\"users_item_count\"]).sum() \/ users_count\n    print(f\"MAP@{TOP_N} = {mapN}\")\n    return mapN\n\n\n\ndef commit_to_neptune(map_mean, cv_metrics=None, params=None, tags=None):\n    run = neptune.init(project='declot\/MTS-recommendation', api_token=NEPTUNE_API_TOKEN, tags=tags)\n    if params is not None:\n        run[\"parameters\"] = params \n    run['oof\/map'] = map_mean\n    if cv_metrics is not None:\n        for m in cv_metrics:\n            run['cv_metrics'].log(m)\n    run.stop()","654fb119":"def recom_by_weights(x, item_id, start_date, max_K):\n    return x.groupby(item_id).agg({'weights': np.sum, start_date: np.max})\\\n                .sort_values(['weights', start_date],ascending=False).head(max_K).index.values","05abbe52":"class PopularRecommender():\n    def __init__(self, max_K=100, days=30, user_column='user_id', item_column='item_id', dt_column='start_date',\n                groupby=None, fit_na_as_common=False):\n        self.max_K = max_K\n        self.days = days\n        self.user_column = user_column\n        self.item_column = item_column\n        self.dt_column = dt_column\n        self.groupby = groupby\n        self.fit_na_as_common = fit_na_as_common\n        self.recommendations = []\n        self.known_items = {}\n        \n    def fit(self, df, df_users=None):\n        min_date = df[self.dt_column].max().normalize() - pd.DateOffset(days=self.days)\n        data = df[df[self.dt_column] > min_date]\n        recomm_common = recom_by_weights(data, self.item_column, self.dt_column, self.max_K)\n        self.recomm_common = recomm_common\n        self.df_users = df_users\n        self.known_items = df.groupby('user_id').item_id.apply(list).to_dict()\n        \n        if self.groupby is not None:\n            if df_users is None:\n                print('No df_users')\n                return None\n            \n            data = data.merge(df_users, on=self.user_column, how='left')\n            self.recommendations = data.groupby(self.groupby).apply(recom_by_weights,\\\n                                                    self.item_column, self.dt_column, self.max_K)\n            # \u0435\u0441\u043b\u0438 \u043d\u0435\u0442 \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0434\u043b\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438, \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u0449\u0435\u0435\n            na_mask = self.recommendations.isna()\n            self.recommendations.loc[na_mask] = self.recommendations[na_mask].apply(lambda x: recomm_common)\n            # \u043d\u0430 \u0441\u043b\u0443\u0447\u0430\u0439, \u0435\u0441\u043b\u0438 \u0441\u043f\u0438\u0441\u043e\u043a \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 \u0431\u0443\u0434\u0435\u0442 \u043a\u043e\u0440\u043e\u0442\u043a\u0438\u043c (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u043e\u043b\u044c\u043a\u043e 2 \u043a\u043d\u0438\u0433\u0438)\n            # \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u043e\u0431\u0449\u0443\u044e \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0430\u0446\u0438\u044e\n            self.recommendations = self.recommendations.apply(lambda x: np.concatenate((x, recomm_common)))\n            # na \u0432 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\u0445\n            if self.fit_na_as_common:\n                na_mask = (self.recommendations.reset_index()[self.groupby]=='nan').sum(axis=1)!=0\n                self.recommendations.loc[na_mask.values] = self.recommendations[na_mask.values].apply(lambda x: recomm_common)\n        else:\n            self.recommendations = recomm_common\n        \n    def recommend(self, users=None, N=10, drop_known=False):\n        recs = self.recommendations.tolist()#[:N]\n        \n        if users is None:\n            if self.groupby is not None:\n                print('For recomendations based on groupby needs used_id')\n                return None\n            return recs[:N]\n        else:\n            if self.groupby is not None:\n                recoms = self.recommendations.apply(lambda x: x[:N]) # \u0442\u043e\u043b\u044c\u043a\u043e N \u043f\u0435\u0440\u0432\u044b\u0445 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439\n                recoms.name = 'recoms'\n                recoms = recoms.reset_index()\n                recoms.loc[:,self.groupby] = recoms[self.groupby].astype('category')\n                data = users.to_frame().merge(self.df_users, on=self.user_column, how='left') # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043f\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f \u043d\u0430 \u0433\u0440\u0443\u043f\u043f\u044b\n                data = data.merge(recoms, on=self.groupby, how='left') # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0438\u0438 \u0441 \u0433\u0440\u0443\u043f\u043f\u043e\u0439\n                # \u0435\u0441\u043b\u0438 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430, \u0442\u043e \u043f\u0440\u0435\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0431\u0443\u0434\u0443\u0442 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u043c\u0438. \u0417\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0438\u0445 \u043e\u0431\u0449\u0438\u043c\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u043c\u0438 \u043f\u043e \u0432\u0441\u0435\u043c\u0443 \u043d\u0430\u0431\u043e\u0440\u0443\n                na_mask = data.iloc[:, -1].isna()\n                data.loc[na_mask, 'recoms'] = data.loc[na_mask, 'recoms'].apply(lambda x: self.recomm_common[:N])\n                recs = data.iloc[:, -1].apply(lambda x: x.tolist()).values\n            \n            else: # \u0435\u0441\u043b\u0438 \u043d\u0435 \u0431\u044b\u043b\u043e \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f \u043d\u0430 \u0433\u0440\u0443\u043f\u043f\u044b\n                recs = list(islice(cycle([recs]), len(users))) # \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u043e\u0431\u0449\u0435\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n            \n            if drop_known:\n                data = users.to_frame()\n                for u in users:\n                    if u not in self.known_items.keys():\n                        self.known_items[u] = []\n                data['item_id'] = recs\n                data['item_id'] = data.apply(lambda x: [f for f in x.item_id \\\n                                                        if f not in self.known_items[x.user_id]][:N], axis=1)\n                return data.item_id\n            else:\n                return recs","f44eb96a":"df_users_nan = df_users.copy()\n# df_users_nan.genre.cat.rename_categories({'no_genre': 'nan'},inplace=True)\ndf_users_nan.sex.cat.rename_categories({'sex_nan': 'nan'},inplace=True)\ndf_users_nan.age.cat.rename_categories({'age_nan': 'nan'},inplace=True)","693cf8ee":"TOP_N = 10 \n\ntemp = train.copy()\ntemp['weights'] = temp.rating.copy().fillna(3.5)\ntemp['weights'] = temp.weights \/ 5\n\npop_model = PopularRecommender(days=21)\npop_model.fit(temp, df_users_nan)\n\nrecs = pd.DataFrame({'user_id': val['user_id'].unique()})\nrecs['item_id'] = pop_model.recommend(recs['user_id'], N=TOP_N, drop_known=True)\nmetrics_map(val, recs)","ac8e838a":"def preprocess_data(df, df_users):\n    temp = df.merge(df_items.set_index('item_id').genres.str.replace(', ', '\/').str.split(',').explode(),\n                on='item_id', how='left')\n    temp = pd.crosstab(temp.user_id, temp.genres) # counts each genre for each users\n\n    genre = temp.idxmax(axis=1) # find argmax genre pre user\n    no_pop_genre = temp.apply(lambda x: (x == x.max()).sum() > 1, axis=1) # find if 2 or more genres have the same counts\n    genre[no_pop_genre] = 'no_genre'\n    genre.name = 'genre'\n\n    df_users = df_users.merge(genre, on='user_id', how='left')\n    df_users.genre.fillna('no_genre', inplace=True)\n    df_users.genre = df_users.genre.astype('category')\n    \n    return df, df_users","f129f8fb":"df_users_cv = []\nfor train_idx, val_idx, _ in cv.split(df):\n    train, val = df.loc[train_idx], df.loc[val_idx]\n    _, df_users_copy = preprocess_data(train, df_users.copy())\n    df_users_cv.append(df_users_copy)","c2d396cb":"temp = train.copy()\n# temp['weights'] = temp.rating.copy().fillna(3.5)\n# temp['weights'] = temp.weights \/ 5\ntemp['weights'] = 1\n\npop_model = PopularRecommender(days=20, groupby='genre')\npop_model.fit(temp, df_users_cv[0])\n# pop_model.recomm_common\n\nrecs = pd.DataFrame({'user_id': val['user_id'].unique()})\nrecs['item_id'] = pop_model.recommend(recs['user_id'], N=TOP_N, drop_known=True)\nmetrics_map(val, recs)","bb1bfc20":"def preprocess_none(df, df_users): return df, df_users\ndef preprocess_recom_none(x, common): return x\n\n\ndef model_Popular_cv(model, df, df_users_cv, drop_known=True, \n                     preprocess_data=preprocess_none,\n                     preprocess_recom=preprocess_recom_none):\n    metrics = []\n    for (train_idx, val_idx, _), df_users in zip(cv.split(df), df_users_cv):\n        train, val = df.loc[train_idx], df.loc[val_idx]\n        train, df_users_copy = preprocess_data(train, df_users.copy())\n\n        model.fit(train, df_users_copy)\n\n        recs = pd.DataFrame({'user_id': val['user_id'].unique()})\n        recs['item_id'] = model.recommend(recs['user_id'], N=TOP_N, drop_known=drop_known)\n#         common = model.recomm_common\n#         recs['item_id'] = recs['item_id'].apply(preprocess_recom, common)\n        \n        metrics.append(metrics_map(val, recs))\n\n    map_mean = np.mean(metrics)\n    print(f'mean MAP@10 = {map_mean}')\n    print(f'std MAP@10 = {np.std(metrics)}')\n    \n    return metrics\n\ndef model_Popular_to_submit(model, df, drop_known=True, \n                            preprocess_data=preprocess_none, name=''):\n    \n    df, df_users_copy = preprocess_data(df, df_users.copy())\n    model.fit(df, df_users_copy)\n\n    user_id = submit['Id'].copy()\n    user_id.rename('user_id',inplace=True)\n    recs = model.recommend(user_id, N=TOP_N, drop_known=drop_known)\n    submit['Predicted'] = recs\n    submit['Predicted'] = submit.Predicted.apply(lambda x: ' '.join(map(str, x)) if x is not np.nan else '')\n    submit.to_csv(f'submit{name}.csv', index=False)\n    return submit","97ce8545":"temp = df.copy()\n# temp['weights'] = temp.rating.copy().fillna(3.5)\n# temp['weights'] = temp.weights \/ 5\ntemp['weights'] = 1\n# temp = temp[temp.progress > 0]\n\nparams = dict(groupby=['genre'],\n              fit_na_as_common=False,\n              days = 20)\n\nmodel = PopularRecommender(**params)\nmetrics = model_Popular_cv(model, temp, df_users_cv, drop_known=True)\n\n# # \u0437\u0430\u043f\u0438\u0441\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432 \u043d\u0435\u043f\u0442\u0443\u043d\n# tags = ['Pop']\n# map_mean = np.mean(metrics)\n# commit_to_neptune(map_mean, metrics, params, tags)","fe1d26fb":"# \u0434\u043b\u044f \u043f\u043e\u0434\u0431\u043e\u0440\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n\n# temp = df.copy()\n# temp['weights'] = 1\n\n# params = dict(groupby=['genre'],\n#               fit_na_as_common=False,\n#               days = 21)\n\n# # model = PopularRecommender(**params)\n# # metrics = model_Popular_cv(model, temp, drop_known=True)\n\n# results = []\n# for days in tqdm(range(15, 30+1)):\n#     params['days'] = days\n#     model = PopularRecommender(**params)\n#     metrics = model_Popular_cv(model, temp, drop_known=True)\n#     to_add = dict(days=days, \n#                   map_mean = np.mean(metrics),\n#                   map_std=np.std(metrics))\n#     results.append(to_add)\n    \n# pd.DataFrame(results).sort_values('map_mean', ascending=False)","b8999aea":"# \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0432\u0441\u0435\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043f\u043e\u043b\u0443\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439\nparams = dict(groupby=['genre'],\n              fit_na_as_common=False,\n              days=20)\n\nmodel = PopularRecommender(**params)\ns = model_Popular_to_submit(model, temp, drop_known=True, \n                            preprocess_data=preprocess_data,\n                            name='_pop_genre') # submit this!!!!!\ns","55d08f1b":"# temp = df.copy()\n# temp = temp.append(to_add, ignore_index=True)\n# temp = temp.drop_duplicates()\n# temp.rating = temp.rating.astype('float')\n\n# temp.start_date = pd.to_datetime(temp.start_date) \n# temp.sort_values('start_date', inplace=True)\n\n# duplicates = temp.duplicated(subset=['user_id', 'item_id'], keep=False)\n# temp_duplicates = temp[duplicates].sort_values(by=['user_id', 'item_id'])\n# temp = temp[~duplicates]\n\n\n# temp_duplicates = temp_duplicates.groupby(['user_id', 'item_id']).agg({\n#                                         'progress': 'max',\n#                                         'rating': 'max',\n#                                         'start_date': 'min',\n# #     rating_user_mean\trating_fillna_user_mean\trating_norm_by_user\tprogress0_rating_na\n#                                         })\n# temp = temp.append(temp_duplicates.reset_index(), ignore_index=True)\n","30759717":"# Baseline model\n","f181b058":"# Validation","2a47083f":"[Neptune dashboard with different params](https:\/\/app.neptune.ai\/declot\/MTS-recommendation\/experiments?split=tbl&dash=leaderboard&viewId=f3009ad0-cb82-41b0-8a2f-5f8252de5a2b)\n\u0417\u0434\u0435\u0441\u044c \u0438\u0441\u0442\u043e\u0440\u0438\u044f \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0440\u0430\u0437\u043d\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u043b\u044f \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438","62360868":"# Preprocessing","a6e1b1b2":"\u041a\u0440\u0430\u0442\u043a\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438:\n* \u0435\u0441\u043b\u0438 \u043d\u0435\u0442 groupby - \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0435 \u043f\u043e \u0432\u0441\u0435\u043c \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f\u043c\n* groupby - \u0441\u043f\u0438\u0441\u043e\u043a \u0444\u0438\u0447\u0435\u0439, \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c 1 \u0438\u043b\u0438 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u043e\u043f\u043b\u0443\u044f\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0433\u0440\u0443\u043f\u043f\u044b\n* \u043a \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044f\u043c \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0437 \u0433\u0440\u0443\u043f\u043f \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0431\u0449\u0430\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044f. \u0422.\u043e. \u0435\u0441\u043b\u0438 \u0434\u043b\u044f \u0433\u0440\u0443\u043f\u043f\u044b \u0431\u0443\u0434\u0435\u0442 \u0432\u0441\u0435\u0433\u043e 2 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u043d\u0434\u0430\u0446\u0438\u0438 \u043c\u044b \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u044c\u043c \u0438\u0445 \u0434\u043e 10 \u0438\u043b\u0438 \u0434\u043e\u043b\u044c\u0448\u0435 \u043e\u0431\u0449\u0435\u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u043c\u0438\n* \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0435\u0439\n* fit_na_as_common - \u0435\u0441\u043b\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043b\u044e\u0431\u043e\u0439 \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438 - \u043f\u0440\u043e\u043f\u0443\u0441\u043a, \u0442\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u044d\u0442\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u043e\u0431\u0449\u0435\u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0435, \u0430 \u043d\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0435 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b (\u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0441 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u043c\u0438 \u043d\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b, \u043a\u0430\u043a \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u0430\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \u0438 \u043b\u0443\u0447\u0448\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043e\u0431\u0449\u0435\u0435)\n\n* \u043f\u0440\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0438, \u0435\u0441\u043b\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043d\u043e\u0432\u044b\u0439 \u0438 \u043d\u0435\u0442 \u0444\u0438\u0447\u0435\u0439 \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0435\u0433\u043e \u0433\u0440\u0443\u043f\u043f\u044b - \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f \u043e\u0431\u0449\u0435\u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0435\n* drop_known - \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0438\u0437 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0443\u0436\u0435 \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u043c \u043a\u043d\u0438\u0433","06c9029b":"\u0423 \u043a\u043d\u0438\u0433\u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043c\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043d\u044b\u0445 \u0438\u0437\u0434\u0430\u043d\u0438\u0439, \u0441\u043e\u043e\u0442\u0432\u0432\u0435\u0442\u0441\u0432\u0443\u044e\u0449\u0438\u0445 \u0440\u0430\u0437\u043d\u044b\u043c \u0433\u043e\u0434\u0430\u043c, \u043d\u043e \u043e\u0431\u044b\u0447\u043d\u043e \u043e\u0442 \u0438\u0437\u0434\u0430\u043d\u0438\u044e \u043a \u0438\u0437\u0434\u0430\u043d\u0438\u044e \u0442\u0435\u043a\u0441 \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f, \u0437\u0430 \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435\u043c \u043d\u0435\u0434\u043e\u043b\u044c\u0448\u0438\u0445 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u043e\u043a. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0437\u0430 \u0433\u043e\u0434 \u043c\u043e\u0436\u043d\u043e:\n* \u0432\u0437\u044f\u0442\u044c \u0441\u0430\u043c\u044b\u0439 \u0440\u0430\u043d\u043d\u0438\u0439 \u0433\u043e\u0434 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435\n* \u0438\u043b\u0438 \u0441\u0430\u043c\u044b\u0439 \u0440\u0430\u043d\u043d\u0438\u0439 \u0433\u043e\u0434, \u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u043e\u043e\u0431\u0449\u0435 \u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n\n\u041f\u0441: \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0442\u043e\u0436\u0435 \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u043d\u0430 \u043f\u0435\u0440\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435","8164870e":"## Cross-val"}}