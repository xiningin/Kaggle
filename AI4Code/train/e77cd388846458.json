{"cell_type":{"9c6931be":"code","e723f488":"code","662bd8ad":"code","c0c2f188":"code","42092746":"code","e3fc7c20":"code","1f97b0a0":"code","2cec1863":"code","fb781d5d":"code","53291008":"code","035f0d3d":"code","e5aecaef":"code","5e0c91eb":"code","29ecb179":"code","8c04bdea":"code","5e675aa7":"code","28d2dba5":"code","06919b27":"code","e585235c":"code","bfde8789":"code","cdbce3d2":"code","2340a965":"code","37eb619d":"code","6bda8ae1":"code","ccfa286c":"code","1226aed4":"code","21750381":"code","732418b8":"code","a957f965":"markdown","e2146d60":"markdown","7a4b730a":"markdown","c057df85":"markdown","a41a2c00":"markdown","c83de6a0":"markdown","c3ca03ce":"markdown","e2a0f8fb":"markdown","70d16d3d":"markdown","8f620308":"markdown","441b0c29":"markdown","edee4e91":"markdown","7e5610ac":"markdown","2499a727":"markdown","d4dca4c4":"markdown","69721d5d":"markdown","ee85abc9":"markdown","b6215956":"markdown","5e9eb30b":"markdown","c0baef16":"markdown","ce7b9f82":"markdown","f843c348":"markdown","6b484811":"markdown","42c55a97":"markdown","db593da9":"markdown","1b0ace26":"markdown","49c499e9":"markdown","b64ed38f":"markdown","cee2c24c":"markdown","8aa71a4d":"markdown","4f78a6d1":"markdown","baab5a51":"markdown","f109926e":"markdown","2a185d15":"markdown","4e5c7498":"markdown","13c3d505":"markdown"},"source":{"9c6931be":"# Importing the Keras libraries and packages\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.vgg19 import VGG19\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom tensorflow.keras.layers import Input, Lambda ,Dense ,Flatten ,Dropout\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport cv2\n\ntrain_dir = \"\/kaggle\/input\/american-sign-language-recognition\/training_set\"\neval_dir = \"\/kaggle\/input\/american-sign-language-recognition\/test_set\"","e723f488":"#Helper function to load images from given directories\ndef load_images(directory):\n    images = []\n    labels = []\n    for idx, label in enumerate(uniq_labels):\n        for file in os.listdir(directory + \"\/\" + label):\n            filepath = directory + \"\/\" + label + \"\/\" + file\n            image = cv2.resize(cv2.imread(filepath), (64, 64))\n            images.append(image)\n            labels.append(idx)\n    images = np.array(images)\n    labels = np.array(labels)\n    return(images, labels)","662bd8ad":"import keras\n\nuniq_labels = sorted(os.listdir(train_dir))\nimages, labels = load_images(directory = train_dir)\n\nif uniq_labels == sorted(os.listdir(eval_dir)):\n    X_eval, y_eval = load_images(directory = eval_dir)","c0c2f188":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, stratify = labels)\n\nn = len(uniq_labels)\ntrain_n = len(X_train)\ntest_n = len(X_test)\n\nprint(\"Total number of symbols: \", n)\nprint(\"Number of training images: \" , train_n)\nprint(\"Number of testing images: \", test_n)\n\neval_n = len(X_eval)\nprint(\"Number of evaluation images: \", eval_n)","42092746":"uniq_labels","e3fc7c20":"y_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\ny_eval = keras.utils.to_categorical(y_eval)","1f97b0a0":"print(y_train[0])\nprint(len(y_train[0]))","2cec1863":"X_train = X_train.astype('float32')\/255.0\nX_test = X_test.astype('float32')\/255.0\nX_eval = X_eval.astype('float32')\/255.0","fb781d5d":"#Initialising vgg16 \nclassifier_vgg16 = VGG16(input_shape= (64,64,3),include_top=False,weights='imagenet')","53291008":"#Initialising vgg16 \nclassifier_resnet = ResNet50(input_shape= (64,64,3),include_top=False,weights='imagenet')","035f0d3d":"#don't train existing weights for vgg16\nfor layer in classifier_vgg16.layers:\n    layer.trainable = False\n\n#don't train existing weights for resnet50\nfor layer in classifier_resnet.layers:\n    layer.trainable = False","e5aecaef":"classifier1 = classifier_vgg16.output#head mode\nclassifier1 = Flatten()(classifier1)#adding layer of flatten\nclassifier1 = Dense(units=256, activation='relu')(classifier1)\nclassifier1 = Dropout(0.6)(classifier1)\nclassifier1 = Dense(units=40, activation='softmax')(classifier1)\n\nmodel = Model(inputs = classifier_vgg16.input , outputs = classifier1)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","5e0c91eb":"classifier2 = classifier_resnet.output#head mode\nclassifier2 = Flatten()(classifier2)#adding layer of flatten\nclassifier2 = Dropout(0.6)(classifier2)\nclassifier2 = Dense(units=40, activation='softmax')(classifier2)\n\nmodel2 = Model(inputs = classifier_resnet.input , outputs = classifier2)\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","29ecb179":"model.summary()","8c04bdea":"model2.summary()","5e675aa7":"#fit the model\n#it will take some time to train\nhistory = model.fit(X_train, y_train, epochs =5, batch_size = 64,validation_data=(X_test,y_test))","28d2dba5":"#fit the model\n#it will take some time to train\nhistory2 = model2.fit(X_train, y_train, epochs =5, batch_size = 64,validation_data=(X_test,y_test))","06919b27":"# Saving the model of vgg16\nmodel.save('model_vgg16.h5')\n# Saving the model of resnet\nmodel2.save('model_resnet.h5')","e585235c":"score = model.evaluate(x = X_test, y = y_test, verbose = 0)\nprint('Accuracy for test images:', round(score[1]*100, 3), '%')\nscore = model.evaluate(x = X_eval, y = y_eval, verbose = 0)\nprint('Accuracy for evaluation images:', round(score[1]*100, 3), '%')","bfde8789":"score = model2.evaluate(x = X_test, y = y_test, verbose = 0)\nprint('Accuracy for test images:', round(score[1]*100, 3), '%')\nscore = model2.evaluate(x = X_eval, y = y_eval, verbose = 0)\nprint('Accuracy for evaluation images:', round(score[1]*100, 3), '%')","cdbce3d2":"#vgg16\nimport matplotlib.pyplot as plt\n\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy of vgg16')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","2340a965":"#vgg16\nimport matplotlib.pyplot as plt\n\n# summarize history for accuracy\nplt.plot(history2.history['accuracy'])\nplt.plot(history2.history['val_accuracy'])\nplt.title('model accuracy of vgg16')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\n\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","37eb619d":"#Helper function to plot confusion matrix\ndef plot_confusion_matrix(y, y_pred):\n    y = np.argmax(y, axis = 1)\n    y_pred = np.argmax(y_pred, axis = 1)\n    cm = confusion_matrix(y, y_pred)\n    plt.figure(figsize = (24, 20))\n    ax = plt.subplot()\n    plt.imshow(cm, interpolation = 'nearest', cmap = plt.cm.Purples)\n    plt.colorbar()\n    plt.title(\"Confusion Matrix\")\n    tick_marks = np.arange(len(uniq_labels))\n    plt.xticks(tick_marks, uniq_labels, rotation=45)\n    plt.yticks(tick_marks, uniq_labels)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    ax.title.set_fontsize(20)\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n    limit = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment = \"center\",color = \"white\" if cm[i, j] > limit else \"black\")\n    plt.show()\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ny_test_pred = model.predict(X_test, batch_size = 64, verbose = 0)\nplot_confusion_matrix(y_test, y_test_pred)","6bda8ae1":"y_eval_pred = model.predict(X_eval, batch_size = 512,verbose = 0)\nplot_confusion_matrix(y_eval, y_eval_pred)","ccfa286c":"def plot_confusion_matrix(y, y_pred):\n    y = np.argmax(y, axis = 1)\n    y_pred = np.argmax(y_pred, axis = 1)\n    cm = confusion_matrix(y, y_pred)\n    plt.figure(figsize = (24, 20))\n    ax = plt.subplot()\n    plt.imshow(cm, interpolation = 'nearest', cmap = plt.cm.Purples)\n    plt.colorbar()\n    plt.title(\"Confusion Matrix\")\n    tick_marks = np.arange(len(uniq_labels))\n    plt.xticks(tick_marks, uniq_labels, rotation=45)\n    plt.yticks(tick_marks, uniq_labels)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    ax.title.set_fontsize(20)\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n    limit = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment = \"center\",color = \"white\" if cm[i, j] > limit else \"black\")\n    plt.show()\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ny_test_pred = model2.predict(X_test, batch_size = 64, verbose = 0)\nplot_confusion_matrix(y_test, y_test_pred)","1226aed4":"y_eval_pred = model2.predict(X_eval, batch_size = 64, verbose = 0)\nplot_confusion_matrix(y_eval, y_eval_pred)","21750381":"# for only one prediction\nimport numpy as np\nfrom keras.preprocessing import image\ntest_image = image.load_img('\/kaggle\/input\/american-sign-language-recognition\/test_set\/best of luck\/11.png',target_size=(64,64))\nplt.imshow(test_image)\ntest_image = image.img_to_array(test_image)\ntest_image = np.expand_dims(test_image, axis=0)\nresult = model.predict(test_image)\n\n\nif result[0][0] == 1:\n    prediction = '1'\nelif result[0][1] == 1:\n    prediction = '10'\nelif result[0][2] == 1:\n    prediction = '2'\nelif result[0][3] == 1:\n    prediction = '3'\nelif result[0][4] == 1:\n    prediction = '4'\nelif result[0][5] == 1:\n    prediction = '5'\nelif result[0][6] == 1:\n    prediction = '6'\nelif result[0][7] == 1:\n    prediction = '7'\nelif result[0][8] == 1:\n    prediction = '8'\nelif result[0][9] == 1:\n    prediction = '9'\nelif result[0][10] == 1:\n    prediction = 'A'\nelif result[0][11] == 1:\n    prediction = 'B'\nelif result[0][12] == 1:\n    prediction = 'C'\nelif result[0][13] == 1:\n    prediction = 'D'\nelif result[0][14] == 1:\n    prediction = 'E'\nelif result[0][15] == 1:\n    prediction = 'F'\nelif result[0][16] == 1:\n    prediction = 'G'\nelif result[0][17] == 1:\n    prediction = 'H'\nelif result[0][18] == 1:\n    prediction = 'I'\nelif result[0][19] == 1:\n    prediction = 'J'\nelif result[0][20] == 1:\n    prediction = 'K'\nelif result[0][21] == 1:\n    prediction = 'L'\nelif result[0][22] == 1:\n    prediction = 'M'\nelif result[0][23] == 1:\n    prediction = 'N'\nelif result[0][24] == 1:\n    prediction = 'O'\nelif result[0][25] == 1:\n    prediction = 'P'\nelif result[0][26] == 1:\n    prediction = 'Q'\nelif result[0][27] == 1:\n    prediction = 'R'\nelif result[0][28] == 1:\n    prediction = 'S'\nelif result[0][29] == 1:\n    prediction = 'T'\nelif result[0][30] == 1:\n    prediction = 'U'\nelif result[0][31] == 1:\n    prediction = 'V'\nelif result[0][32] == 1:\n    prediction = 'W'\nelif result[0][33] == 1:\n    prediction = 'X'\nelif result[0][34] == 1:\n    prediction = 'Y'\nelif result[0][35] == 1:\n    prediction = 'Z'\nelif result[0][36] == 1:\n    prediction = 'best of luck'\nelif result[0][37] == 1:\n    prediction = 'fuck you'\nelif result[0][38] == 1:\n    prediction = 'i love you'\nelse:\n    prediction = '  '\n    \nprint(prediction)","732418b8":"# for only one prediction\nimport numpy as np\nfrom keras.preprocessing import image\ntest_image = image.load_img('\/kaggle\/input\/american-sign-language-recognition\/test_set\/space\/10.png',target_size=(64,64))\nplt.imshow(test_image)\ntest_image = image.img_to_array(test_image)\ntest_image = np.expand_dims(test_image, axis=0)\nresult = model2.predict(test_image)\n\n\nif result[0][0] == 1:\n    prediction = '1'\nelif result[0][1] == 1:\n    prediction = '10'\nelif result[0][2] == 1:\n    prediction = '2'\nelif result[0][3] == 1:\n    prediction = '3'\nelif result[0][4] == 1:\n    prediction = '4'\nelif result[0][5] == 1:\n    prediction = '5'\nelif result[0][6] == 1:\n    prediction = '6'\nelif result[0][7] == 1:\n    prediction = '7'\nelif result[0][8] == 1:\n    prediction = '8'\nelif result[0][9] == 1:\n    prediction = '9'\nelif result[0][10] == 1:\n    prediction = 'A'\nelif result[0][11] == 1:\n    prediction = 'B'\nelif result[0][12] == 1:\n    prediction = 'C'\nelif result[0][13] == 1:\n    prediction = 'D'\nelif result[0][14] == 1:\n    prediction = 'E'\nelif result[0][15] == 1:\n    prediction = 'F'\nelif result[0][16] == 1:\n    prediction = 'G'\nelif result[0][17] == 1:\n    prediction = 'H'\nelif result[0][18] == 1:\n    prediction = 'I'\nelif result[0][19] == 1:\n    prediction = 'J'\nelif result[0][20] == 1:\n    prediction = 'K'\nelif result[0][21] == 1:\n    prediction = 'L'\nelif result[0][22] == 1:\n    prediction = 'M'\nelif result[0][23] == 1:\n    prediction = 'N'\nelif result[0][24] == 1:\n    prediction = 'O'\nelif result[0][25] == 1:\n    prediction = 'P'\nelif result[0][26] == 1:\n    prediction = 'Q'\nelif result[0][27] == 1:\n    prediction = 'R'\nelif result[0][28] == 1:\n    prediction = 'S'\nelif result[0][29] == 1:\n    prediction = 'T'\nelif result[0][30] == 1:\n    prediction = 'U'\nelif result[0][31] == 1:\n    prediction = 'V'\nelif result[0][32] == 1:\n    prediction = 'W'\nelif result[0][33] == 1:\n    prediction = 'X'\nelif result[0][34] == 1:\n    prediction = 'Y'\nelif result[0][35] == 1:\n    prediction = 'Z'\nelif result[0][36] == 1:\n    prediction = 'best of luck'\nelif result[0][37] == 1:\n    prediction = 'fuck you'\nelif result[0][38] == 1:\n    prediction = 'i love you'\nelse:\n    prediction = 'space'\n    \nprint(prediction)","a957f965":"This conversion will turn the one-dimensional array of labels into a two-dimensional array. Each row in the two-dimensional array of one-hot encoded labels corresponds to a different label.","e2146d60":"![American_Sign_Language_ASL.svg](attachment:American_Sign_Language_ASL.svg)","7a4b730a":"# What is a Pre-trained Model?","c057df85":"*VGG16 is a convolution neural net (CNN ) architecture which was used to win ILSVR(Imagenet) competition in 2014. It is considered to be one of the excellent vision model architecture till date. Most unique thing about VGG16 is that instead of having a large number of hyper-parameter they focused on having convolution layers of 3x3 filter with a stride 1 and always used same padding and maxpool layer of 2x2 filter of stride 2. It follows this arrangement of convolution and max pool layers consistently throughout the whole architecture. In the end it has 2 FC(fully connected layers) followed by a softmax for output. The 16 in VGG16 refers to it has 16 layers that have weights. This network is a pretty large network and it has about 138 million (approx) parameters.*","a41a2c00":"# 1. American Sign Language (ASL)","c83de6a0":"A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.","c3ca03ce":"**Next I will plot the confusion matrix for the evaluation images**","e2a0f8fb":"# initializing all the models","70d16d3d":"resnet50 accuracy and loss plot","8f620308":"![CtSIsMP.jpg](attachment:CtSIsMP.jpg)","441b0c29":"![download.png](attachment:download.png)","edee4e91":"# 2. Loading the data","7e5610ac":"*Some of the major problems faced by a person who are unable to speak is they cannot express their emotion as freely in this world. Utilize that voice recognition and voice search systems in smartphone(s).Audio results cannot be retrieved. They are not able to utilize (Artificial Intelligence\/personal Butler) like google assistance, or Apple's SIRI etc because all those apps are based on voice controlling.\nThere is a need for such platforms for such kind of people. American Sign Language (ASL) is a complete, complex language that employs signs made by moving the hands combined with facial expressions and postures of the body. It is the go-to language of many North Americans who are not able to talk and is one of various communication alternatives used by people who are deaf or hard-of-hearing.\nWhile sign language is very essential for deaf-mute people, to communicate both with normal people and with themselves, is still getting less attention from the normal people. The importance of sign language has been tending to ignored, unless there are areas of concern with individuals who are deaf-mute. One of the solutions to talk with the deaf-mute people is by using the mechanisms of sign language.\nHand gesture is one of the methods used in sign language for non-verbal communication. It is most commonly used by deaf & dumb people who have hearing or talking disorders to communicate among themselves or with normal people. Various sign language systems have been developed by many manufacturers around the world but they are neither flexible nor cost-effective for the end users.*","2499a727":"# Architecture of VGG16","d4dca4c4":"# Why use a Pre-trained Model?","69721d5d":"if you want to do it more accurately then take one image from each class and put it in another folder and apply for loop in that custom folder by above code and predict the output. ","ee85abc9":"# if you find it useful please do upvote","b6215956":"now just checking that is it converted or not.","5e9eb30b":"# Architecture of RESNET50","c0baef16":"VGG16 accuracy and loss plot","ce7b9f82":"![Left-ResNet50-architecture-Blocks-with-dotted-line-represents-modules-that-might-be.png](attachment:Left-ResNet50-architecture-Blocks-with-dotted-line-represents-modules-that-might-be.png)","f843c348":"**Note:** In the train-test split I have used the stratify argument on the labels. This argument ensures that the data is split evenly along all labels ","6b484811":"# RESNET50 confusion matrix","42c55a97":"![1_1rUm4OpT_HXl6-HFCW9fUw.png](attachment:1_1rUm4OpT_HXl6-HFCW9fUw.png)","db593da9":"**Normalization will help us remove distortions caused by lights and shadows in an image**","1b0ace26":"**NOTE:-**as you can see in the below image for transfer learning model we have to cut the last fully connected layer because we have to add our own fully connected layer. Now we have 40 classes so we will add fully connected layer of 40 classes,","49c499e9":"Accuracy of **VGG16**","b64ed38f":"VGG16","cee2c24c":" This will be the matrix for the testing data, which gave a high accuracy. We expect to find the diagonal elements to have large values with some values distributed in non-diagonal elements. Note that our the matrix is not normalized, and the total number of testing images per label were 300.","8aa71a4d":"# 4. Preprocessing","4f78a6d1":"# Here, We are going to use VGG16 and RESNET50 to train the model. which are pretrained model ","baab5a51":"**there number various types of transfer learning model which we can use in image classification such as**\n![1_NdCntZms6S2pBmQ3j_wyuw.png](attachment:1_NdCntZms6S2pBmQ3j_wyuw.png)","f109926e":"# VGG16 confusion matrix","2a185d15":"Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from i","4e5c7498":"Accuracy of **RESNET50**","13c3d505":"# 3. Preprocessing: One-hot enconding the data"}}