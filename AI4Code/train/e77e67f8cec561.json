{"cell_type":{"d16f1fda":"code","208c200b":"code","67dd6d5f":"code","76d9f925":"code","6094f14f":"code","54c185bc":"code","f1428b89":"code","66022a31":"code","35715118":"code","d257b62c":"code","157be547":"code","52eb05d9":"code","58ec9647":"code","55a85fce":"code","fdcbdf00":"code","bd8f39a2":"markdown","1c7e7daf":"markdown","25977a5a":"markdown","afe78311":"markdown","2fa55193":"markdown","8a989d56":"markdown","c80fe8a5":"markdown","ed096652":"markdown","af5402b8":"markdown"},"source":{"d16f1fda":"!pip install 'kaggle-environments>=0.1.6'","208c200b":"import os\nimport re\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport copy\nimport base64\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict, namedtuple, deque\nfrom IPython.core.display import HTML\n\n# sys.path.append('..\/input\/connectx\/kaggle-environments-0.1.4')\nfrom kaggle_environments import evaluate, make, utils\n\nimport gym\nfrom gym.spaces import Discrete\n\nimport torch\nfrom torch import optim\nimport torch.nn as nn\nfrom torch.nn import Module\nfrom torch.nn import functional as F\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","67dd6d5f":"class ConnectX(gym.Env):\n    \n    def __init__(self, user, switch_prob=0.5):\n        \n        self.env = make('connectx', debug=True)\n        self.pair = [user, None]\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n        \n        config = self.env.configuration\n        self.action_space = Discrete(config.columns)\n        self.observation = Discrete(config.columns*config.rows)\n        \n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n        \n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if random.uniform(0, 1) < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    \n    def run(self, user_list):\n        return self.env.run(user_list)","76d9f925":"env = ConnectX('negamax')\nenv.run(['random', \"random\"])\nHTML(env.render(mode=\"ipython\", width=700, height=600, header=False))","6094f14f":"class DeepQLearning(Module):\n    \n    def __init__(self):\n        super(DeepQLearning, self).__init__()\n        \n        env = ConnectX('negamax')\n        input_dim = env.observation.n\n        output_dim = env.action_space.n\n        \n        self.fc = nn.Sequential(nn.Linear(input_dim, 32),\n                                nn.ReLU(inplace=True),\n                                nn.Linear(32, 16),\n                                nn.ReLU(inplace=True),\n                                nn.Linear(16, 16),\n                                nn.ReLU(inplace=True),\n                                nn.Linear(16, output_dim))\n        \n        \n    def forward(self, x):\n        out = self.fc(x)\n        return out","54c185bc":"class ExperienceBuffer:\n    \n    def __init__(self, capacity):\n        self.experiences = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\n        self.memory = deque(maxlen=capacity)\n    \n    def __len__(self):\n        return len(self.memory)\n    \n    def add(self, state, action, reward, next_state, done):\n        e = self.experiences(state, action, reward, next_state, done)\n        self.memory.append(e)\n        \n    def sample(self, batch_size=32):           \n        experiences = random.sample(self.memory, batch_size)\n        \n        states = torch.from_numpy(np.vstack([(e.state)[:42] for e in experiences if e.state is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e.action is not None])).long().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward if e.reward is not None else 0.5 for e in experiences])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([(e.next_state)[:42] for e in experiences if e.next_state is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([1 if e.done else 0 for e in experiences if e.done is not None])).int().to(device)\n        \n        return (states,actions,rewards,next_states,dones)\n    \n    \n    \nclass Agent(object):\n    \n    def __init__(self, alpha=1e-2, gamma=0.6, batch_size=64, update_step=1):\n        \n        self.env = ConnectX('negamax')\n        self.model_train = DeepQLearning()\n        self.model_target = DeepQLearning()\n        self.model_target.load_state_dict(self.model_train.state_dict())\n           \n        self.creation = nn.MSELoss()\n        self.optimizer = optim.Adam(self.model_train.parameters(), lr=alpha)\n        self.alpha = alpha\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.t_step = 0\n        self.update_step = update_step\n        self.memory = ExperienceBuffer(10000000)\n        \n    def step(self, state, action, reward, next_state, done):        \n        self.memory.add(state, action, reward, next_state, done)\n        self.t_step = (self.t_step+1) % self.update_step\n        if self.t_step == 0 and (len(self.memory) > self.batch_size):\n            experience = self.memory.sample()\n            self.learn(experience)\n                \n    def act(self, state, epsilon=0):\n        \n        if random.uniform(0, 1) > epsilon:\n            max_reward = -100\n            action_valid = deque()\n#             state_check = np.array(state[:42]).reshape(-1, self.env.action_space.n)\n            for index in range(self.env.action_space.n):\n                if state[index] == 0:\n                    if reward_calculate(state, index) > max_reward:\n                        action_valid = [index]\n                        max_reward = reward_calculate(state, index)\n                    elif reward_calculate(state, index) == max_reward:\n                        action_valid.append(index)\n            action = random.choice(action_valid)\n        else:\n            action = random.choice([c for c in range(self.env.action_space.n) if state[c] == 0])\n        return action\n        \n    def learn(self, experiences):\n        self.model_train.train()\n        self.model_target.eval()\n                \n        states, actions, rewards, next_states, dones = experiences\n        with torch.no_grad():\n            next_state_value = self.model_target(next_states).detach()\n            max_next_state_value = next_state_value.max(1)[0].unsqueeze(1)\n            q_target = rewards + self.gamma*max_next_state_value*(1-dones)\n        \n        q_expected = self.model_train(states).gather(1, actions)\n        loss = self.creation(q_target, q_expected)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        self.model_target.load_state_dict(self.model_train.state_dict())","f1428b89":"def array_to_string(array):\n        return ''.join([str(i) for i in array])\n\ndef check_reward(string, string_opn, mark):\n    if 4*str(mark) in string:\n        reward = 30\n    else:\n        if 4*str(3-mark) in string_opn:\n            reward = 20\n        else:\n            if ('0'+str(mark)*3) in string or (str(mark)*3+'0') in string or \\\n            (str(mark)*2+'0'+str(mark)) in string or (str(mark)+'0'+str(mark)*2) in string:\n                reward = 10\n            else:\n                if ('0'+str(mark)*3) in string_opn or (str(mark)*3+'0') in string_opn or \\\n                (str(mark)*2+'0'+str(mark)) in string_opn or (str(mark)+'0'+str(mark)*2) in string_opn:\n                    reward = 0\n                else:\n                    if ('00'+str(mark)*2) in string or (str(mark)*2+'00') in string:\n                        reward = -10\n                    else:\n                        reward = -20\n\n    return reward\n\ndef reward_calculate(board, action):\n    mark = board[42:][0]\n    state = np.array(board[:42]).reshape(6, -1)\n    state_opn = state.copy()\n    assert action in range(7)\n    assert np.count_nonzero(state[:, action]) < 6\n    \n    for row in range(6)[::-1]:\n        if state[row, action] == 0:\n            valid_row = row\n            state[row, action] = mark\n            break  \n\n    state_opn[valid_row, action] = 3-mark\n\n    #check vertical\n    vertical = state[:, action]\n    vertical_self = array_to_string(vertical)\n    vertical_opn = vertical_self[:valid_row] + str(3-mark) + vertical_self[valid_row+1:]\n    vertical_reward = check_reward(vertical_self, vertical_opn, mark)\n\n    #check horizontal\n    horizontal = state[valid_row, :]\n    horizontal_self = array_to_string(horizontal)\n    horizontal_opn = horizontal_self[:action] + str(3-mark) + horizontal_self[action+1:]\n    horizontal_reward = check_reward(horizontal_self, horizontal_opn, mark)\n\n    #check left diagonal\n    if action - valid_row >= 0:\n        left_self = array_to_string(state[:, action-valid_row:].diagonal(0))\n        if len(left_self) < 4:\n            left_reward = -30\n        else:\n            left_opn = array_to_string(state_opn[:, action-valid_row:].diagonal(0))\n            left_reward = check_reward(left_self, left_opn, mark)\n    else:\n        left_self = array_to_string(state[valid_row-action:,:].diagonal(0))\n        if len(left_self) < 4:\n            left_reward = -30\n        else:\n            left_opn = array_to_string(state_opn[:, action-valid_row+1:].diagonal(0))\n            left_reward = check_reward(left_self, left_opn, mark)\n\n    #check right diagonal        \n    if action + valid_row < 7:\n        right_self = array_to_string(state[:, :action+valid_row+1][:, ::-1].diagonal(0))\n        if len(right_self) < 4:\n            right_reward = -30\n        else:\n            right_opn = array_to_string(state_opn[:, :action+valid_row+1][:, ::-1].diagonal(0))\n            right_reward = check_reward(right_self, right_opn, mark)\n    else:\n        right_self = array_to_string(state[action+valid_row-6:, :][:, ::-1].diagonal(0))\n        if len(right_self) < 4:\n            right_reward = -30\n        else:\n            right_opn = array_to_string(state_opn[action+valid_row-6:, :][:, ::-1].diagonal(0))\n            right_reward = check_reward(right_self, right_opn, mark)\n    \n    reward = max(vertical_reward, horizontal_reward, left_reward, right_reward)\n    \n    # check if next move opponent won\n    if reward < 20:\n        if (state[:, action] == 0).any():\n            valid_row -= 1\n            state_opt_next = state.copy()\n            state_opt_next[valid_row, action] = 3-mark\n\n            #check horizontal\n            horizontal = array_to_string(state_opt_next[valid_row, :])\n            if 4*str(3-mark) in horizontal:\n                return -50\n\n            #check left diagonal\n            if action - valid_row >= 0:\n                left_self = array_to_string(state_opt_next[:, action-valid_row:].diagonal(0))\n            else:\n                left_self = array_to_string(state_opt_next[valid_row-action:,:].diagonal(0))            \n            if 4*str(3-mark) in left_self:\n                return -50\n\n            #check right diagonal\n            if action + valid_row < 7:\n                right_self = array_to_string(state_opt_next[:, :action+valid_row+1][:, ::-1].diagonal(0))\n            else:\n                right_self = array_to_string(state_opt_next[action+valid_row-6:, :][:, ::-1].diagonal(0))\n            if 4*str(3-mark) in right_self:\n                return -50\n        \n    return reward","66022a31":"def dqn(user_list, episodes_list, eps_start=0.999,\n        eps_end=0.01, eps_decay=0.995, gamma=0.6):\n    \n    scores = deque()\n    \n    env = ConnectX('random')\n    agent = Agent()\n    \n    for user, episodes in zip(user_list, episodes_list):\n        env = ConnectX(user)\n        epsilon = eps_start\n        \n        for episode in tqdm(range(episodes)):\n            step = 0\n            observation = env.reset()\n            state = observation.board[:]\n            \n            done = False\n            rewards = 0\n            while not done:\n                    \n                state.append(observation.mark)\n                step += 1\n                action = agent.act(state, epsilon)\n                if isinstance(action, torch.Tensor):\n                    action = action.item()\n                \n                if action == -1:\n                    break\n                    \n                reward = reward_calculate(state, action)\n                next_state, reward, done, _ = env.step(action)\n                next_state = next_state.board[:]\n                \n                rewards += reward\n                agent.step(state, action, reward, next_state, done)\n                \n                state = next_state\n                \n            scores.append(rewards)\n            epsilon = max(epsilon*eps_decay, eps_end)\n\n            if episode % 5 == 0:\n                torch.save(agent.model_train.state_dict(),'model.pth') \n                    \n    env.close()\n    return list(scores)\n\nrandom_number = 100000\nnegamax_number = 10000\nscores = dqn(['random', 'negamax'], [random_number, negamax_number])","35715118":"def display_average(scores, title):\n    \n    episode_list = deque()\n    for episode in range(len(scores)):\n        mean_score = np.mean(list(scores)[:episode+1])\n        episode_list.append(mean_score)\n        \n    plt.figure(figsize=(15, 10))\n    plt.plot(np.arange(len(scores)), episode_list)\n    plt.ylabel('Score')\n    plt.xlabel('Episode')\n    plt.title(title, size=25, color='b')\n    plt.show()\n    \ndisplay_average(scores[:random_number], 'random')","d257b62c":"display_average(scores[random_number:], 'negamax')","157be547":"def string_convert(paras):\n    list_ = []\n    for para in paras:\n        para = re.sub('\\s+', ', ', str(para))\n        para = para.replace('[, ', '[').replace(', ]', ']')\n        list_.append(para)\n\n    string = ','.join(list_)\n    string = 'np.array([' + string + '])'\n    return string","52eb05d9":"model_path = '..\/working\/model.pth'\n \nparam = torch.load(model_path)\nkey = [*param.keys()]\n\nlist_para = []\nfor index in range(len(key)\/\/2):\n    weight = key[2*index]\n    bias = key[2*index+1]\n    \n    list_para.append('w_' + str(index) + ' = ' + string_convert(param[weight].cpu().numpy()))\n    list_para.append('b_' + str(index) + ' = ' + string_convert(param[bias].cpu().numpy()))\n\nstring_para = '\\n    '.join(list_para)","58ec9647":"my_agent = '''import random\nimport numpy as np\n\ndef act(observation, configuration):\n    \n    \n    state = observation.board[:]\n    state = np.array(state, dtype=np.float32)\n'''\n\nmy_agent = my_agent + '    ' + string_para + '''\n    \n    action_values = np.maximum(np.matmul(w_0, state) + b_0, 0)\n    action_values = np.maximum(np.matmul(w_1, action_values) + b_1, 0)\n    action_values = np.maximum(np.matmul(w_2, action_values) + b_2, 0)\n    action_values = np.matmul(w_3, action_values) + b_3\n    \n    value_max = -1e10\n    for index, value in enumerate(action_values):        \n        if value > value_max and observation.board[index] == 0:\n            value_max = value\n            action = index\n        \n    return action\n'''\n\n\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)\n\nout = sys.stdout\nsubmission = utils.read_file('\/kaggle\/working\/submission.py')\nagent = utils.get_last_callable(submission)\nsys.stdout = out\n\nenv = make('connectx', debug=True)\nenv.run(['random', agent])\nprint(env.render())\nprint('Success!' if env.state[0].status == env.state[1].status == 'DONE' else 'Failed...')","55a85fce":"env.run([agent, 'negamax'])\nHTML(env.render(mode=\"ipython\", width=700, height=600, header=False))","fdcbdf00":"env.run(['negamax', agent])\nHTML(env.render(mode=\"ipython\", width=700, height=600, header=False))","bd8f39a2":"## Rewards","1c7e7daf":"## Load weight","25977a5a":"## Visualize game","afe78311":"## Test random game","2fa55193":"## Save agent","8a989d56":"## Train model","c80fe8a5":"## Create strategy","ed096652":"## Stratify agent","af5402b8":"## Create model"}}