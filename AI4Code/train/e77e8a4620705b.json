{"cell_type":{"a858fafe":"code","943099e5":"code","2267b9f0":"code","7eba64a7":"code","25e09fff":"code","10bee4d4":"code","66ccf465":"code","c348c6dd":"code","dafbfd4c":"code","05b68f89":"code","f40357dd":"code","6aa02b57":"code","e4e98df5":"code","7ca25e7b":"code","1776c2f0":"code","9c860d25":"code","935828bc":"code","879d2c5f":"code","7f78362e":"code","d8400f25":"code","37021c38":"code","f66ea8ff":"code","1e71d192":"code","9cbef4db":"code","c24c6b07":"code","40558d6e":"code","86b26a94":"code","0641b76a":"code","3beadeef":"code","64ec93ee":"code","f86ac549":"code","e8c5d93e":"code","d0726da3":"code","94653b0c":"code","f8fcaed0":"code","f60d2c59":"code","69fe19f8":"code","9bb4281e":"code","b0dfb96d":"code","9ee93c07":"code","c2863f42":"code","ad168197":"code","7a05e16c":"code","b86a9272":"code","b2e5b1cc":"code","afb0e851":"code","00206fd3":"code","c13177ce":"code","3631cfb9":"code","9a2bf332":"code","d4722ab5":"code","226cc601":"code","734ac07c":"code","782c630e":"code","7ddc4682":"code","ead9b271":"code","44516487":"code","b1137955":"code","fd4eb588":"code","f52e04ed":"code","2196a3f4":"code","1e42b076":"code","e791ff84":"code","3dc3be98":"code","1d419520":"code","fb29f018":"code","8b8a01e6":"code","a6171674":"code","434138b8":"code","f181066d":"code","9849ca05":"code","db102f87":"code","8032e1c1":"code","6bc892b7":"code","63112767":"code","09bf4e71":"code","fc7e0f17":"code","c5a9e19f":"code","c127552c":"code","4ebf683d":"code","85c4da93":"code","5591e9cc":"code","28864594":"code","1e7aa954":"code","5a961395":"code","7b09575e":"code","c04b34b7":"code","c8d5b7d9":"code","4a6b382a":"code","0cd8a82d":"code","78e7f98c":"code","3e37e64d":"code","77315bb5":"code","550862c8":"code","fd3548f9":"markdown","d5a3c54e":"markdown","bb2ff0e8":"markdown","d655c0d8":"markdown","9d03e3ec":"markdown","5b5f8daa":"markdown","1487417f":"markdown","b2155a41":"markdown","6e11092a":"markdown","128c0a80":"markdown","6e2bd3cf":"markdown","503ee5ac":"markdown","bfe6eb7a":"markdown","c083e0f1":"markdown","6e78186a":"markdown","e299f587":"markdown","f848f336":"markdown","c800efd6":"markdown","5b89902b":"markdown","9c15cdf4":"markdown","5e046489":"markdown","ab6d3554":"markdown","1a851643":"markdown","cef5451e":"markdown","0a806dc1":"markdown","941c77ba":"markdown","a9f7b8bd":"markdown","d9a45e6a":"markdown"},"source":{"a858fafe":"# make calendar maps\n!pip install calmap","943099e5":"import calmap\nimport dateutil.parser","2267b9f0":"# Main libraries that we will use in this kernel\nimport datetime\nimport numpy as np\nimport pandas as pd\n\n# # garbage collector: free some memory is needed\nimport gc\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pip install squarify (algorithm for treemap) if missing\nimport squarify\n\n# statistical package and some useful functions to analyze our timeseries\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.tsa.stattools as stattools\n\nimport time\n\nfrom xgboost import XGBRegressor\nfrom string import punctuation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\ndef print_files():\n    import os\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7eba64a7":"# Let's see how many different files we are dealing with\nprint_files()","25e09fff":"# import the df\nshops = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nshops.shape","10bee4d4":"shops.head()","66ccf465":"# We don't have any duplicates in the shop_name field\nshops.shape[0] == len(shops[\"shop_name\"].unique())","c348c6dd":"# However inspecting the df by name, we can see that shop_id 10 and 11 are very similar. Later we will try and group them once we inspect the sales per shop\nshops[shops[\"shop_id\"].isin([10, 11])]","dafbfd4c":"# The same happens with the shops with shop_id 23 and 24\nshops[shops[\"shop_id\"].isin([23, 24])]","05b68f89":"# No missing values in the shops df\nshops.isnull().sum().sum()","f40357dd":"# let's correct the shops df and also generate a few more features\ndef fix_shops(shops):\n    '''\n    This function modifies the shops df inplace.\n    It correct's 3 shops that we have found to be 'duplicates'\n    and also creates a few more features: extracts the city and encodes it using LabelEncoder\n    '''\n    \n    d = {0:57, 1:58, 10:11, 23:24}\n    \n    # this 'tricks' allows you to map a series to a dictionary, but all values that are not in the dictionary won't be affected\n    # it's handy since if we blindly map the values, the missings values will be replaced with nan\n    shops[\"shop_id\"] = shops[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\n    \n    # replace all the punctuation in the shop_name columns\n    shops[\"shop_name_cleaned\"] = shops[\"shop_name\"].apply(lambda s: \"\".join([x for x in s if x not in punctuation]))\n    \n    # extract the city name\n    shops[\"city\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[0])\n    \n    # encode it using a simple LabelEncoder\n    shops[\"city_id\"] = LabelEncoder().fit_transform(shops['city'])","6aa02b57":"# apply our function to the shops_df\nfix_shops(shops)","e4e98df5":"shops.head()","7ca25e7b":"# import df\nitems_category = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems_category.shape","1776c2f0":"items_category.head()","9c860d25":"# We don't have any duplicates in the item_category_name field\nitems_category.shape[0] == len(items_category[\"item_category_name\"].unique())","935828bc":"# allow pandas to show all the rows from this df\npd.options.display.max_rows = items_category.shape[0]","879d2c5f":"# If we take a closer look, we can see that we have a lot of Play Station categories: like accesories, games and so on. We have the same categories for XBOX and also for PC Games.\n# A lot of categories have to deal with books, presents and computer software and music (CD).\n# We will generate later some features by parsing the names and making groupedby features.\nitems_category.head()","7f78362e":"# If we apply a simple lambda function and extract the everything that contains PS, we will get 16 different categories for PlayStation\nitems_category[\"PS_flag\"] = items_category[\"item_category_name\"].apply(lambda x: True if \"PS\" in x else False)\nitems_category[items_category[\"PS_flag\"] == True]","d8400f25":"# No missing values in the items_category df\nitems_category.isnull().sum().sum()","37021c38":"# import df\nitems = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nitems.shape","f66ea8ff":"# allow pandas to show all the rows from this df\npd.options.display.max_rows = items.shape[0]\nitems.head()","1e71d192":"# We have a lot of items_id, and as we can see some of them are very familiar.\nitems[items[\"item_id\"].isin([69, 70])][\"item_name\"].iloc[0]","9cbef4db":"items[items[\"item_id\"].isin([69, 70])][\"item_name\"].iloc[1]","c24c6b07":"# No missing values in the items category\nitems.isnull().sum().sum()","40558d6e":"# Let's see the top 10 and bottom 10 item categories\nitems_gb = items.groupby(\"item_category_id\").size().to_frame()","86b26a94":"items[items[\"item_category_id\"] == 60]","0641b76a":"items_gb.rename(columns = {0:\"counts\"}, inplace = True)","3beadeef":"items_gb.sort_values(\"counts\", ascending = False, inplace = True)","64ec93ee":"top_10 = items_gb[:10]","f86ac549":"bottom_10 = items_gb[-10:]","e8c5d93e":"top_10 = top_10.append(bottom_10)\ntop_10 = top_10.sort_values(\"counts\", ascending = False)","d0726da3":"top_10.reset_index()","94653b0c":"# We can notice that in the top 10 most popular items products we have PS3\n# At the same time, in the bottom 10 products, we can find 2 PS2.\n# This means, that we have to be careful while generating features like PS\npd.merge(top_10, items_category, left_on = \"item_category_id\", right_on = \"item_category_id\")","f8fcaed0":"# import df\nsales = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nsales.shape","f60d2c59":"sales.sample(10)","69fe19f8":"sales.info()","9bb4281e":"# No null values in the sales df\n\n# Is this True?\n\nsales.isnull().sum()","b0dfb96d":"sorted(list(sales[\"item_cnt_day\"].unique()))[:20]","9ee93c07":"del shops, items_category, items, sales\ngc.collect()","c2863f42":"# a simple function that creates a global df with all joins and also shops corrections\ndef create_df():\n    '''\n    This is a helper function that creates the train df.\n    '''\n    # import all df\n    shops = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\n    fix_shops(shops) # fix the shops as we have seen before\n    \n    items_category = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\n    items = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n    sales = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n    \n    # fix shop_id in sales so that we can leater merge the df\n    d = {0:57, 1:58, 10:11, 23:24}\n    sales[\"shop_id\"] = sales[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\n    \n    # create df by merging the previous dataframes\n    df = pd.merge(items, items_category, left_on = \"item_category_id\", right_on = \"item_category_id\")\n    df = pd.merge(sales, df, left_on = \"item_id\", right_on = \"item_id\")\n    df = pd.merge(df, shops, left_on = \"shop_id\", right_on = \"shop_id\")\n    \n    # convert to datetime and sort the values\n#     df[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\n    df.sort_values(by = [\"shop_id\", \"date\"], ascending = True, inplace = True)\n    \n    return df","ad168197":"df = create_df()","7a05e16c":"df.shape","b86a9272":"df.head()","b2e5b1cc":"# It seems that there are no null values, however this is not fully true. \n# As we will see in the next section, when we groupby and plot the data, there are a lot of months where there have been no sales so basically it's a null value, and we have to impute zero sales for that month.\ndf.isnull().sum().sum()","afb0e851":"df.info()","00206fd3":"# Let's group by Month and see all the sales\n\n# resample in timeseries is the same as groupby\n# in order it to work, we must set the date column as index, and it must be a datetime format (strings are not valid)\n# when we resample it, we can pass D: daily, W: weekly or M: monthly\n# we can then perform operation on the 'resampled' columns like\n# sum, mean and others.\n\n# calculate the monthly sales\ndf[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")","c13177ce":"df[\"Year\"] = df[\"date\"].dt.year","3631cfb9":"df[\"Month\"] = df[\"date\"].dt.month","9a2bf332":"df.head()","d4722ab5":"# resample the data on a monthly basis\nx = df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"M\").sum()\n\n# plot the data using matplotlib\nplt.figure(figsize = (10, 6))\nplt.plot(x, color = \"blue\", label = \"Monthly sales\")\nplt.title(\"Monthly sales\")\nplt.legend();","226cc601":"# perform the same operations but on a weekly basis\nx = df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"W\").sum()\n\nplt.figure(figsize = (10, 6))\nplt.plot(x.index, x, color = \"blue\", label = \"Weekly sales\")\nplt.title(\"Weekly sales\")\nplt.legend();","734ac07c":"russian_holidays_start = [\ndatetime.datetime(2013, 1, 1),\ndatetime.datetime(2013, 2, 23),\ndatetime.datetime(2013, 3, 8),\ndatetime.datetime(2013, 5, 1),\ndatetime.datetime(2013, 5, 9),\ndatetime.datetime(2013, 6, 12),\ndatetime.datetime(2013, 11, 4),\n\ndatetime.datetime(2014, 1, 1),\ndatetime.datetime(2014, 2, 23),\ndatetime.datetime(2014, 3, 8),\ndatetime.datetime(2014, 5, 1),\ndatetime.datetime(2014, 5, 9),\ndatetime.datetime(2014, 6, 12),\ndatetime.datetime(2014, 11, 4),\n\ndatetime.datetime(2015, 1, 1),\ndatetime.datetime(2015, 2, 23),\ndatetime.datetime(2015, 3, 8),\ndatetime.datetime(2015, 5, 1),\ndatetime.datetime(2015, 5, 9),\ndatetime.datetime(2015, 6, 12),\ndatetime.datetime(2015, 11, 4)\n]","782c630e":"russian_holidays_end = [\ndatetime.datetime(2013, 1, 8),\ndatetime.datetime(2013, 2, 23),\ndatetime.datetime(2013, 3, 8),\ndatetime.datetime(2013, 5, 1),\ndatetime.datetime(2013, 5, 9),\ndatetime.datetime(2013, 6, 12),\ndatetime.datetime(2013, 11, 4),\n\ndatetime.datetime(2014, 1, 8),\ndatetime.datetime(2014, 2, 23),\ndatetime.datetime(2014, 3, 8),\ndatetime.datetime(2014, 5, 1),\ndatetime.datetime(2014, 5, 9),\ndatetime.datetime(2014, 6, 12),\ndatetime.datetime(2014, 11, 4),\n\ndatetime.datetime(2015, 1, 8),\ndatetime.datetime(2015, 2, 23),\ndatetime.datetime(2015, 3, 8),\ndatetime.datetime(2015, 5, 1),\ndatetime.datetime(2015, 5, 9),\ndatetime.datetime(2015, 6, 12),\ndatetime.datetime(2015, 11, 4)\n]","7ddc4682":"for iterable in sorted(list(df[\"shop_name\"].unique())[:5]):\n\n    # create the size of the figure\n    plt.figure(figsize = (30, 10))\n\n    # create the subplot for Monthly sales of the each shop\n    plt.subplot(1, 2, 1)\n    \n    # calculate the Monthly sales of each shop\n    short_df = df[df[\"shop_name\"] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(\"M\")[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    # adding moving average\n    short_df[\"MA3M\"] = short_df[\"item_cnt_day\"].rolling(window = 3).mean()\n    short_df[\"MA4M\"] = short_df[\"item_cnt_day\"].rolling(window = 4).mean()\n    short_df[\"MA5M\"] = short_df[\"item_cnt_day\"].rolling(window = 5).mean()\n    \n    # assing the data to plot\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    \n    average_3_months = short_df[\"MA3M\"]\n    average_4_months = short_df[\"MA4M\"]\n    average_5_months = short_df[\"MA5M\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Monthly sales\")\n    \n    plt.plot(dates, average_3_months, '.-', label = \"Average sales of the last 3 months\")\n\n    # get current axis and plot the areas\n    ax = plt.gca()\n    alpha = 0.2\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')    \n       \n    # add title and show legend    \n    plt.title('Monthly sales of shop {}'.format(iterable))\n    plt.ylabel('Total Monthly sales of shop {}'.format(iterable))\n    plt.xlabel(\"Time grouped by month\")\n    plt.legend()\n    \n    #######################################################################################\n    # Weekly sales\n    #######################################################################################\n    \n    plt.subplot(1, 2, 2)\n    \n      # calculate the Weekly sales of each shop\n    short_df = df[df[\"shop_name\"] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(\"W\")[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    # adding moving average\n    short_df[\"MA3W\"] = short_df[\"item_cnt_day\"].rolling(window=3).mean()\n    short_df[\"MA4W\"] = short_df[\"item_cnt_day\"].rolling(window=4).mean()\n    short_df[\"MA5W\"] = short_df[\"item_cnt_day\"].rolling(window=5).mean()\n    \n    # assing the data to plot\n    \n    # general sales\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    \n    average_3_weeks = short_df[\"MA3W\"]\n    average_4_weeks = short_df[\"MA4W\"]\n    average_5_weeks = short_df[\"MA5W\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Weekly sales\")\n    plt.plot(dates, average_3_weeks, '.-', label = \"Average sales of the last 3 weeks\")\n    \n    # get current axis and plot the areas\n    ax = plt.gca()\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')\n    \n    # add title and show legend\n    plt.title('Weekly sales of shop {}'.format(iterable))\n    plt.ylabel('Total Weekly sales of shop {}'.format(iterable))\n    plt.xlabel(\"Time grouped by week\")\n    plt.legend()\n    \n    # general sales\n    plt.show()","ead9b271":"for iterable in sorted(list(df[\"item_category_name\"].unique()))[:5]:\n\n    # create the size of the figure\n    plt.figure(figsize = (30, 10))\n\n    # create the subplot for Monthly sales of the each shop\n    plt.subplot(1, 2, 1)\n    \n    # calculate the Monthly sales of each shop\n    short_df = df[df[\"item_category_name\"] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(\"M\")[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    # adding moving average\n    short_df[\"MA3M\"] = short_df[\"item_cnt_day\"].rolling(window=3).mean()\n    short_df[\"MA4M\"] = short_df[\"item_cnt_day\"].rolling(window=4).mean()\n    short_df[\"MA5M\"] = short_df[\"item_cnt_day\"].rolling(window=5).mean()\n    \n    # assing the data to plot\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    \n    average_3_months = short_df[\"MA3M\"]\n    average_4_months = short_df[\"MA4M\"]\n    average_5_months = short_df[\"MA5M\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Monthly sales\")\n    \n    plt.plot(dates, average_3_months, '.-', label = \"Average sales of the last 3 months\")\n\n    # get current axis and plot the areas\n    ax = plt.gca()\n    alpha = 0.2\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')   \n    \n    # add title and show legend\n    plt.title('Monthly sales of item category {}'.format(iterable))\n    plt.ylabel('Total Monthly sales of item category {}'.format(iterable))\n    plt.xlabel(\"Time grouped by month\")\n    plt.legend()\n    \n\n    #######################################################################################\n    # Weekly sales\n    #######################################################################################\n    \n    plt.subplot(1, 2, 2)\n    \n      # calculate the Weekly sales of each shop\n    short_df = df[df[\"item_category_name\"] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(\"W\")[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    # adding moving average\n    short_df[\"MA3W\"] = short_df[\"item_cnt_day\"].rolling(window = 3).mean()\n    short_df[\"MA4W\"] = short_df[\"item_cnt_day\"].rolling(window = 4).mean()\n    short_df[\"MA5W\"] = short_df[\"item_cnt_day\"].rolling(window = 5).mean()\n    \n    # assing the data to plot\n    \n    # general sales\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    \n    average_3_weeks = short_df[\"MA3W\"]\n    average_4_weeks = short_df[\"MA4W\"]\n    average_5_weeks = short_df[\"MA5W\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Weekly sales\")\n    plt.plot(dates, average_3_weeks, '.-', label = \"Average sales of the last 3 weeks\")\n    \n    # get current axis and plot the areas\n    ax = plt.gca()\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')\n        \n    # add title and show legend\n    plt.title('Weekly sales of item category {}'.format(iterable))\n    plt.ylabel('Total Weekly sales of item category {}'.format(iterable))\n    plt.xlabel(\"Time grouped by week\")\n    plt.legend()\n    # general sales\n    plt.show()","44516487":"# we can observe a general trend of decrasing sales.\n# let's add a second axis to see the variation of intradays sales\n\n# select the columns of interest\ndf_var = df[[\"date\", \"item_cnt_day\"]]\n\n# convert to datetime\ndf_var[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\n\n# set date as index\ndf_var.set_index(\"date\", inplace = True)\n\n# resample\/groupby by date and convert to frame the total daily sales\ndf_var = df_var.resample(\"M\")[\"item_cnt_day\"].sum().to_frame()\n\n# calculate the intra week variation between total sales\ndf_var[\"Variation\"] = df_var[\"item_cnt_day\"].diff()\/df_var[\"item_cnt_day\"].shift(1)\n\ndf_var.head()","b1137955":"# separate x and y\ny_sales = df_var[\"item_cnt_day\"]\ny_variation = df_var[\"Variation\"]\n\n# instanciate the figure\nfig = plt.figure(figsize = (15, 10))\nax = fig.add_subplot(111)\n\n# plot the total sales\nplot1 = ax.plot(y_sales, label = \"Total monthly sales\", color = \"blue\", alpha = 0.5)\n\n# create a secondary axis and plot the variation data\nax_bis = ax.twinx()\nplot2 = ax_bis.plot(y_variation, label = \"Intra - month variation of sales\", color = \"red\", alpha = 0.5)\n\n# create a common legend for both plots\nlns = plot1 + plot2\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc = \"upper left\")\n\n# add a custom title to the plot\nax.set_title(\"Total monthly sales and variation\");","fd4eb588":"# start with the regular df\ndf_for_question_1 = create_df()\n\n\n# we can observe a general trend of decrasing sales.\n# let's add a second axis to see the variation of intradays sales\n\n# select the columns of interest\ndf_for_question_1 = df[[\"date\", \"item_cnt_day\"]]\n\n# convert to datetime\ndf_for_question_1[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\n\ndf_for_question_1.rename(columns = {\"item_cnt_day\":\"item_cnt_day\"}, inplace = True)\n\n# set date as index\ndf_for_question_1.set_index(\"date\", inplace = True)\n\n# resample\/groupby by date and convert to frame the total daily sales\ndf_for_question_1 = df_for_question_1.resample(\"D\")[\"item_cnt_day\"].sum().to_frame()\n\n# calculate the intra week variation between total sales\ndf_for_question_1[\"Variation\"] = df_for_question_1[\"item_cnt_day\"].diff()\/df_for_question_1[\"item_cnt_day\"].shift(7)\n","f52e04ed":"df_for_question_1.head(20)","2196a3f4":"# separate x and y\ny_sales = df_for_question_1[\"item_cnt_day\"]\ny_variation = df_for_question_1[\"Variation\"]\n\n# instanciate the figure\nfig = plt.figure(figsize = (20, 10))\nax = fig.add_subplot(111)\n\n# plot the total sales\nplot1 = ax.plot(y_sales, label = \"Total weak sales\", color = \"blue\", alpha = 0.5)\n\n# create a secondary axis and plot the variation data\nax_bis = ax.twinx()\nplot2 = ax_bis.plot(y_variation, label = \"Intra - weak variation of sales\", color = \"red\", alpha = 0.5)\n\n# create a common legend for both plots\nlns = plot1 + plot2\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc = \"upper left\")\n\n# add a custom title to the plot\nax.set_title(\"Total weak sales and variation\");","1e42b076":"# calendar heatmaps are really useful to see the overall activity for a certain period of time per day and per month.\n# let's build one using python.\n# we will be using the calmap package for this, because it makes it extremenly easy to plot this data\n# select the columns\ndf_calendar = df[[\"date\", \"item_cnt_day\"]]\n\n# set date as index and resample\ndf_calendar.set_index(\"date\", inplace = True)\n# notice that this time, we don't convert it to_frame()\n# df_calendar is a pandas series\n# THIS IS IMPORTANT since calmap expects a series\n# with a datetime index and the values to plot\ndf_calendar = df_calendar.resample(\"D\")[\"item_cnt_day\"].sum()\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data using calmap\ncalmap.calendarplot(df_calendar, # pass the series\n                    fig_kws = {'figsize': (16,10)}, \n                    yearlabel_kws = {'color':'black', 'fontsize':14}, \n                    subplot_kws = {'title':'Total sales per year'}\n                   );","e791ff84":"# This plot are fundamental in timeseries analysis.\n# Basically here we compare the a series again itself but with some lags.\n# These are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.\n\n# More info: \n# https:\/\/machinelearningmastery.com\/gentle-introduction-autocorrelation-partial-autocorrelation\/\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,6), dpi = 80)\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data using the built in plots from the stats module\n\n# The AutoCorrelation plot: compares a value v with the value v but n times in the past.\nplot_acf(df.set_index(\"date\").resample(\"D\")[\"item_cnt_day\"].sum(), ax = ax1, lags = 14)\n\n# The Parcial AutoCorrelation plot: partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nplot_pacf(df.set_index(\"date\").resample(\"D\")[\"item_cnt_day\"].sum(), ax = ax2, lags = 14);","3dc3be98":"# This code snippets show you have to calculate the Partial Autocorrelation\n# Partial Autocorrelation can be very counter intuitive since in some of our steps we are fitting a linear model\n# to predict the values of t - 2 using t - 1\n# Wait, what? Why we use values from yesterday to predict values before yesterday?\n# Basically because we assume that our timeseries is auto regressive. This means that the data at point t captures\n# all the variance\/information from all the previuos data points.\n# This way, t - 1, must have captured all the variance from previous points, thus t - 2, and so t - 1 becomes\n# a good predictor for values from t - 2.","1d419520":"# create a dataframe with total sales per day (all shops and all items)\ndf_total_sales = df.set_index(\"date\").resample(\"D\")[\"item_cnt_day\"].sum().to_frame()\n\n# rename the column item_cnt_day to total_sales\ndf_total_sales.columns = [\"total_sales\"]\n","fb29f018":"df_total_sales.head()","8b8a01e6":"\n# create a few features that we need in order to calculate the parcial autocorrelation\ndf_total_sales[\"T-1\"] = df_total_sales[\"total_sales\"].shift(1)\ndf_total_sales[\"T-2\"] = df_total_sales[\"total_sales\"].shift(2)\n\n# we have a few nan for the first 2 rows so we must drop them\nprint(df_total_sales.shape)\ndf_total_sales.dropna(axis = \"rows\", inplace = True)\nprint(df_total_sales.shape)","a6171674":"# instanciate the Linear model\nmodel = LinearRegression()\n\n# separate X and y\nX = df_total_sales[[\"T-1\"]]\ny = df_total_sales[\"total_sales\"]\n\n# fit and predict with the model\nmodel.fit(X, y)\npredictions = model.predict(X)\n\n# save our predictions to the total_sales df\ndf_total_sales[\"total_sales_from_T-1\"] = predictions","434138b8":"df_total_sales.head()","f181066d":"# instanciate the Linear model\nmodel = LinearRegression()\n\n# separate X and y\nX = df_total_sales[[\"T-1\"]]\ny = df_total_sales[\"T-2\"]\n\n# fit and predict with the model\nmodel.fit(X, y)\npredictions = model.predict(X)\n\n# save our predictions to the total_sales df\ndf_total_sales[\"T-2_from_T-1\"] = predictions","9849ca05":"df_total_sales.head()","db102f87":"# calculate the residual\n# this means: total_sales - total_sales_from_T-1\n# and: T-2 - \"T-2_from_T-1\"\ndf_total_sales[\"Residual_total_sales_T-1\"] = df_total_sales[\"total_sales\"] - df_total_sales[\"total_sales_from_T-1\"]\n\n# this step is very important based on the asumptions we have about many of the timeseries\n# for more information I recommend this read\n# https:\/\/towardsdatascience.com\/understanding-partial-auto-correlation-fa39271146ac\ndf_total_sales[\"Residual_T-2_T-1\"] = df_total_sales[\"T-2\"] - df_total_sales[\"T-2_from_T-1\"]","8032e1c1":"df_total_sales.head()","6bc892b7":"# calculathe the parcial autocorrelation using manual method\nmanual_pacf = df_total_sales.corr(method = \"pearson\")[\"Residual_total_sales_T-1\"][\"Residual_T-2_T-1\"]\nprint(\"Manual parcial autocorrelation method {}\".format(round(manual_pacf, 5)))\n\n# calculate the parcial autocorrelation using statsmodel package\nstats_pacf = pacf(df_total_sales['total_sales'], nlags = 2)[2]\nprint(\"Parcial autocorrelation method using stats package {}\".format(round(stats_pacf, 5)))","63112767":"df_timeindex = df.set_index(\"date\").resample(\"W\")[\"item_cnt_day\"].sum().to_frame()\n\n# decompose the series using stats module\n# results in this case is a special class \n# whose attributes we can acess\nresult = seasonal_decompose(df_timeindex[\"item_cnt_day\"])\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\n# make the subplots share teh x axis\nfig, axes = plt.subplots(ncols = 1, nrows = 4, sharex = True, figsize = (12,10))\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data\n# using this cool thread:\n# https:\/\/stackoverflow.com\/questions\/45184055\/how-to-plot-multiple-seasonal-decompose-plots-in-one-figure\n# This allows us to have more control over the plots\n\n# plot the original data\nresult.observed.plot(ax = axes[0], legend = False)\naxes[0].set_ylabel('Observed')\naxes[0].set_title(\"Decomposition of a series\")\n\n# plot the trend\nresult.trend.plot(ax = axes[1], legend = False)\naxes[1].set_ylabel('Trend')\n\n# plot the seasonal part\nresult.seasonal.plot(ax = axes[2], legend = False)\naxes[2].set_ylabel('Seasonal')\n\n# plot the residual\nresult.resid.plot(ax = axes[3], legend = False)\naxes[3].set_ylabel('Residual')\n\n# ----------------------------------------------------------------------------------------------------\n# prettify the plot\n\n# get the xticks and the xticks labels\nxtick_location = df_timeindex.index.tolist()\n\n# set x_ticks\nax.set_xticks(xtick_location);","09bf4e71":"# start with the regular df\ndf_for_question_2 = create_df()\n\n# select the columns of interest\ndf_for_question_2 = df[[\"date\", \"item_cnt_day\", \"city_id\"]]\n\n# convert to datetime\ndf_for_question_2[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\n\ndf_for_question_2.rename(columns = {\"item_cnt_day\":\"item_cnt_weak\"}, inplace = True)\n\n# set date as index\ndf_for_question_2.set_index(\"date\", inplace = True)\n\ncityes = df[[\"city_id\",\"city\"]].drop_duplicates()\ncityes.set_index(\"city_id\", inplace = True)","fc7e0f17":"for i in cityes.index.to_frame()['city_id']:\n    df_city = df_for_question_2[df_for_question_2[\"city_id\"].isin([i])]\n    if i == 14:\n        df_city.reset_index(inplace=True)\n        df_city['date'] = pd.to_datetime(df['date'])\n        df_city = df_city.set_index('date')\n    fig, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,6), dpi = 80)\n    plot_acf(df_city.resample(\"W\")[\"item_cnt_weak\"].sum(), ax = ax1, lags = 30)\n    plot_pacf(df_city.resample(\"W\")[\"item_cnt_weak\"].sum(), ax = ax2, lags = 30)\n    df_total_sales_weak_city = df_city.resample(\"W\")[\"item_cnt_weak\"].sum().to_frame()\n    df_total_sales_weak_city.columns = [\"total_sales\"]\n    result = seasonal_decompose(df_total_sales_weak_city[\"total_sales\"])\n    fig, axes = plt.subplots(ncols = 1, nrows = 4, sharex = True, figsize = (12,10))\n\n    result.observed.plot(ax = axes[0], legend = False)\n    axes[0].set_ylabel('Observed')\n    axes[0].set_title(\"Decomposition of a series by city \"+cityes['city'][i])\n\n    result.trend.plot(ax = axes[1], legend = False)\n    axes[1].set_ylabel('Trend by city %s' %i)\n\n    result.seasonal.plot(ax = axes[2], legend = False)\n    axes[2].set_ylabel('Seasonal by city %s' %i)\n    \n    result.resid.plot(ax = axes[3], legend = False)\n    axes[3].set_ylabel('Residual by city %s' %i)\n\n    xtick_location = df_total_sales_weak_city.index.tolist()\n\n    ax.set_xticks(xtick_location)\n    del df_city, ax1, ax2, df_total_sales_weak_city, result, fig, axes, xtick_location\n    gc.collect()","c5a9e19f":"# prepare the data\n\n# extract each year using dt.year\ndf[\"YEAR\"] = df[\"date\"].dt.year\n\n# create a smaller df for year 2013\nshort_df = df[df[\"YEAR\"] == 2013][[\"item_cnt_day\", \"city\"]]\n\n# groupby by city and sum all the sales\nshort_df = short_df.groupby(\"city\")[\"item_cnt_day\"].sum().to_frame()\n\n# sort the values in the smaller df inplace\nshort_df.sort_values(\"item_cnt_day\", ascending = False, inplace = True)\n\n# we will do the same plot as before but without custom colors\n# Moscow is a big outlier so it pales the rest of the cities\n\nshort_df = df[df[\"YEAR\"] == 2014][[\"item_cnt_day\", \"city\"]]\nshort_df = short_df.groupby(\"city\")[\"item_cnt_day\"].sum().to_frame()\nshort_df.sort_values(\"item_cnt_day\", ascending = False, inplace = True)\n\nmy_values = short_df[\"item_cnt_day\"]\nmy_pct = short_df[\"item_cnt_day\"]\/short_df[\"item_cnt_day\"].sum()\nlabels = ['{} - Sales :{}k \\n {}% of total'.format(city, sales\/1000, round(pct, 2)*100) for city, sales, pct in zip(short_df.index, my_values, my_pct)]\n\nplt.figure(figsize = (30, 8))\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8)\nplt.title(\"Sales by city and their % over total sales in 2014\",fontsize = 23, fontweight = \"bold\")\n\nplt.axis('off')\nplt.tight_layout()","c127552c":"df[[\"city\", \"city_id\"]].drop_duplicates()","4ebf683d":"# treemaps are very useful to see the difference and the weights of categories\n# but they don't give us that much of information about the distribution of each category\n# let's use boxplot to see the distribution of Moscow city\n\n# we can see huge outliers for Moscow city.\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"city\",\n            y = \"item_cnt_day\", \n            data = df[(df[\"YEAR\"] == 2013) & (df[\"city_id\"] == 13)]\n           );","85c4da93":"# start with the regular df\ndf_for_question_3 = df[[\"item_cnt_day\", \"item_category_id\", \"item_category_name\"]]\n\n# groupby by category and sum all the sales\ndf_for_question_3 = df_for_question_3.groupby(\"item_category_id\")[\"item_cnt_day\"].sum().to_frame()\n\n# sort the values in the smaller df inplace\ndf_for_question_3.sort_values(\"item_cnt_day\", ascending = False, inplace = True)\n\nmy_values = df_for_question_3[\"item_cnt_day\"]\nmy_pct = df_for_question_3[\"item_cnt_day\"]\/df_for_question_3[\"item_cnt_day\"].sum()\n#En el zip podria usarse tambien my_pct[my_pct>=0.010] Pero entiendo que al redondear se incluye en el %1\nlabels = ['{} - Sales :{}k \\n {}% of total'.format(category, sales\/1000, round(pct, 2)*100) for category, sales, pct in zip(df_for_question_3.index, my_values, my_pct[my_pct>=0.005])]\nplt.figure(figsize = (30, 8))\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8)\nplt.title(\"Sales by category and their % over total sales\",fontsize = 23, fontweight = \"bold\")\n\nplt.axis('off')\nplt.tight_layout()","5591e9cc":"# This plot will help us visualize the missing values for each datetime and item_id\n# This is the most granular plots possible, since we will be seeing individual sales by day and item_id\n# This plot can be very consufing, but the main point is to show all the \"missing values\" we have\n# We have seen previously in our EDA, that when we groupby and resamples our sales, we might think\n# that we don't have any missing values. But its not true, we only have the reported sales\n# This means that, if we have a shop or item_id that only had 3 sales per year, when we resample\n# our df by day, pandas will generate additional days with null sales.\n# those null sales is what we want to plot here\n# the values are ordered from less nulls to more nulls\n\ngb_df_ = df.pivot_table(index = [\"date\"], columns = ['item_id'], values = \"item_cnt_day\", aggfunc = sum).isnull()\norder_of_columns = list(gb_df_.sum().sort_values().index)\ngb_df_ = gb_df_[order_of_columns]\nplt.figure(figsize = (20, 10))\nplot = sns.heatmap(gb_df_, cbar = True, cmap = \"inferno\")\nplot.set_title(\"Null sales by item_id and day\");","28864594":"gc.collect()\ndel gb_df_","1e7aa954":"df[[\"shop_name\",\"shop_id\"]].drop_duplicates()","5a961395":"# this will allow us to see a all the columns of the df\npd.options.display.max_columns = 999","7b09575e":"# create a smaller df\nshort_df = df[[\"date\", \"item_cnt_day\", \"shop_name\"]]\n# set the date to be the index (to resample later)\nshort_df.set_index(\"date\", inplace = True)\n# groupby by shop_name\ngb = short_df.groupby(\"shop_name\")\n# resample the df by month sales (resample = groupby by months in timeseries)\ngbr = gb.resample(\"M\")[\"item_cnt_day\"].sum()\n# unstack the gbr to have columns name\ngbr = gbr.unstack(level = -1).T\n# sort the values, from no nulls to more null values\norder_of_columns = list(gbr.isnull().sum().sort_values().index)\n# change the order of the df\ngbr = gbr[order_of_columns]","c04b34b7":"# let's plot the null values for each shop\nplt.figure(figsize=(20, 10))\n# this lines gbr.unstack(level = -1).T.isnull()*1\n# converts any null to 1 and the rest will be 0\nsns.heatmap(gbr.isnull()*1, cmap = \"inferno\", cbar = True).set_title(\"Null values by shop and Month\");","c8d5b7d9":"# create a smaller df\nshort_df = df[[\"date\", \"item_cnt_day\", \"item_category_name\"]]\n\n# set the date to be the index (to resample later)\nshort_df.set_index(\"date\", inplace = True)\n\n# groupby by shop_name\ngb = short_df.groupby(\"item_category_name\")\n\n# resample the df by month sales (resample = groupby by months in timeseries)\ngbr = gb.resample(\"M\")[\"item_cnt_day\"].sum()\n\n# unstack the gbr to have columns name\ngbr = gbr.unstack(level = -1).T\n\n# sort the values, from no nulls to more null values\norder_of_columns = list(gbr.isnull().sum().sort_values().index)\n\n# change the order of the df\ngbr = gbr[order_of_columns]","4a6b382a":"# let's plot the null values for each shop\nplt.figure(figsize=(20, 10))\n\n# this lines gbr.unstack(level = -1).T.isnull()*1\n# converts any null to 1 and the rest will be 0\nsns.heatmap(gbr.isnull()*1, cmap = \"inferno\", cbar = True).set_title(\"Null values by item category and Month\");","0cd8a82d":"# let's look at outliers for item sales\n# We will use boxplots because they are very useful to see the distribution of values\nplt.figure(figsize = (10,4))\nsns.boxplot(x = df[\"item_cnt_day\"]);","78e7f98c":"# let's look at outliers for item price\nplt.figure(figsize = (10,4))\nplt.xlim(df[\"item_price\"].min(), df[\"item_price\"].max()*1.1)\nsns.boxplot(x = df[\"item_price\"]);","3e37e64d":"# joint plot is another very convenient way to plot the relationship between 2 variables\n# but because we have huge outliers, we don't see them \n# https:\/\/seaborn.pydata.org\/generated\/seaborn.jointplot.html\nplt.figure(figsize = (10,4))\nsns.jointplot(x = \"item_price\", y = \"item_cnt_day\", data = df);","77315bb5":"# let's filter the outliers and make the same joint plot\ndf = df[(df[\"item_price\"] < np.percentile(df[\"item_price\"], q = 99)) & (df[\"item_cnt_day\"] >= 0) & (df[\"item_cnt_day\"] < np.percentile(df[\"item_cnt_day\"], q = 99))]","550862c8":"# we have removed the outliers and now \nplt.figure(figsize = (10, 10))\nsns.jointplot(x = \"item_price\", y = \"item_cnt_day\", data = df);","fd3548f9":"<a id = \"fix_shops\"><\/a>\n# Fix shops df and generate some features\n[Go back to the Table of Contents](#table_of_contents)","d5a3c54e":"Analyzing data on a weekly basis, gives us much more information. We can see more variation between weeks, but the main point stays the same: we have spines in January and sales that go down overtime.","bb2ff0e8":"<a id = \"question_2\"><\/a>\n# Question 2: Create a decomposition plot for a city of weekly sales\n[Go back to the Table of Contents](#table_of_contents)","d655c0d8":"<a id =\"table_of_contents\"><\/a>\n# Table of contents\n\n\n[Imports](#imports)\n\n[Quick look at shops df](#quick_look_shops)\n\n[Fix shops df and generate some features](#fix_shops)\n\n[Quick look at item category df](#quick_look_item_cat)\n\n[Quick look at items df](#quick_look_item)\n\n[Quick look at sales df](#quick_look_sales)\n\n[Joining df](#join_df)\n\n[Exploratory Data Analysis (EDA)](#eda)\n\n[Viz of sales per week, month of shops and item_category columns](#sales_viz)\n\n[Total sales and the variation on secondary axis](#sales_viz_2_axis)\n\n--> [Question 1: Create a plot with the moving average of total sales (7 days) and the variation on the second axis.](#question_1)\n\n[Calendar heatmap](#calendar_heatmap)\n\n[Timeseries autocorrelation and partial autocorrelation plots: daily sales](#corr_plots_daily)\n\n[Manually calculate the Partial Autocorrelation](#autocorrelation_calculation)\n\n[Timeseries decomposition plots: weekly sales](#decomp_weekly)\n\n--> [Question 2: Create a decomposition plot for a city of weekly sales](#question_2)\n\n[Visualizing the most important cities](#viz_cities)\n\n--> [Question 3: Create a treemap plot for item_category and the total combined sales](#question_3)\n\n[Visualizing nulls values](#viz_null_values)\n\n[Visualization of outliers](#viz_outliers)\n\n[Conclusion](#conclusion)","9d03e3ec":"<a id = \"join_df\"><\/a>\n# Joining df\n[Go back to the Table of Contents](#table_of_contents)","5b5f8daa":"<a id = \"imports\"><\/a>\n# Imports\n[Go back to the Table of Contents](#table_of_contents)","1487417f":"In the next plots we will represent the monthly sales (left plot) and weekly sales (right plot) for each shop. \n\nIn the light red\/pink areas of each plot, we will mark the national holidays in Russia and see if there is any connection with sales spikes.","b2155a41":"<a id = \"quick_look_sales\"><\/a>\n# Quick look at sales df\n[Go back to the Table of Contents](#table_of_contents)","6e11092a":"<a id = \"question_3\"><\/a>\n# Question 3: Create a treemap plot for item_category and the total combined sales\n\n<span style=\"color:red\">If the % of a category over total is less 1%, don't put any label!!!<\/span>\n\n[Go back to the Table of Contents](#table_of_contents)","128c0a80":"<a id = \"decomp_weekly\"><\/a>\n# Timeseries decomposition plots: weekly sales\n[Go back to the Table of Contents](#table_of_contents)","6e2bd3cf":"# Welcome to this kernel\n\nThe goal of this kernel is very simple. It aims to provide some useful insights about the data and hopefully can guide you into what features to generate and how to tackle the modelling part.","503ee5ac":"<a id = \"quick_look_item_cat\"><\/a>\n# Quick look at items_category df\n[Go back to the Table of Contents](#table_of_contents)","bfe6eb7a":"<a id = \"autocorrelation_calculation\"><\/a>\n# Manually calculate the Partial Autocorrelation\n[Go back to the Table of Contents](#table_of_contents)","c083e0f1":"<a id = \"calendar_heatmap\"><\/a>\n# Calendar heatmap\n[Go back to the Table of Contents](#table_of_contents)","6e78186a":"<a id = \"conclusion\"><\/a>\n# Conclusion\n[Go back to the Table of Contents](#table_of_contents)\n\nAfter taking a look at the sales data, here are some conclusion we can extract:\n\n1. We see that the total sales decrease over time. This is very important because, we have to create features for our model that catch this trend.\n\n2. We have seen that the sales present huge spikes in Christmas season. Datetime features can help a lot our model.\n\n3. Data has a lot of missing values and we have not found a specific or category affected by this. More likely it's just the nature of the data.\n\n4. Top 3 cities capture more than 50% of total sales. City based features can be very helpful for the model.\n\n5. The top 3 categories represent more than 40% of total sales: they are Movies, PC Games and Music.\n\n6. Data presents outliers at the sales and price level. Before generating features or training a model, data must be cleaned properly.\n\n7. We have seen thanks to our calendar plots that we a small increase in sales on the weekends. We do see however bigger sales on 14 of February or 9 of May (holidays).","e299f587":"<a id = \"sales_viz\"><\/a>\n# Viz of sales per week, month of shops and item_category columns\n[Go back to the Table of Contents](#table_of_contents)","f848f336":"<a id = \"quick_look_shops\"><\/a>\n# Quick look at shops df\n[Go back to the Table of Contents](#table_of_contents)","c800efd6":"<a id = \"viz_outliers\"><\/a>\n# Visualization of outliers\n[Go back to the Table of Contents](#table_of_contents)","5b89902b":"<a id = \"viz_null_values\"><\/a>\n# Visualizing nulls values\n[Go back to the Table of Contents](#table_of_contents)","9c15cdf4":"<a id =\"question_1\"><\/a>\n# Question 1: Create a plot with the moving average of total sales (7 days) and the variation on the second axis.\n[Go back to the Table of Contents](#table_of_contents)","5e046489":"<a id = \"eda\"><\/a>\n# Exploratory Data Analysis (EDA)\n[Go back to the Table of Contents](#table_of_contents)","ab6d3554":"<a id = \"sales_viz_2_axis\"><\/a>\n# Total sales and the variation on secondary axis\n[Go back to the Table of Contents](#table_of_contents)","1a851643":"Treemaps are a very useful and visual tools to see different categories and their overall importance in a dataset.\nAlso, they are very cool and easy to make using Python and squarify.","cef5451e":"From our very first and simple figure, we can already extract very useful information.\n* First of all, we can see big spikes in January, like to be motivated with national holidays in Russia.\n* Second: we see a general trend to decline in our timeseries. If you are planning to use a parametrical model, you must take into account this.","0a806dc1":"In the next plots we will represent the monthly sales (left plot) and weekly sales (right plot) for each item category. \n\nWe will also plot the percentile 5 and 95 for each shop by year.\n\nIn the light red\/pink areas of each plot, we will mark the national holidays in Russia and see if there is any connection with sales spikes.","941c77ba":"<a id = \"viz_cities\"><\/a>\n# Visualizing the most important cities\n[Go back to the Table of Contents](#table_of_contents)","a9f7b8bd":"<a id = \"quick_look_item\"><\/a>\n# Quick look at items df\n[Go back to the Table of Contents](#table_of_contents)","d9a45e6a":"<a id = \"corr_plots_daily\"><\/a>\n# Timeseries autocorrelation and partial autocorrelation plots: daily sales\n[Go back to the Table of Contents](#table_of_contents)"}}