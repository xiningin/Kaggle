{"cell_type":{"f0141155":"code","eb6441f1":"code","d68c9835":"code","3d521934":"code","12f15be9":"code","810acd92":"code","1febef36":"markdown"},"source":{"f0141155":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline\n%pip install -U pip efficientnet==1.0.0","eb6441f1":"import scipy.io as sio\nimport os\n\nbatch_size = 64\ninput_shape = (240,240)\ndata_dir='\/kaggle\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/'\ntrain_dir = data_dir +'train'\ntest_dir = data_dir +'test'\n\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1. \/ 255,\n    zoom_range=0.2,  \n    rotation_range = 5,\n    horizontal_flip=True)\n\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator=train_datagen.flow_from_directory(train_dir,\n                                            class_mode=\"categorical\", \n                                            target_size=input_shape, \n                                            batch_size=batch_size)\n\n\nvalidation_generator=test_datagen.flow_from_directory(test_dir,\n                                            class_mode=\"categorical\", \n                                            target_size=input_shape, \n                                            batch_size=batch_size)\n","d68c9835":"from keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization\nfrom keras import Model, optimizers\nimport efficientnet.keras as efn\n\nbase_model = efn.EfficientNetB1(weights='imagenet', include_top=False)\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\npredictions = Dense(len(train_generator.class_indices), activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# fix the feature extraction part of the model\nfor layer in base_model.layers:\n    if isinstance(layer, BatchNormalization):\n        layer.trainable = True\n    else:\n        layer.trainable = False\n        \nmodel.compile(optimizer=optimizers.Adam(lr=0.01), loss='categorical_crossentropy', metrics=['acc'])\nmodel.summary()","3d521934":"history = model.fit_generator(generator=train_generator,\n                    steps_per_epoch=train_generator.samples \/\/ batch_size + 1 ,\n                    validation_data=validation_generator,\n                    validation_steps=validation_generator.samples \/\/ batch_size + 1,\n                    epochs=10,                           \n                    workers=8,             \n                    max_queue_size=32,             \n                    verbose=1)\n\nplt.plot(history.history['acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","12f15be9":"batch_size = 32\ntrain_generator=train_datagen.flow_from_directory(train_dir,\n                                            class_mode=\"categorical\", \n                                            target_size=input_shape, \n                                            batch_size=batch_size)\n\n\nvalidation_generator=test_datagen.flow_from_directory(test_dir,\n                                            class_mode=\"categorical\", \n                                            target_size=input_shape, \n                                            batch_size=batch_size)\n\n\nfor layer in model.layers:\n    layer.trainable = True\n    \nmodel.compile(optimizer=optimizers.SGD(lr=0.01), loss='categorical_crossentropy', metrics=['acc'])\nmodel.summary()","810acd92":"history = model.fit_generator(generator=train_generator,\n                    steps_per_epoch=train_generator.samples \/\/ batch_size + 1 ,\n                    validation_data=validation_generator,\n                    validation_steps=validation_generator.samples \/\/ batch_size + 1,\n                    epochs=10,                           \n                    workers=8,             \n                    max_queue_size=32,             \n                    verbose=1)\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","1febef36":"# Train Google's efficientNet (240 x 240 px) reaching 80% accuracy with pretrained imagenet weights\nfor more details visit [Transfer-Learning for Image classification with effificientNet](http:\/\/digital-thinking.de\/keras-transfer-learning-for-image-classification-with-effificientnet\/)"}}