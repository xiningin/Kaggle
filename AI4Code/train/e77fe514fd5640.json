{"cell_type":{"cc243b1a":"code","fdf5fb11":"code","e3f23549":"code","bba1dd8d":"code","e8616b2e":"code","62a4afa2":"code","ee438dd7":"code","112f9475":"code","4d7fa9f7":"code","c23f9bbb":"code","2dc3198a":"code","754bb76c":"code","e35505c4":"code","7ebe05fc":"code","1fcc4189":"code","ed0c84a9":"code","62aa8c24":"code","b6dab22e":"code","5c0276fa":"code","c58d9fc1":"markdown","5447e368":"markdown","40abd194":"markdown","d57c56cb":"markdown","d7613b61":"markdown","2b702582":"markdown","a153ef9c":"markdown","09c7f29f":"markdown","6b41f908":"markdown","bb32a698":"markdown","0b3e8d0e":"markdown","915f205f":"markdown","41d6bd7c":"markdown","8b70f1f2":"markdown","0f657bb8":"markdown","f70d2cee":"markdown","503e6d5c":"markdown","f38ebb9f":"markdown","309015c7":"markdown","e9f6e16c":"markdown","2d327334":"markdown","ac8844e1":"markdown","d50565d7":"markdown","d1d3530f":"markdown","f58f6009":"markdown","297eb493":"markdown","d676915c":"markdown","b15c6d33":"markdown","b6d38d52":"markdown","4aeb87db":"markdown","34091cf1":"markdown"},"source":{"cc243b1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fdf5fb11":"import matplotlib.pyplot as plt \nimport warnings\nwarnings.filterwarnings('ignore')","e3f23549":"dataset=pd.read_csv('..\/input\/social-network-ads\/Social_Network_Ads.csv')\ndataset.head()","bba1dd8d":"X=dataset.iloc[:,[2,3]].values\ny=dataset.iloc[:,4].values","e8616b2e":"from sklearn.model_selection import train_test_split   #cross_validation doesnt work any more\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0) \n#X_train","62a4afa2":"from sklearn.preprocessing import StandardScaler \nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)\n#X_train","ee438dd7":"from sklearn.tree import DecisionTreeClassifier\nclf=DecisionTreeClassifier(criterion='entropy',random_state=0)\nclf.fit(X_train,y_train)","112f9475":"from sklearn import tree\nplt.figure(figsize=(15,10))\ntree.plot_tree(clf,filled=True)\nplt.show()","4d7fa9f7":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\ny_pred = clf.predict(X_test)\ny_train = clf.predict(X_train)\nfrom sklearn.metrics import accuracy_score \nprint(\"Train Accuracy is:\",accuracy_score(y_train,y_train))\nprint(\"Test Accuracy is:\",accuracy_score(y_test,y_pred))","c23f9bbb":"cm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.show()","2dc3198a":"cr =classification_report(y_test,y_pred)\nprint(\"Classification Report\")\nprint(cr)","754bb76c":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,clf.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('Decision Tree (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","e35505c4":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,clf.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('Decision Tree (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n","7ebe05fc":"path = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nccp_alphas","1fcc4189":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","ed0c84a9":"train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","62aa8c24":"clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.12)\nclf.fit(X_train,y_train)","b6dab22e":"pred=clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, pred)","5c0276fa":"from sklearn import tree\nplt.figure(figsize=(15,10))\ntree.plot_tree(clf,filled=True)\nplt.show()","c58d9fc1":"Number of nodes in the last tree is: 1 with ccp_alpha: 0.2789340883820818","5447e368":"The datset we have the User ID, Gender,Age,Salary and the data if Purchase made by a user.","40abd194":"In this kernel we will be building a decision tree model.After that we will pruning the decision tree to avoid overfittig.In this notebook we will be covering following things\n\n1.Data Import and Preprocesing\n\n2.Feature Engineering\n\n3.Decision Tree Model Build\n\n4.Model Evaluation\n\n5.Decision Tree Prunning\n\n6.Conclusion","d57c56cb":"### Visualising the Test Set","d7613b61":"### Visualising the Training Set","2b702582":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https:\/\/lnkd.in\/gj7bMQA\n\n### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","a153ef9c":"### Splitting Data into Train and Test","09c7f29f":"We have scaled the data because we have considered Age and Estimated salary to make the purchase prediction.There is a big difference in the reange of these features.","6b41f908":"### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","bb32a698":"We can see that our tree has many nodes.So Splitting has taken place many times.The problem with this is that we may have good accuracy on our training Dataset because the model learns by heart the values.We we try to predict the result for unseen data (test) data there is possibility of our accuracy reducing.","0b3e8d0e":"We can see that we have 100% accuracy for our training test and around 92.5% accuracy for our test set.This shows that our training model set is overfitting.","915f205f":"# 2.Feature Engineering","41d6bd7c":"# 3.Model Built","8b70f1f2":"# 4.Model Evaluation","0f657bb8":"### Classification Report","f70d2cee":"### Accuracy Score","503e6d5c":"### Confusion Matrix","f38ebb9f":"### Ploting Decision Tree","309015c7":"From the above cure we can see that we can get train and test set accuracy when we select the alpha value between 0.02 to 0.27.So I am selecting a value of 0.12 for alpha while creating the decision tree model.","e9f6e16c":"### Training and Test Accurac Vs Alpha","2d327334":"We have quite good value of Accuracy and F1 Score.","ac8844e1":"From above training set results we can see that our model is overfitting on training set.We can reduce the overfitting by pruning our decision tree.","d50565d7":"### Importing Python Modules","d1d3530f":"### Pruned Decision Tree","f58f6009":"# 6.Conclusion\n\n1.In this data set we had Age,Salary,Sex and purchase decision.We have made use of Age,Salary to predict whether a customer will make a purchase decision.\n\n2.We have used feature scaling and then built a decision tree model to make purchase prediction.\n\n3.Based on the model evaluation we could conclude that our model had over fitting on the training set.\n\n4.We used Decision Tree Pruning technique to optimise over model.This reduced the size of our decision tree by considering an optimum value of alpha.","297eb493":"### Creating Matrix of Features","d676915c":"# 1.Importing Data and Pre Processing","b15c6d33":"# 5.Decision Tree Pruning\n\nIn the desision tree algorithm there are parameters like min_samples_leaf and max_depth to prevent the tree from overfitting.We can use another method cost complexity pruning to control the size of our trees.The complexity of tree is controlled by a parameter ccp_alpha.Greater value of ccp_alpha meanse more number of nodes in the tree are pruned.","b6d38d52":"### Feature Scaling","4aeb87db":"So by using alpha value as 0.12 we have been able to prune our decision tree to a large extent.This help in time reduction for training and also prevents overfitting of our models","34091cf1":"### Importing Dataset"}}