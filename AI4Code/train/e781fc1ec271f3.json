{"cell_type":{"0e24296e":"code","9a09a760":"code","00e84214":"code","ddd171b7":"code","00bce0d5":"code","59288381":"code","9bfd905c":"code","169b88b3":"code","16c2f2a1":"code","b28a6312":"code","6d82ecf6":"code","39af1011":"code","d340f837":"code","b6a783bc":"markdown","e16cef09":"markdown","b54fcffb":"markdown","db48eed0":"markdown","88f8395f":"markdown"},"source":{"0e24296e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a09a760":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator \nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, AveragePooling2D, BatchNormalization, LeakyReLU, Concatenate\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers, optimizers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint","00e84214":"# Load the data\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","ddd171b7":"y_train = train[\"label\"]\ny_train = to_categorical(y_train, 10)\n\nX_train = train.drop(labels = [\"label\"], axis = 1) \n\nX_train = X_train \/ 255 #Dividing by maximum value\nX_test = test \/ 255\n\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\n\nprint(\"X_train.shape:\", X_train.shape)\nprint(\"y_train.shape\", y_train.shape)","00bce0d5":"\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.1, random_state=1337)\n","59288381":"#Fully connected CNN model\nmodel = Sequential(name = 'cnn_mnist')\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu', input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n#opt = optimizers.Adam\nopt = optimizers.Adam()\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","9bfd905c":"model.summary()","169b88b3":"datagen = ImageDataGenerator(\n                            rotation_range=15,\n                            width_shift_range=0.1,\n                            height_shift_range=0.1,\n                            zoom_range=0.1,\n                            horizontal_flip=False,\n                            vertical_flip=False\n                            )\ndatagen.fit(X_train)","16c2f2a1":"rls = ReduceLROnPlateau(monitor='accuracy', mode = 'max', factor=0.5, min_lr=1e-7, verbose = 1, patience=5)\nes = EarlyStopping(monitor='accuracy', mode='max', verbose = 1, patience=50)\nmc = ModelCheckpoint('cnn_best_model.h5', monitor='accuracy', mode='max', verbose = 1, save_best_only=True)\n\ncallback_list = [rls, es, mc]","b28a6312":"model.fit_generator(datagen.flow(X_train, y_train, batch_size = 64),\n                                 validation_data = (X_valid, y_valid),\n                                 steps_per_epoch = X_train.shape[0] \/\/ 64, \n                                 epochs = 50, \n                                 verbose = 2,\n                                 callbacks = callback_list)","6d82ecf6":"def plot_model(history): \n    fig, axs = plt.subplots(1,2,figsize=(16,5)) \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy'], 'c') \n    axs[0].plot(history.history['val_accuracy'],'m') \n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy') \n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\n    axs[1].plot(history.history['loss'], 'c') \n    axs[1].plot(history.history['val_loss'], 'm') \n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss') \n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'validate'], loc='upper right')\n    plt.show()","39af1011":"plot_model(model.history)\n","d340f837":"saved_model = load_model('cnn_best_model.h5')\n\ny_pred = saved_model.predict_classes(X_test, verbose=0)\n\nsub = pd.DataFrame({\"ImageId\": list(range(1, len(y_pred)+1)),\n                          \"Label\": y_pred})\n\nsub.to_csv(\"submission_digit_recognizer.csv\", index=False)","b6a783bc":"**Feature Scaling**","e16cef09":"Building model","b54fcffb":"Splitting The given train dataset for train and validation","db48eed0":"**Data augmentation**-to create more data","88f8395f":"**Lets Evaluate our model**"}}