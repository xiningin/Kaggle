{"cell_type":{"d8dc0439":"code","5b6dd840":"code","c8864bf8":"code","3f750ddc":"code","8cb1eb84":"code","b2ba3f1e":"code","0c034748":"code","7cadad3f":"code","ecf49b87":"code","250431c4":"code","1b8ea2d1":"code","f8c77b26":"code","fdc49626":"markdown","46a68907":"markdown","527b25e8":"markdown","191b7802":"markdown","f6449412":"markdown","27566b2e":"markdown","e4856332":"markdown","c06fb5c8":"markdown","49fa8384":"markdown","a24d17da":"markdown"},"source":{"d8dc0439":"#import utility functions \nfrom covid19_data_processing_module import *\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport copy \nimport collections\n\n# key word linkage\nrisk_factors = ['smoking', 'reduced lung capacity', 'hand to mouth', 'lung disease', 'oxygen', \n                'chronic', 'cancer', 'high blood pressure', 'diabetes', 'cardiovascular', 'heart', \n                'chronic respiratory disease', 'heart disease']","5b6dd840":"# recreate the schema from \"json_schema.txt\"\nclass author():\n    \n    def __init__(self, input_dict=None):\n        \n        self.first = \"\"\n        self.middle = []\n        self.last = \"\"\n        self.suffix = \"\"\n        self.affiliation = {}\n        self.email = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"first\" in key:\n                    self.first = input_dict[key]\n                if \"middle\" in key:\n                    self.middle = input_dict[key]\n                if \"last\" in key:\n                    self.last = input_dict[key]\n                if \"suffix\" in key:\n                    self.suffix = input_dict[key]\n                if \"affiliation\" in key:\n                    self.affiliation = input_dict[key]\n                if \"email\" in key:\n                    self.email = input_dict[key]    \n    \n    def print_items(self):\n        \n        print(\"first: \" + str(self.first) +  \n              \", middle: \" + str(self.middle) + \n              \", last: \" + str(self.last) + \n              \", suffix: \" + str(self.suffix) +\n              \", email: \" + str(self.email) + \n              \", affiliation: \" + json.dumps(self.affiliation, indent=4, sort_keys=True)\n             )\n\n\nclass inline_ref_span():\n    \n    def __init__(self, input_dict=None):\n        \n        self.start = 0\n        self.end = 0\n        self.text = \"\"\n        self.ref_id = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"start\" in key:\n                    self.start = input_dict[key]\n                if \"end\" in key:\n                    self.end = input_dict[key]\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"ref_id\" in key:\n                    self.ref_id = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"Text: \" + str(self.text) + \", Start: \" + \n              str(self.start) + \", End: \" + str(self.end) + \n              \", Ref_id: \" + str(self.ref_id))\n\n    def step_index(self, n):\n        \n        self.start += n\n        self.end += n\n        \n        \nclass text_block():\n    \n    def __init__(self, input_dict=None):\n        \n        self.text = \"\"\n        self.cite_spans = []\n        self.ref_spans = []\n        self.eq_spans = []\n        self.section = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"cite_spans\" in key:\n                    self.cite_spans = [inline_ref_span(c) for c in input_dict[key]]                \n                if \"ref_spans\" in key:\n                    self.ref_spans = [inline_ref_span(r) for r in input_dict[key]] \n                if \"eq_spans\" in key:\n                    self.eq_spans = [inline_ref_span(e) for e in input_dict[key]]\n                if \"section\" in key:\n                    self.section = input_dict[key]\n        \n    def clean(self, swap_dict=None):\n            \n        self.text = clean(self.text, swap_dict)\n    \n    def print_items(self):\n        \n        print(\"\\ntext: \" + str(self.text))\n        print(\"\\nsection: \" + str(self.section))\n        print(\"\\ncite_spans: \")\n        [c.print_items() for c in self.cite_spans]\n        print(\"\\nref_spans: \")\n        [r.print_items() for r in self.ref_spans]\n        print(\"\\neq_spans: \")\n        [e.print_items() for e in self.eq_spans]\n\n\ndef combine_text_block(text_block_list):\n    \n    if text_block_list:\n        \n        combined_block = text_block_list[0]\n        block_length = len(combined_block.text)\n        \n        for i in range(1,len(text_block_list)):\n            combined_block.text += \" \" + text_block_list[i].text\n            block_length += 1\n            \n            # update spans start & stop index\n            [ref.step_index(block_length) for ref in text_block_list[i].cite_spans]\n            [ref.step_index(block_length) for ref in text_block_list[i].ref_spans]\n            [ref.step_index(block_length) for ref in text_block_list[i].eq_spans]\n            \n            # combine spans\n            combined_block.cite_spans += text_block_list[i].cite_spans\n            combined_block.ref_spans += text_block_list[i].ref_spans\n            combined_block.eq_spans += text_block_list[i].eq_spans           \n            combined_block.section += \", \" + str(text_block_list[i].section)           \n            \n            block_length += len(text_block_list[i].text)\n                       \n        return [combined_block]\n    else:\n        return [text_block()]\n      \n\nclass bib_item():\n    \n    def __init__(self, input_dict=None):\n        \n        self.ref_id: \"\"\n        self.title: \"\"\n        self.authors = []\n        self.year = 0\n        self.venue = \"\"\n        self.volume = \"\"\n        self.issn = \"\"\n        self.pages = \"\"\n        self.other_ids = {}\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"ref_id\" in key:\n                    self.ref_id = input_dict[key]\n                if \"title\" in key:\n                    self.title = input_dict[key]\n                if \"authors\" in key:\n                    self.authors = [author(a) for a in input_dict[key]]\n                if \"year\" in key:\n                    self.year = input_dict[key]\n                if \"venue\" in key:\n                    self.venue = input_dict[key]\n                if \"volume\" in key:\n                    self.volume = input_dict[key]\n                if \"issn\" in key:\n                    self.issn = input_dict[key]\n                if \"pages\" in key:\n                    self.pages = input_dict[key]\n                if \"other_ids\" in key:\n                    self.other_ids = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"\\nBib Item:\")\n        print(\"ref_id: \" + str(self.ref_id))\n        print(\"title:\" + str(self.title))\n        print(\"Authors:\")\n        [a.print_items() for a in self.authors]\n        print(\"year: \" + str(self.year))\n        print(\"venue:\" + str(self.venue))\n        print(\"issn:\" + str(self.issn))\n        print(\"pages:\" + str(self.pages))\n        print(\"other_ids:\" + json.dumps(self.other_ids, indent=4, sort_keys=True))\n        \n        \nclass ref_entries():\n    \n    def __init__(self, ref_id=None, input_dict=None):\n        \n        self.ref_id = \"\"\n        self.text = \"\"\n        self.latex = None\n        self.type = \"\"\n        \n        if ref_id:\n            self.ref_id = ref_id\n            \n            if input_dict:\n                for key in input_dict.keys():\n                    if \"text\" in key:\n                        self.text = input_dict[key]\n                    if \"latex\" in key:\n                        self.latex = input_dict[key]\n                    if \"type\" in key:\n                        self.type = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"ref_id: \" + str(self.ref_id))\n        print(\"text:\" + str(self.text))\n        print(\"latex: \" + str(self.latex))\n        print(\"type:\" + str(self.type))\n        \n                    \nclass back_matter():\n    \n    def __init__(self, input_dict=None):\n        \n        self.text = \"\"\n        self.cite_spans = []\n        self.ref_spans = []\n        self.section = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"cite_spans\" in key:\n                    self.cite_spans = [inline_ref_span(c) for c in input_dict[key]]                \n                if \"ref_spans\" in key:\n                    self.ref_spans = [inline_ref_span(r) for r in input_dict[key]] \n                if \"section\" in key:\n                    self.section = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"text: \" + str(self.text))\n        print(\"cite_spans: \")\n        [c.print_items() for c in self.cite_spans]\n        print(\"ref_spans: \")\n        [r.print_items() for r in self.ref_spans]        \n        print(\"section:\" + str(self.section))\n\n        \n# The following Class Definition is a useful helper object to store various \n# different covid-19 data types.\nclass document():\n    \n    def __init__(self, file_path=None):\n        \n        self.doc_filename = \"\"\n        self.doc_language = {}\n        self.paper_id = \"\"\n        self.title = \"\"\n        self.authors = []\n        self.abstract = []\n        self.text = []\n        self.bib = []\n        self.ref_entries = []\n        self.back_matter = []\n        self.tripples = {}\n        self.key_phrases = {}\n        self.entities = {}\n        \n        # load content from file on obj creation\n        self.load_file(file_path)\n     \n    def _load_paper_id(self, data):\n        \n        if \"paper_id\" in data.keys():\n            self.paper_id = data['paper_id']\n    \n    def _load_title(self, data):\n        \n        if \"metadata\" in data.keys():\n            if \"title\" in data['metadata'].keys():\n                self.title = data['metadata'][\"title\"]\n    \n    def _load_authors(self, data):\n        \n        if \"metadata\" in data.keys():\n            if \"authors\" in data['metadata'].keys():\n                self.authors = [author(a) for a in data['metadata'][\"authors\"]]\n                \n    def _load_abstract(self, data):\n        \n        if \"abstract\" in data.keys():\n            self.abstract = [text_block(a) for a in data[\"abstract\"]]\n    \n    def _load_body_text(self, data):\n        \n        if \"body_text\" in data.keys():\n            self.text = [text_block(t) for t in data[\"body_text\"]]\n    \n    def _load_bib(self, data):\n        \n        if \"bib_entries\" in data.keys():\n            self.bib = [bib_item(b) for b in data[\"bib_entries\"].values()]\n    \n    def _load_ref_entries(self, data):\n        \n        if \"ref_entries\" in data.keys():\n            self.ref_entries = [ref_entries(r, data[\"ref_entries\"][r]) for r in data[\"ref_entries\"].keys()]\n            \n    def _load_back_matter(self, data):\n        \n        if \"back_matter\" in data.keys():\n            self.back_matter = [back_matter(b) for b in data[\"back_matter\"]]\n        \n    def load_file(self, file_path):\n        \n        if file_path:\n            \n            with open(file_path) as file:\n                data = json.load(file)\n                \n                # call inbuilt data loading functions\n                self.doc_filename = file_path\n                self._load_paper_id(data)\n                self._load_title(data)\n                self._load_authors(data)\n                self._load_abstract(data)\n                self._load_body_text(data)\n                self._load_bib(data)\n                self._load_ref_entries(data)\n                self._load_back_matter(data)\n    \n    def combine_data(self):\n        \n        self.data = {'doc_filename': self.doc_filename,\n                     'doc_language': self.doc_language,\n                     'paper_id': self.paper_id,\n                     'title': self.title,\n                     'authors':self.authors,\n                     'abstract': self.abstract,\n                     'text': self.text,\n                     'bib_entries':self.bib,\n                     'ref_entries': self.ref_entries,\n                     'back_matter': self.back_matter,\n                     'tripples': self.tripples,\n                     'key_phrases': self.key_phrases,\n                     'entities': self.entities}\n\n    def extract_data(self):\n        \n        self.doc_filename = self.data['doc_filename']\n        self.doc_language = self.data['doc_language']\n        self.paper_id = self.data['paper_id']\n        self.title = self.data['title']\n        self.authors = self.data['authors']\n        self.abstract = self.data['abstract']\n        self.text = self.data['text']        \n        self.bib = self.data['bib_entries']\n        self.ref_entries = self.data['ref_entries']\n        self.back_matter = self.data['back_matter']\n        self.tripples = self.data['tripples']\n        self.key_phrases = self.data['key_phrases']\n        self.entities = self.data['entities']\n\n    def save(self, dir):\n        \n        self.combine_data()\n\n        if not os.path.exists(os.path.dirname(dir)):\n            try:\n                os.makedirs(os.path.dirname(dir))\n            except OSError as exc:  # Guard against race condition\n                if exc.errno != errno.EEXIST:\n                    raise\n\n        with open(dir, 'w') as json_file:\n            json_file.write(json.dumps(self.data))\n\n    def load_saved_data(self, dir):\n        \n        with open(dir) as json_file:\n            self.data = json.load(json_file)\n        self.extract_data()\n    \n    def print_items(self):\n         \n        print(\"---- Document Content ----\") \n        print(\"doc_filename: \" + str(self.doc_filename))\n        print(\"doc_language: \" + str(self.doc_language))\n        print(\"paper_id: \" + str(self.paper_id))\n        print(\"title: \" + str(self.title))\n        print(\"\\nAuthors: \")\n        [a.print_items() for a in self.authors]\n        print(\"\\nAbstract: \")\n        [a.print_items() for a in self.abstract]\n        print(\"\\nText: \")\n        [t.print_items() for t in self.text]\n        print(\"\\nBib_entries: \")\n        [b.print_items() for b in self.bib]\n        print(\"\\nRef_entries: \")\n        [r.print_items() for r in self.ref_entries]\n        print(\"\\nBack_matter: \")\n        [b.print_items() for b in self.back_matter]\n        \n        print(\"\\nTripples: \")\n        print(json.dumps(self.tripples, indent=4, sort_keys=True))\n        print(\"\\nKey Phrases: \")\n        print(json.dumps(self.key_phrases, indent=4, sort_keys=True))        \n        print(\"\\nEntities: \")\n        print(json.dumps(self.entities, indent=4, sort_keys=True))\n\n    def clean_text(self, swap_dict=None):\n        \n        # clean all blocks of text\n        [t.clean(swap_dict) for t in self.text]\n    \n    def clean_abstract(self, swap_dict=None):\n        \n        [t.clean(swap_dict) for t in self.abstract]\n    \n    def combine_text(self):\n        \n        # this function takes all text blocks within document.text and combines them into a single text_block object\n        self.text = combine_text_block(self.text)\n    \n    def combine_abstract(self):\n        \n        self.abstract = combine_text_block(self.abstract)   \n        \n    def set_abstract_tripples(self):\n                \n        abstract_tripples = {}\n        for i in range(0, len(self.abstract)):\n            #for every block in the abstract, extract entity tripples\n            self.abstract[i].clean()                       \n            pairs, entities = get_entity_pairs(self.abstract[i].text)\n            \n            #if any tripples found\n            if pairs.shape[0]>0:\n                abstract_tripples[\"abstract_\" + str(i)] = pairs.to_json()\n                       \n        self.tripples.update(abstract_tripples)\n        \n    def set_text_tripples(self):\n        \n        text_tripples = {}\n        for i in range(0, len(self.text)):\n            \n            self.text[i].clean()                       \n            pairs, entities = get_entity_pairs(self.text[i].text)\n            if pairs.shape[0]>0:\n                text_tripples[\"text_\" + str(i)] = pairs.to_json()\n                       \n        self.tripples.update(text_tripples)\n        \n    def set_ref_tripples(self):\n        \n        ref_tripples = {}\n        for r in self.ref_entries:\n            pairs, entities = get_entity_pairs(r.text)\n            if pairs.shape[0]>0:\n                ref_tripples[\"ref_\" + r.ref_id] = pairs.to_json()\n        \n        self.tripples.update(ref_tripples)\n        \n    def set_doc_language(self):\n        # set the doc language based on the analysis of the first block within the abstract\n        self.doc_language = get_text_language(self.text[0].text)\n    ","c8864bf8":"dir_input_data = '\/kaggle\/input\/load-and-process-data-abstracts'\n\nfiles = []\nimport os\nfor dirname, _, filenames in os.walk(dir_input_data):\n        filenames = [names for names in filenames if '.pickle' in names]\n        if filenames != []:\n            files.append({'dirpath':dirname, 'filenames':filenames})","3f750ddc":"directory = files[0][\"dirpath\"]\nfilenames = files[0][\"filenames\"]\n\ncorpus_documents = {}\ndocument_linkage_df = []\nrisk_factor_publications = collections.defaultdict(list)\n\ndoc_count = 0\nfor file in filenames:        \n  \n    with open(os.path.join(directory, file),\"rb\") as f:\n        doc_list = pickle.load(f)\n        \n        for doc in doc_list:\n               \n            if doc:\n                \n                title = copy.deepcopy(doc.title)\n                if title not in corpus_documents.keys():\n                    doc_count += 1\n                    corpus_documents.update({title: doc_count})\n                \n                doc_id = corpus_documents[title]\n        \n                ref_titles = []\n                for b in doc.bib:\n                    title = copy.deepcopy(b.title)\n                    if title not in corpus_documents.keys():\n                        doc_count += 1\n                        corpus_documents.update({title: doc_count})\n                                                \n                    ref_titles.append(corpus_documents[title])\n                \n                \n                df = pd.DataFrame({\"object\": [doc_id for r in ref_titles],\n                                   'relation': [\"has reference\" for r in ref_titles],\n                                   'subject': ref_titles})\n                \n                for risk in risk_factors:\n                    if risk in doc.abstract:\n                        risk_factor_publications[risk].append(doc_id)\n                        \n                document_linkage_df.append(df)\n                doc_count += 1\n                \ndocument_linkage_df = pd.concat(document_linkage_df)\n\n# save corpus_documents for cross reference later\nwith open('corpus_documents_lookup.json', 'w', encoding='utf-8') as f:\n    json.dump(corpus_documents, f, ensure_ascii=False, indent=4)\n    \n# del after save to save memory \ndel corpus_documents","8cb1eb84":"def create_kg(pairs):\n    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',edge_attr = ['relation'],\n            create_using=nx.DiGraph())\n    return k_graph","b2ba3f1e":"G = create_kg(document_linkage_df)\nprint(nx.info(G))","0c034748":"def get_corpus_labels(corpus_dir, index):\n    with open(corpus_dir) as file:\n                corpus = json.load(file)\n    return {value:key for key, value in corpus.items() if value in index}\n            ","7cadad3f":"node_rank = nx.degree_centrality(G)\nnode_rank_sorted = {k: v for k, v in sorted(node_rank.items(), key=lambda item: item[1],reverse=True)}\ntop_nodes = [k for k in node_rank_sorted.keys()][1:1000]\n\nG_sub = G.subgraph(top_nodes)\nmapping = get_corpus_labels('corpus_documents_lookup.json', list(G_sub.nodes))\nG_sub = nx.relabel_nodes(G_sub, mapping)\n\n\nsubject = []\nobj = []\nrelation = []\ntasks = []\nfor element in list(G_sub.edges()):\n        subject.append(element[0])\n        obj.append(element[1])\n        relation.append(G_sub.get_edge_data(element[0],element[1])['relation'])\n        \nnode_deg = nx.degree(G_sub)\nlayout = nx.spring_layout(G_sub, k=0.25, iterations=20)\nplt.figure(num=None, figsize=(120, 90), dpi=80)\nnx.draw_networkx(\n    G_sub,\n    node_size=[int(deg[1]) * 500 for deg in node_deg],\n    arrowsize=20,\n    linewidths=1.5,\n    pos=layout,\n    edge_color='red',\n    edgecolors='black',\n    node_color='white',\n    )\n\n\n\nlabels = dict(zip(list(zip(subject, obj)),relation))\nnx.draw_networkx_edge_labels(G_sub, pos=layout, edge_labels=labels,\n                                 font_color='black')\nplt.axis('off')\nplt.show()","ecf49b87":"corpus_documents = document_linkage_df['object'].drop_duplicates().tolist()\npr = nx.pagerank(G)\npr_df = pd.DataFrame({'Publications':list(pr.keys()), 'Ranking':list(pr.values())})\npr_sub_df = pr_df[pr_df.Publications.isin(corpus_documents)]","250431c4":"pr_df.to_csv('Page_Rank.csv', index=False)\npr_sub_df.to_csv('Page_Rank_Sub.csv', index=False)","1b8ea2d1":"for k,v in risk_factor_publications.items():\n    print(k, len(risk_factor_publications[k]))","f8c77b26":"top_nodes = [v_sub for k,v in risk_factor_publications.items() for v_sub in v]\n\nG_sub = G.subgraph(top_nodes)\nmapping = get_corpus_labels('corpus_documents_lookup.json', list(G_sub.nodes))\nG_sub = nx.relabel_nodes(G_sub, mapping)\nG_sub.remove_node('')\n\n\nsubject = []\nobj = []\nrelation = []\ntasks = []\nfor element in list(G_sub.edges()):\n        subject.append(element[0])\n        obj.append(element[1])\n        relation.append(G_sub.get_edge_data(element[0],element[1])['relation'])\n        \nnode_deg = nx.degree(G_sub)\nlayout = nx.spring_layout(G_sub, k=0.25, iterations=20)\nplt.figure(num=None, figsize=(120, 90), dpi=80)\nnx.draw_networkx(\n    G_sub,\n    node_size=[int(deg[1]) * 500 for deg in node_deg],\n    arrowsize=20,\n    linewidths=1.5,\n    pos=layout,\n    edge_color='red',\n    edgecolors='black',\n    node_color='white',\n    )\n\n\n\nlabels = dict(zip(list(zip(subject, obj)),relation))\nnx.draw_networkx_edge_labels(G_sub, pos=layout, edge_labels=labels,\n                                 font_color='black')\nplt.axis('off')\nplt.show()","fdc49626":"Using the pagerank algorithm extract the most impactful publications","46a68907":"Load pre-processed files containing abstracts entities, publications and author information. Files processed using this [notebook](https:\/\/www.kaggle.com\/johndoyle\/load-and-process-data-abstracts)","527b25e8":"# Publication Analysis\nEach publication in the courpus contains a number of cited articles which they reference information, methods or results for use or comparative analysis within their own work. This methodology is used to rank both pubications and authors based on the contribution a publication has made over time. \n\nIn this analysis the linkage of publications will be used to determine the most impactful articles that form the core material and potentially rank the findings of each article.\n\nThe following analysis is build on top of data processing and aggirgiation carried out in a previous notebook, please find the link below:\n[Load and Process Data Abstracts](https:\/\/www.kaggle.com\/johndoyle\/load-and-process-data-abstracts)\n","191b7802":"Build the knowledge graph and evaluate a subset of top nodes.\n","f6449412":"Output PageRank dictionary to json file. ","27566b2e":"A more refined analysis of risk factors will examine publications under the following topics\n**Risk Factors relating to the likelihood of experiencing a severe illness once infected**","e4856332":"## Find a reduced set of publications containg elements of our risk dictionary ","c06fb5c8":"Define Document Structure","49fa8384":"## Run Code to Extract Linkages","a24d17da":"Extract linkages using document title and reference title."}}