{"cell_type":{"cf250b18":"code","236ab594":"code","6569418e":"code","ac6c0e5c":"code","4f2bb51f":"markdown","5e4da548":"markdown","c9c6bc11":"markdown","e00401ea":"markdown","8543a92f":"markdown","c6b5bada":"markdown"},"source":{"cf250b18":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import preprocessing\nimport os","236ab594":"data = np.loadtxt('..\/input\/ex2data2.txt', delimiter = ',')\nx = data[:,:2]\ny = data[:,2].astype(int)\ncolor = ['blue', 'red']\nplt.figure(figsize = (8, 8))\nplt.xlabel('test 1 score')\nplt.ylabel('test 2 score')\nplt.scatter(x[y==0,0], x[y==0,1], c=color[0], marker='+', s=30)\nplt.scatter(x[y==1,0], x[y==1,1], c=color[1])\nplt.legend(('rejected', 'accepted'))\nplt.show()","6569418e":"np.random.seed(2018)\ntrain = np.random.choice([True, False], len(data), replace=True, p=[0.5, 0.5])\nx_train = data[train,:2]\ny_train = data[train,2].astype(int)\nx_test = data[~train,:2]\ny_test = data[~train,2].astype(int)\npoly2=preprocessing.PolynomialFeatures(2)\npoly3=preprocessing.PolynomialFeatures(3)\npoly4=preprocessing.PolynomialFeatures(4)\nx2=poly2.fit_transform(x)\nx3=poly3.fit_transform(x)\nx4=poly4.fit_transform(x)\nx_train_2=x2[train,:]\nx_test_2=x2[~train,:]\nx_train_3=x3[train,:]\nx_test_3=x3[~train,:]\nx_train_4=x4[train,:]\nx_test_4=x4[~train,:]\nregr1=linear_model.LogisticRegression(C=100)\nregr2=linear_model.LogisticRegression(C=100)\nregr3=linear_model.LogisticRegression(C=100)\nregr4=linear_model.LogisticRegression(C=100)\nregr1.fit(x_train ,y_train)\nregr2.fit(x_train_2,y_train)\nregr3.fit(x_train_3,y_train)\nregr4.fit(x_train_4,y_train)\nprint('LR-model score:',regr1.score(x_test,y_test))\nprint('2-degree-polynomialFeatures-LR-model score:',regr2.score(x_test_2,y_test))\nprint('3-degree-polynomialFeatures-LR-model score:',regr3.score(x_test_3,y_test))\nprint('4-degree-polynomialFeatures-LR-model score:',regr4.score(x_test_4,y_test))\nfrom sklearn.metrics import recall_score\nprediction_2 = regr2.predict(x_test_2)\nprint('recall-score of 2-degree-polynomialFeatures-LR-model:',recall_score(y_test, prediction_2))\nfrom sklearn.metrics import precision_score\nprint('precision-score of 2-degree-polynomialFeatures-LR-model:',precision_score(y_test, prediction_2))","ac6c0e5c":"gnb1=GaussianNB()\ngnb2=GaussianNB()\ngnb3=GaussianNB()\ngnb4=GaussianNB()\ngnb1.fit(x_train ,y_train)\ngnb2.fit(x_train_2 ,y_train)\ngnb3.fit(x_train_3 ,y_train)\ngnb4.fit(x_train_4 ,y_train)\nprint('GNB-model score:',gnb1.score(x_test,y_test))\nprint('2-degree-polynomialFeatures-GNB-model score:',gnb2.score(x_test_2,y_test))\nprint('3-degree-polynomialFeatures-GNB-model score:',gnb3.score(x_test_3,y_test))\nprint('4-degree-polynomialFeatures-GNB-model score:',gnb4.score(x_test_4,y_test))\nfrom sklearn.metrics import recall_score\ngnb3_prediction = gnb3.predict(x_test_3)\nprint('recall-score of 3-degree-polynomialFeatures-GNB-model:',recall_score(y_test, gnb3_prediction))\nfrom sklearn.metrics import precision_score\nprint('precision-score of 3-degree-polynomialFeatures-GNB-model:',precision_score(y_test,gnb3_prediction))","4f2bb51f":"## Data Infomation\n **Continuous variable: **\n <br>\n Frst-test result: range from -0.83007 to 1.0709\n <br>\n Second-test result: range from -0.76974 to1.1089\n <br>\n** Binary variable: **\n <br>\nQuality: 0 for reject, 1 for accept","5e4da548":"## Build Gaussian Naive Bayes Model","c9c6bc11":"## Model Conclusion\n* Polynomial model has better performance than one-degree-feature model.\n* Logistic regression model has higher accuracy score than Gaussian Naive Bayes model.\n*  2-degree-polynomial-feature logistic regression model has highest presicion score(0.93).\n*  3-degree-polynomial-feature GNB model has highest recall score(0.85).\n","e00401ea":"# Microchip quality Classifier","8543a92f":"## Problem Statement\nDuring QA, each microchip goes through two tests. From the two test results, determine whether the microchips should be accepted or rejected. dataset includes test results on past microchips.","c6b5bada":"## Build Logistic Regression Model"}}