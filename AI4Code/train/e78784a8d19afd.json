{"cell_type":{"640afcc9":"code","abb73cca":"code","59fe6310":"code","d9e243fe":"code","db161a25":"code","963dc966":"code","8510e207":"code","0f316b69":"code","3d8dcb44":"code","b87f3d15":"code","30701f8c":"code","8f91da6e":"code","ede74561":"code","9f38e95b":"code","091d78d7":"code","c87b8f08":"markdown","743972ce":"markdown","1be87bab":"markdown","acb03531":"markdown","4cd8c3e1":"markdown","de92cbb5":"markdown","590a584c":"markdown","ccf1bba2":"markdown","f793e631":"markdown","5109d8c6":"markdown","6ca60024":"markdown"},"source":{"640afcc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","abb73cca":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom catboost import CatBoostClassifier, Pool\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold # For creating folds\nfrom sklearn.metrics import log_loss # Evaluation metrics","59fe6310":"df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")\nss = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","d9e243fe":"print(f\"Shape of train : {df.shape}\")\nprint(f\"Shape of test : {test.shape}\")\nprint(f\"Shape of sample submission : {ss.shape}\")","db161a25":"df.head()","963dc966":"df.info()","8510e207":"test.info()","0f316b69":"sns.countplot(x= df.target)\n","3d8dcb44":"df[\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.target\nkf = StratifiedKFold(n_splits=5)\nfor f, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n  df.loc[v_,\"kfold\"] = f","b87f3d15":"cat_features = [f\"feature_{i}\" for i in range(75)]","30701f8c":"cat = CatBoostClassifier(task_type='GPU',\n                         iterations=3000,\n                         loss_function='MultiClass',\n                         random_state = 42,\n                         early_stopping_rounds=500,\n                         verbose = 100)","8f91da6e":"logloss = []\ncat_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test.drop([\"id\"], axis=1)\n    \n    #Creating pool\n    train_pool = Pool(data=X_train,label=y_train,cat_features=cat_features)\n    valid_pool = Pool(data=X_valid,label=y_valid,cat_features=cat_features)\n    \n    #Fitting the model\n    cat.fit(train_pool, eval_set=valid_pool, verbose=100)\n    \n    #Predicting for valid and test datasets\n    valid_preds = cat.predict_proba(X_valid)\n    cat_pred += cat.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","ede74561":"lgbm = LGBMClassifier(random_state=42)\nlogloss = []\nlgbm_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test.drop([\"id\"], axis=1)\n    \n    \n    #Fitting the model\n    lgbm.fit(X_train,y_train)\n    \n    #Predicting for valid and test datasets\n    valid_preds = lgbm.predict_proba(X_valid)\n    lgbm_pred += lgbm.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","9f38e95b":"avg_pred = []\navg_pred.append((cat_pred[:,0] + lgbm_pred[:,0]) \/ 2)\navg_pred.append((cat_pred[:,1] + lgbm_pred[:,1]) \/ 2)\navg_pred.append((cat_pred[:,2] + lgbm_pred[:,2]) \/ 2)\navg_pred.append((cat_pred[:,3] + lgbm_pred[:,3]) \/ 2)\navg_pred.append((cat_pred[:,4] + lgbm_pred[:,4]) \/ 2)\navg_pred.append((cat_pred[:,5] + lgbm_pred[:,5]) \/ 2)\navg_pred.append((cat_pred[:,6] + lgbm_pred[:,6]) \/ 2)\navg_pred.append((cat_pred[:,7] + lgbm_pred[:,7]) \/ 2)\navg_pred.append((cat_pred[:,8] + lgbm_pred[:,8]) \/ 2)","091d78d7":"ss[\"Class_1\"] = avg_pred[0]\nss[\"Class_2\"] = avg_pred[1]\nss[\"Class_3\"] = avg_pred[2]\nss[\"Class_4\"] = avg_pred[3]\nss[\"Class_5\"] = avg_pred[4]\nss[\"Class_6\"] = avg_pred[5]\nss[\"Class_7\"] = avg_pred[6]\nss[\"Class_8\"] = avg_pred[7]\nss[\"Class_9\"] = avg_pred[8]\nss.to_csv(\"\/kaggle\/working\/sub.csv\", index=False)","c87b8f08":"**The average log loss is 1.756480218567192**","743972ce":"Defining a variable with all the categorical features to pass to catboost classifier","1be87bab":"# Blending","acb03531":"Creating folds for the train dataset, so that we can train the model for the n folds, to avoid overfitting.","4cd8c3e1":"Traing and evaluating the model in the 5 folds cross validated manner.","de92cbb5":"# CatBoost","590a584c":"There are no missing values in the both train and test datasets and all are integers, so the categories might be encoded already.","ccf1bba2":"# Reading the train, test and sample submission file","f793e631":"# LGBM","5109d8c6":"Target column is imbalanced, so I will use StratifiedKFold for cross validation.","6ca60024":"**The average log loss is 1.7486608709950402**"}}