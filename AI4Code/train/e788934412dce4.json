{"cell_type":{"95420649":"code","5dbbe943":"code","69fe121e":"code","a1154db8":"code","7c1bfdb1":"code","958a9e8a":"code","fe986cb9":"code","e59a02d8":"code","308d1a2b":"code","6fa3839e":"code","3ae1b928":"code","9de76ae4":"code","75a93e37":"code","548c087d":"code","8307df30":"code","c32a668d":"code","593b1a89":"code","a341f032":"code","ef8a4120":"code","2fcc49ca":"code","760542d5":"code","ebe3e96e":"code","b2ae8815":"code","e739d957":"code","a7dfdc4c":"code","15fe2235":"code","d59519d2":"code","896dd506":"code","c5d5fc53":"code","d64103f2":"code","a6a1a88c":"code","8fca8f58":"code","0b8725e3":"code","63bc2bdd":"code","a95ae35a":"code","36bffe75":"code","caa9968b":"code","6ed7d3ed":"markdown","5fc4bd48":"markdown","a34b88cf":"markdown","88683d44":"markdown","daa00b68":"markdown","7af476da":"markdown","47abfa3b":"markdown","b5af84d9":"markdown","21fb0bc5":"markdown","7dec6b47":"markdown","62720b42":"markdown","3a7943b7":"markdown","57821ccb":"markdown","1a48a9a9":"markdown","efe3584c":"markdown","fc413f71":"markdown","198c721d":"markdown","9111e9c4":"markdown"},"source":{"95420649":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5dbbe943":"# imports\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import signal\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, plot_confusion_matrix\nfrom keras.models import Model\nimport keras.layers as L\nimport lightgbm as lgb","69fe121e":"# read data\n# data = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv')\n# data = pd.read_csv('..\/input\/clean-kalman\/train_clean_kalman.csv')\n# data = pd.read_csv('..\/input\/data-without-drift-with-kalman-filter\/train.csv')\n# data = pd.read_csv('..\/input\/clean-2apply-kalman\/train_clean_2apply_kalman.csv')\n# data = pd.read_csv('..\/input\/clean-3apply-kalman\/train_clean_3apply_kalman.csv')\n# data = pd.read_csv('..\/input\/clean-4apply-kalman\/train_clean_4apply_kalman.csv')\ndata = pd.read_csv('..\/input\/clean-5apply-kalman\/train_clean_5apply_kalman.csv')\n\ndata.head()","a1154db8":"def calc_gradients(s, n_grads=4):\n    '''\n    Calculate gradients for a pandas series. Returns the same number of samples\n    '''\n    grads = pd.DataFrame()\n    \n    g = s.values\n    for i in range(n_grads):\n        g = np.gradient(g)\n        grads['grad_' + str(i+1)] = g\n        \n    return grads","7c1bfdb1":"def calc_low_pass(s, n_filts=10):\n    '''\n    Applies low pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.3, n_filts)\n    \n    low_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='low')\n        zi = signal.lfilter_zi(b, a)\n        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return low_pass","958a9e8a":"def calc_high_pass(s, n_filts=10):\n    '''\n    Applies high pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.1, n_filts)\n    \n    high_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='high')\n        zi = signal.lfilter_zi(b, a)\n        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return high_pass","fe986cb9":"def calc_roll_stats(s, windows=[10, 50, 100, 500, 1000]):\n    '''\n    Calculates rolling stats like mean, std, min, max...\n    '''\n    roll_stats = pd.DataFrame()\n    for w in windows:\n        roll_stats['roll_mean_' + str(w)] = s.rolling(window=w, min_periods=1).mean()\n        roll_stats['roll_std_' + str(w)] = s.rolling(window=w, min_periods=1).std()\n        roll_stats['roll_min_' + str(w)] = s.rolling(window=w, min_periods=1).min()\n        roll_stats['roll_max_' + str(w)] = s.rolling(window=w, min_periods=1).max()\n        roll_stats['roll_range_' + str(w)] = roll_stats['roll_max_' + str(w)] - roll_stats['roll_min_' + str(w)]\n        roll_stats['roll_q10_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.10)\n        roll_stats['roll_q25_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.25)\n        roll_stats['roll_q50_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.50)\n        roll_stats['roll_q75_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.75)\n        roll_stats['roll_q90_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.90)\n    \n    # add zeros when na values (std)\n    roll_stats = roll_stats.fillna(value=0)\n             \n    return roll_stats","e59a02d8":"def calc_ewm(s, windows=[10, 50, 100, 500, 1000]):\n    '''\n    Calculates exponential weighted functions\n    '''\n    ewm = pd.DataFrame()\n    for w in windows:\n        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n        \n    # add zeros when na values (std)\n    ewm = ewm.fillna(value=0)\n        \n    return ewm","308d1a2b":"def add_features(s):\n    '''\n    All calculations together\n    '''\n    print(s)\n    gradients = calc_gradients(s)\n    low_pass = calc_low_pass(s)\n    high_pass = calc_high_pass(s)\n    roll_stats = calc_roll_stats(s)\n#     kf_stats = calc_roll_stats_kf(s)\n    ewm = calc_ewm(s)\n    \n    return pd.concat([s, gradients, low_pass, high_pass, roll_stats, ewm], axis=1)\n\n\ndef divide_and_add_features(s, signal_size=500000):\n    '''\n    Divide the signal in bags of \"signal_size\".\n    Normalize the data dividing it by 15.0\n    '''\n    # normalize\n    s = s\/15.0\n    \n    ls = []\n    for i in tqdm(range(int(s.shape[0]\/signal_size))):\n        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n        sig_featured = add_features(sig)\n        ls.append(sig_featured)\n    \n    return pd.concat(ls, axis=0)","6fa3839e":"# apply every feature to data\ndf = divide_and_add_features(data['signal'])\ndf.head()","3ae1b928":"# The low pass lfilter captures the trend of the signal for different cutoff frequencies\ndf[['signal',\n    'lowpass_lf_0.0100',\n    'lowpass_lf_0.0154',\n    'lowpass_lf_0.0239',\n    'lowpass_lf_0.0369',\n    'lowpass_lf_0.5012']].iloc[:200].plot()","9de76ae4":"# The low pass filtfilt captures the trend of the signal for different cutoff frequencies\n# but without delay\ndf[['signal',\n    'lowpass_ff_0.0100',\n    'lowpass_ff_0.0154',\n    'lowpass_ff_0.0239',\n    'lowpass_ff_0.0369',\n    'lowpass_ff_0.5012']].iloc[:200].plot()","75a93e37":"# The high pass lfilter captures fast variation of the signal for different cutoff frequencies\ndf[['signal',\n    'highpass_lf_0.0100',\n    'highpass_lf_0.0163',\n    'highpass_lf_0.0264',\n    'highpass_lf_0.3005',\n    'highpass_lf_0.7943']].iloc[:100].plot()","548c087d":"# The high pass lfilter captures fast variation of the signal for different cutoff frequencies\n# but without delay\ndf[['signal',\n    'highpass_ff_0.0100',\n    'highpass_ff_0.0163',\n    'highpass_ff_0.0264',\n    'highpass_ff_0.3005',\n    'highpass_ff_0.7943']].iloc[:200].plot()","8307df30":"# rolling mean, quantiles and ewm also capture the trend\ndf[['signal',\n    'roll_mean_10',\n    'roll_mean_50',\n    'roll_mean_100',\n    'roll_q50_100',\n    'ewm_mean_10',\n    'ewm_mean_50',\n    'ewm_mean_100']].iloc[:100].plot()","c32a668d":"# quantiles, min, max\ndf[['signal',\n    'roll_min_100',\n    'roll_q10_100',\n    'roll_q25_100',\n    'roll_q50_100',\n    'roll_q75_100',\n    'roll_q90_100',\n    'roll_max_100']].iloc[:1000].plot()","593b1a89":"# rolling std, and emw std\ndf[['signal',\n    'roll_std_10',\n    'roll_std_50',\n    'ewm_std_10',\n    'ewm_std_50']].iloc[:100].plot()","a341f032":"# Get train and test data\nx_train, x_test, y_train, y_test = train_test_split(df.values, data['open_channels'].values, test_size=0.2)\n\ndel data, df\nprint('x_train.shape=', x_train.shape)\nprint('x_test.shape=', x_test.shape)\nprint('y_train.shape=', y_train.shape)\nprint('y_test.shape=', y_test.shape)","ef8a4120":"def get_class_weight(classes, exp=1):\n    '''\n    Weight of the class is inversely proportional to the population of the class.\n    There is an exponent for adding more weight.\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()\/np.power(hist, exp)\n    \n    return class_weight\n\nclass_weight = get_class_weight(y_train)\nprint('class_weight=', class_weight)\nplt.figure()\nplt.title('classes')\nplt.hist(y_train, bins=np.arange(12)-0.5)\nplt.figure()\nplt.title('class_weight')\nplt.bar(np.arange(11), class_weight)\nplt.title('class_weight')","2fcc49ca":"def create_mpl(shape):\n    '''\n    Returns a keras model\n    '''\n    \n    X_input = L.Input(shape)\n    \n    X = L.Dense(150, activation='relu')(X_input)\n    X = L.Dense(150, activation='relu')(X)\n    X = L.Dense(125, activation='relu')(X)\n    X = L.Dense(100, activation='relu')(X)\n    X = L.Dense(75, activation='relu')(X)\n    X = L.Dense(50, activation='relu')(X)\n    X = L.Dense(25, activation='relu')(X)\n    X = L.Dense(11, activation='softmax')(X)\n    \n    model = Model(inputs=X_input, outputs=X)\n    \n    return model\n\n\nmlp = create_mpl(x_train[0].shape)\nmlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\nprint(mlp.summary())","760542d5":"# fit the model\nmlp.fit(x=x_train, y=y_train, epochs=30, batch_size=1024, class_weight=class_weight)","ebe3e96e":"# plot history\nplt.figure(1)\nplt.plot(mlp.history.history['loss'], 'b', label='loss')\nplt.xlabel('epochs')\nplt.legend()\nplt.figure(2)\nplt.plot(mlp.history.history['sparse_categorical_accuracy'], 'g', label='sparse_categorical_accuracy')\nplt.xlabel('epochs')\nplt.legend()","b2ae8815":"# predict on test\nmlp_pred = mlp.predict(x_test)\nprint('mlp_pred.shape=', mlp_pred.shape)","e739d957":"!nvidia-smi","a7dfdc4c":"# build model\ndataset = lgb.Dataset(x_train, label=y_train)\nparams = {'boosting_type': 'gbdt',\n          'metric': 'rmse',\n          'objective': 'regression',\n          'n_jobs': -1,\n#           'n_jobs': 4,\n          'seed': 236,\n          'num_leaves': 280,\n          'learning_rate': 0.026623466966581126,\n          'max_depth': 73,\n          'lambda_l1': 2.959759088169741,\n          'lambda_l2': 1.331172832164913,\n          'bagging_fraction': 0.9655406551472153,\n          'bagging_freq': 9,\n          'colsample_bytree': 0.6867118652742716,\n#           'device': 'gpu',\n#           'gpu_platform_id': 0,\n#           'gpu_device_id': 0\n}","15fe2235":"import gc\ngc.collect()","d59519d2":"# fit the model\nprint('Training LGBM...')\ngbc = lgb.train(params, dataset,\n                num_boost_round=10000,\n                verbose_eval=100)\nprint('LGBM trained!')","896dd506":"# predict on test\ngbc_pred = gbc.predict(x_test)\nprint('gbc_pred.shape=', gbc_pred.shape)","c5d5fc53":"def convert_to_one_hot(pred):\n    '''\n    Convert the prediction into probabilities\n    Example: 1.6 --> [0, 0.4, 0.6, 0, 0, ...]\n    1.6 is closer to 2 than to 1\n    All rows will sum 1\n    '''\n\n    # clip results lower or higher than limits\n    pred = np.clip(pred, 0, 10)\n\n    # convert to \"one-hot\"\n    pred = 1 - np.abs(pred.reshape((-1,1)) - np.arange(11))\n\n    # clip results lower than 0\n    pred = np.clip(pred, 0, 1)\n    \n    return pred\n    \ngbc_pred = convert_to_one_hot(gbc_pred)\n\nprint('gbc_pred.shape=', gbc_pred.shape)","d64103f2":"# lists for keep results\nf1s = []\nalphas = np.linspace(0,1,101)\n\n# loop for every alpha\nfor alpha in tqdm(alphas):\n    y_pred = alpha*mlp_pred + (1 - alpha)*gbc_pred\n    f1 = f1_score(y_test, np.argmax(y_pred, axis=-1), average='macro')\n    f1s.append(f1)\n\n# convert to numpy array\nf1s = np.array(f1s)\n\n# get best_alpha\nbest_alpha = alphas[np.argmax(f1s)]\n\nprint('best_f1=', f1s.max())\nprint('best_alpha=', best_alpha)","a6a1a88c":"plt.plot(alphas, f1s)\nplt.title('f1_score for ensemble')\nplt.xlabel('alpha')\nplt.ylabel('f1_score')","8fca8f58":"# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(16,16)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='viridis', annot=annot, fmt='', ax=ax)","0b8725e3":"f1_mlp = f1_score(y_test, np.argmax(mlp_pred, axis=-1), average='macro')\nplot_cm(y_test, np.argmax(mlp_pred, axis=-1), 'Only MLP \\n f1=' + str('%.4f' %f1_mlp))","63bc2bdd":"f1_gbc = f1_score(y_test, np.argmax(gbc_pred, axis=-1), average='macro')\nplot_cm(y_test, np.argmax(gbc_pred, axis=-1), 'Only GBC \\n f1=' + str('%.4f' %f1_gbc))","a95ae35a":"y_pred = best_alpha*mlp_pred + (1 - best_alpha)*gbc_pred\nf1_ens = f1_score(y_test, np.argmax(y_pred, axis=-1), average='macro')\nplot_cm(y_test, np.argmax(y_pred, axis=-1), 'Ensemble \\n f1=' + str('%.4f' %f1_ens))","36bffe75":"def submit_result(mlp, gbc, alpha):\n    \n    print('Reading data...')\n#     data = pd.read_csv('..\/input\/data-without-drift\/test_clean.csv')\n#     data = pd.read_csv('..\/input\/clean-kalman\/test_clean_kalman.csv')\n#     data = pd.read_csv('..\/input\/data-without-drift-with-kalman-filter\/test.csv')\n#     data = pd.read_csv('..\/input\/clean-2apply-kalman\/test-clean-2apply-kalman.csv')\n#     data = pd.read_csv('..\/input\/clean-3apply-kalman\/test_clean_3apply_kalman.csv')\n#     data = pd.read_csv('..\/input\/clean-4apply-kalman\/test_clean_4apply_kalman.csv')\n    data = pd.read_csv('..\/input\/clean-5apply-kalman\/test_clean_5apply_kalman.csv')\n    \n    print('Feature engineering...')\n    df = divide_and_add_features(data['signal'])\n    \n    print('Predicting MLP...')\n    mlp_pred = mlp.predict(df.values)\n    \n    print('Predicting GBC...')\n    gbc_pred = gbc.predict(df.values)\n    gbc_pred = convert_to_one_hot(gbc_pred)\n    \n    print('Ensembling...')\n    y_pred = alpha*mlp_pred + (1 - alpha)*gbc_pred\n    y_pred = np.argmax(y_pred, axis=-1)\n    \n    print('Writing submission...')\n    submission = pd.DataFrame()\n    submission['time'] = data['time']\n    submission['open_channels'] = y_pred\n    submission.to_csv('submission.csv', index=False, float_format='%.4f')\n    \n    print('Submission finished!')","caa9968b":"submit_result(mlp, gbc, best_alpha)","6ed7d3ed":"# Load data\n## data-without-drift\nThanks to https:\/\/www.kaggle.com\/cdeotte\/data-without-drift.  \n## clean-kalman\nThanks to https:\/\/www.kaggle.com\/ragnar123\/clean-kalman.\n## data-without-drift-with-kalman-filter\nThanks to https:\/\/www.kaggle.com\/michaln\/data-without-drift-with-kalman-filter.  \nThanks to https:\/\/www.kaggle.com\/michaln\/kalman-filter-on-clean-data.\n## clean-2apply-kalman\nhttps:\/\/www.kaggle.com\/shinogi\/kalman-filter-2-apply-on-clean-data","5fc4bd48":"# Build a MLP model","a34b88cf":"# Confusion matrix","88683d44":"### gradients","daa00b68":"### low_pass","7af476da":"# Feature engineering\nAdd to signal several other signals: gradients, rolling mean, std, low\/high pass filters...\n\nFE is the same as this notebook https:\/\/www.kaggle.com\/martxelo\/fe-and-simple-mlp with corrections in filters.","47abfa3b":"# Build LGBM model\nParameters from https:\/\/www.kaggle.com\/ragnar123\/single-model-lgbm. Thanks!","b5af84d9":"## plot\nLet's plot the signals to see how they look like.","21fb0bc5":"# Classes weights","7dec6b47":"# Submit result","62720b42":"## exec feature enginnering","3a7943b7":"# reference\nhttps:\/\/www.kaggle.com\/martxelo\/fe-and-ensemble-mlp-and-lgbm  \nhttps:\/\/www.kaggle.com\/teejmahal20\/single-model-lgbm-kalman-filter","57821ccb":"# Ensemble\nThe idea is to mix both results with a parameter alpha ($0\\le\\alpha\\le1$):\n\n$y_{pred}=\\alpha\u00b7mlp_{pred} + (1-\\alpha)\u00b7gbc_{pred}$","1a48a9a9":"### high pass","efe3584c":"### Rolling","fc413f71":"# Divide in train and test","198c721d":"### exponential weighted functions","9111e9c4":"## definite feature enginnering function"}}