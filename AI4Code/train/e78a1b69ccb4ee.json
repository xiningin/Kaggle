{"cell_type":{"d60a20c3":"code","410a0409":"code","ebedd909":"code","4663fcc8":"code","0899a0d5":"code","71ce1b28":"code","ebe24a04":"code","4e756954":"code","06575f4f":"code","6d9fa128":"code","75ba54bd":"code","cbf9c2fa":"code","fac59ae9":"code","b63e6334":"code","aedbc576":"code","a5ff6919":"code","42f44472":"code","14a95ca7":"code","8031fdf5":"code","61070f54":"code","cf720f84":"code","4472b8ea":"code","99563414":"code","9b212b17":"code","9ed8c82e":"code","8395d9a8":"code","605b5f56":"code","82341893":"code","160f0b81":"code","9bfda40c":"code","7c992ff2":"code","6b9237a7":"code","316129bd":"code","2c68be36":"code","dc6d842a":"code","166a5e90":"code","0ce9a600":"code","277da17c":"code","072eec64":"code","e2be9741":"code","61a75d9f":"code","b0e41ac0":"code","3a551bcb":"code","aeb6ef96":"code","112ae6e1":"code","4ed6a6b0":"code","ee0f8605":"code","c9be1f6e":"code","87e296dc":"code","65e7d2c0":"code","80d93b4c":"code","618e9320":"code","6694f2d3":"code","bd126433":"code","8cbfab74":"code","b94e06ea":"code","21ca8b4f":"code","ada6028e":"code","7b44707b":"code","d0a8ae8d":"code","172c2f21":"code","240f4f46":"code","f6cceec2":"code","ead79da8":"code","31a79d6b":"code","b129da33":"code","64cd749d":"markdown","91e36212":"markdown","8a9d214b":"markdown","befff0c0":"markdown","724dcc83":"markdown","517c3239":"markdown","f59874dc":"markdown","ce2c05a5":"markdown","bbbf4fd1":"markdown","1fb08c0b":"markdown","25939cdb":"markdown","3706a53e":"markdown","58299858":"markdown","1d44f757":"markdown","7af2e95c":"markdown","4ef72523":"markdown","c05d3295":"markdown","85f643ea":"markdown","cfb953cd":"markdown","ceb01dd3":"markdown","bdc4fb42":"markdown","1c75fcc2":"markdown"},"source":{"d60a20c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","410a0409":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport re\nimport codecs\nimport csv\nimport sys\n","ebedd909":"df=pd.read_csv(\"..\/input\/btchistoricaldata\/BTC-Time-Series.csv\",parse_dates=['Date'],index_col=['Date'])\ndf.head()\n# Parse Dates indicates that it is a datetime column","4663fcc8":"df.info()","0899a0d5":"# Coin Desk also has Data available for some Cryptocurrencies via an API\nbtc_price=pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"})\nbtc_price.head()","71ce1b28":"# Getting the Trend Curve for the Prices\nbtc_price.plot(figsize=(9,9))\nplt.ylabel(\"BTC Price\")","ebe24a04":"import csv\nfrom datetime import datetime\ntimesteps=[]\nbtc_prices=[]\nwith open(\"..\/input\/btchistoricaldata\/BTC-Time-Series.csv\",\"r\") as f:\n    csv_reader=csv.reader(f,delimiter=\",\")\n    next(csv_reader)\n    # Iterator and Lines \n    for line in csv_reader:\n        # Get the Dates\n        timesteps.append(datetime.strptime(line[1],'%Y-%m-%d'))\n        # Get the Closing Prices\n        btc_prices.append(float(line[2]))\ntimesteps[:10],btc_prices[:10]","4e756954":"plt.figure(figsize=(9,9))\nplt.plot(timesteps,btc_prices)\nplt.ylabel(\"BTC Price\")\nplt.xlabel(\"Date\")\n","06575f4f":"# Creating Train Test Split for Time Series Data \ntimesteps=btc_price.index.to_numpy()\nprices=btc_price[\"Price\"].to_numpy()","6d9fa128":"# 80% Training Data and 20% Testing Data\nsplit_size=int(0.8*len(prices))\n\n# Create an Internal Train Data Split \nX_train,y_train=timesteps[:split_size],prices[:split_size]\n# Creating test data Splits (beyond the Split)\nX_test,y_test=timesteps[split_size:],prices[split_size:]\nlen(X_train),len(y_train),len(X_test),len(y_test)\n","75ba54bd":"# Plotting the Training and Testing Data\nplt.figure(figsize=(9,9))\nplt.scatter(X_train,y_train,label=\"Train Data\",s=3)\nplt.scatter(X_test,y_test,label=\"Test Data\",s=3)\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\")\nplt.legend(fontsize=12)\nplt.show()","cbf9c2fa":"# Modularity for creating a Plotting function\ndef plot_series(timesteps,values,format=\".\",start=0,end=None,label=None):\n#     plt.figure(figsize=(9,9))\n    # Plot the Series\n    plt.plot(timesteps[start:end],values[start:end],format,label=label)\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"BTC Price\")\n    if label:\n        plt.legend(fontsize=12)\n    plt.grid(True)\n  \n    ","fac59ae9":"# Plotting the Train\nplot_series(timesteps=X_train,values=y_train,label=\"Train Data\")","b63e6334":"# Making a Naive Forecasting Model\n# Offsetting the y_test by 1\nnaive_forecast=y_test[:-1]\nnaive_forecast[:10],naive_forecast[-10:]","aedbc576":"plt.figure(figsize=(12,12))\nplot_series(timesteps=X_train,values=y_train,label=\"Train Data\")\nplot_series(timesteps=X_test,values=y_test,label=\"Test Data\")\nplot_series(timesteps=X_test[1:],values=naive_forecast,label=\"Naive Model Predictions\")\n\n#  They can be offseted and Formatted in different terms","a5ff6919":"# Implementing MASE mean absolute scaled error as  there is not built in functionality in TF\n# No seasonality Version\nimport tensorflow as tf\ndef MASE(y_true,y_pred):\n    mae=tf.reduce_mean(tf.abs(y_true-y_pred))\n    mae_naive_noseason=tf.reduce_mean(tf.abs(y_true[1:]-y_true[:-1]))\n    # The seasonality is one Day in this case of no seasonality\n    return mae\/mae_naive_noseason","42f44472":"MASE(y_true=y_test[1:],y_pred=naive_forecast).numpy()\n","14a95ca7":"# Creating a Super-set Function for evaluating the Prediction on all the given Metrics\ndef evaluate_prediction(y_true,y_pred):\n        # For Evaluation Metrics we need a Float 32 .\n        y_true=tf.cast(y_true,dtype=tf.float32)\n        y_pred=tf.cast(y_pred,dtype=tf.float32)\n        mae=tf.keras.metrics.mean_absolute_error(y_true,y_pred)\n        mse=tf.keras.metrics.mean_squared_error(y_true,y_pred)\n        # Root Mean Squared\n        rmse=tf.sqrt(mse)\n        smape=tf.keras.metrics.mean_absolute_percentage_error(y_true,y_pred)\n        mase=MASE(y_true,y_pred)\n        # Return them into a dictionary Type\n        return {\"mae\":mae.numpy(),\n               \"mse\":mse.numpy(),\n               \"rmse\":rmse.numpy(),\n               \"smape\":smape.numpy(),\n               \"mase\":mase.numpy()}\n","8031fdf5":"# Naive Results\nnaive_model_results=evaluate_prediction(y_true=y_test[1:],y_pred=naive_forecast)\nnaive_model_results","61070f54":"HORIZON=1\nWINDOW=7\n","cf720f84":"def get_window_label(x,horizon=HORIZON):\n    return x[:,:-horizon], x[:,-horizon]\n    ","4472b8ea":"test_window, test_label=get_window_label(tf.expand_dims(tf.range(8)+1,axis=0))\ntest_window, test_label","99563414":"# IMP \n# Sliding window Approach\n# Create function to view NumPy arrays as windows \ndef make_windows(x, window_size=7, horizon=1):\n\n  # Turns a 1D array into a 2D array of sequential windows of window_size.\n \n  # Create a window of specific window_size (add the horizon on the end for later labelling)\n  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n\n  # Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n  # create 2D array of windows of size window_size\n  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T \n  # Index on the target array (time series) with 2D array of multiple window steps\n  windowed_array = x[window_indexes]\n\n  # Get the labelled windows\n  windows, labels = get_window_label(windowed_array, horizon=horizon)\n\n  return windows, labels","9b212b17":"full_windows, full_labels = make_windows(prices, window_size=WINDOW, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n","9ed8c82e":"# Although there is an inbuilt function\n# tf.keras.preprocessing.timeseries_dataset_from_array\n","8395d9a8":"def train_test_splits(windows,labels,test_split=0.2):\n    #Here we will not randomize this Process and Ensure the same Window-Label pairs are together in the same set at the same place\n    split_size=int(len(windows)*(1-test_split)) # 80 % 1-t_s\n    train_windows=windows[:split_size]\n    train_labels=labels[:split_size]\n    test_windows=windows[split_size:]\n    test_labels=labels[split_size:]\n    return train_windows, test_windows, train_labels, test_labels\n    ","605b5f56":"train_windows,test_windows,train_labels, test_labels=train_test_splits(full_windows,full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)","82341893":"# Check to see if same (accounting for horizon and window size)\nnp.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW:])","160f0b81":"import os\nos.mkdir(\"\/kaggle\/working\/model_checkpoints1\")","9bfda40c":"def create_model_checkpoint(model_name, save_path=\".\/model_checkpoints\"):\n  # create filepath to save model\n  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n                                            monitor=\"val_loss\",\n                                            verbose=0, \n                                            save_best_only=True) \n# only output a limited amount of text and Save Only the best Model to the File","7c992ff2":"import tensorflow as tf\nfrom tensorflow.keras import layers\ntf.random.set_seed(42)\n# Creating the Model\nmodel_1=tf.keras.Sequential([layers.Dense(128,activation=\"relu\"),\n                           layers.Dense(HORIZON,activation=\"linear\")],name=\"model_1_Dense\")\n# Compiling the Model\nmodel_1.compile(loss=\"mae\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"mae\",\"mse\"])\n\n# Fitting the Model\nmodel_1.fit(x=train_windows,y=train_labels,epochs=100,verbose=1,batch_size=128,validation_data=(test_windows,test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_1.name)])","6b9237a7":"# Evaluating the Model in the Test Data\nmodel_1.evaluate(test_windows,test_labels)","316129bd":"# The best performance is in the Callbacks and we can Load that\nmodel_1=tf.keras.models.load_model(\".\/model_checkpoints\/model_1_Dense\")\nmodel_1.evaluate(test_windows,test_labels)","2c68be36":"def make_forecast(model,input_data):\n    forecast = model.predict(input_data)\n    # return 1D array of predictions\n    return tf.squeeze(forecast) \n    ","dc6d842a":"model_1_preds = make_forecast(model_1, test_windows)\nlen(model_1_preds)","166a5e90":"#  Evaluating the Model\n# Evaluate preds\nmodel_1_results = evaluate_prediction(y_true=tf.squeeze(test_labels), # reduce to right shape\n                                 y_pred=model_1_preds)\nmodel_1_results","0ce9a600":"test_labels.shape,model_1_preds.shape","277da17c":"offset = 300\nplt.figure(figsize=(9, 9))\n# Account for the test_window offset and index into test_labels to ensure correct plotting\n# Fix This\n# plot_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\")\n# plot_series(timesteps=X_test[-len(test_windows):], values=model_1_preds, start=offset, format=\"-\", label=\"model_1_preds\")","072eec64":"HORIZON = 1 # predict one step at a time\nWINDOW = 30 # use 30 timesteps in the past","e2be9741":"\nfull_windows, full_labels = make_windows(prices, window_size=WINDOW, horizon=HORIZON)\nlen(full_windows), len(full_labels)","61a75d9f":"# Make train and testing windows\ntrain_windows, test_windows, train_labels, test_labels = train_test_splits(windows=full_windows, labels=full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)","b0e41ac0":"import tensorflow as tf\nfrom tensorflow.keras import layers","3a551bcb":"model_2=tf.keras.Sequential([layers.Dense(128,activation=\"relu\"),layers.Dense(1,activation=\"linear\")],name=\"model_2_Dense\")\nmodel_2.compile(loss=\"mae\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"mae\",\"mse\"])\nmodel_2.fit(x=train_windows,y=train_labels,epochs=100, verbose=0,batch_size=128,\n            validation_data=(test_windows,test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_2.name)])","aeb6ef96":"model_2.evaluate(test_windows, test_labels)","112ae6e1":"# Load in best performing model\nmodel_2 = tf.keras.models.load_model(\".\/model_checkpoints\/model_2_Dense\")\nmodel_2.evaluate(test_windows, test_labels)","4ed6a6b0":"# Get forecast predictions\nmodel_2_preds = make_forecast(model_2,input_data=test_windows)\n# model_2_preds","ee0f8605":"# Evaluate results for model 2 predictions\nmodel_2_results = evaluate_prediction(y_true=tf.squeeze(test_labels), # remove 1 dimension of test labels\n                                 y_pred=model_2_preds)\nmodel_2_results","c9be1f6e":"HORIZON = 7\nWINDOW=30","87e296dc":"full_windows, full_labels = make_windows(prices, window_size=WINDOW, horizon=HORIZON)\nlen(full_windows), len(full_labels)","65e7d2c0":"# Make train and testing windows\ntrain_windows, test_windows, train_labels, test_labels = train_test_splits(windows=full_windows, labels=full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)","80d93b4c":"tf.random.set_seed(42)\nmodel_3=tf.keras.Sequential([layers.Dense(128,activation=\"relu\"),layers.Dense(HORIZON)],name=\"model_3_Dense\")\nmodel_3.compile(loss=\"mae\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"mae\",\"mse\"])\nmodel_3.fit(x=train_windows,y=train_labels,epochs=100, verbose=0,batch_size=128,\n            validation_data=(test_windows,test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_3.name)])","618e9320":"# Loading the best version of the Model\nmodel_3 = tf.keras.models.load_model(\".\/model_checkpoints\/model_3_Dense\")\nmodel_3.evaluate(test_windows, test_labels)","6694f2d3":"model_3_preds = make_forecast(model_3,\n                           input_data=test_windows)\nmodel_3_preds[:5]","bd126433":"test_labels.shape,model_3_preds.shape","8cbfab74":"# You'll notice the outputs for model_3_results are multi-dimensional.\n\n# This is because the predictions are getting evaluated across the HORIZON timesteps (7 predictions at a time).\n\n# To fix this, let's adjust our evaluate_preds() function to work with multiple shapes of data.\n\ndef evaluate_preds(y_true, y_pred):\n  # Make sure float32 (for metric calculations)\n  y_true = tf.cast(y_true, dtype=tf.float32)\n  y_pred = tf.cast(y_pred, dtype=tf.float32)\n\n  # Calculate various metrics\n  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n  rmse = tf.sqrt(mse)\n  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n  mase = MASE(y_true, y_pred)\n\n  # Account for different sized metrics (for longer horizons, reduce to single number)\n  if mae.ndim > 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean\n    mae = tf.reduce_mean(mae)\n    mse = tf.reduce_mean(mse)\n    rmse = tf.reduce_mean(rmse)\n    mape = tf.reduce_mean(mape)\n    mase = tf.reduce_mean(mase)\n\n  return {\"mae\": mae.numpy(),\n          \"mse\": mse.numpy(),\n          \"rmse\": rmse.numpy(),\n          \"mape\": mape.numpy(),\n          \"mase\": mase.numpy()}","b94e06ea":"import tensorflow as tf\nfrom tensorflow.keras import layers\n","21ca8b4f":"HORIZON=1\nWINDOW=7","ada6028e":"# We have to reinstatiate it every time\nfull_windows, full_labels = make_windows(prices, window_size=WINDOW, horizon=HORIZON)\nlen(full_windows), len(full_labels)","7b44707b":"# Make train and testing windows\ntrain_windows, test_windows, train_labels, test_labels = train_test_splits(windows=full_windows, labels=full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)","d0a8ae8d":"# Check data sample shapes\ntrain_windows[0].shape # returns (WINDOW_SIZE, )\n# So now we need to expand our dim","172c2f21":"# SO we need to reshape the data to pass it through the Conv1D Layer\nx = tf.constant(train_windows[0])\nx","240f4f46":"#  Adding an extra Dimension to x\n#  So we are turning a Lambda Function to a Keras model passable Layer \nexpand_dims_layer=layers.Lambda(lambda x: tf.expand_dims(x,axis=1))\n","f6cceec2":"#  Building the Model\ntf.random.set_seed(42)\n\n# Create model\nmodel_4 = tf.keras.Sequential([\n  # Create Lambda layer to reshape inputs, without this layer, the model will error\n  layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size \/ Conv1D 3D input requirements\n  layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),\n  layers.Dense(HORIZON)\n], name=\"model_4_conv1D\")\n\n# Compile model\nmodel_4.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\n# Fit model\nmodel_4.fit(train_windows,\n            train_labels,\n            batch_size=128, \n            epochs=100,\n            verbose=0,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_4.name)])","ead79da8":"model_4 = tf.keras.models.load_model(\".\/model_checkpoints\/model_4_conv1D\")\nmodel_4.evaluate(test_windows, test_labels)\n","31a79d6b":"model_4_preds = make_forecast(model_4, test_windows)\nmodel_4_preds","b129da33":"# Evaluate results for model 4 predictions\nmodel_4_results = evaluate_prediction(y_true=tf.squeeze(test_labels), # remove 1 dimension of test labels\n                                 y_pred=model_4_preds)\nmodel_4_results","64cd749d":"<h3> Dense Model but with different size of WINDOW <\/h3>","91e36212":"<h2> Dense Model with Changes in the Window = 30 and HORIZON =1 ","8a9d214b":"The Conv1D layer in TensorFlow takes an input of: (batch_size, timesteps, input_dim):<br>\ntimesteps = WINDOW\ninput_dim = HORIZON \n","befff0c0":"<h2> Conv 1D Model","724dcc83":"<h4> The various types of Time Series \n<ul>\n  <li>Trend - Can be Increasing or Decreasing, Linear, Curved etc.<\/li>\n  <li>Seasonal Patterns - Differs on Datetime and changes by Months, days etc.<\/li>\n  <li>Cyclic - Increases and Decreases but not periodically or on fixed times like Seasonals<\/li>\n<\/ul>\nAlso there are univariate and Multi-variate Time Series Data.\n<ul>\n    <li> Univariate - Using a parameter to Predict the same parameter<\/li>\n    <li> Multi-variate- Using Multiple Time dependent params or dependecies to Predict one parameter <\/li>\n<\/ul>\n<\/h4>","517c3239":"<h4>\nHere we have leveraged Numpy's Array Indexing Methods <br>\nCreate a 2D Array of Windows one after the other as 1-D Arrays in the space.<br>\nThen use the 2D Array of Multiple windows to index on a Target Series.","f59874dc":"<h2> Creating a Modelling Checkpoint  Callback <h2> <h4>So we want the Models best performance at whichever epoch it has and then save it. <br>\n    This facilitates us to compare the best performing versions of each of the Model that we write <br>\n    So we will save the best Only\n    ","ce2c05a5":"<h3>Metrics for Evaluating the Time Series Forecasting <\/h3> \n<ul>\n MAE <br>\n MSE <br>\n Huber Loss <br>\n RMS <br>\n sMAPE <br>\n MASE <br>\n <\/ul>","bbbf4fd1":"<h3> Function for making the Forecasts","1fb08c0b":"<h3>Windowing our Data <br>\nTo make the Problem a Supervised Learning Problem <br>\nSo for each Set of 7 values we Predict for the 8th Value<br>\n<\/h3>","25939cdb":"<h3>Window is the amount of data that is given to the Model.<br>\nThe future for which we want to predict the Value is called the horizon. <br>\nAlso there can be various sequence to sequence Problems with different mappings like\nOne to One, Many to One, One to Many, Many to Many etc.","3706a53e":"<h3> Now we will Split these windows into Training set and Test Set <\/h3>","58299858":"<h2> Model with Window Size of 30 and Horizon size =7 <\/h2>","1d44f757":"<h4>The Time Data cannot be split Randomly. It has to be splitted in the Order of time.<br>\n    Otherwise it will cause a Data leak issue<br>\n    So we have to create a pseudo Future Data as a turnaround for this","7af2e95c":"<h2> There will be Multiple Models trained in this Notebook <h2>\n   <h4> <ul>\n       <li>Naive Bayes Model<\/li>\n       <li>Dense Model <\/li>\n       <li> Dense Model with some Tweaks and Hyperparameters Tuning and changing Horizons and Windows<\/li>\n       <li>Conv 1D<\/li>\n       <li> LSTM - Long short term Memory<\/li>\n       <li>Dense Model by Introducing Multivariate Data<\/li>\n       <li>N-BEATS Algorithm<\/li>\n       <li> Ensembling or Stacking the aforementioned Models <\/li>\n       <li> Future Prediction Model<\/li>\n       <\/ul>\n    <\/h4>\n Note:\n    <h3>* <b>Horizon<\/b> --> The Number of Values that are going to be predicted in the Future<br>\n        * <b>Window Size <\/b> --> The input size that is given to the Model to Predict the future.","4ef72523":"<h2> Dense Model <h2>","c05d3295":"<h3> Time Series has an added concept of time which makes the data dynamic.<br> \nMajorly this notebook is focused on Forecasting Data. <br>\nThe idea is to build a multi-variate Model and to forecast results with Prediction Intervals.\n","85f643ea":"<h2>This will be by far one of the most comprehensive Notebooks on Time Series Analysis \nwith Tensorflow <br> <\/h2>\nThis is the course that I have followed \n<a href=\"https:\/\/www.udemy.com\/course\/tensorflow-developer-certificate-machine-learning-zero-to-mastery\"> TF Course <\/a>\n","cfb953cd":"<h4> So basically what we are doing is that we are Predicting the Previous Steps' value as the value of the Next Step in the Naive's Model","ceb01dd3":"<h3> Naive Model <h3>","bdc4fb42":"<h3> EDA","1c75fcc2":"<h3>\n    An alternative way to read the Data with Python's CSV Function\n   "}}