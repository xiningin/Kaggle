{"cell_type":{"68661e3e":"code","26e2dd2e":"code","6011bafa":"code","75c442bc":"code","cc8857e0":"code","da1a6f11":"code","d25da406":"code","d5e32107":"code","2bc86a78":"code","26c22598":"code","78b90586":"code","2fbca308":"code","94ec59e7":"code","2265dd21":"code","7905c605":"code","bc2da204":"code","fca30da5":"code","aaf9adc6":"code","f8ff493e":"code","cba3e093":"code","ebb49c64":"code","e5f011b5":"code","f61184ad":"markdown","d5f6e735":"markdown","92e278cb":"markdown","c63db5d4":"markdown","1febbd62":"markdown","2d57e202":"markdown","a4f90950":"markdown","f763169a":"markdown","71ce1976":"markdown","cefa6699":"markdown","b14101ad":"markdown","6b98adec":"markdown","751bfa04":"markdown","aebbbcd1":"markdown"},"source":{"68661e3e":"from __future__ import print_function\n\nimport time\nfrom PIL import Image\nimport numpy as np\n\nfrom keras import backend\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\n\nfrom scipy.optimize import fmin_l_bfgs_b\n#from scipy.misc import imsave","26e2dd2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6011bafa":"height = 512\nwidth = 512\n\ncontent_image_path = '..\/input\/alexandra\/a1.jpg'\ncontent_image = Image.open(content_image_path)\n#content_image = content_image.rotate(90)\ncontent_image = content_image.resize((width, height))\ncontent_image","75c442bc":"style_image_path = '..\/input\/modern-art\/m23.jpg'\nstyle_image = Image.open(style_image_path)\nstyle_image = style_image.resize((width, height))\nstyle_image","cc8857e0":"content_array = np.asarray(content_image, dtype='float32')\ncontent_array = np.expand_dims(content_array, axis=0)\nprint(content_array.shape)\n\nstyle_array = np.asarray(style_image, dtype='float32')\nstyle_array = np.expand_dims(style_array, axis=0)","da1a6f11":"content_array[:, :, :, 0] -= 103.939\ncontent_array[:, :, :, 1] -= 116.779\ncontent_array[:, :, :, 2] -= 123.68\ncontent_array = content_array[:, :, :, ::-1]\n\nstyle_array[:, :, :, 0] -= 103.939\nstyle_array[:, :, :, 1] -= 116.779\nstyle_array[:, :, :, 2] -= 123.68\nstyle_array = style_array[:, :, :, ::-1]\n","d25da406":"content_image = backend.variable(content_array)\nstyle_image = backend.variable(style_array)\ncombination_image = backend.placeholder((1, height, width, 3))","d5e32107":"input_tensor = backend.concatenate([content_image,\n                                    style_image,\n                                    combination_image], axis=0)","2bc86a78":"model = VGG16(input_tensor=input_tensor, weights='imagenet',\n              include_top=False)","26c22598":"layers = dict([(layer.name, layer.output) for layer in model.layers])","78b90586":"content_weight = 0.180\nstyle_weight = 1.0\ntotal_variation_weight = 1.0","2fbca308":"loss = backend.variable(0.)\n","94ec59e7":"def content_loss(content, combination):\n    return backend.sum(backend.square(combination - content))\n\nlayer_features = layers['block2_conv1']\ncontent_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\n\nloss =loss+ content_weight * content_loss(content_image_features,\n                                      combination_features)","2265dd21":"def gram_matrix(x):\n    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n    gram = backend.dot(features, backend.transpose(features))\n    return gram","7905c605":"def style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = height * width\n    return backend.sum(backend.square(S - C)) \/ (4. * (channels ** 2) * (size ** 2))\n\nfeature_layers = ['block1_conv2', 'block2_conv2',\n                  'block3_conv3', 'block4_conv3',\n                  'block5_conv3']\nfor layer_name in feature_layers:\n    layer_features = layers[layer_name]\n    style_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    sl = style_loss(style_features, combination_features)\n    loss += (style_weight \/ len(feature_layers)) * sl","bc2da204":"def total_variation_loss(x):\n    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n    return backend.sum(backend.pow(a + b, 1.25))\n\nloss += total_variation_weight * total_variation_loss(combination_image)","fca30da5":"import tensorflow as tf","aaf9adc6":"tf.compat.v1.disable_eager_execution()\n","f8ff493e":"grads = backend.gradients(loss, combination_image)\n","cba3e093":"outputs = [loss]\noutputs += grads\nf_outputs = backend.function([combination_image], outputs)\n\ndef eval_loss_and_grads(x):\n    x = x.reshape((1, height, width, 3))\n    outs = f_outputs([x])\n    loss_value = outs[0]\n    grad_values = outs[1].flatten().astype('float64')\n    return loss_value, grad_values\n\nclass Evaluator(object):\n\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n\n    def loss(self, x):\n        assert self.loss_value is None\n        loss_value, grad_values = eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n\nevaluator = Evaluator()","ebb49c64":"x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.\n\niterations = 10\n\nfor i in range(iterations):\n    print('Start of iteration', i)\n    start_time = time.time()\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n                                     fprime=evaluator.grads, maxfun=20)\n    print('Current loss value:', min_val)\n    end_time = time.time()\n    print('Iteration %d completed in %ds' % (i, end_time - start_time))","e5f011b5":"x = x.reshape((height, width, 3))\nx = x[:, :, ::-1]\nx[:, :, 0] += 103.939\nx[:, :, 1] += 116.779\nx[:, :, 2] += 123.68\nx = np.clip(x, 0, 255).astype('uint8')\n\nImage.fromarray(x)","f61184ad":"# Gram Matrix- If matrix has Real numbers and nos of Coloumns X then Gram Matrix will be X(Transpose)X. ","d5f6e735":"# Content Loss- I tried both L1 and L2 what I found if you are using L1 you wont find much difference so its better if you use L2 Loss.","92e278cb":"# COnverting everything to tensor.","c63db5d4":"# Here We can see  Alexandra's AVATAR","1febbd62":"# Weights These are important Parameters. You must have seen so many codes of NST and these weights are common. You can play with these numbers by changing them. TV_weight is a bit different Total variation loss helps in removing rough texture of the generated image.the image with nonzero total variation loss is very smooth.","2d57e202":"# Here I want to discuss about the layers for style and content. initial layers of model have high content information. e.g. block1conv1 will give you same image but block3conv2 will give you content image but the eages will not be clear and block5conv1 will be complete garbage for our task. \n# For style image we use Gram Matrix The terms of this matrix are proportional to the covariances of corresponding sets of features, and thus captures information about which features tend to activate together. By only capturing these aggregate statistics across the image, they are blind to the specific arrangement of objects inside the image. This is what allows them to capture information about style independent of content.","a4f90950":"# My favourite actress <3<3<3","f763169a":"# using VGG-16 model. you can use more deeper model. deeper model will give you better result I used VGG16 because lesser Training time and I dont have good PC(4GB RAM no GPU). but i did some analysis which i will show later","71ce1976":"# Layers of the model","cefa6699":"# TV Loss used to make it smooth. I dont know much about it. I used the method used in deeplearning.ai specialization course. but here instead of using tf.total_variationloss i simply used the formula you can use either of this.  ","b14101ad":"# Converting  RGB to BGR if you want to preserve colors u can change it to hsv color space i didnt try tho.","6b98adec":"# Gradients and Loss calculation ","751bfa04":"# This Dataset is a private dataset ","aebbbcd1":"# To Calculate Style_loss I simply used formula given in  the research paper. "}}