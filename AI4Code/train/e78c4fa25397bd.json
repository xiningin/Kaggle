{"cell_type":{"57c92590":"code","d4c64c85":"code","487dd0d1":"code","8290240b":"code","302c54f3":"code","5876128f":"code","fcfef449":"code","b65a2fb0":"code","cbab285e":"code","f4811450":"code","4062ac69":"code","e3b2dcf7":"code","ef4b15b7":"code","81a64425":"code","4f0a6b92":"code","fcff04b5":"code","4c00c948":"code","01538c28":"code","80534878":"code","88a0edfa":"code","818ddf54":"markdown","7ab56b5c":"markdown"},"source":{"57c92590":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4c64c85":"# modification\n# 1. lr_scheduler\n# 2. data augument\n# 3. more epochs\n# 4. loss\n# 5. data overlap\n# 6. Ensemble\n# 7. The 12th layer\n# 8. scaler\n\n# 'output_hidden_states':True,\"hidden_dropout_prob\": 0.0\n","487dd0d1":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport os\nimport re\n#import cmudictloss = criteon(logits, y)\n\nimport torch\nfrom transformers import BertTokenizer, BertModel, BertConfig,BertForSequenceClassification, AdamW\n# from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","8290240b":"standard = False\neda_ = False\n\nscaler = StandardScaler()\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntrain['excerpt'] = train['excerpt'].apply(lambda x: x.replace('\\n',''))\n\narray = list(range(len(train))) \nnp.random.shuffle(array)\nval = train.iloc[array[:100],]\ntrain = train.iloc[array[100:],]\n\nif standard:\n    target_ = scaler.fit_transform(np.expand_dims(train['target'].to_numpy(),axis=1))\n    train['target'] = np.squeeze(target_)","302c54f3":"# train\ntext = train.loc[0, 'excerpt']\n\n#Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n#Display the Generated image:\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","5876128f":"# test\ntext = test.loc[0, 'excerpt']\n\n#Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n#Display the Generated image:\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","fcfef449":"stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n\t\t\t'into', 'through', 'during', 'before', 'after', \n\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n\t\t\t'should', 'now', '']\n\nfrom nltk.corpus import wordnet \n\nimport re\ndef get_only_chars(line):\n\n    clean_line = \"\"\n\n    line = line.replace(\"\u2019\", \"\")\n    line = line.replace(\"'\", \"\")\n    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n    line = line.replace(\"\\t\", \" \")\n    line = line.replace(\"\\n\", \" \")\n    line = line.lower()\n\n    for char in line:\n        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n            clean_line += char\n        else:\n            clean_line += ' '\n\n    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n    if clean_line[0] == ' ':\n        clean_line = clean_line[1:]\n    return clean_line\n\n\ndef synonym_replacement(words, n):\n\tnew_words = words.copy()\n\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n\trandom.shuffle(random_word_list)\n\tnum_replaced = 0\n\tfor random_word in random_word_list:\n\t\tsynonyms = get_synonyms(random_word)\n\t\tif len(synonyms) >= 1:\n\t\t\tsynonym = random.choice(list(synonyms))\n\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n\t\t\tnum_replaced += 1\n\t\tif num_replaced >= n: #only replace up to n words\n\t\t\tbreak\n\n\t#this is stupid but we need it, trust me\n\tsentence = ' '.join(new_words)\n\tnew_words = sentence.split(' ')\n\n\treturn new_words\n\ndef get_synonyms(word):\n\tsynonyms = set()\n\tfor syn in wordnet.synsets(word): \n\t\tfor l in syn.lemmas(): \n\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n\t\t\tsynonyms.add(synonym) \n\tif word in synonyms:\n\t\tsynonyms.remove(word)\n\treturn list(synonyms)\n\n\n########################################################################\n# Random deletion\n# Randomly delete words from the sentence with probability p\n########################################################################\n\ndef random_deletion(words, p):\n\n\t#obviously, if there's only one word, don't delete it\n\tif len(words) == 1:\n\t\treturn words\n\n\t#randomly delete words with probability p\n\tnew_words = []\n\tfor word in words:\n\t\tr = random.uniform(0, 1)\n\t\tif r > p:\n\t\t\tnew_words.append(word)\n\n\t#if you end up deleting all words, just return a random word\n\tif len(new_words) == 0:\n\t\trand_int = random.randint(0, len(words)-1)\n\t\treturn [words[rand_int]]\n\n\treturn new_words\n\n########################################################################\n# Random swap\n# Randomly swap two words in the sentence n times\n########################################################################\n\ndef random_swap(words, n):\n\tnew_words = words.copy()\n\tfor _ in range(n):\n\t\tnew_words = swap_word(new_words)\n\treturn new_words\n\ndef swap_word(new_words):\n\trandom_idx_1 = random.randint(0, len(new_words)-1)\n\trandom_idx_2 = random_idx_1\n\tcounter = 0\n\twhile random_idx_2 == random_idx_1:\n\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n\t\tcounter += 1\n\t\tif counter > 3:\n\t\t\treturn new_words\n\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n\treturn new_words\n\n########################################################################\n# Random insertion\n# Randomly insert n words into the sentence\n########################################################################\n\ndef random_insertion(words, n):\n\tnew_words = words.copy()\n\tfor _ in range(n):\n\t\tadd_word(new_words)\n\treturn new_words\n\ndef add_word(new_words):\n\tsynonyms = []\n\tcounter = 0\n\twhile len(synonyms) < 1:\n\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n\t\tsynonyms = get_synonyms(random_word)\n\t\tcounter += 1\n\t\tif counter >= 10:\n\t\t\treturn\n\trandom_synonym = synonyms[0]\n\trandom_idx = random.randint(0, len(new_words)-1)\n\tnew_words.insert(random_idx, random_synonym)\n    \n    \ndef eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=1):\n\t\n\tsentence = get_only_chars(sentence)\n\twords = sentence.split(' ')\n\twords = [word for word in words if word is not '']\n\tnum_words = len(words)\n\t\n\taugmented_sentences = []\n\tnum_new_per_technique = int(num_aug\/4)+1\n\n# \t#sr\n\tif (alpha_sr > 0):\n\t\tn_sr = max(1, int(alpha_sr*num_words))\n\t\tfor _ in range(num_new_per_technique):\n\t\t\ta_words = synonym_replacement(words, n_sr)\n\t\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#ri\n\tif (alpha_ri > 0):\n\t\tn_ri = max(1, int(alpha_ri*num_words))\n\t\tfor _ in range(num_new_per_technique):\n\t\t\ta_words = random_insertion(words, n_ri)\n\t\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#rs\n\tif (alpha_rs > 0):\n\t\tn_rs = max(1, int(alpha_rs*num_words))\n\t\tfor _ in range(num_new_per_technique):\n\t\t\ta_words = random_swap(words, n_rs)\n\t\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#rd\n\tif (p_rd > 0):\n\t\tfor _ in range(num_new_per_technique):\n\t\t\ta_words = random_deletion(words, p_rd)\n\t\t\taugmented_sentences.append(' '.join(a_words))\n\n\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n\tshuffle(augmented_sentences)\n\n\t#trim so that we have the desired number of augmented sentences\n\tif num_aug >= 1:\n\t\taugmented_sentences = augmented_sentences[:num_aug]\n\telse:\n\t\tkeep_prob = num_aug \/ len(augmented_sentences)\n\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n\n\t#append the original sentence\n\taugmented_sentences.append(sentence)\n\n\treturn augmented_sentences","b65a2fb0":"num_aug = 1\nimport random\nfrom random import shuffle\nfrom tqdm import tqdm\nrandom.seed(1)\ntrain_aug = train['excerpt'].tolist()\ntarget_aug = train['target'].tolist()\nprint(\"Input shape:\", np.array(train_aug).shape)\n\nif eda_:\n    for i, line in tqdm(enumerate(train.to_numpy())):\n        label = line[4]  # \n        sentence = line[3]\n    \n        aug_sentences = eda(sentence)\n        train_aug.append(aug_sentences)\n        target_aug.append(label)\n\nprint(\"Output shape:\", np.array(train_aug).shape)","cbab285e":"# train = train.drop(train.index[int(np.where(train['target'].to_numpy()==0)[0])]) \n\nplt.scatter(train['target'].to_numpy(), train['standard_error'].to_numpy())   # 0.0 - > std 0.64967129\n\n# remove\n# incorrect :-(","f4811450":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_path = '..\/input\/bert-base-uncased'\ntokenizer= BertTokenizer.from_pretrained(model_path)\nmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=1)\nmodel\n\nmodel.to(device)\noptim = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)      #  params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.1, last_epoch=-1)\n","4062ac69":"class loader(torch.utils.data.Dataset):\n    def __init__(self, train_tokens, labels):\n        self.train_tokens = train_tokens\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        \n        #train_tokens= tokenizer.batch_encode_plus(self.text_list,max_length=512,padding='longest',truncation=True)\n        \n\n        item = {key: torch.tensor(val[idx]) for key, val in self.train_tokens.items()}\n        item['labels'] = torch.tensor(self.labels[idx],dtype=torch.float)\n        return item\n\n    def __len__(self):\n        return len(self.labels)","e3b2dcf7":"len(target_aug)","ef4b15b7":"if eda_:\n    train_tokens= tokenizer.batch_encode_plus(train_aug, max_length=512, padding='longest', truncation=True)\n    train_dataset = loader(train_tokens, target_aug)\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  \nelse:\n    train_tokens= tokenizer.batch_encode_plus(train['excerpt'].tolist(), max_length=512, padding='longest', truncation=True)\n    train_dataset = loader(train_tokens, train.target.values)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # 4-> 32\n","81a64425":"val_tokens= tokenizer.batch_encode_plus(val['excerpt'].tolist(),max_length=512,padding='longest',truncation=True)\nval_dataset = loader(val_tokens, val.target.values)\n\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\ntest_tokens= tokenizer.batch_encode_plus(test['excerpt'].tolist(),max_length=512,padding='longest',truncation=True)\ntest['target']=0\ntest_dataset = loader(test_tokens, test.target.values)\n\ntest_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)","4f0a6b92":"from transformers import AutoModel,AutoConfig, AutoTokenizer,get_cosine_schedule_with_warmup\n\nlr_scheduler = get_cosine_schedule_with_warmup(optim,num_warmup_steps=0,num_training_steps=  int(len(train)\/10))   # 10 *\n   \n","fcff04b5":"total_test_preds = []\nval_rmse = []\nfrom tqdm import tqdm\nfor epoch in range(3):    #3\n    for batch in tqdm(train_loader):\n   \n#         optim.zero_grad()  ->\n        optim.zero_grad(set_to_none=True) \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]  # MseLossBackward\n#         print(\"loss:\", outputs[0], flush=True)\n        # sqrt\n        # return torch.sqrt(nn.MSELoss()(outputs,targets))\n        loss.backward()\n        optim.step()\n        if lr_scheduler:\n            lr_scheduler.step()\n    \n    \n        # validation\n    total_preds = []\n    # iterate over batches\n    for step, batch in enumerate(val_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n        with torch.no_grad():\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n            \n            preds = outputs[1].detach().cpu().numpy()\n            if standard:\n                preds = scaler.inverse_transform(preds)\n                \n            total_preds.extend(preds-labels.detach().cpu().numpy())\n\n    val_rmse.append(np.sqrt(np.mean(np.square(total_preds))))\n    print(\"Epoch \", epoch, \" RMSE: \", np.sqrt(np.mean(np.square(total_preds))))\n    # test\n    total_preds = []\n    for step, batch in enumerate(test_loader):\n        print(step)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n        with torch.no_grad():\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n\n            preds = outputs[1].detach().cpu().numpy()\n            if standard:\n                    preds = scaler.inverse_transform(preds)\n\n            total_test_preds.append(preds)\n    print(total_test_preds)","4c00c948":"total_preds = []\nfor step, batch in enumerate(val_loader):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n    with torch.no_grad():\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n        loss = outputs[0].detach().cpu().numpy()\n        preds = outputs[1].detach().cpu().numpy()\n        if standard:\n                preds = scaler.inverse_transform(preds)\n\n        total_preds.extend(preds-labels.detach().cpu().numpy())\n\n\nprint(\"RMSE: \", np.sqrt(np.mean(np.square(total_preds))))","01538c28":"print(np.array(total_test_preds))\nval_rmse","80534878":"total_preds = []\nfor step, batch in enumerate(test_loader):\n    print(step)\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n    with torch.no_grad():\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n\n        preds = outputs[1].detach().cpu().numpy()\n        if standard:\n                preds = scaler.inverse_transform(preds)\n\n        total_preds.append(preds)\nprint(total_preds)","88a0edfa":"# total_test_preds = np.array(total_test_preds)\n# total_preds = total_test_preds[np.where(val_rmse==np.min(val_rmse))]\n# print( np.concatenate(total_preds, axis=0))     \n\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsample['target'] = np.concatenate(total_preds, axis=0)\n\nsample.to_csv('\/kaggle\/working\/submission.csv', index=False)\nprint(sample)","818ddf54":"#Load Data","7ab56b5c":"#create a loader"}}