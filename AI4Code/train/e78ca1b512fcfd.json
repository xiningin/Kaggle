{"cell_type":{"1b906fca":"code","028fdccb":"code","60857076":"code","40b187fe":"code","099ba289":"code","14bc60d0":"code","7cdf1797":"code","db0960bf":"code","a93a93d4":"code","271cfc9b":"code","bf343e3b":"code","c3760269":"code","7450149a":"code","1d3ef7f1":"code","027c9b58":"code","d3c287d4":"code","b37ccc39":"code","1211b6b1":"code","666fe6ba":"code","35845f4f":"code","051304fb":"code","d23b5d6b":"code","0779644e":"code","79272e69":"code","b660c2fe":"code","b3b23551":"code","5d5a1247":"code","2089d035":"code","5a26a21c":"code","4ca7707b":"code","03fbab49":"code","86e656a0":"code","e40f23c3":"code","e08a7b92":"code","d56eb5aa":"code","45bb9c72":"code","3f13e12b":"code","201ed3e8":"code","c65c71b9":"code","78225666":"code","6b1c4efb":"code","36ddea09":"code","942fb7d4":"code","280ea9d3":"markdown","4ff25ea9":"markdown","ccbc88e4":"markdown","a26ee766":"markdown","e40edcf9":"markdown","d26df257":"markdown","95f2c636":"markdown"},"source":{"1b906fca":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,precision_recall_curve,roc_curve\nfrom sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models,layers\nfrom statistics import median\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\ncount = 0\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#         pass\n        if count > 20 :\n            break\n        count += 1\n\nplt.style.use(\"fivethirtyeight\")","028fdccb":"imdb_dir = '\/kaggle\/input\/raw-imdb-dataset\/aclImdb'\ntrain_dir = os.path.join(imdb_dir,'train')\ntest_dir = os.path.join(imdb_dir,'test')\ntrain_labels = []\ntrain_texts = []\n\ntest_labels = []\ntest_texts = []","60857076":"for label_type in ['pos','neg']:\n    dir_name = os.path.join(train_dir,label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name,fname))\n            train_texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                train_labels.append(0)\n            else:\n                train_labels.append(1)","40b187fe":"for label_type in ['pos','neg']:\n    dir_name = os.path.join(test_dir,label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name,fname))\n            test_texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                test_labels.append(0)\n            else:\n                test_labels.append(1)","099ba289":"print(f'Length of train texts is {len(train_texts)}')\nprint(f'Length of train labels id {len(train_labels)}')\nprint(f'Length of test texts is {len(test_texts)}')\nprint(f'Length of test labels is {len(test_labels )}')","14bc60d0":"texts_df = pd.DataFrame({'texts': train_texts,\n                        'labels':train_labels})","7cdf1797":"texts_df['word counts'] = texts_df['texts'].apply(lambda x: len(x.split()))","db0960bf":"texts_df.head()","a93a93d4":"median_word_count = median(texts_df['word counts'])\nplt.figure(figsize=(12,6))\nplt.hist(texts_df['word counts'],edgecolor='black')\nplt.title(\"Words Count Distribution\")\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Frequency\/occurrence\")\n\ncolor = '#fc4f30'\n\nplt.axvline(median_word_count,color=color,label=\"Median Word Count\")\nplt.legend()\nplt.tight_layout()","271cfc9b":"positive = texts_df[texts_df['labels']==1]['texts']\nnegative = texts_df[texts_df['labels']==0]['texts']","bf343e3b":"stopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(background_color='white',\n                      stopwords=stopwords,\n                      max_words=200,\n                      max_font_size=40, \n                      random_state=42,\n                      collocations=False\n                      ).generate(str(positive))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","c3760269":"wordcloud = WordCloud(background_color='black',\n                      stopwords=stopwords,\n                      max_words=200,\n                      max_font_size=40, \n                      random_state=42,\n                      collocations=False\n                      ).generate(str(negative))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","7450149a":"MAX_LENGTH = 1000\nMAX_WORDS = 20000\nEMBENDING_DIM = 100","1d3ef7f1":"my_stop_words = ENGLISH_STOP_WORDS.union([\"br\",\"movie\",\"film\"])\ndef remove_stopword(text):\n    \"\"\"\n    Removes StopWords\n    \"\"\"\n    return \" \".join([word for word in text.lower().split() if word not in my_stop_words])","027c9b58":"train_texts = texts_df['texts'].apply(remove_stopword)","d3c287d4":"tokenizer = Tokenizer(num_words=MAX_WORDS)\ntokenizer.fit_on_texts(train_texts)\nsequences = tokenizer.texts_to_sequences(train_texts)","b37ccc39":"word_index = tokenizer.word_index","1211b6b1":"print(f'Found {len(word_index)} unique tokens.' )","666fe6ba":"word_list = []\ncount_list = []\nfor key,val in word_index.items():\n    if val in range(1,21):\n        word_list.append(key)\n        count_list.append(tokenizer.word_counts[key])","35845f4f":"word_list.reverse()\ncount_list.reverse()\nplt.figure(figsize=(12,10))\nplt.barh(word_list,count_list)\nplt.title(\"Top 20 Words\")\nplt.xlabel(\"Counts\")\nplt.ylabel(\"Words\")\nplt.tight_layout()","051304fb":"data = pad_sequences(sequences,maxlen=MAX_LENGTH)\nlabels = np.array(train_labels)","d23b5d6b":"print(f'Shape of Data tensor is {data.shape}')\nprint(f'Shape of Labels tensor is {labels.shape}')","0779644e":"X_train, X_val, y_train, y_val = train_test_split(data,labels,test_size=0.2,random_state=42)","79272e69":"glove_dir = \"\/kaggle\/input\/globe6bzip\/glove.6B.100d.txt\"\n\nembedding_index = {}\nf = open(glove_dir)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.array(values[1:], dtype='float32')\n    embedding_index[word] = coefs\nf.close()\n\nprint(f'Found {len(embedding_index)} word vectors')","b660c2fe":"embedding_index['go'][:10]","b3b23551":"embedding_matrix = np.zeros((MAX_WORDS,EMBENDING_DIM))","5d5a1247":"for word, i in word_index.items():\n    if i < MAX_WORDS:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","2089d035":"embedding_df = pd.DataFrame(embedding_matrix)","5a26a21c":"embedding_df.shape","4ca7707b":"def create_model():\n    model = models.Sequential()\n    model.add(layers.Embedding(MAX_WORDS,EMBENDING_DIM, input_length=MAX_LENGTH))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(128))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1,activation='sigmoid'))\n    model.compile(optimizer='rmsprop', \n              loss='binary_crossentropy',\n              metrics=['acc'])\n    return model","03fbab49":"model = create_model()\nmodel.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False","86e656a0":"history = model.fit(X_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(X_val, y_val))\nmodel.save_weights('pre_trained_glove_model.h5')","e40f23c3":"val_loss = history.history['val_loss']\nval_acc = history.history['val_acc']\ntrain_loss = history.history['loss']\ntrain_acc = history.history['acc']","e08a7b92":"plt.figure(figsize=(12,8))\nepochs = range(1, len(train_acc) + 1)\nplt.plot(epochs, train_loss, label='Training loss')\nplt.plot(epochs, val_loss, label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d56eb5aa":"plt.figure(figsize=(12,8))\nepochs = range(1, len(train_acc) + 1)\nplt.plot(epochs, train_acc, label='Training Accuracy')\nplt.plot(epochs, val_acc, label='Validation Accuracy')\nplt.title('Training and validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","45bb9c72":"final_model = create_model()\nfinal_model.layers[0].set_weights([embedding_matrix])\nfinal_model.layers[0].trainable = False\nfinal_model.fit(data, labels,\n                epochs=2,\n                batch_size=32)","3f13e12b":"test_sequences = tokenizer.texts_to_sequences(test_texts)\ntest_data = pad_sequences(test_sequences,maxlen=MAX_LENGTH)\ntest_labels = np.array(test_labels)","201ed3e8":"predictions = final_model.predict(test_data)\npred_proba = final_model.predict_proba(test_data)","c65c71b9":"pred_labels  = (predictions>0.5)","78225666":"mat = confusion_matrix(pred_labels, test_labels)\nplt.figure(figsize=(4, 4))\nsns.set()\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=np.unique(test_labels),\n            yticklabels=np.unique(test_labels))\nplt.xlabel('true label')\nplt.ylabel('predicted label')","6b1c4efb":"print(classification_report(test_labels,pred_labels))","36ddea09":"precisions, recalls, thresholds = precision_recall_curve(test_labels,pred_proba)\nplt.figure(figsize=(12,6))\nplt.plot(thresholds, precisions[:-1],label='Precision')\nplt.plot(thresholds, recalls[:-1],label=\"Precision\/Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc='upper left')\nplt.ylim([0,1])\nplt.tight_layout()","942fb7d4":"fpr,tpr, thresholds = roc_curve(test_labels,pred_proba)\nplt.figure(figsize=(12,6))\nplt.plot(fpr,tpr, label=None)\nplt.plot([0,1],[0,1])\nplt.axis([0,1,0,1])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.tight_layout()","280ea9d3":"# Exploratory Data Analysis","4ff25ea9":"# Performance Evaluation","ccbc88e4":"# Feature Engineering ","a26ee766":"# Predictions","e40edcf9":"**IMDB Movie Review is a dataset for binary sentiment classification containing 25,000 highly polar movie reviews for training, and 25,000 for testing. We will train a neural network to accomplish the objective of sentiment classification.**","d26df257":"# Model Training","95f2c636":"# Preparing Train & Test Dataframes \n**Each train and test sample is provided as a text file contaning movie review as english text. \nLets extract each file text and create a data frame.**\n   "}}