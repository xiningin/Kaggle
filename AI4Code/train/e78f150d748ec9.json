{"cell_type":{"d1618530":"code","8de56c91":"code","c6d0fb98":"code","acca6d86":"code","b030ff33":"code","284442b9":"code","3c13887e":"code","a2e1653f":"code","88c0265f":"code","75e022aa":"code","d56fd92c":"code","db93ed16":"code","8c6cc324":"code","6b230f59":"code","bae5a283":"code","4db9de4b":"code","7ea2735c":"code","908f5c28":"code","deb0a741":"code","abca4770":"code","66fda225":"code","aecb5e74":"code","8b3a5bbc":"code","152a99d0":"code","d4466786":"code","44c68c97":"code","062bfaea":"code","e8860cdc":"code","781f12ef":"code","ba633d26":"markdown","f0e97176":"markdown","8249d8d4":"markdown","7dc7fbf4":"markdown","d24eebe4":"markdown","c589bccd":"markdown","ddcc12a8":"markdown","1cf56fe7":"markdown","909f659e":"markdown","f6766b9e":"markdown","adbc41c4":"markdown","03ab80ea":"markdown","6d122e5b":"markdown","2098c74a":"markdown","4dc4ae55":"markdown","5c3f5529":"markdown","6901cddc":"markdown","0e06ca78":"markdown","39c5b745":"markdown","dc918f48":"markdown"},"source":{"d1618530":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8de56c91":"#Create a simple dataset from dictionary.\ndictionary_1 = {\"dogru_soru_sayisi\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n              \"puan\":[10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]}\ndata_1 = pd.DataFrame(dictionary_1)\ndata_1.head(20)","c6d0fb98":"#Visualize the data\nplt.scatter(data_1.dogru_soru_sayisi,data_1.puan)\nplt.xlabel(\"dogru_soru_sayisi\")\nplt.ylabel(\"puan\")\nplt.show()","acca6d86":"#Lets look a type\ntype(data_1.dogru_soru_sayisi)","b030ff33":"#Want a array\nx = data_1.dogru_soru_sayisi.values\ny = data_1.puan.values","284442b9":"type(x)","3c13887e":"#How we x of shape?\nx.shape","a2e1653f":"#Want shape (20,1)\nx = data_1.dogru_soru_sayisi.values.reshape(-1,1)\ny = data_1.puan.values.reshape(-1,1)","88c0265f":"x.shape","75e022aa":"#We need sklearn library\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()","d56fd92c":"#We should make fit\nlr.fit(x,y)\n#Visualize\nplt.scatter(x,y)\nplt.xlabel(\"dogru_soru_sayisi\")\nplt.ylabel(\"puan\")\n#We should make predict\ny_head = lr.predict(x)\n#Visualize\nplt.plot(x,y_head, color = \"red\")\nplt.show()","db93ed16":"#Create a simple dataset from dictionary.\ndictionary_2 = {\"enerji\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n                \"guc\":[10,22,34,46,56,61,70,83,97,100,101,104,109,109,118,120,123,123,123,124]}\ndata_2 = pd.DataFrame(dictionary_2)\ndata_2.head(20)","8c6cc324":"#How we make polynomial regression on python?\nx2 = data_2.enerji.values.reshape(-1,1)\ny2 = data_2.guc.values.reshape(-1,1)\n#Library\nfrom sklearn.linear_model import LinearRegression\nlr2 = LinearRegression() \n#Fit\nlr2.fit(x2,y2)\n#Visualize\nplt.scatter(x2,y2, color = \"blue\") #our values\nplt.ylabel(\"guc\")\nplt.xlabel(\"enerji\")\n#Predict\ny_head2 = lr.predict(x2)\n#Visualize\nplt.plot(x2, y_head2 , color=\"red\" )  #that does not represent our values well\n\n#Library\nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 4)  #degree of the polynomial\n#Fit\nx_polynomial = polynomial_regression.fit_transform(x2)\nlr3 = LinearRegression()\nlr3.fit(x_polynomial,y2)\n#Predict\ny_head3 = lr3.predict(x_polynomial)\n#Visualize\nplt.plot(x2,y_head3, color = \"green\")  #this is better\nplt.show()\n","6b230f59":"#How we use decision tree regression on python?\n#We used the data(data_1) we created above.\nx = data_1.dogru_soru_sayisi.values.reshape(-1,1)\ny = data_1.puan.values.reshape(-1,1)\n#Library\nfrom sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\n#Fit\ndtr.fit(x,y)\n#Step\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n#Predict\ny_head4 = dtr.predict(x_)\n#Visualize\nplt.scatter(x,y,color = \"red\")\nplt.plot(x_,y_head4, color = \"green\")\nplt.show()\n\n","bae5a283":"#We build data frames from csv.\ndf = pd.read_csv(\"..\/input\/voice.csv\")","4db9de4b":"df.head()","7ea2735c":"df.info()","908f5c28":"#df = pd.read_csv(\"..\/input\/voice.csv\")\ndf.label.unique()","deb0a741":"df.label = [ 1 if each == \"male\" else 0 for each in df.label]\ndf.label.unique()","abca4770":"y = df.label.values\nx_df = df.drop([\"label\"], axis = 1)","66fda225":"#Normalization on python\nx = (x_df - np.min(x_df))\/(np.max(x_df) - np.min(x_df)).values\n","aecb5e74":"#Train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train:\",x_train.shape)\nprint(\"x_test:\",x_test.shape)\nprint(\"y_train:\",y_train.shape)\nprint(\"y_test:\",y_test.shape)","8b3a5bbc":"#Parameter initialize \ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w , b","152a99d0":"#Sigmoid function\ndef sigmoid(z):\n    y_head_ = 1\/(1 + np.exp(-z))\n    return y_head_\n","d4466786":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head_ = sigmoid(z)\n    loss = -y_train*np.log(y_head_)-(1-y_train)*np.log(1-y_head_)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    \n    #backward propagation\n    derivative_weight = (np.dot(x_train,((y_head_-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head_-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost,gradients","44c68c97":"#Updating parameters\ndef update(w,b,x_train,y_train, learning_rate, num_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    #Updating parameters is num_iteration times\n    for i in range(num_iterations):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate*gradients[\"derivative_weight\"]\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"cost after iteration %i: %f\" %(i,cost))\n            \n            \n    #We update parameters weights and bias\n    parameters = {\"weight\": w, \"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list","062bfaea":"#Prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_predict = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head_=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head_=0)\n    for i in range (z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0\n        else:\n            y_predict[0,i] = 1\n            \n    return y_predict","e8860cdc":"#Logistic regression\ndef logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    # initialize\n    dimension = x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n\n    parameters, gradients, cost_list = update(w,b,x_train,y_train, learning_rate, num_iterations)\n    \n    y_predict_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    # print train\/test errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_predict_test - y_test))*100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 1, num_iterations= 300)","781f12ef":"#Library\nfrom sklearn.linear_model import LogisticRegression\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(logistic_regression.score(x_test.T,y_test.T)))\n","ba633d26":"How much the y_predict values match the y_test values?","f0e97176":"**Hi. I am learning machine learning and want to share what i learn. I hope like this kernel.**","8249d8d4":"The sigmoid function ensures that y_head values are between 0 and 1.","7dc7fbf4":"**Linear Regression**\n\nWe are going to start with linear regression.\n\nFirstly i am going to create a dataset and set up a model of basic linear regression.\n\nLets start..","d24eebe4":"**Polynomial Linear Regression**\n\nIn some cases, the relationship between variables may not be linear. In these cases, polynomial regression is used.","c589bccd":"We should first determine the values of w and b.","ddcc12a8":"Nice...\n\nNow we make linear regression.","1cf56fe7":"**Logistic Regression Classification**\n\nThe logistic regression predicts the likelihood of a result that can only have two values.The logistic regression produces a logistic curve that is limited to values between 0 and 1. Logistic regression is similar to a linear regression, except that the curve is constructed using the natural logarithm of the probabilities of the target variable instead of probability.","909f659e":"We build x_train from %80 of our data and x_test from %20 of our data. X_train will learn and arrive y_train. Later x_test will arrive y_test same way.","f6766b9e":"We need to transform the output of the labels we have into 0 and 1.","adbc41c4":"We arrive cost values from loss values with forward propagation. We want cost values min. Backward propagation allows the updating of cost values by taking derivative of w and b. We will use this function in the update function","03ab80ea":"A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0).","6d122e5b":"Sometimes, a plot of the residuals versus a predictor may suggest there is a nonlinear relationship. One way to try to account for such a relationship is through a polynomial regression model. Such a model for a single predictor.\n\nY=\u03b20+\u03b21*X+\u03b22*X^2+\u2026+\u03b2h*X^h\n\nwhere h is called the degree of the polynomial. For lower degrees, the relationship has a specific name (i.e., h = 2 is called quadratic, h = 3 is called cubic, h = 4 is called quartic, and so on). Although this model allows for a nonlinear relationship between Y and X, polynomial regression is still considered linear regression since it is linear in the regression coefficients","2098c74a":"That's all for now. I will keep to publish by more learning. Thank you.","4dc4ae55":"We will guess with x_test data. We will arrive y_predict values.","5c3f5529":"**Normalization**\n\nRescaling data to have values between 0 and 1. This is usually called feature scaling. One possible formula to achieve this is:\n**(x - min(x))\/(max(x)-min(x))** ","6901cddc":"Logistic Regression with sklearn. It is more easy.","0e06ca78":"Learning rate is our learning speed. Number of iteration is our step count. We will do forward backward propagation by the number of iterations. We created cost_list2 to see the graph better.","39c5b745":"**Decision Tree Regression**\n\nDecision Trees are a type of Supervised Machine Learning, where data is constantly divided according to a certain parameter. The tree can be explained by two entities, decision nodes and leaves. Leaves are decisions or final results. And the decision nodes show where the data is divided.","dc918f48":"We organized x and y."}}