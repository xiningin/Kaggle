{"cell_type":{"52a53b0e":"code","ce75a648":"code","f31ac4b0":"code","4e4e2c34":"code","bd4a7494":"code","631697a0":"code","f724e439":"code","3cc6c74e":"code","b8f23ca4":"code","6120a1e9":"code","7bddfacf":"code","fce583b0":"code","1a8b9526":"code","b125b552":"code","1e9953f5":"code","cae392c2":"code","74ff7a4f":"code","88ddb412":"code","4a12a5bb":"code","4be8ada4":"markdown","d4562719":"markdown","ba80a532":"markdown","6cb64c06":"markdown","b504d797":"markdown","41d5f4c6":"markdown","7ae85418":"markdown","c1005490":"markdown","03b37e70":"markdown","2f7139a2":"markdown","077cfe27":"markdown"},"source":{"52a53b0e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV","ce75a648":"train = pd.read_csv(\"\/kaggle\/input\/jantahack\/train_fNxu4vz.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/jantahack\/test_fjtUOL8.csv\")\nsubm = pd.read_csv(\"\/kaggle\/input\/jantahack\/sample_submission_HSqiq1Q.csv\")","f31ac4b0":"train_df = train.copy()\ntest_df = test.copy()\njoined_df = pd.concat([train_df,test_df],axis=0)\njoined_df = joined_df.reset_index(drop=True)","4e4e2c34":"## Basic Pre-processed Data\njoined_df = pd.read_csv(\"..\/input\/preprocessed\/preprocess\/combined_basic.csv\")\n\n\n## Lets separate train and test and Label Encode and dummy encode and try which models best\ntrain_df = joined_df[:164309]\ntest_df = joined_df[164309:]\ntrain_df.drop(\"Loan_ID\",axis=1,inplace=True)\ntest_df.drop([\"Loan_ID\",\"Interest_Rate\"],axis=1,inplace=True)\nprint(train_df.shape,test_df.shape)\n\nX_train,y_train = train_df.drop(\"Interest_Rate\",1),train_df.loc[:,\"Interest_Rate\"]\nprint(X_train.shape,y_train.shape)","bd4a7494":"## XGboost baseline model and its params\nxgb_obj = XGBClassifier(\n                learning_rate=0.300000012,\n                n_estimators=100,\n                max_depth=6,\n                min_child_weight=1,\n                gamma=0, \n                subsample=1,\n                colsample_bytree=1,\n                objective='multi:softprob', \n                random_state=21, n_jobs=-1,\n                scale_pos_weight=None, \n                verbosity=3,\n                seed=21\n              )","631697a0":"params = {\n 'max_depth':[5,6,7],\n 'min_child_weight':[6,8,10,12,14]\n}\n\nmodel = XGBClassifier(\n              colsample_bytree=0.8, gamma=0,learning_rate=0.300000012, max_depth=6,\n              min_child_weight=1,n_estimators=100, n_jobs=-1,objective='multi:softprob', \n              subsample=0.8,verbosity=3,seed=21)\n\n\ngsearch = GridSearchCV(estimator = model, param_grid = params, scoring='f1_weighted',n_jobs=-1, cv=4,return_train_score=True,verbose=7)\n\ngsearch.fit(X_train,y_train)\nprint(f'Best scor = {gsearch.best_score_} and fixing the params as {gsearch.best_params_}')\nparams = gsearch.cv_results_[\"params\"]\ntest_f1= gsearch.cv_results_[\"mean_test_score\"]\ntrain_f1 = gsearch.cv_results_[\"mean_train_score\"]\ndf = pd.DataFrame({\"xaxis\":params,\"train\":train_f1,\"test\":test_f1})\ndf = df.sort_values(by=\"test\",ascending=False).reset_index(drop=True)\ndf[:10]\n\n## Best scor = 0.5315695946969097 and fixing the params as {'max_depth': 6, 'min_child_weight': 14}\n","f724e439":"params = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\n\nxgb_obj = XGBClassifier(\n                learning_rate=0.300000012,\n                n_estimators=100,\n                subsample=1,\n                colsample_bytree=1,\n                objective='multi:softprob', \n                random_state=21, n_jobs=-1,\n                scale_pos_weight=None, \n                verbosity=3,\n                max_depth=6,\n                min_child_weight =14,\n                seed=21\n              )\n\nrscv = RandomizedSearchCV(xgb_obj,params,random_state=21,cv=4,verbose=4,n_jobs=-1,return_train_score=True,scoring='f1_weighted')\nrscv.fit(X_train,y_train)\n\nx = [i\/10.0 for i in range(0,5)]\ntest_f1= rscv.cv_results_[\"mean_test_score\"]\ntrain_f1 = rscv.cv_results_[\"mean_train_score\"]\nprint(f'Best scor = {rscv.best_score_} and fixing the params as {rscv.best_params_}')\n\ndf = pd.DataFrame({\"xaxis\":x,\"train\":train_f1,\"test\":test_f1})\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=df[\"xaxis\"], y=df[\"train\"],\n                    mode='lines+markers',\n                    name='train_mse'))\nfig.add_trace(go.Scatter(x=df[\"xaxis\"], y=df[\"test\"],\n                    mode='lines+markers',\n                    name='test_mse'))\nfig.show()\n\n## Best scor = 0.531569862841758 and fixing the params as {'gamma': 0.3}","3cc6c74e":"params = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\nmodel = XGBClassifier( \n              colsample_bytree=0.8, gamma=0,learning_rate=0.300000012, max_depth=6,\n              min_child_weight=14,n_estimators=100, n_jobs=-1,objective='multi:softprob', \n              subsample=0.8,verbosity=3 scale_pos_weight=1,seed=21)\n\n\ngsearch = GridSearchCV(estimator = model, param_grid = params, scoring='f1_weighted',n_jobs=-1, cv=4,return_train_score=True,verbose=7)\ngsearch.fit(X_train,y_train)\n\nprint(f'Best scor = {gsearch.best_score_} and fixing the params as {gsearch.best_params_}')\nparams = gsearch.cv_results_[\"params\"]\ntest_f1= gsearch.cv_results_[\"mean_test_score\"]\ntrain_f1 = gsearch.cv_results_[\"mean_train_score\"]\ndf = pd.DataFrame({\"xaxis\":params,\"train\":train_f1,\"test\":test_f1})\ndf = df.sort_values(by=\"test\",ascending=False).reset_index(drop=True)\ndf[:10]\n\n\n##  Here after finding a value they are trying more precise values i.e fine tuning\n\n# Here, we found 0.8 as the optimum value for both subsample and colsample_bytree. \n","b8f23ca4":"params = {\n 'learning_rate':[0.150000006,0.05,0.5,0.1,0.2],\n  'n_estimators':[200,600,60,300,150]\n}\n\nxgb_obj = XGBClassifier(\n                subsample=1,\n                colsample_bytree=1,\n                objective='multi:softprob', \n                random_state=21, n_jobs=-1,\n                scale_pos_weight=None, \n                verbosity=3,\n                max_depth=6,\n                min_child_weight =14,\n                gamma = 0.3,seed=21\n              )\n\nrscv = RandomizedSearchCV(xgb_obj,params,random_state=21,cv=4,verbose=4,n_jobs=-1,return_train_score=True,scoring='f1_weighted')\nrscv.fit(X_train,y_train)\n\n# lr = 0.2\n# n_estimator=150","6120a1e9":"train = pd.read_csv(\"data-loan-cat\/train_fNxu4vz.csv\")\ntest = pd.read_csv(\"data-loan-cat\/test_fjtUOL8.csv\")\nsubm = pd.read_csv(\"data-loan-cat\/sample_submission_HSqiq1Q.csv\")\n\n\ntrain_df,test_df = train.copy(),test.copy()\njoined_df = pd.concat([train_df,test_df],axis=0)\njoined_df.reset_index(drop=True,inplace=True)\nprint(f'OG Train shape ={train_df.shape}\\nOG Test shape ={test_df.shape}\\nOG Join shape ={joined_df.shape}')\n\n## Coverting to int \njoined_df[\"Loan_Amount_Requested\"]= joined_df[\"Loan_Amount_Requested\"].str.replace('\\D+','').astype(int)\n\n\n## Numerical and cat cols\nnumerical = [col for col in joined_df.columns if joined_df[col].dtype!='object']\ncategorical = [col for col in joined_df.columns if joined_df[col].dtype =='object']\n# print(*numerical,sep=\" \")\n# print(\"--------------------------------------------\\n\")\n# print(*categorical,sep=\" \")\n\n\n# ## Imuting missing values\njoined_df[\"Length_Employed\"].fillna(joined_df[\"Length_Employed\"].mode()[0],inplace=True)\njoined_df[\"Home_Owner\"].fillna(joined_df[\"Home_Owner\"].mode()[0],inplace=True)\njoined_df[\"Annual_Income\"].fillna(joined_df[\"Annual_Income\"].median(),inplace=True)\njoined_df[\"Months_Since_Deliquency\"].fillna(joined_df[\"Months_Since_Deliquency\"].median(),inplace=True)\n\n\n## Dropping Id\njoined_df.drop(\"Loan_ID\",axis=1,inplace=True)\nprint(f'Shape after dropping loan id {joined_df.shape}')\n\n## Binning  length employeed\nmapping = {'< 1 year':\"less_than4\",\n           '2 years ':\"less_than4\",\n           '3 years ':\"less_than4\",\n           '4 years':\"between_4to8\",\n           '5 years':\"between_4to8\",\n           '6 years' : \"between_4to8\",\n           '7 years':\"between_4to8\",\n           '8 years':'greater_than8',\n           \"9 years\":'greater_than8',\n           \"10+ years\":'greater_than8'\n    \n}\n\njoined_df['Length_Employed'] = joined_df['Length_Employed'].map(mapping)\nprint(joined_df[\"Length_Employed\"].value_counts())\n\n\n## Binning home owner\nmapping = {'Mortgage':\"Mortgage\",\n           'Rent ':\"Rent\",\n           'Own':\"Own\",\n           'Other':\"Other\",\n           'None':\"Other\"\n           }\n\njoined_df['Home_Owner'] = joined_df['Home_Owner'].map(mapping)\nprint(joined_df[\"Home_Owner\"].value_counts())\n\n## One Hot ENcode\njoined_df = pd.concat([ \n            joined_df.select_dtypes(exclude='object'),\n            pd.get_dummies(joined_df['Length_Employed'],drop_first = True),\n            pd.get_dummies(joined_df['Home_Owner'],drop_first = True),\n            pd.get_dummies(joined_df['Income_Verified'],drop_first = True),\n            pd.get_dummies(joined_df['Purpose_Of_Loan'],drop_first = True),\n            pd.get_dummies(joined_df['Gender'],drop_first = True)\n            \n            ],axis=1)\n\nprint(f'Shape after One HOt encode {joined_df.shape}')\njoined_df.to_csv(\"combined_only_binning.csv\",index=False)\nprint(\"Only binned saved !!\")","7bddfacf":"from itertools import combinations\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef create_ploynomial_interactions(df):\n    print(f'Old shape of the df {df.shape}\\n')\n    \n    combos = list(combinations(list(df.columns), 2))\n    print(f'combination  = {len(combos)}\\n')\n    \n    colnames = list(df.columns)+['_'.join(x) for x in combos]\n    print(f'Total columns now = {len(colnames)}\\n')\n    \n    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n    df = poly.fit_transform(df)\n    print(f'New shape of the df after creating new polynomial features = {df.shape}\\n')\n    df = pd.DataFrame(df)\n    df.columns = colnames\n    \n    ## Dropping the columns which contains all zero as their values i.e column filled with zeroes\n    noint_indices = [col_name for col_name,contains_zero in enumerate(list((df==0).all())) if contains_zero]\n    df= df.drop(df.columns[noint_indices], axis=1)\n    print(f'Final shape of the df after removing zero columns = {df.shape}\\n')\n\n    \n    return df,colnames\n\nX = joined_df.drop(\"Interest_Rate\",1)\nt = X.copy()\ny = joined_df.loc[:,\"Interest_Rate\"]\nX,old_cols= create_ploynomial_interactions(X)\nprint(X.shape)\n\njoined_df = pd.concat([X,y],axis=1)\nprint(f'Final join shape {joined_df.shape}')\njoined_df.to_csv(\"binning_poly.csv\",index=False)\n","fce583b0":"## Let's separate train and test \ntrain_df = joined_df[:164309]\ntest_df = joined_df[164309:]\n\ntest_df.reset_index(drop=True,inplace=True) # since indexes were not starting from zero\ntest_df.drop([\"Interest_Rate\"],axis=1,inplace=True)\nprint(f'Final Train shape {train_df.shape} \\nFinal Test Shape {test_df.shape}')\n\n## Polynomial Dataset\nX_train,y_train = train_df.drop(\"Interest_Rate\",1),train_df.loc[:,\"Interest_Rate\"]\nprint(f'Training shape {X_train.shape,y_train.shape}')","1a8b9526":"## Only Binned Dataset\n\nonly_binned = pd.read_csv(\"combined_only_binning.csv\")\nonly_binned_train = only_binned[:164309]\n\nonly_binned_test = only_binned[164309:]\nonly_binned_test.reset_index(drop=True,inplace=True)\nonly_binned_test.drop(\"Interest_Rate\",axis=1,inplace=True)\n\nob_Xtrain , ob_ytrain = only_binned_train.drop(\"Interest_Rate\",1),only_binned_train.loc[:,\"Interest_Rate\"]\nfinal_Xtrain = pd.concat([ob_Xtrain,X_train[columns]],axis=1)\nfinal_Xtest = pd.concat([only_binned_test,test_df[columns]],axis=1)\n\nprint(ob_Xtrain.shape[1]+100,only_binned_test.shape[1]+100)\nprint(final_Xtrain.shape,final_Xtest.shape)\n\n\n\n## Modelling\n## feature_names must be unique Due to duplicate columns\nfinal_Xtrain = final_Xtrain.loc[:,~final_Xtrain.columns.duplicated()]\nfinal_Xtest = final_Xtest.loc[:,~final_Xtest.columns.duplicated()]\n","b125b552":"columns = ['Loan_Amount_Requested_Annual_Income',\n 'Loan_Amount_Requested_Months_Since_Deliquency',\n 'Loan_Amount_Requested_Inquiries_Last_6Mo',\n 'Loan_Amount_Requested_Debt_To_Income',\n 'Loan_Amount_Requested_Number_Open_Accounts',\n 'vacation',\n 'Loan_Amount_Requested_greater_than8',\n 'Loan_Amount_Requested_Own',\n 'Loan_Amount_Requested_house',\n 'Loan_Amount_Requested_less_than4',\n 'Loan_Amount_Requested_Other',\n 'Loan_Amount_Requested_Total_Accounts',\n 'major_purchase',\n 'Months_Since_Deliquency',\n 'Loan_Amount_Requested_debt_consolidation',\n 'Loan_Amount_Requested',\n 'Loan_Amount_Requested_credit_card',\n 'Loan_Amount_Requested_Male',\n 'Annual_Income_moving',\n 'Loan_Amount_Requested_home_improvement',\n 'Loan_Amount_Requested_educational',\n 'Loan_Amount_Requested_not verified',\n 'Annual_Income_Total_Accounts',\n 'Loan_Amount_Requested_VERIFIED - income source',\n 'Loan_Amount_Requested_medical',\n 'Loan_Amount_Requested_moving',\n 'Annual_Income_Other',\n 'Loan_Amount_Requested_renewable_energy',\n 'Annual_Income_Debt_To_Income',\n 'Annual_Income_less_than4',\n 'Loan_Amount_Requested_vacation',\n 'Annual_Income_greater_than8',\n 'Annual_Income_not verified',\n 'Annual_Income_Number_Open_Accounts',\n 'Annual_Income_Months_Since_Deliquency',\n 'Loan_Amount_Requested_small_business',\n 'Annual_Income_Own',\n 'Annual_Income_Inquiries_Last_6Mo',\n 'Loan_Amount_Requested_other',\n 'Loan_Amount_Requested_major_purchase',\n 'Annual_Income_educational',\n 'Annual_Income_home_improvement',\n 'Annual_Income_VERIFIED - income source',\n 'Debt_To_Income',\n 'home_improvement',\n 'Annual_Income_house',\n 'Annual_Income_credit_card',\n 'Annual_Income',\n 'Annual_Income_debt_consolidation',\n 'wedding',\n 'Annual_Income_medical',\n 'educational',\n 'house',\n 'Total_Accounts',\n 'medical',\n 'Number_Open_Accounts',\n 'Loan_Amount_Requested_wedding',\n 'less_than4',\n 'not verified',\n 'Inquiries_Last_6Mo',\n 'Other',\n 'Annual_Income_major_purchase',\n 'renewable_energy',\n 'greater_than8',\n 'moving',\n 'Own',\n 'Male',\n 'debt_consolidation',\n 'credit_card',\n 'VERIFIED - income source',\n 'other',\n 'small_business']","1e9953f5":"xgb = XGBClassifier(\n                learning_rate=0.2,\n                n_estimators=150,\n                subsample=1,\n                colsample_bytree=1,\n                objective='multi:softprob', \n                n_jobs=-1,\n                scale_pos_weight=None, \n                verbosity=3,\n                max_depth=6,\n                min_child_weight =14,\n                gamma = 0.3\n               \n)","cae392c2":"def cross_val_evaluate(model,X,y,cv,scoring,verbose,model_name):\n    weighted_f1s = cross_val_score(model,X,y,cv=cv,scoring=scoring,verbose=verbose,n_jobs=-1)\n    mean_weighted_f1 = round(np.sum(weighted_f1s)\/cv,5)\n    print(f\" -----------------------{model_name}-------------------------------\")\n    print(f\" weightedF1 for folds = {weighted_f1s}\\n And Mean weighted_f1 on cv = {mean_weighted_f1}\\n\\n\")","74ff7a4f":"## Top 60\ntop60=columns[:60].copy()\n\nfinal_Xtrain = pd.concat([ob_Xtrain,X_train[top60]],axis=1)\nfinal_Xtest = pd.concat([only_binned_test,test_df[top60]],axis=1)\n\nfinal_Xtrain = final_Xtrain.loc[:,~final_Xtrain.columns.duplicated()]\nfinal_Xtest = final_Xtest.loc[:,~final_Xtest.columns.duplicated()]\n\nxgb60 = XGBClassifier(verbosity=3,random_state=21)\ncross_val_evaluate(xgb60,final_Xtrain,y_train,3,'f1_weighted',4,\"XGB\")\n","88ddb412":"## Top 50\ntop50=columns[:50].copy()\n\n\nfinal_Xtrain = pd.concat([ob_Xtrain,X_train[top50]],axis=1)\nfinal_Xtest = pd.concat([only_binned_test,test_df[top50]],axis=1)\n\nfinal_Xtrain = final_Xtrain.loc[:,~final_Xtrain.columns.duplicated()]\nfinal_Xtest = final_Xtest.loc[:,~final_Xtest.columns.duplicated()]\n\nxgb50 = XGBClassifier(verbosity=3,random_state=21)\ncross_val_evaluate(xgb50,final_Xtrain,y_train,3,'f1_weighted',4,\"XGB\")","4a12a5bb":"## Top 40\ntop40=columns[:40].copy()\n\n\nfinal_Xtrain = pd.concat([ob_Xtrain,X_train[top40]],axis=1)\nfinal_Xtest = pd.concat([only_binned_test,test_df[top40]],axis=1)\n\nfinal_Xtrain = final_Xtrain.loc[:,~final_Xtrain.columns.duplicated()]\nfinal_Xtest = final_Xtest.loc[:,~final_Xtest.columns.duplicated()]\n\nxgb40 = XGBClassifier(verbosity=3,random_state=21)\ncross_val_evaluate(xgb40,final_Xtrain,y_train,3,'f1_weighted',4,\"XGB\")","4be8ada4":"# Hyper-parameter tuning","d4562719":"## Tuning learning rate","ba80a532":"# Top 50","6cb64c06":"# Top 40","b504d797":"## Tuning Gamma","41d5f4c6":"## Tuning Subasmple and colsample by tree\n","7ae85418":"## Polynomial Features","c1005490":"# Binning Preprocessing","03b37e70":"## Tuning Max Depth and min_child_wt\n- As these are the parameters which will affect XGBoost the most as they control the tree structure.Hence tuning them first.Leaving other parameters as default\n","2f7139a2":"# Top 60","077cfe27":"## Fine Tunings the above findings"}}