{"cell_type":{"0252f2b8":"code","2df8216a":"code","da23a6aa":"code","139e21be":"code","44cad095":"code","b575bc5f":"code","945547f8":"code","620e346f":"code","3d0aefbb":"markdown","2df65a87":"markdown","05ba750d":"markdown","d8e24e36":"markdown","d6e3f108":"markdown","1aaf26bf":"markdown","812f3dd4":"markdown","d5c98ea5":"markdown","42627da4":"markdown","688dbcb9":"markdown","4a563764":"markdown","94c6c045":"markdown","658d3e7c":"markdown","d59ed828":"markdown","c5a8225a":"markdown","72393af9":"markdown","07f12ef8":"markdown","2f6c725b":"markdown","5f6597d6":"markdown","389ba359":"markdown","35de77f7":"markdown","33726d6e":"markdown","bbbd82bd":"markdown","2d008d7d":"markdown","0a1fc87d":"markdown","4953073d":"markdown","0712a06b":"markdown","ead3e0b1":"markdown","306866d7":"markdown","e3be9e0e":"markdown","8fa26164":"markdown","9e08b1d4":"markdown","027e3a4b":"markdown","729f9a87":"markdown","212e4677":"markdown","08f56495":"markdown","674318b7":"markdown","288a786c":"markdown","5b2e5918":"markdown","4856ed2b":"markdown","6d6971d4":"markdown","028dcdba":"markdown","9521c784":"markdown","7dd02e35":"markdown","331e40c9":"markdown","7e5f42d4":"markdown","09abec54":"markdown","d35fca5f":"markdown","bc111c1d":"markdown","c76c1d12":"markdown","1b9a814e":"markdown","5a1a998a":"markdown","066e8938":"markdown","e8842d4d":"markdown","738f9911":"markdown","320d5219":"markdown","15f6d020":"markdown","d7db840b":"markdown","2367e851":"markdown","67bc19e8":"markdown","fe3758ef":"markdown","6437b286":"markdown","e9d0f2ae":"markdown","20749c3f":"markdown","55ff8240":"markdown","1292fa98":"markdown","2380f203":"markdown","0ffa1bf0":"markdown","7c82636e":"markdown","64a57e58":"markdown","c7aab9ce":"markdown","93ef3c00":"markdown","adf71e42":"markdown","6f5fa2b4":"markdown","541ed07f":"markdown","f1246c0f":"markdown","ea1868f4":"markdown","025df83d":"markdown","0f95d704":"markdown","0e480511":"markdown","00f49e60":"markdown","76bef7f6":"markdown","12bdd421":"markdown"},"source":{"0252f2b8":"model1 = tree.DecisionTreeClassifier()\nmodel2 = KNeighborsClassifier()\nmodel3= LogisticRegression()\n\nmodel1.fit(x_train,y_train)\nmodel2.fit(x_train,y_train)\nmodel3.fit(x_train,y_train)\n\npred1=model1.predict(x_test)\npred2=model2.predict(x_test)\npred3=model3.predict(x_test)\n\nfinal_pred = np.array([])\nfor i in range(0,len(x_test)):\n    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))","2df8216a":"from sklearn.ensemble import VotingClassifier\nmodel1 = LogisticRegression(random_state=1)\nmodel2 = tree.DecisionTreeClassifier(random_state=1)\nmodel = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","da23a6aa":"model1 = tree.DecisionTreeClassifier()\nmodel2 = KNeighborsClassifier()\nmodel3= LogisticRegression()\n\nmodel1.fit(x_train,y_train)\nmodel2.fit(x_train,y_train)\nmodel3.fit(x_train,y_train)\n\npred1=model1.predict_proba(x_test)\npred2=model2.predict_proba(x_test)\npred3=model3.predict_proba(x_test)\n\nfinalpred=(pred1+pred2+pred3)\/3","139e21be":"model1 = tree.DecisionTreeClassifier()\nmodel2 = KNeighborsClassifier()\nmodel3= LogisticRegression()\n\nmodel1.fit(x_train,y_train)\nmodel2.fit(x_train,y_train)\nmodel3.fit(x_train,y_train)\n\npred1=model1.predict_proba(x_test)\npred2=model2.predict_proba(x_test)\npred3=model3.predict_proba(x_test)\n\nfinalpred=(pred1*0.3+pred2*0.3+pred3*0.4)","44cad095":"final_dt = DecisionTreeClassifier(max_leaf_nodes=10, max_depth=5)                   \nfinal_bc = BaggingClassifier(base_estimator=final_dt, n_estimators=40, random_state=1, oob_score=True)\n\nfinal_bc.fit(X_train, train_y)\nfinal_preds = final_bc.predict(X_test)\n\n\nacc_oob = final_bc.oob_score_\nprint(acc_oob)  \n","b575bc5f":"from sklearn.ensemble import RandomForestClassifier\nmodel= RandomForestClassifier(random_state=1)\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)","945547f8":"from sklearn.ensemble import RandomForestRegressor\nmodel= RandomForestRegressor()\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)","620e346f":"from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier(random_state=1)\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)\n\n\n#Sample code for regression problem:\n\nfrom sklearn.ensemble import AdaBoostRegressor\nmodel = AdaBoostRegressor()\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)","3d0aefbb":"Parameters :\n\n**n_estimators:**\nIt defines the number of decision trees to be created in a random forest. Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.\n\n**criterion:**\nIt defines the function that is to be used for splitting. The function measures the quality of a split for each feature and chooses the best split.\n\n**max_features :**\nIt defines the maximum number of features allowed for the split in each decision tree. Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.\n\n**max_depth:**\nRandom forest has multiple decision trees. This parameter defines the maximum depth of the trees. min_samples_split: Used to define the minimum number of samples required in a leaf node before a split is attempted. If the number of samples is less than the required number, the node is not split.\n\n**min_samples_leaf:**\nThis defines the minimum number of samples required to be at a leaf node. Smaller leaf size makes the model more prone to capturing noise in train data.\n\n**max_leaf_nodes:**\nThis parameter specifies the maximum number of leaf nodes for each tree. The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node.\n\n**n_jobs:**\nThis indicates the number of jobs to run in parallel. S","2df65a87":"train algorythm 0 on **A** and make predictions for **B** and **C** and save to **B1,C1**\n\ntrain algorythm 1 on **A** and make predictions for **B** and **C** and save to **B1,C1**","05ba750d":"![Imgur](https:\/\/i.imgur.com\/LmeI08b.png)","d8e24e36":"**Bias error **\n\nis useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends.\n\n**Variance**\n\non the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training. Following diagram will give you more clarity (Assume that red spot is the real value and blue dots are predictions) :\n","d6e3f108":"Bagging is based on the *statistical method of bootstrapping*, Bagging actually refers to (Bootstrap Aggregators). Most any paper or post that references using bagging algorithms will also reference Leo Breiman who wrote a paper in 1996 called \u201c*Bagging Predictors*\u201d.","1aaf26bf":"**Introduction to ensembling**\n\n**Types of ensembling :**\n\n**Basic Ensemble Techniques**\n\n*     Max Voting\n*     Averaging\n*     Weighted Average\n\n**Advanced Ensemble Techniques**\n\n* Stacking\n* Blending\n* Bagging\n* Boosting\n\n**Algorithms based on Bagging and Boosting**\n\n> * Bagging meta-estimator\n* Random Forest\n* AdaBoost\n* GBM\n* XGB\n* Light GBM\n* CatBoost\n","812f3dd4":"**Random Forest** is another ensemble machine learning algorithm that follows the bagging technique. It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest **randomly** selects a set of features which are used to decide the best split at each node of the decision tree.","d5c98ea5":"**Bagging meta-estimator**","42627da4":"**Weighted Average\n**","688dbcb9":"where :\n\n**base_estimator:**\nIt defines the base estimator to fit on random subsets of the dataset. When nothing is specified, the base estimator is a decision tree.\n\n**n_estimators:**\nIt is the number of base estimators to be created. The number of estimators should be carefully tuned as a large number would take a very long time to run, while a very small number might not provide the best results.\n\n**max_samples:**\nThis parameter controls the size of the subsets. It is the maximum number of samples to train each base estimator.\n\n**max_features:**\nControls the number of features to draw from the whole dataset. It defines the maximum number of features required to train each base estimator.\n\n**n_jobs:**\nThe number of jobs to run in parallel. Set this value equal to the cores in your system. If -1, the number of jobs is set to the number of cores.\n\n**random_state:**\nIt specifies the method of random split. When random state value is same for two models, the random selection is same for both models. This parameter is useful when you want to compare different models.","4a563764":"\nStacking is a similar to boosting:\n\nyou also apply several models to your original data. The difference here is, however, that you don't have just an empirical formula for your weight function, rather you introduce a meta-level and use another model\/approach to estimate the input together with outputs of every model to estimate the weights or, in other words, to determine what models perform well and what badly given these input data. and finally I get its true illustration.","94c6c045":"![Imgur](https:\/\/i.imgur.com\/Bo4KItc.png)","658d3e7c":"**Code in python**\n","d59ed828":"**Boosting**\n","c5a8225a":"![Imgur](https:\/\/i.imgur.com\/EQYa8C8.png)","72393af9":"The term \u2018Boosting\u2019 refers to a family of algorithms which **converts weak learner to strong learners**. Boosting is an ensemble method for improving the model predictions of any given learning algorithm. The idea of boosting **is to train weak learners sequentially, each trying to correct its predecessor**.\n\nBoosting is all about \u201c*teamwork*\u201d. Each model that runs, dictates what features the next model will focus on.","07f12ef8":"![Imgur](https:\/\/i.imgur.com\/jFfarvo.png)","2f6c725b":"The error emerging from any model can be broken down into three components mathematically. Following are these component :","5f6597d6":"consider we have a dataset we splite our data set into 3 parts : training, validation , test","389ba359":"**Types of ensembling :****","35de77f7":"**Errors**","33726d6e":"![Imgur](https:\/\/i.imgur.com\/fVazCYe.png)","bbbd82bd":"**Why is this important in the current context?**\nTo understand what really goes behind an ensemble model, we need to first understand what causes error in the model. We will briefly introduce you to these errors and give an insight to each ensemble learner in this regards.","2d008d7d":"At first there is a rational we must stabilize that : combination between models increase accuracy and in machine learning combination is **Ensembling** \n","0a1fc87d":"**Hard voting **\n\nis the simplest case of majority voting. Here, we predict the class label y^ via majority (plurality) voting of each classifier\n\ny^=mode{C1(x),C2(x),...,Cm(x)}\n\nAssuming that we combine three classifiers that classify a training sample as follows:\n\nclassifier 1 -> class 0 classifier 2 -> class 0 classifier 3 -> class 1\n\ny^=mode{0,0,1}=0\n\nVia majority vote, we would we would classify the sample as \"class 0.\"","4953073d":"If all classifiers are able to estimate class probabilities (i.e. they have a predict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers.\n\nThis is called **soft voting** and it often achieves higher performance than hard voting because *it gives more weight to highly confident votes*.\n\nTo perform soft voting, all you need to do is **replace** voting='hard' with voting='soft' **and ensure that all classifiers can estimate class probabilities.\n\n**The SVC class can't estimate class probabilities by default**, so you'll need to set its probability hyperparameter to **True**, as this will make the SVC class use cross-validation to estimate class probabilities (which slows training down), and it will add a predict_proba() method.\n\n**In soft voting**, we predict the class labels based on the predicted probabilities p for classifier -- this approach is only recommended if the classifiers are **well-calibrated**.\n\n*y^=argmaxi\u2211j=1mwjpij,* where **wj** is the weight that can be assigned to the **jth** classifier.\n\nAssuming the example in the previous section was a *binary classification* task with class labels i\u2208{0,1}, our ensemble could make the following prediction:\n\nC1(x)\u2192[0.9,0.1]\n\nC2(x)\u2192[0.8,0.2]\n\nC3(x)\u2192[0.4,0.6]\n\nUsing uniform weights, we compute the average probabilities:\n\np(i0\u2223x)=0.9+0.8+0.43=0.7p(i1\u2223x)=0.1+0.2+0.63=0.3\n\ny^=argmaxi[p(i0\u2223x),p(i1\u2223x)]=0\n\nHowever, assigning the weights {0.1, 0.1, 0.8} would yield a prediction y^=1:\n\np(i0\u2223x)=0.1\u00d70.9+0.1\u00d70.8+0.8\u00d70.4=0.49p(i1\u2223x)=0.1\u00d70.1+0.2\u00d70.1+0.8\u00d70.6=0.51\n\ny^=argmaxi[p(i0\u2223x),p(i1\u2223x)]=1","0712a06b":"![Imgur](https:\/\/i.imgur.com\/eu95V9N.png)","ead3e0b1":"model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. **Ensemble learning is one way to execute this trade off analysis.**","306866d7":"**Code in python**","e3be9e0e":"**Advanced Ensemble techniques**\n","8fa26164":"![Imgur](https:\/\/i.imgur.com\/10slay8.png)","9e08b1d4":"![Imgur](https:\/\/i.imgur.com\/2zjxVBC.png)","027e3a4b":"**Bagging algorithms:**\n","729f9a87":"**Parameters**\n\n**base_estimators:**\nIt helps to specify the type of base estimator, that is, the machine learning algorithm to be used as base learner.\n\n**n_estimators:**\nIt defines the number of base estimators.\nThe default value is 10, but you should keep a higher value to get better performance.\n\n**learning_rate:**\nThis parameter controls the contribution of the estimators in the final combination.\nThere is a trade-off between learning_rate and n_estimators.\n\n**max_depth:**\nDefines the maximum depth of the individual estimator.\nTune this parameter for best performance.\n\n**n_jobs:**\nSpecifies the number of processors it is allowed to use.\nSet value to -1 for maximum processors allowed.\n\n**random_state :**\nAn integer value to specify the random data split.\nA definite value of random_state will always produce same results if given with same parameters and training data.","212e4677":"You can see feature importance by using **model.featureimportances** in random forest.\n\n","08f56495":" **Basic Ensemble Techniques**\n\n*     Max Voting\n*     Averaging\n*     Weighted Average\n\n**Advanced Ensemble Techniques**\n\n* Stacking\n* Blending\n* Bagging\n* Boosting\n\n**Algorithms based on Bagging and Boosting**\n\n> * Bagging meta-estimator\n* Random Forest\n* AdaBoost\n* GBM\n* XGB\n* Light GBM\n* CatBoost\n\n","674318b7":"**AdaBoost**\n","288a786c":"then make this step","5b2e5918":"**Max Voting\n**\n","4856ed2b":"![Imgur](https:\/\/i.imgur.com\/ZDZsSr1.png)","6d6971d4":"lets talk first about Max voting","028dcdba":"**Bagging meta-estimator** is an ensembling algorithm that can be used for **both** classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n\n1-Random subsets are created from the original dataset (Bootstrapping).\n\n2-The subset of the dataset includes all features.\n\n3-A user-specified base estimator is fitted on each of these smaller sets.\n\n4-Predictions from each model are combined to get the final result.","9521c784":"What is in this tutorial?\nin thi tutorial I am trying to illustrate how ensembling techniques work manually and by python code to make a good intuition about why it is useful and why we use it.\n\n","7dd02e35":"**Code in python**","331e40c9":"I hope that I give you a piece of introduction of ensembling methods and this is not the end of my tutorial but this is only the first episode and I will continue soon illustrating the remaining methods of ensemlbing techniques.\n","7e5f42d4":"**This is an extension of the averaging method.** All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.\n\nThe result is calculated as\n\n*[(50.23) + (40.23) + (50.18) + (40.18) + (4*0.18)] = 4.41.*","09abec54":"**Majority Voting \/ Hard Voting\n**\n","d35fca5f":"**Code in python**","bc111c1d":"![Imgur](https:\/\/i.imgur.com\/lpDhGd1.png)","c76c1d12":"**At this moment we stacked predictions to each others thats where stacking name comes from** and then","1b9a814e":"**Random Forest**\n","5a1a998a":"**Code in python**","066e8938":"**Code in python**","e8842d4d":"![Imgur](https:\/\/i.imgur.com\/GEG80ni.png)","738f9911":"**A group of predictors is called an ensemble**; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an** Ensemble method.**\n\nSuppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert's answer. This is called **the wisdom of the crowd**\n\nLikewise, if you aggregate the predictions of a group of predictors (e.g. decision tree classifer, SVM, logistic regression), you will often get better predictions than with the best individual predictor.","320d5219":"![Imgur](https:\/\/i.imgur.com\/bfb2qlr.png)","15f6d020":"**Stability and Accuracy**\n","d7db840b":"By saving each prediction set and averaging them together, you not only lower variance without affecting bias, but your accuracy may be **improved**! In essence, you are creating many slightly different models and ensembling them together; **this avoids over-fitting**, **stabilizes your predictions and increases your accuracy**. Mind you, this assumes your data has variance, if it doesn\u2019t,**bagging won\u2019t help.**","2367e851":"![Imgur](https:\/\/i.imgur.com\/md3L8yB.png)","67bc19e8":"The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a \u2018vote\u2019. The predictions which we get from the majority of the models are used as the final prediction.\n\nFor example, when you asked 5 of your colleagues to rate your movie (out of 5); we\u2019ll assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. You can consider this as taking the mode of all the predictions.\n\nThe result of max voting would be something like this:\n\nColleague 1-5\n\nColleague 2-4\n\nColleague 3-5\n\nColleague 4-4\n\nColleague 5-4\n\nFinalrating-4","fe3758ef":"**Adaptive boosting or AdaBoost** is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n\n**steps:**\n\n1-all observations in the dataset are given equal weights.\n\n2-A model is built on a subset of data.\n\n3-Using this model, predictions are made on the whole dataset.\n\n4-Errors are calculated by comparing the predictions and actual values.\n\n5-While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n\n6-Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n\n7-This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached.","6437b286":"**stacking\n**","e9d0f2ae":"In simple terms, **bagging irons out variance from a data set** . If, after splitting your data into multiple chunks and training them, you find that your predictions are *different*, then your data has *variance*. Bagging can turn a bad thing into a competitive advantage. For more theory behind the magic, check out *Bootstrap Aggregating on Wikipedia.* Bagging was invented by *Leo Breiman* at the University of California. He is also one of the grandfathers of Boosting and Random Forests.","20749c3f":"resources :\n\n\n[Google](https:\/\/www.google.com\/webhp?hl=en&sa=X&ved=0ahUKEwiU0c_cgOLhAhUjQxUIHfetDCwQPAgH)\n    \n\n[Analytics videa](https:\/\/www.analyticsvidhya.com\/)\n    \n\n[youtube](https:\/\/www.youtube.com\/)\n    \n\n[wikipedia](https:\/\/www.wikipedia.org\/)\n    \n  \n  \n  and a lot of other resources .\n    thanks a lot.","55ff8240":"there are 2 methods :\n\n1-Mode\n\n2-Voting classifier","1292fa98":"![Imgur](https:\/\/i.imgur.com\/L2Jaqm8.png)","2380f203":"![Imgur](https:\/\/i.imgur.com\/J9drqs1.png)","0ffa1bf0":"**Averaging**\n\nSimilar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an **average** of predictions from all the models and use it to make the final prediction.Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.For example, in the below case, the averaging method would take the average of all the values.\n\n*i.e. (5+4+5+4+4)\/5 = 4.4*","7c82636e":"![Imgur](https:\/\/i.imgur.com\/Xm5sKxD.png)","64a57e58":"**Bagging** is very common in competitions. I don\u2019t think I have ever seen anybody win without using it. But, in order for this to work, your data must have *variance*, otherwise you\u2019re just adding levels after levels of additional iterations with **little benefit** to your score and a big headache for those maintaining your modeling pipeline in production. Even when it does improve things, you have to asked yourself if its worth all that extra work\u2026","c7aab9ce":"train algorythm 2 on **A** and make predictions for **B** and **C** and save to **B1,C1**","93ef3c00":"**Bagging**","adf71e42":"then we take the data from the validation set which we already knew and we are going to feed a new model .\n\ntrain algorythm 3 on **B1** and make predictions for **C1**","6f5fa2b4":"![Imgur](https:\/\/i.imgur.com\/hfp6JGP.png)","541ed07f":"**Introduction to ensembling :**","f1246c0f":"**Sample code for regression problem:**\n","ea1868f4":"![Imgur](https:\/\/i.imgur.com\/hGkZd9T.png)","025df83d":"**Soft Voting\n**\n","0f95d704":"1-we make subsets with replacement: that means every item may appears in different subsets.\n\n2-apply model for every subset of the sample.\n\n3-The models run in parallel and are independent of each other.\n\n4-predict x-text by using each model\n\n5-then aggregate their predictions (either by voting or by averaging) to form a final prediction.","0e480511":"What is in this tutorial?\nin this tutorial I am trying to illustrate how ensembling techniques work manually and by python code to make a good intuition about why is it  useful and why do we use it.\n","00f49e60":"**Code in python**","76bef7f6":"\n* Bagging meta-estimator\n* Random forest","12bdd421":"step-by-step, this is what a random forest model does:\n\n1-Random subsets are created from the original dataset (bootstrapping).\n\n2-At each node in the decision tree, only a random set of features are considered to decide the best split.\n\n3-A decision tree model is fitted on each of the subsets. The final prediction is calculated by averaging the predictions from all decision trees.\n\n**Note:** The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node.\n\n**To sum up, Random forest randomly selects data points and features, and builds multiple trees (Forest) .**"}}