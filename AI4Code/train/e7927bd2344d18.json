{"cell_type":{"fbe5e391":"code","5d8e7304":"code","f7abfe02":"code","8f1fea91":"code","bf9899c9":"code","e353d5e6":"code","b10976c5":"code","7f4e88d4":"code","bedc6a37":"code","f1cf711b":"code","b80eef41":"code","54f55dc9":"code","898d7b08":"code","db3f1586":"code","b4cf88ad":"code","8acb26cf":"code","9b99eea5":"code","88e9ce89":"code","9c98b6f4":"code","65039e89":"code","3d921996":"code","69560c43":"code","9fc0866e":"code","d5351ad7":"code","411fa58c":"code","7e5bdf4e":"code","458ff9be":"code","f6e04404":"markdown","39d15812":"markdown"},"source":{"fbe5e391":"from IPython.display import Image\nImage(\"..\/input\/graphs\/brands.png\")","5d8e7304":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f7abfe02":"import re\nimport itertools\nfrom __future__ import print_function\nimport pandas as pd\nimport datetime as dt\nimport numpy as np\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\nfrom nltk.tokenize import RegexpTokenizer\nimport string\nimport scipy.sparse as sparse\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\n\n\nimport spacy\nimport en_core_web_sm\nfrom spacy import displacy\nnlp = spacy.load('en')\n#nlp = en_core_web_sm.load()\nfrom nltk.corpus import stopwords\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\n#nltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","8f1fea91":"# to provide serial of numbers\ndef gen(n):\n    while True:\n        yield n\n        n += 1","bf9899c9":"df = pd.read_csv(\"..\/input\/panasonic\/Panasonic_headphones.csv\", encoding='utf-8')\ndf.info()","e353d5e6":"def cleaned_reviews(x):\n    return(''.join(re.sub('[^a-zA-Z_.]', ' ', x)))","b10976c5":"\ndef get_bigram_likelihood(statements, freq_filter=3, nbest=200):\n    \"\"\"\n    Returns n (likelihood ratio) bi-grams from a group of documents\n    :param        statements: list of strings\n    :param output_file: output path for saved file\n    :param freq_filter: filter for # of appearances in bi-gram\n    :param       nbest: likelihood ratio for bi-grams\n    \"\"\"\n\n    #words = list()\n    #tokenize sentence into words\n    #for statement in statements:\n        # remove non-words\n    tokenizer = RegexpTokenizer(r'\\w+')\n    words = tokenizer.tokenize(statements)\n\n    bigram_measures = nltk.collocations.BigramAssocMeasures()\n    bigram_finder = BigramCollocationFinder.from_words(words)\n\n    # only bi-grams that appear n+ times\n    bigram_finder.apply_freq_filter(freq_filter)\n\n    # TODO: use custom stop words\n    bigram_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in nltk.corpus.stopwords.words('english'))\n\n    bigram_results = bigram_finder.nbest(bigram_measures.likelihood_ratio, nbest)\n\n    return bigram_finder.score_ngrams(bigram_measures.likelihood_ratio)","7f4e88d4":"for r in df['Reviews']:\n    if 'sound isolation' in r:\n        #print(\"------\")\n        #print(r)\n        pass","bedc6a37":"df['Reviews'] = df['Reviews'].apply(lambda x : cleaned_reviews(str(x)))","f1cf711b":"review_list = df['Reviews'].tolist()\nprint(len(review_list))\nraw_text = ''.join(review_list)","b80eef41":"bigrams = get_bigram_likelihood(raw_text.lower(), freq_filter=100, nbest=200 )\nbigram_joint = [' '.join(list(s[0])) for s in bigrams]","54f55dc9":"def check_collocation_type(doc, collocation):\n    index = []\n    length =len(doc)\n    for i in range(length-1):\n        if collocation.split(' ')[0] in [doc[i].text, doc[i].lemma_]:\n            if collocation.split(' ')[1] in [doc[i+1].text, doc[i+1].lemma_]:\n                index.append(i)\n    if len(index)> 0:\n        return [(doc[index[0]].text, doc[index[0]+1].text),(doc[index[0]].pos_, doc[index[0]+1].pos_)]\n    else:\n        return False\n\ndef select_sentence(review, collocation):\n    sentence_list = review.split('.')\n    selected_sentences = [sentence for sentence in sentence_list if collocation in sentence]\n    \n    return ' '.join(selected_sentences)\n    \ndef gen(n):\n    while True:\n        yield n\n        n += 1","898d7b08":"selected_collocations_with_type = []\nfor collocation in bigram_joint:\n\n    selected_data = [select_sentence(review.lower(), collocation) for review in review_list if collocation in review]\n\n    \n    g = gen(0)\n    while True:\n        index = next(g)\n        if index >= len(selected_data):\n            break\n        review = selected_data[index]\n        doc= nlp(review)\n        check = check_collocation_type(doc, collocation)\n        if check != False:\n            if  'DET' in check[1]:\n                break\n            if  'NUM' in check[1]:\n                break\n            if check[1][1] == 'ADJ':\n                break\n            if check[1][0] == 'ADJ' and check[1][1] == 'ADV':\n                break\n\n            if check[1][0] == 'ADV' and check[1][1] == 'ADV':\n                break\n            if check[1][1] == 'ADP':\n                break\n            if check[1][0] == 'ADV' and check[1][1] == 'VERB':\n                break\n            if check[1][0] == 'ADJ' and check[1][1] == 'VERB':\n                break\n            if check[1][0] == 'VERB' and check[1][1] == 'ADV':\n                break\n            if check[1][0] == 'VERB' and check[1][1] == 'NOUN':\n                break\n            if check[1][0] == 'VERB' and check[1][1] == 'VERB':\n                break\n            if check[0][1] == 'headphones':\n                break\n            if check[0][1] == 'product':\n                break\n            if check[0][0] in ['good','great','amazing','old','first','nice','better','headphones',\n                               'long', 'little', 'high', 'low', 'mid','build','price','comfortable',\n                              'price','light','second','best','new','reasonable']:\n                break\n            if check[0][1] in ['loves','phone','time','phones','calls','lasts']:\n                break                      \n\n            selected_collocations_with_type.append(check)\n\n            full_data_doc = [(token, token.ent_type_, token.pos_, token.lemma_) for token in doc]\n            break\n        else:   \n            continue\n\n\n","db3f1586":"collocation_type_dict = {}\nfor collocation in selected_collocations_with_type:\n    if collocation[1] not in collocation_type_dict:\n        collocation_type_dict.update({collocation[1]:[]})\n    \n    collocation_type_dict[collocation[1]].append(collocation[0])\n    \nfor c in collocation_type_dict:\n    print(c,\" : \\n\", collocation_type_dict[c])\n    print('\\n')\n\n","b4cf88ad":"selected_collocations = [x[0] for x in selected_collocations_with_type]\nbigrams_updated = [x for x in bigrams if x[0] in selected_collocations]\nfor b in bigrams_updated:\n    print(b)\n\n\n","8acb26cf":"\"\"\"\nContinuing Preprocessing\n\n\"\"\"\n\"\"\"\nHere we replace some of the synonym technical words with the most frequent one\n\"\"\"\ndef resolve_heterogeneity(review_list, listOfWords):\n    frequency_dict = {w:raw_text.count(w) for w in listOfWords}\n    mostFrequency = max(frequency_dict.values())\n    selectedWord = [w for w in frequency_dict.keys() if frequency_dict[w]== mostFrequency][0]\n    \n    for w in listOfWords:\n        for i in range(len(review_list)):\n            if w in review_list[i]:\n                review_list[i] = review_list[i].replace(w, selectedWord)\n                \ndef forced_resolve_heterogeneity(review_list, listOfWords, selectedWord):\n    for w in listOfWords:\n        for i in range(len(review_list)):\n            if w in review_list[i]:\n                review_list[i] = review_list[i].replace(w, selectedWord)\n                \n#Example Noise Canceling, Noise Cancelling, Noise Cancellation, Sound Canceling, Sound Cancelling, Sound Cancellation\nresolve_heterogeneity(review_list, ['canceling','cancelling','cancellation','cancelation'])       \nraw_text = ''.join(review_list)     \n    \n","9b99eea5":"# audio, sound\nforced_resolve_heterogeneity(review_list, ['audio'], 'sound')\nforced_resolve_heterogeneity(review_list, ['bud '], 'buds ')\n#noise cancelation, isolation\nresolve_heterogeneity(review_list, ['canceling','cancelling','cancellation','cancelation'])\nforced_resolve_heterogeneity(review_list, ['outside noise'], 'noise isolation')\nforced_resolve_heterogeneity(review_list, ['background noise'], 'noise cancelling')\nresolve_heterogeneity(review_list, ['isolating','isolation']) \nresolve_heterogeneity(review_list, ['sound cancelling','noise cancelling']) \nresolve_heterogeneity(review_list, ['sound isolation','noise isolation']) \n#dissimilarity\nresolve_heterogeneity(review_list, ['ear','side'])    \nresolve_heterogeneity(review_list, ['left','right'])  \n\nresolve_heterogeneity(review_list, ['ear phones','ear cups']) \n\nforced_resolve_heterogeneity(review_list, ['bluetooth'], 'blue tooth')\n\n#volume control\nforced_resolve_heterogeneity(review_list, ['ear monitors'], 'volume control')\n\n# sound quality, quality sound\nresolve_heterogeneity(review_list, ['sound quality','quality sound']) \n# ear piece, ear pieces\nresolve_heterogeneity(review_list, ['ear piece ','ear pieces ']) \n\nraw_text = ' '.join(review_list)   ","88e9ce89":"bigrams = get_bigram_likelihood(raw_text.lower(), freq_filter=100, nbest=200 )\nbigram_joint = [' '.join(list(s[0])) for s in bigrams]","9c98b6f4":"selected_collocations_with_type = []\nfor collocation in bigram_joint:\n\n    selected_data = [select_sentence(review.lower(), collocation) for review in review_list if collocation in review]\n\n    \n    g = gen(0)\n    while True:\n        index = next(g)\n        if index >= len(selected_data):\n            break\n        review = selected_data[index]\n        doc= nlp(review)\n        check = check_collocation_type(doc, collocation)\n        if check != False:\n            if  'DET' in check[1]:\n                break\n            if  'NUM' in check[1]:\n                break\n            if check[1][1] == 'ADJ':\n                break\n            if check[1][0] == 'ADJ' and check[1][1] == 'ADV':\n                break\n\n            if check[1][0] == 'ADV' and check[1][1] == 'ADV':\n                break\n            if check[1][1] == 'ADP':\n                break\n            if check[1][0] == 'ADV' and check[1][1] == 'VERB':\n                break\n            if check[1][0] == 'ADJ' and check[1][1] == 'VERB':\n                break\n            if check[1][0] == 'VERB' and check[1][1] == 'ADV':\n                break\n            if check[1][0] == 'VERB' and check[1][1] == 'NOUN':\n                break\n            if check[1][0] == 'VERB' and check[1][1] == 'VERB':\n                break\n            if check[0][1] == 'headphones':\n                break\n            if check[0][1] == 'product':\n                break\n            if check[0][0] in ['good','great','amazing','old','first','nice','better','headphones',\n                               'long', 'little', 'high', 'low', 'mid','build','price','comfortable',\n                              'price','light','second','best','new','reasonable']:\n                break\n            if check[0][1] in ['loves','phone','time','phones','calls','lasts']:\n                break            \n\n            selected_collocations_with_type.append(check)\n\n            full_data_doc = [(token, token.ent_type_, token.pos_, token.lemma_) for token in doc]\n            break\n        else:   \n            continue\n\n\n\n","65039e89":"collocation_type_dict = {}\nfor collocation in selected_collocations_with_type:\n    if collocation[1] not in collocation_type_dict:\n        collocation_type_dict.update({collocation[1]:[]})\n    \n    collocation_type_dict[collocation[1]].append(collocation[0])\n    \nfor c in collocation_type_dict:\n    print(c,\" : \\n\", collocation_type_dict[c])\n    print('\\n')\n\n\n\n","3d921996":"selected_collocations = [x[0] for x in selected_collocations_with_type]\nbigrams_updated = [x for x in bigrams if x[0] in selected_collocations]\nfor b in bigrams_updated:\n    print(b)\n\n","69560c43":"selected_collocations_joint = [' '.join(bigram) for bigram in selected_collocations]\nprint(selected_collocations_joint)","9fc0866e":"\"\"\"\nSecond Step: Sentiment Analysis\n\"\"\"\n\"\"\"\nsecond pair\nbackground noise\nnew pair\n\"\"\"\nselected_collocations","d5351ad7":"def filter_reviews(review, bigram):\n    if bigram in review:\n        return True\n    else:\n        return False\ndef review_sentiment(string):\n    sent = analyser.polarity_scores(string)\n    return sent\n\n\n","411fa58c":"BrandsList = [ 'Akg', 'BoseAudio', 'Otium', 'Ailhen',  'Shure']\n#'Panasonic', 'Sennheiser', , 'sony'\nFinal_result = {}\nFinal_dict = {}\nfor Brand in BrandsList:\n    Temp_list = []\n    Temp_dict = {}\n    brand_df = pd.read_csv('..\/input\/brands\/{}_headphones.csv'.format(Brand))\n    print(len(brand_df.index))\n    brand_df = brand_df.rename(index=str, columns={\"Unnamed: 0\": \"ID\"})\n    brand_df['Reviews'] = brand_df['Reviews'].apply(lambda x : cleaned_reviews(str(x)))\n    brand_reviews = brand_df['Reviews'].tolist()\n    brand_reviews = [str(x) for x in brand_reviews]\n    brand_raw_text = ' '.join(brand_reviews)\n    brand_bigrams =get_bigram_likelihood(brand_raw_text, freq_filter=3, nbest=200 )\n    \n    for bigram in selected_collocations_joint:\n        Filtered_list = [select_sentence(review, bigram) for review in brand_reviews if filter_reviews(review, bigram)]\n\n        if len(Filtered_list) > 0:\n            df = pd.DataFrame({\"Reviews\":Filtered_list})\n            df['Sentiments'] = df['Reviews'].apply(lambda x: review_sentiment(x))\n            df = pd.concat([df.drop(['Sentiments'], axis=1), df['Sentiments'].apply(pd.Series)], axis=1)\n            negativity = df['neg'].sum()\n            #print(negativity)\n            positivity = df['pos'].sum()\n            Temp_list.append((bigram,positivity\/(positivity+negativity)))\n            Temp_dict.update({bigram:positivity\/(positivity+negativity)})\n        else:\n            Temp_dict.update({bigram:-1})\n            \n    Final_result.update({Brand: sorted(Temp_list, key = lambda x:x[1])}) \n    Final_dict.update({Brand: Temp_dict})    \n    print(\"Brand : \", Brand)\n    print(\"Bad Features:\")\n    for b in Final_result[Brand][:6]:\n        print(b)\n    print(\"Good Features:\")\n    for b in Final_result[Brand][-6:]:\n        print(b)\n    print('\\n')\n\n    \n\n\n","7e5bdf4e":"Final_table = np.array([[Final_dict[Brand][collocation] for collocation in \n                         selected_collocations_joint] for Brand in BrandsList])\nnumOfBrands = len(Final_table)\nnumOfCollocations = len(Final_table[0])\n\n\ndef regularize(myTable):\n    n = len(myTable)\n    m = len(myTable[0])\n    min_table = min([min(myTable[j]) for j in range(n)])\n    max_table = max([max(myTable[j]) for j in range(n)])\n    return [[(myTable[j][i]-min_table)\/(max_table - min_table)  for i in range(m) ] \n               for j in range(n)]\n\n","458ff9be":"\"\"\"\nWe have 3 missed data for Bose and AKG brands: Bluetooth, Battery life, Foam tips\n\"\"\"\nFinal_table = [[Final_table[j][i] if Final_table[j][i]>-1 \n                else np.mean([Final_table[k][i] for k in range(numOfBrands) if Final_table[k][i]>-1])\n                             for i in range(numOfCollocations) ] \n               for j in range(numOfBrands)]\n\nFinal_table = np.array(regularize(Final_table))\n#","f6e04404":"We can take a look at all the reviews which contain \"sound isolation\" as on of the features.","39d15812":"\nThe convergence and divergence of existing products in the market are mainly a long-term processes. Looking back in the hisory, smart cellphones emerged almost 10 years after the academic breakthroughs in digital CMOS circuits. Research literatures, Patent records, and custormer reviews, and tweets are the main resources to monitor the evolution of products and predict the future of market.\n\nAny hardware or software product in the market can be defined as a set of funtionalities and features. Getting information about the new technological advancement on one hand, and customer interface on the other hand we can predict the possible trends in the market. I am applying recently developed text analysis methods to monitor the diffusion of features for the products in the market. The ultimate goal of my project is developing new methods with predicting capability to study the evolution of new functionalities.\n\nAs a starting point, I focused on headphone reviews on the Amazon. I have extracted all the headphone reviews for a set of 15 renowned headphone brands (Sennheiser, Sony, Boss, AILIHEN, Panasonic, ...). Then, I extracted mainly discussed topics as features. I ended up with features like battery life, bass response, noise cancellation, sound isolation, earbuds, volume control, and bluetooth. Then, I segmented the reviews using LDA. Then I produced a table of brand-feature satisfaction. This table shows the extent of customer satisfaction on each specific feature for each brand. Moreover, I applied k-mean to cluster the reviews based on discussed features. In the next steps I am applying multi-grain LDA (latend dirichlet allocation) for topic modeling. After, I will apply context-aware text analysis to learn the main resource of unsatisfaction for each of discussed features in the reviews. Then, I would apply the fully-automated analysis to some other products and may refine the process or add some more steps. This is all about the first phase of my project and I planned to finish this phase by the end of the March 2019."}}