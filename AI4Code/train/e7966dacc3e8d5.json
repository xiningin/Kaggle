{"cell_type":{"021867d4":"code","d46f8e85":"code","8eaa7343":"code","0666c876":"code","abe6a282":"code","781a053d":"code","8fc0c1cd":"code","45c9c76e":"code","69922503":"code","8122bdd2":"code","c293a964":"code","c440c737":"code","c868224a":"code","3216d55a":"code","20205067":"code","b3718ac5":"code","eff65498":"code","78ceab2a":"code","1b99f78c":"code","ba2662f6":"code","55091832":"code","0a44f0da":"code","f559405e":"code","02a73bc7":"code","352425a5":"code","2d49240e":"code","6aceb1b9":"code","239216a6":"code","7a733fec":"code","4374a04e":"code","af18e7b6":"code","6da848f1":"code","db85b9af":"code","738e0dd3":"code","cf04a988":"code","3f50fa31":"code","9c2b8c3d":"code","344cffe3":"code","3989c23d":"code","eb290dfe":"code","5cf238a7":"code","ee412a6a":"markdown","8c489264":"markdown","c64eb347":"markdown","24b03a83":"markdown","4a88fd7d":"markdown","0ca72e7b":"markdown","ee1e1721":"markdown","60b150b8":"markdown","0601481e":"markdown","c32bfb78":"markdown","b16092f1":"markdown","0fc7a3b5":"markdown","286cd291":"markdown","8dc51570":"markdown","4ad8a641":"markdown","46d1925a":"markdown","55970589":"markdown","fd6f3b3c":"markdown","85a6d36b":"markdown","afb2eaeb":"markdown","22d8c167":"markdown"},"source":{"021867d4":"%load_ext autoreload","d46f8e85":"%autoreload 2","8eaa7343":"%matplotlib inline","0666c876":"import io\nimport os\nimport math\nimport copy\nimport pickle\nimport zipfile\nfrom textwrap import wrap\nfrom pathlib import Path\nfrom itertools import zip_longest\nfrom collections import defaultdict\nfrom urllib.error import URLError\nfrom urllib.request import urlopen\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F \nfrom torch.optim.lr_scheduler import _LRScheduler","abe6a282":"plt.style.use('ggplot')","781a053d":"def set_random_seed(state=1):\n    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n    for set_state in gens:\n        set_state(state)","8fc0c1cd":"RANDOM_STATE = 1\nset_random_seed(RANDOM_STATE)","45c9c76e":"def read_data(path):\n    files = {}\n    for filename in os.listdir(path):\n        stem, suffix =  os.path.splitext(filename)\n        file_path = os.path.join(path,filename)\n        print(filename)\n        if suffix == '.csv':\n            files[stem] = pd.read_csv(file_path)\n        elif suffix == '.dat':\n            if stem == 'ratings':\n                columns = ['userId', 'movieId', 'rating', 'timestamp']\n            else:\n                columns = ['movieId', 'title', 'genres']\n            data = pd.read_csv(file_path, sep='::', names=columns, engine='python')\n            files[stem] = data\n    return files['ratings'], files['movies']","69922503":"# pick one of the available folders\n\nratings, movies = read_data('..\/input\/movielens100k\/')","8122bdd2":"ratings.head()","c293a964":"movies.head()","c440c737":"def tabular_preview(ratings, n=15):\n    \"\"\"Creates a cross-tabular view of users vs movies.\"\"\"\n    \n    user_groups = ratings.groupby('userId')['rating'].count()\n    top_users = user_groups.sort_values(ascending=False)[:n]\n\n    movie_groups = ratings.groupby('movieId')['rating'].count()\n    top_movies = movie_groups.sort_values(ascending=False)[:n]\n\n    top = (\n        ratings.\n        join(top_users, rsuffix='_r', how='inner', on='userId').\n        join(top_movies, rsuffix='_r', how='inner', on='movieId'))\n\n    return pd.crosstab(top.userId, top.movieId, top.rating, aggfunc=np.sum)","c868224a":"tabular_preview(ratings, 20)","3216d55a":"def create_dataset(ratings, top=None):\n    if top is not None:\n        ratings.groupby('userId')['rating'].count()\n    \n    unique_users = ratings.userId.unique()\n    user_to_index = {old: new for new, old in enumerate(unique_users)}\n    new_users = ratings.userId.map(user_to_index)\n    \n    unique_movies = ratings.movieId.unique()\n    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n    new_movies = ratings.movieId.map(movie_to_index)\n    \n    n_users = unique_users.shape[0]\n    n_movies = unique_movies.shape[0]\n    \n    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n    y = ratings['rating'].astype(np.float32)\n    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)","20205067":"(n, m), (X, y), _ = create_dataset(ratings)\nprint(f'Embeddings: {n} users, {m} movies')\nprint(f'Dataset shape: {X.shape}')\nprint(f'Target shape: {y.shape}')","b3718ac5":"class ReviewsIterator:\n    \n    def __init__(self, X, y, batch_size=32, shuffle=True):\n        X, y = np.asarray(X), np.asarray(y)\n        \n        if shuffle:\n            index = np.random.permutation(X.shape[0])\n            X, y = X[index], y[index]\n            \n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.n_batches = int(math.ceil(X.shape[0] \/\/ batch_size))\n        self._current = 0\n        \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        return self.next()\n    \n    def next(self):\n        if self._current >= self.n_batches:\n            raise StopIteration()\n        k = self._current\n        self._current += 1\n        bs = self.batch_size\n        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]","eff65498":"def batches(X, y, bs=32, shuffle=True):\n    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n        xb = torch.LongTensor(xb)\n        yb = torch.FloatTensor(yb)\n        yield xb, yb.view(-1, 1) ","78ceab2a":"for x_batch, y_batch in batches(X, y, bs=4):\n    print(x_batch)\n    print(y_batch)\n    break","1b99f78c":"class EmbeddingNet(nn.Module):\n    \"\"\"\n    Creates a dense network with embedding layers.\n    \n    Args:\n    \n        n_users:            \n            Number of unique users in the dataset.\n\n        n_movies: \n            Number of unique movies in the dataset.\n\n        n_factors: \n            Number of columns in the embeddings matrix.\n\n        embedding_dropout: \n            Dropout rate to apply right after embeddings layer.\n\n        hidden:\n            A single integer or a list of integers defining the number of \n            units in hidden layer(s).\n\n        dropouts: \n            A single integer or a list of integers defining the dropout \n            layers rates applyied right after each of hidden layers.\n            \n    \"\"\"\n    def __init__(self, n_users, n_movies,\n                 n_factors=50, embedding_dropout=0.02, \n                 hidden=10, dropouts=0.2):\n        super().__init__()\n        hidden = get_list(hidden)\n        dropouts = get_list(dropouts)\n        n_last = hidden[-1]\n        \n        def gen_layers(n_in):\n            \"\"\"\n            A generator that yields a sequence of hidden layers and \n            their activations\/dropouts.\n            \n            Note that the function captures `hidden` and `dropouts` \n            values from the outer scope.\n            \"\"\"\n            nonlocal hidden, dropouts\n            assert len(dropouts) <= len(hidden)\n            \n            for n_out, rate in zip_longest(hidden, dropouts):\n                yield nn.Linear(n_in, n_out)\n                yield nn.ReLU()\n                if rate is not None and rate > 0.:\n                    yield nn.Dropout(rate)\n                n_in = n_out\n            \n        self.u = nn.Embedding(n_users, n_factors)\n        self.m = nn.Embedding(n_movies, n_factors)\n        self.drop = nn.Dropout(embedding_dropout)\n        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n        self.fc = nn.Linear(n_last, 1)\n        self._init()\n        \n    def forward(self, users, movies, minmax=None):\n        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n        x = self.drop(features)\n        x = self.hidden(x)\n        out = torch.sigmoid(self.fc(x))\n        if minmax is not None:\n            min_rating, max_rating = minmax\n            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n        return out\n    \n    def _init(self):\n        \"\"\"\n        Setup embeddings and hidden layers with reasonable initial values.\n        \"\"\"\n        def init(m):\n            if type(m) == nn.Linear:\n                torch.nn.init.xavier_uniform_(m.weight)\n                m.bias.data.fill_(0.01)\n                \n        self.u.weight.data.uniform_(-0.05, 0.05)\n        self.m.weight.data.uniform_(-0.05, 0.05)\n        self.hidden.apply(init)\n        init(self.fc)\n    \n    \ndef get_list(n):\n    if isinstance(n, (int, float)):\n        return [n]\n    elif hasattr(n, '__iter__'):\n        return list(n)\n    raise TypeError('layers configuraiton should be a single number or a list of numbers')","ba2662f6":"EmbeddingNet(n, m, n_factors=150, hidden=100, dropouts=0.5)","55091832":"EmbeddingNet(n, m, n_factors=150, hidden=[100, 200, 300], dropouts=[0.25, 0.5])","0a44f0da":"class CyclicLR(_LRScheduler):\n    \n    def __init__(self, optimizer, schedule, last_epoch=-1):\n        assert callable(schedule)\n        self.schedule = schedule\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]","f559405e":"def triangular(step_size, max_lr, method='triangular', gamma=0.99):\n    \n    def scheduler(epoch, base_lr):\n        period = 2 * step_size\n        cycle = math.floor(1 + epoch\/period)\n        x = abs(epoch\/step_size - 2*cycle + 1)\n        delta = (max_lr - base_lr)*max(0, (1 - x))\n\n        if method == 'triangular':\n            pass  # we've already done\n        elif method == 'triangular2':\n            delta \/= float(2 ** (cycle - 1))\n        elif method == 'exp_range':\n            delta *= (gamma**epoch)\n        else:\n            raise ValueError('unexpected method: %s' % method)\n            \n        return base_lr + delta\n        \n    return scheduler","02a73bc7":"def cosine(t_max, eta_min=0):\n    \n    def scheduler(epoch, base_lr):\n        t = epoch % t_max\n        return eta_min + (base_lr - eta_min)*(1 + math.cos(math.pi*t\/t_max))\/2\n    \n    return scheduler","352425a5":"def plot_lr(schedule):\n    ts = list(range(1000))\n    y = [schedule(t, 0.001) for t in ts]\n    plt.plot(ts, y)","2d49240e":"plot_lr(triangular(250, 0.005))","6aceb1b9":"plot_lr(triangular(250, 0.005, 'triangular2'))","239216a6":"plot_lr(triangular(250, 0.005, 'exp_range', gamma=0.999))","7a733fec":"plot_lr(cosine(t_max=500, eta_min=0.0005))","4374a04e":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\ndatasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid)}\ndataset_sizes = {'train': len(X_train), 'val': len(X_valid)}","af18e7b6":"minmax = ratings.rating.min(), ratings.rating.max()\nminmax","6da848f1":"net = EmbeddingNet(\n    n_users=n, n_movies=m, \n    n_factors=150, hidden=[500, 500, 500], \n    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])","db85b9af":"print(n,m)","738e0dd3":"lr = 1e-3\nwd = 1e-5\nbs = 2000 \nn_epochs = 100\npatience = 10\nno_improvements = 0\nbest_loss = np.inf\nbest_weights = None\nhistory = []\nlr_history = []\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nnet.to(device)\ncriterion = nn.MSELoss(reduction='sum')\noptimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\niterations_per_epoch = int(math.ceil(dataset_sizes['train'] \/\/ bs))\nscheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr\/10))\n\nfor epoch in range(n_epochs):\n    stats = {'epoch': epoch + 1, 'total': n_epochs}\n    \n    for phase in ('train', 'val'):\n        training = phase == 'train'\n        running_loss = 0.0\n        n_batches = 0\n        batch_num = 0\n        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n            x_batch, y_batch = [b.to(device) for b in batch]\n            optimizer.zero_grad()\n            # compute gradients only during 'train' phase\n            with torch.set_grad_enabled(training):\n                outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n                loss = criterion(outputs, y_batch)\n                \n                # don't update weights and rates when in 'val' phase\n                if training:\n                    scheduler.step()\n                    loss.backward()\n                    optimizer.step()\n                    lr_history.extend(scheduler.get_lr())\n                    \n            running_loss += loss.item()\n            \n        epoch_loss = running_loss \/ dataset_sizes[phase]\n        stats[phase] = epoch_loss\n        \n        # early stopping: save weights of the best model so far\n        if phase == 'val':\n            if epoch_loss < best_loss:\n                print('loss improvement on epoch: %d' % (epoch + 1))\n                best_loss = epoch_loss\n                best_weights = copy.deepcopy(net.state_dict())\n                no_improvements = 0\n            else:\n                no_improvements += 1\n                \n    history.append(stats)\n    print('[{epoch:03d}\/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n    if no_improvements >= patience:\n        print('early stopping after epoch {epoch:03d}'.format(**stats))\n        break","cf04a988":"ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')","3f50fa31":"_ = plt.plot(lr_history[:2*iterations_per_epoch])","9c2b8c3d":"net.load_state_dict(best_weights)","344cffe3":"groud_truth, predictions = [], []\n\nwith torch.no_grad():\n    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n        x_batch, y_batch = [b.to(device) for b in batch]\n        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n        groud_truth.extend(y_batch.tolist())\n        predictions.extend(outputs.tolist())\n\ngroud_truth = np.asarray(groud_truth).ravel()\npredictions = np.asarray(predictions).ravel()","3989c23d":"final_loss = np.sqrt(np.mean((np.array(predictions) - np.array(groud_truth))**2))\nprint(f'Final RMSE: {final_loss:.4f}')","eb290dfe":"np.array(predictions)","5cf238a7":"with open('best.weights', 'wb') as file:\n    pickle.dump(best_weights, file)","ee412a6a":"---\n## Training Loop\n\nNow we're ready to start the training process. First of all, let's split the original dataset using [train_test_split](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) function from the `scikit-learn` library. (Though you can use anything else instead, like, [get_cv_idxs](https:\/\/github.com\/fastai\/fastai\/blob\/921777feb46f215ed2b5f5dcfcf3e6edd299ea92\/fastai\/dataset.py#L6-L22) from `fastai`).","8c489264":"---\n## Dataset Preparation\n\nNext we need a couple of helpers to prepare the data and make it ready for the training loop. The first one was already implemented in the original post:","c64eb347":"---\n## Dataset Downloading\n\nIn the original post, the `ml-latest-small` archive with movies and users is used but there are a few other bigger datasets. You can download any of them. However, some of the datasets use different format to store the records. The function `read_data` tries to read content of the dataset if it is stored with `.csv` or `.dat` extensions.","24b03a83":"The training process was terminated after _16 epochs_. Now we're going to restore the best weights saved during training, and apply the model to the validation subset of the data to see the final model's performance:","4a88fd7d":"The second one is a small class that allows to iterate through the dataset one batch after another:","0ca72e7b":"**[Credits: Github Notebook](https:\/\/github.com\/devforfu\/pytorch_playground\/blob\/master\/movielens.ipynb)**","ee1e1721":"---\n## Imports","60b150b8":"# Collaborative Filterting on MovieLens Dataset\n\nIn this notebook, I'm making an attempt to \"unwrap\" abstaction layers of this [fast.ai lecture](http:\/\/course.fast.ai\/lessons\/lesson5.html) where the author is explaining how to use embeddings to build a simple recommendation system.\n\nIn other words, I'm going to directly use PyTorch framework to train a small deep learning model to predict user ratings for specific movies. The original lesson's notebook is presented [here](https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/dl1\/lesson5-movielens.ipynb).\n\n> **Note:** The notebook requires Python 3.6 or above. Otherwise, replace `pathlib` objects with plain paths, and interpolated strings with `format()` calls to make it compatible with the older versions of the interpreter.","0601481e":"Our scheduler is very similar to [LambdaLR](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.LambdaLR) one but expects a bit different callback signature. \n\nSo now we only need to define appropriate scheduling functions. We're createing a couple of functions that accept scheduling parameters and return a _new function_ with the appropriate signature:","c32bfb78":"A convenice function `batches` allows us to split the dataset into smaller chunks during training\/validation process:","b16092f1":"---\n## Embeddings\n\nIn this secion we're going to create an `nn.Module` for our embedding network. The main difference from the class provided in the original post is that this class is a bit more generic: it allows to create a network of arbirary depth, with or without dropouts. Also, the `forward` method accepts an additional `minmax` argument which is expected to be a tuple with minimum and maximum values of ratings from the dataset to normalize the predicted value into specific range.\n\nNote that to create a group of hidden layers we use a [generator](https:\/\/wiki.python.org\/moin\/Generators) function `gen_layers` that yields linear and dropout layers depending on values from `hidden` and `dropouts` arrays. Also note that we use the `nn.Sequence` class to group hidden layers into a single module.","0fc7a3b5":"As expected, the learning rate is updated in accordance with cosine annealing schedule.","286cd291":"---\n## Cyclical Learning Rate (CLR)\n\nOne of the `fastai` library features is the cyclical learning rate scheduler. We can implement something similar inheriting the `_LRScheduler` class from the `torch` library. Following the [original paper's](https:\/\/arxiv.org\/abs\/1506.01186) pseudocode, this [CLR Keras callback implementation](https:\/\/github.com\/bckenstler\/CLR), and making a couple of adjustments to support [cosine annealing](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR) with restarts, let's create our own CLR scheduler.\n\nThe implementation of this idea is quite simple. The [base PyTorch scheduler class](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html) has the `get_lr()` method that is invoked each time when we call the `step()` method. The method should return a list of learning rates depending on the current training epoch. In our case, we have the same learning rate for all of the layers, and therefore, we return a list with a single value. \n\nThe next cell defines a `CyclicLR` class that expectes a single callback function. This function should accept the current training epoch and the base value of learning rate, and return a new learning rate value.","8dc51570":"The function below ensures that we seed all random generators with the same value to get reproducible results:","4ad8a641":"---\n## Metrics\n\nTo visualize the training process and to check the correctness of the learning rate scheduling, let's create a couple of plots using collected stats:","46d1925a":"---\n## Dataset Preview\n\nFirst of all, let's repeat the steps of the lecture where the dataset is loaded and prepared for further analysis.","55970589":"If case if something more involved is required, pass a list of values instead:","fd6f3b3c":"To understand how the created functions work, and to check the correctness of our implementation, let's create a couple of plots visualizing learning rates changes depending on the number of epoch:","85a6d36b":"Note that cosine annealing scheduler is a bit different from other schedules as soon as it starts with `base_lr` and gradually decreases it to the minimal value while triangle schedulers increase the original rate.","afb2eaeb":"The next cell is preparing and running the training loop with cyclical learning rate, validation and early stopping. We use `Adam` optimizer with cosine-annealing learnign rate. The rate is decreased on each batch during `2` epochs, and then is reset to the original value.\n\nNote that our loop has two phases. One of them is called `train`. During this phase, we update our network's weights and change the learning rate. The another one is called `val` and is used to check the model's performence. When the loss value decreases, we save model parameters to restore them later. If there is no improvements after `10` sequential training epochs, we exit from the loop.","22d8c167":"For example, if we want to create a network with a single hidden layer and dropout, we can use the following code snippet:"}}