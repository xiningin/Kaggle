{"cell_type":{"421b04d8":"code","34617f7c":"code","01918e7e":"code","260bf0a1":"code","c406745c":"code","9e3266b0":"code","114ccf94":"code","6f8db39a":"code","c714981e":"code","41a0bf77":"code","3c75175a":"code","c35770e3":"code","aa759036":"code","61b13e6b":"code","cb6b2669":"code","4a875a32":"code","8e6fe195":"code","407e9ee0":"code","31d73364":"code","5c2750f4":"code","fd10c96e":"code","7540ef94":"code","4c9caf98":"code","7ace7f08":"code","dd9607d2":"code","2bd0ac73":"code","d1cb4f69":"code","9f84fb3c":"code","02410ee5":"code","850e1ac3":"code","e96f7825":"code","817a467d":"code","8ab3f2ea":"code","0402225a":"code","52e6a1fb":"code","9fede390":"code","37de04ae":"code","1ca668a6":"code","d8aa99f1":"code","07ff2dfb":"code","9664be6a":"code","bb0a303e":"markdown","d8b3f192":"markdown","390c64a7":"markdown","56047544":"markdown","e920e0c4":"markdown","1d20046e":"markdown","17aefb46":"markdown","4643f297":"markdown"},"source":{"421b04d8":"# Libraries\nimport numpy as np\nimport pandas as pd\n\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\nimport shap","34617f7c":"# Data\ndata = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv', index_col=0)\n\npreds = data.columns[:-1]\ntarget = data.columns[-1]","01918e7e":"# Preprocessing\ncat_cols = [col for col in preds if 'cat' in col]\ndata[cat_cols] = data[cat_cols].astype('category')\ntest[cat_cols] = test[cat_cols].astype('category')","260bf0a1":"# Best params\nbest_params = {}","c406745c":"def objective(trial):\n    # Search spaces\n    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        estimator = LGBMRegressor(n_estimators=n_estimators)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","9e3266b0":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*0.25)","114ccf94":"# Best score\nstudy.best_value","6f8db39a":"# Historic\nplot_optimization_history(study)","c714981e":"# Best params\nbest_params.update(study.best_params)\nbest_params","41a0bf77":"def objective(trial):\n    hyper_params = {\n        'num_leaves': trial.suggest_int('num_leaves', 1, 63),\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","3c75175a":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*1.5)","c35770e3":"# Best score\nstudy.best_value","aa759036":"# Historic\nplot_optimization_history(study)","61b13e6b":"# Best params\nbest_params.update(study.best_params)\nbest_params","cb6b2669":"def objective(trial):\n    hyper_params = {\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf ', 1, 100),\n        'max_bin': trial.suggest_int('max_bin', 15, 2043)\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","4a875a32":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*1)","8e6fe195":"# Best score\nstudy.best_value","407e9ee0":"# Historic\nplot_optimization_history(study)","31d73364":"# Importance\nplot_param_importances(study)","5c2750f4":"# Best params\nbest_params.update(study.best_params)\nbest_params","fd10c96e":"def objective(trial): \n    hyper_params = {\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 100),\n        'bagging_fraction': trial.suggest_float('bagging_fraction ', 0, 1.0),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0, 1.0)\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","7540ef94":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*2)","4c9caf98":"# Best score\nstudy.best_value","7ace7f08":"# Historic\nplot_optimization_history(study)","dd9607d2":"# Importance\nplot_param_importances(study)","2bd0ac73":"# Best params\nbest_params.update(study.best_params)\nbest_params","d1cb4f69":"def objective(trial): \n    hyper_params = {\n        'lambda_l1': trial.suggest_float('lambda_l1', 1E-12, 25, log=True)\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","9f84fb3c":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*1)","02410ee5":"# Best score\nstudy.best_value","850e1ac3":"# Historic\nplot_optimization_history(study)","e96f7825":"# Best params\nbest_params.update(study.best_params)\nbest_params","817a467d":"def objective(trial): \n    hyper_params = {\n        'lambda_l2': trial.suggest_float('lambda_l2', 1E-12, 20, log=True),\n        'path_smooth': trial.suggest_float('path_smooth', 1E-12, 20, log=True),\n        'cat_smooth': trial.suggest_float('cat_smooth', 1E-12, 20, log=True)\n    }\n\n    # Evaluation\n    scores = []\n\n    kf = KFold(5)\n    for i, (train_idx, test_idx) in enumerate(kf.split(data)):\n        X_train = data.iloc[train_idx][preds]\n        y_train = data.iloc[train_idx][target]\n        X_test = data.iloc[test_idx][preds]\n        y_test = data.iloc[test_idx][target]\n\n        hyper_params.update(best_params)\n        \n        estimator = LGBMRegressor(**hyper_params)\n\n        estimator.fit(X_train, \n                      y_train, \n                      eval_set=(X_test, y_test), \n                      eval_metric='rmse',\n                      categorical_feature=cat_cols,\n                      verbose=0)\n\n        y_pred = estimator.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred, squared=False)\n        scores.append(rmse)\n\n    return np.mean(scores)","8ab3f2ea":"# Optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*1)","0402225a":"# Best score\nstudy.best_value","52e6a1fb":"# Historic\nplot_optimization_history(study)","9fede390":"# Importance\nplot_param_importances(study)","37de04ae":"# Best params\nbest_params.update(study.best_params)\nbest_params","1ca668a6":"# Evaluation\nk = 10\ntest[target] = 0\n\nscores = []\n\nkf = KFold(k)\nfor i, (train_idx, test_idx) in enumerate(kf.split(data)):\n    X_train = data.iloc[train_idx][preds]\n    y_train = data.iloc[train_idx][target]\n    X_test = data.iloc[test_idx][preds]\n    y_test = data.iloc[test_idx][target]\n\n    \n    best_params['learning_rate'] = 0.005\n    best_params['n_estimators'] = 100000\n    \n    estimator = LGBMRegressor(**best_params)\n\n    estimator.fit(X_train, \n                  y_train, \n                  eval_set=(X_test, y_test), \n                  eval_metric='rmse',\n                  early_stopping_rounds=1000,\n                  categorical_feature=cat_cols,\n                  verbose=1000)\n\n    y_pred = estimator.predict(X_test)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    scores.append(rmse)\n\n    test[target] += estimator.predict(test[preds]) \/ k\n\ntest[target].to_csv('submission.csv')","d8aa99f1":"print(f\"Expected score: {np.mean(scores)}\")","07ff2dfb":"# Shap values - only applied on the last estimator\nexplainer = shap.TreeExplainer(estimator)\nshap_values = explainer.shap_values(X_test)","9664be6a":"# Summary\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","bb0a303e":"## Optimize n_estimators","d8b3f192":"## Setup","390c64a7":"## Optimize regulation - Fraction","56047544":"## Evaluation with low learning rate","e920e0c4":"## Otpimize Tree Properties - Others","1d20046e":"## Optimize Regulation - L1","17aefb46":"## Optimize Tree Properties - Num_leaves","4643f297":"## Optimize regulation - Others"}}