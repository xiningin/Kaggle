{"cell_type":{"0bbbda72":"code","703d3637":"code","b0e5e036":"code","a172526b":"code","d9e2f319":"code","68ef591d":"code","1b7d69c5":"code","9dc11298":"code","b10caa95":"code","2920bfac":"code","6fed7ac2":"code","4b865a1a":"code","839bf83f":"code","b1f6a0f9":"code","e449fd08":"code","8b5e957f":"code","f8940d32":"code","d83def8e":"code","f2b8232f":"code","b990af8b":"code","969560c3":"code","d33022b0":"code","806fd494":"code","f9539d96":"code","1c90fadc":"code","6f21298b":"code","2a1abd28":"code","eb187085":"code","a51a7434":"code","52b2b093":"code","0f386d67":"code","b02f1abd":"code","e463e99e":"code","26d3d079":"code","5c31b92d":"code","7a83d8d2":"code","e3103d12":"code","b60ff431":"code","df298965":"code","5bce7ccf":"code","afbc6063":"code","1f3c56e9":"code","90e4cc2c":"code","9eec9256":"code","3514c814":"code","8fa18191":"code","841fde3b":"code","f39e136d":"markdown","cdff67ac":"markdown","68917025":"markdown","40458446":"markdown","986736f8":"markdown","16aea488":"markdown","cdf268c8":"markdown","ff430fbf":"markdown","a7dd18f3":"markdown","9cb57b39":"markdown","6a0cac54":"markdown","53b63d48":"markdown","53b67902":"markdown","ea72a369":"markdown","d970aaf8":"markdown","ae2e06b7":"markdown","1d29353f":"markdown","51f87b01":"markdown","6e8dd6ff":"markdown","9ab5eeff":"markdown","c5cd7040":"markdown","8ef90585":"markdown","2d327422":"markdown","794678c2":"markdown","082d5620":"markdown","be861398":"markdown","062e0ad0":"markdown","a00d4c06":"markdown"},"source":{"0bbbda72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","703d3637":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nfrom sklearn.preprocessing import LabelEncoder\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\n# print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","b0e5e036":"train=pd.read_csv('\/kaggle\/input\/bnu-esl-2020\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/bnu-esl-2020\/test.csv')\ntest_id=test['id']\ntrain.head()","a172526b":"#\u68c0\u67e5\u6837\u672c\u548c\u7279\u5f81\u7684\u6570\u91cf\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#\u5c06id\u8bbe\u7f6e\u4e3aindex\ntrain_ID = train['Unnamed: 0']\ntest_ID = test['id']\n\n#\u5220\u9664id\u5217\ntrain.drop(\"Unnamed: 0\", axis = 1, inplace = True)\ntest.drop(\"id\", axis = 1, inplace = True)\n\n#\u518d\u6b21\u68c0\u67e5\u6837\u672c\u548c\u7279\u5f81\u7684\u6570\u91cf\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","d9e2f319":"# \u770b population \u548c median_house_value \u7684\u5173\u7cfb\nfig, ax = plt.subplots()\nax.scatter(x = train['population'], y = train['median_house_value'])\nplt.ylabel('median_house_value', fontsize=13)\nplt.xlabel('population', fontsize=13)\nplt.show()","68ef591d":"train = train.drop(train[(train['population']>15000)].index)","1b7d69c5":"#\u518d\u6b21\u68c0\u67e5\u56fe\u50cf\nfig, ax = plt.subplots()\nax.scatter(train['population'], train['median_house_value'])\nplt.ylabel('median_house_value', fontsize=13)\nplt.xlabel('population', fontsize=13)\nplt.show()","9dc11298":"# The relationship between population and median_house_value\nfig, ax = plt.subplots()\nax.scatter(x = train['median_income'], y = train['median_house_value'])\nplt.ylabel('median_house_value', fontsize=13)\nplt.xlabel('median_income', fontsize=13)\nplt.show()","b10caa95":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in train.columns:\n    if train[i].dtype in numeric_dtypes:\n        if i in ['ocean_proximity']:\n            pass\n        else:\n            numeric.append(i) \nnumeric","2920bfac":"# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='median_house_value', hue='median_house_value', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('median_house_value', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","6fed7ac2":"sns.distplot(train['median_house_value'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['median_house_value'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('median_house_value distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['median_house_value'], plot=plt)\nplt.show()","4b865a1a":"# \u5220\u9664\u79bb\u7fa4\u70b9\ntrain = train.drop(train[(train['median_house_value']>500000)].index)","839bf83f":"#\u68c0\u67e5\u6570\u636e\u5e76\u5bf9'median_house_value'\u8fdb\u884c log-transformation\ntrain['median_house_value'] = np.log1p(train['median_house_value'])\n\n#Check the new distribution \nsns.distplot(train['median_house_value'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['median_house_value'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#\u753b\u56fe\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#\u540c\u65f6\u753b\u51faQQ-plot\nfig = plt.figure()\nres = stats.probplot(train['median_house_value'], plot=plt)\nplt.show()\n","b1f6a0f9":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.median_house_value.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['median_house_value'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\nall_data.head()","e449fd08":"data = pd.concat([train['median_house_value'], train['housing_median_age']], axis=1)","8b5e957f":"# \u586b\u5145\u7f3a\u5931\u503c\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(all_data)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss","f8940d32":"plt.plot(all_data['total_bedrooms'])\nall_data.dropna(axis=0, how='any', inplace=True)","d83def8e":"# \u786e\u8ba4\u7f3a\u5931\u503c\u5df2\u5904\u7406\u5b8c\nmissing = percent_missing(all_data)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss","f2b8232f":"# \u68c0\u67e5\u5404\u53d8\u91cf\u4e0e\u9500\u552e\u4ef7\u683c\u7684\u76f8\u5173\u6027\uff0c\u753b\u51fa\u76f8\u5173\u56fe\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","b990af8b":"# \u6807\u7b7e\u7f16\u7801\u5206\u7c7b\u53d8\u91cf\u201cocean_proximity\u201d\nlbl = LabelEncoder() \nlbl.fit(list(all_data['ocean_proximity'].values)) \nall_data['ocean_proximity'] = lbl.transform(list(all_data['ocean_proximity'].values))      \nprint('Shape all_data: {}'.format(all_data.shape))\nall_data","969560c3":"# \u68c0\u67e5\u6240\u6709\u6570\u5b57\u7279\u5f81\u7684\u504f\u5ea6\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness","d33022b0":"# \u7528Box Cox Transformation \u5bf9\u9ad8\u504f\u5ea6\u7684\u53d8\u91cf\u8fdb\u884c\u5904\u7406\nskewness = skewness[abs(skewness) > 0.75].dropna(axis=0)\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","806fd494":"# \u5f97\u5230\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","f9539d96":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","1c90fadc":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","6f21298b":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","2a1abd28":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","eb187085":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","a51a7434":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","52b2b093":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","0f386d67":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","b02f1abd":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e463e99e":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","26d3d079":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5c31b92d":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7a83d8d2":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e3103d12":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","b60ff431":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","df298965":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","5bce7ccf":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","afbc6063":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","1f3c56e9":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","90e4cc2c":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","9eec9256":"import lightgbm as lgb\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","3514c814":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","8fa18191":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","841fde3b":"sub = pd.DataFrame()\nsub['Id'] = test.index\nsub['predicted'] = ensemble\nsub.to_csv('result-kaggle-stacked-regressions.csv',index=False)","f39e136d":"\u4ea4\u53c9\u9a8c\u8bc1\uff1a\u4f7f\u7528Sklearn\u7684cross_val_score\u51fd\u6570\u3002\u7136\u800c\uff0c\u8fd9\u4e2a\u51fd\u6570\u6ca1\u6709shuffle\uff0c\u4f46\u662f\uff0c\u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u884c\u4ee3\u7801\uff0c\u4ee5\u4fbf\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e4b\u524d\u5c06\u6570\u636e\u6253\u4e71","cdff67ac":"\u63a5\u4e0b\u6765\u5206\u6790\u76ee\u6807\u53d8\u91cf \u201cmedian_house_value\u201d\u3002","68917025":"## Stacking models\n\u7b80\u5355\u7684stacking model: Averaging base models\n\n\u4ece\u5e73\u5747\u57fa\u672c\u6a21\u578b\u7684\u7b80\u5355\u65b9\u6cd5\u5f00\u59cb\uff0c\u6784\u5efa\u4e00\u4e2a\u65b0\u7c7b\uff0c\u4ee5\u5c06scikit-learn\u5e94\u7528\u5728\u6a21\u578b\u4e2d\uff0c\u5e76\u5b9e\u73b0\u5c01\u88c5\u548c\u4ee3\u7801\u91cd\u7528(\u7ee7\u627f)\n\n**1. Averaged base models class**","40458446":"**\u6700\u7ec8\u7684\u8bad\u7ec3\u96c6\u4e0e\u9884\u6d4b StackedRegressor:**","986736f8":"# \u603b\u7ed3\u53cd\u601d\n\n\u8fd9\u662f\u6211\u5728kaggle\u4e0a\u5b8c\u6210\u7684\u7b2c\u4e00\u4e2anotebook\uff0c\u5728\u771f\u6b63\u81ea\u5df1\u52a8\u624b\u5728kaggle\u4e0a\u5199\u4e1c\u897f\u4e4b\u540e\uff0c\u7a81\u7136\u6709\u4e86\u4f5c\u4e3a\u6570\u636e\u79d1\u5b66\u5b66\u751f\u5728\u9762\u5bf9kaggle\u7ade\u8d5b\u65f6\u5174\u594b\u611f\uff0c\u71ac\u591c\u5199\u4ee3\u7801\u7ec8\u4e8e\u8dd1\u901a\u7684\u6fc0\u52a8\u611f\uff0c\u4ee5\u53ca\u4e0d\u65ad\u8ffd\u6c42\u66f4\u9ad8\u5f97\u5206\u7684\u6e34\u671b\u3002\u63a5\u4e0b\u6765\u662f\u5bf9\u672c\u9879\u76ee\u7684\u6d41\u7a0b\u6982\u62ec\u4e0e\u81ea\u6211\u53cd\u601d\u3002\n\n## \u9879\u76ee\u6d41\u7a0b\u603b\u7ed3\uff1a\n**\u4e00\u3001\u524d\u671f\u5904\u7406**\n1. \u5bfc\u5165\u6587\u4ef6\u53ca\u76f8\u5173python\u5305\n\n2. \u5927\u81f4\u89c2\u5bdf\u6837\u672c\u7684\u7279\u5f81\uff0c\u5e76\u5c06\u5404\u4e2a\u6570\u636e\u7684id\u4f5c\u4e3aindex\uff0c\u540c\u65f6\u5220\u53bbid\u5217\n\n**\u4e8c\u3001\u7279\u5f81\u5de5\u7a0b\u53ca\u6570\u636e\u9884\u5904\u7406**\n\n1. \u753b\u51fapopulation \u548c median_house_value \u7684\u6563\u70b9\u56fe\uff0c\u53d1\u73b0population\u5927\u4e8e15000\u7684\u662f\u79bb\u7fa4\u70b9\uff0c\u56e0\u6b64\u5220\u9664\u79bb\u7fa4\u70b9\uff0c\u5220\u9664\u540e\u518d\u6b21\u68c0\u67e5\u56fe\u50cf\uff0c\u79bb\u7fa4\u70b9\u6d88\u5931\n\n2. \u4ece\u76f4\u89c2\u4e0a\u6765\u8bf4\uff0cmedian_income\u548cvalue\u5e94\u5f53\u6709\u660e\u663e\u7684\u5173\u8054\uff0c\u56e0\u6b64\u753b\u51fa\u8fd9\u4e24\u8005\u7684\u6563\u70b9\u56fe\uff0c\u53d1\u73b0\u4e24\u8005\u7684\u5173\u7cfb\u4e0d\u662f\u5f88\u660e\u663e\uff0c\u56e0\u6b64\u65e0\u6cd5\u5224\u65ad\u79bb\u7fa4\u70b9\u3002\n\n3. \u5bf9\u6240\u6709\u6570\u503c\u578b\u53d8\u91cf\u8fdb\u884c\u4e24\u4e24\u6563\u70b9\u56fe\u7684\u7ed8\u5236\u3002\n\n4. \u5206\u6790\u76ee\u6807\u53d8\u91cfmedian_house_value\n\n    1. \u53d1\u73b0\u76ee\u6807\u53d8\u91cf\u662f\u53f3\u504f\u7684\uff0c\u7531\u4e8e(\u7ebf\u6027)\u6a21\u578b\u66f4\u9002\u5e94\u6b63\u6001\u5206\u5e03\u7684\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u8fd9\u4e2a\u53d8\u91cf\u8fdb\u884c\u53d8\u6362\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u6b63\u6001\u5206\u5e03\u3002\n    \n    2. \u7ecf\u8fc7\u8f6c\u6362\u540e\uff0c\u6b6a\u659c\u4f3c\u4e4e\u5f97\u5230\u4e86\u7ea0\u6b63\uff0c\u6570\u636e\u4f3c\u4e4e\u66f4\u7b26\u5408\u6b63\u6001\u5206\u5e03\u3002\n    \n    3. \u56e0\u6b64\u5220\u9664\u79bb\u7fa4\u70b9\u540e\u5bf9\u53d8\u91cf\u8fdb\u884clog\u8f6c\u6362\u3002\n    \n5. \u4e3a\u4e86\u65b9\u4fbf\u5904\u7406\uff0c\u4e8e\u662f\u5c06\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u8fde\u63a5\u5230\u4e00\u8d77\u5f62\u6210all_data\u6570\u636e\u96c6\u3002\n\n6. \u53d1\u73b0\u7f3a\u5931\u503c\u662ftotal_bedrooms\uff0c\u800c\u5bf9total_bedrooms\u53d8\u91cf\u8fdb\u884c\u89c2\u5bdf\u540e\u53d1\u73b0\uff0c\u7f3a\u5931\u7684\u6bd4\u4f8b\u662f1.02%\uff0c\u4e14\u5927\u90e8\u5206\u8be5\u53d8\u91cf\u7684\u6570\u503c\u8f83\u5927\uff0c\u5982\u679c\u5c06\u7f3a\u5931\u503c\u586b\u5145\u4e3a0\uff0c\u5f15\u8d77\u7684\u8bef\u5dee\u66f4\u5927\uff0c\u56e0\u6b64\u5c06\u7f3a\u5931\u503c\u5220\u9664\u3002\n\n7. \u89c2\u5bdf\u5404\u53d8\u91cf\u7684\u76f8\u5173\u7cfb\u6570\u56fe\n\n8. \u5bf9\u5206\u7c7b\u53d8\u91cf\u201cocean_proximity\u201d\u8fdb\u884c\u6807\u7b7e\u7f16\u7801\n\n9. \u8ba1\u7b97\u5404\u53d8\u91cf\u7684\u504f\u5ea6\uff0c\u5e76\u5bf9\u504f\u5ea6\u8f83\u9ad8\u7684\u53d8\u91cf\u8fdb\u884clog\u5904\u7406\n\n**\u4e09\u3001\u5efa\u6a21**\n1. \u5f15\u5165\u6a21\u578b\u9700\u8981\u7684\u5404\u79cd\u5305\n\n2. \u8bbe\u5b9a\u4ea4\u53c9\u9a8c\u8bc1\u7684\u5404\u79cd\u53c2\u6570\uff0c\u5e76\u8bbe\u5b9a\u5f15\u7528\u6a21\u578b\u7684\u51fd\u6570\uff0c\u4ee5\u4fbf\u5728\u5957\u7528\u5404\u79cd\u6a21\u578b\u7684\u65f6\u5019\u66f4\u52a0\u4fbf\u6377\n\n3. \u8bbe\u7acb\u5404\u57fa\u672c\u6a21\u578b\u7684\u53c2\u6570\u503c\uff0c\u5305\u62ecLasso Regression\u3001Elastic Net Regression\u3001Kernel Ridge Regression\u3001Gradient Boosting Regression\u3001XGBoost\u3001LightGBM\u516d\u79cd\uff0c\u901a\u8fc7\u67e5\u9605\u5404\u79cd\u8d44\u6599\u4e0e\u5b9e\u9a8c\uff0c\u786e\u5b9a\u6a21\u578b\u7684\u53c2\u6570\u53d6\u503c\uff0c\u5e76\u521d\u6b65\u8ba1\u7b97\u5404\u57fa\u672c\u6a21\u578b\u7684\u5f97\u5206\u3002\n\n4. \u63a5\u4e0b\u6765\u8ba1\u7b97\u5806\u53e0\u7684\u6a21\u578b\n    1. \u9996\u5148\u662f\u7b80\u5355\u7684\u5806\u53e0\uff0c\u5373\u5bf9ENet, GBoost, KRR and lasso \u8fd9\u56db\u4e2a\u6a21\u578b\u7684\u56de\u5f52\u7ed3\u679c\u8fdb\u884c\u5e73\u5747\u5e76\u7b97\u51fa\u5f97\u5206\n    \n    2. \u63a5\u4e0b\u6765\u5c1d\u8bd5\u66f4\u8d1f\u8d23\u7684\u5806\u53e0\u6a21\u578b\uff0c\u5373\u5148\u5c06\u8bad\u7ec3\u53ca\u5212\u5206\u4e3a\u4e24\u4e2a\u65e0\u4ea4\u96c6\u7684\u96c6\u5408\uff0c\u5c06\u5728\u8bad\u7ec3\u96c6\u7684\u7b2c\u4e00\u90e8\u5206\u8bad\u7ec3\u57fa\u7840\u7684\u6a21\u578b\uff0c\u518d\u7528\u7b2c\u4e8c\u90e8\u5206\u6d4b\u8bd5\u6a21\u578b\uff0c\u6700\u540e\u7528\u7b2c\u4e8c\u90e8\u5206\u7684\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u8f93\u5165\uff0c\u8bad\u7ec3\u66f4\u9ad8\u5c42\u7ea7\u7684\u6a21\u578b\uff0c\u5373meta-model\n    \n    3. \u540c\u65f6\uff0c\u4e3a\u4e86\u4f7f\u4ee5\u4e0a\u4e24\u79cd\u6a21\u578b\u66f4\u5177\u6709\u53ef\u6bd4\u6027\uff0c\u5728\u5806\u53e0\u6a21\u578b\u65f6\u4e5f\u5148\u5e73\u5747Enet KRR\u548cGboost\uff0c\u518d\u6dfb\u52a0lasso\u4f5c\u4e3a\u5143\u6a21\u578b\uff0c\u8ba1\u7b97\u51fa\u5f97\u5206\n    \n    4. \u6700\u540e\u5c06\u5168\u4f53\u7684\u5f97\u5206\u7ed3\u679c\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u4f5c\u4e3a\u6700\u540e\u7684\u7ed3\u679c\u5e76\u63d0\u4ea4\n\n## \u53cd\u601d\n\n\u5c3d\u7ba1\u672c\u6a21\u578b\u518d\u56de\u5f52\u4e2d\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u7ed3\u679c\uff0c\u4f46\u4ecd\u4ece\u8fc7\u7a0b\u548c\u5b9e\u8df5\u4e2d\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff1a\n\n1. \u9996\u5148\uff0c\u5bf9\u6a21\u578b\u7684\u7406\u89e3\u4e0d\u591f\u5145\u5206\u3002\u5728\u5957\u7528\u6a21\u578b\u65f6\uff0c\u5f88\u591a\u65f6\u5019\u662f\u5c06\u4e00\u4e9b\u7ecf\u9a8c\u53c2\u6570\u7528\u5728\u6a21\u578b\u91cc\uff0c\u4f46\u5e76\u4e0d\u4e00\u5b9a\u662f\u771f\u6b63\u9002\u5408\u6a21\u578b\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u5bf9\u6a21\u578b\u7684\u53c2\u6570\u7684\u542b\u4e49\u4e0e\u4f5c\u7528\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u5b66\u4e60\u4e0e\u6539\u826f\u3002\n\n2. \u5176\u6b21\uff0c\u5728\u5806\u53e0\u6a21\u578b\u7684\u6743\u91cd\u8bbe\u7f6e\u4e0a\u5b58\u5728\u4e00\u5b9a\u7684\u4e3b\u89c2\u6027\uff0c\u5e94\u8be5\u80fd\u591f\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u6839\u636e\u5404\u57fa\u7840\u6a21\u578b\u7684\u62df\u5408\u6548\u679c\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u6743\u91cd\u5e76\u5f97\u51fa\u62df\u5408\u6548\u679c\u66f4\u597d\u7684\u8f93\u51fa\u503c\u3002\n\n3. \u6700\u540e\uff0c\u8fd9\u4e2anotebook\u5728\u5f88\u591a\u5730\u65b9\u90fd\u5236\u4f5c\u7684\u4e0d\u591f\u7cbe\u7ec6\uff0c\u89e3\u91ca\u7684\u4e5f\u4e0d\u591f\u5230\u4f4d\uff0c\u5bf9\u4e8e\u4f7f\u7528\u6a21\u578b\u7684\u539f\u56e0\u3001\u6a21\u578b\u4e0e\u672c\u7ade\u8d5b\u9898\u76ee\u7684\u8054\u7cfb\u7a0b\u5ea6\u7684\u89e3\u91ca\u4e0d\u591f\u6e05\u6670\uff0c\u53ef\u4ee5\u5728\u66f4\u52a0\u660e\u786e\u6a21\u578b\u53ca\u6a21\u578b\u53c2\u6570\u9009\u7528\u7684\u539f\u7406\u4e0e\u65b9\u6cd5\u540e\u8fdb\u884c\u6539\u826f\u3002\n\n### **\u6700\u540e\u7684\u6700\u540e\uff0c\u975e\u5e38\u611f\u8c22\u5f90\u8001\u5e08\u4e00\u5b66\u671f\u7684\u6559\u5b66\uff0c\u8ba9\u6211\u6536\u83b7\u9887\u4e30\uff0c\u5bf9\u6570\u636e\u79d1\u5b66\u6709\u4e86\u66f4\u6df1\u5165\u7684\u8ba4\u8bc6\uff0c\u4e5f\u6709\u4e86\u66f4\u5927\u7684\u5174\u8da3\u3002\u867d\u7136\u6709\u65f6\u5019\u6211\u4e0d\u592a\u80fd\u542c\u61c2\u5f90\u8001\u5e08\u7684\u8bfeqwq\uff0c\u4f46\u8001\u5e08\u4ecb\u7ecd\u7684\u5f88\u591a\u8d44\u6e90\u4e0e\u65b9\u6cd5\u80fd\u591f\u8ba9\u6211\u5728\u672a\u6765\u9047\u5230\u6311\u6218\u65f6\u627e\u5230\u89e3\u51b3\u7684\u9014\u5f84\uff0c\u5e0c\u671b\u4ee5\u540e\u8fd8\u80fd\u542c\u8001\u5e08\u7684\u8bfe\u5566\uff5e**\n\n\n\n\nbtw \u8001\u5e08\u5982\u679c\u80fd\u7ed9\u9ad8\u5206\u5c31\u66f4\u597d\u5566\u563b\u563b\n\n\n\n\n                                                                                                                \u8096\u9526\u5c9a\n                                                                                                              2020.6.22\n\n\n","16aea488":"**2. Elastic Net Regression(\u5f39\u6027\u7f51\u7edc\u6a21\u578b) **\n\n\u540c\u6837\u9700\u8981\u7a33\u5065\u5904\u7406","cdf268c8":"\u53ea\u6709\u201ctotal_\u5367\u5ba4\u201d\u7f3a\u5c11\u6570\u636e\uff0c\u5176\u767e\u5206\u6bd4\u4e3a1.02%\u3002\u4ece\u4e0b\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0ctotal_\u5367\u5ba4\u7684\u503c\u57281000\u52306500\u4e4b\u95f4\uff0c\u6240\u4ee5\u5c060\u586b\u5145\u5230\u7f3a\u5931\u7684\u503c\u662f\u4e0d\u5408\u7406\u7684\u3002\u7531\u4e8e\u7f3a\u5931\u503c\u7684\u767e\u5206\u6bd4\u8303\u56f4\u76f8\u5bf9\u8f83\u5c0f\uff0c\u6240\u4ee5\u76f4\u63a5\u5220\u9664\u5b83\u4eec\u3002","ff430fbf":"**3. Kernel Ridge Regression \uff08\u6838\u5cad\u56de\u5f52\uff09**","a7dd18f3":"**Averaged base models score**\n\n\u5bf9\u56db\u4e2a\u6a21\u578b\u8fdb\u884c\u5e73\u5747\uff1aENet, GBoost, KRR and lasso. \u540c\u65f6\u8fd9\u6837\u4e5f\u80fd\u66f4\u5bb9\u6613\u52a0\u5165\u66f4\u591a\u6a21\u578b\u3002","9cb57b39":"\u76ee\u6807\u53d8\u91cf\u662f\u53f3\u504f\u7684\u3002\u7531\u4e8e(\u7ebf\u6027)\u6a21\u578b\u559c\u6b22\u6b63\u6001\u5206\u5e03\u7684\u6570\u636e\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u8fd9\u4e2a\u53d8\u91cf\u8fdb\u884c\u53d8\u6362\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u6b63\u6001\u5206\u5e03\u3002\n\n\u7ecf\u8fc7\u8f6c\u6362\u540e\uff0c\u6b6a\u659c\u4f3c\u4e4e\u5f97\u5230\u4e86\u7ea0\u6b63\uff0c\u6570\u636e\u4f3c\u4e4e\u66f4\u7b26\u5408\u6b63\u6001\u5206\u5e03\u3002\n\n# 3. \u7279\u5f81\u5de5\u7a0b\n\n\u9996\u5148\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u8fde\u63a5\u5728\u540c\u4e00\u4e2a\u6570\u636e\u6846\u67b6\u4e2d","6a0cac54":"LightGBM:","53b63d48":"\u901a\u8fc7\u589e\u52a0\u5143\u6a21\u578b\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u66f4\u597d\u7684\u5206\u6570\u3002\n\n\u7ed3\u5408StackedRegressor, XGBoost\u548cLightGBM\uff0c\u6211\u4eec\u5c06XGBoost\u548cLightGBM\u6dfb\u52a0\u5230\u524d\u9762\u5b9a\u4e49\u7684StackedRegressor\u4e2d\u3002\n\n\u9996\u5148\u5b9a\u4e49\u4e00\u4e2armsle\u8bc4\u4f30\u51fd\u6570\uff1a","53b67902":"# 2. Explore the data","ea72a369":"## Base models scores\nFirstly, see how these base models perform on the data by evaluating the cross-validation rmsle error","d970aaf8":"\u68c0\u67e5\u5176\u4ed6\u76f8\u5173\u7684\u53d8\u91cf\uff0c\u6bd4\u5982 'median_income'","ae2e06b7":"# Modelling\nimport necessary libraries","1d29353f":"# <center>BNU ESL 2020 California Housing<\/center>\n<center>\u8096\u9526\u5c9a 201811030452<\/center>\n<center>2020.6.23<\/center>","51f87b01":"**Stacking Averaged models Score \u8fd9\u4e2a\u6a21\u578b\u7684\u5f97\u5206**\n\n\u4e3a\u4e86\u4f7f\u4e24\u79cd\u65b9\u6cd5\u5177\u6709\u53ef\u6bd4\u6027(\u901a\u8fc7\u4f7f\u7528\u76f8\u540c\u6570\u91cf\u7684\u6a21\u578b)\uff0c\u6211\u4eec\u53ea\u662f\u5e73\u5747Enet KRR\u548cGboost\uff0c\u7136\u540e\u6211\u4eec\u6dfb\u52a0lasso\u4f5c\u4e3a\u5143\u6a21\u578b\u3002","6e8dd6ff":"## Basic model\n\n\u5bf9\u4e8e\u6bcf\u4e2a\u6a21\u578b\u7684\u53c2\u6570\u9009\u62e9\uff0c\u6211\u4eec\u53ea\u662f\u9009\u62e9\u4e00\u4e9b\u7ecf\u5e38\u4f7f\u7528\u7684\u53c2\u6570\uff0c\u5e76\u5c1d\u8bd5\u7528\u4e0d\u540c\u7684\u53c2\u6570\u8fdb\u884c\u5efa\u6a21\uff0c\u6700\u7ec8\u9009\u62e9\u6700\u597d\u7684\u4e00\u4e2a\u3002\u540c\u65f6\uff0c\u4e4b\u524d\u6211\u4eec\u5df2\u7ecf\u63d0\u5230\u6a21\u578b\u9700\u8981\u662f\u7a33\u5065\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u4f7f\u7528\u4e00\u4e9b\u65b9\u6cd5\u6765\u6700\u5c0f\u5316\u6bcf\u4e2a\u6a21\u578b\u7684\u8bef\u5dee\u3002\n\n\n**1.Lasso Regression:**\n\n\u8fd9\u4e2a\u6a21\u578b\u53ef\u80fd\u5bf9\u5f02\u5e38\u503c\u975e\u5e38\u654f\u611f\u3002\u6240\u4ee5\u6211\u4eec\u9700\u8981\u8ba9\u5b83\u5bf9\u4ed6\u4eec\u66f4\u6587\u4ef6\u5927\u5c0f\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5728lasso\u4e2d\u4f7f\u7528sklearn\u7684robustscaler()\u65b9\u6cd5","9ab5eeff":"This stacking approach isn't significantly improve the score.\n\u66f4\u590d\u6742\u4e00\u4e9b\u7684 Stacking : Adding a Meta-model\uff0c\u5373\u589e\u52a0\u4e00\u4e2a\u5143\u6a21\u578b\n\nIn this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\nThe procedure, for the training part, may be described as follows:\n\n1. \u5c06\u8bad\u7ec3\u53ca\u5212\u5206\u4e3a\u4e24\u4e2a\u65e0\u4ea4\u96c6\u7684\u96c6\u5408\u3002\n\n2. \u5728\u8bad\u7ec3\u96c6\u7684\u7b2c\u4e00\u90e8\u5206\u8bad\u7ec3\u57fa\u7840\u7684\u6a21\u578b\n\n3. \u7528\u7b2c\u4e8c\u90e8\u5206\u6d4b\u8bd5\u6a21\u578b\n\n4. \u7528\u7b2c\u4e8c\u90e8\u5206\u7684\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u8f93\u5165\uff0c\u8bad\u7ec3\u66f4\u9ad8\u5c42\u7ea7\u7684\u6a21\u578b\uff0c\u5373meta-model\n\n\u5176\u4e2d\uff0c\u524d\u4e09\u6b65\u662f\u8fed\u4ee3\u5b8c\u6210\u7684\u3002\u5982\u679c\u6211\u4eec\u4ee55\u500d\u53e0\u52a0\u4e3a\u4f8b\uff0c\u6211\u4eec\u9996\u5148\u5c06\u8bad\u7ec3\u6570\u636e\u5206\u62105\u500d\u3002\u7136\u540e\u6211\u4eec\u5c06\u8fdb\u884c5\u6b21\u8fed\u4ee3\u3002\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u6211\u4eec\u57284\u4e2a\u6298\u53e0\u4e0a\u8bad\u7ec3\u6bcf\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5bf9\u5269\u4f59\u7684\u6298\u53e0(\u575a\u6301\u6298\u53e0)\u8fdb\u884c\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u57285\u6b21\u8fed\u4ee3\u4e4b\u540e\uff0c\u6211\u4eec\u5c06\u786e\u4fe1\uff0c\u6574\u4e2a\u6570\u636e\u5c06\u88ab\u7528\u6765\u5f97\u5230\u8d85\u51fa\u6298\u53e0\u7684\u9884\u6d4b\uff0c\u7136\u540e\u6211\u4eec\u5c06\u5728\u6b65\u9aa44\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u9884\u6d4b\u4f5c\u4e3a\u65b0\u7684\u7279\u5f81\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u5143\u6a21\u578b\u3002\n\n\u5728\u9884\u6d4b\u90e8\u5206\uff0c\u6211\u4eec\u5c06\u6240\u6709\u7684\u57fa\u7840\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u9884\u6d4b\u5e73\u5747\u503c\u4f5c\u4e3a\u5143\u7279\u5f81\uff0c\u7136\u540e\u7528\u5143\u6a21\u578b\u8fdb\u884c\u6700\u7ec8\u7684\u9884\u6d4b\u3002\n\n\u5728\u8fd9\u91cc\uff0c\u57fa\u7840\u6a21\u578b\u4e3a\u7b97\u6cd50\u30011\u30012\uff0c\u5143\u6a21\u578b\u4e3a\u7b97\u6cd53\u3002\u6574\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\u4e3aA+B(\u76ee\u6807\u53d8\u91cfy\u5df2\u77e5)\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u5206\u4e3a\u8bad\u7ec3\u90e8\u5206(A)\u548c\u575a\u6301\u90e8\u5206(B)\uff0c\u6d4b\u8bd5\u6570\u636e\u96c6\u4e3aC\u3002\n\nB1(\u6765\u81ea\u8bad\u7ec3\u90e8\u5206\u7684\u9884\u6d4b)\u662f\u7528\u4e8e\u8bad\u7ec3\u5143\u6a21\u578b3\u7684\u65b0\u7279\u5f81\uff0cC1(\u6765\u81ea\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u9884\u6d4b)\u662f\u8fdb\u884c\u6700\u7ec8\u9884\u6d4b\u7684\u5143\u7279\u5f81\u3002\n\n\nStacking averaged Models Class","c5cd7040":"XGBoost:","8ef90585":"6. LightGBM \uff08from the example in introduction of LightGBM website\uff09","2d327422":"population\u5927\u4e8e15000\u7684\u662f\u79bb\u7fa4\u70b9\uff0c\u5220\u9664","794678c2":"## 1. Import necessary libraries and data and do some basic data cleaning","082d5620":"**5. XGBoost **\n\ncolsample_bytree\uff1a\u9009\u62e9\u6a21\u578b\u662f\u57fa\u4e8e\u6811\u7684\u6a21\u578b\n\nsilent=1\uff1a\u53ef\u4ee5\u8f93\u51fa\u4fe1\u606f\uff0c\u4e00\u822csilent\u662f0\uff0c\u9759\u9ed8\u6a21\u578b\n\nmin_child_weight\uff1a\u51b3\u5b9a\u6700\u5c0f\u53f6\u5b50\u8282\u70b9\u6837\u672c\u6743\u91cd\u548c\u3002 \u548cGBM\u7684 min_child_leaf \u53c2\u6570\u7c7b\u4f3c\uff0c\u4f46\u4e0d\u5b8c\u5168\u4e00\u6837\u3002XGBoost\u7684\u8fd9\u4e2a\u53c2\u6570\u662f\u6700\u5c0f\u6837\u672c\u6743\u91cd\u7684\u548c\uff0c\u800cGBM\u53c2\u6570\u662f\u6700\u5c0f\u6837\u672c\u603b\u6570\u3002 \u8fd9\u4e2a\u53c2\u6570\u7528\u4e8e\u907f\u514d\u8fc7\u62df\u5408\u3002\u5f53\u5b83\u7684\u503c\u8f83\u5927\u65f6\uff0c\u53ef\u4ee5\u907f\u514d\u6a21\u578b\u5b66\u4e60\u5230\u5c40\u90e8\u7684\u7279\u6b8a\u6837\u672c\u3002 \u4f46\u662f\u5982\u679c\u8fd9\u4e2a\u503c\u8fc7\u9ad8\uff0c\u4f1a\u5bfc\u81f4\u6b20\u62df\u5408\u3002\n\nmax_depth=3\uff1a\u6811\u7684\u6700\u5927\u6df1\u5ea6\uff0c\u907f\u514d\u8fc7\u62df\u5408\n\n\ngamma=0.047\uff1aGamma\u6307\u5b9a\u4e86\u8282\u70b9\u5206\u88c2\u6240\u9700\u7684\u6700\u5c0f\u635f\u5931\u51fd\u6570\u4e0b\u964d\u503c\u3002 \u8fd9\u4e2a\u53c2\u6570\u7684\u503c\u8d8a\u5927\uff0c\u7b97\u6cd5\u8d8a\u4fdd\u5b88\uff0c\u4e0e\u635f\u5931\u51fd\u6570\u76f8\u5173\u3002\n\nsubsample=0.52\uff1a\u63a7\u5236\u5bf9\u4e8e\u6bcf\u68f5\u6811\uff0c\u968f\u673a\u91c7\u6837\u7684\u6bd4\u4f8b\u3002 \u51cf\u5c0f\u8fd9\u4e2a\u53c2\u6570\u7684\u503c\uff0c\u7b97\u6cd5\u4f1a\u66f4\u52a0\u4fdd\u5b88\uff0c\u907f\u514d\u8fc7\u62df\u5408\u3002\u4f46\u662f\uff0c\u5982\u679c\u8fd9\u4e2a\u503c\u8bbe\u7f6e\u5f97\u8fc7\u5c0f\uff0c\u5b83\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6b20\u62df\u5408\u3002 \u5178\u578b\u503c\uff1a0.5-1\n\nreg_lambda=0.85\uff1a\u6743\u91cd\u7684L1\u6b63\u5219\u5316\u9879\u3002(\u548cLasso regression\u7c7b\u4f3c)\u3002 \u53ef\u4ee5\u5e94\u7528\u5728\u5f88\u9ad8\u7ef4\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u5f97\u7b97\u6cd5\u7684\u901f\u5ea6\u66f4\u5feb\u3002","be861398":"4. Gradient Boosting Regression\uff08\u68af\u5ea6\u63d0\u5347\u7b97\u6cd5\uff09 : \n\n\u4f7f\u7528huber\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u5b83\u5bf9\u5f02\u5e38\u503c\u7a33\u5065\u3002\n\n**\u5c06learning_rate\u8bbe\u4e3a0.05:**  \u7ed8\u5236\u6210\u672c\u51fd\u6570\u4e0e\u8fed\u4ee3\u6b21\u6570\u7684\u5173\u7cfb\u3002\u5f53\u5b66\u4e60\u7387\u8fc7\u5927\u65f6\uff0c\u6210\u672c\u51fd\u6570\u53ef\u80fd\u4f1a\u4e0a\u5347\u800c\u4e0d\u662f\u4e0b\u964d\u3002\u6240\u4ee5\u6211\u4eec\u9700\u8981\u7f29\u5c0f\u5b83\u7684\u8303\u56f4\uff0c\u4f46\u662f\u5982\u679c\u5b83\u592a\u5c0f\uff0c\u4ee3\u4ef7\u51fd\u6570\u5c06\u4f1a\u975e\u5e38\u7f13\u6162\u5730\u6536\u655b\u3002\u6700\u540e\u9009\u62e9\u5b66\u4e60\u7387\u4f7f\u4ee3\u4ef7\u51fd\u6570\u4e0b\u964d\u5e76\u4ee5\u6b63\u5e38\u901f\u5ea6\u6536\u655b\u3002\n\n\u8282\u70b9\u6700\u5c0f\u6837\u672c\u6570\u76ee\uff0c\u4e0d\u5e73\u8861\u5206\u7c7b\u95ee\u9898\u9700\u8981\u8bbe\u7f6e\u8f83\u4f4e\u7684\u503c\uff0c\u9632\u6b62\u6700\u5c0f\u7c7b\u522b\u843d\u5165\u6700\u5927\u7c7b\u522b\u533a\u57df\u5185\u3002\n\n\u6700\u5c0f\u5212\u5206\u6837\u672c\u6570\u76ee\uff1a\u6570\u76ee\u4e0d\u80fd\u8fc7\u9ad8\u5426\u5219\u5bb9\u6613\u8fc7\u62df\u5408\u3002\n\n","062e0ad0":"## Ensemble prediction","a00d4c06":"\u4ece\u8fd9\u4e2a\u56fe\u4e2d\u6211\u4eec\u770b\u4e0d\u5230\u4e2d\u503c\u6536\u5165\u548c\u4e2d\u503c\u4f4f\u623f\u4ef7\u503c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u6240\u4ee5\u5220\u9664\u4e00\u4e9b\u5947\u602a\u7684\u6570\u636e\u662f\u4e0d\u660e\u667a\u7684\u3002\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u8ba9\u6211\u4eec\u7684\u6a21\u578b\u5728\u5b83\u4eec\u4e4b\u4e0a\u53d8\u5f97\u7a33\u5065\uff0c\u800c\u4e0d\u662f\u5c06\u5b83\u4eec\u5168\u90e8\u91cd\u65b0\u6620\u5c04\u3002\n\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u63a2\u7d22\u66f4\u591a\u7684\u53d8\u91cf\u3002\u6c42\u51fa\u6240\u6709\u6570\u503c\u53d8\u91cf\uff0c\u5e76\u7ed8\u5236\u6563\u70b9\u56fe\u3002"}}