{"cell_type":{"438ce892":"code","cbb20393":"code","da7af98b":"code","15caba52":"code","1bde05e3":"code","628af258":"code","60cd115e":"code","f5b1f668":"code","8cb6ef46":"code","b5a829a2":"code","614bc874":"code","16e6e43f":"code","893ae896":"code","85f6f8dd":"code","e9b680ad":"code","93f16849":"code","55d56173":"code","45769095":"code","0dddeca3":"code","0fa65c06":"code","d1858904":"code","23421846":"code","fd4ea016":"code","f5bd1c95":"code","09a9f307":"code","f4b89f63":"code","b069beac":"code","b7b2eede":"code","4f20a2dc":"code","0230d294":"code","85475f55":"code","f85a75e9":"code","0c5282fe":"code","900eb2c7":"code","266de343":"code","2df0d202":"code","60cdb9b7":"code","c91c5610":"code","f722f3b7":"code","e1319de8":"code","bfa06891":"code","52475e51":"code","6215db5a":"code","0571af0d":"code","32568409":"markdown","4a500b16":"markdown","9fdeaf9c":"markdown","17882e18":"markdown","e930e8f7":"markdown","0bebb373":"markdown","502c4831":"markdown","8a6705e5":"markdown","c1e55a83":"markdown","be7a4c35":"markdown","fa175410":"markdown","a31d813d":"markdown","ccb87acb":"markdown","a5720741":"markdown","f38ab919":"markdown","d9717bdd":"markdown","652ba77f":"markdown","22ab98b0":"markdown","ac934441":"markdown"},"source":{"438ce892":"import numpy as np \nimport pandas as pd \nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter \nimport seaborn as sns\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cbb20393":"train= pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/train.csv\")\ntest = pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/test.csv\")\nsubmission_csv = pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/sample_submission.csv\")\n\n\nall_data = pd.concat([train, test], axis=0)\nall_data = all_data.drop([\"ID\"], axis=1)\nprint(all_data.info())","da7af98b":"target_values = train[\"TARGET\"].value_counts(dropna = False)\nprint(target_values)\n\nlabels=\"Nan Fraud\",\"Fraud\"\nsizes = target_values\nexplode = (0, 0.3)\n\nfig1, ax1 = plt.subplots(figsize=(22,10))\n\n\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.3f%%',\n        shadow=True, startangle=50, colors=[\"#D6ABE3\", \"#DBE921\"],\n        labeldistance=1.1, pctdistance=0.3, textprops={'fontsize': 22, 'color':\"black\"})\n\nax1.set_title(\"Fraud i\u015flem Oran\u0131\", color=\"red\", size=25)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.show()","15caba52":"columns = [\"CST_NR\", \"CC_NR\",\"TXN_SOURCE\",\"TXN_TRM\",\"TXN_ENTRY\",\"CITY\",\"COUNTRY\",\"MC_NAME\",\"MC_ID\",\"MCC_CODE\"]\nlens = []\nfor i in columns:\n    len_columns = len(train[i].value_counts())\n    lens.append(len_columns)\n    \n    \nprint(lens)\nplt.figure(figsize=(20,10))\nplt.bar(columns, lens, width=0.8, align='center')\nplt.show()","1bde05e3":"txn = train[\"TXN_TIME\"].value_counts()\n#dakika cinsinden 1440 tane cins var bunlar\u0131 saat olarak ay\u0131rabilirz yada hi\u00e7 ellemeyebiliriz.\n#hangi saatler aras\u0131 fraud i\u015flemi olmu\u015f bunun grafi\u011fini \u00e7izdirebiliriz.\nplt.style.use('seaborn')\nplt.figure(figsize=(20,15))\nx_axes = txn.index\nbars = train[\"TXN_TIME\"].value_counts()[:]\nplt.stem(x_axes,bars)\nplt.show()","628af258":"fig, ax = plt.subplots(1, 2, figsize=(22,10))\namount_val = train['TXN_AMNT'].values\ntime_val = train[\"TXN_TIME\"].values\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\nplt.show()","60cd115e":"#all_data[\"TXN_TIME\"] = pd.cut(x.index,24).astype(str)\nall_data[\"TXN_TIME\"] = all_data[\"TXN_TIME\"].astype(str)\nt\u0131me = all_data.iloc[:,5:6].values\nle = LabelEncoder()\nall_data[\"TXN_TIME\"] = le.fit_transform(t\u0131me[:,0])\nall_data[\"TXN_TIME\"] = all_data[\"TXN_TIME\"].astype(int)\n","f5b1f668":"contry_grp = train.groupby([\"COUNTRY\"])[\"TARGET\"].mean()\nprint(contry_grp.mean(axis=0))  #0.06886717738480358 sekt\u00f6rlerin target oran\u0131\n\nall_data = all_data.merge(contry_grp, how=\"left\", on=\"COUNTRY\")\nall_data = all_data.rename(columns={\"TARGET_y\":\"fraud_mean_country\"})\n\nall_data[\"more_fraud_ratio_country\"] = [1 if i> 0.0688*1.2 else 0 for i in all_data[\"fraud_mean_country\"]]\nall_data[\"more_fraud_ratio_country\"].value_counts()","8cb6ef46":"mcc_grp = train.groupby([\"MCC_CODE\"])[\"TARGET\"].mean()\nprint(mcc_grp.mean(axis=0)) \nall_data = all_data.merge(mcc_grp, how=\"left\", on=\"MCC_CODE\")\nall_data = all_data.rename(columns={\"TARGET\":\"fraud_mean_mcc\"})\nall_data[\"more_fraud_ratio_mcc\"] = [1 if i> 0.03187*1.2 else 0 for i in all_data[\"fraud_mean_mcc\"]] #bunu y\u00fckseltirsen nolur\nall_data[\"more_fraud_ratio_mcc\"].value_counts()","b5a829a2":"c\u0131ty_grp = train.groupby([\"CITY\"])[\"TARGET\"].mean()\nprint(c\u0131ty_grp.mean(axis=0)) \nall_data = all_data.merge(c\u0131ty_grp, how=\"left\", on=\"CITY\")\nall_data = all_data.rename(columns={\"TARGET\":\"fraud_mean_c\u0131ty\"})\nall_data[\"more_fraud_ratio_c\u0131ty\"] = [1 if i> 0.0590*1.2 else 0 for i in all_data[\"fraud_mean_c\u0131ty\"]]\nall_data[\"more_fraud_ratio_c\u0131ty\"].value_counts()\nall_data = all_data.rename(columns={\"TARGET_x\":\"TARGET\"})","614bc874":"print(all_data[\"TXN_TRM\"].value_counts())\nprint(\"TXN_TRM kolonu b\u00fct\u00fcn data i\u00e7in Tek bir de\u011fi\u015fkene sahiptir. Modele etkisi olmad\u0131\u011f\u0131n\u0131 d\u00fc\u015f\u00fcnd\u00fc\u011f\u00fcm i\u00e7in silinmi\u015ftir.\")\nall_data = all_data.drop([\"TXN_TRM\"], axis=1)","16e6e43f":"all_data[\"TXN_SOURCE\"].replace({\"O\":0,\"B\":1,\"V\":2,\"M\":3,\"R\":0}, inplace = True)\nkaynak = all_data[\"TXN_SOURCE\"].value_counts()\nprint(kaynak)\nlabels=\"\u0130\u015flem Kayna\u011f\u0131 1\",\"\u0130\u015flem Kayna\u011f\u0131 2\",\"\u0130\u015flem Kayna\u011f\u0131 3\",\"\u0130\u015flem Kayna\u011f\u0131 4\"\nsizes = kaynak\nexplode = (0.1, 0.1,0.1,0.1)\n\nfig1, ax1 = plt.subplots(figsize=(22,10))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.3f%%',\n        shadow=True, startangle=50, colors=[\"#D6ABE3\", \"#DBE921\",\"red\",\"blue\"],\n        labeldistance=1.1, pctdistance=0.3, textprops={'fontsize': 22, 'color':\"black\"})\nax1.set_title(\"\u0130\u015flem Kaynaklar\u0131n\u0131n mevcut datadaki oranlar\u0131\", color=\"red\", size=25)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.show()","893ae896":"#Do\u011frulama y\u00f6ntemlerinin fraud oran\u0131\ndogrulama_fraud_ratio = all_data.groupby([\"TXN_SOURCE\"])[\"TARGET\"].mean()\nprint(dogrulama_fraud_ratio)\n\nheight = dogrulama_fraud_ratio.values\nbars = [\"\u0130\u015flem Kayna\u011f\u0131 1\",\"\u0130\u015flem Kayna\u011f\u0131 2\",\"\u0130\u015flem Kayna\u011f\u0131 3\",\"\u0130\u015flem Kayna\u011f\u0131 4\"]\nx_pos = np.arange(len(bars))\n\nplt.figure(figsize=(22,10))\nplt.title(\"\u0130\u015flem Kayna\u011f\u0131 Tiplerinin Fraud \u0130\u015flem Ortalamas\u0131\", color=\"red\")\nplt.bar(x_pos, height, color=(0.4, 0.5, 0.6, 0.7),  edgecolor='blue')\nplt.xticks(x_pos, bars)\nplt.ylabel(\"Fraud i\u015flem ortalamas\u0131\",fontsize=16, color=\"purple\")\nplt.tick_params(colors='black', which='both')\nplt.show()","85f6f8dd":"all_data[\"TXN_ENTRY\"].replace({\"12d7720f7273e2a1cfb2adf5daba868b767db6281b34b312c4a4\":0,\n                 \"12b4164904d6ecac8163670f59dc63330075d27d1d191a4219b2\":1,\n                \"34d600ed59b9dee0ed2ebefcfc24d306be443c0d04b9ae276364\":2,\n                \"27f9157f828401e4cecd7046da52d6ecf6c6623fc677c1d803ec\":0}, inplace=True)\ndogrulama = all_data[\"TXN_ENTRY\"].value_counts(dropna = False)\nprint(dogrulama)\n\nlabels=\"Do\u011frulama Y\u00f6ntemi 1\",\"Do\u011frulama Y\u00f6ntemi 2\",\"Do\u011frulama Y\u00f6ntemi 3\"\nsizes = dogrulama\nexplode = (0.1, 0.1,0.4)\n\nfig1, ax1 = plt.subplots(figsize=(22,10))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.3f%%',\n        shadow=True, startangle=50, colors=[\"#D6ABE3\", \"#DBE921\",\"red\"],\n        labeldistance=1.1, pctdistance=0.3, textprops={'fontsize': 22, 'color':\"black\"})\n\nax1.set_title(\"Do\u011frulama Y\u00f6ntemlerinin mevcut datadaki oranlar\u0131\", color=\"red\", size=25)\nax1.axis('equal')\nplt.show()","e9b680ad":"#Do\u011frulama y\u00f6ntemlerinin fraud oran\u0131\ndogrulama_fraud_ratio = all_data.groupby([\"TXN_ENTRY\"])[\"TARGET\"].mean()\nprint(dogrulama_fraud_ratio)\n\nheight = dogrulama_fraud_ratio.values\nbars = [\"Do\u011frulama y\u00f6ntemi 1\",\"Do\u011frulama y\u00f6ntemi 2\",\"Do\u011frulama y\u00f6ntemi 3\"]\nx_pos = np.arange(len(bars))\n\nplt.figure(figsize=(22,10))\nplt.title(\"Do\u011frulama Y\u00f6ntemlerinin Fraud \u0130\u015flem Ortalamas\u0131\", color=\"red\")\nplt.bar(x_pos, height, color=(0.4, 0.5, 0.6),  edgecolor='blue')\nplt.xticks(x_pos, bars)\nplt.ylabel(\"Fraud i\u015flem ortalamas\u0131\",fontsize=16, color=\"purple\")\nplt.tick_params(colors='black', which='both')\nplt.show()","93f16849":"all_data[\"total_number_ratio\"] = all_data[\"more_fraud_ratio_mcc\"] + all_data[\"more_fraud_ratio_country\"] + all_data[\"more_fraud_ratio_c\u0131ty\"]","55d56173":"train.groupby([\"MC_ID\"])[\"TARGET\"].mean().sort_values().loc[lambda x: x>0.3].value_counts()\n#MCC IDLER\u0130 TUTMAK YER\u0130NE OLASILIK ORANLARINA G\u00d6RE KATEGOR\u0130C VER\u0130 YAPILAB\u0130L\u0130R.","45769095":"df = train[\"TXN_AMNT\"].describe().to_frame()\ntrainfraud = train[train[\"TARGET\"]==1][\"TXN_AMNT\"]\ntrain_nanf = train[train[\"TARGET\"]==0][\"TXN_AMNT\"]\n\nmean_fraud = trainfraud.mean()\nmean_nanfraud = train_nanf.mean()\nmax_fraud = trainfraud.max()\nmax_nanfraud = train_nanf.max()\nmin_fraud = trainfraud.min()\nmin_nanfraud = train_nanf.min()\nstd_fraud = trainfraud.std()\nstd_nanfraud = train_nanf.std()\n\nlabels = [\"mean\", \"max\",\"min\",\"std\"]\nfraud_inf=[mean_fraud,max_fraud,min_fraud,std_fraud]\nnanfraud_inf=[mean_nanfraud,max_nanfraud,min_nanfraud,std_nanfraud]\nprint(\"Fraud Data Statistical Information; \\nMean:{} \\nMax:{} \\nMin:{} \\nStd:{}\".format(fraud_inf[0],\n                                                                                        fraud_inf[1],\n                                                                                        fraud_inf[2],\n                                                                                        fraud_inf[3]))\nprint(\"*\"*100)\nprint(\"Non Fraud Data Statistical Information; \\nMean:{} \\nMax:{} \\nMin:{} \\nStd:{}\".format(nanfraud_inf[0],\n                                                                                            nanfraud_inf[1],\n                                                                                            nanfraud_inf[2],\n                                                                                            nanfraud_inf[3]))\n","0dddeca3":"plt.style.use('ggplot')\nwidth = 0.45\nx = np.arange(len(labels)) \n\nfig, ax = plt.subplots(figsize=(14,7))\nrects1 = ax.bar(x - width\/2, fraud_inf, width, label='Fraud')\nrects2 = ax.bar(x + width\/2, nanfraud_inf, width, label='NAN Fraud')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Scores')\nax.set_title('Statistical Information About Transaction Amounts')\nax.title.set_color('red')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\nfig.tight_layout()\nplt.show()","0fa65c06":"x = pd.DataFrame(all_data[\"CITY\"].value_counts().loc[lambda x: x>30])\nx = x.reset_index()\nx = x.rename(columns={\"CITY\":\"value_counts\", \"index\":\"CITY\"})\n\nall_data = all_data.merge(x, on=\"CITY\", how=\"left\")\nall_data[\"value_counts\"] = all_data[\"value_counts\"].fillna(0)\nall_data.loc[all_data['value_counts'] == 0, \"CITY\"] = 0\nall_data = all_data.drop([\"value_counts\"],axis=1)\n\nall_data[\"CITY\"] = all_data[\"CITY\"].astype(str)\nc\u0131ty = all_data.iloc[:,9:10].values\nle = LabelEncoder()\nall_data[\"CITY\"] = le.fit_transform(c\u0131ty[:,0])\nprint(\"i\u00e7erdi\u011fi data say\u0131s\u0131 30 dan k\u00fc\u00e7\u00fck olan CITY de\u011fi\u015fkenlerini tek bir de\u011ferde toplamak i\u00e7in yukar\u0131daki i\u015flemler yap\u0131ld\u0131\")\nprint(\"Bu \u015fekilde 2143 olan CITY de\u011fi\u015fkenleri 175'e d\u00fc\u015f\u00fcr\u00fcld\u00fc. B\u00f6ylelikle daha genellenebilir bir yap\u0131 olu\u015fturulmu\u015f olundu\")","d1858904":"x = pd.DataFrame(all_data[\"COUNTRY\"].value_counts().loc[lambda x: x>30])\nx = x.reset_index()\nx = x.rename(columns={\"COUNTRY\":\"value_counts\", \"index\":\"COUNTRY\"})\n\nall_data = all_data.merge(x, on=\"COUNTRY\", how=\"left\")\nall_data[\"value_counts\"] = all_data[\"value_counts\"].fillna(0)\nall_data.loc[all_data['value_counts'] == 0, \"COUNTRY\"] = 0\nall_data = all_data.drop([\"value_counts\"],axis=1)\n           \nall_data[\"COUNTRY\"] = all_data[\"COUNTRY\"].astype(str)\ncountry = all_data.iloc[:,10:11].values\nall_data[\"COUNTRY\"] = le.fit_transform(country[:,0])\nprint(\"i\u00e7erdi\u011fi data say\u0131s\u0131 30 dan k\u00fc\u00e7\u00fck olan COUNTRY de\u011fi\u015fkenlerini tek bir de\u011ferde toplamak i\u00e7in yukar\u0131daki i\u015flemler yap\u0131ld\u0131\")\nprint(\"Bu \u015fekilde 80 olan COUNTRY de\u011fi\u015fkenleri 28'ye d\u00fc\u015f\u00fcr\u00fcld\u00fc. B\u00f6ylelikle daha genellenebilir bir yap\u0131 olu\u015fturulmu\u015f olundu\")","23421846":"#CC_NR maskelenmi\u015f kart no\n\n\"\"\"x = pd.DataFrame(all_data[\"CC_NR\"].value_counts().loc[lambda x: x>5])\nx = x.reset_index()\nx.rename(columns={\"CC_NR\":\"value_counts\", \"index\":\"CC_NR\"}, inplace=True)\n\nall_data = all_data.merge(x, on=\"CC_NR\", how=\"left\")\nall_data[\"value_counts\"] = all_data[\"value_counts\"].fillna(0)\nall_data.loc[all_data['value_counts'] == 0, \"CC_NR\"] = 0\nall_data = all_data.drop([\"value_counts\"],axis=1)\nprint(all_data.info())\n\nall_data[\"CC_NR\"] = all_data[\"CC_NR\"].astype(str)\"\"\"\ncc_nr = all_data.iloc[:,2:3].values\nall_data[\"CC_NR\"] = le.fit_transform(cc_nr[:,0])\n","fd4ea016":"\"\"\"cst_nr = train.groupby([\"MC_ID\"])[\"TARGET\"].mean()\na = cst_nr.mean(axis=0)\nall_data = all_data.merge(cst_nr, how=\"left\", on=\"MC_ID\")\nprint(all_data.info())\n\nall_data = all_data.rename(columns={\"TARGET_y\":\"fraud_mean_mc_\u0131d\"})\nall_data[\"more_MC_fraud\"] = [1 if i> a*2 else 0 for i in all_data[\"fraud_mean_mc_\u0131d\"]]\nall_data[\"more_MC_fraud\"].value_counts()\nall_data = all_data.rename(columns={\"TARGET_x\":\"TARGET\"})\"\"\"\n","f5bd1c95":"#CST_NR maskelenmi\u015f m\u00fc\u015fteri no\n\n\"\"\"x = pd.DataFrame(all_data[\"CST_NR\"].value_counts().loc[lambda x: x>5])\nx = x.reset_index()\nx.rename(columns={\"CST_NR\":\"value_counts\", \"index\":\"CST_NR\"}, inplace=True)\n\nall_data = all_data.merge(x, on=\"CST_NR\", how=\"left\")\nall_data[\"value_counts\"] = all_data[\"value_counts\"].fillna(0)\nall_data.loc[all_data['value_counts'] == 0, \"CST_NR\"] = 0\nall_data = all_data.drop([\"value_counts\"],axis=1)\nprint(all_data.info())\n\nall_data[\"CST_NR\"] = all_data[\"CST_NR\"].astype(str)\"\"\"\ncst_nr = all_data.iloc[:,1:2].values\nall_data[\"CST_NR\"]= le.fit_transform(cst_nr[:,0])\nprint(all_data[\"CST_NR\"].value_counts())","09a9f307":"x = pd.DataFrame(all_data[\"MCC_CODE\"].value_counts().loc[lambda x: x>20])\nx = x.reset_index()\nx  = x .rename(columns={\"MCC_CODE\":\"value_counts\", \"index\":\"MCC_CODE\"})\n\nall_data = all_data.merge(x, on=\"MCC_CODE\", how=\"left\")\nall_data[\"value_counts\"] = all_data[\"value_counts\"].fillna(0) #hi\u00e7birinde olmayan bir de\u011fer olarak 0 se\u00e7ildi.\nall_data.loc[all_data['value_counts'] == 0, \"MCC_CODE\"] = 0\nall_data = all_data.drop([\"value_counts\"],axis=1)\n\nall_data[\"MCC_CODE\"] = all_data[\"MCC_CODE\"].astype(str)\ncc_nr = all_data.iloc[:,13:14].values\nle = LabelEncoder()\nall_data[\"MCC_CODE\"] = le.fit_transform(cc_nr[:,0])\nprint(\"ilgili kolonun 30 de\u011ferinin alt\u0131nda olan MCC_CODE de\u011fi\u015fkeni 'di\u011fer' olarak s\u0131n\u0131fland\u0131r\u0131ld\u0131.\")\nprint(\"239 farkl\u0131 sekt\u00f6r 149'e d\u00fc\u015f\u00fcr\u00fcld\u00fc.\")\n#Her sekt\u00f6r\u00fcn ka\u00e7ak\u00e7\u0131l\u0131k oran\u0131","f4b89f63":"x = pd.DataFrame(all_data[\"MC_ID\"].value_counts().loc[lambda x: x>5])\nx = x.reset_index()\nx  = x .rename(columns={\"MC_ID\":\"value_counts\", \"index\":\"MC_ID\"})\nall_data = all_data.merge(x, on=\"MC_ID\", how=\"left\")\nall_data[\"value_counts\"] = all_data[\"value_counts\"].fillna(0)\nall_data = all_data.drop([\"MC_ID\"], axis=1)\nall_data = all_data .rename(columns={\"value_counts\":\"MC_ID\"})\nmcc_code = all_data.iloc[:,20:21].values\nle = LabelEncoder()\nall_data[\"MC_ID\"] = le.fit_transform(mcc_code[:,0])\n\nprint(\"MC_ID 5 den k\u00fc\u00e7\u00fck dataya ahip olanlar tek bir veri de toplanarak, obje say\u0131s\u0131 7454'den 364 e d\u00fc\u015f\u00fcr\u00fclm\u00fc\u015ft\u00fcr.\")\n#Bu k\u0131s\u0131m modelden \u00e7\u0131kar\u0131lmal\u0131 modeli d\u00fc\u015f\u00fcrd\u00fc bu haliyle.","b069beac":"x = pd.DataFrame(all_data[\"MC_NAME\"].value_counts().loc[lambda x: x>5])\nx = x.reset_index()\nx  = x .rename(columns={\"MC_NAME\":\"value_counts\", \"index\":\"MC_NAME\"})\n\nall_data = all_data.merge(x, on=\"MC_NAME\", how=\"left\")\nall_data[\"value_counts\"] = all_data[\"value_counts\"].fillna(0) #hi\u00e7birinde olmayan bir de\u011fer olarak 0 se\u00e7ildi.\nall_data.loc[all_data['value_counts'] == 0, \"MC_NAME\"] = 0\nall_data = all_data.drop([\"value_counts\"],axis=1)\n\nall_data[\"MC_NAME\"] = all_data[\"MC_NAME\"].astype(str)\ncc_nr = all_data.iloc[:,11:12].values\nle = LabelEncoder()\nall_data[\"MC_NAME\"] = le.fit_transform(cc_nr[:,0])","b7b2eede":"corr = all_data.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.set(style=\"dark\")\nf, ax = plt.subplots(figsize=(18, 12))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.4, center=0,\n            square=True, linewidths=.8, cbar_kws={\"shrink\": .5}, annot=True ,fmt='.1%')","4f20a2dc":"train = all_data.iloc[:607507,]\ntest = all_data.iloc[607507:,1:]\n\nfraud_data = train[train[\"TARGET\"]==1].drop([\"TARGET\"], axis=1)\nnan_fraud_data = train[train[\"TARGET\"]==0].drop([\"TARGET\"], axis=1)","0230d294":"MinMaxScaler = MinMaxScaler()\nfraud_data_minm = MinMaxScaler.fit_transform(fraud_data)\nnan_fraud_data_minm = MinMaxScaler.fit_transform(nan_fraud_data)\n\nrobust = RobustScaler()\nfraud_data_rob = robust.fit_transform(fraud_data)\nnan_fraud_data_rob = robust.fit_transform(nan_fraud_data)\n\nnormalizer = Normalizer()\nfraud_data_norm = normalizer.fit_transform(fraud_data)\nnan_fraud_data_norm = normalizer.fit_transform(nan_fraud_data)\n\nstandard_scaler = StandardScaler()\nfraud_data_std = standard_scaler.fit_transform(fraud_data)\nnan_fraud_data_std = standard_scaler.fit_transform(nan_fraud_data)","85475f55":"from sklearn.decomposition import PCA\n\n#StandartScaler\npca_reduc_nonfraud = PCA(n_components=2, random_state=0).fit_transform(nan_fraud_data_std)\npca_reduc_fraud = PCA(n_components=2, random_state=0).fit_transform(fraud_data_std)\n#MinMAxScaler\npca_reduc_nonfraud_minm = PCA(n_components=2, random_state=0).fit_transform(nan_fraud_data_minm)\npca_reduc_fraud_minm = PCA(n_components=2, random_state=0).fit_transform(fraud_data_minm)\n#RobustScaler\npca_reduc_nonfraud_rob = PCA(n_components=2, random_state=0).fit_transform(nan_fraud_data_rob)\npca_reduc_fraud_rob = PCA(n_components=2, random_state=0).fit_transform(fraud_data_rob)\n#Normalizer\npca_reduc_nonfraud_nor = PCA(n_components=2, random_state=0).fit_transform(nan_fraud_data_norm)\npca_reduc_fraud_nor = PCA(n_components=2, random_state=0).fit_transform(fraud_data_norm)\n\nfig, axs = plt.subplots(2, 2, figsize=(32,16))\nfig.suptitle(\"Dimension Reduction with PCA\", size=20)\n\naxs[0,0].scatter(pca_reduc_nonfraud[:,0], pca_reduc_nonfraud[:,1], color=\"#f7547a\", label=(\"NonFraud\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[0,0].scatter(pca_reduc_fraud[:,0], pca_reduc_fraud[:,1], color=\"#6624f0\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[0,0].set_title(\"Standart Scaler Distribution\", color=\"#FF0040\", size=25)\naxs[0,0].spines['bottom'].set_color('red')\naxs[0,0].spines['left'].set_color('red')\naxs[0,0].legend()\n\naxs[0,1].scatter(pca_reduc_nonfraud_minm[:,0], pca_reduc_nonfraud_minm[:,1], color=\"#f7547a\", label=(\"NonFraud\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[0,1].scatter(pca_reduc_fraud_minm[:,0], pca_reduc_fraud_minm[:,1], color=\"#6624f0\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[0,1].set_title(\"MinMax Scaler Distribution\", color=\"#FF0040\", size=25)\naxs[0,1].spines['bottom'].set_color('red')\naxs[0,1].spines['left'].set_color('red')\naxs[0,1].legend()\n\naxs[1,0].scatter(pca_reduc_nonfraud_rob[:,0], pca_reduc_nonfraud_rob[:,1], color=\"#f7547a\",  label=(\"NonFraud\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[1,0].scatter(pca_reduc_fraud_rob[:,0], pca_reduc_fraud_rob[:,1], color=\"#6624f0\", label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[1,0].set_title(\"Robust Scaler Distribution\", color=\"#FF0040\", size=25)\naxs[1,0].spines['bottom'].set_color('red')\naxs[1,0].spines['left'].set_color('red')\naxs[1,0].legend()\n\naxs[1,1].scatter(pca_reduc_nonfraud_nor[:,0], pca_reduc_nonfraud_nor[:,1], color=\"#f7547a\",  label=(\"NonFraud\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[1,1].scatter(pca_reduc_fraud_nor[:,0], pca_reduc_fraud_nor[:,1], color=\"#6624f0\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[1,1].set_title(\"Normalization Distribution\", color=\"#FF0040\", size=25)\naxs[1,1].spines['bottom'].set_color('red')\naxs[1,1].spines['left'].set_color('red')\naxs[1,1].legend()","f85a75e9":"train = all_data.iloc[:607507,]\ntest = all_data.iloc[607507:,1:]\nX = train.drop([\"TARGET\"],axis=1)\ny = train[\"TARGET\"]\n\nprint(\"train shape is:\", train.shape)\nprint(\"test shape is:\", test.shape)","0c5282fe":"over = SMOTE(sampling_strategy=0.05, k_neighbors=10) #0.9540411386976924\nX_smote, y_smote = over.fit_resample(X, y)\ncounter = Counter(y_smote)\nprint(counter)\ntrain_smote = pd.concat([X_smote,y_smote], axis=1)\n\nfrom imblearn.over_sampling import BorderlineSMOTE\nbr= BorderlineSMOTE(random_state=42,sampling_strategy=0.05)\nX_kmeans, y_kmeans = br.fit_resample(X, y)\ntrain_border= pd.concat([X_kmeans,y_kmeans], axis=1)\ncounter = Counter(y_kmeans)\nprint(counter)\n\nfrom imblearn.over_sampling import ADASYN\nada = ADASYN(random_state=42,sampling_strategy=0.05, n_neighbors=7)  #0.9540411386976924\nX_ada, y_ada = ada.fit_resample(X, y)\ntrain_ada= pd.concat([X_ada,y_ada], axis=1)\ncounter = Counter(y_ada)\nprint(counter)\n\n#FARKLI OVER S\u0130MPL\u0130NG TEKN\u0130KLER\u0130 KULLANIP DE\u011eERLER\u0130 KAR\u015eILA\u015eTIRAB\u0130L\u0130R\u0130Z.\n#G\u00f6rselle\u015ftirme ile her meansin plotu","900eb2c7":"#StandartScaler\n#SMOTEdan \u00f6ncei\npca_reduc_nonfraud = PCA(n_components=2, random_state=0).fit_transform(nan_fraud_data_std)\npca_reduc_fraud = PCA(n_components=2, random_state=0).fit_transform(fraud_data_std)\n\n#SMOTE\n\nfraud_data_n = train_smote.loc[train_smote[\"TARGET\"]==1]\nfraud_data_n = fraud_data_n.drop([\"TARGET\"], axis=1)\nnan_fraud_data_n = train_smote.loc[train_smote[\"TARGET\"]==0]\nnan_fraud_data_n = nan_fraud_data_n.drop([\"TARGET\"], axis=1)\n\nfraud_data_n  =standard_scaler.fit_transform(fraud_data_n)\nnan_fraud_data_n  =standard_scaler.fit_transform(nan_fraud_data_n)\nnan_fraud_data_n = PCA(n_components=2, random_state=0).fit_transform(nan_fraud_data_n)\nfraud_data_n = PCA(n_components=2, random_state=0).fit_transform(fraud_data_n)\n\n#border\n\nfraud_data_bor = train_border.loc[train_border[\"TARGET\"]==1]\nfraud_data_bor = fraud_data_bor.drop([\"TARGET\"], axis=1)\nnan_fraud_data_bor = train_border.loc[train_border[\"TARGET\"]==0]\nnan_fraud_data_bor = nan_fraud_data_bor.drop([\"TARGET\"], axis=1)\n\nfraud_data_bor  = standard_scaler.fit_transform(fraud_data_bor)\nnan_fraud_data_bor  =standard_scaler.fit_transform(nan_fraud_data_bor)\nnan_fraud_data_bor = PCA(n_components=2, random_state=0).fit_transform(nan_fraud_data_bor)\nfraud_data_bor = PCA(n_components=2, random_state=0).fit_transform(fraud_data_bor)\n\n#ada\nfraud_data_ada= train_ada.loc[train_ada[\"TARGET\"]==1]\nfraud_data_bada = fraud_data_ada.drop([\"TARGET\"], axis=1)\nnan_fraud_data_ada = train_ada.loc[train_ada[\"TARGET\"]==0]\nnan_fraud_data_ada = nan_fraud_data_ada.drop([\"TARGET\"], axis=1)\n\nfraud_data_ada  = standard_scaler.fit_transform(fraud_data_ada)\nnan_fraud_data_ada  =standard_scaler.fit_transform(nan_fraud_data_ada)\nnan_fraud_data_ada= PCA(n_components=2, random_state=0).fit_transform(nan_fraud_data_ada)\nfraud_data_ada = PCA(n_components=2, random_state=0).fit_transform(fraud_data_ada)\n\n\n\n\nfig, axs = plt.subplots(2,2, figsize=(32,16))\nfig.suptitle(\"Dimension Reduction with PCA\", size=20)\n\naxs[0,0].scatter(pca_reduc_nonfraud[:,0], pca_reduc_nonfraud[:,1], color=\"#f7547a\", label=(\"NonFraud\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[0,0].scatter(pca_reduc_fraud[:,0], pca_reduc_fraud[:,1], color=\"#6624f0\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[0,0].set_title(\"Original Data\", color=\"#FF0040\", size=25)\naxs[0,0].spines['bottom'].set_color('red')\naxs[0,0].spines['left'].set_color('red')\naxs[0,0].legend()\n\naxs[0,1].scatter(nan_fraud_data_n[:,0], nan_fraud_data_n[:,1], color=\"#f7547a\", label=(\"NonFraud\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[0,1].scatter(fraud_data_n[:,0], fraud_data_n[:,1], color=\"#6624f0\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[0,1].set_title(\"Resampling With Smote\", color=\"#FF0040\", size=25)\naxs[0,1].spines['bottom'].set_color('red')\naxs[0,1].spines['left'].set_color('red')\naxs[0,1].legend()\n\naxs[1,0].scatter(nan_fraud_data_bor[:,0], nan_fraud_data_bor[:,1], color=\"#f7547a\", label=(\"NonFraud\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[1,0].scatter(fraud_data_bor[:,0], fraud_data_bor[:,1], color=\"#6624f0\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[1,0].set_title(\"Resampling With BorderlineSMOTE\", color=\"#FF0040\", size=25)\naxs[1,0].spines['bottom'].set_color('red')\naxs[1,0].spines['left'].set_color('red')\naxs[1,0].legend()\n\naxs[1,1].scatter(nan_fraud_data_ada[:,0], nan_fraud_data_ada[:,1], color=\"#f7547a\", label=(\"NonFraud\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[1,1].scatter(fraud_data_ada[:,0], fraud_data_ada[:,1], color=\"#6624f0\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[1,1].set_title(\"Resampling With Adasyn\", color=\"#FF0040\", size=25)\naxs[1,1].spines['bottom'].set_color('red')\naxs[1,1].spines['left'].set_color('red')\naxs[1,1].legend()","266de343":"all_data = pd.get_dummies(all_data, columns=[\"TXN_SOURCE\",\"TXN_ENTRY\",\"CITY\",\"COUNTRY\",\"MCC_CODE\",\"total_number_ratio\"]) \n\n\nall_data = all_data.drop([\"more_fraud_ratio_country\",\n                          \"fraud_mean_mcc\",\n                          \"fraud_mean_country\",\n                          \"fraud_mean_c\u0131ty\"],axis=1)\n#more_fraud_ratio_c\u0131ty\n#fraud_mean_mcc\n#total_number_ratio\n#more_fraud_ratio_mcc","2df0d202":"t1=time.time()\nkf = KFold(n_splits=5, shuffle=True, random_state = 34)\noff = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\nfor train_index, test_index in kf.split(train):\n    #ilgili iloclara g\u00f6re split edilmesi\n    X_train , X_val = X.iloc[train_index], X.iloc[test_index]  \n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]    \n    \n    print(\"X_train shape is :\", X_train.shape,\"X_val shape is\", X_val.shape)\n    y_pred_list = []\n    for seed in [None]:\n        d_train = lgbm.Dataset(X_train, y_train)\n        d_valid = lgbm.Dataset(X_val, y_val)\n        counter = Counter(y_train)\n        print(\"\u0130lgili Folddaki nonfraud ve fraud say\u0131lar\u0131:\",counter)\n        params = {\"objective\":\"binary\",\n                  \"metric\":\"auc\",\n                  \"verbosity\":-1, \n                  \"boosting_type\":\"dart\",\n                  \"feature_fraction\": 0.08,\n                  \"bagging_fraction\":0.04,\n                  \"num_leaves\":20,\n                  \"max_depth\":8,\n                 \"learning_rate\":0.135,\n                 #\"class_weight\" : \"{0: 0.30 ,  1: 1.2}\"\n                 }\n        \n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                          d_train,\n                          valid_sets = [d_train, d_valid],               \n                          num_boost_round = 500,\n                          #early_stopping_rounds=35,\n                          verbose_eval = 40)\n                   \n        y_pred_list.append(model.predict(X_val))\n        #validation trainin i\u00e7indeki orandan\n        print(roc_auc_score(y_val, np.mean(y_pred_list, axis=0)))\n        \n        #print(f1_score(y_val, y_pred_list))\n        \n        test_preds.append(model.predict(test))\n    off[test_index] = np.mean(y_pred_list, axis=0)\n    score = roc_auc_score(y_val, off[test_index])\n    print(f\"Auc- Fold - {fold}: {score}\")\n    score_list.append(score)\n    fold +=1\n\n    \nprint(np.mean(score_list))\nprint(roc_auc_score(y, off))\nt2=time.time()\nprint(\"LGBM model take : {:.3f} sn.\".format(t2-t1))","60cdb9b7":"test_preds_lgbm = pd.DataFrame(test_preds)\nprint(test_preds_lgbm.head())\n\ntest_preds_lgbm = test_preds_lgbm.T\ntest_preds_lgbm[\"mean\"] = test_preds_lgbm.mean(axis=1)\ntest_preds_lgbm = test_preds_lgbm.iloc[:,5]\n\nsubmission_csv[\"Predicted\"] = test_preds_lgbm.values\n","c91c5610":"kf = KFold(n_splits=5, shuffle=True, random_state = 10)\nscore_list = []\nfold = 1\nxgb_pred_y = []\nfor train_index, test_index in kf.split(train):\n    #ilgili iloclara g\u00f6re split edilmesi\n    X_train , X_val = X.iloc[train_index], X.iloc[test_index]  \n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]    \n    \n    print(\"X_train shape is :\", X_train.shape,\"X_val shape is\", X_val.shape)\n    y_pred_list = []\n    for seed in [None]:\n        d_train = xgb.DMatrix(X_train, y_train)\n        d_valid = xgb.DMatrix(X_val, y_val)\n        params = {'objective': 'binary:logitraw', \n          'eval_metric': 'auc',\n          'max_depth': 10, \n          'subsample': 0.6}\n        \n        params[\"seed\"] = seed\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model_xgb = xgb.train(params, d_train, 30, watchlist, maximize=False)\n        dtest = xgb.DMatrix(test)\n        xgb_preds = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n        xgb_preds = xgb_preds+1\n        xgb_pred_y.append(xgb_preds)\n        #print(f1_score(y_val, y_pred_list))\n        \n    fold +=1\n\nt2=time.time()\nprint(\"XGBOOST model take : {:.3f} sn.\".format(t2-t1))\n","f722f3b7":"\"\"\"from keras.models import Sequential\nfrom keras.layers import Dense\n\nclassifier = Sequential()\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN | means applying SGD on the whole ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100,verbose = 0)\n\nscore, acc = classifier.evaluate(X_train, y_train,\n                            batch_size=10)\nprint('Train score:', score)\nprint('Train accuracy:', acc)\"\"\"","e1319de8":"xgb_pred_y = pd.DataFrame(xgb_pred_y)\nxgb_pred_y = xgb_pred_y.T\nxgb_pred_y[\"mean\"] = xgb_pred_y.mean(axis=1)\nxgb_pred_y = xgb_pred_y.iloc[:,5]\n\nprint(xgb_pred_y)","bfa06891":"xgb.plot_importance(model_xgb)\nplt.show()","52475e51":"from numpy import sqrt, argmax\nfrom sklearn.metrics import roc_curve\n\n\nfpr, tpr, thresholds = roc_curve(y, off)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\nplt.figure(figsize=(12,12))\nplt.plot([0,1], [0,1], linestyle='--', label='No Skill')\nplt.plot(fpr, tpr, marker='.', label='Logistic')\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best',s=100)\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.annotate(text=\"Best Threshold = 0.006871\", xy=(0.06871,0.894),xytext=(0.10,0.80))\nplt.legend()\nplt.show()","6215db5a":"\"\"\"submission_csv[\"Predicted\"] = [0 if i <0.010560 else 1 for i in submission_csv[\"Predicted\"]]\"\"\"","0571af0d":"#Ensemble\nsubmission_csv[\"Predicted\"]  = (submission_csv[\"Predicted\"] + xgb_pred_y) \/2\nprint(submission_csv.head())\nsubmission_csv.to_csv(\"submission_is_bankas\u0131.csv\",index=False)  ","32568409":"<a id=\"roc_curve\"><\/a>\n<span style=\"color:crimson; font-size:25px;\">  Roc Curve And Ensemble <\/span>\n\n","4a500b16":"<a id=\"feature_engineering\"><\/a>\n<span style=\"color:crimson; font-size:35px;\">  FEATURE ENGINEERING <\/span>","9fdeaf9c":"# Bir \u00e7ok ID kolonunun bulundu\u011fu bu data setinde PCA G\u00f6rselle\u015ftirmesinin bizi yan\u0131ltmas\u0131, olduk\u00e7a y\u00fcksek bir ihtimal.Normalizasyon yaparak g\u00f6rselle\u015ftirmeyi daha k\u0131sa s\u00fcrede \u00e7al\u0131\u015ft\u0131rabilece\u011fimi biliyorum. Ancak farkl\u0131 normalization teknikleri ile bir iyile\u015ftirme yakalayabilir miyim ?","17882e18":"<a id=\"deep\"><\/a>\n<span style=\"color:crimson; font-size:20px;\">  Deep Learning <\/span>","e930e8f7":"<a id=\"visualizations\"><\/a>\n<span style=\"color:crimson; font-size:35px;\">  Data Visualizations  <\/span>\n","0bebb373":"<a id=\"under_samp\"><\/a>\n<span style=\"color:crimson; font-size:35px;\">  Resampling process <\/span>","502c4831":"<a id=\"model\"><\/a>\n<span style=\"color:crimson; font-size:35px;\">  MODEL BUILDING <\/span>","8a6705e5":"# Fraud ve nonFraud datay\u0131 feature engineering ile daha iyi ay\u0131rd\u0131\u011f\u0131m\u0131z zaman resampling tekniklerinden daha iyi bir sonu\u00e7 alabiliriz.","c1e55a83":"# Her i\u015flem kayna\u011f\u0131ndaki fraud oran\u0131","be7a4c35":"* Target : Hedef de\u011fi\u015fken (fraud i\u015flemler 1)\n* DAY_OF_MONTH: Ay\u0131n ka\u00e7\u0131nc\u0131 g\u00fcn\u00fc\n* DAY_OF_WEEK: Haftan\u0131n ka\u00e7\u0131nc\u0131 g\u00fcn\u00fc\n* TXN_TIME: \u0130\u015flem saati\n* CC_NR: Maskelenmi\u015f kart no\n* CST_NR: Maskelenmi\u015f m\u00fc\u015fteri no\n* TXN_SOURCE: \u0130\u015flem kayna\u011f\u0131\n* TXN_TRM: Terminal tipi\n* TXN_ENTRY: \u015eifre do\u011frulama y\u00f6ntemi\n* TXN_AMNT: \u0130\u015flem tutar\u0131\n* CITY: \u00dcye i\u015fyeri \u015fehir\n* COUNTRY: \u00dcye i\u015fyeri \u00fclke\n* MC_NAME: \u00dcye i\u015fyeri ad\u0131\n* MC_ID: \u00dcye i\u015fyeri kodu\n* MCC_CODE: Sekt\u00f6r kodu","fa175410":"# Mevcut de\u011ferlendirme metri\u011fi \"Auc\" model sonu\u00e7lar\u0131ndan haz\u0131r gelmektedir. Ancak 1 ve 0 olarak s\u0131n\u0131fland\u0131rmak isteseydik best threshold ve g\u00f6rselle\u015ftirmeyi a\u015fa\u011f\u0131daki algoritmay\u0131 kullanarak yapabiliriz.","a31d813d":"# \"Do\u011frulama y\u00f6ntemi 3\" \u00e7ok y\u00fcksek bir fraud oran\u0131na sahip ancak bunu kullanan m\u00fc\u015fteri say\u0131s\u0131 da olduk\u00e7a az.","ccb87acb":"\n\n<img src= \"https:\/\/miro.medium.com\/max\/700\/1*HEusmKcnGBW_qaMlvg79Rw.jpeg\" alt =\"Titanic\" style='width: 1400px; height: 600px;'>","a5720741":"<a id=\"Conclusions\"><\/a>\n<span style=\"color:crimson; font-size:30px;\">   Results <\/span>\n","f38ab919":"# Content\n# 1. [Data Exploration And Visualization](#visualizations)\n\n\n\n# 2. [Feature Engineering](#feature_engineering)\n\n\n# 3. [Resampling Process](#under_samp)\n\n# 4. [Build Model](#model)\n*      [LIGHTGBM](#L\u0130GHTGBMMODEL)\n*      [XGBOOST ](#XGBOOSTMODEL)\n*      [Deep Learning ](#deep)\n\n\n# 5. [Roc Curve and models conclusion](#roc_curve)\n*      [Results](#Conclusions)\n","d9717bdd":"<a id=\"L\u0130GHTGBMMODEL\"><\/a>\n<span style=\"color:green; font-size:20px;\">  LIGHTGBM <\/span>\n","652ba77f":"<a id=\"XGBOOSTMODEL\"><\/a>\n<span style=\"color:crimson; font-size:20px;\">  XGBOOST  <\/span>","22ab98b0":"# Her iki modelden gelen sonu\u00e7lar\u0131n basit bir \u015fekilde ortalamas\u0131 al\u0131narak tahmin ger\u00e7ekle\u015ftirildi.","ac934441":"# A\u011fa\u00e7lar i\u00e7in normalization \u00f6nemsiz ancak deep learning i\u00e7in bir iyile\u015ftirme olabilir."}}