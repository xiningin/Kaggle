{"cell_type":{"86018ab9":"code","d26358f4":"code","98797cce":"code","614273bb":"code","6b4716b7":"code","e2fb7bb7":"code","1b2839fb":"code","80bedbc7":"code","8980ad06":"code","5e7aeecc":"code","12d2da3f":"code","a5d9f8d7":"code","ac87a6a2":"code","d8af971d":"code","e7bef8a8":"code","fffc0d39":"code","dfe069ad":"code","6e153e8a":"code","3b7ca176":"code","b8082535":"code","f503f187":"code","fa99d0f2":"code","52f33cf9":"code","692532f4":"code","020e63c1":"code","e105aec1":"code","e861ffd4":"code","e4e20d99":"code","e29d7563":"code","4f47e890":"code","6c55a1fb":"code","b103a426":"code","5192c990":"code","517812af":"code","6fa19d91":"code","415bc253":"code","48a75561":"code","2ee1346e":"code","0291715e":"code","a22a8aea":"code","25339485":"code","bd3efa59":"code","e1a4965e":"code","0ea39896":"code","0378f5c3":"code","7e31903f":"code","e67a4f46":"code","98572114":"code","5c5b9612":"code","fc98634b":"code","665f7a86":"code","e7aa9b4a":"code","1463ff8c":"code","0ec01e38":"code","27e07c1b":"code","446ede97":"code","806ea699":"markdown","512c75c7":"markdown","38d67bcc":"markdown","4f9f68ce":"markdown","b3dd1b21":"markdown","f25e309e":"markdown","bf93ff12":"markdown","87cd9093":"markdown","f1fe9ac3":"markdown","84e89fb2":"markdown","fc2a9c21":"markdown","073f2603":"markdown","014ccfa3":"markdown","dc8b9bf4":"markdown","a1ad8446":"markdown","3a0f09c3":"markdown","873ead10":"markdown","d0646f9a":"markdown","94bb8eb9":"markdown","ae612a2d":"markdown","2dd846dd":"markdown","09557de1":"markdown","6906ea36":"markdown"},"source":{"86018ab9":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport gc\nimport pickle\nimport math\nfrom tqdm import tqdm\nos.chdir(\"..\") # set the root path ","d26358f4":"recommend_num = 400  # iterate number\ntopk_num = 100  # final recall number of each recall method","98797cce":"path = '.\/input\/kdd-cup-data\/'\n\ntrain_df_path = path + 'train_clicks.csv'\n\ntrain_df = pd.read_csv(train_df_path)","614273bb":"test_df = pd.read_csv(path+'test_clicks.csv')","6b4716b7":"tmp_df = test_df.groupby('user_id').tail(1)\ntmp1_df = test_df.append(tmp_df).drop_duplicates(subset=['user_id','item_id'],keep=False)\ntrain_df = train_df.append(tmp1_df)\ntest_df = tmp_df","e2fb7bb7":"train_df = train_df.groupby('user_id').tail(5)","1b2839fb":"def make_item_sim_tuple(group_df):\n    group_df = group_df.sort_values(by=['sim'], ascending=False)\n    item_score_tuples = list(zip(group_df['item_id'], group_df['sim']))\n    return item_score_tuples\n\n\n\ndef sub2_df(filename):\n    rec_items = []\n    constant_sim = 100\n    with open(filename) as f:\n        for line in f:\n            row = line.strip().split(\",\")\n            uid = int(row[0])\n            iids = row[1:]\n            phase = uid % 11\n            for idx, iid in enumerate(iids):\n                rec_items.append((uid, int(iid), constant_sim - idx, phase))\n\n    return pd.DataFrame(rec_items, columns=['user_id', 'item_id', 'sim', 'phase'])\n\n\ndef recall_df2dict(phase_df):\n    phase_df = phase_df.groupby('user_id').apply(make_item_sim_tuple).reset_index().rename(\n        columns={0: 'item_score_list'})\n    item_score_list = phase_df['item_score_list'].apply(\n        lambda item_score_list: sorted(item_score_list, key=lambda x: x[1], reverse=True))\n    phase_user_item_score_dict = dict(zip(phase_df['user_id'], item_score_list))\n    return phase_user_item_score_dict\n\n\ndef recall_dict2df(recall_item_score_dict):\n    recom_list = []\n    for u, item_score_list in recall_item_score_dict.items():\n        for item, score in item_score_list:\n            recom_list.append((u, item, score))\n    return pd.DataFrame(recom_list, columns=['user_id', 'item_id', 'sim'])","80bedbc7":"# fill user to 50 items  \ndef get_predict(df, pred_col):\n\n    df.sort_values(pred_col, ascending=False, inplace=True)\n    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')\n    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False)\n    df = df[df['rank'] <= 50]\n    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).str.split(',',\n                                                                                                   expand=True).reset_index()\n    return df","8980ad06":"from collections import defaultdict\n\ndef make_user_time_tuple(group_df, user_col='user_id', item_col='item_id', time_col='time'):\n    user_time_tuples = list(zip(group_df[user_col], group_df[time_col]))\n    return user_time_tuples\n\ndef make_item_time_tuple(group_df, user_col='user_id', item_col='item_id', time_col='time'):\n   # group_df = group_df.drop_duplicates(subset=[user_col, item_col], keep='last')\n    item_time_tuples = list(zip(group_df[item_col], group_df[time_col]))\n    return item_time_tuples\n\ndef get_user_item_time_dict(df, user_col='user_id', item_col='item_id', time_col='time', is_drop_duplicated=False):\n    user_item_ = df.sort_values(by=[user_col, time_col])\n    \n    if is_drop_duplicated:\n        print('drop duplicates...')\n        user_item_ = user_item_.drop_duplicates(subset=['user_id', 'item_id'], keep='last')\n        \n    user_item_ = user_item_.groupby(user_col).apply(lambda group: make_item_time_tuple(group, user_col, item_col, time_col)).reset_index().rename(columns={0: 'item_id_time_list'})\n    user_item_time_dict = dict(zip(user_item_[user_col], user_item_['item_id_time_list']))\n    return user_item_time_dict\n\ndef get_item_user_time_dict(df, user_col='user_id', item_col='item_id', time_col='time'):\n    item_user_df = df.sort_values(by=[item_col, time_col])\n    item_user_df = item_user_df.groupby(item_col).apply(\n        lambda group: make_user_time_tuple(group, user_col, item_col, time_col)).reset_index().rename(\n        columns={0: 'user_id_time_list'})\n    item_user_time_dict = dict(zip(item_user_df[item_col], item_user_df['user_id_time_list']))\n    return item_user_time_dict\n\ndef get_user_item_dict(df, user_col='user_id', item_col='item_id', time_col='time'):\n    user_item_ = df.groupby(user_col)[item_col].agg(set).reset_index()\n    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))\n    return user_item_dict\n\ndef get_user_min_time_dict(df,  user_col='user_id', item_col='item_id', time_col='time'):\n    df = df.sort_values(by=[user_col, time_col])\n    df = df.groupby(user_col).head(1)\n    user_min_time_dict = dict(zip(df[user_col], df[time_col]))\n    return user_min_time_dict\n\n\ndef item_based_recommend(sim_item_corr, user_item_time_dict, user_id, top_k, item_num, alpha=15000,\n                         item_cnt_dict=None, user_cnt_dict=None, adjust_type='v2'):\n\n    rank = {}\n    if user_id not in user_item_time_dict:\n        return []\n    interacted_item_times = user_item_time_dict[user_id]\n    min_time = min([time for item, time in interacted_item_times])\n    interacted_items = set([item for item, time in interacted_item_times])\n\n    miss_item_num = 0\n    for loc, (i, time) in enumerate(interacted_item_times):\n        if i not in sim_item_corr:\n            miss_item_num += 1\n            continue\n        for j, wij in sorted(sim_item_corr[i].items(), key=lambda x: x[1], reverse=True)[0:top_k]:\n            if j not in interacted_items:\n                rank.setdefault(j, 0)\n\n                content_weight = 1.0\n\n                time_weight = np.exp(alpha * (time - min_time))\n                loc_weight = (0.9 ** (len(interacted_item_times) - loc))\n                rank[j] += loc_weight * time_weight * content_weight * wij\n    if miss_item_num > 10:\n        print('user_id={}, miss_item_num={}'.format(user_id, miss_item_num))\n\n    if item_cnt_dict is not None:\n        for loc, item in enumerate(rank):\n            rank[item] = re_rank(rank[item], item, user_id, item_cnt_dict, user_cnt_dict, adjust_type=adjust_type)\n\n    sorted_rank_items = sorted(rank.items(), key=lambda d: d[1], reverse=True)\n\n    return sorted_rank_items[0:item_num]\n\n\ndef user_based_recommend(sim_user_corr, user_item_time_dict, user_id, top_k, item_num, alpha=15000,\n                         item_cnt_dict=None, user_cnt_dict=None, adjust_type='v2'):\n\n    rank = {}\n    interacted_items = set([i for i, t in user_item_time_dict[user_id]])\n    interacted_item_time_list = user_item_time_dict[user_id]\n    interacted_num = len(interacted_items)\n\n    min_time = min([t for i, t in interacted_item_time_list])\n    time_weight_dict = {i: np.exp(alpha * (t - min_time)) for i, t in interacted_item_time_list}\n    loc_weight_dict = {i: 0.9 ** (interacted_num - loc) for loc, (i, t) in enumerate(interacted_item_time_list)}\n\n    for sim_v, wuv in sorted(sim_user_corr[user_id].items(), key=lambda x: x[1], reverse=True)[0:top_k]:\n        if sim_v not in user_item_time_dict:\n            continue\n        for j, j_time in user_item_time_dict[sim_v]:\n            if j not in interacted_items:\n                rank.setdefault(j, 0)\n\n                content_weight = 1.0\n\n                # weight = np.exp(-15000*abs(j_time-q_time))\n                rank[j] += content_weight * wuv\n\n    if item_cnt_dict is not None:\n        for loc, item in enumerate(rank):\n            rank[item] = re_rank(rank[item], item, user_id, item_cnt_dict, user_cnt_dict, adjust_type=adjust_type)\n\n    rec_items = sorted(rank.items(), key=lambda d: d[1], reverse=True)\n\n    return rec_items[:item_num]\n\n\ndef re_rank(sim, i, u, item_cnt_dict, user_cnt_dict, adjust_type='v2'):\n    '''\n    re_rank based on the popularity and similarity\n    '''\n    if adjust_type is None:\n        return sim\n    elif adjust_type == 'v1':\n        # Log\uff0cLinear, 3\/4\n        if item_cnt_dict.get(i, 1.0) < 4:\n            heat = np.log(item_cnt_dict.get(i, 1.0) + 2)\n        elif item_cnt_dict.get(i, 1.0) >= 4 and item_cnt_dict.get(i, 1.0) < 10:\n            heat = item_cnt_dict.get(i, 1.0)\n        else:\n            heat = item_cnt_dict.get(i, 1.0) ** 0.75 + 5.0  # 3\/4\n        sim *= 2.0 \/ heat\n\n    elif adjust_type == 'v2':\n        user_cnt = user_cnt_dict.get(u, 1.0)\n\n        if item_cnt_dict.get(i, 1.0) < 4:\n            heat = np.log(item_cnt_dict.get(i, 1.0) + 2)\n        elif item_cnt_dict.get(i, 1.0) >= 4 and item_cnt_dict.get(i, 1.0) < 10:\n            if user_cnt > 50:\n                heat = item_cnt_dict.get(i, 1.0) * 1\n            elif user_cnt > 25:\n                heat = item_cnt_dict.get(i, 1.0) * 1.2\n            else:\n                heat = item_cnt_dict.get(i, 1.0) * 1.6\n        else:\n            if user_cnt > 50:\n                user_cnt_k = 0.4\n            elif user_cnt > 10:\n                user_cnt_k = 0.1\n            else:\n                user_cnt_k = 0\n            heat = item_cnt_dict.get(i, 1.0) ** user_cnt_k + 10 - 10 ** user_cnt_k  # 3\/4\n        sim *= 2.0 \/ heat\n\n    else:\n        sim += 2.0 \/ item_cnt_dict.get(i, 1.0)\n\n    return sim","5e7aeecc":"def swing(df, user_col='user_id', item_col='item_id', time_col='time'):\n    # 1. item, (u1,t1), (u2, t2).....\n    item_user_df = df.sort_values(by=[item_col, time_col])\n    item_user_df = item_user_df.groupby(item_col).apply(\n        lambda group: make_user_time_tuple(group, user_col, item_col, time_col)).reset_index().rename(\n        columns={0: 'user_id_time_list'})\n    item_user_time_dict = dict(zip(item_user_df[item_col], item_user_df['user_id_time_list']))\n\n    user_item_time_dict = defaultdict(list)\n    # 2. ((u1, u2), i1, d12)\n    u_u_cnt = defaultdict(list)\n    item_cnt = defaultdict(int)\n    for item, user_time_list in tqdm(item_user_time_dict.items()):\n        for u, u_time in user_time_list:\n            # just record\n            item_cnt[item] += 1\n            user_item_time_dict[u].append((item, u_time))\n\n            for relate_u, relate_u_time in user_time_list:\n                if relate_u == u:\n                    continue\n               \n                key = (u, relate_u)  if u <= relate_u else (relate_u, u)\n                u_u_cnt[key].append((item, np.abs(u_time - relate_u_time)))\n\n\n    # 3. (i1,i2), sim\n    sim_item = {}\n    alpha = 5.0\n    for u_u, co_item_times in u_u_cnt.items():\n        num_co_items = len(co_item_times)\n        for i, i_time_diff in co_item_times:\n            sim_item.setdefault(i, {})\n            for j, j_time_diff in co_item_times:\n              if j == i:\n                continue\n              weight = 1.0 # np.exp(-15000*(i_time_diff + j_time_diff)), not effective\n              sim_item[i][j] = sim_item[i].setdefault(j, 0.) + weight \/ (alpha + num_co_items)\n    # 4. norm by item count\n    sim_item_corr = sim_item.copy()\n    for i, related_items in sim_item.items():\n        for j, cij in related_items.items():\n            sim_item_corr[i][j] = cij \/ math.sqrt(item_cnt[i] * item_cnt[j])\n    del user_item_time_dict, sim_item\n    gc.collect()\n    return sim_item_corr","12d2da3f":"def get_time_dir_aware_sim_item(df, user_col='user_id', item_col='item_id', time_col='time'):\n    user_item_time_dict = get_user_item_time_dict(df, user_col, item_col, time_col)\n\n    sim_item = {}\n    item_cnt = defaultdict(int)\n    for user, item_time_list in tqdm(user_item_time_dict.items()):\n        for loc_1, (i, i_time) in enumerate(item_time_list):\n            item_cnt[i] += 1\n            sim_item.setdefault(i, {})\n            for loc_2, (relate_item, related_time) in enumerate(item_time_list):\n                if i == relate_item:\n                    continue\n                loc_alpha = 1.0 if loc_2 > loc_1 else 0.7\n                loc_weight = loc_alpha * (0.8**(np.abs(loc_2-loc_1)-1)) \n                time_weight = np.exp(-15000*np.abs(i_time-related_time))\n\n                sim_item[i].setdefault(relate_item, 0)\n                sim_item[i][relate_item] += loc_weight * time_weight \/ math.log(1 + len(item_time_list))\n                \n    sim_item_corr = sim_item.copy()\n    for i, related_items in tqdm(sim_item.items()):\n        for j, cij in related_items.items():\n            sim_item_corr[i][j] = cij \/ math.sqrt(item_cnt[i] * item_cnt[j])\n            # sim_item_corr[i][j] = cij \/ math.sqrt(item_cnt[i]*item_cnt[j])+cij\/min(item_cnt[i], item_cnt[j])+0.5*cij\/(item_cnt[i]+item_cnt[j])\n    del sim_item, user_item_time_dict\n    gc.collect()\n    return sim_item_corr\n","a5d9f8d7":"def get_bi_sim_item(df, user_col='user_id', item_col='item_id', time_col='time'):\n    item_user_time_dict = get_item_user_time_dict(df, user_col, item_col, time_col)\n    user_item_time_dict = get_user_item_time_dict(df, user_col, item_col, time_col)\n\n    item_cnt = defaultdict(int)  \n    for user, item_times in tqdm(user_item_time_dict.items()):  \n        for i,t in item_times:  \n            item_cnt[i] += 1  \n\n    sim_item = {}\n    \n    for item, user_time_lists in tqdm(item_user_time_dict.items()):\n    \n        sim_item.setdefault(item, {}) \n    \n        for u, item_time in user_time_lists:\n        \n            tmp_len = len(user_item_time_dict[u])\n        \n            for relate_item, related_time in user_item_time_dict[u]:\n                sim_item[item].setdefault(relate_item, 0)\n                weight = np.exp(-15000*np.abs(related_time - item_time))\n                sim_item[item][relate_item] += weight \/ (math.log(len(user_time_lists)+1) * math.log(tmp_len+1))\n    del user_item_time_dict\n    gc.collect()\n    return sim_item","ac87a6a2":"# user-cf\ndef get_sim_user(df, user_col='user_id', item_col='item_id', time_col='time'):\n    # user_min_time_dict = get_user_min_time_dict(df, user_col, item_col, time_col) # user first time \n    # history\n    user_item_time_dict = get_user_item_time_dict(df)\n    # item, [u1, u2, ...,]\n    item_user_time_dict = get_item_user_time_dict(df)\n\n    sim_user = {}\n    user_cnt = defaultdict(int)\n    for item, user_time_list in tqdm(item_user_time_dict.items()):\n        num_users = len(user_time_list)\n        for u, t in user_time_list:\n            user_cnt[u] += 1\n            sim_user.setdefault(u, {})\n            for relate_user, relate_t in user_time_list:\n                # time_diff_relate_u = 1.0\/(1.0+10000*abs(relate_t-t))\n                if u == relate_user:\n                    continue\n                sim_user[u].setdefault(relate_user, 0)\n                weight = 1.0\n                sim_user[u][relate_user] += weight \/ math.log(1 + num_users) # \u6d41\u884c\u5ea6\u9ad8\u7684\u8870\u51cf\n\n    sim_user_corr = sim_user.copy()\n    for u, related_users in tqdm(sim_user.items()):\n        for v, cuv in related_users.items():\n            sim_user_corr[u][v] = cuv \/ math.sqrt(user_cnt[u] * user_cnt[v])\n    del sim_user, user_item_time_dict\n    gc.collect()\n    return sim_user_corr\n","d8af971d":"def norm_recall_item_score_list(sorted_recall_item_list):\n    if len(sorted_recall_item_list) == 0: return sorted_recall_item_list\n\n    assert sorted_recall_item_list[0][1] >= sorted_recall_item_list[-1][1]  # check whether it's the ranked result\n    max_sim = sorted_recall_item_list[0][1]\n    min_sim = sorted_recall_item_list[-1][1]\n\n    norm_sorted_recall_item_list = []\n    for item, score in sorted_recall_item_list:\n        if max_sim > 0:\n            norm_score = 1.0 * (score - min_sim) \/ (max_sim - min_sim) if max_sim > min_sim else 1.0\n        else:\n            norm_score = 0.0  # topk-fill set to 0.0\n        norm_sorted_recall_item_list.append((item, norm_score))\n    return norm_sorted_recall_item_list\n\n\ndef norm_user_recall_item_dict(recall_item_dict):\n    norm_recall_item_dict = {}\n    for u, sorted_recall_item_list in recall_item_dict.items():\n        norm_recall_item_dict[u] = norm_recall_item_score_list(sorted_recall_item_list)\n    return norm_recall_item_dict\n\n\ndef get_recall_results(item_sim_dict, user_item_dict, target_user_ids=None, item_based=True,\n                       item_cnt_dict=None, user_cnt_dict=None, adjust_type='xtf_v6'):\n    if target_user_ids is None:\n        target_user_ids = user_item_dict.keys()\n    recall_item_dict = {}\n\n\n\n    print('adjust_type={}'.format(adjust_type))\n\n    for u in tqdm(target_user_ids):\n        if item_based:\n            recall_items = item_based_recommend(item_sim_dict, user_item_dict, u, recommend_num, topk_num,\n                                                item_cnt_dict=item_cnt_dict, user_cnt_dict=user_cnt_dict,\n                                                adjust_type=adjust_type)\n        else:\n            recall_items = user_based_recommend(item_sim_dict, user_item_dict, u, recommend_num, topk_num,\n                                                item_cnt_dict=item_cnt_dict, user_cnt_dict=user_cnt_dict,\n                                                adjust_type=adjust_type)\n\n\n        recall_item_dict[u] = recall_items\n\n    return recall_item_dict","e7bef8a8":"# item-cf\n# bi-graph\n# user-cf\n# item-cf\ndef agg_recall_results(recall_item_dict_list_dict, is_norm=True, ret_type='tuple',\n                       weight_dict={}):\n    print('aggregate recall results begin....')\n    agg_recall_item_dict = {}\n    for name, recall_item_dict in recall_item_dict_list_dict.items():\n        if is_norm:\n            recall_item_dict = norm_user_recall_item_dict(recall_item_dict)\n        weight = weight_dict.get(name, 1.0)\n        print('name={}, weight={}'.format(name, weight))\n        for u, recall_items in recall_item_dict.items():\n            agg_recall_item_dict.setdefault(u, {})\n            for i, score in recall_items:\n                agg_recall_item_dict[u].setdefault(i, 0.0)\n                agg_recall_item_dict[u][i] += weight * score  # \u7d2f\u52a0\n\n    if ret_type == 'tuple':\n        agg_recall_item_tuple_dict = {}\n        for u, recall_item_dict in agg_recall_item_dict.items():\n            sorted_recall_item_tuples = sorted(recall_item_dict.items(), key=lambda x: x[1], reverse=True)\n            agg_recall_item_tuple_dict[u] = sorted_recall_item_tuples\n        return agg_recall_item_tuple_dict\n\n    if ret_type == 'df':\n        recall_u_i_score_pair_list = []\n        for u, recall_item_dict in agg_recall_item_dict.items():\n            for i, score in recall_item_dict.items():\n                recall_u_i_score_pair_list.append((u, i, score))\n        recall_df = pd.DataFrame.from_records(recall_u_i_score_pair_list, columns=['user_id', 'item_id', 'sim'])\n        return recall_df\n\n    return agg_recall_item_dict","fffc0d39":"import pickle\n\ndef get_multi_source_sim_dict_results(history_df, recall_methods={'item-cf', 'bi-graph', 'user-cf', 'swing'}):\n    recall_sim_pair_dict = {}\n    if 'item-cf' in recall_methods:\n        print('item-cf item-sim begin')\n        item_sim_dict = get_time_dir_aware_sim_item(history_df)\n        recall_sim_pair_dict['item-cf'] = item_sim_dict\n        del item_sim_dict\n        gc.collect()\n        #print('item-cf item-sim-pair done, pair_num={}'.format(len(item_sim_dict)))\n\n    if 'bi-graph' in recall_methods:\n        print('bi-graph item-sim begin')\n        item_sim_dict = get_bi_sim_item(history_df)\n        recall_sim_pair_dict['bi-graph'] = item_sim_dict\n        del item_sim_dict\n        gc.collect()\n        #print('bi-graph item-sim-pair done, pair_num={}'.format(len(item_sim_dict)))\n\n    if 'swing' in recall_methods:\n        print('swing item-sim begin')\n        item_sim_dict = swing(history_df)\n        recall_sim_pair_dict['swing'] = item_sim_dict\n        del item_sim_dict\n        gc.collect()\n        #print('swing item-sim-pair done, pair_num={}'.format(len(item_sim_dict)))\n\n    if 'user-cf' in recall_methods:\n        print('user-cf user-sim begin')\n        user_sim_dict = get_sim_user(history_df)\n        recall_sim_pair_dict['user-cf'] = user_sim_dict\n        del user_sim_dict\n        gc.collect()\n        #print('user-cf user-sim-pair done, pair_num={}'.format(len(user_sim_dict)))\n        \n    return recall_sim_pair_dict\n\n\ndef do_multi_recall_results(recall_sim_pair_dict, user_item_time_dict,\n                            target_user_ids=None, ret_type='df',\n                            item_cnt_dict=None, user_cnt_dict=None, adjust_type='v2'):\n    if target_user_ids is None:\n        target_user_ids = user_item_time_dict.keys()\n\n    recall_item_list_dict = {}\n    for name, sim_dict in recall_sim_pair_dict.items():\n        # item-based\n        if name in {'item-cf', 'bi-graph', 'swing'}:\n            recall_item_dict = get_recall_results(sim_dict, user_item_time_dict, target_user_ids, item_based=True,\n                                                  item_cnt_dict=item_cnt_dict, user_cnt_dict=user_cnt_dict,\n                                                  adjust_type=adjust_type)\n        else:\n            recall_item_dict = get_recall_results(sim_dict, user_item_time_dict, target_user_ids, item_based=False,\n                                                  item_cnt_dict=item_cnt_dict, user_cnt_dict=user_cnt_dict,\n                                                  adjust_type=adjust_type)\n\n        print('{} recall done, recall_user_num={}.'.format(name, len(recall_item_dict)))\n        recall_item_list_dict[name] = recall_item_dict\n\n\n    return agg_recall_results(recall_item_list_dict, is_norm=True, ret_type=ret_type)","dfe069ad":"sim_pair_dict = get_multi_source_sim_dict_results(train_df, recall_methods={'item-cf', 'bi-graph', 'user-cf', 'swing'})\n\nuser_item_time_dict = get_user_item_time_dict(train_df)\ninfer_recall_user_item_dict = do_multi_recall_results(sim_pair_dict, user_item_time_dict, \n                                                target_user_ids=test_df.user_id.values,\n                                                ret_type='tuple',)","6e153e8a":"!pip install deepctr==0.7.5\n!pip uninstall -y tensorflow\n!pip install tensorflow==1.14.0\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom deepctr.models import DeepFM\nfrom deepctr.models.din import DIN\nfrom deepctr.inputs import SparseFeat, DenseFeat, get_feature_names, VarLenSparseFeat\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom tensorflow.python.keras.models import Model, load_model, save_model\n\nfrom deepctr.layers import custom_objects\nimport tensorflow as tf\n#tf.set_random_seed(1234)","3b7ca176":"ranking_train_label = train_df.groupby('user_id').tail(1)\nranking_train_df = train_df.append(ranking_train_label).drop_duplicates(subset=['user_id','item_id','time'])\nranking_train_ids = train_df['user_id'].unique()\nranking_train_ids","b8082535":"ranking_user_recall_item_dict = do_multi_recall_results(sim_pair_dict, user_item_time_dict, \n                                                target_user_ids=ranking_train_ids,\n                                                ret_type='tuple',)","f503f187":"import time\nt = (2020, 4, 10, 0, 0, 0, 0, 0, 0)\ntime_end = time.mktime(t)\nmax_day, max_hour, max_miniute = 7, 24, 60\n\ndef time_info(time_delta):\n    import time\n    timestamp = time_end * time_delta\n    struct_time = time.gmtime(timestamp)\n    day, hour, mini = struct_time.tm_wday+1, struct_time.tm_hour+1, struct_time.tm_min+1\n    return (day, hour, mini)\n\ndef obtain_user_hist_feat(u, user_item_dict):\n    user_hist_seq = [i for i, t in user_item_dict[u]]\n    user_hist_time_seq = [t for i, t in user_item_dict[u]]\n    user_hist_day_seq, user_hist_hour_seq, user_hist_min_seq = zip(*[time_info(t) for i, t in user_item_dict[u]])\n    return [user_hist_seq, user_hist_time_seq, list(user_hist_day_seq), list(user_hist_hour_seq), list(user_hist_min_seq)]\n  \ndef organize_recall_feat_each_user(u, recall_items, user_item_dict, strategy_item_sim_dict):\n    user_hist_info = obtain_user_hist_feat(u, user_item_dict)\n    # hist-item similarity with recall items\n    hist_num = 3\n    recall_items_sum_cf_sim2_hist = []\n    recall_items_max_cf_sim2_hist = []\n    recall_items_cnt_sim2_hist = []\n    \n    user_hist_items = user_item_dict[u][::-1][-hist_num:]\n    \n    for recall_i, rating in recall_items:\n        if rating > 0:\n            max_cf_sim2_hist = []\n            sum_cf_sim2_hist = []\n            cnt_sim2_hist = []\n            for hist_i, t in user_hist_items:\n                sum_sim_value = 0.0\n                max_sim_value = 0.0\n               \n                for strategy, item_sim_dict in strategy_item_sim_dict.items():\n                    strategy_sim_value = item_sim_dict.get(hist_i, {}).get(recall_i, 0.0) + item_sim_dict.get(recall_i, {}).get(hist_i, 0.0)\n                    sum_sim_value += strategy_sim_value\n                    max_sim_value = max(max_sim_value, strategy_sim_value)\n                    \n        \n      \n                sum_cf_sim2_hist.append(sum_sim_value)\n                max_cf_sim2_hist.append(max_sim_value)\n\n\n            while len(sum_cf_sim2_hist) < hist_num:\n                sum_cf_sim2_hist.append(0.0)\n                max_cf_sim2_hist.append(0.0)\n\n                \n        else:\n            sum_cf_sim2_hist = [0.0] * hist_num\n            max_cf_sim2_hist = [0.0] * hist_num\n\n\n        recall_items_sum_cf_sim2_hist.append(sum_cf_sim2_hist)\n        recall_items_max_cf_sim2_hist.append(max_cf_sim2_hist)\n    #print(recall_items_sum_cf_sim2_hist,recall_items_max_cf_sim2_hist)\n    recom_item = []\n    #print('recall items: ',recall_items, '\\n','recall_items_sum_cf_sim2_hist: ', recall_items_sum_cf_sim2_hist, '\\n', 'recall_items_max_cf_sim2_hist:' ,recall_items_max_cf_sim2_hist)\n    for item_rating, sum_cf_sim2_hist, max_cf_sim2_hist in zip(recall_items, recall_items_sum_cf_sim2_hist, recall_items_max_cf_sim2_hist):\n        recom_item.append([u, item_rating[0], item_rating[1]] + sum_cf_sim2_hist + max_cf_sim2_hist + user_hist_info)\n\n    return recom_item\n\ndef organize_recall_feat(recall_item_dict, user_item_hist_dict, item_sim_dict):\n    recom_columns = ['user_id', 'item_id', 'sim'] + \\\n                      ['sum_sim2int_1', 'sum_sim2int_2', 'sum_sim2int_3'] + \\\n                             ['max_sim2int_1', 'max_sim2int_2', 'max_sim2int_3']  + \\\n                          ['hist_item_id', 'hist_time', 'hist_day_id', 'hist_hour_id', 'hist_minute_id']\n    recom_item = []\n    for u, recall_items in recall_item_dict.items():\n       \n        recom_item.extend(organize_recall_feat_each_user(u, recall_items, user_item_hist_dict, item_sim_dict))\n\n    recall_recom_df = pd.DataFrame(recom_item, columns=recom_columns)\n    recall_recom_df['sim_rank_score'] = recall_recom_df.groupby('user_id')['sim'].rank(method='first', ascending=True) \/ topk_num\n    \n    return recall_recom_df","fa99d0f2":"basic_columns = ['user_id','item_id', 'label', ]\ntime_columns = ['time', 'day_id', 'hour_id', 'minute_id']\nhist_columns = ['hist_item_id', 'hist_time', 'hist_day_id', 'hist_hour_id', 'hist_minute_id',]\nsim_columns = ['sim', 'sum_sim2int_1', 'sum_sim2int_2', 'sum_sim2int_3'] + \\\n                             ['max_sim2int_1', 'max_sim2int_2', 'max_sim2int_3', 'sim_rank_score']  \n\nuse_columns =  basic_columns + hist_columns + sim_columns + time_columns\n\ndef organize_label_interact_feat_df(click_last_df, click_last_recall_recom_df, is_consider_cold_start=False):\n    dfm_df = pd.merge(click_last_recall_recom_df, click_last_df[['user_id', 'item_id', 'time']], on=['user_id', 'item_id'], how='left') \n    dfm_df['label'] = dfm_df['time'].apply(lambda x: 0.0 if np.isnan(x) else 1.0) # time\u975e\u7a7a\u4ee3\u8868\u8be5click-item\u88ab\u53ec\u56de\u4e86\n    del dfm_df['time']\n\n    # merge_time\n    click_last_df['day_id'],  click_last_df['hour_id'], click_last_df['minute_id'] = zip(*click_last_df['time'].apply(time_info))\n    dfm_df = pd.merge(dfm_df, click_last_df[['user_id', 'time', 'day_id', 'hour_id', 'minute_id']], on='user_id', how='left')\n\n\n    # click_last_df\u91cc\u5934\u6709\u4e9b\u7528\u6237\u7684\u70b9\u51fb\u6ca1\u6709\u53ec\u56de\u5230\uff0c\u5373\uff1a\u5168\u90e8\u4e3a\u8d1f\u6837\u672c\uff0c\u5bfc\u81f4\u4e0b\u91c7\u6837\u65f6\uff0c\u8fd9\u4e9b\u7528\u6237\u7684\u8d1f\u6837\u672c\u53ef\u80fd\u5168\u88ab\u4e0b\u91c7\u6837\u6389\u4e86\uff0c\u5bfc\u81f4\u8fd9\u4e9b\u7528\u6237id\u4e22\u5931\uff1b\n    # item\u540c\u7406\u3002\u7528\u6237\u771f\u5b9e\u70b9\u51fb\u7684item\u53ef\u80fd\u6ca1\u6709\u53ec\u56de\u5230\u3002\n    dfm_df = downsample_by_user(dfm_df)\n    dfm_df = dfm_df[use_columns]\n\n    # cold_start_item\u76f4\u63a5\u6cc4\u9732, \u8fd9\u4e9bitem\u53ef\u80fd\u5728infer\u9636\u6bb5\u88abrecall\u5230\uff0c\u5bfc\u81f4item_id\u7f3a\u5931\n    cold_start_items = set(click_last_df['item_id'].unique()) - set(dfm_df['item_id'].unique())\n    if is_consider_cold_start and len(cold_start_items) > 0:\n        click_last_cold_start_df = click_last_df[click_last_df['item_id'].isin(cold_start_items)]\n        click_last_cold_start_df['label'] = 1.0\n        for sim_col in sim_columns:\n            mean_value = dfm_df[dfm_df['label'] == 1.0][sim_col].mean()\n            print('sim_col={}, mean_value={}'.format(sim_col, mean_value))\n            click_last_cold_start_df[sim_col] = mean_value\n        click_last_cold_start_df = pd.merge(click_last_cold_start_df, dfm_df[['user_id',] + hist_columns], on='user_id', how='left')\n    \n        print('cold_start_item_num={}, hit_last_cold_start_df_num={}'.format(len(cold_start_items), len(click_last_cold_start_df)))\n        dfm_df = dfm_df.append(click_last_cold_start_df[use_columns])\n\n    return dfm_df\n\ndef downsample_by_user(df, percent=10):\n    '''\n    percent:\u591a\u6570\u7c7b\u522b\u4e0b\u91c7\u6837\u7684\u6570\u91cf\u76f8\u5bf9\u4e8e\u5c11\u6570\u7c7b\u522b\u6837\u672c\u6570\u91cf\u7684\u6bd4\u4f8b\n    '''\n    data_pos = df[df['label'] != 0]\n    data_neg = df[df['label'] == 0]\n\n    def group_neg_sample_func(group_df):\n        total_neg_num = len(group_df)\n        sample_num = max(int(total_neg_num * 0.002), 1) # \u6709\u4e9b\u7528\u6237\u53ec\u56de\u7684\u6570\u91cf\u4e0d\u8db3, \u53d61\u4e2a\n        sample_num = min(sample_num, 5)\n        return group_df.sample(n=sample_num, replace=True)\n\n    data_u_neg = data_neg.groupby('user_id', group_keys=False).apply(group_neg_sample_func) # # \u4fdd\u8bc1user\u5168\u8986\u76d6\n    data_i_neg = data_neg.groupby('item_id', group_keys=False).apply(group_neg_sample_func) # \u4fdd\u8bc1item\u5168\u8986\u76d6\n    data_neg = data_u_neg.append(data_i_neg)\n    data_neg = data_neg.sort_values(['user_id', 'sim']).drop_duplicates(['user_id', 'item_id'], keep='last')\n\n    data = pd.concat([data_neg, data_pos], ignore_index=True)\n    data = data.sample(frac=1.0)\n    return data\n","52f33cf9":"def process_item_feat(item_feat_df):\n    processed_item_feat_df = item_feat_df.copy()\n    txt_dense_feat = ['txt_embed_'+str(i) for i in range(128)] \n    img_dense_feat = ['img_embed_'+str(i) for i in range(128)]\n    dense_feat = txt_dense_feat + img_dense_feat\n    # norm\n    txt_item_feat_np = processed_item_feat_df[txt_dense_feat].values\n    img_item_feat_np = processed_item_feat_df[img_dense_feat].values\n    txt_item_feat_np = txt_item_feat_np \/ np.linalg.norm(txt_item_feat_np, axis=1, keepdims=True)\n    img_item_feat_np = img_item_feat_np \/ np.linalg.norm(img_item_feat_np, axis=1, keepdims=True)\n    processed_item_feat_df[txt_dense_feat] = pd.DataFrame(txt_item_feat_np, columns=txt_dense_feat)\n    processed_item_feat_df[img_dense_feat] = pd.DataFrame(img_item_feat_np, columns=img_dense_feat)\n\n    # item_feat_dict = dict(zip(processed_item_feat_df['item_id'], processed_item_feat_df[dense_feat].values))\n    return processed_item_feat_df, dense_feat\n\ndef process_user_feat(user_feat_df):\n    # sparse encoder\n    user_sparse_feat = ['age_level','gender','city_level']\n    return user_feat_df, user_sparse_feat","692532f4":"def sparse_feat_fit(total_click):\n    global feat_lbe_dict, item_raw_id2_idx_dict, user_raw_id2_idx_dict\n    \n    from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n    # sparse features one-hot\n    feat_lbe_dict = {}\n    for feat in sparse_feat:\n        if feat in time_feat: continue\n        lbe = LabelEncoder()\n        lbe.fit(total_click[feat].astype(str))\n        feat_lbe_dict[feat] = lbe\n    \n    item_raw_id2_idx_dict = dict(zip(feat_lbe_dict['item_id'].classes_, \n                     feat_lbe_dict['item_id'].transform(feat_lbe_dict['item_id'].classes_)+1, )) # \u5f97\u5230\u5b57\u5178\n    user_raw_id2_idx_dict = dict(zip(feat_lbe_dict['user_id'].classes_, \n                     feat_lbe_dict['user_id'].transform(feat_lbe_dict['user_id'].classes_)+1, )) # \u5f97\u5230\u5b57\u5178\n    \n\ndef sparse_feat_transform(df):\n    df['hist_item_id'] = df['hist_item_id'].apply(lambda seq: [item_raw_id2_idx_dict[str(x)] for x in seq])\n    df['seq_length'] = df['hist_item_id'].apply(lambda hist: min(max_seq_len, len(hist)))\n    df['seq_weight'] = df['hist_item_id'].apply(lambda hist: [0.9**(len(hist)-loc) for loc, item in enumerate(hist)])\n\n    for hist_id in var_len_feat: \n        df[hist_id] = tf.keras.preprocessing.sequence.pad_sequences(df[hist_id], \n                                                  value=0, maxlen=max_seq_len, truncating='pre', padding='post').tolist()\n        \n    df['seq_weight'] = tf.keras.preprocessing.sequence.pad_sequences(df['seq_weight'], \n                                                  value=0, maxlen=max_seq_len, truncating='pre', padding='post', dtype=np.float32).tolist()\n    df['seq_weight'] = df['seq_weight'].apply(lambda weights: [[w] for w in weights])\n    \n    for feat in sparse_feat:\n        print(feat)\n        if feat in time_feat: continue\n        df[feat] = feat_lbe_dict[feat].transform(df[feat].astype(str))+1\n    return df\n","020e63c1":"def fillna(df, sparse_feat, dense_feat, var_len_feat):\n    for sp in sparse_feat:\n        df[sp].fillna('-1', inplace=True)\n    \n    for ds in dense_feat:\n        df[ds].fillna(0.0, inplace=True) # all_click_user_item_df[ds].mean()\n        \n#     for ds in var_len_feat:\n#         df[ds].fillna([-1], inplace=True)\n    \n    return df\n  \ndef organize_user_item_feat(df, item_feat_df, sparse_feat, dense_feat, var_len_feat, \n                                        is_interest=True, is_w2v=False):\n    \n    full_user_item_df = pd.merge(df, item_feat_df, how='left', on='item_id')\n    full_user_item_df = fillna(full_user_item_df, sparse_feat, dense_feat, var_len_feat)\n    print('origin data done')\n  \n    if is_interest:\n        # history interest\n        full_user_item_df = obtain_user_hist_interest_feat(full_user_item_df, item_content_vec_dict)\n        print('interest done')\n    \n    if is_w2v:\n        organize_word2vec_feat(full_user_item_df, word2vec_item_embed_dict, word2vec_user_embed_dict)\n        print('word2vec done')\n\n\n    full_user_item_df = sparse_feat_transform(full_user_item_df)\n    \n    return full_user_item_df\n\n\ndef obtain_user_hist_interest_feat(full_user_item_df, item_vec_dict):\n    def weighted_agg_content(hist_item_id_list):\n\n        weighted_content = np.zeros(128*2)\n        hist_num = len(hist_item_id_list)\n        for loc, i in enumerate(hist_item_id_list):\n            loc_weight = (0.9**(hist_num-loc)) \n            if i in item_vec_dict:\n                weighted_content += loc_weight*item_vec_dict[i]\n        return weighted_content\n\n    user_interest_vec = full_user_item_df['hist_item_id'].apply(weighted_agg_content).tolist()\n    user_interest_df = pd.DataFrame(user_interest_vec, columns=['interest_'+col for col in item_dense_feat])\n    \n    full_user_item_df[user_interest_df.columns] = user_interest_df\n\n    # begin compute degree\n    target_item_vec = full_user_item_df[item_dense_feat].values\n    user_interest_vec = np.array(user_interest_vec) \n    \n    txt_interest_degree_array = target_item_vec[:, 0:128] * user_interest_vec[:, 0:128]\n    txt_interest_degree_list = np.sum(txt_interest_degree_array, axis=1)\n    full_user_item_df['txt_interest_degree'] = txt_interest_degree_list.tolist()\n    \n    img_interest_degree_array = target_item_vec[:, 128:] * user_interest_vec[:, 128:]\n    img_interest_degree_list = np.sum(img_interest_degree_array, axis=1)\n    full_user_item_df['img_interest_degree'] = img_interest_degree_list.tolist()\n    \n    full_user_item_df['interest_degree'] =  full_user_item_df['img_interest_degree']  + full_user_item_df['img_interest_degree'] \n    \n    for f in ['interest_'+col for col in item_dense_feat]+['img_interest_degree', 'img_interest_degree', 'interest_degree']:\n        full_user_item_df[f].fillna(0.0, inplace=True)\n    print('obtain user dynamic feat done')\n\n    def hist_2_target_time_diff(hist_time_list, hist_num=3):\n        target_time = hist_time_list[-1]\n        hist_time_list = hist_time_list[: -1]\n        \n        hist_time_diff = []\n        for hist_time in hist_time_list[::-1][0:hist_num]:\n            diff_time = target_time - hist_time\n            hist_time_diff.append(diff_time)\n            \n        while len(hist_time_diff) != hist_num:\n            hist_time_diff.append(0.1)\n\n        return hist_time_diff\n    hist_target_time_series = full_user_item_df['hist_time'] + full_user_item_df['time'].apply(lambda x:[x])\n    full_user_item_df['time_diff_1'], full_user_item_df['time_diff_2'], full_user_item_df['time_diff_3'] = zip(*hist_target_time_series.apply(hist_2_target_time_diff))\n    \n    return full_user_item_df\n","e105aec1":"path = '.\/input\/kdd-cup-data\/'\n\n\nuser_feat_path = path + 'underexpose_user_feat.csv'\nitem_feat_path = path + 'underexpose_item_feat.csv'","e861ffd4":"from tqdm.notebook import tqdm\nfrom collections import defaultdict  \nimport math  \n\nitem_feat_cols = ['item_id',] + ['txt_embed_'+ str(i) for i in range(128)] + ['img_embed_'+ str(i) for i in range(128)]\nitem_feat_df = pd.read_csv(item_feat_path, header=None, names=item_feat_cols)\nitem_feat_df['txt_embed_0'] = item_feat_df['txt_embed_0'].apply(lambda x:float(x[1:]))\nitem_feat_df['txt_embed_127'] = item_feat_df['txt_embed_127'].apply(lambda x:float(x[:-1]))\nitem_feat_df['img_embed_0'] = item_feat_df['img_embed_0'].apply(lambda x:float(x[1:]))\nitem_feat_df['img_embed_127'] = item_feat_df['img_embed_127'].apply(lambda x:float(x[:-1]))\nprocessed_item_feat_df, item_dense_feat = process_item_feat(item_feat_df)\nitem_content_vec_dict = dict(zip(processed_item_feat_df['item_id'], processed_item_feat_df[item_dense_feat].values))","e4e20d99":"def fill_item_feat(processed_item_feat_df, item_content_vec_dict):\n    total_click = train_df\n\n    all_click_feat_df = pd.merge(total_click, processed_item_feat_df, on='item_id', how='left')\n    # miss items\n    missed_items = all_click_feat_df[all_click_feat_df['txt_embed_0'].isnull()]['item_id'].unique()\n    user_item_time_hist_dict = get_user_item_time_dict(total_click)\n\n    # co-occurance\n    co_occur_dict = {}\n    window = 5\n\n    def cal_occ(sentence):\n        for i, word in enumerate(sentence):\n            hist_len = len(sentence)\n            co_occur_dict.setdefault(word, {})\n            for j in range(max(i - window, 0), min(i + window, hist_len)):\n                if j == i or word == sentence[j]: continue\n                loc_weight = (0.9 ** abs(i - j))\n                co_occur_dict[word].setdefault(sentence[j], 0)\n                co_occur_dict[word][sentence[j]] += loc_weight\n\n    for u, hist_item_times in user_item_time_hist_dict.items():\n        hist_items = [i for i, t in hist_item_times]\n        cal_occ(hist_items)\n\n    # fill\n    miss_item_content_vec_dict = {}\n    for miss_item in missed_items:\n        co_occur_item_dict = co_occur_dict[miss_item]\n        weighted_vec = np.zeros(256)\n        sum_weight = 0.0\n        for co_item, weight in co_occur_item_dict.items():\n\n            if co_item in item_content_vec_dict:\n                sum_weight += weight\n                co_item_vec = item_content_vec_dict[co_item]\n                weighted_vec += weight * co_item_vec\n\n        weighted_vec \/= sum_weight\n        txt_item_feat_np = weighted_vec[0:128] \/ np.linalg.norm(weighted_vec[0:128])\n        img_item_feat_np = weighted_vec[128:] \/ np.linalg.norm(weighted_vec[128:])\n        cnt_vec = np.concatenate([txt_item_feat_np, img_item_feat_np])\n        miss_item_content_vec_dict[miss_item] = cnt_vec\n\n    miss_item_feat_df = pd.DataFrame()\n    miss_item_feat_df[item_dense_feat] = pd.DataFrame(miss_item_content_vec_dict.values(),\n                                                      columns=item_dense_feat)\n    miss_item_feat_df['item_id'] = list(miss_item_content_vec_dict.keys())\n    miss_item_feat_df = miss_item_feat_df[['item_id'] + item_dense_feat]\n\n    return miss_item_feat_df, miss_item_content_vec_dict\n\n\ndef obtain_entire_item_feat_df():\n    global item_feat_df\n    processed_item_feat_df = process_item_feat(item_feat_df)\n    item_content_vec_dict = dict(zip(processed_item_feat_df['item_id'], processed_item_feat_df[item_dense_feat].values))\n    miss_item_feat_df, miss_item_content_vec_dict = fill_item_feat(processed_item_feat_df, item_content_vec_dict)\n    processed_item_feat_df = processed_item_feat_df.append(miss_item_feat_df)\n    processed_item_feat_df = processed_item_feat_df.reset_index(drop=True)\n    item_content_vec_dict.update(miss_item_content_vec_dict)\n    return processed_item_feat_df, item_content_vec_dict","e29d7563":"is_fill_missing = True\nif is_fill_missing:\n    miss_item_feat_df, miss_item_content_vec_dict = fill_item_feat(processed_item_feat_df, item_content_vec_dict)\n    processed_item_feat_df = processed_item_feat_df.append(miss_item_feat_df)\n    processed_item_feat_df = processed_item_feat_df.reset_index(drop=True)\n    item_content_vec_dict.update(miss_item_content_vec_dict)","4f47e890":"max_seq_len = 10\ntime_feat = ['day_id', 'hour_id'] #, 'minute_id']  # no need to sparse encoder\ntime_vocab_map = {'day_id': max_day, 'minute_id': max_miniute, 'hour_id': max_hour}\n\nsparse_feat = ['user_id', 'item_id',] + time_feat\nuser_interest_dense_feat = ['interest_'+col for col in item_dense_feat] + ['interest_degree', 'txt_interest_degree', 'img_interest_degree',]\n\nsim_dense_feat = ['sim', 'sum_sim2int_1', 'sum_sim2int_2', 'sum_sim2int_3'] + \\\n                             ['max_sim2int_1', 'max_sim2int_2', 'max_sim2int_3', 'sim_rank_score'] \n\nhist_time_diff_feat = ['time_diff_1', 'time_diff_2', 'time_diff_3']\n                               \ndense_feat = item_dense_feat  +  sim_dense_feat # + item_statistic_feat\nvar_len_feat = ['hist_item_id'] +  ['hist_{}'.format(feat) for feat in time_feat]","6c55a1fb":"sparse_feat_fit(train_df)","b103a426":" item_raw_id2_idx_dict, user_raw_id2_idx_dict","5192c990":"def convert(click_history_df,user_item_time_dict, user_recall_item_dict, sim_pair_dict, test_df):\n\n    # organize recall interact feat\n    click_last_recall_recom_df = organize_recall_feat(user_recall_item_dict, user_item_time_dict,\n                                                          sim_pair_dict)\n    \n    train_full_df = organize_label_interact_feat_df(test_df, click_last_recall_recom_df)\n    return train_full_df","517812af":"ranking_user_recall_item_dict","6fa19d91":"click_last_recall_recom_df = organize_recall_feat(ranking_user_recall_item_dict, user_item_time_dict,\n                                                          sim_pair_dict)","415bc253":"train_full_df = convert(ranking_train_df, user_item_time_dict,ranking_user_recall_item_dict,sim_pair_dict,ranking_train_label)","48a75561":"sparse_feat","2ee1346e":"train_final_df = organize_user_item_feat(train_full_df, processed_item_feat_df, \n                                         sparse_feat, dense_feat, var_len_feat, is_w2v=False, is_interest=True)","0291715e":"train_final_df","a22a8aea":"item_cnt = len(item_raw_id2_idx_dict)\nitem_embed_np = np.zeros((item_cnt+1, 256))\nfor raw_id, idx in item_raw_id2_idx_dict.items():\n    vec = item_content_vec_dict[int(raw_id)]\n    item_embed_np[idx, :] = vec\n# np.save(open(sr_gnn_dir + '\/data\/item_embed_mat.npy', 'wb'), item_embed_np)","25339485":"def get_init_user_embed(full_df):\n    global user_embed_np\n\n    user_item_time_hist_dict = get_user_item_time_dict(full_df)\n\n    def weighted_agg_content(hist_item_id_list):\n        weighted_vec = np.zeros(128*2)\n        hist_num = len(hist_item_id_list)\n        sum_weight = 0.0\n        for loc, (i,t) in enumerate(hist_item_id_list):\n            loc_weight = (0.9**(hist_num-loc)) \n            if i in item_content_vec_dict:\n                sum_weight += loc_weight\n                weighted_vec += loc_weight*item_content_vec_dict[i]\n        if sum_weight != 0:\n            weighted_vec \/= sum_weight\n            txt_item_feat_np = weighted_vec[0:128] \/ np.linalg.norm(weighted_vec[0:128])\n            img_item_feat_np = weighted_vec[128:] \/ np.linalg.norm(weighted_vec[128:])\n            weighted_vec = np.concatenate([txt_item_feat_np,  img_item_feat_np])\n        else:\n            print('zero weight...')\n        return weighted_vec\n    user_cnt = len(user_raw_id2_idx_dict)\n    user_embed_np = np.zeros((user_cnt+1, 256))\n    for raw_id, idx in user_raw_id2_idx_dict.items():\n        if int(raw_id) in user_item_time_hist_dict:\n            hist = user_item_time_hist_dict[int(raw_id)]\n            vec = weighted_agg_content(hist)\n            user_embed_np[idx, :] = vec\n    # np.save(open(sr_gnn_dir + '\/data\/user_embed_mat.npy', 'wb'), user_embed_np)\n","bd3efa59":"from tensorflow.python.keras.initializers import RandomNormal, Constant\nfrom deepctr.inputs import  build_input_features,create_embedding_matrix,SparseFeat,VarLenSparseFeat,DenseFeat,embedding_lookup,get_dense_input,varlen_embedding_lookup,get_varlen_pooling_list,combined_dnn_input\nfrom tensorflow.python.keras.layers import Embedding, Input, Flatten\nfrom tensorflow.python.keras.regularizers import l2\n\ndef kdd_create_embedding_matrix(feature_columns, l2_reg, init_std, seed, prefix=\"\", seq_mask_zero=True):\n    sparse_feature_columns = list(\n        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n    varlen_sparse_feature_columns = list(\n        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n    sparse_emb_dict = kdd_create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, init_std, seed,\n                                            l2_reg, prefix=prefix + 'sparse', seq_mask_zero=seq_mask_zero)\n    return sparse_emb_dict\n\n\ndef kdd_create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, init_std, seed, l2_reg,\n                          prefix='sparse_', seq_mask_zero=True):\n    sparse_embedding = {}\n    for feat in sparse_feature_columns:\n        embed_initializer = RandomNormal(mean=0.0, stddev=init_std, seed=seed)\n        if feat.embedding_name == 'user_id':\n            print('init user embed')\n            embed_initializer = Constant(user_embed_np)\n        if feat.embedding_name == 'item_id':\n            print('init item embed')\n            embed_initializer = Constant(item_embed_np)\n        sparse_embedding[feat.embedding_name] = Embedding(feat.vocabulary_size, feat.embedding_dim,\n                                                                       embeddings_initializer=embed_initializer,\n#                                                                        embeddings_regularizer=l2(l2_reg),\n                                                                       name=prefix + '_emb_' + feat.embedding_name)\n\n    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n        for feat in varlen_sparse_feature_columns:\n            embed_initializer = RandomNormal(mean=0.0, stddev=init_std, seed=seed)\n            if feat.embedding_name == 'user_id':\n                print('init user embed')\n                embed_initializer = Constant(user_embed_np)\n            if feat.embedding_name == 'item_id':\n                print('init item embed')\n                embed_initializer = Constant(item_embed_np)\n            sparse_embedding[feat.embedding_name] = Embedding(feat.vocabulary_size, feat.embedding_dim,\n                                                              embeddings_initializer=embed_initializer,\n#                                                               embeddings_regularizer=l2(l2_reg),\n                                                              name=prefix + '_seq_emb_' + feat.name,\n                                                              mask_zero=seq_mask_zero)\n    return sparse_embedding","e1a4965e":"# -*- coding:utf-8 -*-\nfrom tensorflow.python.keras.layers import Dense,Concatenate, Flatten\nfrom tensorflow.python.keras.models import Model\n\nfrom deepctr.inputs import  build_input_features,create_embedding_matrix,SparseFeat,VarLenSparseFeat,DenseFeat,embedding_lookup,get_dense_input,varlen_embedding_lookup,get_varlen_pooling_list,combined_dnn_input\nfrom deepctr.layers.core import DNN, PredictionLayer\nfrom deepctr.layers.sequence import AttentionSequencePoolingLayer\nfrom deepctr.layers.utils import concat_func, NoMask\n\n\ndef KDD_DIN(dnn_feature_columns, history_feature_list, dnn_use_bn=False,\n        dnn_hidden_units=(200, 80), dnn_activation='relu', att_hidden_size=(80, 40), att_activation=\"dice\",\n        att_weight_normalization=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, init_std=0.0001, seed=1024,\n        task='binary'):\n    \"\"\"Instantiates the Deep Interest Network architecture.\n\n    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n    :param history_feature_list: list,to indicate  sequence sparse field\n    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in deep net\n    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net\n    :param dnn_activation: Activation function to use in deep net\n    :param att_hidden_size: list,list of positive integer , the layer number and units in each layer of attention net\n    :param att_activation: Activation function to use in attention net\n    :param att_weight_normalization: bool.Whether normalize the attention score of local activation unit.\n    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n    :param init_std: float,to use as the initialize std of embedding vector\n    :param seed: integer ,to use as random seed.\n    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n    :return: A Keras model instance.\n\n    \"\"\"\n\n\n    features = build_input_features(dnn_feature_columns)\n\n    sparse_feature_columns = list(filter(lambda x:isinstance(x,SparseFeat),dnn_feature_columns)) if dnn_feature_columns else []\n    dense_feature_columns = list(\n        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n\n\n    history_feature_columns = []\n    sparse_varlen_feature_columns = []\n    history_fc_names = list(map(lambda x: \"hist_\" + x, history_feature_list))\n    for fc in varlen_sparse_feature_columns:\n        feature_name = fc.name\n        if feature_name in history_fc_names:\n            history_feature_columns.append(fc)\n        else:\n            sparse_varlen_feature_columns.append(fc)\n\n\n    inputs_list = list(features.values())\n\n\n    embedding_dict = kdd_create_embedding_matrix(dnn_feature_columns, l2_reg_embedding, init_std, seed, prefix=\"\")\n\n\n    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns, history_feature_list,\n                                      history_feature_list,to_list=True)\n    keys_emb_list = embedding_lookup(embedding_dict, features, history_feature_columns, history_fc_names,\n                                     history_fc_names,to_list=True)\n    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,\n                                          mask_feat_list=history_feature_list,to_list=True)\n    dense_value_list = get_dense_input(features, dense_feature_columns)\n\n    sequence_embed_dict = varlen_embedding_lookup(embedding_dict,features,sparse_varlen_feature_columns)\n    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, sparse_varlen_feature_columns,to_list=True)\n\n    dnn_input_emb_list += sequence_embed_list\n\n\n    keys_emb = concat_func(keys_emb_list, mask=True)\n    deep_input_emb = concat_func(dnn_input_emb_list)\n    query_emb = concat_func(query_emb_list, mask=True)\n    hist = AttentionSequencePoolingLayer(att_hidden_size, att_activation,\n                                         weight_normalization=att_weight_normalization, supports_masking=True)([\n        query_emb, keys_emb])\n\n    deep_input_emb = Concatenate()([NoMask()(deep_input_emb), hist])\n    deep_input_emb = Flatten()(deep_input_emb)\n    dnn_input = combined_dnn_input([deep_input_emb],dense_value_list)\n    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn,\n                 dnn_dropout, dnn_use_bn, seed)(dnn_input)\n    final_logit = Dense(1, use_bias=False)(output)\n\n    output = PredictionLayer(task)(final_logit)\n\n    model = Model(inputs=inputs_list, outputs=output)\n    return model\n\n","0ea39896":"HIDDEN_SIZE = (128, 128)\nBATCH_SIZE = 1024\nEPOCH = 1\nEMBED_DIM = 256\nTIME_EMBED_DIM = 16\ndef generate_din_feature_columns(data, sparse_features, dense_features, use_time_feat):\n    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=len(feat_lbe_dict[feat].classes_)+1, embedding_dim=EMBED_DIM)\n                              for i, feat in enumerate(sparse_features) if feat not in time_feat]\n\n    sparse_feature_columns += [SparseFeat(feat, vocabulary_size=time_vocab_map[feat]+1, embedding_dim=TIME_EMBED_DIM)\n                              for i, feat in enumerate(use_time_feat)]\n    \n\n    dense_feature_columns = [DenseFeat(feat, 1, ) for feat in dense_features]\n\n    var_feature_columns = [VarLenSparseFeat(SparseFeat('hist_item_id', vocabulary_size=len(feat_lbe_dict['item_id'].classes_)+1,\n                                                       embedding_dim=EMBED_DIM, embedding_name='item_id'), \n                                                       maxlen=max_seq_len)]\n\n    var_feature_columns += [VarLenSparseFeat(SparseFeat('hist_{}'.format(feat), vocabulary_size=time_vocab_map[feat]+1,\n                                              embedding_dim=TIME_EMBED_DIM,embedding_name=feat), maxlen=max_seq_len) \n                                                    for i, feat in enumerate(use_time_feat)]\n    # DNN side\n    dnn_feature_columns = sparse_feature_columns + dense_feature_columns + var_feature_columns\n    # FM side\n    linear_feature_columns = sparse_feature_columns + dense_feature_columns + var_feature_columns\n    # all feature names\n    feature_names = get_feature_names(dnn_feature_columns+linear_feature_columns)\n\n    return feature_names, linear_feature_columns, dnn_feature_columns","0378f5c3":"feature_names, linear_feature_columns, dnn_feature_columns = generate_din_feature_columns(train_final_df, ['user_id', 'item_id'], \n                                                                                          dense_features=item_dense_feat+sim_dense_feat+hist_time_diff_feat+user_interest_dense_feat,\n                                                                                          use_time_feat=[])","7e31903f":"train_input = {name: np.array(train_final_df[name].values.tolist()) for name in feature_names}\ntrain_label = train_final_df['label'].values","e67a4f46":"get_init_user_embed(ranking_train_df)","98572114":"behavior_feature_list = ['item_id']\nmodel = KDD_DIN(dnn_feature_columns, behavior_feature_list, dnn_hidden_units=HIDDEN_SIZE,\n                att_hidden_size=(128, 64), att_weight_normalization=True,)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=3e-4), loss=\"binary_crossentropy\",\n                  metrics=['binary_crossentropy', tf.keras.metrics.AUC()], )","5c5b9612":"model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=EPOCH,\n              verbose=1 ) # epoch. 0.8912 -> 0.8918 -> 0.8881","fc98634b":"def get_recall_predict(infer_recall_df):\n    topk_fill_items  = online_top50_click if mode == 'online' else offline_top50_click\n    result = get_predict(infer_recall_df, 'sim')\n    result.to_csv(output_path + '\/baseline_recall_v1_phase_{}.csv'.format(phase), index=False, header=None)\n    return result\n\ndef get_rank_predict(dfm_infer_call_df, infer_target_uids=None, rating_col='prob'):\n    dfm_infer_call_df = dfm_infer_call_df.copy()\n    fake_item = dfm_infer_call_df['item_id'].iloc[0]\n    infer_users = set(dfm_infer_call_df['user_id'].unique())\n  \n    if infer_target_uids is None:\n        infer_target_uids = infer_users\n\n    for uid in infer_target_uids:\n        if uid not in infer_users:\n            print('uid={} not in infer_users'.format(uid))\n            dfm_infer_call_df = dfm_infer_call_df.append({'user_id': uid, 'item_id': fake_item, 'prob': -10000}, ignore_index=True)\n\n    dfm_infer_call_df['user_id'] = dfm_infer_call_df['user_id'].astype(int)\n    dfm_infer_call_df['item_id'] = dfm_infer_call_df['item_id'].astype(int)\n    \n    \n    result = get_predict(dfm_infer_call_df, 'sim')\n    return dfm_infer_call_df, result","665f7a86":"def inference_df_generate(sim_pair_dict, user_item_time_dict,infer_recall_user_item_dict):\n\n\n    test_df = organize_recall_feat(infer_recall_user_item_dict, user_item_time_dict, \n                                                                  sim_pair_dict)\n\n    test_df['time'] = 0.95\n    test_df['day_id'], test_df['hour_id'], test_df['minute_id'] = zip(*test_df['time'].apply(time_info))\n\n    infer_final_df = organize_user_item_feat(test_df, processed_item_feat_df,\n                                          sparse_feat, dense_feat, var_len_feat,is_w2v=False, is_interest=True)\n    \n    \n    return infer_final_df","e7aa9b4a":"infer_df = inference_df_generate(sim_pair_dict,user_item_time_dict,infer_recall_user_item_dict)","1463ff8c":"# din ranking results\ninfer_input = {name: np.array(infer_df[name].values.tolist()) for name in feature_names}\ndin_infer_ans = model.predict(infer_input, batch_size=BATCH_SIZE)\ninfer_df['prob'] = din_infer_ans","0ec01e38":"infer_recall_df, rank_res = get_rank_predict(infer_df, rating_col='prob')","27e07c1b":"class metrics:\n\n    @classmethod\n    def ndcg(cls, label_id, recommend_ids, topK=50):\n        \"\"\"\n        Evaluate the recommended list's relevance to true click. Relevance score is binary (0 or 1) \n        :param label_id: ground-truth next-click item, shape is [None, 1]\n        :param recommend_ids: matched (or ranked) item list, shape is [None, num_items]\n        :param topK\n        :return : mean NDCG score (average over all users), hit rate (over all users) \n        \"\"\"\n        if len(label_id.shape) < 2:\n            label_id = label_id.reshape((-1, 1))\n        assert topK <= recommend_ids.shape[1], \"Recommend list not long enough to cover topK(%d) items\" % topK\n        recommend_ids = recommend_ids[:, :topK]\n        hit_onehot = (label_id == recommend_ids).astype(int)\n        hit_counter = np.sum(hit_onehot, axis=1)\n        hit_rate = np.mean(hit_counter)\n\n        hit_pos = np.argmax(hit_onehot, axis=1)\n        ndcg_per_user = hit_counter \/ np.log2(hit_pos + 2)\n        ndcg_mean = np.mean(ndcg_per_user)\n        return ndcg_mean, hit_rate","446ede97":"rank_res","806ea699":"## ranking model","512c75c7":"### running to prepare data","38d67bcc":"### organize raw user-item feat ","4f9f68ce":"# Recall\nThe code of this part is well checked, you can just run step by step.","b3dd1b21":"## User-CF","f25e309e":"### aggregate multi-recall sources","bf93ff12":"### organize recall feat","87cd9093":"### generate CF results","f1fe9ac3":"## Common Data Preprocessing","84e89fb2":"### recall one source","fc2a9c21":"### covert data format","073f2603":"### organize label ","014ccfa3":"## Swing","dc8b9bf4":"## generate recommend result","a1ad8446":"## ranking data","3a0f09c3":"### do recall \n\n**single thread version, may consume more time**","873ead10":"## construct ranking training data","d0646f9a":"# Setting\n\n\u8bbe\u7f6e\u5168\u5c40\u7684\u914d\u7f6e\uff0c\u4e3b\u8981\u5305\u62ecmode\u3001data_path\u7b49","94bb8eb9":"## Bi-Graph\nReferences: A Simple Recall Method based on Network-based Inference, score:0.18 (phase0-3): https:\/\/tianchi.aliyun.com\/forum\/postDetail?postId=104936","ae612a2d":"### DIN\n\nReferences: DeepCTR, Easy-to-use,Modular and Extendible package of deep-learning based CTR models, https:\/\/github.com\/shenweichen\/DeepCTR","2dd846dd":"## Item-CF\n\nreferences: \n- A simple itemCF Baseline, score:0.1169(phase0-2): https:\/\/tianchi.aliyun.com\/forum\/postDetail?postId=103530\n- \u6539\u8fdb\u9752\u79b9\u5c0f\u751fbaseline\uff0cphase3\u7ebf\u4e0a0.2: https:\/\/tianchi.aliyun.com\/forum\/postDetail?postId=105787","09557de1":"# get recall result ","6906ea36":"# Ranking\n"}}