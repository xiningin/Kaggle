{"cell_type":{"b04d2229":"code","35290a04":"code","7105842e":"code","ed050d79":"code","44158174":"markdown"},"source":{"b04d2229":"import numpy as np \nimport pandas as pd\nimport os\n\nimport pandas as pd\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom itertools import combinations\n","35290a04":"class CFG:\n    data_folder = '..\/input\/jigsaw-toxic-severity-rating\/'\n    nfolds = 10","7105842e":"# ruddit\ndf = pd.read_csv('..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv')[['comment_id', 'txt', 'offensiveness_score']]\ndf = df.loc[(df.txt != '[deleted]') & (df.txt != '[removed]')]\ndf.head(5)","ed050d79":"# create a vector of combinations\nid1 = []\nid2 = []\n\nfor f in combinations(range(df.shape[0]),2):\n\n    id1.append(f[0])\n    id2.append(f[1])\n    \nindices_df = pd.DataFrame(id1, columns=['id1'])\nindices_df['id2'] = id2\n\n# shuffle\nindices_df = indices_df.sample(frac=1).reset_index(drop=True)\n\n# map to texts and scores\nx1 = df.iloc[indices_df.id1][['txt', 'offensiveness_score']].rename(columns={\"txt\": \"txt1\", \"offensiveness_score\": \"sc1\"}).reset_index(drop=True)\nx2 = df.iloc[indices_df.id2][['txt', 'offensiveness_score']].rename(columns={\"txt\": \"txt2\", \"offensiveness_score\": \"sc2\"}).reset_index(drop=True)\n\n# combine\nx3 = pd.concat([x1,x2], axis = 1, ignore_index = True)\nx3.columns = ['txt1', 'sc1', 'txt2', 'sc2']\n\n# dump to file\nx3.to_csv('ruddit_pairs.csv', index = False)","44158174":"# Data\n\nCreate all pairs for the Ruddit dataset - we preserve the original scores, so that a difference (pointing which one is more toxic) can be adjusted at modeling time. "}}