{"cell_type":{"7a76351a":"code","bc593025":"code","4d73b43b":"code","214632db":"code","a38fa849":"code","2c6f4915":"code","8188f89a":"code","2f3d36de":"code","90dee479":"code","102c4126":"code","6c3316b4":"code","d4fe2523":"code","66e090f0":"code","c5118e29":"code","738a6f53":"code","c134e1a5":"code","e8f015a7":"code","a1b684a3":"code","30cb0e8e":"code","2b62cf00":"code","013281bd":"code","d0b41e6b":"code","8797ec50":"code","d02de929":"code","e6a45cae":"code","75b08d09":"code","11bb3086":"code","fc23c400":"code","9ffa488a":"code","6da538de":"code","6c8fa49b":"code","dccf7e2a":"code","44a585c5":"code","441ea2b4":"code","9265b404":"code","df4e547b":"code","0c8ba753":"code","049d03f3":"code","22c1be0e":"code","e560ade2":"code","dea03c87":"code","b9c9918c":"code","95f4cdc5":"code","b45332bb":"code","9a1eaf13":"code","0f99af44":"code","0ba02d1e":"code","1ac1a63b":"code","5bff1c48":"code","a145fbbe":"code","e86249d9":"code","bf07e195":"code","8affa5cd":"code","e9edb92b":"code","55e5e34c":"code","67fa0754":"code","fd3af040":"markdown","6930b016":"markdown","6470dc15":"markdown","a412a2ad":"markdown","2459fb59":"markdown","931f7091":"markdown"},"source":{"7a76351a":"# Regular EDA and plotting libraries\nimport numpy as np # np is short for numpy\nimport pandas as pd # pandas is so commonly used, it's shortened to pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns # seaborn gets shortened to sns\n\n# We want our plots to appear in the notebook\n%matplotlib inline \n\n## Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n## Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","bc593025":"heart_disease = pd.read_csv('\/kaggle\/input\/scikitconf\/heart-disease.csv')\nheart_disease","4d73b43b":"heart_disease.info()","214632db":"df = heart_disease","a38fa849":"df.target.value_counts()","2c6f4915":"df.target.value_counts(normalize = True)","8188f89a":"df.target.value_counts().plot(kind='bar',color=['salmon','lightblue'])\nplt.xlabel(\"1 = disease, 0=No disease\");","2f3d36de":"# checking for missing values\ndf.isna().sum()","90dee479":"df.describe()","102c4126":"df.sex.value_counts()","6c3316b4":"# compare target column with sex column\n\npd.crosstab(df.target, df.sex)","d4fe2523":"pd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),\n                                   color=['salmon','lightblue'])\nplt.title(\"Heart disease frequency for sex\")\nplt.xlabel(\"0 = No disease, 1 = Disease\");\nplt.ylabel(\"Amount\")\nplt.legend([\"female\",\"male\"]);","66e090f0":"# compare target column with max heart rate column\nplt.figure(figsize=(10,6))\nplt.scatter(df.age[df.target==1],df.thalach[df.target==1],color='salmon')\nplt.scatter(df.age[df.target==0],df.thalach[df.target==0],color='lightblue')\nplt.title(\"Heart disease in function with age and Max heart rate\")\nplt.xlabel('Age')\nplt.ylabel('Max Heart Rate')\nplt.legend(['Disease','No Disease']);","c5118e29":"df.age.plot.hist();","738a6f53":"# compare target column with chest pain column\npd.crosstab(df.target,df.cp)\n    ","c134e1a5":"pd.crosstab(df.cp,df.target).plot(kind='bar',color=['lightblue','salmon'],figsize=(10,6))\nplt.title('Heart Disease vs Chest Pain Type Frequency')\nplt.xlabel('Chest pain type')\nplt.ylabel('Frequency')\nplt.legend(['No Disease','Disease'])\nplt.xticks(rotation=0);","e8f015a7":"# find the correlation between independent variables\n\ncorr_matrix  = df.corr()\ncorr_matrix","a1b684a3":"plt.figure(figsize=(15,11))\nsns.heatmap(corr_matrix, annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");","30cb0e8e":"df.head()","2b62cf00":"X = df.drop(\"target\",axis=1)\ny = df.target","013281bd":"X.shape , y.shape","d0b41e6b":"#splitting the data\n\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nX_train.head()","8797ec50":"y_train.head()","d02de929":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","e6a45cae":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","75b08d09":"model_compare = pd.DataFrame(model_scores , index=['accuracy'])\nmodel_compare.T.plot(kind=\"bar\")","11bb3086":"## KNN\n\ntrain_scores =[]\ntest_scores = []\nneighbors = range(1,21)\nknn = KNeighborsClassifier()\n\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","fc23c400":"train_scores","9ffa488a":"test_scores","6da538de":"plt.figure(figsize=(10,6))\nplt.plot(neighbors,train_scores,label='Train Score')\nplt.plot(neighbors,test_scores,label='Test Score')\nplt.xticks(np.arange(1,21,1))\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('model score');\nprint('max test score for KNN is {}%'.format(round(max(test_scores)*100,2)))","6c8fa49b":"# Create a hyperparameter for Logistics Regression and random Forest\nlog_reg_grid = {\"C\" : np.logspace(-4,4,20),\n           \"solver\" : [\"liblinear\"]}\nrf_grid = {'n_estimators':[10,150,300],\n            'max_depth':[30,60,90,None]}","dccf7e2a":"# tuning logisctics regression\n\nnp.random.seed(42)\n\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose =True)\nrs_log_reg.fit(X_train, y_train)","44a585c5":"rs_log_reg.best_params_","441ea2b4":"rs_log_reg.score(X_test,y_test)","9265b404":"# tuning random forest classifier\n\nnp.random.seed(42)\n\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose= True)\nrs_rf.fit(X_train, y_train)","df4e547b":"rs_rf.best_params_","0c8ba753":"rs_rf.score(X_test,y_test)","049d03f3":"# tuning logictics Regression\nlog_reg_grid = {\"C\" : np.logspace(-4,4,30),\n           \"solver\" : [\"liblinear\"]}\nnp.random.seed(42)\n\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose= True)\ngs_log_reg.fit(X_train, y_train)","22c1be0e":"gs_log_reg.best_params_","e560ade2":"gs_log_reg.score(X_test,y_test)","dea03c87":"y_preds = gs_log_reg.predict(X_test)","b9c9918c":"plot_roc_curve(gs_log_reg,X_test,y_test)","95f4cdc5":"#confusion matrix\nimport seaborn as sns\n\nsns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test,y_preds):\n    conf_mat = confusion_matrix(y_test,y_preds)\n    \n    fig,ax = plt.subplots(figsize=(4,4))\n    ax = sns.heatmap(conf_mat,annot=True,cbar=False)\n    \n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True Label\")\n\nplot_conf_mat(y_test,y_preds)","b45332bb":"# classification report ,cross-validation , precision , recall , f1 score\n\nprint(classification_report(y_test,y_preds))","9a1eaf13":"gs_log_reg.best_params_","0f99af44":"clf = LogisticRegression(C= 1.3738237958832638,\n                         solver='liblinear')","0ba02d1e":"cross_val_score_acc = cross_val_score(clf,X,y,cv=5,scoring='accuracy')\ncross_acc = np.mean(cross_val_score_acc)","1ac1a63b":"cross_val_precision = cross_val_score(clf,X,y,cv=5,scoring='precision')\ncross_precision = np.mean(cross_val_precision)","5bff1c48":"cross_val_recall = cross_val_score(clf,X,y,cv=5,scoring='recall')\ncross_recall = np.mean(cross_val_recall)","a145fbbe":"cross_val_f1 = cross_val_score(clf,X,y,cv=5,scoring='f1')\ncross_f1 = np.mean(cross_val_f1)","e86249d9":"# visualizing \ncv_metrics = pd.DataFrame({\"Accuracy\": cross_acc,\n                            \"Precision\": cross_precision,\n                            \"Recall\": cross_recall,\n                            \"F1\": cross_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);","bf07e195":"clf.fit(X_train,y_train)","8affa5cd":"clf.coef_","e9edb92b":"features_dict = dict(zip(df.columns,list(clf.coef_[0])))\nfeatures_dict","55e5e34c":"features_df = pd.DataFrame(features_dict,index=[0])\nfeatures_df","67fa0754":"features_df.T.plot.bar(title='Feature Importance',legend=False);","fd3af040":"Since EDA has no real set methodolgy, the following is a short check list you might want to walk through:\n\n* What question(s) are you trying to solve (or prove wrong)?\n* What kind of data do you have and how do you treat different types?\n* What\u2019s missing from the data and how do you deal with it?\n* Where are the outliers and why should you care about them?\n* How can you add, change or remove features to get more out of your data?","6930b016":"## Tuning using GridSearchCV","6470dc15":"# Feature Importance","a412a2ad":"## Heart Disease Data Dictionary\nA data dictionary describes the data you're dealing with. Not all datasets come with them so this is where you may have to do your research or ask a subject matter expert (someone who knows about the data) for more.\n\nThe following are the features we'll use to predict our target variable (heart disease or no heart disease).\n\n* age - age in years\n* sex - (1 = male; 0 = female)\n* cp - chest pain type\n* 0: Typical angina: chest pain related decrease blood supply to the heart\n* 1: Atypical angina: chest pain not related to heart\n* 2: Non-anginal pain: typically esophageal spasms (non heart related)\n* 3: Asymptomatic: chest pain not showing signs of disease\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital)\nanything above 130-140 is typically cause for concern\n* chol - serum cholestoral in mg\/dl\n* serum = LDL + HDL + .2 * triglycerides\n  above 200 is cause for concern\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n  '>126' mg\/dL signals diabetes\n* restecg - resting electrocardiographic results\n  0: Nothing to note\n  1: ST-T Wave abnormality\n  can range from mild symptoms to severe problems\n  signals non-normal heart beat\n  2: Possible or definite left ventricular hypertrophy\n  Enlarged heart's main pumping chamber\n* thalach - maximum heart rate achieved\n* exang - exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest\n   looks at stress of heart during excercise\n   unhealthy heart will stress more\n* slope - the slope of the peak exercise ST segment\n  0: Upsloping: better heart rate with excercise (uncommon)\n  1: Flatsloping: minimal change (typical healthy heart)\n  2: Downslopins: signs of unhealthy heart\n* ca - number of major vessels (0-3) colored by flourosopy\n  colored vessel means the doctor can see the blood passing through\n  the more blood movement the better (no clots)\n* thal - thalium stress result\n  1,3: normal\n  6: fixed defect: used to be defect but ok now\n  7: reversable defect: no proper blood movement when excercising\n* target - have disease or not (1=yes, 0=no) (= the predicted attribute)","2459fb59":"## Hyperparameter Tuning","931f7091":"## Tuning using RandomizedSearchCV"}}