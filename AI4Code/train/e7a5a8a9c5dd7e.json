{"cell_type":{"c53bd3ef":"code","7ba25b49":"code","aad1e398":"code","433d95a0":"code","3ff25f99":"code","5ecb2083":"code","455e4d17":"code","c844a2f7":"code","474a0e9a":"code","014bae27":"code","0e842795":"code","f7e2a44b":"code","cbf76969":"code","2b09ac34":"code","5d75edb0":"code","5c7a4777":"code","6ef6bd3b":"code","2b907c23":"code","38f8159e":"code","664e3784":"code","b6284836":"code","480370d3":"code","0466704c":"code","cde511e4":"code","82debb0b":"code","614e26b0":"code","f27a3d74":"code","4a5e31fa":"code","ad327bb2":"code","23f506ac":"code","20e339f2":"code","fe679634":"code","35e07328":"code","ba695fb9":"code","c6866581":"code","42745f9e":"code","3a3b7396":"code","b1c71c7d":"code","61c452fd":"code","c29dfac2":"code","163aff09":"code","51191d33":"code","35472eb3":"code","169faa16":"code","4f49be08":"code","245951f1":"code","08e58341":"code","38f26d41":"code","9607884d":"code","388bee0e":"code","e0ba176c":"code","e1e97355":"code","519b0191":"code","8e07601a":"code","ce5116c2":"code","1b7fade1":"code","64639d95":"code","14cb2524":"code","407ad827":"code","2c1ed496":"code","f545c92f":"code","c396db5a":"code","17a82fc1":"code","2332f47b":"code","b6e4d07f":"code","74f7094a":"code","208ad386":"code","fee35766":"code","20fb819c":"code","ab2bd0cf":"code","ffac8695":"code","f015d9dc":"code","fede6003":"code","62f3946c":"code","5652b92e":"code","9287e189":"code","117718a5":"code","13a9c1d3":"code","b1c279f3":"code","1d051f3b":"code","8913c9f7":"code","cdaa787a":"code","49934dc9":"code","afd15595":"code","753322fd":"code","7bb9b89d":"code","5333838e":"code","708bd0f3":"code","48b99bfc":"code","df284b6c":"code","f5dd6d78":"code","35f11483":"code","d0df1b36":"code","d57a1af6":"code","99be687f":"code","6a8519c6":"code","eb263dae":"code","b134f20e":"code","604b8e16":"code","418315b4":"code","63d80eab":"code","10e431ad":"code","9135824e":"code","6761a494":"code","80c4e551":"code","79a70726":"code","ca42d275":"code","54de0588":"code","fd2e506c":"code","1cc8935c":"code","5b6a4552":"code","ca4d80df":"code","500f595a":"code","6fb12da7":"code","c06dbea7":"code","d593e432":"code","3e8accf3":"code","7e1d4736":"code","61d17d66":"code","a171eda5":"code","15d40fff":"code","015996d2":"code","4abd2744":"code","59cdc8b6":"code","924f2c6b":"code","ae0db463":"code","6e57cfd4":"code","cc99b7c5":"code","5dd56efa":"code","e4bc5f5a":"code","1ece399d":"code","77e99e01":"code","fccd5d2d":"code","028bb7b9":"code","d58ecb29":"code","54e1a413":"code","75376af5":"code","551bb361":"code","aed06388":"code","4d815da3":"code","7ed1390d":"code","be90bde5":"code","e489261d":"code","577fc2d9":"code","5056ab76":"code","8a55a232":"code","3680231b":"code","44a4d6e7":"code","28575de0":"code","0bb46aab":"code","66dfad94":"code","9fcdd5ec":"code","95c97247":"code","113c4375":"code","c5cd4680":"code","dc38b261":"code","07949806":"code","7e972def":"code","74bf6ba4":"markdown","9a316506":"markdown","4dd54f59":"markdown","01f16794":"markdown","4092c7cb":"markdown","a70b19ba":"markdown","5c93121e":"markdown","cd6d40cd":"markdown","29ebbe73":"markdown","7a7e9ec9":"markdown","54488c4d":"markdown","e13712e4":"markdown","82fddbda":"markdown","1a7006da":"markdown","722dfee1":"markdown","0e213f11":"markdown","a418933f":"markdown","d596cf53":"markdown","b6615b78":"markdown","332548ae":"markdown","ff4e8f23":"markdown","991c45a7":"markdown","1cd4d9a9":"markdown","1b5eaa1d":"markdown","80aaa677":"markdown","fab32450":"markdown","8825dab9":"markdown","e2cfe42a":"markdown","319623c4":"markdown","108dcd9b":"markdown","befb142c":"markdown","4156f76c":"markdown","85f502f2":"markdown","5fe861b0":"markdown","389c5fa2":"markdown","57a0ab03":"markdown","582847fa":"markdown","a49349fe":"markdown","389ef948":"markdown","135ce1bd":"markdown","fcdcabf1":"markdown","a8aa71b0":"markdown","a00717a1":"markdown","b89206af":"markdown","d09bbb1d":"markdown","0093af65":"markdown","a51f4a46":"markdown","06fcb3da":"markdown","ce014de2":"markdown","a59ce2d6":"markdown","dba23df6":"markdown"},"source":{"c53bd3ef":"from google.colab import files","7ba25b49":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom scipy import stats\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc, f1_score\nfrom sklearn.model_selection import RandomizedSearchCV","aad1e398":"files.upload()","433d95a0":"#from google.colab import drive\n#drive.mount('\/content\/drive')","3ff25f99":"train = pd.read_csv(\"train.csv\")","5ecb2083":"train_dummy = train.copy()","455e4d17":"train_dummy.head(7)","c844a2f7":"train_dummy.describe()","474a0e9a":"train_dummy.dtypes","014bae27":"sns.heatmap(train_dummy.isnull())","0e842795":"train_dummy.isnull().sum()","f7e2a44b":"train_dummy.info()","cbf76969":"train_dummy['TARGET'].unique()","2b09ac34":"target = train_dummy.TARGET\nplt.hist(target)\nplt.show()","5d75edb0":"c1=0\nc2=0\nfor i in train_dummy.TARGET:\n  if i==0:\n    c1+=1\n  elif i==1:\n    c2+=1\n\nx=c1\/(c1+c2)\n\nprint(\"No. of 0s in target: \"+str(c1))\nprint(\"No. of 1s in target: \"+str(c2))\nprint(\"Percentage of customer satisfied: \"+str(x*100)+\"%\")","5c7a4777":"zero_col=[]\nfor col in train_dummy:\n  c=0\n  for data in train_dummy[col]:\n    if data!=0:\n      c+=1\n  if c==0:\n    zero_col.append(col)\n    print(col+\" has all zero values\")\n    print(\"===============================================\")\nprint(zero_col)","6ef6bd3b":"train_dummy = train_dummy.drop(columns=zero_col)","2b907c23":"train_dummy.shape","38f8159e":"duplicate_columns = []\ncolu = train_dummy.columns\ncounter = 0\nfor i in range(len(colu)-1):\n  val = train_dummy[colu[i]].values\n  for j in range(i+1,len(colu)):\n    if np.array_equal(val,train_dummy[colu[j]].values):\n      duplicate_columns.append(colu[j])\n      counter+=1\nprint(\"Duplicate columns in the dataset: \",duplicate_columns)\nprint(\"No. of duplicate columns: \",counter)","664e3784":"train_dummy = train_dummy.drop(columns= duplicate_columns)","b6284836":"train_dummy.shape","480370d3":"cor = train_dummy.corr()\nprint(cor)","0466704c":"for i in range(0,5):\n  for j in range(0,5):\n    x = i*50\n    y = j*50\n    corr = cor.iloc[range(x,x+50),range(y,y+50)]\n    fig, a = plt.subplots(figsize = (15, 10))\n    sns.heatmap(corr,linewidths=0.5,ax = a)","cde511e4":"col_cor = set()\nfor i in range(len(cor.columns)):\n  for j in range(i):\n    if (cor.iloc[i,j]>=0.9) and (cor.columns[j] not in col_cor):\n      col_name = cor.columns[i]\n      col_cor.add(col_name)\n      if col_name in train_dummy:\n        del train_dummy[col_name]\n\nprint(\"cols are: \",col_cor)","82debb0b":"train_dummy.describe()","614e26b0":"train_dummy.shape","f27a3d74":"z_score = []\nfor co in train_dummy:\n  temp = stats.zscore(train_dummy[co])\n  z_score.append(temp)\n  temp = []\n\nprint(z_score)","4a5e31fa":"Q1 = train_dummy.quantile(0.25)\nQ3 = train_dummy.quantile(0.75)\nIQR = Q3-Q1\nprint(\"The IQR of all data: \",IQR)","ad327bb2":"type(IQR)","23f506ac":"train_d_outliers = train_dummy[~((train_dummy < (Q1 - 1.5*IQR)) | (train_dummy > (Q3 + 1.5*IQR))).any(axis=1)]\nprint(train_d_outliers)\ntype(train_d_outliers)","20e339f2":"#Dropping the ID and TARGET columns from datasets.\ntrain_dummy_drop = train_dummy.drop(columns = ['ID','TARGET'])\ntrain_d_outliers_drop = train_d_outliers.drop(columns = ['ID','TARGET'])\ntrain_dummy_drop.shape , train_d_outliers_drop.shape","fe679634":"#Normalizing the data with outliers\ntrain_dummy_norm = preprocessing.normalize(train_dummy_drop)\ntrain_dummy_norm = pd.DataFrame(train_dummy_norm)\n\n#Normalizing the data without outliers\ntrain_d_outliers_norm = preprocessing.normalize(train_d_outliers_drop)\ntrain_d_outliers_norm = pd.DataFrame(train_d_outliers_norm)\n\nprint(train_dummy_norm)\nprint(train_d_outliers_norm)","35e07328":"train_dummy_norm.shape, train_d_outliers_norm.shape","ba695fb9":"pca_santander_w = PCA(n_components = 7)\nPC_santander_w = pca_santander_w.fit_transform(train_dummy_norm)","c6866581":"pca_dataFrame_w = pd.DataFrame(data = PC_santander_w, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7'])\npca_dataFrame_w ","42745f9e":"#Variance ratio \nprint(pca_santander_w.explained_variance_ratio_)","3a3b7396":"pca_santander_wo = PCA(n_components = 7)\nPC_santander_wo = pca_santander_wo.fit_transform(train_d_outliers_norm)","b1c71c7d":"pca_dataFrame_wo = pd.DataFrame(data = PC_santander_wo, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7'])\npca_dataFrame_wo","61c452fd":"#Variance ratio \nprint(pca_santander_wo.explained_variance_ratio_)","c29dfac2":"#Adding the ID and TARGET to the \"pca_dataFrame_w\" dataframe again.\npca_dataFrame_w['ID'] = train_dummy['ID']\npca_dataFrame_w['TARGET'] = train_dummy['TARGET']\npca_dataFrame_w = pca_dataFrame_w[['ID','PC1','PC2','PC3','PC4','PC5','PC6','PC7','TARGET']]\npca_dataFrame_w","163aff09":"#Adding the ID and TARGET to the \"pca_dataFrame_wo\" dataframe again.\npca_dataFrame_wo['ID'] = train_d_outliers['ID']\npca_dataFrame_wo['TARGET'] = train_d_outliers['TARGET']\npca_dataFrame_wo = pca_dataFrame_wo[['ID','PC1','PC2','PC3','PC4','PC5','PC6','PC7','TARGET']]\npca_dataFrame_wo","51191d33":"#Assigning the TARGET column to y and remaining columns to x. \nx = pca_dataFrame_w.drop(columns=['ID','TARGET'])\ny = pca_dataFrame_w['TARGET']\n\n#Splitting the  training dataset into test and train\nx_train, x_test, y_train, y_test = train_test_split(x , y , test_size = 0.3 , random_state = 20)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","35472eb3":"#importing logistic regreassion from sklearn\nfrom sklearn.linear_model import LogisticRegression\n\n#Building model\nmlr = LogisticRegression()\nmlr.fit(x_train,y_train) ","169faa16":"#Testing model on x_test\ny_pred = mlr.predict(x_test)","4f49be08":"#Checking accuracy\nconfusion_matrix(y_test , y_pred)","245951f1":"#Accuracy\nprint(\"Accuracy of the model is: \",accuracy_score(y_test, y_pred)*100,\"%\")","08e58341":"#Classification report\nprint(\"Overall report:\")\nprint(classification_report(y_test,y_pred))","38f26d41":"params_lr = [{\n    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    \n}]","9607884d":"mlr_random = RandomizedSearchCV(mlr,param_distributions=params_lr,scoring='roc_auc',n_jobs=-1,cv=5,verbose=True)","388bee0e":"clf_random_mlr = mlr_random.fit(x_train,y_train)","e0ba176c":"clf_random_mlr.best_estimator_","e1e97355":"mlr_hyp = LogisticRegression(C=11.288378916846883, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n                   warm_start=False)","519b0191":"mlr_hyp.fit(x_train,y_train)","8e07601a":"y_pred_mlr_hyp = mlr_hyp.predict(x_test)","ce5116c2":"cm_mlr = confusion_matrix(y_test,y_pred_mlr_hyp) \ncm_mlr","1b7fade1":"accuracy_mlr = accuracy_score(y_test,y_pred_mlr_hyp)\naccuracy_mlr","64639d95":" print(classification_report(y_test,y_pred_mlr_hyp))","14cb2524":"f1_mlr = f1_score(y_test,y_pred_mlr_hyp)\nf1_mlr","407ad827":"y_prob_mlr = mlr_hyp.predict_proba(x_test)[:,1]\ny_prob_mlr","2c1ed496":"fpr_mlr , tpr_mlr , thr_mlr = roc_curve(y_test,y_prob_mlr)\nfpr_mlr , tpr_mlr , thr_mlr","f545c92f":"auc_mlr = auc(fpr_mlr,tpr_mlr)\nprint(\"Area under the ROC curve is: \",auc_mlr)","c396db5a":"#Now let's draw the ROC\nplt.figure(figsize=(7,7))\nplt.title(\"ROC of Logistic regression\")\nplt.plot(fpr_mlr,tpr_mlr,linestyle = 'solid',label = 'Area Under the Curve = %0.3f' % auc_mlr)\nplt.legend(loc = 'upper left')\nplt.plot([0,1],linestyle = '--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")","17a82fc1":"#importing decision tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Building model\nDT = DecisionTreeClassifier()\nDT.fit(x_train,y_train)","2332f47b":"#Testing model on x_test\ny_pred_dt = DT.predict(x_test)","b6e4d07f":"#Confusion matrix\nconfusion_matrix(y_test,y_pred_dt)","74f7094a":"#Accuracy\naccuracy_dt = accuracy_score(y_test,y_pred_dt)\naccuracy_dt","208ad386":"#Classification report\nprint(classification_report(y_test,y_pred_dt))","fee35766":"f1_dt = f1_score(y_test,y_pred_dt)\nf1_dt","20fb819c":"y_prob_dt = DT.predict_proba(x_test)[:,1]\ny_prob_dt","ab2bd0cf":"fpr_dt , tpr_dt , thr_dt = roc_curve(y_test,y_prob_dt)\nfpr_dt , tpr_dt , thr_dt","ffac8695":"auc_dt = auc(fpr_dt,tpr_dt)\nprint(\"Area under the curve : \",auc_dt)","f015d9dc":"#Now let's draw the ROC\nplt.figure(figsize=(7,7))\nplt.title(\"ROC of Decision Tree\")\nplt.plot(fpr_dt,tpr_dt,linestyle = 'solid',label = 'Area Under the Curve = %0.3f' % auc_dt)\nplt.legend(loc = 'upper left')\nplt.plot([0,1],linestyle = '--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")","fede6003":"params_dt = [\n    {'splitter' : ['best', 'random'],\n     'max_depth' : np.linspace(1, 32, 32, endpoint=True),\n     'min_samples_split' : np.linspace(1, 10, 10, endpoint=True),\n     'min_samples_leaf' : np.linspace(0.1, 0.5, 10, endpoint=True),\n     'max_features' : list(range(1,x_train.shape[1])),\n    }\n]","62f3946c":"dt_random = RandomizedSearchCV(DT,param_distributions=params_dt,scoring='roc_auc',n_jobs=-1,cv=5,verbose=True)","5652b92e":"clf_random_dt = dt_random.fit(x_train,y_train)","9287e189":"clf_random_dt.best_estimator_","117718a5":"DT_hyp = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=5.0, max_features=5, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=0.3666666666666667,\n                       min_samples_split=1.0, min_weight_fraction_leaf=0.0,\n                       presort='deprecated', random_state=None,\n                       splitter='random')","13a9c1d3":"DT_hyp.fit(x_train,y_train)","b1c279f3":"y_pred_dt_hyp = DT_hyp.predict(x_test)","1d051f3b":"confusion_matrix(y_test,y_pred_dt_hyp)","8913c9f7":"accuracy_dt_hyp = accuracy_score(y_test,y_pred_dt_hyp)\naccuracy_dt_hyp","cdaa787a":"print(classification_report(y_test,y_pred_dt_hyp))","49934dc9":"f1_dt_hyp = f1_score(y_test,y_pred_dt_hyp)\nf1_dt_hyp","afd15595":"y_prob_dt_hyp = DT_hyp.predict_proba(x_test)[:,1]\ny_prob_dt_hyp","753322fd":"fpr_dt_hyp , tpr_dt_hyp , thr_dt_hyp = roc_curve(y_test,y_prob_dt_hyp)","7bb9b89d":"auc_dt_hyp = auc(fpr_dt_hyp,tpr_dt_hyp)\nauc_dt_hyp","5333838e":"#Now let's draw the ROC\nplt.figure(figsize=(7,7))\nplt.title(\"ROC of Decision Tree(After tuning Hyperparameters)\")\nplt.plot(fpr_dt_hyp,tpr_dt_hyp,linestyle = 'solid',label = 'Area Under the Curve = %0.3f' % auc_dt_hyp)\nplt.legend(loc = 'upper left')\nplt.plot([0,1],linestyle = '--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")","708bd0f3":"#importing multinomial naive bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n#Building model\nGNB = GaussianNB()\nGNB.fit(x_train,y_train)","48b99bfc":"#Testing model on x_test\ny_pred_nb = GNB.predict(x_test)","df284b6c":"#Confusion matrix\nconfusion_matrix(y_test,y_pred_nb)","f5dd6d78":"#Accuracy\naccuracy_nb = accuracy_score(y_test,y_pred_nb)\naccuracy_nb","35f11483":"#Classification report\nprint(classification_report(y_test,y_pred_nb))","d0df1b36":"f1_nb = f1_score(y_test,y_pred_nb)\nf1_nb","d57a1af6":"y_prob_nb = GNB.predict_proba(x_test)[:,1]\ny_prob_nb","99be687f":"fpr_nb , tpr_nb , thr_nb = roc_curve(y_test,y_prob_nb)\nfpr_nb , tpr_nb , thr_nb","6a8519c6":"auc_nb = auc(fpr_nb,tpr_nb)\nauc_nb","eb263dae":"#Now let's draw the ROC\nplt.figure(figsize=(7,7))\nplt.title(\"ROC of Naive Bayes\")\nplt.plot(fpr_nb,tpr_nb,linestyle = 'solid',label = 'Area Under the Curve = %0.3f' % auc_nb)\nplt.legend(loc = 'upper left')\nplt.plot([0,1],linestyle = '--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")","b134f20e":"params_nb = [\n             {C=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n              gamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n              kernel=['rbf','linear']}\n]","604b8e16":"#importing required package \nfrom sklearn.ensemble import RandomForestClassifier","418315b4":"#Building model\nRF = RandomForestClassifier(n_estimators=100)\nRF.fit(x_train,y_train)","63d80eab":"#Testing model on x_test\ny_pred_rf = RF.predict(x_test)","10e431ad":"#Confusion matrix\nconfusion_matrix(y_test, y_pred_rf)","9135824e":"#Accuracy\naccuracy_score(y_test, y_pred_rf)","6761a494":"#Overall score\nprint(classification_report(y_test, y_pred_rf))","80c4e551":"params_rf = [\n    {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n    'max_features': ['auto', 'sqrt'],\n    'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]}\n]","79a70726":"rf_random = RandomizedSearchCV(RF,param_distributions=params_rf,scoring='roc_auc',n_jobs=-1,cv=2,verbose=True)","ca42d275":"clf_random_rf = rf_random.fit(x_train,y_train)","54de0588":"y_pred_rf_hyp = clf_random_rf.predict(x_test)","fd2e506c":"confusion_matrix(y_test,y_pred_rf_hyp)","1cc8935c":"accuracy_rf = accuracy_score(y_test,y_pred_rf_hyp)\naccuracy_rf","5b6a4552":"print(classification_report(y_test,y_pred_rf_hyp))","ca4d80df":"f1_rf = f1_score(y_test,y_pred_rf_hyp)\nf1_rf","500f595a":"y_prob_rf_hyp = clf_random_rf.predict_proba(x_test)[:,1]\ny_prob_rf_hyp","6fb12da7":"fpr_rf,tpr_rf,thr_rf = roc_curve(y_test,y_prob_rf_hyp)\nfpr_rf,tpr_rf,thr_rf","c06dbea7":"auc_rf = auc(fpr_rf,tpr_rf)\nauc_rf","d593e432":"#Now let's draw the ROC\nplt.figure(figsize=(7,7))\nplt.title(\"ROC of Random Forest\")\nplt.plot(fpr_rf,tpr_rf,linestyle = 'solid',label = 'Area Under the Curve = %0.3f' % auc_rf)\nplt.legend(loc = 'upper left')\nplt.plot([0,1],linestyle = '--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")","3e8accf3":"#importing packages\nfrom sklearn.svm import SVC","7e1d4736":"#Building model\nclf = SVC()\nclf.fit(x_train,y_train)","61d17d66":"#Applying model on x_test\ny_pred_svm = clf.predict(x_test)  ","a171eda5":"#confusion matrix\nconfusion_matrix(y_test,y_pred_svm)","15d40fff":"#Accuracy\naccuracy_score(y_test,y_pred_svm)","015996d2":"#Overall score\nprint(classification_report(y_test,y_pred_svm))","4abd2744":"params_svm = [\n              {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}\n]","59cdc8b6":"random_svm = RandomizedSearchCV(clf,param_distributions=params_svm,scoring='roc_auc',n_jobs=-1,cv=5,verbose=True)","924f2c6b":"clf_random_svm = random_svm.fit(x_train,y_train)","ae0db463":"clf_random_svm.best_estimator_","6e57cfd4":"svm_hyp = SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n    max_iter=-1, probability=True, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)","cc99b7c5":"svm_hyp.fit(x_train,y_train)","5dd56efa":"y_pred_svm_hyp = svm_hyp.predict(x_test)","e4bc5f5a":"confusion_matrix(y_test,y_pred_svm_hyp)","1ece399d":"accuracy_svm = accuracy_score(y_test,y_pred_svm_hyp)\naccuracy_svm","77e99e01":"print(classification_report(y_test,y_pred_svm_hyp))","fccd5d2d":"f1_svm = f1_score(y_test,y_pred_svm_hyp)\nf1_svm","028bb7b9":"y_prob_svm = svm_hyp.predict_proba(x_test)[:,1]\ny_prob_svm","d58ecb29":"fpr_svm, tpr_svm, thr_svm = roc_curve(y_test,y_prob_svm)\nfpr_svm, tpr_svm, thr_svm","54e1a413":"auc_svm = auc(fpr_svm,tpr_svm)\nauc_svm","75376af5":"#Now let's draw the ROC\nplt.figure(figsize=(7,7))\nplt.title(\"ROC of SVM\")\nplt.plot(fpr_svm,tpr_svm,linestyle = 'solid',label = 'Area Under the Curve = %0.3f' % auc_svm)\nplt.legend(loc = 'upper left')\nplt.plot([0,1],linestyle = '--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")","551bb361":"#Setting parameters for xgboost\nparams = {\n    \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n}","aed06388":"#importing necessary pacakges\nimport xgboost","4d815da3":"classifier = xgboost.XGBClassifier()","7ed1390d":"random_search = RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)","be90bde5":"random_search.fit(x_train,y_train)","e489261d":"print(random_search.best_estimator_)\nprint(\"====================================================================================================\")\nprint(random_search.best_params_)","577fc2d9":"classifier = xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0.4,\n              learning_rate=0.15, max_delta_step=0, max_depth=3,\n              min_child_weight=5, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","5056ab76":"from sklearn.model_selection import cross_val_score\nscore_train = cross_val_score(classifier,x_train,y_train,cv=10)\nscore_train","8a55a232":"score_train.mean()","3680231b":"classifier.fit(x_train,y_train)\ny_pred_xgb = classifier.predict(x_test)","44a4d6e7":"#Accuracy\naccuracy_xgb = accuracy_score(y_test,y_pred_xgb)\naccuracy_xgb","28575de0":"#confusion matrix\nconfusion_matrix(y_test,y_pred_xgb)","0bb46aab":"#Overall score\nprint(classification_report(y_test,y_pred_xgb))","66dfad94":"probability_xgb = classifier.predict_proba(x_test)","9fcdd5ec":"print(\"Probability of satisfaction:\",probability_xgb[:,0])\nprint(\"============================================================================================================\")\nprint(print(\"Probability of dis-satisfaction:\",probability_xgb[:,1]))","95c97247":"f1_xgb = f1_score(y_test,y_pred_xgb)\nf1_xgb","113c4375":"y_prob_xgb = probability_xgb[:,1]\ny_prob_xgb","c5cd4680":"fpr_xgb,tpr_xgb,thr_xgb = roc_curve(y_test,y_prob_xgb)\nfpr_xgb,tpr_xgb,thr_xgb","dc38b261":"auc_xgb = auc(fpr_xgb,tpr_xgb)\nauc_xgb","07949806":"#Now let's draw the ROC\nplt.figure(figsize=(7,7))\nplt.title(\"ROC of XGBoost\")\nplt.plot(fpr_xgb,tpr_xgb,linestyle = 'solid',label = 'Area Under the Curve = %0.3f' % auc_xgb)\nplt.legend(loc = 'upper left')\nplt.plot([0,1],linestyle = '--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")","7e972def":"table = pd.DataFrame({\n    'Model' : ['Logistic Regression','Decision Tree','Decision Tree(Hyp)','Naive Bayes','Random Forest','SVM','XGBoost'],\n    'Accuracy' : [accuracy_mlr,accuracy_dt,accuracy_dt_hyp,accuracy_nb,accuracy_rf,accuracy_svm,accuracy_xgb],\n    'AUC' : [auc_mlr,auc_dt,auc_dt_hyp,auc_nb,auc_rf,auc_svm,auc_xgb],\n    'f1-Score' : [f1_mlr,f1_dt,f1_dt_hyp,f1_nb,f1_rf,f1_svm,f1_xgb]\n})\n\n\ntable.style.background_gradient(cmap='hot_r', text_color_threshold=0.5)","74bf6ba4":"**So, We need to remove all 29 duplicate columns too.**","9a316506":"We will not do hyperparameter tuning for this model, because for this dataset Gaussian naive bayes is not appropriate.\n\nIf in case we need to do tuning of hyperparameters, below are the parameters with which we can do tuning using RandomSearchCV or GridSearchCV.","4dd54f59":"ID and TARGET columns are being dropped of the dataset before performing PCA. ","01f16794":"Now we will do test and train split to our training dataset before building the model.","4092c7cb":"**All z-scores are detected.**","a70b19ba":"**Columns are removed having r-value>=0.9.**","5c93121e":"Here, we have done random search to find out the exact parameters for xgboost. In this method we passed classifier, all the selected parameters. ","cd6d40cd":"##PCA","29ebbe73":"**No missing value in dataset.**","7a7e9ec9":"Here also after tuning the parameters, still the model performance remains unchanged.","54488c4d":"###**2. Decision tree**","e13712e4":"Now let's see model performance after hyperparameter tuning by ROC.","82fddbda":"Let's see the perrformance using ROC curve.","1a7006da":"##Data Modelling ","722dfee1":"###PCA of dataset having outliers **\"train_dummy_norm\"**:","0e213f11":"Now we will do the hyperparameter tuning of SVM algo.","a418933f":"Now let's see the model performance using ROC.","d596cf53":" **Correlation heat maps have been displayed.**","b6615b78":"**Almost 96% of total customers are satisfied.**\n\nHence Dataset is unbalanced.","332548ae":"Now we will do hyperparameter tuning of random forest classifier.","ff4e8f23":"Here we can observe one thing that even if after hypertuning the parameters, the evaluated parameters remain unchanged.","991c45a7":"Here we have two datasets:\n1. Dataset with outliers.\n2. Dataset without outliers.\n\nWe shall perform PCA on both the datasets.But before PCA, we need to normalize the data first.","1cd4d9a9":"##Model Comparison","1b5eaa1d":"All above parameters are suitable for our dataset and it will not make the model overfitting.","80aaa677":"**All 29 columns have been dropped.**","fab32450":"Now we will see the model performance using ROC.","8825dab9":"Now we will hypertune our Decision tree parameters and again predict the result.","e2cfe42a":"**There are more no. of \"0\" than \"1\" in the target side.That means more customers are satified.**","319623c4":"**All columns having only 0s are dropped from data set.**","108dcd9b":"**Correlation coefficients of all columns.**","befb142c":"**IQR value detected for the training data set.**","4156f76c":"Before modelling, we need to add both 'ID' and 'TARGET' columns in the normalized PCA dtaset.","85f502f2":"###PCA of dataset without outliers **\"train_d_outliers_norm\"**:","5fe861b0":"**Hence, dataset size became less.**","389c5fa2":"## Exploratory Data Analysis","57a0ab03":"**Size after dropping duplicate columns.**","582847fa":"###**5. SVM**","a49349fe":"**Datatypes of columns(All columns are not shown because of large dataset).**","389ef948":"Now let's go for ROC.","135ce1bd":"# **Santander's Customer Satisfaction**","fcdcabf1":"Now let's observe the model performance using ROC.","a8aa71b0":"Now let's go for ROC.","a00717a1":"###**4. Random Forest**","b89206af":"###**6. XGBoost**","d09bbb1d":"###**1. Logistic regression**","0093af65":"**Summary of dataset.**\n\nHere are the inferences made:\n\n1. All values are numeric.\n2. Multiple columns have only zeros in all the rows.\n3. Some duplicate columns are also there like \"imp_op_var41_efect_ult1\" and \"imp_op_var39_efect_ult1\".\n4. Need impute\/remove outliers.\n5. Normalisation of data is required.","a51f4a46":"Now we will hypertune our logistic regression parameters and again predict the result.","06fcb3da":"**train_d_outliers** is the data-frame where all the rows having outliers are being removed.","ce014de2":"We have built the model and predicted the output for test data.","a59ce2d6":"###**3. Naive Bayes**","dba23df6":"**Used a copy of dataset so that original dataset remains unchannged.**"}}