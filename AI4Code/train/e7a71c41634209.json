{"cell_type":{"1ab6ea99":"code","5123742c":"code","3dd0f903":"code","00a076f3":"code","509dd59b":"code","471666f9":"code","dd0a43bb":"code","f9289163":"code","2cbb136f":"code","e30cf1db":"code","1f8e0a39":"code","61bb18b9":"code","a58c9f31":"markdown","94ae8515":"markdown","bb37a47b":"markdown","3d3912d3":"markdown","ac9426ae":"markdown","5b7b374e":"markdown","195f72ee":"markdown","aff5f62f":"markdown"},"source":{"1ab6ea99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5123742c":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler as sc\nnames = ('Serial No.', 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n       'LOR ', 'CGPA', 'Research', 'Chance of Admit ')\ndata = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv', skiprows = 400, names = names)\ndata.head()","3dd0f903":"data.info()","00a076f3":"data.columns","509dd59b":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.ensemble import AdaBoostRegressor\n\n","471666f9":"X=data.drop(['Chance of Admit ', 'Serial No.'], axis=1)\ny = data['Chance of Admit ']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","dd0a43bb":"regressor = LinearRegression()  \nregressor.fit(X_train, y_train) #training the algorithm\ny_pred = regressor.predict(X_test)\nRMSE_Linear = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\nprint('Mean Absolute Error_lng:', metrics.mean_absolute_error(y_test, y_pred).round(3))  \nprint('Mean Squared Error_lng:', metrics.mean_squared_error(y_test, y_pred).round(3))  \nprint('Root Mean Squared Error_lng:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\nprint('r2_score_lng:', r2_score(y_test, y_pred).round(3))","f9289163":"ridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train) #training the algorithm\n\ny_pred = ridge.predict(X_test)\nRMSE_Ridge = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\nprint('Mean Absolute Error_ridge:', metrics.mean_absolute_error(y_test, y_pred).round(3))  \nprint('Mean Squared Error_ridge:', metrics.mean_squared_error(y_test, y_pred).round(3))  \nprint('Root Mean Squared Error_ridge:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\nprint('r2_score_ridge:', r2_score(y_test, y_pred).round(3))","2cbb136f":"clf = Lasso(alpha=0.1)\n\nclf.fit(X_train, y_train) #training the algorithm\n\ny_pred = clf.predict(X_test)\nRMSE_Lasso = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\nprint('Mean Absolute Error_lasso:', metrics.mean_absolute_error(y_test, y_pred).round(3))  \nprint('Mean Squared Error_lasso:', metrics.mean_squared_error(y_test, y_pred).round(3))  \nprint('Root Mean Squared Error_lasso:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\nprint('r2_score_lasso:', r2_score(y_test, y_pred).round(3))\n","e30cf1db":"rfe = RandomForestRegressor(n_estimators = 100, random_state = 42) \n  \n# fit the regressor with x and y data \nrfe.fit(X, y)   \ny_pred=rfe.predict(X_test)\nRMSE_RFE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('r2_score_RFE:', r2_score(y_test, y_pred).round(3))","1f8e0a39":"ABR = AdaBoostRegressor(n_estimators = 100, random_state = 42) \n  \n# fit the regressor with x and y data \nABR.fit(X, y)   \ny_pred=ABR.predict(X_test)\nRMSE_ABR = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('r2_score_ABR:', r2_score(y_test, y_pred).round(3))","61bb18b9":"submission = pd.DataFrame([RMSE_RFE, RMSE_Linear, RMSE_ABR, RMSE_Lasso, RMSE_Ridge], index=['RMSE_RFE', 'RMSE_Linear', 'RMSE_ABR', 'RMSE_Lasso', \n                                                                                            'RMSE_Ridge'],columns = ['RMSE_Score'])\nsubmission.to_csv('result.csv')","a58c9f31":"> ***Using Linear Regression***","94ae8515":"> ***Using Lasso Regression above***","bb37a47b":"> ***Using RFE above***","3d3912d3":"> ***What is Root Mean Square Error (RMSE)?\n> Root Mean Square Error (RMSE) measures how much error there is between two data sets. \n> In other words, it compares a predicted value and an observed or known value. \n> The smaller an RMSE value, the closer predicted and observed values are.\n> \n> Based on low score of RMSE, RandomForest Regressor is the Best Model*******","ac9426ae":"Our target is y i.e. chance of admission","5b7b374e":"> ***Using Ridge Regression above***","195f72ee":"> ***Using AdaBoost Regression above***","aff5f62f":"Since the Task asks to evaluate the chances of admission based on last 100 readings, we skipped the first 400 reading while reading the data using skiprow function."}}