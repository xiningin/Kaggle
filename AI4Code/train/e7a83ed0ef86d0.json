{"cell_type":{"569b8358":"code","25ce226d":"code","d204e593":"code","2d855fc5":"code","0c2b26cc":"code","ee114d99":"code","938a441c":"code","b81d4908":"code","2e4a135f":"code","3ca5b58d":"code","57e111ca":"code","312ef591":"code","d96bd1bf":"code","d719c794":"code","00e8af80":"code","08da8adc":"code","9f7b28c1":"code","3ed6b4a6":"code","3fe51578":"code","b0bf3019":"code","304a8445":"code","cc1fd83a":"code","41315f27":"code","b059e41e":"code","44796dce":"code","dbb6a2c6":"code","cc0ea7e1":"code","a83170a0":"code","a34f79b2":"code","c1f64bf4":"code","5837437d":"code","4a74dc23":"code","d93cd132":"code","26b90e09":"code","a07c4206":"code","83f52bf7":"code","85d339ed":"code","05b7b83e":"code","f85259b6":"code","3acc0c13":"code","fbff7439":"code","f01f7725":"code","426dc476":"code","51bc69c6":"code","7ebe0a7d":"code","65e42bbe":"code","50529c49":"code","f3761889":"code","6de84b38":"code","683380ab":"code","7c579429":"code","d402a6af":"code","4a9d4a75":"code","7a248983":"code","baf63494":"code","04275439":"code","9e557e42":"code","31900ade":"code","02227ee1":"code","73838cd5":"code","a80957a4":"code","934cbead":"code","30642e57":"code","8427eb05":"markdown","5396ae16":"markdown","490a3ac9":"markdown","8d81af32":"markdown","8738badf":"markdown","47d680c7":"markdown","1330b329":"markdown","d32286c9":"markdown","d29d3664":"markdown","4b4e4fba":"markdown"},"source":{"569b8358":"#imports\nimport pandas as pd","25ce226d":"df=pd.read_csv(\"..\/input\/unsupervised-learning-on-country-data\/Country-data.csv\")\ninfo=pd.read_csv(\"..\/input\/unsupervised-learning-on-country-data\/data-dictionary.csv\")","d204e593":"df_copy_2=df.copy()","2d855fc5":"info","0c2b26cc":"df","ee114d99":"df.describe()","938a441c":"# analyse graphique bidimentionnelle\nfrom pandas.plotting import scatter_matrix \nscatter_matrix(df,figsize=(15,15))","b81d4908":"import warnings\nwarnings.filterwarnings(\"ignore\")","2e4a135f":"df.info()","3ca5b58d":"#quantitative variables \nx = df.iloc[:,1:10].values\n#qualitative variables\ny= df.iloc[:,0]","57e111ca":"x.shape","312ef591":"y","d96bd1bf":"#correlation matrix \ncorr = df.corr()\nprint(corr)","d719c794":"#correlation graph\nimport seaborn as sns \nsns.heatmap(corr,xticklabels=corr.columns.values,\n                 yticklabels=corr.columns.values)","00e8af80":"#covariance matrix \n#Any covariance matrix is symmetric and positive semi-definite and its main diagonal contains variances (i.e., the covariance of each element with itself).\n\ncov_mat=df.cov()\nprint(cov_mat)","08da8adc":"#standard scaling\nfrom sklearn import preprocessing \ndf_stsc=preprocessing.scale(x)\nprint(df_stsc)","9f7b28c1":"#vecteur propres et valeurs propres \nimport numpy as np \nvals,vecs=np.linalg.eig(corr)\nprint(vals)\nprint(vecs)","3ed6b4a6":"print('vectors \\n%s'%vecs)\nprint ('vals \\n%s '%vals)","3fe51578":"#(valeur prpr , vecteurs propres correspondants )\npairs=[(np.abs(vals[i]),vecs[:,i])for i in range(len(vals))]\nprint(pairs[0])","b0bf3019":"#ascendent order of pairs \npairs.sort(key=lambda x : x[0],reverse =True)\nprint(pairs)","304a8445":"len(pairs)","cc1fd83a":"x","41315f27":"tot=sum(vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nmatrix_w = np.hstack((pairs[0][1].reshape(9,1),\n                      pairs[1][1].reshape(9,1)))\n\nprint('Matrix W:\\n', matrix_w)\nY = x.dot(matrix_w)\nprint(Y)","b059e41e":"from sklearn.decomposition import PCA \n\ncountry_pca=PCA(n_components=9).fit(df_stsc)\n\n\n#Cumulative Variance explained by each PC\nimport numpy as np\ncum_var = np.cumsum(np.round(country_pca.explained_variance_ratio_, decimals=4)*100)\nprint(cum_var)","44796dce":"#The amount of variance that each PC explains\nvar = country_pca.explained_variance_ratio_\nprint(var)","dbb6a2c6":"import matplotlib.pyplot as plt\n\n\n#Plot explained variance ratio for each PC\nplt.bar([i for i, _ in enumerate(var)],var,color='green')\nplt.title('PCs and their Explained Variance Ratio', fontsize=15)\nplt.xlabel('Number of components',fontsize=12)\nplt.ylabel('Explained Variance Ratio',fontsize=12)\n","cc0ea7e1":"# Scree Plot\nplt.plot(cum_var, marker='o')\nplt.title('Scree Plot: PCs and their Cumulative Explained Variance Ratio',fontsize=15)\nplt.xlabel('Number of components',fontsize=12)\nplt.ylabel('Cumulative Explained Variance Ratio',fontsize=12)","a83170a0":"final_pca=PCA(n_components=5).fit(df_stsc).transform(df_stsc)\n","a34f79b2":"final_pca","c1f64bf4":"from sklearn.decomposition import PCA\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n \n#df = sns.load_dataset('iris')\n \nn_components = 5\n \n# Do the PCA.\npca = PCA(n_components=n_components)\nreduced = pca.fit_transform(df_stsc)\n\n# Append the principle components for each entry to the dataframe\nfor i in range(0, n_components):\n    df['PC' + str(i + 1)] = reduced[:, i]\n\ndisplay(df.head())\n","5837437d":"df_copy=df.drop('country',axis=1)","4a74dc23":"df_copy","d93cd132":"\ndef plot_circle_correlation(PC1,PC2):\n  # Plot a variable factor map for the first two dimensions.\n  (fig, ax) = plt.subplots(figsize=(8, 10))\n  for i in range(0, pca.components_.shape[1]):\n      ax.arrow(0,\n              0,  # Start the arrow at the origin\n              pca.components_[PC1, i],  #0 for PC1\n              pca.components_[PC2, i],  #1 for PC2\n              head_width=0.1,\n              head_length=0.1)\n\n      plt.text(pca.components_[PC1, i] + 0.05,\n              pca.components_[PC2, i] + 0.05,\n              df_copy.columns.values[i])\n\n\n  an = np.linspace(0, 2 * np.pi, 100)\n  plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n  plt.axis('equal')\n  ax.set_title('Variable factor map')\n  plt.show()","26b90e09":"plot_circle_correlation(0,1)","a07c4206":"plot_circle_correlation(1,2)","83f52bf7":"plot_circle_correlation(2,3)","85d339ed":"plot_circle_correlation(3,4)","05b7b83e":"df_pca=pd.DataFrame(data=df_copy,\n                   columns=[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\"])","f85259b6":"df_pca","3acc0c13":"from sklearn.cluster import KMeans","fbff7439":"#Plotting Elbow Curve\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn import metrics\n\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,10))\nvisualizer.fit(df_stsc)    \nvisualizer.poof()","f01f7725":"# define a dictionary that contains all of our relevant info.\nresults = []\n\n# define how many clusters we want to test up to.\nnum_of_clusters = 10\n\n# run through each instance of K\nfor k in range(2, num_of_clusters):\n    \n    print(\"-\"*100)\n    \n    # create an instance of the model, and fit the training data to it.\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(df_pca)\n    \n    \n    # store the different metrics\n#     results_dict_pca[k]['silhouette_score'] = sil_score\n#     results_dict_pca[k]['inertia'] = kmeans.inertia_\n#     results_dict_pca[k]['score'] = kmeans.score\n#     results_dict_pca[k]['model'] = kmeans\n    \n    results.append(kmeans.inertia_)\n    \n    # print the results    \n    print(\"Number of Clusters: {}\".format(k),kmeans.inertia_)\n","426dc476":"plt.figure(figsize=(15,8)) \n\nplt.plot(range(2, num_of_clusters), results, 'bx-')\n\n\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","51bc69c6":"from sklearn.metrics import davies_bouldin_score, silhouette_score, silhouette_samples\nsse,db,slc = {}, {}, {}\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000,random_state=12345).fit(df_stsc)\n    if k == 4: labels = kmeans.labels_\n    clusters = kmeans.labels_\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n    db[k] = davies_bouldin_score(df_stsc,clusters)\n    slc[k] = silhouette_score(df_stsc,clusters)","7ebe0a7d":"#Plotting Davies-Bouldin Scores\nplt.figure(figsize=(12,6))\nplt.plot(list(db.keys()), list(db.values()))\nplt.xlabel(\"Number of cluster\", fontsize=12)\nplt.ylabel(\"Davies-Bouldin values\", fontsize=12)\nplt.title(\"Davies-Bouldin Scores vs No. of Clusters\", fontsize=15)\nplt.show()","65e42bbe":"model=KMeans(n_clusters=3,random_state=20)\nmodel.fit(df_stsc)\n","50529c49":"print(model.labels_)","f3761889":"pd.Series(model.labels_).value_counts()","6de84b38":"from sklearn import metrics \nmetrics.silhouette_score(df_stsc,model.labels_)","683380ab":"df_copy_2[\"preds\"]=model.labels_","7c579429":"df_copy_2","d402a6af":"#Visualize clusters: Feature Pair-2\nplt.figure(figsize=(12,6))\nplt.scatter(df_stsc[:,4],df_stsc[:,8],c=df_copy_2.preds) # income vs gdpp\nplt.title(\"Income vs GDPP (Visualize KMeans Clusters)\", fontsize=15)\nplt.xlabel(\"Income\", fontsize=12)\nplt.ylabel(\"GDPP\", fontsize=12)\nplt.rcParams['axes.facecolor'] = 'lightblue'\nplt.show()","4a9d4a75":"df_copy['country']=df_copy_2[\"country\"]\ndf_copy['preds']=df_copy_2[\"preds\"]\ndf_copy['index']=[ i for i in range(167) ]","7a248983":"df_copy","baf63494":"labs1=list(df[\"country\"])\nlabs2=list(df_copy[\"index\"])\nx=list(df_copy[\"PC1\"])\ny=list(df_copy[\"PC2\"])\nz=list(df_copy[\"PC3\"])\nt=list(df_copy[\"PC4\"])\nw=list(df_copy[\"PC5\"])\n\n","04275439":"\ndef scatterpolt_pca(df,PC1,PC2,target,list_PC1,list_PC2):\n  g =sns.scatterplot(x=PC1, y=PC2,\n                hue=target,\n                data=df,style=target);\n  for i, txt in enumerate(labs1):\n    plt.annotate(txt, (list_PC1[i], list_PC2[i]))\n","9e557e42":"scatterpolt_pca(df_copy,\"PC1\",\"PC2\",\"preds\",x,y)","31900ade":"scatterpolt_pca(df_copy,\"PC2\",\"PC3\",\"preds\",y,z)","02227ee1":"scatterpolt_pca(df_copy,\"PC3\",\"PC4\",\"preds\",z,t)","73838cd5":"scatterpolt_pca(df_copy,\"PC4\",\"PC5\",\"preds\",t,w)","a80957a4":"#find number of developed country,developing country,under-developed country\nunder_developing=df_copy[df_copy['preds']==2]['country']\ndeveloping=df_copy[df_copy['preds']==0]['country']\ndeveloped=df_copy[df_copy['preds']==1]['country']\n\nprint(\"Number of deveoped countries\",len(under_developing))\nprint(\"Number of developing countries\",len(developing))\nprint(\"Number of under-developing countries\",len(developed))","934cbead":"len(developing)","30642e57":"developed","8427eb05":"**Interperetation**\n\n\n1. **child_mort :**\n    + positive corr : total_fer\n    + negative corr : gdpp , life_expec\n\n\n2. **exports:**\n    + positive corr :imports\n    + negative corr :\n3. **income**\n    + positive corr : gdpp\n    + negative corr : total_fer,child_mort\n4. **life expec**\n    + positicve corr :gdpp,income\n    + negative corr:total_fer,child_mort","5396ae16":"\n# Clustering the Countries by using Unsupervised Learning for HELP International\n\n**Objective:**\n\nTo categorise the countries using socio-economic and health factors that determine the overall development of the country.\n\n**About organization:**\n\nHELP International is an international humanitarian NGO that is committed to fighting poverty and providing the people of backward countries with basic amenities and relief during the time of disasters and natural calamities.\n\n**Problem Statement:**\n\nHELP International have been able to raise around $ 10 million. Now the CEO of the NGO needs to decide how to use this money strategically and effectively. So, CEO has to make decision to choose the countries that are in the direst need of aid. Hence, your Job as a Data scientist is to categorise the countries using some socio-economic and health factors that determine the overall development of the country. Then you need to suggest the countries which the CEO needs to focus on the most.\n\n","490a3ac9":"#Clustering","8d81af32":"# PCA","8738badf":"#EDA","47d680c7":"Calculate silouhette coefficient ","1330b329":"###correlation  PC 0 AND PC 1","d32286c9":"https:\/\/www.kaggle.com\/shadanwar\/clustering-kmeans-agc-pca","d29d3664":"https:\/\/www.kaggle.com\/chandrimad31\/different-clustering-techniques-country-profiles\/notebook ","4b4e4fba":"Using these cumulative variance ratios for all PCs, we will now draw a scree plot. It is used to determine the number of principal components to keep in this principal component analysis."}}