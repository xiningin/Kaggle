{"cell_type":{"c6b24b95":"code","48487b8d":"code","c01afcac":"code","5360c4f1":"code","39f54a98":"code","4fe6a870":"code","6845602b":"code","81b3b111":"code","a3193d91":"code","25a2d146":"code","5356e81f":"code","88cf953c":"code","edd1f427":"code","60d7561c":"code","9681b1a8":"code","eb9166e8":"code","e2c89751":"code","388f1a06":"code","7e9d7ec5":"code","c066bb4e":"code","70f4b1b3":"code","9e03ad20":"code","ee6f409f":"code","ed856372":"code","2041526d":"code","b59fbec0":"code","bf695dcc":"code","2189c3d6":"code","6f0c6a3c":"markdown","c272dd90":"markdown","151fb12f":"markdown","4b95621d":"markdown","edf594f4":"markdown","b91c7c6e":"markdown","75baa468":"markdown","00cb7886":"markdown","121a76cf":"markdown","08fe28e6":"markdown","86b6ea42":"markdown","aab48d92":"markdown","95d7513e":"markdown","e2b78d80":"markdown","4d9c0eef":"markdown","48362343":"markdown","154fde57":"markdown","e2ec057a":"markdown","fd7d708d":"markdown","82c3b5c5":"markdown"},"source":{"c6b24b95":"!pip install xgbfir\n!pip install openpyxl","48487b8d":"import pandas as pd\nimport numpy as np\nimport random\nimport os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nimport datetime\nimport time\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import StandardScaler\nimport xgbfir\nimport xgboost as xgb\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    \n    \nN_SPLITS = 5\nN_ESTIMATORS = 2000\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 42\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nseed_everything(SEED)","c01afcac":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","5360c4f1":"train.shape, test.shape","39f54a98":"# no missing\ntrain.isnull().any().sum(), test.isnull().any().sum()","4fe6a870":"train.drop(columns=['id'], inplace=True)\ntest.drop(columns=['id'], inplace=True)\n","6845602b":"features = [col for col in train.columns if 'f' in col]\norg_features = features.copy()\nTARGET='target'","81b3b111":"train[features].describe().style.background_gradient(cmap='Pastel1')","a3193d91":"target_1 = train[train[TARGET]==0].shape[0]\ntarget_2 = train[train[TARGET]==1].shape[0]\nplt.figure(figsize=(15, 7))\nplt.pie([target_1,target_2], labels = [\"0\" , \"1\"],autopct='%1.1f%%',colors = [\"#17becf\", \"#1f77b4\"])\nplt.title('Target Value')","25a2d146":"cor_1 = train.corr()\ncor_1.head()\ncor_target = cor_1.loc['target':'target']\ncor_2 = cor_target.drop(['target'],axis=1)\ncor_3 = abs(cor_2)\ncor_4 = cor_3.sort_values(by='target',axis=1, ascending=False)\npd.set_option('display.max_rows', 1)\npd.set_option('display.max_columns', 100)\ncor_4.head()","5356e81f":"del cor_1\ndel cor_2\ndel cor_3\ndel cor_4\ngc.collect()\n\npd.set_option('display.max_rows', 20)","88cf953c":"# https:\/\/www.kaggle.com\/legendsoul\/tps-october-21-comprehensive-insight-of-eda\n\ndef correlation_matrix(data, features):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data[features].corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = False, center = 0, cmap = 'jet', mask = mask, linewidths = .5, square = True, cbar_kws = {\"shrink\": .70})\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily = 'sans', rotation = 90, fontsize = 12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily = 'sans', rotation = 0, fontsize = 12)\n    plt.tight_layout()\n    plt.show()\n    \ncorrelation_matrix(train, features)","edd1f427":"train.var().iplot(kind='bar')","60d7561c":"sample_size=1000\ntrain_sample = train.sample(n=sample_size, replace=True, random_state=SEED)\ntest_sample = test.sample(n=sample_size, replace=True, random_state=SEED)\n\nprint(\"Feature distribution of continous features: \")\nncols = 5\nnrows = int(len(features) \/ ncols + (len(features) % ncols > 0))\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 50), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = features[r*ncols+c]\n        sns.kdeplot(x=train_sample[col], ax=axes[r, c], color='#58D68D', label='Train data')\n        sns.kdeplot(x=test_sample[col], ax=axes[r, c], color='#DE3163', label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()\n\n","9681b1a8":"h_skew = train[org_features].loc[:,train[org_features].skew() >= 2].columns\nl_skew = train[org_features].loc[:,train[org_features].skew() < 2].columns\n\nfig, ax = plt.subplots(figsize=(10, 5))\nPearson_val=train[h_skew].nunique().sort_values().values\nGaussian_val=train[l_skew].nunique().sort_values().values\nax.plot(Pearson_val, '.', color='blue',linewidth=1.0, label=\"Pearson\")\nax.plot(Gaussian_val, '.', color='red',linewidth=1.0, label=\"Gaussian\")\nax.set_title('Unique Values')\nax.grid(True)\nax.text(0.6, 0.5, \"Features F1 & F36\", ha=\"center\", va=\"center\", rotation=15, size=15,bbox=dict(boxstyle=\"rarrow,pad=0.3\", fc=\"0.9\", ec=\"b\", lw=2),transform=ax.transAxes)\nax.legend()\nplt.show()","eb9166e8":"tmp_df = train.sample(n=100000)\nxgb_X = tmp_df[org_features]\nxgb_y = tmp_df[TARGET]\n\nxgb_model = xgb.XGBClassifier(random_state=42,tree_method='gpu_hist',eval_metric='auc').fit(xgb_X,xgb_y)\n\nxgbfir.saveXgbFI(xgb_model, feature_names=xgb_X.columns, OutputXlsxFile='xgb.xlsx')\njoint_contrib = pd.read_excel('xgb.xlsx')\n\nxls = pd.ExcelFile('xgb.xlsx')\ndf1 = pd.read_excel(xls, 'Interaction Depth 0')\ndf2 = pd.read_excel(xls, 'Interaction Depth 1')\ndf3 = pd.read_excel(xls, 'Interaction Depth 2')\n\nframes = [df2] # I will be using depth1 interactions only for demonstration\njoint_contrib = pd.concat(frames)\n\nabs_imp_joint_contrib = (joint_contrib.groupby('Interaction')\n                                          .Gain\n                                          .apply(lambda x: x.abs().sum())\n                                           .sort_values(ascending=False))\n# then calculate the % of total joint contribution by dividing by the sum of all absolute vals\nrel_imp_join_contrib = abs_imp_joint_contrib \/ abs_imp_joint_contrib.sum()\nrel_imp_join_contrib.head(15)[::-1].iplot(kind='barh', color='#4358C0', title='Joint Feature Importances');","e2c89751":"joint_contrib.sort_values(by='Gain', ascending=False)","388f1a06":"train['f55_ratio_f34'] = train['f55']\/train['f34']\ntest['f55_ratio_f34'] = test['f55']\/test['f34']\n\ntrain['f34_multiply_f8'] = train['f34']*train['f8']\ntest['f34_multiply_f8'] = test['f34']*test['f8']\n\ntrain['f34_diff_f55'] = train['f34']-train['f55']\ntest['f34_diff_f55'] = test['f34']-test['f55']\n\ntrain['f34_ratio_f80'] = train['f34']\/train['f80']\ntest['f34_ratio_f80'] = test['f34']\/test['f80']\n\ntrain['f43_sum_f34'] = train['f43']+train['f34']\ntest['f43_sum_f34'] = test['f43']+test['f34']\n\ntrain['f55_diff_f56'] = train['f55']-train['f56']\ntest['f55_diff_f56'] = test['f55']-test['f56']\n\ntrain['f27_diff_f55'] = train['f27']-train['f55']\ntest['f27_diff_f55'] = test['f27']-test['f55']\n\ntrain['f41_multiply_f34'] = train['f41']*train['f34']\ntest['f41_multiply_f34'] = test['f41']*test['f34']\n\ntrain['f90_sum_f55'] = train['f90']+train['f55']\ntest['f90_sum_f55'] = test['f90']+test['f55']\n\ntrain['f60_ratio_f55'] = train['f60']\/train['f55']\ntest['f60_ratio_f55'] = test['f60']\/test['f55']\n\nfeature_interactions = ['f55_ratio_f34', 'f34_multiply_f8','f34_diff_f55',\n                   'f34_ratio_f80', 'f43_sum_f34', 'f55_diff_f56',\n                   'f27_diff_f55','f41_multiply_f34','f90_sum_f55',\n                   'f60_ratio_f55'\n                  ]\n\nfeatures += feature_interactions","7e9d7ec5":"train[feature_interactions].sample(n=2000).iplot(kind='histogram',subplots=True,bins=50)","c066bb4e":"seed_everything(SEED)","70f4b1b3":"cor_1 = train.corr()\ncor_1.head()\ncor_target = cor_1.loc['target':'target']\ncor_2 = cor_target.drop(['target'],axis=1)\ncor_3 = abs(cor_2)\ncor_4 = cor_3.sort_values(by='target',axis=1, ascending=False)\npd.set_option('display.max_rows', 1)\npd.set_option('display.max_columns', 100)\ncor_4.head()","9e03ad20":"del cor_1\ndel cor_2\ndel cor_3\ndel cor_4\ngc.collect()\npd.set_option('display.max_rows', 20)","ee6f409f":"# MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads\nif (os.name=='nt'):\n    os.environ['OMP_NUM_THREADS']='4' #Workaround for MKL Windows BUG with K-means++\n\n\nclass KMeansTransformer(object):\n    def __init__(self, cluster_number=None):\n        self._cluster_number = cluster_number\n       \n    def fit(self, X, y):\n        if X.shape[1] == 0:\n            raise Exception(\"input error\")\n\n        if self._cluster_number is None:\n            n_clusters = int(np.log10(X.shape[0]) * 8)\n            n_clusters = max(8, n_clusters)\n            n_clusters = min(n_clusters, X.shape[1])\n        else: \n            n_clusters = self._cluster_number\n            \n\n        self._input_columns = X.columns.tolist()\n        # scale data\n        self._scale = StandardScaler(copy=True, with_mean=True, with_std=True)\n        X = self._scale.fit_transform(X)\n\n        self._kmeans = kmeans = MiniBatchKMeans(n_clusters=n_clusters, init=\"k-means++\")\n        self._kmeans.fit(X)\n        self._create_new_features_names()\n\n    def _create_new_features_names(self):\n        n_clusters = self._kmeans.cluster_centers_.shape[0]\n        self._new_features = [f\"Dist_Cluster_{i}\" for i in range(n_clusters)]\n        self._new_features += [\"Cluster\"]\n\n    def transform(self, X):\n        if self._kmeans is None:\n            raise Exception(\"KMeans not fitted\")\n\n        # scale\n        X_scaled = self._scale.transform(X[self._input_columns])\n\n        # kmeans\n        distances = self._kmeans.transform(X_scaled)\n        clusters = self._kmeans.predict(X_scaled)\n\n        X[self._new_features[:-1]] = distances\n        X[self._new_features[-1]] = clusters\n\n        return X","ed856372":"from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n\nscaler = StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])\ntrain.shape, test.shape","2041526d":"import tensorflow as tf\ntf.random.set_seed(SEED)\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom tensorflow.keras import layers\n\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=5, verbose=0,\n    mode='min')\n    \n# with Discrete layers\ndef base_model_disc(activator='relu',summary=False,bin_data=None,shape_size=None):\n    \n    # define Keras Discretization layer with number of bins parameter \n    disct_layer = tf.keras.layers.Discretization(num_bins=20)\n    disct_layer.adapt(bin_data)\n\n    inputs = tf.keras.Input(shape=(shape_size,), name='input_data')\n    x = tf.keras.layers.Dense(32, activation=activator)(inputs)\n      \n    y = disct_layer(inputs)\n    y = tf.keras.layers.Dense(32, activation=activator)(y)\n  \n    x_cnn = tf.keras.layers.concatenate([x, y])\n\n    \n    x1 = tf.keras.layers.Dense(32, activation=activator)(x_cnn)\n    x1 = tf.keras.layers.Dense(32, activation=activator)(x1)\n    x1 = tf.keras.layers.Dropout(0.2)(x1)\n    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x1)\n    model = tf.keras.Model(inputs, outputs)\n    if (summary):\n        model.summary()\n    return model","b59fbec0":"nn_oof = np.zeros(train.shape[0])\nnn_pred = np.zeros(test.shape[0])\nf_scores = []\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X=train, y=train[TARGET])):\n    print(f\"===== fold {fold} =====\")\n    X_train, y_train = train[features].iloc[trn_idx], train[TARGET].iloc[trn_idx]\n    X_valid, y_valid = train[features].iloc[val_idx], train[TARGET].iloc[val_idx]\n    X_test = test[features]\n       \n    new_features = features.copy()\n    \n    ### kmeans\n        \n    kmeans = KMeansTransformer()\n    kmeans.fit(X_train, y_train)\n    X_train = kmeans.transform(X_train)\n    X_valid = kmeans.transform(X_valid)\n    X_test = kmeans.transform(X_test)\n    kmeans_columns = kmeans._new_features\n    \n    new_features +=kmeans_columns\n        \n    \n\n    start = time.time()\n    \n        \n    nn_model = base_model_disc(activator='relu',summary=False,bin_data=X_train,shape_size=X_train.shape[1])\n    nn_model.compile(\n        keras.optimizers.Adam(learning_rate=0.001),\n        loss='binary_crossentropy',\n        metrics = ['AUC'])\n\n    history = nn_model.fit(X_train, y_train,      \n              batch_size=2048,\n              epochs=700,\n              validation_data=(X_valid, y_valid),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_valid),\n              shuffle=True,\n             verbose = 0)\n    \n    scores = pd.DataFrame(history.history)\n    scores['folds'] = fold\n    \n    if fold == 0:\n        f_scores = scores \n    else: \n        f_scores = pd.concat([f_scores, scores], axis  = 0)\n\n    nn_oof[val_idx] = nn_model.predict(X_valid).reshape(1,-1)[0]\n    nn_pred += nn_model.predict(X_test).reshape(1,-1)[0] \/ N_SPLITS\n\n    \n    elapsed = time.time() - start\n    nn_auc = roc_auc_score(y_valid, nn_oof[val_idx])\n    print(f\"fold {fold} - nn auc: {nn_auc:.6f}, elapsed time: {elapsed:.2f}sec\")\n    gc.collect()\n\nprint(f\"oof nn roc = {roc_auc_score(train[TARGET], nn_oof)}\")\n","bf695dcc":"for fold in range(f_scores['folds'].nunique()):\n    history_f = f_scores[f_scores['folds'] == fold]\n\n    fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(14,4))\n    fig.suptitle('Fold : '+str(fold), fontsize=14)\n        \n    plt.subplot(1,2,1)\n    plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n    plt.legend(fontsize=15)\n    plt.grid()\n    \n    plt.subplot(1,2,2)\n    plt.plot(history_f.loc[:, ['auc', 'val_auc']],label= ['auc', 'val_auc'])\n    plt.legend(fontsize=15)\n    plt.grid()\n    \n    print(\"Validation Loss: {:0.4f}\".format(history_f['val_loss'].min()));","2189c3d6":"sample = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nsample['target'] = nn_pred\nsample.to_csv(\"submission.csv\",index=None)","6f0c6a3c":"# **<span style=\"color:#e76f51;\">Feature Engineering<\/span>**","c272dd90":"# **<span style=\"color:#e76f51;\">Training<\/span>**","151fb12f":"# **<span style=\"color:#e76f51;\">Train and Test Distributions<\/span>**\n","4b95621d":"# **<span style=\"color:#e76f51;\">Scaling<\/span>**","edf594f4":"# **<span style=\"color:#e76f51;\">Feature Unique Values<\/span>**\n\nFollowing is a plot showing unique values in features categorized by shapes (Gaussian and Pearson like). Considering the high number of unique values maybe it's worth trying to discretize the features. Also the two features in red (Gaussian) which have similar number of uniques like blue (Pearson) are f1 and f36.\n","b91c7c6e":"# **<span style=\"color:#e76f51;\">Overview<\/span>**\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting identifying spam emails via various extracted features from the email. Features are anonymized.","75baa468":"# **<span style=\"color:#e76f51;\">Work in progress<\/span>**\n\n- Keras model needs to be optimized\n- feature selection and importance\n- ...","00cb7886":"# **<span style=\"color:#e76f51;\">Feature Interactions<\/span>**\n\nBasic features created with division,multiplication,adding and subtraction operations. I will be using **[xgbfir](https:\/\/github.com\/limexp\/xgbfir)** package to find interacting features. Xgbfir is a XGBoost model dump parser, which ranks features as well as feature interactions by different metrics. Default interaction max value is 2 for xgbfir and can be changed with 'MaxInteractionDepth' parameter.\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/i.imgur.com\/DNdjqWO.jpg\" alt=\"Heat beating\" style=\"height:200px;margin-top:3rem;\"> <\/div>\n\nSome xgbfir output metrics\n\n- Gain: Total gain of each feature or feature interaction\n- FScore: Amount of possible splits taken on a feature or feature interaction\n- wFScore: Amount of possible splits taken on a feature or feature interaction weighted by the probability of the splits to take place\n- Average wFScore: wFScore divided by FScore\n- Average Gain: Gain divided by FScore\n- Expected Gain: Total gain of each feature or feature interaction weighted by the probability to gather the gain\n","121a76cf":"# **<span style=\"color:#e76f51;\">Dataset<\/span>**\n\n- In train dataset we have 600K rows and in test dataset 540K.\n- There are 100 features all continous. \n- Target label is binary. We have a balanced dataset.\n\n","08fe28e6":"# **<span style=\"color:#e76f51;\">Target<\/span>**\n\nOur goal is to **predict** whether email is **spam or ham** based on a binary target feature called 'target'. This is a classification task.","86b6ea42":"# **<span style=\"color:#e76f51;\">History<\/span>**","aab48d92":"# **<span style=\"color:#e76f51;\">K-Means Clustering<\/span>**\n\nWe create a clustering model based on K-Means. Data first scaled then fit in K-Means clustering.  Later we calculate each row's distance to each cluster and predict cluster number also as another feature. <br>\n\nCode modified version of snippet reference: https:\/\/github.com\/mljar\/mljar-supervised <br>","95d7513e":"# **<span style=\"color:#e76f51;\">Submission<\/span>**","e2b78d80":"# **<span style=\"color:#e76f51;\">Target Feature<\/span>**\n\nOur target is binary and dataset distribution is balanced. We have a classification task here.","4d9c0eef":"# **<span style=\"color:#e76f51;\">Basic Statistics<\/span>**","48362343":"# **<span style=\"color:#e76f51;\">Target Correlations (After New Features)<\/span>**\nWe see some improvement with new features in terms of target correlation.","154fde57":"# **<span style=\"color:#e76f51;\">Keras Model with Built-in Discretization Layer<\/span>**","e2ec057a":"We see high variance in features 'f2' and 'f35'","fd7d708d":"# **<span style=\"color:#e76f51;\">Target Feature Correlations<\/span>**\n\nWe see low correlation between target and features.","82c3b5c5":"# **<span style=\"color:#e76f51;\">Features Correlation<\/span>**\n\nOur correlation plot shows that we have very low correlation between features in our dataset."}}