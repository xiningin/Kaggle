{"cell_type":{"870f233d":"code","46830c70":"code","448833e6":"code","583da87a":"code","6509a734":"code","d6d0d6f0":"code","07160ef6":"code","0de8643b":"code","8c223f0b":"code","88f1aa3f":"code","e652a3ee":"code","1ee4a1c1":"code","72f3e395":"code","8796b12b":"code","45cc38d6":"markdown","3cfd1292":"markdown","e44dae4e":"markdown"},"source":{"870f233d":"!apt install -y swig cmake libopenmpi-dev zlib1g-dev\n!pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz","46830c70":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.losses import CategoricalCrossentropy\n\n#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n\ntf.get_logger().setLevel('ERROR')","448833e6":"from tensorflow.python.framework.ops import disable_eager_execution\n\ndisable_eager_execution()","583da87a":"from tensorflow.keras.models import save_model\n\nclass REINFORCE:\n  def __init__(self, env, env_name, path=None):\n    self.env=env \n    self.env_name = env_name\n    self.state_shape=env.observation_space.shape # the state space\n    self.action_shape=env.action_space.n # the action space\n    self.gamma=0.99 # decay rate of past observations\n    self.alpha=1e-4 # learning rate of gradient\n    self.learning_rate=0.01 # learning of deep learning model\n    \n    self.best_reward = 0\n    \n    if not path:\n      self.model=self.build_policy_network() #build model\n    else:\n      self.model=self.load_model(path) #import model\n\n    # record observations\n    self.states=[]\n    self.gradients=[] \n    self.rewards=[]\n    self.probs=[]\n    self.discounted_rewards=[]\n    self.total_rewards=[]\n    \n\n  def build_policy_network(self):\n    model = Sequential()\n    model.add(Dense(24, input_shape=self.state_shape, activation='relu'))\n    model.add(Dense(12, activation='relu'))\n    model.add(Dense(self.action_shape, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.learning_rate))\n    \n    return model\n\n  def hot_encode_action(self, action):\n\n    action_encoded=np.zeros(self.action_shape)\n    action_encoded[action]=1\n\n    return action_encoded\n  \n  def remember(self, state, action, action_prob, reward):\n    encoded_action = self.hot_encode_action(action)\n    self.states.append(state)\n    self.rewards.append(reward)\n    self.probs.append(action_prob)\n    self.gradients.append(encoded_action-action_prob)\n    \n  def clear(self):\n    self.states = []\n    self.gradients=[] \n    self.rewards=[]\n    self.probs=[]\n\n  def compute_action(self, state):\n    state = state.reshape([1, state.shape[0]])\n    \n    action_prob_dist = self.model.predict(state).flatten()\n    action_prob_dist \/= np.sum(action_prob_dist)\n    \n    action = np.random.choice(self.action_shape, 1,p=action_prob_dist)[0]\n\n    return action, action_prob_dist\n\n\n  def get_discounted_rewards(self, rewards): \n   \n    discounted_rewards=[]\n    cumulative_total_return=0\n    # iterate the rewards backwards and and calc the total return \n    for reward in rewards[::-1]:      \n      cumulative_total_return=(cumulative_total_return*self.gamma)+reward\n      discounted_rewards.insert(0, cumulative_total_return)\n\n    # normalize discounted rewards\n    mean_rewards=np.mean(discounted_rewards)\n    std_rewards=np.std(discounted_rewards)\n    norm_discounted_rewards=(discounted_rewards-\n                          mean_rewards)\/(std_rewards+1e-7) # avoiding zero div\n    \n    return norm_discounted_rewards\n\n  def train_policy_network(self):\n       \n    # get X_train\n    states=np.vstack(self.states)\n\n    # get y_train\n    gradients=np.vstack(self.gradients)\n    rewards=np.vstack(self.rewards)\n    discounted_rewards=self.get_discounted_rewards(rewards)\n    gradients*=discounted_rewards\n    y_train=self.alpha*np.vstack([gradients])+self.probs\n    history=self.model.train_on_batch(states, y_train)\n    \n    self.clear()\n\n    return history\n  \n  def plot(self, save=False,file_name=None):\n    plt.figure(figsize=(16,10))\n    plt.title('Reward per epoch')\n    plt.xlabel('epochs')\n    plt.ylabel('reward')\n    plt.plot(reinforce_agent.total_rewards)\n    plt.show()\n\n    if save:\n        if not file_name:\n            raise ValueError('Missing filename')\n        plt.savefig(file_name)\n\n  def model_checkpoint(self, reward):\n    if reward > self.best_reward:\n        save_model(self.model, f'{self.env_name}-best.h5')\n\n\n  def train(self, episodes):\n        total_reward = np.zeros(episodes)\n        \n        for episode in range(episodes):\n            state = self.env.reset()\n            done = False\n            episode_reward = 0\n\n            while not done:\n                action, prob = self.compute_action(state)\n                new_state, reward, done, _ = self.env.step(action)\n                self.remember(state, action, prob, reward)\n                state = new_state\n                episode_reward += reward\n\n                if done:\n                    self.train_policy_network()\n                    self.model_checkpoint(episode_reward)\n                    break\n\n            total_reward[episode]=episode_reward\n            print('episode {}, reward {}'.format(episode, episode_reward))\n        \n        self.total_rewards=total_reward\n ","6509a734":"N_EPISODES = 500","d6d0d6f0":"def running_mean(x, N):\n    cumsum = np.cumsum(np.insert(x, 0, 0)) \n    return (cumsum[N:] - cumsum[:-N]) \/ float(N)","07160ef6":"# Implementation of REINFORCE for the Cartpole environment\n\n\nenv = gym.make('CartPole-v1')\nreinforce_agent = REINFORCE(env, 'cartpole')\nreinforce_agent.train(N_EPISODES)","0de8643b":"alphas = [0.1, 0.01, 0.001, 0.0005]\nrewards = []\n\nfor alpha in alphas:\n    tmp_agent = REINFORCE(env, f'cartpole-alpha-{alpha}')\n    tmp_agent.alpha = alpha\n    tmp_agent.train(N_EPISODES)\n    rewards.append(tmp_agent.total_rewards)","8c223f0b":"plt.figure(figsize=(16,10))\nplt.title('Avg reward per 50 epochs')\nplt.xlabel('epochs')\nplt.ylabel('reward')\nfor i, reward in enumerate(rewards):\n    plt.plot(running_mean(reward, 50), label=alphas[i])\nplt.legend()\nplt.show()","88f1aa3f":"lrs = [0.1, 0.01, 0.001, 0.0005]\nrewards = []\n\nfor lr in lrs:\n    tmp_agent = REINFORCE(env, f'cartpole-lr-{alpha}')\n    tmp_agent.learning_rate = lr\n    tmp_agent.train(N_EPISODES)\n    rewards.append(tmp_agent.total_rewards)","e652a3ee":"plt.figure(figsize=(16,10))\nplt.title('Avg reward per 50 epochs')\nplt.xlabel('epochs')\nplt.ylabel('reward')\nfor i, reward in enumerate(rewards):\n    plt.plot(running_mean(reward, 50), label=alphas[i])\nplt.legend()\nplt.show()","1ee4a1c1":"def build_policy_network(self):\n    model = Sequential()\n    model.add(Dense(10, input_shape=self.state_shape, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(self.action_shape, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.learning_rate))\n    print(model.summary())\n    return model\n\nREINFORCE.build_policy_network = build_policy_network","72f3e395":"env = gym.make('LunarLander-v2')\nreinforce_agent = REINFORCE(env, 'lunar')\nreinforce_agent.learning_rate = 0.0001\nreinforce_agent.alpha = 0.0001\nreinforce_agent.train(1200)","8796b12b":"plt.figure(figsize=(16,10))\nplt.title('Avg reward per 50 epochs')\nplt.xlabel('epochs')\nplt.ylabel('reward')\nplt.plot(running_mean(reinforce_agent.total_rewards, 50))\nplt.show()","45cc38d6":"## 2. Lunarlander environment\n\n- Solve the OpenAI Gym Lunarlander environment with REINFORCE. You can also choose another environment of your choice. \n- Plot the reward history (learning curve). For each episode plot the cumulative reward, or even better: plot average cumulative reward over a certain number of episodes (for example 50 episodes).\n- Compare the results to the ones of Deep Q-learning. Check the speed of learning, consistency of the results, etc.","3cfd1292":"##  1. The cartpole environment\n\n- Solve the OpenAI Gym cartpole environment with REINFORCE. \n- Plot the reward history (learning curve). For each episode plot the cumulative reward, or even better: plot average cumulative reward over a certain number of episodes (for example 50 episodes).\n- What is the effect of alpha, the learning rate for the gradient?\n- Do hyperparamter tuning to increase the speed of learning.\n- Compare the results to the ones of Deep Q-learning. Check the speed of learning, consistency of the results, etc.\n- Explain how the agent will explore a lot in the beginning and gradually will exploit more and more. ","e44dae4e":"# Session 06 - Policy Gradients - Assignment\n\nIn this assignment you will implement REINFORCE, a policy gradient method based on Monte Carlo sampling.\nThe key idea underlying policy gradients is to push up the probabilities of actions that lead to higher return, and push down the probabilities of actions that lead to lower return, until you arrive at the optimal policy.\n\nThe REINFORCE algorithm comprises the following steps:\n0. Initialize and reset the environment.\n1. Get the state from the environment.\n2. Feed forward our policy network to predict the probability of each action we should take. We\u2019ll sample from this distribution to choose which action to take (i.e. toss a biased coin). This implies that the ouput layer of the neural network has a Softmax activation function.\n3. Receive the reward and the next state state from the environment for the action we took.\n4. Store this transition sequence of state, action, reward, for later training.\n5. Repeat steps 1\u20134. If we receive the done flag from the game it means the episode is over.\n6. Once the episode is over, we train our neural network to learn from our stored transitions using our gradient update rule. After training you can clear the stored states, actions and rewards from the memory. \n7. Play next episode and repeat steps above until convergence"}}