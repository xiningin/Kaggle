{"cell_type":{"cca35e1a":"code","a9f12ac7":"code","addd7b25":"code","d172b5b3":"code","08f7df12":"code","6f0d1b00":"code","55465bf3":"code","1df17113":"code","d8bf82f0":"code","fae1283f":"code","f84d35bc":"code","9688992d":"code","d97f7d74":"code","a4638240":"code","04bcdcc4":"markdown"},"source":{"cca35e1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a9f12ac7":"# to prevent unnecessary warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\n\nfrom pathlib import Path\n\n#import useful module for keras library\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# get modules from sklearn library\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report \n\n#import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random","addd7b25":"file = Path(\"..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset\") #dataset location path\nFile_Path = list(file.glob(r\"**\/*.png\"))\nLabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],File_Path))\n\nFile_Path = pd.Series(File_Path).astype(str)\nLabels = pd.Series(Labels)\ndf = pd.concat([File_Path,Labels],axis=1)\ndf.columns = ['image', 'label']\n# Drop all the images that ends with (GT)\n\ndf = df[df[\"label\"].apply(lambda x: x[-2:] != \"GT\")].reset_index(drop=True)\n\ndf.head() #get first 5 rows of the dataset","d172b5b3":"\n# Display 15 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax ,in enumerate(axes.flat):\n    ax.imshow(plt.imread(df.image[i]))\n    ax.set_title(df.label[i])\n    \nplt.tight_layout()\nplt.show()","08f7df12":"\n# split remaining data into train and test sets\nTrain_set, test_set = train_test_split(df, test_size = 0.3, random_state = 42)\n\n#splitting the train set into train and evaluation set\n\ntrain_set, val_set = train_test_split(Train_set, test_size= 0.2, random_state = 42)\n\nprint(train_set.shape)\nprint(test_set.shape)\nprint(val_set.shape)","6f0d1b00":"img_gen = ImageDataGenerator(preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input, rescale=1\/255)\n\n# img_gen cannot take in an array, so ensure the data that is been passed is a dataframe\ntrain = img_gen.flow_from_dataframe(dataframe = train_set,\n    x_col = 'image', #name of the column containing the image in the train set\n    y_col ='label', #name of column containing the target in the train set\n    target_size = (224, 224),\n    color_mode = 'rgb',\n    class_mode = 'categorical',#the class mode here and that for the model_loss(when using sequential model)\n                                    #should be the same\n    batch_size = 32,\n    shuffle = False #not to shuffle the given data\n)\n\ntest = img_gen.flow_from_dataframe(dataframe = test_set,\n    x_col = 'image', #name of the column containing the image in the test set\n    y_col ='label', #name of column containing the target in the test set\n    target_size =(224, 224),\n    color_mode ='rgb',\n    class_mode ='categorical',\n    batch_size = 32,\n    shuffle = False # not to shuffle the given data\n)\n\n\nval = img_gen.flow_from_dataframe(dataframe = val_set,\n    x_col = 'image', #name of the column containing the image in the validation set\n    y_col ='label', #name of column containing the target in the validation set set\n    target_size =(224, 224),\n    color_mode ='rgb',\n    class_mode ='categorical',\n    batch_size = 32,\n    shuffle = False #set to false so as not to shuffle the given data\n)","55465bf3":"#define the input shape\ninput_shape = (224, 224, 3)\n\n# define sequential model\nmodel = tf.keras.models.Sequential()\n# define conv-pool layers - set 1\nmodel.add(tf.keras.layers.Conv2D(filters = 32, kernel_size=(3, 3), strides=(1, 1), \n                                activation='relu', padding='valid', input_shape = input_shape))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n\n# add flatten layer\nmodel.add(tf.keras.layers.Flatten())\n\n# add dense layers with some dropout\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate = 0.3))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\n\n# add output layer\nmodel.add(tf.keras.layers.Dense(9, activation='softmax')) #use softmax as activation in the output layer\n#as it is multiclass. Sigmoid activation is used for binary and 'relu' shouldnt be use for output layer\n\n\n# view model layers\nmodel.summary()","1df17113":"# compile model\nmodel.compile(optimizer='adam', # optimize the model with adam optimizer\n              loss=\"categorical_crossentropy\", \n              metrics=['accuracy']) #to get accuracy of the model in each run\n","d8bf82f0":"history = model.fit(train, #fit the model on the training set\n                    validation_data = val, #add the validation set to evaluate the performance in each run\n                    epochs = 15, #train in 10 epochs\n                    verbose = 1)","fae1283f":"acc = history.history['accuracy'] # get history report of the model\n\nval_acc = history.history['val_accuracy'] # get history of the validation set\n\nloss = history.history['loss'] #get the history of the lossses recorded on the train set\nval_loss = history.history['val_loss'] #get the history of the lossses recorded on the validation set\n\nplt.figure(figsize=(8, 8)) # set figure size for the plot generated\nplt.subplot(2, 1, 1) # a sup plot with 2 rows and 1 column\n\nplt.plot(acc, label='Training Accuracy') #plot accuracy curve for each train run\nplt.plot(val_acc, label='Validation Accuracy') #plot accuracy curve for each validation run\n\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy') #label name for y axis\nplt.ylim([min(plt.ylim()),1]) #set limit for y axis\nplt.title('Training and Validation Accuracy') #set title for the plot","f84d35bc":"plt.figure(figsize=(8, 8)) # set figure size for the plot generated\nplt.subplot(2, 1, 1) # a sup plot with 2 rows and 1 column\n\nplt.plot(loss, label='Training Loss') #plot loss curve for each train run\nplt.plot(val_loss, label='Validation Loss') #plot loss curve for each validation run\n\nplt.legend(loc='lower right')\nplt.ylabel('Loss') #label name for y axis\nplt.ylim([min(plt.ylim()),1]) #set limit for y axis\nplt.title('Training and Validation Loss') #set title for the plot","9688992d":"# Predict the label of the test_images\npred = model.predict(test)\npred = np.argmax(pred,axis = 1) # pick the class with highest probability\n# sequential model predicts by given probability for each of the classes\n#np.argmax is called on the prediction to choose the class with the highest probability\n\n# Map the label\nlabels = (train.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred2 = [labels[k] for k in pred]","d97f7d74":"from sklearn.metrics import classification_report, confusion_matrix # import metrics for evaluation\n\ny_test = test_set.label # set y_test to the expected output\n\nprint(classification_report(y_test, pred2)) # print the classification report","a4638240":"# Display 15 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\ncolor = \"blue\" if pred2[i] == test_set.label.iloc[i] else \"red\"\nfor i, ax ,in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_set.image.iloc[i]))\n    ax.set_title(f\"True: {test_set.label.iloc[i]}\\nPredicted: {pred2[i]}\",color=color)\n    \nplt.subplots_adjust(hspace = 0.3)\nplt.suptitle(\"Model predictions (blue: correct, red: incorrect)\",y=0.98)\nplt.tight_layout()\nplt.show()","04bcdcc4":"### Building Sequential CNN model "}}