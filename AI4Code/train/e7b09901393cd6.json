{"cell_type":{"00850221":"code","b0d40c2e":"code","51d32d90":"code","66bfed84":"code","827d48d1":"code","a4a76dda":"code","ab5c937d":"code","afb33590":"code","a3a159d9":"code","46a5350a":"code","f76b08e1":"code","774b316f":"code","ffe6629e":"code","2037d2fa":"code","8be31a8d":"code","a663f8df":"code","8e8c98d8":"code","9922d5e3":"code","b2064717":"code","3b8df62c":"code","27eac1f4":"code","177b5cbd":"code","ee82b8e0":"code","f19e98d5":"code","8369c6a6":"markdown","fc24f9bd":"markdown","15f8b3f2":"markdown","55249707":"markdown","583479c3":"markdown","48658e21":"markdown","135545f3":"markdown","12133c56":"markdown","f43b5971":"markdown","3c0e824b":"markdown","da85a30a":"markdown","6aa9432e":"markdown","22d3ad13":"markdown","123dc43b":"markdown","92095a0f":"markdown","22a2d71a":"markdown","ad8fc38c":"markdown","4fab8495":"markdown","13e2c544":"markdown","0cd50f8b":"markdown","654c2333":"markdown","14a57e93":"markdown","1d260563":"markdown","1af85c09":"markdown","eacfd09e":"markdown","f0a7d25f":"markdown","cb6f2a4f":"markdown","d04bd601":"markdown","6c5a8317":"markdown","e61e1d2d":"markdown","9002f8d5":"markdown"},"source":{"00850221":"# Data wrangling\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\n# OS\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm.notebook import tqdm # Progress bar\n\n# Scalers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.ensemble import VotingClassifier #for ensembling some models\nfrom sklearn.ensemble import AdaBoostClassifier #Ada Boost\nfrom sklearn.neural_network import MLPClassifier #Multi-layer Perceptron\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Boost\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\n\n# Deep learning\nimport tensorflow as tf\nimport keras\nimport torch","b0d40c2e":"# Set a Global Seed\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)                # type: ignore\n    #torch.backends.cudnn.deterministic = True  # type: ignore\n    #torch.backends.cudnn.benchmark = True      # type: ignore","51d32d90":"# Timer\n@contextmanager\ndef timer(name: str) -> None:\n    \"\"\"Timer Util\"\"\"\n    t0 = time.time()\n    print(\"[{}] start\".format(name))\n    yield\n    print(\"[{}] done in {:.0f} s\".format(name, time.time() - t0))\n\nset_seed(42)","66bfed84":"# 1. Scaling\n# MinMaxScaler : To [0,1]\nfrom sklearn.preprocessing import MinMaxScaler\n\n# StandardScaler : Mean=0, std=1\nfrom sklearn.preprocessing import StandardScaler\n\n# 2. Outliers\n# Winsorization : The main purpose of winsorization is to remove outliers by clipping feature's values.\n\n# 3. Rank\nfrom scipy.stats import rankdata\n\n# 4. Transformation\n# Log transform : np.log(1+x)\n# Raising to the power < 1 : np.sqrt(x + 2\/3)\n\n# FEATURE GENERATION\n# Ex : Generating decimal feature of a sale","827d48d1":"# Label encoding\n# Alphabetical (sorted) : [S,C,Q]->[2,1,3]\nfrom sklearn.preprocessing import LabelEncoder\n\n# Order of appereance : [S,C,Q]->[1,2,3]\npandas.factorize","a4a76dda":"# One-hot encoding\npandas.get_dummies\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Combine more two\/more cat features to one features\n# Example: pclass + sex = pclass_sex","ab5c937d":"# Format example: 25.01.2009\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).year\ndf['month'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).month\ndf['day'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).day","afb33590":"# This code is taken from\n# https:\/\/www.kaggle.com\/pravinborate\/credit-card-frauddetection-balance-imbalanced-data\nfrom sklearn.utils import resample\n\n# Oversampling techniques\nrandom_sampling = resample(fraud,\n                          replace=True,\n                           n_samples = len(not_fraud),\n                           random_state = 42\n                          )\n\n# Combine minority and upsample data\nupsample = pd.concat([not_fraud,random_sampling])\n\n# Check new values are balances for the both classes or not\nupsample['Class'].value_counts()","a3a159d9":"# Downsampling techniques\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# Combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# Checking counts\ndownsampled.Class.value_counts()","46a5350a":"# Using SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n# Example of training\nsmote_logistic = LogisticRegression()\nsmote_logistic.fit(X_train,y_train)","f76b08e1":"# Useful to show all the columns\ntrain.head().T","774b316f":"from sklearn.ensemble import RandomForestClassifier\n\n# Create a copy to work with\nX = train.copy()\n\n# Save and drop labels\ny = train.y\nX = X.drop('y', axis=1)\n\n# fill NANs \nX = X.fillna(-999)\n\n# Label encoder\nfor c in train.columns[train.dtypes == 'object']:\n    X[c] = X[c].factorize()[0]\n    \nrf = RandomForestClassifier()\nrf.fit(X,y)","ffe6629e":"plt.plot(rf.feature_importances_)\nplt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);","2037d2fa":"train_enc =  pd.DataFrame(index = train.index)\n\nfor col in tqdm_notebook(traintest.columns):\n    train_enc[col] = train[col].factorize()[0]","8be31a8d":"mask = (nunique.astype(float)\/train.shape[0] < 0.8) & (nunique.astype(float)\/train.shape[0] > 0.4)\ntrain.loc[:25, mask]","a663f8df":"cat_cols = list(train.select_dtypes(include=['object']).columns)\nnum_cols = list(train.select_dtypes(exclude=['object']).columns)","8e8c98d8":"def autolabel(arrayA):\n    ''' Label each colored square with the corresponding data value. \n    If value > 20, the text is in black, else in white.\n    '''\n    arrayA = np.array(arrayA)\n    for i in range(arrayA.shape[0]):\n        for j in range(arrayA.shape[1]):\n                plt.text(j,i, \"%.2f\"%arrayA[i,j], ha='center', va='bottom',color='w')","9922d5e3":"def hist_it(feat):\n    ''' Make a histogram\n    '''\n    plt.figure(figsize=(16,4))\n    feat[Y==0].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.8)\n    feat[Y==1].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.5)\n    plt.ylim((0,1))\n    \ndef hist_it1(feat):\n    plt.figure(figsize=(16,4))\n    feat[Y==0].hist(bins=100,range=(feat.min(),feat.max()),normed=True,alpha=0.5)\n    feat[Y==1].hist(bins=100,range=(feat.min(),feat.max()),normed=True,alpha=0.5)\n    plt.ylim((0,1))","b2064717":"def gt_matrix(feats,sz=16):\n    '''Make a > (greater than) matrix to observe patterns in features\n    '''\n    a = []\n    for i,c1 in enumerate(feats):\n        b = [] \n        for j,c2 in enumerate(feats):\n            mask = (~train[c1].isnull()) & (~train[c2].isnull())\n            if i>=j:\n                b.append((train.loc[mask,c1].values>=train.loc[mask,c2].values).mean())\n            else:\n                b.append((train.loc[mask,c1].values>train.loc[mask,c2].values).mean())\n\n        a.append(b)\n\n    plt.figure(figsize = (sz,sz))\n    plt.imshow(a, interpolation = 'None')\n    _ = plt.xticks(range(len(feats)),feats,rotation = 90)\n    _ = plt.yticks(range(len(feats)),feats,rotation = 0)\n    autolabel(a)","3b8df62c":"# SOURCE: Kaggle Automated Kernel\n# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","27eac1f4":"# SOURCE: Kaggle Automated Kernel\n# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","177b5cbd":"# SOURCE: Kaggle Automated Kernel\n# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","ee82b8e0":"# Holdout validation : n-groups = 1\n# Enough data but score and optimal parameter are similar\nfrom sklearn.model_selection import ShuffleSplit\n\n# K-fold : n-groups = k\n# Enough data but score and optimal parameter differ\nfrom sklearn.model_selection import Kfold\n\n# Leave-one-out : n-groups = len(train)\n# Small amount of data\nfrom sklearn.model_selection import LeaveOneOut","f19e98d5":"# https:\/\/github.com\/justmarkham\/scikit-learn-tips\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# set up preprocessing for numeric columns\nimp_median = SimpleImputer(strategy='median', add_indicator=True)\nscaler = StandardScaler()\n\n# set up preprocessing for categorical columns\nimp_constant = SimpleImputer(strategy='constant')\nohe = OneHotEncoder(handle_unknown='ignore')\n\n# select columns by data type\nnum_cols = make_column_selector(dtype_include='number')\ncat_cols = make_column_selector(dtype_exclude='number')\n\n# do all preprocessing\npreprocessor = make_column_transformer(\n    (make_pipeline(imp_median, scaler), num_cols),\n    (make_pipeline(imp_constant, ohe), cat_cols))\n\n# create a pipeline\npipe = make_pipeline(preprocessor, LogisticRegression())\n\n# cross-validate the pipeline\ncross_val_score(pipe, X, y).mean()\n\n# fit the pipeline and make predictions\npipe.fit(X, y)\npipe.predict(X_new)","8369c6a6":"<span style='color:blue;font-size:18px' >Work in progress...<\/span>","fc24f9bd":"### Using mask in .loc","15f8b3f2":"<h3 class=\"list-group-item list-group-item-action active\">Exploratory Data Analysis (EDA)<\/h3> <a id=\"eda\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","55249707":"- Make a > (greater than) matrix to observe patterns in features","583479c3":"- Make histogram (Hist_it)","48658e21":"### Submission problems\n\nUsually on Kaggle it is allowed to select two final submissions, which will be checked against the private LB and contribute to the competitor's final position. A common practice is to select one submission with a **best validation score**, and another submission which **scored best on Public LB**.\n\n\n**Cause of LB Shuffle** <br>\n\u2022 Randomness <br>\n\u2022 Little amount of data <br>\n\u2022 Different public\/private distributions","135545f3":"### Retrieving categorical & numerical columns","12133c56":"**Stratification** : Preserve the same target distribution over different folds\n\nStratification is useful for: <br>\n\u2022 Small datasets <br>\n\u2022 Unbalanced datasets <br>\n\u2022 Multiclass clasification","f43b5971":"### Data splitting strategies\nValidation should be set up to mimic the train\/test split of a competition\n\n1. Random, rowwise\n2. Timewise (Ex: Predicting sales in a shop)\n3. By ID\n4. Combined\n\n**Example:** <br>\n\u2022 Train set: Ratio of Men >> Ratio of Women <br>\n\u2022 Test set: Ratio of Women >> Ratio of Men\n\n**What to do ?** <br>\n*Adjust\/force the validation data so that it mimic the test set (Ratio of Women >> Ratio of Men)*","3c0e824b":"- Correlation matrixs","da85a30a":"<h3 class=\"list-group-item list-group-item-action active\">Validation<\/h3> <a id=\"validation\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>\n\n### Common validation strategies\n**Holdout scheme:**\n1. Split train data into two parts: partA and partB.\n2. Fit the model on partA, predict for partB.\n3. Use predictions for partB for estimating model quality. Find such hyper-parameters, that quality on partB is maximized.\n\n**K-Fold scheme:**\n1. Split train data into K folds.\n2. Iterate though each fold: retrain the model on all folds except current fold, predict for the current fold.\n3. Use the predictions to calculate quality on each fold. Find such hyper-parameters, that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement.\n\n**LOO (Leave-One-Out) scheme:**\n1. Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset).\n2. In the end you will get LOO predictions for every sample in the trainset and can calculate loss.","6aa9432e":"### Numeric features","22d3ad13":"### Feature encoding using factorize","123dc43b":"### Additional external sources\n* <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\">Preprocessing in Sklearn<\/a> <br>\n* <a href=\"https:\/\/machinelearningmastery.com\/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\/\">Discover Feature Engineering<\/a>\n","92095a0f":"### Datetime features","22a2d71a":"### Categorical features","ad8fc38c":"- Distribution graphs (histogram\/bar graph) of column data","4fab8495":"### Functions\nCodes are hidden. Expand to show codes.","13e2c544":"<h3 class=\"list-group-item list-group-item-action active\">Common Library and Utilities<\/h3> <a id=\"common\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","0cd50f8b":"<h3 class=\"list-group-item list-group-item-action active\">Imbalance Dataset<\/h3> <a id=\"imb\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","654c2333":"Three common methods to deal with imbalance dataset:\n* Oversampling the minority\n* Undersampling the majority\n* Using SMOTE\n>A technique that is similar to upsampling. Smote creates some synthetic samples. SMOTE (Synthetic Minority Oversampling Technique) works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and drawing a new sample at a point along that line. SMOTE is similar to the nearest neighbor algorithm.","14a57e93":"### Finding out feature importance","1d260563":"<h3 class=\"list-group-item list-group-item-action active\">Simple ML Baseline <\/h3> <a id=\"baseline\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","1af85c09":"- Scatter and Density Plots","eacfd09e":"<h1 align=\"center\"  style='color:white;font-size:30px;background-color:black;'> My Personal Data Science Notes <\/h1>\n<h2> \ud83d\udc4b Hi Everyone, <\/h2>\n\nHere are my personal notes about various **fundamental** and **unique** methods or functions related to **Data Science** that are gathered from multiple notebooks and sources (Mostly from a <a href=\"https:\/\/www.coursera.org\/learn\/competitive-data-science\/\">Coursera Course<\/a>). \n\nI wrote this notebook to support my journey in learning Data Science and I hope that this notebook would be useful for fellow learners too. Thank you!\n\n<a id=\"toc\"><\/a>\n<h2>\ud83d\udde8\ufe0f List of contents:<\/h2>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#common\">Common Library and Utilities<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#imb\">Data Types & Preprocessing<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#tod\">Imbalance Dataset<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#eda\">Exploratory Data Analysis<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#validation\">Validation<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#leakages\">Data Leakages<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#baseline\">Simple ML Baseline<\/a>\n<\/ul>\n<\/div>","f0a7d25f":"### Transpose viewing","cb6f2a4f":"### Additional external sources\n* <a href=\"http:\/\/www.chioka.in\/how-to-select-your-final-models-in-a-kaggle-competitio\/\"> Advices on validation in a competition <\/a>\n* <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\"> Validation in Sklearn <\/a>\n","d04bd601":"- Autolabel","6c5a8317":"<h3 class=\"list-group-item list-group-item-action active\">Data Leakages <\/h3> <a id=\"leakages\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>\n\nThe most common types of leaks:\n1. Date \/ Time\n2. Meta data\n3. Information in IDs -> Might be a hash of something\n4. Row order\n\n### Additional external source\n* <a href=\"https:\/\/www.kaggle.com\/olegtrott\/the-perfect-score-script\">The \"Perfect Score\" Script<a>","e61e1d2d":"### Ordinal features\nExample:\nTicket class, driver's license type, education","9002f8d5":"<h3 class=\"list-group-item list-group-item-action active\">Data Types & Preprocessing<\/h3> <a id=\"tod\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>"}}