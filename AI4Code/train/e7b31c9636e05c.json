{"cell_type":{"7c27f8b2":"code","95ac3bd0":"code","e390d5a0":"code","c7d5983c":"code","0813eab1":"code","d60cd09a":"code","45890959":"code","e2cbe873":"code","80c5b4a2":"code","6d194592":"code","388fbc8e":"code","b39cb2ab":"code","9d289df0":"code","d78e0368":"code","b46b2deb":"code","47d984eb":"code","6e37974e":"code","ee951168":"markdown","5e6e10e1":"markdown","4ef145a2":"markdown","eee6c2d2":"markdown","c4708c6f":"markdown","68627690":"markdown"},"source":{"7c27f8b2":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport time\nimport pprint\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold, train_test_split\nfrom sklearn.cluster import KMeans\n\nSEEDS = 42","95ac3bd0":"def rmse(y_true, y_pred):\n    return (mean_squared_error(y_true, y_pred))** .5","e390d5a0":"# load_data\n# I prepared dataset beforehand and skipped this process in this notebook.\n# You can easily generate augmented dataset. \n# Plese refer to tito's notebook.\n\ntrain = pd.read_json('')\ntest = pd.read_json('')","c7d5983c":"train_data = []\nfor mol_id in train['id'].unique():\n    sample_data = train.loc[train['id'] == mol_id]\n    sample_seq_length = sample_data.seq_length.values[0]\n    \n    for i in range(68):\n        sample_dict = {'id' : sample_data['id'].values[0],\n                       'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                       'sequence' : sample_data['sequence'].values[0][i],\n                       'structure' : sample_data['structure'].values[0][i],\n                       'predicted_loop_type' : sample_data['predicted_loop_type'].values[0][i],\n                       'reactivity' : sample_data['reactivity'].values[0][i],\n                       'reactivity_error' : sample_data['reactivity_error'].values[0][i],\n                       'deg_Mg_pH10' : sample_data['deg_Mg_pH10'].values[0][i],\n                       'deg_error_Mg_pH10' : sample_data['deg_error_Mg_pH10'].values[0][i],\n                       'deg_pH10' : sample_data['deg_pH10'].values[0][i],\n                       'deg_error_pH10' : sample_data['deg_error_pH10'].values[0][i],\n                       'deg_Mg_50C' : sample_data['deg_Mg_50C'].values[0][i],\n                       'deg_error_Mg_50C' : sample_data['deg_error_Mg_50C'].values[0][i],\n                       'deg_50C' : sample_data['deg_50C'].values[0][i],\n                       'deg_error_50C' : sample_data['deg_error_50C'].values[0][i],\n                       'bpps_sum' : sample_data['bpps_sum'].values[0][i],\n                       'bpps_max' : sample_data['bpps_max'].values[0][i],\n                       'bpps_nb' : sample_data['bpps_nb'].values[0][i]}\n        \n        \n        shifts = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n        shift_cols = ['sequence', 'structure', 'predicted_loop_type']\n        for shift,col in itertools.product(shifts, shift_cols):\n            if i - shift >= 0:\n                sample_dict['b'+str(shift)+'_'+col] = sample_data[col].values[0][i-shift]\n            else:\n                sample_dict['b'+str(shift)+'_'+col] = -1\n            \n            if i + shift <= sample_seq_length - 1:\n                sample_dict['a'+str(shift)+'_'+col] = sample_data[col].values[0][i+shift]\n            else:\n                sample_dict['a'+str(shift)+'_'+col] = -1\n\n        shift_cols_2 = ['bpps_sum', 'bpps_max', 'bpps_nb']\n        for shift,col in itertools.product(shifts, shift_cols_2):\n            if i - shift >= 0:\n                sample_dict['b'+str(shift)+'_'+col] = sample_data[col].values[0][i-shift]\n            else:\n                sample_dict['b'+str(shift)+'_'+col] = -999\n            \n            if i + shift <= sample_seq_length - 1:\n                sample_dict['a'+str(shift)+'_'+col] = sample_data[col].values[0][i+shift]\n            else:\n                sample_dict['a'+str(shift)+'_'+col] = -999\n        \n        \n        train_data.append(sample_dict)\ntrain_data = pd.DataFrame(train_data)\ntrain_data.head()","0813eab1":"test_data = []\nfor mol_id in test['id'].unique():\n    sample_data = test.loc[test['id'] == mol_id]\n    sample_seq_length = sample_data.seq_length.values[0]\n    for i in range(sample_seq_length):\n        sample_dict = {'id' : sample_data['id'].values[0],\n                       'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                       'sequence' : sample_data['sequence'].values[0][i],\n                       'structure' : sample_data['structure'].values[0][i],\n                       'predicted_loop_type' : sample_data['predicted_loop_type'].values[0][i],\n                       'bpps_sum' : sample_data['bpps_sum'].values[0][i],\n                       'bpps_max' : sample_data['bpps_max'].values[0][i],\n                       'bpps_nb' : sample_data['bpps_nb'].values[0][i]}\n        \n        shifts = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n        shift_cols = ['sequence', 'structure', 'predicted_loop_type']\n        for shift,col in itertools.product(shifts, shift_cols):\n            if i - shift >= 0:\n                sample_dict['b'+str(shift)+'_'+col] = sample_data[col].values[0][i-shift]\n            else:\n                sample_dict['b'+str(shift)+'_'+col] = -1\n            \n            if i + shift <= sample_seq_length - 1:\n                sample_dict['a'+str(shift)+'_'+col] = sample_data[col].values[0][i+shift]\n            else:\n                sample_dict['a'+str(shift)+'_'+col] = -1\n\n        shift_cols_2 = ['bpps_sum', 'bpps_max', 'bpps_nb']\n        for shift,col in itertools.product(shifts, shift_cols_2):\n            if i - shift >= 0:\n                sample_dict['b'+str(shift)+'_'+col] = sample_data[col].values[0][i-shift]\n            else:\n                sample_dict['b'+str(shift)+'_'+col] = -999\n            \n            if i + shift <= sample_seq_length - 1:\n                sample_dict['a'+str(shift)+'_'+col] = sample_data[col].values[0][i+shift]\n            else:\n                sample_dict['a'+str(shift)+'_'+col] = -999\n        \n        test_data.append(sample_dict)\ntest_data = pd.DataFrame(test_data)\ntest_data.head()","d60cd09a":"# label_encoding\nsequence_encmap = {'A': 0, 'G' : 1, 'C' : 2, 'U' : 3}\nstructure_encmap = {'.' : 0, '(' : 1, ')' : 2}\nlooptype_encmap = {'S':0, 'E':1, 'H':2, 'I':3, 'X':4, 'M':5, 'B':6}\n\nenc_targets = ['sequence', 'structure', 'predicted_loop_type']\nenc_maps = [sequence_encmap, structure_encmap, looptype_encmap]\n\nfor t,m in zip(enc_targets, enc_maps):\n    for c in [c for c in train_data.columns if t in c]:\n        #train_data[c] = train_data[c].replace(m)\n        test_data[c] = test_data[c].replace(m)","45890959":"# I recommend you save both the dataset. Because, it takes long to generate these.\ntest_data.to_cav('aug_test_df_2.csv', index=False)\ntrain_data.to_csv('aug_train_df_2.csvv' index=False)","e2cbe873":"# load prepared train data and test data\ntrain_data = pd.read_csv('..\/input\/augmented-dataset\/aug_train_df_2.csv')\ntest_data = pd.read_csv('..\/input\/augmented-dataset\/aug_test_df_2.csv')\nsubmission = pd.read_csv('..\/input\/stanford-covid-vaccine\/sample_submission.csv')","80c5b4a2":"train_data.head()","6d194592":"# to save time, I only use 3 columns as targets.\n#targets = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\ntargets = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']","388fbc8e":"# features for training\nfeatures = ['sequence', 'structure', 'predicted_loop_type', 'bpps_sum', 'bpps_max', 'bpps_nb', \n            'b1_sequence', 'a1_sequence', 'b1_structure', 'a1_structure', 'b1_predicted_loop_type', \n            'a1_predicted_loop_type', 'b2_sequence', 'a2_sequence', 'b2_structure', 'a2_structure', \n            'b2_predicted_loop_type', 'a2_predicted_loop_type', 'b3_sequence', 'a3_sequence', \n            'b3_structure', 'a3_structure', 'b3_predicted_loop_type', 'a3_predicted_loop_type', \n            'b4_sequence', 'a4_sequence', 'b4_structure', 'a4_structure', 'b4_predicted_loop_type', \n            'a4_predicted_loop_type', 'b5_sequence', 'a5_sequence', 'b5_structure', 'a5_structure', \n            'b5_predicted_loop_type', 'a5_predicted_loop_type', 'b6_sequence', 'a6_sequence', \n            'b6_structure', 'a6_structure', 'b6_predicted_loop_type', 'a6_predicted_loop_type', \n            'b7_sequence', 'a7_sequence', 'b7_structure', 'a7_structure', 'b7_predicted_loop_type', \n            'a7_predicted_loop_type', 'b8_sequence', 'a8_sequence', 'b8_structure', 'a8_structure', \n            'b8_predicted_loop_type', 'a8_predicted_loop_type', 'b9_sequence', 'a9_sequence', \n            'b9_structure', 'a9_structure', 'b9_predicted_loop_type', 'a9_predicted_loop_type', \n            'b10_sequence', 'a10_sequence', 'b10_structure', 'a10_structure', \n            'b10_predicted_loop_type', 'a10_predicted_loop_type', 'b11_sequence', 'a11_sequence', \n            'b11_structure', 'a11_structure', 'b11_predicted_loop_type', 'a11_predicted_loop_type', \n            'b12_sequence', 'a12_sequence', 'b12_structure', 'a12_structure', \n            'b12_predicted_loop_type', 'a12_predicted_loop_type', 'b13_sequence', 'a13_sequence', \n            'b13_structure', 'a13_structure', 'b13_predicted_loop_type', 'a13_predicted_loop_type', \n            'b14_sequence', 'a14_sequence', 'b14_structure', 'a14_structure', \n            'b14_predicted_loop_type', 'a14_predicted_loop_type', 'b15_sequence', 'a15_sequence', \n            'b15_structure', 'a15_structure', 'b15_predicted_loop_type', 'a15_predicted_loop_type', \n            'b1_bpps_sum', 'a1_bpps_sum', 'b1_bpps_max', 'a1_bpps_max', 'b1_bpps_nb', 'a1_bpps_nb', \n            'b2_bpps_sum', 'a2_bpps_sum', 'b2_bpps_max', 'a2_bpps_max', 'b2_bpps_nb', 'a2_bpps_nb', \n            'b3_bpps_sum', 'a3_bpps_sum', 'b3_bpps_max', 'a3_bpps_max', 'b3_bpps_nb', 'a3_bpps_nb', \n            'b4_bpps_sum', 'a4_bpps_sum', 'b4_bpps_max', 'a4_bpps_max', 'b4_bpps_nb', 'a4_bpps_nb', \n            'b5_bpps_sum', 'a5_bpps_sum', 'b5_bpps_max', 'a5_bpps_max', 'b5_bpps_nb', 'a5_bpps_nb', \n            'b6_bpps_sum', 'a6_bpps_sum', 'b6_bpps_max', 'a6_bpps_max', 'b6_bpps_nb', 'a6_bpps_nb', \n            'b7_bpps_sum', 'a7_bpps_sum', 'b7_bpps_max', 'a7_bpps_max', 'b7_bpps_nb', 'a7_bpps_nb', \n            'b8_bpps_sum', 'a8_bpps_sum', 'b8_bpps_max', 'a8_bpps_max', 'b8_bpps_nb', 'a8_bpps_nb', \n            'b9_bpps_sum', 'a9_bpps_sum', 'b9_bpps_max', 'a9_bpps_max', 'b9_bpps_nb', 'a9_bpps_nb', \n            'b10_bpps_sum', 'a10_bpps_sum', 'b10_bpps_max', 'a10_bpps_max', 'b10_bpps_nb', 'a10_bpps_nb', \n            'b11_bpps_sum', 'a11_bpps_sum', 'b11_bpps_max', 'a11_bpps_max', 'b11_bpps_nb', 'a11_bpps_nb', \n            'b12_bpps_sum', 'a12_bpps_sum', 'b12_bpps_max', 'a12_bpps_max', 'b12_bpps_nb', 'a12_bpps_nb', \n            'b13_bpps_sum', 'a13_bpps_sum', 'b13_bpps_max', 'a13_bpps_max', 'b13_bpps_nb', 'a13_bpps_nb', \n            'b14_bpps_sum', 'a14_bpps_sum', 'b14_bpps_max', 'a14_bpps_max', 'b14_bpps_nb', 'a14_bpps_nb', \n            'b15_bpps_sum', 'a15_bpps_sum', 'b15_bpps_max', 'a15_bpps_max', 'b15_bpps_nb', 'a15_bpps_nb']\n# categorical features\ncols_cat = ['sequence', 'structure', 'predicted_loop_type', 'b1_sequence', 'a1_sequence', \n            'b1_structure', 'a1_structure', 'b1_predicted_loop_type', 'a1_predicted_loop_type', \n            'b2_sequence', 'a2_sequence', 'b2_structure', 'a2_structure', 'b2_predicted_loop_type', \n            'a2_predicted_loop_type', 'b3_sequence', 'a3_sequence', 'b3_structure', 'a3_structure', \n            'b3_predicted_loop_type', 'a3_predicted_loop_type', 'b4_sequence', 'a4_sequence', \n            'b4_structure', 'a4_structure', 'b4_predicted_loop_type', 'a4_predicted_loop_type', \n            'b5_sequence', 'a5_sequence', 'b5_structure', 'a5_structure', 'b5_predicted_loop_type', \n            'a5_predicted_loop_type', 'b6_sequence', 'a6_sequence', 'b6_structure', 'a6_structure', \n            'b6_predicted_loop_type', 'a6_predicted_loop_type', 'b7_sequence', 'a7_sequence', \n            'b7_structure', 'a7_structure', 'b7_predicted_loop_type', 'a7_predicted_loop_type', \n            'b8_sequence', 'a8_sequence', 'b8_structure', 'a8_structure', 'b8_predicted_loop_type', \n            'a8_predicted_loop_type', 'b9_sequence', 'a9_sequence', 'b9_structure', 'a9_structure', \n            'b9_predicted_loop_type', 'a9_predicted_loop_type', 'b10_sequence', 'a10_sequence', \n            'b10_structure', 'a10_structure', 'b10_predicted_loop_type', 'a10_predicted_loop_type', \n            'b11_sequence', 'a11_sequence', 'b11_structure', 'a11_structure', \n            'b11_predicted_loop_type', 'a11_predicted_loop_type', 'b12_sequence', 'a12_sequence', \n            'b12_structure', 'a12_structure', 'b12_predicted_loop_type', 'a12_predicted_loop_type', \n            'b13_sequence', 'a13_sequence', 'b13_structure', 'a13_structure', \n            'b13_predicted_loop_type', 'a13_predicted_loop_type', 'b14_sequence', 'a14_sequence', \n            'b14_structure', 'a14_structure', 'b14_predicted_loop_type', 'a14_predicted_loop_type', \n            'b15_sequence', 'a15_sequence', 'b15_structure', 'a15_structure', \n            'b15_predicted_loop_type', 'a15_predicted_loop_type']\n\ntrain_data[cols_cat] = train_data[cols_cat].astype('category')\ntest_data[cols_cat] = test_data[cols_cat].astype('category')\n\n# filtering train_data\ntrain_data = train_data[train_data.SN_filter == 1]","b39cb2ab":"#lgb prediction\nstart = time.time()\n\nFOLD_N = 5\nkf = KFold(n_splits=FOLD_N)\n\n# hyperparameters tuned by optuna lightgbm tuner\nparams_reactivity = {'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'boosting': 'gbdt',\n 'feature_fraction': 0.88,\n 'feature_pre_filter': False,\n 'lambda_l1': 2.763630990419107,\n 'lambda_l2': 1.6734146610847723e-08,\n 'metric': 'rmse',\n 'min_child_samples': 20,\n 'num_leaves': 100,\n 'objective': 'regression',\n 'seed': 42}\nparams_pH10 = {'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'boosting': 'gbdt',\n 'feature_fraction': 0.9840000000000001,\n 'feature_pre_filter': False,\n 'lambda_l1': 1.9508733681194392,\n 'lambda_l2': 1.98933493189486e-08,\n 'metric': 'rmse',\n 'min_child_samples': 20,\n 'num_leaves': 160,\n 'objective': 'regression',\n 'seed': 42}\nparams_50C = {'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'boosting': 'gbdt',\n 'feature_fraction': 0.8999999999999999,\n 'feature_pre_filter': False,\n 'lambda_l1': 6.302957585067799,\n 'lambda_l2': 1.0137550663617707e-08,\n 'metric': 'rmse',\n 'min_child_samples': 20,\n 'num_leaves': 139,\n 'objective': 'regression',\n 'seed': 42}\n\nparams_list = [params_reactivity, params_pH10, params_50C]\n\nresult = {}\noof_df = pd.DataFrame(train_data.id_seqpos)\n\nfor i, target in enumerate(targets):\n    params = params_list[i]\n    oof = pd.DataFrame()\n    preds = np.zeros(len(test_data))\n    print(f'- predict {target}')\n    print('')\n    \n    for n, (tr_idx, vl_idx) in enumerate(kf.split(train_data[features])):\n        tr_x, tr_y = train_data[features].iloc[tr_idx], train_data[target].iloc[tr_idx]\n        vl_x, vl_y = train_data[features].iloc[vl_idx], train_data[target].iloc[vl_idx]\n\n\n        vl_id = train_data['id_seqpos'].iloc[vl_idx]\n\n        t_data = lgb.Dataset(tr_x, label=tr_y) \n        v_data = lgb.Dataset(vl_x, label=vl_y)\n\n        model = lgb.train(params, \n                      t_data, \n                      150000, \n                      valid_sets=[t_data, v_data], \n                      verbose_eval=1000, \n                      early_stopping_rounds=150, \n                     )\n\n        vl_pred = model.predict(vl_x)\n        score = rmse(vl_y, vl_pred)\n        print(f'score : {score}')\n\n        oof = oof.append(pd.DataFrame({'id_seqpos':vl_id, target:vl_pred}))\n        pred = model.predict(test_data[features])\n        preds += pred \/ FOLD_N   \n    \n    oof_df = oof_df.merge(oof, on='id_seqpos', how='inner')\n    rmse_score = rmse(train_data[target], oof_df[target])\n    print(f'{target} rmse: {rmse_score}')\n    submission[target] = preds\n    result[target] = rmse_score\n    \nprint(time.time()-start, 'seconds has passed')","9d289df0":"'''\nVer1 result:\n{'reactivity': 0.21540200147225563,\n 'deg_Mg_pH10': 0.27175680226193105,\n 'deg_Mg_50C': 0.2279103378674039}\n'total : 0.23835638053386354'\n'''\n\ndisplay(result)\ndisplay(f'total : {np.mean(list(result.values()))}')","d78e0368":"oof_df.to_csv('oof_df.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)","b46b2deb":"!pip install optuna\nimport optuna.integration.lightgbm as opt_lgb","47d984eb":"train_opt, val_opt = train_test_split(train_data)\nbestps = []\ntargets = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']\nnum_round = 10000\n\nfor t in targets:\n  params = {'objective': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'seed' : SEEDS}\n  print(f'- tuning for {t}')\n  lgb_train = lgb.Dataset(train_opt[features], train_opt[t])\n  lgb_valid = lgb.Dataset(val_opt[features], val_opt[t])\n\n  best = opt_lgb.train(params, lgb_train, num_boost_round=num_round,\n                        valid_names=[\"train\", \"valid\"], valid_sets=[lgb_train, lgb_valid],\n                        verbose_eval = 0,  early_stopping_rounds = 150)\n  pprint.pprint(best.params)\n  bestps.append(best.params)\n  print('')","6e37974e":"for i in range(3):\n    pprint.pprint(bestps[i])","ee951168":"# Intoroduction","5e6e10e1":"# preprocess data 1  - generate shift features -","4ef145a2":"In this competition, diversity of prediction datasets is really important to improve LB score by blending or stacking.\n\nMost of all notebooks are using neural networks including gru, lstm and graph.\n\nTo increase diversity of prediction dataset, I tried to generate good predictions with lightgbm.\n\nAs a result, I got LB score of 0.27652.\n\nThis notebook is mainly based on https:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation, https:\/\/www.kaggle.com\/its7171\/how-to-generate-augmentation-data both by tito, and https:\/\/www.kaggle.com\/t88take\/openvaccine-simple-lgb-baseline by T88.\n\nThank you very much!\n\n\n\nVersion 2:\n\nI corrected some mistakes. I did parameter tuning for each target by optuna lightgbm tuner. https:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258\n\nLocal CV was improved from 0.23835 to 0.23392.","eee6c2d2":"# LGB train and predict","c4708c6f":"# preprocess data 2","68627690":"# optuna parameter tuning"}}