{"cell_type":{"3108f8b4":"code","170d67c0":"code","ec48cbc4":"code","eed5e659":"code","5255ed81":"code","38195100":"code","2a21ff21":"code","60cfc468":"code","3d661a14":"code","289c7238":"code","1611b709":"code","9c657f60":"code","bef78f15":"code","1b4490da":"code","3c90eb01":"code","672f8a5f":"code","0f26919a":"code","eaa54c1e":"code","5fd06810":"code","5b772f4f":"code","5a2902f2":"code","aa73957e":"code","438430be":"code","3562bf7b":"code","9ef1fba5":"code","45973da3":"markdown","e0a3a675":"markdown","bea1635a":"markdown","a9e7303d":"markdown","709b9d0e":"markdown","250852d6":"markdown","fa45ecd6":"markdown","ab82bf04":"markdown","ef031b52":"markdown","a7cc28b7":"markdown","a6aa854c":"markdown","9182794e":"markdown","630d0a83":"markdown"},"source":{"3108f8b4":"#importing the libaries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","170d67c0":"data=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.head(10)\n#Everything is good for now","ec48cbc4":"data['SalePrice'].describe()","eed5e659":"sns.boxplot(data['SalePrice'],linewidth=1.5)","5255ed81":"sns.distplot(data['SalePrice'],color='red')","38195100":"corrmap = data.corr()\nax = plt.subplots(figsize=(19, 16))\nsns.heatmap(corrmap, vmax=.8, square=True,cmap='coolwarm')","2a21ff21":"#here we are going to choose the most correlated features to target data \ncorr_cols=corrmap.nlargest(9,'SalePrice')['SalePrice'].index\nax = plt.subplots(figsize=(9, 7))\nsns.heatmap(np.corrcoef(data[corr_cols].values.T),cbar=True,cmap='coolwarm',\n           annot=True,square=True,fmt='.2f',annot_kws={'size':9},\n            xticklabels=corr_cols.values,yticklabels=corr_cols.values)\n#here we choose just best8 features the other below 0.5 so it better focus on the important one ","60cfc468":"LAvsPrice=pd.concat([data['SalePrice'],data['GrLivArea']],axis=1)\nsns.regplot(x='GrLivArea',y='SalePrice',data=LAvsPrice)","3d661a14":"data.sort_values(by='GrLivArea',ascending = False)[:2]\ndata=data.drop(data[data['Id']==1299].index)\ndata=data.drop(data[data['Id']==524].index)\nprint(\"done!!\")","289c7238":"LAvsPrice=pd.concat([data['SalePrice'],data['GrLivArea']],axis=1)\nsns.regplot(x='GrLivArea',y='SalePrice',data=LAvsPrice)","1611b709":"X=data.filter(['OverallQual', 'GrLivArea', 'GarageCars',\n      'TotalBsmtSF','1stFlrSF', 'FullBath','TotRmsAbvGrd'],axis=1)\n#notice here that we didnt include 'GarageArea' 'cause obviously is the same as 'GarageCars'\ny=data.filter(['SalePrice'],axis=1)\n\nX.head(10)\n#y.head(10)","9c657f60":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX = sc.fit_transform(X)","bef78f15":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=40)\n\n#X_train\n#X_test\n#y_train\n#y_test","1b4490da":"y_train","3c90eb01":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nSelectedModel = GradientBoostingRegressor(learning_rate=0.05, max_depth=2, \n                                        min_samples_leaf=14,\n                                        min_samples_split=50, n_estimators=3000,\n                                        random_state=40)\nSelectedParameters = {'loss':('ls','huber','lad'\n                            ,'quantile'),'max_features':('auto','sqrt','log2')}\n\n\nGridSearchModel = GridSearchCV(SelectedModel,SelectedParameters,return_train_score=True)\nGridSearchModel.fit(X_train, y_train)\n","672f8a5f":"sorted(GridSearchModel.cv_results_.keys())\nGridSearchResults = pd.DataFrame(GridSearchModel.cv_results_)[['mean_test_score', 'std_test_score'\n                                                               , 'params' , 'rank_test_score' , 'mean_fit_time']]\n\nprint('All Results are :\\n', GridSearchResults )\nprint('Best Score is :', GridSearchModel.best_score_)\nprint('Best Parameters are :', GridSearchModel.best_params_)#this is what we need \nprint('Best Estimator is :', GridSearchModel.best_estimator_)","0f26919a":"#accourding to GridSearchCV the best parametre is {'loss': 'huber', 'max_features': 'sqrt'} \nGBR = GradientBoostingRegressor(learning_rate=0.05, loss='huber', max_depth=2, \n                                       max_features='sqrt', min_samples_leaf=14,\n                                       min_samples_split=50, n_estimators=3000,\n                                       random_state=42)\nGBR.fit(X_train, y_train)\nprint(\"done!!!\")\n","eaa54c1e":"print(\"Train Score\", GBR.score(X_train, y_train))\nprint(\"Test Score\", GBR.score(X_test, y_test))","5fd06810":"#predict the test data \ny_pred = GBR.predict(X_test)\nprint(\"done again !!\")","5b772f4f":"from sklearn.metrics import mean_absolute_error\n#Calculating Mean Absolute Error\nMAEValue = mean_absolute_error(y_test, y_pred, multioutput='uniform_average')\nprint('Mean Absolute Error Value is : ', MAEValue)","5a2902f2":"RealData=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nRealData.head(10)","aa73957e":"X1=RealData.filter(['OverallQual', 'GrLivArea', 'GarageCars',\n      'TotalBsmtSF','1stFlrSF', 'FullBath','TotRmsAbvGrd'],axis=1)\n\nX1.head(10)","438430be":"#there is some values are null so we need to get rid of them \nX1=X1.fillna(0)\nprint(\"coool!!\")","3562bf7b":"#repeat the same step we did with the train file ...\nsc0 = StandardScaler()\nX1 = sc0.fit_transform(X1)\n\n#now let create the file and save our prediction \n\nDoc=pd.DataFrame()\nDoc['Id']=RealData['Id']\nDoc['SalePrice']=np.round(GBR.predict(X1),2)\nprint(Doc['SalePrice'].head(10))","9ef1fba5":"Doc.to_csv('SalePrice_submission.csv',index=False)\nprint('great !!! ')","45973da3":"    Now we found some important point that our data have a positive skewness.that explain the value of the mean.\n","e0a3a675":"We are going to use The Gradient Boosting Regressor but before we need to know what the best parameter to use also we are going to need GridSearchCV for this job.","bea1635a":"    After that we will import Just the train file first for testing the regression and leave the test file until the end of the algorithm to apply.","a9e7303d":"2. Testing to get best possible accuracy \n\n    In this section we are going to create the test and train data from the train file and test the data before moving to the test file.","709b9d0e":"Now let move the last step which is importing the test data and apply the GBR algorithm","250852d6":"1. Relationship with Target Data\n    \n    In this Section we will discuss the relation between the target data which is \"SalePrice\" and *all the numerical features*, and see which one effect the target most.","fa45ecd6":"The Last Step is to save the file","ab82bf04":"Let's move to see the correlation matrix","ef031b52":"Now everything looks good","a7cc28b7":"We notice there is two point are far from regression line and these two may effect the study and make mislead to the predicted data so the solution here is to remove them.","a6aa854c":"Now it's time to split the data we will make 10% for test the the rest for the train ","9182794e":"    Before we go into this section we should study the target data \"SalePrice\"\n    ","630d0a83":"To the next Step !!"}}