{"cell_type":{"9e9255a0":"code","3314bfc4":"code","713353be":"code","7df2dbd8":"code","9cc6d48f":"code","6e76aa6d":"code","a7f52c32":"code","54ee0158":"code","527cc33b":"code","2eea3cf4":"code","da348cbd":"code","1b418673":"code","3d9573b3":"code","7eb73dd6":"code","a96f9a6c":"code","b54fdb52":"code","5481526e":"code","fc1f7a53":"code","ccfd4703":"code","ccb5a351":"code","50ab2b83":"code","30a6ed95":"code","1e868055":"code","41c67922":"code","486523c5":"code","907df476":"code","7fcaee7e":"code","0d3c87a4":"code","386cad9e":"code","6ee45c1c":"code","e6b470cc":"code","6e4d3009":"code","fc529bfa":"code","816096b4":"code","67372c79":"code","287a5ec1":"code","e0fa1b04":"code","dffca65c":"code","382357de":"code","952efc98":"code","e0990da5":"code","60a4ec9f":"markdown","88e79939":"markdown","79fb4930":"markdown","ac35acfd":"markdown","32e5c1f5":"markdown","7b373f76":"markdown","091c0e89":"markdown","60ee9ad6":"markdown","11c3dcd8":"markdown","424bdba4":"markdown","95bde321":"markdown","1d3f3d75":"markdown","81347cc3":"markdown","6909bc35":"markdown","db5cd4e9":"markdown","9916f355":"markdown","4659ee58":"markdown","8fff2c10":"markdown","a1529d08":"markdown","4fa820ae":"markdown"},"source":{"9e9255a0":"def missing(df) : \n    missing_number = df.isnull().sum().sort_values(ascending = False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending = False)\n    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_number', 'Missing_percent'])\n    return missing_values \n\ndef categorize(df) :\n    Quantitive_features = df.select_dtypes([np.number]).columns.tolist()\n    Categorical_features = df.select_dtypes(exclude = [np.number]).columns.tolist()\n    Discrete_features = [col for col in Quantitive_features if len(df[col].unique()) < 10]\n    Continuous_features = [col for col in Quantitive_features if col not in Discrete_features]\n    print(f\"Quantitive feautres : {Quantitive_features} \\nDiscrete features : {Discrete_features} \\nContinous features : {Continuous_features} \\nCategorical features : {Categorical_features}\\n\")\n    print(f\"Number of quantitive feautres : {len(Quantitive_features)} \\nNumber of discrete features : {len(Discrete_features)} \\nNumber of continous features : {len(Continuous_features)} \\nNumber of categorical features : {len(Categorical_features)}\")\n    \ndef unique(df) : \n    tb1 = pd.DataFrame({'Columns' : df.columns, 'Number_of_Unique' : df.nunique().values.tolist(),\n                       'Sample1' : df.sample(1).values.tolist()[0], 'Sample2' : df.sample(1).values.tolist()[0], \n                       'Sample3' : df.sample(1).values.tolist()[0],\n                       'Sample4' : df.sample(1).values.tolist()[0], 'Sample5' : df.sample(1).values.tolist()[0]})\n    return tb1\n    \ndef data_glimpse(df) :   \n    \n    # Dataset preview \n    print(\"1. Dataset Preview \\n\")\n    display(df.head())\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    # Columns imformation\n    print(\"2. Column Imformation \\n\")\n    print(f\"Dataset have {df.shape[0]} columns and {df.shape[1]} rows\")\n    print(\"\\n\") \n    print(f\"Dataset Column name : {df.columns.values}\")\n    print(\"\\n\")\n    categorize(df)\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    # Basic imformation table \n    print(\"3. Missing data table : \\n\")\n    display(missing(df))\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(\"4. Number of unique value by column : \\n\")\n    display(unique(df))\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(\"5. Describe table : \\n\")\n    display(df.describe())\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(df.info())\n    print(\"-------------------------------------------------------------------------------\\n\")","3314bfc4":"# Data Analysis\nimport warnings \nwarnings.filterwarnings('ignore')\n    \nimport pandas as pd\nimport numpy as np\n    \n# Data View\npd.options.display.max_columns = 200\n\n# Import Basic Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n    \n# Data Visualization : Plotly library \nimport cufflinks as cf\ncf.go_offline(connected = True )\n    \nimport plotly.express as px\n    \nimport plotly.graph_objects as go\nimport plotly.offline as pyo\npyo.init_notebook_mode()\n    \nfrom plotly.subplots import make_subplots \nimport plotly.figure_factory as ff ","713353be":"df_train_raw = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col = \"Id\")\ndf_test_raw = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col = \"Id\")\n\ndf_train_raw.head()","7df2dbd8":"data_glimpse(df_train_raw)","9cc6d48f":"# Make Dataset for analysis\n\ndf_EDA = df_train_raw.copy()\n\n# Select is there any NA value\n\nisna_col = df_train_raw.isnull().sum()\n\n# Show them\n\nisna_col[isna_col > 0].sort_values(ascending = False)","6e76aa6d":"# fill Na with None where Na means None in data_descritpion.txt\n\ndf_EDA[['PoolQC', 'MiscFeature', 'Alley', 'Fence', \n              'FireplaceQu', 'GarageType', 'GarageFinish', \n              'GarageCond', 'GarageQual', 'MasVnrType']] = df_EDA[['PoolQC', 'MiscFeature', 'Alley', 'Fence', \n                                                                         'FireplaceQu', 'GarageType', 'GarageFinish', \n                                                                         'GarageCond', 'GarageQual', 'MasVnrType']].fillna('None')\n\ndf_EDA['MasVnrArea'] = df_EDA['MasVnrArea'].fillna(0) # We need to add column 'GarageYrBlt' when we do preprocess for modeling","a7f52c32":"# Fill value with most Frequency \n\ndisplay(df_EDA['LotFrontage'].value_counts().head(3))\n\ndf_EDA['LotFrontage'].fillna(60.0, inplace = True)\n\ndisplay(df_EDA['Electrical'].value_counts().head(3))\n\ndf_EDA['Electrical'].fillna('Sbrkr', inplace= True)","54ee0158":"# There are some diffrences in amount of na value at 'Bsmt' features.\n\n\ndf_EDA[((df_EDA['BsmtFinType1'].isnull() == False) & (df_EDA['BsmtFinType2'].isnull())) | \n             ((df_EDA['BsmtFinType1'].isnull() == False) & (df_EDA['BsmtExposure'].isnull()))]\n\n# First case fill Unf in BsmtFinType2 and fill None in BsmtFinType1\n\ndf_EDA[(df_EDA['BsmtFinType1'].isnull() == False) & (df_EDA['BsmtFinType2'].isnull())].fillna('Unf', inplace = True)\n\ndf_EDA[['BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual']] = df_EDA[['BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual']].fillna('None')","527cc33b":"print(f\"Residual NaN values : {sum(df_EDA.isnull().sum() > 0)}\")","2eea3cf4":"#df_train = df_train.drop('SalePrice', axis = 1)\n\n# Select Numerical Features without target feature\n\nnumerical_cols = [fea for fea in df_EDA.columns if df_EDA[fea].dtype in ['int64', 'float64'] and fea != 'SalePrice']\n\n\n# Select categorical features \n\ncategorical_cols = [fea for fea in df_EDA.columns if df_EDA[fea].dtype == 'object']\n\nprint(f\"Numerical Features({len(numerical_cols)})\\nCategorical_cols({len(categorical_cols)})\")","da348cbd":"# Select low discrete categorical features from categorical_cols\n\nlow_cat_cols = [fea for fea in categorical_cols if len(df_EDA[fea].unique()) <= 10]\n\ncategorical_cols = [fea for fea in categorical_cols if fea not in low_cat_cols]\n\n# Select year features from numerical_cols\n\nyear_cols = [fea for fea in numerical_cols if ('Year' in fea) or ('Yr' in fea)]\n\ndiscrete_cols = [fea for fea in numerical_cols if len(df_EDA[fea].unique()) <= 15 and fea not in year_cols]\n\ncontinuous_cols = [fea for fea in numerical_cols if (fea not in year_cols) and (fea not in discrete_cols)]\n\nprint(f\"Low Cat Features : {len(low_cat_cols)}\\nCategorical Features : {len(categorical_cols)}\")\nprint(f\"Year Features : {len(year_cols)}\\nContinuous Features : {len(continuous_cols)}\\n Discrete Features : {len(discrete_cols)}\")","1b418673":"year_cols","3d9573b3":"# See trend of mean vlaue \n\nfor fea in year_cols : \n    df_year = df_EDA.groupby(by = fea).median()['SalePrice'].reset_index()\n\n    fig = px.scatter(df_year, x = fea, y = 'SalePrice', \n                     trendline = 'ols', \n                     template = 'plotly_dark', \n                     title = f\"Median Price change by {fea}\")\n    \n    fig.update_layout({\n        'title' : {\n            'x' : 0.5, \n            'y' : 0.9, \n            'font' : {\n                'size' : 15\n            }\n        }\n    })\n    fig.show()","7eb73dd6":"discrete_cols","a96f9a6c":"for fea in discrete_cols : \n\n    fig = px.box(df_EDA, x = fea, y = 'SalePrice', \n                 color_discrete_sequence = px.colors.qualitative.G10,\n                 title = f\"Boxplot : {fea} - SalePrice\", \n                 template = \"plotly_dark\")\n\n    fig.update_layout({\n        'title' : {\n            'x' : 0.5, \n            'y' : 0.9, \n            'font' : {\n                'size' : 15\n            }\n        }\n    })\n    fig.show()","b54fdb52":"for fea in low_cat_cols : \n\n    fig = px.box(df_EDA, x = fea, y = 'SalePrice', \n                 color_discrete_sequence = px.colors.qualitative.G10,\n                 title = f\"Boxplot : {fea} - SalePrice\", \n                 template = \"plotly_dark\")\n\n    fig.update_layout({\n        'title' : {\n            'x' : 0.5, \n            'y' : 0.9, \n            'font' : {\n                'size' : 15\n            }\n        }\n    })\n    fig.show()","5481526e":"continuous_cols","fc1f7a53":"for fea in continuous_cols : \n\n    fig = px.scatter(df_EDA, x = fea, y = 'SalePrice', \n                 title = f\"Scatterplot : {fea} - SalePrice\", \n                 trendline = 'ols', \n                 template = \"plotly_dark\")\n\n    fig.update_layout({\n        'title' : {\n            'x' : 0.5, \n            'y' : 0.9, \n            'font' : {\n                'size' : 15\n            }\n        }\n    })\n    fig.show()","ccfd4703":"# Divide target feature from datasets \n\ny = df_train_raw.SalePrice\n\nX_raw = df_train_raw.drop(columns = ['SalePrice'], axis  = 1)\n\nX_test = df_test_raw.copy()","ccb5a351":"# Check correlation features with target 'SalePrice'\n\ncorr_target = df_train_raw.corr(method = 'spearman')['SalePrice'].sort_values(ascending = False)\ndisplay(corr_target)","50ab2b83":"# Selection feature of numerical dtypes which corr is more than 0.2 \n\nnumerical_cols = [col for col in corr_target.index if ((corr_target[col] > 0.2) | (corr_target[col] < -0.2)) and col != 'SalePrice']\n\nprint(\"numerical feature which has correlations : \")\n\nfor col in numerical_cols : \n    print(col, end = \" \")\n    \nprint(f\"\\nTotal length : {len(numerical_cols)}\")","30a6ed95":"# Divide numerical cols which fill NA with most_frequency value when there is NA value. \n\nnum_fill_freq = ['OverallQual', 'YearBuilt', 'FullBath', 'YearRemodAdd', 'TotRmsAbvGrd', 'OpenPorchSF', 'MasVnrArea', 'WoodDeckSF', 'LotFrontage', 'HalfBath', '2ndFlrSF', 'BedroomAbvGr', 'BsmtFullBath', 'EnclosedPorch'] \n\n# Divide numerical cols which fill NA with zero value when there is NA value. \n\nnum_fill_zero = ['GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'Fireplaces', 'BsmtFinSF1']\n                 \n# Divide numericasl cols which fill NA with median value when there is NA value. \n                 \nnum_fill_med = ['GrLivArea', '1stFlrSF', 'LotArea']","1e868055":"# Selection features of categories which shows good coorelation \n\nfor col in low_cat_cols : \n    print(col, end = \" \") \n\nprint(f\"\\nTotal length : {len(low_cat_cols)}\")","41c67922":"# Select feature which fill NA with none when there is NA value. \n\ncat_fill_none = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1' ,'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'Alley']\n\n# Select feature which fill NA with most_frequency \n\ncat_fill_freq = [col for col in low_cat_cols if col not in cat_fill_none]","486523c5":"# Preprocessing \n\n# Select features and call data with those features\n\nmodel_cols = num_fill_freq + num_fill_zero + num_fill_med + cat_fill_none + cat_fill_freq\n\nX_raw = X_raw[model_cols]\n\n# fill NA with zero, median, 'none'\n\nX_raw[num_fill_zero] = X_raw[num_fill_zero].fillna(0)\n\nX_raw[cat_fill_none] = X_raw[cat_fill_none].fillna('none')\n\n# Make imputer for fill na with Simpleimptuer\n\nfrom sklearn.impute import SimpleImputer\n\nnum_med_imputer = SimpleImputer(strategy = 'median')\nnum_freq_imputer = SimpleImputer(strategy = 'most_frequent')\ncat_freq_imputer = SimpleImputer(strategy = 'most_frequent')\n\nX_raw[num_fill_med] = num_med_imputer.fit_transform(X_raw[num_fill_med])\nX_raw[num_fill_freq] = num_freq_imputer.fit_transform(X_raw[num_fill_freq])\nX_raw[cat_fill_freq] = cat_freq_imputer.fit_transform(X_raw[cat_fill_freq])\n\n# Categoreis get dummies \n\nfrom sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n\nOH_encoder.fit(X_raw[low_cat_cols])\n\nOH_X_train = pd.DataFrame(OH_encoder.transform(X_raw[low_cat_cols]))\nOH_X_train.index = X_raw.index\n\nnum_X_train = X_raw.drop(low_cat_cols, axis = 1)\n\nX = pd.concat([num_X_train, OH_X_train], axis = 1)\n\n# Apply StandardScler\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_scaled = scaler.fit_transform(X)","907df476":"# train test split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, train_size = 0.8, test_size = 0.2, random_state = 0) ","7fcaee7e":"# library for scoring \n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n# Function for score matrix\n\ndef accuracy_table(best_model, y_valid, preds) : \n    \n    r2 = r2_score(y_valid, preds)\n    mse = mean_squared_error(y_valid, preds)\n    mae = mean_absolute_error(y_valid, preds)\n    scores = cross_val_score(best_model, X_scaled, y, cv = 4)\n    \n    table = pd.DataFrame([[r2, mse, mae, scores.mean()]], columns = ['r2_score', 'mean_squared_error', 'mean_absolute_error', 'cross_validation_score'])\n    \n    return table","0d3c87a4":"# RandomForestRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Params for GridSearch \n\nparams = {\n    'max_depth' : [2, 3],\n    'n_estimators' : [50, 100, 150, 200]\n}\n\nrfr_model = RandomForestRegressor()\n\nrfr_tree = GridSearchCV(rfr_model, param_grid = params, cv = 4)\nrfr_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", rfr_tree.best_params_)\nprint('best score : ', rfr_tree.best_score_)","386cad9e":"best_model = rfr_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\nrfr_table = accuracy_table(best_model, y_valid, preds)\nrfr_table","6ee45c1c":"# LassoRegression\n\nfrom sklearn import linear_model\n\n# Params for GridSearch\n\nparams = {\n    'alpha' : [0.1, 0.5, 1, 2, 5, 10]\n}\n\nlasso_model = linear_model.Lasso()\n\nlasso_tree = GridSearchCV(lasso_model, param_grid = params, cv = 4)\nlasso_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", lasso_tree.best_params_)\nprint('best score : ', lasso_tree.best_score_)","e6b470cc":"best_model = lasso_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\nlasso_table = accuracy_table(best_model, y_valid, preds)\nlasso_table","6e4d3009":"# Ridge Regression \n\n# Params for GridSearch\n\nparams = {\n    'alpha' : [0.1, 0.2, 0.5, 1, 2, 5, 10]\n}\n\nrdg_model = linear_model.Ridge()\n\nrdg_tree = GridSearchCV(rdg_model, param_grid = params, cv = 4)\nrdg_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", rdg_tree.best_params_)\nprint('best score : ', rdg_tree.best_score_)","fc529bfa":"best_model = rdg_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\nrdg_table = accuracy_table(best_model, y_valid, preds)\nrdg_table","816096b4":"# Linear Regression\n\n\nlgr_model = linear_model.LinearRegression()\n\nlgr_model.fit(X_train, y_train)","67372c79":"preds = lgr_model.predict(X_valid)\n\nlgr_table = accuracy_table(best_model, y_valid, preds)\nlgr_table","287a5ec1":"# XGBR Regressor \n\nfrom xgboost import XGBRegressor\n\n# Params for GridSearch\n\nparams = {\n    'learning_rate' : [0.01, 0.05, 0.1]\n}\n\nxgb_model = XGBRegressor(n_estimators = 1000, n_jobs = 4)\n\nxgb_tree = GridSearchCV(xgb_model, param_grid = params, cv = 4)\nxgb_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", xgb_tree.best_params_)\nprint('best score : ', xgb_tree.best_score_)","e0fa1b04":"best_model = xgb_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\nxgb_table = accuracy_table(best_model, y_valid, preds)\nxgb_table","dffca65c":"total_result = pd.concat([rfr_table, lasso_table, xgb_table, rdg_table, lgr_table], axis =0)\ntotal_result.index = ['Random Forest Regression', 'Lasso Regression', 'XGBoost Regression', 'Ridge Regression', 'Linear Regression']\ntotal_result","382357de":"colors = px.colors.sequential.RdBu[:5]\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x = total_result.index,\n    y = total_result.cross_validation_score,\n    text = round(total_result.cross_validation_score, 3),\n    marker_color = colors\n))\n\nfig.update_layout(\n{\n    \"title\": {\n        \"text\": \"<b>Cross Validation Score by model<\/b>\",   # Can add title value using f\" {}\" \n        \"x\": 0.5,\n        \"y\": 0.9,\n        \"font\": {\n            \"size\": 15\n        }\n    },\n    \"xaxis\": {\n        \"title\": \"Model Selection\",\n        \"showticklabels\":True,\n        \"tickfont\": {\n            \"size\": 10               \n        }\n    },\n    \"yaxis\": {\n        \"title\": \"Accuracy Score(Cross Validation)\",\n        \"tickfont\": {\n            \"size\": 10                \n        }\n    },\n    \"template\":'plotly_dark'\n}\n)\n\nfig.show()","952efc98":"# Apply to X_test\n\nX_test = X_test[model_cols]\n\nX_test[num_fill_zero] = X_test[num_fill_zero].fillna(0)\n\nX_test[cat_fill_none] = X_test[cat_fill_none].fillna('none')\n\nX_test[num_fill_med] = num_med_imputer.transform(X_test[num_fill_med])\nX_test[num_fill_freq] = num_freq_imputer.transform(X_test[num_fill_freq])\nX_test[cat_fill_freq] = cat_freq_imputer.transform(X_test[cat_fill_freq])\n\nOH_X_test = pd.DataFrame(OH_encoder.transform(X_test[low_cat_cols]))\nOH_X_test.index = X_test.index\n\nnum_X_test = X_test.drop(low_cat_cols, axis = 1)\n\nX_test = pd.concat([num_X_test, OH_X_test], axis = 1)\n\n# Aooky StandardSclaer\n\nX_test_scaled = scaler.transform(X_test)\n\n# Check the state\n\nprint(X.shape, X_test.shape)","e0990da5":"best_accuracy_model = xgb_tree.best_estimator_\npreds_test = best_accuracy_model.predict(X_test_scaled)\n\noutput = pd.DataFrame({'Id' : X_test.index,\n                       'SalePrice' : preds_test})\n\noutput.to_csv('submissions.csv', index = False)","60a4ec9f":"## Data importing ","88e79939":"# Evaluation and Conclusion","79fb4930":"# Analysis Preparation\n\n## Function importing","ac35acfd":"**Dividing Features**  \nWe will see features correlation with target value.","32e5c1f5":"# Modeling ","7b373f76":"# <center> House Price Prediction","091c0e89":"## Low_cat_cols Analysis","60ee9ad6":"## Contionuous Feature Analysis","11c3dcd8":"# Deal NaN","424bdba4":"## Library importing","95bde321":"## Discrete Feature Analysis","1d3f3d75":"This is my first kaggle submission with EDA - Visualizaiton - Modeling commit !","81347cc3":"## Year - Sale Price","6909bc35":"As we can see, House price becomes higher when the year of feature is newer, while Yrsold which means year sold shows negative trends.","db5cd4e9":"# EDA + Visualization","9916f355":"# Preprocessing","4659ee58":"We will see that year can affect house price.","8fff2c10":"# Data description\n## Datasource explaining \n- Datasts name : \"train.csv\", \"test.csv\"\n- Datasets Source : [House Price - Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data?select=train.csv)","a1529d08":"As we see the result in data_glimpse function, we can see that there are some columns which have NaN values more than 80%.   \nIt might be the reason that its infrastructure is none, but i will delete them.","4fa820ae":"# Data glimpse"}}