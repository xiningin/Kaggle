{"cell_type":{"09d4276d":"code","46fac18a":"code","f44ec83f":"code","671f8f4a":"code","bbb22e93":"code","c8fb731b":"code","0d9a8432":"code","350e2d15":"code","7df62c1c":"code","89af3e5a":"code","16526184":"code","a5d58f3c":"code","adcacf94":"code","3f92dce4":"code","4a4b08f3":"code","2d2a5fa6":"code","325d358d":"code","b3c0982f":"code","0338feaf":"code","184b6c1d":"code","21ca2c02":"code","869eb51b":"code","96b69a79":"code","f13d9f60":"code","b9e9d3bb":"code","1f383c2d":"code","e7555eeb":"code","0d3506c7":"code","d3c917e8":"code","c2607c6c":"code","6cb61ffc":"code","22ecd2df":"code","46a9b021":"code","b2e9d808":"code","e313f592":"code","984e9a60":"code","06494035":"code","5994b1d6":"markdown","07a2e908":"markdown","7115de07":"markdown","f8521ccd":"markdown","8a20be62":"markdown","839b5b64":"markdown","ddd0d0db":"markdown","292d8a4d":"markdown","4d663a7a":"markdown","e1d47a35":"markdown","d6e4ea17":"markdown","d60d9424":"markdown","691c4523":"markdown","b9731406":"markdown","020e90ea":"markdown","5eb448a7":"markdown","215e3d3b":"markdown","79f9a2d3":"markdown"},"source":{"09d4276d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.utils import class_weight\nimport tensorflow as tf","46fac18a":"data = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')","f44ec83f":"data","671f8f4a":"data.info()","bbb22e93":"img = Image.open('..\/input\/new-york-city-airbnb-open-data\/New_York_City_.png')\n\nplt.figure(figsize=(12, 12))\nplt.axis('off')\nplt.imshow(img, extent=(0, 1, 0, 1))\nplt.show()","c8fb731b":"coords = data.loc[:, ['longitude', 'latitude']].copy()\ncoords","0d9a8432":"min_max_scaler = MinMaxScaler()\n\ncoords = pd.DataFrame(min_max_scaler.fit_transform(coords), columns=coords.columns)\ncoords","350e2d15":"plt.figure(figsize=(12, 12))\nplt.axis('off')\nplt.imshow(img, extent=(-0.02, 1.045, -0.02, 1.015))\nplt.scatter(x=coords['longitude'], y=coords['latitude'], s=2, c='#57db80', alpha=0.5)\nplt.title(\"Airbnb Locations in NYC\")\nplt.show()","7df62c1c":"data","89af3e5a":"unneeded_columns = ['id', 'name', 'host_id', 'host_name']\n\ndata = data.drop(unneeded_columns, axis=1)","16526184":"print(\"Total missing values:\", data.isna().sum().sum())","a5d58f3c":"data.isna().mean()","adcacf94":"data = data.drop('last_review', axis=1)\n\ndata['reviews_per_month'] = data['reviews_per_month'].fillna(0)","3f92dce4":"print(\"Total missing values:\", data.isna().sum().sum())","4a4b08f3":"data","2d2a5fa6":"data['availability_365'] = data['availability_365'].apply(lambda x: 1 if x == 365 else 0)","325d358d":"data['availability_365'].value_counts() \/ len(data['availability_365'])","b3c0982f":"data","0338feaf":"{feature: list(data[feature].unique()) for feature in data.columns if data.dtypes[feature] == 'object'}","184b6c1d":"def onehot_encode(df, columns, prefixes):\n    df = df.copy()\n    for column, prefix in zip(columns, prefixes):\n        dummies = pd.get_dummies(df[column], prefix=prefix)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df","21ca2c02":"data = onehot_encode(\n    data,\n    columns=['neighbourhood_group', 'neighbourhood', 'room_type'],\n    prefixes=['G', 'N', 'R']\n)","869eb51b":"data","96b69a79":"y = data['availability_365'].copy()\nX = data.drop('availability_365', axis=1).copy()","f13d9f60":"scaler = StandardScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)","b9e9d3bb":"X","1f383c2d":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=200)","e7555eeb":"X.shape","0d3506c7":"inputs = tf.keras.Input(shape=(X.shape[1],), name=\"input_layer\")\n\nhidden_1 = tf.keras.layers.Dense(64, activation='relu', name=\"hidden_1\")(inputs)\nhidden_2 = tf.keras.layers.Dense(64, activation='relu', name=\"hidden_2\")(hidden_1)\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output_layer\")(hidden_2)\n\n\nmodel = tf.keras.Model(inputs, outputs)","d3c917e8":"print(model.summary())\ntf.keras.utils.plot_model(model)","c2607c6c":"class_weight = dict(\n    enumerate(\n        class_weight.compute_class_weight(\n            'balanced',\n            y_train.unique(),\n            y_train\n        )\n    )\n)","6cb61ffc":"class_weight","22ecd2df":"batch_size = 32\nepochs = 29\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    class_weight=class_weight,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau()\n    ]\n)","46a9b021":"plt.figure(figsize=(20, 5))\n\nepochs_range = range(epochs)\ntrain_loss, val_loss = history.history['loss'], history.history['val_loss']\ntrain_auc, val_auc = history.history['auc'], history.history['val_auc']\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, train_loss, label=\"Training Loss\")\nplt.plot(epochs_range, val_loss, label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss Over Time\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, train_auc, label=\"Training AUC\")\nplt.plot(epochs_range, val_auc, label=\"Validation AUC\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"AUC\")\nplt.legend()\nplt.title(\"AUC Over Time\")\n\nplt.show()","b2e9d808":"np.argmax(val_auc)","e313f592":"results = model.evaluate(X_test, y_test, verbose=0)\n\nprint(\"Accuracy:\", results[1])\nprint(\" ROC AUC:\", results[2])","984e9a60":"y_true = y_test.reset_index(drop=True)\ny_pred = pd.Series((np.squeeze(model.predict(X_test)) > 0.5).astype(np.int))\n\npositive_indices = y_true[y_true == 1].index","06494035":"print(\"Accuracy within the postive class:\", np.mean(y_true[positive_indices] == y_pred[positive_indices]))","5994b1d6":"Now that the data has been scaled, and we only need to split it into train and test sets.","07a2e908":"# Training  \n  \nLet us compute some class weights before fitting the model (since our classes are so skewed).","7115de07":"Let's apply a MinMaxScaler transform to these two columns so we can plot them along the axes.","f8521ccd":"# Creating Labels  \n  \nWe want to be able to predict if a location will be available 365 days of the year.  \n  \nLet us change the *availability_365* column to be a simple binary column (where 1 denotes 365-availability and 0 denotes otherwise).","8a20be62":"We can use the latitude and longitude values to plot points on the map.","839b5b64":"# Modeling  \n  \nSince we are dealing with tabular data, we will use a simple 2-hidden-layer neural network.","ddd0d0db":"# Splitting\/Scaling  \n  \nOur data is now in fully-numeric form, so let's scale the data so that each column has mean 0 and unit variance.  \nFirst, we need to split the data into X (features) and y (target).","292d8a4d":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/FgesFcRLWBU","4d663a7a":"We can see from the class distribution that we are dealing with severely skewed classes (365-availability only occurs in 2.6485% of the data).","e1d47a35":"It appears that all three categorical features are nominal (no ordering), so we will perform one-hot encoding on them.","d6e4ea17":"# Encoding Features  \n  \nLet's get a list of the unique values for each categorical feature, so we can decide on how to encode them.","d60d9424":"Because the missing values in the *last_review* column are difficult to deal with, we will drop that column and fill the NaN values in *reviews_per_month* with 0's (since a missing value implies there are no reviews for the location).","691c4523":"# Results","b9731406":"Let's generate a summary and graphical representation of the model for better understanding.","020e90ea":"# Task for Today  \n\n***\n\n## NYC Airbnb Availability Prediction  \n\nGiven *data about Airbnb locations in NYC*, let's try to predict whether a given location will be **available 365 days out of the year**.  \n  \nWe will use a TensorFlow ANN to make our predictions.","5eb448a7":"# Cleaning  \n  \nLet's start by removing unnecessary columns and dealing with missing values.","215e3d3b":"# Getting Started","79f9a2d3":"# Visualization  \n  \nLet's plot all the Airbnb locations on the provided image of NYC."}}