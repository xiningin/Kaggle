{"cell_type":{"88968bab":"code","e3fc4d32":"code","6a07d361":"code","fab873f8":"code","400edbee":"code","754a06f3":"code","84ce429b":"code","2f72e6a0":"code","b864867c":"code","5a7c834a":"code","f73b0d01":"code","fd0f64ab":"code","94c75050":"code","0ec3c892":"code","c0eb81ed":"code","f6771838":"code","cc8c5734":"code","1c9059e7":"code","41e6455e":"code","6a347a54":"code","012d2756":"code","ef64d293":"code","dee7e331":"code","14ff21a6":"code","e36a5359":"code","b3f30d9f":"code","e33d2b33":"code","f8e9427c":"code","044f6dcf":"code","7a174ecf":"code","a774fb19":"code","d1a8c996":"code","6e2627df":"code","2fa1a065":"code","2fdd8272":"code","982187ee":"code","670d36e9":"code","f2e14357":"code","9ec41ca3":"code","b9ee7285":"code","974d5a41":"code","4a1cb33b":"code","da80be5b":"code","5a8ef459":"code","44bfc645":"code","d579a63a":"code","6214efe5":"code","8bdae627":"code","8ef0ebd9":"code","c60e829b":"code","478be5c6":"code","e4430620":"code","0cb13efd":"code","b6d453a9":"code","87e220cc":"code","8a007f79":"code","a36a6e18":"code","41d65efe":"code","12d89ffa":"code","a65ff47c":"code","4a459ab6":"code","fa5c9e2a":"code","9d3a6d39":"code","37343266":"code","a6a4831a":"code","8243ac2c":"code","30c83d92":"code","790f6a4c":"code","2546a42b":"code","2438a13e":"code","82e5a2bc":"code","e0a2c704":"code","64462a07":"code","e7c35c95":"code","94bba8ba":"code","16d23540":"code","8749af20":"code","de061d00":"code","a64d8de3":"code","0a5117a0":"code","ae2ff025":"code","bb550df6":"code","2e710f57":"code","f018affe":"code","050468e7":"code","d33e7281":"code","22106734":"markdown","80914595":"markdown","389b424c":"markdown","713f63d3":"markdown","9d41d83f":"markdown","bf2356b6":"markdown","cd020252":"markdown","ebdd8365":"markdown","967b0c28":"markdown","6c95776a":"markdown","c8ab78e5":"markdown","7885a126":"markdown","56e36e4b":"markdown","cce27da0":"markdown","85d7f608":"markdown","2a8fe83e":"markdown","d597695c":"markdown","5fe8414d":"markdown"},"source":{"88968bab":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nstyle.use('ggplot')\n\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\n\npyoff.init_notebook_mode()\n\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom datetime import datetime\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.metrics import mean_squared_error\n\n\nimport os \n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e3fc4d32":"'''\nFiles required:\n\nreviews.csv containing reviews info in Seattle city,\ncalender.csv  containing dates and price info in Seattle city,\nlistings.csv containing major features and information for analysis.\n\n\n'''\n\n#Reading the .csv file into pandas dataframe\n\nc_data = pd.read_csv(\"\/kaggle\/input\/seattle\/calendar.csv\")\nr_data = pd.read_csv(\"\/kaggle\/input\/seattle\/reviews.csv\")\nl_data = pd.read_csv(\"\/kaggle\/input\/seattle\/listings.csv\")\n\n","6a07d361":"#checking and handling rows and columns with missing values\n\nr_data.isnull().sum().plot(kind='bar')\nplt.xticks(rotation=60);","fab873f8":"#listings data\n\nl_data.info()","400edbee":"#Get the column names with null values\n\nl_data.isnull().sum()[l_data.isnull().sum().to_numpy().nonzero()[0]].index","754a06f3":"plt.figure(figsize=(12,6));\nplt.xticks(rotation=90);\nplt.yticks(np.arange(0,4000,200));\nsns.barplot(x=l_data.isnull().sum()[l_data.isnull().sum().to_numpy().nonzero()[0]].index,y=l_data.isnull().sum()[l_data.isnull().sum().to_numpy().nonzero()[0]].values);","84ce429b":"# columns with more than 75 missing values\nset(l_data.columns[l_data.isnull().mean()>= 0.75])","2f72e6a0":"# columns with more than 100% missing values\nset(l_data.columns[l_data.isnull().mean()>= 1])","b864867c":"#Iisnull columns contains more tha n100% missig value and sqaure feet contains more than 75%\n#it wise to drop them\nl_data.drop(['license','square_feet'],axis=1,inplace=True)","5a7c834a":"# missing values in Calender data\n\nplt.figure(figsize=(10,6));\nnull_price = c_data.isnull().sum()\n(null_price\/c_data.shape[0]).plot(kind='bar');\n","f73b0d01":"#function to clean and convert str columns to numeric\n\ndef str_to_num(df,column):\n    df[column] = pd.to_numeric(df[column].apply(lambda x : str(x).replace('$','').replace(\",\",'')),errors='coerce')\n    return df\n\ncolumns = ['price','monthly_price','weekly_price','security_deposit','cleaning_fee']\n\nfor col in columns:\n    l_data = str_to_num(l_data,col)\n\nl_data[columns][:5]","fd0f64ab":"c_data.dropna(axis=0,subset=['price'],inplace=True)\nr_data.dropna(axis=0,subset=['comments'],inplace=True)","94c75050":"#Explore different property types and room types\n\nprop_type = l_data.property_type.value_counts()\n\nplt.figure(figsize=(12,6));\n(prop_type\/l_data.shape[0]).plot(kind='bar');\nplt.xticks(rotation=60);\nplt.title('Percentange of different property types in Seattle city ');\nplt.xlabel('Property Type');\nplt.ylabel('% Availability');","0ec3c892":"room_type = l_data.room_type.value_counts()\n\nplt.figure(figsize=(10,6));\n(room_type\/l_data.shape[0]).plot(kind='bar');\nplt.xticks(rotation=60);\nplt.title('Percentange of room types in Seattle city ');","c0eb81ed":"\nplt.figure(figsize=(14,6))\nl_data.groupby(['property_type','room_type'])['price'].mean().sort_values(ascending=False).plot(kind='bar');\nplt.title('Price of room types as per different property types');\nplt.ylabel('$ Price');","f6771838":"plt.figure(figsize=(14,6));\nl_data.groupby(['property_type'])['price'].mean().sort_values(ascending=False).plot(kind='bar');\nplt.title('Property price as per Property Type');\nplt.xlabel('Property Type');\nplt.ylabel('$ Price');","cc8c5734":"#Exploring neighbourhood and reviews data\n\n\n# neighbourhood distribution\nneighbourhood = l_data.neighbourhood.value_counts()[:40]\n\nplt.figure(figsize=(14,8))\n(neighbourhood\/l_data.shape[0]).plot(kind=\"bar\");\nplt.title(\"Neighbourhood Listings distribution\");\nplt.xlabel('Neighbourhoods');","1c9059e7":"\n# num of reviews distribution \n\nfig = plt.figure(figsize=(10,6))\nl_data.number_of_reviews.hist();\nplt.title('Number of reviews distribution');\nplt.xlabel('Number of Reviews');\nplt.ylabel('Count');","41e6455e":"\n# price distribution\nplt.figure(figsize=(10,6))\nl_data.price.hist();\nplt.title('Price distribution');\nplt.xlabel('Price nights');\nplt.ylabel('Count');","6a347a54":"#get summary of required columns\n\nl_data[['reviews_per_month','number_of_reviews','calculated_host_listings_count','price','minimum_nights','availability_365']].describe()","012d2756":"#Calender dataset\n\n\n# change the date column to a datetime \nc_data['date'] = pd.to_datetime(c_data.date)\nc_data.info(verbose=True, null_counts=True)","ef64d293":"\n# drop the missing values in price and drop \nc_data = c_data.dropna(subset=['price'], axis = 0)\nc_data.info(verbose=True, null_counts=True)","dee7e331":"\n# to clean the price and convert it into a float \nc_data = str_to_num(c_data,'price')\nc_data.info(verbose=True, null_counts=True)","14ff21a6":"c_data.describe()","e36a5359":"# add month and year column to the calender dataset\nc_data['month'], c_data['year'] = c_data.date.dt.month, c_data.date.dt.year\nc_data.info(verbose=True, null_counts=True)","b3f30d9f":"c_data.available.value_counts()","e33d2b33":"\nprice = pd.DataFrame(c_data.groupby(['month','available']).mean()['price'].reset_index())\n\ndata = [\n    go.Scatter(\n        x = price['month'],\n        y = price.price,\n        name = 'Price'\n    )\n]\n\nlayout = go.Layout(\n    title = 'Booking prices as per months',\n    xaxis = dict(title='Months'),\n    yaxis = dict(title= '$ Price'),\n    showlegend=True,\n    \n)\nfig = go.Figure(data=data,layout=layout)\n\npyoff.iplot(fig)","f8e9427c":"#as per the dataset all listings are available\n\navailable_count_daily = c_data.groupby('date').count()[['price']]\navailable_count_daily = available_count_daily.rename({\"price\":\"total_available_houses\"},axis='columns')\n\naverage_price_daily = c_data.groupby('date').mean()[['price']]\n# change column name\naverage_price_daily = average_price_daily.rename({\"price\":\"average_prices\"},axis='columns')","044f6dcf":"# plot total available houses and average prices in one figure\nf, ax = plt.subplots(figsize=(15, 6))\nplt1 = sns.lineplot(x = available_count_daily.index,y = 'total_available_houses', \n                  data = available_count_daily,color=\"b\",legend=False,label='No. of houses available')\n\nax2 = ax.twinx()\nplt2 = sns.lineplot(x = average_price_daily.index,y = 'average_prices',\n             data=average_price_daily,ax=ax2,linestyle=':', legend=False,label='Daily prices')\nax.set_title('Comparing the daily availability of airbnb listing with the daily listing prices');\nax2.legend();\nax.legend();","7a174ecf":"# group the listings by neighbourood and get the average price\nl_data.groupby('neighbourhood')['price'].mean().sort_values(ascending=False)\n","a774fb19":"(l_data.groupby('neighbourhood')['price'].mean().sort_values(ascending=False)).plot(kind=\"bar\", figsize=(16,8));\nplt.title(\"Average Listing price across Seattle neighbourhoods\");\nplt.xlabel('Neighbourhood');\nplt.ylabel('Average Price');","d1a8c996":"#top 10 expensive neighbourhoods\n\n(l_data.groupby('neighbourhood')['price'].mean().sort_values(ascending=False)[:10]).plot(kind=\"bar\", figsize=(16,8));\nplt.title(\"Average Listing price across Seattle neighbourhoods\");\nplt.xlabel('Neighbourhood');\nplt.ylabel('Average Price $');\nplt.xticks(rotation=60);","6e2627df":"(l_data.groupby('neighbourhood')['price'].mean().sort_values(ascending=False)[-5:]).plot(kind=\"bar\", figsize=(16,8));\nplt.title(\"Average Listing price across Seattle neighbourhoods\");\nplt.xlabel('Neighbourhood');\nplt.ylabel('Average Price $');\nplt.xticks(rotation=60);","2fa1a065":"\n# drop redundent columns\nl_data.drop(columns = 'neighbourhood' , inplace = True, axis = 1)\nl_data.info()","2fdd8272":"# columns with more than 75% missing values\ncols_with_missing_val = list(set(l_data.columns[l_data.isnull().mean()>75]))\ncols_with_missing_val","982187ee":"l_data[['review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value','review_scores_rating']].describe()","670d36e9":"# create new columns\n#Fix columns that have only 2 possible with binary values 1 and 0\n\nl_data['has_transit_info'] = l_data.transit.apply(lambda x: 0 if pd.isnull(x) else 1)\nl_data['has_description_info'] = l_data.description.apply(lambda x: 0 if pd.isnull(x) else 1)\nl_data['has_summary_info'] = l_data.summary.apply(lambda x: 0 if pd.isnull(x) else 1)\nl_data['has_neighborhood_info'] = l_data.neighborhood_overview.apply(lambda x: 0 if pd.isnull(x) else 1)\nl_data['has_host_info'] = l_data.host_about.apply(lambda x: 0 if pd.isnull(x) else 1)\nl_data.host_is_superhost = l_data.host_is_superhost.apply(lambda x: 0 if pd.isnull(x) else 1)\n\n# fix t and f columns\nl_data['instant_bookable'] =  l_data['instant_bookable'].apply(lambda x: 1 if x=='t' else 0)\nl_data['require_guest_phone_verification'] =  l_data['require_guest_phone_verification'].apply(lambda x: 1 if x=='t' else 0)\nl_data['require_guest_profile_picture'] =  l_data['require_guest_profile_picture'].apply(lambda x: 1 if x=='t' else 0)\nl_data['host_identity_verified'] =  l_data['host_identity_verified'].apply(lambda x: 1 if x=='t' else 0)\nl_data.host_has_profile_pic = l_data.host_has_profile_pic.apply(lambda x: 1 if x=='t' else 0)","f2e14357":"l_data.info()\n","9ec41ca3":"# Drop irrelevent object columns\n\nl_data.select_dtypes(include=['object']).columns\n","b9ee7285":"columns_to_drop =['listing_url', 'scrape_id', 'last_scraped', 'country_code', 'country', 'picture_url', 'host_url', 'host_thumbnail_url','notes', \n'host_picture_url','transit', 'space', 'description', 'summary', 'neighborhood_overview', 'name', 'latitude', 'longitude'\n, 'is_location_exact', 'host_about','jurisdiction_names','experiences_offered','amenities','calendar_last_scraped','requires_license', 'smart_location', 'host_verifications', 'street',\n'host_verifications', 'calendar_updated','has_availability', 'city', 'state','host_id','zipcode','market','host_location','host_name','host_neighbourhood']\n\nl_data.drop(columns=columns_to_drop, inplace=True)\nl_data.info()","974d5a41":"l_data.drop(['xl_picture_url','thumbnail_url','medium_url'],axis=1,inplace=True)","4a1cb33b":"l_data.select_dtypes(include=['object']).columns\n","da80be5b":"\nnumerical_columns = l_data.select_dtypes(include=['float', 'int']).columns\nnumerical_columns","5a8ef459":"def days_between(date):\n    \"\"\"\n    return the number of days from today to the given date\n    \"\"\"\n    try:\n        date_format = \"%Y-%m-%d\"\n        NOW = datetime.now()\n        d = datetime.strptime(date, date_format)\n        return abs((NOW - d).days)\n    except:\n        return date","44bfc645":"l_data.first_review.apply(lambda x : days_between(x)).head()\n","d579a63a":"l_data['first_review'] = l_data.first_review.apply(lambda x : days_between(x))\nl_data['last_review'] = l_data['last_review'].apply(lambda x : days_between(x))\nl_data['host_since'] = l_data['host_since'].apply(lambda x : days_between(x))\nl_data.info()","6214efe5":"l_data = str_to_num(l_data,'extra_people')","8bdae627":"def clean_response_rate(x):\n    \"\"\"\n    clean the response rate column by eliminating the % and\n    from the string and return a float to be stored in the column\n    \"\"\"\n    try:\n        return float(str( x[:-1]))\/100\n    except:\n        return x","8ef0ebd9":"l_data.host_response_rate = l_data.host_response_rate.apply(lambda x: clean_response_rate(x))","c60e829b":"l_data.info()","478be5c6":"l_data.select_dtypes(include=['object']).columns","e4430620":"l_data.drop(['host_acceptance_rate','neighbourhood_group_cleansed'],axis=1,inplace=True)","0cb13efd":"#Handling categorcial data\n\ndef create_dummy_df(df, cat_cols, dummy_na):\n    '''\n    INPUT:\n    df - pandas dataframe with categorical variables you want to dummy\n    cat_cols - list of strings that are associated with names of the categorical columns\n    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n    \n    OUTPUT:\n    df - a new dataframe that has the following characteristics:\n            1. contains all columns that were not specified as categorical\n            2. removes all the original columns in cat_cols\n            3. dummy columns for each of the categorical columns in cat_cols\n            4. if dummy_na is True - it also contains dummy columns for the NaN values\n            5. Use a prefix of the column name with an underscore (_) for separating \n    '''\n    for col in cat_cols:\n        try:\n            df = pd.concat([df.drop(col, axis =1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)], axis=1)\n        except:\n            continue\n    return df","b6d453a9":"cat_cols_lst = l_data.select_dtypes(include=['object'])\ncat_cols_lst","87e220cc":"# create new columns\n\n# print the shape of the dataframe before handeling the categorical variables\nprint(l_data.shape)\n\nl_data_categorized = create_dummy_df(l_data, cat_cols_lst, dummy_na=False) \n\nprint(l_data_categorized.shape)","8a007f79":"#Feature Scaling and transformation\n\nl_data_categorized.dropna(axis=1,inplace=True)\n\nl_data_categorized.isnull().sum(axis=1).describe()\n\n","a36a6e18":"numerical_columns = l_data_categorized.select_dtypes(include=['float', 'int']).columns\n\nlistings_df_scaled = pd.DataFrame(data = l_data_categorized)\n\n# define the scaler and scale numerical values\nscaler = MinMaxScaler()\nlistings_df_scaled[numerical_columns] = scaler.fit_transform(l_data_categorized[numerical_columns])\n\n# Shows scaled data\nlistings_df_scaled.head()","41d65efe":"#Checking kmeans score to find the optimal cluster number\n\nlistings_df_scaled.dropna(axis=1,inplace=True)\n\nsse = {}\nfor i in range(1,30):\n    Kmeans = KMeans(n_clusters=i).fit(listings_df_scaled)\n    sse[i] = Kmeans.inertia_\n    \nplt.figure(figsize=(8,6))\nplt.plot(list(sse.keys()),list(sse.values()))\nplt.xlabel(\"Number of Clusters\")\nplt.show()","12d89ffa":"#Refit fitting with optimal cluster number\nkmeans = KMeans(n_clusters=10)\nmodel_kmeans = kmeans.fit(listings_df_scaled)\nlabels_listings = model_kmeans.transform(listings_df_scaled)\n\nlistings_kmeans = model_kmeans.predict(listings_df_scaled)\nlistings_kmeans","a65ff47c":"# add the cluster column to the dataset \nlistings_df_scaled['Cluster'] = pd.Series(listings_kmeans, index=listings_df_scaled.index)\nlistings_df_scaled.head()","4a459ab6":"# group the data by different clusters \nlistings_grouped_by_clusters = listings_df_scaled.groupby('Cluster').mean()\nlistings_grouped_by_clusters.head()","fa5c9e2a":"avg_price_per_cluster = listings_grouped_by_clusters.price.sort_values(ascending=False)\navg_price_per_cluster.plot.bar(figsize=(10,4));\nplt.title('Average Listings Price per Cluster');\nplt.ylabel('Average Price');","9d3a6d39":"# clusters column mean value for \nclusters_mean = listings_grouped_by_clusters.mean()\nclusters_mean","37343266":"# in order to compare the different clusters i will subtract the mean\n# and the value in each cluster\n\n# display the percentage for each cluster and the average mean \nclusters_differences_in_mean = ((listings_grouped_by_clusters- clusters_mean)*100\/clusters_mean)\nclusters_differences_in_mean","a6a4831a":"clusters_differences_in_mean_transposed = clusters_differences_in_mean.T\nclusters_differences_in_mean_transposed","8243ac2c":"clusters_differences_in_mean['price']\n","30c83d92":"clusters_differences_in_mean[['price']].sort_values(ascending=False, by='price').plot(kind='barh', figsize= [10,5], fontsize =12, legend=False);\nplt.title('Percentage of Price Difference Across Clusters');\nplt.xlabel('Difference');\nplt.ylabel('Clusters');","790f6a4c":"# investigating the most cluster with the highest and lowest price \n# cluster 5 highest difference score compared to the other clusters\n# cluster 1 had the lowest price\n\ncluster5 = clusters_differences_in_mean_transposed[5].sort_values(ascending=False).head(20)\ncluster5.plot.bar(figsize=(10,4))\nplt.title('Attributes Differeniating Cluster 5');\nplt.ylabel('Difference Percentage');\nplt.xlabel('Attribute');","2546a42b":"cluster5\n\n# Cluster 5\n\n# the highest difference is for property_type_dorm\n# property type: dorm,ced_couch,Tent,Condonium\n# neighbourhoods: South park,Laurwlhuest,Queeen Anne\n# it had the highest price difference","2438a13e":"cluster1 = clusters_differences_in_mean_transposed[1].sort_values(ascending=False).head(20)\ncluster1.plot.bar(figsize=(10,4))\nplt.title('Attributes Differeniating Cluster 1');\nplt.ylabel('Difference Percentage');\nplt.xlabel('Attribute');","82e5a2bc":"# Cluster 1:\n\n# property Bunglow\n# strict cancellation policy moderate\n# neighbourhoods: Pinehusrt,Highland_park,View_ridge","e0a2c704":"# Medium range price cluster 8\ncluster8 = clusters_differences_in_mean_transposed[8].sort_values(ascending=False).head(20)\ncluster8.plot.bar(figsize=(10,4))\n\nplt.title('Attributes Differeniating Cluster 8');\nplt.ylabel('Difference Percentage');\nplt.xlabel('Attribute');","64462a07":"\n# Cluster 8:\n\n# property type mostly Treehouse,Cabin,Townhouse\n# the highest difference is for montlake neighbourhood\n# Strict Canecllation policy\n# neighbourhoods: SunsetHill,Arbor_heights,Ravenna\n\ncluster8","e7c35c95":"# comparing the 3 clusters \ncomparsion_columns = list(set(cluster5.index[:10].tolist() + cluster1.index[:10].tolist() + cluster8.index[:10].tolist()))\nclusters_differences_in_mean_transposed[[5,8,1]].loc[comparsion_columns].sort_values(ascending=False, by=5)","94bba8ba":"l_data.last_review.mean()\n","16d23540":"l_data_categorized.accommodates.value_counts()\n","8749af20":"l_data_categorized.rename(columns={'property_type_Bed_&_Breakfast':'property_type_Bed\/Breakfast'},inplace=True)\n","de061d00":"#implementing a Logistic model\n\nall_columns = []\nfor column in l_data_categorized.columns:\n    column = column.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\").replace(\"-\", \"_\").replace(\"\/\",\"_\")\n    all_columns.append(column)\n\nl_data_categorized.columns = all_columns","a64d8de3":"glm_columns = 'host_is_superhost'\n\nfor column in l_data_categorized.columns:\n    if column not in ['price','id']:\n        glm_columns = glm_columns + ' + ' + column","0a5117a0":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n \n\nglm_model = smf.glm(formula='price ~ {}'.format(glm_columns), data=l_data_categorized, family=sm.families.Binomial())\nres = glm_model.fit()\nprint(res.summary())","ae2ff025":"print(np.exp(res.params));","bb550df6":"#create feature set and labels\nX = l_data_categorized.drop(['price','id'],axis=1)\ny = l_data_categorized.price","2e710f57":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n#train and test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=56)","f018affe":"import xgboost as xgb\n\n#building the model\nxgb_model = xgb.XGBClassifier(learning_rate=0.01, objective= 'binary:logistic',n_jobs=-1).fit(X_train, y_train)\n\n","050468e7":"pred = xgb_model.predict(X_test)\nresults = pd.DataFrame(pred,y_test)\nresults = results.reset_index()\nresults.columns = ['price','pred']\nmse = mean_squared_error(y_pred=results['pred'],y_true=results['price'])\nprint(f\"Mean Squared error of model on test set : {mse}\")","d33e7281":"from xgboost import plot_importance\n\nfig, ax = plt.subplots(figsize=(10,8))\nplot_importance(xgb_model, ax=ax)","22106734":"# Data Modeling","80914595":"In reviews data there are null values in comments column","389b424c":"# Data Preparation\/Wrangling","713f63d3":"# **Removing Null values**","9d41d83f":"Data Wrangling listings data\n\n* Handle missing values\n* Handle categorical data\n* Drop redundent columns","bf2356b6":"# 2) Is there any effect on booking prices at different times of the year? ","cd020252":"# Data Cleaning\n","ebdd8365":"Boat seems to be the most expensive type of property in Seattle city followed by Condominium,Loft,House,TownHouse","967b0c28":"# Data Evaluation","6c95776a":"# ** 1) What are some most expensive property types in Seattle city ?**","c8ab78e5":"\nThis summarized listing csv contains 3818 rows and 92 columns \n\nInsights:\n\nNumber of neighbourhood is 82 with Capitol Hill - Greenwood to have the most listings\n67% of the listings are entire apt.\nminimum nights has an average of 2.36 and a maximum value of 1000 which is an indication of error\navailability_365 is 244 days\nprice is long tailed and is skewed to the right with a mean price of 127 and a max of 1000.\nThings to consider in the wrangling phase:\n","7885a126":"\nInsights:\n\nThe Most Expansive neighbourhood is Fairmount Park followed by Industrial District while the cheapest listings are RoxHills and Olypmpic Hills.","56e36e4b":"#  Exploring Seattle city Airbnb listings data\n\nHere I am going to take look at Seattle Airbnb listings and use CRISP-DM methodology to analyze the data. \n\nFollowing are the 6 phases of CRISP-DM (Cross-Industry Standard Process for Data Mining) :\n\n1) Business Understanding\n\n2) Data Understanding\n\n3) Data Preparation\/ Wrangling\n\n4) Data Modeling\n\n5) Data Evaluation\n\n6) Deployment","cce27da0":"**We can observe here that there is a price hike in mid of the year and prices are lowest at the start of the year.**","85d7f608":"# Whichcare the most impactfull attributes for predicting the price of a listing?","2a8fe83e":"# Visulalizations","d597695c":"# **3) What are the most and least expensive neighbourhoods in Seattle City?**","5fe8414d":"# Summary :\n\u00a0\nHere, I had a look into Airbnb listings data Seattle city and found some interesting insights which can help while exploring the city.\n\n1. Got the names of some most and least expensive neighbourhoods in Seattle.\n2. Compared the available listings data to the daily prices of the listings to find the relationship between them that helped us to identity high seasons,most expensive and cheapest months to visit Seattle.\n3. Identified the distribution of availability in different neighbourhoods also identified some most expensive and least expensive neighbourhoods.\n4. Identified attributes associated with price using clustering methods and xgboost model:\n\n * Availability\n * Neighbourhood\n * Property Type\n * Room Type\u00a0\n * Cancellation Policy\n * No. of Reviews"}}