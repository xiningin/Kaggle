{"cell_type":{"ddaa1379":"code","70389c51":"code","8e0b9f46":"code","6fcf5feb":"code","24e25d8f":"code","3a0b841c":"code","fb645f0b":"code","9e47d5e0":"code","2a36e215":"code","a53d75c6":"code","5bec5c4b":"code","b1aeabc7":"code","0c98870f":"code","044c4c62":"code","d9289027":"code","0fd708b0":"code","9957a0ab":"code","d257ece1":"code","c35ee037":"code","c3431ca0":"code","aefff08a":"code","b6706c9e":"code","2e786105":"code","96a4d324":"code","11460537":"markdown","fa828510":"markdown","64dd1cf9":"markdown","013c6f0f":"markdown","082ccb96":"markdown","f73375ce":"markdown","46e3b275":"markdown"},"source":{"ddaa1379":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70389c51":"df = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv') \n\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\ncategorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor c in categorical_columns:\n    df[c] = df[c].str.lower().str.replace(' ', '_')\n\ndf = df.loc[:, df.columns.intersection(['neighbourhood_group',\n                                        'room_type',\n                                        'latitude',\n                                        'longitude',\n                                        'price',\n                                        'minimum_nights',\n                                        'number_of_reviews',\n                                        'reviews_per_month',\n                                        'calculated_host_listings_count',\n                                        'availability_365'\n                                       ])]","8e0b9f46":"df.dtypes","6fcf5feb":"df.head().T","24e25d8f":"df.isnull().sum()","3a0b841c":"df['reviews_per_month'] = df['reviews_per_month'].fillna(0)","fb645f0b":"df.isnull().sum()","9e47d5e0":"df.neighbourhood_group.mode()","2a36e215":"corrMatrix = df.corr()\ncorrMatrix","a53d75c6":"sns.heatmap(corrMatrix, annot=True)\nplt.show()","5bec5c4b":"df[\"above_average\"] = 0\n\ndf['above_average'] = df['price'].apply(lambda x: 1 if x > 151 else 0)","b1aeabc7":"from sklearn.model_selection import train_test_split","0c98870f":"df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n\nlen(df_train), len(df_val), len(df_test)","044c4c62":"df_full_train = df_full_train.reset_index(drop = True)\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)","d9289027":"from sklearn.metrics import mutual_info_score\n\nmis_1 = mutual_info_score(df_train.above_average, df_train.neighbourhood_group)\n\nmis_2 = mutual_info_score(df_train.above_average, df_train.room_type)\n\nprint(round(mis_1, 2))\n\nprint(round(mis_2, 2))","0fd708b0":"#y_train = df_train.price.values\n#y_val = df_val.price.values\n#y_test = df_test.price.values\n\ny_train = df_train.above_average.values\ny_val = df_val.above_average.values\ny_test = df_test.above_average.values\n\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']\n\ndel df_train['above_average']\ndel df_val['above_average']\ndel df_test['above_average']","9957a0ab":"from sklearn.feature_extraction import DictVectorizer\n\ndv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train[['neighbourhood_group','room_type']].to_dict(orient='records')\nX_train = dv.fit_transform(train_dict)\n\nval_dict = df_val[['neighbourhood_group','room_type']].to_dict(orient='records')\nX_val = dv.transform(val_dict)","d257ece1":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver='lbfgs', C=1.0, random_state=42)\n\nmodel.fit(X_train, y_train)","c35ee037":"model.intercept_[0]","c3431ca0":"model.coef_[0].round(3)","aefff08a":"y_pred = model.predict(X_val)","b6706c9e":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\nprint(cnf_matrix)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","2e786105":"print('accuracy without dropping any column was : 0.596 ')\n\ndrop_test = ['neighbourhood_group', 'room_type', 'number_of_reviews','reviews_per_month']\n\nfor col in drop_test:\n    df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n    df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n    \n    control_list = ['neighbourhood_group','room_type']\n    \n    if col in control_list:\n        control_list.remove(col)\n    \n    df_full_train = df_full_train.reset_index(drop = True)\n    df_train = df_train.reset_index(drop=True)\n    df_val = df_val.reset_index(drop=True)\n    df_test = df_test.reset_index(drop=True)\n    \n    y_train = df_train.above_average.values\n    y_val = df_val.above_average.values\n    y_test = df_test.above_average.values\n    \n    del df_train['above_average']\n    del df_val['above_average']\n    del df_test['above_average']\n    \n    dv = DictVectorizer(sparse=False)\n    \n    df_train.drop(col,axis='columns', inplace=True)\n    df_val.drop(col,axis='columns', inplace=True)\n    df_test.drop(col,axis='columns', inplace=True)\n    \n    train_dict = df_train[control_list].to_dict(orient='records')\n    X_train = dv.fit_transform(train_dict)\n    \n    val_dict = df_val[control_list].to_dict(orient='records')\n    X_val = dv.transform(val_dict)\n    \n    model = LogisticRegression(solver='lbfgs', C=1.0, random_state=42)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_val)\n    \n    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n    print(col, \"\\t\\t\\t dropped and the Accuracy is : \\t\\t\",round(metrics.accuracy_score(y_test, y_pred),3),\n          '\\t\\t\\tdifference :', 0.596-round(metrics.accuracy_score(y_test, y_pred),3))\n    ","96a4d324":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nalpha_val = [0, 0.01, 0.1, 1, 10]\n\nfor value_alpha in alpha_val:\n\n    df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n    df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n    \n    df_full_train = df_full_train.reset_index(drop = True)\n    df_train = df_train.reset_index(drop=True)\n    df_val = df_val.reset_index(drop=True)\n    df_test = df_test.reset_index(drop=True)\n    \n    control_list = ['neighbourhood_group','room_type']\n        \n    y_train = np.log1p(df_train.price.values)\n    y_val = np.log1p(df_val.price.values)\n    y_test = np.log1p(df_test.price.values)\n        \n    del df_train['price']\n    del df_val['price']\n    del df_test['price']\n        \n    train_dict = df_train[control_list].to_dict(orient='records')\n    X_train = dv.fit_transform(train_dict)\n        \n    val_dict = df_val[control_list].to_dict(orient='records')\n    X_val = dv.transform(val_dict)\n    \n    test_dict = df_val[control_list].to_dict(orient='records')\n    X_test = dv.transform(test_dict)\n    \n    rr = Ridge(alpha= value_alpha)\n    rr.fit(X_train, y_train) \n    pred_train_rr= rr.predict(X_train)\n    print('for alpha value of ', value_alpha)\n    \n    print(round(np.sqrt(mean_squared_error(y_train,pred_train_rr)),3))\n    print(round(r2_score(y_train, pred_train_rr),3))\n    \n    pred_test_rr= rr.predict(X_test)\n    print(round(np.sqrt(mean_squared_error(y_test,pred_test_rr)),3)) \n    print(round(r2_score(y_test, pred_test_rr),3))\n    print('\\n\\n')","11460537":"****","fa828510":"**Logistic Regression**","64dd1cf9":"Binarized Prize and neighbourhood_group = 0.05\n\n\nBinarized Prize and room_type = 0.14","013c6f0f":"**most frequent observation (mode) for the column 'neighbourhood_group'**","082ccb96":"**manhattan**","f73375ce":"****","46e3b275":"**Number of Reviews and Reviews per month are the two most correlated features.**"}}