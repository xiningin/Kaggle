{"cell_type":{"6bfbbfa5":"code","2c683f23":"code","dd34e8b3":"code","054d996e":"code","7a513208":"code","45c251ee":"code","08a2b540":"code","dc6cb814":"code","92e0821b":"code","dee94fe1":"code","53b73f53":"code","a6c9a8bc":"code","94653a64":"code","be530721":"code","5047e478":"code","a7ae48cb":"code","89cb7a3f":"code","b8c34c86":"code","0763d8c1":"code","ca2568e1":"code","dbb49c40":"code","d1a69de3":"code","7dd73366":"code","cac4cbfe":"code","c75b6d80":"code","00acd376":"code","833312ae":"code","80ed7fe9":"code","ef82d9fc":"code","71fff91b":"code","84b11efd":"code","f5be9871":"code","c909412e":"code","7a042e73":"code","88acd970":"code","518579be":"code","30a38fcc":"code","e4f4c837":"code","9857d803":"code","1b6b6230":"code","e3c9be15":"code","782a3d4b":"code","3beb1c9a":"markdown","6602fbe2":"markdown","52a4e1d5":"markdown","88727d46":"markdown","f3ed91c9":"markdown","7954c5d9":"markdown","e98c9e49":"markdown","cb4ec815":"markdown","cafcb576":"markdown","66878c42":"markdown","f3632823":"markdown","55825ca4":"markdown","b0dbab4e":"markdown","21e42006":"markdown","55cd37fd":"markdown","b4336d0a":"markdown","2b705bb2":"markdown","dec8c2af":"markdown","56328a39":"markdown","1de734fa":"markdown","785a6757":"markdown","c4360537":"markdown","d5cfe263":"markdown","d7e3832a":"markdown","318d5572":"markdown","5613ab16":"markdown","400bab61":"markdown","202b3b05":"markdown","820b55b5":"markdown","d190a013":"markdown","4b234677":"markdown","c1f7fa24":"markdown"},"source":{"6bfbbfa5":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport warnings\nimport gc\n\n# Scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\n\n# Boosters\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\n# Hyperparameter \nimport optuna\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\n\nDEMO=True","2c683f23":"%%time\n# The id column disrupts the training so delete it\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv').drop(columns=['id'])\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv', index_col=0)\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')","dd34e8b3":"del train\ndel test\ndel submission\ngc.collect()","054d996e":"%%time\ntrain =dt.fread('\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv', columns=lambda cols:[col.name not in ('id') for col in cols]).to_pandas()\ntest = dt.fread('\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv', columns=lambda cols:[col.name not in ('id') for col in cols]).to_pandas()\nsubmission = dt.fread('\/kaggle\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()","7a513208":"train.head().T","45c251ee":"train.info(verbose=True, memory_usage=\"deep\")","08a2b540":"train.describe().T","dc6cb814":"train.sample(4).T","92e0821b":"def convert_type(data, col, new_type):\n    return data[col].astype(new_type)","dee94fe1":"for c in train.columns:\n    if train[c].dtypes == 'float64':\n        train[c] = convert_type(train, c, 'float32')\n    elif train[c].dtypes == 'int64':\n        train[c] = convert_type(train, c, 'bool')","53b73f53":"for c in test.columns:\n    if test[c].dtypes == 'float64':\n        test[c] = convert_type(test, c, 'float32')\n    elif train[c].dtypes == 'int64':\n        test[c] = convert_type(test, c, 'bool')","a6c9a8bc":"test.isna().sum().sum()","94653a64":"print(f'There is:\\n{train.isna().sum().sum()} null values in train dataframe\\n{test.isna().sum().sum()} null values in test dataframe')","be530721":"n_splits = 4\ny = train['target']\nX = train.drop(columns=['target'])","5047e478":"%%time\nX_test, X_train = np.split(X, [train.shape[0] \/\/ n_splits])\ny_test, y_train = np.split(y, [train.shape[0] \/\/ n_splits])","a7ae48cb":"print(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","89cb7a3f":"del y, X, X_test, X_train, y_test, y_train\ngc.collect()","b8c34c86":"%%time\ntrain = train.sample(frac=1).reset_index(drop=True)\ny = train['target']\nX = train.drop(columns=['target'])","0763d8c1":"%%time\n# from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)\nprint(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}\\n')","ca2568e1":"del X_test, X_train, y_test, y_train\ngc.collect()","dbb49c40":"# %%time\n# from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(kf.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n\n    print(f'\\n===== fold {fold} ====\\n X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","d1a69de3":"del X_test, X_train, y_test, y_train\ngc.collect()","7dd73366":"# %%time\n# from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=59) \n\nfor train_index, test_index in skf.split(X=X, y=y):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n\n    print(f' X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","cac4cbfe":"del X_test, X_train, y_test, y_train\ngc.collect()","c75b6d80":"# %%time\n# from sklearn.model_selection import ShuffleSplit\n\nshs = ShuffleSplit(n_splits=n_splits, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(shs.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test = y.iloc[test_index]\n    \n    print(f'\\n===== fold {fold} ====\\n X_train.shape: {X_train.shape} \\n X_test.shape: {X_test.shape} \\n y_train.shape: {y_train.shape} \\n y_test.shape: {y_test.shape}')","00acd376":"# from lightgbm import LGBMClassifier\nparameters = {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'device' : 'gpu'\n}\nmodel = LGBMClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","833312ae":"# from catboost import CatBoostClassifier\nparameters = {\n    'objective' : 'Logloss',\n    'eval_metric' : 'AUC',\n    'task_type' : 'GPU'\n}\n\nmodel = CatBoostClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","80ed7fe9":"# from xgboost import XGBClassifier\nparameters = {\n    'objective': 'binary:logistic',\n    'eval_metric' : 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor'\n}\n\nmodel = XGBClassifier(**parameters)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n    )\ny_predicted = model.predict_proba(X_test)","ef82d9fc":"# y_predicted_oof = np.zeros((train.shape[0],))\nparameters = {\n    'objective': 'binary:logistic',\n    'eval_metric' : 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor'\n}\nmodel = XGBClassifier(**parameters)\n\nshs = ShuffleSplit(n_splits=n_splits, random_state=59)\n\nfor fold, (train_index, test_index) in enumerate(shs.split(X=X)):\n    X_train = X.iloc[train_index]\n    X_test  = X.iloc[test_index]\n    y_train = y.iloc[train_index]\n    y_test  = y.iloc[test_index]\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False\n    )\n    y_predicted = model.predict_proba(X_test)\n    \n    # temp_oof = model.predict(X_valid)\n    # y_predicted_oof[test_index] = temp_oof\n    # print(f'Fold {fold} RMSE: ', get_accuracy_2(y_test, temp_oof))\n    # print(f'fold: {fold} |  Score: {get_score_1(y_test, y_predicted)} \\n')\n    \n    \n# print(f'OOF Accuracy: ', get_accuracy_2(train['claim'], y_predicted_oof) )","71fff91b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","84b11efd":"def lgbm_objective(trial):\n    parameters = {\n        \"device\" : \"gpu\",\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1, # < 0: Fatal, = 0: Error (Warning), = 1: Info, > 1: Debug\n        \"boosting_type\": \"gbdt\", # gbdt, rf, dart, goss\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),# 0.1<-<1.0\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),# 0.1<-<1.0\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n    model = LGBMClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)]\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","f5be9871":"# import optuna\nlgbm_study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\nlgbm_study.optimize(lgbm_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(lgbm_study.trials)))\n\nprint(\"Best trial for LGBM:\")\ntrial = lgbm_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","c909412e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","7a042e73":"def catboost_objective(trial):\n    parameters = {\n            \"eval_metric\" : \"AUC\",\n            \"task_type\" : \"GPU\",\n            \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n            \"depth\": trial.suggest_int(\"depth\", 1, 12),\n            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n            \"bootstrap_type\": trial.suggest_categorical(\n                \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n            )\n        }\n\n    if parameters[\"bootstrap_type\"] == \"Bayesian\":\n        parameters[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif parameters[\"bootstrap_type\"] == \"Bernoulli\":\n        parameters[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n        \n    model = CatBoostClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False,\n        early_stopping_rounds=100\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","88acd970":"# import optuna\ncatboost_study = optuna.create_study(direction=\"maximize\", study_name=\"CatBoost Classifier\")\ncatboost_study.optimize(catboost_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(catboost_study.trials)))\n\nprint(\"Best trial for CatBoost:\")\ntrial = catboost_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","518579be":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 \/ n_splits), random_state=59, shuffle=True)","30a38fcc":"def xgboost_objective(trial):\n    parameters = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        \"predictor\": \"gpu_predictor\",\n        \"eval_metric\" : \"auc\",\n        # use exact for small dataset.\n        \"tree_method\": \"gpu_hist\",\n        # defines booster, gblinear for linear functions.\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\"]),\n        # L2 regularization weight.\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        # L1 regularization weight.\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        # sampling ratio for training data.\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n        # sampling according to each tree.\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n    }\n\n    # maximum depth of the tree, signifies complexity of the tree.\n    parameters[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n    # minimum child weight, larger the term more conservative the tree.\n    parameters[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n    parameters[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n    # defines how selective algorithm is.\n    parameters[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n    parameters[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n\n    model = XGBClassifier(**parameters)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        verbose=False,\n        )\n    y_predicted = model.predict_proba(X_test)\n    \n    return roc_auc_score(y_test, y_predicted)","e4f4c837":"# import optuna\nxgboost_study = optuna.create_study(direction=\"maximize\", study_name=\"XGBoost Classifier\")\nxgboost_study.optimize(xgboost_objective, n_trials=100)\n\nprint(\"Number of finished trials: {}\".format(len(xgboost_study.trials)))\n\nprint(\"Best trial for XGBoost:\")\ntrial = xgboost_study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","9857d803":"# from sklearn.metrics import *\ndef get_score_1(y_test, y_predicted):\n    fpr, tpr, _ = roc_curve(y_test, y_predicted[:, 1])\n    return auc(fpr, tpr)","1b6b6230":"# from sklearn.metrics import *\ndef get_score_2(y_test, y_predicted):\n    return roc_auc_score(y_test, y_predicted[:, 1])","e3c9be15":"# from sklearn.metrics import *\ndef get_accuracy_1(y_test, y_predicted):\n    return mean_squared_error(y_test, y_predicted, squared=False)","782a3d4b":"submission['target'] = model.predict_proba(test)[:, 1]\nsubmission.to_csv('tps-1021-submit.csv', index = False)","3beb1c9a":"<div class=\"alert alert-info\">\n  <svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" width=\"32\" height=\"32\" fill=\"currentColor\" class=\"bi bi-info-circle-fill\" viewBox=\"0 0 16 16\">\n  <path d=\"M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16zm.93-9.412-1 4.705c-.07.34.029.533.304.533.194 0 .487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703 0-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381 2.29-.287zM8 5.5a1 1 0 1 1 0-2 1 1 0 0 1 0 2z\"\/>\n<\/svg>\n<\/svg>\n<b style=\"font-size: x-large;\">MORE INFO<\/b><br>\n   The difference between <code>Pandas<\/code> and <code>Datatable<\/code> and the best way to load data is fully <code><a href=\"https:\/\/www.kaggle.com\/akmeghdad\/tps-1021-how-to-make-better-use-of-kaggle-memory\" target=\"_blank\">explained here<\/a><\/code>\n<\/div>","6602fbe2":"## 6.3 XGBoost<a id=\"6.3\"><\/a>\nOfficial documentation for parameters:\n- [https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)\n- [https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.rst](https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.rst)","52a4e1d5":"## 7.2 Optuna for CatBoost<a id=\"7.2\"><\/a>\n- [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/catboost\/catboost_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/catboost\/catboost_simple.py)","88727d46":"# 4. Preprocessing","f3ed91c9":"- <https:\/\/www.kaggle.com\/akmeghdad\/tps-1021-how-to-make-better-use-of-kaggle-memory>","7954c5d9":"# 7. Hyperparameter <a id=\"7\"><\/a>\nAs I said, parameters are very important in LightGBM, CatBoost and XGBoost methods. Optuna is one of the automatic hyperparameter optimization software framework.\n- [https:\/\/optuna.org](https:\/\/optuna.org)\n- [https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.trial.Trial.html](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.trial.Trial.html)\n\n## 7.1 Optuna for LightGBM<a id=\"7.1\"><\/a>\n- [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_simple.py)","e98c9e49":"# Contents<a id=\"0\"><\/a>\n\n- [1. Abstract](#1)\n- [2. Modules](#2)\n- [3. Load Data](#3)\n  - [3.1 Pandas](#3.1)\n  - [3.2 Datatable](#3.2)\n  - [3.3 Statistics](#3.3)\n- [4. Preprocessing](#4)\n  - [4.1 Missing values](#4.1)\n  - [4.2 Standardization](#4.2)\n- [5. Cross Validation (K-Fold)](#5)\n  - [5.1 Numpy](#5.1)\n  - [5.2 Scikit Learn](#5.2)\n    - [5.2.1 train_test_split](#5.2.1)\n    - [5.2.2 K-Fold](#5.2.2)\n    - [5.2.3 StratifiedKFold](#5.2.3)\n    - [5.2.4 ShuffleSplit](#5.2.4)\n- [6. Modeling](#6)\n  - [6.1 LightGBM](#6.1)\n  - [6.2 CatBoost](#6.1)\n  - [6.3 XGBoost](#6.3)\n  - [6.4 Boosters with cross validation](#6.4)\n- [7. Hyperparameter](#7)\n  - [7.1 Optuna for LightGBM](#7.1)\n  - [7.2 Optuna for CatBoost](#7.2)\n  - [7.3 Optuna for XGBoost](#7.3)\n- [8. Quality of predictions](#8)\n- [9. Submission](#9)","cb4ec815":"### 5.2.2 K-Fold<a id=\"5.2.2\"><\/a>\n\nIn the following three methods, the splitting operation is repeated `n_splits` times.","cafcb576":"# 3. Load Data<a id=\"3\"><\/a>\n\nIt seems that Pandas load data slowly. One alternative is to use `datatable` and convert theme to `Pandas`. [Link](https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro#Read-in-the-massive-dataset)","66878c42":"### 5.2.3 StratifiedKFold<a id=\"5.2.3\"><\/a>","f3632823":"## 6.2 CatBoost<a id=\"6.2\"><\/a>\nOfficial documentation for parameters:\n- [https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html#python-reference_parameters-list](https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html#python-reference_parameters-list)","55825ca4":"## 5.1 Numpy<a id=\"5.1\"><\/a>","b0dbab4e":"## 5.2 Scikit Learn<a id=\"5.2\"><\/a>\n\nThe above two operations are performed in different ways\n\n### 5.2.1 train_test_split<a id=\"5.2.1\"><\/a>","21e42006":"We can't view all the columns and datatypes when we want print a concise summary of a DataFrame, we use `verbose` as argument to solve this problem","55cd37fd":"## 3.3 Statistics<a id=\"3.3\"><\/a>\n\nDisplay head of DataFrame is not normally legible. so we [transpose](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.25.0\/reference\/api\/pandas.DataFrame.T.html) it.","b4336d0a":"## 6.4 Boosters with cross validation<a id=\"6.4\"><\/a>","2b705bb2":"It semble [transpose](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.25.0\/reference\/api\/pandas.DataFrame.T.html) is good idea to show any other methods of DataFrame.","dec8c2af":"## 3.2 Datatable<a id=\"3.2\"><\/a>\n\nTo learn more about Datatable [see this kaggle code](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-python-datatable). Datatable `fread` documentation [is here](https:\/\/datatable.readthedocs.io\/en\/latest\/api\/dt\/fread.html)","56328a39":"# 1. Abstract<a id=\"1\"><\/a>\n\nThese are my personal notes. I would like to share them with you and I hope it can be useful for you, even if it is a small help. I will try to update these notes over time.","1de734fa":"# 5. Cross Validation (K-Fold)<a id=\"5\"><\/a>\n\nThere are several ways to split data. In the following, we consider the number of parts equal to 4. We put 25% of the data for testing and 75% for training.","785a6757":"## 4.1 Missing values\n\nThere is no Missing values in [Tabular Playground Series - Oct 2021 data](https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/data) ","c4360537":"## 3.1 Pandas<a id=\"3.1\"><\/a>","d5cfe263":"# 8. Quality of predictions<a id=\"8\"><\/a>","d7e3832a":"# 2. Modules<a id=\"2\"><\/a>","318d5572":"## 4.2 Standardization<a id=\"4.2\"><\/a>","5613ab16":"The difference between the three methods: [(see here for more info)](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_cv_indices.html)\n\n![kfold-StratifiedKFold](https:\/\/raw.githubusercontent.com\/akmeghdad\/data-science-note\/master\/src\/images\/model-selection-3-models-kfold.jpg)","400bab61":"# 6 Modeling<a id=\"6\"><\/a>\n\nLightGBM, CatBoost and XGBoost are known as new ways to build models. Parameter settings are very important in these three methods. Only the basic parameters are listed here. \n\n## 6.1 LightGBM<a id='6.1'><\/a>\nOfficial documentations for parameters:\n- [https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html)\n- [https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst)","202b3b05":"### 5.2.4 ShuffleSplit<a id=\"5.2.4\"><\/a>","820b55b5":"<div class=\"alert alert-danger\">\n<svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" width=\"32\" height=\"32\" viewBox=\"0 0 16 16\" fill=\"currentColor\">\n  <path d=\"M8.982 1.566a1.13 1.13 0 0 0-1.96 0L.165 13.233c-.457.778.091 1.767.98 1.767h13.713c.889 0 1.438-.99.98-1.767L8.982 1.566zM8 5c.535 0 .954.462.9.995l-.35 3.507a.552.552 0 0 1-1.1 0L7.1 5.995A.905.905 0 0 1 8 5zm.002 6a1 1 0 1 1 0 2 1 1 0 0 1 0-2z\"\/>\n<\/svg>\n<b style=\"font-size: x-large;\">ATTENTION<\/b><br>\n    Standardization with <code>sklearn.preprocessing.*<\/code> will overflow the memory.\n<\/div>","d190a013":"For better results, we can `shuffle` the data","4b234677":"## 7.3 Optuna for XGBoost<a id=\"7.3\"><\/a>\n- [https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/xgboost\/xgboost_simple.py](https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/xgboost\/xgboost_simple.py)","c1f7fa24":"# 9. Submission<a id=\"9\"><\/a>"}}