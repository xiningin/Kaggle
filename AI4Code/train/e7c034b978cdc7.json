{"cell_type":{"d9414453":"code","03a697ca":"code","d96c512e":"code","b391cf85":"code","eada63b7":"code","33f5a77f":"code","264dc97e":"code","cff19635":"code","ad3ca12f":"code","14b45299":"code","ade1e7dc":"code","75047cf7":"code","07d46444":"code","9e9a20df":"code","3f233351":"code","ff1bce69":"markdown","5c2a787c":"markdown","04f25f96":"markdown","ead5fcb7":"markdown","18e3fee3":"markdown","a30eb2c4":"markdown","197d9b45":"markdown","366f46bf":"markdown"},"source":{"d9414453":"#Import the required libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.mlab as mlab\n\n#Supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n","03a697ca":"#Importing the dataset and dropping the unwanted columns \nheart_df=pd.read_csv(\"..\/input\/framingham_heart_disease.csv\")\nheart_df.drop(['education'],axis=1,inplace=True)\nheart_df.head()","d96c512e":"#Renaming the column name\nheart_df.rename(columns={'male':'Sex_male'},inplace=True)\nheart_df.head()","b391cf85":"# Checking the missing values \nheart_df.isnull().sum()","eada63b7":"#Counting the missing values and dropping them\ncount=0\nfor i in heart_df.isnull().sum(axis=1):\n    if i>0:\n        count=count+1\nprint('Total number of rows with missing values is ', count)\nprint('since it is only',round((count\/len(heart_df.index))*100), 'percent of the entire dataset the rows with missing values are excluded.')","33f5a77f":"#Dropping the missing values columns\nheart_df.dropna(axis=0,inplace=True)\nheart_df.describe()","264dc97e":"# Again checking the missing values \nheart_df.isnull().sum()\n","cff19635":"#Logistic regression - Adding a constant\nfrom statsmodels.tools import add_constant as add_constant\nheart_df_constant = add_constant(heart_df)\nheart_df_constant.head()","ad3ca12f":"#Logistic regression - Chi square method\nst.chisqprob = lambda chisq, df: st.chi2.sf(chisq, df)\ncols=heart_df_constant.columns[:-1]\nmodel=sm.Logit(heart_df.TenYearCHD,heart_df_constant[cols])\nresult=model.fit()\nresult.summary()","14b45299":"#Feature Selection: Backward elemination (P-value approach)\n#Takes in the dataframe, the dependent variable and a list of column names, \n#runs the regression repeatedly eleminating feature with the highest\n#P-value above alpha one at a time and returns the regression summary \n#with all p-values below alpha\n\ndef back_feature_elem (data_frame,dep_var,col_list):\n    while len(col_list)>0 :\n        model=sm.Logit(dep_var,data_frame[col_list])\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.05):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)\n\nresult=back_feature_elem(heart_df_constant,heart_df.TenYearCHD,cols)\nresult.summary()\n\n","ade1e7dc":"#Interpreting the results: Odds Ratio, Confidence Intervals and Pvalues\nparams = np.exp(result.params)\nconf = np.exp(result.conf_int())\nconf['OR'] = params\npvalue=round(result.pvalues,3)\nconf['pvalue']=pvalue\nconf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\nprint ((conf))\n","75047cf7":"#Splitting data to train and test split\nimport sklearn\nnew_features=heart_df[['age','Sex_male','cigsPerDay','totChol','sysBP','glucose','TenYearCHD']]\nx=new_features.iloc[:,:-1]\ny=new_features.iloc[:,-1]\n\nfrom sklearn.model_selection import train_test_split  \n#use sklearn.cross_validation in jupyter\/spyder in place of sklearn.model_selection\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=5)","07d46444":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","9e9a20df":"#Confusion matrix\nfrom sklearn import metrics\nconfusion_matrix = metrics.confusion_matrix(y_test,y_pred)\nprint(confusion_matrix)","3f233351":"#Model Accuracy\nsklearn.metrics.accuracy_score(y_test,y_pred)","ff1bce69":"**Accuracy of the model is 86 percent**","5c2a787c":"The results above show some of the attributes with P-value higher than the preferred alpha(5%) and there by showing low statistically significant relationship with the probability of heart disease. Backward elemination approach is used here to remove those attributes with highest P-value one at a time follwed by running the regression repeatedly until all attributes have P-Values less than 0.05.","04f25f96":"**LOGISTIC REGRESSION - HEART DISEASE PREDICTION**","ead5fcb7":"**Logistic Regression**\n\nLogistic regression is a type of regression analysis in statistics used for prediction of outcome of a categorical dependent variable from a set of predictor or independent variables. In logistic regression the dependent variable is always binary. Logistic regression is mainly used to for prediction and also calculating the probability of success.","18e3fee3":"From the above table - There are no missing values in the dataset","a30eb2c4":"**Data Preparation**\n\n**Source:**\nThe dataset is publically available on the Kaggle website, and it is from an ongoing ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients\u2019 information. It includes over 4,000 records and 15 attributes.","197d9b45":"**Introduction**\n\nWorld Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. This research intends to pinpoint the most relevant\/risk factors of heart disease as well as predict the overall risk using logistic regression","366f46bf":"**Logistic regression equation**\n\nP=e(\u03b20+\u03b21X1+....)\/(1+e(\u03b20+\u03b21X1+...))\n \n**When all features plugged in:**\n\nlogit(p)=log(p\/(1\u2212p)) = \u03b20 + \u03b21\u2217Sexmale + \u03b22\u2217age + \u03b23\u2217cigsPerDay  + \u03b24\u2217totChol + \u03b25\u2217sysBP + \u03b26\u2217glucose"}}