{"cell_type":{"a1b800dd":"code","d0c6b362":"code","3ba67c75":"code","76761839":"code","40e380ad":"code","0333ddb2":"code","788d47a1":"code","285cfc41":"code","660bafe4":"code","01733d60":"code","6ca4a296":"code","7a9e1aac":"code","944f8e21":"markdown"},"source":{"a1b800dd":"#SETTINGS\n\n#environment config   =============================================================\nenvironmentType=\"linux\"       # windows , linux\ninstallEnvironment=False\n\n#Colab config ===================================================================\nGdriveMount=\"drive\"\nGdrivePath='\/My Drive\/'         # with slash at the beginning and at the end\n\nimport sys\nrunningOncolab = 'google.colab' in sys.modules\n#Dataset config ===================================================================\ndatasetPath=\"\"\ndatasetToLoaded='FC15FULL'\n\n# ==========================================================================","d0c6b362":"if runningOncolab:\n    from google.colab import drive\n    from os.path import join\n    drive.mount(GdriveMount)\n    \n    !cp \"drive\/My Drive\/dbn\/activations.py\" \"activations.py\"\n    !cp \"drive\/My Drive\/dbn\/models.py\" \"models.py\"\n    !cp \"drive\/My Drive\/dbn\/customTf.py\" \"customTf.py\"\n    !cp \"drive\/My Drive\/dbn\/utils.py\" \"utils.py\"\n\n    !cp \"drive\/My Drive\/FC15FULL.csv\" \"FC15FULL.csv\"\n    \n    print(\"modules copied.\")\n    sys.path.append('\/dbn')","3ba67c75":"import numpy as np\n\nnp.random.seed(1337)  # for reproducibility\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.regression import r2_score, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nfrom sklearn.metrics.classification import accuracy_score","76761839":"def buildDataset(temp, diamLen, volFiber):\n  time=0\n  pos=0\n  npa= np.zeros(shape=(int(7.0\/0.013)+1, 4))\n  while time<7.0:\n    npa[pos,0]=time\n    npa[pos,1]=diamLen\n    npa[pos,2]=volFiber\n    npa[pos,3]=temp\n    time=time+0.013\n    pos=pos+1\n  return npa\n\ndef doPredict(temp, diamLen, volFiber):\n  X_regress=buildDataset(temp, diamLen, volFiber)\n\n  Y_regress = regressor.predict(X_regress)\n  x_res_reg_time, diamLen, volFiber, x_res_reg_temp =X_regress.T\n  return x_res_reg_time, x_res_reg_temp, Y_regress\n\ndef doPlotting():\n  import matplotlib.pyplot as plt\n  import matplotlib.pyplot as pltImg\n\n  print('Plot for d\/l=75 and vol. fiber 0.023:')\n  x_res_reg_time, x_res_reg_temp, Y_regress= doPredict(1, 75, 0.023)\n\n  x_res_exp_time,  diamLen, volFiber, x_res_exp_temp =X_train.T\n\n  #Test\n  Y_pred = regressor.predict(X_test)\n  if np.isnan(Y_pred).any():\n    Y_pred[np.isnan(Y_pred)] = 0\n    print(\"null values found on Y\")\n  rs=r2_score(Y_test, Y_pred)\n  mse=mean_squared_error(Y_test, Y_pred)\n  print('Done.\\nR-squared: %f\\nMSE: %f' % (rs, mse))\n\n  #x1=X_test\n  fig, ax1 = plt.subplots()\n\n  ax1.set_ylabel('DR \/ R0 0.230')\n  ax1.set_xlabel('time (days)', color='g')\n  plt.scatter(x_res_reg_time, Y_regress, color='g', marker='o')\n\n  plt.scatter(x_res_exp_time, Y_train, color='r', marker='x')\n  filename=datasetToLoaded+\"_iter_\"+str(numIterations)+\"_r2s_\"+str(rs)+\"_mse_\"+str(mse)+\".png\"\n\n  plt.savefig(filename)\n  path=\"'\"+GdriveMount+GdrivePath+filename+\"'\"\n  !cp $filename $path\n\n  saveDataToDrive(rs, mse, x_res_exp_time, Y_train,x_res_reg_time,Y_regress)\n\n  print('Plot for d\/l=75 and vol. fiber 0.060:')\n  x_res_reg_time, x_res_reg_temp, Y_regress= doPredict(1, 75, 0.060)\n\n  x_res_exp_time,  diamLen, volFiber, x_res_exp_temp =X_train.T\n\n  #Test\n  Y_pred = regressor.predict(X_test)\n  if np.isnan(Y_pred).any():\n    Y_pred[np.isnan(Y_pred)] = 0\n    print(\"null values found on Y\")\n  rs=r2_score(Y_test, Y_pred)\n  mse=mean_squared_error(Y_test, Y_pred)\n  print('Done.\\nR-squared: %f\\nMSE: %f' % (rs, mse))\n\n  #x1=X_test\n  fig, ax1 = plt.subplots()\n\n  ax1.set_ylabel('DR \/ R0 0.184')\n  ax1.set_xlabel('time (days)', color='g')\n  plt.scatter(x_res_reg_time, Y_regress, color='g', marker='o')\n\n  plt.scatter(x_res_exp_time, Y_train, color='r', marker='x')","40e380ad":"def saveDataToDrive(rs, mse, x_res_exp_time, Y_train,x_res_reg_time,Y_regress):\n  global numIterations, hidden_layers_structure, learning_rate, batch_size, activation_function, GdriveMount,GdrivePath\n  df = pd.DataFrame(list(zip(x_res_exp_time, Y_train,x_res_reg_time,Y_regress.flatten() )), columns =['Time', 'DR\/D0 exp.','Time','DR\/D0 regress.'])\n\n  print(\"saving to csv...\")\n  filename=datasetToLoaded+\"_iter_\"+str(numIterations)+\"_r2s_\"+str(rs)+\"_mse_\"+str(mse)+\".csv\"\n  df.to_csv(filename, sep=';', encoding='utf-8', header=1, index=False)\n  path=\"'\"+GdriveMount+GdrivePath+filename+\"'\"\n\n  !cp $filename $path\n\n  filename=datasetToLoaded+\"_iter_\"+str(numIterations)+\"_r2s_\"+str(rs)+\"_mse_\"+str(mse)+\"_config.txt\"\n  text_file = open(\".\/\"+filename, \"w\")\n  text_file.write(\" num Iterations : \"+str(numIterations))\n  text_file.write(\"\\n Learning rate : \"+str(learning_rate))\n  text_file.write(\"\\n Batch Size : \"+str(batch_size))\n  text_file.write(\"\\n Activation Fucntion : \"+str(activation_function))\n  text_file.write(\"\\n Hidden Layers structure : \"+''.join(str(e)+\", \" for e in hidden_layers_structure))\n  text_file.write(\"\\n R2 : \"+str(rs))\n  text_file.write(\"\\n MSE : \"+str(mse))\n  text_file.close()\n  path=\"'\"+GdriveMount+GdrivePath+filename+\"'\"\n\n  !cp $filename $path\n  print('saved.')","0333ddb2":"# Loading dataset\ndatasetPath=\"\"\ndatasetToLoad=datasetPath+\"FC15FULL.csv\"\ndata_raw= pd.read_csv(datasetToLoad,sep=';', encoding='utf-8', names=['time','er','normEr','diamLen','volFiber','temp','normTemp'])\ncolumns=['time']\ndata_x = data_raw[columns][::]\ndata_x=data_x.to_numpy()\n\ncolumns=['normEr']\ndata_y = data_raw[columns][::]\ndata_y = np.asarray(list(data_y['normEr']), dtype=np.float32)\n\n#data_y=data_y.to_numpy()\n#data_y=np.interp(data_y, (data_y.min(), data_y.max()), (-1, 0))\n\nfrom sklearn.utils import shuffle\ndata_x, data_y = shuffle(data_x, data_y, random_state=0)\n\nprint(data_y.shape)\nprint(data_x.shape)\nprint(data_x[0])\n\n# Splitting data\nX_train, Y_train = data_x, data_y\nX_train2, X_test, Y_train2, Y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=1337)\nimport matplotlib.pyplot as plt\nfig, ax1 = plt.subplots()\nx_exp_time =data_x.T\nax1.set_ylabel('DR \/ R0')\nax1.set_xlabel('time (days)', color='g')\nplt.scatter(x_exp_time, data_y, color='g', marker='o')\n\n#plt.scatter(x_exp_time, x_exp_temp, color='r', marker='x')","788d47a1":"# Data scaling\nmin_max_scaler = MinMaxScaler()\n#X_train = min_max_scaler.fit_transform(X_train)\n\niterArr=[100]\nhidden_layers_structure=[100]\nlearning_rate=0.05\nbatch_size=60\nactivation_function='relu'\n\nfor i in range(len(iterArr)):\n    #Training\n    print(\"number of iterations:\"+ str(iterArr[i]))\n    print(\"___________________________________________________________\")\n    numIterations=iterArr[i]\n\n    regressor = SupervisedDBNRegression(hidden_layers_structure=hidden_layers_structure,\n                                        learning_rate_rbm=learning_rate,\n                                        learning_rate=learning_rate,\n                                        n_epochs_rbm=numIterations,\n                                        n_iter_backprop=numIterations,\n                                        batch_size=batch_size,\n                                        activation_function=activation_function)\n    \n\n    \n    regressor.fit(X_train, Y_train)\n    print(\"Test Data : _______________________________________________\")\n\n    Y_pred = regressor.predict(X_train)\n\n    rs=r2_score(Y_train, Y_pred)\n    mse=mean_squared_error(Y_train, Y_pred)\n    print('Done.\\nR-squared: %f\\nMSE: %f' % (rs, mse))\n\n\n    fig, ax1 = plt.subplots()\n    x_regr_time =X_train.T\n    x_exp_time =X_train.T\n    ax1.set_ylabel('DR \/ R0')\n    ax1.set_xlabel('time (days)', color='g')\n    plt.scatter(x_regr_time, Y_pred, color='g', marker='o')\n    plt.scatter(x_exp_time, Y_train, color='r', marker='x')\n\n    print(\"___________________________________________________________\")\n    #doPlotting()","285cfc41":"from abc import ABCMeta, abstractmethod\n\nimport numpy as np\n\n\nclass ActivationFunction(object):\n    \"\"\"\n    Class for abstract activation function.\n    \"\"\"\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def function(self, x):\n        return\n\n    @abstractmethod\n    def prime(self, x):\n        return\n\n\nclass SigmoidActivationFunction(ActivationFunction):\n    @classmethod\n    def function(cls, x):\n        \"\"\"\n        Sigmoid function.\n        :param x: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        return 1 \/ (1.0 + np.exp(-x))\n\n    @classmethod\n    def prime(cls, x):\n        \"\"\"\n        Compute sigmoid first derivative.\n        :param x: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        return x * (1 - x)\n\n\nclass ReLUActivationFunction(ActivationFunction):\n    @classmethod\n    def function(cls, x):\n        \"\"\"\n        Rectified linear function.\n        :param x: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        return np.maximum(np.zeros(x.shape), x)\n\n    @classmethod\n    def prime(cls, x):\n        \"\"\"\n        Rectified linear first derivative.\n        :param x: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        return (x > 0).astype(int)\n\n\nclass TanhActivationFunction(ActivationFunction):\n    @classmethod\n    def function(cls, x):\n        \"\"\"\n        Hyperbolic tangent function.\n        :param x: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        return np.tanh(x)\n\n    @classmethod\n    def prime(cls, x):\n        \"\"\"\n        Hyperbolic tangent first derivative.\n        :param x: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        return 1 - x * x\n","660bafe4":"from abc import ABCMeta, abstractmethod\n\nimport numpy as np\nfrom scipy.stats import truncnorm\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, RegressorMixin\n\nfrom activations import SigmoidActivationFunction, ReLUActivationFunction\nfrom utils import batch_generator\n\n\nclass BaseModel(object):\n    def save(self, save_path):\n        import pickle\n\n        with open(save_path, 'wb') as fp:\n            pickle.dump(self, fp)\n\n    @classmethod\n    def load(cls, load_path):\n        import pickle\n\n        with open(load_path, 'rb') as fp:\n            return pickle.load(fp)\n\n\nclass BinaryRBM(BaseEstimator, TransformerMixin, BaseModel):\n    \"\"\"\n    This class implements a Binary Restricted Boltzmann machine.\n    \"\"\"\n\n    def __init__(self,\n                 n_hidden_units=100,\n                 activation_function='sigmoid',\n                 optimization_algorithm='sgd',\n                 learning_rate=1e-3,\n                 n_epochs=10,\n                 contrastive_divergence_iter=1,\n                 batch_size=32,\n                 verbose=True):\n        self.n_hidden_units = n_hidden_units\n        self.activation_function = activation_function\n        self.optimization_algorithm = optimization_algorithm\n        self.learning_rate = learning_rate\n        self.n_epochs = n_epochs\n        self.contrastive_divergence_iter = contrastive_divergence_iter\n        self.batch_size = batch_size\n        self.verbose = verbose\n\n    def fit(self, X):\n        \"\"\"\n        Fit a model given data.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        # Initialize RBM parameters\n        self.n_visible_units = X.shape[1]\n        if self.activation_function == 'sigmoid':\n            self.W = np.random.randn(self.n_hidden_units, self.n_visible_units) \/ np.sqrt(self.n_visible_units)\n            self.c = np.random.randn(self.n_hidden_units) \/ np.sqrt(self.n_visible_units)\n            self.b = np.random.randn(self.n_visible_units) \/ np.sqrt(self.n_visible_units)\n            self._activation_function_class = SigmoidActivationFunction\n        elif self.activation_function == 'relu':\n            self.W = truncnorm.rvs(-0.2, 0.2, size=[self.n_hidden_units, self.n_visible_units]) \/ np.sqrt(\n                self.n_visible_units)\n            self.c = np.full(self.n_hidden_units, 0.1) \/ np.sqrt(self.n_visible_units)\n            self.b = np.full(self.n_visible_units, 0.1) \/ np.sqrt(self.n_visible_units)\n            self._activation_function_class = ReLUActivationFunction\n        else:\n            raise ValueError(\"Invalid activation function.\")\n\n        if self.optimization_algorithm == 'sgd':\n            self._stochastic_gradient_descent(X)\n        else:\n            raise ValueError(\"Invalid optimization algorithm.\")\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Transforms data using the fitted model.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        if len(X.shape) == 1:  # It is a single sample\n            return self._compute_hidden_units(X)\n        transformed_data = self._compute_hidden_units_matrix(X)\n        return transformed_data\n\n    def _reconstruct(self, transformed_data):\n        \"\"\"\n        Reconstruct visible units given the hidden layer output.\n        :param transformed_data: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        return self._compute_visible_units_matrix(transformed_data)\n\n    def _stochastic_gradient_descent(self, _data):\n        \"\"\"\n        Performs stochastic gradient descend optimization algorithm.\n        :param _data: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        accum_delta_W = np.zeros(self.W.shape)\n        accum_delta_b = np.zeros(self.b.shape)\n        accum_delta_c = np.zeros(self.c.shape)\n        for iteration in range(1, self.n_epochs + 1):\n            idx = np.random.permutation(len(_data))\n            data = _data[idx]\n            for batch in batch_generator(self.batch_size, data):\n                accum_delta_W[:] = .0\n                accum_delta_b[:] = .0\n                accum_delta_c[:] = .0\n                for sample in batch:\n                    delta_W, delta_b, delta_c = self._contrastive_divergence(sample)\n                    accum_delta_W += delta_W\n                    accum_delta_b += delta_b\n                    accum_delta_c += delta_c\n                self.W += self.learning_rate * (accum_delta_W \/ self.batch_size)\n                self.b += self.learning_rate * (accum_delta_b \/ self.batch_size)\n                self.c += self.learning_rate * (accum_delta_c \/ self.batch_size)\n            if self.verbose:\n                error = self._compute_reconstruction_error(data)\n                print(\">> Epoch %d finished \\tRBM Reconstruction error %f\" % (iteration, error))\n\n    def _contrastive_divergence(self, vector_visible_units):\n        \"\"\"\n        Computes gradients using Contrastive Divergence method.\n        :param vector_visible_units: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        v_0 = vector_visible_units\n        v_t = np.array(v_0)\n\n        # Sampling\n        for t in range(self.contrastive_divergence_iter):\n            h_t = self._sample_hidden_units(v_t)\n            v_t = self._compute_visible_units(h_t)\n\n        # Computing deltas\n        v_k = v_t\n        h_0 = self._compute_hidden_units(v_0)\n        h_k = self._compute_hidden_units(v_k)\n        delta_W = np.outer(h_0, v_0) - np.outer(h_k, v_k)\n        delta_b = v_0 - v_k\n        delta_c = h_0 - h_k\n\n        return delta_W, delta_b, delta_c\n\n    def _sample_hidden_units(self, vector_visible_units):\n        \"\"\"\n        Computes hidden unit activations by sampling from a binomial distribution.\n        :param vector_visible_units: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        hidden_units = self._compute_hidden_units(vector_visible_units)\n        return (np.random.random_sample(len(hidden_units)) < hidden_units).astype(np.int64)\n\n    def _sample_visible_units(self, vector_hidden_units):\n        \"\"\"\n        Computes visible unit activations by sampling from a binomial distribution.\n        :param vector_hidden_units: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        visible_units = self._compute_visible_units(vector_hidden_units)\n        return (np.random.random_sample(len(visible_units)) < visible_units).astype(np.int64)\n\n    def _compute_hidden_units(self, vector_visible_units):\n        \"\"\"\n        Computes hidden unit outputs.\n        :param vector_visible_units: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        v = np.expand_dims(vector_visible_units, 0)\n        h = np.squeeze(self._compute_hidden_units_matrix(v))\n        return np.array([h]) if not h.shape else h\n\n    def _compute_hidden_units_matrix(self, matrix_visible_units):\n        \"\"\"\n        Computes hidden unit outputs.\n        :param matrix_visible_units: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        return np.transpose(self._activation_function_class.function(\n            np.dot(self.W, np.transpose(matrix_visible_units)) + self.c[:, np.newaxis]))\n\n    def _compute_visible_units(self, vector_hidden_units):\n        \"\"\"\n        Computes visible (or input) unit outputs.\n        :param vector_hidden_units: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        h = np.expand_dims(vector_hidden_units, 0)\n        v = np.squeeze(self._compute_visible_units_matrix(h))\n        return np.array([v]) if not v.shape else v\n\n    def _compute_visible_units_matrix(self, matrix_hidden_units):\n        \"\"\"\n        Computes visible (or input) unit outputs.\n        :param matrix_hidden_units: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        return self._activation_function_class.function(np.dot(matrix_hidden_units, self.W) + self.b[np.newaxis, :])\n\n    def _compute_free_energy(self, vector_visible_units):\n        \"\"\"\n        Computes the RBM free energy.\n        :param vector_visible_units: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        v = vector_visible_units\n        return - np.dot(self.b, v) - np.sum(np.log(1 + np.exp(np.dot(self.W, v) + self.c)))\n\n    def _compute_reconstruction_error(self, data):\n        \"\"\"\n        Computes the reconstruction error of the data.\n        :param data: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        data_transformed = self.transform(data)\n        data_reconstructed = self._reconstruct(data_transformed)\n        return np.mean(np.sum((data_reconstructed - data) ** 2, 1))\n\n\nclass UnsupervisedDBN(BaseEstimator, TransformerMixin, BaseModel):\n    \"\"\"\n    This class implements a unsupervised Deep Belief Network.\n    \"\"\"\n\n    def __init__(self,\n                 hidden_layers_structure=[100, 100],\n                 activation_function='sigmoid',\n                 optimization_algorithm='sgd',\n                 learning_rate_rbm=1e-3,\n                 n_epochs_rbm=10,\n                 contrastive_divergence_iter=1,\n                 batch_size=32,\n                 verbose=True):\n        self.hidden_layers_structure = hidden_layers_structure\n        self.activation_function = activation_function\n        self.optimization_algorithm = optimization_algorithm\n        self.learning_rate_rbm = learning_rate_rbm\n        self.n_epochs_rbm = n_epochs_rbm\n        self.contrastive_divergence_iter = contrastive_divergence_iter\n        self.batch_size = batch_size\n        self.rbm_layers = None\n        self.verbose = verbose\n        self.rbm_class = BinaryRBM\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fits a model given data.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        # Initialize rbm layers\n        self.rbm_layers = list()\n        for n_hidden_units in self.hidden_layers_structure:\n            rbm = self.rbm_class(n_hidden_units=n_hidden_units,\n                                 activation_function=self.activation_function,\n                                 optimization_algorithm=self.optimization_algorithm,\n                                 learning_rate=self.learning_rate_rbm,\n                                 n_epochs=self.n_epochs_rbm,\n                                 contrastive_divergence_iter=self.contrastive_divergence_iter,\n                                 batch_size=self.batch_size,\n                                 verbose=self.verbose)\n            self.rbm_layers.append(rbm)\n\n        # Fit RBM\n        if self.verbose:\n            print(\"[START] Pre-training step:\")\n        input_data = X\n        for rbm in self.rbm_layers:\n            rbm.fit(input_data)\n            input_data = rbm.transform(input_data)\n        if self.verbose:\n            print(\"[END] Pre-training step\")\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Transforms data using the fitted model.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        input_data = X\n        for rbm in self.rbm_layers:\n            input_data = rbm.transform(input_data)\n        return input_data\n\n\nclass AbstractSupervisedDBN(BaseEstimator, BaseModel):\n    \"\"\"\n    Abstract class for supervised Deep Belief Network.\n    \"\"\"\n    __metaclass__ = ABCMeta\n\n    def __init__(self,\n                 unsupervised_dbn_class,\n                 hidden_layers_structure=[100, 100],\n                 activation_function='sigmoid',\n                 optimization_algorithm='sgd',\n                 learning_rate=1e-3,\n                 learning_rate_rbm=1e-3,\n                 n_iter_backprop=100,\n                 l2_regularization=1.0,\n                 n_epochs_rbm=10,\n                 contrastive_divergence_iter=1,\n                 batch_size=32,\n                 dropout_p=0,  # float between 0 and 1. Fraction of the input units to drop\n                 verbose=True):\n        self.unsupervised_dbn = unsupervised_dbn_class(hidden_layers_structure=hidden_layers_structure,\n                                                       activation_function=activation_function,\n                                                       optimization_algorithm=optimization_algorithm,\n                                                       learning_rate_rbm=learning_rate_rbm,\n                                                       n_epochs_rbm=n_epochs_rbm,\n                                                       contrastive_divergence_iter=contrastive_divergence_iter,\n                                                       batch_size=batch_size,\n                                                       verbose=verbose)\n        self.unsupervised_dbn_class = unsupervised_dbn_class\n        self.n_iter_backprop = n_iter_backprop\n        self.l2_regularization = l2_regularization\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.dropout_p = dropout_p\n        self.p = 1 - self.dropout_p\n        self.verbose = verbose\n\n    def fit(self, X, y=None, pre_train=True):\n        \"\"\"\n        Fits a model given data.\n        :param X: array-like, shape = (n_samples, n_features)\n        :param y : array-like, shape = (n_samples, )\n        :param pre_train: bool\n        :return:\n        \"\"\"\n        if pre_train:\n            self.pre_train(X)\n        self._fine_tuning(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target given data.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        if len(X.shape) == 1:  # It is a single sample\n            X = np.expand_dims(X, 0)\n        transformed_data = self.transform(X)\n        predicted_data = self._compute_output_units_matrix(transformed_data)\n        return predicted_data\n\n    def pre_train(self, X):\n        \"\"\"\n        Apply unsupervised network pre-training.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        self.unsupervised_dbn.fit(X)\n        return self\n\n    def transform(self, *args):\n        return self.unsupervised_dbn.transform(*args)\n\n    @abstractmethod\n    def _transform_labels_to_network_format(self, labels):\n        return\n\n    @abstractmethod\n    def _compute_output_units_matrix(self, matrix_visible_units):\n        return\n\n    @abstractmethod\n    def _determine_num_output_neurons(self, labels):\n        return\n\n    @abstractmethod\n    def _stochastic_gradient_descent(self, data, labels):\n        return\n\n    @abstractmethod\n    def _fine_tuning(self, data, _labels):\n        return\n\n\nclass NumPyAbstractSupervisedDBN(AbstractSupervisedDBN):\n    \"\"\"\n    Abstract class for supervised Deep Belief Network in NumPy\n    \"\"\"\n    __metaclass__ = ABCMeta\n\n    def __init__(self, **kwargs):\n        super(NumPyAbstractSupervisedDBN, self).__init__(UnsupervisedDBN, **kwargs)\n\n    def _compute_activations(self, sample):\n        \"\"\"\n        Compute output values of all layers.\n        :param sample: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        input_data = sample\n        if self.dropout_p > 0:\n            r = np.random.binomial(1, self.p, len(input_data))\n            input_data *= r\n        layers_activation = list()\n\n        for rbm in self.unsupervised_dbn.rbm_layers:\n            input_data = rbm.transform(input_data)\n            if self.dropout_p > 0:\n                r = np.random.binomial(1, self.p, len(input_data))\n                input_data *= r\n            layers_activation.append(input_data)\n\n        # Computing activation of output layer\n        input_data = self._compute_output_units(input_data)\n        layers_activation.append(input_data)\n\n        return layers_activation\n\n    def _stochastic_gradient_descent(self, _data, _labels):\n        \"\"\"\n        Performs stochastic gradient descend optimization algorithm.\n        :param _data: array-like, shape = (n_samples, n_features)\n        :param _labels: array-like, shape = (n_samples, targets)\n        :return:\n        \"\"\"\n        if self.verbose:\n            matrix_error = np.zeros([len(_data), self.num_classes])\n        num_samples = len(_data)\n        accum_delta_W = [np.zeros(rbm.W.shape) for rbm in self.unsupervised_dbn.rbm_layers]\n        accum_delta_W.append(np.zeros(self.W.shape))\n        accum_delta_bias = [np.zeros(rbm.c.shape) for rbm in self.unsupervised_dbn.rbm_layers]\n        accum_delta_bias.append(np.zeros(self.b.shape))\n\n        for iteration in range(1, self.n_iter_backprop + 1):\n            idx = np.random.permutation(len(_data))\n            data = _data[idx]\n            labels = _labels[idx]\n            i = 0\n            for batch_data, batch_labels in batch_generator(self.batch_size, data, labels):\n                # Clear arrays\n                for arr1, arr2 in zip(accum_delta_W, accum_delta_bias):\n                    arr1[:], arr2[:] = .0, .0\n                for sample, label in zip(batch_data, batch_labels):\n                    delta_W, delta_bias, predicted = self._backpropagation(sample, label)\n                    for layer in range(len(self.unsupervised_dbn.rbm_layers) + 1):\n                        accum_delta_W[layer] += delta_W[layer]\n                        accum_delta_bias[layer] += delta_bias[layer]\n                    if self.verbose:\n                        loss = self._compute_loss(predicted, label)\n                        matrix_error[i, :] = loss\n                        i += 1\n\n                layer = 0\n                for rbm in self.unsupervised_dbn.rbm_layers:\n                    # Updating parameters of hidden layers\n                    rbm.W = (1 - (\n                        self.learning_rate * self.l2_regularization) \/ num_samples) * rbm.W - self.learning_rate * (\n                        accum_delta_W[layer] \/ self.batch_size)\n                    rbm.c -= self.learning_rate * (accum_delta_bias[layer] \/ self.batch_size)\n                    layer += 1\n                # Updating parameters of output layer\n                self.W = (1 - (\n                    self.learning_rate * self.l2_regularization) \/ num_samples) * self.W - self.learning_rate * (\n                    accum_delta_W[layer] \/ self.batch_size)\n                self.b -= self.learning_rate * (accum_delta_bias[layer] \/ self.batch_size)\n\n            if self.verbose:\n                error = np.mean(np.sum(matrix_error, 1))\n                print(\">> Epoch %d finished \\tANN training loss %f\" % (iteration, error))\n\n    def _backpropagation(self, input_vector, label):\n        \"\"\"\n        Performs Backpropagation algorithm for computing gradients.\n        :param input_vector: array-like, shape = (n_features, )\n        :param label: array-like, shape = (n_targets, )\n        :return:\n        \"\"\"\n        x, y = input_vector, label\n        deltas = list()\n        list_layer_weights = list()\n        for rbm in self.unsupervised_dbn.rbm_layers:\n            list_layer_weights.append(rbm.W)\n        list_layer_weights.append(self.W)\n\n        # Forward pass\n        layers_activation = self._compute_activations(input_vector)\n\n        # Backward pass: computing deltas\n        activation_output_layer = layers_activation[-1]\n        delta_output_layer = self._compute_output_layer_delta(y, activation_output_layer)\n        deltas.append(delta_output_layer)\n        layer_idx = list(range(len(self.unsupervised_dbn.rbm_layers)))\n        layer_idx.reverse()\n        delta_previous_layer = delta_output_layer\n        for layer in layer_idx:\n            neuron_activations = layers_activation[layer]\n            W = list_layer_weights[layer + 1]\n            delta = np.dot(delta_previous_layer, W) * self.unsupervised_dbn.rbm_layers[\n                layer]._activation_function_class.prime(neuron_activations)\n            deltas.append(delta)\n            delta_previous_layer = delta\n        deltas.reverse()\n\n        # Computing gradients\n        layers_activation.pop()\n        layers_activation.insert(0, input_vector)\n        layer_gradient_weights, layer_gradient_bias = list(), list()\n        for layer in range(len(list_layer_weights)):\n            neuron_activations = layers_activation[layer]\n            delta = deltas[layer]\n            gradient_W = np.outer(delta, neuron_activations)\n            layer_gradient_weights.append(gradient_W)\n            layer_gradient_bias.append(delta)\n\n        return layer_gradient_weights, layer_gradient_bias, activation_output_layer\n\n    def _fine_tuning(self, data, _labels):\n        \"\"\"\n        Entry point of the fine tuning procedure.\n        :param data: array-like, shape = (n_samples, n_features)\n        :param _labels: array-like, shape = (n_samples, targets)\n        :return:\n        \"\"\"\n        self.num_classes = self._determine_num_output_neurons(_labels)\n        n_hidden_units_previous_layer = self.unsupervised_dbn.rbm_layers[-1].n_hidden_units\n        self.W = np.random.randn(self.num_classes, n_hidden_units_previous_layer) \/ np.sqrt(\n            n_hidden_units_previous_layer)\n        self.b = np.random.randn(self.num_classes) \/ np.sqrt(n_hidden_units_previous_layer)\n\n        labels = self._transform_labels_to_network_format(_labels)\n\n        # Scaling up weights obtained from pretraining\n        for rbm in self.unsupervised_dbn.rbm_layers:\n            rbm.W \/= self.p\n            rbm.c \/= self.p\n\n        if self.verbose:\n            print(\"[START] Fine tuning step:\")\n\n        if self.unsupervised_dbn.optimization_algorithm == 'sgd':\n            self._stochastic_gradient_descent(data, labels)\n        else:\n            raise ValueError(\"Invalid optimization algorithm.\")\n\n        # Scaling down weights obtained from pretraining\n        for rbm in self.unsupervised_dbn.rbm_layers:\n            rbm.W *= self.p\n            rbm.c *= self.p\n\n        if self.verbose:\n            print(\"[END] Fine tuning step\")\n\n    @abstractmethod\n    def _compute_loss(self, predicted, label):\n        return\n\n    @abstractmethod\n    def _compute_output_layer_delta(self, label, predicted):\n        return\n\n\nclass SupervisedDBNClassification(NumPyAbstractSupervisedDBN, ClassifierMixin):\n    \"\"\"\n    This class implements a Deep Belief Network for classification problems.\n    It appends a Softmax Linear Classifier as output layer.\n    \"\"\"\n\n    def _transform_labels_to_network_format(self, labels):\n        \"\"\"\n        Converts labels as single integer to row vectors. For instance, given a three class problem, labels would be\n        mapped as label_1: [1 0 0], label_2: [0 1 0], label_3: [0, 0, 1] where labels can be either int or string.\n        :param labels: array-like, shape = (n_samples, )\n        :return:\n        \"\"\"\n        new_labels = np.zeros([len(labels), self.num_classes])\n        self.label_to_idx_map, self.idx_to_label_map = dict(), dict()\n        idx = 0\n        for i, label in enumerate(labels):\n            if label not in self.label_to_idx_map:\n                self.label_to_idx_map[label] = idx\n                self.idx_to_label_map[idx] = label\n                idx += 1\n            new_labels[i][self.label_to_idx_map[label]] = 1\n        return new_labels\n\n    def _transform_network_format_to_labels(self, indexes):\n        \"\"\"\n        Converts network output to original labels.\n        :param indexes: array-like, shape = (n_samples, )\n        :return:\n        \"\"\"\n        return list(map(lambda idx: self.idx_to_label_map[idx], indexes))\n\n    def _compute_output_units(self, vector_visible_units):\n        \"\"\"\n        Compute activations of output units.\n        :param vector_visible_units: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        v = vector_visible_units\n        scores = np.dot(self.W, v) + self.b\n        # get unnormalized probabilities\n        exp_scores = np.exp(scores)\n        # normalize them for each example\n        return exp_scores \/ np.sum(exp_scores)\n\n    def _compute_output_units_matrix(self, matrix_visible_units):\n        \"\"\"\n        Compute activations of output units.\n        :param matrix_visible_units: shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        matrix_scores = np.transpose(np.dot(self.W, np.transpose(matrix_visible_units)) + self.b[:, np.newaxis])\n        exp_scores = np.exp(matrix_scores)\n        return exp_scores \/ np.expand_dims(np.sum(exp_scores, axis=1), 1)\n\n    def _compute_output_layer_delta(self, label, predicted):\n        \"\"\"\n        Compute deltas of the output layer, using cross-entropy cost function.\n        :param label: array-like, shape = (n_features, )\n        :param predicted: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        dscores = np.array(predicted)\n        dscores[np.where(label == 1)] -= 1\n        return dscores\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predicts probability distribution of classes for each sample in the given data.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        return super(SupervisedDBNClassification, self).predict(X)\n\n    def predict_proba_dict(self, X):\n        \"\"\"\n        Predicts probability distribution of classes for each sample in the given data.\n        Returns a list of dictionaries, one per sample. Each dict contains {label_1: prob_1, ..., label_j: prob_j}\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        if len(X.shape) == 1:  # It is a single sample\n            X = np.expand_dims(X, 0)\n\n        predicted_probs = self.predict_proba(X)\n\n        result = []\n        num_of_data, num_of_labels = predicted_probs.shape\n        for i in range(num_of_data):\n            # key : label\n            # value : predicted probability\n            dict_prob = {}\n            for j in range(num_of_labels):\n                dict_prob[self.idx_to_label_map[j]] = predicted_probs[i][j]\n            result.append(dict_prob)\n\n        return result\n\n    def predict(self, X):\n        probs = self.predict_proba(X)\n        indexes = np.argmax(probs, axis=1)\n        return self._transform_network_format_to_labels(indexes)\n\n    def _determine_num_output_neurons(self, labels):\n        \"\"\"\n        Given labels, compute the needed number of output units.\n        :param labels: shape = (n_samples, )\n        :return:\n        \"\"\"\n        return len(np.unique(labels))\n\n    def _compute_loss(self, probs, label):\n        \"\"\"\n        Computes categorical cross-entropy loss\n        :param probs:\n        :param label:\n        :return:\n        \"\"\"\n        return -np.log(probs[np.where(label == 1)])\n\n\nclass SupervisedDBNRegression(NumPyAbstractSupervisedDBN, RegressorMixin):\n    \"\"\"\n    This class implements a Deep Belief Network for regression problems.\n    \"\"\"\n\n    def _transform_labels_to_network_format(self, labels):\n        \"\"\"\n        Returns the same labels since regression case does not need to convert anything.\n        :param labels: array-like, shape = (n_samples, targets)\n        :return:\n        \"\"\"\n        return labels\n\n    def _compute_output_units(self, vector_visible_units):\n        \"\"\"\n        Compute activations of output units.\n        :param vector_visible_units: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        v = vector_visible_units\n        return np.dot(self.W, v) + self.b\n\n    def _compute_output_units_matrix(self, matrix_visible_units):\n        \"\"\"\n        Compute activations of output units.\n        :param matrix_visible_units: shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        return np.transpose(np.dot(self.W, np.transpose(matrix_visible_units)) + self.b[:, np.newaxis])\n\n    def _compute_output_layer_delta(self, label, predicted):\n        \"\"\"\n        Compute deltas of the output layer for the regression case, using common (one-half) squared-error cost function.\n        :param label: array-like, shape = (n_features, )\n        :param predicted: array-like, shape = (n_features, )\n        :return:\n        \"\"\"\n        return -(label - predicted)\n\n    def _determine_num_output_neurons(self, labels):\n        \"\"\"\n        Given labels, compute the needed number of output units.\n        :param labels: shape = (n_samples, n_targets)\n        :return:\n        \"\"\"\n        if len(labels.shape) == 1:\n            return 1\n        else:\n            return labels.shape[1]\n\n    def _compute_loss(self, predicted, label):\n        \"\"\"\n        Computes Mean squared error loss.\n        :param predicted:\n        :param label:\n        :return:\n        \"\"\"\n        error = predicted - label\n        return error * error\n","01733d60":"# %%\nfrom __main__ import *\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import clear_output\nfrom datetime import datetime\nfrom datetime import timedelta\nimport numpy as np\n\ndef ETC(start, steps, numberEpochs):\n    displayStatsInfo=1\n    time_elapsed = datetime.now() - start\n    eta= (numberEpochs-steps) \/ displayStatsInfo * time_elapsed\n    #avgString = str(avg).split(\".\")[0]\n    \n    hours= int(eta.seconds\/3600)\n    minutes= int((eta.seconds\/60)-hours*60)\n    seconds = int(eta.seconds - minutes*60 -hours*3600)\n    return \"%sh, %s min and %s sec\" % (hours, minutes, seconds)\n\ndef elapsedTime(elapsed):\n    hours= int(elapsed.seconds\/3600)\n    minutes= int((elapsed.seconds\/60)-hours*60)\n    seconds = int(elapsed.seconds - minutes*60 -hours*3600)\n    return \"%sh, %s min and %s sec\" % (hours, minutes, seconds)\n\ndef progress(percent=0, width=30):\n    left = width * percent \/\/ 100\n    right = width - left\n    print('\\r[', '#' * left, ' ' * right, ']',\n          f' {percent:.0f}%\\n',\n          sep='', end='', flush=True)\n\ndef live_plot(resultsRaw, numberEpochs, figsize=(7,5), title=''):\n    clear_output(wait=True)\n    plt.figure(figsize=figsize)\n    plt.xlim(0, numberEpochs)\n    plt.ylim(0, max(resultsRaw))\n\n    m=0\n    if len(steps) > 1:\n        plt.scatter(steps,resultsRaw, label='loss', color='k') \n        m, b = np.polyfit(steps, accuracy, 1)\n        plt.plot(steps, [x * m for x in steps] + b)\n\n    plt.title(title)\n    plt.grid(True)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    #plt.legend(loc='center left') # the plot evolves to the right\n    plt.show();\n    return m\n\ndef calcStats(resultsRaw,numberEpochs, totalStartTime): \n    # step =-1 plots previous data state        \n\n    totalTime =datetime.now()- totalStartTime\n    step=len(resultsRaw)\n    trendSlope= live_plot(resultsRaw, numberEpochs)\n    \n    #TODO: step=0 epochs=0\n    if int(step\/(numberEpochs+1)*100)<=100:\n        print(\"Running...\")\n    progress(int((step+1)\/(numberEpochs+1)*100)) \n    print(\"\")\n    print(\"================= Iteration Stats ================\")\n    print(\"                   step: %i of %i\" % (step,numberEpochs))\n    print(\"     iteration loss: %.1f %%\" % (resultsRaw[len(resultsRaw)-1]))\n    print(\"\")\n    print(\"================= Time           ================\")\n    print(\"                Elapsed: \" + elapsedTime(totalTime))\n    print(\"                    ETC: \" + ETC(totalStartTime,step,numberEpochs))\n    print(\"\")\n    print(\"            Trend slope: %.3f\" % (trendSlope))\n\n    \ndef printResults(outputResultsBuffer):\n    for epoch in range(len(outputResultsBuffer)):\n        print(\"Epoch: %d     Accuracy: %.2f %%   Loss: %.2f\"  % (epoch, outputResultsBuffer[epoch]['output_p_accuracy'][0]*100, outputResultsBuffer[epoch]['loss'][0]))","6ca4a296":"import numpy as np\n\n\ndef batch_generator(batch_size, data, labels=None):\n    \"\"\"\n    Generates batches of samples\n    :param data: array-like, shape = (n_samples, n_features)\n    :param labels: array-like, shape = (n_samples, )\n    :return:\n    \"\"\"\n    n_batches = int(np.ceil(len(data) \/ float(batch_size)))\n    idx = np.random.permutation(len(data))\n    data_shuffled = data[idx]\n    if labels is not None:\n        labels_shuffled = labels[idx]\n    for i in range(n_batches):\n        start = i * batch_size\n        end = start + batch_size\n        if labels is not None:\n            yield data_shuffled[start:end, :], labels_shuffled[start:end]\n        else:\n            yield data_shuffled[start:end, :]\n\n\ndef to_categorical(labels, num_classes):\n    \"\"\"\n    Converts labels as single integer to row vectors. For instance, given a three class problem, labels would be\n    mapped as label_1: [1 0 0], label_2: [0 1 0], label_3: [0, 0, 1] where labels can be either int or string.\n    :param labels: array-like, shape = (n_samples, )\n    :return:\n    \"\"\"\n    new_labels = np.zeros([len(labels), num_classes])\n    label_to_idx_map, idx_to_label_map = dict(), dict()\n    idx = 0\n    for i, label in enumerate(labels):\n        if label not in label_to_idx_map:\n            label_to_idx_map[label] = idx\n            idx_to_label_map[idx] = label\n            idx += 1\n        new_labels[i][label_to_idx_map[label]] = 1\n    return new_labels, label_to_idx_map, idx_to_label_map\n","7a9e1aac":"import atexit\nfrom abc import ABCMeta\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nfrom sklearn.base import ClassifierMixin, RegressorMixin\nfrom models import BinaryRBM, UnsupervisedDBN, SupervisedDBNRegression, SupervisedDBNClassification\n\nfrom models import AbstractSupervisedDBN as BaseAbstractSupervisedDBN\nfrom models import BaseModel\nfrom models import BinaryRBM as BaseBinaryRBM\nfrom models import UnsupervisedDBN as BaseUnsupervisedDBN\nfrom utils import batch_generator\nfrom utils import to_categorical\n\n\ndef close_session():\n    sess.close()\n\n\nsess = tf.Session()\natexit.register(close_session)\n\n\ndef weight_variable(func, shape, stddev, dtype=tf.float32):\n    initial = func(shape, stddev=stddev, dtype=dtype)\n    return tf.Variable(initial)\n\n\ndef bias_variable(value, shape, dtype=tf.float32):\n    initial = tf.constant(value, shape=shape, dtype=dtype)\n    return tf.Variable(initial)\n\n\nclass BaseTensorFlowModel(BaseModel):\n    def save(self, save_path):\n        import pickle\n\n        with open(save_path, 'wb') as fp:\n            pickle.dump(self.to_dict(), fp)\n\n    @classmethod\n    def load(cls, load_path):\n        import pickle\n\n        with open(load_path, 'rb') as fp:\n            dct_to_load = pickle.load(fp)\n            return cls.from_dict(dct_to_load)\n\n    def to_dict(self):\n        dct_to_save = {name: self.__getattribute__(name) for name in self._get_param_names()}\n        dct_to_save.update(\n            {name: self.__getattribute__(name).eval(sess) for name in self._get_weight_variables_names()})\n        return dct_to_save\n\n    @classmethod\n    def from_dict(cls, dct_to_load):\n        pass\n\n    def _build_model(self, weights=None):\n        pass\n\n    def _initialize_weights(self, weights):\n        pass\n\n    @classmethod\n    def _get_weight_variables_names(cls):\n        pass\n\n    @classmethod\n    def _get_param_names(cls):\n        pass\n\n\nclass BinaryRBM(BaseBinaryRBM, BaseTensorFlowModel):\n    \"\"\"\n    This class implements a Binary Restricted Boltzmann machine based on TensorFlow.\n    \"\"\"\n\n    def fit(self, X):\n        \"\"\"\n        Fit a model given data.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        self.n_visible_units = X.shape[1]\n\n        # Initialize RBM parameters\n        self._build_model()\n\n        sess.run(tf.variables_initializer([self.W, self.c, self.b]))\n\n        if self.optimization_algorithm == 'sgd':\n            self._stochastic_gradient_descent(X)\n        else:\n            raise ValueError(\"Invalid optimization algorithm.\")\n        return\n\n    @classmethod\n    def _get_weight_variables_names(cls):\n        return ['W', 'c', 'b']\n\n    @classmethod\n    def _get_param_names(cls):\n        return ['n_hidden_units',\n                'n_visible_units',\n                'activation_function',\n                'optimization_algorithm',\n                'learning_rate',\n                'n_epochs',\n                'contrastive_divergence_iter',\n                'batch_size',\n                'verbose',\n                '_activation_function_class']\n\n    def _initialize_weights(self, weights):\n        if weights:\n            for attr_name, value in weights.items():\n                self.__setattr__(attr_name, tf.Variable(value))\n        else:\n            if self.activation_function == 'sigmoid':\n                stddev = 1.0 \/ np.sqrt(self.n_visible_units)\n                self.W = weight_variable(tf.random_normal, [self.n_hidden_units, self.n_visible_units], stddev)\n                self.c = weight_variable(tf.random_normal, [self.n_hidden_units], stddev)\n                self.b = weight_variable(tf.random_normal, [self.n_visible_units], stddev)\n                self._activation_function_class = tf.nn.sigmoid\n            elif self.activation_function == 'relu':\n                stddev = 0.1 \/ np.sqrt(self.n_visible_units)\n                self.W = weight_variable(tf.truncated_normal, [self.n_hidden_units, self.n_visible_units], stddev)\n                self.c = bias_variable(stddev, [self.n_hidden_units])\n                self.b = bias_variable(stddev, [self.n_visible_units])\n                self._activation_function_class = tf.nn.relu\n            else:\n                raise ValueError(\"Invalid activation function.\")\n\n    def _build_model(self, weights=None):\n        \"\"\"\n        Builds TensorFlow model.\n        :return:\n        \"\"\"\n        # initialize weights and biases\n        self._initialize_weights(weights)\n\n        # TensorFlow operations\n        self.visible_units_placeholder = tf.placeholder(tf.float32, shape=[None, self.n_visible_units])\n        self.compute_hidden_units_op = self._activation_function_class(\n            tf.transpose(tf.matmul(self.W, tf.transpose(self.visible_units_placeholder))) + self.c)\n        self.hidden_units_placeholder = tf.placeholder(tf.float32, shape=[None, self.n_hidden_units])\n        self.compute_visible_units_op = self._activation_function_class(\n            tf.matmul(self.hidden_units_placeholder, self.W) + self.b)\n        self.random_uniform_values = tf.Variable(tf.random_uniform([self.batch_size, self.n_hidden_units]))\n        sample_hidden_units_op = tf.to_float(self.random_uniform_values < self.compute_hidden_units_op)\n        self.random_variables = [self.random_uniform_values]\n\n        # Positive gradient\n        # Outer product. N is the batch size length.\n        # From http:\/\/stackoverflow.com\/questions\/35213787\/tensorflow-batch-outer-product\n        positive_gradient_op = tf.matmul(tf.expand_dims(sample_hidden_units_op, 2),  # [N, U, 1]\n                                         tf.expand_dims(self.visible_units_placeholder, 1))  # [N, 1, V]\n\n        # Negative gradient\n        # Gibbs sampling\n        sample_hidden_units_gibbs_step_op = sample_hidden_units_op\n        for t in range(self.contrastive_divergence_iter):\n            compute_visible_units_op = self._activation_function_class(\n                tf.matmul(sample_hidden_units_gibbs_step_op, self.W) + self.b)\n            compute_hidden_units_gibbs_step_op = self._activation_function_class(\n                tf.transpose(tf.matmul(self.W, tf.transpose(compute_visible_units_op))) + self.c)\n            random_uniform_values = tf.Variable(tf.random_uniform([self.batch_size, self.n_hidden_units]))\n            sample_hidden_units_gibbs_step_op = tf.to_float(random_uniform_values < compute_hidden_units_gibbs_step_op)\n            self.random_variables.append(random_uniform_values)\n\n        negative_gradient_op = tf.matmul(tf.expand_dims(sample_hidden_units_gibbs_step_op, 2),  # [N, U, 1]\n                                         tf.expand_dims(compute_visible_units_op, 1))  # [N, 1, V]\n\n        compute_delta_W = tf.reduce_mean(positive_gradient_op - negative_gradient_op, 0)\n        compute_delta_b = tf.reduce_mean(self.visible_units_placeholder - compute_visible_units_op, 0)\n        compute_delta_c = tf.reduce_mean(sample_hidden_units_op - sample_hidden_units_gibbs_step_op, 0)\n\n        self.update_W = tf.assign_add(self.W, self.learning_rate * compute_delta_W)\n        self.update_b = tf.assign_add(self.b, self.learning_rate * compute_delta_b)\n        self.update_c = tf.assign_add(self.c, self.learning_rate * compute_delta_c)\n\n    @classmethod\n    def from_dict(cls, dct_to_load):\n        weights = {var_name: dct_to_load.pop(var_name) for var_name in cls._get_weight_variables_names()}\n\n        _activation_function_class = dct_to_load.pop('_activation_function_class')\n        n_visible_units = dct_to_load.pop('n_visible_units')\n\n        instance = cls(**dct_to_load)\n        setattr(instance, '_activation_function_class', _activation_function_class)\n        setattr(instance, 'n_visible_units', n_visible_units)\n\n        # Initialize RBM parameters\n        instance._build_model(weights)\n        sess.run(tf.variables_initializer([getattr(instance, name) for name in cls._get_weight_variables_names()]))\n\n        return instance\n\n    def _stochastic_gradient_descent(self, _data):\n        \"\"\"\n        Performs stochastic gradient descend optimization algorithm.\n        :param _data: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        for iteration in range(1, self.n_epochs + 1):\n            idx = np.random.permutation(len(_data))\n            data = _data[idx]\n            for batch in batch_generator(self.batch_size, data):\n                if len(batch) < self.batch_size:\n                    # Pad with zeros\n                    pad = np.zeros((self.batch_size - batch.shape[0], batch.shape[1]), dtype=batch.dtype)\n                    batch = np.vstack((batch, pad))\n                sess.run(tf.variables_initializer(self.random_variables))  # Need to re-sample from uniform distribution\n                sess.run([self.update_W, self.update_b, self.update_c],\n                         feed_dict={self.visible_units_placeholder: batch})\n            if self.verbose:\n                error = self._compute_reconstruction_error(data)\n                print(\">> Epoch %d finished \\tRBM Reconstruction error %f\" % (iteration, error))\n\n    def _compute_hidden_units_matrix(self, matrix_visible_units):\n        \"\"\"\n        Computes hidden unit outputs.\n        :param matrix_visible_units: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        return sess.run(self.compute_hidden_units_op,\n                        feed_dict={self.visible_units_placeholder: matrix_visible_units})\n\n    def _compute_visible_units_matrix(self, matrix_hidden_units):\n        \"\"\"\n        Computes visible (or input) unit outputs.\n        :param matrix_hidden_units: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        return sess.run(self.compute_visible_units_op,\n                        feed_dict={self.hidden_units_placeholder: matrix_hidden_units})\n\n\nclass UnsupervisedDBN(BaseUnsupervisedDBN, BaseTensorFlowModel):\n    \"\"\"\n    This class implements a unsupervised Deep Belief Network in TensorFlow\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super(UnsupervisedDBN, self).__init__(**kwargs)\n        self.rbm_class = BinaryRBM\n\n    @classmethod\n    def _get_param_names(cls):\n        return ['hidden_layers_structure',\n                'activation_function',\n                'optimization_algorithm',\n                'learning_rate_rbm',\n                'n_epochs_rbm',\n                'contrastive_divergence_iter',\n                'batch_size',\n                'verbose']\n\n    @classmethod\n    def _get_weight_variables_names(cls):\n        return []\n\n    def to_dict(self):\n        dct_to_save = super(UnsupervisedDBN, self).to_dict()\n        dct_to_save['rbm_layers'] = [rbm.to_dict() for rbm in self.rbm_layers]\n        return dct_to_save\n\n    @classmethod\n    def from_dict(cls, dct_to_load):\n        rbm_layers = dct_to_load.pop('rbm_layers')\n        instance = cls(**dct_to_load)\n        setattr(instance, 'rbm_layers', [instance.rbm_class.from_dict(rbm) for rbm in rbm_layers])\n        return instance\n\n\nclass TensorFlowAbstractSupervisedDBN(BaseAbstractSupervisedDBN, BaseTensorFlowModel):\n    __metaclass__ = ABCMeta\n\n    def __init__(self, **kwargs):\n        super(TensorFlowAbstractSupervisedDBN, self).__init__(UnsupervisedDBN, **kwargs)\n\n    @classmethod\n    def _get_param_names(cls):\n        return ['n_iter_backprop',\n                'l2_regularization',\n                'learning_rate',\n                'batch_size',\n                'dropout_p',\n                'verbose']\n\n    @classmethod\n    def _get_weight_variables_names(cls):\n        return ['W', 'b']\n\n    def _initialize_weights(self, weights):\n        if weights:\n            for attr_name, value in weights.items():\n                self.__setattr__(attr_name, tf.Variable(value))\n        else:\n            if self.unsupervised_dbn.activation_function == 'sigmoid':\n                stddev = 1.0 \/ np.sqrt(self.input_units)\n                self.W = weight_variable(tf.random_normal, [self.input_units, self.num_classes], stddev)\n                self.b = weight_variable(tf.random_normal, [self.num_classes], stddev)\n                self._activation_function_class = tf.nn.sigmoid\n            elif self.unsupervised_dbn.activation_function == 'relu':\n                stddev = 0.1 \/ np.sqrt(self.input_units)\n                self.W = weight_variable(tf.truncated_normal, [self.input_units, self.num_classes], stddev)\n                self.b = bias_variable(stddev, [self.num_classes])\n                self._activation_function_class = tf.nn.relu\n            else:\n                raise ValueError(\"Invalid activation function.\")\n\n    def to_dict(self):\n        dct_to_save = super(TensorFlowAbstractSupervisedDBN, self).to_dict()\n        dct_to_save['unsupervised_dbn'] = self.unsupervised_dbn.to_dict()\n        dct_to_save['num_classes'] = self.num_classes\n        return dct_to_save\n\n    @classmethod\n    def from_dict(cls, dct_to_load):\n        weights = {var_name: dct_to_load.pop(var_name) for var_name in cls._get_weight_variables_names()}\n        unsupervised_dbn_dct = dct_to_load.pop('unsupervised_dbn')\n        num_classes = dct_to_load.pop('num_classes')\n\n        instance = cls(**dct_to_load)\n\n        setattr(instance, 'unsupervised_dbn', instance.unsupervised_dbn_class.from_dict(unsupervised_dbn_dct))\n        setattr(instance, 'num_classes', num_classes)\n\n        # Initialize RBM parameters\n        instance._build_model(weights)\n        sess.run(tf.variables_initializer([getattr(instance, name) for name in cls._get_weight_variables_names()]))\n        return instance\n\n    def _build_model(self, weights=None):\n        self.visible_units_placeholder = self.unsupervised_dbn.rbm_layers[0].visible_units_placeholder\n        keep_prob = tf.placeholder(tf.float32)\n        visible_units_placeholder_drop = tf.nn.dropout(self.visible_units_placeholder, keep_prob)\n        self.keep_prob_placeholders = [keep_prob]\n\n        # Define tensorflow operation for a forward pass\n        rbm_activation = visible_units_placeholder_drop\n        for rbm in self.unsupervised_dbn.rbm_layers:\n            rbm_activation = rbm._activation_function_class(\n                tf.transpose(tf.matmul(rbm.W, tf.transpose(rbm_activation))) + rbm.c)\n            keep_prob = tf.placeholder(tf.float32)\n            self.keep_prob_placeholders.append(keep_prob)\n            rbm_activation = tf.nn.dropout(rbm_activation, keep_prob)\n\n        self.transform_op = rbm_activation\n        self.input_units = self.unsupervised_dbn.rbm_layers[-1].n_hidden_units\n\n        # weights and biases\n        self._initialize_weights(weights)\n\n        if self.unsupervised_dbn.optimization_algorithm == 'sgd':\n            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n        else:\n            raise ValueError(\"Invalid optimization algorithm.\")\n\n        # operations\n        self.y = tf.matmul(self.transform_op, self.W) + self.b\n        self.y_ = tf.placeholder(tf.float32, shape=[None, self.num_classes])\n        self.train_step = None\n        self.cost_function = None\n        self.output = None\n\n    def _fine_tuning(self, data, _labels):\n        self.num_classes = self._determine_num_output_neurons(_labels)\n        if self.num_classes == 1:\n            _labels = np.expand_dims(_labels, -1)\n\n        self._build_model()\n        sess.run(tf.variables_initializer([self.W, self.b]))\n\n        labels = self._transform_labels_to_network_format(_labels)\n\n        if self.verbose:\n            print(\"[START] Fine tuning step:\")\n        self._stochastic_gradient_descent(data, labels)\n        if self.verbose:\n            print(\"[END] Fine tuning step\")\n\n    def _stochastic_gradient_descent(self, data, labels):\n        for iteration in range(self.n_iter_backprop):\n            for batch_data, batch_labels in batch_generator(self.batch_size, data, labels):\n                feed_dict = {self.visible_units_placeholder: batch_data,\n                             self.y_: batch_labels}\n                feed_dict.update({placeholder: self.p for placeholder in self.keep_prob_placeholders})\n                sess.run(self.train_step, feed_dict=feed_dict)\n\n            if self.verbose:\n                feed_dict = {self.visible_units_placeholder: data, self.y_: labels}\n                feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n                error = sess.run(self.cost_function, feed_dict=feed_dict)\n                print(\">> Epoch %d finished \\tANN training loss %f\" % (iteration, error))\n\n    def transform(self, X):\n        feed_dict = {self.visible_units_placeholder: X}\n        feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n        return sess.run(self.transform_op,\n                        feed_dict=feed_dict)\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target given data.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        if len(X.shape) == 1:  # It is a single sample\n            X = np.expand_dims(X, 0)\n        predicted_data = self._compute_output_units_matrix(X)\n        return predicted_data\n\n    def _compute_output_units_matrix(self, matrix_visible_units):\n        feed_dict = {self.visible_units_placeholder: matrix_visible_units}\n        feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n        return sess.run(self.output, feed_dict=feed_dict)\n\n\nclass SupervisedDBNClassification(TensorFlowAbstractSupervisedDBN, ClassifierMixin):\n    \"\"\"\n    This class implements a Deep Belief Network for classification problems.\n    It appends a Softmax Linear Classifier as output layer.\n    \"\"\"\n\n    def _build_model(self, weights=None):\n        super(SupervisedDBNClassification, self)._build_model(weights)\n        self.output = tf.nn.softmax(self.y)\n        self.cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.y, labels=tf.stop_gradient(self.y_)))\n        self.train_step = self.optimizer.minimize(self.cost_function)\n\n    @classmethod\n    def _get_param_names(cls):\n        return super(SupervisedDBNClassification, cls)._get_param_names() + ['label_to_idx_map', 'idx_to_label_map']\n\n    @classmethod\n    def from_dict(cls, dct_to_load):\n        label_to_idx_map = dct_to_load.pop('label_to_idx_map')\n        idx_to_label_map = dct_to_load.pop('idx_to_label_map')\n\n        instance = super(SupervisedDBNClassification, cls).from_dict(dct_to_load)\n        setattr(instance, 'label_to_idx_map', label_to_idx_map)\n        setattr(instance, 'idx_to_label_map', idx_to_label_map)\n\n        return instance\n\n    def _transform_labels_to_network_format(self, labels):\n        new_labels, label_to_idx_map, idx_to_label_map = to_categorical(labels, self.num_classes)\n        self.label_to_idx_map = label_to_idx_map\n        self.idx_to_label_map = idx_to_label_map\n        return new_labels\n\n    def _transform_network_format_to_labels(self, indexes):\n        \"\"\"\n        Converts network output to original labels.\n        :param indexes: array-like, shape = (n_samples, )\n        :return:\n        \"\"\"\n        return list(map(lambda idx: self.idx_to_label_map[idx], indexes))\n\n    def predict(self, X):\n        probs = self.predict_proba(X)\n        indexes = np.argmax(probs, axis=1)\n        return self._transform_network_format_to_labels(indexes)\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predicts probability distribution of classes for each sample in the given data.\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        return super(SupervisedDBNClassification, self)._compute_output_units_matrix(X)\n\n    def predict_proba_dict(self, X):\n        \"\"\"\n        Predicts probability distribution of classes for each sample in the given data.\n        Returns a list of dictionaries, one per sample. Each dict contains {label_1: prob_1, ..., label_j: prob_j}\n        :param X: array-like, shape = (n_samples, n_features)\n        :return:\n        \"\"\"\n        if len(X.shape) == 1:  # It is a single sample\n            X = np.expand_dims(X, 0)\n\n        predicted_probs = self.predict_proba(X)\n\n        result = []\n        num_of_data, num_of_labels = predicted_probs.shape\n        for i in range(num_of_data):\n            # key : label\n            # value : predicted probability\n            dict_prob = {}\n            for j in range(num_of_labels):\n                dict_prob[self.idx_to_label_map[j]] = predicted_probs[i][j]\n            result.append(dict_prob)\n\n        return result\n\n    def _determine_num_output_neurons(self, labels):\n        return len(np.unique(labels))\n\n\nclass SupervisedDBNRegression(TensorFlowAbstractSupervisedDBN, RegressorMixin):\n    \"\"\"\n    This class implements a Deep Belief Network for regression problems in TensorFlow.\n    \"\"\"\n\n    def _build_model(self, weights=None):\n        super(SupervisedDBNRegression, self)._build_model(weights)\n        self.output = self.y\n        self.cost_function = tf.reduce_mean(tf.square(self.y_ - self.y))  # Mean Squared Error\n        self.train_step = self.optimizer.minimize(self.cost_function)\n\n    def _transform_labels_to_network_format(self, labels):\n        \"\"\"\n        Returns the same labels since regression case does not need to convert anything.\n        :param labels: array-like, shape = (n_samples, targets)\n        :return:\n        \"\"\"\n        return labels\n\n    def _compute_output_units_matrix(self, matrix_visible_units):\n        return super(SupervisedDBNRegression, self)._compute_output_units_matrix(matrix_visible_units)\n\n    def _determine_num_output_neurons(self, labels):\n        if len(labels.shape) == 1:\n            return 1\n        else:\n            return labels.shape[1]\n","944f8e21":"## deep-belief-network\nA simple, clean, fast Python implementation of Deep Belief Networks based on binary Restricted Boltzmann Machines (RBM), built upon NumPy, TensorFlow and scikit-learn:\n\nHinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. \"A fast learning algorithm for deep belief nets.\" Neural computation 18.7 (2006): 1527-1554.\n\nFischer, Asja, and Christian Igel. \"Training restricted Boltzmann machines: an introduction.\" Pattern Recognition 47.1 (2014): 25-39.deep-belief-network\nA simple, clean, fast Python implementation of Deep Belief Networks based on binary Restricted Boltzmann Machines (RBM), built upon NumPy, TensorFlow and scikit-learn:\n\nHinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. \"A fast learning algorithm for deep belief nets.\" Neural computation 18.7 (2006): 1527-1554.\n\nFischer, Asja, and Christian Igel. \"Training restricted Boltzmann machines: an introduction.\" Pattern Recognition 47.1 (2014): 25-39.\n\nhttps:\/\/github.com\/albertbup\/deep-belief-network"}}