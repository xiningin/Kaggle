{"cell_type":{"3c33c128":"code","971751df":"code","798dc948":"code","e30f61c0":"code","a085b2c5":"code","e2642299":"code","668580f3":"code","769bcf07":"code","c6a3002b":"code","e2c21f7e":"code","874fa52c":"code","7622f8ca":"code","dbd7f1cf":"code","ffa0dc4c":"code","2bf85968":"code","1fe7d23f":"code","8064245d":"code","b6a4dfaf":"code","05448fcf":"code","078e59fb":"code","abb69ab7":"code","aa6988f3":"code","daa1aee6":"code","7f57462d":"code","081b9d67":"code","72184d0d":"code","e8779258":"code","a479c4f6":"code","336f2ad8":"code","db41f061":"code","07716e26":"code","c1d29596":"code","d9eeff2e":"code","ec5d292a":"code","31d6287b":"code","ae9ad77d":"code","0b663a27":"code","08322445":"code","d3c2e527":"markdown","c49e3973":"markdown"},"source":{"3c33c128":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","971751df":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","798dc948":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","e30f61c0":"train_data.info()\nprint('_'* 50, '\\n')\ntest_data.info()","a085b2c5":"nullsdf = train_data.isna().sum()\nnullsdf[nullsdf > 0]","e2642299":"nullsdf = test_data.isna().sum()\nnullsdf[nullsdf > 0]","668580f3":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","769bcf07":"train_data[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c6a3002b":"train_data[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e2c21f7e":"train_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","874fa52c":"train_data['FareGroup'] = pd.qcut(train_data['Fare'], 4)\ntrain_data[['FareGroup', 'Survived']].groupby(['FareGroup'], as_index=False).mean().sort_values(by='FareGroup', ascending=True)","7622f8ca":"train_data.drop('FareGroup', axis=1, inplace=True)","dbd7f1cf":"age_mean = train_data[\"Age\"].mean()\nage_std = train_data[\"Age\"].std()\n# imputing the only missing value in fare\nfare_median = train_data.Fare.dropna().median()\nembark_mostfrequent = train_data['Embarked'].value_counts().index[0]","ffa0dc4c":"def preprocess(dataset):\n    \n#     step 1 generate random ages using mean and std \n    is_null = dataset[\"Age\"].isnull().sum()\n#     compute random numbers using the mean, std\n    rand_age = np.random.randint(age_mean - age_std, age_mean + age_std, size = is_null)\n#     fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    \n#     step 2 create age bins\n#     cut_labels = ['Children', 'Youth', 'Adults', 'Seniors']\n    cut_labels = [0, 1, 2, 3]\n    cut_bins = [0, 14, 24, 64, np.inf]\n    dataset[\"Age\"] = pd.cut(dataset['Age'], bins=cut_bins, labels=cut_labels)\n\n#     step 3 fillna with most frequent in Embarked\n    dataset['Embarked'].fillna(embark_mostfrequent, inplace=True)\n\n#     step 4 fillna with mean in Fare\n    dataset.Fare = dataset.Fare.fillna(fare_median)\n\n#     step 5 Convert the Fare feature to ordinal values based on the FareGroups\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n#     step 6 create relatives feature by adding (SibSp + Parch)\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']\n    \n#     step 7 create IsAlone feature if relatives is zero\n    dataset['IsAlone'] = dataset['FamilySize'] == 0\n    \n#     step 8 create Title feature\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n#     replace titles with low frequency with a more common name or classify them as Rare\n    dataset['Title'] = dataset['Title'].replace(\n        ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], \n        'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n#     We can convert the categorical titles to ordinal.\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","2bf85968":"datasets = [train_data, test_data]\nfor df in datasets:\n    preprocess(df)","1fe7d23f":"train_data.head()","8064245d":"# # One Hot Encode categorical features\n# sex_dummies = pd.get_dummies(train_data['Sex'], drop_first=True)\n# embarked_dummies = pd.get_dummies(train_data['Embarked'], drop_first=True)\n# train_data = pd.concat([train_data, sex_dummies, embarked_dummies], axis=1)\n\n\n\n# drop redundant columns\ntrain_data.drop(columns=\n                ['PassengerId', 'Name', 'SibSp', 'Parch', 'Cabin', 'Ticket', 'FamilySize', \n#                  'Sex', 'Embarked'\n                ], \n                inplace=True)\ntrain_data.head()","b6a4dfaf":"train_data.info()","05448fcf":"# One Hot Encode categorical features\n# sex_dummies = pd.get_dummies(test_data['Sex'], drop_first=True)\n# embarked_dummies = pd.get_dummies(test_data['Embarked'], drop_first=True)\n# test_data = pd.concat([test_data, sex_dummies, embarked_dummies], axis=1)\n\n# drop redundant columns\ntest_data.drop(columns=\n                ['Name', 'SibSp', 'Parch', 'Cabin', 'Ticket', 'FamilySize', \n#                  'Sex', 'Embarked'\n                ], \n                inplace=True)\ntest_data.head()","078e59fb":"test_data.info()","abb69ab7":"from sklearn.model_selection import train_test_split\n\nX = train_data.drop(columns=['Survived'])\ny = train_data.Survived\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","aa6988f3":"# k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nacc_knn = knn.score(X_valid, y_valid)\n\nprint('Validation set accuracy: {:2.2%}'.format(acc_knn))","daa1aee6":"# LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_log = logreg.score(X_valid, y_valid)\n\nprint('Validation set accuracy: {:2.2%}'.format(acc_log))","7f57462d":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\nacc_svc = svc.score(X_valid, y_valid)\n\nprint('Validation set accuracy: {:2.2%}'.format(acc_svc))","081b9d67":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nacc_gaussian = gaussian.score(X_valid, y_valid)\n\nprint('Validation set accuracy: {:2.2%}'.format(acc_gaussian))","72184d0d":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nacc_perceptron = perceptron.score(X_valid, y_valid)\nprint('Validation set accuracy: {:2.2%}'.format(acc_perceptron))","e8779258":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nacc_linear_svc = linear_svc.score(X_valid, y_valid)\n\nprint('Validation set accuracy: {:2.2%}'.format(acc_linear_svc))","a479c4f6":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nacc_sgd = sgd.score(X_valid, y_valid)\n\nprint('Validation set accuracy: {:2.2%}'.format(acc_sgd))","336f2ad8":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nacc_decision_tree = decision_tree.score(X_valid, y_valid)\n\nprint('Validation set accuracy: {:2.2%}'.format(acc_decision_tree))","db41f061":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=50)\nrandom_forest.fit(X_train, y_train)\nacc_random_forest = random_forest.score(X_valid, y_valid)\n\nprint('Validation set accuracy: {:2.2%}'.format(acc_random_forest))","07716e26":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","c1d29596":"from sklearn.model_selection import GridSearchCV\n# from sklearn.model_selection import KFold\n# from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.metrics import classification_report\n\nparam_grid = {\n    'max_depth': [3, 5], \n    'n_estimators': [50, 100], \n    'min_samples_leaf': [0.12, 0.1],\n    'random_state': [42], \n    'warm_start': [True]\n             }\n\nskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=42)\n\ngrid_rf = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, n_jobs=-1, cv=skf, verbose=1).fit(X_train, y_train)","d9eeff2e":"print('CV Score: {}'.format(grid_rf.best_score_))\nprint('Test score:', grid_rf.score(X_valid, y_valid))\nprint('Best estimator:\\n{}'.format(grid_rf.best_estimator_))","ec5d292a":"param_grid = {\n    'kernel':['poly', 'rbf', 'sigmoid'],\n    'gamma': ['scale', 'auto'],\n    'degree': [2, 3, 4]\n             }\n\ngrid_svc = GridSearchCV(SVC(), param_grid=param_grid, n_jobs=-1, cv=skf, verbose=1).fit(X_train, y_train)","31d6287b":"print('CV Score: {}'.format(grid_svc.best_score_))\nprint('Test score:', grid_svc.score(X_valid, y_valid))\nprint('Best estimator:\\n{}'.format(grid_svc.best_estimator_))","ae9ad77d":"model = grid_svc.best_estimator_\n\nmodel.fit(X_train, y_train)","0b663a27":"from sklearn.metrics import accuracy_score\n\npredictions = model.predict(X_valid)\nvalid_accuracy = accuracy_score(y_valid, predictions)\nprint('Validation set accuracy: {:2.2%}'.format(valid_accuracy))","08322445":"model.fit(X, y)\nX_test = test_data.drop(columns=['PassengerId'])\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","d3c2e527":"### Preprocessing steps\n|Column(s)|Preprocessing steps|\n|---|---|\nAge| generate random ages using mean and std\nAge | create age bins\nEmbarked | fillna with most frequent in Embarked\nFare | fillna with mean in Fare(only 1 in test data)\nFare | Convert the Fare feature to ordinal values based on the FareGroups\n[relatives] | create relatives feature by adding (SibSp + Parch)\n[IsAlone] | create IsAlone feature if relatives is zero\n[Title] | create Title feature by extracting title from Name\n['Sex', 'Embarked'] | transform categorical features to ordinal|\n['SibSp', 'Parch', 'Cabin', 'Ticket', 'Name', 'Sex', 'Embarked'] | DROP","c49e3973":"perfrom a grid search for the highest two models"}}