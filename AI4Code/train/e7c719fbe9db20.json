{"cell_type":{"00e86a23":"code","1cd5c0b2":"code","50f643c6":"code","2dff9080":"code","3a3482ab":"code","bb84cd4c":"code","af5870c2":"code","d509df7d":"code","a7282c3f":"code","fa8d4a44":"code","fee204e2":"code","42304dc8":"code","05fe5043":"code","d4bcd77d":"code","605f7153":"code","a892760f":"markdown","e0d5782a":"markdown","94d24dbc":"markdown","cb07900f":"markdown","a470bd2f":"markdown","4439413d":"markdown","1591df35":"markdown","c7402987":"markdown","8a78e4bd":"markdown"},"source":{"00e86a23":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u8aad\u307f\u8fbc\u307f\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, glob, pickle, time, gc, copy, sys\nimport pandas_profiling as pdp\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\n# import tensorflow as tf\n# AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100) # \u8868\u793a\u3067\u304d\u308b\u8868\u306e\u5217\u6570","1cd5c0b2":"# train\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\ndf_train = pd.read_csv(\"..\/input\/ai-medical-contest-2021\/train.csv\")\nprint(\"df_train.shape\", df_train.shape) # \u30b7\u30a7\u30a4\u30d7 = (\u884c\u6570, \u5217\u6570)\u3092\u8868\u793a\u3059\u308b\n\n# test\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\ndf_test = pd.read_csv(\"..\/input\/ai-medical-contest-2021\/\/test.csv\")\nprint(\"df_test.shape\", df_test.shape) # \u30b7\u30a7\u30a4\u30d7 = (\u884c\u6570, \u5217\u6570)\u3092\u8868\u793a\u3059\u308b\n\n# submission\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\ndf_sub = pd.read_csv(\"..\/input\/ai-medical-contest-2021\/\/sample_submission.csv\")\nprint(\"df_sub.shape\", df_sub.shape) # \u30b7\u30a7\u30a4\u30d7 = (\u884c\u6570, \u5217\u6570)\u3092\u8868\u793a\u3059\u308b\n\n# ECG\u30c7\u30fc\u30bf\u306epath\u306e\u5217\u3092\u8ffd\u52a0.\ndf_train['path'] = df_train['Id'].apply(lambda x: \"..\/input\/ai-medical-contest-2021\/ecg\/{}.npy\".format(x))\ndf_test['path'] = df_test['Id'].apply(lambda x: \"..\/input\/ai-medical-contest-2021\/ecg\/{}.npy\".format(x))\nprint(df_train['path'][0]) # path\u5217\u306e0\u884c\u76ee\u3092\u8868\u793a\n\n# train\u3068test\u3092\u9023\u7d50\u3059\u308b\ndf_traintest = pd.concat([df_train, df_test]).reset_index(drop=True) # reset_index: \u884c\u306eindex\u3092\u30ea\u30bb\u30c3\u30c8\u3059\u308b\nprint(df_traintest.shape)\n\ncol_target = 'target' # \u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u5217\ncol_index = 'Id' # id\u306e\u5217\nprint(\"rate of positive: {:.6f}\".format(df_train[col_target].mean())) # target\u304c1\u3067\u3042\u308b\u5272\u5408\n\n# \u5404\u5217\u306e\u57fa\u672c\u60c5\u5831\u3092\u8868\u793a\n# \u89e3\u6790\u5bfe\u8c61\u306ftrain+test\n# \u5217\u540d, \u578b, nan\u306e\u6570, unique\u306a\u5024\u306e\u6570, \u5b9f\u969b\u306e\u5024\u306e\u4e00\u90e8, \u3092\u8868\u793a\u3059\u308b\ndf_tmp = df_traintest  # \u89e3\u6790\u3059\u308bDataFrame\u3092\u6307\u5b9a\nfor i, col in enumerate(df_tmp.columns): # \u5404\u5217(column)\u306b\u3064\u3044\u3066\n    col_name = col + \" \" * (22 - len(col)) # \u30ab\u30e9\u30e0\u540d, \u898b\u305f\u76ee\u4e0a\u306e\u6574\u5f62\u306e\u305f\u3081\u306b\u30b9\u30da\u30fc\u30b9\u3092\u52a0\u3048\u308b\n    type_name = \"{}\".format(df_tmp[col].dtype) # \u578b\u540d\n    type_name = type_name + \" \" * (8 - len(type_name)) # \u898b\u305f\u76ee\u4e0a\u306e\u6574\u5f62\u306e\u305f\u3081\u306b\u30b9\u30da\u30fc\u30b9\u3092\u52a0\u3048\u308b\n    num_unique = len(df_tmp[col].unique()) # \u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u6570\n    num_nan = pd.isna(df_tmp[col]).sum() # nan\u306e\u6570\n    col_head = \"{}\".format(df_tmp[col].unique()[:5].tolist())[:40] # \u5b9f\u969b\u306e\u5024\u306e\u4e00\u90e8\n    print(\"{:4d}: {} dtype: {} unique: {:8d}, nan: {:6d}, \u5b9f\u969b\u306e\u5024: {}\".format(\n        i, col_name, type_name, len(df_tmp[col].unique()), num_nan, col_head)) # \u8868\u793a\u3059\u308b\n    \n# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b (\u6570\u5024\u306b\u7f6e\u304d\u63db\u3048\u308b).\ndf_traintest['sex'] = df_traintest['sex'].replace('female', 0) # female\u306b0\u3092\u4ee3\u5165\ndf_traintest['sex'] = df_traintest['sex'].replace('male', 1) # male\u306b1\u3092\u4ee3\u5165\ndf_traintest['sex'] = df_traintest['sex'].astype(int) # \u578b\u3092\u6574\u6570\u306b\u5909\u63db\n\ndf_traintest['label_type'] = df_traintest['label_type'].replace('human', 0) # human\u306b0\u3092\u4ee3\u5165\ndf_traintest['label_type'] = df_traintest['label_type'].replace('auto', 1) # auto\u306b1\u3092\u4ee3\u5165\ndf_traintest['label_type'] = df_traintest['label_type'].astype(int) # \u578b\u3092\u6574\u6570\u306b\u5909\u63db\n\n# train \u3068 test \u3092\u518d\u5ea6\u5207\u308a\u5206\u3051\u308b\ndf_train = df_traintest.iloc[:len(df_train)]\ndf_test = df_traintest.iloc[len(df_train):].reset_index(drop=True)\n\n# \u5168\u3066\u306eECG\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\necg_train = np.zeros([len(df_train), 800, 12], np.float32) # train\u306e\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u306e\u4ee3\u5165\u5148. shape=(\u30c7\u30fc\u30bf\u6570, \u6642\u9593\u65b9\u5411, 12\u8a98\u5c0e)\nfor i in range(len(df_train)): # \u5168\u3066\u306etrain data\u306b\u3064\u3044\u3066\n    path_tmp = df_train['path'][i] # i\u884c\u76ee\u306e\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u306epath\n    ecg_tmp = np.load(path_tmp) # i\u884c\u76ee\u306e\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\n    ecg_train[i] = ecg_tmp # \u8aad\u307f\u8fbc\u3093\u3060\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u3092ecg_train\u306ei\u884c\u76ee\u306b\u4ee3\u5165\n\necg_test = np.zeros([len(df_test), 800, 12], np.float32) # test\u306e\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u306e\u4ee3\u5165\u5148. shape=(\u30c7\u30fc\u30bf\u6570, \u6642\u9593\u65b9\u5411, 12\u8a98\u5c0e)\nfor i in range(len(df_test)): # \u5168\u3066\u306etest data\u306b\u3064\u3044\u3066\n    path_tmp = df_test['path'][i] # i\u884c\u76ee\u306e\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u306epath\n    ecg_tmp = np.load(path_tmp) # i\u884c\u76ee\u306e\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\n    ecg_test[i] = ecg_tmp # \u8aad\u307f\u8fbc\u3093\u3060\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u3092ecg_train\u306ei\u884c\u76ee\u306b\u4ee3\u5165\nprint(\"ecg_train.shape: {}\".format(ecg_train.shape))\nprint(\"ecg_test.shape: {}\".format(ecg_test.shape))\n\n# target\u60c5\u5831\u3092numpy\u5f62\u5f0f\u306b\u5909\u63db\ntarget_train = df_train[col_target].values.astype(np.int) # pandas.Series\u304b\u3089np.ndarray\u3078\u5909\u63db\nprint(\"target_train.shape: {}\".format(target_train.shape))","50f643c6":"# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u884c\u3046\u305f\u3081\u306b\u30c7\u30fc\u30bf\u30925\u5206\u5272\u3059\u308b\n# 4\u3064\u3092\u5b66\u7fd2\u306b\u7528\u3044\u30011\u3064\u3092\u691c\u8a3c\u306b\u8981\u3059\u308b\u3002\u3053\u308c\u30925\u56de\u7e70\u308a\u8fd4\u3059\u3002\nfolds = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(\n    np.arange(len(df_train)), \n    y=df_train[col_target]) # \u5404fold\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u30e9\u30d9\u30eb\u306e\u5206\u5e03\u304c\u305d\u308d\u3046\u3088\u3046\u306b\u3059\u308b = stratified K fold\n)\n\n# fold 0\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u5206\u5272\nfold = 0 # fold 0 \u306b\u3064\u3044\u3066\u306e\u5b66\u7fd2\u3092\u884c\u3046\n\n# \u3053\u306efold\u306b\u304a\u3051\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u5207\u308a\u5206\u3051\nX_train = ecg_train[folds[fold][0]] # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\ny_train = target_train[folds[fold][0]] # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\nX_valid = ecg_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\ny_valid = target_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\nprint(\"X_train.shape: {}, X_valid.shape: {}\".format(X_train.shape, X_valid.shape))\nprint(\"y_train.shape: {}, y_valid.shape: {}\".format(y_train.shape, y_valid.shape))","2dff9080":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom tqdm import tqdm_notebook as tqdm\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","3a3482ab":"input_size = 700\n\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, data, label):\n        super().__init__()\n        \n        self.data = data\n        self.label = label\n        self.len = data.shape[0]\n        self.input_size = 700\n        \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, index):\n        out_data = self.data\n        out_data = out_data[index]\n\n        start_idx = np.random.randint(0,800-self.input_size-1)\n        out_data = out_data[start_idx:start_idx+self.input_size:,:]\n        out_label = self.label[index]\n        label = torch.tensor(out_label).float()\n        \n        return torch.FloatTensor(out_data), label","bb84cd4c":"class Net1D(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv1d(12, 64, kernel_size=7, stride=1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool1d(2)\n\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2)\n        self.bn2 = nn.BatchNorm1d(128)\n        \n        self.conv3 = nn.Conv1d(128,256,kernel_size=3, stride=2)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.gap = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(256,1)\n\n\n    def forward(self,x):\n        #s1, s2, s3 = x.shape\n        #x = x.reshape(s1, s3, s2)\n        x = x.permute(0, 2, 1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.conv3(x)\n        #x = self.gap(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n        x = self.gap(x)\n        x = x.view(x.size(0),-1)\n        x = self.fc(x)\n        x = x.view(-1)\n\n        return x","af5870c2":"model = Net1D()\nprint(model)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(total_params)","d509df7d":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport pandas as pd\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport math\nfrom torch.nn.modules.loss import _WeightedLoss\nclass SmoothCrossEntropy(nn.Module):\n    \n    # From https:\/\/www.kaggle.com\/shonenkov\/train-inference-gpu-baseline\n    def __init__(self, smoothing = 0.05,one_hotted=False):\n        super().__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.one_hotted = one_hotted\n\n    def forward(self, x, target):\n        if self.training:\n            x = x.float()\n            if self.one_hotted!=True:\n                target = F.one_hot(target.long(), x.size(1))\n            target = target.float()\n            logprobs = F.log_softmax(x, dim = -1)\n\n            nll_loss = -logprobs * target\n            nll_loss = nll_loss.sum(-1)\n    \n            smooth_loss = -logprobs.mean(dim=-1)\n            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n            return loss.mean()\n        else:\n            if self.one_hotted!=True:\n                loss = F.cross_entropy(x, target.long())\n            else:\n                loss = OneHotCrossEntropy(x, target)\n            return loss","a7282c3f":"cv = 0\nn_splits = 5\noptimizer_name = 'Adam'\nlr = 0.001\nEPOCHS=40\nimport sys\n\nlist_weights = []\nbest_preds_list = []\nvalid_label_list = []\n\nfor fold in range(n_splits):\n    print(f\"### fold: {fold} ###\")\n    X_train = ecg_train[folds[fold][0]] # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    y_train = target_train[folds[fold][0]] # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    X_valid = ecg_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    y_valid = target_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    \n    #X_train = torch.FloatTensor(X_train).to(device)\n    #y_train = torch.FloatTensor(y_train).to(device)\n    #X_valid = torch.FloatTensor(X_valid).to(device)\n    #y_valid = torch.FloatTensor(y_valid).to(device)\n    best_preds = None\n\n    dataset = MyDataset(X_train, y_train)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n    \n    dataset_val = MyDataset(X_valid, y_valid)\n    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=64, shuffle=False)\n\n    model = Net1D().cuda()\n    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n    #criterion = SmoothCrossEntropy(smoothing =0)\n\n\n    list_loss = []\n    list_valloss = []\n    best_auc = 0\n    for e in range(EPOCHS):\n        avg_loss = 0.\n        model.train()\n        for i, (data, y_target) in enumerate(dataloader):\n            data = data.to(device)\n            y_target = y_target.to(device)\n            \n            #print(data.shape)\n            #sys.exit()\n            \n            optimizer.zero_grad()\n            y_pred = model(data)\n            #print(y_target.shape,y_pred.shape)\n            loss = criterion(y_pred, y_target)\n            \n            loss.backward()\n            optimizer.step()\n            \n            avg_loss += loss.item() \/ len(dataloader)\n\n        #print(avg_loss)\n        model.eval()\n        avg_val_loss = 0.\n        valid_labels = []\n        preds = []\n        \n        for i, (data, y_target) in enumerate(dataloader_val):\n            with torch.no_grad():\n                data = data.to(device)\n                y_target = y_target.to(device)\n                y_pred = model(data)\n                loss = criterion(y_pred, y_target)\n            avg_val_loss += loss.item() \/ len(dataloader_val)\n            valid_labels.append(y_target.to('cpu').numpy())\n            preds.append(F.sigmoid(y_pred).cpu().numpy())\n        preds = np.concatenate(preds)\n        valid_labels = np.concatenate(valid_labels)\n        val_auc = roc_auc_score(valid_labels,preds[:])    \n\n        if e % 1 == 0:\n            print(f'epoch {e}: train loss: {avg_loss} val loss: {avg_val_loss} val AUC: {val_auc}')\n\n        if best_auc < val_auc:\n            best_auc = val_auc\n            best_preds = preds\n            print(f'  Epoch {e} - Save Best AUC: {best_auc:.4f}')\n            best_weight = model.state_dict()\n            torch.save(model.state_dict(), f'fold{fold}_exp000_baseline.pth')\n\n    list_weights.append(best_weight)\n    best_preds_list.append(best_preds)\n    valid_label_list.append(valid_labels)","fa8d4a44":"#oof_auc = roc_auc_score(valid_labels,preds[:,1]) \n#print(f\"OOF_AUC{oof_auc}\")","fee204e2":"## calc oof\nbest_preds_list = np.concatenate(best_preds_list)\nvalid_label_list = np.concatenate(valid_label_list)\noof_auc = roc_auc_score(valid_labels,preds[:]) \nprint(f\"OOF_AUC{oof_auc}\")\n","42304dc8":"preds_test = np.zeros([n_splits, len(df_test)], np.float32) # \u4e88\u6e2c\u7d50\u679c\u306e\u4ee3\u5165\u5148\nfor fold, w in tqdm(enumerate(list_weights)):\n#     model = Anomaly_Classifier(input_size=12, num_classes= 1).to(device)\n    list_test = []\n\n    X_test = torch.FloatTensor(ecg_test).to(device)\n\n    dataset_test = MyDataset(X_test, np.zeros(X_test.shape[0]))\n    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=64, shuffle=False)\n\n    model = Net1D().to(device)\n    model.load_state_dict(w)\n    model.eval()\n    l_p = []\n    with torch.no_grad():\n        for i, (data, y_target) in enumerate(dataloader_test):\n            y_p = model(data)\n            y_p = F.sigmoid(y_p).cpu().numpy()\n            l_p.append(y_p)\n    y_pred = np.concatenate(l_p)\n    preds_test[fold] = y_pred","05fe5043":"import seaborn as sns\n%matplotlib inline","d4bcd77d":"sns.heatmap(preds_test)","605f7153":"### submit\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\npreds_test_mean = preds_test.mean(axis=0) # \u5404fold\u306emodel\u306e\u4e88\u6e2c\u306e\u5e73\u5747\u5024\u3092\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u7d50\u679c\u3068\u3057\u3066\u63a1\u7528\u3059\u308b\nprint(\"preds_test_mean.shape: {}\".format(preds_test_mean.shape))\ndf_sub[col_target] = preds_test.mean(axis=0) # \u63a8\u5b9a\u7d50\u679c\u3092\u4ee3\u5165\n# df_sub[col_target] = preds_test[4]\ndf_sub.to_csv(\"submission.torch.csv\", index=None) # submit\u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\ndf_sub.head() # \u6700\u521d\u306e5\u884c\u3092\u8868\u793a","a892760f":"# model\u306bdata\u3092\u6d41\u3059\u305f\u3081\u306edataset\u3092\u69cb\u7bc9\u3059\u308b\nBATCH_SIZE = 64 # \u30df\u30cb\u30d0\u30c3\u30c1\u306b\u542b\u3081\u308b\u30c7\u30fc\u30bf\u306e\u6570\ndef augment_fn(X, y):\n    \"\"\"\n    augmentation (\u30c7\u30fc\u30bf\u6c34\u5897\u3057)\u3092\u8a2d\u5b9a\u3059\u308b\n    \"\"\"\n    X_new = tf.image.random_crop(X, (700,12)) # \u6642\u9593\u65b9\u5411\u306b800 timepoint\u304b\u3089random\u306b700 timepoint\u3092\u5207\u308a\u51fa\u3059\n    return (X_new, y)\n    \n# train dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(( # np\n    X_train, # \u5165\u529b\u30c7\u30fc\u30bf\n    y_train, # \u6b63\u89e3\u30c7\u30fc\u30bf\n))\ntrain_dataset = train_dataset.shuffle(len(train_dataset), reshuffle_each_iteration=True) # \u5b66\u7fd2\u4e2d\u306b\u30c7\u30fc\u30bf\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b\ntrain_dataset = train_dataset.map(augment_fn, num_parallel_calls=AUTOTUNE) # augmentation\u306e\u9069\u7528\ntrain_dataset = train_dataset.batch(BATCH_SIZE) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5316\u3057\u3066\u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b\n\n# valid dataset\nvalid_dataset = tf.data.Dataset.from_tensor_slices((\n    X_valid, # \u5165\u529b\u30c7\u30fc\u30bf\n    y_valid, # \u6b63\u89e3\u30c7\u30fc\u30bf\n))\nvalid_dataset = valid_dataset.batch(BATCH_SIZE) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5316\u3057\u3066\u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b (\u30b7\u30e3\u30c3\u30d5\u30eb\u306f\u3057\u306a\u3044)\n\n# test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    ecg_test, # \u5165\u529b\u30c7\u30fc\u30bf\n    np.zeros(len(ecg_test)), # \u6b63\u89e3\u30c7\u30fc\u30bf (test\u306b\u6b63\u89e3\u30c7\u30fc\u30bf\u306a\u3044\u305f\u3081\u30c0\u30df\u30fc\u30c7\u30fc\u30bf)\n))\ntest_dataset = test_dataset.batch(BATCH_SIZE) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5316\u3057\u3066\u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b (\u30b7\u30e3\u30c3\u30d5\u30eb\u306f\u3057\u306a\u3044)\n\n\n# dataset\u306e\u8aad\u307f\u8fbc\u307f\u30c6\u30b9\u30c8\necg_batch, target_batch = next(iter(train_dataset)) # \u8a66\u3057\u306b\u30df\u30cb\u30d0\u30c3\u30c1\u3092\u8aad\u307f\u8fbc\u3080\nprint(\"train ecg_batch.shape: {}\".format(ecg_batch.shape))\nprint(\"train target_batch.shape: {}\".format(target_batch.shape))\necg_batch, target_batch = next(iter(valid_dataset)) # \u8a66\u3057\u306b\u30df\u30cb\u30d0\u30c3\u30c1\u3092\u8aad\u307f\u8fbc\u3080\nprint(\"valid ecg_batch.shape: {}\".format(ecg_batch.shape))\nprint(\"valid target_batch.shape: {}\".format(target_batch.shape))","e0d5782a":"# deep learning model\u306e\u4f5c\u6210\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\ndef get_model(input_shape=(800, 12)):\n    model = tf.keras.models.Sequential([ # \u30ec\u30a4\u30e4\u30fc\u306e\u30ea\u30b9\u30c8\u304b\u3089\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\n        tf.keras.Input(shape=input_shape), # \u5165\u529b\u306e\u5f62\u72b6\u306e\u6307\u5b9a. shape=(\u6642\u9593\u8ef8, 12\u8a98\u5c0e)\n        # block1\n        tf.keras.layers.Conv1D(64, 7), # \u6642\u9593\u65b9\u5411\u306e1\u6b21\u5143\u7573\u307f\u8fbc\u307f\u30ec\u30a4\u30e4\u30fc. 32=\u51fa\u529b\u30c1\u30e3\u30cd\u30eb\u6570, 7=\u30ab\u30fc\u30cd\u30eb\u30b5\u30a4\u30ba\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Activation('relu'), \n        tf.keras.layers.MaxPool1D(2), \n        # block2\n        tf.keras.layers.Conv1D(128, 3, strides=2),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Activation('relu'), \n        # block3\n        tf.keras.layers.Conv1D(256, 3, strides=2),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Activation('relu'),\n        # pooling\n        tf.keras.layers.GlobalAveragePooling1D(), # \u6642\u9593\u65b9\u5411\u306eglobal pooling\n        # \u6700\u7d42\u30ec\u30a4\u30e4\u30fc\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    return model\n\nmodel = get_model()\nmodel.summary()","94d24dbc":"# test data\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\npreds_test = np.zeros([5, len(df_test)], np.float32) # \u4e88\u6e2c\u7d50\u679c\u306e\u4ee3\u5165\u5148\nfor fold in range(5): # \u5404fold\u306b\u3064\u3044\u3066\n    print(\"fold: {}\".format(fold))\n    # valid dataset\n    X_valid = ecg_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    y_valid = target_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    valid_dataset = tf.data.Dataset.from_tensor_slices((\n        X_valid, # \u5165\u529b\u30c7\u30fc\u30bf\n        y_valid, # \u6b63\u89e3\u30c7\u30fc\u30bf\n    ))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5316\u3057\u3066\u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b (\u30b7\u30e3\u30c3\u30d5\u30eb\u306f\u3057\u306a\u3044)\n\n    # \u4e88\u6e2c\n    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n    model.load_weights(checkpoint_filepath) # \u6700\u3082valid AUC\u304c\u9ad8\u304b\u3063\u305f\u30a8\u30dd\u30c3\u30af\u306e\u91cd\u307f\u3092\u8aad\u307f\u8fbc\u3080\n    pred_test = model.predict(test_dataset) # \u4e88\u6e2c\u306e\u5b9f\u884c\n    preds_test[fold] = pred_test[:,0] # \u4e88\u6e2c\u7d50\u679c\u306e\u4ee3\u5165\nprint(\"preds_test.shape: {}\".format(preds_test.shape))\nprint(preds_test)","cb07900f":"# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306eAUC\u3092\u8a08\u7b97\u3059\u308b\npreds_valid = np.zeros(len(df_train), np.float32) # \u4e88\u6e2c\u7d50\u679c\u306e\u4ee3\u5165\u5148\nfor fold in range(5): # \u5404fold\u306b\u3064\u3044\u3066\n    print(\"fold: {}\".format(fold))\n    # valid dataset\n    X_valid = ecg_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    y_valid = target_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    valid_dataset = tf.data.Dataset.from_tensor_slices((\n        X_valid, # \u5165\u529b\u30c7\u30fc\u30bf\n        y_valid, # \u6b63\u89e3\u30c7\u30fc\u30bf\n    ))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5316\u3057\u3066\u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b (\u30b7\u30e3\u30c3\u30d5\u30eb\u306f\u3057\u306a\u3044)\n\n    # \u4e88\u6e2c\n    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n    model.load_weights(checkpoint_filepath) # \u6700\u3082valid AUC\u304c\u9ad8\u304b\u3063\u305f\u30a8\u30dd\u30c3\u30af\u306e\u91cd\u307f\u3092\u8aad\u307f\u8fbc\u3080\n    pred_valid = model.predict(valid_dataset) # \u4e88\u6e2c\u306e\u5b9f\u884c\n    preds_valid[folds[fold][1]] = pred_valid[:,0] # \u4e88\u6e2c\u7d50\u679c\u306e\u4ee3\u5165\n\nvalid_auc = metrics.roc_auc_score(df_train[col_target], preds_valid)\nprint(\"CV: {:.6f}\".format(valid_auc))","a470bd2f":"## pytorch","4439413d":"## Tensorflow(baseline)","1591df35":"# \u30e2\u30c7\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb (\u5b66\u7fd2\u6761\u4ef6\u306e\u8a2d\u5b9a)\nmodel = get_model(input_shape=(None, 12)) # \u6642\u9593\u8ef8\u306e\u5165\u529b\u9577\u3055None=\u53ef\u5909\u306b\u3057\u3066\u518d\u5ea6model\u69cb\u7bc9\nmodel.compile(optimizer='adam', # \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306bAdam\u3092\u6307\u5b9a\n              loss='binary_crossentropy', # \u640d\u5931\u95a2\u6570\u306bbinary crossentropy\u3092\u6307\u5b9a\n              metrics=['AUC'], # \u8a55\u4fa1\u95a2\u6570\u306bAUC\u3092\u6307\u5b9a\n             )\n# \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u65b9\u6cd5\u306e\u6307\u5b9a\ncheckpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath, # \u4fdd\u5b58path\n    save_weights_only=True, # \u91cd\u307f\u306e\u307f\u3092\u4fdd\u5b58\n    monitor='val_auc', # validataion\u306eAUC\u306e\u5024\u306b\u57fa\u3065\u3044\u3066\u91cd\u307f\u3092\u4fdd\u5b58\u3059\u308b\n    mode='max', # validataion\u306eAUC\u304c\u6700\u5927\u3068\u306a\u3063\u305f\u6642\u91cd\u307f\u3092\u4fdd\u5b58\u3059\u308b\n    save_best_only=True # AUC\u304c\u6539\u5584\u3057\u305f\u3068\u304d\u306e\u307f\u4fdd\u5b58\u3059\u308b\n)","c7402987":"# fold1-4\u306b\u3064\u3044\u3066\u3082\u5b66\u7fd2\u3092\u884c\u3046\n# for loop\u306e\u4e2d\u8eab\u306f\u4e0a\u8a18\u306efold 0\u3067\u306e\u51e6\u7406\u3068\u540c\u69d8\u3067\u3059\nfor fold in range(5):\n    print(\"fold: {}\".format(fold))\n    X_train = ecg_train[folds[fold][0]] # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    y_train = target_train[folds[fold][0]] # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    X_valid = ecg_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    y_valid = target_train[folds[fold][1]] # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\n    print(\"len train: {}. len valid: {}\".format(len(X_train), len(X_valid)))\n\n    # train dataset\n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        X_train, # \u5165\u529b\u30c7\u30fc\u30bf\n        y_train, # \u6b63\u89e3\u30c7\u30fc\u30bf\n    ))\n    train_dataset = train_dataset.shuffle(len(train_dataset), reshuffle_each_iteration=True) # \u5b66\u7fd2\u4e2d\u306b\u30c7\u30fc\u30bf\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b\n    train_dataset = train_dataset.map(augment_fn, num_parallel_calls=AUTOTUNE) # augmentation\u306e\u9069\u7528\n    train_dataset = train_dataset.batch(BATCH_SIZE) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5316\u3057\u3066\u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b\n\n    # valid dataset\n    valid_dataset = tf.data.Dataset.from_tensor_slices((\n        X_valid, # \u5165\u529b\u30c7\u30fc\u30bf\n        y_valid, # \u6b63\u89e3\u30c7\u30fc\u30bf\n    ))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5316\u3057\u3066\u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3092\u6307\u5b9a\u3059\u308b (\u30b7\u30e3\u30c3\u30d5\u30eb\u306f\u3057\u306a\u3044)\n\n    # model\u69cb\u7bc9\n    model = get_model(input_shape=(None, 12))\n    # \u30e2\u30c7\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb (\u5b66\u7fd2\u6761\u4ef6\u306e\u8a2d\u5b9a)\n    model.compile(optimizer='adam', # \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306bAdam\u3092\u6307\u5b9a\n                  loss='binary_crossentropy', # \u640d\u5931\u95a2\u6570\u306bbinary crossentropy\u3092\u6307\u5b9a\n                  metrics=['AUC'], # \u8a55\u4fa1\u95a2\u6570\u306bAUC\u3092\u6307\u5b9a\n                 )\n    # \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u65b9\u6cd5\u306e\u6307\u5b9a\n    checkpoint_filepath = \"weight_fold{}.ckpt\".format(fold)\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath, # \u4fdd\u5b58path\n        save_weights_only=True, # \u91cd\u307f\u306e\u307f\u3092\u4fdd\u5b58\n        monitor='val_auc', # validataion\u306eAUC\u306e\u5024\u306b\u57fa\u3065\u3044\u3066\u91cd\u307f\u3092\u4fdd\u5b58\u3059\u308b\n        mode='max', # validataion\u306eAUC\u304c\u6700\u5927\u3068\u306a\u3063\u305f\u6642\u91cd\u307f\u3092\u4fdd\u5b58\u3059\u308b\n        save_best_only=True # AUC\u304c\u6539\u5584\u3057\u305f\u3068\u304d\u306e\u307f\u4fdd\u5b58\u3059\u308b\n    )\n    # \u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\n    model.fit(\n        train_dataset, # \u5b66\u7fd2\u306b\u7528\u3044\u308bdataset\n        validation_data=valid_dataset, # \u691c\u8a3c\u306b\u7528\u3044\u308bdataset\n        callbacks=[model_checkpoint_callback], # \u30e2\u30c7\u30eb\u4fdd\u5b58\u65b9\u6cd5\u306e\u6307\u5b9a\n        epochs=16, # epoch\u6570 (1epoch=\u3059\u3079\u3066\u306e\u753b\u50cf\u30921\u56de\u305a\u3064\u5b66\u7fd2\u306b\u5229\u7528\u3059\u308b)\n    )","8a78e4bd":"### submit\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\npreds_test_mean = preds_test.mean(axis=0) # \u5404fold\u306emodel\u306e\u4e88\u6e2c\u306e\u5e73\u5747\u5024\u3092\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u7d50\u679c\u3068\u3057\u3066\u63a1\u7528\u3059\u308b\nprint(\"preds_test_mean.shape: {}\".format(preds_test_mean.shape))\ndf_sub[col_target] = preds_test.mean(axis=0) # \u63a8\u5b9a\u7d50\u679c\u3092\u4ee3\u5165\ndf_sub.to_csv(\"submission.tf.csv\", index=None) # submit\u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\ndf_sub.head() # \u6700\u521d\u306e5\u884c\u3092\u8868\u793a"}}