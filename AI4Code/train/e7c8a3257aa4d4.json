{"cell_type":{"1083b887":"code","7ec413d7":"code","de9c7c97":"code","11fca65c":"code","a371554b":"code","2b7b2797":"code","3c75f416":"code","b50a0e2b":"code","27040ee1":"code","c253ae2c":"code","5db1f94b":"code","b773c18c":"code","803a0e35":"code","6fb64d6a":"code","66a17f39":"code","d7de3933":"code","e2149bdf":"code","94092a1c":"code","c5a9f1e4":"code","a2b94aaf":"code","ca9f6c82":"code","ad8b4837":"code","2d413100":"code","d341ebdb":"code","b9f1dadc":"code","ba68e02c":"code","6a9d8cbe":"code","dd94ce21":"code","d886d015":"code","2e61bfda":"code","0ec02e05":"code","a2c96b25":"code","d93beb94":"code","5c44a453":"code","8408032e":"code","1f083584":"code","b77c295a":"code","3a716b44":"code","d5d05a9c":"code","05d2024d":"code","76c2073c":"code","340991f6":"code","367fe210":"code","9d87fa14":"code","92674dac":"code","0966a6f6":"code","63ae8e4b":"code","bddf61b5":"code","28394bb6":"code","1f56a662":"code","2af2ac2a":"code","47acdd3d":"code","bf8cb67a":"code","5873f783":"code","592b2bb7":"code","4fcbd097":"code","f53b0b0b":"code","5f18bdaa":"code","f55643e4":"code","f1738538":"code","e8efd4be":"code","081c767a":"code","fd7b378d":"code","b117174d":"code","70b3e83e":"code","81553916":"code","adc5236c":"code","95f03aaf":"code","954e277c":"code","6d09adc1":"code","07d98a41":"code","fca7833f":"code","cbb81bd1":"code","6c108cef":"code","ee1c6056":"code","b9cf17d8":"code","881e95f9":"code","ad9dd6d4":"code","61d1fe76":"code","eb2bcda3":"code","b6e769f8":"code","65312987":"code","4c7a066f":"code","860f542e":"code","fa009492":"code","6d56689b":"code","dab62be8":"code","157a5f15":"code","294aab1a":"code","5c29e81e":"code","70998e7c":"code","966b5ecb":"markdown","454772d0":"markdown","7c69cf64":"markdown","3c206350":"markdown","72bd813c":"markdown","bb6b9842":"markdown","d8716878":"markdown","fc568547":"markdown","e948da85":"markdown","13f73147":"markdown"},"source":{"1083b887":"!pip install spacy==3.0.0\n!pip install sklearn_crfsuite\n!pip install scikit-learn==0.23.2\n!pip install pytorch-crf","7ec413d7":"# import main libraries\nfrom IPython.display import Image\nfrom IPython.core.display import display, HTML\nimport pandas as pd\nfrom collections import Counter\nimport random\nfrom spacy import displacy\nimport json\nimport os\nfrom spacy.training import offsets_to_biluo_tags\nimport spacy\nimport eli5\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport sklearn_crfsuite\nfrom sklearn_crfsuite import scorers\nfrom sklearn_crfsuite import metrics\nfrom sklearn.metrics import f1_score","de9c7c97":"with open('..\/input\/mitrestaurant\/restauranttrain.bio', 'r') as f:\n    for line in f.readlines()[:30]:\n        print(line)","11fca65c":"# switch column places\nwith open('..\/input\/mitrestaurant\/restauranttrain.bio', 'r') as f:\n    with open('restauranttrain_updated.bio', 'w') as w:\n        for line in f.readlines():\n            if line == '\\n':\n                w.write(line)\n            else:\n                w.write('\\t'.join(line.strip().split('\\t')[::-1]) + '\\n')","a371554b":"# split into train and validation\ncount = 0\nwith open('..\/input\/mitrestaurant\/restauranttrain.bio', 'r') as f:\n    with open('restauranttrain_updated_train.bio', 'w') as w1:\n        with open('restauranttrain_updated_valid.bio', 'w') as w2:\n            for line in f.readlines():\n                if count < 6500:\n                    if line == '\\n':\n                        w1.write(line)\n                        count += 1\n                    else:\n                        w1.write('\\t'.join(line.strip().split('\\t')[::-1]) + '\\n')\n                else:\n                    if line == '\\n':\n                        w2.write(line)\n                    else:\n                        w2.write('\\t'.join(line.strip().split('\\t')[::-1]) + '\\n')","2b7b2797":"# load the data again\n\ndata = []\nlabels = []\nwith open('restauranttrain_updated.bio', 'r') as f:\n    cur_data = []\n    cur_label = []\n    for line in f.readlines():\n        if line == '\\n':\n            data.append(cur_data)\n            labels.append(cur_label)\n            cur_data = []\n            cur_label = []\n        else:\n            cur_data.append(line.strip().split('\\t')[0])\n            cur_label.append(line.strip().split('\\t')[1])","3c75f416":"for w, e in zip(data[0], labels[0]):\n    print(f'{w}\\t{e}')\n","b50a0e2b":"predictions = []\nfor word in data[0]:\n    if word.isdigit():\n        predictions.append('B-Rating')\n    else:\n        predictions.append('O')","27040ee1":"f1_score(labels[0], predictions, average='weighted')","c253ae2c":"df = pd.DataFrame({'sent_id': [i for j in [[i] * len(s) for i, s in enumerate(data)] for i in j],\n                   'data': [i for j in data for i in j],\n                   'entities': [i for j in labels for i in j]})\ndf.head(10)","5db1f94b":"class SentenceGetter(object):\n    \n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s['data'].values.tolist(), \n                                                           s['entities'].values.tolist())]\n        self.grouped = self.data.groupby('sent_id').apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n        \n    def get_next(self):\n        try: \n            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n            self.n_sent += 1\n            return s \n        except:\n            return None\n\ngetter = SentenceGetter(df)\nsentences = getter.sentences","b773c18c":"def word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n    \n    features = {\n        'bias': 1.0, \n        'word.lower()': word.lower(), \n        'word[-3:]': word[-3:],\n        'word[-2:]': word[-2:],\n        'word.isupper()': word.isupper(),\n        'word.istitle()': word.istitle(),\n        'word.isdigit()': word.isdigit()\n    }\n    if i > 0:\n        word1 = sent[i-1][0]\n        features.update({\n            '-1:word.lower()': word1.lower(),\n            '-1:word.istitle()': word1.istitle(),\n            '-1:word.isupper()': word1.isupper()\n        })\n    else:\n        features['BOS'] = True\n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        features.update({\n            '+1:word.lower()': word1.lower(),\n            '+1:word.istitle()': word1.istitle(),\n            '+1:word.isupper()': word1.isupper()\n        })\n    else:\n        features['EOS'] = True\n\n    return features\n\ndef sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]\n\ndef sent2labels(sent):\n    return [label for token, label in sent]\n\ndef sent2tokens(sent):\n    return [token for token, label in sent]","803a0e35":"X = [sent2features(s) for s in sentences]\ny = [sent2labels(s) for s in sentences]","6fb64d6a":"X[0][:2]","66a17f39":"X_train = X[:6500]\nX_test = X[6500:]\ny_train = y[:6500]\ny_test = y[6500:]","d7de3933":"%%time\ncrf = sklearn_crfsuite.CRF(\n    algorithm='lbfgs',\n    c1=0.1,\n    c2=0.1,\n    max_iterations=100,\n    all_possible_transitions=True,\n    verbose=True\n)\ncrf.fit(X_train, y_train)","e2149bdf":"all_entities = sorted(df.entities.unique().tolist())","94092a1c":"y_pred = crf.predict(X_test)\nmetrics.flat_f1_score(y_test, y_pred, average='weighted', labels=all_entities)","c5a9f1e4":"y_pred = crf.predict(X_test)\nmetrics.flat_f1_score(y_test, y_pred, average='weighted', labels=[i for i in all_entities if i != 'O'])","a2b94aaf":"print(metrics.flat_classification_report(y_test, y_pred, labels = all_entities))","ca9f6c82":"def print_transitions(trans_features):\n    for (label_from, label_to), weight in trans_features:\n        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n\nprint(\"Top likely transitions:\")\nprint_transitions(Counter(crf.transition_features_).most_common(20))\n\nprint(\"\\nTop unlikely transitions:\")\nprint_transitions(Counter(crf.transition_features_).most_common()[-20:])","ad8b4837":"def print_state_features(state_features):\n    for (attr, label), weight in state_features:\n        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n\nprint(\"Top positive:\")\nprint_state_features(Counter(crf.state_features_).most_common(30))\n\nprint(\"\\nTop negative:\")\nprint_state_features(Counter(crf.state_features_).most_common()[-30:])","2d413100":"eli5.show_weights(crf, top=10)","d341ebdb":"!python -m spacy init config base_config.cfg -p ner","b9f1dadc":"!python -m spacy init fill-config base_config.cfg config.cfg","ba68e02c":"!python -m spacy convert restauranttrain_updated.bio . -t json -c ner","6a9d8cbe":"!python -m spacy convert restauranttrain_updated_train.bio . -c ner","dd94ce21":"!python -m spacy convert restauranttrain_updated_valid.bio . -c ner","d886d015":"!python -m spacy train config.cfg --output .\/output --paths.train restauranttrain_updated_train.spacy --paths.dev restauranttrain_updated_valid.spacy","2e61bfda":"!python -m spacy evaluate output\/model-last restauranttrain_updated_valid.spacy","0ec02e05":"nlp = spacy.load(\"output\/model-best\")\nfor i in range(10, 20):\n    doc = nlp(' '.join(data[i]))\n\n    spacy.displacy.render(doc, style=\"ent\", jupyter=True)","a2c96b25":"with open('restauranttrain_updated.json', 'r') as f:\n    d = json.load(f)","d93beb94":"d[0]['paragraphs'][34]['sentences']","5c44a453":"# nlp = spacy.load(\"en_core_web_sm\")","8408032e":"tokens_dict = d[0]['paragraphs'][34]['sentences'][0]['tokens']\ntokens = [i['orth'] for i in tokens_dict]","1f083584":"text = ' '.join(tokens)\ndoc = nlp(text)\nentities = d[0]['paragraphs'][34]['entities']","b77c295a":"text","3a716b44":"entities","d5d05a9c":"offsets_to_biluo_tags(doc, entities)","05d2024d":"len(tokens) == len(offsets_to_biluo_tags(doc, entities))","76c2073c":"for ent in entities:\n    t = text[ent[0]: ent[1]]\n    print(f'Entity: {ent[2]}: {t}')","340991f6":"for k, m in zip(tokens, offsets_to_biluo_tags(doc, entities)):\n    print(f'{k}\\t{m}')","367fe210":"from typing import List, Tuple, Union\ndef convert_to_biluo(text: str = '',\n                     entities: List[Tuple] = None,\n                     tokens: list = None,\n                     missing: str = 'O') -> Tuple[Union[List[str], list, None], List[str]]:\n    \"\"\"\n    Tokenize text and return text tokens and ner labels.\n\n    Args:\n        text: text\n        entities: labels in spacy format\n        tokens: already tokenized text, if you want it\n        missing: lable for tokens without entities\n\n    Returns:\n        tokenized text and labels\n    \"\"\"\n\n    # create dicts with start\/end position of token and its index\n    starts = []\n    ends = []\n    cur_index = 0\n    tokens = text.split() if tokens is None else tokens\n\n    for token in tokens:\n        starts.append(cur_index)\n        ends.append(cur_index + len(token))\n        cur_index += len(token) + 1\n\n    starts = {k: v for v, k in enumerate(starts)}\n    ends = {k: v for v, k in enumerate(ends)}\n\n    # this will be a list with token labels\n    biluo = [\"-\" for _ in text.split()]\n\n    # check that there are no overlapping entities\n    entities_indexes = [list(range(i[0], i[1])) for i in entities]\n    if max(Counter([i for j in entities_indexes for i in j]).values()) > 1:\n        raise ValueError('You have overlapping entities')\n\n    tokens_in_ents = {}\n\n    # Handle entity cases\n    for start_char, end_char, label in entities:\n        for token_index in range(start_char, end_char):\n            tokens_in_ents[token_index] = (start_char, end_char, label)\n        start_token = starts.get(start_char)\n        end_token = ends.get(end_char)\n        # Only interested if the tokenization is correct\n        if start_token is not None and end_token is not None:\n            if start_token == end_token:\n                biluo[start_token] = f\"U-{label}\"\n            else:\n                biluo[start_token] = f\"B-{label}\"\n                for i in range(start_token + 1, end_token):\n                    biluo[i] = f\"I-{label}\"\n                biluo[end_token] = f\"L-{label}\"\n\n    # put missing value for tokens without labels\n    entity_chars = set()\n    for start_char, end_char, label in entities:\n        for i in range(start_char, end_char):\n            entity_chars.add(i)\n\n    for ind, token in enumerate(tokens):\n        for i in range(list(starts.keys())[ind], list(ends.keys())[ind]):\n            if i in entity_chars:\n                break\n        else:\n            biluo[ind] = missing\n\n    return tokens, biluo","9d87fa14":"# difference betwen my function and spacy's function\nnew_data = []\nbiluo_labels = []\nfor i in range(len(d[0]['paragraphs'])):\n    tokens_dict = d[0]['paragraphs'][i]['sentences'][0]['tokens']\n    tokens = [i['orth'] for i in tokens_dict]\n    if len([i['orth'] for i in tokens_dict]) > 1:\n        \n        text = ' '.join(tokens)\n        doc = nlp(text)\n        entities = d[0]['paragraphs'][i]['entities']\n\n        new_ents = offsets_to_biluo_tags(doc, entities)\n        new_data.append(tokens)\n        biluo_labels.append(new_ents)\n        if len(tokens) != len(new_ents):\n            \n            ents2 = convert_to_biluo(text, entities)[1]\n            print(i, entities, text)\n            for ent in entities:\n                t = text[ent[0]: ent[1]]\n                print(f'Entity: {ent[2]}: {t}')\n            for k, m, l in zip(tokens, new_ents, ents2):\n                print(f'{k}\\t{m}\\t{l}')\n            print()\n        if i > 800:\n            break","92674dac":"# convert the data\nnew_data = []\nbiluo_labels = []\nfor i in range(len(d[0]['paragraphs'])):\n    tokens_dict = d[0]['paragraphs'][i]['sentences'][0]['tokens']\n    tokens = [i['orth'] for i in tokens_dict]\n    if len([i['orth'] for i in tokens_dict]) > 1:\n        \n        text = ' '.join(tokens)\n        doc = nlp(text)\n        entities = d[0]['paragraphs'][i]['entities']\n\n        new_ents = offsets_to_biluo_tags(doc, entities)\n        if entities == []:\n            new_ents = ['O'] * len(tokens)\n        new_data.append(tokens)\n        \n        biluo_labels.append(new_ents)\n        if len(tokens) != len(new_ents):\n            \n            ents2 = convert_to_biluo(text, entities)[1]\n            biluo_labels[-1] = ents2\n","0966a6f6":"len(new_data)","63ae8e4b":"df = pd.DataFrame({'sent_id': [i for j in [[i] * len(s) for i, s in enumerate(new_data)] for i in j],\n                   'data': [i for j in new_data for i in j],\n                   'entities': [i for j in biluo_labels for i in j]})\ndf.head()","bddf61b5":"getter = SentenceGetter(df)\nsentences = getter.sentences\nX = [sent2features(s) for s in sentences]\ny = [sent2labels(s) for s in sentences]","28394bb6":"X_train = X[:6500]\nX_test = X[6500:]\ny_train = y[:6500]\ny_test = y[6500:]","1f56a662":"\ncrf = sklearn_crfsuite.CRF(\n    algorithm='lbfgs',\n    c1=0.1,\n    c2=0.1,\n    max_iterations=100,\n    all_possible_transitions=True\n)\ncrf.fit(X_train, y_train)","2af2ac2a":"all_entities = sorted(df.entities.unique().tolist())","47acdd3d":"y_pred = crf.predict(X_test)\nmetrics.flat_f1_score(y_test, y_pred, average='weighted', labels=all_entities)","bf8cb67a":"y_pred = crf.predict(X_test)\nmetrics.flat_f1_score(y_test, y_pred, average='weighted', labels=[i for i in all_entities if i != 'O'])","5873f783":"print(metrics.flat_classification_report(y_test, y_pred, labels = all_entities))","592b2bb7":"def print_transitions(trans_features):\n    for (label_from, label_to), weight in trans_features:\n        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n\nprint(\"Top likely transitions:\")\nprint_transitions(Counter(crf.transition_features_).most_common(20))\n\nprint(\"\\nTop unlikely transitions:\")\nprint_transitions(Counter(crf.transition_features_).most_common()[-20:])","4fcbd097":"def print_state_features(state_features):\n    for (attr, label), weight in state_features:\n        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n\nprint(\"Top positive:\")\nprint_state_features(Counter(crf.state_features_).most_common(30))\n\nprint(\"\\nTop negative:\")\nprint_state_features(Counter(crf.state_features_).most_common()[-30:])","f53b0b0b":"eli5.show_weights(crf, top=10)","5f18bdaa":"import json\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\nimport joblib\nfrom typing import List, Tuple, Union, Dict","f55643e4":"def get_word_to_idx(count: List[Tuple[str, int]],\n                   min_words: Union[int, float] = 0.0,\n                   max_words: Union[int, float] = 1.0) -> Dict[str, int]:\n    max_count = count[0][1]\n    if isinstance(min_words, float):\n        min_words = max_count * min_words\n    if isinstance(max_words, float):\n        max_words = max_count * max_words\n        \n    all_words = [w[0] for w in count if max_words >= w[1] >= min_words]\n    \n    all_words = ['<pad>', '<unk>'] + all_words\n    \n    word_to_idx = {k: v for k, v in zip(all_words, range(0, len(all_words)))}\n    return word_to_idx","f1738538":"count = Counter([i for j in new_data for i in j])\nword_to_idx = get_word_to_idx(count.most_common(), min_words=1)\nlen(word_to_idx)","e8efd4be":"tags = sorted(list({i for j in biluo_labels for i in j}))\ntags.remove('O')\n\ntag_to_idx = {}\nfor ind, entity in enumerate(tags):\n    tag_to_idx[f'{entity}'] = len(tag_to_idx)\n\nfor special_tag in ['O', 'PAD']:\n    tag_to_idx[special_tag] = len(tag_to_idx)\n    \ntag_to_idx","081c767a":"from typing import Optional\ndef pad_sequences(\n    sequences: List,\n    maxlen: Optional[int],\n    dtype: str = 'int32',\n    padding: str = 'post',\n    truncating: str = 'post',\n    value: int = 0,\n) -> np.array:\n    \"\"\"Pad sequences to the same length.\n    from Keras\n\n    This function transforms a list of\n    `num_samples` sequences (lists of integers)\n    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n    `num_timesteps` is either the `maxlen` argument if provided,\n    or the length of the longest sequence otherwise.\n\n    Sequences that are shorter than `num_timesteps`\n    are padded with `value` at the end.\n\n    Sequences longer than `num_timesteps` are truncated\n    so that they fit the desired length.\n    The position where padding or truncation happens is determined by\n    the arguments `padding` and `truncating`, respectively.\n\n    Pre-padding is the default.\n\n    # Arguments\n        sequences: List of lists, where each element is a sequence.\n        maxlen: Int, maximum length of all sequences.\n        dtype: Type of the output sequences.\n            To pad sequences with variable length strings, you can use `object`.\n        padding: String, 'pre' or 'post':\n            pad either before or after each sequence.\n        truncating: String, 'pre' or 'post':\n            remove values from sequences larger than\n            `maxlen`, either at the beginning or at the end of the sequences.\n        value: Float or String, padding value.\n\n    # Returns\n        x: Numpy array with shape `(len(sequences), maxlen)`\n\n    # Raises\n        ValueError: In case of invalid values for `truncating` or `padding`,\n            or in case of invalid shape for a `sequences` entry.\n    \"\"\"\n    if not hasattr(sequences, '__len__'):\n        raise ValueError('`sequences` must be iterable.')\n    num_samples = len(sequences)\n\n    lengths = []\n    for x in sequences:\n        try:\n            lengths.append(len(x))\n        except TypeError:\n            raise ValueError('`sequences` must be a list of iterables. ' 'Found non-iterable: ' + str(x))\n\n    if maxlen is None:\n        maxlen = np.max(lengths)\n\n    # take the sample shape from the first non empty sequence\n    # checking for consistency in the main loop below.\n    sample_shape = ()\n    for s in sequences:\n        if len(s) > 0:\n            sample_shape = np.asarray(s).shape[1:]\n            break\n\n    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n    for idx, s in enumerate(sequences):\n        if not len(s):\n            continue  # empty list\/array was found\n        if truncating == 'pre':\n            trunc = s[-maxlen:]\n        elif truncating == 'post':\n            trunc = s[:maxlen]\n        else:\n            raise ValueError(f'Truncating type \"{truncating}\" ' 'not understood')\n\n        # check `trunc` has expected shape\n        trunc = np.asarray(trunc, dtype=dtype)\n        if trunc.shape[1:] != sample_shape:\n            raise ValueError(\n                f'Shape of sample {trunc.shape[1:]} of sequence at position {idx}'\n                f'is different from expected shape {sample_shape}'\n            )\n\n        if padding == 'post':\n            x[idx, : len(trunc)] = trunc\n        elif padding == 'pre':\n            x[idx, -len(trunc) :] = trunc\n        else:\n            raise ValueError(f'Padding type \"{padding}\" not understood')\n    return x\n","fd7b378d":"from torch.utils.data import Dataset\nclass NerDataset(Dataset):\n    def __init__(self, ner_data: List, ner_tags: List, word_to_idx: Dict, tag_to_idx: Dict, **kwarg: Dict):\n        self.ner_data = ner_data\n        self.ner_tags = ner_tags\n        self.word_to_idx = word_to_idx\n        self.tag_to_idx = tag_to_idx\n        \n    def __len__(self) -> int:\n        return len(self.ner_data)\n    \n    def __getitem__(self, idx: int) -> Tuple[np.array, int, np.array]:\n        line = self.ner_data[idx]\n        \n        tokens = [self.word_to_idx[w] if w in self.word_to_idx else self.word_to_idx['<unk>'] for w in self.ner_data[idx]]\n        \n        labels = [self.tag_to_idx[w] for w in self.ner_tags[idx]]\n        \n        return np.array(tokens), len(tokens), np.array(labels)","b117174d":"X_train = new_data[:6500]\nX_test = new_data[6500:]\ny_train = biluo_labels[:6500]\ny_test = biluo_labels[6500:]\n\ntrain_dataset = NerDataset(X_train, y_train, word_to_idx, tag_to_idx)\nvalid_dataset = NerDataset(X_test, y_test, word_to_idx, tag_to_idx)","70b3e83e":"train_dataset.__getitem__(5)","81553916":"class Collator:\n    def __init__(self, test=False, percentile=100, pad_value=0):\n        self.test = test\n        self.percentile = percentile\n        self.pad_value = pad_value\n\n    def __call__(self, batch):\n        tokens, lens, labels = zip(*batch)\n        lens = np.array(lens)\n\n        max_len = min(int(np.percentile(lens, self.percentile)), 100)\n\n        tokens = torch.tensor(\n            pad_sequences(tokens, maxlen=max_len, padding='post', value=self.pad_value), dtype=torch.long\n        )\n        lens = torch.tensor([min(i, max_len) for i in lens], dtype=torch.long)\n        labels = torch.tensor(\n            pad_sequences(labels, maxlen=max_len, padding='post', value=self.pad_value), dtype=torch.long\n        )\n\n        return tokens, lens, labels","adc5236c":"import torch\ncollator = Collator(percentile=100, pad_value=tag_to_idx['PAD'])\ntrain_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=128,\n            num_workers=0,\n            collate_fn=collator,\n            shuffle=False,\n        )\n\nvalid_loader = torch.utils.data.DataLoader(\n            valid_dataset,\n            batch_size=64,\n            num_workers=0,\n            collate_fn=collator,\n            shuffle=False,\n        )\n","95f03aaf":"for batch in train_loader:\n    break","954e277c":"batch","6d09adc1":"batch[0].shape, batch[1].shape, batch[2].shape","07d98a41":"import fasttext.util\n# fasttext.util.download_model('en', if_exists='ignore')  # English\nft = fasttext.load_model('..\/input\/fasttext-common-crawl-bin-model\/cc.en.300.bin')","fca7833f":"fasttext.util.reduce_model(ft, 100)","cbb81bd1":"ft['Dog']","6c108cef":"def build_matrix(\n    word_dict: Dict,\n    embedding_index,\n    max_features: int = 100000,\n    embed_size: int = 100,\n) -> Tuple[np.array, int, List]:\n    \"\"\"\n    Create embedding matrix\n\n    Args:\n        word_dict: tokenizer\n        embedding_index: Fasttext embeddings\n        max_features: max features to use\n        embed_size: size of embeddings\n\n    Returns:\n        embedding matrix, number of of words and the list of not found words\n    \"\"\"\n    embedding_index = ft\n    nb_words = min(max_features, len(word_dict))\n    embedding_matrix = np.zeros((nb_words, embed_size))\n\n    for word, i in word_dict.items():\n        embedding_matrix[i] = embedding_index[word]\n    return embedding_matrix, nb_words","ee1c6056":"embedding_matrix, nb_words = build_matrix(word_dict=word_to_idx, embedding_index=ft)","b9cf17d8":"from torch import nn","881e95f9":"embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\nembedding = nn.Embedding.from_pretrained(embedding_matrix)\nembedding.weight.requires_grad = False","ad9dd6d4":"import torch.functional as F\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom typing import Any","61d1fe76":"class SpatialDropout(nn.Module):\n    \"\"\"\n    Spatial Dropout drops a certain percentage of dimensions from each word vector in the training sample\n    implementation: https:\/\/discuss.pytorch.org\/t\/spatial-dropout-in-pytorch\/21400\n    explanation: https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/76883\n    \"\"\"\n\n    def __init__(self, p: float):\n        super(SpatialDropout, self).__init__()\n        self.spatial_dropout = nn.Dropout2d(p=p)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.permute(0, 2, 1)  # convert to [batch, channels, time]\n        x = self.spatial_dropout(x)\n        x = x.permute(0, 2, 1)  # back to [batch, time, channels]\n        return x","eb2bcda3":"class MultiHeadSelfAttention(nn.Module):\n    \"\"\"\n    torch.nn.MultiHeadAttention wrapper to unify interface with other Attention classes\n    Implementation of Dot-product Attention\n    paper: https:\/\/arxiv.org\/abs\/1706.03762\n    Time complexity: O(n^2)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, num_heads: int, dropout: float, **kwargs: Dict[str, Any]):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, **kwargs)\n\n    def forward(self, x: torch.Tensor, key_padding_mask: torch.BoolTensor = None) -> torch.Tensor:\n        x = x.transpose(0, 1)\n        attn = self.attention(query=x, key=x, value=x, key_padding_mask=key_padding_mask)[0]\n        attn = attn.transpose(0, 1)\n        return attn","b6e769f8":"class F1Score:\n    \"\"\"\n    Class for f1 calculation in Pytorch.\n    \"\"\"\n\n    def __init__(self, average: str = 'weighted'):\n        \"\"\"\n        Init.\n\n        Args:\n            average: averaging method\n        \"\"\"\n        self.average = average\n        if average not in [None, 'micro', 'macro', 'weighted']:\n            raise ValueError('Wrong value of average parameter')\n\n    @staticmethod\n    def calc_f1_micro(predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculate f1 micro.\n\n        Args:\n            predictions: tensor with predictions\n            labels: tensor with original labels\n\n        Returns:\n            f1 score\n        \"\"\"\n        true_positive = torch.eq(labels, predictions).sum().float()\n        f1_score = torch.div(true_positive, len(labels))\n        return f1_score\n\n    @staticmethod\n    def calc_f1_count_for_label(\n        predictions: torch.Tensor, labels: torch.Tensor, label_id: int\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Calculate f1 and true count for the label\n\n        Args:\n            predictions: tensor with predictions\n            labels: tensor with original labels\n            label_id: id of current label\n\n        Returns:\n            f1 score and true count for label\n        \"\"\"\n        # label count\n        true_count = torch.eq(labels, label_id).sum()\n\n        # true positives: labels equal to prediction and to label_id\n        true_positive = torch.logical_and(torch.eq(labels, predictions), torch.eq(labels, label_id)).sum().float()\n        # precision for label\n        precision = torch.div(true_positive, torch.eq(predictions, label_id).sum().float())\n        # replace nan values with 0\n        precision = torch.where(torch.isnan(precision), torch.zeros_like(precision).type_as(true_positive), precision)\n\n        # recall for label\n        recall = torch.div(true_positive, true_count)\n        # f1\n        f1 = 2 * precision * recall \/ (precision + recall)\n        # replace nan values with 0\n        f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1).type_as(true_positive), f1)\n        return f1, true_count\n\n    def __call__(self, predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculate f1 score based on averaging method defined in init.\n\n        Args:\n            predictions: tensor with predictions\n            labels: tensor with original labels\n\n        Returns:\n            f1 score\n        \"\"\"\n\n        # simpler calculation for micro\n        if self.average == 'micro':\n            return self.calc_f1_micro(predictions, labels)\n\n        f1_score = torch.tensor(0.0).type_as(predictions).float()\n        for label_id in labels.unique():\n            f1, true_count = self.calc_f1_count_for_label(predictions, labels, label_id)\n\n            if self.average == 'weighted':\n                f1_score += f1 * true_count\n            elif self.average == 'macro':\n                f1_score += f1\n\n        if self.average == 'weighted':\n            f1_score = torch.div(f1_score, len(labels))\n        elif self.average == 'macro':\n            f1_score = torch.div(f1_score, len(labels.unique()))\n\n        return f1_score","65312987":"class LayerNorm(nn.Module):\n    \"\"\"\n    Layer Normalization\n    paper: https:\/\/arxiv.org\/abs\/1607.06450\n    \"\"\"\n\n    def __init__(self, normalized_shape: int):\n        super(LayerNorm, self).__init__()\n        self.layer_norm = nn.LayerNorm(normalized_shape)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.layer_norm(x)","4c7a066f":"from torchcrf import CRF\nclass BiLSTMCRFAtt(nn.Module):\n    \"\"\"\n    New model without nn.Embedding layer\n    \"\"\"\n\n    def __init__(self, tag_to_idx: Dict, embeddings_dim: int = 100, hidden_dim: int = 4, spatial_dropout: float = 0.2):\n        super().__init__()\n        self.embedding_dim = embeddings_dim\n        self.hidden_dim = hidden_dim\n        self.tag_to_idx = tag_to_idx\n        self.tagset_size = len(tag_to_idx.values())\n        self.crf = CRF(self.tagset_size, batch_first=True)\n        self.embedding_dropout = SpatialDropout(spatial_dropout)\n\n        self.lstm = nn.LSTM(\n            embeddings_dim, hidden_dim \/\/ 2, num_layers=2, bidirectional=True, batch_first=True, dropout=0.25\n        )\n        # Maps the output of the LSTM into tag space.\n        self.hidden2tag = nn.Linear(hidden_dim, hidden_dim \/\/ 2)\n        self.hidden2tag2 = nn.Linear(hidden_dim \/\/ 2, self.tagset_size)\n        self.rnn_layer_norm = LayerNorm(hidden_dim)\n        self.att = MultiHeadSelfAttention(embed_dim=hidden_dim, num_heads=2, dropout=0.25)\n\n    def _get_lstm_features(self, embeds: torch.Tensor, lens: torch.Tensor, mask: bool) -> torch.Tensor:\n        \"\"\"\n        LSTM forward\n\n        Args:\n            embeds: batch with embeddings\n            lens: lengths of sequences\n        \"\"\"\n        embeds = self.embedding_dropout(embeds)\n        packed_embeds = torch.nn.utils.rnn.pack_padded_sequence(\n            embeds, lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n        lstm_out, self.hidden = self.lstm(packed_embeds)\n        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n        lstm_out = self.rnn_layer_norm(lstm_out)\n        lstm_out = self.att(lstm_out, key_padding_mask=mask)\n        lstm_feats = self.hidden2tag2(self.hidden2tag(lstm_out.reshape(embeds.shape[0], -1, self.hidden_dim)))\n        return lstm_feats\n\n    def forward(\n        self, embeds: torch.Tensor, lens: torch.Tensor, tags: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward\n\n        Args:\n            embeds: batch with embeddings\n            lens: lengths of sequences\n            tags: list of tags (optional)\n        \"\"\"\n        mask1 = tags == self.tag_to_idx['PAD']\n        lstm_feats = self._get_lstm_features(embeds, lens, mask1)\n\n        if tags is not None:\n            mask = tags != self.tag_to_idx['PAD']\n            loss: torch.Tensor = self.crf(lstm_feats, tags, mask=mask)\n            tag_seq = self.crf.decode(emissions=lstm_feats, mask=torch.tensor(mask))  # type: ignore\n\n        else:\n            loss = torch.tensor(0)\n            tag_seq = self.crf.decode(lstm_feats)\n\n        pred: torch.Tensor = torch.tensor([i for j in tag_seq for i in j]).type_as(embeds)\n        return pred, -loss","860f542e":"model = BiLSTMCRFAtt(tag_to_idx, 100, 32, 0.2)\nmodel","fa009492":"metric = F1Score(average='weighted')\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","6d56689b":"# model.cuda();\n# embedding.cuda();","dab62be8":"optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ntrain_f1 = []\nvalid_f1 = []\nfor epoch in range(20):\n    model.train()\n    epoch_train_f1 = []\n    for i, batch in enumerate(train_loader):\n        tokens, lens, labels = batch\n        # tokens, lens, labels = tokens.cuda(), lens.cuda(), labels.cuda()\n        optimizer.zero_grad()\n        \n        tag_seq, loss = model(embedding(tokens), lens, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        labels = labels.flatten()\n        labels = labels[labels != tag_to_idx['PAD']]\n        f1_score = metric(tag_seq, labels).item()\n        epoch_train_f1.append(f1_score)\n        \n    model.eval()\n    epoch_valid_f1 = []\n    with torch.no_grad():\n        for i, batch in enumerate(valid_loader):\n            tokens, lens, labels = batch\n            # tokens, lens, labels = tokens.cuda(), lens.cuda(), labels.cuda()\n\n            tag_seq, loss = model(embedding(tokens), lens, labels)\n            labels = labels.flatten()\n            labels = labels[labels != tag_to_idx['PAD']]\n            f1_score = metric(tag_seq, labels).item()\n            epoch_valid_f1.append(f1_score)\n    \n    mean_epoch_train_f1 = np.mean(epoch_train_f1)\n    mean_epoch_valid_f1 = np.mean(epoch_valid_f1)\n    train_f1.append(mean_epoch_train_f1)\n    valid_f1.append(mean_epoch_valid_f1)\n    print(f'Epoch: {epoch}. Train f1: {mean_epoch_train_f1:0.4f}. Valid f1:  {mean_epoch_valid_f1:0.4f}.')","157a5f15":"idx_to_word = {v: k for k, v in word_to_idx.items()}\nidx_to_tag = {v: k for k, v in tag_to_idx.items()}","294aab1a":"labels = labels.cpu().detach().numpy()\ntokens = tokens.cpu().detach().numpy()\ntag_seq = tag_seq.cpu().detach().numpy()","5c29e81e":"correct_labels = []\ncorrect_tag_seq = []\ncorrect_tokens = []\nfor token in tokens:\n    token = token[token != tag_to_idx['PAD']]\n    correct_tokens.append(token)\n    correct_labels.append(labels[:len(token)])\n    labels = labels[len(token):]\n    correct_tag_seq.append(tag_seq[:len(token)])\n    tag_seq = tag_seq[len(token):]","70998e7c":"for token, label, pred in zip(correct_tokens, correct_labels, correct_tag_seq):\n    if not all(label == pred):\n        label = label[token > 0]\n        pred = pred[token > 0]\n        token = token[token > 0]\n        for t_, l_, p_ in zip([idx_to_word[t] for t in token],\n                              [idx_to_tag[l] for l in label],\n                              [idx_to_tag[p] for p in pred]):\n            print(f'{t_}\\t{l_}\\t{p_}')\n        print('-' * 50)","966b5ecb":"### Rule-based\n\nThe most basic way of making predictions is without machine-learning - by using rules, regexes and any other things. Let's try!","454772d0":"### Bi-LSTM","7c69cf64":"Let's take a look at the first sentence. We see that it has two entities - Rating and Amenity.","3c206350":"### sklearn_crfsuite\n\nNow let's try using a model. sklearn_crfsuite library provides sklearn-like API for NER tasks. To pass the data into the model, we need to convert words in dictionaries with features.","72bd813c":"### Working with the data\n\nLet's have a look at the data. I'll print 30 rows of the dataset.","bb6b9842":"## General information\n\nThis notebook is used in my talk (in Russian Language) about practical aspects of working with Named Entity Recognition Task.\n\nHere is the link to the [talk](https:\/\/youtu.be\/WGZpsDSmHnE)\n\nThe content is the following:\n* working with different formats of NER labels;\n* using sklearn_crfsuite for NER;\n* using spacy for NER;\n* using Bi-LSTM for NER;\n\nI'll be using MIT restaurant dataset: https:\/\/groups.csail.mit.edu\/sls\/downloads\/restaurant\/","d8716878":"We can see that there are two columns - the tags and the words. Each word\/label is on a separate row; the sentences are separated by an empty row. This is one of the most common ways of storing NER datasets. Usually the words are in the first column, though.\n\nNow I'll create a new file with switched columns.\nAdditionally, I'll split the data into separate training and validation files (they will be used for training spacy models later).","fc568547":"### Spacy\n\nNow let's convert the data into spacy format and train a spacy model.","e948da85":"### To biluo","13f73147":"The references:\n* https:\/\/github.com\/susanli2016\/NLP-with-Python\/blob\/master\/NER_sklearn.ipynb\n* https:\/\/zachlim98.github.io\/me\/2021-03\/spacy3-ner-tutorial\n* https:\/\/medium.com\/@phylypo\/nlp-text-segmentation-using-conditional-random-fields-e8ff1d2b6060\n* http:\/\/www.davidsbatista.net\/blog\/2018\/05\/09\/Named_Entity_Evaluation\n* https:\/\/paperswithcode.com\/task\/named-entity-recognition-ner\n* data: https:\/\/groups.csail.mit.edu\/sls\/downloads\/restaurant\/\n* http:\/\/www.davidsbatista.net\/blog\/2018\/05\/09\/Named_Entity_Evaluation\/"}}