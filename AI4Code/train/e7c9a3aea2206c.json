{"cell_type":{"f4a4742d":"code","1091b19c":"code","a041eb4c":"code","cbd12fdb":"code","77ab50bb":"code","73bf2c93":"code","cfb79116":"code","a6f1d010":"code","6b9b488f":"code","2b3b2c69":"code","45318997":"code","c8eae7e3":"code","9e1f8efd":"code","a248342e":"code","c547ee83":"code","dc9c684d":"code","5b243f10":"code","b5526b83":"code","0a8b9e76":"code","b5c39154":"code","9c2ff9cf":"code","94e5b35c":"code","96142328":"code","b452f899":"code","2fd44d94":"code","9d4457a7":"code","8acb786e":"code","5372c2ad":"code","b4d2c606":"code","6287fa99":"code","d7bbb4f2":"code","ea954031":"code","31df443b":"markdown","97daf9cd":"markdown","8642c5cc":"markdown","d916ff9e":"markdown","72d77dd8":"markdown","7df0751a":"markdown","bfc0bdcb":"markdown","b0efd0a8":"markdown","25ad1cef":"markdown","4c29f169":"markdown","c1714c18":"markdown","562cbcf3":"markdown","bcd246e1":"markdown","221aaaf2":"markdown","5ba97f65":"markdown","4e00c631":"markdown","e345e419":"markdown","61420b9a":"markdown","e2d31f05":"markdown","c770590e":"markdown","dfe7878c":"markdown","664a3cd9":"markdown","08ef868c":"markdown","4e3efa2b":"markdown","8ac9c964":"markdown"},"source":{"f4a4742d":"#import packages\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport time \nimport datetime\nimport collections\n# from plotnine import *\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge \nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nimport xgboost as xgb\nimport lightgbm as lgb\n","1091b19c":"data_dir = '\/kaggle\/input\/zillow-prize-1\/'\ny_train_2016 = pd.read_csv(f\"{data_dir}train_2016_v2.csv\")\ny_train_2017 = pd.read_csv(f\"{data_dir}train_2017.csv\")\nX_train_2016 = pd.read_csv(f\"{data_dir}properties_2016.csv\")\nX_train_2017 = pd.read_csv(f\"{data_dir}properties_2017.csv\")\ndf_submission = pd.read_csv(f\"{data_dir}sample_submission.csv\")\n\nprint(f\"\\nX_train_2016 shape: {X_train_2016.shape}\\ny_train_2016 shape: {y_train_2016.shape}\\n2017 X_train shape: {X_train_2017.shape} \\\n      \\n y_train_2017 shape: {y_train_2017.shape}\\ntest_data shape:{df_submission.shape}\")\ndel df_submission","a041eb4c":"data_type = X_train_2016.dtypes\ndata_cnt = X_train_2016.nunique()\nprint(f\"frequencey count of differnet types:{collections.Counter(data_type)}\")\n\ndata_summary = pd.concat([data_type,data_cnt],axis=1)\ndata_summary.columns = ['dtype','unique_cnt']\nprint(data_summary.sort_values(['dtype','unique_cnt']))","cbd12fdb":"#object type deepdive\ncol_object = data_summary.loc[data_summary['dtype']=='object',:].index\nX_train_2016.loc[:,col_object].describe()","77ab50bb":"#convert boolean type to integer\nX_2016_clean = X_train_2016.copy()\nX_2016_clean[['hashottuborspa','fireplaceflag']] = X_2016_clean[['hashottuborspa','fireplaceflag']].fillna(value=0).astype(int)\nX_2016_clean.loc[X_2016_clean.taxdelinquencyflag=='Y','taxdelinquencyflag'] = 1.0\nX_2016_clean.loc[X_2016_clean.taxdelinquencyflag!='Y','taxdelinquencyflag'] = 0.0\ncol_str = ['propertycountylandusecode','propertyzoningdesc']\nfor col in col_str:\n    uniq_val = X_2016_clean[col].unique()\n    df_encoding = pd.DataFrame(np.arange(len(uniq_val)),columns=[col+'_e'])\n    df_encoding[col] = uniq_val\n    X_2016_clean = pd.merge(X_2016_clean, df_encoding, on=[col], how='left')\n    X_2016_clean.drop(col,axis=1,inplace=True)\n    X_2016_clean.rename(columns={col+\"_e\":col},inplace=True)\n\n#convert boolean type to integer\nX_2017_clean = X_train_2017.copy()\nX_2017_clean[['hashottuborspa','fireplaceflag']] = X_2017_clean[['hashottuborspa','fireplaceflag']].fillna(value=0).astype(int)\nX_2017_clean.loc[X_2017_clean.taxdelinquencyflag=='Y','taxdelinquencyflag'] = 1.0\nX_2017_clean.loc[X_2017_clean.taxdelinquencyflag!='Y','taxdelinquencyflag'] = 0.0\ncol_str = ['propertycountylandusecode','propertyzoningdesc']\nfor col in col_str:\n    uniq_val = X_2017_clean[col].unique()\n    df_encoding = pd.DataFrame(np.arange(len(uniq_val)),columns=[col+'_e'])\n    df_encoding[col] = uniq_val\n    X_2017_clean = pd.merge(X_2017_clean, df_encoding, on=[col], how='left')\n    X_2017_clean.drop(col,axis=1,inplace=True)\n    X_2017_clean.rename(columns={col+\"_e\":col},inplace=True)\n\ndel X_train_2016, X_train_2017\n","73bf2c93":"data_summary['missing_rate'] = 1 - X_2016_clean.count()\/X_2016_clean.shape[0]\n#missing rate when joined with target\ntrain_data = pd.merge(y_train_2016, X_2016_clean, on=['parcelid'],how='left')\nmr = 1-train_data.count()\/train_data.shape[0]\nmr.name = 'missing_rate_sample'\n# data_summary.drop('missing_rate_sample', axis=1, inplace=True)\ndata_summary = pd.concat([data_summary, mr],axis=1)\n\nmr = data_summary.copy()\nmr.loc[mr.missing_rate==0,'mr_type'] = 'No Missing'\nmr.loc[mr.missing_rate>0,'mr_type'] = '(0,0.1]'\nmr.loc[mr.missing_rate>0.1,'mr_type'] = '(0.1,0.5]'\nmr.loc[mr.missing_rate>0.5,'mr_type'] = '(0.5,0.9]'\nmr.loc[mr.missing_rate>0.9,'mr_type'] = '(0.9, 1)'\ndf_plt = mr.groupby('mr_type')['dtype'].count()\nax = df_plt.plot(kind='bar')\nax.set_ylabel('No. of Features')\nax.set_xlabel('missing rate range')\nfor i, v in enumerate(df_plt):\n    ax.text(i,v + 0.5,str(v), color='blue', fontweight='bold')\n# data_summary.sort_values('missing_rate_sample',ascending=False)","cfb79116":"df_impute = mr.loc[mr.mr_type.isin(['(0.5,0.9]','(0.1,0.5]']),:]\nX_2016_clean[df_impute.index].describe().T","a6f1d010":"#impute missing count without 0 with 0\ncol1 = list(set(list(df_impute.index.values)) - \nset([c for c in df_impute.index if 'garage' in c]+\n    ['regionidneighborhood','numberofstories','unitcnt','airconditioningtypeid','buildingqualitytypeid','heatingorsystemtypeid'])) \nX_2016_clean[col1] = X_2016_clean[col1].fillna(value=0)\n\n#fill pool id and area with 0 if poolcnt = 0\ncol1 = ['pooltypeid2','pooltypeid7','pooltypeid10','poolsizesum']\nX_2016_clean.loc[X_2016_clean.poolcnt==0,col1] = 0","6b9b488f":"data_summary['missing_rate'] = 1 - X_2016_clean.count()\/X_2016_clean.shape[0]\n#missing rate when joined with target\ntrain_data = pd.merge(y_train_2016, X_2016_clean, on=['parcelid'],how='left')\nmr = 1-train_data.count()\/train_data.shape[0]\nmr.name = 'missing_rate_sample'\ndata_summary.drop('missing_rate_sample', axis=1, inplace=True)\ndata_summary = pd.concat([data_summary, mr],axis=1)\n\nmr = data_summary.copy()\nmr.loc[mr.missing_rate==0,'mr_type'] = 'No Missing'\nmr.loc[mr.missing_rate>0,'mr_type'] = '(0,0.1]'\nmr.loc[mr.missing_rate>0.1,'mr_type'] = '(0.1,0.5]'\nmr.loc[mr.missing_rate>0.5,'mr_type'] = '(0.5,0.9]'\nmr.loc[mr.missing_rate>0.9,'mr_type'] = '(0.9, 1)'\ndf_plt = mr.groupby('mr_type')['dtype'].count()\nax = df_plt.plot(kind='bar')\nax.set_ylabel('No. of Features')\nax.set_xlabel('missing rate range')\nfor i, v in enumerate(df_plt):\n    ax.text(i,v + 0.5,str(v), color='blue', fontweight='bold')\n# data_summary.sort_values('missing_rate_sample',ascending=False)","2b3b2c69":"#count over time\ndate_col ='transactiondate'\ny_train_2016['date_'] = y_train_2016[date_col].apply(lambda x: pd.Timestamp(x))\ny_train_2017['date_'] = y_train_2017[date_col].apply(lambda x: pd.Timestamp(x))\n\nfig, ax = plt.subplots(2,1,figsize=(10,10))\nys = [y_train_2016, y_train_2017]\ntitles = ['Transanction Volume over Time in 2016', 'Transaction Volume over Time in 2017']\nfor i, y in enumerate(ys):\n    plti = y.groupby('date_')['logerror'].count()\n    ax[i].plot(plti.index, plti.values)\n    ax[i].set_title(titles[i])\n    ax[i].set_ylabel('Transaction Count')\n    \n    ","45318997":"#boxplot over time\nfig, ax = plt.subplots(2,1,figsize=(10,10))\nys = [y_train_2016, y_train_2017]\ntitles = ['Logerror Distribution over Time in 2016', 'Log Error Distribution over Time in 2017']\nfor i, y in enumerate(ys):\n    y['ym'] = y.date_.apply(lambda x: x.year*100 + x.month)\n    sns.boxplot(x='ym',y='logerror', data=y, ax=ax[i])\n    ax[i].set_title(titles[i])\n    ax[i].set_ylabel('Transaction Count')\n","c8eae7e3":"transaction_2016_cnt = y_train_2016.groupby('parcelid')['transactiondate'].count()\ntransaction_2017_cnt = y_train_2017.groupby('parcelid')['transactiondate'].count()\nmultiple_2016 = transaction_2016_cnt[transaction_2016_cnt>1]\nmultiple_2017 = transaction_2017_cnt[transaction_2017_cnt>1]\nprint(f\"{len(multiple_2016)+len(multiple_2017)} properties have multiple transactions within 2016 or 2017\")\n","9e1f8efd":"#raw historgram\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nax[0].hist(y_train_2016.logerror,500)\nax[0].set_title('Histogram of log error in 2016')\nax[0].set_ylabel('Count')\nax[0].set_xlabel('logerror')\nax[1].hist(y_train_2017.logerror,500)\nax[1].set_title('Histogram of log error in 2017')\nax[1].set_ylabel('Count')\nax[1].set_xlabel('logerror')","a248342e":"#winsorize\ndef winsorize(df,date_col,data_col,limit=[0.01,0.99]):\n    df['ym'] = df[date_col].apply(lambda x: x.year*100 + x.month)\n    df[data_col+'_wc_'+str(limit[0])] = df.groupby('ym')[data_col].transform(lambda x: x.clip(*x.quantile(limit)))\n    df[data_col+'_w_'+str(limit[0])] = df[data_col].transform(lambda x: x.clip(*x.quantile(limit)))\n    return df\ny_train_2016 = winsorize(y_train_2016,'date_','logerror',[0.01,0.99])\ny_train_2017 = winsorize(y_train_2017,'date_','logerror',[0.01,0.99])\n\n#outlier transform\nfig, ax = plt.subplots(2,2,figsize=(15,10))\nax = ax.ravel()\nax[0].hist(y_train_2016['logerror_w_0.01'],100)\nax[0].set_title('Histogram of winsorized logerror in 2016')\nax[0].set_ylabel('Count')\nax[0].set_xlabel('winsorized at [0.01,0.99] logerror')\nax[1].hist(y_train_2017['logerror_w_0.01'],100)\nax[1].set_title('Histogram of winsorized logerror in 2017')\nax[1].set_ylabel('Count')\nax[1].set_xlabel('winsorized at [0.01,0.99] logerror')\nax[2].hist(y_train_2016['logerror_wc_0.01'],100)\nax[2].set_title('Histogram of winsorized logerror in 2016')\nax[2].set_ylabel('Count')\nax[2].set_xlabel('winsorized by ym at [0.01,0.99] logerror')\nax[3].hist(y_train_2017['logerror_wc_0.01'],100)\nax[3].set_title('Histogram of winsorized logerror in 2017')\nax[3].set_ylabel('Count')\nax[3].set_xlabel('winsorized by ym at [0.01,0.99] logerror')\n","c547ee83":"y_train_2016 = winsorize(y_train_2016,'date_','logerror',[0.05,0.95])\ny_train_2017 = winsorize(y_train_2017,'date_','logerror',[0.05,0.95])\nfig, ax = plt.subplots(2,2,figsize=(15,10))\nax = ax.ravel()\nax[0].hist(y_train_2016['logerror_w_0.05'],100)\nax[0].set_title('Histogram of winsorized logerror in 2016')\nax[0].set_ylabel('Count')\nax[0].set_xlabel('winsorized at [0.05,0.95] logerror')\nax[1].hist(y_train_2017['logerror_w_0.05'],100)\nax[1].set_title('Histogram of winsorized logerror in 2017')\nax[1].set_ylabel('Count')\nax[1].set_xlabel('winsorized at [0.01,0.99] logerror')\nax[2].hist(y_train_2016['logerror_wc_0.05'],100)\nax[2].set_title('Histogram of winsorized logerror in 2016')\nax[2].set_ylabel('Count')\nax[2].set_xlabel('winsorized by ym at [0.05,0.95] logerror')\nax[3].hist(y_train_2017['logerror_wc_0.05'],100)\nax[3].set_title('Histogram of winsorized logerror in 2017')\nax[3].set_ylabel('Count')\nax[3].set_xlabel('winsorized by ym at [0.05,0.95] logerror')\n","dc9c684d":"from sklearn import mixture \nvname = 'logerror_wc_0.01'\ntitles = ['GMM of 2016 '+vname,'GMM of 2017'+vname]\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nfor i, y in enumerate(ys):\n        \n    f = y[[vname]]\n\n    g = mixture.GaussianMixture(n_components=3,covariance_type='full')\n    g.fit(f)\n    weights = g.weights_\n    means = g.means_\n    covars = g.covariances_\n\n    ax[i].hist(f, bins=100, histtype='bar', density=True, ec='red', alpha=0.5)\n    f_axis = f.copy()\n    f_axis.sort_values(vname,inplace=True)\n    ax[i].plot(f_axis,weights[0]*stats.norm.pdf(f_axis,means[0][0],np.sqrt(covars[0][0])).ravel(), c='blue')\n    ax[i].plot(f_axis,weights[1]*stats.norm.pdf(f_axis,means[1][0],np.sqrt(covars[1][0])).ravel(), c='green')\n    ax[i].plot(f_axis,weights[2]*stats.norm.pdf(f_axis,means[2][0],np.sqrt(covars[2][0])).ravel(), c='m')\n    ax[i].set_title(titles[i])","5b243f10":"vname = 'logerror_wc_0.01'\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nax = ax.ravel()\ntitles = ['Multiple vs Unique Transactions of 2016 '+vname,'Multiple vs Unique Transactions of 2017'+vname]\nfor i, y in enumerate(ys):\n    cnt = y.groupby('parcelid')['transactiondate'].count()\n    idx = cnt[cnt>1].index\n    f = y[[vname]]\n    f1 = y.loc[y.parcelid.isin(idx),vname]\n    f2 = y.loc[~y.parcelid.isin(idx),vname]  \n    ax[i].hist(f, bins=100, histtype='bar', density=True, ec='red', alpha=0.5)\n    f_axis = f.copy()\n    f_axis.sort_values(vname,inplace=True)\n    ax[i].plot(f_axis,stats.norm.pdf(f_axis,np.mean(f1),np.std(f1)).ravel(), c='blue',label='multiple transactions')\n    ax[i].plot(f_axis,stats.norm.pdf(f_axis,np.mean(f2),np.std(f2)).ravel(), c='green',label='unique transaction')    \n    ax[i].set_title(titles[i])\n    ax[i].legend()","b5526b83":"col_categorical = col_str + [c for c in X_2016_clean.columns if 'typeid' in c or 'region' in c] +\\\n['fips','rawcensustractandblock','censustractandblock']\ncol_rest = list(set(X_2016_clean.columns) - set(col_categorical))\ncol_rest = [c for c in col_rest if data_summary.loc[c,'unique_cnt']==1]\ncol_categorical = col_categorical + col_rest\nprint(f'{len(col_categorical)} categorical featrues identified')\ncol_categorical_filtered = [c for c in col_categorical if data_summary.loc[c,'missing_rate']<=0.5]\nprint(f'{len(col_categorical_filtered)} categorical features after filtering out high missing rate.')\nl1 = X_2016_clean.shape[1] \ndata_summary.loc[col_categorical_filtered,:].sort_values('unique_cnt')\n","0a8b9e76":"#Frequency chart\ncol_drop = ['propertyzoningdesc','censustractandblock','rawcensustractandblock']\ncol_categorical_filtered = [c for c in col_categorical_filtered if c not in col_drop]\nprint(f'{len(col_categorical_filtered)} categorical features after filtering out features with too many unique values.')\nfig, ax =plt.subplots(3,5, figsize=(30,15))\nax = ax.ravel()\nfor i, c in enumerate(col_categorical_filtered):\n    print(f\"{c} start.\")\n    df_plt = X_2016_clean.loc[X_2016_clean[c].notnull(),:]\n    df_br= df_plt.groupby(c)['parcelid'].count()\n    bri = np.arange(df_br.shape[0])\n    ax[i].bar(bri, df_br.values)\n    if data_summary.loc[c,'unique_cnt']<100:\n        ax[i].set_xticks(bri)\n        ax[i].set_xticklabels(df_br.index,rotation=45)\n        ax[i].set_title(c)\n#     df_plt = X_2016_clean.loc[X_2016_clean[c].notnull(),:]\n#     ax[i].hist(df_plt[c])\n#     ax[i].set_title(c)\ncol_drop =  ['pooltypeid10','pooltypeid2','fireplaceflag','taxdelinquencyflag']\ncol_categorical_filtered = [c for c in col_categorical_filtered if c not in col_drop]\nprint(f'{len(col_categorical_filtered)} categorical features after filtering out features with highly imbalanced class distrubtion.')\ndel df_plt, df_br","b5c39154":"# import category_encoders as ce\n\n# c = 'propertycountylandusecode'\n\n# # dummy_i = pd.get_dummies(X_2016_clean[c])\n# encoder=ce.HashingEncoder(cols=[c],n_components=100)\n# dummy_i = encoder.fit_transform(X_2016_clean.iloc[1:500000])","9c2ff9cf":"# c = 'propertycountylandusecode'\n# encoder=ce.HashingEncoder(cols=c,n_components=50)\n# dummy_i = encoder.fit_transform(X_2016_clean)","94e5b35c":"dummies = []\nfor c in col_categorical_filtered:\n    cnt = X_2016_clean[c].nunique()\n    if cnt <100:\n        print(f\"dummy for {c} created.\")\n        dummy_i = pd.get_dummies(X_2016_clean[c], prefix=f'dummy_{c}')\n        dummy_i = dummy_i.iloc[:,:-1]\n        dummies.append(dummy_i)\ndummies = pd.concat(dummies, axis=1)\nX_2016_clean = pd.concat([X_2016_clean,dummies], axis=1)\nl2 = X_2016_clean.shape[1]\nprint(f\"{l2-l1} categorical dummies created.\")","96142328":"# fig, ax =plt.subplots(4,5, figsize=(30,20))\n# ax = ax.ravel()\n\n\n# for i, c in enumerate(col_categorical_filtered):\n#     print(f\"{c} start.\")\n#     df_plt = train_data.loc[train_data[c].notnull(),:]\n#     df_br= df_plt.groupby(c)['logerror_wc_0.01'].median()\n#     bri = np.arange(df_br.shape[0])\n#     ax[i].scatter(bri, df_br.values)\n#     if data_summary.loc[c,'unique_cnt']<100:\n#         ax[i].set_xticks(bri)\n#         ax[i].set_xticklabels(df_br.index,rotation=45)\n#     ax[i].set_title(c)","b452f899":"# col_categorical_filtered = [c for c in col_categorical_filtered if c not in ['pooltypeid2','pooltypeid7','pooltypeid10','fireplaceflag','taxdelinquencyflag']]\n# print(f\"Drop uninformative features. Left with {len(col_categorical_filtered)} categorical features.\")\n# fig, ax = plt.subplots(figsize=(15, 15)) \n# mask = np.zeros_like(train_data[col_categorical_filtered].corr())\n# mask[np.triu_indices_from(mask)] = 1\n# sns.heatmap(train_data[col_categorical_filtered].corr(method='spearman'), mask= mask, ax= ax, annot= True)","2fd44d94":"col_dummy = [c for c in X_2016_clean if 'dummy' in c]\ncol_numeric = list(set(X_2016_clean.columns) - set(col_categorical+col_dummy))\n\nprint(f'{len(col_numeric)} numeric featrues identified')\ncol_numeric_filtered = [c for c in col_numeric if data_summary.loc[c,'missing_rate']<0.5]\nprint(f'{len(col_numeric_filtered)} numeric features after filtering out high missing rate.')\ndata_summary.loc[col_numeric,:]","9d4457a7":"#Histogram in full sample\nfig, ax =plt.subplots(4,6, figsize=(30,20))\nax = ax.ravel()\nfor i, c in enumerate(col_numeric_filtered):\n#     print(f\"{c} start.\")\n    df_plt = X_2016_clean.loc[X_2016_clean[c].notnull(),:]\n    ax[i].hist(df_plt[c])\n    ax[i].set_title(c)","8acb786e":"df_summary = X_2016_clean[col_numeric_filtered].describe()\ndf_summary.T","5372c2ad":"# correlation time series with y\ndate_col = 'transactiondate'\ntrain_data['date_'] = train_data[date_col].apply(lambda x: pd.Timestamp(x))\n# train_data['ym'] = train_data['date_'].apply(lambda x:x.year*100+x.month)\ntrain_data = winsorize(train_data,'date_','logerror',[0.01,0.99])\ncorr1 = train_data.groupby('ym').corr(method='spearman').reset_index()\ncorr1 = corr1.loc[corr1.level_1=='logerror_wc_0.01',['ym']+col_numeric_filtered]\ncorr1['date_'] = corr1['ym'].apply(lambda x: pd.Timestamp(f\"{str(x)[0:4]}-{str(x)[4:6]}-01\"))\nfig, ax =plt.subplots(4,6, figsize=(30,20))\nax = ax.ravel()\n\nfor i, c in enumerate(col_numeric_filtered):\n    ax[i].plot(corr1.date_, corr1[c].values)\n    ax[i].set_title(c)\n    ax[i].set_ylim([-0.1,0.1])\n    ax[i].axhline(y=0,color='r',linestyle='--')","b4d2c606":"#correlation with each other\ncol_numeric_filtered = [c for c in col_numeric_filtered if c!='assessmentyear']\nfig, ax = plt.subplots(figsize=(15, 15)) \nmask = np.zeros_like(train_data[col_numeric_filtered].corr())\nmask[np.triu_indices_from(mask)] = 1\nsns.heatmap(train_data[col_numeric_filtered].corr(method='spearman'), mask= mask, ax= ax, annot= True)","6287fa99":"# #clustering of features\n# X = train_data[col_numeric_filtered]\n# # setting distance_threshold=0 ensures we compute the full tree.\n# model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\n# model = model.fit(X)\n# plt.title(\"Hierarchical Clustering Dendrogram: Numerical Features\")\n# # plot the top three levels of the dendrogram\n# plot_dendrogram(model, truncate_mode=\"level\", p=3)\n# plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n# plt.show()","d7bbb4f2":"vname = 'logerror_wc_0.01'\ntrain_data_temp = train_data[col_numeric_filtered+[vname]]\ntrain_data_filtered = train_data_temp.dropna(how='any', axis=0)\nprint(f\"data size reduced from {train_data.shape[0]} to {train_data_filtered.shape[0]}\")\nX = train_data_filtered[col_numeric_filtered]\nY = train_data_filtered['logerror_wc_0.01']\nrf = RandomForestRegressor(max_depth=8)\nrf.fit(X, Y)\ny_pred = rf.predict(X)\nprint(\"Features sorted by their score:\")\ndf_importance = pd.DataFrame(rf.feature_importances_,index=col_numeric_filtered,columns=['importance'])\nax = df_importance.sort_values('importance').plot(kind='barh')\nax.set_title('Feature Selection vis RF for Numeric Variables')\n# print(f'oob score is {rf.oob_score_}')\nprint(f\"In sample MAE: {mean_absolute_error(Y, y_pred)}\")","ea954031":"# #winsorize\n# def winsorize(df,date_col,data_col,limit=[0.01,0.99]):\n#     df['ym'] = df[date_col].apply(lambda x: x.year*100 + x.month)\n#     df[data_col+'_wc_'+str(limit[0])] = df.groupby('ym')[data_col].transform(lambda x: x.clip(*x.quantile(limit)))\n#     df[data_col+'_w_'+str(limit[0])] = df[data_col].transform(lambda x: x.clip(*x.quantile(limit)))\n#     return df\n# train_data['date_'] = train_data[date_col].apply(lambda x: pd.Timestamp(x))\n# train_data = winsorize(train_data,'date_','logerror',[0.01,0.99])\n\n# col1 = list(set(list(df_impute.index.values)) - \n# set([c for c in df_impute.index if 'garage' in c]+\n#     ['regionidneighborhood','numberofstories','unitcnt','airconditioningtypeid','buildingqualitytypeid','heatingorsystemtypeid'])) \n# X_2017_clean[col1] = X_2017_clean[col1].fillna(value=0)\n# #fill pool id and area with 0 if poolcnt = 0\n# col1 = ['pooltypeid2','pooltypeid7','pooltypeid10','poolsizesum']\n# X_2017_clean.loc[X_2017_clean.poolcnt==0,col1] = 0\n\n# train_data_2017 = pd.merge(y_train_2017, X_2017_clean, on=['parcelid'],how='left')\n# train_data_2017_tmp = train_data_2017[col_numeric_filtered+col_categorical_filtered+['logerror']]\n# train_data_2017_filtered = train_data_2017_tmp.dropna(how='any', axis=0)\n# y_test = train_data_2017_filtered['logerror']\n# X_test = train_data_2017_filtered[col_numeric_filtered+col_categorical_filtered]\ntrain_data = pd.merge(y_train_2016, X_2016_clean, on=['parcelid'],how='left')\nvname = 'logerror_wc_0.01'\ncol_keep = col_numeric_filtered+[vname]\ncol_dummy = [c for c in train_data.columns if \"dummy_\" in c]\ncolnames = col_keep + col_dummy\ntrain_data_temp = train_data[colnames]\ntrain_data_temp = train_data_temp.fillna(-999)\ntrain_data_filtered = train_data_temp.dropna(how='any', axis=0)\nprint(f\"data size reduced from {train_data.shape[0]} to {train_data_filtered.shape[0]}\")\nX = train_data_filtered[col_numeric_filtered+col_dummy]\nY = train_data_filtered[vname]\nrf = RandomForestRegressor(max_depth=8)\nrf.fit(X, Y)\ny_pred = rf.predict(X)\nprint(\"Features sorted by their score:\")\ndf_importance = pd.DataFrame(rf.feature_importances_,index=col_numeric_filtered+col_dummy,columns=['importance'])\nax = df_importance.sort_values('importance').plot(kind='barh',figsize=(10,10))\nax.set_title('Feature Selection vis RF for All Variables')\n# print(f'oob score is {rf.oob_score_}')\nprint(f\"Initial MAE: {mean_absolute_error(Y, y_pred)}\")","31df443b":"# EDA\n","97daf9cd":"<a id=\"3\"><\/a>\n## Features","8642c5cc":"<a id=\"1.2\"><\/a>\n### Data Size ","d916ff9e":"<a id=\"1.1\"><\/a>\n### Raw Data Structure\n  - raw features include properties features in 2016 and 2017\n  - target data includes house transaction data with time stamp and log error between transaction price and zestimate in 2016 and 2017\n  - submission data: list of properties to predict.\n","72d77dd8":"<a id=\"2.1\"><\/a>\n### Time Series\n* Clear Seasonality in Volume, seasonal dummy as features\n* Outliers each month\n* Multiple transaction per properties: 320\n","7df0751a":"<!-- <a id=\"3.1.3\"><\/a>\n#### Correlation within X -->","bfc0bdcb":"<a id=\"3.1.1\"><\/a>\n#### Distribution\n* Four variables have very high class imabalance or only one value for all observations, should be dropped: pooltypeid10,pooltypeid2,fireplaceflag,taxdelinquencyflag","b0efd0a8":"<a id=\"1.3\"><\/a>\n### Feature Types\n* 52 float, 1 integer and 5 object type\n* Many features have small number of unique values, likely to be categorical variables\n* Based on data dictionary and variable names, there are quite a few cateogrical variables related geographic location too. We need to encode cateogrical variables if we want to use linear regression. \n* Encode string values using numeric integer code","25ad1cef":"* How many properties have more than one transactions in 2016\/2017?\n","4c29f169":"<a id=\"1.4\"><\/a>\n### Missing Rate and Null Treatment\n* Missing Rate: 17 features with more than 90% missing, could drop these. \n* Null treatment: 9 features with significant missing rate between 0.5 to 0.9, worth deepdive to identify appropriate imputation strategy: impute 4 count variable missing to 0.","c1714c18":"<a id=\"2.3\"><\/a>\n### Conditional Distribution\n* Simple 1-d Gaussian Mixture Model ","562cbcf3":"<a id=\"2\"><\/a>\n## Target Variable\n* Clearly there are quite a bit of outliers in target variable","bcd246e1":"* Anything special for properties with multiple transactions?","221aaaf2":"<a id=\"3.2.2\"><\/a>\n#### Correlation with Y","5ba97f65":"<a id=\"3.2.1\"><\/a>\n#### Distribution\n* Many distributions indicate outliers. Some might need winsorization depending on the model we use. These include: unitcnt, finishedsquarefeet2, structuretaxvaluedollarcnt, landtaxvaluedollarcnt, calculatedfinishedsquarefeet, taxamount,landtaxvaluedollarcnt,lotsizesquarefeet, taxvaluedollarcnt, roomcnt.","4e00c631":"<a id=\"2.2\"><\/a>\n### Unconditional Distribution \n* comparison between raw and winsorized data\n* winsorize by yearmon at different threshold","e345e419":"<!-- <a id=\"3.1.2\"><\/a>\n#### Correlation with Y -->","61420b9a":"<a id=\"1\"><\/a>\n## Overview","e2d31f05":"<a id=\"3.2.3\"><\/a>\n#### Correlation within X","c770590e":"<a id=\"3.2.4\"><\/a>\n#### Feature Importance via RF","dfe7878c":"## Tasks\n* Clean up steps: further imputation of missing values, rename variables? - DM, code up data_clean function - XH\n* External data: Zip code, Census data, AHS data - SC, other external datasets - DM, XH \n* Model design: stage 2 discussion","664a3cd9":"<a id=\"3.1\"><\/a>\n### Catgorical Features\n* Filter by Missing Rate: three categorical variables with many unique values, drop ['propertyzoningdesc','censustractandblock','rawcensustractandblock']\n* Distribution: class imbalance, drop ['pooltypeid10','pooltypeid2','fireplaceflag','taxdelinquencyflag']\n* Encode categorical variables with One Hot Encoder. \n<!-- * Relationship with Target: scatter plot of group median\n# * Relationship with each other: correlation, hierarchical clustering (gower taking too long), remove highly correlated ones. \n# * Feature Importance: RF\n#  -->","08ef868c":"<a id=\"3.2\"><\/a>\n### Numeric Features\n\n* Filter by Missing Rate\n* Distribution: histogram, summary table: outlier, needs winsorize\n* Relationship with Target: time series correlation stable.\n* Relationship with each other: correlation, remove highly correlated ones\n* Feature Importance: RF\n","4e3efa2b":"<a id=\"4\"><\/a>\n## New Features\n\n* Interaction of important features: location interaction with volume, location interaction with area\n* Higher order ones\n* Seasonal dummies: seasonal volumes\n* External data: AHS survey: https:\/\/www.census.gov\/programs-surveys\/ahs.html\n* Expanding location related features\n* Include macroeconomic features: mortgage rates","8ac9c964":"# Table of Content\n* [Overview](#1)\n    - [Raw Data Structure](#1.1)\n    - [Data Size](#1.2)\n    - [Feature Type](#1.3)\n    - [Missing Rate and Null Treatment](#1.4) \n* [Target Variable](#2)\n    - [Time Series](#2.1)\n    - [Unconditional Distribution](#2.2)\n    - [Conditional Distribution](#2.3)\n* [Features](#3)\n    - [Categorical Features](#3.1)\n        - [Distribution](#3.1.1)\n        - [Correlation with Y](#3.1.2)\n        - [Correlation within X](#3.1.3)\n        - [Feature Importance via RF](#3.1.4)        \n    - [Numerical Features](#3.2)\n        - [Distribution](#3.2.1)\n        - [Correlation with Y](#3.2.1)\n        - [Correlation within X](#3.2.2)\n        - [Feature Importance via RF](#3.2.3)\n* [New Features](#4)\n \n "}}