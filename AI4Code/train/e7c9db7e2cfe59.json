{"cell_type":{"e8d50789":"code","e389ed15":"code","8c427a4e":"code","5665c15a":"code","c9f86637":"code","0de314b2":"code","26f14858":"code","b065bbb5":"code","78ac75b4":"code","46e33bd1":"code","646a9a7a":"code","cbd78489":"code","02e37451":"code","937ef215":"code","5d7c0bde":"code","2c93cb81":"code","7e93508d":"code","ead3c5d3":"code","a2b1d56f":"markdown","50a79b6a":"markdown","6fd03a89":"markdown","744ab8a7":"markdown","722d7a02":"markdown","1da66933":"markdown","e6cd8c85":"markdown"},"source":{"e8d50789":"import tensorflow as tf\nfrom keras.optimizers import adam\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,Conv2D, MaxPooling2D,LeakyReLU,GRU,LSTM,SimpleRNN,Reshape\nfrom keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\nfrom tensorflow.keras.initializers import he_uniform\nfrom keras.constraints import max_norm","e389ed15":"def txt_extract(names_data,path):\n    data_list=[]\n    for i in names_data:\n        sentence = \"\"\n        data_path = path + str(i)+\".txt\"\n        text= open(data_path,'r')\n        for x in text:\n            sentence+=str(x)\n        data_list.append(sentence)\n        \n        text.close()\n    return data_list","8c427a4e":"def relation_extractor(data):\n    full_extraction=[]\n    for i in data:\n        data_aux=\"\"\n        partial_extraction=[]\n        data_aux=i.replace(\"'\",\"\")\n        data_aux=data_aux.replace(\"[\",\"\")\n        data_aux=data_aux.replace(\"]\",\"\")\n        data_aux=data_aux.replace(\"->\",\",\")\n        data_aux=data_aux.replace(\" \",\"\")\n        data_aux=data_aux.split(\"\\n\")\n        for j in data_aux:\n            if 'ROOT' in j:\n                continue\n            else:\n                list_aux= j.split(\",\")\n                partial_extraction.append(list_aux)\n        full_extraction.append(partial_extraction)\n    return full_extraction\ndef train_data_transform(data):\n    for x in range(len(data)):\n        for y in range(len(data[x])):\n            for z in range(len(data[x][y])):\n                if data[x][y][z]!='':\n                    data[x][y][z] = float(data[x][y][z])\n                else:\n                    data[x][y][z]=0\n    return data\ndef zeros_padding(data):\n    largo_data = shape_calculator(data)\n    len_max = max_array(data)\n    new_data = np.zeros((largo_data,6))\n    x=0\n    for i in range(len(data)):\n        for j in range(len(data[i])):\n            y=0\n            for k in range(len(data[i][j])):\n                new_data[x][y]=data[i][j][k]\n                \n                y+=1\n            x+=1\n    return new_data\ndef max_array(data):\n    largo_max=0\n    for i in data:\n        if len(i)>largo_max:\n            largo_max=len(i)\n        else:\n            continue\n    return largo_max\ndef shape_calculator(data):\n    suma=0\n    for i in data:\n        suma+=len(i)\n    return suma\ndef X_data_import(names_data,path):\n    X = txt_extract(names_data,path)\n    X = relation_extractor(X)\n    X = train_data_transform(X)\n    X = np.array(X)\n    X=zeros_padding(X)\n    return X","5665c15a":"def txt_original_extract(name):\n    original_list=[]\n    data=open(name,'r')\n    data_read=[line.rstrip('\\n') for line in data]\n    for x in data_read:\n        aux_list=[]\n        x_aux=x.split(\"\\t\",1)\n        original_id=x_aux[0]\n        x_aux.remove(original_id)\n        x_aux = \" \".join(str(w) for w in x_aux)\n        aux_list.append(original_id)\n        aux_list.append(x_aux)\n        original_list.append(aux_list)\n    data.close()\n    return original_list","c9f86637":"def cat_encode(data,cat):\n    data=list(data)\n    cat=list(cat)\n    for i in range(len(data)):\n        for j in range(len(cat)):\n            if data[i]==cat[j]:\n                data[i]=j\n                break\n            else:\n                continue\n    return keras.utils.to_categorical(data,len(cat))\n    #return np.asarray(data)\ndef cat_decode(data,cat):\n    #Y=data\n    Y= [np.argmax(y, axis=None, out=None) for y in data]\n    cat=list(cat)\n    \n    for i in range(len(Y)):\n        Y[i]=cat[Y[i]]\n    return Y","0de314b2":"def auto_predict(data_x,data_set,cat,example_show):\n    pred = model.predict(data_x)\n    y_test = cat_decode(pred,cat)\n    data_set['Expected']= y_test\n    data_set.to_csv('sample_submission_1234.csv',columns=['Id','Expected'],index=False)\n    if example_show==True:\n        return print(\"Exito!\\n\",y_test[0:20])\n    else:\n        return print(\"Exito\")","26f14858":"train_labels = pd.read_csv('\/kaggle\/input\/taller\/train_labels.csv')\ntest_labels = pd.read_csv('\/kaggle\/input\/taller\/sample_submission.csv')\ntrain_path = '\/kaggle\/input\/taller\/train\/'\ntest_path = '\/kaggle\/input\/taller\/test\/'\ntrain_message_path= \"\/kaggle\/input\/taller\/train_source_tweets.txt\"\ntest_message_path= \"\/kaggle\/input\/taller\/test_source_tweets.txt\"\n\n###########        This creates original df with original message and label      ##################33\nx_original = txt_original_extract(train_message_path)\ndf_original = pd.DataFrame(x_original,  columns =['id','original message'])\ndf_original['id']= df_original['id'].astype('int64')\ndf_train = pd.merge(df_original, train_labels, how='inner', left_on='id', right_on='id')\ndf_train.head(5)\n##############################################################################33","b065bbb5":"train_message = df_train[\"original message\"].values\ntrain_relations = txt_extract(df_train[\"id\"],train_path )\nY_train = df_train[\"label\"]\ncat = Y_train.unique()\nY_train=cat_encode(Y_train,cat)","78ac75b4":"train_message = df_train[\"original message\"].values\ntrain_relations = X_data_import(df_train[\"id\"],train_path )\ndf_relations = pd.DataFrame(data=train_relations,columns=['P_uid', 'P_tweet_ID',\"P_time\", 'C_uid', 'C_tweet_ID',\"C_time\" ])\ndf_train_full = pd.merge(df_train, df_relations, how='inner', left_on='id', right_on='P_tweet_ID')\ndf_train_full.head()","46e33bd1":"X_train = df_train_full[[\"P_uid\",\"P_tweet_ID\",\"P_time\",\"C_uid\",\"C_tweet_ID\",\"C_time\"]].values\nY_train = df_train_full[\"label\"]\nX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\ncat = Y_train.unique()\nY_train=cat_encode(Y_train,cat)","646a9a7a":"X_train.shape","cbd78489":"Y_train.shape\n","02e37451":"batch_size=128\n\nmodel = Sequential()\nmodel.add(keras.layers.BatchNormalization(input_shape = X_train.shape[1:]))\n#model.add((Dense(518)))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(keras.layers.BatchNormalization())\n#model.add(Reshape((128,6)))# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\nmodel.add(GRU(512,return_sequences=True))\nmodel.add(keras.layers.BatchNormalization())\n# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\nmodel.add(GRU(512,return_sequences=True))\nmodel.add(keras.layers.BatchNormalization())\n\nmodel.add(GRU(512,return_sequences=False))\nmodel.add(keras.layers.BatchNormalization())\n#model.add(SimpleRNN(128))\n#model.add(SimpleRNN(512, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros'))\nmodel.add(Dense(512))\nmodel.add(keras.layers.BatchNormalization())\n\n\nmodel.add(Dense(4,activation='softmax'))\n\nmodel.summary()","937ef215":"####################################        NUEVO                    #######################################\n#Nota: Para el tercer modelo (test) con data augmentation\n    \n    \n#########################################           CALLBACKS           #########################################\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy', patience=5)\nbatch_size=128\ncheck_point = ModelCheckpoint(\n    filepath='\/tmp\/checkpoint',\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True,    \n    verbose=1\n)\n\nreduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.85, patience=4, verbose=0, mode='auto', min_lr=0.00006\n)\n\n#########################################           OPTIMIZER           #########################################\nopt=tf.keras.optimizers.Adam()\n\n########################################           LOSS FUCTION           #######################################\n\n#########################################           Metrics           #########################################\n","5d7c0bde":"model.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"categorical_accuracy\"])\ntrain = model.fit(X_train,Y_train,epochs=20,validation_split=0.3,batch_size=batch_size,shuffle=True,callbacks=[early_stop,check_point,reduce_lr])\nmodel.load_weights('\/tmp\/checkpoint')","2c93cb81":"X_train1 = df_train_full[[\"P_uid\",\"P_tweet_ID\",\"P_time\"]].values\nX_train1 = X_train1.reshape(X_train1.shape[0], 1, X_train1.shape[1])\n","7e93508d":"batch_size=128\n\nmodel = Sequential()\nmodel.add(keras.layers.BatchNormalization(input_shape = X_train1.shape[1:]))\nmodel.add((Dense(518)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(keras.layers.BatchNormalization())\n#model.add(Reshape((128,6)))# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\nmodel.add(GRU(512,return_sequences=True))\nmodel.add(keras.layers.BatchNormalization())\n# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\nmodel.add(GRU(512,return_sequences=False))\nmodel.add(keras.layers.BatchNormalization())\n#model.add(SimpleRNN(128))\n#model.add(SimpleRNN(512, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros'))\nmodel.add(Dense(512))\nmodel.add(keras.layers.BatchNormalization())\n\n\nmodel.add(Dense(4,activation='softmax'))\n\nmodel.summary()\n","ead3c5d3":"model.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"categorical_accuracy\"])\ntrain = model.fit(X_train1,Y_train,epochs=20,validation_split=0.3,batch_size=batch_size,shuffle=True,callbacks=[early_stop,check_point,reduce_lr])","a2b1d56f":"## Weak model","50a79b6a":"## Extracting train data","6fd03a89":"# Loading Data","744ab8a7":"### Model 1 (Strong)","722d7a02":"# Libraries","1da66933":"# fuctions","e6cd8c85":"### Compile"}}