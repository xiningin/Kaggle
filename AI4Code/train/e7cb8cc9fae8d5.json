{"cell_type":{"94fb3a4c":"code","d2d83c0b":"code","a729fe97":"code","e78ed161":"code","58f62b73":"code","9cdf5754":"code","82fd3396":"code","c7919d23":"code","6f514b37":"code","024cf471":"code","a4073a06":"code","4b5b7cc2":"code","bee0ae78":"code","7e57f50b":"code","ddb4bdd3":"code","d4403660":"code","4c90ccce":"code","7d3f53cc":"code","6d1bed23":"code","8333da24":"code","4d0620bc":"code","314d5444":"code","3bb15985":"code","9970bc4e":"code","b6978773":"code","b8bda351":"code","0a91cd4f":"code","ce7dc2a9":"code","f45b4ecb":"code","dbee0be8":"code","b0de28d3":"code","7d16f5db":"code","9ffe5590":"code","4f3d18d5":"code","0ebd3669":"code","e579e7e2":"code","b6ebd1ed":"code","571fe55c":"code","a4535dd6":"code","54cddd03":"code","94b0b09f":"code","c5738c69":"code","5785ce81":"code","f99ddec4":"markdown","eeac8ebd":"markdown","2828817c":"markdown","75306eab":"markdown","f1a36255":"markdown","0f127e1d":"markdown","e9a89517":"markdown","bc7ce1e5":"markdown","8f2e2da6":"markdown","0ac025c4":"markdown","25a86224":"markdown","83e2112b":"markdown","e4ed0e00":"markdown","7b3ae81e":"markdown","dad1c692":"markdown","6ae2b34a":"markdown","72c14668":"markdown","da70a42b":"markdown","ab6d039e":"markdown","ce161df7":"markdown","e9850acc":"markdown","982e34fd":"markdown","031ca9b3":"markdown","b36f1c27":"markdown","407fa00c":"markdown","e673f8c4":"markdown","b8f1bd8b":"markdown","f1dfce13":"markdown","0bdbbd28":"markdown","194a0701":"markdown","cf835f49":"markdown","93d2946c":"markdown","ee89cca9":"markdown","f08b0512":"markdown","f5a94fb3":"markdown","e370006c":"markdown","83d7b74c":"markdown","1e84198e":"markdown","c8e0c29e":"markdown","72986d9d":"markdown","11278329":"markdown","3ba13568":"markdown","0436ba7b":"markdown","606cc6d2":"markdown","b5beff15":"markdown","25b4e4c1":"markdown"},"source":{"94fb3a4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, StratifiedShuffleSplit, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as make_pipeline_imb\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, roc_auc_score, auc, f1_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d2d83c0b":"data = pd.read_csv(\"..\/input\/creditcard.csv\")","a729fe97":"#view the columns of the dataset\ndata.columns","e78ed161":"#view the first 5 samples of the dataset\ndata.head()","58f62b73":"#view the last 4 samples of the dataset\ndata.tail(4)","9cdf5754":"#pick any 3 samples of the dataset\ndata.sample(3)","82fd3396":"data.info()","c7919d23":"#no missing values\nmax(data.isnull().sum())","6f514b37":"#have a view of the features in the dataset\ndata.columns","024cf471":"fraud = data[data.Class==1]\nnon_fraud = data[data.Class==0]\n\n#fraction of frauds\nfrac_of_fraud = len(fraud)\/len(data)\n#fraction of non-frauds\nfrac_of_non_fraud = len(non_fraud)\/len(data)\n\nprint('Percentage of Frauds: {}%'.format(round(frac_of_fraud*100, 2)))\nprint('Percentage of Non-Frauds: {}%'.format(round(frac_of_non_fraud*100, 2)))","a4073a06":"#visualize the imbalance with a bar chart\nplt.title('Distribution of Frauds', fontdict={'size' : 16, 'color':'brown'})\nsns.countplot(x='Class', data=data)\nlabels = ['Non-Fraud', 'Fraud']   #to label the plot\nvals = [0, 1]   #to put the labels right\n\nplt.xticks(vals, labels)\nplt.xlabel('Class', fontdict={'size' : 14, 'color' : 'green'})\nplt.ylabel('Number of transactions', fontdict={'size' : 12, 'color':'green'})","4b5b7cc2":"fig, ax = plt.subplots(figsize=(16, 6), nrows=1, ncols=2)\n\nax1 = sns.distplot(data['Time'], color='brown', ax=ax[0])\nax2 = sns.distplot(data['Amount'], ax=ax[1])\n\nax1.set_title('Distribution of Transaction Time')\nax2.set_title('Distribution of Transaction Amount')","bee0ae78":"#skewness of the Time column\nprint('The Transaction Time has a skewness of {}'.format(round(data.Time.skew(), 4)))\n\n#skewness of the Amount column\nprint('The Transaction Amount has a skewness of {}'.format(round(data.Amount.skew(), 4)))\n\n#skewness of the Class column\nprint('The Class - Target Variable - has a skewness of {}'.format(round(data.Class.skew(), 4)))","7e57f50b":"#import the RobustScaler estimator class\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n\n#apply RobustScaler to the Time and the Amount columns\ndata[['Time', 'Amount']] = scaler.fit_transform(data[['Time', 'Amount']])\n\n#view the dataset to see the new look of the Time and Amount columns\ndata.head(5)","ddb4bdd3":"X = data.drop('Class', axis=1)  #independent variables\ny = data['Class']   #dependent variable","d4403660":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, \n                                                    random_state=0)","4c90ccce":"#importing the classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","7d3f53cc":"models = {\n    'Logistic Regression' : LogisticRegression(), \n    'Naive Bayes' : GaussianNB(),\n     #'Support Vector Classifier' : SVC(),  computationally expensive to run on the whole dataset\n    #'Decision Tree Classifier' : DecisionTreeClassifier()   computationally expensive\n    \n}","6d1bed23":"for name, model in models.items():\n    t0 = time.time()  #start time\n    model.fit(X_train, y_train)\n    accuracy = cross_val_score(model, X_train, y_train, cv=5).mean()\n    t1 = time.time()  #stop time\n    print(name, 'Score: {}%'.format(round(accuracy, 2)*100))\n    print('Computation Time: {}s\\n'.format(round(t1-t0, 2)))\n    print('*'*80)\n    print()","8333da24":"#since I do not know whether the classes are distributed in any unique pattern, I will shuffle the whole dataset\ndata = data.sample(frac=1, random_state=42)\n\n#pick out the fraud and the non-fraud samples from the shuffled dataset\nfraud = data[data.Class==1]\nnon_fraud = data[data.Class==0]\n\n#print out the number of samples in fraud and non_fraud\nprint('Before Undersampling: ')\nprint('Number of Fraudulent Transactions: {}'.format(len(fraud)))\nprint('Number of Non-Fraudulent Transactions: {}'.format(len(non_fraud)))\nprint()\n\n#making the non_fraud transactions(majority class) equal to the fraud transactions(minority class)\nnon_fraud = non_fraud.sample(frac=1)\nnon_fraud = non_fraud[:len(fraud)]\n\n#the non_fraud transactions are now equal to the fraud transaction --- let's visualize\nprint('After Undersampling: ')\nprint('Number of Fraudulent Transactions: {}'.format(len(fraud)))\nprint('Number of Non-Fraudulent Transactions: {}'.format(len(non_fraud)))\n\n\n#now join the fraud dataset to the non_fraud dataset\nsampled_data = pd.concat([fraud, non_fraud], axis=0)\n#shuffle the sampled_data to allow for random distribution of classes in the dataset\nsampled_data = sampled_data.sample(frac=1, random_state=42)","4d0620bc":"fig, ax = plt.subplots(figsize=(15, 5), nrows=1, ncols=2)\n\nax1 = sns.countplot(x='Class', data=data, ax=ax[0])\nax1.set_title('Distribution of Classes before Undersampling', color='brown')\n\nax2 = sns.countplot(x='Class', data=sampled_data, ax=ax[1])\nax2.set_title('Distribution of Classes after Undersampling', color='red')","314d5444":"#a pie chart will also indicate a 50\/50 ratio of fraud to non_fraud\npatches, texts, autotexts = plt.pie(\n    x=[len(fraud), len(non_fraud)], \n    labels=['Fraud', 'Non-Fraud'],\n    explode = [0.012, 0.012],\n    shadow=True,\n    autopct='%.1f%%',\n    radius=1.2,\n    startangle=30\n)\n\nfor text in texts:\n    text.set_color('#22AA11')\n    text.set_size(14)\nfor autotext in autotexts:\n    autotext.set_color('red')","3bb15985":"X_undersampled = sampled_data.drop('Class', axis=1)\ny_undersampled = sampled_data['Class']\n\n\n#splitting the dataset\nX_train_undersampled, X_test_undersampled, y_train_undersampled, y_test_undersampled = train_test_split(X_undersampled, \n                                                                                                        y_undersampled, \n                                                                                                        stratify=y_undersampled, \n                                                                                                        test_size=0.25, \n                                                                                                        random_state=0)","9970bc4e":"#use the cross validation technique - StratifiedShuffleSplit - to perform GridSearch and to calculate the train and test scores\ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n\n\n#using the Gaussian Naive Bayes on the dataset\nbayes = GaussianNB()\nbayes.fit(X_train_undersampled, y_train_undersampled)\nbayes_score = cross_val_score(bayes, X_train_undersampled, y_train_undersampled, cv=cv).mean()\nbayes_predictions = bayes.predict(X_test_undersampled)\nbayes_precision_score = precision_score(y_test_undersampled, bayes_predictions)\nbayes_recall_score = recall_score(y_test_undersampled, bayes_predictions)\nbayes_auc = roc_auc_score(y_test_undersampled, bayes_predictions)\n\n\n\n\n\n#parameters to search for using GridSearchCV\n\n#Logistic Regression\nlogReg_params = {'penalty' : ['l1', 'l2'], 'C' : [0.2, 0.4, 0.6, 0.8, 1.0, 1.3, 1.5, 1.7, 2.0]}\n\n#Support Vector Classifier\nsvc_params = {'C' : [0.1, 1, 10], 'gamma' : [1, 0.1, 0.01, 0.001], 'kernel' : ['linear', 'rbf'] }\n\n#Decision Tree Classifier\ndtree_params = {'criterion' : ['gini', 'entropy'], 'max_depth' : [3, 4, 5]}\n\n\n\n\n#GridSearch on Logistic Regression\nlogReg_grid = GridSearchCV(LogisticRegression(), logReg_params, refit=True, verbose=0, cv=cv, scoring='accuracy')\nlogReg_grid.fit(X_train_undersampled, y_train_undersampled)\nlogReg = logReg_grid.best_estimator_\n\nlogReg_score = cross_val_score(logReg, X_train_undersampled, y_train_undersampled, cv=cv).mean()\nlogReg_predictions = logReg.predict(X_test_undersampled)\nlogReg_precision_score = precision_score(y_test_undersampled, logReg_predictions)\nlogReg_recall_score = recall_score(y_test_undersampled, logReg_predictions)\nlogReg_auc = roc_auc_score(y_test_undersampled, logReg_predictions)\n\n\n#GridSearch on Support Vector Classifier\nsvc_grid = GridSearchCV(SVC(), svc_params, refit=True, verbose=0, cv=cv, scoring='accuracy')\nsvc_grid.fit(X_train_undersampled, y_train_undersampled)\nsvc = svc_grid.best_estimator_\n\nsvc_score = cross_val_score(svc, X_train_undersampled, y_train_undersampled, cv=cv).mean()\nsvc_predictions = svc.predict(X_test_undersampled)\nsvc_precision_score = precision_score(y_test_undersampled, svc_predictions)\nsvc_recall_score = recall_score(y_test_undersampled, svc_predictions)\nsvc_auc = roc_auc_score(y_test_undersampled, svc_predictions)\n\n\n#GridSearch on Decision Tree Classifier\ndtree_grid = GridSearchCV(DecisionTreeClassifier(), dtree_params, refit=True, verbose=0, cv=cv, scoring='accuracy')\ndtree_grid.fit(X_train_undersampled, y_train_undersampled)\ndtree = dtree_grid.best_estimator_\n\ndtree_score = cross_val_score(dtree, X_train_undersampled, y_train_undersampled, cv=cv).mean()\ndtree_predictions = dtree.predict(X_test_undersampled)\ndtree_precision_score = precision_score(y_test_undersampled, dtree_predictions)\ndtree_recall_score = recall_score(y_test_undersampled, dtree_predictions)\ndtree_auc = roc_auc_score(y_test_undersampled, dtree_predictions)","b6978773":"model_names = ['Logistic Regression', 'Gaussian Naive Bayes', 'Support Vector Classifier', 'Decision Tree Classifier']","b8bda351":"for name in model_names:\n    if name == 'Logistic Regression':\n        print(name, 'Scores: \\n')\n        print('Accuracy: {}'.format(round(logReg_score, 2)))\n        print('Precision: {}'.format(round(logReg_precision_score, 2)))\n        print('Recall: {}'.format(round(logReg_recall_score, 2)))\n        print('AUC: {}\\n'.format(round(logReg_auc, 2)))\n        print('*'*90)\n    elif name == 'Gaussian Naive Bayes':\n        print(name, 'Scores: \\n')\n        print('Accuracy: {}'.format(round(bayes_score, 2)))\n        print('Precision: {}'.format(round(bayes_precision_score, 2)))\n        print('Recall: {}'.format(round(bayes_recall_score, 2)))\n        print('AUC: {}\\n'.format(round(bayes_auc, 2)))\n        print('*'*90)\n    elif name == 'Support Vector Classifier':\n        print(name, 'Scores: \\n')\n        print('Accuracy: {}'.format(round(svc_score, 2)))\n        print('Precision: {}'.format(round(svc_precision_score, 2)))\n        print('Recall: {}'.format(round(svc_recall_score, 2)))\n        print('AUC: {}\\n'.format(round(svc_auc, 2)))\n        print('*'*90)\n    elif name == 'Decision Tree Classifier':\n        print(name, 'Scores: \\n')\n        print('Accuracy: {}'.format(round(dtree_score, 2)))\n        print('Precision: {}'.format(round(dtree_precision_score, 2)))\n        print('Recall: {}'.format(round(dtree_recall_score, 2)))\n        print('AUC: {}\\n'.format(round(dtree_auc, 2)))\n        print('*'*90)","0a91cd4f":"print('Random Undersampling Before Data Split')\nprint('.'*60)\nprint()\n\nlogReg_conf_matrix = confusion_matrix(y_test_undersampled, logReg_predictions)\nbayes_conf_matrix = confusion_matrix(y_test_undersampled, bayes_predictions)\nsvc_conf_matrix = confusion_matrix(y_test_undersampled, svc_predictions)\ndtree_conf_matrix = confusion_matrix(y_test_undersampled, dtree_predictions)\n\n\nfig, ax = plt.subplots(figsize=(10, 8), nrows=2, ncols=2)\n\nfig.subplots_adjust(hspace=0.6, wspace=0.4)  #adjust the spaces between the subplots\ntick_labels = ['non_fraud', 'fraud']  #labels for the xticks and yticks\n\n\n#Logistic Regression Confusion Matrix\nax1 = sns.heatmap(logReg_conf_matrix, cbar=False, yticklabels=tick_labels, xticklabels=tick_labels, annot=True, ax=ax[0][0])\nax1.set_title('Logistic Regression Confusion Matrix', color='red')\nax1.set_ylabel('Actual Labels', size=8)\nax1.set_xlabel('Predicted Labels', size=8)\n\n\n#Gaussian Naive Bayes Confusion Matrix\nax2 = sns.heatmap(bayes_conf_matrix, cbar=False, yticklabels=tick_labels, xticklabels=tick_labels, annot=True, ax=ax[0][1])\nax2.set_title('Gaussian Naive Bayes Confusion Matrix', color='red')\nax2.set_ylabel('Actual Labels', size=8)\nax2.set_xlabel('Predicted Labels', size=8)\n\n\n#Support Vector Classifier Confusion Matrix\nax3 = sns.heatmap(svc_conf_matrix, cbar=False, yticklabels=tick_labels, xticklabels=tick_labels, annot=True, ax=ax[1][0])\nax3.set_title('Support Vector Classifier Confusion Matrix', color='red')\nax3.set_ylabel('Actual Labels', size=8)\nax3.set_xlabel('Predicted Labels', size=8)\n\n\n#Decision Tree Confusion Matrix\nax4 = sns.heatmap(dtree_conf_matrix, cbar=False, yticklabels=tick_labels, xticklabels=tick_labels, annot=True, ax=ax[1][1])\nax4.set_title('Decision Tree Classifier Confusion Matrix', color='red')\nax4.set_ylabel('Actual Labels', size=8)\nax4.set_xlabel('Predicted Labels', size=8)\n","ce7dc2a9":"cv = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n\naccuracy = []\nprecision = []\nrecall = []\nauc = []\n\n\n\n#remember X_train and y_train are from the original data\n#join X_train and y_train; join X_test and y_test\n#then undersample only the resulting dataframe of X_train and y_train; and then test using the dataframe resulting from X_test and y_test\ntrain_data = pd.concat([X_train, y_train], axis=1)\ntest_data = pd.concat([X_test, y_test], axis=1)\n\n\n#pick out fraud and non_fraud from the train_data\nfraud = train_data[train_data.Class==1]\nnon_fraud = train_data[train_data.Class==0]\n\n#make fraud equal to non_fraud by picking out random samples from non_fraud and making it equal to fraud\nnon_fraud = non_fraud.sample(n=len(fraud))\n\n#number of fraud and non_fraud\nprint('Number of Fraudulent Transactions: {}'.format(len(fraud)))\nprint('Number of Non-Fraudulent Transactions: {}'.format(len(non_fraud)))\n\n#concatenate fraud and non_fraud and call the dataframe undersampled_train_data\nundersampled_train_data = pd.concat([fraud, non_fraud], axis=0)\nundersampled_train_data = undersampled_train_data.sample(frac=1)  #resample your data to allow for a kind of random\/even distribution\n\n#we will use the whole undersampled_train_data to train our model, and test it using the test_data that isn't undersampled!\ntrain_X = undersampled_train_data.drop('Class', axis=1)\ntrain_y = undersampled_train_data['Class']\n\n#split the test_data\ntest_X = test_data.drop('Class', axis=1)\ntest_y = test_data['Class']\n\n\n#use gridsearch to search for parameters\nlogReg_params = {'penalty' : ['l1', 'l2'], 'C' : [0.4, 0.6, 0.8, 1.0, 1.3, 1.5, 1.7, 2.0]}\n\nlog_grid = GridSearchCV(LogisticRegression(random_state=0), logReg_params, refit=True, verbose=0)\nlog_grid.fit(train_X, train_y)   #training with the undersampled data\nlogReg = log_grid.best_estimator_\n#testing with the original dataset which was not undersampled\npredict = log_grid.predict(test_X)\n\n\n#scores\nlogReg_score = cross_val_score(logReg, train_X, train_y, cv=cv).mean()\n\nlogReg_precision_score = precision_score(test_y, predict)\nlogReg_recall_score = recall_score(test_y, predict)\nlogReg_auc = roc_auc_score(test_y, predict)","f45b4ecb":"print('Logistic Regression Results for Random Undersampling After Data Split\\n')\nprint('*'*78)\nprint()\nprint('Accuracy: {}'.format(round(logReg_score, 2)))\nprint('Precision: {}'.format(round(logReg_precision_score, 2)))\nprint('Recall: {}'.format(round(logReg_recall_score, 2)))\nprint('AUC: {}'.format(round(logReg_auc, 2)))","dbee0be8":"print('Random Undersampling After Data Split')\nprint('.'*60)\nprint()\n\nax1 = sns.heatmap(confusion_matrix(test_y, predict), cbar=False, yticklabels=tick_labels, xticklabels=tick_labels, annot=True)\nax1.set_title('Logistic Regression Confusion Matrix', color='red')\nax1.set_ylabel('Actual Labels', size=8)\nax1.set_xlabel('Predicted Labels', size=8)","b0de28d3":"#function to display the results for the results of NearMiss and SMOTE before cv and during cv\ndef show_metrics(title, accuracy, precision, recall, f1, auc):\n    print(title, 'Results:\\n')\n    print('Accuracy: {}'.format(round(np.mean(accuracy), 2)))\n    print('Precision: {}'.format(round(np.mean(precision), 2)))\n    print('Recall: {}'.format(round(np.mean(recall), 2)))\n    print('F1-Score: {}'.format(round(np.mean(f1), 2)))\n    print('AUC: {}'.format(round(np.mean(auc), 2)))\n    print()\n    print('*'*80)\n    print()","7d16f5db":"#cross validation technique to use during NearMiss\ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.20, random_state=0)","9ffe5590":"nearmiss_before_cv_accuracy = []         #accuracy score\nnearmiss_before_cv_precision = []        #precision score\nnearmiss_before_cv_recall = []           #recall\nnearmiss_before_cv_f1 = []               #f1 score\nnearmiss_before_cv_auc = []              #auc --- area under curve\n\nnearmiss_before_cv_conf_matrices = []    #list to append the confusion matrices obtained during CV\n\n\nprint('Distribution of Target Variable in Original Data: {}\\n'.format(Counter(y)))   #distribution of fraud and non_fraud in original dataset\nprint('Distribution of Target Variable in Training Set of Original Data: {}\\n'.format(Counter(y_train)))   #distribution of fraud and non_fraud in the training part of the original dataset\n\n\n#remember that X_train and y_train are a split from the original dataset\nX_nearmiss, y_nearmiss = NearMiss(random_state=0).fit_sample(X_train, y_train)   #applying NearMiss before CV, WRONG!!!\n\nprint('Distribution of Target Variable in Training Set of Original Data after NearMiss: {}\\n'.format(Counter(y_nearmiss)))  #distribution of fraud and non_fraud must be equal in the training part of the original dataset after NearMiss is applied\n\nfor train, test in cv.split(X_nearmiss, y_nearmiss):   #CV begins here!\n    #pipeline for imbalanced data -- from imblearn.pipeline import make_pipeline as make_make_pipeline_imb\n    pipeline = make_pipeline_imb(LogisticRegression(random_state=42)) \n    nearmiss_before_cv_model = pipeline.fit(X_nearmiss[train], y_nearmiss[train])\n    nearmiss_before_cv_predictions = nearmiss_before_cv_model.predict(X_nearmiss[test])\n    \n    #scoring metrics\n    nearmiss_before_cv_accuracy.append(pipeline.score(X_nearmiss[test], y_nearmiss[test]))\n    nearmiss_before_cv_precision.append(precision_score(y_nearmiss[test], nearmiss_before_cv_predictions))\n    nearmiss_before_cv_recall.append(recall_score(y_nearmiss[test], nearmiss_before_cv_predictions))\n    nearmiss_before_cv_f1.append(f1_score(y_nearmiss[test], nearmiss_before_cv_predictions))\n    nearmiss_before_cv_auc.append(roc_auc_score(y_nearmiss[test], nearmiss_before_cv_predictions))\n    \n    #confusion matrices --- since the no. of splits is 10, I have 10 different confusion matrices\n    nearmiss_before_cv_conf_matrices.append(confusion_matrix(y_nearmiss[test], nearmiss_before_cv_predictions))","4f3d18d5":"nearmiss_during_cv_accuracy = []\nnearmiss_during_cv_precision = []\nnearmiss_during_cv_recall = []\nnearmiss_during_cv_f1 = []\nnearmiss_during_cv_auc = []\n\n\nnearmiss_during_cv_conf_matrices = []\n\n#in order to get the values of the indices obtained during cv easily, I'll convert X_train and y_train into numpy arrays; I can also do this by using the pandas' 'iloc' method\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n\nfor train, test in cv.split(X_train, y_train):   #CV begins here!\n    pipeline = make_pipeline_imb(NearMiss(random_state=0), LogisticRegression(random_state=42)) #NearMiss during CV, CORRECT!!!\n    nearmiss_during_cv_model = pipeline.fit(X_train[train], y_train[train])\n    nearmiss_during_cv_predictions = nearmiss_during_cv_model.predict(X_train[test])\n    \n    #scoring metrics\n    nearmiss_during_cv_accuracy.append(pipeline.score(X_train[test], y_train[test]))\n    nearmiss_during_cv_precision.append(precision_score(y_train[test], nearmiss_during_cv_predictions))\n    nearmiss_during_cv_recall.append(recall_score(y_train[test], nearmiss_during_cv_predictions))\n    nearmiss_during_cv_f1.append(f1_score(y_train[test], nearmiss_during_cv_predictions))\n    nearmiss_during_cv_auc.append(roc_auc_score(y_train[test], nearmiss_during_cv_predictions))\n    \n    #confusion matrices --- since the no. of splits is 10, I must have 10 confusion matrices\n    nearmiss_during_cv_conf_matrices.append(confusion_matrix(y_train[test], nearmiss_during_cv_predictions))","0ebd3669":"print('RESULTS for the NearMiss Algorithm')\nprint('.'*80)\nprint()\n\nshow_metrics('NearMiss Before Cross Validation', nearmiss_before_cv_accuracy, nearmiss_before_cv_precision, nearmiss_before_cv_recall, nearmiss_before_cv_f1, nearmiss_before_cv_auc)\nshow_metrics('NearMiss During Cross Validation', nearmiss_during_cv_accuracy, nearmiss_during_cv_precision, nearmiss_during_cv_recall, nearmiss_during_cv_f1, nearmiss_during_cv_auc)","e579e7e2":"fig, ax = plt.subplots(figsize=(20, 3), nrows=1, ncols=5)\n\ni = 0\nprint('NearMiss Before CV Confusion Matrices')\nprint('.'*60)\n#the first 5 confusion matrices of the 10\nfor nearmiss_before_cv_conf_matrix in nearmiss_before_cv_conf_matrices[:5]:\n    ax0 = sns.heatmap(nearmiss_before_cv_conf_matrix, cbar=False,xticklabels=tick_labels, yticklabels=tick_labels, ax=ax[i], annot=True, annot_kws={'size' : 15})\n    #ax0.set_ylabel('Actual Labels')\n    #ax0.set_xlabel('Predicted Labels')\n    i = i + 1","b6ebd1ed":"fig, ax = plt.subplots(figsize=(20, 3), nrows=1, ncols=5)\n\nk = 0\nprint('NearMiss During CV Confusion Matrices')\nprint('.'*60)\n#the first 5 confusion matrices of the 10\nfor nearmiss_before_cv_conf_matrix in nearmiss_during_cv_conf_matrices[:5]:\n    ax1 = sns.heatmap(nearmiss_before_cv_conf_matrix, cbar=False, xticklabels=tick_labels, yticklabels=tick_labels, ax=ax[k], annot=True, annot_kws={'size' : 15})\n    #ax1.set_ylabel('Actual Labels')\n    #ax1.set_xlabel('Predicted Labels')\n    k= k + 1","571fe55c":"#cross validation technique to use during SMOTE\nkf = StratifiedKFold(n_splits=3, shuffle=False, random_state=0)","a4535dd6":"smote_before_cv_accuracy = []         \nsmote_before_cv_precision = []        \nsmote_before_cv_recall = []           \nsmote_before_cv_f1 = []               \nsmote_before_cv_auc = []              \n\nsmote_before_cv_conf_matrices = []  \n\n\nprint('Distribution of Target Variable in Original Data: {}\\n'.format(Counter(y)))   #distribution of fraud and non_fraud in original dataset\nprint('Distribution of Target Variable in Training Set of Original Data: {}\\n'.format(Counter(y_train)))   #distribution of fraud and non_fraud in the training part of the original dataset\n\n\nsmote_X, smote_y = SMOTE(random_state=0).fit_sample(X_train, y_train)   #applying SMOTE before CV, WRONG!!!\n\nprint('Distribution of Target Variable in Training Set of Original Data after NearMiss: {}\\n'.format(Counter(smote_y)))  #distribution of fraud and non_fraud must be equal in the training part of the original dataset after SMOTE is applied\n\n\nfor train, test in kf.split(smote_X, smote_y):   #CV begins here!\n    pipeline = make_pipeline_imb(LogisticRegression(random_state=42)) \n    smote_before_cv_model = pipeline.fit(smote_X[train], smote_y[train])\n    smote_before_cv_predictions = smote_before_cv_model.predict(smote_X[test])\n    \n    #classification metrics\n    smote_before_cv_accuracy.append(pipeline.score(smote_X[test], smote_y[test]))\n    smote_before_cv_precision.append(precision_score(smote_y[test], smote_before_cv_predictions))\n    smote_before_cv_recall.append(recall_score(smote_y[test], smote_before_cv_predictions))\n    smote_before_cv_f1.append(f1_score(smote_y[test], smote_before_cv_predictions))\n    smote_before_cv_auc.append(roc_auc_score(smote_y[test], smote_before_cv_predictions))\n    \n    #confusion matrices --- since the no. of splits is 10, I have 10 different confusion matrices\n    smote_before_cv_conf_matrices.append(confusion_matrix(smote_y[test], smote_before_cv_predictions))","54cddd03":"smote_during_cv_accuracy = []\nsmote_during_cv_precision = []\nsmote_during_cv_recall = []\nsmote_during_cv_f1 = []\nsmote_during_cv_auc = []\n\n\nsmote_during_cv_conf_matrices = []\n\n#no need to convert X_train, X_test, y_train and y_test to numpy arrays, as I have done that before\nX_train = X_train\nX_test = X_test\ny_train = y_train\ny_test = y_test\n\n\nfor train, test in kf.split(X_train, y_train):   #CV begins here!\n    pipeline = make_pipeline_imb(SMOTE(random_state=0), LogisticRegression(random_state=42)) #SMOTE during CV, CORRECT!!!\n    smote_during_cv_model = pipeline.fit(X_train[train], y_train[train])\n    smote_during_cv_predictions = smote_during_cv_model.predict(X_train[test])\n    \n    #scoring metrics\n    smote_during_cv_accuracy.append(pipeline.score(X_train[test], y_train[test]))\n    smote_during_cv_precision.append(precision_score(y_train[test], smote_during_cv_predictions))\n    smote_during_cv_recall.append(recall_score(y_train[test], smote_during_cv_predictions))\n    smote_during_cv_f1.append(f1_score(y_train[test], smote_during_cv_predictions))\n    smote_during_cv_auc.append(roc_auc_score(y_train[test], smote_during_cv_predictions))\n    \n    \n    #confusion matrices --- since the no. of splits is 10, I must have 10 confusion matrices\n    smote_during_cv_conf_matrices.append(confusion_matrix(y_train[test], smote_during_cv_predictions))","94b0b09f":"print('RESULTS for SMOTE')\nprint('.'*80)\nprint()\n\nshow_metrics('SMOTE Before Cross Validation', smote_before_cv_accuracy, smote_before_cv_precision, smote_before_cv_recall, smote_before_cv_f1, smote_before_cv_auc)\nshow_metrics('SMOTE During Cross Validation', smote_during_cv_accuracy, smote_during_cv_precision, smote_during_cv_recall, smote_during_cv_f1, smote_during_cv_auc)","c5738c69":"fig, ax = plt.subplots(figsize=(9, 3), nrows=1, ncols=3)\n\nx = 0\nprint('NearMiss Before CV Confusion Matrices')\nprint('.'*60)\n#the first 5 confusion matrices of the 10\nfor smote_before_cv_conf_matrix in smote_before_cv_conf_matrices[:5]:\n    ax0 = sns.heatmap(smote_before_cv_conf_matrix, cbar=False, xticklabels=tick_labels, yticklabels=tick_labels, ax=ax[x], annot=True, annot_kws={'size' : 10})\n    #ax0.set_ylabel('Actual Labels')\n    #ax0.set_xlabel('Predicted Labels')\n    x = x + 1","5785ce81":"fig, ax = plt.subplots(figsize=(9, 3), nrows=1, ncols=3)\n\ny = 0\nprint('NearMiss Before CV Confusion Matrices')\nprint('.'*60)\n#the first 5 confusion matrices of the 10\nfor smote_during_cv_conf_matrix in smote_during_cv_conf_matrices[:5]:\n    ax1 = sns.heatmap(nearmiss_before_cv_conf_matrix, cbar=False, xticklabels=tick_labels, yticklabels=tick_labels, ax=ax[y], annot=True, annot_kws={'size' : 10})\n    #ax0.set_ylabel('Actual Labels')\n    #ax0.set_xlabel('Predicted Labels')\n    y = y + 1","f99ddec4":"<a id='smote_during_cv'><\/a>\n<h3>ii. During Cross Validation<\/h3>\n\nThis is <font color='red'>SMOTE done right<\/font>. Here, SMOTE is applied to the dataset during Cross Validation.","eeac8ebd":"<h1 align='center'>CREDIT CARD FRAUD DETECTION<\/h1>\n<h3>Date: 21-05-2019<\/h3>\n\nBeing a part of the Kaggle community has boosted my enthusiasm for DataScience. I have learned a lot through Kaggle in recent times. This is my <b>first<\/b> kernel on Kaggle, and I want to work on a <font color='green'>Credit Card Fraud Detection problem<\/font>.\n***\n\nIf there are any recommendations you would like to see in this notebook of mine, please drop it in the comment section below, as it will be greatly appreciated. If you like this notebook, please feel free to <font color='blue'>UPVOTE<\/font> and\/or leave a <font color='blue'>COMMENT<\/font> below.\n\n***\n\n\n<h2>Overview<\/h2>\nIn this kernel, we will deal with the <font color='green'>Credit Card Fraud Detection<\/font> dataset which is very popular for its <b>high imbalance<\/b>. We will use various algorithms such as <font color='red'>NearMiss<\/font>, <font color='red'>SMOTE<\/font> to deal with the <b>high imbalance<\/b> in the dataset. \n\n\n<h2>Aim<\/h2>\n<ul>\n<li>Check for the extent of imbalance in our dataset.<\/li>\n<li>Use several sampling algorithms like <font color='red'>SMOTE<\/font> and <font color='red'>NearMiss<\/font> to deal with the imbalance.<\/li>\n<li>Use machine learning classification models to predict whether a transcation was <b>fradulent<\/b> or <b>non-fraudulent<\/b><\/li>\n<\/ul>","2828817c":"From the above, we see that we get unexpectedly high accuracies. This is due to the high imbalance in the classes. Since there is a very large imbalance between fraud 1(0.17%) and non-fraud 0(99.83%), the models above will tend to classify all the transcations as non-fraud, and as such, give us way too high accuracies on the training set and the test set.","75306eab":"Since non-fraudlent transcations were labeled 0, and fraudlent ones 1, we can group the transactions that were fradulent as 'fraud', and those that were non-fraudlent as 'non_fraud'.","f1a36255":"Notice that Recall became higher than Precision when SMOTE was applied during Cross Validation.","0f127e1d":"Looking at what we have done above, we can vividly see that we undersampled our data before splitting it into training set and test set. This is Sampling done wrong (cross validation done wrong too!!!). It leads to <b>overfitting<\/b>.","e9a89517":"On comapring the results we just obtained with the results of Linear Regression when Random Undersampling was performed before the split of our dataset, we see that the results of Random Undersampling after the split of our data is more impressive than that of Random Undersampling before the data split - the precision score of the Logistic Regression classifier reduced while the high recall value was still maintained. There was also an increase in the AUC.","bc7ce1e5":"<a id='metrics'><\/a>\n<h2>Part 7: Metrics<\/h2>\n\nThere are a number of metrics that can be used to check the performance of classification problems;\n\n<h4>1. Accuracy<\/h4>\n<p>Accuracy is the total number of correct predictions made by the model all over the total number of predicted labels. Accuracy <b>should not<\/b> be used when there is a high imbalance in the dataset, as the model will tend to classify all the data points in the test data as the majority class.<\/p>\n    <ul>\n    <p>Accuracy = (Total number of correct predictions) \/ (Total number of predicted labels)<\/p>\n    <p>Accuracy = (TP + TN) \/ (Total)<\/p>\n    <\/ul>\n    \n \n<h4>2. Precision<\/h4>\n<p>Precision is the proportion of the fradulent transactions to the total number of transactions that were predicted to be fraudulent.<\/p>\n    <ul>\n    <p>Precision = TP \/ (TP + FP)<\/p>\n    <\/ul>\n\n\n<h4>3. Recall (Sensitivity \/ True Postive Rate)<\/h4>\n<p>Recall is the proportion of transactions that were predicted correctly to be fradulent of the total number of transactions that were actually fraudulent.<\/p>\n    <ul>\n    <p>Recall = TP \/ (TP + FN)<\/p>\n    <\/ul>\n    \n    \n<h4>4. Specificity (False Postive Rate)<\/h4>\n<p>This is the converse of Recall\/Sensitivity. Specificity is the proportion of transactions that were predicted correctly to not be fradulent of the total number of transactions that were actually not fraudulent.<\/p>\n    <ul>\n    <p>Specificity = TN \/ (TN + FP)<\/p>\n    <\/ul>\n    \n    \n<b>Precision, Recall and Specifcity are one of the best metrics to consider when dealing with imbalanced data.<\/b>\n\nIt is always better to classify a non-fraudlent transaction as fradulent, than classify a fraudlent transaction as non-fraudlent...\n\n1. the reason is simply because on calling a non-fraud transaction fraud, you only make security agents investigate an individual who is innocent of fraud. However, after much investigation, the individual will be vindicated because he did not actually commit fraud, it was only your model that predicted it as fraud.\n\n2. on the other hand, it will not be good enough if our model classify a fraudulent transaction as non-fraudulent because this will make fraudsters get away with their crime, thereby increasing thye rate at which credit card fraud is committed. \n\n\nFrom the look of the two points above, we can see that Recall is the best metric to consider. This is because a higher Recall indicates that the number of FN (transaction considered non-fraudulent, but is actually fraudulent) is being reduced to the minimum, which is our goal - try as much as possible not to classify non-fraudulent activities as fraudulent!!! \n\nHowever, on increasing recall, precision decreases. The metric we will consider here is <b>Recall<\/b>...we need a high recall as much as possible.\n\n\nResources: \n* https:\/\/medium.com\/@ogundareoluwafemi2001\/how-to-measure-the-performance-of-machine-learning-classification-models-8994ccf28047\n* <a href=\"https:\/\/towardsdatascience.com\/precision-vs-recall-386cf9f89488\">Precision vs Recall<\/a> \n* https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall","8f2e2da6":"<a id='smote'><\/a>\n<h2>8c. SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE - SMOTE<\/h2>\n\n<b>SMOTE<\/b> is an <b>oversampling<\/b> technique used for dealing with imbalanced data. <b>SMOTE<\/b> synthesises new minority instances between existing(real) minority instances. <font color='brown'>The new minority instances are not just copies of existing minority cases; instead, the algorithm takes samples of the feature space for each target class and its nearest neighbors, and generates new examples that combine features of the target case with features of its neighbors. This approach increases the features available to each class and makes the samples more general<\/font>.\n\nFurther Readings: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/smote","0ac025c4":"<a id='problem_definition'><\/a>\n<h1>1. Problem Definition<\/h1>\n\nThe datasets contains transactions made by credit cards by European cardholders. Pricipal Component Analysis was performed on the original dataset, and features V1, V2, ..., V28 are the first 28 principal components obtained from the dataset. We know nothing about what features V1,..,V28 represent due to privacy reasons. The Time and the Amount features are the only features which are yet to be transformed by the PCA. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nMost of the transcations were <b>non-fraudulent<\/b>, with the <b>fraudulent<\/b> ones being a miniature of the total transactions.","25a86224":"From the above, we can see that running the NearMiss algorithm on our dataset before cross validation leads to high scores than running the algorithm during cross validation. This is due to the fact that there is always data leakage from the training set to the test set when sampling is done before cross validation, which makes our Logistic Regression Classifier to cheat, leading to overfitting, and hence, high scores. When you compare the scores gotten when NearMiss was performed before cross validation to when NearMiss was performed during cross validation, we see that the scores of NearMiss during cross validation was lower than the former.\n\nNotice that when NearMiss was performed during cross validation, we get a very high Recall value to Precision value, which is still very much okay. The accuracy score we then obtain is the true accuracy of the Logistic Regression classifier.","83e2112b":"We see that the number of fraudulent transcations is now equal to the number of non-fraudlent transactions [213236 : 213236] after SMOTE was applied on the training set of the original data.","e4ed0e00":"It is worthy to note that <b>you cannot say which is better between NearMiss and SMOTE<\/b>.  The reason is because <b>NearMiss<\/b> brings about removal of data points which could bring about <b>Underfitting<\/b>, while <b>SMOTE<\/b> brings about addition of new synthetic points which could bring about <b>Overfitting<\/b>. Maybe the idea that SMOTE can lead to overfitting can be traced to the high accuracies we obtained.","7b3ae81e":"<a id='before_cv'><\/a>\n<h3>i. Undersampling Before Cross Validation<\/h3>\n\nHere, we will undersample our dataset before splitting it into training part and test part. This leads to data leakage, as the Random Sampling technique creates new copies of original data points. When this data is split, we end up having some data points in the training set being in the test set too. This makes our model(s) to cheat. ","dad1c692":"We see that Time is negatively skewed(approximately 0), while Amount and Class are highly postively skewed.","6ae2b34a":"## Table of contents\n***\n- [Part 1: Problem Definition](#problem_definition)\n- [Part 2: Importing Necessary Modules](#import_libraries)\n    - [2a. Libraries](#import_libraries)\n    - [2b. Load datasets](#load_data)\n    - [2c. A Glance at the dataset](#glance_dataset)\n- [Part 3: Mini EDA](#eda)\n    - [3a. Class variable](#class)\n    - [3b. Time and Amount](#time_amount)\n- [Part 4: Preprocessing](#preprocess)\n    - [4a. Scaling](#scale)\n    - [4b. Splitting of Data](#split_data)\n      -  [i Seperating the Independent variables from the Dependent variable](#seperate)\n      -  [ii Splitting the Dataset](#split)\n      -  [iii Resampling](#resample)\n- [Part 5: Ways of Combating Imbalance in Classes](#resample)\n- [Part 6: Using the Imbalanced Dataset on Classification Models](#imbalance_data_on_classifier)\n- [Part 7: Classification Metrics to be used](#metrics)\n- [Part 8: Applying Sampling Techniques to the Dataset](#sampling_techniques)\n    - [8a. Random Undersampling](#random_undersampling)\n      -  [i Before Cross Validation](#before_cv)\n      -  [ii During Cross Validation](#during_cv)\n    - [8b. NearMiss](#nearmiss)\n      -  [i Before Cross Validation](#nearmiss_before_cv)\n      -  [ii During Cross Validation](#nearmiss_during_cv)\n    - [8c. SMOTE](#smote)  \n      -  [i Before Cross Validation](#smote_before_cv)\n      -  [ii During Cross Validation](#smote_during_cv)","72c14668":"We see that the number of fraudulent transcations is now equal to the number of non-fraudulent transactions [369 : 369] after the NearMiss algorithm was applied on the training set of the original data.","da70a42b":"<a id='time_amount'><\/a>\n<h2 id=''>3b. Time and Amount features<\/h2>","ab6d039e":"<a id='class'><\/a>\n<h2>3a. Class variable<\/h2>","ce161df7":"<a id='nearmiss'><\/a>\n<h2>8b. NearMiss<\/h2>\n\nNearMiss is an undersampling technique used to deal with imbalanced dataset. It resamples the majority class and makes it equal to the number of samples in the minority class. It uses the <font color='green'>Nearest Neigbors<\/font> technique to do this.","e9850acc":"<a id='preprocess'><\/a>\n<h1>Part 4: Preprocessing<\/h1>\n\n<a id='scale'><\/a>\n<h2>4a. Scaling<\/h2>\n\n***\n\nFeature scaling is a key concept of machine learning models. Most datasets contain features often varying in magnitude and unit. Some machine learning models are not affected by this varying magnitudes and units; however, most machine learning models are affected by this varaiation since they make use of several distance metrics to calculate the distance between data points. \n\nHere, the Time and the Amount features are not in scale with columns V1, V2, ..., V28, and so we have to scale them both.\n\nThere are multiple ways of doing feature scaling. \n<ul>\n    <li type=1><b>StandardScaler<\/b>\n        <ul>\n            <li>Assumes the data is normally distributed within each feature.<\/li>\n            <li>A normally distributed feature have zero skewness, its mean always equals its median, and if the feature is unimodal, then its mode equals the mean and median. <\/li>\n            <li>Scales the data so that it has mean 0 and variance of 1.<\/li>\n        <\/ul>\n        \n   <\/li>\n    <li type=1><b>MinMaxScaler<\/b>\n        <ul>\n            <li>Scales the data so that it ranges from 0 to 1 (-1 to 1 if there are negative values).<\/li>\n            <li>Works better if the data is not Gaussian, or the standard deviation is small. <\/li>\n            <li>Sensitive to outliers.<\/li>\n        <\/ul>\n   <\/li>\n    <li type=1><b>RobustScaler<\/b>\n        <ul>\n            <li>Similar to the MinMaxScaler, but uses the interquartile range for scaling.<\/li>\n            <li>Its use of the interquartile range makes it robust to outliers. <\/li>\n            <li>This means it uses less of the data for scaling, and as such, is more suitable when there are outliers in the data.<\/li>\n        <\/ul>\n   <\/li>\n    <li type=1><b>Unit Vector<\/b>\n        <ul>\n            <li>Ranges from 0 to 1.<\/li>\n            <li>Makes each feature to be a vector of unit length.<\/li>\n        <\/ul>\n   <\/li>\n <\/ul>\n \n ***\n I will make use of the RobustScaler to scale both the Time feature and the Amount feature. This is because the RobustScaler uses less of the data for scaling, and is more suitable when there are outliers in the data.","982e34fd":"<a id='sampling_techniques'><\/a>\n<h2>Part 8: Sampling the Dataset<\/h2>\n\nOne major misconception about sampling datasets with imbalanced classes is that sampling must be done before splitting the dataset into training and test set, or before cross validation. <b>Sampling before cross validation<\/b> is <b>wrong<\/b> because it could cause <b>data leakage<\/b> - making some copies of the same points ending up in both the training set and the test set. This would make the machine learning classifier <b>cheat<\/b> because the classifier will see some data points in the test set being identical to some other data points in the training set, and as such, will lead to overfitting, which will eventually cause inaccurate results.\n\nData sampling is advised to always be done <b>during splitting of data and\/or cross validation<\/b>.","031ca9b3":"<a id='independent_dependent'><\/a>\n<h3>ii. Splitting the Dataset<\/h3>","b36f1c27":"<a id='eda'><\/a>\n<h1>Part 3: Mini EDA<\/h1>","407fa00c":"From the visualizations, we can see that before undersampling, there was a large imbalance between the number of fradulent transactions and the number of non-fraudulent transactions.","e673f8c4":"From the above plots, we can see that the Time, the Amount and the Class columns are skewed because they are all imbalanced.\n\n***\n\n<h3><b>Skewness<\/b><\/h3>\n\n***\n<b>Skewness<\/b> in the field of statistics can be said to be a measure of the imbalance and asymmetry of a data from its mean of distribution. Data is said to be skewed when its mean is not equal to its mode i.e when <b>mean > median<\/b> or when <b>median > mean<\/b>. In contrast, whenever data has its mean <b>equal<\/b> to its median, data is said to be <b>normally distributed<\/b>. Therefore, <b>skewed<\/b> data is one which is not <b>normally distributed<\/b>.\n<ul>\n                    <p><b>Skewness = 3 (mean - median) \/ standard deviation<\/b><\/p>\n    <\/ul>\n<h5>Types of Skew<\/h5>\n<p><b>Positive Skew<\/b>: Mean > median<\/p>\n<p><b>Negative Skew<\/b>: Median > mean<\/p>\n<p><b>Zero Skew<\/b>: Mean = median (normal distribution)<\/p>","b8f1bd8b":"<h2>8a. Undersampling<\/h2>\n\nIn this section of the kernel, I will perform an undersampling data technique known as <b>Random Undersampling<\/b>. The <b>Random Undersampling Technique<\/b> involves the removal of data points from the majority class so as to make the number of samples in the majority class to be <b>equal<\/b> to the number of samples in the minority class i.e a 50\/50 ratio of both classes.","f1dfce13":"<a id='load_dataset'><\/a>\n<h2>2b. Loading the Dataset<\/h2>","0bdbbd28":"<h3>Features to expect in Future Updates<\/h3>:\n\n1. Other models: Gaussian Naive Bayes, Support Vector Classifier and Decision Tree Classifier.\n2. Precision-Recall Curves to show how our model distinguishes the two classes from each other.\n3. Learning Curves to show each of the models' performance over experience and time.","194a0701":"<a id='nearmiss_before_cv'><\/a>\n<h3>i. Before Cross Validation<\/h3>\n\nThis is <font color='red'>NearMiss done wrong<\/font>. Here, the NearMiss algorithm is applied to the dataset before Cross Validation is carried out.","cf835f49":"If we had missing values in our dataset, we colud have replaced them by any of the following techniques:\n\n1. Dropping the samples which had missing values in any of their entries.\n2. Replacing by 0.\n3. Replacing by the mean, median or mode of the feature in which the missing value fall into.\n4. Forward\/Backward filling using the pandas library.\n5. Using regression models such as Linear Regression or K-Nearest Neighbors to predict the missing values.","93d2946c":"<a id='resample'><\/a>\n<h2>Part 5: Ways of Combating Imbalanced Classes in Machine Learning<\/h2>.\n\n<ul>\n    <li type=1><b>Collecting more data<\/b>\n        <ul>\n            <li>A larger dataset might expose a different and perhaps more balanced perspective on the classes.<\/li>\n            <li>Never underestimate the power of more data. <\/li>\n        <\/ul>\n        \n   <\/li>\n    <li type=1><b>Undersampling<\/b>\n        <ul>\n            <li>Deletes copies of the <b>over-represented<\/b> class in the dataset.<\/li>\n        <\/ul>\n   <\/li>\n    <li type=1><b>Oversampling<\/b>\n        <ul>\n            <li>Adds copies of the <b>under-represented<\/b> class to the dataset.<\/li>\n        <\/ul>\n   <\/li>\n    <li type=1><b>Use of the NearMiss Algorithm<\/b>\n        <ul>\n            <li>NearMiss is a special undersampling technique.<\/li>\n            <li>Makes the majority class equal to the minority class.<\/li>\n        <\/ul>\n   <\/li>\n   <li type=1><b>Use of SMOTE<\/b>\n        <ul>\n            <li><b>SMOTE<\/b> stands for <b>Synthetic Minority Oversampling technique<\/b>.<\/li>\n            <li>Creates <b>synthetic(not duplicates)<\/b> samples of the minority class in the dataset.<\/li>\n        <\/ul>\n   <\/li>\n   <li type=1><b>Anomaly Detection<\/b>\n        <ul>\n            <li>This is the detection of rare events in a dataset.<\/li>\n            <li>Considers the minor class as the outliers class which might help you think of new ways to separate and classify samples.<\/li>\n        <\/ul>\n   <\/li>\n <\/ul>\n \n \n Further Readings: 8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset by Jason Brownlee from Machine Learning Mastery https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/","ee89cca9":"<a id='import_libraries'><\/a>\n<h1>Part 2: Import Necessary Libraries and Dataset<\/h1>\n\n***\n\n<h2>2a. Import the libaries<\/h2>","f08b0512":"<a id='imbalance_data_on_classifier'><\/a>\n<h2>6. Using the Imbalanced Dataset on the Classifiers<\/h2>\n\nHere, I will train the classifiers with the imbalanced dataset.","f5a94fb3":"<a id='split_data'><\/a>\n<h2>4b. Splitting of Data<\/h2>\n\n<a id='seperate'><\/a>\n<h3>i. Seperating the Independent variables from the Dependent variable<\/h3>\n\nIndependent variables are the features in a dataset which are used to obtain the dependent variable. The dependent variable is the target variable which we are trying to find. Seperating the independent variables from the dependent variable in machine learning is a very <b>crucial<\/b> task.","e370006c":"Before oversampling or undersmapling the data by using algorithms such as SMOTE and NearMiss, I will split the dataset into a training set and a test set. The purpose of this is to <b>train our model with the oversampled\/undersampled training set, and test it on the original testing set<\/b>.","83d7b74c":"The above is the dataset we are required to work on. It is required of us to use this dataset to create a model which will predict whether a transcation was fraudulent or not.","1e84198e":"<a id='glance_dataset'><\/a>\n<h2>2c. Glance at the Dataset<\/h2>","c8e0c29e":"From the percentages calculated, we can see that there is a very large disparity between the number of fraudulent transactions and non-fraudulent ones. If we feed this dataset into our predictive models, we will get errors as our algorithms will be bound to classify most transactions as non-fraudulent.","72986d9d":"Since we have been given the dataset we are to work on, we need not collect more data. In this kernel, I will:\n\n1. Perform Random undersampling.\n2. Use the NearMiss algorithm to perform undersampling.\n3. Use SMOTE to perform oversampling -- create synthetics samples of the minority class.\n\n<b>In future updates, I will use Anomaly Detection on this dataset<\/b>.\n\n\n\nBut before performing any form of undersampling and oversampling, I will train our classifiers with the imbalanced dataset in order to see how poorly machine learning models perform when fed with imbalanced data.","11278329":"#classification models that I will use in this dataset\n<ul>\n<li>Logistic Regression<\/li>\n<li>Decision Trees Classifier<\/li>\n<li>Random Forest Classifier<\/li>\n<li>Support Vector Classifier<\/li>\n<\/ul>","3ba13568":"We can see that the dataset has 284807 samples and 31 columns (30 features & 1 target variable), with each feature having all its values as floats; the target variable - Class - has its values as integers only, as we are to predict whether a transcation was non-fraudulent 0 or fraudlent 1.\n\nWe can also see that there are no missing values in any of the columns.","0436ba7b":"In order to make this kernel short and straight-forward, I will make use of only <b>Logistic Regression<\/b> as my model till the end of the kernel. I will also apply the NearMiss and SMOTE algorithms on the dataset. In future updates, I will do for the Gaussian Naive Bayes, Support Vector Classifier and the Decision Tree Classifier respectively.","606cc6d2":"<a id='during_cv'><\/a>\n<h3>ii. Undersampling During Cross Validation (Undersampling Done Right)<\/h3>\n\nHere, we will undersample our dataset after splitting it into training data and test data. This is the right way to do random undersampling.","b5beff15":"<a id='nearmiss_during_cv'><\/a>\n<h3>ii. During Cross Validation<\/h3>\n\nThis is <font color='red'>NearMiss done right<\/font>. Here, the NearMiss algorithm is applied to the dataset during Cross Validation.","25b4e4c1":"<a id='smote_before_cv'><\/a>\n<h3>i. SMOTE Before Cross Validation<\/h3>\n\nThis is <font color='red'>SMOTE done wrong<\/font>. Here, SMOTE is applied on the dataset before Cross Validation is carried out."}}