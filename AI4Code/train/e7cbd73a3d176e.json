{"cell_type":{"20465447":"code","ddc853de":"code","0ddf7a93":"code","62dd6e4d":"code","df111dfb":"code","5268ea39":"code","b1fe4f8a":"code","23bd3985":"code","6202be2f":"code","755a3068":"code","9bb233cf":"code","d19304f6":"code","4741374f":"code","1911368c":"code","1b0b6c62":"code","40789d9d":"code","872aebb9":"code","66518a22":"code","ff1a73c2":"code","d7835a75":"code","a9aaab32":"code","3472c48e":"code","84d172cb":"code","017bf391":"code","9ef101d9":"code","6aea66cd":"code","436c49a0":"code","bea3a047":"code","5d41869a":"code","d498f1e7":"code","a1fe0e30":"code","8812a3b6":"code","e3d37714":"markdown","19078142":"markdown","6e2e0d71":"markdown"},"source":{"20465447":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\nnp.set_printoptions(suppress=True)","ddc853de":"data = pd.read_csv('..\/input\/gapminder.csv')","0ddf7a93":"data.dtypes","62dd6e4d":"data.head(5)","df111dfb":"data = data.drop(['Region'], axis =1)","5268ea39":"data.isnull().sum()","b1fe4f8a":"X = data.drop(['life'], axis = 1)\ny = data.life","23bd3985":"X.head()","6202be2f":"y.head()","755a3068":"Training_Accuracy_Before = []\nTesting_Accuracy_Before = []\nTraining_Accuracy_After = []\nTesting_Accuracy_After = []\nModels = ['Linear Regression', 'Lasso Regression', 'Ridge Regression']","9bb233cf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","d19304f6":"logreg = LinearRegression()\nlogreg.fit(X_train, y_train)\n\ntrain_score = logreg.score(X_train, y_train)\nprint(train_score)\ntest_score = logreg.score(X_test, y_test)\nprint(test_score)\n\nTraining_Accuracy_Before.append(train_score)\nTesting_Accuracy_Before.append(test_score)","4741374f":"alpha_space = np.logspace(-4, 0, 30)   # Checking for alpha from .0001 to 1 and finding the best value for alpha\nalpha_space","1911368c":"ridge_scores = []\nridge = Ridge(normalize = True)\nfor alpha in alpha_space:\n    ridge.alpha = alpha\n    val = np.mean(cross_val_score(ridge, X, y, cv = 10))\n    ridge_scores.append(val)","1b0b6c62":"lasso_scores = []\nlasso = Lasso(normalize = True)\nfor alpha in alpha_space:\n    lasso.alpha = alpha\n    val = np.mean(cross_val_score(lasso, X, y, cv = 10))\n    lasso_scores.append(val)","40789d9d":"plt.figure(figsize=(8, 8))\nplt.plot(alpha_space, ridge_scores, marker = 'D', label = \"Ridge\")\nplt.plot(alpha_space, lasso_scores, marker = 'D', label = \"Lasso\")\nplt.legend()\nplt.show()","872aebb9":"# Performing GridSearchCV with Cross Validation technique on Lasso Regression and finding the optimum value of alpha\n\nparams = {'alpha': (np.logspace(-8, 8, 100))} # It will check from 1e-08 to 1e+08\nlasso = Lasso(normalize=True)\nlasso_model = GridSearchCV(lasso, params, cv = 10)\nlasso_model.fit(X_train, y_train)\nprint(lasso_model.best_params_)\nprint(lasso_model.best_score_)","66518a22":"# Using value of alpha as 0.0000171 to get best accuracy for Lasso Regression\nlasso = Lasso(alpha = 0.0000171, normalize = True)\nlasso.fit(X_train, y_train)\n\ntrain_score = lasso.score(X_train, y_train)\nprint(train_score)\ntest_score = lasso.score(X_test, y_test)\nprint(test_score)\n\nTraining_Accuracy_Before.append(train_score)\nTesting_Accuracy_Before.append(test_score)","ff1a73c2":"# Performing GridSearchCV with Cross Validation technique on Ridge Regression and finding the optimum value of alpha\n\nparams = {'alpha': (np.logspace(-8, 8, 100))} # It will check from 1e-08 to 1e+08\nridge = Ridge(normalize=True)\nridge_model = GridSearchCV(ridge, params, cv = 10)\nridge_model.fit(X_train, y_train)\nprint(ridge_model.best_params_)\nprint(ridge_model.best_score_)","d7835a75":"# Using value of alpha as 0.020092 to get best accuracy for Ridge Regression\nridge = Ridge(alpha = 0.020092, normalize = True)\nridge.fit(X_train, y_train)\n\ntrain_score = ridge.score(X_train, y_train)\nprint(train_score)\ntest_score = ridge.score(X_test, y_test)\nprint(test_score)\n\nTraining_Accuracy_Before.append(train_score)\nTesting_Accuracy_Before.append(test_score)","a9aaab32":"coefficients = lasso.coef_\ncoefficients","3472c48e":"plt.figure(figsize = (10, 6))\nplt.plot(range(len(X_train.columns)), coefficients)\nplt.xticks(range(len(X_train.columns)), X_train.columns.values, rotation = 90)\nplt.show()","84d172cb":"X_train.columns","017bf391":"X = data[['fertility', 'HIV', 'CO2', 'BMI_male', 'GDP', 'BMI_female', 'child_mortality']]\ny = data.life","9ef101d9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","6aea66cd":"logreg = LinearRegression()\nlogreg.fit(X_train, y_train)\n\ntrain_score = logreg.score(X_train, y_train)\nprint(train_score)\ntest_score = logreg.score(X_test, y_test)\nprint(test_score)\n\nTraining_Accuracy_After.append(train_score)\nTesting_Accuracy_After.append(test_score)","436c49a0":"# Performing GridSearchCV with Cross Validation technique on Lasso Regression and finding the optimum value of alpha\n\nparams = {'alpha': (np.logspace(-8, 8, 100))} # It will check from 1e-08 to 1e+08\nlasso = Lasso(normalize=True)\nlasso_model = GridSearchCV(lasso, params, cv = 10)\nlasso_model.fit(X_train, y_train)\nprint(lasso_model.best_params_)\nprint(lasso_model.best_score_)","bea3a047":"# Using value of alpha as 0.000705 to get best accuracy for Lasso Regression\nlasso = Lasso(alpha = 0.000705, normalize = True)\nlasso.fit(X_train, y_train)\n\ntrain_score = lasso.score(X_train, y_train)\nprint(train_score)\ntest_score = lasso.score(X_test, y_test)\nprint(test_score)\n\nTraining_Accuracy_After.append(train_score)\nTesting_Accuracy_After.append(test_score)","5d41869a":"# Performing GridSearchCV with Cross Validation technique on Ridge Regression and finding the optimum value of alpha\n\nparams = {'alpha': (np.logspace(-8, 8, 100))} # It will check from 1e-08 to 1e+08\nridge = Ridge(normalize=True)\nridge_model = GridSearchCV(ridge, params, cv = 10)\nridge_model.fit(X_train, y_train)\nprint(ridge_model.best_params_)\nprint(ridge_model.best_score_)","d498f1e7":"# Using value of alpha as 0.020092 to get best accuracy for Ridge Regression\nridge = Ridge(alpha = 0.020092, normalize = True)\nridge.fit(X_train, y_train)\n\ntrain_score = ridge.score(X_train, y_train)\nprint(train_score)\ntest_score = ridge.score(X_test, y_test)\nprint(test_score)\n\nTraining_Accuracy_After.append(train_score)\nTesting_Accuracy_After.append(test_score)","a1fe0e30":"plt.plot(Training_Accuracy_Before, label = 'Training_Accuracy_Before')\nplt.plot(Training_Accuracy_After, label = 'Training_Accuracy_After')\nplt.xticks(range(len(Models)), Models, Rotation = 45)\nplt.title('Training Accuracy Behaviour')\nplt.legend()\nplt.show()","8812a3b6":"plt.plot(Testing_Accuracy_Before, label = 'Testing_Accuracy_Before')\nplt.plot(Testing_Accuracy_After, label = 'Testing_Accuracy_After')\nplt.xticks(range(len(Models)), Models, Rotation = 45)\nplt.title('Testing Accuracy Behaviour')\nplt.legend()\nplt.show()","e3d37714":"Through above graph, we can see that accuracy reduces as value for alpha increases. But best value of alpha can be occupied using GridSearchCV with Cross Validation technique. As, in above chart, CV was not performed, hence we can't have confidence in what we are seeing.","19078142":"**Observations:**  \n  \n  Above charts clearly shows that:  \n    \n    1. Linear Regression did the highest overfitting during its training phase and hence performed worst on test set  \n    2. Ridge didn't overfit the training data much during its training phase, and hence performed good on test set  \n    3. Post ignoring irrelevant feature with the help of  Lasso regression, we fitted the models, and Ridge performed well  \n    4. Moreover, after removal of irrelevant feature:  \n                a) 'Training_Accuracy_After' less overfitted the training data and hence its line curve is lower that 'Testing_Accuracy_Before' (Can be seen in first chart)  \n                b) 'Testing_Accuracy_After' better predicted the testing data and hence its line curve is greater than 'Testing_Accuracy_Before' (Can be seen in Second chart)","6e2e0d71":"After looking at above features, we get to know that prevalent features are:\n    'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP', 'BMI_female', 'child_mortality'"}}