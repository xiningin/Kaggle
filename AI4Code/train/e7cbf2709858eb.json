{"cell_type":{"e37bc941":"code","05281623":"code","eaa4b292":"code","82300780":"code","d22b9a0b":"code","895cc7c1":"code","1becdedb":"code","ce5502da":"code","e85e2633":"code","d270a9bc":"code","a1435ea3":"code","72d9df2e":"code","a7b91896":"code","be06efac":"code","ccb24863":"code","9b2423df":"code","3cc5bd76":"code","573e0b9e":"code","a5538ec6":"code","e9c26c95":"code","8b3746f4":"markdown","d7b59252":"markdown","8284b5af":"markdown","cdee77bf":"markdown","aeb41102":"markdown","74913501":"markdown","4e08716a":"markdown","7f6460f1":"markdown","d168c7db":"markdown","be47852c":"markdown","cf05bf55":"markdown","b0effbd7":"markdown","25abb7ed":"markdown","479c2902":"markdown","0decc198":"markdown","3719e299":"markdown","d1d9e8ed":"markdown","4c5345e5":"markdown","2743daa4":"markdown","e42b4d5a":"markdown","1f21d9eb":"markdown","394619e2":"markdown","2d607c85":"markdown","3105529c":"markdown","dbb37585":"markdown","c87e5721":"markdown"},"source":{"e37bc941":"%matplotlib inline\n\nimport os\nfrom itertools import permutations\nfrom collections import Counter\n\n# data manipulation\nimport numpy as np \nimport pandas as pd \n\n# charts\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n# ML models\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import roc_curve, precision_recall_fscore_support, roc_auc_score, precision_recall_curve, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier, EasyEnsembleClassifier \nfrom imblearn.over_sampling import SMOTE","05281623":"LOAD_DIR = '\/kaggle\/input\/creditcardfraud'\nFILENAME = 'creditcard.csv'\nLOAD_PATH = os.path.join(LOAD_DIR, FILENAME)","eaa4b292":"%%time\ndata = pd.read_csv(LOAD_PATH)","82300780":"data.shape","d22b9a0b":"data.head()","895cc7c1":"data.info()","1becdedb":"data.describe().round(2)","ce5502da":"target_col = ['Class']\nexplicit_cols = ['Time', 'Amount']\nV_cols = np.setdiff1d(data.columns, explicit_cols + target_col)\nfeature_cols = np.r_[V_cols, explicit_cols]","e85e2633":"class_pct = (np.bincount(data['Class']) \/ data.shape[0] * 100).round(2) \nprint(f'normal transactions (negative class): {class_pct[0]}%')\nprint(f'fraud transactions (positive class): {class_pct[1]}%')\nprint(dict(Counter(data['Class'])))","d270a9bc":"def plot_density(frame, col):\n    both = frame[col]\n    normal = frame.loc[frame['Class'] == 0,col]\n    fraud = frame.loc[frame['Class'] == 1,col]\n    data = [both, normal, fraud]\n    colors = ['b', 'g', 'r']\n    titles = ['All transactions', 'Normal transactions', 'Fraud transactions']\n    \n    fig, ax = plt.subplots(1, 3, figsize=(18, 5), sharex='row')\n    for axi, data, color, title in zip(ax, data, colors, titles):\n        sns.distplot(data, \n                     hist=False,\n                     rug=True,\n                     kde_kws=dict(shade=True), \n                     color=color,\n                     ax=axi)\n        axi.set_title(title)","a1435ea3":"%%time\nplot_density(data, 'Time')\nplot_density(data, 'Amount')","72d9df2e":"with sns.axes_style('whitegrid'):\n    ax = data[V_cols].plot.box(figsize=(10, 30), vert=False);","a7b91896":"X = data[feature_cols].values\ny = data[target_col].values.flatten()\nX.shape, y.shape","be06efac":"def stratified_shuffle_split(X, y, train_size=None, val_size=None, test_size=None):\n    assert not(train_size is None and test_size is None), 'both train_size and val_size are unfilled'\n    \n    if val_size is None:\n        val_size = 0\n    if train_size is None:\n        train_size = X.shape[0] - test_size\n    elif test_size is None:\n        test_size = X.shape[0] - train_size\n        \n    assert isinstance(train_size, int) and isinstance(test_size, int) and isinstance(val_size, int), \\\n           'sizes must be integers'\n    \n    ssp = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n    train_val_index, test_index = next(ssp.split(X, y))\n    \n    X_train_val = X[train_val_index]\n    y_train_val = y[train_val_index]\n    X_test = X[test_index]\n    y_test = y[test_index]\n\n    if val_size != 0:\n        ssp2 = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=42)\n        train_index, val_index = next(ssp2.split(X_train_val, y_train_val))\n        \n        X_train = X_train_val[train_index]\n        y_train = y_train_val[train_index]\n        X_val = X_train_val[val_index]\n        y_val = y_train_val[val_index]\n    \n        return X_train, X_val, X_test, y_train, y_val, y_test\n    else:\n        X_train = X_train_val\n        y_train = y_train_val\n        \n    return X_train, X_test, y_train, y_test","ccb24863":"size = X.shape[0]\nvalid_size = int(np.ceil(size * 0.15))\ntest_size = int(valid_size)\ntrain_size = int(size - valid_size - test_size)\nassert train_size + valid_size + test_size == size","9b2423df":"X_train, X_valid, X_test, y_train, y_valid, y_test = stratified_shuffle_split(X, y, train_size=train_size, test_size=test_size, val_size=valid_size)\nprint('shape of splits:')\nprint(X_train.shape, X_valid.shape, X_test.shape, y_train.shape, y_valid.shape, y_test.shape, end='\\n\\n')\n\ntrain_abs = np.unique(y_train, return_counts=True)[1]\nval_abs = np.unique(y_valid, return_counts=True)[1]\ntest_abs = np.unique(y_test, return_counts=True)[1]\ntotal_abs = train_abs  + test_abs + val_abs\n\nprint('percentage of class samples per splits:')\nprint(train_abs \/ total_abs * 100)\nprint(test_abs \/ total_abs * 100)\nprint(test_abs \/ total_abs * 100)","3cc5bd76":"def plot_classifier_roc_auc(classifier, recall_threshold=None, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid):\n    classifier.fit(X_train, y_train)\n    y_valid_proba = classifier.predict_proba(X_valid)[:, 1]\n    y_valid_pred = (y_valid_proba >= 0.5)\n\n    auc = roc_auc_score(y_valid, y_valid_proba) \n    fpr, tpr, _ = roc_curve(y_valid, y_valid_proba)\n    \n    if recall_threshold:\n        precisions, recalls, thresholds = precision_recall_curve(y_valid, y_valid_proba)\n        i = np.argmin(recalls >= recall_threshold)\n        recall_threshold = recalls[i]\n        threshold = thresholds[i]\n        y_valid_pred = (y_valid_proba >= threshold)\n    else:\n        precision, recall, fscore, support = precision_recall_fscore_support(y_valid, y_valid_pred)\n        recall_threshold = recall[1]\n    \n    FPR = fpr[np.argmax(tpr >= recall_threshold)]\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.plot(fpr, tpr, linewidth=2)\n    ax.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\n    ax.hlines(recall_threshold, 0, FPR, linestyle='--', color='red')\n    ax.vlines(FPR, 0, recall_threshold, linestyle='--', color='red')\n    \n    report = classification_report(y_valid, y_valid_pred)\n    label = '\\n'.join(report.split('\\n')[:4]) + \\\n            '\\n' + 25*\" \" + f'AUC = {np.round(auc, 4)}' \n    ax.scatter([FPR], [recall_threshold], s=50, c='r', label=label)\n    \n    ax.set_xlabel('FPR')\n    ax.set_ylabel('TPR (Recall)')\n    ax.set_title(classifier.__class__.__name__)\n    fig.legend(loc=(0.365, 0.15))\n    fig.show()","573e0b9e":"%%time\nclassifiers_undersample =[\n    RandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', random_state=0, n_jobs=-1),\n    BalancedBaggingClassifier(n_estimators=100, replacement=True, random_state=0, n_jobs=-1),\n    BalancedRandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', replacement=True, random_state=0, n_jobs=-1),\n    EasyEnsembleClassifier(n_estimators=100, replacement=True, random_state=0, n_jobs=-1),\n    AdaBoostClassifier(n_estimators=100, random_state=0),\n    GradientBoostingClassifier(n_estimators=100, max_features='auto', random_state=0)\n]\n\nwith sns.axes_style('whitegrid'):\n    for classifier in classifiers_undersample:\n        print(classifier.__class__.__name__)\n        %time plot_classifier_roc_auc(classifier)\n        print()\nprint('TOTAL')","a5538ec6":"X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\nCounter(y_resampled), Counter(y_train)","e9c26c95":"%%time\nclassifiers_oversample =[\n    RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1),\n    ExtraTreesClassifier(n_estimators=100, bootstrap=True, random_state=0, n_jobs=-1),\n    AdaBoostClassifier(n_estimators=100, random_state=0),\n    GradientBoostingClassifier(n_estimators=100, max_features='auto', random_state=0)\n]\n\nwith sns.axes_style('whitegrid'):\n    for classifier in classifiers_oversample:\n        print(classifier.__class__.__name__)\n        %time plot_classifier_roc_auc(classifier, X_train=X_resampled, y_train=y_resampled)\n        print()    \nprint('TOTAL')","8b3746f4":"**Horrible! Huge imbalance! Too few a positive class data.**","d7b59252":"# <a name=\"id5\"><\/a> Build Classification Model","8284b5af":"**Yes, even having very few positive class instances we have to hold out 30% of them!**","cdee77bf":"**This as much compact as possible visualization proves assertions about skewness and kurtosis of *V*-features.**","aeb41102":"### Split Data","74913501":"# <a name=\"id1\"><\/a> Download data","4e08716a":"**We observe that oversampling didn't make things much better. But it worths to point out that  \noversampling in some models resulted in a little bit better recall-precision trade-off in context  \nthat high recall is more important then precision, but the last one is still important :).  \nFor now, Random Forest trained on a oversampled data looks like the best ... from uglies.  \nIn the next section we'll try to fit a Neural Network.**","7f6460f1":"# <a name=\"id3\"><\/a> Take a closer look. Visualize","d168c7db":"**As a *describe* method shown, *Amount* feature is highly skewed. I would not assert  \nthat distributions for fraud and non-fraud transactions across these two variables  \nare significantly different.**","be47852c":"**The dataset contains 284 807 instances and 31 features (1 of them is target)**","cf05bf55":"# <a name=\"id4\"><\/a> Prepare Data","b0effbd7":"**coming soon ... **","25abb7ed":"**This notebook is about handling highly imbalanced classes in a classification tasks.  \nThe example is built on a fraud card transaction detection.**","479c2902":"**Note that we roughly doubled the size of the training set. This will increase training time**","0decc198":"**We see that all the *V*-features are centered (0 mean) but have different variance.  \nPercentiles also tell us that there are highly skewed and narrowed features.**","3719e299":"* [Download data](#id1)  \n* [Take a quick look on a dataset](#id2)  \n* [Take a closer look. Visualize](#id3)  \n* [Prepare Data](#id4)  \n* [Build Classification Model](#id5)  ","d1d9e8ed":"**The good news is that we will not do any complex data preprocessing steps. The reason  \nis that we'll use ensembles of decision trees and the thing I really like about them is  \nthat they almost don't need any data prepatation steps, e.g. scaling is not required.  \nSo, now we only have to split data in the training, validation and test sets using  \nstratified(!) sampling.**\n","4c5345e5":"### Models with Undersampling","2743daa4":"**We will try two main techniques of handling imbalanced data:  \n1) undersampling (+ balanced class weights)  \n2) oversampling** ","e42b4d5a":"**All the data is numeric and clean.**","1f21d9eb":"**Unfortunately, we see that none of the undersampling techniques helped to build a good  \nclassifier, even with balancing class weights. The priority metric is positive class recall  \nand we failed to get even 90% without high drop in precision.**","394619e2":"**Let's don't try simple techniques and instantly apply smart one. The SMOTE (Synthetic  \nMinority Over-sampling Technique) is a great option.**","2d607c85":"**Take a look on distributions.**","3105529c":"**We have a binary classification task and it's good to know a class balance.**","dbb37585":"### Models with Oversampling ","c87e5721":"# <a name=\"id2\"><\/a> Take a quick look on a dataset"}}