{"cell_type":{"752e7c75":"code","31cbdd0a":"code","9e16ed1e":"code","76131aef":"code","f2514c43":"code","cd9b6dfe":"code","622f312b":"code","1989962f":"code","4b4e8c6c":"code","d48db8c0":"code","b4ff3f38":"code","47cfadc9":"code","30955894":"code","7446e069":"code","d3f5c269":"code","83ea5f51":"code","eedd0873":"code","b7f53cb7":"code","322a8bc2":"code","f4110056":"code","454c1d1a":"code","97b2c6b5":"code","4b8620d1":"code","4324d822":"code","642f4fc7":"markdown","c5f7229f":"markdown","64dac8e7":"markdown","82688629":"markdown","51cc989d":"markdown","0801235a":"markdown","eb163f3a":"markdown","ac524baa":"markdown","156fffab":"markdown","5d85a00e":"markdown","fd9a76b0":"markdown","e3dcf77f":"markdown","681af650":"markdown","d9bb695b":"markdown","32a1ae28":"markdown","e12120a9":"markdown","907d2aa2":"markdown","58d14bd9":"markdown","b5c4b7fd":"markdown","a20d2fe6":"markdown","9d493508":"markdown","2f7f8be5":"markdown"},"source":{"752e7c75":"import numpy as np\nimport pandas as pd\n\ndf_train = pd.read_csv('..\/input\/application_train.csv')\ntarget_count = df_train.TARGET.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","31cbdd0a":"df_train.head(5)","9e16ed1e":"# Class count\ncount_class_0, count_class_1 = df_train.TARGET.value_counts()\n\n# Divide by class\ndf_class_0 = df_train[df_train['TARGET'] == 0]\ndf_class_1 = df_train[df_train['TARGET'] == 1]","76131aef":"df_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.TARGET.value_counts())\n\ndf_test_under.TARGET.value_counts().plot(kind='bar', title='Count (TARGET)');","f2514c43":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.TARGET.value_counts())\n\ndf_test_over.TARGET.value_counts().plot(kind='bar', title='Count (TARGET)');","cd9b6dfe":"import imblearn","622f312b":"from sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\n\ndf = pd.DataFrame(X)\ndf['TARGET'] = y\ndf.TARGET.value_counts().plot(kind='bar', title='Count (TARGET)');","1989962f":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","4b4e8c6c":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')","d48db8c0":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(return_indices=True)\nX_rus, y_rus, id_rus = rus.fit_sample(X, y)\n\nprint('Removed indexes:', id_rus)\n\nplot_2d_space(X_rus, y_rus, 'Random under-sampling')","b4ff3f38":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\n\nplot_2d_space(X_ros, y_ros, 'Random over-sampling')","47cfadc9":"from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks(return_indices=True, ratio='majority')\nX_tl, y_tl, id_tl = tl.fit_sample(X, y)\n\nprint('Removed indexes:', id_tl)\n\nplot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')","30955894":"from imblearn.under_sampling import ClusterCentroids\n\ncc = ClusterCentroids(ratio={0: 10})\nX_cc, y_cc = cc.fit_sample(X, y)\n\nplot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')","7446e069":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')","d3f5c269":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek(ratio='auto')\nX_smt, y_smt = smt.fit_sample(X, y)\n\nplot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')","83ea5f51":"# Deploying Logistic Regression\n#Splitting the dataset\n#Keep the following 6 features (variables) which are important\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_smt, y_smt, test_size=0.2, random_state=0)\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","eedd0873":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","b7f53cb7":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))","322a8bc2":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label');","f4110056":"def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    ax.axis('off')\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)","454c1d1a":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=0)\nvisualize_classifier(model, X_smt, y_smt);","97b2c6b5":"from sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_smt, y_smt,\n                                                random_state=0)\nmodel = RandomForestClassifier(n_estimators=1000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","4b8620d1":"from sklearn import metrics\nprint(metrics.classification_report(y_pred, y_test))","4324d822":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label');","642f4fc7":"**Over-sampling followed by under-sampling **","c5f7229f":"**Under-sampling: Tomek links **\u00b6","64dac8e7":"**Python imbalanced-learn module **","82688629":"This technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.\nIn this example we will pass the {0: 10} dict for the parameter ratio, to preserve 10 elements from the majority class (0), and all minority class (1) .","51cc989d":"**Over-sampling: SMOTE **","0801235a":"SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\nWe'll use ratio='minority' to resample the minority class.","eb163f3a":"A number of more sophisticated resapling techniques have been proposed in the scientific literature.\nFor example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\nLet's apply some of these resampling techniques, using the Python library imbalanced-learn. It is compatible with scikit-learn and is part of scikit-learn-contrib projects.","ac524baa":"<h1> Random Forest Classifier ","156fffab":"Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\nIn the code below, we'll use ratio='majority' to resample the majority class.","5d85a00e":"Because the dataset has many dimensions (features) and our graphs will be 2D, we will reduce the size of the dataset using Principal Component Analysis (PCA):","fd9a76b0":"<h1>Deploying Machine Learning Model over Resampled Dataset ","e3dcf77f":"**Under-sampling: Cluster Centroids** ","681af650":"**Random under-sampling**","d9bb695b":"A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).\nDespite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\nLet's implement a basic example, which uses the DataFrame.sample method to get random samples each class:","32a1ae28":"Here I will work on some techniques to handle highly unbalanced datasets, with a focus on resampling. The Home Credit Risk Prediction competition, is a classic problem of unbalanced classes, since Credit Loan in risk can be considered unusual cases when considering all clients. Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks.\nLet's see how unbalanced the dataset is:","e12120a9":"**Introduction**\nReal world datasets commonly show the particularity to have a number of samples of a given class under-represented compared to other classes. This imbalance gives rise to the \u201cclass imbalance\u201d problem (or \u201ccurse of imbalanced datasets\u201d) which is the problem of learning a concept from the class that has a small number of samples.\nThe class imbalance problem has been encountered in multiple areas such as telecommu- nication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition. Imbalanced data substantially compromises the learning process, since most of the standard machine learning algorithms expect balanced class dis- tribution or an equal misclassification cost. Related Inferential Statistics post showing bootstrap permutation ECDF plots: https:\/\/www.kaggle.com\/tini9911\/data-wrangling-eda-inferential-statistics-ml-model","907d2aa2":"For ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:","58d14bd9":"<h1> Resampling","b5c4b7fd":"Now, we will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques:","a20d2fe6":"**Random over-sampling**","9d493508":"**Random under-sampling and over-sampling with imbalanced-learn **","2f7f8be5":"We will also create a 2-dimensional plot function, plot_2d_space, to see the data distribution:"}}