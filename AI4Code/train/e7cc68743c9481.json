{"cell_type":{"ba013199":"code","fff1eb0d":"code","8b24427d":"code","fcf6bb40":"code","561e120d":"code","506a2edb":"code","d7eb1a44":"code","0b822e83":"code","a8a68029":"code","ce9f0452":"code","3794614c":"code","a82f1674":"code","daeb8edf":"code","42e4c2ae":"code","f7635e27":"code","921adf64":"code","6dd4611c":"code","4f692d00":"code","2ea8304a":"code","208883f2":"code","ef6a33a7":"code","042444d5":"code","fe41c0b1":"code","3fc8d7ff":"code","37cb6d61":"code","d7cff54a":"code","694f96df":"code","bf22b7b1":"markdown","0d34b64f":"markdown","86e0e7fe":"markdown","63f4b3d9":"markdown","99e9a14a":"markdown","099b6911":"markdown","51d15ad9":"markdown","ea651b90":"markdown","ace1f242":"markdown","eeb0fb35":"markdown","cb33f447":"markdown","8aada425":"markdown","6694b759":"markdown","5e840479":"markdown","ff60bd3e":"markdown","3c30cdfd":"markdown","196f3ddb":"markdown","98b35d17":"markdown","2f88cc3d":"markdown","daf0a912":"markdown","a82e9715":"markdown","0e713f1b":"markdown","e5fdae3d":"markdown","533d3d49":"markdown","33499d05":"markdown","151c3305":"markdown","04ebf1fa":"markdown","b0661855":"markdown","89da6f4f":"markdown","a4c261b4":"markdown","10e90e6f":"markdown","9cc4497a":"markdown","4ff55426":"markdown","8dbc021f":"markdown","246c6cce":"markdown","80e8ad6a":"markdown","cdd1ae85":"markdown"},"source":{"ba013199":"# Importaci\u00f3n de paquetes\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\n\n%matplotlib inline\n\nnp.random.seed(1) # se establece una semilla para que los resultados sean iguales (necesario para la correcci\u00f3n del taller)","fff1eb0d":"from shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/testcases-v2\/testCases_v2.py\", dst = \"..\/working\/testCases_v2.py\")\ncopyfile(src = \"..\/input\/planar-utils0-py\/planar_utils0.py\", dst = \"..\/working\/planar_utils0.py\")\n\n# import all our functions\n\nfrom planar_utils0 import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\nfrom testCases_v2 import *","8b24427d":"X, Y = load_planar_dataset()","fcf6bb40":"# Visualice los datos:\nplt.scatter(X[0, :], X[1, :], c=Y[0,:], s=40, cmap=plt.cm.Spectral);","561e120d":"### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 3 l\u00edneas de c\u00f3digo)\nshape_X = \nshape_Y = \nm =           # tama\u00f1o del conjunto de entrenamiento\n### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\nprint ('Las dimensiones de X son: ' + str(shape_X))\nprint ('Las dimensiones de Y son: ' + str(shape_Y))\nprint ('Hay m = %d ejemplos de entrenamiento' % (m))","506a2edb":"# Entrene el clasificador log\u00edstico\nclf = sklearn.linear_model.LogisticRegressionCV();\nY2=Y.flatten()\nclf.fit(X.T, Y2);","d7eb1a44":"# Grafique la frontera de decisi\u00f3n para la regresi\u00f3n log\u00edstica\nplot_decision_boundary(lambda x: clf.predict(x), X, Y2)\nplt.title(\"Regresi\u00f3n log\u00edstica\")\n\n# Precisi\u00f3n\nLR_predictions = clf.predict(X.T)\nprint ('Precisi\u00f3n de la regresi\u00f3n log\u00edstica: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))\/float(Y.size)*100) +\n       '% ' + \"(porcentaje de puntos correctamente etiquetados)\")","0b822e83":"# FUNCI\u00d3N A CALIFICAR: layer_sizes\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Input:\n    X: conjunto de datos de entrada de dimensi\u00f3n (tama\u00f1o del input, n\u00famero de ejemplos)\n    Y: etiquetas de tama\u00f1o (tama\u00f1o del output, n\u00famero de ejemplos)\n    Output:\n    n_x: tama\u00f1o de la capa de entrada\n    n_h: tama\u00f1o de la capa escondida\n    n_y: tama\u00f1o de la capa de salida\n    \"\"\"\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 3 l\u00edneas de c\u00f3digo)\n    n_x =    # tama\u00f1o de la capa de entrada\n    n_h = \n    n_y =    # tama\u00f1o de la acapa de salida\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    return (n_x, n_h, n_y)","a8a68029":"X_assess, Y_assess = layer_sizes_test_case()\n(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\nprint(\"El tama\u00f1o de la capa de entrada es: n_x = \" + str(n_x))\nprint(\"El tama\u00f1o de la capa de escondida es: n_h = \" + str(n_h))\nprint(\"El tama\u00f1o de la capa de salida es: n_y = \" + str(n_y))","ce9f0452":"# FUNCI\u00d3N A CALIFICAR: initialize_parameters\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Input:\n    n_x: tama\u00f1o de la capa de entrada\n    n_h: tama\u00f1o de la capa escondida\n    n_y: tama\u00f1o de la capa de salida\n    Output:\n    params: diccionario python con los par\u00e1metros:\n                    W1: matriz de pesos con dimensiones (n_h, n_x)\n                    b1: matriz de sesgos con dimensiones (n_h, 1)\n                    W2: matriz de pesos con dimensiones (n_y, n_h)\n                    b2: matriz de sesgos con dimensiones (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(2) # no cambie esta semilla para la replicabilidad de la simulaci\u00f3n.\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 4 l\u00edneas de c\u00f3digo)\n    W1 = \n    b1 = \n    W2 = \n    b2 = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    assert (W1.shape == (n_h, n_x))\n    assert (b1.shape == (n_h, 1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","3794614c":"n_x, n_h, n_y = initialize_parameters_test_case()\n\nparameters = initialize_parameters(n_x, n_h, n_y)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","a82f1674":"# FUNCI\u00d3N A CALIFICAR: forward_propagation\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Input: \n    X: datos de entrada de tama\u00f1o (n_x, m)\n    parameters: diccionario python con los parameters (salida de la funcion de inicializaci\u00f3n)\n    Output:\n    A2: la salida de la funci\u00f3n sigmoide de la segunda activaci\u00f3n\n    cache: un diccionario conteniendo \"Z1\", \"A1\", \"Z2\" y \"A2\"\n    \"\"\"\n    # Recupere cada par\u00e1metro del diccionario \"parameters\"\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 4 l\u00edneas de c\u00f3digo)\n    W1 = \n    b1 =  \n    W2 =  \n    b2 =  \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Implemente la propagaci\u00f3n hacia delante para calcular A2 (probabilidades)\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 4 l\u00edneas de c\u00f3digo)\n    Z1 =  \n    A1 =  \n    Z2 =  \n    A2 =  \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","daeb8edf":"X_assess, parameters = forward_propagation_test_case()\nA2, cache = forward_propagation(X_assess, parameters)\n\nprint(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))","42e4c2ae":"# FUNCI\u00d3N A CALIFICAR: compute_cost\n\ndef compute_cost(A2, Y, parameters):\n    \"\"\"\n    Compute el coste de entrop\u00eda cruzada de la ecuaci\u00f3n (13)\n    Input:\n    A2: la salida de la funci\u00f3n sigmoide para la segunda activaci\u00f3n, de tama\u00f1o (1, numero de ejemplos)\n    Y: vector de etiquetas de tama\u00f1o (1, numero de ejemplos)\n    parametros: diccionario python con los parametros W1, b1, W2 and b2\n    Ouput:\n    cost: coste de entrop\u00eda cruzada \n    \"\"\"\n    \n    m = Y.shape[1] # numero de ejemplos\n\n    # Coste de entrop\u00eda cruzada\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n    logprobs = \n    cost = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    cost = np.squeeze(cost)     # se asegura que el coste sea de la dimension esperada, e.g. un [[99]] lo torna en 99 \n    assert(isinstance(cost, float))\n    \n    return cost","f7635e27":"A2, Y_assess, parameters = compute_cost_test_case()\n\nprint(\"coste = \" + str(compute_cost(A2, Y_assess, parameters)))","921adf64":"# FUNCI\u00d3N A CALIFICAR: backward_propagation\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implemente Retro-propagacion.\n    Input:\n    parametros: diccionario python con los parameteros \n    cache: un diccionario con \"Z1\", \"A1\", \"Z2\" y \"A2\".\n    X: datos de entrada de tama\u00f1o (2, numero de ejemplos)\n    Y: vector de etiquetas de tama\u00f1o (1, numero de ejemplos)\n    Output:\n    grads: diccionario python con los gradientes de los diferentes parametros\n    \"\"\"\n    m = X.shape[1]\n    \n    # Recupere W1 y W2 del diccionario \"parameters\".\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###(\u2248 2 l\u00edneas de c\u00f3digo)\n    W1 = \n    W2 =\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n    # Recupere A1 y A2 del diccionario \"cache\".\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n    A1 = \n    A2 = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Retro-propagacion: calcule dW1, db1, dW2, db2. \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 6 l\u00edneas de c\u00f3digo)\n    dZ2 = \n    dW2 = \n    db2 = \n    dZ1 = \n    dW1 = \n    db1 = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads","6dd4611c":"parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n\ngrads = backward_propagation(parameters, cache, X_assess, Y_assess)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))","4f692d00":"# FUNCI\u00d3N A CALIFICAR: update_parameters\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Actualice los parametros usando la regla de actualizaci\u00f3n del GD\n    Input:\n    parametros: diccionario python con los parametros \n    grads: diccionario python con los gradientes \n    Output:\n    parametros: diccionario python con los parametros actualizados \n    \"\"\"\n    # Recupere cada parametro del diccionario \"parameters\"\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 4 l\u00edneas de c\u00f3digo)\n    W1 = \n    b1 = \n    W2 = \n    b2 = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Recupere cada gradiente del diccionario \"grads\"\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 4 l\u00edneas de c\u00f3digo)\n    dW1 = \n    db1 =\n    dW2 = \n    db2 =\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Regla de actualizaci\u00f3n para cada parametro\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 4 l\u00edneas de c\u00f3digo)\n    W1 = \n    b1 = \n    W2 = \n    b2 = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","2ea8304a":"parameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","208883f2":"# FUNCI\u00d3N A CALIFICAR: nn_model\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Input:\n    X: datos de entrada de tama\u00f1o (2, numero de ejemplos)\n    Y: vector de etiquetas de tama\u00f1o (1, numero de ejemplos)\n    n_h: tama\u00f1o de la capa escondida\n    num_iterations: numero de iteraciones del bucle del GD\n    print_cost: si \"True\", muestra el coste cada 1000 iteraciones\n    Output:\n    parameters: parametros aprendidos por el modelo. Pueden utilizarse para la predicci\u00f3n\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Inicialize los par\u00e1metros, luego recupere W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 5 l\u00edneas de c\u00f3digo)\n    parameters =\n    W1 = \n    b1 = \n    W2 = \n    b2 = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Bucle (GD)\n\n    for i in range(0, num_iterations):\n         \n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 4 l\u00edneas de c\u00f3digo)\n        # Propagaci\u00f3n hacia delante. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        A2, cache = \n        \n        # Funci\u00f3n de coste. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n        cost = \n \n        # Retro-propagacion. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        grads = \n \n        # Actualizacion de parametros por GD. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        parameters =\n        \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n        # Muestre el coste cada 1000 iteraciones\n        if print_cost and i % 1000 == 0:\n            print (\"Coste tras la iteraci\u00f3n %i: %f\" %(i, cost))\n\n    return parameters","ef6a33a7":"X_assess, Y_assess = nn_model_test_case()\nparameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","042444d5":"# FUNCI\u00d3N A CALIFICAR: predict\n\ndef predict(parameters, X):\n    \"\"\"\n    Usando las estimaciones de los par\u00e1metros, se debe predecir una clase para cada ehemplo de X\n    Input:\n    parameters: diccionario python con los parametros \n    X: datos de entrada de tama\u00f1o (n_x, m)\n    Output:\n    predictions: vector de predicciones para el modelo (rojo: 0 \/ azul: 1)\n    \"\"\"\n    \n    # Compute las probabilidades usando propagacion hacia delante, y clasifica a 0\/1 usando 0.5 como umbral.\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n    A2, cache = \n    predictions = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return predictions","fe41c0b1":"parameters, X_assess = predict_test_case()\n\npredictions = predict(parameters, X_assess)\nprint(\"Predicci\u00f3n media = \" + str(np.mean(predictions)))","3fc8d7ff":"# Construya un modelo con una capa escondida de dimensi\u00f3n $n_h=4$\n### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 l\u00ednea de c\u00f3digo)\nparameters = \n### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n# Grafique la frontera de decisi\u00f3n\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y2)\nplt.title(\"Frontera de decisi\u00f3n para una capa escondida de tama\u00f1o  \" + str(4))","37cb6d61":"# Print accuracy\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))\/float(Y.size)*100) + '%')","d7cff54a":"plt.figure(figsize=(16, 32))\n\n### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 l\u00ednea de c\u00f3digo)\nhidden_layer_sizes = \n### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Tama\u00f1o de la capa escondida %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y2)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))\/float(Y.size)*100)\n    print (\"Accuracy para {} unidades escondidas: {} %\".format(n_h, accuracy))","694f96df":"# Datasets\nnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n\ndatasets = {\"noisy_circles\": noisy_circles,\n            \"noisy_moons\": noisy_moons,\n            \"blobs\": blobs,\n            \"gaussian_quantiles\": gaussian_quantiles}\n\n### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (elija el conjunto de datos)\ndataset = \"\"\n### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\nX, Y = datasets[dataset]\nX, Y = X.T, Y.reshape(1, Y.shape[0])\n\n# binariza los c\u00famulos o clusters (blobs)\nif dataset == \"blobs\":\n    Y = Y%2\n\n# Visualiza los datos\nplt.scatter(X[0, :], X[1, :], c=Y[0,:], s=40, cmap=plt.cm.Spectral);","bf22b7b1":"### 4.1 - Definiendo la estructura de la red neuronal ####\n\n**Ejercicio**: Defina tres variables:\n    - n_x: tama\u00f1o de la capa de entrada\n    - n_h: tama\u00f1o de la capa oculta (en este ejemplo, fije su tama\u00f1o a 4) \n    - n_y: tama\u00f1o de la capa de salida\n\n**Ayuda**: Utilice las dimensiones de X e Y para encontrar n_x y n_y. ","0d34b64f":"Ahora se puede graficar la frontera de decisi\u00f3n de estos modelos. \n\nEjecute el siguiente c\u00f3digo.","86e0e7fe":"**Salida esperada**: \n       \n<table style=\"width:40%\">\n  \n  <tr>\n    <td>**Dimensiones de X**<\/td>\n    <td> (2, 400) <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**Dimensiones de Y**<\/td>\n    <td>(1, 400) <\/td> \n  <\/tr>\n  \n    <tr>\n    <td>**m**<\/td>\n    <td> 400 <\/td> \n  <\/tr>\n  \n<\/table>","63f4b3d9":"Ejecute de nuevo el modelo para (al menos uno de) los conjunto de datos a continuaci\u00f3n. Recuerde presentar los resultados del mejor modelo con un gr\u00e1fico de la frontera de decisi\u00f3n aprendida y su correspondiente medida de precisi\u00f3n ($accuracy$).","99e9a14a":"**Salida esperada**: \n\n<table style=\"width:90%\">\n  <tr>\n    <td>**W1**<\/td>\n    <td> [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]] <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**b1**<\/td>\n    <td> [[ 0.]\n [ 0.]\n [ 0.]\n [ 0.]] <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**W2**<\/td>\n    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]<\/td> \n  <\/tr>\n  \n\n  <tr>\n    <td>**b2**<\/td>\n    <td> [[ 0.]] <\/td> \n  <\/tr>\n  \n<\/table>\n\n","099b6911":"Ahora que ya has calculado $A^{[2]}$ (en la variable Python \"`A2`\"), conteniendo los $a^{[2](i)}$ para cada ejemplo ($i$), puede computar la funci\u00f3n de coste:\n\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n\n**Ejercicio**: Implemente `compute_cost()` para calcular el valor de la p\u00e9rdida $J$.\n\n**Instrucciones**:\n- Hay muchas maneras de implementar la p\u00e9rdida de entrop\u00eda-cruzada. Por ejemplo, se puede implementar de la siguiente manera\n$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n```python\nlogprobs = np.multiply(np.log(A2),Y)\ncost = - np.sum(logprobs)                # no hay necesidad de usar un bucle!\n```\n\n(puede usar tanto `np.multiply()` y luego `np.sum()`, como tambi\u00e9n `np.dot()`).\n","51d15ad9":"**Salida esperada**: \n<table style=\"width:20%\">\n  <tr>\n    <td>**coste**<\/td>\n    <td> 0.693058761... <\/td> \n  <\/tr>\n  \n<\/table>","ea651b90":"### 4.2 - Inicializaci\u00f3n de los par\u00e1metros del modelo ####\n\n**Ejercicio**: Implemente la funci\u00f3n `initialize_parameters()`.\n\n**Instrucciones**:\n- Aseg\u00farese de que las dimensiones de sus par\u00e1metros sean las correctas. Se puede referir al modelo de red neuronal de la figura anterior. \n- Inicialize los pesos con valores aleatorios. \n    - Use: `np.random.randn(a,b) * 0.01` para esta inicializaci\u00f3n aleatoria de una matriz de dimensiones (a,b).\n- Inicialize los vectores de sesgo con ceros. \n    - Use: `np.zeros((a,b))` para la inicializaci\u00f3n con ceros de la matriz de tama\u00f1o (a,b).","ace1f242":"### 4.6 - Optimize el tama\u00f1o de la capa escondida ###\n\nDe manera discreta, mediante un conjunto de posibles tama\u00f1os para la capa escondida (numero de neuronas en la capa escondida): hidden_layer_sizes = [1, 2, .. , H], encuentre el valor que logre los mejores resultados. Podr\u00e1 observar distintos comportamientos del modelo para distintos tama\u00f1os de la capa escondida. ","eeb0fb35":"**Salida esperada**: \n\n\n<table style=\"width:80%\">\n  <tr>\n    <td>**W1**<\/td>\n    <td> [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]<\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**b1**<\/td>\n    <td> [[ -1.02420756e-06]\n [  1.27373948e-05]\n [  8.32996807e-07]\n [ -3.20136836e-06]]<\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**W2**<\/td>\n    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] <\/td> \n  <\/tr>\n  \n\n  <tr>\n    <td>**b2**<\/td>\n    <td> [[ 0.00010457]] <\/td> \n  <\/tr>\n  \n<\/table>  ","cb33f447":"**Salida esperada**:\n\n<table style=\"width:50%\">\n  <tr>\n      \n    <td>**Coste tras la iteraci\u00f3n 9000**<\/td>\n    <td> 0.218571 <\/td> \n  <\/tr>\n  \n<\/table>\n","8aada425":"**Ejercicio**: Implemente la regla de actualizaci\u00f3n. Use el m\u00e9todo de Descenso del Gradiente (GD). Debe usar (dW1, db1, dW2, db2) para actualizar (W1, b1, W2, b2).\n\n**Regla general del GD**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ donde $\\alpha$ es la tasa de aprendizaje y $\\theta$ representa un par\u00e1metro.\n","6694b759":"**Interpretaci\u00f3n**:\n- Modelos m\u00e1s complejos (con m\u00e1s unidades escondidas) logran un mejor ajuste al conjunto de entrenamiento, hasta que eventualmente los modelos m\u00e1s grandes se sobreajustan a los datos. \n- EL mejor tama\u00f1o para la capa escondida para estar alrededor de $n_h = 5$. De esta manera, un valor de esta magnitud parece ajustarse a los datos de buena manera, controlando el posible sobreajuste. \n- M\u00e1s adelante estudiaremos la regularizaci\u00f3n, permitiendo el uso d emodelos m\u00e1s complejos (por ejemplo con $n_h = 50$) cuidando de no cometer un sobreajuste muy alto. ","5e840479":"**Anotaci\u00f3n**: como se esperaba, el conjunto de datos no es linealmente separable. Por esto, la regresi\u00f3n log\u00edstica no logra un buen desempe\u00f1o. Puede ser que la red neuronal logre un mejor desempe\u00f1o. \n\nVamos a ver! ","ff60bd3e":"**Salida esperada**: \n\n<table style=\"width:20%\">\n  <tr>\n    <td>**Precisi\u00f3n**<\/td>\n    <td> 47% <\/td> \n  <\/tr>\n  \n<\/table>\n","3c30cdfd":"## 5) Desempe\u00f1o sobre otros conjuntos de datos ","196f3ddb":"### 4.3 - El bucle ####\n\n**Pregunta**: Implemente `forward_propagation()`.\n\n**Instrucciones**:\n- Revise arriba la representaci\u00f3n matem\u00e1tica de su clasificador.\n- Puede usar la funci\u00f3n `sigmoid()` (se ha importado junto con el cuaderno).\n- Puede usar la funci\u00f3n `np.tanh()` (es parte de la biblioteca numpy).\n- Los pasos que debe implementar son:\n    1. Recupere cada par\u00e1metro del diccionario \"parameters\" (que es la salida de `initialize_parameters()`), utilizando `parameters[\"..\"]`.\n    2. Implemente la propagaci\u00f3n hacia delante. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ y $A^{[2]}$ (el vector de todas sus predicciones sobre todos los ejemplos del conjunto de entrenamiento).\n- Los valores necesarios para calcular la retro-propagaci\u00f3n son guardados en \"`cache`\". La `cache` ser\u00e1 proporcionada como entrada a la funci\u00f3n de retro-propagaci\u00f3n.","98b35d17":"Ahora es momento de ejecutar el modelo y ver su desempe\u00f1o sobre el conjunto de datos. Ejecute el siguiente c\u00f3digo para probar el modelo con una sola capa escondida de $n_h$ neuronas\/unidades escondidas.","2f88cc3d":"**Salida esperada**: \n<table style=\"width:50%\">\n  <tr>\n    <td> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719 <\/td> \n  <\/tr>\n<\/table>","daf0a912":"**Salida esperada**:  por favor tenga en cuenta que esta comprobaci\u00f3n es s\u00f3lo para verificar que la funci\u00f3n que ha programado es correcta. **En la implemenatci\u00f3n de la red utilizaremos otras dimensiones** \n\n\n<table style=\"width:20%\">\n  <tr>\n    <td>**n_x**<\/td>\n    <td> 5 <\/td> \n  <\/tr>\n  \n    <tr>\n    <td>**n_h**<\/td>\n    <td> 4 <\/td> \n  <\/tr>\n  \n    <tr>\n    <td>**n_y**<\/td>\n    <td> 2 <\/td> \n  <\/tr>\n  \n<\/table>","a82e9715":"Visualice los datos mediante matplotlib. Los datos tienen puntos rojos (y=0) y azules (y=1). El objetivo es el de constuir un modelo que se ajuste a estos datos. ","0e713f1b":"**Salida esperada**: \n\n<table style=\"width:90%\">\n\n<tr> \n    <td> \n        **coste tras la iteracion 1000**\n    <\/td>\n    <td> \n        0.000218\n    <\/td>\n<\/tr>\n\n<tr> \n    <td> \n        <center> $\\vdots$ <\/center>\n    <\/td>\n    <td> \n        <center> $\\vdots$ <\/center>\n    <\/td>\n<\/tr>\n\n  <tr>\n    <td>**W1**<\/td>\n    <td> [[-0.65848169  1.21866811]\n [-0.76204273  1.39377573]\n [ 0.5792005  -1.10397703]\n [ 0.76773391 -1.41477129]]<\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**b1**<\/td>\n    <td> [[ 0.287592  ]\n [ 0.3511264 ]\n [-0.2431246 ]\n [-0.35772805]] <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**W2**<\/td>\n    <td> [[-2.45566237 -3.27042274  2.00784958  3.36773273]] <\/td> \n  <\/tr>\n  \n\n  <tr>\n    <td>**b2**<\/td>\n    <td> [[ 0.20459656]] <\/td> \n  <\/tr>\n  \n<\/table>  ","e5fdae3d":"Utilizando la cache computada en la propagacion hacia delante, puede implementar propagaci\u00f3n hacia atr\u00e1s.\n\n**Ejercicio**: Implemente la funci\u00f3n `backward_propagation()`.\n\n**Instrucciones**:\nLa retro-propagaci\u00f3n suele ser la parte m\u00e1s dif\u00edcil (y m\u00e1s matem\u00e1tica) del deep learning. Recordemos lo que es la retro-propagaci\u00f3n. Se deben utilizar las seis ecuaciones siguientes, desde que se est\u00e1 implementando la versi\u00f3n vectorizada.  \n\n<!--\ncomentar en bloque\n!-->\n$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n\n$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n\n- N\u00f3tese que $*$ denota multiplicaci\u00f3n por cada elemento.\n- La notaci\u00f3n que se utiliza es com\u00fan en c\u00f3digo de deep learning:\n    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n    \n\n\n- Ayuda:\n    - Para computar dZ1 se debe calcular primero $g^{[1]'}(Z^{[1]})$. Como $g^{[1]}(.)$ es la funci\u00f3n de activaci\u00f3n tanh, si $a = g^{[1]}(z)$ entonces $g^{[1]'}(z) = 1-a^2$. Entonces puede calcular \n    $g^{[1]'}(Z^{[1]})$ utilizando `(1 - np.power(A1, 2))`.\n    - Tenga en cuenta las dimensiones correctas para la implementaci\u00f3n del paso hacia atr\u00e1s en el c\u00e1lculo de $db$. Por ejemplo, para $db1$, si quiere sumar las columnas (filas), $axis=1$ ($axis=0$).\n    - Tambi\u00e9n tenga en cuenta que las dimensiones se pueden manetener fijas mediante el argumento $keepdims=True$.","533d3d49":"# Clasificaci\u00f3n con una capa escondida\n\nEn este taller vamos a implementar una red neuronal con una sola capa escondida. Podremos ver la diferencia de este modelo con la regresi\u00f3n log\u00edstica que implementamos en el taller anterior.\n\n**Tras este taller usted va a ser capaz de:**\n- Implementar una red neuronal de una capa escondida para un problema de clasificaci\u00f3n binario\n- Usar unidades\/neuronas con una funci\u00f3n de activaci\u00f3n no-lineal, como por ejemplo la tanh \n- Computar la funci\u00f3n de p\u00e9rdida de entrop\u00eda cruzada \n- Implementar la propagaci\u00f3n hacia delante y hacia atr\u00e1s\n","33499d05":"**Preguntas sugeridas**:\n\nPuede explorar qu\u00e9 ocurre si.. \n- ..cambia la funci\u00f3n de activaci\u00f3n de tanh a una activaci\u00f3n sigmoide o ReLU?\n- ..cambia la tasa de aprendizaje. \n\nSe consiguen mejores resultados? \n\n\n- Y qu\u00e9 pasa si cambiamos el conjunto de datos? (Ver el siguiente punto)\n\nPuede elegir un conjunto de datos diferente, y probar qu\u00e9 ocurre al cambiar la funci\u00f3n de activaci\u00f3n o la tasa de aprendizaje. Visualize los resultados del mejor modelo.","151c3305":"## 3 - Regresi\u00f3n Log\u00edstica\n\nAntes de contruir una red neuronal, primero estudiemos c\u00f3mo la regresi\u00f3n log\u00edstica se comporta con este problema. Se pueden utilizar las funciones de sklearn para hacerlo. Ejecute el siguiente c\u00f3digo para entrenar un clasificador de regresi\u00f3n log\u00edstica sobre el conjunto de datos.","04ebf1fa":"### 4.5 Predicciones\n\n**Ejercicio**: Construya su modelo mediante la funcion predict().\nUse propagacion hacia delante para predecir los resultados.\n\n**Recuerde**: predicciones = $y_{prediccion} = \\mathbb 1 \\text{{activacion > 0.5}} = \\begin{cases}\n      1 & \\text{if}\\ activacion > 0.5 \\\\\n      0 & \\text{$e.c.c.$}\n    \\end{cases}$  \n    \nComo un ejemplo, si quiere fijar las entradas de una matriz de X a 0 y 1 basados en un $umbral$, se puede hacer de la siguiente manera: $$X_{nuevo} = (X > umbral)$$","b0661855":"## 1 - Paquetes ##\n\nPrimero, importamos los paquetes que vamos a necesitar a lo largo de este taller. \n- [numpy](www.numpy.org) paquete b\u00e1sico para ciencias computacionales con Python.\n- [sklearn](http:\/\/scikit-learn.org\/stable\/) herramientas eficientes para el an\u00e1lisis y la miner\u00eda de datos. \n- [matplotlib](http:\/\/matplotlib.org) librer\u00eda para graficar en Python.\n- testCases tiene los ejemplos de prueba para evaluar la implementacion de las funciones\n- planar_utils provee distintas funciones que se van a usar durante el taller","89da6f4f":"**Salida esperada**: \n\n\n<table style=\"width:40%\">\n  <tr>\n    <td>**Predicci\u00f3n media**<\/td>\n    <td> 0.666666666667 <\/td> \n  <\/tr>\n  \n<\/table>","a4c261b4":"<font color='blue'>\n**En este taller debe haber aprendido a:**\n- Construir una red neuronal completa con una capa escondida\n- Hacer buen uso de una unidad no-lineal \n- Implementar propagaci\u00f3n hacia delante y hacia atr\u00e1s, y entrenar una red neuronal\n- Ver el impacto de cambiar el tama\u00f1o de la capa escondida, junto con la comprensi\u00f3n del sobre-ajuste.\n\nReferencias:\n- http:\/\/scs.ryerson.ca\/~aharley\/neural-networks\/\n- http:\/\/cs231n.github.io\/neural-networks-case-study\/\n\nEspero te haya gustado este Notebook. Por favor compartelo para que entre todos aprendamos juntos.\n\nPuedes Seguirme a mi cuenta en Twitter **[@andres_jejen](https:\/\/twitter.com\/andres_jejen)** Constantemente comparto noticias y contenido educativo sibre Machine Learning, Big Data y Data Science.","10e90e6f":"### 4.4 - Integre las partes 4.1, 4.2 y 4.3 en nn_model() ####\n\n**Ejercicio**: Construya su modelo de red neuronal en `nn_model()`.\n\n**Instrucciones**: El modelo de la red debe usar las funciones previamente construidas en el orden correcto.","9cc4497a":"Entonces se tiene una matriz X con los patrones (x1, x2), y un vector Y con las etiquetas (rojo:0, azul:1).\n\nExaminemos a continuaci\u00f3n los datos.\n\n**Ejercicio**: Cu\u00e1ntos ejemplos de entrenamiento tenemos? Adicionalmente, cu\u00e1les son las dimensiones `shape` de `X` e `Y`? \n\n**Ayuda**: Recuerde c\u00f3mo obtener la forma de un arreglo numpy [(ayuda)](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.ndarray.shape.html)","4ff55426":"La precisi\u00f3n ($accuracy$) es alta comparada con los resultados de la regresi\u00f3n log\u00edstica. El modelo ha aprendido los patrones de los datos (su forma en flor). Las redes neuronales pueden aprender fronteras de decisi\u00f3n en alto grado no-lineales, a diferencia de la regresi\u00f3n log\u00edstica. \n\nAhora, encuentre el mejor modelo al intentar distintos tama\u00f1os de la capa escondida.","8dbc021f":"## 4 - Modelo de red neuronal\n\nLa regresi\u00f3n log\u00edstica no logr\u00f3 buenos resultados sobre el conjunto de datos. Ahora debe entrenar una red neuronal con una sola capa escondida.\n\n**Formulaci\u00f3n matem\u00e1tica**:\n\nPara un ejemplo $x^{(i)}$:\n$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n$$y^{(i)}_{prediccion} = \\begin{cases} 1 & \\mbox{si } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{en caso contrario (e.c.c.) } \\end{cases}\\tag{5}$$\n\nDadas las predicciones sobre todos los ejemplos, tambien puede computar el coste $J$ como sigue: \n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n\n**Recuerde**: La metodolog\u00eda general para construir una red neuronal:\n    1. Definir una estructura de red neuronal ( # de unidades de input,  # de unidades escondidas, etc). \n    2. Inicialice los par\u00e1metros del modelo\n    3. Bucle:\n        - Implemente propagaci\u00f3n hacia delante\n        - Compute la p\u00e9rdida\n        - Implemente la propagaci\u00f3n hacia atr\u00e1s y obtenga los gradientes\n        - Actualice los par\u00e1metros (Descenso en la direcci\u00f3n del gradiente: GD)\n\nSe pueden construir funciones auxiliares para computar los pasos 1-3 y luego fusionarlas en una funci\u00f3n (madre) llamada `nn_model()`. Una vez construida `nn_model()` y habiendo aprendido los par\u00e1metros adecuados, se pueden hacer predicciones sobre nuevos datos.","246c6cce":"**Salida esperada**: \n\n<table style=\"width:15%\">\n  <tr>\n    <td>**Accuracy**<\/td>\n    <td> 90% <\/td> \n  <\/tr>\n<\/table>","80e8ad6a":"**Salida esperada**: \n\n\n\n<table style=\"width:80%\">\n  <tr>\n    <td>**dW1**<\/td>\n    <td> [[ 0.00301023 -0.00747267]\n [ 0.00257968 -0.00641288]\n [-0.00156892  0.003893  ]\n [-0.00652037  0.01618243]] <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**db1**<\/td>\n    <td>  [[ 0.00176201]\n [ 0.00150995]\n [-0.00091736]\n [-0.00381422]] <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**dW2**<\/td>\n    <td> [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]] <\/td> \n  <\/tr>\n  \n\n  <tr>\n    <td>**db2**<\/td>\n    <td> [[-0.16655712]] <\/td> \n  <\/tr>\n  \n<\/table>  ","cdd1ae85":"## 2 - Conjunto de datos ##\n\nPrimero, carguemos el conjunto de datos sobre el que se va a trabajar. El siguiente c\u00f3digo va a cargar un conjunto de datos en forma de flor, con dos clases en las variables `X` e `Y`."}}