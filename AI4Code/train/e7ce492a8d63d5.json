{"cell_type":{"0a98a9aa":"code","15e4e2c3":"code","596f9a9b":"code","fb498c7d":"code","87f33da7":"code","1c4bf167":"code","0a886b41":"code","94bc4ed2":"code","5cb16312":"code","495540ba":"code","6a03fa2b":"code","57d6f357":"code","82ebbaa2":"code","94070a9e":"code","b96fec24":"code","3df2fc1a":"code","d9192197":"code","562ad47d":"code","e42ef8f1":"code","76e97ca2":"code","a8f227bc":"markdown","ae9c2feb":"markdown","4dbe9249":"markdown","3538db3d":"markdown","a2cb3149":"markdown","b09ea359":"markdown","a73d06ce":"markdown","4f563c1a":"markdown","d5908cf1":"markdown","589b94d2":"markdown"},"source":{"0a98a9aa":"#import package\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport re\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer","15e4e2c3":"# Reading from file \nfake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","596f9a9b":"print(true.shape)\nprint(true.info())\ntrue.head()","fb498c7d":"print(fake.shape)\nprint(fake.info())\nfake.head()","87f33da7":"fake['Label'] = 1\ntrue['Label'] = 0","1c4bf167":"data = pd.concat([true,fake],axis=0,ignore_index=True)\nprint(data.shape)\ndata.head()","0a886b41":"data.describe()","94bc4ed2":"data['text']=data['title']+data['text']\ndata=data.drop(['title'], axis=1)","5cb16312":"sns.countplot(data.Label)","495540ba":"data.isnull().sum()","6a03fa2b":"data.subject.value_counts()","57d6f357":"def clean_text(text):\n    \n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('Reuters','',text)\n    return text\n\ndata['text'] = data['text'].apply(lambda x:clean_text(x))\n","82ebbaa2":"stop = stopwords.words('english')\ndata['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n","94070a9e":"def lemmatize_words(text):\n    wnl = nltk.stem.WordNetLemmatizer()\n    lem = ' '.join([wnl.lemmatize(word) for word in text.split()])    \n    return lem\n\ndata['text'] = data['text'].apply(lemmatize_words)","b96fec24":"y = data['Label']\nX_train, X_test, y_train, y_test = train_test_split(data['text'], y,test_size=0.33,random_state=53)","3df2fc1a":"count_vectorizer = CountVectorizer(stop_words='english')\ncount_train = count_vectorizer.fit_transform(X_train.values)\ncount_test = count_vectorizer.transform(X_test.values)\nprint(count_train.shape)","d9192197":"# Model 1 - default parameter \nfrom sklearn.metrics import classification_report\n\nnb_classifier1 = MultinomialNB()\nnb_classifier1.fit(count_train, y_train)\n\npred1 = nb_classifier1.predict(count_test)\n\nprint(classification_report(y_test, pred1, target_names = ['Fake','True']))","562ad47d":"#model 2\nnb_classifier2 = MultinomialNB(alpha = 1000)\nnb_classifier2.fit(count_train, y_train)\n\npred2 = nb_classifier2.predict(count_test)\n\nprint(classification_report(y_test, pred2, target_names = ['Fake','True']))","e42ef8f1":"# 1\nfrom sklearn.svm import SVC\n\nsvc_model1 = SVC(C=1, kernel='linear', gamma= 1)\nsvc_model1.fit(count_train, y_train)\n\nprediction1 = svc_model1.predict(count_test)\n\nprint(classification_report(y_test, prediction1, target_names = ['Fake','True']))","76e97ca2":"# 2\nsvc_model2 = SVC(C= 100, kernel='linear', gamma= 1)\nsvc_model2.fit(count_train, y_train)\n\nprediction2 = svc_model2.predict(count_test)\n\nprint(classification_report(y_test, prediction2, target_names = ['Fake','True']))","a8f227bc":"# **<span style=\"color:#6daa9f;\">MODEL <\/span>**\n\nUsing 2 different model with different parameter for parameter investigation values of alpha and c\n\n**Naive Bayes**","ae9c2feb":"## Fake and Real News Dataset\n\n\n# **<span style=\"color:#6daa9f;\">IMPORT LIBRARY & PACKAGES <\/span>**\n","4dbe9249":"# **<span style=\"color:#6daa9f;\">DATA CLEANING <\/span>**\n\n**Lowercase words, remove the word 'Reuters', remove square brackets, links, words containing numbers and punctuations**\n\n* Cleaning our text data is important so that the model wont be fed noises that would not help with the prediction. \n* The word reuters was removed as it always appear in the real news article therefore I removed it as it is an obvious indicator to the model ","3538db3d":"**Remove stop words**","a2cb3149":"**Support Vector Machine (SVM)**","b09ea359":"### Hi there!\ud83d\ude04 I am new to data science and this is my try on the Fake and Real News dataset. Feel free to comment if you have any questions, insights or advice on this or any data science related :) Upvote if you find my work useful for you! Thank you!","a73d06ce":"**Using Bag of words model for data transformation**\n\nSince we are dealing with text data, we cannot fed it directly to our model. Therefore, I am using bag of words model to extract features from our text data and convert it into numerical feature vectors that can be fed directly to the algorithm","4f563c1a":"**Lemmatize words**\n\nWords were lemmatized so that only root words are retain in the data and fed into the model ","d5908cf1":"# **<span style=\"color:#6daa9f;\">EXPLORATORY DATA ANALYSIS <\/span>**\n","589b94d2":"**Split data into train and test set**"}}