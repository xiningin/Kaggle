{"cell_type":{"43aa2f9b":"code","b6e50ec0":"code","bea90d53":"code","d59da860":"code","b0e1efde":"code","e1c8f88e":"code","c170e097":"code","2dcc4f40":"markdown","9bc046bc":"markdown","49faeebe":"markdown","ec6d15ac":"markdown","f79444b4":"markdown","3b648845":"markdown"},"source":{"43aa2f9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6e50ec0":"import tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nIMG_SIZE = (224, 224)\n\ntrain_dir = \"\/kaggle\/input\/yoga-pose-image-classification-dataset\/dataset\/\"\n\nimage_generator = ImageDataGenerator(rescale=1\/255., validation_split=0.2) \n\ntrain_dataset = image_generator.flow_from_directory(batch_size=32,\n                                                 directory=train_dir,\n                                                 shuffle=True,\n                                                 target_size=(224, 224), \n                                                 subset=\"training\",\n                                                 class_mode='categorical')\n\ntest_dataset = image_generator.flow_from_directory(batch_size=32,\n                                                 directory=train_dir,\n                                                 shuffle=True,\n                                                 target_size=(224, 224), \n                                                 subset=\"validation\",\n                                                 class_mode='categorical')","bea90d53":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\n\nefficientnet_url = \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/feature-vector\/1\"","d59da860":"IMAGE_SHAPE = (224, 224)\n\ndef create_model(model_url, num_classes=107):\n  \"\"\"Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.\n  \n  Args:\n    model_url (str): A TensorFlow Hub feature extraction URL.\n    num_classes (int): Number of output neurons in output layer,\n      should be equal to number of target classes, default 10.\n\n  Returns:\n    An uncompiled Keras Sequential model with model_url as feature\n    extractor layer and Dense output layer with num_classes outputs.\n  \"\"\"\n  # Download the pretrained model and save it as a Keras layer\n  feature_extractor_layer = hub.KerasLayer(model_url,\n                                           trainable=False, # freeze the underlying patterns\n                                           name='feature_extraction_layer',\n                                           input_shape=IMAGE_SHAPE+(3,)) # define the input image shape\n  \n  # Create our own model\n  model = tf.keras.Sequential([\n    feature_extractor_layer, # use the feature extraction layer as the base\n    layers.Dense(num_classes, activation='softmax', name='output_layer') # create our own output layer      \n  ])\n\n  return model","b0e1efde":"model = create_model(efficientnet_url, num_classes=107)\n\nmodel.compile(loss='categorical_crossentropy',\n                     optimizer=tf.keras.optimizers.Adam(),\n                     metrics=['accuracy'])","e1c8f88e":"eff_history = model.fit(train_dataset,\n                                  epochs=5,\n                                  steps_per_epoch=len(train_dataset),\n                                  validation_data=test_dataset,\n                                  validation_steps=int(0.25 * len(test_dataset)))","c170e097":"evaluation = model.evaluate(test_dataset)","2dcc4f40":"# **We are acheiving 58% Accuracy**","9bc046bc":"# Get the Eff B0 Feature Vector URL","49faeebe":"# Create the Model","ec6d15ac":"# Lets split the dataset into Train and Val sets","f79444b4":"# Evaluate the model on entire test set. Because we used only 25% data while training","3b648845":"# Fit the model"}}