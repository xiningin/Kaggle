{"cell_type":{"4a6fe9a0":"code","3d40ae17":"code","e59f77cf":"code","98e99fba":"code","3a597357":"code","aa6eaa47":"code","3255cc0e":"code","87faec25":"code","3c3f4ce1":"code","0c956b0a":"code","95d403db":"code","f2a9f5df":"code","940de8ab":"code","e6793877":"code","ed03d912":"code","7c08ff94":"code","79e21fd4":"code","b611bc59":"code","cde5631d":"code","dfaea12f":"code","401f189a":"code","75724d4f":"code","9a6d4c5d":"code","cb70d8cb":"code","5692beba":"code","53134317":"code","d31532c6":"code","6a1942f5":"code","efaf05e1":"code","fae9640b":"code","36d69e0f":"code","06385ca7":"code","36f8f4d8":"code","f02c64d1":"code","0003d649":"code","5777cd56":"markdown","f3797062":"markdown","bb4cc2fe":"markdown","8d7f12a8":"markdown","cf555d61":"markdown","eeef914f":"markdown","9516bc59":"markdown","a4e9da02":"markdown","5ace29a0":"markdown","1bbf503d":"markdown","eac02adb":"markdown","9b9fe564":"markdown","2fbe8532":"markdown","261601f8":"markdown","37c8c7fa":"markdown","f98a9632":"markdown","baed4497":"markdown","8900a797":"markdown","3ee464bc":"markdown"},"source":{"4a6fe9a0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\n# from sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.cluster import KMeans\n# import pickle\n# from sklearn.externals import joblib\n\n# import pandas_profiling as pp\n\nwarnings.filterwarnings('ignore')\ngc.enable()\n%matplotlib inline","3d40ae17":"#CHECKING ALL AVAILABLE FILES\npath='\/kaggle\/input\/tabular-playground-series-sep-2021\/'\ndata_files=list(os.listdir(path))\ndf_files=pd.DataFrame(data_files,columns=['file_name'])\ndf_files['size_in_mb']=df_files.file_name.apply(lambda x: round(os.path.getsize(path+x)\/(1024*1024),2))\ndf_files['type']=df_files.file_name.apply(lambda x:'file' if os.path.isfile(path+x) else 'directory')\ndf_files['file_count']=df_files[['file_name','type']].apply(lambda x: 0 if x['type']=='file' else len(os.listdir(path+x['file_name'])),axis=1)\n\nprint('Following files are available under path:',path)\ndisplay(df_files)","e59f77cf":"#ALL CUSTOM FUNCTIONS\n\n#FUNCTION FOR PROVIDING FEATURE SUMMARY\ndef feature_summary(df_fa):\n    print('DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    col_list=['null','unique_count','data_type','max\/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])\/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(round(df_fa[col].max(),2))+'\/'+str(round(df_fa[col].min(),2))\n            df.at[col,'mean']=round(df_fa[col].mean(),4)\n            df.at[col,'median']=round(df_fa[col].median(),4)\n            df.at[col,'mode']=round(df_fa[col].mode()[0],4)\n            df.at[col,'std']=round(df_fa[col].std(),4)\n            df.at[col,'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(df_fa[col].max())+'\/'+str(df_fa[col].min())\n        df.at[col,'sample_values']=list(df_fa[col].unique())\n    display(df_fa.head())      \n    return(df.fillna('-'))","98e99fba":"#PREDICTION FUNCTIONS\n\ndef claim_predictor(X,y,test,model,model_name):  \n\n    df_preds=pd.DataFrame()\n    df_preds_x=pd.DataFrame()\n    k=1\n    splits=5\n    avg_score=0\n\n    #CREATING STRATIFIED FOLDS\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n    print('\\nStarting KFold iterations...')\n    for train_index,test_index in skf.split(X,y):\n        df_X=X[train_index,:]\n        df_y=y[train_index]\n        val_X=X[test_index,:]\n        val_y=y[test_index]\n       \n\n    #FITTING MODEL\n        model.fit(df_X,df_y)\n\n    #PREDICTING ON VALIDATION DATA\n        col_name='cat_predsx_'+str(k)\n        preds_x=pd.Series(model.predict_proba(val_X)[:,1])\n        df_preds_x[col_name]=pd.Series(model.predict_proba(X)[:,1])\n\n    #CALCULATING ACCURACY\n        acc=roc_auc_score(val_y,preds_x)\n        print('Iteration:',k,'  roc_auc_score:',acc)\n        if k==1:\n            score=acc\n            best_model=model\n            preds=pd.Series(model.predict_proba(test)[:,1])\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=pd.Series(model.predict_proba(X)[:,1])\n        else:\n            preds1=pd.Series(model.predict_proba(test)[:,1])\n            preds=preds+preds1\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=pd.Series(model.predict_proba(X)[:,1])\n            if score<acc:\n                score=acc\n                best_model=model\n        avg_score=avg_score+acc        \n        k=k+1\n    print('\\n Best score:',score,' Avg Score:',avg_score\/splits)\n    #TAKING AVERAGE OF PREDICTIONS\n    preds=preds\/splits\n    \n    print('Saving train predictions per iteration...')\n    df_preds.to_csv(model_name+'.csv',index=False)\n    x_preds=df_preds.mean(axis=1)\n    return preds,best_model,x_preds ","3a597357":"%%time\n#READING DATASET\n\ndf_train=pd.read_csv(path+'train.csv')\ndf_test=pd.read_csv(path+'test.csv')\ndf_submission=pd.read_csv(path+'sample_solution.csv')","aa6eaa47":"#Understanding Target (claim) feature distribution\npie_labels=['Claim-'+str(df_train['claim'][df_train.claim==1].count()),'No Claim-'+\n            str(df_train['claim'][df_train.claim==0].count())]\npie_share=[df_train['claim'][df_train.claim==1].count()\/df_train['claim'].count(),\n           df_train['claim'][df_train.claim==0].count()\/df_train['claim'].count()]\nfigureObject, axesObject = plt.subplots(figsize=(6,6))\npie_colors=('orange','grey')\npie_explode=(.01,.01)\naxesObject.pie(pie_share,labels=pie_labels,explode=pie_explode,autopct='%.2f%%',colors=pie_colors,startangle=30,shadow=True)\naxesObject.axis('equal')\nplt.title('Percentage of Claim - No Claim Observations',color='blue',fontsize=12)\nplt.show()","3255cc0e":"#Correlation check\ncorr = df_train.iloc[:,1:].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Plotting correlation heatmap\nplt.subplots(figsize=(22,20))\nsns.heatmap(corr,mask=mask,xticklabels=corr.columns,yticklabels=corr.columns)\nplt.show()","87faec25":"%%time\nX=df_train.drop(['id','claim'],axis=1)\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nX_= imputer.fit_transform(X)\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(X_)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2','principal_component_3'])\nprincipalDf['claim']=df_train['claim']\n\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection = '3d')\n\nax.set_xlabel(\"principal_component_1\")\nax.set_ylabel(\"principal_component_2\")\nax.set_zlabel(\"principal_component_3\")\n\nsc=ax.scatter(xs=principalDf['principal_component_1'], ys=principalDf['principal_component_2'],\n              zs=principalDf['principal_component_3'],c=principalDf['claim'],cmap='OrRd')\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\nplt.show()","3c3f4ce1":"gc.collect()","0c956b0a":"del X,X_\ngc.collect()","95d403db":"pd.set_option('display.max_rows', len(df_train.columns))\nfeature_summary(df_train)","f2a9f5df":"feature_summary(df_submission)","940de8ab":"gc.collect()","e6793877":"%%time\nfeatures=list(df_test.columns[1:])\n\ndf_train['n_missing'] = df_train[features].isna().sum(axis=1)\ndf_test['n_missing'] = df_test[features].isna().sum(axis=1)\n\ndf_train['std'] = df_train[features].std(axis=1)\ndf_test['std'] = df_test[features].std(axis=1)\n\ndf_train['mean'] = df_train[features].mean(axis=1)\ndf_test['mean'] = df_test[features].mean(axis=1)\n\ndf_train['kurt'] = df_train[features].kurtosis(axis=1)\ndf_test['kurt'] = df_test[features].kurtosis(axis=1)\n\nfeatures += ['n_missing', 'std','mean','kurt']","ed03d912":"%%time\ndf_train[features] = df_train[features].fillna(df_train[features].mean())\ndf_test[features] = df_test[features].fillna(df_test[features].mean())","7c08ff94":"%%time\nscaler = StandardScaler()\ndf_train[features] = scaler.fit_transform(df_train[features])\ndf_test[features] = scaler.transform(df_test[features])","79e21fd4":"X=df_train.drop(['id','claim'],axis=1).to_numpy()\ny=df_train['claim'].values\ntest=df_test.drop(['id'],axis=1).to_numpy()","b611bc59":"del df_train,df_test,scaler\ngc.collect()","cde5631d":"X.shape,y.shape,test.shape","dfaea12f":"%%time\nmodel=LogisticRegression()\nprint('Logistic Regression parameters:\\n',model.get_params())\n\nlogistic_predictions,best_logistic_model,LRpreds=claim_predictor(X,y,test,model,'LR')","401f189a":"logistic_predictions","75724d4f":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_logistic_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (20,25))\nsns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt);","9a6d4c5d":"%%time\ncatb_params = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585, \n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\nmodel=CatBoostClassifier(**catb_params)\nprint('CatBoost paramters:\\n',model.get_params())\n\ncatb_predictions,best_catb_model,CBpreds=claim_predictor(X,y,test,model,'CB')","cb70d8cb":"catb_predictions","5692beba":"gc.collect()","53134317":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_catb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (10,25))\nsns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt);","d31532c6":"%%time\nlgbm_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n    'device_type': 'gpu', \n    'n_estimators': 10000, \n    'learning_rate': 0.12230165751633416, \n    'num_leaves': 1400, \n    'max_depth': 8, \n    'min_child_samples': 3100, \n    'reg_alpha': 10, \n    'reg_lambda': 65, \n    'min_split_gain': 5.157818977461183, \n    'subsample': 0.5, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.2\n}\n\nmodel=lgb.LGBMClassifier(**lgbm_params)\nprint('LGBM parameters:\\n',model.get_params())\n\nlgb_predictions,best_lgb_model,LGBpreds=claim_predictor(X,y,test,model,'LGB')","6a1942f5":"lgb_predictions","efaf05e1":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_lgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (10,25))\nsns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt);","fae9640b":"%%time\nxgb_params = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n    'tree_method': 'gpu_hist', \n    'gpu_id': 0, \n    'predictor': 'gpu_predictor', \n    'n_estimators': 10000, \n    'learning_rate': 0.01063045229441343, \n    'gamma': 0.24652519525750877, \n    'max_depth': 4, \n    'min_child_weight': 366, \n    'subsample': 0.6423040816299684, \n    'colsample_bytree': 0.7751264493218339, \n    'colsample_bylevel': 0.8675692743597421, \n    'lambda': 0, \n    'alpha': 10\n}\nmodel=xgb.XGBClassifier(**xgb_params)\nprint('XGB parameters:\\n',model.get_params())\n\nxgb_predictions,best_xgb_model,XGBpreds=claim_predictor(X,y,test,model,'XGB')","36d69e0f":"xgb_predictions","06385ca7":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_xgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (10,25))\nsns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt);","36f8f4d8":"%%time\nblending_ratios=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nroc_final=0\nLR_ratio=0\nLGB_ratio=0\nCB_ratio=0\nXGG_ratio=0\nfor i in blending_ratios:\n    for j in blending_ratios:\n        for k in blending_ratios:\n            for l in blending_ratios:\n                if((i+j+k+l==1) and (i>0 and j>0 and k>0 and l>0)):\n                    roc_new=roc_auc_score(y,(LRpreds*i+LGBpreds*j+CBpreds*k+XGBpreds*l))\n                    print(\"LR ratio: \",i,\" LGB ratio: \",j,\" CB ratio:\",k,\" XGB ratio: \",l,\" ROC score: \",roc_new)\n                    if roc_new>roc_final:\n                        roc_final=roc_new\n                        LR_ratio=i\n                        LGB_ratio=j\n                        CB_ratio=k\n                        XGB_ratio=l\nprint(\"Final Ratios, LR ratio: \",LR_ratio,\" LGB ratio: \",LGB_ratio,\" CB ratio:\",CB_ratio,\" XGB ratio: \",XGB_ratio,\" ROC score: \",roc_final)","f02c64d1":"df_submission['claim']=lgb_predictions*LGB_ratio+catb_predictions*CB_ratio+logistic_predictions*LR_ratio+xgb_predictions*XGB_ratio\n\ndf_submission.to_csv('submission.csv',index=False)","0003d649":"df_submission","5777cd56":"## Understanding Target (claim) feature distribution","f3797062":"# Models","bb4cc2fe":"## READING DATASET","8d7f12a8":"## Calculating best blending Ratios (using training preditions to calculate blending ratios)\nWe are trying to calculate best blending ratio on trained dataset and then applying the same to predicted test values\n\n## Observation\n\n*     This approach lead to selecting ratio 1.0 for best model and 0.0 for others.\n*     If we observe results by above three models, we can conclude different models are working better than other on different part of dataset.Now how can we bring this to our blending strategy?\n*     As changed blending strategy, we will try to find best blending ratio with each ratio greater than zero\n","cf555d61":"## Understanding Training dataset features\nUserstanding Training dataset features using basic statistical measures","eeef914f":"## XGBClassifier","9516bc59":"## CUSTOM FUNCTIONS","a4e9da02":"# Import Libraries","5ace29a0":"# SUBMISSION","1bbf503d":"## PREDICTION FUNCTIONS","eac02adb":"## LogisticRegression","9b9fe564":"# CREATING SUMBISSION FILE","2fbe8532":"## Correlation Check\nLets check if there are any correlated features. If two features are highly correlated we can remove one of the feature. This will help in dimentionality reduction.\n\n## Observation\nNo correlation is observed among Training dataset features.","261601f8":"# Blending","37c8c7fa":"## CatBoostClassifier\n","f98a9632":"## Visualizating Training dataset\nWe are making use of PCA, dimentionality reduction technique to Visualize Training dataset.\nVisualization is also helpful in understanding any grouping or patterns within dataset.\n\n#### Observation\nNo pattern or grouping observed in training dataset","baed4497":"# Exploratory Data Analysis EDA","8900a797":"## LGBMClassifier","3ee464bc":"## CHECKING ALL AVAILABLE FILES"}}