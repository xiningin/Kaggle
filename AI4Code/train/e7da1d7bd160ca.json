{"cell_type":{"e9fb619d":"code","8711c6f4":"code","1e92429a":"code","01afa970":"code","b95e4d38":"code","eda194a5":"code","5b8f73a0":"code","ba389154":"code","73a89ad6":"code","a7bfd43c":"code","709c6bc3":"code","a0371860":"code","eb079e8d":"code","072d7a5a":"code","2db2d548":"code","d32a1e71":"code","937dc54d":"code","87a6e8e3":"code","d87ab7b7":"code","abe33749":"code","b217c014":"code","e2060117":"code","7c490606":"code","b199c8d8":"code","5f827aca":"code","4e60b785":"code","355615ae":"code","4cb4552d":"code","f21cda94":"code","a48f01d0":"code","8430d028":"code","fe7a3646":"code","5b2917c9":"code","0987ddd0":"code","e9390719":"code","98083c79":"code","9b612ebc":"code","53035aa2":"code","a06229a9":"code","aa349b2b":"markdown","7ba2d0fc":"markdown","c589449f":"markdown","a2cb0090":"markdown","b7f92718":"markdown","d7c767a7":"markdown","c2fc00ea":"markdown","8cc8c22c":"markdown","7651bce4":"markdown","1d7b8670":"markdown","06fff9eb":"markdown","df822dc5":"markdown","c66ff990":"markdown","0b71fde6":"markdown","7122c915":"markdown","1d168057":"markdown","fd149533":"markdown","e340b08f":"markdown","273fe76c":"markdown","7c97b53f":"markdown","abf5e419":"markdown","be9780c9":"markdown","6b47d25d":"markdown","aeb5e173":"markdown","2eac9108":"markdown","bbbd5e62":"markdown"},"source":{"e9fb619d":"import pandas as pd\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')","8711c6f4":"df = pd.read_csv('..\/input\/adult-pmr3508\/train_data.csv')","1e92429a":"df.info()","01afa970":"df[100:115]","b95e4d38":"df = pd.read_csv('..\/input\/adult-pmr3508\/train_data.csv', na_values='?')","eda194a5":"df.info()","5b8f73a0":"df.isnull().sum()","ba389154":"df = df.fillna(df.mode().iloc[0])","73a89ad6":"df.isnull().sum()","a7bfd43c":"plt.figure(figsize = (5, 5))\ng = sns.countplot(x = df['sex'], hue=df['income'])\ng.set_xticklabels(['Feminino', 'Masculino'])\nplt.title('G\u00eanero', fontsize = 20)\nplt.ylabel('Contagem')\nplt.xlabel('')\nplt.show()    ","709c6bc3":"plt.figure(figsize = (7, 5))\ng = sns.countplot(x = df['income'] , hue = df['workclass'])\nplt.title('Workclass', fontsize = 20)\nplt.ylabel('Contagem')\nplt.xlabel('')\nplt.show()    ","a0371860":"g = sns.FacetGrid(df, col=\"sex\", row = 'income', hue = 'income')\ng.map(sns.histplot, \"hours.per.week\", kde = True, fill = True, bins = 30)\n'''plt.title('Distribui\u00e7\u00e3o de sal\u00e1rio base', fontsize = 20)\nplt.xlabel('D\u00f3lares')\nplt.ylabel('Contagem')'''\ng.set_xlabels('Horas semanais')\nplt.gcf().set_size_inches(12, 8)\nplt.show()","eb079e8d":"g = sns.FacetGrid(df, col=\"sex\", row = 'income', hue = 'income')\ng.map(sns.histplot, \"age\", kde = True, fill = True, bins = 30)\n'''plt.title('Distribui\u00e7\u00e3o de sal\u00e1rio base', fontsize = 20)\nplt.xlabel('D\u00f3lares')\nplt.ylabel('Contagem')'''\ng.set_xlabels('Idade')\nplt.gcf().set_size_inches(12, 8)\nplt.show()","072d7a5a":"df[\"native.country\"].value_counts().plot(kind=\"bar\")","2db2d548":"# Distribuicao do indice fnwlwgt, que apresenta uma forte assimetria\nplt.hist(df['fnlwgt'], edgecolor='black', linewidth=1.2, \n         color='navy');\nplt.xlabel('Valor');\nplt.ylabel('Contagem');\nplt.title('Distribui\u00e7\u00e3o do \u00edndice fnwlwgt');","d32a1e71":"df['fnlwgt'] = np.log(df['fnlwgt'])","937dc54d":"plt.hist(df['fnlwgt'], edgecolor='black', linewidth=1.2, \n         color='navy');\nplt.xlabel('Valor');\nplt.ylabel('Contagem');\nplt.title('Distribui\u00e7\u00e3o do \u00edndice fnwlwgt');","87a6e8e3":"plt.figure(figsize = (15, 5))\ng = sns.countplot(x = df['occupation'], hue=df['income'])\nplt.title('Ocupa\u00e7\u00e3o', fontsize = 20)\nplt.ylabel('Contagem')\nplt.xticks(rotation=-70)\nplt.xlabel('')\nplt.show()   ","d87ab7b7":"plt.figure(figsize = (10, 5))\ng = sns.countplot(x = df['race'], hue=df['income'])\nplt.title('Etnia', fontsize = 20)\nplt.ylabel('Contagem')\nplt.xlabel('')\nplt.show()   ","abe33749":"df_test = pd.read_csv('..\/input\/adult-pmr3508\/test_data.csv', na_values='?')","b217c014":"df_test.info()","e2060117":"df_test = df_test.fillna(df_test.mode().iloc[0])","7c490606":"df_test.info()","b199c8d8":"# Iterando sobre todas as colunas categ\u00f3ricas\nfor column in df.select_dtypes(include=['object']).columns.tolist():\n    df[column] = df[column].astype('category').cat.codes","5f827aca":"df","4e60b785":"# Iterando sobre todas as colunas categ\u00f3ricas do teste\nfor column in df_test.select_dtypes(include=['object']).columns.tolist():\n    df_test[column] = df_test[column].astype('category').cat.codes","355615ae":"matriz_correlacao= df.corr()  #Pegamos a matriz de correla\u00e7\u00e3o dos dados\nmask = np.array(matriz_correlacao)  #fazemos uma mask da matriz anterior\nmask[np.tril_indices_from(mask)] = False  #Setamos todos os valores de baixo da mask como False\nfig=plt.gcf()  #Get curret figure\nfig.set_size_inches(30,12)  #Tamanho\nsns.heatmap(data=matriz_correlacao, #mask=mask,  #Plotar o heatmap, vc pode escolher usar a mask ou n\u00e3o, \n            #basta descomentar a parte acima.\n            square=True,annot=True,cbar=True, cmap = 'BuPu') #Tente com 'grays' o cmap, \u00e9 legal","4cb4552d":"# ferramenta de normaliza\u00e7\u00e3o, essencial para o modelo\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n# o modelo de classifica\u00e7\u00e3o propriamente dito\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# algumas fun\u00e7\u00f5es para testar o modelo posteriormete:\n# (explicadas no Turing Talks #11)\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score","f21cda94":"# Separando as features da target\nX_train = df.drop('income', axis = 1)\ny_train = df['income']\n\nX_test = df_test","a48f01d0":"#Criando o dicion\u00e1rio para o par\u00e2metro n_neighbors de 'knn':\nparams = {'knn__n_neighbors': [i for i in range(1, 80, 5)]}\n\n#Criando o pipeline com o passo: 1. KNeighborsClassifier\npipe = Pipeline(steps=[('knn', KNeighborsClassifier())])\n\n#Instanciando o GridSearchCV\nclf = GridSearchCV(estimator=pipe,  #Nosso estimador \u00e9 o pipeline       \n                   param_grid=params,  #Os par\u00e2metros a serem testados s\u00e3o passados aqui pelo dic\n                   cv=5,  #N\u00famero de folds para a valida\u00e7\u00e3o cruzada\n                   verbose=1,  #Para plotar um pouco o processo do modelo\n                   scoring = 'accuracy',  #M\u00e9trica de an\u00e1lise f1\n                   n_jobs = -1,  #Todos os workers trabalhando em paralelo para ir r\u00e1pido\n                   return_train_score=True) # Turn on cv train scores\n\n\nclf.fit(X_train, y_train)  #fit do modelo em treino","8430d028":"clf.best_estimator_ #Vamos ver qual o melhor modelo estimado","fe7a3646":"#Vamos plotar os resultados\nscore = {'M\u00e9trica utilizada': 'f1_score', 'Pontua\u00e7\u00e3o': clf.best_score_,\n         'Melhor par\u00e2metro': clf.best_params_['knn__n_neighbors']}\nscore_df = pd.DataFrame(score, index = [1])\nscore_df","5b2917c9":"# Separando as features da target\nX_new_train = df[['age', 'education', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'relationship']]\ny_train = df['income']\n\nX_new_test = df_test[['age', 'education', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'relationship']]","0987ddd0":"X_new_train","e9390719":"#Criando o dicion\u00e1rio para o par\u00e2metro n_neighbors de 'knn':\nparams = {'knn__n_neighbors': [i for i in range(1, 80, 5)]}\n\n#Criando o pipeline com o passo: 1. KNeighborsClassifier\npipe = Pipeline(steps=[('knn', KNeighborsClassifier())])\n\n#Instanciando o GridSearchCV\nclf = GridSearchCV(estimator=pipe,  #Nosso estimador \u00e9 o pipeline       \n                   param_grid=params,  #Os par\u00e2metros a serem testados s\u00e3o passados aqui pelo dic\n                   cv=5,  #N\u00famero de folds para a valida\u00e7\u00e3o cruzada\n                   verbose=1,  #Para plotar um pouco o processo do modelo\n                   scoring = 'accuracy',  #M\u00e9trica de an\u00e1lise f1\n                   n_jobs = -1,  #Todos os workers trabalhando em paralelo para ir r\u00e1pido\n                   return_train_score=True) # Turn on cv train scores\n\n\nclf.fit(X_new_train, y_train)  #fit do modelo em treino","98083c79":"clf.best_estimator_ #Vamos ver qual o melhor modelo estimado","9b612ebc":"#Vamos plotar os resultados\nscore = {'M\u00e9trica utilizada': 'accuracy', 'Pontua\u00e7\u00e3o': clf.best_score_,\n         'Melhor par\u00e2metro': clf.best_params_['knn__n_neighbors']}\nscore_df = pd.DataFrame(score, index = [1])\nscore_df","53035aa2":"# Faz a classificacao dos dados de teste\ny = clf.best_estimator_.predict(X_new_test)\nprev = pd.DataFrame()\nprev[0] = X_test['Id']\nprev[1] = y\nprev.columns = ['Id', 'income']\nprev['income'] = prev['income'].map({0: '<=50K', 1: '>50K'})\nprev.head()","a06229a9":"prev.to_csv('prev.csv', index=False)","aa349b2b":"### Explora\u00e7\u00e3o do dataset\nCom o acesso ao dataset de treino, vamos fazer uma pequena explora\u00e7\u00e3o no mesmo:","7ba2d0fc":"Agora que temos um dataset completamente num\u00e9rico, vamos enfim implementar o modelo","c589449f":"#### Matriz de correla\u00e7\u00e3o","a2cb0090":"Como esperado tamb\u00e9m existem dados faltantes em teste, vamos lidar com eles da mesma forma que foi feita no caso anterior:","b7f92718":"O modelo que ser\u00e1 aplicado \u00e9 um KNN, que \u00e9 um algor\u00edtmo de classifica\u00e7\u00e3o dependente de um par\u00e2metro K. Para encontrar o valor \u00f3timo deste par\u00e2metro vamos utilizar uma metodologia chamada grid search, que consiste em uma busca extensiva de par\u00e2metros, essencialmente o c\u00f3digo ser\u00e1 executado diversas vezes at\u00e9 que o valor de melhor desempenho seja encontrado. Antes disso \u00e9 necess\u00e1rio fazer o tratamento das features categ\u00f3ricas, vamos utilizar o label encoder, que deixa a base mais limpa que abordagens como one hot encoder.","d7c767a7":"Acima podemos perceber a import\u00e2ncia de uma boa sele\u00e7\u00e3o de features para reduzir a complexidade do modelo a ser utilizado. Neste caso temos que o modelo com algumas features mais correlacionadas linearmente apresenta melhor desempenho que os modelos treinados com todos os dados.","c2fc00ea":"Novamente mais uma interven\u00e7\u00e3o aqui para falar de `GridSearchCV`. Consiste de um m\u00e9todo de busca exaustiva pelo melhor par\u00e2metro. Basicamente funciona da seguinte forma:\n1. Voc\u00ea cria um dicion\u00e1rio contendo as chaves como os nomes do par\u00e2metros que v\u00e3o ser testados e atribui uma lista com os valores que quer testar. \n2. Executa o `GridSearchCV`, passando o dicion\u00e1rio como `params_grid`, ele vai executar o modelo para todos os par\u00e2metros e escolher aquele que retorna maior desempenho em uma m\u00e9trica a sua escolha, padr\u00e3o \u00e9 acur\u00e1cia. \n\nEle faz valida\u00e7\u00e3o cruzada, e voc\u00ea pode escolher quantos *folds* quer. `Pipeline`  \u00e9 uma forma de instanciar diferentes m\u00e9todos de transforma\u00e7\u00e3o a serem aplicados de forma sequ\u00eancial, com um preditor\/classificador ao final. No caso abaixo estou passando o `StandardScaler`, transformador, e `KNeighborsClassifier`, classficador. Algumas das vantagens de utilizar o `pipeline` s\u00e3o:\n* Conveni\u00eancia e encapsulamento: Chama o m\u00e9todo `fit` e `predict` apenas uma vez;\n* Escolha conjunta de par\u00e2metros: Integra\u00e7\u00e3o com o `GridSearch`;\n* Seguran\u00e7a: Evita\/reduz o vazamento de estat\u00edsticas para a base de treino na valida\u00e7\u00e3o cruzada;\n\nVou usar a dist\u00e2ncia como par\u00e2metro dos pesos para um desempenho melhor:","8cc8c22c":"O primeiro passo para come\u00e7ar a trabalhar com um dataset em python \u00e9 realizar a importa\u00e7\u00e3o das bibliotecas que ser\u00e3o necess\u00e1rias, com isso acima fizemos os imports de algumas bibliotecas b\u00e1sicas, como pandas, numpy, para manipula\u00e7\u00e3o dos dados, sklearn para trabalhar com modelos de machine learning e matplotlib e seaborn para visualiza\u00e7\u00e3o. A seguir vamos usar o pandas para importar a base de dados que est\u00e1 em um arquivo csv.","7651bce4":"Aqui vemos uma vari\u00e2ncia maior quando analisamos por ocupa\u00e7\u00e3o. Por exemplo a propor\u00e7\u00e3o de managers executivos que ganham mais de 50 mil por ano \u00e9 quase 1:1 em rela\u00e7\u00e3o aos que n\u00e3o tem o mesmo ganho. Por outro lado funcion\u00e1rios da \u00e1rea de limpeza tem uma rela\u00e7\u00e3o muito menor entre os que ganham menos de 50 mil e mais de 50 mil. Com isso vamos que esta \u00e9 uma feature muito relevante no contexto deste dataset para explicar a vari\u00e2ncia da nossa target.","1d7b8670":"Vemos pelas entradas acima que, aparentementente n\u00e3o existem valores nulos no dataset, mas esse dado pode n\u00e3o estar correto, vamos verificar um pouco:","06fff9eb":"## Aplica\u00e7\u00e3o dos modelos","df822dc5":"Vejamos o \u00edndice abaixo, ele apresenta uma assimetria grande e uma grande concentra\u00e7\u00e3o de dados em uma determinada regi\u00e3o, assim vamos tentar ajustar este dado:","c66ff990":"Com rela\u00e7\u00e3o \u00e0 classe de trabalho, a absoluta maioria, n\u00e3o importanto se ganha mais ou menos de 50 mil, trabalha para a iniciativa privada. Aqui parace que n\u00e3o existem grandes distin\u00e7\u00f5es, a propor\u00e7\u00e3o se mantem a mesma considerando apenas ganhar mais ou menos de 50 mil.","0b71fde6":"\u00c9 poss\u00edvel visualizar acima que o dataset n\u00e3o est\u00e1 balanceado, existe uma amostragem maior de mulheres que homens. Existe tamb\u00e9m uma propor\u00e7\u00e3o maior de mulheres que ganham mais de 50 mil que de homens. Este dado pode estar enviesado dependendo da forma de amostragem que foi utilizada.","7122c915":"### Preprocessamento da base de teste\n\nTudo que fizemos aqui foi em rela\u00e7\u00e3o \u00e0 base de treino, \u00e9 necess\u00e1rio analisar um pouco a base de teste tamb\u00e9m. N\u00e3o vou fazer an\u00e1lises gr\u00e1ficas neste caso, mas tenho interesse em verificar os dados faltantes na base de teste e lidar com eles caso positivo. Vejamos:","1d168057":"Agora sim podemos ver que existem 3 colunas com dados faltantes, workclass, occupation e native country. Estes dados precisam ser ajustados antes de poder ser passados para os modelos a serem utilizados. Existem diversos m\u00e9todos para lidar com dados faltantes, como simplesmente dropar a linha com o dado faltante, fazer imputa\u00e7\u00e3o com m\u00e9dia, moda, o valor de cima\/baixo, ou at\u00e9 utilizar um classificador simples para tentar aproximar o valor original do dado. Aqui vamos utilizar a moda para imputar os valores faltantes, como as colunas que possuem esse tipo de dado s\u00e3o todas categ\u00f3ricas:","fd149533":"### Teste e gera\u00e7\u00e3o do csv","e340b08f":"Como podemos ver existem entradas com o valor '?', que corresponde \u00e0 um dado faltante, mas como a entrada est\u00e1 formatada o m\u00e9todo index() n\u00e3o interpreta isso como um valor nulo, portanto vamos reler o dataset, substitu\u00edndo estas entradas por um valor nulo:","273fe76c":"O mesmo agora est\u00e1 muito mais bem distribu\u00eddo ao redor de um valor m\u00e9dio.","7c97b53f":"Agora que n\u00e3o temos mais dados faltantes, vamos seguir explorando as colunas e vendo suas rela\u00e7\u00f5es com nossa terget, que \u00e9 a coluna income:","abf5e419":"O gr\u00e1fico acima mostra tamb\u00e9m que a maior parte do dataset \u00e9 focada nos EUA, mesmo que a variedade de pa\u00edses seja grande, a imensa maioria das amostras apresenta os EUA como pa\u00eds nativo.","be9780c9":"\u00c9 poss\u00edvel perceber tamb\u00e9m que a distribui\u00e7\u00e3o \u00e9 muito semelhante tamb\u00e9m quando usamos a idade e separamos por g\u00eanero. \u00c9 poss\u00edvel perceber que a distribui\u00e7\u00e3o masculina e feminina entre os que ganham menos de 50 mil \u00e9 uma vers\u00e3o pr\u00f3xima da dos que ganham mais de 50 mil, apenas mais achatada, devido ao n\u00famero menor de indiv\u00edduos na segunda classe.","6b47d25d":"### Modelo com todas as features","aeb5e173":"Nesta an\u00e1lise introduzimos uma vari\u00e1vel num\u00e9rica, e por isso o histograma \u00e9 mais apropriado para a an\u00e1lise. \u00c9 poss\u00edvel perceber aqui que as propor\u00e7\u00f5es por idade, separados os g\u00eaneros, parece se manter, com os que recebem mais de 50 mil estando apenas em uma quantidade absoluta menor. A linha estimadora da distribui\u00e7\u00e3o KDE mostra que elas aparentam ser parecidas. Apesar de parecer que para as mulheres a faixa dos 40 anos parece ser a mais prop\u00edcia a ganhar maiores sal\u00e1rios, isto n\u00e3o se confirma ao ver que a quantidade de pessoas nesta faixa de idades \u00e9 muito maior que nas outras.","2eac9108":"### Avalia\u00e7\u00e3o","bbbd5e62":"### Modelo com sele\u00e7\u00e3o de features"}}