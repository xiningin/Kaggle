{"cell_type":{"8ff1085c":"code","cceeab89":"code","75f01562":"code","fcbb7ea8":"code","9556e5a6":"code","9e31219f":"code","322d1cbf":"code","cc915953":"code","04dd7117":"code","0e72c35e":"code","82d743ed":"code","17d5d75c":"code","d08f1916":"code","5cdf5528":"code","2750f3e0":"code","9defbb55":"code","bb13646f":"code","2b5ddfe3":"code","229ba568":"code","d8a8d78e":"code","0b6711ef":"code","74d8d043":"code","53f9133b":"code","bd5c8502":"code","5871e279":"code","9434b57a":"code","1a016f4f":"code","514e641e":"code","eee94014":"code","c1ae0e55":"code","30269f43":"code","11061a2d":"code","1dc973c9":"code","9e8de322":"code","c7b5ab42":"code","ea1641a1":"code","ed78ce15":"code","41cefbf8":"code","a3177f93":"code","a3ac9db4":"code","614b617e":"code","38169e3a":"code","5f8d889e":"code","a37e41c5":"code","07abfada":"code","b83dc701":"code","6fb83663":"code","702ee5e1":"code","3bef5461":"code","abc2f936":"code","ea46a007":"code","882faaa6":"code","bf2f6113":"code","e668e356":"code","941e926d":"code","2af790ff":"code","2f8c9fad":"code","9d9a82a9":"code","eed0b793":"code","6b6fe724":"code","379f6dd4":"code","7364e619":"code","e38d23c6":"code","e9c05f1e":"code","1a5631b2":"code","a7e6e711":"code","ebb0d91c":"code","e5949968":"code","2414ce38":"code","78500775":"code","3be5eb6a":"code","d95d0a28":"code","27eb3a7c":"code","31462dab":"code","9bb759eb":"markdown","57875606":"markdown","251b0d1b":"markdown","3c7f2990":"markdown","f8023a94":"markdown","3ff2e412":"markdown","8ace4bcd":"markdown","ee2e043b":"markdown","85d8b867":"markdown","0d6c6d7b":"markdown","3e47f2db":"markdown","075d38eb":"markdown","139e7e14":"markdown","5b6f31cf":"markdown","f312f301":"markdown","17b9151b":"markdown","5ef269f0":"markdown","3894b4f1":"markdown","d91c6315":"markdown","96c778dc":"markdown","69bb9d7f":"markdown","656c39da":"markdown","14a33c75":"markdown","6fef15ab":"markdown","d7a5a02c":"markdown","f57a1ef7":"markdown","75bc5853":"markdown","7886d7f2":"markdown","a8e2ad6c":"markdown","34d4a7b9":"markdown","0940c26e":"markdown","5f0024a1":"markdown","a2833746":"markdown","206d39c6":"markdown","e6717408":"markdown","ba0fc49d":"markdown","a745b89b":"markdown","ee1e8289":"markdown","e32d3e99":"markdown","eb0abf0b":"markdown","be049a02":"markdown","4a2ddce1":"markdown","cf4be4e3":"markdown","9a749526":"markdown","8e5b6aa9":"markdown","01ba2887":"markdown","8ca4de75":"markdown","d8772058":"markdown","aaf0fd50":"markdown","5ca1df81":"markdown","6eb06fac":"markdown","f163abb3":"markdown","d08df314":"markdown","c874f65a":"markdown","ff020d90":"markdown","6af35018":"markdown","e4ee12f0":"markdown","c39c84c2":"markdown","d673f3d5":"markdown"},"source":{"8ff1085c":"# Primary libraries\nimport numpy as np\nimport pandas as pd\nimport os\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\n\n#Stats\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n#Modelling\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, ElasticNetCV,LassoCV, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split\n\n# Others\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.decomposition import PCA\n\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n# avoid warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","cceeab89":"df_train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ndf_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\nprint (\"Data is loaded!\")","75f01562":"df_train .head()","fcbb7ea8":"# See the name of the columns\ndf_train.columns","9556e5a6":"df_train.info()","9e31219f":"df_train.describe()","322d1cbf":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndf_train.hist(bins=40, figsize=(20,60))\n#save_fig(\"attribute_histogram_plots\")\nplt.show()","cc915953":"#descriptive statistics summary\ndf_train['SalePrice'].describe()","04dd7117":"#histogram\nplt.figure(figsize=(10,5))\nsns.distplot(df_train['SalePrice']);","0e72c35e":"print(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","82d743ed":"#Scatter plot of SalePrice\/GrLivArea\ndf_train.plot(kind=\"scatter\", x=\"GrLivArea\", y=\"SalePrice\", alpha=1)","17d5d75c":"#Scatter plot of SalePrice\/ TotalBsmtSF\ndf_train.plot(kind=\"scatter\", x=\"TotalBsmtSF\", y=\"SalePrice\", alpha=1)","d08f1916":"df_train.plot(kind=\"scatter\", x=\"OverallQual\", y=\"SalePrice\", alpha=1)","5cdf5528":"# Remove outliers\ndf_train.drop(df_train[(df_train['GrLivArea']>4500) & (df_train['SalePrice']<300000)].index, inplace=True)\ndf_train.drop(df_train[(df_train['TotalBsmtSF']>6000) & (df_train['SalePrice']<300000)].index, inplace=True)\ndf_train.drop(df_train[(df_train['OverallQual']<4) & (df_train['SalePrice']>200000)].index, inplace=True)\ndf_train.reset_index(drop=True, inplace=True)","2750f3e0":"#box plot overallqual\/saleprice\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=df_train[\"OverallQual\"], y=df_train[\"SalePrice\"])\nfig.axis(ymin=0, ymax=800000);","9defbb55":"#box plot overallqual\/saleprice\nf, ax = plt.subplots(figsize=(20, 10))\nfig = sns.boxplot(x=df_train[\"YearBuilt\"], y=df_train[\"SalePrice\"])\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","bb13646f":"corr_matrix = df_train.corr()","2b5ddfe3":"corr_matrix[\"SalePrice\"].sort_values(ascending=False)","229ba568":"attributes = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nscatter_matrix(df_train[attributes], figsize=(20,10))\n\nplt.show()","d8a8d78e":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","0b6711ef":"# numpy fucntion log1p transforms it\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n#See the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","74d8d043":"# Split features and labels\ny = df_train['SalePrice'].reset_index(drop=True)\ntrain_features = df_train.drop(['SalePrice'], axis=1)\ntest_features = df_test\n\n# Combine train and test features\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","53f9133b":"#first check the missing data\ntotal = all_features.isnull().sum().sort_values(ascending=False)\npercent = (all_features.isnull().sum()\/all_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent*100], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(35)","bd5c8502":"# Visualize missing values\nmissing_data=missing_data.head(35)\nf, ax = plt.subplots(figsize=(16, 8))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.title('Percent missing data by feature', fontsize=15)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)","5871e279":"# PoolQC : data description says NA means \"no pool\"\nall_features[\"PoolQC\"] = all_features[\"PoolQC\"].fillna(\"None\")\n# MiscFeature : data description says NA means \"no misc feature\"\nall_features[\"MiscFeature\"] =all_features[\"MiscFeature\"].fillna(\"None\")\n# Alley : data description says NA means \"no alley access\"\nall_features[\"Alley\"] = all_features[\"Alley\"].fillna(\"None\")\n# Fence : data description says NA means \"no fence\"\nall_features[\"Fence\"] = all_features[\"Fence\"].fillna(\"None\")\n# FireplaceQu : data description says NA means \"no fireplace\"\nall_features[\"FireplaceQu\"] =all_features[\"FireplaceQu\"].fillna(\"None\")\n# LotFrontage : filling missing values by the median LotFrontage of the neighborhood\nall_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n# GarageType etc : data description says NA for garage features is \"no garage\"\nfor col in ( 'GarageCond', 'GarageQual','GarageType', 'GarageFinish'):\n    all_features[col] = all_features[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageCars','GarageArea'):\n    all_features[col] = all_features[col].fillna(0)\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n     all_features[col] =  all_features[col].fillna('None')       \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n     all_features[col] =  all_features[col].fillna(0)      \n# MasVnrType : NA most likely means no veneer\nall_features[\"MasVnrArea\"] = all_features[\"MasVnrArea\"].fillna(0)\nall_features[\"MasVnrType\"] = all_features[\"MasVnrType\"].fillna(\"None\")\n# MSZoning : RL is the most common value\nall_features['MSZoning'] = all_features['MSZoning'].fillna(all_features['MSZoning'].mode()[0])\n#Functional : data description says Typ means typical\nall_features[\"Functional\"] = all_features[\"Functional\"].fillna(\"Typ\")\n# Utilities : NA most likely means all public utilities\nall_features[\"Utilities\"] = all_features[\"Utilities\"].fillna(\"AllPub\")\n# SBrkr is the most common value\nall_features['Electrical'] = all_features['Electrical'].fillna(\"SBrkr\")\n#all_features['Electrical'] = all_features['Electrical'].fillna(all_features['Electrical'].mode()[0])\n# KitchenQual : NA most likely means typical\nall_features[\"KitchenQual\"] = all_features[\"KitchenQual\"].fillna(\"TA\")\n#Functional : data description says NA most likely means No building class          \nall_features['MSSubClass'] = all_features['MSSubClass'].fillna(\"None\")\n# Replace few other missing values with their mode\nall_features['Exterior1st'] = all_features['Exterior1st'].fillna(all_features['Exterior1st'].mode()[0])\nall_features['Exterior2nd'] = all_features['Exterior2nd'].fillna(all_features['Exterior2nd'].mode()[0])\nall_features['SaleType'] = all_features['SaleType'].fillna(all_features['SaleType'].mode()[0])","9434b57a":"#convert them into strings\nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","1a016f4f":"# Filter the skewed features\nnumeric_features = all_features.select_dtypes(include='number').columns\nskew_features = all_features[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewness = skew_features[skew_features > 0.5]\nskew_index = skewness.index\n\nskewness = pd.DataFrame({'Skew' :skewness})\nskewness.head(10)","514e641e":"#fix skewed features\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","eee94014":"all_features[\"AllSF\"] = all_features[\"GrLivArea\"] + all_features[\"TotalBsmtSF\"]\nall_features[\"AllFlrsSF\"] = all_features[\"1stFlrSF\"] + all_features[\"2ndFlrSF\"]\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\nall_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['HasWoodDeck'] = (all_features['WoodDeckSF'] == 0) * 1\nall_features['HasOpenPorch'] = (all_features['OpenPorchSF'] == 0) * 1\nall_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] == 0) * 1\nall_features['Has3SsnPorch'] = (all_features['3SsnPorch'] == 0) * 1\nall_features['HasScreenPorch'] = (all_features['ScreenPorch'] == 0) * 1","c1ae0e55":"def logs(columns):\n    for col in columns:\n        all_features[col+\"_log\"] = np.log(1.01+all_features[col])  \n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal','YearRemodAdd','TotalSF']\n\nlogs(log_features)","30269f43":"def squares(columns):\n    for col in columns:\n        all_features[col+\"_sq\"] =  all_features[col] * all_features[col]\n\nsquared_features = ['GarageArea_log','GarageCars_log','GrLivArea_log','YearRemodAdd', 'LotFrontage_log', 'TotalBsmtSF_log', '2ndFlrSF_log', 'GrLivArea_log' ]\n\nsquares(squared_features)","11061a2d":"# Encode some categorical features as ordered numbers when there is information in the order\nall_features = all_features.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )","1dc973c9":"#numerically encode categorical features\nall_features = pd.get_dummies(all_features).reset_index(drop=True)","9e8de322":"# let's see the shape one more time\nall_features.shape","c7b5ab42":"all_features.head()","ea1641a1":"# check is there any NAs in the dataset \nprint(\"NAs for categorical features in the dataset : \" + str(all_features.isnull().values.sum()))","ed78ce15":"X = all_features.iloc[:len(y), :]\nX_test = all_features.iloc[len(y):, :]","41cefbf8":"X.shape, y.shape, X_test.shape","a3177f93":"kf = KFold(n_splits=12, random_state=42, shuffle=True)","a3ac9db4":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef rmse_cv(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","614b617e":"#We will also calculate the best alpha and ridge model in details\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10,15, 20,25, 30, 60,90])\nridge.fit(X, y)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4, alpha * 1.5, alpha * 1.6, alpha * 1.8], \n                cv = kf)\nridge.fit(X, y)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nalphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4, alpha * 1.5, alpha * 1.6, alpha * 1.8]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas, cv=kf))","38169e3a":"#We will also calculate the best alpha and lasso model in details\nlasso = LassoCV(alphas = [0.0001, 0.0003,0.0005, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = kf)\nlasso.fit(X, y)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = kf)","5f8d889e":"#We will also calculate the best alpha, ratio and the elasticNet model in details\nelasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7,0.75, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0005, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 5000, cv = kf)\nelasticNet.fit(X, y)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 0.8, 1, 3, 6], \n                          max_iter = 5000, cv = kf)\nelasticNet.fit(X, y)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 5000, cv = kf)\n","a37e41c5":"svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))","07abfada":"KRR = make_pipeline(RobustScaler(), KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5))\n","b83dc701":"rf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)","6fb83663":"gbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)","702ee5e1":"xgboost = XGBRegressor(learning_rate=0.01, n_estimators=4060,gamma=0.0482,\n                                     max_depth=4, min_child_weight=0,\n                                     subsample=0.7,colsample_bytree=0.4603, \n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,reg_lambda=0.8571,\n                                     reg_alpha=0.00006,random_state=42)","3bef5461":"lightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=6500,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=5, \n                       bagging_seed=9,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_data_in_leaf =6,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)","abc2f936":"stackReg = StackingCVRegressor(regressors=(lightgbm,gbr, rf, ridge,svr,lasso),\n                                meta_regressor=(xgboost),\n                                use_features_in_secondary=True, \n                                random_state=42)","ea46a007":"scores = {}\n\nscore = rmse_cv(ridge)\nridge_model= ridge.fit(X, y)\nprint(\"ridge : {:0.4f}\".format(score.mean()))\nscores['ridge'] = score.mean()","882faaa6":"score = rmse_cv(lasso)\nlasso_model= lasso.fit(X, y)\nprint(\"lasso : {:0.4f}\".format(score.mean()))\nscores['lasso'] = score.mean()","bf2f6113":"score = rmse_cv(svr)\nsvr_model= svr.fit(X, y)\nprint(\"svr: {:0.4f}\".format(score.mean()))\nscores['svr'] = score.mean()","e668e356":"score = rmse_cv(KRR)\nKRR_model= KRR.fit(X, y)\nprint(\"KRR: {:0.4f}\".format(score.mean()))\nscores['KRR'] = score.mean()","941e926d":"score = rmse_cv(rf)\nrf_model= rf.fit(X, y)\nprint(\"rf: {:0.4f}\".format(score.mean()))\nscores['rf'] = score.mean()","2af790ff":"score = rmse_cv(gbr)\ngbr_model= svr.fit(X, y)\nprint(\"gbr: {:0.4f}\".format(score.mean()))\nscores['gbr'] = score.mean()","2f8c9fad":"score = rmse_cv(xgboost)\nxgboost_model= xgboost.fit(X, y)\nprint(\"xgboost: {:0.4f}\".format(score.mean()))\nscores['xgboost'] = score.mean()","9d9a82a9":"score = rmse_cv(lightgbm)\nlgb_model= lightgbm.fit(X, y)\nprint(\"lightgbm: {:0.4f}\".format(score.mean()))\nscores['lgb'] = score.mean()","eed0b793":"score = rmse_cv(stackReg)\nstackReg_model= stackReg.fit(X, y)\nprint(\"stackReg: {:0.4f}\".format(score.mean()))\nscores['stackReg'] = score.mean()","6b6fe724":"model =Sequential()\n\nmodel.add(Dense(358,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(180,activation='relu'))\n\nmodel.add(Dense(90,activation='relu'))\n\nmodel.add(Dense(45,activation='relu'))\n\nmodel.add(Dense(20,activation='relu'))\n\nmodel.add(Dense(10,activation='relu'))\n\nmodel.add(Dense(4,activation='relu'))\n\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')","379f6dd4":"scale = StandardScaler()\nX_train = scale.fit_transform(X)\nX_val=scale.transform(X_test)","7364e619":"model.fit(X_train, y.values, epochs=50,batch_size=32)","e38d23c6":"loss_df=pd.DataFrame(model.history.history)\nloss_df.plot()","e9c05f1e":"#Check the rmse of the deep neural network model\nrmse=mean_squared_error(model.predict(X_train), y)**0.5\nrmse\nscores['deepNeural'] = rmse","1a5631b2":"scores","a7e6e711":"#preeict Test Sales Price\nyhat=np.expm1(model.predict(X_val))\nyhat","ebb0d91c":"#Also ignoring the deep neural network model as this can overfit the test data\ndef blendedModelPredictions(X,X_train):\n    return ((0.15 * ridge_model.predict(X)) + \\\n            (0.2 * svr_model.predict(X)) + \\\n            (0.1 * gbr_model.predict(X)) + \\\n            (0.15 * xgboost_model.predict(X)) + \\\n            (0.1 * lgb_model.predict(X)) + \\\n            (0.3 * stackReg_model.predict(np.array(X))))","e5949968":"yhat11=blendedModelPredictions(X_test,X_val)\nnp.expm1(yhat11)","2414ce38":"# blendedModelPrediction\nblended_score = rmsle(y, blendedModelPredictions(X,X_train))\nprint(\"blended score: {:.4f}\".format(blended_score))\nscores['blended'] =  blended_score","78500775":"pd.Series(scores).sort_values(ascending=True)","3be5eb6a":"submission = pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')\n","d95d0a28":"submission.iloc[:,1] = np.floor(np.expm1(blendedModelPredictions(X_test,X_val)))","27eb3a7c":"submission.head()","31462dab":"submission.to_csv(\"submission_V1.csv\", index=False)","9bb759eb":"<h5> Setup cross validation <\/h5>","57875606":"<h5>Define error metrics <\/h5>","251b0d1b":"<h5> Create new features <\/h5>","3c7f2990":"You can ignore the detiails of this Elastic net model as this is taking longer time to run. You can run just the followinng:\n#elasticNet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0006, l1_ratio=.8075, random_state=3))","f8023a94":"<h5> XGBoost Regressor <\/h5>","3ff2e412":"Let's look at the other values. The describe() method shows a summary of the numnerical attributes.","8ace4bcd":"<h5> Light Gradient Boosting Regressor <\/h5>","ee2e043b":"<h3> Predictions and Submission <\/h3>","85d8b867":"<h5> Base Models Scores <\/h5>","0d6c6d7b":"Another way to check for correlation between attributes is to use the\npandas scatter_matrix() function, which plots every numerical\nattribute against every other numerical attribute. Since there are now 81\nnumerical attributes, you would get 81*81 plots, which would not fit on\na page\u2014so let\u2019s just focus on a few promising attributes that seem most\ncorrelated with the SalePrice,","3e47f2db":"<h5> Stacking CV Regressor <\/h5>","075d38eb":"The describe() method shows a descriptive statistics summary of the SalePrice.","139e7e14":"<h5> Imputing missing values <\/h5>","5b6f31cf":"<h5> Gradient Boosting Regressor <\/h5>","f312f301":"The info() method is useful to get a quick description of the data, in\nparticular the total number of rows, each attribute\u2019s type, and the number\nof nonnull values","17b9151b":"<h5> Create train and test sets <\/h5>","5ef269f0":"Each row reprsents one district. There are 81 attributes","3894b4f1":"<h5> Base Models <\/h5>","d91c6315":"The goal of our project is to predict the SalePrice which is our target variable, given these features. Let's analyze the salePrice.","96c778dc":"<h5> Let's get started <\/h5>","69bb9d7f":"The 25%, 50%, and 75% rows\nshow the corresponding percentiles: a percentile indicates the value below\nwhich a given percentage of observations in a group of observations fall. These\nare often called the 25th percentile (or first quartile), the median, and the\n75th percentile (or third quartile).","656c39da":"There are 1460 instances in the dataset","14a33c75":"<h5> Kernel Ridge Regression <\/h5>","6fef15ab":"<h3> Exploratory Data Analysis (EDA) <\/h3>","d7a5a02c":"<h5> Thank you very much <\/h5>","f57a1ef7":"<h1> <center> Housing Prices Kaggle Competition<\/center><\/h1>","75bc5853":"Let's import the libraries","7886d7f2":"<h5> Log Transformation of the target varibale <\/h5>","a8e2ad6c":"<h5>Support Vector Regressor <\/h5>","34d4a7b9":"Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","0940c26e":"I found the following Kernel very useful\n1. https:\/\/www.kaggle.com\/itslek\/blend-stack-lr-gb-0-10649-house-prices-v57\/data?scriptVersionId=11189608\n2. https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n\nThis is my first Kernel in Kaggle. Please let me know if you think this Kerner can be further improved or find an error. \nA few upvotes would by highly appreciated,  if you can find this Kernel useful. \n    \n","5f0024a1":"The SalePrice is deviated from the normal distribution and shows positive skewness. ","a2833746":"Another quick way to get a feel of the type of data is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).","206d39c6":"<h5> Data Cleaning <\/h5>","e6717408":"<h3> Predict sales prices and practice feature engineering, RFs, and gradient boosting<\/h3>","ba0fc49d":"<h5> Some numerical features are actually really categories<\/h5>","a745b89b":"<h5>  Linear Regression with Ridge regularization (L2 penalty) <\/h5>","ee1e8289":"<h5 > Looking for Correlations <\/h5>","e32d3e99":"See the loss ","eb0abf0b":"<h5> Fix skewed features <\/h5>","be049a02":"Let's take a quick look at the top 5 rows of the training data using the DataFrame's head() method","4a2ddce1":"#####  Linear Regression with ElasticNet regularization (L1 and L2 penalty)","cf4be4e3":"### Modelling","9a749526":"<h5> Competition Description <\/h5> ","8e5b6aa9":"Let's import the data","01ba2887":"This is my frist competion in Kaggle and found this dataset as a good start. I tried to explian the feature engineering\nparts, applied different models inlcuding a deep neural network  and blended these models for a higher accuracey.\n\nThis Kernel will be very helpful for the beginner. I have also recommended few other Kernels at the end which I found very\nuseful. If you like this Kernerl Please give an upvote. \n","8ca4de75":"Let's create a few more features by calcualting the log and the square of the features","d8772058":"#####  Linear Regression with Lasso regularization (L1 penalty)","aaf0fd50":"<h3> Prepare the Data for Machine Learning Algorithms <\/h3>","5ca1df81":"<h5> The target variable: SalesPrice <\/h5>","6eb06fac":"<h5> Deep Neural Network with Dropout <\/h5>","f163abb3":"Let's analyze the relationship of SalePrice with other numerical variables  ","d08df314":"<h5> Random Forest Regressor <\/h5>","c874f65a":"<h5> Discover and Visualize the Data to Gain\nInsights<\/h5>","ff020d90":"This shows a linear relationship and there are two large GrLivArea have low price. These two are outliers. ","6af35018":"The correlation coefficient ranges from \u20131 to 1. When it is close to 1, it\nmeans that there is a strong positive correlation. When the\ncoefficient is close to \u20131, it means that there is a strong negative\ncorrelation. Finally, coefficients close to 0 mean that there is no\nlinear correlation.","e4ee12f0":"<h3> Blend all the models and let's get the predictions <\/h3>","c39c84c2":"<h5> SalePrice relationship with categorial features <\/h5>","d673f3d5":"Let's remove the outliers"}}