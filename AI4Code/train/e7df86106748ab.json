{"cell_type":{"3d78a730":"code","2def2fdb":"code","945ac559":"code","9b8d25d3":"code","9f700d98":"code","1d611719":"code","a588df5f":"code","b7a4d903":"code","f90ad78c":"code","16696afd":"code","8b9d444d":"code","3c9f9e46":"code","2d5f1f4f":"code","21a4ae3f":"code","e586948d":"code","7255346b":"markdown","f9e552cf":"markdown","6e7bde5f":"markdown","4a5cfb1c":"markdown","99fbbd19":"markdown","8d6bb18b":"markdown","0a66849e":"markdown","f0074e44":"markdown","e27a9e4d":"markdown","7fab9fcf":"markdown","91e8b90d":"markdown","651ba02d":"markdown","6e4b97b4":"markdown"},"source":{"3d78a730":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2def2fdb":"df=pd.read_csv('..\/input\/2021-jan-nifty\/2021 JAN NIFTY.txt',header=None)\ndf.drop(labels=[0,7,8],axis=1,inplace=True)\ndf.columns=['date','time','open','high','low','close']","945ac559":"df.head()","9b8d25d3":"df[\"day\"]=df.date%5\ndf[\"month\"]=((df.date\/100)%100).astype(int)\ndf[\"year\"]=(df.date\/10000).astype(int)","9f700d98":"df['close'].plot()\nplt.show()\ndf['close'].hist()\nplt.show()\ndf['close'].plot(kind='kde')\nplt.show()","1d611719":"from pandas.plotting import lag_plot\nfrom pandas import DataFrame\nfrom pandas import concat\nlags=8\n\ncolumns = [df['close']]\nfor i in range(1,(lags + 1)):\n  columns.append(df['close'].shift(i))\ndataframe = concat(columns, axis=1)\ncolumns = ['t']\nfor i in range(1,(lags + 1)):\n  columns.append('t-' + str(i))\ndataframe.columns = columns\nplt.figure(1)\nfor i in range(1,(lags + 1)):\n  ax = plt.subplot(240 + i)\n  ax.set_title('t vs t-' + str(i))\n  plt.scatter(x=dataframe['t'].values, y=dataframe['t-'+str(i)].values)\nplt.show()\n","a588df5f":"from pandas.plotting import autocorrelation_plot\n\nautocorrelation_plot(df['close'])\n","b7a4d903":"from math import sqrt\nfrom numpy import mean\nfrom sklearn.metrics import mean_squared_error\nwindow = 3\nhistory = [df['close'][i] for i in range(window)]\ntest = [df['close'][i] for i in range(window, len(df['close']))]\npredictions = list()\n# walk forward over time steps in test\nfor t in range(len(test)):\n  length = len(history)\n  yhat = mean([history[i] for i in range(length-window,length)])\n  yhat=round(yhat \/ 0.05)*0.05\n  obs = test[t]\n  predictions.append(yhat)\n  history.append(obs)\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.show()\n# zoom plot\nplt.plot(test[:100])\nplt.plot(predictions[:100], color='red')\nplt.show()\n","f90ad78c":"from pandas import DataFrame\nfrom pandas import concat\nfrom statsmodels.graphics.gofplots import qqplot\nvalues = DataFrame(df['close'].values)\ndataframe = concat([values.shift(1), values], axis=1)\ndataframe.columns = ['t', 't+1']\n# split into train and test sets\nX = dataframe.values\ntrain_size = int(len(X) * 0.9)\ntrain, test = X[1:train_size], X[train_size:]\ntrain_X, train_y = train[:,0], train[:,1]\ntest_X, test_y = test[:,0], test[:,1]\n# persistence model\npredictions = [x for x in test_X]\nrmse = sqrt(mean_squared_error(test_y, predictions))\nprint('Persistence RMSE: %.3f' % rmse)\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.show()\n# zoom plot\nplt.plot(test[:100])\nplt.plot(predictions[:100], color='red')\nplt.show()\n# calculate residuals\nresiduals = [test_y[i]-predictions[i] for i in range(len(predictions))]\nresiduals = DataFrame(residuals)\nresiduals.hist()\nplt.show()\n# density plot\nresiduals.plot(kind='kde')\nplt.show()\nautocorrelation_plot(residuals)\nplt.show()\nresiduals = np.array(residuals)\nqqplot(residuals, line='r')\nplt.show()\n","16696afd":"from statsmodels.tsa.ar_model import AR\nX = df['close'].values\ntrain, test = X[1:int(len(X)*0.9)], X[int(len(X)*0.9):]\n# train autoregression\nmodel = AR(train)\nmodel_fit = model.fit()\nwindow = model_fit.k_ar\ncoef = model_fit.params\n# walk forward over time steps in test\nhistory = train[len(train)-window:]\nhistory = [history[i] for i in range(len(history))]\npredictions = list()\nfor t in range(len(test)):\n  length = len(history)\n  lag = [history[i] for i in range(length-window,length)]\n  yhat = coef[0]\n  for d in range(window):\n    yhat += coef[d+1] * lag[window-d-1]\n    yhat = round(yhat \/ 0.05)*0.05\n  obs = test[t]\n  predictions.append(yhat)\n  history.append(obs)\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.show()\nplt.plot(test[:100])\nplt.plot(predictions[:100], color='red')\nplt.show()","8b9d444d":"values = DataFrame(df['close'].values)\ndataframe = concat([values.shift(1), values], axis=1)\ndataframe.columns = ['t', 't+1']\n# split into train and test sets\nX = dataframe.values\ntrain_size = int(len(X) * 0.9)\ntrain, test = X[1:train_size], X[train_size:]\ntrain_X, train_y = train[:,0], train[:,1]\ntest_X, test_y = test[:,0], test[:,1]\n# persistence model on training set\ntrain_pred = [x for x in train_X]\n# calculate residuals\ntrain_resid = [train_y[i]-train_pred[i] for i in range(len(train_pred))]\n# model the training set residuals\nmodel = AR(train_resid)\nmodel_fit = model.fit()\nwindow = model_fit.k_ar\ncoef = model_fit.params\n# walk forward over time steps in test\nhistory = train_resid[len(train_resid)-window:]\nhistory = [history[i] for i in range(len(history))]\npredictions = list()\nfor t in range(len(test_y)):\n  # persistence\n  yhat = test_X[t]\n  error = test_y[t] - yhat\n  # predict error\n  length = len(history)\n  lag = [history[i] for i in range(length-window,length)]\n  pred_error = coef[0]\n  for d in range(window):\n    pred_error += coef[d+1] * lag[window-d-1]\n  # correct the prediction\n  yhat = yhat + pred_error\n  yhat = round(yhat \/ 0.05)*0.05\n  predictions.append(yhat)\n  history.append(error)\n# error\nrmse = sqrt(mean_squared_error(test_y, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot predicted error\nplt.plot(test_y)\nplt.plot(predictions, color='red')\nplt.show()\nplt.plot(test[:100])\nplt.plot(predictions[:100], color='red')\nplt.show()","3c9f9e46":"from statsmodels.tsa.stattools import adfuller\nresult = adfuller(df['close'])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n  print('\\t%s: %.3f' % (key, value))","2d5f1f4f":"diff = list()\nfor i in range(1, len(df['close'])):\n  value = df['close'][i] - df['close'][i - 1]\n  diff.append(value)\nresult = adfuller(diff)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n  print('\\t%s: %.3f' % (key, value))\nautocorrelation_plot(diff)\nplt.show()\nplt.hist(diff)\nplt.show()\nplt.plot(diff)","21a4ae3f":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(df['close'],lags=200)\nplt.show()\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(df['close'])\nplt.show()\n","e586948d":"from statsmodels.tsa.arima_model import ARIMA\nX = df['close'].values\nsize = int(len(X) * 0.9)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\ni=0\n# walk-forward validation\nfor t in range(len(test)):\n  model = ARIMA(history, order=(4,1,2))\n  model_fit = model.fit(disp=0)\n  output = model_fit.forecast()\n  yhat=output[0]-0.42\n  yhat = np.round(yhat \/ 0.05) * 0.05\n  predictions.append(yhat)\n  obs = test[t]\n  history.append(obs)\n  if(np.round(output[2][0][0] \/ 0.05)*0.05>obs or round(output[2][0][1] \/ 0.05)*0.05<obs):\n    i=i+1\n# evaluate forecasts\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\nprint(100*i\/len(test))\nresiduals = [test[i]-predictions[i] for i in range(len(test))]\nresiduals = DataFrame(residuals)\nprint(residuals.describe())\nplt.figure()\nplt.subplot(211)\nresiduals.hist(ax=plt.gca())\nplt.subplot(212)\nresiduals.plot(kind='kde', ax=plt.gca())\nplt.show()\n\n# plot forecasts against actual outcomes\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.show()\n","7255346b":"**We see that ARIMA model perform better than the baseline Persistence model.But not by much.\n**","f9e552cf":"We get d=1 for ARIMA parameters p,q,d","6e7bde5f":"Stationay test","4a5cfb1c":"Moving Average Prediction\n","99fbbd19":"AR Model","8d6bb18b":"MA Model","0a66849e":"ARIMA","f0074e44":"BASELINE MODEL-Persistence Algorithm","e27a9e4d":"From the pacf plot we get q=2\nThe acf plot suggests a high value of p","7fab9fcf":"Taking p=4","91e8b90d":"d=1 as discussed above\n","651ba02d":"ACF & PACF plots","6e4b97b4":"The series is clearly nonstationary.Differencing to make it stationary."}}