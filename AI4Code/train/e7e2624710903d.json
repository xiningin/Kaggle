{"cell_type":{"9c399d95":"code","606213e3":"code","7fee32c5":"code","0bdaec42":"code","a60b868c":"code","ce6ffd30":"code","e23fde43":"code","2859b3e1":"code","2213f690":"code","d7b4b70a":"code","626a82e4":"code","abfe26e4":"code","484a3c61":"code","6449bfd5":"code","1f43ea1e":"code","b0ce3fc5":"code","dd9e49cf":"code","37e2ddb7":"code","8f507027":"code","5f948176":"code","520d13f9":"code","8a3b70c6":"code","a9f612e4":"code","11d381e7":"code","0f897f9e":"code","3eabffd2":"code","06696847":"code","193d9fa4":"code","3ada29d8":"code","e0f47414":"code","6462870d":"code","e90b2fe5":"code","0d8a3158":"code","59152699":"code","c11e6354":"code","e0d1c7a4":"code","5297e2c6":"code","884537bb":"code","da6ec32c":"code","cf2cd1c1":"code","ad14925b":"code","0ec196ca":"code","a1ea1fc5":"code","9bd9ebff":"code","0ae83936":"code","939ccb9a":"code","f51f8371":"code","ac826018":"code","d1fb5507":"code","c3ac8015":"code","96b93dae":"markdown","a3bbcd90":"markdown","bf821bf2":"markdown","61ec67c7":"markdown","ba67346d":"markdown","02169651":"markdown","9f06a17b":"markdown","fe6a00d5":"markdown","c90daf53":"markdown","47099528":"markdown","8524cff7":"markdown","80a56281":"markdown","bb132078":"markdown","67c05b44":"markdown"},"source":{"9c399d95":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","606213e3":"dataDict = pd.read_csv('..\/input\/country-socioeconomic-data\/data-dictionary.csv')\ndataDict","7fee32c5":"df = pd.read_csv('..\/input\/country-socioeconomic-data\/Country-data.csv')\ndf.head()","0bdaec42":"def shape(x):\n    rows, cols = df.shape\n    print(f'The dataframe has {rows} rows and {cols} cols!')","a60b868c":"shape(df)","ce6ffd30":"df.info()","e23fde43":"df.describe()","2859b3e1":"df.describe(include='object')","2213f690":"#Covariance Table\npd.DataFrame(np.cov(df.iloc[:,1:].T), columns=df.columns[1:], index=df.columns[1:])","d7b4b70a":"#correlation plot\ndf.corr()","626a82e4":"plt.figure(figsize=(16,6))\nmask = np.triu(df.corr(),k=1)\nsns.heatmap(df.corr(), annot=True, linewidths=1, mask=mask, cmap='coolwarm')\nplt.title('Correlation Plot')\nplt.show()","abfe26e4":"sns.pairplot(df, diag_kind='kde', palette='Pastel1')\nplt.show()","484a3c61":"df.isna().sum().sum() #there are no missing values in this data","6449bfd5":"df['country'].nunique()","1f43ea1e":"country = df['country']\ndf.drop(['country'],1,inplace = True)","b0ce3fc5":"#standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nXsc = sc.fit_transform(df)","dd9e49cf":"dfsc = pd.DataFrame(Xsc, columns=df.columns)\ndfsc.head(2)","37e2ddb7":"from sklearn.decomposition import PCA\n\npca = PCA()\npcadf = pca.fit_transform(Xsc)","8f507027":"pcadf = pd.DataFrame(pcadf, columns=['PC'+str(i) for i in range(1,pcadf.shape[1]+1)])\npcadf.head(2)","5f948176":"np.cumsum(pca.explained_variance_ratio_)","520d13f9":"plt.figure(figsize=(18,5))\nclrs = ['grey' if i<0.90 else 'orange' for i in np.cumsum(pca.explained_variance_ratio_)]\ng = sns.barplot(x=pcadf.columns, y = pca.explained_variance_ratio_, palette = clrs)\nsns.lineplot(x=pcadf.columns, y = pca.explained_variance_ratio_,color='black')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance')\nplt.yticks(np.arange(0,0.7,0.1))\nfor p in g.patches:\n    g.annotate('{:.3f}'.format(p.get_height()), (p.get_x()+0.35, p.get_height()+0.003), ha='center',\n              va='bottom',color='black')\nplt.show()","8a3b70c6":"plt.figure(figsize=(18,5))\nsns.lineplot(x=pcadf.columns, y=np.cumsum(pca.explained_variance_ratio_),drawstyle='steps-pre',color='orange')\nplt.axhline(0.90,color='green')\nplt.axvline(4, color='green')\nplt.title('Cumulative Variance Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance')\nplt.grid()\nplt.show()","a9f612e4":"#refitting the pca with 5 principal components\npca = PCA(n_components=5)\npcadf = pca.fit_transform(Xsc)","11d381e7":"pcadf = pd.DataFrame(pcadf, columns=['PC'+str(i) for i in range(1,pcadf.shape[1]+1)])\npcadf.head(2)","0f897f9e":"mask = np.triu(pcadf.corr())\nsns.heatmap(pcadf.corr(), annot=True, linewidths=1, mask=mask, cmap='coolwarm',vmax=1)\nplt.title('PCA - Correlation Plot')\nplt.show()","3eabffd2":"sns.pairplot(pcadf)","06696847":"plt.figure(figsize=(18,8))\ni = 1\nfor pc in pcadf.columns:\n    plt.subplot(2,3,i)\n    sns.boxplot(x=pcadf[pc],palette='Pastel2')\n    i += 1\nplt.suptitle('Outliers in the principal components', color='darkgreen', fontsize=16)\nplt.show()","193d9fa4":"#Since we have a few outliers in the PCs, we will cap them as KMeans are sensitive to outliers\nfor pc in pcadf:\n    q1,q3,q10,q90 = pcadf[pc].quantile([0.25,0.75,0.1,0.90])\n    iqr = q3-q1\n    ul = q3 + 1.5*iqr\n    ll = q1 - 1.5*iqr\n    pcadf[pc] = pcadf[pc].apply(lambda x: q10 if x<ll else q90 if x>ul else x)","3ada29d8":"plt.figure(figsize=(18,8))\ni = 1\nfor pc in pcadf.columns:\n    plt.subplot(2,3,i)\n    sns.boxplot(x=pcadf[pc],palette='Pastel2')\n    i += 1\nplt.suptitle('Outliers in the principal components - post treatment', color='darkgreen', fontsize=16)\nplt.show()","e0f47414":"#finding the optimal number of cluster value\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ninertia_score = []\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(pcadf)\n    inertia_score.append(kmeans.inertia_)\n\n#Visualizing the inertia vs k plot\nplt.figure(figsize=(18,6))\nsns.lineplot(x=range(1,11),y=inertia_score,color='green')\nplt.xticks(range(1,11))\nplt.xlabel('k values')\nplt.ylabel('Inertia')\nplt.title('Elbow Plot to find the optimal number of clusters')\nplt.grid()\nplt.show()","6462870d":"km = KMeans(n_clusters=3)\nkm.fit(pcadf)\nprint(f'Inertia Score --> {km.inertia_}')\nprint(f'Silhouette Score --> {silhouette_score(pcadf,km.labels_)}')","e90b2fe5":"#Checking the distribution of cluster between PC1 and PC2\nplt.figure(figsize=(18,6))\nplt.scatter(pcadf['PC1'], pcadf['PC2'], c=km.labels_, cmap='viridis')\nplt.title('Comparing the clusters between PC1 and PC2')\nplt.show()","0d8a3158":"#Preparing an AGC model\nfrom scipy.cluster.hierarchy import cophenet,dendrogram,linkage\nfrom scipy.spatial.distance import pdist\nfor link in ['single','complete','average','ward']:\n    z = linkage(pcadf,link)\n    c, coph_dist = cophenet(z,pdist(pcadf))\n    print(f'{link} --> {c}') #closer it is to 1, better the clustering","59152699":"#As average has the best cophenet index, we will use it for the Agglomerative Clustering\nfrom sklearn.cluster import AgglomerativeClustering\nm2 = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='average')\nm2.fit(pcadf)","c11e6354":"#Plotting a Dendrogram\nplt.figure(figsize=(18,8))\nplt.title('Agglomerative Hierarchical Clustering - Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nz = linkage(pcadf,'average')\ndendrogram(z,leaf_rotation=90.0, leaf_font_size=8,truncate_mode='level',p=5,color_threshold=4)\nplt.tight_layout()","e0d1c7a4":"#silhouette score for the AGC model\nsilhouette_score(pcadf,m2.labels_)","5297e2c6":"#calculating the intertia for AGC model, since it's not an inbuilt method\npcadf_labelled = pcadf.copy(deep=True)\npcadf_labelled['class']=m2.labels_\nagc_clusters = pcadf_labelled.groupby('class')\ndf0 = agc_clusters.get_group(0)\ndf1 = agc_clusters.get_group(1)\ndf2 = agc_clusters.get_group(2)\n#Calculating the centroids\nc0 = np.array(df0.iloc[:,:-1].mean())\nc1 = np.array(df1.iloc[:,:-1].mean())\nc2 = np.array(df2.iloc[:,:-1].mean())","884537bb":"agc_inert0 = 0\nagc_inert1 = 0\nagc_inert2 = 0\nfor i in np.arange(df0.shape[0]):\n    agc_inert0 = agc_inert0+np.sum((df0.iloc[i,:-1]-c0)**2)\nfor i in np.arange(df1.shape[0]):\n    agc_inert1 += np.sum((df1.iloc[i,:-1]-c1)**2)\nfor i in np.arange(df2.shape[0]):\n    agc_inert2 += np.sum((df2.iloc[i,:-1]-c2)**2)\nagc_inertia = agc_inert0+agc_inert1+agc_inert2\nprint(agc_inertia)","da6ec32c":"#Comparing the inertia score of kmeans and agc model\nprint(f'KMeans Inertia Score --> {km.inertia_}')\nprint(f'AGC Inertia Score --> {agc_inertia}')\n\n#Comparing the silhouette score of kmeans and agc models\nprint(f'KMeans Silhouette Score --> {silhouette_score(pcadf,km.labels_)}')\nprint(f'AGC Silhouette Score --> {silhouette_score(pcadf,m2.labels_)}')","cf2cd1c1":"#Plotting a 3D plot between PC1,PC2 and PC2\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(8,6))\nax = Axes3D(fig, elev=-150, azim=100)\nax.scatter(pcadf['PC1'],pcadf['PC2'],pcadf['PC3'],c=km.labels_,cmap='viridis')\nax.set_title('Comparing the clusters between PC1, PC2 and PC3')\nax.set_xlabel('PC1')\nax.set_ylabel('PC2')\nax.set_zlabel('PC3')\nplt.show()","ad14925b":"#adding labels to the original dataframe\ndf1 = df.copy(deep=True)\ndf1['country'] = country\ndf1['labels'] = km.labels_\ndf1.head()","0ec196ca":"df1['labels'].value_counts()","a1ea1fc5":"df1['labels'].value_counts().plot(kind='pie', autopct='%.2f%%', explode=[0.1,0.1,0.1])","9bd9ebff":"df1.groupby('labels').mean()","0ae83936":"metrics = pd.DataFrame({'KMeans':[km.inertia_,silhouette_score(pcadf,km.labels_)],\n                       'AGC':[agc_inertia,silhouette_score(pcadf,m2.labels_)]},index=['Inertia','Silhouette Score'])\nmetrics","939ccb9a":"#Visualizing the same using barplots, separated by labels\ni = 1\nplt.figure(figsize=(18,12))\nfor col in df1.columns[:-2]:\n    plt.subplot(3,3,i)\n    sns.barplot(x=df1['labels'], y=df1[col], palette = 'Pastel1', ci=None)\n    i += 1\nplt.suptitle('Distribution of various parameters among different labels', color='darkgreen', fontsize=16, y=1.01)\nplt.tight_layout()\nplt.show()","f51f8371":"#finding out the list of countries in different labels\ncountry_clusters = df1.groupby('labels')['country']","ac826018":"#Developed Countries\nprint(country_clusters.get_group(2).unique())","d1fb5507":"#Developing Countries\nprint(country_clusters.get_group(0).unique())","c3ac8015":"#Poor Countries\nprint(country_clusters.get_group(1).unique())","96b93dae":"<p style='font-family:verdana; color:green'><b>There is one categorical, and 9 numerical columns. There are no missing values in the dataset<\/b><\/p>","a3bbcd90":"<p style='font-family:verdana; color:green'><b>All the outliers have been capped within 10th and 90th quantile of the features<\/b><\/p>","bf821bf2":"<p style='font-family:verdana; color:green'><b>A lower intertia is considered good, and so does a higher silhouette score\nAs we can see above, KMeans has got both, a lower inertia and a better silhouette score.\nSo we are choosing KMeans over AGC.\nA positive silhouette score is considered as a good model. As we can see, our silhouette score is approximately 0.4, which is a good one.\nSo we can say that it is a good model with good clusters.<\/b><\/p>","61ec67c7":"<p style='font-family:verdana; color:green'><b>As we can see from the cumulative variance plot, we need 5 principal components to retain >90% variance<\/b><\/p>","ba67346d":"<p style='font-family:verdana; color:green'><b>as we can see, there is a clear difference between the means of all the different labels, which suggest it is a good cluster<\/b><\/p>","02169651":"<p style='font-family:verdana; color:green'><b>There is some multicollinearity present in the data as \"total_fer\" is highly correlated with \"child_mort\", and \"gdpp\" is highly correlated with \"income\"\nAlso \"imports\" and \"exports\" are correlated with each other, and \"heatlh\" and \"gdpp\" have a positive correlation\n\"life_expec\" has a high negative correlation with the \"child_mort\" and \"total_fer\" <\/b><\/p>","9f06a17b":"<p style='font-family:verdana; color:green'><b>This dataset contains records for 167 different countries<\/b><\/p>","fe6a00d5":"<p style='font-family:verdana; color:green'><b>As we have seen from the correlation matrix, there is some mild to high correlation between the variables, which results in multicollinearity. To reduce the effect of multicollinearity, and to reduce the dimension, we use PCA. <\/b><\/p>","c90daf53":"___","47099528":"<p style='font-family:verdana; color:green'><b>As we can see, there is no multicollinearity in the dataset<\/b><\/p>","8524cff7":"<p style='font-family:verdana; color:green'><b>The different columns have different scale of data, so we'll need to do Standard Scaling before proceeding with the clusters<\/b><\/p>","80a56281":"<p style='font-family:verdana; color:green'><b>As we can see, KMeans model has lesser Inertia and Higher Silhoutte score than the AGC model, which suggests that it is a better model for this dataset<\/b><\/p>","bb132078":"<p style='font-family:verdana; color:green'><b>From the elbow plot, we can see that the optimal number of clusters is 3<\/b><\/p>","67c05b44":"As per the above table, there are 3 clusters of countries\nlabel 0 --> Having a very high GDP, exports and imports, income, health and life expectancy, and very low child mortality, inflation and total fertility\nlabel 1 --> Having least GDP, exports and imports, income, health and life expectancy, and high child mortality, inflation and total fertility\nlabel 2 --> all the parameters are between 0 and 1\n\nBased on the parameters, we can say that label 0 are developed, label 1 are poor and label 2 are developing countries"}}