{"cell_type":{"7eef2d70":"code","da3f0bc3":"code","31c950df":"code","cea6530e":"code","11986b78":"code","632a5149":"code","4b567485":"code","ec837f46":"markdown","9d396b81":"markdown","decb7766":"markdown","87e3c89a":"markdown","c55d488d":"markdown","0fa9a092":"markdown","64a3a426":"markdown","d53d7a7e":"markdown","c441fb5d":"markdown","db63d6bb":"markdown"},"source":{"7eef2d70":"import keras\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD","da3f0bc3":"# Load data\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint('x_shape:',x_train.shape)\nprint('y_shape:',y_train.shape)","31c950df":"# -1 means 28 * 28\n# \/255 means Normalization\nx_train = x_train.reshape(x_train.shape[0],-1)\/255.0\nx_test = x_test.reshape(x_test.shape[0],-1)\/255.0\n\n# One-Hot Encoding\ny_train = np_utils.to_categorical(y_train, num_classes=10)\ny_test =  np_utils.to_categorical(y_test,  num_classes=10)","cea6530e":"model = Sequential()\nmodel.add(Dense(units=10, input_dim=784, bias_initializer='one', activation='softmax'))\n\n# define learning rate of optimizer\nsgd = SGD(lr = 0.2)\n\n# define loss function and calculate score in compile function\nmodel.compile(optimizer=sgd, \n              loss='mse',\n              metrics=['accuracy'])","11986b78":"# epochs:iteration=10\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\n\n# value model\nloss, accuracy = model.evaluate(x_test, y_test)\n\nprint('\\ntest loss',loss)\nprint('accuracy', accuracy)","632a5149":"model = Sequential()\nmodel.add(Dense(units=10, input_dim=784, bias_initializer='one', activation='softmax'))\n\n# define learning rate of optimizer\nsgd = SGD(lr = 0.2)\n\n# define loss function and calculate score in compile function\nmodel.compile(optimizer=sgd, \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","4b567485":"# epochs:iteration=10\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\n\n# value model\nloss, accuracy = model.evaluate(x_test, y_test)\n\nprint('\\ntest loss',loss)\nprint('accuracy', accuracy)","ec837f46":"Train the model and value the model","9d396b81":"Build model:784 input neurons,10 output neurons","decb7766":"![image.png](attachment:image.png)","87e3c89a":"### Why one-hot encoding?\n\n![image.png](attachment:image.png)","c55d488d":"- 1.Convert trian_data shape(60000,28,28) to (60000,784)\n- 2.Normalization\n- 3.Hot-encoding\n\n![image.png](attachment:image.png)","0fa9a092":"But In reality it is often not the case.The adjustment is very slow at the gradient of 0.98, and the adjustment is very fast at the gradient of 0.82, which does not meet our expectations\n![image.png](attachment:image.png)","64a3a426":"### Conclusion\uff1a\nThe convergence speed is faster and the result is better when the loss function is cross-entropy model","d53d7a7e":"#### Q:Can we do better ?\n#### A:Yes,we change loss fuction 'mse' to 'crossentropy'\n\n![image.png](attachment:image.png)\n\nImagine \uff1aThe larger the gradient f'(z) of the activation function, the faster the size of w is adjusted, and the slower the adjustment, the slower the training convergence.","c441fb5d":"We do not change the activation function, but change the cost function to cross entropy.Different from the sigmoid function, the weight adjustment in the cross entropy function is proportional to the error\n![1.png](attachment:1.png)\n\n#### POINT:The cross-entropy function is often used in the classification application model, and the quadratic cost function is used in the regression model","db63d6bb":"![image.png](attachment:image.png)"}}