{"cell_type":{"fce80e08":"code","32fbdeed":"code","a4d26fa7":"code","a4011b6b":"code","5e2d089b":"code","1063b2a9":"code","136e6299":"code","8e3bb3fb":"code","278f763e":"code","6ccc1b98":"code","186d4120":"code","b96743a3":"code","05df00f6":"code","fea5981d":"code","08447096":"code","913fdf17":"code","4ee17826":"code","8d5a4f31":"code","d435530c":"code","7ecdb792":"code","700d4bdf":"code","d043adb2":"code","88c67e06":"markdown","49a244f3":"markdown","91285f6e":"markdown","6920e007":"markdown","0a3afe21":"markdown","a94e2ec3":"markdown","6639a9e7":"markdown","99c09d82":"markdown","7309ef8e":"markdown","e1ba419d":"markdown","b7ae5233":"markdown","5a947f78":"markdown","e71a5e25":"markdown","49d9a73a":"markdown","0989e5be":"markdown","a7bdf562":"markdown","8ae97cb5":"markdown","81854a38":"markdown","be8ff0a1":"markdown","c29b8137":"markdown","8711b2e8":"markdown","2dedc559":"markdown","ffa8edaf":"markdown","d3c6430e":"markdown","54c10ccd":"markdown","13601462":"markdown","3b4c2522":"markdown","b9c76582":"markdown","b4131cd7":"markdown","32c30307":"markdown","38933147":"markdown"},"source":{"fce80e08":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","32fbdeed":"%%writefile memory_patterns.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 12\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action\n","a4d26fa7":"from kaggle_environments import make, evaluate\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 1000})\n\nenv.run([\"memory_patterns.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=800, height=800)","a4011b6b":"seasons = 100\nepisodes = 1000","5e2d089b":"import numpy as np\nimport pandas as pd\nimport json\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make\n\nfrom IPython.display import Markdown as md\n\naction_board = pd.DataFrame(columns = [\"season\",\n                                      \"episode\",\n                                      \"Memory Action\",\n                                      \"Nash Action\",\n                                      \"Memory Reward\",\n                                      \"Nash Reward\"])\nleaderboard = pd.DataFrame(columns = [\"season\",\n                                      \"Memory Reward\",\n                                      \"Nash Reward\"])\n\n\nindex = 0\nenv = make(\"rps\", configuration={\"episodeSteps\": episodes})\n\nfor season in range(seasons):\n    env.reset()\n    results = env.run([\"memory_patterns.py\", \"nash_equilibrium.py\"])\n    for result in results:\n        if (result[0].observation.step == 0):\n            continue\n        action_board = action_board.append({\"season\": season,\n                              \"episode\": result[0].observation.step,\n                              \"Memory Action\": result[0].action,\n                              \"Nash Action\": result[1].action,\n                              \"Memory Reward\": result[0].reward,\n                              \"Nash Reward\": result[1].reward},\n                                        ignore_index=True)\n        if result[0].status == \"DONE\":\n            leaderboard = leaderboard.append({\"season\": season,\n                              \"Memory Reward\": result[0].reward,\n                              \"Nash Reward\": result[1].reward},\n                                        ignore_index=True)","1063b2a9":"md('# Memory Patterns - Nash Equilibrium : {} - {}'.format(len(leaderboard[leaderboard[\"Memory Reward\"] > 0]), len(leaderboard[leaderboard[\"Nash Reward\"] > 0])))","136e6299":"md('# Tie : {}'.format(len(leaderboard[leaderboard[\"Memory Reward\"] == 0])))","8e3bb3fb":"if (len(leaderboard[leaderboard[\"Memory Reward\"] > 0]) == len(leaderboard[leaderboard[\"Nash Reward\"] > 0])):\n    winner = \"Tie!\"\nelif (len(leaderboard[leaderboard[\"Memory Reward\"] > 0]) > len(leaderboard[leaderboard[\"Nash Reward\"] > 0])):\n    winner = \"Winner is Memory Patterns!\"\nelse:\n    winner = \"Winner is Nash!\"\nmd('<a id=\"11\"><\/a><h1 style=\\'background:#FBE338; border:0; color:black\\'><center>{}<center><h2>'.format(winner))","278f763e":"leaderboard.plot(subplots=True, figsize=(15,10))","6ccc1b98":"leaderboard[['Memory Reward', 'Nash Reward']].plot.hist(bins=10,  alpha=0.5, figsize=(15,10))","186d4120":"action_board[['Memory Action', 'Nash Action']].plot.hist(bins=3, alpha=0.5, xticks=[0,1,2], figsize=(15,10))","b96743a3":"fig, ax = plt.subplots(figsize=(20,10))\nfor i, g in action_board.groupby('season'):\n    g.plot(x='episode', y='Memory Reward', ax=ax, legend=False )","05df00f6":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[(action_board['episode']<episodes\/2)].groupby('season'):\n    g.plot(x='episode', y='Memory Reward', ax=ax, legend=False )","fea5981d":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[((action_board['episode']>episodes\/3) & (action_board['episode']<2*episodes\/3))].groupby('season'):\n    g.plot(x='episode', y='Memory Reward', ax=ax, legend=False )","08447096":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[action_board['episode']>episodes\/2].groupby('season'):\n    g.plot(x='episode', y='Memory Reward', ax=ax, legend=False )","913fdf17":"leaderboard.head()","4ee17826":"leaderboard.tail()","8d5a4f31":"leaderboard.describe()","d435530c":"action_board.head()","7ecdb792":"action_board.tail()","700d4bdf":"action_board.drop(columns='season').describe()","d043adb2":"# Report boards\nleaderboard_csv = 'Memory_Patterns_leaderboard_S' + str(seasons) + 'E' + str(episodes) + '.csv'\naction_board_csv = 'Memory_Patterns_action_board_S'+ str(seasons) + 'E' + str(episodes) + '.csv'\nleaderboard.to_csv(leaderboard_csv)\naction_board.to_csv(action_board_csv)\nprint(leaderboard_csv)\nprint(action_board_csv)","88c67e06":"## Actions Statistics","49a244f3":"# Season's results","91285f6e":"<h1 style='background:#FBE338; border:0; color:black'><center>Analysis<center><h1>","6920e007":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>","0a3afe21":"## Rewards Statistics ","a94e2ec3":"# References","6639a9e7":"# Data export","99c09d82":"Dataset is exported, collected and publicly shared in [Rock Paper Scissors Agents Battles](https:\/\/www.kaggle.com\/jumaru\/rock-paper-scissors-agents-battles) dataset.","7309ef8e":"* [Rock Paper Scissors - Nash Equilibrium Strategy](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-nash-equilibrium-strategy) & [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison) by [Yaroslav Isaienkov](https:\/\/www.kaggle.com\/ihelon)\n* [(Not so) Markov](https:\/\/www.kaggle.com\/alexandersamarin\/not-so-markov) by [Alexander Samarin](https:\/\/www.kaggle.com\/alexandersamarin)\n* [LB simulation](https:\/\/www.kaggle.com\/superant\/lb-simulation) by [Ant \ud83d\udc1c](https:\/\/www.kaggle.com\/superant)\n* [Memory_Patterns](https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns) by [Yegor Biryukov](https:\/\/www.kaggle.com\/yegorbiryukov)","e1ba419d":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a9\/John_Forbes_Nash%2C_Jr._by_Peter_Badge.jpg\/220px-John_Forbes_Nash%2C_Jr._by_Peter_Badge.jpg)","b7ae5233":"### Last 5 seasons rewards","5a947f78":"## Last 5 actions","e71a5e25":"## First half rewards","49d9a73a":"# Action board","0989e5be":"<a id=\"11\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Validate<center><h2>\n\n\n","a7bdf562":"## Mid-episodes reward","8ae97cb5":"<a id=\"11\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Action<center><h2>\n\n","81854a38":"# Season's reward histogram","be8ff0a1":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Memory Patterns\n    <center><h2>","c29b8137":"## Leaderboard","8711b2e8":"# Actions histogram","2dedc559":"*...if we all go for the blonde we are blocking each other.*","ffa8edaf":"<h1 style='background:#FBE338; border:0; color:black'><center>Dataset<center><h1>","d3c6430e":"<h1 style='background:#FBE338; border:0; color:black'><center>Conclusion<center><h1>","54c10ccd":"<h1 style='background:#FBE338; border:0; color:black'><center>Result<center><h1>\n","13601462":"## Last half rewards","3b4c2522":"## All episodes reward","b9c76582":"## First 5 actions","b4131cd7":"# Memory Patterns vs Nash Equilibrium: Rock Paper Scissors\n\n\n### 100 seasons of Memory Patterns vs Nash on Rock Paper Scissors\n### 1000 episodes per season\n\n### Bonus: Dataset generation","32c30307":"### First 5 seasons rewards","38933147":"[Memory_Patterns](https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns) by [Yegor Biryukov](https:\/\/www.kaggle.com\/yegorbiryukov)"}}