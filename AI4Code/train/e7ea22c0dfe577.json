{"cell_type":{"9b5abfe4":"code","33823675":"code","8de88524":"code","08404b3a":"code","b16ade2f":"code","c59d469a":"code","a6f49066":"code","c56abba2":"code","20fc186f":"code","4202cd6b":"code","91309885":"code","6846098e":"code","15db0aaa":"code","719b575f":"code","a6877c87":"code","11c23a9c":"code","cb807dc8":"code","449bfa8c":"code","e055b4d9":"code","40065961":"code","407ca2fe":"code","504c75c5":"code","e572c126":"code","843b7310":"markdown","1f2e87d9":"markdown","6e9f107e":"markdown","f8ac023a":"markdown","fdf6ea74":"markdown","d872e86e":"markdown","a782475f":"markdown","e459db9e":"markdown","abde2333":"markdown","5d305672":"markdown","c3a83d0e":"markdown","c66eb9bf":"markdown","fbfa4a1d":"markdown","803283ec":"markdown","2562f622":"markdown","b68c57fc":"markdown","2c6cc353":"markdown","b9b88022":"markdown","d8b944ca":"markdown","d09e159f":"markdown"},"source":{"9b5abfe4":"import collections\nimport numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf\nimport random\n\nfrom nltk.translate.bleu_score import corpus_bleu\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras import Sequential","33823675":"DF = pd.read_csv(\"..\/input\/language-translation-englishfrench\/eng_-french.csv\")","8de88524":"DF","08404b3a":"DF = DF.rename(columns={'English words\/sentences': 'English', 'French words\/sentences': 'French'})\nDF","b16ade2f":"english = DF.English\nenglish","c59d469a":"french = DF.French\nfrench","a6f49066":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i,text in enumerate(english):\n    stri = \"\"\n    txt = tokenizer.tokenize(text)\n    for j in txt:\n        j = j.lower()\n        stri = stri + j\n        stri = stri + \" \"\n    english[i] = stri","c56abba2":"print(english[0:10])","20fc186f":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i,text in enumerate(french):\n    stri = \"\"\n    txt = tokenizer.tokenize(text)\n    for j in txt:\n        j = j.lower()\n        stri = stri + j\n        stri = stri + \" \"\n    french[i] = stri","4202cd6b":"print(french[0:10])","91309885":"n1 = 0\nn2 = 100\neng = list(english)\nfre = list(french)\n\n# for DF in english:eng.append(DF)\n\n# for DF in french:fre.append(DF)\n\neng = np.asarray(eng)\nfre = np.asarray(fre)\n\neng = eng[0:175000]\nfre = fre[0:175000]\n\nfor i in range(n1,n2):\n  print(eng[i] + \"\\t->\\t\" + fre[i] + \"\\n\")","6846098e":"e = [word for sentence in eng for word in sentence.split(\" \")]\nf = [word for sentence in fre for word in sentence.split(\" \")]\nenglish_word_counter = collections.Counter(e)\nfrench_word_counter = collections.Counter(f)","15db0aaa":"print('{} English words.'.format(len(e)))\nprint('{} French words.'.format(len(f)))\nprint(\"\\n\")\nprint('{} unique English words.'.format(len(english_word_counter)))\nprint('{} unique French words.'.format(len(french_word_counter)))\nprint(\"\\n\")\nprint('10 Most common words in the English dataset:')\nprint('\"' + '\" \"'.join(list(zip(*english_word_counter.most_common(10)))[0]) + '\"')\nprint(\"\\n\")\nprint('10 Most common words in the French dataset:')\nprint('\"' + '\" \"'.join(list(zip(*french_word_counter.most_common(10)))[0]) + '\"')","719b575f":"dict1 = {1: [\"English \", 1133720, 13917 ], \n     2: [\"French\", 1250733, 23918] \n     } \n# Print the names of the columns. \nprint (\"{:<15} {:<15} {:<15}\".format('LANGUAGE', 'TOTAL WORDS', 'UNIQUE WORDS')) \n  \n# print each data item. \nfor key, value in dict1.items(): \n    language, total_words, unique_words = value \n    print (\"{:<15} {:<15} {:<15}\".format(language, total_words, unique_words)) ","a6877c87":"def tokenize(x):\n    tokenizer = Tokenizer(char_level=False,oov_token=\" \")\n    tokenizer.fit_on_texts(x)\n    return tokenizer.texts_to_sequences(x), tokenizer\n\ntext_sentences = [\n    'An apple a day keeps a doctor away .',\n    'well, hope this letter of mine finds u in pink of your health .',\n    'This is a short sentence .']\n  \ntext , tokenizer = tokenize(text_sentences)\nprint(text)\nprint(tokenizer.word_index)","11c23a9c":"def pad(x,length=None):\n    if (length==None):\n        length = max([len(sentence) for sentence in x])\n    a = pad_sequences(x,maxlen=length,padding=\"post\")\n    return a\n\n","cb807dc8":"def preprocess(x,y):\n    preprocess_x,x_tk = tokenize(x)\n    preprocess_y,y_tk = tokenize(y)\n\n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n    print(*preprocess_y.shape)\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n\n    return preprocess_x, preprocess_y, x_tk, y_tk\n\ndef preprocessing(x):\n    preprocess_x,x_tk = tokenize(x)\n    preprocess_x = pad(preprocess_x)\n    return preprocess_x, x_tk","449bfa8c":"pre_eng,pre_fre,eng_tk,fre_tk = preprocess(eng,fre)\nmax_eng_seq_len = pre_eng.shape[1]\nmax_fr_seq_len = pre_fre.shape[1]\nenglish_vocab_size = len(eng_tk.word_index)\nfrench_vocab_size = len(fre_tk.word_index)\n\nprint('Data Preprocessed')\nprint(\"Max English sentence length:\", max_eng_seq_len)\nprint(\"Max French sentence length:\", max_fr_seq_len)\nprint(\"English vocabulary size:\", english_vocab_size)\nprint(\"French vocabulary size:\", french_vocab_size)","e055b4d9":"def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n\n    learning_rate = 0.001\n    model = keras.Sequential([\n                                Embedding(english_vocab_size+1, \n                                          128, \n                                          input_length = input_shape[1]),\n        \n                                Bidirectional(GRU(128, \n                                                  return_sequences=True)),\n        \n                                tf.keras.layers.Dropout(0.25),\n        \n                                TimeDistributed(Dense(french_vocab_size, \n                                                      activation='softmax'))\n                                ])\n    model.summary()\n    \n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(learning_rate),\n                  metrics=['accuracy'])\n    \n    return model\n\ntmp_x = pad(pre_eng, \n            max_fr_seq_len)\n\nrnn_model = embed_model(tmp_x.shape,\n                        max_fr_seq_len,\n                        english_vocab_size,\n                        french_vocab_size)\n\nrnn_model.fit(tmp_x, pre_fre, batch_size=1024, epochs=20, validation_split=0.2)\n\nrnn_model.save_weights(\"rnn_model_weights.h5\")","40065961":"def logits_to_text(logits, tokenizer):\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = '<PAD>'\n\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])","407ca2fe":"for k in range(10):\n    predicted=[]\n    a = random.randint(0,100000)\n    print('Random Index: ', a)\n\n    print(\"PREDICTED:\\t\", end=' ')\n    for i in range(5):\n        x = logits_to_text(rnn_model.predict(tmp_x[a])[i], fre_tk)\n        print(x, end =' ')\n        if x!='<PAD>':\n            predicted.append(x)\n\n    \n    english = eng[a].split()\n    french = fre[a].split()\n\n    print(\"\\n\\nENGLISH:\\t\", eng[a] + \"\\nFRENCH:\\t\\t \" + fre[a] + \"\\n\")\n    print('\\nIndexing: ',tmp_x[a])\n    \n    print('\\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\\n')","504c75c5":"n = []\nlst=[]\nk=0\nwhile k<1000:\n    \n    predicted=[]\n    a = random.randint(0,175000)\n    \n    for i in range(5):\n        x = logits_to_text(rnn_model.predict(tmp_x[a])[i], fre_tk)\n    \n        if x!='<PAD>':\n            predicted.append(x)\n\n    english = eng[a].split()\n    french = fre[a].split()\n\n    references = [[french]]\n    candidates = [predicted]\n    score = corpus_bleu(references, candidates, weights=(0.05, 0.25, 0.35, 0.35))\n    lst.append(score)\n    if score>.8:\n        n+=[score]\n    k+=1\n    \n    if k%100==0:\n        print(k)        \ndef average(lst):\n    print(sum(lst)\/len(lst))   \naverage(n)","e572c126":"print(\"AVERAGE BLEU SCORE: \", end ='\\t') \naverage(n)\n","843b7310":"## Reading the Data","1f2e87d9":"## Transforming data into arrays","6e9f107e":"# Pre-Processing of the Data","f8ac023a":"# Model Implementation ","fdf6ea74":"## Import necessary libraries","d872e86e":"## Tokenizing the English DataFrame","a782475f":"## Counting English and French Words","e459db9e":"## Separating the dataset into English and French","abde2333":"## Renaming the columns of DataFrame","5d305672":"## Printing the first Ten Tokenized words of English DataFrame","c3a83d0e":"## Sorting the above data into table form","c66eb9bf":"## Indexing of complete Dataset","fbfa4a1d":"---","803283ec":"## Tokenizing the French DataFrame","2562f622":"## Printing the first Ten Tokenized words of French DataFrame","b68c57fc":"## CALCULATING BLEU SCORE","2c6cc353":"## Checking for translation ","b9b88022":"## Viewing the datasest","d8b944ca":"## Preprocessed Information about the data","d09e159f":"## Indexing of a smaple text with help of tokenization"}}